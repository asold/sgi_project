{
  "title": "On the Calibration of Massively Multilingual Language Models",
  "url": "https://openalex.org/W4385573123",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2955136849",
      "name": "Kabir Ahuja",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2013710467",
      "name": "Sunayana Sitaram",
      "affiliations": [
        "Microsoft Research (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2148686933",
      "name": "Sandipan Dandapat",
      "affiliations": [
        "Microsoft (India)"
      ]
    },
    {
      "id": "https://openalex.org/A2162966668",
      "name": "Monojit Choudhury",
      "affiliations": [
        "Microsoft (India)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4285241202",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W2739967986",
    "https://openalex.org/W3159721665",
    "https://openalex.org/W3100880133",
    "https://openalex.org/W3174750111",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3099023595",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W3038047279",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W4286901803",
    "https://openalex.org/W2970752815",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W4221150735",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4297807150",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4297798436",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W3206907172",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W2915774325",
    "https://openalex.org/W2950733326",
    "https://openalex.org/W3007404550",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2595653137"
  ],
  "abstract": "Massively Multilingual Language Models (MMLMs) have recently gained popularity due to their surprising effectiveness in cross-lingual transfer. While there has been much work in evaluating these models for their performance on a variety of tasks and languages, little attention has been paid on how well calibrated these models are with respect to the confidence in their predictions. We first investigate the calibration of MMLMs in the zero-shot setting and observe a clear case of miscalibration in low-resource languages or those which are typologically diverse from English. Next, we empirically show that calibration methods like temperature scaling and label smoothing do reasonably well in improving calibration in the zero-shot scenario. We also find that few-shot examples in the language can further help reduce calibration errors, often substantially. Overall, our work contributes towards building more reliable multilingual models by highlighting the issue of their miscalibration, understanding what language and model-specific factors influence it, and pointing out the strategies to improve the same.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 4310–4323\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nOn the Calibration of Massively Multilingual Language Models\nKabir Ahuja1 Sunayana Sitaram1 Sandipan Dandapat2 Monojit Choudhury2\n1 Microsoft Research, India\n2 Microsoft R&D, India\n{t-kabirahuja,sadandap,sunayana.sitaram,monojitc}@microsoft.com\nAbstract\nMassively Multilingual Language Models\n(MMLMs) have recently gained popularity due\nto their surprising effectiveness in cross-lingual\ntransfer. While there has been much work in\nevaluating these models for their performance\non a variety of tasks and languages, little at-\ntention has been paid on how well calibrated\nthese models are with respect to the confidence\nin their predictions. We first investigate the\ncalibration of MMLMs in the zero-shot set-\nting and observe a clear case of miscalibra-\ntion in low-resource languages or those which\nare typologically diverse from English. Next,\nwe empirically show that calibration methods\nlike temperature scaling and label smoothing\ndo reasonably well in improving calibration in\nthe zero-shot scenario. We also find that few-\nshot examples in the language can further help\nreduce calibration errors, often substantially.\nOverall, our work contributes towards building\nmore reliable multilingual models by highlight-\ning the issue of their miscalibration, understand-\ning what language and model-specific factors\ninfluence it, and pointing out the strategies to\nimprove the same.\n1 Introduction\nMassively Multilingual Language Models\n(MMLMs) like mBERT (Devlin et al., 2019),\nXLMR (Conneau et al., 2020), mT5 (Xue et al.,\n2021) and mBART (Liu et al., 2020) have been\nsurprisingly effective at zero-shot cross-lingual\ntransfer i.e. when fine-tuned on an NLP task in one\nlanguage, they often tend to generalize reasonably\nwell in languages unseen during fine-tuning.\nThese models have been evaluated for their per-\nformance across a range of multilingual tasks (Pan\net al., 2017; Nivre et al., 2018; Conneau et al., 2018)\nand numerous methods like adapters (Pfeiffer et al.,\n2020), sparse fine-tuning (Ansell et al., 2022) and\nfew-shot learning (Lauscher et al., 2020) have been\nproposed to further improve performance of cross-\nlingual transfer.\nDespite these developments, there has been lit-\ntle to no attention paid to the calibration of these\nmodels across languages i.e. how reliable the con-\nfidence predictions of these models are. As these\nmodels find their way more and more into the\nreal word applications with safety implications,\nlike Hate Speech Detection (Davidson et al., 2017;\nDeshpande et al., 2022) it becomes important to\nonly take extreme actions for high confidence pre-\ndictions by the model (Sarkar and KhudaBukhsh,\n2021). Hence, calibrated confidences are desirable\nto have when deploying such systems in practice.\nGuo et al. (2017) showed that modern neural net-\nworks used for Image Recognition (He et al., 2016)\nthough perform much better than the ones intro-\nduced decades ago (Lecun et al., 1998), but are sig-\nnificantly worse calibrated and often over-estimate\ntheir confidence on incorrect predictions. For\nNLP tasks specifically, Desai and Durrett (2020)\nshowed that classifiers trained using pre-trained\ntransformer-based models (Devlin et al., 2019) are\nwell calibrated both in-domain and out-of-domain\nsettings compared to non-pre-trained model base-\nlines (Chen et al., 2017). Notably, Ponti et al.\n(2021) highlights, since zero-shot cross-lingual\ntransfer represents shifts in the data distribution\nthe point estimates are likely to be miscalibrated,\nwhich forms the core setting of this work.\nIn light of this, our work has three main contri-\nbutions. First, we investigate the calibration of two\ncommonly used MMLMs: mBERT and XLM-R on\nfour NLU tasks under zero-shot setting where the\nmodels are fine-tuned in English and calibration\nerrors are computed on unseen languages. We find\na clear increase in calibration errors compared to\nEnglish as can be seen in Figures 1a and 1b, with\ncalibration being significantly worse for Swahili\ncompared to English.\nSecond, we look for factors that might affect the\nzero-shot calibration of MMLMs and find in most\ncases that the calibration error is strongly correlated\n4310\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n% of Samples\nEnglish\n(Out Of Box)\nAvg.\nAccuracy\nAvg.\nConﬁdence\n0.0 0.5 1.0\nConﬁdence\n0.0\n0.5\n1.0\nAccuracy Error: 6.66\nOutput\nGap\n(a)\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n% of Samples\nSwahili\n(Out Of Box)\nAvg.\nAccuracy\nAvg.\nConﬁdence\n0.0 0.5 1.0\nConﬁdence\n0.0\n0.5\n1.0\nAccuracy Error: 16.99\nOutput\nGap (b)\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n% of Samples\nSwahili\n(Calibrate using en data)\nAvg.\nAccuracy\nAvg.\nConﬁdence\n0.0 0.5 1.0\nConﬁdence\n0.0\n0.5\n1.0\nAccuracy Error: 11.54\nOutput\nGap (c)\n0.0 0.5 1.0\n0.0\n0.5\n1.0\n% of Samples\nSwahili\n(Calibrate using sw data)\nAvg.\nAccuracy\nAvg.\nConﬁdence\n0.0 0.5 1.0\nConﬁdence\n0.0\n0.5\n1.0\nAccuracy Error: 5.69\nOutput\nGap (d)\nFigure 1: Reliability Diagrams for XLMR fine-tuned on XNLI using training data in English. 1a and 1b provides the\ncalibration of XLMR fine-tuned without any calibration technique on English and Swahili. 1c and 1d are obtained\nby calibrating the model using TS + LS and Self-TS + LS techniques respectively as described in Section 2.1\nwith pre-training data size, syntactic similarity, and\nsub-word overlap between the unseen language\nand English. This reveals that MMLMs are mis-\ncalibrated in the zero-shot setting for low-resource\nlanguages and languages that are typologically dis-\ntant from English.\nFinally, we show that model calibration across\ndifferent languages can be substantially improved\nby utilizing standard calibration techniques like\nTemperature Scaling (Guo et al., 2017) and Label\nSmoothing (Pereyra et al., 2017) without collecting\nany data in the language (see Figure 1c). Using\na few examples in a language (the few-shot set-\nting), we see even more significant drops in the\ncalibration errors as can be seen in Figure 1d.\nTo the best of our knowledge, ours is the first\nwork to investigate and improve the calibration of\nMMLMs. We expect this study to be a significant\ncontribution towards building reliable and linguisti-\ncally fair multilingual models. To encourage future\nresearch in the area we make our code publicly\navailable1.\n2 Calibration of Pre-trained MMLMs\nConsider a classifier h: X→ [K] obtained by fine-\ntuning an MMLM for some task with training data\nin a pivot language p, where [K] denotes the set\nof labels {1,2,··· ,K}. We assume hcan predict\nconfidence of each of the [K] labels and is given\nby hk(x) ∈[0,1] for the kth label. his said to be\n1https://github.com/microsoft/MMLMCalibration\ncalibrated if the following equality holds:\np(y= k|hk(x) =q) =q\nIn other words, for a perfectly calibrated classi-\nfier, if the predicted confidence for a label k on\nan input xis q, then with a probability qthe input\nshould actually be labelled k. Naturally, in practi-\ncal settings the equality does not hold, and neural\nnetwork based classifiers are often miscalibrated\n(Guo et al., 2017). One way of quantifying this\nnotion of miscalibration is through the Expected\nCalibration Error (ECE) which is defined as the\ndifference in expectation between the confidence\nin classifier’s predictions and their accuracies (Re-\nfer Appendix A.1 for details). In our experiments\nwe compute ECE on each language l’s test data\nand denote their corresponding calibration errors\nas ECE(l).\n2.1 Calibration Methods\nWe briefly review some commonly used methods\nfor calibrating neural network based classifiers.\n1. Temperature Scaling (TS and Self-TS)(Guo\net al., 2017) is applied by scaling the output logits\nusing a temperature parameter T before applying\nsoftmax i.e. :\nhk(x) = exp ok(x)/T∑\nk′∈K exp ok′(x)/T\n, where ok denotes the logits corresponding to the\nkth class. T is a learnable parameter obtained post-\ntraining by maximizing the log-likelihood on the\ndev set while keeping other network parameters\n4311\nfixed. We experiment with two settings for improv-\ning calibration on a target language: using dev data\nin English to perform temperature scaling (TS) and\nusing the target language’s dev data (Self-TS).\n2. Label Smoothing (LS)(Pereyra et al., 2017) is a\nregularization technique that penalizes low entropy\ndistributions by using soft labels that are obtained\nby assigning a fixed probability q = 1−αto the\ntrue label (0 <α< 1), and distributing the remain-\ning probability mass uniformly across the remain-\ning classes. Label smoothing has been empirically\nshown to be competitive with temperature scaling\nfor calibration (Müller et al., 2019) especially in\nout of domain settings (Desai and Durrett, 2020)\n3. Few-Shot Learning (FSL)We also investigate\nif fine-tuning the MMLM on a few examples in a\ntarget language in addition to the data in English,\nleads to any improvement in calibration as it does\nin the performance (Lauscher et al., 2020). Since\nthese models are expected to be calibrated worse\nfor out-of-domain data compared to in-domain data\n(Desai and Durrett, 2020), we try to improve cali-\nbration by reducing the domain shift through few-\nshot learning.\nApart from these, we also consider combinations\nof different calibration methods in our experiments,\nincluding Label Smoothing with Temperature Scal-\ning (TS + LS or Self-TS + LS) and Few-Shot Learn-\ning with Label Smoothing (FSL + LS).\n3 Experiments\nWe seek to answer the following research questions:\na) How well calibrated are fine-tuned MMLMs in\nthe zero-shot cross lingual setting? b) What linguis-\ntic and model-specific factors influence calibration\nerrors across languages? c) Can we improve the\ncalibration of fine-tuned models across languages?\n3.1 Experimental Setup\nDatasets We consider 4 multilingual classification\ndatasets to study calibration of MMLMs which in-\nclude: i) The Cross-Lingual NLI Corpus (XNLI)\n(Conneau et al., 2018), ii) Multilingual Dataset\nfor Causal Commonsense Reasoning (XCOPA)\n(Ponti et al., 2020), iii) Multilingual Amazon Re-\nviews Corpus (MARC) (Keung et al., 2020) and, iv)\nCross-lingual Adversarial Dataset for Paraphrase\nIdentification (PAWS-X) (Yang et al., 2019). Statis-\ntics of these datasets can be found in Table 5.\nTraining setup We consider two commonly\nused MMLMs in our experiments i.e. Multilin-\nDataset MMLM ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nXNLI XLM-R 7.32 13.34 19.07 (sw)\nmBERT 5.44 12.34 45.15 (th)\nXCOPA XLM-R 14.54 20.07 29.33 (sw)\nmBERT 23.4 23.51 29.02 (sw)\nMARC XLM-R 7.15 9.65 13.45 (zh)\nmBERT 9.38 11.11 17.33 (ja)\nPAWS-X XLM-R 1.93 4.28 5.88 (ja)\nmBERT 3.57 10.32 15.65 (ko)\nTable 1: Calibration Errors across tasks for XLM-R\nand mBERT.L′ in the fourth column denotes the set of\nsupported languages in a task other than English. The\nlanguage in parenthesis in the column 5 denotes the\nlanguage with maximum calibration error.\nDataset SIZE SYN SWO\nXNLI -0.8 -0.88 -0.85\nXCOPA -0.85 -0.73 -0.62\nMARC -0.41 -0.46 -0.27\nPAWS-X -0.48 -0.93 -0.92\nTable 2: Pearson correlation coefficient of ECE with\nSIZE, SYN, and SWO features of different languages\nin the test set for XLMR.\ngual BERT (mBERT) (Devlin et al., 2019), and\nXLM-RoBERTa (XLMR) (Conneau et al., 2020).\nmBERT is only available in the base variant with\n12 layers and for XLMR we use the large variant\nwith 24 layers. We use English training data to\nfine-tune the two MMLMs on all the tasks and eval-\nuate the accuracies and ECEs on the test data for\ndifferent languages. For the few-shot case we use\nthe validation data in target languages to do con-\ntinued fine-tuning (FSL) and temperature scaling\n(Self-TS). Refer to Section A.3 in the Appendix for\nmore details.\n3.2 Results\nOut of Box Zero-Shot Calibration (OOB)We\nfirst investigate how well calibrated MMLMs are\non the languages unseen during fine-tuning without\napplying any calibration techniques. As can be seen\nin the Table 1, the average calibration error on lan-\nguages other than English (column 4) is almost al-\nways significantly worse than the errors on English\ntest data (column 3) for both mBERT and XLMR\nacross the 4 tasks. Along with the expected calibra-\ntion errors across unseen languages we also report\nthe worst case ECE (in column 5), where we see\n2×to 5×increase in errors compared to English.\nThe worst case calibration is commonly observed\n4312\nDataset MMLM Zero-Shot Calibration Few-Shot Calibration\nOOB TS LS TS + LS Self-TS Self-TS + LS FSL FSL + LS\nXNLI XLM-R 13.34 6.74 6.93 4.89† 5.41 4.05‡ 7.67 4.36\nmBERT 12.34 6.29† 10.42 6.70 4.77 4.69 3.14 2.55‡\nXCOPA XLM-R 20.07 15.95 5.47 4.52† 16.02 4.06‡ 8.94 4.39\nmBERT 23.51 20.02 12.41 6.77† 20.09 6.89 3.75 3.54‡\nTable 3: Calibration Errors (El∈L′ [ECE(l)]) for XLM-R and mBERT on using different methods for calibration. We\ncategorize the methods into zero-shot i.e. the methods that do not use any target language data to calibrate and\nfew-shot for the methods that require some examples in target language. Detailed results are in Table 9 of Appendix\nfor low resource languages like Swahili or the lan-\nguages that are typologically diverse from English\nlike Japanese. Consequently, the overall calibration\nis worse on tasks like XCOPA and XNLI compared\nto PAWS-X and MARC, as the former two have\nmore diverse set of languages while the latter con-\nsists of high resource languages only.\nFactors Affecting Calibration Next, we ana-\nlyze which model-specific and typological features\nmight influence the out-of-box calibration across\nlanguages. For a given task and MMLM, we com-\npute Pearson correlation coefficients between the\ncalibration errors and three factors studied exten-\nsively in zero shot cross lingual transfer literature\nwhich are:\ni) SIZE: Logarithm of the pre-training data size\n(number of tokens) in a language i.e. how well\na language is represented in the pre-training cor-\npus of an MMLM (Lauscher et al., 2020; Wu and\nDredze, 2020),\nii) SYN: We utilize the syntactic features provided\nby the URIEL project (Littell et al., 2017) to com-\npute the syntactic similarity between the pivot and\ntarget language as done in Lin et al. (2019).\niii) SWO: Finally we consider the sub-word over-\nlap between the pivot and target language as de-\nfined in Srinivasan et al. (2021). To compute SWO,\nfirst vocabularies Vp and Vt are identified for the\npivot and target langauge respectively by tokeniz-\ning the wikipedias in the two languages and getting\nrid of the tokens that appear less than 10 times in\nthe corpora. The subword overlap is then computed\nas :\nSWO = |Vp ∩Vt|\n|Vp ∪Vt|\n.\nIn the case of XLMR, for all the tasks except\nMARC, we observe strong negative correlations be-\ntween ECE and the three factors mentioned above\n(Table 2), meaning that lower the amount of pre-\ntraining data present for a language or its relat-\nedness with English, the higher is the calibration\nerror. Out of the three, the correlations are more\nconsistently (negatively) high with SYN. We ob-\nserve similar correlations albeit to a slightly lower\nextent for mBERT as well (Table 6 in Appendix).\nImproving Calibration Now that we have iden-\ntified miscalibration as an issue in MMLMs and\nfactors influencing the same, we seek to improve\ntheir calibration across languages. We utilize the\ncalibration methods described in Section 2.1, and\nreport the average calibration errors across the un-\nseen languages in Table 3 for XNLI and XCOPA\ndatasets 2. Both zero-shot calibration methods (TS\nand LS) that only make use of English data to cal-\nibrate can be seen to obtain substantially lower\ncalibration errors compared to out of box calibra-\ntion (OOB) across all the tasks and MMLMs. Out\nof the two, temperature scaling often results in big-\nger drops in ECE compared to label smoothing\nwith an exception of XCOPA dataset where label\nsmoothing performs much better. In majority of\nthe cases the errors can be reduced even further by\nconsidering the combination of the two techniques\ni.e. TS + LS. Temperature scaling by design does\nnot affect the accuracy of uncalibrated models. In\nall our experiments, we observe that models trained\nwith label smoothing also obtain accuracies very\nclose to their uncalibrated counterparts. Refer to\nAppendix Table 8 for the exact accuracy numbers.\nNext, we investigate if it is possible to reduce\nthe calibration errors on a language even further\nif we are allowed to collect a few-samples in that\nlanguage. We observe that when combined with\nlabel smoothing, both using few-shot data to do\ntemperature scaling (Self-TS + LS) or fine-tuning\n(FSL + LS), often results in significant drops over\nthe errors corresponding to the best performing\n2Refer to Table 7 in Appendix for results on MARC and\nPAWS-X\n4313\nzero-shot calibration method. We do not use more\nthan 2500 examples in the target language in any\nof the experiments, and the number of examples\ncan be as low as 100 for XCOPA. For XNLI and\nXCOPA datasets, we observe that Self-TS + LS\nperforms better than FSL+LS for XLMR and the\nreverse is true for mBERT. One advantage of using\nFSL over TS is that it can often result in increase in\naccuracy in addition to the reducing the calibration\nerrors. However, we do observe an exception to\nthis phenomenon for XCOPA where fine-tuning on\nthe 100 validation examples hurts the overall test\naccuracy of the models. Hence, our general recom-\nmendation is to use FSL + LS for calibrating the\nmodels if the amount of data that can be collected is\nnot too low, otherwise it might be more appropriate\nto use Self-TS + LS as learning just one parameter\n(T) should be less prone to overfitting compared to\nthe weights of the entire network.\n4 Conclusion\nIn this work we showed that MMLMs like mBERT\nand XLMR are miscalibrated in a zero-shot cross\nlingual setting, with the calibration errors being\neven worse on low resource languages and lan-\nguages that are typologically distant from the pivot\nlanguage (often English). We then demonstrated\nthe effectiveness of standard calibration techniques\nfor improving calibration across languages both\nwith and without collecting any new language-\nspecific labelled data. We recommend that re-\nsearchers and practitioners consider, measure and\nreport the calibration of multilingual models while\nusing them for scientific studies and building sys-\ntems. In future work, we aim to bridge the gap\nbetween zero-shot and few-shot calibration meth-\nods by exploring unsupervised calibration methods\nunder domain shift (Pampari and Ermon, 2020;\nPark et al., 2020) that utilizes unlabelled data in\nnew domains to improve calibration. Investigating\nthe cross lingual calibration of MMLMs for tasks\nother than sentence classification like Sequence La-\nbelling (Pan et al., 2017; Nivre et al., 2018) and\nQuestion Answering (Artetxe et al., 2020) is an-\nother natural extension of our work.\nLimitations\nOur work focused on measuring and improving cal-\nibration of MMLMs across different languages and\ntasks. The languages that we considered in our ex-\nperiments were the ones for which labelled test sets\nwere available in the 4 multilingual benchmarks\nthat we considered. The number of languages in\nthese benchmarks ranged from 6 in case of MARC\nto 15 in XNLI, covering mostly high resource lan-\nguages 3 with Swahili being the lowest resource\nlanguage studied. However, the MMLMs consid-\nered in this work supports around 100 languages\nmany of which are arguably even lower resource\ncompared to Swahili. Hence, how well the methods\ndiscussed in the paper work towards improving the\ncalibration for such languages needs to be explored\nbut is limited by the current state of multilingual\nbenchmarking (Ahuja et al., 2022).\nAdditionally, investigating the state of calibra-\ntion across the languages for the 4 tasks and 2\nMMLMs for different hyper-parameters and ran-\ndom seeds required a reasonably large amount of\nGPU resources (we used NVIDIA V100 and P100\nGPUs). However, the calibration methods that we\ndescribe in the paper can work with little (for tem-\nperature scaling and few-shot learning) to no (for\nlabel smoothing) additional compute over the stan-\ndard model training.\nEthics Statement\nOur work deals with calibration of confidence pre-\ndictions of classifiers trained on top of pre-trained\nmultilingual models. Having well calibrated pre-\ndictions is imperative for building robust NLP sys-\ntems especially when using them for security sen-\nsitive applications like Hate Speech Detection to\nflag social media accounts (Cuthbertson, 2021),\ndecision making in law enforcement and fraud de-\ntection (Metz and Satariano, 2020), where extreme\nactions should only be taken when we are confident\nabout the system’s prediction. However, the pre-\ndicted confidences mean essentially nothing if the\nmodel is miscalibrated, posing major risks in using\nsuch models. Through our work we highlight that\nthe commonly used multilingual models are highly\nmiscalibrated when used in a zero-shot setting for\nlow resource and typologically diverse languages.\nAdditionally, we manage to substantially improve\nthe calibration of these models across languages,\naddressing this linguistic disparity and boosting the\nreliability of such models.\n3class 3 or above according to the hierarchy defined by\nJoshi et al. (2020)\n4314\nAcknowledgements\nWe sincerely thank the the reviewers for their valu-\nable feedback about our work. We would also like\nto thank Abhinav Kumar for brain-storming about\nthe work and his helpful suggestions.\nReferences\nKabir Ahuja, Sandipan Dandapat, Sunayana Sitaram,\nand Monojit Choudhury. 2022. Beyond static models\nand test sets: Benchmarking the potential of pre-\ntrained models across tasks and languages. In Pro-\nceedings of NLP Power! The First Workshop on Ef-\nficient Benchmarking in NLP, pages 64–74, Dublin,\nIreland. Association for Computational Linguistics.\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan\nVuli´c. 2022. Composable sparse fine-tuning for cross-\nlingual transfer. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1778–1796,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of ACL\n2020.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui\nJiang, and Diana Inkpen. 2017. Enhanced LSTM for\nnatural language inference. In Proceedings of the\n55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers) ,\npages 1657–1668, Vancouver, Canada. Association\nfor Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Adina\nWilliams, Samuel Bowman, Holger Schwenk, and\nVeselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nEMNLP 2018, pages 2475–2485.\nAnthony Cuthbertson. 2021. Ai mistakes ‘black and\nwhite’ chess chat for racism. The Independent.\nThomas Davidson, Dana Warmsley, Michael W. Macy,\nand Ingmar Weber. 2017. Automated hate speech\ndetection and the problem of offensive language. In\nICWSM, pages 512–515.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295–302, Online.\nAssociation for Computational Linguistics.\nNeha Deshpande, Nicholas Farris, and Vidhur Kumar.\n2022. Highly generalizable models for multilingual\nhate speech detection.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International Con-\nference on Machine Learning , volume 70 of Pro-\nceedings of Machine Learning Research, pages 1321–\n1330. PMLR.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recogni-\ntion. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 770–778.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282–6293, Online. Association for Computational\nLinguistics.\nPhillip Keung, Yichao Lu, György Szarvas, and Noah A.\nSmith. 2020. The multilingual amazon reviews cor-\npus. CoRR, abs/2010.02573.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR (Poster).\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline. Association for Computational Linguistics.\nY . Lecun, L. Bottou, Y . Bengio, and P. Haffner. 1998.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278–2324.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani, Junx-\nian He, Zhisong Zhang, Xuezhe Ma, Antonios Anas-\ntasopoulos, Patrick Littell, and Graham Neubig. 2019.\nChoosing transfer languages for cross-lingual learn-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4315\n3125–3135, Florence, Italy. Association for Compu-\ntational Linguistics.\nPatrick Littell, David R. Mortensen, Ke Lin, Katherine\nKairis, Carlisle Turner, and Lori Levin. 2017. URIEL\nand lang2vec: Representing languages as typological,\ngeographical, and phylogenetic vectors. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers, pages 8–14, Valencia, Spain.\nAssociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nCade Metz and Adam Satariano. 2020. An algorithm\nthat grants freedom, or takes it away. The New York\nTimes.\nRafael Müller, Simon Kornblith, and Geoffrey E Hin-\nton. 2019. When does label smoothing help? In\nAdvances in Neural Information Processing Systems,\nvolume 32. Curran Associates, Inc.\nJoakim Nivre, Mitchell Abrams, Željko Agi ´c, Lars\nAhrenberg, Lene Antonsen, Maria Jesus Aranzabe,\nGashaw Arutie, Masayuki Asahara, Luma Ateyah,\nMohammed Attia, et al. 2018. Universal dependen-\ncies 2.2.\nAnusri Pampari and Stefano Ermon. 2020. Unsu-\npervised calibration under covariate shift. CoRR,\nabs/2006.16405.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of ACL 2017, pages 1946–1958.\nSangdon Park, Osbert Bastani, James Weimer, and Insup\nLee. 2020. Calibrated prediction with covariate shift\nvia unsupervised domain adaptation. In Proceed-\nings of the Twenty Third International Conference on\nArtificial Intelligence and Statistics, volume 108 of\nProceedings of Machine Learning Research, pages\n3219–3229. PMLR.\nGabriel Pereyra, George Tucker, Jan Chorowski, Łukasz\nKaiser, and Geoffrey Hinton. 2017. Regularizing\nneural networks by penalizing confident output dis-\ntributions.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nEdoardo M. Ponti, Ivan Vuli´c, Ryan Cotterell, Marinela\nParovic, Roi Reichart, and Anna Korhonen. 2021.\nParameter space factorization for zero-shot learning\nacross tasks and languages. Transactions of the Asso-\nciation for Computational Linguistics, 9:410–428.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362–2376, Online. As-\nsociation for Computational Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI spring symposium: logical formal-\nizations of commonsense reasoning, pages 90–95.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nRupak Sarkar and Ashiqur R. KhudaBukhsh. 2021. Are\nchess discussions racist? an adversarial hate speech\ndata set (student abstract). Proceedings of the AAAI\nConference on Artificial Intelligence, 35(18):15881–\n15882.\nAnirudh Srinivasan, Sunayana Sitaram, Tanuja Ganu,\nSandipan Dandapat, Kalika Bali, and Monojit Choud-\nhury. 2021. Predicting the performance of multilin-\ngual nlp models.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? In Proceedings\n4316\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual adversar-\nial dataset for paraphrase identification. In Proceed-\nings of EMNLP 2019, pages 3685–3690.\nYuan Zhang, Jason Baldridge, and Luheng He. 2019.\nPAWS: Paraphrase adversaries from word scrambling.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1298–1308,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\n4317\nA Appendix\nA.1 Measuring Calibration\nReliability Diagrams are an effective way of vi-\nsualizing the calibration of a classifier. To plot\nthese, we first predict the (maximum) confidence\nfor each example in the test set and group the data\npoints into M equally sized bins based on the pre-\ndicted confidence. For each bin, accuracy is com-\nputed (by considering label with maximum confi-\ndence as prediction) and plotted on the y axis with\nconfidence being on the x-axis, as can be seen in\nFigure 1 (blue bars). For a perfectly calibrated clas-\nsifier the confidence of a bin should match with the\nclassifier’s accuracy on that bin i.e. the accuracy\nvs confidence curve should lie on the line y = x\n(red-dotted line in Figure 1). The gap between the\ntwo is plotted (as red bars in Figure 1) to represent\nthe calibration error of the classifier.\nExpected Calibration Error (ECE)is defined\nas expected value of the difference between the con-\nfidence and accuracy of the classifier’s predictions.\nIn practice ECE of a classifier is computed using a\nbinning strategy as described in Guo et al. (2017)\nwhere the confidence predictions (corresponding to\nthe maximum confidence class) on the ntest exam-\nples are grouped into M uniform-sized bins, such\nthat set of examples belonging to the mth bin are\ndenoted by Bm. Accuracy (acc(Bm)) and average\nconfidence (conf(Bm)) for each bin is computed\nand a weighted average of the differences between\nthe two is taken to obtain ECE.\nECE =\nM∑\nm=1\n|Bm|\nn |acc(Bm) −conf(Bm)|\nA.2 Datasets Description\n1. XNLI4 (Conneau et al., 2018) is a Natural Lan-\ngauge Inference task where a premise and hypothe-\nsis are given and the task is to predict if the hypoth-\nesis is entailed in premise, contradicts the premise,\nor is neutral towards it, hence being a three way\nclassification problem. MultiNLI (Williams et al.,\n2018) corpus which is available in English is used\nas training set, and dev and validation sets are\nobtained by manually translating crowd sourced\nEnglish sentences for the task into 14 other lan-\nguages (Arabic, Bulgarian, German, Greek, Span-\nish, French, Hindi, Russian, Swahili, Thai, Turkish,\nUrdu, Vietnamese and Chinese.)\n4https://github.com/facebookresearch/XNLI\n2. XCOPA5 (Ponti et al., 2020) is a multilingual\nbenchmark for causal commonsense reasoning. It\nwas obtained by extending the dev and test sets of\nChoice of Plausible Alternatives (COPA)6 (Roem-\nmele et al., 2011) dataset to 11 typologically di-\nverse languages. The task here is, given a premise\nand two alternatives, predict which alternative has\na causal relationship with the premise. The origi-\nnal COPA dataset has only 400 training examples\nin English, hence it is common to first train the\nmodel on Social-IQA (SIQA) 7 (Sap et al., 2019)\ndataset (which has around 33k training examples),\nand then fine-tune it on COPA, and that’s the strat-\negy we adopt in our experiments. SIQA is similar\nto COPA but the questions or premise are defined\nin such a way that the possible answers have social\nimplications, and instead of two alternatives it has\nthree. Out of the 11 supported languages we ex-\nperiment with 9 languages, ignoring Quechua and\nHaitian Creole as both mBERT and XLM-R were\nnot pre-trained on these languages, leaving us with\nGreek, Indonesian, Italian, Swahili, Tamil, Thai,\nTurkish, Vietnamese and Chinese.\n3. MARC8 (Keung et al., 2020) is the multilingual\nAmazon product reviews corpus. We are given\ntitle, body and category of the review and the task\nis to predict the corresponding rating from 1 to\n5. The corpus contains train, dev and test sets\nin six high resource languages: English, German,\nSpanish, French, Japanese and Chinese. In our\nexperiments we only fine-tune using title and body\ntext and ignore the category information.\n4. PAWS-X9 (Yang et al., 2019) is an adversar-\nial dataset for multilingual paraphrase detection.\nIt was adapted from PAWS dataset (Zhang et al.,\n2019) by manually translating the dev and test\nsets in English to six high resource languages\n(French, German, Spanish, Chinese, Japanese and\nKorean). The task is, given a pair of sentences pre-\ndict whether the two are paraphrases of each other.\nFor training the original English PAWS dataset is\nused.\nThe statistics of all these datasets are provided\nin Table 5\n5https://huggingface.co/datasets/xcopa\n6https://huggingface.co/datasets/super_glue\n7https://huggingface.co/datasets/social_i_qa\n8https://registry.opendata.aws/\namazon-reviews-ml/\n9https://huggingface.co/datasets/paws-x\n4318\nA.3 Detailed Experimental Setup\nFor fine-tuning the model we do a grid search over\nthe learning rate( [1e-5,3e-5,5e-6,7e-6]) and the\nnumber of epochs ([1,3,4,5]), run each setting for\n3 random seeds (1, 11 and 22) and select the best\nhyper-parameter set corresponding to the average\ndev accuracy on English data 10. The final set of\nhyperparameters used for each dataset and MMLM\nare provided in Table 4. Apart from these we use\na batch size of 8 in all experiments. We use Adam\noptimizer (Kingma and Ba, 2015) to train all of\nour models and LBFGS to learn the temperature\nparameter while performing temperature scaling.\nFor computing ECE values for different tasks\nand languages we use M = 10i.e. 10 buckets. For\nlabel smoothing we set the smoothing parameter\nα= 0.1 in all experiments and initialize the tem-\nperature T = 1.5 while doing temperature scaling.\nFor few-shot cases we use min(2500,|Dval(t,l)|)\nexamples, where |Dval(t,l)|denotes the number\nof dev examples available in task t for language l,\nand use that to perform continued fine-tuning or\ntemperature scaling.\nAll the experiments were run on NVIDIA V100\nand P100 GPUs with 32GB and 16GB memory re-\nspectively. We use pre-trained models available in\nHugging Face’s Transformers library (Wolf et al.,\n2020). For computing the calibration errors as well\nas plotting the reliability diagrams we use the open\nsource tool Calibration Framework11. To encour-\nage the research in this area we will make our code\npublic.\n10For XCOPA we did use dev data in all languages for\nmodel selection as we saw performance in English data to\nnot necessarily correlate well with performance on other lan-\nguages.\n11https://github.com/fabiankueppers/\ncalibration-framework\n4319\nDataset MMLM Learning Rate Epochs Few-Shot\nLearning\nRate\nFew-Shot\nEpochs\nXNLI XLM-R 7e-6 3 5e-06 2\nmBERT 3e-5 3 3e-5 1\nXCOPA XLM-R 5e-6 (for SIQA) &\n5e-6 (for COPA)\n4 (for S-IQA) &\n10 (for COPA)\n1e-5 1\nmBERT 1e-5 (for S-IQA) &\n3e-5 (for COPA)\n3 (for SIQA) &\n10 (for COPA)\n3e-5 10\nMARC XLM-R 5e-6 3 5e-6 1\nmBERT 5e-6 3 5e-6 1\nPAWS-X XLM-R 7e-6 3 3e-5 1\nmBERT 1e-5 3 3e-5 1\nTable 4: Final list of hyperparameters used for reporting results.\nDataset Number of\nLanguages\nNumber\nof Labels\nTraining Size Dev Size Test Size\nXNLI (sub-sampled) 15 3 40000 2500 2500\nXCOPA 9 2 33410 (S-IQA)\n+ 500 (COPA)\n100 400\nMARC (sub-sampled) 6 5 40000 2500 5000\nPAWS-X 7 2 50000 2000 2000\nTable 5: Dataset statistics for the 4 multilingual classification tasks that we study in our experiments. We use\nsub-sampled versions of XNLI and MARC and only use the first 40k examples in their training sets to reduce the\ncompute overhead and making the scale of training data consistent across all the tasks. For completeness we also\nrun some preliminary experiments with using the entire data for fine-tuning XNLI and present the calibration errors\nin Figure 2.\nar bg de el en es fr hi ru sw th tr ur vi zh\nlang\n0.0\n0.1\n0.2\n0.3\n0.4\nECE\nMMLM = mBERT\nar bg de el en es fr hi ru sw th tr ur vi zh\nlang\nMMLM = XLM-R\ntrain size\n40000\n392702\nFigure 2: Out-of-Box Calibration Errors (ECE) for XLMR and mBERT trained on XNLI with 40k samples\n(sub-sampled) and 392k samples(full data). Even though the calibration is better with using the entire data for\ntraining, the observed patterns about the models being mis-calibrated on languages other than English, especially\nlow resource languages like Swahili, Thai and Urdu still hold true.\n4320\nDataset SIZE SYN SWO\nXNLI -0.4 -0.38 -0.41\nXNLI (wo th) -0.86 -0.88 -0.78\nXCOPA -0.14 -0.22 0.07\nMARC -0.5 -0.69 -0.86\nPAWS-X -0.91 -0.91 -0.98\nTable 6: Pearson correlation coefficient between the\nExpected Calibration Error (ECE) and SIZE, SYN, and\nSWO features of different languages in the test set for\nmBERT\n0 500 1000 1500 2000 2500\nFew-shot Size\n0.04\n0.05\n0.06\n0.07\nECE\nXLMR\nSelf-TS\nSelf-TS + LS\n(a)\n0 500 1000 1500 2000 2500\nFew-shot Size\n0.04\n0.06\n0.08\n0.10\nECE\nmBERT\nSelf-TS\nSelf-TS + LS\n(b)\nFigure 3: Variation in ECE as we use more and more\ndata for calibrating using Self-TS method across lan-\nguages for XNLI dataset. As can be seen 500 samples\nare sufficient to obtain low calibration errors.\n9.5 10.0 10.5 11.0 11.5\nSIZE\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nECE\nar\nbg\nde\nel\nenes\nfr\nhi\nru\nsw\nth\ntr\nur\nvizh\nσ = −0.8\n(a)\n0.5 0.6 0.7 0.8 0.9 1.0\nSYN\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200\nECE\nar\nbg de\nel\nen\nesfr\nhi\nru\nsw\nth\ntr\nur\nvi zh\nσ = −0.88\n(b)\n0.2 0.4 0.6 0.8 1.0\nSWO\n0.05\n0.10\n0.15\nECE\nar\nbg deel\nenes\nfr\nhi\nru\nsw\nth\ntr\nur\nvizh\nσ = −0.85\n(c)\nFigure 4: Visualizing the correlations of ECE with SIZE,\nSYN and SWO for XLMR fine-tuned on XNLI.\n4321\nDataset MMLM Zero-Shot Calibration Few-Shot Calibration\nOOB TS LS TS + LS Self-TS Self-TS + LS FSL FSL + LS\nMARC XLM-R 9.65 4.22† 7.93 4.45 3.55 3.36 2.51‡ 2.58\nmBERT 11.11 6.14 5.96 4.46† 4.62 4.71 2.12‡ 3.01\nPAWS-X XLM-R 4.28 2.29† 3.37 4.44 - - - -\nmBERT 10.33 5.64 5.58 5.36† - - - -\nTable 7: Calibration Errors for MARC and PAWS-X tasks for XLM-R and mBERT on using different methods\nfor calibration. For PAWS-X we only report numbers for Zero-Shot methods as the dev data and test data of the\nbenchmark have sentences in common (even though the pairs are unique), hence we avoid using dev examples as\nfew-shot in this case.\nDataset MMLM Zero-Shot Calibration Few-Shot Calibration\nOOB TS LS TS + LS Self-TS Self-TS + LS FSL FSL + LS\nXNLI XLM-R 74.9 74.9 74.6 74.6 74.9 74.6 79.4 79.3\nmBERT 56.7 56.7 57.5 57.5 56.7 57.5 60.6 61.2\nXCOPA XLM-R 74.3 74.3 75.1 75.1 74.3 75.1 67.2 69.5\nmBERT 54.5 54.5 54.4 54.4 54.5 54.4 53.0 52.9\nMARC XLM-R 57.7 57.7 57.6 57.6 57.7 57.6 59.4 59\nmBERT 42.9 42.9 42.6 42.6 42.9 42.6 49.6 49.2\nPAWS-X XLM-R 76.1 76.1 75.6 75.6 - - - -\nmBERT 80.4 80.4 81.1 81.1 - - - -\nTable 8: Accuracy (El∈L′ [Accuracy(l)]) for XLM-R and mBERT on using different methods for calibration. Similar\nto Table 7, here again we skip few-shot calibration for PAWS-X due to the possible data leakage.\n4322\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 7.32 13.34 19.07\nTS 2.02 6.74 11.81\nLS 3.2 6.93 12.1\nTS + LS 4.1 4.9 9.35\nFew-Shot Calibration\nSelf-TS 2.02 5.41 9.7\nSelf-TS + LS 4.1 4.05 4.64\nFSL 7.32 7.67 9.23\nFSL + LS 3.2 4.37 5.73\n(a) Detailed results on XNLI with XLMR\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 5.44 12.34 45.15\nTS 2.51 6.29 37.25\nLS 4.51 10.42 38.36\nTS + LS 2.61 6.71 30.51\nFew-Shot Calibration\nSelf-TS 2.51 4.77 29.82\nSelf-TS + LS 2.61 4.7 23.23\nFSL 5.44 3.14 4.28\nFSL + LS 2.61 2.55 4.26\n(b) Detailed results on XNLI with mBERT\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 14.54 20.01 29.33\nTS 12.31 15.95 24.04\nLS 9.66 5.47 9.42\nTS + LS 8.98 4.52 7.2\nFew-Shot Calibration\nSelf-TS 12.31 16.02 24.02\nSelf-TS + LS 8.98 4.06 5.36\nFSL 14.54 8.93 14.92\nFSL + LS 9.66 4.4 5.74\n(c) Detailed results on XCOPA with XLMR\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 23.4 23.51 29.02\nTS 21.76 20.02 23.99\nLS 15.85 12.41 15.87\nTS + LS 11.28 6.77 8.93\nFew-Shot Calibration\nSelf-TS 21.76 20.1 24.12\nSelf-TS + LS 11.28 6.89 9.01\nFSL 23.4 3.75 10.5\nFSL + LS 15.85 3.54 6.65\n(d) Detailed results on XCOPA with mBERT\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 7.15 9.65 13.45\nTS 2.75 4.22 5.8\nLS 5.36 7.93 10.66\nTS + LS 3.39 4.55 6.33\nFew-Shot Calibration\nSelf-TS 2.75 3.56 4.1\nSelf-TS + LS 3.39 3.36 3.64\nFSL 7.15 2.51 3.51\nFSL + LS 5.36 2.59 3.28\n(e) Detailed results on MARC with XLMR\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 9.38 11.1 17.3\nTS 3.56 6.14 10.9\nLS 5.19 5.96 12.9\nTS + LS 3.70 4.47 10.1\nFew-Shot Calibration\nSelf-TS 3.56 4.62 8.34\nSelf-TS + LS 3.70 4.71 7.50\nFSL 9.38 2.12 3.45\nFSL + LS 5.19 3.02 4.24\n(f) Detailed results on MARC with mBERT\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 1.93 4.28 5.88\nTS 0.81 2.29 3.39\nLS 4.83 3.37 3.85\nTS + LS 6.30 4.44 5.07\n(g) Detailed results on PAWS-X with XLMR\nMethod ECE(en) E\nl∈L′\n[ECE(l)] max\nl∈L′\nECE(l)\nZero-Shot Calibration\nOOB 3.57 10.3 15.6\nTS 0.99 5.64 9.80\nLS 3.35 5.57 8.94\nTS + LS 5.27 5.36 7.40\n(h) Detailed results on PAWS-X with mBERT\nTable 9: Detailed results on improving calibration for the 4 datasets and 2 MMLMs considered in our experiments\n4323",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8213150501251221
    },
    {
      "name": "Calibration",
      "score": 0.6847286224365234
    },
    {
      "name": "Smoothing",
      "score": 0.5817528367042542
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5731857419013977
    },
    {
      "name": "Popularity",
      "score": 0.5415405035018921
    },
    {
      "name": "Language model",
      "score": 0.5060448050498962
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.49999022483825684
    },
    {
      "name": "Scaling",
      "score": 0.4907175302505493
    },
    {
      "name": "Natural language processing",
      "score": 0.4542331397533417
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4481715261936188
    },
    {
      "name": "Machine learning",
      "score": 0.3230150640010834
    },
    {
      "name": "Statistics",
      "score": 0.11611557006835938
    },
    {
      "name": "Computer vision",
      "score": 0.10158136487007141
    },
    {
      "name": "Mathematics",
      "score": 0.08170878887176514
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Social psychology",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210124949",
      "name": "Microsoft Research (India)",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I4210162141",
      "name": "Microsoft (India)",
      "country": "IN"
    }
  ]
}