{
    "title": "Application of Large Language Models in Power System Operation and Control",
    "url": "https://openalex.org/W4405808675",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2103337840",
            "name": "Yiqian Zhang",
            "affiliations": [
                "Xiamen University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2160058123",
        "https://openalex.org/W3033581805",
        "https://openalex.org/W2966715458",
        "https://openalex.org/W4389156660",
        "https://openalex.org/W4311407471",
        "https://openalex.org/W6847505539",
        "https://openalex.org/W4402716477",
        "https://openalex.org/W4323706279",
        "https://openalex.org/W4386076097",
        "https://openalex.org/W4387294588",
        "https://openalex.org/W4323572061",
        "https://openalex.org/W4385645323",
        "https://openalex.org/W4391215636"
    ],
    "abstract": "The introduction of \"carbon peak\" and \"carbon neutrality\" targets and the vigorous advancement of national energy market construction, renewable energy sources like wind and solar power are experiencing rapid development. However, this also brings challenges in accounting for uncertainties in various operational scheduling and optimization control processes of the power system, especially for the theoretical control methods. Fortunately, Large language model (LLM) with rapid development has shown promising prospects in the power sector. This review summarizes the application of LLM technology in power system operation and control, outlining the new power system's demand for AI technology, the impact of LLM on system management, and the technological foundation including network architecture, training methods, and data configuration. Finally, it explores the applications of LLM in power system operation and control from the perspectives of generation, transmission, distribution, consumption, and equipment.",
    "full_text": "Journal of Computing and Electronic Information Management \nISSN: 2413-1660 | V ol. 15, No. 3, 2024 \n \n79 \nApplication of Large Language Models in Power System \nOperation and Control \nYiqian Zhang* \nElectronic Information Engineering, International Education College, Xiamen University of Technology, Xiamen, Fujian Province, 361000, \nChina \n* Corresponding author Email: Kricese@163.com \n \nAbstract: The introduction of \"carbon peak\" and \"carbon neutrality\" targets and the vigorous advancement of national energy \nmarket construction, renewable energy sources like wind and solar power are experiencing r apid development. However, this \nalso brings challenges in accounting for uncertainties in various operational scheduling and optimization control processes o f \nthe power system, especially for the theoretical control methods. Fortunately, Large language mod el (LLM) with rapid \ndevelopment has shown promising prospects in the power sector. This review summarizes the application of LLM technology in \npower system operation and control, outlining the new power system's demand for AI technology, the impact of LLM on system \nmanagement, and the technological foundation including network architecture, training methods, and data configuration. Finally, \nit explores the applications of LLM in power system operation and control from the perspectives of generation, transmi ssion, \ndistribution, consumption, and equipment. \nKeywords: Large language model; Power system; Artificial intelligence. \n \n1. Introduction \nWith the deepening advancement of decarbonization and \nclean energy in the power industry, increasing the proportion \nof renewable energy sources, primarily wind and solar power, \nis a crucial approach to achieving a cleaner and lower-carbon \nelectricity grid transition. Unlike conventional fossil fuels, \nrenewable energy exhibits significant volatility, intermittency, \nand uncertainty, making it difficult to accurately predict its \noutput curves. To accommodate a high proportion of \nrenewable energy, the power system needs to addres s \nuncertainties in various operational scheduling and \noptimization control processes, which poses significant \nchallenges to the grid's security, stability, and low -carbon \noperation. Balancing safety and economic considerations \nwhile scheduling units to ensure adequate reserve capacity is \na key issue for increasing renewable energy integration, \npromoting low-carbon development in the power sector, and \nreducing grid operation costs. \nTo optimize the power source structure, the power system \nhas begun focusing o n developing uncertainty optimization \nscheduling methods that coordinate generation, grid, load, \nand storage. These methods use distributed resources like \nenergy storage and demand response to smooth out the grid's \nuncertainty and volatility [ 1][2]. The mul ti-dimensional \nnature of distributed resources adds complexity to existing \ncentralized optimization problems and increases the difficulty \nof traditional mathematical optimization methods. Compared \nto traditional centralized approaches, distributed optimization \nmethods enhance the efficiency of solving grid operation and \ncontrol optimization problems and facilitate the coordinated \noptimization of various distributed resources. However, \ndistributed optimization methods still rely on accurate \ngeneration and load forecasting and mathematical modelling, \nrequiring the decoupling of multiple optimization \nsubproblems and iterative calculations for problem -solving. \nTraditional mathematical modelling approaches face \nlimitations in modelling accuracy and computational \nprecision, making it challenging to achieve the desired \nscheduling outcomes when dealing with grid nonlinearity, \nstrong randomness, and uncertainty. \nUnlike traditional model -based methods, artificial \nintelligence technologies that rely on deep learning and \nreinforcement learning algorithms offer a 'model -free' \ntechnical advantage in optimization and control problems. \nThese technologies exhibit strong p attern recognition and \nrapid decision -making capabilities when assisting grid \noperators with operational analysis and control decisions. \nAdditionally, with the rapid growth of data and advancements \nin computational resources, Large language models \nrepresented by Chat -GPT have made significant \nbreakthroughs. They have demonstrated tremendous \napplication value across various domains, scenarios, and tasks. \nThe vertical deepening application of these models in the \nenergy and power sectors is showing both urgent demand and \nbroad prospects. LLM technology refers to the use of massive \nneural network models with numerous parameters to address \ncomplex natural language processing and cognitive tasks. \nRepresentative examples of these large models include the \nGPT (Gener ative Pre -trained Transformer) series, such as \nGPT-3 and GPT -4, and the BERT (Bidirectional Encoder \nRepresentations from Transformers) model. These models, \nthrough pre -training on extensive datasets, can acquire rich \nsemantic and linguistic knowledge, enabling capabilities such \nas text generation, question answering, and semantic \nunderstanding. The rise of these large models has led to \nbreakthroughs in language processing, intelligent dialogue, \nand knowledge reasoning [3][4]. \nAccording to the above, LLMs ar e effective tools for \nhandling natural language and multimodal big data, and their \ndeepening application in the energy and power sectors also \nreveals broad development prospects. In the field of power \nsystem operation and control, the current paradigm of human-\nmachine hybrid intelligence involves using human cognitive \nintelligence to handle all tasks and business at various levels \nof semantic knowledge. On this basis, clearly defined \n \n80 \nnumerical problems, including simulation, computation, \noptimization, and d ecision-making, are addressed using \nsimulators, optimizers, and AI algorithms. However, this \nparadigm will be disrupted by AI large models. Artificial \nintelligence will partially replace abstract and semantic -level \nissues that previously relied solely on h uman cognitive \nintelligence. Consequently, the intelligence of grid operation \nand control will advance significantly, leading to substantial \ntransformation. \nThis paper will first introduce what is large language mode \nand how does it be used in vertical in dustry technologies. It \nwill then provide a comprehensive overview and outlook on \nthe development of new theoretical methods and engineering \napplications of LLMs in power system. \n2. Concept of LLM \n2.1. The architecture of LLM \nTransformer-based large language models have made \nsignificant advances in the field of natural language \nprocessing [ 5]. Through unsupervised pre -training and \nautoregressive generation, these models can learn rich \nsemantic information from vast corpora, achi eving \nimpressive performance across various NLP downstream \ntasks. However, real -world tasks often involve complex \ninformation from multiple modalities, such as text, vision, \nand audio. Current large language models are limited to \nunderstanding textual info rmation, which greatly restricts \ntheir task handling capabilities and variety. Therefore, \nmultimodal large models that support unified knowledge \nrepresentation of text, images, and audio represent a major \ndirection for future advancements in artificial intelligence. \nPrior to the explosion of LLM technology, the technical \napproaches for multimodal large models primarily focused on \ndeveloping general foundational architectures capable of \nhandling multiple modalities of data and their downstream \ntasks, as il lustrated in Figure 1. For instance, in the case of \nimage and text modalities, early multimodal models \npredominantly employed architectures combining text \nencoders with pre -trained visual object detectors [ 7]. \nHowever, since the pre -trained object detectors  no longer \nparticipated in end -to-end training, it became challenging to \nalign the distribution of text and image information in the \nlatent space. The Transformer architecture demonstrate \nsubstantial representational and modelling capabilities in the \nimage domain. Consequently, subsequent multimodal models \nshifted towards unified architectures based on Transformer \nencoding. These models have evolved from being capable of \nonly text -image understanding and discriminative tasks to \naccommodating generative tasks as well. \n \nFigure 1. General architecture for multimodal models \nFollowing the explosion of LLMs, the parameter scale of \nlarge models has surpassed the hundred-billion mark, making \nend-to-end training of multimodal models that integrate tens-\nof-billion parameter pre -trained LLMs extremely costly. \nConsequently, multimodal large model architectures like \nBLIP-2, which use frozen parameters of LLMs as the \nbackbone, have been proposed [8]. As shown in Figure 2, the \nBLIP-2 architecture freezes the parameters o f pre -trained \nimage encoders and LLMs, and introduces a lightweight \nQuery Transformer (Q -Former). This approach uses a two -\nstage training process —representation learning and \ngeneration learning —to bridge the modality gap between \nvisual features and LLM text features. It achieves significant \nimprovements in downstream multimodal task accuracy with \na small number of parameters and has become a key reference \narchitecture for subsequent multimodal large models. \n \nFigure 2. BLIP-2 multimodal large model architecture \n2.2. Loss function of LLM  \nUsing image and text modalities as an example, the \ndownstream tasks of multimodal models primarily include \nimage captioning, visual question answering, image \ngeneration, and cross -modal retrieval. For these tasks, the \nmainstream loss functions used during the training phase of \nmultimodal large models generally fall into the following four \ncategories [9]. \nImage-text contrastive (ITC) loss: ITC is based on the idea \nof contrastive learning. It learns shared representations of \nimages and texts by maximizing the similarity of positive \npairs (images and texts with the same content) and \nminimizing the similarity of negative pairs (images and texts \nwith different content). The goal of the ITC loss function is to \nmap similar images and texts to close regions in the \nrepresentation space, thereby achieving cross-modal semantic \nalignment and relevance modelling \nImage-text matching (ITM) loss: ITM loss aims to measure \nthe semantic matching between images and texts. It constructs \na binary classification task to maximize the  matching \nrelationship between image -text pairs. Compared to ITC, \nITM loss places greater emphasis on the alignment between \nspecific image-text pairs. \nMasked language modelling (MLM) loss: MLM involves \nrandomly selecting and masking certain words in the in put \nsequence, and then requiring the model to predict the masked \nwords based on the context. By forcing the model to learn \nsemantic and syntactic information from the context, MLM \n\n \n81 \nhelps the model develop more comprehensive language \nrepresentations. This se lf-supervised learning approach \nenhances the model's understanding and generation \ncapabilities, providing valuable pre -trained features for \nvarious downstream NLP tasks. \nLanguage modelling (LM) loss : The goal of LM is to \npredict the probability distributio n of the next word or \ncharacter given the context. By maximizing the probability of \ncorrectly predicting the next word, LM enables the model to \nlearn the probability distribution and structure of language, \nthereby enhancing its understanding and generation  \ncapabilities. This provides the model with fundamental \nabilities for modelling and generating language sequences \n3. Technical path for adapting LLM to \nvertical industries \nThe technical pathways for adapting AI large models to \nvertical industries include data collection and preprocessing, \nindustry-specific knowledge construction, model selection \nand fine -tuning (industry -specific model development), \nintelligent task decompos ition and execution for complex \ntasks, and integration and deployment —such as AutoGPT. \nThese technical pathways and methods provide guidance and \nreference for the application of AI large models in vertical \nindustries [10]. \nData Collection and Preprocessing : For specific vertical \nindustries, collect large -scale relevant data based on general \nAI large models, including text, images, audio, etc. Data \nquality and suitability are crucial for model performance, \nnecessitating preprocessing steps such as data cleani ng, \ndenoising, and annotation. \nVertical Industry Knowledge Construction : Vertical \nindustries often have unique domain knowledge, including \nspecialized terminology, rules, and constraints. Integrating \ndomain knowledge into AI large models can enhance their \neffectiveness in the industry. Common methods include \nmanual data annotation, knowledge graph construction, and \nexpert knowledge extraction. \nModel Selection and Fine-Tuning: Start with a pre -trained \nmodel and fine-tune it using industry-specific data to adapt it \nto particular tasks and scenarios. Fine -tuning involves \nadjusting hyperparameters, training set sampling strategies, \nand optimization algorithms. This process benefits from \nreinforcement learning based on human feedback and \nmultimodal knowledge graph techniques, leveraging domain \nexperts' knowledge to ensure the model meets industry needs. \nIntelligent Task Decomposition and Execution : Vertical \nindustries have specific tasks and requirements that may \nrequire customizing AI large models. For example, adding \nspecific category labels for text classification tasks or \nadditional convolutional layers or classifiers for image \nprocessing tasks. This customization helps the model better \nmeet the specific requirements of vertical industry tasks. \nIntegration and Deployment - AutoGPT: After training and \ncustomizing the model, integrate and deploy it within the \nvertical industry's systems. This includes developing and \ndebugging interfaces to ensure stable operation in practical \napplications while considering the mo del's performance and \nefficiency to meet industry needs. \n \nFigure 3. Technical pathways for adapting LLM to vertical \nindustries \n4. Applications of LLM in power \nsystem \n4.1. Equipment fault diagnosis \nCurrent power generation equipment fault records are \nmostly logged by on -site personnel, leading to issues with \nirregular data formats, lengthy text, excessive details, and \ndifficulty in quickly obtaining solutions. By integrating the \ngeneral semantic understanding capabilities of multimodal \nlarge models with histo rical fault data and operational \nprocedures, we can develop an industry -specific multimodal \nlarge model for the power generation sector. This model \nwould assist business personnel in quickly diagnosing \nequipment issues and recommending corrective actions, \nthereby reducing unplanned downtime. \nSpecifically, we will first address the issues present in \ncurrent power generation operations, such as data redundancy, \nsignificant data gaps, diverse sources, and complex structures, \nby organizing and constructing a fault diagnosis and decision \nsupport corpus. This involves researching unified information \nrepresentation techniques for multi -source heterogeneous \ndata to achieve standardized management of equipment data. \nSecondly, in the context of adapting large models  with \nbillions of parameters for industry -specific tasks, traditional \nfull-parameter fine -tuning methods present challenges such \nas high training costs and extensive computational time. \nTherefore, we will develop lightweight fine -tuning methods \nbased on the organized multimodal power generation corpus \nto minimize the number of fine -tuning parameters and \ncomputational complexity, thereby enhancing the \nperformance of downstream diagnostic decision tasks. \nFurthermore, to effectively integrate the high -precision \nsimulations of existing mechanistic models with the rapid \ngeneralization and inference capabilities of large models, we \nwill investigate data-mechanism fusion-driven fault diagnosis \nalgorithms that combine mechanistic model information. This \napproach aims to achieve high -precision and rapid diagnosis \nof equipment faults. Finally, to improve the large model ’s \nsemantic understanding and response accuracy for complex \nbusiness problems, we will explore multimodal human -\nmachine interaction interface design met hods for various \nbusiness scenarios. This will facilitate rapid evaluation of the \ncontent generated by large models by business experts. Based \non interaction records, we will investigate reinforcement \nlearning techniques informed by expert feedback to \ncontinuously enhance the model's ability to analyse and \nhandle complex tasks, ensuring the controllability of \n\n \n82 \ndecisions. The specific technical plan is illustrated in Figure \n4. \n \nFigure 4. Application of LLM in fault diagnosis of equipment \n4.2. Transmission network operation control \nThe operation of large power grids is complex, with control \nand regulation involving numerous specialized business areas \nand substantial electrical computations. This results in high \ndemands for the timeliness and effectiveness of strategy \nformulation. In the context of new power systems, the \nchallenges faced by main grid dispatch and control are \nbecoming increasingly severe. Meanwhile, the power grid, \nover long periods of operation, accumulates vast amounts of \nstatus data, procedural experience, and control records. Fully \nleveraging this multi -source heterogeneous data can greatly \nenhance the current control mode, which relies on dispatcher \nexperience. \nAs illustrated in Figure 5, for the dispatch and control of \nthe main grid, a gene ral large model can be trained using \nmulti-source heterogeneous data, such as grid control \nprocedures, contingency plans, historical operation data, and \noffline simulation data. This control model, acting as the \ncontrol brain, can infer specific control ta sks and objectives \nbased on the grid's operational status, combined with \ndispatcher judgments and requirements. It matches with the \ncontrol knowledge base to provide corresponding specific \nstrategy recommendation. By integrating and coordinating \nvarious ty pes of control business knowledge, the control \nmodel clarifies control tasks with its powerful analytical \ncapabilities. It interacts at a high level with dispatchers for \nnew power system control at the task level, while maintaining \ncompatibility with exist ing dispatch system functions, \nthereby improving control efficiency. \n \nFigure 5. Framework of the main network control LLM \n4.3. Distribution network operation control \nCompared to transmission networks, distribution networks \nhave a more flexible structure but lower reliability, making \ntheir control tasks more complex. Traditional control issues in \ndistribution networks mainly rely on methods such as model \npredictive control and optimization theory. With the \naccelerated construction of new power systems and the large-\nscale integration of high proportions of renewable energy into \ndistribution networks, innovations in renewable energy \ngeneration, grid integration, energy storage, and electric \nvehicle technologies have been driven forward. At the same \ntime, the inherent volatility, intermittency, and uncertainty of \nrenewable energy present new challenges to the stability and \npower quality of distribution networks. In the context of high \nproportions of renewable energy integration, new issues have \nemerged in distribution networks, such as reverse power flow, \ninsufficient regulation capacity, and deterioration of power \nquality. \nAs shown in Figure 6, first, a distribution network control \nknowledge base is constructed using prior knowledge such as \ndistribution network con trol procedures, contingency plans, \nand historical incident records, based on technologies like \nknowledge graphs. Distribution networks, being directly user-\nfacing, exhibit typical characteristics of massive, multi -\nsource, and heterogeneous data. If all co ntrol-related \ninformation is built into a knowledge graph, it might lead to \nchallenges such as high construction difficulty, large storage \nrequirements, and limited retrieval speed. Vector databases, \nthrough machine learning algorithms, convert unstructure d \ndata such as text and images into vector embeddings, storing \nand managing related data while enabling fast information \nretrieval based on similarity, thus achieving efficient storage \nand utilization of massive data. Therefore, control data can be \nvectorized using natural language processing and an industry-\nspecific vector database can be constructed. Both knowledge \ngraphs and vector databases are methods for organizing and \nrepresenting information and can be used as input for fine -\ntuning large models. Knowledge graphs offer natural human-\nmachine interaction advantages in decision support through \nrelational data visualization, while vector databases provide a \nuniversal data format for large models to understand business \ncontexts through vector representation, acting as memory and \nstorage modules that drive model training and inference. \nUltimately, on the basis of a general -purpose large model, \nmodel fine -tuning with distribution network control data \nleads to the development of a distribution network control \nlarge model. \n \nFigure 6. Distribution network regulation LLM \n5. Conclusions \nArtificial intelligence large models are disruptive and \ntransformative technologies that have recently emerged and \nwill change the future development and application paradigm \nof AI  technologies. Currently, AI large models are \ncontinuously being improved. First, large models will possess \nmore comprehensive multimodal characteristics. In the \n\n \n83 \nindustrial field, the incorporation of multi -source \nheterogeneous big data will enrich multimo dal features, \nmaking perception and expression pathways more diverse. \nSecond, as large models integrate with intelligent hardware, \nsuch as the Internet of Things (IoT) and robotics, large -scale \ninformation infrastructure will achieve higher levels of \nabstract thinking and semantic processing, advancing \ntraditional information infrastructure to a new stage of \nintelligence. Furthermore, the next step in the development of \nlarge models will include self -understanding of tasks, self -\niteration, and self -improvement functions, gradually \nenhancing and accumulating intelligence beyond basic \ncapabilities. These three aspects will guide the future \ndevelopment of large models and can also serve as important \ndirections for the intelligent development of new power \nsystem operation and scheduling. The emergence of AI large \nmodels offers unprecedented opportunities for the intelligent \ncontrol of power system operations, and it is hoped that this \npaper will contribute valuable insights into AI large model -\nbased theories, methods, and engineering applications for grid \noperation control. \nReferences \n[1] Liu, X., & Xu, W. (2010). Economic load dispatch constrained \nby wind power availability: A here -and-now approach. IEEE \nTransactions on sustainable energy, 1(1), 2-9. \n[2] Alam, M. S., Al -Ismail, F. S., Salem, A., & Abido, M. A. \n(2020). High -level penetration of renewable energy sources \ninto grid utility: Challenges and solutions.  IEEE access,  8, \n190277-190299. \n[3] Lu, J., Batra, D., Parikh, D., & Lee, S. (2019). Vilbert: \nPretraining task-agnostic visiolinguistic representations for \nvision-and-language tasks.  Advances in neural information \nprocessing systems, 32. \n[4] Cao, Y., Li, S., Liu, Y., Yan, Z., Dai, Y., Yu, P. S., & Sun, L. \n(2023). A comprehensive survey of ai-generated content (aigc): \nA history of generative ai from gan to chatgpt.  arXiv preprint \narXiv:2303.04226. \n[5] Driess, D., Xia, F., Sajjadi, M. S., Lynch, C., Chowdhery, A., \nIchter, B., ... & Florence, P. (2023). Palm -e: An embodied \nmultimodal language model. arXiv preprint arXiv:2303.03378. \n[6] Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., ... & \nChen, W. (2024). Mmmu: A massive multi -discipline \nmultimodal understanding and reasoning benchmark for expert \nagi. In Proceedings of the IEEE/CVF Conference on Computer \nVision and Pattern Recognition (pp. 9556-9567). \n[7] Yang, Z., Li, L., Lin, K., Wang, J., Lin, C. C., Liu, Z., & Wang, \nL. (2023). The dawn of lmms: Preliminary explorations with \ngpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1), 1. \n[8] Xue, L., Gao, M., Xing, C., Martí n-Martí n, R., Wu, J., Xiong, \nC., ... & Savarese, S. (2023). Ulip: Learning a unified \nrepresentation of language, images, and point clouds for 3d \nunderstanding. In Proceedings of the IEEE/CVF conference on \ncomputer vision and pattern recognition (pp. 1179-1189). \n[9] Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., ... & Wang, \nL. (2023). Mm -vet: Evaluating large multimodal models for \nintegrated capabilities. arXiv preprint arXiv:2308.02490. \n[10] Shanahan, M. (2024). Talking about large language \nmodels. Communications of the ACM, 67(2), 68-79. \n "
}