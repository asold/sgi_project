{
  "title": "TransRender: a transformer-based boundary rendering segmentation network for stroke lesions",
  "url": "https://openalex.org/W4387577165",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5002156157",
      "name": "Zelin Wu",
      "affiliations": [
        "Taiyuan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100445969",
      "name": "Xueying Zhang",
      "affiliations": [
        "Taiyuan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5056376704",
      "name": "Fenglian Li",
      "affiliations": [
        "Taiyuan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5059361164",
      "name": "Suzhe Wang",
      "affiliations": [
        "Taiyuan University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5100404215",
      "name": "Jiaying Li",
      "affiliations": [
        "Shanxi Medical University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4294549670",
    "https://openalex.org/W6795435739",
    "https://openalex.org/W3164581645",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3091556692",
    "https://openalex.org/W4316019748",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2913876336",
    "https://openalex.org/W4283724240",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W6810673825",
    "https://openalex.org/W4225371204",
    "https://openalex.org/W4281999467",
    "https://openalex.org/W2966434031",
    "https://openalex.org/W4308529562",
    "https://openalex.org/W3035358681",
    "https://openalex.org/W3035485193",
    "https://openalex.org/W3095270930",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W2762420610",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W2800452457",
    "https://openalex.org/W2310992461",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2888358068",
    "https://openalex.org/W2971013993",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6791764970",
    "https://openalex.org/W4372218843",
    "https://openalex.org/W4297756481",
    "https://openalex.org/W4378976919",
    "https://openalex.org/W6766121741",
    "https://openalex.org/W4322627140",
    "https://openalex.org/W2607363228",
    "https://openalex.org/W6791406852",
    "https://openalex.org/W3026947018",
    "https://openalex.org/W4205568574",
    "https://openalex.org/W4378189111",
    "https://openalex.org/W2966967545",
    "https://openalex.org/W6801350836",
    "https://openalex.org/W4307726656",
    "https://openalex.org/W4213099919",
    "https://openalex.org/W3014795415",
    "https://openalex.org/W2191637929",
    "https://openalex.org/W4226085666",
    "https://openalex.org/W4225912841",
    "https://openalex.org/W3198035652",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2561675875"
  ],
  "abstract": "Vision transformer architectures attract widespread interest due to their robust representation capabilities of global features. Transformer-based methods as the encoder achieve superior performance compared to convolutional neural networks and other popular networks in many segmentation tasks for medical images. Due to the complex structure of the brain and the approximate grayscale of healthy tissue and lesions, lesion segmentation suffers from over-smooth boundaries or inaccurate segmentation. Existing methods, including the transformer, utilize stacked convolutional layers as the decoder to uniformly treat each pixel as a grid, which is convenient for feature computation. However, they often neglect the high-frequency features of the boundary and focus excessively on the region features. We propose an effective method for lesion boundary rendering called TransRender, which adaptively selects a series of important points to compute the boundary features in a point-based rendering way. The transformer-based method is selected to capture global information during the encoding stage. Several renders efficiently map the encoded features of different levels to the original spatial resolution by combining global and local features. Furthermore, the point-based function is employed to supervise the render module generating points, so that TransRender can continuously refine the uncertainty region. We conducted substantial experiments on different stroke lesion segmentation datasets to prove the efficiency of TransRender. Several evaluation metrics illustrate that our method can automatically segment the stroke lesion with relatively high accuracy and low calculation complexity.",
  "full_text": "TYPE Original Research\nPUBLISHED /one.tnum/two.tnum October /two.tnum/zero.tnum/two.tnum/three.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nOPEN ACCESS\nEDITED BY\nXiangzhi Bai,\nBeihang University, China\nREVIEWED BY\nCaglar Gurkan,\nEskisehir Technical University, Türkiye\nZhenghua Xu,\nHebei University of Technology, China\n*CORRESPONDENCE\nXueying Zhang\nzhangxy@tyut.edu.cn\nFenglian Li\nlifenglian@tyut.edu.cn\nRECEIVED /one.tnum/seven.tnum July /two.tnum/zero.tnum/two.tnum/three.tnum\nACCEPTED /two.tnum/six.tnum September /two.tnum/zero.tnum/two.tnum/three.tnum\nPUBLISHED /one.tnum/two.tnum October /two.tnum/zero.tnum/two.tnum/three.tnum\nCITATION\nWu Z, Zhang X, Li F, Wang S and Li J (/two.tnum/zero.tnum/two.tnum/three.tnum)\nTransRender: a transformer-based boundary\nrendering segmentation network for stroke\nlesions. Front. Neurosci./one.tnum/seven.tnum:/one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/three.tnum Wu, Zhang, Li, Wang and Li. This is an\nopen-access article distributed under the terms\nof the\nCreative Commons Attribution License\n(CC BY) . The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication in\nthis journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted which\ndoes not comply with these terms.\nTransRender: a transformer-based\nboundary rendering segmentation\nnetwork for stroke lesions\nZelin Wu/one.tnum, Xueying Zhang /one.tnum*, Fenglian Li /one.tnum*, Suzhe Wang /one.tnumand\nJiaying Li /two.tnum\n/one.tnumCollege of Electronic Information and Optical Engineering, Taiyua n University of Technology, Taiyuan,\nChina, /two.tnumThe First Clinical Medical College, Shanxi Medical University , Taiyuan, China\nVision transformer architectures attract widespread interest d ue to their robust\nrepresentation capabilities of global features. Transform er-based methods as\nthe encoder achieve superior performance compared to convolution al neural\nnetworks and other popular networks in many segmentation tasks for medical\nimages. Due to the complex structure of the brain and the approxima te\ngrayscale of healthy tissue and lesions, lesion segmentatio n suﬀers from\nover-smooth boundaries or inaccurate segmentation. Existin g methods, including\nthe transformer, utilize stacked convolutional layers as the d ecoder to uniformly\ntreat each pixel as a grid, which is convenient for feature computa tion. However,\nthey often neglect the high-frequency features of the boundar y and focus\nexcessively on the region features. We propose an eﬀective meth od for lesion\nboundary rendering called TransRender, which adaptively select s a series of\nimportant points to compute the boundary features in a point- based rendering\nway. The transformer-based method is selected to capture global information\nduring the encoding stage. Several renders eﬃciently map the enco ded features\nof diﬀerent levels to the original spatial resolution by comb ining global and local\nfeatures. Furthermore, the point-based function is employe d to supervise the\nrender module generating points, so that TransRender can conti nuously reﬁne\nthe uncertainty region. We conducted substantial experiments o n diﬀerent stroke\nlesion segmentation datasets to prove the eﬃciency of TransRe nder. Several\nevaluation metrics illustrate that our method can automatically segment the stroke\nlesion with relatively high accuracy and low calculation complexit y.\nKEYWORDS\ntransformer, deep learning, stroke, segmentation, boundary\n/one.tnum. Introduction\nReliable segmentation is the cornerstone for identifying disease types and making\ntreatment strategies, and it plays an indispensable role in assisted therapy and intelligent\nhealthcare (\nTajbakhsh et al., 2020 ). Deep learning-based methods attract enormous\nresearch interest in various segmentation tasks, such as stroke lesion segmentation\n(\nGBD 2016 lifetime risk of stroke collaborators, 2018 ; Wu Z. et al., 2023 ), skin\nlesion segmentation ( Yuan et al., 2017 ; Khattar and Kaur, 2022 ), and brain tumor\nsegmentation ( Pereira et al., 2016 ; Huang P. et al., 2022 ). Ischemic stroke is a\nseries of sudden neurological deﬁcits caused by localized cerebral ischemia and\npermanent infarction, and it has become a major cause of injury and even death\n(\nMatsuo et al., 2017 ). For the detection and treatment of stroke, magnetic resonance\nimaging (MRI) has become an indispensable method with the advantage of high\nresolution. Deep learning-based techniques produce rapid and accurate lesion segmentation\nFrontiers in Neuroscience /zero.tnum/one.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nthat assists physicians in making timely medical decisions ( Nielsen\net al., 2018 ). In the last decade, convolutional neural networks\n(CNN) have grown popular for researchers in the image processing\nﬁeld due to their success at extracting feature representations (\nWu\nJ. et al., 2023 ). U-Net ( Ronneberger et al., 2015 ) is a popular\nencoder-decoder symmetric structure that achieves great success\nfor various 2D segmentation tasks. Many of the proposed methods\n(\nMilletari et al., 2016 ; Schlemper et al., 2019 ; Zhou Y. et al.,\n2021) are improved based on U-Net, providing spatial information,\nsemantic information, and more. However, CNN-based methods\nare intractable for establishing long-distance features because of the\nlimitations of their inherent structure.\nIn the last few years, transformer (\nVaswani et al., 2017 ), which\noriginated in the ﬁeld of natural language processing (NLP),\nhas shown great potential in a series of visual tasks. The vision\ntransformer (\nDosovitskiy et al., 2021 ; Wang et al., 2021 ; Chen et al.,\n2022) is applied directly from NLP to image classiﬁcation task\nand outperforms the CNN-based methods. Transformer and its\nderived methods demonstrate impressive achievements in a variety\nof visual tasks. The pure transformer is not appropriate, and the\nstructure of hybrid CNN-transformer methods becomes the model\nof choice in medical image analysis (\nHe et al., 2022 ). TransUNet\n(Chen et al., 2021 ), the ﬁrst hybrid architecture in medical ﬁeld,\nextracts the global features of medical images through transformer\nlayers. For the organs segmentation, TransUNet realizes excellent\nresults that outperform existing CNN-based methods. In contrast\nto the cascade structure, TransFuse (\nZhang et al., 2021a ) utilizes\nboth CNN and transformer in a parallel connection. The above-\nmentioned methods reﬁne the feature representation of the encoder\nfrom diﬀerent perspectives, while for the decoder they employ the\ntraditional convolutional upsampling method. It’s undeniable that\nthe long range modeling capability of transformer is very powerful.\nUnfortunately, stroke lesion segmentation still faces enormous\nchallenges, as shown in\nFigure 1. and the diﬃculty of identifying\nlesion boundaries. We can see that the location of the lesions\nare diﬀerent due to the individual diﬀerences of patients and\ntheir lifestyle habits. The uncertain location of occurrence and the\ncomplex brain structure cause the shape of the lesion is extremely\nirregular. Furthermore, the statistical features of focal tissue are\nnot signiﬁcantly diﬀerent from those of healthy tissue, leading to\nchallenging segmentation of irregular lesion boundaries.\nMost of the existing methods tend to treat all feature\nrepresentations of the lesion region uniformly in a regular grid way,\nignoring the high-frequency information at the boundaries, which\nmakes the segmentation of the lesion boundary more diﬃcult.\nTo ameliorate these diﬃculties, we propose a point-based\nboundary segmentation method, TransRender, which comprises\nthe transformer as the encoder and the render-based module\nas the decoder. The transformer-based encoder constructs global\nfeatures of the input image sequence at several scales. The\nrender-based decoder utilizes a subdivision strategy that adaptively\nselects an uncertain set of points to recompute the original\nsegmentation. Furthermore, the render module leverages both\nCNN and transformer features to recover the resolution of the\nsegmentation results, which enriches the local-global features of\nthe deep semantic information. To illustrate the validity of the\nTransRender, we implement the comparative experiments using\ndiﬀerent stroke lesion datasets. The experimental results from these\ndatasets suggest that TransRender achieves excellent performance\nin the lesion segmentation task.\nTo summarize, our main contributions are as follows:\n1. We construct a boundary-related network structure for stroke\nlesion segmentation, called TransRender, by adopting both the\nmulti-scale transformer to build long-distance dependency and\nrender-based decoder to compute the original recovery images.\n2. We propose a render-based decoder that is trained to predict\nuncertain points, allowing the decoder to ﬁne-tune the\nlesion boundaries.\n3. We design multi-level point-to-point supervision to\noptimize the point selection strategy. The comprehensive\nexperiments are conducted on two MRI-based stroke\nlesion datasets to conﬁrm the superior performance of\nthe TransRender.\n/two.tnum. Related work\nWe will review the relevant literature from\nCNN-based methods, hybrid architecture-based\nmethods, and boundary-related methods in\nthis section.\n/two.tnum./one.tnum. CNN-based methods\nIn several ﬁelds, such as image classiﬁcation and image\nsegmentation, the CNN methods have gained enormous success\n(\nZhao et al., 2021 , 2023; Guo et al., 2022 ). Traditional segmentation\nmethods generally use convolutional and pooling layers to extract\nlocal features and thus perform segmentation (\nLi et al., 2021 ). U-\nNet ( Ronneberger et al., 2015 ) is a popular symmetric structure\nbased on convolution layers. The skip connection serves as a\nbridge to connect diﬀerent semantic information, making U-\nNet suitable for medical image processing tasks. Some studies\non stroke attempt to improve the U-Net method to realize\naccurate lesion segmentation. D-UNet (\nZhou Y. et al., 2021 )\nutilizes the dimensional transformation module to extract the\nspatial information between slices through the combination of\n2D detail features and 3D spatial features. The multi-inputs\nUNet (\nZhang et al., 2021b ) takes 3D diﬀeomorphic registration\nwith the original MRI as inputs, providing rich prior knowledge\nfor the subsequent UNet segmentation network. The CNN-\nbased encoder is limited by convolutional operations and still\nlacks the ability to extract global information.\nYang et al.\n(2019) proposed a network that adopts DenseUNet as the\nencoder and uses a long short-term memory module to fuse\ncontextual information on the decoder. The two-stage U-Net\n(\nAgnes et al., 2022 ) proposes a feature combination module to\neﬃciently extract global information. Unfortunately, these methods\nintroduce global features from diﬀerent perspectives, but do not\nqualitatively eliminate the limitations of the convolutional inherent\nreceptive ﬁelds.\nFrontiers in Neuroscience /zero.tnum/two.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nFIGURE /one.tnum\nStroke lesions are distributed in diﬀerent locations and have ex tremely irregular sizes and shapes. Furthermore, the similar ity of the lesion to the\nsurrounding healthy tissue further increases the diﬃculty of segmentation.\n/two.tnum./two.tnum. Hybrid architecture-based methods\nTransformer has spread from NLP to computer vision since it\nis excellent at attracting long-distance information and encoding\nshape representations (\nHan et al., 2022 ). The vision transformer\n(ViT) ( Dosovitskiy et al., 2021 ) is the ﬁrst structure to be used\nfor image classiﬁcation tasks and obtains results that exceed the\nCNN methods. As the interest grows, ViT and its derived methods\n(\nLiu et al., 2021 ) display powerful performance in a series of\nvisual segmentation tasks. Because of the complex structure and\ntissue intensity similarity of medical images, a pure transformer\nis hard to realize the desired segmentation outcomes. The hybrid\narchitectures of CNN combined with transformer have become the\nmodel of choice in the medical ﬁeld (\nHe et al., 2022 ). TransUNet\n(Chen et al., 2021 ) is the ﬁrst hybrid structure that is utilized\nto segment the abdominal organs. TransUNet extracts deep-level\nfeatures by using stacked convolutional layers and then establishes\nlong-term associations by stacking transformer layers in a cascade\nway. On the contrary, BiFusion module (\nZhang et al., 2021a ) is\nproposed to integrate the parallel convolutional and transformer\nbranches, and the proposed method achieves excellent performance\nwhile being highly eﬃcient. Swin-Unet is proposed by\nCao et al.\n(2023), combining a Swin transformer with a U-shaped structure.\nSwin-Unet can capture local semantic features and build long-\ndistance context information. The nnFormer is proposed by\nZhou\nH. Y. et al. (2021), which optimally combines convolution with\na self-attentive mechanism to surpass previous methods on brain\ntumor segmentation. As for the decoder, both of them employ the\ntraditional convolutional upsampling path or transformer layers,\nwhich tend to degrade the boundary information due to the\nuniform computation of the pixels around the edge (\nKirillov et al.,\n2020).\n/two.tnum./three.tnum. Boundary-related methods\nWe notice recent works in medical image segmentation that\ncan be related to the proposed method. de Vries et al. (2023)\nadopts general architecture as the encoder-decoder, while they\nintroduce the multiple cross-attention module to receive the\ntemporal information.\nZhu et al. (2023) proposed a fusion network\nthat extracts edge features from CNN and edge spatial attention\nblocks, and fuses edge features with semantic features from the\ntransformer. To clarify the structure boundaries, the boundary\npreserving module (\nLee et al., 2020 ) is proposed to generate a\nkey point map and explore the boundaries of the target object.\nKirillov et al. (2020) proposed a unique idea of considering image\nsegmentation as a rendering issue. The rendering-based approach\nis eﬀective and qualitative in the instance segmentation and\nsemantic segmentation tasks. In the boundary-rendering network\n(\nHuang R. et al., 2022 ), a point selection module is proposed to\nconcentrate on the area of unclear edge. Moreover, a boundary\nrendering module is employed to discover the contour information.\nSome other methods (\nChu et al., 2020 ; Kervadec et al., 2021 ) to\ndesign boundary loss functions to mitigate the diﬃculties of highly\nunbalanced problems in medical images. However, the existing\nmethods tend to generate over-smooth or inaccurate predictions\n(\nHuang R. et al., 2022 ). We propose an improved render-based\ndecoder and combine it with a transformer-based encoder, which\ncan accurately segment lesions via ﬁne-level details on a grid and\nglobal semantic information.\n/three.tnum. Methodology\nThe structure of the TransRender is described in Figure 2.\nThe transformer-based encoder, render-based decoder, and fusion\nmodule are the three parts of the proposed network architecture.\nFor each sliced input image, TransRender utilizes a multi-scale\ntransformer as an encoder to establish long-range dependencies\nbetween the patch sequences. Then, the render-based decoder\nrecovers the resolution of the segmentation by upsampling strategy\nwith local-global features. Finally, a fusion module is adopted as the\npostprocessing to integrate the segmentation maps at each level.\nFurthermore, the proposed method trains renders with several\npoint-based supervisions. We introduce the detailed structure of\nthese three parts in this section.\n/three.tnum./one.tnum. The encoder\nFigure 2 gives the structure of the encoder, which mainly\nconsists of several transformer modules and convolutional\nFrontiers in Neuroscience /zero.tnum/three.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nFIGURE /two.tnum\nThe structure of the TransRender, which includes a multi-sc ale transformer-based encoder, render-based decoder, and a fus ion module. Moreover,\nthe orange line, green line, and blue line mean global skip connection , local skip connection, and point-based loss function, respectivel y.\nmodules. To encode the hierarchical context information of the\ninput image, we ﬁrst utilize a hierarchical transformer. With a\nparticular input X ∈ RH×W×C, we denote its spatial resolution by\nH × W and its channel number by C, respectively. The MRI image\nX is initially split into a patch sequence {xi\np ∈ RP2·C|i = 1, .., N} in\nthe linear embedding layer, where the height and width of each\npatch are PH and PW , and N stands for the amount of patches.\nThen we ﬂatten and reﬂect these patches to a D-dimensional feature\nrepresentation via the linear projection:\nz0 = [x1\npE; x2\npE; ...; xN\np E], s.t. z0 ∈ RN×D, E ∈ R(P2·C)×D, (1)\nwhere z0 represents the ﬁnal features, and E is the patch\nembedding projection.\nFinally, a positional embedding Epos ∈ RN×D to be added\nis signiﬁcant for the divided patches to integrate positional\ninformation. The encoded patch sequence will be fed into the\ntransformer layers. As illustrated in\nFigure 3A, the cascaded multi-\nhead self-attention (MSA) layer and the multi-layer perception\n(MLP) layer comprise the transformer, which is computed as:\nt′l = MSA(LN(tl−1)) + tl−1, (2)\ntl = MLP(LN(t′l)) + t′l, (3)\nwhere tl and tl−1 represents the resulting features of the\ncorresponding transformer layers, and LN(·) denotes the layer\nnormalization. The MSA is deﬁned as:\nMSAi(X) = σ 1 (\nQiKiT\n√\nd\n)Vi, s.t. Qi = XWQ\ni , Ki = XWK\ni , Vi = XWV\ni ,\n(4)\nwhere d denotes the feature dimension, and Qi ∈ RN×Dq ,\nKi ∈ RN×Dk , and Vi ∈ RN×Dv are the query, key, and value,\nrespectively. The WQ\ni ,WK\ni , and WV\ni are the weight matrices,\nand σ 1 means the nonlinear function softmax. Moreover, patch\nmerging is employed between the two transformer modules, which\nreduces the spatial resolution of patches and doubles the channel\ndimension simultaneously.\nAs mentioned in the previous section, pure transformer\narchitecture is not optimal for the diﬀerent segmentation tasks.\nWe utilize the convolutional modules additionally to enrich the\nlocal representation. In the initial stage of encoding, the undivided\ninput X ∈ RH×W×C is directly fed into the Conv2D module. The\nstructure of the Conv2D is shown in\nFigure 3B, which is deﬁned\nas follows:\nConv2D(X) = σ 2 (BN(C1\n3(σ 2 (BN(C1\n3(X))) + X))) +\nσ 2 (BN(C1\n3(X))) + X, (5)\nwhere C1\n3(·) denotes a two-dimensional convolution with the 3 × 3\nkernel and the 1 ×1 stride, σ 2 means the PReLU linear function, and\nBN(·) represents the batch normalization. TransRender extracts\nthe local features and long-distance dependency of the image at\nthe encoding stage, which will be used by the decoder to perform\nresolution recovery of the predicted image.\n/three.tnum./two.tnum. The decoder\nDue to the complexity of cerebral structures, the boundaries\nof stroke lesions are diﬃcult to identify. The traditional\nCNN methods treat all pixels of the irregular target object\nuniformly in a convolutional way (\nKirillov et al., 2020 ), either\nFrontiers in Neuroscience /zero.tnum/four.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nFIGURE /three.tnum\nThe structure of three modules: (A) Transformer layer composed of MSA, layer normalization, and MLP; (B) Conv/two.tnumD module consist of two stacked of\nconvolutional layers and activation functions; (C) Fusion module comprises GAP, FC layer, and sigmoid function.\nFIGURE /four.tnum\nThe structure of render module. With the transformer features from the global skip connection and convolutional features from the local s kip\nconnection as input, render adaptively selects key points by poi nt selection strategy and combines point features to make re-pre diction.\nat the lesion boundary or the lesion core. And the proposed\nrender module ﬁrst selects the set of uncertain points and\nextracts the feature representations corresponding to these\npoints, and implements the re-prediction of these uncertain\npoints by using the prediction head. The accurate localization\nof the lesion boundary is accomplished by further prediction\nof the selected uncertainty points. We take several renders\nto build a decoder that adaptively predicts points with high\nuncertainty. The render mainly includes three steps, as shown\nin\nFigure 4: point selection strategy, point re-prediction, and\npoint replacement.\nFirst, we introduce the point selection strategy using the last\nlayer of the proposed method as an example. For the given feature\nmap Xt ∈ R\nH\n2 × W\n2 ×1, the render ﬁrst upsamples it by a 2 ×\ninterpolate function to obtain the initial coarse segmentation ˆXt ∈\nRH×W×1. The values from [0, 1] on the segmentation ˆXt represent\nthe possibility of whether the current pixel is a lesion or not.\nWe give the distribution of pixel values and pixel positions in\nFrontiers in Neuroscience /zero.tnum/five.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nFigure 5. Diﬀerent colors of points represent diﬀerent values, where\nblack, orange, and red represent healthy tissue, lesions, and fuzzy\nboundaries, respectively. If the value of the pixel is closer to 0, it is\nmore likely that the current pixel is a background (healthy tissue),\nand vice versa, if the value is closer to 1, it means a lesion. When the\nsegmentation threshold is set to 0.5, the closer the threshold is, the\nhigher the uncertainty. Although the number of pixel points near\nthe threshold is sparse in\nFigure 5A, they are essential for the clear\nlocalization of the boundary. These values are sorted in descending\norder for each pixel, which is calculated as follows:\n∀1 ≤ h ≤ H, ∀1 ≤ w ≤ W, Ms = { p1\nh,w, p2\nh,w, .., pn\nh,w}, (6)\nwhere pi\nh,w represents the prediction value at the ( h, w) location\nand n = H × W denotes the number of pixels, Ms is the feature\nmap derived after sorting, and the point in the Ms follows the rule\nthat pi\nh,w is greater than pi+1\nh,w . Based on the Ms, we construct the\nuncertain points map Mu. It can be obtained as follows:\nMu = | pi\nh,w − pthd|, (7)\nwhere pthd represents the threshold value of uncertainty. For\nthe ﬁnal uncertainty map Mu, a smaller value at a pixel means that\nthe segmentation network has more uncertainty in the prediction.\nThat means the smaller the diﬀerence between pi\nh,w and pthd, the\nlarger the uncertainty of the current pixel. To eliminate the strong\nbias due to the space position, the proposed render sampling k × N\npoints across the Mu at random. It identiﬁes that the β × N points\nwith the highest uncertainty in the set of points are located around\nthe boundary region, as shown in\nFigure 5B. These points will be\nimportant to correct the segmentation at the lesion boundary.\nThen, the render module integrates the features based on the\nselected points, combining the contextual semantic information\nfrom the global skip connection and local detail information from\nthe local skip connection. The feature sequences corresponding\nto the selected points are fed into the mixer layer for point-\nbased re-prediction, where the mixer layer consists of two trainable\nMLP layers. Finally, the re-predicted points set replaces the high\nuncertainty points set in the initial segmentation to accomplish the\nprecise localization of the lesion boundary. The structure of the\nrender is shown in\nFigure 4.\nBased on the render module proposed above, we construct\nthe render-based decoder (see in Figure 2), which combines local\nand global features at multiple scales. Furthermore, we introduce\na fusion module at the end of the decoder in\nFigure 3C, which\nfuses multiple layers of decoded features. The segmentation of\nthe diﬀerent levels renders is merged as input Xr to perform the\nfollowing operations:\nA(Xr) = σ 3(FC(GAP(Xr))) + Xr, (8)\nwhere GAP(·), FC(·), and σ 3 denotes global average pooling,\nfully connected layer, and sigmoid function, respectively. The\nfusion module emphasizes segmentation-related information and\nsuppresses irrelevant features in an attentive manner.\n/three.tnum./three.tnum. Loss function\nThe multi-scale render decoder adaptively selects the boundary\nkey points, thus improving the segmentation performance. In the\ntraining stage, we design a combined loss function from two\naspects: segmentation loss and point loss, which is calculated as:\nLtotal(p, g) = Ldice(p, g) + λ\nn∑\ni=1\nLi\nbce(p, g), (9)\nwhere Ldice(p, g) indicates segmentation loss and Lbce(p, g) is point\nloss. λ represents the weight parameter, and the default setting\nis λ = 0.7. The segmentation loss supervises the network to\ngenerate regional details in the whole upsampling recovery process,\nand point-to-point losses are employed to monitor each render\nmodule in the decoder. The weight parameters of the MLP layer in\nrender are dynamically updated when the point selection strategy\ncalculates the point loss between the selected points on ground\ntruth and the points after re-prediction. Both two loss functions are\ncalculated as:\nLdice(p, g) = 1 −\n2 ∑ N\ni=1 pigi + δ\n∑ N\ni=1 p2\ni + ∑ N\ni=1 g2\ni + δ\n, (10)\nLbce(p, g) = − (p log(g) + p′ log(g′)), (11)\nwhere p represents the prediction probability, g represents the\nexpert annotation. p′ and g′ represents the contrary prediction\nprobability of p and g, respectively.\n/four.tnum. Experiments and conﬁgurations\n/four.tnum./one.tnum. Datasets\nDiﬀerent stroke lesion segmentation datasets, including 490\nMRI images, are used to conduct experiments to validate the\nproposed method. They include brain MRIs of stroke patients in the\nacute, sub-acute, and post-stroke stages. The details of both datasets\nare introduced as follows.\nThe anatomical tracings of lesions after stroke (ATLAS)\nis a publicly available dataset that includes 240 MRI images.\nEach image contains the MRI for the t1-weighted modality and\nthe corresponding lesion annotation. The ischemic stroke lesion\nsegmentation (ISLES2022) is provided for use at the MICCAI 2022\ngrand challenge, which contains 250 MRI images. In contrast to\nATLAS, ISLES2022 contains three diﬀerent modalities: ADC, DWI,\nand FLAIR. The original size of ATLAS is 233 ×197×189, while the\noriginal size of ISLES2022 varies over a wide range. After we slice\nthese 3D MRIs into 2D images, the slices are resized to a uniform\nresolution of 208 ×176. In\nTable 1, we compare both two datasets in\nterms of imaging method, data source, modality, number of images,\nand dataset division.\n/four.tnum./two.tnum. Conﬁgurations\nThe PyTorch framework and Python are used to carry out\nthe experiments. We adopted AdamW as the optimizer with\ndefault parameter settings. The epoch-based early stop strategy is\nutilized to determine whether the model optimization is complete.\nFurthermore, the transformer layers are pre-trained on the large\nimages dataset. All experiments are performed on GeForce RTX\n2080 super with 8 GB memory.\nFrontiers in Neuroscience /zero.tnum/six.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nFIGURE /five.tnum\nThe distribution of pixel values and pixel locations: (A) The distribution of pixel points in diﬀerent intervals, with [ /zero.tnum./three.tnum, /zero.tnum./seven.tnum] as the high uncertainty\ninterval; (B) The ambiguous pixel points are mainly distributed at the boundar y of the lesion core area.\nTABLE /one.tnumThe data comparison of ATLAS and ISLES/two.tnum/zero.tnum/two.tnum/two.tnum dataset.\nDataset Imaging method Data source Modality Number of images Dataset division\nATLAS MRI Public T1WI 240 160/40/40\nISLES2022 MRI Public DWI, ADC, FLAIR 250 168/41/41\nWe select common metrics to measure the advantages of\nTransRender, including DSC, Precision, Recall, and HD to evaluate\nthe similarity between prediction results and lesion labels. We\nconsider the ﬁrst two metrics, DSC and HD, more signiﬁcant than\nthe classic F2, Precision, and Recall. The DSC calculates the region\nsimilarity, and the HD calculates the boundary similarity between\nthe two inputs.\n/four.tnum./three.tnum. Experiments\n/four.tnum./three.tnum./one.tnum. Comparison experiment\nWe compare our TransRender with previous methods: U-Net\n(\nRonneberger et al., 2015 ), AG U-Net ( Schlemper et al., 2019 ), D-\nUNet ( Zhou Y. et al., 2021 ), CLCI-Net ( Yang et al., 2019 ), SAN-\nNet ( Yu et al., 2023 ), TransUNet ( Chen et al., 2021 ), TransFuse\n(Zhang et al., 2021a ), and MLRA-Net ( Wu et al., 2022 ) using\nATLAS dataset to illustrate eﬃciency of the TransRender. Table 2\nshows the performance comparison, where the experimental result\nof the proposed TransRender is presented in the last line. Further\nexperiments are implemented on the ISLES2022 to validate the\ngeneralizability of the TransRender, as shown in\nTable 3. All\nof the above experiments employ cross-validation methods to\navoid randomness.\n/four.tnum./three.tnum./two.tnum. Ablation experiment\nThe four ablation experiments on decoders are conducted to\nassess the availability of the render module, which are shown\nbelow: (1) The encoder uses U-Net and traditional convolutional\nupsampling path as the decoder; (2) The encoder uses U-Net and\nrender-based upsampling path as the decoder; (3) TransRender\nas the encoder and traditional convolutional upsampling path\nas the decoder; (4) TransRender as the encoder and render-\nbased upsampling path as the decoder.\nTable 4 shows the\ncomparative results.\n/four.tnum./three.tnum./three.tnum. Hyper-parameter comparison\nThe render module automatically selects k × N points as the\nuncertain points set to predict. The value of k directly aﬀects how\nmany points are selected in network learning and, consequently,\nthe segmentation capacity of the proposed TransRender. We set\nk = 1, 2, 3, 5 in the render module to compare the performance\nusing ATLAS dataset, respectively.\nTable 6 shows the results of\nthis experiment. It is also worthwhile to investigate the value of\nβ , which indicates the diﬀerent percentiles of points selected as\nimportant points. These important β ×N points are sampled for the\nfeatures of spatial location, while the other (1 − β ) × N points are\nrandomly assigned features. We conduct a comparative experiment\nto explore the eﬀect on segmentation performance by using β =\n0.1, 0.5, 0.6, 0.7, 0.8 on the ATLAS dataset, respectively. The results\nof this experiment as shown in\nTable 7.\n/five.tnum. Result and discussion\n/five.tnum./one.tnum. Comparison experiment\nTable 2 reports the quantitative results using the ATLAS\ndataset. Comparative experiments with eight diﬀerent existing\nmethods are conducted to analyze the segmentation eﬀectiveness\nof the point-based TransRender. The comparison results indicate\nFrontiers in Neuroscience /zero.tnum/seven.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nTABLE /two.tnumThe quantitative comparison of TransRender with the previous eight methods on the ATLAS dataset.\nMethod DSC (F/one.tnum)(%) HD (px) F/two.tnum(%) Precision (%) Recall (%)\nU-Net (Ronneberger et al., 2015 ) 48.34 51.35 49.50 54.45 53.68\nAG U-Net ( Schlemper et al., 2019 ) 49.60 50.12 53.67 49.25 62.53\nCLCI-Net (Yang et al., 2019 ) 51.74 – 51.28 – 51.39\nMI-Net (Zhang et al., 2021b ) 56.72 38.80 – 60.90 59.38\nSAN-Net (Yu et al., 2023 ) 57.11 – 56.23 – 59.77\nD-UNet (Zhou Y. et al., 2021 ) 53.49 – – 63.31 52.43\nTransUNet (Chen et al., 2021 ) 56.23 45.44 59.64 57.15 65.95\nTransFuse (Zhang et al., 2021a ) 58.18 41.56 62.40 57.64 70.06\nTransRender 59.79 33.98 59.38 63.91 68.08\nThe bold values in the table represent the best results.\nTABLE /three.tnumThe performance comparison of TransRender with the previous ﬁve m ethods on the ISLES/two.tnum/zero.tnum/two.tnum/two.tnum dataset.\nMethod DSC (F/one.tnum) (%)HD (px) F/two.tnum (%)Precision (%) Recall (%)\nU-Net (Ronneberger et al., 2015 ) 82.04 36.82 81.52 85.31 81.44\nAG U-Net ( Schlemper et al., 2019 ) 81.45 37.01 80.99 84.70 80.98\nTransUNet (Chen et al., 2021 ) 84.23 29.98 84.01 86.88 84.19\nTransFuse (Zhang et al., 2021a ) 84.39 29.19 84.06 87.36 84.15\nMLRA-Net (Wu et al., 2022 ) 84.73 29.95 84.48 87.03 84.70\nTransRender 85.37 27.60 84.87 86.48 83.94\nThe bold values in the table represent the best results.\nTABLE /four.tnumThe ablation comparison of TransRender on the ATLAS dataset.\nEncoder Render DSC (F/one.tnum) (%)HD (px)\nCNN U-Net – 48.34 51.35\n✓ 54.13 40.71\nAG U-Net – 49.60 50.12\n✓ 55.21 38.14\nTransformer TransUNet – 56.23 45.44\n✓ 57.86 37.42\nTransRender – 58.27 37.86\n✓ 59.79 33.98\nThe bold values in the table represent the best results.\nTABLE /five.tnumThe complexity comparison of TransRender and U-Net w/o\nRender.\nEncoder Render FLOPs (G) Params ( M)\nU-Net – 30.5 31.0\n✓ 15.6 18.9\nTransRender – 118.4 43.6\n✓ 100.1 32.2\nThe bold values in the table represent the best results.\nthat TransRender exceeds the previous method, performance\ngains range from 1.61%, 7.58 px, and 0.60–11.45%, 17.37 px, and\n9.46% considering the DSC, HD, and PRE, respectively. The\nTABLE /six.tnumSegmentation performance comparison of diﬀerent initial k.\nValue of k DSC (F/one.tnum)(%) HD (px) F/two.tnum(%)\n1 59.07 34.96 58.97\n2 59.27 34.77 59.21\n3 59.79 33.98 59.38\n5 59.26 34.85 59.17\nThe bold values in the table represent the best results.\nTABLE /seven.tnumSegmentation performance comparison of diﬀerent β .\nValue of β DSC (F/one.tnum)(%) HD (px) F/two.tnum(%)\n0.1 55.17 45.33 54.99\n0.5 58.76 38.49 57.48\n0.6 59.38 36.90 59.61\n0.7 59.79 33.98 59.38\n0.8 59.02 34.57 57.24\nThe bold values in the table represent the best results.\nsigniﬁcant improvements demonstrate that applying a render-\nbased decoder to TransRender is better at capturing boundary\nsemantic information than a standard decoder. For the DSC, our\nmethod achieves a mean DSC of 59.79%, which is improved by\n2.77% than the second-best TransFuse. We would also like to\nmention that the diﬀerence in the HD metric is pretty large. Our\nmethod does not obtain the best performance in terms of F2 and\nFrontiers in Neuroscience /zero.tnum/eight.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nRE, only 59.38 and 68.08%, which are the third- and second-\nbest ranks, respectively. However, we recognize that region overlap\n(DSC) and boundary distance (HD) is more important between the\nprediction results and the physicians annotation. Excellent results\nverify that adaptively predicting selected points can improve lesion\nsegmentation at the boundary.\nFurthermore, the qualitative comparisons of the ATLAS\ndataset are displayed in\nFigure 5. As we can see from the\nvisualization results, whether the lesion size is large or small,\nthe lesion location is left or right, our method produces visually\nsuperior segmentation. We visualize four methods, including U-\nNet (\nRonneberger et al., 2015 ), AG U-Net ( Schlemper et al.,\n2019), TransUNet ( Chen et al., 2021 ), and TransFuse ( Zhang\net al., 2021a ) to compare visually with the TransRender. The\nscale, location, and shape of each lesion are diﬀerent in the\nselected ﬁve brain images. In Case 1, the target object consists\nof an infarct lesion and multiple embolic, the latter of which\nsize is extremely small. All methods identify infarct lesions\nwith more or less accuracy, but our TransRender achieves the\nbest regional similarity. And for the multiple embolic, only\nAG U-Net and TransRender locate the lesion, where the latter\nobtains more correct segmentation and less over-segmentation.\nThe lesion size in Case 2 is small, so U-Net and TransFuse\nonly segment a small part of the lesion or even fail to identify\nit. The other two previous methods realize correct segmentation\nalmost completely, but at the cost of severe over-segmentation.\nBeneﬁting from the prediction of the boundary key points by\nthe render module, the proposed method greatly reduces over-\nsegmentation. In Case 3, transformer-based methods display\nsigniﬁcantly improved segmentation performance compared to\nCNN-based methods. However, these methods suﬀer from diﬀerent\ndegrees of under-segmentation. The TransRender yields precise\ndetails of the lesion boundary, with almost no under-segmentation.\nWe regard Case 4 in\nFigure 6 as a diﬃcult segmentation issue\ndue to its close location to the skull. None of the ﬁve methods\ncompletely segments the lesion, whereas TransRender achieves the\ncorrect segmentation of the most pixels. It is necessary to mention\nthat TransRender suﬀers from a slight over-segmentation. The\ncomplex tissue structure in the area of the focal lesion aﬀects\nthe segmentation performance of all methods. So in Case 5, the\nprediction results of each method are coarse and discontinuous.\nTransRender yields fewer over-segmentation than the transformer-\nbased methods, and fewer under-segmentation than the CNN-\nbased methods.\nWe further carry out comparisons to validate the performance\nrobustness of the TransRender. The quantitative comparison is\nreported in\nTable 3 between the TransRender and ﬁve methods\nusing the ISLES2022 dataset. We can observe that the CNN\nmethods are signiﬁcantly worse than that the transformer methods\nin terms of ﬁve metrics. MLRA-Net outperforms fourth-best\nTransUNet and third-best TransFuse by 0.50 and 0.34% on the\nDSC metric, respectively, but it is worse than TransFuse on the\nHD metric. The proposed method uses a multi-scale transformer\nas the encoder with render as the decoder that yields the best\nscores on the DSC, HD, and F2 metrics. It might be interpreted\nthat render successfully corrects the error segmentation at the\nlesion boundary.\nFigure 7 displays the qualitative comparison using the\nISLES2022 dataset. Four brain images are selected for visualization\nand comparison, each of which has diﬀerent modalities, lesion\nshapes, and locations. In Case 1 and Case 3, all methods only\nsegment parts of the lesion to a more or less degree, while\nTransRender realizes the best region overlap and boundary\nsimilarity. The lesions in Case 2 are multiple embolic, and only\nthe proposed method segments the lesions nearly completely. The\nexcellent results on these two datasets validate the segmentation\naccuracy of the TransRender for multiple embolic. The existing\nmethods all identify Case 4 as having multiple lesions, and the\nreason may be that the lesion occurs in the cerebral cortex.\nTransRender identiﬁes Case 4 as a whole lesion and completes\nmore correct segmentation.\nOverall, on these two datasets, the proposed TransRender can\nyield satisfactory segmentation performance, both qualitatively and\nquantitatively. These results indicate the eﬃcacy and robustness of\nTransRender for stroke lesion segmentation.\n/five.tnum./two.tnum. Ablation experiment\nAblation experiments on decoders are conducted to investigate\nthe impact of the render module on lesion segmentation.\nThe comparison results for performance and complexity are\npresented in\nTables 4, 5. When using the render-based decoder,\nthe DSC scores of both U-Net and TransRender are improved,\nwhile the HD scores are descended. We carry out experiments\nwith the U-Net or TransRender as encoders, and the classical\nconvolutional upsampling or render modules as decoders,\nrespectively. The DSC and HD using a convolutional upsampling-\nand render-based decoder are improved from 48.34% and 51.35 px\nto 54.13% and 40.71 px, which gain improvements of 11.98\nand 20.72%. With TransRender as the encoder, we employ\nrender as the decoder, which attains superior performance,\nscoring 59.79% in DSC and 33.98 px in HD. It is worth\nnoting that by using render as the decoder, the calculation\ncomplexity and the network parameters are also decreased. These\nablation comparisons demonstrate that the proposed render\noﬀers a competitive advantage over convolution methods in\nterms of its ability to process high-frequency information at\nthe boundary.\n/five.tnum./three.tnum. Hyper-parameter comparison\nFurther comparison experiments are conducted to explore\nwhether the hyper-parameters k and β would aﬀect the\nsegmentation performance.\nTable 6 presents the comparison\nresults using diﬀerent numbers of selected points. The number\nof points selected is desired to match the lesion due to the\ndiﬀerent sizes. When k = 3, TransRender gives the best result\nin all metrics. In the experiments, we set k = 3 by default.\nThe performance comparison using the diﬀerent numbers of\nimportant points is shown in\nTable 7. The comparison indicates\nthat there is a signiﬁcant inﬂuence of β values on the segmentation.\nFrontiers in Neuroscience /zero.tnum/nine.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nNote that we set β = 0.1 to suppress the features of important\npoints and highlight the random features of other points. The\ncomparison results indicate that more important points should be\nselected for feature extraction. As β increases, the segmentation\nperformance becomes more favorable until β = 0.7. This\nmight be due to some point features that mistakenly guide the\ndecoding process. In the other experiments, we set β to 0.7\nby default.\n/six.tnum. Conclusion\nIn this study, we propose a novel point-based boundary\nsegmentation method for stroke lesions using diﬀerent MRI\nimages. The TransRender is built on a multi-scale transformer\nencoder because of its strong ability to establish long-distance\ndependencies. The render-based decoder implements the non-\nuniform grid representation, which allows more attention to the\nFIGURE /six.tnum\nThe visual segmentation results of TransRender and the four pre vious methods on the ATLAS dataset. Where, the red color, blue color, and green\ncolor mean correct, insuﬃcient, and excessive segmentation, res pectively.\nFrontiers in Neuroscience /one.tnum/zero.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nFIGURE /seven.tnum\nThe visual segmentation results of TransRender and the four pre vious methods on the ISLES/two.tnum/zero.tnum/two.tnum/two.tnum dataset. Where, the red color, bluecolor, and green\ncolor mean correct, insuﬃcient, and excessive segmentation, res pectively.\nprecise features at the boundaries. Furthermore, a combined\nsupervision loss is utilized to optimize the point selection of\nthe render. Extensive experiments are conducted using the\ndiﬀerent ischemic stroke datasets to evaluate TransRender. And the\nexperimental results indicate that TransRender has a competitive\nadvantage over the existing networks in terms of both accuracy\nand complexity. Unfortunately, the improved render module is not\nadequate to achieve accurate segmentation due to the variety of\nlesions. We may consider the use of other network structures in\nthe future to accomplish the re-prediction of selection points in the\nrender module.\nData availability statement\nThe original contributions presented in the study are included\nin the article/supplementary material, further inquiries can be\ndirected to the corresponding authors.\nAuthor contributions\nZW: Conceptualization, Methodology, Visualization,\nWriting—original draft. XZ: Software, Supervision,\nWriting—review and editing. FL: Investigation, Supervision,\nWriting—review and editing. SW: Formal analysis, Validation,\nWriting—review and editing. JL: Validation, Writing—original\ndraft, Writing—review and editing.\nFunding\nThe author(s) declare ﬁnancial support was received\nfor the research, authorship, and/or publication of\nthis article. This work was supported by the National\nNatural Science Foundation of China (Nos. 62171307\nFrontiers in Neuroscience /one.tnum/one.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nand 62271342) and the Shanxi Province Natural Science\nFoundation (No. 202103021224113).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those\nof the authors and do not necessarily represent those of\ntheir aﬃliated organizations, or those of the publisher,\nthe editors and the reviewers. Any product that may be\nevaluated in this article, or claim that may be made by\nits manufacturer, is not guaranteed or endorsed by the\npublisher.\nReferences\nAgnes, S. A., Anitha, J., and Solomon, A. A. (2022). Two-stage lung nodule detection\nframework using enhanced UNet and convolutional LSTM networ ks in CT images.\nComput. Biol. Med. 149:106059. doi: 10.1016/j.compbiomed.2022.106059\nCao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., et al. (2023).\n“Swin-unet: Unet-like pure transformer for medical image seg mentation, ” in\nLecture Notes in Computer Science(Cham: Springer Nature Switzerland), 205–218.\ndoi: 10.1007/978-3-031-25066-8_9\nChen, X., Wang, X., Zhang, K., Fung, K.-M., Thai, T. C., Moore , K., et al. (2022).\nRecent advances and clinical applications of deep learning in med ical image analysis.\nMed. Image Anal. 79:102444. doi: 10.1016/j.media.2022.102444\nChen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., et al. (202 1). TransUNet:\ntransformers make strong encoders for medical image segmen tation. arXiv preprint:\narXiv:2102.04306. doi: 10.48550/arXiv.2102.04306\nChu, J., Chen, Y., Zhou, W., Shi, H., Cao, Y., Tu, D., et al. (202 0). “Pay\nmore attention to discontinuity for medical image segmenta tion, ” in Medical\nImage Computing and Computer Assisted Intervention?MICCAI 2020(Lima: Springer\nInternational Publishing), 166–175. doi: 10.1007/978-3-0 30-59719-1_17\nde Vries, L., Emmer, B. J., Majoie, C. B., Marquering, H. A., an d Gavves,\nE. (2023). PerfU-net: baseline infarct estimation from CT per fusion source data\nfor acute ischemic stroke. Med. Image Anal. 85:102749. doi: 10.1016/j.media.2023.\n102749\nDosovitskiy, A., Beyer, L., Kolesnikov, et al. (2021). An imag e is worth 16x16 words:\ntransformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\nGBD 2016 lifetime risk of stroke collaborators (2018). Global, regi onal, and country-\nspeciﬁc lifetime risks of stroke, 1990 and 2016. N. Engl. J. Med. 379, 2429–2437.\ndoi: 10.1056/NEJMoa1804492\nGuo, Y., Du, R., Li, X., Xie, J., Ma, Z., and Dong, Y. (2022). Lea rning calibrated class\ncenters for few-shot classiﬁcation by pair-wise similarity. IEEE Trans. Image Process.\n31, 4543–4555. doi: 10.1109/TIP.2022.3184813\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z., et al. (20 22). A\nsurvey on vision transformer. IEEE Trans. Pattern Anal. Mach. Intell. 44, 1–23.\ndoi: 10.1109/TPAMI.2022.3152247\nHe, K., Gan, C., Li, Z., Rekik, I., Yin, Z., Ji, W., et al. (2022) . Transformers in medical\nimage analysis: a review. arXiv preprint: arXiv:2202.12165.\nHuang, P., Li, D., Jiao, Z., Wei, D., Cao, B., Mo, Z., et al. (202 2). Common\nfeature learning for brain tumor MRI synthesis by context-aw are generative adversarial\nnetwork. Med. Image Anal. 79:102472. doi: 10.1016/j.media.2022.102472\nHuang, R., Lin, M., Dou, H., Lin, Z., Ying, Q., Jia, X., et al. (2 022). Boundary-\nrendering network for breast lesion segmentation in ultrasou nd images. Med. Image\nAnal. 80:102478. doi: 10.1016/j.media.2022.102478\nKervadec, H., Bouchtiba, J., Desrosiers, C., Granger, E., Do lz, J., and Ayed, I.\nB. (2021). Boundary loss for highly unbalanced segmentation. Med. Image Anal.\n67:101851. doi: 10.1016/j.media.2020.101851\nKhattar, S., and Kaur, R. (2022). Computer assisted diagnosi s of skin\ncancer: a survey and future recommendations. Comput. Electr. Eng. 104:108431.\ndoi: 10.1016/j.compeleceng.2022.108431\nKirillov, A., Wu, Y., He, K., and Girshick, R. (2020). “PointRen d: image\nsegmentation as rendering, ” in 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR)(IEEE), 1–10. doi: 10.1109/CVPR42600.2020.00982\nLee, H. J., Kim, J. U., Lee, S., Kim, H. G., and Ro, Y. M. (2020). “ Structure\nboundary preserving segmentation for medical image with amb iguous boundary, ”\nin 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\n(Seattle, WA: IEEE), 4817–4826. IEEE. doi: 10.1109/CVPR426 00.2020.00487\nLi, X., Sun, Z., Xue, J.-H., and Ma, Z. (2021). A concise revie w of\nrecent few-shot meta-learning methods. Neurocomputing 456, 463–468.\ndoi: 10.1016/j.neucom.2020.05.114\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (202 1). “Swin\ntransformer: hierarchical vision transformer using shift ed windows, ” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV)(Montreal, QC:\nIEEE), 9992–10002. doi: 10.1109/ICCV48922.2021.00986\nMatsuo, R., Yamaguchi, Y., Matsushita, T., Hata, J., Kiyuna , F., Fukuda, K., et al.\n(2017). Association between onset-to-door time and clinica l outcomes after ischemic\nstroke. Stroke 48, 3049–3056. doi: 10.1161/STROKEAHA.117.018132\nMilletari, F., Navab, N., and Ahmadi, S.-A. (2016). “V-net: fu lly convolutional neural\nnetworks for volumetric medical image segmentation, ” in 2016 Fourth International\nConference on 3D Vision (3DV)(Stanford: IEEE), 565–571. doi: 10.1109/3DV.2016.79\nNielsen, A., Hansen, M. B., Tietze, A., and Mouridsen, K. (201 8). Prediction of\ntissue outcome and assessment of treatment eﬀect in acute is chemic stroke using deep\nlearning. Stroke 49, 1394–1401. doi: 10.1161/STROKEAHA.117.019740\nPereira, S., Pinto, A., Alves, V., and Silva, C. A. (2016). Brai n tumor segmentation\nusing convolutional neural networks in MRI images. IEEE Trans. Med. Imaging35,\n1240–1251. doi: 10.1109/TMI.2016.2538465\nRonneberger, O., Fischer, P., and Brox, T. (2015). “U-net: c onvolutional networks\nfor biomedical image segmentation, ” in Lecture Notes in Computer Science(Munich:\nSpringer International Publishing), 234–241. doi: 10.1007/ 978-3-319-24574-4_28\nSchlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glo cker, B., et al.\n(2019). Attention gated networks: learning to leverage salien t regions in medical\nimages. Med. Image Anal. 53, 197–207. doi: 10.1016/j.media.2019.01.012\nTajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J. N., Wu, Z., a nd Ding, X. (2020).\nEmbracing imperfect datasets: a review of deep learning soluti ons for medical image\nsegmentation. Med. Image Anal. 63:101693. doi: 10.1016/j.media.2020.101693\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et\nal. (2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems, Vol. 30(Long Beach, CA: Curran Associates, Inc.), 5998–6008.\nWang, W., Chen, C., Ding, M., Yu, H., Zha, S., and Li, J. (2021) . “TransBTS:\nmultimodal brain tumor segmentation using transformer, ” in Medical Image\nComputing and Computer Assisted Intervention(Strasbourg: Springer International\nPublishing), 109–119. doi: 10.1007/978-3-030-87193-2_11\nWu, J., Guo, D., Wang, L., Yang, S., Zheng, Y., Shapey, J., et al. ( 2023).\nTISS-net: brain tumor image synthesis and segmentation usi ng cascaded dual-\ntask networks and error-prediction consistency. Neurocomputing 544:126295.\ndoi: 10.1016/j.neucom.2023.126295\nWu, Z., Zhang, X., Li, F., Wang, S., and Huang, L. (2022). Multi -scale long-range\ninteractive and regional attention network for stroke lesio n segmentation. Comput.\nElectr. Eng. 103:108345. doi: 10.1016/j.compeleceng.2022.108345\nWu, Z., Zhang, X., Li, F., Wang, S., and Huang, L. (2023). W-ne t: a boundary-\nenhanced segmentation network for stroke lesions. Expert Syst. Appl. 229:120637.\ndoi: 10.1016/j.eswa.2023.120637\nYang, H., Huang, W., Qi, K., Li, C., Liu, X., Wang, M., et al. (20 19). “CLCI-net: cross-\nlevel fusion and context inference networks for lesion segmen tation of chronic stroke, ”\nin Medical Image Computing and Computer Assisted Intervention(Shenzhen: Springer\nInternational Publishing), 266–274. doi: 10.1007/978-3-0 30-32248-9_30\nYu, W., Huang, Z., Zhang, J., and Shan, H. (2023). SAN-net: Le arning generalization\nto unseen sites for stroke lesion segmentation with self-adapt ive normalization.\nComput. Biol. Med. 156:106717. doi: 10.1016/j.compbiomed.2023.106717\nYuan, Y., Chao, M., and Lo, Y.-C. (2017). Automatic skin lesio n segmentation using\ndeep fully convolutional networks with Jaccard distance. IEEE Trans. Med. Imaging36,\n1876–1886. doi: 10.1109/TMI.2017.2695227\nZhang, Y., Liu, H., and Hu, Q. (2021a). “TransFuse: fusing tr ansformers\nand CNNs for medical image segmentation, ” in Medical Image Computing and\nComputer Assisted Intervention(Strasbourg: Springer International Publishing), 14–24.\ndoi: 10.1007/978-3-030-87193-2_2\nFrontiers in Neuroscience /one.tnum/two.tnum frontiersin.org\nWu et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnins./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/two.tnum/five.tnum/nine.tnum/six.tnum/seven.tnum/seven.tnum\nZhang, Y., Wu, J., Liu, Y., Chen, Y., Wu, E. X., and Tang, X. (20 21b). MI-UNet:\nmulti-inputs UNet incorporating brain parcellation for stroke les ion segmentation\nfrom T1-weighted magnetic resonance images. IEEE J. Biomed. Health Inform. 25,\n526–535. doi: 10.1109/JBHI.2020.2996783\nZhao, F., Zhao, W., Lu, H., Liu, Y., Yao, L., and Liu, Y. (2021) . Depth-\ndistilled multi-focus image fusion. IEEE Trans. Multim . 25, 966–978.\ndoi: 10.1109/TMM.2021.3134565\nZhao, Q., Wan, Y., Xu, J., and Fang, L. (2023). Cross-modal at tention\nfusion network for RGB-d semantic segmentation. Neurocomputing 548:126389.\ndoi: 10.1016/j.neucom.2023.126389\nZhou, Y., Huang, W., Dong, P., Xia, Y., and Wang, S. (2021). D- unet:\na dimension-fusion u shape network for chronic stroke lesion s egmentation.\nIEEE/ACM Trans. Comput. Biol. Bioinform. 18, 940–950. doi: 10.1109/TCBB.2019.\n2939522\nZhou, H.-Y., Guo, J., Zhang, Y., Yu, L., Wang, L., and Yu, Y. (20 21). nnFormer:\ninterleaved transformer for volumetric segmentation. arXiv preprint: arXiv:2109.\n03201.\nZhu, Z., He, X., Qi, G., Li, Y., Cong, B., and Liu, Y. (2023). Bra in tumor segmentation\nbased on the fusion of deep semantics and edge information in multimodal MRI.\nInform. Fus. 91, 376–387. doi: 10.1016/j.inﬀus.2022.10.022\nFrontiers in Neuroscience /one.tnum/three.tnum frontiersin.org",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7663683891296387
    },
    {
      "name": "Segmentation",
      "score": 0.7626920938491821
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6725723743438721
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6369761228561401
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.6038326025009155
    },
    {
      "name": "Encoder",
      "score": 0.5092426538467407
    },
    {
      "name": "Computer vision",
      "score": 0.49793505668640137
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.43858957290649414
    },
    {
      "name": "Transformer",
      "score": 0.4198923110961914
    },
    {
      "name": "Pixel",
      "score": 0.41148611903190613
    },
    {
      "name": "Voltage",
      "score": 0.1096659004688263
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9086337",
      "name": "Taiyuan University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I17721919",
      "name": "Shanxi Medical University",
      "country": "CN"
    }
  ]
}