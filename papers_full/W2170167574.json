{
  "title": "Aligning context-based statistical models of language with brain activity during reading",
  "url": "https://openalex.org/W2170167574",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2061328669",
      "name": "Leila Wehbe",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2171687631",
      "name": "Ashish Vaswani",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2101069083",
      "name": "Kevin Knight",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2120740404",
      "name": "Tom Mitchell",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3031967244",
    "https://openalex.org/W2949950585",
    "https://openalex.org/W2048631316",
    "https://openalex.org/W932413789",
    "https://openalex.org/W1533179050",
    "https://openalex.org/W2060366056",
    "https://openalex.org/W4403257984",
    "https://openalex.org/W2166362343",
    "https://openalex.org/W2474824677",
    "https://openalex.org/W1990381576",
    "https://openalex.org/W2107344926",
    "https://openalex.org/W1967125431",
    "https://openalex.org/W2740295777",
    "https://openalex.org/W1987951491",
    "https://openalex.org/W2161632259",
    "https://openalex.org/W1665214252",
    "https://openalex.org/W1596515083",
    "https://openalex.org/W2153705299",
    "https://openalex.org/W2409599279",
    "https://openalex.org/W1992570774",
    "https://openalex.org/W1980592753",
    "https://openalex.org/W4205470728"
  ],
  "abstract": "Many statistical models for natural language processing exist, including context-based neural networks that (1) model the previously seen context as a latent feature vector, (2) integrate successive words into the context using some learned representation (embedding), and (3) compute output probabilities for incoming words given the context.On the other hand, brain imaging studies have suggested that during reading, the brain (a) continuously builds a context from the successive words and every time it encounters a word it (b) fetches its properties from memory and (c) integrates it with the previous context with a degree of effort that is inversely proportional to how probable the word is.This hints to a parallelism between the neural networks and the brain in modeling context (1 and a), representing the incoming words (2 and b) and integrating it (3 and c).We explore this parallelism to better understand the brain processes and the neural networks representations.We study the alignment between the latent vectors used by neural networks and brain activity observed via Magnetoencephalography (MEG) when subjects read a story.For that purpose we apply the neural network to the same text the subjects are reading, and explore the ability of these three vector representations to predict the observed word-by-word brain activity.Our novel results show that: before a new word i is read, brain activity is well predicted by the neural network latent representation of context and the predictability decreases as the brain integrates the word and changes its own representation of context.Secondly, the neural network embedding of word i can predict the MEG activity when word i is presented to the subject, revealing that it is correlated with the brain's own representation of word i.Moreover, we obtain that the activity is predicted in different regions of the brain with varying delay.The delay is consistent with the placement of each region on the processing pathway that starts in the visual cortex and moves to higher level regions.Finally, we show that the output probability computed by the neural networks agrees with the brain's own assessment of the probability of word i, as it can be used to predict the brain activity after the word i's properties have been fetched from memory and the brain is in the process of integrating it into the context.",
  "full_text": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 233–243,\nOctober 25-29, 2014, Doha, Qatar.c⃝2014 Association for Computational Linguistics\nAligning context-based statistical models of language\nwith brain activity during reading\nLeila Wehbe1,2, Ashish Vaswani3, Kevin Knight3 and Tom Mitchell1,2\n1 Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA\n2 Center for the Neural Basis of Computation, Carnegie Mellon University, Pittsburgh, PA\n3 Information Sciences Institute, University of Southern California, Los Angeles, CA\nlwehbe@cs.cmu.edu, vaswani@usc.edu, knight@isi.edu, tom.mitchell@cs.cmu.edu\nAbstract\nMany statistical models for natural language pro-\ncessing exist, including context-based neural net-\nworks that (1) model the previously seen context\nas a latent feature vector, (2) integrate successive\nwords into the context using some learned represen-\ntation (embedding), and (3) compute output proba-\nbilities for incoming words given the context. On\nthe other hand, brain imaging studies have sug-\ngested that during reading, the brain (a) continu-\nously builds a context from the successive words\nand every time it encounters a word it (b) fetches its\nproperties from memory and (c) integrates it with\nthe previous context with a degree of effort that is\ninversely proportional to how probable the word is.\nThis hints to a parallelism between the neural net-\nworks and the brain in modeling context (1 and a),\nrepresenting the incoming words (2 and b) and in-\ntegrating it (3 and c). We explore this parallelism to\nbetter understand the brain processes and the neu-\nral networks representations. We study the align-\nment between the latent vectors used by neural net-\nworks and brain activity observed via Magnetoen-\ncephalography (MEG) when subjects read a story.\nFor that purpose we apply the neural network to the\nsame text the subjects are reading, and explore the\nability of these three vector representations to pre-\ndict the observed word-by-word brain activity.\nOur novel results show that: before a new word i\nis read, brain activity is well predicted by the neural\nnetwork latent representation of context and the pre-\ndictability decreases as the brain integrates the word\nand changes its own representation of context. Sec-\nondly, the neural network embedding of word i can\npredict the MEG activity when word i is presented\nto the subject, revealing that it is correlated with the\nbrain’s own representation of wordi. Moreover, we\nobtain that the activity is predicted in different re-\ngions of the brain with varying delay. The delay is\nconsistent with the placement of each region on the\nprocessing pathway that starts in the visual cortex\nand moves to higher level regions. Finally, we show\nthat the output probability computed by the neural\nnetworks agrees with the brain’s own assessment of\nthe probability of wordi, as it can be used to predict\nthe brain activity after the word i’s properties have\nbeen fetched from memory and the brain is in the\nprocess of integrating it into the context.\n1 Introduction\nNatural language processing has recently seen a\nsurge in increasingly complex models that achieve\nimpressive goals. Models like deep neural net-\nworks and vector space models have become pop-\nular to solve diverse tasks like sentiment analy-\nsis and machine translation. Because of the com-\nplexity of these models, it is not always clear how\nto assess and compare their performances as they\nmight be useful for one task and not the other.\nIt is also not easy to interpret their very high-\ndimensional and mostly unsupervised representa-\ntions. The brain is another computational system\nthat processes language. Since we can record brain\nactivity using neuroimaging, we propose a new di-\nrection that promises to improve our understand-\ning of both how the brain is processing language\nand of what the neural networks are modeling by\naligning the brain data with the neural networks\nrepresentations.\nIn this paper we study the representations of two\nkinds of neural networks that are built to predict\nthe incoming word: recurrent and ﬁnite context\nmodels. The ﬁrst model is the Recurrent Neural\nNetwork Language Model (Mikolov et al., 2011)\nwhich uses the entire history of words to model\ncontext. The second is the Neural Probabilistic\nLanguage Model (NPLM) which uses limited con-\ntext constrained to the recent words (3 grams or 5\ngrams). We trained these models on a large Harry\nPotter fan ﬁction corpus and we then used them to\npredict the words of chapter 9 ofHarry Potter and\nthe Sorcerer’s Stone(Rowling, 2012). In paral-\nlel, we ran an MEG experiment in which 3 subject\nread the words of chapter 9 one by one while their\nbrain activity was recorded. We then looked for\nthe alignment between the word-by-word vectors\nproduced by the neural networks and the word-by-\nword neural activity recorded by MEG.\nOur neural networks have 3 key constituents:\na hidden layer that summarizes the history of the\nprevious words ; an embeddings vector that sum-\nmarizes the (constant) properties of a given word\nand ﬁnally the output probability of a word given\n233\nReading comprehension is reﬂected in the subsequent acti-\nvation of the left superior temporal cortex at 200–600 ms\n(Halgren et al., 2002; Helenius et al., 1998; Pylkka¨ nen\net al., 2002, 2006; Pylkka¨ nen and Marantz, 2003; Simos\net al., 1997). This sustained activation diﬀerentiates\nbetween words and nonwords (Salmelin et al., 1996; Wil-\nson et al., 2005; Wydell et al., 2003). Apart from lexical-se-\nmantic aspects it also seems to be sensitive to phonological\nmanipulation (Wydell et al., 2003).\nAs discussed above, in speech perception activation is\nconcentrated to a rather small area in the brain and we\nhave to rely on time information to dissociate between dif-\nferent processes. Here, the diﬀerent processes are separable\nboth in timing and location. Because of that, one might\nthink that it is easier to characterize language-related pro-\ncesses in the visual than auditory modality. However, here\nthe diﬃculties appear at another level. In reading, activa-\ntion is detected bilaterally in the occipital cortex, along\nthe temporal lobes, in the parietal cortex and, in vocalized\nreading, also in the frontal lobes, at various times with\nrespect to stimulus onset. Interindividual variability further\ncomplicates the picture, resulting in practically excessive\namounts of temporal and spatial information. The areas\nand time windows depicted in Fig. 5, with speciﬁc roles\nin reading, form a limited subset of all active areas\nobserved during reading. In order to perform proper func-\ntional localization one needs to vary the stimuli and tasks\nsystematically, in a parametric fashion. Let us now consid-\ner how one may extract activation reﬂecting pre-lexical let-\nter-string analysis and lexical-semantic processing.\n3.2. Pre-lexical analysis\nIn order to tease apart early pre-lexical processes in\nreading, Tarkiainen and colleagues (Tarkiainen et al.,\n1999) used words, syllables, and single letters, imbedded\nin a noisy background, at four diﬀerent noise levels\n(Fig. 6). For control, the sequences also contained symbol\nstrings. One sequence was composed of plain noise stimuli.\nThe stimuli were thus varied along two major dimensions:\nthe amount of features to process increased with noise and\nwith the number of items, letters or symbols. On the other\nhand, word-likeness was highest for clearly visible complete\nwords and lowest for symbols and noise.\nAt the level of the brain, as illustrated in Fig. 7, the data\nshowed a clear dissociation between two processes within\nthe ﬁrst 200 ms: visual feature analysis occurred at about\n100 ms after stimulus presentation, with the active areas\naround the occipital midline, along the ventral stream. In\nthese areas, the signal increased with increasing noise and\nwith the number of items in the string, similarly for letters\nand symbols. Only 50 ms later, at about 150 ms, the left\ninferior occipitotemporal cortex showed letter-string spe-\nciﬁc activation. This signal increased with the visibility of\nthe letter strings. It was strongest for words, weaker for syl-\nlables, and still weaker for single letters. Crucially, the acti-\nvation was signiﬁcantly stronger for letter than symbol\nstrings of equal length.\nBilateral occipitotemporal activation at about 200 ms\npost-stimulus is consistently reported in MEG studies of\nreading (Cornelissen et al., 2003b; Pammer et al., 2004; Sal-\nmelin et al., 1996, 2000b) but, interestingly, functional\nspeciﬁcity for letter-strings is found most systematically\nin the left hemisphere. The MEG data on letter-string spe-\nciﬁc activation are in good agreement with intracranial\nrecordings, both with respect to timing and location and\nthe pre-lexical nature of the activation (Nobre et al., 1994).\n3.3. Lexical-semantic analysis\nTo identify cortical dynamics of reading comprehension,\nHelenius and colleagues (Helenius et al., 1998) employed a\nVisual feature\nanalysis\nNon-specific Words =\nNonwords Nonwords\nLetter-string\nanalysis\nTime (ms)\n0 400 800 0 400 800 0 400 800\nLexical-semantic\nanalysis\nFig. 5. Cortical dynamics of silent reading. Dots represent centres of active cortical patches collected from individual subjects. The curves display the\nmean time course of activation in the depicted source areas. Visual feature analysis in the occipital cortex (/C24100 ms) is stimulus non-speciﬁc. The stimulus\ncontent starts to matter by /C24150 ms when activation reﬂecting letter-string analysis is observed in the left occipitotemporal cortex. Subsequent activation\nof the left superior temporal cortex at /C24200–600 ms reﬂects lexical-semantic analysis and, probably, also phonological analysis. Modiﬁed from Salmelin\net al. (2000a).\n/C211Lippincott Williams & Wilkins 2000\nR. Salmelin / Clinical Neurophysiology xxx (2006) xxx–xxx 5\nARTICLE IN PRESS\nPlease cite this article as: Riitta Salmelin, Clinical neurophysiology of language: The MEG approach, Clinical Neurophysiology\n(2006), doi:10.1016/j.clinph.2006.07.316\nFigure 1: Cortical dynamics of silent reading. This ﬁgure\nis adapted from (Salmelin, 2007). Dots represent projected\nsources of activity in the visual cortex (left brain sketch) and\nthe temporal cortex (right brain sketch). The curves display\nthe mean time course of activation in the depicted source ar-\neas for different conditions. The initial visual feature anal-\nysis in the visual cortex at ∼100 ms is non-speciﬁc to lan-\nguage. Comparing responses to letter strings and other vi-\nsual stimuli reveals that letter string analysis occurs around\n150 ms. Finally comparing the responses to words and non-\nwords (made-up words) reveals lexical-semantic analysis in\nthe temporal cortex at ∼200-500ms.\nthe context. We set out to ﬁnd the brain analogs\nof these model constituents using an MEG decod-\ning task. We compare the different models and\ntheir representations in terms of how well they\ncan be used to decode the word being read from\nMEG data. We obtain correspondences between\nthe models and the brain data that are consistent\nwith a model of language processing in which\nbrain activity encodes story context, and where\neach new word generates additional brain activity,\nﬂowing generally from visual processing areas to\nmore high level areas, culminating in an updated\nstory context, and reﬂecting an overall magnitude\nof neural effort inﬂuenced by the probability of\nthat new word given the previous context.\n1.1 Neural processes involved in reading\nHumans read with an average speed of 3 words\nper second. Reading requires us to perceive in-\ncoming words and gradually integrate them into\na representation of the meaning. As words are\nread, it takes 100ms for the visual input to reach\nthe visual cortex. 50ms later, the visual input is\nprocessed as letter strings in a specialized region\nof the left visual cortex (Salmelin, 2007). Be-\ntween 200-500ms, the word’s semantic properties\nare processed (see Fig. 1). Less is understood\nabout the cortical dynamics of word integration, as\nmultiple theories exist (Friederici, 2002; Hagoort,\n2003).\nMagnetoencephalography (MEG) is a brain-\nimaging tool that is well suited for studying lan-\nguage. MEG records the change in the magnetic\nﬁeld on the surface of the head that is caused by\na large set of aligned neurons that are changing\ntheir ﬁring patterns in synchrony in response to\na stimulus. Because of the nature of the signal,\nMEG recordings are directly related to neural ac-\ntivity and have no latency. They are sampled at\na high frequency (typically 1kHz) that is ideal for\ntracking the fast dynamics of language processing.\nIn this work, we are interested in the mecha-\nnism of human text understanding as the meaning\nof incoming words is fetched from memory and\nintegrated with the context. Interestingly, this is\nanalogous to neural network models of language\nthat are used to predict the incoming word. The\nmental representation of the previous context is\nanalogous to the latent layer of the neural network\nwhich summarizes the relevant context before see-\ning the word. The representation of the meaning\nof a word is analogous to the embedding that the\nneural network learns in training and then uses.\nFinally, one common hypotheses is that the brain\nintegrates the word with inversely proportional ef-\nfort to how predictable the word is (Frank et al.,\n2013). There is a well studied response known as\nthe N400 that is an increase of the activity in the\ntemporal cortex that has been recently shown to be\ngraded by the amount of surprisal of the incoming\nword given the context (Frank et al., 2013). This is\nanalogous to the output probability of the incom-\ning word from the neural network.\nFig. 2 shows a hypothetical activity in an MEG\nsensor as a subject reads a story in our experi-\nment, in which words are presented one at a time\nfor 500ms each. We conjecture that the activity in\ntime window a, i.e. before word iis understood, is\nmostly related to the previous context before see-\ning word i. We also conjecture that the activity in\ntime window bis related to understanding word i\nand integrating it into the context, leading to a new\nrepresentation of context in window c.\nUsing three types of features from neural net-\nworks (hidden layer context representation, output\nprobabilities and word embeddings) from three\ndifferent models of language (one recurrent model\nand two ﬁnite context models), we therefore set to\npredict the activity in the brain in different time\nwindows. We want to align the brain data with the\nvarious model constituents to understand where\nand when different types of processes are com-\nputed in the brain, and simultaneously, we want to\n234\nHarry'\nHarry'\nhad'\nhad'\nnever'\nnever'\nembedding(iC1)' embedding(i)' embedding(i+1)'\ncontext(iC1)'context(iC2)' context(i)'\nout.'prob.(iC1)' out.'prob.(i)' out.'prob.(i+1)'\nStudying'the'construc<on'of'meaning'\n3'\nword i+1 word i-1 word i \n0.5's'0.5's' 0.5's'\na\nb\nc\nLeila'Wehbe'\n'…''Harry'''''''had''''''''never'…''\nFigure 2: [Top] Sketch of the updates of a neural network\nreading chapter 9 after it has been trained. Every word cor-\nresponds to a ﬁxed embedding vector (magenta). A context\nvector (blue) is computed before the word is seen given the\nprevious words. Given the context vector, the probability of\nevery word can be computed (symbolized by the histogram\nin green). We only use the output probability of the actual\nword (red circle). [Bottom] Hypothetical activity in an MEG\nsensor when the subject reads the corresponding words. The\ntime periods approximated as a, b and c can be tested for in-\nformation content relating to: the context of the story before\nseeing word i (modeled by the context vector at i), the repre-\nsentation of the properties of word i (the embedding of word\ni) and the integration of word i into the context (the output\nprobability of word i). The periods drawn here are only a\nconjecture on the timings of such cognitive events.\nuse the brain data to shed light on what the neural\nnetwork vectors are representing.\nRelated work\nDecoding cognitive states from brain data is a\nrecent ﬁeld that has been growing in popularity.\nMost decoding studies that study language use\nfunctional Magnetic Resonance Imaging (fMRI),\nwhile some studies use MEG. MEG’s high tempo-\nral resolution makes it invaluable for looking at the\ndynamics of language understanding. (Sudre et\nal., 2012) decode from MEG the word a subject is\nreading. The authors estimate from the MEG data\nthe semantic features of the word and use these as\nan intermediate step to decode what the word is.\nThis is in principle similar to the classiﬁcation ap-\nproach we follow, as we will also use the feature\nvectors as an intermediate step for word classiﬁca-\ntion. However the experimental paradigm in (Su-\ndre et al., 2012) is to present to the subjects sin-\ngle isolated words and to ﬁnd how the brain rep-\nresents their semantic features; whereas we have a\nmuch more complex and “naturalistic” experiment\nin which the subjects read a non-artiﬁcial passage\nof text, and we look at processes that exceed in-\ndividual word processing: the construction of the\nmeanings of the successive words and the predic-\ntion/integration of incoming words.\nIn (Frank et al., 2013), the amount of surprisal\nthat a word has given its context is used to pre-\ndict the intensity of the N400 response described\npreviously. This is the closest study we could ﬁnd\nto our approach. This study was concerned with\nanalyzing the brain processes related only to sur-\nprisal while we propose a more integral account\nof the processes in the brain. The study also didn’t\naddress the major contribution we propose here,\nwhich is to shed light on the inner constituents of\nlanguage models using brain imaging.\n1.2 Recurrent and ﬁnite context neural\nnetworks\nSimilar to standard language models, neural lan-\nguage models also learn probability distributions\nover words given their previous context. However,\nunlike standard language models, words are rep-\nresented as real-valued vectors in a high dimen-\nsional space. These word vectors, referred to as\nword embeddings, can be different for input and\noutput words, and are learned from training data.\nThus, although at training and test time, the in-\nput and output to the neural language models are\none-hot representation of words, it is their em-\nbeddings that are used to compute word proba-\nbility distributions. After training the embedding\nvectors are ﬁxed and it is these vectors that we\nwill use later on to predict MEG data. To predict\nMEG data, we will also use the latent vector rep-\nresentations of context that these neural networks\nproduce, as well as the probability of the current\nword given the context. In this section, we will\ndescribe how recurrent neural network language\nmodels and feedforward neural probabilistic lan-\nguage models compute word probabilities. In the\ninterest of space, we keep this description brief,\nand for details, the reader is requested to refer to\nthe original papers describing these models.\n235\nw(t) s(t−1)\ny(t) c(t)\nhidden\ns(t)\noutput\nP(wt+1 |s(t))\nD W\nD′ X\nFigure 3: Recurrent neural network language model.\nRecurrent Neural Network Language Model\nUnlike standard feedforward neural language\nmodels that only look at a ﬁxed number of past\nwords, recurrent neural network language models\nuse all the previous history from position1 to t−1\nto predict the next word. This is typically achieved\nby feedback connections, where the hidden layer\nactivations used for predicting the word in posi-\ntion t−1 are fed back into the network to com-\npute the hidden layer activations for predicting the\nnext word. The hidden layer thus stores the history\nof all previous words. We use the RNNLM archi-\ntecture as described in Mikolov (2012), shown in\nFigure 3. The input to the RNNLM at position t\nare the one-hot representation of the current word,\nw(t), and the activations from the hidden layer at\nposition t−1, s(t−1). The output of the hidden\nlayer at position t−1 is\ns(t) =φ(Dw(t) +Ws(t−1)) ,\nwhere D is the matrix of input word embeddings,\nW is a matrix that transforms the activations from\nthe hidden layer in position t −1, and φ is a\nsigmoid function, deﬁned as φ(x) = 1\n1+exp(−x) ,\nthat is applied elementwise. We need to compute\nthe probability of the next word w(t+ 1)given\nthe hidden state s(t). For fast estimation of out-\nput word probabilities, Mikolov (2012) divides the\ncomputation into two stages: First, the probability\ndistribution over word classes is computed, after\nwhich the probability distribution over the subset\nof words belonging to the class are computed. The\nclass probability of a particular class with indexm\nat position tis computed as:\nP(cm(t) |s(t)) = exp (s(t)Xvm)∑C\nc=1 (exp (s(t)Xvc))\n,\nwhere X is a matrix of class embeddings and vm\nis a one-hot vector representing the class with in-\ndex m. The normalization constant is computed\nu1 u2\ninput\nwords\ninput\nembeddings\nhidden\nh1\nhidden\nh2\noutput\nP(w |u)\nD′\nM\nC1 C2\nD\nFigure 4: Neural probabilistic language model\nover all classes C. Each class speciﬁes a subset\nV′of words, potentially smaller than the entire vo-\ncabulary V. The probability of an output word lat\nposition t+ 1given that its class is mis deﬁned\nas:\nP(yl(t+ 1)|cm(t),s(t)) =\nexp (s(t)D′vl)∑V ′\nk=1 (exp (s(t)D′vk))\n,\nwhere D′is a matrix of output word embeddings\nand vl is a one hot vector representing the word\nwith index l. The probability of the word w(t+ 1)\ngiven its class ci can now be computed as:\nP(w(t+ 1)|s(t)) =P(w(t+ 1)|ci,s(t))\nP(ci |s(t)).\nNeural Probabilistic Language Model\nWe use the feedforward neural probabilistic lan-\nguage model architecture of Vaswani et al. (2013),\nas shown in Figure 4. Each context u comprises\na sequence of words uj (1 ≤j ≤n−1) repre-\nsented as one-hot vectors, which are fed as input\nto the neural network. At the output layer, the neu-\nral network computes the probability P(w|u) for\neach word w, as follows.\nThe output of the ﬁrst hidden layer h1 is\nh1 = φ\n\n\nn−1∑\nj=1\nCjDuj + b1\n\n,\nwhere D is a matrix of input word embeddings\nwhich is shared across all positions, theCj are the\ncontext matrices for each word in u, b1 is a vec-\ntor of biases with the same dimension ash1, and φ\nis applied elementwise. Vaswani et al. (2013) use\nrectiﬁed linear units (Nair and Hinton, 2010) for\n236\nthe hidden layers h1 and h2, which use the activa-\ntion function φ(x) = max(0,x).\nThe output of the second layer h2 is\nh2 = φ(Mh1 + b2) ,\nwhere M is a weight matrix between h1 and h2\nand b2 is a vector of biases for h2. The probabil-\nity of the output word is computed at the output\nsoftmax layer as:\nP(w|u) = exp\n(\nvwD′h2 + bT vw\n)\n∑V\nw′=1 exp (vw′D′h2 + bT vw′)\n,\nwhere D′ is the matrix of output word embed-\ndings, b is a vector of biases for every output word\nand vw its the one hot representation of the word\nwin the vocabulary.\n2 Methods\nWe describe in this section our approach. In sum-\nmary, we trained the neural network models on\na Harry Potter fan ﬁction database. We then ran\nthese models on chapter 9 of Harry Potter and the\nSorcerer’s Stone(Rowling, 2012) and computed\nthe context and embedding vectors and the output\nprobability for each word. In parallel, 3 subjects\nread the same chapter in an MEG scanner. We\nbuild models that predict the MEG data for each\nword as a function of the different neural network\nconstituents. We then test these models with a\nclassiﬁcation task that we explain below. We de-\ntect correspondences between the neural network\ncomponents and the brain processes that under-\nlie reading in the following fashion. If using a\nneural network vector (e.g. the RNNLM embed-\nding vector) allows us to classify signiﬁcantly bet-\nter than chance in a given region of the brain at\na given time (e.g. the visual cortex at time 100-\n200ms), then we can hypothesize a relationship\nbetween that neural network constituent and the\ntime/location of the analogous brain process.\n2.1 Training the Neural Networks\nWe used the freely available training tools pro-\nvided by Mikolov (2012) 1 and Vaswani et al.\n(2013)2 to train our RNNLM and NPLM models\nused in our brain data classiﬁcation experiments.\nOur training data comprised around 67.5 million\n1http://rnnlm.org/\n2http://nlg.isi.edu/software/nplm\nwords for training and100 thousand words for val-\nidation from the Harry Potter fan ﬁction database\n(http://harrypotterfanﬁction.com). We restricted\nthe vocabulary to the top 100 thousand words\nwhich covered all but 4 words from Chapter 9 of\nHarry Potter and the Sorcerer’s Stone.\nFor the RNNLM, we trained models with differ-\nent hidden layers and learning rates and found the\nRNNLM with 250 hidden units to perform best on\nthe validation set. We extracted our word embed-\ndings from the input matrixD (Figure 3). We used\nthe default settings for all other hyper parameters.\nWe trained 3-gram and 5-gram NPLMs with\n150 dimensional word embeddings and experi-\nmented with different number of units for the ﬁrst\nhidden layer (h1 in Figure 4), and different learn-\ning rates. For both the 3-gram and 5-gram mod-\nels, we found 750 hidden units to perform the best\non the validation set and chose those models for\nour ﬁnal experiments. We used the output word\nembeddings D′ in our experiments. We visually\ninspected the nearest neighbors in the 150 dimen-\nsional word embedding space for some words and\ndidn’t ﬁnd the neighbors from D′or D to be dis-\ntinctly better than each other. We leave the com-\nparison of input and output embeddings on brain\nactivity prediction for future work.\n2.2 MEG paradigm\nWe recorded MEG data for three subjects (2 fe-\nmales and one male) while they read chapter 9\nof Harry Potter and the Sorcerer’s Stone(Rowl-\ning, 2012). The participants were native English\nspeakers and right handed. They were chosen to\nbe familiar with the material: we made sure they\nhad read the Harry Potter books or seen the movies\nseries and were familiar with the characters and\nthe story. All the participants signed the consent\nform, which was approved by the University of\nPittsburgh Institutional Review Board, and were\ncompensated for their participation.\nThe words of the story were presented in rapid\nserial visual format (Buchweitz et al., 2009):\nwords were presented one by one at the center\nof the screen for 0.5 seconds each. The text was\nshown in 4 experimental blocks of ∼11 minutes.\nIn total, 5176 words were presented. Chapter 9\nwas presented in its entirety without modiﬁcations\nand each subject read the chapter only once.\nOne can think of an MEG machine as a large\nhelmet, with sensors located on the helmet that\n237\nrecord the magnetic activity. Our MEG recordings\nwere acquired on an Elekta Neuromag device at\nthe University of Pittsburgh Medical Center Pres-\nbyterian Hospital. This machine has 306 sensors\ndistributed into 102 locations on the surface of the\nsubject’s head. Each location groups 3 sensors or\ntwo types: one magnometer that records the in-\ntensity of the magnetic ﬁeld and two planar gra-\ndiometers that record the change in the magnetic\nﬁeld along two orthogonal planes3.\nOur sampling frequency was 1kHz. For prepro-\ncessing, we used Signal Space Separation method\n(SSS, (Taulu et al., 2004)), followed by its tempo-\nral extension (tSSS, (Taulu and Simola, 2006)).\nFor each subject, the experiment data consists\ntherefore of a 306 dimensional time series of\nlength ∼45 minutes. We averaged the signal in ev-\nery sensor into 100ms non-overlapping time bins.\nSince words were presented for 500ms each, we\ntherefore obtain for every word p = 306×5 val-\nues corresponding to 306 vectors of 5 points.\n2.3 Decoding experiment\nTo ﬁnd which parts of brain activity are related to\nthe neural network constituents (e.g. the RNNLM\ncontext vector), we run a prediction and classiﬁca-\ntion experiment in a 10-fold cross validated fash-\nion. At every fold, we train a linear model to pre-\ndict MEG data as a function of one of the feature\nsets, using 90% of the data. On the remaining 10%\nof the data, we run a classiﬁcation experiment.\nMEG data is very noisy. Therefore, classify-\ning single word waveforms yields a low accuracy,\npeaking at 60%, which might lead to false nega-\ntives when looking for correspondences between\nneural network features and brain data. To reveal\ninformative features, one can boost signal by ei-\nther having several repetitions of the stimuli in the\nexperiment and then averaging (Sudre et al., 2012)\nor by combining the words into larger chunks (We-\nhbe et al., 2014). We chose the latter because the\nformer sacriﬁces word and feature diversity.\nAt testing, we therefore repeat the following\n300 times. Two sets of words are chosen ran-\ndomly from the test fold. To form the ﬁrst set, 20\nwords are sampled without replacement from the\ntest sample (unseen by the classiﬁer). To form the\nsecond set, the kth word is chosen randomly from\nall words in the test fold having the same length as\n3In this paper, we treat these three different sensors as\nthree different dimensions without further exploiting their\nphysical properties.\nthe kth word of the ﬁrst set. Since every fold of\nthe data was used 9 times in the training phase and\nonce in the testing phase, and since we use a high\nnumber of randomized comparisons, this averages\nout biases in the accuracy estimation. Classifying\nsets of 20 words improves the classiﬁcation accu-\nracy greatly while lowering its variance and makes\nit dissociable from chance performance. We com-\npare only between words of equal length, to mini-\nmize the effect of the low level visual features on\nthe classiﬁcation accuracy.\nAfter averaging out the results of multiple folds,\nwe end up with average accuracies that reveal how\nrelated one of the models’ constituents (e.g. the\nRNNLM context vector) is to brain data.\n2.3.1 Annotation of the stimulus text\nWe have 9 sets of annotations for the words of the\nexperiment. Each set j can be described as a ma-\ntrix Fj in which each rowicorresponds to the vec-\ntor of annotations of word i. Our annotations cor-\nrespond to the 3 model constituents for each of the\n3 models: the hidden layer representation before\nword i, the output probability of word i and the\nlearned embeddings for word i.\n2.3.2 Classiﬁcation\nIn order to align the brain processes and the differ-\nent constituents of the different models, we use a\nclassiﬁcation task. The task is to classify the word\na subject is reading out of two possible choices\nfrom its MEG recording. The classiﬁer uses one\ntype of feature in an intermediate classiﬁcation\nstep. For example, the classiﬁer learns to predict\nthe MEG activity for any setting of the RNNLM\nhidden layer. Given an unseen MEG recording for\nan unknown word iand two possible story words\ni′and i′′(one of which being the true word i), the\nclassiﬁer predicts the MEG activity when reading\ni′and i′′from their hidden layer vectors. It then\nassigns the label i′ or i′′ to the word recording i\ndepending on which prediction is the closest to the\nrecording. The following are the detailed steps of\nthis complex classiﬁcation task. However, for the\nrest of the paper the most useful point to keep in\nmind is that the main purpose of the classiﬁcation\nis to ﬁnd a correspondence between the brain data\nand a given feature set j.\n1. Normalize the columns of M (zero mean,\nstandard deviation = 1). Pick feature set Fj\nand normalize its columns to a minimum of 0\nand a maximum of 1.\n238\n2. Divide the data into 10 folds, for each fold b:\n(a) Isolate Mb and Fb\nj as test data. The re-\nmainder M−b and F−b\nj will be used for\ntraining4.\n(b) Subtract the mean of the columns of\nM−b from Mb and M−b and the mean\nof the columns of F−b\nj from Fb\nj and F−b\nj\n(c) Use ridge regression to solve\nM−b = F−b\nj ×βt\nj\nby tuning the λparameter to every one\nof the p output dimensions indepen-\ndently. λis chosen via generalized cross\nvalidation (Golub et al., 1979).\n(d) Perform a binary classiﬁcation. Sample\nfrom the set of words in ba set cof 20\nwords. Then sample from banother set\nof 20 words such that the kth word in c\nand d have the same number of letters.\nFor every sample (c,d):\ni. predict the MEG data for cand das:\nPc = Fc\nj ×Γb\nj and Pd = Fd\nj ×Γb\nj\nii. assign to Mc the label cor ddepend-\ning on which of Pc or Pd is closest\n(Euclidean distance).\niii. assign to Md the label c or d de-\npending on which of Pc or Pd is\nclosest (Euclidean distance).\n3. Compute the average accuracy.\n2.3.3 Restricting the analysis spatially: a\nsearchlight equivalent\nWe adapt the searchlight method (Kriegeskorte et\nal., 2006) to MEG. The searchlight is a discovery\nprocedure used in fMRI in which a cube is slid\nover the brain and an analysis is performed in each\nlocation separately. It allows to ﬁnd regions in the\nbrain where a speciﬁc phenomenon is occurring.\nIn the MEG sensor space, for every one of the 102\nsensor locations ℓ, we assign a group of sensorsgℓ.\nFor every location ℓ, we identify the locations that\nimmediately surround it in any direction (Anterior,\nRight Anterior, Right etc...) when looking at the\n2D ﬂat representation of the location of the sensors\nin the MEG helmet (see Fig. 9 for an illustration of\nthe 2D helmet). gℓ therefore contains the 3 sensors\nat location ℓand at the neighboring locations. The\nmaximum number of sensors in a group is 3 ×9.\n4The rows from M−b and F−b\nj that correspond to the ﬁve\nwords before or after the test set are ignored in order to make\nthe test set independent.\nThe locations at the edge of the helmet have fewer\nsensors because of the missing neighbor locations.\n2.3.4 Restricting the analysis temporally\nInstead of using the entire time course of the word,\nwe can use only one of the corresponding 100ms\ntime windows. Obtaining a high classiﬁcation ac-\ncuracy using one of the time windows and feature\nset jmeans that the analogous type of information\nis encoded at that time.\n2.3.5 Classiﬁcation accuracy by time and\nregion\nThe above steps compute whole brain accuracy us-\ning all the time series. In order to perform a more\nprecise spatio-temporal analysis, one can use only\none time windowmand one locationℓfor the clas-\nsiﬁcation. This can answer the question of when\nand where different information is represented by\nbrain activity. For every location, we will use only\nthe columns corresponding to the time pointmfor\nthe sensors belonging to the group gℓ. Step (d) of\nthe classiﬁcation procedure is changed as such:\n(d) Perform a binary classiﬁcation. Sample from\nthe set of words in ba set cof 20 words. Then\nsample from b another set of 20 words such\nthat the kth word in c and d have the same\nnumber of letters. For every sample (c,d), and\nfor every setting of {m,ℓ}:\ni. predict the MEG data for c and d as:\nPc\n{m,ℓ}= Fc\nj ×Γb\nj,{m,ℓ}and\nPd\n{m,ℓ}= Fd\nj ×Γb\nj,{m,ℓ}\nii. assign to Mc\n{m,ℓ}the label cor ddepend-\ning on which of Pc\n{m,ℓ}or Pd\n{m,ℓ}is clos-\nest (Euclidean distance).\niii. assign to Md\n{m,ℓ}the label cor ddepend-\ning on which of Pc\n{m,ℓ}or Pd\n{m,ℓ}is clos-\nest (Euclidean distance).\n2.3.6 Statistical signiﬁcance testing\nWe determine the distribution for chance perfor-\nmance empirically. Because the successive word\nsamples in our MEG and feature matrices are not\nindependent and identically distributed, we break\nthe relationship between the MEG and feature ma-\ntrices by shifting the feature matrices by large de-\nlays (e.g. 2000 to 2500 words) and we repeat\nthe classiﬁcation using the delayed matrices. This\nsimulates chance performance more fairly than a\npermutation test because it keeps the time struc-\nture of the matrices. It was used in (Wehbe et al.,\n239\n2014) and inspired by (Chwialkowski and Gret-\nton, 2014). For every {m,ℓ}setting we can there-\nfore compute a standardized z-value by subtract-\ning the mean of the shifted classiﬁcations and di-\nviding by the standard deviation. We then com-\npute the p-value for the true classiﬁcation accu-\nracy being due to chance. Since the three p-values\nfor the three subjects for a given {m,ℓ}are inde-\npendent, we combine them using Fisher’s method\nfor independent test statistics (Fisher, 1925). The\nstatistics we obtain for every {m,ℓ}are depen-\ndent because they comprise nearby time and space\nwindows. We control the false discovery rate us-\ning (Benjamini and Yekutieli, 2001) to adjust for\nthe testing at multiple locations and time windows.\nThis method doesn’t assume any kind of indepen-\ndence or positive dependence.\n3 Results\nWe present in Fig. 5 the accuracy using all the time\nwindows and sensors. In Fig. 6 we present the\nclassiﬁcation accuracy when running the classiﬁ-\ncation at every time window exclusively. In Fig. 9\nwe present the accuracy when running the classiﬁ-\ncation using different time windows and groups of\nsensors centered at every one of the 102 locations.\nIt is important to lay down some conventions\nto understand the complex results in these plots.\nTo recap, we are trying to ﬁnd parallels between\nmodel constituents and brain processes. We use:\n•a subset of the data (for example the time\nwindow 0-100ms and all the sensors)\n•one type of feature (for example the hidden\ncontext layer from the NPLM 3g model)\nand we obtain a classiﬁcation accuracy A. If A\nis low, there is probably no relationship between\nthe feature set and the subset of data. If Ais high,\nit hints to an association between the subset of data\nand the mental process that is analogous to the fea-\nture set. For example, when using all the sensors\nand time window 0-100ms, along with the NPLM\n3g hidden layer, we obtain an accuracy of 0.70\n(higher than chance with p < 10−14, see Fig. 6).\nSince the NPLM 3g hidden layer summarizes the\ncontext of the story before seeing wordi, this sug-\ngests that the brain is still processing the context\nof the story before word ibetween 0-100ms.\nFig. 6 shows the accuracy for different types\nof features when using all of the time points and\nall the sensors to classify a word. We can see\nNPLM 3g NPLM 5g RNNLM\n0.6\n0.8\n1\nclassification accuracy \n \nhidden layer output probability embeddings\nhidden layer output probability embeddings\n0.6\n0.8\n1\nclassification accuracy \n \nNPLM 3g NPLM 5g RNNLM\nFigure 5: Average accuracy using all time windows and\nsensors, grouped by model (top) and type of feature (bot-\ntom). All accuracies are signiﬁcantly higher than chance\n(p <10−8).\n0 200 400\n0.5\n0.6\n0.7\nNPLM 3g\n0 200 400\n0.5\n0.6\n0.7\nNPLM 5g\n0 200 400\n0.5\n0.6\n0.7\nRNNLM\n \n \nhidden layer\noutput probability\nembeddings\nFigure 6: Average accuracy in different time windows\nwhen using different types of features as input to the clas-\nsiﬁer, for different models. Accuracy is plotted in the center\nof the respective time window. Points marked with a circle\nare signiﬁcantly higher than chance accuracy for the given\nfeature set and time window after correction.\nsimilar classiﬁcation accuracies for the three types\nof models, with RNNLM ahead for the hidden\nlayer and embeddings and behind for the output\nprobability features. The hidden layer features\nare the most powerful for classiﬁcation. Between\nthe three types of features, the hidden layer fea-\ntures are the best at capturing the information con-\ntained in the brain data, suggesting that most of\nthe brain activity is encoding the previous context.\nThe embedding features are the second best. Fi-\nnally the output probability have the smallest ac-\ncuracies. This makes sense considering that they\ncapture much less information than the other two\nhigh dimensional descriptive vectors, as they do\nnot represent the complex properties of the words,\nonly a numerical assessment of their likelihood.\nFig. 6 shows the accuracy when using different\nwindows of time exclusively, for the 100ms time\n240\nwindows starting at 0,100 ... 400ms after word\npresentation. We can see that using the embed-\nding vector becomes increasingly more useful for\nclassiﬁcation until 300-400ms, and then its perfor-\nmance starts decreasing. This results aligns with\nthe following hypothesis: the word is being per-\nceived and understood by the brain gradually after\nits presentation, and therefore the brain represen-\ntation of the word becomes gradually similar to the\nneural network representation of the word (i.e. the\nembedding vector). The output probability feature\naccuracy peaks at a later time than the embeddings\naccuracy. Obtaining a higher than chance accu-\nracy at time window musing the output probabil-\nity as input to the classiﬁer suggests strongly that\nthe brain is integrating the word at time window\nm, because it is responding differently for pre-\ndictable and unpredictable words 5. The integra-\ntion step happens after the perception step, which\nis probably why the output probability curves peak\nlater than the embeddings curves.\n−500 0 500 1000\n0.5\n0.6\n0.7\n0.8\nHidden Layer\n \n \nNPLM 3G\nNPLM 5G\nRNNLM\nFigure 7: Average accuracy in time for the different hidden\nlayers. The analysis is extended to the time windows before\nand after the word is presented, the input feature is restricted\nto be the hidden layer before the central word is seen. The\nﬁrst vertical bar indicates the onset of the word, the second\none indicates the end of its presentation.\n−1000 0 1000\n0.6\n0.8\nSubject 1\n−1000 0 1000\n0.6\n0.8\nSubject 2\n−1000 0 1000\n0.6\n0.8\nSubject 3\n \n \nhidden layer output probability embeddings\nFigure 8: Accuracy in time when using the RNNLM fea-\ntures for each of the three subjects.\nTo understand the time dynamics of the hidden\nlayer accuracy we need to see a larger time scale\nthan the word itself. The hidden layer captures the\n5the fact that we can classify accurately during windows\n300-400ms indicates that the classiﬁer is taking advantage of\nthe N400 response discussed in the introduction\ncontext before word iis seen. Therefore it seems\nreasonable that the hidden layer is not only related\nto the activity when the word is on the screen, but\nalso related to the activity before the word is pre-\nsented, which is the time when the brain is inte-\ngrating the previous words to build that context.\nOn the other hand, as the word i and subsequent\nwords are integrated, the context starts diverging\nfrom the context of word i(computed before see-\ning word i). We therefore ran the same analysis\nas before, but this time we also included the time\nwindows before and after word i in the analysis,\nwhile maintaining the hidden layer vector to be the\ncontext before word iis seen. We see the behav-\nior we predicted in the results: the context before\nseeing word ibecomes gradually more useful for\nclassiﬁcation until word iis seen, and then it grad-\nually decreases until it is no longer useful since\nthe context has changed. We observe the RNNLM\nhidden layer has a higher classiﬁcation accuracy\nthan the ﬁnite context NPLMs. This might be due\nto the fact that the RNNLM has a more complete\nrepresentation of context that captures more of the\nproperties of the previous words.\nTo show the consistency of the results, we plot\nas illustration the three curves we obtain for each\nsubject for the RNNLM (Fig. 8). The patterns\nseem very consistent indicating the phenomena we\ndescribed can be detected at the subject level.\nWe now move on to the spatial decomposition\nof the analysis. When the visual input enters the\nbrain, it ﬁrst reaches the visual cortex at the back\nof the head, and then moves anteriorly towards the\nleft and right temporal cortices and eventually the\nfrontal cortex. As it ﬂows through these areas, it\nis processed to higher levels of interpretations. In\nFig. 9, we plot the accuracy for different regions\nof the brain and different time windows for the\nRNNLM features. To make the plots simpler we\nmultiplied by zero the accuracies which were not\nsigniﬁcantly higher than chance. We expand a few\ncharacteristic plots. We see that in the back of the\nhead the embedding features have an accuracy that\nseems to peak very early on. As we move forward\nin the brain towards the left and right temporal cor-\ntices, we see the embeddings accuracy peaking at\na later time, reﬂecting the delay it takes for the in-\nformation to reach this part of the brain. The out-\nput probability start being useful for classiﬁcation\nafter the embeddings, and speciﬁcally in the left\ntemporal cortex which is the cite where the N400\n241\nBack%\n \n \nhidden layer\noutput probability\nembeddings 0 5000.50.60.7\ntime (s)\naccuracy\nLe(% Right%\n \n \nhidden layer\noutput probability\nembeddings 0 5000.5\n0.6\n0.7\ntime (s)\naccuracy\n \n \nhidden layer\noutput probability\nembeddings 0 5000.5\n0.6\n0.7\ntime (s)\naccuracy\n \n \nhidden layer\noutput probability\nembeddings\n0 500\n0.5\n0.6\n0.7\ntime (s)\naccuracy\n \n \nhidden layer\noutput probability\nembeddings\n0 500\n0.5\n0.6\n0.7\ntime (s)\naccuracy\n \n \nhidden layer\noutput probability\nembeddings\n0 500\n0.5\n0.6\n0.7\ntime (s)\naccuracy\nFigure 9: Average accuracy in time and space on the MEG helmet when using the RNNLM features. For each of the 102\nlocations the average accuracy for the group of sensors centered at that location is plotted versus time. The axes are deﬁned\nin the rightmost, empty plot. Three plots have been magniﬁed to show the increasing delay in high accuracy when using the\nembeddings feature, reﬂecting the delay in processing the incoming word as information travels through the brain. A sensor\nmap is provided in the lower right corner: visual cortex = cyan, temporal = red, frontal = dark green.\nis reported in the literature. Finally, as we reach\nthe frontal cortex, we see that the embeddings fea-\ntures have an even later accuracy peak.\n4 Conclusion and contributions\nNovel brain data exploration We present here\na novel and revealing approach to shed light on\nthe brain processes involved in reading. This is a\ndeparture from the classical approach of control-\nling for a few variables in the text (e.g. showing\na sentence with an expected target word versus an\nunexpected one). While we cannot make clear cut\ncausal claims because we did not control for our\nvariables, we are able to explore the data much\nmore and offer a much richer interpretation than\nis possible with artiﬁcially constrained stimuli.\nComparing two models of language Adding\nbrain data into the equation allowed us to com-\npare the performance of the models and to identify\na slight advantage for the RNNLM in capturing\nthe text contents. Numerical comparison is how-\never a secondary contribution of our approach. We\nshowed that it might be possible to use brain data\nto understand, interpret and illustrate what exactly\nis being encoded by the obscure vectors that neural\nnetworks compute, by drawing parallels between\nthe models constituents and brain processes.\nAnecdotally, in the process of running the ex-\nperiments, we noticed that the accuracy for the\nhidden layer of the RNNLM was peaking in the\ntime window corresponding to wordi−2, and that\nit was decreasing during word i−1. Since this\nwas against our expectations, we went back and\nlooked at the code and found that it was indeed\nreturning a delayed value and corrected the fea-\ntures. We therefore used the brain data in order to\ncorrect a mis-speciﬁcation in our neural network\nmodel. This hints if not proves the potential of our\napproach for assessing language models.\nFuture Work The work described here is our\nﬁrst attempt along the promising endeavor of\nmatching complex computational models of lan-\nguage with brain processes using brain recordings.\nWe plan to extend our efforts by (1) collecting data\nfrom more subjects and using various types of text\nand (2) make the brain data help us with training\nbetter statistical language models by using it to de-\ntermine whether the models are expressive enough\nor have reached a sufﬁcient degree of convergence.\nAcknowledgements\nThis research was supported in part by NICHD\ngrant 5R01HD07328-02. We thank Nicole Raﬁdi\nfor help with data acquisition.\n242\nReferences\nYoav Benjamini and Daniel Yekutieli. 2001. The con-\ntrol of the false discovery rate in multiple testing un-\nder dependency. Annals of statistics, pages 1165–\n1188.\nAugusto Buchweitz, Robert A Mason, L ˆeda Tomitch,\nand Marcel Adam Just. 2009. Brain activation\nfor reading and listening comprehension: An fMRI\nstudy of modality effects and individual differences\nin language comprehension. Psychology & neuro-\nscience, 2(2):111–123.\nKacper Chwialkowski and Arthur Gretton. 2014.\nA kernel independence test for random processes.\narXiv preprint arXiv:1402.4501.\nRonald Aylmer Fisher. 1925. Statistical methods for\nresearch workers. Genesis Publishing Pvt Ltd.\nStefan L Frank, Leun J Otten, Giulia Galli, and\nGabriella Vigliocco. 2013. Word surprisal predicts\nN400 amplitude during reading. In Proceedings of\nthe 51st annual meeting of the Association for Com-\nputational Linguistics, pages 878–883.\nAngela D Friederici. 2002. Towards a neural basis\nof auditory sentence processing. Trends in cognitive\nsciences, 6(2):78–84.\nGene H Golub, Michael Heath, and Grace Wahba.\n1979. Generalized cross-validation as a method for\nchoosing a good ridge parameter. Technometrics,\n21(2):215–223.\nPeter Hagoort. 2003. How the brain solves the binding\nproblem for language: a neurocomputational model\nof syntactic processing. Neuroimage, 20:S18–S29.\nNikolaus Kriegeskorte, Rainer Goebel, and Peter Ban-\ndettini. 2006. Information-based functional brain\nmapping. Proceedings of the National Academy\nof Sciences of the United States of America ,\n103(10):3863–3868.\nTomas Mikolov, Stefan Kombrink, Anoop Deoras,\nLukar Burget, and J Cernocky. 2011. RNNLM-\nrecurrent neural network language modeling toolkit.\nIn Proc. of the 2011 ASRU Workshop, pages 196–\n201.\nTomas Mikolov. 2012. Statistical Language Models\nBased on Neural Networks. Ph.D. thesis, Brno Uni-\nversity of Technology.\nVinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed\nlinear units improve restricted Boltzmann machines.\nIn Proceedings of ICML, pages 807–814.\nJoanne K. Rowling. 2012. Harry Potter and the Sor-\ncerer’s Stone. Harry Potter US. Pottermore Limited.\nRiitta Salmelin. 2007. Clinical neurophysiology of\nlanguage: the MEG approach. Clinical Neurophysi-\nology, 118(2):237–254.\nGustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila\nWehbe, Alona Fyshe, Riitta Salmelin, and Tom\nMitchell. 2012. Tracking neural coding of percep-\ntual and semantic features of concrete nouns. Neu-\nroImage, 62(1):451–463.\nSamu Taulu and Juha Simola. 2006. Spatiotem-\nporal signal space separation method for rejecting\nnearby interference in MEG measurements. Physics\nin medicine and biology, 51(7):1759.\nSamu Taulu, Matti Kajola, and Juha Simola. 2004.\nSuppression of interference and artifacts by the sig-\nnal space separation method. Brain topography,\n16(4):269–275.\nAshish Vaswani, Yinggong Zhao, Victoria Fossum, and\nDavid Chiang. 2013. Decoding with large-scale\nneural language models improves translation.\nLeila Wehbe, Brian Murphy, Partha Talukdar, Alona\nFyshe, Aaditya Ramdas, and Tom Mitchell. 2014.\nSimultaneously uncovering the patterns of brain re-\ngions involved in different story reading subpro-\ncesses. in press.\n243",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.738357424736023
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6611757278442383
    },
    {
      "name": "Magnetoencephalography",
      "score": 0.6608034372329712
    },
    {
      "name": "Artificial neural network",
      "score": 0.6512049436569214
    },
    {
      "name": "Word (group theory)",
      "score": 0.5992380380630493
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5656659603118896
    },
    {
      "name": "Word embedding",
      "score": 0.5423229336738586
    },
    {
      "name": "Natural language processing",
      "score": 0.5353959202766418
    },
    {
      "name": "Representation (politics)",
      "score": 0.5227732062339783
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5073511004447937
    },
    {
      "name": "Reading (process)",
      "score": 0.4713537096977234
    },
    {
      "name": "Language model",
      "score": 0.4533504843711853
    },
    {
      "name": "Context model",
      "score": 0.4123293161392212
    },
    {
      "name": "Psychology",
      "score": 0.17301824688911438
    },
    {
      "name": "Embedding",
      "score": 0.1473066806793213
    },
    {
      "name": "Linguistics",
      "score": 0.14119985699653625
    },
    {
      "name": "Electroencephalography",
      "score": 0.12131643295288086
    },
    {
      "name": "Neuroscience",
      "score": 0.10160103440284729
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ]
}