{
  "title": "The Effect Of Smoothing In Language Models For Novelty Detection",
  "url": "https://openalex.org/W2128054329",
  "year": 2007,
  "authors": [
    {
      "id": "https://openalex.org/A5058407281",
      "name": "Ronald T. Fernández",
      "affiliations": [
        "Universidade de Santiago de Compostela"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6668655984",
    "https://openalex.org/W1904228841",
    "https://openalex.org/W2131133093",
    "https://openalex.org/W6680399881",
    "https://openalex.org/W1499108793",
    "https://openalex.org/W2093390569",
    "https://openalex.org/W2136542423",
    "https://openalex.org/W1481997832",
    "https://openalex.org/W2096623622",
    "https://openalex.org/W63837240",
    "https://openalex.org/W4206765718",
    "https://openalex.org/W2138049151",
    "https://openalex.org/W4240913316",
    "https://openalex.org/W2072284402",
    "https://openalex.org/W4213175497"
  ],
  "abstract": "The novelty task consists of finding relevant and novel sentences in a ranking of documents given a query. In the literature, different techniques have been applied to address this problem. Nevertheless, little is known about Language Models for novelty detection and, especially, the effect of smoothing on the selection of novel sentences. Language Models can be used to study novelty and relevance in a principled way. These statistical models have been shown to perform well empirically in many Information Retrieval tasks. In this work we study formally the effects of smoothing on novelty detection. To this aim, we compare different techniques based on the Kullback-Leibler divergence and we analyze the sensitivity of retrieval performance to the smoothing parameters. The ability of Language Modeling estimation methods to handle quantitatively the uncertainty associated to the use of natural language is a powerful tool that can drive the future development of novelty-based mechanisms.",
  "full_text": "BC S IR SG  Sym posium : Future D irections in Inform ation Access (FD IA 2007) \nThe Effect O f Sm oothing In Language \nM odels For N ovelty D etection \nRonald T. Fernández \nDepartamento de Electrónica y Computación.  \nUniversidad de Santiago de Compostela \nCampus Sur, s/n.  \n15782 Santiago de Compostela, SPAIN \nronald.teijeira@ rai.usc.es \nAbstract: The novelty task consists of finding relevant and novel sentences in a ranking of documents given a \nquery. In the literature, different techniques have been applied to address this problem. Nevertheless, little is \nknown about Language Models for novelty detection and, especially, the effect of smoothing on the selection of \nnovel sentences. Language Models can be used to study novelty and relevance in a principled way. These \nstatistical models have been shown to perform well empirically in many Information Retrieval tasks. In this work \nwe study formally the effects of smoothing on novelty detection. To this aim, we compare different techniques \nbased on the Kullback-Leibler divergence and we analyze the sensitivity of retrieval performance to the \nsmoothing parameters. The ability of Language Modeling estimation methods to handle quantitatively the \nuncertainty associated to the use of natural language is a powerful tool that can drive the future development of \nnovelty-based mechanisms. \nKeyw ords: Language M odels, Sm oothing, N ovelty D etection, Kullback-Leibler D ivergence. \n1. IN TR O D U C TIO N  \nN ovelty detection is an im portant research topic w hose applications span a w ide range of Inform ation R etrieval (IR ) \nproblem s [6]. W e adopt here the novelty detection task as defined in the TR EC  conference [3, 8, 9]. The groups \nparticipating in this task start from  a com m on ranking of docum ents for each query. The task is decom posed into \ntw o problem s: 1) to produce a ranked set of relevant sentences (sentence retrieval stage), and 2) to filter out \nredundant sentences from  this set (novelty detection stage). The task is an effort to go beyond the typical ranked \nlist of docum ents. It explores retrieval techniques that return relevant and novel sentences (i.e. key sentences) \nrather than w hole docum ents containing extraneous or duplicate inform ation. Although the effectiveness of the first \nstage (sentence retrieval) is an im portant issue [1], w e focus here on novelty detection. In particular, w e are \ninterested in Language M odeling (LM ) techniques and the effect of sm oothing on the selection of novel sentences. \nLM  is a principled statistical fram ew ork that has proved to w ork w ell in different areas, such as Speech R ecognition, \nM achine Translation and Inform ation R etrieval (IR ). The original proposal to apply LM  for IR  w as done by Ponte \nand C roft [7] and it w as follow ed by a num ber of studies dedicated to the subject [4, 2]. Sm oothing is a core \nproblem  in LM  estim ation. It adjusts the m axim um  likelihood estim ator so as to correct the inaccuracy due to data \nsparseness. The type and level of sm oothing affects directly perform ance in docum ent retrieval [11]. H ow ever, the \nrole of sm oothing in novelty detection is largely unknow n and there are not studies reporting how  novelty \ntechniques behave w ith varying levels of sm oothing. \nIn this paper w e evaluate novelty detection applying LM s estim ated using different sm oothing techniques. W e study \nhow  the perform ance of novelty detection changes as the level of sm oothing is increased. W e com pare aggregate \nm odels, w here the set of seen sentences is m odeled by a single LM , and non-aggregate m odels, w here every seen \nsentence is handled by an individual LM . This study helps to gain insight into the role that LM  estim ation can play in \ncurrent novelty detection system s. M ost novelty techniques applied in the past (e.g. N ew  W ords, Set D ifference or \nC osine D istance [1]) are rather sim plistic and lack the estim ation pow er w hich is inherent to LM s. M oreover, m any \nof the initial novelty approaches w ere only tested w ith the TR EC  2002 novelty track collection. This is problem atic \nbecause this collection contains little redundancy. \nThe rest of this paper is organized as follow s. Section 2 review s som e papers related to our research. In section 3 \nw e briefly explain the LM  estim ation of m ethods used and Kullback-Leibler divergence. The em pirical evaluation \nconducted is reported in section 4. The paper ends w ith som e conclusions. \n2. R ELATED  W O R K  \nThe novelty detection m ethods experim ented in the context of the TR EC  novelty tracks scan the output of a \nsentence retrieval com ponent (a list of sentences ranked in decreasing order of sim ilarity to a given query) and \ndiscard the sentences that do not contain new  m aterial. Initially, the ranked list of presum ed relevant sentences is \nre-ordered in the order given by the task (sentences are considered in the sam e order in w hich the relevant \nThe Effect O f Sm oothing In Language M odels For N ovelty D etection \nBC S IR SG  Sym posium : Future D irections in Inform ation Access (FD IA 2007) \ndocum ents w ere originally ranked and m ultiple sentences from  the sam e docum ent are considered in the order in \nw hich they appear in the docum ent). The first sentence is often assigned the highest score of novelty and the \nrem aining sentences are scored in term s of som e m easure of overlapping betw een the sentence and the \npreviously seen sentences. Sim ple w ord count m easures, such as N ew  W ords, Set D ifference or C osine D istance \n[1] have been applied successfully in the past. \nSom e studies, such as the one conducted in [5], have applied LM s based on the Kullback-Leibler D ivergence \n(KLD ). KLD  is applied to m easure the divergence betw een a LM  com puted for a given sentence and a LM  \nassociated to the previously seen sentences. These LM s are m axim um  likelihood m odels sm oothed using linear \ninterpolation.  \nAnother w ork [1] evaluates novelty using different sm oothing techniques. The m odels based on D irichlet \nSm oothing, Shrinkage Sm oothing and Sentence C ore M ixture M odel apply pair-w ise com parison betw een the LM  \nof the current sentence and every LM  of the seen sentences. The m inim um  divergence (com puted applying KLD ) is \nused to estim ate the degree of novelty of the current sentence. The LM s are com puted as follow s. Shrinkage \nSm oothing and Sentence C ore M ixture M odel apply linear interpolation betw een three different LM s: a sentence \nLM , a topic LM  and a general English LM . D irichlet Sm oothing sim ply applies sm oothing over the sentence m odels \nin a (sentence) length-dependent w ay. The m odel based on Interpolate Aggregate Sm oothing uses the KLD  \nbetw een the LM  of the current sentence and a LM  constructed from  all previously seen sentences. These LM s are \nestim ated using Jelinek-M ercer sm oothing. \nN evertheless, there is no evidence about the perform ance of novelty detection w hen different levels of sm oothing \nare applied. In our w ork w e evaluate several sm oothing techniques w ith different param eter settings and exam ine \nthe sensitivity of novelty detection to the sm oothing param eters. O ur experim ental setting is com plete as w e use \nthe three TR EC  novelty track collections. This leads to a deep analysis on novelty perform ance under very different \nscenarios. \n3. LA N G U A G E M O D ELS \nA Statistical Language M odel is a probabilistic m echanism  for explaining the generation of text. It basically defines \na distribution over all possible w ord sequences. The sim plest LM  is the unigram  LM , w hich is a w ord distribution. In \nthis w ork w e em ploy unigram  LM s, w hose effectiveness for IR  tasks has been dem onstrated in the literature [11]. \nA sim ple LM  for a docum ent d is the m axim um  likelihood estim ator (m le). It associates a probability greater than \nzero for each term  w hich appears in the docum ent and a zero probability for the unseen term s. M ore specifically, \nfor each term  w , the probability Pm le(w |d) represents the relative frequency of the term  w  in d. \nThis estim ator is problem atic because assigning probabilities equal to zero to any unseen term  m ay be very strict. \nTo overcom e this problem , m les are often sm oothed using som e fallback m odel that suffers less from  sparseness \n(e.g. a m odel constructed from  a large collection of docum ents). These sm oothing techniques are explained in the \nnext section. \n3.1 Sm oothing \nSm oothing techniques try to balance the probability of term s w hich appear in a docum ent w ith those ones w hich \nare m issing. It discounts the probability m ass assigned to the seen w ords and distributes the extra probability to the \nunseen term s according to som e fallback m odel. \nJelinek-M ercer sm oothing involves a linear interpolation of the m axim um  likelihood m odel w ith the collection m odel, \nusing a coefficient . \nC)| 3\u000bZ+\nd)c(w;\nd)c(w; \f-(1=C)| 3\u000bZ+d)|(w \f3-(1=d)|P(w mle\n \nw here P(w |C ) is the m le constructed from  the set of docum ents in the collection (C ) and c(w ;d) the term  count of w  \nin d.  \nD irichlet sm oothing adjusts the am ount of reliance on the observed text according to the length of this text: \n +d)c(w;\nC)| 3\u000bZ+d)c(w;=d)|P(w  \nw here  is the sm oothing param eter. \nAs argued in [10], applying D irichlet sm oothing w ith query likelihood leads to a retrieval form ula w ith com ponents \nsim ilar to the tf-idf w eights and a docum ent length correction. \n \n \nThe Effect O f Sm oothing In Language M odels For N ovelty D etection \nBC S IR SG  Sym posium : Future D irections in Inform ation Access (FD IA 2007) \n3.2 K LD  \nKLD  m easures the divergence betw een tw o probability distributions. It can be used as a “distance” * between LMs. \nKLD is always positive and bigger than zero. \n)d|P(w\n)d|P(wlog)d|P(w=)d||KLD(d\n2\n1\nw\n121  \nThe two smoothing techniques explained above and KLD have been jointly applied to model novelty. The \nsubsequent experiments are reported in the next section. \n4. EXPER IM EN TS \nWe used the three collections of data which were made available in the context of the TREC Novelty tracks in \n2002, 2003 and 2004 [3, 8, 9]. In 2002 and 2003 the ranking of documents given a query provided by NIST \nconsists only of relevant documents. In 2004 the collection is more realistic because the ranked set of documents \ncontains both relevant and non-relevant documents.  \nGiven the ranked list of documents, the groups had to locate the sentences in the documents that are relevant and \nnovel. Each document was also automatically split into sentences at NIST and sentences were assigned identifiers. \nTo study novelty detection properly we need first to rank sentences in decreasing order of presumed relevance \n(first stage: sentence retrieval). To get this initial ranking we applied a variation of tf-idf that has shown to be very \neffective and robust in the past [1]: \ntt\nt,q +sf5.0\n1n+log)1+(tflogR(s|q)=  \nwhere tft,q is the number of times term t occurs in the query, tft,s is the number of times term t occurs in the \nsentence, sft is the number of sentences in which term t appears, and n is the number of sentences in the collection \nbeing scored. \nGiven this relevance ranking, we first experimented with two different baselines to detect novelty. The first baseline \n(named BNN – Baseline with No Novelty detection) ranks sentences using directly its relevance score. Sentences \nwith higher tf-idf scores are placed in top positions in the ranking. This means that the novelty-oriented ranking of \nsentences is exactly the same as the relevance ranking (i.e. the highest the tf-idf similarity between the sentence \nand the query, the more novel the sentence is assumed to be). The second baseline (named BDOC – Baseline \nordered by DOCument) consists of a reordering of the sentences from the relevance ranking. The sentences are \nconsidered in the same order in which the documents were originally ranked by NIST and multiple sentences from \nthe same document are considered in the order in which they appear in the document. These baselines have been \nused in the past [1, 6] but there is not any comparative study which evaluates its relative merits for novelty \ndetection. \nIn our experiments, BDOC performed clearly better than BNN. This is not surprising because the assessors that \nproduced the sentence-level novelty judgments followed the same re-ordering policy taken in BDOC (i.e. they first \nidentified the relevant sentences and, next, these sentences are considered in the order given by the task). We \ntherefore set the BDOC baseline to be the reference baseline in our experiments. \nWe experimented with two different LM techniques for novelty detection. The first one, called NAM (Non-Aggregate \nModel) generates a smoothed LM for each sentence and computes the KLD between the LM of the current \nsentence si and the LM of every seen sentence sj (where j=0,… ,i-1). The minimum KLD between the sentence si \nand the seen sentences sj is used as the novelty score for the sentence si. The second technique, the AM \n(Aggregate Model), generates only two LMs: a smoothed LM for the current sentence (si) and another LM for the \nset of seen sentences sj (where j=0,… ,i-1). The KLD between these two models is used to measure the degree of \nnovelty of the sentence si. \nWe applied two different smoothing techniques to compute the LMs: Jelinek-Mercer and Dirichlet. \nTo study performance we computed precision at 10 sentences (P@10) and precision at 30 sentences (P@30). For \nthe sake of brevity, we only analyze here the P@30 ratios (P@10 results showed similar trends but there was not \nsignificant differences between the baseline and the LM runs). \nWe applied these techniques for both short (title) and long (verbose) queries but trends were similar for both cases \n(we report here the results for short queries). \nFigures 1 to 3 show results for NAM and AM applying Dirichlet and Jelinek-Mercer smoothing on the three \ncollections. \n                                                           \n* Note that it is not symmetric and does not satisfy the triangle inequality. \nThe Effect O f Sm oothing In Language M odels For N ovelty D etection \nBC S IR SG  Sym posium : Future D irections in Inform ation Access (FD IA 2007) \n \nFIG U R E 1: D irichlet and Jelinek-M ercer Sm oothing results applied over the TR EC  2002. \n \n \nFIG U R E 2: D irichlet and Jelinek-M ercer Sm oothing results applied over the TR EC  2003. \n \nFIG U R E 3: Jelinek-M ercer Sm oothing results applied over the TR EC  2004. \n \nIn TR EC  2002, precision at 30 is significantly w orse than in the other tw o collections; there are few  relevant \nsentences in TR EC  (§2% ) and, therefore, the initial sentence retrieval stage w orks poorly. In fact, the results \nachieved by the groups participating in the TR EC  2002 novelty track are sim ilar to ours. \nThe experim ents reported show  som e interesting trends. O n one hand, D irichlet sm oothing perform s better than \nJelinek-M ercer. In docum ent retrieval this w as also the case as indicated in [11]. D irichlet sm oothing looks slightly \nThe Effect O f Sm oothing In Language M odels For N ovelty D etection \nBC S IR SG  Sym posium : Future D irections in Inform ation Access (FD IA 2007) \nm ore sensitive to the param eter settings than Jelinek-M ercer but, still, nearly all D irichlet runs im proved over any \nJelinek-M ercer run. \nO n the other hand, N AM  perform s alw ays better than AM . N AM  generates an individual LM  for each sentence and, \ngiven a sentence si, its degree of novelty is estim ated from  the previously seen sentence having the sm allest \ndivergence. O n the other hand, AM  considers the history of seen sentences as a w hole. This seem s to be harm ing. \nA LM  for the set of seen sentences m ight be too general. C onsider a sentence w hich is an exact repetition of a past \nsentence. In N AM  the sentence w ould receive the low est novelty score. In AM , this is not guaranteed. The larger \nthe history is, the less im portant the term s of the sentence are in the LM  of the seen sentences. Therefore, it is still \npossible that the sentence is classified as novel. N ote also that AM  perform s w orse as sm oothing increases. This is \nquite natural because, as w e m ake the LM  m ore general (w e give m ore im portance to term s in the collection), \nterm s seen in the history receive increasingly less im portance.  \nIn TR EC  2002 the baseline is very com petitive and no LM  run w as able to im prove over the baseline. This is \nnaturally explained by the population of novel sentences in this collection. M ore than 90%  of the relevant sentences \nw ere judged as novel by the TR EC  2002 assessors. This m eans that a basic re-ordering of a relevance ranking \n(BD O C ) is enough and no additional novelty oriented adjustm ents are needed. In contrast, the other tw o collections \nhave m uch m ore redundancy and, therefore, the LM  approaches im prove over the baseline. \nIn all N AM  cases, the optim um  perform ance of D irichlet sm oothing tends to be found w hen the sm oothing \nSDUDPHWHU\u0003\u000b \f\u0003is around 1000. The best precision w ith Jelinek-M ercer sm oothing is reached w hen  is sm all (0.01).  \nThese results are prelim inary and w e still need to conduct further analysis on the different trends found. At the \nm om ent, AM  does not seem  a good technique to m odel the history of seen sentences because it can potentially \n“hide” the redundant pieces of texts into a global LM where many terms have non-marginal probabilities. In \ncontrast, NAM looks quite effective but we still need to design new experiments and comparisons against other \nmodels. \n5. C O N C LU SIO N S \nIn this paper we studied novelty detection at the sentence level using smoothed LMs and KLD. We focused our \nwork in studying the performance of applying different smoothing techniques (Dirichlet smoothing and Jelinek-\nMercer smoothing) with varying parameter setting. \nTo study novelty we applied two different techniques to build the LMs (NAM – Non-Aggregate Model – and AM – \nAggregate Model) and used KLD to estimate the divergence between such models. We observed that NAM \nperforms better than AM. We also showed that Dirichlet smoothing is a better estimation method than Jelinek-\nMercer for novelty detection purposes. This observation agrees with the results shown in [11] for document \nretrieval. Dirichlet smoothing looks therefore a very robust smoothing method that works properly in different IR \ntasks. The comparison reported here is more exhaustive than other reports published in the literature because we \nhave used the three TREC novelty collections available. \n6. A C K N O W LED G EM EN TS \nMy thanks to my PhD. advisor, Dr. David E. Losada, who supervised this research. This work was partially \nsupported by projects TIN2005-08521-C02-01 and PGIDIT06PXIC206023PN; and the Galician network 2006/23. \nREFERENCES \n[1] J. Allan, C. Wade, and A. Bolivar (2003). Retrieval and novelty detection at the sentence level. Proceedings of \nthe 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval \n(SIGIR 2003), pages 314-321. \n[2] W. B. Croft and J. Lafferty (Eds.) (2003). Language Modeling for Information Retrieval. Kluwer, 2003. \n[3] D. Harman (2002). Overview of the TREC 2002 Novelty Track. Proceedings of the 11th Text Retrieval \nConference (TREC 2002), Gaithersburg, MD. \n[4] D. Hiemstra (2001). Using Language Models for Information Retrieval. Ph.D. Thesis, Centre for Telematics and \nInformation Technology, University of Twente, January 2001. \n[5] L. S. Larkey, J. Allan, M. E. Connell, A. Bolivar and C. Wade (2002). UMass at TREC 2002: Cross Language \nand Novelty Tracks. \n[6] X. Li (2006). Sentence Level Information Patterns for Novelty Detection. Ph.D. Thesis, Department of Computer \nScience, University of Massachusetts at Amherst, September 2006. \n[7] J. Ponte and W. B. Croft (2002). A language modeling approach to information retrieval. Proceedings of the 21st \nAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 1998), \npages 275-281, 1998. \n[8] I. Soboroff (2004) Overview of the TREC 2004 Novelty Track. Proceedings of the 13th Text REtrieval Conference \n(TREC 2004) , Gaithersburg, MD. \nThe Effect O f Sm oothing In Language M odels For N ovelty D etection \nBC S IR SG  Sym posium : Future D irections in Inform ation Access (FD IA 2007) \n[9] I. Soboroff and D . H arm an (2003). O verview  of the TR EC  2003 N ovelty Track. Proceedings of the 12th Text \nR Etrieval C onference (TR EC  2003) , G aithersburg, M D . \n[10] C . X. Zhai (2002). R isk M inim ization and Language M odeling in Text R etrieval. Ph.D . Thesis, Language \nTechnologies Institute, C arnegie M ellon U niversity, July 2002. \n[11] C . X. Zhai and J. Lafferty (2001). A Study of Sm oothing M ethods for Language M odels Applied to Ad H oc \nInform ation R etrieval. Proceedings of the 24th Annual International AC M  SIG IR  C onference on R esearch and \nD evelopm ent in Inform ation R etrieval (SIG IR  2001), pages 334-342. ",
  "topic": "Novelty",
  "concepts": [
    {
      "name": "Novelty",
      "score": 0.9034327268600464
    },
    {
      "name": "Smoothing",
      "score": 0.877996563911438
    },
    {
      "name": "Computer science",
      "score": 0.7881392240524292
    },
    {
      "name": "Novelty detection",
      "score": 0.7811521291732788
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.7124029994010925
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6261793375015259
    },
    {
      "name": "Language model",
      "score": 0.6149112582206726
    },
    {
      "name": "Divergence (linguistics)",
      "score": 0.5652954578399658
    },
    {
      "name": "Relevance (law)",
      "score": 0.5641967058181763
    },
    {
      "name": "Machine learning",
      "score": 0.5615931153297424
    },
    {
      "name": "Task (project management)",
      "score": 0.5237441062927246
    },
    {
      "name": "Natural language processing",
      "score": 0.42799875140190125
    },
    {
      "name": "Data mining",
      "score": 0.3392420709133148
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Theology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200284239",
      "name": "Universidade de Santiago de Compostela",
      "country": "ES"
    }
  ]
}