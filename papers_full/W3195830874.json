{
    "title": "Do Vision Transformers See Like Convolutional Neural Networks?",
    "url": "https://openalex.org/W3195830874",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4222115527",
            "name": "Raghu, Maithra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222167770",
            "name": "Unterthiner, Thomas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222115528",
            "name": "Kornblith, Simon",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2358990535",
            "name": "Zhang Chiyuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2748553518",
            "name": "Dosovitskiy, Alexey",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2980287048",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W3004895274",
        "https://openalex.org/W3033210410",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W3163465952",
        "https://openalex.org/W2942810103",
        "https://openalex.org/W2160654481",
        "https://openalex.org/W2995049146",
        "https://openalex.org/W3145450063",
        "https://openalex.org/W3095872829",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2947706148",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2983829373",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2112552549",
        "https://openalex.org/W2937843571",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W2950500611",
        "https://openalex.org/W3015146382",
        "https://openalex.org/W2949846184",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2971278662",
        "https://openalex.org/W2971531230",
        "https://openalex.org/W3170227631",
        "https://openalex.org/W3145185940",
        "https://openalex.org/W3157506437",
        "https://openalex.org/W1920328734",
        "https://openalex.org/W2972249811",
        "https://openalex.org/W2127658298",
        "https://openalex.org/W3022717752",
        "https://openalex.org/W3162276117",
        "https://openalex.org/W3034487470",
        "https://openalex.org/W2799124508",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3161120562",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W3095592098",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3126004537",
        "https://openalex.org/W2971293928",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963759780",
        "https://openalex.org/W2952502547",
        "https://openalex.org/W2108598243"
    ],
    "abstract": "Convolutional neural networks (CNNs) have so far been the de-facto model for visual data. Recent work has shown that (Vision) Transformer models (ViT) can achieve comparable or even superior performance on image classification tasks. This raises a central question: how are Vision Transformers solving these tasks? Are they acting like convolutional networks, or learning entirely different visual representations? Analyzing the internal representation structure of ViTs and CNNs on image classification benchmarks, we find striking differences between the two architectures, such as ViT having more uniform representations across all layers. We explore how these differences arise, finding crucial roles played by self-attention, which enables early aggregation of global information, and ViT residual connections, which strongly propagate features from lower to higher layers. We study the ramifications for spatial localization, demonstrating ViTs successfully preserve input spatial information, with noticeable effects from different classification methods. Finally, we study the effect of (pretraining) dataset scale on intermediate features and transfer learning, and conclude with a discussion on connections to new architectures such as the MLP-Mixer.",
    "full_text": "Do Vision Transformers See Like Convolutional\nNeural Networks?\nMaithra Raghu\nGoogle Research, Brain Team\nmaithrar@gmail.com\nThomas Unterthiner\nGoogle Research, Brain Team\nunterthiner@google.com\nSimon Kornblith\nGoogle Research, Brain Team\nkornblith@google.com\nChiyuan Zhang\nGoogle Research, Brain Team\nchiyuan@google.com\nAlexey Dosovitskiy\nGoogle Research, Brain Team\nadosovitskiy@google.com\nAbstract\nConvolutional neural networks (CNNs) have so far been the de-facto model for\nvisual data. Recent work has shown that (Vision) Transformer models (ViT) can\nachieve comparable or even superior performance on image classiﬁcation tasks.\nThis raises a central question: how are Vision Transformers solving these tasks?\nAre they acting like convolutional networks, or learning entirely different visual\nrepresentations? Analyzing the internal representation structure of ViTs and CNNs\non image classiﬁcation benchmarks, we ﬁnd striking differences between the two ar-\nchitectures, such as ViT having more uniform representations across all layers. We\nexplore how these differences arise, ﬁnding crucial roles played by self-attention,\nwhich enables early aggregation of global information, and ViT residual connec-\ntions, which strongly propagate features from lower to higher layers. We study\nthe ramiﬁcations for spatial localization, demonstrating ViTs successfully preserve\ninput spatial information, with noticeable effects from different classiﬁcation meth-\nods. Finally, we study the effect of (pretraining) dataset scale on intermediate\nfeatures and transfer learning, and conclude with a discussion on connections to\nnew architectures such as the MLP-Mixer.\n1 Introduction\nOver the past several years, the successes of deep learning on visual tasks has critically relied on\nconvolutional neural networks [20, 16]. This is largely due to the powerful inductive bias of spatial\nequivariance encoded by convolutional layers, which have been key to learning general purpose\nvisual representations for easy transfer and strong performance. Remarkably however, recent work\nhas demonstrated that Transformer neural networks are capable of equal or superior performance\non image classiﬁcation tasks at large scale [14]. These Vision Transformers (ViT) operate almost\nidentically to Transformers used in language [13], using self-attention, rather than convolution, to\naggregate information across locations. This is in contrast with a large body of prior work, which has\nfocused on more explicitly incorporating image-speciﬁc inductive biases [30, 9, 4]\nThis breakthrough highlights a fundamental question: how are Vision Transformers solving these\nimage based tasks? Do they act like convolutions, learning the same inductive biases from scratch?\nOr are they developing novel task representations? What is the role of scale in learning these\nrepresentations? And are there ramiﬁcations for downstream tasks? In this paper, we study these\nquestions, uncovering key representational differences between ViTs and CNNs, the ways in which\nthese difference arise, and effects on classiﬁcation and transfer learning. Speciﬁcally, our contributions\nare:\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2108.08810v2  [cs.CV]  3 Mar 2022\n• We investigate the internal representation structure of ViTs and CNNs, ﬁnding striking differences\nbetween the two models, such as ViT having more uniform representations, with greater similarity\nbetween lower and higher layers.\n• Analyzing how local/global spatial information is utilised, we ﬁnd ViT incorporates more global\ninformation than ResNet at lower layers, leading to quantitatively different features.\n• Nevertheless, we ﬁnd that incorporating local information at lower layers remains vital, with\nlarge-scale pre-training data helping early attention layers learn to do this\n• We study the uniform internal structure of ViT, ﬁnding that skip connections in ViT are even more\ninﬂuential than in ResNets, having strong effects on performance and representation similarity.\n• Motivated by potential future uses in object detection, we examine how well input spatial informa-\ntion is preserved, ﬁnding connections between spatial localization and methods of classiﬁcation.\n• We study the effects of dataset scale on transfer learning, with a linear probes study revealing its\nimportance for high quality intermediate representations.\n2 Related Work\nDeveloping non-convolutional neural networks to tackle computer vision tasks, particularly Trans-\nformer neural networks [44] has been an active area of research. Prior works have looked at local\nmultiheaded self-attention, drawing from the structure of convolutional receptive ﬁelds [ 30, 36],\ndirectly combining CNNs with self-attention [ 4, 2, 46] or applying Transformers to smaller-size\nimages [6, 9]. In comparison to these, the Vision Transformer [14] performs even less modiﬁcation\nto the Transformer architecture, making it especially interesting to compare to CNNs. Since its\ndevelopment, there has also been very recent work analyzing aspects of ViT, particularly robustness\n[3, 31, 28] and effects of self-supervision [5, 7]. Other recent related work has looked at designing\nhybrid ViT-CNN models [49, 11], drawing on structural differences between the models. Comparison\nbetween Transformers and CNNs are also recently studied in the text domain [41].\nOur work focuses on the representational structure of ViTs. To study ViT representations, we draw on\ntechniques from neural network representation similarity, which allow the quantitative comparisons\nof representations within and across neural networks [17, 34, 26, 19]. These techniques have been\nvery successful in providing insights on properties of different vision architectures [ 29, 22, 18],\nrepresentation structure in language models [48, 25, 47, 21], dynamics of training methods [33, 24]\nand domain speciﬁc model behavior [27, 35, 38]. We also apply linear probes in our study, which has\nbeen shown to be useful to analyze the learned representations in both vision [1] and text [8, 32, 45]\nmodels.\n3 Background and Experimental Setup\nOur goal is to understand whether there are differences in the way ViTs represent and solve image\ntasks compared to CNNs. Based on the results of Dosovitskiy et al. [14], we take a representative\nset of CNN and ViT models — ResNet50x1, ResNet152x2, ViT-B/32, ViT-B/16, ViT-L/16 and\nViT-H/14. Unless otherwise speciﬁed, models are trained on the JFT-300M dataset [40], although we\nalso investigate models trained on the ImageNet ILSVRC 2012 dataset [12, 37] and standard transfer\nlearning benchmarks [50, 14]. We use a variety of analysis methods to study the layer representations\nof these models, gaining many insights into how these models function. We provide further details of\nthe experimental setting in Appendix A.\nRepresentation Similarity and CKA (Centered Kernel Alignment) : Analyzing (hidden) layer\nrepresentations of neural networks is challenging because their features are distributed across a large\nnumber of neurons. This distributed aspect also makes it difﬁcult to meaningfully compare represen-\ntations across neural networks. Centered kernel alignment (CKA) [17, 10] addresses these challenges,\nenabling quantitative comparisons of representations within and across networks. Speciﬁcally, CKA\ntakes as input X ∈Rm×p1 and Y ∈Rm×p2 which are representations (activation matrices), of two\nlayers, with p1 and p2 neurons respectively, evaluated on the samem examples. Letting K = XXT\nand L = Y YT denote the Gram matrices for the two layers (which measures the similarity of a pair\nof datapoints according to layer representations) CKA computes:\nCKA(K, L) = HSIC(K, L)√\nHSIC(K, K)HSIC(L, L)\n, (1)\n2\n0 20 40 60 80 100 120 140\nLayers ViT-L/16\n0\n20\n40\n60\n80\n100\n120\n140Layers ViT-L/16\nViT-L/16\n0 25 50 75 100 125 150 175\nLayers ViT-H/14\n0\n25\n50\n75\n100\n125\n150\n175Layers ViT-H/14\nViT-H/14\n0 20 40 60 80 100 120\nLayers R50\n0\n20\n40\n60\n80\n100\n120Layers R50\nR50\n0 50 100 150 200 250 300 350\nLayers R152\n0\n50\n100\n150\n200\n250\n300\n350Layers R152\nR152\nFigure 1: Representation structure of ViTs and convolutional networks show signiﬁcant differences, with\nViTs having highly similar representations throughout the model, while the ResNet models show much\nlower similarity between lower and higher layers.We plot CKA similarities between all pairs of layers across\ndifferent model architectures. The results are shown as a heatmap, with the x and y axes indexing the layers from\ninput to output. We observe that ViTs have relatively uniform layer similarity structure, with a clear grid-like\npattern and large similarity between lower and higher layers. By contrast, the ResNet models show clear stages\nin similarity structure, with smaller similarity scores between lower and higher layers.\n0 20 40 60 80 100 120\nLayers R50\n0\n20\n40\n60\n80\n100\n120\n140Layers ViT-L/16\nViT-L/16 vs R50\n0 20 40 60 80 100 120\nLayers R50\n0\n25\n50\n75\n100\n125\n150\n175Layers ViT-H/14\nViT-H/14 vs R50\nFigure 2: Cross model CKA heatmap\nbetween ViT and ResNet illustrate that\na larger number of lower layers in the\nResNet are similar to a smaller set of the\nlowest ViT layers. We compute a CKA\nheatmap comparing all layers of ViT to all\nlayers of ResNet, for two different ViT mod-\nels. We observe that the lower half of ResNet\nlayers are similar to around the lowest quar-\nter of ViT layers. The remaining half of the\nResNet is similar to approximately the next\nthird of ViT layers, with the highest ViT layers\ndissimilar to lower and higher ResNet layers.\nwhere HSIC is the Hilbert-Schmidt independence criterion [ 15]. Given the centering matrix\nH = In −1\nn 11T and the centered Gram matrices K′= HKH and L′= HLH, HSIC(K, L) =\nvec(K′) ·vec(L′)/(m −1)2, the similarity between these centered Gram matrices. CKA is invariant\nto orthogonal transformation of representations (including permutation of neurons), and the normal-\nization term ensures invariance to isotropic scaling. These properties enable meaningful comparison\nand analysis of neural network hidden representations. To work at scale with our models and tasks,\nwe approximate the unbiased estimator of HSIC [39] using minibatches, as suggested in [29].\n4 Representation Structure of ViTs and Convolutional Networks\nWe begin our investigation by using CKA to study the internal representation structure of each\nmodel. How are representations propagated within the two architectures, and are there signs of\nfunctional differences? To answer these questions, we take every pair of layers X, Y within a\nmodel and compute their CKA similarity. Note that we take representations not only from outputs of\nViT/ResNet blocks, but also from intermediate layers, such as normalization layers and the hidden\nactivations inside a ViT MLP. Figure 1 shows the results as a heatmap, for multiple ViTs and ResNets.\nWe observe clear differences between the internal representation structure between the two model\narchitectures: (1) ViTs show a much more uniform similarity structure, with a clear grid like structure\n(2) lower and higher layers in ViT show much greater similarity than in the ResNet, where similarity\nis divided into different (lower/higher) stages.\nWe also perform cross-model comparisons, where we take all layers X from ViT and compare to all\nlayers Y from ResNet. We observe (Figure 2) that the lower half of 60 ResNet layers are similar to\napproximately the lowest quarter of ViT layers. In particular, many more lower layers in the ResNet\nare needed to compute similar representations to the lower layers of ViT. The top half of the ResNet is\napproximately similar to the next third of the ViT layers. The ﬁnal third of ViT layers is less similar to\n3\n0 2 4 6 8 10 12 14\nSorted Attention Head\n0\n20\n40\n60\n80\n100\n120Mean Distance\nViT-L/16 JFT to ImageNet\nencoder_block0\nencoder_block1\nencoder_block22\nencoder_block23\n0 2 4 6 8 10 12 14\nSorted Attention Head\n0\n20\n40\n60\n80\n100\n120Mean Distance\nViT-H/14 JFT to ImageNet\nencoder_block0\nencoder_block1\nencoder_block30\nencoder_block31\nFigure 3: Plotting attention head mean distances shows lower ViT layers attend both locally and globally,\nwhile higher layers primarily incorporate global information.For each attention head, we compute the pixel\ndistance it attends to, weighted by the attention weights, and then average over 5000 datapoints to get an average\nattention head distance. We plot the heads sorted by their average attention distance for the two lowest and two\nhighest layers in the ViT, observing that the lower layers attend both locally and globally, while the higher layers\nattend entirely globally.\nall ResNet layers, likely because this set of layers mainly manipulates the CLS token representation,\nfurther studied in Section 6.\nTaken together, these results suggest that (i) ViT lower layers compute representations in a different\nway to lower layers in the ResNet, (ii) ViT also more strongly propagates representations between\nlower and higher layers (iii) the highest layers of ViT have quite different representations to ResNet.\n5 Local and Global Information in Layer Representations\nIn the previous section, we observed much greater similarity between lower and higher layers in\nViT, and we also saw that ResNet required more lower layers to compute similar representations to a\nsmaller set of ViT lower layers. In this section, we explore one possible reason for this difference:\nthe difference in the ability to incorporate global information between the two models. How much\nglobal information is aggregated by early self-attention layers in ViT? Are there noticeable resulting\ndifferences to the features of CNNs, which have ﬁxed, local receptive ﬁelds in early layers? In\nstudying these questions, we demonstrate the inﬂuence of global representations and a surprising\nconnection between scale and self-attention distances.\nAnalyzing Attention Distances: We start by analyzing ViT self-attention layers, which are the\nmechanism for ViT to aggregate information from other spatial locations, and structurally very\ndifferent to the ﬁxed receptive ﬁeld sizes of CNNs. Each self-attention layer comprises multiple\nself-attention heads, and for each head we can compute the average distance between the query patch\nposition and the locations it attends to. This reveals how much local vs global information each\nself-attention layer is aggregating for the representation. Speciﬁcally, we weight the pixel distances by\nthe attention weights for each attention head and average over 5000 datapoints, with results shown in\nFigure 3. In agreement with Dosovitskiy et al. [14], we observe that even in the lowest layers of ViT,\nself-attention layers have a mix of local heads (small distances) and global heads (large distances).\nThis is in contrast to CNNs, which are hardcoded to attend only locally in the lower layers. At higher\nlayers, all self-attention heads are global.\nInterestingly, we see a clear effect of scale on attention. In Figure 4, we look at attention distances\nwhen training only on ImageNet (no large-scale pre-training), which leads to much lower performance\nin ViT-L/16 and ViT-H/14 [14]. Comparing to Figure 3, we see that with not enough data, ViT does\nnot learn to attend locally in earlier layers. Together, this suggests that using local information early\non for image tasks (which is hardcoded into CNN architectures) is important for strong performance.\nDoes access to global information result in different features?The results of Figure 3 demonstrate\nthat ViTs have access to more global information than CNNs in their lower layers. But does this\nresult in different learned features? As an interventional test, we take subsets of the ViT attention\nheads from the ﬁrst encoder block, ranging from the subset corresponding to the most local attention\n4\n0 2 4 6 8 10 12 14\nSorted Attention Head\n0\n20\n40\n60\n80\n100\n120Mean Distance\nViT-L/16 on ImageNet\nencoder_block0\nencoder_block1\nencoder_block22\nencoder_block23\n0 2 4 6 8 10 12 14\nSorted Attention Head\n0\n20\n40\n60\n80\n100\n120Mean Distance\nViT-H/14 on ImageNet\nencoder_block0\nencoder_block1\nencoder_block30\nencoder_block31\nFigure 4: With less training data, lower attention layers do not learn to attend locally. Comparing the\nresults to Figure 3, we see that training only on ImageNet leads to the lower layers not learning to attend more\nlocally. These models also perform much worse when only trained on ImageNet, suggesting that incorporating\nlocal features (which is hardcoded into CNNs) may be important for strong performance. (See also Figure C.5.)\n40 50 60 70 80 90 100\nMean Distance\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60CKA Similarity\nLocal and Global ViT-L/16 \n Representations compared to R50\nblock1unit1\nblock1unit2\nblock1unit3\nblock2unit1\nblock2unit2\n20 40 60 80 100\nMean Distance\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60CKA Similarity\nLocal and Global ViT-H/14 \n Representations compared to R50\nblock1unit1\nblock1unit2\nblock1unit3\nblock2unit1\nblock2unit2\n40 50 60 70 80 90 100\nMean Distance\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50CKA Similarity\nLocal and Global ViT-L/16 \n Representations compared to R152\nblock1unit1\nblock1unit2\nblock1unit3\nblock2unit1\nblock2unit2\nFigure 5: Lower layer representations of ResNet are most similar to representations corresponding to\nlocal attention heads of ViT.We take subsets of ViT attention heads in the ﬁrst encoder block, ranging from\nthe most locally attending heads (smallest mean distance) to the most global heads (largest mean distance). We\nthen compute CKA similarity between these subsets and lower layer representations in the ResNet. We observe\nthat lower ResNet layers are most similar to the features learned by local attention heads of ViT, and decrease\nmonotonically in similarity as more global information is incorporated, demonstrating that the global heads learn\nquantitatively different features.\nheads to a subset of the representation corresponding to the most global attention heads. We then\ncompute CKA similarity between these subsets and the lower layer representations of ResNet.\nThe results, shown in Figure 5, which plot the mean distance for each subset against CKA similarity,\nclearly show a monotonic decrease in similarity as mean attention distance grows, demonstrating that\naccess to more global information also leads to quantitatively different features than computed by the\nlocal receptive ﬁelds in the lower layers of the ResNet.\nEffective Receptive Fields: We conclude by computing effective receptive ﬁelds [23] for both\nResNets and ViTs, with results in Figure 6 and Appendix C. We observe that lower layer effective\nreceptive ﬁelds for ViT are indeed larger than in ResNets, and while ResNet effective receptive\nﬁelds grow gradually, ViT receptive ﬁelds become much more global midway through the network.\nViT receptive ﬁelds also show strong dependence on their center patch due to their strong residual\nconnections, studied in the next section. As we show in Appendix C, in attention sublayers, receptive\nﬁelds taken before the residual connection show far less dependence on this central patch.\n6 Representation Propagation through Skip Connections\nThe results of the previous section demonstrate that ViTs learn different representations to ResNets in\nlower layers due to access to global information, which explains some of the differences in represen-\n5\nAttention 1\n Attention 3\n Attention 6\n Attention 9\n Attention 12\nViT-B/32\nInitial Conv\n Block 3\n Block 7\n Block 12\n Block 16\nResNet-50\nFigure 6: ResNet effective re-\nceptive ﬁelds are highly local\nand grow gradually; ViT effec-\ntive receptive ﬁelds shift from\nlocal to global . We measure\nthe effective receptive ﬁeld of\ndifferent layers as the absolute\nvalue of the gradient of the cen-\nter location of the feature map\n(taken after residual connections)\nwith respect to the input. Results\nare averaged across all channels\nin each map for 32 randomly-\nselected images.\n0 2 4 6 8\nToken index\n0\n2\n4\n6\n8\n10Block Index\nRatio of Norms (first few tokens)\n2\n4\n6\n8\n10\n12\n0 2 4 6 8 10 12 14\nBlock Index\n0\n2\n4\n6\n8\n10Average norm of representation\nRatio of Norms (aggregated)\nSelf-Attention\nMLP\nSelf-Attention CLS\nMLP CLS\nResNet50\nFigure 7: Most information in ViT passes through skip connections. Comparison of representation norms\nbetween the skip-connection (identity) and the long branch for ViT-B/16 trained on ImageNet and a ResNet. For\nViT, we show the CLS token separately from the rest of the representation. (left) shows the ratios separated for\nthe ﬁrst few tokens (token 0 is CLS), (right) shows averages over all tokens.\ntation structure observed in Section 4. However, the highly uniform nature of ViT representations\n(Figure 1) also suggests lower representations are faithfully propagated to higher layers. But how does\nthis happen? In this section, we explore the role of skip connections in representation propagation\nacross ViTs and ResNets, discovering ViT skip connections are highly inﬂuential, with a clear phase\ntransition from preserving the CLS (class) token representation (in lower layers) to spatial token\nrepresentations (in higher layers).\nLike Transformers, ViTs contain skip (aka identity or shortcut) connections throughout, which are\nadded on after the (i) self-attention layer, and (ii) MLP layer. To study their effect, we plot the\nnorm ratio ||zi||/||f(zi)||where zi is the hidden representation of the ith layer coming from the skip\nconnection, and f(zi) is the transformation of zi from the long branch (i.e. MLP or self-attention.)\nThe results are in Figure 7 (with additional cosine similarity analysis in Figure E.2.) The heatmap\non the left shows ||zi||/||f(zi)||for different token representations. We observe a striking phase\ntransition: in the ﬁrst half of the network, the CLS token (token 0) representation is primarily\npropagated by the skip connection branch (high norm ratio), while the spatial token representations\nhave a large contribution coming from the long branch (lower norm ratio). Strikingly, in the second\nhalf of the network, this is reversed.\nThe right pane, which has line plots of these norm ratios across ResNet50, the ViT CLS token and\nthe ViT spatial tokens additionally demonstrates that skip connection is much more inﬂuential in ViT\ncompared to ResNet: we observe much higher norm ratios for ViT throughout, along with the phase\ntransition from CLS to spatial token propagation (shown for the MLP and self-attention layers.)\nViT Representation Structure without Skip Connections: The norm ratio results strongly suggest\nthat skip connections play a key role in the representational structure of ViT. To test this intervention-\nally, we train ViT models with skip connections removed in blocki for varying i, and plot the CKA\nrepresentation heatmap. The results, in Figure 8, illustrate that removing the skip connections in a\nblock partitions the layer representations on either side. (We note a performance drop of 4% when\n6\n0 10 20 30 40 50\n0\n10\n20\n30\n40\n50layer\nRemove Skip-Connection at Block 1\n0 10 20 30 40 50\nRemove Skip-Connection at Block 7\n0 10 20 30 40 50\nRemove Skip-Connection at Block 10\nFigure 8: ViT models trained without any skip connections in block i show very little representation\nsimilarity between layers before/after block i. We train several ViT models without any skip connections at\nblock i for varying i to interventionally test the effect on representation structure. For middle blocks without\nskip connections, we observe a performance drop of 4%. We also observe that removing a skip connection at\nblock i partitions similar representations to before/after block i — this demonstrates the importance of skip\nconnections in ViT’s standard uniform representation structure.\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(0, 0)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(2, 1)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 2)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 4)\nViT-B/32 final block\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken0 Location(0, 0)\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken31 Location(2, 3)\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken35 Location(2, 7)\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken61 Location(4, 5)\nViT-L/16 final block\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(0, 0)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(2, 1)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 2)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 4)\nR50x1 final unit final block\nFigure 9: Higher layers of ViT maintain spatial location information more faithfully than ResNets. Each\nheatmap plot shows the CKA similarity between a single token representation in ﬁnal block of the model and\nthe input images, which are divided into non-overlapping patches. We observe that ViT tokens have strongest\nsimilarity to their corresponding spatial location in the image, but tokens corresponding to spatial locations at\nthe edge of the image (e.g. token 0) additionally show similarity to other edge positions. This demonstrates\nthat spatial information from the input is preserved even at the ﬁnal layer of ViT. By contrast, ResNet “tokens”\n(features at a speciﬁc spatial location) are much less spatially discriminative, showing comparable similarity\nacross a broad set of input spatial locations. See Appendix for additional layers and results.\nremoving skip connections from middle blocks.) This demonstrates the importance of representations\nbeing propagated by skip connections for the uniform similarity structure of ViT in Figure 1.\n7 Spatial Information and Localization\nThe results so far, on the role of self-attention in aggregating spatial information in ViTs, and skip-\nconnections faithfully propagating representations to higher layers, suggest an important followup\nquestion: how well can ViTs perform spatial localization? Speciﬁcally, is spatial information from\nthe input preserved in the higher layers of ViT? And how does it compare in this aspect to ResNet?\nAn afﬁrmative answer to this is crucial for uses of ViT beyond classiﬁcation, such as object detection.\n7\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(0, 0)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(2, 1)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 2)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 4)\nViT-B/32 GAP final block\nFigure 10: When trained with global average pooling (GAP) instead of a CLS token, ViTs show less clear\nlocalization (compare Figure 9). We plot the same CKA heatmap between a token and different input images\npatches as in Figure 9, but for a ViT model trained with global average pooling (like ResNet) instead of a CLS\ntoken. We observe signiﬁcantly less localization.\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n0\n10\n20\n30\n40\n50\n60\n70Test accuracy (%)\nViT-B/32 CLS\nViT-B/32 GAP\nResNet50\n0.0 0.2 0.4 0.6 0.8 1.0\nLayer number\n0\n10\n20\n30\n40\n50\n60\n70Test accuracy (%)\nCLS ; CLS token\nCLS ; GAP except CLS token\nCLS ; GAP\nGAP ; First token\nGAP ; GAP\n(a) Individual token evaluation (b) CLS vs GAP models\nFigure 11: Spatial localization experiments with linear probes. We train linear classiﬁers on 10-shot\nImageNet classiﬁcation from the representations extracted from different layers of ViT-B/32 models. We then\nplot the accuracy of the probe versus the (normalized) layer number. Left: We train a classiﬁer on each token\nseparately and report the average accuracy over all tokens (excluding the CLS token for the ViT CLS model.)\nRight: Comparison of ViT models pre-trained with a classiﬁcation token or with global average pooling (GAP)\nand then evaluated with different ways of aggregating the token representations.\nWe begin by comparing token representations in the higher layers of ViT and ResNet to those of input\npatches. Recall that ViT tokens have a corresponding input patch, and thus a corresponding input\nspatial location. For ResNet, we deﬁne a token representation to be all the convolutional channels at a\nparticular spatial location. This also gives it a corresponding input spatial location. We can then take\na token representation and compute its CKA score with input image patches at different locations.\nThe results are illustrated for different tokens (with their spatial locations labelled) in Figure 9.\nFor ViT, we observe that tokens corresponding to locations at the edge of the image are similar to\nedge image patches, but tokens corresponding to interior locations are well localized, with their\nrepresentations being most similar to the corresponding image patch. By contrast, for ResNet, we see\nsigniﬁcantly weaker localization (though Figure D.3 shows improvements for earlier layers.)\nOne factor inﬂuencing this clear difference between architectures is that ResNet is trained to classify\nwith a global average pooling step, while ViT has a separate classiﬁcation (CLS) token. To examine\nthis further, we test a ViT architecture trained with global average pooling (GAP) for localization (see\nAppendix A for training details). The results, shown in Figure 10, demonstrate that global average\npooling does indeed reduce localization in the higher layers. More results in Appendix Section D.\nLocalization and Linear Probe Classiﬁcation: The previous results have looked at localization\nthrough direct comparison of each token with input patches. To complete the picture, we look at\nusing each token separately to perform classiﬁcation with linear probes. We do this across different\nlayers of the model, training linear probes to classify image label with closed-form few-shot linear\nregression similar to Dosovitskiy et al. [14] (details in Appendix A). Results are in Figure 11, with\nfurther results in Appendix F. The left pane shows average accuracy of classiﬁers trained on individual\ntokens, where we see that ResNet50 and ViT with GAP model tokens perform well at higher layers,\nwhile in the standard ViT trained with a CLS token the spatial tokens do poorly – likely because\ntheir representations remain spatially localized at higher layers, which makes global classiﬁcation\nchallenging. Supporting this are results on the right pane, which shows that a single token from the\n8\n0 5 10 15 20\nBlock Index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Similarity to model trained on 100% of data (CKA)\nEffect of Training Set Size (ViT-L/16)\n 3% of full data\n10% of full data\n30% of full data\n0 2 4 6 8 10\nBlock Index\nEffect of Training Set Size (ViT-B/32)\n 3% of full data\n10% of full data\n30% of full data\nFigure 12: Measuring similarity of representations learned with varying amounts of data shows the\nimportance of large datasets for higher layers and larger model representations.We compute the similarity\nof representations at each block for ViT models that have been trained on smaller subsets of the data to a model\nthat has been trained on the full data on ViT-L/16 (left) and ViT-B/32 (right). We observe that while lower\nlayer representations have high similarity even with 10% of the data, higher layers and larger models require\nsigniﬁcantly more data to learn similar representations.\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n0\n10\n20\n30\n40\n50\n60\n70\n80Test accuracy (%)\nViT-L/16 JFT-300M\nViT-H/14 JFT-300M\nViT-L/16 ImageNet\nViT-H/14 ImageNet\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n0\n10\n20\n30\n40\n50\n60\n70\n80Test accuracy (%)\nViT-B/32\nViT-B/16\nViT-L/16\nViT-H/14\nResNet50\nResNet152\nResNet152x2\n(a) JFT-300M vs ImageNet pre-training (b) ViTs vs ResNets\nFigure 13: Experiments with linear probes. We train linear classiﬁers on 10-shot ImageNet classiﬁcation from\nthe aggregated representations of different layers of different models. We then plot the accuracy of the probe\nversus the (normalized) layer number. Left: Comparison of ViTs pre-trained on JFT-300M or ImageNet and\nevaluated with linear probes on Imagenet. Right: Comparison of ViT and ResNet models trained JFT-300m,\nevaluated with linear probes on ImageNet.\nViT-GAP model achieves comparable accuracy in the highest layer to all tokens pooled together.\nWith the results of Figure 9, this suggests all higher layer tokens in GAP models learn similar (global)\nrepresentations.\n8 Effects of Scale on Transfer Learning\nMotivated by the results of Dosovitskiy et al. [14] that demonstrate the importance of dataset scale\nfor high performing ViTs, and our earlier result (Figure 4) on needing scale for local attention, we\nperform a study of the effect of dataset scale on representations in transfer learning.\nWe begin by studying the effect on representations as the JFT-300M pretraining dataset size is varied.\nFigure 12 illustrates the results on ViT-B/32 and ViT-L/16. Even with3% of the entire dataset, lower\nlayer representations are very similar to the model trained on the whole dataset, but higher layers\nrequire larger amounts of pretraining data to learn the same representations as at large data scale,\nespecially with the large model size. In Section G, we study how much representations change in\nﬁnetuning, ﬁnding heterogeneity over datasets.\nWe next look at dataset size effect on the larger ViT-L/16 and ViT-H/14 models. Speciﬁcally, in the\nleft pane of Figure 13, we train linear classifer probes on ImageNet classes for models pretrained\non JFT-300M vs models only pretrained on ImageNet. We observe the JFT-300M pretained models\nachieve much higher accuracies even with middle layer representations, with a 30% gap in absolute\n9\naccuracy to the models pretrained only on ImageNet. This suggests that for larger models, the larger\ndataset is especially helpful in learning high quality intermediate representations. This conclusion is\nfurther supported by the results of the right pane of Figure 13, which shows linear probes on different\nResNet and ViT models, all pretrained on JFT-300M. We again see the larger ViT models learn much\nstronger intermediate representations than the ResNets. Additional linear probes experiments in\nSection F demonstrate this same conclusion for transfer to CIFAR-10 and CIFAR-100.\n9 Discussion\nLimitations: Our study uses CKA [17], which summarizes measurements into a single scalar, to\nprovide quantitative insights on representation similarity. While we have complemented this with\ninterventional tests and other analyses (e.g. linear probes), more ﬁne-grained methods may reveal\nadditional insights and variations in the representations.\nConclusion: Given the central role of convolutional neural networks in computer vision break-\nthroughs, it is remarkable that Transformer architectures (almost identical to those used in language)\nare capable of similar performance. This raises fundamental questions on whether these architec-\ntures work in the same way as CNNs. Drawing on representational similarity techniques, we ﬁnd\nsurprisingly clear differences in the features and internal structures of ViTs and CNNs. An analysis\nof self-attention and the strength of skip connections demonstrates the role of earlier global features\nand strong representation propagation in ViTs for these differences, while also revealing that some\nCNN properties, e.g. local information aggregation at lower layers, are important to ViTs, being\nlearned from scratch at scale. We examine the potential for ViTs to be used beyond classiﬁcation\nthrough a study of spatial localization, discovering ViTs with CLS tokens show strong preservation of\nspatial information — promising for future uses in object detection. Finally, we investigate the effect\nof scale for transfer learning, ﬁnding larger ViT models develop signiﬁcantly stronger intermediate\nrepresentations through larger pretraining datasets. These results are also very pertinent to under-\nstanding MLP-based architectures for vision proposed by concurrent work [42, 43], further discussed\nin Section H, and together answer central questions on differences between ViTs and CNNs, and\nsuggest new directions for future study. From the perspective of societal impact, these ﬁndings and\nfuture work may help identify potential failures as well as greater model interpretability.\nReferences\n[1] G. Alain and Y . Bengio. Understanding intermediate layers using linear classiﬁer probes. arXiv\npreprint arXiv:1610.01644, 2016.\n[2] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V . Le. Attention augmented convolutional\nnetworks. In Proceedings of the IEEE/CVF International Conference on Computer Vision ,\npages 3286–3295, 2019.\n[3] S. Bhojanapalli, A. Chakrabarti, D. Glasner, D. Li, T. Unterthiner, and A. Veit. Understanding\nrobustness of transformers for image classiﬁcation. arXiv preprint arXiv:2103.14586, 2021.\n[4] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object\ndetection with transformers. In European Conference on Computer Vision, pages 213–229.\nSpringer, 2020.\n[5] M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging\nproperties in self-supervised vision transformers. arXiv preprint arXiv:2104.14294, 2021.\n[6] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever. Generative pretraining\nfrom pixels. In International Conference on Machine Learning , pages 1691–1703. PMLR,\n2020.\n[7] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised vision transformers.\narXiv preprint arXiv:2104.02057, 2021.\n[8] A. Conneau, G. Kruszewski, G. Lample, L. Barrault, and M. Baroni. What you can cram into a\nsingle vector: Probing sentence embeddings for linguistic properties. In ACL, 2018.\n10\n[9] J.-B. Cordonnier, A. Loukas, and M. Jaggi. On the relationship between self-attention and\nconvolutional layers. arXiv preprint arXiv:1911.03584, 2019.\n[10] C. Cortes, M. Mohri, and A. Rostamizadeh. Algorithms for learning kernels based on centered\nalignment. The Journal of Machine Learning Research, 13(1):795–828, 2012.\n[11] S. d’Ascoli, H. Touvron, M. Leavitt, A. Morcos, G. Biroli, and L. Sagun. Convit: Improving\nvision transformers with soft convolutional inductive biases. arXiv preprint arXiv:2103.10697,\n2021.\n[12] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages\n248–255. Ieee, 2009.\n[13] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[14] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[15] A. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Schölkopf, A. J. Smola, et al. A kernel\nstatistical test of independence. In Nips, volume 20, pages 585–592. Citeseer, 2007.\n[16] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly, and N. Houlsby. Big transfer\n(bit): General visual representation learning. arXiv preprint arXiv:1912.11370, 6(2):8, 2019.\n[17] S. Kornblith, M. Norouzi, H. Lee, and G. Hinton. Similarity of neural network representations\nrevisited. In ICML, 2019.\n[18] S. Kornblith, H. Lee, T. Chen, and M. Norouzi. What’s in a loss function for image classiﬁcation?\narXiv preprint arXiv:2010.16402, 2020.\n[19] N. Kriegeskorte, M. Mur, and P. A. Bandettini. Representational similarity analysis-connecting\nthe branches of systems neuroscience. Frontiers in systems neuroscience, 2:4, 2008.\n[20] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. Advances in neural information processing systems, 25:1097–1105, 2012.\n[21] S. R. Kudugunta, A. Bapna, I. Caswell, N. Arivazhagan, and O. Firat. Investigating multilingual\nnmt representations at scale. arXiv preprint arXiv:1909.02197, 2019.\n[22] G. W. Lindsay. Convolutional neural networks as a model of the visual system: past, present,\nand future. Journal of cognitive neuroscience, pages 1–15, 2020.\n[23] W. Luo, Y . Li, R. Urtasun, and R. Zemel. Understanding the effective receptive ﬁeld in deep\nconvolutional neural networks. arXiv preprint arXiv:1701.04128, 2017.\n[24] N. Maheswaranathan, A. H. Williams, M. D. Golub, S. Ganguli, and D. Sussillo. Universality\nand individuality in neural dynamics across large populations of recurrent networks. Advances\nin neural information processing systems, 2019:15629, 2019.\n[25] A. Merchant, E. Rahimtoroghi, E. Pavlick, and I. Tenney. What happens to bert embeddings\nduring ﬁne-tuning? arXiv preprint arXiv:2004.14448, 2020.\n[26] A. S. Morcos, M. Raghu, and S. Bengio. Insights on representational similarity in neural\nnetworks with canonical correlation. arXiv preprint arXiv:1806.05759, 2018.\n[27] B. Mustafa, A. Loh, J. Freyberg, P. MacWilliams, M. Wilson, S. M. McKinney, M. Sieniek,\nJ. Winkens, Y . Liu, P. Bui, et al. Supervised transfer learning at scale for medical imaging.\narXiv preprint arXiv:2101.05913, 2021.\n[28] M. Naseer, K. Ranasinghe, S. Khan, M. Hayat, F. S. Khan, and M.-H. Yang. Intriguing properties\nof vision transformers, 2021.\n11\n[29] T. Nguyen, M. Raghu, and S. Kornblith. Do wide and deep networks learn the same things?\nuncovering how neural network representations vary with width and depth. arXiv preprint\narXiv:2010.15327, 2020.\n[30] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran. Image\ntransformer. In International Conference on Machine Learning , pages 4055–4064. PMLR,\n2018.\n[31] S. Paul and P.-Y . Chen. Vision transformers are robust learners.arXiv preprint arXiv:2105.07581,\n2021.\n[32] M. E. Peters, M. Neumann, L. Zettlemoyer, and W.-t. Yih. Dissecting contextual word embed-\ndings: Architecture and representation. In EMNLP, 2018.\n[33] A. Raghu, M. Raghu, S. Bengio, and O. Vinyals. Rapid learning or feature reuse? towards\nunderstanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.\n[34] M. Raghu, J. Gilmer, J. Yosinski, and J. Sohl-Dickstein. Svcca: Singular vector canonical corre-\nlation analysis for deep learning dynamics and interpretability.arXiv preprint arXiv:1706.05806,\n2017.\n[35] M. Raghu, C. Zhang, J. Kleinberg, and S. Bengio. Transfusion: Understanding transfer learning\nfor medical imaging. arXiv preprint arXiv:1902.07208, 2019.\n[36] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya, and J. Shlens. Stand-alone\nself-attention in vision models. arXiv preprint arXiv:1906.05909, 2019.\n[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\njournal of computer vision, 115(3):211–252, 2015.\n[38] J. Shi, E. Shea-Brown, and M. Buice. Comparison against task driven artiﬁcial neural networks\nreveals functional properties in mouse visual cortex.Advances in Neural Information Processing\nSystems, 32:5764–5774, 2019.\n[39] L. Song, A. Smola, A. Gretton, J. Bedo, and K. Borgwardt. Feature selection via dependence\nmaximization. Journal of Machine Learning Research, 13(5), 2012.\n[40] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international conference on computer vision,\npages 843–852, 2017.\n[41] Y . Tay, M. Dehghani, J. Gupta, D. Bahri, V . Aribandi, Z. Qin, and D. Metzler. Are pre-trained\nconvolutions better than pre-trained transformers? arXiv preprint arXiv:2105.03322, 2021.\n[42] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner, J. Yung, D. Keysers,\nJ. Uszkoreit, M. Lucic, et al. Mlp-mixer: An all-mlp architecture for vision. arXiv preprint\narXiv:2105.01601, 2021.\n[43] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby, E. Grave, A. Joulin, G. Synnaeve,\nJ. Verbeek, and H. Jégou. Resmlp: Feedforward networks for image classiﬁcation with data-\nefﬁcient training. arXiv preprint arXiv:2105.03404, 2021.\n[44] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[45] E. V oita, R. Sennrich, and I. Titov. The bottom-up evolution of representations in the transformer:\nA study with machine translation and language modeling objectives. In EMNLP, 2019.\n[46] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez, K. Keutzer, and\nP. Vajda. Visual transformers: Token-based image representation and processing for computer\nvision. arXiv preprint arXiv:2006.03677, 2020.\n[47] J. M. Wu, Y . Belinkov, H. Sajjad, N. Durrani, F. Dalvi, and J. Glass. Similarity analysis of\ncontextual word representation models. arXiv preprint arXiv:2005.01172, 2020.\n12\n[48] S. Wu, A. Conneau, H. Li, L. Zettlemoyer, and V . Stoyanov. Emerging cross-lingual structure\nin pretrained language models. arXiv preprint arXiv:1911.01464, 2019.\n[49] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z. Jiang, F. E. Tay, J. Feng, and S. Yan.\nTokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021.\n[50] X. Zhai, J. Puigcerver, A. Kolesnikov, P. Ruyssen, C. Riquelme, M. Lucic, J. Djolonga, A. S.\nPinto, M. Neumann, A. Dosovitskiy, et al. The visual task adaptation benchmark. 2019.\n13\nAppendix\nAdditional details and results from the different sections are included below.\nA Additional details on Methods and the Experimental Setup\nTo understand systematic differences between ViT and CNNs, we use a representative set of different\nmodels of each type, guided by the performance results in [14]. Speciﬁcally, for ViTs, we look at\nViT-B/32, ViT-L/16 and ViT-H/14, where the smallest model (ViT-B/32) shows limited improvements\nwhen pretraining on JFT-300M [40] vs. the ImageNet Large Scale Visual Recognition Challenge 2012\ndataset [12, 37], while the largest, ViT-H/14, achieves state of the art when pretrained on JFT-300M\n[40]. ViT-L/16 is close to the performance ViT-H/14 [14]. For CNNs, we look at ResNet50x1 which\nalso shows saturating performance when pretraining on JFT-300M, and also ResNet152x2, which in\ncontrast shows large performance gains with increased pretraining dataset size. As in Dosovitskiy\net al. [14], these ResNets follow some of the implementation changes ﬁrst proposed in BiT [16].\nIn addition to the standard Vision Transformers trained with a classiﬁcation token (CLS), we also\ntrained ViTs with global average pooling (GAP). In these, there is no classiﬁcation token – instead,\nthe representations of tokens in the last layer of the transformer are averaged and directly used to\npredict the logits. The GAP model is trained with the same hyperparameters as the CLS one, except\nfor the initial learning rate that is set to a lower value of 0.0003.\nFor analyses of internal model representations, we observed no meaningful difference between\nrepresentations of images drawn from ImageNet and images drawn from JFT-300M. Figures 1, 2, 9,\nand 10 use images from the JFT-300M dataset that were not seen during training, while Figures 6,\n3, 4, 5 7, 8, and 12 use images from the ImageNet 2012 validation set. Figures 11 and 13 involve\n10-shot probes trained on the ImageNet 2012 training set, tuned hyperparameters on a heldout portion\nof the training set, and evaluated on the validation set.\nAdditional details on CKA implementation To compute CKA similarity scores, we use minibatch\nCKA, introduced in [29]. Speciﬁcally, we use batch sizes of 1024 and we sample a total of 10240\nexamples without replacement. We repeat this 20 times and take the average. Experiments varying\nthe exact batch size (down to 128), and total number of examples used for CKA ( down to 2560 total\nexamples, repeated 10 times), had no noticeable effect on the results.\nAdditional details on linear probes. We train linear probes as regularized least-squares regression,\nfollowing Dosovitskiy et al. [14]. We map the representations training images to {−1, 1}N target\nvectors, where N is the number of classes. The solution can be recovered efﬁciently in closed form.\nFor vision transformers, we train linear probes on representations from individual tokens or on\nthe representation averaged over all tokens, at the output of different transformer layers (each\nlayer meaning a full transformer block including self-attention and MLP). For ResNets, we take\nrepresentation at the output of each residual block (including 3 convolutional layers). The resolution\nof the feature maps changes throughout the model, so we perform an additional pooling step bringing\nthe feature map to the same spatial size as in the last stage. Moreover, ResNets differ from ViTs\nin that the number of channels changes throughout the model, with fewer channels in the earlier\nlayers. This smaller channel count in the earlier layers could potentially lead to worse performance\nof the linear probes. To compensate for this, before pooling we split the feature map into patches and\nﬂattened each patch, so as to arrive at the channel count close to the channel count in the ﬁnal block.\nAll results presented in the paper include this additional patching step; however, we have found that it\nbrings only a very minor improvement on top of simple pooling.\nB Additional Representation Structure Results\nHere we include some more CKA heatmaps, which provide insights on model representation structures\n(compare to Figure 1, Figure 2 in the main text.) We observe similar conclusions: ViT representation\nstructure has a more uniform similarity structure across layers, and comparing ResNet to ViT\n14\nrepresentations show a large fraction of early ResNet layers similar to a smaller number of ViT layers.\n0 10 20 30 40 50 60 70\nLayers ViT-B/32\n0\n10\n20\n30\n40\n50\n60\n70Layers ViT-B/32\nViT-B/32\n0 20 40 60 80 100 120\nLayers R50\n0\n10\n20\n30\n40\n50\n60\n70Layers ViT-B/32\nViT-B/32 vs R50\n0 50 100 150 200 250 300 350\nLayers R152\n0\n25\n50\n75\n100\n125\n150\n175Layers ViT-H/14\nViT-H/14 vs R152\nFigure B.1: Additional CKA heatmap results. Top shows CKA heatmap for ViT-B/32, where we can also\nobserve strong similarity between lower and higher layers and the grid like, uniform representation structure.\nBottom shows (i) ViT-B/32 compared to R50, where we again see that 60 of the lowest R50 layers are similar\nto about 25 of the lowest ViT-B/32 layers, with the remaining layers most similar to each other (ii) ViT-H/14\ncompared to R152, where we see the lowest 100 layers of R152 are most similar to lowest 30 layers of\nViT-H/14.\nC Additional Local/Global Information Results\nIn Figures C.1, C.2, and C.3, we provide full plots of effective receptive ﬁelds of all layers of\nViT-B/32, ResNet-50, and ViT-L/16, taken after the residual connections as in Figure 6 in the text. In\nFigure C.4, we show receptive ﬁelds of ViT-B/32 and ResNet-50 takenbefore the residual connections.\nAlthough the pre-residual receptive ﬁelds of ViT MLP sublayers resemble the post-residual receptive\nﬁelds in Figure C.1, the pre-residual receptive ﬁelds of attention sublayers have a smaller relative\ncontribution from the corresponding input patch. These results support our ﬁndings in Section 5\nregarding the global nature of attention heads, but suggest that network representations remain tied to\ninput patch locations because of the strong contributions from skip connections, studied in Section 6.\nResNet-50 pre-residual receptive ﬁelds look similar to the post-residual receptive ﬁelds.\n15\nAttention 1\n MLP 1\n Attention 2\n MLP 2\n Attention 3\n MLP 3\nAttention 4\n MLP 4\n Attention 5\n MLP 5\n Attention 6\n MLP 6\nAttention 7\n MLP 7\n Attention 8\n MLP 8\n Attention 9\n MLP 9\nAttention 10\n MLP 10\n Attention 11\n MLP 11\n Attention 12\n MLP 12\nViT-B/32\nFigure C.1: Post-residual receptive ﬁelds of all ViT-B/32 sublayers.\nInitial Conv\n Block 1\n Block 2\n Block 3\n Block 4\n Block 5\nBlock 6\n Block 7\n Block 8\n Block 9\n Block 10\n Block 11\nBlock 12\n Block 13\n Block 14\n Block 15\n Block 16\nResNet-50\nFigure C.2: Post-residual receptive ﬁelds of all ResNet-50 blocks.\n16\nAttention 1\n MLP 1\n Attention 2\n MLP 2\n Attention 3\n MLP 3\nAttention 4\n MLP 4\n Attention 5\n MLP 5\n Attention 6\n MLP 6\nAttention 7\n MLP 7\n Attention 8\n MLP 8\n Attention 9\n MLP 9\nAttention 10\n MLP 10\n Attention 11\n MLP 11\n Attention 12\n MLP 12\nAttention 13\n MLP 13\n Attention 14\n MLP 14\n Attention 15\n MLP 15\nAttention 16\n MLP 16\n Attention 17\n MLP 17\n Attention 18\n MLP 18\nAttention 19\n MLP 19\n Attention 20\n MLP 20\n Attention 21\n MLP 21\nAttention 22\n MLP 22\n Attention 23\n MLP 23\n Attention 24\n MLP 24\nViT-L/16\nFigure C.3: Post-residual receptive ﬁelds of all ViT-L/16 sublayers.\n17\nAttention 1\n MLP 1\n Attention 2\n MLP 2\n Attention 3\n MLP 3\nAttention 4\n MLP 4\n Attention 5\n MLP 5\n Attention 6\n MLP 6\nAttention 7\n MLP 7\n Attention 8\n MLP 8\n Attention 9\n MLP 9\nAttention 10\n MLP 10\n Attention 11\n MLP 11\n Attention 12\n MLP 12\nViT-B/32\nInitial Conv\n Block 1\n Block 2\n Block 3\n Block 4\n Block 5\nBlock 6\n Block 7\n Block 8\n Block 9\n Block 10\n Block 11\nBlock 12\n Block 13\n Block 14\n Block 15\n Block 16\nResNet-50\nFigure C.4: Pre-residual receptive ﬁelds of all ViT-B/32 sublayers and ResNet-50 blocks.In ViT-B, we see\nthat the pre-residual receptive ﬁelds of later attention sublayers are not dominated by the center patch, in contrast\nto the post-residual receptive ﬁelds shown in Figure C.1. Thus, although later attention sublayers integrate\ninformation across the entire input image, network representations remain localized due to the strong skip\nconnections. ResNet-50 pre-residual receptive ﬁelds generally resemble the post-residual receptive ﬁelds shown\nin Figure C.2. The receptive ﬁeld appears to “shrink” at blocks 4, 8, and 14, which are each the ﬁrst in a stage,\nand for which we plot only the longer branch and not the shortcut. The receptive ﬁeld does not shrink when\ncomputed after the summation of these branches, as shown in Figure C.2.\n18\n0 2 4 6 8 10\nSorted Attention Head\n0\n20\n40\n60\n80\n100\n120Mean Distance\nViT-B/32 on JFT\nencoder_block0\nencoder_block1\nencoder_block10\nencoder_block11\n0 2 4 6 8 10\nSorted Attention Head\n0\n20\n40\n60\n80\n100\n120Mean Distance\nViT-B/32 on ImageNet\nencoder_block0\nencoder_block1\nencoder_block10\nencoder_block11\nFigure C.5: Plot of attention head distances for ViT-B/32 when trained on JFT-300M and on only Ima-\ngeNet shows that ViT-B/32 learns to attend locally even on a smaller dataset. Compare to Figure 3 in the\nmain text. While ViT-L and ViT-H show large performance improvements when (i) ﬁnetuned on ImageNet having\nbeen pretrained on JFT compared to (ii) being trained on only ImageNet, ViT-B/32 has similar performance\nin both settings. We also observe that ViT-H, ViT-L don’t learn to attend locally in the lowest layers when\nonly trained on ImageNet (Figure 3), whereas here we see ViT-B/32 still learns to attend locally — suggesting\nconnections between performance and heads learning to attend locally.\nD Localization\nBelow we include additional localization results: computing CKA between different input patches\nand tokens in the higher layers of the models. We show results for ViT-H/14, additional higher layers\nfor ViT-L/16, ViT-B/32 and addtional layers for ResNet.\n19\n0 2 4 6 8 10 12 14\nSorted Attention Head\n0\n20\n40\n60\n80\n100\n120Mean Distance\nViT-H/14 on JFT\nencoder_block0\nencoder_block1\nencoder_block30\nencoder_block31\nFigure C.6: Additional plot of attention head distances for ViT-H/14 on JFT-300M. Compare to Figure 3\nin the main text. For each attention head, we compute the pixel distance it attends to, weighted by the attention\nweights, and then average over 5000 datapoints to get an average attention head distance. We plot the heads\nsorted by their average attention distance for the two lowest and two highest layers in the ViT, observing that the\nlower layers attend both locally and globally, while the higher layers attend entirely globally.\n0123456789101112131415\n0123456789101112131415\nToken at Location(0, 0)\n0123456789101112131415\n0123456789101112131415\nToken at Location(2, 5)\n0123456789101112131415\n0123456789101112131415\nToken at Location(5, 10)\n0123456789101112131415\n0123456789101112131415\nToken at Location(5, 11)\nViT-H/14 final block\n0123456789101112131415\n0123456789101112131415\nToken at Location(0, 0)\n0123456789101112131415\n0123456789101112131415\nToken at Location(2, 5)\n0123456789101112131415\n0123456789101112131415\nToken at Location(5, 10)\n0123456789101112131415\n0123456789101112131415\nToken at Location(5, 11)\nViT-H/14 penultimate block\nFigure D.1: Localization heatmaps for ViT-H/14. We see that ViT-H/14 is also well localized, both in the\nﬁnal block and penultimate block, with tokens with corresponding locations in the interior of the image most\nsimilar to the image patches at those locations, while tokens on the edge are similar to many edge positions.\n20\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(0, 0)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(2, 1)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 2)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 4)\nViT-B/32 penultimate block\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken0 Location(0, 0)\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken31 Location(2, 3)\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken35 Location(2, 7)\n0 1 2 3 4 5 6 7 8 910111213\n012345678910111213\nToken61 Location(4, 5)\nViT-L/16 penultimate block\nFigure D.2: Additional localization heatmaps for other higher layers of ViT-L/16 and ViT-B/32 . We see\nthat models (as expected) remain well localized in higher layers other than the ﬁnal block.\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(0, 0)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(2, 1)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 2)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 4)\nR50x1 middle unit final block\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(0, 0)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(2, 1)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 2)\n0 1 2 3 4 5 6\n0\n1\n2\n3\n4\n5\n6\nToken at Location(4, 4)\nR50x1 unit1 final block\nFigure D.3: Localization heatmaps for layers of ResNet below ﬁnal layer . Comparing to Figure 9 in the\nmain text, we see that layers in the ResNet lower than the ﬁnal layer display better localization, but still not as\nclear as the CLS trained ViT models.\n21\nLayer 1\n Layer 2\n Layer 6\n Layer 11\n Layer 12\n0.7\n0.8\n0.9\n1.0\n1.1\n1.25\n1.50\n1.75\n2.00\n2.25\n4\n6\n8\n10\n6\n8\n10\n12\n6\n8\n10\nViT-B/32 CLS\nLayer 1\n Layer 2\n Layer 6\n Layer 11\n Layer 12\n0.6\n0.7\n0.8\n0.9\n1.0\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n6\n8\n10\n39\n40\n41\n42\n43\n44\n57.0\n57.2\n57.4\n57.6\n57.8\n58.0\nViT-B/32 GAP\nBlock 1, unit 1\n Block 2, unit 1\n Block 3, unit 1\n Block 4, unit 1\n Block 4, unit 3\n0.7\n0.8\n0.9\n1.0\n0.6\n0.7\n0.8\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n6\n8\n10\n12\n14\n44\n46\n48\n50\n52\nResNet50\nFigure D.4: Linear probes spatial localization. We train a linear probe on each individual token and plot the\naverage accuracy over the test set, in percent. Here we plot the results for each token a subset of layers in\n3 models: ViT-B/32 trained with a classiﬁcation token (CLS) or global average pooling (GAP), as well as a\nResNet50. Note the different scales of values in different sub-plots.\n22\nE Additional Representation Propagation Results\nFigure E.1 shows the ratio of representation norms between skip connections and MLP and Self-\nAttention Blocks. In both cases, we observe that the CLS token representation is mostly unchanged in\nthe ﬁrst few layers, while later layers change it rapidly, mostly via MLP blocks. The reverse is true for\nthe spatial tokens representing image patches, whose representation is mostly changed in earlier layers\nand does not change much during later layers. Looking at the cosine similarity of representations\nbetween output in Figure E.2 conﬁrms these ﬁndings: while spatial token representations change\nmore in early layers, the output of later blocks is very similar to the representation present on the skip\nconnections, while the inverse is true for the CLS token.\n0 10 20 30 40\nToken index\n0\n2\n4\n6\n8\n10 Block Index\nRatio of norm(identity)/norm(MLP)\n0 10 20 30 40\nToken index\nRatio of norm(Identity)/norm(SelfAttn)\n2\n4\n6\n8\n10\n12\n14\n16\nFigure E.1: Additional Heatmaps of Representation Norms Ratio Representation Norms ||zi||/||f(zi)||\nbetween skip connection and the MLP or Self-Attention block for the hidden representation of each block on\nViT-B/32, separately for each Token (Token 0 is CLS).\n0 2 4 6 8 10 12 14\nBlock Index\n0.6\n0.7\n0.8\n0.9\n1.0Average Cosine Similarity\nCosine similarity between output/branch output\nSelf/Attention\nMLP\nSelf/Attention CLS\nMLP CLS\nResNet50\nFigure E.2: Most information in ViT passes through Identity Connections. Cosine Similarity of represen-\ntations between the skip-connection (identity) and the longer branch for ViT-B/16 trained on ImageNet and a\nResNet v1. For ViT, we show the CLS token separately from the rest of the representation.\n23\nF Additional results on linear probes\nHere we provide additional results on linear probes complementing Figures 11 and 13 of the main\npaper. In particular, we repeat the linear probes on the CIFAR-10 and CIFAR-100 datasets and, in\nsome cases, add more models to comparisons. For CIFAR-10 and CIFAR-100, we use the ﬁrst 45000\nimages of the training set for training and the last 5000 images from the training set for validation.\nAdditional results are shown in Figures F.1, F.2, F.3.\nMoreover, we discuss the results in the main paper in more detail. In Figure 11 (left) we experiment\nwith different ways of evaluating a ViT-B/32 model. We vary two aspects: 1) the classiﬁer with which\nthe model was trained, classiﬁcation token (CLS) or global average pooling (GAP), 2) The way the\nrepresentation is aggregated: by just taking the ﬁrst token (which for the CLS models is the CLS\ntoken), averaging all tokens, or averaging all tokens except for the ﬁrst one.\nThere are three interesting observations to be made. First, CLS and GAP models evaluated with their\n“native” representation aggregation approach – ﬁrst token for CLS and GAP for GAP – perform very\nsimilarly. Second, the CLS model evaluated with the pooled representation performs on par with the\nﬁrst token evaluation up to last several layers, at which point the performance plateaus. This suggests\nthat the CLS token is crucially contributing to information aggregation in the latter layers. Third,\nlinear probes trained on the ﬁrst token of a model trained with a GAP classiﬁer perform very poorly\nfor the earlier layers, but substantially improve in the latter layers and almost match the performance\nof the standard GAP evaluation in the last layer. This suggests all tokens are largely interchangeable\nin the latter layers of the GAP model.\nTo better understand the information contained in individual tokens, we trained linear probes on all\nindividual tokens of three models: ViT-B/32 trained with CLS or GAP, as well as ResNet50. Figure 11\n(right) plots average performance of these per-token classiﬁers. There are two main observations\nto be made. First, in the ViT-CLS model probes trained on individual tokens perform very poorly,\nconﬁrming that the CLS token plays a crucial role in aggregating global class-relevant information.\nSecond, in ResNet the probes perform poorly in the early layers, but get much better towards the end\nof the model. This behavior is qualitatively similar to the ViT-GAP model, which is perhaps to be\nexpected, since ResNet is also trained with a GAP classiﬁer.\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n0\n10\n20\n30\n40\n50\n60\n70\n80Test accuracy (%)\nViT-B/32 JFT-300M\nViT-L/16 JFT-300M\nViT-B/16 JFT-300M\nViT-H/14 JFT-300M\nViT-B/32 ImageNet\nViT-L/16 ImageNet\nViT-H/14 ImageNet\nViT-B/16 ImageNet\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n50\n60\n70\n80\n90\n100Test accuracy (%)\nViT-B/32 JFT-300M\nViT-L/16 JFT-300M\nViT-B/16 JFT-300M\nViT-H/14 JFT-300M\nViT-B/32 ImageNet\nViT-L/16 ImageNet\nViT-H/14 ImageNet\nViT-B/16 ImageNet\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n20\n30\n40\n50\n60\n70\n80\n90Test accuracy (%)\nViT-B/32 JFT-300M\nViT-L/16 JFT-300M\nViT-B/16 JFT-300M\nViT-H/14 JFT-300M\nViT-B/32 ImageNet\nViT-L/16 ImageNet\nViT-H/14 ImageNet\nViT-B/16 ImageNet\n(a) ImageNet (b) CIFAR-10 (c) CIFAR-100\nFigure F.1: Experiments with linear probes. Additional results on models pre-trained on JFT-300M and\nImageNet (Fig. 13 left) – with the addition of ViT-B models and CIFAR-10/100 datasets.\n24\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n40\n50\n60\n70\n80\n90\n100Test accuracy (%)\nViT-B/32\nViT-B/16\nViT-L/16\nViT-H/14\nResNet50\nResNet152\nResNet152x2\n0.0 0.2 0.4 0.6 0.8 1.0\nNormalized layer number\n10\n20\n30\n40\n50\n60\n70\n80\n90Test accuracy (%)\nViT-B/32\nViT-B/16\nViT-L/16\nViT-H/14\nResNet50\nResNet152\nResNet152x2\n(a) CIFAR-10 (b) CIFAR-100\nFigure F.2: Experiments with linear probes. Additional results on comparison of ViT and ResNet models\n(Fig. 13 right) on CIFAR-10/100 datasets.\n0.0 0.2 0.4 0.6 0.8 1.0\nLayer number\n40\n50\n60\n70\n80\n90\n100Test accuracy (%)\nCLS ; CLS token\nCLS ; GAP except CLS token\nCLS ; GAP\nGAP ; First token\nGAP ; GAP\n0.0 0.2 0.4 0.6 0.8 1.0\nLayer number\n10\n20\n30\n40\n50\n60\n70\n80Test accuracy (%)\nCLS ; CLS token\nCLS ; GAP except CLS token\nCLS ; GAP\nGAP ; First token\nGAP ; GAP\n(a) CIFAR-10 (b) CIFAR-100\nFigure F.3: Experiments with linear probes. Additional results on comparison of ViT and ResNet models\n(Fig. 11 right) on CIFAR-10/100 datasets.\nG Effects of Scale on Transfer Learning\nFinally, we study how much representations change through the ﬁnetuning process for a model pre-\ntrained on JFT-300M, ﬁnding signiﬁcant variation depending on the dataset. For tasks like ImageNet\nor Cifar100 which are very similar to the natural images setting of JFT300M, the representation\ndoes not change too much. For medical data (Diabethic Retinopathy detection) or satellite data\n(RESISC45), the changes are more pronounced. In all cases, it seems like the ﬁrst four to ﬁve layers\nremain very well preserved, even accross model sizes. This indicates that the features learned there\nare likely to be fairly task agnostic, as seen in Figure G.1 and Figure G.2.\n25\n0 2 4 6 8 10\nBlock Index\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Similarity to pretrained model (CKA)\nEffect of Finetuning\nimagenet2012\ncifar100\ncifar10\noxford_pets\noxford_flowers\nretinopathy\nresisc45\n0 5 10 15 20 25 30\nBlock\n0.2\n0.4\n0.6\n0.8\n1.0Similarity (CKA)\nSimilarity of Representations before/after Finetuning\nImageNet\nCifar10\nDiabetic Rethinopaty\nViT-B/32\nViT-L/16\nViT-H/14\nFigure G.1: (top) Similarity of representations at each block for ViT-B/16 models compared to before ﬁnetuning.\n(bottom) Similarity of representations at each block for different ViT model sizes.\n0 20 40 60 80\nLayer ViT-L/16\n0\n10\n20\n30\n40Layers ViT-B/32\nViT-B/32 vs ViT-L/16\n0 20 40 60 80 100 120\nLayer ViT-H/14\n0\n10\n20\n30\n40Layers ViT-B/32\nViT-B/32 vs ViT-H/14\n0 20 40 60 80 100 120\nLayer ViT-H/14\n0\n20\n40\n60\n80Layers ViT-L/16\nViT-L/16 vs ViT-H/14\nFigure G.2: Similarity of representations of different ViT model sizes.\nH Preliminary Results on MLP-Mixer\nFigure H.1 shows the representations from various MLP-Mixer models. The representations seem to\nalso fall very clearly into distinct, uncorrelated blocks, with a smaller block in the beginning and a\nlarger block afterwards. This is independent of model size. Comparing these models with ViT or\nResNet as in Figure H.2 models makes it clear that overall, the models behave more similar to ViT\nthan ResNets (c.f. Fig. 1 and 2).\n0 10 20 30 40\nlayer\n0\n10\n20\n30\n40layer\nMixer-B/32\n0 10 20 30 40\nlayer\n0\n10\n20\n30\n40layer\nMixer-B/16\n0 20 40 60 80\nlayer\n0\n20\n40\n60\n80layer\nMixer-L/16\n0 20 40 60 80 100 120\nlayer\n0\n20\n40\n60\n80\n100\n120layer\nMixer-H/14\nFigure H.1: Similarity of representations of different ViT model sizes.\n26\n0 20 40 60 80\nlayer ViT-L16\n0\n20\n40\n60\n80layer Mixer-L/16\nMixer L/16 vs ViT-L16\n0 10 20 30\nlayer ResNet50x1\n0\n20\n40\n60\n80layer Mixer-L/16\nMixer L/16 vs ResNet50x1\nFigure H.2: Similarity of representations of different ViT model sizes.\n27"
}