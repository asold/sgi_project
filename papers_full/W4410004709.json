{
  "title": "A review of automatic item generation techniques leveraging large language models",
  "url": "https://openalex.org/W4410004709",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2121296513",
      "name": "Bin Tan",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5095042856",
      "name": "Nour Armoush",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A5092602297",
      "name": "Elisabetta Mazzullo",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2120159399",
      "name": "Okan Bulut",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2250521832",
      "name": "Mark Gierl",
      "affiliations": [
        "University of Alberta"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388140488",
    "https://openalex.org/W4368276316",
    "https://openalex.org/W4200025781",
    "https://openalex.org/W4293147552",
    "https://openalex.org/W2220688940",
    "https://openalex.org/W2075950485",
    "https://openalex.org/W4286561208",
    "https://openalex.org/W4312097335",
    "https://openalex.org/W1967555382",
    "https://openalex.org/W4381956373",
    "https://openalex.org/W4309232742",
    "https://openalex.org/W4295846739",
    "https://openalex.org/W2906432682",
    "https://openalex.org/W4385573815",
    "https://openalex.org/W4321260681",
    "https://openalex.org/W3101000947",
    "https://openalex.org/W2903761270",
    "https://openalex.org/W2994178546",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W7061537676",
    "https://openalex.org/W4289523162",
    "https://openalex.org/W4214509822",
    "https://openalex.org/W4361733316",
    "https://openalex.org/W4312863132",
    "https://openalex.org/W3112417698",
    "https://openalex.org/W4318480024",
    "https://openalex.org/W4285155190",
    "https://openalex.org/W2142413505",
    "https://openalex.org/W6625268989",
    "https://openalex.org/W2562551526",
    "https://openalex.org/W4210850835",
    "https://openalex.org/W4379781152",
    "https://openalex.org/W4302041232",
    "https://openalex.org/W4367844670",
    "https://openalex.org/W4223544034",
    "https://openalex.org/W3129150706",
    "https://openalex.org/W4381956360",
    "https://openalex.org/W3203318778",
    "https://openalex.org/W6962852072",
    "https://openalex.org/W4292549551",
    "https://openalex.org/W2907125152",
    "https://openalex.org/W3202810978",
    "https://openalex.org/W4285503625",
    "https://openalex.org/W4320151791",
    "https://openalex.org/W4220822134",
    "https://openalex.org/W1964234352",
    "https://openalex.org/W2989613245",
    "https://openalex.org/W2183581773",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3010246167",
    "https://openalex.org/W2908927666",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3208210283",
    "https://openalex.org/W4292197005",
    "https://openalex.org/W4282567490",
    "https://openalex.org/W4290973226",
    "https://openalex.org/W4317795002",
    "https://openalex.org/W3093858354",
    "https://openalex.org/W4388729351",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W4310995245",
    "https://openalex.org/W2094719514",
    "https://openalex.org/W4294768397",
    "https://openalex.org/W4283314185",
    "https://openalex.org/W3092999391",
    "https://openalex.org/W4379806099",
    "https://openalex.org/W3033487789",
    "https://openalex.org/W4205497773",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4297412105",
    "https://openalex.org/W4297796135",
    "https://openalex.org/W4286639707",
    "https://openalex.org/W2233859745",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W4391227797",
    "https://openalex.org/W2999777802",
    "https://openalex.org/W4385573161",
    "https://openalex.org/W2985620815",
    "https://openalex.org/W2532028706",
    "https://openalex.org/W4287123055",
    "https://openalex.org/W3083239953",
    "https://openalex.org/W4298084493",
    "https://openalex.org/W4313022557",
    "https://openalex.org/W3176011152",
    "https://openalex.org/W4323075761",
    "https://openalex.org/W4288059420",
    "https://openalex.org/W2145216917",
    "https://openalex.org/W4205528697",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2116100614",
    "https://openalex.org/W4361193274",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4221146870"
  ],
  "abstract": "This study reviews existing research on the use of large language models (LLMs) for automatic item generation (AIG). We performed a comprehensive literature search across seven research databases, selected studies based on predefined criteria, and summarized 60 relevant studies that employed LLMs in the AIG process. We identified the most commonly used LLMs in current AIG literature, their specific applications in the AIG process, and the characteristics of the generated items. We found that LLMs are flexible and effective in generating various types of items across different languages and subject domains. However, many studies have overlooked the quality of the generated items, indicating a lack of a solid educational foundation. Therefore, we share two suggestions to enhance the educational foundation for leveraging LLMs in AIG, advocating for interdisciplinary collaborations to exploit the utility and potential of LLMs.",
  "full_text": " \nInternational Journal of Assessment Tools in Education \n 2025, Vol. 12, No. 2, 317–340 \nhttps://doi.org/10.21449/ijate.1602294 \njournal homepage: https://dergipark.org.tr/en/pub/ijate                                                         Research Article \n \n \n317 \n \nA review of automatic item generation techniques leveraging large language \nmodels \n \n \nBin Tan 1*,  Nour Armoush 1, Elisabetta Mazzullo 1, Okan Bulut 2, Mark J. Gierl 2 \n \n1University of Alberta, Faculty of Education, Measurement, Evaluation, and Data Science, Edmonton, Canada \n2University of Alberta, Faculty of Education, Centre for Research in Applied Measurement and Evaluation , \nEdmonton, Canada \n \nARTICLE HISTORY \nReceived: Dec. 16, 2024 \nAccepted: Apr. 28, 2025 \n \nKeywords: \nAutomatic item generation,  \nAutomatic question generation,  \nLarge language models,  \nGenerative artificial \nintelligence,  \nEducational technology. \nAbstract: This study reviews existing research on the use of large language \nmodels (LLMs) for automatic item generation (AIG). We performed a \ncomprehensive literature search across seven research databases, selected \nstudies based on predefined criteria, and summarized 60 relevant studies that \nemployed LLMs in the AIG process. We identified the most commonly used \nLLMs in current AIG literature, their specific applications in the AIG process, \nand the characteristics of the generated items. We found that LLMs are \nflexible and effective in generating various types of items across diffe rent \nlanguages and subject domains. However, many studies have overlooked the \nquality of the generated items, indicating a lack of a solid educational \nfoundation. Therefore, we share two suggestions to enhance the educational \nfoundation for leveraging LLMs  in AIG, advocating for interdisciplinary \ncollaborations to exploit the utility and potential of LLMs. \n1. INTRODUCTION \nAs a  foundational component of the modern education system, assessments serve multiple \npurposes such as facilitating student learning, evaluating teaching outcomes, and informing \neducational policy (Black, 1998; Lee et al., 2020). Advancements in educational technology, \nsuch as e -learning platforms (Granić et al ., 2022), adaptive t esting (Yen et al ., 2012), and \nformative assessments (Dalby & Swan, 2019; Spector et al., 2016), have increased the demand \nfor high -quality assessment items. However, traditional item development methods are \nstruggling to keep pace with this demand. For example, the cost of developing a single item for \nhigh-stakes assessments can reach up to $2,000 (Rudner, 2010) . Additionally, items for low-\nstakes assessments often suffer from low-quality content, thus failing to provide comprehensive \nor valuable feedback to students (Lim, 2019; Wylie & Lyon, 2015). \nIn response to the growing demand for high -quality items,  educational measurement \nresearchers first turned their attention toward a solution called automatic item generation (AIG). \nThe goal of AIG is to generate  large banks of high-quality items while reducing overall costs \n                                                             \n*CONTACT: Bin TAN    btan4@ualberta.ca    Measurement, Evaluation, and Data Science, Faculty of Education, \nUniversity of Alberta, 6-110 Education Centre North, 11210 87 Ave NW, Edmonton, AB T6G 2G5 CANADA \nThe copyright of the published article belongs to its author under CC BY 4.0 license. To view a copy of this licence, visit \nhttp://creativecommons.org/licenses/by/4.0/ \ne-ISSN: 2148-7456 \n\nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n318 \n(Gierl & Lai, 2012; Lai et al., 2009). The template-based method proposed by Gierl et al. (2012) \nis the most widely used method for AIG, which involves a three -step process that requires the \nconstruction of cognitive models and item models (Gierl & Lai, 2012, 2016). This method has \nbeen successfully implemented in various educational practices (e.g., medical licensing \nexaminations), demonstrating its viability and cost-effectiveness in creating high-quality items \n(Kosh et al., 2018; Pugh et al., 2020). Coincidentally, there is a related line of research in the \ndomain of computer science, specifically focusing on automatic question generation (AQG) for \neducational purposes. Originally rooted in n atural language processing (NLP) research for \ndeveloping chatbots and conversational agents, AQG for educational purposes has evolved to \nshare a similar goal with AIG: the automatic generation of a large number of assessment items \nusing computer algorithms . Despite this shared goal, key terminological differences persist \nbetween the two fields. In educational measurement , the term “items” can be presented in \ndiversified types to measure students' knowledge and skills, while “questions” typically have a \nmuch narrower format, usually referring to interrogative sentences ending with a question mark \n(Gierl et al ., 2021). Although an item may include a question, not all questions qualify as \ncomplete items. Despite this distinction, researchers of AQG for educational purposes often use \nthe term “questions” broadly, even when referring to non-interrogative item formats. \nThe work conducted in AQG research for educational purposes has been summarized in two \nrecent review studies (Ch & Saha, 2018; Kurdi et al., 2020). Regrettably, in both reviews, there \nwere no keywords of “automatic item generation” or even word variants of “items” in the search \nterms used. As a result, many significant publications in the AIG literature were not included \nin their review. Furthermore, educational measurement researchers have been less familiar with \nAQG due to differences in research interests, technical skill sets, and limited participation in \nNLP conferences. This gap is evident in the minimal overlap between the references and \ncitations used in the two research domains. For example, nearly all references presented in the \nreview paper of Kurdi et al . (2020) were published in computer science venues, while the \nreferences used in a review paper of AIG (Falcao et al ., 2022) were mostly in education, \npsychology, and testing journals. As a result, historically, researchers in both domains have had \nlimited knowledge of each other’s work or were not even aware of each other’s existence. More \nimportantly, the use of different terminology (e.g., “items” vs. “questions”) to describe and \naddress similar problems has created communication barriers between the AIG and AQG \nresearch domains, hindering their advancement. \nMarked as recent breakthroughs in generative artificial intelligence research, pre -trained \nlanguage models (PLMs) and large language models (LLMs) have overcome the limitations of \nearlier autoregressive algorithms (e.g., rule-based systems) which relied on fixed architectures \nfor rigid, sequential text prediction . Unlike traditional autor egressive algorithms, modern \nlanguage models offer the distinct advantage of being retrainable and fine -tunable, enabling \nthem to develop more sophisticated language comprehension and produce more contextually \nappropriate assessment items (e.g., Vu & Van N guyen, 2022). These language models focus \nspecifically on text generation but have also demonstrated their flexibility and extraordinary \nperformance in understanding the context of language and in various language processing tasks \n(Ackerman & Balyan, 2023). The versatility and potential application of the models has have \nsignificantly sparked the interest of both educational measurement researchers and applied NLP \nresearchers in employing them as an innovative methodology for automating the generation of \na large number of items.  Technically, LLMs are essentially scaled -up versions of PLMs, \ncharacterized by an increased number of parameters and larger training data sizes. However, \nthere is no strict cutoff value that differentiates the number of parameters of  PLMs from those \nof LLMs (Zhao et al ., 2023). Therefore, in the remaining sections of this review, the term \n“LLMs” is used to refer to both PLMs and LLMs, as it is a more commonly recognized term. \nAs AIG and AQG researchers are increasingly sharing the sam e goal of generating questions \nfor educational purposes and are also adopting similar methodologies such as the use of LLMs, \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n319 \nthere is substantial decrease in the distinctions between them. Therefore, it has become crucial \nto raise awareness of both AIG and AQG research and map the literature of both fields to gain \na comprehensive understanding of the utility and potential of LLMs. The purpose of this review \nis not only to identify what has been achieved in the literature regarding the use of LLMs for \nAIG but also to explore the knowledge gaps between the two research domains. Additionally, \nwe aim to provide  suggestions for future research  to advance automated item generation \npractices in both fields. To ensure consistent terminology, we will use “AIG” to ref er to both \nAIG and AQG for educational purposes in this paper, as the term “item” is more inclusive than \n“question” (Gierl et al., 2021). However, it is important to note that these terms have distinct \nhistorical origins and development trends, but they ar e used interchangeably here due to their \nsimilar research objectives and the focus on leveraging LLMs in this review. To map the current \nstate of research on leveraging LLMs for AIG, our review is guided by the following research \nquestions (RQs): \nRQ1: What are the most used LLMs for AIG, and how do their underlying architectures and \nfeatures differ? \nRQ2: In what specific ways are LLMs most frequently used, and how do the LLMs’ features \ninfluence their use in the AIG process? \nRQ3: What types of items were generated in the reviewed studies in terms of item type, subject \ndomain, and language? Can LLMs generate valid, reliable, and high-quality items? \n2. METHOD \n2.1. Study Search and Selection  \nThe study search and selection process are displayed in Figure 1 . Keyword searches were \nconducted in seven research databases, which were selected for their comprehensive coverage \nof literature on AIG (Alsubait, 2015) or relevance to LLMs. The searches used two keyword \ncomponents: AIG and LLMs, connected by an \"AND\" operator, as detailed in Table 1. Our \nsearch targeted full texts, with no restrictions on publication types in the search strategy, and \nwas limited to publications after 2018, marking the emergence of pre-trained language models \n(Zhao et al., 2023). The last search was conducted on August 1st, 2023, which yielded 831 \nunique study records from these research databases. \nAfter the study search, two reviewers independently screened the titles, abstracts, and keywords \nof the 831 identified studies, using predefined inclusion criteria for selection. We only retained \nstudies with full texts in English and focused exclusively on empirical studies, excluding \neditorials, commentaries, and position papers. Given that question generation has applications \nin other domains like chatbots and customer service, we confined our selection to studies in the \neducation field. This focus was achieved by identifying education -related words or phrases in \nthe title, abstracts, or keywords. Additionally,  we selected publications that focused on item \ngeneration, excluding other forms of text generation, such as the creation of reading passages \nwithout items generated. After the initial screening, we noted 47 disagreements in selection \ndecisions, resulting in a high agreement rate of 93.44%. The disagreements primarily resulted \nfrom vague mentions of education -related phrases in abstracts from papers published in \ncomputer science venues. These disagreements were resolved through further discussion and \nconsultation of the full texts of the studies, resulting in the selection of 65 studies for full -text \nreview and information extraction. After a detailed review, 5 studies were excluded based on \nour criteria, leaving 60 studies in this review. \n \n \n \n \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n320 \nFigure 1. The flow diagram of search process. \n \nTable 1. Keywords used for study search. \nKeywords related to AIG Keywords related to LLMs \nItem generation*, \nAIG, \nQuestion generation, \nDistractor generation, \nTest development, \nItem development, \nGenerat* Item*, \nGenerat* Question* \nLarge language model, Pre-trained language model, PLM, \nLLM, language model, BERT, GPT*, ChatGPT, T5, mT5, \nPanGu, T0, CodeGen, Tk-Instruct, UL2, OPT, NLLB, \nGLM, Flan-T5, BLOOM, mT0, Galactia, BLOOMZ, OPT-\nIML, LLaMA, CodeGeeX, Pythia, GShard, Codex, ERNIE, \nJurassic-1, HyperCLOVA, FLAN, Yuan, Anthropic, \nWebGPT, Gopher, GlaM, LaMDA, MT-NLG, AlphaCode, \nInstructGPT, Chinchilla, PaLM, AlexaTM, Sparrow, \nWeLM, U-PaLM, Flan-U-PaLM, (Transformer AND \nModel) \nNote: We focused our search on publications written in English and published in or after 2018. We conducted searches using \ntitles, abstracts, and keywords. “*” represents variants of the word, such as plurals. \n2.2. Information Extraction and Analysis \nTo address the proposed research questions, a coding fra mework was developed through an \niterative process to extract information from the included studies. Initially, one reviewer \nproposed a preliminary coding framework and piloted it by extracting information from 20 of \nthe included studies. Refinements to the  coding framework, such as adding new coding \n\nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n321 \nsubsections, removing redundant or irrelevant ones, and defining coding categories within each \nsubsection, were conducted. Corresponding to the RQs, the finalized coding framework \ncomprised three key aspects: (1) the specific LLMs employed in each study, (2) their \nimplementation methods categorized by role in the AIG process, and (3) the characteristics of \ngenerated items. Subsequently, with the coding framework, two reviewers independently coded \nall the studie s after training. During this process, several ambiguities emerged, such as the \ndistinctions between LLM applications (e.g., item filtering versus quality ranking for quality \ncontrol purposes) and classification of LLM roles in multi-stage AIG processes (as LLMs might \nbe employed at multiple stages). To resolve these ambiguities and any coding disagreements, \nthe reviewers first discussed their interpretations, with a third reviewer consulted when \nconsensus could not be reached. In the final analysis phase, one reviewer systematically \nquantified the studies according to each coding category and synthesized the findings. For \ninstance, LLM applications were categorized into three distinct AIG process stages: pre -\ngeneration stage, item generation stage, and post-generation stage.  \nWhile conducting the information extraction, we realized some information was missing in the \ncurrent literature for us to answer our research questions. Particularly, many studies did not \nreport the measurement properties of the generat ed items. Therefore, we employed a simple \nsearch strategy to identify the severity of this issue – searching for keywords that are expected \nto appear in the reviewed studies. We developed a set of basic keywords that are related to the \nmeasurement properties of items as well as individuals who are often in charge of evaluating \nitem quality such as content specialists and SMEs. Then, we built a simple information extractor \nusing the Python programming language and used it to summarize the patterns of occurre nces \nof keywords across the reviewed studies. \n3. RESULTS \nOur review indicated that there were two studies published in 2019, followed by an increase to \nfive in 2020, 9 in 2021, a peak of 33 in 2022, and finally, eleven studies as of August 2023. The \nincluded studies can be found in Appendix A. Over half of these studies ( n = 31) were papers \npublished at conferences, such as the International Conference on Artificial Intelligence in \nEducation and the European Conference on Technology Enhanced Learning. Fourteen of the \nreviewed studies were journal articles, which span two broad research domains, with \neducational technology and assessment journals like Education and Information Technologies \nand International Journal of Assessment Tools in Education, and applied artificial intelligence \njournals such as  Frontiers in Artificial Intelligence and IEEE Access. Moreover, there were \nnon-refereed studies, including two master’s theses, one doctorate dissertation, and 12 \npreprints. These avenues imply the multidisciplinary and developing nature of studies in this \nfield. \n3.1. RQ1: What Are the Most Used LLMs for AIG, and How Do Their Underlying \nArchitectures and Features Differ? \nWe identified Google’s Text-to-Text Transfer Transformer (T5; n = 32), Bidirectional Encoder \nRepresentations from Transformers (BERT; n = 26), and OpenAI’s Generative Pre -Trained \nTransformer (GPT; n = 19) as three major base types of LLMs commonly used in AIG. In \naddition to these most common LLMs, we also found mo dels that were only used once or a \ncouple of times such as BART and PEGASUS. Because there were cases where more than one \nLLM was used in a study, we summarized the frequency of use by instance. That is, each study \ncould be counted multiple times depending on how many LLMs they used. \nThough all follow the transformer -learning paradigm, each type of LLM employs a distinct \napproach to language modeling, encompassing unique features and strengths for specific tasks. \nDue to the transfer learning paradigm, these  LLMs often undergo further training with \nadditional text data to enhance the base model to the desired task. This results in a variety of \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n322 \nvariants and series. For a better understanding of their use in the AIG stages, the features and \nvariants of each base type of LLM are outlined in the following subsections. \n3.1.1. T5 variants \nIntroduced by Raffel et al. in 2020, the T5 model treats all language processing tasks as text -\nto-text conversions. That is, the model receives text input and directly generates text output. \nThe dataset used to train this LLM was the C4 dataset which contains approximately 750GB of \nEnglish texts sourced from the public Common Crawl web scrape (Raffel et al., 2020). The T5 \nmodel is available in several sizes, such as T5-small, T5-base, and T5-large, each differing in \nthe number of parameters. The different sizes of these models enable their application in varied \ncontexts, accommodating diverse requirements in terms of computational resources  and \nperformance capabilities. \n3.1.2. BERT variants  \nBERT (Devlin et al., 2018) employs a bidirectional approach to understanding the context of \nwords within a sentence.  That is, the model analyzes texts from both left to right and right to \nleft, which helps it gain a more comprehensive understanding of sentence contexts. BERT was \noriginally trained using 3.3 billion words sourced from Wikipedia and BooksCorpus, based on \ntwo key training strategies: masked language model and next -sentence prediction. BERT can \nalso be further trained with additiona l text data or training strategies, allowing it to exhibit \ndifferential performance in specific contexts.  For example, multilingual BERT (Devlin et al., \n2019) are trained with additional text data from multiple languages to improve performance in \nmultilingual understanding. The size of training datasets, training architecture, and the number \nof parameters can also lead to variants such as ALBERT (BERT with reduced parameters and \nsentence order prediction; Lan et al., 2019), DistilBERT (BERT with distillatio n; Sanh et al., \n2019), RoBERTa (BERT without NSP but dynamic masking; Liu et al., 2019), and XLNet \n(BERT with all tokens masked but in random order; Yang et al., 2019). \n3.1.3. GPT series  \nGPT (OpenAI, 2018) employs the next-word prediction strategy to generate words sequentially. \nThat is, it predicts the next word in a sentence based on probabilities conditioned on both the \noriginal input it receives (i.e., the prompt) and the words it has previously generated. The series \nof GPT models has seen remarkable advancements over the last few years, with each iteration \nincreasing in parameters and enhancing capabilities and complexity. In this series, GPT -1 \ncontained 117 million parameters, GPT -2 e xpanded to 1.5 billion parameters, while GPT -3 \nfurther increased the scale to 175 billion parameters. At the time of this review, GPT -4 (Open \nAI, 2023) stood as the latest iteration,  reflecting substantial improvements in language \nprocessing, reasoning, an d multimodal functionality. Subsequent models have continued to \nadvance the field, focusing not only on scale but also on real-world usability by optimizing for \nfaster inference speeds, reduced computational costs, and greater affordability for end users.  \nFurthermore, these models have become increasingly accessible, with intuitive platforms \nenabling seamless deployment for both developers and non-technical users. \nBuilding upon the core models of such as GPT -4 and GPT -3, Open AI has developed \nspecialized de rivatives tailored for specific applications.  For example, the well -known \nChatGPT (Open AI, 2022) is specifically fine -tuned to engage in interactive conversational \ntasks. This fine -tuning involves encoding given human language inputs (prompts) into rich, \ncontextual-embedded textual representations that the machine system can understand. \nSubsequently, it decodes these textual representations sequentially, conditioned on the original \nprompts and the previously generated words, to produce coherent and contextually appropriate \nresponses in human languages. Similarly, Codex, built based on GPT-3, is trained to specialize \nin understanding and generating codes of programming languages (Open AI, 2021). \n \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n323 \n3.1.4. Summary of findings for RQ1 \nThe most used base types of LLMs are T5, BERT, and GPT. These models exhibit differences \nin their training data sources, the size of their training sets, their training architectural \nframeworks, and their unique features.  \n3.2. RQ2: In What Specific Ways Are LLMs Most Frequently Used, and How Do the \nLLMs’ Features Influence Their Use in the AIG Process? \nAs depicted in Figure 2, we summarized and synthesized the specific uses of LLMs into three \nstages: pre-generation, item generation, and post-generation. In the pre-generation stage, LLMs \nare used for preparation tasks before generating items such as cleaning, structuring, and \nunderstanding the original input text data which can be lengthy and covers various topics. This \npreparation i s essential for the subsequent item generation stage to produce items that are \ncontextually relevant, accurate, and coherent. In the item generation stage, items are created \neither directly by LLMs or through traditional methods like template-based approaches. Studies \nusing template-based approaches were included because they employed LLMs in either the pre- \nor post-generation stages. After item generation is the post -generation stage, where items are \nevaluated and selected based on certain criteria such as quality and difficulty. With LLMs, this \nstage can be automated, resulting in filtered items as the final output of the AIG process. The \nmajority of these studies (n = 44) employed LLMs in only one of the three AIG stages, 13 \nstudies used LLMs in two stages, and 3 studies used LLMs across all three stages. \nFigure 2. The specific use of LLMs in the AIG process. \n \n3.2.1. Pre-generation stage \nIn the pre-generation stage, BERT was used 20 times, T5 was used 13 times, and GPT was used \n9 times. We identified and categorized the following specific uses of LLMs: (1) text embedding \n(n = 8), (2) key phrase identification ( n = 8), (3) text summarization ( n = 6), (4) word \ntokenization (n = 3), (5) prompt engineering ( n = 2), (6) word sense disambiguation ( n = 2), \n(7) label extraction (n = 1), and (8) discourse segmentation (n = 1). These specific uses make \nup a toolbox for processing texts for subsequent tasks of generating desired items. The specific \nuses of LLMs were categorized into three themes, as described below. \nThe first theme is data transformation, which converts original texts into computationally \ntractable and manageable units. For example, word tokenization is the process of transforming \ntexts into smaller, more manageable tokens. This task is often necessar y before many \nsubsequent preprocessing NLP tasks. LLM tokenizers have several advantages over traditional \nrule-based tokenizers, such as understanding context (Singh et al ., 2019). In addition, text \nembedding transforms text into numerical vectors, known a s textual representations. These \n\nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n324 \nnumerical vectors can capture the semantic meaning of the original texts; texts with similar \nmeanings are positioned closely in the embedding space. These numerical vectors are used and \nprocessed in later language processing tasks. For instance, they can be utilized for subsequent \nNLP tasks such as calculating the cosine similarity metric between items or distractors of \nmultiple-choice questions to evaluate their semantic similarity (e.g., Min et al., 2021). \nThe second theme is related to semantic processing, which involves understanding the context \nof the text data or specific sentences. For example, key phrase identification involves extracting \nimportant and relevant phrases from texts . This step can help to frame  the focus of the item \ngeneration. For example, Tsai et al. (2021) employed BERT to extract keywords from an input \ntextbook, which were then used to construct important complete sentences as preparation for \nthe item generation stage. In addition, word sense disambiguation is the process of determining \nthe actual meaning of a word with multiple meanings in a given sentence and context. This step \nis crucial for computers to interpret words correctly in context, ensuring that later generated \nitems are contextually  appropriate and unambiguous. Moreover, we found that discourse \nsegmentation was performed in two studies. This task divides texts into coherent segments such \nas sentences, paragraphs, or topics, which helps to structure the text and generate items in a \nway that reflects the logical and semantic composition of the input texts. Furthermore, label \nextraction assigns labels to texts according to categories like reading difficulty, content domain, \nor question type. The extracted labels serve as control labels for cre ating targeted and relevant \nitems. For example, Zhao et al. (2022) trained an LLM to extract one of seven question types \nfrom input texts, which was then used to generate questions matching the intended type. Lastly, \ntext summarization aims to convey the m ain points of the original text while significantly \nreducing its length, thereby improving the efficiency of subsequent item -generation tasks \n(Malhar et al., 2022). \nThe last theme is prompt engineering, which involves constructing commands or instructions \nthat effectively communicate a task to LLMs. Some generative artificial intelligence such as \nGPT operate based on the prompt it receives. In AIG, the characteristics and quality of the \nprompt determine how LLMs generate items that meet specific assessment criteria, cater to \nvarious contexts, or assess across different domains. Therefore, researchers have used LLMs to \ngenerate effective prompts for subsequent tasks. For instance, in the study by Ghanem et al. \n(2022), T5 was trained on how to formulate questions and consider relevant aspects for \nsubsequent item generation. \n3.1.2. Item generation stage \nWe identified that 53 studies used LLMs during the item generation stage. Some studies used \nmultiple LLMs, either to complement each other in different tasks or to compare and identify \nthe best-performing models in the same task. Despite BERT and GPT-2 being introduced earlier \nthan T5, the latter emerged as the most frequently employed LLM for item generation, being \nused 33 times. For example, Akyön et al. (2022) trained a variant of T5 (mT5) to first extract \nanswers and then used these extracted answers as text input to generate questions as text output. \nGPT also comes as a popular option, having been used 15 times. GPT-variants predict the next \nword in a text sequence, enabling prompt -based generation. For instance, Wang et al. (2022) \ncompared the performance of GPT-3 in item generation when using different prompts. BERT \nwas used in 9 instances. Given BERT’s characteristics, BERT variants were employed to \npredict masked tokens, creating specific item types such as fill-in-the-blank and cloze questions \n(e.g., Matsumori et al., 2023). In addition, we identified instances where other LLMs, such as \nBART and PEGASUS, were used.  \n3.1.3. Post-generation stage \nTo ensure the quality and relevance of the generated items for a given educational context, \nLLMs can be used in the post -generation stage to evaluate, filter, or classify the previously \ngenerated items. Among them, evaluation was conducted by calculating specific criteria or \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n325 \nmetrics. For instance, Jiao et al. (2023) utilized GPT-2 to calculate perplexity values, reflecting \nthe fluency of the generated items. They also employed BERT -large for coherence evaluation \nand BertScore for assessing creativity by calculating semantic differences among items. Having \nthe evaluation metrics calculated, filtering involves remov ing low -quality or irrelevant \nquestions or distractors of multiple -choice questions that were generated by the LLM. For \nexample, Offerijns et al. (2020) used GPT -2 to generate question-answer pairs and then used \nBERT to remove the questions that could not be answered or did not make sense. Lastly, \nclassification refers to categorizing questions by type or topic. For example, Nguyen et al . \n(2022) first employed T5 to generate questions and then used GPT-3 to classify these questions \nbased on their utility in learning specific topics. \n3.1.4. Summary of findings for RQ2 \nWe revealed distinct usage patterns of BERT, GPT, and T5 across the three AIG stages, \nreflecting the inherent features and strengths of these LLMs. As BERT excels at language \nunderstanding, it w as predominantly utilized in the pre -generation stage. On the other hand, \nGPT was primarily used in the item -generation stage, highlighting its strength in text \ngeneration. T5 demonstrated its flexibility by being used comparably in all three AIG stages. \n3.3. RQ3: What Types of Items Were Generated in the Reviewed Studies in Terms of Item \nType, Subject Domain, and Language? Can LLMs Generate Valid, Reliable, and High -\nQuality Items? \n3.3.1. Item type \nWe found 39 studies that generated constructed-response items; 29 studies generated selected-\nresponse items. Eight studies generated both types. The constructed -response items can be \nfurther categorized as Wh-questions, cloze questions, and Fill-in-the-blank questions, whereas \nselected-response items included True -False and multiple -choice questions, including their \ndistractor generation.  \n3.3.2. Subject domain \nOur review revealed that the majority of items generated primarily focus on two subject \ndomains: language learning ( n = 24) and general knowledge acquisition ( n = 17). General \nknowledge items are often developed using general datasets, such as the Stanford Question \nAnswering Dataset (SQuAD), which contains passages from Wikipedia and thus does not focus \non a specific subject domain. Following closely behind are science -related disciplines (n = 9) \nsuch as agronomy, biology, chemistry, physics, and science history. In addition, computer \nscience and mathematics education, had five and four studies on item generation, respectively. \nWe also found studies addressing other subject domains, including social science, medicine, \nand even literary experiences such as fairy tales.  \n3.3.3. Language  \nOverall, we identified a total of 12 different languages in the item s generated. This finding \nsuggests the potential for generalizability and the wide linguistic context in which LLMs can \nbe utilized for AIG. In most instances, items were generated in English (n = 51), but there were \nalso instances of generation in other languages, including Arabic, Chinese, French, German, \nHindi, Indonesian, Korean, Lao, Marathi, Spanish, Swedish, Turkish, and Vietnamese. \n3.3.4. Data source \nLLMs are typically developed for general language processing purposes and often require \nadditional training to effectively perform specific tasks, such as generating context -relevant \nquestions. Such additional training involves training LLMs on new datasets to adapt them to \ndifferent tasks or content domains. In the reviewed studies, the most commonly utilized datasets \nfor additional training have included SQuAD ( n = 24), which encompasses a collection of \npassages with corresponding reading comprehension questions, and the ReAding \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n326 \nComprehension dataset from Examinations (RACE) dataset (n = 10), which focuses on English \nlanguage exams. Additionally, some researchers have crafted their self -collected datasets by \naggregating educational materials. These materials consisted of open -access journal articles \n(e.g., von Davier 2019), questions extracted from onli ne learning platforms or communities \n(e.g., Stack Overflow; Tsai et al ., 2021), LLM -generated texts (Bulut & Yildirim -Erbasli, \n2022), teacher-created questions (Matsumori et al., 2023), stories and fairy tales (e.g., Ghanem \net al. 2022), knowledge maps (Aigo et al., 2021), slides (Chughtai et al., 2022), textbooks (e.g., \nSteuer et al. 2020), and other course materials (e.g., Gopal, 2022). \n3.3.5. Item properties  \nAs noted in the methodology section, we did not find many studies reporting the measurement \nproperties of the generated items. Therefore, instead, we searched for keywords pertinent to \nmeasurement properties across the 60 reviewed studies. Figure 3 depicts the number of studies \ncontaining each keyword. Notably, only 10 out of the 60 studies mentioned “validity”. This was \nfollowed by “reliability” and “pedagogical”, which found their places in eight studies. Other \nkeywords were used even less frequen tly in the reviewed studies. The infrequent occurrences \nof these keywords signal a concerning issue: the majority of the reviewed studies seem to \nneglect measurement properties of items when generating items for educational purposes, \nwhich potentially impacts the validity and reliability of the assessment results. Moreover, this \ncould result in generating questions that are too simple and do not require higher cognitive \nthinking to answer, failing to meet the measurement or pedagogical purposes (e.g., deliv ering \nfeedback or evaluating achievement). \nWe further extracted sentences containing the keywords. However, we found that many \noccurrences of these keywords were not related to the measurement properties of the generated \nitems. For instance, most descriptions of “reliability” were in contexts other than the reliability \nof items or assessment results. They commonly referred to terms like “inter -annotator \nreliability” and “the reliability of the data collection”. Similarly, the instances of “fairness” \nwere exclusively related to the fairness of experiments comparing model performance, rather \nthan to the fairness of the assessment items themselves. Therefore, the issue of lacking sufficient \nconsideration for the measurement properties of items may be even more severe than it appears \nin Figure 3. \nFigure 3. Number of studies containing each keyword. \n \n\nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n327 \n3.3.6. Summary of findings for RQ3 \nWe found that LLMs can be an effective and flexible solution to generating a large number of \nitems, with few constraints on item type, language, subject domain, or the data source used for \ntraining LLMs to create items. However, we did not find many studies reporting the \nmeasurement properties of the generated items. \n4. DISCUSSION and CONCLUSION \n4.1. The Current State of Research on AIG  \nAs a summary of the findings from this review, we identified the most commonly used LLMs \nin the current AIG literature, such as T5, BERT, GPT, and their variants. We described the \ncharacteristics and features of each base type of LLM, linking them to their specific uses in the \nAIG process. LLMs used in the pre -generation stage focus on preparing, processing, and \nunderstanding texts for subsequent item generation to ensure high quality. In the post -\ngeneration stage, LLMs are primarily used to filter out low-quality items (i.e., mostly focusing \non the correctness of grammar and syntax as well as the semantic relevancy and similarity) or \ndetermine the usefulness of the generated items.  \nAfter reviewing the existing stu dies, we conclude that LLMs prove useful in generating large \nbanks of items. Additionally, we revealed that LLMs offer a highly flexible solution for AIG, \nas they have virtually no constraints in terms of item type, language, the subject domain of the \nitems to be generated, or the data source used for further training of LLMs. \n4.2. Current Research Gaps in the Literature \nWhile the reviewed studies often show that LLMs are effective and flexible in creating a large \nnumber of items, we found that many studies  applying LLMs in AIG often lacked a solid \neducational foundation. This might be because many of the authors were NLP researchers who \npossessed limited recognition and knowledge of learning or measurement theories. \nAlternatively, it could be because creati ng high -quality items that are readily usable for \neducational contexts was not their primary interest or research focus. Accordingly, many of \nthose items are generated without deep consideration of their measurement purposes and item \nproperties, which are essential to meet the requirements of educational assessment. \nIn traditional item development or template -based AIG, item generation starts with a clear \ndefinition of what to measure (i.e., identifying the construct to be measured by considering the \nexpected learner outcomes and instructional objectives), why to measure (i.e., the purpose of \nassessment), and how to measure (i.e., assessment design and item format), while only a few \nstudies leveraging LLMs for AIG considered these important aspects. For exam ple, many of \nthose generated items in the existing studies do not attempt to evaluate the higher -level \ncognitive processes specified in Bloom’s taxonomy, such as applying, analyzing, evaluating, \nor creating. This is because they are mostly created by resea rchers from the AQG area whose \noriginal purpose was to develop chatbots and conversational agents. Therefore, their item \ngeneration predominantly focuses on the levels of remembering or understanding, similar to the \ngoal of conversational agents, which does not always meet or fit the measurement purposes and \ngoals that are to evaluate the complex learning progress and outcomes of human students. \nMoreover, while some studies invited human participants to evaluate the quality of the items \nafter being generated, only a few involved SMEs or measurement specialists in the AIG process. \nHowever, human experts are crucial for guiding item development, ens uring quality, and \npotentially enhancing students’ learning outcomes through formative use (e.g., Gierl & Lai, \n2012; Lu et al., 2021). The absence of expert guidance has led many existing AIG studies using \nLLMs to overlook important measurement properties such as item difficulty and item \ndiscrimination. Thus, the current literature mostly provides evidence that LLMs can be \nleveraged to generate a large number of items, but little is known about whether these items \npossess the high quality necessary for educ ational purposes such as pedagogical teaching and \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n328 \nassessment. Considering the goal of AIG, which is to generate large banks of high-quality items \nwhile reducing overall costs (Gierl & Lai, 2012; Lai et al., 2009), we argue that a thorough item \nevaluation after generation is missing in the current literature. That is, AIG does not end up \ngenerating a large number of items but ensuring that these items are of high quality and can \nfulfill their demands for educational purposes and contexts. Thus, we argue that  AIG is still a \ndeveloping and promising research domain, and its further development and application will \ndepend on the involvement of human experts and the integration of learning and measurement \ntheories. \n4.3. Suggestions for Future Research \nThe current research gaps, particularly the lack of educational foundations in the reviewed AIG \nstudies, led us to advocate for two suggestions for future research in this area.  \nFirst, future research should prioritize clarifying the assessment context and measurement goals \nin AIG applications. While AIG benefits from advancements in NLP, such as LLMs, it differs \nfrom general text generation by creating items that meet the specific purposes of assessments \nin real educational contexts. Newton (2007) distinguished 18 d ifferent educational assessment \napplications, such as formative, diagnostic, qualification certification, and comparative \npurposes. For example, formative assessments take place during students’ learning processes \nand support learning by providing effectiv e feedback, while summative assessments aim to \ncollect information about students’ learning outcomes after the learning process has occurred. \nAs the assessment purposes differ, the desired characteristics of the generated items also differ. \nWhen generating items for a specific educational and assessment context, researchers must \nidentify the nature and desired characteristics of these items. For instance, in the case of \nformative assessments aimed at aiding students’ learning, items should prioritize pedago gical \nvalue by providing feedback to help identify misconceptions. To assess students’ learning \noutcomes following a teaching program, items should be balanced and cover a broad range of \nkey concepts taught, rather than disproportionately focusing on conce pts within a narrow \ncontent area. Furthermore, regardless of whether items are formative or summative or created \nfor other assessment purposes, they must demonstrate high quality, with strong reliability and \nvalidity, to be effective, because they directly  impact the feedback provided to students or the \nability to draw inferences about students’ learning progress and outcomes. \nSecond, we recommend evaluating both the measurement properties and pedagogical \nsoundness of generated items as an essential step in AIG. From a practical standpoint, item \ndevelopment does not end with item generation; the evaluation of item quality is crucial to \nensure usability in educational contexts. For example, measurement properties of items (e.g., \ndifficulty and discrimination parameters) and tests (e.g., reliability and validity) can be \nexamined using measurement theories such as classical test theory and item response theory. \nFinally, after items are generated and evaluated for their measurement properties, the focus \nshould sh ift back to meeting the intended measurement purposes or pedagogical value of \nassessments. Educators and SMEs can assess the educational or pedagogical value of \nautomatically generated items. It is important to ensure that the generated items can effectively \nserve their intended purpose. If not, it is necessary to revisit previous item development stages \nfor revisions to create items that better align with the learning objectives and measurement \ngoals. \nThe three-stage AIG framework emerging from our analysi s (i.e., pre -generation stage, item \ngeneration stage, post-generation stage) offers a structured approach for integrating these two \nrecommendations into practice. This framework not only categorizes LLM applications but also \nhas a potential for seamlessly integrating human expertise for a human-in-the-loop approach to \nstrengthen educational foundations of AIG practices. In the pre-generation stage, which focuses \non preparation tasks before item generation, AIG practitioners can enhance outcomes by \nincorporating human expertise. For example, they may define assessment goals and formats or \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n329 \ncontribute to model fine-tuning by providing context-relevant educational materials (e.g., item \nbanks). Such practices have been shown to improve both the model’s contextual awareness and \ntask-specific performance (Ghanem et al ., 2022; Ratcheva et al ., 2022). During the item \ngeneration stage, human experts (e.g., educators) can refine prompts to elicit more appropriate \nitems aligned with specific learning or assessment goals. Finally, in the post -generation stage, \nsubject-matter experts, students (as target test -takers), and even LLMs the mselves may \nparticipate in evaluating the generated items. \nWhile not explicitly framed in these terms, Sayin and Gierl's (2024) study demonstrates strong \nalignment with this three -stage approach when analyzed through our conceptual framework. \nTheir methodo logy began with what we would characterize as pre -generation preparation, \ninvolving the development of three foundational models: a cognitive model to define target \nknowledge and skills, an item model to establish construction guidelines, and a specialized text \nanalysis model to ensure passage coherence and plausible distractors. These models then \ninformed precise prompt engineering for GPT -3.5, enabling systematic generation of 12,500 \ngrade-level items for high-stakes examinations - a process corresponding to our item generation \nstage. Finally, their comprehensive quality assurance procedures, including expert reviews and \nempirical field testing to assess critical psychometric properties like item difficulty and \ndiscrimination indices, exemplify what our framework identifies as post-generation validation. \n4.4. Limitations \nWe did not conduct a formal assessment of the methodological quality of the included studies, \nwhich is generally considered a limitation of scoping reviews. This is because the primary \npurpose of this study, and of scoping reviews in general, is to map the existing literature on a \nparticular topic to explore the range, nature, and extent of research activities related to the topic, \nrather than to evaluate the quality of the studies (Arksey &  O'Malley, 2005). Additionally, we \nacknowledge that the field of leveraging LLMs is rapidly evolving. As we completed our \nkeyword search, new LLMs have been introduced, boasting significantly larger parameter sizes \nand enhanced functionalities. This rapid development has not only expanded the utility and \npotential of LLMs but has also made advanced technology more accessible to users with \nvarying levels of programming skills. Consequently, the use of LLMs in education continues \nto evolve, and understanding how these models can best serve the AIG process remains an \nongoing journey. \n4.5. Conclusion \nAs a category of generative artificial intelligence technology, LLMs focus on language \nunderstanding and text generation, which has sparked researchers' interest in  using them to \nautomatically generate questions for educational purposes. Through the mapping and synthesis \nof the reviewed studies on leveraging LLMs for AIG, we identified that the most commonly \nused LLMs are T5, BERT, GPT, and their variants. We have al so categorized the current \napplications of LLMs into three stages of the AIG process: pre-generation, item generation, and \npost-generation. The findings reveal that using LLMs to generate items is an effective and \nflexible solution, with few constraints on item type, language, subject domain, or the data source \nused for training LLMs to create items. Due to the exceptional language understanding abilities \nof LLMs, the generated items are typically free from grammar errors and contextually relevant \nto the desired content domain. However, we also noted a lack of a solid educational foundation \nin many of the reviewed studies, as they did not incorporate learning and measurement theories \ninto the item generation process. We attribute this issue to the absence of involvement of human \nexperts such as SMEs and measurement specialists. Although one part of the goal of AIG was \nto reduce financial costs and human burdens, it was never meant to exclude human involvement \nfrom the item development process. Importantly, AIG is still considered an augmented \nintelligence approach, which means it requires both human expertise and the capabilities of a \ncomputer. Considering the goal of AIG, we not only want to generate a large number of items \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n330 \nbut also care more about the item quality and characteristics. Hence, future researchers should \nconsider enhancing the educational foundation in the AIG process. The three-stage framework \nproposed in this review provides a structured approach to integrating human expertise at each \nstage—pre-generation, item generation, and post -generation—to ensure high -quality item \ngeneration. This way, LLMs can be leveraged to produce items that are not only grammatically \ncorrect and contextually relevant but also reliable, valid, and pedagogically sound. \nAcknowledgments \nThis work has been previously presented at the 2024 National Council on Measurement in \nEducation (NCME) Annual Meeting, Philadelphia, Pennsylvania. \nDeclaration of Conflicting Interests and Ethics \nThe authors declare no conflict of interest. This research study complies with research \npublishing ethics. The scientific and legal responsibility for manuscripts published in IJATE \nbelongs to the authors.  \nContribution of Authors \nBin Tan: Conceptualization, methodology, literature search, article screening, formal analysis \nand investigation, writing – original draft preparation, writing – review and editing . Nour \nArmoush: Conceptualization, methodology, literature search, article screening, formal analysis \nand investigation, writing – original draft pr eparation. Elisabetta Mazzullo: Formal analysis \nand investigation, writing – original draft preparation . Okan Bulut:  Conceptualization, \nmethodology, writing – review and editing, supervision. Mark Gierl:  Conceptualization, \nmethodology, writing – review and editing, supervision. \nOrcid \nBin Tan   https://orcid.org/0000-0001-6717-5620 \nNour Armoush   https://orcid.org/0009-0008-2310-5098 \nElisabetta Mazzullo   https://orcid.org/0009-0008-4847-9934 \nOkan Bulut   https://orcid.org/0000-0001-5853-1267 \nMark J. Gierl   https://orcid.org/0000-0001-6717-5620 \nREFERENCES \nAckerman, R., & Balyan, R. (2023). Automatic multilingual question generation for health data \nusing LLMs. In International Conference on AI -generated Content (pp. 1-11). Singapore: \nSpringer Nature Singapore. https://doi.org/10.1007/978-981-99-7587-7_1 \nAgrawal, A., & Shukla, P. (2023). Context aware automatic subjective and objective question \ngeneration using Fast Text to text transfer learning. International Journal of Advanced \nComputer Science and Applications, 14(4), 456-463.  \nAigo, K., Tsunakawa, T., Nishida, M., & Nishimura, M. (2021). Question generation using \nknowledge graphs with the T5  language model and masked self -attention. In 2021 IEEE \n10th Global Conference on Consumer Electronics (pp. 85-87). IEEE. https://doi.org/10.11\n09/GCCE53005.2021.9621874 \nAkyön, F.Ç., Cavusoglu, A.D.E., Cengiz, C., Altinuç, S.O., & Temizel, A. (2022). Automated \nquestion generation and question answering from Turkish texts. Turkish Journal of \nElectrical Engineering and Computer Sciences, 30(5), 1931-1940. https://doi.org/10.55730\n/1300-0632.3914 \nAlsubait, T., Parsia, B., & Sattler, U. (2016). Ontology -based multiple choice question \ngeneration. KI-Künstliche Intelligenz , 30, 183 -188. https://doi.org/10.1007/s13218-015-\n0405-9 \nAlves, C.B., Gierl, M.J., & Lai, H. (2010, April). Using automated item generation to promote \ntest design and development  [Paper presentation]. American Educational Research \nAssociation Annual Meeting, Denver, CO, United States. \n\nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n331 \nArksey, H., & O'malley, L. (2005). Scoping studies: towards a methodological framework. \nInternational Journal of Social Research Methodology, 8(1), 19-32. https://doi.org/10.1080\n/1364557032000119616 \nAttali, Y., Runge, A., LaFlair, G.T., Yancey, K., Goodwin, S., Park, Y., & Von Davier, A.A. \n(2022). The interactive reading task: Transformer -based automatic item generation. \nFrontiers in Artificial Intelligence, 5, 903077. https://doi.org/10.3389/frai.2022.903077 \nBerger, G., Rischewski, T., Chiruzzo, L., & Rosá, A. (2022). Generation of Engl ish question \nanswer exercises from texts using transformers-based models. In 2022 IEEE Latin American \nConference on Computational Intelligence (pp. 1 -5). IEEE. https://doi.org/10.1109/LA-\nCCI54402.2022.9981171 \nBlack, P., & Wiliam, D. (1998). Assessment and classroom learning. Assessment in Education: \nPrinciples, Policy & Practice, 5(1), 7-74. https://doi.org/10.1080/0969595980050102 \nBulathwela, S., Muse, H., & Yilmaz, E. (2023). Scalable educational question generation with \npre-trained language models. In International  Conference on Artificial Intelligence in \nEducation (pp. 327-339). Cham: Springer Nature Switzerland. https://doi.org/10.1007/978-\n3-031-36272-9_27 \nBulut, O., & Yildirim -Erbasli, S.N. (2022). Automatic story and item generation for reading \ncomprehension assessments with transformers. International Journal of Assessment Tools \nin Education, 9(Special Issue), 72-87. https://doi.org/10.21449/ijate.1124382 \nBulut, O., Gorgun, G., Yildirim‐Erbasli, S.N., Wongvorachan, T., Daniels, L.M., Gao, Y., ... & \nShin, J. (2023). Standing on the shoulders of giants: Online formative assessments as the \nfoundation for predictive learning analytics models. British Journal of Educational \nTechnology, 54(1), 19-39. https://doi.org/10.1111/bjet.13276 \nCh, D.R., & Saha, S.K. (2018). Automatic multiple choice question generation from text: A \nsurvey. IEEE Transactions on Learning Technologies, 13(1), 14-25. https://doi.org/10.110\n9/TLT.2018.2889100 \nChiang, S.H., Wang, S.C., & Fan, Y.C. (2024). Cdgp: Automatic cloze distractor generation \nbased on pre-trained language model. arXiv preprint arXiv:2403.10326. https://doi.org/10.\n18653/v1/2022.findings-emnlp.429 \nChughtai, R., Azam, F., Anwar, M.W., But, W.H., & Farooq, M.U. (2022). A lecture centric \nautomated distractor generation for post-graduate software engineering courses. In 2022 \nInternational Conference on Frontiers of Information Technology (FIT)  (pp. 100 -105). \nIEEE. https://doi.org/10.1109/FIT57066.2022.00028 \nChung, H.L., Chan, Y.H., & Fan, Y.C. (2020). A BERT -based distractor generation scheme \nwith multi-tasking and negative answer training strategies. arXiv preprint arXiv:2010.0538\n4. https://arxiv.org/abs/2010.05384 \nDalby, D., & Swan, M. (2019). Using digital technology to enhance formative assessment in \nmathematics classrooms. British Journal of Educational Technology, 50 (2), 832 -845. \nhttps://doi.org/10.1111/bjet.12606 \nDembitzer, L., Zelikovitz, S., & Kettler, R.J. (2017). Designing computer-based assessments: \nMultidisciplinary findings and student perspectives. International Journal of Educational \nTechnology, 4(3), 20-31. https://educationaltechnology.net/ijet/index.php/ijet/article/view/4\n7 \nDesai, T. (2021). Discourse parsing and its application to question generation  [Unpublished \ndissertation]. The University of Texas at Dallas. \nDevlin, J., Chang, M.W., Lee, K., & Toutanova, K. (2018). Bert: Pre -training of deep \nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805 . \nhttps://doi.org/10.48550/arXiv.1810.04805 \nDijkstra, R., Genç, Z., Kayal, S., & Kamps, J. (2022). Reading comprehension quiz generation \nusing generative pre -trained transformers. In S. Sosnovsky, P. Brusilovsky, & A. L an \n(Eds.), Proceedings of the Fourth International Workshop on Intelligent Textbooks \n2022 (pp. 4–7). CEUR-WS. http://ceur-ws.org/Vol-3192/ \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n332 \nDrori, I., Zhang, S., Shuttleworth, R., Tang, L., Lu, A., Ke, E., ... & Strang, G. (2022). A neural \nnetwork solves, explains, and generates university math problems by program synthesis and \nfew-shot learning at human level. Proceedings of the National Academy of Sciences , \n119(32), e2123433119. https://doi.org/10.1073/pnas.2123433119 \nFalcão, F., Costa, P., & Pêgo, J.M. (2022). Feasibility assurance: a review of automatic item \ngeneration in medical assessment. Advances in Health Sciences Education, 27(2), 405-425. \nhttps://doi.org/10.1007/s10459-022-10092-z \nFemi, J.G., & Nayak, A.K. (2022). EQGTL: An Ensemble Model for Relevant Question \nGeneration using Transfer Learning. In 2022 Internat ional Conference on Machine \nLearning, Computer Systems and Security (pp. 253-258). IEEE. https://doi.org/10.1109/M\nLCSS57186.2022.00054 \nFuadi, M., & Wibawa, A.D. (2022). Automatic question generation from indonesian texts using \ntext-to-text transformers. In 2022 International Conference on Electrical and Information \nTechnology (IEIT) (pp. 84-89). IEEE. https://doi.org/10.1109/IEIT56384.2022.9967858 \nFung, Y.C., Kwok, J.C.W., Lee, L.K., Chui, K.T., & U, L.H. (2020). Automatic question \ngeneration system for english reading comprehension. In Technology in Education. \nInnovations for Online Teaching and Learning: 5th International Conference, ICTE 2020, \nMacau, China, August 19 -22, 2020, Revised Selected Papers 5  (pp. 136 -146). Springer \nSingapore. https://doi.org/10.1007/978-981-33-4594-2_12 \nFung, Y.C., Lee, L.K., & Chui, K.T. (2023). An automatic question generator for Chinese \ncomprehension. Inventions, 8(1), 31. https://doi.org/10.3390/inventions8010031 \nGhanem, B., Col eman, L.L., Dexter, J.R., von der Ohe, S.M., & Fyshe, A. (2022). Question \ngeneration for reading comprehension assessment by modeling how and what to ask. arXiv \npreprint arXiv:2204.02908. https://doi.org/10.48550/arXiv.2204.02908 \nGierl, M.J., & Lai, H. (2012). The role of item models in automatic item generation. \nInternational Journal of Testing, 12(3), 273-298. https://doi.org/10.1080/15305058.2011.6\n35830 \nGierl, M.J., & Lai, H. (2015). Automatic item generation. In Handbook of test development (pp. \n410-429). Routledge. \nGierl, M.J., & Lai, H. (2016). A process for reviewing and evaluating generated test items. \nEducational Measurement: Issues and Practice, 35(4), 6-20. https://doi.org/10.1111/emip.\n12129 \nGierl, M.J., Lai, H., & Tanygin, V. (2021). Advanced methods in automatic item generation . \nRoutledge. \nGodslove, J.F., & Nayak, A.K. (2023). Ge nerative model for formulating relevant questions \nand answers using transfer learning. In AIP Conference Proceedings (Vol. 2819, No. 1). AIP \nPublishing. https://doi.org/10.1063/5.0136892 \nGopal, A. (2022). Automatic question generation for Hindi and Marathi. In 2022 International \nConference on Advanced Learning Technologies (ICALT)  (pp. 19 -21). IEEE. \nhttps://doi.org/10.1109/ICALT55010.2022.00012 \nGoyal, R., Kumar, P., & Singh, V.P. (2023). Automated question and answer generation from \ntexts using text -to-text transformers. Arabian Journal for Science and Engineering , 1-15. \nhttps://doi.org/10.1007/s13369-023-07840-7 \nGranić, A. ( 2022). Educational technology adoption: A systematic review. Education and \nInformation Technologies, 27(7), 9725-9744. https://doi.org/10.1007/s10639-022-10951-7 \nGrover, K., Kaur, K., Tiwari, K., Rupali, & Kumar, P. (2021). Deep learning based question \ngeneration using t5 transformer. In Advanced Computing: 10th International Conference, \nIACC 2020, Panaji, Goa, India, December 5 –6, 2020, Revised Selected Papers, P art I 10  \n(pp. 243-255). Springer Singapore. https://doi.org/10.1007/978-981-16-0401-0_18 \nHan, Z. (2022). Unsupervised multilingual distractor generation for fill-in-the-blank questions \n[Unpublished thesis]. Uppsala University. \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n333 \nJiao, Y., Shridhar, K., Cui, P., Zhou, W., & Sachan, M. (2023). Automatic educational question \ngeneration with difficulty level controls. In International Conference on Artificial \nIntelligence in Education  (pp. 476 -488). Cha m: Springer Nature Switzerland. \nhttps://doi.org/10.1007/978-3-031-36272-9_39 \nKalpakchi, D., & Boye, J. (2021). BERT -based distractor generation for Swedish reading \ncomprehension questions using a small -scale dataset. arXiv preprint arXiv:2108.03973 . \nhttps://doi.org/10.48550/arXiv.2108.03973 \nKasakowskij, R., Kasakowskij, T., & Seidel, N. (2022). Generation of multiple true false \nquestions. 20. Fachtagung Bildungstechnologien. https://doi.org/10.18420/delfi2022-026 \nKhandait, K., Bhura, S., & Asole, S.S. (2022). Automatic question generation through word \nvector synchronization using lamma. Indian Journal of Computer Science and Engineering, \n13(4), 1083-1095. https://doi.org/10.21817/indjcse/2022/v13i4/221304046 \nKosh, A.E., Simpson, M.A., Bickel, L., Kellogg, M., & Sanford‐Moore, E. (2019). A cost –\nbenefit analysis of automatic item generation. Educational Measurement: Issues and \nPractice, 38(1), 48-53. https://doi.org/10.1111/emip.12237 \nKumar, A., Kharadi, A., Singh, D., & Kumari, M. (2021). Automatic question -answer pair \ngeneration using deep learning. In 2021 Third International Conference on Inventive \nResearch in Computing Applications (pp. 794-799). IEEE. https://doi.org/10.1109/ICIRCA\n51532.2021.9544654 \nKumar, N.S., Mali, R., Ratnam, A., Kurpad, V., & Magapu, H. (2022). Identification and \naddressal of knowledge gaps in students. In 2022 3rd International Conference for Emerging \nTechnology (pp. 1-6). IEEE. https://doi.org/10.1109/INCET54531.2022.9824483 \nKumar, S., Chauhan, A., & Kumar C, P. (2022). Learning enhancement using question-answer \ngeneration for e-book using contrastive fine-tuned T5. In International Conference on Big \nData Analytics (pp. 68-87). Cham: Springer Nature Switzerland. https://doi.org/10.1007/97\n8-3-031-24094-2_5 \nKumari, V., Keshari, S., Sharma, Y., & Goel, L. (2022). Context -based question answering \nsystem with suggested questions. In 2022 12th International Conference on Cloud \nComputing, Data Science & Engineering (pp. 368-373). IEEE. https://doi.org/10.1109/Con\nfluence52989.2022.9734207 \nKuo, C.Y., & Wu, H.K. (2013). Toward an integrated model for designing assessment systems: \nAn analysis of the current status of computer -based assessments in science. Computers & \nEducation, 68, 388-403. https://doi.org/10.1016/j.compedu.2013.06.002 \nKurdi, G., Leo, J., Parsia, B., Sattler, U., & Al -Emari, S. (2020). A systematic review of \nautomatic question generation for educational purposes. International Journal of Artificial \nIntelligence in Education, 30, 121-204. https://doi.org/10.1007/s40593-019-00186-y \nLai, H., Alves, C., & Gierl, M.J. (2009). Using automatic item generation to address item \ndemands for CAT. In D.J. Weiss (Ed.), Proceedings of the 2009 GMAC Co nference on \nComputerized Adaptive Testing. www.psych.umn.edu/psylabs/CATCentral/ \nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2019). Albert: A lite \nbert for self -supervised learning of language representations. arXiv preprint \narXiv:1909.11942. https://doi.org/10.48550/arXiv.1909.11942 \nLee, H., Chung, H.Q., Zhang, Y., Abedi, J., & Warschauer, M. (2020). The effectiv eness and \nfeatures of formative assessment in US K -12 education: A systematic review. Applied \nMeasurement in Education, 33(2), 124-140. https://doi.org/10.1080/08957347.2020.17323\n83 \nLim, Y.S. (2019). Students’ perception of formative assessment as an instructional tool in \nmedical education. Medical Science Educator, 29(1), 255-263. https://doi.org/10.1007/s40\n670-018-00687-w \nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., ... & Stoyanov, V. (2019). Roberta: A \nrobustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 . \nhttps://doi.org/10.48550/arXiv.1907.11692 \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n334 \nLu, O.H.T., Huang, A.Y.Q., Tsai, D.C.L., & Yang, S.J.H. (2021). Expert -authored and \nmachine-generated short- answer questions for assessing students’ learning performance. \nEducational Technology & Society, 24(3), 159–173. https://www.jstor.org/stable/27032863 \nMaheen, F., Asif, M., Ahmad, H., Ahmad, S., Alturise, F., Asiry, O., & Ghadi, Y.Y. (2022). \nAutomatic computer science domain multiple -choice questions generation based on \ninformative sentences. PeerJ Computer Science, 8, e1010. https://doi.org/10.7717/peerj-\ncs.1010 \nMalhar, A., Sawant, P., Chhadva, Y., & Kurhade, S. (2022). Deep learning -based Answering \nQuestions using T5 and Structured Question Generation System. In 2022 6th International \nConference on Intelligent Computing and Control Systems  (pp. 1544 -1549). IEEE. \nhttps://doi.org/10.1109/ICICCS53718.2022.9788264 \nMathur, A., & Suchithra, M. (2022). Application of abstractive summarization in multiple \nchoice question generation. In 2022 International Conference on Computational Intelligence \nand Sustainable Engineering Solutions (pp. 409-413). IEEE. https://doi.org/10.1109/CISE\nS54857.2022.9844396 \nMatsumori, S., Okuoka, K., Shibata, R., Inoue, M., Fukuchi, Y., & Imai, M. (2023). Mask and \ncloze: Automatic open cloze question generation using a masked  language model. IEEE \nAccess, 11, 9835-9850. https://doi.org/10.1109/ACCESS.2023.3239005 \nMaurya, K.K., & Desarkar, M.S. (2020). Learning to distract: A hierarchical multi -decoder \nnetwork for automated generation of long distractors for multiple -choice questions for \nreading comprehension. In Proceedings of the 29th ACM international conference on \ninformation & knowledge management (pp. 1115-1124). https://doi.org/10.1145/3340531.3\n411997 \nMazzullo, E., Bulut, O., Wongvorachan, T., & Tan, B. (2023). Learning Analytics in the Era of \nLarge Language Models. Analytics, 2(4), 877-898. https://doi.org/10.3390/analytics204004\n6 \nMin, B., Ross, H., Sulem, E., Veyseh, A.P.B., Nguyen, T.H., Sainz, O., ... & Roth, D. (2023). \nRecent advances in natural language processing via large pre -trained language models: A \nsurvey. ACM Computing Surveys, 56(2), 1-40. https://doi.org/10.1145/3605943 \nMuse, H., Bulathwela, S., & Yilmaz, E. (2022). Pre -training with scientific text improves \neducational question generation. arXiv preprint arXiv:2212.03869. https://doi.org/10.4855\n0/arXiv.2212.03869 \nNewton, P.E. (2007). Clarifying the purposes of educational assessment. Assessment in \neducation, 14(2), 149-170. https://doi.org/10.1080/09695940701478321 \nNguyen, H.A., Bhat, S., Moore, S., Bier, N., & Stamper, J. (2022). Towards generalized \nmethods for automatic question generation in educational domains. In European conference \non technology enhanced learning  (pp. 272-284). Cham: Springer International Publishing. \nhttps://doi.org/10.1007/978-3-031-16290-9_20 \nNittala, S., Agarwal, P., Vishnu, R., & Shanbhag, S. (2022). Speaker Diarization and BERT -\nBased Model for Question Set Generation from Video Lectures. In Information and \nCommunication Technology for Competitive Strategies ICT: Applications and Social \nInterfaces (pp. 441-452). Singapore: Springer Nature Singapore. https://doi.org/10.1007/97\n8-981-19-0095-2_42 \nOfferijns, J., Verberne, S., & Verhoef, T. (2020). Better distractions: Transformer -based \ndistractor generation and multiple -choice question filtering. arXiv preprint \narXiv:2010.09598. https://doi.org/10.48550/arXiv.2010.09598 \nPochiraju, D., Chakilam, A., Betham, P., Chimulla, P., & Rao, S.G. (2023). Extractive \nsummarization and multiple -choice question generation using XLNet. In 2023 7th \nInternational Conference on Intelligent Computing and Control Systems  (pp. 1001-1005). \nIEEE. https://doi.org/10.1109/ICICCS56967.2023.10142220 \nPugh, D., De Champlain, A., Gierl, M., Lai, H., & Touchie, C. (2020). Can automated item \ngeneration be used to develop high quality MCQs that assess application of knowledge?. \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n335 \nResearch and Practice in Technology Enhanced Learning, 15, 1-13. https://doi.org/10.118\n6/s41039-020-00134-8 \nQiu, X., Xue, H., Liang, L., Xie, Z., Liao, S., & Shi, G. (2021). Automatic generation of \nmultiple-choice cloze -test questions for lao language learning. In 2021 International \nConference on Asian Language Processing (pp. 125-130). IEEE. https://doi.org/10.1109/I\nALP54817.2021.9675153 \nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., ... & Liu, P. J. (2020). \nExploring the limits of transfer learning with a unified text -to-text transformer. Journal of \nMachine Learning Research, 21(140), 1-67. https://doi.org/10.48550/arXiv.1910.10683 \nRaina, V., & Gales, M. (2022). Multiple -choice question generation: Towards an automated \nassessment framework. arXiv preprint arXiv:2209.11830. https://doi.org/10.48550/arXiv.2\n209.11830 \nRatcheva, M.G., Navale, R., & Desai, B.C. (2022). An online MCQ sub-system for CrsMgr. In \nProceedings of the 26th International Database Engineered Applications Symposium  (pp. \n128-133). https://doi.org/10.1145/3548785.3548789 \nRodriguez-Torrealba, R., Garcia-Lopez, E., & Garcia-Cabot, A. (2022). End-to-end generation \nof multiple-xhoice questions using text-to-text transfer transformer models. Expert Systems \nwith Applications, 208, 118258. https://doi.org/10.1016/j.eswa.2022.118258 \nRudner, L.M. (2009). Implementin g the graduate management admission test computerized \nadaptive test. In Elements of adaptive testing  (pp. 151 -165). Springer New York. \nhttps://doi.org/10.1007/978-0-387-85461-8_8 \nSanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: \nsmaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. https://doi.org/10.48\n550/arXiv.1910.01108 \nSayin, A., & Gierl, M. (2024). Using OpenAI GPT to generate reading comprehension items.  \nEducational Measurement: Issues and Practice, 43(1), 5-18. https://doi.org/10.1111/emip.\n12590 \nShan, J., Nishihara, Y., Maeda, A., & Yamanishi, R. (2022). Question generation for reading \ncomprehension test complying with types of question. Journal of Information Science & \nEngineering, 38(3). https://doi.org/10.6688/JISE.202205_38(3).0005 \nShan, J., Nishihara, Y., Yamanishi, R., & Maeda, A. (2019). Question generation for reading \ncomprehension of language learning test: A method using Seq2Seq approach with \ntransformer model. In 2019 International Conference on Technologies and Applications of \nArtiﬁcial Intelligence (pp. 1-6). IEEE. https://doi.org/10.1109/TAAI48200.2019.8959903 \nShridhar, K., Macina, J., El-Assady, M., Sinha, T., Kapur, M., & Sachan, M. (2022). Automatic \ngeneration of socratic subquestions for teaching math word problems. arXiv preprint \narXiv:2211.12835. https://doi.org/10.48550/arXiv.2211.12835 \nSingh, J., McCann, B., Socher, R., & Xiong, C. (2019). BERT is not an interlingua and the bias \nof tokenization. In Proceedings of the 2nd Workshop on Deep Learning Approaches for \nLow-Resource NLP (DeepLo 2019) (pp. 47-55). https://doi.org/10.18653/v1/D19-6106 \nSpector, J.M., Ifenthaler, D., Samspon, D., Yang, L., Mukama, E., Warusavitarana, A., … \nGibson, D.C. (2016). Technology enhanced formative assessment for 21st century learning. \nEducational Technology & Society, 19(3), 58-71. https://www.jstor.org/stable/jeductechsoc\ni.19.3.58 \nSrivastava, M., & Goodman, N. (2021). Question generation for adaptive education. arXiv \npreprint arXiv:2106.04262. https://doi.org/10.48550/arXiv.2106.04262 \nSteuer, T., Filighera, A., & Rensing, C. (2020). Exploring artificial jabbering for automatic text \ncomprehension question generation. In Addressing Global C hallenges and Quality \nEducation: 15th European Conference on Technology Enhanced Learning, EC -TEL 2020, \nHeidelberg, Germany, September 14 –18, 2020, Proceedings 15  (pp. 1 -14). Springer \nInternational Publishing. https://doi.org/10.1007/978-3-030-57717-9_1 \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n336 \nTsai, D.C., Chang, W., & Yang, S. (2021). Short answer questions generation by Fine -Tuning \nBERT and GPT -2. In Proceedings of the 29th International Conference on Computers in \nEducation Conference (Vol. 64). https://icce2021.apsce.net/wp-content/uploads/2021/12/I\nCCE2021-Vol.II-PP.-508-514.pdf \nvon Davier, M. (2019). Training Optimus prime, MD: Generating medical certification items \nby fine -tuning OpenAI's gpt2 transformer model. arXiv preprint arXiv:1908.08594 . \nhttps://doi.org/10.48550/arXiv.1908.08594 \nVu, N., & Van Nguyen, K. (2022). Enhancing Vietnamese question generation with \nreinforcement learning. In Asian Conference on Intelligent Information and Database \nSystems (pp. 559-570). Cham: Springer International Publishing. https://doi.org/10.1007/97\n8-3-031-21743-2_45 \nWang, B., Yao, T., Chen, W., Xu, J., & Wang, X. (2021). Multi -lingual question generation \nwith language agnostic language model. In Findings of the Association for Computational \nLinguistics: ACL-IJCNLP 2021 (pp. 2262-2272). https://aclanthology.org/2021.findings-\nacl.199.pdf \nWang, H.C., Maslim, M., & Kan , C.H. (2023). A question –answer generation system for an \nasynchronous distance learning platform. Education and Information Technologies , 28(9), \n12059-12088. https://doi.org/10.1007/s10639-023-11675-y \nWang, Z., Valdez, J., Basu Mallick, D., & Baraniuk, R.G. (2022). Towards human -like \neducational question generation with large language models. In International conference on \nartificial intelligence in education (pp. 153-166). Cham: Springer International Publishing. \nhttps://doi.org/10.1007/978-3-031-11644-5_13 \nWylie, E.C., & Lyon, C.J. (2015). The fidelity of formative assessment implementation: Issues \nof breadth and quality. Assessment in Education: Principles, Policy & Practice, 22(1), 140-\n160. https://doi.org/10.1080/0969594X.2014.990416 \nXie, J., Peng, N., Cai, Y., Wang, T., & Huang, Q. (2021). Diverse distractor generation for \nconstructing high -quality multiple choice questions. IEEE/ACM Transactions on Audio, \nSpeech, and Language Processing, 30, 280-291. https://doi.org/10.1109/TASLP.2021.313\n8706 \nYang, Z., Dai, Z., Yang , Y., Carbonell, J., Salakhutdinov, R.R., & Le, Q.V. (2019). Xlnet: \nGeneralized autoregressive pretraining for language understanding. Advances in neural \ninformation processing systems, 32. https://dl.acm.org/doi/10.5555/3454287.3454804 \nYen, Y. -C., Ho, R. -G., Liao, W. -W., & Chen, L. -J. (2012). Reducing the impact of \ninappropriate items on reviewable computerized adaptive testing. Educational Technology \n& Society, 15(2), 231–243. https://www.jstor.org/stable/jeductechsoci.15.2.231 \nZhang, C. (2023). Automatic Generation of Multiple-Choice Questions. arXiv preprint arXiv: \n2303.14576v1. https://doi.org/10.48550/arXiv.2303.14576 \nZhao, W.X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., ... & Wen, J.R. (2023). A survey of \nlarge language models. arXiv preprint arXiv:2303.18223. https://doi.org/10.48550/arXiv.2\n303.18223 \nZhao, Z., Hou, Y., Wang, D., Yu, M., Liu, C., & Ma, X. (2022). Educational question generation \nof children storybooks via question type distribution learning and event -centric \nsummarization. arXiv preprint arXiv:2203.14187. https://doi.org/10.48550/arXiv.2203.141\n87 \n  \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n337 \nAPPENDIX A \nSummary of the included studies  \nAuthors Venue Description of the use of LLMs \nAgrawal & \nShukla, 2023 \nInternational Journal of \nAdvanced Computer Science \nand Applications \nT5 was trained to generate questions using texts and \nanswers together as input. \nAigo et al., 2021 Global Conference on \nConsumer Electronics \nT5 was used for question generation based on \nknowledge graphs. \nAkyön et al., \n2022 \nTurkish Journal of Electrical \nEngineering and Computer \nSciences \nmT5 was trained to extract answers first, then use these \nas input for generating questions. Other models were \nused for comparisons. \nAttali et al., 2022 Frontiers in Artificial \nIntelligence \nGPT-3 was first used to generate  question generation \ninstructions, which were then used to generate \nassociated questions and answers. \nBerger et al., \n2022 \nLatin American Conference on \nComputational Intelligence \nT5-small was used to generate questions with text -\nanswer pairs as input. \nBulathwela et al., \n2023 \narxiv T5 was used to generate questions with raw question -\nanswer pair texts as input \nBulut & \nYildirim-Erbasli, \n2022 \nInternational Journal of \nAssessment Tools in Education \nGPT was used to predict the next word for generating \nreading stories with two prompts, while T5 was utilized \nfor text-to-text generation with texts as input to generate \nquestions. \nChiang et al., \n2022 \nEmpirical Methods in Natural \nLanguage Processing \n(Conference) \nBERT was used with a QA pair as input and a distractor \ncandidate as output, which were then filtered by a text \nembedding model. \nChughtai et al., \n2022 \nInternational Conference on \nFrontiers of Information \nTechnology \nT5 was used to tokenize and summarize texts, and to \ngenerate question -answer pairs using contexts and \nparagraphs as input. \nChung et al., \n2020 \narxiv BERT was employed to iteratively predict the next \ntoken of distractors based on the context, question, \ncorrect answer, and previously predicted distractor \ntokens. GPT was used as a baseline model. \nDesai, 2021 Doctoral dissertation BERT was used for discourse segmentation, textual \nrepresentation, and discourse parsing (identifying \nconnections of tokens). However, the question \ngeneration process still relies on template -based \nmethods with masked tokens/words. \nDijkstra et al., \n2022 \nInternational Workshop on \nIntelligent Textbooks \nTwo prompts were compared for GPT to generate \nquestions: (1) Questions were generated based on the \ncontext first and then combined with the generated \nanswers to create distractors. (2) GPT directly \ngenerated the quiz based on the context alone. \nDrori et al., 2022 arxiv The LLMs were used to generate questions using two \nprompting strategies: one shot vs. few shots examples. \nFemi & Nayak, \n2022 \nInternational Conference on \nMachine Learning, Computer \nSystems and Security \nBERT was initially used to make sense of the keywords \ngenerated by WordNet and to put the keywords in \ncontext, followed by T5 generating questions, answers, \nand distractors. \n \nFuadi & Wibawa, \n2022 \nInternational Conference on \nElectrical and Information \nTechnology \nT5 was firstly used to generate answers based on \ncontexts, and these answers, along with the contexts, \nwere then used to generate questions. \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n338 \nFung et al., 2020 International Conference on \nTechnology in Education \nT5 was trained to generate questions using a source \nsentence with an answer phrase as input. \nFung et al., 2023 Inventions T5 was used for question generation with contexts and \nanswers as input. The generated questions were \ncompared with existing questions for feedback, which \nfed into the next iteration of question generation. \nGhanem et al., \n2022 \narxiv T5 was trained to teach the model how to ask questions \n(a skill label), then used to generate questions with a \nstory as input and QA pairs as output. \nGodslove & \nNayak, 2023 \nInternational Conference on \nApplied Mathematics in \nScience and Engineering \nBertWSD was used for word sense disambiguation, \nwhile T5 was utilized to generate questions, answers, \nand distractors with processed texts as input (including \ntext summarization and extraction). \nGopal, 2022 International Conference on \nAdvanced Learning \nTechnologies \nT5 and GPT -2 were used to generate texts with input \ntexts. \nGoyal et al., 2023 Arabian Journal for Science \nand Engineering \nT5 was used to identify answers and was trained to \ngenerate answers with texts as input, which were then \ncombined with these answers to generate questions. \nGrover et al., \n2021 \nInternational Advanced \nComputing Conference \nT5 was trained to generate multiple questions by \nproviding context paragraphs. \nHan, 2022 Master’s thesis BERT variants and BART variants were used to \ngenerate foreign language distractors following the \nmasked token prediction method. The LLMs were also \nused to generate distractors. \nJiao et al., 2023 International Conference on \nArtificial Intelligence in \nEducation \nT5, BERT, and GPT were compared for AIG tasks. In \naddition, GPT -2 was used to calculate perplexity \nvalues, indicating the fluency of the generated items. \nBERT-large was also used to evaluate coherence. \nBertScore was employed to calculate semantic \ndifferences in order to assess creativity. \nKalpakchi & \nBoye, 2021 \narxiv BERT was used to generate questions with question, \ncontext, and correct answers as input in two modes: \nsequential and non-ordered. \nKasakowskij et \nal., 2022 \nFachtagung \nBildungstechnologien (DELFI) \nGPT-2 was used to generate false statements for \ntrue/false questions based on the extracted correct \nstatements from texts. \nKhandait et al., \n2022 \nIndian Journal of Computer \nScience and Engineering \nT5 models received input paragraphs to generate \nquestions. Other LLMs were used as baseline models \nwithout detailed information for implementation. \nKumar et al., \n2021 \nInternational Conference on \nInventive Research in \nComputing Applications \nFirst, T5 was employed to extract answers from given \npassages. Next, T5 was used to generate questions with \npassages and answers as inputs. \nKumar et al., \n2022 \nInternational Conference for \nEmerging Technology \nT5 was used to summarize texts and to generate single-\nline questions with the candidate sentences as input and \nQA pairs as outputs. GPT -2 was used to generate \ntrue/false questions. Alternative sentences were firstly \ngenerated by GPT-2, followed by using Sentence BERT \nto filter out alternative sentences that are too similar to \nthe candidate sentences. Sentence BERT was also used \nto calculate the similarity scores of distractors for \nquestion filtering. \nKumar et al., International Conference on T5 was used to summarize the raw texts and generate \nquestion-answer pairs. BERT is used to rank the top \nTan et al.,                                                                              Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n339 \n2022 Big Data Analytics question-answer pairs. \nKumari et al., \n2022 \nInternational Conference on \nCloud Computing, Data \nScience & Engineering \nT5 was used to detect answers from texts and then use \nthese answers to generate questions. \nMaheen et al., \n2022 \nPeerJ Computer Science BERT was used solely for text embeddings. \nMalhar et al., \n2022 \nInternational Conference on \nIntelligent Computing and \nControl Systems \nThe T5 model was used to tokenize sentences, generate \nquestion-answer pair output from paragraph input, and \nthen use the question -answer output to generate \ndistractors for multiple -choice questions. BERT was \nused for question filtering and evaluation, as well as for \ntext summar ization to identify the most important \nsentences. \nMathur & \nSuchithra, 2022 \nInternational Conference on \nComputational Intelligence and \nSustainable Engineering \nSolutions \nBERT was used for text embeddings to extract key \nphrases (serving as answers), which were then used to \ngenerate questions and distractors by another NLP \nmodel (not a large language model). \nMatsumori et al., \n2023 \nIEEE Access BERT variants are used to predict masked tokens to \ncreate cloze questions. \nMaurya & \nDesarkar, 2020 \nInternational Conference on \nInformation & Knowledge \nManagement \nBERT was used to embed the text of input triplets in the \nformat of <article, question, correct answer>. The \nembedded tokens were then passed to a sequence -to-\nsequence model to generate distractors. \nMuse et al., 2022 arxiv T5 was utilized to generate questions with either pre -\ntraining or no pre-training. \nNguyen et al., \n2022 \nEuropean Conference on \nTechnology Enhanced \nLearning \nT5 was used to generate questions with texts processed \nby different methods. GPT-3 was then used to evaluate \nthe soundness of the generated questions. \nNittala et al., \n2022 \nInformation and \nCommunication Technology \nfor Competitive Strategies \nBERT was used for tokenization and word embedding. \nSCIBERT served as the generator, using the toke ns of \ncontexts and answers as input and also creating \nembeddings for these inputs. \nOfferijns et al., \n2020 \narxiv In the first phase, GPT -2 generated questions using \ncontext and answers as input, and in the second phase, \nit generated distractors using context, answers, and \nquestions as input. BERT was used to filter questions \nfor answerability and coherence (question filtering). \nPochiraju & al., \n2023 \nInternational Conference on \nIntelligent Computing and \nControl Systems \nXLNet and BERT were used as text summarizers, with \nthe output passed to other general NLP tools for \nquestion generation. \nQiu et al., 2021 International Conference on \nAsian Language Processing \nBERT was utilized for text embedding and question \nfiltering. The distractor generation was still  based on \ntraditional methods such as random selection, cosine \nsimilarity, and Lowenstein distance. \nRaina & Gales, \n2022 \narxiv GPT-3 was used in a zero -shot manner, while T5 was \nused with context texts (without answers or key \nphrases) as input. \nRatcheva et al., \n2022 \nInternational Database \nEngineered Applications \nSymposium \nT5 models were fine -tuned to generate questions with \ntexts as input and with texts and answers combined as \ninput. \nRodriguez-\nTorrealba et al., \n2022 \nExpert Systems with \nApplications \nFirst, T5-small was used to extract answers based on \ninput texts. Next, the sentence -answer pairs were fed \ninto a T5-base model to generate questions. \nTan et al.,                                                                            Int. J. Assess. Tools Educ., Vol. 12, No. 2, (2025) pp. 317–340 \n \n340 \nShan et al., 2019 International Conference on \nTechnologies and Applications \nof Artiﬁcial Intelligence \nBERT was first used for sentence embedding of articles, \nenabling their entry into a transformer model to extract \nsentences corresponding to a given question and the \nanswer (key phrase identification). Next, the question \ngeneration task is conducted using a sequ ence-to-\nsequence model with the extracted sentences as input \nand questions as output. \nShan et al., 2022 Journal of Information Science \nand Engineering \nBERT was used for text embedding, followed by \nquestion generation by a transformer model using the \nembedded tokens as input. \nShridhar et al., \n2022 \narxiv T5 was used with questions and contexts as input. \nSrivastava & \nGoodman, 2021 \narxiv GPT-2 was used to generate new questions based on \nstudent performance and desired difficulty for the next \nitem. \nSteuer et al., \n2020 \nEuropean Conference on \nTechnology Enhanced \nLearning \nKey phrase extraction was conducted before training \nGPT-2 to generate questions based on discussing the \ndetermined key phrases (a type of template -based \ngeneration).  \nTsai et al., 2021 International Conference on \nComputers in Education \nConference \nBERT was used to extract keywords from input \ntextbooks, which were then used to construct important \ncomplete sentences. Subsequently, GPT -2 iteratively \npredicted the next word for question generation, using \nthese complete sentences as input. \nvon Davier, 2019 arxiv GPT-2 was utilized to generate distractors with QA \ninputs like “Q: What was A?” and “A: A was Y”, and \nto create question passages based on prompts. \nVu & Van \nNguyen, 2022 \nAsian Conference on \nIntelligent Information and \nDatabase Systems \nLLMs were trained to generate texts by employing \nreinforcement learning for attention selection in their \nmodel architecture. \nWang et al. 2021 Annual Meeting of the \nAssociation for Computational \nLinguistics \nBERT served as a baseline model in their study, with no \nfurther details of implementation provided. \nWang et al. 2022 International Conference on \nArtificial Intelligence in \nEducation \nGPT-3 was employed using various prompting \nstrategies for comparison. The first strategy involved \nusing context and answer as input, while the second also \nutilized context and answer as input. \nWang et al. 2023 Education and Information \nTechnologies \nSentenceBERT was used to filter texts from slides and \nspeech recognition, using the results as input for \ntraining T5, which was then used to generate answers. \nThese answers were combined with the input texts to \ngenerate questions. \nWu, 2022 Master’s thesis Prompts with masked tokens: “Question for answer A: \n[MASK] for context C”. \nXie et al., 2021 IEEE/ACM Transactions on \nAudio, Speech, and Language \nProcessing \nT5 was used to generate questions with the filtered key \nsentences as input. \nZhang, 2023 arxiv T5 was trained using texts and answers as input. \nZhao et al., 2022 Annual Meeting of the \nAssociation for Computational \nLinguistics \nFirst, BERT was used to extract the question type \ninformation. Second, this information was used as a \nsignal for the BART model to summarize texts, which \nthen passed to another BART model to generate  \nquestions. BERT was used solely for text embeddings. \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6314198970794678
    },
    {
      "name": "Natural language processing",
      "score": 0.40625709295272827
    },
    {
      "name": "Data science",
      "score": 0.4002552330493927
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    }
  ],
  "cited_by": 5
}