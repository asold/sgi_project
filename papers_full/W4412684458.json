{
  "title": "Research quality evaluation by AI in the era of large language models: advantages, disadvantages, and systemic effects – An opinion paper",
  "url": "https://openalex.org/W4412684458",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5034823602",
      "name": "Mike Thelwall",
      "affiliations": [
        null,
        "University of Sheffield"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2996660452",
    "https://openalex.org/W4402398108",
    "https://openalex.org/W3033593507",
    "https://openalex.org/W2605256253",
    "https://openalex.org/W4403700580",
    "https://openalex.org/W3083485178",
    "https://openalex.org/W2169112665",
    "https://openalex.org/W4391839308",
    "https://openalex.org/W4404782918",
    "https://openalex.org/W4293080803",
    "https://openalex.org/W3023149598",
    "https://openalex.org/W2068452509",
    "https://openalex.org/W2076765281",
    "https://openalex.org/W1966661928",
    "https://openalex.org/W3170933632",
    "https://openalex.org/W4382462089",
    "https://openalex.org/W2969633830",
    "https://openalex.org/W4405487201",
    "https://openalex.org/W4391745580",
    "https://openalex.org/W3158683096",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3044557777",
    "https://openalex.org/W4392908777",
    "https://openalex.org/W4389433538",
    "https://openalex.org/W4391508636",
    "https://openalex.org/W7057614815",
    "https://openalex.org/W2277805675",
    "https://openalex.org/W4408176132",
    "https://openalex.org/W4408904916",
    "https://openalex.org/W4407553542",
    "https://openalex.org/W2987034230",
    "https://openalex.org/W4391709668",
    "https://openalex.org/W4405623605",
    "https://openalex.org/W4406574563",
    "https://openalex.org/W4382654337",
    "https://openalex.org/W4377023061",
    "https://openalex.org/W4379364503",
    "https://openalex.org/W4366437426",
    "https://openalex.org/W3024823753",
    "https://openalex.org/W2588841625",
    "https://openalex.org/W2088848272",
    "https://openalex.org/W4407385616",
    "https://openalex.org/W4409369199",
    "https://openalex.org/W4410883332",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W2315404267"
  ],
  "abstract": "Abstract Artificial Intelligence (AI) technologies like ChatGPT now threaten bibliometrics as the primary generators of research quality indicators. They are already used in at least one research quality evaluation system and evidence suggests that they are used informally by many peer reviewers. Since harnessing bibliometrics to support research evaluation continues to be controversial, this article reviews the corresponding advantages and disadvantages of AI-generated quality scores. From a technical perspective, generative AI based on Large Language Models (LLMs) equals or surpasses bibliometrics in most important dimensions, including accuracy (mostly higher correlations with human scores), and coverage (more fields, more recent years) and may reflect more research quality dimensions. Like bibliometrics, current LLMs do not “measure” research quality, however. On the clearly negative side, LLM biases are currently unknown for research evaluation, and LLM scores are less transparent than citation counts. From a systemic perspective, a key issue is how introducing LLM-based indicators into research evaluation will change the behaviour of researchers. Whilst bibliometrics encourage some authors to target journals with high impact factors or to try to write highly cited work, LLM-based indicators may push them towards writing misleading abstracts and overselling their work in the hope of impressing the AI. Moreover, if AI-generated journal indicators replace impact factors, then this would encourage journals to allow authors to oversell their work in abstracts, threatening the integrity of the academic record.",
  "full_text": "Vol.:(0123456789)\nScientometrics\nhttps://doi.org/10.1007/s11192-025-05361-8\nResearch quality evaluation by AI in the era of large \nlanguage models: advantages, disadvantages, and systemic \neffects – An opinion paper\nMike Thelwall1 \nReceived: 3 October 2024 / Accepted: 9 June 2025 \n© The Author(s) 2025\nAbstract\nArtificial Intelligence (AI) technologies like ChatGPT now threaten bibliometrics as the \nprimary generators of research quality indicators. They are already used in at least one \nresearch quality evaluation system and evidence suggests that they are used informally by \nmany peer reviewers. Since harnessing bibliometrics to support research evaluation con-\ntinues to be controversial, this article reviews the corresponding advantages and disadvan-\ntages of AI-generated quality scores. From a technical perspective, generative AI based \non Large Language Models (LLMs) equals or surpasses bibliometrics in most important \ndimensions, including accuracy (mostly higher correlations with human scores), and cov -\nerage (more fields, more recent years) and may reflect more research quality dimensions. \nLike bibliometrics, current LLMs do not “measure” research quality, however. On the \nclearly negative side, LLM biases are currently unknown for research evaluation, and LLM \nscores are less transparent than citation counts. From a systemic perspective, a key issue is \nhow introducing LLM-based indicators into research evaluation will change the behaviour \nof researchers. Whilst bibliometrics encourage some authors to target journals with high \nimpact factors or to try to write highly cited work, LLM-based indicators may push them \ntowards writing misleading abstracts and overselling their work in the hope of impress-\ning the AI. Moreover, if AI-generated journal indicators replace impact factors, then this \nwould encourage journals to allow authors to oversell their work in abstracts, threatening \nthe integrity of the academic record.\nKeywords Research evaluation · ChatGPT · Large Language Models · Research ethics\nIntroduction\nPost-publication research quality evaluation is a time consuming but important part of \nmodern research systems. On the basis that not all research is equal and the need to hire or \nreward scholars producing somehow “better” research, an unknown but substantial amount \n * Mike Thelwall \n m.a.thelwall@sheffield.ac.uk\n1 Information School, University of Sheffield, Sheffield, UK\n Scientometrics\nof expert time is devoted to post-publication research quality evaluation. For example, \nfor the UK Research Excellence Framework (REF) 2021, 185,594 research outputs were \nassessed by 1120 experts (mainly senior professors) over a year, with the results determin-\ning UK block research funding grants until 2029. More informally, academic appointments \nand promotions anywhere in the world may entail experts assessing the work of shortlisted \ncandidates.\nBecause research quality judgement is a slow task, there has been a natural tendency \nto look for quick alternatives as quality proxies, such as journal prestige, journal citation \nrates, or article citation counts. This has proven controversial with strong advocates and \nopposition. Recently, however, Large Language Models (LLMs) like ChatGPT have started \nto emerge as an alternative to bibliometrics for supporting research evaluations. This arti-\ncle assesses the potential of LLMs for this task and reviews the issues that may be raised by \ntheir apparent new role in research evaluation.\nExpert review and research quality\nFor important tasks, “research quality” is often judged by peer or expert review, but the \nconcept is not always defined. In systematic examples, guidelines are sometimes created \nthat explicitly or implicitly define quality in a particular context. A comparison of these \nguidelines revealed that the meaning of research quality  is not universal. For example, \nclarity might be considered central to a mathematics journal, and societal impacts might \nbe the sole consideration for a commercial research funder. Nevertheless, most guidelines \ninclude rigour, originality, and significance (scientific and/or societal) as important factors \nfor reviewers to consider (Langfeldt et al., 2020). Thus, it seems reasonable to regard these \nthree as the core dimensions of academic research quality, whilst acknowledging that it is \nnot a fixed concept.\nPeer review in terms of academics reviewing each other’s work submitted to jour -\nnals, conferences, or book publishers, plays a central gatekeeping quality control purpose \nas well as providing feedback for improvement. Another type is post-publication peer \nreview, which involves peer review type evaluations of academic research after it has been \npublished, as type of self-correction function for science (Bordignon, 2020; Da Silva & \nDobránszki, 2015; Harms & Credé, 2020; Hunter, 2012; Winker, 2015). In contrast, this \narticle is concerned exclusively with post-publication expert review, where the purpose is \nprimarily or exclusively to assess research quality. The result might be a numerical score \n(e.g., 3/10 or two stars out of four), perhaps accompanied by a justification, and perhaps \nwith separate scores for each dimension.\nExpert and peer review seem to be regarded as the best methods to assess research qual-\nity. Based on the three quality dimensions, disciplinary expertise is needed to judge the \nrigour of a study (e.g., the suitability of the research design, the appropriateness of the sta-\ntistics, the exhaustiveness of the argumentation), and its originality (by assessing the extent \nto which the topic, methods/approach or findings are novel). Judging significance is less \nobviously an expert task because it involves guessing the future. Nevertheless, an expert \nmight be expected to be more capable than most of estimating the likelihood that research \ngains traction within a field (e.g., if it is on a hot topic), or has societal value (e.g., in \ncomparison to which work in the field has or hasn’t generated societal impact in the past). \nOverall, however, the fundamental reasons why disciplinary expertise seems necessary for \neffectively evaluating the quality of academic research are that each academic output is \nScientometrics \nunique and complex, and assessing its rigour, originality, and (likely future) significance \nseem to require a deep understanding of the field(s) of the output.\nA complicating factor is that expert quality judgements depend on the nature of the \nexpertise of the assessor, so different ostensibly similar research evaluation tasks with \ndifferent types of assessors can assess research differently. If an output in a speciality is \nassessed by someone from that speciality (e.g., in recruitment for a postdoc position), then \nthey can be expected to give a sharp evaluation of rigour and originality and to assess sig-\nnificance at least partly from the perspective of the speciality. In contrast, if the same out-\nput is assessed by a non-specialist from the same field (e.g., a REF assessor), then they \nwould probably be less able to judge rigour and originality, or may judge them from the \nwider field perspective, and may judge significance primarily from the wider field perspec-\ntive (e.g., does the output have implications outside of its speciality?) The situation would \nbe different again if the assessor was a non-specialist (e.g., someone on an interdisciplinary \npanel selecting award recipients) who had no ability to assess rigour and might guess at \noriginality and significance from a general science-wide perspective.\nA fundamental problem with peer and expert review is that there is no objective truth \nin nature (except perhaps in pure maths) and so all science is subjective (Strevens, 2020). \nThus, no study can be fully rigorous, and the rigour of an output is therefore a judgment \nabout the extent to which the authors have successfully reduced the chances that their find-\nings are false. For example, whilst a nineteenth chemistry experiment using spring water for \nan aqueous solution might be judged rigorous for its time, a similar contemporary experi-\nment might be unacceptable without industrially produced ultrapure water. The problem \nof subjectivity in science can be compounded by researchers having beliefs, expectations, \nand moral perspectives. These can lead them to be more critical of, or cautious with, stud-\nies that contradict their understanding of the world and therefore be more ready to allocate \nthem lower scores, particularly for rigour. Whilst this can occur at any scale within science, \nit sometimes translates into the better-known phenomenon of competing paradigms, with \nadherents being sceptical of claims from competitors (e.g., nature vs. nurture, qualitative \nvs. quantitative, gene-centric biology vs. systems biology).\nAn additional problem with peer and expert review is personal bias. Particularly if they \nlack the skill or time for an effective evaluation, an assessor may consciously or uncon-\nsciously be influenced by factors unrelated to the quality of the output, such as the writing \nstyle, the gender, nationality, ethnicity, or reputation of the author, or the prestige of their \ninstitution. A reviewer may also know the author(s) and be influenced by whether they like \nthem.\nWhilst the above discussion is theoretical, reviewers seem to be often uncertain (Barnett \net al., 2024), and many empirical studies have confirmed that disagreement between peer \nreviewers is common (Feliciani et al., 2022; Thelwall & Hołyst, 2023). It is therefore rea-\nsonable to believe, albeit with less evidence, that disagreement between expert reviewers \nfor post-publication assessments would also be common.\nBibliometrics and responsible uses\nBibliometric research quality indicators are typically based on counting citations to aca-\ndemic outputs because citations can reflect influence (Merton, 1973). On this basis, cita-\ntion counts might be indicators of scholarly significance. In practice, citations can reflect, \nor be influenced by, many factors other than direct influence (Kousha & Thelwall, 2024; \n Scientometrics\nTahamtan et  al., 2016). Despite this complicating factor, evidence that citation-based \nindicators can weakly or moderately associate with expert research quality judgements \nin health, physical and life sciences as well as economics (Thelwall et al., 2023b) means \nthat it is technically valid to use them as research quality indicators. Similarly, the citation \nrate of the publishing journal tends to weakly or moderately associate with expert research \nquality judgements in the same fields (Thelwall et al., 2023c), so it is also technically valid \nto use journal citation rates as article quality indicators (Waltman & Traag, 2020).\nIn the above discussion, the term “indicator” is used in its technical sense of a quan-\ntity that associates with research but does not necessarily measure it and is not necessar -\nily accurate. In practice, article-based indicators tend to be field and year normalised and \nexpressed as citation ratios when used in large scale evaluations.\nThe majority view within the research evaluation community seems to be that peer \nreview is always better than bibliometric indicators for research evaluation but that indica-\ntors can play a supporting role. This is the position of the Leiden Manifesto (Hicks et al., \n2015), the Metric Tide (Wilsdon et  al., 2015), DORA (sfdora.org) and CoARA (coara.\neu). One reason for this is that citations might often directly reflect scholarly influence but \ncould only ever be indirect indicators of rigour, originality, and societal significance. In \naddition, the cited work might be criticised by the citing work, and the cited work might \nbe subsequently discredited. These are valid reasons against using citation-based indicators \nfor individual articles but not for large sets of articles because positive correlations overall \nwith research quality judgements show that these factors tend to cancel out. This does not \nrule out the possibility of systematic citation biases, however, and these can be serious, \ndepending on the sets of articles compared. For example, if using bibliometric indicators to \ncompare departments in a broad field, then the results would be biased in favour of depart-\nments working in high citation specialties. Thus, caution is always needed with bibliomet-\nric indicators.\nSome within the research evaluation community regard citations rather than peer review \nas the best evidence of research quality (Rushforth & Hammarfelt, 2023). They can point to \nevidence of subjectivity and biases within peer review and claim that citation counts con-\nstitute the cumulative decisions of many expert authors in the field. This does not address \nthe fact that citations only directly reflect the scholarly impact dimension of research qual-\nity, however.\nA third perspective within the wider research evaluation community, and expressed in \nthe Leiden Manifesto (Hicks et al., 2015) and the Metric Tide (Wilsdon et al., 2015), is \nthe need to consider the systemic effects of any evaluation process (Rushforth & Ham-\nmarfelt, 2023). The use of citation-based indicators in important research evaluations can \nincentivise researchers to chase citations instead of research quality. For example, this may \ndrive them towards generating scientific impacts (e.g., by introducing new research meth-\nods) rather than societal impacts (e.g., by assessing the value of a method to an end user \ncommunity) and drive scholars towards theoretical rather than applied research. DORA \n(sfdora.org) seems to be a response to the overvaluing of individual journals by scholars, \nparticularly in the biomedical community, which may have pushed some academics too \nfar towards conducting the type of research that can be published in high impact journals \nrather than the type of research that would be more widely valuable. Expert review seems \nto have a substantial advantage from the systemic perspective because, even if the experts \nare poor at evaluating research quality, expert review only incentivises research adhering to \nthe statement of quality used in the evaluation process. A partial exception to this is that if \nthe evaluator identities are known far enough in advance, then the people evaluated might \nbe incentivised to tailor their outputs to the type that likely assessors are perceived to value.\nScientometrics \nMachine learning, LLMs, and generative AI\nMachine learning is a generic term for artificial intelligence programs that learn to com-\nplete a task from examples. It is particularly useful when there are multiple factors that are \nknown to influence a target variable (e.g., research quality) but the relationship between \nthem is complex or non-linear. Machine learning has been used to predict long term cita-\ntion counts based on early citation counts, journal impact factors, authorship team size \nand other variables (Qiu & Han, 2024) or through LLM text analysis (Zhao et al., 2024). \nMachine learning long term citation predictions do not ever seem to have been used in \npractice for research evaluations, perhaps because they reduce transparency and add com-\nplexity. There has also been an attempt to predict article quality scores with machine learn-\ning based on bibliometric inputs (e.g., article and journal normalised citation rates, author \nnumbers, title and abstract words and short phrases), producing the most accurate results \nin the health, life and physical sciences and economics, but never exceeding 75% accurate \non the four-point scale used (Thelwall et  al., 2023a). The target four-point quality score \nwas that used in the UK REF2021: internationally relevant 1*, internationally relevant 2*, \ninternationally excellent 3*, and world leading 4*, all in terms of originality, rigour and \nsignificance. This solution would need to be integrated with large scale expert reviews, \nhowever, because it needs a large amount of training data to be effective. This makes it of \nniche value, only useful to aid large scale national research evaluations.\nLarge Language Models are technically a form of machine learning because they are \nessentially generic network structures that learn about language through being fed huge \nvolumes of text. Nevertheless, they are usefully considered as a separate AI category. There \nare two main types of LLM, discriminative and generative. Whilst discriminative LLMs \nlike BERT (Bidirectional Encoder Representations from Transformers) (Devlin, 2018) can \nbe fed with text and report context about it, generative AI based on LLMs, including Chat-\nGPT, can write new text. Early generative AI LLMs worked by predicting the likely next \ntokens in an input sequence, sometimes with a probability factor for randomisation. For \nexample, feeding an early LLM with “my tea is too” would be likely to predict/generate \n“hot” or “cold” but not “blue” or “is” as the next word. With huge amounts of input text, \nentire coherent sentences and paragraphs can be reliably produced. ChatGPT took this fur -\nther by training LLM systems not just on general text but also on question-response data \n(and probably other tasks like software code writing) in a process known as reinforcement \nlearning from human feedback (Ouyang et al., 2022). This makes the system more effective \nat responding to a wide range of user input requests. It harnesses real user data from the \nweb version to improve its performance continually or periodically.\nAt the time of writing (early 2025), the set of public Generative Pre-trained Transform-\ners (GPTs) generative AI LLMs had expanded to include Google Gemini, Claude, Face-\nbook’s LLaMA (sometimes written Llama, available at meta.ai), and DeepSeek, as well as \nderivatives like Microsoft Copilot (based on ChatGPT-4).\nIn addition to the big public GPTs, there is a range of open source shared GPT mod-\nels that can be used to perform the same function, many of which are available at Hug-\ngingface.co. These include free versions of commercial models, like DeepSeek and \nLLaMA (e.g., Masalkhi et  al., 2024), as well as models created by researchers, such as \nOpenGPT-2 (Cohen & Gokaslan, 2020). The same site also shares non-GPT LLMs. For \nexample, the many Bidirectional Encoder Representations from Transformers (BERT) var-\niants do not generate text but can be used to classify text (e.g., for sentiment) (Sun et al., \n2019). Although open source LLMs are less powerful, they are free and can be safely used \n Scientometrics\n(including in document processing pipelines) with private data. Automated processing is \nalso possible with the online commercial LLMs through their Applications Programming \nInterfaces (APIs), allowing batch processing of documents or requests.\nLLM‑generated research quality indicators\nLLM-based generative AI systems like ChatGPT can be directly tasked with research qual-\nity evaluation by feeding them with a research quality definition and an article and then \nasking them to score that article. This is very different from bibliometrics and traditional \nmachine learning because it mimics expert review and focuses on the article text rather \nthan harnessing citations and/or metadata. Many studies have now shown that LLMs can \ngive useful peer review style feedback on academic papers (Du et al., 2024; Lu et al., 2024; \nZhou et al., 2024) and can make predictions about acceptance/rejection that correlate posi-\ntively with editorial decisions (Thelwall & Yaghi, 2024a; Zhou et al., 2024; Zhuang et al., \n2025). Early research with ChatGPT 4o has now also given promising results for research \nquality evaluation too. Of course, some reviewers also harness ChatGPT to help with writ-\ning (Liang et al., 2024).\nSeveral small-scale and medium-scale studies and one large scale study have found \npositive correlations between expert quality scores or peer review decisions and Chat-\nGPT’s predictions. A study of 21 medical papers found a statistically significant correla-\ntion between human and ChatGPT 3.5 peer review recommendations (accept, accept with \nrevisions, reject) but not for ChatGPT 4o (Saad et  al., 2024), possibly due to the small \nsample size in the latter case. The remaining three studies have all used the REF quality \nscoring guidelines and scoring system (1*, 2*, 3* or 4*) (REF, 2019). The first uploaded \n51 PDFs and Word documents of published or unpublished journal articles from the field \nof library and information science to the web interface of ChatGPT 4o, asking for a REF \nscore for each one. The results correlated positively but weakly (0.20) with scores given by \nthe author, but the correlation increased to moderate (0.51) when the articles were submit-\nted 15 times and the results averaged (Thelwall, 2024). A follow-up study with the same set \nof papers used the ChatGPT API instead of the web interface and only submitted the titles \nand abstracts, without the full texts. This achieved even higher correlations (0.67) between \nthe ChatGPT average (over 30 iterations) and the author’s score. The results were worse \n(lower correlations) with the article title alone so the main power of ChatGPT seems to be \nin interpreting the author’s information about (or claims) for originality, significance and/\nor rigour from their abstract (Thelwall, 2025a). Similar but slightly weaker positive results \nhave also been found for Google Gemini 1.5 Flash, but PDF inputs produced stronger cor -\nrelations than titles and abstracts in this case (Thelwall, 2025b).\nMedium scale studies have tended to replicate the small-scale study results for individ-\nual fields. In the REF2021 category of Clinical Medicine, ChatGPT 40-mini scores for \n9872 articles found a weak positive correlation with the average research quality of the \ndepartment associated with the article. Average ChatGPT scores correlated more positively \nwith average departmental scores for articles at the journal level, although the results sug-\ngested that the dry reporting style of some prestigious medical journals tended to under -\nmine their articles’ scores (Thelwall et al., 2024). For the related goal of novelty scoring, \nLLMs have been shown to have some ability to predict novelty scores for computer science \npapers from one conference from the introduction, results, and discussion but not from the \nfull text (Wu et al., 2025; see also: de Winter, 2024). For a different output type, ChatGPT \nScientometrics \n4o-mini scores based on titles and blurbs/abstracts weakly but positively associate with \nbook citation rates in the social sciences and humanities (Thelwall & Cox, 2025).\nThe one large-scale analysis of ChatGPT quality scores so far selected articles from \nhigh and low scoring departments in each of the 34 Units of Assessment (UoAs—these are \nessentially broad fields or sets of similar small fields) and correlated the ChatGPT 4o qual-\nity scores (averaged over 30 iterations; titles and abstracts submitted through the API) with \nthe departmental average scores. The correlations were positive in all UoAs except Clinical \nMedicine (−0.15) and were weak (0.05 to 0.3) mainly in the arts and humanities, moder -\nate (0.3 to 0.5) mainly in the social sciences and strong (0.5 to 0.8) mainly in the health \nand physical sciences and engineering (Thelwall & Yaghi, 2024b). Although the results \nwere promising, they only used UK articles and, most importantly, relied on public data in \nterms of the departmental average REF scores. Thus, whilst they are consistent with Chat-\nGPT having a near-universal ability to detect research quality, they do not prove it, because \nChatGPT might have leveraged public information about departmental REF quality pro-\nfiles when scoring individual articles. The reason for the outlying field, clinical medicine, \nmight be that abstracts in clinical areas seem to report facts free of interpretation in their \nabstracts, giving ChatGPT less context to deduce significance. For completeness, there is \nalso an even larger-scale investigation with ChatGPT 4o-mini average quality scores for \n90% of REF2021 journal articles. It showed that ChatGPT scores tended to positively cor -\nrelate more highly with raw citation counts than with field (and year) normalised citation \nrates (Thelwall & Jiang, 2025).\nFrom a different perspective, another large-scale study has investigated the potential \nbiases of LLMs for research evaluation, using 117,650 articles from five years (2003, 2008, \n2013, 2018 and 2023) in 26 Scopus fields. It found a slight tendency for more recent arti-\ncles to receive higher scores over the 20 years. There were also field differences in aver -\nage ChatGPT scores, and articles with longer abstracts tended to receive higher scores \n(Thelwall & Kurt, 2024). These disparities are not necessarily biases, however, because \nof the lack of a ground truth in the study. For example, it is possible that research quality \nhas improved over time or that for some reason better quality articles tend to have longer \nabstracts.\nWhilst the results above are not conclusive, they are suggestive of, and consistent with, \nLLM-based quality evaluations being already superior to bibliometrics as research quality \nindicators, although with clear and not yet well understood biases. Given that LLM tech-\nnologies are still evolving, it seems reasonable to consider potential use within practical \nresearch evaluation contexts.\nAdvantages\nLLM-based quality evaluations seem to have at least three clear advantages over bibliomet-\nrics for research quality indicators. These are summarised here.\nMore accurate\nChatGPT 4o scores seem to be more accurate than bibliometrics in the sense of higher \ncorrelations with human scores in most fields (Thelwall & Yaghi, 2024b). Other factors \nbeing equal (e.g., biases) they are therefore more useful in any task that bibliometrics are \ncurrently used for and in theory could be used for purposes that bibliometrics are currently \nnot.\n Scientometrics\nGreater coverage of science (years, fields)\nOther than those for journals, citation-based indicators need at least two or three years \nto mature and so are not applicable to recently published research. In contrast, LLMs \ncan be applied to research of any age, as far as is currently known. This is a substantial \nadvantage given that the most recent research seems likely to be the most relevant to \nevaluate in practical contexts. In addition, citation-based indicators are useless (very \nweak or zero correlations with expert quality scores) for all arts and humanities and \nmany social sciences (Thelwall et al., 2023b), possibly because of different uses of cita-\ntions for knowledge building in these areas, whereas ChatGPT 4o does not rely on cita-\ntions and seems stronger is most fields.\nMore quality dimensions assessed\nWhilst citation-based indicators only directly reflect scholarly impact and are at best \nindirect indicators of rigour, originality, and societal impact, LLM research quality rat-\nings can, in theory, cover all dimensions and there is some, albeit weak (Thelwall & \nYaghi, 2024a), evidence that it can in some contexts. Whilst LLMs rely on the dimen-\nsions of quality defined for them in the systems instructions, and these have human \nlimitations in terms of potential inaccuracies and biases, humans are the ultimate gate-\nkeepers of research quality and so there does not seem to be an alternative source of \ndefinitions.\nSimilarities\nAn important similarity with bibliometrics is that LLMs do not “measure” research \nquality. In the case of citation-based indicators, they primarily reflect one type of schol-\narly impact and so are not an overall research quality measure. Whilst there has been \na systematic attempt to ameliorate this issue with alternative quantitative indicators, \naltmetrics, these have not filled the gap in most contexts. LLMs currently work most \neffectively with article titles and abstracts and therefore are clearly guessing at research \nquality rather than measuring it. Moreover, they may ignore or undervalue less common \ntypes of research or unusual contributions, including culturally specific work that differs \nfrom the mainstream despite being of equal quality. Whilst an LLM-based evaluation \nmight use full texts rather than titles and abstracts, this would not mean that research \nquality was being measured. This is because more accurate results with titles and \nabstracts alone would be consistent with full-text-based evaluations primarily assessing \ntitles and abstracts, with the rest of the text possibly confusing the LLM.\nDisadvantages\nThere are clear disadvantages of LLMs compared to bibliometrics, as of September \n2024. Some of these may lessen over time.\nScientometrics \nUnknown biases\nAI systems can learn biases from their training data and even generate new biases \nbecause of how their algorithms work (Kordzadeh & Ghasemaghaei, 2022). Thus, it is \nplausible that LLMs have learned relevant human biases, such as gender prejudice or \nfavouritism towards the work of successful authors or those from prestigious institu-\ntions. They may also have learned a hierarchy of methodological approaches, disciplines \nor research topics that would influence their quality judgements. Despite the current \nabsence of evidence that these or any other biases exist (with the possible exception \nof document age, field, and abstract length: Thelwall & Kurt, 2024), it is not reason-\nable to assume that they don’t and therefore applications should be cautious and new \nresearch into bias is needed. It is possible that identified AI biases could be mitigated, \nfor example by identifying and correcting for particular dimensions, or by emphasis-\ning in prompts the need to be unbiased, but this is an unexplored area. In comparison, \nbibliometric data has been tested for many types of bias. There seem to be minor gender \nbias at the article level (Thelwall, 2020) but more substantial career biases (Kelly & \nJennions, 2006) for citations as well as perhaps national self-citation biases and national \nbiases due to limited coverage of a nation’s publications, causing lost citations (Pendle-\nbury, 2020). These are known factors, and the biases overall are probably most impor -\ntant for international comparisons. Any responsible application of either bibliometrics \nor LLMs should at least consider the possibility of biases and take steps to mitigate \nthem and/or decide whether biases are too serious for the data to be used.\nLower transparency\nBibliometric data is relatively transparent, although there are some opaque or obscure \nelements. These include the decision-making process for including documents to index \nin the citation database (which can include human factors), and the details of the algo-\nrithms for extracting references from documents and matching them with the cited \nworks. In addition, the thought process of each citer is unknown. In contrast, LLMs are \nalmost fully opaque. Although the LLM architecture is known (transformer variants), \nthe data used to train commercial variants is unknown, as are the workings of the many \nadditional algorithms used for each complete system. Most significantly, however, the \nalgorithms are so complex that even full knowledge of them would give little insight \ninto how they can successfully score articles for quality or the conditions under which \nthey would make mistakes. Although each LLM score can be accompanied by a detailed \nexplanation and justification, these seem to be too vague to be helpful (Thelwall, 2024, \n2025a) and may be inaccurate.\nLack of research into different use contexts\nAnother disadvantage of LLMs is that research into their value for research quality assess-\nment is limited now (February, 2025) and it would be helpful to have a range of different \nstudies and approaches to confirm or contradict those published so far. This is needed to \ngive end users confidence in the value of the LLM scores. Moreover, since LLMs have \nbeen only used once so far in research evaluations (for project grants: Carbonell Cortés, \n Scientometrics\n2024), there is a lack of experience about how to use them and of the approaches that \nwould be most effective.\nMore serious side effects of gaming\nAn important disadvantage of LLM scores is their potential for gaming. Whilst citations \ncan be manipulated through excessive author self-citations (or journal self-citations for \njournal citation rate indicators), citation cartels, or other strategic citation practices (e.g., \nBaccini et al., 2019) this does not seem to have been a major concern in any research evalu-\nation so far, other than for journal rankings (e.g., Caon, 2017). In contrast, the potential \nof LLMs to be gamed is unknown and therefore cannot be assumed to be negligible. For \ninstance, if LLMs primarily leverage author claims in abstracts then the use of LLMs in \nserious research evaluation tasks would incentivise authors to overclaim as much as they \ncould get away with. Reviewers and journal editors would then need to control any per -\nceived exaggeration. If LLM-based indicators (e.g., average scores) were used for journals \nthen this would give a similar incentive to editors and publishers to allow inflated author \nabstract claims. In either case, this would undermine the usefulness of abstracts as infor -\nmation carriers and would compromise the integrity of research. It might also reduce the \neffectiveness of LLMs.\nConclusion: responsible uses of LLMs in research evaluation\nAs the above discussion suggests, LLM scores have the technical potential to complement \nor surpass bibliometrics for research quality indicators, but there are too many unknowns \nyet to use them in any important context. If more information can be gained about their \nbiases, limitations, and potential for gaming then it might be possible to start using them in \nminor roles to support peer review. In the longer term, successful applications, and a lack \nof issues from gaming might allow them to be used in less minor roles, taking over from \nbibliometrics. For example, they might be offered instead of bibliometrics as supporting \ninformation for expert reviewers in future versions of the UK REF national research evalu-\nation exercise.\nFunding The first author is a member of the Distinguished Reviewers Board of this journal and was funded \nby an ESRC Metascience grant (UKRI1079).\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nBaccini, A., De Nicolao, G., & Petrovich, E. (2019). Citation gaming induced by bibliometric evaluation: A \ncountry-level comparative analysis. PLoS ONE, 14(9), Article e0221212.\nScientometrics \nBarnett, A., Allen, L., Aldcroft, A., Lash, T. L., & McCreanor, V. (2024). Examining uncertainty in journal \npeer reviewers’ recommendations: A cross-sectional study. Royal Society Open Science, 11(9), Article \n240612.\nBordignon, F. (2020). Self-correction of science: A comparative study of negative citations and post-publi-\ncation peer review. Scientometrics, 124, 1225–1239. https:// doi. org/ 10. 1007/ s11192- 020- 03536-z\nCaon, M. (2017). Gaming the impact factor: Where who cites what, whom and when. Australasian Physical \n& Engineering Sciences in Medicine, 40, 273–276.\nCarbonell Cortés, C. (2024). AI-assisted pre-screening of biomedical research proposals: ethical considera-\ntions and the pilot case of \"la Caixa\" Foundation. https:// www. youtu be. com/ watch?v= O2DcX zEtCmg\nCohen, V., & Gokaslan, A. (2020). OpenGPT-2: Open language models and implications of generated text. \nXRDS: Crossroads, the ACM Magazine for Students, 27(1), 26–30.\nDa Silva, J., & Dobránszki, J. (2015). Problems with traditional science publishing and finding a wider \nniche for post-publication peer review. Accountability in Research, 22, 22–40. https:// doi. org/ 10. 1080/ \n08989 621. 2014. 899909\nde Winter, J. (2024). Can ChatGPT be used to predict citation counts, readership, and social media interac-\ntion? An exploration among 2222 scientific abstracts. Scientometrics, 129(4), 2469–2487.\nDevlin, J. (2018). BERT: Pre-training of deep bidirectional transformers for language understanding. arXiv \npreprint arXiv: 1810. 04805.\nDu, J., Wang, Y., Zhao, W., Deng, Z., Liu, S., Lou, R., & Yin, W. (2024). LLMs assist NLP researchers: \nCritique paper (meta-) reviewing. arXiv preprint arXiv: 2406. 16253.\nFeliciani, T., Luo, J., & Shankar, K. (2022). Peer reviewer topic choice and its impact on interrater reliabil-\nity: A mixed-method study. Quantitative Science Studies, 3(3), 832–856.\nHarms, P., & Credé, M. (2020). Bringing the review process into the 21st century: Post-publication peer \nreview. Industrial and Organizational Psychology, 13, 51–53. https:// doi. org/ 10. 1017/ iop. 2020. 13\nHicks, D., Wouters, P., Waltman, L., De Rijcke, S., & Rafols, I. (2015). Bibliometrics: The Leiden Mani-\nfesto for research metrics. Nature, 520(7548), 429–431.\nHunter, J. (2012). Post-publication peer review: Opening up scientific conversation. Frontiers in Computa-\ntional Neuroscience. https:// doi. org/ 10. 3389/ fncom. 2012. 00063\nKelly, C. D., & Jennions, M. D. (2006). The h index and career assessment by numbers. Trends in Ecology \n& Evolution, 21(4), 167–170.\nKordzadeh, N., & Ghasemaghaei, M. (2022). Algorithmic bias: Review, synthesis, and future research \ndirections. European Journal of Information Systems, 31(3), 388–409.\nKousha, K., & Thelwall, M. (2024). Factors associating with or predicting more cited or higher quality jour-\nnal articles: An Annual Review of Information Science and Technology (ARIST) paper. Journal of the \nAssociation for Information Science and Technology, 75(3), 215–244.\nLangfeldt, L., Nedeva, M., Sörlin, S., & Thomas, D. A. (2020). Co-existing notions of research quality: A \nframework to study context-specific understandings of good research. Minerva, 58(1), 115–137.\nLiang, W., Izzo, Z., Zhang, Y., Lepp, H., Cao, H., Zhao, X., & Zou, J. Y. (2024). Monitoring ai-modified \ncontent at scale: A case study on the impact of ChatGPT on AI conference peer reviews. arXiv preprint \narXiv: 2403. 07183.\nLu, Y., Xu, S., Zhang, Y., Kong, Y., & Schoenebeck, G. (2024). Eliciting Informative Text Evaluations with \nLarge Language Models. arXiv preprint arXiv: 2405. 15077.\nMasalkhi, M., Ong, J., Waisberg, E., Zaman, N., Sarker, P., Lee, A. G., & Tavakkoli, A. (2024). A side-by-\nside evaluation of Llama 2 by meta with ChatGPT and its application in ophthalmology. Eye, 38(10), \n1789–1792.\nMerton, R. K. (1973). The sociology of science: Theoretical and empirical investigations. University of \nChicago.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., & Lowe, R. (2022). Training lan-\nguage models to follow instructions with human feedback. Advances in Neural Information Processing \nSystems, 35, 27730–27744.\nPendlebury, D. A. (2020). When the data don’t mean what they say: Japan’s comparative underperformance \nin citation impact. Evaluative Informetrics: The Art of Metrics-Based Research Assessment: Fest-\nschrift in Honour of Henk F. Moed, 115–143.\nQiu, J., & Han, X. (2024). An early evaluation of the long-term influence of academic papers based on \nmachine learning algorithms. IEEE Access, 12, 41773–41786.\nREF (2019). Panel criteria and working methods (2019/02). https:// 2021. ref. ac. uk/ publi catio ns- and- repor ts/ \npanel- crite ria- and- worki ng- metho ds- 201902/ index. html\nRushforth, A., & Hammarfelt, B. (2023). The rise of responsible metrics as a professional reform move-\nment: A collective action frames account. Quantitative Science Studies, 4(4), 879–897.\n Scientometrics\nSaad, A., Jenko, N., Ariyaratne, S., Birch, N., Iyengar, K. P., Davies, A. M., Vaishya, R., & Botchu, R. \n(2024). Exploring the potential of ChatGPT in the peer review process: An observational study. Dia-\nbetes & Metabolic Syndrome: Clinical Research & Reviews, 18(2), Article 102946. https:// doi. org/ 10. \n1016/j. dsx. 2024. 102946\nStrevens, M. (2020). The knowledge machine: How an unreasonable idea created modern science. Penguin \nUK.\nSun, C., Qiu, X., Xu, Y., & Huang, X. (2019). How to fine-tune BERT for text classification? China national \nconference on Chinese computational linguistics (pp. 194–206). Springer International Publishing.\nTahamtan, I., SafipourAfshar, A., & Ahamdzadeh, K. (2016). Factors affecting number of citations: A com-\nprehensive review of the literature. Scientometrics, 107, 1195–1225.\nThelwall, M., Jiang, X., & Bath, P. A. (2025). Evaluating the quality of published medical research with \nChatGPT. Information Processing & Management, 62(4), 104123. https:// doi. org/ 10. 1016/j. ipm. 2025. \n104123.\nThelwall, M., & Yaghi, A. (2024). In which fields can ChatGPT detect journal article quality? An evalua-\ntion of REF2021 results. https:// arxiv. org/ abs/ 2409. 16695\nThelwall, M., & Yaghi, A. (2025). Evaluating the predictive capacity of ChatGPT for academic peer review \noutcomes across multiple platforms. Scientometrics. https:// doi. org/ 10. 1007/ s11192- 025- 05287-1.\nThelwall, M., & Kurt, Z. (2024). Research evaluation with ChatGPT: Is it age, country, length, or field \nbiased? arXiv preprint arXiv: 2411. 09768.\nThelwall, M., & Jiang, X. (2025). Is OpenAlex Suitable for Research Quality Evaluation and Which Cita-\ntion Indicator is Best? arXiv preprint arXiv: 2502. 18427.\nThelwall, M. & Cox, A. (2025). Estimating the quality of academic books from their descriptions with \nChatGPT. Journal of Academic Librarianship, 51(2), 103023. https:// doi. org/ 10. 1016/j. acalib. 2025. \n103023.\nThelwall, M. (2020). Female citation impact superiority 1996–2018 in six out of seven English-speaking \nnations. Journal of the Association for Information Science and Technology, 71(8), 979–990.\nThelwall, M. (2024). Can ChatGPT evaluate research quality? Journal of Data and Information Science, \n9(2), 1–21. https:// doi. org/ 10. 2478/ jdis- 2024- 0013\nThelwall, M. (2025a). Evaluating research quality with large language models: An analysis of ChatGPT’s \neffectiveness with different settings and inputs. Journal of Data and Information Science, 10(1), 7–25. \nhttps:// doi. org/ 10. 2478/ jdis- 2025- 0011\nThelwall, M. (2025b). Is Google Gemini better than ChatGPT at evaluating research quality? Journal of \nData and Information Science, 10(1), 1–5. https:// doi. org/ 10. 2478/ jdis- 2025- 0014\nThelwall, M., & Hołyst, J. A. (2023). Can journal reviewers dependably assess rigour, significance, and \noriginality in theoretical papers? Evidence from Physics. Research Evaluation, 32(2), 526–542.\nThelwall, M., Kousha, K., Makita, M., Abdoli, M., Stuart, E., Wilson, P., & Levitt, J. (2023c). In which \nfields do higher impact journals publish higher quality articles? Scientometrics, 128(7), 3915–3933.\nThelwall, M., Kousha, K., Stuart, E., Makita, M., Abdoli, M., Wilson, P., & Levitt, J. (2023b). In which \nfields are citations indicators of research quality? Journal of the Association for Information Science \nand Technology, 74(8), 941–953.\nThelwall, M., Kousha, K., Wilson, P., Makita, M., Abdoli, M., Stuart, E., Levitt, J., Knoth, P., & Cancel-\nlieri, M. (2023a). Predicting article quality scores with machine learning: The UK Research Excellence \nFramework. Quantitative Science Studies, 4(2), 547–573. https:// doi. org/ 10. 1162/ qss_a_ 00258\nWaltman, L., & Traag, V. A. (2020). Use of the journal impact factor for assessing individual articles: Sta-\ntistically flawed or not? F1000Research, 9, 366.\nWilsdon, J., Allen, L., Belfiore, E., Campbell, P., Curry, S., Hill, S., spsampsps Johnson, B. (2015). The \nmetric tide: Independent review of the role of metrics in research assessment and management. https:// \nwww. ukri. org/ publi catio ns/ review- of- metri cs- in- resea rch- asses sment- and- manag ement/\nWinker, M. (2015). The promise of post-publication peer review: How do we get there from here? Learned \nPublishing, 28(2), 143–145. https:// doi. org/ 10. 1087/ 20150 209\nWu, W., Zhang, C., Bao, T., & Zhao, Y. (2025). SC4ANM: Identifying optimal section combinations for \nautomated novelty prediction in academic papers. Expert Systems with Applications, 273, 126778.\nZhao, P., Xing, Q., Dou, K., Tian, J., Tai, Y., Yang, J., & Li, X. (2024). From Words to Worth: Newborn \nArticle Impact Prediction with LLM. arXiv preprint arXiv: 2408. 03934.\nZhou, R., Chen, L., & Yu, K. (2024). Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM \non Automatic Paper Reviewing Tasks. In  Proceedings of the 2024 Joint International Conference \non Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)  (pp. \n9340–9351).\nZhuang, Z., Chen, J., Xu, H., Jiang, Y., & Lin, J. (2025). Large language models for automated scholarly \npaper review: A survey. arXiv preprint arXiv: 2501. 10326.\nScientometrics \nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.",
  "topic": "Quality (philosophy)",
  "concepts": [
    {
      "name": "Quality (philosophy)",
      "score": 0.6055895090103149
    },
    {
      "name": "Computer science",
      "score": 0.48583075404167175
    },
    {
      "name": "Epistemology",
      "score": 0.2803182601928711
    },
    {
      "name": "Philosophy",
      "score": 0.13520243763923645
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I91136226",
      "name": "University of Sheffield",
      "country": "GB"
    }
  ],
  "cited_by": 2
}