{
  "title": "Clinical entity augmented retrieval for clinical information extraction",
  "url": "https://openalex.org/W4406596702",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5084224715",
      "name": "Iván López",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5079965457",
      "name": "Akshay Swaminathan",
      "affiliations": [
        "Stanford Medicine",
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5115763802",
      "name": "Karthik S. Vedula",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5040520263",
      "name": "Sanjana Narayanan",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5056466429",
      "name": "Fateme Nateghi Haredasht",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5029989846",
      "name": "P. Stephen",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5099179337",
      "name": "April S. Liang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5059833188",
      "name": "Steven Tate",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5115941975",
      "name": "Manoj Maddali",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5066571318",
      "name": "Robert J. Gallo",
      "affiliations": [
        "Center for Innovation",
        "Stanford University",
        "VA Palo Alto Health Care System"
      ]
    },
    {
      "id": "https://openalex.org/A5041175834",
      "name": "Nigam H. Shah",
      "affiliations": [
        "Stanford Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A5046725885",
      "name": "Jonathan H. Chen",
      "affiliations": [
        "Stanford Medicine"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2114388055",
    "https://openalex.org/W1483357098",
    "https://openalex.org/W4391709260",
    "https://openalex.org/W4389205282",
    "https://openalex.org/W3031956118",
    "https://openalex.org/W4390889502",
    "https://openalex.org/W2099369363",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W4312113143",
    "https://openalex.org/W1596932914",
    "https://openalex.org/W3165345393",
    "https://openalex.org/W4375955751",
    "https://openalex.org/W3085853817",
    "https://openalex.org/W4387117651",
    "https://openalex.org/W4306253706",
    "https://openalex.org/W2138162199",
    "https://openalex.org/W3169284847",
    "https://openalex.org/W3173561451",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W3145725122",
    "https://openalex.org/W4210491535",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4282983782",
    "https://openalex.org/W3166444100",
    "https://openalex.org/W3024538009",
    "https://openalex.org/W4392740458",
    "https://openalex.org/W4392193048",
    "https://openalex.org/W4390833194",
    "https://openalex.org/W4365143687",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4390745503",
    "https://openalex.org/W4393027162",
    "https://openalex.org/W4402683949",
    "https://openalex.org/W4389519019",
    "https://openalex.org/W4402671766",
    "https://openalex.org/W4402671258",
    "https://openalex.org/W4389984066",
    "https://openalex.org/W4401043168",
    "https://openalex.org/W4386976808",
    "https://openalex.org/W4385572149",
    "https://openalex.org/W4389523718",
    "https://openalex.org/W6849898756",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W4393299232",
    "https://openalex.org/W3152740956",
    "https://openalex.org/W4385571012",
    "https://openalex.org/W4389520779",
    "https://openalex.org/W4396823873",
    "https://openalex.org/W4400222836",
    "https://openalex.org/W4385570284",
    "https://openalex.org/W3006227201",
    "https://openalex.org/W4386270925",
    "https://openalex.org/W3015487282",
    "https://openalex.org/W4399979066",
    "https://openalex.org/W4287825939",
    "https://openalex.org/W2904183610",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W4395443393",
    "https://openalex.org/W4401306886",
    "https://openalex.org/W2769041395",
    "https://openalex.org/W4386794805",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W2039691998",
    "https://openalex.org/W2959716049"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-024-01377-1\nClinical entity augmented retrieval for\nclinical information extraction\nCheck for updates\nIvan Lopez 1,2,14 , Akshay Swaminathan 1,2,14, Karthik Vedula3, Sanjana Narayanan4,\nFateme Nateghi Haredasht4,S t e p h e nP .M a5, April S. Liang6,S t e v e nT a t e7,M a n o jM a d d a l i2,8,\nRobert Joseph Gallo 9,10, Nigam H. Shah4,11,12 &J o n a t h a nH .C h e n2,4,5,12,13\nLarge language models (LLMs) with retrieval-augmented generation (RAG) have improved information\nextraction over previous methods, yet their reliance on embeddings often leads to inefﬁcient retrieval.\nWe introduce CLinical Entity Augmented Retrieval (CLEAR), a RAG pipeline that retrieves information\nusing entities. We compared CLEAR to embedding RAG and full-note approaches for extracting 18\nvariables using six LLMs across 20,000 clinical notes. Average F1 scores were 0.90, 0.86, and 0.79;\ninference times were 4.95, 17.41, and 20.08 s per note; average model queries were 1.68, 4.94, and\n4.18 per note; and average input tokens were 1.1k, 3.8k, and 6.1k per note for CLEAR, embedding\nRAG, and full-note approaches, respectively. In conclusion, CLEAR utilizes clinical entities for\ninformation retrieval and achieves >70% reduction in token usage and inference time with improved\nperformance compared to modern methods.\nFree-text notes in electronic health records (EHRs) are rich with data not\nfound within structuredﬁelds, like symptoms, diagnoses, disease course,\nsocial determinants of health, family history, and patient perspectives1,2.T h e\nability to process this data unlocks various important research and quality\nimprovement use cases, including cohort selection3, phenotyping4,o b s e r -\nvational data analysis5, and predictive modeling6.\nDespite the amount of valuable information in EHRs, extracting\ninformation from clinical notes remains challenging7,8. Clinical information\nextraction comprises several tasks,including named entity recognition\n(NER) (e.g., recognizing“t2dm” as type II diabetes mellitus)9,s e n s ed i s -\nambiguation (e.g., understanding“mi”as “myocardial infarction”or “mitral\ninsufﬁciency” depending on the context)10, and relation extraction (e.g.,\nlinking a symptom with medication if reported as a side-effect)11.\nThe simplest clinical information extraction approaches use rules and\ndictionaries to identify entities of interest12,13, such as diagnosis codes like the\nInternational Classiﬁcation of Diseases (ICD). In a 2018 review of 263\nclinical information extraction methods, 65% were rule-based8. These sys-\ntems are interpretable, easy to deploy, and achieve reasonable performance\non many tasks\n12. However, structuredﬁelds like diagnosis codes are unable\nto fully capture a patient’s medical history in the current state. For example,\ndespite a recent increase in the use ofdiagnosis codes to represent social\ndeterminants of health, they remain underutilized and often miss crucial\ncontextual details only foundin the unstructured text of EHRs\n14,15.M o r e -\nover, for many conditions, such as cancer, ICD codes do not reﬂect the true\nsource of diagnosis; in these cases, pathology reports are the gold\nstandard\n16,17. Natural language processing methods are therefore necessary\nto extract these insights, allowing for a more comprehensive understanding\nof patient health. Additionally, hard-coded rules and word lists fail to\ncapture the wide variation in clinical language, including synonyms and\nabbreviations, and miss nuanced descriptions in EHR notes\n18.\nSupervised machine learning approaches that take in a labeled dataset\ncan recognize more complex linguistic relationships than rules- or\ndictionary-based methods. Neural network architectures like bi-directional\nLong Short-Term Memory networks (LSTMs) are well suited for sequence\ndata-based tasks like NER, given their ability to learn relationships between a\nt o k e na n di t sn e i g h b o rt o k e n si ne i t h e rd i r e c t i o n\n19. For example, Stanza20,21,a\nwidely used Python library for NER, uses a bi-directional LSTM with a\nConditional Random Field trained on the 2010 i2b2/VA dataset\n22.A\n1Stanford University School of Medicine, Stanford, CA, USA.2Department of Biomedical Data Science, Stanford, CA, USA.3Poolesville High School, Poolesville,\nMD, USA.4Stanford Center for Biomedical Informatics Research, Stanford, CA, USA.5Division of Hospital Medicine, Stanford University School of Medicine,\nStanford, CA, USA.6Division of Clinical Informatics, Stanford University School of Medicine, Stanford, CA, USA.7Department of Psychiatry and Behavioral\nSciences, Stanford University School of Medicine, Stanford, CA, USA.8Division of Pulmonary, Allergy, and Critical Care Medicine, Stanford University School of\nMedicine, Stanford, CA, USA.9Center for Innovation to Implementation, VA Palo Alto Healthcare System, Menlo Park, CA, USA.10Department of Health Policy,\nStanford University, Stanford, CA, USA.11Technology and Digital Solutions, Stanford Healthcare, Palo Alto, USA.12Clinical Excellence Research Center, Stanford\nUniversity School of Medicine, Stanford, CA, USA.13Department of Medicine, Stanford, CA, USA.14These authors contributed equally: Ivan Lopez, Akshay\nSwaminathan. e-mail: ivlopez@stanford.edu\nnpj Digital Medicine|            (2025) 8:45 1\n1234567890():,;\n1234567890():,;\ndisadvantage of machine learning approaches is that they often require\nlarge, labeled training datasets, which can be time-consuming and expensive\nto obtain. Weak supervision offers analternative to human-labeled data,\nwhere programmatic labeling functions are used to automatically assign\n“weak” labels. Although the quality of weak labels is lower than human\nl a b e l s ,t r a i n i n go nal a r g en u m b e ro fw e a kl a b e l sh a sb e e ns h o w nt oo u t -\nperform training on a small number of human labels. Labeling functions can\nbe manually curated or sourced fromontologies or smaller models. For\nexample, TROVE uses ontologies like the Uniﬁed Medical Language System\n(UMLS) to create labeling functions and uses these weak labels toﬁne-tune a\nBERT-based model for identifying symptoms and risk factors for\nCOVID-19\n23.\nPre-trained deep learning models like BERT use multidimensional\nembeddings learned from large, unlabeled corpuses. These embeddings\nrepresent semantic information that can be used as features for a variety of\ndownstream tasks. For instance,ﬁne-tuned BERT-based models have been\nemployed for tasks including named entity recognition, assertion status\ndetermination, sense disambiguation, and relation extraction\n24,25,a n da r e\noften adapted to speciﬁc clinical domains, like radiology26,27.A l t h o u g ht h e s e\nmodels can beﬁne-tuned to perform tasks like diagnostic code assignment,\ntreatment assignment28, and open-ended reasoning29–32, decoder-only\nmodels are typically better equipped for this task.\nRecently, large language models (LLMs) trained with transformer-\nbased architectures on large unlabeled text corpuses have demonstrated\nimpressive performance on both information extraction and natural lan-\nguage understanding tasks, such asinformation extraction (e.g.,“does this\npatient have diabetes?”)\n33, text summarization (e.g., “summarize this\npatient’sh i s t o r y”)34, and conversational capabilities (e.g.,“draft a response to\nthis patient’s message”)35. One advantage of LLMs is their“few-shot” and\n“zero-shot”prompting capabilities, enabling them to accomplish tasks with\nfew to no labeled examples— tasks that previously required training orﬁne-\ntuning separate models with labeled datasets36–38. Recent work has used\nLLMs to extract clinical variables from EHR notes, including social deter-\nminants of health, medications, and postpartum hemorrhage4,39,40.W h i l e\nLLMs show great promise in clinical information retrieval, they face several\nlimitations. For instance, the length of patient notes can surpass an LLM’s\ncontext window— the amount of text that can be passed into the model.\nNaive approaches like truncation or selecting only documents thatﬁtw i t h i n\nthe context window risk excluding valuable information\n4,39,41. Dividing a\nnote into smaller chunks with adjoining strides can address context window\nlimits but still requires multiple LLM queries per patient, which can be\ncomputationally expensive. In addition, LLM performance has been shown\nto degrade on reasoning tasks as inputlength increases, even on models with\nlarge context windows42–45, suggesting that inputting long EHR excerpts\ncontaining extraneous information can reduce performance.\nRetrieval-augmented generation (RAG) attempts to address this lim-\nitation by retrieving and appending query-relevant information to the input\ncontext. The retriever typically usesan encoder model to represent both the\nquery and reference information in embedding space and retrieve infor-\nmation whose embeddings are close to that of the query46.S o m eR A G\nworkﬂows embed small chunks of text that canﬁtw i t h i nt h em o d e l’sc o n t e x t\nwindow and store those embeddings in adatabase for downstream retrieval.\nOther approaches, mostly explored in the general domain, involve\nembedding and retrieving fact triplets from knowledge graphs\n47–49.\nAn important challenge in RAG-based methods is ensuring that the\nretrieved information is relevant to the query and does not contain extraneous\ninformation that can hinder LLM reasoning and add to inference costs\n50,51.\nTo address the above limitations of LLMs for clinical information\nextraction, we propose CLinical Entity Augmented Retrieval (CLEAR), a\nRAG pipeline that retrieves note chunkscontaining clinical entities relevant\nto the input query. We hypothesized thatretrieval based on relevant clinical\nentities would lead to more efﬁcient and relevant information retrieval\ncompared to RAG approaches based on note chunk embeddings. We make\nthree contributions. First, we validate the entity recognition and entity\nselection steps of the CLEAR pipeline, which identify clinical entities in\nclinical notes and select a subset relevant to the input query. Second, we\ncompare CLEAR to a RAG approach that embeds note chunks and a full-\nnote retrieval approach in performing information extraction for 18 clinical\nvariables. Third, we explore the feasibility of using CLEAR to generate labels\nto ﬁne-tune a BERT-sized model in performing information extraction. We\nconduct all experiments on two real-world EHR-derived datasets that\ninclude labels for substance use (e.g., alcohol dependence, tobacco depen-\ndence), mental health (e.g., attention-de ﬁcit/hyperactivity disorder\n[ADHD], bipolar disorder, depression),social determinants of health (e.g.,\nhomelessness, unemployment), and chest radiographﬁndings (e.g., pneu-\nmonia, cardiomegaly).\nResults\nInter-rater reliability\nIn the Stanford MOUD dataset, the unweighted Cohen’sK a p p av a l u ew a s\n0.86 (95% CI: 0.79-0.93). In the CheXpert dataset, the unweighted Cohen’s\nKappa was 0.93 (95% CI: 0.88–0.98). These values indicate excellent\nagreement between annotators.\nNER and entity selection evaluation\nZero-shot NER using Flan-T5 identiﬁed 1269 out of 1382 entities (96%\nsensitivity) in the NCBI disease dataset and 440 out of 450 entities (99%\nsensitivity) in the Stanford MOUD dataset. We measured to what extent\nontology and LLM augmentation recover entities missed in the NER step\n(false negatives) by using the UMLS ontology and GPT-4 to generate\nsynonyms as if they were the target entity. This augmentation step increases\nsensitivity to 99% and 100% in the NCBI disease dataset and Stanford\nMOUD dataset, respectively, indicating that even if the target entity is\nmissed during NER, it is very likely to be detected through ontology and\nLLM augmentation (Supplementary Table 1). The performance of each of\nthe four zero-shot NER prompts on the Stanford MOUD dataset is detailed\nin Supplementary Table 2. We report the classiﬁcation of false negatives for\nthis analysis in Supplementary Table 3.\nWe studied the impact of the initial NER step on the overall perfor-\nmance of CLEAR. Overall, removing NER from CLEAR and relying only on\nontology and LLM augmentation hurts downstream information extraction\ntask performance, resulting in a 0.11 decrease in F1 across all 13 variables in\nthe Stanford MOUD dataset (0.86 without NER vs. 0.97 with NER). For\nunhoused, personality disorder, ADHD, PTSD, suicidal behavior, liver\ndisease, and unemployment, removing NER resulted in an F1 drop of≤0.02.\nFor other variables, removing NER led to a drop in F1 from 0.18 (bipolar\ndisorder) to 0.40 (substance use disorder) (Fig.1 and Supplementary Table\n4). This suggests that for several variables, LLMs and ontologies do not\ncapture the natural variation in clinical variables as effectively as NER. For a\nfull list of high-yield terms missed by Ontology+LLM augmentations, refer\nto Supplementary Table 5.\nInformation extraction evaluation\nOn the Stanford MOUD Dataset, the average F1 score across all 13 variables\nand all 6 models was 0.90, ranging from 0.78 (Med42) and 0.97 (GPT-4)\nacross models. GPT-4 had the highest F1 score for 10 out of 13 variables.\nF1 scores across variables ranged from 0.61 (Med42 on depression) to 1.00\n(GPT-4 on personality disorder, bipolar disorder, PTSD, and unemployment;\nL l a m a - 3o nu n h o u s e d ;F l a n - T 5o nu n e m p l o y m e n t ) .O nt h eC h e X p e r t\nDataset, the average F1 score across al l5t e s ts e t sf o ra l l6m o d e l sw a s0 . 9 6 ,\nranging from 0.91 (Flan-UL2) and 0.98 (Flan-T5 and Mixtral). Flan-T5 had\nthe highest F1 score for 3 out of 5 variables. F1 scores across variables ranged\nfrom 0.90 (Med42 on pneumothorax) to 1.00 (Flan-T5 on cardiomegaly and\npleural effusion) (Table1). A full breakdown of CLEAR, chunk embedding,\nand full-note performance per variableis reported in Supplementary Table 6.\nWe used CLEAR to label a dataset toﬁne-tune a Bio+Clinical BERT\nfor information extraction of the 13 variables in the Stanford MOUD dataset\na n d5v a r i a b l e si nt h eC h e X p e r td a t aset. Within the 13 Stanford MOUD\nDataset classiﬁers, two showed perfect discrimination on the test set\n(AUC = 1). The“suicidal behavior” classiﬁer had the lowest AUC (0.83).\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 2\nWithin the CheXpert Dataset, the“cardiomegaly”classiﬁer had the highest\nAUC (AUC = 1), and the“pulmonary edema”classiﬁer had the lowest AUC\n(AUC = 0.97) (Supplementary Table 7). Using a predicted probability\nthreshold of 0.5, theﬁne-tuned BERT model’s F1 scores were consistently\nwithin the range of the larger models’F1 scores. For alcohol dependence and\nchronic pain, theﬁne-tuned BERT model F1 was higher than the trainer\nmodel’sF 1s c o r e( T a b l e1).\nAdditionally, results from the weak labeling experiments suggest that\nour CLEAR outperforms weak supervision using regular expressions, which\nresulted in lower average F1 scores compared to all LLMs used with CLEAR\n(Supplementary Table 8).\nComparison to chunk embedding and full-note approaches\nAcross all models, chunk embedding(top-5) and full-note methods per-\nformed worse on the information extraction task compared to CLEAR\n(Supplementary Table 9). The performance delta was largest for GPT-4\n(average F1 0.97 CLEAR vs. 0.88 chunk embedding vs. 0.90 full note) and\nsmallest for Flan-T5 (average F1 0.91 CLEAR vs. 0.88 chunk embedding vs.\n0.88 full note) (Fig.2a, Supplementary Table 9). Increasing top-k improves\nchunk embedding performance, but even with k = 10, CLEAR out-\nperformed chunk embedding across all models except Med42, where the\nchunk embedding approach outperformed CLEAR by 0.01 (F1 0.79 vs. 0.78)\n(Supplementary Table 10). We conducted additional experiments by\nincreasing the CLEAR context window size from+/− 150 words to+/−\n185 words, and reducing token chunks for chunk embeddings from 490 to\n390. As a result, the token counts for CLEAR became larger than those for\nchunk embedding. We re-ran our analysis using Mixtral, and the results\nshowed that the average F1 score for CLEAR increased by 0.01, while the\naverage F1 score for chunk embedding decreased by 0.02 (Supplementary\nTable 11), compared to the originalresults in Supplementary Table 9.\nCLEAR outperformed chunk embedding and full-note approaches on\nnearly all efﬁciency metrics. Average inference time per note ranged from\nTable 1 | CLEAR F1 scores for information extraction on the Stanford MOUD and CheXpert datasets\nVariable Flan-T5 Flan-UL2 GPT-4 Med42 Llama-3 Mixtral Range Fine-tuned BERT\nCheXpert\nCardiomegaly 1.00 0.96 0.95 0.97 0.95 0.99 1.00 –0.95 0.95\nPulmonary edema 0.98 0.96 0.96 0.91 0.96 0.98 0.98 –0.91 0.97\nPleural effusion 1.00 0.84 0.97 0.97 0.98 0.98 1.00 –0.84 0.89\nPneumonia 0.95 0.84 0.99 0.88 0.94 0.95 0.99 –0.84 0.94\nPneumothorax 0.98 0.95 0.99 0.90 0.97 0.98 0.99 –0.90 0.96\nAverage 0.98 0.91 0.97 0.93 0.96 0.98 0.98 –0.91 0.94\nStanford MOUD\nDepression 0.86 0.87 0.97 0.61 0.88 0.93 0.97 –0.61 0.91\nAlcohol dependence 0.85 0.81 0.91 0.69 0.74 0.75 0.91 –0.69 0.91 a\nSubstance use disorder 0.89 0.88 0.91 0.71 0.84 0.94 0.94 –0.71 0.87\nUnhoused 0.97 0.97 0.97 0.96 1.00 0.97 1.00 –0.96 0.94\nTobacco dependence 0.95 0.98 0.99 0.70 0.90 0.92 0.99 –0.70 0.90\nPersonality disorder 0.81 0.90 1.00 0.67 0.97 0.86 1.00 –0.67 0.95\nBipolar disorder 0.90 0.94 1.00 0.91 0.89 0.94 1.00 –0.89 0.90\nPTSD 0.95 0.95 1.00 0.89 0.96 0.94 1.00 –0.89 0.85\nADHD 0.94 0.97 0.97 0.77 0.87 0.84 0.97 –0.77 0.77\nSuicidal behavior 0.96 0.95 0.99 0.83 0.91 0.97 0.99 –0.83 0.87\nLiver disease 0.82 0.97 0.99 0.62 0.81 0.94 0.99 –0.62 0.89\nChronic pain 0.95 0.97 0.95 0.88 0.94 0.94 0.97 –0.88 0.98 a\nUnemployment 1.00 0.98 1.00 0.88 0.84 0.95 1.00 –0.84 0.98\nAverage 0.91 0.93 0.97 0.78 0.89 0.91 0.97 –0.78 0.90\naFine-tuned BERT F1 score higher than the trainer model’s F1 score on the same held-out test set.\nFig. 1 | CLEAR information retrieval ablation F1 scores on Stanford MOUD dataset.F1 scores for information retrieval using NER, Ontology, LLM augmentation, or\nOntology + LLM Augmentation on the Stanford MOUD Dataset. F1 scores were calculated for all 13 variables using GPT-4.\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 3\n1.04 s (Flan-T5) to 10.24 s (Med42) for CLEAR; from 4.92 s (Flan-T5) to\n35.07 s (Med42) for chunk embedding; and from 7.20 s (Flan-UL2) to\n42.43 s (Med42) for full note. The average number of model calls per note\nwas 1.68 for CLEAR vs. 4.94 for chunk embedding. These numbers were the\nsame across models since all models were called once per retrieved chunk.\nF o rt h ef u l l - n o t ea p p r o a c h ,t h en o t ew a sc h u n k e da c c o r d i n gt ot h ec o n t e x t\nwindow of each model. As a result, models with large input token limits—\nlike GPT-4 (125k) and Mixtral (128k)— required fewer model calls. The\naverage number of input tokens per note was substantially less in CLEAR\ncompared to chunk embedding and full note. On average, CLEAR had 81%\nfewer input tokens than the full-noteapproach and 71% fewer input tokens\nthan chunk embedding (Fig.2b–d and Supplementary Table 12). We esti-\nmated the time required for human evaluators to extract the same infor-\nmation from the training data by recording how long it took our domain\nexpert to annotate 100 notes. On average, it took 57 s for a human to\nannotate one variable in a clinical note, which would result in approximately\n3299 h to complete the annotation of 13 variables in 16,031 notes. In\ncomparison, CLEAR would process approximately 1.681 note chunks per\nnote, resulting in 26,948 model callsfor this same annotation task. Our\nfastest model takes an average time of 1.039 s per note chunk, reducing the\ntask to 101 h, while the slowest model takes approximately 10.241 s, totaling\n997 h. This represents a 96.9% efﬁciency gain with the fastest model and a\n69.8% gain with the slowest, compared to human annotation.\nWe calculated ROUGE-L F-measures to test whether chunk embed-\nding performed worse at information ex t r a c t i o nw h e nt h er e t r i e v e dt e x t\noverlapped less with the text retrieved by CLEAR. When both CLEAR and\nchunk embedding succeeded (true positives and true negatives), the average\nROUGE-L was 77%. When CLEAR succeeded, but chunk embedding failed\n(false positives and false negatives), ROUGE-L was also 77%, suggesting that\nperformance differences cannot be attributed to lack of overlap in the\nretrieved text (p-value > 0.05). However, the average top-k ranks for TPs and\nTNs with the highest F-measure (TPs = 3.12, TNs = 4.08) were more\nfavorable than those for FPs and FNs (FPs = 4.11, FNs = 5) (p-value = 0.01),\nindicating that the embedding similarity measure used by the chunk\nembedding method may not effectively prioritize the most relevant chunks\n(Supplementary Table 13). Overall, while both CLEAR and chunk\nembedding methods retrieve similarly high-yield content, CLEAR proves to\nbe a more efﬁcient information retrieval tool, returning relevant content in\nfewer chunks (4.94 average chunk embedding chunks per note vs. 1.681\naverage CLEAR embedding chunks per note) (Supplementary Table 12).\nDiscussion\nIn this paper, we propose CLEAR, a RAG pipeline that retrieves note\nexcerpts containing clinical-named entities relevant to the input query. We\nshow that CLEAR, when used for extraction of 13 variables from clinical\nnotes, outperformed chunk embeddingand full-note approaches, achieving\n3% higher F1 on average with 71% fewer input tokens, 72% faster inference\ntime, and 66% fewer model queries. We also demonstrated that CLEAR\noutputs can be used toﬁne-tune BERT-sized modelsfor variable extraction,\nresulting in performance comparable to larger models.\nOur analysis suggests that CLEAR outperforms chunk embedding and\nfull-note approaches for two main reasons. First, CLEAR retrieves shorter\ncontext segments. Prior studies have shown that longer contexts can\ndegrade LLM performance. For example, in the FlenQA dataset, which\ninvolves three reasoning tasks, Levyet al. observed that as input length\nincreases, model performance deteriorates regardless of whether the key\ninformation is located at the beginning, middle, or end of the input context,\nand that degradation occurs well before reaching the context limit of the\nmodels\n42. Similarly, Liu et al. report the“lost in the middle” phenomenon,\nwhere LLMs perform worse when key information is buried in the middle of\nthe input context compared to being at the beginning or end52.T h e ya l s o\nnoted that models with longer context capabilities, such as the 16k versions\nof GPT-3.5, did not outperform shorter context models. In our own analysis,\nmodels like Mixtral, Llama, and GPT-4, despite having context windows\nlarge enough to accommodate multiple notes, did not perform as well as\nCLEAR when processing the full note.\nSecond, we noted that the embedding model tends to rank chunks\ndifferently than CLEAR, often downranking critical chunks. This observa-\ntion is consistent with ourﬁndings that chunk embedding performance\nimproves as the number of chunks retrieved increases from 3 to 5 to 10. Note\nthat we processed each chunk in separate model calls rather than within a\nsingle large context. Prior research supports the idea that retrieval of most\nFig. 2 | LLM information extraction comparisons on Large Token Stanford\nMOUD Dataset.Average F1 Score comparison between CLEAR and full-note or\nchunk embedding approach. F1 scores were averaged across our 13 held-out test sets.\nP-values reﬂect the Wilcoxon Signed-Rank Test on F1 scores across all 13 held-out\ntest sets between CLEAR and full-note or chunk embedding comparisons (a). Chunk\nembedding top-k equals 5 in these experiments. We evaluated average inference time\nper note (b), average model queries per note (c), and average input tokens per note\n(d) on the Large Token Stanford MOUD Dataset across full note, chunk embed-\ndings, and CLEAR methods forﬁve models. Chunk embedding top-k equals 5 in\nthese experiments. All metrics are calculated on 4xNVIDIA A100 80GB GPUs. To\ncalculate the total tokens retrieved for GPT-4, we used Med42 as the representative\ntokenizer. *p < 0.05,**p < 0.01.\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 4\ns i m i l a rd o c u m e n tc h u n k si sn o ta l w a y so p t i m a l .F o ri n s t a n c e ,G a ne ta l .\npropose METRAG, which combines a similarity model with a utility model\nfor retrieval,ﬁnding that their approach outperforms traditional similarity-\nbased RAG approaches across various QA datasets.\nCLEAR’s use of NER aligns with a robust precedent in RAG meth-\nodologies. A recent review of RAG approaches included 16 studies that\nincorporate entity recognition and entity-based reasoning in different ways\nfor RAG\n53. For instance, NER can be employed to edit or revise generated\ncontent. In CBRKBQA, NER aids in revising results by aligning generated\nrelations with those in the local neighborhood of the query entity within a\nknowledge graph\n54. Similarly, GMT-KBQA re-ranks retrieved entities and\nrelations and conducts relation classiﬁcation and entity disambiguation\nprior to generation55. Beyond content revision, several approaches use\nentities to extract information directly from knowledge graphs. For exam-\nple, FC-KBQA, StructGPT, and KAPING retrieve relevant triplets and facts\nbased on entity matching49,56,57. Xu et al. search across entities to identify\nrelevant subgraphs in knowledgegraphs for customer support issues58,a n d\nKnowledgeNavigator leverages NER for iterativeﬁltering of relations to\nretrieve pertinent triplets from knowledge graphs59.F u r t h e r m o r e ,R H O\nintegrates entity embedding with knowledge graph embeddings to enhance\ndialog generation\n60. These methodologies underscore the versatility of NER\nin RAG, not only for retrieving information but also for structuring and\nreﬁning content generation. NER can also be used to facilitate automated\nknowledge graph generation, suggesti n gt h a tC L E A Rc o u l db eu s e dt ob o t h\ngenerate knowledge graphs\n61 and retrieve from them to improve LLM\nperformance47–49.\nOur study faces certain limitations. First, we restricted our evaluation to\nthe task of clinical variable extraction. Future research should explore the\nperformance of CLEAR on other tasks that can beneﬁt from retrieval,\nincluding summarization, question answering, and clinical reasoning. Uti-\nlizing benchmarks like MedAlign\n62 can provide a more comprehensive\nevaluation of CLEAR’s capabilities across a broader range of tasks. Second,\nin our chunk embedding comparison, we segmented chunks based on the\ncontext window of the embedding model. While this approach is consistent\nwith prior methods\n3, it is possible that using different-sized embedding\nchunks could yield similar accuracy to CLEAR. However, in our experiment,\nwhere we increased the CLEAR token size and decreased the chunk\nembedding token size, CLEAR still outperformed chunk embedding (see\nSupplementary Table 11). Theseﬁndings are consistent with the data in\nSupplementary Table 13, where CLEAR and chunk embedding methods do\nnot retrieve substantially different information, and increasing CLEAR’s\ncontext size does not negatively impact CLEAR’s performance. Instead,\nchunk embedding underperforms because the embedding similarity mea-\nsures may not effectively prioritize the most relevant chunks. We believe\nfurther exploration is warranted, although a deep dive was beyond the scope\nof this paper. Future experiments should investigate the impact of chunk\nsize tuning on performance. Third,our task required the information\nretrieval LLM process only one note chunk at a time. This can be adapted if\nthe task requires extracting information from multiple note types. Several\nCLEAR note chunks from different notes can be combined into a single\nprompt for LLM inference, however, this was not explored in our paper.\nFourth, additional prompt tuning for CLEAR steps (NER, LLM augmen-\ntation, and entity selection) is needed for full optimization, and language\ncould have been made more consistent between the prompts used at dif-\nferent stages of the pipeline. Fifth, changes in data over time are inherent in\nmedical studies. The data split we selected resulted in a higher proportion of\nCOVID and post-COVID era notes in the Stanford MOUD testing dataset,\nwhich may contain a higher proportion of notes reﬂecting worsened mental\nhealth among patients\n63. Although we made efforts to check for imbalances\nin our training and testing datasets, these inherent differences may still exist\nand could impact our evaluation. Lastly, our analysis did not incorporate\nmodel quantization methods for LLMinference. Implementing model\nquantization could strike a balance between efﬁciency and performance,\nmaking it a valuable area for future research. By optimizing model conﬁg-\nurations through quantization, we can enhance scalability and applicability\nin diverse contexts without compromising on performance, thereby pro-\nviding more comprehensive insights into the optimal use of CLEAR in\nclinical information extraction.\nTraditional methods of using LLM for clinical information extraction\nare time-consuming and cost-prohibitive. Our work introduces a more\nefﬁcient RAG pipeline that identiﬁes relevant note chunks using clinical\nNER before performing variable extraction, leading to a more than 70%\nreduction in both token usage and processing time. Importantly, these\nefﬁciencies were achieved with a slight gain in performance when compared\nto approaches that utilize entire documents or embed note chunks for\nretrieval. This work demonstrates that the application of LLMs in healthcare\ncan be made more affordable and practical. We have validated this method\nin the context of variable extraction, showing its potential to transform the\nlandscape of clinical informationprocessing in healthcare settings.\nMethods\nData source\nWe used data from two EHR-derived datasets from Stanford Hospital. The\nﬁrst was the Stanford Medicationfor Opioid Use Disorder (MOUD)\ncohort6,64. This cohort includes data from patients treated for opioid use\ndisorder at Stanford Hospital between 2009 and 2023. Patients aged 18–89\nwho were prescribed buprenorphine-naloxone for more than a day were\nincluded. The cohort was split into a training and testing dataset by treatment\nstart dates, using data up to 2020 for training and from 2021 onwards for\ntesting. The treatment start date was used to split the data to simulate data cut-\noffs that would be expected in a real-world deployment of CLEAR. To evaluate\nthe similarity between the training andtesting data after the date split, we\nanalyzed the proportion of key concept mentions (unemployment, home-\nlessness, food insecurity, substance dependence, suicidal ideation, depression,\noverdose) in both sets. Using a regular expression search with synonyms for\neach variable (Supplementary Table14), we found minor differences in\nconcept mentions across the datasets (Supplementary Table 15). There were\n767 patients in the training dataset with 16031 unique notes and 505 patients\nin the testing dataset with 12319 unique notes. Combined, the testing and\ntraining datasets had a min, median, and max token lengths of 218, 1778, and\n10,981, respectively. Thirteen variables were selected for manual annotation\nby a board-certiﬁed addiction medicine physician due to their importance in\ndelivering medication-assisted therapy. These variables included clinical\ndiagnoses (depression, alcohol dependence, substance use disorder, ADHD,\nbipolar disorder, chronic pain, liver disease, personality disorder, PTSD, sui-\ncidal behavior, tobacco dependence) and social determinants of health\n(housing and employment status. All data were de-identiﬁed using the Safe\nHarbor method according to NIST guidelines, with clinical text undergoing\nadditional anonymization via the TiDE algorithm\n65. Approval for the study\nwas obtained from the Stanford University Institutional Review Board, pro-\ntocol number 67423. This study was an analysis of routinely collected EHR\ndata, and posed no additional risk to patients.\nThe second data source was CheXpert, a dataset of radiology reports\nfrom Stanford Hospital66 with programmatic labels forﬁve well-deﬁned\nclinical entities commonly found in chest x-ray reports: cardiomegaly,\npulmonary edema, pleural effusion, pneumonia, and pneumothorax. These\nﬁve were selected at random out of the 14 labeled observations in CheXpert.\nWe downsampled the CheXpert dataset into a testing and training dataset\nby using the existing CheXpert agent’s labels to randomly sample from the\nlarger CheXpert dataset. For the training dataset, we randomly sampled 700\nnotes for each of theﬁve selected clinical entities. We only sampled notes\nthat the CheXpert agent labeled as“present”or “negated”to upsample notes\nwith information relevant to our retrieval task. We used a similar approach\nfor the testing dataset, sampling 200 notes per entity, 100 of which had been\nlabeled by CheXpert as“present” and 100 as“negated”. After selection,\nCheXpert labels were discarded for both datasets. In total, we had 3500\npatients containing 3500 unique notes in the testing dataset, and 1000\npatients containing 1000 unique notes in the training dataset. Combined,\nthe testing and training datasets had a min, median, and max token length of\n41, 189, and 1025, respectively.\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 5\nTo prevent data leakage, we removed testing notes for any patients\nwhose IDs were present in the training data. This step ensured that no\npatient appeared in both the testing and training datasets for the Stanford\nMOUD and CheXpert tasks.\nData annotation\nFive board-certiﬁed physicians and one medical student collaboratively\nperformed manual annotation of clinicalvariables to obtain reference labels.\nWe randomly sampled 420 unique notes from the Stanford MOUD testing\ndataset to generate reference labels for 13 clinical entities outlined in Sup-\nplementary Table 16. To reduce class imbalance skewed towards negative\nand absent cases, weﬁltered the 420 unique notes using patient-level\nstructured data (ICD-10 codes) and a regular expression search, returning\nnotes containing any of the speciﬁed strings or from patients with at least\none relevant ICD-10 code (Supplementary Table 17). Ultimately, we created\n13 individual annotation datasets. The labels generated from these 13\ndatasets were used as our held-out test sets. 247 notes (20 from each dataset)\nwere randomly selected for duplicate annotation to calculate inter-rater\nreliability (IRR). For the CheXpert test set, we generate reference labels for all\n1000 notes outlined in SupplementaryTable 1. 100 notes from this subset\nwere randomly selected for duplicate annotation to calculate inter-rater\nreliability. For the Stanford MOUD and CheXpert datasets, labelers received\nspeciﬁc instructions that outlined the criteria for annotating each variable.\nTheir task involved identifying and labeling notes for the presence, absence,\nor uncertainty of a variable.\nFor each annotated variable, annotators received instructions to\nimprove consistency. Instructionsfor the Stanford MOUD Dataset anno-\ntation task can be found in Supplementary Table 18. Annotators labeled\npositive mentions of a variable as present. Negation or absent mentions were\nboth treated similarly. Ambiguous instances were marked as uncertain. 12\nnotes in the Stanford MOUD Dataset and 8 notes in CheXpert had con-\nﬂicting duplicate-annotated labels for IRR. These notes were excluded from\nt h eh e l d - o u tt e s ts e t s .\nTo evaluate the sensitivity of our information retrieval pipeline, one\nmedical student annotated a specialized dataset known as the Stanford\nMOUD NER Dataset. This dataset was created by randomly selecting 215\nzero-shot NER input texts from the Stanford MOUD Dataset and manually\nextracting clinically relevant entities and concepts. Instructions for the\nannotation task can be found in Supplementary Table 19. We used this to\nevaluate the sensitivity of our information retrieval pipeline on real-world\nclinical datasets. After annotation, we had 450 unique clinical entities and\nconcepts for our evaluation. Details outlining the creation of the zero-shot\nNER input texts can be found below.\nFull details on all datasets used in this study can be found in Supple-\nmentary Fig. 1.\nClinical entity augmented retrieval\nCLEAR uses NER to improve the accuracy and efﬁciency of clinical LLM\ntasks. CLEAR takes in two inputs: clinical notes and entities of interest. The\npipeline begins with NER to identify all clinical entities within the notes.\nNext, the identiﬁed entities areﬁltered down to those relevant to the entities\nof interest. Theﬁltered list is then augmented using ontologies and LLMs to\nincrease sensitivity. The augmented list is fed to a target matcher that\nretrieves a context window surrounding each relevant entity. The retrieved-\ncontext windows can be used for downstream tasks like summarization,\nquestion answering, or information extraction. This multi-step approach is\noutlined below and in Fig.3.\nThe ﬁrst step in the CLEAR pipeline is identifying all clinical entities in\nthe input clinical notes using a NER model. The output of this step is a list of\nunique clinical entities contained in the notes. We implemented the initial\nNER step using zero-shot NER with Flan-T5-XXL due to its specialized\nNER instruction tuning\n67. Most clinical NER models are domain-speciﬁc\nand are highly dependent on the dataset they wereﬁne-tuned on. To\nminimize these limitations, we chose Flan-T5, a domain-agnostic model, for\nNER. The model was run on a PHI-compliant virtual machine with\n8xNVIDIA L4 24GB GPUs. Given the 512context window limit of Flan-T5,\nwe used prompts of fewer than 20 tokens and chunked input text to under\n100 tokens with a 15-token stride. Weused four distinct prompts, and the\noutput-named entities from each prompt were aggregated and de-\nduplicated to form theﬁnal list of entities. Illustrations of each prompt\ntype are provided in Supplementary Fig. 2. Our approach leverages NER\nprompts to capture all clinically named entities; however, users have the\nability to craft more focused NER prompts (e.g.,“Return all named entities\nrelated to congestive heart failure”).\nTo evaluate the performance of our zero-shot NER approach, we used\ntwo datasets: (1) the NCBI Disease Dataset, which contains annotations for\n1382 unique disease names and concepts\n68, (2) the Stanford MOUD NER\nDataset, which contains annotations for 450 clinical entities and concepts.\nFig. 3 | Overview of CLEAR pipeline.CLEAR requires two inputs: (1) clinical notes\nand (2) a target entity. Initially, our CLEAR implementation applies an NER model\nto the clinical notes to extract a dataset of relevant entities. These entities are then\nﬁltered using word embeddings and cosine similarity to ensure relevance to the\ntarget entity. Next, additional entities related to the target entity are identiﬁed using\nontologies and LLMs. Theﬁnal list of entities is used to retrieve note chunks through\nregular expression matches. These chunks support a downstream LLM task (clinical\ninformation extraction).\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 6\nWe report the sensitivity of Flan-T5-XXL in identifying these entities.\nAdditionally, we characterize the false negatives into the following\ncategories:\n1. Acronym recognition failures: the model recognized either the full\nterm or an acronym for a concept, but not both (ex:“colorectal cancer”\nwas identiﬁed, but“crc” was missed).\n2. Morphological variance failures: the model recognized either the\npleural or singular noun version of the concept, but not both\n(ex: “glioblastoma” was identiﬁed, but“glioblastomas” was missed).\n3. Partial failures: model failed to recognize the same concept in different\ncontexts (ex:“tay sachs disease”, “tay sachs mutation”, “ashkenazi tay\nsachs disease”, “tay sachs disease gene”were identiﬁed, but“tay sachs”\nwas missed).\n4. Other failures: zero-shot NER failures that do not fall into any of the\nother categories (ex:“retinitis punctata albescens” was missed).\nWe also investigated the impact of removing the NER step on the\noverall performance of CLEAR. On all 13 variables in the Stanford MOUD\nDataset, we ran the CLEAR pipeline with and without the initial NER step.\nWhen running CLEAR without NER, the only entities selected are those\nidentiﬁed during entity augmentation (described below) with ontologies\nand LLMs. The selected entities were then used to retrieve sections of the\nnote that were passed to GPT-4 to extract information about a variable of\ninterest. We report the average F1 of the information extraction task.\nOnce the unique clinical entities from all notes are identiﬁed, the entities\nrelevant to the input target entity are selected. These selected entities are\neventually used to retrieve relevant context windows for downstream LLM\ntasks. First, all entities identiﬁed via NER and the target entity were embedded\nusing Bio+Clinical BERT\n69, and those entities with a cosine similarity≥0.85\ncompared to the target entity were retained. We selected a cosine similarity\nthreshold of 0.85 after empirical testing attempting to balance the exclusion of\nirrelevant entities with the retention ofrelevant ones. The resulting entities\nwere passed to GPT-4 with a prompt toﬁlter the list to those most relevant to\nthe target entity (Supplementary Fig. 3). The entityﬁltering step is modular,\nallowing users to apply Bio+Clinical BERT cosine similarity, an LLM, a\nhuman, or any combination to improve entity selection.\nNext, theﬁltered entity list is augmented to account for entities missed\nduring NER. Without an augmentation step, entities missed during NER\ncould lead to incorrect context retrieval downstream. For example, if the entity\n“lesch nyhan” w a sm i s s e db yN E R ,a n dt h et a r g e te n t i t yi s“Lesch-Nyhan\nsyndrome”, the downstream information retrieval might fail to retrieve sec-\ntions of the note that mention“lesch nyhan”. Here, we used the UMLS\nontology\n70 and GPT-4 to augment the list of entities from NER. We used the\nsearch endpoint from the UMLS API to retrieve concept names related to the\ntarget entity, and retained concept names originating from the National\nLibrary of Medicine Metathesaurus or SNOMED CT\n71.W ea l s op r o m p t e d\nGPT-4 to generate synonyms for the target entity (Supplementary Fig. 4).\nTo evaluate the impact of the entityaugmentation step, we measured to\nwhat extent ontology and LLM augmentation recover entities missed in the\nNER step (false negatives). To evaluatethe impact of the entity augmentation\nstep, we measured how well ontology and LLM augmentation recover entities\nmissed in the NER step (false negatives), as minimizing false negatives is crucial\nfor downstream information retrieval. Weprioritized maximizing sensitivity/\nrecall, as false positives can be managed by the downstream LLM task, whereas\nfalse negatives result in complete loss ofinformation. For each entity missed by\nthe NER step, we treat a variant of the missed entity as the target entity and use\nthe UMLS ontology and GPT-4 to generate synonyms as described above. For\nexample, the formal name of an entity was used (“Lesch-Nyhan syndrome”)i fa\nvariant was missed (“lesch nyhan”) .I ft h ef o r m a ln a m ew a st h em i s s e dt e r m ,a\nbroader term that would encompass the formal name (“purine salvage deﬁ-\nciencies”) was used as the target entity. We report the proportion of entities\nmissed during NER that were recovered through this augmentation step.\nThe selected entities are used to develop a regular expression tool for\ninformation retrieval. Speciﬁcally, we employed the Target Matcher pro-\nvided by MedSpaCy\n72. We used the TargetRule class from the MedSpaCy\nNER module for identifying mentions of the selected entities within clinical\nnotes and then pulled a context window of 150 words before and after the\ntarget entity. These retrieved-context windows are passed to an LLM for\ndownstream inference tasks.\nInformation extraction\nWe used the retrieved-context windows from CLEAR to extract the infor-\nmation (ex: is the feature present, negated/absent, or uncertain) of 13\nvariables in the Stanford MOUD dataset and 5 variables in the CheXpert\ndataset. We compared the performance of several models with different\ncontext windows on this task. These models included: Med42–70b\n73,\nMixtral-8x7B-Instruct-v0.174,L l a m a - 3–70b75,F l a n - T 5 - X X L67,F l a n - U L 276,\nand GPT-477. We ran GPT-4 via a secure Azure PHI-compliant instance.\nThe otherﬁve models were run on a PHI-compliant virtual machine with\n4xNVIDIA A100 80GB GPUs.\nWe designed prompts that included synthetic in-context examples\ngenerated by GPT-478.W ei n c l u d e dﬁve examples for each entity, covering all\npossible labels that the LLM was to discern:“0”for entity negated/absence,“1”\nfor presence, and“2”for uncertainty. We selected 5-shot prompting based on\ni t sd e m o n s t r a t e dp e r f o r m a n c eg a i n si np r i o rw o r k36. To mitigate any potential\nissues with the context window limitations of each model, we kept the syn-\nthetic data points under 100 tokens. We kept each example under 100 tokens\nto ensure they provide meaningful insights into the task, improving LLM\ninstruction following without consuming excessive tokens. In comparing our\nmethods to the traditional approach,also known as the full-note method, we\naccounted for the increase in token length due to our prompting strategies,\nensuring the models’token limits were not exceeded. An example of our LLM\ni n f o r m a t i o ne x t r a c t i o np r o m p ti sp r o v i d e di nS u p p l e m e n t a r yF i g .5 .\nFor each model, we report classiﬁcation metrics (sensitivity, speciﬁcity,\nNPV, PPV, F1) comparing the LLM information extraction labels to\nhuman-annotated labels.\nWeak labeling\nWe compared CLEAR to a weak labelingapproach. Weak supervision was\nused to label the 2000 CheXpert notes in the training set for ourﬁve\nCheXpert entities of interest. Labeling functions are rough heuristics used to\nprogrammatically generate weak labels from unlabeled data. We manually\nreviewed the 250 notes in the training set and created labeling functions\nusing keyword matching and regular expressions from a list of synonyms\ncreated by a domain expert and supplemented by GPT-4 using the prompt\nin Supplementary Fig. 4. Theﬁnal list of synonyms can be found in Sup-\nplementary Table 20. For example, a labeling function might label a note as\nhaving cardiomegaly if it contains the strings“enlarged cardiac silhouette,”\n“enlarged heart,” or “ventricular hypertrophy,” and abstain otherwise. The\nlabeling functions for each entity were used to train a model that combines\nthe outputs of multiple labeling functions for a given entity, leveraging their\ncollective knowledge and handling their conﬂicts\n79.\nModel distillation\nWe investigated whether the output of CLEAR could be used toﬁne-tune a\nsmaller language model to perform the information extraction task. We\nused the output of CLEAR toﬁne-tune BERT models to perform a binary\nclassiﬁcation task (present vs. negated/absent) for each of the variables in the\nStanford MOUD and CheXpert datasets. We omitted the“uncertain”class\ndue to small sample sizes in certainﬁne-tuning datasets (Supplementary\nTable 21). Weﬁne-tuned Bio+Clinical BERT, which was initialized from\nBioBERT and trained on all MIMIC notes\n69. For each variable, we selected\nthe best performing LLM, excluding GPT-4 on the information extraction\ntask to weakly label theﬁne-tuning dataset (Supplementary Table 21). We\nexcluded GPT-4 since OpenAI terms of use prohibit using GPT-4 outputs to\ndevelop competitor models\n80.T h eﬁne-tuning dataset for each variable\nconsisted of every note chunk containing an entity of interest (inputs) and\nlabel generated by an LLM (label). We removed note chunks that contained\n>10% overlapping words, resulting in less than 2% of note chunks being\nﬁltered out.\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 7\nThe ﬁne-tuning datasets for each variable were divided into a 70%\ntraining set and a 30% validation set.Hyperparameters were tuned using 10-\nfold cross-validation on 70% of the training data to maximize the area under\nthe receiver operating curve (AUC). We selected a range of variables for each\nhyperparameter and performed a grid search toﬁnd the best hyperparameter\nconﬁgurations. Our grid search included learning rate (5e-5, 3e-5, 2e-5),\nbatch size (8, 16, 32), number of training epochs (4, 5, 10), and weight decay\n(0.01, 0.05, 0.1). Theﬁnal models wereﬁne-tuned on 100% of theﬁne-tuning\ndataset. Performance metrics for theﬁne-tuned classiﬁer were generated\nusing the held-out test sets. To preventdata leakage, we removed testing notes\nfor any patients whose IDs were present in theﬁne-tuning data (training and\nvalidation datasets). This ensured that no patient appeared in both the held-\no u tt e s ts e ta n dﬁne-tuning datasets for the Stanford and CheXpert variables.\nComparison to chunk embedding and full-note approaches\nTo quantify the impact of retrieving text around entities, we compared\nCLEAR to a RAG pipeline leveraging note chunk embeddings and a naive\napproach that retrieves the full note. Weﬁltered the test sets down to longer\nnotes to focus this comparison on notes that approached or exceeded\nmodels’input context window. Speciﬁcally, we select the 50% longest notes\nin the Stanford MOUD Dataset.\nFor the chunk embedding RAG pipeline, we used the BAAI General-\nized Embeddings (BGE) model as our embedding model and cosine simi-\nlarity as the retriever (Supplementary Fig. 6). BGE is a high-performance\nembedding model known for its accuracy on retrieval benchmarks\n81.W eﬁrst\nsegmented all patient notes into chunks of 490 tokens with a stride of 128\ntokens, given BGE’s maximum context window of 512. We then generated\nembeddings for each chunk as well as every target entity and its deﬁnition\nand stored them in an embedding database. To select the most relevant note\nchunks for our information extraction task, we perform cosine similarity to\nmeasure the alignment of each note chunk against the target entity’sd eﬁ-\nnition embedding. We retrieved the top-k (where k =3 ,5 ,1 0 )n o t ec h u n k s\nbased on cosine similarity scores. For notes with fewer than k chunks, we\nretrieved all chunks. The retrieved chunks were passed to an LLM for the\ninformation extraction task. For notes with multiple chunks, we aggregated\nthe LLM labels from these chunks to generate aﬁnal label for the note.\nFor the full-note approach, we chunked notes based on each model’s\ncontext window limit, using a stride of 128 tokens\n4 (Supplementary Fig. 7).\nWe passed each chunk to an LLM for the information extraction task and\naggregated the labels from these segments to produce aﬁnal label for\neach note.\nFor all LLMs, we compare the performance of CLEAR, chunk\nembeddings, and the full-note approachon the information extraction tasks\nas well as on three metrics related to inference efﬁciency. For the infor-\nmation extraction tasks, we report the average inference time per note,\naverage model queries per note, and average tokens retrieved per note. We\ndo not report inference time for GPT-4since it was run using a proprietary\nAPI. We used the Wilcoxon Signed-Rank Test to compare the differences\nbetween the three methods\n82.\nWe tested two hypotheses regarding performance differences between\nCLEAR and chunk embedding. First, we hypothesized that chunk embed-\ndings would perform worse than CLEAR when the retrieved chunks\noverlapped less with the chunks retrieved by CLEAR. To test this, we cal-\nculated ROUGE-L F-measure— a measure of the longest common sub-\nsequence between two strings— on the chunks retrieved by CLEAR and\nchunk embedding, treating the CLEAR chunks as the reference. We report\nROUGE-L for cases where both CLEAR and chunk embeddings succeeded\n(true negatives or true positives), and for cases where CLEAR succeeded but\nchunk embeddings failed (false negatives and false positives). Second, we\nhypothesized that chunk embeddings would perform worse than CLEAR\nwhen the parts of the note retrieved by CLEAR were ranked lower by the\nchunk embedding model. To do this, we calculated the chunk embedding\nmodel ranking of the chunk that overlapped most with the CLEAR chunk\n(as measured by ROUGE-L).\nModel summary\nFor NER, we relied on Flan-T5-XXL. LLM Augmentation used GPT-4. Our\nentity selection cosine similarity model was Bio+ClinicalBERT, and the\nentity ﬁltering LLM was GPT-4. Information extraction was tested on six\nmodels: Med42–70b, Mixtral-8x7B-Instruct-v0.1, Llama-3–70b, Flan-T5-\nXXL, Flan-UL2, and GPT-4. The chunk embedding model was BAAI\nGeneralized Embeddings Large English v1.5, and for model distillation, we\nﬁne-tuned Bio+ClinicalBERT (Supplementary Table 22). Model usage\nparameters for NER and information extraction are reported in Supple-\nmentary Table 23.\nData availability\nThe Stanford MOUD Cohort Dataset used in this study contains identiﬁable\nprotected health information and, therefore, cannot be shared publicly.\nStanford University investigators with appropriate IRB approval can contact\nthe authors directly regarding data access.\nCode availability\nThe code used to run CLEAR and reproduce results can be found athttps://\ngithub.com/iv-lop/clear.\nReceived: 1 July 2024; Accepted: 8 December 2024;\nReferences\n1. Ross, M. K., Wei, W. & Ohno-Machado, L. Big data and the electronic\nhealth record.Yearb. Med. Inform.9,9 7–104 (2014).\n2. Meystre, S. M., Savova, G. K., Kipper-Schuler, K. C. & Hurdle, J. F.\nExtracting information from textual documents in the electronic health\nrecord: a review of recent research.Yearb. Med. Inform.17, 128–144\n(2008).\n3. Wornow, M. et al. Zero-shot clinical trial patient matching with LLMs.\nPreprint athttps://doi.org/10.48550/arXiv.2402.05125 (2024).\n4. Alsentzer, E. et al. Zero-shot interpretable phenotyping of postpartum\nhemorrhage using large language models.Npj Digit. Med.6,1 –10\n(2023).\n5. Callahan, A., Shah, N. H. & Chen, J. H. Research and reporting\nconsiderations for observational studies using electronic health\nrecord data.Ann. Intern. Med.172, S79–S84 (2020).\n6. Lopez, I. et al. Predicting premature discontinuation of medication for\nopioid use disorder from electronic medical records.Amia. Annu.\nSymp. Proc.2023, 1067–1076 (2024).\n7. Zweigenbaum, P., Demner-Fushman, D., Yu, H. & Cohen, K. B.\nFrontiers of biomedical text mining: current progress.Brief. Bioinform.\n8, 358–375 (2007).\n8. Wang, Y. et al. Clinical information extraction applications: a literature\nreview. J. Biomed. Inform.77,3 4–49 (2018).\n9. Lample, G., Ballesteros, M., Subramanian, S., Kawakami, K. & Dyer,\nC. Neural Architectures for Named Entity Recognition. inProceedings\nof the 2016 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies (eds. Knight, K., Nenkova, A. & Rambow, O.) 260–270\nhttps://doi.org/10.18653/v1/N16-1030 (Association for\nComputational Linguistics, San Diego, California, 2016).\n10. Kågebäck, M. & Salomonsson, H. Word Sense Disambiguation using\na Bidirectional LSTM. inProceedings of the 5th Workshop on\nCognitive Aspects of the Lexicon (CogALex - V)(eds. Zock, M., Lenci,\nA. & Evert, S.) 51–56 (The COLING 2016 Organizing Committee,\nOsaka, Japan, 2016).\n11. Wu, H. et al. A survey on clinical natural language processing in the\nUnited Kingdom from 2007 to 2022.Npj Digit. Med.5,1 –15 (2022).\n12. Jung, K. et al. Functional evaluation of out-of-the-box text-mining\ntools for data-mining tasks.J. Am. Med. Inform. Assoc. JAMIA22,\n121–131 (2015).\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 8\n13. Percha, B. Modern clinical text mining: a guide and review.Annu. Rev.\nBiomed. Data Sci.4, 165–187 (2021).\n14. Agarwal, A. R., Prichett, L., Jain, A. & Srikumaran, U. Assessment of\nuse of ICD-9 and ICD-10 codes for social determinants of health in the\nUS, 2011-2021.JAMA Netw. Open6, e2312538 (2023).\n15. Truong, H. P. et al. Utilization of social determinants of health ICD-10\nZ-codes among hospitalized patients in the United States,\n2016–2017. Med. Care58, 1037 (2020).\n16. Swaminathan, A. et al. Selective prediction for extracting unstructured\nclinical data.J. Am. Med. Inform. Assoc.31, 188–197 (2024).\n17. Liu, C. et al. Predictive value of clinical complete response after\nchemoradiation for rectal cancer.J. Am. Coll. Surg.235, S51 (2022).\n18. Liao, K. P. et al. Development of phenotype algorithms using\nelectronic medical records and incorporating natural language\nprocessing. BMJ 350, h1885 (2015).\n19. Huang, Z., Xu, W. & Yu, K. Bidirectional LSTM-CRF models for\nsequence tagging. Preprint athttp://arxiv.org/abs/1508.01991\n(2015).\n20. Zhang, Y., Zhang, Y., Qi, P., Manning, C. D. & Langlotz, C. P.\nBiomedical and clinical English model packages for the Stanza\nPython NLP library.J. Am. Med. Inform. Assoc. JAMIA28, 1892–1899\n(2021).\n21. Qi, P., Zhang, Y., Zhang, Y., Bolton, J. & Manning, C. D. Stanza: a\nPython natural language processing toolkit for many human\nlanguages. inProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics: System Demonstrations\n(eds. Celikyilmaz, A. & Wen, T.-H.) 101–108 (Association for\nComputational Linguistics, Online).https://doi.org/10.18653/v1/\n2020.acl-demos.14 (2020).\n22. Uzuner, Ö., South, B. R., Shen, S. & DuVall, S. L. 2010 i2b2/VA\nchallenge on concepts, assertions, and relations in clinical text.J. Am.\nMed. Inform. Assoc. JAMIA18, 552–556 (2011).\n23. Fries, J. A. et al. Ontology-driven weak supervision for clinical entity\nclassiﬁcation in electronic health records.Nat. Commun.12, 2017\n(2021).\n24. Jaber, A. & Martínez, P. Disambiguating clinical abbreviations using a\none-ﬁts-all classiﬁer based on deep learning techniques.Methods Inf.\nMed. 61, e28–e34 (2022).\n25. Gu, Y. et al. Domain-speciﬁc language model pretraining for\nbiomedical natural language processing.ACM Trans Comput.\nHealthc. 3,1 –23 (2022).\n26. Zambrano Chaves, J. et al. RaLEs: a Benchmark for Radiology\nLanguage Evaluations. in Advances in Neural Information Processing\nSystems (eds. Oh, A. et al.) vol. 36 74429–74454 (Curran Associates,\nInc., 2023).\n27. Yan, A. et al. RadBERT: adapting transformer-based language\nmodels to radiology.Radiol. Artif. Intell.4, e210258 (2022).\n28. Sushil, M., Ludwig, D., Butte, A. J. & Rudrapatna, V. A. Developing a\ngeneral-purpose clinical language inference model from a large\ncorpus of clinical notes. arXiv.orghttps://arxiv.org/abs/2210.06566v1\n(2022).\n29. Lin, B. Y. et al. Differentiable Open-Ended Commonsense Reasoning.\nin Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language\nTechnologies (eds. Toutanova, K. et al.) 4611–4625. https://doi.org/\n10.18653/v1/2021.naacl-main.366 (Association for Computational\nLinguistics, Online, 2021).\n30. Klein, T. & Nabi, M. Attention is (not) all you need for commonsense\nreasoning. https://arxiv.org/abs/1905.13497v1 (2019).\n31. Li, L., Xin, X. & Guo, P. The exploration of the reasoning capability of\nBERT in relation extraction. in2020 10th International Conference on\nInformation Science and Technology (ICIST)219–228. https://doi.org/\n10.1109/ICIST49303.2020.9202183 (2020).\n32. Amirizaniani, M., Martin, E., Sivachenko, M., Mashhadi, A. & Shah, C.\nDo LLMs exhibit human-like reasoning? Evaluating theory of mind in\nLLMs for open-ended responses.https://arxiv.org/abs/2406.\n05659v1 (2024).\n33. Sushil, M. et al. CORAL: Expert-curated oncology reports to advance\nlanguage model inference.NEJM AI1, AIdbp2300110 (2024).\n34. Van Veen, D. et al. Adapted large language models can outperform\nmedical experts in clinical text summarization.Nat. Med.1 –9 https://\ndoi.org/10.1038/s41591-024-02855-5 (2024).\n35. Tu, T. et al. Towards conversational diagnostic AI. Preprint athttps://\ndoi.org/10.48550/arXiv.2401.05654 (2024).\n36. Brown, T. et al. Language Models are Few-Shot Learners. in\nAdvances in Neural Information Processing Systems vol. 33\n1877–1901 (Curran Associates, Inc., 2020).\n37. Moor, M. et al. Foundation models for generalist medical artiﬁcial\nintelligence. Nature 616, 259–265 (2023).\n38. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large\nlanguage models are few-shot clinical information extractors. in\nProceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing(eds. Goldberg, Y., Kozareva, Z. & Zhang, Y.)\n1998–2022. https://doi.org/10.18653/v1/2022.emnlp-main.130\n(Association for Computational Linguistics, Abu Dhabi, United Arab\nEmirates, 2022).\n39. Guevara, M. et al. Large language models to identify social\ndeterminants of health in electronic health records.Npj Digit. Med.7,\n1–14 (2024).\n40. Goel, A. et al. LLMs Accelerate Annotation for Medical Information\nExtraction. inProceedings of the 3rd Machine Learning for Health\nSymposium 82–100 (PMLR, 2023).\n41. Mahbub, M. et al. Leveraging large language models to extract\ninformation on substance use disorder severity from clinical notes: a\nzero-shot learning approach. Preprint athttp://arxiv.org/abs/2403.\n12297 (2024).\n42. Levy, M., Jacoby, A. & Goldberg, Y. Same Task, More Tokens: the\nImpact of Input Length on the Reasoning Performance of Large\nLanguage Models. inProceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics(Volume 1: Long Papers)\n(eds. Ku, L.-W., Martins, A. & Srikumar, V.) 15339–15353. https://doi.\norg/10.18653/v1/2024.acl-long.818 (Association for Computational\nLinguistics, Bangkok, Thailand, 2024).\n43. Shaham, U., Ivgi, M., Efrat, A., Berant, J. & Levy, O. ZeroSCROLLS: a\nzero-shot benchmark for long text understanding.https://arxiv.org/\nabs/2305.14196v3 (2023).\n44. Bai, Y. et al. LongBench: A bilingual, multitask benchmark for long\ncontext understanding.https://arxiv.org/abs/2308.14508v1 (2023).\n45. Li, J., Wang, M., Zheng, Z. & Zhang, M. LooGLE: Can long-context\nlanguage models understand long contexts?https://arxiv.org/abs/\n2311.04939v1 (2023).\n46. Gao, Y. et al. Retrieval-augmented generation for large language\nmodels: a survey. Preprint athttps://doi.org/10.48550/arXiv.2312.\n10997 (2024).\n47. Agrawal, G., Kumarage, T., Alghamdi, Z. & Liu, H. Can Knowledge\nGraphs Reduce Hallucinations in LLMs? : A Survey. inProceedings of\nthe 2024 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language\nTechnologies (Volume 1: Long Papers) (eds. Duh, K., Gomez, H. &\nBethard, S.) 3947–3960. https://doi.org/10.18653/v1/2024.naacl-\nlong.219. (Association for Computational Linguistics, Mexico City,\nMexico, 2024).\n48. Wu, Y. et al. Retrieve-rewrite-answer: a KG-to-text enhanced LLMs\nframework for knowledge graph question answering. Preprint at\nhttp://arxiv.org/abs/2309.11206 (2023).\n49. Baek, J., Aji, A. F. & Saffari, A. Knowledge-Augmented Language\nModel Prompting for Zero-Shot Knowledge Graph Question\nAnswering. inProceedings of the 1st Workshop on Natural Language\nReasoning and Structured Explanations (NLRSE)(eds. Dalvi Mishra,\nB., Durrett, G., Jansen, P., Neves Ribeiro, D. & Wei, J.) 78–106. https://\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 9\ndoi.org/10.18653/v1/2023.nlrse-1.7. (Association for Computational\nLinguistics, Toronto, Canada, 2023).\n50. Li, Y., Dong, B., Guerin, F. & Lin, C. Compressing Context to Enhance\nInference Efﬁciency of Large Language Models. inProceedings of the\n2023 Conference on Empirical Methods in Natural Language\nProcessing (eds. Bouamor, H., Pino, J. & Bali, K.) 6342–6353. https://\ndoi.org/10.18653/v1/2023.emnlp-main.391 (Association for\nComputational Linguistics, Singapore, 2023).\n51. Mialon, G. et al. Augmented language models: a survey. Preprint at\nhttp://arxiv.org/abs/2302.07842 (2023).\n52. Liu, N. F. et al. Lost in the Middle: How Language Models Use Long\nContexts. Trans. Assoc. Comput. Linguist.12, 157–173 (2024).\n53. Zhao, P. et al. Retrieval-augmented generation for AI-generated\ncontent: a survey. Preprint athttp://arxiv.org/abs/2402.19473 (2024).\n54. Das, R. et al. Case-based Reasoning for Natural Language Queries\nover Knowledge Bases. inProceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing(eds. Moens, M.-\nF., Huang, X., Specia, L. & Yih, S. W.) 9594–9611. https://doi.org/10.\n18653/v1/2021.emnlp-main.755. (Association for Computational\nLinguistics, Online and Punta Cana, Dominican Republic, 2021).\n55. Hu, X., Wu, X., Shu, Y. & Qu, Y. Logical Form Generation via Multi-task\nLearning for Complex Question Answering over Knowledge Bases. in\nProceedings of the 29th International Conference on Computational\nLinguistics (eds. Calzolari, N. et al.) 1687–1696 (International\nCommittee on Computational Linguistics, Gyeongju, Republic of\nKorea, 2022).\n56. Zhang, L. et al. FC-KBQA: Aﬁne-to-coarse composition framework\nfor knowledge base question answering. inProceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers)(eds. Rogers, A., Boyd-Graber, J. & Okazaki,\nN.) 1002–1017 (Association for Computational Linguistics, Toronto,\nCanada, 2023).https://doi.org/10.18653/v1/2023.acl-long.57 (2023).\n57. Jiang, J. et al. StructGPT: A General Framework for Large Language\nModel to Reason over Structured Data. inProceedings of the 2023\nConference on Empirical Methods in Natural Language Processing\n(eds. Bouamor, H., Pino, J. & Bali, K.) 9237–9251 (Association for\nComputational Linguistics, Singapore, 2023).https://doi.org/10.\n18653/v1/2023.emnlp-main.574 (2023).\n58. Xu, Z. et al. Retrieval-Augmented Generation with Knowledge Graphs\nfor Customer Service Question Answering. inProceedings of the 47th\nInternational ACM SIGIR Conference on Research and Development\nin Information Retrieval 2905–2909. https://doi.org/10.1145/\n3626772.3661370. (Association for Computing Machinery, New York,\nNY, USA, 2024).\n59. Guo, T. et al. KnowledgeNavigator: leveraging large language models\nfor enhanced reasoning over knowledge graph.Complex Intell. Syst.\n10, 7063–7076 (2024).\n60. Ji, Z. et al. RHO: reducing hallucination in open-domain dialogues with\nknowledge grounding. inFindings of the Association for\nComputational Linguistics: ACL 2023(eds. Rogers, A., Boyd-Graber,\nJ. & Okazaki, N.) 4504–4522 (Association for Computational\nLinguistics, Toronto, Canada, 2023).https://doi.org/10.18653/v1/\n2023.\nﬁndings-acl.275 (2023).\n61. Al-Moslmi, T., Gallofré Ocaña, M., Opdahl, A. L. & Veres, C. Named\nentity extraction for knowledge graphs: a literature overview.IEEE\nAccess 8, 32862–32881 (2020).\n62. Fleming, S. L. et al. MedAlign: A Clinician-Generated Dataset for\nInstruction Following with Electronic Medical Records.Proc. AAAI\nConf. Artif. Intell.38, 22021–22030 (2024).\n63. Galea, S., Merchant, R. M. & Lurie, N. The mental health consequences of\nCOVID-19 and physical distancing: the need for prevention and early\nintervention.JAMA Intern. Med.180,8 1 7–818 (2020).\n64. Nateghi Haredasht, F. et al. Predictability of buprenorphine-naloxone\ntreatment retention: A multi-site analysis combining electronic health\nrecords and machine learning.Addiction 119, 1792–1802 (2024).\n65. Datta, S. et al. A new paradigm for accelerating clinical data science at\nStanford Medicine. Preprint athttps://doi.org/10.48550/arXiv.2003.\n10534 (2020).\n66. Irvin, J. et al. CheXpert: A Large Chest Radiograph Dataset with\nUncertainty Labels and Expert Comparison.Proc. AAAI Conf. Artif.\nIntell. 33, 590–597 (2019).\n67. Chung, H. W. et al. Scaling Instruction-Finetuned Language Models.\nJ. Mach. Learn. Res.25,1 –53 (2024).\n68. Do ğan, R. I., Leaman, R. & Lu, Z. NCBI disease corpus: a resource for\ndisease name recognition and concept normalization.J. Biomed.\nInform. 47,1 –10 (2014).\n69. Alsentzer, E. et al. Publicly Available Clinical BERT Embeddings. in\nProceedings of the 2nd Clinical Natural Language Processing\nWorkshop (eds. Rumshisky, A., Roberts, K., Bethard, S. & Naumann,\nT.) 72–78. https://doi.org/10.18653/v1/W19-1909. (Association for\nComputational Linguistics, Minneapolis, Minnesota, USA, 2019).\n70. Bodenreider, O. The Uniﬁed Medical Language System (UMLS):\nintegrating biomedical terminology.Nucleic Acids Res32,\nD267–D270 (2004).\n71. Stearns, M. Q., Price, C., Spackman, K. A. & Wang, A. Y. SNOMED\nclinical terms: overview of the development process and project\nstatus. Proc. AMIA Symp. 2001, 662–666 (2001).\n72. Eyre, H. et al. Launching into clinical space with medspaCy: a new\nclinical text processing toolkit in Python.AMIA. Annu. Symp. Proc.\n2021, 438–447 (2022).\n73. Christophe, C. et al. Med42 -- Evaluating Fine-Tuning Strategies for\nMedical LLMs: Full-Parameter vs. Parameter-Efﬁcient Approaches.\nPreprint athttps://doi.org/10.48550/arXiv.2404.14779 (2024).\n74. AI, M.Mixtral of Experts. https://mistral.ai/news/mixtral-of-experts/(2023).\n75. Gratta ﬁori, A. et al. The Llama 3 Herd of Models. Preprint athttps://doi.\norg/10.48550/arXiv.2407.21783 (2024).\n76. Tay, Y. et al.Unifying Language Learning Paradigms. https://arxiv.org/\nabs/2205.05131 (2023).\n77. OpenAI et al.GPT-4 Technical Report. https://arxiv.org/abs/2303.\n08774v6 (2023).\n78. Dong, Q. et al. A survey on in-context learning.https://arxiv.org/abs/\n2301.00234v3 (2022).\n79. Ratner, A. et al. Snorkel: rapid training data creation with weak\nsupervision. Proc. VLDB Endow.11, 269–282 (2017).\n80. OpenAI is an AI research and deployment company.Terms of Use.\nhttps://openai.com/policies/terms-of-use/ (2023).\n81. Xiao, S., Liu, Z., Zhang, P. & Muennighoff, N. C-Pack: packaged\nresources to advance general Chinese embedding. Preprint athttps://\ndoi.org/10.48550/arXiv.2309.07597 (2023).\n82. Wilcoxon, F. Individual comparisons by ranking methods.Biom. Bull.\n1,8 0–83 (1945).\nAcknowledgements\nDr. Chen has received research funding support in part by the NIH/National\nInstitute of Allergy and Infectious Diseases (1R01AI17812101), NIH/National\nInstitute on Drug Abuse Clinical Trials Network (UG1DA015815-CTN-0136),\nGordon and Betty Moore Foundation (Grant #12409), Stanford Artiﬁcial\nIntelligence in Medicine and Imaging— Human-Centered Artiﬁcial\nIntelligence (AIMI-HAI) Partnership Grant, American Heart Association—\nStrategically Focused Research Network— Diversity in Clinical Trials, NIH-\nNCATS-CTSA grant (UL1TR003142) for common research resources. The\ncontent is solely the responsibility of the authors and does not necessarily\nrepresent the ofﬁcial views of the NIH, Stanford Healthcare, or any other\norganization.\nAuthor contributions\nConceptualization: I.L., A.S., K.V., F.N.H. Supervision: J.H.C., N.H.S.\nWriting: I.L., A.S., K.V., S.N. Data acquisition: I.L., F.N.H., J.H.C. Data\nanalysis: I.L. A.S., S.N. Critical review: I.L., A.S., K.V., S.N., F.N.H., S.P.M.,\nA.S.L., S.T., M.M., R.J.G., N.S., J.H.C. All authors read and approved theﬁnal\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 10\nmanuscript and hadﬁnal responsibility for the decision to submit it for\npublication.\nCompeting interests\nA.S. owns stock in Roche (RHHVF) and Cerebral Inc. and is an adviser to\nDaybreak Health and Cerebral Inc. S.N. owns stock in Meta, works at Insitro\n(an ML for drug discovery company), and owns stock options for Insitro.\nN.H.S. reported being a co-founder of Prealize Health (a predictive analytics\ncompany) and Atropos Health (an on-demand evidence generation com-\npany); receiving funding from the Gordon and Betty Moore Foundation for\ndeveloping virtual model deployments; and serving on the Board of the\nCoalition for Healthcare AI (CHAI), a consensus-building organization pro-\nviding guidelines for the responsible use of artiﬁcial intelligence in health-\ncare. J.H.C. reported being a co-founder of Reaction Explorer LLC, develops\nand licenses organic chemistry education software, paid consulting fees\nfrom Sutton Pierce, Younker Hyde MacFarlane, and Sykes McAllister as a\nmedical expert witness, and paid consulting fees from ISHI Health. The\nremaining authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-024-01377-1\n.\nCorrespondenceand requests for materials should be addressed to\nIvan Lopez.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-024-01377-1 Article\nnpj Digital Medicine|            (2025) 8:45 11",
  "topic": "Pipeline (software)",
  "concepts": [
    {
      "name": "Pipeline (software)",
      "score": 0.7319805026054382
    },
    {
      "name": "Computer science",
      "score": 0.707923948764801
    },
    {
      "name": "Inference",
      "score": 0.704677402973175
    },
    {
      "name": "Embedding",
      "score": 0.6728214025497437
    },
    {
      "name": "Security token",
      "score": 0.670012891292572
    },
    {
      "name": "Information retrieval",
      "score": 0.5840103030204773
    },
    {
      "name": "Information extraction",
      "score": 0.5185014009475708
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3403254747390747
    },
    {
      "name": "Natural language processing",
      "score": 0.32243120670318604
    },
    {
      "name": "Computer security",
      "score": 0.07492488622665405
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210137306",
      "name": "Stanford Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210138309",
      "name": "Center for Innovation",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204866599",
      "name": "VA Palo Alto Health Care System",
      "country": "US"
    }
  ]
}