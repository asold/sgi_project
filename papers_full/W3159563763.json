{
    "title": "Multi-agent modeling and simulation in the AI age",
    "url": "https://openalex.org/W3159563763",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2121475782",
            "name": "Wenhui Fan",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2102513436",
            "name": "Peiyu Chen",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A3159385778",
            "name": "Daiming Shi",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2098219144",
            "name": "Xudong Guo",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2107473445",
            "name": "Li Kou",
            "affiliations": [
                "Tsinghua University"
            ]
        },
        {
            "id": "https://openalex.org/A2121475782",
            "name": "Wenhui Fan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102513436",
            "name": "Peiyu Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3159385778",
            "name": "Daiming Shi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098219144",
            "name": "Xudong Guo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107473445",
            "name": "Li Kou",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6763581886",
        "https://openalex.org/W2385924",
        "https://openalex.org/W2996182391",
        "https://openalex.org/W4256290105",
        "https://openalex.org/W2794210286",
        "https://openalex.org/W1506789237",
        "https://openalex.org/W1582756739",
        "https://openalex.org/W2758442112",
        "https://openalex.org/W2982316857",
        "https://openalex.org/W1971883363",
        "https://openalex.org/W2103821666",
        "https://openalex.org/W2391666574",
        "https://openalex.org/W4293640309",
        "https://openalex.org/W2103594294",
        "https://openalex.org/W2169861177",
        "https://openalex.org/W1982800541",
        "https://openalex.org/W2045285586",
        "https://openalex.org/W4251697378",
        "https://openalex.org/W2136883857",
        "https://openalex.org/W2964338167",
        "https://openalex.org/W2155027007",
        "https://openalex.org/W2156194062",
        "https://openalex.org/W4240204763",
        "https://openalex.org/W1606059141",
        "https://openalex.org/W2965768658",
        "https://openalex.org/W3142541040",
        "https://openalex.org/W1517032976",
        "https://openalex.org/W1548163982",
        "https://openalex.org/W2155555292",
        "https://openalex.org/W4233805335",
        "https://openalex.org/W2141042444",
        "https://openalex.org/W3010742245",
        "https://openalex.org/W2025760344",
        "https://openalex.org/W4214717370",
        "https://openalex.org/W1512349031",
        "https://openalex.org/W2085607178",
        "https://openalex.org/W2884888736",
        "https://openalex.org/W2981038142",
        "https://openalex.org/W2114570931",
        "https://openalex.org/W3147731578",
        "https://openalex.org/W1672238326",
        "https://openalex.org/W2601392504",
        "https://openalex.org/W3125993293",
        "https://openalex.org/W2908261578",
        "https://openalex.org/W2617547828",
        "https://openalex.org/W3149462652",
        "https://openalex.org/W2103285838",
        "https://openalex.org/W4298857966",
        "https://openalex.org/W2156134502",
        "https://openalex.org/W2988934653",
        "https://openalex.org/W2963717208",
        "https://openalex.org/W2803155336",
        "https://openalex.org/W2808584387",
        "https://openalex.org/W2227070351",
        "https://openalex.org/W2053719588",
        "https://openalex.org/W2588460386",
        "https://openalex.org/W2121863487",
        "https://openalex.org/W1559964999",
        "https://openalex.org/W1983939224",
        "https://openalex.org/W2019833972",
        "https://openalex.org/W1557577667",
        "https://openalex.org/W2963658727",
        "https://openalex.org/W2028815266",
        "https://openalex.org/W2119832659",
        "https://openalex.org/W2964251366",
        "https://openalex.org/W3089435010",
        "https://openalex.org/W2108113516",
        "https://openalex.org/W4252047811",
        "https://openalex.org/W2093587840",
        "https://openalex.org/W1974697458",
        "https://openalex.org/W2517895319",
        "https://openalex.org/W4283688468",
        "https://openalex.org/W1542941925",
        "https://openalex.org/W2377289382",
        "https://openalex.org/W4299802797",
        "https://openalex.org/W2095880182",
        "https://openalex.org/W2075348693",
        "https://openalex.org/W2179886102",
        "https://openalex.org/W2377008790",
        "https://openalex.org/W2145736592",
        "https://openalex.org/W22404886",
        "https://openalex.org/W2099618002",
        "https://openalex.org/W1539477377",
        "https://openalex.org/W2767617058",
        "https://openalex.org/W1192553058",
        "https://openalex.org/W2059426695",
        "https://openalex.org/W2949464762",
        "https://openalex.org/W3105365024",
        "https://openalex.org/W2161922997",
        "https://openalex.org/W3122275712",
        "https://openalex.org/W2612590324",
        "https://openalex.org/W1877378160",
        "https://openalex.org/W4297789683",
        "https://openalex.org/W2316629923",
        "https://openalex.org/W1583639502",
        "https://openalex.org/W2535584654",
        "https://openalex.org/W3087811647",
        "https://openalex.org/W1570818786",
        "https://openalex.org/W1481361735",
        "https://openalex.org/W136723019",
        "https://openalex.org/W2945479117",
        "https://openalex.org/W1757796397",
        "https://openalex.org/W2147492008",
        "https://openalex.org/W3005773208",
        "https://openalex.org/W3101632300"
    ],
    "abstract": "With the rapid development of artificial intelligence (AI) technology and its successful application in various fields, modeling and simulation technology, especially multi-agent modeling and simulation (MAMS), of complex systems has rapidly advanced. In this study, we first describe the concept, technical advantages, research steps, and research status of MAMS. Then we review the development status of the hybrid modeling and simulation combining multi-agent and system dynamics, the modeling and simulation of multi-agent reinforcement learning, and the modeling and simulation of large-scale multi-agent. Lastly, we introduce existing MAMS platforms and their comparative studies. This work summarizes the current research situation of MAMS, thus helping scholars understand the systematic technology development of MAMS in the AI era. It also paves the way for further research on MAMS technology.",
    "full_text": "TSINGHUA SCIENCE AND TECHNOLOGY\nISSN ll1007-0214 04/15 pp608–624\nDOI: 1 0 . 2 6 5 9 9 / T S T . 2 0 2 1 . 9 0 1 0 0 0 5\nVolume 26, Number 5, October 2021\n\rC The author(s) 2021. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\nMulti-Agent Modeling and Simulation in the AI Age\nWenhui Fan\u0003, Peiyu Chen\u0003, Daiming Shi, Xudong Guo, and Li Kou\nAbstract: With the rapid development of artiﬁcial intelligence (AI) technology and its successful application in various\nﬁelds, modeling and simulation technology, especially multi-agent modeling and simulation (MAMS), of complex\nsystems has rapidly advanced. In this study, we ﬁrst describe the concept, technical advantages, research steps,\nand research status of MAMS. Then we review the development status of the hybrid modeling and simulation\ncombining multi-agent and system dynamics, the modeling and simulation of multi-agent reinforcement learning,\nand the modeling and simulation of large-scale multi-agent. Lastly, we introduce existing MAMS platforms and\ntheir comparative studies. This work summarizes the current research situation of MAMS, thus helping scholars\nunderstand the systematic technology development of MAMS in the AI era. It also paves the way for further research\non MAMS technology.\nKey words: artiﬁcial intelligence; system dynamics; reinforcement learning; large-scale multi-agent\n1 Introduction\nAgents emerged in distributed artiﬁcial intelligence\nin the 1970s. Since the 1990s, agents have become\nan essential frontier in computer research, especially\nin the ﬁeld of artiﬁcial intelligence (AI). At the\nsame time, research on agent-based modeling and\nsimulation (ABMS) has been conducted in various ﬁelds\n(such as social science, economic system, biological\nscience, ecological science, engineering technology, and\nsimulation science), and the related inﬂuential research\nand application are fruitful.\nAgents have various deﬁnitions in different ﬁelds and\ndisciplines. Agents, which were initially used only as\na system component, have been extended to become\nintelligent software, devices, robots, computers, or even\nhuman beings. An agent is an autonomous individual\n\u000f Wenhui Fan, Peiyu Chen, Daiming Shi, Xudong Guo,\nand Li Kou are with Department of Automation, Tsinghua\nUniversity, Beijing 100084, China. E-mail: fanwenhui@\ntsinghua.edu.cn; cpy19@mails.tsinghua.edu.cn; shidm18@\nmails.tsinghua.edu.cn; gxd20@mails.tsinghua.edu.cn; koul13@\nmails.tsinghua.edu.cn.\n\u0003 To whom correspondence should be addressed.\nManuscript received: 2021-01-07; accepted: 2021-01-19\nthat can perceive the environment it is in, calculate and\nreason in accordance with the information obtained,\ncommunicate with other individuals, coordinate with\neach other, and cooperate with each other to complete\na speciﬁc task and exert inﬂuence on the external\nenvironment. In this process, each agent not only has its\nown behavioral characteristics, such as autonomy, social\nability, responsiveness, and initiative, but also has mental\nstate characteristics, such as goals, knowledge, beliefs,\nresponsibilities, and commitments according to their\ndifferent roles and functions[1]. Agents also have other\nimportant characteristics, such as mobility, adaptability,\nand reasoning ability[2].\nAlthough agents have various characteristics and\npossess the ability of decision-making and execution, the\nability of a single agent remains limited. Such limitation\nmotivates the emergence of multi-agent systems (MASs).\nCompared with single-agent systems, MASs not only\nhave stronger performance but also help multiple\nagents solve certain problems independently through\nthe information resources of the system. In addition,\nagents can cope with increasingly complex problems\nthrough mutual assistance\n[3], which encourages the\nemergence and development of multi-agent modeling\nand simulation (MAMS) technology.\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 609\n2 Multi-Agent Modeling and Simulation\n2.1 Concepts and ideas\nGenerally, traditional system modeling and simulation\nmethods establish system models using the deductive\nreasoning method before conducting simulation\nexperiments and analyses. This approach is a typical\nengineering simulation modeling method, a top-down\nmodeling method, as well as an idea of reductionism.\nLocal and foreign researchers have indicated that\nthe existing traditional modeling methods based on\nreductionism cannot appropriately describe complex\nsystems. In the modeling and simulation of complex\nsystems, inductive reasoning is commonly used to build\nthe formal abstract models of the systems, that is, using\nabstract representations of the system to gain an insight\ninto the objective world and the natural phenomena;\nsuch approach is based on system theory and is a\nbottom-up modeling method. MAMS, which builds\nthe model by using the basic elements of a complex\nsystem and their interactive relationship, organically\nlinks the microscopic behavior and the “emergent”\nphenomenon in the macroscope of complex systems.\nMAMS, a method of ontology, is one of the most\neffective simulation methods to solve the problem of\ncomplex systems.\nMulti-agent simulation mainly consists of two\naspects. On the macro aspect, the mechanism,\nprotocol, and strategy of communication, coordination,\nand collaboration between agents, as well as the\ndecomposition and assignment of tasks, are included. On\nthe micro level, the dynamics, reasoning, and behavior of\nthe agents are studied. The prior task of complex system\nsimulation is to establish the system model and conduct\nexperiments to study the emergence of the system, that\nis, studying when, where, and what kind of emergence\nthe studied object appears. The concepts and ideas of\nMAMS mainly include the following six aspects[4]:\n(1) Agents. Agents are autonomous computing\nentities and basic modeling and simulation units, which\ncan sense the environment through sensors (physical or\nsoftware) and act on the environment through effectors.\nA computing entity is a program that physically exists\nand operates on a computing device. Autonomy means\nthat it can control its own behavior to a certain extent\nand can take certain actions without the interventions of\nhuman beings or other systems. To meet the design goals\nof a system, agents pursue the corresponding subgoals\nand perform the corresponding tasks. In addition, these\nsubgoals and tasks can be complementary or conﬂicting.\n(2) Model aggregation. Agents are a nested\nhierarchical concept. Basic agents, called meta-agents,\nhave all the attributes of an agent. Model aggregation\nis needed to solve problems in complex large-scale\nsystems; in some cases, hierarchical modeling that\naggregates multi-granular, multi-scale, and multilevel\nmodels is also needed.\n(3) Perception and action. Agents perceive\ninformation of the surrounding environment and other\nagents through internal perceptions and then react to the\nenvironment and other agents with the help of effectors,\nthereby realizing the functional behavior modeling of\nindividual agents.\n(4) Decentralized control. Each agent is relatively\nindependent in MAMS, where no central control\nand coordination agents are needed for usual cases.\nMoreover, multi-agent control has been applied in\nvarious practical applications, such as robotics.\n(5) External environment. In MAMS, the external\nenvironment not only provides several conditions for the\nexistence of agents, but also acts as the perception and\naction object of agents.\n(6) Interactions and associations. The interaction\nand association between agents are the cause of complex\nbehavior in complex systems, as well as an important\nway to solve the dependence, conﬂict, and competition\namong agents.\n2.2 Technological advantage\nCompared with other complex system modeling and\nsimulation technologies, MAMS has the following\nadvantages[5]:\n(1) MAMS can describe complex systems\nnaturally. In many cases, MAMS is the most\nnatural description of a complex system composed of\nnumerous entities, and the concept of individuals in the\nsystem is consistent with agents.\n(2) MAMS can capture the emergence\nphenomenon in complex systems. The “whole is\ngreater than the sum of its parts” feature of emergence\nmakes it difﬁcult to predict. MAMS is a normative\nmethod for modeling this emergence phenomenon. By\nmodeling the behavior of the micro individuals of the\nsystem, the purpose of describing the macro behavior of\nthe system is achieved.\n(3) MAMS has ﬂexible organizational framework\nand evolution mechanism. Agent-based paradigms\nhave a ﬂexible potential computing mechanism for the\n610 Tsinghua Science and T echnology, October2021, 26(5): 608–624\nformation, maintenance, evolution, and disintegration\nof organizations and are especially suitable for solving\nthe problem of ﬂexible organization and scheduling in\ndifferent complex systems.\n(4) Agent autonomic solving and decision-making\nability. The autonomic solution ability is the capacity\nof accepting the “stimulus” of the environment and\nother agents, reacting to the environment and other\nagents in accordance with the internal state and stimulus\ninformation, and modifying its own rules and states.\n(5) Agent’s ability to interact with users. Direct\nmanipulation interfaces are used in traditional complex\nmodeling systems; however, with the increase in\ntask complexity, the manipulation process becomes\nincreasingly complicated and thus can eventually affect\nthe stability of the system. Nevertheless, the intelligence\nmechanism of agents creates technical conditions for\nhuman-computer interaction and cooperation, thus easily\nforming a harmoniously coexisting problem-solving\nsystem where the interaction and collaboration between\nusers and agents are realized. In addition, the ability also\nguarantees system stability; thus, a single module error\ndoes not easily lead to system collapse.\n(6) Suitable for distributed simulation. Agents\ndescribe the individual entities of a system and can\nbe conveniently distributed to multiple computer nodes,\nthus providing the possibility for a distributed simulation\nof large-scale complex systems.\n(7) Reusability of the model. Agents are abstraction\nentities with greater granularity than the object. By\nreusing a mature agent model, the efﬁciency of software\ndevelopment can be improved.\n2.3 Research procedure\nThe MAMS procedure is mainly divided into the\nfollowing ﬁve steps [6]:\n(1) Speciﬁcation of the aims of simulation. The\ncomplexity characteristics of the target system and\nthe requirements of simulation are analyzed. First, the\ncomplexity of the target system is analyzed, and the\nobjectives, requirements, and system boundaries of the\nsimulation are speciﬁed. Then the speciﬁc characteristics\nof the entities in the system are analyzed, and the\nformal expression of the system is determined. Finally,\nthe evaluation mechanism and method are deﬁned, the\ndata expression mode is determined, and the supporting\nfunctions of the simulation environment are summarized\nand formulated.\n(2) Reasonable selection of abstract level.\nHierarchy structures are the inherent structures of\ncomplex systems. Therefore, a multilevel abstract\nmodeling method must be used to establish the abstract\nmodel of various object entities in a complex system,\nthus ensuring that the abstract level is reasonable and\nsufﬁcient.\n(3) Message ﬂow analysis. The procedure includes\nclassifying the message types, ensuring the ﬂow patterns\nof various messages, and determining whether the\nsimulation target needs further decomposition.\n(4) Agent modeling. The procedure includes building\nanalysis trees for complex systems through hierarchical\ndecomposition and message ﬂow analysis, and building a\nmeta-agent model for each leaf node and an aggregation-\nagent model for each non-leaf node, and abstracting out\ndifferent agents in accordance with the entities’ different\nfunctions.\n(5) Distribution of agents.The distribution of agents\ndepends on the speciﬁc application requirements of\nthe complex system simulation, the adopted simulation\nalgorithms, and the hardware environment of the\nsimulation.\n2.4 Research on application status\nMAMS has been applied in many ﬁelds, including\nsocial sciences, economics, artiﬁcial life, geographical\nand ecological processes, as well as the industrial and\nmilitary ﬁelds. However, most studies are still in\ntheir infancy, that is, these studies are still\n\u001cthought\nexperiments\u001din the laboratory and have the nature of\nacademic research. In reality, simulation analysis and\ncontrol of complex systems still have a long way to go.\nNevertheless, research on the ABMS of complex systems\nhave been applied in practice.\n2.4.1 Social ﬁeld\nThe social sciences are one of the ﬁelds where MAMS\nis most widely applied. Its research focuses on the\nemerging behavior and self-organization of human\nsystems; MAMS, which has been widely recognized\nby many social scientists, is the most suitable method\nto capture these phenomena\n[7]. “Humans” in social\nsystems and agents in MAMS are essentially similar.\n“Humans” are abstracted as agents with autonomous\ndecision-making, learning, memory, coordination, and\norganization abilities. Therefore, neural networks,\nevolutionary computation, or other learning skills\nare required for agents to describe the learning and\nadaptive ability of “humans”. For example, neural\nnetworks were used to detect network’s abnormal\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 611\nbehavior[8] and isolation forest was applied to fault\ndiagnosis[9]. Research applications of MAMS in\nthe social ﬁeld include ﬂow [10], such as trafﬁc [11],\nevacuation in emergency circumstances, customer ﬂow\nmanagement, organization formation, and political\ninteraction[12]. Casti from the Santa Fe Institute\nsimulated the trafﬁc and environmental conditions\nin Albuquerque [13], whereas Raney et al. studied\nthe trafﬁc problems in Switzerland [14]. In addition,\nEpstein and Axtell developed an agent model\nbased simulation software called ResortScape, which\ncan be used for parking lot management and\ndecision-making[15]. Bilge developed an agent model\nbased software called SIMSTORE for supermarket\nmanagement and monitoring; the software was\npractically applied to the operation and management\nof several supermarkets in the UK[16].\n2.4.2 Economic ﬁeld\nEconomics is a ﬁeld in which MAMS is widely applied.\nResearchers in Sandia National Laboratories in the\nUnited States developed an agent-based US economic\nsimulation model called Aspen\n[17], which is a blend\nof the latest technology of evolution learning and\nparallel computing in the Sandia lab; this model is\nsuperior in many aspects compared with the traditional\neconomic model. The variation of the inﬂuence of\nlaws, rules, and policies is considered in a single,\nconsistent economy simulation computing environment\n(e.g., building detailed models for monetary policy,\ntax law, and trade policy research; analyzing different\neconomic sectors separately or together with other\ndepartments, thus facilitating the understanding of the\nentire economic process; accurately simulating the\nbehavior of the basic decision-making sectors of the\neconomy, such as residents, banks, companies, and\npolicies). Aspen analyzed the effect of policies on\nmicro and macro units by regarding micro units, such\nas individuals, residents, and enterprises, as simulated\nobjects. Through the statistics, analysis, inference, and\nsynthesis of characteristic variables, the inﬂuence of\npolicy changes on micro individuals and the effects of\npolicy implementation on macro and various levels can\nbe observed. Sandia National Laboratory has already\nestablished a prototype model of the simple market\neconomy (a simple simulation of the US economy) and\na transition economy simulation model (a transition\neconomy simulation), which is a detailed model[18].\nIn addition, the virtual stock market developed by\na Bios team led by Arthur of the Santa Fe Institute\nhas been successfully applied to the NASDAQ stock\nmarket simulation\n[19, 20]. The agent-based NASDAQ\nsimulation model successfully combined the agent-based\nmodeling idea with AI technologies, such as neural\nnetworks and reinforcement learning. This simulation\nmethod has further been used to study the US housing\nmarket\n[21]. Agents in the stock market interact by\nadopting different strategies from simple to complex.\nThrough the interaction between agents, the dynamic\nbehavior of the entire stock market can be expressed.\n2.4.3 Military ﬁeld\nThe military ﬁeld is a new area of application for MAMS.\nMilitary confrontation and land warfare systems are\ncomplex adaptive systems (CASs) [22–25] , which have\nbeen recognized by researchers. Therefore, MAMS can\nbe used to study battleﬁeld behavior, such as military\nconfrontation[26]. The results of existing research show\nthat MAMS, which has strong vitality, is more effective\nthan the current combat model based on the Lanchester\nequation and is a good method for battleﬁeld simulation.\nThe US Department of Defense (DOD) hopes to have\nreal-time, omnidirectional access to information in future\nbattleﬁelds. For C4ISR to be useful, advanced real-\ntime distributed modeling and simulation tools must be\nused, and complexity science can help develop C4ISR.\nAs the MAMS is a complexity science methodology,\nit naturally became the DOD’s advanced modeling\nand simulation methodology. The DOD’s applications\non ABMS include the irreducible semi-autonomous\nadaptive combat (ISAAC) developed by the US Marine\nCorps Combat Development Command (MCCDC), the\nenhanced ISAAC neural simulation toolkit (EINSTein)\nand SWarrior, the adaptive collection management\nenvironment (ACME) developed by the US Army\nIntelligence and Security Command (INSCOM), and the\ntactical sensor and ubiquitous network agent modeling\ninitiative (TSUNAMI) co-developed by the Naval\nWarfare Development Command and Argonne National\nLaboratory’s Center for Complex Adaptive Systems\nSimulation.\nISAAC[22, 23] was developed on the basis of the agent\nmodel. Through the simulation of war, questions, such\nas “to what extent do land warfare systems have the\ncharacteristic of self-organizing CAS”, can be answered.\nThe software was not designed to construct a system-\nlevel battleﬁeld model but to serve as a simulation kit\nto explore interactions from different low-level rules\n612 Tsinghua Science and T echnology, October2021, 26(5): 608–624\nto high-level emergent behavior (for example, from\nindividual warriors to a squad). ISAAC’s long-term\ngoal is to turn its subsequent product into a toolkit\nthrough which the emergent aggregation behavior on the\nbattleﬁeld can be explored. The agent in ISAAC has four\ncharacteristics of rule, task, situational awareness, and\nself-adaptability. Through the interaction of simple rules,\nthe ISAAC system presents operational concepts, such\nas forward advance, frontline attack, local clustering,\npenetration, retreat, attack attitude, containment and\ncontainment, encircle maneuver, and guerrilla attack.\nEINSTein[22, 23] is an enhanced version of ISAAC. The\nmain improvements include a Windows-style Console\nUser Interface (CUI) interface, an object-oriented\nC++ code, context-dependent and user-deﬁned agent\nbehavior, personalized script representation, online\ngenetic algorithms, neural networks, reinforcement\nlearning and pattern recognition toolkits, online data\ncollection and multidimensional visualization toolkits,\nan online analysis toolbox, and a ﬁtness coevolution\nlegend display. At present, EINSTein studies two basic\nproblems: command and control of topology and battle-\nrelated information.\nMCCDC also developed SWarrior[24], which is based\non Swarm, by combining certain characteristics of\nISAAC. SWarrior aims to transform Swarm into a new\nanalytical tool to gain an insight into future military\nconfrontation based on agent-based simulation.\nINSCOM and the Bios team of Santa Fe developed\nACME[25], which helps commanders manage and\nacquire real-time battleﬁeld information and obtain the\nenemy command post locations with the constantly\nchanging battleﬁeld maps.\nAn agent-based model was built by TSUNAMI [25]\nfor the red, blue, and neutral forces, which have\ncomplex behavior and different properties, such as\ncommunication equipment, perception ability, mobility,\nmemory ability, and fuel and battery energy. TSUNAMI\nsimulates the battleﬁeld space motion and interaction\nby describing the real terrain, and it can “clone” various\nsensors and use rule sets to simulate message ﬂow and\nservice protocol quality.\nIn addition, the Australian Defense Force Academy\ndeveloped the reducible agent battleﬁeld behaviour\nthrough life emulation (RABBLE) [27]. In contrast to\nISAAC, RABBLE uses an MAS structure, adding up\nthe learning mechanism and thus making the simulated\ngroup behavior conducive to decision-making. SW ARM\nand Battle Model[26] developed by the Air Operations\nDivision in Australia builds an agent-based model for\npilots, ﬁght managers, sensor managers, air combat\ndefense commanders, and ground crew in air combat.\nIn the military ﬁeld, Heinze investigated the military\ncombat concept by using the agent-based model\n[28],\nand Hill designed and implemented Tactical Simulation,\nan intelligent agent software model for air combat\n(especially over-the-horizon combat) simulation[29].\n3 Hybrid Modeling and Simulation Based\non Multi-Agent and System Dynamics\nHybrid modeling and simulation have received\nconsiderable attention over the past few years due to their\nﬂexibility and improved support tools. The construction\nof hybrid modeling and simulation aims to describe the\ncomplex behavior of the system[30]. A hybrid simulation\nmethod should comprehensively consider all types of\ninteractions in the hybrid model to simulate various\nbehaviors in the complex system. MAMS and system\ndynamics modeling and simulation (SDMS) are two\nimportant simulation methods[31].\nSDMS is a top-down feedback method proposed by\nForrester[32]. The essence of this method is a high-order,\nmulti-loop, and nonlinear feedback structure. SDMS is\na method for visualizing, analyzing, and understanding\ncomplex dynamic feedback[33]. The advantage of SDMS\nlies in its ability to consider nonlinear feedback and the\ntime-delay characteristics of the dynamics [34]. SDMS\nhas been widely used to solve various problems in social,\nindustrial, environmental, and project management\nsystems[35]. It uses feedback loops, stocks, and streams\nto simulate the dynamics of complex systems over\ntime. In SDMS, the behavior of the system consists\nof two basic types of feedback loops, including a\nnegative feedback loop for system regulation and a\npositive feedback loop with the function of strengthening\ninput[36].\nMAMS is a computer simulation method describing\nthe behavior of complex systems. It adopts the bottom-\nup information feedback method to describe the\ninteraction between individuals, identiﬁes each entity\nas heterogeneous rather than identical, and allows\nindividuals to evolve and adapt dynamically\n[37]. In\nMAMS, the complexity of the system is reﬂected in the\ninteraction between different agents[32, 38]. The purpose\nof simulation is to track the interactions between agents\nin artiﬁcial environments and understand the emergence\nof global patterns [39]. Agents in MAMS have certain\nproperties and interact by deﬁning appropriate rules in\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 613\na given environment[40]. Agents act or produce output\non the basis of their interactions, environments, and the\nrules they follow.\nThe hybrid model combining MAMS and SDMS\nemerged in the late 1990s. Scholl[41] conducted the ﬁrst\nstudies on the applicable scope of SDMS and MAMS.\nIn this study, the author also studied the advantages\nand disadvantages of each method and summarized the\npossibility of multimethod modeling based on SDMS\nand MAMS[41]. Pourdehnad et al.[42] deepened the above\nwork by conceptually comparing these two methods.\nThe authors discussed the potential synergies between\nthe two paradigms to solve problems in the teaching\nand decision-making processes[42]. Similarly, Stemate et\nal.[43] compared these modeling approaches and listed a\nrange of possible cross-applications, which provided a\nreliable basis for researchers to conduct further research.\nLorenz and Jost [25] believed that the combination of\nSDMS and MAMS makes the simulation model closer\nto reality.\nSchieritz et al. [44, 45] mainly worked on comparing\nSDMS and MAMS in the ﬁeld of operations research.\nThey identiﬁed the unique features of each approach and\npresented a table of major differences. In Ref. [ 46], a\nmethod that combines SDMS and MAMS was proposed\nto solve supply chain management problems. The results\nsuggested that the use of combined methods does not\nproduce the same results as using SDMS alone. To\nexplore the reasons for these differences, the authors\npointed out that further research is needed[46].\nRamandad and Sterman [22] compared the results of\nsimulating the dynamic process of infectious disease\ntransmission using MAMS and SDMS, transformed\nMAMS into SDMS, and examined the effects\nof individual heterogeneity and different network\ntopologies. They concluded that SDMS produces a\nsingle trajectory for each parameter set, whereas random\nMAMS produces a distribution of the results.\nScholl[41] provided an overview of the general\nmodeling principles of SDMS and MAMS, described\ntheir areas of applicability, discussed their relative\nstrengths and weaknesses, and attempted to identify\nareas where the two modeling and simulation methods\ncomplement each other and overlap. MAMS is inductive\nbecause the resulting emergent behavior of such agents\nis the basic unit of analysis. By contrast, SDMS\nis deductive because it is described by its feedback\nstructure at an aggregate level. Both techniques aim to\ndiscover leverage points in complex aggregate systems;\nmodelers of agent-based models seek them in rules and\nagents, whereas SDMS does so in the feedback structure\nof a system.\nSimilarly, Lorenz [24] proposed three aspects to\nconsider when choosing between SDMS and MAMS:\nstructure, behavior, and appearance. Structure is related\nto how the model is built. The structure of the SDMS\nmodel is static, whereas that of MAMS is dynamic. In\nSDMS, all elements of emulation are preset, whereas\nin MAMS, agents can be created or destroyed, and\ninteraction rules are deﬁned during simulation. Another\naspect is the difference in the behavior center generators\nof the model. For SDMS, the behavior generators are\nfeedback and accumulation, whereas for MAMS, the\ngenerators are the interactions between system elements.\nBoth methods involve feedback. However, MAMS has\nfeedback at multiple modeling levels. In addition, the\nability of emergence in the simulation varies in the\ntwo methods. In this study, the author pointed out that\nMAMS can simulate emergence, whereas the single-\nlayer structure of SDMS cannot[24].\nDespite the increasing amount of research on hybrid\nsimulation methods, research on the properties and types\nof hybrid models is limited\n[27], and the hybrid simulation\nframework should be able to consider all types of\ninteractions within the hybrid model [30]. To address\nthis problem, Swinerd and McNaught [27] proposed\nthree hybrid SDMS-MAMS simulation frameworks of\nintegration, interface, and sequence.\nIn recent years, the comparative research results\nof SDMS and MAMS show that SDMS and MAMS\nhave their advantages and disadvantages in complex\nsystem simulation. SDMS focuses on the dynamic\nbehavior of the system and the analysis of the interaction\nand accumulation effect between different elements\nwhile ignoring the spatial factors. MAMS focuses\non the interaction in space and ignores the feedback\neffect of macro-socioeconomic factors on agents[26]. To\ncapture the heterogeneity and homogeneity of complex\nsocioeconomic system models in dynamic simulation\nenvironments, two types of hybrid models combining\nMAMS and SDMS were proposed. In the ﬁrst type,\nMAMS is used to create aggregate constructions, and\nSDMS is used to aggregate constructions and thus\ngenerate dynamic behavior. In the second type, SDMS is\nused to deduce the characteristics (states) of agents at the\nmicro level, whereas MAMS deals with the interaction\nprocess of these agents under different rules[47]. In the\npast two decades, an increasing number of scholars\n614 Tsinghua Science and T echnology, October2021, 26(5): 608–624\nhave begun to explore the combination of SDMS and\nMAMS[22, 46]. The hybrid approach has already been\napplied to solve problems in different ﬁelds, such\nas transportation[48], health-related research[24, 25, 43, 48],\npsychology[49], work environment [50], and ecological\nmodeling[51–54] .\n4 Multi-Agent Reinforcement Learning\nModeling and Simulation\nReinforcement learning is a mode of machine learning\nthat can actively perceive the environment through\ndifferent behaviors or actions, evaluate behavior\nsimultaneously, and apply the evaluation to adjust\nthe follow-up behavior; that is, it is a learning\ntechnology that can map different environmental states\nto behaviors[55]. The main purpose of reinforcement\nlearning is to select the optimal behavior of an agent to\ncomplete the target. It is widely used in robot control\nsystems[56, 57], intelligent decision[58], nonlinear optimal\ncontrol[59–61] , and other ﬁelds [62]. Complex practical\nproblems cannot be reﬂected due to the lack of decision-\nmaking and environmental awareness ability of a single\nagent; thus, the concept of multi-agent, which is a\ncollection of multiple intelligence called an MAS, was\nproposed in the late 20th century. MAS, a frontier\ndomain of distributed AI, is mainly used to study the\ncoordination[63], communication, and conﬂict between\ngroups of agents[64].\n4.1 Single-agent reinforcement learning\nStudies on reinforcement learning mainly began in the\n1850s[55], and the idea of trial-and-error learning was\nestablished soon after. The representative of modern\nreinforcement learning, i.e., temporal difference learning,\nbegan to be applied to the Markov decision process, and\nits efﬁciency was much higher than that of the traditional\nreinforcement learning algorithm[65]. Subsequently, the\nresearchers integrated the time-series difference method\nand other optimization methods and proposed the Q-\nlearning algorithm\n[66], which is acknowledged as the\nbasis of most of the current deep reinforcement learning\nalgorithms.\nAfter the proposal of deep learning, traditional\nreinforcement learning actively explored the possibility\nof combining with deep learning. In the beginning,\nthe combination of deep learning and reinforcement\nlearning methods is mostly value-based, that is, it\nuses a deep neural network approximation function or\nan action-value function to extend the problems that\nreinforcement learning can solve from limited state\nspace to continuous state space. The value-based deep\nreinforcement learning algorithm can ﬁt the continuous\nstate space; however, the value-based method cannot\nsolve the problem of continuous action space. Hence, the\nstochastic strategy gradient method and the deterministic\nstrategy gradient algorithm, which are based on the\nactor-critic framework policy gradient algorithm, were\ngradually developed[67].\n4.2 Multi-agent reinforcement learning\nIn 2000, machine learning researchers took MAS as an\nimportant application background of AI[68] and proposed\nthe concept of multi-agent learning in 2006 [69]. In\n2013, the deep Q-network combined reinforcement\nlearning with deep learning for the ﬁrst time [70] and\nthus remarkably improved the performance and stability\nof reinforcement learning algorithms, attracted several\nresearchers to take reinforcement learning as the learning\nmethod of MAS, and initiated the study of the multi-\nagent reinforcement learning[71, 72]. In the game theory\nﬁeld, multi-agent reinforcement learning is classiﬁed\ninto complete cooperation, perfect competition, and\nmixed type in accordance with the task types to be\nsolved\n[73]; it can also be classiﬁed into four types\naccording to different research contents and methods:\nanalysis of emergence behavior, learning how to\ncommunicate, cooperate, and model the adversary[74].\nThe ﬁrst is the analysis of emergence behavior. This\npart of research mainly applies the existing single-\nagent deep reinforcement learning algorithm to MAS,\nanalyzes the performance and behavior of multi-agent,\nand observes the evolution of multi-agent. For example,\nthe competitive behavior of deep reinforcement learning\nagents in table tennis tasks was studied[75]. In Ref. [76],\nthe performance and property of typical reinforcement\nlearning algorithms in counterattack tasks were explored.\nThe second is to learn how to communicate. In this\npart of the study, the communication between agents can\nhelp agents in completing tasks. This method mainly\nlearns when and how agents communicate with each\nother. For example, Facebook proposed a CommNet\nnetwork structure that aggregates the communication\nbetween agents through a summation operation\n[77]. In\nRef. [ 78], the communication relationship and\ncommunication information between agents were\nintelligently selected on the basis of the attention\nmechanism.\nThe third part is to learn how to cooperate. This part\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 615\nignores how agents communicate with each other and\nfocuses on the performance of their cooperation. Multi-\nagent deep deterministic policy gradient, an extension\nalgorithm based on deep deterministic policy gradient,\nproposed a learning method of centralized training-\ndecentralized execution, which has become the paradigm\nfor most cooperative methods [79]. By contrast, the\nresearchers proposed a value decomposition network,\nwhich decomposes the global value function into the\naccumulation of the multi-agent’s local value functions\nand further simpliﬁes the center’s evaluation network [80].\nThe fourth part, which is how to model the adversary,\nmodels the adversary’s strategy. The modeling of other\nagents has applications and beneﬁts in cooperative and\ncompetitive tasks and can also help explain the behavior\nof agents[81]. For example, performance can be improved\nin the hiding information task by modeling others[82] and\nin the study of the overﬁtting problem in the strategy\nmodeling of other agents[83].\n4.3 Status and outlook\nWith the popularity of robots, manufacturing, logistics,\ndisaster relief, unmanned vehicles, and other ﬁelds have\nbecome the typical application scenarios of MASs. With\neconomic development, MASs with high efﬁciency and\nintelligence are urged. As the frontier of solving MAS\nproblems in the ﬁeld of AI, multi-agent reinforcement\nlearning provides a feasible method for developing\nintelligent algorithms in different environments and\ntasks. However, the current multi-agent reinforcement\nlearning algorithm still suffers from the following\nproblems:\n(1) The convergence and stability of multi-agent\nreinforcement learning have yet to be proven\nsystematically.\nReinforcement learning optimizes\nstrategies by the agents’ self-exploration, which requires\nagents to balance random exploration and autonomous\ndecision-making. The convergence and stability of\nvarious single-agent reinforcement learning algorithms\nhave been widely studied and demonstrated; however,\nthe convergence of multi-agent reinforcement learning\nalgorithms depends not only on the environment but\nalso on the performance of other agents, thus making\nit difﬁcult to prove the convergence of the optimal\nstrategy and the stability after convergence of multi-\nagent reinforcement learning[73].\n(2) The state space of multi-agent reinforcement\nlearning is huge, and the training time is long. To\nensure the convergence of the strategy optimization\nprocess, the single-agent reinforcement learning\nalgorithm needs to explore massive possibilities of the\n“state-action” space. The joint “state-action” space of a\nmulti-agent increases exponentially with the number of\nagents in the system, leading to an exponential increase\nin training time, limiting the extensibility of the number\nof agents in multi-agent reinforcement learning.\n(3) The expansibility and knowledge transfer\nability of multi-agent reinforcement learning\nare poor.\nOn one hand, the current multi-agent\nreinforcement learning can only be designed for speciﬁc\ntasks and has trouble expanding and transferring\nstrategies for different tasks so that each task needs to\nbe trained and learned again. On the other hand, each\nretraining requires training for multiple agents. How to\nuse experience between agents[84] and how to store and\ntransfer knowledge[85] are still hot topics in this ﬁeld.\nAlthough the theory and practice of multi-agent\nreinforcement learning still face great challenges, multi-\nagent reinforcement learning is still regarded as an\nessential method to produce collective intelligence\n[86].\nThis study argues that the contributions of multi-agent\nreinforcement learning have two aspects. First, multi-\nagent reinforcement learning can bring robot teams with\ncooperative consciousness and human-like intelligence.\nMulti-agent reinforcement learning can train AI to\ncomplete complex multi-agent tasks of Dota2\n[87] and\nStarCraft II[88], and its performance can reach the level\nof e-sports athletes. Second, research on the training\nprocess of multi-agent reinforcement learning can help\nexplain the emergence and evolution of biological group\nbehavior and provide quantitative model support for\nthe development of anthropology and sociology. The\nmulti-agent reinforcement learning method combines\nthe actual characteristics of distributed decision between\nbiological population and robot population and the\nadvantages of independent learning optimization strategy\nin AI. Moreover, it is becoming an interdisciplinary\nscience of multi-agent simulation, swarm game theory,\nreinforcement learning, and other ﬁelds.\n5 Large-Scale MAMS\nIn many disciplines, such as physics, social sciences,\nelectronic communication, ecology, and military\nresearch, the number of agents involved in MAMS is\nalways large, and the millionth magnitude of scale makes\nit difﬁcult for ordinary computers to provide enough\ncomputing power\n[89]. Similar computing problems\n616 Tsinghua Science and T echnology, October2021, 26(5): 608–624\noccur when the agent-based simulation algorithm\nis complex [90]. These problems can be collectively\nregarded as large-scale MAMS problems. Numerous\napproaches have emerged in various ﬁelds trying to\nsolve this problem. Among them are two most effective\nand common solutions: One is to restructure the model\nat the software level by super individual or other\nmethods; another is to speed up large-scale computing\nby distributed parallel computing\n[91, 92] or using new\ncomputation tools, such as the Quantum tool[93].\nNumerous software reorganizations have been\nsuccessful. For example, Blythe et al. successfully\nsimulated approximately 3 million agents and generated\na total of 30 million operations by using stationary\nprobability models, embedding link prediction, and\nintroducing Bayesian models, thus enabling large-\nscale multi-agent simulation to model the evolution\nof GitHub (a large collaborative software development\necosystem)[94]. Campagne et al. proposed the use\nof morphology to represent and control the state of\nlarge organizations composed of large-scale agents that\nrepresented the state of the system as the shape in\nabstract geometric space\n[95]. In the era of AI, neural\nnetworks can also be used in the abstraction and\nsimpliﬁcation of models. Zhou et al. integrated the\nemerging mean ﬁeld game theory with reinforcement\nlearning technology on the basis of self-organizing\nneural networks, which effectively break the “curse of\ndimension” of large-scale MAMS and greatly reduce\ncomputational complexity[96].\nProgress has been achieved in distributed parallel\ncomputing in recent years. Fachada et al. compared\ndifferent parallel computing strategies, such as\nequal, equal with repeatability, equal with row\nsynchronization, and on-demand, and pointed out\nthat different parallelization strategies have speciﬁc\ntrade-offs in terms of performance and simulation\nrepeatability\n[97]. Predator-prey for high-performance\ncomputing, an effective reference model that can\ncompare different parallelization strategies from\nperformance and statistical accuracy, was also\nproposed\n[98]. Many scholars have begun to explore\nthe use of GPU and other hardwares. For example,\nthe GPU-based mobility simulator GEMSim designed\nby Saprykin et al. accelerates the process of large-\nscale multi-agent simulation, and its simulation\ncycle is more than 12 times faster than the\nprevious method MATSim\n[99]. P-HASE designed by\nMarurngsith and Mongkolsin can generate the GPU code\nOpenCL automatically without any GPU programming\nlanguage knowledge to optimize large-scale multi-\nagent simulation. By being evaluated through the\nexperiment on two GPU platforms, NVIDIA GeForce\n240m LE and AMD Radeon HD6650M, the large-scale\nmulti-agent model in support of the newly generated\nGPU can be 14 times faster than its multicore CPU\nversion[100]. The REPAST HPC framework raised by\nCollier and North uses C++ and MPI for large-scale\ndistributed computing, which is accelerated by multi-\nprocess parallel computing[101].\nThe application of the above two methods enables us\nto extend the multi-agent simulation modeling method\nto large-scale problems and calculate the simulation\nresults in a short time without losing too much\ncomputational accuracy, which has important application\nand promotion value. For example, in the transportation\nﬁeld, Kl ¨ugl and Rindsf ¨uer simulated a scene of more\nthan 40 000 agents passing through the Bourne railway\nstation within 1.5 virtual hours, not only ensuring that\nthe agents are moving without collision between two\npredeﬁned positions but also planning and replanning\nthe way the agents pass through the railway station\nﬂexibly\n[102]. Zhang et al. simulated the trafﬁc ﬂow\nsituation of Shanghai, China, with 200 000 agents on a\nnetwork with 50 000 links[103]. In the ﬁeld of medical\nimaging, Haroun et al. introduced large-scale multi-\nagents to merge the local image processing results\nafter local processing on brain magnetic resonance\nimaging and improve the quality of the image after\nsegmentation\n[104]. Zhang and Verbraeck studied the\ncontrol strategy for the mass spread of infectious diseases\non the basis of the large-scale ABMS method on PC; the\npopulation size, namely, the number of agents, reached\n19.6 million[105]. Suzumura et al. [106] proposed agent-\nbased complex cellular automata architecture, which\nrealizes the trafﬁc ﬂow simulation of one billion agents\non a supercomputer. The Los Amos National Laboratory\ndeveloped an agent-based model software package called\nTrafﬁc Analysis Simulation System [107]. The software\nis currently used to simulate the trafﬁc conditions in\nPortland, including 120 000 trafﬁc links and 1.5 million\nagents.\n6 Multi-Agent Modeling and Simulation\nPlatform\nWith the constant advancement of agent research,\nvarious agent simulation platforms have been developed\ninternationally. Among them are typical platforms\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 617\nwith strong pertinence to a certain ﬁeld. Examples\ninclude OpEMCSS, which can perform complex trafﬁc\nsystem simulation; MaDKit, which simulates complex\nsupply chains; and James providing multi-negotiation\nsimulation between agents. However, these platforms\nhave poor generality, that is, their strong simulation\ncapability only precipitates in speciﬁc ﬁelds. As\nfor the general agent simulation platform, typical\nplatforms, including Java agent development framework\n(JADE)[108], NetLogo [109], Swarm [110], REPAST [111],\nMASON[112], AnyLogic[113], and JCass[114], have been\nwidely used.\n6.1 JADE\nJADE is a software platform that provides basic,\nintermediate layer functions [108]. It follows the rules\nof the Foundation for Intelligent Physical Agents (FIPA)\nand can develop standard agent programs to complete\nthe interaction and simulation between multi-agents.\nFIPA, established in 1996, aims to standardize agent\ntechnology and improve the availability of agents. The\norigin of JADE was to validate the FIPA speciﬁcation set,\nwhich was launched by Telecom Italia in 1998 and has\nsince evolved into JADE. The research of the platform\nfocuses on the simplicity and usability of agent-based\nsoftware development. The platform opened its source\ncode in 2000, becoming a free, open-source platform.\nThe biggest advantage of JADE is that it uses Java\nlanguage for agent abstract programming, making JADE\nﬂexible, portable, and maintainable.\nJADE can complete all agent basic services, such\nas life cycle management, mobility, white and yellow\npage services, information transmission, and security\nmanagement. In JADE, agents communicate with each\nother in an asynchronous mode. Each agent has its own\nunique ID, and its own message queue for sending and\nreceiving messages, and these features do not rely on\nlocation. JADE does not provide a visual window for\nmodel simulation and thus needs further development.\nIn recent years, JADE has been widely applied in various\nagent-based simulation developments.\nThe agent of JADE exists in the container, and JADE\nis composed of many agent containers distributed on the\nnetwork. The container is a Java process, which can run\nmultiple agents and provide the services needed by JADE\nto run, manage, and execute agents. A main container\nmust be started ﬁrst to provide access to JADE, and other\ncontainers can only be added to the main container if\nthey are registered with it.\n6.2 NetLogo\nNetLogo is a programmable modeling environment\nfor simulating natural and social phenomena [109]. Uri\nWilensky introduced the tool in 1999 (http://ccl.\nnorthwestern.edu/netlogo/), and the tool has been further\ndeveloped at the Center of Connected Learning and\nComputer-Based Modeling, Northwestern University.\nNetLogo, equipped with various types of agents, such\nas turtle, patch, and link, is especially suitable for\nmodeling the evolution of complex systems. Modelers\ncan send instructions to thousands of independently\noperated agents and thus help in gaining insight into\nthe connection between individual behavior at the\nmicro level and the macroscopic model, which emerges\nthrough many individual interactions.\nThe bottom layer of NetLogo is implemented in\nthe Java programming language and can run on all\nmajor platforms (Mac, Windows, Linux, etc.) or in\nbrowsers, such as Java applets. NetLogo has detailed\ndocumentation and teaching materials. It also comes\nwith a model library that contains many already written\nsimulation models, which cover many areas of the\nnatural and social sciences, including biology and\nmedicine, physics and chemistry, mathematics and\ncomputer science, economics, social psychology, and so\non.\nNetLogo, a programming development platform\ninherited from the Logo language, can control thousands\nof individuals in modeling and thus compensate for the\ndeﬁciency wherein the Logo language can only control\na single individual. Therefore, NetLogo modeling can\nwell simulate the behavior of micro individuals and the\nemergence of macro patterns, as well as the relationship\nbetween them. NetLogo is a programming language and\nmodeling platform for natural and social phenomenon\nsimulation, especially for complex systems that develop\nover time.\n6.3 Swarm\nSwarm is a standard multi-agent software toolset for\ncomputer simulation developed by the Santa Fe Institute\nof the United States on the basis of the theory of\nCASs[110]. It provides an efﬁcient, reliable, and reusable\nsoftware experiment platform. By establishing computer\nmodels based on Swarm and invoking the rich class\nlibraries provided in the platform, simulation can be\nperformed in many research ﬁelds.\nGiven that the model and the way of interaction\nbetween the model elements in Swarm are without\nrestrictions, the user can focus on their interested speciﬁc\n618 Tsinghua Science and T echnology, October2021, 26(5): 608–624\nsystems rather than being bothered by data process, user\ninterface, and other pure software and programming\nproblems. It is also user friendly for noncomputer\nprofessional scholars. Therefore, Swarm has received\nwide attention from economics, management science,\necology, systematics, military simulation, computer\nscience, and other ﬁelds.\nGenerally, the Swarm model is composed of\nModelSwarm, ObserverSwarm, and Individual Agent\nand Environment, and it supports the analysis, display,\nand control of simulation experiments through the class\nlibrary. ModelSwarm includes a schedule of actions\nin the model and a set of inputs and outputs. The\ninput includes model parameters, such as the number\nof objects and initial value. The output includes the\nvalue of the variable to be observed and the result\nof the model. ObserverSwarm is a window for target\nmodel observation and measurement, including a set of\nindividuals and a schedule of behavior. Among them,\nthe individual is the detector used for observation and\nthe output interface, such as charts and two-dimensional\ngrid points. The behavior schedule is used to describe\nthe interval and sequence of sampling for each detector.\nThe Swarm agent can be further summarized by\nfour characteristics based on the CAS-based Swarm\nmodeling idea and its structure. The ﬁrst is aggregation\nthat a single agent can adhere to each other to form\nthe aggregation of multi-agents that have the same\nmovement trend as a single agent. The second is\nnonlinearity, that is, agents and their properties in the\nevent of change are not completely linear but nonlinear.\nThe third is ﬂow, which refers to the exchange of\ninformation ﬂow, energy ﬂow, and material ﬂow between\nagents. In addition, the channel and speed of ﬂow directly\naffect the process of the system. The fourth is diversity,\nthat is, a differentiation trend exists between agents.\nIn addition, the activation mechanisms of Swarm\nagents are as follows: The ﬁrst is tagging, which\nhelps realize information exchange and propose speciﬁc\nimplementation methods of searching and receiving\ninformation in the environment. The second is internal\nmodels, which indicate the concept of hierarchy, that is,\nevery agent has a complex internal mechanism. The third\nis building blocks, that is, complex systems are often\nformed on the basis of relatively simple components by\nchanging their combination. Therefore, Swarm’s agents\nare living individuals with multiple levels, continuous\ninteraction with the outside world, and continuous\ndevelopment and evolution.\n6.4 REPAST\nREPAST was jointly developed by the University of\nChicago and Argonne National Laboratory and was\nsubsequently maintained and updated by the REPAST\nOrganization for Architecture and Development\n[111].\nThe platform supports Java, C#, and Python. Its software\narchitecture is similar to Swarm, which is mainly used\nin the ﬁeld of social science and has professional tools\nfor social science model development. The platform\nprovides some simple model libraries, class libraries,\nand genetic, regression, and other algorithms, which\ncan be used for model development by using interfaces\nand display agent simulation data. REPAST was only\navailable as an implementation in Java and has been\navailable in C# and Python since version 3.0 was\nproposed to expand the REPAST user base.\nSince the release of REPAST, several applications can\nbe divided into the following four categories:\n(1) Theoretical research: The generation process\nof a speciﬁc phenomenon in the system is observed\nthrough simulation, and the general rules of CAS can be\nfound and veriﬁed. Examples include implementing\nthe famous model “ECHO” in CAS theory with\nREPAST and studying game theory through multi-agent\nsimulation, such as the prisoner’s dilemma problem.\n(2) Social system simulation: Studies include the\ninteraction between agents and their environment and\nhow multi-agents with different goals and interests\nachieve cooperative behavior. In addition, the agent\ncan be an individual or an organization.\n(3) Economic system simulation: In agent-based\ncomputational economics (an emerging branch of\neconomics), REPAST has been used to implement and\nsimulate economic models (e.g., commercial network\nsimulation and supply chain simulation).\n(4) Comprehensive application: The Argonne\nNational Laboratory of the United States has extended\nREPAST to support GIS, distributed simulation, and\nother functions, and developed some large CAS\nsimulation (CASS) on this basis (e.g., the United States\nelectricity market simulation).\nAdvantages of REPAST: REPAST borrows\nsubstantial design experience from Swarm, and the\ngraphical user interfaces of the two are similar. Hence,\nREPAST is considered as a Swarm-like emulation\nkit. Four multi-agent simulation tools were evaluated\nand compared, including REPAST and Swarm, and\nthe results showed that REPAST ranked the ﬁrst in\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 619\nalmost all the scoring programs, such as documentation,\nmodeling, simulation capability, and usability; its\nsynthesis score is also the highest.\n6.5 MASON\nMASON, which was developed by George MASON\nUniversity[112], is programmed with Java language\nand is mainly used for agent-based discrete event\nsimulation. MASON’s main characteristics are fast\nexecution, ﬂexible use, and graphical interface for 2D\nand 3D visual display. Given the limited software size of\nthe MASON platform, it can only perform simulations\nof a lower-magnitude model.\n6.6 AnyLogic\nAnyLogic, which was developed by XJ\nTechnologies[113], is a widely used tool for discrete,\nsystem dynamics, and multi-agent and hybrid system\nmodeling and simulation. In addition to the basic\nsimulation, the platform also contains enterprise\nlibraries. AnyLogic supports development with Java\nand Uniﬁed Modeling Language (UML)-Real Time, as\nwell as models by differential equations. Its professional\nlibrary covers a wide range of ﬁelds, including logistics,\ntransportation, urban planning, and other aspects.\nAnyLogic is the ﬁrst to use UML for simulation and\nis the only commercial software that supports mixed\nstate machine language for simulation development.\nAnyLogic has a complete visual window that can\nobserve the simulation process clearly and intuitively.\nIt can integrate multi-intelligence simulation with\nmachine learning, build a training environment for\nmulti-agent reinforcement learning schemes, and model\nall cooperative, competitive, or hierarchical behavior.\n6.7 JCass\nJCass is a distributed simulation platform for complex\nsystems and was developed by the State Key Laboratory\nof Parallel and Distributed Processing, National\nUniversity of Defense Technology[114]. It is a general\nCASS platform that can be applied in many disciplines.\nJCass supports hierarchical simulation and\nhierarchical scheduling, allowing users to build\nand test multitier models for the description of\nemergence. The basic unit of JCass simulation is agents\nwho communicate with each other through a message\nmechanism. JCass provides not only a conservative\ntime promotion mechanism and an optimistic time\npromotion mechanism but also a hierarchical hybrid\ntime management protocol. It is developed in Java,\nsupports cross-platform emulation, and provides\nlibraries for building agents and debugging programs.\n6.8 Comparison among platforms\nConsidering the space environment of the model,\nJADE and Swarm only support 2D space environments,\nwhereas other platforms can support 2D and 3D\nspace environments. In terms of algorithms, NetLogo\ndoes not support complex algorithms temporarily.\nSwarm and REPAST support genetic algorithms, neural\nnetwork algorithms, and other Java computing packages.\nMASON supports evolutionary algorithms and other\nJava computing packages. AnyLogic and JADE support\nany Java language based algorithm. In terms of graphics\nand data visualization, all the platforms except JADE\nare equipped with a visual display module. In terms\nof performance, MASON, REPAST, and Swarm are\nframework class library platforms, among which the\ndevelopment of MASON is relatively immature, whereas\nthe development of Swarm is relatively mature. Swarm\nis a stand-alone platform, which has low portability\nand poor simulation performance for complex situations.\nSimilar to Swarm, REPAST is suitable for simple and\nsmall-scale simulations, and its composition and design\ncannot be self-improved. NetLogo and AnyLogic are\ncommercial softwares without independent property\nrights for platform development and thus need a\ncertain fee to obtain all functions. JADE, an open-\nsource platform, is ﬂexible and thus can be distributed\nacross different hosts. JCass is a general simulation\nplatform that uses the Java programming language.\nIt can strongly support the heterogeneous distributed\nplatform, design the rule base based on XML, and\nsupport hierarchical multi-agent modeling. However,\nit also has shortcomings, such as the single underlying\ncommunication method, high cost of time management,\nnonsupport of environment visual modeling, and\ninsufﬁcient support for evolution mechanism modeling.\n7 Conclusion\nThe concepts and ideas of MAMS, including\ncomputational entities, model aggregation, perception\nand behavior, decentralized control, environment, and\ninteraction, were introduced. This study expounded the\nadvantages of multi-agent simulation technology from\nseven aspects, namely, descriptive ability, emergence\nanalysis, organizational framework and evolutionary\nmechanism, autonomic solution and decision-making\nability, interaction ability, distributed simulation, and\n620 Tsinghua Science and T echnology, October2021, 26(5): 608–624\nmodel reuse, and presented the application status of the\ntechnology in the social, economic, and military ﬁelds.\nThis work studied the developing status of hybrid\nmodeling and simulation based on system dynamics\nand multi-agents and pointed out the advantages and\ndisadvantages of SDMS and MAMS in the complex\nsystem simulation. SDMS focuses on system dynamic\nbehavior, analyzing the interaction between different\nelements and accumulation effects while ignoring the\nspatial factors. MAMS focuses on the interaction in\nspace and ignores the feedback effect of macro social\nand economic factors on agents. The hybrid model based\non MAMS and SDMS is an effective method for complex\nsystem modeling and simulation.\nOur study reviewed the development of multi-agent\nreinforcement learning modeling and simulation and\npresented the algorithm problems to be solved in\nmulti-agent reinforcement learning. These problems\nare summarized as follows: First, the convergence\nand stability of multi-agent reinforcement learning\nhave not been proven systematically. Second, the state\nspace of multi-agent reinforcement learning is too large,\nand the training time is long. Third, the expansibility\nand knowledge transfer of multi-agent reinforcement\nlearning are poor.\nThe current situation of large-scale MAMS was also\nsummarized. The two most effective and common\nsolutions for large-scale MAMS were analyzed. First,\nthe model was reorganized at the software level\nvia “super individual”. Second, distributed parallel\ncomputing was used to accelerate large-scale computing.\nTypical MAMS platforms, including JADE, NetLogo,\nSwarm, REPAST, MASON, and AnyLogic, were\nintroduced. Furthermore, representative and widely used\ngeneral multi-agent modeling and simulation platforms\nwere compared and analyzed.\nReferences\n[1] C. M. Macal and M. J. North, Agent-based modeling and\nsimulation: Desktop ABMS, presented at 2007 Winter\nSimulation Conf., Washington, DC, USA, 2007, pp. 95–\n106.\n[2] Z. B. Tao He, Research on the key technology of multi-\nagent system design, (in Chinese),Modern Electron. Techn.,\nvol. 29, no. 14, pp. 31–34, 2006.\n[3] K. Liu, Modeling and simulation of complex adaptive\nsystem based on multi-agent, PhD dissertation, Central\nSouth University, Changsha, China, 2011.\n[4] M. Abbas, Agent-based modeling and simulation,\npresented at Artiﬁcial Intelligence Applications to Critical\nTransportation Issues, Washington, DC, USA, 2012, p. 58.\n[5] H. Z. Deng, Modeling and simulation method and its\napplication based on multi-agent, (in Chinese), PhD\ndissertation, National University of Defense Technology,\nXi’an, China, 2002.\n[6] H. Z. Deng, Y . J. Tan, and Y . Chi, A complex system\nresearch method-multiagent-based ensemble modeling and\nsimulation method, (in Chinese), Syst. Eng., vol. 18, no. 4,\npp. 73–78, 2000.\n[7] B. J. L. Berry, L. D. Kiel, and E. Elliott, Adaptive agents,\nintelligence, and emergent human organization: Capturing\ncomplexity through agent-based modeling, Proc. Nat. Acad.\nSci. USA, vol. 99, no. S3, pp. 7187–7188, 2002.\n[8] M. H. Haghighat and J. Li, Intrusion detection system\nusing voting-based neural network, Tsinghua Science and\nTechnology, vol. 26, no. 4, pp. 484–495, 2021.\n[9] J. Zhang, Z. Tang, Y . Xie, M. Ai, and W. Gui, Visual\nperception-based fault diagnosis in froth ﬂotation using\nstatistical approaches, Tsinghua Science and Technology,\nvol. 26, no. 2, pp. 172–184, 2021.\n[10] D. Bloembergen, K. Tuyls, D. Hennes, and M. Kaisers,\nEvolutionary dynamics of multi-agent learning: A survey,\nJ . Artif. Intell. Res., vol. 53, no. 1, pp. 659–697, 2015.\n[11] P. Tranouez, E. Daud´e, and P. Langlois, A multiagent urban\ntrafﬁc simulation, arXiv preprint arXiv: 1201.5472, 2012.\n[12] D. Schreiber, The emergence of parties: An agent-based\nsimulation, Polit. Res. Quart., vol. 67, no. 1, pp. 136–151,\n2014.\n[13] J. L. Casti, Would-be Worlds: How Simulation is Changing\nthe Frontiers of Science. New York, NY , USA: John Wiley\n& Sons, Inc., 1998.\n[14] B. Raney, N. Cetin, A. V¨ollmy, M. Vrtic, K. Axhausen, and\nK. Nagel, An agent-based microsimulation model of swiss\ntravel: First results, Netw. Spat. Econ., vol. 3, no. 1, pp.\n23–41, 2003.\n[15] J. M. Epstein and R. Axtell, Growing Artiﬁcial Societies:\nSocial Science from the Bottom Up. Washington, DC, USA:\nBrookings Institution Press, 1996.\n[16] C. Bilge, Venables, an agent-based model of a supermarket,\nhttp://www.simworld.co.uk/, 2021.\n[17] R. Pryor, N. Basu, and T. Quint, Development of\nASPEN: A microanalytic simulation model of the US\neconomy, Working Paper SAND-96–0434, Sandia National\nLaboratories, Albuquerque, NM, USA, 1996.\n[18] N. Basu and R. J. Pryor, Growing a market economy.\nSAND-97-2093 Distribution, presented at Unlimited\nRelease Category UCD905, Albuquerque, NM, USA, 1997.\n[19] W. B. Arthur, S. N. Durlauf, and D. Lane, The Economy\nAs an Evolving Complex System II. Boca Raton, FL, USA:\nCRC Press, 1997.\n[20] W. B. Arthur, J. H. Holland, B. LeBaron, R. Palmer, and\nP. Tayler, Asset pricing under endogenous expectations in\nan artiﬁcial stock market, presented at the Economy As an\nEvolving Complex System II, Reading, MA, USA, 1996.\n[21] A. Khalafallah, Neural network based model for predicting\nhousing market performance, Tsinghua Science and\nTechnology, vol. 13, no. S1, pp. 325–328, 2008.\n[22] H. Rahmandad and J. Sterman, Heterogeneity and network\nstructure in the dynamics of diffusion: Comparing agent-\nbased and differential equation models, Manag. Sci., vol.\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 621\n54, no. 5, pp. 998–1014, 2008.\n[23] M. Komosinski and A. Adamatzky,Artiﬁcial Life Models\nin Software. 2nd ed. London, UK: Springer Science &\nBusiness Media, 2009.\n[24] T. Lorenz, Abductive fallacies with agent-based modeling\nand system dynamics, in Int. Workshop on Epistemological\nAspects of Computer Simulation in the Social Sciences, F.\nSquazzoni, ed. Berlin, Germany: Springer-Verlag, 2009, pp.\n141–152.\n[25] T. Lorenz and A. Jost, Towards an orientation framework in\nmulti-paradigm modeling, in Proc.24t h Int. Conf. System\nDynamics Society, Nijmegen, the Netherlands, 2006, pp.\n2134–2151.\n[26] Z. K. Ding, W. Y . Gong, S. H. Li, and Z. Z. Wu, System\ndynamics versus agent-based modeling: A review of\ncomplexity simulation in construction waste management,\nSustainability, vol. 10, no. 7, p. 2484, 2018.\n[27] C. Swinerd and K. R. McNaught, Design classes for hybrid\nsimulations involving agent-based and system dynamics\nmodels, Simulat. Model. Pract. Theory, vol. 25, pp. 118–\n133, 2012.\n[28] C. Heinze, B. Smith, and M. Cross, Thinking quickly:\nAgents for modeling air warfare, in Australian Joint Conf.\nArtiﬁcial Intelligence AI 1998, G. Antoniou and J. Slaney,\neds. Berlin, Germany: Springer, 1998, pp. 47–58.\n[29] R. W. Hill, J. Chen, J. Gratch, P. Rosenbloom, and M.\nTambe, Soar-RWA: Planning, teamwork, and intelligent\nbehavior for synthetic rotary wing aircraft, inProc.7t h Conf.\nComputer Generated Forces & Behavioral Representation,\nOrlando, FL, USA, 1998, pp. 12–14.\n[30] A. Alvanchi, S. Lee, and S. AbouRizk, Modeling\nframework and architecture of hybrid system dynamics and\ndiscrete event simulation for construction, Comp. Aided\nCivil Infrastruct. Eng., vol. 26, no. 2, pp. 77–91, 2011.\n[31] L. L¨attil¨a, P. Hilletofth, and B. S. Lin, Hybrid simulation\nmodels–when, why, how? Expert Syst. Appl., vol. 37, no.\n12, pp. 7969–7975, 2010.\n[32] J. W. Forrester, System dynamics, systems thinking, and\nsoft OR, Syst. Dynam. Rev., vol. 10, nos. 2\u00063, pp. 245–256,\n1994.\n[33] F. Nasirzadeh, M. Khanzadi, and M. Mir, A hybrid\nsimulation framework for modelling construction projects\nusing agent-based modelling and system dynamics: An\napplication to model construction workers’ safety behavior,\nInt.J . Construct. Manag., vol. 18, no. 2, pp. 132–143, 2018.\n[34] D. D. Wu, K. F. Xie, H. Liu, S. Zhao, and D. L.\nOlson, Modeling technological innovation risks of an\nentrepreneurial team using system dynamics: An agent-\nbased perspective, Technol. Forecast.Soc. Change, vol. 77,\nno. 6, pp. 857–869, 2010.\n[35] F. Nasirzadeh, A. Afshar, and M. Khanzadi, System\ndynamics approach for construction risk analysis, Int.\nJ .\nCivil Eng., vol. 6, no. 2, pp. 120–131, 2008.\n[36] J. Swanson, Business dynamics-systems thinking and\nmodeling for a complex world, J . Oper. Res. Soc., vol.\n53, no. 4, pp. 472–473, 2002.\n[37] M. Watkins, A. Mukherjee, N. Onder, and K. Mattila,\nUsing agent-based modeling to study construction labor\nproductivity as an emergent property of individual and crew\ninteractions, J . Construct. Eng. Manage., vol. 135, no. 7,\npp. 657–667, 2009.\n[38] S. E. Phelan, A note on the correspondence between\ncomplexity and systems theory, Syst. Pract. Act. Res., vol.\n12, no. 3, pp. 237–246, 1999.\n[39] J. M. Epstein and R. Axtell, Growing Artiﬁcial Societies:\nSocial Science from the Bottom Up. Cambridge, MA, USA:\nMIT Press, 1996.\n[40] M. Barbati, G. Bruno, and A. Genovese, Applications of\nagent-based models for optimization problems: A literature\nreview, Expert Syst. Appl., vol. 39, no. 5, pp. 6020–6028,\n2012.\n[41] H. J. Scholl, Agent-based and system dynamics modeling:\nA call for cross study and joint research, in Proc.34t h Ann.\nHawaii Int. Conf. System Sciences, Maui, HI, USA, 2001.\n[42] J. Pourdehnad, K. Maani, and H. Sedehi, System dynamics\nand intelligent agent-based simulation: Where is the\nsynergy, in Proc. 20t h Int. Conf. System Dynamics Society,\nPalermo, Italy, 2002, pp. 1–16.\n[43] L. Stemate, C. Pasca, and I. Taylor, A comparison\nbetween system dynamics and agent based modeling\nand opportunities for cross-fertilization, in Proc. Winter\nSimulation Conf., Washington, DC, USA, 2007, pp. 2376.\n[44] N. Schieritz, Integrating system dynamics and agent-based\nmodeling, in Proc. 20t h Int. Conf. System Dynamics Society,\nPalermo, Italy, 2002, pp. 1–3.\n[45] N. Schieritz and P. M. Milling, Modeling the forest or\nmodeling the trees: A comparison of system dynamics and\nagent-based simulation, in Proc. System Dynamics Conf.,\nNew York, NY , USA, 2003, pp. 1–15.\n[46] N. Schieritz N and A. Grobler, Emergent structures in\nsupply chains-a study integrating agent-based and system\ndynamics modeling, in Proc. 36t h Ann. Hawaii Int. Conf.\nSystem Sciences, Big Island, HI, USA, 2003, p. 9.\n[47] M. Teose, K. Ahmadizadeh, E. O’Mahony, R. L. Smith, L.\nZhao, S. P. Ellner, C. Gomes, and Y . Grohn, Embedding\nsystem dynamics in agent based models for complex\nadaptive systems, in Proc. 22nd Int. Joint Conf. Artiﬁcial\nIntelligence, Barcelona, Spain, 2011, pp. 2531–2538.\n[48] E. Shaﬁei, H. Stefansson, E. I. Asgeirsson, B. Davidsdottir,\nand M. Raberto, Integrated agent-based and system\ndynamics modelling for simulation of sustainable mobility,\nTransp. Rev., vol. 33, no. 1, pp. 44–70, 2013.\n[49] C. M. Macal, To agent-based simulation from system\ndynamics, in Proc. 2010 Winter Simulation Conf.,\nBaltimore, MD, USA, 2010, pp. 371–382.\n[50] A. Djanatliev, R. German, P. Kolominsky-Rabas, and B. M.\nHofmann, Hybrid simulation with loosely coupled system\ndynamics and agent-based models for prospective health\ntechnology assessments, in Proc. 2012 Winter Simulation\nConf. (WSC), Berlin, Germany, 2012, pp. 1–12.\n[51] G. P. Figueredo, P. O. Siebers, U. Aickelin, A.\nWhitbrook, and J. M. Garibaldi, Juxtaposition of system\ndynamics and agent-based simulation for a case study in\nimmunosenescence, PLoS One, vol. 10, no. 3, p. e0118359,\n2015.\n[52] J. Schryver, J. Nutaro, and M. Shankar, Emulating a\nsystem dynamics model with agent-based models: A\nmethodological case study in simulation of diabetes\nprogression, Open J. Model. Simul., vol. 3, no. 4, p. 60811,\n622 Tsinghua Science and T echnology, October2021, 26(5): 608–624\n2015.\n[53] F. Ferrada and L. M. Camarinha-Matos, A system\ndynamics and agent-based approach to model emotions\nin collaborative networks, in Doctoral Conference\non Computing, Electrical and Industrial Systems, L.\nCamarinha-Matos, M. Parreira-Rocha, and J. Ramezani,\neds. Springer, 2017, pp. 29–43.\n[54] N. Marilleau, C. Lang, and P. Giraudoux, Coupling agent-\nbased with equation-based models to study spatially explicit\nmegapopulation dynamics, Ecol. Model., vol. 384, pp. 34–\n42, 2018.\n[55] R. S. Sutton and A. G. Barto, Reinforcement Learning: An\nIntroduction. Cambridge, MA, USA: MIT Press, 1998.\n[56] M. D. Awheda and H. M. Schwartz, The residual gradient\nFACL algorithm for differential games, presented at\n2015 IEEE 28th Canadian Conf. Electrical and Computer\nEngineering (CCECE), Halifax, Canada, 2015, pp. 1006–\n1011.\n[57] W. D. Smart and L. Pack Kaelbling, Effective reinforcement\nlearning for mobile robots, in Proc. 2002 IEEE Int.\nConf. Robotics and Automation (Cat.No.02CH37292),\nWashington, DC, USA, 2002, pp. 3404–3410.\n[58] M. L. Littman, Markov games as a framework for multi-\nagent reinforcement learning, in Proc. 12 t h Int. Conf.,\nElsevier, 1994, pp. 157–163.\n[59] B. Luo, H. N. Wu, and T. W. Huang, Off-policy\nreinforcement learning for H1 control design, IEEE Trans.\nCybernet., vol. 45, no. 1, pp. 65–76, 2015.\n[60] B. Luo, H. N. Wu, and H. X. Li, Data-based suboptimal\nneuro-control design with reinforcement learning for\ndissipative spatially distributed processes, Ind. Eng. Chem.\nRes., vol. 53, no. 19, pp. 8106–8119, 2014.\n[61] W. Dixon, Optimal adaptive control and differential games\nby reinforcement leanring principles [book review], IEEE\nContr. Syst. Mag., vol. 34, no. 3, pp. 80–82, 2014.\n[62] P. X. Cai and Y . Zhang, Intelligent cognitive spectrum\ncollaboration: Convergence of spectrum sensing, spectrum\naccess, and coding technology, Intelligent and Converged\nNetworks, vol. 1, no. 1, pp. 79–98, 2020.\n[63] B. Hou, F. C. Sun, H. B. Li, and G. B. Liu, Consensus\nof second-order multi-agent systems with time-varying\ndelays and antagonistic interactions, Tsinghua Science and\nTechnology, vol. 20, no. 2, pp. 205–211, 2015.\n[64] Z. H. Zhao, Y . Gao, B. Luo, and S. F. Chen, Reinforcement\nlearning technology in multi-agent system, (in Chinese),\nComp. Sci., vol. 31, no. 3, pp. 23–27, 2004.\n[65] R. S. Sutton and A. G. Barto, Reinforcement Learning: An\nIntroduction. Cambridge, MA, USA: MIT Press, 1998, pp.\n133–160.\n[66] C. J. C. H. Watkins and P. Dayan,Q-learning, Mach. Learn.,\nvol. 8, nos. 3\u00064, pp. 279–292, 1992.\n[67] R. S. Sutton, D. McAllester, S. Singh, and Y . Mansour,\nPolicy gradient methods for reinforcement learning with\nfunction approximation, in Proc. 12\nt h Int. Conf. Neural\nInformation Proc. Systems, Denver, CO, USA, 2000, pp.\n1057–1063.\n[68] P. Stone and M. Veloso, Multiagent systems: A survey from\na machine learning perspective, Auton. Robots, vol. 8, no.\n3, pp. 345–383, 2000.\n[69] Y . Shoham, R. Powers, and T. Grenager, If multi-agent\nlearning is the answer, what is the question? Artif. Intell.,\nvol. 171, no. 7, pp. 365–377, 2006.\n[70] V . Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I.\nAntonoglou, D. Wierstra, and M. Riedmiller, Playing atari\nwith deep reinforcement learning, arXiv preprint arXiv:\n1312.5602, 2013.\n[71] G. Papoudakis, F. Christianos, A. Rahman, and S. V .\nAlbrecht, Dealing with non-stationarity in multi-agent deep\nreinforcement learning, arXiv preprint arXiv: 1906.04737,\n2019.\n[72] T. T. Nguyen, N. D. Nguyen, and S. Nahavandi, Deep\nreinforcement learning for multiagent systems: A review\nof challenges, solutions, and applications, IEEE Trans.\nCybernet., vol. 50, no. 9, pp. 3826–3839, 2020.\n[73] L. Busoniu, R. Babuska, and B. De Schutter, A\ncomprehensive survey of multiagent reinforcement learning,\nIEEE Trans. Syst. Man Cybernet. Part C Appl. Rev., vol. 38,\nno. 2, pp. 156–172, 2008.\n[74] P. Hernandez-Leal, B. Kartal, and M. E. Taylor, A survey\nand critique of multiagent deep reinforcement learning,\nAuton. Agents Multi Agent Syst., vol. 33, no. 6, pp. 750–\n797, 2019.\n[75] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus,\nJ. Aru, J. Aru, and R. Vicente, Multiagent cooperation and\ncompetition with deep reinforcement learning, PLoS One,\nvol. 12, no. 4, p. e0172395, 2017.\n[76] M. Raghu, A. Irpan, J. Andreas, B. Kleinberg, Q. V .\nLe, and J. Kleinberg, Can deep reinforcement learning\nsolve erdos-selfridge-spencer games? arXiv preprint arXiv:\n1711.02301, 2018.\n[77] S. Sukhbaatar, A. Szlam, and R. Fergus, Learning\nmultiagent communication with backpropagation,\npresented at 30\nth Ann. Conf. Neural Information Proc.\nSystems, Barcelona, Spain, 2016, pp. 2244–2252.\n[78] J. C. Jiang and Z. Q. Lu, Learning attentional\ncommunication for multi-agent cooperation, in Proc.\n32nd Int. Conf. Neural Information Processing Systems,\nMontr´eal, Canada, 2018, pp. 7254–7264.\n[79] R. Lowe, Y . I. Wu, A. Tamar, J. Harb, O. P. Abbeel, and I.\nMordatch, Multi-agent actor-critic for mixed cooperative-\ncompetitive environments, in Proc. 31s t Int. Conf. Neural\nInformation Processing Systems, Long Beach, CA, USA,\n2017, pp. 6379–6390.\n[80] J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and\nS. Whiteson, Counterfactual multi-agent policy gradients,\narXiv preprint arXiv: 1705.08926, 2018.\n[81] S. V . Albrecht and P. Stone, Autonomous agents modelling\nother agents: A comprehensive survey and open problems,\nArtif. Intell., vol. 258, pp. 66–95, 2018.\n[82] R. Raileanu, E. Denton, A. Szlam, and R. Fergus, Modeling\nothers using oneself in multi-agent reinforcement learning,\narXiv preprint arXiv: 1802.09640, 2018.\n[83] M. Lanctot, V . Zambaldi, A. Gruslys, A. Lazaridou, K.\nTuyls, J. P´erolat, D. Silver, and T. Graepel, A uniﬁed game-\ntheoretic approach to multiagent reinforcement learning,\nin Proc. 31s t Int. Conf. Neural Information Processing\nSystems, Long Beach, CA, USA, 2017, pp. 4190–4203.\nWenhui Fan et al.: Multi-Agent Modeling and Simulation in the AI Age 623\n[84] L. O. Souza, G. de Oliveira Ramos, and C. G. Ralha,\nExperience sharing between cooperative reinforcement\nlearning agents, presented at 2019 IEEE 31 st Int. Conf.\nTools with Artiﬁcial Intelligence (ICTAI), Portland, OR,\nUSA, 2019, pp. 963–970.\n[85] S. Barrett, A. Rosenfeld, S. Kraus, and P. Stone, Making\nfriends on the ﬂy: Cooperating with new teammates, Artif.\nIntell., vol. 242, pp. 132–171, 2017.\n[86] P. Y . Jiang, M. L. Yang, W. D. Li, J. J. Liu, W. Guo, and P.\nL. Li, Ci literature review and its application exploration in\nsocial manufacturing, (in Chinese), Chin Mech. Eng., vol.\n31, no. 15, pp. 1852–1865, 2020.\n[87] OpenAI, Openai ﬁve, https://blog.openai.com/openai-ﬁve/,\n2018.\n[88] O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu,\nA. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds,\nP. Georgiev, et al., Grandmaster level in starcraft II using\nmulti-agent reinforcement learning, Nature, vol. 575, no.\n7782, pp. 350–354, 2019.\n[89] H. R. Parry and M. Bithell, Large scale agent-based\nmodelling: A review and guidelines for model scaling,\nin Agent-Based Models of Geographical Systems, A.\nHeppenstall, A. Crooks, L. See, and M. Batty, eds. Springer,\n2012, pp. 271–308.\n[90] H. R. Parry and A. J. Evans, A comparative analysis\nof parallel processing and super-individual methods for\nimproving the computational performance of a large\nindividual-based model, Ecol. Model., vol. 214, nos. 2\u00064,\npp. 141–152, 2008.\n[91] F. Ablayev, M. Ablayev, J. Z. Huang, K. Khadiev, N.\nSalikhova, and D. M. Wu, On quantum methods for\nmachine learning problems part I: Quantum tools, Big Data\nMining and Analytics, vol. 3, no. 1, pp. 41–55, 2020.\n[92] J. Q. Huang, W. T. Han, X. Y . Wang, and W. G. Chen,\nHeterogeneous parallel algorithm design and performance\noptimization for WENO on the sunway taihulight\nsupercomputer, Tsinghua Scicence and Technology, vol.\n25, no. 1, pp. 56–67, 2020.\n[93] Z. Bo, H. C. Zhou, G. Q. Li, and Y . H. Huang, ZenLDA:\nLarge-scale topic model training on distributed data-parallel\nplatform, Big Data Mining and Analytics, vol. 1, no. 1, pp.\n57–74, 2018.\n[94] J. Blythe, J. Bollenbacher, D. Huang, P. M. Hui, R. Krohn,\nD. Pacheco, G. Muric, A. Sapienza, A. Tregubov, Y . Y . Ahn,\net al., Massive multi-agent data-driven simulations of the\nGitHub ecosystem, in Int. Conf. Practical Applications of\nAgents and Multi-Agent Systems, Y . Demazeau, E. Matson,\nJ. Corchado, and F. De la Prieta, eds. Springer, 2019, pp.\n3–15.\n[95] J. C. Campagne, A. Cardon, E. Collomb, and T. Nishida,\nMassive multi-agent systems control, in Int. Workshop\non Formal Approaches to Agent-Based Systems, M. G.\nHinchey, J. L. Rash, W. F. Truszkowski, and C. A. Rouff,\neds. Springer, 2004, pp. 275–280.\n[96] Z. J. Zhou and H. Xu, Decentralized adaptive optimal\ncontrol for massive multi-agent systems using mean ﬁeld\ngame with self-organizing neural networks, presented at\n2019 IEEE 58th Conf. Decision and Control (CDC), Nice,\nFrance, 2019, pp. 1225–1230.\n[97] N. Fachada, V . V . Lopes, R. C. Martins, and A. C. Rosa,\nParallelization strategies for spatial agent-based models,\nInt.J . Parallel Program., vol. 45, no. 3, pp. 449–481, 2017.\n[98] N. Fachada, V . V . Lopes, R. C. Martins, and A. C. Rosa,\nTowards a standard model for research in agent-based\nmodeling and simulation, PeerJ Comp. Sci., vol. 1, no. 2, p.\ne36, 2015.\n[99] A. Saprykin, N. Chokani, and R. S. Abhari, Large-scale\nmulti-agent mobility simulations on a GPU: Towards high\nperformance and scalability, Proced. Comp. Sci., vol. 151,\npp. 733–738, 2019.\n[100] W. Marurngsith and Y . Mongkolsin, Creating GPU-\nenabled agent-based simulations using a PDES tool, in\nDistributed Computing and Artiﬁcial Intelligence Advances\nin Intelligent Systems & Computing, S. Omatu, J. Neves, J.\nRodriguez, J. Paz Santana, and S. Gonzalez, eds. Springer,\nvol. 217, pp. 227–234, 2013.\n[101] N. Collier and M. North, Parallel agent-based simulation\nwith repast for high performance computing, Simulation,\nvol. 89, no. 10, pp. 1215–1235, 2013.\n[102] F. Kl ¨ugl and G. Rindsf ¨user, Large-scale agent-based\npedestrian simulation, in German Conf. Multiagent System\nTechnologies, P. Petta, J. P. M ¨uller, M. Klusch, and M.\nGeorgeff, eds. Springer, 2007, pp. 145–156.\n[103] L. Zhang, W. C. Yang, J. M. Wang, and Q. Rao, Large-\nscale agent-based transport simulation in Shanghai, China,\nTransport. Res. Rec.J . Transport. Res. Board, vol. 2399,\nno. 1, pp. 34–43, 2013.\n[104] R. Haroun, F. Boumghar, S. Hassas, and L. Hamami, A\nmassive multi-agent system for brain MRI segmentation,\npresented at Int. Workshop on Massively Multiagent\nSystems, Kyoto, Japan, 2004, pp. 174–186.\n[105] M. X. Zhang, R. Q. Meng, and A. Verbraeck, Including\npublic transportation into a large-scale agent-based model\nfor epidemic prediction and control, in Proc. Summer\nComputer Simulation (SummerSim ’15). Society for\nComputer Simulation International, San Diego, CA, USA,\n2015, pp. 1–8.\n[106] T. Suzumura, C. Houngkaew, and H. Kanezashi, Towards\nbillion-scale social simulations, in Proc. Winter Simulation\nConf., Savannah, GA, USA, 2015, pp. 781–792.\n[107] L. A. N. Laboratory, Trafﬁc analysis simulation system,\nhttp://transims.tsasa.lanl.gov, 2021.\n[108] W. H. Yu,JADE-based Multi-agent System Development\nTechnology, (in Chinese). Beijing, China: National Defense\nIndustry Press, 2011.\n[109] C. Zhu, Macroscopic and microscopic expressway trafﬁc\nﬂow modeling and simulation based on multi-agent system,\nMaster dissertation, Beijing University of Technology,\nBeijing, China, 2016.\n[110] S. Y . Liao, J. Chen, H. W. Lu, and J. H. Dai, Summarization\nof agent-based modeling and simulation, (in Chinese),\nComput. Simul., vol. 25, no. 12, pp. 1–7, 2008.\n[111] D. C Zhou and X. P. Wu, Realization of ACS in repast,\n(in Chinese), J . Wuhan Univ. Technol., vol. 29, no. 8, pp.\n121–124, 2007.\n[112] Department of Computer Science, Nguyen Engineering\nBuilding, 4400 University Drive, Mason, https://\n624 Tsinghua Science and T echnology, October2021, 26(5): 608–624\ncs.gmu.edu/, 2021.\n[113] T. A. Company, Anylogic, https://www.anylogic.cn/, 2021.\n[114] M. Zhou, The design and implement of agent modeling\nin complex system distributed simulation platform, (in\nChinese), Master dissertation, National University of\nDefense Technology, Xi’an, China, 2013.\nWenhui Fan received the PhD degree\nin mechanical engineering from Zhejiang\nUniversity, Hangzhou, China in 1998. He\nobtained the postdoctoral certiﬁcate from\nTsinghua University, Beijing, China in 2000.\nHe is a vice president of China Simulation\nFederation. He is currently a professor at\nTsinghua University, Beijing, China. His\ncurrent research interests include multi-agent modeling and\nsimulation, large scale agent modeling and simulation, and multi-\nagent reinforcement learning.\nPeiyu Chen received the BS degree\nin automation from Beihang University,\nBeijing, China in 2019. She is now\npursuing the PhD degree at Department of\nAutomation, Tsinghua University, Beijing,\nChina. Her research interests include\ncomplex network, multi-agent system, and\nhybrid simulation.\nDaming Shi received the BS degree\nin automation from Beihang University,\nBeijing, China in 2018. He is now\npursuing the PhD degree at Department of\nAutomation, Tsinghua University, Beijing,\nChina. His main research interests include\nmulti-agent system, reinforcement learning,\nscheduling, and game theory.\nXudong Guo received the BS degree\nin automation from Tsinghua University,\nBeijing, China in 2020. He is now\npursuing the PhD degree at Department of\nAutomation, Tsinghua University, Beijing,\nChina. His research interests include\ncomplex network modeling and multi-agent\nsystem.\nLi Kou received the master degree from\nthe University of Defense and Technology,\nChangsha, Hunan in 2006. Currently,\nhe is pursuing the PhD degree at\nTsinghua University. His research interests\ninclude modeling and simulation of\ncomplex system, intelligent decision, and\ninformation system engineering."
}