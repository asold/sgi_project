{
  "title": "Randomized Positional Encodings Boost Length Generalization of Transformers",
  "url": "https://openalex.org/W4385571459",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3007157607",
      "name": "Anian Ruoss",
      "affiliations": [
        "Dalle Molle Institute for Artificial Intelligence Research",
        "University of Applied Sciences and Arts of Southern Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A3094273019",
      "name": "Grégoire Delétang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2015153470",
      "name": "Tim Genewein",
      "affiliations": [
        "University of Applied Sciences and Arts of Southern Switzerland",
        "Dalle Molle Institute for Artificial Intelligence Research"
      ]
    },
    {
      "id": "https://openalex.org/A2731150805",
      "name": "Jordi Grau Moya",
      "affiliations": [
        "University of Applied Sciences and Arts of Southern Switzerland",
        "Dalle Molle Institute for Artificial Intelligence Research"
      ]
    },
    {
      "id": "https://openalex.org/A2918212282",
      "name": "Róbert Csordás",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2237219533",
      "name": "Mehdi Bennani",
      "affiliations": [
        "Dalle Molle Institute for Artificial Intelligence Research",
        "University of Applied Sciences and Arts of Southern Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2100602521",
      "name": "Shane Legg",
      "affiliations": [
        "Dalle Molle Institute for Artificial Intelligence Research",
        "University of Applied Sciences and Arts of Southern Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A2007821246",
      "name": "Joel Veness",
      "affiliations": [
        "Dalle Molle Institute for Artificial Intelligence Research",
        "University of Applied Sciences and Arts of Southern Switzerland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3005187920",
    "https://openalex.org/W3035206215",
    "https://openalex.org/W3099140684",
    "https://openalex.org/W4284701759",
    "https://openalex.org/W4301259831",
    "https://openalex.org/W3098666169",
    "https://openalex.org/W3033208747",
    "https://openalex.org/W4296957682",
    "https://openalex.org/W3191727383",
    "https://openalex.org/W2996132992",
    "https://openalex.org/W4224874866",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W3014096773",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4285206226",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3103682594",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W4287755175",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W2972515356",
    "https://openalex.org/W2124479173",
    "https://openalex.org/W4302011318",
    "https://openalex.org/W3197009789",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3104739822",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W4286903975",
    "https://openalex.org/W4283825976"
  ],
  "abstract": "Anian Ruoss, Grégoire Delétang, Tim Genewein, Jordi Grau-Moya, Róbert Csordás, Mehdi Bennani, Shane Legg, Joel Veness. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 1889–1903\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nRandomized Positional Encodings\nBoost Length Generalization of Transformers\nAnian Ruoss∗1 Grégoire Delétang∗1 Tim Genewein1 Jordi Grau-Moya1\nRóbert Csordás†2 Mehdi Bennani1 Shane Legg1 Joel Veness1\nAbstract\nTransformers have impressive generalization\ncapabilities on tasks with a fixed context length.\nHowever, they fail to generalize to sequences\nof arbitrary length, even for seemingly sim-\nple tasks such as duplicating a string. More-\nover, simply training on longer sequences is\ninefficient due to the quadratic computation\ncomplexity of the global attention mechanism.\nIn this work, we demonstrate that this failure\nmode is linked to positional encodings being\nout-of-distribution for longer sequences (even\nfor relative encodings) and introduce a novel\nfamily of positional encodings that can over-\ncome this problem. Concretely, our random-\nized positional encoding scheme simulates the\npositions of longer sequences and randomly\nselects an ordered subset to fit the sequence’s\nlength. Our large-scale empirical evaluation of\n6000 models across 15 algorithmic reasoning\ntasks shows that our method allows Transform-\ners to generalize to sequences of unseen length\n(increasing test accuracy by 12.0% on average).\n1 Introduction\nTransformers are emerging as the new workhorse\nof machine learning as they underpin many recent\nbreakthroughs, including sequence-to-sequence\nmodeling (Vaswani et al., 2017), image recog-\nnition (Dosovitskiy et al., 2021), and multi-task\nlearning (Reed et al., 2022). However, recent\nwork (Delétang et al., 2023) demonstrated that\nTransformers fail to generalize to longer sequences\non seemingly simple tasks such as binary addition.\nThus, while certain problems can be solved without\nlength generalization, algorithmic reasoning gener-\nally requires this ability, similar to many real-world\nsettings such as online or continual learning.\nWhile the Transformer’s attention mechanism\ncan recognize complex relationships amongst to-\n*Equal contribution. 1DeepMind. 2The Swiss AI\nLab, IDSIA, USI & SUPSI. †Work performed while the\nauthor was at DeepMind. Correspondence to {anianr,\ngdelt}@deepmind.com.\nFigure 1: Test-time evaluation with longer inputs.\nThe standard positional encoding vector has values\nlarger than those observed during training. Our ap-\nproach avoids this problem by assigning a random (or-\ndered) positional encoding vector using the full range\nof possible test positions to each training example.\nkens in the input sequence, it is limited by its lack\nof positional awareness. Thus, the input sequence\nis generally augmented with positional encodings\nto inject position information into the computation.\nHowever, current approaches only consider posi-\ntions up to the maximum training sequence length\nN, and thus all the positions N + 1, . . . , Mfor test\nsequences of length up to M will appear out-of-\ndistribution during evaluation (top of Fig. 1).\nThis work We introduce a novel family of ran-\ndomized positional encodings, which significantly\nimproves Transformers’ length generalization ca-\npabilities on algorithmic reasoning tasks. Our ap-\nproach is compatible with any existing positional\nencoding scheme and augments the existing meth-\nods by subsampling an ordered set of positions\nfrom a much larger range of positions than those\nobserved during training or evaluation (i.e., up\n1889\nto L ≫ M; bottom of Fig. 1). Thus, over the\ncourse of training, the Transformer will learn to\nhandle very large positional encodings and, there-\nfore no longer encounter out-of-distribution inputs\nduring evaluation. Importantly, our method leaves\nin-domain generalization performance unaffected\nand is also significantly more efficient than the\nnaive approach of simply training the Transformer\non longer sequences. Our main contributions are:\n• A novel family of positional encoding\nschemes that significantly improves the length\ngeneralization capabilities of Transformers,\nwhile leaving their in-domain generalization\nperformance unaffected.\n• A large-scale empirical evaluation on a wide\nrange of algorithmic reasoning tasks showing\nthe superiority of our method over prior work\n(an increase of the test accuracy by 12.0% on\naverage and up to 43.5% on certain tasks).\n• An open-source implementation of our\nmethod, available at https://github.\ncom/deepmind/randomized_\npositional_encodings.\n2 Related Work\nOur work is most closely related to the growing\nline of research on Transformers’ positional encod-\nings. The first approaches simply added a trans-\nformation of the tokens’ positions, e.g., scaled si-\nnusoids (Vaswani et al., 2017) or learned embed-\ndings (Gehring et al., 2017), to the embeddings\nof the input sequence. Dai et al. (2019) subse-\nquently showed that computing the attention (at\nevery layer) using the relative distances between\nthe key and query vectors improves the modeling\nof long-term (inter-context) dependencies. Simi-\nlarly, Su et al. (2021) proposed to inject position\ninformation by rotating the key-query products ac-\ncording to their relative distances. Finally, Press\net al. (2022) improved the length generalization\non natural language processing tasks by adding\na constant bias to each key-query attention score\n(proportional to their distance). However, as our ex-\nperiments in Section 4 will show, these approaches\nfail at length generalization on algorithmic reason-\ning tasks, which is precisely the goal of our work.\nA concurrent work developed randomized\nlearned positional encodings (Li and McClelland,\n2022), which are a special case of our family of ran-\ndomized positional encodings. We also note that\nthe necessity of feature and position randomization\nfor length generalization has been discussed in the\ncontext of graph neural networks, which subsume\nTransformers (Ibarz et al., 2022; Sato et al., 2021).\nFinally, Liu et al. (2020b) proposed to model the\nposition information as a continuous dynamical\nsystem in an effort to handle sequences longer than\nthose seen during training time.\nOur work is also related to the research area\non improving the systematic (length) generaliza-\ntion capabilities of Transformers (Ontañón et al.,\n2022), which includes approaches investigating em-\nbedding scaling or early stopping (Csordás et al.,\n2021), adaptive computation time (Dehghani et al.,\n2019), geometric attention with directional posi-\ntional encodings and gating (Csordás et al., 2022),\nand hierarchical reinforcement learning (Liu et al.,\n2020a). Such length generalization studies are of-\nten conducted in the context of formal language\ntheory, and we evaluate our method on the recent\nbenchmark by Delétang et al. (2023), which unifies\na large body of work on Transformers’ capability\nto recognize formal languages (Ackerman and Cy-\nbenko, 2020; Bhattamishra et al., 2020; Ebrahimi\net al., 2020; Hahn, 2020; Hao et al., 2022; Merrill,\n2019; Merrill and Sabharwal, 2022).\n3 Randomized Positional Encodings\nUnlike RNNs (Elman, 1990), which are unrolled\nover tokens one step at a time, Transformers pro-\ncess large chunks of the input sequence in parallel\nvia global attention (Vaswani et al., 2017). As a\nresult, Transformers do not need to “remember”\nprevious tokens, but they do have to break the\npermutation-invariance of the attention mechanism.\nTo that end, the embeddings of the input sequence\nare generally augmented with positional encodings.\nFor example, the vanilla Transformer adds the fol-\nlowing positional encodings to the embedded input\nsequence before passing it to the attention layers:\nPE(pos, 2i) = sin\n(\npos\n10000\n2i\ndmodel\n)\n, (1)\nPE(pos, 2i + 1) = cos\n(\npos\n10000\n2i\ndmodel\n)\n, (2)\nwhere pos is the token’s position in the sequence,\ndmodel ∈N is the dimension of the input embed-\nding, and i ∈{1, 2, . . . , dmodel/2}.\nWhile positional encodings generally succeed\nat inducing the required positional information\n1890\nfor sequences of fixed length, they are one of the\nmain failure modes preventing length generaliza-\ntion. Concretely, for a Transformer with standard\npositional encodings trained on a curriculum of se-\nquences of maximum length N, test sequences of\nlength M > Nwill shift the distribution of the re-\nsultant positional encodings away from those seen\nin training, with the shift getting increasingly large\nas M grows. To address this, we propose a random-\nized encoding scheme, which relies only on order\ninformation, and can be expected to generalize up\nto sequences of length M, where N < M ≤L,\nwith a configurable hyperparameter L.\nRandomized positional encodings We assume\nthat each training step will perform a step of loss\nminimization on a batch of data of fixed size. Let\nU(S) denote the discrete uniform distribution over\nset S, and let Pk := {S ⊆{1, . . . , L}|| S|= k}.\nFor each training step, we first sample a random\nlength n ∼U ({1, . . . , N}) (following Delétang\net al., 2023) and then a random set of indices I ∼\nU(Pn). We then sort I in ascending order, such\nthat I = {i1, . . . , in}for i1 < i2 < ··· < in, not-\ning that I is sampled without replacement. Finally,\nwe compute our randomized positional encoding\nfor token 1 ≤j ≤N as RPE(j, ·) := PE(ij, ·).\nAt test time, when processing a sequence of length\nM > N, we use the same procedure but for all to-\nken positions 1 ≤j ≤M. The intuition behind our\nmethod is to preserve the known good properties of\nrelative encoding but in a way that is independent\nof the maximum training length N and thus allows\ngeneralization to longer sequences at test time.\nWhen applying our randomized positional en-\ncoding scheme, we subsample the extended posi-\ntions only once per batch and not individually for\nevery sequence. For the sin / cos (Vaswani et al.,\n2017), learned (Gehring et al., 2017), and RoPE\nencodings (Su et al., 2021), we apply our method\nas described above, i.e., we directly replace the\noriginal token positions with their sampled counter-\npart. For the relative encoding (Dai et al., 2019), we\ncompute the relative distances between the sampled\npositions instead of the original positions. Finally,\nfor ALiBi (Press et al., 2022), we sample the bias\nvalues from the set of extended positions.\nAs a consequence, our tokens’ positional encod-\nings are no longer directly related to their exact\nposition (the encodings even change during train-\ning as they are resampled at every step). However,\nsince we maintain the order of the encodings, the\nTransformer can still learn to extract the relevant\npositional information from the subsampled encod-\nings. Indeed, we validate the necessity of ordering\nthe sampled positions in our ablation study in Ap-\npendix B.1. Thus, the success of our encoding\nscheme offers an interesting insight into the induc-\ntive biases of the Transformer architecture.\nAs we will show in Section 4, our randomized\nencodings trained only on lengths up to N perform\nthe same on sequences of length M as prior ap-\nproaches trained on lengths up to M. Therefore,\nour method demonstrates that Transformers can be\nefficiently trained on short sequences as long as\n(i) the longer sequences share the same structure\nand (ii) the longer positions are observed during\ntraining. Moreover, as the running time of global\nattention is O(ℓ2) for sequence length ℓ, our en-\ncoding scheme is significantly faster than directly\ntraining a model on long sequences. Furthermore,\nwe also note that our randomized positional en-\ncoding scheme significantly boosts length general-\nization while leaving the in-domain generalization\nperformance largely unaffected (see Fig. 4).\nThe main limitation of our approach is that the\nmaximum test sequence length M has to be known\nin advance to choose L ≫ M. However, our\nmethod is compatible with a wide range of val-\nues for L (see Appendix B.1), and we note that this\nis a much weaker assumption than that required\nfor the naive approach of simply training on longer\nsequences. However, note that if L is chosen to\nbe much larger than N or M, it is theoretically\nunlikely for the model to encounter enough unique\nindices during training, likely leading to poor per-\nformance (both in- and out-of-distribution).\n4 Experimental Evaluation\nProblem setup We closely follow the experi-\nment setup of Delétang et al. (2023) and eval-\nuate our method on a wide range of algo-\nrithmic reasoning tasks such as modular arith-\nmetic, reversing/duplicating a string, binary ad-\ndition/multiplication, and bucket sort. The tasks\nare derived from formal language recognition and\nthus grouped according to the Chomsky hierar-\nchy (Chomsky, 1956), which partitions languages\ninto regular (R), context-free, context-sensitive\n(CS), and recursively enumerable. Regular tasks\ncan be solved by a finite-state automaton (FSA), de-\nterministic context-free (DCF) tasks can be solved\nby an FSA with access to a deterministic stack, and\n1891\nTable 1: Accuracy (in percentage) averaged over all test lengths and maximized over 10 random seeds and 3 learning\nrates. The random accuracy is 50%, except forMODULAR ARITHMETIC (SIMPLE ), CYCLE NAVIGATION , BUCKET\nSORT, and MODULAR ARITHMETIC , where it is 20%. Our randomized method increases the test accuracy by\n12.0% on average. The randomized learned encodings (denoted with ⋆) are equivalent to label-based encodings (Li\nand McClelland, 2022). †denotes permutation-invariant tasks, which can be solved without positional information.\nRandomized (Ours)\nLevel Task None sin/cos Relative ALiBi RoPE Learnedsin/cos Relative ALiBi RoPE Learned⋆\nR\nEVENPAIRS 50.4 50.9 96.4 67.3 51.0 50.7 100.0 100.0 81.5 100.0 97.5MODULARARITHMETIC(SIMPLE) 20.1 20.5 21.8 24.2 21.6 20.2 25.7 28.1 21.2 25.5 21.1PARITYCHECK† 51.9 50.5 51.8 51.7 51.3 50.3 52.6 52.2 50.3 52.3 52.6CYCLENAVIGATION† 61.9 26.3 23.0 37.6 23.6 24.2 59.0 58.8 29.8 73.6 49.7\nDCF\nSTACKMANIPULATION 50.3 50.1 53.6 57.5 51.2 49.2 72.8 77.9 70.6 68.2 69.1REVERSESTRING 52.8 50.6 58.3 62.3 51.9 50.7 75.6 95.1 77.1 69.9 52.9MODULARARITHMETIC 31.0 28.3 30.3 32.5 25.1 25.1 33.8 34.9 31.3 32.7 31.9SOLVEEQUATION 20.1 21.0 23.0 25.7 23.1 20.4 24.5 28.1 22.0 24.5 22.1\nCS\nDUPLICATESTRING 52.8 50.7 51.7 51.3 50.9 50.8 72.4 75.1 68.9 68.9 53.0MISSINGDUPLICATE 52.5 51.3 54.0 54.3 56.5 51.0 52.5 100.0 79.7 88.7 52.7ODDSFIRST 52.8 51.6 52.7 51.4 51.3 50.6 65.9 69.3 64.7 65.6 52.7BINARYADDITION 50.1 49.8 54.3 51.4 50.4 49.8 64.4 64.5 56.2 60.2 61.7BINARYMULTIPLICATION 49.9 50.1 52.2 51.0 50.2 49.6 52.1 50.1 50.5 51.7 51.9COMPUTESQRT 50.2 50.1 52.4 50.9 50.5 50.2 52.5 53.3 51.2 52.3 52.0BUCKETSORT† 23.7 30.1 91.9 38.8 30.6 25.9 100.0 100.0 99.6 99.6 99.5\nCS tasks can be solved by an FSA with access to a\nbounded tape. Note that the relation to the Chom-\nsky hierarchy is largely irrelevant for our work and\nonly included for completeness. We evaluate our\nmethod on Delétang et al. (2023)’s benchmark as\nit is currently out of reach for Transformers and\nclearly demonstrates their failure to generalize on\nalgorithmic reasoning tasks. We refer interested\nreaders to the original paper for more details.\nWe consider the encoder-only model of the orig-\ninal seq-to-seq Transformer (Vaswani et al., 2017),\nas used in popular pre-trained language models\nsuch as BERT (Devlin et al., 2019) or Gopher (Rae\net al., 2021). Thus, for tasks that require a multi-\ntoken output sequence y (e.g., duplicating a string),\nwe pad the input sequence with |y|empty tokens\nand compute the entire Transformer output from\nthe padded sequence (i.e., we do not use autoregres-\nsive sampling). We train the model on sequences\nof length sampled uniformly from U(1, N), with\nN = 40, and evaluate it on sequences of length\n{N + 1, . . . , M}, with M = 500. We set the max-\nimum position L = 2048 (and visualize the im-\npact of other values on the performance in Ap-\npendix B.1). We report the accuracy averaged over\nall unseen sequence lengths, i.e., N + 1, . . . , M,\nfor the best-performing model out of 10 differ-\nent parameter initialization seeds and three learn-\ning rates 1 ×10−4, 3 ×10−4, 5 ×10−4. We\nuse the same hyperparameters as Delétang et al.\n(2023) and provide the full experiment setup in\nAppendix A. We make our code publicly avail-\nable at https://github.com/deepmind/\nrandomized_positional_encodings.\nComparison to prior work We compare our\nmethod to a wide range of positional encodings:\nnone, sin / cos (Vaswani et al., 2017), relative (Dai\net al., 2019), ALiBi (Press et al., 2022), RoPE (Su\net al., 2021), learned (Gehring et al., 2017), and\nlabel-based (Li and McClelland, 2022). Note that\nthe label encodings proposed by Li and McClelland\n(2022) are equivalent to randomized learned posi-\ntional encodings and thus subsumed by our method.\nWe instantiate our randomized positional encoding\nscheme with all the above encodings and show the\naverage test accuracy in Table 1 (with performance\ncurves over test lengths in Appendix B.2). We ob-\nserve that our randomized versions significantly\nincrease the test accuracy across most tasks (by\n12.0% on average and up to 43.5%). In particular,\nthe randomized relative encoding solves tasks that\nwere previously out of reach for prior work (e.g.,\nREVERSE STRING or MISSING DUPLICATE ).\nEfficiency comparison We now show that our\nmethod allows us to train a model on short se-\nquences and obtain a test accuracy above 90%,\nroughly 35.4 times faster than the naive approach\nof training a model on longer sequences. To that\nend, we train the randomized relative encodings on\nsequences up to length 40 and the classical relative\npositional encoding (Dai et al., 2019) on sequences\n1892\n0 1000 2000 3000 4000\nTraining time (s)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Average test accuracy\nRandom relative (ours) on U(1, 40)\nRelative on U(1, 500)\nFigure 2: Average accuracy over unseen test lengths\non the MISSING DUPLICATE task over training time\n(seconds) for two models: (i) our randomized relative\npositional encoding with a maximum training sequence\nlength of 40, and (ii) the classical relative positional\nencoding but with a maximum training length of 500.\nup to length 500 and show the test accuracy (aver-\naged over lengths 41 to 500) in Fig. 2 over training\ntime (in seconds). Our model obtains a strong test\naccuracy significantly faster due to the quadratic\ncost (in terms of sequence length) of global atten-\ntion, which means that our model trains at 168.4\nsteps per second compared to 22.1 steps per second\nfor the naive approach (on a NVIDIA V100 GPU).\n5 Conclusion\nWe introduced a novel family of positional encod-\nings that significantly improves the length gener-\nalization capabilities of Transformers. Our po-\nsitional encodings are based on the insight that\nconventional positional encodings will be out-of-\ndistribution when increasing the sequence length.\nThus, to overcome this issue, we randomly sample\nour encodings from a wider range than the lengths\nseen at test time while keeping the order. Our large-\nscale empirical evaluation demonstrates that our\nmethod significantly outperforms prior work in\nterms of length generalization while offering supe-\nrior computational performance over the naive ap-\nproach of training the model on longer sequences.\nLimitations\nWhile our work shows promising results in improv-\ning the generalization capabilities of Transform-\ners to sequences of arbitrary length, some limita-\ntions must be considered. First, our evaluation is\nconfined to synthetic algorithmic reasoning tasks,\nwhich may not fully capture the complexity and\ndiversity of natural language. We focused on syn-\nthetic datasets since they showed clear and some-\nwhat surprising limitations of Transformer architec-\ntures (Delétang et al., 2023). However, the general-\nizability of our approach to other tasks and domains\nremains an open question, and additional research,\nsuch as evaluation on SCAN (Lake and Baroni,\n2018), CFQ (Keysers et al., 2020), COGS (Kim\nand Linzen, 2020), or the Long Range Arena (Tay\net al., 2021), is necessary to understand its potential\nin real-world applications. Second, our approach\nintroduces a new hyperparameter – the maximum\nsequence position L. Although our experiments\nin Appendix B.1 show that our method’s perfor-\nmance is largely unaffected by the precise value of\nL, practitioners may still have to tune the param-\neter depending on their specific problem domains.\nThird, we only isolate and ameliorate one failure\nmode of Transformer length generalization on syn-\nthetic datasets. However, there are other factors\ncontributing to poor length generalization, such\nas attention becoming less peaked for longer se-\nquences (Chiang and Cholak, 2022). Overall, we\nbelieve that our study’s limitations offer several\ninteresting directions for future research.\nAcknowledgements\nWe thank Chris Cundy, Elliot Catt, Kevin Li, Lau-\nrent Orseau, Marcus Hutter, Petar Veliˇckovi´c, Vin-\ncent Dutordoir, and the anonymous reviewers for\ntheir helpful feedback.\nReferences\nJoshua Ackerman and George Cybenko. 2020. A\nsurvey of neural networks and formal languages.\narXiv:2006.01338.\nSatwik Bhattamishra, Kabir Ahuja, and Navin Goyal.\n2020. On the ability and limitations of transformers\nto recognize formal languages. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nDavid Chiang and Peter Cholak. 2022. Overcoming a\ntheoretical limitation of self-attention. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics.\nNoam Chomsky. 1956. Three models for the description\nof language. IRE Trans. Inf. Theory.\nRóbert Csordás, Kazuki Irie, and Jürgen Schmidhuber.\n2021. The devil is in the detail: Simple tricks im-\nprove systematic generalization of transformers. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing.\nRóbert Csordás, Kazuki Irie, and Jürgen Schmidhuber.\n2022. The neural data router: Adaptive control flow\n1893\nin transformers improves systematic generalization.\nIn The Tenth International Conference on Learning\nRepresentations.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a fixed-length context. In Proceedings of\nthe 57th Conference of the Association for Computa-\ntional Linguistics.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations.\nGrégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim\nGenewein, Li Kevin Wenliang, Elliot Catt, Chris\nCundy, Marcus Hutter, Shane Legg, Joel Veness, and\nPedro A. Ortega. 2023. Neural networks and the\nchomsky hierarchy. In The Eleventh International\nConference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies,.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In 9th International Conference\non Learning Representations.\nJavid Ebrahimi, Dhruv Gelda, and Wei Zhang. 2020.\nHow can self-attention networks recognize dyck-n\nlanguages? In Findings of the Association for Com-\nputational Linguistics.\nJeffrey L. Elman. 1990. Finding structure in time. Cogn.\nSci.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings of the\n34th International Conference on Machine Learning.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Trans. Assoc.\nComput. Linguistics.\nYiding Hao, Dana Angluin, and Robert Frank. 2022.\nFormal language recognition by hard attention trans-\nformers: Perspectives from circuit complexity. Trans.\nAssoc. Comput. Linguistics.\nBorja Ibarz, Vitaly Kurin, George Papamakarios, Kyria-\ncos Nikiforou, Mehdi Bennani, Róbert Csordás, An-\ndrew Joseph Dudzik, Matko Bosnjak, Alex Vitvitskyi,\nYulia Rubanova, Andreea Deac, Beatrice Bevilacqua,\nYaroslav Ganin, Charles Blundell, and Petar Velick-\novic. 2022. A generalist neural algorithmic learner.\nIn Learning on Graphs Conference, LoG 2022, 9-12\nDecember 2022, Virtual Event.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang,\nMarc van Zee, and Olivier Bousquet. 2020. Measur-\ning compositional generalization: A comprehensive\nmethod on realistic data. In 8th International Confer-\nence on Learning Representations.\nNajoung Kim and Tal Linzen. 2020. COGS: A compo-\nsitional generalization challenge based on semantic\ninterpretation. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Interna-\ntional Conference on Learning Representations.\nBrenden M. Lake and Marco Baroni. 2018. General-\nization without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks.\nIn Proceedings of the 35th International Conference\non Machine Learning.\nYuxuan Li and James L. McClelland. 2022. Systematic\ngeneralization and emergent structures in transform-\ners trained on structured tasks. arXiv:2210.00400.\nQian Liu, Shengnan An, Jian-Guang Lou, Bei Chen,\nZeqi Lin, Yan Gao, Bin Zhou, Nanning Zheng, and\nDongmei Zhang. 2020a. Compositional generaliza-\ntion by learning analytical expressions. In Advances\nin Neural Information Processing Systems 33.\nXuanqing Liu, Hsiang-Fu Yu, Inderjit S. Dhillon, and\nCho-Jui Hsieh. 2020b. Learning to encode position\nfor transformer with continuous dynamical model. In\nProceedings of the 37th International Conference on\nMachine Learning.\nWilliam Merrill. 2019. Sequential neural networks as\nautomata. arXiv:1906.01615.\nWilliam Merrill and Ashish Sabharwal. 2022. Log-\nprecision transformers are constant-depth uniform\nthreshold circuits. arXiv:2207.00729.\nSantiago Ontañón, Joshua Ainslie, Zachary Fisher, and\nVaclav Cvicek. 2022. Making transformers solve\ncompositional tasks. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In The Tenth International\nConference on Learning Representations.\n1894\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d’Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam S. Isaac, Edward Lockhart, Simon Osindero,\nLaura Rimell, Chris Dyer, Oriol Vinyals, Kareem\nAyoub, Jeff Stanway, Lorrayne Bennett, Demis Hass-\nabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021.\nScaling language models: Methods, analysis & in-\nsights from training gopher. arXiv:2112.11446.\nScott E. Reed, Konrad Zolna, Emilio Parisotto,\nSergio Gómez Colmenarejo, Alexander Novikov,\nGabriel Barth-Maron, Mai Gimenez, Yury Sulsky,\nJackie Kay, Jost Tobias Springenberg, Tom Eccles,\nJake Bruce, Ali Razavi, Ashley Edwards, Nicolas\nHeess, Yutian Chen, Raia Hadsell, Oriol Vinyals,\nMahyar Bordbar, and Nando de Freitas. 2022. A\ngeneralist agent. Trans. Mach. Learn. Res.\nRyoma Sato, Makoto Yamada, and Hisashi Kashima.\n2021. Random features strengthen graph neural net-\nworks. In Proceedings of the 2021 SIAM Interna-\ntional Conference on Data Mining.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\nLiu. 2021. Roformer: Enhanced transformer with\nrotary position embedding. arXiv:2104.09864.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen,\nDara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. 2021. Long\nrange arena : A benchmark for efficient transform-\ners. In 9th International Conference on Learning\nRepresentations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30.\n1895\nA Experimental Details\nWe use the experiment suite proposed by Delétang\net al. (2023), which consists of 15 algorith-\nmic reasoning tasks and is publicly available\nat https://github.com/deepmind/\nneural_networks_chomsky_hierarchy\nunder the Apache 2.0 License. The tasks do not\nconsist of fixed-size datasets but define training\nand testing distributions from which one can\nsample continuously. We train the models for\n2 000 000steps with a batch size of 128, which cor-\nresponds to 256 000 000(potentially non-unique)\ntraining examples. At test time, we evaluate\na single batch of size 500 for every sequence\nlength in {41, . . . ,500}, which corresponds to\n230 000testing examples. We use the Adam\noptimizer (Kingma and Ba, 2015) with gradient\nclipping and sweep over three learning rates:\n1 ×10−4, 3 ×10−4, and 5 ×10−4. Furthermore,\nfor each task and positional encoding, we use 10\ndifferent parameter initialization random seeds.\nWe consider the encoder-only Transformer ar-\nchitecture (Vaswani et al., 2017), with 5 blocks\nof 8 heads each and dmodel = 64, which cor-\nresponds to 249 026parameters ( 270 146in the\ncase of relative and randomized relative posi-\ntional encodings). We run every task-encoding-\nhyperparameter triplet on a single NVIDIA V100\nGPU from our internal cluster. As a result,\nwe used 15 (tasks)·13 (positional encodings)·\n3 (learning rates)·10 (seeds) = 5850GPU-units\nfor the results in Tables 1, 4 and 5 and Fig. 4.\nFor the results in Fig. 2, we used an additional\n2 (positional encodings)·3 (learning rates)·\n10 (seeds) = 60GPU-units. Finally, for Fig. 3, we\nused 4 (maximum positions)·3 (learning rates)·\n10 (seeds) = 120GPU-units, yielding a grand to-\ntal of 6030 GPU-units. We report all running times\nin Table 2 and observe that our method induces a\nnegligible computational overhead.\nB Additional Results\nB.1 Ablation Study\nIn this section, we conduct an ablation study over\nthe two main components of our method: (i) the\nmaximum sampling position L, and (ii) the sorting\nof the subsampled positions.\nWe train the randomized relative positional en-\ncoding for a wide range of different maximum po-\nsitions L: 1024, 2048, 4096, and 8192. Figure 3\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n1024\n2048\n4096\n8192\n(a) REVERSE STRING (DCF)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n1024\n2048\n4096\n8192\n(b) MISSING DUPLICATE (CS)\nFigure 3: Sweep over the maximum position L for our\nrandomized relative positional encodings. The test accu-\nracy (averaged over unseen sequence lengths) is largely\nunaffected by the concrete value of L (for reasonably\nsmall values of L), showing the stability of our method.\nHowever, if L is much larger than the maximum train-\ning (N) or testing (M) sequence length, we expect the\nperformance to degrade since it the model is unlikely to\nencounter enough unique indices during training time.\nshows that the test accuracy (averaged over all un-\nseen sequence lengths) is largely unaffected by the\nvalue of L on the REVERSE STRING and MISSING\nDUPLICATE tasks. As a consequence, a practi-\ntioner wanting to apply our method will not have\nto carry out extensive tuning of this parameter (as\nlong as it is larger than the maximum evaluation\nsequence length M, but not unreasonably large).\nNext, we investigate the performance of our ran-\ndomized sin / cos positional encoding with and\nwithout sorting of the subsampled positions. Note\nthat this experiment is meant as a “sanity-check”\nsince we do not expect the Transformer to perform\nwell without order information. Table 3 shows the\ntest accuracy (averaged over all unseen sequence\nlengths) for the two versions of our method. We\nobserve that sorting the positions is crucial, as it\nincreases the test accuracy by 15.7% on average\n1896\nTable 2: Mean and standard deviation of the running times (in hours) for all the positional encodings and tasks.\nRandomized (Ours)\nLevel Task None sin/cos Relative ALiBi RoPE Learnedsin/cos Relative ALiBi RoPE Learned⋆\nR\nPARITYCHECK† 0.86±0.17 0.87±0.17 1.63±0.28 0.87±0.17 1.41±0.24 0.90±0.18 0.92±0.18 1.75±0.29 0.94±0.19 1.66±0.31 1.12±0.23REVERSESTRING 1.17±0.21 1.18±0.22 2.61±0.39 1.17±0.22 2.01±0.35 1.23±0.23 1.24±0.23 2.75±0.41 1.27±0.24 2.42±0.43 1.62±0.32CYCLENAVIGATION† 0.86±0.17 0.87±0.17 1.62±0.27 0.86±0.17 1.41±0.25 0.91±0.18 0.92±0.18 1.75±0.29 0.94±0.19 1.66±0.31 1.12±0.22EVENPAIRS 0.86±0.17 0.87±0.17 1.63±0.27 0.86±0.17 1.41±0.24 0.91±0.18 0.92±0.18 1.75±0.29 0.95±0.19 1.65±0.31 1.12±0.22\nDCF\nSTACKMANIPULATION 8.09±0.97 8.00±0.82 9.50±0.89 8.07±0.94 8.87±0.84 8.46±0.84 8.47±0.88 10.04±0.96 8.55±0.90 10.61±1.58 9.58±1.12MODULARARITHMETIC 5.48±0.63 5.55±0.67 6.32±0.81 5.50±0.65 6.07±0.69 5.69±0.65 5.66±0.64 6.56±0.70 5.69±0.65 6.41±0.84 5.92±0.80BINARYMULTIPLICATION1.83±0.33 1.83±0.30 2.86±0.43 1.84±0.31 2.32±0.39 2.24±0.35 2.23±0.35 3.13±0.43 2.24±0.35 3.21±0.51 2.88±0.46BINARYADDITION 1.83±0.32 1.82±0.31 2.89±0.42 1.81±0.32 2.34±0.39 2.22±0.35 2.22±0.35 3.17±0.44 2.24±0.35 3.29±0.62 2.90±0.49\nCS\nBINARYADDITION 1.83±0.32 1.82±0.31 2.89±0.42 1.81±0.32 2.34±0.39 2.22±0.35 2.22±0.35 3.17±0.44 2.24±0.35 3.29±0.62 2.90±0.49COMPUTESQRT 1.39±0.24 1.40±0.25 2.20±0.34 1.40±0.25 1.86±0.30 1.73±0.29 1.72±0.29 2.43±0.37 1.74±0.30 2.53±0.41 2.23±0.38SOLVEEQUATION 5.60±0.65 5.60±0.67 6.41±0.68 5.63±0.66 6.14±0.68 5.74±0.65 5.78±0.66 6.69±0.76 5.83±0.69 6.50±0.80 6.01±0.84DUPLICATESTRING 1.58±0.28 1.59±0.28 4.10±0.54 1.58±0.27 2.71±0.40 1.64±0.28 1.65±0.29 4.24±0.54 1.67±0.29 3.18±0.49 2.05±0.38MODULARARITHMETIC(SIMPLE) 0.99±0.19 1.00±0.19 1.74±0.29 0.99±0.19 1.51±0.26 1.03±0.20 1.05±0.20 1.87±0.31 1.06±0.21 1.74±0.31 1.23±0.23MISSINGDUPLICATE 0.88±0.17 0.90±0.18 1.64±0.27 0.88±0.17 1.43±0.26 0.93±0.19 0.94±0.19 1.78±0.30 0.97±0.19 1.66±0.30 1.15±0.23ODDSFIRST 1.17±0.22 1.19±0.22 2.61±0.38 1.17±0.22 2.00±0.31 1.23±0.23 1.24±0.23 2.74±0.40 1.26±0.23 2.40±0.39 1.59±0.29BUCKETSORT† 1.17±0.23 1.18±0.22 2.61±0.43 1.16±0.22 2.01±0.34 1.22±0.23 1.24±0.23 2.74±0.40 1.25±0.23 2.40±0.41 1.60±0.30\nTable 3: Accuracy (in percentage) averaged over all test\nlengths and maximized over 10 seeds and 3 learning\nrates for our randomized sin / cos positional encoding\nwith and without sorting of the subsampled positions.\nRandomizedsin/cos\nLevel Task w/o Sorting w/ Sorting\nR\nEVENPAIRS 50.4 100.0\nMODULARARITHMETIC(SIMPLE) 20.0 25.7\nPARITYCHECK† 52.2 52.6\nCYCLENAVIGATION† 59.3 59.0\nDCF\nSTACKMANIPULATION 50.4 72.8\nREVERSESTRING 52.8 75.6\nMODULARARITHMETIC 31.0 33.8\nSOLVEEQUATION 20.2 24.5\nCS\nDUPLICATESTRING 52.8 72.4\nMISSINGDUPLICATE 53.1 52.5\nODDSFIRST 52.8 65.9\nBINARYADDITION 50.0 64.4\nBINARYMULTIPLICATION 49.9 52.1\nCOMPUTESQRT 50.2 52.5\nBUCKETSORT† 23.7 100.0\nand up to 76.3% on certain tasks. In fact, without\nsorting, our approach fails to beat the (baseline) ran-\ndom accuracy on all but the CYCLE NAVIGATION\ntask, which is permutation-invariant (i.e., it can\nbe solved without positional information). This\nconfirms our intuition that the Transformer only\nneeds to know the relative order of the positional\nencodings (and not their exact values), but that it\nfails to solve tasks when presented with positional\nencodings whose order does not correspond to the\ntokens’ positions.\nB.2 Comparison to Prior Work\nIn Section 4, we compared our method to\na wide range of positional encodings: none,\nsin / cos (Vaswani et al., 2017), relative (Dai et al.,\n2019), ALiBi (Press et al., 2022), RoPE (Su et al.,\n2021), learned (Gehring et al., 2017), and label-\nbased (Li and McClelland, 2022). Here, we pro-\nvide additional results for these experiments, as\nwell as a comparison to the geometric attention and\ndirectional encodings of Csordás et al. (2022).\nWe recall that Table 1 showed the test accuracy\nmaximized over the 10 parameter initialization\nseeds and the three different learning rates. We\nreported the maximum following the experiment\nsetup in Delétang et al. (2023), which investigates\nwhether an architecture is capable of solving a task\nat all (and not on average). However, we also re-\nport the means and standard deviations (over the\nrandom seeds) in Table 4 for the best-performing\nlearning rate. We observe that our randomized posi-\ntional encoding also significantly outperform their\noriginal counterparts on average. We visualize the\ntest accuracy per sequence length in Fig. 4.\nWe highlight the case of learned positional en-\ncodings, which fail to beat the random accuracy\nbaseline (cf. Tables 1 and 4). This is because the\ncolumns of the embedding matrix corresponding\nto the positions that are larger than the maximum\ntraining length N are not learned during training\nand are thus entirely random. In contrast, our ran-\ndomized version of the learned encodings consid-\ners all possible embedding columns during training\nand thus achieves non-trivial to strong length gen-\neralization on most tasks.\nFinally, we also compare our method to a variant\nof the Neural Data Router (NDR) (Csordás et al.,\n2022), which was developed to improve the sys-\ntematic generalization capabilities of Transformers.\nWe only consider the most related aspects of the\nNDR architecture, i.e., the geometric attention and\nthe directional encoding (we do not use gating or\nshared layers). Table 5 compares the test accuracy\nof geometric attention and directional encodings\n1897\nTable 4: Means and standard deviations (computed over random seeds) of the score (accuracy averaged over all\ntest lengths) for the results of the main experiment (see Table 1). The random accuracy is 50%, except for CYCLE\nNAVIGATION , BUCKET SORT, and the modular arithmetic tasks, where it is 20%. We denote permutation-invariant\ntasks, which can be solved without positional information, with †. Numbers in bold are the best performers, per task.\nThese results underline the superiority of our method, and especially when applied to relative positional encodings.\nRandomized (Ours)\nLevel Task None sin/cos Relative ALiBi RoPE Learnedsin/cos Relative ALiBi RoPE Learned⋆\nR\nEVENPAIRS 50.1±0.1 50.4±0.2 67.6±15.3 59.8±3.2 50.4±0.3 50.4±0.2 99.7±0.3 99.6±0.6 71.4±5.6 100.0±0.0 96.2±0.7MODULARARITHMETIC(SIMPLE) 20.0±0.0 20.2±0.2 20.7±0.5 23.2±0.9 20.8±0.5 20.1±0.1 24.2±1.4 24.9±1.7 20.8±0.3 23.5±1.6 20.2±0.4PARITYCHECK† 50.4±0.8 50.3±0.2 50.4±0.6 50.5±0.6 50.4±0.4 50.0±0.1 51.1±1.3 51.4±0.5 50.0±0.2 50.4±1.0 50.6±0.9CYCLENAVIGATION† 33.9±10.5 23.8±1.4 21.7±0.8 31.1±3.8 22.3±0.9 21.0±1.2 30.3±10.7 45.9±9.9 26.3±2.4 52.9±15.3 31.9±8.2\nDCF\nSTACKMANIPULATION 50.2±0.1 47.3±1.9 50.1±3.3 51.0±8.0 49.6±3.0 44.9±3.7 69.2±3.2 71.7±4.7 69.5±1.1 66.0±2.0 66.1±2.5REVERSESTRING 52.7±0.1 50.4±0.1 54.2±1.5 56.3±2.6 51.2±0.3 50.4±0.2 72.9±1.6 77.1±6.6 75.1±1.3 67.7±1.1 52.7±0.2MODULARARITHMETIC 31.0±0.1 24.3±2.2 26.1±2.0 28.1±3.4 24.0±2.4 22.3±1.5 29.6±4.6 28.8±5.5 29.3±1.6 28.6±3.9 30.3±2.6SOLVEEQUATION 20.1±0.0 20.9±0.2 21.9±0.7 23.6±1.9 21.9±0.6 20.2±0.2 23.6±0.5 25.4±1.8 21.1±0.7 22.3±1.6 21.1±0.7\nCS\nDUPLICATESTRING 52.7±0.1 50.4±0.2 51.0±0.4 51.0±0.2 50.4±0.2 50.4±0.2 69.0±2.9 73.1±1.5 67.9±1.4 67.1±2.0 52.8±0.1MISSINGDUPLICATE 51.4±1.0 50.1±0.6 51.1±1.1 53.5±0.4 53.9±1.6 50.1±0.4 50.4±1.5 91.4±9.8 75.2±3.4 73.2±1.2 51.2±1.4ODDSFIRST 52.7±0.1 51.3±0.2 51.5±0.5 51.1±0.2 50.8±0.2 50.5±0.1 62.5±2.0 65.9±1.6 62.2±1.4 62.9±1.3 52.7±0.1BINARYADDITION 49.4±0.3 47.3±3.8 51.7±1.3 48.5±3.6 47.8±5.4 48.9±0.8 61.2±1.7 62.0±1.1 54.3±1.5 57.4±1.2 59.9±1.3BINARYMULTIPLICATION 49.8±0.0 48.8±1.0 50.2±3.5 49.9±2.3 49.6±0.6 48.7±1.7 51.8±0.2 39.1±7.1 49.2±1.2 45.7±6.6 51.6±0.2COMPUTESQRT 50.2±0.0 50.1±0.0 51.5±0.4 50.5±0.2 50.3±0.1 50.1±0.1 51.9±0.5 52.4±0.6 51.1±0.1 51.8±0.3 51.0±0.8BUCKETSORT† 23.7±0.0 25.6±2.6 83.4±6.6 29.3±6.7 23.6±3.8 20.7±2.9 99.3±0.4 99.4±0.3 98.8±0.7 99.3±0.3 98.9±0.4\nTable 5: Accuracy (in %) averaged over all test lengths\nfor geometric attention with directional encoding.\nMax Avg ±SD\nLevel Task Table 1 Geometric Table 4 Geometric\nR\nEVENPAIRS 100.0 100.0100.0±0.0 94.5±8.8MODULARARITHMETIC(SIMPLE) 28.1 43.624.9±1.7 27.2±8.2PARITYCHECK† 52.6 52.451.4±0.5 51.6±0.6CYCLENAVIGATION† 73.6 41.352.9±15.3 32.9±4.7\nDCF\nSTACKMANIPULATION 77.9 58.371.7±4.7 55.6±2.3REVERSESTRING 95.1 65.277.1±6.6 59.3±3.2MODULARARITHMETIC 34.9 36.530.3±2.6 32.8±2.8SOLVEEQUATION 28.1 31.725.4±1.8 28.5±2.0\nCS\nDUPLICATESTRING 75.1 58.673.1±1.5 54.9±1.6MISSINGDUPLICATE 100.0 64.491.4±9.8 60.3±2.3ODDSFIRST 69.3 64.265.9±1.6 58.1±2.6BINARYADDITION 64.5 54.962.0±1.1 53.5±1.5BINARYMULTIPLICATION 50.1 53.651.8±0.2 52.1±2.5COMPUTESQRT 53.3 54.152.4±0.6 52.3±0.9BUCKETSORT† 100.0 78.399.5±0.3 57.7±11.4\nwith the best results from Table 1 (for the maxi-\nmum) and Table 4 (for the mean). We observe that\nour randomized positional encodings outperform\nthe geometric attention overall (with a 9.7% higher\nmaximum test accuracy on average) but not on all\ntasks. In particular, geometric attention performs\nsubstantially better on MODULAR ARITHMETIC\n(SIMPLE ), which has an inherent locality bias, i.e.,\nnumbers closer to the operation symbols are gen-\nerally more relevant, which can be captured by\n“radiating outwards” as geometric attention does.\nB.3 Analysis\nAnalyzing the activations As illustrated in\nFig. 1, the main intuition behind our random-\nized encodings is that they do not lead to out-\nof-distribution activations when evaluating on se-\nquences longer than the maximal training length.\nWe confirm this intuition in our analysis in Fig. 5,\nwhich shows a 2D projection of activations onto the\nfirst two principal components when evaluating on\nsequences of length 40 (i.e., the maximum training\nlength N, shown in blue) and length 150 (i.e., the\ngeneralization regime, shown in orange), using the\nsame transformation. While the activations of our\nrandomized relative encoding strongly overlap for\nthe training and the generalization regimes in all\nlayers, the standard relative encoding leads to out-\nof-distribution activations for sequence length 150\nin layers 3 and 4. We obtained qualitatively similar\nresults for the sin / cos and learned encodings.\nTo compute the results in Fig. 5, we generated\n30 sequences of length 40 and 150 respectively,\non the REVERSE STRING task and passed them\nthrough a well-trained model with either relative\nor randomized relative encodings. For each layer\nshown, we fitted a (non-whitened) 2D PCA on the\nactivations obtained from sequence length 40 and\nprojected all activations from sequence length 150\ninto two dimensions using the same transforma-\ntions (yielding 30 ×40 and 30 ×150 activation-\ndatapoints per layer). The random relative encod-\ning (our method) attains an average accuracy of1.0\nand 0.994 on the 30 sequences of length 40 and\n150, respectively. The standard relative encoding\n(the baseline) attains an average accuracy of 1.0 on\nsequence-length 40 and 0.596 on length 150, indi-\ncating the model’s failure to generalize well under\nthe standard relative encoding.\nAnalyzing the attention matrices We also ana-\nlyze the attention matrices learned with the relative\npositional encoding and our corresponding random-\n1898\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\n(a) EVEN PAIRS (R)\n0 100 200 300 400 500\nSequence length\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (b) MODULAR ARITHMETIC (SIMPLE ) (R)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy (c) PARITY CHECK (R)\n0 100 200 300 400 500\nSequence length\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n(d) CYCLE NAVIGATION (R)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy (e) STACK MANIPULATION (DCF)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy (f) REVERSE STRING (DCF)\n0 100 200 300 400 500\nSequence length\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\n(g) MODULAR ARITHMETIC (DCF)\n0 100 200 300 400 500\nSequence length\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy\nNONE\nSIN_COS\nRELATIVE\nALIBI\nROTARY\nLEARNT\nNOISY_SIN_COS\nNOISY_RELATIVE\nNOISY_ALIBI\nNOISY_ROTARY\nNOISY_LEARNT (h) SOLVE EQUATION (DCF)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy (i) DUPLICATE STRING (CS)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\n(j) MISSING DUPLICATE (CS)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy (k) ODDS FIRST (CS)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy (l) BINARY ADDITION (CS)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy\n(m) BINARY MULTIPLICATION (CS)\n0 100 200 300 400 500\nSequence length\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0Accuracy (n) COMPUTE SQRT (CS)\n0 100 200 300 400 500\nSequence length\n0.2\n0.4\n0.6\n0.8\n1.0Accuracy (o) BUCKET SORT (CS)\nFigure 4: Performance curves on all tasks for all the positional encodings. The dashed vertical red line is the\ntraining range, meaning that sequences to the right have not been seen during training and thus measure length\ngeneralization. The sequences to the left of the dashed line visualize the in-domain generalization performance.\n1899\n(a) Relative positional encoding (Dai et al., 2019).\n(b) Randomized relative positional encoding (ours).\nFigure 5: 2D PCA projections of the activations of the initial embeddings and the encoder layers for30 sequences on\nthe REVERSE STRING task. For sequence-lengths beyond the training length (shown in orange), the standard relative\nencoding clearly leads to out-of-distribution activations for layers 3 and 4 compared to those obtained with the\nmaximum training length (shown in blue). In contrast, our randomized version does not lead to out-of-distribution\nactivations for sequences longer than the maximum training length, confirming the intuition in Fig. 1.\nized version on the REVERSE STRING task. To that\nend, we follow Csordás et al. (2022) and visualize\nthe maximum over the 8 attention matrices (one\nper head) for each of the 5 layers in Fig. 6. We\ncompare the attention matrices for sequences of\nlength 40 (i.e., the maximum training length) and\n150 (i.e., significantly longer than the maximum\ntraining length). For length 40, both encodings pro-\nduce a noticeable X pattern, which corresponds to\nthe reversal of the string. However, for length 150,\nthe pattern only remains visible for our randomized\nencodings while it breaks down for the original\nversion, indicating the failure to generalize.\n1900\nLayer 1\n Layer 2\n Layer 3\n Layer 4\n Layer 5\n(a) Relative (baseline) with a sequence of length 40 (in-distribution).\nLayer 1\n Layer 2\n Layer 3\n Layer 4\n Layer 5\n(b) Relative (baseline) with a sequence of length 150 (out-of-distribution).\nLayer 1\n Layer 2\n Layer 3\n Layer 4\n Layer 5\n(c) Randomized relative (our method) with a sequence of length 40 (in-distribution).\nLayer 1\n Layer 2\n Layer 3\n Layer 4\n Layer 5\n(d) Randomized relative (our method) with sequence of length 150 (out-of-distribution).\nFigure 6: Analysis of the attention matrices for the relative and randomized relative positional encodings on the\nREVERSE STRING task using sequences of length 40 (i.e., maximum training length) and 150 (i.e., beyond training\nlengths). We visualize the maximum over the 8 heads per layer (following Csordás et al., 2022) and observe a clear\nX pattern, which corresponds to the reversal of the sequence. Our randomized relative encodings maintain that\npattern on longer sequences, while it breaks down for the standard relative encoding.\n1901\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations section\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4 and Appendix A\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4 and Appendix A\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nAppendix A\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 4 and Appendix A\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4 and Appendix A\nC □\u0013 Did you run computational experiments?\nSection 4 and Appendix B\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4 and Appendix A\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1902\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4 and Appendices A and B\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAppendix B\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1903",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5943275690078735
    },
    {
      "name": "Generalization",
      "score": 0.5580344200134277
    },
    {
      "name": "Computer science",
      "score": 0.49779605865478516
    },
    {
      "name": "Library science",
      "score": 0.364470899105072
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3272375464439392
    },
    {
      "name": "Mathematics",
      "score": 0.2828575372695923
    },
    {
      "name": "Engineering",
      "score": 0.14348244667053223
    },
    {
      "name": "Electrical engineering",
      "score": 0.10603803396224976
    },
    {
      "name": "Mathematical analysis",
      "score": 0.08071494102478027
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2614128279",
      "name": "Dalle Molle Institute for Artificial Intelligence Research",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I15196421",
      "name": "University of Applied Sciences and Arts of Southern Switzerland",
      "country": "CH"
    }
  ]
}