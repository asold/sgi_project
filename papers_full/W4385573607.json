{
  "title": "Transformer Language Models without Positional Encodings Still Learn Positional Information",
  "url": "https://openalex.org/W4385573607",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2745317158",
      "name": "Adi Guila Haviv",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2917746027",
      "name": "Ori Ram",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2765070949",
      "name": "Ofir Press",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2883286745",
      "name": "Peter Izsak",
      "affiliations": [
        "Meta (Israel)"
      ]
    },
    {
      "id": "https://openalex.org/A2250897584",
      "name": "Omer Levy",
      "affiliations": [
        "Meta (Israel)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2969945254",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035691519",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2951008357"
  ],
  "abstract": "Causal transformer language models (LMs), such as GPT-3, typically require some form of positional encoding, such as positional embeddings. However, we show that LMs without any explicit positional encoding are still competitive with standard models and that this phenomenon is robust across different datasets, model sizes, and sequence lengths.Probing experiments reveal that such models acquire an implicit notion of absolute positions throughout the network, effectively compensating for the missing information.We conjecture that causal attention enables the model to infer the number of predecessors that each token can attend to, thereby approximating its absolute position.Our findings indicate that causal LMs might derive positional awareness not only from the explicit positioning mechanism but also from the effects of the causal mask.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 1382–1390\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nTransformer Language Models without Positional Encodings\nStill Learn Positional Information\nAdi Havivτ Ori Ramτ Ofir Pressω Peter Izsakι Omer Levyτµ\nτ Tel Aviv University\nω University of Washington\nι Intel Labs\nµ Meta AI\nAbstract\nCausal transformer language models (LMs),\nsuch as GPT-3, typically require some form\nof positional encoding, such as positional em-\nbeddings. However, we show that LMs with-\nout any explicit positional encoding are still\ncompetitive with standard models, and that this\nphenomenon is robust across different datasets,\nmodel sizes, and sequence lengths. Probing\nexperiments reveal that such models acquire an\nimplicit notion of absolute positions throughout\nthe network, effectively compensating for the\nmissing information. We conjecture that causal\nattention enables the model to infer the num-\nber of predecessors that each token can attend\nto, thereby approximating its absolute position.\nOur findings indicate that causal LMs might\nderive positional awareness not only from the\nexplicit positioning mechanism, but also from\nthe effects of the causal mask.\n1 Introduction\nThe attention mechanism (Bahdanau et al., 2015)\nof the transformer (Vaswani et al., 2017) is agnos-\ntic to the position and order of tokens in the input\nsequence. It is therefore common practice to in-\nject positional information via absolute positional\nembeddings (Vaswani et al., 2017; Radford et al.,\n2018) or relative bias factors (Shaw et al., 2018;\nRaffel et al., 2020; Press et al., 2022). Here, we\ndemonstrate that transformer language modelswith-\nout any explicit positional information can and do\nlearn an implicit notion of absolute positions that\nis sufficient to achieve competitive performance.\nWe compare the performance of language mod-\nels trained with no explicit positional informa-\ntion ( NoPos language models) to those trained\nwith three different position-aware mechanisms,\nnamely: sinusoidal embeddings (Vaswani et al.,\n2017), learned embeddings (Gehring et al., 2017),\nand ALiBi (Press et al., 2022). Results show that\nNoPos models are competitive with position-aware\nNoPos Learned Sinusoidal ALiBi\n10\n11\n12\n13\n14Perplexity\n13.10 13.05 12.93\n12.51\nFigure 1: Transformer language models trained without\nexplicitly encoding positional information (NoPos) ap-\nproach the performance of models trained with various\npositional encoding methods. All models have 1.3B\nparameters, and are trained on an excerpt of the Pile.\nmodels consistently across datasets, model sizes,\nand input sequence lengths (e.g., Figure 1).\nTo shed light on our findings, we probe into\nthe position-awareness of NoPos language models,\ncompared to models that use relative or absolute\nposition mechanisms. Specifically, we train classi-\nfiers to predict the position of a token given its rep-\nresentation across different layers in the network.\nOur probes reveal that the NoPos model achieves\nsimilar mean absolute distance between the pre-\ndicted and the expected positions, as a model with\nlearned absolute position embeddings.\nWe hypothesize that this surprising behavior is\ntied to the causal attention mask, which implicitly\ninjects positional information into the self-attention\nlayer in order to preserve the autoregressive nature\nof language models. Intuitively, a model that is\nable to count the predecessors of a given token\ncan essentially infer its absolute position. To test\n1382\nour hypothesis, we run similar experiments for\nmasked language models (MLM) (Devlin et al.,\n2019), which use order-invariant attention (since\nno causal mask is applied). Indeed, bidirectional\nmodels fail to converge when position information\nis absent, substantiating our hypothesis. To con-\nclude, our main contributions are:\n• We demonstrate the robustness of the NoPos\nmodel (compared to position-aware models)\nwith respect to model size, dataset and se-\nquence length.\n• We provide an analysis of the trained NoPos\nmodel, and show that it encoded absolute po-\nsitions.\n• We show that the success of NoPos models is\nunique to causal language models.\n2 Positional Encodings\nTransformer models consist of interleaved self-\nattention and feed-forward layers, which are both\norder-invariant. Therefore, to convey the order of\nthe input tokens, some form of positional informa-\ntion is explicitly introduced into the model. Abso-\nlute positions are commonly encoded as vectors\n(one for each position), which are then added to\nthe input tokens’ embeddings and fed to the first\nlayer of the transformer. Relative positionsare typ-\nically encoded as biases (added to attention scores)\nwithin the self-attention layers. In this work, we\nconsider three popular methods as baselines:\nLearned. Embeddings trained to represent abso-\nlute positions (Sukhbaatar et al., 2015; Gehring\net al., 2017). Learned positional embeddings are\ncommonly used in MLMs (Devlin et al., 2019; Liu\net al., 2019) as well as in large autoregressive lan-\nguage models, such as GPT-3 (Brown et al., 2020).\nSinusoidal. Constant vectors computed by a non-\nparametric function of the input token’s absolute\nposition. Sine and cosine functions of different\nfrequencies are used, such that each dimension\nof the positional encoding corresponds to a sinu-\nsoid. Sinusoidal embeddings were introduced in\nVaswani et al. (2017) for machine translation, and\nare also used in language modeling (Baevski and\nAuli, 2019).\nALiBi. Attention with LInear BIases (Press et al.,\n2022) injects information about the relative dis-\ntances between tokens by adding negative biases\nto attention scores, which grow linearly with the\ndistance between each pair of tokens.\n3 Experiment Setup\nIntuitively, encoding positional information explic-\nitly is crucial for enabling transformer language\nmodels to predict the next token in a sequence. To\ntest this intuition, we compared the validation set\nperplexity of models trained from scratch with no\nexplicit positional information (denoted as NoPos)\nto those trained with the various positional encod-\ning methods discussed in Section 2. We investi-\ngated the canonical WikiText-103 setting (Merity\net al., 2017; Baevski and Auli, 2019), as well as a\nnewer, large-scale setting based on the Pile corpus\n(Gao et al., 2020) on model architectures inspired\nby Brown et al. (2020), where we cover a spectrum\nof models sizes and sequence lengths.\nThe Canonical Setting (WikiText-103). The\nWikiText-103 corpus (Merity et al., 2017) consists\nof over 100 million words extracted from a set\nof high-quality Wikipedia articles. The corpus is\ntokenized at the word level, resulting in a vocab-\nulary of over 267K tokens. For this corpus, we\nused the adaptive embedding transformer model of\nBaevski and Auli (2019), which contains 16 trans-\nformer layers with 1024 model dimensions, 4096\nfeed-forward dimensions, and 8 attention heads.\nOverall, this model has 247M parameters in total.\nWe trained with their exact optimization hyperpa-\nrameters, as implemented in fairseq (Ott et al.,\n2019), with the exception of the input sequence\nlength, which was shortened to 512 tokens (instead\nof 3072), as in Press et al. (2022). See App. C for\ndetailed hyperparameters.\nThe Large-Scale Setting (The Pile). The Pile\n(Gao et al., 2020) is an 800GB English text dataset\ncomposed of Common Crawl and 22 other diverse\nsources. For our experiments, we used 2 out of\n30 shards;1 of these, we filtered out the GitHub\nand DM Mathematics sources and removed the\nshortest 1% and longest 1% of examples from each\nsource to reduce noise. We used GPT-2’s tokenizer\n(Radford et al., 2019) to convert the text into token\nsequences over a vocabulary of 50K tokens. We\nrandomly sampled a validation set of 2000 doc-\numents (2.6M tokens) from the corpus, while the\nremaining 15M documents (21B tokens) comprised\n1Shards 00 and 01 can be downloaded from: https://\nthe-eye.eu/public/AI/pile/train/\n1383\nWikiText-103 The Pile\nNoPos 20.97 13.10\nLearned 20.42 13.05\nSinusoidal 20.16 12.93\nALiBi 19.71 12.51\nTable 1: Validation set perplexity of transformer lan-\nguage models trained with various positional encoding\nmethods. The WikiText-103 setting (Merity et al., 2017)\nuses the model of Baevski and Auli (2019) on sequences\nof 512 tokens, while the Pile settings (Gao et al., 2020)\nuses a more recent 1.3B parameter architecture (Brown\net al., 2020) over 1024 token sequences.\nthe training set. The baseline model in this setting\nfollows the 1.3B parameter architecture of Brown\net al. (2020), also known as GPT-3 XL: 24 trans-\nformer layers with 2048 model dimensions, 8192\nfeed-forward dimensions, and 32 attention heads.\nThe default input sequence length is 1024 tokens.\nWe refer to App.C for detailed hyperparameters.\nTo demonstrate the consistency of our results\nin different settings, we perform two scaling ex-\nperiments. We first scale the model size by ex-\nperimenting with the small (125M parameters),\nmedium (350M parameters), large (760M parame-\nters) and the XL (1.3B parameters) variants of the\nBrown et al. (2020) architecture on the Pile set-\ntings. In addition, we evaluate the effect of varying\nthe sequence length using the XL (1.3B parameter)\nmodel. Specifically, we experiment with sequences\nof lengths {256, 512, 1024, 2048}.\nLast, to shed additional light on differences be-\ntween the NoPos model to other methods, we com-\npare the model’s performance on different parts of\nthe sequence. Details of this analysis and results\nare given in App. A.\n4 Results\nTable 1 compares the performance of training LMs\nwith different position encoding methods. We ob-\nserve that NoPos LMs approach the performance of\nthe other models, with gaps of 0.55 (WikiText-103)\nand 0.05 (the Pile) perplexity from models with\nlearned positional embeddings. In the Pile setting,\nperformance differences between NoPos, Learned,\nand Sinusoidal are small both in absolute terms\nand with respect to their difference with ALiBi. In\nthe WikiText-103 setting, performance gaps are\nwider but still modest with respect to random seed\nvariance.2 These results strongly suggest that train-\ning transformer language models without explicit\npositional encoding is indeed possible.\nTable 2 explores the effects of scaling the num-\nber of parameters in the Pile setting. While smaller\nmodels benefit from fixed, non-parametric posi-\ntional encodings (Sinusoidal and ALiBi), these per-\nformance gaps narrow in larger models. Table 3\nshows the effect of varying the sequence length\nin the same setting. In this experiment, the gaps\nbetween NoPos, Learned, and Sinusoidal remain\nalmost constant, while the benefit of using ALiBi\nincreases as sequences become longer. Overall, we\nshow that transformer language modeling without\nexplicit positional encoding is robust to the selec-\ntion of corpus, model size, and sequence length.\nAs training models at the 1.3B parameter scale is\nresource-intensive, we publicly release our trained\nmodels for future research and analysis.3\nModel Size 125M 350M 760M 1.3B\nNoPos 22.15 16.87 14.29 13.10\nLearned 22.04 16.84 14.21 13.05\nSinusoidal 21.49 16.58 14.04 12.93\nALiBi 19.94 15.66 13.53 12.51\nTable 2: Validation set perplexity on the Pile, as a func-\ntion of positional encoding method and model size. All\nmodels operate on sequences of 1024 tokens. Smaller\nmodels benefit from fixed, non-parametric positional en-\ncodings (Sinusoidal and ALiBi), but these performance\ngaps diminish as the models scale up.\nSeq Length 256 512 1024 2048\nNoPos 14.98 13.82 13.10 12.87\nLearned 14.94 13.77 13.05 12.72\nSinusoidal 14.84 13.66 12.93 12.62\nALiBi 14.65 13.37 12.51 12.06\nTable 3: Validation set perplexity on the Pile, as a func-\ntion of positional encoding method and sequence length.\nAll models have 1.3B parameters. The performance dif-\nferences between NoPos, Learned, and Sinusoidal are\nconsistently small, while ALiBi slowly becomes more\nbeneficial as sequences become longer.\nIn a Concurrent work, Scao et al. (2022) makes\na similar observation in one of their ablation exper-\niments and further show that NoPos models gain\n2For context, Press et al. (2020) report that training the\nsinusoidal model with inputs of length 3072 on WikiText-103\nwith 5 different seeds can result in gaps of up to 0.9 perplexity\nbetween runs (0.34 standard deviation).\n3https://github.com/adihaviv/NoPos\n1384\n0 4 8 12 16 20 24\nLayer\n0\n50\n100\n150\n200\n250\n300\n350Mean Absolute Distance\nNoPos\nLearned\nSinusoidal\nALiBi\nRandom\nFigure 2: Through probing, we find that the NoPos\nmodel behaves similarly to models that use absolute\nlearned position embeddings. We evaluated perfor-\nmance using mean absolute distance on 1.3B parameter\nmodels trained on the Pile.\ncompetitive performances for downstream tasks as\nwell. Specifically, they evaluated 27 diverse down-\nstream tasks. They showed that the NoPos model\nreached an average accuracy of 41.23% over all\ntasks, comparing to Learned and ALiBi who gained\n41.72% and 43.70% respectively.\n5 Analysis\nIn this section, we examine whether the NoPos\nmodel is able to encode positional information and\nshow that such information is essential for its suc-\ncess.\nNoPos models acquire positional information\nDo NoPos LMs learn some form of positional en-\ncoding to compensate for the absence of explicit\npositional modeling? To answer this question, we\nprobe each layer of our trained models 4 for posi-\ntional information. Specifically, we use the tokens’\nlast hidden representation after each transformer\nlayer, produced by the evaluated LM, and train a\n2-layer feed-forward ReLU network to predict the\nabsolute position (0 to 1023) of each token (i.e., as\na multiclass classification problem). Notably, we\ndo not change the weights of the evaluated LMs\nand thus, do not provide any position information\n4We used the 1.3B parameter models trained over 1024-\ntoken sequences of the Pile (Section 3).\nof the tokens to the LM in this experiment, which\nensures the validity of our findings.\nEach layer’s probe was trained separately (hy-\nperparameters are provided in App. C). As a soft\naccuracy metric, we measured the mean absolute\ndistance between the probe’s prediction and the\ntoken’s actual position.\nFigure 2 shows that even though NoPos model\nstarts, as expected, with no positional information\nin the first layer (on par with a random baseline),\nit becomes position-aware within four layers and\nappears to contain more positional information than\nALiBi. By the middle layer, NoPos can predict\nabsolute positions about as well as the model with\nlearned positional embeddings. Finally, we observe\nthat all models shed off a significant amount of\npositional information in the final layers, in line\nwith the findings of V oita et al. (2019). Overall,\nthe probe reveals that the NoPos models learn an\nimplicit notion of absolute positions.\nTo elucidate what positional information the No-\nPos model learns, we visualize the predictions of\nthe probe. We examine a sample of 100 predictions\nfrom the validation set of the best-performing probe\ntrained over the NoPos model. Figure 3 shows the\npredictions over the 512 token sequences sampled\nrandomly from the validation set and a single exam-\nple from the same set. We observe that the probe\nis more accurate at the beginning of the sequence,\nbut becomes fuzzier as it progresses.\nPositional information matters NoPos is able to\ninfer absolute positions, but are they necessary? We\nanswer this using a trained NoPos model. Instead\nof computing the loss over the entire sequence, we\nselect a single random token, shuffle the previous\ntokens that it is conditioned on, and compare to\na baseline where the prefix remains intact. We\nfind that in the case where the suffix is shuffled,\nthe average token-level loss increases dramatically\n(from ∼4 to ∼11). Details of this experiment are\ngiven in App. B.\nThis finding indicates that the NoPos model in-\ndeed uses the positional information it acquires, as\notherwise we would expect similar loss values in\nthese two settings.\n6 Conjecture\nHow do transformers without explicit positional\nencoding learn absolute positions? We conjecture\nthat the causal attention in autoregressive trans-\nformer language models allows them to predict the\n1385\n0 64 128 192 256 320 384 448 512\nTarget Position\n0\n64\n128\n192\n256\n320\n384\n448\n512Predicted Position\nNoPos Probe Predictions\n(mean and conf. interval)\nNoPos Probe\nSingle Example Predictions\nGround Truth\nFigure 3: A visualization of the absolute position predic-\ntions of a probe trained over a NoPos language model.\nThe blue line shows the mean of the generated predic-\ntions for every target position and the blue area repre-\nsents the 95%-confidence interval. The predictions for\na single random sequence are depicted as green dots.\nnumber of attendable tokens at each position, i.e.\nthe number of tokens in the sequence that precede\nthe current one. Such a mechanism could effec-\ntively encode the absolute position of each token\ninto its vector representation. Indeed, our analysis\n(Section 5) reveals that some notion of absolute\npositions exists in the hidden layers of language\nmodels even when they are trained without explicit\npositional encoding, and that this information is ac-\nquired throughout the first few layers. On the other\nhand, bidirectional transformer encoders (which\nare used in masked language modeling, e.g. Devlin\net al. 2019) do not contain causal attention masks\nor any other limitation on the attention mechanism;\nthus, they should be unable to learn absolute po-\nsitions without explicit positional encoding. We\ntested this corollary by training a masked language\nmodel based on RoBERTa large (Liu et al., 2019)\non the Pile (see App. C for hyperparameters). Ta-\nble 4 shows that, indeed, the NoPos model has\nsignificantly worse perplexities than the position-\ninformed baselines. This result echoes the find-\nings of Sinha et al. (2021), who also observed that\nMLMs without positional embeddings suffer sig-\nnificant performance degradation.\n7 Related Work\nWhile there has been ample research on positional\nencoding variants, there has been relatively little\nprior work that investigate models’ ability to infer\nMLM Perplexity\nNoPos 147.18\nLearned 4.06\nSinusoidal 4.07\nALiBi 4.00\nTable 4: Validation set perplexity ofmasked language\nmodels (Devlin et al., 2019) trained with various po-\nsitional encoding methods on an excerpt of the Pile\n(Gao et al., 2020). The model architecture is based\non RoBERTa large (Liu et al., 2019), and processes\n128 tokens per sequence. While position-aware models\nconverge to very low perplexities, training without posi-\ntional encodings (NoPos) fails.\npositions implicitly. Prior to our work, Irie et al.\n(2019) explored transformer language models for\nspeech recognition and found that such models,\nwhen trained without positional encoding, outper-\nform those trained with sinusoidal embeddings. In\naddition, a focused language modeling experiment\nby Stella Rose Biderman5 showed that the NoPos\nmethod attains similar results to other position em-\nbedding methods; however, that experiment was on\na small 350M parameter model trained on a small\ncharacter-level dataset (enwik8). Here we show\nthat this result holds across multiple datasets and\nmodel sizes, provide an analysis of the model’s\ninternal representations, and hypothesize how this\nphenomenon could occur.\n8 Conclusion\nWe show that, contrary to popular belief, transform-\ners language models do learn positional informa-\ntion even when are not provided with any explicit\npositional encoding. Our experiments systemati-\ncally demonstrate that this phenomenon is robust\nacross different language modeling settings, and\nthat one can approximate the absolute position of\neach token from the model’s internal representa-\ntions to a surprising degree. However, this phe-\nnomenon does not extend to transformer encoders\ntrained on the MLM objective. We conjecture that\nthe causal attention mechanism, which limits atten-\ntion in one direction of the sequence, is responsible\nfor implicitly imbuing the transformer with posi-\ntional information.\n5https://twitter.com/BlancheMinerva/status/\n1394089508723900422\n1386\n9 Limitations\nOur work explores language models in the 125M to\n1.3B parameter range. We show that as parameter\ncount increases the gap between the NoPos method\nand the other position methods narrows. This trend\nleads us to believe that our findings should hold for\neven larger models, but the current biggest models\nare more than one hundred times bigger (in terms\nof parameters) than our 1.3B parameter models,\nand so the results in that setting can be unexpected.\nIn addition, training models at the 1.3B parameter\nscale is resource-intensive and might hinder repro-\nducibility. We therefore release our trained models.\nIn Addition, when comparing the perplexity of No-\nPos to other models, although the margins are very\nsmall, NoPos is always slightly worse, suggesting\nthat the inductive bias of positional encoding is\nindeed important.\nAcknowledgements\nThis work was supported by Intel Corporation and\nMeta Platforms Inc.\nReferences\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The Pile: An\n800gb dataset of diverse text for language modeling.\narXiv preprint arXiv:2101.00027.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann Dauphin. 2017. Convolutional se-\nquence to sequence learning. In ICML.\nKazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann\nNey. 2019. Language modeling with deep transform-\ners. In INTERSPEECH.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In 5th International Conference on Learning\nRepresentations, ICLR 2017, Toulon, France, April\n24-26, 2017, Conference Track Proceedings. Open-\nReview.net.\nYurii Nesterov. 1983. A method for unconstrained con-\nvex minimization problem with the rate of conver-\ngence o(1/k2).\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A fast, extensible toolkit for\nsequence modeling. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Associa-\ntion for Computational Linguistics (Demonstrations),\npages 48–53, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nOfir Press, Noah A. Smith, and Omer Levy. 2020. Im-\nproving transformer models by reordering their sub-\nlayers. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n2996–3005, Online. Association for Computational\nLinguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\n1387\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile\nSaulnier, Stas Bekman, M Saiful Bari, Stella Bider-\nman, Hady Elsahar, Jason Phang, Ofir Press, Colin\nRaffel, Victor Sanh, Sheng Shen, Lintang Sutawika,\nJaesung Tae, Zheng Xin Yong, Julien Launay, and\nIz Beltagy. 2022. What language model to train if\nyou have one million GPU hours? In Challenges &\nPerspectives in Creating Large Language Models.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-attention with relative position representations.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 464–468, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked language modeling and the distributional hy-\npothesis: Order word matters pre-training for little.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2888–2913, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nSainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and\nRob Fergus. 2015. End-to-end memory networks. In\nProceedings of the 28th International Conference on\nNeural Information Processing Systems - Volume 2,\nNIPS’15, page 2440–2448, Cambridge, MA, USA.\nMIT Press.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\n1388\nA NoPos Performance Across Different\nSegments of the Input\nTo shed more light on the findings shown in sec-\ntion 4, we explore whether there are parts of the\nsequence that the NoPos model better predicts com-\npared to other positional methods (e.g., is the No-\nPos model performs better at the beginning or the\nend the sequence). We compute the model’s aver-\nage loss in different parts of the sequences. Specifi-\ncally, we split each input sequence into eight con-\nsecutive segments and compute the loss for each\nsegment separately.\nWe evaluate the NoPos and Sinusoidal models\ntrained on the WikiText-103 dataset, with an in-\nput sequence length of 512, and use the standard\nvalidation set. Figure 4 shows the results of this\nexperiment. The NoPos model performs similarly\nor slightly worse than the baseline model on all\ninput parts.\n1:64 65:128 129:192 193:256 257:320 321:384 385:448 449:512\nSequence Split\n4.2\n4.3\n4.4\n4.5\n4.6\n4.7\n4.8\n4.9Loss\nNoPos\nSinusoidal\nFigure 4: NoPos model shows similar performances on\neach part of the sequence, comparing to the baseline\nSinusoidal position encoding.\nB Word Order Analysis\nIs positional information necessary for language\nmodeling, or does the order of the input tokens not\nmatter? To answer this, we conduct the following\nexperiment: instead of computing the loss on the\ncomplete sequence, we pick a specific token in the\nsequence. The next token prediction is conditioned\non the previous tokens in the sequence, and so we\nshuffle the order of the tokens in the prefix and\ncompute the loss only for that specific token. We\nrepeat the experiment with the original, un-shuffled\nprefix sequence as the baseline and compare the\nresults.\nThe experiment was conducted on the NoPos\nmodel with an input sequence length of 512 using\nthe WikiText-103 dataset. We randomly sample\nan index between 5 and 512 for the token we pick\nfrom each input sequence from the validation set.\nFigure 5 shows the results of this experiment for\n100 different inputs. These results clearly show\nthat the transformer language model’s next word\npredictions are not order-invariant.\nBaseline Shuffled Prefix\n4\n6\n8\n10\n12Token-Level Loss\nFigure 5: Shuffling input tokens (for causal langauge\nmodeling) leads to a massive degradation in token-level\nloss.\nC Hyperparameters\nTable 5 provides the optimization hyperparameters\nfor each one of our experiments, and Table 6 shows\nthe model hyperparameters in the modern (Pile)\nsetting.\n1389\nWikiText-103 The Pile Probe Masked LM\nSequence Length 512 1024 1024 128\nOptimizer NAG Adam Adam Adam\nPeak Learning Rate 1 2e-3 2e-3 1e-3\nWarmup Steps 16,000 500 500 500\nTotal Steps 286,000 10,000 10,000 10,000\nTokens per Batch 72,000 256,000 64,000 1,024,000\nDropout 0.3 0 0 0.1\nWeight Decay 0 0.01 0.01 0.01\nTable 5: The optimization hyperparameters used in this work. The NAG optimizer refers to Nesterov accelerated\ngradient (Nesterov, 1983), and Adam refers to (Kingma and Ba, 2015).\n125M 350M 760M 1.3B\nLayers 12 24 24 24\nModel Dimensions 768 1024 1536 2048\nFeed-forward Dimensions 3072 4096 6144 8192\nAttention Heads 12 16 16 32\nTable 6: The models hyperparameters by size.\n1390",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7457433342933655
    },
    {
      "name": "Transformer",
      "score": 0.6783631443977356
    },
    {
      "name": "Security token",
      "score": 0.6148717403411865
    },
    {
      "name": "Language model",
      "score": 0.5593274235725403
    },
    {
      "name": "Encoding (memory)",
      "score": 0.538352370262146
    },
    {
      "name": "Question answering",
      "score": 0.5098311305046082
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43451988697052
    },
    {
      "name": "Position (finance)",
      "score": 0.432628870010376
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}