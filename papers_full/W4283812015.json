{
  "title": "Deeply Tensor Compressed Transformers for End-to-End Object Detection",
  "url": "https://openalex.org/W4283812015",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2984419317",
      "name": "Peining Zhen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2113149399",
      "name": "Ziyang Gao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2107071711",
      "name": "Tianshu Hou",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2102674343",
      "name": "Yuan Cheng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2973285282",
      "name": "Hai-Bao Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2984419317",
      "name": "Peining Zhen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2113149399",
      "name": "Ziyang Gao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2107071711",
      "name": "Tianshu Hou",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2102674343",
      "name": "Yuan Cheng",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2973285282",
      "name": "Hai-Bao Chen",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2772955562",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W3103465009",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2981342956",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2737121650",
    "https://openalex.org/W6693397755",
    "https://openalex.org/W2777406049",
    "https://openalex.org/W6748601877",
    "https://openalex.org/W6656357712",
    "https://openalex.org/W6753494528",
    "https://openalex.org/W2747590145",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W2548228487",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W6731892127",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W2925359305",
    "https://openalex.org/W2949603537",
    "https://openalex.org/W2927302606",
    "https://openalex.org/W3035078980",
    "https://openalex.org/W6748780714",
    "https://openalex.org/W2941472577",
    "https://openalex.org/W3155631715",
    "https://openalex.org/W2991833700",
    "https://openalex.org/W2891967877",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2964228333",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W2024165284",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W4293583914",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3012573144",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963363373",
    "https://openalex.org/W3101415077",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4394657967",
    "https://openalex.org/W2570343428",
    "https://openalex.org/W2605203995",
    "https://openalex.org/W4288325606",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W4295262505",
    "https://openalex.org/W3176934054",
    "https://openalex.org/W2982479999",
    "https://openalex.org/W2963094099",
    "https://openalex.org/W2986357608",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2982770724",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W2962766718",
    "https://openalex.org/W3035396860",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2786098160"
  ],
  "abstract": "DEtection TRansformer (DETR) is a recently proposed method that streamlines the detection pipeline and achieves competitive results against two-stage detectors such as Faster-RCNN. The DETR models get rid of complex anchor generation and post-processing procedures thereby making the detection pipeline more intuitive. However, the numerous redundant parameters in transformers make the DETR models computation and storage intensive, which seriously hinder them to be deployed on the resources-constrained devices. In this paper, to obtain a compact end-to-end detection framework, we propose to deeply compress the transformers with low-rank tensor decomposition. The basic idea of the tensor-based compression is to represent the large-scale weight matrix in one network layer with a chain of low-order matrices. Furthermore, we propose a gated multi-head attention (GMHA) module to mitigate the accuracy drop of the tensor-compressed DETR models. In GMHA, each attention head has an independent gate to determine the passed attention value. The redundant attention information can be suppressed by adopting the normalized gates. Lastly, to obtain fully compressed DETR models, a low-bitwidth quantization technique is introduced for further reducing the model storage size. Based on the proposed methods, we can achieve significant parameter and model size reduction while maintaining high detection performance. We conduct extensive experiments on the COCO dataset to validate the effectiveness of our tensor-compressed (tensorized) DETR models. The experimental results show that we can attain 3.7 times full model compression with 482 times feed forward network (FFN) parameter reduction and only 0.6 points accuracy drop.",
  "full_text": "Deeply Tensor Compressed Transformers for End-to-End Object Detection\nPeining Zhen, Ziyang Gao, Tianshu Hou, Yuan Cheng, and Hai-Bao Chen\u0003\nShanghai Jiao Tong University, China\n{zhenpn, gaoziyang, houtianshu, cyuan328, haibaochen}@sjtu.edu.cn\nAbstract\nDEtection TRansformer (DETR) is a recently proposed\nmethod that streamlines the detection pipeline and achieves\ncompetitive results against two-stage detectors such as Faster-\nRCNN. The DETR models get rid of complex anchor gen-\neration and post-processing procedures thereby making the\ndetection pipeline more intuitive. However, the numerous re-\ndundant parameters in transformers make the DETR mod-\nels computation and storage intensive, which seriously hin-\nder them to be deployed on the resources-constrained de-\nvices. In this paper, to obtain a compact end-to-end detection\nframework, we propose to deeply compress the transform-\ners with low-rank tensor decomposition. The basic idea of\nthe tensor-based compression is to represent the large-scale\nweight matrix in one network layer with a chain of low-order\nmatrices. Furthermore, we propose a gated multi-head atten-\ntion (GMHA) module to mitigate the accuracy drop of the\ntensor-compressed DETR models. In GMHA, each attention\nhead has an independent gate to determine the passed atten-\ntion value. The redundant attention information can be sup-\npressed by adopting the normalized gates. Lastly, to obtain\nfully compressed DETR models, a low-bitwidth quantization\ntechnique is introduced for further reducing the model stor-\nage size. Based on the proposed methods, we can achieve sig-\nniﬁcant parameter and model size reduction while maintain-\ning high detection performance. We conduct extensive exper-\niments on the COCO dataset to validate the effectiveness of\nour tensor-compressed (tensorized) DETR models. The ex-\nperimental results show that we can attain 3.7× full model\ncompression with 482×feed forward network (FFN) param-\neter reduction and only 0.6 points accuracy drop.\nIntroduction\nRecent years have witnessed great success in object detec-\ntion area with the rapid development of deep learning and\nneural networks (Ren et al. 2015; Lin et al. 2017; Car-\nion et al. 2020). Numerous detectors have been proposed\nto continually push forward the state-of-the-art. Nowadays,\nanchor-based and anchor-free detectors are the dominant\nmethods for efﬁcient target object detection. These meth-\nods make bounding box (bbox) predictions depending on\nproposals (Cai and Vasconcelos 2018), anchors (Lin et al.\n∗Corresponding Author\nCopyright c⃝ 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n2017), or key points (Zhou, Wang, and Kr ¨ahenb¨uhl 2019;\nLaw and Deng 2018). However, the design and generation of\nthe above prior knowledge make the corresponding network\nstructures more complex. Currently, another branch of ob-\nject detection methods beyond anchor-based ones has drawn\nwide attention (Carion et al. 2020; Zhu et al. 2020; Dai et al.\n2021). These methods are inspired by the widely used trans-\nformer architecture (Vaswani et al. 2017) and streamline the\ndetection pipeline.\nThe detection transformer (DETR) (Carion et al. 2020),\nwhich achieves competitive results against strong baselines,\nis the ﬁrst attempt to simplify the detection pipeline based\non transformers. However, the vanilla DETR models suf-\nfer from large model size and high computational cost since\nmost of their network structures are linear layers. Such high\ndemands for computing resources seriously hinder them to\nbe stored and deployed on mobile devices with limited hard-\nware resources and a tight power budget. It thereby remains\na great challenge to develop compact DETR models while\nmaintaining the simplicity of their detection pipeline. Al-\nthough the works (Zhu et al. 2020; Sun et al. 2021; Dai\net al. 2021) make efforts to improve the training efﬁciency\nof DETR models, they do not focus on reducing the model\nsize. For resources-constrained AI applications, such as self-\ndriving cars and AR glasses, model compression is particu-\nlarly necessary.\nIt is intuitive to develop model compression methods\nfor reducing the complexity of neural networks. One of\nthe major compression methods is quantization-aware train-\ning (QAT), which aims to reduce the bitwidth of net-\nwork weights and activations during training based on pre-\ntrained models (Han, Mao, and Dally 2015; Jacob et al.\n2018). Nevertheless, these QAT methods are tedious and\ntime-consuming, which hinder the immediate application\nof quantization techniques.f Downstream toolchains (e.g.,\nONNX or TensorFlow-Lite) are required to accomplish the\nmodel size reduction after QAT. Another main compres-\nsion method is network pruning (He, Zhang, and Sun 2017;\nZhuang et al. 2018), which focuses on removing the re-\ndundant network structure. However, the pruning metric is\ndifﬁcult to decide and the retraining procedure is compli-\ncated. In this work, we propose to compress the DETR mod-\nels through low-rank tensor decomposition. The neural net-\nwork weights can be represented as a combination of mul-\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n4716\ntiple very small tensor arrays. Consequently, the number of\nrequired trainable parameters can be signiﬁcantly reduced.\nDifferent from QAT and pruning methods, the tensor-based\ncompression methods directly reconstruct the network struc-\nture and the parameters can be learned from scratch. As a\nresult, the expensive computational costs in the retraining\nprocedure can be avoided, and high detection accuracy can\nremain.\nIn this paper, we propose a tensorized compression frame-\nwork for the detection transformer. By reshaping the weight\nmatrices into high-dimensional tensors and adopting a low-\nrank tensor factorization, signiﬁcant redundant parameter\nreduction can be achieved with maintained detection accu-\nracy. Furthermore, to alleviate the accuracy drop caused by\nthe tensorized compression, we propose a gated multi-head\nattention (GMHA) module. Learnable gates are attached\nto every single attention head for spotlighting meaningful\nheads and down-weighting uninformative ones. At last, a\npost-training quantization technique is introduced for fur-\nther model size reduction. For the COCO detection bench-\nmark, the tensorized DETR shows 3.7×model compression\nwith 482× FFN parameter reduction and only 0.6% accu-\nracy drop (39.5% vs. 40.1%). The experimental results and\ndiscussions can demonstrate the advantages of our proposed\nmethod.\nThe highlights of this paper can be summarized as fol-\nlows: 1) We ﬁrst propose to compress the detection trans-\nformer based on the tensor decomposition. The storage-\nintensive structures such as FFNs can be reconstructed in\ntensor format to directly achieve parameter reduction. This\nmethod does not need any downstream toolchains or back-\nends to realize model size compression; 2) A gated multi-\nhead attention module is proposed to mask the redundant\nheads in the transformer architectures. The controllable gate\nparameters are independently learned during training rather\nthan manually ﬁxed. The accuracy drop after compression\ncan be mitigated with limited parameters increase; 3) A\npost-training quantization technique is introduced to fur-\nther compress the model size. This method preserves high\nmodel performance without any tedious model retraining\nprocedure; 4) Extensive results on the COCO dataset can\ndemonstrate the advantages of our proposed methods. The\nDETR model is deeply compressed with competitive accu-\nracy against state-of-the-art approaches.\nRelated Work\nObject Detection\nModern object detection methods can be mainly divided\ninto two categories: anchor-based and anchor-free methods.\nThe anchor-based methods (Redmon and Farhadi 2017; He\net al. 2017) make predictions with the pre-deﬁned anchor\nboxes, and the quality of anchors is proven to have a signiﬁ-\ncant inﬂuence on the ﬁnal performance (Zhang et al. 2020).\nIn general, the anchor-based methods have higher accuracy\ncompared with anchor-free methods since the positive and\nnegative samples are more balanced. However, the design\nand generation of anchors will greatly increase the model\ncomplexity, therefore the inference speed and model size\nof anchor-based detectors are far from satisfactory. Most\nanchor-free methods (Law and Deng 2018; Tian et al. 2019)\nregress bounding boxes with the extracted keypoints. The\nanchor-free methods remove heavy anchor generation net-\nworks and post-processing procedures thus they can easily\nachieve high speed and lightweight while maintaining high\naccuracy.\nRecently, many works have been proposed to improve\nthe efﬁciency of DETR-series models (Zhu et al. 2020; Dai\net al. 2021; Zheng et al. 2020; Sun et al. 2021). In the\nwork (Zhu et al. 2020), the authors propose multi-scale de-\nformable attention with learnable sparse sampling for boost-\ning the detection performance without increasing redundant\nparameters. UP-DETR (Dai et al. 2021) involves the unsu-\npervised pretraining mechanism into DETR for fast conver-\ngence and high performance. The adaptive clustering trans-\nformer (ACT) (Zheng et al. 2020) proposes to cluster the\nquery features adaptively using locality sensitive hashing\n(LSH) for reducing the inference computation cost. TSP-\nRCNN and TSP-FCOS are proposed based on the trans-\nformer set prediction (TSP) in (Sun et al. 2021) and can\nachieve faster convergence speed with better detection per-\nformance. The above methods mainly focus on improving\nthe training efﬁciency of DETR models while our method\nis devoted to reducing the DETR model size for hardware-\nfriendly deployment on resources-constrained devices.\nNeural Network Compression\nResearchers have proposed a variety of neural network com-\npression methods. One of the major compression meth-\nods is quantization, which narrows the bitwidth of network\nweights or activations to limit the model size. Recent re-\nsearch efforts such as (Han, Mao, and Dally 2015; Hubara\net al. 2016; Rastegari et al. 2016) have signiﬁcantly reduced\nthe computational complexity and model size by adopting\nlow-bitwidth weights and activations. Speciﬁcally, in BNN\n(2016) and XNOR-Net (2016), the network parameters are\naggressively quantized into 2 bits with negligible accuracy\nloss. Besides the above approaches with ﬁxed bitwidth, the\nworks (Khoram and Li 2018; Wang et al. 2019) develop ﬂex-\nible quantization policies. The network layers have different\nbitwidths during training; nevertheless, they obtain similar\naccuracy compared with ﬁxed bitwidth methods. In sum-\nmary, the above methods belong to quantization-aware train-\ning which require full training data and a time-consuming\nretraining procedure.\nRecently, the studies (Li, Wang, and Kong 2018; Gusak\net al. 2019; Yin et al. 2021) propose to compress neural\nnetworks via tucker tensor decomposition methods. In (Li,\nWang, and Kong 2018; Yin et al. 2021), the authors adopt\ntucker based low-rank approximation to compress the neu-\nral networks for classiﬁcation tasks. And in (Li, Wang, and\nKong 2018), the compressed GoogLeNet can achieve about\n2×compression on an ARM-based cell phone. In (Gusak\net al. 2019), the authors leverage tucker decomposition to\ncompress object detection models in an iterative procedure.\nThe compressed Faster-RCNN can achieve up to 3.16×pa-\nrameters reduction with negligible detection accuracy loss.\nBesides, in (Wang et al. 2018), the authors can compress the\n4717\nnetwork up to 243×without losing accuracy. However, the\nauthors mainly focus on shallow networks and simple tasks;\nthey do not provide the results on more difﬁcult tasks such\nas detection or segmentation. In our work, the transformer\nweights are factorized into a sequence of small tensors and\nwe can achieve a much higher compression ratio.\nThe Proposed Method\nNotations and The Basis of Tensor A tensor-based com-\npression method with open boundary conditions, namely\ntensor-train decomposition, is introduced in this paper for\nfurther optimizing the DETR model. We limit our study to\nonly change FFN weights in this section.\nIn general, tensors are multi-dimensional arrays that can\ngeneralize vectors and matrices. We refer the vectors and\nmatrices as 1-dimensional arrays and 2-dimensional arrays\nrespectively. In this section, the vectors are denoted by ordi-\nnary lower case letters (e.g. v); the matrices are denoted by\nboldface upper case letters (e.g. V); the tensors are denoted\nby calligraphic upper case letters (e.g. V). A tensor slice is\na 2-dimensional fragment of one tensor, obtained by ﬁxing\ntwo of all indices. For example, given a 3-dimensional ten-\nsor V∈ RI×J ×K , the horizontal, lateral, and frontal slices\nof this tensor can be represented as V(i;:;:), V(:;j; :), and\nV(:;:;k) respectively.\nTensor-Train Preliminaries Via the tensor-train decom-\nposition, a high-order tensor can be expressed as a chain of\nmatrix products. For instance, we have a d-dimensional (or\nd-way) tensor V∈ Rl1×l2×···×ld , where entries are indexed\nby dindices p1;p2;:::;p k;:::;p d. Each pk is within [1;lk]\nfor k = 1; 2;:::;d . The high-order tensor Vcan then be\ndecomposed and represented using a collection of dtensor\ncores G(k) ∈Rrk−1×lk×rk with k ∈[1;d]. Each entry in V\ncan be reconstructed by these tensor cores as:\nVp1;p2;···;pd =\ndY\nk=1\nG(k)\npk ; (1)\nwhere G(k)\npk = G(k) (:;pk;:) ∈Rrk−1×rk is the pk-th lat-\neral slice of the k-th tensor core G(k). In each tensor core,\nr ∈ {r0;r1;···;rd}is called the tensor-train rank where\nboth boundaries r0 and rd are ﬁxed to 1. Due to the compact\ntensor representation scheme, the number of parameters to\nformulate a d-way tensor Vis only Pd\nk=1 rk−1 lkrk while\nthe conventional explicit representation needs Qd\nk=1 lk pa-\nrameters. We graphically show an example of tensor-train\ndecomposition in Fig. 1 for better explanation. For a 2-way\nweight matrix, it has to be rearranged into a d-way tensor\nrepresentation before conducting the tensor-train decompo-\nsition.\nTensor-Train Decomposition for Feed Forward Network\nFeed forward networks in the DETR model are sequentially\nconnected linear layers, which take up most of the model\nsize. We simplify (omit the bias) and denote the linear lay-\ners in FFN modules with the matrix-by-vector multiplication\nform as y = W ·f, where input feature vector f ∈RN ,\nweight matrix W ∈ RM×N , and output feature vector\n2 – dimensional \nmatrix \nd – dimensional\ntensor (d = 3)\nreshape\nFigure 1: A 2-way weight matrix example for tensor-train\ndecomposition.\ny ∈RM . Then we can rewrite this layer more explicitly\nwith indices yi = PN\nj=1Wi;j ·fj, where i ∈[1;M] and\nj ∈[1;N].\nFor the 2-way weight matrix W as well as the input fea-\nture f and output feature yin FFN modules, they should be\nrearranged to d-way tensor representations by the projection\nfunction \u001eas follows:\n\u001ew : RM×N → R(m1×n1)×···×(md×nd);\u001ew(W) = W;\n\u001ey : RM → Rm1×\nm2×···×md ;\u001ey(y) = Y;\n\u001ef : RN → Rn1×n2×···×nd ;\u001ef (f) = F:\n(2)\nWe assume that\nboth M and N can be factorized into two\ninteger arrays with M = Qd\nk=1 mk and N = Qd\nk=1 nk\nrespectively. Following the above equation (2), the expres-\nsion of the tensorized weight can be changed from Wi;j to\nWi1;j1;i2;j2;···;id;jd with 2d-tuple indices.\nThen in FFN modules, the 2-way weightW and the input\nfeature f multiplication of linear layers can be represented\nin tensor format as follows:\nYi1;i2;···;id =\nX\nj1;j2;···;jd\nWi1;j1;i2;j2;···;id;jd ·Fj1;j2;···;jd ; (3)\nwhere Y, F, and Ware the high-order\ntensor representations\nof y, f, and W as depicted in Eq. (2).\nFollowing the preliminaries, the d-way weight tensor W\ncan be further represented in the basic tensor-train form:\nWi1;j1;i2;j2;···;id;jd =\ndY\nk=1\neG(k)\nik;jk ; (4)\nwhere eG(k)\nik;jk = eG(k) (:;ik;jk;:) can be viewed as anrk−1 ×\nrk slice of the 4-d tensor core eG(k) ∈Rrk−1×mk×nk×rk .\nHere the index lk in G(k) ∈Rrk−1×lk×rk is supposed to\nbe factorized as lk = mk ·nk. We show the tensor-train\ndecomposition of FFN weight matrix W in Fig. 2.\nIn our experiments, the tensor decomposition is applied\nto every W ·f operation in the FFN modules following Eq.\n(4) since linear layers are the most storage-intensive parts in\nthe DETR models. The tensorized multiplication of the input\nfeature and weights can then be represented as follows:\nYi1;i2;···;id =\nX\nj1;j2;···;jd\neG(1)\ni1;j1\neG(2)\ni2;j2 ···eG(d\n)\nid;jd ·Fj1;j2;···;jd : (5)\nIn our tensorized DETR\nmodel, each FFN module con-\nsists of two tensor-compressed linear layers. The ﬁrst lin-\near layer has input length 256 and output length 2048 while\n4718\nW\nLinear layer \nin FFN Tensor-train Representation\nFigure 2: Tensor-train decomposition of weight matrix W\nin the feed forward networks.\nthe second linear layer has the swapped input and output\nlength. In the validation experiments, the vector length 256\n(N) is factorized to [2;4;4;4;2] and 2048 (M) is factorized\nto [4;4;8;4;4]. We have d = 5. The tensor-train ranks are\nset to {1;4;4;4;4;1}.\nTheoretical Beneﬁts The main beneﬁt of using tensorized\nlinear is that the model size and computational complexity\ncan be reduced. For a vanilla linear layer in the FFN module,\nthe time complexity would be O(NM). Whereas in the ten-\nsorized linear layer, the ﬁnal time complexity isO(d^r2 ^nM),\nwhere ^n = max k(nk), ^r is the maximal rank (ranks are\nequal in our experiments), and dis the dimensionality of a\ntensor. The space complexities areO(NM) and O(d^r2 ^m^n)\nrespectively, where ^m= maxk(mk).\nWe can also derive the compression ratio (z) in the FFN\nlinear layers as the ratio between the number of weights in a\nvanilla layer and that in its tensor-compressed form:\nz =\nQd\nk=1 nk ·Qd\nk=1 mk\nPd\nk=1 rk−1 mknkrk\n: (6)\nWe give the numerical analysis with a simple example. Sup-\npose we have a linear layer with input length 256 and output\nlength 2048, we will need 524,288 parameters to represent\nthe weight matrix. If we factorize the input vector into di-\nmension 2 ×4 ×4 ×4 ×2 and the output vector into dimen-\nsion 4 ×4 ×8 ×4 ×4, we only need 1088 parameters to\nget the weight matrix using a tensor-train rank 4; and if the\ntensor-train rank is 3, we will only need 624 parameters. As\na result, compression ratios of the corresponding ranks are\naround 482×and 840×respectively.\nThe Gated Multi-head Attention Module\nFormulation of the Learnable Gate Parameters Multi-\nhead attention (MHA) is a powerful and ubiquitous module\nin the DETR model, which allows the network to obtain in-\nformation from different mapping spaces. In vanilla DETR\nmodels, we usually have 8 heads in each attention module.\nHowever, not all the heads contribute equally for making\npredictions (Vig and Belinkov 2019). The redundant infor-\nmation causes the DETR model to overﬁt. We normalize the\nvanilla multi-head attention module by multiplying the sin-\ngle head representation Hi with a learnable gate parameter\ngi. Consequently, the proposed GMHA can be written as:\nG-MultiHead(Q; K; V) = Concat (gi ·Hi) WO; (7)\nwhere i ∈ {1;:::;nh}denotes the number of heads. In-\nspired by (Gal, Hron, and Kendall 2017; Maddison, Mnih,\nand Teh 2017), our gate parameters are independently\nlearned from the binary hard concrete distributions. Given\nthe learnable parameter set q for all heads in a MHA mod-\nule, our gate parameter set g can be derived as follows:\ns = Sigmoid ((q+ logu−log (1−u)) =T) ; (8)\n\u0016 s= s (\u0015−\u0016) + \u0016; (9)\ng = clamp (\u0016 s;0;1) : (10)\nThe set q only contains nh parameters which is the number\nof heads. In the above Eq. (8), u is a random noise drawn\nfrom the uniform distribution U(0;1) and can push the s\naway from middle values. T ∈(0;1) is the temperature that\ncontrols the magnitude of value. As we can see from the Eq.\n(8), the range of s is within (0;1). Since we want the heads\ncan be totally opened or closed in GMHA modules, we in-\ntroduce two parameters \u0016;\u0015 that can stretch the valve of s\nto (\u0016;\u0015) with \u0016 <0 and \u0015 >1 as described in Eq. (9).\nThen we can apply a hard-sigmoid function to clip the value\nof gate g into range [0;1] following Eq. (10). In our experi-\nments, the \u0016and \u0015are set to -0.1 and 1.1 respectively. T is\nselected as 0.33.\nTo evaluate the performance of the GMHA module, we\nconduct a series of experiments with GMHA applied to only\nencoder self-attention and all multi-head attention (i.e. en-\ncoder and decoder self-attention and cross-attention from\nencoder to decoder). Based on the experimental results, if\nnot speciﬁed, the GMHA module is only applied to encoder\nself-attention.\nLearning of the Gate Parameters Since some of the\nheads contain redundant information that hinders the model\nperformance, we would like to down-weight or even dis-\nable these heads. Inspired by (Louizos, Welling, and Kingma\n2017), we propose a penalty function during the training\nof tensorized DETR models that would push the models to\nswitch off those redundant heads. The penalty loss function\nLp can be expressed as follows:\nLp (g1; g2; ···; gnh ) = Pnh\ni=1 (1 −P (gi = 0 |'i)) ; (11)\nwhere gi is the gate parameter. P is the probability mass\nderived from the hard concrete distribution and'i is the dis-\ntribution parameter. This penalty function is a relaxation of\nL0 norm with the sum of probability mass of non-zero gates\n(Louizos, Welling, and Kingma 2017). During the training\nof our tensorized DETR models, the learning objective func-\ntion Lis a linear combination of the original DETR loss and\nthe penalty loss function L = Ld + \u001a·Lp, where \u001a is a\nbalancing parameter.\nLow Bitwidth Compression\nIn the vanilla DETR models, the backbone network is an-\nother major source of the large model size. To obtain a fully\ncompressed DETR model, we introduce the post-training\nquantization technique for low-bitwidth backbone network\ncompression. This post-training quantization technique does\nnot require any tedious retraining procedure and the full\ntraining dataset compared with QAT methods. Given the\n4719\nweight representation W, we need to ﬁnd an optimal quanti-\nzation interval for constraining the ﬂoating-point values into\na ﬁnite set of values. Following the uniform quantization for-\nmulation, we obtain the quantization point:\nWq = clamp\n\u0012\n⌊W\n\u0001 ⌉;−2s−1; 2s−1 −1\n\u0013\n; (12)\nwhere \u0001 is the quantization scale,sis the quantization level,\nand Wq is the quantized weight. ⌊·⌉denotes rounding to the\nnearest integer.\nFor post-training quantization, our goal is to learn an opti-\nmal mapping between the outputs of full-precision and low-\nbitwidth convolutions. This quantization procedure can be\nformulated as an optimization problem:\narg min\n\u0001;Wq\n∥y−\u0001Wq ·f∥2\nF ; (13)\nwhere \u0001 is the scale to be learned, y is the full-precision\nconvolution output, andf is the input feature. The similarity\nbetween the vanilla and quantized output is measured using\nthe euclidean metric. This optimization problem for quan-\ntization can be solved in an iterative way. The quantization\nscale can be derived as:\n\u0001 = y⊤f⊤W⊤\nq\nWqff ⊤W⊤q\n: (14)\nThe quantization points can be optimized bit-by-bit follow-\ning (Wang et al. 2020).\nExperiments\nDatasets\nCOCO (Lin et al. 2014) is used to validate our proposed\nmethod, which contains 118k training images and 5k valida-\ntion images. Following the DETR baseline, we report bbox\nAP on the validation dataset for ablations and comparisons\nwith state-of-the-art.\nImageNet-1k (Deng et al. 2009) is leveraged to calibrate\nand validate our quantization compressed backbone. The\ndataset consists of 1.28M training images and 50k valida-\ntion images from total 1000 semantic categories.\nImplementation Details\nDETR Training We adopt the vanilla DETR as our\nbaseline model which is implemented based on the\nmmdetection. The CNN backbone is the ImageNet pre-\ntrained ResNet-50 (He et al. 2016). All models are trained\non 4 NVIDIA GTX 1080Ti GPUs with 2 images per GPU.\nWe train the models by using AdamW optimizer with 150\nepochs in total. The learning rates of transformer encoder-\ndecoder and the CNN backbone are initialized to 5 ×10−5\nand 5×10−6 respectively. The weight decay is set to 0.0001.\nThe learning rates are divided by 10 at the decay step 100\nepoch. The balancing parameter \u001afor the penalty function is\nset to 0.1. As for the transformer implementations, we lever-\nage 6 encoder layers and 6 decoder layers with embedded\ndimension 256. Each encoder and decoder layer has 8 at-\ntention heads. Following DETR, we leverage a simple data\naugmentation technique by resizing the input images with\nthe short side ranging from 480 to 800 pixels and the long\nside at most 1333 pixels.\nAll the results of speed (FPS) in our experiments are mea-\nsured under one NVIDIA GTX 1080Ti GPU. FPS is com-\nputed by averaging the speed for inferencing 2000 images\nfrom the COCO validation dataset.\nQuantization ResNet-50 is employed as the basic back-\nbone network of DETR models. We randomly sample 300\nimages from ImageNet-1k and COCO training datasets re-\nspectively as our calibration datasets.\nAblation Study\nEffectiveness of the Tensorized Compression We per-\nform ablation studies on the COCO dataset to verify the ef-\nfectiveness of tensor decomposition. The experimental re-\nsults are given in Table 1. We ﬁrst reproduce the baseline\nmodel and it can achieve 40.1% AP as shown in Table 1.\nThen we show the results obtained by the proposed ten-\nsorized DETR (T-DETR) model. Note that the GMHA mod-\nule and quantization are not leveraged in this experiment. As\ncan be seen from Table 1, our proposed tensorized DETR\nmodel only has 2.5 points accuracy drop with 47.9MB trans-\nformer model size reduction. It should be noted that the\nvanilla ResNet-50 backbone has 90.0MB model size, which\noccupies most of the model size in one DETR model. Here\nwe only consider the transformer architecture in the DETR\nmodel, and we achieve 3.3× model size reduction with the\nintroduced tensor decomposition method.\nMethod AP Size FPS Mem\nDETR 40.1 69.0 MB 16.6 7.9 GB\nOurs 37.6 21.1 MB 15.6 7.5 GB\nTable 1: Comparison between the vanilla DETR model and\nour tensorized DETR model. Size here is the transformer\nstorage size. Mem denotes the training memory consump-\ntion.\nIn addition, as shown in Table 1, the tensorized DETR\nmodel has slightly lower FPS than the vanilla DETR model.\nBecause tensor decomposition requires more mathematical\noperations instead of one highly optimized matrix multipli-\ncation during implementation (in PyTorch). However, these\ncomputation overheads are acceptable since we mainly aim\nat deeply compressing the model size. The experimental\nresults can demonstrate that the proposed method beneﬁts\nfrom the tensor decomposition.\nAnalysis About the Tensor-Train Rank The tensor-train\ndecomposition rank r plays an important role in reducing\nthe redundant parameters of the transformers. We provide\na numerical analysis of different tensor-train ranks to show\ntheir impact on the model performance. The experiments are\nconducted based on the FFN modules with two linear layers\nin the DETR model. The input vector of length 256 (N ) is\nfactorized as 2 ×4 ×4 ×4 ×2, and the output vector of\nlength 2048 (M) is factorized as 4 ×4 ×8 ×4 ×4. In most\napplications, ranks are manually selected and set to equal\n4720\nsince we can not list all the rank values exhaustively. Here\nthe ranks are selected from 3 to 5. The experimental results\nare summarized in Table 2. In this table, the ranks are ﬁxed\nwith a total of 6 numbers, for example, {1;4;4;4;4;1}.\nMethod Rank #Param z AP FPS\nOurs (T-DETR)\nFFN 1,048,576 1× 40.1 16.6\n3 1,248 840× 36.9 15.9\n4 2,176 482× 37.6 15.6\n5 3,360 312× 37.7 15.6\nTucker (2009) 2 2,208 475× 37.4 9.0\nTucker (2009) 3 118,338 9× 37.7 8.8\nTucker-2 (2009) 4 14,283 73× 37.6 16.3\nTable 2: Numerical analysis about the tensor-train rank and\ncomparisons with other tensor decomposition methods. z\ndenotes the compression ratio. AP and FPS are measured\nusing corresponding DETR models.\nAs shown in Table 2, the tensor-train decomposition can\nachieve up to 840×parameter reduction with a slight accu-\nracy drop. Moreover, we can see that the detection accuracy\nwill slightly increase as the tensor-train rank grows; how-\never, the number of parameters increases signiﬁcantly. We\nhave similar speed due to the powerful parallel computing\ncapabilities of GPUs. As a result, to obtain the best accuracy\nand parameter number trade-off, we choose the tensor-train\nrank 4 as the default setting in our experiments.\nWe also make comparisons with other tensor compres-\nsion methods in Table 2. When using ranks 2 and 3, tucker-\ncompressed DETR models show lower compression ratio\nand similar detection accuracy. When the rank is 4, the pa-\nrameters in one tucker-compressed FFN is 2,097,472, which\nare more than those in one vanilla FFN. The parameters\nin tucker representation will explode when the rank grows\nsince they are proportional to r10. Our method shows much\nbetter efﬁciency than the tucker decomposition method.\nEffectiveness of the GMHA Module In this section, ex-\nperiments are performed to demonstrate the effectiveness of\nthe proposed GMHA module. The quantitative results are\nshown in Table 3. First, we integrate the proposed GMHA\nmodule into vanilla DETR models. It is shown in Table 3 that\nthe GMHA module helps the vanilla DETR model achieve\n1.6 points AP improvement. Furthermore, we can see that\nthe tensorized DETR (T-DETR) model obtains 1.9 points\naccuracy improvement with the GMHA module. The experi-\nmental results can demonstrate the advantages of the GMHA\nmodule for suppressing the uninformative attention values\nand alleviating the overﬁtting problem.\nWe ﬁnd that our GMHA module is more effective when\napplied to only encoder attentions. The experimental results\nare summarized in Table 4. In this experiment, GMHA is\nleveraged based on the tensor-compressed DETR model. As\nshown in Table 4, the obtained accuracy is 39.2% for models\nwith GMHA module in all attentions, whereas the accuracy\nis 39.5% for models with GMHA module in only encoder\nattentions. Note that we only need 48 additional parameters\nMethod AP Size FPS Mem\nDETR 40.1 69.0 MB 16.6 7.9 GB\n+ GMHA 41.7 (1.6 ↑) 69.0 MB 16.3 7.9 GB\nOurs (T-DETR) 37.6 21.1 MB 15.6 7.5 GB\n+ GMHA 39.5 (1.9 ↑) 21.1 MB 15.2 7.5 GB\nTable 3: Effect of the proposed GMHA module on model\nperformance. Size is the transformer storage size.\nfor leveraging the GMHA modules into only encoder lay-\ners and 144 additional parameters for leveraging them into\nall encoder and decoder layers. Results came out that these\ngate parameters do not cause signiﬁcant drop in the detec-\ntion quality, and also do not increase the model size.\nAttentions AP #Param g Size FPS\nOnly encoder 39.5 48 21.1 MB 15.2\nAll heads 39.2 144 21.1 MB 14.9\nTable 4: Detection performance for GMHA modules in dif-\nferent attentions. #Param represents the additional gate pa-\nrameters. Size is the transformer storage size.\nFurthermore, we show an example of the learned gate pa-\nrameters g in Fig. 3. The ﬁrst observation is that most of\nthe gates are totally opened or closed, which means the gate\nvalues are either 1 or 0. The second observation is that the\nﬁrst head contains the most redundant information since all\nthe ﬁrst heads have gate parameters 0. We hypothesize this\nis because edge pixels of images or features contribute little\nto the detection results. The third observation is that the ﬁrst\ntwo layers are less important than the following layers. The\nﬁrst two layers have three closed gates while the other layers\nmostly have one closed gate. We argue that deep feature rep-\nresentations are more informative than the shallow features\nin the transformer architecture.\n1 2 3 4 5 6 7 8\nHeads\n1\n2\n3\n4\n5\n6 Layers\n0.00 1.00 1.00 1.00 0.00 1.00 0.00 1.00\n0.00 1.00 1.00 1.00 0.00 1.00 0.00 1.00\n0.00 1.00 1.00 0.97 1.00 1.00 1.00 0.97\n0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n0.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nGate Value\nFigure 3: Heap map of the learned gate parameters from the\nencoder layers.\nQuantization Evaluation For ImageNet-1k evaluation,\nthe PyTorch pretrained ResNet-50 is leveraged. We quantize\nall the weight layers into 4-bit and 8-bit while keeping the\nactivations in full precision (FP32). The results are shown in\n4721\nMethod Backbone Size AP AP 50 AP75 APS APM APL\nRetinaNet (Lin et al. 2017) ResNet-50 145.1 MB 36.5 55.4 39.1 20.4 40.3 48.1\nFaster-RCNN DC5 (Ren et al. 2015) ResNet-50 631.8 MB 37.2 58.3 39.9 19.5 41.4 50.4\nFaster-RCNN FPN (Ren et al. 2015) ResNet-50 159.5 MB 37.4 58.1 40.4 21.2 41.0 48.1\nMask-RCNN (He et al. 2017) ResNet-50 169.6 MB 38.2 58.8 41.1 21.9 40.9 49.5\nFaster-RCNN FPN (Ren et al. 2015) ResNet-101 232.2 MB 39.4 60.1 43.1 22.4 43.7 51.1\nFCOS-GN (Tian et al. 2019) ResNet-50 123.5 MB 36.6 56.0 38.8 21.0 40.6 47.0\nRepPoints (Yang et al. 2019) ResNet-50 140.8 MB 37.0 56.7 39.7 20.4 41.0 49.0\nDeformable-DETR†(Zhu et al. 2020) ResNet-50 156.0 MB 39.7 60.1 42.4 21.2 44.3 56.0\nYOLOS-S (Fang et al. 2021) DeiT-S 351.3 MB 36.1 56.4 37.1 15.3 38.5 56.2\nUP-DETR (Dai et al. 2021) ResNet-50 164.0 MB 40.5 60.8 42.8 19.0 44.4 60.0\nBaseline (DETR) ResNet-50 159.0 MB 40.1 60.6 42.0 18.3 43.3 59.5\nOurs (T-DETR) ResNet-50 111.1 MB 39.5 59.9 41.6 18.7 42.5 58.0\nOurs (T-DETR) R-50 8-bit 43.6 MB 39.5 59.8 41.6 18.8 42.4 58.0\nOurs (T-DETR) R-50 4-bit 33.4 MB 37.9 57.9 39.8 17.3 40.6 56.3\nTable 5: The quantitative comparisons with state-of-the-art methods on the COCO validation dataset. For fair comparison, the\nresults are measured under mmdetection implementation. Size is the full model storage size here. †denotes single-scale\nDeformable-DETR. ResNet-101 is not desirable in our method since we aim to reduce the model size.\nTable 6. As shown in this table, the 8-bit quantized model\nhas higher accuracy than the full-precision model. As for\nthe 4-bit quantized model, there is only 0.64 / 0.27 accu-\nracy drop from the full-precision model. In the classiﬁcation\nevaluation, the size of the full-precision model is 97.7MB\nwhile the sizes of 8-bit and 4-bit quantized models would\nbe 24.5MB and 12.3MB. Note that there are no linear layers\nin the backbone of DETR models, thus the backbone model\nsize would be 90.0MB.\nFP32 8-bit 4-bit\n76.15 / 92.87 76.18 / 92.95 75.51 / 92.60\nTable 6: Performance comparison between full-precision\nand quantized backbone networks. Acc (%): top-1 / top-5.\nFurthermore, we quantize the backbone network in DETR\nmodels and measure the performance on the COCO valida-\ntion dataset. The results are summarized in Table 7. The re-\nsults in this table show that the 8-bit quantized backbones\nlead to no accuracy degradation. For vanilla DETR and our\ntensor-compressed DETR models, the bbox AP results are\nthe same for both full-precision and 8-bit quantized back-\nbones. We observe a signiﬁcant accuracy drop when quanti-\nzation is extended to the tensorized transformers. The exper-\nimental results demonstrate that quantization is an effective\nway for our backbone compression in DETR models.\nComparison with State-of-the-arts\nResearch works focusing on compressing detection trans-\nformers are limited; therefore in this section, we directly\nprovide the quantitative comparisons with other object de-\ntection methods on the COCO validation dataset. To make\nthe comparisons fair, we report the results based on the\nmmdetection implementation rather than from the orig-\ninal paper. The results are summarized in Table 5. First\nand foremost, our tensorized DETR model with an 8-bit\nBackbone AP AP50 AP75 APS APM APL\nR-50 40.1 60.6 42.0 18.3 43.3 59.5\nR-50 8-bit 40.1 60.7 42.0 18.5 43.4 59.6\nR-50 39.5 59.9 41.6 18.7 42.5 58.0\nR-50 8-bit 39.5 59.8 41.6 18.8 42.4 58.0\nR-50 4-bit 37.9 57.9 39.8 17.3 40.6 56.3\nTable 7: Quantitative comparisons between full-precision\nand quantized DETR backbones. The top and bottom rows\nshow the results of the baseline DETR and our T-DETR re-\nspectively.\nquantized backbone achieves 3.7×model size compression\nwith only 0.6 points accuracy drop. Moreover, with a 4-bit\nbackbone, we obtain 4.8×model size compression with 2.2\npoints accuracy drop. The experimental results show that our\ncompressed models can maintain competitive detection ac-\ncuracy against state-of-the-art methods.\nConclusion\nIn this paper, we demonstrate that tensor decomposition is\na promising way to efﬁciently compress the detection trans-\nformer. By incorporating the tensor-train decomposition into\nFFN modules, we obtain up to 840× parameter reduction\nunder comparable detection accuracy. A novel gated multi-\nhead attention module with limited parameters is proposed\nfor down-weighting the redundant attention values. Based\non the GMHA module, the signiﬁcant detection accuracy\ndrop can be circumvented. With further low-bitwidth quan-\ntization techniques, we achieve up to 4.8× whole DETR\nmodel compression. Extensive experiments are performed\non the COCO benchmark to validate our proposed meth-\nods. The results show we can achieve 39.5% AP after com-\npression which is only 0.6 percentage points lower than the\nvanilla DETR model.\n4722\nAcknowledgements\nThis work is supported by the National Key Re-\nsearch and Development Program of China under grant\n2019YFB2205005.\nReferences\nCai, Z.; and Vasconcelos, N. 2018. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 6154–6162.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Proceedings of the European Confer-\nence on Computer Vision, 213–229.\nChen, K.; Wang, J.; Pang, J.; Cao, Y .; Xiong, Y .; Li, X.; Sun,\nS.; Feng, W.; Liu, Z.; Xu, J.; Zhang, Z.; Cheng, D.; Zhu, C.;\nCheng, T.; Zhao, Q.; Li, B.; Lu, X.; Zhu, R.; Wu, Y .; Dai,\nJ.; Wang, J.; Shi, J.; Ouyang, W.; Loy, C. C.; and Lin, D.\n2019. MMDetection: Open MMLab Detection Toolbox and\nBenchmark. arXiv preprint arXiv:1906.07155.\nDai, Z.; Cai, B.; Lin, Y .; and Chen, J. 2021. Up-detr: Unsu-\npervised pre-training for object detection with transformers.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 1601–1610.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. Imagenet: A large-scale hierarchical image\ndatabase. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 248–255.\nFang, Y .; Liao, B.; Wang, X.; Fang, J.; Qi, J.; Wu, R.; Niu,\nJ.; and Liu, W. 2021. You Only Look at One Sequence:\nRethinking Transformer in Vision through Object Detection.\narXiv preprint arXiv:2106.00666.\nGal, Y .; Hron, J.; and Kendall, A. 2017. Concrete dropout.\narXiv preprint arXiv:1705.07832.\nGusak, J.; Kholiavchenko, M.; Ponomarev, E.; Markeeva,\nL.; Blagoveschensky, P.; Cichocki, A.; and Oseledets, I.\n2019. Automated multi-stage compression of neural net-\nworks. In Proceedings of the IEEE International Conference\non Computer Vision Workshops.\nHan, S.; Mao, H.; and Dally, W. J. 2015. Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint\narXiv:1510.00149.\nHe, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In Proceedings of the IEEE International Conference\non Computer Vision, 2961–2969.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition ,\n770–778.\nHe, Y .; Zhang, X.; and Sun, J. 2017. Channel pruning for ac-\ncelerating very deep neural networks. In Proceedings of the\nIEEE International Conference on Computer Vision, 1389–\n1397.\nHubara, I.; Courbariaux, M.; Soudry, D.; El-Yaniv, R.; and\nBengio, Y . 2016. Binarized neural networks. InAdvances in\nNeural Information Processing Systems, volume 29.\nJacob, B.; Kligys, S.; Chen, B.; Zhu, M.; Tang, M.; Howard,\nA.; Adam, H.; and Kalenichenko, D. 2018. Quantization and\ntraining of neural networks for efﬁcient integer-arithmetic-\nonly inference. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2704–2713.\nKhoram, S.; and Li, J. 2018. Adaptive quantization of neural\nnetworks. In International Conference on Learning Repre-\nsentations.\nKolda, T. G.; and Bader, B. W. 2009. Tensor Decomposi-\ntions and Applications. SIAM Review, 51(3): 455–500.\nKuchaiev, O.; and Ginsburg, B. 2017. Factorization tricks\nfor LSTM networks. arXiv preprint arXiv:1703.10722.\nLaw, H.; and Deng, J. 2018. Cornernet: Detecting objects as\npaired keypoints. In Proceedings of the European Confer-\nence on Computer Vision, 734–750.\nLi, D.; Wang, X.; and Kong, D. 2018. Deeprebirth: Accel-\nerating deep neural network execution on mobile devices.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 32.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P.\n2017. Focal loss for dense object detection. In Proceedings\nof the IEEE International Conference on Computer Vision ,\n2980–2988.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In Proceedings of the\nEuropean Conference on Computer Vision, 740–755.\nLouizos, C.; Welling, M.; and Kingma, D. P. 2017. Learn-\ning sparse neural networks through L0 regularization. arXiv\npreprint arXiv:1712.01312.\nMaddison, C.; Mnih, A.; and Teh, Y . 2017. The concrete dis-\ntribution: A continuous relaxation of discrete random vari-\nables. In Proceedings of the International Conference on\nLearning Representations.\nRastegari, M.; Ordonez, V .; Redmon, J.; and Farhadi, A.\n2016. Xnor-net: Imagenet classiﬁcation using binary con-\nvolutional neural networks. In Proceedings of the European\nConference on Computer Vision, 525–542.\nRedmon, J.; and Farhadi, A. 2017. YOLO9000: better,\nfaster, stronger. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 7263–7271.\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster r-\ncnn: Towards real-time object detection with region proposal\nnetworks. In Advances in Neural Information Processing\nSystems, 91–99.\nSun, Z.; Cao, S.; Yang, Y .; and Kitani, K. M. 2021. Rethink-\ning transformer-based set prediction for object detection. In\nProceedings of the IEEE International Conference on Com-\nputer Vision, 3611–3620.\nTian, Z.; Shen, C.; Chen, H.; and He, T. 2019. Fcos: Fully\nconvolutional one-stage object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, 9627–9636.\n4723\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in Neural Information\nProcessing Systems, 5998–6008.\nVig, J.; and Belinkov, Y . 2019. Analyzing the Structure of\nAttention in a Transformer Language Model. InProceedings\nof the ACL Workshop BlackboxNLP, 63–76.\nWang, K.; Liu, Z.; Lin, Y .; Lin, J.; and Han, S. 2019. Haq:\nHardware-aware automated quantization with mixed preci-\nsion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 8612–8620.\nWang, P.; Chen, Q.; He, X.; and Cheng, J. 2020. Towards\naccurate post-training network quantization via bit-split and\nstitching. In International Conference on Machine Learn-\ning, 9847–9856.\nWang, W.; Sun, Y .; Eriksson, B.; Wang, W.; and Aggarwal,\nV . 2018. Wide compression: Tensor ring nets. In Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 9329–9338.\nYang, Z.; Liu, S.; Hu, H.; Wang, L.; and Lin, S. 2019. Rep-\npoints: Point set representation for object detection. In Pro-\nceedings of the IEEE International Conference on Computer\nVision, 9657–9666.\nYin, M.; Liao, S.; Liu, X.-Y .; Wang, X.; and Yuan, B. 2021.\nTowards Extremely Compact RNNs for Video Recognition\nWith Fully Decomposed Hierarchical Tucker Structure. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 12085–12094.\nZhang, S.; Chi, C.; Yao, Y .; Lei, Z.; and Li, S. Z. 2020.\nBridging the gap between anchor-based and anchor-free de-\ntection via adaptive training sample selection. In Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition, 9759–9768.\nZheng, M.; Gao, P.; Wang, X.; Li, H.; and Dong, H. 2020.\nEnd-to-end object detection with adaptive clustering trans-\nformer. arXiv preprint arXiv:2011.09315.\nZhou, A.; Yao, A.; Guo, Y .; Xu, L.; and Chen, Y . 2017. In-\ncremental network quantization: Towards lossless cnns with\nlow-precision weights. arXiv preprint arXiv:1702.03044.\nZhou, X.; Wang, D.; and Kr ¨ahenb¨uhl, P. 2019. Objects as\npoints. arXiv preprint arXiv:1904.07850.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2020.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. arXiv preprint arXiv:2010.04159.\nZhuang, Z.; Tan, M.; Zhuang, B.; Liu, J.; Guo, Y .; Wu, Q.;\nHuang, J.; and Zhu, J. 2018. Discrimination-aware channel\npruning for deep neural networks. In Advances in Neural\nInformation Processing Systems, 883–894.\n4724",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6432706117630005
    },
    {
      "name": "Data mining",
      "score": 0.5006017684936523
    },
    {
      "name": "Computation",
      "score": 0.44828757643699646
    },
    {
      "name": "Algorithm",
      "score": 0.36755135655403137
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    }
  ]
}