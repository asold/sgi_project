{
  "title": "Towards more Human-like Language Models based on Contextualizer Pretraining Strategy",
  "url": "https://openalex.org/W4389519531",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5073544000",
      "name": "Chenghao Xiao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5079144774",
      "name": "G Thomas Hudson",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5017619842",
      "name": "Noura Al Moubayed",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3016970897",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2605959375",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4210489638",
    "https://openalex.org/W2962784628"
  ],
  "abstract": "Taking inspiration from human children learning, we pose a question: can a \"baby language model\" gradually internalize a concept by exposing itself to the concept in unlimited, oftentimes irrelevant contexts, and what this means to limited pretraining resource (both data-wise and GPU-wise).Throughout the study, we restrict our experiments to two data-limited settings, 10M and 100M tokens, which are respectively 1/3000 and 1/300 to what were available to the training of RoBERTa.Our best performing training recipe performs within 1.2% of RoBERTa, and on-par with BERT, on the BLiMP zero-shot linguistic knowledge benchmark, using 1/300 RoBERTa's pretraining data and can be trained on only 1 GPU in 4 days, trained for only 1 epoch.",
  "full_text": "Proceedings of the 27th Conference on Computational Natural Language Learning:\nVolume 2: The BabyLM Challenge, pages 317–326\nDecember 6-7, 2023 ©2023 Association for Computational Linguistics\nTowards more Human-like Language Models based on Contextualizer\nPretraining Strategy\nChenghao Xiao G Thomas Hudson Noura Al Moubayed\nDepartment of Computer Science\nDurham University\nAbstract\nTaking inspiration from human children learn-\ning, we pose a question: can a “baby language\nmodel” gradually internalize a concept by ex-\nposing itself to the concept in unlimited, often-\ntimes irrelevant contexts, and what this means\nto limited pretraining resource (both data-wise\nand GPU-wise).\nThroughout the study, we restrict our experi-\nments to two data-limited settings, 10M and\n100M tokens, which are respectively 1/3000\nand 1/300 to what were available to the train-\ning of RoBERTa. Our best performing training\nrecipe performs within 1.2% of RoBERTa, and\non-par with BERT, on the BLiMP zero-shot\nlinguistic knowledge benchmark, using 1/300\nRoBERTa’s pretraining data and can be trained\non only 1 GPU in 4 days, trained for only 1\nepoch.\n1 Introduction\nIn recent years, the success of pretrained language\nmodels has relied on scaling up both parameter\ncounts and the size of the datasets that models\nare exposed to, in order to improve performance.\nAccording to (Warstadt et al., 2023), the number\nof words that the modestly-sized language model,\nChinchilla (Hoffmann et al., 2022) goes through\n(1.4 trillion words), is equivalent to over 10000\nwords for every one word a 13-year-old child has\nheard in their entire life.\nIn this work, we take a contextualization per-\nspective to rethink, why can a human child build\nup their understanding of the world with an expo-\nsure to merely 2M-7M words per year (Gilkerson\net al., 2017), and without largely changing the cur-\nrent pretraining techniques, how can we facilitate\nthe learning of a language model to imitate such\nbehaviors to the greatest extent.\nWith many nuanced experimental findings, our\nmain findings can be summarized analogically as\none trick:\n“Learning to solve math problems in a history\nclass”.\nMetaphorically, exposing a language model to\ndatasets of different domains is like sending a kid\nto a kindergarden that teaches classes of diverse\ncontent. Taking the learning of math as an example,\na child does not only do math in a math class, nor\nis their math capability only aroused when they\nsee a math test paper. They do math in a math\nclass, at home, and during playing time. If a child\nis good at math in a math class, they theoretically\nshould be able to demonstrate their math abilities\nany time when presented with a real-life scenario\nthat requires these skills.\nWe argue that such ability should also apply\nto language models, and find that, exposing a\nlanguage model to knowledges of a domain sur-\nrounded by knowledges of that same domain, poses\na “contextualization trap”. This induces over-\nfitting to contexts, over-attendance to spuriously\nrelevant tokens, and thus under-exploitation of se-\nmantics signals in the limited data available.\nIn fact, if a language model can recover masked\nWikipedia texts surrounded by Wikipedia texts, it\nshould be no worse at recovering them when it is\n“watching” cartoons.\nWe find that, designing training recipes solely\nbased on this inspiration largely improves pretrain-\ning performance, enabling a baby language model\nto achieve similar performance to RoBERTa on\nzero-shot linguistic knowledge tasks, and competi-\ntive SuperGLUE performance, with less than 1/300\nof its pretraining data.\n2 Method: Contextualizer\nTo exploit the limited data available, we propose\nContextualizer, a framework to create more (the-\noretically unlimited) contexts that a fixed input is\nsurrounded by.\n317\n(2) Clean\n(4a) Context-fixed Padding:\nShuffle the padded (3) and\ndo not re-pad\n(3) Noisy padding:\nFirst Portion\n(4b) Context-augmented Padding:\nShuffle unpadded (1) then re-pad\n(1) Original Inputs\nMath History Politics Physics Break (Padding Tokens)\nFigure 1: Concept of Contextualizer. Assume we build a training set with four datasets of different domains (say\nmath, history, politics, and physics), each with one input. Clean Padding splits every input to different chunks, then\npads the end of each input with padding tokens, and does not allow mixing components of different inputs to a same\nchunk. Noisy Padding, on the other hand, allows different inputs to be padded to the same chunk. Context-fixed\nPadding simply takes the chunks padded by Noisy Padding and does a round of shuffling. Context-augmented\nPadding shuffles the original inputs every time and re-pads them, allowing original inputs to be truncated at diverse\npositions and joint with different contexts. In our best training configuration, we do (4b) 40 times.\n2.1 Recipes\nWe have designed two recipes to facilitate\nContextualizer. 1) Contextclean. Aligning with\nthe process of children learning, one would expect\nthat teaching a concept to a baby for the first time re-\nquires exposing them to the concept in a \"clean con-\ntext\", i.e., the context that the concept’s supposed\nto be in. 2) Contextnoisy. After a model/baby\nhas attained certain level of knowledge, we con-\nsider augmenting this knowledge by practicing the\nknowledge in different contexts. As a further intu-\nitive example, after a baby has remembered a quote\nfrom some cartoon character, they can repeat this\nsentence in a standalone manner in any context,\nand this does not require locating this sentence in a\ncontext with (max sequence length - quote length)\nof relevant cartoon dialogues any more.\nApart from intuitions from children learning,\nthe spirit of the recipes has also seen its empiri-\ncal ground in previous research. For one, research\non shortcut learning (Geirhos et al., 2020) has at-\ntributed vulnerability of a model’s prediction partly\nto its overfitting towards spurious correlation. Tak-\ning Tweet toxicity classification as an example, a\nmodel can easily learn its over-reliance on @ as\nan indicator for toxicity, because of the frequent\nappearance of @user in toxic tweets. Such vulner-\nability has hindered a real understanding towards\nthe semantics of a large amount of tokens. Our con-\ntextualization method has largely removed learning\nshortcuts, by truncating complete inputs at diverse\npositions, making tokens in the input unseen from\none another from time to time, while co-occurring\nwith unlimited contexts from other inputs (be them\nrelevant or irrelevant). This improves the model’s\nrobustness against irrelevant noise, while training\nits intra-input attentions to be activated by real rel-\nevant tokens.\n2.2 Implementation Details\nAs discussed, we process data into Contextclean\nand Contextnoisy on a high level. However, un-\nder the category of Contextnoisy, we have de-\nsigned three settings, namely, Noisy Padding, fol-\nlowed by either Context-fixed Padding and Context-\naugmented Padding (Figure 1). As we will show\nin later experiments, these techniques show large\nbehavioral and performance gap.\n318\nContextclean The concept of Contextclean is\nvery straightforward and only facilitated by one\nsetting: Clean Padding (Figure 1). Taking the orig-\ninal inputs, Clean Padding truncates and allocates\neach original input to different chunks, and extends\neach input with [pad] tokens, if the last portion of\nthe input is shorter than the max chunk length by\nitself.\nContextnoisy Contextnoisy, on the other hand, is\nfacilitated by three settings. As opposed to Clean\nPadding, techniques in Contextnoisy allow text\nfrom different original inputs to appear in a same\nchunk. 1) \"Noisy Padding: First Portion\" is used\nto create the first portion of noisy training set, in a\ndataset curriculum order (will be discussed later).\nIn this setting, datasets are first concatenated in\na pre-defined easy-to-difficult order. At the end\nof each input, the next input will follow immedi-\nately, instead of starting a new chunk. 2) Later\nportions of the training set are created by two op-\ntions: 2a) Context-fixed Padding directly takes\nthe first portion created by noisy padding, and shuf-\nfles them on a chunk level. This will only enable\ndifferent chunks to appear in different batches in\nlater training, but will not re-pad different contexts\nin a same chunk. In other words, content in a\nchunk always stays the same, but is just shuffled\nto different indexes in the training set portion. 2b)\nContext-augmented Padding is the most noisy\nsetting (and most beneficial, as will be shown). At\neach operation, it shuffles the original input order\nagain, and conducts Noisy Padding. In other words,\nContext-augmented Padding is in essence Noisy\nPadding without the dataset curriculum scheme.\nUsing Context-augmented Padding, we can theoret-\nically create n! training data examples by exhaust-\ning the order permutation of the original inputs,\nwhere n is the number of original inputs. As we\nwill show, this technique leads to the most perfor-\nmance gain, by allowing the model/baby to revisit\nthe same knowledge in many contexts, with dif-\nferent amounts of clean context available, and to\ndevelop different ‘perspectives’ to understand the\nsame knowledge.\nDatasets Our 10M and 100M datasets come\nfrom the two tracks of the BabyLM Challenge\n(Warstadt et al., 2023), including data sam-\npled from CHILDES, Switchboard, OpenSubtitles,\nBNC, QED, CBT, Children Stories, Gutenberg,\nSimple Wikipedia, and Wikipedia, covering chil-\ndren speech, transcribed text, children stories, and\nWikipedia data. In order to further imitate human\nlearning, we apply a rough dataset-level curricu-\nlum for clean padding and the first portion of noisy\npadding, by manually arranging the order of im-\nporting the 10 datasets.\nNotably, we did not apply any annotations or or-\ndering to input-level data, but only arranged the\ntraining set at the dataset level following com-\nmonsense understanding (such as children speech\ndatasets at the beginning, followed by children sto-\nries, and lastly Wikipedia datasets), further con-\nfirmed by manual inspection of linguistic statistics.\nWe leverage the TCT toolkit (Simig et al., 2022)\nto generate these statistics. For every dataset, we\nfirst compute the statistics on sentence level-inputs,\nand then average all outputs on the dataset level.\nAs a further note, sentence-level statistics are just\nused to compute dataset-level statistics to roughly\nconfirm our dataset order, and none of these statis-\ntics provides signals to the training inputs in\nany form. Also, there is no evidence that apply-\ning dataset-level curriculum in the first portion is\nuseful to our method, but we would like to provide\na starting point for future studies to combine our\nmethod with more human-like data orders.\nTable 1 presents statistics of four of the repre-\nsentative properties: Age of Acquisition (Mean),\nAge of Acquisition (Max), Flesch Reading Ease,\nand Flesch-Kincaid Grade Level. We also ran com-\nputations on Word per Sentence, Average Word\nLength - syllables, Average Word Length - letters,\ntype-token ratio computed over all words, lexical\ndiversity, mean meaningfulness, etc. We find that,\nalbeit we cannot find a dataset order that makes\nall linguistic statistics monotonically decrease or\nincrease, statistics computed on all linguistic prop-\nerties display a strong correlation. These statistics\nalso align with one’s common sense.\nFollowing these inspections, the final data or-\nder is determined to be: CHILDES, Switchboard,\nOpenSubtitles, BNC, QED, CBT, Children Stories,\nGutenberg, Simple Wikipedia, and Wikipedia. For\nContextclean and the first portion of Contextnoisy,\nall datasets are concatenated in this order before\nprocessing. They are then shuffled before creating\nlater portions of Contextnoisy.\n2.3 Other technical Details\nTokenizer For 10M and 100M settings, we train\nseparate BPE tokenizers (Sennrich et al., 2016)\n319\nCHILDES Switchboard OpenSub. BNC QED CBTChild.StoriesGutenbergSim.Wiki. Wiki.\nAge of Acquisition (Mean) 4.38 4.69 4.76 4.72 5.01 4.89 4.84 5.49 5.84 5.79\nAge of Acquisition (Max) 5.43 6.80 6.66 6.94 7.88 8.62 9.58 9.38 9.99 11.37\nFlesch Reading Ease 105.41 101.19 94.83 96.88 85.61 84.51 83.00 79.07 58.51 62.68\nFlesch-Kincaid Grade Level -0.28 1.03 1.53 2.15 3.86 6.19 6.80 4.35 7.67 9.30\nTable 1: Dataset-level statistics of selected linguistic features, computed with TCT toolkit (Simig et al., 2022).\nThese statistics align well with common sense, and confirm our manual dataset order.\nfrom scratch with a fixed vocabulary size of 50k,\nin line with the original RoBERTa. We find that a\nvocabulary size of 10k and 30k degrades the perfor-\nmance of most BLiMP tasks (except on Irregular\nForms, noticeably) in inital experiments with 10M\ndatasets.\nArch./Size/Init. We use the architecture and pa-\nrameter size of RoBERTa-base (Liu et al., 2019),\nand initialize the models with random weights.\nTraining Cost We fix the computation cost for\nmodels under the same track to be roughly the\nsame. For 10M track, every model takes around\n6-8 hours on a single RTX 3090; and for 100M\ntrack, every model takes around 3-4 days. The only\nfactor that brings this around 10% - 20% training\ntime difference is whether we add a round or two\nof Contextclean before or after the training with\nContextnoisy. We will explain how we decide the\ncomputation cost in experiment setting section.\nChunk Length For 10M track, we set the max\nsequence length of each padded input to be 64\n(i.e., max length of input chunks), and for 100M\ntrack, we set it to 128. We find that a max chunk\nlength of 128 degrades the performance of mod-\nels trained on 10M corpus on BLiMP tasks. No-\ntably, in initial tokenization before post-processing\nwith Contextualizer, we do not impose any\nmax sequence length, and keep every token avail-\nable before context augmentation padding with\nContextualizer, i.e., the “max sequence length”\nonly applies to padding complete inputs to chunks.\nWe hypothesize that there exists a training\nstability-oriented scaling law between corpus size\nand max sequence length to be padded to, due to\nthe difficulty of learning robust long-range depen-\ndencies with limited amount of training examples.\nTraining Objectives We stick to MLM objective\nwith 15% masking rate, and use dynamic masking\n(Liu et al., 2019). For all stratigies we use, we\nconduct random masking on the tokenized inputs\non the fly (in training loop instead of before).\nInterestingly, in initial experiments, we find that\nusing reconstruction loss instead of MLM loss im-\nproves performance of checkpoints in early phase\nof training, and the isotropy of the embeddings\nencoded (Xiao et al., 2023) (also better zero-shot\nperformance on sts-b). However, the gap could be\nbridged in later training. We leave further explo-\nration of this phenomenon for future work.\nWe have also tried combining mlm loss and un-\nsupervised contrastive loss (Gao et al., 2021b), and\nfind unstable improvements (better on tasks related\nto representation - such as QQP and NLI tasks, and\nopposite otherwise). We have also tried masking\nrate curriculum and contrastive loss weighting cur-\nriculum, and find unstable improvements as well.\nTherefore, we decide to only use a MLM loss\nwith a static masking rate to focus on the study of\ncontextualization.\nOther Dataset Pre-processing We include a few\nextra pre-processing steps for all experiments. For\nContextclean, we filter all original tokenized inputs\nthat have only 2 tokens ([cls] and [sep] tokens) to\nmake sure that the processed chunks later are not\nempty strings with only [cls], [sep] and the rest\nbeing all [pad] tokens. For Contextnoisy, we only\nkeep original tokenized inputs with at least 5 tokens\nbefore conducting noisy padding. This is because\ninputs with too few tokens are not self-contained.\nFor instance, predicting “[cls] Hello! [sep]” with\n“hello” masked would only provide signals for the\nmodel’s prediction to converge to token frequency-\nbased probability distribution of the corpus (Chang\nand Bergen, 2022), and it is not useful for our noisy-\ncontext strategy. In terms of the datasets, we find\nthat the Gutenberg dataset provided officially by\nBabyLM contains nextline splits in every paragraph\nonce each line reaches certain length, and it is not\nideal - because after tokenization, this would give\nunwanted [sep] tokens within a complete sentence\nthat is not supposed to be split. Thus, we remove\nall nextline splits within same paragraphs. We find\nperformance gains in initial experiments for all pre-\n320\nTask→Model↓ Anaph.\nAgr.\nAgr.\nStruct.Bndg. Ctrl./Raise. D-NAgr. Ell. F-G. Irreg.\nForms IslandEffects NPILic. Qnts. S-VAgr. MainAvg.\nBaseline 89.50 71.30 71.00 67.10 93.10 83.80 68.00 89.60 54.50 66.30 70.30 76.20 75.06\n1-40 n (ours) 96.01 78.84 76.68 74.52 96.45 91.97 76.13 90.48 70.67 71.99 65.20 83.41 81.03\n40-1 n (ours) 97.49* 79.58 79.98*78.26 96.80 92.73**83.94*94.50* 78.18 81.22 73.31**90.35 85.53\n40-1 cnc (ours)97.55* 80.15* 77.06 80.11 96.57 92.26**84.97* 90.53 80.12** 83.71* 73.80**89.63 85.54\nBERT 97.03 79.62 81.23 81.02 96.83 89.03 81.85 94.30 79.56 84.97 69.91 91.80 85.60\nRoBERTa 97.70 83.04 79.21 81.90 97.28 92.15 89.39 95.67 79.67 82.58 70.40 91.47 86.70\nTable 2: BLiMP Results of 100M recipes. Bold Numbers represent the best performance among our training\nrecipes. Underlined Numbers represent second best. * denotes that the performance outperforms either BERT or\nRoBERTa. ** denotes that the performance outperforms both BERT and RoBERTa. Notably, our performances on\nEllipsis, Island Effects, and Quantifiers outperform both BERT and RoBERTa, using respectively under 1/40 and\n1/300 of their training data.\nprocessing steps stated above.\nExperiment Settings We conduct experiments\nwith combinations of the above described\nContextualizer data processing settings.\nAs stated, we fix the computation cost of exper-\niments in the same track to be roughly the same.\nThe exact cost is decided to align with the epoch\nnumber used in RoBERTa. We calculated that\nRoBERTa was roughly trained for 40 epochs on\ntheir training set. Therefore, for the noisy training\nset created by Context-fixed Padding, we train the\nmodel for 40 epochs (in result tables, we call this\nsetting “1-40”). Then to align with this computa-\ntion cost for context-augmented experiments, we\nperform Context-augmented Padding for 39 times\non top of Noisy Padding first portion, creating a\nnoisy training set 40 times larger than the context-\nfixed training set, and train it for only 1 epoch (we\nrefer to this as \"40-1\" in result tables). Further-\nmore, we consider adding a round or two of Clean\nPadding data before or after the noisy data. This\ntypically brings around 10% to 20% computation\ncost difference, since clean padding data has more\nexamples (For instance, if we have 10 original in-\nputs, each with 10 tokens, they could fit in one\nsingle chunk using Noisy Padding under a max\nchunk length of 128, but would create 10 chunks,\nusing Clean Padding).\nConcretely, “1-40 n” in result tables means: we\ntrain the model only on 1 portion of noisy data\nfor 40 epochs. This is achieved by doing Noisy\nPadding to create one portion of data, and just shuf-\nfle this portion in the rest of the 39 epochs (essen-\ntially 39 times of “context-fixed padding”). On\nthe other hand, \"40-1 n\" means that we create the\nfirst portion of data, and create 39 more portions\nwith context-augmented padding, training on this\n40-times larger training set for 1 epoch. “c” in the\nresult tables denotes the number of clean data con-\ncatenated before and after noisy data. For instance,\n“1-40 ccn” denotes first training on clean data twice,\nthen 1 portion of noisy data for 40 epochs.\n3 Results\nWe evaluate our models on BLiMP, SuperGLUE\nand MSGS tasks (Warstadt et al., 2020a; Wang\net al., 2019; Warstadt et al., 2020b; Gao et al.,\n2021a). Notably, we use the versions processed\nby BabyLM, where each word has appeared in the\n10M training set at least twice.\n3.1 BLiMP Results\n100M Track For the 100M track (Table 2), we\ncan clearly see the benefits brought by Context-\naugmented Padding (40-1 n), outperforming its\nContext-fixed Padding counterpart (1-40 n) by a\nlarge margin on the zero-shot BLiMP benchmark,\nand outperforming BabyLM official baseline for\nover 10 absolute percentage points. The model\ntrained with Context-augmented Padding outper-\nforms Context-fixed Padding on all BLiMP tasks,\nshowing no trade-offs in introducing more noise\nfrom mixing contexts in the same inputs in the\n100M setting.\nAdding a round of clean data before and after\nnoisy data (40-1 cnc) improves tasks like Agree-\nment Structure, Control/Raising, Island Effects,\nand NPI Licensing, but degrades the model’s perfor-\nmance largely on Irregular Forms, leading to only\na small gain on average performance of all tasks.\nWe hypothesize that there might exist a better data\nshuffling strategy when combining noisy and clean\ndata, such as doing another round of training set-\nlevel shuffling after concatenating clean and noisy\ndata. We leave this for future work.\nNotably, our best performing models are on-\n321\nModel CoLA SST-2 MRPC QQP MNLI MNLI-mm QNLI RTE BoolQ MuiltiRC WSCMain Avg.\nStrict-Small (10M Track)\nBaseline 25.80 87.00 79.20 73.70 73.20 74.00 77.00 61.60 66.30 61.40 61.40 67.33\n1-40 cnc (Ours)38.70 89.76 79.55 84.19 73.61 74.89 83.01 52.53 66.39 61.23 61.45 69.58\nStrict (100M Track)\nBaseline 45.30 88.60 80.50 78.50 68.70 78.00 82.30 51.50 59.90 61.30 61.40 68.73\n40-1 cnc (Ours)56.09 90.55 83.74 85.63 77.92 78.36 83.60 53.54 68.46 64.40 59.04 72.85\nModel CR LC MV RP SC CR-LC CR-RTP MV-LC MV-RTP SC-LC SC-RP Main Avg.\nStrict-Small (10M Track)\nBaseline 43.10 100.0 97.70 76.70 86.20 -28.30 -77.70 -99.30 -79.40 16.30 -45.00 8.21\n1-40 cnc (Ours)75.68 100.00 99.93 99.9685.67 -46.28 -89.12 -100.00 -62.37 13.40 -37.24 12.69\nStrict (100M Track)\nBaseline 74.7 100.0 99.9 100.0 59.2 -89.0 -91.2 -99.8 -15.3 -57.7 -39.2 3.78\n40-1 cnc (Ours)96.48 100.00 100.00 100.00 96.68 88.03 71.76 -32.02 30.91 21.97 -35.93 57.99\nTable 3: SuperGLUE and MSGS results for 10M track and 100M track, comparing our selected models and\nbaselines. Except CoLA (MCC), MRPC (F1) and QQP (F1), all other scores are Accuracy.\npar with BERT, and is within a 1.2% gap with\nRoBERTa, using 1/40 and 1/300 of their train-\ning data respectively. This validates that, us-\ning Context-augmented Padding, we can create\nmore pseudo-data that behaves closely like the real\ndata. In our case, by running Context-augmented\nPadding 39 times, we actually create a 4B-token\ndataset using the 100M-token dataset. This is on-\npar with the training set with BERT, and actually\ngives us a model that behaves on-par with BERT\non BLiMP. Given enough compute resource, we\nwould expect running the augmentation 299 times\nwould give us a model that performs more on-par\nwith RoBERTa.\n10M Track We find that, the best strategy for\n10M deviates from the best strategy for 100M.\nWe suggest this is because Context-Augmented\npadding dilutes the impact of informative datasets\nsuch as Wikipedia with noise (children mum-\nbling, onomatopoeia data) from datasets such as\nCHILDES. Therefore, the decision for the optimal\n10M strategy has been more difficult and nuanced.\nWe leave the full 10M BLiMP results of 7 strate-\ngies that we have explored in Appendix A, and\nonly present the SuperGLUE and MSGS results for\none representative strategy (1-40 cnc, created by\nContext-fixed Padding) in the next section.\n3.2 SuperGLUE and MSGS Results\nTable 3 presents the results of SuperGLUE and\nMSGS tasks. Due to compute constraints, we\nonly compare one of our models in each track\nwith the BabyLM baselines. Our hyperparameter\nsearch space only concerns learning rate and batch\nsize, with the rest of hyperparameters following\nBabyLM’s offical repo. With limited compute re-\nsource (evaluating all fine-tuning tasks once takes\naround 13-15 hours on 2 RTX 3090s), we have only\nexplored the combinations of {5e−5, 64}, {3e−5,\n32} and {2e −5, 16}, instead of exhaustive permu-\ntations of them. This follows the empirical intuition\nthat, smaller batch sizes lead to more unstable opti-\nmization, and should be paired with small learning\nrates. We find that smaller learning rates and batch\nsizes generally work better for small datasets like\nCoLA, MultiRC and RTE.\nFor 100M track, our model outperforms baseline\nmodels on all SuperGLUE and MSGS tasks. On av-\nerage, our model outperforms baselines by 4.1 and\n54.2 absolute percentage points on SuperGLUE\nand MSGS respectively.\nFor 10M track, our methods also provide com-\npetitive results, outperforming baselines by 2.3 and\n4.5 absolute percentage points on SuperGLUE and\nMSGS respectively.\nThis again confirms the universal effectiveness\nof our recipe pool, and also provides support for\nour hypothesis when evaluating BLiMP that, 10M\ndatasets seem to work less well with our method\ncompared to 100M dataset, because of the less self-\ncontained original inputs provided by informative\ndatasets like Wikipedia.\n3.3 BabyLM challenge\nThe resultant models are submitted as part of\nthe BabyLM challenge. Considering all results,\nwe submitted the 1-40 cnc model for 10M track,\nand 40-1 cnc model for 100M track, and named\n322\nTasks→\nModels↓ BLiMP BLiMP-sup SuperGLUE MSGS Weighted Avg.\nContextualizer-RoBERTa-base-10M-v179.24 62.30 69.58 12.69 60.54\nContextualizer-RoBERTa-base-10085.54 63.35 72.85 57.99 72.96\nTable 4: Results on Dynabench Leaderboard. Notably, the BabyLM official evaluation has further included 5\nBLiMP supplementary tasks, denoted as BLiMP-sup here.\nthem Contextualizer-RoBERTa-base-10M-v11\nand Contextualizer-RoBERTa-base-100M2.\nTable 4 presents the official BabyLM Challenge\nresults of our two models on the Dynabench Leader-\nboard.\n4 Inner-workings Analysis\nAs simple as learning the same things repeatedly\nin different, oftentimes irrelevant contexts is, our\nmethod achieves surprising results without chang-\ning other technical details. It is a natural question\nto wonder how the method has facilitated better\nlearning.\nAs partly discussed in the Recipe section, we hy-\npothesize that the inner-working of this method is\nlargely relevant to mitigating shortcut learning, and\nspurious correlation. For instance, if the same data\nkeeps being displayed to the models throughout\nall epochs, the model might tend to overfit to the\nco-occurrence of words in certain inputs, or even\nsimply remember the sequence.\nFor instance, without our method, if a padded\ninput “[cls] Figure 1 is an example figure for the\nconcept. [cls] This is a completely irrelevant sen-\ntence. ”is seen by the model 40 times, the model\nmight incorrectly learn a rule that “example” is two\ntokens before “for”, or even depends on “irrelevant”\nin the irrelevant chunk padded to the same input,\ndue to stochasticity in optimization, instead of re-\nlying “example” on the information in “Figure 1”\nand “for the concept”.\nBy contrast, our method makes sure 1) an input\nis padded with a different input in every portion,\nso it will not be padded with the same “This is\na completely random sentence.” and seen by the\nmodel multiple times. This way, the model learns\nto focus attention within one document, instead of\n“peeking” tokens in other text chunks that happen\nto be padded into the same input with them. 2) an\ninput is cropped at different positions in different\nportions of the data, making sure the model utilize\n1Dynabench ID: 1450\n2Dynabench ID: 1343\ninformation in a flexible way, instead of building\nover-reliance on certain shortcuts. As an example,\nin one portion, it might be \"an example figure for\nthe concept. [cls] sentences from dataset 1\"; and\nin a different portion, it might be \"sentences from\ndataset 5 [cls] Figure 1 is an example\".\nWe have conducted a proof-of-concept exper-\niment to support this hypothesis. We take the\n100M 1-40 n and 40-1 n models respectively. Note\nthat the 1-40n model sees the same padded inputs\n40 times, and is theoretically prone to shortcut\nlearning; while the 40-1 n model is expected to\nlearn more actual dependencies among tokens, as\nit keeps seeing the different combinations of in-\nputs. Note that they are both trained with 15%\nmasking probability. We take the padded training\nset that is exposed to the 1-40 model, and mask\n{50%, 85%, 95%}tokens in every input respec-\ntively, we then compare the mlm loss produced\nby both models. Masking more than 50% of to-\nkens should have already made most documents\nlose its semantics. If the mlm loss produced by\n1-40 n model is much lower than 40-1 n model, we\ncan conclude that, it presents certain levels of over-\nfitting and shortcut learning, shown by its ability to\nrecover more tokens, with very broken evidence.\nMask Prob.→\nModel ↓ 50% 85% 95%\n1-40 n 2.20 4.53 6.23\n40-1 n 2.67 4.69 5.81\nTable 5: Memory Analysis. Both models are trained\nwith 15% mask probability. We get the mlm losses with\ndifferent mask probabilities on data exposed to 1-40 n\nmodel in training.\nAs Table 5 shows, this pattern clearly holds. For\n50% and 85%, 1-40n model produces much lower\nmlm losses, showing its imposed memory on the\ncorpus.\nHowever, when the masking rate is increased\nto 95%, the 40-1 n model produces a lower loss.\nWe speculate that this is because with 95% tokens\n323\nmasked (leaving around 6 tokens in every 128-\ntoken input), the documents are extremely broken,\nand knowledge about basic grammar depending on\nthese ∼6 tokens are beneficial to recover parts\nof the tokens. Therefore, 40-1 n shows its robust\ngrammar understanding towards more ubiquitous\ngrammar phenomena.\n5 Discussions\nDue to limited time and compute resources, we\nposition this work as a humble first step in studying\ncontextualization as a data augmentation method\nfor more human-like learning. We have proved the\neffectiveness of this context augmentation frame-\nwork with its strong zero-shot linguistic knowledge\nperformance gain, and leave design of other de-\ntails such as the optimal way to tokenize, process\nand train the [cls] token for better performance on\ndownstream fine-tuning tasks, as future work.\nMoreover, while this work is largely restricted\nto studying pretraining of encoder models because\nof limited compute resources, we envision that the\ngeneral findings are transferable to, or at least worth\nattention in several settings, including:\n1) Multilingual Models. In the training of multi-\nlingual models, does practicing multiple languages\nin the same input chunk improve performance not\nonly in code-switching scenarios, but also reflect\nin all languages individually? Does this perfor-\nmance manifest differently in low-resource lan-\nguages, than in high-resource languages?\n2) Generative LLMs. Does doing multiple in-\nstructions at the same time not only improve a\nLLM’s multi-tasking abilities, but also improve\nabilities of individual tasks? How will this affect\nhallucinations?\nLimitations\nAs discussed, our work is restricted to the study\nof encoder models. However, we envision cer-\ntain transferability of our conclusions to encoder-\ndecoder models and decoder-only models, and\nleave these for future work.\nMoreover, again due to compute constraints, we\nhave not been able to study the scaling law between\nmodel size and the number of times to augment the\ndata with our Context-augmented Padding opera-\ntion. We envision this to be very interesting, and\nSOTA-result-promising, as discussed in § 3.1.\nReferences\nTyler A Chang and Benjamin K Bergen. 2022. Word ac-\nquisition in neural language models. Transactions of\nthe Association for Computational Linguistics, 10:1–\n16.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021a. A\nframework for few-shot language model evaluation.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In 2021 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2021,\npages 6894–6910. Association for Computational\nLinguistics (ACL).\nRobert Geirhos, Jörn-Henrik Jacobsen, Claudio\nMichaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. 2020.\nShortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665–673.\nJill Gilkerson, Jeffrey A Richards, Steven F Warren, Ju-\ndith K Montgomery, Charles R Greenwood, D Kim-\nbrough Oller, John HL Hansen, and Terrance D Paul.\n2017. Mapping the early language environment using\nall-day recordings and automated analysis. American\njournal of speech-language pathology , 26(2):248–\n265.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725.\nDaniel Simig, Tianlu Wang, Verna Dankers, Peter Hen-\nderson, Khuyagbaatar Batsuren, Dieuwke Hupkes,\nand Mona Diab. 2022. Text characterization toolkit\n(tct). In Proceedings of the 2nd Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 12th International Joint\nConference on Natural Language Processing: System\nDemonstrations, pages 72–87.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\n324\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nAlex Warstadt, Aaron Mueller, Leshem Choshen,\nEthan Gotlieb Wilcox, Chengxu Zhuang, Juan Ciro,\nRafael Mosquera, Adina Williams, Bhargavi Paran-\njabe, Tal Linzen, and Ryan Cotterell. 2023. Findings\nof the 2023 BabyLM Challenge: Sample-efficient\npretraining on developmentally plausible corpora. In\nProceedings of the 2023 BabyLM Challenge. Associ-\nation for Computational Linguistics (ACL).\nAlex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mo-\nhananey, Wei Peng, Sheng-Fu Wang, and Samuel R\nBowman. 2020a. Blimp: The benchmark of linguis-\ntic minimal pairs for english. Transactions of the\nAssociation for Computational Linguistics , 8:377–\n392.\nAlex Warstadt, Yian Zhang, Haau Sing Li, Haokun Liu,\nand Samuel R Bowman. 2020b. Learning which\nfeatures matter: Roberta acquires a preference for\nlinguistic generalizations (eventually). In 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, pages 217–235. Associa-\ntion for Computational Linguistics (ACL).\nChenghao Xiao, Yang Long, and Noura Al Moubayed.\n2023. On isotropy, contextualization and learning dy-\nnamics of contrastive-based sentence representation\nlearning. In Findings of the Association for Compu-\ntational Linguistics: ACL 2023, pages 12266–12283.\nA 10M BLiMP results\nThe decision for which strategy to use for 10M\ntrack has been more nuanced and difficult. As\nshown in Table 6, we could see that Context-\naugmented Padding models (40-1) still outperform\ntheir Context-fixed Padding counterparts (1-40).\nHowever, this gap is not as large as the results in\n100M, which as we discussed in main sections,\nis probably because of that, there exists much\nless self-contained data to resist against the noise\nbrought by less semantic data (like children mum-\nbling).\nAs shown in Table 2, using pure noisy data (en-\ntries with only \"n\" but no \"c\"), Context-augmented\nPadding (40-1 n) still outperforms Context-fixed\nPadding (1-40 n). However, when combining Clean\nPadding data, it seems to be detrimental to Context-\naugmented Padding data, while is consistently con-\ntributing to Context-fixed Padding data. We hy-\npothesize that, mixing Clean data and Noisy data\nin essence is a context-augmenting operation itself.\nAnd as we discussed, for 10M track, a moderate\namount of noisy context, instead of too much, is\nbetter.\nWe have also run some experiments on fine-\ntuning tasks on all model entries, and decided to use\n\"1-40 cnc\" for our final submission. Even though\n\"40-1 n\" provides the best BLiMP scores, it seems\nto rely on tasks with unstable behaviours such as\nirregular forms and quantifiers.\n325\nTask→Model↓ Anaph.\nAgr.\nAgr.\nStruct.Bndg. Ctrl./Raise. D-NAgr. Ell. F-G. Irreg.\nForms IslandEffects NPILic. Qnts. S-VAgr. MainAvg.\nBaseline 81.50 67.10 67.30 67.90 90.80 76.40 63.50 87.40 39.90 55.90 70.50 65.40 69.47\n1-40 n 90.18 74.73 69.07 71.90 94.59 86.49 72.74 88.40 59.60 67.70 69.45 84.17 77.42\n1-40 cn 91.62 76.13 69.83 71.23 94.51 89.03 77.30 82.29 58.67 67.04 72.21 83.96 77.82\n40-1 n 92.02 76.02 70.66 73.07 95.66 82.39 77.48 93.13 61.47 66.75 81.43 85.56 79.64\n40-1 cn 93.15 76.33 70.73 73.93 96.96 82.91 77.90 87.74 63.23 66.50 77.77 87.35 79.54\n40-1 ccn 92.38 77.30 70.97 74.61 95.48 82.56 78.20 86.77 62.03 62.92 71.90 87.05 78.51\n1-40 cnc 92.79 76.33 71.64 72.56 95.65 89.15 75 4.99 87.63 61.47 69.78 72.46 86.43 79.24\n40-1 cnc 94.12 74.48 67.60 71.45 95.12 82.79 76.38 91.55 58.67 74.05 82.17 83.34 79.31\nTable 6: BLiMP Results of 10M recipes. Clearly, while all of our strategies outperform the baseline by a large\nmargin, results are more nuanced and it is not that straightforward to see which strategy is the best.\n326",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7512398958206177
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7287319302558899
    },
    {
      "name": "Language model",
      "score": 0.5795145034790039
    },
    {
      "name": "Natural language processing",
      "score": 0.5722150802612305
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5393713712692261
    },
    {
      "name": "Training set",
      "score": 0.532475471496582
    },
    {
      "name": "Language acquisition",
      "score": 0.4217376410961151
    },
    {
      "name": "Machine learning",
      "score": 0.32255101203918457
    },
    {
      "name": "Psychology",
      "score": 0.18178033828735352
    },
    {
      "name": "Mathematics education",
      "score": 0.12451499700546265
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 2
}