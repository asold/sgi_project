{
  "title": "Enhancing semantical text understanding with fine-tuned large language models: A case study on Quora Question Pair duplicate identification",
  "url": "https://openalex.org/W4406262262",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2136453959",
      "name": "Sifei Han",
      "affiliations": [
        "Children's Hospital of Philadelphia"
      ]
    },
    {
      "id": "https://openalex.org/A2147048979",
      "name": "Lingyun Shi",
      "affiliations": [
        "Children's Hospital of Philadelphia"
      ]
    },
    {
      "id": "https://openalex.org/A2803608570",
      "name": "Fuchiang (Rich) Tsui",
      "affiliations": [
        "Children's Hospital of Philadelphia",
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2136453959",
      "name": "Sifei Han",
      "affiliations": [
        "Children's Hospital of Philadelphia"
      ]
    },
    {
      "id": "https://openalex.org/A2147048979",
      "name": "Lingyun Shi",
      "affiliations": [
        "Children's Hospital of Philadelphia"
      ]
    },
    {
      "id": "https://openalex.org/A2803608570",
      "name": "Fuchiang (Rich) Tsui",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3002474999",
    "https://openalex.org/W3213718671",
    "https://openalex.org/W6838865847",
    "https://openalex.org/W2081497813",
    "https://openalex.org/W1851422430",
    "https://openalex.org/W4303438740",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W6858023062",
    "https://openalex.org/W4210800464",
    "https://openalex.org/W4400646305",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3080427797",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3014140399",
    "https://openalex.org/W2786055572",
    "https://openalex.org/W1996211074",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2808633496",
    "https://openalex.org/W3202929920",
    "https://openalex.org/W3158481101",
    "https://openalex.org/W2808308446",
    "https://openalex.org/W4205365495",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2925863688",
    "https://openalex.org/W3204150987",
    "https://openalex.org/W4321472314",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4285045845",
    "https://openalex.org/W2768228940",
    "https://openalex.org/W4210475840",
    "https://openalex.org/W6626659314",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3213730158",
    "https://openalex.org/W2085281262",
    "https://openalex.org/W1584308190",
    "https://openalex.org/W4307479172",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W6785708172",
    "https://openalex.org/W6811243207",
    "https://openalex.org/W4292957286",
    "https://openalex.org/W2963082289",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W1020065631",
    "https://openalex.org/W3104216863",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W2964105864",
    "https://openalex.org/W4226128078",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385570056",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3168997536"
  ],
  "abstract": "Semantical text understanding holds significant importance in natural language processing (NLP). Numerous datasets, such as Quora Question Pairs (QQP), have been devised for this purpose. In our previous study, we developed a Siamese Convolutional Neural Network (S-CNN) that achieved an F1 score of 82.02% (95% C.I.: 81.83%-82.20%). Given the growing attention toward large language models (LLMs) like ChatGPT, we aimed to explore their effectiveness in text similarity tasks. In this research, we leveraged 5 pretrained LLMs, conducted various fine-tuning approaches (prompt engineering, n-shot learning, and supervised learning using the low-rank adaptation [LoRA]), and compared their performance using F1 score. To ensure a fair comparison, we followed our previous study’s design and dataset by employing a 10-fold cross-validation for supervised model training and evaluation. Additionally, we conducted a secondary study by introducing a recent larger LLM with 70B parameters and comparing it with the 7B model using the GLUE benchmark, and both models were finetuned with the corpus. The fine-tuned LLaMA model with 7B parameters (qLLaMA_LoRA-7B) using 100,000 QQP corpus yielded the best results, achieving an F1 score of 84.9% (95% C.I.: 84.13%-85.67%), which outperformed the Alpaca_LoRA-65B (finetuned based on LLaMA-65B) (F1: 64.98% [64.72%-65.25%]; P&lt;0.01) and had a 3% improvement compared to our previously published best model, S-CNN. The finetuned LLaMA3.1-70B (qLLaMA3.1_LoRA-70B) with 70B parameters (F1: 74.4%) outperformed the qLLaMA_LoRA-7B (F1: 71.9%) using the GLUE benchmark. The study demonstrated an effective LLM finetuning framework, which highlights the importance of finetuning LLMs for improved performance. Our task-specific supervised finetuning demonstrated improved LLM performance compared to larger pretrained models with or without n-shot learning; moreover, finetuning a larger LLM further improved performance compared to finetuning a smaller LLM. Our LLM-based finetuning framework may potentially improve various document similarity tasks, such as matching resumes with job descriptions, recommending subject-matter experts, or identifying potential reviewers for grant proposals or manuscript submissions.",
  "full_text": "RESEA RCH ARTICL E\nEnhancing semantical text understanding\nwith fine-tuned large language models: A case\nstudy on Quora Question Pair duplicate\nidentification\nSifei Han\nID\n1\n, Lingyun Shi\n1\n, Fuchiang (Rich) Tsui\nID\n1,2,3,4\n*\n1 Department of Biomedic al and Health Informatics , Tsui Laboratory, Children ’s Hospital of Philadelph ia,\nPhiladelph ia, PA, United States of America, 2 Department of Anesthesi ology and Critical Care, Children’s\nHospital of Philadelph ia, Philadelph ia, PA, United States of America, 3 Department of Anesthesi ology and\nCritical Care, University of Pennsylvan ia Perelm an School of Medicine, Philadelph ia, PA, United States of\nAmerica, 4 Department of Biostatist ics, Epidem iology and Informatics , University of Pennsylvan ia Perelman\nSchool of Medicine, Philadelph ia, PA, United States of America\n* tsuif@ chop.edu\nAbstract\nSemantical text understanding holds significant importance in natural language processing\n(NLP). Numerous datasets, such as Quora Question Pairs (QQP), have been devised for\nthis purpose. In our previous study, we developed a Siamese Convolution al Neural Network\n(S-CNN) that achieved an F1 score of 82.02% (95% C.I.: 81.83%-82.20%). Given the grow-\ning attention toward large language models (LLMs) like ChatGPT, we aimed to explore their\neffectiveness in text similarity tasks. In this research, we leveraged 5 pretrained LLMs, con-\nducted various fine-tuning approaches (prompt engineering, n-shot learning, and super-\nvised learning using the low-rank adaptation [LoRA]), and compared their performance\nusing F1 score. To ensure a fair comparison, we followed our previous study’s design and\ndataset by employing a 10-fold cross-validation for supervised model training and evalua-\ntion. Additionally, we conducted a secondary study by introducing a recent larger LLM with\n70B parameters and comparing it with the 7B model using the GLUE benchmark, and both\nmodels were finetuned with the corpus. The fine-tuned LLaMA model with 7B parameters\n(qLLaMA_LoRA-7B) using 100,000 QQP corpus yielded the best results, achieving an F1\nscore of 84.9% (95% C.I.: 84.13%-85.67% ), which outperforme d the Alpaca_LoRA-65B\n(finetuned based on LLaMA-65B) (F1: 64.98% [64.72%-65.25%]; P<0.01) and had a 3%\nimprovement compared to our previously published best model, S-CNN. The finetuned\nLLaMA3.1-70B (qLLaMA3.1_LoRA-70B ) with 70B parameters (F1: 74.4%) outperformed\nthe qLLaMA_LoRA-7B (F1: 71.9%) using the GLUE benchmark. The study demonstrated\nan effective LLM finetuning framework, which highlights the importance of finetuning LLMs\nfor improved performance. Our task-specific supervised finetuning demonstrate d improved\nLLM performance compared to larger pretrained models with or without n-shot learning;\nmoreover, finetuning a larger LLM further improved performance compared to finetuning a\nsmaller LLM. Our LLM-based finetuning framework may potentially improve various docu-\nment similarity tasks, such as matching resumes with job descriptions, recommendin g\nPLOS ONE\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 1 / 15\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Han S, Shi L, Tsui F(Rich) (2025)\nEnhancing semantical text understandi ng with fine-\ntuned large language models: A case study on\nQuora Question Pair duplicate identifica tion. PLoS\nONE 20(1): e0317042. https://doi.o rg/10.1371/\njournal.pone .0317042\nEditor: Sohaib Mustafa, Beijing Univers ity of\nTechnology , CHINA\nReceived: December 29, 2023\nAccepted: December 18, 2024\nPublished: January 10, 2025\nCopyright: © 2025 Han et al. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: All the data are\navailable from Kaggle Quora Question Pairs\ndataset: https://www.k aggle.com/ c/quora-\nquestion-pa irs/data.\nFunding: The Children’s Hospita l of Philadelphi a.\nThe funders had no role in study design, data\ncollection and analysis , decision to publish, or\npreparation of the manuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nsubject-matter experts, or identifying potential reviewers for grant proposals or manuscript\nsubmissions.\n1. Introduction\nSemantical text understanding is a fundamental aspect of natural language processing (NLP)\ntasks, enabling machines to comprehend and extract meaning from textual data. It plays a piv-\notal role in various applications, including question pair classification [1], document similarity\nmeasurement [2], and content matching [3]. In this study, we focus on the duplicate identifica-\ntion of Quora Question Pair (QQP) [4] and propose a novel approach leveraging the power of\nlarge language models (LLMs) [5] for improved performance.\nQuora, a well-known question-and-answer platform, grapples with the issue of pinpointing\nduplicate questions to improve user experience and avoid repetition. Conventional methods\nused for duplicate (similarity) detection commonly rely on heuristics and manually curated\nfeatures [6,7], which restricts their ability to truly understand the subtle semantic aspects of\nnatural language. As a result, it becomes imperative to investigate more sophisticated\napproaches that can harness the natural language comprehension abilities offered by large lan-\nguage models.\nIn our previous work, we developed the Siamese convolutional neural network (S-CNN) in\nconjunction with Bidirectional Encoder Representations from Transformer (BERT) model\n[8], which achieved promising results with an F1-score of 82.02% on the Quora Question Pair\ndataset.\nWith the advent of large language models (LLMs) like ChatGPT [9] by OpenAI and Gemini\nby Google, they have shown remarkable capabilities in natural language understanding. How-\never, researchers face challenges using commercial LLMs, such as their black-box models that\nmay not be updated by researchers and data being commonly processed outside researchers’\nfacilities, which may raise data security and confidentiality concerns. Recent open-sourced\nLLMs (using 10\n9\n–10\n11\nparameters), e.g., Large Language model Meta AI (LLaMA) [10] devel-\noped by Meta AI and Mistral models by Mistral AI [11], shed light to address the challenges\nresearchers are facing. However, it remains unknown to what extent such open-sourced LLMs\ncan further improve previously developed language models (using approximately 10\n8\nparame-\nters) such as our previously developed S-CNN in conjunction with the BERT model in ques-\ntion pair classification tasks.\n1.1 Research objectives\nThe primary objective of this research is to assess the effectiveness of open-sourced LLaMA\nmodels in improving text similarity tasks. Specifically, we aimed to investigate the potential of\nLLaMA when fine-tuned using the low-rank adaptation (LoRA) technique. This study seeks to\nadvance our understanding of how LLMs can enhance text similarity measurements, with a\nparticular focus on the QQP dataset. Additionally, we aimed to achieve a better classification\nperformance, evaluated with the F1-score, and determined the extent of improvement com-\npared to our prior best-performing model [8]. Furthermore, we explored prompt engineering\nand few-shot prompting approaches on LLMs and compared performance between different\napproaches.\nThe contributions of this work include demonstrating the potential of large language mod-\nels, such as LLaMA-7B, in improving question pair classification tasks, and fine-tuning is\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 2 / 15\nessential for specific tasks. Additionally, our approach shows promise in enhancing document\nsimilarity measurement, potentially enabling practical applications in fields such as resume\nmatching [12], expert finding [13], request-for-proposal recommendations [14], and assisting\neditors in identifying potential reviewers.\nThe main contributions of this paper are summarized as follows:\n1. Exploring five pretrained open-sourced LLMs: We undertook a comprehensive exploration\nof large language models, experimenting with various approaches to identify effective meth-\nods for downstream tasks. To the best of our knowledge, this is the first study that applied\nLLMs to similarity comparison in the QQP dataset.\n2. Optimizing LLMs using task-specific finetuning framework: We developed a framework to\nfine-tune LLMs and evaluated the performance using the existing QQP corpus and an open\nbenchmark dataset. We also compared a finetuned LLM to our previous best-performing\nmodel, the Siamese-CNN model [8].\n3. Evaluating Prompt Engineering and n-shot prompting approaches: Our research under-\nscores the role of prompt engineering when utilizing large language models, and evaluated\nperformance between different n-shot prompting approaches.\n4. Comparing finetuned LLMs using the General Language Understanding Evaluation\n(GLUE) Benchmark [15]: We compared two fine-tuned LLMs (qLLaMA-LoRA-7B and\nqLLaMA3.1_LoRA-70B) using the GLUE Benchmark, a common standard to assess model\nperformance using QQP test data.\n1.2 Related work\nThis section briefly describes our previous best model and introduces the challenges associated\nwith this task.\n1.2.1 Siamese Convolutional Neural Network (S-CNN). In our previous work, we devel-\noped an S-CNN model [8] to identify duplicate Quora Question Pairs. This approach utilized\na Siamese architecture [1,16,17] consisting of two parallel convolutional neural networks\n(CNNs) [18–21] with shared weights. Each network independently processed one question\nfrom the pair, enabling the model to learn representations that captured the semantic similari-\nties [22,23] and differences between the questions. To enhance processing, attention mecha-\nnisms [24–26] were incorporated to focus on different sections of the question text based on\ntheir significance when determining similarity. The outputs of the Siamese networks were\nthen passed through a similarity measure, generating a similarity probability and then deter-\nmining whether the question pair was a duplicate or not.\n1.2.2 LLaMA models. MetaAI has developed several large language models (LLaMA)\nthat use the Transformer architecture to process natural language on a vast scale [10].\nLLaMA models have been trained on an extensive corpus of text and code, which enhances\nits capabilities in generating, understanding, and answering queries in natural language.\nThe foundation LLaMA models (first introduced in 2023) range from 7B to 405B\n(LLaMA3.1) parameters, and the models were trained on trillions of tokens. The LLaMA-\n13B outperformed GPT-3 (175B) [27] on most benchmarks, and LLaMA-65B is competitive\nwith models like Chinchilla-70B [28] and PaLM-540B [29]. MetaAI has released the models\nto the research community.\n1.2.3 GLUE benchmark. The General Language Understanding Evaluation (GLUE)\nbenchmark [15], developed by New York University and DeepMind researchers, is pivotal in\nassessing machine learning models across a spectrum of language understanding tasks, such as\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 3 / 15\nsentiment analysis, question answering, and textual entailment. It includes datasets like the\nQuora Question Pairs (QQP), allowing models like ours to be rigorously evaluated on their\nability to generalize across various linguistic tasks. GLUE’s structure promotes advancements\nin NLP by testing models, particularly those based on transformative architectures like BERT\n[30] and GPT [27], creating a competitive yet collaborative research environment.\nRecent advancements in NLP are primarily fueled by transformer-based architectures\nsuch as BERT, GPT, and Google’s T5 [31]. BERT enhances understanding through bidirec-\ntional training, while GPT-3, with its vast parameter scale, excels in generating contextually\nrelevant text without specific tuning. T5 simplifies processing by treating all language tasks\nas text-to-text conversions, showcasing the shift towards versatile, adaptable models that\nstreamline deployment across different applications, thus driving significant progress in the\nfield.\n1.2.4 Challenges and limitations of current NLP models. While our previously pub-\nlished Siamese-CNN model achieved promising results with an F1-score of 82.02% on the\nQuora Question Pair dataset [32], there still exist challenges and limitations.\n• Natural Language Understanding\nThe Siamese-CNN approach, which utilizes feature representations for semantical under-\nstanding, faces challenges and limitations in capturing the subtleties and complexities of lan-\nguage, potentially compromising the accuracy of tasks like question pair classification; such\nlimited context understanding limits its generalizability in different applications and results in\nmany model variations for specific tasks, e.g., bioBERT [33], clinicalBERT [34], and stroke-\nBERT [35]. In contrast, advancements in natural language understanding have been exempli-\nfied by the emergence of large language models, such as OpenAI’s ChatGPT-3 [9] with 175\nbillion parameters. These models significantly outperform smaller-scale models like the BERT\nmodel with 26 million parameters, particularly in grasping contextual nuances and semantic\nintricacies, thus promising substantial improvements across a range of NLP applications\n[36,37].\n• Performance Improvement\nGiven the limitations of our previous approach and the growing interest in LLM, we aimed\nto explore the potential of fine-tuning a large language model for question pair duplicate iden-\ntification. This exploration presented an opportunity to achieve further improvements in per-\nformance and enhance the accuracy of classification.\nIn this study, we hypothesized that by fine-tuning a large language model, we could harness\nits language understanding abilities and improve the performance of duplicate question-pair\nidentification. The key contributions of this study include 1) addressing these challenges and\nlimitations of current NLP models by developing a framework to optimize large language\nmodels and 2) comparing their performance using different optimization steps in our QQP\ncorpus and the standard GLUE benchmark. Our work leverages the strengths of large language\nmodels, focusing on their natural language understanding abilities to improve question pair\nclassification tasks.\n2 Materials and methods\nWe first describe the QQP corpus used in this study followed by the introduction of the low-\nrank adaptation (LoRA) algorithm [Section 2.3] [38], which enables finetuning an LLM by\nupdating a small fraction of the model’s parameters to shorten the training time and save\nresources.\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 4 / 15\n2.1 Quora Question Pair (QQP) corpus\nIn this study, we used the Quora Question Pairs (QQP) dataset as our study corpus, shared via\na Kaggle competition, consisting of a publicly available annotated dataset with 404,290 ques-\ntion pairs. The dataset contains 255,027 pairs (63.08%) labeled as ’0’ (indicating non-duplicate\nor dissimilar pairs) and 149,263 pairs (36.92%) marked as ’1’ (indicating duplicate or similar\npairs). These labels were meticulously assigned by human experts.\n2.2 Classification task/outcome\nThe task for the language models in this study is to provide a binary classification on whether\nthe question pair of interest is a duplicate (marked as “yes”) or non-duplicate (marked as “no”)\npair.\n2.3 The LoRA algorithm for updating parameters of a large language model\nTraditionally, fine-tuning a language model involves updating all parameters of the pre-trained\nmodel. However, this can be time-consuming and costly for researchers and companies. To\naddress this, we employed the Low-Rank Adaptation (LoRA) [38] for effectively updating\nparameters in an LLM. LoRA maintains the weights of the pre-trained model while introduc-\ning trainable low-rank decomposition matrices. By freezing the pre-trained model weights and\nincorporating trainable low-rank matrices into each layer, LoRA significantly reduces the total\nnumber of trainable parameters. This reduction in parameters makes it feasible to train LLMs\nwith significantly fewer computational resources. The following equation summarizes the\nLoRA algorithm where h represents the output of a given network layer, x is the input from\nthe previous layer, W\n0\n2 R \nd�k\nrepresents the majority of weights in the current layer, k is the\ninput dimension from the previous layer, d represents the output dimension; A and B are two\nlow-rank decomposed trainable matrices, A 2 R \nd �r\nand B 2 R \nr �k\n, where r denotes the pre-\ndetermined rank. During the forward pass with an input x, the equation is as follows:\nh ¼ W\n0\nx þ DWx ¼ W\n0\nx þ ABx; A 2 R \nd�r\n; B 2 R \nr �k\nDuring training, W\n0\nremains frozen (unchanged) and does not receive gradient updates,\nwhile A and B are updated. By selecting a rank, the memory consumption can be effectively\nreduced as there is no need to store the optimizer states for the large frozen matrix.\n2.4 LLM Optimization Framework\nIn this framework, we describe three steps to optimize a large language model: prompt engi-\nneering, n-shot promoting, and supervised fine-tuning.\n2.4.1 Prompt engineering. Prompt engineering involves crafting input prompts that\neffectively guide LLMs to perform specific tasks. This technique includes designing clear\ninstructions, embedding relevant context, or using structured templates to maximize the mod-\nel’s output quality without retraining. The goal is to optimize the LLM’s performance by\nadjusting how information is presented, enabling it to handle various tasks effectively with\nminimal data or fine-tuning. Effective prompt engineering requires iterative experimentation\nto determine the best structure for achieving desired results. In this step, providing examples\n(n-shot prompting) is an option (see the next section).\n2.4.2 N-shot prompting. N-shot prompting refers to providing an LLM with ‘n’ labeled\nexamples in the prompt to help it perform a specific task. The number ‘n’ indicates how\nmany examples are shown to the model before it generates a response for a new, unseen\ninput. This technique leverages the model’s pre-trained knowledge, using minimal examples\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 5 / 15\nto enhance performance on tasks without needing extensive retraining. Fig 1 shows the process\nof 1-shot prompting (learning), a crucial aspect of our few-shot prompting [39–41] experi-\nments, which aim to optimize the model’s ability to perform tasks using only limited input. The\nflowchart highlights the steps involved in leveraging such examples to guide the model.\nIn our specific use case, each 1-shot prompt [42] consists of a concatenation of two ques-\ntions, separated by the special token [SEP], followed by the query “Are they duplicates?” The\nLLaMA model is then tasked with generating a response either “Yes” or “No” based on the\ninformation provided. -represents this process emphasizes the role of prompt engineering in\neffectively guiding the model’s reasoning with minimal data. To enable scalability in few-shot\nprompting, we employ the use of “###” as a separator between different examples within the\nprompt, allowing the model to handle multiple samples efficiently. This approach ensures that\nthe model remains flexible, processing various datasets with different numbers of examples\nwhile maintaining accuracy and consistency. With this approach, the model can acquire new\nknowledge quickly and perform better on the downstream task with limited labeled data.\n2.4.3 Supervised fine-tuning (SFT). Supervised finetuning is the process of further train-\ning a pre-trained LLM on a specific dataset where both input data and corresponding output\nlabels are provided. This allows the model to adapt its knowledge to perform well on a particu-\nlar task or domain. Unlike n-shot prompting, which relies on few examples embedding in\nprompts and does not change the parameters of the LLM, supervised fine-tuning involves\ntraining the model on larger labeled datasets and subsequently updates the parameters of the\nLLM, allowing the LLM to make more accurate and reliable predictions.\nIn this step, we took a pre-trained model, such as LLaMA-7B, and fed the labeled exam-\nples from the QQP dataset. These labeled examples consist of inputs (e.g., question pairs)\nand the corresponding outputs (e.g., “Yes” or “No”). Through iterative training, the model\nadjusts its internal parameters to minimize the difference between its predicted outputs and\nthe ground truth provided in the dataset. With this approach, it makes model highly task-\nspecific, improving performance on specialized tasks like duplicate identification [8], senti-\nment analysis [43]. Finetuning helps to address limitations in the general pre-trained model\nby training it on domain-specific terminology and nuances. This process generally results\nin better performance than N-shot prompting (learning), as the model becomes more finely\ntuned to the specific dataset it will encounter in production. We named the model\nqLLaMA-7B-LoRA as the finetuned LLaMA-7B applied to QQP through the supervised\nfinetuning using LoRA (Fig 2).\n2.5 Eight classification models: three state-of-the-art NLP Models and five\nLLMs\nIn the main study, we compared and evaluated 8 models: three previously published state-of-\nthe-art NLP models (Simple Contrastive Sentence Embeddings (SimCSE) [44], a contrastive\nFig 1. One-shot prompt for the Large Language model Meta AI (LLaMA) model with 7 billion parameters .\nhttps://do i.org/10.1371/j ournal.pone .0317042.g00 1\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 6 / 15\nlearning-based approach MirrorBERT [45], and S-CNN), four pre-trained LLMs (Alpaca_-\nLoRA-7B, Alpaca_LoRA-65B, LLaMA-7B, LLaMA-33B) and one supervised fine-tuning LLM\n(qLLaMA-7B-LoRA). We further compared the performance of 1-shot and 5-shot prompts\nbetween LLaMA-7B and LLaMA-33B: LLaMA-7B_1shot, LLaMA-7B_5shot, LLaMA-\n33B_1shot, and LLaMA-33B_5shot. SimCSE and MirrorBERT are contrastive learning-based\nmodels, while S-CNN utilizes an auto-encoder with a Siamese-CNN network; the three mod-\nels, having the number of variables ranging between 10\n7\nand 10\n8\n, are considered as baseline\nmodels for the textual semantic similarity task. The four pre-trained LLMs having the number\nof variables ranging between 10\n9\nto 10\n10\n, are described below:\n• LLaMA-7B is a highly efficient model from Meta AI, designed to perform well with fewer\nresources while maintaining strong performance across various NLP tasks. It has 7 billion\nparameters and trained on 1.4 trillion tokens from a curated dataset, which consists of about\n1 million unique documents [10]. It strikes a balance between computational efficiency and\nrobust task execution.\n• LLaMA-33B is a larger variant of the LLaMA family, consisting of 33 billion parameters and\ntrained on 1.4 trillion tokens same as LLaMA-7B [10]. This model offers enhanced capacity\nand performance.\n• Alpaca_lora_7B is a fine-tuned version of LLaMA using LoRA which enables more efficient\ntraining with limited computational resources, it retains LLaMA’s base architecture but is\ndesigned to excel in instruction-based tasks through additional fine-tuning on a dataset of\n52,000 instruction-following examples generated by OpenAI’s text-davinci-003 [46].\nFig 2. The fine-tuni ng flowchart for the qLLaMA_Lo RA-7B model by updating the pre-trained LLaMA-7B model’s parame ters using the Low-Ran k\nAdaptation (LoRA), a supervised learning algorithm , from 100,000 Quora question pairs. LLaMA-7B: Large Language model Meta AI with 7 billion\nparameters.\nhttps://doi.o rg/10.1371/j ournal.pone .0317042.g002\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 7 / 15\n• Alpaca_lora_65B is a larger version of Alpaca, with 65 billion parameters, offering improved\ncontextual understanding and handling of complex language tasks. It was fine-tuned using\nthe same instruction-following dataset as Alpaca_lora_7B model.\n2.6 Model training and evaluation\nWe implemented a 10-fold cross-validation [47] strategy for training and assessing the fine-\ntuned LLaMA model. The dataset was initially divided into ten stratified folds. Subsequently, a\nmodel was trained using nine of the 10 folds and then evaluated using the remaining fold as a\nhold-out dataset. The process was repeated 10 times. In our previous study [8], we established\na benchmark by conducting a comprehensive comparison of diverse deep-learning\napproaches, ultimately identifying the S-CNN as the superior model. To ensure fairness in our\ncomparisons, we applied the identical fold splits that were previously created in our previous\nstudy [8] and conducted experiments on the same machine when assessing the performance of\nlarge language models. Due to the complexity of LLMs, we performed stratified sampling of\neach training fold from 363,861 to 100,000 question pairs (comprising 36,920 similar and\n63,080 dissimilar pairs). We kept the same size of each test fold for performance evaluation.\n2.6.1 LLM fine-tuning hyperparameters. For the fine-tuning process, we used a batch\nsize of 128 and conducted ten epochs and the additional hyperparameters adopted from the\nAlpaca model, which were maintained at their default settings, include a learning rate of 3e-4,\nlora_r set to 8, lora_alpha set to 16, and a lora_dropout rate of 0.05. In the inference stage, the\ndefault settings were retained with a “temperature” (a hyperparameter used in softmax calcula-\ntion during the generation process. A higher temperature value leads to more randomness in\nthe selection of words, potentially creating more diverse but less predictable text.) of 0.1, top_p\n(nucleus sampling, a method used in text generation where the model only considers a subset\nof the vocabulary with the highest probabilities whose cumulative sum is less than or equal to\nthe specified p value) of 0.75, top_k (top_k sampling, model only considers the top k most\nprobable next words at each step of the generation) of 40, and num_beams (beam search,\nnumber of sequences considered at each step of the generation process) of 4.\n2.7 Model evaluation metrics\nEvaluation metrics for NLP models in this study include precision, recall/sensitivity, accuracy,\nand F1-score [48], defined below: Prec isi on ¼\nTP\nTPþFP\n; Rec all ¼\nTP\nTPþFN\n; F 1 sco re ¼ 2∗\nPrec isio n∗Re call\nPre cisi onþRec all\nwhere TP, FP, and FN are true positive, false positive, and false negative, respectively. The\nideal classifier has precision and recall equal to one, which means FP and FN are zero. F1-score\nis the harmonic mean of precision and recall.\n2.8 Secondary study: Fine-tuning and evaluating the LLaMA3.1-70B model\nWith the release of more advanced, larger, and robust foundation models, we extended our\nmain analysis of 10 models by fine-tuning another recent pre-trained model, LLaMA3.1-70B,\nwith 70 billion parameters; the fine-tuned model, qLLaMA3.1-LoRA-70B, was then evaluated\nin an external dataset, on the GLUE Benchmark. Using the same approach and prompts\ndescribed in the main study, we fine-tuned the LLaMA3.1-70B model for the QQP task\nthrough supervised training using LoRA. Due to the complexity of LLMs, we performed a\nstratified sampling of 100,000 question pairs from one of the training folds comprising 363,861\nquestion pairs as described in Section 2.6. The supervised fine-tuning was based on the\n100,000 pairs to adjust the model’s weights and optimize its performance on the QQP task.\nAfter completing the fine-tuning process, the fine-tuned qLLaMA3.1-LoRA-70B model was\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 8 / 15\nthen externally validated using the GLUE Benchmark dataset. The results generated from the\nLLaMA3.1-70B model were then submitted to the official GLUE Benchmark platform for per-\nformance scoring. This submission allowed us to obtain comparative performance metrics,\nproviding a clearer understanding of how the fine-tuned LLaMA3.1-70B model fares against\nother models in terms of handling the QQP task.\n3 Results\n3.1 Main study: Comparison between three state-of-the-art NLP models\nand five LLMs\nTable 1 summarizes classification performance of eight models, three state-of-the-art NLP\nmodels (SimCSE, MirrorBERT, and S-CNN) and five LLMs (qLLaMA_LoRA-7B, Alpaca_-\nLoRA-7B, Alpaca_LoRA-65B, LLaMA-7B, and LLaMA-33B), with 1-shot and 5-shot prompt-\nings between LLaMA-7B and LLaMA-33B. The fine-tuned LLaMA-7B model using LoRA\n(qLLaMA_LoRA-7B) demonstrated the best F1 score (84.9% [95% CI: 84.13–85.67]), precision\n(83.64% [95% CI: 81.1–86.18]), and accuracy (88.67% [95% CI: 88.35–88.99]) with statistical\nsignificance (P < .05), compared to other nine NLP models using t-test. SimCSE had the best\nrecall of 99.78 (95% CI, 99.73–99.83). The fine-tuning procedure was performed on a server\nwith a single Nvidia Tesla V100 graphics processing unit (GPU) with 32GB, which took\nTable 1. Model comparison for question pairs detection using 10-fold cross validation.\nModel F1 [%]\n(95% C.I.)\nPrecision [%]\n(95% C.I.)\nRecall [%]\n(95% C.I.)\nAccuracy [%]\n(95% C.I.)\nSimCSE 61.22\n(61.15–61.29)\n44.16\n(44.09–44 .23)\n99.78\n(99.73–99.8 3)\n53.34\n(53.21–53 .46)\nMirrorBER T 60.14\n(60.05–60.22)\n43.07\n(42.98–43 .16)\n99.60\n(99.54–99 .66)\n51.25\n(51.07–51 .42)\nS-CNN 82.02\n(81.83–82.20)\n82.18\n(81.99–82 .38)\n81.88\n(81.60–82 .17)\n83.32\n(83.17–83 .47)\nqLLaMA_L oRA-7B 84.90\n(84.13–85.67)\n83.64\n(81.10–86.1 8)\n86.43\n(82.39–90 .46)\n88.67\n(88.35–8 8.99)\nAlpaca_Lo RA-7B 4.67\n(4.39–4.95)\n34.13\n(28.41–39 .84)\n2.50\n(2.38–2.63)\n62.40\n(61.86–62 .95)\nAlpaca_Lo RA-65B 64.98\n(64.72–65.25)\n58.59\n(58.35–58 .83)\n72.94\n(72.56–73 .33)\n70.96\n(70.75–71 .16)\nLLaMA-7B\n0-shot prompting 0.22 (0.21–0.23) 33.45 (30.12–36.78) 0.11 (0.10–0.11) 0.18 (0.17–0.18 )\n1-shot prompting (LLaMA- 7B_1shot) 20.90\n(20.54–21.27)\n38.13\n(37.40–38 .86)\n14.40\n(14.14–14 .65)\n59.77\n(59.54–60 )\n5-shot prompting (LLaMA- 7B_5shot) 28.03\n(27.61–28.45)\n40.78\n(40.19–41 .38)\n21.36\n(21–21.72 )\n59.52\n(59.27–59 .76)\nLLaMA-33 B\n0-shot prompting 0.41 (0.39–0.42) 35.66 (33.67–37.65) 0.20 (0.19–0.21) 0.34 (0.32–0.35 )\n1-shot prompting (LLaMA- 33B_1shot) 31.77\n(31.31–32.22)\n37.27\n(36.84–37 .70)\n27.68\n(27.19–28 .16)\n56.10\n(55.87–56 .33)\n5-shot prompting (LLaMA- 33B_5shot) 49.39\n(48.92–49.85)\n50.40\n(50–50.81 )\n48.41\n(47.84–48 .98)\n63.36\n(63.08–63 .65)\nThe boldfaced numbers represent the best performance across models. SimCSE: Simple Contrastive Sentence Embeddin gs, a contrastive learning -based model for\nsentence embedding; MirrorBERT: a contrastive learning-ba sed model designed to improve sentence representation s; S-CNN: Siamese Convolution al Neural Network\nfor text similarity tasks; qLLaMA_L oRA-7B: Quantiz ed Low-Ran k Adaption of LLaMA with 7 billion paramete rs; Alpaca_LoR A-7B/65B: Finetune d model by using\nLoRA based on LLaMA- 7B/65B model. LLaMA-7B/3 3B_1shot/5s hot: LLaMA model with 7/33 billion param eters using 1-shot or 5-shot promp ting.\nhttps://do i.org/10.1371/j ournal.pone .0317042.t001\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 9 / 15\napproximately 66 hours for one-fold training. The prompting of LLaMA-7B, Alpaca_LoRA-\n7B, and Alpaca_LoRA-65B models were tested on another server with a single Nvidia Tesla\nA100-80GB GPU. The LLaMA-33B model was tested on the third server with 4 A100-80GB\nGPUs given the MetaAI script requirement.\nFew-shot prompting was particularly effective for larger models, showing notable improve-\nments between the 1-shot and 5-shot settings. For example, the LLaMA-7B model’s F1 score\nwas increased by 34.1% from 20.90% (95% CI: 20.54%-21.27%) in the 1-shot prompting to\n28.03% (95% CI: 27.61%-28.45%) in the 5-shot prompting. Additionally, the larger model\nLLaMA-33B had further improvement using few-shot learning compared to the smaller model\nLLaMA-7B, i.e., LLaMA-33B’s F1 score was increased by 55.5% from 1-shot 31.77% (95% CI:\n31.31%-32.22%) to 5-shot 49.39% (95% CI: 48.92%-49.85%).\n3.2 Secondary study: External validation using the GLUE benchmark\ndataset\nTable 2 summarizes the external validation performance results of two supervised fine-tuned\nLLaMA models with 7B and 70B parameters: qLLaMA_LoRA-7B and qLLaMA3.1_LoRA-70B.\nThe qLLaMA3.1_LoRA-70 B outperformed the smaller model qLLaMA_LoRA-7B. The qLLa-\nMA3.1-LoRA-70B model was trained on a server with four Nvidia A100-80GB GPUs, which\ntook approximately 18 hours for model fine-tuning.\n4 Discussion\nIn this study, we tested our hypothesis on whether an optimized open-sourced LLM, e.g.,\nqLLaMA_LoRA-7B, through supervised learning using Quora-question pairs, can improve\nsimilarity classification performance compared to the current state-of-the-art language models\n(simCSE, mirrorBERT, S-CNN) and out-of-the-box pre-trained LLMs. At the outset of this\nstudy, LLaMA models stood out as the leading open-source model in terms of both perfor-\nmance and accessibility for research purposes. The supervised fine-tuned model, qLLaMA_-\nLoRA-7B, performed best compared to the other seven models, including unsupervised\nlearning, zero-shot, one-shot, and 5-shot prompting on varying sizes of LLMs; it had an\nimproved F1-score of 84.90% on the QQP dataset compared to the three state-of-the-art lan-\nguage models with statistical significance (P < .01). This improvement highlights the potential\nof LLaMA in enhancing the task of classifying question pairs and showcases the benefits of\nleveraging LLMs for semantical text understanding. Notably, the fine-tuned qLLaMA_LoRA-\n7B model outperformed larger LLMs, including those with 33B and 65B parameters, which\nsuggests a smaller fine-tuned LLM can potentially perform better than those larger LLMs with-\nout supervised fine-tuning given that supervised fine-tuning allowed the model to adapt to the\nspecific task of question-pair duplicate detection.\nTable 2. External validati on results between two fine-tuned models (qLLaMA_Lo RA-7B and qLLaMA3. 1_LoRA-\n70B) based on GLUE benchm ark.\nModel F1 (%) Accuracy (%)\nqLLaMA _LoRA-7B 71.9 89.1\nqLLaMA 3.1_LoRA-7 0B 74.4 90.3\nqLLaMA _LoRA-7B : the fine-tun ed LLaMA -7B model using supervised learning based on LoRA. qLLaMA 3.1_LoRA-\n70B: the fine-tuned LLaMA3.1- 70B model using supervised learning based on LoRA. No confidence interval was\nobtained since the analyzed performance results were computed by the GLUE benchma rk.\nhttps://d oi.org/10.1371/j ournal.pon e.0317042.t00 2\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 10 / 15\nThe S-CNN baseline model employed a Siamese network structure combined with convo-\nlutional neural networks, necessitating more intricate architectural design, and achieved an\nF1-score of 82.02%. In contrast, the out-of-the-box Alpaca_LoRA-65B model, while demon-\nstrating its inherent power, had a lower F1-score of 64.98%, suggesting that a pretrained LLM\nmight not be fully optimized for the specific task compared to current state-of-the-art NLP\nmodels. On the other hand, the qLLaMA_LoRA-7B, a fine-tuned version of the LLaMA-7B\nmodel tailored for the Quora Question Pairs dataset, significantly improved performance with\nan F1-score of 84.9%.\nThe disparity in performance between out-of-the-box Alpaca_LoRA-7B and Alpaca_-\nLoRA-65B is stark, with F1 scores of 4.67% and 64.98%, respectively. This significant differ-\nence highlights the impact of pretrained model size on task performance. The 7B model, with\nits relatively limited parameter count, struggled to capture the complex patterns necessary for\naccurate text similarity classifying. In contrast, the 65B model, with its vastly larger parameter\nspace, demonstrated an improved ability to comprehend and process linguistic nuances, lead-\ning to a much higher F1 score. This suggests that for certain NLP tasks, particularly those\nrequiring nuanced understanding, larger models may offer substantial advantages.\nFew-shot prompting can potentially improve classification performance in the same model.\nNotably, the pre-trained LLaMA-7B with 0-shot prompting had only 0.24% F1-score, whereas\nLLaMA-7B_1shot and LLaMA-7B_5shot had 20.9% and 28.03% F1scores (87 and 117 times\nhigher than the 0-shot performance), respectively. Similarly, the pre-trained LLaMA-33B had\nimproved F1 scores of 0.41%, 31.77%, and 49.39% using 0, 1 and 5 shots of prompting, respec-\ntively. The 1-shot and 5-shot F1 scores were x and y times higher than the 0-shot F1 score.\nThese findings underscore the effectiveness of few-shot prompting, especially when rapid out-\ncome improvement is needed without running LoRA. We attribute the improved performance\nthrough n-shot prompting is due to learning of question-answer patterns and additional con-\ntextual guidance [27].\nA larger LLM can potentially benefit more from few-shot prompting compared to a smaller\nLLM. Between the two pretrained models (LLaMA-7B and LLaMA-33B), after 5-shot prompt-\ning, the larger model LLaMA-33B had best F1 score (49.39% vs. 28.03%) with statistical signifi-\ncance, P < .05. Also, compared to LLaMA-7B, the LLaMA-33B model had a much higher F1\nscore improvement rate between 1-shot and 5-shot, i.e., 55.5% vs. 34%. It can be attributed by\nthe flexibility and the number of parameters of a larger LLM.\nLarger LLMs generally performed better than smaller LLMs at 0-shot prompting. Larger\nmodels like Alpaca_LoRA-65 and LLaMA-33B, significantly outperformed their counterpart\nsmaller models, Alpaca_LoRA-7B and LlaMA-7B. It suggests that the same task could be\nachieved with improved performance given the large model’s greater capacity for learning and\ngeneralizing. The larger number of parameters enables the bigger models to capture more\nintricate and nuanced patterns in the data, enabling them to reach a higher level of abstraction\nand understanding.\nIn Section 1.2.3, we noted that the QQP dataset is part of the GLUE benchmark. To better\nunderstand the performance of the qLLaMA_LoRA-7B model compared to other approaches,\nwe evaluated it on this dataset. Our model achieved an F1-score of 71.9% and an accuracy of\n89.1%, which were slightly lower (4.5% and 1.8%) than the scores of the leading team (Micro-\nsoft) [49], respectively. However, it is worth noting that human baselines were 59.5% F1-score\nand 80.4% accuracy.\nA larger supervised finetuned (SFT) LLM performed better than a smaller SFT LLM. As\noutlined in the secondary study, we examined the improvements across different model gener-\nations. Our SFT qLLaMA3.1_LoRA-70 B outperformed the qLLaMA-LoRA-7B by achieving\nan F1-score of 74.4% (vs. 71.9%) and an accuracy of 90.3% (vs. 89.1%). The 70B model closely\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 11 / 15\nmatched the top-ranked 8th model on the leaderboard (F1-score of 74.7%, accuracy of 90.6%)\nwith very close performance. As both LLMs were trained based on 100K pairs vs. others that\nwere trained with the full set of 255K pairs, we believe our SFT LLMs could have potentially\nincreased performance through additional training pairs.\n4.1 Limitations\nOur study has the following limitations. First, we only performed few-shot prompting on two\nLLMs instead of all 5 LLMs. It was due to available hardware resources. Since the two Alpaca\nmodels were derived from LLaMA models, we believe the impact without considering the\nAlpaca n-shot prompting is minimal. Second, we did not perform prompt engineering to com-\npare multiple prompt questions, e.g., “Are the two questions similar?”, for few-shot prompting\nexperiments. Literature shows that while few-shot prompting holds significant potential for\nenhancing model performance, the effectiveness of the approach heavily depends on well-\ncrafted prompts. Further investigations with refined and optimized prompts could be consid-\nered as it may yield more accurate and robust outcomes, providing a clearer picture of the\nmodel’s true capabilities in few-shot prompting scenarios. Third, to reduce the computational\nload, we did not use the full 255K pairs for SFT LLMs but rather downsampled the full anno-\ntated dataset to 100,000 pairs. We may consider to train the LLMs with the full QQP dataset.\nFinally, we did not perform 10-fold cross-validation on the qLLaMA3.1_LoRA-70B to provide\na more comprehensive evaluation with the 8 models due to limited resources.\n4.2 Future work\nThe research presented in this study introduces opportunities for future investigation and\nadvancement. The following paths can be pursued to expand our work and progress further.\n1. Scaling and Efficiency: As LLMs continue to grow in size and complexity, research efforts\ncan focus on improving their scalability and computational efficiency. Techniques such as\nmodel distillation, pruning, and model compression/quantiz ation [50–53] can be explored\nto make these models more accessible and practical for real-world applications.\n2. Prompt Engineering: LLMs are highly sensitive to context, and identical semantic inputs\ncan yield different outcomes. To optimize the performance of these models, experimenting\nwith various prompt formats, styles, and the inclusion of different types of information will\nbe beneficial.\n5 Conclusions\nIn this study, we compared the performance of eight NLP models for document similarity clas-\nsification, including three current-state-of-the-art NLP models (SimCSE, MirrorBERT, and\nS-CNN) and five open-sourced large language models. A large language model with a smaller\nsize (e.g., 7B) with supervised fine-tuning for a specific task can potentially perform better\nthan those models with a much larger size (e.g., 65B) and current state-of-the-art text-similar-\nity classification models. A finetuned larger LLM is likely to outperform a smaller finetuned\nLLM. Our work not only demonstrates its potential in measuring document similarity, but\nalso potentially enables broad applicability in various natural language understanding tasks. By\naccurately capturing semantic nuances and language patterns, an NLP model can contribute\nto tasks such as aligning resumes with job descriptions, locating experts, recommending RFPs\nto researchers of interest, and aiding editors in finding suitable reviewers.\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 12 / 15\nAcknowledgmen ts\nWe would like to thank Paul Kingsbury in the Tsui Laboratory for proofreading the manu-\nscript and making valuable recommendations.\nAuthor Contributions\nConceptualization: Fuchiang (Rich) Tsui.\nFormal analysis: Sifei Han.\nFunding acquisition: Fuchiang (Rich) Tsui.\nInvestigation: Sifei Han, Fuchiang (Rich) Tsui.\nMethodology: Sifei Han.\nProject administration: Lingyun Shi.\nSupervision: Fuchiang (Rich) Tsui.\nValidation: Lingyun Shi, Fuchiang (Rich) Tsui.\nVisualization: Sifei Han.\nWriting – original draft: Sifei Han.\nWriting – review & editing: Lingyun Shi, Fuchiang (Rich) Tsui.\nReferences\n1. Imtiaz Z, Umer M, Ahmad M, Ullah S, Choi GS, Mehmood A. Duplicate questions pair detection using\nsiamese malstm. IEEE Access. 2020; 8:21932–42.\n2. Huang A, others. Similarity measures for text docume nt clustering . In: Proceedings of the sixth new zea-\nland computer science research student conferen ce (NZCS RSC2008), Christchurch, New Zealand.\n2008. p. 9–56.\n3. Kim JG. A study on metav erse culture contents matching platfor m. Internat ional Journal of Advanced\nCulture Technology (IJACT). 2021; 9(3):232–7 .\n4. Chen Z, Zhang H, Zhang X, Zhao L. Quora question pairs. University of Waterlo o. 2018;\n5. Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large language models are zero-shot reasone rs. Adv\nNeural Inf Process Syst. 2022; 35:22199–213 .\n6. Hidalgo JMG, Almeida TA, Yamakami A. On the validity of a new SMS spam collection. In: 2012 11th\nInternational Conferen ce on Machine Learning and Applications. 2012. p. 240–5.\n7. Crawford M, Khoshgofta ar TM, Prusa JD, Richte r AN, Al Najada H. Survey of review spam detection\nusing machine learning technique s. J Big Data. 2015; 2(1):1–24.\n8. Han S, Shi L, Richie R, Tsui FR. Building siamese attention -augmented recurrent convolutio nal neural\nnetworks for document similarity scoring. Inf Sci (N Y). 2022; 615:90– 102.\n9. OpenAI. ChatGP T [Internet] . Text genera ted by ChatGP T. 2023. Availab le from: https://c hat.openai.\ncom\n10. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al. LLaMA: Open and Efficient\nFoundati on Language Models. 2023.\n11. Jiang AQ, Sablayrolle s A, Mensch A, Bamford C, Chaplot DS, de Las Casas D, et al. Mistral 7B. ArXiv\n[Internet ]. 2023;abs /2310.0. Availab le from: https://api.s emanticsc holar.org/Cor pusID:2 63830494\n12. Shao T, Song C, Zheng J, Cai F, Chen H, others. Exploring Internal and Externa l Interactions for Semi-\nStructured Multivaria te Attribute s in Job-Resum e Matching. International Journal of Intelligent Systems.\n2023;2023.\n13. Nagaraj P, Deepalakshm i P. An intelligen t fuzzy inference rule-based expert recommenda tion system\nfor predictive diabetes diagnos is. Int J Imaging Syst Technol. 2022; 32(4):137 3–96.\n14. Han S, Richie R, Shi L, Tsui FR. Automate d Matchmak ing of Researcher Biosketches and Funder\nRequests for Proposals Using Deep Neural Networks. IEEE access. 2024; 12:9809 6–106.\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 13 / 15\n15. Wang A, Singh A, Michael J, Hill F, Levy O, Bowman S. GLUE: A Multi-Task Benchmar k and Analysis\nPlatform for Natural Language Understan ding. In: Linzen T, Chrupała G, Alishah i A, editor s. Proceed-\nings of the 2018 EMNLP Workshop BlackboxN LP: Analyzing and Interpre ting Neural Networks for NLP\n[Internet ]. Brussels, Belgium: Association for Comp utational Linguistics; 2018. p. 353–5. Available\nfrom: https://aclantho logy.org /W18-5446\n16. Chicco D. Siamese neural networks: An overview. Artificial Neural Networks. 2021;73 –94. https://doi.\norg/10.1007/ 978-1-071 6-0826-5_3 PMID: 32804361\n17. Reimers N, Gurevych I. Sentence -BERT: Senten ce embeddi ngs using siames e BERT-netwo rks.\nEMNLP-IJC NLP 2019–2 019 Conferen ce on Empirica l Methods in Natural Language Processin g and\n9th Internat ional Joint Conferen ce on Natural Language Processin g, Procee dings of the Conferen ce.\n2019;3982– 92.\n18. Li Z, Liu F, Yang W, Peng S, Zhou J. A survey of convolutio nal neural networks: analysis , applications ,\nand prospects. IEEE Trans Neural Netw Learn Syst. 2021;\n19. Aloysius N, Geetha M. A review on deep convolutio nal neural networks. In: 2017 internationa l confer-\nence on communication and signal processing (ICCSP). 2017. p. 588–92 .\n20. Rios A, Kavuluru R. Convolution al neural networks for biomedi cal text classification: application in\nindexing biomedi cal articles. In: Procee dings of the 6th ACM Conferen ce on Bioinformatic s, Computa-\ntional Biology and Health Informatics . 2015. p. 258–67 .\n21. Kim Y. Convolut ional neural networks for sentence classificat ion. EMNLP 2014–2014 Conferen ce on\nEmpirica l Methods in Natural Language Processin g, Proceedings of the Conference . 2014;1746–51 .\n22. Lan W, Xu W. Neural network models for paraphras e identification, semant ic textual similarity, natural\nlanguage inferenc e, and question answering. In: Proceedings of the 27th Internat ional Conferen ce on\nComputat ional Linguistics. 2018. p. 3890–9 02.\n23. Pan M, Wang J, Huang JX, Huang AJ, Chen Q, Chen J. A probabilis tic framework for integrat ing sen-\ntence-leve l semantics via BERT into pseudo-rele vance feedback. Inf Process Manag. 2022; 59\n(1):102734 .\n24. Okon E, Shi L, Tsui R. Automated Diagnos is Coding from Clinical Notes Using Attention-Au gmented\nRecurren t Convolution al Neural Networks. In: AMIA Sympos ium. 2020.\n25. Tan C, Wei F, Wang W, Lv W, Zhou M. Multiw ay Attention Networ ks for Modelin g Sentence Pairs. In:\nIJCAI. 2018. p. 4411–7.\n26. An P, Wang Z, Zhang C. Ensemble unsuperv ised autoenco ders and Gaussian mixture model for cyber-\nattack detection. Inf Process Manag. 2022; 59(2):1028 44.\n27. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Languag e Models are Few-Sho t\nLearners. 2020.\n28. Hoffmann J, Borgeaud S, Mensch A, Buchatsk aya E, Cai T, Rutherfor d E, et al. Training Compute-O pti-\nmal Large Langua ge Models. 2022.\n29. Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, et al. PaLM: Scaling Language\nModelin g with Pathways . 2022.\n30. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-train ing of Deep Bidirec tional Transforme rs for\nLanguage Understand ing. In: Proceedings of the 2019 Conferen ce of the North American Chapter of\nthe Association for Comp utational Linguistics: Human Langua ge Technolog ies, Volume 1 (Long and\nShort Papers). 2019. p. 4171–86.\n31. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the limits of transfer learn-\ning with a unified text-to-te xt transforme r. Journal of machine learning research . 2020; 21(140):1– 67.\n32. Chen Z, Zhang H, Zhang X, Zhao L. Quora question pairs. University of Waterlo o. 2018;\n33. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBER T: a pre-traine d biomedica l language repre-\nsentation model for biomedi cal text mining. Bioinform atics. 2020; 36(4):1234 –40. https:// doi.org/10.\n1093/bioinfo rmatics/btz682 PMID: 31501885\n34. Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, et al. Publicl y available clinical BERT\nembeddi ngs. arXiv preprint arXiv:1904 03323. 2019;\n35. Lin CH, Hsu KC, Liang CK, Lee TH, Liou CW, Lee JD, et al. A disease-sp ecific language representa tion\nmodel for cerebr ovascular disease resear ch. Comput Methods Programs Biomed . 2021; 211:106446.\nhttps://doi.or g/10.101 6/j.cmpb.202 1.10644 6 PMID: 34627022\n36. Zhou C, Li Q, Li C, Yu J, Liu Y, Wang G, et al. A Comp rehensive Survey on Pretrained Foundation Mod-\nels: A History from BERT to ChatGPT . 2023.\n37. Luo R, Sun L, Xia Y, Qin T, Zhang S, Poon H, et al. BioGPT: generat ive pre-traine d transforme r for bio-\nmedical text genera tion and mining. Brief Bioinform . 2022; 23(6):bbac40 9. https://doi.or g/10.1093/ bib/\nbbac409 PMID: 36156661\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 14 / 15\n38. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, et al. LoRA: Low-R ank Adaptation of Large Lan-\nguage Models. 2021.\n39. Qin Z, Wang H, Mawuli CB, Han W, Zhang R, Yang Q, et al. Multi-in stance attention network for few-\nshot learning. Inf Sci (N Y). 2022; 611:464–75.\n40. Sung F, Yang Y, Zhang L, Xiang T, Torr PHS, Hospedales TM. Learning to compare: Relation network\nfor few-shot learning. In: Procee dings of the IEEE conferen ce on computer vision and pattern recogni -\ntion. 2018. p. 1199–208.\n41. Zhang X, Cai F, Hu X, Zheng J, Chen H. A Contrastiv e learning-bas ed Task Adaptation model for few-\nshot intent recognition. Inf Process Manag. 2022; 59(3):1028 63.\n42. Vinyals O, Blunde ll C, Lillicrap T, Wierstra D, others. Matching networks for one shot learning. Adv Neu-\nral Inf Process Syst. 2016;29.\n43. Han S, Kavuluru R. On assessin g the sentiment of general tweets. Vol. 9091, Lecture Notes in Com-\nputer Science (including subseries Lecture Notes in Artifici al Intelligence and Lecture Notes in Bioinfor-\nmatics). 2015.\n44. Gao T, Yao X, Chen D. {SimCSE}: Simple Contrastive Learning of Sentence Embeddings . In: Empirica l\nMethods in Natural Language Processin g (EMNLP) . 2021.\n45. Liu F, Vulić I, Korhonen A, Collier N. Fast, Effectiv e, and Self-Supe rvised: Transfor ming Masked Lan-\nguage Models into Universal Lexical and Sentence Encoders. In: Proceedings of the 2021 Conferen ce\non Empirica l Methods in Natural Language Processin g [Interne t]. Online and Punta Cana, Dominica n\nRepublic: Association for Comput ational Linguistics; 2021. p. 1442–59. Available from: https://\naclanthol ogy.org/2021. emnlp-main. 109\n46. Hashimo to RT and IG and TZ and YD and XL and CG and PL and TB. Alpaca: A Strong, Replicable\nInstruction- Following Model [Interne t]. 2023. Availab le from: https://c rfm.stanford .edu/202 3/03/13/\nalpaca.html\n47. Fushiki T. Estimation of predicti on error by using K-fold cross-valida tion. Stat Comput. 2011; 21:137–\n46.\n48. Goutte C, Gaussier E. A probabilist ic interpretatio n of precision , recall and F-score, with implication for\nevaluatio n. In: European conferen ce on informati on retrieval. 2005. p. 345–59 .\n49. Patra B, Singhal S, Huang S, Chi Z, Dong L, Wei F, et al. Beyond English-C entric Bitexts for Better Mul-\ntilingual Language Representa tion Learning [Internet]. 2022. Available from: https://arxiv.o rg/abs/2210.\n14867\n50. Sanh V, Debut L, Chaumond J, Wolf T. DistilBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:19100 1108. 2019;\n51. Wang Z, Wohlwend J, Lei T. Structured pruning of large language models. arXiv preprint\narXiv:191004 732. 2019;\n52. Xu G, Hu Q. Can model compression improve nlp fairness. arXiv preprint arXiv:220108 542. 2022;\n53. Movva R, Lei J, Longpre S, Gupta A, DuBois C. Combining Compress ions for Multiplicat ive Size Scaling\non Natural Language Tasks. arXiv preprint arXiv:2 20809684. 2022.\nPLOS ONE\nFine-tu ned LLMs on downstream tasks\nPLOS ONE | https://doi.or g/10.137 1/journal.po ne.03170 42 January 10, 2025 15 / 15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7968600988388062
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6414691209793091
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5916462540626526
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5753774046897888
    },
    {
      "name": "Natural language processing",
      "score": 0.5658077001571655
    },
    {
      "name": "F1 score",
      "score": 0.49768713116645813
    },
    {
      "name": "Language model",
      "score": 0.4829972982406616
    },
    {
      "name": "Deep learning",
      "score": 0.4324076771736145
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4301436245441437
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.4111766815185547
    },
    {
      "name": "Machine learning",
      "score": 0.37631309032440186
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1335321130",
      "name": "Children's Hospital of Philadelphia",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    }
  ]
}