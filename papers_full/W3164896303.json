{
  "title": "Knowledge Inheritance for Pre-trained Language Models",
  "url": "https://openalex.org/W3164896303",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098243531",
      "name": "Yujia Qin",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Tsinghua University",
        "Center for Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2131512363",
      "name": "Yankai Lin",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100298610",
      "name": "Jing Yi",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Center for Information Technology",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2097317206",
      "name": "Jiajie Zhang",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence",
        "Center for Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": [
        "Tsinghua University",
        "Center for Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2104573188",
      "name": "Zhengyan Zhang",
      "affiliations": [
        "Tsinghua University",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2096159446",
      "name": "Yu-Sheng Su",
      "affiliations": [
        "Tsinghua University",
        "Center for Information Technology",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Center for Information Technology",
        "Tsinghua University",
        "Peng Cheng Laboratory",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A1906085637",
      "name": "Peng Li",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Center for Information Technology",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Center for Information Technology",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3205544360",
    "https://openalex.org/W3090789082",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2803023299",
    "https://openalex.org/W4394647257",
    "https://openalex.org/W2944551240",
    "https://openalex.org/W3153553004",
    "https://openalex.org/W4285169833",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2808556605",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W4287204036",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3123639697",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W3094039914",
    "https://openalex.org/W3034793897",
    "https://openalex.org/W3023166997",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W2944815030",
    "https://openalex.org/W3034756453",
    "https://openalex.org/W2945667196",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4302023899",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W3156891177",
    "https://openalex.org/W2900096133",
    "https://openalex.org/W2972969579",
    "https://openalex.org/W3162276117",
    "https://openalex.org/W2964222566",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3101066076",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2945785363",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2930786691",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2279376656",
    "https://openalex.org/W2903707108",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W2991040477",
    "https://openalex.org/W3114219454",
    "https://openalex.org/W2946345909",
    "https://openalex.org/W3097132740",
    "https://openalex.org/W2981828710",
    "https://openalex.org/W2952650870",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4308900169",
    "https://openalex.org/W3170925726",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2996428491"
  ],
  "abstract": "Yujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han, Zhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3921 - 3937\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nKnowledge Inheritance for Pre-trained Language Models\nYujia Qin1,2,3, Yankai Lin4, Jing Yi1,2,3, Jiajie Zhang1,2,3, Xu Han1,2,3,\nZhengyan Zhang1,2,3, Yusheng Su1,2,3, Zhiyuan Liu1,2,3,5,6∗, Peng Li7†,\nMaosong Sun1,2,3,5∗, Jie Zhou4\n1Department of Computer Science and Technology, Tsinghua University, Beijing, China\n2Beijing National Research Center for Information Science and Technology\n3Institute for Artiﬁcial Intelligence, Tsinghua University, Beijing, China\n4Pattern Recognition Center, WeChat AI, Tencent Inc.\n5International Innovation Center of Tsinghua University, Shanghai, China\n6Quan Cheng Laboratory\n7Institute for AI Industry Research (AIR), Tsinghua University, China.\nqyj20@mails.tsinghua.edu.cn\nAbstract\nRecent explorations of large-scale pre-trained\nlanguage models (PLMs) have revealed the\npower of PLMs with huge amounts of pa-\nrameters, setting off a wave of training ever-\nlarger PLMs. However, it requires tremen-\ndous computational resources to train a large-\nscale PLM, which may be practically unaf-\nfordable. In addition, existing large-scale\nPLMs are mainly trained from scratch individ-\nually, ignoring that many well-trained PLMs\nare available. To this end, we explore the\nquestion how could existing PLMs beneﬁt\ntraining large-scale PLMs in future. Speciﬁ-\ncally, we introduce a pre-training framework\nnamed “knowledge inheritance” (KI) and ex-\nplore how could knowledge distillation serve\nas auxiliary supervision during pre-training\nto efﬁciently learn larger PLMs. Experi-\nmental results demonstrate the superiority of\nKI in training efﬁciency. We also conduct\nempirical analyses to explore the effects of\nteacher PLMs’ pre-training settings, includ-\ning model architecture, pre-training data, etc.\nFinally, we show that KI could be applied\nto domain adaptation and knowledge trans-\nfer. The implementation is publicly avail-\nable at https://github.com/thunlp/\nKnowledge-Inheritance.\n1 Introduction\nRecently, it has become a consensus in the NLP\ncommunity to use pre-trained language models\n(PLMs) as the backbone for various downstream\ntasks (Han et al., 2021; Min et al., 2021). Despite\n∗Corresponding author.\n†Part of the work was done while Peng Li was working\nat Tencent.\nthe great follow-up efforts of exploring various\npre-training techniques and model architectures,\nresearchers ﬁnd that simply enlarging the model\ncapacity, data size and training steps can further\nimprove the performance of PLMs (Kaplan et al.,\n2020; Li et al., 2020b). This discovery sets off a\nwave of training large-scale PLMs (Raffel et al.,\n2019; Brown et al., 2020; Fedus et al., 2021).\nAlthough huge PLMs have shown awesome per-\nformance (Bommasani et al., 2021), it requires\ntremendous computational resources to train large-\nscale PLMs (Schwartz et al., 2019), raising severe\nenvironmental concerns on the prohibitive compu-\ntational costs. Moreover, existing PLMs are gener-\nally trained from scratch individually, ignoring that\nmany well-trained PLMs are available. This leaves\nus an important question: how could existing PLMs\nbeneﬁt training larger PLMs in future?\nConsidering that humans can leverage the knowl-\nedge summarized by their predecessors to learn\nnew tasks, so that the learning process could be-\ncome efﬁcient; similarly, it is worth inheriting the\nimplicit knowledge distributed in existing PLMs.\nIn this sense, we could distill the knowledge sum-\nmarized by an existing small PLM during pre-\ntraining to efﬁciently learn larger PLMs. We dub\nthe above process as knowledge inheritance (KI).\nThis intuition is similar to reversed KD (Yuan et al.,\n2020) in the ﬁeld of computer vision. They indi-\ncate that a delicate student model could still beneﬁt\nfrom a teacher with an inferior architecture for a\nspeciﬁc downstream task.\nHowever, the success of reversed KD in su-\npervised downstream tasks does not guarantee its\nfeasibility under the scenario of large-scale self-\nsupervised pre-training. Therefore, in this paper,\n3921\nwe strive to answer the following research ques-\ntions: ( RQ1) could distilling knowledge from an\nexisting trained PLM beneﬁt large PLMs’ training\nfrom scratch? ( RQ2) Considering human beings\nare able to hand down knowledge from generation\nto generation, could KI similarly be sequentially\nperformed among a series of PLMs with growing\nsizes? ( RQ3) As more and more PLMs with dif-\nferent pre-training settings (model architectures,\ntraining data, training strategies, etc) emerge, how\nwould different settings affect the performance of\nKI? ( RQ4) Besides training a large PLM from\nscratch, when adapting an already trained large\nPLM to a new domain, how could smaller domain\nteachers beneﬁt such a process?\nIn conclusion, the contributions of this paper\nare summarized as follows: (1) we are the ﬁrst to\nformulate the problem of knowledge inheritance,\nand demonstrate the feasibility of inheriting the\nknowledge from previously trained PLMs for efﬁ-\nciently training larger ones; (2) we show that the\nlearned knowledge in PLMs could accumulate and\nfurther be passed down from generation to gen-\neration; (3) we systematically conduct empirical\nanalyses to show the effects of various teacher pre-\ntraining settings, which may indicate how to select\nthe most appropriate PLM as the teacher for KI; (4)\nwe further show that during domain adaptation, an\nalready trained large PLM could beneﬁt from mul-\ntiple small PLMs of different domains under the KI\nframework. The above empirical studies indicate\nthat KI can well support cross-model knowledge\ntransfer, providing a promising direction to share\nthe knowledge learned by different PLMs and con-\ntinuously promote their performance.\n2 Related Work\nEfﬁcient Pre-training for NLP. Recently, re-\nsearchers ﬁnd that the performance of PLMs can\nbe simply improved by increasing the model size,\ndata size and training steps (Liu et al., 2019; Raffel\net al., 2019; Kaplan et al., 2020), sparking a wave\nof training ever-larger PLMs. For instance, the\nrevolutionary GPT-3 (Brown et al., 2020), which\ncontains 175 billion parameters, shows strong capa-\nbilities for language understanding and generation.\nThis means that utilizing PLMs with huge parame-\nters for downstream tasks may greatly relieve the\ncost of manual labeling and model training for new\ntasks. However, larger models require greater com-\nputational demands (Patterson et al., 2021). To this\nend, researchers propose to accelerate pre-training\nby mixed-precision training (Shoeybi et al., 2019),\ndistributed training (Shoeybi et al., 2019), large\nbatch optimization (You et al., 2020), etc.\nAnother line of methods (Gong et al., 2019; Gu\net al., 2021; Chen et al., 2022; Qin et al., 2022) pro-\nposes to pre-train larger PLMs progressively. They\nﬁrst train a small PLM, and then gradually increase\nthe depth or width of the network based on param-\neter recycling (PR). Although PR could be used for\nthe goal of KI, these methods typically have strict\nrequirements on the architectures of both models,\nwhich is not ﬂexible for practical uses; instead, we\nresort to KD as the solution for KI without archi-\ntecture constraints. In addition, different from KI,\nPR is not applicable for absorbing knowledge from\nmultiple teacher models and domain adaptation.\nMore detailed comparisons between KI and PR are\ndiscussed in appendix E.\nKnowledge Distillation for PLMs. Knowledge\nDistillation (KD) (Hinton et al., 2015) aims to com-\npress a large model into a fast-to-execute one. KD\nhas renewed a surge of interest in PLMs recently.\nSome explore KD at different training phases, e.g.,\npre-training (Sanh et al., 2019), downstream ﬁne-\ntuning (Sun et al., 2019; Krishna et al., 2020), or\nboth of them (Jiao et al., 2020); others explore dis-\ntilling not only the ﬁnal logits output by the large\nPLM, but also the intermediate hidden representa-\ntions (Sanh et al., 2019; Jiao et al., 2020; Sun et al.,\n2020). Conventional KD presumes that teacher\nmodels play pivotal roles in mastering knowledge,\nand student models generally cannot match their\nteachers in performance. When it comes to the\nscenario of KI, since student models have larger\ncapacities, the performance of teacher models is no\nlonger an “upper bound” of student models. Out-\nside NLP, researchers recently demonstrate that\na student model could also beneﬁt from a poor\nteacher for a speciﬁc downstream task (Yuan et al.,\n2020) (reversed KD). Based on the prior explo-\nrations, in this paper, we investigate the application\nof reversed KD in pre-training.\n3 Knowledge Inheritance\nTask Formulation. Given a textual input x =\n{x1,...,x n}and the corresponding label y ∈RK,\nwhere K is the number of classes for the spe-\nciﬁc pre-training task, e.g., the vocabulary size\nfor masked language modeling (MLM) (Devlin\net al., 2019), a PLM M converts each token\n3922\nxj ∈x to task-speciﬁc logits zj = [zj\n1,...,z j\nK].\nzj is then converted to a probability distribution\nP(xj; τ) = [p1(xj; τ),...,p K(xj; τ)] using a soft-\nmax function with temperature τ. Mis pre-trained\nwith the objective LSELF(x,y) = H(y,P(x; τ)),\nwhere His the loss function, e.g., cross-entropy for\nMLM. Assume that we have a well-trained small\nPLM MS optimized with the self learning objec-\ntive LSELF (such as MLM), our goal is leveraging\nMS’s knowledge to efﬁciently train a larger PLM\nML on the corpora DL = {(xi,yi)}|DL|\ni=1 .\nInvestigated Methodology. Speciﬁcally, impart-\ning MS’s knowledge to ML on DL is im-\nplemented by minimizing the Kullback-Leibler\n(KL) divergence between two probability dis-\ntributions output by MS and ML on the\nsame input xi ∈ DL, i.e., LKI(xi; MS) =\nτ2KL(PMS (xi; τ)||PML(xi; τ))). In addition,\nML is also encouraged to conduct self-learning\nby optimizing LSELF(xi,yi). Both LSELF and LKI\nare balanced with an inheritance rate α:\nL(DL; MS) =\n∑\n(xi,yi)∈DL\n(1 −α)LSELF(xi,yi) +αLKI(xi; MS)\n=\n∑\n(xi,yi)∈DL\n(1 −α)H(yi,PML(xi; 1))\n+ ατ2KL(PMS (xi; τ)||PML(xi; τ))).\n(1)\nSince larger models generally converge faster\nand can achieve better ﬁnal performance (Li et al.,\n2020b), ML becomes more and more knowledge-\nable during the learning process, and would sur-\npass the teacher eventually. Thus, it is necessary\nto encourage ML increasingly learning knowledge\non its own, not only following the teacher’s in-\nstructions. Additionally, after ML has surpassed\nits teacher, it no longer needs the guidance from\nMS and should conduct pure self-learning from\nthen on. Therefore, different from reversed KD,\nwe dynamically change the inheritance rate α.\nSpeciﬁcally, for a total training steps of T, we lin-\nearly decay αt with a slope of αT\nT . The student\nonly inherits knowledge from the teacher for T\nαT\nsteps, and then conducts pure self-learning, i.e.,\nαt = max(1−αT ×t\nT,0). Formally, at step t, the\nloss function for inheriting knowledge of MS on\nDL is formulated as:\nL(DL; MS) =\n∑\n(xi,yi)∈DL\n(1−αt)LSELF(xi,yi)+αtLKI(xi; MS).\n(2)\nNote the logits of MS on DL can be pre-\ncomputed and saved ofﬂine so that we do not need\nto re-compute the logits of MS when training ML.\nThis process is done once and for all.\n4 Empirical Analysis\nIn this section, we answer our research questions\nproposed before. Speciﬁcally, (1) we ﬁrst demon-\nstrate the effectiveness of KI in § 4.1. (2) Then\nwe show PLMs can accumulate knowledge over\ngenerations in § 4.2. (3) We also investigate the ef-\nfects of different pre-training settings of the teacher\nmodels in § 4.3. (4) Finally, we show that KI could\nbeneﬁt domain adaptation, and a trained PLM can\nlearn more efﬁciently with the help of multiple\ndomain teachers in § 4.4. Detailed pre-training\nhyper-parameters are listed in appendix B.\n4.1 RQ1: How Could Knowledge Inheritance\nBeneﬁt Large PLMs’ Training?\nSetting. Our KI framework is agnostic to the\nspeciﬁc self-supervised pre-training task and the\nPLM architecture. Without loss of generality, we\nmainly focus on the representative MLM task and\nuse the model architecture of RoBERTa (Liu et al.,\n2019). Speciﬁcally, we ﬁrst choose RoBERTaBASE\n(denoted as BASE) as the teacher ( MS) and\nRoBERTaLARGE (denoted as LARGE) as the student\n(ML). We also experiment on auto-regressive lan-\nguage modeling using GPT (Radford et al., 2018)\nto show KI is model-agnostic.\nFor pre-training data, we use the concatenation\nof Wikipedia and BookCorpus (Zhu et al., 2015)\nsame as BERT (Devlin et al., 2019), with roughly\n3,400M tokens in total. All models (MS and ML)\nare trained for 125k steps, with a batch size of\n2,048 and a sequence length of 512. Note the\nwhole training computations are comparable with\nthose of BERT. We pre-train ML by inheriting\nMS’s knowledge under KI (denoted as “BASE →\nLARGE”). We compare it with “LARGE” that only\nconducts self-learning from beginning to end.\nFor performance evaluation, we report the vali-\ndation perplexity (PPL) during pre-training and the\ndownstream performance on development sets of\neight GLUE (Wang et al., 2019) tasks. Note com-\npared with the self-learning baseline, in KI, the log-\nits output by ML are additionally used to calculate\nLKI, we empirically ﬁnd that the additional compu-\ntations caused by it are almost negligible compared\nwith the cumbersome computations in Transformer\nblocks. Therefore, it requires almost the same com-\nputational cost between KI and the baseline for\n3923\n0 20k 40k 60k 80k 100k 120k\nSteps\n3\n44.18\n5\n6\n7\n8\nPerplexity\nLARGE\nBASE LARGE\nBASE(final)\n(a)\n20k 30k 40k 50k 60k\nSteps\n4.75\n4.955\n5.25\n5.5\n5.75\n6\nPerplexity\nBASE\nMEDIUM BASE_Heviside\nMEDIUM BASE_Constant\nMEDIUM BASE_Linear\nMEDIUM(final) (b)\n10k 20k 30k 40k 50k\nSteps\n5\n5.5\n6\n6.5\n7\n7.5Perplexity\nBASE\nMEDIUM BASE\nMEDIUM BASE_Top 10\nMEDIUM BASE_Top 50\nMEDIUM BASE_Top 100\nMEDIUM BASE_Top 1000\n (c)\nFigure 1: (a) The validation PPL curve for pre-training ML under KI framework (BASE →LARGE) and the self-\nlearning baseline (LARGE). The teacher’s (BASE) performance is 4.18. (b) Pre-training BASE under KI with three\nstrategies for the inheritance rate αt: Linear, Heviside and Constant. The teacher’s ( MEDIUM) perfor-\nmance is 4.95. (c) Pre-training BASE under KI with top-Klogits, we vary Kin {10,50,100,1000}, respectively.\neach step. Hence, we report the performance w.r.t\ntraining step (Li et al., 2020a), while the perfor-\nmance w.r.t. FLOPs (Schwartz et al., 2019) and\nwall-clock time (Li et al., 2020b) can be roughly\nobtained by stretching the ﬁgure horizontally.\nOverall Results. As shown in Figure 1 (a), we\nconclude that: (1) training ML under KI con-\nverges faster than the self-learning baseline, in-\ndicating that inheriting the knowledge from an ex-\nisting teacher is far more efﬁcient than solely learn-\ning such knowledge. That is, to achieve the same\nlevel of validation PPL, KI requires fewer com-\nputational costs. Speciﬁcally, under the guidance\nof MS, whose validation PPL is 4.18, BASE →\nLARGE achieves a validation PPL of3.41 at the end\nof pre-training, compared with baseline (LARGE)\n3.58. After BASE →LARGE stops learning from\nthe teacher at the 40k-th step, it improves the val-\nidation PPL from 4.60 (LARGE) to 4.28, which is\nalmost the performance when the baseline LARGE\nconducts self-learning for 55k steps, thus saving\nroughly 27.3% computational costs1. The results\nin Table 1 show that (2) ML trained under KI\nachieves better performance than the baseline\non downstream tasks at each step. We also found\nempirically that, under the same setting (e.g., data,\nhyper-parameters and model architectures), lower\nvalidation PPL generally indicates better down-\nstream performance. Since the performance gain in\ndownstream tasks is consistent with that reﬂected\nin PPL, we only show the latter for the remaining\nexperiments. Concerning the energy cost, for the\nremaining experiments, unless otherwise speciﬁed,\nwe choose MEDIUM (9 layers, 576 hidden size) as\n1If we load BASE and compute its logits during pre-\ntraining, 18.7% FLOPs can be saved roughly, since the for-\nward passes of the small teacher also take up a small part.\nMS and BASE as ML.\nEffects of Inheritance Rate. We setαtin Eq. (2)\nto be linearly decayed (denoted as Linear) to\ngradually encourage ML exploring knowledge\non its own. We analyze whether this design is\nnecessary by comparing it with two other strate-\ngies: the ﬁrst is to only learn from the teacher at\nﬁrst and change to pure self-learning (denoted as\nHeviside) at the 35k-th step; the second is to\nuse a constant ratio (1 : 1) between LSELF and LKI\nthroughout the whole training process (denoted as\nConstant). We can conclude from Figure 1 (b)\nthat: (1) annealing at ﬁrst is necessary. The vali-\ndation PPL curve of Linear converges the fastest,\nwhile Heviside tends to increase afterMLstops\nlearning from the teacher, indicating that, due to\nthe difference between learning from the teacher\nand self-learning, annealing at ﬁrst is necessary so\nthat the performance won’t decay at the transition\npoint (the 35k-th step). (2) Supervision from the\nteacher is redundant after ML surpasses MS.\nAlthough Constant performs well in the begin-\nning, its PPL gradually becomes even worse than\nthe other two strategies. This indicates that after\nML has already surpassed MS, it will be encum-\nbered by keeping following guidance from MS.\nSaving Storage Space with Top-K Logits.\nLoading the teacher MS repeatedly for KI is cum-\nbersome, and an alternative way is to pre-compute\nand save the predictions of MS ofﬂine once and\nfor all. We show that using the information of\ntop-Klogits (Tan et al., 2019) can reduce the mem-\nory footprint without much performance decrease.\nSpeciﬁcally, we save only top-K probabilities of\nPS(xj; τ) followed by re-normalization, instead of\nthe full distribution over all tokens. For RoBERTa,\n3924\nStep Model CoLA MNLI QNLI RTE SST-2 STS-B MRPC QQP Avg\n5k\nLARGE 0.0 73 .5 81 .7 53 .0 81 .7 45 .8 71 .4 87 .5 61 .8\nBASE →LARGE 17.4 75 .8 83 .4 54 .7 85 .7 72 .0 72 .6 88 .6 68 .8\n45k\nLARGE 61.8 84 .9 91 .7 63 .4 92 .9 88 .6 87 .7 91.5 82.8\nBASE →LARGE 64.3 85 .9 92 .2 75 .3 93 .2 89 .3 89 .4 91.5 85.2\n85k\nLARGE 64.5 86 .8 92 .7 69 .7 93 .5 89 .9 89 .7 91 .7 84 .8\nBASE →LARGE 65.7 87 .2 93 .0 77 .0 94 .3 90 .0 90 .4 91 .8 86 .2\n125k\nLARGE 64.3 87 .1 93.2 73.4 94 .1 90 .3 90.1 91.8 85 .5\nBASE →LARGE 67.7 87 .7 93.1 74.9 94 .8 90 .6 88.2 91.9 86 .1\nTable 1: Downstream performances on GLUE tasks (dev). KI requires fewer pre-training steps to get a high score\nafter ﬁne-tuning. Detailed results at different training steps are illustrated in appendix A.6.\nthe dimension of PS(xj; τ) is decided by its vo-\ncabulary size, which is around 50,000. We thus\nvary K in {10,50,100,1000}to see its effects in\nFigure 1 (c), from which we observe that: top-K\nlogits contain the vast majority of information .\nChoosing a relatively small K(e.g., 10) is already\ngood enough for inheriting knowledge from the\nteacher without much performance decrease. Pre-\nvious work also indicates the relation between KD\nand label smoothing (Shen et al., 2021), however,\nwe show in appendix F that the improvements of\nKI are not because of beneﬁting from optimizing\nsmoothed targets, which impose regularization.\nExperiments on GPT. To demonstrate that KI\nis model-agnostic, we conduct experiments on\nauto-regressive language modeling and choose\nGPT (Radford et al., 2018) architecture with grow-\ning sizes of {73M,124M,209M,354M,773M,\n1B}parameters in total, respectively. The de-\ntailed architectures are speciﬁed in Table 6. All the\nteacher models are pre-trained for 62.5k steps with\na batch size of 2,048. As reﬂected in Figure 2 (a),\ntraining larger GPTs under our KI framework con-\nverges faster than the self-learning baseline, which\ndemonstrates KI is agnostic to the speciﬁc pre-\ntraining objective and PLM architecture.\n4.2 RQ2: Could Knowledge Inheritance be\nPerformed over Generations?\nHuman beings can inherit the knowledge from\ntheir antecedents, reﬁne it and pass it down to\ntheir offsprings, so that knowledge can gradually\naccumulate over generations. Inspired by this,\nwe investigate whether PLMs also have this kind\nof pattern. Speciﬁcally, we experiment with the\nknowledge inheritance among three generations\nof RoBERTa with roughly 1.7x growth in model\nsize: G1 (BASE, 125M), G2 (BASE_PLUS, 211M)\nand G3 (LARGE, 355M), whose architectures are\nlisted in Table 6. All models are trained from\nscratch for 125k steps with a batch size of 2,048\non the same corpus. We compare the differences\namong (1) self-learning for each generation (de-\nnoted as G1, G2 and G3), (2) KI over two gen-\nerations (denoted as G1 →G2, G1 →G3 and\nG2 →G3), and (3) KI over three generations (de-\nnoted as G1 →G2 →G3), where G2 ﬁrst inherits\nthe knowledge from G1, reﬁnes it by additional\nself-exploring and passes its knowledge down to\nG3. The results are drawn in Figure 2 (b). Com-\nparing the performance of G2 and G1 →G2, G3\nand G1 →G3, or G3 and G2 →G3, we can again\ndemonstrate the superiority of KI over self-training\nas concluded before. Comparing the performance\nof G1 →G3 and G1 →G2 →G3, or G2 →G3\nand G1 →G2 →G3, it is observed that the per-\nformance of G3 beneﬁts from the involvements of\nboth G1 and G2, which means knowledge could\nbe accumulated through more generations’ involve-\nments.\n4.3 RQ3: How Could MS’s Pre-training\nSetting Affect Knowledge Inheritance?\nExisting PLMs are typically trained under quite dif-\nferent settings, and it is unclear how these different\nsettings will affect the performance of KI. Formally,\nwe have a series of well-trained smaller PLMs\nMS = {M1\nS,..., MNS\nS }, each having been opti-\nmized on DS = {D1\nS,..., DNS\nS }, respectively. Con-\nsidering that the PLMs in MS, consisting of varied\nmodel architectures, are pre-trained on different\ncorpora of various sizes and domains with arbitrary\nstrategies, thus the knowledge they master is also\nmanifold. In addition, ML’s pre-training dataDL\nmay also consist of massive, heterogeneous corpora\nfrom multiple sources, i.e., DL = {D1\nL,..., DNL\nL }.\nDue to the difference between DL and DS, MS\n3925\n10k 20k 30k 40k 50k\nSteps\n20\n25\n30\n35\n40\n45\n50Perplexity\n124M\n73M 124M\n209M\n124M 209M\n354M\n209M 354M\n773M\n354M 773M\n1B\n773M 1B\n(a)\n10k 20k 30k 40k 50k 60k\nSteps\n4\n4.5\n5\n5.5\n6\n6.5Perplexity\nG1\nG2\nG3\nG1 G2\nG1 G3\nG2 G3\nG1 G2 G3\n (b)\n20k 40k 60k 80k 100k 120k\nSteps\n4\n4.25\n4.5\n4.75\n5\n5.25\n5.5\n5.75\n6\nPerplexity\nBASE\nH_4 BASE\nH_6 BASE\nH_8 BASE\nH_10 BASE\n (c)\nFigure 2: (a) Experiments on GPT. (b) KI over generations. (c) Effects of MS’s architecture (depth).\nmay be required to transfer its knowledge on in-\nstances unseen during its pre-training. Ideally, we\nwant MS to teach the courses it is skilled in. There-\nfore, it is essential to choose the most appropriate\nteacher for each composition D∗\nL ∈DL. To this\nend, we conduct thorough experiments to analyze\nthe effects of several representative factors: model\narchitecture, pre-training data, MS’s pre-training\nstep (appendix A.2) and batch size (appendix A.3).\nEffects of Model Architecture. Large PLMs\ngenerally converge faster and achieve lower PPL,\nthus serving as more competent teachers. We ex-\nperiment with two widely chosen architecture vari-\nations, i.e., depth (number of layers) and width\n(hidden size), to explore the effects ofMS’s model\narchitectures. We choose BASE (12 layer, 768\nhidden size) as ML’s architecture, and choose\nthe architecture of MS to differ from ML in ei-\nther depth or width. Speciﬁcally, for MS, we\nvary the depth in {4,6,8,10}, and the width in\n{384,480,576,672}, respectively, and pre-train\nMS under the same setting asML. The PPL curve\nfor each teacher model is shown in appendix A.7,\nfrom which we observe that deeper / wider teachers\nwith more parameters converge faster and achieve\nlower PPL. After that, we pre-train ML under KI\nleveraging these teacher models. As shown in Fig-\nure 2 (c) and appendix A.4, choosing a deeper\n/ wider teacher accelerates ML’s convergence,\ndemonstrating the beneﬁts of learning from a more\nknowledgeable teacher. Since the performance of\nPLMs is weakly related to the model shape but\nhighly related to the model size (Li et al., 2020b),\nit is always a better strategy to choose the larger\nteacher if other settings are kept the same. In exper-\niments, we also ﬁnd empirically that, the optimal\nduration of learning from the teacher is longer for\nlarger teachers, which means it takes more time to\nlearn from a more knowledgeable teacher.\nEffects of Pre-training Data. In previous exper-\niments, we assume ML is pre-trained on the same\ncorpus as MS, i.e., DL = DS. However, in real-\nworld scenarios, it may occur that the pre-training\ncorpus used by both ML and MS is mismatched,\ndue to three main factors: (1) data size. When\ntraining larger models, the pre-training corpus is of-\nten enlarged to improve downstream performance,\ni.e., |DS|≪|D L|; (2) data domain. PLMs are\ntrained on heterogeneous corpora from various\nsources (e.g., news articles, literary works, etc.),\ni.e., PDS ̸= PDL. The different knowledge con-\ntained in each domain may affect PLMs’ gener-\nalization in downstream tasks; (3) data privacy.\nEven if both size and domain of DS and DL are\nensured to be the same, it may be hard to retrieve\nthe pre-training corpus used by MS due to privacy\nconcerns, with an extreme case: DL ∩DS = ∅.\nThe gap between DS and DL may hinder MS’s\nsuccessful knowledge transfer. We thus design ex-\nperiments to analyze the effects of these factors,\nwith three observations concluded:\n•Obs. 1: PLMs can image the big from\nthe small for in-domain data . To evaluate the\neffects of data size, we ﬁrst pre-train teacher\nmodels on different partitions of the original\ntraining corpus under the same setting by ran-\ndomly sampling {1\n16 ,1\n8 ,1\n4 ,1\n2 ,1\n1 }of it, resulting\nin teacher models with ﬁnal validation PPL of\n{5.43,5.15,5.04,4.98,4.92}, respectively. The ﬁ-\nnal validation PPL increases as we shrink the size\nof MS’s pre-training corpus, which implies that\ntraining with less data weakens the teacher’s abil-\nity. Next, we compare the differences when their\nknowledge is inherited by ML. As reﬂected in\nFigure 3 (a), however, the performance of KI is\nnot substantially undermined until only 1\n16 of the\noriginal data is leveraged by the teacher. This in-\ndicates that PLMs can well image the overall data\ndistribution even if it only sees a small part. Hence,\n3926\n10k 20k 30k 40k 50k\nSteps\n5\n5.5\n6\n6.5\n7\n7.5Perplexity\nBASE\nMEDIUM_ 1\n16 BASE\nMEDIUM_1\n4 BASE\nMEDIUM_1\n2 BASE\nMEDIUM BASE\n(a)\n10k 20k 30k 40k 50k\nSteps\n5\n5.5\n6\n6.5\n7\n7.5Perplexity\nBASE\nMEDIUM_WB : CS = 1 : 2 BASE\nMEDIUM_WB : CS = 2 : 1 BASE\nMEDIUM_WB : CS = 3 : 1 BASE\nMEDIUM_WB : CS = 4 : 1 BASE\nMEDIUM_WB BASE\n (b)\n10k 20k 30k 40k 50k\nSteps\n5\n5.5\n6\n6.5\n7\n7.5Perplexity\nBASE_B\nMEDIUM_A BASE_B\nMEDIUM_B BASE_B\n (c)\nFigure 3: Effects of MS’s pre-training (a) data size, (b) data domain and (c) data privacy for KI.\nwhen training larger PLMs, unless the data size is\nextensively enlarged, its impact can be ignored.\n•Obs. 2: Inheriting on similar domain im-\nproves performance. To evaluate the effects of\ndata domain, we experiment on the cases whereDS\nand DL have domain mismatch. Speciﬁcally, keep-\ning data size the same, we mix Wikipedia and Book-\nCorpus (WB) used before with computer science\n(CS) papers from S2ORC (Lo et al., 2020), whose\ndomain is distinct from WB, using different propor-\ntions, i.e., WB : CS = {1 : 2,2 : 1,3 : 1,4 : 1},\nrespectively. We pre-trainMS on the constructed\ncorpora, then test the performance whenML inher-\nits these teachers’ knowledge on the WB domain\ndata. As shown in Figure 3 (b), with the domain\nof the constructed corpus MS is trained on becom-\ning gradually similar to WB, the beneﬁts from KI\nbecome more obvious, which means it is essential\nthat both MS and ML are trained on similar do-\nmain of data, so that MS can successfully impart\nknowledge to ML by teaching the “right” course.\n•Obs. 3: Data privacy is not that important\nif the same domain is ensured. To evaluate the ef-\nfects of data privacy, we experiment in an extreme\ncase where DS and DL have no overlap at all. To\navoid the inﬂuences of size and domain, we ran-\ndomly split the WB domain training corpus Dinto\ntwo halves (DA and DB) and pre-train two teacher\nmodels (denoted as MEDIUMA and MEDIUMB) on\nthem. After pre-training, both of them achieve al-\nmost the same ﬁnal PPL (4.99) on the same valida-\ntion set. They are then inherited by the student\nmodel BASE on DB (denoted as MEDIUMA →\nBASEB and MEDIUMB →BASEB), which is ex-\nactly the pre-training corpus of MEDIUMB and has\nno overlap with that of MEDIUMA. We also choose\nML that conducts pure self-learning on DB as the\nbaseline (denoted as BASEB). It is observed from\nFigure 3 (c) that, there is little difference between\nthe validation PPL curves of MEDIUMA →BASEB\nand MEDIUMB →BASEB, indicating that whether\nthe pre-training corpus of MS and ML has data\noverlap or not is not a serious issue as long as they\nshare the same domain. This is meaningful when\norganizations aim to share the knowledge of their\nPLMs without exposing either the pre-training data\nor the model parameters due to privacy concerns.\n4.4 RQ4: How Could Knowledge Inheritance\nBeneﬁt Domain Adaptation?\nWith streaming data of various domains continu-\nously emerging, training domain-speciﬁc PLMs\nand storing the model parameters for each domain\ncan be prohibitively expensive. To this end, re-\nsearchers recently demonstrated the feasibility of\nadapting PLMs to the target domain through con-\ntinual pre-training (Gururangan et al., 2020). In\nthis section, we further extend KI and demonstrate\nthat domain adaptation for PLM can beneﬁt from\ninheriting knowledge of existing domain experts.\nSpeciﬁcally, instead of training large PLMs from\nscratch, which is the setting used before, we focus\non adapting BASEWB, which has been well-trained\non the WB domain for 125k steps, to two target do-\nmains, i.e., computer science (CS) and biomedical\n(BIO) papers from S2ORC (Lo et al., 2020). The\nproximity (vocabulary overlap) of three domains\nis listed in appendix D. We assume there exist two\ndomain experts, i.e., MEDIUMCS and MEDIUMBIO.\nEach model has been trained on CS / BIO domain\nfor 125k steps. Note their training computation is\nfar less than BASEWB due to fewer model param-\neters. Hence, either MEDIUMCS or MEDIUMBIO is\nno match for BASEWB in WB domain but has richer\nknowledge in CS / BIO domain. For evaluation,\nwe compare both (1) the validation PPL on the tar-\nget domain and (2) the performance (test F1) on\ndownstream tasks, i.e. ACL-ARC (Jurgens et al.,\n2018) for CS domain and CHEMPROT (Kringelum\net al., 2016) for BIO domain. Before adaptation,\n3927\nNtokens 3,400M 200M 100M 40M 20M\nMetrics F1 PPL F1 PPL F1 PPL F1 PPL F1 PPL\nCS\nSL 69.8 3 .12 71 .7 3 .17 71 .4 3 .24 68 .3 3 .51 67 .5 4 .07\nKI 72.9 3 .06 72 .6 3 .09 71 .9 3 .11 71 .1 3 .21 70 .8 3 .37\nBIO\nSL 84.0 2 .67 82 .8 2 .72 83 .2 2 .83 83 .3 3 .16 82 .7 3 .81\nKI 84.5 2 .65 83 .4 2 .66 83 .9 2 .69 83 .6 2 .82 83 .5 3 .01\nTable 2: The validation PPL (PPL) and downstream performance (F1) on the target domain (CS / BIO) after\nBASEWB is post-trained for 4k steps with self-learning (SL) or knowledge inheritance (KI). We experiment with\ndifferent sizes of domain corpus. All downstream experiments are repeated 10 times with different seeds.\nNtokens 3,400M 200M 100M 40M\nMetrics F1C PPLC F1B PPLB F1C PPLC F1B PPLB F1C PPLC F1B PPLB F1C PPLC F1B PPLB\nSL 71.7 3 .15 83 .7 2 .71 70 .5 3 .97 82 .7 3 .36 67 .7 5 .95 81 .7 4 .84 68 .3 11 .7 81 .1 10 .5\nKI 72.2 3 .15 83 .9 2 .70 71 .8 3 .42 83 .1 2 .92 69 .8 3 .90 82 .6 3 .32 69 .1 5 .70 81 .3 4 .64\nTable 3: The results when BASEWB is post-trained on two new domains simultaneously with self-learning (SL) or\nknowledge inheritance (KI). We report both validation PPL (PPL B / PPLC) and downstream performance (F1 B /\nF1C) for BIO / CS domain. We observe that SL exhibits severe overﬁtting when data is relatively scarce.\nBASEWB achieves a PPL of 5.41 / 4.86 and F1 of\n68.5 / 81.6 on CS / BIO domain, while MEDIUMCS\nachieves 2.95 (PPL) and 69.4 (F1) on CS domain,\nMEDIUMBIO achieves 2.55 (PPL) and 83.6 (F1) on\nBIO domain. This demonstrates the superiority of\ntwo teachers over the student in their own domain\ndespite their smaller model capacity.\nWe compare two strategies for domain adapta-\ntion: (1) only conducting self-learning on the target\ndomain and (2) inheriting knowledge from well-\ntrained domain teachers. Speciﬁcally, BASEWB is\npost-trained for additional 4k steps on either CS or\nBIO domain to learn new knowledge. In addition,\nconsidering that in real-world scenarios, it can be\nhard to retrieve enough pre-training data for a spe-\nciﬁc domain, due to some privacy issues. Hence,\nwe conduct experiments with different sizes of do-\nmain corpus. In Table 2, BASEWB is post-trained\non either CS or BIO domain while in Table 3, it\nis trained on synthetic domain data ( BIO : CS =\n1 : 1) to absorb knowledge from two domains si-\nmultaneously (we assume ML is trained with the\noptimal teacher selection strategy, i.e., each teacher\nimparts the knowledge on its own domain data). It\ncan be concluded from Table 2 and Table 3 that:\n(1) KI is more training-efﬁcient . Compared\nwith self-learning, inheriting knowledge from do-\nmain teachers achieves lower ﬁnal PPL and im-\nproved performance in domain-speciﬁc down-\nstream tasks, indicating that, for domain adapta-\ntion, KI is more training-efﬁcient so that an already\ntrained large PLM could absorb more knowledge\nfrom new domain with the same training budget.\n(2) KI is more data-efﬁcient . The PPL gap be-\ntween KI and SL is further enlarged when there is\nless domain-speciﬁc data available for adaptation,\nwhich means KI is more data-efﬁcient especially\nunder the low-resource setting, where domain data\nis scarce. In other words, only providing a small\nportion of domain-speciﬁc data is enough for sat-\nisfactory adaptation performance under KI, while\nself-learning exhibits overﬁtting to some extent. (3)\nLarge PLMs can simultaneously absorb knowl-\nedge from multiple domains and thus become\nomnipotent. From Table 3, we observe BASEWB\nachieves improved performance on both domains\nafter being taught by two teachers simultaneously.\nKI shows superiority over self-learning. However,\nsimultaneous learning overﬁts training data more\neasily and its performance on either domain is no\nmatch for learning only one domain at a time.\n5 Conclusion and Future Work\nIn this work, we propose a general knowledge in-\nheritance (KI) framework that leverages previously\ntrained PLMs for training larger ones. We con-\nduct sufﬁcient empirical studies to demonstrate its\nfeasibility. In addition, we show that KI could\nwell support knowledge transfer over a series of\nPLMs with growing sizes. We also comprehen-\nsively analyze various pre-training settings of the\nteacher model that may affect KI’s performance,\nthe results shed light on how to choose the most\nappropriate teacher PLM for KI. Finally, we ex-\n3928\ntend KI and show that, during domain adaptation,\nan already trained large PLM could beneﬁt from\nsmaller domain teachers. In general, we provide\na promising direction to share and exchange the\nknowledge learned by different models and contin-\nuously promote their performance.\nIn future, we aim to explore the following di-\nrections: (1) the efﬁciency of KI, i.e., given lim-\nited computational budget and pre-training corpus,\nhow to more efﬁciently absorb knowledge from\nteacher models. Potential solutions include denois-\ning teacher models’ predictions and utilizing more\ninformation from the teacher. How to select the\nmost representative data points for KI is also an\ninteresting topic; (2) the effectiveness of KI under\ndifferent settings, i.e., how can KI be applied if the\nteachers and the students are pre-trained on differ-\nent vocabularies, languages, pre-training objectives\nand modalities.\nFinally, we believe it is vital to use fair bench-\nmarking that can accurately and reliably judge each\nKI algorithm. Thus, we suggest future work to:\n(1) conduct all experiments under the same com-\nputation environment and report the pre-training\nhyper-parameters and hardware deployments in de-\ntail, (2) evaluate the downstream tasks with multi-\nple different random seeds and choose tasks that\ngive relatively stable and consistent results, which\ncould serve as better indicators for PLMs’ effective-\nness. In addition, it is also essential that PLMs are\ntested on diverse downstream tasks which evaluate\nPLMs’ different abilities, (3) save the checkpoint\nmore frequently during pre-training and evaluate\nthe downstream performance, which can better in-\ndicate the trend of PLMs’ effectiveness, and (4)\nopen-source all the codes and model parameters\nfor future comparisons.\nAcknowledgments\nThis work is supported by the National Key R&D\nProgram of China (No. 2020AAA0106502), Insti-\ntute Guo Qiang at Tsinghua University, NExT++\nproject from the National Research Foundation,\nPrime Minister’s Ofﬁce, Singapore under its\nIRC@Singapore Funding Initiative, and Beijing\nAcademy of Artiﬁcial Intelligence (BAAI). This\nwork is also supported by the Pattern Recognition\nCenter, WeChat AI, Tencent Inc. Yujia Qin and\nYankai Lin designed the methods and the experi-\nments. Yujia Qin, Jing Yi and Jiajie Zhang con-\nducted the experiments. Yujia Qin, Yankai Lin and\nXu Han wrote the paper. Zhiyuan Liu, Peng Li,\nMaosong Sun and Jie Zhou advised the project. All\nthe authors participated in the discussion.\nReferences\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities\nand risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nCheng Chen, Yichun Yin, Lifeng Shang, Xin Jiang,\nYujia Qin, Fengyu Wang, Zhi Wang, Xiao Chen,\nZhiyuan Liu, and Qun Liu. 2022. bert2bert: To-\nwards reusable pretrained language models. In As-\nsociation for Computational Linguistics: ACL 2022,\nOnline.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. ArXiv\npreprint, abs/2101.03961.\nTommaso Furlanello, Zachary Chase Lipton, Michael\nTschannen, Laurent Itti, and Anima Anandkumar.\n2018. Born-again neural networks. In Proceedings\nof the 35th International Conference on Machine\nLearning, ICML 2018, Stockholmsmässan, Stock-\nholm, Sweden, July 10-15, 2018 , volume 80 of\nProceedings of Machine Learning Research , pages\n1602–1611. PMLR.\nLinyuan Gong, Di He, Zhuohan Li, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2019. Efﬁcient training of\nBERT by progressively stacking. In Proceedings\n3929\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 2337–2346. PMLR.\nXiaotao Gu, Liyuan Liu, Hongkun Yu, Jing Li, Chen\nChen, and Jiawei Han. 2021. On the transformer\ngrowth for progressive BERT training. In Proceed-\nings of the 2021 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5174–5180, Online. Association for Computational\nLinguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu,\nXiao Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang,\nWentao Han, Minlie Huang, Qin Jin, Yanyan Lan,\nYang Liu, Zhiyuan Liu, Zhiwu Lu, Xipeng Qiu,\nRuihua Song, Jie Tang, Ji-Rong Wen, Jinhui Yuan,\nWayne Xin Zhao, and Jun Zhu. 2021. Pre-trained\nmodels: Past, present and future. ArXiv preprint,\nabs/2106.07139.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. ArXiv\npreprint, abs/1503.02531.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4163–4174, Online. Association for Computational\nLinguistics.\nDavid Jurgens, Srijan Kumar, Raine Hoover, Dan Mc-\nFarland, and Dan Jurafsky. 2018. Measuring the evo-\nlution of a scientiﬁc ﬁeld through citation frames.\nTransactions of the Association for Computational\nLinguistics, 6:391–406.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\nArXiv preprint, abs/2001.08361.\nJens Kringelum, Sonny Kim Kjaerulff, Søren Brunak,\nOle Lund, Tudor I Oprea, and Olivier Taboureau.\n2016. Chemprot-3.0: a global chemical biology dis-\neases mapping. Database, 2016.\nKalpesh Krishna, Gaurav Singh Tomar, Ankur P.\nParikh, Nicolas Papernot, and Mohit Iyyer. 2020.\nThieves on sesame street! model extraction of bert-\nbased apis. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nMengtian Li, Ersin Yumer, and Deva Ramanan. 2020a.\nBudgeted training: Rethinking deep neural network\ntraining under resource constraints. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joey Gonzalez. 2020b.\nTrain big, then compress: Rethinking model size\nfor efﬁcient training and inference of transformers.\nIn Proceedings of the 37th International Conference\non Machine Learning, ICML 2020, 13-18 July 2020,\nVirtual Event, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 5958–5968. PMLR.\nTianyang Lin, Yuxin Wang, Xiangyang Liu, and\nXipeng Qiu. 2021. A survey of transformers. ArXiv\npreprint, abs/2106.04554.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. As-\nsociation for Computational Linguistics.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar\nSainz, Eneko Agirre, Ilana Heinz, and Dan Roth.\n2021. Recent advances in natural language process-\ning via large pre-trained language models: A survey.\narXiv preprint arXiv:2111.01243.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen\nLiang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. 2021. Car-\nbon emissions and large neural network training.\nArXiv preprint, abs/2104.10350.\nYujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022. Elle: Ef-\nﬁcient lifelong pre-training for emerging data. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, Online.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. ArXiv preprint, abs/1910.10683.\n3930\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv\npreprint, abs/1910.01108.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and\nOren Etzioni. 2019. Green ai. ArXiv preprint ,\nabs/1907.10597.\nZhiqiang Shen, Zechun Liu, Dejia Xu, Zitian Chen,\nKwang-Ting Cheng, and Marios Savvides. 2021. Is\nlabel smoothing truly incompatible with knowledge\ndistillation: An empirical study. arXiv preprint\narXiv:2104.00676.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\nArXiv preprint, abs/1909.08053.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332, Hong Kong, China. Association for\nComputational Linguistics.\nSiqi Sun, Zhe Gan, Yuwei Fang, Yu Cheng, Shuohang\nWang, and Jingjing Liu. 2020. Contrastive distil-\nlation on intermediate representations for language\nmodel compression. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 498–508, Online.\nAssociation for Computational Linguistics.\nXu Tan, Yi Ren, Di He, Tao Qin, Zhou Zhao, and\nTie-Yan Liu. 2019. Multilingual neural machine\ntranslation with knowledge distillation. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nChenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan L\nYuille. 2019. Training deep neural networks in gen-\nerations: A more tolerant teacher educates better stu-\ndents. In Proceedings of the AAAI Conference on Ar-\ntiﬁcial Intelligence, volume 33, pages 5628–5635.\nYang You, Jing Li, Jonathan Hseu, Xiaodan Song,\nJames Demmel, and Cho-Jui Hsieh. 2019. Reduc-\ning bert pre-training time from 3 days to 76 minutes.\nArXiv preprint, abs/1904.00962.\nYang You, Jing Li, Sashank J. Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2020. Large batch optimization for deep learn-\ning: Training BERT in 76 minutes. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nLi Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Ji-\nashi Feng. 2020. Revisiting knowledge distillation\nvia label smoothing regularization. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 3903–3911.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19–27.\nIEEE Computer Society.\n3931\nAppendices\nA Additional Experiments and Analysis\nA.1 Effects of Model Size\nWe experiment on four PLMs with roughly 1.7x\ngrowth in model size: M1 (RoBERTaMEDIUM,\n73.5M), M2 (RoBERTaBASE, 125M), M3\n(RoBERTaBASE_PLUS, 211M) and M4\n(RoBERTaLARGE, 355M), whose architectures are\nlisted in Table 6. We ﬁrst pre-train a teacher\nPLM Mi (MS) for 125k steps with a batch\nsize of 2,048 under the same setting then train\na larger one Mi+1 (ML) by inheriting Mi’s\nknowledge under KI framework (denoted as\nMi → Mi+1,i ∈ {1,2,3}). We compare\nMi → Mi+1 with Mi+1 that conducts self-\nlearning from beginning to end. As shown in\nFigure 4, the superiority of KI is observed across\nall models. In addition, with the overall model\nsize of MS and ML gradually increasing, the\nbeneﬁts of KI become more evident, reﬂected in\nthe broader absolute gap between the PPL curve of\nMi →Mi+1 and Mi+1 when igradually grows.\nThis implies that with the advance of computing\npower in future, training larger PLMs will beneﬁt\nmore and more from our KI framework.\nA.2 Effects of MS’s Pre-training Steps\nLonger pre-training has been demonstrated as\nan effective way for PLMs to achieve better\nperformance (Liu et al., 2019) and thus be-\ncome more knowledgeable. To evaluate the ben-\neﬁts of more pre-training steps for MS, we\nﬁrst vary RoBERTaMEDIUM’s pre-training steps in\n{62.5k, 125k, 250k, 500k}, and keep all other\nsettings the same. After pre-training, these\nteacher models achieve the ﬁnal validation PPL\nof {5.25,4.92,4.72,4.51}, respectively. Then we\ncompare the performances when RoBERTaBASE\nlearn from these teacher models and visualize\nthe results in Figure 4, from which we can con-\nclude that, inheriting knowledge from teachers with\nlonger pre-training time (steps) helpsMLconverge\nfaster. However, such a beneﬁt is less and less ob-\nvious as MS’s pre-training steps increase, which\nmeans after enough training computations are in-\nvested, the teacher model enters a plateau of con-\nvergence in validation PPL, and digging deeper in\nknowledge becomes even harder. The bottleneck\nlies in other factors, e.g., the size and diversity of\npre-training data, which hinder MS from becom-\ning more knowledgeable. We also found empiri-\ncally that, after being pre-trained for 125k steps\non the corpus with a batch size of 2,048, all the\nmodels used in this paper have well converged, and\nlonger pre-training only results in limited perfor-\nmance gain in either PPL or downstream perfor-\nmance.\nA.3 Effects of ML’s Batch Size\nBatch size is highly related to PLM’s training ef-\nﬁciency, and previous work (Liu et al., 2019; Li\net al., 2020b; You et al., 2019) found that slow-but-\naccurate large batch sizes can bring improvements\nto model training, although the improvements be-\ncome marginal after increasing the batch size be-\nyond a certain point (around2,048). BERT (Devlin\net al., 2019) is pre-trained for 1,000k steps with\na batch size of 256, and the computational cost is\nequivalent to training for 125k steps with a batch\nsize of 2,048 (Liu et al., 2019), which is the pre-\ntraining setting chosen in our main paper. Choos-\ning RoBERTaMEDIUM as the teacher model and\nRoBERTaBASE as the student model, in Figure 4 we\ncompare the validation PPL as we vary the batch\nsize in {256,512,1024,2,048}, controlling for the\nnumber of passes through the pre-training corpus.\nWe also vary the peak learning rate in {1.0 ×\n10−4,2.5×10−4,3.8×10−4,5.0×10−4}and pre-\ntrain for {1,000k,500k,250k,125k}steps, respec-\ntively, when increasing the batch size. We observe\nthat increasing the batch size results in improved\nﬁnal validation PPL, which is aligned with previ-\nous ﬁndings (Liu et al., 2019). When adjusting\nbatch size, KI accelerates the convergence unani-\nmously, and its beneﬁts become more evident when\ntraining with a smaller batch size, reﬂected in the\nabsolute improvement in ﬁnal validation PPL. We\nhypothesize that this is because learning from the\nsmoothed target probability of KI, containing rich\nsecondary information (Yang et al., 2019) or dark\nknowledge (Furlanello et al., 2018), makes the pre-\ntraining process more stable. The student PLM is\nprevented from ﬁtting to unnecessarily strict distri-\nbutions and can thus learn faster.\nA.4 Additional Experiments of the Effects of\nTeacher Model MS’s architecture\n(width)\nWe show in Figure 5 the validation PPL of ML\nwhen choosing the teacher PLM MS with differ-\nent hidden sizes ( {384,480,576,672}). As men-\ntioned in our main paper, choosing a wider teacher\n3932\n20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n3\n4\n5\n6\n7\n8MLM validation perplexity\nRoBERTaBASE\nRoBERTaBASE_PULS\nRoBERTaLARGE\nRoBERTaMEDIUM RoBERTaBASE\nRoBERTaBASE RoBERTaBASE_PULS\nRoBERTaBASE_PULS RoBERTaLARGE\n10k 20k 30k 40k 50k\nNumber of gradient steps\n5\n5.5\n6\n6.5\n7\n7.5MLM validation perplexity\nRoBERTaBASE\nRoBERTaMEDIUM_62.5k RoBERTaBASE\nRoBERTaMEDIUM_125k RoBERTaBASE\nRoBERTaMEDIUM_250k RoBERTaBASE\nRoBERTaMEDIUM_500k RoBERTaBASE\n2k 4k 6k 8k 10k\nNumber of gradient steps\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5MLM validation perplexity\nRoBERTaBASE_2048\nRoBERTaBASE_1024\nRoBERTaBASE_512\nRoBERTaBASE_256\nRoBERTaMEDIUM RoBERTaBASE_2048\nRoBERTaMEDIUM RoBERTaBASE_1024\nRoBERTaMEDIUM RoBERTaBASE_512\nRoBERTaMEDIUM RoBERTaBASE_256\nFigure 4: Left: effects of ML’s model size. Middle: effects of MS’s number of pre-training steps. Right: effects\nof ML’s batch size.\n20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n4\n4.25\n4.5\n4.75\n5\n5.25\n5.5\n5.75\n6\nMLM validation perplexity\nRoBERTaBASE\nRoBERTaD_384 RoBERTaBASE\nRoBERTaD_480 RoBERTaBASE\nRoBERTaD_576 RoBERTaBASE\nRoBERTaD_672 RoBERTaBASE\n1k 2k 3k 4k 5k\nNumber of gradient steps\n3.0\n3.1\n3.2\n3.3\n3.4\n3.5\n3.6MLM validation perplexity\nRoBERTaCS_40M\nRoBERTaBASE_WB CS_40M\nRoBERTaCS_100M\nRoBERTaBASE_WB CS_100M\nRoBERTaCS_200M\nRoBERTaBASE_WB CS_200M\nRoBERTaCS_3400M\nRoBERTaBASE_WB CS_3400M\n1k 2k 3k 4k 5k\nNumber of gradient steps\n2.6\n2.7\n2.8\n2.9\n3.0\n3.1\n3.2MLM validation perplexity\nRoBERTaBIO_40M\nRoBERTaBASE_WB BIO_40M\nRoBERTaBIO_100M\nRoBERTaBASE_WB BIO_100M\nRoBERTaBIO_200M\nRoBERTaBASE_WB BIO_200M\nRoBERTaBIO_3400M\nRoBERTaBASE_WB BIO_3400M\nFigure 5: Left: the PPL curve when choosing the teacher PLM with different hidden sizes. Middle & Right:\nadapting RoBERTaBASE_WB to CS (middle) / BIO (right) domain with different number of training steps on dif-\nferent sizes of domain data. We compare two strategies: self-learning and KI. For example, RoBERTa CS_3400M\ndenotes post-training RoBERTaBASE_WB with the self-learning strategy on the 3,400M token CS domain corpus.\nRoBERTaBASE_WB→CS_3400M denotes post-training RoBERTaBASE_WB with the KI strategy on the 3,400M token CS\ndomain corpus.\nmodel improves the training efﬁciency of the stu-\ndent PLM.\nA.5 Additional Experiments of Knowledge\nInheritance for Domain Adaptation\nDomain Strategy 3,400M 200M 100M 40M\nCS\nSL 6.71 7 .01 7 .39 8 .77\nKI 8.63 9 .39 9 .48 9 .87\nBIO\nSL 7.29 6 .61 8 .16 10 .34\nKI 10.74 10 .78 10 .93 11 .66\nTable 4: The validation PPL on the source domain\n(WB) after RoBERTaBASE_WB is post-trained on the tar-\nget domain (CS / BIO) with self-learning (SL) and\nknowledge inheritance (KI).\nDifferent Number of Post-training Steps. In\nthe main paper, we adapt RoBERTaBASE_WB to ei-\nther CS or BIO domain by post-training it for 4k\nsteps. We further vary the number of training steps\nin {1k,2k,3k,4k,5k}and visualize the validation\nPPL in Figure 5. We also experiment on different\nsizes of domain corpus, i.e.,3,400M, 200M, 100M,\n40M tokens, respectively, as done in the main pa-\nper. We observe that generally the validation PPL\non each domain decreases with the training step\ngrowing, and the performance of KI is always bet-\nter than self-learning. The improvement of KI over\nself-learning is further enlarged when there is less\ntarget domain data available, demonstrating that\nKI is more data-efﬁcient and can work well in low-\nresource settings. In addition, self-learning exhibits\noverﬁtting problems when the data size of the target\ndomain is relatively small, which is not observed\nunder our KI framework, which means KI can mit-\nigate overﬁtting under low-resource settings.\nCatastrophic Forgetting on the Source Domain.\nTable 4 lists the validation PPL on the source do-\nmain (WB) after RoBERTaBASE_WB is post-trained\non the target domain (CS / BIO) with self-learning\n(SL) and knowledge inheritance (KI) for 4k steps.\nWe show the results w.r.t. different sizes of domain\ncorpus (3,400M, 200M, 100M and 40M tokens).\nWe observe that after domain adaptation, the vali-\ndation PPL on the source domain increases, which\nmeans PLMs may forget some key knowledge on\nthe source domain when learning new knowledge\n3933\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n40\n45\n50\n55\n60\n65\n70Validation Matthews corr\nCoLA\nRoBERTaLARGE\nRoBERTaBASE RoBERTaLARGE\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n74\n76\n78\n80\n82\n84\n86\n88Validation Accuracy\nMNLI\nRoBERTaLARGE\nRoBERTaBASE RoBERTaLARGE\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n82\n84\n86\n88\n90\n92\n94Validation Accuracy\nQNLI\nRoBERTaLARGE\nRoBERTaBASE RoBERTaLARGE\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n55\n60\n65\n70\n75Validation Accuracy\nRTE\nRoBERTaLARGE\nRoBERTaBASE RoBERTaLARGE\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n82\n84\n86\n88\n90\n92\n94Validation Accuracy\nSST-2\nRoBERTaLARGE\nRoBERTaBASE RoBERTaLARGE\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n80\n82\n84\n86\n88\n90Validation Pearson corr\nSTS-B\nRoBERTaLARGE\nRoBERTaBASE RoBERTaLARGE\nFigure 6: Downstream performance visualization on six GLUE tasks comparing RoBERTa LARGE and\nRoBERTaBASE →RoBERTaLARGE. For CoLA, RTE, SST-2 and STS-B, we repeat ﬁne-tuning for 5 times; for\nMNLI and QNLI, we repeat ﬁne-tuning for 3 times.\nin the target domain, i.e., the catastrophic forgetting\nproblem. In addition, we ﬁnd that the problem is\nmore evident for KI than self-learning. We expect\nfuture work to further explore how to mitigate the\ncatastrophic forgetting.\nA.6 Detailed Downstream Performances on\nGLUE Tasks\nFigure 6 visualizes the downstream perfor-\nmance of RoBERTaLARGE and RoBERTaBASE →\nRoBERTaLARGE on the dev sets of six GLUE tasks\nat different pre-training steps with an interval of\n5k. It can be observed that the downstream perfor-\nmance of RoBERTaBASE →RoBERTaLARGE rises\nfaster than the baseline, which means it takes fewer\npre-training steps for our KI framework to get a\nhigh score in downstream tasks. Aligned with pre-\nvious ﬁndings (Li et al., 2020b), we found MNLI\nand SST-2 to be the most stable tasks in GLUE,\nwhose variances are lower.\nWe also list the average GLUE performance for\nRoBERTaBASE → RoBERTaLARGE and the base-\nline RoBERTaLARGE in Table 5, from which we\nobserve that the baseline at 70k-th step achieves\nalmost the same GLUE performance as our method\nat 40k-th step, which means our framework saves\naround 42.9% FLOPs, much higher than the re-\nported 27.3% FLOPs saved based on the pre-\ntraining PPL metric in the main paper. In addition,\nour method achieves almost the same GLUE perfor-\nmance as the baseline at the ﬁnal step (125k) with\nonly 70k steps, which means our framework saves\n44% FLOPs in total. Both the perplexity in the\npre-training stage and performance in downstream\ntasks can be chosen as the evaluation metric for\nmeasuring the computational cost savings. How-\never, in this paper, we choose the former because it\nis more stable and accurate than the latter. We ﬁnd\nempirically that some GLUE tasks like CoLA have\nhigher variances than others, which might make\nthe measurement inaccurate.\nBesides, when discussing the effects of model\narchitectures in the main paper, we only show the\nvalidation PPL of each model during pre-training,\nwe visualize the corresponding downstream perfor-\nmance (MNLI) in Figure 7, from which it can be\nobserved that learning from teacher models with\nmore parameters helps achieve better downstream\nperformance at the same pre-training step. In gen-\neral, we observe that, under our setting, the perfor-\nmance gain in downstream tasks is aligned with\nthat reﬂected in validation PPL during pre-training.\nA.7 Teacher Models’ Validation PPL Curves\nduring Pre-training for “Effects of\nModel Architecture”\nFigure 7 visualizes the validation PPL curves for\nall the teacher models used in the experiments\n3934\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0Validation Accuracy\nMNLI\nRoBERTaBASE\nRoBERTaD_384 RoBERTaBASE\nRoBERTaD_480 RoBERTaBASE\nRoBERTaD_576 RoBERTaBASE\nRoBERTaD_672 RoBERTaBASE\n0 20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n70.0\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0Validation Accuracy\nMNLI\nRoBERTaBASE\nRoBERTaH_4 RoBERTaBASE\nRoBERTaH_6 RoBERTaBASE\nRoBERTaH_8 RoBERTaBASE\nRoBERTaH_10 RoBERTaBASE\n20k 40k 60k 80k 100k 120k\nNumber of gradient steps\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5MLM validation perplexity\nRoBERTaBASE\nRoBERTaH_4\nRoBERTaH_6\nRoBERTaH_8\nRoBERTaH_10\nRoBERTaD_384\nRoBERTaD_480\nRoBERTaD_576\nRoBERTaD_672\nFigure 7: Left & Middle: downstream performances corresponding to the experiments on effects of MS’s model\narchitecture (width (left) & depth (middle)). Right: validation PPL during pre-training for the teacher models used\nin experiments of effects of teacher model architecture.\nStep RoBERTaBASE RoBERTaBASE →RoBERTaLARGE\n5k 61.8 68 .8\n10k 75.6 78 .1\n15k 79.3 81 .5\n20k 80.4 82 .8\n25k 81.7 83 .6\n30k 82.4 83 .9\n35k 83.1 84 .1\n40k 83.6 84 .5\n45k 82.8 85 .2\n50k 83.9 84 .6\n55k 83.4 85 .2\n60k 84.0 85 .7\n65k 84.1 85 .3\n70k 84.3 85 .5\n75k 85.0 85 .8\n80k 84.7 85 .8\n85k 84.8 86 .2\n... ... ...\n125k 85.5 86 .1\nTable 5: Average GLUE performance comparing both\nRoBERTaBASE and RoBERTaBASE → RoBERTaLARGE\nat different pre-training steps.\non the effects of model architecture. The teacher\nmodels differ from RoBERTaBASE in either\nthe depth or width. Speciﬁcally, we vary the\ndepth in {4,6,8,10}(denoted as {RoBERTaH_4,\nRoBERTaH_6, RoBERTaH_8, RoBERTaH_10}),\nand the width in {384,480,576,672} (de-\nnoted as {RoBERTaD_384, RoBERTaD_480,\nRoBERTaD_576, RoBERTaD_672}). Generally,\nPLMs with larger model parameters converge\nfaster and achieve better ﬁnal performance.\nB Pre-training Hyper-parameters\nIn Table 6, we list the architectures we used for all\nmodels, covering the details for the total number\nof trainable parameters (nparams), the total number\nof layers (nlayers), the number of units in each bot-\ntleneck layer (dmodel), the total number of attention\nheads (nheads), the inner hidden size of FFN layer\n(dFFN) and the learning rate when batch size is set\nto 2,048 (lr). The training-validation ratio of pre-\ntraining data is set to 199 : 1. We set the weight\ndecay to 0.01, dropout rate to 0.1, and use linear\nlearning rate decay. Adam is chosen as the opti-\nmizer. The learning rate is warmed up for the ﬁrst\n10% steps. The hyper-parameters for Adam op-\ntimizer is set to 1 ×10−6,0.9,0.98 for ϵ,β1,β2,\nrespectively. For a fair comparison, all experiments\nare done in the same computation environment\nwith 8 NVIDIA 32GB V100 GPUs. Table 7 de-\nscribes the total number of pre-training steps for\neach (ML, MS) pair chosen in our experiments.\nC Fine-tuning Hyper-parameters\nTable 8 describes the hyper-parameters for ACL-\nARC, CHEMPROT and GLUE tasks. The selec-\ntion of these hyper-parameters closely follows (Liu\net al., 2019) and (Gururangan et al., 2020).\nD Domain Proximity of WB, CS and BIO\nTable 9 lists the domain proximity (vocabulary\noverlap) of WB, CS and BIO used in this paper.\nE Comparison between Knowledge\nInheritance and Parameter Recycling\nParameter recycling (i.e., progressive training) ﬁrst\ntrains a small PLM, and then gradually increases\nthe depth or width of the network based on pa-\nrameter initialization. It is an orthogonal research\ndirection against our KI, and has many limitations\nas follows:\nArchitecture Mismatch. Existing parameter re-\ncycling methods (Gong et al., 2019; Gu et al., 2021)\nrequire that the architectures of both small PLMs\n3935\nModel Name nparams nlayers dmodel nheads dFFN lr (bs = 2,048)\nRoBERTaMEDIUM 74M 9 576 12 3072 5 .0 ×10−4\nRoBERTaD_d - 12 d 12 3072 5 .0 ×10−4\nRoBERTaH_h - h 768 12 3072 5 .0 ×10−4\nRoBERTaBASE 125M 12 768 12 3072 5 .0 ×10−4\nRoBERTaBASE_PLUS 211M 18 864 12 3600 3 .5 ×10−4\nRoBERTaLARGE 355M 24 1024 16 4096 2 .5 ×10−4\nGPT73M 73M 9 576 12 3072 5 .0 ×10−4\nGPT124M 124M 12 768 12 3072 5 .0 ×10−4\nGPT209M 209M 18 864 12 3600 4 .0 ×10−4\nGPT354M 354M 24 1024 16 4096 3 .5 ×10−4\nGPT773M 773M 36 1280 20 5120 3 .0 ×10−4\nGPT1B 1068M 40 1440 20 5760 2 .5 ×10−4\nTable 6: Model architectures for all the models we used in this paper.\nML MS Steps of teacher-guided learning\nRoBERTaBASE\nRoBERTaMEDIUM 35k\nRoBERTaD_384 28k\nRoBERTaD_480 40k\nRoBERTaD_576 70k\nRoBERTaD_672 85k\nRoBERTaH_4 22k\nRoBERTaH_6 35k\nRoBERTaH_8 55k\nRoBERTaH_10 65k\nRoBERTaBASE_PLUS RoBERTaBASE 55k\nRoBERTaLARGE\nRoBERTaBASE 40k\nRoBERTaBASE_PLUS 65k\nRoBERTaBASE →RoBERTaBASE_PLUS 75k\nGPT124M GPT73M 10k\nGPT209M GPT124M 15k\nGPT354M GPT209M 18k\nGPT773M GPT354M 16k\nGPT1B GPT773M 20k\nTable 7: The total number of steps for teacher-guided learning for different (ML, MS) pairs.\nand large PLMs are matched to some extent, how-\never, our KI does not have such a requirement. For\nexample, Gong et al. (2019); Gu et al. (2021) ei-\nther requires the number of layers, or the hidden\nsize/embedding size of a large PLM to be the inte-\nger multiples of that of a small PLM. Hence, it is\nnot ﬂexible to train larger PLMs with arbitrary ar-\nchitectures, making parameter recycling hard to be\nimplemented practically. Besides, there are more\nand more advanced non-trivial Transformer mod-\niﬁcations appearing (we refer to Lin et al. (2021)\nfor details), e.g., pre-normalization, relative em-\nbedding, sparse attention, etc. It is non-trivial to\ndirectly transfer the parameters between two PLMs\nif they have different inner structures. Nevertheless,\nour KI framework will not be inﬂuenced by such\narchitectural mismatches.\nInability for Multi-to-one Knowledge Inheri-\ntance. It is non-trivial to support absorbing\nknowledge from multiple teacher models by jointly\nrecycling their model parameters. Instead, it is easy\nto implement for KI. As shown in our experiments,\nwe demonstrate that under our framework, large\nPLMs can simultaneously absorb knowledge from\nmultiple teachers.\nInability of Knowledge Inheritance for Domain\nAdaptation. Parameter recycling is hard to sup-\nport continual learning, which makes large PLMs\nabsorb knowledge from small ones in a lifelong\nmanner. In real-world scenarios, numerous PLMs\nof different architectures are trained locally with\n3936\nHyperParam ACL-ARC & CHEMPROT GLUE\nLearning Rate 2 ×10−5 {1 ×10−5,2 ×10−5,3 ×10−5}\nBatch Size 256 {16,32}\nWeight Decay 0.1 0 .1\nMax Epochs 10 10\nLearning Rate Decay Linear Linear\nWarmup Ratio 0.06 0 .06\nTable 8: Hyper-parameters for ﬁne-tuning RoBERTa on ACL-ARC, CHEMPROT and GLUE.\nWB CS BIO\nWB 100% 19.1% 25.6%\nCS 19.1% 100% 22.5%\nBIO 25.6% 22.5% 100%\nTable 9: Domain proximity (vocabulary overlap)\namong three domains (WB, CS, BIO) discussed in this\npaper. Following (Gururangan et al., 2020), we create\nthe vocabulary for each domain by considering the top\n10k most frequent words (excluding stopwords).\ndifferent data. These small PLMs can be seen as\ndomain experts, and it is essential that larger PLMs\ncan continuously beneﬁt from these existing PLMs\nefﬁciently by incorporating their knowledge so that\nlarger PLMs can become omnipotent. As described\nbefore, it is easy to implement for our framework\nand we have demonstrated the effectiveness.\nModel Privacy. Parameter recycling requires the\navailability of the parameters of an existing PLM,\nwhich may be impractical due to some privacy is-\nsues, e.g., GPT-3 only provides API access for pre-\ndiction instead of the model parameters. Instead,\nour KI framework does not presume access to an\nexisting model parameter since the predictions of\nthe small model can be pre-computed and saved of-\nﬂine. This superiority will further make it possible\nfor API-based online knowledge transfer.\nF Comparing Label Smoothing and\nKnowledge Inheritance\nPrevious work shows the relation between label\nsmoothing and knowledge distillation to some ex-\ntent (Shen et al., 2021). To demonstrate that the\nsuccess of our KI is not because of learning from\na more smoothed target, we conduct experiments\ncomparing both label smoothing and our KI in Ta-\nble 10. Speciﬁcally, for label smoothing, PLMs\noptimize a smoothed target yS\ni = (1−α) ∗yi +\nα∗⃗1/(K −1), where α = 0 denotes learning\nfrom scratch with no label smoothing, larger α\nmeans a more smoothed target for PLMs to learn\nStep 20k 40k 60k 80k 100k\nα= 0.3 8 .68 7 .29 6 .90 6 .57 6 .26\nα= 0.2 7 .27 6 .47 5 .95 5 .68 5 .46\nα= 0.1 6 .71 5 .74 5 .35 5 .06 4 .86\nα= 0 6 .13 5 .21 4 .83 4 .57 4 .36\nKI 5.69 5 .17 4 .78 4 .52 4 .32\nTable 10: Validation PPL for training RoBERTa BASE\nwith different strategies. KI denotes our knowledge in-\nheritance framework, where RoBERTaMEDIUM is chosen\nas the teacher.\nfrom, Kdenotes the vocabulary size. Speciﬁcally,\nwe choose αfrom {0.1,0.2,0.3}. It can be con-\ncluded from the results in Table 10 that adding\nlabel smoothing into the pre-training objectives\nof PLMs leads to far worse performance than the\nvanilla baseline, which shows that the improve-\nments of our knowledge inheritance framework are\nnon-trivial: larger PLMs are indeed inheriting the\n“knowledge” from smaller ones, instead of bene-\nﬁting from optimizing a smoothed target, which\nimposes regularization.\n3937",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.8844970464706421
    },
    {
      "name": "Inheritance (genetic algorithm)",
      "score": 0.72793048620224
    },
    {
      "name": "Computer science",
      "score": 0.6132901310920715
    },
    {
      "name": "Computational linguistics",
      "score": 0.5102086067199707
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5004613399505615
    },
    {
      "name": "Natural language processing",
      "score": 0.4713553786277771
    },
    {
      "name": "Linguistics",
      "score": 0.41705816984176636
    },
    {
      "name": "Cognitive science",
      "score": 0.3980256915092468
    },
    {
      "name": "Philosophy",
      "score": 0.3222191333770752
    },
    {
      "name": "History",
      "score": 0.2048640251159668
    },
    {
      "name": "Psychology",
      "score": 0.19323298335075378
    },
    {
      "name": "China",
      "score": 0.16680222749710083
    },
    {
      "name": "Biology",
      "score": 0.12518540024757385
    },
    {
      "name": "Genetics",
      "score": 0.08955225348472595
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I29955533",
      "name": "Center for Information Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    }
  ]
}