{
    "title": "Hybrid Local-Global Transformer for Image Dehazing.",
    "url": "https://openalex.org/W3200587509",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5001295341",
            "name": "Zhao Dong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5059344854",
            "name": "Jia Li",
            "affiliations": [
                "Beihang University"
            ]
        },
        {
            "id": "https://openalex.org/A5100402298",
            "name": "Hongyu Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5100647349",
            "name": "Long Xu",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3035212293",
        "https://openalex.org/W2783573276",
        "https://openalex.org/W2128254161",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W3034725788",
        "https://openalex.org/W2121880036",
        "https://openalex.org/W2519481857",
        "https://openalex.org/W2948606054",
        "https://openalex.org/W3179869055",
        "https://openalex.org/W3034331889",
        "https://openalex.org/W2885621364",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W2003709967",
        "https://openalex.org/W3110069174",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3089141666",
        "https://openalex.org/W2901543290",
        "https://openalex.org/W2963139417",
        "https://openalex.org/W2028763589",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W3034278302",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W3139049060",
        "https://openalex.org/W2962782447",
        "https://openalex.org/W2746139371",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W2125027853",
        "https://openalex.org/W2962785568",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W2998249728",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3017136408",
        "https://openalex.org/W3173269149",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2256362396",
        "https://openalex.org/W3093348923",
        "https://openalex.org/W3166368936"
    ],
    "abstract": "Recently, the Vision Transformer (ViT) has shown impressive performance on high-level and low-level vision tasks. In this paper, we propose a new ViT architecture, named Hybrid Local-Global Vision Transformer (HyLoG-ViT), for single image dehazing. The HyLoG-ViT block consists of two paths, the local ViT path and the global ViT path, which are used to capture local and global dependencies. The hybrid features are fused via convolution layers. As a result, the HyLoG-ViT reduces the computational complexity and introduces locality in the networks. Then, the HyLoG-ViT blocks are incorporated within our dehazing networks, which jointly learn the intrinsic image decomposition and image dehazing. Specifically, the network consists of one shared encoder and three decoders for reflectance prediction, shading prediction, and haze-free image generation. The tasks of reflectance and shading prediction can produce meaningful intermediate features that can serve as complementary features for haze-free image generation. To effectively aggregate the complementary features, we propose a complementary features selection module (CFSM) to select the useful ones for image dehazing. Extensive experiments on homogeneous, non-homogeneous, and nighttime dehazing tasks reveal that our proposed Transformer-based dehazing network can achieve comparable or even better performance than CNNs-based dehazing models.",
    "full_text": "JOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 1\nComplementary Feature Enhanced Network with\nVision Transformer for Image Dehazing\nDong Zhao, Jia Li, Senior Member, IEEE, Hongyu Li, Long Xu\nAbstract— Conventional CNNs-based dehazing models suffer\nfrom two essential issues: the dehazing framework (limited in\ninterpretability) and the convolution layers (content-independent\nand ineffective to learn long-range dependency information). In\nthis paper, ﬁrstly, we propose a new complementary feature\nenhanced framework, in which the complementary features are\nlearned by several complementary subtasks and then together\nserve to boost the performance of the primary task. One of\nthe prominent advantages of the new framework is that the\npurposively chosen complementary tasks can focus on learning\nweakly dependent complementary features, avoiding repetitive\nand ineffective learning of the networks. We design a new\ndehazing network based on such a framework. Speciﬁcally, we\nselect the intrinsic image decomposition as the complementary\ntasks, where the reﬂectance and shading prediction subtasks are\nused to extract the color-wise and texture-wise complementary\nfeatures. To effectively aggregate these complementary features,\nwe propose a complementary features selection module (CFSM)\nto select the more useful features for image dehazing. Fur-\nthermore, we introduce a new version of vision transformer\nblock, named Hybrid Local-Global Vision Transformer (HyLoG-\nViT), and incorporate it within our dehazing networks. The\nHyLoG-ViT block consists of the local and the global vision\ntransformer paths used to capture local and global dependencies.\nAs a result, the HyLoG-ViT introduces locality in the networks\nand captures the global and long-range dependencies. Extensive\nexperiments on homogeneous, non-homogeneous, and nighttime\ndehazing tasks reveal that the proposed dehazing network can\nachieve comparable or even better performance than CNNs-based\ndehazing models.\nIndex Terms —Image dehazing, Complementary feature\nenhanced framework, Vision transformer, Intrinsic image decom-\nposition\nI. I NTRODUCTION\nI\nN bad weather conditions (such as haze, mist, and fog), the\ncaptured outdoor images are usually degraded by the small\nparticles or water droplets suspended in the atmosphere [1].\nDue to the atmospheric scattering, emission and absorption,\nthe degraded images suffer from color distortion and tex-\nture blurring [2]. Mathematically, the Atmospheric Scattering\nModel (ASM) popularly used to describe the hazy images is:\nID(p) = (IH(p) −A)/t(p) + A, (1)\n. D. Zhao, J. Li, and H. Li are with the State Key Laboratory of Virtual Reality\nTechnology and Systems, School of Computer Science and Engineering,\nBeihang University.\n. J. Li and L. Xu are with the Key Laboratory of Solar Activity, National\nAstronomical Observatories, Chinese Academy of Sciences.\n. J. Li and L. Xu are also with the Peng Cheng Laboratory, Shenzhen 518000,\nChina (e-mail: jiali@buaa.edu.cn)\n. J. Li is the corresponding author. URL: http://cvteam.net\nwhere IH is the hazy image, ID is the scene radiance (haze-\nfree image), A is the atmospheric light, t is the transmission\nmap, and p is the pixel position.\nTo solve the ill-posed problem of the ASM (1), early prior-\nbased single image dehazing methods use handcrafted priors\nthat are observed from the color information, such as dark\nchannel prior (DCP) [3], color attenuation prior (CAP) [4]\nand haze-line prior (HLP) [5], and the texture information,\nsuch as change of detail prior (CoDP) [6] and gradient channel\nprior (GCP) [7]. However, when the priors are invalid in some\ninstances, these methods may generate unnatural artifacts.\nRecently, it has witnessed the ﬂourishing and rapid de-\nvelopment of Convolutional Neural Networks (CNNs), and\nmany CNNs-based image dehazing methods [8, 9, 10] have\nbeen proposed to estimate the haze effects. These methods\ncommonly outperform the prior-based method since the deep\nnetworks can implicitly learn the rich haze-relevant features\nand overcome the limitations of a single speciﬁc prior [11].\nHowever, existing CNNs-based dehazing models suffer from\nessential issues that stem from two aspects: the dehazing\nframework and the convolution layers.\nTo the ﬁrst aspect , existing CNNs-based dehazing meth-\nods can be divided into two categories. The ﬁrst is the\nphysical-based framework (as illustrated in Fig.1 (a)), such\nas DehazeNet [8], multi-scale CNN model (MSCNN) [9]\nand aggregated transmission propagation networks [12], which\ntry to predict the atmospheric light A and/or transmission\nmap t at the ﬁrst step, and then use them to calculate the\nhaze-free image ID according to the ASM (1). However, the\nASM cannot completely represent the complex hazy imag-\ning process since it ignores the emission and absorption of\natmospheric particles, leading to ineffective dehazing results\n[13]. Another category is the fully learning-based [11, 14]\n(as illustrated in Fig.1 (b)), including enhanced Pix2pixHD\ndehazing network (EPDN) [15], GridDehazeNet (GDNet) [16],\nmulti-scale boosted dehazing network (MSBDN) [17] and\nfeature fusion attention network (FFA) [18]. Trained in an\nend-to-end fashion, these models directly recover the haze-\nfree image. However, they have limitations in interpretability\n[11] and usually appear ineffective in dense haze removal. As\nto the second aspect, on the one hand, convolution is content-\nindependent as the same convolution kernel is shared to the\nentire image, ignoring the distinct nature between different\nimage regions [19]. On the other hand, due to inductive biases\nsuch as locality, the convolutions are ineffective to learn long-\nrange dependency information [20, 21]. The above discussions\ninspire us to provide a new framework and a more powerful\nmechanism to replace some of the convolution layers.\narXiv:2109.07100v3  [cs.CV]  5 Jan 2022\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 2\ncomplementary \nfeatures dk\nsubnetwork \nfor task k   \nsubnetwork \nfor dehazing\nshared \nfeatures \nextraction\nsubnetwork \nfor task 1\nsubnetwork \nfor dehazing\nfeatures \nextraction\nsubnetwork for\ntransmission \nestimation\nfeatures \nextraction subnetwork for\natmospheric light \nestimation\nAtmospheric \nScattering \nModel\nhazy imagehazy image\ndehazed image\nhazy image\ndehazed imagedehazed image\n(a) Physical-based Framework (b) Fully Learning-based Framework\n(c) Complementary Feature Enhanced Framework\ns\ncomplementary \nfeature selection \nmodule (CFSM)complementary \nfeatures d1\nelement-wise \naddition\nFig. 1. Comparisons with the different CNNs-based dehazing frameworks:\n(a) physical-based framework, (b) fully learning-based framework, and (c) the\nproposed complementary feature enhanced framework.\nTo these ends, we ﬁrst introduce a novel complementary\nfeature enhanced framework, as illustrated in Fig.1 (c). Un-\nlike previous frameworks that learn the numbers of various\nfeatures at one time implicitly and inefﬁciently, the core\nidea behind the new framework is: different complementary\nsubtasks focus on learning different speciﬁc features, i.e. the\ntask-relevant complementary features, respectively; then, these\ncomplementary features are aggregated together and served\nto the primary task, i.e. the dehazing. We design a new\ndehazing network based on such framework, where speciﬁc\ncomplementary features are explored by corresponding sub-\nnetworks and then collaborated within the dehazing network.\nMotivated by the observations that color and texture percep-\ntions are the critical visual cues toward identifying objects and\nunderstanding scenes [22, 23], we select the intrinsic image\ndecomposition as the ‘complementary tasks’, in which the\nintermediate features learned from reﬂectance prediction are\nserved as the ‘color-wise’ complementary features, and the\n‘texture-wise’ ones are learned from the shading prediction.\nThat is, our method jointly learns intrinsic image decompo-\nsition and haze removal from a single image. Notice that\ndirectly aggregating the redundant, complementary features\nis inefﬁcient, as the complementary subnetworks may output\nsome haze-irrelevant features. Therefore, we further propose\na Complementary Features Selection Module (CFSM) to au-\ntomatically select the ‘right’ features that are helpful to the\ndehazing task and weaken the irrelevant ones.\nVery recently, vision transformer [24] has been known as\nan alternative to CNNs to learn visual representations due to\nits content-dependent interactions [25] and ﬂexibility in mod-\nelling long-range dependencies [26]. However, the quadratic\nincrease in complexity with image size hinders its application\non dehazing tasks, which requires high-resolution feature\nmaps. Additionally, it is undeniable that global and local\ninformation aggregation are useful for low-level vision tasks,\nwhile the vision transformer does not possess the locality [24].\nInspired by these researches, we propose a Hybrid Local-\nGlobal Vision Transformer (HyLoG-ViT), which consists of\nthe local and global vision transformer paths. In the local\nvision transformer path, the standard transformer blocks are\noperated in a grid of non-overlapped regions, enabling the\nmodel to capture the ﬁne-grained and short-distance informa-\ntion within the local regions. In the global vision transformer\npath, one transformer block is operated on the downscaled\nfeature maps to capture the global and long-range dependen-\ncies. Then, the features from the two paths are hybridized by\na convolution layer to improve the local continuity. Compared\nwith the vanilla vision transformer architecture, the HyLoG-\nViT has lower computational complexity and brings locality\nmechanisms to the networks.\nIncorporating the HyLoG-ViT within the complementary\nfeature enhanced framework, we build our dehazing network.\nPrevious works designed speciﬁc approaches for different hazy\nscenes, such as the two-branch neural network (TBNN) [27],\ndiscrete wavelet transform GAN (DW-GAN) [28] and en-\nsemble dehazing networks (EDN) [29] for non-homogeneous\ndehazing, and maximum reﬂectance prior (MRP) [30] and\noptimal-scale fusion-based dehazing (OSFD) [31] methods\nfor nighttime dehazing. Interestingly, extensive experiments\non homogeneous, non-homogeneous, and nighttime dehazing\ntasks reveal that our network exhibits good generalization\nperformances on different hazy scenes without any changes\nin the network architecture during the training. The main\ncontributions of this work are summarized as follows:\n• We propose a new framework and built a complementary\nfeature enhanced network for image dehazing by jointly\nlearning the intrinsic image decomposition and image\ndehazing. The reﬂectance and shading prediction tasks\nprovide rich complementary features for the dehazing\ntask, enabling the network to generate high-quality haze-\nfree images with natural color and ﬁne details.\n• To effectively fuse the complementary features, we\npropose a Complementary Features Selection Module\n(CFSM). The CFSM considerably improves the effective-\nness of feature aggregation by adaptively enhancing the\nproper complementary feature channels while weakening\nthe irrelevant ones.\n• We propose a new variant of vision transformer, namely\nHybrid Local-Global Vision Transformer (HyLoG-ViT),\nwhich can model both local and global dependen-\ncies with lower computational cost than the vanilla\nvision transformer. With the HyLoG-ViT, we propose a\ntransformer-based dehazing network for the ﬁrst time.\nII. R ELATED WORK\nA. Single Image Dehazing\nSingle image dehazing is an ill-posed problem. Without any\nauxiliary information, previous dehazing algorithms require\nthe introduction of speciﬁc prior knowledge. Through research\nin the last decade, many priors have been proposed based on\nthe cues of color (such as the DCP [3], CAP [4], and HLP [5])\nand texture (such as the CoDP [6] and GCP [7]). For example,\nBerman et al. [5] ﬁnd that for a haze-free image, its pixels that\nbelong to the same color cluster form to a point while forming\na line in a hazy image, namely the haze-line. The GCP uses\nthe image gradient to estimate the depth information and the\natmospheric light, preserving texture details more efﬁciently.\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 3\nShading of IC  (ICS)\nReflectance of IC  (ICR)\nClear image (IC)\nHazy image (IH)\n Shading of IH  (IHS)\nReflectance of IH  (IHR) Visible gradients of IH\n0\n5\n10\n15\n20\n25\n30\n35\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nVisible gradients of ICS Visible gradients of IC\nΔE map of IH ΔE map of  IHR ΔE map of ICR\nHomogeneous haze \n(O-HAZE)\nNon-Homogeneous haze \n(NH-HAZE)\nNighttime haze\n(NHR)\nRate of Newly Visible Edges ( )\n(a) Intrinsic image decomposition\n(b) Investigations on the reflectance components (compared with the clear image IC)\n(c) Investigations on the shading components (compared with the hazy image IH)\nCIEDE2000 ( ) with [KL, K1, K2]=[100, 0.045, 0.015]\nHomogeneous haze \n( O-HAZE)\nNon-Homogeneous haze \n(NH-HAZE)\nNighttime haze\n(NHR)\nICS ICIHS\n-0.21\n0.52\n0.72\nICS ICIHS\n-0.19\n1.60\n2.07\nICS ICIHS\n-0.08\n0.28\n0.44\n9.56 10.41\n7.34\nIHR ICRIH\n10.17 9.92\n6.00\nIHR ICRIH\n14.26\n17.51\n6.62\nIHR ICRIH\nFig. 2. Investigations on the intrinsic image decomposition. (a) The reﬂectance and shading maps of clear and hazy images. (b) Investigations on the reﬂectance,\nincluding ∆E maps (left) and CIEDE2000 (right) scores of hazy image IH, hazy reﬂectance IHR and clear reﬂectance ICR compared with the clear image\nIC. The ICR gets the lowest scores on both of the three datasets, indicating that ICR preserves similar color information as the IC. (c) Investigations on\nthe shading, including the visible gradients maps (left) and the rate of newly visible edges indicators (right) of the hazy shading IHS , clear shading ICS and\nclear image IC. As we can ﬁnd, ICS and IC contains rich edges information with higher scores.\nRecently, CNNs-based dehazing networks have been exten-\nsively studied. One kind of CNNs-based network restores the\nclear image by estimating the intermediate variables in the\natmospheric scattering model [32] and then calculates the clear\nimage. For example, the multi-scale CNNs dehazing model\n(MSCNN) [9] utilizes a coarse-scale network to estimate the\ncomplete transmission map and use a ﬁne-scale network to\nreﬁne the results. More recent CNNs-based networks tend to\nlearn hazy-to-clear image translation directly [33]. For exam-\nple, the MSBDN [17] removes haze in an end-to-end manner\nby incorporating boosting and error feedback principles into a\nU-Net [34] architecture with a dense feature fusion module.\nUnlike the previous dehazing frameworks, our method relies\nneither on the atmospheric scattering model nor on a com-\npletely black box system. The subnetworks of intrinsic image\ndecomposition provide color- and texture-wise complementary\nfeatures, enabling the dehazing subnetwork to yield haze-free\nimages with natural color and ﬁne details.\nNon-homogeneous and nighttime dehazing . For different\ncomplicated haze patterns, the dehazing method should be\nspeciﬁcally designed, such as the TBNN [27], DW-GAN [28]\nand EDN [29] for non-homogeneous dehazing, and MRP\n[30] and OSFD [31] methods for nighttime dehazing. Unlike\nthese models, training on different datasets, our network\nexhibits good generalization performances on homogeneous,\nnon-homogeneous, and nighttime dehazing tasks without any\nchanges in the network architecture.\nB. Vision Transformer\nVery recently, vision transformers [24] have received in-\ncreasing research interest in image and video vision tasks,\nincluding object detection [35], image classiﬁcation [24] and\nsemantic segmentation [20]. Many new versions of vision\ntransformers [36, 37, 38, 39] have been proposed to relieve\nthe high computational cost problems. For example, the Swin\nTransformer [37] uses a locally-grouped self-attention [26],\nwhere the input features are separated into a grid of non-\noverlapped windows and the vision transformer is operated\nonly within each window. Many other methods have been\nproposed to bring inductive biases into the vision transformer\n[26, 40, 41, 42], such as the LocalViT [41], which brings\na locality mechanism to ViT by employing the depth-wise\nconvolution into the feed-forward network. CvT [40] proposes\na convolutional token embedding to model local spatial con-\ntexts and a convolutional projection layer to provide efﬁciency\nbeneﬁts.\nNew versions of vision transformers are also developed and\nused in low-level vision tasks. For example, the Uformer [43]\ndesigns a general U-shaped Transformer-based structure for\nimage restoration. It also proposes a locally-enhanced window\nTransformer block to reduce the computation cost. The SwinIR\n[21] utilizes residual Swin Transformer blocks to extract deep\nfeatures for image restoration.\nIn our HyLoG-ViT, the local vision transformer path is sim-\nilar to the LeWin transformer proposed in Uformer. However,\nthe local version used in Uformer alone fails to effectively\nmodel global dependencies and preserve the local continuity\naround those regions. By contrast, our HyLoG-ViT block can\ncapture both local and global dependencies simultaneously.\nIII. M ETHOD\nA. Complementary Features Enhanced Framework\nTo achieve high performance of haze removal, we pro-\npose a novel framework from a new perspective, namely the\ncomplementary feature enhanced framework. In this frame-\nwork, each group of complementary features are learned\nby corresponding complementary tasks, and together they\nserve the primary task (i.e. the dehazing task in this paper).\nUnlike previous CNNs-based approaches that learn haze-\nrelevant features implicitly, complementary features here can\nbe purposively chosen and learned by training under spe-\nciﬁc subnetworks and datasets. Problems that arise here are:\nwhich complementary features should be considered for image\ndehazing, and which complementary tasks are used to learn\nsuch features effectively?\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 4\n(c) Decoder for Reflectance/ \nShading prediction (DR/DS)\n(d) Decoder for dehazing \n(DD)\n(a) Overall dehazing network\nHazy Image\nIH\nHyLoG-ViT\n Block\nHyLoG-ViT \nBlock\nHyLoG-\nViT Block\n...\nConv. 2\nConv. 2\nFeature\n Extraction\nLayer\ne0\ne1\neZ-1\neZ\nHyLoG-ViT \nBlock\nHyLoG-ViT\n Block\nImage \nReconstruction\nLayer\nReflectance / Shading \nImage IR/IS\n...\nDeConv. 2 \nDeConv. 2 \nHyLoG-\nViT Block\nConcatenation\nConcatenation\neZ\neZ-1\ne0\ne1\ndR/S\nZ-1\ndR/S\n1\n Encoder (E)\nDecoder (DR)\nDecoder (DS)\nDecoder (DD)\nGTGenerated\nLR\nLD\nLS\nUSI3D\ndS\nZ-1\nHyLoG-ViT \nBlock\nHyLoG-ViT\n Block\nImage \nReconstruction\nLayer\nDehazed  Image\nID\n...\nDeConv. 2 \nDeConv. 2 \nHyLoG-\nViT Block\nCFSM\nCFSM\ndR\nZ-1\ndR\n1\ndS\n1\n(b) Encoder (E)\neZ\ne0\nFig. 3. Architectures of our dehazing network. (a): the overall dehazing network, which consists of an encoder and three different decoders. (b): the architecture\nof the shared encoder E. (c): the architecture of decoder DR/DS. (d): the architecture of decoder DD. USI3D: a pre-trained unsupervised intrinsic image\ndecomposition model proposed by [44]. CFSM: the complementary features selection module. LR, LS and LD: loss functions for the three tasks.\nBefore answering these problems, we here analyse the\nessential criterion the complementary features should follow.\nFirst, each kind of complementary feature represents one\ninterference factor closely related to the prime task. Second,\nthe different complementary features should be weakly cor-\nrelated with each other so that each complement subtask can\nfocus on doing one thing more effectively. Otherwise, multiple\nsubnetworks learn similar representations, making the network\nvery inefﬁcient. That is, ‘Each performs one’s own functions;\ntogether serve their common mission’.\nComplementary Features: Intuitively, the images captured\nin hazy or foggy scenes suffer from some interference factors,\nand the most conspicuous ones are the variation of color\nand blurriness of edges. Many previous works have witnessed\nthat the information of color [3, 4, 5] and texture [6, 7]\nare the crucial priors for estimating the haze distribution.\nInspired by these observations, we endeavour to build the\ncorresponding subnetworks to learn the color-wise and texture-\nwise complementary features.\nComplementary Tasks: In further investigations, we fund\nthat the intrinsic image decomposition (which decomposes\nan image into reﬂectance and shading, as shown in Fig.2\n(a)) is a good candidate as the complementary task; as the\nreﬂectance component contains the actual color of the scenes,\nand the shading component contains the structure and tex-\nture information [45]. To demonstrate these, we analyse the\ncharacters of the reﬂectance and shading of the hazy and clear\nimages on the homogeneous, non-homogeneous and nighttime\nhaze datasets. Fig.2 (b) shows the ∆E maps and CIEDE2000\n[46]1 metrics of the hazy image IH, hazy reﬂectance IHR\nand clear reﬂectance ICR compared with the clear image IC.\nThe investigations demonstrate that the clear reﬂectance ICR\nmaintain the color information of clear image IC with very\nlow ∆E and CIEDE2000. Similarly, we visualise the visible\ngradients maps and calculate the Rate of Newly Visible Edges\nrnve [47] of hazy shading IHS , clear shading ICS and clear\nimage IC compared with the hazy image IC, as illustrated in\n1. We set the Luminance coefﬁcient KL, weighting factors K1 and K2 here\nare 100, 0.045 and 0.015, respectively, to offset the impact of the luminance\ncomponent.\nFig.2 (c). We ﬁnd that the rnve of ICS is very close to the\none of IC on the tree datasets, indicating that the shading ICS\npreserves rich edge and texture information.\nFinally, we choose the Unsupervised Learning for Intrin-\nsic Image Decomposition (USI3D) [44] method to gener-\nate clear reﬂectance and shading samples in our training\ndatasets. Most importantly, the USI3D assumes reﬂectance-\nshading independence, coinciding with the second criterion\nof complementary features as aforementioned. The reﬂectance\npredilection subnetwork in our model can focus on color-wise\ncomplementary features learning, avoiding the interferences of\nshading components; and vice versa.\nB. Dehazing Network\nIn our dehazing model, the intrinsic image decomposition\nand dehazing are considered a joint model by exploring\nthe former’s complementary features that the latter requires.\nSpeciﬁcally, our network is a multi-task learning model with\na decoder-focused architecture [48].\n1) Overall Architecture: The overview of our dehazing\nnetwork consists of a shared encoder and three decoders, as\nillustrated in Fig.3. Denote the hazy image is IH, the shared\nencoder E is used to extract the shallow and deep features:\ne0 = F0\nE(IH), ez = Fz\nE(ez−1), (2)\nwhere F0\nE denotes the feature extraction layer used to extract\nshallow feature e0; Fz\nE denotes z-th stage of encoder E, ez\nrefers to the deep feature at stage z. z ∈[1,··· ,Z], and Z\nis the total number of stages. Three parallel decoders follow\nthe encoder. Decoders DR and DS are used to predict the\nreﬂectance and shading of the haze-free image, respectively,\nand their intermediate features are served as complementary\nfeatures to the decoder DD to generate the high-quality haze-\nfree image. The decoders are described as:\ndZ\nR = FZ\nDR(dZ\nE), dz\nR = Fz\nDR(dz−1\nR ,ez), (3)\ndZ\nS = FZ\nDS (dZ\nE), dz\nS = Fz\nDS (dz−1\nS ,ez), (4)\ndZ\nD = FZ\nDD (eZ,dZ\nR,dZ\nS ), dz\nD = Fz\nDD (dz−1\nD ,dz\nR,dz\nS), (5)\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 5\nwhere Fz\nDR, Fz\nDS and Fz\nDD denote the z-th stage of decoder\nDR, DS and DD, respectively, z ∈[1,··· ,Z]. dz\nR, dz\nR and\ndz\nR are the intermediate features of the decoder DR, DS and\nDD at stage z, respectively. The ﬁnal reﬂectance IR, shading\nIS, and haze-free image ID are generated through three image\nreconstruction layers, i.e., F0\nDR, F0\nDS and F0\nDD , respectively:\nIR =F0\nDR(d1\nR,e0), I S = F0\nDS (d1\nS,e0),\nID =F0\nDD (d1\nD,d1\nR,d1\nS,e0). (6)\n2) Details of the Encoder and Decoder: The encoder E\ncomprises of a feature extraction layer and a series of HyLoG-\nViT blocks (see section III-D). The extraction layer is built\nby a 5 ×5 convolution and a basic ResNet block [49] to\nextract shallow features. Each HyLoG-ViT block is followed\nby a 4×4 convolution to downscale the spatial resolution with\nstride 2 and double the channel number. The convolutions used\nhere can also bring the inductive bias into this ‘transformer-\nbased’ encoder. In the decoder DR and DS, except the bottom\nstage Z, feature dz−1\nR/S from the previous decoder block is\nﬁrst upscaled by a 4 ×4 deconvolution to expand the spatial\nresolution with stride 2 and halve the channel number. Then,\nthe output is concatenated with the feature ez from the same\nstage of encoder E. Therefore, the subnetworks E-DR and E-\nDS are formed into two U-shaped structures, which alleviates\nthe issue of spatial information loss caused by downscaling.\nDifferent from decoder DR and DS, for z-th stage in DD,\nthere have three inputs: the feature dz−1\nD from the previous\nstage; the complementary features dz\nR and dz\nS from the same\nstage of DR and DS, respectively. These input features are\nfed into a complementary features selection module to select\nthe most useful channels from dz\nR and dz\nS dynamically.\nC. Complementary Features Selection Module (CFSM)\nOne can simply aggregate the complementary features dz\nR\nand dz\nS via element-wise summation or concatenation oper-\nation, which, however, is inefﬁcient. Therefore, we propose\nthe CFSM to fuse the complementary features in a nonlinear\nfashion. The architecture of the CFSM is illustrated in Fig.4.\nGiven the complementary features dz\nR and dz\nS and the\nfeature dz−1\nD (∈Rh×w×c), they are ﬁrst combined via element-\nwise summation. Then, the outputs are transformed to two\nchannel-wise statistics save and smax (∈R1×1×c) by a global\naverage pooling and a global max pooling, respectively. Each\nstatistic is separated into two streams: one stream for dz\nR\nfeature selection and another for dz\nS. Take dz\nR stream as\nan example, the save and smax are followed by channel-\ndownscaling 1×1 convolution layers to calculate the compact\nfeature vectors, vave R and vmax R (∈R1×1×c/r, where r= 4\nin our model), respectively. The feature vectors are fed into\ntwo parallel channel-upscaling 1 ×1 convolution layers and\nprovide feature descriptors tave R and tmax R (∈R1×1×c).\nThe ﬁnal feature descriptor is deﬁned as tR = tave R+tmax R.\nThe tR passes through the Sigmoid function to generate\nattention score aR (∈R1×1×c) for dz\nR. Similar, we can get\nthe attention score aS for dz\nS. The overall process of feature\nrecalibration and aggregation is deﬁned as:\nˆdz−1\nD = dz−1\nD + aR ·dz\nR + aS ·dz\nS. (7)\nGlobal \nMax \nPooling\nSigmoid\nConv. 1×1\nReLUReLU\nConv. 1×1\nGlobal \nAverage \nPooling\nSigmoid\nConv. 1×1\nReLUReLU\nConv. 1×1\nh×w×c\nh×w×c\nh×w×c\nh×w×c\n1×1×c\nConv. 1×1 Conv. 1×1Conv. 1×1 Conv. 1×1\n1×1×c\n1×1×c/r1×1×c/r\n1×1×c1×1×c\nh×w×c\nh×w×c\ntR\naR\ntS\naS\naS  dS\nz   \naR  dR\nz   \nh×w×c\nsave\nsmax\ndD\nZ-1\ndD\nZ-1  \ndD\nZ-1\ndR\nZ\ndS\nZ\nFig. 4. Schematic for the complementary features selection module (CFSM).\n⊕denotes the element-wise summation. ⊗denotes the element-wise produc-\ntion.\nNote that our CFSM utilizes the global average- and max-\npooling to gather important clues about complementary fea-\ntures, increasing the representation power and can inferring\nﬁner channel-wise attention [50].\nD. Hybrid Local-Global Vision Transformer (HyLoG-ViT)\nWe propose a HyLoG-ViT model to address the challenges\nof high computation cost and lack of locality. As shown in\nFig.5, a HyLoG-ViT block consists of two paths. In the local\nvision transformer path, the input feature map is grouped into\ngrids of non-overlapped regions, and the transformer block\nis applied only within each region. Given the feature maps\nX ∈RH×W×C where the H, W and C are the height, width\nand channel number of the maps, the computation of the local\nvision transformer path can be expressed as:\nX = {X1,X2,··· ,XNl×Nl};\nXi\nl = Transformer (Xi), i ∈[1,2,··· ,Nl ×Nl];\nXl = {X1\nl ,X2\nl ,··· ,XNl×Nl},\n(8)\nwhere Xi\nl is the output of the i-th region, Xl is the combined\noutput of the local vision transformer path; Transformer (·)\nrepresents the standard transformer layer, as illustrated in Fig.5\n(b). Nl is the region number per column/row. This design\nenables the vision transformer to focus on capturing region-\nlevel attention and exploring local context information. How-\never, the local vision transformer path cannot model global\ndependencies and lose local continuity around those regions.\nTherefore, we also introduce the global vision transformer\npath, where the input feature is down-sampled by the average\npooling operation to reduce the spatial resolution. The output\nis fed into the standard transformer layer. The computation of\nthe global vision transformer path is:\nXg =Upsamp↑Ng (Transformer (Avepool↓Ng (X)));\n(9)\nwhere Xg is the output of the global vision transformer path.\nAvepool↓Ng (·) is the average pooling with reduction ratio\nNg; Upsamp↑Ng (·) represents the upsampling operation with\nupscale ratio Ng. The global vision transformer path improves\nefﬁciency while still maintaining the capability of aggregating\nglobal information. The hybrid outputs of the two paths are\nconcatenated and transformed to the original dimension by a\n3 ×3 convolution layer:\nXh = Conv3×3(Concat(Xl,Xg)); (10)\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 6\nf\nGlobal Vision Transformer Path\n(a) Hybrid Local-Global Vision Transformer  (HyLoG-ViT) block (b) Transformer layer\nPatch Embedding\nLayer \nNormalization\nMulti-head\nSelf-Attention\nLayer \nNormalization\nFeed Foward\nadd\nadd\nLocal Vision Transformer Path\nTransformer layer\nFig. 5. Diagram of the hybrid local-global vision transformer (HyLoG-ViT).\nf⃝represents the fuse operation.\nwhere Xh is the ﬁnal output of HyLoG-ViT block; Concat(·)\nis the channel-wise concatenation; Conv3×3(·) is a 3 ×3\nconvolution layer. This fuse operation maintains the merits of\nthe local and global vision transformer and introduces locality\ninto the network.\nPositional Encoding: The positional encoding used in trans-\nformers aims to retain positional information. However, for\nlow-level vision tasks, the input images with varying sizes\nare commonly different from the training ones. As a result, it\nmay decrease performance [51, 52] and break the translation\ninvariance [26]. Note that our model can achieve good results\neven without additional positional encoding modules. We\nargue that the 3 ×3 convolution used in (10) is sufﬁcient to\nprovide positional information for Transformers.\nComplexity Analysis: For the local vision transformer path,\ndenote the input feature X ∈ RH×W×C is grouped into\nNl ×Nl regions and the spatial-resolution of each region is\nH\nNl\n×W\nNl\n×C. Then, the complexity of the self-attention in the\nlocal vision transformer path is reduced from O((HW)2C)\nto O((HW )2\nN2\nl\nC) compared with the standard self-attention.\nFor the global vision transformer path, denote the spatial\ndimension of the input feature is downscaled by a reduction\nratio Ng, and the complexity is O((HW )2\nN2g\nC). Therefore, the\ntotal self-attention computational complexity of HyLoG-ViT\nblock is O(( 1\nN2\nl\n+ 1\nN2g\n)(HW)2C).\nE. Loss Function\nIn our training, the ground truths of reﬂectance ICR and\nshading ICS are generated by operating the pre-trained intrin-\nsic image decomposition model USI3D [44] on the ground\ntruth of clear image IC. For reﬂectance and haze-free image\nestimations, we train the networks using the L2 reconstruction\nloss, conditional adversarial loss [53, 54], and SSIM loss (as\nused in [55]). For the shading estimation, we use the L2\nreconstruction loss and an edge preserve loss which is deﬁned\nas:\nLe =EIS [||▽xIS(p) −▽xICS (p)||2\n+ ||▽yIS(p)) −▽yICS (p)||2], (11)\nwhere E is the mean operation on a batch of samples; ▽x\nand ▽y are the spatial derivatives at x and y directions,\nrespectively. prefers to the spatial location. Overall, the hybrid\nloss function contains three parts, that is:\nL= λRLR + λSLS + λDLD, (12)\nwhere LR, LS, and LD are the loss functions for the three\nvision tasks, respectively.\nIV. E XPERIMENT\nA. Experiment Setup\n1) Dataset: Our dehazing model is trained on outdoor\ndatasets, including RESIDE dataset [56], NH-HAZE dataset\n[57] and NHR dataset [31] for homogeneous haze, non-\nhomogeneous haze and nighttime haze removal, respectively.\nFor RESIDE dataset, we randomly select 41240 samples from\nOTS [56] (outdoor training subset in RESIDE) for training\nand 500 samples from SOTS [56] (synthetic objective testing\nsubset in RESIDE) for testing. For the NH-HAZE dataset,\nwe synthetic 9800 samples augmented from 50 original high-\nresolution samples by randomly cropping, ﬂipping, and ro-\ntating; and the 41 ∼45-th samples are used for qualitative\nevaluations. The NHR dataset contains 17900 samples; we\nselect the last 475 for testing and others for training. We also\ncollect 372 and 132 real-world daytime and nighttime hazy\nimages to evaluate the performance of our model.\n2) Implemental Details: Our dehazing model is imple-\nmented with PyTorch library and trained on one NVIDIA\nGeForce RTX 3090 GPU. Our model is trained for 32 epochs\non the RESIDE dataset, 20 epochs on the NH-HAZE dataset\nand 5 epochs on the NHR dataset. We adopt Adam [58]\noptimizer with the initial learning rate is 10−4 for both\nCNN and vision transformer backbones. We use the layer\nnormalization [59] in the vision transformer blocks and Ac-\ntivation Normalization [60] in CNN layers. The parameters\nof the loss functions are set as ( λR,λS, λD) = (1,1,1.5).\nFor the HyLoG-ViT block, we set Nl = 8 and the patch\nsize as 2 ×2 in the local vision transformer path; and in\nthe global vision transformer path, we set Ng = 4 and the\npatch size as 4 ×4. The code of our model is available at\nhttps://github.com/phoenixtreesky7/CFEN-ViT-Dehazing.\n3) Metrics: We use the peak signal-to-noise ratio (PSNR),\nstructural similarity index measure (SSIM) [61], learned per-\nceptual image patch similarity (LPIPS) [62] and CIEDE2000\n[46] to evaluate the dehazing methods on the synthetic\ndatasets. LPIPS measures the ‘perceptual distance’ of the two\ncompared images using deep features, which coincides well\nwith human perceptual similarity judgments [63]. We use\nthe CIEDE2000 to measure the color difference between the\ndehazed image and its corresponding clear image. We set the\nparameters [KL, K1, K2] in CIEDE2000 are [2, 0.045, 0.015]\nin the experiments. Another metric we used is the bind contrast\nenhancement assessment [47] which contains three indicators,\nthat are: 1) the rate of newly visible edges ( rnve) to evaluate\nthe ability of edge restoration; 2) the geometric mean of the\nnormalized gradient (mng) to measure the quality of the image\ncontrast; 3) the rate of saturated pixels ( rsp) to measure the\ndegree of over-saturation [47]. These three indicators are used\nto evaluate dehazing models on real-world datasets.\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 7\n(ii) O-HAZE\n(b) DCP\n (c) HLP\n (d) MSCNN\n (e) GDNet\n (f) EPDN\n (h) FFA\n(g) MSBDN\n(a) hazy image\n (l) GT\n(k) Ours\n(iii) HazeRD\n (i) SOTS\nFig. 6. Dehazing results of the Synthetic Homogeneous hazy images (on the RESIDE, O-HAZE and HazeRD datasets). (a): the hazy images. (b)-(k): the\ndehazing results of DCP [3], HLP [5], MSCNN [9], GDNet [16], EPDN [15], MSBDN [17], FFA [18] and Ours, respectively. (l): the ground truth.\n(b) Ours\n(a) hazy image\n ( c) DCP\n (d) HLP\n (e) MSCNN\n (f) GDNet\n (h) MSBDN\n (i) FFA\n(g) EPDN\nFig. 7. Dehazing results of the Real-World Homogeneous hazy images. (a): the hazy images. (b): Our dehazing results. (c)-(k): the dehazing results of Ours,\nDCP [3], HLP [5], MSCNN [9], GDNet [16], EPDN [15], MSBDN [17] and FFA [18], respectively.\nB. Comparisons with State-of-the-art Methods\n1) Homogeneous Dehazing: To demonstrate the effective-\nness of our dehazing model on homogeneous image dehazing,\nwe ﬁrstly evaluate it on the synthetic datasets, including SOTS\n(outdoor) [56], O-HAZE [64], and HazeRD [65]. We compare\nour method with the prior-based and learning-based models,\nincluding DCP [3], HLP [5], MSCNN [9], GDNet [16], EPDN\n[15], MSBDN [17], and FFA [18].\nIt can be found from Fig.6 ( i), over-saturation phenomenon\nemerges at the sky regions in the prior-based methods of\nDCP and HLP, indicating that those priors may be invalid\nunder such conditions. The deep learning-based approaches\ncan provide visually pleasant dehazed results on the SOTS\ndataset [56]. The metrics of PSNR, SSIM and LPIPS also\ndemonstrate that GDNet and MSBDN obtain the top two\nscores on the SOTS dataset. However, those methods exhibit\nrelatively poor performances on the other two challenging\ndatasets, either leading to color distortion results (such as\nEPDN) or leaving haze at distant scenes (such as GDNet,\nMSBDN and FFA), as illustrated in Fig.6 ( ii) and ( iii). Our\nmodel obtains the best PSNR, SSIM, LPIPS and CIEDE2000\non the O-HAZE and HazeRD datasets.\nWe further evaluate our method on real-world datasets. The\nvisual comparisons are shown in Fig.7. The prior-based meth-\nods, i.e. DCP and HLP, tend to yield over-enhanced visual arti-\nfacts, especially in the sky regions. Although the CNNs-based\nmethods mitigate the over-estimation artifacts successfully,\nsome of them would produce insufﬁcient dehazing in distant\nscenes. For example, the dehazed images of MSBDN and FFA\nappeared almost indistinguishable from the original hazy ones.\nThe reason might lie in the overﬁtting on the training datasets.\nAs obviously shown in Fig.7 (b), our dehazing model produces\nplausible visual dehazing results with vivid color and ﬁne\ndetails. The clear superiority of our model on the real-world\ndataset is attributed to the complementary feature enhanced\nframework. The experimental results also demonstrate that\nour model learns the color- and texture-wise complementary\nfeatures effectively.\n2) Non-Homogeneous Dehazing: We then evaluate our\nmethod on the non-homogeneous dehazing dataset NH-\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 8\nTABLE I\nHOMOGENEOUS IMAGE DEHAZING RESULTS ON THE RESIDE, O-HAZE, H AZE RD, AND REAL -WORLD DATASETS .[a]\nDataset Metric DCP HLP MSCNN GDNet EPDN MSBDN FFA Ours\nSOTS\nPSNR↑ 14.57 16.44 19.15 29.78 27.42 33.79 29.92 32.17\nSSIM↑ 0.749 0.784 0.855 0.978 0.946 0.985 0.975 0.970\nLPIPS↓ 0.157 0.140 0.112 0.036 0.056 0.038 0.043 0.041\nCIEDE200 ↓ 33.01 28.65 21.65 10.86 25.55 10.36 11.24 14.32\nO-HAZE\nPSNR↑ 15.09 15.28 17.22 16.67 17.17 16.76 16.09 29.87\nSSIM↑ 0.449 0.466 0.521 0.480 0.600 0.460 0.465 0.758\nLPIPS↓ 0.318 0.306 0.285 0.296 0.292 0.318 0.323 0.155\nCIEDE2000 ↓ 45.72 46.81 37.11 36.39 33.79 36.67 35.15 28.80\nHazeRD\nPSNR↑ 15.92 15.03 15.64 15.32 15.76 15.67 16.09 17.39\nSSIM↑ 0.643 0.601 0.624 0.673 0.608 0.679 0.667 0.706\nLPIPS↓ 0.228 0.284 0.235 0.231 0.243 0.222 0.232 0.224\nCIEDE2000 ↓ 31.21 27.83 26.88 26.08 30.61 26.25 26.46 25.75\nReal World\nrnve↑ 1.393 1.418 0.822 0.704 1.664 1.429 0.600 1.767\nmng ↑ 1.570 1.699 1.437 1.116 1.587 1.303 1.123 2.165\nrsp ↓ 0.0003 0.0195 0.0267 0.0010 0.0001 0.0023 0.0010 0.0000\n[a] The bold results indicate the best performances, and the second-best are underlined . ↑: The larger the better. ↓: The smaller the better.\n(f) Ours\n(e) DWGAN\n(c) EDN-3J\n (g) GT\n(a) hazy image\n (b) EDN-AT\n (d) TWNN\nFig. 8. Dehazing results of the Non-Homogeneous hazy images (on the NH-HAZE dataset). (a): the hazy images. (b)-(f): the dehazing results of EDN-AT\n[29], EDN-3J[29], TWNN [27], DWGAN [28] and Ours, respectively. (g): the ground truth.\nHAZE. Our model is compared with several latest non-\nhomogeneous dehazing models, including EDN-AT [29],\nEDN-3J[29], TWNN [27] and DWGAN [28]. Fig.8 reveals\nthe visual comparisons. As we can see, all of the methods\nremove the non-homogeneous haze successfully. However, the\nEDN-3J fails to remove haze from the stone steps; the other\ncompared methods seem to fail to restore accurate color at the\nbushes and lawns (see the second row in Fig.8). While our\nmodel successfully avoids these issues due to the reﬂectance\nsubnetwork learning accurate color information, achieving\nmore accurate dehazing results on the NH-HAZE dataset than\nother methods. The quantitative results are shown in Table II\nalso reveal that our model obtains the best results, surpassing\nthe second best methods by a considerable margin, surpassing\nthe second best methods 2.13 dB on PSNR, 0.005 on SSIM,\n0.025 on LPIPS, and 0.48 on CIEDE2000, respectively. The\nreason may lie in the proposed HyLoG-ViT, which has a\nstronger ability in modelling long-range context information\nthan the vanilla CNN.\n3) Nighttime Dehazing: We further evaluate our dehzaing\nmodel on the nighttime dehazing, comparing with a low-\nlight image enhancement and several latest nighttime dehazing\nmodels, i.e. the LIME [66], LCD [67], MRP [30] and OSFD\n[31]. The visual results tested on the synthetic NHR dataset\n[31] and real-world dataset are illustrated in Fig.9 and Fig.10,\nTABLE II\nNON-HOMOGENEOUS DEHAZING RESULTS ON THE NH-HAZE DATASET.\nDataset Metric EDN-AT EDN-3J TBNN DWGAN Our\nNH-\nHAZE\nPSNR↑ 18.92 17.78 17.78 22.13 24.26\nSSIM↑ 0.778 0.735 0.706 0.800 0.805\nLPIPS↓ 0.235 0.271 0.272 0.224 0.199\nCIEDE2000↓ 23.80 24.29 26.50 23.62 23.14\nrespectively. The low-light image enhancement method LIME\nonly improves the luminance of the nighttime image while\nfailing to remove the color shift and haze. Our method and the\nother nighttime dehazing models can improve the luminance,\ncorrect color shift and remove haze simultaneously. However,\nall of LCD, MRP and OSFD yield unnatural results at the sky\nregions in Fig.9 and bright regions around the street lamps\nin Fig.10 with over-saturation artifacts. While our method\nproduces pleasingly smooth results in such regions, which\nlooks more natural. The qualitative results as shown in Table\nIII also demonstrate that our model can perform surprisingly\nwell on nighttime dehazing, outperforming the state-of-the-art\nMRP and OSFD methods with respect to PSNR, SSIM, LPIPS\nand CIEDE2000. Especially, our CIEDE2000 is signiﬁcantly\nlower than the MRP with 14.54, indicating that the color-\nwise complementary features learned from the reﬂectance\nsubnetwork play a beneﬁcial role in preserving accurate color.\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 9\n(a) hazy image(b) LIME(e) OSFD(f) Ours (d) MRP(g) GT\n (c) LCD\nFig. 9. Dehazing results of the Synthetic Nighttime hazy images (on NHR\ndataset). (a): the hazy images. (b)-(f): the dehazing results of LIME [66], LCD\n[67], MRP [30], OSFD [31] and Ours, respectively. (g): the ground truth.\nTABLE III\nNIGHTTIME DEHAZING RESULTS ON THE NHR AND REAL -WORLD\nDATASETS .\nDataset Metric LIME LCD MRP OSFD Ours\nNHR\nPSNR↑ 13.93 13.31 15.067 19.88 22.94\nSSIM↑ 0.767 0.608 0.694 0.700 0.814\nLPIPS↓ 0.470 0.439 0.387 0.371 0.207\nCIEDE2000↓ 43.36 35.02 32.79 33.35 18.25\nReal-\nWorld\nrnve ↑ 0. 144 0.190 0.169 0.149 0.219\nmng ↑ 5.049 5.589 5.406 4.906 5.846\nrsp ↓ 0.0104 0.0136 0.0190 0.0163 0.0000\nC. Discussion\nDue to the stronger ability in modelling long-range context\ninformation, the network structure with transformer blocks\nis naturally good at learning the spatially variant features\n[43]. Our experiments verify this conclusion. The samples\nin the SOTS are synthetic by setting the atmospheric light\nand scattering coefﬁcient as global constants. While in the\nO-HAZE and NH-HAZE datasets, hazy images are captured\nin natural scenes where the haze is generated by professional\nhaze machines [64]. Under foggy and hazy conditions, the\natmospheric light and scattering coefﬁcient will no longer be\nglobally constant in natural scenes. Therefore, as shown in\nTable I and II, our method can surpass the SOTAs by a con-\nsiderable margin on the O-HAZE and NH-HAZE, performing\nrelatively better than on the homogeneous haze SOTS dataset.\n(a) hazy image(c) LIME(f) OSFD (b) Ours\n(e) MRP\n (d) LCD\nFig. 10. Dehazing results of the Real-World Nighttime hazy images. (a): the\nhazy images. (b): Our dehazing results. (c)-(f): the dehazing results of LIME\n[66], LCD [67], MRP [30] and OSFD [31], respectively.\nD. Ablation Study\nFor quick experiments, all of the models are trained for 10\nepochs and evaluated on 300 samples randomly selected from\nthe SOTS [56] dataset in our ablation studies.\n1) Evaluations on the Framework: We conduct the ablation\nstudies to demonstrate the contribution of the joint model. 1)\nw/o-RS: our dehazing network without any complementary\nfeatures, removing the decoders DR and DS; 2) w/o-S: our\ndehazing model removing the decoder DS; 3) w/o-R: our\ndehazing model removing the decoder DR. Comparing models\nof w/o-RS, w/o-S, and w/o-R, the latter two models obtain\nhigher PSNR and SSIM than the former, indicating that the\njoint learning mechanism with complementary features of\nreﬂectance or shading indeed boosts the network’s dehazing\nperformance. Furthermore, when both complementary features\nare leveraged in our model, it achieves the best performance,\ndemonstrating the effectiveness of the proposed framework.\n2) Evaluations on the CFSM: For checking the contribution\nof the CFSM, we conduct the experiment where the model\nw/o-CFSM refers to the dehazing model that with-out CFSM\nyet merge the complementary features dz\nR, dz\nS and features\ndz\nD by summation. The dehazing results are shown in the\nsecond row in Tabel V. As we can ﬁnd, without the CFSM,\nthe dehazing results reduce 0.671 dB on PSNR and 0.0029\non SSIM, indicating the importance and beneﬁts of using the\nCFSM in the dehazing network.\n3) Evaluations on the HyLoG-ViT: We further evaluate the\neffectiveness of the proposed HyLoG-ViT model for image\ndehazing by involving the following different conﬁgurations,\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 10\nTABLE IV\nABLATION STUDIES ON THE FRAMEWORK AND CFSM.\nMetric w/o-RS w/o-S w/o-R w/o-CFSM Our\nPSNR ↑ 27.248 28.740 27.895 28.911 29.582\nSSIM ↑ 0.9419 0.9507 0.9599 0.9586 0.9617\nTABLE V\nABLATION STUDY ON THE HYLOG-V IT.\nMetric CNN ViT L-ViT G-ViT LoG-ViT Our\nPSNR↑ 28.020 27.727 28.375 27.365 28.913 29.582\nSSIM↑ 0.9529 0.9404 0.9582 0.9467 0.9591 0.9617\nwhere the HyLoG-ViT block is replaced by: 1) two basic\nResNet [49] blocks ( CNN); 2) the basic ViT block ( ViT); 3)\nthe local ViT path ( L-ViT); 4) the global ViT path ( G-ViT);\n5) the sequential stacked local and global ViT paths ( LoG-\nViT). The model CNN achieves better dehazing performance\nthan the models of ViT and G-ViT. One of the reasons is that\nTransformers-based models lack some of the inductive biases\ninherent to model CNN, such as translation equivariance and\nlocality, and therefore do not generalize well when trained on\ninsufﬁcient data. Models of LoG-ViT and HyLoG-ViT obtain\nthe top two PSNR and SSIM, indicating that combining local\nand global interactions are more effective than other models.\nComparing them, we can ﬁnd that our parallel hybrid scheme\nis better than the sequentially stacked one.\n4) Position Encoding: In this experiment, we will show\nthat, without the position encoding/embedding, our model can\nalso achieve high performance on image dehazing. We also\nverify that the 3 ×3 convolution layer that fuses the hybrid\nfeatures from local and global ViT paths is useful. Hence, we\ncompared the following models: 1) PE: our HyLoG adds the\nlearnable position encodings as used in [24, 68]; 2) ADD: the\nhybrid features are fused by element-wise summation. The\nquantitative results are shown in Table VI. As we can ﬁnd,\nthere are no signiﬁcant performance gains to the SSIM when\nadding the position encoding. The PE model gets even lower\nthe PSNR than Our model. However, the ADD model performs\nworse than the other two models with respect to the PSNR and\nSSIM, demonstrating that the 3×3 convolution layer for hybrid\nfeatures’ fusion is vital in the HyLoG-ViT.\nV. C ONCLUTION\nThis paper aims to mitigate the issues of CNNs-based\ndehazing networks by introducing a new complementary\nfeature enhanced framework and a hybrid local and global\nvision transformer. To these ends, we ﬁrst propose a new\ndehazing framework, which jointly learns the intrinsic image\ndecomposition and dehazing. The reﬂectance and shading\nprediction tasks encourage the networks to learn more useful\ncomplementary features for image dehazing task. We propose\na complementary features selection module to effectively\naggregate those complementary features to enhance the useful\ncomplementary features while weakening the irrelevant ones.\nThen, we introduce vision transformers into the dehazing\ntask. We design a new hybrid local-global vision transformer\n(HyLoG-ViT) which is more computationally efﬁcient than the\nTABLE VI\nABLATION STUDY ON THE POSITION ENCODING .\nMetric PE ADD Our\nPSNR↑ 29.49 28.34 29.58\nSSIM↑ 0.963 0.955 0.962\nstandard ViT, as it can capture both local and global depen-\ndencies. We conduct extensive experiments on homogeneous,\nnon-homogeneous, and nighttime dehazing tasks to evaluate\nour method. Qualitative and quantitative results reveal that\nour method achieves comparable or even better performance\nthan CNNs-based dehazing methods, demonstrating the ef-\nfectiveness of the proposed framework and the HyLoG-ViT.\nWe strongly believe that the complementary feature enhanced\nframework can be deeply explored and achieve outstanding\nperformances on other image restoration tasks, such as image\nderaining, low-light image enhancement and old photo restora-\ntion.\nREFERENCES\n[1] S. Nayar and S. Narasimhan, “Vision in bad weather,” in\nProceedings of the IEEE International Conference on Computer\nVision, vol. 2, 1999, pp. 820–827.\n[2] J. Dong and J. Pan, “Physics-based feature dehazing networks,”\nin Proceedings of the European Conference on Computer Vision.\nSpringer, 2020, pp. 188–204.\n[3] K. He, J. Sun, and X. Tang, “Single image haze removal using\ndark channel prior,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 33, no. 12, pp. 2341–2353, 2011.\n[4] Q. Zhu, J. Mai, and L. Shao, “A fast single image haze removal\nalgorithm using color attenuation prior,” IEEE Transactions on\nImage Processing, vol. 24, no. 11, pp. 3522–3533, 2015.\n[5] D. Berman, T. Treibitz, and S. Avidan, “Single image dehazing\nusing haze-lines,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 42, no. 3, pp. 720–734, 2018.\n[6] J. Li, H. Zhang, D. Yuan, and M. Sun, “Single image dehazing\nusing the change of detail prior,” Neurocomputing, vol. 156, pp.\n1–11, 2015.\n[7] M. Kaur, D. Singh, V . Kumar, and K. Sun, “Color image\ndehazing using gradient channel prior and guided l 0 ﬁlter,”\nInformation Sciences, vol. 521, no. C, p. 326–342, 2020.\n[8] B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao, “Dehazenet:\nAn end-to-end system for single image haze removal,” IEEE\nTransactions on Image Processing , vol. 25, no. 11, pp. 5187–\n5198, 2016.\n[9] W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang,\n“Single image dehazing via multi-scale convolutional neural\nnetworks,” in Proceedings of the European Conference on\nComputer Vision, 2016, pp. 154–169.\n[10] B. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, “Aod-net:\nAll-in-one dehazing network,” in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2017, pp. 4770–\n4778.\n[11] P. Li, J. Tian, Y . Tang, G. Wang, and C. Wu, “Deep retinex\nnetwork for single image dehazing,” IEEE Transactions on\nImage Processing, vol. 30, pp. 1100–1115, 2020.\n[12] R. Liu, X. Fan, M. Hou, Z. Jiang, Z. Luo, and L. Zhang,\n“Learning aggregated transmission propagation networks for\nhaze removal and beyond,” IEEE Transactions on Neural Net-\nworks and Learning Systems , vol. 30, no. 10, pp. 2973–2986,\n2019.\n[13] H. Li, J. Li, D. Zhao, and L. Xu, “Dehazeﬂow: Multi-scale\nconditional ﬂow network for single image dehazing,” in Pro-\nceedings of the ACM international conference on Multimedia ,\n2021, pp. 2577–2585.\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 11\n[14] B.-H. Chen, S.-C. Huang, C.-Y . Li, and S.-Y . Kuo, “Haze\nremoval using radial basis function networks for visibility\nrestoration applications,” IEEE Transactions on Neural Net-\nworks and Learning Systems , vol. 29, no. 8, pp. 3828–3838,\n2018.\n[15] Y . Qu, Y . Chen, J. Huang, and Y . Xie, “Enhanced pix2pix\ndehazing network,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2019, pp. 8160–\n8168.\n[16] X. Liu, Y . Ma, Z. Shi, and J. Chen, “Griddehazenet: Attention-\nbased multi-scale network for image dehazing,” in Proceedings\nof the IEEE International Conference on Computer Vision ,\n2019, pp. 7314–7323.\n[17] H. Dong, J. Pan, L. Xiang, Z. Hu, X. Zhang, F. Wang, and M.-H.\nYang, “Multi-scale boosted dehazing network with dense feature\nfusion,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2020, pp. 2157–2167.\n[18] X. Qin, Z. Wang, Y . Bai, X. Xie, and H. Jia, “Ffa-net: Feature\nfusion attention network for single image dehazing.” in Pro-\nceedings of the Association for the Advancement of Artiﬁcial\nIntelligence, 2020, pp. 11 908–11 915.\n[19] M. Naseer, K. Ranasinghe, S. Khan, M. Hayat, F. S. Khan,\nand M.-H. Yang, “Intriguing properties of vision transformers,”\narXiv preprint arXiv:2105.10497 , 2021.\n[20] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu,\nJ. Feng, T. Xiang, P. H. Torr et al. , “Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers,” in Proceedings of the IEEE International Con-\nference on Computer Vision , 2021, pp. 6881–6890.\n[21] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,\n“Swinir: Image restoration using swin transformer,” in Proceed-\nings of the IEEE International Conference on Computer Vision\nWorkshop, 2021.\n[22] D. Zhou, “Human texture perception,” https://www.cs.auckland.\nac.nz/∼georgy/research/texture/thesis-html/node6.html, 2006,\naccessed: 2006-2-22.\n[23] B. Julesz, “Visual pattern discrimination,” IRE transactions on\nInformation Theory, vol. 8, no. 2, pp. 84–92, 1962.\n[24] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly et al., “An image is worth 16x16 words: Transformers\nfor image recognition at scale,” Proceedings of the International\nConference on Learning Representations , 2020.\n[25] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hecht-\nman, and J. Shlens, “Scaling local self-attention for parameter\nefﬁcient visual backbones,” in Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition, 2021, pp.\n12 894–12 904.\n[26] X. Chu, Z. Tian, Y . Wang, B. Zhang, H. Ren, X. Wei, H. Xia,\nand C. Shen, “Twins: Revisiting the design of spatial attention\nin vision transformers,” arXiv preprint arXiv:2104.13840, 2021.\n[27] Y . Yu, H. Liu, M. Fu, J. Chen, X. Wang, and K. Wang,\n“A two-branch neural network for non-homogeneous dehazing\nvia ensemble learning,” in Proceedings of the Conference on\nComputer Vision and Pattern Recognition Workshop , 2021, pp.\n193–202.\n[28] M. Fu, H. Liu, Y . Yu, J. Chen, and K. Wang, “Dw-gan: A\ndiscrete wavelet transform gan for nonhomogeneous dehazing,”\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops , 2021, pp.\n203–212.\n[29] M. Yu, V . Cherukuri, T. Guo, and V . Monga, “Ensemble\ndehazing networks for non-homogeneous haze,” in Proceedings\nof the Conference on Computer Vision and Pattern Recognition\nWorkshop, 2020.\n[30] J. Zhang, Y . Cao, S. Fang, Y . Kang, and C. Wen Chen, “Fast\nhaze removal for nighttime image using maximum reﬂectance\nprior,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2017, pp. 7418–7426.\n[31] J. Zhang, Y . Cao, Z.-J. Zha, and D. Tao, “Nighttime dehazing\nwith a synthetic benchmark,” in Proceedings of the ACM\ninternational conference on Multimedia , 2020, pp. 2355–2363.\n[32] S. G. Narasimhan and S. K. Nayar, “Vision and the atmosphere,”\nInternational Journal of Computer Vision , vol. 48, no. 3, pp.\n233–254, 2002.\n[33] H. Wu, Y . Qu, S. Lin, J. Zhou, R. Qiao, Z. Zhang, Y . Xie,\nand L. Ma, “Contrastive learning for compact single image\ndehazing,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2021, pp. 10 551–10 560.\n[34] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolu-\ntional networks for biomedical image segmentation,” in Interna-\ntional Conference on Medical Image Computing and Computer-\nAssisted Intervention, 2015, pp. 234–241.\n[35] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,”\nin Proceedings of the European Conference on Computer Vision,\n2020, pp. 213–229.\n[36] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu,\nP. Luo, and L. Shao, “Pyramid vision transformer: A versatile\nbackbone for dense prediction without convolutions,” in Pro-\nceedings of the IEEE International Conference on Computer\nVision, 2021, pp. 568–578.\n[37] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer us-\ning shifted windows,” in Proceedings of the IEEE International\nConference on Computer Vision , 2021, pp. 10 012–10 022.\n[38] Y . Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh,\n“Dynamicvit: Efﬁcient vision transformers with dynamic token\nsparsiﬁcation,” arXiv preprint arXiv:2106.02034 , 2021.\n[39] Z. Chen, L. Xie, J. Niu, X. Liu, L. Wei, and Q. Tian, “Visformer:\nThe vision-friendly transformer,” in Proceedings of the IEEE\nInternational Conference on Computer Vision , 2021, pp. 589–\n598.\n[40] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and\nL. Zhang, “Cvt: Introducing convolutions to vision transform-\ners,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) , 2021, pp. 22–31.\n[41] Y . Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool, “Lo-\ncalvit: Bringing locality to vision transformers,” arXiv preprint\narXiv:2104.05707, 2021.\n[42] J. Yang, C. Li, P. Zhang, X. Dai, B. Xiao, L. Yuan, and\nJ. Gao, “Focal self-attention for local-global interactions in\nvision transformers,” arXiv preprint arXiv:2107.00641 , 2021.\n[43] Z. Wang, X. Cun, J. Bao, and J. Liu, “Uformer: A general\nu-shaped transformer for image restoration,” arXiv preprint\narXiv:2106.03106, 2021.\n[44] Y . Liu, Y . Li, S. You, and F. Lu, “Unsupervised learning\nfor intrinsic image decomposition from a single image,” in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2020.\n[45] A. S. Baslamisli, T. T. Groenestege, P. Das, H.-A. Le,\nS. Karaoglu, and T. Gevers, “Joint learning of intrinsic images\nand semantic segmentation,” in Proceedings of the European\nConference on Computer Vision , 2018, pp. 286–302.\n[46] G. Sharma, W. Wu, and E. N. Dalal, “The ciede2000 color-\ndifference formula: Implementation notes, supplementary test\ndata, and mathematical observations,” Color Research & Appli-\ncation: Endorsed by Inter-Society Color Council, The Colour\nGroup (Great Britain), Canadian Society for Color, Color\nScience Association of Japan, Dutch Society for the Study of\nColor, The Swedish Colour Centre Foundation, Colour Society\nof Australia, Centre Franc ¸ais de la Couleur, vol. 30, no. 1, pp.\n21–30, 2005.\n[47] N. Hautiere, J.-P. Tarel, D. Aubert, and E. Dumont, “Blind\ncontrast enhancement assessment by gradient ratioing at visible\nedges,” Image Analysis & Stereology, vol. 27, no. 2, pp. 87–95,\n2008.\n[48] S. Vandenhende, S. Georgoulis, W. Van Gansbeke, M. Proes-\nJOURNAL OF XXXXX, VOL. X, NO. X, XX 20XX 12\nmans, D. Dai, and L. Van Gool, “Multi-task learning for dense\nprediction tasks: A survey,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , 2021.\n[49] K. He, X. Zhang, S. Ren, and S. Jian, “Deep residual learning\nfor image recognition,” in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition , 2016, pp. 770–\n778.\n[50] S. Woo, J. Park, J.-Y . Lee, and I. So Kweon, “Cbam: Convolu-\ntional block attention module,” in Proceedings of the European\nConference on Computer Vision , 2018, pp. 3–19.\n[51] X. Chu, Z. Tian, B. Zhang, X. Wang, X. Wei, H. Xia, and\nC. Shen, “Conditional positional encodings for vision transform-\ners,” arXiv preprint arXiv:2102.10882 , 2021.\n[52] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Al-\nvarez, and P. Luo, “Segformer: Simple and efﬁcient design\nfor semantic segmentation with transformers,” arXiv preprint\narXiv:2105.15203, 2021.\n[53] M. Mirza and S. Osindero, “Conditional generative adversarial\nnets,” Proceedings of the Neural Information Processing Sys-\ntems, 2014.\n[54] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-\nimage translation with conditional adversarial networks,” in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2017, pp. 1125–1134.\n[55] J. Cai, S. Gu, and L. Zhang, “Learning a deep single image con-\ntrast enhancer from multi-exposure images,” IEEE Transactions\non Image Processing , vol. 27, no. 4, pp. 2049–2062, 2018.\n[56] B. Li, W. Ren, D. Fu, D. Tao, D. Feng, W. Zeng, and\nZ. Wang, “Benchmarking single-image dehazing and beyond,”\nIEEE Transactions on Image Processing , vol. 28, no. 1, pp.\n492–505, 2018.\n[57] C. O. Ancuti, C. Ancuti, and R. Timofte, “Nh-haze: An image\ndehazing benchmark with non-homogeneous hazy and haze-free\nimages,” in Proceedings of the Conference on Computer Vision\nand Pattern Recognition Workshop, 2020, pp. 444–445.\n[58] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” in Proceedings of the International Conference\non Learning Representations , 2015.\n[59] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,”\narXiv preprint arXiv:1607.06450 , 2016.\n[60] D. P. Kingma and P. Dhariwal, “Glow: Generative ﬂow with\ninvertible 1x1 convolutions,” arXiv preprint arXiv:1807.03039 ,\n2018.\n[61] Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli et al. ,\n“Image quality assessment: from error visibility to structural\nsimilarity,” IEEE Transactions on Image Processing , vol. 13,\nno. 4, pp. 600–612, 2004.\n[62] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,\n“The unreasonable effectiveness of deep features as a perceptual\nmetric,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition , 2018, pp. 586–595.\n[63] A. Gonzalez-Garcia, J. Van De Weijer, and Y . Bengio, “Image-\nto-image translation for cross-domain disentanglement,” in Pro-\nceedings of the Neural Information Processing Systems , 2018.\n[64] C. O. Ancuti, C. Ancuti, R. Timofte, and C. De Vleeschouwer,\n“O-haze: a dehazing benchmark with real hazy and haze-free\noutdoor images,” inProceedings of the Conference on Computer\nVision and Pattern Recognition Workshop .\n[65] Y . Zhang, L. Ding, and G. Sharma, “Hazerd: an outdoor\nscene dataset and benchmark for single image dehazing,” in\nProceedings of the IEEE International Conference on Image\nProcessing, 2017, pp. 3205–3209.\n[66] X. Guo, Y . Li, and H. Ling, “Lime: Low-light image enhance-\nment via illumination map estimation,” IEEE Transactions on\nImage Processing, vol. 26, no. 2, pp. 982–993, 2016.\n[67] J. Zhang, Y . Cao, and Z. Wang, “Nighttime haze removal\nbased on a new imaging model,” in Proceedings of the IEEE\nInternational Conference on Image Processing, 2014, pp. 4557–\n4561.\n[68] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma,\nC. Xu, C. Xu, and W. Gao, “Pre-trained image processing\ntransformer,” in Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2021, pp. 12 299–12 310.\nACKNOWLEDGMENT\n. This work is supported by grants from National Natural\nScience Foundation of China (No.61922006, No.62132002)\nand CAAI-Huawei MindSpore Open Fund.\nDong Zhao received the M.S. degree in Key Lab-\noratory of Complex System Intelligent Control and\nDecision, Department of Automation, Beijing Insti-\ntute of Technology, Beijing, China, in 2016, and\nthe Ph.D. degree from National Astronomical Ob-\nservatories, Chinese Academy of Sciences , Beijing,\nChina, in 2020. His research interests include com-\nputer vision and image processing.\nJia Li (M’12-SM’15) received the B.E. degree from\nTsinghua University in 2005 and the Ph.D. degree\nfrom the Institute of Computing Technology, Chi-\nnese Academy of Sciences, in 2011. He is currently\na Full Professor with the School of Computer Sci-\nence and Engineering, Beihang University, Beijing,\nChina. Before he joined Beihang University in Jun.\n2014, he used to conduct research in Nanyang Tech-\nnological University, Peking University and Shanda\nInnovations. He is the author or coauthor of over\n70 technical articles in refereed journals and con-\nferences such as TPAMI, IJCV , TIP, CVPR and ICCV . His research interests\ninclude computer vision and multimedia big data, especially the understanding\nand generation of visual contents. He is supported by the Research Funds\nfor Excellent Young Researchers from National Nature Science Foundation\nof China since 2019. He was also selected into the Beijing Nova Program\n(2017) and ever received the Second-grade Science Award of Chinese Institute\nof Electronics (2018), two Excellent Doctoral Thesis Award from Chinese\nAcademy of Sciences (2012) and the Beijing Municipal Education Commis-\nsion (2012), and the First-Grade Science-Technology Progress Award from\nMinistry of Education, China (2010). He is a senior member of IEEE, CIE\nand CCF. More information can be found at http://cvteam.net.\nHongyu Li is currently pursuing the master degree\nwith the State Key Laboratory of Virtual Real-\nity Technology and System, School of Computer\nScience and Engineering, Beihang University. He\nreceived the B.E. degree from Beihang University\nin Jul. 2020. His research interests include computer\nvision and image processing.\nLong Xu (M’12) received his M.S. degree in applied\nmathematics from Xidian University, Xi’an, China,\nin 2002, and the Ph.D. degree from the Institute\nof Computing Technology, Chinese Academy of\nSciences, Beijing, China. He was a Postdoc with the\nDepartment of Computer Science, City University\nof Hong Kong, the Department of Electronic En-\ngineering, Chinese University of Hong Kong, from\nJuly Aug. 2009 to Dec. 2012. From Jan. 2013 to\nMarch 2014, he was a Postdoc with the School\nof Computer Engineering, Nanyang Technological\nUniversity, Singapore. Currently, he is with the Key Laboratory of Solar\nActivity, National Astronomical Observatories, Chinese Academy of Sciences.\nHis research interests include image/video processing, solar radio astronomy,\nwavelet, machine learning, and computer vision. He was selected into the\n100-Talents Plan, Chinese Academy of Sciences, 2014."
}