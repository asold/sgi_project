{
  "title": "Compositional Transformers for Scene Generation",
  "url": "https://openalex.org/W3211927563",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225821087",
      "name": "Hudson, Drew A.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3183588634",
      "name": "Zitnick, C. Lawrence",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3101254953",
    "https://openalex.org/W2911448865",
    "https://openalex.org/W2948968751",
    "https://openalex.org/W3112587064",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W2990690382",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2950560720",
    "https://openalex.org/W3105675572",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W2962760235",
    "https://openalex.org/W2298992465",
    "https://openalex.org/W2785961484",
    "https://openalex.org/W2801780873",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3034431451",
    "https://openalex.org/W2140881840",
    "https://openalex.org/W2994971263",
    "https://openalex.org/W2964024144",
    "https://openalex.org/W2553882142",
    "https://openalex.org/W2963767194",
    "https://openalex.org/W2987919422",
    "https://openalex.org/W2618599721",
    "https://openalex.org/W3112593261",
    "https://openalex.org/W2990397898",
    "https://openalex.org/W2962741254",
    "https://openalex.org/W2946952476",
    "https://openalex.org/W3167002466",
    "https://openalex.org/W3173241699",
    "https://openalex.org/W2964082390",
    "https://openalex.org/W2963951231",
    "https://openalex.org/W3034521057",
    "https://openalex.org/W2999219213",
    "https://openalex.org/W2963522749",
    "https://openalex.org/W2963901923",
    "https://openalex.org/W2963242738",
    "https://openalex.org/W3102696055",
    "https://openalex.org/W2962974533",
    "https://openalex.org/W2845413374",
    "https://openalex.org/W3176179930",
    "https://openalex.org/W2156406284",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W2995801453",
    "https://openalex.org/W2949864127",
    "https://openalex.org/W3000266068",
    "https://openalex.org/W2949933669",
    "https://openalex.org/W2963330667",
    "https://openalex.org/W3098124890",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3148140980",
    "https://openalex.org/W2963921132",
    "https://openalex.org/W2963800363",
    "https://openalex.org/W3042183427",
    "https://openalex.org/W2970955049",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W3122887115",
    "https://openalex.org/W3034526383",
    "https://openalex.org/W2964216930",
    "https://openalex.org/W3129912460",
    "https://openalex.org/W3028869078",
    "https://openalex.org/W2963184176",
    "https://openalex.org/W2891997758",
    "https://openalex.org/W2005134942",
    "https://openalex.org/W2963717490",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2962785568",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W3037507281",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2962879001",
    "https://openalex.org/W3014688857"
  ],
  "abstract": "We introduce the GANformer2 model, an iterative object-oriented transformer, explored for the task of generative modeling. The network incorporates strong and explicit structural priors, to reflect the compositional nature of visual scenes, and synthesizes images through a sequential process. It operates in two stages: a fast and lightweight planning phase, where we draft a high-level scene layout, followed by an attention-based execution phase, where the layout is being refined, evolving into a rich and detailed picture. Our model moves away from conventional black-box GAN architectures that feature a flat and monolithic latent space towards a transparent design that encourages efficiency, controllability and interpretability. We demonstrate GANformer2's strengths and qualities through a careful evaluation over a range of datasets, from multi-object CLEVR scenes to the challenging COCO images, showing it successfully achieves state-of-the-art performance in terms of visual quality, diversity and consistency. Further experiments demonstrate the model's disentanglement and provide a deeper insight into its generative process, as it proceeds step-by-step from a rough initial sketch, to a detailed layout that accounts for objects' depths and dependencies, and up to the final high-resolution depiction of vibrant and intricate real-world scenes. See https://github.com/dorarad/gansformer for model implementation.",
  "full_text": "Compositional Transformers for Scene Generation\nDrew A. Hudson\nDepartment of Computer Science\nStanford University\ndorarad@cs.stanford.edu\nC. Lawrence Zitnick\nFacebook AI Research\nFacebook, Inc.\nzitnick@fb.com\nAbstract\nWe introduce the GANformer2 model, an iterative object-oriented transformer,\nexplored for the task of generative modeling. The network incorporates strong\nand explicit structural priors, to reﬂect the compositional nature of visual scenes,\nand synthesizes images through a sequential process. It operates in two stages:\na fast and lightweight planning phase, where we draft a high-level scene layout,\nfollowed by an attention-based execution phase, where the layout is being reﬁned,\nevolving into a rich and detailed picture. Our model moves away from conventional\nblack-box GAN architectures that feature a ﬂat and monolithic latent space towards\na transparent design that encourages efﬁciency, controllability and interpretability.\nWe demonstrate GANformer2’s strengths and qualities through a careful evaluation\nover a range of datasets, from multi-object CLEVR scenes to the challenging\nCOCO images, showing it successfully achieves state-of-the-art performance in\nterms of visual quality, diversity and consistency. Further experiments demonstrate\nthe model’s disentanglement and provide a deeper insight into its generative process,\nas it proceeds step-by-step from a rough initial sketch, to a detailed layout that\naccounts for objects’ depths and dependencies, and up to the ﬁnal high-resolution\ndepiction of vibrant and intricate real-world scenes. See https://github.com/\ndorarad/gansformer for model implementation.\n1 Introduction\nDrawing, the practice behind human visual and artistic expression, can essentially be deﬁned as\nan iterative process. It starts from an initial outline, with just a few strokes that specify the spatial\narrangement and overall layout, and is then gradually reﬁned and embellished with color, depth and\nrichness, until a vivid picture eventually emerges. These initial schematic sketches can serve as an\nabstract scene representation that is very concise, yet highly expressive: several lines are enough to\ndelineate three dimensional structures, account for perspective and proportion, convey shapes and\ngeometry, and even imply semantic information [52, 63, 67]. A large body of research in psychology\nhighlights the importance of sketching in stimulating creative discovery [40, 58], fostering cognitive\ndevelopment [27], and facilitating problem solving [26, 44]. In fact, a large variety of generative and\nRL tasks either in the visual [ 5], textual [22, 78] or symbolic modalities [2], can beneﬁt from the\nsame strategy – prepare a high-level plan ﬁrst, and then carry out the details.\nAt the heart of this hierarchical strategy stands the principle of compositionality – where the meaning\nof the whole can be derived from those of its constituents. Indeed, the world around us is highly\nstructured. Our environment consists of a varied collection of objects, tightly interconnected through\ndependencies of all sorts: from close-by to long-range, and from physical and dynamic to abstract and\nsemantic [10, 66]. As pointed out by prior literature [24, 51, 55, 57], compositional representations\nare pivotal to human intelligence, supporting our capabilities of reasoning [ 32], planning [ 53],\nlearning [54] and imagination [6]. Realizing this principle, and explicitly capturing the objects and\nelements composing the scene, is thereby a desirable goal, that can make the generative process more\nexplainable, controllable, versatile and efﬁcient.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2111.08960v1  [cs.CV]  17 Nov 2021\nFigure 1: The GANformer2 model has a set of latent variables that represent the objects and entities within the\ngenerated scene. It proceeds through two stages, planning and execution: ﬁrst iteratively drafting a high-level\nlayout of the scene’s spatial structure, and then reﬁning and translating it into a photo-realistic image. Each latent\ncorresponds to a different layout’s segment, controlling its structure during the planning, and its style during the\nexecution. On the discriminator side, we introduce new structural losses for compositional scene generation.\nYet, the vast majority of generative models do not directly reﬂect this compositionality so intrinsic to\nvisual scenes. Rather, most networks, and GANs in particular, aim to generate the entire scene all at\nonce from a single monolithic latent space, through a largely opaque transformation. Consequently,\nwhile they excel in producing stunning, strikingly-realistic portrayals of faces, sole centralized objects,\nor natural scenery, they still struggle to faithfully mimic richer or densely-packed scenes, as those\ncommon to everyday contexts, falling short of mastering their nuances and intricacies [ 5, 50, 76].\nLikewise, while there is some evidence for property disentanglement within their latent space to\nindependent axes of variation at the global scale [28, 36, 64, 71], most existing GANs fail to provide\ncontrollability at the level of the individual object or local region. Understanding of the mechanisms\nby which these models give rise to an output image, and the transparency of their synthesis process,\nthereby remain rather limited.\nMotivated to alleviate these shortcomings and make generative modeling more compositional, inter-\npretable and controllable, we propose GANformer2, a structured object-oriented transformer that\ndecouples the visual synthesis task into two stages: planning and execution. We begin by sampling\na set of latent variables, corresponding to the objects and entities that compose the scene. Then, at\nthe planning stage, we transform the latents into a schematic layout – a scene representation that\ndepicts the object segments, reﬂecting their shapes, positions, semantic classes and relative ordering.\nIts construction occurs in an iterative manner, to capture the dependencies and interactions between\ndifferent objects. Next, at the execution stage, the layout is translated into the ﬁnal image, with the\nlatents cooperatively guiding the content and style of their respective segments through bipartite\nattention [33]. This approach marks a shift towards a more natural process of image generation,\nin which objects and the relations among them are explicitly modeled, while the initial sketch is\nsuccessively tweaked and reﬁned to ultimately produce a rich photo-realistic scene.\nWe study the model’s behavior and performance through an extensive set of experiments, where it\nattains state-of-the-art results in both conditional and unconditional synthesis, as measured along\nmetrics of ﬁdelity, diversity and semantic consistency. GANformer2 reaches high versatility, demon-\nstrated through multiple datasets of challenging simulated and real-world scenes. Further analysis\nillustrates its capacity to manipulate chosen objects and properties in an independent manner, achiev-\ning both spatial disentanglement and separation between structure and style. We notably observe\namodal completion of occluded objects, likely driven by the sequential nature of the computation.\nMeanwhile, inspection of the produced layouts and their components shed more light upon the syn-\nthesis of each resulting scene, substantiating the model’s transparency and explainability. Overall, by\nintegrating strong compositional structure into the model, we can move in the right direction towards\nmultiple favorable properties: increasing robustness for rich and diverse visual domains, enhancing\ncontrollability over individual objects, and improving the generative process’s interpretability.\n2 Related Work\nOver the last years, the ﬁeld of visual sythesis has witnessed astonishing progress, driven in particular\nby the emergence of Generative Adversarial Networks [23]. Tremendous strides have been made in a\nwide variety of tasks, from image [11] to video generation [7], to super-resolution [45], style transfer\n[15] and translation [35]. Meanwhile, a growing body of literature explored object discovery and\n2\nFigure 2: Layout and image generation. The GANformer2 model generates each image by ﬁrst creating a\nlayout that speciﬁes its outline and object composition, which is then translated into the ﬁnal image. While doing\nso, the model produces depth maps of the object segments which are used to determine their relative ordering,\nand notably are implicitly derived by the model from the layouts in a fully unsupervised manner.\nsequential scene decomposition, albeit in the context of variational inference [1, 16, 19, 49]. DRAW\n[25] uses recurrent attention to encode and decode MNIST digits, while AIR [20], MONet [12] and\nIODINE [24] reconstruct simple simulated scenes by iteratively factorizing them into semantically\nmeaningful segments. We draw inspiration from this pioneering line of works and marry the merits\nof its strong structure with the scalability and ﬁdelity obtained by GANs, yielding a compositional\nmodel that successfully creates intricate real-world scenes, while offering a more transparent and\ncontrollable synthesis along the way, conceptually meant to better resemble the causal generative\nprocess and serve as a more natural emulation of how a human might approach the creative task.\nTo this end, our model leverages the GANformer architecture we introduced earlier this year [33]\n– a bipartite transformer that uses soft-attention to cooperetaviely translate a set of latents into a\ngenerated image [33, 70]. In GANformer2, we take this idea a step further, and show how we can use\nthe bipartite transformer as a backbone for a more structured process, that enables not only an implicit,\nbut also an explicit modeling of the objects and constituents composing the image’s scene. Our\niterative design endows the GANformer2 model with the ﬂexibility to consider the correlations and\ninteractions not only among latents but between synthesized scene elements directly, circumventing\nthe need for partial independence assumptions made by earlier works [ 18, 56, 69], and thereby\nmanaging to scale to natural real-world scenes. Concurrently, the explicit object-oriented structure\nenhances the model’s compositionallity so to outperform past coarse-to-ﬁne cascade [80], layered\n[34], and hierarchical approaches [5], which, like us, employ a two-stage layout-based procedure, but\ncontrary to our work, do so in a non-compositional fashion, and indeed were mostly explored within\nlimited domains, such as indoor NYUv2 [73], faces [41], pizzas [59], or bird photographs [77].\nOur model links to research on conditional generative modeling [ 72, 85, 87], including semantic\nimage synthesis and text-to-image generation. The seminal Pix2Pix [ 35] and SPADE [60] works\nrespectively adopt either a U-Net [ 62] or spatially-adaptive normalization [ 61] to map a given\ninput layout into an output high-resolution image. However, beyond their notable weakness of\nbeing conditional and thus unable to create scenes from scratch, their reliance on both static class\nembeddings for feature modulation, along with perceptual or feature-matching reconstruction losses,\nrender their sample variation rather low [68, 86]. In our work, we mitigate this issue by proposing\nnew purely generative structural loss functions: Semantic Matching and Segment Fidelity, which\nmaintain the one-to-many relation that holds between layouts and images, and consequently, as\ndemonstrated in section 3.3, signiﬁcantly enhance the output diversity.\nWhile GANformer2 is an unconditional model, it shares some high-level ideas with conditional\ntext-to-image [30, 31] and scene-to-image [ 4, 38] techniques, which use semantic layouts as a\n3\nFigure 3: Recurrent scene generation. GANformer2 creates the layout sequentially, segment-by-segment, to\ncapture the scene’s compositionality, effectively allowing us to add or remove objects from the resulting images.\nbottleneck intermediate representation. But whereas these works focus on heavily-engineered\npipelines consisting of multiple pre- and post-processing stages and different sources of supervision,\nincluding textual descriptions, scene graphs, bounding boxes, segmentation masks and images,\nwe propose a simpler, more elegant design, with a streamlined architecture and fewer supervision\nsignals – relying on images and auto-predicted layouts only, to widen its applicability across domains.\nFinally, our approach relates to prior works on visual editing [8, 9, 46, 84] and image compositing\n[3, 13, 47, 79], but while these methods modify an existing source image, GANformer2 stands out\nbeing capable of creating structured manipulatable scenes from scratch.\n3 The GANformer2 model\nThe GANformer2 model is a compositional object-oriented transformer for visual scene generation.\nIt features a set of latent variables corresponding to the objects and entities composing the scene, and\nproceeds through two synthesis stages: (1) a sequential planning stage, where a scene layout – a\ncollection of interacting object segments, is being created (section 3.2), followed by (2) a parallel\nexecution stage, where the layout is being reﬁned and translated into an output photo-realistic image\n(section 3.3). Both stages are implemented using customized versions of the GANformer model\n(section 3.1) – a bipartite transformer that uses multiple latent variables to modulate the evolving\nvisual features of a generated image, so to create it in a compositional fashion.\nIn this paper, we explore learning from two training sets: of images and layouts. Speciﬁcally, we\nuse panoptic segmentations [ 43], which specify for every segment si its instance identity pi and\nsemantic category mi, but other types of segmentations can likewise be used. The images and layouts\ncan either be paired or unpaired, and our training scheme accommodates both options: either by\npre-training each stage independently ﬁrst and then ﬁne-tune them jointly in the paired case, or\ntraining them together from scratch in the unpaired one (See section F for further details).\n3.1 Generative Adversarial Transformers\nThe GANformer [ 33] is a transformer-based Generative Adversarial Network: it consists of a\ngenerator G(Z) =X that translates a partitioned latent Zk×d = [z1,...,z k] into an image XHWc,\nand a discriminator D(X) that seeks to discern between real and fake images. The generator begins\nby mapping the normally-distributed latents into an intermediate space W = [w1,...,w k] using a\nfeed-forward network. Then, it sets an initial 4×4 grid, which gradually evolves through convolution\nand upsampling layers to the ﬁnal image. Contrary to traditional GANs, the GANformer also\nincorporates bipartite-transformer layers into the generator, which compute attention between the k\nlatents and the image features, to support spatial and contentual region-wise modulation:\nAttention(X,W ) =softmax\n(q(X)k(W)T\n√\nd\n)\nv(W) (1)\nu(X,W ) =γa(X,W ) ⊙LayerNorm(X) +βa(X,W ) (2)\nWhere the equations respectively express the attention and modulation between the latents and the\nimage; q(·),k(·),v(·) are the query, key, and value mappings; andγ(·),β(·) compute multiplicative\nand additive styles (gain and bias) as a function of Attention(X,W ). The update rule lets the latents\nshape and impact the image regions as induced by the key-value attention image-to-latents assignment\ncomputed between them. This encourages visual compositionality, where different latents specialize\nto represent and control various semantic regions or visual elements within the generated scene.\n4\nFigure 4: GANformer2 style variation. Images are produced by varying the style latents of the objects while\nkeeping their structure latents ﬁxed, achieving high visual diversity while maintaining structural consistency.\nAs we will see below, for each of the two generation stages, we couple a modiﬁed version of the\nGANformer with novel loss functions and training schemes. One key point in which we notably\ndepart from the original GANformer regards to the number of latent variables used by the model.\nWhile the GANformer employs a ﬁxed number ofklatent variables, GANformer2 supports avariable\nnumber of latents, thereby becoming more ﬂexible in generating scenes of diverse complexity.\n3.2 Planning Stage – Layout Synthesis\nIn the ﬁrst stage, we create from scratch a schematic layout depicting the structure of the scene to\nbe formed (see ﬁgure 2). Formally, we deﬁne a layout Sas a set of ksegments {s1,...,s k}. Each\ngenerated segment si is associated with the following1:\n• Shape & position: speciﬁed by a spatial distribution Pi(x,y) over the image grid H×W.\n• Semantic category: a soft distribution Mi(c) over the semantic classes.\n• Unsupervised depth-ordering: per-pixel values di(x,y) indicating the order relative to other\nsegments, internally used by the model to compose the segments into a layout (details below).\nRecurrent Layout Generation. To explicitly capture conditional dependencies among segments, we\nconstruct the layout in an iterative manner, recurrently applying a parameter-lightweight GANformer\nG1 over T steps (around 2–4). At each step t, we generate a variable number of segments, kt ∼\nN(µ,σ2) sampled from a trainable normal distribution2, and gradually aggregate them into a scene\nlayout St. To generate the segments at step t, we use a GANformer with kt sampled latents, each one\nzi is mapped through a feed-forward network F1(zi) =ui into an intermediate structure latent ui,\nwhich then corresponds to segment si, guiding the synthesis of its shape, depth and position.\nG1(Ukt×d,St−1) =St (3)\nTo make the GANformer recurrent, and condition the generation of new segments on ones produced\nin prior steps, we change the query function q(·) to consider not only the newly formed layout St\n(as in the standard GANformer) but also the previously aggregated one St−1 (concatenating them\ntogether and passing as an input), allowing the model to explicitly capture dependencies across steps.\nObject Manipulation. Notably, this construction supports a posteriori object-wise manipulation.\nGiven a generated layout S, we can recreate a segment si by modifying its corresponding latent from\nzi to ˆzi while conditioning on the other segments S−i, mathematically by G1(ˆz1×d\ni ,S−i). We thus\nreach a favorable balance between expressive and contextual modeling of relational interactions on\nthe one hand, and stronger object disentanglement and controllability on the other.\nLayout Composition. To compound the segments si together into a layout St, we overlay them\naccording to their predicted depths di(x,y) and shapes Pi(x,y) by computing\nSoftmax(di(x,y) + logPi(x,y))k\ni=1 (4)\n1To produce the segments (instead of RGB values), we modify the output layer to predict for every segment\nits shape Pi, category Mi and depth di. Since the distributions are soft, the model stays fully differentiable.\n2Segments are synthesized in groups for computational efﬁciency when generating crowded scenes.\n5\nFigure 6: Object controllability. GANformer2 supports entity-level manipulation, and separates style from\nstructure, enabling control over the chosen objects and properties. At each row, we begin by generating a scene\nfrom scratch (leftmost image) and then gradually interpolate the latent vector of a single object: (1) modify its\nstyle latent wi, leading to a color change, (2) modify its structure latent ui, resulting in a position change, and\n(3) modify both the structure and style latents, yielding respective changes in shape and material.\nFigure 5: Hierarchical\nvs. random noise.\nwhich yields for each image position (x,y) a distribution over the segments.\nIntuitively, this allows resolving occlusions among overlapping segments.\nE.g. if a cube segment i should appear before a sphere segment j, then\ndi(x,y) >dj(x,y) within the overlap region {(x,y)}. Notably, we do not\nprovide any supervision to the model about the true depths of the scene\nelements, and it rather creates its own internal depth ordering in an unsu-\npervised manner, only indirectly moderated by the need to produce realistic\nlooking layouts when stitching the segments together.\nNoise for Discrete Synthesis. The segmentations we train the planning\nstage over are inherently discrete, while the generated layouts are soft and\ncontinuous. To maintain stable GAN training under these conditions, and\ninspired by the instance noise technique [65], we add a simple hierarchical\nnoise map to the layouts fed into the discriminator, which helps ensuring its\ntask remains challenging. We deﬁne the noise as ∑8\nr=6 Upsample(n2r×2r\nr ),\nsumming up noise grids nr ∼N(0, σ2) across multiple resolutions, making\nit partly correlated between adjacent positions and more structured overall.\nThis empirically enhances the effectiveness and substantially improves the generated layouts’s quality.\n3.3 Execution Stage: Layout-to-Image Translation\nIn the second stage, we proceed from planning to execution, and translate the generated layout Sinto\na photo-realistic image X. For each segment si, we ﬁrst concatenate its mean depth di and category\ndistribution mi to its respective latent zi, and map them through a feed-forward network F2 into an\nintermediate style latent wi, which will determine the content, texture and style of its segment. Then,\nwe pass the resulting vectors into a second GANformer G2(W,S) =X to render the ﬁnal image.\nLayout Translation. As opposed to the original GANformer [33] that computes Attention(X,W )\non-the-ﬂy to determine the assignment between latents and spatial regions, here we rely instead on\nthe pre-planned layout Sas a conditional guidance source to specify this association. That way, each\nlatent wi modulates the features within the soft region of its respective segment si, achieving a more\nexplicit semantically-meaningful assignment between latents and scene elements. At the same time,\nsince the whole image is still generated together, with all regions processed through the convolution,\nupsampling and modulation layers of G2, the model can still ensure global visual coherence.\nLayout Reﬁnement. To increase the model’s ﬂexibility while rendering the output image, we\noptionally multiply the latent-to-segment distribution induced by the layout S with a trainable\n6\nFigure 7: Conditional generation. Sample images generated by GANformer2 conditioned on input layouts.\nsigmoidal gate σ(g(S,X,W )), which considers the layout S, the evolving image features X, and\nthe style latents W. This allows the model to locally adjust the modulation strength as it sees ﬁt, to\nreﬁne the pre-computed layout during the ﬁnal image synthesis.\nSemantic-Matching Loss. To promote strcutrual and semantic consistency between the layoutSand\nimage Xproduced by each of the synthesis stages, we complement the standard ﬁdelity loss L(D(·))\nwith two new objectives: Semantic Matching and Segment Fidelity. We note that the relation between\na given layout and the set of possible images that could correspond to it is a one-to-many relation.\nTo encourage it, we incorporate into the discriminator a U-Net [62] to predict the segmentation S′of\neach image, compare it using cross-entropy to the source layout, and add the resulting loss term to\nthe discriminator (over real samples) and to the generator (over fake ones).\nLSM(S,S′) where S′= Seg(X) (5)\nThis stands in a stark contrast to the reconstruction perceptual or feature-matching losses com-\nmonly used in conditional layout-to-image approaches [14, 35, 60], which, unjustiﬁably, compare\nthe appearance of a newly generated image X to that of a natural source image X′(mathematically,\nthrough either LP(X,X′) or LFM(f(X),f(X′))), even though there is no reason for them to be\nsimilar in terms of style and texture, beyond sharing the same layout. Consequently, those losses\nseverely and fundamentally hinder the diversity of the generated scenes (as shown in section 4). We\ncircumvent this deﬁciency by comparing instead the layout implied by the new image with the source\nlayout it is meant to follow, thereby staying true to the original objective of the translation task.\nSegment-Fidelity Loss. To further promote the semantic alignment between the layout and the image,\nand encourage ﬁdelity not only of the whole scene, but also of each segment it is composed of, we\nintroduce the Segment-Fidelity loss. Given the layout Sand the generated image X, the discriminator\nﬁrst concatenates and processes them together through a shared convolution-downsampling stem to\nreduce their resolution. Then, it partitions the image according to the layout (like a jigsaw puzzle)\nand assesses the ﬁdelity of each of the nsegments (si,xi) independently, through:\n1\nn\n∑\nLSF(D(si,xi)) (6)\n7\nTable 1: Unconditional approaches comparison. We compare the models on generating images from scratch,\nusing the FID and Precision/Recall metrics for evaluation, computed over 50k sample images. COCOp refers to\na mean score over a partitioned version of the dataset, created to improve samples’ visual quality (section I).\nCLEVR Bedrooms FFHQ\nModel FID ↓ Precision Recall FID ↓ Precision Recall FID ↓ Precision Recall\nGAN 25.02 21.77 16.76 12.16 52.17 13.63 13.18 67.15 17.64\nk-GAN 28.29 22.93 18.43 69.90 28.71 3.45 61.14 50.51 0.49\nSAGAN 26.04 30.09 15.16 14.06 54.82 7.26 16.21 64.84 12.26\nStyleGAN2 16.05 28.41 23.22 11.53 51.69 19.42 10.83 68.61 25.45\nVQGAN 32.60 46.55 63.33 59.63 55.24 28.00 63.12 67.01 29.67\nSBGAN 35.13 48.12 64.41 48.75 56.14 31.02 24.52 58.32 8.17\nGANformer 9.17 47.55 66.63 6.51 57.41 29.71 7.42 68.77 5.76\nGANformer2 4.70 64.18 67.03 6.05 60.93 37.15 7.77 61.54 34.45\nCOCO COCOp Cityscapes\nModel FID ↓ Precision Recall FID ↓ Precision Recall FID ↓ Precision Recall\nGAN 41.00 48.57 7.06 37.53 51.02 9.30 11.56 61.09 15.30\nk-GAN 63.51 42.06 5.42 61.05 45.37 7.04 51.08 18.80 1.73\nSAGAN 46.09 50.00 7.47 42.05 51.71 7.84 12.81 43.48 7.97\nStyleGAN2 26.79 50.92 23.30 25.24 52.93 24.48 8.35 59.35 27.82\nVQGAN 63.12 53.05 27.22 65.20 55.42 30.12 173.8 30.74 43.00\nSBGAN 108.1 42.53 17.53 104.32 44.43 19.12 54.92 52.12 25.08\nGANformer 25.24 53.68 12.31 23.81 55.29 14.48 5.76 48.06 33.65\nGANformer2 21.58 49.12 29.03 20.41 51.07 32.84 6.21 56.12 54.18\nUsing the stem both reduces the image dimension before splitting it up to improve computational\nefﬁciency, and further allows for local information about the contextual surroundings of each segment\nto propagate into its representation. This improves over the stricter patchGAN [35] which blindly\nsplits the image into a ﬁxed tabular grid, while our new loss offers a more natural choice, dividing it\nup into semantically-meaningful regions.\nOverall, to build the complete end-to-end GANformer2 model, we chain the two synthesis stages\ntogether: ﬁrst performing the planning stage, translating sampled latents into a schematic layout, and\nthen proceeding to the execution stage and transforming the layout into the ﬁnal output image.\n4 Experiments\nWe investigate GANformer2’s properties through a suite of quantitative and qualitative experiments.\nAs shown in section 4.1, the model attains state-of-the-art performance, successfully generating\nimages of high quality, diversity and semantic consistency, over multiple challenging datasets\nof highly-structured simulated and real-world scenes. In section 4.2, we compare the execution\nstage to leading layout-to-image approaches, demonstrating signiﬁcantly larger output variation,\npossibly thanks to the new structural loss functions we incorporate into the network. Further analysis\nconducted in sections 4.3 and 4.4 reveals several favorable properties that GANformer2 possesses,\nspeciﬁcally of enhanced interpretabilty and strong spatial disentanglement, which enables object-level\nmanipulation. In the supplementary, we provide additional visualizations of style and structure\ndisentanglement as well as ablation and variation studies, to shed more light upon the model behavior\nand operation. Taken altogether, the evaluation offers solid evidence for the effectiveness of our\napproach in modeling compositional scenes in a robust, controllable and interpretable manner.\n4.1 State-of-the-Art Comparison\nWe compare our model with both baselines as well as leading approaches for visual synthesis.\nUnconditional models include a baseline GAN [23], StyleGAN2 [42], Self-Attention GAN [81], the\nautoregressive VQGAN [21], the layerwise k-GAN [69], the two-stage SBGAN [5] and the original\nGANformer. Conditional approaches include SBGAN, BicycleGAN [ 86], Pix2PixHD [ 72], and\nSPADE [60]. We implement the unconditional methods within our public GANformer codebase and\nuse the authors’ ofﬁcial implementations for the conditional ones. All models have been trained with\nresolution of 256 ×256 and for an equal number of training steps, roughly spanning 10 days on a\nsingle V100 GPU per model. See section H for description of baselines and competing approaches,\nimplementation details, hyperparameter settings, data preparations and training conﬁguration.\nAs tables 1 and 3 show, the GANformer2 model outperforms prior work along most metrics and\ndatasets in both the conditional and unconditional settings. It achieves substantial gains in terms\nof FID score, which correlates with visual quality [ 29], as is likewise reﬂected through the good\n8\nTable 2: Disentanglement & controllability comparison over sample CLEVR images, using the DCI metrics\nfor latent-space disentanglement and our correlation-based metrics for controllability (see section 4.4).\nModel Disentanglement Modularity Completeness Informativeness Informativeness’ Object-ρ↓ Property-ρ↓\nGAN 0.126 0.631 0.071 0.583 0.434 0.72 0.85\nStyleGAN 0.208 0.703 0.124 0.685 0.332 0.64 0.49\nGANformer 0.768 0.952 0.270 0.972 0.963 0.38 0.45\nMONet 0.821 0.912 0.349 0.955 0.946 0.29 0.37\nIodine 0.784 0.948 0.382 0.941 0.937 0.27 0.35\nGANformer2 0.852 0.946 0.413 0.974 0.965 0.19 0.32\nPrecision scores. Notable also are the large improvements along Recall, in the unconditional and\nespecially the conditional settings, which indicate wider coverage of the natural image distribution.\nWe note that the performance gains are highest for the CLEVR [37] and the COCO [48] datasets,\nboth focusing on varied arrangements of multiple objects and high structural variance. These results\nserve as a strong evidence for the particular aptitude of GANformer2 for compositional scenes.\n4.2 Scene Diversity & Semantic Consistency\nFigure 8: Learning curves\nBeyond visual ﬁdelity, we study the models under the potentially\ncompeting aims of content variability on the one hand and semantic\nconsistency on the other. To assess consistency, we compare the\ninput layouts Swith those implied by the synthesized images, using\nstandard metrics of mean IoU, pixel accuracy and ARI. Following\nprior works [60, 86], the inferred layouts S′are obtained by a pre-\ntrained segmentation model that we use for evaluation purposes. To\nmeasure diversity, we sample n = 20 images Xi for each input\nlayout S, and compute the mean LPIPS pairwise distance [82] over\nimage pairs Xi,Xj. Ideally, we expect a good model to achieve\nstrong alignment between each input layout and the one derived from\nthe output synthesized image, while still maintaining high variance\nbetween samples that are conditioned on the same source layout.\nWe see in table 3 that GANformer2 produces images of signiﬁcantly\nbetter diversity, as reﬂected both through the LPIPS and Recall scores.\nOther conditional approaches such as Pix2PixHD, BicycleGAN and\nSPADE that rely on pixel-wise perceptual or feature matching, reach\nhigh precision at the expense of considerably reduced variability,\nrevealing the weakness of those loss functions. We further note that\nour model achieves better consistency than prior work, especially for\nLSUN-Bedrooms and COCO. These are also illustrated by ﬁgures 23-25 which present qualitative\ncomparison between synthesized samples along different dimensions. These results demonstrate the\nbeneﬁts of our new structural losses (section 3.3), as is also corroborated by the model’s learning\nefﬁciency when evaluated with different objectives (ﬁgure 8). We compare the Semantic-Matching\nand Segment-Fidelity losses with several alternatives (section D), and observe improved performance\nand accelerated learning when using the new objectives, possibly thanks to the local semantic\nalignment and ﬁner-detail ﬁdelity they respectively encourage.\n4.3 Transparency & Interpretability\nAn advantage of our model compared to traditional GANs, is that we incorporate explicit structure\ninto the visual generative process, which both makes it more explainable, while providing means for\ncontrolling individual scene elements (section 4.4). In ﬁgures 3 and 12, we see how GANformer2\ngradually constructs the image layout, recurrently adding new object segments into the scene. The\ngains of iterative over non-iterative generation are also demonstrated by ﬁgures 8 and 22. We note\nthat different datasets beneﬁt from a different number of recurrent synthesis steps, reﬂecting their\nrespective degree of structural compositionality (section E). Figures 2 and 11 further show that the\nmodel develops an internal notion for segments’ depth, which aids it in determining the most effective\norder to compile them together. Remarkably, no explicit information is given to the network about\nthe depth of entities within the scene, nor about effective orders to create them, and it rather learns\nthese on its own in an unsupervised manner, through the indirect need to identify relative depths and\nsegment ordering that will enable creating feasible and compelling layouts.\n9\nTable 3: Conditional approaches comparison. We compare conditional generative models that map input\nlayouts into output images. Multiple metrics are used for different purposes: FID, Inception (IS) and Precision\nscores for visual quality; class-weighted mean IoU, Pixel Accuracy (pAcc) and Adjusted Rand Index (ARI) for\nlayout-image consistency; Recall for coverage of the natural image distribution; and LPIPS for measuring the\noutput visual diversity given an input layout. All metrics are computed over 50k samples per model.\nCLEVR\nModel FID ↓ IS ↑ Precision ↑ Recall ↑ mIoU ↑ pAcc ↑ ARI ↑ LPIPS ↑\nPix2PixHD 4.88 2.19 72.49 60.40 98.18 99.06 97.90 6e-8\nSPADE 5.74 2.23 70.25 61.96 98.30 99.12 98.01 0.12\nBicycleGAN 35.62 2.29 6.27 10.11 88.15 92.78 90.21 0.10\nSBGAN 18.02 2.22 52.29 48.20 95.27 97.81 92.34 8e-7\nGANformer2 0.99 2.34 73.26 78.38 97.83 99.15 98.46 0.19\nBedrooms\nModel FID ↓ IS ↑ Precision ↑ Recall ↑ mIoU ↑ pAcc ↑ ARI ↑ LPIPS ↑\nPix2PixHD 32.44 2.27 70.46 2.41 64.54 76.62 75.48 2e-8\nSPADE 17.06 2.33 80.20 7.04 70.27 80.63 79.44 0.07\nBicycleGAN 23.34 2.52 49.68 10.45 58.47 70.76 72.73 0.22\nSBGAN 29.35 2.28 56.21 7.93 65.31 71.39 73.24 3e-8\nGANformer2 5.84 2.55 54.92 41.94 79.06 86.62 83.00 0.50\nCelebA\nModel FID ↓ IS ↑ Precision ↑ Recall ↑ mIoU ↑ pAcc ↑ ARI ↑ LPIPS ↑\nPix2PixHD 54.47 2.44 57.48 0.73 68.04 80.18 63.75 4e-9\nSPADE 33.42 2.35 85.08 2.72 67.99 80.11 63.80 0.07\nBicycleGAN 56.56 2.53 52.93 2.36 66.95 79.41 62.50 0.15\nSBGAN 50.92 2.44 59.28 3.62 67.42 80.12 61.42 4e-9\nGANformer2 6.87 3.17 57.32 37.06 68.88 80.66 64.65 0.38\nCityscapes\nModel FID ↓ IS ↑ Precision ↑ Recall ↑ mIoU ↑ pAcc ↑ ARI ↑ LPIPS ↑\nPix2PixHD 47.74 1.51 50.53 0.34 73.20 83.49 78.48 3e-8\nSPADE 22.98 1.46 83.32 9.55 75.96 85.25 80.74 3e-3\nBicycleGAN 25.02 1.56 63.01 9.49 67.24 78.43 72.20 7e-4\nSBGAN 45.82 1.52 51.38 5.39 72.04 81.90 79.32 4e-8\nGANformer2 5.95 1.63 84.12 37.72 76.11 85.58 80.12 0.27\nCOCO\nModel FID ↓ IS ↑ Precision ↑ Recall ↑ mIoU ↑ pAcc ↑ ARI ↑ LPIPS ↑\nPix2PixHD 18.91 17.97 80.91 16.35 58.75 71.94 68.20 1e-8\nSPADE 22.80 16.29 79.00 10.63 55.72 69.55 67.73 0.19\nBicycleGAN 69.88 9.16 40.05 5.83 33.58 47.98 56.55 0.18\nSBGAN 45.94 14.31 58.93 10.27 47.82 54.58 62.29 4e-8\nGANformer2 17.39 18.01 70.20 24.86 74.93 84.37 76.41 0.38\n4.4 Controllability & Disentanglement\nFigure 9: Amodal completion and re-\nﬂection update for object removal.\nThe iterative nature of GANformer2 and its compositional latent\nspace enhance its disentanglement along two key dimensions:\nspatially, enabling independent control over chosen objects,\nand semantically, separating between structure and style – the\nformer is considered during planning while the latter at exe-\ncution. Figures 6, 10 and 14-17 feature examples where we\nmodify an object of interest, changing its color, material, shape,\nor position, all without negatively impacting its surroundings.\nIn ﬁgures 4 and 18-21, we produce images of diverse colors and\ntextures while conforming to a source layout, and conversely,\nvary the scene composition while maintaining a consistent style.\nFinally, as illustrated by ﬁgures 9 and 13, we can even add or remove objects to the synthesized scene,\nwhile respecting interactions of occlusions, shadows and reﬂections, effectively achieving amodal\ncompletion, and potentially extrapolating beyond the training data (see sections C-B).\nTo quantify our model’s degree of disentanglement, we refer to the classic DCI and modularity metrics\n[17, 83], which measure the extant to which there is 1-to-1 correspondence between latent factors\nand image attributes. Following the protocol in [ 33, 75], we consider a sample set of synthesized\nCLEVR images, using a pre-trained object detector to extract and summarize their visual and semantic\nattributes. Note that since the CLEVR object detector reaches accuracy of 0.997, it does not introduce\nimprecisions into the evaluation. To further assess the level of controllability, we perform random\nperturbations over the latent variables used to generate the scenes, and estimate the mean correlation\nbetween the resulting semantic changes among object and property pairs. Intuitively, we expect a\ndisentangled representation to yield uncorrelated changes among objects and attributes. As shown\n10\nFigure 10: Localized property manipulation of selected objects, without negatively impacting their surround-\nings (rows 2-5), and unsupervised depth maps produced by GANformer2 (row 1).\nin table 2, GANformer2 attains higher disentanglement and stronger controllability than leading\nGAN-based and variational approaches, quantitatively conﬁrming the efﬁcacy of our approach.\n5 Conclusion\nIn this paper, we have tackled the task of visual synthesis, and introduced the GANformer2 model, an\niterative transformer for transparent and controllable scene generation. Similar to how a person may\napproach the creative task, it ﬁrst sketches a high-level outline and only then ﬁlls in the details. By\nequipping the network with a strong explicit structure, we endow it with the capacity to plan ahead\nits generation process, capture the dependencies and interactions between objects within the scene,\nand reason about them directly, moving a step closer towards compositional generative modeling.\nA limitation of our work is its reliance on training layouts, but we believe it could be mitigated\nby future work to integrate the generative efforts with unsupervised object discovery and scene\nunderstanding. Further exploration into disentanglement and means to encourage it may also be\nfruitful. From a broader perspective, enhancing properties of interpretability and controllability can\nmake content generation more reliable, accessible, and easy to interact with, for the beneﬁt of artists,\ngraphic designers, the ﬁlm industry and even the general society, and may open the door for new and\npromising avenues and opportunities.\n6 Acknowledgements\nWe performed the experiments for the paper on AWS cloud, thanks to Stanford HAI credit award.\nDrew A. Hudson (Dor) is a PhD student at Stanford University and C. Lawrence Zitnick is a research\nscientist at Facebook FAIR. We wish to thank the anonymous reviewers for their thorough, insightful\nand constructive feedback, questions and comments.\nDor wishes to thank Prof. Christopher D. Manning for the kind PhD support over the years that\nallowed this work to happen!\n11\nReferences\n[1] Titas Anciukevicius, Christoph H Lampert, and Paul Henderson. Object-centric image gen-\neration with factored depths, locations, and appearances. arXiv preprint arXiv:2004.00642,\n2020.\n[2] Jacob Andreas, Dan Klein, and Sergey Levine. Modular multitask reinforcement learning with\npolicy sketches. In International Conference on Machine Learning, pp. 166–175. PMLR, 2017.\n[3] Relja Arandjelovi´c and Andrew Zisserman. Object discovery with a copy-pasting GAN. arXiv\npreprint arXiv:1905.11369, 2019.\n[4] Oron Ashual and Lior Wolf. Specifying object attributes and relations in interactive scene\ngeneration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp.\n4561–4569, 2019.\n[5] Samaneh Azadi, Michael Tschannen, Eric Tzeng, Sylvain Gelly, Trevor Darrell, and Mario\nLucic. Semantic bottleneck scene generation. arXiv preprint arXiv:1911.11357, 2019.\n[6] Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, and Trevor Darrell. Compositional GAN:\nLearning image-conditional binary composition. International Journal of Computer Vision, 128\n(10):2570–2585, 2020.\n[7] Amir Bar, Roei Herzig, Xiaolong Wang, Anna Rohrbach, Gal Chechik, Trevor Darrell, and Amir\nGloberson. Compositional video synthesis with action graphs. arXiv preprint arXiv:2006.15327,\n2020.\n[8] David Bau, Hendrik Strobelt, William Peebles, Jonas Wulff, Bolei Zhou, Jun-Yan Zhu, and\nAntonio Torralba. Semantic photo manipulation with a generative image prior. arXiv preprint\narXiv:2005.07727, 2020.\n[9] David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, and\nAntonio Torralba. Paint by word. arXiv preprint arXiv:2103.10951, 2021.\n[10] Irving Biederman. Recognition-by-components: a theory of human image understanding.\nPsychological review, 94(2):115, 1987.\n[11] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity\nnatural image synthesis. In 7th International Conference on Learning Representations, ICLR,\n2019.\n[12] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt\nBotvinick, and Alexander Lerchner. MONet: Unsupervised scene decomposition and represen-\ntation. arXiv preprint arXiv:1901.11390, 2019.\n[13] Lucy Chai, Jonas Wulff, and Phillip Isola. Using latent space regression to analyze and leverage\ncompositionality in GANs. arXiv preprint arXiv:2103.10426, 2021.\n[14] Qifeng Chen and Vladlen Koltun. Photographic image synthesis with cascaded reﬁnement\nnetworks. In Proceedings of the IEEE international conference on computer vision, pp. 1511–\n1520, 2017.\n[15] Yunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.\nStarGAN: Uniﬁed generative adversarial networks for multi-domain image-to-image translation.\nIn 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt\nLake City, UT, USA, June 18-22, 2018, pp. 8789–8797. IEEE Computer Society, 2018. doi:\n10.1109/CVPR.2018.00916.\n[16] Zhiwei Deng, Jiacheng Chen, Yifang Fu, and Greg Mori. Probabilistic neural programmed\nnetworks for scene generation. In Proceedings of the 32nd International Conference on Neural\nInformation Processing Systems, pp. 4032–4042, 2018.\n[17] Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of\ndisentangled representations. In International Conference on Learning Representations, 2018.\n[18] Sébastien Ehrhardt, Oliver Groth, Aron Monszpart, Martin Engelcke, Ingmar Posner, Niloy J.\nMitra, and Andrea Vedaldi. RELATE: physically plausible multi-object scene synthesis using\nstructured latent spaces. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-\nFlorina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems\n33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual, 2020.\n12\n[19] Martin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS:\ngenerative scene inference and sampling with object-centric latent representations. In 8th\nInternational Conference on Learning Representations, ICLR, 2020.\n[20] S. M. Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Koray\nKavukcuoglu, and Geoffrey E. Hinton. Attend, infer, repeat: Fast scene understanding with\ngenerative models. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon,\nand Roman Garnett (eds.), Advances in Neural Information Processing Systems, pp. 3225–3233,\n2016.\n[21] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution\nimage synthesis. arXiv preprint arXiv:2012.09841, 2020.\n[22] Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv\npreprint arXiv:1805.04833, 2018.\n[23] Ian J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. arXiv preprint\narXiv:1406.2661, 2014.\n[24] Klaus Greff, Raphaël Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess,\nDaniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object rep-\nresentation learning with iterative variational inference. In Kamalika Chaudhuri and Ruslan\nSalakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,\nICML 2019, 9-15 June 2019, Long Beach, California, USA , volume 97 of Proceedings of\nMachine Learning Research, pp. 2424–2433. PMLR, 2019.\n[25] Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra.\nDRAW: A recurrent neural network for image generation. In Francis R. Bach and David M. Blei\n(eds.), Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,\nLille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference Proceedings, pp.\n1462–1471. JMLR.org, 2015.\n[26] Fanny Guérin, Bernadette Ska, and Sylvie Belleville. Cognitive processing of drawing abilities.\nBrain and Cognition, 40(3):464–478, 1999.\n[27] Fanny Guérin, Bernadette Ska, and Sylvie Belleville. Cognitive processing of drawing abilities.\nBrain and Cognition, 40(3):464–478, 1999.\n[28] Erik Härkönen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. GANspace: Discovering\ninterpretable GAN controls. arXiv preprint arXiv:2004.02546, 2020.\n[29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule converge to a local nash equilibrium. In\nIsabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N.\nVishwanathan, and Roman Garnett (eds.), Advances in Neural Information Processing Systems\n30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017,\nLong Beach, CA, USA, pp. 6626–6637, 2017.\n[30] Tobias Hinz, Stefan Heinrich, and Stefan Wermter. Generating multiple objects at spatially\ndistinct locations. arXiv preprint arXiv:1901.00686, 2019.\n[31] Seunghoon Hong, Dingdong Yang, Jongwook Choi, and Honglak Lee. Inferring semantic\nlayout for hierarchical text-to-image synthesis. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pp. 7986–7994, 2018.\n[32] Drew A Hudson and Christopher D Manning. Learning by abstraction: The neural state machine.\narXiv preprint arXiv:1907.03950, 2019.\n[33] Drew A Hudson and C. Lawrence Zitnick. Generative adversarial transformers. Proceedings of\nthe 38th International Conference on Machine Learning, ICML, 2021.\n[34] Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, and Roland Memisevic. Generating images\nwith recurrent adversarial networks. arXiv preprint arXiv:1602.05110, 2016.\n[35] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation\nwith conditional adversarial networks. In 2017 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pp. 5967–5976. IEEE\nComputer Society, 2017. doi: 10.1109/CVPR.2017.632.\n13\n[36] Ali Jahanian, Lucy Chai, and Phillip Isola. On the\" steerability\" of generative adversarial\nnetworks. arXiv preprint arXiv:1907.07171, 2019.\n[37] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick,\nand Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary\nvisual reasoning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pp.\n1988–1997, 2017.\n[38] Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In\nProceedings of the IEEE conference on computer vision and pattern recognition , pp. 1219–\n1228, 2018.\n[39] Asako Kanezaki. Unsupervised image segmentation by backpropagation. In 2018 IEEE\ninternational conference on acoustics, speech and signal processing (ICASSP), pp. 1543–1547.\nIEEE, 2018.\n[40] Andrea Kantrowitz. The man behind the curtain: what cognitive science reveals about drawing.\nJournal of Aesthetic Education, 46(1):1–14, 2012.\n[41] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs\nfor improved quality, stability, and variation. In 6th International Conference on Learning\nRepresentations, ICLR, 2018.\n[42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of styleGAN. In 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp.\n8107–8116. IEEE, 2020. doi: 10.1109/CVPR42600.2020.00813.\n[43] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár. Panoptic\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 9404–9413, 2019.\n[44] D Lane. Drawing and sketching: Understanding the complexity of paper–pencil interactions\nwithin technology education. Handbook of technology education. Springer international\npublishing AG, pp. 385–402, 2018.\n[45] Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro\nAcosta, Andrew P. Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi.\nPhoto-realistic single image super-resolution using a generative adversarial network. In IEEE\nConference on Computer Vision and Pattern Recognition, 2017.\n[46] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. MaskGAN: Towards diverse and\ninteractive facial image manipulation. InProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 5549–5558, 2020.\n[47] Chen-Hsuan Lin, Ersin Yumer, Oliver Wang, Eli Shechtman, and Simon Lucey. St-GAN:\nSpatial transformer generative adversarial networks for image compositing. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition, pp. 9455–9464, 2018.\n[48] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollár, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In European\nconference on computer vision, pp. 740–755. Springer, 2014.\n[49] Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong\nJiang, and Sungjin Ahn. SPACE: unsupervised object-oriented scene representation via spatial\nattention and decomposition. In 8th International Conference on Learning Representations,\nICLR, 2020.\n[50] Xihui Liu, Guojun Yin, Jing Shao, Xiaogang Wang, and Hongsheng Li. Learning to pre-\ndict layout-to-image conditional convolutions for semantic image synthesis. arXiv preprint\narXiv:1910.06809, 2019.\n[51] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg\nHeigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with\nslot attention. arXiv preprint arXiv:2006.15055, 2020.\n[52] Catarina Silva Martins. From scribbles to details. A Political Sociology of Educational\nKnowledge: Studies of Exclusions and Difference, pp. 103, 2017.\n14\n[53] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n[54] Li Nanbo, Cian Eastwood, and Robert Fisher. Learning object-centric representations of multi-\nobject scenes from multiple views. Advances in Neural Information Processing Systems, 33,\n2020.\n[55] Charlie Nash, SM Ali Eslami, Chris Burgess, Irina Higgins, Daniel Zoran, Theophane Weber,\nand Peter Battaglia. The multi-entity variational autoencoder. In NIPS Workshops, 2017.\n[56] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-Liang Yang, and Niloy Mitra. Block-\nGAN: Learning 3d object-aware scene representations from unlabelled images. arXiv preprint\narXiv:2002.08988, 2020.\n[57] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional genera-\ntive neural feature ﬁelds. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 11453–11464, 2021.\n[58] Takeshi Okada and Kentaro Ishibashi. Imitation, inspiration, and creation: Cognitive process of\ncreative drawing by copying others’ artworks.Cognitive science, 41(7):1804–1837, 2017.\n[59] Dim P Papadopoulos, Youssef Tamaazousti, Ferda Oﬂi, Ingmar Weber, and Antonio Torralba.\nHow to make a pizza: Learning a compositional layer-based GAN model. In proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8002–8011, 2019.\n[60] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis\nwith spatially-adaptive normalization. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pp. 2337–2346. Computer\nVision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00244.\n[61] Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film:\nVisual reasoning with a general conditioning layer. In Sheila A. McIlraith and Kilian Q.\nWeinberger (eds.), Proceedings of the Thirty-Second AAAI Conference on Artiﬁcial Intelligence,\npp. 3942–3951, 2018.\n[62] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for\nbiomedical image segmentation. In International Conference on Medical image computing and\ncomputer-assisted intervention, pp. 234–241. Springer, 2015.\n[63] RA Salome and BE Moore. The ﬁve stages of development in children’s art, 2010.\n[64] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Interpreting the latent space of GANs for\nsemantic face editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9243–9252, 2020.\n[65] Casper Kaae Sønderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszár. Amortised\nmap inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.\n[66] Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):\n89–96, 2007.\n[67] Bob Steele. Draw me a story: An illustrated exploration of drawing-as-language. Portage &\nMain Press, 1998.\n[68] Vadim Sushko, Edgar Schönfeld, Dan Zhang, Juergen Gall, Bernt Schiele, and Anna\nKhoreva. You only need adversarial supervision for semantic image synthesis. arXiv preprint\narXiv:2012.04781, 2020.\n[69] Sjoerd van Steenkiste, Karol Kurach, Jürgen Schmidhuber, and Sylvain Gelly. Investigating\nobject compositionality in generative adversarial networks. Neural Networks, 130:309–325,\n2020.\n[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\nGarnett (eds.), Advances in Neural Information Processing Systems, pp. 5998–6008, 2017.\n[71] Andrey V oynov and Artem Babenko. Unsupervised discovery of interpretable directions in the\nGAN latent space. In International Conference on Machine Learning, pp. 9786–9796. PMLR,\n2020.\n15\n[72] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.\nHigh-resolution image synthesis and semantic manipulation with conditional GANs. In Pro-\nceedings of the IEEE conference on computer vision and pattern recognition, pp. 8798–8807,\n2018.\n[73] Xiaolong Wang and Abhinav Gupta. Generative image modeling using style and structure\nadversarial networks. In European conference on computer vision, pp. 318–335. Springer, 2016.\n[74] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2.\nhttps://github.com/facebookresearch/detectron2, 2019.\n[75] Zongze Wu, Dani Lischinski, and Eli Shechtman. StyleSpace analysis: Disentangled controls\nfor StyleGAN image generation. arXiv preprint arXiv:2011.12799, 2020.\n[76] Kun Xu, Haoyu Liang, Jun Zhu, Hang Su, and Bo Zhang. Deep structured generative models.\narXiv preprint arXiv:1807.03877, 2018.\n[77] Jianwei Yang, Anitha Kannan, Dhruv Batra, and Devi Parikh. Lr-GAN: Layered recursive\ngenerative adversarial networks for image generation. arXiv preprint arXiv:1703.01560, 2017.\n[78] Denis Yarats and Mike Lewis. Hierarchical text generation and planning for strategic dialogue.\nIn International Conference on Machine Learning, pp. 5591–5599. PMLR, 2018.\n[79] Fangneng Zhan, Jiaxing Huang, and Shijian Lu. Hierarchy composition GAN for high-ﬁdelity\nimage synthesis.\n[80] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and\nDimitris N Metaxas. StackGAN: Text to photo-realistic image synthesis with stacked generative\nadversarial networks. In Proceedings of the IEEE international conference on computer vision,\npp. 5907–5915, 2017.\n[81] Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention\ngenerative adversarial networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),\nProceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June\n2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,\npp. 7354–7363. PMLR, 2019.\n[82] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unrea-\nsonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 586–595, 2018.\n[83] Bo Zhao, Bo Chang, Zequn Jie, and Leonid Sigal. Modular generative adversarial networks. In\nProceedings of the European conference on computer vision (ECCV), pp. 150–165, 2018.\n[84] Haitian Zheng, Zhe Lin, Jingwan Lu, Scott Cohen, Jianming Zhang, Ning Xu, and Jiebo\nLuo. Semantic layout manipulation with high-resolution sparse attention. arXiv preprint\narXiv:2012.07288, 2020.\n[85] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image\ntranslation using cycle-consistent adversarial networks. In IEEE International Conference on\nComputer Vision, ICCV, pp. 2242–2251, 2017.\n[86] Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver\nWang, and Eli Shechtman. Toward multimodal image-to-image translation. arXiv preprint\narXiv:1711.11586, 2017.\n[87] Peihao Zhu, Rameen Abdal, Yipeng Qin, and Peter Wonka. SEAN: image synthesis with\nsemantic region-adaptive normalization. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR, pp. 5103–5112. IEEE, 2020.\n16\nCompositional Transformers for Scene Generation\nSupplementary Material\nFigure 11: A visualization of the layouts and unsupervised depth mapsproduced by GANformer2’s planning\nstage while synthesizing varied images, making the generative process more structured and interpretable.\n17\nFigure 12: Recurrent scene generation. GANformer2 creates the layout sequentially, segment-by-segment, to\ncapture the scene’s compositionality, effectively allowing us to add or remove objects from the resulting images.\n18\nFigure 13: Object removal. Since GANformer2 creates each scene as a composition of interacting segments, it\nsupports adding and removal of objects while respecting various dependencies with their surroundings: Amodal\ncompletion of occluded objects is denoted by pink, updates of shadows and especially reﬂections by cyan, and\nother object removals cases by yellow.\n19\nFigure 14: Object controllability over structural attributes, achieved by modifying the model’s structure\nlatent ui of the chosen object during the planning stage (section 3.2). Shape manipulation is denoted by green,\nwhile position changes by yellow.\n20\nFigure 15: Object controllability over stylistic attributes, achieved by modifying the model’s style latentwi\nof the chosen object during the execution stage (section 3.3). Color manipulation is denoted by pink, while\nupdates of material by cyan.\n21\nFigure 16: Localized property manipulation of selected objects without negatively impacting their surround-\nings, over LSUN-Bedrooms scenes.\n22\nFigure 17: Localized property manipulation of selected objects, without negatively impacting their surround-\nings over LSUN-Bedrooms scenes (continued).\n23\nFigure 18: GANformer2 conditional generative diversity for CelebA. Images conform to the source layouts\nwhile still demonstrating high variance, featuring diversity both in stylistic aspects of lighting conditions and\ncolor scheme, but also in structural ones, as reﬂected through the hair type, background and age.\n24\nFigure 19: GANformer2 conditional generative diversity for LSUN-Bedrooms. Images conform to the\nsource layouts on the one end while still demonstrating high variance on the other, featuring diversity both in\nstylistic aspects of lighting condition and color scheme, but also in structural ones, as reﬂected through the\nwindows, paintings and bedding.\n25\nFigure 20: GANformer2 style and structure separation . We manipulate each aspect while maintaining\nconsistency over the other, varying the structure between columns and style between rows.\n26\nFigure 21: GANformer2 conditional generative diversity for CLEVR. The scenes signiﬁcantly vary in\ncombinations of objects’ colors and materials (contrary to competing approaches, as shown in ﬁgures 24-25,\nwhile still closely following the source layouts.\n27\nSteps 0 1 2 3\nCLEVR 11.95 7.54 4.70 5.25\nFFHQ 8.21 7.77 8.84 9.92\nCityscapes 9.83 8.51 7.26 6.21\nBedrooms 11.45 9.37 6.05 7.21\nCOCO 31.15 28.45 24.30 21.58\nFigure 22: Learning curves and performance comparison when varying the loss function for the execution\nstage (left), and the number of recurrent planning steps for the layout synthesis (center, right). In each step,\na random number of segments is generated, sampled from a trainable normal distribution, to ﬂexibly model\nhighly-structured scenes and capture conditional object dependencies within them while still maintaining good\ncomputational efﬁciency. “0” denotes a non-compositional model that generates the layouts in a single pass\nrather than as a collection of segments.\nA Overview\nIn the following, we provide additional qualitative and quantitative experiments for the GANformer2\nmodel. Figure 11 shows visualizations of the model’s generative process as it produces depth-\naware scene layouts which in turn guide and facilitate the synthesis of output photo-realistic images.\nThe model’s compositional structure does not only enhance its transparency, but also increases\nits controllability ( section B, ﬁgures 14-17), allowing GANformer2 to manipulate properties of\nindividual objects without negatively altering their surroundings, and even develop capacity for\namodal completion of occluded objects (section C, ﬁgures 12-13). The obtained decoupling between\nstructure and style is further illustrated through ﬁgures 18-19 and 20-21, respectively demonstrating\nvariability along one aspect while maintaining consistency over the other.\nWe believe the diversity achieved by GANformer2 results from the new structural losses we introduce\n(section 3.3), which we compare to several baselines and alternatives in ﬁgures 23-25 and section\nD. In section E, we proceed through additional model ablation and variation studies, to asses the\ncontribution of its different architectural components, and the number of recurrent planning steps\nin particular. Section F focuses on the training conﬁguration, comparing and contrasting between\npaired vs. unpaired settings. We conclude the supplementary by providing implementation details\n(section G), description of baselines and competing methods (section H), and information about data\npreparation procedures (section I).\nB Style & Structure Disentanglement\nGANformer2 decomposes the synthesis into two stages of planning and execution, the former\nproduces the scene layout and structure while the latter controls its texture, colors and style. Figures\nﬁgures 18-19 demonstrate the high content variability achieved over different datasets, while still\ncomplying with shared source layouts. Indeed, the styles differ substantially between one sample to\nanother, and variation is notably observed in local structures and elements, while still conforming to\nthe layout at the global scale. This is most noticeable in the CelebA case (ﬁgure 19), where some of\nthe images produced for given layouts depict young people while other old ones, and likewise some\nfeature straight hair while other curly hair, even when originating from the same layout. Likewise, in\nthe case of LSUN-Bedrooms (ﬁgure 18), we observe diversity that goes beyond aspects of texture\nand color scheme, featuring structural variations in entities such as windows, paintings, and bedding,\namong others. Meanwhile, ﬁgures 20-21 inversely show structural variability while maintaining a\nconsistent style, obtained by rendering different layouts using the same set of style latents {wi}.\nThe high diversity and consistency demonstrated here, which also quantitatively surpass the competing\napproaches (section 4.2), may be attributed to the new structural loss functions we employ (section\n3.3). These purely generative losses liberate us from resorting to perceptual and feature-matching\nobjectives common to prior work, which impose unnecessary and unjustiﬁed pixel-wise similarity\nconditions on the generated images, inhibiting their diversity. By sidestepping this reliance, we can\nunlock wider variation among the synthesized scenes, both in terms of structure and style. See a\ncomparison between synthesized samples of different approaches at ﬁgures 23-25.\n28\nTable 4: Paired vs. unpaired training\nPaired Unpaired Parallel\nCLEVR 4.70 6.58 6.72\nFFHQ 7.77 8.12 8.35\nCityscapes 6.21 7.32 7.81\nBedrooms 6.05 8.24 8.55\nCOCO 21.58 25.02 27.41\nTable 5: Hyperparameters\nMax Latents R1 reg\n#Latents Overall Dim weight (γ)\nFFHQ 20 128 10\nCLEVR 16 512 40\nCityscapes 64 512 20\nBedroom 64 512 100\nCOCO 64 512 100\nTable 6: Dataset conﬁgurations\nDataset Size Resolution Augment\nFFHQ 70,000 256×256 Flip\nCLEVR 100,015 256×256 None\nCityscapes 24,998 256×256 Flip\nBedrooms 3,033,042 256×256 None\nCOCO 287,330 256×256 Crop + Flip\nTable 7: COCO subset statistics\nCategory Size Category Size\nPeople 50293 Sports 37304\nChildren 13840 Skiing 5840\nEating 5096 Baseball 10067\nPlaying 5798 Tennis 11721\nRural 12084 Skating 6412\nOthers 13475 Surﬁng 3264\nAnimals 64066 Indoors 30946\nDogs 5245 Bathrooms 17121\nCats 5543 Kitchens 10427\nBirds 8153 Bedrooms 3398\nSheep 8134\nBears 8138 Outdoors 20713\nElephants 16204 Beaches 8396\nGiraffes 6352 Cities 8578\nZebras 6297 Streets 3739\nVehicles 53714 Misc 30294\nAirplanes 21232 Desserts 11370\nBuses 7436 Food 4176\nTrains 6661 Toys 7242\nBikes 18385 Electronics 7506\nC Object Controllability & Amodal Completion\nIn section 4.4, we explore the model’s spatial and semantic disentanglement, and study the degree of\ncontrollability achieved over individual objects and properties. Figure 14-17 provide a qualitative\nillustration, presenting examples of latent-space interpolations that lead to smooth and localized\nchanges of chosen objects and properties, selectively controlling either their structure or style.\nThanks to the compositional nature of the layout generation, we can even add or remove objects from\nthe scene, as is illustrated in ﬁgures 12-13, while respecting object interactions and dependencies\nsuch as shadows, reﬂections and occlusions. In particular, since GANformer2 creates the layouts\nsequentially by laying segments on top of each other (e.g. ﬁrst generating a road, and then placing a\ncar on top of it), it provides us with practical means to then remove the front segments and reveal the\nones behind them, effectively achieving amodal completion of occluded objects. Consequently, by\nvarying the number of generation steps, GANformer2 is also capable of extrapolating beyond the\ntraining data, e.g. creating empty CLEVR scenes (ﬁgure 3) even though the training data features at\nleast 3 objects at every image.\nD Structural Losses Comparison\nAs discussed in section 3.3, we introduce two new losses to the model’s execution stage, where we\ntransform input layouts into output photo-realistic images. The new losses of Semantic Matching\nand Segment Fidelity respectively encourage structural consistency between the layouts and the\nimages, and ﬁdelity at the level of the individual segment. In ﬁgure 22, we compare their performance\nin terms of FID score with several alternative objectives over the COCO dataset.\nSpeciﬁcally, we explore the following baselines: (1) Using no consistency loss at all (training with the\nstandard ﬁdelity loss only L(D(·))), in hopes that the model’s layout-conditioned feature modulation\nwill serve as an architectural bias to promote structural alignment; (2) A simple concatenation of\nthe layout Sand image X as they are fed into the discriminator D; (3) A Feature-Marching loss, as\nwidely used in prior work, LFM(f(X),f(X′)), that compares VGG features of the generated image\nwith those of the source natural imageX′that underlies the input layout S; and (4) an Edge-Matching\nloss, LEM(e(S),e(S′)), that compares the binary segmentation edges between the input layout S\nand a layout S′induced by the generated image X.\nAs ﬁgure 22 shows, our newly proposed losses, and the Semantic-Matching loss especially, surpass\nthe discussed baselines and effectively encourage GANformer2 to generate high-quality images. For\nthe Semantic-Matching loss, LSM(S,S′), we believe that providing semantic pixel-wise guidance to\n29\nLayout\n Pix2PixHD\n BicycleGAN\n SPADE\n GANformer2\nFigure 23: Comparison between samples of conditional generative models. GANformer2 achieves better\nvisual quality and demonstrates a larger variance in the spectrum of colors and textures, contrasting with other\napproaches that converge to a narrow range of gray-brownish hues.\nthe generator in terms of the classes each region in the image should depict, while not limiting its\ncontent to match an arbitrary natural image, as is the case in feature-matching losses, accelerates the\ngenerator’s learning without inhibiting its output diversity, which in turn yields better FID scores.\nLikewise, for the Segment-Fidelity loss, 1\nn\n∑LSF(D(si,xi)), promoting ﬁdelity of individual\nsegments xi, rather than just of the whole picture X, naturally enhances learning, especially for rich\nand highly-structured scenes, as we observe for the COCO dataset.\nE Model Ablations & Iterative Generation\nTo validate the efﬁcacy of our approach and better assess the relative contribution of each design\nchoice, we perform ablation and variation studies over the planning and execution stages. We begin by\nexploring the impact of varying the number of recurrent generation steps for planning the scene layout,\nand further compare it with a single-pass approach that generates the layout in a non-compositonal\nfashion – as a one image like standard GANs, rather then as a collection of interacting segments.\nWe note that each generation step creates a random number of segments, sampled from a trainable\nnormal distribution, and therefore reducing the number of steps does not limit the maximum number\nof segments the model can create overall. Adding more recurrent steps can instead enhance the\nmodel’s capability to capture conditional dependencies across segments, while keeping the planning\nprocess shorter can naturally increase the computational efﬁciency.\nCompositional Layout Synthesis. As ﬁgure 22 shows, compositional layout generation performs\nsubstantially better than the standard single-pass approach (denoted by “0”) across all datasets.\nIntuitively, we believe that the efﬁciency gains arise from the ability of the recurrent approach to\ndecompose the combinatorial space of possible scene layouts into several smaller tasks, such that\neach step focuses on a few segments only, rather than modeling the whole scene at once. This could\nbe especially useful for highly-structured scenes with multiple objects and dependencies.\n30\nLayout\n Pix2PixHD\n BicycleGAN\n SPADE\n GANformer2\nFigure 24: Comparison between samples of conditional generative models , demonstrating the GAN-\nformer2’s wider diversity in object’s properties, its higher compliance with the source layouts, especially\nin terms of shape consistency, and the more precise separation between close object segments.\nLayout Synthesis Length. We can see how the optimal number of synthesis steps vary between\ndifferent datasets: while CelebA layouts seem simple enough for the model to comprehend and\nsynthesize in just one step, CLEVR and LSUN-Bedrooms beneﬁt from 2 steps, and the richer\nCityscapes and COCO see performance improvement in even 3 layout generation steps. The most\neffective lengths seem to indicate the compositionality degree of each dataset: In CLEVR, objects\nare mostly independent of each other (holding only weak relations of not mutually occupying the\nsame space), and so their segments can be produced mostly in parallel, over less recurrent steps.\nMeanwhile, more intricate scenes, as in COCO and Cityscapes, beneﬁt from a longer sequential\ngeneration that can explicitly capture conditional dependencies among objects (e.g. generating a cup\nonly after creating the table it is placed on), demonstrating the strength of recurrent synthesis.\nLayout Reﬁnement . In the execution stage, which transforms layouts into output images, we\nexplore the contribution of the layout reﬁnement mechanism (section 3.3). It introduces a sigmoidal\ngate σ(g(S,X,W )) to support local layout adjustments, meant to increase the model’s ﬂexibility\nand expressivity during the translation. We study ablations over CLEVR, either not applying the\nreﬁnement or using limited gating versions, constraining the inputs to be the latents W, layout S,\nor image X only. We see that compared to the default model’s FID score of 4.70, using weaker\nreﬁnements lead to deterioration of 0.72, 0.78 and 0.85 when inputting the latents, layout or image\nrespectively, and a larger reduction of 1.45 points, when ablating the gating mechanism completely.\nThese results provide evidence for the beneﬁt of using the gating mechanism to reﬁne the scene’s\nlayout during the execution stage.\nF Paired vs. Unpaired Training\nWe train GANformer2 over two sets: of images {Xi}and layouts {Si}, used during the planning\nand execution stages respectively. The training sets can either be paired: listing the alignment\n31\nLayout\n Pix2PixHD\n BicycleGAN\n SPADE\n GANformer2\nFigure 25: Comparison between samples of conditional generative models , demonstrating the GAN-\nformer2’s wider diversity in object’s properties, its higher compliance with the source layouts, and the more\nprecise separation between different object segments (continued).\nbetween images and layouts, namely {(Si,Xi)}, or unpaired: {Xi},{Si}, and our training scheme\naccommodates both options: For the paired case, we ﬁrst train the planning stage over layouts {Si},\nand the execution stage conditionally over {(Si,Xi)}, such that it learns to translate ground-truth\nlayouts into output images. Then, to make both stages work in tandem, we ﬁne-tune them together,\nwhere this time the execution stage translates generated layouts {Sg\ni}into output images {Xg\ni}.\nFor the unpaired case, we can either (1) train the planning stage over the layouts {Si}, and then\nﬁne-tune it together with the execution stage, or (2) jointly train the two stages from scratch (we\ncall this scheme Parallel). Since the Segment-Fidelity loss assumes access to paired layout-image\nsamples, both fake and real, we do not use it in the unpaired and parallel cases, and instead use the\nEdge-Matching loss, described in D. We adjust the Edge-Matching and Semantic-Matching losses for\nboth the generator and the discriminator to be computed over generated pairs {(Sg\ni,Xg\ni)}, which are\navailable even when the training data is unpaired. Table 4 compares the model performance between\nthe paired, unpaired and parallel settings for different datasets. The he model manages to perform\nwell even in the unpaired and parallel settings, but achieves strongest results in the paired case.\nG Implementation & Training Details\nWe implement all the unconditional methods within the shared GANformer codebase [33], to ensures\nthey are tested under comparable conditions in terms of training details, model sizes, and optimiza-\ntion scheme. For the conditional models, we use the authors’ ofﬁcial implementations, likewise\nimplemented as extensions to the Pix2PixHD repository for conditional generative modeling. All\napproaches have been trained with images of 256 ×256 resolution and data augmentation as detailed\nin section I. See table 5 for the particular settings and hyperparameters of each model. The overall la-\ntent dimension is chosen based on performance among {128,256,512}. The R1 regularization factor\nγis likewise chosen based on performance and training stability among {1,10,20,40,80,100}.\n32\nIn terms of the loss function, optimization and training conﬁguration, we adopt the settings and\ntechniques used in StyleGAN2 and GANformer [33, 42], including in particular style mixing, Xavier\nInitialization, stochastic variation, exponential moving average for weights, and a non-saturating\nlogistic loss with lazy R1 regularization. We use Adam optimizer with batch size of 32 (4 times 8\nusing gradient accumulation), equalized learning rate of 0.001, β1 = 0.9 and β1 = 0.999 as well as\nleaky ReLU activations with α= 0.2, bilinear ﬁltering in all up/downsampling layers and minibatch\nstandard deviation layer at the end of the discriminator. The mapping layer of the generator consists of\n8 layers, and ResNet connections are used throughout the model, for the mapping network, synthesis\nnetwork and discriminator. All models have been trained for the same number of training steps,\nroughly spanning 10 days on 1 NVIDIA V100 GPU per model.\nH Baselines & Prior Approaches\nWe compare GANformer2 to both unconditional and conditional generative models. First, we inspect\nunconditional methods which synthesize images from scratch, including in particular: (1) a baseline\nGAN [23], (2) the StyleGAN2 [ 42] model, (3) SAGAN [ 81] which utilizes self-attention across\nspatial regions, (4) k-GAN [69] that blends together kgenerated images through alpha-composition,\n(5) VQGAN [21], a visual autoregessive autoencoder, (6) SBGAN [5], a non-compositional two-stage\napproach, and (7) the original GANformer [33].\nWe then proceed to compare our execution stage to popular conditional semantic generation models,\nincluding the aforementioned SBGAN and also: (8) Pix2PixHD [72] which uses a U-Net [62] to trans-\nlate source to target images, (9) BicycleGAN [86] that promotes cycle consistency among domains,\nand (10) SPADE [60], which performs spatial modualtion using a ﬁxed set of trainable semantic\ncategory vectors. For disentanglement and controllability experiments, we compare the GANformer2\nto a baseline GAN, StyleGAN2 and GANformer, as well as: (11) MONet and (12) Iodine, two\nsequential variational autoencoders.\nI Data Preparations\nWe train all models on images of 256 ×256 resolution, padded as necessary. See dataset statistics\nin table 6. The images in the Cityscapes and FFHQ datasets are mirror-augmented, while the\nimages in the COCO dataset are both mirror-augmented and also randomly cropped, to increase the\neffective training set size. We assume access to a training data of image and panoptic segmentations\n[43], indicating the segment unique identity and its semantic class. Contrary to prior conditional\nworks which rely on costly hand-annotated ground-truth segmentations, and to demonstrate the model\nrobustness, we instead intentionally explore training on auto-predicted segmentations (either produced\nin an unsupervised manner [39] for CLEVR [37] or by pre-trained segmentor [74] otherwise).\nFor the COCO dataset [ 48], we note that it introduces challenges from two perspectives, being\nboth highly-structured, with each scene populated by many objects that hold intricate relations and\ndependencies, but also visually and semantically diverse, consisting of varied images from a wide\nrange of domains. In order to isolate the former challenge of modeling compositional scenes from\nthe latter important but different challenge of covering a diverse image distribution, we study training\non a topical partition of COCO, named COCOp, that groups images into 7 semantically-related splits,\nlisted in table 7. To partition the dataset, we cluster t-SNE processed ResNet activations of the COCO\nimages into 31 subsets, which are then semantically grouped into the 7 splits. We train the models\non each split separately, and report the mean scores. As expected, and also empirically suggested\nby table 1, the partition leads to improved visual quality across all models – both baselines and new\nones, likely due to the more uniform resulting training distributions.\n33",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.7881566286087036
    },
    {
      "name": "Computer science",
      "score": 0.7794376015663147
    },
    {
      "name": "Sketch",
      "score": 0.5818101763725281
    },
    {
      "name": "Generative grammar",
      "score": 0.488800972700119
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47226205468177795
    },
    {
      "name": "Generative model",
      "score": 0.4518064260482788
    },
    {
      "name": "Transformer",
      "score": 0.4301041066646576
    },
    {
      "name": "Controllability",
      "score": 0.4231886565685272
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.41235774755477905
    },
    {
      "name": "Algorithm",
      "score": 0.18147459626197815
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Applied mathematics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    }
  ]
}