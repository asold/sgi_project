{
  "title": "Reveal of Vision Transformers Robustness against Adversarial Attacks",
  "url": "https://openalex.org/W3171408944",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5056110552",
      "name": "Ahmed Aldahdooh",
      "affiliations": [
        "Institut National des Sciences Appliquées de Rennes"
      ]
    },
    {
      "id": "https://openalex.org/A5084205574",
      "name": "Wassim Hamidouche",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5031377954",
      "name": "Olivier Déforges",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034994123",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W2963001136",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2970115835",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2005089986",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W3142085127",
    "https://openalex.org/W2171349048",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3102564565",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2543927648",
    "https://openalex.org/W2503523779",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W3163461448",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W3161120562",
    "https://openalex.org/W3035345420",
    "https://openalex.org/W3157326250",
    "https://openalex.org/W2607219512",
    "https://openalex.org/W3107235539",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W3013520104",
    "https://openalex.org/W3145185940",
    "https://openalex.org/W2893554781",
    "https://openalex.org/W2538525524",
    "https://openalex.org/W2158940042",
    "https://openalex.org/W2965496811",
    "https://openalex.org/W3080297477",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3104548192",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963178695",
    "https://openalex.org/W2612372205",
    "https://openalex.org/W2955127209",
    "https://openalex.org/W2963857521",
    "https://openalex.org/W3143373604",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3138049982",
    "https://openalex.org/W2963557656",
    "https://openalex.org/W2963143631",
    "https://openalex.org/W2774644650",
    "https://openalex.org/W2964253222",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3097217077",
    "https://openalex.org/W2963564844",
    "https://openalex.org/W3168997536",
    "https://openalex.org/W2180612164"
  ],
  "abstract": "The major part of the vanilla vision transformer (ViT) is the attention block that brings the power of mimicking the global context of the input image. For better performance, ViT needs large-scale training data. To overcome this data hunger limitation, many ViT-based networks, or hybrid-ViT, have been proposed to include local context during the training. The robustness of ViTs and its variants against adversarial attacks has not been widely investigated in the literature like CNNs. This work studies the robustness of ViT variants 1) against different Lp-based adversarial attacks in comparison with CNNs, 2) under adversarial examples (AEs) after applying preprocessing defense methods and 3) under the adaptive attacks using expectation over transformation (EOT) framework. To that end, we run a set of experiments on 1000 images from ImageNet-1k and then provide an analysis that reveals that vanilla ViT or hybrid-ViT are more robust than CNNs. For instance, we found that 1) Vanilla ViTs or hybrid-ViTs are more robust than CNNs under Lp-based attacks and under adaptive attacks. 2) Unlike hybrid-ViTs, Vanilla ViTs are not responding to preprocessing defenses that mainly reduce the high frequency components. Furthermore, feature maps, attention maps, and Grad-CAM visualization jointly with image quality measures, and perturbations' energy spectrum are provided for an insight understanding of attention-based models.",
  "full_text": "IEEE TRANSACTIONS ON XXXX 1\nReveal of Vision Transformers Robustness\nagainst Adversarial Attacks\nAhmed Aldahdooh,Wassim Hamidouche,and Olivier Déforges\nAbstract—The major part of the vanilla vision transformer (ViT) is the attention block that brings the power of mimicking the global\ncontext of the input image. For better performance, ViT needs large-scale training data. To overcome this data hunger limitation, many\nViT-based networks, or hybrid-ViT, have been proposed to include local context during the training. The robustness of ViTs and its\nvariants against adversarial attacks has not been widely investigated in the literature likeCNNs. This work studies the robustness ofViT\nvariants 1) against differentLp-based adversarial attacks in comparison withCNNs, 2) under adversarial examples (AEs) after applying\npreprocessing defense methods and 3) under the adaptive attacks using expectation over transformation (EOT) framework. To that end,\nwe run a set of experiments on 1000 images from ImageNet-1k and then provide an analysis that reveals that vanilla ViT or hybrid-ViT\nare more robust than CNNs. For instance, we found that 1) Vanilla ViTs or hybrid-ViTs are more robust than CNNs underLp-based\nattacks and under adaptive attacks. 2) Unlike hybrid-ViTs, VanillaViTs are not responding to preprocessing defenses that mainly reduce\nthe high frequency components. Furthermore, feature maps, attention maps, and Grad-CAM visualization jointly with image quality\nmeasures, and perturbations’ energy spectrum are provided for an insight understanding of attention-based models.\nIndex Terms—Vision transformer, convolutional neural network, robustness, adversarial attacks, deep learning.\n!\n1 I NTRODUCTION\nI\nMAGE classiﬁcation task models have remarkable progress\nin its prediction accuracy especially when convolutional\nblocks serve as the main building block of the model [1].\nConvolutional blocks have the ability to exploit the spatial\nfeatures and in particular the low-level features [2]. On\nthe other hand, self-attention blocks in Transformers [3]\nshowed great success in natural language processing ( NLP)\nmodels [3], and recently, Dosovitskiy et al. proposed vision\ntransformer (ViT), vanilla ViT, the ﬁrst image classiﬁcation\nmodel that uses the pure transformer encoder blocks [4]\nand image patches, as tokenization, to build the classiﬁer.\nTo overcome the lack of the inductive biases inherent to\nconvolutional neural networks ( CNNs), it was shown that\nViT achieves better performance than state-of-the-art CNNs\nmodels of similar capacity, such as ResNet [1] and its vari-\nants if ViT is trained with signiﬁcantly large-scale training\ndatasets, such as JFT-300M [4], [5]. Moreover, ViT models\nthat are trained on large-scale datasets can be downgraded\nto smaller datasets, such as ImageNet-1k [6], via transfer\nlearning, leading to performance comparable to or better\nthan state-of-the-art CNNs models. Given the advantage\nof the ViT and on the other side its limitation to the huge\nneed of the data, other models that combine the vanilla ViT\nwith other modules, like tokens-to-token ViT ( T2T-ViT) [2],\ntransformer-in-transformer (TNT) [7], and CvT [8] models\n• All authors are with INSA Rennes, CNRS, IETR - UMR 6164, University\nof Rennes, 35000, Rennes, France.\n• Corresponding author: A. Aldahdooh,\nE-mail: ahmed.aldahdooh@insa-rennes.fr\n• This work has been submitted to the IEEE for possible publication.\nCopyright may be transferred without notice, after which this version\nmay no longer be accessible.\nwere proposed for better learning the low-level features in\nthe transformer and hence, reduce their dependency on large\ndatasets. These models are also known as hybrid-ViT models\nGiven many CNN image classiﬁcation task models [1],\n[9], [10], [11], their properties are well studied and analyzed\nincluding the robustness against adversarial examples ( AEs)\n[12]. AE is a combination of the original image and a\ncarefully crafted perturbation [12]. This perturbation is\nhardly perceptible to humans, while it causes the deep\nlearning (DL) model to misclassify the input image. Since\nthe feature space identiﬁcation of the AE is hard to predict\n[13], [14], the adversarial attacks threat is very challenging.\nAdversary can generate AEs under white box, black box, and\ngray box attack scenarios [15], [16].\nDue to the success of ViT and its variants in various\ncomputer vision tasks, the insight properties and robustness\nstudies for such transformers are yet under investigation.\nRecently, four studies [17], [18], [19], [20] showed some\nof these robustness properties for the vision transformers\nand compared them with the competitive CNN models. In\nthis work, we experimentally investigate the robustness of\ndifferent ViT variants against different Lp-based and color\nchannel perturbations ( CCP) attacks and their strength of\npredicting the preprocessed AEs, as defense. The results\nare compared with competitive CNN models. We attend\nto answer the following research questions 1) Are ViT\nvariants more robust than CNNs against L0, L1, L2, and L∞\nbased attacks and against CCP attack? 2) Are ViT variants\nmore robust than CNNs under the preprocessed AEs? 3) Is\nincreasing the number of attention blocks has an effect on\nthe robustness against the AEs and under the preprocessed\nAEs? 4) Is enhancing ViT tokenization method has an effect\non increasing the robustness against the AEs and under the\npreprocessed AEs? Hence, this work will provide researchers\narXiv:2106.03734v2  [cs.CV]  20 Sep 2021\nIEEE TRANSACTIONS ON XXXX 2\n0 100-010 20 30 40 50 60 70 80 90\n88\n90\n92\n94\n96\n98\n100ASR(%) of AEs ( ↓)\nViT\n100-010 20 30 40 50 60 70 80 90\nTarget model top-1 error of AEs after preprocessing ( ↓)\nHybrid-ViT\n10010 20 30 40 50 60 70 80 90\nCNN\nPreprocessing Method\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nTarget Models\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nFig. 1: The attack success rate (ASR) of target models, on 1000 images from ImageNet-1k, against AutoAttack, in average ϵ= {1,2,4}/255, and\nthe target model top-1 error of the preprocessed AEs for six different preprocessing defense methods including SS: local spatial smoothing, NLM:\nnon-local mean, TVM: total variation minimization, JPEG: JPEG compression, CR: cropping and re-scaling and CCP: color channel perturbations.\nwith an in-depth understanding on how vision transformers\nbehave against different attack settings with preprocessing\ndefenses, see Figure 1. The main contribution of this work is\nto highlight the following observations:\n• ViT-based models are more robust than CNNs against\nLp-norm attacks.\n• Neither larger model’s architecture, like ViT-L, nor\nbringing convolutional modules for tokenization in ViTs,\nlike TNT, will necessarily enhance the robustness.\n• In general, transferability exists within the model ar-\nchitecture family and the transferability became lower\nwhen transfer from large to small variants and vice\nversa.\n• Black-box attacks are transferable to CNNs when they\nare generated using vanilla ViTs and hybrid-ViTs and\nnot vice versa.\n• Vanilla ViTs are not responding to preprocessing de-\nfenses that mainly reduce the high frequency compo-\nnents, such as local spatial smoothing ( SS) and JPEG\ncompression (JPEG). Hybrid-ViTs are more responsive\nto preprocessing defenses than vanilla ViTs and CNNs,\nsee Figure 1.\n• Hybrid-ViT models show better robustness than vanilla\nViTs and ResNets under the expectation over transfor-\nmation (EOT) robustness test.\n2 R ELATED WORK\nRobustness of computer vision CNNs against AEs is well\nstudied in the literature [12], and many countermeasures [15],\n[16], [21], [22], [23], i.e. defenses and detectors, were imple-\nmented to characterize the feature space of the AEs. On\nthe other hand, self-attention based computer vision models\nare under investigation. The work in [24] used vanilla self-\nattention transformer to improve the object detection task\ndetector’s robustness against AEs. The new module used\nthe self-attention transformer to model the features that are\nextracted from fast region convolutional neural network\n(Fast-RCNN) [25] before running the detection classiﬁer.\nRecently, four studies [17], [18], [19], [20] provided some\nunderstanding of the robustness of ViT, and its variants.\nAuthors in [17], [19], [20] studied the robustness of the\nvanilla ViTs, while the work in [18] studied the robustness\nof vanilla ViTs and of other models that combine vanilla\nViT with other modules like convolutional blocks, hybrid-\nViTs. The work in [17] investigated the ViT robustness\nwith respect to the perturbations of both the input and\nmodel. The study of input perturbations included natural\ncorruption, real-world distribution shifts, natural adversarial\nperturbations, adversarial attack perturbations, adversarial\nspatial perturbations, and texture bias. The adversarial attack\nperturbations are generated using L∞bounded (ϵ) AEs using\nfast gradient sign method (FGSM) [26] and projected gradient\ndescent (PGD) [27] attacks where ϵwas set to one gray-level\n(1/255). The input perturbations are tested on models that\nare pretrained on ImageNet-1k [6], ImageNet-21k [28], and\nJFT-300M [5] datasets. It was shown that, like CNNs, ViTs\nare vulnerable to AEs and it is more robust than CNNs with\ncomparable capacity when trained on sufﬁcient training data.\nSimilarly, the work in [19] investigated the robustness of ViT\nand big transfer (BiT) CNN [29] against common corruptions\nand perturbations, distribution shifts, and natural adversarial\nexamples. The work in [18] showed that this observation is\nnot necessarily true and claimed that the robustness of ViT is\nmore related to transformer architecture rather than the pre-\ntraining. In [18], it was shown that introducing convolutional\nblocks or token s-to-token [2] modules compromises the\nmodel robustness when tested against L∞ PGD [27] and\nauto projected gradient descent ( Auto-PGD) [30] attacks.\nMoreover, [18] showed that CNNs are less robust than\nViTs since CNN tends to learn low-level features that are\nmuch vulnerable to adversarial perturbations. The three\nworks in [17], [18], [19] showed that AEs do not transfer\nacross different model architectures’ families which leads\nto have an ensemble models approach to defend against\nthe attacks. Hence, the work in [19], included more L∞\nattacks like; Carlini-Wagner (CW) [31], momentum iterative\ngradient-based method (MI-FGSM) [32], and backward pass\ndifferentiable approximation (BPDA) [33], and focused on the\ntransferability of the attacks to introduce the self-attention\nIEEE TRANSACTIONS ON XXXX 3\ngradient attack (SAGA) that can fool the ensemble defense\nmodel. In this work, we investigate the robustness against\ndifferent Lp norms attacks and the robustness under the\npre-processed AEs, as a defense technique.\n3 P RELIMINARIES\n3.1 Models’ architectures\nIn this section, a brief description of models that are inves-\ntigated in our experiments is introduced. Table 1 gives the\nparameters of the investigated models1.\nVanilla transformer [4]. The vanilla self-attention Trans-\nformer was introduced for NLP tasks. It stacks the encoder\nand the decoder blocks, where each block stacks N attention\nblocks. The attention block has two sub-layers. The ﬁrst\none is a multi-head self-attention mechanism, and the\nsecond one is a simple, position-wise fully connected feed-\nforward network. The residual connection around each of\nthe two sub-layers followed by layer normalization are\nemployed. Encoder and decoder input tokens are converted\nto a sequence of vectors using learned embeddings. Finally,\nthe learned linear transformation and softmax function are\nused to convert the decoder output to predict next-token\nprobabilities.\nVision transformer (ViT) [4]. It is the ﬁrst model that\nemployed the vanilla transformer architecture and achieved\nstate-of-the-art performance on image classiﬁcation task. To\nadapt the vanilla transformer for image classiﬁcation, ViT\n1) divides the input image into a sequence of patches and\nthen linearly projects them to the transformer, 2) appends\n[CLS] token to the input and output representations that are\npassed to multi-layer perceptron (MLP) for the classiﬁcation\ntask. In our investigation, we consider different ViT models\nincluding ViT-S/B/L-16 models. In the single-head self-\nattention ( SHSA) of ViTs [4], each image X ∈ Rn×d is\npresented as a sequence of npatches (x1,x2,...x n), where\ndis the embedding dimension to represent each patch. Then,\nthe image sequence, X, is linearly projected onto query\n(Q= XWq), key (K = XWk), and value (V = XWv), where\nWq ∈Rd×dq , Wk ∈Rd×dk , and Wv ∈Rd×dv . In this way, the\nimage patch is encoded in terms of the global information,\nwhich enables the self-attention structure mechanism to\ncapture the interactions among image patches. By applying\nscaled dot-product attention mechanism, the output of one\nself-attention Z is computed as:\nZ = Attention(Q,K,V ) =softmax\n(QKT\n√dk\n)\nV. (1)\nAccording to (1), ﬁrstly, the attention scores of input query\nand key are calculated, S = QKT , which is then normalized\nby\n√\ndto prevent pushing the softmax function into regions\nwhere it has extremely small gradients when d becomes\nlarge. Then, the probabilities of the normalized scores are\ncalculated, P = softmax(S/\n√\nd). Finally, the value vector V\nis multiplied by the probabilities P to calculate Z in which\nlarger probabilities are the focus in the following layers.\nThe SHSA limits the capability to highlight the impor-\ntance of other equally important patches at the same time.\n1. The weights are available here , for all models except for T2T-ViTs\nwhich are available here.\nHaving multi-head self-attention (MHSA) can mitigate this\nproblem by having different Q, K, and V vectors for each\nself-attention, and hence, can jointly attend to information\nfrom different representation subspaces at different positions.\nIn MHSA, the Q, K, and V have the size of X ∈Rn×d/h,\nwhere his the number of heads, and dq = dk = dv = d/6.\nHence, the output of MHSA has the output of size X ∈Rn×d\nand is calculated as follows:\nMultiHead(Q′,K′,V ′) = (head1 ⊕···⊕ headh)Wo,\nheadi = Attention(Qi,Ki,Vi), ∀i∈{1,··· ,h}, (2)\nwhere Q′, K′, V′ are the concatenation of {Qi}h\ni=1,\n{Ki}h\ni=1, and {Vi}h\ni=1 respectively, and Wo ∈Rd×d is the\nlinear projection matrix. ⊕stands for vector concatenation\noperation.\nThe ViT’s encoder block stacks N layers of attention\nblocks. The attention block has two sub-layers. The ﬁrst\none is a MHSA, and the second one is a simple MLP layer,\nalso known as position-wise fully connected feed-forward\nnetwork (FFN). Layernorm (LN) is applied before every\nsub-layer, and residual connections are applied after each\nsub-layer. This latter consists of two linear transformation\nlayers and a nonlinear activation function, Gaussian error\nlinear units (GELU) [34], in between.\nThe work in [18] showed that the fact of having ViTs\ntrained on large-scale dataset yield to more robust models\nthan CNNs is not necessarily true and claimed that the\nrobustness of ViTs is more related to transformer structure\nrather than the pre-training. Hence, in our investigation,\nVanilla ViTs are pretrained on ImageNet [28] and ﬁne-tuned\non ImageNet-1k [6], while hybrid- ViTs and CNN are trained\nfrom scratch on ImageNet-1k.\nHybrid-ViT models [2], [4], [7] . One key of CNNs\nsuccess is the ability to learn local features, while the key\nsuccess of self-attention based transformer is its ability to\nlearn the global features. Hence, many approaches were\nintroduced to integrate local feature representation in ViT\nmodel. The work in [4], replaced input image patches with\nthe CNN feature map patches and introduced ViT-Res that\nused the ﬂattened ResNet feature maps to generate the input\nsequence. In [2], tokens-to-token ViT (T2T-ViT) is introduced.\nIt replaces input image patches with a layer-wise token s-\nto-token (T2T) transformation, then, the features that are\nlearned by T2T module are passed to the ViT. The aim\nof T2T module is to progressively structurize the image\ninto tokens by recursively aggregating neighboring tokens\ninto one token. The work in [7] proposed a transformer-in-\ntransformer (TNT) architecture to model both patch-level and\npixel-level representations. The TNT is made up by stacking\nTNT blocks. Each TNT block has an inner transformer and an\nouter transformer. The inner transformer block extracts local\nfeatures from pixel embeddings. In order to add the output of\nthe inner transformer into the patch embeddings, the output\nof the inner transformer is projected to the space of patch\nembedding using a linear transformation layer. The outer\ntransformer block is used to process patch embeddings. In\nour investigation, we consider T2T-ViT-14/24, and TNT-S-16\nmodels.\nConvolutional neural network (CNN). The convolu-\ntional layer is the basic building block for the CNN models. It\nhas a set of learnable small receptive ﬁeld ﬁlters. These ﬁlters\nIEEE TRANSACTIONS ON XXXX 4\nTABLE 1: ViT, ViT-based, and CNN model variants that are investigated in this work.\nCategory Model ViT Backbone Params (M) Top-1 Acc.(%)\non ImageNet-1kLayers Hidden size MLP size\nVanilla ViT †\nViT-S-16 [35] 8 786 2358 49 77.858\nViT-B-16 [4] 12 786 3072 87 81.786\nViT-L-16 [4] 24 1024 4096 304 83.062\nHybrid-ViT ∗\nViT-Res-16 (384) [4] 12 786 3072 87 84.972\nT2T-ViT-14 [2] 14 384 1152 22 81.7\nT2T-ViT-24 [2] 24 512 1536 64 82.6\nTNT-S-16 [7] 12 384 1536 24 81.518\nCNN Backbone Conv. Layers\nCNN ∗\nResNet50 [1] 49 23 79.038\nResNet50-32x4d\n(ResNeXt50) [36] 49 25 79.676\nVGG16 [9] 13 138 71.594\n†Pre-trained on ImageNet-21k [28] and ﬁne tuned on ImageNet-1k [6],\n∗Trained from scratch on ImageNet-1k [6]\nare convolved across the full depth of the input to produce\nthe feature maps. Hence, feature maps can remarkably repre-\nsent local structure of the input image. Many architectures\nhave been proposed in the literature [37]. In our investigation,\nwe consider ResNet50 [1], ResNeXt50_32x4d [36], and VGG16\n[9] models.\n3.2 Adversarial attacks\nOur investigation assumes that the adversary can create\nan AE x′ by perturbing the input image x with a cer-\ntain amount of noise ϵ, such that ||x−x′||p ≤ ϵ, where\np∈{0,1,2,..., ∞}, under different attack scenarios; white-\n, black-, hybrid-, and gray-box attacks. We investigate the\ntarget models with 1000 images that are correctly classiﬁed\nby all the target models. The test samples are collected\nfrom ImageNet-1k validation dataset [6]. Our investigation,\nincluded ﬁve white-box attacks: jacobian saliency map attack\n(JSMA) [38], FGSM [26], PGD [27], universal adversarial\nperturbations (UAP) [39], and CW [31]. For the black-box\nattacks, we consider square attack ( SA) [40], ray searching\n(RayS) [41], and CCP [42] attacks. Moreover, we consider\nautoattack ( AA) [30], as hybrid-box attack, which is an\nensemble attack that runs three attacks to generate the AEs:\nthe Auto-PGD, acfab [43], and SA. For the gray-box attacks,\nthe adversary has knowledge about training data but not\nthe model architecture and depends on the transferability\nproperty of the attacks to generate the AEs using a surrogate\nmodel. Given the surrogate model, the aforementioned\nattacks can be used to generate AEs. In our investigation, one\nof our target models will be considered as a surrogate model\nto generate the AEs and these AEs will be tested on the other\ntarget models. Table 2 lists the attack’s parameters that are\nused in our experiments. All the attacks are generated using\nadversarial robustness toolbox (ART) 2, except RayS attack\nthat is generated using the ofﬁcial implementation 3.\nIn [18], a preliminary results were introduced to train\nrobust ViT classiﬁers using adversarial training ( AT). It\nis stated that “ ViT does not advance the robust accuracy\nafter adversarial training compared to large CNNs such as\n2. https://github.com/Trusted-AI/adversarial-robustness-toolbox\n3. https://github.com/uclaml/RayS\nWideResNet-34-10”. That is because ViT may need larger\ntraining data or longer training epochs to further improve its\nrobust training performance. Moreover, it is found that AT\ncan still cause catastrophic overﬁtting for ViT when fast AT\ntraining is conducted, to mitigate the overﬁtting, with FGSM,\nfurther adjustments were needed to propose AT for ViT. Due\nto such reasons we couldn’t conduct a fair AT comparison\nwith CNN and we left it as future work.\n3.3 Pre-processing defense methods\nWe consider pre-processing methods that were previously\napplied to the AEs before being forwarded to the CNNs in\norder to alleviate the effect of added perturbations. In our\ninvestigation, we consider local smoothing [44], non-local\nmean denoising [44], total variation minimization [45] , JPEG\nimage compression [46], [47], and crop and re-scaling [48].\nLocal smoothing [44]. Is a method that uses a pixel\nneighborhood to smooth out each pixel. For a given n×n\nsliding window, local smoothing changes each pixel, center of\nthe sliding window, with the mean, the median, or Gaussian\nsmooth of the window. In [44], it was shown that median\nﬁlter is more efﬁcient in projecting the AE back to the data\nmanifold especially for L0 based attacks, since it is capable\nof removing sparsely pixels in the input image, and at the\nsame time it preserves edges. In our investigation we use the\nmedian ﬁlter with 3 ×3 window size.\nNon-local mean denoising [44]. As the name indicates,\nthe non-local smoothing not only uses the nearby pixels,\nbut uses several similar patches within a search window,\nas well, to smooth out the current patch. While preserving\nimage edges, it is assumed that averaging similar patches\nto smooth the current patch will remove the perturbations\nwhen the mean of the noise is zero. In our investigation,\nwe use the patch size of 7 ×7 and the search window of\n23 ×23 and the strength of 0.07. We use skimage restoration\nlibrary to run the non-local mean denoising. For each noisy\nimage, i.e. the AE, the robust wavelet-based estimator of the\nGaussian noise standard deviation is used before applying\nthe non-local mean (NLM) denoiser [49].\nTotal variation minimization [45]. Total variation de-\nnoising was shown to be effective in removing adversarial\nIEEE TRANSACTIONS ON XXXX 5\nTABLE 2: Considered adversarial attacks and their parameters.\nScenario Attack norm Parameters\nWhite box\nFGSM L∞ ϵ ∈{1,2,4,8,16,24}/255\nPGD L1 ϵ ∈{100 ,150 ,200 ,400 ,600 ,800 ,1000 }, ϵstep = ϵ/10, max. iterations =10\nPGD L2 ϵ ∈{0.5,1,2,3,4,5}, ϵstep = ϵ/10, max. iterations =10\nPGD L∞ ϵ ∈{1,2,4}/255 , ϵstep = ϵ/10, max. iterations =10\nCW L2 max. iterations =10, learning rate =5e-3, initial const =2/255, binary search steps= 10\nCW L∞ ϵ = 8/255 , conﬁdence =0, max. iterations =50, learning rate =5e-3\nUAP L∞ ϵ ∈{1,2,4}/255 , attacker =BIM, ϵstep = ϵ/10, max. iterations =10\nJSMA L0 θ = 0.1, γ = 1,\nBlack box SA L∞ ϵ ∈{8,16}/255 , p = 0.05, max. iterations =300, restarts =1\nRayS L∞ ϵ = 8/255 , query =2000\nHybrid box AA L∞ ϵ ∈{1,2,4}/255\nOther Attacks CCP - seed =0 for ﬁxed random-weight based CCP attack, s = 2 and b = 30.\nperturbations due to its ability to minimize the total variation\nof the image by producing similar images to the noisy images.\nIt was shown that total variation simultaneously preserves\nedges and denoises ﬂat regions. We use the Chambolle\nalgorithm [50] that is implemented in skimage library with\nstrength of 0.1 and ϵ= 2e−4.\nJPEG image compression [46], [47]. Compression is used\nto remove redundancy in images by removing high frequency\ncomponents that are imperceptible to humans. On the other\nhand, adversarial perturbation is imperceptible and hence\nit is assumed that the high frequency details of the image\nare more vulnerable to adversarial perturbations. In our\nexperiments, we compress the AEs at quality level of 65%.\nCropping and re-scaling [48]. It was shown that image\ncropping and re-scaling method is an effective way to remove\nthe adversarial perturbations effect due to the spatial re-\npositioning of the pixels. This method may harm the structure\nof the carefully crafted adversarial perturbations. We perform\ncenter cropping with 2-pixel margin for top, bottom, left, and\nright of the image, and then re-scale the cropped image to\nthe input size.\nFinally, it is worth mentioning that the pre-processing\ndefense can be easily fooled using BPDA [33] and EOT [51]\ntechniques. In our investigation, we consider the EOT attack\nto have a deep insight in the robustness of the ViTs based\nmodels.\n3.4 Analysis tools\nWe rely our investigations on tools that can reveal the\nproperties of the model’s behavior such as feature [2],\nattention [4], and gradient-weighted class activation mapping\n(Grad-CAM) [52] maps. Moreover, we use discrete cosine\ntransform (DCT) based decomposition of the perturbations\n[53] and the visual quality assessment of the AE [54] as tools\nto assess the robustness of target models against the AEs. We\nconsider three objective visual quality assessment metrics\nincluding peak signal to noise ratio ( PSNR), structural\nsimilarity index measure ( SSIM) [55], and most apparent\ndistortion (MAD) [56].\nFeature and attention maps: In [2], a visualization\nmethod to visualize the feature maps for CNNs and ViTs\nis recommended. For simplicity, we get the feature maps\nof clean and adversarial samples and then visualize the\ndifference of one channel only, randomly selected, and ﬁnally\nnormalize it to [0, 1] scale. As recommended in [2], input\nsample is upsampled to clearly visualize the ViT feature\nmaps. To visualize the attention map, we follow the rollout-\nbased attention visualization method that is provided by [4]\nand implemented in the ViT-pytorch github repository4.\nGrad-CAM [52] uses the gradients of any target class\nﬂowing into the ﬁnal convolutional layer to produce a coarse\nlocalization map highlighting the important regions in the\nimage in the class prediction. Since convolutional blocks are\nmissing in ViTs, [57] uses the LN output of the last attention\nblock to calculate the gradients and then [57] reshapes the\nactivation and gradients to 2D spatial images to ﬁt the\nGrad-CAM algorithm.\nDCT-based decomposition of perturbations. In [53], it\nwas shown that construction of the class decision boundary\nis extremely sensitive to small perturbations of the training\nsamples. Moreover, it was shown that CNNs mainly exploit\ndiscriminative features in the low frequencies of MNIST,\nCIFAR-10, and ImageNet-1k datasets. This explains why,\nin some query-based black-box attacks like in [58], using\nlow-frequency perturbations improves the efﬁciency of the\nattack’s query. To show how we take advantage of this\nobservation in our investigation, we consider the DCT de-\ncomposition of the perturbations for non norm-constrained 5\nattacks, like CW attacks, and for norm-constrained attacks,\nlike FGSM and PGD-based attacks as shown in Figure\n2 and Figure 3, respectively. For ResNet, it is very clear\nthat the perturbations’ discriminative features are more\ncentered around low frequency components. While, for ViTs,\nthe perturbations’ discriminative features are spread in all\nfrequency spectrum region. We conclude that, the wider the\nspread of perturbations’ discriminative features the more\nrobust the model against the attacks, because the adversarial\nattack algorithms have to affect a wider range of frequency\nspectrum.\nPSNR. It is the most widely used metric because it is\nsimple and mathematically convenient for optimization.\n4. https://github.com/jeonsworld/ViT-pytorch\n5. In [53], to measure perturbation distance to the decision boundary,\nthey found that non norm-constrained attacks are more suitable for the\nstudy. In our experiments, we show that, for ResNet, the perturbations’\ndiscriminative features are more centered around low frequency in non\nnorm-constrained attacks.\nIEEE TRANSACTIONS ON XXXX 6\nClean Image Perturbation from ViT-B-16 Perturbation from ResNet50\nViT-B-16 :: CW- Linf\n−60\n−40\n−20\n0\n20\nEnergy (Log Scale)\nResNet50 :: CW- Linf\n−60\n−40\n−20\n0\n20\nEnergy (Log Scale)\nFig. 2: The perturbation (top), generated using CW-L∞attack with\nViT-B-16 (middle) and ResNet50(left), and the corresponding DCT-based\nspectral decomposition heatmap. Perturbation is scaled from [-1, 1] to\n[0, 255].\nClean Image Perturbation from T2T-ViT-14 Perturbation from ResNet50\nT2T-ViT-14 :: PGD- inf ϵ= 4/255\n−20\n−10\n0\n10\n20\n30\nEnergy (Log Scale)\nResNet50 :: PGD- inf ϵ= 4/255\n−20\n−10\n0\n10\n20\n30\nEnergy (Log Scale)\nFig. 3: The perturbation (top), generated using PGD-L∞ϵ = 4/255\nattack with T2T-ViT-14 (middle) and ResNet50(left), and the correspond-\ning DCT-based spectral decomposition heatmap. Perturbation is scaled\nfrom [-1, 1] to [0, 255].\nUnfortunately, PSNR doesn’t correlate well with human\nperception. It is calculated as:\nPSNR = 10log10\n(MAX2\nMSE\n)\nMSE = 1\nN\nm∑\ni=1\nn∑\nj=1\n[x(i,j) −x′(i,j)]2,\n(3)\nwhere MAX is the maximum density value, 28 −1 in\n8-bit images, MSE is the mean squared error, mand nare\nthe image width and height, N = mn, and PSNR ∈[0,∞[.\nThe higher the PSNR the better the quality and PSNR →∞\nmeans that the two images are identical.\nSSIM [55]. It is a full reference image quality metric that\nrelies on the fact that the human visual system (HVS) highly\ntends to extract structural information from the image. SSIM\nextracts information that is related to the luminance ( l), the\ncontrast (c), and the structure ( s) from two images, here x\nand x′, and measures the similarity between them as:\nSSIM(x,x′) =l(x,x′)c(x,x′)s(x,x′)\n= (2µxµx′ + c1)(2σxx′ + c2)\n(µ2x + µ2\nx′ + c1)(σ2x + σ2\nx′ + c2), (4)\nwhere µx is the average of x, µx′ is the average of x′,\nσx is the variance of x, σx′ is the variance of x′, σxx′ is the\ncovariance of xand x′, and c1 and c2 are hyperparameter\nto avoid instability when denominator is close to zero. The\nrange of SSIM is [0,1], where SSIM = 1means that the two\nimages are identical. In our experiments, we assume that the\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nMost Apparent Distortion (MAD) ( ↓)\n88\n90\n92\n94\n96\n98\n100ASR(%) of AEs ( ↓)\nViT\nCNN\nHybrid-ViT\nTarget Models\nViT-S-16\nViT-B-16\nViT-L-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nFig. 4: The attack success rate (ASR) of target models, on 1000 images\nfrom ImageNet-1k against AutoAttack, in average ϵ = {1,2,4}/255,\nand the most apparent distortion (MAD)\ntarget model Mis more robust than other models if the AE\nthat is generated using the Mhas lower SSIM score than\nother models because the adversarial attack algorithm has\nto generate AE with higher perturbation that inﬂuences the\nstructure of the AEs.\nMAD [56]. Adversary usually generates AE that are\nimperceptible to human as much as they can by adding\nperturbations that are near threshold distortions. On the\nother hand, MAD attempts to explicitly model two strategies\nemployed by the HVS. The ﬁrst one is a detection-based\nstrategy for high-quality images containing near threshold\ndistortions6 that are typical situation for AEs with low\nperturbation. The second one is an appearance-based strategy\nfor low-quality images containing clearly suprathreshold\ndistortions, that are typical situation for AEs with high\nperturbation. As stated in [56], local luminance and contrast\nmasking are used to estimate detection-based perceived\ndistortion in high-quality images, whereas changes in the\nlocal statistics of spatial-frequency components are used\nto estimate appearance-based perceived distortion in low-\nquality images. Moreover, it was shown in [54] that MAD\nhas better correlation to subjective scores for AEs. The range\nof MAD score is [0,∞] where MAD = 0is close to the clean\nimage. In our experiments, we assume, as shown in Figure 4,\nthat the target model Mis more robust than other models if\nthe AE that is generated using the Mhas higher MAD score\nthan other models because the adversarial attack algorithm\ndon’t be able to prevent the perception of the perturbation.\n4 R EVEAL OF THE ROBUSTNESS ATTRIBUTES\n4.1 General observation\nIn Figure 5 and Figure 7, for instance, the ﬁrst row shows the\nAEs, while the top row of Figures 5b and 7b, and Figures 5d,\n7d represent the Grad-CAM and the attention maps of the\nclean(top) and AEs imags, respectively. It is clear that ViTs\nhave the capability to track the global features of the input\nimage while CNNs track local and centered features. Figures\n5a and 7a represent the perturbations of the generated AEs\n6. Some image quality assessment methods assume that there is a\ndistortion level at which the distortions start be visible to the observers.\nWhen there is a distortion in an image and is not perceptible, it is\ncalled near threshold distortion. When the distortion is visible with high\nmagnitude in the image, it is called suprathreshold distortion.\nIEEE TRANSACTIONS ON XXXX 7\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\n(b)\n(c)\n(d)\nFig. 5: JSMA attack: The\nﬁrst row shows the clean\nsample and the AEs. The\nclean image is correctly\nclassiﬁed by tested models\nand all AEs are success-\nful attacks. (a) The pertur-\nbation (top) and the corre-\nsponding DCT-based spec-\ntral decomposition heatmap.\nPerturbation is shown in\nblack and white colors only.\n(b) Grad-CAM of the clean\n(top) and AE samples. (c)\nFeature map difference be-\ntween clean and AE feature\nmaps that are computed af-\nter the ﬁrst basic block of\nthe model, attention block\nfor ViTs and convolutional\nlayer for CNNs. (d) The at-\ntention map from last atten-\ntion block for clean (top) and\nAE samples.\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n1\n2\n3\n4\n5L0↓\nL0 SSIM L∞ MAD\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\nSSIM↑\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\nL∞↓\n40\n50\n60\n70\n80\n90\nMAD↓\nJSMA\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n100.00 4.00 18.00 12.00 12.00 8.00 39.00\n100.00 0.00 12.00 13.00 11.00 11.00 35.00\n100.00 3.00 11.00 14.00 12.00 8.00 31.00\n100.00 0.00 2.00 5.00 5.00 10.00 15.00\n100.00 1.00 11.00 10.00 3.00 0.00 18.00\n100.00 0.00 10.00 9.00 3.00 0.00 14.00\n100.00 3.00 16.00 9.00 6.00 0.00 17.00\n100.00 1.00 21.00 13.00 8.00 8.00 29.00\n100.00 5.00 19.00 19.00 9.00 5.00 34.00\n100.00 30.00 60.00 60.00 43.00 65.00 79.00\nJSMA\n0\n20\n40\n60\n80\n100\nASR(%)/top-1 error\n (b)\nFig. 6: JSMA attack:(a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs on 100 images\nfrom imagenet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression. CR: cropping\nand re-scaling. CCP: color channel perturbations.\nfor each model. It is clear that the perturbations highly\ntarget the main object of the image, the goose or the dog\nfor both CNNs and vanilla ViTs. Moreover, the effect of\nthe perturbation can be noticed on the Grad-CAM. The\nimpacted regions of the attention and Grad-CAM maps\nspan to regions that are not related to the main object of\nthe image. On the other hand, model’s feature maps were\nknown to be affected by the perturbations and propagate\nwhen the model goes deeper. As shown in [2], CNNs and\nhybrid-ViTs tend to better learn low-level features. Figures\n5c, and 7c show the difference of clean and AE feature maps\nafter the ﬁrst convolutional layer for CNNs and after the\nﬁrst attention block for ViTs. As feature maps show, the\nperturbation impacted learned features, especially the low-\nlevel features.\n4.2 Vanilla ViTs are more robust againstL0-based at-\ntacks\nL0-based attacks rely on changing few image pixels in order\nto fool the neural networks. Finding these few pixels is\nchallenging and computationally consuming process since\nthe attack algorithms need to search over the image space to\nidentify these pixels. In this work we consider the white-box\nJSMA attack [38], and generate 100 AEs from ImageNet-1k\nvalidation images and a sample is shown in the ﬁrst row\nIEEE TRANSACTIONS ON XXXX 8\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\n(b)\n(c)\n(d)\nFig. 7: PGD-L1 ϵ= 400at-\ntack: The ﬁrst row shows the\nclean sample and the AEs.\nThe clean image is correctly\nclassiﬁed by tested models\nand all AEs are successful\nattacks. (a) The perturbation\n(top) and the corresponding\nDCT-based spectral decom-\nposition heatmap. Perturba-\ntion is scaled from [-1, 1]\nto [0, 255]. (b) Grad-CAM\nof the clean (top) and AE\nsamples. (c) Feature map dif-\nference between clean and\nAE feature maps that are\ncomputed after the ﬁrst ba-\nsic block of the model, atten-\ntion block for ViTs and con-\nvolutional layer for CNNs.\n(d) The attention map from\nlast attention block for clean\n(top) and AE samples.\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n50\n52\n54\n56\n58PSNR↑\nPSNR SSIM L∞ MAD\n0.9965\n0.9970\n0.9975\n0.9980\n0.9985\n0.9990\n0.9995\nSSIM↑\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\nL∞↓\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMAD↓\nPGD-L1 ϵ= 400\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n92.20 73.80 30.90 36.80 59.90 79.50 66.70\n92.70 77.40 23.40 35.10 57.80 75.40 46.60\n87.20 72.00 20.20 33.90 51.00 67.40 38.90\n74.70 30.80 4.90 6.70 11.80 43.10 20.00\n92.60 26.50 16.80 13.50 12.00 45.90 39.10\n86.70 22.70 12.00 10.80 10.30 38.10 28.80\n85.80 43.90 18.10 18.10 17.90 51.40 47.60\n89.10 34.40 23.80 18.90 15.50 63.90 54.10\n88.70 35.00 26.40 21.10 14.30 63.00 50.40\n99.20 74.00 48.00 46.50 52.10 97.30 89.20\nPGD-L1 ϵ= 400\n20\n40\n60\n80\nASR(%)/top-1 error\n (b)\nFig. 8: PGD-L1 ϵ= 400attack: (a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs on 1000\nimages from ImageNet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression. CR:\ncropping and re-scaling. CCP: color channel perturbations.\nof Figure 5. The results are investigated and the following\nobservations are noted.\nJSMA identiﬁes input features that signiﬁcantly impact\nthe classiﬁcation output. In this experiment, JSMA achieves\n100% ASR for all tested models, see the ﬁrst column of Figure\n6b. Hence, to study the robustness of a model against L0-\nbased attacks, we analyzed 1) the perturbation DCT-based\nspectral decomposition [53], see Figure 5a, that shows that\nViTs have a wider spread energy spectrum which conﬁrms\na) ViTs are less sensitive to low-level features and exploit\nthe global context of the image, and b) wider frequency\nspectrum has to be affected to generate AE for ViT models\nand its variants, 2) the visual quality measures of AEs in\nFigure 6a show that JSMA, for vanilla ViTs, targeted less\nthan 1% of the pixels ( L0) and has to increase the densities\n(L∞) of the input pixels to more than 0.8, in order to fool\nthe models, which makes vanilla ViTs more robust against\nL0-based attacks than hybrid-ViTs.\nOn the other hand, in CNN, a higher number of pixels\nare targeted with a high change in pixels intensities, which\nclearly affect the structure of the AE as SSIM metric clearly\nshows. While, in hybrid-ViT models, less number of pixels\nwith low pixel intensity change is enough to generate an\nAE with high quality relative to human perception as MAD\nmeasure shows. Moreover, a narrower spread of spectrum\nenergy has been noticed on the DCT-based decomposition,\nhence, bringing convolutional layers into ViTs doesn’t in-\ncrease robustness under the L0-based attacks.\nIEEE TRANSACTIONS ON XXXX 9\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\nFig. 9: CW-L2 attack: The ﬁrst row\nshows the clean sample and the AEs.\nThe clean image is correctly classi-\nﬁed by tested models and all AEs\nare successful attacks. (a) The pertur-\nbation (top) and the corresponding\nDCT-based spectral decomposition\nheatmap. Perturbation is scaled from\n[-1, 1] to [0, 255].\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n0.3\n0.4\n0.5\n0.6\n0.7\nL2↓\nL2 SSIM L∞ MAD\n0.9988\n0.9990\n0.9992\n0.9994\n0.9996\n0.9998\nSSIM↑\n0.0125\n0.0150\n0.0175\n0.0200\n0.0225\n0.0250\n0.0275\n0.0300\n0.0325\nL∞↓\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMAD↓\nCW-L2\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n99.70 8.70 22.40 14.20 21.30 3.50 34.50\n98.10 9.30 15.00 10.30 20.50 4.60 22.60\n95.30 11.70 12.90 9.60 19.70 5.30 22.10\n99.90 1.20 3.40 2.60 6.10 0.80 34.50\n99.50 2.60 14.60 9.10 7.40 1.10 22.60\n98.80 1.60 10.90 7.50 6.30 1.20 22.10\n99.40 4.40 15.00 10.40 9.20 0.80 9.70\n99.50 2.90 21.00 14.40 6.70 1.00 14.60\n99.80 4.20 24.30 17.10 9.60 1.30 13.20\n99.90 12.30 42.10 36.30 21.20 3.60 16.90\nCW-L2\n20\n40\n60\n80\nASR(%)/top-1 error\n (b)\n.\nFig. 10: CW-L2 attack: (a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs on 1000 images\nfrom ImageNet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression. CR: cropping\nand re-scaling. CCP: color channel perturbations.\n4.3 T2T-ViT-24 and TNT-S-16 are more robust against\nL1-based attacks\nL1-based attacks minimize the perturbation δ, where δ =\n||x−x′||1, δ≤ϵ, and f(x) ̸= f(x′), where f(.) is a prediction\nfunction. In this work we consider the white-box PGD-\nL1 attack [27], and generate 1000 AEs from ImageNet-1k\nvalidation images and a sample is shown in the ﬁrst row\nof Figure 7. In this experiment, we set ϵto 400. As shown\nin the ﬁrst column of Figure 8b, PGD-L1 achieves less ASR\non ViT-L-16, T2T-ViT-24 and TNT-S-16 models. As shown in\nFigure 7a, perturbations that are generated using T2T-ViT-\n24 and TNT-S-16 models have wider frequency spectrum\nspread than ViT-Res-16 and ViT-L-16 models, which makes\nthem more robust than other models. While perturbations\nthat are generated using ResNets have a wider spread of\nfrequency spectrum than ViT-S/B-16 and T2T-ViT-14. From\nthe visual quality assessment point of view, see Figure 8a, 1)\nas SSIM scores show, perturbations that are generated using\nT2T-ViT-24 and TNT-S-16 models have more inﬂuence to\nalter the adversarial image structure than the structure of\nAEs generated by other models, 2) as PSNR and MAD scores\nshow, perturbations that are generated using ViT variants,\nin general and speciﬁcally T2T-ViT-24 and TNT-S-16, have\nhigher L∞than other tested models which yields to have\nlower image quality of AE. VGG16 has the highest ASR\nwhich makes it less robust than other models. One reason\nfor that is that VGG16 has less accuracy performance than\nother models which makes the learned features not robust.\nIn this experiment, ViT-Res-16 is excluded from the analysis\nsince the image size is different.\n4.4 Vanilla ViTs are more robust under CW-L2, while\nhybrid-ViTs are more robust under PGD-L2 attacks\nL2-based attacks minimize the perturbation δ, where δ =\n||x−x′||2, δ≤ϵ, and f(x) ̸= f(x′). In this work we consider\nthe white-box CW-L2 [31] and PGD-L2 attacks [27], and\ngenerate 1000 AEs from ImageNet-1k validation images and\na sample is shown in the ﬁrst row of Figure 9.\n4.4.1 Robustness under CW-L2 attacks\nAs shown in the the ﬁrst column of Figure 10b, CW-L2 attack\nachieves comparable ASR on all tested models except for\nViT-L-16 which achieves less ASR. Compared to ResNet,\nthe DCT decomposition of the perturbation shows the huge\nwide spread of energy spectrum for vanilla ViTs, especially\nViT-L-16. Moreover, the algorithm of CW-L2 has to increase\nL2 and L∞distortions, see Figure 10a, for ViTs in order to\ngenerate successful attacks. Hence, SSIM and MAD show\nless quality score for AEs that are generated using vanilla\nViTs. ResNet shows robustness over T2T-ViT-14 since the\nperturbations that are generated using ResNet have a wider\nspread of frequency spectrum than T2T-ViT-14 but not wider\nthan T2T-ViT-24 and TNT-S-16. The visual quality scores,\ngiven in Figure 10a, conﬁrm that ResNet is more robust than\nT2T-ViT-14 but less robust than T2T-ViT-24 and TNT-S-16.\nIEEE TRANSACTIONS ON XXXX 10\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\nFig. 11: PGD-L2 ϵ= 2attack: The\nﬁrst row shows the clean sample\nand the AEs. The clean image is\ncorrectly classiﬁed by tested models\nand all AEs are successful attacks.\n(a) The perturbation (top) and the\ncorresponding DCT-based spectral\ndecomposition heatmap. Perturba-\ntion is scaled from [-1, 1] to [0, 255].\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n50\n51\n52\n53\n54\n55PSNR↑\nPSNR SSIM L∞ MAD\n0.9965\n0.9970\n0.9975\n0.9980\n0.9985\nSSIM↑\n0.035\n0.040\n0.045\n0.050\n0.055\nL∞↓\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nMAD↓\nPGD-L2 ϵ= 2\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n90.00 73.70 31.50 38.30 60.60 79.10 66.80\n94.10 79.50 25.10 37.50 60.70 79.50 47.00\n88.40 75.80 20.70 36.70 54.40 71.10 39.90\n94.10 60.90 5.80 10.40 20.60 72.10 31.10\n87.40 25.00 16.60 13.00 11.50 43.50 36.80\n79.60 20.90 11.70 10.30 9.10 34.60 27.10\n79.90 40.60 18.10 17.70 17.10 48.00 45.20\n90.70 37.80 24.10 19.60 17.00 69.60 58.20\n91.10 37.90 26.90 21.00 15.40 67.80 53.90\n98.90 78.10 48.10 47.10 54.00 97.80 90.90\nPGD-L2 ϵ= 2\n20\n40\n60\n80\nASR(%)/top-1 error\n (b)\nFig. 12: PGD-L2 ϵ= 2attack: (a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs on 1000\nimages from imagenet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression. CR:\ncropping and re-scaling. CCP: color channel perturbations.\n4.4.2 Robustness under PGD-L2 attacks\nFigure 11 shows an example to study the target models’\nrobustness against PGD-L2 attack with ϵ= 2. From the ﬁrst\ncolumn of Figure 12b, we concluded that the hybrid- ViT are\nmore robust than other models against PGD-L2 attacks. The\nASR of the hybrid- ViTs is less than the ASR of the other\ntarget models. Moreover, when we look at Figure 11a, we can\nsee that hybrid-ViTs have a wider spread of discriminative\nfeatures on all frequencies than other models. Moreover, the\nViT-S/B/L-16 models show better robustness over ResNet.\nWhen looking at Figure 12a, we can see that ViT variants\nhave higher L∞score, especially the ViT-S-16, than ResNet\nwhich affects the AE structure and visual quality as SSIM\nand MAD scores show. VGG16 has the highest ASR and\nhigh distortion which makes it less robust. One reason for\nthat is that VGG16 has less accuracy performance than other\nmodels which makes the learned features not robust. In this\nexperiment, ViT-Res-16 is excluded from the analysis since\nthe image size is different.\nHence, we can conclude that neither larger model’s\narchitecture, like ViT-L, nor bringing convolutional modules\nfor tokenization in ViTs, like TNT, will necessarily enhance\nthe robustness.\n4.5 Hybrid-ViTs and small ViT are matters under some\nL∞-based attacks.\nL∞-based attacks minimize the perturbation δ, where δ =\n||x−x′||∞, δ≤ϵ, and f(x) ̸= f(x′). In this work we consider\nthe white-box FGSM-L∞[26], CW-L∞[31], PGD-L∞ [27],\nand UAP [39] attacks. Moreover, two black box attacks are\nconsidered; RayS [41] and SA [40] attacks and the AA-L∞\n[30] hybrid attack is considered as well.\n4.5.1 Robustness under PGD-L∞, AA, and UAP attacks\nWe generate 1000 AEs from ImageNet-1k validation images\nand a sample is shown in the ﬁrst row of Figure 13. PGD-L∞\nattack achieves less ASR on ViT-S-16 than other tested mod-\nels. While ResNets have comparable robustness to ViT-L-16,\nT2T-ViT-24, and TNT-S-16, and have better robustness over\nViT-B-16, ViT-Res-16, and T2T-ViT-14. The perturbations that\nare generated using ViT-S-16 have the following properties:\n1) more spread of energy spectrum as DCT decomposition\nshows in Figure 13a , 2) lower visual quality as PSNR, SSIM\nscores show in Figure 14a, and 3) higher L1 score. L1 score\nfor ViT-Res-16 is not considered in the analysis since the\nimage size is different. Similar to L1-based attack, VGG16\nhas the highest ASR which makes it less robust. VGG16 has\nthe highest ASR and high distortion which makes it less\nrobust than other target tested models.\nUnder the AA and UAP attacks, we noticed that ViT-S-16\nhas better robustness than other target models, as Figure 4\nIEEE TRANSACTIONS ON XXXX 11\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\nFig. 13: PGD-L∞ ϵ = 1 /255 at-\ntack: First row shows the clean sam-\nple and the AEs. The clean image is\ncorrectly classiﬁed by tested models\nand all AEs are successful attacks.\n(a) The perturbation (top) and the\ncorresponding DCT-based spectral\ndecomposition heatmap. Perturba-\ntion is scaled from [-1, 1] to [0, 255].\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n50.0\n50.5\n51.0\n51.5\n52.0\n52.5\n53.0\nPSNR↑\nPSNR SSIM L1 MAD\n0.9968\n0.9970\n0.9972\n0.9974\n0.9976\n0.9978\n0.9980\n0.9982\n0.9984\nSSIM↑\n300\n400\n500\n600\n700\n800\n900L1↓\n0.0000\n0.0002\n0.0004\n0.0006\n0.0008\n0.0010\n0.0012\nMAD↓\nPGD-Linf ϵ= 1/255\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n72.00 50.60 26.50 27.50 18.20 50.60 50.70\n85.10 58.40 21.00 26.60 17.30 54.30 34.20\n78.40 59.60 17.80 26.70 15.30 48.70 29.40\n92.30 53.90 5.10 9.10 4.90 58.00 25.20\n90.00 23.30 17.50 13.10 7.10 37.30 36.20\n79.50 19.70 12.30 10.90 5.40 30.80 25.40\n81.20 37.40 18.20 17.10 8.70 41.40 44.30\n79.10 21.70 22.70 17.90 6.60 38.40 39.70\n79.60 24.30 25.80 19.70 9.50 43.30 37.80\n98.00 54.60 46.10 42.20 21.70 87.60 78.00\nPGD-Linf ϵ= 1/255\n20\n40\n60\n80\nASR(%)/top-1 error\n.\n(b)\nFig. 14: PGD-L∞ϵ= 1/255 attack: (a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs on\n1000 images from imagenet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression. CR:\ncropping and re-scaling. CCP: color channel perturbations.\nshows for AA, but due to space limitation, we only visualized\nPGD-L∞attacks.\n4.5.2 Robustness under FGSM-L∞attacks\nFigure 15 shows an example to study the target models’\nrobustness against FGSM-L∞attack. Figure 16a, 18a shows\nthe visual quality scores of the AEs that are generated using\nFGSM-L∞attack. While Figure 16b shows the ASR of the\nAEs and the top-1 error of the pre-processed AEs that are\ngenerated using FGSM-L∞attack.\nFigure 16b shows that hybrid-ViTs have lower ASR than\nother models. Vanilla ViT-L-16 shows more robustness over\nResNet and ReNeXt, while ResNet shows robustness over\nvanilla ViT-S/B-16. The wider spread of the energy spectrum\nof the DCT decomposition conﬁrms the robustness of hybrid-\nViTs over other models, as shown in Figure 15a. Moreover,\nSSIM scores, illustrated in Figure 16a, show that hybrid-ViT\nhas lower score than other models, except for VGG16. VGG16\nhas the lowest robustness since it has the highest ASR and\nhigh distortion as SSIM score indicates. In FGSM attack, it is\nhard to use PSNR and MAD to judge the robustness of the\ntarget models since the ||δ||2 of all generated AEs are equal.\n4.5.3 Robustness under CW-L∞attacks\nFigure 17 shows an example to study the target models’\nrobustness against CW-L∞ attack. Figure 18a shows the\nvisual quality scores of the AEs that are generated using CW-\nL∞attack. While Figure 18b shows the ASR of the AEs and\nthe top-1 error of the pre-processed AEs that are generated\nusing CW-L∞attack.\nFor CW-L∞, Figure 18b shows that the ASR of the ViT-\nB/L-16 is lower than the ASR of other models. On the other\nhand, the DCT decomposition, see Figure 17a, shows that the\nspread of the discriminative features is wider on ViT-B/L-16\nthan other models. Moreover, it is shown that T2T-ViT-24\nand TNT-S-16 have wider spread of DCT decomposition\nthan ResNet. Finally, from Figure 18a, we can conclude that\nCW-L∞generates AEs with higher perturbations for ViT-\nB/L-16 and for T2T-ViT-24 models than other model which\nclearly affected the image structure and the visual perception\nas SSIM and MAD scores indicate.\n4.5.4 Robustness under RayS attacks\nFigure 19 shows an example to study the target models’\nrobustness against RayS attack. Figure 20a shows the visual\nquality scores of the AEs that are generated using RayS\nattack. While Figure 20b shows the ASR of the AEs and the\ntop-1 error of the pre-processed AEs that are generated using\nRayS attack.\nFor RayS attack, by looking at Figure 20b, Figure 19a,\nand Figure 20a, we can conclude that hybrid- ViTs are more\nrobust than other target models. The ﬁgures show 1) T2T-ViT-\n24 has lower ASR than other models. 2) hybrid- ViTs have\nwider energy spectrum spread than other models. 3) RayS\ngenerates AEs with higher perturbations for hybrid-ViTs that\nare perceptible to human.\nIEEE TRANSACTIONS ON XXXX 12\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\nFig. 15: FGSM-L∞ϵ = 1/255 at-\ntack: The ﬁrst row shows the clean\nsample and the AEs. The clean im-\nage is correctly classiﬁed by tested\nmodels and all AEs are successful\nattacks. (a) The perturbation (top)\nand the corresponding DCT-based\nspectral decomposition heatmap.\nPerturbation is scaled from [-1, 1]\nto [0, 255].\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n−0.002\n−0.001\n0.000\n0.001\n0.002\n0.003\n0.004\nPSNR↑\n+4.8200000000 × 101\nPSNR SSIM MAD\n0.9938\n0.9940\n0.9942\n0.9944\n0.9946\n0.9948\nSSIM↑\n0.00\n0.02\n0.04\n0.06\n0.08\nMAD↓\nFGSM ϵ= 1/255\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n47.80 33.60 26.10 23.50 15.90 33.20 42.40\n50.00 35.50 20.90 23.30 14.00 34.50 28.30\n39.80 30.00 17.10 21.50 12.00 27.70 25.00\n52.90 28.50 5.70 9.50 4.80 32.30 21.40\n35.00 16.30 16.60 12.80 6.50 20.50 27.30\n28.30 12.80 12.10 9.70 4.40 16.60 19.30\n38.50 22.60 17.10 15.60 7.20 22.80 28.00\n43.80 17.00 22.90 18.60 6.60 21.80 30.10\n46.10 18.60 25.70 19.50 8.50 26.60 29.60\n89.00 50.60 45.90 43.40 21.70 75.10 70.80\nFGSM ϵ= 1/255\n10\n20\n30\n40\n50\n60\n70\n80\nASR(%)/top-1 error\n (b)\nFig. 16: FGSM-L∞ϵ= 1/255 attack: (a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs\non 1000 images from imagenet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression.\nCR: cropping and re-scaling. CCP: color channel perturbations.\n4.6 Transfer attacks: increasing the number of atten-\ntion blocks reduces the transferability\nRecent studies in [17], [18], [19] showed that there is low\ntransferability between different models’ families. In this\nwork we conﬁrm that and the results are shown in Figure\n21a. Here, we show two new observations. The ﬁrst one\nis that the transferability, within the same model family, is\nbecoming lower when the model is becoming larger. Hence,\nadding more attention blocks to ViT variant models reduces\nthe effect of the transferability property, as shown in Figure\n21a. The second 7 observation is that the black box based\nAEs, RayS and SA, that are generated using ViT variants are\nmore transferable to CNNs while the black box based AEs\nthat are generated using CNN are much less transferable to\nViT variants. As shown in Figure 21b, we notice that when\nCNNs serve as target models, last three columns, the ASR is\nhigher than those of when CNNs serve as the source model,\nlast three rows. According to the DCT decomposition of SA-\nbased AEs, see Figure 22, one possible explanation to this\ncase is that the generated perturbations highly affect the local\nfeatures than global features and since ViTs are less sensitive\nto local features making perturbation effect less transferable.\n7. Using source model to generate black box AEs for a different target\nmodel is not popular in real-world but, here the goal is to note the\nobservation.\n4.7 Color channel perturbations (CCP) is an attack and\ndefense!\nThe CCP attack is based on the color property of the image.\nIt uses the original color channels, Red R, Green Gand Blue\nB of the image input xto generate the AE (x′). The AE is\ncomposed of new transformed color channels, R′, G′, and\nB′of the transformed image ( x′). The transformed channels\nare calculated as follows:\nR′= s\n(\nαrR+ αgG+ αbB\n3\n)\n+ b,\nG′= s\n(\nβrR+ βgG+ βbB\n3\n)\n+ b,\nB′= s\n(\nγrR+ γgG+ γbB\n3\n)\n+ b,\n(5)\nwhere sis a scale factor hyperparameter, bis a bias hyper-\nparameter, {α, β, γ} ∈[0,1], and {R′, G′, B′} ∈R. The scale\nsand bias bare used to adjust the visual appearance of the\ngenerated AE. Figure 23 shows examples for CCP attack.\nImages might be vulnerable to natural perturbations like\ncolor brightness change. Table 3 shows the ASR of the target\nmodels against the CCP attack. Although this attack doesn’t\nachieve high ASR when tested on neural network models, it\ncan reveal robustness of these models. The results in Table\n3 show that hybrid-ViTs have the highest robustness, while\nIEEE TRANSACTIONS ON XXXX 13\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\nFig. 17: CW-L∞ attack: The ﬁrst\nrow shows the clean sample and\nthe AEs. The clean image is cor-\nrectly classiﬁed by tested models\nand all AEs are successful attacks.\n(a) The perturbation (top) and the\ncorresponding DCT-based spectral\ndecomposition heatmap. Perturba-\ntion is scaled from [-1, 1] to [0, 255].\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nL2↓\nL2 SSIM L1 MAD\n0.988\n0.990\n0.992\n0.994\n0.996\n0.998\nSSIM↑\n100\n200\n300\n400\n500\n600\n700\n800L1↓\n0\n1\n2\n3\n4\n5\nMAD↓\nCW-Linf\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n100.00 21.20 23.00 15.80 29.30 23.20 40.10\n95.80 29.30 16.90 15.10 32.30 24.70 27.50\n94.80 33.00 15.40 15.40 36.10 26.10 28.10\n99.40 10.60 3.70 3.10 7.50 14.20 14.70\n98.10 4.90 15.10 10.00 8.20 7.90 21.40\n98.70 4.80 11.20 8.60 7.40 8.80 19.90\n98.80 9.80 15.30 11.20 11.00 7.20 27.10\n100.00 6.20 21.40 15.10 9.20 11.10 28.60\n100.00 6.70 24.30 17.80 10.10 6.00 25.20\n100.00 14.50 42.20 36.80 21.80 11.70 44.40\nCW-Linf\n20\n40\n60\n80\n100\nASR(%)/top-1 error\n (b)\nFig. 18: CW-L∞attack: (a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs on 1000 images\nfrom imagenet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression. CR: cropping\nand re-scaling. CCP: color channel perturbations.\nResNets and ViT-B/L-16 have comparable robustness. One\nexplanation to that is that CCP limits CNNs’ and vanilla\nViTs’ capabilities to extract the local and global features,\nrespectively. Hence, enhancing the tokenization process of\nthe vanilla ViTs as in hybrid- ViTs has an added value\nto model’s robustness. Surprisingly, we found that when\napplying CCP attack on AEs, CCP is able to project the AEs\nback to its original manifold. Last columns of Figures 6b,\n8b, 10b, and, 14b show the ASR after considering CCP as\na preprocess method for the AEs. Although removing the\neffect of perturbations depends on noise amount, hybrid-ViTs\nshow better performance on predicting the original class due\nthe presence of the tokenization process enhancements in\nhybrid-ViTs. Future investigations are highly recommended\nto study this phenomenon since brightness change is a\ncommon process in many real-world applications.\n4.8 Vanilla ViTs are not responding to preprocessing\ndefenses that mainly reduce the high frequency compo-\nnents.\nPreprocessing is one of the defense methods that is applied to\nthe model’s input to remove the effect of perturbations that\nare added to the input image. In this experiment, we apply\nﬁve preprocessing methods that are used in the literature\nand brieﬂy mentioned in Section 3.3. Samples of top-1 error\nafter perprocessing of some attacks are shown in Figures 6b,\n8b, 10b, 12b, 14b, 16b, 18b, and 20b, while Figure 1 shows\nthe top-1 error AA, in average. For L0-based and CW-L2\nattacks, SS and cropping and re-scaling ( CR), as expected,\nhave the capability to remove the perturbations effect and\nto project AEs back to input manifold, see Figure 6b, by re-\npositioning the perturbation structure. The limited success of\nthe preprocessing against L0-based AEs that are generated\nusing VGG16 is due the large number of the impacted pixels.\nFor other attacks, NLM and total variation minimization\n(TVM) show better performance on the preprocessing of AEs.\nThat’s because these two denoising methods try to restore the\noriginal image while preserving the global image structure\nand contours. While other preprocessing methods including\nSS, JPEG, and CR, are highly impacting the high frequency\ncomponents of the AEs, hence ResNets show better top-1\nerror over vanilla ViTs. Hybrid-ViTs have lower top-1 error\nthan vanilla ViTs and CNNs due to its power of identifying\nglobal and local features. Figure 24 shows examples of the\npreprocessing process for PGD-L∞attacks.\n4.9 Robustness under EOT attack\nEOT is a framework that constructs AEs that remain adver-\nsarial over a chosen transformation distribution T, i.e. the\npreprocessing defenses. When processing defenses are used\nthe stochastic gradients issue of the classiﬁer f(.) arises and\nhence, to have successful attack, it is necessary to estimate the\ngradient over the expected transformation to the input t(x),\nwhere, t(.) is the transformation function. EOT optimizes the\nIEEE TRANSACTIONS ON XXXX 14\nTABLE 3: The ASR of the target models against color channel perturbations (CCP).\nViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\nASR(%) 22.6 15.2 12.5 5.4 10.1 7.1 8.1 11.9 14.9 32.6\nTABLE 4: The top-1 error(%) of the preprocessing defense methods that are applied to PGD-L∞ϵ = 4/255 under the\nEOT. 1000 images from imagenet-1k are used. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation\nminimization. JPEG: JPEG compression. CR: cropping and re-scaling. EOT: expectation over transformation.\nModel No defense SS NLM TVM JPEG CR\nEOT % % ! % ! % ! % ! % !\nViT-S-16 99.9 96.3 98 43.8 77.7 62 91.2 91.8 94.3 97.4 90.3\nViT-B-16 99.5 97.2 97.9 35.9 86.4 58.5 94.4 90.2 94.8 96.4 95.6\nViT-L-16 98.8 96.5 97 33.2 83.9 58.6 91.6 88.2 94 93.7 92\nViT-Res-16 100 96.7 99.6 12.3 89.6 34.1 97.8 57.5 96.9 98.1 98.2\nT2T-ViT-14 99.9 58.1 96.8 22.8 77.1 24.3 87.4 25 77.4 74.7 70\nT2T-ViT-24 99.6 52.3 94.6 18.1 66 21.2 82.1 23.9 70.1 67.4 63.8\nTNT-S-16 99.7 80.7 97.8 27.1 78.8 34.2 91.1 38.4 82.2 83 79.7\nResNet50 98.7 61.4 95.7 26.9 81.3 26.3 89 27.9 85.4 86.8 79.4\nResNet50-32x4d 98.4 56.3 95 29.4 79.8 25.4 87.2 22.5 79.3 81.8 71.9\nVGG16 99.6 94.1 96.5 55 89.2 57.7 90.2 73.8 95.5 99.4 94.5\nexpectation over the transformation t∼T f(t(x)) which can\nbe solved by minimizing the expected perceived distance as\nseen by the classiﬁer Et∼T [d(t(x′),t(x))], where d(.) is the\ndistance function.\nWe consider the distribution of transformations that\nincludes SS, NLM, TVM, JPEG, and CR. We 1) use 1000\nimages from the ImageNet-1k validation set, 2) generate AEs\nusing PGDs-L∞ ϵ = 4/255 and apply the preprocessing\ndefense methods for the generated AEs, 3) use EOT to\nsynthesize AEs that are robust over the given distribution\nT and apply the preprocessing defense methods for the\nsynthesized AEs. Table 4 shows the top-1 error for the target\nmodels against the preprocessed AEs with and without\nconsidering the EOT.\nIt is clear that the EOT kept the input samples as\nadversarial over the tested transformations except for the\ncropping and re-scaling ( CR) transformation. For SS and\nJPEG , the top-1 error is highly increased in T2T-ViT and\nResNets models and the T2T-ViT models show less top-1\nerror. While for NLM, the small ViT, T2T-ViT, and TNT\nmodels have less top-1 error compared to other models. It\nis interesting to notice that, with the use EOT, the ResNet\nmodels have less top-1 error than ViT-B/L models. One\nreason for that is that the transformation under the EOT\nhighly impacts the global structure of the input sample. For\nTVM, the top-1 error of T2T-ViT models is less than other\ntarget models. The top-1 error for using CR under the EOT\nis comparable to the top-1 error without CR. One reason for\nthat is that CR, under any framework, targets restructuring\nthe adversarial sample by re-positioning the input pixels.\nIn general, we conclude that T2T-ViT and TNT models\nshow better robustness than vanilla ViTs and ResNets under\nthe EOT robustness test.\n5 C ONCLUSION\nIn this paper, we have studied the robustness of vision\ntransformers and their variants and compared them with\nCNNs. Our analysis showed 1) either vanilla ViTs or hybrid-\nViTs are more robust than CNNs against Lp-based attacks\nand CCP attacks. We analysed the energy spectrum of DCT\ndecomposition of the perturbations and the different visual\nquality measures. 2) CCP can be used as a preprocessing\ndefense method. 3) Increasing the number of attention blocks\nwill increase the robustness against the transfer attacks but\nnot against white box attacks. 4) Enhancing ViT tokenization\nmight not increase the robustness against the AEs but will\nincrease the robustness under the preprocessed AEs.\nACKNOWLEDGMENTS\nThe project is funded by both Région Bretagne (Brittany\nregion), France, and direction générale de l’armement (DGA).\nREFERENCES\n[1] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2016, pp. 770–778. 1, 4\n[2] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. H. Tay, J. Feng, and\nS. Yan, “Tokens-to-Token ViT: Training vision transformers from\nscratch on imagenet,” CoRR, vol. abs/2101.11986, 2021. 1, 2, 3, 4, 5,\n7\n[3] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” in\nAdvances in Neural Information Processing Systems 30, December 4-9,\n2017, Long Beach, CA, USA, 2017, pp. 5998–6008. 1\n[4] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in 9th International\nConference on Learning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net, 2021. 1, 3, 4, 5\n[5] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting\nunreasonable effectiveness of data in deep learning era,” in IEEE\nInternational Conference on Computer Vision, ICCV 2017, Venice, Italy,\nOctober 22-29, 2017. IEEE Computer Society, 2017, pp. 843–852. 1,\n2\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, inCVPR09,\n2009. 1, 2, 3, 4\nIEEE TRANSACTIONS ON XXXX 15\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\nFig. 19: RayS attack:The ﬁrst row\nshows the clean sample and the\nAEs. The clean image is correctly\nclassiﬁed by tested models and all\nAEs are successful attacks. (a) The\nperturbation (top) and the corre-\nsponding DCT-based spectral de-\ncomposition heatmap. Perturbation\nis scaled from [-1, 1] to [0, 255].\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n26\n28\n30\n32\n34\n36\n38PSNR↑\nPSNR SSIM L∞ MAD\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\nSSIM↑\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nL∞↓\n20\n30\n40\n50\n60\n70\nMAD↓\nRayS\n(a)\nAEs\nSS(AEs)\nNLM(AEs)\nTVM(AEs)\nJPEG(AEs)\nCR(AEs)\nCCP(AEs)\nPreprocess Method\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\n99.80 35.90 31.90 29.80 21.10 6.50 30.30\n99.80 29.80 22.50 22.50 11.20 6.10 19.60\n99.60 30.50 18.60 20.40 10.80 6.40 18.00\n99.40 40.40 13.60 26.00 9.70 4.10 9.70\n97.40 13.40 21.50 18.30 10.20 3.10 15.30\n95.80 13.00 19.10 15.60 10.20 3.10 11.50\n98.80 17.60 22.20 21.00 10.90 2.60 14.10\n99.80 38.10 28.40 31.40 15.80 5.10 17.40\n99.70 27.10 29.40 28.10 14.10 3.50 21.20\n99.90 40.50 45.10 46.80 25.80 10.40 36.10\nRayS\n20\n40\n60\n80\nASR(%)/top-1 error\n (b)\nFig. 20: RayS attack:(a) AEs quality assessment measures. (b) The ASR of the AEs and the top-1 error of the pre-processed AEs on 1000 images\nfrom imagenet-1k. SS: local spatial smoothing. NLM: non-local mean. TVM: total variation minimization. JPEG: JPEG compression. CR: cropping\nand re-scaling. CCP: color channel perturbations.\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nSource Model\n86.60 55.50 47.80 23.00 36.80 31.70 42.70 30.40 38.50 61.60\n46.40 71.70 50.30 23.00 27.60 22.70 26.90 26.10 31.80 58.40\n40.10 51.70 64.20 21.90 22.50 18.60 22.20 27.10 31.50 64.60\n10.60 11.50 10.10 65.20 10.30 8.30 9.70 12.30 14.40 36.20\n20.90 15.40 13.30 14.10 52.90 28.80 23.50 18.30 22.80 39.70\n19.30 16.20 14.40 14.50 30.80 42.80 20.00 15.90 22.10 41.00\n28.40 21.40 18.60 15.40 27.30 22.60 53.20 20.20 23.60 52.00\n20.30 16.50 13.20 17.90 18.90 14.90 17.50 62.60 44.60 61.30\n14.40 10.30 9.40 11.80 13.90 9.50 10.80 24.40 53.30 47.30\n18.70 15.40 10.60 18.00 19.00 12.90 17.70 31.50 36.10 94.00\nFGSM ϵ= 16/255\n10\n20\n30\n40\n50\n60\n70\n80\n90\nASR(%)\n(a)\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nTarget Model\nViT-S-16\nViT-B-16\nViT-L-16\nViT-Res-16\nT2T-ViT-14\nT2T-ViT-24\nTNT-S-16\nResNet50\nResNeXt50\nVGG16\nSource Model\n57.00 13.90 9.30 7.60 4.00 1.90 5.80 28.10 29.80 64.70\n8.30 68.70 13.20 8.80 4.00 1.60 4.50 29.10 30.20 58.30\n9.10 15.10 59.70 8.10 3.80 1.30 3.80 26.70 29.20 52.90\n6.20 10.90 9.00 41.60 3.00 0.70 3.10 19.70 22.00 44.30\n9.30 14.10 10.20 8.20 48.40 2.40 5.20 33.20 36.10 70.40\n9.30 12.80 9.30 8.10 4.60 39.10 4.60 32.10 36.40 70.90\n10.50 14.20 11.10 7.40 4.80 2.00 43.90 30.60 36.10 70.80\n10.00 12.50 9.90 7.00 3.30 2.70 4.30 72.30 37.50 71.30\n10.10 12.80 8.50 8.00 5.00 2.00 5.40 34.80 80.30 68.20\n10.70 12.90 8.60 6.70 5.50 2.30 5.80 35.90 39.00 96.60\nSA-Linf ϵ= 16/255\n20\n40\n60\n80\nASR(%)\n (b)\nFig. 21: ASR of transfer attack using FGSM and SA on 1000 images from ImageNet-1k . The row represents the source model that is used to\ngenerate AE. The column represents the target model. Dark blue column means that the target model is vulnerable to transfer attacks.\nIEEE TRANSACTIONS ON XXXX 16\nClean ViT-S-16 ViT-B-16 ViT-L-16 ViT-Res-16 T2T-ViT-14 T2T-ViT-24 TNT-S-16 ResNet50 ResNeXt50 VGG16\n(a)\nFig. 22: SA-L∞ϵ = 8/255 attack:\nThe ﬁrst row shows the clean sample\nand the AEs. The clean image is\ncorrectly classiﬁed by tested models\nand all AEs are successful attacks.\n(a) The perturbation (top) and the\ncorresponding DCT-based spectral\ndecomposition heatmap. Perturba-\ntion is scaled from [-1, 1] to [0, 255].\nFig. 23: Examples for CCP attack. Images are from ImageNet-\n1k validation set.\nFig. 24: Examples of the defense preprocessing. The top row shows\nthe clean, AE, and the preprocessed AE. The middle row shows the\nGrad-CAM of the clean, AE, and the preprocessed AE. The bottom row\nshows the energy spectrum of the DCT decomposition of AE, and the\npreprocessed AE. Red label means the image is misclassiﬁed.\n[7] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, “Transformer\nin transformer,” CoRR, vol. abs/2103.00112, 2021. 1, 3, 4\n[8] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n“CvT: Introducing convolutions to vision transformers,” CoRR, vol.\nabs/2103.15808, 2021. 1\n[9] K. Simonyan and A. Zisserman, “Very deep convolutional networks\nfor large-scale image recognition,” in 3rd International Conference\non Learning Representations, ICLR 2015, San Diego, CA, USA, May\n7-9, 2015, Conference Track Proceedings, Y. Bengio and Y. LeCun, Eds.,\n2015. 1, 4\n[10] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n“Rethinking the inception architecture for computer vision,” in\nProceedings of the IEEE conference on computer vision and pattern\nrecognition, 2016, pp. 2818–2826. 1\n[11] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang,\nT. Weyand, M. Andreetto, and H. Adam, “MobileNets: Efﬁcient\nconvolutional neural networks for mobile vision applications,”\narXiv preprint arXiv:1704.04861, 2017. 1\n[12] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Good-\nfellow, and R. Fergus, “Intriguing properties of neural networks,”\nin 2nd International Conference on Learning Representations, ICLR 2014,\nBanff, AB, Canada, April 14-16, 2014, Conference Track Proceedings ,\n2014. 1, 2\n[13] N. Carlini and D. Wagner, “Adversarial examples are not easily\ndetected: Bypassing ten detection methods,” in Proceedings of the\n10th ACM Workshop on Artiﬁcial Intelligence and Security , 2017, pp.\n3–14. 1\n[14] A. Ilyas, S. Santurkar, D. Tsipras, L. Engstrom, B. Tran, and\nA. Madry, “Adversarial examples are not bugs, they are features,”\nin Advances in Neural Information Processing Systems , 2019, pp. 125–\n136. 1\n[15] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep\nlearning in computer vision: A survey,” IEEE Access, vol. 6, pp.\n14 410–14 430, 2018. 1, 2\n[16] H. X. Y. M. Hao-Chen, L. D. Deb, H. L. J.-L. T. Anil, and K. Jain,\n“Adversarial attacks and defenses in images, graphs and text: A\nreview,”International Journal of Automation and Computing , vol. 17,\nno. 2, pp. 151–178, 2020. 1, 2\n[17] S. Bhojanapalli, A. Chakrabarti, D. Glasner, D. Li, T. Unterthiner,\nand A. Veit, “Understanding robustness of transformers for image\nclassiﬁcation,” CoRR, vol. abs/2103.14586, 2021. 1, 2, 12\n[18] R. Shao, Z. Shi, J. Yi, P . Chen, and C. Hsieh, “On the adversarial\nrobustness of visual transformers,”CoRR, vol. abs/2103.15670, 2021.\n1, 2, 3, 4, 12\n[19] K. Mahmood, R. Mahmood, and M. van Dijk, “On the robustness\nof vision transformers to adversarial examples,” 2021. 1, 2, 12\n[20] S. Paul and P . Chen, “Vision transformers are robust learners,”\nCoRR, vol. abs/2105.07581, 2021. 1, 2\n[21] X. Yuan, P . He, Q. Zhu, and X. Li, “Adversarial examples: Attacks\nand defenses for deep learning,”IEEE transactions on neural networks\nand learning systems, vol. 30, no. 9, pp. 2805–2824, 2019. 2\n[22] A. Chakraborty, M. Alam, V . Dey, A. Chattopadhyay, and\nD. Mukhopadhyay, “Adversarial attacks and defences: A survey,”\nCoRR, vol. abs/1810.00069, 2018. 2\n[23] A. Aldahdooh, W. Hamidouche, S. A. Fezza, and O. Déforges,\n“Adversarial example detection for DNN models: A review,” CoRR,\nvol. abs/2105.00203, 2021. 2\n[24] F. Alamri, S. Kalkan, and N. Pugeault, “Transformer-encoder de-\ntector module: Using context to improve robustness to adversarial\nattacks on object detection,” in 2020 25th International Conference on\nPattern Recognition (ICPR). IEEE, 2021, pp. 9577–9584. 2\n[25] S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards\nreal-time object detection with region proposal networks,” IEEE\nTrans. Pattern Anal. Mach. Intell., vol. 39, no. 6, pp. 1137–1149, 2017.\n2\n[26] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and\nharnessing adversarial examples,” in 3rd International Conference on\nLearning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings, 2015. 2, 4, 10\n[27] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,\n“Towards deep learning models resistant to adversarial attacks,” in\n6th International Conference on Learning Representations, ICLR 2018,\nVancouver, Canada, 2018. OpenReview.net, 2018. 2, 4, 9, 10\n[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:\nA large-scale hierarchical image database,” in 2009 IEEE Conference\non Computer Vision and Pattern Recognition, 2009, pp. 248–255. 2, 3, 4\n[29] A. Kolesnikov, L. Beyer, X. Zhai, J. Puigcerver, J. Yung, S. Gelly,\nand N. Houlsby, “Big transfer (BiT): General visual representation\nlearning,” in Computer Vision - ECCV 2020 - 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part V , ser. Lecture\nNotes in Computer Science, vol. 12350. Springer, 2020, pp. 491–507.\n2\n[30] F. Croce and M. Hein, “Reliable evaluation of adversarial robustness\nwith an ensemble of diverse parameter-free attacks,” in Proceedings\nof the 37th International Conference on Machine Learning, ICML 2020,\n13-18 July 2020, Virtual Event, ser. Proceedings of Machine Learning\nResearch, vol. 119. PMLR, 2020, pp. 2206–2216. 2, 4, 10\n[31] N. Carlini and D. Wagner, “Towards evaluating the robustness of\nneural networks,” in 2017 ieee symposium on security and privacy (sp) .\nIEEE, 2017, pp. 39–57. 2, 4, 9, 10\n[32] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, “Boosting\nadversarial attacks with momentum,” in 2018 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2018, Salt Lake City,\nUT, USA, June 18-22, 2018. Computer Vision Foundation / IEEE\nComputer Society, 2018, pp. 9185–9193. 2\nIEEE TRANSACTIONS ON XXXX 17\n[33] A. Athalye, N. Carlini, and D. A. Wagner, “Obfuscated gradients\ngive a false sense of security: Circumventing defenses to adversarial\nexamples,” in Proceedings of the 35th International Conference on\nMachine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,\nJuly 10-15, 2018 , ser. Proceedings of Machine Learning Research,\nvol. 80. PMLR, 2018, pp. 274–283. 2, 5\n[34] D. Hendrycks and K. Gimpel, “Gaussian error linear units\n(GELUs),” CoRR, vol. abs/1606.08415, 2016. 3\n[35] R. Wightman, “Pytorch image models,” https://github.com/\nrwightman/pytorch-image-models, 2019. 4\n[36] S. Xie, R. B. Girshick, P . Dollár, Z. Tu, and K. He, “Aggregated\nresidual transformations for deep neural networks,” in 2017 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2017,\nHonolulu, HI, USA, July 21-26, 2017 . IEEE Computer Society, 2017,\npp. 5987–5995. 4\n[37] Z. Li, W. Yang, S. Peng, and F. Liu, “A survey of convolutional\nneural networks: Analysis, applications, and prospects,” CoRR, vol.\nabs/2004.02806, 2020. 4\n[38] N. Papernot, P . McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and\nA. Swami, “The limitations of deep learning in adversarial settings,”\nin 2016 IEEE European symposium on security and privacy (EuroS&P) .\nIEEE, 2016, pp. 372–387. 4, 7\n[39] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P . Frossard,\n“Universal adversarial perturbations,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2017, pp. 1765–\n1773. 4, 10\n[40] M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein,\n“Square attack: a query-efﬁcient black-box adversarial attack via ran-\ndom search,” in European Conference on Computer Vision. Springer,\n2020, pp. 484–501. 4, 10\n[41] J. Chen and Q. Gu, “Rays: A ray searching method for hard-\nlabel adversarial attack,” in Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining , 2020,\npp. 1739–1747. 4, 10\n[42] J. Kantipudi, S. R. Dubey, and S. Chakraborty, “Color channel\nperturbation attacks for fooling convolutional neural networks\nand a defense against such attacks,” IEEE Transactions on Artiﬁcial\nIntelligence, vol. 1, no. 2, pp. 181–191, 2020. 4\n[43] F. Croce and M. Hein, “Minimally distorted adversarial examples\nwith a fast adaptive boundary attack,” in International Conference on\nMachine Learning. PMLR, 2020, pp. 2196–2205. 4\n[44] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adver-\nsarial examples in deep neural networks,” in 25th Annual Network\nand Distributed System Security Symposium, NDSS 2018, San Diego,\nCalifornia, USA, February 18-21, 2018 . The Internet Society, 2018. 4\n[45] C. Guo, M. Rana, M. Cissé, and L. van der Maaten, “Countering\nadversarial images using input transformations,” in 6th Interna-\ntional Conference on Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings .\nOpenReview.net, 2018. 4\n[46] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, “A study of\nthe effect of JPG compression on adversarial images,” CoRR, vol.\nabs/1608.00853, 2016. 4, 5\n[47] N. Das, M. Shanbhogue, S. Chen, F. Hohman, L. Chen, M. E.\nKounavis, and D. H. Chau, “Keeping the bad guys out: Protecting\nand vaccinating deep learning with JPEG compression,” CoRR, vol.\nabs/1705.02900, 2017. 4, 5\n[48] A. Graese, A. Rozsa, and T. E. Boult, “Assessing threat of\nadversarial examples on deep neural networks,” in 2016 15th\nIEEE International Conference on Machine Learning and Applications\n(ICMLA). IEEE, 2016, pp. 69–74. 4, 5\n[49] D. L. Donoho and J. M. Johnstone, “Ideal spatial adaptation by\nwavelet shrinkage,” biometrics, vol. 81, no. 3, pp. 425–455, 1994. 4\n[50] A. Chambolle, “An algorithm for total variation minimization and\napplications,” Journal of Mathematical imaging and vision , vol. 20,\nno. 1, pp. 89–97, 2004. 5\n[51] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok, “Synthesizing\nrobust adversarial examples,” in International conference on machine\nlearning. PMLR, 2018, pp. 284–293. 5\n[52] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and\nD. Batra, “Grad-CAM: Visual explanations from deep networks\nvia gradient-based localization,” International Journal of Computer\nVision, vol. 128, no. 2, p. 336–359, Oct 2019. 5\n[53] G. Ortiz-Jiménez, A. Modas, S. Moosavi-Dezfooli, and P . Frossard,\n“Hold me tight! inﬂuence of discriminative features on deep\nnetwork boundaries,” in Advances in Neural Information Processing\nSystems 33: NeurIPS 2020, December 6-12, 2020, virtual , 2020. 5, 8\n[54] S. A. Fezza, Y. Bakhti, W. Hamidouche, and O. Déforges, “Per-\nceptual evaluation of adversarial attacks for cnn-based image\nclassiﬁcation,” in 2019 Eleventh International Conference on Quality of\nMultimedia Experience (QoMEX). IEEE, 2019, pp. 1–6. 5, 6\n[55] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P . Simoncelli, “Image\nquality assessment: from error visibility to structural similarity,”\nIEEE transactions on image processing , vol. 13, no. 4, pp. 600–612,\n2004. 5, 6\n[56] E. C. Larson and D. M. Chandler, “Most apparent distortion: full-\nreference image quality assessment and the role of strategy,”Journal\nof electronic imaging, vol. 19, no. 1, p. 011006, 2010. 5, 6\n[57] J. Gildenblat and contributors, “Pytorch library for CAM methods,”\nhttps://github.com/jacobgil/pytorch-grad-cam, 2021. 5\n[58] Y. Sharma, G. W. Ding, and M. A. Brubaker, “On the effectiveness\nof low frequency perturbations,” in Proceedings of the Twenty-Eighth\nInternational Joint Conference on Artiﬁcial Intelligence, IJCAI 2019,\nMacao, China, August 10-16, 2019 . ijcai.org, 2019, pp. 3389–3396. 5",
  "topic": "Robustness (evolution)",
  "concepts": [
    {
      "name": "Robustness (evolution)",
      "score": 0.7867997884750366
    },
    {
      "name": "Preprocessor",
      "score": 0.7310229539871216
    },
    {
      "name": "Computer science",
      "score": 0.7004832029342651
    },
    {
      "name": "Artificial intelligence",
      "score": 0.615935742855072
    },
    {
      "name": "Adversarial system",
      "score": 0.603961706161499
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5917510986328125
    },
    {
      "name": "Machine learning",
      "score": 0.5017118453979492
    },
    {
      "name": "MNIST database",
      "score": 0.44832271337509155
    },
    {
      "name": "Transformer",
      "score": 0.440510630607605
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41376906633377075
    },
    {
      "name": "Artificial neural network",
      "score": 0.37268537282943726
    },
    {
      "name": "Engineering",
      "score": 0.14027664065361023
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I28221208",
      "name": "Institut National des Sciences Appliquées de Rennes",
      "country": "FR"
    }
  ]
}