{
  "title": "Efficient Training of Audio Transformers with Patchout",
  "url": "https://openalex.org/W3205475937",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3209284683",
      "name": "Koutini, Khaled",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214145472",
      "name": "Schlüter, Jan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281597132",
      "name": "Eghbal-zadeh, Hamid",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281357259",
      "name": "Widmer, Gerhard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3104613728",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3205743929",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4298091485",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4398958419",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2903137797",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W3032502780",
    "https://openalex.org/W2987999870",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2971105107",
    "https://openalex.org/W3169064633",
    "https://openalex.org/W4205689591",
    "https://openalex.org/W3165587897",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2052666245",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3164024107",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W4285704109",
    "https://openalex.org/W3196974791",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W3094550259",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "The great success of transformer-based models in natural language processing (NLP) has led to various attempts at adapting these architectures to other domains such as vision and audio. Recent work has shown that transformers can outperform Convolutional Neural Networks (CNNs) on vision and audio tasks. However, one of the main shortcomings of transformer models, compared to the well-established CNNs, is the computational complexity. In transformers, the compute and memory complexity is known to grow quadratically with the input length. Therefore, there has been extensive work on optimizing transformers, but often at the cost of degrading predictive performance. In this work, we propose a novel method to optimize and regularize transformers on audio spectrograms. Our proposed models achieve a new state-of-the-art performance on Audioset and can be trained on a single consumer-grade GPU. Furthermore, we propose a transformer model that outperforms CNNs in terms of both performance and training speed. Source code: https://github.com/kkoutini/PaSST",
  "full_text": "Efﬁcient Training of Audio Transformers with Patchout\nKhaled Koutini1,2, Jan Schl¨uter1, Hamid Eghbal-zadeh1,2, Gerhard Widmer1,2\nInstitute of Computational Perception1 & LIT AI Lab2, Johannes Kepler University Linz, Austria\nfirst.last@jku.at\nAbstract\nThe great success of transformer-based models in natural lan-\nguage processing (NLP) has led to various attempts at adapt-\ning these architectures to other domains such as vision and au-\ndio. Recent work has shown that transformers can outperform\nConvolutional Neural Networks (CNNs) on vision and audio\ntasks. However, one of the main shortcomings of transformer\nmodels, compared to the well-established CNNs, is the com-\nputational complexity. In transformers, the compute and mem-\nory complexity is known to grow quadratically with the input\nlength. Therefore, there has been extensive work on optimiz-\ning transformers, but often at the cost of degrading predictive\nperformance. In this work, we propose a novel method to op-\ntimize and regularize transformers on audio spectrograms. Our\nproposed models achieve a new state-of-the-art performance on\nAudioset and can be trained on a single consumer-grade GPU.\nFurthermore, we propose a transformer model that outperforms\nCNNs in terms of both performance and training speed.1\nIndex Terms: transformers, audio-tagging, attention models,\naudio classiﬁcation\n1. Introduction\nThe transformer architecture [1] has proven very successful\nin sequence modeling. It allows learning dependencies be-\ntween different items in the sequence regardless of their posi-\ntions or their separation in the sequence. Transformers are the\nstate-of-the-art models in different natural language processing\ntasks [2–4]. More recently, they have been adapted to computer\nvision [5–7] by extracting small patches from the input image\nand adding a learnable positional encoding to each patch. The\nresulting patches form a sequence that can be fed to the trans-\nformer. These vision transformer models achieve state-of-the-\nart performance on image classiﬁcation tasks, but require large\namounts of training data (e.g, in Vision Transformer (ViT) [5]),\nor heavily depend on extensive data augmentation and knowl-\nedge distillation from a CNN model (e.g, in Data-efﬁcient Im-\nage Transformers (DeiT) [8]). Gong et al. [9] further adapted\nvision transformers to audio spectrograms, achieving state-of-\nthe-art performance on Audioset [10] by using pre-trained mod-\nels from computer vision and using overlapping patches from\naudio spectrograms for ﬁne-tuning.\nThe transformer architecture consists of a series of self-\nattention layers [1]. Each layer relies on calculating a distance\nbetween each pair of items from the input sequence. Although\nthis allows each input item to attend to any other item in the\nsequence, this results in a complexity of O(n2) with respect to\nthe input sequence length n, in terms of both memory and com-\nputation effort. Reducing the quadratic complexity has been\nthe target of several approaches in natural language process-\ning, the idea being to restrict each input item (token) to attend\n1Source code and pretrained models: https://github.com/\nkkoutini/PaSST\n20 40 60 80 100 120 140 160 180\nTraining speed: sample/second\n0.435\n0.440\n0.445\n0.450\n0.455\n0.460\n0.465\n0.470\n0.475\n0.480mAP\nAST\nAST-N\nPaSST-S\nPaSST-L-S\nPaSST-N-SPaSST-U\nRF-CNN CNN141 GB/s 0.5 GB/s\nAST CNN PaSST\nFigure 1: Performance vs training speed on Audioset. The ra-\ndius of the circle indicates the required GPU memory per sam-\nple for training. On the largest publicly available audio dataset,\nour approach PaSST-S can reach the state-of-the-art perfor-\nmance in less than 2 days on a single consumer GPU. Details\nare presented in Table 1.\nonly to a pre-selected subset of input items (tokens). One ex-\nample is to allow attending only to neighbours inside a sliding\nwindow [11, 12]. Additionally, Kitaev et al. [13] use locality-\nsensitive hashing to approximate attention, reducing the atten-\ntion complexity to O(n log n). BigBird [14] combines sliding\nwindows, global attention, and random interaction between the\nsequence items. Masking portions of the input sequence has\nbeen shown to be an extremely effective method for training\nencoder/decoder transformers in NLP [2]. In computer vision,\nthe idea of removing patches during inference was investigated\nto assess the vision transformer’s robustness against input per-\nturbations [15].\nIn this paper, we focus on applying transformers to au-\ndio processing. We address the shortcomings of current audio\ntransformers from the aspect of computational complexity and\nmemory requirements by introducing a new simple yet effective\nmethod for training transformers on spectrograms. In summary,\nthe main contributions of our work are as follows:\n• We propose Patchout, a method that signiﬁcantly re-\nduces the computation and memory complexity of train-\ning transformers for the audio domain. Patchout also\nfunctions as a regularizer, improving the generalization\nof the trained transformers.\n• We disentangle the transformer’s positional encoding [5,\n8,9] into time and frequency positional encoding, allow-\ning for straightforward inference on audio snippets of\narXiv:2110.05069v3  [cs.SD]  29 Mar 2022\nvariable length without the need for ﬁne-tuning or in-\nterpolating positional encodings.\n• We investigate additional methods for reducing train-\ning complexity and demonstrate how they affect perfor-\nmance on the larger general-purpose Audioset as well as\ndomain-speciﬁc downstream tasks.\nOur proposed models can achieve state-of-the-art performance\non several audio tagging and classiﬁcation tasks using a single\nconsumer GPU, in a relatively short time (see Figure 1). When\ndifferent complexity reduction methods are combined, the mod-\nels outperform CNNs in terms of training speed and memory\nrequirements, in addition to generalization.\n2. The Patchout faSt Spectrogram\nTransformer (PaSST)\nThe Vision Transformer (ViT) [5] works by extracting small\npatches from an input image and projecting these patches lin-\nearly onto a sequence of embeddings. The sequence is then\naugmented by adding trainable positional encodings as biases to\nthe input sequence. A special classiﬁcation embedding (classi-\nﬁcation token) is then appended to the sequence, which is con-\nnected to a classiﬁer after the self-attention layers. In Data-\nefﬁcient Image Transformers (Deit) [8] another special embed-\nding for distillation (distillation token) is added. Gong et al. [9]\nshowed thatoverlapping the extracted patches improves the per-\nformance when training ViT on spectrograms. On the other\nhand, patch overlapping increases the total number of patches,\ni.e., the input sequence length. Therefore, overlapping greatly\nincrease the memory and compute requirements for training the\ntransformers. We propose a new method called Patchout (Sec-\ntion 2.2) to overcome these issues.\nFigure 2 summarizes the proposed transformer architecture:\nThe pipeline starts at the upper left with an audio spectrogram\nbeing fed into the model as input. (1) is the patch extraction\nand linear projection steps as explained in [5]. In (2), frequency\nand time positional encodings are added, as discussed below.\nIn (3), we apply Patchout as explained in Section 2.2, and add\nthe classiﬁcation token. In (4), we ﬂatten the sequence and pass\nthrough d layers blocks of self-attention ( d is the depth of the\ntransformer). Finally, a classiﬁer operates on the mean of the\ntransformed C and D tokens.\nIn Addition to Patchout (step 3 in Figure 2), the second\nmain difference between our work and previous work [5, 8, 9]\nis that we disentangle the positional encoding for the time and\nfrequency dimensions, and as a result, we have two positional\nencodings: one representing the frequency, and one for time\n(step 2 in Figure 2). This makes inference and the tuning of\nthe pre-trained models on downstream tasks with shorter audio\nlength simpler. When ﬁne-tuning or inference on shorter audio\nclips, we simply crop the time positional encoding parameters,\nwithout changing the frequency encoding parameter.\n2.1. Complexity Analysis\nMulti-head attention layers [1] rely on computing a distance\nbetween each pair of positions in the input sequence (in the\nform of an attention matrix), therefore having a complexity of\nO(n2) where n is the input sequence length. As the sequence\nlength grows – for example, when overlapping input patches or\nfor longer audio clips – the compute and memory requirements\nquickly become problematic. More speciﬁcally, given an input\nof b samples of the dimension (n×e), where b is the batch size,\nFigure 2: The Patchout transformer (PaSST) architecture as ex-\nplained in Section 2. The Self-attention layer + FFN (Feed-\nforward network) is explained in detail in [5]. C: classiﬁcation\ntoken. D: distillation token (only for models based on DeiT).\nn is the input sequence length, e is the embeddings size, each\nmulti-head attention layer projects each input sample toh query\nQ, key K, and value V matrices, where h is the number of at-\ntention heads [1]. Each of Q, K and V has a shape of (n ×e\nh ).\nThe attention matrix A is then computed by the matrix multipli-\ncation Q ×KT , scaling, and applying the soft-max activation\nfunction A = Softmax (Q×KT\n√\nd ). A has a shape of (n ×n)\nand is multiplied with V giving the attention output: A ×V\nresulting in a new sequence with the same shape as the input\n(n ×e). As a result, the computation complexity (and mem-\nory requirements) for all the operations on the attention matrix\nA grow quadratically O(n2) with sequence length n, while the\noperations in the rest of the network have a linear complexity re-\nlationship with n [1,5,8]. In short, reducing the sequence length\nwould have a large impact on the computational complexity of\nthese models.\n2.2. Patchout\nMotivated by (a) the impact of reducing the sequence length on\nthe computation complexity of training transformer models; (b)\nthe fact that audio events are expected to be spread out in time\nand frequency in an audio clip; (c) the insight that CNNs can\nbeneﬁt from having a small receptive ﬁeld during training for\ndifferent audio tasks, as shown in [16], we propose Patchout, a\nmethod to efﬁciently train transformer models on audio spec-\ntrograms. The idea is to drop parts of the transformer’s input\nsequence when training, encouraging the transformer to per-\nform the classiﬁcation using an incomplete sequence. We ﬁrst\nextract small overlapping patches from the input spectrograms\nand linearly project them to vectors, forming the transformer\ninput sequence. We augment the patches with both frequency\nand time encoding. When training, we randomly drop parts of\nthe sequence, reducing the sequence length, and effectively reg-\nularizing the training process. Similar to DropOut [17], during\ninference, the whole input sequence is presented to the trans-\nformer. We distinguish between different types of Patchout as\nfollows:\nUnstructured Patchout is the basic form of Patchout,\nwhere we select the patches randomly regardless of their po-\nsition. We refer to models trained with this method asPaSST-U.\nStructured Patchout: We randomly pick some frequency\nbins/time frames and remove a whole column/row of extracted\npatches. This structure is inspired by SpecAugment [18]. We\nrefer to models trained with this method as PaSST-S.\n2.3. Further Complexity Reduction Methods\n2.3.1. Reducing the extracted patches’ overlap\nReducing the overlap between patches results in a lower number\nof extracted patches, and therefore a smaller transformer input\nsequence length. However, Gong et al. [9] showed that reducing\nthe overlap (or training without overlapping) degrades the per-\nformance of the transformer on Audioset. Patchout can also be\nused even when there is no overlap between patches. We refer\nto the system without patch overlapping as PaSST-N.\n2.3.2. Reducing the depth of the transformer\nThe depth of the transformer is the number of successive self-\nattention blocks ( d in Figure 2). The depth has a linear rela-\ntionship with the overall training and inference complexity and\ninﬂuences the total number of parameters of the model. Since\nwe are starting the training from models pre-trained on Ima-\ngenet [19] (as explained in Section 3.2), we remove every other\nself-attention block. This allows us to beneﬁt from the pre-\ntraining, compared to removing consecutive blocks, since the\nresidual activations will have a less sudden change. We will\nrefer to the model with the removed blocks as PaSST-L. It has\nd = 7 self-attention blocks and 50M parameters compared to\n87M in the full model.\n3. Experiment Setup\nWe train our models on Audioset [10], the largest publicly avail-\nable audio dataset, consisting of around 2 million audio clips\nfrom Youtube. The task is to tag the audio clips with labels\nfrom 527 possible classes. Furthermore, we ﬁne-tune the mod-\nels trained on Audioset on various audio classiﬁcation and tag-\nging tasks, namely, instrument recognition, environmental au-\ndio classiﬁcation, and acoustic scene classiﬁcation.\n3.1. Preprocessing and Training Setup\nWe use mono audio with a sampling rate of 32 kHz. We ex-\ntract Mel features from a window of 25 ms with a hop length\nof 10 ms, resulting in 128 mel bands, similar to [9]. Kong et\nal. [20] showed the importance of balancing Audioset; there-\nfore, we balance our training data using importance sampling.\nWe assign a sampling weight to each sample proportional to\nthe inverse frequency of its label 1\nfreq(label)+100 . We train on\n1, 893, 693 (approx. 2M) training segments, and evaluate on\n18, 951 audio clips. For each epoch, we sample 200k sam-\nples from the full 2M Audioset without replacement. We use\nthe AdamW [21] optimizer with weight decay of 10−4, with a\nmaximum learning rate of 10−5. We use a linear learning rate\ndecay from epoch 50 to 100, dropping the learning rate to10−7\nand ﬁne-tune the model for a further 20 epochs.\n3.2. ImageNet Pretraining\nGong et al. [9] showed that using pre-trained models on Im-\nagenet signiﬁcantly improves their performance on Audioset.\nTherefore, we will use pre-tranined models in all our experi-\nmAP Speed ( ×AST) Mem Seq\nBaseline [10] .314 - - -\nPANNs [20] .439 131.0 ( 5.7×) .213 -\nAST [9] .459 23.1 ( 1×) 2.33 1212\nAST [9] ⋆ .459 23.1 ( 1×) 2.33 1190\nAST-N [9] ⋆ .454 80. ( 3.4×) .534 498\nCNN [23] ⋆ .438 126.3 ( 5.5×) .213 -\nPaSST-B ⋆ .462 23.1 ( 1×) 2.33 1190\nPaSST-U ⋆ .466 43.2 ( 1.9×) 1.14 790\nPaSST-S ⋆ .471 88.7 ( 3.8×) .513 474\nPaSST-S-L ⋆ .459 148.6 ( 6.4×) .311 474\nPaSST-S-N ⋆ .466 184.2 (8×) .202 254\nTable 1: Single-model results on Audioset. ⋆ indicates our run.\nmAP is the mean average precision (also referred to as preci-\nsion/recall area under curve). Speed: training throughput in\nspectrograms per second on an Nvidia Titan RTX GPU (show-\ning the speedup compared to AST [9]). Mem: the required GPU\nmemory to train per sample. Seq: the training sequence length.\nB: Baseline without Patchout. U: Unstructured Patchout. S:\nStructured Patchout. N: no-overlap of the extracted patches. L:\nlighter model with reduced depth=7.\nments.Our base model is DeiT B ↑384 [8]. We also achieve a\ncomparable performance using computationally more complex\nViT models such as stripped-down ViT-hug224 [5]; by remov-\ning half of the self-attention blocks, its depth was reduced to\nonly 16 blocks (with the methods explained in Section 2.3.2);\nthis will not be further explored in this paper.\n3.3. Data Augmentation\nThe transformer models are very prone to overﬁtting, therefore\ndata augmentation plays an essential role in the training process\n[8]. In our experiments, the following augmentation strategies\nare used:\nTwo-level Mix-Up: We use Mix-up [22] since it has been\nshown to improve performance [9, 23]. We mix both the raw\nwaveforms randomly from the dataset as well as the ﬁnal spec-\ntrograms.\nSpecaugment: We use SpecAugment [18] by masking up\nto 48 frequency bins and 192 time frames similar to [9].\nRolling: We roll the waveforms randomly over time.\nRandom Gain: We multiply the audio waveforms to\nchange the gain by ±7 dB.\n4. Results\n4.1. Audio Tagging on Audioset\nTable 1 shows the mean average precision mAP (also referred to\nas precision-recall area under-curve) results on Audioset [10].\nAs can be seen, the proposed model PaSST achieves a new\nstate-of-the-art performance on the largest available audio tag-\nging dataset. The proposed model outperforms AST [9] and\nsigniﬁcantly outperforms CNNs. Using Patchout not only im-\nproves the performance of the transformer architecture, but also\nincreases the training speed approximately 4 times, and reduces\nthe required GPU memory to less than 25%. As a result,\nit is possible to train PaSST on a single Nvidia RTX 2080ti\n(consumer GPU), achieving state-of-the-art performance in 50\nhours. Furthermore, PaSST-L-S (with a scaled down depth of\nd = 7) and PaSST-S-N (without patch overlap) signiﬁcantly\noutperform CNNs while maintaining a higher training through-\nput, and with similar GPU memory requirements. PaSST-S-L\nand PaSST-S-N can be trained on a single GPU to reach .459\nand .466 mAP in approximately 25 hours. Applying Patchout\non the transformer without overlap ( PaSST-S-N) outperforms\nthe baseline PaSST-B(without Patchout) and AST [9] while be-\ning up to 8 times faster, and requiring less than 10% of the GPU\nmemory for training. The results are also illustrated in Figure 1.\nThe only difference between the baseline PaSST-Band the AST\nmodel is the positional encoding. AST [9], like vision trans-\nformers [5, 8], employs grid positional encoding. PaSST-B, on\nthe other hand, utilises disentangled time and frequency posi-\ntional encoding (see Section 2).\nmAP\nBaseline [10] .314\nPSLA (Ensemble-S) [24] .469\nPSLA (Ensemble-M) [24] .474\nAST (Ensemble-M) [9] .475\nAST (Ensemble-M) [9] .485\nPaSST-S S16,14 (2 models)⋆ .486\nPaSST-S S10-16 (4 models)⋆ .493\nPaSST-S S10-16 (5 models)⋆ .495\nPaSST-S S10-16 (9 models)⋆ .496\nTable 2: Ensemble results on Audioset. ⋆ indicates our run.\nmAP is the mean average precision (also referred to as preci-\nsion/recall area under curve).\nTable 2 shows the result of ensemble models. We ensem-\nble models with different overlap values between input patches\n(Figure 2). S indicates the patches stride, S16 means no overlap\nbetween the patches. The ﬁrst ensemble (2 models) averages\nthe logits of a model with no patches overlap and a model with\nan overlap of 2 (stride=14). S10-S16 indicates that the models\nused have strides of 10,12,14 and 16.\n4.2. Fine-tuning and Transfer to Downstream Tasks\nWe ﬁne-tune the pre-trained (on Audioset) models on several\ndownstream audio tagging and classiﬁcation tasks with dif-\nferent dataset sizes, Table 3 summarizes the results. PaSST-\n(B,U,S) models use the pre-trained PaSST-S on Audioset, but\nfor ﬁne-tuning, we use no Patchout, unstructured Patchout, and\nstructured Patchout respectively. It is worth noting that the\ntransformer models can be ﬁne-tuned using a small number of\nepochs. The results suggest that researchers and practitioners\ncan use pre-trained PaSST and ﬁne-tune them on downstream\ntasks without the need for large computational resources.\nIn summary, ﬁne-tuning the transformer model outperforms\nstate-of-the-art CNNs on all tasks. Patchout results in signif-\nicant speedups and, in many cases, improved generalization.\nWhen combined with Structured Patchout ( -S), reducing com-\nplexity by removing patch overlap (-N) performs better than re-\nducing transformer depth (-L) and enables faster ﬁne-tuning.\nWe only replace the MLP classiﬁer in the pre-trained mod-\nels for ﬁne-tuning. When we use Patchout, we randomly re-\nmove roughly half of the input sequence. Each experiment was\nrepeated three times, and the average results are reported. The\nspeedup in Table 3 is relative to PaSST-B and is rounded up to\nthe nearest integer. Details on the setup of each task can be\nfound in our github repository.\nPolyphonic Musical Instrument Recognition: The task here\nis to detect all the instruments present in an audio clip. The\nOpenMIC dataset [25] consists 20,000 audio clips. Each clip\nis 10 seconds long and can be assigned multiple tags out of 20\nclasses. The metric for the task is the mean average precision.\nThe state-of-the-art methods for this task are CNNs with re-\nstricted receptive ﬁelds [23]. PaSST-S-N reaches the state-of-\nthe-art performance in less than 30 minutes on a single con-\nsumer GPU.\nEnvironmental Sound Classiﬁcation: The ESC50 dataset [26]\nconsists of 2,000 environmental 5-second audio clips. The task\nis to classify each clip into one out of 50 possible classes. We\nreport the accuracy averaged over the 5 ofﬁcial folds [26]. All\nPaSST variants (with Patchout) can be ﬁne-tuned on this dataset\nin less than 5 minutes on a single GPU. The state-of-the-art per-\nformance was achieved using the AST transformer model [9].\nThe difference between AST and PaSST-B is in the positional\nencoding, as explained in Section 2.\nAcoustic Scene Classiﬁcation: The task is to recognize the\nacoustic scene of 10-second audio clips. We use the TAU\nUrban Acoustic Scenes 2020 Mobile dataset [27] as used in\nthe DCASE 2020 challenge ( DCASE20). The audio clips are\nrecorded with different devices and further simulated devices\nare introduced. The performance is measured using accuracy\non a dataset including unseen devices. The ﬁrst place in the\nchallenge used CNNs [28]. Patchout accelerates training on this\ntask, reaching state-of-the-art in less than an hour. Patchout also\nallows for ﬁne-tuning on a single consumer GPU. It does, how-\never, lead to a decrease in accuracy.\nSound Event Recognition (Tagging) on FSD50K: The\nFSD50K dataset [29] consists of 51K audio clips annotated\nwith 200 sound event classes taken from the Audioset ontol-\nogy [10]. The dataset contains 100 hours of audio and is the\nsecond largest publicly available general purpose sound event\nrecognition dataset after Audioset. Furthermore, the FSD50K\nevaluation set is of high quality, with each evaluation label be-\ning double-checked and assessed by two to ﬁve independent an-\nnotators [29]. The reported results are on the ofﬁcial evaluation\nsubset of FSD50K using the best model on the validation subset.\nThe state-of-the-art in PSLA [24] is achieved through CNN ar-\nchitecture and a collection of performance-improving methods\nsuch as ImageNet pre-training, label enhancement, balancing,\ndata augmentation, and weight averaging. On this dataset, our\napproach signiﬁcantly outperforms the current state-of-the-art.\nFine-tuning PaSST-Sand PaSST-S-N takes less than 2 hours and\n1 hour, respectively.\nOpenMIC ESC50 DCASE20 FSD50K\nBaseline .795 [25] 76.9 [26] 54.1 [27] .434 [29]\nSOTA .831 [23] 95.6 [9] 73.7 [28] .558 [24]\n-B ⋆ .837|1× 96.3|1× 76.3|1× .649|1×\n-U ⋆ .843|4× 96.5|2× 75.6|4× .639|4×\n-S ⋆ .843|4× 96.8|2× 75.6|4× .653|4×\n-S-L ⋆ .841|6× 95.5|2× 73.7|6× .584|6×\n-S-N ⋆ .840|8× 96.4|4× 73.9|8× .637|8×\nTable 3: Results in performance and speedup compared to\nthe base model PaSST-B for the downstream tasks: poly-\nphonic instrument tagging using OpenMIC [25] dataset\n(mean average precision), Environmental Sound Classiﬁcation\nESC50 [26] (accuracy), Cross-device Acoustic Scene Classi-\nﬁcation DCASE20 [27] (accuracy). Sound Event Recognition\n(Tagging) in FSD50K [29](mean average precision). The sec-\nond part of the table compares different PaSST variants ( Ta-\nble 1).\n5. Conclusion\nWe propose a new method for efﬁciently training transformers\non audio spectrograms, achieving state-of-the-art performance\non Audioset as well as several downstream tasks. Furthermore,\nPatchout signiﬁcantly reduces compute complexity and mem-\nory requirements for training transformers. We investigate ad-\nditional methods for reducing training complexity and propose\ntwo models, PaSST-S-Land PaSST-S-N, that outperform CNNs\nwhile having a faster training speed and comparable memory\nrequirements. Our pre-trained models can be ﬁne-tuned on sev-\neral audio downstream tasks with little resources and little ad-\nditional training time.\n6. ACKNOWLEDGMENT\nThis work has been supported by the COMET-K2 Center of the\nLinz Center of Mechatronics (LCM) funded by the Austrian\nFederal Government and the Federal State of Upper Austria.\nThe LIT AI Lab is ﬁnanced by the Federal State of Upper Aus-\ntria. The computational results presented have been achieved in\npart using the Vienna Scientiﬁc Cluster (VSC).\n7. References\n[1] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin Advances in Neural Information Processing Systems, 2017, pp.\n5998–6008.\n[2] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT: pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in NAACL-HLT Minneapolis, MN, USA, June 2-7,\n2019, Volume 1, 2019, pp. 4171–4186.\n[3] D. R. So, W. Manke, H. Liu, Z. Dai, N. Shazeer, and Q. V . Le,\n“Primer: Searching for efﬁcient transformers for language model-\ning,” Advances in neural information processing systems, 2021.\n[4] I. Yamada, A. Asai, H. Shindo, H. Takeda, and Y . Matsumoto,\n“LUKE: deep contextualized entity representations with entity-\naware self-attention,” in Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing, EMNLP .\nAssociation for Computational Linguistics, 2020, pp. 6442–6454.\n[5] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly, J. Uszkoreit, and N. Houlsby, “An image is worth 16x16\nwords: Transformers for image recognition at scale,” in Interna-\ntional Conference on Learning Representations ICLR 2021, Vir-\ntual, May 3-7, 2021.\n[6] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using\nshifted windows,” in 2021 IEEE/CVF International Conference\non Computer Vision, ICCV 2021, Montreal, QC, Canada, October\n10-17, 2021. IEEE, 2021, pp. 9992–10 002.\n[7] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable\nDETR: deformable transformers for end-to-end object detection,”\nin International Conference on Learning Representations, ICLR\n2021, Virtual Event, Austria, May 3-7, 2021.\n[8] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and\nH. J´egou, “Training data-efﬁcient image transformers & distilla-\ntion through attention,” in ICML 2021, 18-24 July 2021, Virtual ,\nvol. 139, 2021, pp. 10 347–10 357.\n[9] Y . Gong, Y .-A. Chung, and J. Glass, “AST: Audio Spectrogram\nTransformer,” in Interspeech 2021, Brno, Czechia, 30 August - 3\nSeptember, 2021, pp. 571–575.\n[10] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio\nset: An ontology and human-labeled dataset for audio events,”\nin ICASSP 2017, New Orleans, LA, 2017.\n[11] Z. Wang, P. Ng, X. Ma, R. Nallapati, and B. Xiang, “Multi-\npassage BERT: A globally normalized BERT model for open-\ndomain question answering,” in EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7,, 2019, pp. 5877–5881.\n[12] S. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin, “Adaptive\nattention span in transformers,” inProceedings of the 57th Confer-\nence of the Association for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, pp. 331–335.\n[13] N. Kitaev, L. Kaiser, and A. Levskaya, “Reformer: The efﬁcient\ntransformer,” inInternational Conference on Learning Represen-\ntations ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\n[14] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Al-\nberti, S. Onta ˜n´on, P. Pham, A. Ravula, Q. Wang, L. Yang, and\nA. Ahmed, “Big bird: Transformers for longer sequences,” inAd-\nvances in neural information processing systems, 2020.\n[15] M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. S. Khan,\nand M. Yang, “Intriguing properties of vision transformers,” in\nAdvances in neural information processing systems, 2021.\n[16] K. Koutini, H. Eghbal-zadeh, M. Dorfer, and G. Widmer, “The\nReceptive Field as a Regularizer in Deep Convolutional Neural\nNetworks for Acoustic Scene Classiﬁcation,” in EUSIPCO 2019,\nA Coru˜na, Spain, 2019.\n[17] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: A simple way to prevent neural net-\nworks from overﬁtting,” Journal of Machine Learning Research,\nvol. 15, no. 56, pp. 1929–1958, 2014.\n[18] D. S. Park, W. Chan, Y . Zhang, C. Chiu, B. Zoph, E. D. Cubuk,\nand Q. V . Le, “Specaugment: A simple data augmentation method\nfor automatic speech recognition,” in Interspeech 2019, Graz,\nAustria, 15-19 September, 2019, pp. 2613–2617.\n[19] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Im-\nagenet: A large-scale hierarchical image database,” in IEEE Con-\nference On Computer Vision and Pattern Recognition., 2009, pp.\n248–255.\n[20] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D. Plumb-\nley, “Panns: Large-scale pretrained audio neural networks for au-\ndio pattern recognition,” IEEE ACM Trans. Audio Speech Lang.\nProcess., vol. 28, pp. 2880–2894, 2020.\n[21] I. Loshchilov and F. Hutter, “Decoupled weight decay regulariza-\ntion,” in International Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019, 2019.\n[22] H. Zhang, M. Ciss ´e, Y . N. Dauphin, and D. Lopez-Paz, “Mixup:\nBeyond empirical risk minimization,” in International Confer-\nence on Learning Representations, ICLR 2018, Vancouver, BC,\nCanada, April 30 - May 3, 2018, Conference Track Proceedings ,\n2018.\n[23] K. Koutini, H. Eghbal-zadeh, and G. Widmer, “Receptive ﬁeld\nregularization techniques for audio classiﬁcation and tagging with\ndeep convolutional neural networks,”IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, vol. 29, pp. 1987–2000,\n2021.\n[24] Y . Gong, Y .-A. Chung, and J. Glass, “PSLA: Improving audio\ntagging with pretraining, sampling, labeling, and aggregation,”\nIEEE/ACM Transactions on Audio, Speech, and Language Pro-\ncessing, 2021.\n[25] E. Humphrey, S. Durand, and B. McFee, “OpenMIC-2018: An\nopen data-set for multiple instrument recognition,” in ISMIR\n2018, Paris, France, September 23-27, 2018, pp. 438–444.\n[26] K. J. Piczak, “ESC: Dataset for Environmental Sound Classiﬁ-\ncation,” in Proceedings of the 23rd Annual ACM Conference on\nMultimedia. ACM Press, 2015, pp. 1015–1018.\n[27] T. Heittola, A. Mesaros, and T. Virtanen, “Acoustic scene classiﬁ-\ncation in DCASE 2020 Challenge: generalization across devices\nand low complexity solutions,” inDCASE2020 Workshop, 2020.\n[28] S. Suh, S. Park, Y . Jeong, and T. Lee, “Designing Acoustic Scene\nClassiﬁcation Models with CNN Variants,” DCASE2020 Chal-\nlenge, Tech. Rep., 2020.\n[29] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, “FSD50K:\nan open dataset of human-labeled sound events,” IEEE ACM\nTrans. Audio Speech Lang. Process., vol. 30, pp. 829–852, 2022.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8115551471710205
    },
    {
      "name": "Transformer",
      "score": 0.7711684703826904
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5858582258224487
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4671458899974823
    },
    {
      "name": "Speech recognition",
      "score": 0.4514563977718353
    },
    {
      "name": "Audio visual",
      "score": 0.42715027928352356
    },
    {
      "name": "Artificial neural network",
      "score": 0.42143508791923523
    },
    {
      "name": "Computer engineering",
      "score": 0.3886418342590332
    },
    {
      "name": "Machine learning",
      "score": 0.38829657435417175
    },
    {
      "name": "Multimedia",
      "score": 0.13841447234153748
    },
    {
      "name": "Engineering",
      "score": 0.10818979144096375
    },
    {
      "name": "Electrical engineering",
      "score": 0.08039531111717224
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I121883995",
      "name": "Johannes Kepler University of Linz",
      "country": "AT"
    }
  ],
  "cited_by": 154
}