{
  "title": "LSG Attention: Extrapolation of Pretrained Transformers to Long Sequences",
  "url": "https://openalex.org/W4307537353",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3003621210",
      "name": "Charles Condevaux",
      "affiliations": [
        "Université de Nîmes"
      ]
    },
    {
      "id": "https://openalex.org/A288053221",
      "name": "Sébastien Harispe",
      "affiliations": [
        "Université de Montpellier",
        "EuroMov Digital Health in Motion"
      ]
    },
    {
      "id": "https://openalex.org/A3003621210",
      "name": "Charles Condevaux",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A288053221",
      "name": "Sébastien Harispe",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2963691842",
    "https://openalex.org/W3099950029",
    "https://openalex.org/W3204112174",
    "https://openalex.org/W6605905859",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W4225727438",
    "https://openalex.org/W2915716523",
    "https://openalex.org/W2934798344",
    "https://openalex.org/W3170490008",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3106504817",
    "https://openalex.org/W2953280096",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W6605100834",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6609612681",
    "https://openalex.org/W4285171765",
    "https://openalex.org/W6781533629",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W3169942382",
    "https://openalex.org/W2118323718",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4294619417",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4303684868",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W3155147984",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2924214462",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2964347512",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2896457183"
  ],
  "abstract": null,
  "full_text": "LSG Attention: Extrapolation of pretrained\nTransformers to long sequences\nCharles Condevaux\nCHROME\nUniversity of Nîmes, France\ncharles.condevaux@unimes.fr\nSébastien Harispe\nEuroMov Digital Health in Motion,\nUniv Montpellier, IMT Mines Ales, France\nsebastien.harispe@mines-ales.fr\nAbstract\nTransformer models achieve state-of-the-art\nperformance on a wide range of NLP tasks.\nThey however suffer from a prohibitive limita-\ntion due to the self-attention mechanism, induc-\ning O(n2) complexity with regard to sequence\nlength. To answer this limitation we introduce\nthe LSG architecture which relies on Local,\nSparse and Global attention. We show that\nLSG attention is fast, efﬁcient and competitive\nin classiﬁcation and summarization tasks on\nlong documents. Interestingly, it can also be\nused to adapt existing pretrained models to ef-\nﬁciently extrapolate to longer sequences with\nno additional training. Along with the intro-\nduction of the LSG attention mechanism, we\npropose tools to train new models and adapt\nexisting ones based on this mechanism.\n1 Introduction\nTransformer models [1] are nowadays state-of-the-art\nin numerous domains, and in particular in NLP where\nthey are used in general language models, and to suc-\ncessfully tackle several speciﬁc tasks such as document\nsummarization, machine translation and speech process-\ning to cite a few [2, 3]. The cornerstone of Transformer\nmodels is the Attention mechanism used to iteratively\nbuild complex context-dependent representations of se-\nquence elements, e.g. tokens, by dynamically aggre-\ngating prior representations of these elements. Using\nself-attention, a popular Attention ﬂavour, this is made\nby computing full attention scores deﬁning how each\nprior element representation will contribute to building\nthe new representation of an element. Considering a\nsequence of nelements, the computation of the atten-\ntion scores is therefore of complexity O(n2) which is\nprohibitive when large sequences have to be processed.\nFurthermore, in the current context where a large num-\nber of models based on full attention have been trained\non various datasets and tasks, we are also interested\nin extrapolating those models to longer sequences by\nsimply substituting full attention by new attention mech-\nanisms post training. Common pretrained models (e.g\nBERT, RoBERTa) are indeed known to underperform\nwhen extrapolated to sequences of length exceeding the\n512 tokens considered during training. This is due to the\nnature of the attention mechanism which largely impacts\nextrapolation capabilities: full attention usually fails to\nextrapolate, even considering post hoc adaptations, e.g.\nadding constants in the score matrix [4], using a relative\npositional embedding [5] or duplicating the positional\nembedding [6]. Deﬁning new attention mechanisms\nthat can efﬁciently substitute full attention in pretrained\nmodels that are not originally capable of handling long\nsequences would avoid the costs induced by training\nlarge language models from scratch.\nThe main contributions of this paper are:\n1. LSG (Local Sparse Global) attention, an efﬁcient\nO(n) approach to approximate self-attention for pro-\ncessing long sequences, is introduced.1\n2. Results demonstrating that LSG is fast, efﬁcient\nand competitive on classiﬁcation and summarization\ntasks applied to long documents are presented. It is\nalso shown that LSG can adapt and extrapolate existing\npretrained models not based on LSG, with minimal to\nno additional training.\n3. A procedure and companion tools are proposed to\nconvert various existing models and checkpoints (BERT,\nRoBERTa, DistilBERT, BART) from HuggingFace to\ntheir LSG variant.2\nCompared to several contributions aiming at reducing\nthe complexity of self-attention introduced hereafter, a\nspeciﬁc focus is given in our work on the extrapolation\nof existing Transformer models, i.e. reuse, to longer\nsequences.\n2 Related works\nSeveral contributions have been devoted to the optimiza-\ntion of the Attention mechanism. Four categories of\n1https://huggingface.co/ccdv\n2https://github.com/ccdv-ai/convert_checkpoint_to_lsg\n1\narXiv:2210.15497v1  [cs.CL]  13 Oct 2022\nCondevaux et al.\napproaches can be distinguished in the literature: (i) re-\ncurrent models such as Transformers-XL [7] and Com-\npressive Transformers [8] which maintain a memory\nof past activation at each layer to preserve long-range\ncontextual information; (ii) models based on factoriza-\ntion or kernels aiming at compressing attention score\nmatrices, such as Linformer [9] or Performer [10]; (iii)\nmodels based on clustering such as Reformer [11] that\ndynamically deﬁne eligible attention patterns (i.e. where\nattention may be made); and (iv) models based on ﬁxed\nor adaptative attention patterns, e.g. Longformer [6] or\nBig Bird [12].\nRecurrent approaches iteratively process the sequence\nby maintaining a memory to enable long-range depen-\ndencies. They generally suffer limitations induced by\nspeciﬁc, slow, and difﬁcult to implement forward and\nback propagation procedures. Alternatively, one of the\nmain line of study for reducing the complexity of Atten-\ntion is thus to perform sparsity by limiting the number\nof elements on which new representations will be based,\ni.e. reducing the number of elements with non-null\nattention scores. This approach is motivated by the ob-\nservation of global or data-dependent positional patterns\nof non-null attention scores depending on the task [13].\nThe sparsity of attention scores in the traditional Atten-\ntion mechanism is indeed documented in the literature.\nIt has for instance been shown that in practice, full atten-\ntion tends to overweight close elements in average, in\nparticular for MLM, machine translation, and seq-to-seq\ntasks in general [14]. Moreover, according to analyses\non the use of multi-head full attention on speciﬁc tasks,\ne.g. machine translation, numerous heads learn simi-\nlar simple patterns [15]. Such redundant patterns may\nbe hardcoded implementing ﬁxed-positional patterns,\neventually in a task-dependent manner.\nTwo main approaches are discussed in the literature\nfor implementing sparsity: ﬁxed or adaptative patterns\nbased on whether attention scores are computed con-\nsidering (1) predeﬁned ﬁxed elements based on their\nlocation in the sequence, or (2) elements selected from\na given procedure. As an example, [16] have shown that\nﬁxed O(n) convolutions can perform competitively on\nmachine translation. Longformer proposes an alterna-\ntive O(n) approach based on sliding and global patterns\n[6]. In the context of image, audio, and text processing,\n[13] propose sparse Transformer, an O(n√n) model\nbased on sparse factorization of the attention matrix rely-\ning on speciﬁc 2D factorized attention schemes. Those\napproaches however prevent the use of task-dependent\ndynamic patterns. Considering adaptative patterns, [16]\nalso introduced dynamic convolutions as an O(n) com-\nplexity substitute to self-attention. Kernels deﬁning the\nimportance of context elements are speciﬁed at infer-\nence time rather than ﬁxed after training. Another ex-\nample is Reformer [11], an O(nlogn ) approach based\non locality-sensitive hashing (LSH) based on random\nprojections.\nIn a transverse manner, several authors, explicitly or\nimplicitly motivated by the compositional nature of lan-\nguage have studied structured approaches in which sub-\nsequences (i.e. blocks) are processed independently\nand then aggregated. This aims at implementing a lo-\ncal or global dynamic memory for considering close to\nlong-range dependencies. [ 17] introduce a blockwise\napproach to reduce the quadratic complexity induced by\nlarge sequences in encoder-decoder architectures. [18]\npropose a chunkwise attention in which attention is per-\nformed in a blockwise manner adaptively splitting the\nsequence into small chunks over which soft attention is\ncomputed. This idea is also used in Transformer-XL [7].\n[19] propose a masked block self-attention mechanism\nin which the entire sequence is divided into blocks, to\nfurther 1) apply self-attention intra-block for modeling\nlocal contexts, to further 2) apply self-attention inter-\nblock for capturing long-range dependencies. Such an\napproach enables implementing some forms of connec-\ntivity between all positions over several steps without\nbeing restricted by full attention limitations. This can\nalso be achieved by factorization techniques, e.g. [13].\nMore recently authors have proposed global attention\nmechanisms encoding information related to blocks on\nwhich attention is based [20, 21, 22].\nThis paper presents the LSG (Local, Sparse and Global)\nattention based on block local attention to capture local\ncontext, sparse attention to capture extended context and\nglobal attention to improve information ﬂow. Contrary\nto prior work mostly focusing on deﬁning new models,\nthe proposed LSG Attention mechanism is model agnos-\ntic and aims at facilitating adapting existing (pretrained)\nmodels for them to be used on long sequences.\n3 LSG Attention\nLSG attention relies on two main points. It is assumed\nthat locally, a token needs to capture low level informa-\ntion thus dense attention is prefered. On the other hand,\nas the context grows, higher level information is sufﬁ-\ncient. This translates into the need for connections to a\nlimited number of tokens following speciﬁc selection\nand computation rules. The LSG approach relies on 3\ncomponents: block local attention to capture local con-\ntext, sparse attention to capture extended context and\nglobal attention to improve information ﬂow. A compar-\nison to Big Bird and Longformer attention patterns is\nshown in Figure 1.\nLSG Big Bird Longformer\nFigure 1: Attention patterns\n3.1 Local Attention\nLongformer depends on a ﬁxed length sliding window\nto perform local attention. However this approach is\ndifﬁcult to optimize and must rely on a custom CUDA\nkernel to be computationally efﬁcient. To improve over-\nall training and inference speed, we take advantage of a\nblock-based process similar to Big Bird. The sequence\n2\nCondevaux et al.\nis split into nb non-overlapping chunks of size bt. For\na given block, each token attends to the tokens inside\nthe block, as well as to those in the previous and next\nblocks. In this conﬁguration, the local attention win-\ndow is asymmetrical since a token can connect up to\n2 ×bt −1 tokens on the left or on the right.\n3.2 Sparse Attention\nSparse connections are used to expand the local con-\ntext by selecting an additional set of tokens following\na set of rules. These tokens can be directly selected\nbased on a speciﬁc metric or using some computation\nsuch as a pooling method. In the proposed approach,\neach attention head can process different sparse tokens\nindependently. Sparse attention also relies on a block\nstructure where the sparse selection is done inside each\nblock. Five alternative criteria can be used in LSG.\nHead-wise strided Inspired by the Hepos model [23],\na ﬁxed selection pattern is deﬁned. Each attention head\nwill attend to a set of tokens following a speciﬁc stride\ndeﬁned as the sparsify factor f. Figure 2 shows the\nselection pattern.\nHead 1\nHead 2\nBlock\nSequence\nFigure 2: Head-wise strided selection with a stride of 2.\nHead-wise block strided This selection pattern is\nsimilar to the previous one but selects consecutive to-\nkens instead. Figure 2 shows the selection pattern.\nHead 1\nHead 2\nBlock\nSequence\nFigure 3: Block strided selection with a stride of 2.\nAverage pooling A simple way to reduce sequence\nlength. After chunking the sequence into blocks, sparse\ntokens are computed using average pooling. For a block\nof size bt and a sparsify factor f, we pool inside each\nblock with a window of f and a stride of f to produce\nbt/f tokens.\nMax norm The objective of a norm-based approach\nis to select tokens that are most likely highly weighted\nin the score matrix. Finding those keys efﬁciently is\ndifﬁcult in practice so we use a simple and deterministic\nmetric. For a query and a key q,k ∈Rd, we can write:\nqk⊤= cos(θ)∥q∥∥k∥\nIn this situation cos(θ) sign is unknown. However, if it\nis positive and ∥k∥is high, the key will likely dominate\nthe softmax regardless of the query. After chunking the\nsequence into blocks, we select inside each block and\neach head bt/f tokens with the highest key norm.\nLSH Clustering This approach is a non deterministic\none since it relies on the LSH algorithm [24]. For each\nblock, bt/f clusters are built using a single round LSH.\nTo get c = bt/f hashes and for an input x ∈Rd, a\nrandom matrix R ∈Rd×c/2 is generated, such that\nh(x) = arg max([xR; −xR])\nwith [a; b] the concatenation of two vectors. Using the\nkey matrix as input, each token inside the block gets\na cluster index from h(x). Tokens inside a cluster are\naveraged.\nComputation To reduce the computational cost, the\nattention pattern is designed to compute each connection\nonce. For this, the local and sparse tokens are selected\nsuch that there is no overlap between them during atten-\ntion computation. Each query is connected to 3 local\nblocks and 2 sparse blocks of keys. The maximum con-\ntext length (distance between two keys) is then equal\nto 3 ×bt + 2×bt ×f. The concatenation of local and\nsparse keys is shown Figure 4. For causal attention, the\nthird local block and the second sparse block can be\nignored during computation.\na b\nSparse context Sparse context\na b\nSequence\nLocal context\nFigure 4: Local and sparse contexts with a block size of\n2 and a sparsity factor of 4. Queries aand bwill attend\nto 6 local keys and 4 sparse keys.\n3.3 Global Attention\nGlobal tokens improve the ﬂow of information inside the\nmodel. They attend to every tokens across the sequence\nand all tokens attend to them. Rather than picking a\nsubset of tokens and deﬁning them as global, they are\nprepended to the sequence and trained using their own\nembedding matrix, thus their number is an additional\nhyperparameter. When a model is converted to its LSG\nversion, the ﬁrst global token is initialized as the sum\nof the [CLS] (or <s>) token and the ﬁrst position from\nthe positional embedding. The other global tokens are\ninitialized as the sum of [MASK] (or <mask>) token\nand the other positions from the positional embedding.\n3.4 Positional Embedding\nIt is necessary to modify the positional embedding ma-\ntrix to reuse existing models to process long sequences.\n3\nCondevaux et al.\nSimilarly to Longformer’s authors [6], instead of ran-\ndomly initializing the new positions, the original matrix\nis duplicated and concatenated until the desired max\nsequence length is reached.\n4 Experiments\nThe LSG model is implemented in PyTorch and aims\nat performing model extrapolation by replacing full at-\ntention by the LSG attention in various architectures of\nthe HuggingFace library. In the experiments, the ofﬁcial\nRoBERTa-base checkpoint for classiﬁcation tasks and\nBART-base checkpoint for summarization tasks are ex-\ntrapolated using LSG attention. All metrics are reported\nfor the test set except in the case where only the valida-\ntion set is available. We use a batch size of 32, a linear\ndecaying learning rate, a dropout rate of 0.10 and Adam\n(0.9, 0.999) optimizer [25] for classiﬁcation and summa-\nrization experiments. An experiment comparing several\nattention approximations to extrapolate RoBERTa in an\nMLM task is ﬁrst discussed as it is used to limit the\nnumber of tested alternatives, and therefore reduce the\ncost of the proposed evaluations. All experiments are\nrun on NVIDIA Quadro RTX 8000 48Gb GPUs.\n4.1 RoBERTa extrapolation on MLM\nA simple test on a MLM task is performed to test for\nthe ability of an attention mechanism to extrapolate a\nmodel to longer sequences without additional training.\nTo do so, a RoBERTa-base model is considered and two\nexperiments are conducted. First, the full attention is\nsubstituted by different kinds of attention (kernel, factor-\nization, local, ﬁxed pattern) and each model is evaluated\non sequences of the same length as those considered\nduring RoBERTa initial training (512 tokens). For the\nsecond experiment, their ability to extrapolate to 4,096\ntokens sequences without additional training is tested\n(the positional embedding being duplicated 8 times).\nA random sample from Wikipedia + BookCorpus +\nCC_News is used; BPC and MLM accuracy are reported\nin Table 1. RoBERTa’s author report a 1.880 BPC loss;\nwe obtain a comparable loss of 1.881 on this random\nsample.\nOnly Longformer, Big Bird and LSG attention manage\nto obtain competing BPC while processing sequences of\nthe same length as those considered during the original\nRoBERTa training. Other approaches such as Linformer,\nPerformer or Reformer requires additional MLM ﬁne-\ntuning to leverage an existing checkpoint. It can be seen\nthat RoBERTa fails to extrapolate to longer sequences\n(+2.454 BPC), which highlights that full attention is not\nsuitable for extrapolation. Longformer and Big Bird\nattention results show the capability of these approaches\nto perform some form of extrapolation. Therefore, we\nrestrict our comparison to these two approaches in order\nto reduce the costs of our experimentation.\n4.2 Classiﬁcation Tasks\nTo evaluate the relevance of LSG, we compare our ap-\nproach to common model architectures able to process\nlong sequences with a similar number of parameters.\nExperiments are performed on Longformer [ 6], Big\nBird [12] and on all sparse attention types with a block\nsize of 128 and a sparsify factor of 4. All models are\nﬁne-tuned on IMDb, ArXiv, Patent, Scotus, EcthrA and\nEcthrB datasets presented below.\n4.2.1 Datasets\nDatasets are available on the HuggingFace hub, see\nAppendix D, e.g. detailed statistics in Table 15.\nIMDb [29] binary sentiment analysis classiﬁcation\ntask from movie reviews.\nArXiv [30] set of documents from ArXiv where the\nobjective is to predict a topic from 11 available classes.\nBecause there is no ofﬁcial split, a random one is made\nof 28K, 2.5K and 2.5K documents for train, validation\nand test.\nPatent [31] subset of the Big Patent summarization\ndataset. The task is redeﬁned as a classiﬁcation task\nwhere the objective is to predict the patent category\nusing the full document (9 classes). A random split of\n25K, 5K and 5K documents for train, validation and test\nis created.\nSome speciﬁc domains are highly dependent on pro-\ncessing long sequences, e.g. legal domain in which\nsentences tend to be long and complex. To demonstrate\nthe ability of LSG attention to leverage pretrained mod-\nels in such cases, the following three datasets are chosen\nfrom LexGlue [32], a benchmark focused on legal docu-\nments. Tasks where the input is on average signiﬁcantly\nlonger than 512 tokens have been selected.\nScotus Given a court opinion, the task is to predict\nthe relevant issue area among 14 choices.\nECtHRa and ECtHRb The objective is to predict\nwhich articles of the European Court of Human Rights\n(ECHR) have been violated (if any) from case descrip-\ntion: multi-label task (10 + 1 labels).\n4.2.2 Training setup and architecture\nTo make a fair comparison between models and archi-\ntectures, ﬁne-tuning is done with the same learning\nrate, number of steps (or epochs) and batch size. To\nshow that the LSG attention is compatible with differ-\nent architectures, the LexGlue tasks are also run with\nLEGAL-BERT [33] converted to its LSG version using\nthe provided conversion tools.\n4.2.3 Results\nWe report all experiment results in Table 2. We observe\nthat LSG is competitive with Longformer and Big Bird\nmodels with input sequences up to 4096 tokens long. A\nmajor difference lies in the implementation itself since\nthe LSG model is twice as fast to train on these lengths\nwithout additional memory cost; this aspect is discussed\nin Section 5.\n4\nCondevaux et al.\nAttention 512 length 4,096 length\nBPC Accuracy BPC Accuracy\nRoBERTa (full) [26] 1.881 0.732 4.335 0.359\nLinear Attn. [27] 11.324 0.061 11.474 0.058\nEfﬁcient Attn. [28] 21.022 0.102 20.574 0.097\nPerformer [10] 10.382 0.107 10.556 0.102\nLinformer (128 proj.) [9] 22.176 0.098 20.386 0.032\nReformer [11] 17.602 0.003 18.608 0.002\nLongformer (512) [6] 1.929 0.726 2.051 0.708\nBig Bird (64) [12] 1.881 0.732 2.439 0.659\nLSG-Norm (128/2) (block size / sparsity) 1.919 0.727 2.032 0.712\nLSG-Stride (128/2) 1.938 0.724 2.046 0.710\nLSG-BlockStride (128/2) 1.940 0.724 2.048 0.709\nLSG-Pooling (128/2) 1.968 0.720 2.064 0.706\nLSG-LSH (128/2) 1.969 0.719 2.065 0.705\nTable 1: BPC and MLM accuracy of RoBERTa-base with various Attention mechanisms.\nOn Patent, ECtHRa and ECtHRb tasks, the ability\nto process longer sequences improves signiﬁcantly\nthe F-measures compared to a vanilla (full attention)\nRoBERTa model. We also observe that Big Bird model\nis in general slightly under its counterpart except for the\nECtHRb dataset. This probably comes from the random\nattention mechanism which may require additional train-\ning steps. LSG-LSH and Big Bird models are affected\nby randomness during inference, thus their performance\ncan differ between runs.\nExtrapolating LEGAL-BERT with LSG to handle\nlonger sequences improves predictions; this behavior is\nexpected and has been observed by the authors of the\nLexGlue benchmark. The choice of the sparse attention\nis likely task speciﬁc. Using local attention only with\na large block size is also a viable option. The role of\nglobal tokens is not discussed here since we only use\none for all experiments. We show in the next section\nwith summarization tasks the utility of such tokens.\n4.3 Summarization Tasks\nAll summarization experiments are run using a 8e-5\nlearning rate, a 10% warmup, a length penalty of 2.0\nand a beam size of 5 for beam search. The validation set\nis used to choose the max generation length. We choose\nto evaluate our models on summarization tasks where\nthe input is signiﬁcantly longer than 1k tokens only. We\nﬁne-tune our model on ArXiv, PubMed, MultiNews and\nMediaSum datasets we present below during respec-\ntively 6, 8, 12 and 6 epochs for 4,096-length inputs.\n4.3.1 Datasets\nThe next datasets are available on the HuggingFace hub,\nsee Appendix D.\nArXiv and Pubmed [34] are sets of documents from\nArXiv and Pubmed; the goal is to generate an abstract\nusing a document as input.\nMultiNews [35] involves generating human-written\nsummaries from sets of news documents.\nMediaSum [36] consists of using interview tran-\nscripts from CNN and NPR media to generate a sum-\nmary.\nWe report detailed statistics in Table 15 in Appendix D.\nThe average length and the 90% quantile of all docu-\nments and summaries using a whitespace separator are\nreported. Note that ArXiv abstracts are signiﬁcantly\nlonger in the training set (300 on average) than in the\nvalidation and test sets (173 on average). Most summa-\nrization models are limited to 1,000 tokens inputs, thus\nthey are not able to process a full document to generate\na summary.\n4.3.2 Training setup and architecture\nWe ﬁrst convert the BART-base model [37] to its LSG\nversion by replacing the full attention in the encoder part\nand adding global tokens. The model is then ﬁne-tuned\non 4,096-length inputs and evaluated. To reduce compu-\ntational costs, experiments on 16,384-length inputs are\nwarm started from the 4,096-length experiments using\nthe conversion script. The model is then ﬁne-tuned dur-\ning a single epoch if necessary using the same training\nparameters. We propose 3 setups for the 16,384-length.\nFirst we evaluate the model with pure extrapolation\nfrom 4,096-length (no additional training). In the sec-\nond setup, we extrapolate and add 64 global tokens we\nchoose to ﬁne-tune. In the last setup, we extrapolate, we\nadd 64 global tokens and we ﬁne-tune the full model.\nExtrapolation is done by concatenating 4 copies of the\npositional embedding matrix (4 ×4096).\nCompared to the existing literature, the model is rather\nsmall and an input sequence of 16384 tokens can ﬁt on a\n48Gb GPU during training without relying on gradient-\ncheckpointing. The size of various summarization mod-\nels from the literature are reported in Table 3.\n5\nCondevaux et al.\nIMDb Arxiv Patent Scotus ECtHRa ECtHRb\nEpochs 3 3 3 7 5 5\nLearning rate 2e-5 5e-5 2e-5 1e-4 1e-4 1e-4\nRoBERTa (512-length) 95.5 87.2/86.8 66.6/61.8 69.4/60.8 62.9/58.2 72.0/65.9\nLongformer 95.9 88.2/87.9 69.8/63.8 72.9/62.6 68.3/59.7 78.9/72.2\nBig Bird ETC 95.4 85.9/85.5 69.4/63.9 69.4/58.2 68.3/60.3 80.0/70.6\nLSG-Local (256/0) 96.0 87.5/87.1 69.9/64.8 73.3/63.7 68.8/63.7 79.9/73.4\nLSG-Stride (128/4) 95.6 88.2/87.9 69.2/64.0 70.5/60.0 69.5/62.3 79.3/71.6\nLSG-BlockStride (128/4) 95.7 87.7/87.4 69.6/64.1 72.5/63.1 69.1/58.6 79.5/71.8\nLSG-Norm (128/4) 95.7 87.0/86.6 70.0/64.4 71.3/60.8 70.1/61.9 79.4/72.1\nLSG-Pooling (128/4) 95.9 87.5/87.3 69.4/64.1 72.6/60.9 70.2/61.4 79.0/73.1\nLSG-LSH (128/4) 95.8 88.2/87.9 69.5/64.2 70.3/54.6 71.0/60.3 78.9/71.0\nLegal-BERT (512-length) - - - 73.5/60.5 64.2/58.2 73.2/65.9\nLSG-Legal-BERT (256/0) - - - 74.5/62.6 71.7/63.9 81.0/75.1\nTable 2: Micro/Macro F-1 on classiﬁcation datasets.\nModels Parameters\nPRIMERA [38] 447M\nLED [6] 460M\nHAT-BART [39] 471M\nPegasus [40] 577M\nBig Bird-Peg. [12] 577M\nHepos [23] 406M\nLongT5-Base [41] 220M\nLongT5-L 770M\nLongT5-XL 3B\nOurs, LSG-BART-base (256/0) 145M\nTable 3: Parameters count of summarization models.\n4.3.3 Results\nLSG-BART is compared to state-of-the-art models by\nreporting the results from their respective papers. We\nuse ROUGE-1, ROUGE-2 and ROUGE-L evaluation\nmetrics as comparison points.\nAs shown in Tables 4, 5, 6 and 7, our approach can\nachieve competitive performances with a limited size\nwithout pretraining a new model from scratch. The sec-\nond important element is the ability of this approach\nto improve metrics from 4.096 to 16.384-length inputs\nwithout additional ﬁne-tuning, this is especially true\non ArXiv and PubMed datasets which have the longest\ninput sequences. Fine tuning additional global tokens\nfurther improveS metrics while limiting cost and train-\ning time compared to a fully tuned model.\nOn the ArXiv dataset (Table 4), a max sequence gener-\nation of 320 tokens is chosen, our approach is compet-\nitive with every size of the LongT5 model. However,\nthe authors pointed out that they used greedy genera-\ntion instead of beam search, thus their results are likely\nunderestimated.\nOn the PubMed dataset (Table 5), a max sequence gen-\neration of 512 tokens is chosen, our approach is close to\nHepos models which also rely on BART. LongT5 is sig-\nModels R1 R2 RL\nPegasus (1K) 44.70 17.27 25.80\nBig Bird-Peg. (4K) 46.63 19.02 41.77\nLED (4K) 44.40 17.94 39.76\nLED (16K) 46.63 19.62 41.83\nPRIMERA (4K) 47.58 20.75 42.57\nHAT-BART (4K) 46.68 19.07 42.17\nHepos-LSH (7.2K) 48.24 20.26 41.78\nHepos-SKN (10.2K) 47.87 20.00 41.50\nLongT5-Base (4K) 44.87 18.54 40.97\nLongT5-L (16K) 48.28 21.63 44.11\nLongT5-XL (16K) 48.35 21.92 44.27\nOurs (4K) 46.65 18.91 42.18\nOurs (16K) 47.03 20.19 42.69\n+ global tuning 48.08 20.42 43.65\n+ full tuning 48.74 20.88 44.23\nTable 4: ROUGE performances on ArXiv dataset.\nniﬁcantly better here and this difference may be related\nto the way this model is pretrained and the dataset used\nfor this.\nOn the MultiNews dataset (Table 6), a max sequence\ngeneration of 320 tokens is chosen, our approach is\nclose again to the LongT5 models. While extrapolation\nimproves metrics, additional ﬁne-tuning has a negative\nimpact. Since this dataset is rather small (45K examples,\n1,400 steps), ﬁne-tuning a single epoch is not enough\nfor the model to converge properly, longer training is\nrequired.\nOn the MediaSum dataset (Table 7), a max sequence\ngeneration of 128 tokens is chosen. Our approach is\nclose to the LongT5-base model again. This dataset\nhas the shortest inputs, thus processing a maximum of\n16,384 tokens has a marginal impact on performances.\nAdditional results using different types of sparse atten-\ntion are detailed in Appendix C.\n6\nCondevaux et al.\nModels R1 R2 RL\nPegasus (1K) 45.49 19.90 27.69\nBig Bird-Peg. (4K) 46.32 20.65 42.33\nHAT-BART (4K) 48.36 21.43 37.00\nHepos-LSH (7.2K) 48.12 21.06 42.72\nHepos-SKN (10.2K) 47.93 20.74 42.58\nLongT5-Base (4K) 47.77 22.58 44.38\nLongT5-L (16K) 49.98 24.69 46.46\nLongT5-XL (16K) 50.23 24.76 46.67\nOurs (4K) 47.37 21.74 43.67\nOurs (16K) 48.03 22.42 44.32\n+ global tuning 48.12 20.46 44.40\n+ full tuning 48.32 22.52 44.57\nTable 5: ROUGE performances on PubMed dataset.\nModels R1 R2 RL\nTG-MultiSum 47.10 17.55 20.73\nPRIMERA (4K) 49.90 21.10 25.9\nLongT5-Base (4K) 46.01 17.37 23.50\nLongT5-L (4K) 46.99 18.21 24.08\nLongT5-L (8K) 47.18 18.44 24.18\nLongT5-XL (8K) 48.17 19.43 24.90\nOurs (4K) 47.10 18.94 25.22\nOurs (16K) 47.30 19.19 25.38\n+ global tuning 47.23 19.18 25.29\n+ full tuning 47.07 19.04 25.35\nTable 6: ROUGE performances on MultiNews.\nModels R1 R2 RL\nBART-Large (1K) 35.09 18.05 31.44\nT5-large (1K) 30.68 14.88 27.88\nLongT5-Base (4K) 35.09 18.35 31.87\nLongT5-L (4K) 35.54 19.04 32.20\nLongT5-XL (4K) 36.15 19.66 32.80\nOurs (4K) 35.16 18.13 32.20\nOurs (16K) 35.17 18.13 32.21\n+ global tuning 35.22 18.08 32.22\n+ full tuning 35.31 18.35 32.47\nTable 7: ROUGE performances on MediaSum.\nModels Time/step Memory\nRoBERTa (512) 1.18 s 28.8/32.1 Gb\nLongformer 3.27 s 39.2/38.1 Gb\nBig Bird 2.89 s 44.5/44.4 Gb\nLSG-Local (256/0) 1.42 s 40.7/32.3 Gb\nLSG-Norm (128/4) 1.52 s 40.4/33.4 Gb\nLSG-Norm (32/4) 1.24 s 26.1/24.3 Gb\nTable 8: Training speed and memory with a batch of\n16384 tokens (Adam optimizer). All models rely on\nsequences of 4096 tokens except RoBERTa. Memory\nusage is computed with and without attention dropout.\n5 Implementation details\nThe proposed implementations are exclusively based\non those of HuggingFace in which the global tokens\nare prepended to the sequence and the attention layer is\nreplaced by its efﬁcient version; other elements are not\nmodiﬁed.\nTo improve efﬁciency, the inputs are split into blocks.\nEach block of queries is connected to 3 blocks of local\nkeys, 2 blocks of sparse keys and to all global keys. Thus\nfor head h, queries, keys and values are of shape Qh ∈\nRnb×bt×dh and Kh,V h ∈ Rnb×(5bt+g)×dh with nb\nthe number of blocks, bt the size of blocks, dh the size\nof the head and g the number of global tokens. This\nformat improves computational speed as shown in Table\n8. Global attention is computed independently.\n6 Conclusion\nWe have presented LSG attention, a novel efﬁcientO(n)\nalternative to the full attention mechanism relying on\nlocal, sparse and global attentions. Our results on MLM,\nclassiﬁcation and summarization tasks show that LSG\nis a competitive full attention substitute for pretrained\nTransformers to efﬁciently extrapolate to long input\nsequences. We also proposed an optimized implementa-\ntion of the LSG attention mechanism on HuggingFace,\nimproving training speed by a factor of 2 without addi-\ntional memory cost compared to Longformer and Big\nBird models. By providing a conversion tool to lever-\nage existing models and checkpoints (BERT, RoBERTa,\nDistilBERT, BART), the proposed approach removes\nthe need of a costly re-training of existing models to\nhandle long sequences.\nLimitations\nAlthough the proposed conversion tool allows to con-\nvert existing checkpoints of commonly used models,\nit is today necessary to reimplement the approach for\neach architecture due to the lack of homogeneity of\nHuggingFace implementations (no wrapper available\nyet). Maintenance may therefore be a problem in the\nlong run to ensure compatibility. We however provide\nimplementation examples as well as documentation to\nease the conversion process.\nConcerning the proposed attention itself, the choice of\nthe sparse attention remains an additional hyperparam-\neter which is task speciﬁc. There is no rule of thumb\nto choose the sparse type, the size of blocks and the\nsparsity factor. The role of global tokens is also de-\nbatable. Their use can slow down training speed and\nconvergence. In practice, the impact of these tokens is\npositive if the model is trained a sufﬁcient number of\nsteps.\nAlthough the approach allows an existing model to be\nreused without having to pretrain from scratch and to\nreduce the duration of ﬁne-tuning phases, the complex-\nity remains linear with the length of the input. This\ndoes not eliminate the energy costs required to deploy\nTransformer models.\n7\nCondevaux et al.\nAcknowledgements\nThis work has beneﬁted from LAWBOT (ANR-20-\nCE38-0013) grant and HPC resources of IDRIS (al-\nlocation 2022-AD011011309R2) made by GENCI.\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805,\n2018.\n[3] Colin Raffel, Noam Shazeer, Adam Roberts,\nKatherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, Peter J Liu, et al. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. J. Mach. Learn. Res., 21(140):1–\n67, 2020.\n[4] Oﬁr Press, Noah A. Smith, and Mike Lewis.\nTrain short, test long: Attention with lin-\near biases enables input length extrapolation.\narXiv:2108.12409, 2021.\n[5] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\nSelf-attention with relative position representa-\ntions. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), pages 464–\n468, New Orleans, Louisiana, jun 2018. Associa-\ntion for Computational Linguistics.\n[6] Iz Beltagy, Matthew E. Peters, and Arman Co-\nhan. Longformer: The long-document transformer.\narXiv:2004.05150, 2020.\n[7] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime\nCarbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\n[8] Jack W Rae, Anna Potapenko, Siddhant M Jayaku-\nmar, and Timothy P Lillicrap. Compressive trans-\nformers for long-range sequence modelling. arXiv\npreprint arXiv:1911.05507, 2019.\n[9] Sinong Wang, Belinda Z. Li, Madian Khabsa, Han\nFang, and Hao Ma. Linformer: Self-attention with\nlinear complexity. CoRR, abs/2006.04768, 2020.\n[10] Krzysztof Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Belanger, Lucy\nColwell, and Adrian Weller. Rethinking attention\nwith performers. arXiv:2009.14794, 2021.\n[11] Nikita Kitaev, Lukasz Kaiser, and Anselm Lev-\nskaya. Reformer: The efﬁcient transformer. CoRR,\nabs/2001.04451, 2020.\n[12] Manzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago\nOntanon, Philip Pham, Anirudh Ravula, Qifan\nWang, Li Yang, et al. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33, 2020.\n[13] Rewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509,\n2019.\n[14] Kevin Clark, Urvashi Khandelwal, Omer Levy,\nand Christopher D Manning. What does bert look\nat? an analysis of bert’s attention. arXiv preprint\narXiv:1906.04341, 2019.\n[15] Alessandro Raganato, Yves Scherrer, and Jörg\nTiedemann. Fixed encoder self-attention patterns\nin transformer-based machine translation. arXiv\npreprint arXiv:2002.10260, 2020.\n[16] Felix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. Pay less attention\nwith lightweight and dynamic convolutions. arXiv\npreprint arXiv:1901.10430, 2019.\n[17] Denny Britz, Melody Y Guan, and Minh-\nThang Luong. Efﬁcient attention using a ﬁxed-\nsize memory representation. arXiv preprint\narXiv:1707.00110, 2017.\n[18] Chung-Cheng Chiu and Colin Raffel. Mono-\ntonic chunkwise attention. arXiv preprint\narXiv:1712.05382, 2017.\n[19] Tao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nand Chengqi Zhang. Bi-directional block self-\nattention for fast and memory-efﬁcient sequence\nmodeling. arXiv preprint arXiv:1804.00857,\n2018.\n[20] Joshua Ainslie, Santiago Ontanon, Chris Alberti,\nVaclav Cvicek, Zachary Fisher, Philip Pham,\nAnirudh Ravula, Sumit Sanghai, Qifan Wang, and\nLi Yang. Etc: Encoding long and structured inputs\nin transformers. arXiv preprint arXiv:2004.08483,\n2020.\n[21] Xingxing Zhang, Furu Wei, and Ming Zhou. Hi-\nbert: Document level pre-training of hierarchical\nbidirectional transformers for document summa-\nrization. arXiv preprint arXiv:1905.06566, 2019.\n[22] Qipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan\nShao, Xiangyang Xue, and Zheng Zhang. Star-\ntransformer. arXiv preprint arXiv:1902.09113,\n2019.\n[23] Luyang Huang, Shuyang Cao, Nikolaus Parulian,\nHeng Ji, and Lu Wang. Efﬁcient attentions for long\ndocument summarization. In Proceedings of the\n2021 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 1419–1436,\nOnline, jun 2021. Association for Computational\nLinguistics.\n[24] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven,\nIlya P. Razenshteyn, and Ludwig Schmidt. Practi-\ncal and optimal LSH for angular distance. CoRR,\nabs/1509.02897, 2015.\n8\nCondevaux et al.\n[25] Diederik P. Kingma and Jimmy Ba. Adam: A\nmethod for stochastic optimization, 2014.\n[26] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du,\nMandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692, 2019.\n[27] Angelos Katharopoulos, Apoorv Vyas, Nikolaos\nPappas, and François Fleuret. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. CoRR, abs/2006.16236, 2020.\n[28] Zhuoran Shen, Mingyuan Zhang, Shuai Yi, Jun-\njie Yan, and Haiyu Zhao. Factorized attention:\nSelf-attention with linear complexities. CoRR,\nabs/1812.01243, 2018.\n[29] Andrew L. Maas, Raymond E. Daly, Peter T.\nPham, Dan Huang, Andrew Y . Ng, and Christo-\npher Potts. Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguis-\ntics: Human Language Technologies, pages 142–\n150, Portland, Oregon, USA, jun 2011. Associa-\ntion for Computational Linguistics.\n[30] Jun He, Liqun Wang, Liu Liu, Jiao Feng, and Hao\nWu. Long document classiﬁcation from local word\nglimpses via recurrent attention learning. IEEE\nAccess, 7:40707–40718, 2019.\n[31] Eva Sharma, Chen Li, and Lu Wang. Bigpatent:\nA large-scale dataset for abstractive and coherent\nsummarization. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 2204–2213, Florence, Italy, jul\n2019. Association for Computational Linguistics.\n[32] Ilias Chalkidis, Abhik Jana, Dirk Hartung,\nMichael Bommarito, Ion Androutsopoulos,\nDaniel Martin Katz, and Nikolaos Aletras.\nLexglue: A benchmark dataset for legal language\nunderstanding in english. In Proceedings of\nthe 60th Annual Meeting of the Association for\nComputational Linguistics, Dubln, Ireland, 2022.\n[33] Ilias Chalkidis, Manos Fergadiotis, Prodromos\nMalakasiotis, Nikolaos Aletras, and Ion Androut-\nsopoulos. Legal-bert: The muppets straight out\nof law school. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, pages\n2898–2904, Online, nov 2020. Association for\nComputational Linguistics.\n[34] Arman Cohan, Franck Dernoncourt, Doo Soon\nKim, Trung Bui, Seokhwan Kim, Walter Chang,\nand Nazli Goharian. A discourse-aware attention\nmodel for abstractive summarization of long doc-\numents. Proceedings of the 2018 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers), 2018.\n[35] Alexander R. Fabbri, Irene Li, Tianwei She, Suyi\nLi, and Dragomir R. Radev. Multi-news: a large-\nscale multi-document summarization dataset and\nabstractive hierarchical model, 2019.\n[36] Chenguang Zhu, Yang Liu, Jie Mei, and Michael\nZeng. Mediasum: A large-scale media interview\ndataset for dialogue summarization.arXiv preprint\narXiv:2103.06410, 2021.\n[37] Mike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed,\nOmer Levy, Veselin Stoyanov, and Luke Zettle-\nmoyer. Bart: Denoising sequence-to-sequence\npre-training for natural language generation, trans-\nlation, and comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 7871–7880, Online,\njul 2020. Association for Computational Linguis-\ntics.\n[38] Wen Xiao, Iz Beltagy, Giuseppe Carenini, and\nArman Cohan. Primera: Pyramid-based masked\nsentence pre-training for multi-document summa-\nrization. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguis-\ntics (Volume 1: Long Papers), pages 5245–5263,\nDublin, Ireland, may 2022. Association for Com-\nputational Linguistics.\n[39] Tobias Rohde, Xiaoxia Wu, and Yinhan Liu. Hier-\narchical learning for generation with long source\nsequences. CoRR, abs/2104.07545, 2021.\n[40] Jingqing Zhang, Yao Zhao, Mohammad Saleh,\nand Peter J. Liu. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summariza-\ntion, 2019.\n[41] Mandy Guo, Joshua Ainslie, David C. Uthus,\nSantiago Ontañón, Jianmo Ni, Yun-Hsuan Sung,\nand Yinfei Yang. Longt5: Efﬁcient text-to-\ntext transformer for long sequences. CoRR,\nabs/2112.07916, 2021.\nA Training parameters\nWe use a batch size of 32, a linear decaying learning\nrate, a dropout rate of 0.10 and Adam optimizer for all\ntasks. Other parameters are reported in Table 9.\nEpochs LR Warmup\nClassiﬁcation\nIMDb 3 2e-5 0%\nArXiv 3 5e-5 0%\nPatent 3 2e-5 0%\nScotus 7 1e-4 0%\nECtHRa 5 1e-4 0%\nECtHRb 5 1e-4 0%\nSummarization\nArXiv 6/1 8e-5 10%\nPubMed 8/1 8e-5 10%\nMultiNews 12/1 8e-5 10%\nMediaSum 6/1 8e-5 10%\nTable 9: Training parameters for all tasks.\n9\nCondevaux et al.\nB Additional classiﬁcation results\nAdditional classiﬁcation results using a smaller block\nsize are presented in Table 10. It shows that the use of a\nsmaller block size remains competitive even if a slight\nloss in performance is observed.\nC Additional summarization results\nModels (4,096) R1 R2 RL Ctx.\n32/4\nPooling 42.75 16.34 38.23 160\nStride 44.23 17.21 39.72 160\nBlock Stride 44.15 17.10 39.60 160\nNorm 42.02 15.65 37.45 160\nLSH 42.58 16.21 38.04 160\n128/4\nPooling 46.27 18.68 41.82 644\nStride 46.34 18.64 41.87 644\nBlock Stride 46.23 18.62 41.80 644\nNorm 45.96 18.46 41.51 644\nLSH 46.19 18.72 41.76 644\nReference 46.65 18.91 42.18 784\nTable 11: ROUGE performances on ArXiv dataset.\nModels (4,096) R1 R2 RL Ctx.\n32/4\nPooling 44.60 19.35 40.85 160\nStride 45.52 20.07 41.75 160\nBlock Stride 45.30 19.89 41.54 160\nNorm 44.30 19.05 40.47 160\nLSH 44.53 19.27 40.74 160\n128/4\nPooling 47.11 21.42 43.40 644\nStride 47.16 21.49 43.44 644\nBlock Stride 47.13 21.46 43.42 644\nNorm 47.09 21.44 43.36 644\nLSH 47.11 21.41 43.42 644\nReference 47.37 21.74 43.67 784\nTable 12: ROUGE performances on PubMed dataset.\nTrained models on summarization tasks are reevaluated\nafter changing the type of sparse attention and block\nsize. Results on ArXiv and PubMed are reported in\nTables 11 and 12. The context column refers to the num-\nber of keys each query attends to. As reference models\nare trained on large (256) local blocks, the number of\nconnections is 3 ×256. By using 20% less keys (644),\ninference results are still competitive even though the\nmodel has never seen these speciﬁc sparse patterns be-\nfore. By limiting connections to 20% of the keys (160),\na performance drop is observed even if the metrics still\nremain respectable. Under these conditions, stride and\nblock-stride approaches generate better predictions.\nD Datasets and Models\nAll the datasets evalued are available on the Hugging-\nFace hub, links are provided in Table 13.\nSummarization checkpoints are available on the Hug-\ngingFace hub, links are provided in Table 14.\nAverage input size are provided in Table 15. Note that\ntoken counts are obtained using a whitespace split. Sub-\nword tokenization increases these numbers by 30% to\n40% depending on the tokenizer and the vocabulary\nsize.\nE Hyperparameters and complexity\nLSG attention is sensitive to several hyperparameters\nand the nature of the pretrained model.\nBlock size Generally speaking, block size improves\nperformance to a certain extent. If the converted model\nis trained on 512-length sequences, a block size beyond\n256 has a negative impact and requires a longer ﬁne-\ntuning phase. A smaller block reduces training and\nmemory cost.\nSparsity factor The sparsity factor is generally cho-\nsen between 0 (no sparse attention), 2, 4 and 8. Al-\nthough the choice remains task-dependent, a factor\nabove 8 tends on average to decrease the level of perfor-\nmance, especially for pooling-based approaches. This\nhyperparameter is chosen to be small when the task fo-\ncuses on local information (MLM, NER) and can be\nlarger for tasks requiring a wider context (summariza-\ntion, question-answering).\nGlobal tokens The more global tokens there are, the\nlonger it takes for the model to converge and obtain\nperformance gains. The initialization of these tokens is\nimportant, in particular for the ﬁrst one which is used\nas a pooling token for classiﬁcation tasks ([CLS] or <s>\n+ position 0).\nOverall complexity LSG attention has some similari-\nties with Big Bird attention and has the sameO(n) com-\nplexity with regard to sequence length. Figure 5 shows\nthe effect of increasing sequence length on training time\nand memory consumption (with Adam optimizer).\n10\nCondevaux et al.\nIMDb Arxiv Patent Scotus ECtHRa ECtHRb\nRoBERTa (512-length) 95.5 87.2/86.8 66.6/61.8 69.4/60.8 62.9/58.2 72.0/65.9\nLongformer 95.9 88.2/87.9 69.8/63.8 72.9/62.6 68.3/59.7 78.9/72.2\nBig Bird 95.4 85.9/85.5 69.4/63.9 69.4/58.2 68.3/60.3 80.0/70.6\nLSG-Local (256/0) 96.0 87.5/87.1 69.9/64.8 73.3/63.7 68.8/63.7 79.9/73.4\nLSG-Stride (128/4) 95.6 88.2/87.9 69.2/64.0 70.5/60.0 69.5/62.3 79.3/71.6\nLSG-BlockStride (128/4) 95.7 87.7/87.4 69.6/64.1 72.5/63.1 69.1/58.6 79.5/71.8\nLSG-Norm (128/4) 95.7 87.0/86.6 70.0/64.4 71.3/60.8 70.1/61.9 79.4/72.1\nLSG-Pooling (128/4) 95.9 87.5/87.3 69.4/64.1 72.6/60.9 70.2/61.4 79.0/73.1\nLSG-LSH (128/4) 95.8 88.2/87.9 69.5/64.2 70.3/54.6 71.0/60.3 78.9/71.0\nLSG-Stride (32/4) 95.6 85.0/84.5 69.2/63.0 72.4/62.7 69.9/58.4 79.2/72.3\nLSG-BlockStride (32/4) 95.4 86.6/86.3 69.4/63.4 72.4/62.6 70.3/60.9 79.2/69.4\nLSG-Norm (32/4) 95.7 85.3/84.9 69.3/63.9 72.2/62.5 68.9/60.9 79.1/73.5\nLSG-Pooling (32/4) 95.7 88.2/88.0 69.2/63.3 72.6/60.2 69.6/59.1 79.3/72.1\nLSG-LSH (32/4) 95.7 88.0/87.7 68.9/62.9 71.9/60.4 70.0/61.1 80.3/73.1\nLegal-BERT (512-length) - - - 73.5/60.5 64.2/58.2 73.2/65.9\nLSG-Legal-BERT - - - 74.5/62.6 71.7/63.9 81.0/75.1\nTable 10: Micro/Macro F-1 on classiﬁcation datasets.\nFigure 5: Memory and time complexity as a function of input sequence length.\nLink to the hub\nClassiﬁcation\nIMDb imdb\nArXiv arxiv-classiﬁcation\nPatent patent-classiﬁcation\nScotus lex_glue/scotus\nECtHRa lex_glue/ecthr_a\nECtHRb lex_glue/ecthr_b\nSummarization\nArXiv scientiﬁc_papers/arxiv\nPubMed scientiﬁc_papers/pubmed\nMultiNews multi_news\nMediaSum mediasum\nTable 13: Links to datasets.\nSummarization Link to the hub\nLSG-PubMed (4,096) lsg-pubmed\nLSG-PubMed (16,384) lsg-pubmed\nLSG-ArXiv (4,096) lsg-arxiv\nLSG-ArXiv (16,384) lsg-arxiv\nLSG-MediaSum (4,096) lsg-mediasum\nLSG-MediaSum (16,384) lsg-mediasum\nLSG-MultiNews (4,096) lsg-multinews\nAdditional checkpoints lsg-checkpoints\nTable 14: Links to model checkpoints.\n11\nCondevaux et al.\nDatasets Example count Input length Summary length\nTrain Validation Test Average q-90% Average q-90%\nClassiﬁcation\nIMDb 25,000 - 25,000 234 458 - -\nArXiv 28,000 2,500 2,500 9,115 16,264 - -\nPatent 25,000 5,000 5,000 3,593 6,579 - -\nScotus 5,000 1,400 1,400 6,992 14,570 - -\nECtHRa 9,000 1,000 1,000 1,681 3,728 - -\nECtHRb 9,000 1,000 1,000 1,681 3,728 - -\nSummarization\nArXiv 203,037 6,436 6,440 6,030 11,165 272 321\nPubMed 119,924 6,633 6,658 3,050 5,737 202 304\nMultiNews 44,972 5,622 5,622 1,792 3,375 217 294\nMediaSum 443,596 10,000 10,000 1,582 2,883 15 32\nTable 15: Statistics of evaluated datasets, lengths are whitespace based.\n12",
  "topic": "Extrapolation",
  "concepts": [
    {
      "name": "Extrapolation",
      "score": 0.8267480134963989
    },
    {
      "name": "Computer science",
      "score": 0.7771016359329224
    },
    {
      "name": "Transformer",
      "score": 0.6837214231491089
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45340460538864136
    },
    {
      "name": "Machine learning",
      "score": 0.33035898208618164
    },
    {
      "name": "Electrical engineering",
      "score": 0.16716092824935913
    },
    {
      "name": "Mathematics",
      "score": 0.08830302953720093
    },
    {
      "name": "Statistics",
      "score": 0.05494990944862366
    },
    {
      "name": "Voltage",
      "score": 0.0538562536239624
    },
    {
      "name": "Engineering",
      "score": 0.05016180872917175
    }
  ]
}