{
  "title": "Usability Test Results for a Discovery Tool in an Academic Library",
  "url": "https://openalex.org/W2162861387",
  "year": 2008,
  "authors": [
    {
      "id": "https://openalex.org/A5046543717",
      "name": "Jody Condit Fagan",
      "affiliations": [
        "James Madison University"
      ]
    },
    {
      "id": "https://openalex.org/A5113695524",
      "name": "Meris Mandernach",
      "affiliations": [
        "James Madison University"
      ]
    },
    {
      "id": "https://openalex.org/A5043767240",
      "name": "Carl S. Nelson",
      "affiliations": [
        "James Madison University"
      ]
    },
    {
      "id": "https://openalex.org/A5053314010",
      "name": "Jonathan Paulo",
      "affiliations": [
        "James Madison University"
      ]
    },
    {
      "id": "https://openalex.org/A5012341606",
      "name": "Grover Saunders",
      "affiliations": [
        "James Madison University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2154467739",
    "https://openalex.org/W2063779836",
    "https://openalex.org/W2072318492",
    "https://openalex.org/W2113522075",
    "https://openalex.org/W2031500514",
    "https://openalex.org/W2088392822",
    "https://openalex.org/W2006470576",
    "https://openalex.org/W4239539948",
    "https://openalex.org/W2021521823",
    "https://openalex.org/W1981904378",
    "https://openalex.org/W2025367853",
    "https://openalex.org/W2024280792",
    "https://openalex.org/W2139939749",
    "https://openalex.org/W1486912020",
    "https://openalex.org/W2029962166",
    "https://openalex.org/W2008572983",
    "https://openalex.org/W2082218505",
    "https://openalex.org/W2075375687",
    "https://openalex.org/W2141732390",
    "https://openalex.org/W2172229294",
    "https://openalex.org/W2003555234",
    "https://openalex.org/W2495858068",
    "https://openalex.org/W2114415111",
    "https://openalex.org/W2040383126",
    "https://openalex.org/W6636459339",
    "https://openalex.org/W1511505723",
    "https://openalex.org/W2060659933",
    "https://openalex.org/W2119930144",
    "https://openalex.org/W1997065057",
    "https://openalex.org/W2133358039",
    "https://openalex.org/W2107375983",
    "https://openalex.org/W205287285",
    "https://openalex.org/W1974921155",
    "https://openalex.org/W2041460347",
    "https://openalex.org/W1490309090",
    "https://openalex.org/W2094710333",
    "https://openalex.org/W1592715006",
    "https://openalex.org/W2013454029",
    "https://openalex.org/W1989433791",
    "https://openalex.org/W2910064571",
    "https://openalex.org/W1923701196",
    "https://openalex.org/W1981610984",
    "https://openalex.org/W4235846949",
    "https://openalex.org/W2201404374",
    "https://openalex.org/W2319964538",
    "https://openalex.org/W598719621",
    "https://openalex.org/W4254735570",
    "https://openalex.org/W182256368",
    "https://openalex.org/W4236230826",
    "https://openalex.org/W2173196798",
    "https://openalex.org/W1606694463",
    "https://openalex.org/W1633197",
    "https://openalex.org/W19399978",
    "https://openalex.org/W4250679610"
  ],
  "abstract": "Discovery tools are emerging in libraries. These tools offer library patrons the ability to concurrently search the library catalog and journal articles. While vendors rush to provide feature-rich interfaces and access to as much content as possible, librarians wonder about the usefulness of these tools to library patrons. In order to learn about both the utility and usability of EBSCO Discovery Service, James Madison University conducted a usability test with eight students and two faculty members. The test consisted of nine tasks focused on common patron requests or related to the utility of specific discovery tool features. Software recorded participants’ actions and time on task, human observers judged the success of each task, and a post-survey questionnaire gathered qualitative feedback and comments from the participants. Overall, participants were successful at most tasks, but specific usability problems suggested some interface changes for both EBSCO Discovery Service and JMU’s customizations of the tool. The study also raised several questions for libraries above and beyond any specific discovery tool interface, including the scope and purpose of a discovery tool versus other library systems, working with the large result sets made possible by discovery tools, and navigation between the tool and other library services and resources. This article will be of interest to those who are investigating discovery tools, selecting products, integrating discovery tools into a library web presence, or performing evaluations of similar systems.",
  "full_text": "Usability Test Results for a Discovery \nTool in an  \nAcademic Library \nJody Condit Fagan \nMeris Mandernach \nCarl S. Nelson \nJonathan R. Paulo \nGrover Saunders \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 83 \nABSTRACT \nDiscovery tools are emerging in libraries. These tools offer library patrons the ability to concurrently \nsearch the library catalog and journal articles. While vendors rush to provide feature-rich interfaces \nand access to as much content as possible, librarians wonder about the usefulness of these tools to \nlibrary patrons. To learn about both the utility and usability of EBSCO Discovery Service, James \nMadison University (JMU) conducted a usability test with eight students and two faculty members. \nThe test consisted of nine tasks focused on common patron requests or related to the utility of specific \ndiscovery tool features. Software recorded participants’ actions and time on task, human observers \njudged the success of each task, and a post–survey questionnaire gathered qualitative feedback and \ncomments from the participants. Participants were successful at most tasks, but specific usability \nproblems suggested some interface changes for both EBSCO Discovery Service and JMU’s \ncustomizations of the tool. The study also raised several questions for libraries above and beyond any \nspecific discovery-tool interface, including the scope and purpose of a discovery tool versus other \nlibrary systems, working with the large result sets made possible by discovery tools, and navigation \nbetween the tool and other library services and resources. This article will be of interest to those who \nare investigating discovery tools, selecting products, integrating discovery tools into a library web \npresence, or performing evaluations of similar systems. \nINTRODUCTION \nDiscovery tools appeared on the library scene shortly after the arrival of next-generation catalogs. \nThe authors of this paper define discovery tools as web software that searches journal-article and \nlibrary-catalog metadata in a unified index and presents search results in a single interface. This \ndiffers from federated search software, which searches multiple databases and aggregates the \nresults. Examples of discovery tools include Serials Solutions Summon, EBSCO Discovery Service,  \n \nJody Condit Fagan (faganjc@jmu.edu) is Director, Scholarly Content Systems, Meris Mandernach \n(manderma@jmu.edu) is Collection Management Librarian, Carl S. Nelson (nelsoncs@jmu.edu) is \nDigital User Experience Specialist, Jonathan R. Paulo (paulojr@jmu.edu) is Education Librarian, \nand Grover Saunders (saundebn@jmu.edu) is Web Media Developer, Carrier Library, James \nMadison University, Harrisonburg, VA. \n \n\n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 84 \n \nEx Libris Primo, and OCLC WorldCat Local; examples of federated search software include Serials \nSolutions WebFeat and EBSCO Integrated Search. With federated search software, results rely on \nthe search algorithm and relevance ranking as well as each tool’s algorithms and relevance \nrankings.  \n \nDiscovery tools, which import metadata into one index, apply one set of search algorithms to \nretrieve and rank results. This difference is important because it contributes to a fundamentally \ndifferent user experience in terms of speed, relevance, and ability to interact consistently with \nresults. Combining the library catalog, article indexes, and other source types in a unified interface \nis a big change for users because they no longer need to choose a specific search tool to begin their \nsearch. Research has shown that such a choice has long been in conflict with users’ expectations.1 \nFederated search software was unable to completely fulfill users’ expectations because of its \nlimited technology.2 Now that discovery tools provide a truly integrated search experience, with \ngreatly improved relevance rankings, response times, and increased consistency, libraries can \nfinally begin to meet this area of user expectation. However, discovery tools present new \nchallenges for users: will they be able to differentiate between source types in the integrated \nresults sets? Will they be able to limit large results sets effectively? Do they understand the scope \nof the tool and that other online resources exist outside the tool’s boundaries? \n \nThe sea change brought by discovery tools also raises challenges for librarians, who have grown \ncomfortable with the separation between the library catalog and other online databases. \nDiscovery tools may mask important differences between disciplinary searching, and they do not \ncurrently offer discipline-specific strategies or limits. They also lack authority control, which \nmakes topical precision a challenge. Their usual prominence on library websites may direct traffic \naway from carefully cultivated and organized collections of online resources. Discovery tools offer \nboth opportunities and challenges for library instruction, depending on the academic discipline, \nusers’ knowledge, and information-seeking need.  \nJames Madison University (JMU) is a predominantly undergraduate institution of approximately \n18,000 students in Virginia. JMU has a strong information literacy program integrated into the \ncurriculum through the university’s Information Seeking Skills Test (ISST). The ISST is completed \nbefore students are able to register for third-semester courses. Additionally, the library provides \nan information literacy tutorial, “Go for the Gold,” that supports the skills needed for the ISST. \nJMU launched EBSCO Discovery Service (EDS) in August 2010 after participating as a beta \ndevelopment partner in spring and summer 2010. As with other discovery tools, the predominant \nfeature of EDS is integration of the library catalog with article databases and other types of \nsources. At the time of this study, EDS had a few differentiating features. First, because of EBSCO’s \nbusiness as a database and journal provider, article metadata was drawn from a combination of \njournal-publisher information and abstracts and index records. The latter included robust subject \nindexing (e.g., the medical subject headings in CINAHL). The content searched by EDS varies by \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 85 \n \ninstitution according to the institution’s subscription. JMU had a large number of EBSCO databases \nand third-party database subscriptions through EBSCO, so the quantity of information searched by \nEDS at JMU is quite large.  \nEDS also allowed for extensive customization of the tool, including header navigation links, \nresults-screen layout, and the inclusion of widgets in the right-hand column of the results screen. \nJMU Libraries developed a custom “Quick Search” widget based on EDS for the library home page \n(see figure 1), which allows users to add limits to the discovery-tool search and assists with local \nauthentication requirements. Based on experience with a pilot test of the open-source VuFind \nnext-generation catalog, JMU Libraries believed users would find the ability to limit up-front \nuseful, so Quick Search’s first drop-down menu contained keyword, title, and author field limits; \nthe second drop-down contained limits for books, articles, scholarly articles, “Just LEO Library \nCatalog,” and the library website (which did not use EDS). The “Just LEO Library Catalog” option \nlimited the user’s search to the library catalog database records but used the EDS interface to \nperform the search. To access the native catalog interface, a link to LEO Library Catalog was \nincluded immediately above the search box as well as in the library website header.  \n \nFigure 1. Quick Search Widget on JMU Library Homepage \n\n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 86 \n \nEvaluation was included as part of the implementation process for the discovery tool, and \ntherefore a usability test was conducted in October 2010. The purpose of the study was to explore \nhow patrons used the discovery tool, to uncover any usability issues with the chosen system and \nto investigate user satisfaction. Specific tasks addressed the use of facets within the discovery tool, \npatrons’ use of date limiters, and the usability of the Quick Search widget. The usability test also \nhad tasks in which users were asked to locate books and articles using only the discovery tool, \nthen repeat the task using anything but the discovery tool. \nThis article interprets the usability study’s results in the context of other local usability tests and \nweb-usage data from the first semester of use. Some findings were used to implement changes to \nQuick Search and the library website, and to recommend changes to EBSCO; however, other \nfindings suggested general questions related to discovery tool software that libraries will need to \ninvestigate further. \nLITERATURE REVIEW \n \nLiterature reviewed for this article included some background reading on users and library \ncatalogs, library responses to users’ expectations, usability studies in libraries, and usability \nstudies of discovery tools specifically.  \n \nThe first group of articles comprised a discussion about the limitations of traditional library \ncatalogs. The strengths and weaknesses of library catalogs were reported in several academic \nlibraries’ usability studies.3 Calhoun recognized that library users’ preference for Google caused a \ndecline in the use and value of library catalogs, and encouraged library leaders to “establish the \ncatalog within the framework of online information discovery systems.” 4 This awareness of \nchanges in user expectations during a time when Google set the benchmark for search simplicity \nwas echoed by numerous authors who recognized the limits of library catalogs and expressed a \nneed for the catalog to be greatly modernized to keep pace with the evolution of the web.5  \n \nLibraries have responded in several ways to the call for modernization, most notably through \ninvestigations related to federated searching and next-generation catalogs. Several articles have \npresented usability studies results for various federated searching products.6 Fagan provided a \nthorough literature review of faceted browsing and next-generation catalogs.7 Western Michigan \nUniversity presented usability study results for the next-generation catalog VuFind, revealing that \nparticipants took advantage of the simple search box but did not use the next-generation catalog \nfeatures of tagging, comments, favorites, and SMS texting.8 The University of Minnesota conducted \ntwo usability studies of Primo and reported that participants were satisfied with using Primo to \nfind known print items, limit by author and date, and find a journal title.9 Tod Olson conducted a \nstudy with graduate students and faculty using the AquaBrowser interface, and his participants \nlocated sources for their research they had not previously been able to find.10  \n \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 87 \n \nThe literature also revealed both opportunities and limitations of federated searching and next-\ngeneration catalogs. Allison presented statistics from Google Analytics for an implementation of \nEncore at the University of Nebraska-Lincoln. 11 The usage statistics revealed an increased use of \narticle databases as well as an increased use of narrowing facets such as format and media type, \nand library location. Allison concluded that Encore increased users’ exposure to the entire \ncollection. Breeding concluded that federated searching had various limitations, especially search \nspeed and interface design, and was thus unable to compete with Google Scholar.12 Usability \nstudies of next-generation catalogs revealed a lack of features necessary to fully incorporate an \nentire library’s collection. Breeding also recognized the limitations of next-generation library \ncatalogs and saw discovery tools as their next step in evolution: “It’s all about helping users \ndiscover library content in all formats, regardless of whether it resides within the physical library \nor among its collections of electronic content, spanning both locally owned materials and those \naccessed remotely through subscriptions.” 13 \n \nThe dominant literature related to discovery tools discussed features,14 reviewed them from a \nlibrary selector perspective,15 summarized academic libraries’ decisions following selection,16 \npresented questions related to evaluation after selection,17 and offered a thorough evaluation of \ncommon features.18 Allison concluded that “usability testing will help clarify what aspects need \nimprovement, what additions will make [the interface] more useful, and how the interface can be \nmade so intuitive that user training is not needed.”19 Breeding noted “it will only be through the \nexperience of library users that these products will either prove themselves or not.”20  \n \nLibraries have been adapting techniques from the field of usability testing for over a decade to \nlearn more about user behavior, usability, and user satisfaction, with library web sites and \nsystems. 21 Rubin and Chisnell and Dumas and Redish provided an authoritative overview of the \nbenefits and best practices of usability testing. 22 In addition, Campbell and Norlin and Winters \noffered specific usability methodologies for libraries.23  \n \nWorldCat Local has dominated usability studies of discovery tools published to date. Ward, Shadle, \nand Mofield conducted a usability study at the University of Washington.24 Although the second \nround of testing was not published, the first round involved seven undergraduate and three \ngraduate students; its purpose “was to determine how successful UW students would be in using \nWorldCat Local to discover and obtain books and journal articles (in both print and electronic \nform) from the UW collection, from the Summit consortium, and from other WorldCat libraries.”25 \nAlthough participants were successful at completing these tasks, a few issues arose out of the \nusability study. Users had difficulty with the brief item display because reviews were listed higher \nthan the actual items. The detailed item display also hindered users’ ability to decipher between \nvarious editions and formats. The second round of usability testing, not yet published, included \ntasks related to finding materials on specific subject areas.  \n \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 88 \n \nBoock, Chadwell, and Reese conducted a usability study of WorldCat Local at Oregon State \nUniversity.26 The study included four tasks and five evaluative questions. Forty undergraduate \nstudents, sixteen graduate students, twenty-four library employees, four instructors, and eighteen \nfaculty members took part in the study. They summarized that users found known-title searching \nto be easier in the library catalog but found topical searches to be more effective in WorldCat \nLocal.The participants preferred WorldCat Local for the ability to find articles and search for \nmaterials in other institutions. \n \nWestern Washington University also conducted a usability study of WorldCat Local. They selected \ntwenty-four participants with a wide range of academic experience to conduct twenty tasks in \nboth WorldCat Local and the traditional library catalog.27 The comparison revealed several \nproblems in using WorldCat Local, including users’ inability to determine the scope of the content, \nconfusion over the intermixing of formats, problems with the display of facet option, and difficulty \nwith known-item searches. Western Washington University decided not to implement WorldCat \nLocal.  \n \nOCLC published a thorough summary of several usability studies conducted mostly with academic \nlibraries piloting the tool, including the University of Washington; the University of California \n(Berkeley, Davis, and Irvine campuses); Ohio State University; the Peninsula Library System in San \nMateo, California; and the Free Library of Urbana and the Des Plaines Public Library, both in \nIllinois.28 The report conveyed favorable user interest in searching local, group, and global \ncollections together. Users also appreciated the ability to search articles and books together. The \nauthors commented, “however, most academic participants in one test (nine of fourteen) wrongly \nassumed that journal article coverage includes all the licensed content available at their \ncampuses.”29 OCLC used the testing results to improve the order of search results, provide clarity \nabout various editions, improve facets for narrowing a search, provide links to electronic \nresources, and increase visibility of search terms. \n \nAt Grand Valley State University, Doug Way conducted an analysis of usage statistics after \nimplementing the discovery tool Summon in 2009; the usage statistics revealed an increased use \nof full-text downloads and link resolver software but a decrease in the use of core subject \ndatabases.30 The usage statistics showed promising results, but Way recommended further studies \nof usage statistics over a longer period of time to better understand how discovery tools affect \nentire library collections. North Carolina State University Libraries released a final report about \ntheir usability study of Summon.31 The results of these usability studies were similar to other \nstudies of discovery tools: users were satisfied with the ability to search the library catalog and \narticle databases with a single search, but users had mixed results with known-item searching and \nconfusion about narrowing facets and results ranking. Although several additional academic \nlibraries have conducted usability studies of Encore, Summon, and EBSCO Discovery Service, the \nresults have not yet been published.32 \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 89 \n \n \nOnly one usability study of EBSCO Discovery Service was found. In a study with six participants, \nWilliams and Foster found users were satisfied and able to adapt to the new system quickly but \ndid not take full advantage of the rich feature set.33 \n \nCombined with the rapid changes in these tools, the literature illustrates a current need for more \nusability studies related to discovery tools. The necessary focus on specific software \nimplementations and different study designs make it difficult to identify common themes. \nAdditional usability studies will offer greater breadth and depth to the current dialogue about \ndiscovery tools. This article will help fill the gap by presenting results from a usability study of \nEBSCO Discovery Service. Publishing such usability results of discovery tools will inform \ninstitutional decisions, improve user experiences, and advance the tools’ content, features, and \ninterface design. In addition, libraries will be able to more thoroughly modernize library catalogs \nto meet users’ changing needs and expectations as well as keep pace with the evolution of the web.  \n \nMETHOD \n \nJames Madison University Libraries’ usability lab features one workstation with two pieces of \nusability software: Techsmith’s Morae (version 3) (http://www.techsmith.com/morae.asp), which \nrecords screen captures of participant actions during the usability studies, and the Usability \nTesting Environment (UTE) (version 3), which presents participants with tasks in a web-browser \nenvironment. The UTE also presents end-of-task questions to measure time on task and task \nsuccess.  \nThe study of EDS, conducted in October 2010, was covered by an institutional review board–\napproved protocol. Participants were recruited for the study through a bulk email sent to all \nstudents and faculty. Interested respondents were randomly selected to include a variety of grade \nlevels and majors for students and years of service and disciplines taught for faculty members.  \nThe study included ten participants with ranging levels of experience: two freshman, two \nsophomores, two juniors, one senior, one graduate student, and two faculty members. Three of the \nparticipants were from the school of business, one from education, two from the arts and \nhumanities, and two from the sciences. The remaining two participants had dual majors in the \nhumanities and the sciences. A usability rule of thumb is that at least five users will reveal more \nthan 75 percent of usability issues.34 Because the goal was to observe a wide range of user \nbehaviors and usability issues, and to gather data about satisfaction from a variety of perspectives, \nthis study used two users of each grade level plus two faculty participants (for a total of ten) to \nprovide as much heterogeneity as possible.  \nStudent participants were presented with ten pre–study questions, and faculty participants were \nasked nine pre–study questions (see appendix A). The pre–study questions were intended to \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 90 \n \ngather information about participants’ background, including their time at JMU, their academic \ndiscipline, and their experience with the library website, the EBSCOhost interface, the library \ncatalog, and library instruction. Since participants were anonymous, we hoped their answers \nwould help us interpret unusual comments or findings. Pre–test results were not used to form \ncomparison groups (e.g., freshmen versus senior) because these groups would not be \nrepresentative of their larger populations. These questions were followed by a practice task to \nhelp familiarize participants with the testing software.  \nThe study consisted of nine tasks designed to showcase usability issues, show the researchers how \nusers behaved in the system, and measure user satisfaction. Appendix B lists the tasks and what \nthey were intended to measure. In designing the test, determining success on some tasks seemed \nvery objective (find a video about a given topic) while others appeared to be more subjective \n(those involving relevance judgments). For this reason, we asked participants to provide \nsatisfaction information on some tasks and not others. In retrospect, for consistency of \ninterpretation, we probably should have asked participants to rate or comment on every task. All \nof the tasks were presented in the same order. Tasks were completed either by clicking “Answer” \nand answering a question (multiple choice or typed response), or by clicking “Finished” after \nnavigating to a particular webpage. Participants also had the option to skip the task they were \nworking on and move to the next task. Allowing participants to skip a task helps differentiate \nbetween genuinely incorrect answers and incorrect answers due to participant frustration or \nguessing. A time limit of 5 minutes was set for tasks 1–7, while tasks 8 and 9 were given time \nlimits of 8 minutes, after which the participant was timed out. Time limits were used to ensure \nparticipants were able to complete all tasks within the agreed-upon session. Average time on task \nacross all tasks was 1 minute, 35 seconds.  \nAfter the study was completed, participants were presented with the System Usability Scale (SUS), \na ten-item scale using statements of subjective assessment and covering a variety of aspects of \nsystem usability.35 SUS scores, which provide a numerical score out of 100, are affected by the \ncomplexity of both the system and the tasks users may have performed before taking the SUS. The \nSUS was followed by a post–test consisting of six open-ended questions, plus one additional \nquestion for faculty participants, intended to gather more qualitative feedback about user \nsatisfaction with the system (see appendix A). \nA technical glitch with the UTE software affected the study in two ways. First, on seven of the \nninety tasks, the UTE failed to enforce the five-minute maximum time limit, and participants \nexceeding a task’s time limit were allowed to continue the task until they completed or skipped \nthe task. One participant exceeded the time limit on task 1 while three of these errors occurred \nduring both tasks 8 and 9. This problem potentially limits the ability to compare the average time \non task across tasks; however, since this study used time on task in a descriptive rather than \ncomparative way, the impact on interpreting results is minimal. The seven instances in which the \nglitch occurred were included in the average time on task data found in figure 3 because the times \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 91 \n \nwere not extreme and the time limit had been imposed mostly to be sure participants had time to \ncomplete all the tasks. A second problem with the UTE was that it randomly and prematurely \naborted some users’ tasks; when this happened, participants were informed that their time had \nrun out and were then moved on to the next task. This problem is more serious because it is \nunknown how much more time or effort the participant would have spent on the task or whether \nthey would have been more successful. Because of this, the results below specify how many \nparticipants were affected for each task. Although this was unfortunate, the results of the \nparticipants who did not experience this problem still provide useful cases of user behavior, \nespecially because this study does not attempt to generalize observed behavior or usability issues \nto the larger population. Although a participant mentioned a few technical glitches during testing \nto the facilitator, the extent of software errors was not discovered until after the tests were \ncomplete (and the semester was over) because the facilitator did not directly observe participants \nduring sessions.  \nRESULTS \nThe participants were asked several pre–test questions to learn about their research habits. All \nbut one participant indicated they used the library website no more than six times per month (see \nfigure 2). Common tasks this study’s student participants said they performed on the website \nwere searching for books and articles, searching for music scores, “research using databases,” and \nchecking library hours. The two faculty participants mentioned book and database searches, \nelectronic journal access, and interlibrary loan. Participants were shown the Quick Search widget \nand were asked “how much of the library’s resources do you think the Quick Search will search?” \nSeven participants said “most”; only one person, a faculty member, said it would search “all” the \nlibrary’s resources.  \n \nFigure 2. Monthly Visits to Library Website \n< 1 \nvisit (2)\n1 - 3 \nvisits (4)\n4 - 6 \nvisits (3)\n> 7 visits (1)\n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 92 \n \nWhen shown screenshots of the library catalog and an EBSCOhost database, seven participants \nwere sure they had used LEO Library Catalog, and three were not sure. Three indicated that they \nhad used an EBSCO database before, five had not, and two were not sure. Participants were also \nasked how often they had used library resources for assignments in their major field of study; four \nsaid “often,” two said “sometimes,” one “rarely/never,” and one “very often.” Students were also \nasked “has a librarian spoken to a class you’ve attended about library research?” and two said yes, \nfive said no, and one was not sure. \nA “practice task” was administered to ensure participants were comfortable with the workstation \nand software: “Use Quick Search to search a topic relating to your major/discipline or another \ntopic of interest to you. If you were writing a paper on this topic how satisfied would you be with \nthese results?” No one selected “no opinion” or very unsatisfied”; Sixty percent were “very \nsatisfied” or “satisfied” with their results; forty percent were “somewhat unsatisfied.” \nFigure 3 shows the time spent on each task, while figure 4 describes participants’ success on the \ntasks.  \n \n  \nTask \n1 \nTask \n2 \nTask \n3 \nTask \n4 \nTask \n5 \nTask \n6 \nTask \n7 \nTask \n8 \nTask \n9 \nNo. Of Responses \n(not including \ntimeouts) 10 9 5 7 9 10 10 8 10 \nAvg. Time on Task \n(in seconds) 175* 123 116 97 34 120 92 252* 255* \nStandard Deviation 212 43 50 49 26 36 51 177 174 \n*Includes time(s) in excess of the set time limit. Excess time allowed by \nsoftware error.     \nFigure 3. Average Time Spent on Tasks \n175\n123 116 97\n34\n120\n92\n292\n255\n0\n50\n100\n150\n200\n250\n300\n350\nTask 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9\nTime on Task (in seconds)\nAverage Time for All Tasks\n(not including timeouts)\n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 93 \n \nThe first task (“What was the last thing you searched for when doing a research assignment for \nclass? Use Quick Search to re-search for this.”) started participants on the library homepage. \nParticipants were then asked to “Tell us how this compared to your previous experience” using a \ntext box. The average time on task was almost 2 minutes; however one faculty participant took \nmore than 12 minutes on this task; if his or her time was removed, the time on task average was 1 \nminute, 23 seconds. Figure 5 shows the participants’ search terms and their comments.  \n  Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 \nHow \nSuccess \nDeter-\nmined \nUsers \nonly \nasked to \nprovide \nfeedback \nValid \ntyped-in \nresponse \nprovided \nHow many \nsubtasks \ncompleted \n(out of 3) \nHow many \nsubtasks \ncompleted \n(out of 2) \nCorrect \nmultiple \nchoice \nanswer \nHow many \nsubtasks \ncompleted \n(out of 2) \nEnd task \nat correct \nweb \nlocation \nHow \nmany \nsubtasks \ncomplete\nd (out of \n4) \nHow many \nsubtasks \ncompleted \n(out of 4) \nP01 N/A Correct 3 2 TIMEOUT 2 Correct 0* 0** \nP02 N/A Correct 3* 1 Correct 2 Correct 0** 3 \nP03 N/A Correct 0* 1 Incorrect 2 Correct 4 3 \nP04 N/A Correct 2 0* Correct 2 SKIP 3 2 \nP05 N/A Correct* 2 2 Correct 1 Correct 4 2 \nP06 N/A Correct 3* 1 Correct 1 Correct 3 0** \nP07 N/A Correct 2 1* Correct 1 Correct 0 2 \nP08 N/A Correct 2 0* Correct 0 SKIP TIMEOUT 0** \nP09 N/A Correct 2* SKIP Correct 2 Correct 4 2 \nP10 N/A Correct 1* 1 Correct 2 SKIP 4 2 \nNote: “TIMEOUT” indicates an immediate timeout error. Users were unable to take any action on \nthe task. \n*User experienced a timeout error while working on the task. This may have affected their ability \nto complete the task. \n**User did not follow directions. \nFigure 4. Participants’ Success on Tasks \n  \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 94 \n \n \nParticipant JMU Status Major/Discipline Search Terms \nP01 Faculty Geology large low shear wave velocity province \n  Comments: Ebsco did a fairly complete job. There were some irrelevant results that I \ndon’t remember seeing when I used GeoRef. \nP02 Faculty Computer \nInformation Systems \n& Management \nScience (statistics) \nstudent cheating \n  Comments: This is a topic that I am somewhat familiar with the related literature. I was \npleased with the diversity of journals that were found in the search. The \ntopics of the articles was right on target. The recency of the articles was \ngreat. This is a topic for which I am somewhat familiar with the related \nliterature. I was impressed with the search results regarding: diversity of \njournals; recency of articles; just the topic in articles I was looking for. \nP03 Graduate \nStudent \nEducation Death of a Salesman \n  Comments: There is a lot of variety in the types of sources that Quick Search is pulling up \nnow. I would still have liked to see more critical sources on the play but I \ncould probably have found more results of that nature with a better search \nterm, such as “death of a salesman criticism.” \nP04 1st year Voice Performance current issues in Russia \n  Comments: It was somewhat helpful in the way that it gave me information about what \nhad happened in the past couple months, but not what was happening now \nin russia. \nP05 3rd year Nursing uninsured and health care reform \n  Comments: The quick search gave very detailed articles I thought, which could be good, \nbut were not exactly what I was looking for. Then again, I didn’t read all \nthese articles either \nP06 1st year History headscarf law \n  Comments: This search yielded more results related to my topic. I needed other sources \nfor an argument on the French creating law banning religious dress and \nsymbols in school. Using other methods with the same keyword, I had an \nenormous amount of trouble finding articles that pertained to my essay. \nP07 3rd year English Jung \n  Comments: I like the fact that it can be so defined to help me get exactly what I need. \nP08 4th year Spanish restaurant industry \n  Comments: This is about the same as the last time that I researched this topic. \nP09 2nd year Hospitality aphasia \n  Comments: There are many good sources, however there are also completely irrelevant \nsources. \nP10 2nd year Management Rogers five types of feedback \n  Comments: There is not many documents on the topic I searched for. This may be \nbecause the topic is not popular or my search is not specific/too specific. \nFigure 5. Participants’ Search Terms and Comments \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 95 \n \nThe second task started on the library homepage and asked participants to find a video related to \nearly childhood cognitive development. This task was chosen because JMU Libraries have \nsignificant video collections and because the research team hypothesized users might have trouble \nbecause there was no explicit way to limit to videos at the time. The average time on this task was \ntwo minutes, with one person experiencing an arbitrary time out by the software. Participants \nwere judged to be successful on this task by the researchers if they found any video related to the \ntopic. All participants were successful on this task, but four entered, then left the discovery tool \ninterface to complete the task. Five participants looked for a video search option in the drop-down \nmenu, and of these, three immediately used something other than Quick Search when they saw \nthat there was no video search option. Of those who tried Quick Search, six opened the source type \nfacet in EDS search results and four selected a source type limit, but only two selected a source \ntype that led directly to success (“non-print resources”). \nTask 3 started participants in EDS (see figure 6) and asked them to search on speech pathology, \nfind a way to limit search results to audiology, and limit their search results to peer-reviewed \nsources. Participants spent an average of 1 minute, 40 seconds on this task, with five participants \nbeing artificially timed out by the software. Participants’ success on this task was determined by \nthe researchers’ examination of the number of subtasks they completed. The three subtasks \nconsisted of successfully searching for the given topic (speech language pathology) limiting the \nsearch results to audiology, and further limiting the results to peer reviewed sources. Four \nparticipants were able to complete all three subtasks, including two who were timed out. (The \ntimes for those who were timed out were not included in time on task averages, but they were \ngiven credit for success.) Five completed just two of the subtasks, failing to limit to peerreviewed; \none of these because of a timeout. It was unclear why the remaining participants did not attempt \nto alter the search results to “peer reviewed.” Looking at the performed actions, six of the ten \ntyped “AND audiology” into search keywords to narrow the search results, while one found and \nused “audiology” in the Subject facet on the search results page. Six participants found and used \nthe “Scholarly (Peer Reviewed) Journals” checkbox limiter. \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 96 \n \n \nFigure 6. EBSCO Discovery Service Interface\nBeginning with the results they had from task 3, task 4 asked participants to find more recent \nsources and to select the most recent source available. Task success was measured by correct \ncompletion of two subtasks: limiting the search results to the last five years and finding the most \nrecent source. The average time on task was 1 minute, 14 seconds, with three artificial timeouts. \nOf those who did not time out, all seven were able to limit their sources to be more recent in some \nway, but only three were able to select the most recent source. In addition to this being a common \nresearch task, the team was interested to see how users accomplished this task. Three typed in the \nlimiter in the left-hand column, two typed in the limiter on the advanced search screen, and two \nused the date slider. Two participants used the “sort” drop-down menu to change the sort order to \n“Date Descending,” which helped them complete this task. Other participants changed the dates, \nand then selected the first result, which was not the most recent.  \nTask 5, which started within EDS, asked participants to find a way to ask a JMU librarian for help. \nThe success of this task was measured by whether they reached the correct URL for the Ask-a-\n\n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 97 \n \nLibrarian page; eight of the ten participants were successful. This task took an average of only 31 \nseconds to complete, and eight of the ten used the Ask-a-Librarian link at the top of the page. Of \nthe two unsuccessful participants, one was timed out, while another clicked “search modes” for no \napparent reason, then clicked back and decided to finish the task. \nTask 6 started in the EDS interface and asked participants to locate the journal Yachting and \nBoating World and select the correct coverage dates and online status from a list of four options; \nparticipants were deemed successful at two subtasks if they selected the correct option and \nsuccessful at one subtask if they chose an option that was partially correct. Participants took an \naverage of two minutes on this task; only five answered correctly. During this task, three \nparticipants used the EBSCO search option “SO Journal Title/Source,” four used quotation marks, \nand four searched or re-searched with the “Title” drop-down menu option. Three chose the \ncorrect dates of coverage, but were unable to correctly identify the online availability. It is \nimportant to note that only searching and locating the journal title were accomplished with the \ndiscovery tool; to see dates of coverage and online availability, users clicked JMU’s link resolver \nbutton, and the resulting screen was served from Serials Solutions’ Article Linker product. \nAlthough some users spent more time than perhaps was necessary using the EDS search options \nto locate the journal, the real barriers to this task were encountered when trying to interpret the \nSerials Solutions screen.  \nTask 7, where participants started in EDS, was designed to determine whether users could \nnavigate to a research database outside of EDS. Users were asked to look up the sculpture Genius \nof Mirth and were told the library database Camio would be the best place to search. They were \ninstructed to “locate this database and find the sculpture.” The researcher observed the recordings \nto determine success on this task, which was defined as using Camio to find the sculpture. \nParticipants took an average of 1 minute, 32 seconds on this task; seven were observed to \ncomplete the task successfully, while three chose to skip the task. To accomplish this task, seven \nparticipants used the JMU Research Databases link in the header navigation at some point, but \nonly four began the task by doing this. Six participants began by searching within EDS.  \nThe final two tasks started on the library homepage and were a pair: participants were asked to \nfind two books and two recent, peer-reviewed articles (from the last five years) on rheumatoid \narthritis. Task 8 asked them to use the library’s EDS widget, Quick Search, to accomplish this, and \ntask 9 asked them to accomplish the same task without using Quick Search. When they found \nsources, they were asked to enter the four relevant titles in a text-entry box. The average time \nspent on these tasks was similar: about four minutes per task. Comparing these tasks was \nsomewhat confusing because some participants did not follow instructions. User success was \ndetermined by the researchers’ observation of how many of the four subtasks the user was able to \ncomplete successfully: find two books, find two articles, limit to peer reviewed, and select articles \nfrom last five years (with or without using a limiter); figure 4 shows their success.  \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 98 \n \nLooking at the seven users who used Quick Search on the Quick Search tasks, six limited to \n“Scholarly (Peer Reviewed) Journals”; six limited to the last five years; and seven narrowed results \nusing the source type facet. The average number of subtasks completed on task eight was 3.14 out \nof 4. Looking at the seven users who followed instructions and did not use Quick Search on task 9, \nall began with the library catalog and tried to locate articles within the library catalog. The average \nnumber of subtasks completed on task 9 was 2.29 out of 4. Some users tried to locate articles by \nsetting the catalog’s material type drop-down menu to “Periodicals” and others used the catalog’s \n“Periodical” tab, which performed a title keyword search of the e-journal portal. For task 9, only \ntwo users eventually chose a research database to find articles. User behavior can only be \ncompared for the six users (all students) who followed instructions on both tasks; a summary is \nprovided in figure 4.  \nAfter completing all nine tasks, participants were presented with the System Usability Scale. EDS \nscored 56 out of 100. Following the SUS, participants were asked a series of post–test questions. \nOnly one of the faculty members chose to answer the post–test questions. When asked how they \nwould use Quick Search, all eight students explicitly mentioned class assignments, and the \nparticipating faculty member replied “to search for books.” Two students mentioned books \nspecifically, while the rest used the more generic term “sources” to describe items for which they \nwould search. When asked “when would you not use this search tool?” the faculty member said “I \nwould just have to get used to using it. I mainly go to [the library catalog] and then research \ndatabases.” Responses from the six students who answered this question were vague and hard to \ncategorize: \n• “Not really sure for more general question/learning” \n• “When just browsing” \n• “For quick answers” \n• “If I could look up the information on the internet” \n• “When the material I need is broad” \n• “Basic searching when you do not need to say where you got the info from” \nWhen asked for the advantages of Quick Search, four specifically mentioned the ability to narrow \nresults, three respondents mentioned “speed,” three mentioned ease of use, and three mentioned \nrelevance in some way (e.g., “it does a pretty good job associating keywords with sources”). Two \nmentioned the broad coverage and one compared it to Google, “which is what students are looking \nfor.” When asked to list disadvantages, the faculty member mentioned he/she was not sure what \npart of the library home page was actually “Quick Search,” and was not sure how to get to his/her \nlibrary account. Three students talked about Quick Search being “overwhelming” or “confusing” \nbecause of the many features, although one of these also stated, “like anything you need to learn in \norder to use it efficiently.” One student mentioned the lack of an audio recording limit and another \nsaid “when the search results come up it is hard to tell if they are usable results.” \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 99 \n \nKnowing that Quick Search may not always provide the best results, the research team also asked \nusers what they would do if they were unable to find an item using Quick Search. A faculty \nparticipant said he or she would log into the library catalog and start from there. Five students \nmentioned consulting a library staff member in some fashion. Three mentioned moving on from \nlibrary resources, although not necessarily as their first step. One said “find out more information \non it to help narrow down my search.” Only one student mentioned the library catalog or any \nother specific library resource.  \nWhen participants were asked if “Quick Search” was an appropriate name, seven agreed that it \nwas. Of those who did not agree, one participant’s comment was “not really, though I don’t think it \nmatters.” And another’s was “I think it represents the idea of the search, but not the action. It could \nbe quicker.” The only alternative name suggestion was “Search Tool.” \nWeb Traffic Analysis \nWeb traffic through Quick Search and in EDS provides additional context for this study’s results. \nDuring August–December 2010, Quick Search was searched 81,841 times from the library \nhomepage. This is an increase from traffic into the previous widget in this location that searched \nthe catalog, which received 41,740 searches during the same period in 2009. Even adjusting for an \napproximately 22 percent increase in website traffic from 2009 to 2010, this is an increase of 75 \npercent. Interestingly, the traffic to the most popular link on the library homepage, Research \nDatabases, went from 55,891 in 2009 to 30,616 in 2010, a decrease of 55 percent when adjusting \nfor the change in website traffic. \nDuring fall 2010, 28 percent of Quick Search searches from the homepage were executed using at \nleast one drop-down menu. Twelve percent changed Quick Search’s first drop-down menu to \nsomething other than the keyword default, with “title” being the most popular option (7 percent of \nsearches) followed by author (4 percent of searches). Twenty percent of users changed the second \ndrop-down option; “Just Articles” and “Just Books” were the most popular options, garnering 7 \npercent and 6 percent of searches, respectively, followed by “Just Scholarly Articles,” which \naccounted for 4 percent of searches.  \nLooking at EBSCO’s statistical reports for JMU’s implementation of EDS, there were 85,835 \nsessions and approximately 195,400 searches during August–December 2010. This means about \n95 percent of EDS sessions were launched using Quick Search from the homepage. There were an \naverage of 2.3 searches per session, which is comparable to past behavior in JMU’s other \nEBSCOhost databases.  \n \nDISCUSSION \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 100 \n \nThe goal of this study was to gather initial data about user behavior, usability issues, and user \nsatisfaction with discovery tools. The task design and technical limitations of the study mean that \ncomparing time on task between participants or tasks would not be particularly illuminating; and, \nwhile the success rates on tasks are interesting, they are not generalizable to the larger JMU \npopulation. Instead, this study provided observations of user behavior that librarians can use to \nimprove services, it suggested some “quick fixes” to usability issues, and it pointed to several \nresearch questions. When possible, these observations are supplemented by comparisons \nbetween this study and the only other published usability study of EDS.36  \nThis study confirmed a previous finding of user studies of federated search software and \ndiscovery tools: students have trouble determining what is searched by various systems.37 On the \ntasks in which they were asked to not use Quick Search to find articles, participants tried to search \nfor articles in the library catalog. Although all but one of this study’s participants correctly \nanswered that Quick Search did not search “all” library resources, seven thought it searched \n“most.” Both “most” or “some” would be considered correct; however, it is interesting that \nanswering this question more specifically is challenging even for librarians. Many journals in \nsubject article indexes and abstracts are included in the EDS Foundation Index; furthermore, \nJMU’s implementation of EDS includes all of JMU’s EBSCO subscription resources as well, making it \nimpractical to assemble a master list of indexed titles. Of course, there are numerous online \nresources with contents which may never be included in a discovery tool, such as political voting \nrecords, ethnographic files, and financial data. Users often have access to these resources through \ntheir library. However, if they do not know the library has a database of financial data, they will \ncertainly not consider this content in their response to a question of how many of the library \nresources are included in the discovery tool. As discovery tools begin to fulfill users’ expectations \nfor a “single search,” libraries will need to share best practices for showcasing valuable, useful \ncollections that fall outside the discovery tool’s scope or abilities. This is especially critical when \nreviewing the 72 percent increase in homepage traffic to the homepage search widget compared \nwith the 55 percent decrease in homepage traffic to the research databases page. It is important to \nnote these trends do not mean the library’s other research databases have fallen in usage by 55 \npercent. Though there was not a comprehensive examination of usage statistics, spot-checking \nsuggested EBSCO and non-EBSCO subject databases had both increases and decreases in usage \nfrom previous years.  \nAnother issue libraries should consider, especially when preparing for instruction classes, is that \nusers do not seem to understand which information needs are suited to a discovery tool versus \nthe catalog or subject-specific databases. Several tasks provided additional information about \nusers’ mental models of the tool, which may help libraries make better decisions about navigation \ncustomizations in discovery tool interfaces and on library websites. Task 7 was designed to \ndiscover whether users could find their way to a database outside of EDS if they knew they needed \nto use a specific database. Six participants, including one of the faculty members, began by \nsearching EDS for the name of the sculpture and/or the database name. On task 1, a graduate \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 101 \n \nstudent who searched on “Death of a Salesman” and was asked to comment on how Quick Search \nresults compared to his or her previous experience, said, “I would still have liked to see more \ncritical sources on the play but I could probably have found more results of that nature with a \nbetter search term, such as ‘death of a salesman criticism.’” While true, most librarians would \nsuggest using a literary criticism database, which would target this information need.  \nLibrarians may have differing opinions regarding the best research starting point, but their \nrationale would be much different than that of the students in this study. This study’s participants \nsaid they would use Quick Search/EDS when they were doing class work or research, but would \nnot use it for general inquiries. If librarians were to list which user information needs are best met \nby a discovery tool versus a subject-specific database, the types of information needs listed would \nbe much more numerous and diverse, regardless of differences over how to classify them.  \nIn addition to helping users choose between a discovery tool or a subject-specific database, \nlibraries will need to conceptualize how users will move in and out of the discovery tool to other \nlibrary resources, services, and user accounts. While users had no trouble finding the Ask-a-\nLibrarian link in the header, it might have been more informative if users started from a search-\nresults page to see if they would find the right-hand column’s Ask-a-Librarian link or links to \nlibrary subject guides and database lists. Discovery tools vary in their abilities to connect users \nwith their online library accounts and are changing quickly in this area.  \nThis study also provided some interesting observations about discovery tool interfaces. The \ndefault setting for EBSCO Discovery Service is a single search box. However, this study suggests \nthat while users desire a single search, they are willing to use multiple interface options. This was \nsupported by log analysis of the library’s locally developed entry widget, Quick Search, in which \n28 percent of searches included the use of a drop-down menu. On the first usability task, users left \nQuick Search’s options set to the default. On other tasks, participants frequently used the drop-\ndown menus and limiters in both Quick Search and EDS. For example, on task 2, which asked them \nto look for videos, five users looked in the Quick Search format drop-down menu. On the same task \nwithin EDS, six users attempted to use the source type facet. Use of limiters was similarly \nobserved by Williams and Foster in their EDS usability study.38 \nOne EDS interface option that was not obvious to participants was the link to change the sort \norder. When asked to find the most recent article, only two participants changed the sort option. \nMost others used the date input boxes to limit their search, then selected the first result even \nthought it was not the most recent one. It is unclear whether the participant assumed the first \nresult was the most recent or whether they could not figure out how to display the most recent \nsources. \nFinding a journal title from library homepages has long been a difficult task,39 and this study \nprovided no exception, even with the addition of a discovery tool. It is important to note that the \nstandard EDS implementation would include a “Publications” or “Journals A–Z” link in the header; \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 102 \n \nin EDS, libraries can customize the text of this link. JMU did not have this type of link enabled in \nour test, since the hope was that users could find journal titles within the EDS results. However, \nneither EDS nor the Quick Search widget’s search interfaces offered a way to limit the search to a \njournal title at the time of this study. During the usability test, four participants changed the field \nsearch drop-down menu to “Title” in EDS, and three participants changed the EDS field search \ndrop-down menu to “SO Journal Title/Source,” which limits the search to articles within that \njournal title. While both of these ideas were good, neither one resulted in a precise results set in \nEDS for this task unless the user also limited to “JMU Catalog Only,” a nonintuitive option. Since the \ntest, JMU has added a “Journal Titles” option to Quick Search that launches the user’s search into \nthe journal A–Z list (provided by Serials Solutions). In two months after the change (February and \nMarch 2011), only 391 searches were performed with this option. This was less than 1 percent of \nall searches, indicating that while it may be an important task, it is not a popular one.  \nLike many libraries with discovery tools, JMU added federated search capabilities to EDS using \nEBSCOhost Integrated Search software in an attempt to draw some traffic to databases not \nincluded in EDS (or not subscribed to through EBSCO by JMU), such as MLA International \nBibliography, Scopus, and Credo Reference. Links to these databases appeared in the upper-right-\nhand column of EDS during the usability study (see figure 6.) Usage data from EBSCO showed that \nless than 1 percent of all JMU’s EDS sessions for fall 2010 included any interaction with this area. \nLikewise, Williams and Foster observed their participants did not use their federated search until \nexplicitly asked to do so.40 Perhaps users faced with discovery tool results simply have no \nmotivation to click on additional database results. Since the usability test, JMU has replaced the \nright-hand column with static links to Ask-a-Librarian, subject guides, and research database lists.  \nReaders may wonder why one of the most common tasks, finding a specific book title, was not \nincluded in this usability study; this was because JMU Libraries posed this task in a concurrent \nhomepage usability study. On that study, twenty of the twenty-five participants used Quick Search \nto find the title “Pigs in Heaven” and choose the correct call number. Eleven of the twenty used the \nQuick Search drop-down menu to choose a title search option, further confirming users’ \nwillingness to limit up-front. The average time on this task was just under a minute, and all \nparticipants completed this task successfully, so this task was not repeated in the EDS usability \ntest. Other studies have reported trouble with this type of task;41 much could depend on the item \nchosen as well as the tool’s relevance ranking. \nUser satisfaction with EDS can be summarized from the open-ended post–study questions, from \nthe responses to task 1 (figure 5), and the SUS scale. Answers to the post–study questions \nindicated participants liked the ability to narrow results, the speed and ease of use, and relevance \nof the system. A few participants did describe the system as being “overwhelming” or “confusing” \nbecause of the many features, which was also supported by the SUS scores. JMU has been using the \nSUS to understand the relative usability of library systems. The SUS offers a benchmark for system \nimprovement; for example, EBSCO Discovery Service received an SUS of only 37 in spring 2010 (N \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 103 \n \n= 7) but a 56 on this study in fall 2010 (N = 10). This suggests the interface has become more \nusable. In 2009, JMU Libraries also used the SUS to test the library catalog’s classic interface as \nwell as a VuFind interface to the library catalog, which received scores of 68 (N = 15) and 80 (N = \n14), respectively. The differences between the catalog scores and EDS indicate an important \ndistinction between usability and usefulness, with the latter concept encompassing a system’s \ncontent and capabilities. The library catalog is, perhaps, a more straightforward tool than a \ndiscovery tool and attempts to provide access to a smaller set of information. It has none of the \ncomplexity involved in finding article-level or book chapter information. All else being equal, \nsimpler tools will be more usable. In an experimental study, Tsakonas and Paptheodorou found \nthat while users did not distinguish between the concepts of usability and usefulness, they prefer \nattributes composing a useful system in contrast to those supporting usability.42 Discovery tools, \nwhich support more tasks, must make compromises in usability that simpler systems can avoid. In \ntheir study of EDS, Williams and Foster also found overall user satisfaction with EDS. Their \nparticipants made positive comments about the interface as well as the usefulness and relevance \nof the results.43  \nJMU passed on several suggestions to EBSCO related to EDS based on the test results. EBSCO \nsubsequently added “Audio” and “Video” to the source types, which enabled JMU to add a “Just \nVideos at JMU” option to Quick Search. While it is confusing that “Audio” and “Video” source types \ncurrently behave differently than the others in EDS, in that they limit to JMU’s catalog as well as to \nthe source type, this behavior produces what most local users expect. A previous usability study of \nWorldCat Local showed users have trouble discriminating between source types in results lists, so \nthe source types facet is important.44 Another piece of feedback provided to EBSCO was that on \nthe task where users needed to choose the most recent result, only two of our participants sorted \nby date descending. Perhaps the textual appearance of the sort option (instead of a drop-down \nmenu) was not obvious to participants (see figure 6); however, Williams and Foster did not \nobserve this to be an issue in their study.45  \nFUTURE RESEARCH \nThe findings of this study suggest many avenues for future research. Libraries will need to revisit \nthe scope of their catalogs and other systems to keep up with users’ mental models and \ninformation needs. Catalogs and subject-specific databases still perform some tasks much better \nthan discovery tools, but libraries will need to investigate how to situate the discovery tool and \nspecialized tools within their web presence in a way that will make sense to users. When should a \nuser be directed to the catalog versus a discovery tool? What items should libraries continue to \ninclude in their catalogs? What role do institutional repositories play in the suite of library tools, \nand how does the discovery tool connect to them (or include them?) How do library websites \nbegin to make sense of the current state of library search systems? Above all, are users able to find \nthe best resources for their research needs? Although research on searchers’ mental models has \nbeen extensive,46 librarians’ mental models have not been studied as such. Yet placing the \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 104 \n \ndiscovery tool among the library’s suite of services will involve compromises between these two \nmodels.  \nAnother area needing research is how to instruct users to work with the large numbers of results \nreturned by discovery tools. In subject-specific databases, librarians often help users measure the \nsuccess of their strategy—or even their topic—by the number of results returned: in Criminal \nJustice Abstracts, 5,000 results means a topic is too broad or the search strategy needs refinement. \nIn a discovery tool, a result set this large will likely have some good results on the first couple of \npages if sorted by relevance; however, users will still need to know how to grow or reduce their \nresults sets. Participants in this study showed a willingness to use limiters and other interface \nfeatures, but not always the most helpful ones. When asked to narrow a broad subject on task 3 of \nthis study, only one participant chose to use the “Subject” facet even when the subtopic, audiology, \nwas clearly available. Most added search terms. It will be important for future studies to \ninvestigate the best way for users to narrow large results set in a discovery tool. \nThis study also suggested possible areas of investigation for future user studies. One interesting \nfinding related to this study’s users’ information contexts was that when users were asked to \nsearch on their last research topic, it did not always match up with their major: a voice \nperformance student searched on “current issues in Russia,” and the hospitality major searched on \n“aphasia.” To what extent does a discovery tool help or hinder students who are searching outside \ntheir major area of study? One of JMU’s reference librarians noted that while he would usually \nteach a student majoring in a subject how to use that subject’s specific indexes, as opposed to a \ndiscovery tool, a student outside the major might not need to learn the subject-specific indexes for \nthat subject and could be well served by the discovery tool. Future studies could also investigate \nthe usage and usability of discovery tool features in order to continue informing library \ncustomizations and advice to vendors. For example, this study did not have a task related to \nlogging into a patron account or requesting items, but that would be good to investigate in a \nfollow-up study. Another area ripe for further investigation is discovery tool limiters. This study’s \nparticipants frequently attempted to use limiters, but didn’t always choose the correct ones for the \ntask. What are the ideal design choices for making limiters intuitive? This study found almost no \nuse of the embedded federated search add-on: is this true at other institutions? Finally, this study \nand others reveal difficulty in distinguishing source types. Development and testing of interface \nenhancements to support this ability would be helpful to many libraries’ systems.  \nCONCLUSION  \nThis usability test of a discovery tool at James Madison University did not reveal as many \ninterface-specific findings as it did questions about the role of discovery tools in libraries. Users \nwere generally able to navigate through the Quick Search and EDS interfaces and complete tasks \nsuccessfully. Tasks that are challenging in other interfaces, such as locating journal articles and \ndiscriminating between source types, continued to be challenging in a discovery tool interface. \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 105 \n \nThis usability test suggested that while some interface features were heavily used, such as drop-\ndown limits and facets, other features were not used, such as federated search results. As \ndiscovery tools continue to grow and evolve, libraries should continue to conduct usability tests, \nboth to find usability issues and to understand user behavior and satisfaction. Although discovery \ntools challenge libraries to think not only about access but also about the best research pathways \nfor users, they provide users with a search that more closely matches their expectations.  \nACKNOWLEDGEMENT \nThe authors would like to thank Patrick Ragland for his editorial assistance in preparing this \nmanuscript. \nCORRECTION \nApril 12, 2018: At the request of the author, this article was revised to remove a link to a website.  \nREFERENCES \n                                                           \n1. Emily Alling and Rachael Naismith, “Protocol Analysis of a Federated Search Tool: \nDesigning for Users,” Internet Reference Services Quarterly 12, no. 1 (2007): 195, \nhttp://scholarworks.umass.edu/librarian_pubs/1/ (accessed Jan. 11, 2012); Frank Cervone, \n“What We've Learned From Doing Usability Testing on OpenURL Resolvers and Federated Search \nEngines,” Computers in Libraries 25, no. 9 (2005): 10 ; Sara Randall, “Federated Searching and \nUsability Testing: Building the Perfect Beast,” Serials Review 32, no. 3 (2006): 181–82, \ndoi:10.1016/j.serrev.2006.06.003; Ed Tallent, “Metasearching in Boston College Libraries—A Case \nStudy of User Reactions,” New Library World 105, no. 1 (2004): 69-75, DOI: \n10.1108/03074800410515282. \n2. S. C. Williams and A. K. Foster, “Promise Fulfilled? An EBSCO Discovery Service Usability \nStudy,” Journal of Web Librarianship 5, no. 3 (2011), \nhttp://www.tandfonline.com/doi/pdf/10.1080/19322909.2011.597590 (accessed Jan. 11, 2012). \n3. Janet K. Chisman, Karen R. Diller, and Sharon L. Walbridge, “Usability Testing: A Case Study,” \nCollege & Research Libraries 60, no. 6 (November 1999): 552–69, \nhttp://crl.acrl.org/content/60/6/552.short (accessed Jan. 11, 2012); Frances C. Johnson and \nJenny Craven, “Beyond Usability: The Study of Functionality of the 2.0 Online Catalogue,” New \nReview of Academic Librarianship 16, no. 2 (2010): 228–50, DOI: 10.1108/00012531011015217 \n(accessed Jan, 11, 2012); Jennifer E. Knievel, Jina Choi Wakimoto, and Sara Holladay, “Does \nInterface Design Influence Catalog Use? A Case Study,” College & Research Libraries 70, no. 5 \n(September 2009): 446–58, http://crl.acrl.org/content/70/5/446.short (accessed Jan. 11, 2012); \nJia Mi and Cathy Weng, “Revitalizing the Library OPAC: Interface, Searching, and Display \nChallenges,” Information Technology & Libraries 27, no. 1 (March 2008): 5–22, http://0-\n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 106 \n \n                                                                                                                                                                                                           \nwww.ala.org.sapl.sat.lib.tx.us/ala/mgrps/divs/lita/publications/ital/27/1/mi.pdf (accessed Jan. \n11, 2012). \n4. Karen Calhoun, “The Changing Nature of the Catalog and its Integration with Other \nDiscovery Tools,” http://www.loc.gov/catdir/calhoun-report-final.pdf (accessed Mar. 11, 2011). \n5. Dee Ann Allison, “Information Portals: The Next Generation Catalog,” Journal of Web \nLibrarianship 4, no. 1 (2010): 375–89, \nhttp://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1240&context=libraryscience \n(accessed January 11, 2012); Marshall Breeding, “The State of the Art in Library Discovery,” \nComputers in Libraries 30, no. 1 (2010): 31–34; C. P Diedrichs, “Discovery and Delivery: Making it \nWork for Users . . . Taking the Sting out of Serials!” (lecture, North American Serials Interest Group, \nInc. 23rd Annual Conference, Phoenix, Arizona, June 5–8, 2008), DOI:  \n10.1080/03615260802679127; Ian Hargraves, “Controversies of Information Discovery,” \nKnowledge, Technology & Policy 20, no. 2 (Summer 2007): 83, \nhttp://www.springerlink.com/content/au20jr6226252272/fulltext.html (accessed Jan. 11, 2012); \nJane Hutton, “Academic Libraries as Digital Gateways: Linking Students to the Burgeoning Wealth \nof Open Online Collections,” Journal of Library Administration 48, no. 3 (2008): 495–507, DOI: \n10.1080/01930820802289615; OCLC, “Online Catalogs: What Users and Librarians Want: An \nOCLC Report,” http://www.oclc.org/reports/onlinecatalogs/default.htm (accessed Mar. 11 2011). \n6. C. J. Belliston, Jared L. Howland, and Brian C. Roberts, “Undergraduate Use of Federated \nSearching: A Survey of Preferences and Perceptions of Value-Added Functionality,” College & \nResearch Libraries 68, no. 6 (November 2007): 472–86, \nhttp://crl.acrl.org/content/68/6/472.full.pdf+html (accessed Jan. 11, 2012); Judith Z. Emde, Sara \nE. Morris, and Monica Claassen‐Wilson, “Testing an Academic Library Website for Usability with \nFaculty and Graduate students,” Evidence Based Library & Information Practice 4, no. 4 (2009): 24–\n36, http://kuscholarworks.ku.edu/dspace/bitstream/1808/5887/1/emdee_morris_CW.pdf \n(accessed Jan. 11,2012); Karla Saari Kitalong, Athena Hoeppner, and Meg Scharf, “Making Sense of \nan Academic Library Web Site: Toward a More Usable Interface for University Researchers,” \nJournal of Web Librarianship 2, no. 2/3 (2008): 177–204, \nhttp://www.tandfonline.com/doi/abs/10.1080/19322900802205742 (accessed Jan. 11, 2012); \nEd Tallent, “Metasearching in Boston College Libraries—A Case Study of User Reactions,” New \nLibrary World 105, no. 1 (2004): 69–75, DOI: 10.1108/03074800410515282; Rong Tang, Ingrid \nHsieh-Yee, and Shanyun Zhang, “User Perceptions of MetaLib Combined Search: An Investigation \nof How Users Make Sense of Federated Searching,” Internet Reference Services Quarterly 12, no. 1 \n(2007): 211–36, http://www.tandfonline.com/doi/abs/10.1300/J136v12n01_11 (accessed Jan. \n11, 2012). \n7. Jody Condit Fagan, “Usability Studies of Faceted Browsing: A Literature Review,” \nInformation Technology & Libraries 29, no. 2 (2010): 58–66, \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 107 \n \n                                                                                                                                                                                                           \nhttp://web2.ala.org/ala/mgrps/divs/lita/publications/ital/29/2/fagan.pdf (accessed Jan. 11, \n2012). \n8. Birong Ho, Keith Kelley, and Scott Garrison, “Implementing VuFind as an Alternative to \nVoyager’s Web Voyage Interface: One Library’s Experience,” Library Hi Tech 27, no. 1 (2009): 8292, \nDOI: 10.1108/07378830910942946 (accessed Jan. 11, 2012). \n9. Tamar Sadeh, “User Experience in the Library: A Case Study,” New Library World 109, no. 1 \n(2008): 7–24, DOI: 10.1108/03074800810845976 (accessed Jan. 11, 2012). \n10. Tod A. Olson, “Utility of a Faceted Catalog for Scholarly Research,” Library Hi Tech 25, no. 4 \n(2007): 550–61, DOI: 10.1108/07378830710840509 (accessed Jan. 11, 2012). \n11. Allison, “Information Portals,” 375–89. \n12. Marshall Breeding, “Plotting a New Course for Metasearch,” Computers in Libraries 25, no. 2 \n(2005): 27. \n13. Ibid.  \n14. Dennis Brunning and George Machovec, “Interview About Summon with Jane Burke, Vice \nPresident of Serials Solutions,” Charleston Advisor 11, no. 4 (2010): 60–62; Dennis Brunning and \nGeorge Machovec, “An Interview with Sam Brooks and Michael Gorrell on the EBSCOhost \nIntegrated Search and EBSCO Discovery Service,” Charleston Advisor 11, no. 3 (2010): 62–65, \nhttp://www.ebscohost.com/uploads/discovery/pdfs/topicFile-121.pdf (accessed Jan. 11, 2012). \n15. Ronda Rowe, “Web-Scale Discovery: A Review of Summon, EBSCO Discovery Service, and \nWorldCat Local,” Charleston Advisor 12, no. 1 (2010): 5–10; K. Stevenson et al., “Next-Generation \nLibrary Catalogues: Reviews of Encore, Primo, Summon and Summa,” SERIALS 22, no. 1 (2009): \n68–78. \n16. Jason Vaughan, “Chapter 7: Questions to Consider,” Library Technology Reports 47, no. 1 \n(2011): 54; Paula L. Webb and Muriel D. N ero, “OPACs in the Clouds,” Computers in Libraries 29, no. \n9 (2009): 18. \n17. Jason Vaughan, “Investigations into Library Web Scale Discovery Services,” Articles \n(Libraries), paper 44 (2011), http://digitalcommons.library.unlv.edu/lib_articles/44. \n18. Marshall Breeding, “The State of the Art in Library Discovery,” 31–34; Sharon Q. Yang and \nKurt Wagner, “Evaluating and Comparing Discovery Tools: How Close are We Towards Next \nGeneration Catalog?” Library Hi Tech 28, no. 4 (2010): 690–709. \n19. Allison, “Information Portals,” 375–89. \n20. Breeding, “The State of the Art in Library Discovery,” 31–34. \n21. Galina Letnikova, “Usability Testing of Academic Library Websites: A Selective \nBibliography,” Internet Reference Services Quarterly 8, no. 4 (2003): 53–68. \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 108 \n \n                                                                                                                                                                                                           \n22. Jeffrey Rubin and Dana Chisnell, Handbook of Usability Testing: How to Plan, Design, and \nConduct Effective Tests, 2nd ed. (Indianapolis, IN: Wiley, 2008); Joseph S. Dumas and Janice Redish, \nA Practical Guide to Usability Testing, rev. ed. (Portland, OR: Intellect, 1999). \n23. Nicole Campbell, ed., Usability Assessment of Library-Related Web Sites: Methods and Case \nStudies (Chicago: Library & Information Technology Association, 2001); Elaina Norlin and C. M. \nWinters, Usability Testing for Library Web Sites: A Hands-On Guide (Chicago: American Library \nAssociation, 2002). \n24. Jennifer L. Ward, Steve Shadle, and Pam Mofield, “User Experience, Feedback, and Testing,” \nLibrary Technology Reports 44, no. 6 (2008): 17. \n25. Ibid. \n26. Michael Boock, Faye Chadwell, and Terry Reese, “WorldCat Local Task Force Report to \nLAMP,” http://hdl.handle.net/1957/11167 (accessed Mar. 11 2011). \n27. Bob Thomas and Stefanie Buck, “OCLC’s WorldCat Local Versus III’s WebPAC: Which \nInterface is Better at Supporting Common User Tasks?” Library Hi Tech 28, no. 4 (2010): 648–71. \n28. OCLC, “Some Findings from WorldCat Local Usability Tests Prepared for ALA Annual,” \nhttp://www.oclc.org/worldcatlocal/about/213941usf_some_findings_about_worldcat_local.pdf \n(accessed Mar. 11, 2011). \n29. Ibid., 2. \n30. Doug Way, “The Impact of Web-Scale Discovery on the Use of a Library Collection,” Serials \nReview 36, no. 4 (2010): 21420. \n31. North Carolina State University Libraries, “Final Summon User Research Report,” \nhttp://www.lib.ncsu.edu/userstudies/studies/2010_summon/ (accessed Mar. 28, 2011). \n32. Alesia McManus, “The Discovery Sandbox: Aleph and Encore Playing Together,” \nhttp://www.nercomp.org/data/media/Discovery%20Sandbox%20McManus.pdf (accessed Mar. \n28, 2011); PRWeb, “Deakin University in Australia Chooses EBSCO Discovery Service,” \nhttp://www.prweb.com/releases/Deakin/ChoosesEDS/prweb8059318.htm (accessed Mar. 28, \n2011); University of Manitoba, “Summon Usability: Partnering with the Vendor,” \nhttp://prezi.com/icxawthckyhp/summon-usability-partnering-with-the-vendor (accessed Mar. \n28, 2011). \n33. Williams and Foster, “Promise Fulfilled?” \n34. Jakob Nielsen, “Why You Only Need to Test with 5 Users,” \nhttp://www.useit.com/alertbox/20000319.html (accessed Aug. 20, 2011). \n35. John Brooke, “SUS: A ‘Quick and Dirty’ Usability Scale,” in Usability Evaluation in Industry, \ned. P. W. Jordanet al. (London: Taylor & Francis, 1996), \nhttp://www.usabilitynet.org/trump/documents/Suschapt.doc (accessed Apr. 6, 2011). \n36. Williams and Foster, “Promise Fulfilled?” \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 109 \n \n                                                                                                                                                                                                           \n37. Seikyung Jung et al., “LibraryFind: System Design and Usability Testing of Academic \nMetasearch System,” Journal of the American Society for Information Science & Technology 59, no. 3 \n(2008): 375–89; Williams and Foster, “Promise Fulfilled?”; Laura Wrubel and Kari Schmidt, \n“Usability Testing of a Metasearch Interface: A Case Study,” College & Research Libraries 68, no. 4 \n(2007): 292–311. \n38. Williams and Foster, “Promise Fulfilled?” \n39. Letnikova, “Usability Testing of Academic Library Websites,” 53–68; Tom Ipri, Michael \nYunkin, and Jeanne M. Brown, “Usability as a Method for Assessing Discovery,” Information \nTechnology & Libraries 28, no. 4 (2009): 181–86; Susan H. Mvungi, Karin de Jager, and Peter G. \nUnderwood, “An Evaluation of the Information Architecture of the UCT Library Web Site,” South \nAfrican Journal of Library & Information Science 74, no. 2 (2008): 171–82. \n40. Williams and Foster, “Promise Fulfilled?” \n41. Ward et al., “User Experience, Feedback, and Testing,” 17. \n42. Giannis Tsakonas and Christos Papatheodorou, “Analysing and Evaluating Usefulness and \nUsability in Electronic Information Services,” Journal of Information Science 32, no. 5 (2006): 400–\n419. \n43.  Williams and Foster, “Promise Fulfilled?” \n44. Bob Thomas and Stefanie Buck, “OCLC’s WorldCat Local Versus III’s WebPAC: Which \nInterface is Better at Supporting Common User Tasks?” Library Hi Tech 28, no. 4 (2010): 648–71. \n45.  Williams and Foster, “Promise Fulfilled?” \n46. Tracy Gabridge, Millicent Gaskell, and Amy Stout, “Information Seeking through Students’ \nEyes: The MIT Photo Diary Study,” College & Research Libraries 69, no. 6 (2008): 510–22; Yan \nZhang, “Undergraduate Students’ Mental Models of the Web as an Information Retrieval System,” \nJournal of the American Society for Information Science & Technology 59, no. 13 (2008): 2087–98; \nBrenda Reeb and Susan Gibbons, “Students, Librarians, and Subject Guides: Improving a Poor Rate \nof Return,” Portal: Libraries and the Academy 4, no. 1 (2004): 123–30; Alexandra Dimitroff, \n“Mental Models Theory and Search Outcome in a Bibliographic Retrieval System,” Library & \nInformation Science Research 14, no. 2 (1992): 141–56. \n \n \n \n \n \n \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 110 \n \n                                                                                                                                                                                                           \n \n \n \n \n \nAPPENDIX A \nTask \nPre–Test 1: Please indicate your JMU status (1st Year, 2nd Year, 3rd Year, 4th Year, Graduate Student, Faculty, Other)  \nPre–Test 2: Please list your major(s) or area of teaching (open ended) \nPre–Test 3: How often do you use the library website? (Less than once a month, 1–3 visits per month, 4–6 visits per \nmonth, more than 7 visits per month) \nPre–Test 4: What are some of the most common things you currently do on the library website? (open ended) \nPre–Test 5: How much of the library’s resources do you think the Quick Search will search? (Less than a third, Less \nthan half, Half, Most, All) \nPre–Test 6: Have you used LEO? (show screenshot on printout) (Yes, No, Not Sure) \nPre–Test 7: Have you used EBSCO? (show screenshot on printout) (Yes, No, Not Sure) \nPre–Test 8 (Student participants only): How often have you used library web resources for course assignments in \nyour major? (Rarely/Never, Sometimes, Often, Very Often) \nPre–Test 9 (Student participants only): How often have you used library resources for course assignments outside \nof your major? (Rarely/Never, Sometimes, Often, Very Often) \n Pre–Test 10 (Student participants only): Has a librarian spoken to a class you've attended about library research? \n(Yes, No, Not Sure) \n Pre–Test 11 (Faculty participants only): How often do you give assignments that require the use of library \nresources? (Rarely/Never, Sometimes, Often, Very Often) \n Pre–Test 12 (Faculty participants only): How often have you had a librarian visit one of your classes to teach your \nstudents about library research? (Rarely/Never, Sometimes, Often, Very Often) \nPost–Test 1: When would you use this search tool? \nPost–Test 2: When would you not use this search tool?  \nPost–Test 3: What would you say are the major advantages of Quick Search? \n \nINFORMATION TECHNOLOGY AND LIBRARIES | MARCH 2012 111 \n \n                                                                                                                                                                                                           \nPost–Test 4: What would you say are the major problems with Quick Search? \nPost–Test 5: If you were unable to find an item using Quick Search/EBSCO Discovery Service what would your next \nsteps be? \nPost–Test 6: Do you think the name “Quick Search” is fitting for this search tool? If not, what would you call it?  \nPost–Test 7 (Faculty participants only): If you knew students would use this tool to complete assignments would \nyou alter how you structure assignments and how? \nAPPENDIX B  \nTask Purpose \n• Practice Task: Use Quick Search to \nsearch a topic relating to your major / \ndiscipline or another topic of interest to \nyou. If you were writing a paper on this \ntopic how satisfied would you be with \nthese results? \n \nHelp users get comfortable with the \nusability testing software. Also, since the \nfirst time someone uses a piece of software \ninvolves behaviors unique to that case, we \nwanted participants’ first use of EDS to be \nwith a practice task.  \n1. What was the last thing you searched for \nwhen doing a research assignment for \nclass? Use Quick Search to re-search for \nthis. Tell us how this compared to your \nprevious experience. \nHaving participants re-search a topic with \nwhich they had some experience and \ninterest would motivate them to engage \nwith results and provide a comparison point \nfor their answer. We hoped to learn about \ntheir satisfaction with relevance, quality, \nand quantity of results. (user behavior, user \nsatisfaction)  \n2. Using Quick Search find a video related \nto early childhood cognitive \ndevelopment. When you’ve found a \nsuitable video recording, click ANSWER \nand copy and paste the title. \nThis task aimed to determine whether \nparticipants could complete the task, as well \nas show us which features they used in their \nattempts. (usability, user behavior) \n3. Search on speech pathology and find a \nway to limit your search results to \naudiology. Then, limit your search \nresults to peer reviewed sources. How \nsatisfied are you with the results?  \nSince there are several ways to limit results \nin EDS, we designed this task to show us \nwhich limiters participants tried to use, and \nwhich limiters resulted in success. We also \nhoped to learn about whether they thought \nthe limiters provided satisfactory results. \n(usability, user behavior, user satisfaction) \n \nUSABILITY TEST RESULTS FOR A DISCOVERY TOOL IN AN ACADEMIC LIBRARY | FAGAN ET AL 112 \n \n                                                                                                                                                                                                           \n4. You need more recent sources. Please \nlimit these search results to the last 5 \nyears, then select the most recent source \navailable. Click Finished when you are \ndone. \nSince there are several ways to limit by date \nin EDS, we designed this task to show us \nwhich limiters participants tried to use, and \nwhich limiters resulted in success. \n(usability, user behavior) \n5. Find a way to ask a JMU librarian for \nhelp using this search tool. After you’ve \nfound the correct web page, click \nFINISHED. \n \nWe wanted to determine whether the user \ncould complete this task, and which \npathway they chose to do it. (usability, user \nbehavior) \n6. Locate the journal Yachting and Boating \nWorld. What are the coverage dates? Is \nthis journal available in online full text? \nWe wanted to determine whether the user \ncould locate a journal by title. (usability) \n7. You need to look up the sculpture Genius \nof Mirth. You have been told that the \nlibrary database, Camio, would be the \nbest place to search for this. Locate this \ndatabase and find the sculpture. \nWe wanted to know whether users who \nknew they needed to use a specific database \ncould find that database from within the \ndiscovery tool. (usability, user behavior). \n8. Use Quick Search to find 2 books and 2 \nrecent peer reviewed articles (from the \nlast 5 years) on rheumatoid arthritis. \nWhen you have found suitable source \nclick ANSWER and copy and paste the \ntitles. Click BACK TO WEBPAGE if you \nneed to return to your search results. \n \n \n \nThese two tasks were intended to show us \nhow users completed a common, broad task \nwith and without a discovery tool, whether \nthey would be more successful with or \nwithout the tool, and what barriers existed \nwith and without the tool (usability, user \nbehavior) \n9. Without using Quick Search, find 2 books \nand 2 recent peer reviewed articles \n(from the last 5 years) on rheumatoid \narthritis. When you have found suitable \nsources click ANSWER and copy and \npaste the titles. Click BACK TO \nWEBPAGE if you need to return to your \nsearch results. \n \n ",
  "topic": "Usability",
  "concepts": [
    {
      "name": "Usability",
      "score": 0.8274834156036377
    },
    {
      "name": "Computer science",
      "score": 0.7263882160186768
    },
    {
      "name": "World Wide Web",
      "score": 0.6646376848220825
    },
    {
      "name": "Task (project management)",
      "score": 0.587685227394104
    },
    {
      "name": "Library catalog",
      "score": 0.5569900870323181
    },
    {
      "name": "Service (business)",
      "score": 0.519792914390564
    },
    {
      "name": "Scope (computer science)",
      "score": 0.518481433391571
    },
    {
      "name": "Interface (matter)",
      "score": 0.49817776679992676
    },
    {
      "name": "Test (biology)",
      "score": 0.47083544731140137
    },
    {
      "name": "Digital library",
      "score": 0.46540743112564087
    },
    {
      "name": "Service discovery",
      "score": 0.4423835873603821
    },
    {
      "name": "Library classification",
      "score": 0.4269290566444397
    },
    {
      "name": "Data science",
      "score": 0.35266953706741333
    },
    {
      "name": "Human–computer interaction",
      "score": 0.21692347526550293
    },
    {
      "name": "Web service",
      "score": 0.20589667558670044
    },
    {
      "name": "Engineering",
      "score": 0.10795801877975464
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Poetry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Maximum bubble pressure method",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Bubble",
      "score": 0.0
    }
  ]
}