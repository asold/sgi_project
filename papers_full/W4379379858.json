{
  "title": "Post Hoc Explanations of Language Models Can Improve Language Models",
  "url": "https://openalex.org/W4379379858",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2981944953",
      "name": "Satyapriya Krishna",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2236133143",
      "name": "Jiaqi Ma",
      "affiliations": [
        "Harvard University Press"
      ]
    },
    {
      "id": "https://openalex.org/A2935322568",
      "name": "Dylan Slack",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2943533337",
      "name": "Asma Ghandeharioun",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2143533944",
      "name": "Sameer Singh",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2237122046",
      "name": "Himabindu Lakkaraju",
      "affiliations": [
        "Harvard University Press"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6675775337",
    "https://openalex.org/W6602430550",
    "https://openalex.org/W6827106861",
    "https://openalex.org/W6606425255",
    "https://openalex.org/W6695661434",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W1583837637",
    "https://openalex.org/W6815744885",
    "https://openalex.org/W6630379773",
    "https://openalex.org/W3187467055",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4375958700",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3121975067",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W4385572952",
    "https://openalex.org/W4225896729",
    "https://openalex.org/W4285070039",
    "https://openalex.org/W4385574286",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W3168461525",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4385571157",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W4293861706",
    "https://openalex.org/W4365601026",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W1849277567"
  ],
  "abstract": "<title>Abstract</title> Large Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks. Moreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-Thought prompting) during in-context learning can significantly enhance the performance of these models, particularly on tasks that require reasoning capabilities. However, incorporating such rationales poses challenges in terms of scalability as this requires a high degree of human involvement. In this work, we present a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post Hoc Explanations (AMPLIFY), which addresses the aforementioned challenges by automating the process of rationale generation. To this end, we leverage post hoc explanation methods which output attribution scores (explanations) capturing the influence of each of the input features on model predictions. More specifically, we construct automated natural language rationales that embed insights from post hoc explanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets demonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25% over a wide range of tasks, including those where prior approaches which rely on human-annotated rationales such as Chain-of-Thought prompting fall short. Our work makes one of the first attempts at highlighting the potential of post hoc explanations as valuable tools for enhancing the effectiveness of LLMs. Furthermore, we conduct additional empirical analyses and ablation studies to demonstrate the impact of each of the components of AMPLIFY, which, in turn, lead to critical insights for refining in-context learning.",
  "full_text": "Post Hoc Explanations of Language Models Can\nImprove Language Models\nSatyapriya Krishna  (  skrishna@g.harvard.edu )\nhttps://orcid.org/0000-0002-5324-5824\nJiaqi Ma \nDylan Slack \nAsma Ghandeharioun \nSameer Singh \nHimabindu Lakkaraju \nResearch Article\nKeywords: Explainable Machine Learning, Language Models\nPosted Date: June 5th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-3006112/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations:\nCompeting interests: The authors declare no competing interests.\nPost Hoc Explanations of Language Models Can Improve\nLanguage Models\nSatyapriya Krishna1, Jiaqi Ma 1, Dylan Slack 2, Asma Ghandeharioun 3,\nSameer Singh2, Himabindu Lakkaraju 1\n1Harvard University .\n2University of California, Irvine (UCI).\n3Google Inc.\nCorresponding author: skrishna@g.harvard.edu;\nAbstract\nLarge Language Models (LLMs) have demonstrated remarkable capabilities in performing complex tasks.\nMoreover, recent research has shown that incorporating human-annotated rationales (e.g., Chain-of-\nThought prompting) during in-context learning can signiﬁcantly enhance the performance of these models,\nparticularly on tasks that require reasoning capabilities. However, incorporating such rationales poses\nchallenges in terms of scalability as this requires a high degree of human involvement. In this work, we\npresent a novel framework, Amplifying Model Performance by Leveraging In-Context Learning with Post\nHoc Explanations ( AMPLIFY), which addresses the aforementioned challenges by automating the process\nof rationale generation. T o this end, we leverage post hoc explanation methods which output attribution\nscores (explanations) capturing the inﬂuence of each of the input features on model predictions. More\nspeciﬁcally , we construct automated natural language rationales that embed insights from post hoc\nexplanations to provide corrective signals to LLMs. Extensive experimentation with real-world datasets\ndemonstrates that our framework, AMPLIFY, leads to prediction accuracy improvements of about 10-25%\nover a wide range of tasks, including those where prior approaches which rely on human-annotated\nrationales such as Chain-of-Thought prompting fall short. Our work makes one of the ﬁrst attempts\nat highlighting the potential of post hoc explanations as valuable tools for enhancing the eﬀectiveness\nof LLMs. F urthermore, we conduct additional empirical analyses and ablation studies to demonstrate\nthe impact of each of the components of\nAMPLIFY, which, in turn, lead to critical insights for reﬁning\nin-context learning.\nKeywords: Explainable Machine Learning, Language Models\n1 Introduction\nIn recent years, Large Language Models (LLMs) [ 1] have ushered in a new era for machine learning research.\nThese models are exhibiting emergent capabilities that enable them to excel at complex tasks that involve\nsophisticated capabilities such as reasoning and language understanding [ 2, 3]. Moreover, these models do not\nonly exhibit remarkable performance on tasks they were trained for but also quickly adapt to other novel and\ncomplex tasks. This is made possible through a mechanism known as in-context learning, which allows these\nmodels to learn from a limited number of input and label pairs, commonly referred to as few-shot prompts[ 4],\nprovided during test time. Prior research has also demonstrated that the performance of these models\non sophisticated reasoning tasks can be signiﬁcantly improved by presenting them with human-annotated\nrationales alongside input/label pairs during test time [ 5, 6]. While incorporating such human-annotated\nrationales has contributed to enhancing model performance, it is not a scalable approach as it involves\na lot of human intervention, thus, limiting its applicability to the ever-expanding range of tasks that are\nbeing handled by LLMs [ 7, 8]. Additionally , prompting these models with human-annotated rationales is not\nalways eﬀective, and may lead to drops in performance on certain tasks requiring sophisticated language\n1\nFig. 1: The AMPLIFY framework consists of four steps aimed at improving the performance of LLMs. (1) W e select a\nproxy model, such as GPT-2 or BER T, which is signiﬁcantly smaller in size compared to the LLMs and for which\nit is computationally feasible to generate post hoc explanations. (2) By leveraging the validation set, we identify\nsamples that were misclassiﬁed by the LLM. Subsequently , we select the samples that the proxy model exhibits the\nhighest level of conﬁdence in misclassifying. (3) W e then use explainability techniques to compute explanations for\nthe selected samples with respect to their ground truth labels. (4) W e construct the few-shot prompt for LLM using\nthe samples selected and their corresponding explanations to feed as input to LLM for prediction.\nunderstanding as demonstrated by prior works [ 9]. This could be due to the fact that the seemingly innocuous\nbiases introduced by human-annotated rationales do not necessarily align with the performance goals [ 10].\nT o address the aforementioned challenges, we propose a novel framework Amplifying Model Performance\nby Leveraging In-Context Learning with Post Hoc Explanations ( AMPLIFY) which can automatically generate\nrationales to improve the performance of LLMs on tasks involving sophisticated reasoning and language\nunderstanding. T o this end, we leverage post hoc explanation methods which output attribution scores that\ncapture the inﬂuence of each of the input features on model predictions. More speciﬁcally , our framework\nconstructs natural language rationales that embed insights from post hoc explanations to provide corrective\nsignals to LLMs. F or example, when a LLM makes an error on an instance, our framework enables the model\nto correct itself by ﬁrst identifying the top keywords by computing post hoc explanations (e.g., gradients)\nof the model with respect to the given instance and its true label, and then prompting the model to pay\nattention to the top keywords identiﬁed, thus, amplifying this signal. T o illustrate, let us consider a sentiment\nclassiﬁcation task where a LLM is assessing the sentiment associated with the sentence \"RRR movie has\na great story and amazing visuals.\". If the model is incorrect in its assessment, it can be provided with a\ncorrective input such as \"The keywords ’great’ and ’amazing’ are important cues in predicting the sentiment\nof this sentence\" where the keywords themselves are automatically identiﬁed by a post hoc explanation\nmethod. While post hoc explanations have generally been considered valuable tools for deepening our\nunderstanding of model behavior [ 11] and for identifying root causes of errors made by ML models [ 12, 13],\nour work is the ﬁrst to explore their utility in improving the performance of LLMs.\nWhile the use of post hoc explanation methods to rectify LLMs’ behavior eliminates the need for\nhuman intervention, it encounters two signiﬁcant challenges: First, calculating gradients (and hence post\nhoc explanations) for LLMs with several billions of parameters is computationally intensive. Second, many\nLLMs operate as black boxes, depriving end users of access to gradients or other internal model details. T o\nmitigate these issues, we compute post hoc explanations for a proxy model (e.g. GPT-2, BER T, etc.) that is\nconsiderably smaller than a large language model with billions of parameters. W e then incorporate these\nexplanations into input prompts for larger language models (e.g. GPT-3, GPT-3.5, etc.). This approach not\nonly improves eﬃciency and feasibility (as we are now calculating gradients for models with 100-200 million\nparameters, instead of those with 100 billion parameters) but also takes advantage of the accessibility of\nsmaller models that are open sourced. In summary , our\nAMPLIFY framework follows a four-step approach:\n(1) Select a proxy model for which post hoc explanation generation is computationally viable; (2) Identify\nsamples that are most likely to provide corrective signals to LLMs; (3) Compute post hoc explanations for\nthe samples identiﬁed in the previous step; (4) Construct a few-shot prompt using the samples chosen in\nstep (2), their true labels, and the post hoc explanations obtained in step 3 as rationales. This prompt is\nthen provided as input to the LLM at test time.\nOur ﬁndings demonstrate that AMPLIFY leads to performance improvements of about 10-25% across a\nwide range of tasks, including those where previously considered prompting techniques such as Chain-of-\nThought prompting which rely on human-annotated explanations, fall short. This highlights the potential of\npost hoc explanation methods as valuable tools for enhancing the eﬀectiveness of LLMs. F urthermore, we\nconduct an extensive empirical analysis to examine the impact of each step of our framework AMPLIFY. This\nallows for a better understanding of the change in LLM performance with diﬀerent choices of proxy model\n(step 1), selection strategy (step 2), post hoc explanation method (step 3), and rationale templates (step\n4). Thus, we oﬀer critical insights for reﬁning in-context learning while addressing the limitations posed by\nmethods dependent on human-annotated rationales.\n2\n2 Related W orks\nIn-context Learning\nOver the past decade, numerous language models have been developed to excel at a wide range of complex\npredictive tasks [ 2]. This is accomplished by training and ﬁne-tuning language models on datasets associated\nwith various tasks. While these advancements have led to highly eﬀective language models for numerous tasks,\nthey have also increased the models’ parameter sizes and the computational costs for additional ﬁne-tuning\non new tasks. T o address this issue, recent research has demonstrated that modern language models can\nlearn new tasks in-context, which allows the model to perform well on new tasks by simply providing a few\ntask samples in the prompt [ 14]. This method of learning contrasts with the conventional ﬁne-tuning process,\nwhich incurs additional computational costs [ 14]. This in-context learning ability is even more pronounced\nin extremely large language models (>100 billion parameters), where it is also referred to as an \"emergent\nability\" [ 2]. These ﬁndings have garnered signiﬁcant attention, and various approaches have been proposed\nto enhance in-context learning by incorporating additional signals into the prompt [ 6]. The current state-of-\nthe-art among such approaches is the Chain-of-Thought (CoT) technique [ 5] which augments prompts with\nhuman-annotated rationales comprising of step-by-step instructions on how to perform a new task. This\nmethod has substantially improved language models’ capacity to tackle highly challenging tasks that involve\nsophisticated reasoning. However, this method relies heavily on human annotations and is therefore not very\nscalable. F urther, prior works have demonstrate that this method also leads to poor performance in certain\nkinds of reasonings tasks[ 9]. These limitations persist, and there are largely no solutions for them yet. In this\nwork, we propose an approach that demonstrates a lot of promise in alleviating the aforementioned issues.\nPost Hoc Explanations\nAs language models have become more capable and complex, understanding their behavior and the rationale\nbehind their predictions has grown increasingly challenging [ 15]. T o understand the predictions made by these\nblack boxes, various methods have been developed to provide explanations in the form of feature attributions\nwhich capture the inﬂuence of each input feature on a given model prediction. These methods are known as\npost hoc explanation methods [ 16]. Post hoc explanation methods can be broadly classiﬁed into two primary\ncategories: (1) perturbation-based methods and (2) gradient-based methods. Perturbation-based methods\ninvolve creating an interpretable approximation of the original black-box model using perturbations of input\nsamples. Notable examples of these methods include LIME [ 17], SHAP [ 18], Occlusion [ 19], and others. In\naddition, gradient-based methods scuh as SmoothGrad and Integrated Gradients [ 20, 21] calculate model\ngradients with respect to input features to determine the sensitivity of the model’s output to each feature.\nHowever, to the best of our knowledge, the utility of these explanation methods has not been studied in the\ncontext of LLMs. Our work makes the ﬁrst attempt at exploring the utility of these methods in improving\nthe performance of LLMs.\n3 Our F rameworkAMPLIFY\nIn this section, we describe our framework Amplifying Model Performance by Leveraging In-Context Learning\nwith Post Hoc Explanations ( AMPLIFY) in detail. Recall that the main goal of our framework is to eliminate\nthe need for human-annotated rationales, and to instead generate automated rationales which can, in turn,\nprovide corrective signals to improve the performance of LLMs on sophisticated reasoning and language\nunderstanding tasks. T o this end, our framework leverages post hoc explanations to construct such rationales.\nMore speciﬁcally , our framework adopts the following four-step approach: (1) Select a proxy model for which\npost hoc explanation generation is computationally viable; (2) Identify samples that are most likely to\nprovide corrective signals to LLMs; (3) Compute post hoc explanations for the samples identiﬁed in the\nprevious step; (4) Construct a few-shot prompt using the samples chosen in step (2), their true labels, and\nthe post hoc explanations (rationales) obtained in step 3. This prompt is then provided as input to the LLM\nat test time. W e discuss each of these steps in more detail below.\nStep (1): Proxy Model Selection.\nIn this step, we choose a proxy model, typically one that is substantially smaller in size compared to LLMs\nwith billions of parameters, so that generating post hoc explanations is computationally viable. F urther,\nwe consider a couple of strategies when selecting a suitable proxy model: (i) Use a pre-trained model such\nas GPT-2, BER T, etc., which has been shown to perform quite well on several tasks and is thousands of\ntimes smaller than LLMs (GPT-3, Bloom, etc.) or (ii) Fine-tune or pre-train a smaller language model from\nscratch on the target task. The major diﬀerence between the two strategies is that the ﬁrst one requires no\nadditional computational cost as we directly use a pre-trained (potentially open-sourced) model. W e test\nboth proxy model selection strategies to discern performance variations between them. Lastly , it is important\n3\nto note that proxy models of the size we select in this step (e.g., GPT-2, BER T etc.) do not exhibit complex\nreasoning abilities [\n2]. Consequently , they do not perform well on reasoning tasks by themselves [ 7]. However,\nour analyses (more details in Section 4) demonstrate that such smaller models can be used to improve the\nreasoning capabilities and task performance of LLMs.\nStep (2): Few-shot Sample Selection.\nThe goal of this step is to identify samples i.e., (input, label) pairs that are most likely to provide corrective\nsignals to the LLM. T o this end, we ﬁrst identify instances from the validation set that are misclassiﬁed by\nthe LLM. W e then rank these instances using a metric we introduce called the Misclassiﬁcation Conﬁdence\nScore (MCS). F ormally , MCS(x) =f(x)ˆ y− f(x)y. Here, x ∈ RN represents the input sequence containing N\ntokens, f : RN → R|L| is the ﬁne-tuned language model that produces class probability scores for each label in\nthe label set L, f(x)ˆ yrepresents the class probability score for the incorrect label ( ˆy) predicted by the model,\nand f(x)y represents the class probability score for the ground truth label ( y). The samples that exhibit\nthe highest MCS represent the most egregious misclassiﬁcations. By incorporating these samples and their\ncorresponding corrective rationales into the few-shot prompt, the LLM is likely to receive strong supervision\nto avoid similar misclassiﬁcations. In summary , this step results in s samples of (input ( x), label ( y)) pairs,\nﬁltered from the validation set, that are likely to carry the most useful corrective signals to assist LLMs.\nStep (3): Rationale Generation.\nIn this step, we compute post hoc explanations for each sample obtained from the previous step. Speciﬁcally ,\nfor each sample, we use a post hoc explanation method with the (input, label) pair, along with the proxy\nmodel, which then calculates the attribution scores for each token in the input sentence. These attribution\nscores, associated with each token, indicate the contribution each token in the input sentence makes towards\nthe proxy model’s prediction of the provided label. W e then compute attribution scores for each word by\naveraging the attributions obtained for each token in that word. Finally , we ﬁlter the top- k words with the\nhighest attribution scores. As a result, this step outputs a set of k words for each sample selected in the\nprevious step. The most commonly used post hoc explanation methods for computing attribution scores\nof input tokens are based on gradient computations [ 16]. That is, the attribution for the token xi in input\nx ∈ RN is computed as ∂f (x)y\n∂ xi\n, as is the case with V anilla Gradients [ 22]. W e experiment with several other\npost hoc explanation methods discussed in more detail in the experiment section.\nStep (4): Prompt Design for LLMs.\nIn the ﬁnal step, we proceed to construct the corrective rationale for each selected sample using\nthe template: \"The key words:\nword1, word2, ...and wordk are important clues to predict [Label]\nas the correct answer.\" In this template, \"[ word1, word2..., and wordk ]\" refers to the top- k\nmost important words in the input sentence for the true label, which were obtained from the\nprevious step. Using the few-shot template [Input][Rationale][Label], we construct the\ns-shot prompt\nas \"[Input1][Rationale1][Label1]...[Inputs][Rationales][Labels]\", which is simply the concatenation of\n[Input][Rationale][Label] for each selected sample. This constructed prompt is then combined with the input\nof the test sample to form the ﬁnal input prompt for the LLMs, enabling them to make predictions for the\nsamples in the test set. This process is illustrated in the last step of Figure 1.\n4 Experimental Evaluation\nIn this section, we discuss our empirical evaluation in detail. First, we describe our experiment setup and\nprovide details about the datasets and tasks we experiment with. Next, we evaluate the eﬀectiveness of our\nframework in improving task performance of LLMs on a variety of real-world tasks. Lastly , we examine the\nimpact of each step of our framework AMPLIFY by conducting rigorous ablation studies.\nDatasets.\nW e evaluate our framework AMPLIFY on some of the popular datasets from the Big-Bench-Hard[ 7] benchmark.\nMore speciﬁcally , we experiment with: (1) The Snarks[7] dataset which gauges a model’s proﬁciency\nin discerning sarcastic sentences from a selection of alternatives; (2) The Causal Judgment [7] dataset,\ndesigned to evaluate a model’s ability in accurately deducing the causative factors of an event from a\ndetailed summary; (3) The Ruin Names[7] task, which involves the identiﬁcation of comical modiﬁcations\nto artist or movie names; (4) The Formal Fallacies[7] task, where machine learning models are put to\nthe test to distinguish between logically sound arguments and those with logical discrepancies; (5) The\nSalient Translation Error Detection[7] task, engineered to train models in identifying one out of six\npredetermined translation errors given a pair of translations; (6) The CommonsenseQA [23] dataset, a\n4\nT able 1:F ew-shot prompting performance of several large language models on the seven datasets. AO: standard\n“answer-only” prompting. CoT: chain-of-thought prompting. Best model performance is in bold. The LLMs we\nexperimented with are GPT-3 and GPT-3.5. The recorded performance in this table represents the percentage of test\nsamples for which the LLM accurately predicted the true label.\n[5, 7] Human-Rater [ 8] GPT-3 GPT-3.5\nExperiment T asks Random SOT A A vg. Max AO CoT AMPLIFY AO CoT AMPLIFY\nSnarks 50.0 71.3 76.7 100 52.7 61.1 80.5 75.0 69.4 91.6\nCausal Judgment 50.0 62.1 69.6 100 55.2 55.2 60.5 57.8 63.1 76.3\nRuin Names 25.0 72.8 77.7 100 64.0 62.9 78.6 69.6 62.9 77.5\nF ormal F allacies 25.0 52.2 90.8 100 53.6 50.8 60.1 51.4 54.6 59.9\nSalient T ranslation Error Detection 16.7 31.9 36.7 80.0 48.2 50.2 51.7 43.2 54.7 60.8\nCommonsenseQA 20.0 80.0 90.0 100 69.3 72.6 73.5 75.7 75.2 77.9\nCoin Flip (OOD) - - - - 54.7 63.3 65.7 52.9 61.0 65.3\nAll T asks (avg) 31.1 61.7 73.5 96.6 56.8 58.0 67.2 60.8 62.9 72.7\nmultiple-choice question answering platform that necessitates a wide variety of commonsense knowledge for\naccurately determining the correct responses; (7) Lastly , the Coin Flip [5] dataset, a synthetically generated\ndataset used for assessing the symbolic reasoning capacity of LLMs.\nLarge Language Models.\nOur methodology is assessed in comparison to baseline approaches on two LLMs. First, GPT-3 [ 24], a\nlanguage model with 175 billion parameters, demonstrates robust performance across a range of natural\nlanguage processing tasks without the need for explicit training or ﬁne-tuning. Second, GPT-3.5 [\n25] is a\nseries of models that were trained on a mix of text and code data before the end of the fourth quarter in 2021.\nThese models, expressly crafted for chat applications, function as an advanced version of InstructGPT [ 26].\nPost Hoc Explanation Techniques.\nIn this study , we use three widely adopted post hoc explanation methods to generate explanations that\nare later incorporated as rationales into prompts for in-context learning. These methods include V anilla\nGradients [\n22], Gradient x Input [ 27], and contrastive explanations [ 28]. V anilla Gradients [ 22] calculates\nfeature attributions by computing the norm of the gradients of model output with respect to each token’s\nembedding. Gradient x Input derives attribution scores by taking the product of gradient and input embedding.\nFinally , contrastive gradients determine attribution scores by subtracting the gradients with respect to the\nmodel prediction token from those associated with the truth label. W e apply these explanation techniques\nto two proxy models for the generation of corrective rationales in step 3 of AMPLIFY: GPT-2 ( ∼ 125 Mn\nparameters)[29] and BER T ( ∼ 110 Mn parameters) [ 30].\nBaseline Methods.\nIn our experiments, we evaluate the performance of AMPLIFY in comparison to two alternative approaches,\nnamely Answer-Only (AO) prompts [ 7] and Chain-of-Thought (CoT) [ 5]. AO prompting represents the\nstandard few-shot prompting technique, in which the input prompt consists of a few (input, label) pairs\nand the test input sentence, followed by an answer delimiter (’A:’) for the LLM to generate the response.\nChain-of-Thought (CoT), on the other hand, is the current state-of-the-art method that enhances the input\nprompt by including human-annotated rationales for each few-shot example. The LLM is then expected to\ngenerate a rationale followed by an answer.\nImplementation Details.\nIn our experiments, we implemented the AO and CoT baselines using the same methodology as described\nin their respective works. F or CoT, we directly used the provided rationales from the original work for the\ncorresponding datasets [ 5]. In the case of AMPLIFY, we employed GPT-2[ 31] ﬁne-tuned for the target task as\nthe proxy model for step 1, unless stated otherwise. W e utilized a rationale template with k = 5, which is\nof the form: \"The key words: word1, word2, ...and word5 are important clues to predict [ground truth label]\nas the correct answer\". These keywords \"word1, word2, ...and word5\" were chosen based on the highest\nattribution scores obtained from the post hoc explanation computed in step 3. T o compute these attribution\nscores, we used Gradient x Input as the default post hoc explanation method for generating explanations.\n4.1 Empirical Analysis\nOverall Task Performance.\nW e demonstrate the eﬀectiveness of AMPLIFY by comparing the prediction accuracy of LLMs using prompts\ngenerated by AMPLIFY against baselines, i.e., Answer-Only (AO) prompts and Chain-of-Thought (CoT)\n5\nT able 2:F ew-shot prompting performance of multiple LLMs on the seven datasets when post hoc explanations,\nwhich form the rationale in the prompt constructed during step 4 of AMPLIFY, are generated using models with\nvarying degrees of ﬁne-tuning of the proxy model (GPT-2 in this case). Here, \"E\" represents the number of epochs\nthe proxy model was ﬁne-tuned. \"E = 0\" indicates that the proxy model was used to generate post hoc explanations\nwithout any ﬁne-tuning. The recorded performance in this table represents the percentage of test samples for which\nthe LLM accurately predicted the true label.\nGPT-3 GPT-3.5\nExperiment T asks E = 0 E = 10 E = 200 E = 0 E = 10 E = 200\nSnarks 77.7 80.5 80.5 88.8 88.8 91.6\nCausal Judgment 55.2 57.8 60.5 71.0 73.6 76.3\nRuin Names 74.1 75.2 78.6 65.1 67.4 77.5\nF ormal F allacies 53.7 56.9 60.1 48.3 51.6 59.8\nSalient T ranslation Error Detection 49.7 51.2 51.7 57.7 60.8 60.8\nCommonsenseQA 69.1 72.6 73.5 71.9 75.8 77.9\nCoin Flip (OOD) 56.4 60.8 65.7 55.4 61.4 65.3\nAll T asks (avg) 62.2 65.0 67.2 65.4 68.4 72.7\nT able 3:F ew-shot prompting performance of various large language models on the seven datasets is analyzed based\non diﬀerent selection strategies used for choosing samples during prompt design of LLMs, speciﬁcally in step 2 of\nFigure 1. The \"Random\" selection refers to randomly chosen samples. \"L-MCS \" signiﬁes the selection of samples with\nthe lowest Misclassiﬁcation Conﬁdence Score ( MCS). \"H-MCS \" represents the selection strategy of choosing samples\nwith the Misclassiﬁcation Conﬁdence Score ( MCS) for prompt design. \"F-Exp\" indicates the selection strategy of\nchoosing samples with the most faithful explanations for LLM prompt. The recorded performance in this table\nrepresents the percentage of test samples for which the LLM accurately predicted the true label.\nGPT-3 GPT-3.5\nExperiment T asks Random L-MCS H-MCS F-Exp Random L-MCS H-MCS F-Exp\nSnarks 69.4 80.5 80.5 77.7 69.4 88.8 91.6 88.8\nCausal Judgment 57.8 60.5 60.5 57.8 68.4 73.6 76.3 71.0\nRuin Names 65.1 77.5 78.6 74.1 66.2 77.5 77.5 73.0\nF ormal F allacies 52.3 57.9 60.1 59.7 46.7 51.5 59.9 58.6\nSalient T ranslation Error Detection 48.7 50.2 51.7 51.7 53.2 59.2 60.8 58.7\nCommonsenseQA 67.6 71.5 73.5 70.9 72.9 76.6 77.9 77.2\nCoin Flip (OOD) 54.7 60.1 65.7 58.5 57.8 61.6 65.3 61.1\nAll T asks (avg) 59.3 65.4 67.2 64.3 62.0 69.8 72.7 69.7\nT able 4:The table presents a performance comparison for when prompt is created using explanations generated by\nfour diﬀerent post hoc explanation methods. Grad: V anilla Gradient Method, Grad × Inp : Gradient x Input Method ,\nC-Grad and C-Grad × Inp are contrastive version of V anilla gradient and Gradient x Input. The recorded performance\nin this table represents the percentage of test samples for which the LLM accurately predicted the true label.\nGPT-3 GPT-3.5\nExperiment T asks Grad Grad × Inp C-Grad C-Grad × Inp Grad Grad × Inp C-Grad C-Grad × Inp\nSnarks 77.7 80.5 80.5 85.7 88.8 91.6 88.8 91.6\nCausal Judgment 60.5 60.5 57.8 60.5 71.0 76.3 71.0 73.6\nRuin Names 71.9 78.6 75.2 77.5 65.1 77.5 73.0 74.1\nF ormal F allacies 59.7 60.1 59.7 58.6 59.9 59.9 59.4 57.6\nSalient T ranslation Error Detection 49.7 51.7 51.7 50.7 59.7 60.8 60.8 60.8\nCommonsenseQA 72.1 73.5 72.9 73.0 73.7 77.9 75.5 77.9\nCoin Flip (OOD) 62.9 64.1 62.6 65.7 62.6 63.9 62.4 65.3\nAll T asks (avg) 64.9 67.0 65.7 67.3 68.6 72.5 70.1 71.5\nprompts. T able 1 displays the results of GPT-3 and GPT-3.5 across all datasets. W e observe that incorporating\nrationales generated by our approach leads to a substantial improvement in accuracy compared to both\nstandard Answer-Only (AO) prompts and Chain-of-Thought (CoT) prompts. Speciﬁcally , GPT-3.5 achieves\nstate-of-the-art performance on the Snarks dataset with a 91.6% accuracy in identifying the correct option;\nthis is 16% better than standard answer-only prompting and over 20% better than CoT. Similar trends were\nobserved for Causal Judgment, where our method delivered the best performance of 76.3%, signiﬁcantly\nsurpassing CoT (63.1%) and AO (57.8%). When using GPT-3, our approach attained the highest performance\nin Ruin Names (78.6%), a trend also evident in the case of F ormal F allacies. Finally , our method achieved\nthe top performance with the GPT-3.5, registering an accuracy of 60.8% for the Salient T ranslation Error\nDetection task. In the scenarios of commonsense reasoning (CommonsenseQA) and symbolic reasoning (Coin\nFlip) tasks, we noticed consistent trends, with\nAMPLIFY recording the highest performance. In the next set of\nanalyses, we examine the eﬀects of diﬀerent steps in AMPLIFY on the performance of LLMs.\n6\nT able 5:Proxy models performance with and without ﬁne-tuning.\nGPT-2 BER T\nExperiment T asks Pre-trained Fine-tuned Pre-trained Fine-tuned\nSnarks 38.8 47.2 30.5 38.8\nCausal Judgment 44.7 55.2 44.7 52.6\nRuin Names 07.8 26.9 10.1 22.4\nF ormal F allacies 50.5 54.4 51.6 53.5\nSalient T ranslation Error Detection 14.0 27.1 11.5 22.6\nCommonsenseQA 07.4 29.1 08.8 26.9\nCoin Flip 45.2 59.4 51.1 59.7\nImpact of Proxy Model Selection on LLM Performance.\nIn the following analysis, we investigate how the choices made at each step of AMPLIFY aﬀect the performance\nof LLM on the seven tasks. W e begin with step 1, which involves selecting a proxy model for sample selection\n(step 2) and computing post hoc explanations(step 3). In the experiments conducted to calculate the overall\nmodel performance, as shown in T able 1, we utilized a ﬁnetuned GPT-2 model for the target task as the\nproxy model. While using GPT-2 for ﬁnetuning is computationally cheaper compared to other LLMs, it\nis still expensive to ﬁnetune a model with more than 100 million parameters. Therefore, we examined the\nperformance of LLMs based on the amount of ﬁne-tuning, measured in terms of the number of epochs\n(E). This analysis aimed to understand the impact of ﬁnetuning on improving model performance. T able 2\npresents the model performance scores of LLMs when the proxy model is GPT-2 without any ﬁne-tuning on\nthe target task (E=0), with minor ﬁne-tuning (E=10), and when GPT-2 has achieved its best performance\nat epoch E=200. As depicted in T able 2, we observe that the model performance of LLMs with GPT-2 (E=0)\nis already quite close to the best performance achieved when GPT-2 is ﬁnetuned to saturation (E=200)\nfor most datasets. Speciﬁcally , the performance of GPT-3.5 for Snarks with GPT-2 (E=0) is 88.8%, which\nis signiﬁcantly better than the best performance of CoT shown in T able 1. This pattern is also evident\nin the case of Causal Judgment, Salient T ranslation Error Detection, and Ruin Names. There are two\nprimary reasons for this behavior. Firstly , GPT-2 possesses suﬃcient language understanding capabilities to\nprovide useful post hoc explanations that lead to accurate predictions. Secondly , most of the tasks where\nGPT-2 (E=0) achieved the best or near-best performance have very limited training data, which might not\nbe suﬃcient for GPT-2 to learn anything beyond what it has already acquired during pre-training. This\nobservation is further supported by the ﬁndings presented in T able 5, where the accuracy improvements for\nmost datasets are not substantial. These ﬁndings suggest that an additional step of ﬁne-tuning a pre-trained\nmodel may not always be necessary when selecting a proxy model in step 1 of AMPLIFY, thereby reducing\ncomputational costs even further.\nImpact of Selection Strategies on LLM Performance.\nIn this analysis, we focus on step 2 of AMPLIFY, which involves selecting samples for few-shot prompts that\ncan provide eﬀective corrective signals to assist LLMs in reducing misclassiﬁcations. As explained in section 3,\nthis step in AMPLIFY includes two sub-steps: ﬁrst, identifying misclassiﬁed samples by LLM on the validation\nset, and second, selecting samples with the highest MCS calculated with respect to proxy model. The ﬁrst\nstep is straightforward as we speciﬁcally focus on samples that were misclassiﬁed earlier by LLMs. F or\nsubsequent ﬁltering based on conﬁdence, we experiment with several sample selection strategies to better\nunderstand how the performance of the language model is sensitive to these diﬀerent strategies. The results,\nin terms of language model performance scores, are shown in T able 3. Speciﬁcally , we experiment with three\nselection strategies in addition to the one that selects samples with the highest MCS score, referred to as\n\"H-MCS \" in T able 3: (1) \"Random\": randomly selecting samples without considering Misclassiﬁcation\nConﬁdence Score, (2) \"L-MCS\": selecting samples with the lowest MCS, and (3) \"F-EXP\": selecting samples\nbased on the most faithful explanations, measured by the diﬀerence in model output when the top-K features\nidentiﬁed by the explanation are perturbed [ 10, 32]. Among these strategies, we ﬁnd that selecting samples\nwith the highest Misclassiﬁcation Conﬁdence Score yields the best performance, outperforming all other\nstrategies. Random selection performs the worst across all datasets, while the performance using \"L-MCS\" is\ncomparable to that of \"H-MCS\" selections. This suggests that samples for which the model is less conﬁdent\nin its predictions can still provide reasonable corrective signals in reducing misclassiﬁcations. Notably , the\nLLM performance with \"F-EXP\" sample selection is worse than \"L-MCS\" for most datasets (Snarks, Causal\nJudgment, Ruin Names, and CommonsenseQA), indicating that relying solely on faithful explanations may\nnot be suﬃcient for achieving optimal performance.\nImpact of Post Hoc Explanation Method on LLM Performance.\nW e then examine the impact on LLM performance due to the choice of post hoc explanation used to\nidentify top-k keywords in step 3. T o investigate this, we employ four diﬀerent explanation methods for\n7\nFig. 2: This image exempliﬁes an instance of Causal Judgment task where standard prompts and CoT produce\ninaccurate responses. The CoT response fails to take into account that the red wire should not make contact with\nthe battery , which caused the short circuit. In contrast, the response generated by AMPLIFY emphasizes this crucial\ndetail. Please note that while we inject rationales in terms of k individual words, we do not restrict LLMs from\ngenerating rationales in terms of phrases or multiple words. This is why we often see LLM-generated rationales having\nmulti-word clues, such as \"red wire,\" \"never supposed,\" and so on.\nstep 3 of AMPLIFY and record the LLM performance corresponding to each post hoc explanation method\nchoice in T able 4. Speciﬁcally , the four post hoc explanation methods used in this analysis are: (1) V anilla\nGradients [ 22] (Grad), (2) Gradient × Input [ 27] (Grad × Inp), (3) Contrastive Gradient [ 28] (C-Grad), and\n(4) Contrastive Gradient × Input [ 28] (C-Grad × Inp). Based on T able 4, we observe that the LLM performs\nbest in general when Gradient x Input or its contrastive variant is used to generate explanations. However,\nwe also note that there aren’t drastic changes in LLM performance across diﬀerent methods. F or instance,\nGPT-3.5 performance on Snarks doesn’t change much across diﬀerent methods, as the accuracy remains\nconsistently around 88.8-91.6%. This suggests that LLM performance isn’t sensitive to rationales generated\nusing diﬀerent variants of gradient-based post hoc explanation methods.\nImpact of Rationale Template on LLM Performance.\nLastly , in the ﬁnal step of AMPLIFY, we generate a few-shot prompt by combining an (input, label) pair and\nits corresponding set of more important words using the rationale template as \"The key words: word1, word2,\n...and word5 are crucial clues for predicting [ground truth label] as the correct answer\". W e have observed\nthat while using a task-independent rationale template leads to improvements in performance, tailoring the\nrationale to the question asked in the input sentence for a given dataset also provides beneﬁts. F or example,\nin the case of Causal Judgment, each sample includes a generic question: \"How would a typical person answer\neach of the following questions about causation?\" If we utilize the rationale template as \"After observing the\nkey words: word1, word2, ...and word5, a typical person would respond with [label] as the correct answer\",\nwe notice a slight enhancement in GPT-3 performance, rising from 60.5% to 63.1%. However, we did not\nobserve the model’s sensitivity to minor changes in the template, such as punctuation variations.\n8\nQualitative Analysis.\nIn addition to quantitatively evaluating the performance of AMPLIFY compared to other baselines, we also\nqualitatively examine how LLM responses diﬀer for certain test samples using each of the baseline prompting\napproaches, and compare them to the responses generated by AMPLIFY. Figure 2 illustrates the responses\ngenerated by GPT-3.5 for an input sample using the Standard Prompt (AO), Chain-of-Thought (CoT), and\nAMPLIFY. In this particular example, both AO and CoT yield incorrect responses, whereas AMPLIFY produces\nthe correct response. Analyzing the responses reveals that CoT and AO miss an important sentence in\nthe sample: \"The red wire is never supposed to touch the battery as it moves around inside the machine\".\nInterestingly , GPT-3.5 captures this crucial information when the prompt is augmented with post hoc\nexplanations using AMPLIFY. W e observe similar examples for CommonsenseQA, such as \"Q: Unlike a spider\nand its many observers, people only have what? Answer Choices: (A) tongues (B) names (C) brains (D)\nfeelings (E) two eyes.\". In this case, CoT incorrectly selects option (C), whereas AMPLIFY correctly predicts\noption (E). This error also stems from the same issue of CoT overlooking crucial information that the\nquestion pertains to eyes rather than the entire body , a nuance that\nAMPLIFY successfully captures. This\ndemonstrates that the rationales generated by AMPLIFY can assist LLMs in capturing critical signals that\nmight have otherwise been overlooked.\n5 Conclusion\nIn this work, we introduce AMPLIFY, a novel framework aimed at improving the performance of LLMs\nby replacing human-annotated rationales with automated rationales obtained using post hoc explanation\ntechniques. Our unique four-step approach leverages smaller, open-source models for eﬃcient computation of\npost hoc explanations. Our framework results in performance improvements of 10-25% across diverse tasks,\noutperforming conventional techniques such as CoT prompting which rely on human-annotated rationales.\nOur ﬁndings highlight the potential of post hoc explanation methods as valuable tools for enhancing the\neﬀectiveness of LLMs.\nAcknowledgments. W e thank Adam Pearce for insightful feedback on this work.\nReferences\n[1]\nBommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., Arx, S., Bernstein, M.S., Bohg, J.,\nBosselut, A., Brunskill, E., et al.: On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258 (2021)\n[2] W ei, J., T ay , Y., Bommasani, R., Raﬀel, C., Zoph, B., Borgeaud, S., Y ogatama, D., Bosma, M., Zhou, D.,\nMetzler, D., et al.: Emergent abilities of large language models. arXiv preprint arXiv:2206.07682 (2022)\n[3] Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P ., Lee, Y.T., Li,\nY., Lundberg, S., et al.: Sparks of artiﬁcial general intelligence: Early experiments with gpt-4. arXiv\npreprint arXiv:2303.12712 (2023)\n[4] Dong, Q., Li, L., Dai, D., Zheng, C., W u, Z., Chang, B., Sun, X., Xu, J., Sui, Z.: A survey for in-context\nlearning. arXiv preprint arXiv:2301.00234 (2022)\n[5] W ei, J., W ang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., Zhou, D.: Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022)\n[6] Lampinen, A.K., Dasgupta, I., Chan, S.C., Matthewson, K., T essler, M.H., Creswell, A., McClelland,\nJ.L., W ang, J.X., Hill, F.: Can language models learn from explanations in context? arXiv preprint\narXiv:2204.02329 (2022)\n[7] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch, A., Brown, A.R., Santoro, A.,\nGupta, A., Garriga-Alonso, A., et al.: Beyond the imitation game: Quantifying and extrapolating the\ncapabilities of language models. arXiv preprint arXiv:2206.04615 (2022)\n[8] Zhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S., W ang, Y., Saied, A., Chen, W., Duan, N.: Agieval: A\nhuman-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364 (2023)\n[9] Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., T ay , Y., Chung, H.W., Chowdhery , A., Le, Q.V.,\nChi, E.H., Zhou, D., et al.: Challenging big-bench tasks and whether chain-of-thought can solve them.\narXiv preprint arXiv:2210.09261 (2022)\n9\n[10] T urpin, M., Michael, J., Perez, E., Bowman, S.R.: Language models don’t always say what they think:\nUnfaithful explanations in chain-of-thought prompting. arXiv preprint arXiv:2305.04388 (2023)\n[11] Choudhary , S., Chatterjee, N., Saha, S.K.: Interpretation of black box nlp models: A survey . arXiv\npreprint arXiv:2203.17081 (2022)\n[12] Idahl, M., Lyu, L., Gadiraju, U., Anand, A.: T owards benchmarking the utility of explanations for\nmodel debugging. arXiv preprint arXiv:2105.04505 (2021)\n[13] Jesus, S., Belém, C., Balayan, V., Bento, J., Saleiro, P ., Bizarro, P ., Gama, J.: How can i choose an\nexplainer? an application-grounded evaluation of post-hoc explanations. In: Proceedings of the 2021\nACM Conference on F airness, Accountability , and T ransparency , pp. 805–815 (2021)\n[14] Liu, P ., Y uan, W., F u, J., Jiang, Z., Hayashi, H., Neubig, G.: Pre-train, prompt, and predict: A systematic\nsurvey of prompting methods in natural language processing. ACM Computing Surveys 55(9), 1–35\n(2023)\n[15] Doshi-V elez, F., Kim, B.: T owards a rigorous science of interpretable machine learning. arXiv preprint\narXiv:1702.08608 (2017)\n[16] Madsen, A., Reddy , S., Chandar, S.: Post-hoc interpretability for neural nlp: A survey . ACM Computing\nSurveys 55(8), 1–42 (2022)\n[17] Ribeiro, M.T., Singh, S., Guestrin, C.: “ Why should I trust you?\" Explaining the predictions of any\nclassiﬁer. In: ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (2016)\n[18] Lundberg, S.M., Lee, S.-I.: A uniﬁed approach to interpreting model predictions. In: Advances in Neural\nInformation Processing Systems (2017)\n[19] Zeiler, M.D., F ergus, R.: Visualizing and understanding convolutional networks. In: Computer Vision–\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I\n13, pp. 818–833 (2014). Springer\n[20] Smilkov, D., Thorat, N., Kim, B., Viégas, F., W attenberg, M.: Smoothgrad: Removing noise by adding\nnoise. arXiv preprint arXiv:1706.03825 (2017)\n[21] Sundararajan, M., T aly , A., Y an, Q.: Axiomatic attribution for deep networks. In: International\nConference on Machine Learning (2017)\n[22] Simonyan, K., V edaldi, A., Zisserman, A.: Deep inside convolutional networks: Visualising image\nclassiﬁcation models and saliency maps. In: International Conference on Learning Representations (2014)\n[23] T almor, A., Herzig, J., Lourie, N., Berant, J.: CommonsenseQA: A question answering challenge targeting\ncommonsense knowledge. In: Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language T echnologies, V olume 1 (Long and Short\nPapers), pp. 4149–4158. Association for Computational Linguistics, Minneapolis, Minnesota (2019).\nhttps://doi.org/10.18653/v1/N19-1421 . https://aclanthology .org/N19-1421\n[24] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P ., Neelakantan, A., Shyam, P .,\nSastry , G., Askell, A., et al.: Language models are few-shot learners. Advances in neural information\nprocessing systems 33, 1877–1901 (2020)\n[25] GPT-3.5-T urbo. https://platform.openai.com/docs/model-index-for-researchers . Accessed: 2022-01-01\n[26] Ouyang, L., W u, J., Jiang, X., Almeida, D., W ainwright, C., Mishkin, P ., Zhang, C., Agarwal, S., Slama,\nK., Ray , A., et al.: T raining language models to follow instructions with human feedback. Advances in\nNeural Information Processing Systems 35, 27730–27744 (2022)\n[27] Shrikumar, A., Greenside, P ., Kundaje, A.: Learning important features through propagating activation\ndiﬀerences. In: Proceedings of the 34th International Conference on Machine Learning (2017)\n[28] Yin, K., Neubig, G.: Interpreting language models with contrastive explanations. arXiv preprint\narXiv:2202.10419 (2022)\n10\n[29] Radford, A., W u, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are unsupervised\nmultitask learners. (2019)\n[30] Devlin, J., Chang, M., Lee, K., T outanova, K.: BER T: pre-training of deep bidirectional transformers\nfor language understanding. In: Annual Conference of the North American Chapter of the Association\nfor Computational Linguistics (NAACL) (2018)\n[31] Radford, A., W u, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. : Language models are\nunsupervised multitask learners. OpenAI blog 1(8), 9 (2019)\n[32] Slack, D., Krishna, S., Lakkaraju, H., Singh, S.: T alktomodel: Understanding machine learning models\nwith open ended dialogues. arXiv preprint arXiv:2207.04154 (2022)\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7551418542861938
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6127254366874695
    },
    {
      "name": "Language model",
      "score": 0.5145191550254822
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5041543245315552
    },
    {
      "name": "Scalability",
      "score": 0.4782768189907074
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4581761062145233
    },
    {
      "name": "Data science",
      "score": 0.42903053760528564
    },
    {
      "name": "Construct (python library)",
      "score": 0.4263423681259155
    },
    {
      "name": "Machine learning",
      "score": 0.35462141036987305
    },
    {
      "name": "Cognitive science",
      "score": 0.33124101161956787
    },
    {
      "name": "Psychology",
      "score": 0.13823577761650085
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2801851002",
      "name": "Harvard University Press",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204250578",
      "name": "University of California, Irvine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ]
}