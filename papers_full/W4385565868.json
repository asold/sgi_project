{
  "title": "Detecting Label Errors by Using Pre-Trained Language Models",
  "url": "https://openalex.org/W4385565868",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5113999788",
      "name": "Derek Chong",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5107516917",
      "name": "Jenny Hong",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A5046006076",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3156669901",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3202345803",
    "https://openalex.org/W4221015417",
    "https://openalex.org/W2564143201",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2121056381",
    "https://openalex.org/W2008463764",
    "https://openalex.org/W2043765769",
    "https://openalex.org/W1532027897",
    "https://openalex.org/W3042609801",
    "https://openalex.org/W4287252712",
    "https://openalex.org/W2027731328",
    "https://openalex.org/W3209948030",
    "https://openalex.org/W4295253428",
    "https://openalex.org/W3192957100",
    "https://openalex.org/W3212733079",
    "https://openalex.org/W4287110955",
    "https://openalex.org/W3137010024",
    "https://openalex.org/W1759994076",
    "https://openalex.org/W3168363436",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3013573099",
    "https://openalex.org/W3104300670",
    "https://openalex.org/W2252039660",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4224903910",
    "https://openalex.org/W2068376489",
    "https://openalex.org/W3037062548",
    "https://openalex.org/W3123404044",
    "https://openalex.org/W3215436086",
    "https://openalex.org/W4287323896",
    "https://openalex.org/W371426616",
    "https://openalex.org/W3162754606",
    "https://openalex.org/W3195098128",
    "https://openalex.org/W2618574054",
    "https://openalex.org/W3010009477",
    "https://openalex.org/W1966716734",
    "https://openalex.org/W2963939124",
    "https://openalex.org/W4225463537",
    "https://openalex.org/W3154813414",
    "https://openalex.org/W2979501189",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3104382126"
  ],
  "abstract": "We show that large pre-trained language models are inherently highly capable of identifying label errors in natural language datasets: simply examining out-of-sample data points in descending order of fine-tuned task loss significantly outperforms more complex error-detection mechanisms proposed in previous work. To this end, we contribute a novel method for introducing realistic, human-originated label noise into existing crowdsourced datasets such as SNLI and TweetNLP. We show that this noise has similar properties to real, hand-verified label errors, and is harder to detect than existing synthetic noise, creating challenges for model robustness.We argue that human-originated noise is a better standard for evaluation than synthetic noise. Finally, we use crowdsourced verification to evaluate the detection of real errors on IMDB, Amazon Reviews, and Recon, and confirm that pre-trained models perform at a 9–36% higher absolute Area Under the Precision-Recall Curve than existing models.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9074–9091\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nDetecting Label Errors by using Pre-Trained Language Models\nDerek Chong∗\nStanford University\nderekch@stanford.edu\nJenny Hong∗\nStanford University\njennyhong@cs.stanford.edu\nChristopher D. Manning\nStanford University\nmanning@cs.stanford.edu\nAbstract\nWe show that large pre-trained language\nmodels are inherently highly capable of\nidentifying label errors in natural language\ndatasets: simply examining out-of-sample\ndata points in descending order of fine-\ntuned task loss significantly outperforms\nmore complex error-detection mechanisms\nproposed in previous work. To this end,\nwe contribute a novel method for introduc-\ning realistic, human-originated label noise\ninto existing crowdsourced datasets such as\nSNLI and TweetNLP. We show that this\nnoise has similar properties to real, hand-\nverified label errors, and is harder to detect\nthan existing synthetic noise, creating chal-\nlenges for model robustness. We argue that\nhuman-originated noise is a better standard\nfor evaluation than synthetic noise. Finally,\nwe use crowdsourced verification to evalu-\nate the detection of real errors on IMDB,\nAmazon Reviews, and Recon, and confirm\nthat pre-trained models perform at a 9–36%\nhigher absolute Area Under the Precision-\nRecall Curve than existing models.\n1 Introduction\nImproving model performance in the presence\nof label errors comprises an area of active re-\nsearch (Song et al., 2022). However, existing\nmethods focus on label errors in training data.\nAlthough seldom acknowledged, evaluation la-\nbel errors are at least as pernicious as training\nlabel errors: pervasive errors in commonly used\nNLP benchmarks have been found to destabi-\nlize model performance (Malik and Bhardwaj,\n2011; Northcutt et al., 2021b). Such findings\nsuggest that improving training methods does\nnot preclude the need for improving the un-\nderlying data. We propose a simple method\nfor using large, pre-trained language models\n∗Equal contribution.\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPrecision\nBaseline\nFM Loss\nFM Ensemble\nCL Overlays\nFigure 1: Precision-recall curves for label error\ndetection: Large language models detect label er-\nrors with high precision, and far more effectively\nthan a baseline word vector-based neural classifier.\nOverlaying a state-of-the-art model-agnostic error\ndetection method, Confident Learning, results in\nlittle to no improvement (TweetNLP-5; §7).\n(LLMs) to directly identify label errors for the\npurposes of correcting or removing them.\nThe majority of work in identifying label\nerrors, and in data-centric artificial intelli-\ngence (DCAI) more broadly, focuses on image\nand healthcare data (DCAI Workshop, 2021).\nHowever, the success of the foundation model\n(FM) paradigm in applying pre-trained lan-\nguage models to a variety of NLP tasks (Bom-\nmasani et al., 2021; Reiss et al., 2020) sug-\ngests that FMs may be a powerful tool for de-\ntecting and correcting label errors in language\ndatasets. Pre-training has been shown to im-\nbue models with properties such as resistance\nto label errors, class imbalance (Karthik et al.,\n2021), out-of-distribution detection (Hendrycks\net al., 2018), and confidence calibration (De-\nsai and Durrett, 2020), while conferring ro-\n9074\nDataset Text Label Sentiment\nIMDB It is really unfortunate that a movie so well produced turns out to be\nsuch a disappointment. I thought this was full of (silly) cliches. It had\nall sorts of differences that it tried to tie together (not a bad thing in\nitself) but the result is at best awkward, but in fact ridiculous–too many\nclashes that wouldn’t really happen. Then the end of the movie–the\nlast 10 minutes–ruined all the rest. At first I thought Xavier was OK\nbut with retrospect I think he was pretty bad. And that’s all really too\nbad, because technically it was really good, and the soundtrack was\ngreat too. So the form was good, but the content pretty horrible.\nPositive Negative\nIMDB The ending made my heart jump up into my throat. I proceeded to\nleave the movie theater a little jittery. After all, it was nearly midnight.\nThe movie was better than I expected. I don’t know why it didn’t\nlast very long in the theaters or make as much money as anticipated.\nDefinitely would recommend.\nNegative Positive\nAmazon The new design only has a thin layer of cellulose sponge material. It\nwill not last as long. Already showing signs of wearing out. The picture\ndoes not represent the item received.\nNeutral Negative\nTable 1: Organic label errors from sentiment datasets IMDB and Amazon, shown with the original dataset\nlabel. Each example was hypothesized by our model to be erroneous, and later verified by crowd workers.\nbustness, generalization, and natural language\nunderstanding capabilities (Wang et al., 2018;\nPetroni et al., 2019). Our primary contribu-\ntion is to show that simply verifying items in\norder of their out-of-sample loss on a founda-\ntion model improves precision by an absolute\n15–28% and Area Under the Precision-Recall\nCurve (AUPR) by an absolute 9–36%.\nMany methods for label error detection rely\non artificially introduced label errors as ground\ntruth for evaluating their methods. Northcutt\net al. (2021a) develop a state-of-the-art model\nfor identifying label errors, Confident Learning\n(CL), and use the better approach of crowd-\nsourced human evaluations to determine the\nground truth of label errors. We model our\nexperiments on real data after their verifica-\ntion protocol, replicating this on real errors in\nIMDB (Maas et al., 2011), Amazon Reviews\n(McAuley et al., 2015), and Recon (Hong et al.,\n2021), with adaptations to mitigate annotator\nfraud (Kennedy et al., 2020).\nIn the process of assessing our results, we\ncontribute a novel technique and protocol for\nintroducing realistic, human-originated label\nnoise into existing crowdsourced datasets, and\napply it to two such datasets, TweetNLP (Gim-\npel et al., 2010) and SNLI (Bowman et al.,\n2015). We demonstrate that our technique bet-\nter approximatesorganic(real, naturally occur-\nring) label errors than existing methods. We\nprovide evidence that this realism is essential\nto properly assessing model performance: even\nmodels that are robust to standard synthetic\nnoising approaches show limited robustness to\nhuman-originated noise.1\n2 Related Work\nLearning with Noisy Labels (LNL) focuses on\nthe model-training stage. Noise-robust ap-\nproaches examine model enhancements such\nas the design of loss functions (Joulin et al.,\n2016; Amid et al., 2019; Liu and Guo, 2020;\nMa et al., 2020), regularization (Azadi et al.,\n2015; Zhou and Chen, 2021), reweighting (Bar\net al., 2021; Kumar and Amid, 2021), hard neg-\native mining and contrastive learning (Zhang\nand Stratos, 2021). Noise-cleansing approaches\naim to segregate clean data from noisy data in\ntraining, e.g. bagging and boosting (Wheway,\n2000; Sluban et al., 2014),k-nearest neighbors\n(Delany et al., 2012), outlier detection (Gam-\nberger et al., 2000; Thongkam et al., 2008),\nbootstrapping (Reed et al., 2014), and neural\nnetworks supervised directly on detecting an\nerror, when such data exist (Jiang et al., 2018).\nLNL methods have in most cases been eval-\nuated using artificially-generated label noise.\nA typical evaluation of an LNL method uses\na standard benchmark dataset, and program-\nmatically corrupts training labels via one of\nthree main noising schemes (Frenay and Ver-\nleysen, 2014; Algan and Ulusoy, 2020).Uni-\n1Data noising library and evaluation data available\nat https://github.com/dcx/lnlfm.\n9075\nform noiseis most commonly used but unre-\nalistic; deep neural networks have been found\nto perform well even when noised labels out-\nnumber original labels at a ratio of 100 to 1\n(Rolnick et al., 2017).Class-dependent noise\nrandomly permutes labels based on a confu-\nsion matrix. However, research on annotator\ndisagreement suggests that label errors tend\nto result from feature-based, not class-based\nambiguity (Hendrycks et al., 2018). Training\nmodels to generate realisticfeature-based or\ninstance-dependent noisehas recently emerged\nas an area of active research (Chen et al., 2021b;\nXu et al., 2021; Dawson and Polikar, 2021).\nHowever, Algan and Ulusoy (2020) report that\nfeature-dependent noise may bias benchmark\nperformance toward similar models to the ones\nused to generate this noise.\nThe noising schemes above each fail in some\nway to simulate organic, naturally occurring\nlabel errors, which are estimated to occur in\ncommon benchmarks at 1–5% of labels (Red-\nman, 1998; Müller and Markert, 2019; North-\ncutt et al., 2021b; Kreutzer et al., 2022) or even\nas much as 20% (Hovy et al., 2014; Abedjan\net al., 2016). For organic errors, CL (Northcutt\net al., 2021a) predicts errors in IMDB, Ama-\nzon Reviews, and other datasets by estimating\na joint distribution between noisy and uncor-\nrupted labels; Reiss et al. (2020) pioneers using\nBERT for error detection on ConLL-2003 via\na classifier trained over a frozen BERT embed-\ndings layer.\n3 Methods\nMotivation. Empirical evidence on image\ndata suggests that models exhibit high loss\non label errors in training data relative to the\nunderlying features (Huang et al., 2019; Kim\net al., 2021; Hong et al., 2021; Chen et al.,\n2021a). Hendrycks and Gimpel (2017) show\nthat predicted probabilities of (non pre-trained)\nneural networks can identify out-of-distribution\nexamples. We consider the framing that label\nerrors are one type of out-of-distribution data.\nIndeed, CL (Northcutt et al., 2021a) uses nor-\nmalized predicted probabilities, also from non\npre-trained models, to directly identify label er-\nrors. Foundation models are highly performant;\nwe hypothesize that a low likelihood label is\nlikely to be an error.\n100 101\nAverage Loss\n0.0\n0.2\n0.4\n0.6\nError Detection Precision\nBERT\nDeBERTa\nGPT-2\nRoBERTa\nOther\nRandom\nFigure 2: Loss exhibits a strong log-linear rela-\ntionship with error detection precision at a fixed\nthreshold, across a broad range of models and hy-\nperparameters (r2: 0.94; TweetNLP-5, §7).\nFoundation models. The success of Reiss\net al. (2020)’s approach in using frozen BERT\nembeddings motivates directly applying the\nfoundation model paradigm: we use a large\nlanguage model that was first pre-trained on a\ntask-agnostic dataset, then fine-tune the model\nfor a given task.\nWe address classification tasks: given a\nmodel’s scorefi,c for each itemiand classc, its\npredicted probability is the softmax-normalized\nscore p(c|xi). Because each item belongs to\nexactly one class, the contribution of itemi to\nthe loss is the negative log probability of the\nscore for the assigned classyi:\nLi =\n∑\ni\n−log p(yi |xi).\nWe fine-tune such a model for the training split\nof each data set. To identify label errors on\na validation or test set, we hypothesize items\nfrom the dataset as a label error in order of\nthe item’s loss on that out-of-distribution set.\nWe propose two main methods. Founda-\ntion Model Loss (FML) uses a single foun-\ndation model, fine-tuned on the correspond-\ning task (e.g., sentiment classification, POS\ntagging), to hypothesize items in order of the\nmodel-predicted loss. We augment FML using\ntask-adaptive pre-training (TAPT; Gururan-\ngan et al., 2020), which is further pre-training\non in-domain data, using only text on the pre-\ntraining objective without using any labels for\nfine-tuning on the cross-entropy objective.\nFoundation Model Ensembling (FME) com-\nbines multiple foundation models on the same\n9076\n0% 5% 10% 15%\n75%\n80%\n85%\n90%\n95%\nTask Accuracy\nUniform Noise\n0% 5% 10% 15%\nClass-Based Noise\n0% 5% 10% 15%\nDissenting Label\n0% 5% 10% 15%\nDissenting Worker\n0% 5% 10% 15%\n% Dataset Noised\nCrowd Majority\nMeasurable Performance (Noisy Test) True Performance (Clean Test)\nFigure 3: Assessing model robustness against a range of noising methods on TweetNLP, with methods\nordered by hypothesized realism. Solid orange lines report task performance on noisy test data, reflecting\nobservations in practice; dashed blue lines report task performance on underlying clean test data, reflecting\nmodels’ actual performance. Models may be robust to uniform and class-dependent noise, where the true\nperformance remains high even with increasing levels of noise. However, they are not necessarily robust to\nhuman-originated noise, where the true test performance decreases with increasing noise.\ntask. We hypothesize that ensembling may be\ndisproportionately effective at detecting label\nerrors, as training noise induces models to learn\nrandom spurious correlations (Watson et al.,\n2022). Rather than using a validation set to\nchoose the single model with the lowest loss\non the task, FME uses the top three models\ntrained in a hyperparameter sweep, and dif-\nfering in both hyperparameters and random\ninitialization, as fully described in Appendix D.\nFME creates a synthetic probability distribu-\ntion over the task outputs by averaging the\nprobabilities predicted using each individual\nmodel. FME then hypothesizes items in order\nof loss over the synthetic distribution.\n4 Generating Realistic Label Noise\nTo better evaluate label noise detection perfor-\nmance, we prepare a set of benchmark datasets\npopulated with controllable, highly realistic,\nhuman-originated label noise.\nSources of human error. We observe that\ndatasets often undergo multiple annotation\npasses: crowdsourced labels typically aggregate\nseveral annotators’ inputs (Hovy et al., 2014;\nWei et al., 2022), and subsets of data may re-\nceive more extensive validation (Bowman et al.,\n2015), gold labels by trained experts (Plank\net al., 2014), or correction passes (Reiss et al.,\n2020). We hypothesize that differences between\nsuch annotations may be usefully repurposed\nas a source of realistic,human-originated la-\nbel noise, as disagreements between annotators\nis known to reflect systematic ambiguity and\nhuman error (Plank et al., 2014; Zhang et al.,\n2017), and differs from the type of noise studied\nusing existing synthetic methods.\nWe construct three noising methods which\nmay be applied in many of the above scenar-\nios. For any dataset which includes two levels\nof label quality, thedissenting label method\nreplaces final labels with disagreeing labels at\nrandom, simulating imperfect quality control.\nDatasets which provide individual annotator\nidentifiers may apply thedissenting workerap-\nproach: select one annotator at random, apply\nall of their labels which disagree with final la-\nbels, and repeat until reaching the target noise\nrate. This simulates gaps in annotator train-\ning, which introduce systematic idiosyncrasies.\nFinally thecrowd majoritymethod applies to\nany dataset in which individual annotations\ncan be aggregated to produce a label other\nthan the final label: the former label simulates\nchallenging, systematic errors in the latter.\nNoising and robustness. We assess the ef-\nfect of these noising methods using TweetNLP\n(Gimpel et al., 2010), a corpus of 26,435 to-\nkens from 1,827 American English tweets col-\nlected from Twitter used to train part-of-speech\n(POS) tagging. TweetNLP includes gold labels\nannotated by 17 experts, but later received\na separate crowdsourced assessment, aggre-\ngated by majority vote (Hovy et al., 2014).\nWe noise TweetNLP to eight levels from 0-\n20% separately for each method, fine-tune\nDeBERTA-v3-base (He et al., 2021) on each\nnoising, and evaluate models on both noisy\nand clean test sets. Results from noisy test sets\nrepresent model performance asmeasurable in\n9077\n0 2 4 6 8\nLosses of Noised Test Items\n0.0\n0.2\n0.4\n0.6\nDensity\nUniform Noise\nClass-Based Noise\nDissenting Label\nDissenting Worker\nCrowd Majority\nClean Data Points\nFigure 4: Distributions of losses of label errors\non TweetNLP at 5% noising. Uniform and class-\nbased noise produce high and distinctive losses;\nhuman-originated noise is widely distributed, and\nhas greater overlap with the distribution of clean\ndata points; §6.\npractice; real datasets contend with noise in\nevaluation data. Clean test set results repre-\nsent true model performance. Fig. 3 reports\nthe results of this evaluation.\nFor uniform and class-dependent noise, true\nperformance remains high even for high noise\nlevels (per Rolnick et al., 2017). But crucially,\nthis robustness does not extend to human-\noriginated noise: human label errors are corre-\nlated to input text, and so contain system-\natic erroneous features, which models may\nlearn in training. On more challenging nois-\ning methods, although measured performance\nappears to increase, true performance actually\nlinearly decreaseswith noise. Fig. 4 explores\nthis further via the distributions of model\nlosses for each noising method: loss induced by\nhuman-originated noise overlaps significantly\nwith clean items, whereas loss from uniform\nand class-based noising is distinctively higher.\nNoise detection benchmarks. We stan-\ndardize a set of benchmarks from existing\ndatasets for use in our main experiments.\nTweetNLP-5 and SNLI-5 aim to simulate typi-\ncal data noise conditions: we apply dissenting\nworker and dissenting label noising to a 5%\nlevel (see Appendix A for details). SNLI is\na corpus of 570,152 sentence pairs, in which\nthe task is to label each pair with entailment,\ncontradiction, or semantic independence; we\nuse the 10% subset which includes five crowd-\nsourced annotations per item, as collected by\nBowman et al. (2015) during data validation.\nWe construct TweetNLP-M to investigate ro-\nbustness to systematic error introduced by the\ncrowdsourcing process. We apply crowd ma-\njority noising, comparing noisy majority-vote\naggregated labels by Hovy et al. (2014) to clean\nexpert labels, which serve as a measure of true\nperformance. Accordingly, we retain all dis-\nagreements, or 20.46% of the dataset. We also\nreport results on Recon, a legal classification\ndataset of 1,279 documents in which Hong et al.\n(2021) found label errors to destabilize model\nevaluation; as above, we compare non-expert\nand expert annotator labels.\n5 Validation on Real Label Errors\nIn addition to human-originated noise datasets,\nwe evaluate error detection performance on\norganic errors in two benchmark datasets, fol-\nlowing Northcutt et al. (2021a)’s protocol.\nDatasets. The IMDB Large Movie Review\nDataset is a collection of movie reviews for\nbinary sentiment classification (Maas et al.,\n2011), and is split into train and test sets of\n25,000 items each. Amazon Reviews is a collec-\ntion of reviews and 5-point star ratings from\nAmazon customers (McAuley et al., 2015). We\nused the version released by Northcutt et al.\n(2021a), which includes the following modifica-\ntions: It uses 1-star, 3-star, 5-star reviews with\nnet positive helpful upvotes as a ternary senti-\nment task, resulting in a dataset of 9,996,437\nreviews. For tractability we use a train split of\na random sample of 2.5 million items, and a\ntest split of 25,000 items.\nBaseline protocol. Workers are presented\nwith review text and asked to determine\nwhether overall sentiment is positive, negative,\nneutral, or off-topic. Each review is indepen-\ndently presented to five workers. An example\nis considered a “Non-Error” if at least three\nworkers agree the original label is correct. Oth-\nerwise, we consider the label to be correctly\nidentified as an error. We further categorize\nlabel errors as “Correctable” if at least three\nworkers agree on the same replacement label,\nor “Non-Agreement” if no majority exists.\nNew adaptations. While conducting initial\nexperiments, we found that the Northcutt et al.\n9078\nIMDB New Protocol\nOld Protocol C NA NE Total\nCorrectable 105 44 24 173\nNon-Agreement 75 252 225 552\nNon-Error 3 62 520 585\nTotal 183 358 769 1310\nAmazon New Protocol\nOld Protocol C NA NE Total\nCorrectable 142 43 117 302\nNon-Agreement 140 79 211 430\nNon-Error 75 31 162 268\nTotal 357 153 490 1000\nTable 2: Re-evaluation of baselines: The number\nof Correctable, Non-Agreement, andNon-Error\nassessments produced by the CL Mechanical Turk\nevaluation protocol and the new protocol, on the\nsame set of items. The new protocol substantially\nreduces annotator non-agreement; §5.\n(2021a) MTurk protocol resulted in a signifi-\ncant amount of annotator fraud. Some workers\nspent unreasonably short amounts of time on\nthe text, and frequently disagreed with both\nexpert and peer annotators, reflecting increas-\ningly common issues in crowdsourced anno-\ntations (Kennedy et al., 2020). Appendix C\ndescribes four extra conditions we added to\nimprove the Northcutt et al. (2021a) protocol.\nIn order to establish an accurate baseline, we\nre-evaluate the label errors hypothesized by CL\n(Northcutt et al., 2021a). On the new protocol,\nFleiss’ κ inter-annotator agreement increases\nfrom 0.131 to 0.464 for IMDB, and 0.014 to\n0.556 for Amazon, and Table 2 shows that Non-\nAgreement decreases by 35% in IMDB and\n65% in Amazon. This suggests a substantial\ndecrease in low-quality annotations.\n6 Experiments\nLabel noise realism. Section 4 defined the\nhuman-originated noising protocol used to gen-\nerate TweetNLP-5, TweetNLP-M, and SNLI-\n5. Section 5 specified a protocol for identify-\ning organic label errors present in IMDB and\nAmazon. We assess the realism of synthetic\nnoise methods by comparing loss distributions\nagainst models trained with organic noise (for\nreal label errors, we refer to items verified as\nCorrectable via MTurk). We quantify the de-\n0 2 4 6 8\nLosses of Noised Test Items\n0.0\n0.2\n0.4\n0.6\nDensity\nUniform Noise\nClass-Based Noise\nReal Label Errors\nClean Data Points\nFigure 5: Distributions of losses of hypothesized\nlabel errors that MTurk workers verified for IMDB.\nAs with Fig. 4, uniform and class-based methods\ndo not approximate real, worker-identified errors,\nand losses of real label errors have greater overlap\nwith the distribution of clean data; §6.\ngree to which noising induces erroneous learn-\ning by measuring the Wasserstein distances\nbetween noisy and clean loss distributions.\nOverall LLM performance. We assess\nbroad error detection capabilities by evaluating\n13 commonly-used LLMs on TweetNLP-5. We\nmeasure performance against loss, model size,\nand GLUE score (a proxy for general model\ncapability; Wang et al., 2018). Appendix D\nprovides implementation details. This experi-\nment’s results inform model selection: we use\nDeBERTA-v3-base for all further experiments.2\nMain experiment. Using our realistic nois-\ning benchmarks, and the MTurk baselines and\nverification protocol, we can now assess the per-\nformance of each label error detection method.\nWe evaluate Foundation Model Loss (FML)\nand Foundation Model Ensembling (FME).\nAs a baseline, we evaluate Confident Learn-\ning (CL; Northcutt et al., 2021a). CL is not a\nstandalone method; it augments existing mod-\nels. Given an underlying model’s predicted\nscores for each class and the true proportion\nof each class, CL forms a reweighting matrix,\ncalled the confident joint. To form a label er-\nror prediction score, CL reweights the model’s\nscores by the confident joint. CL hypothesizes\nitems in order of this resulting score.\nCL uses FastText (Joulin et al., 2017) for\nIMDB and Amazon, but includes no implemen-\n2We also use RoBERTa-BigBird for Recon in order\nto handle its long input passages (Hong et al., 2021).\n9079\nArea Under Precision-Recall Curve Precision, Recall @ Error% 3 Recall @ 2· Error%\nI Am. R T-5 T-M S-5 I Am. R T-5 T-M S-5 R T-5 T-M S-5\nH&G - - - 0.30 0.41 0.20 - - - 0.31 0.44 0.22 - 0.54 0.63 0.34\nCL 0.24 0.31 0.25 0.30 0.41 0.17 0.41 0.51 0.31 0.36 0.44 0.18 0.46 0.47 0.63 0.32\nFML 0.58 0.39 0.37 0.66 0.48 0.54 0.68 0.64 0.46 0.65 0.47 0.45 0.62 0.88 0.64 0.66\nFME 0.60 0.40 0.38 0.68 0.48 0.61 0.69 0.66 0.38 0.66 0.48 0.46 0.69 0.88 0.65 0.68\nFME+CL 0.20 0.17 0.37 0.68 0.48 0.62 - - 0.38 0.69 0.48 0.47 0.69 0.89 0.66 0.68\nTable 3: Main experiment: Evaluating label error detection methods using datasets containing highly-\nrealistic label errors (IMDB, Amazon Reviews,Recon, TweetNLP-5, TweetNLP-M, SNLI-5). Foundation\nmodel-based methods significantly outperform baselines on every dataset, as shown by an overall perfor-\nmance metric (AUPR). In practice, estimating the number of dataset errors and checking this many items\nquickly catches up to 69% of errors, at the same accuracy (P,R@Err%).3 For improved coverage, checking\ntwice this number of items catches up to 89% of errors (R@2·Err%).\ntations for POS tagging or NLI. As a result,\nfor TweetNLP and SNLI, we apply CL to the\nH&G baseline (Hendrycks and Gimpel, 2017),\na two-layer neural classifier over word vectors\npre-trained on a corpus of 56 million tweets\n(Owoputi et al., 2013). For all datasets, we\nalso assess applying CL to foundation models\n(FME+CL).\nFor each dataset, we run 25 hyperparameter\nsweeps which each fine-tune a model for the\ngiven task (e.g., POS tagging) using noisy data,\nand select the model with the best validation\nset task performance. We report label error\ndetection performance (not task performance).\nArea Under the Precsion-Recall Curve (AUPR)\nprovides an overall performance score (Saito\nand Rehmsmeier, 2015; Hendrycks and Gim-\npel, 2017). We also report metrics representing\nperformance on competing data cleaning pri-\norities: efficiency requires high precision on a\nsmall number of items, whereas coverage re-\nquires high recall on a larger number of items.\nAppendix E.1 describes the Truncated AUPR\nused for IMDB and Amazon, which are too\ncostly to fully crowd verify.\nEnd-to-end noising. We finally isolate the\neffects of noise and label error correction for\nvalidation and test splits. For each dataset, we\nprepare three versions of the validation and test\nsplits, respectively: aclean version assumed to\ncontain zero errors,4 a noisy version, with label\n3Precision and recall are equal when evaluating a\nnumber of items equal to the total error count.\n4For TweetNLP, we justify our assumption in §4:\nexpert labels by Hovy et al. (2014) are considered noise\nfree compared to crowd labels. For IMDB and Amazon,\nwe follow Northcutt et al. (2021a), which adds several\npercentage points more noise than naturally occurs.\nnoise deliberately introduced, and acorrected\nversion generated from noisy splits using our\nmain error detection method (ranking errors\nwith FME and correcting the top Err% data\npoints). We train 40 hyperparameter sweeps,\nwith performance cross-evaluated on all pre-\npared data splits.\nWe report three different metrics. We report\neach model’s accuracy on the clean test split\nas thetrue accuracy. Following the norms of\nFig. 3, we report themeasurable accuracy as\nthe accuracy of the model selected using per-\nformance on the noisy or corrected validation\nsplit on the corresponding test split. Finally,\nwe report therank of the model as the rank\nof the model’s performance on clean test data.\nThe best performing model among all sweeps\nhas rank 1, and the worst has rank 40. This\nmetric emphasizes that different validation sets\nselect different models.\nWe perform this exercise using IMDB\nand Amazon noised to 5% (I-5, A-5), and\nTweetNLP-5 and TweetNLP-M.\n7 Results\nLabel noise realism. Human-originated\nnoise appears to closely approximate real label\nnoise. Figs. 4 and 5 show that the losses of\nboth real and human-originated label errors\nare lower and more widely-distributed than\nexisting noising methods. Their Wasserstein\ndistances to the distribution of clean data are\nsignificantly lower than existing noising meth-\nods, suggesting comparable erroneous learning\n(Appendix B).\nOverall LLM performance. We discover\na strong log-linear relationship between error\n9080\n0.0 0.2 0.4 0.6\nError Detection Recall (Estimated)\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nError Detection Precision\nBaseline\nFM Loss\nFM Ensemble\nCL Overlays\nFigure 6: Precision-recall curves for label error\ndetection on Amazon by method. FML+CL and\nFME+CL produce fewer items and do not extend\nto a recall past 0.21. Applying CL to FM changes\nlittle compared to using FM alone.\ndetection performance and loss, which holds\nacross many model families and configurations\n(r2: 0.94, Fig. 2). We also find relationships\nbetween error detection performance and gen-\neral model capability, in terms of GLUE score\n(r2: 0.79) and model size (Fig. 10). Fig. 7 illus-\ntrates key findings using models’ receiver oper-\nating characteristic (ROC) curves. Ensembling\nconfers signficantly more gains in error detec-\ntion performance higher than gains on under-\nlying task performance, across a broad range\nof models and hyperparameters; Appendix E.3\nexplores ensembling in greater detail.\nMainexperiment. Table3showsthatFoun-\ndation Model Ensembling significantly im-\nproves AUPR from the CL and H&G baselines\non all datasets, with an absolute difference of\n0.36 on IMDB, 0.09 on Amazon, and a differ-\nence of 0.07–0.44 on synthetic data.\nFig. 1 shows that applying CL to FME has\nminimal effect on performance at every level of\nrecall; most numbers are identical across the\nFME and FME+CL rows of Table 3. In fact,\nCL does not necessarily improve upon the H&G\nbaseline across datasets, with CL performance\nsometimes dipping below H&G by 0.01–0.03.\nWhilelossnaturallyranksalldatapoints, CL\nonly hypothesizes a fixed number of potential\nerrors: Appendix E.2 shows the raw counts of\nitems at fixed thresholds, per the original CL\nstudy. At the CL threshold, we outperform\nCL by an absolute 15–28%. At the CL+FME\nthreshold, predicted items are almost exactly\n0.0 0.1 0.2 0.3 0.4\nFalse Positive Rate\n0.6\n0.7\n0.8\n0.9\n1.0\nTrue Positive Rate\n77%: bert-small\n84%: bert-base-uncased\n86%: bert-large-uncased\n87%: roberta-base\n88%: deberta-v3-small\n90%: roberta-large\n90%: deberta-v3-base\n91%: deberta-v3-large\n91%: deberta-v2-xlarge\nFigure 7: ROC curves for error detection perfor-\nmance on TweetNLP-5: LLM loss is highly effec-\ntive for detecting label errors, and performance is\nhighly correlated with general language understand-\ning (GLUE,r2: 0.79).\nthe same, with Jaccard similarities of 0.59–0.99.\nBy contrast, ensembling improves performance\nover FML by a greater amount on almost every\nmeasure, and introduces no such constraint.\nEnd-to-end noising. Cleaning validation\ndata selects better models. Noise in validation\nsplits reduces performance by encouraging the\nselectionofmodelswithlowertrueperformance.\nNoise in test splits significantly reduces mea-\nsureable (noisy test) performance, as expressed\nby the difference between measureable and true\nperformance. In general, correcting label errors\nimproves task performance: even when the re-\nported task performance worsens, the reported\nperformance is closer to the true performance\nof the model, measured using clean training\nand validation data.\n8 Discussion\nRapid data “health check”.Sorting evalu-\nation data by each item’s loss is an easy way to\nquickly highlight label errors. Using this sim-\nple technique with a foundation model appears\nto generally identify over half of all label errors\nthrough human re-evaluation of a single-digit\npercentage of all data (Table 3). We expect\nthis technique to work across deep learning\ndomains, due to its simplicity and the exten-\nsive use of training loss in LNL research (Song\net al., 2022). Given estimates for typical rates\nof label errors and the gain observed in the end-\nto-end experiment, our technique may enable\na 1–2% increase in reportable test accuracy\n9081\nEval. Test Perf. I-5 A-5 T-5 T-M\nNoisy\nMeasurable 90.1 88.3 89.3 89.3\nTrue 94.2 91.0 92.8 82.0\nRank 10 1 3 10\nCorr.\nMeasurable 95.1 90.7 92.9 88.5\nTrue 95.1 90.8 93.0 82.0\nRank 4 5 2 8\nClean True 95.8 91.0 93.8 82.1\nTable 4: End-to-end effects of label noise on task\nperformance, as evaluated on noisy, corrected, and\nclean validation and test data splits. True accu-\nracy is measured on clean test sets, and measurable\naccuracy on noisy or corrected test sets. Rank is\na relative measure of true accuracy; lower numeri-\ncal ranks have higher accuracy. Corrections which\nimprove or reduce performance metrics are high-\nlighted in green or red, respectively. Metrics are\nevaluated on models trained on noisy data.\nacross many datasets, in addition to the gains\nfrom improving model selection.\nPre-training and robustness. We demon-\nstrate that despite established findings on ar-\ntificial noising (Hendrycks et al., 2018), pre-\ntraining confers limited robustness to realistic\nhuman noise. The majority of label errors are\nsystematic in nature (Snow et al., 2008; Plank\net al., 2014; Samuel et al., 2022), and crowd-\nsourced labels form, to an extent, a different\ndistribution from reality, as approximated by\nexpert labels (Hendrycks et al., 2020). When\ntrained on crowdsourced or other data con-\ntaining systematic errors, FMs quickly drift\ntowards this incorrect distribution.\nApplying AI to data-centric AI. Data-\ncentric AI aims to improve AI through labeling,\ncurating, and augmenting the underlying data.\nWe find that AI itself can be applied towards\nimproving data quality, as part of a human-in-\nthe-loop (HITL) iteration, which contributes\nan additional positive feedback loop between\ndata quality and AI performance.\nNew challenges in LNL. Standard noising\nmethods are unrealistic and no longer challeng-\ning for state-of-the-art language models (Algan\nand Ulusoy, 2020); recent LNL analyses study\nconditions where up to 80% of labels are noised\n(Song et al., 2022). Our findings reinforce the\nneed to reassess LNL methods in the context\nof more realistic noise (Zhu et al., 2022).\nOur human-originated noising method pro-\nduces realistic label errors, and can be applied\nto any crowdsourced dataset which includes\nraw annotation data. As such datasets emerge\nacross deep learning domains (Wei et al., 2022),\nwe hope this method may inspire challeng-\ning and realistic new LNL performance bench-\nmarks. Our method also enables detailed explo-\nration of the properties of human noise, which\nmay support work on open LNL problems such\nas improving feature-based noising techniques,\nand estimating dataset noise (Bäuerle et al.,\n2022; Northcutt et al., 2021b).\nEnd-to-end noising. The study of model\nperformance on noise in validation and test\ndata is essential: noise in other splits can affect\nreported model performance as much as noise\nin training data. Clean and noisy performance\non evaluation data provide useful insight into\nmodels’ overall performance.\n9 Conclusions and Future Work\nPre-trained models effectively identify label er-\nrors on real NLP datasets, definitively outper-\nforming existing methods on the same bench-\nmarks by an absolute 9–36% in AUPR.\nHuman-originated noising techniques may\npresent a solution to the clear limitations of\ncurrent LNL noising schemes: they are highly\nrealistic and yet controllable for experimen-\ntal purposes. We invite further exploration of\nthis family of label noising techniques. We be-\nlieve human-originated noising enables future\nadvancements across multiple areas of LNL,\nsupporting new tasks and metrics in areas such\nas the cost of human reannotation, estimation\nof dataset error, and mitigation of bias.\nFinally, we advocate for LNL to move to-\nwards an end-to-end approach ofevaluating\nwith label noise, which takes into account noise\nwithin validation and test splits, and more accu-\nrately models the conditions of data in practice.\nLimitations\nPartial metrics. Determining the true re-\ncall of a label error detection method on a\nreal datasets is generally infeasible due to its\nhighcost; thisrequiresacompletere-evaluation\nso as to identify every label error within the\ndataset. While some datasets exist in which\nthis has been undertaken, such as Hovy et al.\n9082\n(2014) for TweetNLP, for most datasets con-\ntaining organic label errors, we can only assess\nprecision directly.\nTo mitigate this, we can estimate recall by\nestimating total dataset error counts using sam-\npling techniques. As a result of this limitation,\nwe prefer AUPR over AUROC (Area Under the\nReceiving Operating Curve) as our overall as-\nsessment metric: estimates of AUPR are scaled\nby a fixed ratio, and therefore comparable be-\ntween models on the same dataset, whereas\nAUROC is nonlinear with respect to the esti-\nmate.\nRequires multiple annotations per label.\nHuman-originated noising methods are only\napplicable to datasets which include at least\ntwo human annotations per label. While it\nis becoming increasingly common to release\nindividual-level annotator data, this is not an\nubiquitous practice.\nCleaning benchmark data. In our analy-\nsis of model performance gains derived from\napplying our methods to cleaning evaluation\ndata, we find that cleaning validation splits\nenables the selection of models with better test\nperformance. Such a method may be useful in\na large number of applications.\nHowever, we caution against using this\nmethod to clean data intended for use in com-\nparing performance across model families and\nvariants: the cleaning process may bias any\nsuch benchmarks toward the models most simi-\nlar to the model used to clean the data. While\nour method improves the performance of a\ngiven model on a task, and correcting label\nerrors always improves the validity of test data,\nthese improvements is unlikely to improve the\nperformance of all models by the same amount.\nThis limitation is shared with other ex-\nisting model-based scoring methods such as\nBERTScore (Zhang* et al., 2020).\nAcknowledgements\nWe would like to thank Google.org for credits\nfor use of the Google Cloud Platform. We are\nalso grateful to our anonymous reviewers, mem-\nbers of the Stanford NLP Group, and Bryan\nH. Chong for their constructive feedback, as\nwell as the many researchers who made data\npublicly available to enable our present work.\nReferences\nZiawasch Abedjan, Xu Chu, Dong Deng, Raul Cas-\ntro Fernandez, Ihab F Ilyas, Mourad Ouzzani,\nPaolo Papotti, Michael Stonebraker, and Nan\nTang. 2016. Detecting data errors: Where are\nwe and what needs to be done?Proceedings of\nthe VLDB Endowment, 9(12):993–1004.\nJon Agley, Yunyu Xiao, Rachael Nolan, and Lil-\nian Golzarri-Arroyo. 2021. Quality control ques-\ntions on Amazon’s Mechanical Turk (MTurk):\nA randomized trial of impact on the USAUDIT,\nPHQ-9, and GAD-7.Behavior research methods,\npages 1–13.\nGörkem Algan and Ilkay Ulusoy. 2020. Label\nnoise types and their effects on deep learning.\narXiv:2003.10471.\nEhsan Amid, Manfred KK Warmuth, Rohan Anil,\nand Tomer Koren. 2019. Robust bi-tempered\nlogistic loss based on Bregman divergences.Ad-\nvances in Neural Information Processing Systems,\n32.\nSamaneh Azadi, Jiashi Feng, Stefanie Jegelka, and\nTrevor Darrell. 2015. Auxiliary image regular-\nization for deep CNNs with noisy labels.arXiv\npreprint arXiv:1511.07069.\nNoga Bar, Tomer Koren, and Raja Giryes.\n2021. Multiplicative reweighting for robust\nneural network optimization. arXiv preprint\narXiv:2102.12192.\nAlex Bäuerle, Ángel Alexander Cabrera, Fred\nHohman, Megan Maher, David Koski, Xavier\nSuau, Titus Barik, and Dominik Moritz. 2022.\nSymphony: Composing interactive interfaces for\nmachine learning. InCHI Conference on Human\nFactors in Computing Systems, pages 1–14.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine\nBosselut, Emma Brunskill, et al. 2021. On the\nopportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258.\nSamuel R. Bowman, Gabor Angeli, Christopher\nPotts, and Christopher D. Manning. 2015. A\nlarge annotated corpus for learning natural lan-\nguage inference. InProceedings of the 2015 Con-\nference on Empirical Methods in Natural Lan-\nguage Processing, pages 632–642, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nJiefeng Chen, Frederick Liu, Besim Avci, Xi Wu,\nYingyu Liang, and Somesh Jha. 2021a. Detecting\nerrors and estimating accuracy on unlabeled data\nwith self-training ensembles.Advances in Neural\nInformation Processing Systems, 34.\nPengfei Chen, Junjie Ye, Guangyong Chen, Jingwei\nZhao, and Pheng-Ann Heng. 2021b. Beyond\n9083\nclass-conditional assumption: A primary attempt\nto combat instance-dependent label noise. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 11442–11450.\nGlenn Dawson and Robi Polikar. 2021. Rethink-\ning noisy label models: Labeler-dependent noise\nwith adversarial awareness. arXiv preprint\narXiv:2105.14083.\nDCAI Workshop. 2021. NeurIPS data-centric AI\nworkshop. NeurIPS 2021 Data-Centric AI Work-\nshop.\nSarah Jane Delany, Nicola Segata, and Brian\nMac Namee. 2012. Profiling instances in noise\nreduction. Knowledge-Based Systems, 31:28–40.\nSean A Dennis, Brian M Goodson, and Christo-\npher A Pearson. 2020. Online worker fraud and\nevolving threats to the integrity of MTurk data:\nA discussion of virtual private servers and the\nlimitations of IP-based screening procedures.Be-\nhavioral Research in Accounting, 32(1):119–134.\nShrey Desai and Greg Durrett. 2020. Calibration\nof pre-trained transformers. In Proceedings of\nthe 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages\n295–302, Online. Association for Computational\nLinguistics.\nBenoit Frenay and Michel Verleysen. 2014. Clas-\nsification in the presence of label noise: A sur-\nvey. IEEE Transactions on Neural Networks and\nLearning Systems, 25(5):845–869.\nDragan Gamberger, Nada Lavrac, and Saso Dze-\nroski. 2000. Noise detection and elimination in\ndata preprocessing: experiments in medical do-\nmains. Applied artificial intelligence, 14(2):205–\n223.\nKevin Gimpel, Nathan Schneider, Brendan\nO’Connor, Dipanjan Das, Daniel Mills, Jacob\nEisenstein, Michael Heilman, Dani Yogatama,\nJeffrey Flanigan, and Noah A Smith. 2010.\nPart-of-speech tagging for Twitter: Annotation,\nfeatures, and experiments. Technical report,\nCarnegie-Mellon Univ Pittsburgh Pa School of\nComputer Science.\nSuchin Gururangan, Ana Marasović, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretrain-\ning: Adapt language models to domains and\ntasks. InProceedings of the 58th Annual Meeting\nof the Association for Computational Linguis-\ntics, pages 8342–8360, Online. Association for\nComputational Linguistics.\nPengcheng He, Jianfeng Gao, and Weizhu\nChen. 2021. DeBERTaV3: Improving\nDeBERTa using Electra-style pre-training\nwith gradient-disentangled embedding sharing.\narXiv:2111.09543.\nDanHendrycksandKevinGimpel.2017. Abaseline\nfor detecting misclassified and out-of-distribution\nexamples in neural networks.Proceedings of In-\nternational Conference on Learning Representa-\ntions.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song.\n2020. Pretrained transformers improve out-of-\ndistribution robustness. In Proceedings of the\n58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 2744–2751, Online.\nAssociation for Computational Linguistics.\nDan Hendrycks, Mantas Mazeika, Duncan Wilson,\nand Kevin Gimpel. 2018. Using trusted data to\ntrain deep networks on labels corrupted by severe\nnoise. Advances in neural information processing\nsystems, 31.\nJenny Hong, Derek Chong, and Christopher Man-\nning. 2021. Learning from limited labels for long\nlegal dialogue. InProceedings of the Natural Le-\ngal Language Processing Workshop 2021, pages\n190–204, Punta Cana, Dominican Republic. As-\nsociation for Computational Linguistics.\nDirk Hovy, Barbara Plank, and Anders Søgaard.\n2014. Experiments with crowdsourced re-\nannotation of a POS tagging data set. InPro-\nceedings of the 52nd Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 2:\nShort Papers), pages 377–382, Baltimore, Mary-\nland. Association for Computational Linguistics.\nJinchi Huang, Lie Qu, Rongfei Jia, and Binqiang\nZhao. 2019. O2u-net: A simple noisy label de-\ntection approach for deep neural networks. In\nProceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 3326–3334.\nLu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia\nLi, and Li Fei-Fei. 2018. Mentornet: Learning\ndata-driven curriculum for very deep neural net-\nworks on corrupted labels. InInternational Con-\nference on Machine Learning, pages 2304–2313.\nPMLR.\nYiding Jiang, Vaishnavh Nagarajan, Christina\nBaek, and J Zico Kolter. 2022. Assessing gener-\nalization of SGD via disagreement. InInterna-\ntional Conference on Machine Learning.\nArmand Joulin, Edouard Grave, Piotr Bojanowski,\nand Tomas Mikolov. 2017. Bag of tricks for ef-\nficient text classification. InProceedings of the\n15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Vol-\nume 2, Short Papers, pages 427–431. Association\nfor Computational Linguistics.\nArmand Joulin, Laurens van der Maaten, Allan\nJabri, and Nicolas Vasilache. 2016. Learning\nvisual features from large weakly supervised data.\nIn European Conference on Computer Vision,\npages 67–84. Springer.\n9084\nShyamgopal Karthik, Jérome Revaud, and Boris\nChidlovskii. 2021. Learning from long-tailed data\nwith noisy labels.arXiv:2108.11096.\nRyan Kennedy, Scott Clifford, Tyler Burleigh,\nPhilip D Waggoner, Ryan Jewell, and\nNicholas JG Winter. 2020. The shape of\nand solutions to the MTurk quality crisis.\nPolitical Science Research and Methods ,\n8(4):614–629.\nTaehyeon Kim, Jongwoo Ko, JinHwan Choi, Se-\nYoung Yun, et al. 2021. FINE samples for learn-\ning with noisy labels.Advances in Neural Infor-\nmation Processing Systems, 34.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ah-\nsan Wahab, Daan van Esch, Nasanbayar Ulzii-\nOrshikh, Allahsera Tapo, Nishant Subramani,\nArtem Sokolov, Claytone Sikasote, et al. 2022.\nQuality at a glance: An audit of web-crawled\nmultilingual datasets. Transactions of the Asso-\nciation for Computational Linguistics, 10:50–72.\nAbhishek Kumar and Ehsan Amid. 2021. Con-\nstrained instance and class reweighting for robust\nlearning under label noise.arXiv.\nYang Liu and Hongyi Guo. 2020. Peer loss func-\ntions: Learning from noisy labels without know-\ning noise rates. InInternational Conference on\nMachine Learning, pages 6226–6236. PMLR.\nXingjun Ma, Hanxun Huang, Yisen Wang, Simone\nRomano, Sarah Erfani, and James Bailey. 2020.\nNormalized loss functions for deep learning with\nnoisy labels. In International Conference on\nMachine Learning, pages 6543–6553. PMLR.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y. Ng, and Christopher\nPotts. 2011. Learning word vectors for sentiment\nanalysis. InProceedings of the 49th Annual Meet-\ning of the Association for Computational Lin-\nguistics: Human Language Technologies, pages\n142–150, Portland, Oregon, USA. Association for\nComputational Linguistics.\nHassan H Malik and Vikas S Bhardwaj. 2011. Au-\ntomatic training data cleaning for text classifica-\ntion. In2011 IEEE 11th international conference\non data mining workshops, pages 442–449. IEEE.\nJulian McAuley, Christopher Targett, Qinfeng Shi,\nand Anton van den Hengel. 2015. Image-based\nrecommendations on styles and substitutes. In\nSIGIR 2015, page 43–52, New York, NY, USA.\nAssociation for Computing Machinery.\nAlexandra M Mellis and Warren K Bickel. 2020.\nMechanical Turk data collection in addiction\nresearch: Utility, concerns and best practices.\nAddiction, 115(10):1960–1968.\nAaron Moss and Leib Litman. 2018. After the bot\nscare: Understanding what’s been happening\nwith data collection on MTurk and how to stop\nit. CloudResearch.\nNicolas M Müller and Karla Markert. 2019. Iden-\ntifying mislabeled instances in classification\ndatasets. In 2019 International Joint Confer-\nence on Neural Networks (IJCNN), pages 1–8.\nIEEE.\nCurtis Northcutt, Lu Jiang, and Isaac Chuang.\n2021a. Confident learning: Estimating uncer-\ntainty in dataset labels. Journal of Artificial\nIntelligence Research, 70:1373–1411.\nCurtis G Northcutt, Anish Athalye, and Jonas\nMueller. 2021b. Pervasive label errors in test\nsets destabilize machine learning benchmarks.\narXiv:2103.14749.\nOlutobi Owoputi, Brendan O’Connor, Chris Dyer,\nKevin Gimpel, Nathan Schneider, and Noah A.\nSmith. 2013. Improved part-of-speech tagging\nfor online conversational text with word clus-\nters. In Proceedings of the 2013 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 380–390, Atlanta, Georgia.\nAssociation for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as\nknowledge bases? In Proceedings of the 2019\nConference on Empirical Methods in Natural\nLanguage Processing and the 9th International\nJoint Conference on Natural Language Process-\ning (EMNLP-IJCNLP), pages 2463–2473, Hong\nKong, China. Association for Computational Lin-\nguistics.\nBarbara Plank, Dirk Hovy, and Anders Søgaard.\n2014. Linguistically debatable or just plain\nwrong? In Proceedings of the 52nd Annual\nMeeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 507–\n511.\nThomas C Redman. 1998. The impact of poor data\nquality on the typical enterprise.Communica-\ntions of the ACM, 41(2):79–82.\nScott Reed, Honglak Lee, Dragomir Anguelov,\nChristian Szegedy, Dumitru Erhan, and Andrew\nRabinovich. 2014. Training deep neural net-\nworks on noisy labels with bootstrapping.arXiv\npreprint arXiv:1412.6596.\nFrederick Reiss, Hong Xu, Bryan Cutler, Karthik\nMuthuraman, and Zachary Eichenberger. 2020.\nIdentifying incorrect labels in the CoNLL-2003\ncorpus. In Proceedings of the 24th conference on\ncomputational natural language learning, pages\n215–226.\n9085\nDavid Rolnick, Andreas Veit, Serge Belongie, and\nNir Shavit. 2017. Deep learning is robust to\nmassive label noise.arXiv:1705.10694.\nTakaya Saito and Marc Rehmsmeier. 2015. The\nprecision-recall plot is more informative than the\nROC plot when evaluating binary classifiers on\nimbalanced datasets.PloS one, 10(3):e0118432.\nJim Samuel, Gavin Rozzi, and Ratnakar Palle.\n2022. The dark side of sentiment analysis: An ex-\nploratory review using lexicons, dictionaries, and\na statistical monkey and chimp.Dictionaries,\nand a Statistical Monkey and Chimp.(January 6,\n2022).\nAntonios Saravanos, Stavros Zervoudakis, Dong-\nnanzi Zheng, Neil Stott, Bohdan Hawryluk, and\nDonatella Delfino. 2021. The hidden cost of us-\ning Amazon Mechanical Turk for research. In\nInternational Conference on Human-Computer\nInteraction, pages 147–164. Springer.\nBorut Sluban, Dragan Gamberger, and Nada\nLavrač. 2014. Ensemble-based noise detection:\nnoise ranking and visual performance evaluation.\nData mining and knowledge discovery, 28(2):265–\n303.\nRion Snow, Brendan O’Connor, Daniel Jurafsky,\nand Andrew Ng. 2008. Cheap and fast – but\nis it good? evaluating non-expert annotations\nfor natural language tasks. InProceedings of the\n2008 Conference on Empirical Methods in Nat-\nural Language Processing, pages 254–263, Hon-\nolulu, Hawaii. Association for Computational\nLinguistics.\nHwanjun Song, Minseok Kim, Dongmin Park,\nYooju Shin, and Jae-Gil Lee. 2022. Learning\nfrom noisy labels with deep neural networks: A\nsurvey. IEEE Transactions on Neural Networks\nand Learning Systems.\nJaree Thongkam, Guandong Xu, Yanchun Zhang,\nand Fuchun Huang. 2008. Support vector ma-\nchine for outlier detection in breast cancer sur-\nvivability prediction. InAsia-Pacific Web Con-\nference, pages 99–109. Springer.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis\nplatform for natural language understanding. In\nProceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMatthew Watson, Bashar Awwad Shiekh Hasan,\nand Noura Al Moubayed. 2022. Agree to dis-\nagree: When deep learning models with identical\narchitectures produce distinct explanations. In\nProceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pages\n875–884.\nJiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang\nLiu, Gang Niu, and Yang Liu. 2022. Learn-\ning with noisy labels revisited: A study using\nreal-world human annotations. InInternational\nConference on Learning Representations.\nVirginia Wheway. 2000. Using boosting to detect\nnoisy data. In Pacific Rim International Con-\nference on Artificial Intelligence, pages 123–130.\nSpringer.\nLiang Xu, Jiacheng Liu, Xiang Pan, Xiao-\njing Lu, and Xiaofeng Hou. 2021. Data-\nCLUE: A benchmark suite for data-centric NLP.\narXiv:2111.08647.\nJing Zhang, Victor S Sheng, Qianmu Li, Jian Wu,\nand Xindong Wu. 2017. Consensus algorithms\nforbiasedlabelingincrowdsourcing. Information\nSciences, 382:254–273.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kil-\nian Q. Weinberger, and Yoav Artzi. 2020.\nBERTScore: Evaluating text generation with\nBERT. InInternational Conference on Learning\nRepresentations.\nWenzheng Zhang and Karl Stratos. 2021. Under-\nstanding hard negatives in noise contrastive esti-\nmation. InProceedings of the 2021 Conference of\nthe North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 1090–1101, Online. Associa-\ntion for Computational Linguistics.\nWenxuan Zhou and Muhao Chen. 2021. Learning\nfrom noisy labels for entity-centric information\nextraction. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 5381–5392, Online and Punta\nCana, Dominican Republic. Association for Com-\nputational Linguistics.\nDawei Zhu, Michael A Hedderich, Fangzhou Zhai,\nDavid Ifeoluwa Adelani, and Dietrich Klakow.\n2022. Is BERT robust to label noise? A study\non learning with noisy labels in text classification.\narXiv:2204.09371.\nA Noising Benchmarks\nThis section specifies how noising protocols\nwere applied to create each fixed crowdsourced\ndataset. Crowdlabelsforeachdatasetareavail-\nable to download from the respective GitHub\nprojects.\nA.1 TweetNLP-5\nTweetNLP-5(%) is a fixed noising of TweetNLP\nto a 5% noise level in each split. Of the label\nerrors, 80% (i.e. 4% of each split) are assigned\nusing thedissenting worker method. The re-\nmaining 20% (i.e. 1% of each split) are assigned\n9086\nusing thedissenting labelmethod. Fig. 4 shows\nthat both methods provide similar distribu-\ntions of label errors. Although the dissenting\nworker method more realistically captures indi-\nvidual worker idiosyncrasies, the dissenting la-\nbel method is actually slightly lower loss during\ntraining (i.e. harder for a model to distinguish\nfrom correct labels).\nA.2 TweetNLP-M\nTweetNLP-M(ajority) directly uses the major-\nity class labels collected by Hovy et al. (2014)\non the Crowdflower platform, which have a\n79.54% agreement with the high-quality expert\ngold labels collected by Gimpel et al. (2010).\nPer the Hovy et al. (2014) protocol, in the rare\ncase of ties, the tie is broken in favor of the\nlabel that matches the gold label, if applicable.\nOtherwise, a label is selected at random. The\n“-M” suffix distinguishes the Hovy et al. (2014)\nlabels from the gold labels.\nA.3 SNLI-5\nThe Stanford Natural Language Inference\ndataset (SNLI) annotations do not include a\nworkeridentifier, meaningeachitemisattached\nto five crowdsourced labels, but there is no indi-\ncationofwhichlabelscamefromthesameanno-\ntator across the dataset. As a result, we cannot\napply the dissenting worker noising method.\nSNLI-5 has exactly 5% of its data noised\nin each split. Of the label errors, 80% (i.e.\n4% of each split) are assigned using a method\nthat represents systematic errors, to simulate of\ndissenting worker method: We use the minority\nlabel when there is a 3-2 split between the five\nlabels. The remaining 20% (i.e. 1% of each\nsplit) are assigned using the dissenting label\nmethod, as in TweetNLP-5.\nB Loss Distributions\nSection 4 examines dataset noisings primarily\nin terms of loss distributions on noised labels.\nTo provide additional context, Fig. 8 provides\nan equivalent view for SNLI, and Fig. 9 shows\ncombined distributions of both clean and noisy\ndata points on TweetNLP.\nTable 5 reports the Wasserstein distances (or\nearth mover’s distances) measured between the\nloss distributions of noisy and clean data points\nfor models trained on TweetNLP and IMDB, as\n0 1 2 3 4 5 6\nLosses of Noised Test Items\n0.0\n0.5\n1.0\n1.5\nDensity\nUniform Noise\nClass-Based Noise\nMalicious Worker\nDissenting Label\nCrowd Majority\nClean Data Points\nFigure 8: Distributions of losses of label errors on\nSNLI at 5% noising, which demonstrates similar\nperformance characteristics to TweetNLP, as shown\nin Fig. 4.\n0 2 4 6 8\nLosses of All Test Items\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\nDensity\nUniform Noise\nClass-Based Noise\nDissenting Label\nDissenting Worker\nCrowd Majority\nFigure 9: Combined distributions of losses of both\nnoisy and clean data points, for TweetNLP with\n5% noising.\ndescribed in Section 7. Human-originated label\nnoise more closely resembles both clean data\npoints and real label noise as its hypothesized\nrealism increases.\nC Mechanical Turk Protocol\nC.1 Change Specifications\nWe use Amazon Mechanical Turk to validate\nreal label errors from IMDB (Maas et al., 2011)\nand Amazon Reviews (McAuley et al., 2015).\nWe begin with the Northcutt et al. (2021a)\nprotocol, and add four additional conditions,\nso as to mitigate annotator fraud.\nFirst, we pre-qualify workers by requiring\nthem to correctly answer a qualification test of\nfour unambiguous questions (Hovy et al., 2014;\nAgley et al., 2021).\nSecond, after the initial qualification, we con-\n9087\nNoising Method TweetNLP IMDB\nUniform Noise 5.62 5.04\nClass-Based Noise 5.23 4.91\nDissenting Label 4.02 -\nDissenting Worker 3.44 -\nCrowd Majority 2.33 -\nReal Label Errors - 2.67\nTable 5: Wasserstein distances between loss distri-\nbutions of noisy and clean data points: Human-\noriginated noising exhibits comparable levels of er-\nroneous learning to organic label errors.\nIMDB Amazon\nOriginal Protocol 0.1314 0.0141\nNew Protocol 0.4643 0.5561\nTable 6: A comparison of inter-annotator agree-\nment between the original and new MTurk protocol\nresults using Fleiss’κ. A score of 1.0 represents\nperfect agreement between workers, and 0.0 repre-\nsents guessing at random. Annotations from the\noriginal protocol are substantially closer to random\nchance.\ntinue to monitor worker quality by introducing\nsentinel questions with known answers into the\nworkers’ regular tasks. We periodically remove\nworkers who fail the tasks.\nThird, we set filter criteria to limit workers\nto the following Anglosphere countries: United\nStates, Canada, United Kingdom, Ireland, Aus-\ntralia, and New Zealand (Moss and Litman,\n2018), to improve the chances of finding anno-\ntators with sufficient cultural context to cor-\nrectly interpret review text.5 Our filter criteria\ninclude the standard recommendations of re-\nquiring a ≥99% positive task approval rate\nwith ≥500 tasks approved.\nFinally, wesetabaselinetargetrateofUS$10\nper hour, calculated using word counts and av-\nerage reading speed (primarily for ethical rea-\nsons; the effect of compensation and annotation\nquality is an area of active research; Saravanos\net al., 2021).\nThe new protocol’s labels are produced using\na final set of approximately 70 workers. Work-\ners averaged at least 12 seconds on each task;\n5Despite these precautions, we recognize that every\nprecaution is subject to fraud, e.g., location is subject\nto VPN and bot attacks. (Dennis et al., 2020; Mellis\nand Bickel, 2020; Kennedy et al., 2020)\nhalf the time needed to read prompts at an\naverage reading speed. The average time spent\nby a worker in the Northcutt et al. (2021a)\nprotocol was 5 seconds.6\nC.2 Protocol Validation\nWe hypothesize that the Non-Agreements in\nthe original protocol represent not only am-\nbiguous data points, but also noise in the orig-\ninal protocol resulting from low quality work.\nTables 2 and 6 show that the new protocol\nimproves the level of agreement between work-\ners. As such, we confirm that the increased\nagreement between workers in the new protocol\nresults from higher quality labels.\nFollowing the Northcutt et al. (2021a) proto-\ncolforexpertreview, weadditionallyselectato-\ntal of 50 items from each of IMDB and Amazon\nfor expert review. The experts are blinded to\nboth the original labels and MTurk results and\nasked to label each item from scratch. They\nthen reconciled results and came to a consen-\nsus for each item. The results are compared\nat the aggregate level of “Correctable,” “Non-\nAgreement,” and “Non-Error,” as opposed to\nthe individual sentiment level (Positive, Nega-\ntive, Neutral, or Off-Topic). The expert agree-\nment with one another was 79%, so in 21% of\nthe items, the expert label was considered to\nbe Non-Agreement and matched the MTurk\nworkers only if the workers also produced Non-\nAgreement. Table 7 provides the result of this\nassessment.\nFor the original protocol, 52% of the\nitems agreed with expert annotators, 31% of\nthe items were incorrectly labeled as Non-\nAgreement, 12% of the items were incorrectly\nlabeled as Correctables, and 5% of the items\nwere incorrectly labeled as Non-Errors. 8% of\nitems were disagreements between experts and\ncrowd workers where neither side had a Non-\nAgreement. In other words, 8% of all items\nwere disagreements between Correctable and\nNon-Error.\nFor the new protocol, 72% of the items\nagreed with expert annotators, 4% of the items\nwere incorrectly labeled as Non-Agreement, 7%\nof the items were incorrectly labeled as Cor-\nrectable, and 17% were incorrectly labeled as\n6The reported time is anupper bound on the aver-\nage time a worker spends on a task.\n9088\nIMDB Amazon Total\nOriginal Correct 33 19 52\nNew Correct 41 31 72\nBoth Correct 28 14 42\nTable 7: A comparison of original and new MTurk\nprotocol results against 100 expert-labeled data\npoints.\nNon-Errors. 5% of items were disagreements\nbetween experts and crowd workers where nei-\nther side had a Non-Agreement.\nD Overall LLM Performance\nExperiments\nDue to the high costs associated with ex-\npert and crowdsourced validation, we use\nTweetNLP-5 as a development dataset for\nmodel selection.\nWe selected the following models for ex-\nploration: XLNet (base, large), RoBERTa\n(base, large), BERT (small, base, large),\nDeBERTa (V3:xsmall, small, base, large,\nand V2: xlarge, xxlarge), GPT (assorted).\nWe performed 25 hyperparameter sweeps with\neach model, selecting the top three runs for fur-\nther analysis. In order to avoid model family-\nlevel bias in the choice of hyperparameters, we\nset a broad shared range for three hyperparam-\neters: learning rate varying from10−6 to 10−3,\nthe number of epochs from 2 to 8, and the\nbatch size between 8, 16, 64, and 128. Train-\ning time and the final hyperparameters varied\nbased on the model.\nWe ultimately selectedDeBERTA-v3-base as\na compromise between performance and train-\ning speed. We used Google Cloud Platform\nfor training infrastructure. Experiments were\nrun using NVIDIA A100 GPUs, and runtime\nper training run was approximately 20 minutes\nfor IMDB, Recon, and SNLI, 3 minutes for\nTweetNLP, and 4 hours for Amazon, when con-\nfigured with a 2.5 million data point training\nsplit.\nE Main Experiment\nE.1 Metrics\nWe calculate the Area Under the Precsion-\nRecall Curve (AUPR) using the trapezoidal\nrule, given individual measurements of preci-\nsion and recall at every possible threshold.\n108 109\nModel Size (# Parameters)\n0.5\n0.6\n0.7\nError Detection F1\nBERT\nDeBERTa\nGPT\nRoBERTa\nFigure 10: Label noise detection performance by\nmodel size and family, evaluated on TweetNLP-5.\nGPT-based models exhibit similar scaling trends,\ndespite intrinsic disadvantages on classification\ntasks (due to pure autoregressive pre-training).\nWe report the Truncated AUPR on IMDB\nand Amazon. Because IMDB and Amazon are\ntoo expensive to fully crowd verify, we cannot\ncalculate precision and recall at the 25,000th\nitem for each method, for each dataset, as it\nwould require every data point to be relabeled\non MTurk. Instead, we use the CL framework\nof predicting a fixed number of items. For ex-\nample, for IMDB, CL hypothesizes 1,310 out\nof the 25,000 items to be label errors. We\ncan calculate the precision and recall for every\nthreshold, up to the number hypothesized by\nConfident Learning. We can calculate the pre-\ncision and recall of the 1st, 2nd, 3rd, ..., and\n1,310th items.\nWe know the exact recall for all synthetic\ndatasets. For IMDB and Amazon, we use\nthe estimate that 5% of the data is erroneous,\nwhich is consistent with common understand-\ning of the prevalence of label errors (Redman,\n1998; Müller and Markert, 2019; Northcutt\net al., 2021b; Kreutzer et al., 2022).\nAll results reported on synthetic datasets re-\nflect the average of individual scores from the\nthree top-performing models from 25 hyperpa-\nrameter sweeps. However, for cost-efficiency,\nresults which require crowdsourced evaluation\n(such as IMDB and Amazon) are based on one\nrun selected at random from a top three.\nE.2 Confident Learning\nNorthcutt et al. (2021b) reports results using\nraw counts, not the accuracy, precision, recall,\n9089\nDataset Num. Errors\nHypothesized\nCorrectable Non-Agreement Non-Error\nCL FML FME CL FML FME CL FML FME\nIMDB 1310 183 323 328 358 573 581 769 414 401\nAmazon 1000 357 508 517 148 131 143 495 361 340\nTweetNLP-M 250 121 158 165 - - - 129 92 85\nTable 8: The number of each type of error accurately identified for each dataset by each noise detection\nmethod, keeping the number of errors hypothesized fixed for ease of comparison. (TweetNLP is expert\nreviewed and by construction does not have any Non-Agreement types.)\nDataset Num. Errors\nHypothesized\nCorrectable Non-Agreement Non-Error Jaccard\nSimilarityFME FME+CL FME FME+CL FME FME+CL\nIMDB 316 168 168 108 108 40 40 0.99\nAmazon 381 226 204 65 56 90 121 0.60\nTweetNLP-M 129 93 98 - - 36 31 0.59\nTable 9: Examining the performance of overlaying Confident Learning on FME, comparing the number of\nerrors hypothesized by FME+CL. We also report the Jaccard similarity between the two models.\nor any other metric. For ease of comparability,\nTable 8 reports the number of correctable, non-\nagreement, and non-error items identified by\neach method on each dataset. CL hypothesizes\na fixed number of items, which is reported\nin the last column, and we assess a matching\nnumber of items from each method.\nWhen hypothesizing a fixed number of items,\nthe foundation model approaches far outper-\nform CL baselines. On IMDB, FME correctly\nidentifies 909 label errors, a 28% absolute im-\nprovement in accuracy. On Amazon, the FME\napproach correctly identifies 660 label errors,\ncompared to the 505 identified by CL, a 15.5%\nabsolute improvement.\nApplying CL to FME results in a different\nmodel that hypothesizes a different number\nof items (fewer, in all cases). Table 9 shows\nthe raw counts of correctable, non-agreement,\nand non-error items when each of our models\nhypothesizes items at this reduced threshold.\nOverlaying CL on foundation model loss\nappears to have little marginal utility. Ta-\nble 9 also shows a high Jaccard similarity\nacross all datasets, suggesting that applying\nCL on top of an FM changes little about the\nitems hypothesized. On many datasets, FME\nand FME+CL perform almost identically in\nthe number of items correctly hypothesized,\nslightly harming performance on Amazon Re-\nviews, andslightlyimprovingitonTweetNLP-5\n(Table 10). FME+CL decreases the total num-\nber of hypothesized items compared to FME\nbecause of the threshold set by CL. We com-\npare the FME and FME+CL approaches at the\nreduced number of hypothesized items in order\nto assess the impact of CL in the presence of\npre-training.\nNot only is aggregate performance nearly\nidentical, we see in Figs. 1 and 6 that FME\nand FME+CL perform similarly for theentire\nrangeof items hypothesized along the Precision-\nRecall curve. The primary difference is that\nFME can continue hypothesizing items even\npast FME+CL’s threshold.\nE.3 Ensembling\nResults from Tables 3 and 10 show that en-\nsembling (FME) improves error detection per-\nformance over using a single model (FML) in\nalmost every scenario tested, at a rate several\ntimes higher than gains to underlying task per-\nformance.\nWe also observe a phenomenon of dispro-\nportionately high variance in model error de-\ntection performance: Table 10 quantifies the\nstandard deviation of the former at three times\nthe standard deviation of performance on the\nunderlying task, and Fig. 2 shows this to be\nthe case even when comparing models with a\nfixed loss. This finding persisted even when\nholding all hyperparameters and data constant,\nwith only the random seed being changed.\nWe hypothesize that label noise in training\ndata induces models to learn spurious correla-\ntions, which cause models to make errors in a\n9090\nMethod Task Accuracy FM Error Detection Performance Effects of CL Overlay\nNoisy Clean Precision Recall F1 Precision Recall F1\nAveraged 0.88 ± 0.03 0 .91 ± 0.03 0 .50 ± 0.11 0 .65 ± 0.03 0 .56 ± 0.08 0 .51 ± 0.11 0 .67 ± 0.03 0 .57 ± 0.07\nEnsembled 0.89 ± 0.02 0 .92 ± 0.03 0 .56 ± 0.12 0 .62 ± 0.03 0 .58 ± 0.08 0 .58 ± 0.11 0 .65 ± 0.03 0 .61 ± 0.07\nDifference +1.14% +1 .24% +12 .52% −4.31% +4 .66% +13 .89% −2.98% +6 .03%\nTable 10: Ensembling confers gains in error detection performance disproportionate to gains in underlying\ntask performance, across a broad range of models and hyperparameters (on TweetNLP-5, results from top\nthree models per sweep, as measured at the fixed threshold set by CL).\nstructured manner (Watson et al., 2022; Jiang\net al., 2022); this results in greater levels of\nmodel disagreement, with minimal impact on\ntop-line performance. Ensembling may be dis-\nproportionately effective because it serves an\nadded function of reducing variance caused by\nthese low-quality features.\nE.4 TAPT\nWe perform Task-Assisted Pretraining (TAPT;\nGururangan et al., 2020) using the original\nhyperparameters everywhere except for the\noptimizer, in which we use AdamW instead\nof Adam for DeBERTa. We run TAPT on\nthe all splits of the corresponding data for all\ndatasets except Amazon Reviews, where be-\ncause of its size, we use TAPT on only 50,000\ndata points, or 0.5% of the full dataset. Af-\nter running TAPT, we then run 25 fine-tune\nsweeps.\n9091",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8659318089485168
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7374429702758789
    },
    {
      "name": "Noise (video)",
      "score": 0.6408072710037231
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6224140524864197
    },
    {
      "name": "Machine learning",
      "score": 0.5775314569473267
    },
    {
      "name": "Language model",
      "score": 0.5476972460746765
    },
    {
      "name": "Task (project management)",
      "score": 0.5408798456192017
    },
    {
      "name": "Recall",
      "score": 0.5104881525039673
    },
    {
      "name": "Precision and recall",
      "score": 0.4740174412727356
    },
    {
      "name": "Training set",
      "score": 0.4403872489929199
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39299458265304565
    },
    {
      "name": "Speech recognition",
      "score": 0.3604409098625183
    },
    {
      "name": "Data mining",
      "score": 0.34178435802459717
    },
    {
      "name": "Natural language processing",
      "score": 0.3231162130832672
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 8
}