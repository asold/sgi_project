{
  "title": "CogView: Mastering Text-to-Image Generation via Transformers",
  "url": "https://openalex.org/W3165647589",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2116977239",
      "name": "Ding Ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2260001493",
      "name": "Yang, Zhuoyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2343470462",
      "name": "Hong Wenyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133735529",
      "name": "Zheng, Wendi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2146497671",
      "name": "Zhou Chang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2229041292",
      "name": "Yin Da",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352364427",
      "name": "Lin, Junyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2147022182",
      "name": "Zou Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142957329",
      "name": "Shao, Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2034242580",
      "name": "Yang Hong-xia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2076873566",
      "name": "Tang Jie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2242818861",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W2965289598",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W3167432598",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2400369679",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W3145450063",
    "https://openalex.org/W2153156486",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W54257720",
    "https://openalex.org/W3137214022",
    "https://openalex.org/W2267126114",
    "https://openalex.org/W2964024144",
    "https://openalex.org/W2963981733",
    "https://openalex.org/W1710476689",
    "https://openalex.org/W2962741254",
    "https://openalex.org/W3048484056",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2964000524",
    "https://openalex.org/W3141023492",
    "https://openalex.org/W2966792645",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W2405756170",
    "https://openalex.org/W2963143316",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W2963966654",
    "https://openalex.org/W3118580076",
    "https://openalex.org/W2989851933",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W46679369"
  ],
  "abstract": "Text-to-Image generation in the general domain has long been an open problem, which requires both a powerful generative model and cross-modal understanding. We propose CogView, a 4-billion-parameter Transformer with VQ-VAE tokenizer to advance this problem. We also demonstrate the finetuning strategies for various downstream tasks, e.g. style learning, super-resolution, text-image ranking and fashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses. CogView achieves the state-of-the-art FID on the blurred MS COCO dataset, outperforming previous GAN-based models and a recent similar work DALL-E.",
  "full_text": "CogView: Mastering Text-to-Image Generation via\nTransformers\nMing Ding†, Zhuoyi Yang†, Wenyi Hong†, Wendi Zheng†, Chang Zhou‡, Da Yin†,\nJunyang Lin‡, Xu Zou†, Zhou Shao♠, Hongxia Yang‡, Jie Tang†♠\n†Tsinghua University ‡DAMO Academy, Alibaba Group ♠BAAI\n{dm18@mails, jietang@mail}.tsinghua.edu.cn\nAbstract\nText-to-Image generation in the general domain has long been an open problem,\nwhich requires both a powerful generative model and cross-modal understanding.\nWe propose CogView, a 4-billion-parameter Transformer with VQ-V AE tokenizer\nto advance this problem. We also demonstrate the ﬁnetuning strategies for various\ndownstream tasks, e.g. style learning, super-resolution, text-image ranking and\nfashion design, and methods to stabilize pretraining, e.g. eliminating NaN losses.\nCogView achieves the state-of-the-art FID on the blurred MS COCO dataset,\noutperforming previous GAN-based models and a recent similar work DALL-E. 1\nA tiger is playing football. A coﬀee cup printed with \na cat. Sky background.\nA beautiful young blond \nwoman talking on a phone.\nA Big Ben clock towering \nover the city of London.\nA man is ﬂying to the \nmoon on his bicycle\u0011\nA couple wearing leather bik‐\ner garb rides a motorcycle.\nSuper-resolution\u001d\u0003mid-lake pavilion\nChinese traditional draw‐\ning. Statue of Liberty. Oil painting. Lion. Cartoon. A tiger is playing \nfootball.Sketch. Houses.\nFigure 1: Samples generated by CogView. The text in the ﬁrst line is either from MS COCO (outside\nour training set) or user queries on our demo website. The images in the second line are ﬁnetuned\nresults for different styles or super-resolution. The actual input text is in Chinese, which is translated\ninto English here for better understanding. More samples for captions from MS COCO are included\nin Appendix F.\n1 Introduction\n“There are two things for a painter, the eye and the mind... eyes, through which\nwe view the nature; brain, in which we organize sensations by logic for meaningful\nexpression. ” (Paul Cézanne [17])\n1Codes and models are at https://github.com/THUDM/CogView. We also have a demo website of our\nlatest model at https://wudao.aminer.cn/CogView/index.html (without post-selection).\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2105.13290v3  [cs.CV]  5 Nov 2021\nAs contrastive self-supervised pretraining has revolutionized computer vision (CV) [24, 21, 8, 32],\nvisual-language pretraining, which brings high-level semantics to images, is becoming the next\nfrontier of visual understanding [38, 30, 39]. Among various pretext tasks, text-to-image generation\nexpects the model to (1) disentangle shape, color, gesture and other features from pixels, (2) under-\nstand the input text, (2) align objects and features with corresponding words and their synonyms and\n(4) learn complex distributions to generate the overlapping and composite of different objects and\nfeatures, which, like painting, is beyond basic visual functions (related to eyes and the V1–V4 in\nbrain [22]), requiring a higher-level cognitive ability (more related to the angular gyrus in brain [3]).\nThe attempts to teach machines text-to-image generation can be traced to the early times of deep gen-\nerative models, when Mansimov et al. [35] added text information to DRAW [20]. Then Generative\nAdversarial Nets [19] (GANs) began to dominate this task. Reed et al. [42] fed the text embeddings\nto both generator and discriminator as extra inputs. StackGAN [54] decomposed the generation into\na sketch-reﬁnement process. AttnGAN [51] used attention on words to focus on the corresponding\nsubregion. ObjectGAN [29] generated images following a text→boxes→layouts→image process.\nDM-GAN [55] and DF-GAN [45] introduced new architectures, e.g. dyanmic memory or deep fusion\nblock, for better image reﬁnement. Although these GAN-based models can perform reasonable\nsynthesis in simple and domain-speciﬁc dataset, e.g. Caltech-UCSD Birds 200 (CUB), the results on\ncomplex and domain-general scenes, e.g. MS COCO [31], are far from satisfactory.\nRecent years have seen a rise of the auto-regressive generative models. Generative Pre-Training (GPT)\nmodels [37, 4] leveraged Transformers [48] to learn language models in large-scale corpus, greatly\npromoting the performance of natural language generation and few-shot language understanding [33].\nAuto-regressive model is not nascent in CV . PixelCNN, PixelRNN [47] and Image Transformer [36]\nfactorized the probability density function on an image over its sub-pixels (color channels in a\npixel) with different network backbones, showing promising results. However, a real image usually\ncomprises millions of sub-pixels, indicating an unaffordable amount of computation for large models.\nEven the biggest pixel-level auto-regressive model, ImageGPT [7], was pretrained on ImageNet at a\nmax resolution of only 96 ×96.\nThe framework of Vector Quantized Variational AutoEncoders (VQ-V AE) [46] alleviates this problem.\nVQ-V AE trains an encoder to compress the image into a low-dimensional discrete latent space, and\na decoder to recover the image from the hidden variable in the stage 1. Then in the stage 2, an\nauto-regressive model (such as PixelCNN [ 47]) learns to ﬁt the prior of hidden variables. This\ndiscrete compression loses less ﬁdelity than direct downsampling, meanwhile maintains the spatial\nrelevance of pixels. Therefore, VQ-V AE revitalized the auto-regressive models in CV [41]. Following\nthis framework, Esser et al. [15] used Transformer to ﬁt the prior and further switches from L2\nloss to GAN loss for the decoder training, greatly improving the performance of domain-speciﬁc\nunconditional generation.\nThe idea of CogView comes naturally: large-scale generative joint pretraining for both text and image\n(from VQ-V AE) tokens. We collect 30 million high-quality (Chinese) text-image pairs and pretrain a\nTransformer with 4 billion parameters. However, large-scale text-to-image generative pretraining\ncould be very unstable due to the heterogeneity of data. We systematically analyze the reasons and\nsolved this problem by the proposed Precision Bottleneck Relaxation and Sandwich Layernorm. As a\nresult, CogView greatly advances the quality of text-to-image generation.\nA recent work DALL-E [39] independently proposed the same idea, and was released earlier than\nCogView. Compared with DALL-E, CogView steps forward on the following four aspects:\n• CogView outperforms DALL-E and previous GAN-based methods at a large margin ac-\ncording to the Fréchet Inception Distance (FID) [25] on blurred MS COCO, and is the ﬁrst\nopen-source large text-to-image transformer.\n• Beyond zero-shot generation, we further investigate the potential of ﬁnetuning the pretrained\nCogView. CogView can be adapted for diverse downstream tasks, such as style learn-\ning (domain-speciﬁc text-to-image), super-resolution (image-to-image), image captioning\n(image-to-text), and even text-image reranking.\n• The ﬁnetuned CogView enables self-reranking for post-selection, and gets rid of an additional\nCLIP model [38] in DALL-E. It also provides a new metric Caption Loss to measure the\nquality and accuracy for text-image generation at a ﬁner granularity than FID and Inception\nScore (IS) [43].\n2\n• We proposed PB-relaxation and Sandwich-LN to stabilize the training of large Transformers\non complex datasets. These techniques are very simple and can eliminate overﬂow in\nforwarding (characterized as NaN losses), and make CogView able to be trained withalmost\nFP16 (O22). They can also be generalized to the training of other transformers.\n2 Method\n2.1 Theory\nIn this section, we will derive the theory of CogView from V AE 3 [26]: CogView optimizes the\nEvidence Lower BOund (ELBO) of joint likelihood of image and text. The following derivation will\nturn into a clear re-interpretation of VQ-V AE if without textt.\nSuppose the dataset (X,T) ={xi,ti}N\ni=1 consists of N i.i.d. samples of image variable x and its\ndescription text variable t. We assume the image x can be generated by a random process involving\na latent variable z: (1) ti is ﬁrst generated from a prior p(t; θ). (2) zi is then generated from the\nconditional distribution p(z|t = ti; θ). (3) xi is ﬁnally generated from p(x|z = zi; ψ). We will use a\nshorthand form like p(xi) to refer to p(x = xi) in the following part.\nLet q(z|xi; φ) be the variational distribution, which is the output of the encoder φ of V AE. The\nlog-likelihood and the evidence lower bound (ELBO) can be written as:\nlog p(X,T; θ,ψ) =\nN∑\ni=1\nlog p(ti; θ) +\nN∑\ni=1\nlog p(xi|ti; θ,ψ) (1)\n≥−\nN∑\ni=1\n(\n−log p(ti; θ)  \nNLL loss for text\n+ E\nzi∼q(z|xi;φ)\n[−log p(xi|zi; ψ)]\n  \nreconstruction loss\n+ KL\n(\nq(z|xi; φ)∥p(z|ti; θ)\n)\n  \nKL between qand (text conditional) prior\n)\n. (2)\nThe framework of VQ-V AE differs with traditional V AE mainly in the KL term. Traditional V AE\nﬁxes the prior p(z|ti; θ), usually as N(0,I), and learns the encoder φ. However, it leads to posterior\ncollapse [23], meaning that q(z|xi; φ) sometimes collapses towards the prior. VQ-V AE turns to ﬁxφ\nand ﬁt the prior p(z|ti; θ) with another model parameterized by θ. This technique eliminates posterior\ncollapse, because the encoder φis now only updated for the optimization of the reconstruction loss.\nIn exchange, the approximated posterior q(z|xi; φ) could be very different for different xi, so we\nneed a very powerful model for p(z|ti; θ) to minimize the KL term.\nCurrently, the most powerful generative model, Transformer (GPT), copes with sequences of tokens\nover a discrete codebook. To use it, we make z ∈{0,..., |V|− 1}h×w, where |V|is the size of\ncodebook and h×wis the number of dimensions of z. The sequences zi can be either sampled from\nq(z|xi; φ), or directly zi = argmaxz q(z|xi; φ). We choose the latter for simplicity, so that q(z|xi; φ)\nbecomes a one-point distribution on zi. The Equation (2) can be rewritten as:\n−\nN∑\ni=1\n(\nE\nzi∼q(z|xi;φ)\n[−log p(xi|zi; ψ)]\n  \nreconstruction loss\n−log p(ti; θ)  \nNLL loss for text\n−log p(zi|ti; θ)  \nNLL loss for z\n)\n. (3)\nThe learning process is then divided into two stages: (1) The encoder φand decoder ψ learn to\nminimize the reconstruction loss. (2) A single GPT optimizes the two negative log-likelihood (NLL)\nlosses by concatenating text ti and zi as an input sequence.\nAs a result, the ﬁrst stage degenerates into a pure discrete Auto-Encoder, serving as animage tokenizer\nto transform an image to a sequence of tokens; the GPT in the second stage undertakes most of the\nmodeling task. Figure 3 illustrates the framework of CogView.\n2meaning that all computation, including forwarding and backwarding are in FP16 without any conversion,\nbut the optimizer states and the master weights are FP32.\n3In this paper, bold font denotes a random variable, and regular font denotes a concrete value. See this\ncomprehensive tutorial [12] for the basics of V AE.\n3\n2.2 Tokenization\nIn this section, we will introduce the details about the tokenizers in CogView and a comparison about\ndifferent training strategies about the image tokenizer (VQV AE stage 1).\nTokenization for text is already well-studied, e.g. BPE [16] and SentencePiece [28]. In CogView, we\nran SentencePiece on a large Chinese corpus to extract 50,000 text tokens.\nThe image tokenizer is a discrete Auto-Encoder, which is similar to the stage 1 of VQ-V AE [46] or\nd-V AE [39]. More speciﬁcally, the Encoder φmaps an image xof shape H×W ×3 into Encφ(x)\nof shape h×w×d, and then each d−dimensional vector is quantized to a nearby embedding in a\nlearnable codebook {v0,...,v |V|−1},∀vk ∈Rd. The quantized result can be represented by h×w\nindices of embeddings, and then we get the latent variable z ∈{0,..., |V|−1}h×w. The Decoder ψ\nmaps the quantized vectors back to a (blurred) image to reconstruct the input. In our 4B-parameter\nCogView,|V|= 8192,d = 256,H = W = 256,h = w= 32.\nThe training of the image tokenizer is non-trivial due to the existence of discrete selection. Here we\nintroduce four methods to train an image tokenizer.\n• The nearest-neighbor mapping, straight-through estimator [2], which is proposed by the\noriginal VQV AE. A common concern of this method [39] is that, when the codebook is\nlarge and not initialized carefully, only a few of embeddings will be used due to the curse of\ndimensionality. We did not observe this phenomenon in the experiments.\n• Gumbel sampling, straight-through estimator. If we follow the original V AE to reparam-\neterize a categorical distribution of latent variable z based on distance between vectors,\ni.e. p(zi×w+j = vk|x) = e−∥vk−Encφ(x)ij∥2/τ\n∑|V|−1\nk=0 e−∥vk−Encφ(x)ij∥2/τ, an unbiased sampling strategy is\nzi×w+j = argmaxkgk−∥vk−Encφ(x)ij∥2/τ, gk ∼Gumbel(0,1),where the temperature\nτ is gradually decreased to 0. We can further use the differentiable softmax to approximate\nthe one-hot distribution from argmax. DALL-E adopts this method with many other tricks\nto stabilize the training.\n• The nearest-neighbor mapping, moving average, where each embedding in the codebook is\nupdated periodically during training as the mean of the vectors recently mapped to it [46].\n• The nearest-neighbor mapping, ﬁxed codebook, where the codebook is ﬁxed after initialized.\nFigure 2: L2 loss curves during training\nimage tokenizers. All the above methods\nﬁnally converge to a similar loss level.\nComparison. To compare the methods, we train four\nimage tokenizers with the same architecture on the\nsame dataset and random seed, and demonstrate the\nloss curves in Figure 2. We ﬁnd that all the methods are\nbasically evenly matched, meaning that the learning of\nthe embeddings in the codebook is not very important, if\ninitialized properly. In pretraining, we use the tokenizer\nof moving average method.\nThe introduction of data and more details about tok-\nenization are in Appendix A.\n2.3 Auto-regressive Transformer\nThe backbone of CogView is a unidirectional Trans-\nformer (GPT). The Transformer has 48 layers, with the\nhidden size of 2560, 40 attention heads and 4 billion parameters in total. As shown in Figure 3, four\nseperator tokens, [ROI1] (reference text of image), [BASE], [BOI1] (beginning of image), [EOI1]\n(end of image) are added to each sequence to indicate the boundaries of text and image. All the\nsequences are clipped or padded to a length of 1088.\nThe pretext task of pretraining is left-to-right token prediction, a.k.a. language modeling. Both image\nand text tokens are equally treated. DALL-E [39] suggests to lower the loss weight of text tokens; on\nthe contrary, during small-scale experiments we surprisingly ﬁnd the text modeling is the key for the\nsuccess of text-to-image pretraining. If the loss weight of text tokens is set to zero, the model will fail\nto ﬁnd the connections between text and image and generate images totally unrelated to the input text.\n4\nEncoder\nDecoder\n\u000b7KH\u0003KHDG\u0003RI\u0003D\u0003ORYHO\\\u0003FDW\u0011\f\n̶\n Discretize Recover\nݝ\u0003ᆽ\u0003ጱ\u0003ੜ\u0003ሞ\u0003ጱ\u0003؟\u0003̶\nText Tokenizer (sentence pieces)\nImage Tokenizer\n(Discrete AutoEncoder)\n[ROI1]\n Text Token\n Text Token\n [BASE]\n [BOI1]\n [EOI1]\nImage Token\n Image Tokenŏŏ ŏŏ\nFlattern\nInput Text: Input Image:\nTransformer (GPT)\nz }| {\n<latexit sha1_base64=\"WkmkOQqV4y/G2CwEGjey+GFekFc=\">AAACAnicbVDLSgMxFM3UV62vUVfiJlgEV2VGi7osuHFZwT6gM5RMeqcNzWSGJCOUobjxV9y4UMStX+HOvzHTzkJbD4Qczrn3JvcECWdKO863VVpZXVvfKG9WtrZ3dvfs/YO2ilNJoUVjHstuQBRwJqClmebQTSSQKODQCcY3ud95AKlYLO71JAE/IkPBQkaJNlLfPvJiYweSUMi8kUry+9JJ9HTat6tOzZkBLxO3IFVUoNm3v7xBTNMIhKacKNVzzRw/I1IzymFa8VIFZv6YDKFnqCARKD+brTDFp0YZ4DCW5giNZ+rvjoxESk2iwFRGRI/UopeL/3m9VIfXfsZEkmoQdP5QmHKsY5zngQdMAtV8Ygihkpm/YjoiJg9tUquYENzFlZdJ+7zmXtScu3q1US/iKKNjdILOkIuuUAPdoiZqIYoe0TN6RW/Wk/VivVsf89KSVfQcoj+wPn8A712XuA==</latexit>\nz }| {\n<latexit sha1_base64=\"WkmkOQqV4y/G2CwEGjey+GFekFc=\">AAACAnicbVDLSgMxFM3UV62vUVfiJlgEV2VGi7osuHFZwT6gM5RMeqcNzWSGJCOUobjxV9y4UMStX+HOvzHTzkJbD4Qczrn3JvcECWdKO863VVpZXVvfKG9WtrZ3dvfs/YO2ilNJoUVjHstuQBRwJqClmebQTSSQKODQCcY3ud95AKlYLO71JAE/IkPBQkaJNlLfPvJiYweSUMi8kUry+9JJ9HTat6tOzZkBLxO3IFVUoNm3v7xBTNMIhKacKNVzzRw/I1IzymFa8VIFZv6YDKFnqCARKD+brTDFp0YZ4DCW5giNZ+rvjoxESk2iwFRGRI/UopeL/3m9VIfXfsZEkmoQdP5QmHKsY5zngQdMAtV8Ygihkpm/YjoiJg9tUquYENzFlZdJ+7zmXtScu3q1US/iKKNjdILOkIuuUAPdoiZqIYoe0TN6RW/Wk/VivVsf89KSVfQcoj+wPn8A712XuA==</latexit>\n7H[W\u0003WRNHQV\u000f\u0003UDQJLQJ\u0003IURP\u0003\u001b\u0014\u001c\u0015\u0003WR\u0003\u0018\u001b\u0014\u001c\u0015\u0011\u0003 \u0014\u0013\u0015\u0017\u0003,PDJH\u0003WRNHQV\u000f\u0003UDQJLQJ\u0003IURP\u0003\u0013\u0003WR\u0003\u001b\u0014\u001c\u0015\u0011\u0003\nFigure 3: The framework of CogView. [ROI1], [BASE1], etc., are seperator tokens.\nWe hypothesize that text modeling abstracts knowledge in hidden layers, which can be efﬁciently\nexploited during the later image modeling.\nWe train the model with batch size of 6,144 sequences (6.7 million tokens per batch) for 144,000 steps\non 512 V100 GPUs (32GB). The parameters are updated by Adam with max lr = 3×10−4,β1 =\n0.9,β2 = 0.95,weight decay = 4×10−2. The learning rate warms up during the ﬁrst 2% steps and\ndecays with cosine annealing [34]. With hyperparameters in an appropriate range, we ﬁnd that the\ntraining loss mainly depends on the total number of trained tokens (tokens per batch ×steps), which\nmeans that doubling the batch size (and learning rate) results in a very similar loss if the same number\nof tokens are trained. Thus, we use a relatively large batch size to improve the parallelism and reduce\nthe percentage of time for communication. We also design a three-region sparse attention to speed up\ntraining and save memory without hurting the performance, which is introduced in Appendix B.\n2.4 Stabilization of training\nCurrently, pretraining large models (>2B parameters) usually relies on 16-bit precision to save GPU\nmemory and speed up the computation. Many frameworks, e.g. DeepSpeed ZeRO [40], even only\nsupport FP16 parameters. However, text-to-image pretraining is very unstable under 16-bit precision.\nTraining a 4B ordinary pre-LN Transformer will quickly result in NaN loss within 1,000 iterations. To\nstabilize the training is the most challenging part of CogView, which is well-aligned with DALL-E.\nWe summarize the solution of DALL-E as totolerate the numerical problem of training. Since the\nvalues and gradients vary dramatically in scale in different layers, they propose a new mixed-precision\nframework per-resblock loss scaling and store all gains, biases, embeddings, and unembeddings in\n32-bit precision, with 32-bit gradients. This solution is complex, consuming extra time and memory\nand not supported by most current training frameworks.\nCogView instead regularizes the values. We ﬁnd that there are two kinds of instability: overﬂow\n(characterized by NaN losses) and underﬂow (characterized by diverging loss). The following\ntechniques are proposed to solve them.\nPrecision Bottleneck Relaxation (PB-Relax).After analyzing the dynamics of training, we ﬁnd\nthat overﬂow always happens at two bottleneck operations, the ﬁnal LayerNorm or attention.\n• In the deep layers, the values of the outputs could explode to be as large as 104 ∼\n105, making the variation in LayerNorm overﬂow. Luckily, as LayerNorm(x) =\nLayerNorm(x/max(x)), we can relax this bottleneck by dividing the maximum ﬁrst4.\n• The attention scores QTK/\n√\ndcould be signiﬁcantly larger than input elements, and result\nin overﬂow. Changing the computational order into QT(K/\n√\nd) alleviates the problem.\nTo eliminate the overﬂow, we notice that softmax(QTK/\n√\nd) = softmax(QTK/\n√\nd−\n4We cannot directly divide x by a large constant, which will lead to underﬂow in the early stage of training.\n5\nFigure 4: (a) Illustration of different LayerNorm structures in Transformers. Post-LN is from the\noriginal paper; Pre-LN is the most popular structure currently; Sandwich-LN is our proposed structure\nto stabilize training. (b) The numerical scales in our toy experiments with 64 layers and a large\nlearning rate. Trainings without Sandwich-LN overﬂow in main branch; trainings without PB-relax\noverﬂow in attention; Only the training with both can continue.\nconstant), meaning that we can change the computation of attention into\nsoftmax(QTK√\nd\n) =softmax\n(( QT\nα\n√\nd\nK−max( QT\nα\n√\nd\nK)\n)\n×α\n)\n, (4)\nwhere αis a big number, e.g. α = 32.5 In this way, the maximum (absolute value) of\nattention scores are also divided by αto prevent it from overﬂow. A detailed analysis about\nthe attention in CogView is in Appendix C.\nSandwich LayerNorm (Sandwich-LN).The LayerNorms [ 1] in Transformers are essential for\nstable training. Pre-LN [50] is proven to converge faster and more stable than the original Post-LN,\nand becomes the default structure of Transformer layers in recent works. However, it is not enough\nfor text-to-image pretraining. The output of LayerNorm (x−¯x)\n√\nd√∑\ni(xi−¯x)2 γ+ βis basically proportional\nto the square root of the hidden size of x, which is\n√\nd=\n√\n2560 ≈50 in CogView. If input values\nin some dimensions are obviously larger than the others – which is true for Transformers – output\nvalues in these dimensions will also be large (101 ∼102). In the residual branch, these large values\nare magniﬁed and be added back to the main branch, which aggravates this phenomenon in the next\nlayer, and ﬁnally causes the value explosion in the deep layers.\nThis reason behind value explosion inspires us to restrict the layer-by-layer aggravation. We propose\nSandwich LayerNorm, which also adds a LayerNorm at the end of each residual branch. Sandwich-\nLN ensures the scale of input values in each layer within a reasonable range, and experiments on\ntraining 500M model shows that its inﬂuence on convergence is negligible. Figure 4(a) illustrates\ndifferent LayerNorm structures in Transformers.\nToy Experiments.Figure 4(b) shows the effectiveness of PB-relax and Sandwich-LN with a toy\nexperimental setting, since training many large models for veriﬁcation is not realistic. We ﬁnd that\ndeep transformers (64 layers, 1024 hidden size), large learning rates (0.1 or 0.01), small batch\nsize (4) can simulate the value explosion in training with reasonable hyperparameters. PB-relax +\nSandwich-LN can even stabilize the toy experiments.\nShrink embedding gradient. Although we did not observe any sign of underﬂow after using\nSandwich-LN, we ﬁnd that the gradient of token embeddings is much larger than that of the other\nparameters, so that simply shrinking its scale by α= 0.1 increases the dynamic loss scale to further\nprevent underﬂow, which can be implemented by emb=emb*alpha+emb.detach()*(1-alpha)\nin Pytorch. It seems to slow down the updating of token embeddings, but actually does not hurt\nperformance in our experiments, which also corresponds to a recent work MoCo v3 [9].\nDiscussion. The PB-relax and Sandwich-LN successfully stabilize the training of CogView and\na 8.3B-parameter CogView-large. They are also general for all Transformer pretraining, and will\nenable the training of very deep Transformers in the future. As an evidence, we used PB-relax\nsuccessfully eliminating the overﬂow in training a 10B-parameter GLM [14]. However, in general,\n5The max must be at least head-wise, because the values vary greatly in different heads.\n6\nthe precision problems in language pretraining is not so signiﬁcant as in text-to-image pretraining.\nWe hypothesize that the root is the heterogeneity of data, because we observed that text and image\ntokens are distinguished by scale in some hidden states. Another possible reason is hard-to-ﬁnd\nunderﬂow, guessed by DALL-E. A thorough investigation is left for future work.\n3 Finetuning\nCogView steps further than DALL-E on ﬁnetuning. Especially, we can improve the text-to-image\ngeneration via ﬁnetuning CogView for super-resolution and self-reranking. All the ﬁnetuning tasks\ncan be completed within one day on a single DGX-2.\n3.1 Super-resolution\nSince the image tokenizer compresses 256 ×256-pixel images into 32 ×32-token sequences before\ntraining, the generated images are blurrier than real images due to the lossy compression. However,\nenlarging the sequence length will consume much more computation and memory due to the O(n2)\ncomplex of attention operations. Previous works [13] about super-resolution, or image restoration,\nusually deal with images already in high resolution, mapping the blurred local textures to clear\nones. They cannot be applied to our case, where we need to add meaningful details to the generated\nlow-resolution images. Figure 5 (b) is an example of our ﬁnetuning method, and illustrates our\ndesired behavior of super-resolution.\nThe motivation of our ﬁnetuning solution for super-resolution is a belief that CogView is trained on\nthe most complex distribution in general domain, and the objects of different resolution has already\nbeen covered.6 Therefore, ﬁnetuning CogView for super-resolution should not be hard.\nSpeciﬁcally, we ﬁrst ﬁnetune CogView into a conditional super-resolution model from16 ×16 image\ntokens to 32 ×32 tokens. Then we magnify an image of 32 ×32 tokens to 64 ×64 tokens (512 ×512\npixels) patch-by-patch via a center-continuous sliding-window strategy in Figure 5 (a). This order\nperforms better that the raster-scan order in preserving the completeness of the central area.\nTo prepare data, we crop about 2 million images to 256 ×256 regions and downsample them to\n128×128. After tokenization, we get 32×32 and 16×16 sequence pairs for different resolution. The\npattern of ﬁnetuning sequence is “[ROI1] text tokens [BASE][BOI1] 16 ×16 image tokens [EOI1]\n[ROI2][BASE] [BOI2]32 ×32 image tokens [EOI2]”, longer than the max position embedding\nindex 1087. As a solution, we recount the position index from 0 at [ROI2].7\n1 2 3\n4\n5 6\n7 8 9\n\u000bD\f\u0003&HQWHU\u0010FRQWLQXRXV\u0003VOLGLQJ\u0003ZLQGRZ \u000bE\f\u0003'LƈHUHQW\u0003VXSHU\u0010UHVROXWLRQ\u0003UHVXOWV\u0003IRU\u0003ŉD\u0003WLJHU\u0003LV\u0003SOD\\LQJ\u0003IRRWEDOOŊ\u0011\nFigure 5: (a) A 64 ×64-token image are generated patch-by-patch in the numerical order. The\noverlapping positions will not be overwritten. The key idea is to make the tokens in the 2nd and 4th\nregions – usually regions of faces or other important parts – generated when attending to the whole\nregion. (b) The ﬁnetuned super-resolution model does not barely transform the textures, but generates\nnew local structures, e.g. the open mouth or tail in the example.\n6An evidence to support the belief is that if we append “close-up view” at the end of the text, the model will\ngenerate details of a part of the object.\n7One might worry about that the reuse of position indices could cause confusions, but in practice, the model\ncan distinguish the two images well, probably based on whether they can attend to a [ROI2] in front.\n7\n3.2 Image Captioning and Self-reranking\nTo ﬁnetune CogView for image captioning is straightforward: exchanging the order of text and image\ntokens in the input sequences. Since the model has already learnt the corresponding relationships\nbetween text and images, reversing the generation is not hard. We did not evaluate the performance\ndue to that (1) there is no authoritative Chinese image captioning benchmark (2) image captioning is\nnot the focus of this work. The main purpose of ﬁnetuning such a model is for self-reranking.\nWe propose the Caption Loss (CapLoss) to evaluate the correspondence between images and text.\nMore speciﬁcally, CapLoss(x,t) = 1\n|t|\n∑|t|\ni=0 −log p(ti|x,t0:i−1), where t is a sequence of text\ntokens and xis the image. CapLoss(x,t) is the cross-entropy loss for the text tokens, and this method\ncan be seen as an adaptation of inverse prompting [56] for text-to-image generation. Finally, images\nwith the lowest CapLosses are chosen.\nCompared to additionally training another constrastive self-supervised model, e.g. CLIP [ 38], for\nreranking, our method consumes less computational resource because we only need ﬁnetuning. The\nresults in Figure 9 shows the images selected by our methods performs better in FID than those\nselected by CLIP. Figure 6 shows an example for reranking.\nFigure 6: 60 generated images for “A man in red shirt is playing video games” (selected at random\nfrom COCO), displayed in the order of CapLoss. Most bad cases are ranked in last places. The\ndiversity also eases the concern that CogView might be overﬁtting a similar image in the training set.\n3.3 Style Learning\nAlthough CogView is pretrained to cover diverse images as possible, the desire to generate images of a\nspeciﬁc style or topic cannot be satisﬁed well. We ﬁnetune models on four styles: Chinese traditional\ndrawing, oil painting, sketch, and cartoon. Images of these styles are automatically extracted from\nsearch engine pages including Google, Baidu and Bing, etc., with keyword as “An image of{style}\nstyle”, where {style} is the name of style. We ﬁnetune the model for different styles separately,\nwith 1,000 images each.\nDuring ﬁnetuning, the corresponding text for the images are also “An image of {style} style“.\nWhen generating, the text is “A {object} of {style} style“, where {object} is the object to\ngenerate. In this way, CogView can transfer the knowledge of shape of the objects learned from\npretraining to the style of ﬁnetuning. Figure 7 shows examples for the styles.\nFigure 7: Generated images for “The Oriental Pearl” (a landmark of Shanghai) in different styles.\n8\n3.4 Industrial Fashion Design\nFigure 8: Generated images for fashion design.\nWhen the generation targets at a single domain, the\ncomplexity of the textures are largely reduced. In\nthese scenarios, we can (1) train a VQGAN [ 15]\ninstead of VQV AE for the latent variable for more\nrealistic textures, (2) decrease the number of pa-\nrameters and increase the length of sequences for\na higher resolution. Our three-region sparse atten-\ntion (Appendix B) can speed up the generation of\nhigh-resolution images in this case.\nWe train a 3B-parameter model on about 10 million\nfashion-caption pairs, using50×50 VQGAN image\ntokens and decodes them into 800 ×800 pixels.\nFigure 8 shows samples of CogView for fashion\ndesign, which has been successfully deployed to Alibaba Rhino fashion production.\n4 Experimental Results\n4.1 Machine Evaluation\nAt present, the most authoritative machine evaluation metrics for general-domain text-to-image\ngeneration is the FID on MS COCO, which is not included in our training set. To compare with\nDALL-E, we follow the same setting, evaluating CogView on a subset of 30,000 captions sampled\nfrom the dataset, after applying a Gaussian ﬁlter with varying radius to both the ground-truth and\ngenerated images.8 The captions are translated into Chinese for CogView by machine translation.\nTo fairly compare with DALL-E, we do not use super-resolution. Besides, DALL-E generates 512\nimages for each caption and selects the best one by CLIP, which needs to generate about 15 billion\ntokens. To save computational resource, we select the best one from 60 generated images according\nto their CapLosses. The evaluation of CapLoss is on a subset of 5,000 images. We ﬁnally enhance\nthe contrast of generated images by 1.5. Table 1 shows the metrics for CogView and other methods.\nTable 1: Metrics for machine evaluation. Statistics about DALL-E and GANs are extracted from their\nﬁgures. FID-kmeans that all the images are blurred by a Gaussian Filter with radius k.\nModel FID-0 FID-1 FID-2 FID-4 FID-8 IS CapLoss\nAttnGAN 35.2 44.0 72.0 108.0 100.0 23.3 3.01\nDM-GAN 26.5 39.0 73.0 119.0 112.3 32.2 2.87\nDF-GAN 26.5 33.8 55.9 91.0 97.0 18.7 3.09\nDALL-E 27.5 28.0 45.5 83.5 85.0 17.9 —\nCogView 27.1 19.4 13.9 19.4 23.6 18.2 2.43\nCaption Loss as a Metric. FID and IS are designed to measure the quality of unconditional\ngeneration from relatively simple distributions, usually single objects. However, text-to-image\ngeneration should be evaluated pair-by-pair. Table 1 shows that DM-GAN achieves the best unblurred\nFID and IS, but is ranked last in human preference (Figure 10(a)). Caption Loss is an absolute\n(instead of relative, like CLIP) score, so that it can be averaged across samples. It should be a better\nmetrics for this task and is more consistent with the overall scores of our human evaluation in § 4.2.\nFigure 9: IS and FID-0 for CLIP and self-ranking.\nComparing self-reranking with CLIP. We\nevaluate the FID-0 and IS of CogView-generated\nimages selected by CLIP and self-reranking on\nMS COCO. Figure 9 shows the curves with dif-\nferent number of candidates. Self-reranking gets\nbetter FID, and steadily reﬁnes FID as the num-\nber of candidates increases. CLIP performs bet-\nter in increasing IS, but as discussed above, it is\nnot a suitable metric for this task.\n8We use the same evaluation codes with DM-GAN and DALL-E, which is available athttps://github.\ncom/MinfengZhu/DM-GAN.\n9\nDiscussion about the differences in performance between CogView and DALL-E.Since DALL-\nE is pretrained with more data and parameters than CogView, why CogView gets a better FID even\nwithout super-resolution? It is hard to know the accurate reason, because DALL-E is not open-source,\nbut we guess that the reasons include: (1) CogView uses PB-relax and Sandwich-LN for a more stable\noptimization. (2) DALL-E uses many cartoon and rendered data, making the texture of generated\nimages quite different from that of the photos in MS COCO. (3) Self-reranking selects images better\nin FID than CLIP. (4) CogView is trained longer (96B trained tokens in CogView vs. 56B trained\ntokens in DALL-E).\n4.2 Human Evaluation\nHuman evaluation is much more persuasive than machine evaluation on text-to-image generation. Our\nhuman evaluation consists of 2,950 groups of comparison between images generated by AttnGAN,\nDM-GAN, DF-GAN, CogView, and recovered ground truth, i.e., the ground truth blurred by our\nimage tokenizer. Details and example-based comparison between models are in Appendix E.\nResults in Figure 10 show that CogView outperforms GAN-based baselines at a large margin.\nCogView is chosen as the best one with probability 37.02%, competitive with the performance\nof recovered ground truth (59.53%). Figure 10(b)(c) also indicates our super-resolution model\nconsistently improves the quality of images, especially the clarity, which even outperforms the\nrecovered ground truth.\nFigure 10: Human Evaluation results. The recovered ground truth is obtained by ﬁrst encoding the\nground truth image and then decoding it, which is theoretically the upper bound of CogView.\n5 Conclusion and Discussion\nLimitations. A disadvantage of CogView is the slow generation, which is common for auto-regressive\nmodel, because each image is generated token-by-token. The blurriness brought by VQV AE is also\nan important limitation. These problems will be solved in the future work.\nEthics Concerns.Similar to Deepfake, CogView is vulnerable to malicious use [49] because of its\ncontrollable and strong capacity to generate images. The possible methods to mitigate this issue are\ndiscussed in a survey [5]. Moreover, there are usually fairness problems in generative models about\nhuman 9. In Appendix D, we analyze the situation about fairness in CogView and introduce a simple\n“word replacing” method to solve this problem.\nWe systematically investigate the framework of combining VQV AE and Transformers for text-to-\nimage generation. CogView demonstrates promising results for scalable cross-modal generative\npretraining, and also reveals and solves the precision problems probably originating from data\nheterogeneity. We also introduce methods to ﬁnetune CogView for diverse downstream tasks. We\nhope that CogView could advance both research and application of controllable image generation\nand cross-modal knowledge understanding, but need to prevent it from being used to create images\nfor misinformation.\n9https://thegradient.pub/pulse-lessons\n10\nAcknowledgments and Disclosure of Funding\nWe would like to thank Zhao Xue, Zhengxiao Du, Hanxiao Qu, Hanyu Zhao, Sha Yuan, Yukuo Cen,\nXiao Liu, An Yang, Yiming Ju for their help in data, machine maintaining or discussion. We would\nalso thank Zhilin Yang for presenting this work at the conference of BAAI.\nFunding in direct support of this work: a fund for GPUs donated by BAAI, a research fund from\nAlibaba Group, NSFC for Distinguished Young Scholar (61825602), NSFC (61836013).\nReferences\n[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[2] Y . Bengio, N. Léonard, and A. Courville. Estimating or propagating gradients through stochastic\nneurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\n[3] H. M. Bonnici, F. R. Richter, Y . Yazar, and J. S. Simons. Multimodal feature integration in\nthe angular gyrus during episodic and semantic retrieval. Journal of Neuroscience, 36(20):\n5462–5471, 2016.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[5] M. Brundage, S. Avin, J. Clark, H. Toner, P. Eckersley, B. Garﬁnkel, A. Dafoe, P. Scharre,\nT. Zeitzoff, B. Filar, et al. The malicious use of artiﬁcial intelligence: Forecasting, prevention,\nand mitigation. arXiv preprint arXiv:1802.07228, 2018.\n[6] A. Caliskan, J. J. Bryson, and A. Narayanan. Semantics derived automatically from language\ncorpora contain human-like biases. Science, 356(6334):183–186, 2017.\n[7] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever. Generative pretraining\nfrom pixels. In International Conference on Machine Learning , pages 1691–1703. PMLR,\n2020.\n[8] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on machine learning, pages 1597–1607.\nPMLR, 2020.\n[9] X. Chen, S. Xie, and K. He. An empirical study of training self-supervised visual transformers.\narXiv preprint arXiv:2104.02057, 2021.\n[10] K. Clark, U. Khandelwal, O. Levy, and C. D. Manning. What does bert look at? an analysis of\nbert’s attention.arXiv preprint arXiv:1906.04341, 2019.\n[11] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages\n248–255. Ieee, 2009.\n[12] M. Ding. The road from MLE to EM to V AE: A brief tutorial. URL https://www.\nresearchgate.net/profile/Ming-Ding-2/publication/342347643_The_Road_\nfrom_MLE_to_EM_to_VAE_A_Brief_Tutorial/links/5f1e986792851cd5fa4b2290/\nThe-Road-from-MLE-to-EM-to-VAE-A-Brief-Tutorial.pdf .\n[13] C. Dong, C. C. Loy, K. He, and X. Tang. Learning a deep convolutional network for image\nsuper-resolution. In European conference on computer vision, pages 184–199. Springer, 2014.\n[14] Z. Du, Y . Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang. All nlp tasks are generation tasks:\nA general pretraining framework. arXiv preprint arXiv:2103.10360, 2021.\n[15] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis.\narXiv preprint arXiv:2012.09841, 2020.\n11\n[16] P. Gage. A new algorithm for data compression. C Users Journal, 12(2):23–38, 1994.\n[17] J. Gasquet. Cézanne. pages 159–186, 1926.\n[18] X. Glorot and Y . Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on artiﬁcial intelligence\nand statistics, pages 249–256. JMLR Workshop and Conference Proceedings, 2010.\n[19] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,\nand Y . Bengio. Generative adversarial networks.arXiv preprint arXiv:1406.2661, 2014.\n[20] K. Gregor, I. Danihelka, A. Graves, D. Rezende, and D. Wierstra. Draw: A recurrent neural\nnetwork for image generation. In International Conference on Machine Learning , pages\n1462–1471. PMLR, 2015.\n[21] J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. H. Richemond, E. Buchatskaya, C. Doersch,\nB. A. Pires, Z. D. Guo, M. G. Azar, et al. Bootstrap your own latent: A new approach to\nself-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\n[22] K. Grill-Spector and R. Malach. The human visual cortex. Annu. Rev. Neurosci., 27:649–677,\n2004.\n[23] J. He, D. Spokoyny, G. Neubig, and T. Berg-Kirkpatrick. Lagging inference networks and\nposterior collapse in variational autoencoders. In International Conference on Learning Repre-\nsentations, 2018.\n[24] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual\nrepresentation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9729–9738, 2020.\n[25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilibrium. In Proceedings of the 31st\nInternational Conference on Neural Information Processing Systems, pages 6629–6640, 2017.\n[26] D. P. Kingma and M. Welling. Auto-encoding variational bayes.arXiv preprint arXiv:1312.6114,\n2013.\n[27] J. Y . Koh, J. Baldridge, H. Lee, and Y . Yang. Text-to-image generation grounded by ﬁne-grained\nuser attention. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer\nVision, pages 237–246, 2021.\n[28] T. Kudo and J. Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71,\nBrussels, Belgium, Nov. 2018. Association for Computational Linguistics. doi: 10.18653/v1/\nD18-2012. URL https://www.aclweb.org/anthology/D18-2012.\n[29] W. Li, P. Zhang, L. Zhang, Q. Huang, X. He, S. Lyu, and J. Gao. Object-driven text-to-image\nsynthesis via adversarial training. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12174–12182, 2019.\n[30] J. Lin, R. Men, A. Yang, C. Zhou, M. Ding, Y . Zhang, P. Wang, A. Wang, L. Jiang, X. Jia, et al.\nM6: A chinese multimodal pretrainer. arXiv preprint arXiv:2103.00823, 2021.\n[31] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár, and C. L. Zitnick.\nMicrosoft coco: Common objects in context. In European conference on computer vision, pages\n740–755. Springer, 2014.\n[32] X. Liu, F. Zhang, Z. Hou, Z. Wang, L. Mian, J. Zhang, and J. Tang. Self-supervised learning:\nGenerative or contrastive. arXiv preprint arXiv:2006.08218, 1(2), 2020.\n[33] X. Liu, Y . Zheng, Z. Du, M. Ding, Y . Qian, Z. Yang, and J. Tang. Gpt understands, too.arXiv\npreprint arXiv:2103.10385, 2021.\n12\n[34] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts.arXiv preprint\narXiv:1608.03983, 2016.\n[35] E. Mansimov, E. Parisotto, J. L. Ba, and R. Salakhutdinov. Generating images from captions\nwith attention. ICLR, 2016.\n[36] N. Parmar, A. Vaswani, J. Uszkoreit, L. Kaiser, N. Shazeer, A. Ku, and D. Tran. Image\ntransformer. In International Conference on Machine Learning , pages 4055–4064. PMLR,\n2018.\n[37] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[38] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision.\narXiv preprint arXiv:2103.00020, 2021.\n[39] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. V oss, A. Radford, M. Chen, and I. Sutskever.\nZero-shot text-to-image generation. arXiv preprint arXiv:2102.12092, 2021.\n[40] J. Rasley, S. Rajbhandari, O. Ruwase, and Y . He. Deepspeed: System optimizations enable\ntraining deep learning models with over 100 billion parameters. InProceedings of the 26th ACM\nSIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505–3506,\n2020.\n[41] A. Razavi, A. v. d. Oord, and O. Vinyals. Generating diverse high-ﬁdelity images with vq-vae-2.\narXiv preprint arXiv:1906.00446, 2019.\n[42] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial\ntext to image synthesis. In International Conference on Machine Learning, pages 1060–1069.\nPMLR, 2016.\n[43] T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Radford, and X. Chen. Improved\ntechniques for training gans. In Proceedings of the 30th International Conference on Neural\nInformation Processing Systems, pages 2234–2242, 2016.\n[44] P. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed,\nimage alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers) , pages\n2556–2565, 2018.\n[45] M. Tao, H. Tang, S. Wu, N. Sebe, F. Wu, and X.-Y . Jing. Df-gan: Deep fusion generative\nadversarial networks for text-to-image synthesis. arXiv preprint arXiv:2008.05865, 2020.\n[46] A. van den Oord, O. Vinyals, and K. Kavukcuoglu. Neural discrete representation learning. In\nProceedings of the 31st International Conference on Neural Information Processing Systems,\npages 6309–6318, 2017.\n[47] A. Van Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. In\nInternational Conference on Machine Learning, pages 1747–1756. PMLR, 2016.\n[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[49] M. Westerlund. The emergence of deepfake technology: A review. Technology Innovation\nManagement Review, 9(11), 2019.\n[50] R. Xiong, Y . Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y . Lan, L. Wang, and T. Liu.\nOn layer normalization in the transformer architecture. In International Conference on Machine\nLearning, pages 10524–10533. PMLR, 2020.\n[51] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He. Attngan: Fine-grained\ntext to image generation with attentional generative adversarial networks. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 1316–1324, 2018.\n13\n[52] S. Yuan, H. Zhao, Z. Du, M. Ding, X. Liu, Y . Cen, X. Zou, and Z. Yang. Wudaocorpora: A\nsuper large-scale chinese corpora for pre-training language models. Preprint, 2021.\n[53] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P. Pham, A. Ravula,\nQ. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. arXiv preprint\narXiv:2007.14062, 2020.\n[54] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N. Metaxas. Stackgan: Text to\nphoto-realistic image synthesis with stacked generative adversarial networks. In Proceedings of\nthe IEEE international conference on computer vision, pages 5907–5915, 2017.\n[55] M. Zhu, P. Pan, W. Chen, and Y . Yang. Dm-gan: Dynamic memory generative adversarial\nnetworks for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5802–5810, 2019.\n[56] X. Zou, D. Yin, Q. Zhong, H. Yang, Z. Yang, and J. Tang. Controllable generation from\npre-trained language models via inverse prompting. arXiv preprint arXiv:2103.10685, 2021.\n14\nA Data Collection and Details about the Tokenizers\nWe collected about 30 million text-image pairs from multiple channels, and built a 2.5TB new\ndataset (after tokenization, the size becomes about 250GB). The dataset is an extension of project\nWudaoCorpora [52]10. About 50% of the text is in English, including Conceptual Captions [44]. They\nare translated into Chinese by machine translation. In addition, we did not remove the watermarks\nand white edges in the dataset even though they affect the quality of generated images, because we\nthink it will not inﬂuence the conclusions of our paper from the perspective of research.\nThe sources of data are basically classiﬁed into the following categories: (1) Professional image\nwebsites (both English and Chinese). The images in the websites are usually with captions. Data\nfrom this channel constitute the highest proportion. (2) Conceptual Captions [44] and ImageNet [11].\n(3) News pictures online with their surrounding text. (4) A small part of item-caption pairs from\nAlibaba . (5) Image search engines. In order to cover as many common entities as possible, we made\na query list consist of 1,200 queries. Every query was an entity name extracted from a large-scale\nknowledge graph. We choose seven major categories: food, regions, species, people names, scenic,\nproducts and artistic works. We extracted top-kentities for each category based on their number of\noccurrences in the English Wikipedia, where kis manually selected for each category. We collected\nthe top-100 images returned by every major search engine website for each query.\nWe have already introduced tokenizers in section 2.2, and here are some details. The text tokenizer are\ndirectly based on the SentencePiece package at https://github.com/google/sentencepiece.\nThe encoder in the image tokenizer is a 4-layer convolutional neural network (CNN) with 512 hidden\nunits and ReLU activation each layer. The ﬁrst three layers have a receptive ﬁeld of 4 and stride of\n2 to half the width and height of images, and the ﬁnal layer is a 1 ×1 convolution to transform the\nnumber of channels to 256, which is the hidden size of embeddings in the dictionary. The decoder\nhave the same architecture with the encoder except replacing convolution as deconvolution. The\nembeddings in the dictionary are initialized via Xavier uniform initialization [18].\nB Sparse Attention\nO\nText Text Text Text Text Text Text Text\nAll texts and some random “pivot” attention \n+ Blockwise window attention\nFigure 11: Illustration about our three-\nregion sparse attention. The sequence is\nshown as a H ×W image and some text\ntokens in front. Colored grids are all the to-\nkens attended to by the token marked “O”.\nIn this case, each block consists of four con-\nsecutive tokens.\nAs shown in Figure 11, we design the three-region\nsparse attention, an implementation-friendly sparse at-\ntention for text-to-image generation. Each token attends\nto all text tokens, all pivots tokens and tokens in the\nblocks in an adjacent window before it.\nThe pivot tokens are image tokens selected at random,\nsimilar to big bird [53]. They are re-sampled every time\nwe enter a new layer. We think they can provide global\ninformation about the image.\nThe blockwise window attention provides local infor-\nmation, which is the most important region. The for-\nward computation of 1-D window attention can be ef-\nﬁciently implemented inplace by carefully padding and\naltering the strides of tensors, because the positions to\nbe attended are already continguous in memory. How-\never, we still need extra memory for backward computa-\ntion if without customized CUDA kernels. We alleviate\nthis problem by grouping adjacent tokens into blocks,\nin which all the tokens attend to the same tokens (be-\nfore causally masking). More details are included in\nour released codes.\nIn our benchmarking on sequences of 4096 tokens,\nthe three-region sparse attention (768 text and pivot\ntokens, 768 blockwise window tokens) is 2.5×faster\nthan vanilla attention, and saves 40% GPU memory.\n10https://wudaoai.cn/data\n15\nThe whole training is 1.5×faster than that with vanilla attention and saves 20% GPU memory. With\nthe same hyperparameters, data and random seeds, their loss curves are nearly identical, which means\nthe sparse attention will not inﬂuence the convergence.\nHowever, we did not use three-region sparse attention during training the 4-billion-parameter\nCogView, due to the concern that it was probably not compatible with ﬁnetuning for super-resolution\nin section 3.1. But it successfully accelerated the training of CogView-fashion without side effects.\nC Attention Analysis\nTo explore the attention mechanism of CogView, we visualize the attention distribution during\ninference by plotting heat maps and marking the most attended tokens. We discover that our model’s\nattention heads exhibit strong ability on capturing both position and semantic information, and\nattention distribution varies among different layers. The analysis about the scale of attention scores is\nin section C.4.\nC.1 Positional Bias\nThe attention distribution is highly related to images’ position structures. There are a lot of heads\nheavily attending to ﬁxed positional offsets, especially multiple of 32 (which is the number of tokens\na row contains) (Figure 12 (a)). Some heads are specialized to attending to the ﬁrst few rows in the\nimage (Figure12 (b)) . Some heads’ heat maps show checkers pattern (Figure 12 (c)), indicating\ntokens at the boundary attends differently from that at the center. Deeper layers also show some broad\nstructural bias. For example, some heads attend heavily on tokens at top/lower half or the center of\nimages (Figure 12 (d)(e)).\nFigure 12: (a)(b)(c) Our model’s attention is highly related to images’ positional structures. (d)(e)\nOur model’s attention show some broad structural bias. (f) Some heads only attend to a few tokens\nsuch as separator token.\nC.2 Semantic Segmentation\nThe attention in CogView also shows that it also performs implicit semantic segmentation. Some\nheads highlight major items mentioned in the text. We use \"There is an apple on the table, and there\nis a vase beside it, with purple ﬂowers in it.\" as input of our experiment. In Figure 13 we marked\n16\nFigure 13: Our model’s attention heads successfully captured items like apple and purple ﬂowers.\nPixels corresponding to the most highly attended tokens are marked with red dots.\npixels corresponding to the most highly attended tokens with red dots, and ﬁnd that attention heads\nsuccessfully captured items like apple and purple ﬂowers.\nC.3 Attention Varies with Depth\nAttention patterns varies among different layers. Earlier layers focus mostly on positional information,\nwhile later ones focus more on the content. Interestingly, we observe that attention become sparse in\nthe last few layers (after layer 42), with a lot of heads only attend to a few tokens such as separator\ntokens (Figure 12 (f)). One possible explanation is that those last layers tend to concentrate on\ncurrent token to determine the output token, and attention to separator tokens may be used as a no-op\nfor attention heads which does not substantially change model’s output, similar to the analysis in\nBERT [10]. As the result, the last layers’ heads disregard most tokens and make the attention layers\ndegenerate into feed-forward layers.\nC.4 Value Scales of Attention\nAs a supplement to section 2.4, we visualize the value scales of attention in the 38-th layer, which has\nthe largest scale of attention scores QTK/\n√\ndin CogView. The scales varies dramatically in different\nheads, but the variance in each single head is small (that is why the attention does not degenerate,\neven though the scores are large). We think the cause is that the model wants different sensitiveness\nin different heads, so that it learns to multiply different constants to getQand K. As a side effect, the\nvalues may have a large bias. The PB-relax for attention is to remove the bias during computation.\nFigure 14: Illustration of scales of attention scores in the 38-th layer. Only half are heads are shown\nfor display reasons. The error bar is from the minimum to the maximum of scores. The values of\ntext-to-text attention scores are smaller, indicating the scales are related to the data.\n17\nFigure 15: The distribution of different genders, races and ages of the generation of “a face, photo”.\nD Fairness in CogView: Situation and Solution\nEvaluation of the situation of fairness in CogView.We examine the bias in the proportion of\ndifferent races and genders. Firstly, if given the detailed description in the text, e.g. a black man\nor an Asian woman, CogView can generate correctly for almost all samples. We also measure the\nproportion of the generated samples without speciﬁc description by the text “a face, photo”. The\nﬁgure of proportions in different races and genders are in Figure 15. The (unconditional) generated\nfaces are relatively balanced in races and ages, but with more men than women due to the data\ndistribution.\nCogView is also beset by the bias in gender due to the stereotypes if not specifying the gender.\nHowever if we specify the gender, almost all the gender and occupation are correct. We tested\nthe examples introduced in [ 6], and generated images for the text {male, female} ×{“science”,\nmathematics”, “arts”, “literature”}. Results are showed in this outer link to reduce the size of our\npaper.\nWord Replacing Solution.Different from the previous unconditional generative models, we have a\nvery simple and effective solution for racial and gender fairness.\nWe can directly add some adjective words sampled from “white”, “black”, “Asian”, ..., and “male”,\n“female” (if not speciﬁed) in the front of the words for human, like “people” or “person”, in the text.\nThe sampling is according to the real proportion in the whole population. We can train an additional\nNER model to ﬁnd the words about human.\nSince CogView will predict correctly according to the results above, if given description, this method\nwill greatly help solve the fairness problem in generative models.\nE Details about Human Evaluation\nTo evaluate the performance, we conduct a human evaluation to make comparisons between various\nmethods, similar to previous works [27, 39]. In our designed evaluation, 50 images and their captions\nare randomly selected from the MS COCO dataset. For each image, we use the caption to generate\nimages based on multiple models including AttnGAN, DM-GAN, DF-GAN and CogView. We do not\ngenerate images with DALL-E as their model has not been released yet. For each caption, evaluators\nare asked to give scores to 4 generated images and the recovered ground truth image respectively. The\nrecovered ground truth image refers to the image obtained by ﬁrst encoding the ground truth image\n(the original image in the MS COCO dataset after cropped into the target size) and then decoding it.\nFor each image, evaluators ﬁrst need to give 3 scores (1 ∼5) to evaluate the image quality from three\naspects: the image clarity, the texture quality and the relevance to the caption. Then, evaluators will\ngive an overall score (1 ∼10) to the image. After all 5 images with the same caption are evaluated,\nevaluators are required to select the best image additionally.\n72 anonymous evaluators are invited in the evaluation. To ensure the validity of the evaluation results,\nwe only collect answers from evaluators who complete all questions and over 80% of the selected\nbest images are accord with the one with the highest overall quality score. Finally, 59 evaluators\nare kept. Each evaluator is awarded with 150 yuan for the evaluation. There is no time limit for the\nanswer.\nTo further evaluate the effectiveness of super-resolution, we also introduced a simple A-B test in the\nhuman evaluation. Evaluators and captions are randomly divided into two groups Ea,Eb and Ca,Cb\n18\nrespectively. For evaluators in Ea, the CogView images with captions from Ca are generated without\nsuper-resolution while those from Cb are generated with super-resolution. The evaluators in Eb do\nthe reverse. Finally, we collected equal number of evaluation results for CogView images with and\nwithout super-resolution.\nThe average scores and their standard deviation are plotted in Figure 10. Several examples of captions\nand images used in the human evaluation are listed in Figure 16. The evaluation website snapshots\nare displayed in Figure 17.\n一名男子在小雨中偷看\n窗外。\nA man peeks out the \nwindow in the light rain.\nRecovered \nGround Truth AttnGAN DF-GAN DM-GAN CogView CogView\nsuper-resolution\n水中房屋的倒影。\nThe reflection of the \nhouse in the water.\n上面飞着鸟儿的码头照\n片。\nA picture of the pier with \nbirds flying above.\n三只毛绒熊拥抱并坐在\n蓝色枕头上\nThree plush bears hug \nand sit on blue pillows\n在城市街道上行驶的城\n市公交车\nA city bus driving on the \ncity street\n一个女人在一座白色的\n大山上滑雪。\nA woman is skiing on a \nwhite mountain.\n一只猫站在梳妆台抽屉\n里。\nA cat is standing in the \ndresser drawer.\n一种非常可爱的毛绒动\n物，戴着一顶有趣的帽\n子。\nA very cute stuffed \nanimal with a funny hat.\n一间有面向森林的大窗\n户的客厅。\nA living room with large \nwindows facing the \nforest.\n一个男人拿着盘子吃一\n块披萨的特写镜头。\nClose-up of a man \neating a piece of pizza \nwhile holding a plate.\nFigure 16: Human evaluation examples. The captions for evaluation are selected at random from MS\nCOCO.\nF Show Cases for captions from MS COCO\nIn Figure 18, we provide further examples of CogView on MS COCO.\n19\nFigure 17: Snapshots of the human evaluation website. The left side is the scoring page for images\nand the right side is the best-selection page for all images with the same caption.\n20\nFigure 18: More generated images for COCO captions (after super-resolution).\n21",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8255555033683777
    },
    {
      "name": "Generative grammar",
      "score": 0.7153688669204712
    },
    {
      "name": "Computer science",
      "score": 0.6310200691223145
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5547486543655396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4934453070163727
    },
    {
      "name": "Modal",
      "score": 0.4232063591480255
    },
    {
      "name": "Generative model",
      "score": 0.41227987408638
    },
    {
      "name": "Chemistry",
      "score": 0.09834113717079163
    },
    {
      "name": "Engineering",
      "score": 0.09725970029830933
    },
    {
      "name": "Voltage",
      "score": 0.072649747133255
    },
    {
      "name": "Electrical engineering",
      "score": 0.06991487741470337
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ]
}