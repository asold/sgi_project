{
  "title": "CataLM: empowering catalyst design through large language models",
  "url": "https://openalex.org/W4406424217",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2275644038",
      "name": "Ludi Wang",
      "affiliations": [
        "Computer Network Information Center",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2120035901",
      "name": "Xueqing Chen",
      "affiliations": [
        "Computer Network Information Center",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098036377",
      "name": "Yi Du",
      "affiliations": [
        "Computer Network Information Center",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Institute for Advanced Study"
      ]
    },
    {
      "id": "https://openalex.org/A2107216140",
      "name": "Yuanchun ZHOU",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Institute for Advanced Study",
        "Computer Network Information Center"
      ]
    },
    {
      "id": "https://openalex.org/A2065795670",
      "name": "Gao Yang",
      "affiliations": [
        "National Center for Nanoscience and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2078532592",
      "name": "Wenjuan Cui",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Computer Network Information Center",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2275644038",
      "name": "Ludi Wang",
      "affiliations": [
        "Computer Network Information Center",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2120035901",
      "name": "Xueqing Chen",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Computer Network Information Center",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098036377",
      "name": "Yi Du",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    },
    {
      "id": "https://openalex.org/A2107216140",
      "name": "Yuanchun ZHOU",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    },
    {
      "id": "https://openalex.org/A2065795670",
      "name": "Gao Yang",
      "affiliations": [
        "National Center for Nanoscience and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2078532592",
      "name": "Wenjuan Cui",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Computer Network Information Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2941803205",
    "https://openalex.org/W2573377826",
    "https://openalex.org/W2016366655",
    "https://openalex.org/W2076968595",
    "https://openalex.org/W2991239953",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W6756821666",
    "https://openalex.org/W6601899773",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4389520259",
    "https://openalex.org/W6606878106",
    "https://openalex.org/W4362519688",
    "https://openalex.org/W1976492731",
    "https://openalex.org/W1992985800",
    "https://openalex.org/W4288751893",
    "https://openalex.org/W4290887282",
    "https://openalex.org/W3201869313",
    "https://openalex.org/W4387779638",
    "https://openalex.org/W4385627740",
    "https://openalex.org/W4385027818",
    "https://openalex.org/W6608687280",
    "https://openalex.org/W4400543975",
    "https://openalex.org/W4391821988",
    "https://openalex.org/W6601076540",
    "https://openalex.org/W4383550741",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W6603143895",
    "https://openalex.org/W4361298520",
    "https://openalex.org/W4385565396",
    "https://openalex.org/W6826116265",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3213675203",
    "https://openalex.org/W4380434108"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nInternational Journal of Machine Learning and Cybernetics (2025) 16:3681–3691 \nhttps://doi.org/10.1007/s13042-024-02473-0\nORIGINAL ARTICLE\nCataLM: empowering catalyst design through large language models\nLudi Wang1 · Xueqing Chen1,2 · Yi Du1,2,3 · Yuanchun Zhou1,2,3 · Yang Gao4 · Wenjuan Cui1,2\nReceived: 26 June 2024 / Accepted: 14 November 2024 / Published online: 15 January 2025 \n© The Author(s) 2024\nAbstract\nThe field of catalysis holds paramount importance in shaping the trajectory of sustainable development, prompting intensive \nresearch efforts to leverage artificial intelligence (AI) in catalyst design. Presently, the fine-tuning of open-source large lan-\nguage models (LLMs) has yielded significant breakthroughs across various domains such as biology and healthcare. Drawing \ninspiration from these advancements, we introduce CataLM (Catalytic Language Model), a large language model tailored \nto the domain of electrocatalytic materials. Our findings demonstrate that CataLM exhibits remarkable potential for facili-\ntating human-AI collaboration in catalyst knowledge exploration and design. To the best of our knowledge, CataLM stands \nas the pioneering LLM dedicated to the catalyst domain, offering novel avenues for catalyst discovery and development.\nKeywords AI for science (AI4S) · Large language models (LLMs) · Electrocatalytic materials · Catalyst design\n1 Introduction\nThe field of catalysis is crucial to the future of sustainable \ndevelopment. Innovative catalysts can generate clean fuels, \nreduce the impact of global warming and provide solutions \nto environmental pollution [1 , 2]. Theoretical calculations \nand simulations can accelerate catalyst screening through \nactivity descriptors that link structure to catalyst activity \n[3–5]. However, numerous variables exist in the synthesis, \ncomposition, structure, and performance of electrocatalysts, \nwith much of this critical knowledge often elusive within \nscientific literature. This poses challenges in elucidating \nthe intricate correlations from limited experimental data. \nArtificial intelligence can be used to extract, analyze and \nunderstand key information embedded in the vast scientific \nliterature on catalysis that can be dedicated to predicting new \ncatalysts. Natural language processing techniques and gener-\native language models enable the extraction and analysis of \ntextual information from scientific literature and the genera-\ntion of domain-relevant on-demand text, which has shown \npotential in recent biological and thermoelectric research. \nHowever, language models in the field of catalysis are sparse \nand limited in scale, which restricts their use in empower -\ning knowledge extraction in catalytic materials research and \nfurther enabling the discovery of new catalysts.\nPre-trained models have demonstrated their powerful \ncapabilities in Natural Language Processing (NLP). There \nare two main types of pre-trained models: (1) BERT-like \nLudi Wang and Xueqing Chen have contributed equally to this \nwork.\n * Yang Gao \n gaoyang@nanoctr.cn\n * Wenjuan Cui \n wenjuancui@cnic.cn\n Ludi Wang \n wld@cnic.cn\n Xueqing Chen \n xqchen@cnic.cn\n Yi Du \n duyi@cnic.cn\n Yuanchun Zhou \n zyc@cnic.cn\n1 Computer Network Information Center, Chinese Academy \nof Sciences, Beijing 100083, China\n2 University of Chinese Academy of Sciences, Beijing 100049, \nChina\n3 Hangzhou Institute for Advanced Study, UCAS, \nHangzhou 310000, China\n4 CAS Key Laboratory of Nanosystem and Hierarchical \nFabrication, National Center for Nanoscience \nand Technology (NCNST), Beijing 100190, China\n3682 International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691\nmodels [6 –8], which are mainly used for language com-\nprehension tasks, and (2) GPT-like models [9 –11], which \nare mainly used for language generation tasks. Currently, \nlarge-scale language models (LLMs) such as GPT −4.0 \n[12] have laid a solid foundation for various applications. \nAlthough current large language models are effective in gen-\neral domains, they often fail to meet the needs of catalytic \nscientists. Much of this inadequacy is attributed to the lack \nof reliable knowledge about catalysts, as relevant catalyst \nstructural features and performance analyses are rarely pre-\nsent in commonly used pre-trained text corpora, such as C4 \n[13] and the Pile [ 14]. Furthermore, the best performing \nlarge language models like ChatGPT are only served through \nAPIs, which creates a barrier to research and progress in \nexternal domains. Fine-tuning open-source large language \nmodels is an effective way to meet domain-specific needs.\nCurrently, fine-tuning open-source large language models \nhave reached considerable success in fields such as biol-\nogy, healthcare, and finance. In biology, a domain-specific \npre-trained Transformer language model, BiOGPT [15], \nhas been developed for biomedical text generation and min-\ning. The model can be optimised and enhanced for perfor -\nmance in tasks such as biological named entity tasks and \nprotein molecular design. In healthcare, models like Huat-\nuoGPT [16] and DoctorGLM [17 ] have been developed to \naddress healthcare challenges, which exhibit a high degree \nof expertise and provide valuable insights into the healthcare \ndomain. In recent years, researchers have utilized existing \ndatabases such as Atomly [18], OQMD [19], MaterialsPro-\nject [20] and others. They have successfully explored the \ncomplex relationship between material structure and prop-\nerties [21], addressing the challenges posed by the scarcity \nof material data by developing more accurate AI optimisa-\ntion [22] and training methods [23]. With the application \nof LLMs, materials science researchers have explored the \nuse of these models to address challenges such as chemical \nreactions and the complex nature of structures. Examples \ninclude the MatSciBERT [24] model for the task of mate-\nrials named entity recognition. MatSciBERT uses a large \namount of materials science literature to fine-tune the BERT \nmodel [6], demonstrating the ability to automatically extract \ninformation from the literature, perform data mining, and \nconstruct knowledge graphs. MatChat [25] optimises the \nLLaMA2-7B model using knowledge of inorganic materi-\nals science literature and presents a viable solution for pre-\ndicting chemical synthesis pathways of inorganic materials, \nopening up new possibilities for the use of language models \nin materials science. To the best of our knowledge, there \nhas been no reported utilization of large language models in \ncatalyst science so far.\nIn this work, we provide CataLM, a large language \nmodel aligned with knowledge in the field of electrocatalytic \nmaterials. This large language model takes advantage of the \npre-trained Vicuna-13B model, and is trained on domain \nliterature and data annotated by experts. With this extensive \nand diverse data, the original LLM is specialized with two \nphases: Domain Pre-training, where the model har -\nvests the chemical knowledge from domain field literature, \nand Instruction Tuning, where the model further \nunderstands the requirements of downstream task with the \nannotation data. We use two tasks to validate CataLM, \nnamely entity extraction task and control method recommen-\ndation task. In addition to using the constructed knowledge \nbase for validation, we also invited domain experts to evalu-\nate the answers of CataLM to verify its generalization abil-\nity. Results show that our large language model has potent \npotential for human-AI collaboration in catalyst knowledge \nsearch and design. To the best of our knowledge, CataLM \nis the first LLM that focus on the catalyst domain field, and \nwe believe it can bring new possibilities for the preparation \nof new catalysts.\n2  Related work\nChatGPT was selected as one of Nature’s Top 10 Indi-\nviduals of 2023, marking the unprecedented selection of a \ncomputer program-the first non-human entity in history-to \nreceive such recognition. Nature states that this award aims \nto recognize the role of large language models (LLMs) in \nscientific development and progress. In the field of materials, \nnumerous studies have utilized language models to address \ndiverse tasks. Chen et al. provide the model MatChat [25], \nfor predicting inorganic material synthesis pathways. Xie \net al. [26] use FAIR database to fine-tune LLMs and design \na downstream task named SII which aims to extract hier -\narchical, domain specific material and device information, \nsuch as composition, structure, preparation conditions, etc., \nfrom unstructured scientific texts. Zheng et al. [27] used \nprompt engineering to guide ChatGPT in the automation of \ntext mining of metal-organic framework (MOF) synthesis \nconditions from diverse formats and styles of the scientific \nliterature. InstructMol [28] adopts Vicuna to multiple chemi-\ncal tasks with task-specific fine-tuning. Meta FAIR research-\ners launched the Open Materials 2024 (OMat24) dataset and \nlarge pre-trained models focusing on structural and compo-\nsitional diversity. OMat24 dataset [29] is one of the largest \npublicly available datasets in material fields. Leong et al \n[30] explored a multimodal large language model (MLLM) \nbased on scientific literature to analyze various data inputs \nfor automated electrosynthetic reaction mining. However, \nprevious works focus on the development of new materials \ninstead of new catalyst designing. Considering the diversity \nof structural characteristics such as composition, crystal \nstructure, and crystal plane of materials, potential catalysts \nare very abundant. Secondly, domain fine-tuning data sets \n3683International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691 \nwhich are consistent with downstream applications are cru-\ncial for the capability migration of LLMs, which is lacking \nin the field of catalyst design. This deficiency results in the \nmodel’s lack of catalyst knowledge, making it challenging \nto achieve satisfactory parameters.\nTo promote the creative utilization of large language \nmodels in catalysts science, this study utilizes a meticulously \ncrafted database for question-answering to investigate their \ncapabilities in the field of catalysts science. While building \nthis model, we also refer to the successful experiences in the \nfield of other science domains. For example, DeepGO-SE \n[31] tries to predict GO functions from protein sequences \nusing a pretrained large language model. MedPaLM2 [32] \nand PMC-LLaMa [33] attempt to tailor LLMs specifically \nfor the fields of biology and medicine through fine-tuning \nwith domain-specific instructions. In addition, Chen et al \nconstruct the xTrimo model [34] based on massive protein \nsequence data training, achieving sota levels on multiple \nprediction tasks.\n3  CataLM\nAs shown in Fig. 1, the training of CataLM consists of two \nstages, which are Domain Pre-training and Instruction Tun-\ning respectively. Due to the lack of open-source corpora for \nrecommending catalyst control methods, we utilized expert \nannotated corpora, as well as the retrieval enhanced corpora \ngenerated by large language models for training during the \ninstruction fine-tuning stage.\n3.1  Domain pre‑training\nIn this work, the text corpus we used to further pre-train \nVicuna-33b-v1.3, including the full text of open-access cata-\nlytic papers published in selected high-quality journals in the \nfield of electrocatalytic science. We used Web of Science to \nfind scientific literature on electrocatalytic CO 2 reduction. \nSpecifically, we exported the metadata of more than 22,000 \narticles from Web of Science using the keywords “CO 2 ”, \n“Reduction”, and “Electro*” as subject indexes. Eventually, \nwe used the full-text PDFs of 12,643 open-access papers to \nbuild the text corpus.\nPDF parsing. We build an automatic PDF parsing toolkit \nbased on the PyMuPDF library [35]. Since the processed \ndocuments contain irrelevant tags, we developed a data \ncleaning method for parsing article tag strings into consist-\nently formatted text paragraphs while retaining the same sec-\ntion and paragraph structure as the original paper. Finally, \nwe use regular expressions and rule-based scripts to clean \nthe data, removing the text obstructing reading, garbled, and \nimpurity data.\nVector database. Despite the fact that Language Model \nModels (LLMs) are capable of responding to broad inquir -\nies, they are limited in their ability to provide in-depth, pre-\ncise, and timely information within specific vertical domain. \nTo tackle this issue, we have employed vector databases to \naugment the reasoning capabilities of LLMs in vertical \ndomain contexts. Vector databases can transform litera-\nture and data into vector representations through the pro-\ncess of embedding vectors. For the establishment of vector \nFig. 1  The training pipeline of CataLM. The bottom part illustrates the primary training pipeline of CataLM, while the top part of the figure \ndelineates the entire data preparation process for training\n3684 International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691\ndatabases, Sci-BERT [36] has been utilized as an embedding \nmodel.\nThe study involved retrieving titles and abstracts from \na dataset containing 12,643 documents, manually annotat-\ning catalytic reaction processes by domain experts, and then \nmerging and converting these textual elements into vector \nrepresentations using Sci-BERT as an embedding frame-\nwork. In the context of a catalytic domain-specific task such \nas Name Entity Recognition (NER), the embedding model \noperates by converting the user query into vector form. \nRelevant articles are then identified by vector distance cal-\nculations to facilitate the retrieval of accurate and relevant \ninformation.\n3.2  Instruction tuning\nIn order to align pre-trained models with domain user intent, \nwe need to construct instruction tuning datasets. Currently \navailable generic instruction tuning datasets such as Alpaca-\nGPT4 [37] and ToolBench [38] can only teach models to fol-\nlow human instructions. For the specialized field of catalytic \nmaterials, we need to train models with knowledge-intensive \ndata that can reflect domain knowledge. Considering the \nrelatively small sample of data annotated by the experts, we \nuse the large language model pre-trained in the previous sec-\ntion to expand it by automatically extracting abstracts from \n12,643 documents. The entities were extracted based on an \nexpert-constructed system of electrocatalytic reduction sys-\ntems for literature content, including materials, conditioning \nmethods, products, faradaic efficiency, cell setup, electrolyte, \nsynthesis method, current density and voltage. The specific \nmeanings and dataset formats of these entities can be found \nin the previous corpus construction work [39].\nFirstly, we invite experts in the field of catalysis to per -\nform manual annotation using a well-developed annotation \ntool, AutoDive [40]. This tool allows annotators to access \nmaterial literature through a web browser, view sentences \nfor annotation, and interact with predefined entity types and \ndescriptions. Annotators have the flexibility to include new \nentities, rearrange existing ones, or make edits in a separate \nview. We end up with a standard corpus [39] in the field of \nelectrocatalytic CO2 reduction containing 6985 entities, with \neach record containing the entity extracted from the paper, \nits corresponding label, and the context sentence in which \nthe entity is located. The standard corpus is provided as a file \nin CSV format, and the details are shown in Table 1.\nNext, we used the pre-trained large language model based \non vector database augmentation from the previous section \nto perform automatic extraction of literature abstracts in the \nfield of catalysis, which extracts a total of 30,283 entities. It \nis important to highlight that the synthetic method of expert \nannotation in the dataset is an unstructured text paragraph \ndescription. We used a multi-model algorithm combining \npattern recognition and neural networks to convert it into a \nstructured synthetic pathway containing information about \nthe prepared and target materials, synthetic operations \nand operating conditions. This structuring of information \nenhances the interpretability of domain knowledge by the \nexpansive models.\nAfter rigorous filtering, de-duplication and cleaning, we \nobtained a training set consisting of 13,432 highly reliable \ncatalytic process descriptions, based on the combination of \ndomain entities. Next, this dataset is further pre-processed \nand integrated into an instruction question-answering for -\nmat. For example, for a certain catalytic reaction, using the \nentities provided in the dataset, we can reconstruct it as a \nrecommendation task for catalyst preparation for a given \nproduct. As shown in Fig. 2, the prompt involves a specific \ncatalyst material query for a given product, and the answer \nprovides the recommended material and its preparation \nmethod.\n3.3  Training process\nThe parameters of the model fine-tuning stage and applica-\ntion stage are listed in Table 2. In named entity recognition \ntask, we set temperature as 0.1 to limit the divergence of \nCataLM, and we tried various temperature values to verify \nthe recommendation ability of the CataLM. We use NVIDIA \nA100 GPUs for training, and techniques such as low-rank \nadaptation [41] is adopted to save storage memory and accel-\nerate the process. Low Rank Adaptation of Large Language \nModels, also known as LoRA, is a technology developed by \nMicrosoft researchers to address fine-tuning of large lan-\nguage models. The approach of LoRA is to freeze the pre-\ntrained model weight parameters, and then inject trainable \nlayers into each Transformer block. Since there is no need \nto recalculate the gradient of the model weight parameters, \nit greatly reduces the computational workload that needs to \nTable 1  The summary of the standard corpus\nEntity type Bench-\nmark \nCorpus\nMaterial 1092\nControl method 1086\nProduct (including the second and third product) 1340\nFaradaic efficiency (including the Faradaic efficiency of \nsecond and third product)\n1135\nCell setup 435\nElectrolyte 475\nSynthesis method 228\nCurrent density 393\nVoltage 801\nTotal 6985\n3685International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691 \nbe trained. Research has found that the fine-tuning quality of \nLoRA is comparable to that of full model fine-tuning, thus \nwe chose this method in the training process of CataLM. \nDue to hardware limitations, we will batch size as 10, and \nthe lora r was default setting in the work [41]. The alpha and \ndropout are set refer to PEFT in Huggingface library [42].\n4  Evaluation\n4.1  Named entity recognition task\nThe first task is named entity recognition, which aims to \nextract entity from the abstract of given literature. In this \ntask, we use a dataset of 12,643 abstract from electrocatalytic \nscientific literature (the full text of these literature also be \nused in the fine-tuning of CataLM) for named entity recog-\nnition. We extracted eight types of entity labels, including \nmaterial, control method, product, faradaic efficiency, cell \nsetup, electrolyte, current density, and voltage. When per -\nforming entity recognition, the user first inputs the text to be \nextracted, and the embedding model transforms it into vec-\ntors. Then the similar articles will be obtained by calculating \nthe vector distance, and will be used to generate precise and \npertinent information, which be shown in Fig. 3. The prompt \nwill be fed into the fine-tuned LLM for entity recognition.\nFor the evaluation and validation of the the entity extrac-\ntion capability of CataLM, we randomly select 160 entries \nand validate the LLM’s answers for them by experts, and \nensure that each category has 20 test data. The evaluation \nresult is shown in Table  3. The Count represents the total \namount of samples from different categories, the Correct \nrepresents the number of correctly identified entities, and \nthe Existence represents the number of entities of this type \nthat do exist in the text input to the large language model. It \nis worth mentioning that if there is indeed no corresponding \nentity in the text input to the large language model, the situ-\nation where the large language model answers empty should \nalso be considered as correct recognition. Therefore, we use \nModified Correct to remove the above influence. We have \nredesigned the validation method, comparing and analyzing \nthe results extracted from the large model with those anno-\ntated by experts, and calculating specific indicators based \non binary classification. The calculation method is shown \nin the Fig. 4.\nTo the best of our knowledge, CataLM is the first work \nto use LLMs for extracting knowledge content in the field \nof photocatalysis. However, in the field of materials, some \nscholars have used supervised intelligent methods to assist \nin the recognition of material entities. Wang et al. [43] pro-\nposed CHEMNER, an ontology guided, distantly-super -\nvised method for fine grained chemistry NER, and achieved \n45.96% F1. In the work [44], a supervised entity extraction \nFig. 2  Catalytic material recom-\nmended scenario’s command \nformat\nTable 2  Parameter set\nParameter Value\nFine-tuning stage\n Batch size 10\n Learning rate 3 ∗ 10−4\n Lora r 8\n Lora alpha 32\n Lora dropout 0.1\nNER stage\n Temperature /u1D6FD 0.1\n Top p 0.75\n Top k 40\n Beams 4\n Max tokens 1248\nRecommendation stage\n Temperature /u1D6FD 0.1/0.5\n Top p 0.75\n Top k 40\n Beams 4\n Max tokens 1248\n3686 International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691\nmethod was proposed in the field of Catalyst (the entity \nlabel are material, product, control method and faradaic \nefficiency), and obtain 57.68% F1. In this work, the ontol-\nogy is more complex and hard to recognize, in the entity \nlabel as catalyst, product and faraday efficiency, CataLM’s \nmodified accuracy (which also can be seen as F1) are 75, 85 \nand 90%, but it performs poorly in entity extraction for con-\ntrol method, cell setup, and the reason may be those entity \nare more difficult to understand and cause hallucinations in \nLLMs.\nWe also conducted ablation experiments in this paper. \nWe decomposed the model into two modules, namely the \nmodel Domain pre-training module and the Retrieval-Aug-\nmented Generation (RAG) module, and they were combined \nin pairs to form four possibilities. From Table  4, it can be \nseen that our method (i.e. Pre-training LLM + Few shot) \nperforms the best. We can also see that both the fine-tuned \nmodule and the RAG module contribute to the improvement \nof model extraction accuracy. To present the recent trends \nin the development of CO2 reduction electrocatalysts, we \nshowcased and analyzed the information in the extracted \ndatabase. Notably, for the sake of clarity, less frequently \nreported catalyst categories have been excluded. E/C and \nE-M are recommended choices for producing CO, while \nE-M and EOx demonstrate the potential for formic acid \nproduction. As for C2 products like C2 H4 and C2 H5OH, \nFig. 3  Prompt in the named \nentity recognition task\nFig. 4  Metric setting in CataLM\nTable 3  The evaluation of entity \nrecognition of CataLM Entity Count Correct Existence Modified \ncorrect\nRecall Precision F1\nMATERIAL 20 17 17 15 88.23% 83.33% 75%\nCONTROL METHOD 20 19 19 13 68.42% 92.86% 65%\nPRODUCT 20 17 17 17 100% 85% 85%\nFARADAIC EFFICIENCY 20 11 11 18 100% 84.62% 90%\nELECTROLYTE 20 10 10 10 90% 50% 50%\nPOTENTIAL 20 7 7 16 100% 63.63% 80%\nCURRENT DENSITY 20 7 7 12 100% 46.67% 60%\nCELL SETUP 20 6 6 9 100% 35.29% 45%\nOVERALL 160 85 94 110 90.43% 67.46% 68.75%\n3687International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691 \nboth E and EOx are viable alternatives. We also open source \nthe fine-tuning data, and domain experts can conduct more \nin-depth analysis.\n4.2  Control method recommendation task\nWith the continuous development of big data technology, \nbasic scientific research has shifted from the traditional \n\"random trial and error\" to the \"data-driven AI\" scientific \nmodel. Domain experts have also begun to attempt to use \nlarge language models to promote scientific innovation, such \nas literature understanding and summarization, experimen-\ntal scheme generation, as well as unmanned experimental \nsystems and scientific data sharing platforms, in order to \nimprove scientific research efficiency and promote scien-\ntific progress and development. CataLM focuses on the \nscientific problems in the Catalyst Control field, and tries to \nassist scientists in catalyst design. In Retrieval Augmented \nGeneration(RAG) method, we use Sci-BERT [36] as the \nembedding method to construct the vector database, and \nselect three papers as the references.\nHowever, how to evaluate the effectiveness of recom-\nmended catalyst control methods is a challenge faced by \nCataLM. In this paper, we invite domain experts to evalu-\nate and analyze the recommendation methods generated \nby CataLM and the original LLM. Several representative \nresults are listed in Table  5. The original LLM typically \noffers vague material selections and often lacks domain-\nspecific expertise. Additionally, its understanding of control \nmethods usually involves mere restatements of definitions, \nsometimes leading to significant misconceptions. In con-\ntrast, CataLM consistently integrates domain knowledge \nto provide more specific and accurate material choices. Its \nresponses are logically coherent, with reasonable explana-\ntions of the mechanisms behind the selected materials. As \nillustrated in Fig.  5, CataLM demonstrates its ability to \naccurately interpret control methods and offer reasonable \nsuggestions, such as the composite system of Cu and car -\nbon, along with clear explanations for the choice of carbon \nmaterials. In contrast, the original LLM’s misunderstanding \nof the control method is evident in its incorrect suggestion of \nPt, which is not a catalyst for CO2 reduction. Additionally, \nwe investigated the impact of hyperparameter selection on \nthe responses generated by CataLM. All CataLMs produced \nreasonable answers, albeit with varying emphases, similar to \nhow different researchers might provide distinct academic \nperspectives on the same research question.\nFor the quantitative analysis in recommendation, we \nchoose Faraday efficiency as a measure of the effectiveness \nof the regulation methods. This index refers to the percent-\nage of actual product and theoretical product, which is as \nimportant as the onset potential and the total current density \nthat can be reached at a certain potential. The specific calcu-\nlation formula is as follows:\nTable 4  Results of ablation \nexperiment Model Correct Modified \ncorrect\nRecall Precision F1\nOriginal LLM + Zero shot 27 59 40.23% 36.05% 36.88%\nOriginal LLM + Few shot 37 66 42.04% 46.25% 41.25%\nPre-training LLM + Zero shot 49 85 50.93% 71.43% 53.12%\nOur method 85 110 90.43% 67.46% 68.75%\nFig. 5  Answer from CataLM \nand original LLM\n\n3688 International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691\nTable 5  The input and answer provided by orginal LLM and CataLM \nInput Original LLM CataLM ( /u1D6FD=0.1) CataLM ( /u1D6FD=0.5)\nC2H5OH, Single metal, structure control The suitable catalyst material for the produc-\ntion of C 2H5 OH (ethanol) would be a noble \nmetal such as platinum or palladium. These \nmetals are known for their ability to catalyze \na wide range of chemical reactions and \nare often used in the production of ethanol \nthrough the hydrolysis of ethylene. As for \nthe control method type, structure control \nwould be the most appropriate\nBased on the given information, the most \nsuitable catalyst material for producing \nC 2H5 OH is Palladium (Pd). The control \nmethod that should be used is creating high-\nfacets of wrinkled Pd surrounded by PdO \nmesh patterns.\nBased on the given information, the most suit-\nable catalyst material for producing C 2H5 OH \nis Cu, and the suitable control method is \nnanostructured self-supporting Cu electrodes\nC2H4 , Metal oxide, structure control A suitable catalyst material for the production \nof C 2H4 (ethylene) would be a metal oxide \nsuch as iron oxide or copper oxide. These \nmaterials are known to be effective in the \nproduction of ethylene through the cracking \nof hydrocarbons. As for the control method, \nstructure control would be an appropriate \nmethod to use. This type of control method \ninvolves manipulating the structure of the \ncatalyst material in order to optimize its \nperformance\nBased on the information provided, the most \nsuitable catalyst material for producing \nC 2H4 is OxygenCu (OCu) combination. \nThe control method that should be used is \nanodized copper (AN-Cu) Cu(OH)(2) cata-\nlysts by a simple electrochemical synthesis \nmethod.\nBased on the information provided, the most \nsuitable catalyst material is Cu2 O, and the \ncontrol method is the selectivity and activity \nof the C 2H4 production depend strongly on \nthe crystal facets exposed in Cu2 O NPs\nHCOOH, Composites consisting of metal and \ncarbon, composite\nBased on the given information, a suitable cat-\nalyst material for the production of HCOOH \nwould be a composite consisting of metal \nand carbon. One example of such a catalyst \nis platinum-on-carbon (Pt/C). As for the \ncontrol method, a suitable method would be \nto use a feedback control system. This type \nof control system uses a sensor to measure \nthe concentration of HCOOH in the reaction \nmixture, and adjusts the reaction conditions \nin real-time to maintain a constant concen-\ntration of HCOOH\nBased on the given information, the most \nsuitable catalyst material for the synthesis of \nHCOOH would be a composite of Copper \nnanoparticles embedded in Nitrogen-doped \ngraphene oxide (Cu/N-GO). The control \nmethod used in this composite would be the \ninteraction between the copper nanoparticles \nand the nitrogen-doped graphene oxide sup-\nport. This interaction is expected to enhance \nthe electrocatalytic reduction of CO2 to \nHCOOH, due to the synergistic effects \nbetween the copper nanoparticles and the \nnitrogen-doped\nBased on the given information, the most \nsuitable catalyst material for the synthesis of \nHCOOH is Cu@CuxO Nanoparticles Deco-\nrated on 3D Vertical Graphene, and the con-\ntrol method is The improved CO2 reduction \nperformance of Cu/VG-Ar originates from \nthe well-dispersed Cu@CuxO nanoparticles \ndeposited on the defective VG-Ar\nCO, Alloys/composites of two or more metals, \nalloy\nA suitable catalyst material for producing \nCO (carbon monoxide) from an alloy or \ncomposite of two or more metals would be \na metal oxide such as iron oxide or cobalt \noxide. As for the control method, a suitable \noption would be to use a feedback control \nsystem\nBased on the given information, the most \nsuitable catalyst material for the target \nproduct CO is Gold-Copper alloy. The \ncontrol method for this catalyst material is \nalso Alloy\nBased on the given information, the most suit-\nable catalyst material for the target product \nCO is Copper-Indium (CuIn) and the control \nmethod is alloy\n3689International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691 \nwhere m is the actual mole number of the product, n  is the \nnumber of reaction electrons, f is the Faraday constant, that \nis, the electric quantity contained in one mole of electrons, \nI is the current, t  is the time. In order to quickly and auto-\nmatically evaluate this indicator, we use the deep learning \nprediction model based on semantic information (word \nembedding) and correlation of knowledge triples (graph \nembedding), which is proposed in work [44]. This method \nwas trained by 533 Faraday efficiency data samples, with \nthe form as [data: (material, product, method, method type, \nmaterial type), label: Faraday Efficiency], and outperformed \nother prediction models on Faraday Efficiency prediction. \nThe evaluation result is shown in Table  6.\nFrom the result, we can observe that the control method \ndetails generate by CataLM are obtain better performance \nthan original LLM. However, beside the target product and \nmaterial, the prediction results of the model are also influ-\nenced by the category of regulatory methods, which is an \ninput parameter. This means that for the original LLM and \nCataLM, the regulatory methods are the same, so the differ-\nence in the predicted Faraday effect is not significant.\n5  Conclusion\nIn this paper, we introduce CataLM , an effective attempt \ntowards catalyst design leveraging the capabilities of large \nlanguage models. By undergoing domain pre-training and \ninstruction tuning, our large language model has exhib-\nited robust comprehension and reasoning skills in catalyst \nknowledge and patterns, achieving advanced performance \nin application tasks like knowledge extraction and recom-\nmendation of control methods. We have open sourced the \nCataLM model and fine-tuning data to facilitate further \nexpansion and development by interested researchers, which \nis available at https:// github. com/ kg4sci/ CataLM. The result \nof NER task is available at Science Data Bank (ScienceDB), \nwhich is a public, general-purpose data repository aiming \nto provide data services for researchers, research projects/\nteams, journals, institutions, universities, etc, the link is \nhttps:// www. scidb. cn/ en/ detail? dataS etId= 3f620 4bc48 704fa \nc9b64 b8e95 a904e 02.\n(1)FE = m × n × f\nI × t\nCurrently, CataLM focuses more on catalyst design in the \nfield of carbon dioxide reduction due to limitations in the \nscope of field literature. In the future, while continuously \nenhancing the field understanding ability of CataLM by \nincorporating more domain literature, we will also design \nand develop an auxiliary platform for field researchers based \non it, in order to improve the efficiency of catalyst design \nwork in practical applications. We believe that large lan-\nguage models will bring new and infinite possibilities to \nbasic scientific research.\nAcknowledgements This work was supported by the National Key \nR&D Program of China under Grant No.2022YFF0711900 and \n2022YFF0712200, the Natural Science Foundation of China under \nGrant No. T2322027, Youth Innovation Promotion Association CAS. \nWe thank all the domain researchers that help to annotate and verify \nthe corpus.\nAuthor Contributions All authors contributed substantively to the work \npresented in this paper. Conception and Supervision: W. Cui, Y. Gao; \nData acquisition: L. Wang, X. Chen, Y. Du; Data validation: Y. Gao, \nY. Zhou, W. Cui; Technical validation: L. Wang, X. Chen; Dataset \nmining: Y. Gao, W. Cui; Writing and Proof reading: L. Wang, Y. Gao, \nX. Chen, W. Cui, Y. Du.\nData Availibility No datasets were generated or analysed during the \ncurrent study.\nDeclarations \nConflict of interest The authors declare no Conflict of interest.\nOpen Access This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit \nto the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material \nderived from this article or parts of it. The images or other third party \nmaterial in this article are included in the article’s Creative Commons \nlicence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons licence and \nyour intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http://crea-\ntivecommons.org/licenses/by-nc-nd/4.0/.\nReferences\n 1. De Luna P, Hahn C, Higgins D, Jaffer SA, Jaramillo TF, Sar -\ngent EH (2019) What would it take for renewably powered \nelectrosynthesis to displace petrochemical processes? Science \n364(6438):3506\n 2. Seh ZW, Kibsgaard J, Dickens CF, Chorkendorff I, Nørskov JK, \nJaramillo TF (2017) Combining theory and experiment in electro-\ncatalysis: Insights into materials design. Science 355(6321):4998\n 3. Nørskov JK, Bligaard T, Rossmeisl J, Christensen CH (2009) \nTowards the computational design of solid catalysts. Nat Chem \n1(1):37–46\nTable 6  Results of quantitative analysis\nModel Average predicted \nfaraday efficiency\nOriginal LLM 61.40%\nCataLM 64.15%\n3690 International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691\n 4. Suntivich J, May KJ, Gasteiger HA, Goodenough JB, Shao-\nHorn Y (2011) A perovskite oxide optimized for oxygen evo-\nlution catalysis from molecular orbital principles. Science \n334(6061):1383–1385\n 5. Liu J, Liu H, Chen H, Du X, Zhang B, Hong Z, Sun S, Wang \nW (2020) Progress and challenges toward the rational design of \noxygen electrocatalysts based on a descriptor approach. Adv Sci \n7(1):1901614\n 6. Devlin J, Chang M-W, Lee K, Toutanova K (2018) Bert: Pre-train-\ning of deep bidirectional transformers for language understanding. \narXiv preprint arXiv: 1810. 04805\n 7. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, Levy O, Lewis M, \nZettlemoyer L, Stoyanov V (2019) Roberta: A robustly optimized \nbert pretraining approach. arXiv preprint arXiv: 1907. 11692\n 8. Clark K, Luong M-T, Le QV (2020) Manning, CD Electra: Pre-\ntraining text encoders as discriminators rather than generators. \narXiv preprint arXiv: 2003. 10555\n 9. Radford A, Narasimhan K, Salimans T, Sutskever I et al (2018) \nImproving language understanding by generative pre-training\n 10. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I et al \n(2019) Language models are unsupervised multitask learners. \nOpenAI Blog 1(8):9\n 11. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal \nP, Neelakantan A, Shyam P, Sastry G, Askell A et al (2020) \nLanguage models are few-shot learners. In: Advances in neural \ninformation processing systems vol. 33, p. 1877–1901\n 12. OpenAI R (2023) Gpt-4 technical report. ArXiv: 2303. 08774\n 13. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, \nZhou Y, Li W, Liu PJ (2020) Exploring the limits of transfer \nlearning with a unified text-to-text transformer. J Mach Learn \nRes 21(140):1–67\n 14. Gao L, Biderman S, Black S, Golding L, Hoppe T, Foster C, \nPhang J, He H, Thite A, Nabeshima N et al (2020) The pile: \nAn 800gb dataset of diverse text for language modeling. arXiv \npreprint arXiv: 2101. 00027\n 15. Luo R, Sun L, Xia Y, Qin T, Zhang S, Poon H, Liu T-Y (2022) \nBiogpt: generative pre-trained transformer for biomedical text \ngeneration and mining. Brief Bioinformatics 23(6):409\n 16. Zhang H, Chen J, Jiang F, Yu F, Chen Z, Li J, Chen G, Wu X, \nZhang Z, Xiao Q et al (2023) Huatuogpt, towards taming lan-\nguage model to be a doctor. arXiv preprint arXiv: 2305. 15075\n 17. Xiong H, Wang S, Zhu Y, Zhao Z, Liu Y, Huang L, Wang Q, \nShen D (2023) Doctorglm: Fine-tuning your chinese doctor is \nnot a herculean task. arXiv preprint arXiv: 2304. 01097\n 18. Xie F, Lu T, Yu Z, Wang Y, Wang Z, Meng S, Liu M (2023) \nLu-h-n phase diagram from first-principles calculations. Chin \nPhys Lett 40(5):057401\n 19. Saal JE, Kirklin S, Aykol M, Meredig B, Wolverton C (2013) \nMaterials design and discovery with high-throughput density \nfunctional theory: the open quantum materials database (oqmd). \nJom 65:1501–1509\n 20. Jain A, Ong SP, Hautier G, Chen W, Richards WD, Dacek S, \nCholia S, Gunter D, Skinner D, Ceder G et al (2013) Commen-\ntary: The materials project: A materials genome approach to \naccelerating materials innovation. APL Mater 1(1):011002\n 21. Liang Y, Chen M, Wang Y, Jia H, Lu T, Xie F, Cai G, Wang Z, \nMeng S, Liu M (2023) A universal model for accurately predict-\ning the formation energy of inorganic compounds. Sci China \nMater 66(1):343–351\n 22. Liu Z, Guo J, Chen Z, Wang Z, Sun Z, Li X, Wang Y (2022) \nSwarm intelligence for new materials. Comput Mater Sci \n214:111699\n 23. Guo J, Chen Z, Liu Z, Li X, Xie Z, Wang Z, Wang Y (2022) \nNeural network training method for materials science based on \nmulti-source databases. Sci Reports 12(1):15326\n 24. Gupta T, Zaki M, Krishnan NA, Mausam (2022) Matscibert: A \nmaterials domain language model for text mining and informa-\ntion extraction. NPJ Comput Mater 8(1):102\n 25. Chen Z-Y, Xie F-K, Wan M, Yuan Y, Liu M, Wang Z-G, Meng \nS, Wang Y-G (2023) Matchat: a large language model and \napplication service platform for materials science. Chin Phys \nB 32(11):118104. https:// doi. org/ 10. 1088/ 1674- 1056/ ad04cb\n 26. Xie T, Wan Y, Huang W, Zhou Y, Liu Y, Linghu Q, Wang S, \nnCG, Zhang W, Hoex B (2023) Large language models as mas-\nter key: unlocking the secrets of materials science with GPT \narXiv: 2304. 02213\n 27. Zheng Z, Zhang O, Borgs C, Chayes JT, Yaghi OM (2023) Chat-\ngpt chemistry assistant for text mining and the prediction of mof \nsynthesis. J Am Chem Soc 145(32):18048–18062. https:// doi.  \norg/ 10. 1021/ jacs. 3c058 19\n 28. Cao H, Liu Z, Lu X, Yao Y, Li Y (2023) InstructMol: multi-\nmodal integration for building a versatile and reliable molecular \nassistant in drug discovery. arXiv preprint arXiv: 2311. 16208\n 29. Barroso-Luque L, Shuaibi M, Fu X, Wood BM, Dzamba M, Gao \nM, Rizvi A, Zitnick CL, Ulissi ZW (2024) Open materials 2024 \n(omat24) inorganic materials dataset and models. arXiv preprint \narXiv: 2410. 12771\n 30. Leong SX, Pablo-García S, Zhang Z, Aspuru-Guzik A (2024) \nAutomated electrosynthesis reaction mining with multimodal \nlarge language models (mllms). Chemical Science. arXiv pre-\nprint arXiv: 2311. 16208\n 31. Kulmanov M, Guzmán-Vega FJ, Duek Roggli P, Lane L, Arold \nST, Hoehndorf R (2024) Protein function prediction as approxi -\nmate semantic entailment. Nat Mach Intell 6(2):220–228\n 32. Qian J, Jin Z, Zhang Q, Cai G, Liu B (2024) A liver cancer \nquestion-answering system based on next-generation intelli-\ngence and the large model med-palm 2. Int J Comput Sci Inf \nTechnol 2(1):28–35\n 33. Wu C, Zhang X, Zhang Y, Wang Y, Xie W (2023) Pmc-llama: \nFurther finetuning llama on medical papers. arXiv preprint \narXiv: 2304. 14454\n 34. Chen B, Cheng X, Li P, Geng Y-a, Gong J, Li S, Bei Z, Tan X, \nWang B, Zeng X et al (2024) xtrimopglm: unified 100b-scale \npre-trained transformer for deciphering the language of protein. \narXiv preprint arXiv: 2401. 06199\n 35. Liu R, McKie J (2018) PyMuPDF. May. http:// pymup df. readt  \nhedocs. io/ en/ latest/\n 36. Beltagy I, Lo K, Cohan A (2019) Scibert: A pretrained language \nmodel for scientific text. arXiv preprint arXiv: 1903. 10676\n 37. Wang Y, Kordi Y, Mishra S, Liu A, Smith NA, Khashabi D, \nHajishirzi H (2022) Self-instruct: Aligning language models \nwith self-generated instructions. arXiv preprint arXiv: 2212.  \n10560\n 38. Qin Y, Liang S, Ye Y, Zhu K, Yan L, Lu Y, Lin Y, Cong X, \nTang X, Qian B et al (2023) Toolllm: Facilitating large language \nmodels to master 16000+ real-world apis. arXiv preprint arXiv:  \n2307. 16789\n 39. Wang L, Gao Y, Chen X, Cui W, Zhou Y, Luo X, Xu S, Du Y, \nWang B (2023) A corpus of co2 electrocatalytic reduction pro -\ncess extracted from the scientific literature. Sci Data 10(1):175\n 40. Du Y, Wang L, Huang M, Song D, Cui W, Zhou Y (2023) Auto-\ndive: An integrated onsite scientific literature annotation tool. \nIn: Proceedings of the 61st Annual Meeting of the Association \nfor Computational Linguistics (Volume 3: System Demonstra-\ntions), pp. 76–85\n 41. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, Wang L, \nChen W (2021) LoRA: low-rank adaptation of large language \nmodels. arXiv preprint arXiv: 2106. 09685\n 42. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, \nCistac P, Rault T, Louf R, Funtowicz M, Davison J, Shleifer \nS, Platen P, Ma C, Jernite Y, Plu J, Xu C, Scao TL, Gugger S, \n3691International Journal of Machine Learning and Cybernetics (2025) 16:3681–3691 \nDrame M, Lhoest Q, Rush AM (2020) Transformers: State-\nof-the-art natural language processing. In: Proceedings of the \n2020 Conference on Empirical Methods in Natural Language \nProcessing: System Demonstrations, pp. 38–45. Association for \nComputational Linguistics, Online . https:// www. aclweb. org/  \nantho logy/ 2020. emnlp- demos.6\n 43. Wang X, Hu V, Song X, Garg S, Xiao J, Han J (2021) Chemner: \nFine-grained chemistry named entity recognition with ontology-\nguided distant supervision. In: Proceedings of the 2021 Confer -\nence on Empirical Methods in Natural Language Processing\n 44. Gao Y, Wang L, Chen X, Du Y, Wang B (2023) Revisiting elec-\ntrocatalyst design by a knowledge graph of cu-based catalysts \nfor co 2 reduction. ACS Catal 13:8525–8534. https:// doi. org/ 10. \n1021/ acsca tal. 3c007 59\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computational intelligence",
  "concepts": [
    {
      "name": "Computational intelligence",
      "score": 0.6879251003265381
    },
    {
      "name": "Computer science",
      "score": 0.5135617256164551
    },
    {
      "name": "Catalysis",
      "score": 0.4321352243423462
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31261855363845825
    },
    {
      "name": "Chemistry",
      "score": 0.11280074715614319
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210108629",
      "name": "Computer Network Information Center",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210141483",
      "name": "National Center for Nanoscience and Technology",
      "country": "CN"
    }
  ]
}