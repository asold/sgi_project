{
  "title": "Large Language Model–Based Assessment of Clinical Reasoning Documentation in the Electronic Health Record Across Two Institutions: Development and Validation Study",
  "url": "https://openalex.org/W4407922161",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2921240349",
      "name": "Verity Schaye",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4280892323",
      "name": "David DiTullio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3202625345",
      "name": "Benedict Vincent Guzman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2944807781",
      "name": "Scott Vennemeyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114478386",
      "name": "Hanniel Shih",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3092455046",
      "name": "Ilan Reinstein",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3198224201",
      "name": "Danielle E. Weber",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114478387",
      "name": "Abbie Goodman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2274171929",
      "name": "Danny T. Y. Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2638126210",
      "name": "Daniel J. Sartori",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1240991458",
      "name": "Sally A. Santen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2789883384",
      "name": "Larry Gruppen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1850176264",
      "name": "Yindalon Aphinyanaphongs",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222393872",
      "name": "Jesse Burk-Rafel",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2168490582",
    "https://openalex.org/W2606047652",
    "https://openalex.org/W2029907219",
    "https://openalex.org/W2155128311",
    "https://openalex.org/W3159307263",
    "https://openalex.org/W1978236494",
    "https://openalex.org/W2769809650",
    "https://openalex.org/W2795594780",
    "https://openalex.org/W2937037822",
    "https://openalex.org/W2885740121",
    "https://openalex.org/W4385381951",
    "https://openalex.org/W4390546628",
    "https://openalex.org/W2906460342",
    "https://openalex.org/W3131346895",
    "https://openalex.org/W3048489732",
    "https://openalex.org/W2967778248",
    "https://openalex.org/W4283015839",
    "https://openalex.org/W4394879936",
    "https://openalex.org/W4394610413",
    "https://openalex.org/W4386347795",
    "https://openalex.org/W4403813762",
    "https://openalex.org/W4380730209",
    "https://openalex.org/W4393386222",
    "https://openalex.org/W4384561103",
    "https://openalex.org/W4395961747",
    "https://openalex.org/W4405228616",
    "https://openalex.org/W4404759579",
    "https://openalex.org/W4391324456",
    "https://openalex.org/W4406866020",
    "https://openalex.org/W4400324908",
    "https://openalex.org/W4387930849",
    "https://openalex.org/W2107431862",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W2915623326",
    "https://openalex.org/W2135192531",
    "https://openalex.org/W2767106145",
    "https://openalex.org/W4386305978",
    "https://openalex.org/W2160987310",
    "https://openalex.org/W1034374084",
    "https://openalex.org/W2768488789",
    "https://openalex.org/W1980276147",
    "https://openalex.org/W3135890563",
    "https://openalex.org/W2743176753",
    "https://openalex.org/W4397003497",
    "https://openalex.org/W3106224367",
    "https://openalex.org/W4252492177"
  ],
  "abstract": "Background Clinical reasoning (CR) is an essential skill; yet, physicians often receive limited feedback. Artificial intelligence holds promise to fill this gap. Objective We report the development of named entity recognition (NER), logic-based and large language model (LLM)–based assessments of CR documentation in the electronic health record across 2 institutions (New York University Grossman School of Medicine [NYU] and University of Cincinnati College of Medicine [UC]). Methods The note corpus consisted of internal medicine resident admission notes (retrospective set: July 2020-December 2021, n=700 NYU and 450 UC notes and prospective validation set: July 2023-December 2023, n=155 NYU and 92 UC notes). Clinicians rated CR documentation quality in each note using a previously validated tool (Revised-IDEA), on 3-point scales across 2 domains: differential diagnosis (D0, D1, and D2) and explanation of reasoning, (EA0, EA1, and EA2). At NYU, the retrospective set was annotated for NER for 5 entities (diagnosis, diagnostic category, prioritization of diagnosis language, data, and linkage terms). Models were developed using different artificial intelligence approaches, including NER, logic-based model: a large word vector model (scispaCy en_core_sci_lg) with model weights adjusted with backpropagation from annotations, developed at NYU with external validation at UC, NYUTron LLM: an NYU internal 110 million parameter LLM pretrained on 7.25 million clinical notes, only validated at NYU, and GatorTron LLM: an open source 345 million parameter LLM pretrained on 82 billion words of clinical text, fined tuned on NYU retrospective sets, then externally validated and further fine-tuned at UC. Model performance was assessed in the prospective sets with F1-scores for the NER, logic-based model and area under the receiver operating characteristic curve (AUROC) and area under the precision-recall curve (AUPRC) for the LLMs. Results At NYU, the NYUTron LLM performed best: the D0 and D2 models had AUROC/AUPRC 0.87/0.79 and 0.89/0.86, respectively. The D1, EA0, and EA1 models had insufficient performance for implementation (AUROC range 0.57-0.80, AUPRC range 0.33-0.63). For the D1 classification, the approach pivoted to a stepwise approach taking advantage of the more performant D0 and D2 models. For the EA model, the approach pivoted to a binary EA2 model (ie, EA2 vs not EA2) with excellent performance, AUROC/AUPRC 0.85/ 0.80. At UC, the NER, D-logic–based model was the best performing D model (F1-scores 0.80, 0.74, and 0.80 for D0, D1, D2, respectively. The GatorTron LLM performed best for EA2 scores AUROC/AUPRC 0.75/ 0.69. Conclusions This is the first multi-institutional study to apply LLMs for assessing CR documentation in the electronic health record. Such tools can enhance feedback on CR. Lessons learned by implementing these models at distinct institutions support the generalizability of this approach.",
  "full_text": null,
  "topic": "Preprint",
  "concepts": [
    {
      "name": "Preprint",
      "score": 0.7558925747871399
    },
    {
      "name": "Electronic health record",
      "score": 0.7219310402870178
    },
    {
      "name": "Documentation",
      "score": 0.7202231884002686
    },
    {
      "name": "Health records",
      "score": 0.4828256070613861
    },
    {
      "name": "Computer science",
      "score": 0.4010865390300751
    },
    {
      "name": "Data science",
      "score": 0.35005655884742737
    },
    {
      "name": "Natural language processing",
      "score": 0.32582172751426697
    },
    {
      "name": "Psychology",
      "score": 0.32213664054870605
    },
    {
      "name": "World Wide Web",
      "score": 0.29084059596061707
    },
    {
      "name": "Health care",
      "score": 0.26057168841362
    },
    {
      "name": "Programming language",
      "score": 0.10681384801864624
    },
    {
      "name": "Economic growth",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}