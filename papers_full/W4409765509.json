{
  "title": "Uncertainty quantification for neural network potential foundation models",
  "url": "https://openalex.org/W4409765509",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A1992793865",
      "name": "Jenna A. Bilbrey",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": null,
      "name": "Jesun S. Firoz",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2114690160",
      "name": "Mal Soon Lee",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2153547168",
      "name": "Sutanay Choudhury",
      "affiliations": [
        "Pacific Northwest National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1992793865",
      "name": "Jenna A. Bilbrey",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jesun S. Firoz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114690160",
      "name": "Mal Soon Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153547168",
      "name": "Sutanay Choudhury",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4205426733",
    "https://openalex.org/W4312043350",
    "https://openalex.org/W3186459065",
    "https://openalex.org/W2785813126",
    "https://openalex.org/W3037608693",
    "https://openalex.org/W3033994814",
    "https://openalex.org/W4323320070",
    "https://openalex.org/W3049695724",
    "https://openalex.org/W4367174359",
    "https://openalex.org/W4362522009",
    "https://openalex.org/W3185456481",
    "https://openalex.org/W4389831747",
    "https://openalex.org/W4386837007",
    "https://openalex.org/W4377221266",
    "https://openalex.org/W3183048323",
    "https://openalex.org/W4206902561",
    "https://openalex.org/W4405883509",
    "https://openalex.org/W3035688557",
    "https://openalex.org/W3183869552",
    "https://openalex.org/W3157278045",
    "https://openalex.org/W4389486201",
    "https://openalex.org/W2541404351",
    "https://openalex.org/W3035839386",
    "https://openalex.org/W4392553098",
    "https://openalex.org/W4400734331",
    "https://openalex.org/W4310135808",
    "https://openalex.org/W3093999435",
    "https://openalex.org/W4283809948",
    "https://openalex.org/W4321085451",
    "https://openalex.org/W4400526657",
    "https://openalex.org/W4401233871",
    "https://openalex.org/W4400266871",
    "https://openalex.org/W3102100346",
    "https://openalex.org/W4367048790",
    "https://openalex.org/W4241996101",
    "https://openalex.org/W2480019932",
    "https://openalex.org/W2096904991",
    "https://openalex.org/W2124934380",
    "https://openalex.org/W3084211373",
    "https://openalex.org/W3102400420",
    "https://openalex.org/W4226242928",
    "https://openalex.org/W3141471971",
    "https://openalex.org/W3164731060"
  ],
  "abstract": null,
  "full_text": "npj |computational materials Article\nPublished in partnership with the Shanghai Institute of Ceramics of the Chinese Academy of Sciences\nhttps://doi.org/10.1038/s41524-025-01572-y\nUncertainty quantiﬁcation for neural\nnetwork potential foundation models\nCheck for updates\nJenna A. Bilbrey1 , Jesun S. Firoz2,M a l - S o o nL e e3 & Sutanay Choudhury4\nFor neural network potentials (NNPs) to gain widespread use, researchers must be able to trust model\noutputs. However, the blackbox nature of neural networks and their inherent stochasticity are often\ndeterrents, especially for foundation models trained over broad swaths of chemical space. Uncertainty\ninformation provided at the time of prediction can help reduce aversion to NNPs. In this work, we detail\ntwo uncertainty quantiﬁcation (UQ) methods. Readout ensembling, byﬁnetuning the readout layers of\nan ensemble of foundation models, provides information about model uncertainty, while quantile\nregression, by replacing point predictions with distributional predictions, provides information about\nuncertainty within the underlying training data. We demonstrate our approach with the MACE-MP-0\nmodel, applying UQ to the foundation model and a series ofﬁnetuned models. The uncertainties\nproduced by the readout ensemble and quantile methods are demonstrated to be distinct measures\nby which the quality of the NNP output can be judged.\nNeural network potentials (NNPs) are a class of machine learning intera-\ntomic potentials (MLIPs) trained to approximate the energy landscape of\natomic systems in order to drive atomistic simulations. Speciﬁcally, NNPs\nmodel the relationship between the atomic conﬁguration and associated\nsystem energy and atomic forces. When well-trained and used in-domain,\nNNPs combine the accuracy of quantum mechanical methods with the\nefﬁciency of classical potentials\n1– 4. In practice, distinguishing in-domain\nfrom out-of-domain structures is challenging. Out-of-domain structures\ncan easily be generated during the course of a simulation begun from an in-\ndomain sample. Errors on these new out-of-domain structures can com-\npound over the course of the simulation, leading to inaccurate probability\ndistributions, incorrect observables, oreven unphysical results. This effect is\nespecially pronounced in cases where errors lead to the creation of artiﬁcial\nattractive forces\n5.\nUncertainty quantiﬁcation (UQ) is used to identify poorly learned or\nout-of-domain structures for active learning, with model ensembling being\na popular technique. In this approach, a set of NNPs are independently\ntrained using a common dataset but different initializations and/or network\narchitectures. A variety of methods for calculating the uncertainty have been\ndemonstrated\n6– 9, typically involving the standard deviation of ensemble\npredictions. Because of the computational expense of training NNPs,\nensembles are often limited to 5– 10 independent models.\nSingle-model UQ techniques havebeen explored to reduce the com-\nputational expense of ensembling. Wenet al. developed a NNP architecture\nthat incorporated dropout-based uncertainty10. Zhu et al. used a Gaussian\nmixture model11, while Thaler et al. applied a Bayesian method12 to estimate\nthe uncertainty of a single NNP. Soleimany et al.13 implemented evidential\ndeep learning for molecular property prediction, which has since been\nextended to NNPs14– 16.I nas i m i l a rf a s h i o n ,B u s ke ta l .17 and Carrete et al.18\ncoupled a pretrained model head and a nonlinear scaling function to attach a\nvariance to the energy contribution from each atom, which were then\nsummed to produce the sample uncertainty.\nDebate is ongoing as to the technique that best measures uncertainty in\nneural networks\n19. In an examination of the quality of ensemble-based\nuncertainty estimates, Kahle et al. observed that ensembles tended to\nunderestimate uncertainty and suggested that the ideal ensembling tech-\nnique must be optimized for each dataset and network architecture\n20.\nConversely, Tan et al. claimed that ensembling leads to more generalizable\nand robust NNPs than single-model uncertainty techniques15. For single-\nmodel NNP active learning, Thomas-Mitchell et al. found that uncertainties\nfrom Gaussian processes are not reliable, even after post-hoc calibration,\nand advocated the use of a student-t process21. Meanwhile, Dai et al. per-\nformed a broad examination of existing UQ methods for atomistic machine\nlearning approaches and found that inmany cases predicted uncertainties\ndo not match well with the observed errors22.\nFurther confounding the issue, the dataset used to train the NNP can\nalso contain inherent uncertainty. In classical molecular dynamics (MD)\nsimulations, stochastic uncertaintyarises from the chaotic nature of MD\nand the extreme sensitivity of Newtonian dynamics to initial conditions23,24.\nDensity functional theory (DFT), which is used to collect the vast majority of\n1AI & Data Analytics, Paciﬁc Northwest National Laboratory, Richland, WA, USA.2Advanced Computing, Mathematics, & Data, Paciﬁc Northwest National\nLaboratory, Seattle, WA, USA.3Chemical Physics & Analysis, Paciﬁc Northwest National Laboratory, Richland, WA, USA.4Advanced Computing, Mathematics, &\nData, Paciﬁc Northwest National Laboratory, Richland, WA, USA. e-mail: jenna.pope@pnnl.gov; sutanay.choudhury@pnnl.gov\nnpj Computational Materials|          (2025) 11:109 1\n1234567890():,;\n1234567890():,;\nNNP training data, introduces energyﬂuctuations that are dependent on the\nexchange-correlation functional25. For higher levels of theory, statistical\nnoise results from convergence criteria, among other subtle computational\nchoices26.\nIn an effort to improve the generalizablility of NNPs and potentially\nreduce epistemic uncertainties arising from poorly approximated energy\nlandscapes, NNP researchers have begun to produce foundation models.\nFoundation models are trained over large, structurally diverse datasets, often\nat signiﬁcant computational cost, to capture general relationships present in\nthe data. Such models can then be adapted to speciﬁc applications through\nﬁnetuning with less data and at reduced computational cost.\nDevelopers of the ANI-1 architecture\n27,28 have recently explored its use\nas a foundation model for condensed phase reactive chemistry of structures\ncontaining H, C, N, and/or O\n29 and for drug-like molecules30.F o u n d a t i o n\nmodels for solid-phase materials have been produced for the CHGNet31,\nMACE32,a n dM 3 G N e t33 architectures using the Materials Project Trajec-\ntory (MPtrj) Dataset, which contains 1.6M materials and spans 89 elements.\nThe Open Catalyst Project has developed foundation models for solid-phase\ncatalysis using an open database of > 10M structures\n34– 36.\nDespite the success of NNP foundation models to produce accurate\nenergy predictions over a broad range of structures, extension to novel\nsystems remains a challenge. Numerous assessments of current NNP\nfoundation models have noted the need forﬁnetuning when extrapolating\nto new tasks or out-of-domain atomic environments\n37– 41.T h ed i fﬁculty of\ndistinguishing out-of-domain from in-domain structures necessitates the\nneed for quantifying uncertainty during inference. More generally, if NNPs\nare to gain widespread practical use, UQ provides a way to establish trust in\nthe output of NNP-driven simulations.\nHerein, we demonstrate two UQ methods for NNP foundation\nmodels: readout ensembling and quantile regression. Each method has\nunique advantages. Ensembling is useful for identifying epistemic uncer-\ntainties, while quantile regressioncaptures aleatoric uncertainties\n42.B o t h\napproaches are applied to MACE-MP-032 to generate uncertainties for the\nfoundation model. We then demonstratetransfer to novel datasets: a high\nentropy alloy dataset with high chemical complexity43 and a highly speciﬁc\nzeolite dataset with varying numbers of water molecules inside the pores.\nWe ﬁnd that quantile regression is useful for capturing variations in che-\nmical complexity, while ensembling isuseful for capturing out-of-domain\nstructures. Weﬁnd that the ensemble is overconﬁdent in its predictions and,\nthough ensemble uncertainty tends to increase with error, the magnitude of\nuncertainty is lower than the error by orders of magnitude. Conversely, the\nquantile uncertainty more accurately reﬂects the model’sp r e d i c t i o na b i l i t y\nand tends to increase with system size.\nResults\nUncertainty quantiﬁcation for neural network potentials\nModel ensembling helps to reduce model bias and mitigate overﬁtting,\nresulting in higher accuracy predictions. For foundation models, training is\noften highly compute intensive. For instance, the MACE-MP-0 foundation\nmodels used 40– 80 NVIDIA A100 GPUs when training a single model32.\nThis high computational cost hinders the training of a full ensemble of\nmodels. To reduce computational costs and maintain the learned repre-\nsentation of the foundation model, we apply readout ensembling Fig.1.I n\nreadout ensembling, only the weights of theﬁnal readout layers are updated\nduring training. Because each model in the ensemble is initialized with the\nsame weights, stochasticity is introduced byﬁnetuning the readout layers on\ndifferent subsets of the full training set. The lower number of weights to be\nupdated and smaller dataset lead to greatly decreased computational costs,\nsuch that each model in the readout ensemble could be trained on a single\nNVIDIA P100 GPU.\nEach model in the ensemble is trained using the Huber loss function,\nwhich is a piecewise function that switches between the mean squared error\n(MSE) and mean absolute error (MAE) depending on a set threshold. The\ns y m m e t r i cn a t u r eo ft h eH u b e rl o s sf u n c t i o n– a n do fM S Ea n dM A E\nindividually – ensures that predictions higher and lower than the target\nvalue are penalized equally. Variability in predictions given by an ensemble\nof models can be standard deviation, while conﬁdence intervals (CIs) can be\ncomputed from the Student’s t-distribution.\nQuantile regression makes use of an asymmetric function that pena-\nlizes above and below the target value differently. In this way, ground truth\nquantiles are not required for training. For instance, to predict the 95th\npercentile, a penalty of 0.95 times the prediction error would be given when\nthe prediction is higher than the target value and a penalty of 0.05 would be\ngiven when the prediction is lower. The asymmetric penalization causes the\nprediction, after further training epochs, to move towards the desired\nquantile. To form uncertainty bounds using quantile regression, we modify\nthe network architecture to have two readout layers with opposite penali-\nzation (Fig.1): one readout is penalized by 0.95 and 0.05 for higher and lower\npredictions, while the other readout ispenalized 0.05 and 0.95, respectively.\nThis architecture produces two predictions targeted at the 95th and 5th\npercentiles, respectively. The difference between these predictions gives the\n90% CI.\nThough both ensembling and quantiler e g r e s s i o nc a np r o d u c eC I s ,t h e\nmethods apply different statistical assumptions. Ensembling approximates\nthe model posterior, while quantile regression approximates the conditional\ndistribution\n44,45.W i t hi nﬁnite data and a perfect model, the model posterior\nuncertainty would vanish, but the conditional distribution would still exist.\nThese different assumptions lead to different types of uncertainty. Quantile\nregression captures aleatoric uncertainty in the training data distribution,\nwhile ensembling captures both epistemic uncertainty in model parameters\nand aleatoric uncertainty. Because epistemic uncertainty is captured, CIs\nderived from the ensemble are wider inregions of parameter uncertainty or\nsparse data.\nUncertainty in the MACE-MP-0 foundation model\nWe ﬁrst examine UQ for the out-of-the-box MACE-MP-0 foundation\nmodel by readout ensembling and quantile regression. Each of the 7 models\nin the readout ensemble was trainedon a unique set of 90,000 structures\nchosen at random from the MPtrj dataset, using 80,000 for training and\n10,000 for validation. The quantile model was trained on a separate set of\n90,000 MPtrj structures. All testing was performed on a common set of\n10,000 MPtrj structures. We report the errors in energy prediction and\nassociated uncertainties as per-electron values (meV/e\n−)t or e m o v es i z e\nextensive effects of DFT-calculated energies, which scale with the number of\nelectrons. Such scaling enables comparison among the wide range of\nstructures contained in the MPtrj dataset. Alternative units for the values\nreported below are given in Table S1 in the Supplementary Information.\nThe readout ensemble and quantile model give similar values for the\nmean absolute error (MAE) in energy prediction on the MPtrj test set of\n0.721 and 0.890 meV/e\n−, respectively. It should be noted that these errors are\nin line with those reported for MACE-MP-0 afterﬁnetuning the‘small’\nmodel for 50 additional epochs with higher weighting of the energy com-\nponent of the loss function\n32. The pretrained MACE-MP-0 withoutﬁne-\ntuning gives a test set error of 0.739 meV/e−, which is equivalent to the MAE\nof 13 meV/atom reported by Batatia et al. The MAEs of the readout\nensemble and quantile model translate to 13 meV/atom and 16 meV/atom,\nrespectively. Therefore, we can claim that our models are well-trained on the\nMPtrj dataset.\nA ss h o w ni nT a b l e1, the mean uncertainty of the quantile model\n(1.391 meV/e\n−) is over an order of magnitude higher than that of the\nreadout ensemble (0.036 meV/e−). We evaluate the quality of the estimated\nuncertainties in two ways, shown in Fig.2. First, we calculate the coverage,\ndeﬁned here as the percent of samples in which the target value falls within\nthe 5th and 95th percentiles derived from the uncertainty. The coverage by\nthe quantile model (87%) greatly exceeds that of the readout ensemble\n(11%), which results from the larger uncertainties of the quantile model.\nNext, we examine the correlation between uncertainty and prediction error.\nIn the ideal case, high uncertainty should correspond to high prediction\nerror. Examining the uncertainty distributions of samples within speciﬁed\nMAE ranges, the uncertainty from the quantile model clearly increases with\nhttps://doi.org/10.1038/s41524-025-01572-y Article\nnpj Computational Materials|          (2025) 11:109 2\nMAE and is closer in magnitude than that from the readout ensemble. It\nshould be noted that as the mean uncertainty increases with MAE, so does\nthe spread of uncertainties. Therefore, a single uncertainty is not directly\nrepresentative of the prediction error, but in general, larger uncertainties\nindicate higher errors.\nThe wide breadth of structural space covered by MPtrj and the slight\nvariations in simulation procedures (inconsistent application of HubbardU\ncorrection, varying convergence criteria, etc.) contributes to increased\naleatoric uncertainty in the data, which is reﬂected in the quantile uncer-\ntainty. Conversely, the low readout ensemble uncertainty indicates low\nepistemic uncertainty, which reﬂects the high quality of MACE-MP-0.\nThe number of models in an ensemble will affect the resultant\nuncertainty. More models generally lead to more reliable uncertainty esti-\nmates, though with diminishing return. To examine this effect, we recal-\nculated the uncertainties using our readout ensemble models trained on the\nMPtrj data by leaving 1, 2, or 3 models out of the ensemble. We calculated\nthe uncertainty for each combination of model removal and provide sta-\ntistics in Table2. As expected, the uncertainty decreases as the number of\nmodels in the ensemble increases.\nTransfer learning to a dataset with high chemical complexity\nWe then explored UQ during transfer learning of the MACE-MP-0 foun-\ndation model. Finetuning a foundation model limits the amount of sto-\nchasticity when ensembling, speci ﬁcally in terms of randomized\ninitialization (initial weights are transferred from the foundation model)\nand dataset splitting (taking unique subsets of a small dataset would lead to\nvery small training sets). This leaves the seed controlling the random\nnumber generator, which inﬂuences shufﬂing of the training set between\nepochs and nondeterministic algorithms used by the PyTorch and cuDNN\nlibraries, as the only source of stochasticity for ourﬁnetuned foundation\nreadout ensemble.\nThe variety of elements present in each sample makes HEA25 a useful\ndataset for examining effects of chemical complexity on both UQ and model\nperformance. As shown in Table1, the readout ensemble and quantile\nmodel give similar MAEs of 0.971 and 1.013 meV/e\n−, respectively, while the\nuncertainty from the readout ensemble (0.132 meV/e−)i sm u c hl o w e rt h a n\nthat from the quantile model (1.829 meV/e−). The low uncertainty for the\nreadout ensemble indicates that the model-derived uncertainty is low (i.e.,\nthe models have all learned similar sample spaces), while the comparatively\nhigher uncertainty for the quantile model indicates the aleatoric uncertainty\nis high. Because aleatoric uncertainty reﬂects the inherent variability in the\ntraining data, we can interpret the higher uncertainty in the quantile model\nto result from the chemical complexity of the HEA25 dataset. Interestingly,\nthe quantile uncertainty roughly correlates with MAE, as shown byﬁtted\nloss curve in Fig.3.\nTransfer learning to a dataset of highly ordered conﬁgurations\nWe then examine transfer learning of the MACE-MP-0 model to a dataset\nwith highly ordered conﬁgurations: the aluminosilicate zeolite H-ZSM-5\ninﬁltrated with water. Five ab initio molecular dynamics (AIMD) simula-\ntions were performed withn =1 ,2 ,3 ,8 ,o r1 6w a t e rm o l e c u l e si nap o r e .\nSimulations withn =1 – 3 water molecules were used toﬁnetune MACE-\nMP-0, andn = 8, 16 were used as holdout sets to examine the effect of larger\nsystems on the uncertainty.\nThe training dataset is quite small (8543 samples) and has limited\nchemical complexity (4 types of elements per sample), but each sample has a\nrelatively large number of atoms (295−301). As shown in Table1,t h e\nreadout ensemble and quantile model produce similar MAEs on then =1 – 3\ntest set of 0.035 and 0.031 meV/e\n−, respectively. The high accuracy is likely a\nresult of 1) the speciﬁcity of the H-ZSM-5 dataset and 2) the sufﬁcient\nrepresentation of ZSM-5 atomic neighborhoods in the MPtrj dataset, as\ndemonstrated by the good zero-shot performance by MACE-MP-032.T h e\nﬁnetuned readout ensemble gives comparable uncertainty to the MACE-\nMP-0 readout ensemble (0.032 vs 0.036 meV/e−, respectively), while the\nquantile model shows greatly reduced uncertainty (0.056 vs 1.391 meV/e−).\nThe large reduction in uncertainty fromt h eq u a n t i l em o d e li sl i k e l yd u et o\nthe rigid structure of H-ZSM-5, which limits the conﬁgurations that can be\nsampled during MD simulations.\nTable 3 shows the MAEs and uncertainties for each subset, including\nthe holdout sets withn=8, 16 water molecules. The MAE increased almost\n6-fold whenn = 8 and 10-fold whenn = 16 for both the readout ensemble\nand quantile model. The uncertainty from both models also increases for the\nn = 8, 16 holdout sets. For the readout ensemble, all training subsets\n(n =1 – 3) have the same mean uncertainty of 0.032 meV/e\n−, while that of the\nholdout sets (n = 8, 16) increases slightly with number of water molecules.\nMeanwhile, the quantile uncertainty increases nearly linearly with number\nof water molecules across all subsets. These differing uncertainty behaviors\nfurther support the notion that ensemble uncertainty reﬂects conﬁgurations\nthat are poorly learned by the model (i.e., out-of-domain), while the quantile\nuncertainty indicates possible variability in a conﬁguration.\nFigure 4 compares the computed versus predicted total energies for\neach subset and gives uncertainty bands corresponding to the average upper\nand lower bounds. While the training subsets are centered around the ideal\nﬁt, the extrapolation subsets are clearly offset from ideal with the offset\nincreasing withn (see Table3). Table S2 and Figure S1 show the calibrated\nMAE and coverage. Note that the uncertainty does not change when cor-\nrecting for this offset. Correcting for the offset by subtracting the mean\ndifference between the predicted and computed energies for each subset\ndecreases the MAE to 0.041 meV/e\n− forn = 8 for both the readout ensemble\nand quantile model and to 0.045 and 0.044 meV/e− forn =1 6f o rt h er e a d o u t\nensemble and quantile model, respectively. As a result, the coverage of the\nTable 1 | Mean per-electron errors (meV/e−) and uncertainties\n(meV/e−) along with coverage (%) for test sets of the examined\ndatasets\nEnsemble Quantile\nDataset MAE\n(meV/\ne−)\nU\n(meV/\ne−)\nCoverage MAE\n(meV/\ne−)\nU\n(meV/\ne−)\nCoverage\nMPtrj 0.721 0.036 11% 0.890 1.391 87%\nHEA25 0.971 0.132 21% 1.013 1.829 88%\nH-ZSM-5 0.035 0.032 64% 0.031 0.056 86%\nFig. 1 | Schematic of the MACE-MP-0 readout\nensemble and quantile model.All weights from the\nMACE-MP-0 interaction head were frozen during\ntraining, and only weights from the readout layers\nwere updated.\nhttps://doi.org/10.1038/s41524-025-01572-y Article\nnpj Computational Materials|          (2025) 11:109 3\nreadout ensemble increases to 51 and 53% forn = 8, 16, and that of the\nquantile model increases to 85 and 91%, respectively.\nDiscussion\nIn this work, we demonstrated two methods for UQ in NNP foundation\nmodels: readout ensembling and quantile regression. The uncertainty\nderived from quantile regression reﬂects the chemical complexity in the\ntraining set (aleatoric uncertainty), while the uncertainty derived from\nensembling predominantly results from model training (epistemic uncer-\ntainty). Altering only the readout layers maintains the bulk of the foundation\nmodel weights, and therefore, only a small amount of training is necessary to\nincorporate uncertainty estimation into the foundation model. Notably,\nboth UQ methods can be applied to any NNP foundation model with\nminimal alteration.\nThe two methods provide distinct information about the quality of the\npredicted output. Ensemble uncertainty identiﬁes when a structure is poorly\ndescribed by the model (i.e., out-of-domain). In general, such uncertainties\ncan be improved by extending model training, expanding the training\ndataset, or moving to a new model architecture. In contrast, quantile\nuncertainty is a reﬂection of the variability in the underlying training data,\nwhich is propagated to the model’s predictions. In principle, this type of\nuncertainty cannot be improved with additional training or model archi-\ntecture choices. Altering the training dataset will have an effect, but not\nalways as expected. In fact, more complex samples (i.e., high entropy alloys)\nwere shown to increase the quantile uncertainty, even though the number of\nunique materials in the training set decreased and a common set of simu-\nlation parameters was used to generate the HEA25 dataset.\nThough in this work we examined the two methods separately, they\nc o u l db ei n t e g r a t e di n t oau n iﬁed framework to adapt to both the amount of\navailable data and the inherent variability in different regions of the input\ndata. For example, to obtain a conservative estimate of the uncertainty, the\nset of CIs could be combined by taking the maximum of the lower bounds\nand minimum of the upper bounds. In data-dense regions where the\nensemble is conﬁdent but the data distribution varies, the intervals from\nquantile regression would dominate, while the ensemble intervals would\ndominate in data-poor regions where the ensemble gives high uncertainty.\nThis is just one example of how the two methods could be combined, and\nmore research would be needed to determine the most effective way of\ncombining methods. UQ was applied only to energy predictions in this\nwork. Future work will examine applications of UQ to atomic forces, as well\nas propagation of uncertainty during NNP-driven simulations. The code-\nbase used in this work is available atgithub.com/pnnl/SNAP.\nMethods\nMACE readout ensembling\nThree different-sized models are provided under the MACE-MP-0\numbrella, labeled‘small’, ‘medium’,a n d‘large’ depending on the model\nsize32. To demonstrate proof of concept, we use the‘small’ MACE-MP-0\nmodel throughout this work (see Code Block S1 in the Supplementary\nInformation for a full accounting of the model architecture). It should be\nnoted that the‘small’ model contains approximately 3.85 million learnable\nparameters. In this instance,‘small’is in reference to the other MACE-MP-0\nmodels and should not be considered small in comparison to other NNPs.\nEach ensemble was made up of 7 models that were trained by freezing the\nFig. 2 | Uncertainty in the MACE-MP-0 founda-\ntion model for the MPtrj dataset. ARegression of\nuncertainty (U) vs absolute error (AE) of predictions\nwith the readout ensemble and quantile model on\nthe MPtrj test set. The lowess curve shows the 90%\nCI. B Coverage of the readout ensemble and quantile\nmodel. C, D Density maps of conﬁgurations con-\ntributing to theﬁtted curve in (A).\nTable 2 | Per-electron uncertainties (meV/e−) of readout\nensembles trained on the MPtrj dataset\nN Ensemble Models U (meV/e −)\nRemoved Remaining Min Mean Max\n07 – 0.036 –\n1 6 0.040 0.040 0.040\n2 5 0.045 0.046 0.046\n3 4 0.053 0.055 0.056\nhttps://doi.org/10.1038/s41524-025-01572-y Article\nnpj Computational Materials|          (2025) 11:109 4\nweights of the MACE-MP-0 interaction head and only allowing theﬁnal\nreadout module to update. For the foundation ensemble, each model was\ntrained on a unique randomly sampled 80,000 subset of the full MPtrj\ndataset using a unique seed. When transferring to new datasets, the same\ntraining set was used for each model in the ensemble, but unique seeds were\nset. The ensemble uncertainty U is deﬁned as half the length of the 90% CI of\nthe model predictions determined from the Student’s t-distribution. The\n90% CI was chosen to provide a direct comparison to the CI output from the\nquantile model.\nA learning rate of 0.001 was used with MPtrj, which was increased to\n0.01 for transfer learning. The learningrate was adaptively lowered after 10\nconsecutive epochs with no improvement in validation loss. After a mini-\nmum of 50 epochs, training was stopped after no improvement in validation\nloss was obtained over 15 consecutiveepochs. All models were trained on a\nsingle NVIDIA P100 GPU using the PyTorch Lightning framework\n46.\nQuantile regression\nWe updated the MACE-MP-0 architecture such that instead of predicting a\nsingle target output, our model predicts the 5th and 95th quantiles of the\ntarget output. This architecture change is achieved by simply duplicating the\nﬁnal readout module so that each quantile is predicted by a unique readout\nmodule. Quantile regression is then applied to simultaneously optimize the\nreadout modules. The quantile model was trained similarly to the readout\nensemble models by freezing the MACE-MP-0 interaction head and using\nthe same learning rate and early stopping procedures. The main difference\nin training is the use of a loss function speciﬁc to quantile regression.\nQuantile regression typically applies the pinball loss function to train a\nmodel to produce a probabilistic prediction of each sample in the form of\nupper and lower quantiles. See the following works for a thorough discus-\nsion of quantile regression\n47,48. Hatalis et al. showed that a smooth\napproximation of the pinball loss function, originally described by Songfeng\nZheng49, improved the performance of quantile regression on neural\nnetworks50. Though their target application was wind power forecasting, we\nﬁnd their smooth pinball loss to be useful for training NNPs. The smooth\npinball loss functionL is the sum of loss contributionsLq from the selected\nquantilesq:\nLq ¼ qðE /C0 EqÞþ 1\nα logð1 þ expðαðEq /C0 EÞÞÞ; ð1Þ\nwhere q is the optimization quantile,E is the prediction target,Eq is the\nprediction at quantileq,a n dα is a smoothing parameter. In this work we use\nq = 0.05, 0.95 as the lower and upper bounds to the energy prediction, which\ngives L ¼ L0:05 þ L0:95. During inference, the model outputs both the\nupper and lower bounds. The uncertainty U is calculated as follows:U =\n(E0.95 − E0.05)/2. Though quantile regression does not make assumptions\nabout the symmetry of the distribution, for simplicity, we take the mean of\nthe quantile predictions as the predicted target value.\nDatasets\nMPtrj. The MPtrj dataset31 contains nearly 1.6 million conﬁgurations for\n150,000 unique inorganic crystals derived from DFT static and relaxation\ntrajectories collected over a decade. MPtrj was used to train MACE-MP-0,\nand a thorough description of the MPtrj dataset can be found in the\nsupporting information of the report introducing MACE-MP-0\n32. In total,\n89 elements are represented, though individual conﬁgurations are com-\nposed of≤ 8 elements with the majority containing 3 elements. Conﬁg-\nurations range in size from 1– 444 atoms, with the vast majority (~ 97%)\nhaving < 100 atoms.\nHEA25. The HEA25 dataset from Lopanitsyna et al.43 includes 25,628\nconﬁgurations of high entropy alloys (HEAs) consisting of 36 atoms for\nbody-center cubic (bcc) lattices and 48 atoms for face-centered cubic (fcc)\nlattices with mixtures of up to 25d-block transition metals. The dis-\nordered multi-element conﬁgurations present in HEA25 give rise to a\nlarge variety of atomic neighborhoods.\nH-ZSM-5 ⋅ nH2O. We prepared a dataset composed of the H form of\nZeolite Socony Mobil-5 (H-ZSM-5), in which H+ ions occupy the zeolite\nion exchange sites. H-ZSM-5 is composed of 4 elements (H, O, Al, and Si)\nTable 3 | MAE (meV/e−), mean U (meV/e−), coverage (%), and mean offset for eachnH2O subset in the H-ZSM-5 dataset\nEnsemble Quantile\nn MAE U Coverage Offset MAE U Coverage Offset\n1 0.032 0.032 65% 0.041 0.028 0.054 89% −0.002\n2 0.035 0.032 68% 0.040 0.032 0.056 83% −0.013\n3 0.036 0.032 61% 0.049 0.032 0.057 85% −0.004\n8 0.207 0.035 0% 0.612 0.181 0.074 2% 0.537\n16 0.323 0.041 0% 0.983 0.301 0.091 0% 0.914\nFig. 3 | Uncertainty in MACE-MP-0 transfered to HEA25. AUncertainty (U) vs absolute error (AE) of predictions with the readout ensemble and quantile model on the\nHEA25 test set. The lowess curve shows the 90% CI. (B, C) Density maps of conﬁgurations contributing to theﬁtted curve in (A).\nhttps://doi.org/10.1038/s41524-025-01572-y Article\nnpj Computational Materials|          (2025) 11:109 5\nand belongs to the pentasil family of zeolites. Channels run parallel and\nperpendicular through the structure, allowing inﬁltration of solvents and\nsmall molecules within the pores. We examined the inﬁltration of 1, 2, 3,\n8, or 16 water molecules. Simulation details are provided in the Supple-\nmentary Information.\nData availability\nThe MPtrj dataset31 and HEA25 dataset43 are available online, with infor-\nmation given in their respective publications.\nCode availability\nThe codebase developed for this study is available on GitHub athttps://\ngithub.com/pnnl/SNAP.\nReceived: 16 December 2024; Accepted: 11 March 2025;\nReferences\n1. Gokcan, H. & Isayev, O. Learning molecular potentials with neural\nnetworks. Wiley Interdiscip. Rev.: Computational Mol. Sci.12, e1564\n(2022).\n2. Kocer, E., Ko, T. W. & Behler, J. Neural network potentials: A concise\noverview of methods.Annu. Rev. Phys. Chem.73, 163– 186 (2022).\n3. Duval, A. et al. A hitchhiker’s guide to geometric gnns for 3d atomic\nsystems. arXiv preprint arXiv:2312.07511(2023).\n4. Käser, S., Vazquez-Salazar, L. I., Meuwly, M. & Töpfer, K. Neural\nnetwork potentials for chemistry: concepts, applications and\nprospects. Digital Discov.2,2 8– 58 (2023).\n5. Behler, J. & Csányi, G. Machine learning potentials for extended\nsystems: a perspective.Eur. Phys. J.B94,1 – 11 (2021).\n6. Smith, J. S., Nebgen, B., Lubbers, N., Isayev, O. & Roitberg, A. E. Less\nis more: Sampling chemical space with active learning.J. Chem. Phys.\n148, 241733 (2018).\n7. Jeong, W., Yoo, D., Lee, K., Jung, J. & Han, S. Efﬁcient atomic-\nresolution uncertainty estimation for neural network potentials\nusing a replica ensemble.J. Phys. Chem. Lett.11, 6090– 6096\n(2020).\n8. Schran, C., Brezina, K. & Marsalek, O. Committee neural network\npotentials control generalization errors and enable active learning.J.\nChem. Phys153, 104105 (2020).\n9. Kulichenko, M. et al. Uncertainty-driven dynamics for active learning\nof interatomic potentials.Nat. Compu. Sci.3, 230– 239 (2023).\n10. Wen, M. & Tadmor, E. B. Uncertainty quantiﬁcation in molecular\nsimulations with dropout neural network potentials.npj Comput.\nMater. 6, 124 (2020).\n11. Zhu, A., Batzner, S., Musaelian, A. & Kozinsky, B. Fast uncertainty\nestimates in deep learning interatomic potentials.J Chem. Phys.158,\n164111 (2023).\n12. Thaler, S., Doehner, G. & Zavadlav, J. Scalable Bayesian uncertainty\nquantiﬁcation for neural network potentials: promise and pitfalls.J.\nChem. Theory Comput.19, 4520– 4532 (2023).\n13. Soleimany, A. P. et al. Evidential deep learning for guided molecular\nproperty prediction and discovery.ACS Cent. Sci.7, 1356– 1367\n(2021).\n14. Wollschläger, T., Gao, N., Charpentier, B., Ketata, M. A. &\nGünnemann, S. Uncertainty estimation for molecules: Desiderata and\nmethods. InInternational conference on machine learning,\n37133– 37156 (PMLR, 2023).\n15. Tan, A. R., Urata, S., Goldman, S., Dietschreit, J. C. & Gómez-\nBombarelli, R. Single-model uncertainty quantiﬁcation in neural\nnetwork potentials does not consistently outperform model\nensembles. npj Computational Mater.9, 225 (2023).\n16. Xu, H. et al. Evidential deep learning for interatomic potentials.arXiv\npreprint arXiv:2407.13994(2024).\n17. Busk, J., Schmidt, M. N., Winther, O., Vegge, T. & Jørgensen, P. B.\nGraph neural network interatomic potential ensembles with calibrated\nFig. 4 | Uncertainty bands for H-ZSM-5 predictions.Comparison of total energies\n(eV) from DFT and predicted by the readout ensemble and quantile model for H-\nZSM-5 inﬁltrated with n = 1, 2, 3, 8, or 16 water molecules. Markers represent\nindividual predictions, solid lines represent average upper and lower bounds based\non the uncertainty, and black dotted lines show the idealﬁt. Energies are centered\naround the mean value from DFT for each subset.\nhttps://doi.org/10.1038/s41524-025-01572-y Article\nnpj Computational Materials|          (2025) 11:109 6\naleatoric and epistemic uncertainty on energy and forces.Phys.\nChem. Chem. Phys.25, 25828– 25837 (2023).\n18. Carrete, J., Montes-Campos, H., Wanzenböck, R., Heid, E. & Madsen,\nG. K. Deep ensembles vs committees for uncertainty estimation in\nneural-network forceﬁelds: Comparison and application to active\nlearning. J. Chem. Phys.158, 204801 (2023).\n19. Gawlikowski, J. et al. A survey of uncertainty in deep neural networks.\nArtif. Intell. Rev.56, 1513– 1589 (2023).\n20. Kahle, L. & Zipoli, F. Quality of uncertainty estimates from neural\nnetwork potential ensembles.Phys. Rev. E105, 015311 (2022).\n21. Thomas-Mitchell, A., Hawe, G. & Popelier, P. L. Calibration of\nuncertainty in the active learning of machine learning forceﬁelds.\nMach. Learn.: Sci. Technol.4, 045034 (2023).\n22. Dai, J., Adhikari, S. & Wen, M. Uncertainty quantiﬁcation and\npropagation in atomistic machine learning.Rev. Chem. Eng.https://\ndoi.org/10.1515/revce-2024-0028 (2024).\n23. Wan, S., Sinclair, R. C. & Coveney, P. V. Uncertainty quantiﬁcation in\nclassical molecular dynamics.Philos. Trans. R. Soc. A379, 20200082\n(2021).\n24. Vassaux, M., Wan, S., Edeling, W. & Coveney, P. V. Ensembles are\nrequired to handle aleatoric and parametric uncertainty in molecular\ndynamics simulation.J. Chem. Theory Comput.17, 5187– 5197\n(2021).\n25. Henkel, P. & Mollenhauer, D. Uncertainty of exchange-correlation\nfunctionals in density functional theory calculations for lithium-based\nsolid electrolytes on the case study of lithium phosphorus oxynitride.\nJ. Computational Chem.42, 1283– 1295 (2021).\n26. Goswami, S., Käser, S., Bemish, R. J. & Meuwly, M. Effects of aleatoric\nand epistemic errors in reference data on the learnability and quality of\nnn-based potential energy surfaces.Artif. Intell. Chem.2, 100033\n(2024).\n27. Smith, J. S., Isayev, O. & Roitberg, A. E. Ani-1: an extensible neural\nnetwork potential with dft accuracy at forceﬁeld computational cost.\nChem. Sci.8, 3192– 3203 (2017).\n28. Gao, X., Ramezanghorbani, F., Isayev, O., Smith, J. S. & Roitberg, A. E.\nTorchani: a free and open source pytorch-based deep learning\nimplementation of the ani neural network potentials.J. Chem. Inf.\nmodeling 60, 3408– 3415 (2020).\n29. Zhang, S. et al. Exploring the frontiers of condensed-phase chemistry\nwith a general reactive machine learning potential.Nat. Chem.16,\n727– 734 (2024).\n30. Allen, A. E. et al. Learning together: Towards foundation models for\nmachine learning interatomic potentials with meta-learning.npj\nComputational Mater.10\n, 154 (2024).\n31. Deng, B. et al. Chgnet: Pretrained universal neural network potential\nfor charge-informed atomistic modeling (2023). 2302.14231.\n32. Batatia, I. et al. A foundation model for atomistic materials chemistry\n(2023). 2401.00096.\n33. Chen, C. & Ong, S. P. A universal graph deep learning interatomic\npotential for the periodic table.Nat. Computational Sci.2, 718– 728\n(2022).\n34. Chanussot, L. et al. Open catalyst 2020 (oc20) dataset and community\nchallenges. Acs Catal.11, 6059– 6072 (2021).\n35. Kolluru, A. et al. Open challenges in developing generalizable large-\nscale machine-learning models for catalyst discovery.ACS Catal.12,\n8572– 8581 (2022).\n36. Tran, R. et al. The open catalyst 2022 (oc22) dataset and challenges\nfor oxide electrocatalysts.ACS Catal.13, 3066– 3084 (2023).\n37. Focassio, B., M. Freitas, L. P. & Schleder, G. R. Performance\nassessment of universal machine learning interatomic potentials:\nchallenges and directions for materials’ surfaces. ACS Appl. Mater.\nInterfaces 17, 13111– 13121 (2024).\n38. Yu, H., Giantomassi, M., Materzanini, G., Wang, J. & Rignanese, G.-M.\nSystematic assessment of various universal machine-learning\ninteratomic potentials.Mater. Genome Eng. Adv.2, e58 (2024).\n39. Deng, B. et al. Overcoming systematic softening in universal machine\nlearning interatomic potentials byﬁne-tuning. arXiv preprint\narXiv:2405.07105 (2024).\n40. Niblett, S. P., Kourtis, P., Magdău, I.-B., Grey, C. P. & Csányi, G.\nTransferability of datasets between machine-learning interaction\npotentials. arXiv preprint arXiv:2409.05590(2024).\n41. Clausen, C. M., Rossmeisl, J. & Ulissi, Z. W. Adapting oc20-trained\nequiformerv2 models for high-entropy materials.J. Phys. Chem. C.\n128, 11190– 11195 (2024).\n42. Abdar, M. et al. A review of uncertainty quantiﬁcation in deep learning:\nTechniques, applications and challenges.Inf. fusion76, 243–297 (2021).\n43. Lopanitsyna, N., Fraux, G., Springer, M. A., De, S. & Ceriotti, M.\nModeling high-entropy transition metal alloys with alchemical\ncompression. Phys. Rev. Mater.7, 045802 (2023).\n44. Koenker, R. & Hallock, K. F. Quantile regression.J. Econ. Perspect.\n15, 143– 156 (2001).\n45. Koenker, R.Fundamentals of Quantile Regression,2 6–67. Econometric\nSociety Monographs (Cambridge University Press, 2005).\n46. Falcon, W. & The PyTorch Lightning team. PyTorch Lightning (Version\n1.4) [Computer software]. (2019).\n47. Koenker, R. Quantile regression.Cambridge Univ Pr(2005).\n48. Takeuchi, I., Le, Q. V., Sears, T. D., Smola, A. J. & Williams, C.\nNonparametric quantile estimation.J. Machine Learn. Res.7,\n1231– 1264 (2006).\n49. Zheng, S. Gradient descent algorithms for quantile regression with\nsmooth approximation.Int. J. Mach. Learn. Cybern.2,1 9 1–207 (2011).\n50. Hatalis, K., Lamadrid, A. J., Scheinberg, K. & Kishore, S. Smooth\npinball neural network for probabilistic forecasting of wind power.\narXiv preprint arXiv:1710.01720(2017).\nAcknowledgements\nThis work was supported by the \"Transferring exascale computational\nchemistry to cloud computing environment and emerging hardware\ntechnologies (TEC\n4)\" project, which is funded by the U.S. Department of\nEnergy, Ofﬁce of Science, Ofﬁce of Basic Energy Sciences, the Division of\nChemical Sciences, Geosciences, and Biosciences (under FWP 82037). The\nsimulation data used in this study was supported by the U.S. Department of\nEnergy (DOE), Ofﬁce of Science, Ofﬁce of Basic Energy Sciences, Division of\nChemical Sciences, Geosciences & Biosciences (under FWP 47319). Paciﬁc\nNorthwest National Laboratory (PNNL) is a multiprogram national laboratory\noperated for the U.S. Department of Energy (DOE) by Battelle Memorial\nInstitute under Contract No. DE-AC05-76RL0-1830.\nAuthor contributions\nJ.A.B. and S.C. conceived of the idea. J.A.B. developed and implemented\nthe method and carried out the experiments. J.S.F. curated and processed\nthe datasets. M-S.L. created the H-ZSM-5 dataset. All authors contributed\nto the interpretation of the results and writing of the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41524-025-01572-y\n.\nCorrespondenceand requests for materials should be addressed to\nJenna A. Bilbrey or Sutanay Choudhury.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nhttps://doi.org/10.1038/s41524-025-01572-y Article\nnpj Computational Materials|          (2025) 11:109 7\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© Battelle Memorial Institute 2025\nhttps://doi.org/10.1038/s41524-025-01572-y Article\nnpj Computational Materials|          (2025) 11:109 8",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7295593023300171
    },
    {
      "name": "Artificial neural network",
      "score": 0.5674720406532288
    },
    {
      "name": "Computer science",
      "score": 0.42344895005226135
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42043739557266235
    },
    {
      "name": "History",
      "score": 0.10804766416549683
    },
    {
      "name": "Archaeology",
      "score": 0.08017390966415405
    }
  ],
  "institutions": []
}