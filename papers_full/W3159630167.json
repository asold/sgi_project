{
  "title": "Contrastive Out-of-Distribution Detection for Pretrained Transformers",
  "url": "https://openalex.org/W3159630167",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2138658255",
      "name": "Zhou, Wenxuan",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2223022681",
      "name": "Liu, Fangyu",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A3167527667",
      "name": "Chen, Muhao",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4297808394",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2964201905",
    "https://openalex.org/W3104110041",
    "https://openalex.org/W3021561894",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3041609598",
    "https://openalex.org/W2740930567",
    "https://openalex.org/W2952140516",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3122924117",
    "https://openalex.org/W3092527263",
    "https://openalex.org/W3118062200",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963693742",
    "https://openalex.org/W2767414122",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4287614078",
    "https://openalex.org/W3043138801",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W3173783447",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3034230713",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2786599352",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2867167548",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W2963149653",
    "https://openalex.org/W2786808285",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2889625178",
    "https://openalex.org/W2963909453",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2963384319",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2951883849",
    "https://openalex.org/W2512924740",
    "https://openalex.org/W1493526108",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W3034781633",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2963924212",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2904981516",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W2555897561",
    "https://openalex.org/W2996179709",
    "https://openalex.org/W2890884881",
    "https://openalex.org/W4287812705",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3100345210",
    "https://openalex.org/W4287724856"
  ],
  "abstract": "Pretrained Transformers achieve remarkable performance when training and test\\ndata are from the same distribution. However, in real-world scenarios, the\\nmodel often faces out-of-distribution (OOD) instances that can cause severe\\nsemantic shift problems at inference time. Therefore, in practice, a reliable\\nmodel should identify such instances, and then either reject them during\\ninference or pass them over to models that handle another distribution. In this\\npaper, we develop an unsupervised OOD detection method, in which only the\\nin-distribution (ID) data are used in training. We propose to fine-tune the\\nTransformers with a contrastive loss, which improves the compactness of\\nrepresentations, such that OOD instances can be better differentiated from ID\\nones. These OOD instances can then be accurately detected using the Mahalanobis\\ndistance in the model's penultimate layer. We experiment with comprehensive\\nsettings and achieve near-perfect OOD detection performance, outperforming\\nbaselines drastically. We further investigate the rationales behind the\\nimprovement, finding that more compact representations through margin-based\\ncontrastive learning bring the improvement. We release our code to the\\ncommunity for future research.\\n",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1100–1111\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1100\nContrastive Out-of-Distribution Detection for Pretrained Transformers\nWenxuan Zhou\nUniversity of Southern California\nzhouwenx@usc.edu\nFangyu Liu\nUniversity of Cambridge\nfl399@cam.ac.uk\nMuhao Chen\nUniversity of Southern California\nmuhaoche@usc.edu\nAbstract\nPretrained Transformers achieve remarkable\nperformance when training and test data are\nfrom the same distribution. However, in real-\nworld scenarios, the model often faces out-of-\ndistribution (OOD) instances that can cause\nsevere semantic shift problems at inference\ntime. Therefore, in practice, a reliable model\nshould identify such instances, and then ei-\nther reject them during inference or pass them\nover to models that handle another distribu-\ntion. In this paper, we develop an unsupervised\nOOD detection method, in which only the in-\ndistribution (ID) data are used in training. We\npropose to ﬁne-tune the Transformers with a\ncontrastive loss, which improves the compact-\nness of representations, such that OOD in-\nstances can be better differentiated from ID\nones. These OOD instances can then be accu-\nrately detected using the Mahalanobis distance\nin the model’s penultimate layer. We experi-\nment with comprehensive settings and achieve\nnear-perfect OOD detection performance, out-\nperforming baselines drastically. We further\ninvestigate the rationales behind the improve-\nment, ﬁnding that more compact representa-\ntions through margin-based contrastive learn-\ning bring the improvement. We release our\ncode to the community for future research1.\n1 Introduction\nMany natural language classiﬁers are developed\nbased on a closed-world assumption, i.e., the train-\ning and test data are sampled from the same distri-\nbution. However, training data can rarely capture\nthe entire distribution. In real-world scenarios, out-\nof-distribution (OOD) instances, which come from\ncategories that are not known to the model, can of-\nten be present in inference phases. These instances\ncould be misclassiﬁed by the model into known cat-\negories with high conﬁdence, causing the semantic\n1The implementation and datasets are available at https:\n//github.com/wzhouad/Contra-OOD\nshift problem (Hsu et al., 2020). As a practical\nsolution to this problem in real-world applications,\nthe model should detect such instances, and sig-\nnal exceptions or transmit to models handling other\ncategories or tasks. Although pretrained Transform-\ners (Devlin et al., 2019) achieve remarkable results\nwhen intrinsically evaluated on in-distribution (ID)\ndata, recent work (Hendrycks et al., 2020) shows\nthat many of these models fall short of detecting\nOOD instances.\nDespite the importance, few attempts have been\nmade for the problem of detecting OOD in NLP\ntasks. One proposed method is to train a model on\nboth the ID and OOD data and regularize the model\nto produce lower conﬁdence on OOD instances\nthan ID ones (Hendrycks et al., 2018; Larson et al.,\n2019). However, as the OOD instances reside in an\nunbounded feature space, their distribution during\ninference is usually unknown. Hence, it is hard\nto decide which OOD instances to use in training,\nlet alone that they may not be available in lots of\nscenarios. Another practiced method for OOD de-\ntection is to use the maximum class probability as\nan indicator (Shu et al., 2017; Hendrycks et al.,\n2020), such that lower values indicate more proba-\nble OOD instances. Though easy to implement, its\nOOD detection performance is far from perfection,\nas prior studies (Dhamija et al., 2018; Liang et al.,\n2018) show that OOD inputs can often get high\nprobabilities as well.\nIn this paper, we aim at improving the OOD de-\ntection ability of natural language classiﬁers, in\nparticular, the pretrained Transformers, which have\nbeen the backbones of many SOTA NLP systems.\nFor practical purposes, we adopt the setting where\nonly ID data are available during task-speciﬁc train-\ning. Moreover, we require that the model should\nmaintain classiﬁcation performance on the ID task\ndata. To this end, we propose a contrastive learn-\ning framework for unsupervised OOD detection,\nwhich is composed of a contrastive loss and an\n1101\nOOD scoring function. Our contrastive loss aims at\nincreasing the discrepancy of the representations of\ninstances from different classes in the task. During\ntraining, instances belonging to the same class are\nregarded as pseudo-ID data while those of differ-\nent classes are considered mutually pseudo-OOD\ndata. We hypothesize that increasing inter-class dis-\ncrepancies can help the model learn discriminative\nfeatures for ID/OOD distinctions, and therefore\nhelp detect true OOD data at inference. We study\ntwo versions of the contrastive loss: a similarity-\nbased contrastive loss (Sohn, 2016; Oord et al.,\n2018; Chen et al., 2020) and a margin-based con-\ntrastive loss. The OOD scoring function maps\nthe representations of instances to OOD detection\nscores, indicating the likelihood of an instance\nbeing OOD. We examine different combinations\nof contrastive losses and OOD scoring functions,\nincluding maximum softmax probability, energy\nscore, Mahalanobis distance, and maximum cosine\nsimilarity. Particularly, we observe that OOD scor-\ning based on the Mahalanobis distance (Lee et al.,\n2018b), when incorporated with the margin-based\ncontrastive loss, generally leads to the best OOD\ndetection performance. The Mahalanobis distance\nis computed from the penultimate layer2 of Trans-\nformers by ﬁtting a class-conditional multivariate\nGaussian distribution.\nThe main contributions of this work are three-\nfold. First, we propose a contrastive learning frame-\nwork for unsupervised OOD detection, where we\ncomprehensively study combinations of different\ncontrastive learning losses and OOD scoring func-\ntions. Second, extensive experiments on various\ntasks and datasets demonstrate the signiﬁcant im-\nprovement our method has made to OOD detection\nfor Transformers. Third, we provide a detailed\nanalysis to reveal the importance of different incor-\nporated techniques, which also identiﬁes further\nchallenges for this emerging research topic.\n2 Related Work\nOut-of-Distribution Detection. Determining\nwhether an instance is OOD is critical for the safe\ndeployment of machine learning systems in the real\nworld (Amodei et al., 2016). The main challenge\nis that the distribution of OOD data is hard to es-\ntimate a priori. Based on the availability of OOD\ndata, recent methods can be categorized into su-\npervised, self-supervised, and unsupervised ones.\n2I.e., the input to the softmax layer.\nSupervised methods train models on both ID and\nOOD data, where the models are expected to out-\nput a uniform distribution over known classes on\nOOD data (Lee et al., 2018a; Dhamija et al., 2018;\nHendrycks et al., 2018). However, it is hard to\nassume the presence of a large dataset that pro-\nvides comprehensive coverage for OOD instances\nin practice. Self-supervised methods (Bergman and\nHoshen, 2020) apply augmentation techniques to\nchange certain properties of data (e.g., through ro-\ntation of an image) and simultaneously learn an\nauxiliary model to predict the property changes\n(e.g., the rotation angle). Such an auxiliary model\nis expected to have worse generalization on OOD\ndata which can in turn be identiﬁed by a larger\nloss. However, it is hard to deﬁne such transforma-\ntions for natural language. Unsupervised methods\nuse only ID data in training. They detect OOD\ndata based on the class probabilities (Bendale and\nBoult, 2016; Hendrycks and Gimpel, 2017; Shu\net al., 2017; Liang et al., 2018) or other latent space\nmetrics (Liu et al., 2020; Lee et al., 2018b). Particu-\nlarly, Vyas et al. (2018) randomly split the training\nclasses into two subsets and treat them as pseudo-\nID and pseudo-OOD data, respectively. They then\ntrain an OOD detector that requires the entropy\nof probability distribution on pseudo-OOD data\nto be lower than pseudo-ID data. This process is\nrepeated to obtain multiple OOD detectors, and\ntheir ensemble is used to detect the OOD instances.\nThis method conducts OOD detection at the cost of\nhigh computational overhead in training redundant\nmodels and has the limitation of not supporting the\ndetection for binary classiﬁcation tasks.\nThough extensively studied for computer vi-\nsion (CV), OOD detection has been overlooked\nin NLP, and most prior works (Kim and Kim, 2018;\nHendrycks et al., 2018; Tan et al., 2019) require\nboth ID and OOD data in training. Hendrycks et al.\n(2020) use the maximum softmax probability as the\ndetection score and show that pretrained Transform-\ners exhibit better OOD detection performance than\nmodels such as LSTM (Hochreiter and Schmidhu-\nber, 1997), while the performance is still imperfect.\nOur framework, as an unsupervised OOD detection\napproach, signiﬁcantly improves the OOD detec-\ntion of Transformers only using ID data.\nContrastive Learning. Recently, contrastive\nlearning has received a lot of research attention.\nIt works by mapping instances of the same class\ninto a nearby region and make instances of differ-\n1102\nent classes uniformly distributed (Wang and Isola,\n2020). Many efforts on CV (Misra and Maaten,\n2020; He et al., 2020; Chen et al., 2020) and\nNLP (Giorgi et al., 2021) incorporate contrastive\nlearning into self-supervised learning, which seeks\nto gather the representations of different augmented\nviews of the same instance and separate those of\ndifferent instances. Prior work on image classi-\nﬁcation (Tack et al., 2020; Winkens et al., 2020)\nshows that model trained with self-supervised con-\ntrastive learning generates discriminative features\nfor detecting distributional shifts. However, such\nmethods heavily rely on data augmentation of in-\nstances and are hard to be applied to NLP. Other\nefforts on CV (Khosla et al., 2020) and NLP (Gunel\net al., 2021) conduct contrastive learning in a super-\nvised manner, which aims at embedding instances\nof the same class closer and separating different\nclasses. They show that models trained with super-\nvised contrastive learning exhibit better classiﬁca-\ntion performance. To the best of our knowledge,\nwe are the ﬁrst to introduce supervised contrastive\nlearning to OOD detection. Such a method does\nnot rely on data augmentation, thus can be easily\nadapted to existing NLP models. We also propose\na margin-based contrastive objective that greatly\noutperforms standard supervised contrastive losses.\n3 Method\nIn this section, we ﬁrst formally deﬁne the OOD\ndetection problem (Sec. 3.1), then introduce the\noverall framework (Sec. 3.2), and ﬁnally present\nthe contrastive representation learning and scoring\nfunctions (Sec. 3.3 and Sec. 3.4).\n3.1 Problem Deﬁnition\nWe aim at improving the OOD detection perfor-\nmance of natural language classiﬁers that are based\non pretrained Transformers, using only ID data\nin the main-task training. Generally, the out-\nof-distribution (OOD) instances can be deﬁned\nas instances (x,y) sampled from an underlying\ndistribution other than the training distribution\nP(Xtrain,Ytrain), where Xtrain and Ytrain are the train-\ning corpus and training label set, respectively. In\nthis context, literature further divides OOD data\ninto those with semantic shift or non-semantic\nshift (Hsu et al., 2020). Semantic shiftrefers to the\ninstances that do not belong to Ytrain. More speciﬁ-\ncally, instances with semantic shift may come from\nunknown categories or irrelevant tasks. Therefore,\nthe model is expected to detect and reject such in-\nstances (or forward them to models handling other\ntasks), instead of mistakenly classifying them into\nYtrain. Non-semantic shift, on the other hand, refers\nto the instances that belong to Ytrain but are sam-\npled from a distribution other than Xtrain, e.g., a\ndifferent corpus. Though drawn from OOD, those\ninstances can be classiﬁed into Ytrain, thus can be\naccepted by the model. Hence, in the context of\nthis paper, we primarily consider an instance (x,y)\nto be OOD if y /∈Ytrain, i.e., exhibiting semantic\nshift, to be consistent with the problem settings of\nprior studies (Hendrycks and Gimpel, 2017; Lee\net al., 2018b; Hendrycks et al., 2020).\nWe hereby formally deﬁne the OOD detection\ntask. Speciﬁcally, given a main task of natural lan-\nguage classiﬁcation (e.g., sentence classiﬁcation,\nNLI, etc.), for an instance xto be classiﬁed, our\ngoal is to develop an auxiliary OOD scoring func-\ntion f(x) :X→ R. This function should return a\nlow score for an ID instance wherey∈Ytrain, and a\nhigh score for an OOD instance wherey /∈Ytrain (y\nis the underlying label for xand is unknown at in-\nference). During inference, we can set a threshold\nfor the OOD score to ﬁlter out most OOD instances.\nThis process involves a trade-off between false neg-\native and false positive and may be speciﬁc to the\napplication. Meanwhile, we expect that the OOD\ndetection auxiliary should not negatively affect the\nperformance of the main task on ID data.\n3.2 Framework Overview\nNext, we introduce the formation of our contrastive\nlearning framework for OOD detection. We decom-\npose OOD detection into two steps. The ﬁrst step\nis contrastive representation learning, where we\nfocus on learning a representation space Hwhere\nthe distribution of ID and that of OOD data are\ndistinct. Accordingly, we need another function\nto map the representation to an OOD score. This\nprocess is equivalent to expressing OOD detection\nas f(x) =g(h), where h∈H is the dense repre-\nsentation of the input text xgiven by an encoder,\ng : H →R is a scoring function mapping the\nrepresentation to an OOD detection score. Using\nthis decomposition, we can use different training\nstrategies for hand different functions for g, which\nare studies in the following sections.\nThe learning process of our framework is de-\nscribed in Alg. 1. In the training phase, our frame-\nwork takes training and validation datasets that are\n1103\nAlgorithm 1: Learning Process\nInput: ID training set Dtrain and ID validation set\nDval.\nOutput: A trained classiﬁer and an OOD detector.\nInitialize the pretrained Transformer M.\nfor t= 1...T do\nSample a batch from Dtrain.\nCalculate the classiﬁcation loss Lce.\nCalculate the contrastive loss Lcont as either Lscl\nor Lmargin\nL = Lce + λLcont.\nUpdate model parameters w.r.t. L.\nif t% evaluation steps= 0then\nFit the OOD detector on Dval.\nEvaluate both the classiﬁer and OOD\ndetector on Dval.\nReturn the best model checkpoint.\nboth ID as input. The model is optimized with\nboth the (main task) classiﬁcation loss and the con-\ntrastive loss on batches sampled from ID training\ndata. The best model is selected based on the ID\nvalidation data. Speciﬁcally, for a distribution-\nbased OOD scoring function such as the Maha-\nlanobis distance, we ﬁrst need to ﬁt the OOD de-\ntector on the ID validation data. We then evaluate\nthe trained model on the ID validation data, where\na satisfactory model should have a low contrastive\nloss and preserve the classiﬁcation performance. In\nthe end, our framework returns a classiﬁer to han-\ndle the main task on ID data and an OOD detector\nto identify OOD instances at inference.\n3.3 Contrastive Representation Learning\nIn this section, we discuss how to learn distinctive\nrepresentations for OOD detection. For better OOD\ndetection performance, the representation space H\nis supposed to minimize the overlap of the repre-\nsentations of ID and OOD data. In a supervised set-\nting where both ID and OOD data are available in\ntraining, it would be easy to obtain such H. For ex-\nample, Dhamija et al. (2018) train the neural model\non both ID and OOD data and require the mag-\nnitude of representations of OOD instances to be\nsmaller than ID representations. However, in real-\nworld applications, the distribution of OOD data\nis usually unknown beforehand. We thus tackle a\nmore general problem setting where the OOD data\nare assumed unavailable in training (unsupervised\nOOD detection, introduced below).\nIn this unsupervised setup, though all training\ndata used are ID, they may belong to different\nclasses. We leverage data of distinct classes to\nlearn more discriminative features. Through a con-\ntrastive learning objective, instances of the same\n<latexit sha1_base64=\"WGwSd0RWO6Hy1FDeipdcpjOm4MQ=\">AAACAHicbVA9SwNBEN2LXzF+nVpY2BwGwSrciahl0MbCIoL5gCSEvc0kWbK3d+zOieG4xr9iY6GIrT/Dzn/jXnKFJj4YeLw3w8w8PxJco+t+W4Wl5ZXVteJ6aWNza3vH3t1r6DBWDOosFKFq+VSD4BLqyFFAK1JAA19A0x9fZ37zAZTmobzHSQTdgA4lH3BG0Ug9+6ATUBwxKpLbtJd0EB4xYZCmPbvsVtwpnEXi5aRMctR69lenH7I4AIlMUK3bnhthN6EKOROQljqxhoiyMR1C21BJA9DdZPpA6hwbpe8MQmVKojNVf08kNNB6EvimMztXz3uZ+J/XjnFw2U24jGIEyWaLBrFwMHSyNJw+V8BQTAyhTHFzq8NGVFGGJrOSCcGbf3mRNE4r3nnFuzsrV6/yOIrkkByRE+KRC1IlN6RG6oSRlDyTV/JmPVkv1rv1MWstWPnMPvkD6/MH8u6XSQ==</latexit>\nL ce\n<latexit sha1_base64=\"vBngqMekMnOqSrJms4ruepQvhOo=\">AAACGnicbVBNS8NAEN3Ur1q/qh69BIsgCCURUY9FLx48VLAf0JSy2U7bpZtN2J2IJeR3ePGvePGgiDfx4r9x2+agbR8MPN6bYWaeHwmu0XF+rNzS8srqWn69sLG5tb1T3N2r6zBWDGosFKFq+lSD4BJqyFFAM1JAA19Awx9ej/3GAyjNQ3mPowjaAe1L3uOMopE6RdcLKA4YFclt2kk8hEdMGKTpySI9lJimnWLJKTsT2PPEzUiJZKh2il9eN2RxABKZoFq3XCfCdkIVciYgLXixhoiyIe1Dy1BJA9DtZPJaah8ZpWv3QmVKoj1R/04kNNB6FPimc3ywnvXG4iKvFWPvsp1wGcUIkk0X9WJhY2iPc7K7XAFDMTKEMsXNrTYbUEUZmjQLJgR39uV5Uj8tu+dl9+6sVLnK4siTA3JIjolLLkiF3JAqqRFGnsgLeSPv1rP1an1Yn9PWnJXN7JN/sL5/AZHxoxY=</latexit>\nL ce + L cont\n<latexit sha1_base64=\"PT7aDEmkSRhT9CHRnZ0H1gi+TEQ=\">AAACA3icbVBNS8NAEN3Ur1q/ot70EiyCp5KIqMeiF48V7Ac0IWy2m3bpZhN2J2IJAS/+FS8eFPHqn/Dmv3HT5qCtDwYe780wMy9IOFNg299GZWl5ZXWtul7b2Nza3jF39zoqTiWhbRLzWPYCrChngraBAae9RFIcBZx2g/F14XfvqVQsFncwSagX4aFgISMYtOSbB26EYUQwz3q5n7lAHyADiZnIc9+s2w17CmuROCWpoxIt3/xyBzFJIyqAcKxU37ET8DIsgRFO85qbKppgMsZD2tdU4IgqL5v+kFvHWhlYYSx1CbCm6u+JDEdKTaJAdxYXq3mvEP/z+imEl17GRJICFWS2KEy5BbFVBGINmKQE+EQTTCTTt1pkhCUmoGOr6RCc+ZcXSee04Zw3nNuzevOqjKOKDtEROkEOukBNdINaqI0IekTP6BW9GU/Gi/FufMxaK0Y5s4/+wPj8AZpgmMk=</latexit>\nX train\n<latexit sha1_base64=\"uC+uA+nN3oBgXmQxEjp7lH/QZm8=\">AAACAXicbVDLSsNAFJ34rPUVdSO4GSyCq5KIqMuiLty1gn1AE8JkOmmHTh7M3IglxI2/4saFIm79C3f+jdM2C209cOFwzr3ce4+fCK7Asr6NhcWl5ZXV0lp5fWNza9vc2W2pOJWUNWksYtnxiWKCR6wJHATrJJKR0Bes7Q+vxn77nknF4+gORglzQ9KPeMApAS155r4TEhhQIrJO7mUOsAfI6vXrPPfMilW1JsDzxC5IBRVoeOaX04tpGrIIqCBKdW0rATcjEjgVLC87qWIJoUPSZ11NIxIy5WaTD3J8pJUeDmKpKwI8UX9PZCRUahT6unN8r5r1xuJ/XjeF4MLNeJSkwCI6XRSkAkOMx3HgHpeMghhpQqjk+lZMB0QSCjq0sg7Bnn15nrROqvZZ1b49rdQuizhK6AAdomNko3NUQzeogZqIokf0jF7Rm/FkvBjvxse0dcEoZvbQHxifP1sal3k=</latexit>\nX OOD\nFigure 1: Illustration of our proposed contrastive loss.\nThe contrastive loss seeks to increase the discrepancy\nof the representations for instances from different train-\ning classes, such that OOD instances from unknown\nclasses can be better differentiated.\nclass form compact clusters, while instances of dif-\nferent classes are encouraged to live apart from\neach other beyond a certain margin, as illustrated\nin Fig. 1. The discriminative feature space is gen-\neralizable to OOD data, which ultimately leads\nto better OOD detection performance in inference\nwhen encountering an unknown distribution. We\nrealize such a strategy using two alternatives of\ncontrastive losses, i.e., the supervised contrastive\nloss and the margin-based contrastive loss.\nSupervised Contrastive Loss. Different from the\ncontrastive loss used in self-supervised representa-\ntion learning (Chen et al., 2020; He et al., 2020) that\ncompares augmented instances to other instances,\nour contrastive loss contrasts instances to those\nfrom different ID classes. To give a more speciﬁc\nillustration of our technique, we ﬁrst consider the\nsupervised contrastive loss (Khosla et al., 2020;\nGunel et al., 2021). Speciﬁcally, for a multi-class\nclassiﬁcation problem with Cclasses, given a batch\nof training instances {(xi,yi)}M\ni=1, where xi is the\ninput text, yi is the ground-truth label, the super-\nvised contrastive loss can be formulated as:\nLscl =\nM∑\ni=1\n−1\nM|P(i)|\n∑\np∈P(i)\nlog ez⊺\ni zp/τ\n∑\na∈A(i)\nez⊺\ni za/τ,\nwhere A(i) = {1,...,M }\\{i}is the set of all an-\nchor instances, P(i) = {p ∈A(i) : yi = yp}is\nthe set of anchor instances from the same class as\ni, τ is a temperature hyper-parameter, zis the L2-\nnormalized [CLS] embedding before the softmax\nlayer (Khosla et al., 2020; Gunel et al., 2021). The\nL2 normalization is for avoiding huge values in the\ndot product, which may lead to unstable updates.\nIn this case, this loss is optimized to increase the\n1104\ncosine similarity of instance pairs if they are from\nthe same class and decrease it otherwise.\nMargin-based Contrastive Loss. The supervised\ncontrastive loss produces minimal gradients when\nthe similarity difference of positive and negative in-\nstances exceeds a certain point. However, to better\nseparate OOD instances, it is beneﬁcial to enlarge\nthe discrepancy between classes as much as possi-\nble. Therefore, we propose another margin-based\ncontrastive loss. It encourages the L2 distances\nof instances from the same class to be as small\nas possible, forming compact clusters, and the L2\ndistances of instances from different classes to be\nlarger than a margin. Our loss is formulated as:\nLpos =\nM∑\ni=1\n1\n|P(i)|\n∑\np∈P(i)\n∥hi −hp∥2,\nLneg =\nM∑\ni=1\n1\n|N(i)|\n∑\nn∈N(i)\n(ξ−∥hi −hn∥2)+,\nLmargin = 1\ndM\n(\nLpos + Lneg\n)\n.\nHere N(i) ={n∈A(i) :yi ̸= yn}is the set of an-\nchor instances from other classes than yi, h∈Rd\nis the unnormalized [CLS] embedding before the\nsoftmax layer, ξ is a margin, d is the number of\ndimensions of h. As we do not use OOD data\nin training, it is hard to properly tune the margin.\nHence, we further incorporate an adaptive margin.\nIntuitively, distances between instances from the\nsame class should be smaller than those from dif-\nferent classes. Therefore, we deﬁne the margin as\nthe maximum distance between pairs of instances\nfrom the same class in the batch:\nξ=\nM\nmax\ni=1\nmax\np∈P(i)\n∥hi −hp∥2.\nWe evaluate both contrastive losses in experiments.\nIn training, the model is jointly optimized with\nthe cross-entropy classiﬁcation loss Lce and the\ncontrastive loss Lcont:\nL= Lce + λLcont,\nwhere λis a positive coefﬁcient. We tune λbased\non the contrastive loss and the classiﬁcation perfor-\nmance on the ID validation set, where a selected\nvalue forλshould achieve a smaller contrastive loss\nwhile maintaining the classiﬁcation performance.\n3.4 OOD Scoring Functions\nNext, we introduce the modeling of the OOD scor-\ning function g. The goal of the scoring functiongis\nto map the representations of instances to OOD de-\ntection scores, where higher scores indicate higher\nlikelihoods for being OOD. In the following, we\ndescribe several choices of this scoring function.\nMaximum Softmax Probability (MSP).\nHendrycks and Gimpel (2017) use the maxi-\nmum class probability 1 −maxC\nj=1 pj among C\ntraining classes in the softmax layer as an OOD\nindicator. This method has been widely adopted\nas a baseline for OOD detection (Hendrycks and\nGimpel, 2017; Hsu et al., 2020; Bergman and\nHoshen, 2020; Hendrycks et al., 2020).\nEnergy Score (Energy). Liu et al. (2020) inter-\npret the softmax function as the ratio of the joint\nprobability in X×Y to the probability in X, and\nestimates the probability density of inputs as:\ng= −log\nC∑\nj=1\nexp(w⊺\njh),\nwhere wj ∈Rd is the weight of the jth class in the\nsoftmax layer, his the input to the softmax layer.\nA higher gmeans lower probability density in ID\nclasses and thus implies higher OOD likelihood.\nMahalanobis Distance (Maha). Lee et al.\n(2018b) model the ID features with class-\nconditional multivariate Gaussian distributions. It\nﬁrst ﬁts the Gaussian distributions on the ID val-\nidation set Dval = {(xi,yi)}M\ni=1 using the input\nrepresentation hin the penultimate layer of model:\nµj = Eyi=j [hi] ,j = 1,...,C,\nΣ = E[(hi −µyi ) (hi −µyi )⊺] ,\nwhere Cis the number of classes, µj is the mean\nvector of classes, and Σ is a shared covariance\nmatrix of all classes. Then, given an instance x\nduring inference, it calculates the OOD detection\nscore as the minimum Mahalanobis distance among\nthe Cclasses:\ng= −\nC\nmin\nj=1\n(h−µj)⊺Σ+(h−µj),\nwhere Σ+ is the pseudo-inverse of Σ. The Maha-\nlanobis distance calculates the probability density\nof hin the Gaussian distribution.\nCosine Similarity can also be incorporated to con-\nsider the angular similarity of input representations.\n1105\nTo do so, the scoring function returns the OOD\nscore as the maximum cosine similarity of hto\ninstances of the ID validation set:\ng= −\nM\nmax\ni=1\ncos(h,h(val)\ni ).\nThe above OOD scoring functions, combined\nwith options of contrastive losses, lead to differ-\nent variants of our framework. We evaluate each\ncombination in experiments.\n4 Experiments\nThis section presents experimental evaluations of\nthe proposed OOD detection framework. We start\nby describing experimental datasets and settings\n(Sec.4.1 and 4.2), followed by detailed results anal-\nysis and case studies (Sec.4.3 to 4.5).\n4.1 Datasets\nPrevious studies on OOD detection mostly focus\non image classiﬁcation, while few have been made\non natural language. Currently, there still lacks a\nwell-established benchmark for OOD detection in\nNLP. Therefore, we extend the selected datasets\nby Hendrycks et al. (2020) and propose a more\nextensive benchmark, where we use different pairs\nof NLP datasets as ID and OOD data, respectively.\nThe criterion for dataset selection is that the OOD\ninstances should not belong to ID classes. To\nensure this, we refer to the label descriptions in\ndatasets and manually inspect samples of instances.\nWe use the following datasets as alternatives of\nID data that correspond to three natural language\nclassiﬁcation tasks:\n• Sentiment Analysis. We include two datasets\nfor this task. SST2 (Socher et al., 2013) and\nIMDB (Maas et al., 2011) are both datasets for\nsentiment analysis, where the polarities of sen-\ntences are labeled either positive or negative. For\nSST2, the train/validation/test splits are provided\nin the dataset. For IMDB, we randomly sample\n10% of the training instances as the validation\nset. Note that both datasets belong to the same\ntask and are not considered OOD to each other.\n• Topic Classiﬁcation. We use 20 Newsgroup\n(Lang, 1995), a dataset for topic classiﬁcation\ncontaining 20 classes. We randomly divide\nthe whole dataset into an 80/10/10 split as the\ntrain/validation/test set.\n• Question Classiﬁcation. TREC-10 (Li and\nRoth, 2002) classiﬁes questions based on the\nDataset # train # dev # test # class\nSST2 67349 872 1821 2\nIMDB 22500 2500 25000 2\nTREC-10 4907 545 500 6\n20NG 15056 1876 1896 20\nMNLI - - 19643 -\nRTE - - 3000 -\nMulti30K - - 2532 -\nWMT16 - - 2999 -\nTable 1: Statistics of the datasets.\ntypes of their sought-after answers. We use its\ncoarse version with 6 classes and randomly sam-\nple 10% of the training instances as the valida-\ntion set.\nMoreover, for the above three tasks, any pair\nof datasets for different tasks can be regarded as\nOOD to each other. Besides, following Hendrycks\net al. (2020), we also select four additional datasets\nsolely as the OOD data: concatenations of the\npremises and respective hypotheses from two NLI\ndatasets RTE (Dagan et al., 2005; Bar-Haim et al.,\n2006; Giampiccolo et al., 2007; Bentivogli et al.,\n2009) and MNLI (Williams et al., 2018), the En-\nglish source side of Machine Translation (MT)\ndatasets English-German WMT16 (Bojar et al.,\n2016) and Multi30K (Elliott et al., 2016). We\ntake the test splits in those datasets as OOD in-\nstances in testing. Particularly, for MNLI, we use\nboth the matched and mismatched test sets. For\nMulti30K, we use the union of the ﬂickr 2016 En-\nglish test set, mscoco 2017 English test set, and\nﬁlckr 2018 English test set as the test set. There are\nseveral reasons for not using them as ID data: (1)\nWMT16 and Multi30K are MT datasets and do not\napply to a natural language classiﬁcation problem.\nTherefore, we cannot train a classiﬁer on these\ntwo datasets. (2) The instances in NLI datasets\nare labeled either as entailment/non-entailment for\nRTE or entailment/neural/contradiction for MNLI,\nwhich comprehensively covers all possible relation-\nships of two sentences. Therefore, it is hard to\ndetermine OOD instances for NLI datasets. The\nstatistics of the datasets are shown in Tab. 1.\n4.2 Experimental Settings\nEvaluation Protocol. We train the model on the\ntraining split of each of the four aforementioned\nID datasets in turn. In the inference phase, the\nrespective test split of that dataset is used as ID test\ndata, while all the test splits of datasets from other\n1106\nAUROC ↑ / FAR95 ↓ Avg SST2 IMDB TREC-10 20NG\nw/o Lcont\nMSP 94.1 / 35.0 88.9 / 61.3 94.7 / 40.6 98.1 / 7.6 94.6 / 30.5\nEnergy 94.0 / 34.7 87.7 / 63.2 93.9 / 49.5 98.0 / 10.4 96.5 / 15.8\nMaha 98.5 / 7.3 96.9 / 18.3 99.8 / 0.7 99.0 / 2.7 98.3 / 7.3\nCosine 98.2 / 9.7 96.2 / 23.6 99.4 / 2.1 99.2 / 2.3 97.8 / 10.7\nw/ Lscl\nLscl + MSP 90.4 / 46.3 89.7 / 59.9 93.5 / 48.6 90.2 / 36.4 88.1 / 39.2\nLscl + Energy 90.5 / 43.5 88.5 / 64.7 92.8 / 50.4 90.3 / 32.2 90.2 / 26.8\nLscl + Maha 98.3 / 10.5 96.4 / 26.6 99.6 / 2.0 99.2 / 1.9 97.9 / 11.6\nLscl + Cosine 97.7 / 13.0 95.9 / 28.2 99.2 / 4.2 99.0 / 2.4 96.8 / 17.0\nw/ Lmargin\nLmargin + MSP 93.0 / 33.7 89.7 / 49.2 93.9 / 46.3 97.6 / 6.5 90.9 / 32.6\nLmargin + Energy 93.9 / 31.0 89.6 / 48.8 93.4 / 52.1 98.4 / 4.6 94.1 / 18.6\nLmargin + Maha 99.5 / 1.7 99.9 / 0.6 100 / 0 99.3 / 0.4 98.9 / 6.0\nLmargin + Cosine 99.0 / 3.8 99.6 / 1.7 99.9 / 0.2 99.0 / 1.5 97.4 / 11.8\nTable 2: OOD detection performance (in %) of RoBERTa LARGE trained on the four ID datasets. Due to space\nlimits, for each of the four training ID dataset, we report the macro average of AUROC and FAR95 on all OOD\ndatasets (check Appendix for full results). Results where the contrastive loss improves OOD detection on both\nevaluation metrics are highlighted in green .“w/o Lcont+MSP” thereof is the method in Hendrycks et al. (2020).\ntasks are treated as OOD test data.\nWe adopt two metrics that are commonly used\nfor measuring OOD detection performance in ma-\nchine learning research (Hendrycks and Gimpel,\n2017; Lee et al., 2018b): (1) AUROC is the area\nunder the receiver operating characteristic curve,\nwhich plots the true positive rate (TPR) against\nthe false positive rate (FPR). A higher AUROC\nvalue indicates better OOD detection performance,\nand a random guessing detector corresponds to an\nAUROC of 50%. (2) FAR95 is the probability for\na negative example (OOD) to be mistakenly clas-\nsiﬁed as positive (ID) when the TPR is 95%, in\nwhich case a lower value indicates better perfor-\nmance. Both metrics are threshold-independent.\nCompared Methods. We evaluate all conﬁgura-\ntions of contrastive losses and OOD scoring func-\ntions. Those include 12 settings composed of 3 al-\nternative setups for contrastive losses (Lscl, Lmargin\nor w/o a contrastive loss) and 4 alternatives of OOD\nscoring functions (MSP, the energy score, Maha, or\ncosine similarity).\nModel Conﬁguration. We implement our frame-\nwork upon Huggingface’s Transformers (Wolf\net al., 2020) and build the text classiﬁer based\non RoBERTa LARGE (Liu et al., 2019) in the\nmain experiment. All models are optimized with\nAdam (Kingma and Ba, 2015) using a learning rate\nof 1e−5, with a linear learning rate decay towards\n0. We use a batch size of 32 and ﬁne-tune the model\nfor 10 epochs. When training the model on each\ntraining split of a dataset, we use the respective val-\nidation split for both hyper-parameter tuning and\nThe hyper-parameters are tuned according to the\nclassiﬁcation performance and the contrastive loss\non the ID validation set. We ﬁnd that τ = 0.3 and\nλ= 2work well with Lscl, while λ= 2work well\nwith Lmargin, and we apply them to all datasets.\n4.3 Main Results\nWe hereby discuss the main results of the OOD de-\ntection performance. Note that the incorporation of\nour OOD techniques does not lead to noticeable in-\nterference of the main-task performance, for which\nan analysis is later given in Sec. 4.5.\nThe OOD detection results by different conﬁgu-\nrations of models are given in Tab. 2. For all results,\nwe report the average of 5 runs using different ran-\ndom seeds. Each model conﬁguration is reported\nwith separate sets of results when being trained\non different datasets, on top of which the macro\naverage performance is also reported. For settings\nwith Lscl and Lmargin, results better than the base-\nlines (w/o a contrastive loss) are marked as red. We\nobserve that: (1) Among OOD detection functions,\nthe Mahalanobis distance performs the best on aver-\nage and drastically outperforms the MSP baseline\nused in Hendrycks et al. (2020). This is due to that\nthe Mahalanobis distance can better capture the\ndistributional difference. (2) Considering models\ntrained on different ID datasets, the model variants\nwith Lmargin have achieved near-perfect OOD detec-\ntion performance on SST2, IMDB, and TREC-10.\nWhile on the 20 Newsgroup dataset that contains\narticles from multiple genres, there is still room for\nimprovement. (3) Overall, The margin-based con-\ntrastive loss (Lmargin) signiﬁcantly improves OOD\ndetection performance. Particularly, it performs\n1107\nZ \u0012 R \u0003 L c o n t Z \u0012 \u0003 L s c l Z \u0012 \u0003 L m a r g L n\n<latexit sha1_base64=\"2lYbiD9qb3KjOYQxWATzjDtW6WY=\">AAACCHicbVC7TgMxEPSFVwivA0oKLBIkqnCXAigjaCgogkQeUhJFPsdJrPjsk70HRKcrafgVGgoQouUT6PgbnEcBgZFWGs3sancniAQ34HlfTmZhcWl5JbuaW1vf2Nxyt3dqRsWasipVQulGQAwTXLIqcBCsEWlGwkCwejC8GPv1W6YNV/IGRhFrh6QveY9TAlbquPt3xwoXWiGBASUiuUo7SQvYPSRUSUjTQsfNe0VvAvyX+DOSRzNUOu5nq6toHDIJVBBjmr4XQTshGjgVLM21YsMiQoekz5qWShIy004mj6T40Cpd3FPalgQ8UX9OJCQ0ZhQGtnN8sZn3xuJ/XjOG3lk74TKKgUk6XdSLBQaFx6ngLteMghhZQqjm9lZMB0QTCja7nA3Bn3/5L6mViv5J0b8u5cvnsziyaA8doCPko1NURpeogqqIogf0hF7Qq/PoPDtvzvu0NePMZnbRLzgf3whVmgA=</latexit>\nw/o L cont\n<latexit sha1_base64=\"5klJgy8GiMzY+CDhk0k4ss9NN6o=\">AAACBnicbVC7TsNAEDyHVwgvAyVCOpEgUQU7BVBG0FBQBIk8pDiKzpdzcsr5obs1EFmuaPgVGgoQouUb6PgbzokLCIy00mhmV7s7biS4Asv6MgoLi0vLK8XV0tr6xuaWub3TUmEsKWvSUISy4xLFBA9YEzgI1okkI74rWNsdX2R++5ZJxcPgBiYR6/lkGHCPUwJa6pv7d8e44vgERpSI5CrtJw6we0gUFWla6Ztlq2pNgf8SOydllKPRNz+dQUhjnwVABVGqa1sR9BIigVPB0pITKxYROiZD1tU0ID5TvWT6RooPtTLAXih1BYCn6s+JhPhKTXxXd2YHq3kvE//zujF4Z72EB1EMLKCzRV4sMIQ4ywQPuGQUxEQTQiXXt2I6IpJQ0MmVdAj2/Mt/SatWtU+q9nWtXD/P4yiiPXSAjpCNTlEdXaIGaiKKHtATekGvxqPxbLwZ77PWgpHP7KJfMD6+AVCVmQs=</latexit>\nw/ L scl\n<latexit sha1_base64=\"zoUJ+B/EEL320qIQ78Dt/B3v8eU=\">AAACCXicbVC7TsNAEDyHVwivACXNiQSJKtgpgDKChoIiSOQhJZZ1vlySU85n624NRJZbGn6FhgKEaPkDOv6GS+ICEkZaaTSzq90dPxJcg21/W7ml5ZXVtfx6YWNza3unuLvX1GGsKGvQUISq7RPNBJesARwEa0eKkcAXrOWPLid+644pzUN5C+OIuQEZSN7nlICRvCK+P8HlbkBgSIlIrlMv6QJ7gCQgasBlmpa9Ysmu2FPgReJkpIQy1L3iV7cX0jhgEqggWnccOwI3IQo4FSwtdGPNIkJHZMA6hkoSMO0m009SfGSUHu6HypQEPFV/TyQk0Hoc+KZzcrOe9ybif14nhv65m3AZxcAknS3qxwJDiCex4B5XjIIYG0Ko4uZWTIdEEQomvIIJwZl/eZE0qxXntOLcVEu1iyyOPDpAh+gYOegM1dAVqqMGougRPaNX9GY9WS/Wu/Uxa81Z2cw++gPr8wfChZpl</latexit>\nw/ L margin\nFigure 2: Visualization of the representations for positive, negative instances in SST2 and OOD ones. The discrep-\nancy between ID and OOD representations is greater on representations obtained with Lmargin.\nAUROC ↑ / FAR95 ↓ TREC-10 20NG\nMSP 73.7 / 56.5 76.4 / 80.7\nMaha 75.5 / 56.1 77.2 / 74.1\nLmargin + MSP 64.1 / 66.4 74.6 / 82.0\nLmargin + Maha 76.6 / 61.3 78.5 / 72.7\nTable 3: Novel class detection performance.\nthe best with the Mahalanobis distance, reducing\nthe average FAR95 of Maha by 77% from 7.3%\nto 1.7%. (4) The supervised contrastive loss (Lscl)\ndoes not effectively improve OOD detection in gen-\neral. In many cases, its performance is even worse\nthan the baseline.\n4.4 Novel Class Detection\nWe further evaluate our framework in a more chal-\nlenging setting of novel class detection. Given a\ndataset containing multiple classes (≥3), We ran-\ndomly reserve one class as OOD data while treat-\ning others as ID data. We then train the model on\nthe ID data and require it to identify OOD data\nin inference. In this case, the OOD data are sam-\npled from the same task corpus as the ID data, and\nthus is much harder to be distinguished. We re-\nport the average performance of 5 trials in Tab. 3.\nThe results are consistent with the main results in\ngeneral. The Mahalanobis distance consistently\noutperforms consistently outperforms MSP, and\nthe Lmargin achieves better performance except for\nthe FAR95 metric on the TREC-10 dataset. How-\never, the performance gain is notably smaller than\nthat in the main experiments. Moreover, none of\nthe compared methods achieve an AUROC score of\nover 80%. This experiment shows that compared to\ndetecting OOD instances from other tasks, detect-\ning OOD instances from similar corpora is much\nmore challenging and remains room for further in-\nvestigation.\nAccuracy SST2 IMDB TREC-10 20NG\nw/o Lcont 96.4 95.3 97.7 93.6\nw/ Lscl 96.3 95.3 97.4 93.4\nw/ Lmargin 96.3 95.3 97.5 93.9\nTable 4: Accuracy of the trained classiﬁer.\nAUROC↑/ FAR95↓ L1 Cosine L2\nMSP 93.6 / 31.1 94.1 / 30.992.2 / 32.0\nEnergy 93.8 / 27.2 94.7 / 26.994.4 / 27.5\nMaha 99.3 / 2.8 99.2 / 3.0 99.4 / 1.7\nCosine 98.1 / 10.9 98.8 / 5.3 99.0 / 3.9\nTable 5: Average OOD detection performance of differ-\nent distance metrics.\n4.5 Analysis\nVisualization of Representations. To help under-\nstand the increased OOD detection performance of\nour method, we visualize the penultimate layer of\nthe Transformer trained with different contrastive\nlosses. Speciﬁcally, we train the model on SST2\nand visualize instances from the SST2 validation\nset and OOD datasets using t-SNE (Van der Maaten\nand Hinton, 2008), as shown in Fig. 2. We observe\nthat the representations obtained with Lmargin can\ndistinctly separate ID and OOD instances, such that\nID and OOD clusters see almost no overlap.\nMain Task Performance. As stated in Sec. 3.1,\nthe increased OOD detection performance should\nnot interfere with the classiﬁcation performance on\nthe main task. We evaluate the trained classiﬁer on\nthe four ID datasets. The results are shown in Tab. 4.\nWe observe that the contrastive loss does not no-\nticeably decrease the classiﬁcation performance,\nnor does it increase the performance, which differs\nfrom the observations by Gunel et al. (2021).\nDistance Metrics. Besides L2 distance, we further\nevaluate the L1 distance and the cosine distance\n1108\nAUROC↑/ FAR95↓ Maha Maha + Lmargin\nBERTBASE 95.7 / 21.5 98.4 / 8.1\nBERTLARGE 97.7 / 13.3 99.1 / 3.9\nRoBERTaBASE 98.4 / 9.3 99.6 / 2.0\nRoBERTaLARGE 98.5 / 7.3 99.4 / 1.7\nTable 6: Average OOD detection performance of other\npretrained Transformers.\nwith the margin-based contrastive loss Lmargin. Re-\nsults are shown in Tab. 5. Due to space limita-\ntions, we only report the average OOD perfor-\nmance on the four ID datasets. We observe that the\nthree metrics achieve similar performance, and all\noutperform the baseline when using Maha as the\nscoring function. Among them, L2 distance gets\nslightly better OOD detection performance. More-\nover, Lmargin signiﬁcantly outperforms Lscl when\nboth use cosine as the distance metric. It shows\nthat their performance difference arises from the\ncharacteristics of the losses instead of the metric.\nOOD Detection by Other Transformers. We\nalso evaluate the OOD detection ability of other\npretrained Transformers in Tab. 6 and report the\naverage performance on the four ID datasets. For\nBERT (Devlin et al., 2019), we use λ= 0.2. We\nobserve that: (1) Larger models have better OOD\ndetection ability. For both BERT and RoBERTa,\nthe large versions offer better results than the base\nversions. (2) Pretraining on diverse data improves\nOOD detection. RoBERTa, which uses more pre-\ntraining corpora, outperforms BERT models. (3)\nThe margin-based contrastive loss consistently im-\nproves OOD detection on all encoders.\n5 Conclusion\nThis work presents an unsupervised OOD detec-\ntion framework for pretrained Transformers requir-\ning only ID data. We systematically investigate\nthe combination of contrastive losses and scoring\nfunctions, the two key components in our frame-\nwork. In particular, we propose a margin-based\ncontrastive objective for learning compact repre-\nsentations, which, in combination with the Maha-\nlanobis distance, achieves the best performance:\nnear-perfect OOD detection on various tasks and\ndatasets. We further propose novel class detection\nas the future challenge for OOD detection.\nEthical Consideration\nThis work does not present any direct societal con-\nsequences. The proposed work seeks to develop\na general contrastive learning framework that han-\ndles unsupervised OOD detection in natural lan-\nguage classiﬁcation. We believe this study leads to\nintellectual merits that beneﬁt with reliable applica-\ntion of NLU models. Since in real-world scenarios,\na model may face heterogeneous inputs with signif-\nicant semantic shifts from its training distributions.\nAnd it potentially has broad impacts since the tack-\nled issues also widely exist in tasks of other areas.\nAll experiments are conducted on open datasets.\nAcknowledgment\nWe appreciate the anonymous reviewers for their in-\nsightful comments and suggestions. This material\nis supported by the National Science Foundation\nof United States Grant IIS 2105329.\nReferences\nDario Amodei, Christopher Olah, J. Steinhardt, P. F.\nChristiano, John Schulman, and Dandelion Mané.\n2016. Concrete problems in ai safety. ArXiv,\nabs/1606.06565.\nRoy Bar-Haim, Ido Dagan, B. Dolan, L. Ferro, Danilo\nGiampiccolo, and B. Magnini. 2006. The second\npascal recognising textual entailment challenge.\nAbhijit Bendale and T. Boult. 2016. Towards open\nset deep networks. 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n1563–1572.\nL. Bentivogli, Peter Clark, Ido Dagan, and Danilo Gi-\nampiccolo. 2009. The sixth pascal recognizing tex-\ntual entailment challenge. In TAC.\nLiron Bergman and Yedid Hoshen. 2020.\nClassiﬁcation-based anomaly detection for gen-\neral data. In International Conference on Learning\nRepresentations, volume abs/2005.02359.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Matthias Huck, An-\ntonio Jimeno Yepes, Philipp Koehn, Varvara Lo-\ngacheva, Christof Monz, et al. 2016. Findings of\nthe 2016 conference on machine translation. In Pro-\nceedings of the First Conference on Machine Trans-\nlation: Volume 2, Shared Task Papers, pages 131–\n198.\nTing Chen, Simon Kornblith, Mohammad Norouzi,\nand Geoffrey Hinton. 2020. A simple framework for\ncontrastive learning of visual representations. In In-\nternational conference on machine learning, pages\n1597–1607. PMLR.\n1109\nIdo Dagan, Oren Glickman, and B. Magnini. 2005.\nThe pascal recognising textual entailment challenge.\nIn MLCW.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota.\nA. Dhamija, Manuel Günther, and T. Boult. 2018. Re-\nducing network agnostophobia. In NeurIPS.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30K: Multilingual English-\nGerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language, pages 70–\n74, Berlin, Germany. Association for Computational\nLinguistics.\nDanilo Giampiccolo, B. Magnini, Ido Dagan, and\nW. Dolan. 2007. The third pascal recognizing tex-\ntual entailment challenge. In ACL-PASCAL@ACL.\nJohn Giorgi, Osvald Nitski, Bo Wang, and Gary Bader.\n2021. DeCLUTR: Deep contrastive learning for\nunsupervised textual representations. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 879–895,\nOnline. Association for Computational Linguistics.\nBeliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoy-\nanov. 2021. Supervised contrastive learning for pre-\ntrained language model ﬁne-tuning. In International\nConference for Learning Representations.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for unsu-\npervised visual representation learning. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 9729–9738.\nDan Hendrycks and Kevin Gimpel. 2017. A baseline\nfor detecting misclassiﬁed and out-of-distribution\nexamples in neural networks. In International Con-\nference on Learning Representations.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online.\nDan Hendrycks, Mantas Mazeika, and Thomas Diet-\nterich. 2018. Deep anomaly detection with outlier\nexposure. In International Conference on Learning\nRepresentations.\nS. Hochreiter and J. Schmidhuber. 1997. Long short-\nterm memory. Neural Computation, 9:1735–1780.\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and\nZ. Kira. 2020. Generalized odin: Detecting out-\nof-distribution image without learning from out-of-\ndistribution data. 2020 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR),\npages 10948–10957.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron\nSarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\npervised contrastive learning. Advances in Neural\nInformation Processing Systems, 33.\nJoo-Kyung Kim and Young-Bum Kim. 2018. Joint\nlearning of domain classiﬁcation and out-of-domain\ndetection with dynamic class weighting for satisﬁc-\ning false acceptance rates. Proc. Interspeech 2018,\npages 556–560.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference for Learning Representations.\nKen Lang. 1995. Newsweeder: Learning to ﬁlter\nnetnews. In Machine Learning Proceedings 1995,\npages 331–339. Elsevier.\nStefan Larson, Anish Mahendran, Joseph J Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K Kummerfeld, Kevin Leach, Michael A\nLaurenzano, Lingjia Tang, et al. 2019. An eval-\nuation dataset for intent classiﬁcation and out-of-\nscope prediction. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1311–1316.\nKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin.\n2018a. Training conﬁdence-calibrated classiﬁers for\ndetecting out-of-distribution samples. In Interna-\ntional Conference on Learning Representations.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018b. A simple uniﬁed framework for detecting\nout-of-distribution samples and adversarial attacks.\nIn Proceedings of the 32nd International Conference\non Neural Information Processing Systems, pages\n7167–7177.\nXin Li and Dan Roth. 2002. Learning question clas-\nsiﬁers. In COLING 2002: The 19th International\nConference on Computational Linguistics.\nShiyu Liang, Yixuan Li, and R Srikant. 2018. Enhanc-\ning the reliability of out-of-distribution image detec-\ntion in neural networks. In International Conference\non Learning Representations.\nWeitang Liu, Xiaoyun Wang, John Owens, and Yix-\nuan Li. 2020. Energy-based out-of-distribution de-\ntection. In Advances in Neural Information Process-\ning Systems, volume 33.\n1110\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nIshan Misra and Laurens van der Maaten. 2020. Self-\nsupervised learning of pretext-invariant representa-\ntions. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n6707–6717.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nLei Shu, Hu Xu, and Bing Liu. 2017. DOC: Deep\nopen classiﬁcation of text documents. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2911–2916,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nKihyuk Sohn. 2016. Improved deep metric learning\nwith multi-class n-pair loss objective. In Proceed-\nings of the 30th International Conference on Neural\nInformation Processing Systems, pages 1857–1865.\nJihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jin-\nwoo Shin. 2020. Csi: Novelty detection via con-\ntrastive learning on distributionally shifted instances.\nIn 34th Conference on Neural Information Process-\ning Systems (NeurIPS) 2020. Neural Information\nProcessing Systems.\nMing Tan, Yang Yu, Haoyu Wang, Dakuo Wang, Sa-\nloni Potdar, Shiyu Chang, and Mo Yu. 2019. Out-of-\ndomain detection for low-resource text classiﬁcation\ntasks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3566–3572, Hong Kong, China.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nApoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Di-\npankar Das, Bharat Kaul, and Theodore L Willke.\n2018. Out-of-distribution detection using an ensem-\nble of self supervised leave-out classiﬁers. In Pro-\nceedings of the European Conference on Computer\nVision (ECCV), pages 550–564.\nTongzhou Wang and Phillip Isola. 2020. Understand-\ning contrastive representation learning through align-\nment and uniformity on the hypersphere. In Inter-\nnational Conference on Machine Learning, pages\n9929–9939. PMLR.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert\nStanforth, Vivek Natarajan, Joseph R Ledsam, Patri-\ncia MacWilliams, Pushmeet Kohli, Alan Karthike-\nsalingam, Simon Kohl, et al. 2020. Contrastive\ntraining for improved out-of-distribution detection.\narXiv preprint arXiv:2007.05566.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online.\n1111\nAUROC SST2 IMDB TREC-10 20NG\nMSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine\nSST2 - - - - - - - - 97.1 94.8 97.4 97.9 98.6 99.6 99.4 99.7\nIMDB - - - - - - - - 98.9 98.8 99.5 99.5 95.9 97.8 98.9 98.6\nTREC-10 91.8 91.5 97.8 97.0 94.9 94.0 100 99.5 - - - - 95.1 97.6 98.9 98.7\n20NG 93.6 93.4 94.9 93.2 96.0 95.6 99.8 99.6 98.2 99.0 99.5 99.6 - - - -\nMNLI 84.6 83.6 95.1 94.6 93.1 92.4 99.5 99.0 97.0 97.3 98.9 98.8 94.1 96.1 98.1 97.5\nRTE 89.2 87.4 98.4 98.1 93.9 93.3 99.8 99.5 98.6 98.8 99.5 99.4 90.3 92.8 96.5 95.2\nWMT16 84.0 82.4 96.4 95.9 93.4 92.7 99.7 99.1 97.9 98.2 99.4 99.3 92.7 95.0 97.8 96.8\nMulti30K 90.2 88.1 98.8 98.6 96.4 95.6 99.9 99.8 99.1 99.2 99.7 99.6 95.7 97.0 98.7 98.3\nAvg 88.9 87.7 96.9 96.2 94.7 93.9 99.8 99.4 98.1 98.0 99.0 99.2 94.6 96.5 98.3 97.8\nFAR95 SST2 IMDB TREC-10 20NG\nMSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine\nSST2 - - - - - - - - 14.5 28.5 12.0 6.4 9.0 2.3 0.3 0.9\nIMDB - - - - - - - - 2.9 4.5 0.2 0.3 25.3 10.5 4.4 6.4\nTREC-10 61.3 63.1 13.2 19.4 37.4 57.7 0 1.5 - - - - 35.0 14.9 1.3 8.3\n20NG 52.1 52.5 39.5 55.9 28.4 32.1 0.2 0.6 7.2 6.6 0.3 1.0 - - - -\nMNLI 68.4 68.7 27.0 31.4 51.4 55.4 2.2 4.8 13.6 15.5 3.2 4.2 36.0 20.2 10.1 13.9\nRTE 59.7 62.4 8.0 9.0 49.9 54.7 0.7 1.9 5.2 5.5 0.8 1.4 49.0 30.1 17.1 21.9\nWMT16 69.4 70.6 17.2 20.2 50.9 57.6 1.2 3.8 8.5 10.2 1.8 2.2 40.5 23.1 11.9 16.4\nMulti30K 57.3 61.7 5.5 6.0 25.7 34.3 0 0.2 1.3 1.8 0.3 0.2 18.9 9.2 5.9 7.0\nAvg 61.3 63.2 18.3 23.6 40.6 48.6 0.7 2.1 7.6 10.4 2.7 2.3 30.5 15.8 7.3 10.7\nTable 7: AUROC and FAR95 (in %) of RoBERTaLARGE model trained w/o Lcont. Results are averaged over 5 runs\nwith different seeds.\nAUROC SST2 IMDB TREC-10 20NG\nMSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine\nSST2 - - - - - - - - 96.2 96.6 98.4 97.8 96.3 98.1 99.5 99.0\nIMDB - - - - - - - - 99.3 99.7 99.6 99.3 94.5 96.9 99.0 98.4\nTREC-10 95.1 94.9 99.5 99.0 93.8 93.3 100 100 - - - - 88.0 92.4 99.6 96.5\n20NG 95.2 95.0 100 100 95.4 95.3 100 99.9 99.2 99.8 99.8 99.7 - - - -\nMNLI 82.8 82.7 99.8 99.5 92.4 91.7 100 99.9 96.6 97.6 99.2 98.8 91.0 94.2 98.4 97.2\nRTE 87.4 87.5 100 99.9 92.9 92.1 100 99.9 96.6 98.1 99.6 99.2 84.5 88.7 98.2 95.6\nWMT16 83.9 84.0 99.9 99.4 92.9 92.2 100 99.9 97.1 98.0 99.4 99.1 88.3 92.5 98.5 96.7\nMulti30K 93.5 93.6 100 99.9 95.9 95.7 100 100 97.9 98.9 99.5 99.3 93.7 96.0 99.1 98.3\nAvg 89.7 89.6 99.9 99.6 93.9 93.4 100 99.9 97.6 98.4 99.3 99.0 90.9 94.1 98.9 97.4\nFAR95 SST2 IMDB TREC-10 20NG\nMSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine MSP Energy Maha Cosine\nSST2 - - - - - - - - 11.9 10.4 1.6 6.9 13.7 5.3 1.2 2.6\nIMDB - - - - - - - - 0.5 0.2 0 0 23.6 11.4 4.7 7.4\nTREC-10 35.3 35.0 2.4 4.3 50.0 54.0 0 0 - - - - 27.2 13.8 1.4 4.4\n20NG 36.4 36.3 0 0 37.8 33.1 0 0 0.6 0.2 0 0 - - - -\nMNLI 64.6 64.3 0.4 2.6 52.2 83.8 0.1 0.9 9.6 6.7 0.7 1.9 37.4 24.7 9.6 16.7\nRTE 58.3 57.7 0 0.3 52.9 54.3 0 0.3 9.8 6.2 0.1 0.5 52.9 35.4 11.1 24.2\nWMT16 64.3 64.1 0.5 3.0 53.7 55.7 0 0.4 7.9 5.7 0.5 1.3 45.3 27.8 7.5 18.7\nMulti30K 36.3 35.4 0 0.3 30.9 31.9 0 0 5.3 2.6 0 0.2 27.8 12.0 6.9 8.7\nAvg 49.2 48.8 0.6 1.7 46.3 52.1 0 0.2 6.5 4.6 0.4 1.5 32.6 18.6 6.0 11.8\nTable 8: AUROC and FAR95 (in %) of RoBERTaLARGE model trained w/ Lmargin. Results are averaged over 5 runs\nwith different seeds.\nA Full Results\nWe show the full OOD detection performance of ID datasets on OOD datasets. The results of w/o Lcont\nand w/ Lmargin are shown in Tab. 7 and Tab. 8, respectively.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7611968517303467
    },
    {
      "name": "Transformer",
      "score": 0.7268339991569519
    },
    {
      "name": "Inference",
      "score": 0.7211860418319702
    },
    {
      "name": "Mahalanobis distance",
      "score": 0.7060582041740417
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.592420220375061
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5722020268440247
    },
    {
      "name": "Language model",
      "score": 0.45960381627082825
    },
    {
      "name": "Machine learning",
      "score": 0.439902663230896
    },
    {
      "name": "Natural language processing",
      "score": 0.37394148111343384
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33831408619880676
    },
    {
      "name": "Engineering",
      "score": 0.12224918603897095
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}