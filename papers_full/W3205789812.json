{
  "title": "CLIP-Adapter: Better Vision-Language Models with Feature Adapters",
  "url": "https://openalex.org/W3205789812",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2109793426",
      "name": "Gao Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746964385",
      "name": "Geng, Shijie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222134701",
      "name": "Zhang, Renrui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3173333905",
      "name": "Ma, Teli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226817511",
      "name": "Fang, Rongyao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1865257807",
      "name": "Zhang Yongfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1969345873",
      "name": "Li Hongsheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2052563777",
      "name": "Qiao Yu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3198675127",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2963521239",
    "https://openalex.org/W3168114581",
    "https://openalex.org/W3170928047",
    "https://openalex.org/W2966683369",
    "https://openalex.org/W3203974803",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2964067226",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W24089286",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W3204250462",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3198571508",
    "https://openalex.org/W3188447078",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2964194231",
    "https://openalex.org/W12634471",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W3214395274",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3212487317",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2964303773"
  ],
  "abstract": "Large-scale contrastive vision-language pre-training has shown significant progress in visual representation learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm was introduced in \\cite{radford2021learning} to directly learn to align images with raw texts in an open-vocabulary setting. On downstream tasks, a carefully chosen text prompt is employed to make zero-shot predictions.~To avoid non-trivial prompt engineering, context optimization \\cite{zhou2021coop} has been proposed to learn continuous vectors as task-specific prompts with few-shot training examples.~In this paper, we show that there is an alternative path to achieve better vision-language models other than prompt tuning.~While prompt tuning is for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with feature adapters on either visual or language branch. Specifically, CLIP-Adapter adopts an additional bottleneck layer to learn new features and performs residual-style feature blending with the original pre-trained features.~As a consequence, CLIP-Adapter is able to outperform context optimization while maintains a simple design. Experiments and extensive ablation studies on various visual classification tasks demonstrate the effectiveness of our approach. Code is released at t https://github.com/gaopengcuhk/CLIP-Adapter.",
  "full_text": "Springer Nature 2021 LATEX template\nCLIP-Adapter: Better Vision-Language Models with Feature\nAdapters\nPeng Gao1‚Ä†, Shijie Geng3‚Ä†, Renrui Zhang1,2‚Ä†, Teli Ma1, Rongyao Fang2, Yongfeng\nZhang3, Hongsheng Li2,4* and Yu Qiao1*\n1Shanghai AI Laboratory, Shanghai, China.\n2CUHK MMLab, Hong Kong SAR, China.\n3Rutgers University, New Jersey, US.\n4Centre for Perceptual and Interactive Intelligence (CPII), Hong Kong SAR, China.\n*Corresponding author(s). E-mail(s): hsli@ee.cuhk.edu.hk; qiaoyu@pjlab.org.cn;\nContributing authors: gaopeng@pjlab.org.cn; sg1309@rutgers.edu;\nzhangrenrui@pjlab.org.cn;\n‚Ä†These authors contributed equally to this work.\nAbstract\nLarge-scale contrastive vision-language pretraining has shown significant progress in visual representa-\ntion learning. Unlike traditional visual systems trained by a fixed set of discrete labels, a new paradigm\nwas introduced in [ 50] to directly learn to align images with raw texts in an open-vocabulary setting.\nOn downstream tasks, a carefully chosen text prompt is employed to make zero-shot predictions. To\navoid non-trivial prompt engineering, context optimization [ 75] has been proposed to learn continuous\nvectors as task-specific prompts with few-shot training examples. In this paper, we show that there is\nan alternative path to achieve better vision-language models other than prompt tuning. While prompt\ntuning is for the textual inputs, we propose CLIP-Adapter to conduct fine-tuning with feature adapters\non either visual or language branch. Specifically, CLIP-Adapter adopts an additional bottleneck layer to\nlearn new features and performs residual-style feature blending with the original pretrained features. As\na consequence, CLIP-Adapter is able to outperform context optimization while maintaining a simple\ndesign. Experiments and extensive ablation studies on various visual classification tasks demonstrate\nthe effectiveness of our approach. Code is available at https://github.com/gaopengcuhk/CLIP-Adapter.\nKeywords: Feature Adapter, Vision-Language Model, Few-shot learning, Open-Vocabulary\n1 Introduction\nVisual understanding tasks, such as classifica-\ntion [12, 15, 20, 25, 32, 46, 60], object detection [ 5,\n16, 52], and semantic segmentation [42], have been\nimproved significantly based on the better architec-\nture designs and large-scale high-quality datasets.\nUnfortunately, collecting large-scale high-quality\ndatasets for every visual task is labor-intensive and\ntoo expensive to scale. To solve the problem, the\n‚Äúpretraining-finetuning‚Äù paradigm, namely pretrain-\ning on large-scale datasets like ImageNet [ 32] and\nthen fine-tuning on a variety of downstream tasks,\nhas been widely adopted in vision domain. How-\never, such approaches still need a huge amount of\nannotations for fine-tuning on many downstream\n1\narXiv:2110.04544v2  [cs.CV]  25 Mar 2025\nSpringer Nature 2021 LATEX template\n2 CLIP-Adapter\nVisualEncoder\nTextEncoder\nVisualAdapter\naphotoofadog TextAdapter\nPredictionVisualEncoder\nTextEncoder\n[ùëâ!] dog\nPredictionGradient[ùëâ\"][ùëâ#]‚Ä¶\n Gradient\nCoOp CLIP-AdapterBack-propagateFore-propagate\nLearnable\nFrozen\nGPU\tMemory:\t7193\tMiB GPU\tMemory:\t2227\tMiBTrain\tTime:\t14h\t40min Train\tTime:\t50min\nFig. 1 Comparison of CoOp and our CLIP-Adapter. Instead of using learnable prompts, CLIP-Adapter adopts lightweight\nresidual-style adapters after CLIP‚Äôs text and visual encoders. During training, CLIP-Adapter does not require to calculate\nand propagate the gradients through CLIP‚Äôs encoder, which thus consumes less computational cost.\ntasks. Recently, Contrastive Language-Image Pre-\ntraining (CLIP) [ 50] was proposed for solving\nvision tasks by exploiting contrastive learning\nwith large-scale noisy image-text pairs. It achieves\ninspirational performances on various visual clas-\nsification tasks without any annotations (i.e.,\nzero-shot transfer) by putting visual categories into\nsuitable hand-crafted template as prompts.\nAlthough prompt-based zero-shot transfer\nlearning showed promising performances, design-\ning good prompts remains an engineering problem\nthat demands substantial time and domain knowl-\nedge. To address the issue, Context Optimization\n(CoOp) [75] further proposed to learn continuous\nsoft prompts with few-shot examples for replacing\nthe carefully-chosen hard prompts. CoOp brings\nabout significant improvement on few-shot clas-\nsification over both zero-shot CLIP and linear\nprobe CLIP settings, exhibiting the potential of\nprompt tuning on large-scale pretrained vision-\nlanguage models. In this paper, we propose a\ndifferent approach for better adapting vision-\nlanguage models with feature adapters instead\nof prompt tuning. Different from CoOp that per-\nforms soft prompt optimization, we simply conduct\nfine-tuning on the light-weight additional feature\nadapters. Because of the over-parameterization\nof CLIP and lack of enough training examples,\nnaive finetuning would lead to overfitting on spe-\ncific datasets and the training process would be\nvery slow owing to the forward and backward\npropagations across all CLIP layers.\nMotivated by the adapter modules in\nparameter-efficient transfer learning [24], we pro-\npose CLIP-Adapter, which only finetunes a small\nnumber of additional weights instead of optimiz-\ning all parameters of CLIP, as shown in Figure 1.\nCLIP-Adapter adopts a lightweight bottleneck\narchitecture to prevent the potential overfitting\nproblem of few-shot learning by reducing the\nnumber of parameters. Meanwhile, CLIP-Adapter\nis different from [ 24] in two important aspects:\nCLIP-Adapter only adds two additional linear\nlayers following the last layer of vision or lan-\nguage backbone. This is because the original\npretrained encoder of CLIP has already been\nequipped with strong representation capabilities,\nso it only requires a lightweight adaption in the\nform of residuals. In contrast, the original adapter\nmodules are inserted into all layers of the lan-\nguage backbone; In addition, CLIP-Adapter mixes\nthe original zero-shot visual or language embed-\nding with the corresponding finetuning feature\nvia residual connection. Through such a ‚Äúresidual-\nstyle blending‚Äù, CLIP-Adapter can simultaneously\nexploit the knowledge stored in the original CLIP\nand the freshly learned knowledge originated from\nthe few-shot training examples. Figure 2 gives an\nintuitive illustration of the differences between\nour CLIP-Adapter and other visual classification\narchitectures. Overall, our contributions can be\nsummarized as follows:\n‚Ä¢ We propose CLIP-Adapter that conducts\nresidual-style feature blending to achieve efficient\nfew-shot transfer learning via fine-tuning.\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 3\n‚Ä¢ Compared with CoOp, CLIP-Adapter achieves\nbetter few-shot classification performance while\nhaving a much simpler design, demonstrating\nthat CLIP-Adapter is a promising alternative to\nprompt tuning.\n‚Ä¢ We perform extensive ablation studies of CLIP-\nAdapter on eleven classification datasets to\nanalyze its characteristics.\n2 Related Work\nModel Fine-Tuning. Deep neural network is\ndata-hungry. However, collecting and annotating\nlarge amount of high-quality data is costly and\neven impossible for some special domains. The\n‚Äúpretraining-finetuning paradigm‚Äù offers a good\nsolution to different computer vision [ 20, 32, 54]\nand natural language processing [ 8, 10, 11] tasks\nand has been widely adopted for many years. For\ndata-efficient finetuning over downstream tasks,\nadapter modules [ 24] is proposed to freeze the\nweight of backbones and insert learnable linear lay-\ners to each Transformer layer. Subsequent work\nsuch as Parallel Adapter [ 19] and VL-Adapter\n[58] further extend [ 24]‚Äôs ability to language and\nmultimodal tasks. Some other methods, e.g., black-\nbox tuning [ 56], ladder side-tuning [ 57], sparse\nstructure search[26], and Scaling&Shifting [38] also\nintroduce different techniques for adapting large\nlanguage and vision models in a parameter-efficient\nmanner. Different from existing approaches, the\nproposed CLIP-Adapter applies a simple residual\ntransformation layer over the feature embedding\nor classifier weight generated by CLIP. Thanks\nto the residual connection and bottleneck linear\nlayer, CLIP-Adapter can improve the performance\nof CLIP on few-shot learning setting and achieve\nsuperior performance than the recently proposed\nCoOp. To alleviate the performance gap under\ndistribution shifting, WiSE-FT [ 66] proposes a\npost-ensemble method for improving CLIP‚Äôs out-of-\ndistribution robustness. While our CLIP-Adapter\nadopts a learnable gating ratio to dynamically\nbalance and mix the knowledge from the original\nfeatures and CLIP-Adapter‚Äôs outputs throughout\nthe training stage.\nPrompt Design. Prompt design [40] are popu-\nlarized by the success of GPT series [4, 49]. GPT-3\nshowed that a huge autoregressive language model\ntrained on a large-scale dataset can perform any\nNLP tasks in a zero-shot or few-shot style with-\nout finetuning the base architecture. Following\nthe brand new ‚Äúpretrain, prompt, and predict‚Äù\nparadigm [40], various prompt design approaches\nare proposed recently. As the earliest attempt, one\ntype of them focus on tuning pretrained language\nor vision-language models with natural language\ndiscrete prompts [1, 17, 61, 65, 68, 69] or prompt\nengineering by mining or generating proper natural\nlanguage discrete prompts [17, 29, 53]. In contrast,\ncontinuous prompts circumvent the restriction\nfrom pretrained language models and are adopted\nby various approaches such as [ 18, 33, 37, 41] on\nNLP tasks. Recently, they have also been intro-\nduced to vision tasks [ 28]. Motivated by GPT-3,\nCLIP trains a large contrastive learning model over\n400 million image-text pairs and demonstrates the\npotential for prompt-based zero-shot visual classi-\nfication. With CLIP as the backbone, CoOp [ 75]\nfurther shows that optimizing continuous prompts\ncan largely surpass manually-designed discrete\nprompts on vision tasks. In this paper, we demon-\nstrate that prompt tuning is not the only path to\nbetter vision-language models. Fine-tuning with a\nsmall portion of parameters can also achieve com-\nparable or even better performance on vision tasks\nyet with much simpler design.\nVision-Language Models. Exploring the inter-\naction between vision and language is a core\nresearch topic in artificial intelligence. Previously,\nattention-based approaches such as bottom-up\ntop-down attention [2], BAN [30], Intra-Inter [14],\nand MCAN [ 70] had dominated visual-language\ntasks. Inspired by the success of BERT [ 10], ViL-\nBERT [43], LXMERT [59], UNITER [6], Oscar [36],\nALBEF [35], and BEiT [64] further push the bound-\nary of multimodal reasoning. Recently, CLIP [ 50]\nand ALIGN [27] demonstrates the power of visual-\nlanguage contrastive representation learning. They\nachieve astonishing results on a wide spectrum of\nvision tasks, including 3D [ 74, 76], video [39], and\ndepth [71] understanding. To further close the gap\nbetween CLIP and supervised training, CoOp pro-\nposes a continuous prompt optimization method\nfor improving the performance on visual classifica-\ntion tasks. While CoOp improves vision-language\nmodels from the perspective of prompt design,\nour CLIP-Adapter explores simple finetuning with\nlightweight feature adapters.\nSpringer Nature 2021 LATEX template\n4 CLIP-Adapter\nCNN\nTransformer\ncar\nplane\ndog\nflower\nVisual Embedding\n Classifier Weight \nMatMul \ncar plane dog flower\nNaive Classification\ncar\nplane\ndog\nflower\nCLIP\n[CLS] [V]1 [V]2 [V]n dog\ncar plane dog flower\ncar\nplane\ndog\nflower\nCoOp\ncar plane dog flower\nCLIP-Adapter\ncar\nplane\ndog\nflower\n[CLS] a photo of\n a dog\nVisual Adapter\nText-EncoderText-Encoder\nText Adapter\nText-Encoder\n[CLS] a photo of\n a dog\nFig. 2 Comparison of different visual classification architectures. The image in the top row with a green region shows the\nnaive pipeline for image classification [ 32], where f and W represents the visual feature and classifier weight respectively.\nThe following pink, yellow and blue regions represent the pipeline of CLIP [ 50], CoOp [75], and our proposed CLIP-Adapter\nrespectively.\n3 Our Approach\nIn this section, we introduce the proposed CLIP-\nAdapter. In Section 3.1, we first revisit CLIP and\nCoOp from the perspective of classifier weight gen-\neration. In Section 3.2, we elaborate the details\nof the proposed CLIP-Adapter. In Section 3.3, we\nprovide several variants of CLIP-Adapter.\n3.1 Classifier Weight Generation for\nFew-Shot Learning\nLet us first review the basic framework for image\nclassification using deep neural networks: Given an\nimage I ‚àà RH√óW√ó3, where H and W stands for\nthe height and width of the image respectively, a\nneural network backbone that consists of cascade of\nbasic components (e.g., CNN, Transformer [ 62] or\nthe mixture of both) takes I and transforms it into\na feature manifold f ‚àà RD, where D represents\nthe feature dimensionality. To perform classifica-\ntion, the image feature vector f is then multiplied\nwith a classifier weight matrix W ‚àà RD√óK, where\nK represents the number of classes to be classi-\nfied. After matrix multiplication, we can obtain a\nK-dimensional logit. A Softmax function is used to\nconvert the logit into a probability vector p ‚àà RK\nover the K classes. The whole process can be\nwritten as the following equations:\nf = Backbone(I), pi = exp(WT\ni f)/œÑPK\nj=1 exp(WT\nj f)/œÑ\n, (1)\nwhere œÑ stands for the temperature of Softmax,\nWi represents the prototype weight vector for class\ni, and pi denotes the probability of category i.\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 5\nDifferent from supervised training, in the paper,\nwe are interested in image classification with few-\nshot examples. Training the backbone and classifier\ntogether from scratch with a small number of sam-\nples is prone to overfit certain datasets and might\nsuffer from severe performance drop on the test\nsplit. Typically, the representative paradigm on\nfew-shot learning is to first pretrain the backbone\non a large-scale dataset, and then transfer the\nlearned knowledge to downstream tasks by either\nconducting zero-shot prediction directly or further\nfine-tuning on few-shot examples.\nCLIP adheres to the zero-shot transfer style ‚Äì\nit first pretrains the visual backbone and textual\nencoder through contrastive learning on large-scale\nnoisy image-text pairs, and then after pretraining,\nCLIP directly performs image classification with-\nout any finetuning. Given an image classification\ndownstream dataset that contains K categories\nwith their natural language name {C1, . . . , Ck},\nCLIP constructs to place each category name Ci\ninto the pre-defined hard prompt templateH. Then\nthe language feature extractor encodes the result-\ning prompt as a classifier weight Wi. We denote\nthe classifier weight generation process as below:\nWi = Text-Encoder(Tokenizer([H; Ci])). (2)\nAlternatively, CoOp adopts continuous\nprompts instead of hand-crafted hard prompts.\nCoOp creates a list of random-initialized learnable\nsoft tokens S ‚àà RL√óD, where L stands for the\nlength of the soft token sequence. The soft token\nsequence S is then concatenated to each class\nname Ci and thus form a prompt. We represent\nthe whole process as\nWi = Text-Encoder([S; Tokenizer(Ci)]). (3)\nFor both CLIP and CoOp, with the generated\nclassifier weight Wi, where i ‚àà {1, ¬∑¬∑¬∑ , K}, we\ncan thus calculate the prediction probability pi for\nclass i by the previously mentioned Eq. (1).\n3.2 CLIP-Adapter\nUnlike CoOp‚Äôs prompt tuning, we present an\nalternative framework for achieving better vision-\nlanguage models on few-shot image classification\nby fine-tuning additional feature adapters. We\nclaim that the previous widely-adopted ‚Äúpretrain-\nfinetuning‚Äù paradigm would fail in finetuning the\nwhole CLIP backbone under the few-shot setting\ndue to the enormous amount of parameters and the\nshortage of training examples. Hence, we propose\nCLIP-Adapter, which only appends a small number\nof additional learnable bottleneck linear layers to\nCLIP‚Äôs language and image branches while keeping\nthe original CLIP backbone frozen during few-shot\nfine-tuning. However, naive fine-tuning with addi-\ntional layer may still fall into overfitting on the\nfew-shot examples. To deal with overfitting and\nimprove the robustness of CLIP-Adapter, we fur-\nther adopt residual connections to dynamically\nblend the fine-tuned knowledge with the original\nknowledge from CLIP‚Äôs backbone.\nSpecifically, given the input image I and a\nset of categories‚Äô natural language names {Ci}K\ni=1,\nthe image feature f and classifier weight W from\nthe original CLIP backbone are computed with\nEquations (1) and (2). Afterwards, two learnable\nfeature adapters, Av(¬∑) and At(¬∑), each of which\ncontains two layers of linear transformations, are\nintegrated to transform f and W, respectively. We\nadopt a residual connection for the feature adapter\nto avoid forgetting the original knowledge encoded\nby the pretrained CLIP. Two constant valuesŒ± and\nŒ≤ are employed as ‚Äúresidual ratio‚Äù to help adjust\nthe degree of maintaining the original knowledge\nfor better performance. In summary, the feature\nadapters can be written as\nAv(f) = ReLU(fT Wv\n1)Wv\n2, (4)\nAt(W) = ReLU(WT Wt\n1)Wt\n2, (5)\nwhere Wv\n1, Wv\n2 and Wt\n1, Wt\n2 are the weights of\nbottleneck linear layers for visual branch and text\nbranch, respectively. The new knowledge captured\nvia finetuning is added with the original features\nvia residual connections:\nf‚ãÜ = Œ±Av(f)T + (1 ‚àí Œ±)f, (6)\nW‚ãÜ = Œ≤At(W)T + (1 ‚àí Œ≤)W. (7)\nAfter obtaining new image feature f‚ãÜ and classifier\nweight W‚ãÜ, we also adopt Equation(1) to calculate\nthe category probability vector P = {pi}K\ni=1 and\npredict the image category by selecting the class ÀÜi\nthat has the highest probability: ÀÜi = arg maxi pi.\nSpringer Nature 2021 LATEX template\n6 CLIP-Adapter\nDuring the few-shot training, the weights of\nAv(¬∑) and At(¬∑) are optimized with the contrastive\nloss following original CLIP [50] as below:\nLŒ∏ = ‚àí 1\nN\nNX\ni\nlog exp\n\u0000\nW‚ä§\ni fi/œÑ\n\u0001\nPN\nj=1 exp\n\u0000\nW‚ä§\nj fi/œÑ\n\u0001,\nwhere N is the total number of training examples;\nŒ∏ = {Wv\n1, Wv\n2, Wt\n1, Wt\n2} represents all learnable\nparameters of CLIP-Adapter.\n3.3 Variants of CLIP-Adapter\nOur CLIP-Adapter has three structural variants: 1)\nonly fine-tuning the feature adapter for the image\nbranch while keeping the text branch frozen; 2)\nonly fine-tuning the feature adapter for the text\nbranch while keeping the image branch frozen; 3)\nfine-tuning both the image and text branches of\nCLIP backbone. In terms of the hyperparameters\nŒ± and Œ≤, we observe that different datasets have\ndifferent optimal Œ± and Œ≤ values. Choosing the\nhyperparameters manually is time-consuming and\nlaborious. Thus we also explore learning Œ± and\nŒ≤ in a differentiable manner by setting them as\nlearnable parameters. In this way, Œ± and Œ≤ can be\ndynamically predicted from either visual feature\nor classifier weight via a hypernetwork Q: Œ±, Œ≤=\nQ(f, W).\n4 Experiments\n4.1 Experimental Setups\nDatasets. Following CLIP and CoOp, we\nselect 11 image classification datasets to vali-\ndate CLIP-Adapter‚Äôs effectiveness, namely Ima-\ngeNet [ 9], StanfordCars [ 31], UCF101 [ 55], Cal-\ntech101 [ 13], Flowers102 [ 47], SUN397 [ 67],\nDTD [ 7], EuroSAT [ 21], FGVCAircraft [ 45],\nOxfordPets [48], and Food101 [3]. Specifically, we\ntrain our CLIP-Adapter under the few-shot setups\nof 1, 2, 4, 8, 16 shots and then test the tuned mod-\nels on full test splits. Considering the randomness\nof few-shot training, we run every setting three\ntimes and report the average accuracy. We conduct\nall experiments on a single NVIDIA A100 GPU.\nImplementation Details. The first variant of\nCLIP-Adapter is adopted by default if not specified,\nwhich finetunes the image feature while freezing\nthe classifier weight. In other words, it only imple-\nments CLIP-Adapter for the visual adapter. The\nresults of other variants that activate text adapter\nare presented in Section 4.2.5. We use the same\ntraining hyperparameters as CoOp, including a\nbatch size of 32 and a learning rate of 1 √ó 10‚àí5\nfor all datasets except for the residual ratio Œ±. We\nperform hyperparameter searching over different\nvalue selections of Œ± for each dataset and report\nthe best performance among all searching spaces.\nWe use ResNet-50 [ 20] as the visual backbone\n(visual encoder) and 12-layer Transformer as classi-\nfier weight generator (textual encoder). The hidden\nembedding dimensionality of both visual and text\nbottleneck layers is set to 256, which is a quarter of\nthe original embedding dimensionality. In contrast\nto the learnable continuous prompts in CoOp, sim-\nple hand-crafted hard prompts are utilized as the\ntext inputs of CLIP-Adapter, which is the same as\nCLIP. For generic-category image datasets, such\nas ImageNet, we adopt ‚Äúa photo of a {class}‚Äù as\nthe hard prompt template. For fine-grained clas-\nsification datasets, we specify its corresponding\ndomain keyword in the template for a better per-\nformance, for instance, ‚Äúa centered satellite photo\nof {class}‚Äù for EuroSAT, and similarly for other\nfine-grained datasets.\nNotes on Image Pre-processing. There are two\nimage pre-processing methods adopted by existing\nmethods. The first one is adopted by CLIP and\nthe second one is reported in CoOp. We denote\nthem as CLIP-style and CoOp-style preprocess-\nings, respectively. They are both composed of\nrandom cropping, resizing, and random horizon-\ntal flip transformations. Their differences lie in the\nresizing. The CLIP-style pre-processing resizes the\ncropped image‚Äôs short side to 224 while keeping its\noriginal aspect ratio. In contrast, the CoOp-style\nresizes an image‚Äôs both sides to 224. By default,\nwe follow CoOp-style preprocessing. In Section A\nof the Appendix, we present the result compar-\nison under the CLIP-style preprocessing which\npreserves the original aspect ratios of the cropped\nimages.\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 7\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n35\n40\n45\n50\n55\n60\n65\n70\n75Score (%)\nZero-shot\nCLIP\nAverage over 11 datasets\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n20\n25\n30\n35\n40\n45\n50\n55\n60Score (%)\nZero-shot\nCLIP\nImageNet\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n70\n75\n80\n85\n90Score (%)\nZero-shot\nCLIP\nCaltech101\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n30\n40\n50\n60\n70\n80Score (%)\nZero-shot\nCLIP\nOxfordPets\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n30\n40\n50\n60\n70Score (%)\nZero-shot\nCLIP\nStanfordCars\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n60\n65\n70\n75\n80\n85\n90\n95Score (%)\nZero-shot\nCLIP\nFlowers102\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n30\n40\n50\n60\n70Score (%)\nZero-shot\nCLIP\nFood101\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n10\n15\n20\n25\n30\n35\n40Score (%)\nZero-shot\nCLIP\nFGVCAircraft\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n35\n40\n45\n50\n55\n60\n65\n70Score (%)\nZero-shot\nCLIP\nSUN397\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n25\n30\n35\n40\n45\n50\n55\n60\n65Score (%)\nZero-shot\nCLIP\nDTD\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n20\n30\n40\n50\n60\n70\n80Score (%)\nZero-shot\nCLIP\nEuroSAT\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n40\n45\n50\n55\n60\n65\n70\n75Score (%)\nZero-shot\nCLIP\nUCF101\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\nFig. 3 Main results of few-shot learning on 11 datasets. CLIP-Adapter consistently shows better performance over previous\nbaselines across different training shots.\n4.2 Comparison on Few-Shot\nLearning\n4.2.1 Baseline Models\nWe compare our CLIP-Adapter with three baseline\nmodels ‚Äì the Zero-shot CLIP [50], Linear probe\nCLIP [50], and CoOp [75]. In our implementation,\nCLIP-Adapter shares the same hand-crafted hard\nprompts with Zero-shot CLIP [ 50] for fair compar-\nisons. CoOp [75] substitutes discrete tokens with\nlearnable continuous vectors. Thus there are multi-\nple candidate positions to place the class token in\nthe prompt template, namely at the front, in the\nmiddle, or at the end. Here, we choose CoOp‚Äôs best-\nperformance variant ‚Äì placing the class token at the\nend of the 16-token soft prompt and shares such\na context among different classes. Linear probe\nCLIP [50] trains an additional linear classifier on\ntop of its visual encoder and follows a few-shot\ntraining manner. It is different from our bottleneck\nadapter that finetunes both the image feature and\nclassifier weight in a dynamic and residual fashion.\nSpringer Nature 2021 LATEX template\n8 CLIP-Adapter\nTable 1 Comparison of training time, parameters and classification accuracy of 16-shot learning on ImageNet [9] dataset.\nCLIP-Adapter shows the best cost-accuracy trade-off. We test Linear Probe CLIP with Scikit-learn on CPU following CoOp,\nand implement other methods with batch size 32 and adopt ResNet-50 as the image backbone on a single NVIDIA A100 GPU.\nModels Train Time Parameters GPU Mem. Infer Speed Accuracy\nZero-shot CLIP 0 0 2227 MiB 10.2ms 55.41%\nLinear Probe CLIP 13min 1.02M - - 53.44%\nCoOp 14h 40min 0.02M 7193 MiB 299.6ms 60.46%\nCLIP-Adapter 50min 0.52M 2227 MiB 10.6ms 61.33%\n4.2.2 Performance Comparison &\nAnalysis\nThe main results are presented in Figure 3. From\nthe average accuracy over the 11 datasets shown\nat the top-left corner, CLIP-Adapter clearly out-\nperforms the other three baseline models on all\ndifferent shot setups, demonstrating its supe-\nrior few-shot learning capacity. It is especially\nworth noticing that, under extreme conditions\nsuch as 1-shot or 2-shot training setup, CLIP-\nAdapter achieves larger performance improvements\nagainst the baselines, which indicates a better\ngeneralization ability in data-deficient training\ncircumstances.\nCompared with Zero-shot CLIP [50], our\nCLIP-Adapter achieves significant performance\ngains over all 11 datasets. The ranked absolute\nperformance improvements for all 11 datasets\nunder the 16-shot training setup are shown in\nFigure 4. For the first five fine-grained datasets,\nfrom EuroSAT to FGVCAircraft, CLIP-Adapter\nachieves huge performance boosts ranging from\n20% to 50%. The improvements become smaller\non more challenging and generic datasets, such\nas Caltech101 and ImageNet. As for OxfordPets\nand Food101, CLIP-Adapter shows relatively lim-\nited improvements, since the original results of\nZero-shot CLIP are already quite decent.\nCompared with Linear probe CLIP [50],\nwhich follows a similar style to finetune the pre-\ntrained vision-language models, CLIP-Adapter\nalso shows comprehensive performance advantages.\nUnder 1-shot and 2-shot training setups, Linear\nprobe CLIP barely reaches the performance of Zero-\nshot CLIP, but CLIP-Adapter can always surpass\nZero-shot CLIP and exceed Linear probe CLIP by\na large margin. For instance, the absolute margin\nof 1-shot and 2-shot training setups are 53 .6% and\n42.16% for OxfordPets, and 37 .17% and 27 .58%\nfor ImageNet, respectively.\nEuroSAT\nFlowers102\nDTD\nStanfordCarsFGVCAircraft\nUCF101SUN397\nCaltech101ImageNetOxfordPets\nFood101\n0\n10\n20\n30\n40\n50\n60Absolute Improvement (%)\n51.32\n31.89\n25.4324.89\n21.96\n15.45\n11.9\n9.01\n5.83 4.19\n2.02\nCLIP-Adapter vs. Zero-shot CLIP\nFig. 4 Absolute performance gain of CLIP-Adapter against\nhand-crafted prompts on different datasets.\nCompared with CoOp [75], although it has\nalready gained huge improvements over Zero-shot\nCLIP, CLIP-Adapter still outperforms CoOp on\nall datasets and different shot settings. Note that\nCLIP-Adapter handles few-shot learning from\na totally different perspective (i.e., fine-tuning)\ninstead of CoOp‚Äôs prompt tuning. This sug-\ngests finetuning lightweight adapters with residual\nconnections for prompt-fixed pretrained vision-\nlanguage models can achieve better performance\nthan prompt engineering [40].\n4.2.3 Efficiency Comparison & Analysis\nIn Table 1, we provide the comparison of param-\neters, training budget, and inference speed for\ndifferent methods. Compared to the baselines,\nCLIP-Adapter achieves the best accuracy while\nmaintaining the best trade-off between parame-\nters and efficiency. Specifically, CLIP-Adapter uses\nonly half amount of parameters as Linear probe\nCLIP does. Although CLIP-Adapter costs 37 more\nminutes than Linear probe CLIP, CLIP-Adapter\nachieves a large 7.89% accuracy boost. Moreover,\nCLIP-Adpater takes 16 √ó less training time and\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 9\n60 70 80 90 100\nScore (%)\nImageNet\nCaltech101\nEuroSAT\nDTD\n61.33\n93.43\n82.85\n66.06\n60.06\n92.78\n81.85\n63.63\n60.7\n93.1\n82.74\n64.78\nVariants of CLIP-Adapter\nvisual adapter\ntext adapter\nvisual + text adapter\nFig. 5 Comparison among different variants of CLIP-\nAdapter.\n29√ó faster inference speed than CoOp. Although\nCoOp only uses a small number of parameters,\nits learnable prompts are equipped in front of the\ntext encoder and require both the forward and\nbackward propagation, as shown by green dotted\nlines in Figure 1. Calculating the weights and gra-\ndients for the large-scale text encoder cost much\nmore GPU memory and training time. In con-\ntrast, our CLIP-Adapter utilizes the hand-crafted\nprompts and only back-propagates the gradi-\nents through the lightweight adapters, achieving\nsuperior computational efficiency.\n4.2.4 Observation on Optimal Residual\nRatio\nInterestingly, we observe the best residual ratio\nŒ±, to some extent, reflects the characteristics of\ndifferent datasets under the ‚Äúpretrain-finetuning‚Äù\nparadigm. A larger semantic gap between pre-\ntrained and finetuning datasets requires CLIP-\nAdapter to learn a higher portion of knowledge\nfrom the newly adapted feature compared to\nthe original CLIP‚Äôs output, thus resulting in a\nlarger optimal residual ratio, and vice versa. For\nfine-grained datasets on specialized domains, like\nEuroSAT of satellite images and DTD of detailed\ntextures, the optimal residual ratio Œ± is usu-\nally located within the range from 0 .6 to 0 .8.\nBy contrast, the best Œ± value of comprehensive\nand generic image datasets (e.g., Caltech-101 and\nImageNet) is often around 0 .2.\n4.2.5 Variants with Text Adapter\nHere, we investigate the other two variants of\nCLIP-Adapter mentioned in Section 3.3 ‚Äì fine-\ntuning the text adapter while keeping the visual\nadapter frozen and finetuning both the text and\nvisual adapters. Rather than manually selecting\nthe residual ratios for each dataset, we utilize learn-\nable parameters Œ± and Œ≤ since it is time-efficient\nand can also achieve satisfactory performance. We\ncompare their performances on four datasets that\ncan be divided into two categories ‚Äì fine-grained\ndatasets (EuroSAT & DTD) and generic datasets\n(Caltech101 & ImageNet). As shown in Figure 5,\nwe can conclude that the text adapter and visual\nadapter perform comparably and both improve the\nclassification accuracy greatly over Zero-shot CLIP.\nIn addition, adopting visual adapter only is better\nthan text adapter only. This indicates that it is\nmore important to conduct image feature adaption\nthan text feature adaption for few-shot image clas-\nsification, since the semantic gap between visual\nfeatures in pretrained and finetuning datasets is\nlarger than that of text features. Surprisingly, com-\nbining both adapters together does not observe a\nbetter performance than visual adapter only. This\ndemonstrates that the text and visual adapters\nmight capture redundant information or even\nconflict with each other.\n4.2.6 Where to Insert CLIP-Adapter?\nBy default, we insert our residual-style adapters\nat the end of CLIP‚Äôs encoder. We also investigate\nother positions in the visual backbone to equip our\nadapter. In Table 2, we adopt ViT-B/16 as the\nvisual backbone and respectively add the visual\nadapter after its 2nd, 4th, 6th, 8th, 10th, and 12th\nlayers, where the 12th-layer variant denotes our\nfinal solution. As shown, our approach achieves\nsuperior performance with minimal computational\ncost when inserted at the end. Inserting at ear-\nlier layers requires more computation resources\nfor back-propagating the gradients and, to some\ndegree, harms the pretrained knowledge in CLIP.\nInserting adapters in all layers yields a total of\n5.20M parameters, which is much heavier-weight\nthan inserting only at the 12th layer (0.52M). The\nlatter can well alleviate the over-fitting issue on\nfew-shot training data, and largely preserve the\npre-trained knowledge of CLIP by inserting the\nadapter at the end.\nSpringer Nature 2021 LATEX template\n10 CLIP-Adapter\nTable 2 Comparison of adapters inserted into different layers of CLIP. We adopt ViT-B/16 as the image backbone for the\nvisual adapter,\nand report the 16-shot ImageNet performance.\nInsert Layers All 12 10 8 6 4 2 0\nAccuracy (%) 67.67 70.88 71.85 70.55 70.03 70.14 69.43 69.05\nGPU Memory (GiB) 5.98 2.22 2.65 2.89 3.23 4.46 4.88 5.14\nParameters (M) 5.20 0.52 0.78 0.78 0.78 0.78 0.78 0.78\nTable 3 Comparison of CLIP-Adapter and existing\nadapter-based methods on ImageNet with the 16-shot setup.\nWe adopt ViT-B/16 as the backbone with visual adapters.\nMethod CLIP-Adapter Houlsby [24] He [19]\nAccuracy (%) 70.88 70.16 70.84\nGPU Memory (GiB) 2.22 6.74 6.06\nParameters (M) 0.52 12.84 9.62\nTable 4 Comparison of CLIP-Adapter and ELEVATER\nbenchmark approaches on ImageNet with the 16-shot setup.\nWe adopt ViT-B/16 as the backbone with visual adapters.\nMethod ELEVATER [34] CoOp CLIP-Adapter\nR-2P L-2P L-1P\nAccuracy (%) 68.94 69.77 70.38 70.16 70.88\n4.2.7 Comparison with other Adapter\nMethods\nIn Table 3, we equip CLIP with other existing\nadapter-based methods [ 19, 24] and compare with\nour CLIP-Adapter. As shown, our approach out-\nperforms them in terms of both performance and\nefficiency. This is because we largely preserve the\nknowledge obtained from CLIP pretraining by\ninserting adapters at the end with residual con-\nnections, while others adopt non-residual forms\nand insert them densely in the middle of the back-\nbone, which adversely influences CLIP‚Äôs pretrained\nknowledge and results in overfitting.\n4.2.8 Comparison with ELEVATER\nBenchmark Baselines\nIn Table 4, we compare CLIP-Adapter with the\nnewly proposed ELEVATER benchmark [ 34] for\npretrained vision-language models. ELEVATER\nincludes three baselines: Random-Init with Two-\nProjection, Language-Init with Two-Projection,\nand Language-Init with One-Projection. As shown\nin the table, compared to training additional pro-\njection layers initialized by randomness on the text\nencoder, our CLIP-Adapter achieves higher clas-\nsification accuracy, since the residual design can\nlargely preserve the pretrained knowledge in CLIP.\n4.3 Visualization of Learned\nManifold\nWe use t-SNE [ 44] to visualize the manifold\nof CLIP, CoOp, CLIP-Adapter without residual\nconnections, and CLIP-Adapter with residual con-\nnections after training them on the EuroSAT\ndataset. The t-SNE visualization results are pre-\nsented in Figure 6, where the numbers 0 to 9\nstand for the categories of AnnualCrop, Forest,\nHerbaceous Vegetation Land , Highway or Road ,\nIndustrial Buildings , Pasture Land , Permanent\nCrop Land, Residential Buildings, River, Sea or\nLake, respectively. It is clearly illustrated that in\nhigh-dimensional classification space, the CLIP-\nAdapter with residual connections in sub-figure\n(d) shows much more obvious separation of image\nfeatures belong to different categories. As for the\nconfusing categories such as Highway or Road (red\npoints), Permanent Crop Land (pink points), and\nPasture Land (brown points), compared with\nother methods, our CLIP-Adapter is more effec-\ntive in detecting the similarities among the image\nmanifolds from the same class. In summary, the\nvisualization results prove that CLIP-Adapter is\ngood at learning better feature manifolds under\nfew-shot setups.\n4.4 Ablation Studies\nIn this section, we perform several ablation\nstudies for CLIP-Adapter. We choose the best-\nperformance variant which only activates the visual\nadapter, and select two datasets ‚Äì DTD & Ima-\ngeNet, serving as the representatives of fine-grained\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 11\nTable 5 Ablations on varying the hidden dimension of bottleneck layers.\nDimension D D/ 2 D/4 D/8 D/16 D/32\nDTD (%) 65.03 65.62 66.06 64.93 63.75 63.50\nImageNet (%) 59.78 60.03 61.33 60.06 60.02 59.45\nTable 6 Ablations on varying the residual ratio Œ±.\nRatio Œ± 0 0 .2 0 .4 0 .6 0 .8 1 .0\nDTD (%) 40.72 54.59 64.84 66.06 65.96 63.79\nImageNet (%) 60.46 61.33 61.17 60.77 59.79 59.05\nFig. 6 Visualization of different learned feature manifolds via t-SNE.\nand generic datasets, to perform the ablation\nstudies.\nTable 7 Few-shot performance on ImageNet using\ndifferent prompt styles.\nPrompt Style Hard Hard Ensemble Hard + Soft\nCLIP-Adapter (%) 61.33 61.68 59.69\nDimension of Bottleneck Layer. We first\nconduct ablations by varying the hidden dimen-\nsion of bottleneck layers. The results are shown\nin Table 5, where D represents the dimension of\nthe original image feature. By reducing the hidden\ndimension from D to D/32, we observe that either\ntoo small or too large intermediate dimensionality\nwill deteriorate the performance significantly and\nthe best bottleneck dimension is D/4, which is able\nto preserve enough semantics without redundancy.\nSpringer Nature 2021 LATEX template\n12 CLIP-Adapter\nTable 8 Ablations of different visual backbones.\nDataset Method RN50 RN101 ViT-B/32 ViT-B/16\nDTD(%) CoOp 62.55 65.37 65.43 67.67\nCLIP-Adapter 66.06 67.02 66.37 70.86\nImageNet(%) CoOp 60.46 64.39 64.92 70.13\nCLIP-Adapter 61.33 64.77 64.99 70.88\nTable 9 Evaluation on robustness to distribution shift.\nDatasets\nSource Target\nImageNet ImageNetV2 ImageNet-Sketch ImageNet-A ImageNet-R\nZero-Shot CLIP 55.41 48.08 31.67 18.63 53.45\nLinear Probe CLIP 53.44 43.40 17.63 11.66 32.63\nCoOp 60.46 52.17 31.14 19.62 53.31\nCLIP-Adapter 61.33 52.67 32.04 20.12 54.75\nTable 10 Comparison of finetuning different components of CLIP-Adapter.\nVisual Encoder Textual Encoder Adapter Accuracy Train Time\n- - ! 61.33% 50min\n! - ! 60.07% 1h 20min\n- ! ! 57.88% 3h+\n! ! ! 52.78% 3h+\nResidual Ratio Œ±. Moreover, we perform abla-\ntion study of the residual ratio Œ±. From Table 6,\nwe can see that the best residual ratio of fine-\ngrained dataset DTD is 0 .6, and that of generic\ndataset ImageNet is 0 .2. This verifies our obser-\nvation in Section 4.2.4 that adapting fine-grained\ndataset requires more new knowledge than old\nknowledge, and the case is opposite to the generic\ndataset. Note that when Œ± equals to 0, it is equiva-\nlent to Zero-shot CLIP since no new knowledge is\nlearned. When Œ± is set to 1 .0, the classification is\nfully rely on the adapted feature (CLIP-Adapter\nw/o Res). However, this is not optimal because\nCLIP-Adapter tends to overfit in such condi-\ntion. Combining Table 6 and Figure 6, we can also\nconclude the advantages of residual connections in\nCLIP-Adapter: 1) avoids overfitting on few-shot\nexamples and improves the generalization ability\nof CLIP-Adapter with the help of zero-shot knowl-\nedge; 2) preserves the freedom for learning better\nimage feature or classifier weight through few-shot\nfine-tuning.\nInfluence of Prompt Styles. In this section,\nwe investigate the influence of different prompt\nstyles on few-shot performance. For ImageNet\ndataset, the default hard prompt used as text\ninputs of CLIP-Adapter is simply ‚Äúa photo of\na {class}‚Äù. Besides, we also try prompt ensem-\nbling [75] of 7 hard prompts. The 7 hard prompt\ntemplates are: ‚Äúitap of a {class}‚Äù, ‚Äúa bad photo\nof the {class}‚Äù, ‚Äúa origami {class}‚Äù, ‚Äúa photo\nof the large {class}‚Äù, ‚Äúa {class} in a video\ngame‚Äù, ‚Äúart of the {class}‚Äù and ‚Äúa photo of\nthe small {class}‚Äù. Another candidate prompt\nstyle is the mixture of hard prompt and learn-\nable soft prompt [ 75]. As shown in Table 7, the\nprompt ensembling strategy slightly outperforms\nhard prompt and achieves the best performance\namong all three prompt styles. The experimental\nresults prove that raw text descriptions contain\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 13\nhelpful knowledge which is effective and robust\nunder different situations. In contrast, soft prompts\ndon‚Äôt have clear meaning and are not a ideal source\nfor zero-shot knowledge.\nAblation of Visual Backbones. We also study\nthe influence of visual backbones on few-shot\nlearning performance (16 shots). There are four\ncandidate visual backbones including ResNet-50,\nResNet-101, ViT-B/32, and ViT-B/16. As reported\nin Table 8, CLIP-Adapter consistently outperforms\nCoOp when we vary the visual backbones on both\nDTD and ImageNet datasets.\nRobustness under Distribution Shift. To fur-\nther validate the robustness of CLIP-Adpater, we\nalso perform experiments to observe performance\nvariation by shifting the distribution. We train\nour CLIP-Adapter on ImageNet [ 9] and respec-\ntively evaluate on four out-of-distribution datasets:\nImageNetV2 [51], ImageNet-Sketch [23], ImageNet-\nA [63], and ImageNet-R [ 22]. As shown in Table 9,\nCLIP-Adapter consistently outperforms other base-\nlines and demonstrates enough robustness against\ndistribution shift.\nFinetuning Whole CLIP vs. CLIP-\nAdapter. To verify the claim that finetuning the\nwhole CLIP would lead to overfitting. We perform\nablation experiments on finetuning different com-\nponents of CLIP-Adapter ( ‚úì denotes unfrozen\nfor training). For finetuning CLIP‚Äôs encoders,\nwe adopt early stopping as suggested to obtain\nthe highest accuracy. From the results presented\nin Table 10, we observe that finetuning either\nCLIP‚Äôs visual or textual encoder would hurt the\nperformance and take more training time. This\nindicates the overfitting of the huge-parameter\nCLIP on the few-shot dataset and the effectiveness\nof the proposed adapter.\n5 Conclusions and Future\nWork\nWe present CLIP-Adapter as an alternative of\nprompt-based approaches for few-shot image classi-\nfication. The CLIP-Adapter revives the ‚Äúpretrain-\nfinetuning‚Äù paradigm by only fine-tuning a small\nnumber of additional bottleneck layers. To fur-\nther improve the generalization ability, we adopt\nresidual connections parameterized by a residual\nratio to dynamically blend zero-shot knowledge\nwith new adapted features. According to the\nexperimental results, CLIP-Adapter outperforms\ncompetitive baselines on eleven image classification\ndatasets under different few-shot setups. Exten-\nsive ablation studies confirm our design and prove\nCLIP-Adapter‚Äôs ability in learning better feature\nmanifolds. In the future, we plan to extend CLIP-\nAdapter to more vision-language applications and\ntasks. We will also combine CLIP-Adapter with\ncache models [72, 77] and enhanced prompts [ 73]\nto further unleash the power of CLIP backbone.\nData Availability Statement\nNo new data were created during the study. All\nexperiments of this manuscript were conducted\n(training and evaluation) on 11 publicly available\nimage classification datasets [3, 7, 9, 13, 21, 31, 45,\n47, 48, 55, 67].\nAcknowledgement\nThis project is funded in part by the National Natu-\nral Science Foundation of China (No.62206272), by\nthe National Key R&D Program of China Project\n(No.2022ZD0161100), by the Centre for Perceptual\nand Interactive Intelligence (CPII) Ltd under the\nInnovation and Technology Commission (ITC)‚Äôs\nInnoHK, and by the General Research Fund of\nHong Kong RGC Project 14204021. Hongsheng Li\nis a PI of CPII under the InnoHK.\nSpringer Nature 2021 LATEX template\n14 CLIP-Adapter\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n35\n40\n45\n50\n55\n60\n65\n70\n75Score (%)\nZero-shot\nCLIP\nAverage over 11 datasets\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n20\n30\n40\n50\n60Score (%)\nZero-shot\nCLIP\nImageNet\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n70\n75\n80\n85\n90Score (%)\nZero-shot\nCLIP\nCaltech101\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n30\n40\n50\n60\n70\n80\n90Score (%)\nZero-shot\nCLIP\nOxfordPets\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n30\n40\n50\n60\n70Score (%)\nZero-shot\nCLIP\nStanfordCars\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n60\n65\n70\n75\n80\n85\n90\n95Score (%)\nZero-shot\nCLIP\nFlowers102\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n30\n40\n50\n60\n70\n80Score (%)\nZero-shot\nCLIP\nFood101\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n15\n20\n25\n30\n35\n40Score (%)\nZero-shot\nCLIP\nFGVCAircraft\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n35\n40\n45\n50\n55\n60\n65\n70Score (%)\nZero-shot\nCLIP\nSUN397\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n25\n30\n35\n40\n45\n50\n55\n60\n65Score (%)\nZero-shot\nCLIP\nDTD\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n40\n50\n60\n70\n80Score (%)\nZero-shot\nCLIP\nEuroSAT\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\n0 2 4 6 8 10 12 14 16\nNumber of labeled training examples per class\n40\n45\n50\n55\n60\n65\n70\n75Score (%)\nZero-shot\nCLIP\nUCF101\nCLIP-Adapter\nCoOp\nLinear probe CLIP\nZero-shot CLIP\nFig. 7 Results under CLIP-style preprocessing of few-shot learning on 11 datasets. Compared to CoOp-style preprocessing\nin Figure 3 of the main body, all results are boosted and CLIP-Adapter still shows leading performance over previous\nbaselines across different training shots.\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 15\nA Appendix\nResult Comparison under CLIP-Style\nPreprocessing.\nIn Figure 7, we present the result comparison under\nCLIP-style preprocessing of few-shot learning on\n11 datasets. Compared to CoOp-style preprocess-\ning, the performances of all methods are improved\nunder CLIP-style preprocessing. Similar to Figure\n3 of the main body, CLIP-Adapter still outperforms\nother baselines across different shot settings.\nReferences\n[1] Alayrac JB, Donahue J, Luc P, et al (2022)\nFlamingo: a visual language model for few-\nshot learning. In: Oh AH, Agarwal A, Belgrave\nD, et al (eds) Advances in Neural Information\nProcessing Systems\n[2] Anderson P, He X, Buehler C, et al (2018)\nBottom-up and top-down attention for image\ncaptioning and visual question answering. In:\nCVPR\n[3] Bossard L, Guillaumin M, Van Gool L (2014)\nFood-101‚Äìmining discriminative components\nwith random forests. In: European conference\non computer vision, Springer, pp 446‚Äì461\n[4] Brown T, Mann B, Ryder N, et al (2020)\nLanguage models are few-shot learners. In:\nNeurIPS\n[5] Carion N, Massa F, Synnaeve G, et al (2020)\nEnd-to-end object detection with transform-\ners. In: ECCV\n[6] Chen YC, Li L, Yu L, et al (2020) Uniter:\nLearning universal image-text representations.\nIn: ECCV\n[7] Cimpoi M, Maji S, Kokkinos I, et al (2014)\nDescribing textures in the wild. In: Proceed-\nings of the IEEE Conference on Computer\nVision and Pattern Recognition, pp 3606‚Äì3613\n[8] Conneau A, Khandelwal K, Goyal N, et al\n(2020) Unsupervised cross-lingual representa-\ntion learning at scale. In: ACL\n[9] Deng J, Dong W, Socher R, et al (2009)\nImagenet: A large-scale hierarchical image\ndatabase. In: CVPR\n[10] Devlin J, Chang MW, Lee K, et al (2019) Bert:\nPre-training of deep bidirectional transformers\nfor language understanding. In: NAACL-HLT\n[11] Dong L, Yang N, Wang W, et al (2019) Uni-\nfied language model pre-training for natural\nlanguage understanding and generation. In:\nNeurIPS\nSpringer Nature 2021 LATEX template\n16 CLIP-Adapter\n[12] Dosovitskiy A, Beyer L, Kolesnikov A, et al\n(2021) An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In:\nICLR\n[13] Fei-Fei L, Fergus R, Perona P (2004) Learn-\ning generative visual models from few training\nexamples: An incremental bayesian approach\ntested on 101 object categories. In: 2004\nconference on computer vision and pattern\nrecognition workshop, IEEE, pp 178‚Äì178\n[14] Gao P, Jiang Z, You H, et al (2019) Dynamic\nfusion with intra-and inter-modality attention\nflow for visual question answering. In: CVPR\n[15] Gao P, Lu J, Li H, et al (2021) Container:\nContext aggregation network. In: NeurIPS\n[16] Gao P, Zheng M, Wang X, et al (2021)\nFast convergence of detr with spatially mod-\nulated co-attention. In: Proceedings of the\nIEEE/CVF international conference on com-\nputer vision, pp 3621‚Äì3630\n[17] Gao T, Fisch A, Chen D (2021) Making\npre-trained language models better few-shot\nlearners. In: ACL-IJCNLP\n[18] Gu Y, Han X, Liu Z, et al (2022) Ppt: Pre-\ntrained prompt tuning for few-shot learning.\nIn: Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics\n(Volume 1: Long Papers), pp 8410‚Äì8423\n[19] He J, Zhou C, Ma X, et al (2022) Towards\na unified view of parameter-efficient trans-\nfer learning. In: International Conference on\nLearning Representations\n[20] He K, Zhang X, Ren S, et al (2016) Deep\nresidual learning for image recognition. In:\nCVPR\n[21] Helber P, Bischke B, Dengel A, et al (2019)\nEurosat: A novel dataset and deep learning\nbenchmark for land use and land cover clas-\nsification. IEEE Journal of Selected Topics\nin Applied Earth Observations and Remote\nSensing 12(7):2217‚Äì2226\n[22] Hendrycks D, Basart S, Mu N, et al (2021)\nThe many faces of robustness: A critical\nanalysis of out-of-distribution generalization.\nIn: Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pp\n8340‚Äì8349\n[23] Hendrycks D, Zhao K, Basart S, et al (2021)\nNatural adversarial examples. In: Proceedings\nof the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp 15,262‚Äì\n15,271\n[24] Houlsby N, Giurgiu A, Jastrzebski S, et al\n(2019) Parameter-efficient transfer learning for\nnlp. In: International Conference on Machine\nLearning, PMLR, pp 2790‚Äì2799\n[25] Howard AG, Zhu M, Chen B, et al (2017)\nMobilenets: Efficient convolutional neural net-\nworks for mobile vision applications. arXiv\npreprint arXiv:170404861\n[26] Hu S, Zhang Z, Ding N, et al (2022) Sparse\nstructure search for parameter-efficient tuning.\narXiv preprint arXiv:220607382\n[27] Jia C, Yang Y, Xia Y, et al (2021) Scaling\nup visual and vision-language representation\nlearning with noisy text supervision. In: ICML\n[28] Jia M, Tang L, Chen BC, et al (2022) Visual\nprompt tuning. In: ECCV, pp 709‚Äì727\n[29] Jiang Z, Xu FF, Araki J, et al (2020) How can\nwe know what language models know? Trans-\nactions of the Association for Computational\nLinguistics 8:423‚Äì438\n[30] Kim JH, Jun J, Zhang BT (2018) Bilinear\nattention networks. In: NIPS\n[31] Krause J, Stark M, Deng J, et al (2013) 3d\nobject representations for fine-grained cat-\negorization. In: Proceedings of the IEEE\ninternational conference on computer vision\nworkshops, pp 554‚Äì561\n[32] Krizhevsky A, Sutskever I, Hinton GE (2012)\nImagenet classification with deep convolu-\ntional neural networks. In: NIPS\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 17\n[33] Lester B, Al-Rfou R, Constant N (2021) The\npower of scale for parameter-efficient prompt\ntuning. In: EMNLP\n[34] Li C, Liu H, Li LH, et al (2022) ELEVATER:\nA benchmark and toolkit for evaluating\nlanguage-augmented visual models. In: Thirty-\nsixth Conference on Neural Information Pro-\ncessing Systems Datasets and Benchmarks\nTrack\n[35] Li J, Selvaraju R, Gotmare A, et al (2021)\nAlign before fuse: Vision and language rep-\nresentation learning with momentum dis-\ntillation. Advances in neural information\nprocessing systems 34:9694‚Äì9705\n[36] Li X, Yin X, Li C, et al (2020) Oscar:\nObject-semantics aligned pre-training for\nvision-language tasks. In: ECCV\n[37] Li XL, Liang P (2021) Prefix-tuning: Opti-\nmizing continuous prompts for generation. In:\nACL\n[38] Lian D, Zhou D, Feng J, et al (2022) Scaling\n& shifting your features: A new baseline for\nefficient model tuning. Advances in Neural\nInformation Processing Systems 35:109‚Äì123\n[39] Lin Z, Geng S, Zhang R, et al (2022) Frozen\nclip models are efficient video learners. ECCV\n2022\n[40] Liu P, Yuan W, Fu J, et al (2023) Pre-\ntrain, prompt, and predict: A systematic\nsurvey of prompting methods in natural lan-\nguage processing. ACM Computing Surveys\n55(9):1‚Äì35\n[41] Liu X, Zheng Y, Du Z, et al (2021) Gpt under-\nstands, too. arXiv preprint arXiv:210310385\n[42] Long J, Shelhamer E, Darrell T (2015) Fully\nconvolutional networks for semantic segmen-\ntation. In: CVPR\n[43] Lu J, Batra D, Parikh D, et al (2019) Vilbert:\nPretraining task-agnostic visiolinguistic rep-\nresentations for vision-and-language tasks. In:\nNeurIPS\n[44] Van der Maaten L, Hinton G (2008) Visu-\nalizing data using t-sne. Journal of machine\nlearning research 9(11)\n[45] Maji S, Rahtu E, Kannala J, et al (2013) Fine-\ngrained visual classification of aircraft. arXiv\npreprint arXiv:13065151\n[46] Mao M, Zhang R, Zheng H, et al (2021)\nDual-stream network for visual recognition.\nAdvances in Neural Information Processing\nSystems 34:25,346‚Äì25,358\n[47] Nilsback ME, Zisserman A (2008) Automated\nflower classification over a large number of\nclasses. In: 2008 Sixth Indian Conference on\nComputer Vision, Graphics & Image Process-\ning, IEEE, pp 722‚Äì729\n[48] Parkhi OM, Vedaldi A, Zisserman A, et al\n(2012) Cats and dogs. In: 2012 IEEE confer-\nence on computer vision and pattern recogni-\ntion, IEEE, pp 3498‚Äì3505\n[49] Radford A, Wu J, Child R, et al (2019)\nLanguage models are unsupervised multitask\nlearners. OpenAI blog\n[50] Radford A, Kim JW, Hallacy C, et al (2021)\nLearning transferable visual models from nat-\nural language supervision. In: International\nConference on Machine Learning, PMLR, pp\n8748‚Äì8763\n[51] Recht B, Roelofs R, Schmidt L, et al (2019) Do\nimagenet classifiers generalize to imagenet? In:\nInternational Conference on Machine Learn-\ning, PMLR, pp 5389‚Äì5400\n[52] Ren S, He K, Girshick R, et al (2015) Faster r-\ncnn: Towards real-time object detection with\nregion proposal networks. In: NIPS\n[53] Shin T, Razeghi Y, Logan IV RL, et al (2020)\nAutoprompt: Eliciting knowledge from lan-\nguage models with automatically generated\nprompts. In: EMNLP\n[54] Simonyan K, Zisserman A (2015) Very deep\nconvolutional networks for large-scale image\nrecognition. In: ICLR\nSpringer Nature 2021 LATEX template\n18 CLIP-Adapter\n[55] Soomro K, Zamir AR, Shah M (2012) Ucf101:\nA dataset of 101 human actions classes\nfrom videos in the wild. arXiv preprint\narXiv:12120402\n[56] Sun T, Shao Y, Qian H, et al (2022) Black-box\ntuning for language-model-as-a-service. In:\nInternational Conference on Machine Learn-\ning, PMLR, pp 20,841‚Äì20,855\n[57] Sung YL, Cho J, Bansal M (2022) Lst: Lad-\nder side-tuning for parameter and memory\nefficient transfer learning. Advances in Neural\nInformation Processing Systems 35:12,991‚Äì\n13,005\n[58] Sung YL, Cho J, Bansal M (2022) Vl-adapter:\nParameter-efficient transfer learning for vision-\nand-language tasks. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp 5227‚Äì5237\n[59] Tan H, Bansal M (2019) Lxmert: Learning\ncross-modality encoder representations from\ntransformers. In: EMNLP-IJCNLP\n[60] Touvron H, Cord M, Douze M, et al (2021)\nTraining data-efficient image transformers &\ndistillation through attention. In: ICML\n[61] Tsimpoukelli M, Menick JL, Cabi S, et al\n(2021) Multimodal few-shot learning with\nfrozen language models. Advances in Neural\nInformation Processing Systems 34:200‚Äì212\n[62] Vaswani A, Shazeer N, Parmar N, et al (2017)\nAttention is all you need. In: NIPS\n[63] Wang H, Ge S, Lipton Z, et al (2019) Learning\nrobust global representations by penalizing\nlocal predictive power. Advances in Neural\nInformation Processing Systems 32\n[64] Wang W, Bao H, Dong L, et al (2022) Image\nas a foreign language: Beit pretraining for\nall vision and vision-language tasks. arXiv\npreprint arXiv:220810442\n[65] Wang Z, Yu J, Yu AW, et al (2022) SimVLM:\nSimple visual language model pretraining with\nweak supervision. In: International Conference\non Learning Representations\n[66] Wortsman M, Ilharco G, Kim JW, et al (2022)\nRobust fine-tuning of zero-shot models. In:\nProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp\n7959‚Äì7971\n[67] Xiao J, Hays J, Ehinger KA, et al (2010) Sun\ndatabase: Large-scale scene recognition from\nabbey to zoo. In: 2010 IEEE computer society\nconference on computer vision and pattern\nrecognition, IEEE, pp 3485‚Äì3492\n[68] Yao Y, Zhang A, Zhang Z, et al (2021)\nCpt: Colorful prompt tuning for pre-trained\nvision-language models. arXiv preprint\narXiv:210911797\n[69] Yao Y, Chen Q, Zhang A, et al (2022) PEVL:\nPosition-enhanced pre-training and prompt\ntuning for vision-language models. In: Pro-\nceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp\n11,104‚Äì11,117\n[70] Yu Z, Yu J, Cui Y, et al (2019) Deep modu-\nlar co-attention networks for visual question\nanswering. In: CVPR\n[71] Zhang R, Zeng Z, Guo Z (2022) Can language\nunderstand depth? ACM MM 2022\n[72] Zhang R, Zhang W, Fang R, et al (2022) Tip-\nadapter: Training-free adaption of clip for few-\nshot classification. In: ECCV 2022. Springer\nNature Switzerland\n[73] Zhang R, Hu X, Li B, et al (2023) Prompt,\ngenerate, then cache: Cascade of foundation\nmodels makes strong few-shot learners. CVPR\n2023\n[74] Zhang R, Wang L, Qiao Y, et al (2023)\nLearning 3d representations from 2d pre-\ntrained models via image-to-point masked\nautoencoders. CVPR 2023\n[75] Zhou K, Yang J, Loy CC, et al (2022) Learn-\ning to prompt for vision-language models.\nInternational Journal of Computer Vision pp\n1‚Äì12\nSpringer Nature 2021 LATEX template\nCLIP-Adapter 19\n[76] Zhu X, Zhang R, He B, et al (2022) Pointclip\nv2: Adapting clip for powerful 3d open-world\nlearning. ICCV 2023\n[77] Zhu X, Zhang R, He B, et al (2023) Not all\nfeatures matter: Enhancing few-shot clip with\nadaptive prior refinement. ICCV 2023",
  "topic": "Adapter (computing)",
  "concepts": [
    {
      "name": "Adapter (computing)",
      "score": 0.8033766746520996
    },
    {
      "name": "Computer science",
      "score": 0.7941925525665283
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5402793884277344
    },
    {
      "name": "Vocabulary",
      "score": 0.5247766375541687
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.49292707443237305
    },
    {
      "name": "Bottleneck",
      "score": 0.4377145767211914
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4324556589126587
    },
    {
      "name": "Feature learning",
      "score": 0.4318595826625824
    },
    {
      "name": "Language model",
      "score": 0.42344987392425537
    },
    {
      "name": "Natural language processing",
      "score": 0.3553439974784851
    },
    {
      "name": "Speech recognition",
      "score": 0.32777339220046997
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    }
  ],
  "institutions": []
}