{
    "title": "Large language models and humans converge in judging public figures’ personalities",
    "url": "https://openalex.org/W4402650660",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4284547777",
            "name": "Xubo Cao",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2149302410",
            "name": "Michal Kosinski",
            "affiliations": [
                "Stanford University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4361204756",
        "https://openalex.org/W4387378202",
        "https://openalex.org/W6873894026",
        "https://openalex.org/W2754674998",
        "https://openalex.org/W3179093522",
        "https://openalex.org/W4392976263",
        "https://openalex.org/W2162090451",
        "https://openalex.org/W2231666394",
        "https://openalex.org/W2115686068",
        "https://openalex.org/W2251410821",
        "https://openalex.org/W4376117416",
        "https://openalex.org/W4403863303",
        "https://openalex.org/W3101573103",
        "https://openalex.org/W4311408938"
    ],
    "abstract": "Abstract ChatGPT-4 and 600 human raters evaluated 226 public figures’ personalities using the Ten-Item Personality Inventory. The correlation between ChatGPT-4 and aggregate human ratings ranged from r = 0.76 to 0.87, outperforming the models specifically trained to make such predictions. Notably, the model was not provided with any training data or feedback on its performance. We discuss the potential explanations and practical implications of ChatGPT-4's ability to mimic human responses accurately.",
    "full_text": null
}