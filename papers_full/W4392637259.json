{
  "title": "InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems",
  "url": "https://openalex.org/W4392637259",
  "year": 2023,
  "authors": [
    {
      "id": null,
      "name": "Chung, Willy Hoo Choun",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4202039510",
      "name": "Cahyawijaya, Samuel",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4221498621",
      "name": "Lovenia, Holy",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4221498622",
      "name": "Wilie, Bryan",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": null,
      "name": "Fung, Pascale Ngan",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2102229948",
    "https://openalex.org/W4367369802",
    "https://openalex.org/W4385571031",
    "https://openalex.org/W2798914047",
    "https://openalex.org/W2048369091",
    "https://openalex.org/W2998432370",
    "https://openalex.org/W2963201498",
    "https://openalex.org/W4292122324",
    "https://openalex.org/W2963789888",
    "https://openalex.org/W4297728544",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4294974598",
    "https://openalex.org/W3099453223",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W2438667436",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4389518325",
    "https://openalex.org/W3207583316",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W3199079601",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W3106241909",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3088273075",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W3036362489",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2117989772",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W1979299372",
    "https://openalex.org/W3100128199",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4206496297",
    "https://openalex.org/W4318908031",
    "https://openalex.org/W2986193249",
    "https://openalex.org/W3114032074",
    "https://openalex.org/W4392669926",
    "https://openalex.org/W3170632963",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W2910795780",
    "https://openalex.org/W3035337525",
    "https://openalex.org/W4378474059",
    "https://openalex.org/W2979520138",
    "https://openalex.org/W4385570287",
    "https://openalex.org/W2016971098",
    "https://openalex.org/W3114038149",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W2014552602",
    "https://openalex.org/W2999134550",
    "https://openalex.org/W3173119568",
    "https://openalex.org/W4287795696",
    "https://openalex.org/W2087388117",
    "https://openalex.org/W4317897852",
    "https://openalex.org/W4290742115",
    "https://openalex.org/W2132997613",
    "https://openalex.org/W4353015365",
    "https://openalex.org/W3156697766",
    "https://openalex.org/W2964077278",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4297783677",
    "https://openalex.org/W2945475330",
    "https://openalex.org/W4307309259",
    "https://openalex.org/W4389519244",
    "https://openalex.org/W3035301094",
    "https://openalex.org/W4300513200",
    "https://openalex.org/W2104544334",
    "https://openalex.org/W4245816780",
    "https://openalex.org/W4287854472",
    "https://openalex.org/W2997108628",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4386566800",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4365601361",
    "https://openalex.org/W4389009441"
  ],
  "abstract": "Large language models (LLMs) have been used for diverse tasks in natural language processing (NLP), yet remain under-explored for task-oriented dialogue systems (TODS), especially for end-to-end TODS. We present InstructTODS, a novel off-the-shelf framework for zero-shot end-to-end task-oriented dialogue systems that can adapt to diverse domains without fine-tuning. By leveraging LLMs, InstructTODS generates a proxy belief state that seamlessly translates user intentions into dynamic queries for efficient interaction with any KB. Our extensive experiments demonstrate that InstructTODS achieves comparable performance to fully fine-tuned TODS in guiding dialogues to successful completion without prior knowledge or task-specific data. Furthermore, a rigorous human evaluation of end-to-end TODS shows that InstructTODS produces dialogue responses that notably outperform both the gold responses and the state-of-the-art TODS in terms of helpfulness, informativeness, and humanness. Moreover, the effectiveness of LLMs in TODS is further supported by our comprehensive evaluations on TODS subtasks: dialogue state tracking, intent classification, and response generation. Code and implementations could be found here1 .",
  "full_text": "Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of\nthe Asia-Paciﬁc Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1–21\nNovember 1–4, 2023. ©2023 Association for Computational Linguistics\n1\nInstructTODS: Large Language Models for End-to-End Task-Oriented\nDialogue Systems\nWilly Chung, Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia, Pascale Fung\nHong Kong University of Science and Technology\nClear Water Bay, Hong Kong\nwhcchung@connect.ust.hk\nAbstract\nLarge language models (LLMs) have been\nused for diverse tasks in natural language pro-\ncessing (NLP), yet remain under-explored for\ntask-oriented dialogue systems (TODS), espe-\ncially for end-to-end TODS. We present In-\nstructTODS, a novel off-the-shelf framework\nfor zero-shot end-to-end task-oriented dialogue\nsystems that can adapt to diverse domains with-\nout fine-tuning. By leveraging LLMs, Instruct-\nTODS generates a proxy belief state that seam-\nlessly translates user intentions into dynamic\nqueries for efficient interaction with any KB.\nOur extensive experiments demonstrate that In-\nstructTODS achieves comparable performance\nto fully fine-tuned TODS in guiding dialogues\nto successful completion without prior knowl-\nedge or task-specific data. Furthermore, a rig-\norous human evaluation of end-to-end TODS\nshows that InstructTODS produces dialogue\nresponses that notably outperform both the\ngold responses and the state-of-the-art TODS in\nterms of helpfulness, informativeness, and hu-\nmanness. Moreover, the effectiveness of LLMs\nin TODS is further supported by our compre-\nhensive evaluations on TODS subtasks: dia-\nlogue state tracking, intent classification, and\nresponse generation. Code and implementa-\ntions could be found here1.\n1 Introduction\nLLMs have consistently pushed new frontiers in\nnatural language processing (NLP) in terms of per-\nformance across a variety of benchmarks, such\nas MMLU (Hendrycks et al., 2020), BIG-Bench\n(Lewkowycz et al., 2022) and HELM (Bommasani\net al., 2022), achieving state-of-the-art results in\nboth natural language understanding (NLU) and\ngeneration (NLG) tasks (Bang et al., 2023). Vari-\nous applications of LLMs have also been adopted\nin the industry, most prominently ChatGPT2 and\n1https://github.com/WillyHC22/InstructTODS/\n2http://chatgpt.openai.com/\nFigure 1: InstructTODS is the first zero-shot end-to-\nend task-oriented dialogue system that requires no task-\nspecific annotations, and ontology while generating\nmore human-preferred responses.\nGPT-43, which can provide a natural answer to a\ndiverse range of questions fluently and coherently.\nAmong the manifold tasks in NLP, task-oriented\ndialogue systems (TODS) represent a crucial do-\nmain. In general, TODS can be categorized into:\nthe pipelined approach (Ham et al., 2020; Hosseini-\nAsl et al., 2020a; Ohashi and Higashinaka, 2022),\nrelying on multiple sequential modules and heavy\nannotations for dialogue states and system actions,\nand the end-to-end approach (Banerjee and Khapra,\n2019; Qin et al., 2020; He et al., 2022), where\nthe systems generate responses directly from the\nuser input and the KB. Both approaches lack adapt-\nability to unseen domains. This adaptability often\nrequires domain-specific structures (ontology), and\ndata for TODS is notoriously expensive to collect\nand annotate (Eric et al., 2020). In this regard,\nLLMs present great potential thanks to their exten-\nsive pre-trained knowledge, enabling them to adapt\nto contextual information without any parameter\nupdates or additional task-specific data.\nHowever, utilizing LLMs for tasks requiring\nknowledge grounding, such as TODS, poses a criti-\n3https://openai.com/gpt-4\n2\nSetting Model\nBanking77 CLINC150\nSingle Multi Single Multi OOS*top-1 top-2 top-3 top-1 top-2 top-3\nRoBERTalarge 78.99 – – – 89.89 – – – –\nFew-ShotICDAXL 83.90 – – – 92.62 – – – –\nBaseline DNNC 80.40 – – – 91.02 – – – –\nCPFT 80.86 – – – 92.34 – – – –\nZero-ShotBARTlarge-MNLI 35.91 35.91 49.09 56.1426.44 26.44 36.46 43.13 0.2\nBaseline BARTlarge-MNLIcorr 42.24 42.24 55.19 62.2 40.16 40.16 51.88 57.66 0.9\nZero-ShotModular (GPT-3.5)65.45 64.5177.69 83.1864.91 63.22 72.20 82.42 10.9\nLLM Modular (GPT-4)74.09 64.0680.75 86.3373.91 69.90 81.88 90.33 62.1\nTable 1: Performance comparison on intent classification. LLMs outperform most baselines in our benchmark.\nBest performances in each section are in bold. *Out-of-scope intent of CLINC150.\ncal challenge that calls for thorough investigation\nand exploration. TODS requires dialogue systems\nto adeptly complete a specific goal by interacting\nwith a user in natural language according to a cer-\ntain set of bounded functions, ontology, and knowl-\nedge within the corresponding domain. Neverthe-\nless, naively feeding all of the knowledge to the\nLLMs in TODS could lead to the generation of\nmisleading and unfaithful information, i.e., halluci-\nnation (Ji et al., 2023; Azamfirei et al., 2023).\nIn this work, we first investigate the capability\nof LLMs to perform three key TODS objectives in\nzero-shot settings, specifically dialogue state track-\ning (DST), intent classification (IC), and response\ngeneration (RG). While LLMs demonstrate impres-\nsive capabilities and understanding of these tasks\nindividually, a closer examination of their short-\ncomings reveals that the modular approach is not\nthe most suitable for effectively using LLMs in\nTODS due to its restrictiveness. Rather than con-\nfining interactions within predefined elements like\nslots, values, or system actions, it is more advan-\ntageous to harness the emergent abilities of LLMs\nto process unstructured information, which also\nenables the system to easily adapt to new domains.\nFrom these observations, we propose Instruct-\nTODS, a fully off-the-shelf framework to perform\nend-to-end unified TODS in a zero-shot setting us-\ning LLMs. InstructTODS is adaptable to any KB\nand does not require any ontologies or task-specific\ndata. Instead of using predefined slot values, In-\nstructTODS generates an unstructured proxy belief\nstate from the dialogue context. Then, an action\nthought is generated to query the KB dynamically\nin natural language using an LLM. The retrieved\ninformation is then given to generate the response.\nIn summary, our contributions are as follows:\n• We provide an extensive evaluation and com-\nprehensive analysis of LLMs’ zero-shot per-\nformance in several TODS subtasks, notably\nintent classification, dialogue state tracking,\nand response generation.\n• We introduce InstructTODS, a fully off-the-\nshelf framework to leverage instruction-tuned\nLLMs in zero-shot setting for end-to-end uni-\nfied task-oriented dialogue, with the benefit of\nbeing effectively adaptable to any knowledge\nbase (KB) while alleviating the need for any\nadditional form of task-relevant data, such as\nintent, belief state, system action, etc.\n• We provide valuable insights from the TODS\nexperiments on the more general advantages\nand failure cases of LLMs to perform complex\nzero-shot NLP tasks.\n2 Evaluating LLMs on Zero-Shot\nTask-Oriented Dialogue Subtasks\nAs an intermediary step in exploring the potential\nof end-to-end TODS solutions, we first investigate\nhow well the performance of state-of-the-art LLMs\n(we presented the comparison of different LLMs\nover multiple tasks in Appendix A), i.e., GPT-3.5\nand GPT-4, in performing various modular task-\noriented objectives in their respective settings.\n2.1 TODS Subtasks\nLet us define a dialogue set Dn =\n{u1, r1, u2, r2, ..., un, rn} where ui and ri\ndenotes the user utterance and the system reply at\nturn i, respectively.\nIntent Classification (IC) For IC, we have the\nset of labels C = {c1, c2, ..., ct}, from which\nwe build the input for the LLM as xic\ni =\nPic(Iic, Concat(cj)t\nj=0, ui) where Pic(.) is the IC\n3\nModel Attraction Hotel Restaurant Taxi Train Average\nJGA Slot-F1JGA Slot-F1JGA Slot-F1JGA Slot-F1JGA Slot-F1JGA Slot-F1\nTRADE 20.06 – 14.20 – 12.59 – 59.21 – 22.39 – 25.69 –\nMA-DST 22.46 – 16.28 – 13.56 – 59.27 – 22.76 – 26.87 –\nTransferQA 31.25 – 22.72 – 26.28 – 61.87 – 36.72 – 35.77 –\nT5Dep 37.83 – 26.50 – 27.05 – 69.23 – 40.27 – 40.18 –\nModular (GPT-3.5, w/ all slots)30.23 65.38 26.77 76.28 48.28 82.90 56.22 75.33 53.75 83.64 42.02 78.60\nModular (GPT-3.5, w/ domain slot)39.53 74.89 27.03 79.78 51.72 85.06 63.24 83.98 52.50 84.84 44.48 82.53\nModular (GPT-4, w/ all slots)39.53 78.99 31.23 84.07 55.86 88.23 63.24 82.71 59.83 89.72 48.16 85.62\nModular (GPT-4, w/ domain slot)46.51 81.13 31.76 83.42 56.90 88.47 65.96 84.33 52.50 89.73 48.35 85.82\nTable 2: Performance comparison on zero-shot DST benchmark. LLMs outperform all baselines in our benchmark.\nBaseline results are directly taken from their respective works. The best performances in each section are in bold.\ninput template, Iic refers to the natural language\ninstruction for IC and Concat(cj) is the concate-\nnation of all labels. We evaluate two generation\nsettings, a single output setting where we query the\nmodel for the inferred intent, and a multi-output\nsetting where we query the model for the top-3 in-\ntents given the user query by simply changing the\ninstruction Iic. As such, we recast the classifica-\ntion task in a text-generation manner and compare\nour results with state-of-the-art IC baselines.\nDialogue State Tracking (DST) For\nDST, we define the total set of slots\nS = {s1,D1 , s2,D1 , ..., sk,Dl } where si,Dj is\nthe i-th slot associated to domain Dj. We\ngive a singular hand-crafted exemplar dis-\ntinct from the dataset to guide the generation\nformat directly as JSON. We build the input\nxdst\ni = Pdst(Idst, fdst(S), Di) by providing the\nentire dialogue context, where Pdst(.) is the DST\ninput template, Idst denotes the instruction for\nDST fdst(S) refers to a textual transformation\nof the set of slots. We evaluate two settings with\ndifferent slot transformations: one by providing all\nslots and another with only the active domain slots.\nResponse Generation (RG) For RG, given a di-\nalogue D, we define the set of oracle system ac-\ntions A = {a1,1, a1,2, ..., an,m} where ai,j denotes\nthe j-th system action of turn i. We construct the\ninput xrg\ni = Prg(Irg, frg(ai,1, ai,2, ..., ai,m), Di)\nwhere Prg(.) is RG input template, Irg denotes the\ninstruction for RG and frg(.) refers to a textual\ntransformation of the set of system actions. We\nevaluate the capability of LLMs to leverage a struc-\ntured system action while addressing the dialogue\ncontext to generate a response to the user.\n2.2 Experiment Settings\nDataset For the dialogue state tracking, we eval-\nuate the LLMs’ capability on MultiWOZ 2.1\n(MWOZ) (Eric et al., 2020). For intent classi-\nfication, we evaluate two datasets: Banking77\n(Casanueva et al., 2020), a fine-grained intent\ndataset in the banking domain, and CLINC150\n(Larson et al., 2019), coarse-grained intents classifi-\ncation datasets covering over 10 different domains.\nThe main challenge of the CLINC150 dataset is on\ninferring out-of-scope intent, which is particularly\nchallenging without any model training.\nEvaluation We evaluate dialogue state track-\ning with Joint Goal Accuracy (JGA) and Slot-\nF1. We compute JGA using exact matching in-\nstead of fuzzy matching, with minor typo fixes\nin MWOZ following prior works (Hosseini-Asl\net al., 2020a; Su et al., 2022). For intent classi-\nfication, we evaluate the accuracy when predict-\ning only one intent (single) and the top-3 intents\n(multi) in a text generation setting. BLEU (Pap-\nineni et al., 2002), Inform, and Success (Eric et al.,\n2020) are used for response generation. In addition\nto these metrics, we also compare lexical diver-\nsity (Shen, 2022), i.e., HDD (McCarthy and Jarvis,\n2010), MATTR (Covington and McFall, 2010),\nMTLD (McCarthy, 2005), and VOCD (McCarthy\nand Jarvis, 2007), fluency through perplexity, and\nhuman-likability using USL-H (Phy et al., 2020).\nBaseline For intent classification, we com-\npare with various few-shot fine-tuned base-\nlines: RoBERTa (Liu et al., 2019), ICDA (Lin\net al., 2023), DNNC (Zhang et al., 2020a), and\nCPFT (Zhang et al., 2021). While for zero-\nshot baseline, we employ MNLI (Williams et al.,\n2018) fine-tuned BARTlarge models (Lewis et al.,\n2020) by framing intent classification as an NLI\ntask. For dialogue state tracking, we compare\nwith multiple strong zero-shot baselines in the\nsingle-domain setting: TRADE (Wu et al., 2019a),\nMA-DST (Kumar et al., 2020), TransferQA (Lin\net al., 2021a) and T5Dep (Wang et al., 2022).\n4\nModel Reference-based Fluency Lexical diversity Human pref.\nBLEU Inf. Succ. PPL HDD MATTR MTLD VOCD USL-H\nLA V A 11.33 95.8 94.9 25.45 65.35 74.84 30.72 25.84 59.68\nSFN 14.11 97.7 91.6 51.97 70.68 78.67 34.25 36.02 65.41\nDAMD 14.94 78 68.7 58.41 71.45 78.09 29.08 37.57 62.62\nMARCO 16.5 95.3 91.1 36.00 73.40 83.39 44.48 42.78 70.35\nMinTL 18.39 85 80.8 49.77 71.31 82.76 38.99 37.26 65.36\nBORT 16.75 91.1 88.3 53.45 70.94 81.82 38.41 36.28 66.00\nHDSA 20.02 95.8 90.2 43.37 71.71 82.95 42.04 38.02 68.36\nRSTOD 15.98 91.6 86.9 76.05 73.11 82.41 42.08 41.88 68.54\nModular (GPT-4)6.12 86.42 78.48 36.63 80.59 89.56 66.64 70.13 89.66\nTable 3: Performance comparison on response generation. Although lower in BLEU, responses by the LLM-\npowered modular TODS are more human-preferred. The reported results for the baselines are taken from their\nrespective work. The best performances in each group are in bold.\nFor the response generation, we compare with\nmodular and non-unified end-to-end TODS—e.g.,\nhaving split decoder modules for DST and re-\nsponse generation—including SFN (Mehri et al.,\n2019), LA V A (Lubis et al., 2020), DAMD (Zhang\net al., 2020b), MARCO (Wang et al., 2020),\nMinTL (Lin et al., 2020), HDSA (Santra et al.,\n2021), RSTOD (Cholakov and Kolev, 2022), and\nBORT (Sun et al., 2022).\n2.3 Key Takeaways\nThe evaluation results for intent classification, DST,\nand response generation are shown in Table 1, Ta-\nble 2, and Table 3, respectively. We summarize the\nkey insights as follows:\nLLMs outperform most baselines. LLMs show\nsignificant improvements in intent classification\nand DST tasks compared to other zero-shot and\nfew-shot baselines and perform almost comparably\nto few-shot models in the intent classification task.\nLLMs offer better generalization and adapt-\nable solutions to TOD. Unlike fine-tuned mod-\nels, LLMs approach all tasks in an autoregressive\ngeneration manner, allowing greater flexibility and\nscalability to adapt to other tasks and domains.\nLLMs generate responses that better reflect\nhuman preference. Unlike other fine-tuned ap-\nproaches, LLMs generate responses that are dis-\ntinct from the gold responses resulting in lower\nBLEU scores. Nevertheless, the responses from\nmodular LLMs are more fluent, diverse, and\nhuman-likable compared to baselines, while having\ncompetitive Inform and Success rates.\nLLMs do not solve multi-domain DST problems\nDespite their strong performance, LLMs often over-\npredict active slots, leading to errors in the all slots\nsetting. LLMs tend to mix up parallel slots over\ndifferent domains, especially for either temporal or\nspatial information, e.g., destination and departure,\nleave time and arrival time, etc.\n3 InstructTODS: An Instruction-Based\nZero-shot End-to-End TODS\nBy leveraging the insights from solving TODS\nsubtasks on §2.3, we develop the first zero-shot\nend-to-end framework that operates without any\ndomain information (ontology) and requires no\ntask-specific annotations such as dialogue state,\nsystem act, intent, etc. This method is not only\ncost-efficient but also alleviates the ontology con-\nstraint of LLMs in the modular DST task and pro-\nmotes the strength of LLMs in generating better\nand more human-preferred responses. Let us define\na structured knowledge base (KB) as a set of tu-\nples K = {(va1\n1 , ..., vak\n1 ), ...,(va1\np , ..., vak\np )} where\n(ai)k\ni=0 are the attributes of the KB, and (vai\nj )p\nj=0\nare all the values associated to the attribute ai.\nWe first define a naive modular LLM response\ngeneration approach that serves as a baseline, de-\nnoted as RGnaive.4 RGnaive generates the user\nresponse by taking the entire KB along with the dia-\nlogue context as input. In this approach, we rely on\nthe ability of the LLM to parse the entire KB during\ninference while processing the dialogue context, in\norder to perform in-context retrieval and response\ngeneration at the same time. As such, we build\nthe input xRG\ni = PRG(IRG, fRG(K), Di) where\nPRG(.) is the response generation input template,\nIRG denotes the instruction for response genera-\ntion and fRG(K) refers to a textual transformation\n4RGnaive is based on Bang et al. (2023)’s limited observa-\ntion of zero-shot end-to-end TODS with ChatGPT (see §6.2).\n5\nFigure 2: Overview of InstructTODS, a framework to utilize LLM for zero-shot end-to-end task-oriented dialogue.\nof the KB where we filter unnecessary information\nand values that are too long as they are not needed\nto accomplish the user goal. In this approach, the\nbottleneck resides in the context window limit of\nthe LLMs. Unlike other approaches, InstructTODS\naims to make the best use of the LLM abilities to\nperform end-to-end tasks in zero-shot settings with-\nout the need for additional modular NLU and DST\nmodels, allowing zero-cost adaptation to various\ndomains with no parameter update.\nIn general, in order to process the dialogue his-\ntory and interact with the KB, InstructTODS in-\ntroduces two concepts, i.e., proxy belief state and\naction thought. The results from KB and the dia-\nlogue history are then fed as a context to the LLM\nfor generating the user response.\nIn the following paragraphs, we describe each\ncomponent of InstructTODS in more detail.\nProxy Belief State We generate a proxy belief\nstate ˜Bi = PBS (Di) from the dialogue history\nwhere PBS (.) denotes the prompt template and Di\nthe dialogue context. ˜Bi encapsulates everything\nthat the user is looking for in natural language at\nthis point of the dialogue. Note that, the proxy be-\nlief state does not need any prior knowledge about\nthe domain nor any ontology to operate (e.g. do-\nmain, trackable slots, values, types of information,\netc.). The proxy belief state is directly used to\ninteract with the KB in a multi-turn fashion.\nKB Interaction To interact with the KB, we gen-\nerate an Action thought A = Pact( ˜Bi, (ai)k\ni=0)\nwhere Pact(.) is the template for action generation\nand (ai)k\ni=0 the attributes of the KB. By providing\nthe existing attributes of the KB at this step, we\nground the LLM to accurately translate the belief\nstate into information that can be queried from the\nKB, while filtering out unnecessary data. The ac-\ntion thought serves as an intermediary to leverage\nthe code generation ability of LLM by generating a\nquery Q = PKB (A, K) where PKB (.) is the tem-\nplate for code generation. The output from the KB\nis then parsed by the LLM to extract relevant infor-\nmation, denoted as I, presented in natural language,\nwhich provides a summary of the KB interaction. It\nalso determines whether the current action thought\nhas been fulfilled. If it remains unanswered, a new\naction thought is generated based on the extracted\ninformation, and the process repeats until a stop-\nping criterion is reached indicating that no relevant\nknowledge is found in the KB.\nResponse Generation Once the KB interaction\nconcludes, the final information, together with the\noriginal dialogue context, is passed to the model\nto generate the response Y = PRG(I, Di) where\nPRG represents the response generation template\nand I the final information from the KB interaction.\nIn the case where no knowledge is found in the KB,\nthe LLM prompts the user to provide additional\ninformation. We provide the prompt template in\nAppendix D. The depiction of how the Instruct-\nTODS framework works is presented in Figure 2.\n4 Experiment settings\nBaselines Our framework is compared to other\nend-to-end unified TODS approaches that per-\nform end-to-end TODS using a unified text-to-\ntext paradigm through a single generalized text\n6\nModel\nAttraction Hotel Restaurant Taxi Train All\nInf. Succ. Inf. Succ. Inf. Succ. Inf. Succ. Inf. Succ. BLEU Inf. Succ.\nSOLOIST 100 90.90 90.00 85.00 78.30 70.00 100 100 81.80 78.80 13.58 88.80 84.30\nUBAR 100 90.90 85.00 70.00 91.70 83.30 100 90.00 90.90 84.80 15.05 91.90 82.10\nAUGPT 90.90 81.80 71.70 60.00 81.70 73.30 100 84.00 97.00 93.90 15.56 86.10 76.20\nGALAXY 90.90 72.70 81.70 76.70 91.70 83.80 100 100 93.90 93.90 18.10 91.00 86.10\nPPTOD 81.80 81.80 71.70 71.70 86.70 86.70 100 100 89.20 84.80 16.44 89.20 84.80\nRGnaive 81.80 36.37 90.00 83.33 96.70 83.33 100 89.80 97.00 63.67 3.95 94.90 82.16\nInstructTODS72.70 54.55 85.00 75.00 91.70 73.33 100 89.80 90.90 72.73 3.94 90.70 76.20\nTable 4: Task completion performance comparison. InstructTODS have competitive Inform and Success rates\ncompared to other end-to-end fine-tuned TODS baselines. Bold represents the highest score in each column.\ngeneration model, i.e., SimpleTOD (Hosseini-\nAsl et al., 2020b), PPTOD (Su et al., 2022),\nSoloist (Peng et al., 2021), UBAR (Yang et al.,\n2021), AuGPT (Kulhánek et al., 2021), and\nGalaxy (He et al., 2022). In addition, as described\nin §3, we add the naive version of the LLM re-\nsponse generation approach which is fed by the full\nKB (RGnaive), as an additional baseline to better\nevaluate the effectiveness of our framework.\nDatasets We evaluate the end-to-end zero-shot\ncapability on MultiWOZ 2.1 (MWOZ) (Eric et al.,\n2020; Lewkowycz et al., 2022). We split the eval-\nuation into two settings, i.e., single-domain and\nmulti-domain evaluation settings, where we show\nthe capability of LLMs to tackle more complex\nTODS tasks in zero-shot end-to-end settings.\nAutomatic Evaluation For evaluating the end-\nto-end framework, we measure the per domain In-\nform and Success rates, and the BLEU (Papineni\net al., 2002), Inform rate, and Success rate (Eric\net al., 2020) for all domains. The evaluation met-\nric is computed on the delexicalized responses to\navoid favoring models that provide more informa-\ntion than others and focus solely on the vocabulary\nused for the response generation. Additionally,\nwe also incorporate an automatic human-likability\nscore, namely USL-H (Phy et al., 2020).\nHuman Evaluation We conduct an extensive hu-\nman evaluation to measure the capability of LLMs\nin conducting zero-shot end-to-end unified TOD.\nSpecifically, we conduct two human evaluations,\nwhich measure: 1) the informativeness, helpful-\nness, and humanness of the generated responses,\nand 2) the information correctness and hallucina-\ntion rate of our InstructTODS. For evaluating infor-\nmativeness, helpfulness, and humanness, we ask 3\nannotators to rate the quality of the response using\na 4-point Likert scale (see Appendix B). The sys-\ntem is helpful if it answers the user’s request while\npushing the conversation towards goal completion,\ninformative if the system provides enough related\ninformation while answering the user, and human if\nthe generated answer is fluent and human-preferred.\nFor measuring the incorrectness and the hallucina-\ntion rate, the metrics are evaluated by a single TOD\nexpert. The incorrectness and hallucination rate are\nmeasured by manually checking the ratio of cor-\nrect, incorrect, and hallucinated entities provided\nin the generated responses. We conduct the human\nevaluation by taking 50 generated responses from\nall the models and the gold responses.\n5 Results and Analysis\n5.1 Automatic Evaluation\nOur automatic evaluation is shown in Table 4. In\ngeneral, we find a similar trend with the modular\nLLMs where LLMs produce lower BLEU scores—\n∼4 BLEU against ∼15 BLEU—with competitive\nInform and Success rates compared to other end-\nto-end unified TODS baselines. Note that, as men-\ntioned in §2.3, LLMs often generate completely\ndifferent responses to the gold knowledge, hence\nproducing low automatic evaluation scores. Nev-\nertheless, the low automatic evaluation scores do\nnot sufficiently reflect the capability of Instruct-\nTODS. We will further elaborate on this in §5.2,\nraising a question of the sufficiency of evaluating\nTODS quality using only a single gold response.\nSome comparative generation samples between the\ndifferent models can be found in Appendix C.\n5.2 Informativeness, Helpfulness, and\nHumanness of InstructTODS\nThe results for our human evaluation are shown in\nFigure 3 for InstructTODS in comparison with the\nnaive approach, the gold responses, and the two\nbest-performing baselines in task completion (i.e.,\n7\nFigure 3: Human evaluation comparison on informativeness (left), helpfulness (center), and humanness (right).\nFigure 4: InstructTODS have higher human preference\nscores than the gold responses and baselines.\nGalaxy and PPTOD). From the results, we show\nthat InstructTODS is more informative, helpful,\nand human-like than the two fine-tuned end-to-end\nbaselines by a noticeable margin. For both help-\nfulness and humanness, InstructTODS also outper-\nforms RGnaive and the gold response. Aligning\nwith the human evaluation results, the generated\nresponses by our framework also have higher hu-\nmanness scores as shown in Figure 4, even higher\nthan the gold responses. RGnaive is the most infor-\nmative, which is expected as the model processes\nthe entire KB for information, however, the quality\nof the information greatly differs as shown in§5.3.\n5.3 Incorrectness and Hallucination\nWe show the results for incorrectness and hallu-\ncination for the LLM-generated responses in Fig-\nure 5. While a sample can be incorrect, e.g., if\nthe LLM database interaction fails, the LLMs do\nnot necessarily generate unfaithful information. In-\nstructTODS is more robust than naively employing\nthe LLMs, improving the correctness by 15% and\nshowing 11% of hallucination, half the amount of\nthe RGnaive. We observe that some types of in-\nformation are more prone to hallucination, notably\ntime and address. This bias towards temporal and\nspatial information aligns with our observation of\nLLMs’ performance in DST (§2.3).\nFigure 5: Human evaluation on correctness, incor-\nrectness, and hallucination for RGnaive and Instruct-\nTODS.\n5.4 LLMs on Multi-Domain TOD\nWhile it is possible to use InstructTODS in multi-\ndomain with distinct KBs per domain, as we see\nin Figure 6, the performance degrades quickly for\nSuccess and slightly less for Inform as the number\nof domains increases. While fine-tuned end-to-end\nbaselines operate with only one KB at a given turn\nby tracking the active domain through either state\nchanges (Peng et al., 2021; Yang et al., 2021) or\nslot names (Kulhánek et al., 2021), our zero-shot\nframework does not assume any external knowl-\nedge nor ontology information. As such, all KBs\nare provided at each turn, and due to different KBs\nattributes overlapping in MWOZ, InstructTODS\noften queries incompatible information from the\nproxy belief state (e.g., \"food\" and \"destination\" at\nthe same time), which are in different KBs. Hence,\nmulti-domain degradation is largely due to the KB\ninteraction failure.\n6 Related Work\n6.1 Task-Oriented Dialogue System\nTask-oriented dialogue systems (TODS) can be\nbroadly classified into two categories (Chen et al.,\n2017; Gao et al., 2018; Zhang et al., 2020c) which\ninclude pipelined approaches (Nagata and Mori-\nmoto, 1994; Levin et al., 1997, 2000; Hurtado\net al., 2005; Williams and Young, 2007; Hori\n8\nFigure 6: End-to-end TODS performance degrades as the number of active domains in the dialogue increases.\net al., 2009; Lee et al., 2009) and end-to-end ap-\nproaches (Madotto et al., 2018; Wu et al., 2019b;\nMadotto et al., 2020b; Raghu et al., 2019; Qin et al.,\n2020; Hosseini-Asl et al., 2020a; Lei et al., 2018;\nHosseini-Asl et al., 2020b; Lin et al., 2020; He\net al., 2022; Kulhánek et al., 2021; Peng et al., 2021;\nSu et al., 2022; Yang et al., 2021). The pipelined\napproach utilizes multiple modules in order to gen-\nerate the system responses. While the end-to-end\napproach directly generates responses from the user\ninput and the KB in an end-to-end manner.\nEnd-to-end TODS Early approaches for end-to-\nend TODS employ template responses in a retrieval\nor generation setting (Zhao et al., 2017; Eric et al.,\n2017; Wu et al., 2019b). While other approaches\ninject KBs directly into the model to perform end-\nto-end generation (Madotto et al., 2018, 2020a).\nA more recent end-to-end TODS tackles end-to-\nend response generation in a single sequence pre-\ndiction problem (Hosseini-Asl et al., 2020a; Yang\net al., 2021; Peng et al., 2021) with an autoregres-\nsive model. These approaches still mostly leverage\nTOD data (belief states, system acts, etc.) dur-\ning generation. As general pre-trained LMs were\nshown to be effective for TODS (Mehri et al.,\n2019; Lubis et al., 2020; Lin et al., 2020), sev-\neral subsequent works have explored pre-training\napproaches directly tailored towards TODS (Zhang\net al., 2020b; Su et al., 2022; He et al., 2022). To\nthe best of our knowledge, prior works require a\nstructured format of dialogue states, system acts,\nand/or template responses, whereas InstructTODS\nalleviates such needs by incorporating an unstruc-\ntured proxy belief state, which requires no domain-\nspecific knowledge nor ontology to operate, allow-\ning zero-shot adaptation to various TOD domains.\n6.2 Zero-Shot Generalization of LLMs\nLLMs have shown remarkable zero-shot generaliza-\ntion capabilities in various NLP tasks (Brown et al.,\n2020; Scao et al., 2022; Chowdhery et al., 2022;\nThoppilan et al., 2022). This is further improved\nthrough instruction tuning (Wei et al., 2021; Sanh\net al., 2021; Wei et al., 2022; Chung et al., 2022;\nLongpre et al., 2023; Cahyawijaya et al., 2023b),\nwhich enables a better generalization to unseen\ntasks, and reinforcement learning with human feed-\nback (Christiano et al., 2017; Ouyang et al., 2022;\nBai et al., 2022), which enables a better alignment\nof human preferences. The zero-shot generaliza-\ntion ability of LLMs has also been explored in\nmore specific cases, e.g., multiple choice question\nanswering (Robinson and Wingate, 2023), biomed-\nical NLP (Fries et al., 2022), reasoning (Bang et al.,\n2023), low-resource languages (Cahyawijaya et al.,\n2023a,b; Asai et al., 2023), code-switching (Yong\net al., 2023; Zhang et al., 2023).\nLLMs for TODS Recent works explore the\napplicability of LLMs in solving modular TOD\ntasks (Bang et al., 2023; Hudeˇcek and Dušek, 2023)\nand a pipeline manner (Hosseini-Asl et al., 2020b;\nSu et al., 2022; Peng et al., 2021; Yang et al., 2021;\nKulhánek et al., 2021; He et al., 2022). Addition-\nally, Bang et al. (2023) inspect ChatGPT’s capa-\nbility for zero-shot end-to-end TODS, however, it\nis limited to only ∼1% of the test set available.\nTherefore, to the best of our knowledge, our work\nis the first to comprehensively study the utilization\nof LLMs for zero-shot end-to-end TODS.\n7 Conclusion\nIn this paper, we introduce InstructTODS, an off-\nthe-shelf framework to effectively perform end-\nto-end TODS in zero-shot utilizing LLMs. We\ncompare InstructTODS to several state-of-the-art\nfully fine-tuned end-to-end TODS and show that\nInstructTODS manages to guide the conversation\ntowards goal completion similarly to the fine-tuned\nsystems on MWOZ while generating answers that\nare more informative, helpful, and human-like than\nprevious approaches. Furthermore, we investigate\nthe capability of LLMs in performing various TOD\n9\nsubtasks in zero-shot settings, demonstrating better\ndiversity and human preference on response gen-\neration, and state-of-the-art zero-shot results on\ndialogue state tracking and intent classification.\n8 Limitation\nGeneralization to Other Datasets In our work,\nwe only assess the effectiveness of InstructTODS\non MultiWoZ 2.1 dataset, whose size is a magni-\ntude higher than other TODS datasets (Eric et al.,\n2020). We conjecture that the generalization to\nother datasets will follow the same trend as de-\nscribed in §5, where it excels in the single-domain\nsetting while still struggling in the multi-domain\nsetting. We expect future work to extend the as-\nsessment on InstructTODS to other datasets and\ndomains.\nGeneralization to Other Languages In recent\nyears, various task-oriented dialogue systems in\nlanguages other than English have been introduced,\nsuch as CrossWoZ (Zhu et al., 2020), BiTOD (Lin\net al., 2021b), GlobalWoZ (Ding et al., 2022),\nand COD (Majewska et al., 2023). As suggested\nin prior works evaluating LLMs in low-resource\nlanguages (Bang et al., 2023; Asai et al., 2023;\nCahyawijaya et al., 2023b,a; Workshop et al., 2023;\nKabra et al., 2023; Zhang et al., 2023), we conjec-\nture that the performance in other languages follow\nthe general trend in LLMs where the performance\nin low-resource languages will be lower compared\nto the high-resource languages. Future work might\nexplore and further extend methods for improving\nthe generalization of InstructTODS to other lan-\nguages.\nGeneralization to Other LLMs In this work,\nwe only explore two proprietary LLMs which dis-\nplay strong performance on various NLP tasks, i.e.,\nGPT-3.5 and GPT-4. Despite the lack of trans-\nparency of these models, we expect that when other\npublicly available LLMs achieve the same perfor-\nmance as these proprietary LLMs, a similar capa-\nbility of zero-shot end-to-end TODS will emerge.\nWe expect future work to explore the generaliza-\ntion of InstructTODS and its improvement in other\nLLMs.\n9 Ethics Statement\nOur research endeavors to develop an off-the-shelf\nframework for zero-shot end-to-end Task-Oriented\nDialogue Systems (TODS) using Large Language\nModels (LLMs). It is important to note that this\nstudy does not involve the use of any sensitive data\nand the experimental evaluation is conducted on\npublicly available datasets. To ensure the quality\nof our results, we have employed crowdsourcing\nfor the human evaluation of the generated dialogue\nresponses. While our study does not raise any ethi-\ncal concerns regarding privacy, confidentiality, or\nbias, we recognize that the use of LLMs in dialogue\nsystems may have ethical implications related to\npotential biases in the training data and the gen-\nerated responses. Therefore, we emphasize the\nimportance of ongoing research toward developing\nethical guidelines and best practices for the use of\nLLMs in dialogue systems. In line with our com-\nmitment to transparency and reproducibility, we\nwill be releasing our code publicly. We believe\nthat this will encourage open and collaborative re-\nsearch towards the development of more ethical\nand effective dialogue systems.\n10\nReferences\nAnonymous. 2023. Nusawrites: Constructing high-\nquality corpora for underrepresented and extremely\nlow-resource languages. Anonymous preprint under\nreview.\nAkari Asai, Sneha Kudugunta, Xinyan Velocity Yu,\nTerra Blevins, Hila Gonen, Machel Reid, Yulia\nTsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi.\n2023. Buffet: Benchmarking large language models\nfor few-shot cross-lingual transfer.\nRazvan Azamfirei, Sapna R Kudchadkar, and James\nFackler. 2023. Large language models and the perils\nof their hallucinations. Critical Care, 27(1):1–2.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022. Training a help-\nful and harmless assistant with reinforcement learn-\ning from human feedback.\nSuman Banerjee and Mitesh M Khapra. 2019. Graph\nconvolutional network with sequential attention for\ngoal-oriented dialogue systems. Transactions of the\nAssociation for Computational Linguistics , 7:485–\n500.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. arXiv\npreprint arXiv:2302.04023.\nRishi Bommasani, Percy Liang, and Tony Lee. 2022.\nHolistic evaluation of language models. Annals of\nthe New York Academy of Sciences.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSamuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji,\nGenta Indra Winata, Bryan Wilie, Rahmad Mahendra,\nChristian Wibisono, Ade Romadhony, Karissa Vin-\ncentio, Fajri Koto, Jennifer Santoso, David Moeljadi,\nCahya Wirawan, Frederikus Hudi, Ivan Halim Parmo-\nnangan, Ika Alfina, Muhammad Satrio Wicaksono, Il-\nham Firdausi Putra, Samsul Rahmadani, Yulianti Oe-\nnang, Ali Akbar Septiandri, James Jaya, Kaustubh D.\nDhole, Arie Ardiyanti Suryani, Rifki Afina Putri,\nDan Su, Keith Stevens, Made Nindyatama Nityasya,\nMuhammad Farid Adilazuarda, Ryan Ignatius, Ryan-\ndito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang\nDai, Yan Xu, Dyah Damapuspita, Cuk Tho, Ichwanul\nMuslim Karo Karo, Tirana Noor Fatyanosa, Ziwei\nJi, Pascale Fung, Graham Neubig, Timothy Baldwin,\nSebastian Ruder, Herry Sujaini, Sakriani Sakti, and\nAyu Purwarianti. 2023a. Nusacrowd: Open source\ninitiative for indonesian nlp resources.\nSamuel Cahyawijaya, Holy Lovenia, Tiezheng Yu,\nWilly Chung, and Pascale Fung. 2023b. Instruct-\nalign: Teaching novel languages with to llms through\nalignment-based cross-lingual instruction.\nIñigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efficient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45.\nHongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang\nTang. 2017. A survey on dialogue systems: Recent\nadvances and new frontiers. SIGKDD Explor. Newsl.,\n19(2):25–35.\nRadostin Cholakov and Todor Kolev. 2022. Efficient\ntask-oriented dialogue systems with response selec-\ntion as an auxiliary task. In Proceedings of the 5th\nInternational Conference on Natural Language and\nSpeech Processing (ICNLSP 2022) , pages 12–18,\nTrento, Italy. Association for Computational Linguis-\ntics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nPaul F. Christiano, Jan Leike, Tom B. Brown, Miljan\nMartic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. In\nProceedings of the 31st International Conference on\nNeural Information Processing Systems , NIPS’17,\npage 4302–4310, Red Hook, NY , USA. Curran Asso-\nciates Inc.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nMichael A. Covington and Joe D. McFall. 2010. Cutting\nthe gordian knot: The moving-average type–token\nratio (MATTR). Journal of Quantitative Linguistics,\n17(2):94–100.\nBosheng Ding, Junjie Hu, Lidong Bing, Mahani Alju-\nnied, Shafiq Joty, Luo Si, and Chunyan Miao. 2022.\nGlobalWoZ: Globalizing MultiWoZ to develop mul-\ntilingual task-oriented dialogue systems. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1639–1657, Dublin, Ireland. Association\nfor Computational Linguistics.\n11\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\nSanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj\nGoyal, Peter Ku, and Dilek Hakkani-Tur. 2020. Mul-\ntiwoz 2.1: A consolidated multi-domain dialogue\ndataset with state corrections and state tracking base-\nlines. In Proceedings of the Twelfth Language Re-\nsources and Evaluation Conference, pages 422–428.\nMihail Eric, Lakshmi Krishnan, Francois Charette, and\nChristopher D Manning. 2017. Key-value retrieval\nnetworks for task-oriented dialogue. In Proceedings\nof the 18th Annual SIGdial Meeting on Discourse\nand Dialogue, pages 37–49.\nJason Fries, Leon Weber, Natasha Seelam, Gabriel Al-\ntay, Debajyoti Datta, Samuele Garda, Sunny Kang,\nRosaline Su, Wojciech Kusa, Samuel Cahyawijaya,\nFabio Barth, Simon Ott, Matthias Samwald, Stephen\nBach, Stella Biderman, Mario Sänger, Bo Wang,\nAlison Callahan, Daniel León Periñán, Théo Gi-\ngant, Patrick Haller, Jenny Chim, Jose Posada, John\nGiorgi, Karthik Rangasai Sivaraman, Marc Pàmies,\nMarianna Nezhurina, Robert Martin, Michael Cul-\nlan, Moritz Freidank, Nathan Dahlberg, Shubhan-\nshu Mishra, Shamik Bose, Nicholas Broad, Yanis\nLabrak, Shlok Deshmukh, Sid Kiblawi, Ayush Singh,\nMinh Chien Vu, Trishala Neeraj, Jonas Golde, Albert\nVillanova del Moral, and Benjamin Beilharz. 2022.\nBigbio: A framework for data-centric biomedical\nnatural language processing. In Advances in Neural\nInformation Processing Systems, volume 35, pages\n25792–25806. Curran Associates, Inc.\nJianfeng Gao, Michel Galley, and Lihong Li. 2018. Neu-\nral approaches to conversational ai. In The 41st In-\nternational ACM SIGIR Conference on Research &\nDevelopment in Information Retrieval, pages 1371–\n1374. ACM.\nDonghoon Ham, Jeong-Gwan Lee, Youngsoo Jang, and\nKee-Eung Kim. 2020. End-to-end neural pipeline\nfor goal-oriented dialogue systems using gpt-2. In\nProceedings of the 58th annual meeting of the associ-\nation for computational linguistics, pages 583–592.\nWanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu,\nZheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei\nHuang, Luo Si, et al. 2022. Galaxy: A generative\npre-trained model for task-oriented dialog with semi-\nsupervised learning and explicit policy injection.Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nChiori Hori, Kiyonori Ohtake, Teruhisa Misu, Hideki\nKashioka, and Satoshi Nakamura. 2009. Statistical\ndialog management applied to wfst-based dialog sys-\ntems. In IEEE International Conference on Acous-\ntics, Speech and Signal Processing, 2009. ICASSP\n2009., pages 4793–4796. IEEE.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020a. A simple\nlanguage model for task-oriented dialogue. Advances\nin Neural Information Processing Systems, 33:20179–\n20191.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020b. A sim-\nple language model for task-oriented dialogue. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 20179–20191. Curran Associates,\nInc.\nV ojtˇech Hudeˇcek and Ondˇrej Dušek. 2023. Are llms all\nyou need for task-oriented dialogue? arXiv preprint\narXiv:2304.06556.\nLluıs F Hurtado, David Griol, Emilio Sanchis, and En-\ncarna Segarra. 2005. A stochastic approach to dia-\nlog management. In IEEE Workshop on Automatic\nSpeech Recognition and Understanding, 2005., pages\n226–231. IEEE.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1–38.\nAnubha Kabra, Emmy Liu, Simran Khanuja, Al-\nham Fikri Aji, Genta Indra Winata, Samuel Cahyawi-\njaya, Anuoluwapo Aremu, Perez Ogayo, and Graham\nNeubig. 2023. Multi-lingual and multi-cultural figu-\nrative language understanding.\nJonáš Kulhánek, V ojtˇech Hudeˇcek, Tomáš Nekvinda,\nand Ondˇrej Dušek. 2021. AuGPT: Auxiliary tasks\nand data augmentation for end-to-end dialogue with\npre-trained language models. In Proceedings of the\n3rd Workshop on Natural Language Processing for\nConversational AI, pages 198–210, Online. Associa-\ntion for Computational Linguistics.\nAdarsh Kumar, Peter Ku, Anuj Goyal, Angeliki Metalli-\nnou, and Dilek Hakkani-Tur. 2020. Ma-dst: Multi-\nattention-based scalable dialog state tracking. In\nProceedings of the AAAI conference on artificial in-\ntelligence, volume 34, pages 8107–8114.\nStefan Larson, Anish Mahendran, Joseph J Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K Kummerfeld, Kevin Leach, Michael A\nLaurenzano, Lingjia Tang, et al. 2019. An evaluation\ndataset for intent classification and out-of-scope pre-\ndiction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n1311–1316.\nCheongjae Lee, Sangkeun Jung, Seokhwan Kim, and\nGary Geunbae Lee. 2009. Example-based dialog\nmodeling for practical multi-domain dialog system.\nSpeech Communication, 51(5):466–484.\n12\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren,\nXiangnan He, and Dawei Yin. 2018. Sequicity: Sim-\nplifying task-oriented dialogue systems with single\nsequence-to-sequence architectures. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1437–1447.\nEsther Levin, Roberto Pieraccini, and Wieland Eck-\nert. 1997. Learning dialogue strategies within the\nmarkov decision process framework. In 1997 IEEE\nWorkshop on Automatic Speech Recognition and Un-\nderstanding Proceedings, pages 72–79. IEEE.\nEsther Levin, Roberto Pieraccini, and Wieland Eckert.\n2000. A stochastic model of human-machine interac-\ntion for learning dialog strategies. IEEE Transactions\non speech and audio processing, 8(1):11–23.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nAitor Lewkowycz, Ambrose Slone, Anders Andreassen,\nDaniel Freeman, Ethan S Dyer, Gaurav Mishra, Guy\nGur-Ari, Jaehoon Lee, Jascha Sohl-dickstein, Kristen\nChiafullo, et al. 2022. Beyond the imitation game:\nQuantifying and extrapolating the capabilities of lan-\nguage models.\nYen-Ting Lin, Alexandros Papangelis, Seokhwan Kim,\nSungjin Lee, Devamanyu Hazarika, Mahdi Namazi-\nfar, Di Jin, Yang Liu, and Dilek Hakkani-Tur. 2023.\nSelective in-context data augmentation for intent de-\ntection using pointwise V-information. In Proceed-\nings of the 17th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 1463–1476, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nZhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan\nMoon, Zhenpeng Zhou, Paul A Crook, Zhiguang\nWang, Zhou Yu, Eunjoon Cho, Rajen Subba, et al.\n2021a. Zero-shot dialogue state tracking via cross-\ntask transfer. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7890–7900.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020. MinTL: Minimalist trans-\nfer learning for task-oriented dialogue systems. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3391–3405, Online. Association for Computa-\ntional Linguistics.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nPeng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and\nPascale Fung. 2021b. Bitod: A bilingual multi-\ndomain dataset for task-oriented dialogue modeling.\narXiv preprint arXiv:2106.02787.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nNurul Lubis, Christian Geishauser, Michael Heck,\nHsien-chin Lin, Marco Moresi, Carel van Niekerk,\nand Milica Gasic. 2020. LA V A: Latent action spaces\nvia variational auto-encoding for dialogue policy op-\ntimization. In Proceedings of the 28th International\nConference on Computational Linguistics, pages 465–\n479, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nAndrea Madotto, Samuel Cahyawijaya, Genta Indra\nWinata, Yan Xu, Zihan Liu, Zhaojiang Lin, and Pas-\ncale Fung. 2020a. Learning knowledge bases with\nparameters for task-oriented dialogue systems. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 2372–2394.\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu,\nJamin Shin, and Pascale Fung. 2020b. Attention\nover parameters for dialogue systems.\nAndrea Madotto, Chien-Sheng Wu, and Pascale Fung.\n2018. Mem2seq: Effectively incorporating knowl-\nedge bases into end-to-end task-oriented dialog sys-\ntems. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1468–1478.\nOlga Majewska, Evgeniia Razumovskaia, Edoardo M\nPonti, Ivan Vuli´c, and Anna Korhonen. 2023. Cross-\nlingual dialogue dataset creation via outline-based\ngeneration. Transactions of the Association for Com-\nputational Linguistics, 11:139–156.\nPhilip M. McCarthy. 2005. An assessment of the range\nand usefulness of lexical diversity measures and the\npotential of the measure of textual, lexical diversity\n(MTLD). Ph.D. thesis, The University of Memphis.\nPhilip M McCarthy and Scott Jarvis. 2007. vocd: A the-\noretical and empirical evaluation. Language Testing,\n24(4):459–488.\nPhilip M. McCarthy and Scott Jarvis. 2010. MTLD,\nvocd-d, and HD-d: A validation study of sophis-\nticated approaches to lexical diversity assessment.\nBehavior Research Methods, 42(2):381–392.\nShikib Mehri, Tejas Srinivasan, and Maxine Eskenazi.\n2019. Structured fusion networks for dialog. In\nProceedings of the 20th Annual SIGdial Meeting on\nDiscourse and Dialogue, pages 165–177, Stockholm,\nSweden. Association for Computational Linguistics.\n13\nMasaaki Nagata and Tsuyoshi Morimoto. 1994. First\nsteps towards statistical modeling of dialogue to pre-\ndict the speech act type of the next utterance. Speech\ncommunication, 15(3-4):193–203.\nAtsumoto Ohashi and Ryuichiro Higashinaka. 2022.\nPost-processing networks: Method for optimizing\npipeline task-oriented dialogue systems using rein-\nforcement learning. In Proceedings of the 23rd An-\nnual Meeting of the Special Interest Group on Dis-\ncourse and Dialogue, pages 1–13.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2021. Soloist:\nBuilding task bots at scale with transfer learning and\nmachine teaching. Transactions of the Association\nfor Computational Linguistics, 9:807–824.\nVitou Phy, Yang Zhao, and Akiko Aizawa. 2020. Decon-\nstruct to reconstruct a configurable evaluation metric\nfor open-domain dialogue systems. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 4164–4178, Barcelona, Spain (On-\nline). International Committee on Computational Lin-\nguistics.\nLibo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, and\nTing Liu. 2020. Dynamic fusion network for multi-\ndomain end-to-end task-oriented dialog. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 6344–6354.\nDinesh Raghu, Nikhil Gupta, et al. 2019. Disentangling\nlanguage and knowledge in task-oriented dialogs. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1239–1255.\nJoshua Robinson and David Wingate. 2023. Leveraging\nlarge language models for multiple choice question\nanswering. In The Eleventh International Conference\non Learning Representations.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2021. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nBishal Santra, Potnuru Anusha, and Pawan Goyal. 2021.\nHierarchical transformer for task oriented dialog sys-\ntems. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 5649–5658, Online. Association for\nComputational Linguistics.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nLucas Shen. 2022. LexicalRichness: A small module to\ncompute textual lexical richness.\nYixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta,\nDeng Cai, Yi-An Lai, and Yi Zhang. 2022. Multi-task\npre-training for plug-and-play task-oriented dialogue\nsystem. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 4661–4676.\nHaipeng Sun, Junwei Bao, Youzheng Wu, and Xiaodong\nHe. 2022. BORT: Back and denoising reconstruc-\ntion for end-to-end task-oriented dialog. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 2156–2170, Seattle, United\nStates. Association for Computational Linguistics.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. Lamda: Language models for dialog appli-\ncations.\n14\nKai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, and\nJianxing Yu. 2020. Multi-domain dialogue acts and\nresponse co-generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7125–7134, Online. Association\nfor Computational Linguistics.\nQingyue Wang, Yanan Cao, Piji Li, Yanhe Fu, Zheng\nLin, and Li Guo. 2022. Slot dependency modeling\nfor zero-shot cross-domain dialogue state tracking.\nIn Proceedings of the 29th International Conference\non Computational Linguistics, pages 510–520.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nJason D Williams and Steve Young. 2007. Partially ob-\nservable markov decision processes for spoken dialog\nsystems. Computer Speech & Language, 21(2):393–\n422.\nBigScience Workshop, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel\nHesslow, Roman Castagné, Alexandra Sasha Luc-\ncioni, François Yvon, Matthias Gallé, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Benoît Sagot, Niklas Muennighoff, Albert Vil-\nlanova del Moral, Olatunji Ruwase, Rachel Bawden,\nStas Bekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-\ndro Ortiz Suarez, Victor Sanh, Hugo Laurençon,\nYacine Jernite, Julien Launay, Margaret Mitchell,\nColin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong,\nDaniel van Strien, David Ifeoluwa Adelani, Dragomir\nRadev, Eduardo González Ponferrada, Efrat Lev-\nkovizh, Ethan Kim, Eyal Bar Natan, Francesco De\nToni, Gérard Dupont, Germán Kruszewski, Giada\nPistilli, Hady Elsahar, Hamza Benyamina, Hieu Tran,\nIan Yu, Idris Abdulmumin, Isaac Johnson, Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse\nDodge, Jian Zhu, Jonathan Chang, Jörg Frohberg,\nJoseph Tobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak, Kimbo Chen, Kyle Lo, Leandro V on Werra,\nLeon Weber, Long Phan, Loubna Ben allal, Lu-\ndovic Tanguy, Manan Dey, Manuel Romero Muñoz,\nMaraim Masoud, María Grandury, Mario Šaško,\nMax Huang, Maximin Coavoux, Mayank Singh,\nMike Tian-Jian Jiang, Minh Chien Vu, Moham-\nmad A. Jauhar, Mustafa Ghaleb, Nishant Subramani,\nNora Kassner, Nurulaqilla Khamis, Olivier Nguyen,\nOmar Espejel, Ona de Gibert, Paulo Villegas, Pe-\nter Henderson, Pierre Colombo, Priscilla Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani,\nRoberto Luis López, Rui Ribeiro, Salomey Osei,\nSampo Pyysalo, Sebastian Nagel, Shamik Bose,\nShamsuddeen Hassan Muhammad, Shanya Sharma,\nShayne Longpre, Somaieh Nikpoor, Stanislav Silber-\nberg, Suhas Pai, Sydney Zink, Tiago Timponi Tor-\nrent, Timo Schick, Tristan Thrush, Valentin Danchev,\nVassilina Nikoulina, Veronika Laippala, Violette\nLepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Ta-\nlat, Arun Raja, Benjamin Heinzerling, Chenglei Si,\nDavut Emre Ta¸ sar, Elizabeth Salesky, Sabrina J.\nMielke, Wilson Y . Lee, Abheesht Sharma, Andrea\nSantilli, Antoine Chaffin, Arnaud Stiegler, Debajy-\noti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan\nFries, Jos Rozen, Leo Gao, Lintang Sutawika, M Sai-\nful Bari, Maged S. Al-shaibani, Matteo Manica, Ni-\nhal Nayak, Ryan Teehan, Samuel Albanie, Sheng\nShen, Srulik Ben-David, Stephen H. Bach, Taewoon\nKim, Tali Bers, Thibault Fevry, Trishala Neeraj, Ur-\nmish Thakker, Vikas Raunak, Xiangru Tang, Zheng-\nXin Yong, Zhiqing Sun, Shaked Brody, Yallow Uri,\nHadar Tojarieh, Adam Roberts, Hyung Won Chung,\nJaesung Tae, Jason Phang, Ofir Press, Conglong Li,\nDeepak Narayanan, Hatim Bourfoune, Jared Casper,\nJeff Rasley, Max Ryabinin, Mayank Mishra, Minjia\nZhang, Mohammad Shoeybi, Myriam Peyrounette,\nNicolas Patry, Nouamane Tazi, Omar Sanseviero,\nPatrick von Platen, Pierre Cornette, Pierre François\nLavallée, Rémi Lacroix, Samyam Rajbhandari, San-\nchit Gandhi, Shaden Smith, Stéphane Requena, Suraj\nPatil, Tim Dettmers, Ahmed Baruwa, Amanpreet\nSingh, Anastasia Cheveleva, Anne-Laure Ligozat,\nArjun Subramonian, Aurélie Névéol, Charles Lover-\ning, Dan Garrette, Deepak Tunuguntla, Ehud Reiter,\nEkaterina Taktasheva, Ekaterina V oloshina, Eli Bog-\ndanov, Genta Indra Winata, Hailey Schoelkopf, Jan-\nChristoph Kalo, Jekaterina Novikova, Jessica Zosa\nForde, Jordan Clive, Jungo Kasai, Ken Kawamura,\nLiam Hazan, Marine Carpuat, Miruna Clinciu, Na-\njoung Kim, Newton Cheng, Oleg Serikov, Omer\nAntverg, Oskar van der Wal, Rui Zhang, Ruochen\nZhang, Sebastian Gehrmann, Shachar Mirkin, Shani\nPais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov,\nVladislav Mikhailov, Yada Pruksachatkun, Yonatan\nBelinkov, Zachary Bamberger, Zdenˇek Kasner, Al-\nice Rueda, Amanda Pestana, Amir Feizpour, Am-\nmar Khan, Amy Faranak, Ana Santos, Anthony\nHevia, Antigona Unldreaj, Arash Aghagol, Are-\nzoo Abdollahi, Aycha Tammour, Azadeh HajiHos-\nseini, Bahareh Behroozi, Benjamin Ajibade, Bharat\n15\nSaxena, Carlos Muñoz Ferrandis, Danish Contrac-\ntor, David Lansky, Davis David, Douwe Kiela,\nDuong A. Nguyen, Edward Tan, Emi Baylor, Ez-\ninwanne Ozoani, Fatima Mirza, Frankline Onon-\niwu, Habib Rezanejad, Hessie Jones, Indrani Bhat-\ntacharya, Irene Solaiman, Irina Sedenko, Isar Ne-\njadgholi, Jesse Passmore, Josh Seltzer, Julio Bonis\nSanz, Livia Dutra, Mairon Samagaio, Maraim El-\nbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Clémentine Fourrier, Daniel León\nPeriñán, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Ranga-\nsai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko\nTakeuchi, Marc Pàmies, Maria A Castillo, Mari-\nanna Nezhurina, Mario Sänger, Matthias Samwald,\nMichael Cullan, Michael Weinberg, Michiel De\nWolf, Mina Mihaljcic, Minna Liu, Moritz Freidank,\nMyungsun Kang, Natasha Seelam, Nathan Dahlberg,\nNicholas Michio Broad, Nikolaus Muellner, Pascale\nFung, Patrick Haller, Ramya Chandrasekhar, Renata\nEisenberg, Robert Martin, Rodrigo Canalli, Rosaline\nSu, Ruisi Su, Samuel Cahyawijaya, Samuele Garda,\nShlok S Deshmukh, Shubhanshu Mishra, Sid Ki-\nblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Ku-\nmar, Stefan Schweter, Sushil Bharati, Tanmay Laud,\nThéo Gigant, Tomoya Kainuma, Wojciech Kusa, Ya-\nnis Labrak, Yash Shailesh Bajaj, Yash Venkatraman,\nYifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli\nXie, Zifan Ye, Mathilde Bras, Younes Belkada, and\nThomas Wolf. 2023. Bloom: A 176b-parameter\nopen-access multilingual language model.\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl,\nCaiming Xiong, Richard Socher, and Pascale Fung.\n2019a. Transferable multi-domain state generator\nfor task-oriented dialogue systems. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 808–819.\nChien-sheng Wu, Richard Socher, and Caiming Xiong.\n2019b. Global-to-local memory pointer networks for\ntask-oriented dialogue. In 7th International Confer-\nence on Learning Representations, ICLR 2019.\nMinghao Wu, Abdul Waheed, Chiyu Zhang, Muham-\nmad Abdul-Mageed, and Alham Fikri Aji. 2023.\nLamini-lm: A diverse herd of distilled models from\nlarge-scale instructions.\nYunyi Yang, Yunhao Li, and Xiaojun Quan. 2021.\nUBAR: Towards fully end-to-end task-oriented di-\nalog system with GPT-2. Proceedings of the AAAI\nConference on Artificial Intelligence, 35(16):14230–\n14238.\nZheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde,\nSkyler Wang, Samuel Cahyawijaya, Holy Lovenia,\nGenta Indra Winata, Lintang Sutawika, Jan Chris-\ntian Blaise Cruz, Long Phan, Yin Lin Tan, and Al-\nham Fikri Aji. 2023. Prompting multilingual large\nlanguage models to generate code-mixed texts: The\ncase of south east asian languages.\nJianguo Zhang, Trung Bui, Seunghyun Yoon, Xiang\nChen, Zhiwei Liu, Congying Xia, Quan Hung Tran,\nWalter Chang, and Philip Yu. 2021. Few-shot intent\ndetection via contrastive pre-training and fine-tuning.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1906–1912, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJianguo Zhang, Kazuma Hashimoto, Wenhao Liu,\nChien-Sheng Wu, Yao Wan, Philip Yu, Richard\nSocher, and Caiming Xiong. 2020a. Discrimina-\ntive nearest neighbor few-shot intent detection by\ntransferring natural language inference. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n5064–5082, Online. Association for Computational\nLinguistics.\nRuochen Zhang, Samuel Cahyawijaya, Jan Chris-\ntian Blaise Cruz, and Alham Fikri Aji. 2023. Mul-\ntilingual large language models are not (yet) code-\nswitchers.\nYichi Zhang, Zhijian Ou, and Zhou Yu. 2020b. Task-\noriented dialog systems that consider multiple appro-\npriate responses under the same context. Proceed-\nings of the AAAI Conference on Artificial Intelligence,\n34(05):9604–9611.\nZheng Zhang, Ryuichi Takanobu, Qi Zhu, MinLie\nHuang, and XiaoYan Zhu. 2020c. Recent advances\nand challenges in task-oriented dialog systems. Sci-\nence China Technological Sciences , 63(10):2011–\n2027.\nTiancheng Zhao, Allen Lu, Kyusong Lee, and Maxine\nEskenazi. 2017. Generative encoder-decoder models\nfor task-oriented spoken dialog systems with chatting\ncapability. In Proceedings of the 18th Annual SIGdial\nMeeting on Discourse and Dialogue, pages 27–36.\nQi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and\nMinlie Huang. 2020. Crosswoz: A large-scale chi-\nnese cross-domain task-oriented dialogue dataset.\nTransactions of the Association for Computational\nLinguistics, 8:281–295.\n16\nA Comparison of LLMs over Various\nNLP Tasks\nWe show the performance comparison of vari-\nous LLMs on both NLU and NLG tasks in Fig-\nure A1. The data are collected from various prior\nworks focusing on benchmarking the capabilities\nof LLMs (Bang et al., 2023; Cahyawijaya et al.,\n2023a; Anonymous, 2023; Asai et al., 2023; Ope-\nnAI, 2023; Wu et al., 2023).\nFigure A1: Average LLM performance comparison on\nvarious (left) NLG and (right) NLU tasks.\nB Human Evaluation\nWe give additional details concerning the human\nevaluation in this section. The instructions for each\nmetric given to the evaluators are defined as follow:\nInformativeness Amount of information that the\nsystem provides while answering the user’s utter-\nance.\n1. The response has no information at all\n2. The response provides at least one piece of\ninformation, but clearly not enough.\n3. The response provides several pieces of infor-\nmation but more could be provided\n4. The response gives all the information you\nwould expect in that turn\nHelpfulness The system answers the user’s utter-\nance and pushes the conversation towards comple-\ntion\n1. The response is doing neither\n2. The response is just pushing the conversa-\ntion towards completion without answering\nthe question\n3. The response is just answering the question\nbut not pushing the conversation towards com-\npletion\n4. The response is doing both\nHumanness The system answers the user’s utter-\nance in a human-like manner\n1. The response is completely machine-like\n2. The response contains human-like acknowl-\nedgement, but still sound machine-like\n3. The response has human acknowledgement\nand you could imagine it being said by a hu-\nman\n4. The response is completely indiscernible from\na human answer\nC Generation Samples\nWe show a few generation samples of Instruct-\nTODS as well as the modular evaluation of TODS\nsubtasks. In table A1, we show different delexical-\nized responses of the fine-tuned baseline compared\nto the generation by InstructTODS. For the modular\nsubtasks, we show a sample of correct prediction\nas well as difference failure cases. The samples for\ndialogue state tracking are in Table A2, for intent\nclassification in Table A3 and for response genera-\ntion in Table A4.\nD Prompts\nIn table A5, we give the templates and samples of\nthe prompts used inRGnaive, and in the end-to-end\nsetting for the proxy belief state, the initial knowl-\nedge base interaction and the response generation\nusing the extracted information.\n17\nID SNG0991.json\nDialogue\nContext\nAlright, let’s try for a cheap hotel.\nGold Do you want a cheap hotel or a cheap guesthouse?\nSOLIST I have [value_count] [value_pricerange] hotels. [value_count] in the [value_area] and [value_count] in the\n[value_area]. Which area do you prefer ?\nUBAR What area would you like to stay in?\nAUGPT I have [value_count] [value_price range] [value_type] available. Do you have a preference for the area?\nGALAXY [value_name] meets your criteria. Would you like me to book you a room ?\nPPTOD There are [value_choice] [value_price] [value_type] with free parking. Do you have a preference for the area ?\nInstrucTODS\n(Zero-Shot)\nGreat, I found [value_name] located at [value_address] [value_postcode]. It has free parking and falls within the\n[value_pricerange] price range. Would you like me to confirm a booking for you?\nID MUL2320.json\nDialogue\nContext\nNo, that’s everything I needed today. Thank you!\nGold Ok, thank you for calling. Goodbye.\nSOLIST You are welcome. Have a great day !\nUBAR Thank you for using our system!\nAUGPT You’re welcome. Have a great day!\nGALAXY You are welcome. Have a great day !\nPPTOD You are welcome. Have a great day !\nInstrucTODS\n(Zero-Shot)\nGreat, let me know if you need any further assistance. Have a great day!\nID MUL1489.json\nDialogue\nContext\nUSER: Yes, please. I need a table for 8 on Friday at 17:30, please.\nSYSTEM: Ok, you are all set for cote on Friday, table for 8 at 17:30. Can I help with anything else?\nUSER: Can I have the reference number for the reservation please?\nSYSTEM: Booking was unsuccessful. Can you try another time slot?\nUSER: What about 16:30?\nGold One moment while I try to make the reservation of table for 8, Friday at 16:30.\nSOLIST I have made that reservation for you. Your reference number is [value_reference]. Is there anything else I can help\nyou with ?\nUBAR I was able to book your table for [value_day] at [value_time]. Your reference number is [value_reference].\nAUGPT Yes, the reference number is [value_reference].\nGALAXY I am sorry, but I was unable to book that for you. Would you like to find another restaurant?\nPPTOD Your reference number is [value_reference]. Is there anything else I can help you with ?\nInstrucTODS\n(Zero-Shot)\nSure your reference number for the reservation at [value_name] on [value_day] at [value_time] for a table of\n[value_people] is [value_reference]. Is there anything else I can assist you with?\nTable A1: Samples of different system responses for both the proposed E2E framework in comparison with the\ngold response. InstrucTOD’s response are delexicalized after generation, while the fine-tuned systems generate the\ndelexicalized response directly.\n18\nID SNG01679.json\nPrompt Generate the dialogue state of the following dialogue between a USER and a task-oriented dialogue SYSTEM.\nThe results should be in a single python dictionary following this format: \"domain1-slot1\":value1, \"domain2-\nslot2\":\"value2\". Use the provided domain and slots, and nothing else:\nSLOTS:\nattraction-area, attraction-name, [...], hotel-day, hotel-stars, [...], train-leaveAt\nCONTEXT:\nUSER: I need a taxi to take me to Pipasha Restaurant to leave after 01:45.\nBELIEF STATES:\nGold {’taxi-leaveAt’: ’01:45’, ’taxi-destination’: ’pipasha restaurant’}\nModular\n(GPT-4)\n{\"taxi-leaveAt\":\"01:45\", \"taxi-destination\":\"Pipasha Restaurant\"}\nID SNG01936.json\nPrompt Generate the dialogue state of the following dialogue between a USER and a task-oriented dialogue SYSTEM.\nThe results should be in a single python dictionary following this format: \"domain1-slot1\":value1, \"domain2-\nslot2\":\"value2\". Use the provided domain and slots, and nothing else:\nSLOTS:\nattraction-area, attraction-name, [...], hotel-day, hotel-stars, [...], train-leaveAt\nCONTEXT:\nUSER: I am looking for city centre north b and b\nSYSTEM: I have found the guesthouse you were wanting. Would you like me to book this for you?\nUSER: Yes, please book it for 1 person and for 5 nights starting Friday.\nBELIEF STATES:\nGold {’hotel-stay’: ’5’, ’hotel-day’: ’friday’, ’hotel-people’: ’1’, ’hotel-name’: ’city centre north b and b’}\nModular\n(GPT-4)\n{\"hotel-stay\": \"5\", \"hotel-day\": \"Friday\", \"hotel-people\": \"1\", \"hotel-area\": \"city centre north\", \"hotel-type\": \"b\nand b\"}\nComment In this example, the LLM mistakes the first part of the name \"city centre north b and b\" as \"hotel-area\" since it\ncontains \"centre north\", and the end of the name as a \"hotel-type\" of hotel \"b and b\".\nID SNG0263.json\nPrompt Generate the dialogue state of the following dialogue between a USER and a task-oriented dialogue SYSTEM.\nThe results should be in a single python dictionary following this format: \"domain1-slot1\":value1, \"domain2-\nslot2\":\"value2\". Use the provided domain and slots, and nothing else:\nSLOTS:\nattraction-area, attraction-name, [...], hotel-day, hotel-stars, [...], train-leaveAt\nCONTEXT:\nUSER: I am looking for a train from Cambridge to Birmingham New Street.\nSYSTEM: The next train leaving from Cambridge for Birmingham New Street departs Friday at 5:01, and will\narrive by 7:44.\nUSER: I need a train that departs after 08:30 on Friday.\nSYSTEM: I have a train leaving Cambridge arriving at Birmingham New Street on Friday at 9:01. Would you like\nme to book this for you?\nUSER: Yes please, that sounds perfect.\nBELIEF STATES:\nGold {’train-leaveAt’: ’08:30’, ’train-destination’: ’birmingham new street’, ’train-day’: ’friday’, ’train-departure’:\n’cambridge’}\nModular\n(GPT-4)\n{\"train-leaveAt\":\"08:30\", \"train-destination\":\"Birmingham New Street\", \"train-day\":\"Friday\", \"train-\ndeparture\":\"Cambridge\", \"train-arriveBy\":\"9:01\"}\nComment In this example, the LLM mistakenly parse the time given in the system utterance as \"train-arriveBy\". This type of\nmistake happens often in domains with temporal information as the dialogue history gets longer\nTable A2: Samples of Zero-Shot Dialogue State Tracking. First example is correct, the other two show different\ntype of LLM mistakes.\n19\nPrompt Select the correct intent that is expressed in the given SENTENCE among the list of INTENTS provided. Generate\nthe correct intent and nothing more:\nINTENTS:\n0 activate_my_card\n1 age_limit\n2 apple_pay_or_google_pay\n[...]\n74 why_verify_identity\n75 wrong_amount_of_cash_received\n76 wrong_exchange_rate_for_cash_withdrawal\nSENTENCE:\n\"Is there a way to know when my card will arrive?\"\nGold card_arrival\nModular\n(GPT-4)\n11 card_arrival\nPrompt Select the correct intent that is expressed in the given SENTENCE among the list of INTENTS provided. Generate\nthe correct intent and nothing more:\nINTENTS:\n0 activate_my_card\n1 age_limit\n2 apple_pay_or_google_pay\n[...]\n74 why_verify_identity\n75 wrong_amount_of_cash_received\n76 wrong_exchange_rate_for_cash_withdrawal\nSENTENCE:\n\"Can I reactivate my lost card that I found this morning in my jacket pocket?\"\nGold card_linking\nModular\n(GPT-4)\n41 lost_or_stolen_card\nComment The LLM focuses more on \"lost\" in the user utterance, although the emphasis to understand the correct intent\nshould be put on \"reactivate\".\nPrompt Select the correct intent that is expressed in the given SENTENCE among the list of INTENTS provided. Generate\nthe correct intent and nothing more:\nINTENTS:\n0 activate_my_card\n1 age_limit\n2 apple_pay_or_google_pay\n[...]\n74 why_verify_identity\n75 wrong_amount_of_cash_received\n76 wrong_exchange_rate_for_cash_withdrawal\nSENTENCE:\n\"Why won’t my card show up on the app?\"\nGold card_linking\nModular\n(GPT-4)\ncard_not_working\nComment In the utterance context, the main source of confusion is due to the card linking being implicit (by mentionning the\napp), while the card not working is more explicit (not show up).\nTable A3: Samples of Zero-Shot Intent Classification. First example is correct, the other two show different type of\nLLM mistakes.\n20\nID PMUL3027.json\nPrompt In a task oriented dialogue setting, generate a SYSTEM response to the USER query in the conversation provided\nin CONTEXT. You should follow the information provided in ACT to generate this answer. Do not answer with\nanything other than what is provided in the dialogue act:\nCONTEXT:\nUSER: I am staying in the centre of town for the weekend, what is there to do there?\nSYSTEM: We have several things to do! Architecture, colleges, museums...What type of attraction are you most\ninterested in?\nUSER: It doesn’t matter but can you recommend one and give me the entrance fee?\nACT:Recommend the user for free, castle galleries.\nSYSTEM:\nGold I recommend castle galleries and it’s free to get in!\nModular\n(GPT-4)\nI recommend visiting the castle galleries, which are a great attraction and the best part is that the entrance is free!\nID PMUL3027.json\nPrompt In a task oriented dialogue setting, generate a SYSTEM response to the USER query in the conversation provided\nin CONTEXT. You should follow the information provided in ACT to generate this answer. Do not answer with\nanything other than what is provided in the dialogue act:\nCONTEXT:\nUSER: I want to find a moderately priced restaurant.\nSYSTEM: I have many options available for you! Is there a certain area or cuisine that interests you?\nUSER: Yes I would like the restaurant to be located in the center of the attractions.\nSYSTEM: There are 21 restaurants available in the centre of town. How about a specific type of cuisine?\nUSER: i need to know the food type and postcode and it should also have mutliple sports\nSYSTEM: I am sorry I do not understand what you just said. Please repeat in a way that makes sense.\nUSER: get me the food type and the post code\nSYSTEM: My favorite it The Copper Kettle at 4 Kings Parade City Centre CB21SJ. It serves British food. Does\nthat interest you?\nUSER: Absolutely! Thank you! I also need information on the attractions that have multiple sports in town, in the\nsame area as the restaurant please.\nSYSTEM: The only multiple sports attraction is located in the east of town. Would you like more information?\nUSER: No, I want to be in the centre of town. What about architecture attractions?\nSYSTEM: All Saints Church is an architectural attraction. It is in the Centre of town and entrance is free!\nUSER: Can I get their phone number and address please?\nSYSTEM: Their phone number is 01223452587. Is there anything else I can assist you with?\nUSER: Oh, what type of architecture is it?\nACT:Inform the user that the type is architecture.\nSYSTEM:\nGold It is architecture type. Anything else?\nModular\n(GPT-4)\nAll Saints Church features Gothic architecture.\nComment The type of architecture \"Gothic\" is hallucinated by the LLM here.\nTable A4: Samples of Zero-Shot Response Generation using oracle system action. Second example show hallucina-\ntion.\n21\nSetting Template Sample\nRGnaive Generate the answer of the SYSTEM in the following\nconversation between a USER and a SYSTEM in a\ntask-oriented dialogue setting. You can either request\nmore details to the user that is available in the knowledge\nbase to complete the goal, or simply answer the user’s\nrequest. Do not provide multiple choice for the user to\nchoose, just recommend one, and generate nothing other\nthan the SYSTEM reply. Use the following knowledge\nbase to interact with the user and perform {TASK}:\n{DATABASE}\n{DIALOGUE_CONTEXT}\nSYSTEM:\nGenerate the answer of the SYSTEM in the following\nconversation between a USER and a SYSTEM in a\ntask-oriented dialogue setting. You can either request\nmore details to the user that is available in the knowledge\nbase to complete the goal, or simply answer the user’s\nrequest. Do not provide multiple choice for the user to\nchoose, just recommend one, and generate nothing other\nthan the SYSTEM reply. Use the following knowledge\nbase to interact with the user and perform restaurant\nbooking:\nname: ..., address: ..., food: ...,\nname: ..., address:..., food: ...,\nUSER: Hi, any indian restaurants here?\nSYSTEM: Yes, we have Indian Express in the intermedi-\nate pricerange, would you want to know more?\nUSER: Do you have any restaurant in the cheap\npricerange?\nSYSTEM:\nProxy BS Suppose you have access to a database with all necessary\nINFORMATION, what do you need to query to the\ndatabase in order to reply to the user in the following\nconversation?\nINFORMATION: {SLOTS}\nYou can follow these examples: {EXAMPLES}\n{DIALOGUE_CONTEXT}\nNeed:\nSuppose you have access to a database with all necessary\nINFORMATION, what do you need to query to the\ndatabase in order to reply to the user in the following\nconversation?\nINFORMATION: address, area, name, phone, postcode,\npricerange, entrance fee, food, internet, parking, stars\nYou can follow these examples:\nUSER: I need fruits.\nSYSTEM: Do you have any preferences?\nUSER: Yes, apples if possible. How expensive are they?\nNeed: Information about pricerange for apple\nUSER: Hi, any indian restaurants here?\nSYSTEM: Yes, we have Indian Express in the intermedi-\nate pricerange, would you want to know more?\nUSER: Do you have any restaurant in the cheap\npricerange?\nNeed:\nInitial KB\nInteraction\nIf there are multiple options fitting this criteria, pick a\nfew to propose: {Proxy BS}\nIf there are multiple options fitting this criteria, pick a few\nto propose: Information about cheap indian restaurant\nE2E RG In a task-oriented dialogue setting, generate a natural\nand helpful SYSTEM response to the USER query in the\nconversation provided in CONTEXT. You should follow\nthe information provided in ACT to generate this answer.\nDo not mention that you are referring to a dataframe and\ndon’t overload the user with too many choices. You can\nperform {TASK}.\nYou can follow these examples:\n{EXAMPLES}\nCONTEXT: {CONTEXT}\nACT: {ACT}\nSYSTEM:\nIn a task-oriented dialogue setting, generate a natural\nand helpful SYSTEM response to the USER query in the\nconversation provided in CONTEXT. You should follow\nthe information provided in ACT to generate this answer.\nDo not mention that you are referring to a dataframe and\ndon’t overload the user with too many choices. You can\nperform restaurant booking.\nYou can follow these examples:\nUSER: I need a place to fish.\nSYSTEM: Any preference in the type of fish?\nUSER: Preferably salmons, but sardines are also fine.\nACT: Blue Lake, 37th Avenue\nSYSTEM: How about in blue lake, 37th avenue?\nCONTEXT:\nUSER: Hi, any indian restaurants here?\nSYSTEM: Yes, we have Indian Express in the intermedi-\nate pricerange, would you want to know more?\nUSER: Do you have any restaurant in the cheap\npricerange?\nACT: Royal Naan\nSYSTEM:\nTable A5: Templates and samples for the prompt used in the RGnaive, Proxy BS, initial KB interaction and E2E\nRG settings.",
  "topic": "End-to-end principle",
  "concepts": [
    {
      "name": "End-to-end principle",
      "score": 0.778609037399292
    },
    {
      "name": "Computer science",
      "score": 0.7703244686126709
    },
    {
      "name": "Task (project management)",
      "score": 0.5944238901138306
    },
    {
      "name": "End-user development",
      "score": 0.43821507692337036
    },
    {
      "name": "End user",
      "score": 0.37306666374206543
    },
    {
      "name": "Programming language",
      "score": 0.35034215450286865
    },
    {
      "name": "Artificial intelligence",
      "score": 0.18584021925926208
    },
    {
      "name": "Engineering",
      "score": 0.13041871786117554
    },
    {
      "name": "Operating system",
      "score": 0.11265614628791809
    },
    {
      "name": "Systems engineering",
      "score": 0.10751277208328247
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}