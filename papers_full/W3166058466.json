{
    "title": "Tuned bidirectional encoder representations from transformers for fake news detection",
    "url": "https://openalex.org/W3166058466",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5041152023",
            "name": "Amsal Pardamean",
            "affiliations": [
                "Seva Mandir"
            ]
        },
        {
            "id": "https://openalex.org/A5018559332",
            "name": "Hilman F. Pardede",
            "affiliations": [
                "Indonesian Institute of Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2977371261",
        "https://openalex.org/W2925285378",
        "https://openalex.org/W3081652108",
        "https://openalex.org/W3061785441",
        "https://openalex.org/W3080997347",
        "https://openalex.org/W2973869284",
        "https://openalex.org/W3027288733",
        "https://openalex.org/W4287818206",
        "https://openalex.org/W3089735125",
        "https://openalex.org/W3005212272",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2979771119",
        "https://openalex.org/W3006094059",
        "https://openalex.org/W2969278054",
        "https://openalex.org/W3094374364",
        "https://openalex.org/W3045669112",
        "https://openalex.org/W3001219191",
        "https://openalex.org/W3088886232",
        "https://openalex.org/W3036832155",
        "https://openalex.org/W3004179129",
        "https://openalex.org/W2974596429",
        "https://openalex.org/W3096210881",
        "https://openalex.org/W3047016663",
        "https://openalex.org/W3074473399",
        "https://openalex.org/W2979860911",
        "https://openalex.org/W3004844052",
        "https://openalex.org/W2997049449"
    ],
    "abstract": "Online medias are currently the dominant source of Information due to not being limited by time and place, fast and wide distributions. However, inaccurate news, or often referred as fake news is a major problem in news dissemination for online medias. Inaccurate news is information that is not true, that is engineered to cover the real information and has no factual basis. Usually, inaccurate news is made in the form of news that has mass appeal and is presented in the guise of genuine and legitimate news nuances to deceive or change the reader's mind or opinion. Identification of inaccurate news from real news can be done with natural language processing (NLP) technologies. In this paper, we proposed bidirectional encoder representations from transformers (BERT) for inaccurate news identification. BERT is a language model based on deep learning technologies and it has found effective for many NLP tasks. In this study, we use transfer learning and fine-tuning to adapt BERT for inaccurate news identification. The experiments show that our method could achieve accuracy of 99.23%, recall 99.46%, precision 98.86%, and F-Score of 99.15%. It is largely better than traditional method for the same tasks.",
    "full_text": "Indonesian Journal of Electrical Engineering and Computer Science \nVol. 22, No. 3, June 2021, pp. 1667~1671 \nISSN: 2502-4752, DOI: 10.11591/ijeecs.v22.i3.pp1667-1671      1667 \n  \nJournal homepage: http://ijeecs.iaescore.com \nTuned bidirectional encoder representations from transformers \nfor fake news detection \n \n \nAmsal Pardamean1, Hilman F. Pardede2 \n1,2Graduate School of Computer Science, STMIK Nusa Mandiri, Indonesia \n2Research Center for Informatics, Indonesian Institute of Sciences, Indonesia \n \n \nArticle Info  ABSTRACT \nArticle history: \nReceived Jan 20, 2021 \nRevised Apr 10, 2021 \nAccepted Apr 12, 2021 \n \n Online medias are currently the dominant source of Information due to not \nbeing limited by time and place, fast and wide distributions. However, \ninaccurate news, or often referred as fake news is a major problem in news \ndissemination for online medias. Inaccurate news is information that is not \ntrue, that is engineered to cover the real information and has no factual basis. \nUsually, inaccurate news is made in the form of news that has mass appeal \nand is presented in the guise of genuine and legitimate news nuances to \ndeceive or change the reader's mind or o pinion. Identification of inaccurate \nnews from real news can be done with natural language p rocessing (NLP) \ntechnologies. In this paper, we proposed bidirectional encoder \nrepresentations from transformers (BERT) for inaccurate news identification. \nBERT is a language model based on deep learning technologies and it has \nfound effective for many NLP tasks. In this study, we use transfer learning \nand fine-tuning to adapt BERT for inaccurate news identification. The \nexperiments show that our method could achieve  accuracy of 99.23%, recall \n99.46%, precision 98.86%, and F -Score of 99.15%. It is largely better than \ntraditional method for the same tasks. \nKeywords: \nBERT \nFake news  \nFine-tuning \nNatural language processing \nThis is an open access article under the CC BY-SA license. \n \nCorresponding Author: \nAmsal Pardamean  \nSTMIK Nusa Mandiri \nGraduate School of Computer Science \nJakarta, Indonesia \nEmail: amsalpardamean@gmail.com \n \n \n1. INTRODUCTION  \nOnline medias are currently the dominant source of information due to not being limited by time and \nplace, fast and wide distributions [1], [2]. Inaccurate news or often referred as fake news is a major problem \nin news dissemination for online medias today . Inaccurate news is information that is not true that is \nengineered to cover actual information. It has no factual basis which is made in the form of news that has \nmass appeal and is presented in the guise of genuine and legitimate news nuances to deceive  or change the \nreader's mind. Inaccurate news is dangerous because the writings or contents on the news is disseminated by \nirresponsible sources. Inaccurate news could change the mass opinions and hence affecting the society on \nmaking wrong decisions or ac tions that can later harm individuals or other groups [3 ]-[6]. Inaccurate news \nalso more often appears on political elections to seek sympathy and increase the number of votes even to \nattack political opponents [7]. \nTherefore, it is important to have tools  to detect inaccurate news. By doing so, the news that is \nconsumed by the public can be verified for truth,  and hence reduce the expansion of inaccurate news and \nimpose penalties on the creators or sites that widen an d initiate hoaxes [3]. Natural l anguage  \n\n                ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 22, No. 3, June 2021 :  1667 - 1671 \n1668 \nprocessing (NLP) technologies, a part of artificial intelligence which centralize learning on natural language \nprocessing or human language to communicate [8], [9] could be implemented for this purpose. \nFake news detection is a quite active studies in N LP and several studies have explored algorithms in \nsolving inaccurate news detection. In [10], term frequency -inverse document frequency of bi -grams and \nprobabilistic context -free grammar i s used as features and Support vector machines, stochastic g radient \ndescent, gradient boosting, bounded decision trees, and r andom forests are used as classifiers. The s tudy \nfound that term frequency -inverse document frequency (TF-IDF) and the stochastic g radient achiev ed \naccuracy of 77.2%. In [7], n aïve bayes (NB) is com pared with hybrid convolution neural n etwork and \nrecurrent neural n etwork models. The comparison results state that research using the deep learning model \ngets 82% accuracy. Various features such as count vectors , TF -IDF by applyin g word level, N -gram leve l \nand character level, and word e mbedding are evaluated in [6] for inaccurate news detection in Twitter. The \nresults show that support vector machine  (SVM) has an accuracy of 89 .34% with TF -IDF and Word2Vec. \nfeed-forward (FF) and back -propagation (BP), neural networks classification algorithms are used in [4]. The \nstudy achieved of 78.76%. In [5], TF -IDF with bag of w orld and n -gram are used as feature extraction and \ndeep multi-layer perceptron (MLP) are used as classification on 600 datasets of inaccurat e news in Indonesia. \nThe study achieved 83% accuracy, precision, 84%, recall 0 .73, and F1 -score 78%. In [3], c ount vectorizer, \nTF-IDF vectorizer, and word e mbedding followed are usd with SVM, logistic regression, decision tree, \nrandom forest, XG -Boost, g radient boosting, and deep learning neural n etworks. The best results are \nachieved for TF-IDF with SVM with 94% accuracy results. \nOne r ecent technology in NLP called bidirectional encoder representations from t ransformers \n(BERT) [11]. BERT introduces a two -ways training for a transformer, a deep learning architectures that fits \nto deal with sequence data. BERT has been used in various NLP problems such as the stanford sentiment \ntreebank (SST) sentiment classification [12], the classification of people's livel ihood texts [13], relation \nextraction in chinese medical texts [14] and website category classification [15]. These studies shows that the \nimplementation of BERT achieve most recent results to solve many problems in text classification. In this \nstudy, we propose to use BERT with fine tuning for classification of inaccurate news. \n \n \n2. PROPOSE METHOD \nBidirectional encoder representation of t ransformers (BERT) is a proposed model to perform \nnatural language processing (NLP) test ed with bidirectional representation and is based on pre -training \nneural network techniques  [11], [16] . BERT is determined from the transformer methodology and uses the \nattention mechanism. Attention mechanism is obtained by adopting encoder -decoder architectures to \ntransformer s architectures with the aim to find the “summary” of the data with encoder and then the \ndecoder translate them. In this sense, the transformers try to find mindfulness and find a way of looking at \nthe relationships between words in a particular sentence  [17], [18] . Currently, BERT is one of the most \npowerful representations of context and word.  \nBERT is designed to be able to distinguish a process that has a different meaning.  BERT uses \nunlabeled in designing deep bidirectional representations by moving the context left and right across all \nlayers [11], [19]. The results of the previously trained BERT model can then be tuned with one output \nlayer to create a model for performing var ious tasks including answering questions, language inference \nwithout substantial modification of the task -specific architecture. BERT has the latest results on eleven \nproblem solving tasks in natural language. the BERT model is conceptually simple and empi rically  \nstrong [20]-[22].  \nIn this study, we use pre -training BERT and then fine tune it for inaccurate news detection. T he \npre-training modeling is carried out on data that does not have a label and fine -tuning initializes the \nparameters for training and  in fine-tuning modeling the data used are labeled data  [11], [17] . The model of \nBERT is shown in Fig ure 1.  \nFirst, we adopt google BERT pre-training model. The encoder layer of BERT consists of  token \nembedding layer, segment embedding layer and position embedding layer [17], [23] . BERT uses \ntransformer, an attention to the mechanism in studying the relationship between the meaning or context \ncontained in the text between one word and another or in sub words. Transformer consists of two different \nmechanisms which are predictive encoders for tasks [24], [25]. BERT transformer encoder reads the entire \nword to build a model in learning to understand the meaning of words from the words in the surround ings.  \nWe retrain the BERT model to our inaccurate news task. By doing Pre -trained and fine tune we could \nadapt BERT model only using a small corpus for inaccurate news detection  [12], [17] . The representation \nof BERT is shown in Fig ure 2. \n \n \nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nTuned bidirectional encoder representations from transformers for fake news detection  (Amsal Pardamean) \n1669 \n \n \nFigure 1. BERT model [11] \n \n \n \n \nFigure 2. BERT representation [11] \n \n \n3. RESEARCH METHOD \nWe evaluate our method on inacc urate news data taken from the k aggle website. The dataset has \n28,711 news data which comprise of 12,999 inaccurate news and 15,712 true news. The data set is news from \n100percentfedup, 21stcenturywire, abcnews, abeldanger, abovetopsecret, activistpost, addictinginfo, \nblacklistednews, collective -evolution, counterpunch, dailywire, New York Times, Cable News Network  \n(CNN), Atlantic, Fox News, National Review, Guardian, Reuters, Washington Post and Vox [26]. \nWe divide the data into 2 parts: training data and testing data randomly with ration of 80% training \ndata and 20% testing data. The next stage is data are passed through the BERT Tokenizer process. \nHyperparameter settings in data pre -processing are 350 words for each column and a maximum of 35,000 \nfeatures. BERT Tokenizer specifically for pre -processing BERT model data and all vocabulary is available. \nWe apply learning rate 2e -5 and epoch value=3. This is based on our empirical observations that only small \nnumber of epoch is required to re -train the BERT model. We also apply Naive Bayes -SVM (NBSVM) as \nreference methods in the study. \n \n \n4. RESULTS AND DISCUSSION \nThe comparisons of BERT and NBSVM is shown i n Figure 3. It is clear that BERT achieves better \nresults than NBSVM, confirming the effectiveness of our method. Need to be noted that the results is \nobtained when we use only 3 epochs for BERT while we need 25 epochs for NBSVM . In this study the \nresults of the Performance Measure on BERT fine-tuning in the form of accuracy of 99.23%, recall 99.46%, \nprecision 98.86%, and F1-score of 99.15% are results that have been proven to provide good performance.  \nThe progression of BERT fine-tuning from epoch 1-3 is shown in Figure 4. The bar chart shows an \nincrease in the performance measure which is getting higher with an increase in the epoch value. For the \nepoch value of 1, the method we propose gets a higher value with the best value on the NB -SVM method. \nThen the increase in epoch 2 gives a quite different difference and finally with the epoch 3 value the increase \nstill occurs but is not so different from the epoch 2 value. \nThe progression of NBSVM from epoch 3 -25 is shown in Figure 5. The bar chart shows an increase \nin performance measure when the epoch value is further increased in the NBSVM model. However, it can be \nseen after epoch  22, the progression of the performance is stable, indicating that the best performance is \nachieved when epoch is around 20. Even with more epochs, NB-SVM is still worse than our method.  \n\n                ISSN: 2502-4752 \nIndonesian J Elec Eng & Comp Sci, Vol. 22, No. 3, June 2021 :  1667 - 1671 \n1670 \n  \n \nFigure 3. Comparisons of BERT and NBSVM \n \nFigure 4. Performance measure BERT fine-tuning \nepoch 1-3 \n \n \n \n \nFigure 5. Performance measure NBSVM epoch 3-25 \n \n \n5. RESULTS AND DISCUSSION \nIn this paper, we propose pre -trained BERT and fine tuning for inaccurate news detection. We \ncompare it with NBSVM. Our experiments confirm that our method achieve better performance when with \nsmaller number of epoch. However, need to be noted that BERT i s highly computational models and efforts \nto reduce computational load of BERT is needed.  \nIn the future, we plan to use combination models ( hybrid) to further improve the resu lts of the \nperformance measure accuracy, precision, recall and f -score on the detection of inaccurate news.  \nCombinations of features and the use of feature learning as inputs for BERT is also our future plan . We also \nplan to use BERT for indonesian inaccurate news detection. It is also interesting to see how BERT perform \nfor other natural language p rocessing problems such as spam detection in SMS or e -mail, text classification, \nsentiment analysis, detection of emotions from text for Indonesian data. \n \n \nACKNOWLEDGEMENTS \nThis Research is supported by department computer science of STMIK Nusa Mandiri Jakarta. \n \n\nIndonesian J Elec Eng & Comp Sci  ISSN: 2502-4752  \n \nTuned bidirectional encoder representations from transformers for fake news detection  (Amsal Pardamean) \n1671 \nREFERENCES \n[1] A. Fronzetti Colladon, “Forecasting election results by studying brand importance in online news,” International Journal \nof Forecasting, vol. 36, no. 2, pp. 414–427, 2020, doi: 10.1016/j.ijforecast.2019.05.013. \n[2] X. Zhang a nd A. A. Ghorbani, “An overview of online fake news: Characterization, detection, and discussion,” \nInformation Processing and Management, vol. 57, no. 2, p. 102025, 2020, doi: 10.1016/j.ipm.2019.03.004. \n[3] N. Smitha, “Performance Comparison of Machine Learning Classifiers for Fake News Detection,” in 2020 Second \nInternational Conference on Inventive Research in Computing Applications (ICIRCA) , pp. 696 –700, 2020 ,  \ndoi: 10.1109/ICIRCA48905.2020.9183072. \n[4] C. W. Kencana, E. B. Setiawan, and I. Kurniawan, “Hoax Detection System on Twitter using Feed-Forward and Back-\nPropagation Neural Networks Classification Method ,” RESTI, vol. 4, no. 4, pp. 655 -663, Aug. 2020 ,  \ndoi: 10.29207/resti.v4i4.2038. \n[5] A. Rusli, J. C. Young, and N. M. S. Iswari, “Identifying fake news in indonesian via supervised binary text classification,” \n2020 IEEE International Conference on Industry 4.0, Artificial Intelligence, and Communications Technology (IAICT), \n2020, pp. 86-90, doi: 10.1109/IAICT50021.2020.9172020. \n[6] A. A.-Tanvir, E. M. Mahir, S. Akhter, and M. R. Huq, “Detecting Fake News using Machine Learning and Deep \nLearning Algorithms,” 2019 7th International Conference on Smart Computing and Communications (ICSCC), 2019, pp. \n1-5, doi: 10.1109/ICSCC.2019.8843612. \n[7] W. Han and V. Mehta, “Fake news detection in social networks using machine learning and deep learning: Performance \nevaluation,” 2019 IEEE International Conference on Industrial Internet (ICII) , 2019, pp. 375 –380,  \ndoi: 10.1109/ICII.2019.00070. \n[8] L. Zhao et al., “Natural Language Processing (NLP) for requirements engineering: A systematic mapping study,” arXiv, \nno. v, 2020. \n[9] A. Ly, B. Uthayasooriyar, and T. Wang, “A survey on natural language processing (nlp) and applications in insurance,” \narXiv,  pp. 1-34, 2020. \n[10] S. Gilda, “Evaluating machine learning algorithms for fake news detection,” IEEE Student Conference on Research and \nDevelopment: Ins piring Technology for Humanity ( SCOReD), vol. 2018, 2018, pp. 110 –115,  \ndoi: 10.1109/SCORED.2017.8305411. \n[11] J. Devlin, M. W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language \nunderstanding,” 2019 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, vol. 1, 2019, pp. 4171–4186. \n[12] M. Munikar, S. Shakya and A. Shrestha, “Fine-grained sentiment classification using BERT,” arXiv, vol. 1, pp. 1–5, 2019. \n[13] S. Liu, H. Tao, and S. Feng, “Text Classification Research Based on Bert Model and Bayesian Network,” 2019 Chinese \nAutomation Congress (CAC), 2019, pp. 5842–5846, 2019, doi: 10.1109/CAC48633.2019.8996183. \n[14] K. Xue, Y. Zhou, Z. Ma, T. Ruan, H. Zhang, and P. He, “Fine-tuning BERT for joint entity and relation extraction in \nchinese medical text,” arXiv, pp. 892–897, 2019. \n[15] F. Demirkıran, A. Çayır, U. Ünal, and H. Dağ., “Website Category Classification Using Fine-tuned BERT Language \nModel,” 2020 5th International Conference on Computer Science and Engineering (UBMK) , 2020,  \npp. 333-336, doi: 10.1109/UBMK50275.2020.9219384. \n[16] T. Wang, K. Lu, K. P. Chow, and Q. Zhu, “COVID-19 Sensing: Negative Sentiment Analysis on Social Media in China \nvia BERT Model,” IEEE Access, vol. 8, pp. 138162-138169, 2020, doi: 10.1109/ACCESS.2020.3012595. \n[17] A. G. D’Sa, I. Illina, and D. Fohr, “BERT and fastText Embeddings for Automatic Detection of Toxic Speech,” 2020 \nInternational Multi-Conference on: Organization of Knowledge and Advanced Technologies (OCTA), 2020, pp. 1-5,  \ndoi: 10.1109/OCTA49274.2020.9151853. \n[18] Cai, Ren et al., “Sentiment Analysis About Investors and Consumers in Energy Market Based on BERT-BiLSTM,” IEEE \nAccess, vol. 8, pp. 171408-171415, doi: 10.1109/ACCESS.2020.3024750. \n[19] Dong, Junchao, F. He, Y. Guo, and H . Zhang, “A Commodity Review Sentiment Analysis Based on BERT -CNN \nModel,” 2020 5th International Conference on Computer and Communication Systems (ICCCS), pp. 143–147, 2020,  \ndoi: 10.1109/ICCCS49078.2020.9118434. \n[20] W. Li, S. Gao, H. Zhou, Z. Huang, K. Zhang, and W. Li., “The automatic text classification method based on bert and \nfeature union,” 2019 IEEE 25th International Conference on Parallel and Distributed Systems (ICPADS) , 2019,  \npp. 774–777, doi: 10.1109/ICPADS47876.2019.00114. \n[21] C. J. Lin, C. H. Huang, and C. H. Wu., “Using BERT to process chinese ellipsis and coreference in clinic dialogues,” \n2019 IEEE 20th International Conference on Information Reuse a nd Integration for Data Science  (IRI), 2019,  \npp. 414–418, doi: 10.1109/IRI.2019.00070. \n[22] I. Annamoradnejad, M. Fazli, and J. Habibi, “Predicting Subjective Features from Questions on QA Websites using \nBERT,” arXiv, pp. 240–244, 2020. \n[23] J. Yadav, D. Kumar, and D. Chauhan., “Cyberbullying Detection using Pre-Trained BERT Model,” 2020 International \nConference on Electr onics and Sustainable Communication Systems ( ICESC), 2020, pp. 1096 –1100,  \ndoi: 10.1109/ICESC48915.2020.9155700. \n[24] W. Maharani., “Sentiment Analysis during Jakarta Flood for Emergency Responses and Situational Awareness in \nDisaster Management using BERT,” 2020 8th International Conference on Information and Communication Technology \n(ICoICT), 2020, doi: 10.1109/ICoICT49345.2020.9166407. \n[25] Gao, Zhengjie, Ao Feng, Xinyu Song, and Xi Wu.,  “Target-Dependent Sentiment Classification with BERT,” IEEE \nAccess, vol. 7, pp. 154290 – 154299, doi: 10.1109/ACCESS.2019.2946594 . \n[26] Kaggle, Fake News Classifier - Final Project, 2018. [Online]. Availbale: https://www.kaggle.com/anthonyc1/fake-\nnews-classifier-final-project. "
}