{
  "title": "Training a Deep Contextualized Language Model for International Classification of Diseases, 10th Revision Classification via Federated Learning: Model Development and Validation Study",
  "url": "https://openalex.org/W4308774387",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5040535325",
      "name": "Pei‐Fu Chen",
      "affiliations": [
        "Far Eastern Memorial Hospital",
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5052511894",
      "name": "Tai-Liang He",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5080379148",
      "name": "Sheng-Che Lin",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5059739856",
      "name": "Yuan-Chia Chu",
      "affiliations": [
        "National Taipei University of Nursing and Health Science",
        "Taipei Veterans General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5025346186",
      "name": "Chen-Tsung Kuo",
      "affiliations": [
        "National Taipei University of Nursing and Health Science",
        "Taipei Veterans General Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5017526842",
      "name": "Feipei Lai",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5050385951",
      "name": "Ssu-Ming Wang",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5047787616",
      "name": "Wan-Xuan Zhu",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5022837496",
      "name": "Kuan‐Chih Chen",
      "affiliations": [
        "Far Eastern Memorial Hospital",
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A5006126251",
      "name": "Lu-Cheng Kuo",
      "affiliations": [
        "National Taiwan University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5060599903",
      "name": "Fang‐Ming Hung",
      "affiliations": [
        "Far Eastern Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5053662503",
      "name": "Yu‐Cheng Lin",
      "affiliations": [
        "Far Eastern Memorial Hospital",
        "National Yang Ming Chiao Tung University"
      ]
    },
    {
      "id": "https://openalex.org/A5036180882",
      "name": "I-Chang Tsai",
      "affiliations": [
        "Far Eastern Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5059475002",
      "name": "Chi-Hao Chiu",
      "affiliations": [
        "Far Eastern Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5009887194",
      "name": "Shu-Chih Chang",
      "affiliations": [
        "Far Eastern Memorial Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A5076429069",
      "name": "Chi-Yu Yang",
      "affiliations": [
        "Far Eastern Memorial Hospital"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2103519904",
    "https://openalex.org/W2998230683",
    "https://openalex.org/W3198282417",
    "https://openalex.org/W3131759468",
    "https://openalex.org/W4206115141",
    "https://openalex.org/W3188160313",
    "https://openalex.org/W2963183964",
    "https://openalex.org/W2970885630",
    "https://openalex.org/W3106323439",
    "https://openalex.org/W4283687125",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2964142373",
    "https://openalex.org/W3086809868",
    "https://openalex.org/W3182125009",
    "https://openalex.org/W3156869386",
    "https://openalex.org/W3175985785",
    "https://openalex.org/W2806631561"
  ],
  "abstract": "Background The automatic coding of clinical text documents by using the International Classification of Diseases, 10th Revision (ICD-10) can be performed for statistical analyses and reimbursements. With the development of natural language processing models, new transformer architectures with attention mechanisms have outperformed previous models. Although multicenter training may increase a model’s performance and external validity, the privacy of clinical documents should be protected. We used federated learning to train a model with multicenter data, without sharing data per se. Objective This study aims to train a classification model via federated learning for ICD-10 multilabel classification. Methods Text data from discharge notes in electronic medical records were collected from the following three medical centers: Far Eastern Memorial Hospital, National Taiwan University Hospital, and Taipei Veterans General Hospital. After comparing the performance of different variants of bidirectional encoder representations from transformers (BERT), PubMedBERT was chosen for the word embeddings. With regard to preprocessing, the nonalphanumeric characters were retained because the model’s performance decreased after the removal of these characters. To explain the outputs of our model, we added a label attention mechanism to the model architecture. The model was trained with data from each of the three hospitals separately and via federated learning. The models trained via federated learning and the models trained with local data were compared on a testing set that was composed of data from the three hospitals. The micro F1 score was used to evaluate model performance across all 3 centers. Results The F1 scores of PubMedBERT, RoBERTa (Robustly Optimized BERT Pretraining Approach), ClinicalBERT, and BioBERT (BERT for Biomedical Text Mining) were 0.735, 0.692, 0.711, and 0.721, respectively. The F1 score of the model that retained nonalphanumeric characters was 0.8120, whereas the F1 score after removing these characters was 0.7875—a decrease of 0.0245 (3.11%). The F1 scores on the testing set were 0.6142, 0.4472, 0.5353, and 0.2522 for the federated learning, Far Eastern Memorial Hospital, National Taiwan University Hospital, and Taipei Veterans General Hospital models, respectively. The explainable predictions were displayed with highlighted input words via the label attention architecture. Conclusions Federated learning was used to train the ICD-10 classification model on multicenter clinical text while protecting data privacy. The model’s performance was better than that of models that were trained locally.",
  "full_text": "Original Paper\nTraining a Deep Contextualized Language Model for International\nClassification of Diseases, 10th Revision Classification via\nFederated Learning: Model Development and Validation Study\nPei-Fu Chen1,2*, MD; Tai-Liang He3*, MSc; Sheng-Che Lin3, MSc; Yuan-Chia Chu4,5,6, PhD; Chen-Tsung Kuo4,5,6,\nPhD; Feipei Lai1,3,7, PhD; Ssu-Ming Wang1, MSc; Wan-Xuan Zhu8, BSc; Kuan-Chih Chen1,9, MSc, MD; Lu-Cheng\nKuo10, MSc, MD; Fang-Ming Hung11,12, MD; Yu-Cheng Lin13,14, MD, PhD; I-Chang Tsai15, PhD; Chi-Hao Chiu16,\nMS; Shu-Chih Chang17, MA; Chi-Yu Yang18,19, MD\n1Graduate Institute of Biomedical Electronics and Bioinformatics, National Taiwan University, Taipei, Taiwan\n2Department of Anesthesiology, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n3Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan\n4Department of Information Management, Taipei Veterans General Hospital, Taipei City, Taiwan\n5Medical Artificial Intelligence Development Center, Taipei Veterans General Hospital, Taipei City, Taiwan\n6Department of Information Management, National Taipei University of Nursing and Health Sciences, Taipei City, Taiwan\n7Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan\n8Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan\n9Department of Internal Medicine, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n10Department of Internal Medicine, National Taiwan University Hospital, National Taiwan University College of Medicine, Taipei, Taiwan\n11Department of Medical Affairs, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n12Department of Surgical Intensive Care Unit, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n13Department of Pediatrics, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n14School of Medicine, National Yang Ming Chiao Tung University, Taipei, Taiwan\n15Artificial Intelligence Center, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n16Section of Health Insurance, Department of Medical Affairs, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n17Medical Records Department, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n18Department of Information Technology, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n19Section of Cardiovascular Medicine, Cardiovascular Center, Far Eastern Memorial Hospital, New Taipei City, Taiwan\n*these authors contributed equally\nCorresponding Author:\nChi-Yu Yang, MD\nDepartment of Information Technology\nFar Eastern Memorial Hospital\nNo 21, Section 2, Nanya S Rd, Banciao District\nNew Taipei City, 220216\nTaiwan\nPhone: 886 2 8966 7000\nEmail: chiyuyang1959@gmail.com\nAbstract\nBackground: The automatic coding of clinical text documents by using the International Classification of Diseases, 10th\nRevision (ICD-10) can be performed for statistical analyses and reimbursements. With the development of natural language\nprocessing models, new transformer architectures with attention mechanisms have outperformed previous models. Although\nmulticenter training may increase a model’s performance and external validity, the privacy of clinical documents should be\nprotected. We used federated learning to train a model with multicenter data, without sharing data per se.\nObjective: This study aims to train a classification model via federated learning for ICD-10 multilabel classification.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 1https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nMethods: Text data from discharge notes in electronic medical records were collected from the following three medical centers:\nFar Eastern Memorial Hospital, National Taiwan University Hospital, and Taipei Veterans General Hospital. After comparing\nthe performance of different variants of bidirectional encoder representations from transformers (BERT), PubMedBERT was\nchosen for the word embeddings. With regard to preprocessing, the nonalphanumeric characters were retained because the model’s\nperformance decreased after the removal of these characters. To explain the outputs of our model, we added a label attention\nmechanism to the model architecture. The model was trained with data from each of the three hospitals separately and via federated\nlearning. The models trained via federated learning and the models trained with local data were compared on a testing set that\nwas composed of data from the three hospitals. The micro F1 score was used to evaluate model performance across all 3 centers.\nResults: The F1 scores of PubMedBERT, RoBERTa (Robustly Optimized BERT Pretraining Approach), ClinicalBERT, and\nBioBERT (BERT for Biomedical Text Mining) were 0.735, 0.692, 0.711, and 0.721, respectively. The F1 score of the model that\nretained nonalphanumeric characters was 0.8120, whereas the F1 score after removing these characters was 0.7875—a decrease\nof 0.0245 (3.11%). The F1 scores on the testing set were 0.6142, 0.4472, 0.5353, and 0.2522 for the federated learning, Far Eastern\nMemorial Hospital, National Taiwan University Hospital, and Taipei Veterans General Hospital models, respectively. The\nexplainable predictions were displayed with highlighted input words via the label attention architecture.\nConclusions: Federated learning was used to train the ICD-10 classification model on multicenter clinical text while protecting\ndata privacy. The model’s performance was better than that of models that were trained locally.\n(JMIR Med Inform 2022;10(11):e41342) doi: 10.2196/41342\nKEYWORDS\nfederated learning; International Classification of Diseases; machine learning; natural language processing; multilabel text\nclassification\nIntroduction\nBackground\nThe World Health Organization published a unified\nclassification system for diagnoses of diseases called the\nInternational Classification of Diseases (ICD), and the ICD\n10th Revision (ICD-10) is widely used [1]. Coders classify\ndiseases according to the rules of the ICD, and the resulting\nICD codes are used for surveys, statistics, and reimbursements.\nThe ICD-10 Clinical Modification (ICD-10-CM) is used for\ncoding medical diagnoses and includes approximately 69,000\ncodes [2,3]. ICD-10-CM codes contain 7 digits; the structure\nis shown in Figure 1.\nFigure 1. Structure of an International Classification of Diseases, 10th Revision, Clinical Modification code.\nIn hospitals, diagnoses for each patient are first written as text\ndescriptions in the electronic health record. A coder then reads\nthese records to classify diagnoses into ICD codes. Because\ndiagnoses are initially written as free text, the text's ambiguity\nmakes diagnoses difficult to code. Classifying each diagnosis\nis very time-consuming. A discharge record may contain 1 to\n20 codes. Per the estimation of a trial, coders spent 20 minutes\nassigning codes to each patient on average [4]. An automatic\ntool can be used to increase the efficiency of and reduce the\nlabor for ICD classification.\nRelated Work\nRecently, deep learning and natural language processing (NLP)\nmodels have been developed to turn plain text into vectors,\nmaking it possible to automatically classify them. Shi et al [5]\nproposed a hierarchical deep learning model with an attention\nmechanism. Sammani et al [6] introduced a bidirectional gated\nrecurrent unit model to predict the first 3 or 4 digits of ICD\ncodes based on discharge letters. Wang et al [7] proposed a\nconvolutional neural network model with an attention\nmechanism and gated residual network to classify Chinese\nrecords into ICD codes. Makohon et al [8] showed that deep\nlearning with an attention mechanism effectively enhances\nICD-10 predictions. Previous studies also mentioned the\nnecessity of enormous data sets and how privacy-sensitive\nclinical data limited the development of models for automatic\nICD-10 classification [6].\nFederated learning has achieved impressive results in the\nmedical field, being used to train models on multicenter data\nwhile keeping them private. Federated learning is widely used\nin medical image and signal analyses, such as brain imaging\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 2https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nanalysis [9] and the classification of electroencephalography\nsignals [10]. In the clinical NLP field, Liu et al [11] proposed\na 2-stage federated method that involved using clinical notes\nfrom different hospitals to extract phenotypes for medical tasks.\nPreviously, we applied a Word2Vec model with a bidirectional\ngated recurrent unit to classify ICD-10-CM codes from\nelectronic medical records [12]. We analyzed the distribution\nof ICD-10-CM codes and extracted features from discharge\nnotes. The model had an F1 score of 0.625 for ICD-10-CM code\nclassification. To improve the model’s performance, we\nimplemented bidirectional encoder representations from\ntransformers (BERT) and found an improved F1 score of 0.715\nfor ICD-10-CM code classification [4]. We also found that the\ncoding time decreased when coders used classification model\naids; the median F1 score significantly improved from 0.832 to\n0.922 (P<.05) in a trial [4]. Furthermore, we constructed a\nsystem to improve ease of use, comprising data processing,\nfeature extraction, model construction, model training, and a\nweb service interface [4]. Lastly, we included a rule-based\nalgorithm in the preprocessing process and improved the F1\nscore to 0.853 for ICD-10-CM classification [13].\nObjective\nThis study aims to further improve the performance of the\nICD-10 classification model and enable the model’s use across\nhospitals. In this study, we investigated the effect of federated\nlearning on the performance of a model that was trained on\nmedical text requiring ICD-10 classification.\nMethods\nEthics Approval\nThe study protocol was approved by the institutional review\nboards of Far Eastern Memorial Hospital (FEMH; approval\nnumber: 109086-F), National Taiwan University Hospital\n(NTUH; approval number: 201709015RINC), and Taipei\nVeterans General Hospital (VGHTPE; approval number:\n2022-11-005AC), and the study adhered to the tenets of the\nDeclaration of Helsinki. Informed consent was not applicable\ndue to the use of deidentified data.\nData Collection\nOur data were acquired from electronic health records at FEMH\n(data recorded between January 2018 and December 2020),\nNTUH (data recorded between January 2016 and July 2018),\nand VGHTPE (data recorded between January 2018 and\nDecember 2020). The data contained the text of discharge notes\nand ICD-10-CM codes. Coders in each hospital annotated the\nground truth ICD-10 codes.\nData Description\nAfter duplicate records were removed, our data set contained\n100,334, 239,592, and 283,535 discharge notes from FEMH,\nNTUH, and VGHTPE, respectively. Each record contained\nbetween 1 and 20 ICD-10-CM labels. The distribution of labels\nfor each chapter is shown in Figure 2. These chapters are\nclassified by the first three digits. Codes for chapters V01 to\nY98 are not used for insurance reimbursement; hence, they were\nexcluded from our data set. The minimum number of\nICD-10-CM labels was found for chapters U00 to U99, and the\nmaximum number was found for chapters J00 to J99. Counts\nof ICD-10-CM labels from the three hospitals are shown in\nMultimedia Appendix 1.\nThe text in the data set contained alphabetic characters,\npunctuation, and a few Chinese characters. The punctuation\ncount and the top 10 Chinese characters are shown in\nMultimedia Appendix 2. The most common punctuation mark\nwas the period (“.”), and the least common was the closing brace\n(“}”).\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 3https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 2. Counts of ICD-10-CM labels for 22 chapters from (A) Far Eastern Memorial Hospital, (B) National Taiwan University Hospital, and (C)\nTaipei Veterans General Hospital. ICD-10-CM: International Classification of Diseases, 10th Revision, Clinical Modification.\nPreprocessing\nWe first removed duplicate medical records from the data set.\nWe then transformed all full-width characters into half-width\ncharacters and all alphabetic characters into lowercase letters.\nRecords shorter than 5 characters were removed, as these were\nusually meaningless words, such as “nil” and “none.” We also\nremoved meaningless characters, such as newlines, carriage\nreturns, horizontal tabs, and formed characters (“\\n,” “\\r,” “\\t,”\nand “\\f,” respectively). Finally, all text fields were concatenated.\nTo choose a better method for managing punctuation and\nChinese characters during the preprocessing stage, we\ndetermined model performance by using FEMH data, given the\ninclusion of these characters in the data. Each experiment used\n2 versions of the data. In the first version, we retained these\nspecific characters, and in the second, we removed them.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 4https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nExperiment P investigated the effect of punctuation, experiment\nC investigated the effect of Chinese characters, and experiment\nPC investigated the effects of both punctuation and Chinese\ncharacters. Another method of retaining Chinese character\ninformation is using English translations of Chinese characters.\nTherefore, we also compared the model’s performance when\nChinese characters were retained to its performance when\nGoogle Translate was used to obtain English translations.\nOne-hot encoding was used for the labels. Of the 69,823\navailable ICD-10-CM codes, 17,745 appeared in our combined\ndata set, resulting in a one-hot encoding vector length of 17,745.\nThe final cohort comprised 100,334, 239,592, and 283,535\nrecords from FEMH, NTUH, and VGHTPE, respectively; 20%\n(FEMH: 20,067/100,334; NTUH: 47,918/239,592; VGHTPE:\n56,707/283,535) of the records were randomly selected for the\ntesting set, and the remaining records were used as the training\nset.\nClassification Model\nWe compared the performance of different variants of BERT,\nincluding PubMedBERT [14], RoBERTa (Robustly Optimized\nBERT Pretraining Approach) [15], ClinicalBERT [16], and\nBioBERT (BERT for Biomedical Text Mining) [17]. BioBERT\nwas pretrained with text from PubMed—the most popular\nbibliographic database in the health and medical science fields.\nClinicalBERT was pretrained with the MIMIC-III (Medical\nInformation Mart for Intensive Care III) data set, and its\nvocabulary was from English Wikipedia and the BookCorpus\ndata set. PubMedBERT is another variant of BERT that uses\ntraining data from PubMed. The main difference between\nPubMedBERT and BioBERT is their vocabularies. The\nvocabulary of BioBERT was from English Wikipedia and the\nBookCorpus data set—as was the vocabulary of\nBERT—whereas that of PubMedBERT was from PubMed. This\ndifference in vocabularies affects the ability to recognize words\nin clinical text. RoBERTa used the original BERT model, but\nit also used a longer training time, a larger batch size, and more\ntraining data. The training data were from the BookCorpus,\nCC-News (CommonCrawl News), and OpenWebText data sets.\nRoBERTa also applied dynamic masking, which meant that the\nmasked tokens would be changed multiple times instead of\nbeing fixed in the original BERT. The vocabularies and corpora\nof these BERT variants are summarized in Table 1.\nFor our comparison, the text was first fed into the BERT\ntokenizer, which transformed strings into tokens. The number\nof tokens was then truncated to 512 for every text datum that\nmet the input length limit of 512. A linear layer connected the\nword embeddings produced from the models to the output layers\nof the one-hot–encoded multilabels. The output size of the linear\nlayer was 17,745, which matched the one-hot encoding vector\nsize of the labels. Binary cross-entropy was used to calculate\nthe model loss. We trained our model for 100 epochs, with a\nlearning rate of 0.00005. These models were fine-tuned for our\nICD-10-CM multilabel classification task to compare their\nperformance. Figure 3 summarizes the model architecture and\npreprocessing flowchart. The best-performing model and\npreprocessing method were chosen for subsequent federated\nlearning.\nTable 1. Summary of the vocabulary and corpus sources for the various bidirectional encoder representations from transformers (BERT) models.\nCorpus sources (training data)Vocabulary sourcesModels\nPubMedPubMedPubMedBERT\nThe BookCorpus, CC-News, and OpenWebText data setsThe BookCorpus, CC-Newsb, and OpenWebText data setsRoBERTaa\nThe MIMIC-IIIc data setEnglish Wikipedia and the BookCorpus data setClinicalBERT\nPubMedEnglish Wikipedia and the BookCorpus data setBioBERTd\naRoBERTa: Robustly Optimized BERT Pretraining Approach.\nbCC-News: CommonCrawl News.\ncMIMIC-III: Medical Information Mart for Intensive Care III.\ndBioBERT: BERT for Biomedical Text Mining.\nFigure 3. Model architecture and processing flowchart. CLS: class token; ICD-10-CM: International Classification of Diseases, 10th Revision, Clinical\nModification.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 5https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFederated Learning\nWith federated learning, a model can be trained without sharing\ndata [18]. Clients (ie, local machines) keep their training data\non the same model architecture while exchanging the weights\nof model parameters. A server receives the weights from each\nclient and averages their weights. After updating the model, the\nserver sends new weights back to the clients. The clients can\nthen start a new training round. We updated the weights of our\nmodel parameters with the FederatedAveraging algorithm [18]\nand used Flower for federated learning [19].\nFlower is an open-source federated learning framework for\nresearchers [19]. Flower has a server-client structure. The server\nand clients need to be started individually, and a server needs\nto be assigned to each client. They communicate via the\nopen-source Google Remote Procedure Call (gRPC; Google\nLLC) [20]. With the gRPC, a client application can directly call\na method on a server application, and this can be done on\ndifferent machines. There is a registration center on the server\nfor managing communication with all clients. There are 3 main\nmodules in the server. The first—a connection management\nmodule—maintains all current gRPC connections. On the server,\neach gRPC corresponds to each client. When a gRPC is\nestablished, the register function is triggered to store the clients’\ninformation in an array. If a client initiates a disconnection or\nthe connection times out, the register function will be called to\nclear the client. The second module—a bridge module—caches\nthe information, regardless of whether the gRPC information\nfrom the clients or the server will be stored in the module.\nHowever, since the buffer is shared in both directions, it is\nnecessary to use the state transition method to ensure that all of\nthe information in the buffer is the same. There are five\nstates—the close, waiting for client write, waiting for client\nread, waiting for server write, and waiting for server read states.\nThe third module—a server handler—manages the traffic\nbetween the server and the clients.\nClients were set in the three hospitals, where the model was\ntrained on local data. The weights from each client were\ntransferred to the server, where the weights were averaged, and\nglobal models were made (Figure 4). We set 5 epochs for each\ntraining round on clients and 20 rounds for the server\naggregation. Our study was conducted on 2 nodes. Each node\nhad a NVIDIA RTX 2080 Ti graphics processing unit (NVIDIA\nCorporation) with 64 GB of RAM, and one node had 2 NVIDIA\nTITAN RTX graphics processing units with 64 GB of RAM\n(NVIDIA Corporation).\nFigure 4. Federated learning architecture. FEMH: Far Eastern Memorial Hospital; NTUH: National Taiwan University Hospital; VGHTPE: Taipei\nVeterans General Hospital.\nLabel Attention\nTo explain the outputs of our model, we added a label attention\narchitecture [21]. It calculated the attention based on the inner\nproducts of word vectors and each label vector separately. Figure\n5 shows how we added the label attention architecture to our\nmodel. First, we fine-tuned the BERT model by using the\ndefinitions of ICD-10-CM codes to generate the label vectors.\nSecond, we constructed a fully connected layer, of which the\nweights were initialized with the label vectors. Third, the output\nproduced by BERT was passed through the hyperbolic tangent\nfunction, thereby producing word vectors. We inputted the word\nvectors (Ζ) into the fully connected layer and softmax layer.\nThe output ( ) of the softmax layer was the attention. Fourth,\nwe inputted the hyperbolic tangent function of word vectors\n(H), which were multiplied by attention ( ), into another fully\nconnected layer and sigmoid layer. This was similar to our\noriginal architecture. The output (y) could be subtracted from\nthe one-hot–encoded labels for the loss calculation. Finally,\nattention was used to explain how the model predicted the labels.\nAttention was given to the input text for corresponding\nICD-10-CM codes. The performance of the model after adding\nthe label attention architecture was compared to its performance\nwithout this architecture.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 6https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 5. Our model architecture with label attention. BERT: bidirectional encoder representations from transformers.\nMetrics\nWe used the micro F1 score to evaluate performance because\nit is the harmonic mean of precision and recall and therefore\nyields more balanced results than those yielded when using\nprecision or recall only. The micro F1 score was calculated as\nfollows:\nwhere\nand\nTPsum indicates the sum of true positives, FPsum indicates the\nsum of false positives, and FNsum indicates the sum of false\nnegatives.\nResults\nComparing the Performance of Different BERT\nModels\nThe F1 scores of PubMedBERT, RoBERTa, ClinicalBERT, and\nBioBERT were 0.735, 0.692, 0.711, and 0.721, respectively.\nThe F1 score of PubMedBERT was the highest, and that of\nRoBERTa was the lowest among all models (Table 2). Due to\nthese results, we used PubMedBERT in the subsequent\nexperiments.\nTable 2. Performance of different bidirectional encoder representations from transformers (BERT) models.\nRecallPrecisionF1 scoreModels\n0.7150.7560.735PubMedBERT\n0.6660.7190.692RoBERTaa\n0.6890.7350.711ClinicalBERT\n0.6910.7540.721BioBERTb\naRoBERTa: Robustly Optimized BERT Pretraining Approach.\nbBioBERT: BERT for Biomedical Text Mining.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 7https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nThe Model’s Performance When Retaining or\nRemoving Punctuation or Chinese Characters\nTable 3 shows the mean number of tokens for each data set\npreprocessing case. The mean number of tokens when removing\npunctuation and Chinese characters was 52.9. The mean number\nof tokens when the characters were retained in experiment P\n(punctuation), experiment C (Chinese characters), and\nexperiment PC (punctuation and Chinese characters) was 65.0,\n53.1, and 65.1, respectively. Punctuation and Chinese characters\ncomprised 18.3% (1,301,988/7,096,460) and 0.1%\n(7948/7,096,460) of the tokens in our data, respectively.\nTable 3. Mean number of data tokens for retaining or removing punctuation or Chinese characters.\nMean number of tokensExperiment\n52.9Removed punctuation and Chinese characters (baseline)\n65.0Retained punctuation\n53.1Retained Chinese characters\n65.1Retained punctuation and Chinese characters\nTable 4 shows the F1 scores for each data set preprocessing\ncase. The baseline performance of the model after removing\npunctuation and Chinese characters was 0.7875. In experiment\nP, the F1 score for retaining punctuation was 0.8049—an\nincrease of 0.0174 (2.21%). In experiment C, the F1 score for\nretaining Chinese characters was 0.7984—an increase of 0.0109\n(1.38%). In experiment PC, the F1 score for retaining\npunctuation and Chinese characters was 0.8120—an increase\nof 0.0245 (3.11%). In all experiments, retaining these characters\nwas better than removing them, with experiment PC showing\nthe largest improvement in performance.\nTable 4. F1 scores for retaining or removing punctuation or Chinese characters.\nAbsolute increases (percentage)F1 scoreExperiment\nN/Aa0.7875Removed punctuation and Chinese characters (baseline)\n0.0174 (2.21%)0.8049Retained punctuation\n0.0109 (1.38%)0.7984Retained Chinese characters\n0.0245 (3.11%)0.8120Retained punctuation and Chinese characters\naN/A: not applicable.\nThe Model’s Performance Before and After\nTranslation\nIn the experiment where we translated Chinese into English,\nthe F1 score for retaining the Chinese characters was 0.7984,\nand that for translating them into English was 0.7983.\nFederated Learning\nTable 5 shows the performance of the models that were trained\nin the three hospitals. The models trained in FEMH, NTUH,\nand VGHTPE had validation F1 scores of 0.7802, 0.7718, and\n0.6151, respectively. The FEMH model had testing F1 scores\nof 0.7412, 0.5116, and 0.1596 on the FEMH, NTUH, and\nVGHTPE data sets, respectively. The NTUH model had testing\nF1 scores of 0.5583, 0.7710, and 0.1592 on the FEMH, NTUH,\nand VGHTPE data sets, respectively. The VGHTPE model had\ntesting F1 scores of 0.1081, 0.1058, and 0.5692 on the FEMH,\nNTUH, and VGHTPE data sets, respectively. The weighted\naverage testing F1 scores were 0.4472, 0.5353, and 0.2522 for\nthe FEMH, NTUH, and VGHTPE models, respectively.\nTable 6 shows the federated learning model’s performance in\nthe three hospitals. The federated learning model had validation\nF1 scores of 0.7464, 0.6511, and 0.5979 on the FEMH, NTUH,\nand VGHTPE data sets, respectively. The federated learning\nmodel had testing F1 scores of 0.7103, 0.6135, and 0.5536 on\nthe FEMH, NTUH, and VGHTPE data sets, respectively. The\nweighted average testing F1 score was 0.6142 for the federated\nlearning model.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 8https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 5. Models that were trained in the three hospitals for International Classification of Diseases, 10th Revision classification.\nWeighted average testing F1 scoresTesting F1 scoresValidation F1 scoreHospitals\n0.44720.7802FEMHa • 0.7412 (FEMH)\n• 0.5116 (NTUHb)\n• 0.1596 (VGHTPEc)\n0.53530.7718NTUH • 0.5583 (FEMH)\n• 0.7710 (NTUH)\n• 0.1592 (VGHTPE)\n0.25220.6151VGHTPE • 0.1081 (FEMH)\n• 0.1058 (NTUH)\n• 0.5692 (VGHTPE)\naFEMH: Far Eastern Memorial Hospital.\nbNTUH: National Taiwan University Hospital.\ncVGHTPE: Taipei Veterans General Hospital.\nTable 6. The federated learning model’s performance in the three hospitals.\nTesting F1 scoreaValidation F1 scoreData\n0.71030.7464FEMHb data\n0.61350.6511NTUHc data\n0.55360.5979VGHTPEd data\naThe weighted average testing F1 score was 0.6142.\nbFEMH: Far Eastern Memorial Hospital.\ncNTUH: National Taiwan University Hospital.\ndVGHTPE: Taipei Veterans General Hospital.\nLabel Attention\nThe F1 scores of the model with and without the label attention\nmechanism were 0.804 (precision=0.849; recall=0.763) and\n0.813 (precision=0.852; recall=0.777), respectively.\nFigure 6 shows a visualization of the attention for ICD-10-CM\ncodes and their related input text. The words were colored blue\nbased on the attention scores for different labels. The intensity\nof the blue color represented the magnitude of the attention\nscore. We used ICD-10-CM codes E78.5 (“Hyperlipidemia,\nunspecified”) and I25.10 (“Atherosclerotic heart disease of\nnative coronary artery without angina pectoris”) as examples.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 9https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 6. Attention for International Classification of Diseases, 10th Revision, Clinical Modification codes (A) E78.5 (“Hyperlipidemia, unspecified”)\nand (B) I25.10 (“Atherosclerotic heart disease of native coronary artery without angina pectoris”). The intensity of the blue color represents the magnitude\nof the attention score.\nDiscussion\nPrincipal Findings\nThe federated learning model outperformed each local model\nwhen tested on external data. The weighted average F1 scores\non the testing set were 0.6142, 0.4472, 0.5353, and 0.2522 for\nthe federated learning, FEMH, NTUH, and VGHTPE models,\nrespectively (Table 5 and Table 6). The model’s performance\ndecreased when tested on external data. Because different\ndoctors, coders, and diseases are found in different hospitals,\nthe style of clinical notes may be distinct across hospitals.\nOvercoming such gaps among hospitals is challenging. Although\nthe performance of the federated learning model was inferior\nto that of the models trained on local data when tested on local\ndata, its performance was higher than that of the models trained\non local data when tested on external data. Moreover, in the\nVGHTPE data set, the label distribution was very different from\nthe label distributions in the other two hospitals’ data sets\n(Figure 2). Therefore, the VGHTPE model only achieved F1\nscores of 0.1058 and 0.1081 on the NTUH and FEMH testing\nsets, respectively. The FEMH and NTUH models had F1 scores\nof 0.1596 and 0.1592, respectively, on the VGHTPE testing set\n(Table 5).\nFederated learning improves model performance on external\ndata. Federated learning can be used to build an ICD coding\nsystem for use across hospitals. However, the training time\nrequired for federated learning is longer than the training time\nrequired for local deep learning. Federated learning takes\napproximately 1 week, and local training takes approximately\n2 days. There are 2 reasons for this. First, the communication\nbetween the server and the clients takes longer if the model is\nlarge. The size of our model is approximately 859 MB. Second,\ndifferent clients may have different computing powers, and the\nslower client becomes a bottleneck [22,23]. Other clients may\nwait for the slower client until it completes its work.\nThe performance of PubMedBERT was better than that of\nBioBERT, ClinicalBERT, and RoBERTa. Table 2 shows that\nthe vocabulary of BERT models is an important factor of model\nperformance. The vocabulary of PubMedBERT contains\npredominantly medical terms, whereas the vocabularies of the\nother three models contain common words. This difference\naffects the ability to recognize words in clinical text. Most\npublished BERT models use a vocabulary of 30,522 WordPieces\n[24]. However, these vocabulary data do not contain some words\nfrom special fields. For example, the medical term “lymphoma”\nis in the vocabulary of PubMedBERT but not in the vocabularies\nof BioBERT, ClinicalBERT, and RoBERTa. The term\n“lymphoma” can be transformed into the token “lymphoma”\nby the PubMedBERT tokenizer, but the term would be split\ninto 3 tokens—“l”, “##ymph”, and “##oma”—by BioBERT,\nClinicalBERT, and RoBERTa.\nIn most scenarios, nonalphanumeric characters are removed\nbecause they are considered useless to the models [25]. In\ncontrast to models with attention mechanisms, early NLP models\ncould not pay attention to punctuation. Additional characters\nwould make the models unable to focus well on keywords. The\nremoval of punctuation in English text and text in other\nlanguages, such as Arabic, has been performed for NLP [26].\nEk et al [27] compared 2 data sets of daily conversation\ntext—one retained punctuation, and the other did not. Their\nresults showed better performance for the data set that retained\npunctuation.\nFor experiments P, C, and PC, all models performed better when\nadditional characters were retained (Table 4). Experiment P\ndemonstrated that PubMedBERT could use embedded\npunctuation. As punctuation marks are used to separate different\nsentences, removing them connects all sentences and thus makes\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 10https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nit harder for a model to understand the text content. The\nimprovement in our F1 score for retaining punctuation is similar\nto the results of previous work by Ek et al [27]. Our results\ndemonstrate that retaining punctuation can improve the\nperformance of text classification models for text from the\nclinical field. Experiment C demonstrated that PubMedBERT\ncould use embedded Chinese characters. Although\nPubMedBERT was pretrained with mostly English text, its\nvocabulary contains many Chinese characters. The tokens from\nChinese characters may contribute to the ICD-10 classification\ntask for clinical text because they provide information such as\nplace names, trauma mechanisms, and local customs. The results\nof experiment PC indicate that the benefits of retaining\npunctuation and retaining Chinese characters are additive. In\nthe translation experiment, the F1 scores did not considerably\ndiffer. This result indicates that the model can extract\ninformation from clinical text in either English or Chinese. The\nuse of the attention mechanisms of BERT increased our model’s\nability to pay attention to keywords. Punctuation and Chinese\ncharacters contribute helpful information to these models.\nTherefore, this preprocessing strategy—retaining more\nmeaningful tokens—provides more information for ICD-10\nclassification task models.\nIn our previous study, we introduced an attention mechanism\nto visualize the attention given to the input text for ICD-10\ndefinitions [4]. Through this approach, we trained a model to\npredict ICD-10 codes and trained another model to extract\nattention data. This approach might result in inconsistencies\nbetween the predictions and attention. In this study, we\nintroduced the label attention architecture to visualize the\nattention given to the input text for ICD-10 codes [21]. This\nmethod better illustrated the attention given to the input words\nthat were used to predict ICD codes, as it is consistent with the\nmethods used by prediction models.\nThe F1 score of the model, after the label attention mechanism\nwas added, decreased by 0.009. Although the F1 score decreased,\nwe obtained explainable predictions. For ICD-10-CM codes\nE78.5 (“Hyperlipidemia, unspecified”) and I25.10\n(“Atherosclerotic heart disease of native coronary artery without\nangina pectoris”), our model successfully paid great attention\nto the related words “hyperlipidemia” and “coronary artery”\n(Figure 6). Our visualization method (ie, highlighting input\nwords) allows users to understand how our model identified\nICD-10-CM codes from text.\nLimitations\nOur study has several limitations. First, our data were acquired\nfrom 3 tertiary hospitals in Taiwan. The extrapolation of our\nresults to hospitals in other areas should be studied in the future.\nSecond, although our results suggest that model performance\nis better when punctuation and Chinese characters are retained,\nthis effect may be restricted to specific note types. This finding\nshould be further examined in the context of classifying other\ntypes of clinical text. Third, the translated text in our last\nexperiment may not be as accurate as translations by a native\nspeaker. However, it is difficult to manually translate large\namounts of data. As such, we could only automatically translate\nthe text by using Google Translate.\nIt should be noted that there is a primary and secondary\ndiagnosis code for each discharge note. Although choosing the\nprimary code makes reimbursements different, the model\nproposed in this study did not identify primary codes. To make\nour model capable of identifying a primary code, we proposed\na sequence-to-sequence model in our previous work [4]. It\ntransforms the original predicted labels that were concatenated\nalphabetically, so that they are ordered by diagnosis code. This\nstructure can be added to the model proposed in this study.\nPredictions based on primary and secondary diagnosis codes\ncan further improve the usability of this system.\nConclusions\nFederated learning was used to train the ICD-10 classification\nmodel on multicenter clinical text while protecting data privacy.\nThe model’s performance was better than that of models that\nwere trained locally. We showed the explainable predictions by\nhighlighting input words via a label attention architecture. We\nalso found that the PubMedBERT model can use the meanings\nof punctuation and non-English characters. This finding\ndemonstrates that changing the preprocessing method for\nICD-10 multilabel classification can improve model\nperformance.\nAcknowledgments\nThis study was supported by grants from the Ministry of Science and Technology, Taiwan (grants MOST 110-2320-B-075-004-MY\nand MOST 110-2634-F-002-032-); Far Eastern Memorial Hospital, Taiwan (grant FEMH-2022-C-058); and Taipei Veterans\nGeneral Hospital (grants V111E-002 and V111E-005-2). The sponsors had no role in the study design, data collection and analysis,\npublication decision, or manuscript drafting.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nCounts of ICD-10-CM labels from the three hospitals. (A) Ranking of counts of labels in a medical record. (B) Ranking of counts\nof ICD-10-CM codes. ICD-10-CM: International Classification of Diseases, 10th Revision, Clinical Modification.\n[DOCX File , 662 KB-Multimedia Appendix 1]\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 11https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nMultimedia Appendix 2\nThe punctuation count and the top 10 Chinese characters.\n[DOCX File , 17 KB-Multimedia Appendix 2]\nReferences\n1. World Health Organization. International Statistical Classification of Diseases and Related Health Problems, 10th Revision:\nVolume 1, Tabular List, Fifth Edition 2016. Geneva, Switzerland: World Health Organization; 2016.\n2. Mills RE, Butler RR, McCullough EC, Bao MZ, Averill RF. Impact of the transition to ICD-10 on Medicare inpatient\nhospital payments. Medicare Medicaid Res Rev 2011 Jun 06;1(2):001.02.a02 [FREE Full text] [doi:\n10.5600/mmrr.001.02.a02] [Medline: 22340773]\n3. Kusnoor SV, Blasingame MN, Williams AM, DesAutels SJ, Su J, Giuse NB. A narrative review of the impact of the\ntransition to ICD-10 and ICD-10-CM/PCS. JAMIA Open 2019 Dec 26;3(1):126-131 [FREE Full text] [doi:\n10.1093/jamiaopen/ooz066] [Medline: 32607494]\n4. Chen PF, Wang SM, Liao WC, Kuo LC, Chen KC, Lin YC, et al. Automatic ICD-10 coding and training system: Deep\nneural network based on supervised learning. JMIR Med Inform 2021 Aug 31;9(8):e23230 [FREE Full text] [doi:\n10.2196/23230] [Medline: 34463639]\n5. Shi H, Xie P, Hu Z, Zhang M, Xing EP. Towards automated ICD coding using deep learning. arXiv. Preprint posted online\non November 11, 2017 [FREE Full text]\n6. Sammani A, Bagheri A, van der Heijden PGM, Te Riele ASJM, Baas AF, Oosters CAJ, et al. Automatic multilabel detection\nof ICD10 codes in Dutch cardiology discharge letters using neural networks. NPJ Digit Med 2021 Feb 26;4(1):37 [FREE\nFull text] [doi: 10.1038/s41746-021-00404-9] [Medline: 33637859]\n7. Wang X, Han J, Li B, Pan X, Xu H. Automatic ICD-10 coding based on multi-head attention mechanism and gated residual\nnetwork. 2022 Presented at: 2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM); December\n9-12, 2021; Houston, TX p. 536-543. [doi: 10.1109/bibm52615.2021.9669625]\n8. Makohon I, Li Y. Multi-label classification of ICD-10 coding and clinical notes using MIMIC and CodiEsp. 2021 Presented\nat: 2021 IEEE EMBS International Conference on Biomedical and Health Informatics (BHI); July 27-30, 2021; Athens,\nGreece p. 1-4. [doi: 10.1109/bhi50953.2021.9508541]\n9. Silva S, Gutman BA, Romero E, Thompson PM, Altmann A, Lorenzi M. Federated learning in distributed medical databases:\nMeta-analysis of large-scale subcortical brain data. 2019 Presented at: 2019 IEEE 16th International Symposium on\nBiomedical Imaging (ISBI 2019); April 8-11, 2019; Venice, Italy p. 270-274. [doi: 10.1109/isbi.2019.8759317]\n10. Gao D, Ju C, Wei X, Liu Y, Chen T, Yang Q. HHHFL: Hierarchical heterogeneous horizontal federated learning for\nelectroencephalography. arXiv. Preprint posted online on September 11, 2019 [FREE Full text]\n11. Liu D, Dligach D, Miller T. Two-stage federated phenotyping and patient representation learning. 2019 Presented at: 18th\nBioNLP Workshop and Shared Task; August 1, 2019; Florence, Italy p. 283-291 URL: https://aclanthology.org/W19-5030v1.\npdf [doi: 10.18653/v1/w19-5030]\n12. Wang SM, Chang YH, Kuo LC, Lai F, Chen YN, Yu FY, et al. Using deep learning for automatic Icd-10 classification\nfrom free-text data. Eur J Biomed Inform (Praha) 2020;16(1):1-10 [FREE Full text] [doi: 10.24105/ejbi.2020.16.1.1]\n13. Chen PF, Chen KC, Liao WC, Lai F, He TL, Lin SC, et al. Automatic International Classification of Diseases coding\nsystem: Deep contextualized language model with rule-based approaches. JMIR Med Inform 2022 Jun 29;10(6):e37557\n[FREE Full text] [doi: 10.2196/37557] [Medline: 35767353]\n14. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-specific language model pretraining for biomedical\nnatural language processing. ACM Trans Comput Healthc 2022 Jan;3(1):1-23. [doi: 10.1145/3458754]\n15. Liu Y, Ott M, Goyal N, Du J, Joshi M, Chen D, et al. RoBERTa: A robustly optimized BERT pretraining approach. arXiv.\nPreprint posted online on July 29, 2019 [FREE Full text]\n16. Huang K, Altosaar J, Ranganath R. ClinicalBERT: Modeling clinical notes and predicting hospital readmission. arXiv.\nPreprint posted online on April 10, 2019 [FREE Full text]\n17. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining. Bioinformatics 2020 Feb 15;36(4):1234-1240 [FREE Full text] [doi:\n10.1093/bioinformatics/btz682] [Medline: 31501885]\n18. McMahan HB, Moore E, Ramage D, y Arcas BA. Federated learning of deep networks using model averaging. arXiv.\nPreprint posted online on Februrary 17, 2016 [FREE Full text]\n19. Beutel DJ, Topal T, Mathur A, Qiu X, Parcollet T, Lane ND. Flower: A friendly federated learning research framework.\narXiv. Preprint posted online on July 28, 2020 [FREE Full text]\n20. gRPC Authors. gRPC: A high performance, open source universal RPC framework. gRPC. URL: https://grpc.io [accessed\n2022-09-17]\n21. Mullenbach J, Wiegreffe S, Duke J, Sun J, Eisenstein J. Explainable prediction of medical codes from clinical text. 2018\nPresented at: 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 12https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nLanguage Technologies; June 1-6, 2018; New Orleans, Louisiana p. 1101-1111 URL: https://aclanthology.org/N18-1100.\npdf [doi: 10.18653/v1/n18-1100]\n22. Li L, Fan Y, Tse M, Lin KY. A review of applications in federated learning. Comput Ind Eng 2020 Nov;149:106854. [doi:\n10.1016/j.cie.2020.106854]\n23. Imteaj A, Thakker U, Wang S, Li J, Amini MH. A survey on federated learning for resource-constrained IoT devices. IEEE\nInternet Things J 2022 Jan 1;9(1):1-24. [doi: 10.1109/jiot.2021.3095077]\n24. Zhao S, Gupta R, Song Y, Zhou D. Extremely small BERT models from mixed-vocabulary training. 2021 Presented at:\n16th Conference of the European Chapter of the Association for Computational Linguistics; April 19-23, 2021; Online p.\n2753-2759 URL: https://aclanthology.org/2021.eacl-main.238.pdf [doi: 10.18653/v1/2021.eacl-main.238]\n25. Biswas B, Pham TH, Zhang P. TransICD: Transformer based code-wise attention model for explainable ICD coding. 2021\nPresented at: 19th International Conference on Artificial Intelligence in Medicine, AIME 2021; June 15-18, 2021; Virtual\nEvent p. 469-478. [doi: 10.1007/978-3-030-77211-6_56]\n26. Abdullah M, AlMasawa M, Makki I, Alsolmi M, Mahrous S. Emotions extraction from Arabic tweets. International Journal\nof Computers and Applications 2018 Jun 07;42(7):661-675. [doi: 10.1080/1206212x.2018.1482395]\n27. Ek A, Bernardy JP, Chatzikyriakidis S. How does punctuation affect neural models in natural language inference. 2020\nPresented at: Probability and Meaning Conference (PaM 2020); October 14-15, 2020; Gothenburg, Sweden p. 109-116\nURL: https://aclanthology.org/2020.pam-1.15.pdf\nAbbreviations\nBERT: bidirectional encoder representations from transformers\nBioBERT: Bidirectional Encoder Representations From Transformers for Biomedical Text Mining\nCC-News: CommonCrawl News\nFEMH: Far Eastern Memorial Hospital\ngRPC: Google Remote Procedure Call\nICD-10: International Classification of Diseases, 10th Revision\nICD-10-CM: International Classification of Diseases, 10th Revision, Clinical Modification\nICD: International Classification of Diseases\nMIMIC-III: Medical Information Mart for Intensive Care III\nNLP: natural language processing\nNTUH: National Taiwan University Hospital\nRoBERTa: Robustly Optimized Bidirectional Encoder Representations From Transformers Pretraining Approach\nVGHTPE: Taipei Veterans General Hospital\nEdited by C Lovis; submitted 24.07.22; peer-reviewed by I Li, N Nuntachit; comments to author 15.08.22; revised version received\n03.10.22; accepted 08.10.22; published 10.11.22\nPlease cite as:\nChen PF, He TL, Lin SC, Chu YC, Kuo CT, Lai F, Wang SM, Zhu WX, Chen KC, Kuo LC, Hung FM, Lin YC, Tsai IC, Chiu CH, Chang\nSC, Yang CY\nTraining a Deep Contextualized Language Model for International Classification of Diseases, 10th Revision Classification via\nFederated Learning: Model Development and Validation Study\nJMIR Med Inform 2022;10(11):e41342\nURL: https://medinform.jmir.org/2022/11/e41342\ndoi: 10.2196/41342\nPMID:\n©Pei-Fu Chen, Tai-Liang He, Sheng-Che Lin, Yuan-Chia Chu, Chen-Tsung Kuo, Feipei Lai, Ssu-Ming Wang, Wan-Xuan Zhu,\nKuan-Chih Chen, Lu-Cheng Kuo, Fang-Ming Hung, Yu-Cheng Lin, I-Chang Tsai, Chi-Hao Chiu, Shu-Chih Chang, Chi-Yu\nYang. Originally published in JMIR Medical Informatics (https://medinform.jmir.org), 10.11.2022. This is an open-access article\ndistributed under the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which\npermits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR\nMedical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on\nhttps://medinform.jmir.org/, as well as this copyright and license information must be included.\nJMIR Med Inform 2022 | vol. 10 | iss. 11 | e41342 | p. 13https://medinform.jmir.org/2022/11/e41342\n(page number not for citation purposes)\nChen et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6955315470695496
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5682705044746399
    },
    {
      "name": "Natural language processing",
      "score": 0.5051978230476379
    },
    {
      "name": "Training (meteorology)",
      "score": 0.486128568649292
    },
    {
      "name": "Deep learning",
      "score": 0.4467921853065491
    },
    {
      "name": "Training set",
      "score": 0.42932960391044617
    },
    {
      "name": "Machine learning",
      "score": 0.3849620521068573
    },
    {
      "name": "Meteorology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210102996",
      "name": "Far Eastern Memorial Hospital",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I191969501",
      "name": "National Taipei University of Nursing and Health Science",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I2803004286",
      "name": "Taipei Veterans General Hospital",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I4210131804",
      "name": "National Taiwan University Hospital",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I148366613",
      "name": "National Yang Ming Chiao Tung University",
      "country": "TW"
    }
  ]
}