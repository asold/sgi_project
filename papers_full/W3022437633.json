{
  "title": "Progressive Transformers for End-to-End Sign Language Production",
  "url": "https://openalex.org/W3022437633",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5033011330",
      "name": "Ben Saunders",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A5058880614",
      "name": "Necati Cihan Camgöz",
      "affiliations": [
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A5044490167",
      "name": "Richard Bowden",
      "affiliations": [
        "University of Surrey"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2954798773",
    "https://openalex.org/W2188882108",
    "https://openalex.org/W157023553",
    "https://openalex.org/W2270585835",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2962795401",
    "https://openalex.org/W116902681",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2139359116",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2966391918",
    "https://openalex.org/W2802023636",
    "https://openalex.org/W2587277634",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2426359742",
    "https://openalex.org/W2903739847",
    "https://openalex.org/W2284660317",
    "https://openalex.org/W2970730223",
    "https://openalex.org/W2981618422",
    "https://openalex.org/W2941870244",
    "https://openalex.org/W2759302818",
    "https://openalex.org/W2888163892",
    "https://openalex.org/W2250689755",
    "https://openalex.org/W2901607128",
    "https://openalex.org/W2989607414",
    "https://openalex.org/W2970756316",
    "https://openalex.org/W2107603586",
    "https://openalex.org/W3024396289",
    "https://openalex.org/W2066601700",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2963373786",
    "https://openalex.org/W2906958245",
    "https://openalex.org/W2250670799",
    "https://openalex.org/W3014443981",
    "https://openalex.org/W2962824709",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W2755802490",
    "https://openalex.org/W2614519765",
    "https://openalex.org/W3034765865",
    "https://openalex.org/W602977392",
    "https://openalex.org/W1568066624",
    "https://openalex.org/W2964076328",
    "https://openalex.org/W3004835408",
    "https://openalex.org/W3127711005",
    "https://openalex.org/W2799020610",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962730651",
    "https://openalex.org/W2463640844",
    "https://openalex.org/W2157548127",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2781902187",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2124934925",
    "https://openalex.org/W2746301562",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2968383879",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2997573805",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "The goal of automatic Sign Language Production (SLP) is to translate spoken language to a continuous stream of sign language video at a level comparable to a human translator. If this was achievable, then it would revolutionise Deaf hearing communications. Previous work on predominantly isolated SLP has shown the need for architectures that are better suited to the continuous domain of full sign sequences. In this paper, we propose Progressive Transformers, a novel architecture that can translate from discrete spoken language sentences to continuous 3D skeleton pose outputs representing sign language. We present two model configurations, an end-to-end network that produces sign direct from text and a stacked network that utilises a gloss intermediary. Our transformer network architecture introduces a counter that enables continuous sequence generation at training and inference. We also provide several data augmentation processes to overcome the problem of drift and improve the performance of SLP models. We propose a back translation evaluation mechanism for SLP, presenting benchmark quantitative results on the challenging RWTH-PHOENIX-Weather-2014T(PHOENIX14T) dataset and setting baselines for future research.",
  "full_text": "Progressive Transformers for End-to-End Sign\nLanguage Production\nBen Saunders, Necati Cihan Camgoz, and Richard Bowden\nUniversity of Surrey\n{b.saunders,n.camgoz,r.bowden}@surrey.ac.uk\nAbstract. The goal of automatic Sign Language Production ( SLP) is to\ntranslate spoken language to a continuous stream of sign language video\nat a level comparable to a human translator. If this was achievable, then\nit would revolutionise Deaf hearing communications. Previous work on\npredominantly isolated SLP has shown the need for architectures that\nare better suited to the continuous domain of full sign sequences.\nIn this paper, we propose Progressive Transformers, the ﬁrst SLP model\nto translate from discrete spoken language sentences to continuous 3D\nsign pose sequences in an end-to-end manner. A novel counter decoding\ntechnique is introduced, that enables continuous sequence generation at\ntraining and inference. We present two model conﬁgurations, an end-to-\nend network that produces sign direct from text and a stacked network\nthat utilises a gloss intermediary. We also provide several data augmenta-\ntion processes to overcome the problem of drift and drastically improve\nthe performance of SLP models.\nWe propose a back translation evaluation mechanism for SLP, presenting\nbenchmark quantitative results on the challenging RWTH-PHOENIX-\nWeather-2014T (PHOENIX14T) dataset and setting baselines for fu-\nture research. Code available at https://github.com/BenSaunders27/\nProgressiveTransformersSLP.\nKeywords: Sign Language Production, Continuous Sequence Synthesis,\nTransformers, Sequence-to-Sequence, Human Pose Generation\n1 Introduction\nSign language is the language of communication for the Deaf community, a rich vi-\nsual language with complex grammatical structures. As it is their native language,\nmost Deaf people prefer using sign as their main medium of communication, as\nopposed to a written form of spoken language. Sign Language Production ( SLP),\nconverting spoken language to continuous sign sequences, is therefore essential\nin involving the Deaf in the predominantly spoken language of the wider world.\nPrevious work has been limited to the production of concatenated isolated signs\n[53,64], highlighting the need for improved architectures to properly address the\nfull remit of continuous sign language.\nIn this paper, we propose Progressive Transformers, the ﬁrst SLP model\nto translate from text to continuous 3D sign pose sequences in an end-to-end\narXiv:2004.14874v2  [cs.CV]  20 Jul 2020\n2 B. Saunders et al.\nPT\nSymbolic\tEncoder \nProgressive\tDecoder \nT2G2P\nST\nSymbolic\tEncoder \nSymbolic\tDecoder \nBack\tTranslation\nST\nSymbolic\tEncoder \nSymbolic\tDecoder \nT2P\nPT\nSymbolic\tEncoder \nProgressive\tDecoder \n<bos>am tag\n<bos>TAGMEHR\nFig. 1.Overview of the Progressive Transformer architecture, showing Text to Gloss\nto Pose (T2G2P) and Text to Pose (T2P) model conﬁgurations. (PT: Progressive\nTransformer, ST: Symbolic Transformer)\nmanner. Our novelties include an alternative formulation of transformer decoding\nfor continuous variable sequences, where there is no pre-deﬁned vocabulary.\nWe introduce a counter decoding technique to predict continuous sequences of\nvariable lengths by tracking the production progress, hence the name Progressive\nTransformers. This approach also enables the driving of timing at inference,\nproducing stable sign pose outputs. We also propose several data augmentation\nmethods that assist in reducing drift in model production.\nAn overview of our approach is shown in Figure 1. We evaluate two diﬀerent\nmodel conﬁgurations, ﬁrst translating from spoken language to sign pose via\ngloss1 intermediary (T2G2P), as this has been shown to increase translation\nperformance [7]. In the second conﬁguration we go direct, translating end-to-end\nfrom spoken language to sign (T2P).\nTo evaluate performance, we propose a back translation evaluation method\nfor SLP, using a Sign Language Translation ( SLT) model to translate back to\nspoken language (dashed lines in Figure 1). We evaluate on the challenging\nRWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset, presenting several\nbenchmark results to underpin future research. We also share qualitative results to\ngive further insight of the models performance to the reader, producing accurate\nsign pose sequences of an unseen text sentence.\nThe rest of this paper is organised as follows: In Section 2, we go over the\nprevious research on SLT and SLP. In Section 3, we introduce our Progressive\nTransformer SLP model. Section 4 outlines the evaluation protocol and presents\nquantitative results, whilst Section 5 showcases qualitative examples. Finally, we\nconclude the paper in Section 6 by discussing our ﬁndings and possible future work.\n1 Glosses are a written representation of sign, deﬁned as minimal lexical items.\nProgressive Transformers for End-to-End SLP 3\n2 Related Work\nSign Language Recognition & Translation: Sign language has been a\nfocus of computer vision researchers for over 30 years [ 4,52,57], primarily on\nisolated Sign Language Recognition ( SLR) [46,56] and, relatively recently, the\nmore demanding task of Continuous Sign Language Recognition ( CSLR) [33,6].\nHowever, the majority of work has relied on manual feature representations\n[11] and statistical temporal modelling [ 60]. The availability of larger datasets,\nsuch as RWTH-PHOENIX-Weather-2014 (PHOENIX14) [17], have enabled the\napplication of deep learning approaches such as Convolutional Neural Networks\n(CNNs) [32,34,36] and Recurrent Neural Networks (RNNs) [12,35].\nDistinct to SLR, the task of SLT was recently introduced by Camgoz et\nal. [7], aiming to directly translate sign videos to spoken language sentences\n[15,31,45,62]. SLT is more challenging than CSLR due to the diﬀerences in\ngrammar and ordering between sign and spoken language. Transformer based\nmodels are the current state-of-the-art in SLT, jointly learning the recognition\nand translation tasks [8].\nSign Language Production: Previous approaches to SLP have extensively\nused animated avatars [ 20,27,42] that can generate realistic sign production,\nbut rely on phrase lookup and pre-generated sequences. Statistical Machine\nTranslation (SMT) has also been applied to SLP [28,37], relying on static rule-\nbased processing that can be diﬃcult to encode.\nRecently, deep learning approaches have been applied to the task of SLP\n[15,53,61]. Stoll et al. present an initial SLP model using a combination of Neural\nMachine Translation (NMT) and Generative Adversarial Networks (GANs) [ 54].\nThe authors break the problem into three separate processes that are trained\nindependently, producing a concatenation of isolated 2D skeleton poses [ 16]\nmapped from sign glosses via a look-up table. Contrary to Stoll et al., our paper\nfocuses on automatic sign production and learning the mapping between text\nand skeleton pose sequences directly, instead of providing this a priori.\nThe closest work to this paper is that of Zelinka et al., who build a neural-\nnetwork-based translator between text and synthesised skeletal pose [ 63]. The\nauthors produce a single sign for each source word with a set size of 7 frames,\ngenerating sequences with a ﬁxed length and ordering. In contrast, our model\nallows a dynamic length of output sign sequence, learning the correct length and\nordering of each word from the data, whilst using counter decoding to determine\nthe end of sequence generation. Unlike [63], who work on a proprietary dataset, we\nproduce results on the publicly available PHOENIX14T, providing a benchmark\nfor future SLP research.\nNeural Machine Translation:NMT aims to learn a mapping between lan-\nguage sequences, generating a target sequence from a source sequence of another\nlanguage. RNNs were ﬁrst proposed to solve the sequence-to-sequence problem,\nwith Kalchbrenner et al. [ 26] introducing a single RNN that iteratively applied\n4 B. Saunders et al.\na hidden state computation. Further models were later developed [ 10,55] that\nintroduced encoder-decoder architectures, mapping both sequences to an inter-\nmediate embedding space. Bahdanau et al. [ 3] overcame the bottleneck problem\nby adding an attention mechanism that facilitated a soft-search over the source\nsentence for the context most useful to the target word prediction.\nTransformer networks [58], a recent NMT breakthrough, are based solely on\nattention mechanisms, generating a representation of the entire source sequence\nwith global dependencies. Multi-Headed Attention ( MHA) is used to model dif-\nferent weighted combinations of an input sequence, improving the representation\npower of the model. Transformers have achieved impressive results in many classic\nNatural Language Processing (NLP) tasks such as language modelling [13,65] and\nsentence representation [14] alongside other domains including image captioning\n[40,66] and action recognition [ 19]. Related to this work, transformer networks\nhave previously been applied to continuous output tasks such as speech synthesis\n[41,50,59], music production [24] and image generation [47].\nApplying NMT methods to continuous output tasks is a relatively underre-\nsearched problem. Encoder-decoder models and RNNs have been used to map\ntext to a human action sequence [ 1,49] whilst adversarial discriminators have\nenabled the production of realistic pose [ 18,39]. In order to determine sequence\nlength of continuous outputs, previous works have used a ﬁxed output size that\nlimits the models ﬂexibility [ 63], a binary end-of-sequence (EOS) ﬂag [ 22] or a\ncontinuous representation of an EOS token [44].\n3 Progressive Transformers\nIn this section, we introduce Progressive Transformers, an SLP model which\nlearns to translate spoken language sentences to continuous sign pose sequences.\nOur objective is to learn the conditional probability p(Y |X) of producing a\nsequence of signs Y = (y1, ..., yU ) with U time steps, given a spoken language\nsentence X = (x1, ..., xT ) with T words. Gloss can also be used as intermediary\nsupervision for the network, formulated as Z = (z1, ..., zN ) with N glosses, where\nthe objective is then to learn the conditional probabilities p(Z|X) and p(Y |Z).\nProducing a target sign sequence from a reference text sequence poses several\nchallenges. Firstly, the sequences have drastically varying length, with the number\nof frames much larger than the number of words ( U >> T). The sequences also\nhave a non-monotonic relationship due to the diﬀerent vocabulary and grammar\nused in sign and spoken languages. Finally, the target signs inhabit a continuous\nvector space requiring a diﬀering representation to the discrete space of text.\nTo address the production of continuous sign sequences, we propose a pro-\ngressive transformer-based architecture that allows translation from a symbolic\nto a continuous sequence domain. We ﬁrst formalise a Symbolic Transformer\narchitecture, converting an input to a symbolic target feature space, as detailed\nin Figure 2a. This is used in our Text to Gloss to Pose ( T2G2P) model to convert\nfrom spoken language to gloss representation as an intermediary step before pose\nproduction, as seen in Figure 1.\nProgressive Transformers for End-to-End SLP 5\nST \nSymbolic\tEmbedding Symbolic\tEmbedding \nSelf-MHA \nFeed\tForward \n(Masked)\tSelf-MHA \nEncoder -Decoder\tMHA \nFeed\tForward \nGLOSS \nSoftmax \nPT \nSymbolic\tEmbedding Continuous\tEmbedding \nSelf-MHA \nFeed\tForward \n(Masked)\tSelf-MHA \nEncoder -Decoder\tMHA \nFeed\tForward \nDec \nEnc \nDec \nEnc \nPE \n PE \n PE \n CE \nGLOSS TEXT TEXT\tor\tGLOSS \na) b) \nFig. 2. Architecture details of (a) Symbolic and (b) Progressive Transformers.\n(ST: Symbolic Transformer, PT: Progressive Transformer, PE: Positional Encoding,\nCE: Counter Embedding, MHA: Multi-Head Attention)\nWe then describe the Progressive Transformer architecture, translating from a\nsymbolic input to a continuous output representation, as shown in Figure 2b. We\nuse this model for the production of realistic and understandable sign language\nsequences, either via gloss supervision in the T2G2P model or direct from spoken\nlanguage in our end-to-end Text to Pose (T2P) model. To enable sequence length\nprediction of a continuous output, we introduce a counter decoding that allows\nthe model to track the progress of sequence generation. In the remainder of this\nsection we describe each component of the architecture in detail.\n3.1 Symbolic Transformer\nWe build on the classic transformer [ 58], a model designed to learn the mapping\nbetween symbolic source and target languages. In this work, Symbolic Trans-\nformers (Figure 2a) translate from source text to target gloss sequences. As\nper the standard NMT pipeline [43], we ﬁrst embed the source, xt, and target,\nzn, tokens via a linear embedding layer, to represent the one-hot-vector in a\nhigher-dimensional space where tokens with similar meanings are closer. Symbolic\nembedding, with weight, W, and bias, b, can be formulated as:\nwt = Wx ·xt + bx, g n = Wz ·zn + bz (1)\nwhere wt and gn are the vector representations of the source and target tokens.\n6 B. Saunders et al.\nTransformer networks do not have a notion of word order, as all source\ntokens are fed to the network simultaneously without positional information.\nTo compensate for this and provide temporal ordering, we apply a temporal\nembedding layer after each input embedding. For the symbolic transformer, we\napply positional encoding [58], as:\nˆwt = wt + PositionalEncoding(t) (2)\nˆgn = gn + PositionalEncoding(n) (3)\nwhere PositionalEncoding is a predeﬁned sinusoidal function conditioned on the\nrelative sequence position t or n.\nOur symbolic transformer model consists of an encoder-decoder architecture.\nThe encoder ﬁrst learns the contextual representation of the source sequence\nthrough self-attention mechanisms, understanding each input token in relation to\nthe full sequence. The decoder then determines the mapping between the source\nand target sequences, aligning the representation sub-spaces and generating target\npredictions in an auto-regressive manner.\nThe symbolic encoder ( ES) consists of a stack of L identical layers, each\ncontaining 2 sub-layers. Given the temporally encoded source embeddings, ˆwt, a\nMHA mechanism ﬁrst generates a weighted contextual representation, performing\nmultiple projections of scaled dot-product attention. This aims to learn the\nrelationship between each token of the sequence and how relevant each time step\nis in the context of the full sequence. Formally, scaled dot-product attention\noutputs a vector combination of values, V , weighted by the relevant queries, Q,\nkeys, K, and dimensionality, dk:\nAttention(Q, K, V) = softmax(QKT\n√dk\n)V (4)\nMHA stacks parallel attention mechanisms inh diﬀerent mappings of the same\nqueries, keys and values, each with varied learnt parameters. This allows diﬀerent\nrepresentations of the input to be generated, learning complementary information\nin diﬀerent sub-spaces. The outputs of each head are then concatenated together\nand projected forward via a ﬁnal linear layer, as:\nMHA(Q, K,V) = [head1, ..., headh] ·WO,\nwhere headi = Attention(QWQ\ni , KWK\ni , V WV\ni ) (5)\nand WO,WQ\ni ,WK\ni and WV\ni are weights related to each input variable.\nThe outputs of MHA are then fed into the second sub-layer of a non-linear\nfeed-forward projection. A residual connection [ 23] and subsequent layer norm\n[2] is employed around each of the sub-layers, to aid training. The ﬁnal symbolic\nencoder output can be formulated as:\nht = ES( ˆwt|ˆw1:T ) (6)\nwhere ht is the contextual representation of the source sequence.\nProgressive Transformers for End-to-End SLP 7\nThe symbolic decoder ( DS) is an auto-regressive architecture that produces\na single token at each time-step. The positionally embedded target sequences,\nˆgn, are passed through an initial MHA self-attention layer similar to the encoder,\nwith an extra masking operation. Alongside the fact that the targets are oﬀset\nfrom the inputs by one position, the masking of future frames prevents the model\nfrom attending to subsequent time steps in the sequence.\nA further MHA sub-layer is then applied, which combines encoder and decoder\nrepresentations and learns the alignment between the source and target sequences.\nThe ﬁnal sub-layer is a feed forward layer, as in the encoder. After all decoder\nlayers are processed, a ﬁnal non-linear feed forward layer is applied, with a\nsoftmax operation to generate the most likely output token at each time step.\nThe output of the symbolic decoder can be formulated as:\nzn+1 = argmax\ni\nDS(ˆgn|ˆg1:n−1, h1:T ) (7)\nwhere zn+1 is the output at time n + 1, from a target vocabulary of size i.\n3.2 Progressive Transformer\nWe now adapt our symbolic transformer architecture to cope with continuous\noutputs, in order to convert source sequences to a continuous target domain.\nIn this work, Progressive Transformers (Figure 2b) translate from the symbolic\ndomains of gloss or text to continuous sign pose sequences that represent the\nmotion of a signer producing a sentence of sign language. The model must produce\nskeleton pose outputs that can both express an accurate translation of the given\ninput sequence and a realistic sign pose sequence.\nWe represent each sign pose frame, yu, as a continuous vector of the 3D\njoint positions of the signer. These joint values are ﬁrst passed through a linear\nembedding layer, allowing sign poses of similar content to be closely represented\nin the dense space. The continuous embedding layer can be formulated as:\nju = Wy ·yu + by (8)\nwhere ju is the embedded 3D joint coordinates of each frame, yu.\nWe next apply a counter embedding layer to the sign poses as temporal\nembedding (CE in Figure 2). The counter, c, holds a value between 0 and\n1, representing the frame position relative to the total sequence length. The\njoint embeddings, ju, are concatenated with the respective counter value, cu,\nformulated as:\nˆju = [ju, CounterEmbedding(u)] (9)\nwhere CounterEmbedding is a linear projection of the counter value for frame u.\nAt each time-step, counter values are predicted alongside the skeleton pose, as\nshown in Figure 3, with sequence generation concluded once the counter reaches\n1. We call this process Counter Decoding, determining the progress of sequence\ngeneration and providing a way to predict the end of sequence without the use\nof a tokenised vocabulary.\n8 B. Saunders et al.\nProgressive\tDecoder \n... \n... \nFig. 3.Counter decoding example, showing the simultaneous prediction of sign pose,\nˆyu, and counter, ˆcu ∈ {0 : 1}, with ˆc = 1.0 denoting end of sequence\nThe counter provides the model with information relating to the length and\nspeed of each sign pose sequence, determining the sign duration. At inference, we\ndrive the sequence generation by replacing the predicted counter value, ˆc, with\nthe ground truth timing information, c∗, to produce a stable output sequence.\nThe Progressive Transformer also consists of an encoder-decoder architecture.\nDue to the input coming from a symbolic source, the encoder has a similar setup\nto the symbolic transformer, learning a contextual representation of the input\nsequence. As the representation will ultimately be used for the end goal of SLP,\nthese representations must also contain suﬃcient context to fully and accurately\nreproduce sign. Taking as input the temporally embedded source embeddings,\nˆwt, the encoder can be formulated as:\nrt = ES( ˆwt|ˆw1:T ) (10)\nwhere ES is the symbolic encoder and rt is the encoded source representation.\nThe progressive decoder ( DP ) is an auto-regressive model that produces\na sign pose frame at each time-step, alongside the counter value described\nabove. Distinct from symbolic transformers, the progressive decoder produces\ncontinuous sequences that hold a sparse representation in a large continuous\nsub-space. The counter-concatenated joint embeddings, ˆju, are extracted as target\ninput, representing the sign information of each frame.\nA self-attention MHA sub-layer is ﬁrst applied, with target masking to avoid\nattending to future positions. A further MHA mechanism is then used to map\nthe symbolic representations from the encoder to the continuous domain of the\ndecoder, learning the important alignment between spoken and sign languages.\nA ﬁnal feed forward sub-layer follows, with each sub-layer followed by a\nresidual connection and layer normalisation as before. No softmax layer is used\nas the skeleton joint coordinates can be regressed directly and do not require\nProgressive Transformers for End-to-End SLP 9\nstochastic prediction. The progressive decoder output can be formulated as:\n[ˆyu+1, ˆcu+1] = DP (ˆju|ˆj1:u−1, r1:T ) (11)\nwhere ˆyu+1 corresponds to the 3D joint positions representing the produced\nsign pose of frame u + 1 and ˆcu+1 is the respective counter value. The decoder\nlearns to generate one frame at a time until the predicted counter value reaches 1,\ndetermining the end of sequence. Once the full sign pose sequence is produced, the\nmodel is trained end-to-end using the Mean Squared Error ( MSE) loss between\nthe predicted sequence, ˆy1:U , and the ground truth, y∗\n1:U :\nLMSE = 1\nU\nu∑\ni=1\n(y∗\n1:U −ˆy1:U )2 (12)\nThe progressive transformer outputs, ˆy1:U , represent the 3D skeleton joint\npositions of each frame of a produced sign sequence. To ease the visual comparison\nwith reference sequences, we apply Dynamic Time Warping ( DTW) [5] to align\nthe produced sign pose sequences. Animating a video from this sequence is then\na trivial task, plotting the joints and connecting the relevant bones, with timing\ninformation provided from the counter. These 3D joints could subsequently be\nused to animate an avatar [30,42] or condition a GAN [25,67].\n4 Quantitative Experiments\nIn this section, we share our SLP experimental setup and report experimental\nresults. We ﬁrst provide dataset and evaluation details, outlining back trans-\nlation. We then evaluate both symbolic and progressive transformer models,\ndemonstrating results of data augmentation and model conﬁguration.\n4.1 Sign Language Production Dataset\nForster et al. released PHOENIX14 [17] as a large video-based corpus containing\nparallel sequences of German Sign Language - Deutsche Gebrdensprache ( DGS)\nand spoken text extracted from German weather forecast recordings. This dataset\nis ideal for computational sign language research due to the provision of gloss\nlevel annotations, becoming the primary benchmark for both SLR and CSLR.\nIn this work, we use the publicly available PHOENIX14T dataset introduced\nby Camgoz et al. [ 7], a continuous SLT extension of the original PHOENIX14.\nThis corpus includes parallel sign videos and German translation sequences with\nredeﬁned segmentation boundaries generated using the forced alignment approach\nof [36]. 8257 videos of 9 diﬀerent signers are provided, with a vocabulary of 2887\nGerman words and 1066 diﬀerent sign glosses from a combined 835,356 frames.\nWe train our SLP network to generate sequences of 3D skeleton pose. 2D\njoint positions are ﬁrst extracted from each video using OpenPose [ 9]. We then\nutilise the skeletal model estimation improvements presented in [ 63] to lift the\n10 B. Saunders et al.\nFig. 4.Skeleton pose extraction, using OpenPose [9] and 2D to 3D mapping [63]\n2D joint positions to 3D. An iterative inverse kinematics approach is applied\nto minimise 3D pose whilst maintaining consistent bone length and correcting\nmisplaced joints. Finally, we apply skeleton normalisation similar to [ 53] and\nrepresent 3D joints as x, y and z coordinates. An example is shown in Figure 4.\n4.2 Evaluation Details\nIn this work, we propose back-translation as a means of SLP evaluation, trans-\nlating back from produced sign to spoken language. This provides a measure of\nhow understandable the productions are, and how much translation content is\npreserved. Evaluation of a generative model is often diﬃcult but we ﬁnd a close\ncorrespondence between back translation score and the visual production quality.\nWe liken it to the wide use of the inception score for generative models [ 51], using\na pre-trained classiﬁer. Similarly, recent SLP work used an SLR discriminator to\nevaluate isolated skeletons [61], but did not measure the translation performance.\nWe utilise the state-of-the-art SLT [8] as our back translation model, modiﬁed\nto take sign pose sequences as input. This is again trained on the PHOENIX14T\ndataset, ensuring a robust translation from sign to text. We generate spoken\nlanguage translations of the produced sign pose sequences and compute BLEU\nand ROUGE scores. We provide BLEU n-grams from 1 to 4 for completeness.\nIn the following experiments, our symbolic and progressive transformer models\nare each built with 2 layers, 8 heads and embedding size of 256. All parts of our\nnetwork are trained with Xavier initialisation [ 21], Adam optimization [ 29] with\ndefault parameters and a learning rate of 10 −3. Our code is based on Kreutzer et\nal.’s NMT toolkit, JoeyNMT [38], and implemented using PyTorch [48].\n4.3 Symbolic Transformer: Text to Gloss\nOur ﬁrst experiment measures the performance of the symbolic transformer\narchitecture for sign language understanding. We train our symbolic transformer\nto predict gloss representations from source spoken language sentences. Table\n1 shows our model achieves state-of-the-art results, signiﬁcantly outperforming\nthat of Stoll et al. [ 53], who use an encoder-decoder network with 4 layers of\n1000 Gated Recurrent Units (GRUs). This supports our use of the proposed\ntransformer architecture for sign language understanding.\nProgressive Transformers for End-to-End SLP 11\nTable 1.Symbolic Transformer results for Text to Gloss translation\nDEV SET TEST SET\nApproach: BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGEBLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\nStoll et al. [53]16.34 22.30 32.47 50.15 48.42 15.26 21.54 32.25 50.67 48.10\nOurs20.23 27.36 38.21 55.65 55.41 19.10 26.24 37.10 55.18 54.55\nTable 2. Progressive Transformer results for Gloss to Sign Pose production, with\nmultiple data augmentation techniques. FP: Future Prediction, GN: Gaussian Noise\nDEV SET TEST SET\nApproach: BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGEBLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\nBase 7.04 9.10 13.12 24.20 25.53 5.03 6.89 10.81 23.03 23.31\nFuture Prediction9.96 12.71 17.83 30.03 31.03 8.38 11.04 16.41 28.94 29.73\nJust Counter11.04 13.86 19.05 31.16 32.45 9.16 11.96 17.41 30.08 30.41\nGaussian Noise11.88 15.07 20.61 32.53 34.19 10.02 12.96 18.58 31.11 31.83\nFP & GN11.93 15.08 20.50 32.40 34.01 10.43 13.51 19.19 31.80 32.02\n4.4 Progressive Transformer: Gloss to Pose\nIn our next set of experiments, we evaluate our progressive transformer and its\ncapability to produce a continuous sign pose sequence from a given symbolic\ninput. As a baseline, we train a progressive transformer model to translate from\ngloss to sign pose without augmentation, with results shown in Table 2 (Base).\nWe believe our base progressive model suﬀers from prediction drift, with\nerroneous predictions accumulating over time. As transformer models are trained\nto predict the next time-step of all ground truth inputs, they are often not robust\nto noise in target inputs. At inference time, with predictions based oﬀ previous\noutputs, errors are propagated throughout the full sequence generation, quickly\nleading to poor quality production. The impact of drift is heightened due to\nthe continuous distribution of the target skeleton poses. As neighbouring frames\ndiﬀer little in content, a model learns to just copy the previous ground truth\ninput and receive a small loss penalty. We thus experiment with various data\naugmentation approaches in order to overcome drift and improve performance.\nFuture PredictionOur ﬁrst data augmentation method is conditional future\nprediction, requiring the model to predict more than just the next frame in the\nsequence. Experimentally, we ﬁnd the best performance comes from a prediction\nof all of the next 10 frames from the current time step. As can be seen in Table 2,\nprediction of future time steps increases performance from the base architecture.\nWe believe this is because the model now cannot rely on just copying the previous\nframe, as there are more considerable changes to the skeleton positions in 10\nframes time. The underlying structure and movement of sign has to be learnt,\nencoding how each gloss is represented and reproduced in the training data.\nJust Counter Inspired by the memorisation capabilities of transformer models,\nwe next experiment with a pure memorisation approach. Only the counter\nvalues are provided as target input to the model, as opposed to the usual full\n12 B. Saunders et al.\nTable 3. Results of the Text2Pose (T2P) and Text2Gloss2Pose (T2G2P) network\nconﬁgurations for Text to Sign Pose production\nDEV SET TEST SET\nConﬁguration:BLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGEBLEU-4 BLEU-3 BLEU-2 BLEU-1 ROUGE\nT2P 11.82 14.80 19.97 31.41 33.18 10.51 13.54 19.04 31.36 32.46\nT2G2P11.43 14.71 20.71 33.12 34.05 9.68 12.53 17.62 29.74 31.07\n3D skeleton joint positions. We show a further performance increase with this\napproach, considerably increasing the BLEU-4 score as shown in Table 2.\nWe believe the just counter model setup helps to allay the eﬀect of drift, as\nthe model now must learn to decode the target sign pose solely from the counter\nposition, without relying on the ground truth joint embeddings it previously had\naccess to. This setup is now identical at both training and inference, with the\nmodel having to generalise only to new data rather than new prediction inputs.\nGaussian Noise Our ﬁnal augmentation experiment examines the eﬀect of\napplying noise to the skeleton pose sequences during training, increasing the\nvariety of data to train a more robust model. For each joint, statistics on the\npositional distribution of the previous epoch are collected, with randomly sampled\nnoise applied to the inputs of the next epoch. Applied noise is multiplied by\na noise factor, rn, with empirical validation suggesting rn = 5 gives the best\nperformance. An increase of Gaussian noise causes the model to become more\nrobust to prediction inputs, as it must learn to correct the augmented inputs\nback to the target outputs.\nTable 2 (FP & GN) shows that the best BLEU-4 performance comes from a\ncombination of future prediction and Gaussian noise augmentation. The model\nmust learn to cope with both multi-frame prediction and a noisy input, building\na ﬁrm robustness to drift. We continue with this setup for further experiments.\n4.5 Text2Pose v Text2Gloss2Pose\nOur ﬁnal experiment evaluates the two network conﬁgurations outlined in Figure\n1, sign production either direct from text or via a gloss intermediary. Text to Pose\n(T2P) consists of a single progressive transformer model with spoken language\ninput, learning to jointly translate from the domain of spoken language to sign\nand subsequently produce meaningful sign representations. Text to Gloss to Pose\n(T2G2P) uses an initial symbolic transformer to convert to gloss, which is then\ninput into a further progressive transformer to produce sign pose sequences.\nAs can be seen from Table 3, the T2P model outperforms that of T2G2P.\nThis is surprising, as a large body of previous work has suggested that using\ngloss as intermediary helps networks learn [7]. However, we believe this is because\nthere is more information available within spoken language compared to a\ngloss representation, with more tokens per sequence to predict from. Predicting\ngloss sequences as an intermediary can act as a bottleneck, as all information\nProgressive Transformers for End-to-End SLP 13\nInputGround Truth Produced Output\nheute\tnacht\tminus\tzwei\tgrad\tan\tder\tnordsee\tin\tder\tmitte\tmitunter\twerte\tunter\tminus\tzwanzig\tgrad \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(trans:\t tonight\tminus\ttwo\tdegrees\ton\tthe\tnorth\tsea\tin\tthe\tmiddle\tsometimes\tbelow\tminus\ttwenty\tdegrees ) \nund\tnun\tdie\twettervorhersage\tfür\tmor gen\tsonntag\tden\tsechsten\tseptember\t \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(trans:\t and\tnow\tthe\tweather\tforecast\tfor\ttomorrow\tsunday\tthe\tsixth\tof\tseptember ) \nInputGround Truth Produced Output\nFig. 5.Examples of produced sign pose sequences. The top row shows the spoken\nlanguage input from the unseen validation set alongside English translation. The middle\nrow presents our produced sign pose sequence from this text input, with the bottom\nrow displaying the ground truth video for comparison.\nrequired for production needs to be present in the gloss. Therefore, any contextual\ninformation present in the source text can be lost.\nThe success of the T2P network shows that our progressive transformer\nmodel is powerful enough to complete two sub-tasks; ﬁrstly mapping spoken\nlanguage sequences to a sign representation, then producing an accurate sign\npose recreation. This is important for future scaling and application of the SLP\nmodel architecture, as many sign language domains do not have gloss availability.\nFurthermore, our ﬁnal BLEU-4 scores outperform similar end-to-end Sign to\nText methods which do not utilize gloss information [ 7] (9.94 BLEU-4). Note\nthat this is an unfair direct comparison, but it does provide an indication of\nmodel performance and the quality of the produced sign pose sequences.\n5 Qualitative Experiments\nIn this section we report qualitative results for our progressive transformer model.\nWe share snapshot examples of produced sign pose sequences in Figure 5, with\nmore examples provided in supplementary material. The unseen spoken language\n14 B. Saunders et al.\nsequence is shown as input alongside the sign pose sequence produced by our\nProgressive Transformer model, with ground truth video for comparison.\nAs can be seen from the provided examples, our SLP model produces visually\npleasing and realistic looking sign with a close correspondence to the ground truth\nvideo. Body motion is smooth and accurate, whilst hand shapes are meaningful\nif a little under-expressed. We ﬁnd that the most diﬃcult production occurs with\nproper nouns and speciﬁc entities, due to the lack of grammatical context and\nexamples in the training data.\nThese examples show that regressing continuous sequences can be successfully\nachieved using an attention-based mechanism. The predicted joint locations for\nneighbouring frames are closely positioned, showing that the model has learnt the\nsubtle movement of the signer. Smooth transitions between signs are produced,\nhighlighting a diﬀerence from the discrete generation of spoken language.\n6 Conclusion\nSign Language Production ( SLP) is an important task to improve communica-\ntion between the Deaf and hearing. Previous work has focused on producing\nconcatenated isolated signs instead of full continuous sign language sequences. In\nthis paper, we proposed Progressive Transformers, a novel transformer architec-\nture that can translate from discrete spoken language to continuous sign pose\nsequences. We introduced a counter decoding that enables continuous sequence\ngeneration without the need for an explicit end of sequence token. Two model\nconﬁgurations were presented, an end-to-end network that produces sign direct\nfrom text and a stacked network that utilises a gloss intermediary.\nWe evaluated our approach on the challengingPHOENIX14T dataset, setting\nbaselines for future research with a back translation evaluation mechanism. Our\nexperiments showed the importance of several data augmentation techniques\nto reduce model drift and improve SLP performance. Furthermore, we have\nshown that a direct text to pose translation conﬁguration can outperform a gloss\nintermediary model, meaning SLP models are not limited to only training on\ndata where expensive gloss annotation is available.\nAs future work, we would like to expand our network to multi-channel sign\nproduction, focusing on non-manual aspects of sign language such as body pose,\nfacial expressions and mouthings. It would be interesting to condition a GAN to\nproduce sign videos, learning a prior for each sign represented in the data.\n7 Acknowledgements\nThis work received funding from the SNSF Sinergia project ’SMILE’ (CRSII2\n160811), the European Union’s Horizon2020 research and innovation programme\nunder grant agreement no. 762021 ’Content4All’ and the EPSRC project ’ExTOL’\n(EP/R03298X/1). This work reﬂects only the authors view and the Commission\nis not responsible for any use that may be made of the information it contains.\nWe would also like to thank NVIDIA Corporation for their GPU grant.\nProgressive Transformers for End-to-End SLP 15\nReferences\n1. Ahn, H., Ha, T., Choi, Y., Yoo, H., Oh, S.: Text2Action: Generative Adversarial\nSynthesis from Language to Action. In: International Conference on Robotics and\nAutomation (ICRA) (2018)\n2. Ba, J.L., Kiros, J.R., Hinton, G.E.: Layer Normalization. arXiv preprint\narXiv:1607.06450 (2016)\n3. Bahdanau, D., Cho, K., Bengio, Y.: Neural Machine Translation by Jointly Learning\nto Align and Translate. arXiv:1409.0473 (2014)\n4. Bauer, B., Hienz, H., Kraiss, K.F.: Video-Based Continuous Sign Language Recog-\nnition using Statistical Methods. In: Proceedings of 15th International Conference\non Pattern Recognition (ICPR) (2000)\n5. Berndt, D.J., Cliﬀord, J.: Using Dynamic Time Warping to Find Patterns in Time\nSeries. In: AAA1 Workshop on Knowledge Discovery in Databases (KDD) (1994)\n6. Camgoz, N.C., Hadﬁeld, S., Koller, O., Bowden, R.: SubUNets: End-to-end Hand\nShape and Continuous Sign Language Recognition. In: Proceedings of the IEEE\nInternational Conference on Computer Vision (ICCV) (2017)\n7. Camgoz, N.C., Hadﬁeld, S., Koller, O., Ney, H., Bowden, R.: Neural Sign Language\nTranslation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR) (2018)\n8. Camgoz, N.C., Koller, O., Hadﬁeld, S., Bowden, R.: Sign Language Transformers:\nJoint End-to-end Sign Language Recognition and Translation. In: Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2020)\n9. Cao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: OpenPose: Realtime Multi-\nPerson 2D Pose Estimation using Part Aﬃnity Fields. In: Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) (2017)\n10. Cho, K., van Merri¨ enboer, B., Bahdanau, D., Bengio, Y.: On the Properties of\nNeural Machine Translation: Encoder–Decoder Approaches. In: Proceedings of the\nSyntax, Semantics and Structure in Statistical Translation (SSST) (2014)\n11. Cooper, H., Ong, E.J., Pugeault, N., Bowden, R.: Sign Language Recognition using\nSub-units. Journal of Machine Learning Research (JMLR) 13 (2012)\n12. Cui, R., Liu, H., Zhang, C.: Recurrent Convolutional Neural Networks for Contin-\nuous Sign Language Recognition by Staged Optimization. In: Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017)\n13. Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R.: Transformer-\nXL: Attentive Language Models Beyond a Fixed-Length Context. In: International\nConference on Learning Representations (ICLR) (2019)\n14. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep\nBidirectional Transformers for Language Understanding. In: Proceedings of the\nConference of the North American Chapter of the Association for Computational\nLinguistics (ACL) (2018)\n15. Duarte, A.C.: Cross-modal Neural Sign Language Translation. In: Proceedings of\nthe ACM International Conference on Multimedia (ICME) (2019)\n16. Ebling, S., Camg¨ oz, N.C., Braem, P.B., Tissi, K., Sidler-Miserez, S., Stoll, S.,\nHadﬁeld, S., Haug, T., Bowden, R., Tornay, S., et al.: SMILE: Swiss German Sign\nLanguage Dataset. In: Proceedings of the International Conference on Language\nResources and Evaluation (LREC) (2018)\n17. Forster, J., Schmidt, C., Koller, O., Bellgardt, M., Ney, H.: Extensions of the Sign\nLanguage Recognition and Translation Corpus RWTH-PHOENIX-Weather. In:\nProceedings of the International Conference on Language Resources and Evaluation\n(LREC) (2014)\n16 B. Saunders et al.\n18. Ginosar, S., Bar, A., Kohavi, G., Chan, C., Owens, A., Malik, J.: Learning Individual\nStyles of Conversational Gesture. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) (2019)\n19. Girdhar, R., Carreira, J., Doersch, C., Zisserman, A.: Video Action Transformer\nNetwork. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2019)\n20. Glauert, J., Elliott, R., Cox, S., Tryggvason, J., Sheard, M.: VANESSA: A System\nfor Communication between Deaf and Hearing People. Technology and Disability\n(2006)\n21. Glorot, X., Bengio, Y.: Understanding the Diﬃculty of Training Deep Feedforward\nNeural Networks. In: Proceedings of the International Conference on Artiﬁcial\nIntelligence and Statistics (AISTATS) (2010)\n22. Graves, A.: Generating Sequences With Recurrent Neural Networks. arXiv preprint\narXiv:1308.0850 (2013)\n23. He, K., Zhang, X., Ren, S., Sun, J.: Deep Residual Learning for Image Recogni-\ntion. In: Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (2016)\n24. Huang, C.Z.A., Vaswani, A., Uszkoreit, J., Shazeer, N., Simon, I., Hawthorne,\nC., Dai, A.M., Hoﬀman, M.D., Dinculescu, M., Eck, D.: Music Transformer. In:\nInternational Conference on Learning Representations (ICLR) (2018)\n25. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-Image Translation with Condi-\ntional Adversarial Networks. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) (2017)\n26. Kalchbrenner, N., Blunsom, P.: Recurrent Continuous Translation Models. In: Pro-\nceedings of the Conference on Empirical Methods in Natural Language Processing\n(EMNLP) (2013)\n27. Karpouzis, K., Caridakis, G., Fotinea, S.E., Efthimiou, E.: Educational Resources\nand Implementation of a Greek Sign Language Synthesis Architecture. Computers\n& Education (CAEO) (2007)\n28. Kayahan, D., G¨ ung¨ or, T.: A Hybrid Translation System from Turkish Spoken Lan-\nguage to Turkish Sign Language. In: IEEE International Symposium on INnovations\nin Intelligent SysTems and Applications (INISTA) (2019)\n29. Kingma, D.P., Ba, J.: Adam: A Method for Stochastic Optimization. In: Proceedings\nof the International Conference on Learning Representations (ICLR) (2014)\n30. Kipp, M., Heloir, A., Nguyen, Q.: Sign Language Avatars: Animation and com-\nprehensibility. In: International Workshop on Intelligent Virtual Agents (IVA)\n(2011)\n31. Ko, S.K., Kim, C.J., Jung, H., Cho, C.: Neural Sign Language Translation based\non Human Keypoint Estimation. Applied Sciences (2019)\n32. Koller, O., Camgoz, N.C., Bowden, R., Ney, H.: Weakly Supervised Learning\nwith Multi-Stream CNN-LSTM-HMMs to Discover Sequential Parallelism in Sign\nLanguage Videos. IEEE Transactions on Pattern Analysis and Machine Intelligence\n(TPAMI) (2019)\n33. Koller, O., Forster, J., Ney, H.: Continuous Sign Language Recognition: Towards\nLarge Vocabulary Statistical Recognition Systems Handling Multiple Signers. Com-\nputer Vision and Image Understanding (CVIU) (2015)\n34. Koller, O., Ney, H., Bowden, R.: Deep Hand: How to Train a CNN on 1 Million\nHand Images When Your Data Is Continuous and Weakly Labelled. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(2016)\nProgressive Transformers for End-to-End SLP 17\n35. Koller, O., Zargaran, S., Ney, H.: Re-Sign: Re-Aligned End-to-End Sequence Mod-\nelling with Deep Recurrent CNN-HMMs. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) (2017)\n36. Koller, O., Zargaran, S., Ney, H., Bowden, R.: Deep Sign: Hybrid CNN-HMM for\nContinuous Sign Language Recognition. In: Proceedings of the British Machine\nVision Conference (BMVC) (2016)\n37. Kouremenos, D., Ntalianis, K.S., Siolas, G., Stafylopatis, A.: Statistical Machine\nTranslation for Greek to Greek Sign Language Using Parallel Corpora Produced\nvia Rule-Based Machine Translation. In: IEEE 31st International Conference on\nTools with Artiﬁcial Intelligence (ICTAI) (2018)\n38. Kreutzer, J., Bastings, J., Riezler, S.: Joey NMT: A Minimalist NMT Toolkit\nfor Novices. In: Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) (2019)\n39. Lee, H.Y., Yang, X., Liu, M.Y., Wang, T.C., Lu, Y.D., Yang, M.H., Kautz, J.:\nDancing to Music. In: Advances in Neural Information Processing Systems (NIPS)\n(2019)\n40. Li, G., Zhu, L., Liu, P., Yang, Y.: Entangled Transformer for Image Captioning. In:\nProceedings of the IEEE International Conference on Computer Vision (CVPR)\n(2019)\n41. Li, N., Liu, S., Liu, Y., Zhao, S., Liu, M.: Neural Speech Synthesis with Transformer\nNetwork. In: Proceedings of the AAAI Conference on Artiﬁcial Intelligence (2019)\n42. McDonald, J., Wolfe, R., Schnepp, J., Hochgesang, J., Jamrozik, D.G., Stumbo,\nM., Berke, L., Bialek, M., Thomas, F.: Automated Technique for Real-Time Pro-\nduction of Lifelike Animations of American Sign Language. Universal Access in the\nInformation Society (UAIS) (2016)\n43. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed Represen-\ntations of Words and Phrases and their Compositionality. In: Advances in Neural\nInformation Processing Systems (NIPS) (2013)\n44. Mukherjee, S., Ghosh, S., Ghosh, S., Kumar, P., Roy, P.P.: Predicting Video-\nframes Using Encoder-convlstm Combination. In: IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP) (2019)\n45. Orbay, A., Akarun, L.: Neural Sign Language Translation by Learning Tokenization.\narXiv preprint arXiv:2002.00479 (2020)\n46. ¨Ozdemir, O., Camg¨ oz, N.C., Akarun, L.: Isolated Sign Language Recognition\nusing Improved Dense Trajectories. In: Proceedings of the Signal Processing and\nCommunication Application Conference (SIU) (2016)\n47. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser,  L., Shazeer, N., Ku, A., Tran, D.:\nImage Transformer. In: International Conference on Machine Learning (ICML)\n(2018)\n48. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,\nDesmaison, A., Antiga, L., Lerer, A.: Automatic Diﬀerentiation in PyTorch. In:\nNIPS Autodiﬀ Workshop (2017)\n49. Plappert, M., Mandery, C., Asfour, T.: Learning a Bidirectional Mapping between\nHuman Whole-Body Motion and Natural Language using Deep Recurrent Neural\nNetworks. Robotics and Autonomous Systems (2018)\n50. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., Liu, T.Y.: FastSpeech:\nFast, Robust and Controllable Text to Speech. In: Advances in Neural Information\nProcessing Systems (NIPS) (2019)\n51. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., Chen, X.:\nImproved Techniques for Training GANs. In: Advances in Neural Information\nProcessing Systems (NIPS) (2016)\n18 B. Saunders et al.\n52. Starner, T., Pentland, A.: Real-time American Sign Language Recognition from\nVideo using Hidden Markov Models. Motion-Based Recognition (1997)\n53. Stoll, S., Camgoz, N.C., Hadﬁeld, S., Bowden, R.: Sign Language Production using\nNeural Machine Translation and Generative Adversarial Networks. In: Proceedings\nof the British Machine Vision Conference (BMVC) (2018)\n54. Stoll, S., Camgoz, N.C., Hadﬁeld, S., Bowden, R.: Text2Sign: Towards Sign Language\nProduction using Neural Machine Translation and Generative Adversarial Networks.\nInternational Journal of Computer Vision (IJCV) (2020)\n55. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to Sequence Learning with Neural\nNetworks. In: Proceedings of the Advances in Neural Information Processing Systems\n(NIPS) (2014)\n56. S¨ uzg¨ un, M.,¨Ozdemir, H., Camg¨ oz, N., Kındıro˘ glu, A., Ba¸ saran, D., Togay, C.,\nAkarun, L.: Hospisign: An Interactive Sign Language Platform for Hearing Impaired.\nJournal of Naval Sciences and Engineering (JNSE) (2015)\n57. Tamura, S., Kawasaki, S.: Recognition of Sign Language Motion Images. Pattern\nRecognition (1988)\n58. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention Is All You Need. In: Advances in Neural Information\nProcessing Systems (NIPS) (2017)\n59. Vila, L.C., Escolano, C., Fonollosa, J.A., Costa-juss` a, M.R.: End-to-End Speech\nTranslation with the Transformer. In: Advances in Speech and Language Technolo-\ngies for Iberian Languages (IberSPEECH) (2018)\n60. Vogler, C., Metaxas, D.: Parallel Midden Markov Models for American Sign Lan-\nguage Recognition. In: Proceedings of the IEEE International Conference on Com-\nputer Vision (ICCV) (1999)\n61. Xiao, Q., Qin, M., Yin, Y.: Skeleton-based Chinese Sign Language Recognition and\nGeneration for Bidirectional Communication between Deaf and Hearing People. In:\nNeural Networks (2020)\n62. Yin, K.: Sign Language Translation with Transformers. arXiv preprint\narXiv:2004.00588 (2020)\n63. Zelinka, J., Kanis, J.: Neural Sign Language Synthesis: Words Are Our Glosses. In:\nThe IEEE Winter Conference on Applications of Computer Vision (WACV) (2020)\n64. Zelinka, J., Kanis, J., Salajka, P.: NN-Based Czech Sign Language Synthesis. In:\nInternational Conference on Speech and Computer (SPECOM) (2019)\n65. Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., Liu, Q.: ERNIE: Enhanced\nLanguage Representation with Informative Entities. In: 57th Annual Meeting of\nthe Association for Computational Linguistics (ACL) (2019)\n66. Zhou, L., Zhou, Y., Corso, J.J., Socher, R., Xiong, C.: End-to-End Dense Video\nCaptioning with Masked Transformer. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) (2018)\n67. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired Image-to-Image Translation us-\ning Cycle-Consistent Adversarial Networks. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) (2017)\nProgressive Transformers for End-to-End Sign\nLanguage Production: Supplementary Material\nBen Saunders, Necati Cihan Camgoz, and Richard Bowden\nUniversity of Surrey\n{b.saunders,n.camgoz,r.bowden}@surrey.ac.uk\nIn this supplementary material, we give further qualitative results for our\nProgressive Transformer SLP model. We share snapshot examples of produced\nsign pose sequences in Figures 1 and 2, showing (a) input spoken language\nalongside (b) produced sign pose sequence, with (c) ground truth pose and (d)\noriginal video provided for comparison.\nund\tnun\tdie\twettervorhersage\tfür\tmor gen\tsonntag\tden\tsechsten\tseptember\t \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(trans:\t and\tnow\tthe\tweather\tforecast\tfor\ttomorrow\tfriday\tthe\t21st\tof\tmay ) \nInputProduced PoseGround Truth PoseOriginal Video\nmittwoch\tund\tdonnerstag\twechselhaft\thier\tund\tda\tfällt\tetwas\tregen\toder\tschnee\tder\tzunehmend\tauch\tin\tden\tniederungen\tmöglich\tist \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(trans:\t W ednesdays\tand\tThursdays\tchangeable\there\tand\tthere\ta\tlittle\train\tor\tsnow\tthat\tis\tincreasingly\tpossible\tin\tthe\tlowlands ) \nInputProduced PoseGround Truth PoseOriginal Video\na) \nb) \nc) \nd) \na) \nb) \nc) \nd) \nFig. 1.Sign language production examples showing (a) spoken language input, (b)\nproduced sign pose, (c) ground truth pose and (d) original video.\n20 B. Saunders et al.\ndas\thoch\tüber\tden\tazoren\tdehnt\tsich\tüber\tmitteleuropa\tnach\tosten\taus\tund\tsor gt\tmor gen\tkurzzeitig\tfür\tmeist\tfreundliches\twetter \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(trans:\t the\thigh\tabove\tthe\tazores\tstretches\teastwards\tover\tcentral\teurope\tand\ttomorrow\twill\tprovide\tmostly\tfriendly\tweather ) \nInputProduced PoseGround Truth PoseOriginal Video\nam\tdienstag\tin\tder\tsüdhälfte\tweitere\tschauer\tvereinzelt\tauch\tgewitter\tin\tder\tnordhälfte\tfreundliche\tabschnitte \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(trans:\t on\ttuesday\tin\tthe\tsouthern\thalf\tmore\tshowers\toccasionally\tthunderstorms\tin\tthe\tnorthern\thalf\tfriendly\tsections ) \nInputProduced PoseGround Truth PoseOriginal Video\nin\tder\twesthälfte\tgeht\tder\tregen\tin\tschauer\tüber\tin\tden\tber gen\tschneeschauer \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t(trans:\t in\tthe\twestern\thalf\tthe\train\tturns\tinto\tshowers\tin\tthe\tmountains\tsnow\tshowers ) \nInputProduced PoseGround Truth PoseOriginal Video\na) \nb) \nc) \nd) \na) \nb) \nc) \nd) \na) \nb) \nc) \nd) \nFig. 2.Sign language production examples showing (a) spoken language input, (b)\nproduced sign pose, (c) ground truth pose and (d) original video.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7930637001991272
    },
    {
      "name": "Sign language",
      "score": 0.723441481590271
    },
    {
      "name": "Transformer",
      "score": 0.6417953968048096
    },
    {
      "name": "Inference",
      "score": 0.5443394780158997
    },
    {
      "name": "Architecture",
      "score": 0.49761632084846497
    },
    {
      "name": "End-to-end principle",
      "score": 0.4663704037666321
    },
    {
      "name": "Spoken language",
      "score": 0.4640428423881531
    },
    {
      "name": "Speech recognition",
      "score": 0.4379065930843353
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.4316398799419403
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40042030811309814
    },
    {
      "name": "Natural language processing",
      "score": 0.3447820544242859
    },
    {
      "name": "Engineering",
      "score": 0.1295514702796936
    },
    {
      "name": "Linguistics",
      "score": 0.10332179069519043
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}