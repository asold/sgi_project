{
  "title": "Interpreting Word-Level Hidden State Behaviour of Character-Level LSTM Language Models",
  "url": "https://openalex.org/W2911665948",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2914627570",
      "name": "Avery Hiebert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2148729386",
      "name": "Cole Peterson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A212974769",
      "name": "Alona Fyshe",
      "affiliations": [
        "University of Alberta"
      ]
    },
    {
      "id": "https://openalex.org/A2304559919",
      "name": "Nishant Mehta",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2601243251",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2087946919",
    "https://openalex.org/W1899794420",
    "https://openalex.org/W2292919134",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2428136159",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W4248358431",
    "https://openalex.org/W2799149681",
    "https://openalex.org/W2752194699",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W1951216520",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2116261113",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2101609803",
    "https://openalex.org/W2949563612",
    "https://openalex.org/W2964159778",
    "https://openalex.org/W591184081",
    "https://openalex.org/W2136848157",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2251253014",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W151377110",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2554915555"
  ],
  "abstract": "While Long Short-Term Memory networks (LSTMs) and other forms of recurrent neural network have been successfully applied to language modeling on a character level, the hidden state dynamics of these models can be difficult to interpret. We investigate the hidden states of such a model by using the HDBSCAN clustering algorithm to identify points in the text at which the hidden state is similar. Focusing on whitespace characters prior to the beginning of a word reveals interpretable clusters that offer insight into how the LSTM may combine contextual and character-level information to identify parts of speech. We also introduce a method for deriving word vectors from the hidden state representation in order to investigate the word-level knowledge of the model. These word vectors encode meaningful semantic information even for words that appear only once in the training text.",
  "full_text": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 258–266\nBrussels, Belgium, November 1, 2018.c⃝2018 Association for Computational Linguistics\n258\nInterpreting Word-Level Hidden State Behaviour of Character-Level\nLSTM Language Models\nAvery Hiebert†∗, Cole Peterson†, Alona Fyshe‡, Nishant A. Mehta†\n†Department of Computer Science, University of Victoria, Canada\n‡Computing Science / Psychology Departments, University of Alberta, Canada\naveryhiebert@gmail.com, cpeterso@uvic.ca\nalona@ualberta.ca, nmehta@uvic.ca\nAbstract\nWhile Long Short-Term Memory networks\n(LSTMs) and other forms of recurrent neural\nnetwork have been successfully applied to lan-\nguage modeling on a character level, the hid-\nden state dynamics of these models can be\ndifﬁcult to interpret. We investigate the hid-\nden states of such a model by using the HDB-\nSCAN clustering algorithm to identify points\nin the text at which the hidden state is similar.\nFocusing on whitespace characters prior to the\nbeginning of a word reveals interpretable clus-\nters that offer insight into how the LSTM may\ncombine contextual and character-level infor-\nmation to identify parts of speech. We also\nintroduce a method for deriving word vectors\nfrom the hidden state representation in order\nto investigate the word-level knowledge of the\nmodel. These word vectors encode meaning-\nful semantic information even for words that\nappear only once in the training text.\n1 Introduction\nRecurrent Neural Networks (RNNs), including\nLong Short-Term Memory (LSTM) networks\n(Hochreiter and Schmidhuber, 1997; Gers et al.,\n2000), have been widely applied to natural lan-\nguage processing tasks including character-level\nlanguage modeling (Mikolov et al., 2012; Graves,\n2013). However, like other types of neural net-\nworks, the hidden states and behaviour of a given\nLSTM can be difﬁcult to understand and interpret,\ndue to both the distributed nature of the hidden\nstate representations and the relatively opaque re-\nlationship between the hidden state and the ﬁnal\noutput of the network. It is also not clear how a\ncharacter-level LSTM language model takes ad-\nvantage of orthographic patterns to infer higher-\nlevel information.\n∗Corresponding author\nIn this paper, we investigate the hidden state dy-\nnamics of a character-level LSTM language model\nboth directly and — through the use of output gate\nactivations — indirectly. As an overview, our main\ncontributions are:\n1. We use clustering to investigate similar hid-\nden states (and output gate activations) at dif-\nferent points in a text, paying special atten-\ntion to whitespace characters. We provide in-\nsight into the model’s awareness of both or-\nthographic patterns and word-level grammat-\nical information.\n2. Inspired by our ﬁndings from clustering, we\nintroduce a method for extracting meaning-\nful word embeddings from a character-level\nmodel, allowing us to investigate the word-\nlevel knowledge of the model.\nFirst, we use the HDBSCAN clustering algo-\nrithm (Campello et al., 2013) to reveal locations\nwithin a text at which the hidden state of the\nLSTM is similar, or at which a similar combina-\ntion of cell state dimensions is relevant (as deter-\nmined by output gates). Interestingly, focusing\non moments when the network must predict the\nﬁrst letter of a word reveals clusters that are in-\nterpretable on the level of words and which dis-\nplay both character-level patterns and grammati-\ncal structure (i.e. separating parts of speech). We\ngive examples of clusters of similar hidden states\nthat appear to be heavily inﬂuenced by local ortho-\ngraphic patterns but also distinguish between dif-\nferent grammatical functions of the pattern — for\nexample, a cluster containing whitespace charac-\nters following possessive uses, but not contractive\nuses, of the afﬁx “’s”. This sheds light on the use\nof orthographic patterns to infer higher-level infor-\nmation.\nWe also introduce a method for extracting word\n259\nembeddings from a character-level model and per-\nform qualitative and quantitative analyses of these\nembeddings. Surprisingly, this method can as-\nsign meaningful representations even to words that\nappear only once in the text, including associ-\nating the rare word “scrutinizingly” with “ques-\ntioningly” and “attentively”, and correctly iden-\ntifying “deck” as a verb based on a single use\ndespite its lack of meaningful subword compo-\nnents. These results suggests that the model is ca-\npable of deducing meaningful information about a\nword based on the context of a single use. While\nthese embeddings do not achieve state-of-the-art\nperformance on word similarity benchmarks, they\ndo outperform the older methods of Turian et al.\n(2010) despite the small corpus size and the fact\nthat our language model was not designed with the\nintent of producing word embeddings.\nThe rest of the paper is structured as follows:\nThe following section describes related work. Sec-\ntion 3 describes the architecture and training of the\nLSTM language model used in our experiments.\nIn Section 4, we describe our clustering methods\nand show examples of the clusters found, as well\nas a part of speech analysis. In Section 5, we de-\nscribe and analyze our method for extracting word\nembeddings from the character-level model. Fi-\nnally, we conclude and suggest directions for fu-\nture work.\n2 Related Work\n2.1 Analyzing Hidden State Dynamics\nMany researchers have investigated techniques for\nunderstanding the meaning and dynamics of the\nhidden states of recurrent neural networks. In\nhis seminal paper (Elman, 1990) introducing the\nsimple recurrent network (SRN) (or “Elman net-\nwork”), Elman uses hierarchical clustering to in-\nvestigate the hidden states of a word-level RNN\nmodeling a toy language of 29 words. Our ap-\nproach in Section 4 is in some ways similar, al-\nthough we use real English data and a character-\nlevel LSTM model. This also bears some similari-\nties to a visualization technique used by Krakovna\nand Doshi-Velez (2016) to investigate a hybrid\nHMM-LSTM model, although their work uses\nonly 10 k-means clusters and does not deeply in-\nvestigate clustering. Elman also uses principal\ncomponent analysis to visualize hidden state over\ntime (1991), and many researchers have used di-\nmensionality reduction methods such as t-SNE\n(Van der Maaten and Hinton, 2008) to visualize\nsimilarity between word embeddings, as well as\nother forms of distributed representation. More\nrecently, Li et al. (2016) directly visualize repre-\nsentations over time using heatmaps, and Strobelt\net al. (2018) develop interactive tools for visual-\nizing LSTM hidden states and testing hypotheses\nabout distributed representations.\nOther researchers have investigated methods for\nclarifying the function of speciﬁc hidden dimen-\nsions. Karpathy et al. (2015) use static visu-\nalizations to demonstrate the existence of cells\nin an LSTM language model with interpretable\nbehaviour representing long-term dependencies\n(such as cells tracking line length or quotations\nin a text). Another approach is that of K ´ad´ar\net al. (2017), who introduce a “Top K Contexts”\nmethod for interpreting the function of certain hid-\nden dimensions, identifying the K points in a se-\nquence which experience the highest activations\nfor the dimension in question.\n2.2 Character-Level Word Embeddings\nMultiple researchers have developed methods for\ncreating word embeddings that incorporate sub-\nword level (Luong et al., 2013) or character-level\n(Santos and Zadrozny, 2014; Ling et al., 2015) in-\nformation in order to better handle rare or out-of-\nvocabulary words. These approaches differ from\nour work in Section 5 in that they use architec-\ntures speciﬁcally designed to create word embed-\ndings, while we create embeddings from the hid-\nden state of a character-level model not designed\nfor this purpose. In addition, we are interested not\nin the embeddings themselves, but rather in what\nthey tell us about the word-level knowledge of the\nlanguage model.\nKim et al. (2016) investigate word embeddings\ncreated by a character-aware language model;\nhowever, the model uses word-level inputs that are\nfurther subdivided into character-level information\nand makes predictions on the word level, while we\nuse an entirely character-level model.\n3 Model\nIn this paper we focus on the task of language\nmodeling on the character level. Given an input se-\nquence of characters, the model is tasked with pre-\ndicting the log probability of the following charac-\nter.\nWe trained two models on different data sets us-\n260\ning the same architecture. Most of the paper fo-\ncuses on the War and Peacemodel, but Section\n5 uses embeddings derived from the Lancaster-\nOslo/Bergen Corpusmodel when measuring per-\nformance against word embedding benchmarks.\n3.1 Training Data\nOur ﬁrst model uses a relatively small data set,\nconsisting of the text of War and Peaceby Tol-\nstoy1. This data set was chosen due to its conve-\nnience as a sufﬁciently long but stylistically con-\nsistent example of English text. The text contains\n3,201,616 characters. We use the ﬁrst 95% of the\ndata for training and the last 5% for validation.\nOur second model uses a slightly larger data set,\nconsisting of the Lancaster-Oslo/Bergen (LOB)\ncorpus (Johansson et al., 1978) 2, which we re-\nmoved all markup from. This data set draws\nfrom a wide variety of ﬁction and non-ﬁction texts\nwritten in British English in 1961, and contains\n5,818,332 characters total. It was chosen for use\nin Section 5 because it covers a wide range of top-\nics (allowing us to extract word embeddings for a\nwider vocabulary) while still remaining at a man-\nageable size. We use the last 95% of the data for\ntraining and the ﬁrst 5% for validation.\n3.2 Model Architecture and Implementation\nWe use a simple LSTM architecture consisting\nof a 256-dimensional character embedding layer,\nfollowed by three 512-dimensional LSTM layers,\nand a ﬁnal layer producing a log softmax distribu-\ntion over the set of possible characters. The model\nwas implemented in PyTorch (Paszke et al., 2017)\nusing the default LSTM implementation3.\nThis architecture was chosen mostly arbitrarily,\nand distantly inspired by Karpathy et al. (2015).\n3.3 Training\nThe War and Peacemodel was trained for 170\nepochs using stochastic gradient descent and the\nnegative log likelihood loss function, with mini-\nbatches of size 100 and truncated backpropaga-\ntion through time (BPTT) of 100 time steps. Dur-\ning training, dropout was applied after each LSTM\nlayer with a dropout rate of 0.5. The learning rate\n1(Tolstoy, 2009), translated to English by Louise and\nAylmer Maude.\n2retrieved from http://purl.ox.ac.uk/ota/\n0167\n3We intend to release our code, including the trained mod-\nels.\nwas initially set to 1 and halved every time the\nloss on the validation data set plateaued. The ﬁnal\nmodel achieved 1.660 bits-per-character (BPC) on\nthe validation data.\nThe Lancaster-Oslo/Bergen model was trained\nfor 100 epochs using the PyTorch implementation\nof AdaGrad, with mini-batches of size 100, trun-\ncated BPTT of 100 time steps, a dropout rate of\n0.5, and an initial learning rate of 0.01.4 The ﬁnal\nmodel achieved 1.787 BPC on the validation data.\n4 Cluster Analysis of Character-Level\nand Word-Level Patterns\nIn this section we analyze points in the training\ntext by clustering according to hidden state val-\nues and output gate activations, revealing a combi-\nnation of grammatical and word-level patterns re-\nﬂected in the hidden state of our language model.\n4.1 Data For Clustering\nWe created two sets of data for use in clustering: a\n“full” data set and a “whitespace” data set.\nTo create the “full” data set, we ran ourWar and\nPeace language model on the ﬁrst 50,000 charac-\nters5 of the training data and recorded the hidden\nstate (i.e. the values often denotedht in the LSTM\nliterature, rather than the cell state ct) and the sig-\nmoid activations of the output gate of the third\nLSTM layer at each time step. We focus on the\nthird layer based on the expectation that it will en-\ncode more high-level information than earlier lay-\ners, an expectation which was supported by brief\nexperimentation on the ﬁrst layer.\nTo create the “whitespace” data set, we ran the\nWar and Peacemodel on the ﬁrst 250,000 charac-\nters of the training data and recorded data only for\ntimesteps when the input character was a space or\na new line character.\n4.2 Basic Clustering Experiment\nWe chose to use the HDBSCAN clustering algo-\nrithm (Campello et al., 2013), since it is designed\nto work with non-globular clusters of varying den-\nsity, does not require that an expected number of\nclusters be speciﬁed in advance, and is willing to\navoid assigning points to a cluster if they do not\n4Training parameters were not tuned to the data and differ\nmainly because the models were not trained at the same time,\nwith unrelated experiments intervening.\n5This smaller data set was used due to the relatively\nslow speed of the HDBSCAN implementation on high-\ndimensional data.\n261\nCluster Sample Cluster Members\n4\neven wi[s]h to; conversing wi[t]h;\ncase wi[t]h; whi[c]h was; him wi[t]h;\nhis wi[f]e; very wi[t]ty; acts whi[c]h;\n7\nso[m]ething like; she sa[w] that;\nHardenburg sa[y]s; the sa[m]e time;\nnone se[e]med to; words su[g]gested.\n14 e[x]plains; e[v]erything; e[x]posed;\ne[x]pectations; e[l]derly; e[x]pression;\n39 thi[s] reception; tha[t] profound; like thi[s]?”;\nthe[y] promised; the[y] have; The[r]e is;\n54\nwho[ ]is; He[ ]spoke; he[ ]indicated;\nwho[ ]had; He[ ]frowned; She[ ]was;\nwho[ ]was; she[ ]said; why[ ]he\n56\non[ ]the; for[ ]God’s; of[ ]the;\nof[ ]them; by[ ]imbecility; for[ ]Pierre;\nof[ ]young people; from[ ]abroad;\n62\nhad[ ]gone; had[ ]the; had[ ]been;\nhave[ ]reference; have[ ]promised;\nhad[ ]also; has[ ]been; has[ ]to;\n63\nher[ ]house; that[ ]is; his[ ]boats;\nthis[ ]pretty; that[ ]this; prevented her[ ]from;\nher[ ]age; his[ ]way; her[ ]duties;\nTable 1: Example members of clusters found using\nhidden state values based on the “full” data set. Cluster\nmembers (indicated by brackets) are accompanied by\ntext excerpts (separated by semicolons) to give context.\nseem to be a good ﬁt for any cluster. We used the\nPython implementation of McInnes et al. (2017).\nUsing the “full” data set, we attempted to\ncluster the time steps according to either hid-\nden state or output gate activations. We used\nthe Euclidean metric and the HDBSCAN param-\neters min cluster size=100 and min samples=10.\nThis was chosen somewhat arbitrarily and not on\nthe basis of a parameter search; we did brieﬂy\ntry other settings during preliminary research and\nfound that the results were similar 6. Clustering\nby hidden state values and clustering by output\ngate activations both produced a number of inter-\npretable clusters7.\nTable 1 shows a representative sample of the\nclusters found when using the hidden state for\nclustering8. We found that most clusters seemed to\nhave interpretable meanings on the character level,\noften including characters near the start of words\nthat begin with a particular character or characters,\nas in clusters 4, 7, and 14. In some cases, these\nclusters seem to locate orthographic patterns that\n6Of course, allowing smaller clusters results in more clus-\nters, while requiring larger clusters results in fewer, broader\nclusters, but there were no major qualitative differences in the\ntypes of clusters produced.\n7Clustering by hidden state produced 67 clusters, while\nclustering by output gate activations produced 87 clusters.\n8The output gate clusters were similar and are omitted to\nsave space.\nare useful in predicting the following character;\nfor example, the characters in cluster 4 are often\nfollowed by an “h”, and cluster 39 contains mostly\nletters at the end of a word (i.e. usually followed\nby whitespace). However, we did not ﬁnd clusters\nthat were characterised only by the following char-\nacters and not by patterns in the preceding charac-\nters.\nMore interestingly, clusters consisting of points\nimmediately preceding the start of a word tended\nto reﬂect word-level information relating to the\npreceding word. For example, cluster 54 con-\nsists of spaces immediately following the pro-\nnouns “he” and “she”, as well as the interroga-\ntive pronoun “who”9, while cluster 56 consists of\nspaces following certain prepositions. This was\nobserved in both the clusters based on hidden state\nand the clusters based on output gate activation.\nThis could be due to the fact that the output gate\nactivations, which also impact the hidden state,\ncan be intepreted as choosing which dimensions\nof the cell state are relevant for the network’s “de-\ncision” at a given time, and we would expect that\nword-level information is relevant when choosing\na distribution over the ﬁrst letter of the next word.\n4.3 Whitespace Clustering and\nPart-of-Speech Analysis\nSince the clusters including whitespace tended\nto reﬂect word-level grammatical information (as\nseen in clusters 54, 56, and 62 from Table 1), we\nperformed another round of clustering restricting\nour focus to only spaces and new lines. Cluster-\ning was performed on the “whitespace” data ac-\ncording to either hidden states or output gate acti-\nvations, again producing many interpretable clus-\nters10.\nFor the purposes of word-level analysis, each\ndata point (corresponding to a whitespace charac-\nter in the text) was equated with the word imme-\ndiately preceding it. The Stanford Part-of-Speech\nTagger (Toutanova et al., 2003) was used to tag\nthe text with part of speech (POS) information,\nand for each cluster the precision (percentage of\nwords in the cluster having a given tag) and re-\ncall (percentage of words with a given tag falling\n9 This cluster also occasionally includes spaces following\nthe word “why”, which may be due to orthographic similarity\nto “who”, or due to the fact that “why” is often followed by a\nverb, as in “Why is...”.\n10Clustering by hidden states produced 70 clusters, while\nclustering by output gate activations produced 77 clusters.\n262\nCluster Sample Members of Cluster POS - Precision POS - Recall\nHS-35 asked; replied; remarked; continued; replied; cried; cried;\nrepeated; continued; exclaimed; remarked; remarked; continued; declared; VBD: 100% VBD: 4.5%\nHS-40\nwill; will; don’t; don’t; don’t; cannot; just; can’t;\nwill; will; might; could; shall; will; would; just;\nwould; just; just; don’t; don’t; could;\nMD: 59.2%\nRB: 18.3%\nNN: 21.3% 11\nMD: 89.9%\nRB: 5.1%\nNN: 2.9%\nHS-57 looking; looked; looking; looked; looked; looking; walked; glanced;\nlooking; looking; glancing; looked; looked; looked; looking\nVBD: 60.0%\nVBG: 40.0%\nVBD: 2.4%\nVBG: 4.8%\nHS-59\ntrembled; jumped; tucked; smoothed; smiled; raised;\nstanding; smiled; pushed; smiled; passed; crowding;\nturning; raised; climbed; watched; turned; changed;\nVBD: 59.1%\nVBG: 31.5%\nVBN: 8.7%\nVBD: 6.0%\nVBG: 9.7%\nVBN: 3.3%\nHS-62\nunnatural; beautiful; beautiful; beautiful; terriﬁed;\nsuppressed; proud-looking; polished; well-garnished; nice-looking;\nswaggering; wonderful; embittered; alarmed; mournful\nJJ: 84.4%\nVBN: 6.7%\nVBG: 4.4%\nJJ: 4.1%\nVBN: 0.9%\nVBG: 0.5%\nOG-69\nlaughter; mother; father; daughter; father; father; matter;\ndaughter; father; manner; ofﬁcer; father; daughter;\nmother; laughter; daughter; daughter; father; ofﬁcer;\nNN: 94.1%\nNNS: 2.9%\nJJ: 2.9%\nNN: 1.9%\nNNS: 0.3%\nJJ: 0.1%\nOG-74 emancipation; nation; conversation; conversation; opinion\nconversation; resignation; conversation; conversation; expression;\nNN: 95.7%\nNNP: 4.4%\nNN: 3.8%\nNNP: 0.3%\nTable 2: Cluster members and POS statistics. Example cluster members (corresponding to whitespace characters)\nare drawn uniformly at random from the cluster and are represented by the preceding word. Note that some words\nappear multiple times since each appearance of the word in the text corresponds to a different data point. POS\ntags are those used by the Stanford POS tagger. Statistics are reported for the three parts of speech with highest\nprecision.\ninto the cluster)12 were calculated with respect to\neach tag. Since the clusters are based only on data\ncorresponding to whitespace, words not followed\nby whitespace (approximately 16% of all words)\nwere not counted when calculating recall.\nA selection of clusters, example members, and\nPOS statistics can be seen in Table 2. Clusters are\ndesignated “OG” or “HS” for “output gate” and\n“hidden state” respectively, so “HS-35” means the\n35th cluster produced when clustering by hidden\nstate values. These clusters were selected to illus-\ntrate the interesting patterns present, rather than to\nrepresent “typical” clusters.\nThe resulting clusters based on hidden states\nwere similar to those based on output gate acti-\nvations. Both approaches resulted in some clus-\nters based on a mix of orthographic and semantic\nsimilarity — for example, both produced a clus-\nter consisting primarily of three-letter verbs be-\nginning with “s” (particularly “sat”, “saw”, and\n“say”), as well as clusters consisting of possessive\nuses of the sufﬁx “’s”, but not uses of “’s” as a\ncontraction of “is” (as in “it’s”, “that’s”, etc.), de-\nspite the existence of several such uses in the text.\n11Manual inspection suggests that the claimed 22% preci-\nsion for nouns is actually due to the POS tagger mistaking\n“don’t”, “can’t” etc. for nouns, probably due to poor tok-\nenization, meaning that the true precision for modal verbs in\nthis cluster is 80% if we consider these to be modal verbs.\n12Note that when measured in this way, recall will usually\nbe quite low, since most clusters only contain some particular\nsubset of words with a given tag.\nIn fact, some early experimentation resulted in a\ndistinct cluster for the contractive use of “’s”, al-\nthough this does not occur with the parameters we\nchose for our canonical data. Additionally, in both\ncases the majority of clusters contained instances\nof only a single word or a small set of words —\nfor example, a cluster consisting entirely of the\nword “the”, a cluster consisting almost entirely of\nthe words “he” and “she”, and a cluster containing\nonly the words “me” and “my”. In total, 71% of\nclusters either contained only one or two words, or\nwere determined by preceding punctuation.\nHowever, there were qualitative differences be-\ntween the two approaches. Some of the hidden\nstate clusters appear to be based on semantic sim-\nilarities that go beyond mere grammatical similar-\nity; in particular, cluster HS-35 (as seen in Table 2)\ncontains words related to dialogue (and additional\ncontext reveals that members of this cluster always\nfollow the end of a quotation), while cluster HS-\n57 contains multiple words related to looking (in-\ncluding “gazed”, although it does not appear in the\ntable). Additionally, cluster HS-40 ﬁnds modal\nverbs with high precision and 89.9% recall, along\nwith the words “just” and “still”, which might be\nincluded due to orthographic similarity to “must”\nand “will”.\nIn contrast, clusters based on output gate acti-\nvations appear to be somewhat more closely re-\nlated to orthographic similarities. Several of these\nclusters display orthographic patterns that corre-\n263\nlate strongly with parts of speech; for example,\nclusters OG-69 and OG-74 contain “-ion” nouns\nand “-er” nouns (but not “-er” adjectives) respec-\ntively, and rather than including all modal verbs\nin a single cluster, the output gate clusters group\nthe words “would”, “could”, and “should” sepa-\nrately from “don’t”, “won’t”, and “can’t” (which\nare in turn separate from the cluster containing\n“will” and “still”). This suggests that character-\nlevel patterns correlated with grammatical infor-\nmation could strongly inﬂuence output gate activa-\ntions in a way that contributes to the grammatical\nunderstanding of the model13.\n5 Extracting Word Embeddings\nAs seen in Section 4.3, hidden states after whites-\npace characters encode word-level information.\nThis suggests a method for deriving word embed-\ndings from a character-level model, in order to bet-\nter investigate the model’s word-level knowledge.\nTo obtain word embeddings, we ran the War\nand Peace model on the entire text of War and\nPeace, storing hidden state values at each point in\nthe text. We then associated each word appear-\ning at least once in the text14 with the average hid-\nden state vector for whitespace characters follow-\ning the word in question. This produced a set of\n512-dimensional embeddings for a vocabulary of\n15,750 distinct words15.\nTable 3 shows the nearest neighbours 16 of the\nembeddings of several words, as well as a count of\nhow frequently the word appears in the text. While\nnot all nearest neighbours seem to be relevant (par-\nticularly for e.g. “write” and “food”), it nonethe-\nless appears that for words well-represented in\nthe text, these embeddings do reﬂect meaning\n(e.g. “loved” is similar to “liked”, “soldier” to “of-\nﬁcer”, and so on). In the case of words that are less\nwell represented (e.g. “write”, “food”), the nearest\nneighbours often seem to be retrieved based more\non orthographic similarities; however, “food” is\n13Though the ability of RNNs to learn and represent syntax\nhas been studied in RNNs with explicit access to grammati-\ncal structure (Kuncoro et al., 2017), to our knowledge, syn-\ntax representations have not been explored in character-level\nRNNs.\n14Excluding words that are never followed immediately by\na whitespace character (about 16% of all words).\n1517,510 words in total, but 1,760 are a combination of two\nwords joined by an em-dash. We ignore these “words” in our\nnearest neighbours analysis.\n16A tool from scikit-learn (Pedregosa et al., 2011) was used\nto ﬁnd nearest neighbours by cosine similarity. Using the\nEuclidean metric instead gives very similar results.\nWord (Occurrences) 5 Nearest Neighbours\nprince (1,926) princess, pwince, princes,\nplat´on, phillip\nwe (1069) I, tu, you, ve, he\nsoldier (201) ofﬁcer, footman, soldiers,\ntraveler, landowner\nloved (120) liked, longed, saved,\nlived, lose\nfrenchman (100)\nfrenchwoman, englishman,\nhuntsman, coachman,\nfrenchmen\nwrite (61) wring, wake, wipe, strive, live\nfood (41) foot, folk, fool, fear, form\ntu (4) we, I, thou, you, je\nuntruth (3) distrust, entreaty, rescript,\nrupture, ruse\ncannonading (2)\nundertaking, attacking,\noutﬂanking, maintaining\ntormenting\nscrutinizingly (1)\nquestioningly, challengingly,\nattentively, imploringly,\ndespairingly\nmoscovite (1) honneur, moravian, tshausen,\nchinese, grenadier\ncustodian (1) guardian, battalion, nightmare,\nrepublican, mathematician\nconduce (1) convince, conclude, conduced\ninduce, introduce\ndeck (1) delve, dwell, descry,\ndeny, decide\nTable 3: Sample vocabulary words and the number\nof times each appears in the text, compared with the\n5 nearest neighbours according to our extracted word\nembeddings.\nstill associated with nouns, and “write” with verbs,\nand more generally the embedding usually appears\nto at least reﬂect basic part of speech information.\nMore surprising, however, is the treatment of\nwords that appear only once in the text. In\nsome cases, the embeddings of these words do re-\nﬂect not only grammatical information but also\ntheir actual meaning; the word “moscovite”,\nfor example, is correctly associated with the\nwords “moravian” and “chinese” which also de-\nscribe geographic origin, and the word “scruti-\nnizingly” is associated with “questioningly” and\n“challengingly”. In these cases, since the word\n“moscovites” and various forms of “scrutinize”\ndo appear more frequently in the text, it is pos-\nsible that orthographic similarity and an under-\nstanding of morphemes such as “-s”, “-ing” and\n“-ly” contribute to these embeddings. This would\nbe consistent with the ﬁndings of e.g. Santos and\nZadrozny (2014) and others who have used the or-\nthographic information associated with words to\ndevelop word embeddings that perform well for\nrare words and even out-of-vocabulary words.\n264\nTask Pairs Found and Correlation\nOur Embeddings Metaoptimize Skip-Gram\nWS-353 290 0.1376 351 0.1013 353 0.6392\nWS-353-SIM 164 0.2265 201 0.1507 203 0.6962\nWS-353-REL 215 0.1384 252 0.0929 252 0.6094\nMC-30 25 0.1808 30 -0.1351 30 0.6258\nRG-65 48 0.2051 64 -0.0182 65 0.5386\nRare-Word 604 0.1500 1159 0.1085 1435 0.3878\nMEN 2317 0.1800 2915 0.0908 2999 0.6462\nMTurk-287 232 0.3681 284 0.0922 286 0.6698\nMTurk-771 689 0.0920 770 0.1016 771 0.5679\nYP-130 111 0.1311 124 0.0690 130 0.3992\nSimLex-999 948 0.0827 998 0.0095 998 0.3131\nVerb-144 144 0.3437 144 0.0553 144 0.2728\nSimVerb-3500 3052 0.0098 3447 0.0009 3492 0.2172\nTable 4: Performance of the word vectors derived from our Lancaster-Oslo/Bergen model on word similarity tasks,\ncompared with scores (taken from http://wordvectors.org (Faruqui and Dyer, 2014)) for the Metaopti-\nmize (Turian et al., 2010) and Skip-Gram (Mikolov et al., 2013) embeddings. For each set of embeddings and each\ntask we list the number of word pairs found and the measured correlation (Spearman’s rank correlation coefﬁcient).\nHowever, this does not explain the case of\n“deck”. When this word appears in the text, it is\nused in its sense as a verb. The only other ap-\npearance of the string “deck” in the text is the\nword “decks”, referring to the noun form of the\nword, and yet the embedding for “deck” is cor-\nrectly similar to other verbs. For this reason, and\nbecause the word “deck” is short and does not con-\nsist of meaningful sub-word entities, it is unlikely\nthat the verb-ness of “deck” was deduced from the\nword itself. This suggests that the model was able\nto determine the part of speech of the word from\nits use in a single context(e.g. the fact that it was\npreceded by “do not”). A similar mechanism may\nalso be responsible for the understanding of the\nFrench word “tu”, which is correctly identiﬁed as\na personal pronoun similar to both “you” (its trans-\nlation, appearing 3,509 times) and “je” (the French\n1st-person singular pronoun, appearing 16 times)\ndespite containing little orthographic information.\nIt should also be noted that while it is not the norm\nfor these embeddings of singleton words to reﬂect\nmeaning (as in the case of “scrutinizingly”), the\nmajority of embeddings do appear to at least iden-\ntify part of speech (as in the case of “deck”), sug-\ngesting a fairly robust mechanism for determining\nthis information from context.\nThe goal of this experiment was not to produce\nhigh-quality embeddings, but rather to understand\nthe word-level knowledge of a character-level lan-\nguage model. Nonetheless, we decided to evaluate\nword embeddings obtained in this manner against\nsome word similarity benchmarks. In order to ob-\ntain a broader vocabulary, we used word embed-\ndings derived from the model we trained on the\nLancaster-Oslo/Bergen corpus. While this train-\ning data is still quite small (less than 6 million\ncharacters), it covers a wider range of authors,\nstyles, and topics, including ﬁction, non-ﬁction,\nscientiﬁc papers and news articles, and thus is bet-\nter suited to producing general-purpose word em-\nbeddings. The embeddings we extracted from this\ncorpus cover a vocabulary of 38,981 words.\nWe assessed these embeddings us-\ning the 13 word similarity tasks of\nhttp://wordvectors.org (Faruqui and\nDyer, 2014), achieving the results shown in Table\n4. While these results are far from state-of-the-art,\nthey do outperform the representations of Turian\net al. (2010) on all tasks except for MTurk-771.\nFurthermore, our embeddings perform compa-\nrably on the “Rare Words” task compared to\nseveral other tasks, despite the small corpus size,\npresumably due to the use of orthographic and\ncontextual information by the language model.\n6 Discussion and Conclusion\nIn this paper, we used clustering to investigate the\ntype of information reﬂected in the hidden states\nand output gate activations of an LSTM language\nmodel. Focusing on whitespace characters re-\nvealed clusters containing words with meaningful\nsemantic similarities, as well as clusters reﬂecting\northographic patterns that correlate with grammat-\nical information.\nWe also described a method for extracting\nword embeddings from a character-level language\nmodel. Analysis suggests that the model is able to\n265\nlearn meaningful semantic information even about\nwords that appear only once in the training text,\nusing some combination of orthographic and con-\ntextual information.\nDirections for future work related to our cluster-\ning analysis could include applying similar tech-\nniques to other RNN architectures (e.g. the GRU\nof Cho et al. (2014)), comparing the effectiveness\nof different clustering algorithms for this type of\nanalysis, and scaling up the clustering experiments\nusing more computational resources, a more efﬁ-\ncient algorithm, and a larger corpus.\nAnother promising direction is to expand on the\nﬁndings of Section 5 by analyzing the quality of\nword embeddings produced from character-level\nmodels trained on a larger corpus, and investi-\ngating the capability of character level models to\nproduce word embeddings for out-of-vocabulary\nwords when given a small amount of context.\nCollectively, our ﬁndings regarding clustering\nanalysis and extraction of word embeddings offer\ninteresting insight into the behaviour of character-\nlevel recurrent language models, and we hope that\nthey will prove a useful contribution in the ongo-\ning effort to increase the interpretability of recur-\nrent neural networks.\nAcknowledgments\nThis research was supported by NSERC (Natural\nSciences and Engineering Research Council),\nincluding an Undergraduate Student Research\nAward for Avery Hiebert, and by CIFAR (Cana-\ndian Institute for Advanced Research).\nReferences\nRicardo J. G. B. Campello, Davoud Moulavi, and Joerg\nSander. 2013. Density-based clustering based on hi-\nerarchical density estimates. In Advances in Knowl-\nedge Discovery and Data Mining, pages 160–172,\nBerlin, Heidelberg. Springer Berlin Heidelberg.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using rnn encoder-decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP 2014), Doha,\nQatar.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nJeffrey L Elman. 1991. Distributed representations,\nsimple recurrent networks, and grammatical struc-\nture. Machine learning, 7(2-3):195–225.\nManaal Faruqui and Chris Dyer. 2014. Community\nevaluation and exchange of word vectors at word-\nvectors.org. In Proceedings of the 52nd Annual\nMeeting of the Association for Computational Lin-\nguistics: System Demonstrations, Baltimore, USA.\nAssociation for Computational Linguistics.\nFelix A Gers, J¨urgen Schmidhuber, and Fred Cummins.\n2000. Learning to forget: Continual prediction with\nLSTM. Neural Computation, 12(10):2451–2471.\nAlex Graves. 2013. Generating sequences with recur-\nrent neural networks. Computing Research Reposi-\ntory, arXiv:1308.0850. Version 5.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nStig Johansson, Geoffrey N Leech, and Helen Good-\nluck. 1978. Manual of information to accompany\nthe Lancaster-Oslo/Bergen Corpus of British En-\nglish, for use with digital computer. Department of\nEnglish, University of Oslo.\nAkos K ´ad´ar, Grzegorz Chrupała, and Afra Alishahi.\n2017. Representation of linguistic form and func-\ntion in recurrent neural networks. Computational\nLinguistics, 43(4):761–780.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\nIn Workshop track - ICLR 2016.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In AAAI, pages 2741–2749.\nViktoriya Krakovna and Finale Doshi-Velez. 2016. In-\ncreasing the interpretability of recurrent neural net-\nworks using hidden Markov models. In Proceedings\nof NIPS 2016 Workshop on Interpretable Machine\nLearning for Complex Systems, Barcelona, Spain.\nAdhiguna Kuncoro, Miguel Ballesteros, Lingpeng\nKong, Chris Dyer, Graham Neubig, and Noah A.\nSmith. 2017. What do recurrent neural network\ngrammars learn about syntax? In Proceedings of\nthe 15th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Volume\n1, Long Papers, pages 1249–1258. Association for\nComputational Linguistics.\nJiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.\n2016. Visualizing and understanding neural models\nin NLP. In Proceedings of the 2016 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 681–691, San Diego, California. As-\nsociation for Computational Linguistics.\n266\nWang Ling, Tiago Lu´ıs, Lu´ıs Marujo, R´amon Fernan-\ndez Astudillo, Silvio Amir, Chris Dyer, Alan W\nBlack, and Isabel Trancoso. 2015. Finding function\nin form: Compositional character models for open\nvocabulary word representation. EMNLP.\nThang Luong, Richard Socher, and Christopher Man-\nning. 2013. Better word representations with recur-\nsive neural networks for morphology. In Proceed-\nings of the Seventeenth Conference on Computa-\ntional Natural Language Learning, pages 104–113.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-SNE. Journal of Machine\nLearning Research, 9(Nov):2579–2605.\nLeland McInnes, John Healy, and Steve Astels. 2017.\nhdbscan: Hierarchical density based clustering. The\nJournal of Open Source Software, 2(11):205.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In Workshop Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR) 2013.\nTom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSon Le, Stefan Kombrink, and Jan ˘Cernock´y.\n2012. Subword language modeling with neural\nnetworks. preprint http://www.fit.vutbr.\ncz/˜imikolov/rnnlm/char.pdf, 8.\nAdam Paszke, Sam Gross, Soumith Chintala, Gre-\ngory Chanan, Edward Yang, Zachary DeVito, Zem-\ning Lin, Alban Desmaison, Luca Antiga, and Adam\nLerer. 2017. Automatic differentiation in PyTorch.\nIn NIPS 2017 Autodiff Workshop, Long Beach, Cal-\nifornia, USA.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Pas-\nsos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. 2011. Scikit-learn: Machine learning\nin Python. Journal of Machine Learning Research,\n12:2825–2830.\nC´ıcero D Santos and Bianca Zadrozny. 2014. Learning\ncharacter-level representations for part-of-speech\ntagging. In Proceedings of the 31st International\nConference on Machine Learning (ICML-14), pages\n1818–1826.\nH. Strobelt, S. Gehrmann, H. Pﬁster, and A. M. Rush.\n2018. LSTMVis: A tool for visual analysis of hid-\nden state dynamics in recurrent neural networks.\nIEEE Transactions on Visualization and Computer\nGraphics, 24(1):667–676.\nLeo Tolstoy. 2009. War and Peace. Project Guten-\nberg, Urbana, IL. Translated by Louise and Aylmer\nMaude.\nKristina Toutanova, Dan Klein, Christopher D Man-\nning, and Yoram Singer. 2003. Feature-rich part-of-\nspeech tagging with a cyclic dependency network.\nIn Proceedings of the 2003 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics on Human Language Technology-\nVolume 1, pages 173–180. Association for Compu-\ntational Linguistics.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: a simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th annual meeting of the association for compu-\ntational linguistics, pages 384–394. Association for\nComputational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8590866327285767
    },
    {
      "name": "Word (group theory)",
      "score": 0.7609341144561768
    },
    {
      "name": "Character (mathematics)",
      "score": 0.742077648639679
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7024810314178467
    },
    {
      "name": "Natural language processing",
      "score": 0.6560355424880981
    },
    {
      "name": "Language model",
      "score": 0.585634708404541
    },
    {
      "name": "Cluster analysis",
      "score": 0.5347480177879333
    },
    {
      "name": "Representation (politics)",
      "score": 0.5048035383224487
    },
    {
      "name": "ENCODE",
      "score": 0.45120465755462646
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4496215581893921
    },
    {
      "name": "State (computer science)",
      "score": 0.43913891911506653
    },
    {
      "name": "Artificial neural network",
      "score": 0.42730221152305603
    },
    {
      "name": "Speech recognition",
      "score": 0.3724285364151001
    },
    {
      "name": "Linguistics",
      "score": 0.11447149515151978
    },
    {
      "name": "Algorithm",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I212119943",
      "name": "University of Victoria",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I154425047",
      "name": "University of Alberta",
      "country": "CA"
    }
  ]
}