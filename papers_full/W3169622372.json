{
  "title": "Do Transformers Really Perform Bad for Graph Representation?",
  "url": "https://openalex.org/W3169622372",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Ying, Chengxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286240181",
      "name": "Cai, Tianle",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2347333117",
      "name": "Luo, Shengjie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2024089860",
      "name": "Zheng ShuXin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2227149612",
      "name": "Ke, Guolin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098476997",
      "name": "He Di",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2269889618",
      "name": "Shen Yan-ming",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209371238",
      "name": "Liu, Tie-Yan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3007332492",
    "https://openalex.org/W3021975806",
    "https://openalex.org/W2947722296",
    "https://openalex.org/W3084428871",
    "https://openalex.org/W3025165719",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W2110505218",
    "https://openalex.org/W3194729832",
    "https://openalex.org/W3142310598",
    "https://openalex.org/W3136399186",
    "https://openalex.org/W2754490690",
    "https://openalex.org/W2950697450",
    "https://openalex.org/W2962767366",
    "https://openalex.org/W3131343002",
    "https://openalex.org/W2965096954",
    "https://openalex.org/W3212115192",
    "https://openalex.org/W2768242641",
    "https://openalex.org/W3113177135",
    "https://openalex.org/W2970493342",
    "https://openalex.org/W3169575312",
    "https://openalex.org/W3122934853",
    "https://openalex.org/W3035649237",
    "https://openalex.org/W2951142218",
    "https://openalex.org/W2624431344",
    "https://openalex.org/W3093694263",
    "https://openalex.org/W1995683849",
    "https://openalex.org/W2606780347",
    "https://openalex.org/W3000577518",
    "https://openalex.org/W3089895233",
    "https://openalex.org/W3111485660",
    "https://openalex.org/W2613900957",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2998702685",
    "https://openalex.org/W2097308346",
    "https://openalex.org/W3132845217",
    "https://openalex.org/W3110111880",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2996086147",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W634562057",
    "https://openalex.org/W3172431826",
    "https://openalex.org/W3102154670",
    "https://openalex.org/W2962711740",
    "https://openalex.org/W2946721323",
    "https://openalex.org/W3133264589",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2996604169",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3104591796",
    "https://openalex.org/W3007488165",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3155952169",
    "https://openalex.org/W2997347790",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3173130715",
    "https://openalex.org/W2945811181",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "The Transformer architecture has become a dominant choice in many domains, such as natural language processing and computer vision. Yet, it has not achieved competitive performance on popular leaderboards of graph-level prediction compared to mainstream GNN variants. Therefore, it remains a mystery how Transformers could perform well for graph representation learning. In this paper, we solve this mystery by presenting Graphormer, which is built upon the standard Transformer architecture, and could attain excellent results on a broad range of graph representation learning tasks, especially on the recent OGB Large-Scale Challenge. Our key insight to utilizing Transformer in the graph is the necessity of effectively encoding the structural information of a graph into the model. To this end, we propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data. Besides, we mathematically characterize the expressive power of Graphormer and exhibit that with our ways of encoding the structural information of graphs, many popular GNN variants could be covered as the special cases of Graphormer.",
  "full_text": "Do Transformers Really Perform Bad\nfor Graph Representation?\nChengxuan Ying1∗, Tianle Cai2, Shengjie Luo3∗,\nShuxin Zheng4†, Guolin Ke4, Di He4†, Yanming Shen1, Tie-Yan Liu4\n1Dalian University of Technology 2Princeton University\n3Peking University 4Microsoft Research Asia\nyingchengsyuan@gmail.com, tianle.cai@princeton.edu, luosj@stu.pku.edu.cn\n{shuz†, guoke, dihe†, tyliu}@microsoft.com, shen@dlut.edu.cn\nAbstract\nThe Transformer architecture has become a dominant choice in many domains, such\nas natural language processing and computer vision. Yet, it has not achieved com-\npetitive performance on popular leaderboards of graph-level prediction compared\nto mainstream GNN variants. Therefore, it remains a mystery how Transformers\ncould perform well for graph representation learning. In this paper, we solve this\nmystery by presenting Graphormer, which is built upon the standard Transformer\narchitecture, and could attain excellent results on a broad range of graph representa-\ntion learning tasks, especially on the recent OGB Large-Scale Challenge. Our key\ninsight to utilizing Transformer in the graph is the necessity of effectively encoding\nthe structural information of a graph into the model. To this end, we propose several\nsimple yet effective structural encoding methods to help Graphormer better model\ngraph-structured data. Besides, we mathematically characterize the expressive\npower of Graphormer and exhibit that with our ways of encoding the structural\ninformation of graphs, many popular GNN variants could be covered as the special\ncases of Graphormer. The code and models of Graphormer will be made publicly\navailable at https://github.com/Microsoft/Graphormer.\n1 Introduction\nThe Transformer [ 49] is well acknowledged as the most powerful neural network in modelling\nsequential data, such as natural language [ 11, 35, 6] and speech [ 17]. Model variants built upon\nTransformer have also been shown great performance in computer vision [12, 36] and programming\nlanguage [19, 63, 44]. However, to the best of our knowledge, Transformer has still not been\nthe de-facto standard on public graph representation leaderboards [ 22, 14, 21]. There are many\nattempts of leveraging Transformer into the graph domain, but the only effective way is replacing\nsome key modules (e.g., feature aggregation) in classic GNN variants by the softmax attention\n[50, 7, 23, 51, 61, 46, 13]. Therefore, it is still an open question whether Transformer architecture is\nsuitable to model graphs and how to make it work in graph representation learning.\nIn this paper, we give an afﬁrmative answer by developing Graphormer, which is directly built\nupon the standard Transformer, and achieves state-of-the-art performance on a wide range of graph-\nlevel prediction tasks, including the very recent Open Graph Benchmark Large-Scale Challenge\n(OGB-LSC) [21], and several popular leaderboards (e.g., OGB [22], Benchmarking-GNN [14]). The\nTransformer is originally designed for sequence modeling. To utilize its power in graphs, we believe\n∗Interns at MSRA.\n†Corresponding authors.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.05234v5  [cs.LG]  24 Nov 2021\nthe key is to properly incorporate structural information of graphs into the model. Note that for each\nnode i, the self-attention only calculates the semantic similarity between iand other nodes, without\nconsidering the structural information of a graph reﬂected on the nodes and the relation between\nnode pairs. Graphormer incorporates several effective structural encoding methods to leverage such\ninformation, which are described below.\nFirst, we propose a Centrality Encoding in Graphormer to capture the node importance in the graph.\nIn a graph, different nodes may have different importance, e.g., celebrities are considered to be\nmore inﬂuential than the majority of web users in a social network. However, such information\nisn’t reﬂected in the self-attention module as it calculates the similarities mainly using the node\nsemantic features. To address the problem, we propose to encode the node centrality in Graphormer.\nIn particular, we leverage the degree centrality for the centrality encoding, where a learnable vector\nis assigned to each node according to its degree and added to the node features in the input layer.\nEmpirical studies show that simple centrality encoding is effective for Transformer in modeling the\ngraph data.\nSecond, we propose a novel Spatial Encoding in Graphormer to capture the structural relation\nbetween nodes. One notable geometrical property that distinguishes graph-structured data from other\nstructured data, e.g., language, images, is that there does not exist a canonical grid to embed the graph.\nIn fact, nodes can only lie in a non-Euclidean space and are linked by edges. To model such structural\ninformation, for each node pair, we assign a learnable embedding based on their spatial relation.\nMultiple measurements in the literature could be leveraged for modeling spatial relations. For a\ngeneral purpose, we use the distance of the shortest path between any two nodes as a demonstration,\nwhich will be encoded as a bias term in the softmax attention and help the model accurately capture\nthe spatial dependency in a graph. In addition, sometimes there is additional spatial information\ncontained in edge features, such as the type of bond between two atoms in a molecular graph. We\ndesign a new edge encoding method to further take such signal into the Transformer layers. To\nbe concrete, for each node pair, we compute an average of dot-products of the edge features and\nlearnable embeddings along the shortest path, then use it in the attention module. Equipped with\nthese encodings, Graphormer could better model the relationship for node pairs and represent the\ngraph.\nBy using the proposed encodings above, we further mathematically show that Graphormer has strong\nexpressiveness as many popular GNN variants are just its special cases. The great capacity of the\nmodel leads to state-of-the-art performance on a wide range of tasks in practice. On the large-scale\nquantum chemistry regression dataset 3 in the very recent Open Graph Benchmark Large-Scale\nChallenge (OGB-LSC) [21], Graphormer outperforms most mainstream GNN variants by more than\n10% points in terms of the relative error. On other popular leaderboards of graph representation\nlearning (e.g., MolHIV , MolPCBA, ZINC) [22, 14], Graphormer also surpasses the previous best\nresults, demonstrating the potential and adaptability of the Transformer architecture.\n2 Preliminary\nIn this section, we recap the preliminaries in Graph Neural Networks and Transformer.\nGraph Neural Network (GNN). Let G = (V,E) denote a graph where V = {v1,v2,··· ,vn},\nn = |V|is the number of nodes. Let the feature vector of node vi be xi. GNNs aim to learn\nrepresentation of nodes and graphs. Typically, modern GNNs follow a learning schema that iteratively\nupdates the representation of a node by aggregating representations of its ﬁrst or higher-order\nneighbors. We denote h(l)\ni as the representation of vi at the l-th layer and deﬁne h(0)\ni = xi. The l-th\niteration of aggregation could be characterized by AGGREGATE-COMBINE step as\na(l)\ni = AGGREGATE(l)\n({\nh(l−1)\nj : j ∈N(vi)\n})\n, h (l)\ni = COMBINE (l)\n(\nh(l−1)\ni ,a(l)\ni\n)\n, (1)\nwhere N(vi) is the set of ﬁrst or higher-order neighbors of vi. The AGGREGATE function is used to\ngather the information from neighbors. Common aggregation functions include MEAN, MAX, SUM,\nwhich are used in different architectures of GNNs [26, 18, 50, 54]. The goal of COMBINE function\nis to fuse the information from neighbors into the node representation.\n3https://ogb.stanford.edu/kddcup2021/pcqm4m/\n2\nCentrality EncodingNode Feature\nv5\nv5\nv4\nv4\nv1\nv2\nv3\nv1 v3v2\nSpatial Encoding\nQ K V\nMatMul \nScale \nSoftMax \nMatMul \nv1\nv2\nv3\nv4\nv5\nLinear Linear Linear \nv5\nv5\nv4\nv4\nv1\nv2\nv3\nv1 v3v2\nEdge Encoding\nFigure 1: An illustration of our proposed centrality encoding, spatial encoding, and edge encoding in\nGraphormer.\nIn addition, for graph representation tasks, a READOUT function is designed to aggregate node\nfeatures h(L)\ni of the ﬁnal iteration into the representation hG of the entire graph G:\nhG = READOUT\n({\nh(L)\ni |vi ∈G\n})\n. (2)\nREADOUT can be implemented by a simple permutation invariant function such as summation [54]\nor a more sophisticated graph-level pooling function [1].\nTransformer. The Transformer architecture consists of a composition of Transformer layers [49].\nEach Transformer layer has two parts: a self-attention module and a position-wise feed-forward\nnetwork (FFN). Let H =\n[\nh⊤\n1 ,··· ,h⊤\nn\n]⊤\n∈Rn×d denote the input of self-attention module where d\nis the hidden dimension and hi ∈R1×d is the hidden representation at position i. The input H is\nprojected by three matrices WQ ∈Rd×dK ,WK ∈Rd×dK and WV ∈Rd×dV to the corresponding\nrepresentations Q,K,V . The self-attention is then calculated as:\nQ= HWQ, K = HWK, V = HWV, (3)\nA= QK⊤\n√dK\n, Attn (H) = softmax (A) V, (4)\nwhere Ais a matrix capturing the similarity between queries and keys. For simplicity of illustration,\nwe consider the single-head self-attention and assumedK = dV = d. The extension to the multi-head\nattention is standard and straightforward, and we omit bias terms for simplicity.\n3 Graphormer\nIn this section, we present our Graphormer for graph tasks. First, we elaborate on several key\ndesigns in the Graphormer, which serve as an inductive bias in the neural network to learn the graph\nrepresentation. We further provide the detailed implementations of Graphormer. Finally, we show\nthat our proposed Graphormer is more powerful since popular GNN models [ 26, 54, 18] are its\nspecial cases.\n3\n3.1 Structural Encodings in Graphormer\nAs discussed in the introduction, it is important to develop ways to leverage the structural information\nof graphs into the Transformer model. To this end, we present three simple but effective designs of\nencoding in Graphormer. See Figure 1 for an illustration.\n3.1.1 Centrality Encoding\nIn Eq.4, the attention distribution is calculated based on the semantic correlation between nodes.\nHowever, node centrality, which measures how important a node is in the graph, is usually a strong\nsignal for graph understanding. For example, celebrities who have a huge number of followers are\nimportant factors in predicting the trend of a social network [40, 39]. Such information is neglected\nin the current attention calculation, and we believe it should be a valuable signal for Transformer\nmodels.\nIn Graphormer, we use the degree centrality, which is one of the standard centrality measures in\nliterature, as an additional signal to the neural network. To be speciﬁc, we develop a Centrality\nEncoding which assigns each node two real-valued embedding vectors according to its indegree and\noutdegree. As the centrality encoding is applied to each node, we simply add it to the node features\nas the input.\nh(0)\ni = xi + z−\ndeg−(vi) + z+\ndeg+(vi), (5)\nwhere z−,z+ ∈Rd are learnable embedding vectors speciﬁed by the indegree deg−(vi) and out-\ndegree deg+(vi) respectively. For undirected graphs, deg−(vi) and deg+(vi) could be uniﬁed to\ndeg(vi). By using the centrality encoding in the input, the softmax attention can catch the node\nimportance signal in the queries and the keys. Therefore the model can capture both the semantic\ncorrelation and the node importance in the attention mechanism.\n3.1.2 Spatial Encoding\nAn advantage of Transformer is its global receptive ﬁeld. In each Transformer layer, each token can\nattend to the information at any position and then process its representation. But this operation has a\nbyproduct problem that the model has to explicitly specify different positions or encode the positional\ndependency (such as locality) in the layers. For sequential data, one can either give each position an\nembedding (i.e., absolute positional encoding [49]) as the input or encode the relative distance of any\ntwo positions (i.e., relative positional encoding [45, 47]) in the Transformer layer.\nHowever, for graphs, nodes are not arranged as a sequence. They can lie in a multi-dimensional\nspatial space and are linked by edges. To encode the structural information of a graph in the\nmodel, we propose a novel Spatial Encoding. Concretely, for any graph G, we consider a function\nφ(vi,vj) : V ×V →R which measures the spatial relation between vi and vj in graph G. The\nfunction φcan be deﬁned by the connectivity between the nodes in the graph. In this paper, we\nchoose φ(vi,vj) to be the distance of the shortest path (SPD) between vi and vj if the two nodes\nare connected. If not, we set the output of φto be a special value, i.e., -1. We assign each (feasible)\noutput value a learnable scalar which will serve as a bias term in the self-attention module. Denote\nAij as the (i,j)-element of the Query-Key product matrix A, we have:\nAij = (hiWQ)(hjWK)T\n√\nd\n+ bφ(vi,vj), (6)\nwhere bφ(vi,vj) is a learnable scalar indexed by φ(vi,vj), and shared across all layers.\nHere we discuss several beneﬁts of our proposed method. First, compared to conventional GNNs\ndescribed in Section 2, where the receptive ﬁeld is restricted to the neighbors, we can see that in\nEq. (6), the Transformer layer provides a global information that each node can attend to all other\nnodes in the graph. Second, by using bφ(vi,vj), each node in a single Transformer layer can adaptively\nattend to all other nodes according to the graph structural information. For example, if bφ(vi,vj) is\nlearned to be a decreasing function with respect to φ(vi,vj), for each node, the model will likely pay\nmore attention to the nodes near it and pay less attention to the nodes far away from it.\n4\n3.1.3 Edge Encoding in the Attention\nIn many graph tasks, edges also have structural features, e.g., in a molecular graph, atom pairs may\nhave features describing the type of bond between them. Such features are important to the graph\nrepresentation, and encoding them together with node features into the network is essential. There are\nmainly two edge encoding methods used in previous works. In the ﬁrst method, the edge features are\nadded to the associated nodes’ features [22, 30]. In the second method, for each node, its associated\nedges’ features will be used together with the node features in the aggregation [15, 54, 26]. However,\nsuch ways of using edge feature only propagate the edge information to its associated nodes, which\nmay not be an effective way to leverage edge information in representation of the whole graph.\nTo better encode edge features into attention layers, we propose a new edge encoding method in\nGraphormer. The attention mechanism needs to estimate correlations for each node pair (vi,vj), and\nwe believe the edges connecting them should be considered in the correlation as in [34, 51]. For each\nordered node pair (vi,vj), we ﬁnd (one of) the shortest path SPij = (e1,e2,...,e N) from vi to vj,\nand compute an average of the dot-products of the edge feature and a learnable embedding along the\npath. The proposed edge encoding incorporates edge features via a bias term to the attention module.\nConcretely, we modify the (i,j)-element of Ain Eq. (3) further with the edge encoding cij as:\nAij = (hiWQ)(hjWK)T\n√\nd\n+ bφ(vi,vj) + cij, where cij = 1\nN\nN∑\nn=1\nxen(wE\nn)T, (7)\nwhere xen is the feature of the n-th edge en in SPij, wE\nn ∈RdE is the n-th weight embedding, and\ndE is the dimensionality of edge feature.\n3.2 Implementation Details of Graphormer\nGraphormer Layer. Graphormer is built upon the original implementation of classic Transformer\nencoder described in [ 49]. In addition, we apply the layer normalization (LN) before the multi-head\nself-attention (MHA) and the feed-forward blocks (FFN) instead of after [ 53]. This modiﬁcation\nhas been unanimously adopted by all current Transformer implementations because it leads to more\neffective optimization [43]. Especially, for FFN sub-layer, we set the dimensionality of input, output,\nand the inner-layer to the same dimension with d. We formally characterize the Graphormer layer as\nbelow:\nh\n′(l) = MHA(LN(h(l−1))) +h(l−1) (8)\nh(l) = FFN(LN(h\n′(l))) +h\n′(l) (9)\nSpecial Node. As stated in the previous section, various graph pooling functions are proposed to\nrepresent the graph embedding. Inspired by [ 15], in Graphormer, we add a special node called\n[VNode] to the graph, and make connection between [VNode] and each node individually. In the\nAGGREGATE-COMBINE step, the representation of[VNode] has been updated as normal nodes in\ngraph, and the representation of the entire graph hG would be the node feature of [VNode] in the\nﬁnal layer. In the BERT model [11, 35], there is a similar token, i.e., [CLS], which is a special token\nattached at the beginning of each sequence, to represent the sequence-level feature on downstream\ntasks. While the [VNode] is connected to all other nodes in graph, which means the distance of\nthe shortest path is 1 for any φ([VNode],vj) and φ(vi,[VNode]), the connection is not physical. To\ndistinguish the connection of physical and virtual, inspired by [ 25], we reset all spatial encodings for\nbφ([VNode],vj) and bφ(vi,[VNode]) to a distinct learnable scalar.\n3.3 How Powerful is Graphormer?\nIn the previous subsections, we introduce three structural encodings and the architecture of\nGraphormer. Then a natural question is: Do these modiﬁcations make Graphormer more pow-\nerful than other GNN variants? In this subsection, we ﬁrst give an afﬁrmative answer by showing\nthat Graphormer can represent the AGGREGATE and COMBINE steps in popular GNN models:\n5\nFact 1. By choosing proper weights and distance function φ, the Graphormer layer can represent\nAGGREGATE and COMBINE steps of popular GNN models such as GIN, GCN, GraphSAGE.\nThe proof sketch to derive this result is: 1) Spatial encoding enables self-attention module to\ndistinguish neighbor set N(vi) of node vi so that the softmax function can calculate mean statistics\nover N(vi); 2) Knowing the degree of a node, mean over neighbors can be translated to sum over\nneighbors; 3) With multiple heads and FFN, representations of vi and N(vi) can be processed\nseparately and combined together later. We defer the proof of this fact to Appendix A.\nMoreover, we show further that by using our spatial encoding, Graphormer can go beyond classic\nmessage passing GNNs whose expressive power is no more than the 1-Weisfeiler-Lehman (WL) test.\nWe give a concrete example in Appendix A to show how Graphormer helps distinguish graphs that\nthe 1-WL test fails to.\nConnection between Self-attention and Virtual Node.Besides the superior expressiveness than\npopular GNNs, we also ﬁnd an interesting connection between using self-attention and the virtual\nnode heuristic [15, 31, 24, 22]. As shown in the leaderboard of OGB [ 22], the virtual node trick,\nwhich augments graphs with additional supernodes that are connected to all nodes in the original\ngraphs, can signiﬁcantly improve the performance of existing GNNs. Conceptually, the beneﬁt of\nthe virtual node is that it can aggregate the information of the whole graph (like the READOUT\nfunction) and then propagate it to each node. However, a naive addition of a supernode to a graph\ncan potentially lead to inadvertent over-smoothing of information propagation [24]. We instead ﬁnd\nthat such a graph-level aggregation and propagation operation can be naturally fulﬁlled by vanilla\nself-attention without additional encodings. Concretely, we can prove the following fact:\nFact 2. By choosing proper weights, every node representation of the output of a Graphormer layer\nwithout additional encodings can represent MEAN READOUT functions.\nThis fact takes the advantage of self-attention that each node can attend to all other nodes. Thus it can\nsimulate graph-level READOUT operation to aggregate information from the whole graph. Besides\nthe theoretical justiﬁcation, we empirically ﬁnd that Graphormer does not encounter the problem of\nover-smoothing, which makes the improvement scalable. The fact also inspires us to introduce a\nspecial node for graph readout (see the previous subsection).\n4 Experiments\nWe ﬁrst conduct experiments on the recent OGB-LSC [ 21] quantum chemistry regression (i.e.,\nPCQM4M-LSC) challenge, which is currently the biggest graph-level prediction dataset and contains\nmore than 3.8M graphs in total. Then, we report the results on the other three popular tasks: ogbg-\nmolhiv, ogbg-molpcba and ZINC, which come from the OGB [ 22] and benchmarking-GNN [14]\nleaderboards. Finally, we ablate the important design elements of Graphormer. A detailed description\nof datasets and training strategies could be found in Appendix B.\n4.1 OGB Large-Scale Challenge\nBaselines. We benchmark the proposed Graphormer with GCN [ 26] and GIN [ 54], and their\nvariants with virtual node (-VN) [15]. They achieve the state-of-the-art valid and test mean absolute\nerror (MAE) on the ofﬁcial leaderboard4 [21]. In addition, we compare to GIN’s multi-hop variant [5],\nand 12-layer deep graph network DeeperGCN [30], which also show promising performance on other\nleaderboards. We further compare our Graphormer with the recent Transformer-based graph model\nGT [13].\nSettings. We primarily report results on two model sizes: Graphormer (L= 12,d = 768), and\na smaller one GraphormerSMALL (L = 6,d = 512). Both the number of attention heads in the\nattention module and the dimensionality of edge features dE are set to 32. We use AdamW as the\noptimizer, and set the hyper-parameter ϵto 1e-8 and (β1,β2) to (0.99,0.999). The peak learning rate\nis set to 2e-4 (3e-4 for GraphormerSMALL ) with a 60k-step warm-up stage followed by a linear decay\nlearning rate scheduler. The total training steps are 1M. The batch size is set to 1024. All models are\ntrained on 8 NVIDIA V100 GPUS for about 2 days.\n4https://github.com/snap-stanford/ogb/tree/master/examples/lsc/pcqm4m#performance\n6\nTable 1: Results on PCQM4M-LSC. * indicates the results are cited from the ofﬁcial leaderboard [21].\nmethod #param. train MAE validate MAE\nGCN [26] 2.0M 0.1318 0.1691 (0.1684*)\nGIN [54] 3.8M 0.1203 0.1537 (0.1536*)\nGCN-VN [26, 15] 4.9M 0.1225 0.1485 (0.1510*)\nGIN-VN [54, 15] 6.7M 0.1150 0.1395 (0.1396*)\nGINE-VN [5, 15] 13.2M 0.1248 0.1430\nDeeperGCN-VN [30, 15] 25.5M 0.1059 0.1398\nGT [13] 0.6M 0.0944 0.1400\nGT-Wide [13] 83.2M 0.0955 0.1408\nGraphormerSMALL 12.5M 0.0778 0.1264\nGraphormer 47.1M 0.0582 0.1234\nResults. Table 1 summarizes performance comparisons on PCQM4M-LSC dataset. From the table,\nGIN-VN achieves the previous state-of-the-art validate MAE of 0.1395. The original implementation\nof GT [13] employs a hidden dimension of 64 to reduce the total number of parameters. For a fair\ncomparison, we also report the result by enlarging the hidden dimension to 768, denoted by GT-Wide,\nwhich leads to a total number of parameters of 83.2M. While, both GT and GT-Wide do not outperform\nGIN-VN and DeeperGCN-VN. Especially, we do not observe a performance gain along with the growth\nof parameters of GT.\nCompared to the previous state-of-the-art GNN architecture, Graphormer noticeably surpasses GIN-\nVN by a large margin, e.g., 11.5% relative validate MAE decline. By using the ensemble with\nExpC [55], we got a 0.1200 MAE on complete test set and won the ﬁrst place of the graph-level track\nin OGB Large-Scale Challenge[21, 58]. As stated in Section 3.3, we further ﬁnd that the proposed\nGraphormer does not encounter the problem of over-smoothing, i.e., the train and validate error keep\ngoing down along with the growth of depth and width of models.\n4.2 Graph Representation\nIn this section, we further investigate the performance of Graphormer on commonly used graph-level\nprediction tasks of popular leaderboards, i.e., OGB [22] (OGBG-MolPCBA, OGBG-MolHIV), and\nbenchmarking-GNN [14] (ZINC). Since pre-training is encouraged by OGB, we mainly explore\nthe transferable capability of a Graphormer model pre-trained on OGB-LSC (i.e., PCQM4M-LSC).\nPlease note that the model conﬁgurations, hyper-parameters, and the pre-training performance of\npre-trained Graphormers used for MolPCBA and MolHIV are different from the models used in\nthe previous subsection. Please refer to Appendix B for detailed descriptions. For benchmarking-\nGNN, which does not encourage large pre-trained model, we train an additional Graphormer SLIM\n(L= 12,d = 80, total param.= 489K) from scratch on ZINC.\nBaselines. We report performance of GNNs which achieve top-performance on the ofﬁcial leader-\nboards5 without additional domain-speciﬁc features. Considering that the pre-trained Graphormer\nleverages external data, for a fair comparison on OGB datasets, we additionally report performance\nfor ﬁne-tuning GIN-VN pre-trained on PCQM4M-LSC dataset, which achieves the previous state-of-\nthe-art valid and test MAE on that dataset.\nSettings. We report detailed training strategies in Appendix B. In addition, Graphormer is more\neasily trapped in the over-ﬁtting problem due to the large size of the model and the small size of the\ndataset. Therefore, we employ a widely used data augmentation for graph - FLAG [27], to mitigate\nthe over-ﬁtting problem on OGB datasets.\nResults. Table 2, 3 and 4 summarize performance of Graphormer comparing with other GNNs on\nMolHIV , MolPCBA and ZINC datasets. Especially, GT [13] and SAN [28] in Table 4 are recently\nproposed Transformer-based GNN models. Graphormer consistently and signiﬁcantly outperforms\nprevious state-of-the-art GNNs on all three datasets by a large margin. Specially, except Graphormer,\n5https://ogb.stanford.edu/docs/leader_graphprop/\nhttps://github.com/graphdeeplearning/benchmarking-gnns/blob/master/docs/07_\nleaderboards.md\n7\nTable 2: Results on MolPCBA.\nmethod #param. AP (%)\nDeeperGCN-VN+FLAG [30] 5.6M 28.42±0.43\nDGN [2] 6.7M 28.85±0.30\nGINE-VN [5] 6.1M 29.17±0.15\nPHC-GNN [29] 1.7M 29.47±0.26\nGINE-APPNP [5] 6.1M 29.79±0.30\nGIN-VN[54] (ﬁne-tune) 3.4M 29.02±0.17\nGraphormer-FLAG 119.5M 31.39±0.32\nTable 3: Results on MolHIV .\nmethod #param. AUC (%)\nGCN-GraphNorm [5, 8] 526K 78.83±1.00\nPNA [10] 326K 79.05±1.32\nPHC-GNN [29] 111K 79.34±1.16\nDeeperGCN-FLAG [30] 532K 79.42±1.20\nDGN [2] 114K 79.70±0.97\nGIN-VN[54] (ﬁne-tune) 3.3M 77.80±1.82\nGraphormer-FLAG 47.0M 80.51±0.53\nTable 4: Results on ZINC.\nmethod #param. test MAE\nGIN [54] 509,549 0.526±0.051\nGraphSage [18] 505,341 0.398±0.002\nGAT [50] 531,345 0.384±0.007\nGCN [26] 505,079 0.367±0.011\nGatedGCN-PE [4] 505,011 0.214±0.006\nMPNN (sum) [15] 480,805 0.145±0.007\nPNA [10] 387,155 0.142±0.010\nGT [13] 588,929 0.226±0.014\nSAN [28] 508, 577 0.139±0.006\nGraphormerSLIM 489,321 0.122±0.006\nthe other pre-trained GNNs do not achieve competitive performance, which is in line with previous\nliterature [20]. In addition, we conduct more comparisons to ﬁne-tuning the pre-trained GNNs, please\nrefer to Appendix C.\n4.3 Ablation Studies\nWe perform a series of ablation studies on the importance of designs in our proposed Graphormer,\non PCQM4M-LSC dataset. The ablation results are included in Table 5. To save the computation\nresources, the Transformer models in table 5 have 12 layers, and are trained for 100K iterations.\nNode Relation Encoding. We compare previously used positional encoding (PE) to our proposed\nspatial encoding, which both aim to encode the information of distinct node relation to Transformers.\nThere are various PEs employed by previous Transformer-based GNNs, e.g., Weisfeiler-Lehman-\nPE (WL-PE) [ 61] and Laplacian PE [ 3, 14]. We report the performance for Laplacian PE since\nit performs well comparing to a series of PEs for Graph Transformer in previous literature [ 13].\nTransformer architecture with the spatial encoding outperforms the counterpart built on the positional\nencoding, which demonstrates the effectiveness of using spatial encoding to capture the node spatial\ninformation.\nCentrality Encoding. Transformer architecture with degree-based centrality encoding yields a\nlarge margin performance boost in comparison to those without centrality information. This indicates\nthat the centrality encoding is indispensable to Transformer architecture for modeling graph data.\nEdge Encoding. We compare our proposed edge encoding (denoted as via attn bias) to two\ncommonly used edge encodings described in Section 3.1.3 to incorporate edge features into GNN,\ndenoted as via node and via Aggr in Table 5. From the table, the gap of performance is minor between\nthe two conventional methods, but our proposed edge encoding performs signiﬁcantly better, which\nindicates that edge encoding as attention bias is more effective for Transformer to capture spatial\ninformation on edges.\n8\nTable 5: Ablation study results on PCQM4M-LSC dataset with different designs.\nNode Relation Encoding Centrality Edge Encoding valid MAELaplacian PE[13] Spatial via node via Aggr via attn bias(Eq.7)\n- - - - - - 0.2276\n\u0013 - - - - - 0.1483\n- \u0013 - - - - 0.1427\n- \u0013 \u0013 - - - 0.1396\n- \u0013 \u0013 \u0013 - - 0.1328\n- \u0013 \u0013 - \u0013 - 0.1327\n- \u0013 \u0013 - - \u0013 0.1304\n5 Related Work\nIn this section, we highlight the most recent works which attempt to develop standard Transformer\narchitecture-based GNN or graph structural encoding, but spend less effort on elaborating the works\nby adapting attention mechanism to GNNs [33, 60, 7, 23, 1, 50, 51, 61, 48].\n5.1 Graph Transformer\nThere are several works that study the performance of pure Transformer architectures (stacked by\ntransformer layers) with modiﬁcations on graph representation tasks, which are more related to our\nGraphormer. For example, several parts of the transformer layer are modiﬁed in [ 46], including\nan additional GNN employed in attention sub-layer to produce vectors of Q, K, and V, long-range\nresidual connection, and two branches of FFN to produce node and edge representations separately.\nThey pre-train their model on 10 million unlabelled molecules and achieve excellent results by\nﬁne-tuning on downstream tasks. Attention module is modiﬁed to a soft adjacency matrix in [ 41]\nby directly adding the adjacency matrix and RDKit6-computed inter-atomic distance matrix to the\nattention probabilites. Very recently, Dwivediet al. [13] revisit a series of works for Transformer-\nbased GNNs, and suggest that the attention mechanism in Transformers on graph data should only\naggregate the information from neighborhood (i.e., using adjacent matrix as attention mask) to ensure\ngraph sparsity, and propose to use Laplacian eigenvector as positional encoding. Their model GT\nsurpasses baseline GNNs on graph representation task. A concurrent work [28] propose a novel full\nLaplacian spectrum to learn the position of each node in a graph, and empirically shows better results\nthan GT.\n5.2 Structural Encodings in GNNs\nPath and Distance in GNNs.Information of path and distance is commonly used in GNNs. For\nexample, an attention-based aggregation is proposed in [ 9] where the node features, edge features,\none-hot feature of the distance and ring ﬂag feature are concatenated to calculate the attention\nprobabilites; similar to [9], path-based attention is leveraged in [56] to model the inﬂuence between\nthe center node and its higher-order neighbors; a distance-weighted aggregation scheme on graph is\nproposed in [59]; it has been proved in [32] that adopting distance encoding (i.e., one-hot feature of\nthe distance as extra node attribute) could lead to a strictly more expressive power than the 1-WL test.\nPositional Encoding in Transformer on Graph.Several works introduce positional encoding\n(PE) to Transformer-based GNNs to help the model capture the node position information. For\nexample, Graph-BERT [61] introduces three types of PE to embed the node position information\nto model, i.e., an absolute WL-PE which represents different nodes labeled by Weisfeiler-Lehman\nalgorithm, an intimacy based PE and a hop based PE which are both variant to the sampled subgraphs.\nAbsolute Laplacian PE is employed in [ 13] and empircal study shows that its performance surpasses\nthe absolute WL-PE used in [61].\nEdge Feature. Except the conventionally used methods to encode edge feature, which are described\nin previous section, there are several attempts that exploit how to better encode edge features: an\nattention-based GNN layer is developed in [ 16] to encode edge features, where the edge feature\n6https://www.rdkit.org/\n9\nis weighted by the similarity of the features of its two nodes; edge feature has been encoded into\nthe popular GIN [54] in [5]; in [13], the authors propose to project edge features to an embedding\nvector, then multiply it by attention coefﬁcients, and send the result to an additional FFN sub-layer to\nproduce edge representations;\n6 Conclusion\nWe have explored the direct application of Transformers to graph representation. With three novel\ngraph structural encodings, the proposed Graphormer works surprisingly well on a wide range of\npopular benchmark datasets. While these initial results are encouraging, many challenges remain.\nFor example, the quadratic complexity of the self-attention module restricts Graphormer’s application\non large graphs. Therefore, future development of efﬁcient Graphormer is necessary. Performance\nimprovement could be expected by leveraging domain knowledge-powered encodings on particular\ngraph datasets. Finally, an applicable graph sampling strategy is desired for node representation\nextraction with Graphormer. We leave them for future works.\n7 Acknowledgement\nWe would like to thank Mingqi Yang and Shanda Li for insightful discussions.\nReferences\n[1] Jinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with graph\nmultiset pooling. ICLR, 2021.\n[2] Dominique Beaini, Saro Passaro, Vincent Létourneau, William L Hamilton, Gabriele Corso, and Pietro\nLiò. Directional graph networks. In International Conference on Machine Learning, 2021.\n[3] Mikhail Belkin and Partha Niyogi. Laplacian eigenmaps for dimensionality reduction and data representa-\ntion. Neural computation, 15(6):1373–1396, 2003.\n[4] Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint arXiv:1711.07553,\n2017.\n[5] Rémy Brossard, Oriel Frigo, and David Dehaene. Graph convolutions that can ﬁnally model local structure.\narXiv preprint arXiv:2011.15069, 2020.\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors,\nAdvances in Neural Information Processing Systems, volume 33, pages 1877–1901. Curran Associates,\nInc., 2020.\n[7] Deng Cai and Wai Lam. Graph transformer for graph-to-sequence learning. In Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, volume 34, pages 7464–7471, 2020.\n[8] Tianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu, and Liwei Wang. Graphnorm: A principled\napproach to accelerating graph neural network training. In International Conference on Machine Learning,\n2021.\n[9] Benson Chen, Regina Barzilay, and Tommi Jaakkola. Path-augmented graph transformer network. arXiv\npreprint arXiv:1905.12712, 2019.\n[10] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Liò, and Petar Veliˇckovi´c. Principal neighbour-\nhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33, 2020.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidi-\nrectional transformers for language understanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 4171–4186, 2019.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n10\n[13] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. AAAI\nWorkshop on Deep Learning on Graphs: Methods and Applications, 2021.\n[14] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson. Bench-\nmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.\n[15] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message\npassing for quantum chemistry. In International Conference on Machine Learning, pages 1263–1272.\nPMLR, 2017.\n[16] Liyu Gong and Qiang Cheng. Exploiting edge features for graph neural networks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9211–9219, 2019.\n[17] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang,\nZhengdong Zhang, Yonghui Wu, et al. Conformer: Convolution-augmented transformer for speech\nrecognition. arXiv preprint arXiv:2005.08100, 2020.\n[18] William L Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\nIn NIPS, 2017.\n[19] Vincent J Hellendoorn, Charles Sutton, Rishabh Singh, Petros Maniatis, and David Bieber. Global relational\nmodels of source code. In International conference on learning representations, 2019.\n[20] W Hu, B Liu, J Gomes, M Zitnik, P Liang, V Pande, and J Leskovec. Strategies for pre-training graph\nneural networks. In International Conference on Learning Representations (ICLR), 2020.\n[21] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogb-lsc: A\nlarge-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021.\n[22] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,\nand Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv preprint\narXiv:2005.00687, 2020.\n[23] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. Heterogeneous graph transformer. InProceedings\nof The Web Conference 2020, pages 2704–2710, 2020.\n[24] Katsuhiko Ishiguro, Shin-ichi Maeda, and Masanori Koyama. Graph warp module: an auxiliary module for\nboosting the power of graph neural networks in molecular graph analysis.arXiv preprint arXiv:1902.01020,\n2019.\n[25] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the positional encoding in language pre-training. ICLR,\n2020.\n[26] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional networks.\narXiv preprint arXiv:1609.02907, 2016.\n[27] Kezhi Kong, Guohao Li, Mucong Ding, Zuxuan Wu, Chen Zhu, Bernard Ghanem, Gavin Taylor,\nand Tom Goldstein. Flag: Adversarial data augmentation for graph neural networks. arXiv preprint\narXiv:2010.09891, 2020.\n[28] Devin Kreuzer, Dominique Beaini, William Hamilton, Vincent Létourneau, and Prudencio Tossou. Re-\nthinking graph transformers with spectral attention. arXiv preprint arXiv:2106.03893, 2021.\n[29] Tuan Le, Marco Bertolini, Frank Noé, and Djork-Arné Clevert. Parameterized hypercomplex graph neural\nnetworks for graph classiﬁcation. arXiv preprint arXiv:2103.16584, 2021.\n[30] Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train deeper\ngcns. arXiv preprint arXiv:2006.07739, 2020.\n[31] Junying Li, Deng Cai, and Xiaofei He. Learning graph-level representation for drug discovery. arXiv\npreprint arXiv:1709.03741, 2017.\n[32] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding: Design provably more\npowerful neural networks for graph representation learning. Advances in Neural Information Processing\nSystems, 33, 2020.\n[33] Yuan Li, Xiaodan Liang, Zhiting Hu, Yinbo Chen, and Eric P. Xing. Graph transformer, 2019.\n[34] Xi Victoria Lin, Richard Socher, and Caiming Xiong. Multi-hop knowledge graph reasoning with reward\nshaping. arXiv preprint arXiv:1808.10568, 2018.\n[35] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021.\n11\n[37] Shengjie Luo, Shanda Li, Tianle Cai, Di He, Dinglan Peng, Shuxin Zheng, Guolin Ke, Liwei Wang, and\nTie-Yan Liu. Stable, fast and accurate: Kernelized attention with relative positional encoding. NeurIPS,\n2021.\n[38] Haggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks.\nIn H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors, Advances\nin Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n[39] P David Marshall. The promotion and presentation of the self: celebrity as marker of presentational media.\nCelebrity studies, 1(1):35–48, 2010.\n[40] Alice Marwick and Danah Boyd. To see and be seen: Celebrity practice on twitter. Convergence,\n17(2):139–158, 2011.\n[41] Łukasz Maziarka, Tomasz Danel, Sławomir Mucha, Krzysztof Rataj, Jacek Tabor, and Stanisław Jastrz˛ ebski.\nMolecule attention transformer. arXiv preprint arXiv:2002.08264, 2020.\n[42] Maho Nakata and Tomomi Shimazaki. Pubchemqc project: a large-scale ﬁrst-principles electronic structure\ndatabase for data-driven chemistry. Journal of chemical information and modeling , 57(6):1300–1308,\n2017.\n[43] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma\nMalkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modiﬁcations transfer across\nimplementations and applications? arXiv preprint arXiv:2102.11972, 2021.\n[44] Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, and Tie-Yan Liu. How could neural networks\nunderstand programs? In International Conference on Machine Learning. PMLR, 2021.\n[45] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJournal of Machine Learning Research, 21(140):1–67, 2020.\n[46] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-\nsupervised graph transformer on large-scale molecular data. Advances in Neural Information Processing\nSystems, 33, 2020.\n[47] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464–468, 2018.\n[48] Yunsheng Shi, Zhengjie Huang, Wenjin Wang, Hui Zhong, Shikun Feng, and Yu Sun. Masked label predic-\ntion: Uniﬁed message passing model for semi-supervised classiﬁcation. arXiv preprint arXiv:2009.03509,\n2020.\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n[50] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.\nGraph attention networks. ICLR, 2018.\n[51] Guangtao Wang, Rex Ying, Jing Huang, and Jure Leskovec. Direct multi-hop attention based graph neural\nnetwork. arXiv preprint arXiv:2009.14332, 2020.\n[52] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\ncomplexity. arXiv preprint arXiv:2006.04768, 2020.\n[53] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture. In International\nConference on Machine Learning, pages 10524–10533. PMLR, 2020.\n[54] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?\nIn International Conference on Learning Representations, 2019.\n[55] Mingqi Yang, Yanming Shen, Heng Qi, and Baocai Yin. Breaking the expressive bottlenecks of graph\nneural networks. arXiv preprint arXiv:2012.07219, 2020.\n[56] Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, and Dacheng Tao. Spagan: Shortest path graph\nattention network. Advances in IJCAI, 2019.\n[57] Chengxuan Ying, Guolin Ke, Di He, and Tie-Yan Liu. Lazyformer: Self attention with lazy update. arXiv\npreprint arXiv:2102.12702, 2021.\n[58] Chengxuan Ying, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai, Chenglin Wu, Yuxin\nWang, Yanming Shen, and Di He. First place solution of kdd cup 2021 & ogb large-scale challenge\ngraph-level track. arXiv preprint arXiv:2106.08279, 2021.\n[59] Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In International\nConference on Machine Learning, pages 7134–7143. PMLR, 2019.\n12\n[60] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim. Graph transformer\nnetworks. Advances in Neural Information Processing Systems, 32, 2019.\n[61] Jiawei Zhang, Haopeng Zhang, Congying Xia, and Li Sun. Graph-bert: Only attention is needed for\nlearning graph representations. arXiv preprint arXiv:2001.05140, 2020.\n[62] Chen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced adversarial\ntraining for natural language understanding. In ICLR, 2020.\n[63] Daniel Zügner, Tobias Kirschstein, Michele Catasta, Jure Leskovec, and Stephan Günnemann. Language-\nagnostic representation learning of source code from structure and context. In International Conference on\nLearning Representations, 2020.\n13\nA Proofs\nA.1 SPD can Be Used to Improve WL-Test\nFigure 2: These two graphs cannot be distinguished by 1-WL-test. But the SPD sets, i.e.,\nthe SPD from each node to others, are different: The two types of nodes in the left graph\nhave SPD sets {0,1,1,2,2,3},{0,1,1,1,2,2}while the nodes in the right graph have SPD sets\n{0,1,1,2,3,3},{0,1,1,1,2,2}.\n1-WL-test fails in many cases [38, 32], thus classic message passing GNNs also fail to distinguish many pairs of\ngraphs. We show that SPD might help when 1-WL-test fails, for example, in Figure 2 where 1-WL-test fails, the\nsets of SPD from all nodes to others successfully distinguish the two graphs.\nA.2 Proof of Fact 1\nMEAN AGGREGATE. We begin by showing that self-attention module with Spatial Encoding can repre-\nsent MEAN aggregation. This is achieved by in Eq. (6): 1) setting bφ = 0if φ= 1and bφ = −∞otherwise\nwhere φis the SPD; 2) setting WQ = WK = 0and WV to be the identity matrix. Then softmax (A) V gives\nthe average of representations of the neighbors.\nSUM AGGREGATE. The SUM aggregation can be realized by ﬁrst perform MEAN aggregation and then\nmultiply the node degrees. Speciﬁcally, the node degrees can be extracted from Centrality Encoding by an\nadditional head and be concatenated to the representations after MEAN aggregation. Then the FFN module in\nGraphormer can represent the function of multiplying the degree to the dimensions of averaged representations\nby the universal approximation theorem of FFN.\nMAX AGGREGATE. Representing the MAX aggregation is harder than MEAN and SUM. For each\ndimension tof the representation vector, we need one head to select the maximal value over t-th dimension\nin the neighbor by in Eq. (6): 1) setting bφ = 0if φ = 1and bφ = −∞otherwise where φis the SPD; 2)\nsetting WK = et which is the t-th standard basis; WQ = 0and the bias term (which is ignored in the previous\ndescription for simplicity) of Qto be T1; and WV = et, where T is the temperature that can be chosen to be\nlarge enough so that the softmax function can approximate hard max and 1 is the vector whose elements are all\n1.\nCOMBINE. The COMBINE step takes the result of AGGREGATE and the previous representation of\ncurrent node as input. This can be achieved by the AGGREGATE operations described above together with\nan additional head which outputs the features of present nodes, i.e., in Eq. (6): 1) setting bφ = 0if φ= 0and\nbφ = −∞otherwise where φis the SPD; 2) setting WQ = WK = 0and WV to be the identity matrix. Then\nthe FFN module can approximate any COMBINE function by the universal approximation theorem of FFN.\nA.3 Proof of Fact 2\nMEAN READOUT. This can be proved by setting WQ = WK = 0, the bias terms of Q,K to be T1, and\nWV to be the identity matrix where T should be much larger than the scale of bφ so that T211⊤dominates the\nSpatial Encoding term.\nB Experiment Details\nB.1 Details of Datasets\nWe summarize the datasets used in this work in Table 6. PCQM4m-LSC is a quantum chemistry graph-level\nprediction task in recent OGB Large-Scale Challenge, originally curated under the PubChemQC project [ 42].\n14\nTable 6: Statistics of the datasets.\nDataset Scale # Graphs # Nodes # Edges Task Type\nPCQM4M-LSC Large 3,803,453 53,814,542 55,399,880 Regression\nOGBG-MolPCBA Medium 437,929 11,386,154 12,305,805 Binary classiﬁcation\nOGBG-MolHIV Small 41,127 1,048,738 1,130,993 Binary classiﬁcation\nZINC (sub-set) Small 12,000 277,920 597,960 Regression\nThe task of PCQM4M-LSC is to predict DFT(density functional theory)-calculated HOMO-LUMO energy gap\nof molecules given their 2D molecular graphs, which is one of the most practically-relevant quantum chemical\nproperties of molecule science. PCQM4M-LSC is unprecedentedly large in scale comparing to other labeled\ngraph-level prediction datasets, which contains more than 3.8M graphs. Besides, we conduct experiments on two\nmolecular graph datasets in popular OGB leaderboards, i.e., OGBG-MolPCBA and OGBG-MolHIV . They are\ntwo molecular property prediction datasets with different sizes. The pre-trained knowledge of molecular graph on\nPCQM4M-LSC could be easily leveraged on these two datasets. We adopt ofﬁcial scaffold split on three datasets\nfollowing [ 21, 22]. In addition, we employ another popular leaderboard, i.e., benchmarking-gnn [14]. We use\nthe ZINC datasets, which is the most popular real-world molecular dataset to predict graph property regression\nfor contrained solubility, an important chemical property for designing generative GNNs for molecules. Different\nfrom the scaffold spliting in OGB, uniform sampling is adopted in ZINC for data splitting.\nB.2 Details of Training Strategies\nB.2.1 PCQM4M-LSC\nTable 7: Model Conﬁgurations and Hyper-parameters of Graphormer on PCQM4M-LSC.\nGraphormerSMALL Graphormer\n#Layers 6 12\nHidden Dimensiond 512 768\nFFN Inner-layer Dimension 512 768\n#Attention Heads 32 32\nHidden Dimension of Each Head 16 24\nFFN Dropout 0.1 0.1\nAttention Dropout 0.1 0.1\nEmbedding Dropout 0.0 0.0\nMax Steps 1M 1M\nMax Epochs 300 300\nPeak Learning Rate 3e-4 2e-4\nBatch Size 1024 1024\nWarm-up Steps 60K 60K\nLearning Rate Decay Linear Linear\nAdam ϵ 1e-8 1e-8\nAdam (β1, β2) (0.9, 0.999) (0.9, 0.999)\nGradient Clip Norm 5.0 5.0\nWeight Decay 0.0 0.0\nWe report the detailed hyper-parameter settings used for training Graphormer in Table 7. We reduce the FFN\ninner-layer dimension of 4din [ 49] to d, which does not appreciably hurt the performance but signiﬁcantly\nsave the parameters. The embedding dropout ratio is set to 0.1 by default in many previous Transformer\nworks [11, 35]. However, we empirically ﬁnd that a small embedding dropout ratio (e.g., 0.1) would lead to an\nobservable performance drop on validation set of PCQM4M-LSC. One possible reason is that the molecular\ngraph is relative small (i.e., the median of #atoms in each molecule is about 15), making graph property more\nsensitive to the embeddings of each node. Therefore, we set embedding dropout ratio to 0 on this dataset.\nB.2.2 OGBG-MolPCBA\nPre-training. We ﬁrst report the model conﬁgurations and hyper-parameters of the pre-trained Graphormer\non PCQM4M-LSC. Empirically, we ﬁnd that the performance on MolPCBA beneﬁts from the large pre-training\nmodel size. Therefore, we train a deep Graphormer with 18 Transformer layers on PCQM4M-LSC. The hidden\ndimension and FFN inner-layer dimension are set to 1024. We set peak learning rate to 1e-4 for the deep\n15\nTable 8: Hyper-parameters for Graphormer on OGBG-MolPCBA, where the text in bolddenotes the\nhyper-parameters we eventually use.\nGraphormer\nMax Epochs {2, 5, 10}\nPeak Learning Rate {2e-4, 3e-4}\nBatch Size 256\nWarm-up Ratio 0.06\nAttention Dropout 0.3\nm {1, 2,3,4}\nα 0.001\nϵ 0.001\nGraphormer. Besides, we enlarge the attention dropout ratio from 0.1 to 0.3 in both pre-training and ﬁne-tuning\nto prevent the model from over-ﬁtting. The rest of hyper-parameters remain unchanged. The pre-trained\nGraphormer used for MolPCBA achieves a valid MAE of 0.1253 on PCQM4M-LSC, which is slightly worse\nthan the reports in Table 1.\nFine-tuning. Table 8 summarizes the hyper-parameters used for ﬁne-tuning Graphormer on OGBG-\nMolPCBA. We conduct a grid search for several hyper-parameters to ﬁnd the optimal conﬁguration. The\nexperimental results are reported by the mean of 10 independent runs with random seeds. We use FLAG [27]\nwith minor modiﬁcations for graph data augmentation. In particular, except the step size αand the number\nof steps m, we also employ a projection step in [ 62] with maximum perturbation ϵ. The performance of\nGraphormer on MolPCBA is quite robust to the hyper-parameters of FLAG. The rest of hyper-parameters are the\nsame with the pre-training model.\nB.2.3 OGBG-MolHIV\nTable 9: Hyper-parameters for Graphormer on OGBG-MolHIV , where thetext in bolddenotes the\nhyper-parameters we eventually use.\nGraphormer\nMax Epochs 8\nPeak Learning Rate 2e-4\nBatch Size 128\nWarm-up Ratio 0.06\nDropout 0.1\nAttention Dropout 0.1\nm {1,2,3,4}\nα {0.001, 0.01, 0.1, 0.2}\nϵ {0, 0.001, 0.01, 0.1}\nPre-training. We use the Graphormer reported in Table 1 as the pre-trained model for OGBG-MolHIV ,\nwhere the pre-training hyper-parameters are summarized in Table 7.\nFine-tuning. The hyper-parameters for ﬁne-tuning Graphormer on OGBG-MolHIV are presented in Table\n9. Empirically, we ﬁnd that the different choices of hyper-parameters of FLAG (i.e., step size α, number of\nsteps m, and maximum perturbation ϵ) would greatly affect the performance of Graphormer on OGBG-MolHiv.\nTherefore, we spend more effort to conduct grid search for hyper-parameters of FLAG. We report the best\nhyper-parameters by the mean of 10 independent runs with random seeds.\nB.2.4 ZINC\nTo keep the total parameters of Graphormer less than 500K per the request from benchmarking-GNN leader-\nboard [14], we train a slim 12-layer Graphormer with hidden dimension of 80, which is called GraphormerSLIM\nin Table 4, and has about 489K learnable parameters. The number of attention heads is set to 8. Table 10\nsummarizes the detailed hyper-parameters on ZINC. We train 400K steps on this dataset, and employ a weight\ndecay of 0.01.\n16\nTable 10: Model Conﬁgurations and Hyper-parameters on ZINC(sub-set).\nGraphormerSLIM\n#Layers 12\nHidden Dimension 80\nFFN Inner-Layer Hidden Dimension 80\n#Attention Heads 8\nHidden Dimension of Each Head 10\nFFN Dropout 0.1\nAttention Dropout 0.1\nEmbedding Dropout 0.0\nMax Steps 400K\nMax Epochs 10K\nPeak Learning Rate 2e-4\nBatch Size 256\nWarm-up Steps 40K\nLearning Rate Decay Linear\nAdam ϵ 1e-8\nAdam (β1, β2) (0.9, 0.999)\nGradient Clip Norm 5.0\nWeight Decay 0.01\nTable 11: Hyper-parameters for ﬁne-tuning GROVER on MolHIV and MolPCBA.\nGROVER GROVER LARGE\nDropout {0.1, 0.5} {0.1, 0.5}\nMax Epochs {10, 30, 50} {10, 30}\nLearning Rate {5e-5, 1e-4, 5e-4, 1e-3} {5e-5, 1e-4, 5e-4, 1e-3}\nBatch Size {64, 128} {64, 128}\nInitial Learning Rate 1e-7 1e-7\nEnd Learning Rate 1e-9 1e-9\nB.3 Details of Hyper-parameters for Baseline Methods\nIn this section, we present the details of our re-implementation of the baseline methods.\nB.3.1 PCQM4M-LSC\nThe ofﬁcial Github repository of OGB-LSC7 provides hyper-parameters and codes to reproduce the results on\nleaderboard. These hyper-parameters work well on almost all popular GNN variants, except the DeeperGCN-VN,\nwhich results in a training divergence. Therefore, for DeeperGCN-VN, we follow the ofﬁcial hyper-parameter\nsetting8 provided by the authors [30]. For a fair comparison to Graphormer, we train a 12-layer DeeperGCN.\nThe hidden dimension is set to 600. The batch size is set to 256. The learning rate is set to 1e-3, and a step\nlearning rate scheduler is employed with the decaying step size and the decaying factor γas 30 epochs and 0.25.\nThe model is trained for 100 epochs.\nThe default dimension of laplacian PE of GT [13] is set to 8. However, it will cause 2.91% small molecules\n(less than 8 atoms) to be ﬁltered out. Therefore, for GT and GT-Wide, we set the dimension of laplacian PE to\n4, which results in only 0.08% ﬁltering out. We adopt the default hyper-parameter settings described in [ 13],\nexcept that we decrease the learning rate to 1e-4, which leads to a better convergence on PCQM4M-LSC.\nB.3.2 OGBG-MolPCBA\nTo ﬁne-tune the pre-trained GIN- VN on MolPCBA, we follow the hyper-parameter settings provided in the\noriginal OGB paper [ 22]. To be more concrete, we load the pre-trained checkpoint reported in Table 1 and\nﬁne-tune it on OGBG-MolPCBA dataset. We use the grid search on the hyper-parameters for better ﬁne-tuning\n7https://github.com/snap-stanford/ogb/tree/master/examples/lsc/pcqm4m\n8https://github.com/lightaime/deep_gcns_torch/tree/master/examples/ogb/ogbg_mol#\ntrain\n17\nTable 12: Comparison to pre-trained Transformer-based GNN on MolHIV . * indicates that additional\nfeatures for molecule are used.\nmethod #param. AUC (%)\nMorgan Finger Prints + Random Forest* 230K 80.60±0.10\nGROVER*[46] 48.8M 79.33±0.09\nGROVERLARGE *[46] 107.7M 80.32±0.14\nGraphormer-FLAG 47.0M 80.51±0.53\nTable 13: Comparison to pre-trained Transformer-based GNN on MolPCBA. * indicates that\nadditional features for molecule are used.\nmethod #param. AP (%)\nGROVER*[46] 48.8M 16.77±0.36\nGROVERLARGE *[46] 107.7M 13.05±0.18\nGraphormer-FLAG 47.0M 31.39±0.32\nperformance. In particular, the learning rate is selected from {1e−5,1e−4,1e−3}; the dropout ratio is\nselected from {0.0,0.1,0.5}; the batch size is selected from {32,64}.\nB.3.3 OGBG-MolHIV\nSimilarly, we ﬁne-tune the pre-trained GIN-VN on MolHIV by following the hyper-parameter settings provided\nin the original OGB paper [ 22]. We also conduct the grid search to look for optimal hyper-parameters. The\nranges for each hyper-parameter of grid search are the same as the previous subsection.\nC More Experiments\nAs described in the related work, GROVER is a Transformer-based GNN, which has 100 million parameters and\npre-trained on 10 million unlabelled molecules using 250 Nvidia V100 GPUs. In this section, we report the\nﬁne-tuning scores of GROVER on MolHIV and MolPCBA, and compare with proposed Graphormer.\nWe download the pre-trained GROVER models from its ofﬁcial Github webpage9, follow the ofﬁcial instruc-\ntions10 and ﬁne-tune the provided pre-trained checkpoints with careful search of hyper-parameters (in Table\n11). We ﬁnd that GROVER could achieve competitive performance on MolHIV only if employing additional\nmolecular features, i.e., morgan molecular ﬁnger prints and 2D features11. Therefore, we report the scores of\nGROVER by taking these two additional molecular features. Please note that, from the leaderboard12, we can\nknow such additional molecular features are very effective on MolHIV dataset.\nTable 12 and 13 summarize the performance of GROVER and GROVERLARGE comparing with Graphormer on\nMolHIV and MolPCBA. From the tables, we observe that Graphormer could consistently outperform GROVER\neven without any additional molecular features.\nD Discussion & Future Work\nComplexity. Similar to regular Transformer, the attention mechanism in Graphormer scales quadratically\nwith the number of nodesnin the input graph, which may be prohibitively expensive for largenand precludes its\nusage in settings with limited computational resources. Recently, many solutions have been proposed to address\nthis problem in Transformer [25, 52, 57, 37]. This issue would be greatly beneﬁt from the future development of\nefﬁcient Graphormer.\nChoice of centrality andφ. In Graphormer, there are multiple choices for the network centrality and the\nspatial encoding function φ(vi,vj). For example, one can leverage the L2 distance in 3D structure between two\natoms in a molecule. In this paper, we mainly evaluate general centrality and distance metric in graph theory, i.e.,\nthe degree centrality and the shortest path. Performance improvement could be expected by leveraging domain\nknowledge powered encodings on particular graph dataset.\n9https://github.com/tencent-ailab/grover\n10https://github.com/tencent-ailab/grover/blob/main/README.md#\nfinetuning-with-existing-data\n11https://github.com/tencent-ailab/grover#optional-molecular-feature-extraction-1\n12https://ogb.stanford.edu/docs/leader_graphprop/\n18\nNode Representation. There is a wide range of node representation tasks on graph structured data, such as\nﬁnance, social network, and temporal prediction. Graphormer could be naturally used for node representation\nextraction with an applicable graph sampling strategy. We leave it for future work.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.746099054813385
    },
    {
      "name": "Transformer",
      "score": 0.6905792355537415
    },
    {
      "name": "Graph",
      "score": 0.5672325491905212
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5621976256370544
    },
    {
      "name": "Architecture",
      "score": 0.540373682975769
    },
    {
      "name": "Expressive power",
      "score": 0.47379395365715027
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3872530460357666
    },
    {
      "name": "Engineering",
      "score": 0.08935752511024475
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": []
}