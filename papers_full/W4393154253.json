{
  "title": "CRA-PCN: Point Cloud Completion with Intra- and Inter-level Cross-Resolution Transformers",
  "url": "https://openalex.org/W4393154253",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5083147441",
      "name": "Yi Rong",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A5069159378",
      "name": "Haoran Zhou",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A5075846473",
      "name": "Lixin Yuan",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A5100932262",
      "name": "Mei Cheng",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A5100329224",
      "name": "Jiahao Wang",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A5101625676",
      "name": "Tong Lu",
      "affiliations": [
        "Nanjing University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3124420883",
    "https://openalex.org/W2555618208",
    "https://openalex.org/W4386083014",
    "https://openalex.org/W2939201152",
    "https://openalex.org/W2342277278",
    "https://openalex.org/W2560722161",
    "https://openalex.org/W2335364074",
    "https://openalex.org/W6793697131",
    "https://openalex.org/W2759753143",
    "https://openalex.org/W4367000009",
    "https://openalex.org/W3008961427",
    "https://openalex.org/W2905076052",
    "https://openalex.org/W4322759976",
    "https://openalex.org/W2991623768",
    "https://openalex.org/W2211722331",
    "https://openalex.org/W3025708905",
    "https://openalex.org/W6794359098",
    "https://openalex.org/W6811122175",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W2993447582",
    "https://openalex.org/W2798336850",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W2072723786",
    "https://openalex.org/W2606202972",
    "https://openalex.org/W6730270829",
    "https://openalex.org/W4226316298",
    "https://openalex.org/W6764697628",
    "https://openalex.org/W2938428612",
    "https://openalex.org/W3015869083",
    "https://openalex.org/W6747904511",
    "https://openalex.org/W4226100169",
    "https://openalex.org/W3020954686",
    "https://openalex.org/W3112134311",
    "https://openalex.org/W2900731076",
    "https://openalex.org/W3191573718",
    "https://openalex.org/W3034126769",
    "https://openalex.org/W2794614374",
    "https://openalex.org/W6846354724",
    "https://openalex.org/W2796426482",
    "https://openalex.org/W6756828990",
    "https://openalex.org/W3195390362",
    "https://openalex.org/W6754345201",
    "https://openalex.org/W2960986959",
    "https://openalex.org/W6788305448",
    "https://openalex.org/W4286750723",
    "https://openalex.org/W3192240783",
    "https://openalex.org/W2963125977",
    "https://openalex.org/W4312704718",
    "https://openalex.org/W4312663892",
    "https://openalex.org/W2963648573",
    "https://openalex.org/W3034584726",
    "https://openalex.org/W2886499109",
    "https://openalex.org/W2997088169",
    "https://openalex.org/W2963158438",
    "https://openalex.org/W3109518641",
    "https://openalex.org/W4313183789",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2944579304",
    "https://openalex.org/W3034493208",
    "https://openalex.org/W2963390820",
    "https://openalex.org/W2963231572",
    "https://openalex.org/W4226368343",
    "https://openalex.org/W2963083779",
    "https://openalex.org/W2968296999",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2979750740",
    "https://openalex.org/W4312848711",
    "https://openalex.org/W2953668091",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W4394671432",
    "https://openalex.org/W4287556845",
    "https://openalex.org/W4312616477",
    "https://openalex.org/W4287023049",
    "https://openalex.org/W3184736166",
    "https://openalex.org/W4386075817",
    "https://openalex.org/W4386072236",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3035014292",
    "https://openalex.org/W2990613095",
    "https://openalex.org/W2964137676",
    "https://openalex.org/W2557465155"
  ],
  "abstract": "Point cloud completion is an indispensable task for recovering complete point clouds due to incompleteness caused by occlusion, limited sensor resolution, etc. The family of coarse-to-fine generation architectures has recently exhibited great success in point cloud completion and gradually became mainstream. In this work, we unveil one of the key ingredients behind these methods: meticulously devised feature extraction operations with explicit cross-resolution aggregation. We present Cross-Resolution Transformer that efficiently performs cross-resolution aggregation with local attention mechanisms. With the help of our recursive designs, the proposed operation can capture more scales of features than common aggregation operations, which is beneficial for capturing fine geometric characteristics. While prior methodologies have ventured into various manifestations of inter-level cross-resolution aggregation, the effectiveness of intra-level one and their combination has not been analyzed. With unified designs, Cross-Resolution Transformer can perform intra- or inter-level cross-resolution aggregation by switching inputs. We integrate two forms of Cross-Resolution Transformers into one up-sampling block for point generation, and following the coarse-to-fine manner, we construct CRA-PCN to incrementally predict complete shapes with stacked up-sampling blocks. Extensive experiments demonstrate that our method outperforms state-of-the-art methods by a large margin on several widely used benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.",
  "full_text": "CRA-PCN: Point Cloud Completion with Intra- and Inter-level Cross-Resolution\nTransformers\nYi Rong, Haoran Zhou, Lixin Yuan, Cheng Mei, Jiahao Wang, Tong Lu*\nState Key Laboratory for Novel Software Technology, Nanjing University\nfrongyi, dg1833031, meicheng, wangjhg@smail.nju.edu.cn, hrzhou98@gmail.com, lutong@nju.edu.cn\nAbstract\nPoint cloud completion is an indispensable task for recov-\nering complete point clouds due to incompleteness caused\nby occlusion, limited sensor resolution, etc. The family of\ncoarse-to-ﬁne generation architectures has recently exhibited\ngreat success in point cloud completion and gradually be-\ncame mainstream. In this work, we unveil one of the key\ningredients behind these methods: meticulously devised fea-\nture extraction operations with explicit cross-resolution ag-\ngregation. We present Cross-Resolution Transformer that ef-\nﬁciently performs cross-resolution aggregation with local at-\ntention mechanisms. With the help of our recursive designs,\nthe proposed operation can capture more scales of features\nthan common aggregation operations, which is beneﬁcial for\ncapturing ﬁne geometric characteristics. While prior method-\nologies have ventured into various manifestations of inter-\nlevel cross-resolution aggregation, the effectiveness of intra-\nlevel one and their combination has not been analyzed. With\nuniﬁed designs, Cross-Resolution Transformer can perform\nintra- or inter-level cross-resolution aggregation by switching\ninputs. We integrate two forms of Cross-Resolution Trans-\nformers into one up-sampling block for point generation, and\nfollowing the coarse-to-ﬁne manner, we construct CRA-PCN\nto incrementally predict complete shapes with stacked up-\nsampling blocks. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods by a large mar-\ngin on several widely used benchmarks. Codes are available\nat https://github.com/EasyRy/CRA-PCN.\nIntroduction\nDriven by the rapid development of 3D acquisition technolo-\ngies, 3D vision is in great demand for research. Among var-\nious types of 3D data, point cloud is the most popular de-\nscription and commonly used in real-world applications (Ca-\ndena et al. 2016; Reddy, V o, and Narasimhan 2018; Rusu\net al. 2008). However, due to self-occlusion and limited\nsensor resolution, acquired point clouds are usually highly\nsparse and incomplete, which impedes further applications.\nTherefore, recovering complete point clouds is an indispens-\nable task, whose major purposes are to preserve details of\npartial observations, infer missing parts, and densify sparse\nsurfaces.\n*Corresponding author\nCopyright © 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nencoding phasedecoding phase\ngeneration blockencoding block\ngenerated point clouds \n…input\ninter-level\n……\ninter-level\n1 21 2 inter-levelintra-levelcross-resolutionaggregationpoint expansion\n(a) (b)\n(c) (d)\n(e) farthest pointsampling\nFigure 1: Illustration of our main idea. Here, we analyze\nseveral point generation methods from the perspective of\ncross-resolution aggregation (CRA). (a) Common pipeline\nof coarse-to-ﬁne completion approaches. (b) The plain gen-\neration operation simply generates points without consider-\ning explicit CRA. (c) & (d) Several methods exploit skip\nconnections to aggregate features of other generated point\nclouds or partial inputs for the current point cloud, which can\nefﬁciently capture multi-scale features. (e) Our method not\nonly extracts more fruitful multi-scale features with novel-\ndesigned enhanced inter-level CRA but also combines intra-\nand inter-level CRA for better capturing geometric charac-\nteristics.\nIn the realm of deep learning, various approaches have\nbeen proposed to tackle this problem (Yang et al. 2018;\nChoy et al. 2016; Girdhar et al. 2016; Xie et al. 2020). Es-\npecially with the success of PointNet (Qi et al. 2017a) and\nits successors (Qi et al. 2017b; Wang et al. 2019; Zhao et al.\n2021), most approaches recover complete point clouds di-\nrectly based on 3D coordinates (Yuan et al. 2018; Wen et al.\n2020, 2021; Tchapmi et al. 2019; Yu et al. 2021; Wang,\nAng Jr, and Lee 2020). Due to the unordered and unstruc-\ntured nature of point cloud data, learning ﬁne geometric\ncharacteristics and structural features is essential for pre-\ndicting reasonable shapes. To accomplish this aim, main-\nstreams of the recent methods (Xiang et al. 2021; Zhou\net al. 2022; Huang et al. 2020; Yan et al. 2022; Huang et al.\n2020; Tang et al. 2022; Wang et al. 2022) formulate the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4676\npoint cloud completion task as a tree-like generation pro-\ncess where an encoder-decoder architecture is adopted to ex-\ntract shape representation (e.g., a latent vector representing\na complete shape) and subsequently recover the complete\npoint cloud from low-resolution to high-resolution, as shown\nin Fig. 1(a).\nAs illustrated in Fig. 1, we select several representa-\ntive methods and conduct an analysis from the perspective\nof cross-resolution aggregation (CRA), which is a crucial\ncomponent of these hierarchical methods. Prior to gener-\nating the points, it is crucial to extract semantic informa-\ntion from the current point cloud. The plain methods (Yuan\net al. 2018; Huang et al. 2020) adopt common feature ex-\ntraction operations without considering aggregating features\nfrom point clouds with different resolutions, namely cross-\nresolution aggregation. Based on skip connections (or dense\nconnections) among different generation stages, the meth-\nods showed in Fig. 1(c) realize cross-resolution aggregation\nvia interpolation followed by multi-layer perceptrons (Yifan\net al. 2019) or transformers (Yan et al. 2022; Zhou et al.\n2022). As illustrated in Fig. 1(d), U-Net-like methods (Wen\net al. 2020; Yu et al. 2021) aggregate features of partial in-\nputs for current point clouds, commonly based on global at-\ntention mechanisms due to the mismatching of shapes, yet\nanother form of cross-resolution aggregation. While plain\nmethods can extract multi-scale representations in the gen-\neration phase, the success of the latter two methods suggests\nthat explicit cross-resolution aggregation is beneﬁcial for ef-\nﬁciently capturing multi-scale features and promoting the\ncompletion performance.\nAlthough prior approaches accomplished cross-resolution\naggregation with inter-layer connections (i.e., inter-level\nCRA), they overlooked cross-resolution aggregation inside\nlayers (i.e., intra-level CRA). In the process of point cloud\ncompletion, missing points are not generated all at once but\nare gradually completed through layer-by-layer generation,\nwhich indicates that the details of the intermediate point\nclouds are not exactly the same, making it hard for inter-\nlevel CRA-based methods to ﬁnd generation patterns that\nﬁt local regions best. In other words, it is crucial to com-\nbine inter-level and intra-level CRA, as shown in Fig. 1(e),\nto overcome the ﬂaws of previous methods.\nTo this end, we propose Cross-Resolution Transformer\nthat performs multiple scales of cross-resolution aggrega-\ntion with recursive designs and local attention mechanisms.\nCross-Resolution Transformer has several favorable proper-\nties: (1) Explicit cross-resolution aggregation: Point cloud\ndirectly aggregates features from ones with different reso-\nlutions without intermediate aids; (2) Multi-scale aggrega-\ntion: Unlike previous methods, which only realized CRA on\na few scales, our method can extract more scales of features\nwith recursive designs, even taking two resolutions of point\nclouds as input; (3) Uniﬁed design: Intra- and inter-level\nCRA share the same implementation, and different forms\nof CRA can be achieved by switching inputs; (4) Plug-and-\nplay: Without introducing signiﬁcant computation, Cross-\nresolution Transformer can serve as a plug-and-play module\nto extract local features in the decoder or encoder with large\nreceptive ﬁelds.\nWe integrate two forms of Cross-Resolution Transform-\ners in one up-sampling block for intra- and inter-level cross-\nresolution aggregation, respectively. Thanks to this combi-\nnation, the decoding block can precisely capture multi-scale\ngeometric characteristics and generate points that best ﬁt lo-\ncal regions. Mainly based on the up-sampling block, we pro-\npose CRA-PCN for point cloud completion. CRA-PCN has\nan encode-decoder architecture, and its decoder consists of\nthree consecutive up-sampling blocks. With the help of this\neffective decoder, our method outperforms state-of-the-art\ncompletion networks on several widely used benchmarks.\nThe contributions in this work can be summarized as fol-\nlows:\n• We show that one of the key ingredients behind the suc-\ncess of prior methods is explicit cross-resolution aggre-\ngation (CRA) and propose the combination of inter- and\nintra-level CRA to extract ﬁne local features.\n• We devise an effective local aggregation operation named\nCross-Resolution Transformer, which can adaptively\nsummarize multi-scale geometric characteristics in the\nmanner of cross-resolution aggregation.\n• We propose a novel CRA-PCN for point cloud comple-\ntion, which precisely captures multi-scale local proper-\nties and predicts rich details.\nRelated Work\nPoint Cloud Learning\nEarly works usually adopt multi-view projection (Li, Zhang,\nand Xia 2016; Chen et al. 2017; Lang et al. 2019) or 3D\nvoxelization (Maturana and Scherer 2015; Song et al. 2017;\nRiegler, Osman Ulusoy, and Geiger 2017; Choy, Gwak, and\nSavarese 2019) to transform the irregular point clouds into\nregular representations, followed by 2D/3D CNNs. How-\never, the converting costs are expensive, and geometric de-\ntails will inevitably be lost in the transforming process.\nTherefore, researchers have designed deep networks which\ncan directly process 3D coordinates based on permutation-\ninvariant operators. PointNet (Qi et al. 2017a) and Point-\nNet++ (Qi et al. 2017b) are the pioneering point-based net-\nworks which adopt MLPs and max pooling to extract and\naggregate features across the whole set or around local re-\ngions. Based on them, a number of point-based methods\nhave been proposed. Some methods (Wang et al. 2019; Zhao\net al. 2019; Simonovsky and Komodakis 2017) convert the\nlocal region to a graph, followed by graph convolution lay-\ners. Other methods (Xu et al. 2018; Wu, Qi, and Fuxin 2019;\nThomas et al. 2019) deﬁne novel convolutional operators\nthat apply directly to the 3D coordinates without quantiza-\ntion. Recently, attention-based methods, especially the fam-\nily of transformers (Vaswani et al. 2017; Zhao et al. 2021;\nGuo et al. 2021; Park et al. 2022), have achieved impres-\nsive success thanks to the inherent permutation-invariant,\nwhere the whole point set or local region is converted to\na sequence and the aggregation weights are adaptively de-\ntermined by data. Among them, we exploit vector atten-\ntion mechanisms (Zhao et al. 2021) to construct our Cross-\nResolution Transformer.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4677\ninterpolation\ninterpolation\n(a)Inter-levelCross-Resolution Transformer(b) Intra-level Cross-Resolution Transformer\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nSupport Point CloudQuery Point Cloud\nVA\nVA\nVA\nFPS\nFPS\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nFPS\nFPS\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nQuery & Support Point Cloud\nVA\nVA\nVA\nFPS\nFPS\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\nMRI-PCN: Point Cloud Completion with Intra- and Inter-level Multi-Resolution\nInteraction\nFirst Author\nInstitution1\nInstitution1 address\nfirstauthor@i1.org\nSecond Author\nInstitution2\nFirst line of institution2 address\nsecondauthor@i2.org\nAbstract\n1. Introduction\nP 2\nq P 1\nq P 0\nq P 2\ns P 1\ns P 0\ns\nx\nF 2\nq F 1\nq F 0\nq F 2\ns F 1\ns F 0\ns\nx\nP 2\nq , F 2\nq\nx\nP 1\nq , F 1\nq\nx\nP 0\nq , F 0\nq\nx\nP 2\ns , F 2\ns\nx\nP 1\ns , F 1\ns\nx\nP 0\ns , F 0\ns\nx\nF 2\nh F 1\nh F 0\nh\n1\ninterpolation\ninterpolation\nInter-level CRT\nQuerySupport\nIntra-level CRT\nQuerySupport\nVAVector AttentionInterpolationCat & MLP\nFigure 2: Illustration of Cross-Resolution Transformer (CRT). CRT considers the cross-resolution aggregation onmscales, and\nm= 3in this ﬁgure. (a) Inter-level CRT lets the query point cloud (blue) aggregate features from the support one (green); both\nof them are intermediate point clouds during the generation phase. (b) Intra-level CRT realizes cross-resolution aggregation\ninside the current point cloud and is a degenerate form of inter-level one.\nPoint Cloud Completion\nLike traditional methods for point cloud learning, early at-\ntempts (Choy et al. 2016; Girdhar et al. 2016; Han et al.\n2017) at 3D shape completion usually adopt intermediate\naids (i.e., voxel grids). However, these methods usually suf-\nfer from heavy computational costs and geometric informa-\ntion loss. Boosted by the point-based learning mechanisms\ndiscussed above, researchers pay more attention to designing\npoint-based completion methods. PCN (Yuan et al. 2018) is\nthe ﬁrst learning-based work that adopts an encoder-decoder\narchitecture. It recovers point cloud in a two-stage process\nwhere a coarse result is predicted by MLP and then the\nﬁne result is predicted with folding operation (Yang et al.\n2018) from the coarse one. Along this line, more meth-\nods (Xiang et al. 2021; Zhou et al. 2022; Yan et al. 2022;\nTang et al. 2022; Wang et al. 2022; Li et al. 2023; Chen\net al. 2023) spring up and yield impressive results with the\nhelp of more generation stages, better feature extraction,\nor structured generation process. As discussed before, ex-\nplicit cross-resolution aggregation is another key ingredi-\nent behind their success. Following the coarse-to-ﬁne man-\nner, we propose CRA-PCN, and our insight into promoting\npoint cloud completion is to introduce well-designed cross-\nresolution aggregation mechanisms.\nMethod\nIn this section, we will ﬁrst elaborate on the details of Cross-\nResolution Transformer. Then, we will illustrate the archi-\ntecture of CRA-PCN in detail. Lastly, we will introduce the\nloss function used for training.\nInter-level Cross-Resolution Transformer\nAs shown in Fig. 2(a), the purpose of Cross-Resolution\nTransformer (CRT for short) is to aggregate features of the\nsupport point cloud (green) for the query point cloud (blue)\non multiple scales. The query point cloud is the current point\ncloud to be up-sampled, and the support one was gener-\nated in the early generation stage; therefore, the query one\nhas a higher resolution than the support one. We design\nCRT in a recursive manner, which means it can work on\narbitrary-number scales and the computational complexity\nis bounded.\nGiven coordinates Pq 2 RNq\u00023 and the correspond-\ning features Fq 2 RNq\u0002D of the query point cloud with\nPs 2RNs\u00023 and Fs 2RNs\u0002D of the support one, we ﬁrst\nadopt hierarchical down-sampling to obtain their subsets, re-\nspectively. The number of subsets, namely number of scales,\nis m. The subsets of original coordinates and features are\ndenoted as fPl\nqgm\u00001\nl=0 , fFl\nqgm\u00001\nl=0 , fPl\nsgm\u00001\nl=0 , and fFl\nsgm\u00001\nl=0 ,\nlike shown in Fig. 2(a). Note that the index 0 corresponds to\nthe original point cloud. Then, we exploit the local attention\nmechanism (Zhao et al. 2021) to achieve cross-resolution\naggregation on the (m\u00001)-th scale. The attention relations\nin the neighborhood of each query point can be calculated as\nfollows:\n^am\u00001\nij = \u000bm\u00001(qm\u00001\ni \u0000km\u00001\nj + \u000em\u00001\nij ); (1)\nwhere \u000bm\u00001 is a non-linear projection function imple-\nmented with multi-layer perceptron (MLP). Letpm\u00001\nq;i be the\ni-th coordinate of Pm\u00001\nq , we use knn algorithm to search\nthe indexes of its neighborhood in the support point cloud,\nwhich is denoted as idx(pm\u00001\nq;i ) = fj 2 knn(pm\u00001\nq;i )g.\n\u000em\u00001\nij 2RD is the position encoding generated by subtrac-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4678\nE\n S\n U\n U\n U\nmerge\n&\nFPS\nPartial Complete\nE\n S\n UEncoder Seed generator Up-sampling block\nDeconv\nIntra-level\nCRT\nInter-level\nCRT\nMLP\n…\n…\nMLP\ni\ni 1i+ duplicate\ni\ni\n1i−\nf\n cat\n0 1 2 3 iN\n0 1 2 3 iN\nper-point feature\n…\nir×\ni\n(a) Overview of CRA-PCN (b) Details of up-sampling block  \n sd 1 2 3\n0f\n,pp\n0 1sd\nmerge\n&\nFPS\ncat\nduplicate\nFigure 3: (a) The overall architecture of CRA-PCN, which consists of encoder, seed generator, and stacked up-sampling blocks.\n(b) The details of the up-sampling block, which is composed of MLP, inter-level Cross-Resolution Transformer, intra-level\nCross-Resolution Transformer, and deconvolution.\ntion between pm\u00001\nq;i and pm\u00001\ns;j followed by a MLP. qm\u00001\ni\nis the query vector projected from the i-th feature vector\nof Fm\u00001\nq via linear projection, and km\u00001\nj is projected from\nthe corresponding feature vector of pm\u00001\ns;j (i.e., fm\u00001\ns;j ). For\nall j 2 idx(pm\u00001\nq;i ), we normalize ^am\u00001\nij for am\u00001\nij with\nchannel-wise softmax function and aggregate local features:\nhm\u00001\ni =\nX\nj2idx(pm\u00001\nq;i )\nam\u00001\nij \f(vm\u00001\nj + \u000em\u00001\nij ): (2)\nHere, hm\u00001\ni indicates the i-th feature of Fm\u00001\nh , as shown\nin Fig. 2(a), and vm\u00001\nj is projected from fm\u00001\ns;j via linear\nprojection, like km\u00001\nj . Next, we adopt feature interpolation\n(Qi et al. 2017b) to map Fm\u00001\nh onto the coordinates Pm\u00002\nq\nand Pm\u00002\ns , and the two interpolated features are concate-\nnated with Fm\u00002\nq and Fm\u00002\ns followed by MLPs for feature\nfusion, respectively. The two enhanced features are passed\nthrough the aforementioned attention mechanism forFm\u00002\nh .\nThe process is iterated until we obtain F0\nh, which is the out-\nput of CRT.\nThe common cross-resolution aggregation operations\nonly work on the 0-th scale, i.e., the bottom scale shown\nin Fig. 2. We argue that these single-scale designs restrict\nthe modeling power, leading to unreasonable predictions.\nWhile several methods (Qian et al. 2021; He et al. 2023)\nadopt stacked feature extraction layers with dense connec-\ntions among these layers to capture multi-scale features, the\ncomputational complexities of them are not bounded as the\nnumber of layers grows, and the receptive ﬁelds grow slowly\nwithout explicit hierarchical down-sampling.\nIntra-level Cross-Resolution Transformer\nWith the help of uniﬁed implementation, let the query point\ncloud and support one be the same, inter-level CRT degrades\nto the intra-level one. In this case, the cross-resolution aggre-\ngation is decomposed into self-attention, interpolation, and\nfeature fusion, as illustrated in Fig. 2(b).\nInter-level CRT plays an important role in preserving\nlearned features and capturing multi-scale features. How-\never, the generated point cloud with lower resolution is not\nnecessarily a subset of the current one, and the local de-\ntails of them are sometimes different due to irregular out-\nliers or shrinkages caused by unreasonable coordinate pre-\ndictions. Therefore, only using inter-level CRTs is not sufﬁ-\ncient for ﬁnding ﬁne generation patterns that ﬁt local regions\nbest. To alleviate this problem, we introduce intra-level CRT\nand integrate both into an up-sampling block, like shown in\nFig. 3(b). Note that the combination of inter- and intra-level\nCRT is not an incremental improvement. The experiments\ndemonstrate that their combination outperforms the variants\nonly using two consecutive intra- or inter-level CRTs by a\nlarge margin with the same number of parameters.\nCRA-PCN\nThe overall architecture of CRA-PCN is illustrated in\nFig. 3(a), which consists of encoder, seed generator, and de-\ncoder. The decoder consists of three up-sampling blocks.\nEncoder and seed generator. The purpose of the en-\ncoder is to extract a shape vector and per-point features of\npooled partial input. The input of the encoder is the par-\ntial point cloud P 2RN\u00023. With hierarchical aggrega-\ntion and down-sampling followed by a max-pooling oper-\nation, we can obtain the shape vector f 2RC and down-\nsampled partial points Pp 2RNp\u00023 and corresponding fea-\ntures Fp 2RNp\u0002Cp. Speciﬁcally, we adopt three layers of\nset abstraction (Qi et al. 2017b) (SA layer) to down-sample\npoint set and aggregate local features; moreover, we insert\nan intra-level CRT between two consecutive SA layers to\nenrich semantic contexts.\nThe seed generator aims to produce a representation with\ncoordinates and features representing a sketch point cloud\nwith low resolution yet complete shape, which is named\nseed. To leverage the impressive detail-preserving ability of\nUpsample Transformer (Zhou et al. 2022) for seed genera-\ntion, we feed Pp and Fp into Upsample Transformer with-\nout softmax activation. The seed generation phase can be\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4679\nMethods Average Plane Cabinet Car Chair Lamp Couch Table Boat\nFoldingNet (Yang et al. 2018) 14.31 9.49 15.80 12.61 15.55 16.41 15.97 13.65 14.99\nTopNet (Tchapmi et al. 2019) 12.15 7.61 13.31 10.90 13.82 14.44 14.78 11.22 11.12\nPCN (Yuan et al. 2018) 9.64 5.50 22.70 10.63 8.70 11.00 11.34 11.68 8.59\nGRNet (Xie et al. 2020) 8.83 6.45 10.37 9.45 9.41 7.96 10.51 8.44 8.04\nPoinTr (Yu et al. 2021) 8.38 4.75 10.47 8.68 9.39 7.75 10.93 7.78 7.29\nSnowﬂakeNet (Xiang et al. 2021) 7.21 4.29 9.16 8.08 7.89 6.07 9.23 6.55 6.40\nFBNet (Yan et al. 2022) 6.94 3.99 9.05 7.90 7.38 5.82 8.85 6.35 6.18\nProxyFormer (Li et al. 2023) 6.77 4.01 9.01 7.88 7.11 5.35 8.77 6.03 5.98\nSeedFormer (Zhou et al. 2022) 6.74 3.85 9.05 8.06 7.06 5.21 8.85 6.05 5.85\nAnchorFormer (Chen et al. 2023) 6.59 3.70 8.94 7.57 7.05 5.21 8.40 6.03 5.81\nOurs 6.39 3.59 8.70 7.50 6.70 5.06 8.24 5.72 5.64\nTable 1: Results on PCN dataset in terms of L1 Chamfer Distance \u0002103 (lower is better).\nPCN GRNet SnowflakeNet SeedFormer PointMSTrans GTPartial input\nChair Lamp Table Plane\nOurs GTSeedFormerSnowflakeNetGRNetPCNPartial input\nPlaneTableLampChair\nPCN GRNet SnowflakeNet SeedFormer PointMSTrans GTPartial input\nChair Lamp Table Plane\nPCN GRNet SnowflakeNet SeedFormer PointMSTrans GTPartial input\nChair Lamp Table Plane\nPCN GRNet SnowflakeNet SeedFormer PointMSTrans GTPartial input\nChair Lamp Table Plane\nPCN GRNet SnowflakeNet SeedFormer PointMSTrans GTPartial input\nChair Lamp Table Plane\nPCN GRNet SnowflakeNet SeedFormer PointMSTrans GTPartial input\nChair Lamp Table Plane\nPCN GRNet SnowflakeNet SeedFormer PointMSTrans GTPartial input\nChair Lamp Table Plane\nFigure 4: Visual comparison on PCN dataset.\nformulated as follows:\nFsd = UpTrans(Pp;Fp);Psd = MLP(cat(Fsd;f)); (3)\nwhere Fsd 2RNsd\u0002D is the set of seed features that carry\nthe local properties of seed points, and UpTrans indicates\nUpsample Transformer. We concatenateFsd with f and then\nfeed the concatenated features into a MLP for seed coor-\ndinates Psd 2RNsd\u00023. For better optimization, following\nprior methods (Xiang et al. 2021; Wang, Ang Jr, and Lee\n2020), we merge partial input Pwith Psd and down-sample\nthe merged set to obtain the starting points P0 2RN0\u00023.\nDecoder. The objective of the decoder is to gradually re-\ncover the complete shape from starting points P0. Our de-\ncoder consists of three up-sampling blocks. Given starting\ncoordinates P0, the decoder will gradually produce P1, P2,\nand P3, where Pi 2RNi\u00023 is the input of the i-th block and\nNi+1 = Ni \u0002ri for each i< 2.\nUp-sampling block. As illustrated in Fig 3(b), each block\nis mainly composed of a mini-PointNet (Qi et al. 2017a),\ninter-level CRT, intra-level CRT, and deconvolution (Xiang\net al. 2021). The i-th block can upsample current point set\nPi at the rate of ri. With shape vector f, we ﬁrst adopt a\nmini-PointNet (Qi et al. 2017a) to learn per-point features\nHi 2RNi\u0002D. Then, let Pi\u00001 be the coordinates of the sup-\nport point cloud with corresponding features Fi\u00001 and Pi\nbe the coordinates of the query one with corresponding fea-\ntures Hi, we employ inter-level CRT to extract features (i.e.,\nGi 2RNi\u0002D) in the cross-resolution manner. Note that, in\nthe case of i = 0, Pi\u00001 = Psd and Fi\u00001 = Fsd. Next, Gi\nand Pi are fed into intra-level CRT to capture internal multi-\nscale geometric features and obtain feature Fi. Finally, with\nFi, we use deconvolution to predict ri offsets for each point\nand subsequently obtain up-sampled coordinatesPi+1. Note\nthat, Fi and Pi are sent to the next up-sampling block via\nskip connection.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4680\nMethods Table Chair Air\nplane Car Sofa Bird\nhouse Bag Remote Key\nboard Rocket CD-S CD-M CD-H CD-Avg\nFoldingNet 2.53 2.81 1.43 1.98 2.48 4.71 2.79 1.44 1.24 1.48 2.67 2.66 4.05 3.12\nPCN 2.13 2.29 1.02 1.85 2.06 4.50 2.86 1.33 0.89 1.32 1.94 1.96 4.08 2.66\nTopNet 2.21 2.53 1.14 2.28 2.36 4.83 2.93 1.49 0.95 1.32 2.26 2.16 4.30 2.91\nPFNet 3.95 4.24 1.81 2.53 3.34 6.21 4.96 2.91 1.29 2.36 3.83 3.87 7.97 5.22\nGRNet 1.63 1.88 1.02 1.64 1.72 2.97 2.06 1.09 0.89 1.03 1.35 1.71 2.85 1.97\nPoinTr 0.81 0.95 0.44 0.91 0.79 1.86 0.93 0.53 0.38 0.57 0.58 0.88 1.79 1.09\nSeedFormer 0.72 0.81 0.40 0.89 0.71 1.51 0.79 0.46 0.36 0.50 0.50 0.77 1.49 0.92\nOurs 0.66 0.74 0.37 0.85 0.66 1.36 0.73 0.43 0.35 0.50 0.48 0.71 1.37 0.85\nTable 2: Results on ShapeNet-55 in terms of L2 Chamfer Distance \u0002103 (lower is better).\nLoss Function\nWe use Chamfer Distance (CD) (Fan, Su, and Guibas\n2017) of the L1-norm as the primary component of our\nloss function. We supervise all predicted point sets P =\nfPs;P1;P2;P3gwith ground truth Pt 2RNt\u00023, and the\nloss function Lcan be written as:\nL=\nX\nPg2P\nCD(Pg;FPS(Pt;Ng)): (4)\nHere, we use FPS algorithm (Qi et al. 2017b) to down-\nsample Pt to the same density (i.e., Ng \u00023) as with Pg.\nExperiments\nIn this section, we ﬁrst conduct extensive experiments to\ndemonstrate the effectiveness of our model on three widely\nused benchmark datasets including PCN (Yuan et al. 2018),\nShapeNet-55/34 (Yu et al. 2021) and MVP (Pan et al.\n2021b). Then, we conduct the ablation studies on PCN\ndataset to analyze different designs of our model.\nTraining Setup\nWe implement CRA-PCN with Pytorch (Paszke et al. 2019).\nAll models are trained on two NVIDIA Tesla V100 graphic\ncards with a batch size of 72. We use Adam optimizer\n(Kingma and Ba 2014) with \f1 = 0:9 and \f2 = 0:999.\nWe set the feature dim Din the decoder to 128. The initial\nlearning rate is set to 0.001 with continuous decay of 0.1\nfor every 100 epochs. We train our model for 300 epochs\non PCN dataset (Yuan et al. 2018) and ShapeNet-55/34 (Yu\net al. 2021) while 50 epochs on MVP dataset (Pan et al.\n2021b).\nEvaluation on PCN Dataset\nDataset and metric. The PCN dataset (Yuan et al. 2018)\nis the subset of ShapeNet dataset (Chang et al. 2015) and\nit consists of 30,974 shapes from 8 categories. Each ground\ntruth point cloud is generated by evenly sampling 16,384\npoints on the mesh surface. The partial inputs of 2,048 points\nare generated by back-projecting 2.5D depth images into\n3D. For a fair comparison, we follow the standard evaluation\nmetric in (Yuan et al. 2018), and all results on PCN dataset\nare evaluated in terms of L1 Chamfer Distance (CD-`1).\nQuantitative results. In Tab. 1, we report the quantita-\ntive results of our CRA-PCN and other methods on PCN\ndataset. We see that CRA-PCN achieves the best perfor-\nmance over all previous methods in all categories. Compared\nto the second-ranked AnchorFormer, our method reduces the\naverage CD-`1 by 0:20, which is 3:0% lower than Anchor-\nFormer. While several previous methods (Xiang et al. 2021;\nZhou et al. 2022; Yan et al. 2022) in Tab. 1 also use a similar\ncoarse-to-ﬁne architecture, CRA-PCN outperforms them by\na large margin with the help of several novel designs.\nQualitative results. In Fig. 4, we visually compare CRA-\nPCN with previous state-of-the-art methods in four selected\ncategories (Plane, Table, Lamp, and Chair). The visual re-\nsults show that CRA-PCN can predict smoother surfaces and\nproduce less noise. Speciﬁcally, inTablecategory in the sec-\nond row, the visible seat predicted by CRA-PCN has smooth\ndetails, while other methods generate many noise points. As\nfor Lamp category in the third row, all other methods fail to\nreconstruct the missing spherical surface.\nEvaluation on ShapeNet-55/34\nDataset and metric. To further evaluate the generalization\nability of our model, we conduct experiments on ShapeNet-\n55 and ShapeNet-34 (Yu et al. 2021). Like PCN dataset,\nShapeNet-55 and ShapeNet-34 are derived from ShapeNet,\nbut contain all the objects in ShapeNet from 55 categories.\nSpeciﬁcally, ShapeNet-55 contains 41,952 models for train-\ning and 10,518 models for testing, while the training set\nof ShapeNet-34 contains 46,765 objects from 34 categories\nand its testing set consists of 3,400 objects from 34 seen\ncategories and 2,305 objects from the remaining 21 novel\ncategories. Using the training and evaluation protocol pro-\nposed in (Yu et al. 2021), partial inputs are generated on-\nline. Speciﬁcally, partial point clouds are generated by se-\nlecting certain viewpoints and removing n percentage far-\nthest points of complete shapes, where viewpoints are ran-\ndomly selected during training and ﬁxed during evaluation.\nDuring the evaluation, nis set to 25%, 50%, and 75%, re-\nspectively, corresponding to three difﬁculty degrees, namely\nsimple, moderate and hard. Following (Yu et al. 2021), we\nevaluate the performance of the methods (Yang et al. 2018;\nYuan et al. 2018; Tchapmi et al. 2019; Huang et al. 2020;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4681\n34 seen catagories 21 unseen categories\nMethods CD-S CD-M CD-H CD-Avg CD-S CD-M CD-H CD-Avg\nFoldingNet (Yang et al. 2018) 1.86 1.81 3.38 2.35 2.76 2.74 5.36 3.62\nPCN (Yuan et al. 2018) 1.87 1.81 2.97 2.22 3.17 3.08 5.29 3.85\nTopNet (Tchapmi et al. 2019) 1.77 1.61 3.54 2.31 2.62 2.43 5.44 3.50\nPFNet (Huang et al. 2020) 3.16 3.19 7.71 4.68 5.29 5.87 13.33 8.16\nGRNet (Xie et al. 2020) 1.26 1.39 2.57 1.74 1.85 2.25 4.87 2.99\nPoinTr (Yu et al. 2021) 0.76 1.05 1.88 1.23 1.04 1.67 3.44 2.05\nSeedFormer (Zhou et al. 2022) 0.48 0.70 1.30 0.83 0.61 1.07 2.35 1.34\nOurs 0.45 0.65 1.18 0.76 0.55 0.97 2.19 1.24\nTable 3: Results on ShapeNet-34 in terms of L2 Chamfer Distance \u0002103 (lower is better).\nMethods CD-`2 # F-Score%\"\nPCN (Y\nuan et al. 2018) 9.77 0.320\nT\nopNet (Tchapmi et al. 2019) 10.11 0.308\nMSN\n(Liu et al. 2020) 7.90 0.432\nCDN\n(Wang, Ang Jr, and Lee 2020) 7.25 0.434\nECG\n(Pan 2020) 6.64 0.476\nVRCNet\n(Pan et al. 2021a) 5.96 0.499\nOurs 5.33 0.529\nTable 4: Results on MVP dataset in terms of L2 Chamfer\nDistance \u0002104 (lower is better) and F-Score (higher is bet-\nter).\nXie et al. 2020; Yu et al. 2021; Zhou et al. 2022) in terms of\nCD-`2 under the above three difﬁculties, and we also report\nthe average CD.\nResults on ShapeNet-55. The last four columns of Tab. 2\nshow the superior performance of CRA-PCN under various\nsituations with diverse viewpoints and diverse incomplete\npatterns. Besides, we sample 10 categories and report CD-\n`2 of them. Categories in the columns from 2 to 6 contain\nsufﬁcient samples, while the quantities of the following ﬁve\ncategories are insufﬁcient. According to the results, we can\nclearly see the powerful generalization ability of our model.\nResults on ShapeNet-34. As shown in Tab. 3, our method\nachieves the best CD-` 2 in the 34 seen categories of\nShapeNet-34. Moreover, as for the 21 unseen categories that\ndo not appear in training phase, our method outperforms\nother competitors. Especially, we achieve 6:8% improve-\nment compared with the second-ranked SeedFormer under\nhard setting in 21 unseen categories, which justiﬁes the ef-\nfectiveness of our method.\nEvaluation on MVP Dataset\nDataset and metric. The MVP dataset (Pan et al. 2021b)\nis a multi-view partial point cloud dataset which consists of\n16 categories of high-quality partial/complete point clouds.\nFor each complete shape, 26 partial point clouds are gener-\nated by selecting 26 camera poses which are uniformly dis-\ntributed on a unit sphere. MVP dataset provides various res-\nMethods Latency # Memory # CD \u0002103\nFBNet 0.905 s\n3421 MB 6.94\nSeedFormer 0.297 s\n10701 MB 6.74\nAnchorFormer 0.608 s\n3735 MB 6.59\nOurs 0.469 s\n4459 MB 6.39\nTable 5: Run-time memory usage and latency, which were\nevaluated on a single GTX 1080Ti graphic card with a batch\nsize of 32.\nVariations Inter-le\nvel Intra-level CD-`1\nA 7.85\nB X 6.92\nC X 6.80\nD \u00022 6.74\nE \u00022 6.61\nF second ﬁrst 6.54\nG X X 6.39\nTable 6: Analysis of inter- and intra-level Cross-Resolution\nTransformer in up-sampling block.\nVariations inter-le\nvel intra-level CD-`1\nA m= 1 m=\n1 6.75\nB m= 1 m=\n3 6.50\nC m= 3 m=\n1 6.51\nD m= 3 m=\n3 6.39\nTable 7: Analysis of the number of scales in Cross-\nResolution Transformer.\nolutions of ground truth, and we choose 2,048 among them.\nIn training phase, we follow the way of data splitting in (Pan\net al. 2021a). In evaluation phase, we evaluate performances\nin terms of CD-`2 and F-Score (Tatarchenko et al. 2019).\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4682\nResults. We report the results in Tab. 4 and our method\noutperforms all competitors. Compared with VRCNet (Pan\net al. 2021a), our method reduces CD-`2 by 0:63 and im-\nproves F-Score by 0:03, which demonstrates that CRA-PCN\nnot only predicts the complete shape but also preserves the\ndetails of partial input.\nAccuracy-Complexity Trade-Offs\nWe report the run-time memory usage and latency in Tab. 5.\nAll methods were evaluated on a single Nvidia GeForce\nGTX 1080Ti graphic card with a batch size of 32. For fair\ncomparisons, we disable gradient calculation and use point\nclouds with a resolution of 2,048. From the results, we see\nCRA-PCN can achieve better trade-offs than prior methods.\nAblation Studies\nHere, we present ablation studies demonstrating the effec-\ntiveness of several proposed operations. All experiments are\nconducted under uniﬁed settings on the PCN dataset.\nAnalysis of Cross-Resolution Transformers. Tab. 6\nsummarizes the evaluation results of inter- and intra-level\nCross-Resolution Transformers, where model G is our CRA-\nPCN. We remove both inter- and intra-level CRT (model A),\nand we ﬁnd the performance drops a lot, which justiﬁes the\neffectiveness of cross-resolution aggregation. Then we re-\nmove inter-level CRT (model B) or intra-level CRT (model\nC), respectively. To keep the number of parameters the same\nwith CRA-PCN, we double the number of intra-level CRT\n(model D) or inter-level CRT (model E). However, the afore-\nmentioned four variations exhibit poor performances com-\npared to model G, and the results highlight the importance of\nthe combination of intra- and inter-level cross-resolution ag-\ngregation. Moreover, we switch the order of inter- and intra-\nlevel CRT (model F), and we can ﬁnd the performance drops\na little. The empirical results here justify the effectiveness of\nCross-Resolution Transformer and the combination of inter-\nand intra-level CRA.\nAnalysis of recursive/multi-scale designs. To justify the\nadvantages of our recursive designs on multiple scales for\npoint cloud completion, we conduct ablation study and re-\nport the results in Tab. 7. Model D is our CRA-PCN, and we\nset the number of scales mto 3. Note that, if we set mto 1,\nCross-Resolution Transformer will degrade to point trans-\nformer (Zhao et al. 2021) which can only achieve single-\nscale cross-resolution aggregation. We replace all multi-\nscale CRTs with single-scale ones (model A) by setting both\nmto 1, and we can clearly ﬁnd the performance drops a lot.\nWe separately set mof inter-level CRT and intra-level CRT\nto 1 (model B and C), they increase CD-`1 by0:11 and 0:12,\nrespectively. The results of these three variations conﬁrm the\neffectiveness of our recursive and multi-scale designs.\nConclusion\nIn this paper, focusing on explicit cross-resolution aggre-\ngation, we present Cross-Resolution Transformer that ef-\nﬁciently performs cross-resolution and multi-scale feature\naggregation. Moreover, we propose a combination of intra-\nand inter-level cross-resolution aggregation with the uni-\nﬁed designs of Cross-Resolution Transformer. Based on the\naforementioned techniques, we propose a novel deep net-\nwork for point cloud completion, named CRA-PCN, which\nadopts an encoder-decoder architecture. Extensive experi-\nments demonstrate the superiority of our method. It will be\nan interesting future direction to extend our work to similar\ntasks such as point cloud reconstruction and up-sampling.\nAcknowledgments\nThis work was supported by the National Natural Science\nFoundation of China (Grant No. 62372223).\nReferences\nCadena, C.; Carlone, L.; Carrillo, H.; Latif, Y .; Scaramuzza,\nD.; Neira, J.; Reid, I.; and Leonard, J. J. 2016. Past, present,\nand future of simultaneous localization and mapping: To-\nward the robust-perception age. IEEE Transactions on\nrobotics, 32(6): 1309–1332.\nChang, A. X.; Funkhouser, T.; Guibas, L.; Hanrahan, P.;\nHuang, Q.; Li, Z.; Savarese, S.; Savva, M.; Song, S.; Su,\nH.; et al. 2015. Shapenet: An information-rich 3d model\nrepository. arXiv preprint arXiv:1512.03012.\nChen, X.; Ma, H.; Wan, J.; Li, B.; and Xia, T. 2017. Multi-\nview 3d object detection network for autonomous driving.\nIn Proceedings of the IEEE conference on Computer Vision\nand Pattern Recognition, 1907–1915.\nChen, Z.; Long, F.; Qiu, Z.; Yao, T.; Zhou, W.; Luo, J.; and\nMei, T. 2023. AnchorFormer: Point Cloud Completion From\nDiscriminative Nodes. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n13581–13590.\nChoy, C.; Gwak, J.; and Savarese, S. 2019. 4d spatio-\ntemporal convnets: Minkowski convolutional neural net-\nworks. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, 3075–3084.\nChoy, C. B.; Xu, D.; Gwak, J.; Chen, K.; and Savarese, S.\n2016. 3d-r2n2: A uniﬁed approach for single and multi-\nview 3d object reconstruction. In Computer Vision–ECCV\n2016: 14th European Conference, Amsterdam, The Nether-\nlands, October 11-14, 2016, Proceedings, Part VIII 14, 628–\n644. Springer.\nFan, H.; Su, H.; and Guibas, L. J. 2017. A point set gen-\neration network for 3d object reconstruction from a single\nimage. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 605–613.\nGirdhar, R.; Fouhey, D. F.; Rodriguez, M.; and Gupta, A.\n2016. Learning a predictable and generative vector rep-\nresentation for objects. In Computer Vision–ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands,\nOctober 11-14, 2016, Proceedings, Part VI 14, 484–499.\nSpringer.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.;\nand Hu, S.-M. 2021. Pct: Point cloud transformer. Compu-\ntational Visual Media, 7: 187–199.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4683\nHan, X.; Li, Z.; Huang, H.; Kalogerakis, E.; and Yu, Y .\n2017. High-resolution shape completion using deep neu-\nral networks for global structure and local geometry infer-\nence. In Proceedings of the IEEE international conference\non computer vision, 85–93.\nHe, Y .; Tang, D.; Zhang, Y .; Xue, X.; and Fu, Y . 2023. Grad-\nPU: Arbitrary-Scale Point Cloud Upsampling via Gradient\nDescent with Learned Distance Functions. InProceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5354–5363.\nHuang, Z.; Yu, Y .; Xu, J.; Ni, F.; and Le, X. 2020. Pf-net:\nPoint fractal network for 3d point cloud completion. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, 7662–7670.\nKingma, D. P.; and Ba, J. 2014. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980.\nLang, A. H.; V ora, S.; Caesar, H.; Zhou, L.; Yang, J.; and\nBeijbom, O. 2019. Pointpillars: Fast encoders for object de-\ntection from point clouds. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition ,\n12697–12705.\nLi, B.; Zhang, T.; and Xia, T. 2016. Vehicle detection from\n3d lidar using fully convolutional network. arXiv preprint\narXiv:1608.07916.\nLi, S.; Gao, P.; Tan, X.; and Wei, M. 2023. ProxyFormer:\nProxy Alignment Assisted Point Cloud Completion with\nMissing Part Sensitive Transformer. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9466–9475.\nLiu, M.; Sheng, L.; Yang, S.; Shao, J.; and Hu, S.-M. 2020.\nMorphing and sampling network for dense point cloud com-\npletion. In Proceedings of the AAAI conference on artiﬁcial\nintelligence, volume 34, 11596–11603.\nMaturana, D.; and Scherer, S. 2015. V oxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn 2015 IEEE/RSJ international conference on intelligent\nrobots and systems (IROS), 922–928. IEEE.\nPan, L. 2020. ECG: Edge-aware point cloud completion\nwith graph convolution.IEEE Robotics and Automation Let-\nters, 5(3): 4392–4398.\nPan, L.; Chen, X.; Cai, Z.; Zhang, J.; Zhao, H.; Yi, S.; and\nLiu, Z. 2021a. Variational relational point completion net-\nwork. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, 8524–8533.\nPan, L.; Wu, T.; Cai, Z.; Liu, Z.; Yu, X.; Rao, Y .; Lu,\nJ.; Zhou, J.; Xu, M.; Luo, X.; et al. 2021b. Multi-\nview partial (mvp) point cloud challenge 2021 on comple-\ntion and registration: Methods and results. arXiv preprint\narXiv:2112.12053.\nPark, C.; Jeong, Y .; Cho, M.; and Park, J. 2022. Fast point\ntransformer. InProceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 16949–16958.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information pro-\ncessing systems, 32.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet:\nDeep learning on point sets for 3d classiﬁcation and segmen-\ntation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 652–660.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. Advances in neural information processing\nsystems, 30.\nQian, G.; Abualshour, A.; Li, G.; Thabet, A.; and Ghanem,\nB. 2021. Pu-gcn: Point cloud upsampling using graph\nconvolutional networks. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n11683–11692.\nReddy, N. D.; V o, M.; and Narasimhan, S. G. 2018. Car-\nfusion: Combining point tracking and part detection for dy-\nnamic 3d reconstruction of vehicles. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 1906–1915.\nRiegler, G.; Osman Ulusoy, A.; and Geiger, A. 2017. Oct-\nnet: Learning deep 3d representations at high resolutions. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 3577–3586.\nRusu, R. B.; Marton, Z. C.; Blodow, N.; Dolha, M.; and\nBeetz, M. 2008. Towards 3D point cloud based object maps\nfor household environments. Robotics and Autonomous Sys-\ntems, 56(11): 927–941.\nSimonovsky, M.; and Komodakis, N. 2017. Dynamic\nedge-conditioned ﬁlters in convolutional neural networks on\ngraphs. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 3693–3702.\nSong, S.; Yu, F.; Zeng, A.; Chang, A. X.; Savva, M.; and\nFunkhouser, T. 2017. Semantic scene completion from a\nsingle depth image. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 1746–1754.\nTang, J.; Gong, Z.; Yi, R.; Xie, Y .; and Ma, L. 2022. LAKe-\nNet: topology-aware point cloud completion by localizing\naligned keypoints. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 1726–\n1735.\nTatarchenko, M.; Richter, S. R.; Ranftl, R.; Li, Z.; Koltun,\nV .; and Brox, T. 2019. What do single-view 3d reconstruc-\ntion networks learn? In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 3405–\n3414.\nTchapmi, L. P.; Kosaraju, V .; Rezatoﬁghi, H.; Reid, I.; and\nSavarese, S. 2019. Topnet: Structural point cloud decoder.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 383–392.\nThomas, H.; Qi, C. R.; Deschaud, J.-E.; Marcotegui, B.;\nGoulette, F.; and Guibas, L. J. 2019. Kpconv: Flexible and\ndeformable convolution for point clouds. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n6411–6420.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4684\nWang, X.; Ang Jr, M. H.; and Lee, G. H. 2020. Cascaded\nreﬁnement network for point cloud completion. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 790–799.\nWang, Y .; Sun, Y .; Liu, Z.; Sarma, S. E.; Bronstein, M. M.;\nand Solomon, J. M. 2019. Dynamic graph cnn for learn-\ning on point clouds. Acm Transactions On Graphics (tog),\n38(5): 1–12.\nWang, Y .; Tan, D. J.; Navab, N.; and Tombari, F. 2022.\nLearning local displacements for point cloud completion. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 1568–1577.\nWen, X.; Li, T.; Han, Z.; and Liu, Y .-S. 2020. Point cloud\ncompletion by skip-attention network with hierarchical fold-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 1939–1948.\nWen, X.; Xiang, P.; Han, Z.; Cao, Y .-P.; Wan, P.; Zheng, W.;\nand Liu, Y .-S. 2021. Pmp-net: Point cloud completion by\nlearning multi-step point moving paths. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 7443–7452.\nWu, W.; Qi, Z.; and Fuxin, L. 2019. Pointconv: Deep con-\nvolutional networks on 3d point clouds. In Proceedings of\nthe IEEE/CVF Conference on computer vision and pattern\nrecognition, 9621–9630.\nXiang, P.; Wen, X.; Liu, Y .-S.; Cao, Y .-P.; Wan, P.; Zheng,\nW.; and Han, Z. 2021. Snowﬂakenet: Point cloud completion\nby snowﬂake point deconvolution with skip-transformer. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, 5499–5509.\nXie, H.; Yao, H.; Zhou, S.; Mao, J.; Zhang, S.; and Sun,\nW. 2020. Grnet: Gridding residual network for dense point\ncloud completion. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part IX, 365–381. Springer.\nXu, Y .; Fan, T.; Xu, M.; Zeng, L.; and Qiao, Y . 2018. Spider-\ncnn: Deep learning on point sets with parameterized convo-\nlutional ﬁlters. In Proceedings of the European conference\non computer vision (ECCV), 87–102.\nYan, X.; Yan, H.; Wang, J.; Du, H.; Wu, Z.; Xie, D.; Pu, S.;\nand Lu, L. 2022. Fbnet: Feedback network for point cloud\ncompletion. In Computer Vision–ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part II, 676–693. Springer.\nYang, Y .; Feng, C.; Shen, Y .; and Tian, D. 2018. Foldingnet:\nPoint cloud auto-encoder via deep grid deformation. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 206–215.\nYifan, W.; Wu, S.; Huang, H.; Cohen-Or, D.; and Sorkine-\nHornung, O. 2019. Patch-based progressive 3d point set up-\nsampling. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 5958–5967.\nYu, X.; Rao, Y .; Wang, Z.; Liu, Z.; Lu, J.; and Zhou, J. 2021.\nPointr: Diverse point cloud completion with geometry-\naware transformers. In Proceedings of the IEEE/CVF in-\nternational conference on computer vision, 12498–12507.\nYuan, W.; Khot, T.; Held, D.; Mertz, C.; and Hebert, M.\n2018. Pcn: Point completion network. In 2018 international\nconference on 3D vision (3DV), 728–737. IEEE.\nZhao, H.; Jiang, L.; Fu, C.-W.; and Jia, J. 2019. Pointweb:\nEnhancing local neighborhood features for point cloud pro-\ncessing. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 5565–5573.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P. H.; and Koltun, V . 2021.\nPoint transformer. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, 16259–16268.\nZhou, H.; Cao, Y .; Chu, W.; Zhu, J.; Lu, T.; Tai, Y .; and\nWang, C. 2022. Seedformer: Patch seeds based point\ncloud completion with upsample transformer. In Computer\nVision–ECCV 2022: 17th European Conference, Tel Aviv, Is-\nrael, October 23–27, 2022, Proceedings, Part III, 416–432.\nSpringer.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4685",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5363345146179199
    },
    {
      "name": "Point cloud",
      "score": 0.4565041661262512
    },
    {
      "name": "Computer science",
      "score": 0.3823355734348297
    },
    {
      "name": "Electrical engineering",
      "score": 0.1433945894241333
    },
    {
      "name": "Engineering",
      "score": 0.13070502877235413
    },
    {
      "name": "Artificial intelligence",
      "score": 0.1300709843635559
    },
    {
      "name": "Voltage",
      "score": 0.044227540493011475
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I881766915",
      "name": "Nanjing University",
      "country": "CN"
    }
  ]
}