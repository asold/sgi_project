{
  "title": "UniCase -- Rethinking Casing in Language Models",
  "url": "https://openalex.org/W3094522776",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5046898079",
      "name": "Rafał Powalski",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5054685387",
      "name": "Tomasz Stanisławek",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W3021778780",
    "https://openalex.org/W3047555776",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2997200074"
  ],
  "abstract": "In this paper, we introduce a new approach to dealing with the problem of case-sensitiveness in Language Modelling (LM). We propose simple architecture modification to the RoBERTa language model, accompanied by a new tokenization strategy, which we named Unified Case LM (UniCase). We tested our solution on the GLUE benchmark, which led to increased performance by 0.42 points. Moreover, we prove that the UniCase model works much better when we have to deal with text data, where all tokens are uppercased (+5.88 point).",
  "full_text": "UniCase - Rethinking Casing in Language Models\nRafał Powalski\nApplica.ai\nrafal.powalski@applica.ai\nTomasz Stanisławek\nApplica.ai\nWarsaw University of Technology\ntomasz.stanislawek@applica.ai\nAbstract\nIn this paper, we introduce a new ap-\nproach to dealing with the problem of case-\nsensitiveness in Language Modelling (LM).\nWe propose simple architecture modiﬁcation\nto the RoBERTa language model, accompa-\nnied by a new tokenization strategy, which\nwe named Uniﬁed Case LM (UniCase). We\ntested our solution on the GLUE benchmark,\nwhich led to increased performance by 0.42\npoints. Moreover, we prove that the UniCase\nmodel works much better when we have to\ndeal with text data, where all tokens are upper-\ncased (+5.88 point).1\n1 Introduction\nMany natural languages in their written form en-\ncode some information in the case of the letter: the\nbeginning of the sentence, proper nouns, headings\nof publication titles, to name a few. People can\nprocess that kind of information in a special way,\nlearning word semantics separately from the case\ninformation. However, state-of-the-art (SOTA) ap-\nproaches for building Language Models do not use\nthis property (Devlin et al., 2019; Liu et al., 2019;\nRaffel et al., 2020; Brown et al., 2020). As an ex-\nample, consider RoBERTa language model, where\nwe have different tokenizer outputs for each word\ncase type (lower, title, upper) and multiple vocabu-\nlary entries for the same word but different cases\n(see Table 1).\nWhereas different approaches to dealing with\ncase-sensitivity issues were proposed and tested\non Machine Translation systems (Etchegoyhen and\nGete, 2020; Shi et al., 2020) where the impact of\nthe casing can be signiﬁcant, there are no similar\nstudies in the context of building Language Models.\nIn fact, most of LMs use cased (with the original\ntext) and uncased (with lowercased text) version\n1Work in progress.\nWord Vocab subtokens (_ means spaces)\nacknowledgement _acknowledgement\nAcknowledgement _A/cknowled/gement\nACKNOWLEDGEMENT _AC/KN/OW/LED/G/EMENT\nother _other\nOther _Other\nOTHER _OTHER\nTable 1: RoBERTa tokenization examples. Acknowl-\nedgement is tokenized differently for each case variant\nand other has redundant entries in the vocabulary.\nof the model (Devlin et al., 2019; Liu et al., 2019;\nConneau et al., 2020).\nIn this paper, we present a model that resolves\nthe problems presented in Table 1. Particularly,\nwe provide two main contributions. Firstly, we\npropose UniCase: a new language model based on\ntransformer architecture with novel tokenization\nstrategy. Our method improves RoBERTa scores\nby 0.42 on the GLUE benchmark (Wang et al.,\n2018). Secondly, we test our new UniCase model\non noisy texts, where true case is unknown (all\ntexts were converted to uppercase or lowercase).\nWe will release all pretrained models in an open,\npublic repository.\n2 Related work\n2.1 Language Modelling\nState-of-the-art approaches (Devlin et al., 2019; Liu\net al., 2019; Raffel et al., 2020; Brown et al., 2020)\nfor building Language Models use Transformer\narchitecture (Vaswani et al., 2017) with BPE (Sen-\nnrich et al., 2016) or Unigram LM-based tokeniza-\ntion methods (Kudo, 2018), where each subtoken\nfrom the vocabulary has only one semantic em-\nbedding in the model. With that architecture there\nare two common approaches for dealing with the\nproblem of case-sensitiveness in Language Mod-\nelling: cased (with original text) (Liu et al., 2019;\nConneau et al., 2020; Raffel et al., 2020; Sun et al.,\narXiv:2010.11936v1  [cs.CL]  22 Oct 2020\n2019) and uncased (with lowercased text) (Devlin\net al., 2019; Iandola et al., 2020; Sanh et al., 2020),\nwhere cased models proved to be more suitable for\nmajority of NLP tasks (Wang et al., 2018, 2020;\nDevlin et al., 2019).\n2.2 Tokenization\nTokenization is a way of splitting a text into to-\nkens, which NLP models use as smallest piece of\ninformation. Over the years researchers have been\nintroducing different approaches to tokenization\nwith three types of tokens as bases: words, charac-\nters and subwords. Subwords are considered to be\nthe most effective one (Rai and Borah, 2021).\nByte-Pair-Encoding (BPE) (Sennrich et al.,\n2016) segmentation balances vocabulary size and\nthe length of a sequence proccessed by the model\nin the single pass. The general idea behind BPE is\nto create the vocabulary by iteratively merging the\nmost frequent pair of characters or subtokens into\na new one. Thus, words with low frequencies in\nthe corpus will be represented as combinations of\nmultiple subtokens or characters. It turns out, that\nthis solution has its own drawbacks, such as lack of\nmultiple segmentations endowed with probabilities,\nand regularization techniques. These two issues\nwere ﬁxed by introducing tokenization based on\nunigram language model, which can produce mul-\ntiple subword segmentations endowed with prob-\nabilities (Kudo, 2018). It has been proved that\nlanguage models based on Unigram LM tokenizer\nwork better (Bostrom and Durrett, 2020).\n2.3 Neural Machine Translation\nNeural Machine Translation (NMT) has recently\nbeen the subﬁeld of NLP where many new tok-\nenization techniques were introduced, before being\nmore widely adopted by the whole NLP commu-\nnity (Sennrich et al., 2016; Kudo, 2018). This is\nalso true when it comes to the new approaches\nto encoding case information into the neural mod-\nels (Bérard et al., 2019; Etchegoyhen and Gete,\n2020; Shi et al., 2020). The following methods are\ncommonly used in NMT: Truecasing, Recasing,\nCase Factors (CF) and Inline Casing (IC). Two\nof them can be naturally adopted to the problem\nof language modelling: CF (subtoken embedding\nis the concatenation of lowercased base-token em-\nbedding and the case embedding for each variant\ni.e. title, uppercase, mixed) and IC (working on\nlowercased text but adding extra tags before words\nwhich indicate case variants). In this paper we pro-\npose a solution which is similar to the Case Factors\nmethod with some modiﬁcations.\n3 Method\nIn this section we describe our approach to dealing\nwith casing in language models. It can be decom-\nposed into two main parts: tokenization and model\narchitecture.\n3.1 Tokenization\nAs described in Section 2.2 Unigram tokenizer\noutperforms BPE on Language Model pretraining\n(Bostrom and Durrett, 2020), therefore we are bas-\ning our solution on this method. In addition, we\nwant to create a tokenizer which is able to fulﬁl the\nfollowing conditions:\n• tokenization should be the same for texts re-\ngardless of different casing, with the excep-\ntion of the next point\n• the above condition can be violated for words\nwritten with mixed casing (e.g. camelCasing),\nsuch words could be split in places where let-\nter casing changes (e.g. word RoBERTa could\nbe splitted into _Ro/BERT/a, even though\nword Roberta could be represented as a single\ntoken).\nIn order to do that, we trained the Unigram Sen-\ntencepiece tokenizer (Kudo and Richardson, 2018)\nin a way, where word tokens are kept in various\ncase variants. We obtained that, in order to sat-\nisfy above conditions, we need only 3 case variants\n(shapes) i.e. lowercase (aaa), uppercase (AAA) and\ntitlecase (Aaa). Other shapes are only needed in\nmixed case variants which we decided to split. It is\nworth mentioning, that such token multiplication\nby its shape variants is valid only for word tokens.\nFor tokens which contain numbers, punctuation\nmarks, etc. we kept only the original.\n3.2 Model\nModel architecture is modiﬁed in order to utilize\nthe information, that some word tokens in the dic-\ntionary, are linked with the same base (lowercase)\nform. To this end, we decompose the token embed-\nding into base-token embedding and case embed-\nding (Figure 1). These embeddings are trainable\nand added to each other in the same fashion as\npositional embeddings are added in the original\nTransformer architecture (Vaswani et al., 2017). In\nthe consequence of such decomposition, models\nFigure 1: Unicase input representation. The input embeddings are the sum of the base token embeddings, the\npositional embeddings and case embeddings.\nwith the same number of parameters, can utilize\nmuch bigger vocabularies.\nDuring the pretraining phase, we are also decom-\nposing the original masked token prediction task\ninto base-token prediction and case prediction tasks.\nFinal loss is computed as a weighted sum of two\ntasks’ losses (1). By using weights, we are forcing\nthe model to focus more on base-token prediction.\nOn the initial setting we chose α= 0.1.\nL= Lbase_token + αLcase (1)\n4 Experiments\n4.1 Implementation\nWe are basing our UniCase architecture on\nRoBERTa code implemented in FAIRSEQ (Ott\net al., 2019). Code and pretrained models will\nbe publicly released.\n4.2 Unsupervised training\n4.2.1 Models\nWe have trained two versions of UniCase model\ncorresponding to different tokenizer settings.\n• UniCase model based on UniCase Tokenizer\nwith 20k base tokens, which correspond to\nbase_token_embedding_size = 20 k and\nvocab_size≈ 57k\n• UniCase model based on UniCase Tokenizer\nwith 32k base tokens, which correspond to\nbase_token_embedding_size = 32 k and\nvocab_size≈ 90k\nBoth models correspond to RoBERTa-base in terms\nof size.\nAs a baseline model we chose RoBERTa-base\narchitecture and trained it from scratch. We mod-\niﬁed original setup and used a Unigram tokenizer\n(vocab_size = 32k) to be sure that potential per-\nformance deterioration is not caused by the BPE\ntokenizer, which was proved to be not an opti-\nmal choice for tokenization (Bostrom and Durrett,\n2020).\nWe have trained all models on DGX-2 server us-\ning the setting recommended by RoBERTa authors\nwith batch_size= 2048for 100kupdate steps.\n4.2.2 Data\nThe size and quality of pretraining data was proved\nto play important role for achieving state-of-the-\nart results (Liu et al., 2019; Brown et al., 2020).\nThus, all our models were pretrained on the CCNet\ndataset (Wenzek et al., 2019), which contains about\n700 millions of documents for English language,\ncorresponding to 330 GB of uncompressed text.\n4.3 Experiments\n4.3.1 Data\nWe conducted all our experiments on the GLUE\nbenchmark (Wang et al., 2018), which is a collec-\ntion of well known datasets for testing natural lan-\nguage understanding systems. The original bench-\nmark contains 9 tasks, from which we skip the\nproblematic WNLI set. All our results were based\non development sets.\n4.3.2 Settings\nWe have trained all models separately for each of\nthe GLUE tasks, using only the training data for the\ncorresponding task. For ﬁnetuning on each task we\nhave used parameters recommended by RoBERTa\nauthors (Liu et al., 2019). All results presented in\ntables are medians over four random initializations.\n4.3.3 Results on original texts\nAt the beginning, we evaluated all pretrained mod-\nels (described in section 4.2.1) by using text in\noriginal casing (see Table 2. We observed that both\nUniCase models variants perform better than base-\nline model. Only on SST dataset RoBERTa model\nis better. Interestingly, this is the only task in GLUE\nModel CoLA MNLI MRPC QNLI QQP RTE SST STS-B Average\nOriginal casing\nUniCase (20k) 59.82 85.40 90.89 91.22 88.01 69.68 92.83 87.96 83.20\nUniCase (32k) 58.29 85.18 90.88 91.29 88.16 71.84 92.78 88.18 83.29\nRoBERTa (32k) 57.92 84.84 90.33 91.01 88.14 69.31 94.04 87.80 82.87\nAll texts from train and development sets were lowercased\nUniCase (32k) 55.99 85.23 90.85 90.90 88.13 70.40 92.89 88.26 82.80\nRoBERTa (32k) 55.08 84.82 90.65 90.31 88.14 67.87 94.15 87.70 82.31\nAll texts from train and development sets were uppercased\nUniCase (32k) 56.25 85.19 91.28 91.10 88.09 71.84 92.83 88.11 83.07\nRoBERTa (32k) 39.24 80.11 87.84 87.23 86.90 62.82 89.11 85.21 77.19\nTable 2: Results on GLUE. The “Average” column is slightly different than the ofﬁcial GLUE score, since we\nexclude the problematic WNLI set. F1 scores are reported for QQP and MRPC, Spearman correlations are reported\nfor STS-B, and accuracy scores are reported for the other tasks. All task results are median over four runs.\nwhere text is lowercased2. The inﬂuence of vocab-\nulary size in UniCase model is not exactly known\nas more experiments need to be done . Neverthe-\nless, the best model was trained with vocabulary\nsize containing 32k of base-tokens. Therefore, that\nmodel will be used in further experiments.\n4.3.4 Results on noisy texts\nFirstly, intuitively all models achieved better scores\nwith original casing but only UniCase model have\nstable overall results between different experiment\nsettings: original (83.29), lowercase (82.80) and\nuppercase (83.07). Surprisingly, on MRPC dataset,\nUniCase model achieved higher score on average\nwhere texts was uppercased, which might be ex-\nplained with very small development set size and\nhigher deviation of results.\nWe ﬁnd that overall performance of the UniCase\nmodel is better that RoBERTa when we must deal\nwith noisy texts 2. However, only in the case where\ntexts where uppercased, we can observe signiﬁ-\ncant difference between these two models, which\nis 5.88 points based on average metric from pre-\nsented GLUE tasks. We think that tokenization of\nuppercase document is heavily fragmented which\nin conjunction with relatively poor uppercase repre-\nsentation in pretraining corpora might lead to poor\nuppercase text understanding.\n2Further investigation is needed to determine the reason of\nthat.\n5 Conclusion\nWe have presented and tested new UniCase archi-\ntecture dealing with case-sensitivity in Language\nModelling by decomposing information about cas-\ning into a separate component. Consequently, we\nwere able to build models with the same number\nof parameters utilizing larger vocabularies. In con-\ntrast to classic Language Models, UniCase does not\nhave to build a semantic understanding of words or\nsentences in all case variants, potentially leading to\nmore effective training.\nWe showed that our method outperforms the\nRoBERTa baseline on almost all tested tasks. No-\ntably, results reported on uppercased GLUE tasks\nshow that models trained with our method under-\nstand uppercased documents much better. That\nshows a promising application of UniCase models\nin understanding documents where uppercasesed\nletters or words are more common, i.e., business\ndocuments, forms, invoices.\nAcknowledgments\nWe thank Filip Grali ´nski, Łukasz Garncarek,\nMichał Pietruszka, Łukasz Borchmann and Dawid\nJurkiewicz for their discussion about the paper\nand ours managing directors at Applica.ai: Adam\nDancewicz and Piotr Surma.\nThe authors would like to acknowledge the sup-\nport the Applica.ai project has received as being\nco-ﬁnanced by the European Regional Develop-\nment Fund (POIR.01.01.01-00- 0144/17-00).\nReferences\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners.\nAlexandre Bérard, Ioan Calapodescu, and Claude\nRoux. 2019. Naver labs europe’s systems for the\nwmt19 machine translation robustness task.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nThierry Etchegoyhen and Harritxu Gete. 2020. To case\nor not to case: Evaluating casing methods for neural\nmachine translation. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n3752–3760, Marseille, France. European Language\nResources Association.\nForrest N. Iandola, Albert E. Shaw, Ravi Krishna, and\nKurt W. Keutzer. 2020. Squeezebert: What can\ncomputer vision teach nlp about efﬁcient neural net-\nworks?\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nAbigail Rai and Samarjeet Borah. 2021. Study of\nvarious methods for tokenization. In Applications\nof Internet of Things, pages 193–200, Singapore.\nSpringer Singapore.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nXuewen Shi, Heyan Huang, Ping Jian, and Yi-Kun\nTang. 2020. Case-sensitive neural machine trans-\nlation. In Advances in Knowledge Discovery and\nData Mining, pages 662–674, Cham. Springer Inter-\nnational Publishing.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2019. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. arXiv preprint arXiv:1907.12412.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2020. Superglue: A\nstickier benchmark for general-purpose language un-\nderstanding systems.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2019. Ccnet: Ex-\ntracting high quality monolingual datasets from web\ncrawl data.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.746638834476471
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7072635293006897
    },
    {
      "name": "Lexical analysis",
      "score": 0.6542423963546753
    },
    {
      "name": "Point (geometry)",
      "score": 0.5663251280784607
    },
    {
      "name": "Language model",
      "score": 0.530693769454956
    },
    {
      "name": "Architecture",
      "score": 0.528483510017395
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5157256722450256
    },
    {
      "name": "Programming language",
      "score": 0.5077470541000366
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4583333134651184
    },
    {
      "name": "Natural language processing",
      "score": 0.3957447409629822
    },
    {
      "name": "Mathematics",
      "score": 0.10847049951553345
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ]
}