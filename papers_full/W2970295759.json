{
  "title": "3D Scene Reconstruction With Multi-Layer Depth and Epipolar Transformers",
  "url": "https://openalex.org/W2970295759",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5028338565",
      "name": "Daeyun Shin",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A5014894873",
      "name": "Zhile Ren",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5076761279",
      "name": "Erik B. Sudderth",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A5044978076",
      "name": "Charless C. Fowlkes",
      "affiliations": [
        "UC Irvine Health"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2129201358",
    "https://openalex.org/W2136550140",
    "https://openalex.org/W1927784829",
    "https://openalex.org/W6676449312",
    "https://openalex.org/W2556802233",
    "https://openalex.org/W6750115007",
    "https://openalex.org/W6742627460",
    "https://openalex.org/W6752062589",
    "https://openalex.org/W2964239644",
    "https://openalex.org/W2963369474",
    "https://openalex.org/W2598591334",
    "https://openalex.org/W6760094274",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W6703405610",
    "https://openalex.org/W6687484953",
    "https://openalex.org/W2594568905",
    "https://openalex.org/W1893912098",
    "https://openalex.org/W2795912842",
    "https://openalex.org/W6739954837",
    "https://openalex.org/W2963527086",
    "https://openalex.org/W6605121731",
    "https://openalex.org/W2211722331",
    "https://openalex.org/W6745947945",
    "https://openalex.org/W6746099373",
    "https://openalex.org/W6751360438",
    "https://openalex.org/W2609883120",
    "https://openalex.org/W2563100679",
    "https://openalex.org/W6756498474",
    "https://openalex.org/W6748927947",
    "https://openalex.org/W6729687604",
    "https://openalex.org/W6640300118",
    "https://openalex.org/W2981827368",
    "https://openalex.org/W6752996450",
    "https://openalex.org/W2557465155",
    "https://openalex.org/W2025498056",
    "https://openalex.org/W1949568868",
    "https://openalex.org/W2606840594",
    "https://openalex.org/W1998793490",
    "https://openalex.org/W2099940712",
    "https://openalex.org/W4250952223",
    "https://openalex.org/W6618372016",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W6703956885",
    "https://openalex.org/W2559882727",
    "https://openalex.org/W4247250903",
    "https://openalex.org/W6685261749",
    "https://openalex.org/W2884654623",
    "https://openalex.org/W6762390211",
    "https://openalex.org/W2560722161",
    "https://openalex.org/W2963739349",
    "https://openalex.org/W6752817829",
    "https://openalex.org/W6750179221",
    "https://openalex.org/W2142912032",
    "https://openalex.org/W6764557091",
    "https://openalex.org/W2788158258",
    "https://openalex.org/W2963893349",
    "https://openalex.org/W6650328104",
    "https://openalex.org/W2964121028",
    "https://openalex.org/W2771919418",
    "https://openalex.org/W2339763956",
    "https://openalex.org/W125693051",
    "https://openalex.org/W2850910281",
    "https://openalex.org/W2963141648",
    "https://openalex.org/W2962778872",
    "https://openalex.org/W158943247",
    "https://openalex.org/W2551540143",
    "https://openalex.org/W2190691619",
    "https://openalex.org/W2766479775",
    "https://openalex.org/W2963547760",
    "https://openalex.org/W2342277278",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W1920022804",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2963557767",
    "https://openalex.org/W2787366651",
    "https://openalex.org/W2963872169",
    "https://openalex.org/W2981829592",
    "https://openalex.org/W2171740948",
    "https://openalex.org/W2946127255",
    "https://openalex.org/W2883907768",
    "https://openalex.org/W2963073398",
    "https://openalex.org/W2964137676",
    "https://openalex.org/W1999305212",
    "https://openalex.org/W2336961836"
  ],
  "abstract": "We tackle the problem of automatically reconstructing a complete 3D model of a scene from a single RGB image. This challenging task requires inferring the shape of both visible and occluded surfaces. Our approach utilizes viewer-centered, multi-layer representation of scene geometry adapted from recent methods for single object shape completion. To improve the accuracy of view-centered representations for complex scenes, we introduce a novel \"Epipolar Feature Transformer\" that transfers convolutional network features from an input view to other virtual camera viewpoints, and thus better covers the 3D scene geometry. Unlike existing approaches that first detect and localize objects in 3D, and then infer object shape using category-specific models, our approach is fully convolutional, end-to-end differentiable, and avoids the resolution and memory limitations of voxel representations. We demonstrate the advantages of multi-layer depth representations and epipolar feature transformers on the reconstruction of a large database of indoor scenes.",
  "full_text": "3D Scene Reconstruction with Multi-layer Depth and Epipolar Transformers\nDaeyun Shin1 Zhile Ren2 Erik B. Sudderth1 Charless C. Fowlkes1\n1University of California, Irvine 2Georgia Institute of Technology\nhttps://research.dshin.org/iccv19/multi-layer-depth\nAbstract\nWe tackle the problem of automatically reconstructing\na complete 3D model of a scene from a single RGB im-\nage. This challenging task requires inferring the shape of\nboth visible and occluded surfaces. Our approach utilizes\nviewer-centered, multi-layer representation of scene geom-\netry adapted from recent methods for single object shape\ncompletion. To improve the accuracy of view-centered\nrepresentations for complex scenes, we introduce a novel\n“Epipolar Feature Transformer” that transfers convolu-\ntional network features from an input view to other virtual\ncamera viewpoints, and thus better covers the 3D scene ge-\nometry. Unlike existing approaches that ﬁrst detect and\nlocalize objects in 3D, and then infer object shape us-\ning category-speciﬁc models, our approach is fully convo-\nlutional, end-to-end differentiable, and avoids the resolu-\ntion and memory limitations of voxel representations. We\ndemonstrate the advantages of multi-layer depth represen-\ntations and epipolar feature transformers on the reconstruc-\ntion of a large database of indoor scenes.\n1. Introduction\nWhen we examine a photograph of a scene, we not only\nperceive the 3D shape of visible surfaces, but effortlessly\ninfer the existence of many invisible surfaces. We can make\nstrong predictions about the complete shapes of familiar ob-\njects despite viewing only a single, partially occluded as-\npect, and can infer information about the overall volumetric\noccupancy with sufﬁcient accuracy to plan navigation and\ninteractions with complex scenes. This remains a daunting\nvisual task for machines despite much recent progress in\ndetecting individual objects and making predictions about\ntheir shape. Convolutional neural networks (CNNs) have\nproven incredibly successful as tools for learning rich rep-\nresentations of object identity which are invariant to intra-\ncategory variations in appearance. Predicting 3D shape\nrather than object category has proven more challenging\nsince the output space is higher dimensional and carries\nmore structure than simple regression or classiﬁcation tasks.\nFigure 1: Given a single input view of a scene (top left), we\nwould like to predict a complete geometric model. Depth\nmaps (top right) provide an efﬁcient representation of scene\ngeometry but are incomplete, leaving large holes (e.g., the\nwardrobe). We propose multi-layer depth predictions (bot-\ntom left) that provide complete view-based representations\nof shape, and introduce an epipolar transformer network\nthat allows view-based inference and prediction from vir-\ntual viewpoints (like overhead views, bottom right).\nEarly successes at using CNNs for shape prediction\nleveraged direct correspondences between the input and\noutput domain, regressing depth and surface normals at ev-\nery input pixel [8]. However, these so-called 2.5D represen-\ntations are incomplete: they don’t make predictions about\nthe back side of objects or other occluded surfaces. Sev-\neral recent methods instead manipulate voxel-based repre-\nsentations [41] and use convolutions to perform translation-\ncovariant computations in 3D. This provides a more com-\nplete representation than 2.5D models, but suffers from sub-\nstantial storage and computation expense that scales cubi-\ncally with resolution of the volume being modeled (with-\nout specialized representations like octrees [31]). Other\n1\narXiv:1902.06729v2  [cs.CV]  27 Aug 2019\nFigure 2: Overview of our system for reconstructing a complete 3D scene from a single RGB image. We ﬁrst predict a\nmulti-layer depth map that encodes the depths of front and back object surfaces as seen from the input camera. Given the\nextracted feature map and predicted multi-layer depths, the epipolar feature transformer network transfers features from the\ninput view to a virtual overhead view, where the heights of observed objects are predicted. Semantic segmentation masks are\ninferred and inform our geometry estimates, but explicit detection of object instances is not required, increasing robustness.\napproaches represent shape as an unstructured point cloud\n[29, 42], but require development of suitable convolutional\noperators [11, 49] and fail to capture surface topology.\nIn this paper, we tackle the problem of automatically re-\nconstructing a complete 3D model of a scene from a single\nRGB image. As depicted in Figures 1 and 2, our approach\nuses an alternative shape representation that extends view-\nbased 2.5D representations to a complete 3D representation.\nWe combine multi-layer depth maps that store the depth to\nmultiple surface intersections along each camera ray from\na given viewpoint, with multi-view depth maps that record\nsurface depths from different camera viewpoints.\nWhile multi-view and multi-layer shape representations\nhave been explored for single object shape completion, for\nexample by [36], we argue that multi-layer depth maps\nare particularly well suited for representing full 3D scenes.\nFirst, they compactly capture high-resolution details about\nthe shapes of surfaces in a large scene. V oxel-based rep-\nresentations allocate a huge amount of resources to simply\nmodeling empty space, ultimately limiting shape ﬁdelity to\nmuch lower resolution than is provided by cues like occlud-\ning contours in the input image [41]. A multi-layer depth\nmap can be viewed as a run-length encoding of dense rep-\nresentations that stores only transitions between empty and\noccupied space. Second, view-based depths maintain ex-\nplicit correspondence between input image data and scene\ngeometry. Much of the work on voxel and point cloud rep-\nresentations for single object shape prediction has focused\non predicting a 3D representation in an object-centered co-\nordinate system. Utilizing such an approach for scenes re-\nquires additional steps of detecting individual objects and\nestimating their pose in order to place them back into some\nglobal scene coordinate system [45]. In contrast, view-\nbased multi-depth predictions provide a single, globally co-\nherent scene representation that can be computed in a “fully\nconvolutional” manner from the input image.\nOne limitation of predicting a multi-layer depth repre-\nsentation from the input image viewpoint is that the repre-\nsentation cannot accurately encode the geometry of surfaces\nwhich are nearly tangent to the viewing direction. In ad-\ndition, complicated scenes may contain many partially oc-\ncluded objects that require a large number of layers to rep-\nresent completely. We address this challenge by predicting\nadditional (multi-layer) depth maps computed from virtual\nviewpoints elsewhere in the scene. To link these predictions\nfrom virtual viewpoints with the input viewpoint, we intro-\nduce a novel Epipolar Feature Transformer (EFT) network\nmodule. Given the relative poses of the input and virtual\ncameras, we transfer features from a given location in the\ninput view feature map to the corresponding epipolar line\nin the virtual camera feature map. This transfer process is\nmodulated by predictions of surface depths from the input\nview in order to effectively re-project features to the correct\nlocations in the overhead view.\nTo summarize our contributions, we propose a view-\nbased, multi-layer depth representation that enables fully\nconvolutional inference of 3D scene geometry and shape\ncompletion. We also introduce EFT networks that provide\ngeometrically consistent transfer of CNN features between\ncameras with different poses, allowing end-to-end train-\ning for multi-view inference. We experimentally character-\nize the completeness of these representations for describing\nthe 3D geometry of indoor scenes, and show that models\ntrained to predict these representations can provide better\n2\nrecall and precision of scene geometry than existing ap-\nproaches based on object detection.\n2. Related Work\nThe task of recovering 3D geometry from 2D images has\na rich history dating to the visionary work of Roberts [32].\nMonocular Object Shape Prediction. Single-view 3D\nshape reconstruction is challenging because the output\nspace is under-constrained. Large-scale datasets like\nShapeNet [1, 53] facilitate progress in this area, and re-\ncent methods have learned geometric priors for object cat-\negories [22, 52], disentangled primitive shapes from ob-\njects [13, 61], or modeled surfaces [15, 36, 56]. Other work\naims to complete the occluded geometric structure of ob-\njects from a 2.5D image or partial 3D scan [33, 6, 51, 55].\nWhile the quality of such 3D object reconstructions con-\ntinues to grow [23, 49], applications are limited by the as-\nsumption that input images depict a single, centered object.\n3D Scene Reconstruction. We seek to predict the geom-\netry of full scenes containing an unknown number of ob-\njects; this task is signiﬁcantly more challenging than ob-\nject reconstruction. Tulsiani et al. [45] factorize 3D scenes\ninto detected objects and room layout by integrating sep-\narate methods for 2D object detection, pose estimation,\nand object-centered shape prediction. Given a depth im-\nage as input, Song et al. [41] propose a volumetric recon-\nstruction algorithm that predicts semantically labeled 3D\nvoxels. Another general approach is to retrieve exemplar\nCAD models from a large database and reconstruct parts\nof scenes [18, 60, 14], but the complexity of CAD models\nmay not match real-world environments. While our goals\nare similar to Tulsiani et al., our multi-layered depth esti-\nmates provide a denser representation of complex scenes.\nRepresentations for 3D Shape Prediction.Most recent\nmethods use voxel representations to reconstruct 3D geom-\netry [3, 41, 38, 47, 37], in part because they easily integrate\nwith 3D CNNs [53] for high-level recognition tasks [25].\nOther methods [9, 24] use dense point clouds representa-\ntions. Classic 2.5D depth maps [8, 2] recover the geome-\ntry of visible scene features, but do not capture occluded\nregions. Shin et al. [36] empirically compared these repre-\nsentations for object reconstruction. We extend these ideas\nto whole scenes via a multi-view, multi-layer depth repre-\nsentation that encodes the shape of multiple objects.\nLearning Layered Representations. Layered represen-\ntations [48] have proven useful for many computer vision\ntasks including segmentation [12] and optical ﬂow predic-\ntion [44]. For 3D reconstruction, decomposing scenes into\nlayers enables algorithms to reason about object occlusions\nand depth orderings [16, 39, 50]. Layered 2.5D represen-\ntations such as the two-layer decompositions of [46, 7] in-\nfer the depth of occluded surfaces facing the camera. Our\nmulti-layer depth representation extends this idea by includ-\ning the depth of back surfaces (equiv. object thickness). We\nalso infer depths from virtual viewpoints far from the input\nview for more complete coverage of 3D scene geometry.\nOur use of layers generalizes [30], who used multiple in-\ntersection depths to model non-convexities for constrained\nscenes containing a single, centered object. Concurrently\nto our work, [27] predicts object-level thicknesses for volu-\nmetric RGB-D fusion and [10] estimates 3D human shape.\nMulti-view Shape Synthesis. Many classic 3D recon-\nstruction methods utilize multi-view inputs to synthesize 3D\nshapes [17, 40, 4]. Given monocular inputs, several recent\nmethods explore ways of synthesizing object appearance or\nimage features from novel viewpoints [59, 54, 20, 3, 28, 43].\nOther work uses unsupervised learning from stereo or video\nsequences to reason about depths [58, 21]. Instead of simply\ntransferring the pixel colors associated with surface points\nto novel views, we transfer whole CNN feature maps over\ncorresponding object volumes, and thereby produce more\naccurate and complete 3D reconstructions.\n3. Reconstruction with Multi-Layer Depth\nTraditional depth maps record the depth at which a ray\nthrough a given pixel ﬁrst intersects a surface in the scene.\nSuch 2.5D representations of scene geometry accurately de-\nscribe visible surfaces, but cannot encode the shape of par-\ntially occluded objects, and may fail to capture the complete\n3D shape of unoccluded objects (due to self-occlusion). We\ninstead represent 3D scene geometry by recording multiple\nsurface intersections for each camera ray. As illustrated in\nFigure 3(a), some rays may intersect many object surfaces\nand require several layers to capture all details. But as the\nnumber of layers grows, multi-layer depths completely rep-\nresent 3D scenes with multiple non-convex objects.\nWe use experiments to empirically determine a ﬁxed\nnumber of layers that provides good coverage of typical nat-\nural scenes, while remaining compact enough for efﬁcient\nlearning and prediction. Another challenge is that surfaces\nthat are nearly tangent to input camera rays are not well\nrepresented by a depth map of ﬁxed resolution. To address\nthis, we introduce an additional virtual view where tangent\nsurfaces are sampled more densely (see Section 4).\n3.1. Multi-Layer Depth Maps from 3D Geometry\nIn our experiments, we focus on a ﬁve-layer model de-\nsigned to represent key features of 3D scene geometry for\n¯D1 ¯D1,2 ¯D1,2,3 ¯D1..4 ¯D1..5 ¯D1..5 +Ovh.\n0.237 0.427 0.450 0.480 0.924 0.932\nTable 1: Scene surface coverage (recall) of ground truth\ndepth layers with a 5cm threshold. Our predictions cover\n93% of the scene geometry inside the viewing frustum.\n3\nFigure 3: Epipolar transfer of features from the input image\nto a virtual overhead view. Given multi-layer depth predic-\ntions of surface entrances and exits, each pixel in the input\nview is mapped to zero, one, or two segments of the corre-\nsponding epipolar line in the virtual view.\ntypical indoor scenes. To capture the overall room layout,\nwe model the room envelope (ﬂoors, walls, ceiling, win-\ndows) that deﬁnes the extent of the space. We deﬁne the\ndepth D5 of these surfaces to be the last layer of the scene.\nTo model the shapes of observed objects, we trace rays\nfrom the input view and record the ﬁrst intersection with a\nvisible surface in depth map D1. This resembles a standard\ndepth map, but excludes the room envelope. If we continue\nalong the same ray, it will eventually exit the object at a\ndepth we denote byD2. For non-convex objects the ray may\nintersect the same object multiple times, but we only record\nthe last exit in D2. As many indoor objects have large con-\nvex parts, the D1 and D2 layers are often sufﬁcient to accu-\nrately reconstruct a large proportion of foreground objects\nFigure 4: A volumetric visualization of our predicted multi-\nlayer surfaces and semantic labels on SUNCG. We project\nthe center of each voxel into the input camera, and the voxel\nis marked occupied if the depth value falls in the ﬁrst object\ninterval (D1,D2) or the occluded object interval (D3,D4).\nin real scenes. While room envelopes typically have a very\nsimple shape, the prediction of occluded structure behind\nforeground objects is more challenging. We deﬁne layer\nD3 > D2 as the depth of the next object intersection, and\nD4 as the depth of the exit from that second object instance.\nWe let ( ¯D1, ¯D2, ¯D3, ¯D4, ¯D5) denote the ground truth\nmulti-layer depth maps derived from a complete 3D model.\nSince not all viewing rays intersect the same number of ob-\njects (e.g., when the room envelope is directly visible), we\ndeﬁne a binary mask ¯Mℓ which indicates the pixels where\nlayer ℓhas support. Note that ¯M1 = ¯M2, and ¯M3 = ¯M4,\nsince D2 (ﬁrst instance exit) has the same support as D1.\nExperiments in Section 5 evaluate the relative importance\nof different layers in modeling realistic 3D scenes.\n3.2. Predicting Multi-Layer Depth Maps\nTo learn to predict ﬁve-channel multi-layer depths D=\n(D1,D2,D3,D4,D5) from images, we train a standard\nencoder-decoder network with skip connections, and use\nthe Huber loss ρh(.,.) to measure prediction errors:\nLd(D) =\n5∑\nℓ=1\n( ¯Mℓ\n||¯Mℓ||1\n)\n·ρh(Dℓ, ¯Dℓ). (1)\nWe also predict semantic segmentation masks for the ﬁrst\nand third layers. The structure of the semantic segmenta-\ntion network is similar to the multi-layer depth prediction\nnetwork, except that the output has 80 channels (40 object\ncategories in each of two layers), and errors are measured\nvia the cross-entropy loss. To reconstruct complete 3D ge-\nometry from multi-layer depth predictions, we use predicted\nmasks and depths to generate meshes corresponding to the\nfront and back surfaces of visible and partially occluded ob-\njects, as well as the room envelope. Without the back sur-\nfaces [35], ground truth depth layers ¯D1,3,5 cover only 82%\nof the scene geometry inside the viewing frustum (vs. 92%\nincluding back surfaces of objects, see Table 1).\n4. Epipolar Feature Transformer Networks\nTo allow for richer view-based scene understanding, we\nwould like to relate features visible in the input view to\nfeature representations in other views. To achieve this, we\ntransfer features computed in input image coordinates to the\ncoordinate system of a “virtual camera” placed elsewhere in\nthe scene. This approach more efﬁciently covers some parts\nof 3D scenes than single-view, multi-layer depths.\nFigure 2 shows a block diagram of our Epipolar Feature\nTransformer (EFT) network. Given features F extracted\nfrom the image, we choose a virtual camera location with\ntransformation mapping T and transfer weights W, and\nuse these to warp F to create a new “virtual view” feature\nmap G. Like spatial transformer networks (STNs) [19] we\nperform a parametric, differentiable “warping” of a feature\n4\nmap. However, EFTs incorporate a weighted pooling oper-\nation informed by multi-view geometry.\nEpipolar Feature Mapping. Image features at spatial lo-\ncation (s,t) in an input view correspond to information\nabout the scene which lies somewhere along the ray\n\n\nx\ny\nz\n\n= zKI\n−1\n\n\ns\nt\n1\n\n, z ≥0,\nwhere KI ∈R3×3 encodes the input camera intrinsic pa-\nrameters, as well as the spatial resolution and offset of the\nfeature map. z is the depth along the viewing ray, whose\nimage in a virtual orthographic camera is given by\n[u(s,t,z )\nv(s,t,z )\n]\n= KV\n\nzRKI\n−1\n\n\ns\nt\n1\n\n+ t\n\n, z ≥0.\nHere KV ∈R2×3 encodes the virtual view resolution and\noffset, and R and t the relative pose. 1 Let T(s,t,z ) =\n(u(s,t,z ),v(s,t,z )) denote the forward mapping from\npoints along the ray into the virtual camera, and Ω(u,v) =\n{(s,t,z ) : T(s,t,z ) = (u,v)}be the pre-image of (u,v).\nGiven a feature map computed from the input view\nF(s,t,f ), where f indexes the feature dimension, we syn-\nthesize a new feature map G corresponding to the virtual\nview. We consider general mappings of the form\nG(u,v,f ) =\n∑\n(s,t,z)∈Ω(u,v) F(s,t,f )W(s,t,z )\n∑\n(s,t,z)∈Ω(u,v) W(s,t,z ) ,\nwhere W(s,t,z ) ≥0 is a gating function that may depend\non features of the input image. 2 When Ω(u,v) is empty,\nwe set G(u,v,f ) = 0 for points (u,v) outside the viewing\nfrustum of the input camera, and otherwise interpolate fea-\nture values from those of neighboring virtual-view pixels.\nChoice of the Gating FunctionW. By design, the trans-\nformed features are differentiable with respect to F and W.\nThus in general we could assign a loss to predictions from\nthe virtual camera, and learn an arbitrary gating functionW\nfrom training data. However, we instead propose to leverage\nadditional geometric structure based on predictions about\nthe scene geometry produced by the frontal view.\nSuppose we have a scene depth estimate D1(s,t) at ev-\nery location in the input view. To simplify occlusion rea-\nsoning we assume that relative to the input camera view, the\nvirtual camera is rotated around the x-axis by θ <90◦and\ntranslated in y and z to sit above the scene so that points\n1For a perspective model the righthand side is scaled byz′(s, t, z), the\ndepth from the virtual camera of the point at location z along the ray.\n2For notational simplicity, we have written G as a sum over a discrete\nset of samples Ω. To make G differentiable with respect to the virtual\ncamera parameters, we perform bilinear interpolation.\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nFigure 5: Single image scene reconstruction via multi-layer\ndepth maps. Estimates of the front (green) and back (cyan)\nsurfaces of objects, as seen from the input view, are comple-\nmented by heights estimated by a virtual overhead camera\n(dark green) via our epipolar feature transform. Room en-\nvelope estimates are rendered in gray.\nwhich project to larger tin the input view have larger depth\nin the virtual view. Setting the gating function as\nW1\nsurf(s,t,z ) = δ[D1(s,t) = z]\nt−1∏\nˆt=0\nδ[D1(s,ˆt)+(t−ˆt) cosθ̸= z]\nyields an epipolar feature transform that re-projects each\nfeature at input location (s,t) into the overhead view via\nthe depth estimate D1, but only in cases where it is not oc-\ncluded by a patch of surface higher up in the scene. In our\nexperiments we computeWℓ\nsurf for each Dℓ, ℓ∈{1,2,3,4},\nand use Wsurf = maxℓ Wℓ\nsurf to transfer input view features\nto both visible and occluded surfaces in the overhead feature\nmap. We implement this transformation using a z-buffering\napproach by traversing the input feature map from bottom\nto top, and overwriting cells in the overhead feature map.\nFigure 3(b) illustrates this feature mapping applied to\ncolor features using the ground-truth depth map for a scene.\nIn some sense, this surface-based reprojection is quite con-\nservative because it leaves holes in the interior of objects\n(e.g., the interior of the orange wood cabinet). If the frontal\nview network features at a given spatial location encode the\npresence, shape, and pose of some object, then those fea-\ntures really describe a whole volume of the scene behind the\nobject surface. It may thus be preferable to instead transfer\nthe input view features to the entire expected volume in the\noverhead representation.\nTo achieve this, we use the multi-layer depth representa-\ntion predicted by the frontal view to deﬁne a range of scene\n5\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nFigure 6: Evaluation of 3D reconstruction on the\nNYUv2 [26] dataset. Tulsiani et al. [45] are sensitive to\nthe performance of 2D object detectors, and their voxelized\noutput is a coarse approximation of the true 3D geometry.\ndepths to which the input view feature should be mapped.\nIf D1(s,t) is the depth of the front surface and D2(s,t) is\nthe depth at which the ray exits the back surface of an object\ninstance, we deﬁne a volume-based gating function:\nWvol(s,t,z ) = δ[z∈(D1(s,t),D2(s,t))].\nAs illustrated in Figure 3(a), volume-based gating copies\nfeatures from the input view to entire segments of the epipo-\nlar line in the virtual view. In our experiments we use this\ngating to generate features for (D1,D2) and concatenate\nthem with a feature map generated using (D3,D4).\nOverhead Viewpoint Generation. For cluttered indoor\nscenes, there may be many overlapping objects in the input\nview. Overhead views of such scenes typically have much\nless occlusion and should be simpler to reason about geo-\nmetrically. We thus select a virtual camera that is roughly\noverhead and covers the scene content visible from the ref-\nerence view. We assume the input view is always taken with\nthe gravity direction in the y,z plane. We parameterize the\noverhead camera relative to the reference view by a trans-\nlation (tx,ty,tz) which centers it over the scene at ﬁxed\nheight above the ﬂoor, a rotation θ which aligns the over-\nhead camera to the gravity direction, and a scaleσthat cap-\ntures the radius of the orthographic camera frustum.\n5. Experiments\nBecause we model complete descriptions of the ground-\ntruth 3D geometry corresponding to RGB images, which is\nnot readily available for natural images, we learn to predict\nmulti-layer and multi-view depths from physical renderings\nof indoor scenes [57] provided by the SUNCG dataset [41].\n5.1. Generation of Training Data\nThe SUNCG dataset [41] contains complete 3D meshes\nfor 41,490 houses that we render to generate our training\ndata. For each rendered training image, we extract the sub-\nset of the house model that is relevant to the image content,\nwithout making assumptions about the room size. We trans-\nform the house mesh to the camera’s coordinate system and\ntruncate polygons that are outside the left, top, right, bot-\ntom, and near planes of the perspective viewing frustum.\nObjects that are projected behind the depth image of the\nroom envelope are also removed. The ﬁnal ground truth\nmesh that we evaluate against contains all polygons from\nthe remaining objects, as well as the true room envelope.\nFor each rendered training image, we generate target\nmulti-depth maps and segmentation masks by performing\nmulti-hit ray tracing on the ground-truth geometry. We sim-\nilarly compute ground-truth height maps for a virtual ortho-\ngraphic camera centered over each scene. To select an over-\nhead camera viewpoint for training that covers the relevant\nscene content, we consider three heuristics: (i) Convert the\ntrue depth map to a point cloud, center the overhead cam-\nera over the mean of these points, and set the camera ra-\ndius to 1.5 times their standard deviation; (ii) Center the\noverhead camera so that its principal axis lies in the same\nplane as the input camera, and offset in front of the input\nview by the mean of the room envelope depths; (iii) Select\na square bounding box in the overhead view that encloses\nPrecision Recall\nD1,2,3,4,5 & Overhead 0.221 0.358\nTulsiani et al. [45] 0.132 0.191\nTable 2: We quantitatively evaluate the synthetic-to-real\ntransfer of 3D geometry prediction on the ScanNet [4]\ndataset (threshold of 10cm). We measure recovery of true\nobject surfaces and room layouts within the viewing frus-\ntum.\n6\nFigure 7: Qualitative comparison of our viewer-centered,\nend-to-end scene surface prediction (left) and the object-\nbased detection and voxel shape prediction of [45] (right).\nObject-based reconstruction is sensitive to detection and\npose estimation errors, while our method is more robust.\nall points belonging to objects visible from the input view.\nNone of these heuristics worked perfectly for all training\nexamples, so we compute our ﬁnal overhead camera view\nvia a weighted average of these three candidates.\n5.2. Model Architecture and Training\nAs illustrated in Figure 2, given an RGB image, we ﬁrst\npredict a multi-layer depth map as well as a 2D seman-\ntic segmentation map. Features used to predict multi-layer\ndepths are then mapped through our EFT network to syn-\nthesize features for a virtual camera view, and predict an\northographic height map. We then use the multi-layer depth\nmap, semantic segmentation map, and overhead height map\nto predict a dense 3D reconstruction of the imaged scene.\nWe predict multi-layer depth maps and semantic seg-\nmentations via a standard convolutional encoder-decoder\nwith skip connections. The network uses dilated convolu-\ntion and has separate output branches for predicting each\ndepth layer using the Huber loss speciﬁed in Section 3.2.\nFor segmentation, we train a single branch network using a\nsoftmax loss to predict 40 semantic categories derived from\nthe SUNCG mesh labels (see supplement for details).\nOur overhead height map prediction network takes as in-\nput the transformed features of our input view multi-layer\ndepth map. The overhead model integrates 232 channels\n(see Figure 2) including epipolar transformations of a 48-\nchannel feature map from the depth prediction network,\na 64-channel feature map from the semantic segmentation\nnetwork, and the RGB input image. These feature maps are\nextracted from the frontal networks just prior to the predic-\ntive branches. Other inputs include a “best guess” overhead\nheight map derived from frontal depth predictions, and a\nmask indicating the support of the input camera frustum.\nThe frustum mask can be computed by applying the epipo-\nlar transform with F = 1, W = 1. The best-guess overhead\ndepth map can be computed by applying an unnormalized\ngating function W(s,t,z ) = z·δ[D1(s,t) = z] to the y-\ncoordinate feature F(s,t) = t.\nWe also train a model to predict the virtual camera pa-\nrameters which takes as input feature maps from our multi-\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nThreshold (meters)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Inliers\nPrecision\nD1\nD2\nD3\nD4\nD5\nOverhead\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nThreshold (meters)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Inliers\nPrecision\nD1, D2, D3, D4, Overhead\nTulsiani et al. (2018)\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nThreshold (meters)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Inliers\nRecall\nD1\nD2\nD3\nD4\nD5\nOverhead\n0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40\nThreshold (meters)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Inliers\nRecall\nD1, D2, D3, D4, Overhead\nTulsiani et al. (2018)\nFigure 8: Precision and recall of scene geometry as a func-\ntion of match distance threshold. Left: Reconstruction qual-\nity for different model layers. Dashed lines are the per-\nformance bounds provided by ground-truth depth layers\n( ¯D1, ¯D2, ¯D3, ¯D4, ¯D5). Right: Accuracy of our model rel-\native to the state-of-the-art, evaluated against objects only.\nThe upper and lower band indicate 75th and 25th quantiles.\nThe higher variance of Tulsianiet al. [45] may be explained\nin part by the sensitivity of the model to having the correct\ninitial set of object detections and pose estimates.\ndepth prediction network, and attempts to predict the target\noverhead viewpoint (orthographic translation (tx,ty) and\nfrustum radius σ) chosen as in Section 5.1. While the EFT\nnetwork is differentiable and our ﬁnal model can in prin-\nciple be trained end-to-end, in our experiments we simply\ntrain the frontal model to convergence, freeze it, and then\ntrain the overhead model on transformed features without\nbackpropagating overhead loss back into the frontal-view\nmodel parameters. We use the Adam optimizer to train all\nof our models with batch size 24 and learning rate 0.0005\nfor 40 epochs. The Physically-based Rendering [57] dataset\nuses a ﬁxed downward tilt camera angle of 11 degrees, so\nwe do not need to predict the gravity angle. At test time, the\nheight of the virtual camera is the same as the input frontal\ncamera and assumed to be known. We show qualitative 3D\nreconstruction results on the SUNCG test set in Figure 5.\n5.3. Evaluation\nTo reconstruct 3D surfaces from predicted multi-layer\ndepth images as well as the overhead height map, we ﬁrst\nconvert the depth images and height maps into a point cloud\nand triangulate vertices that correspond to a2×2 neighbor-\nhood in image space. If the depth values of two adjacent\npixels is greater than a threshold δ·a, where δ is the foot-\nprint of the pixel in camera coordinates and a = 7, we do\nnot create an edge between those vertices. We do not pre-\ndict the room envelope from the virtual overhead view, so\nonly pixels with height values higher than 5 cm above the\nﬂoor are considered for reconstruction and evaluation.\n7\nPrecision Recall\nD1 0.525 0.212\nD1 & Overhead 0.553 0.275\nD1,2,3,4 0.499 0.417\nD1,2,3,4 & Overhead 0.519 0.457\nTable 3: Augmenting the frontal depth prediction with the\npredicted virtual view height map improves both precision\nand recall (match threshold of 5cm).\nMetrics. We use precision and recall of surface area as\nthe metric to evaluate how closely the predicted meshes\nalign with the ground truth meshes, which is the native\nformat for SUNCG and ScanNet. Coverage is determined\nas follows: We uniformly sample points on surface of the\nground truth mesh then compute the distance to the clos-\nest point on the predicted mesh. We use sampling den-\nsity ρ = 10000/meter2 throughout our experiments. Then\nwe measure the percentage of inlier distances for given a\nthreshold. Recall is the coverage of the ground truth mesh\nby the predicted mesh. Conversely, precision is the cover-\nage of the predicted mesh by the ground truth mesh.\n3D Scene Surface Reconstruction.To provide an upper-\nbound on the performance of our multi-layer depth repre-\nsentation, we evaluate how well surfaces reconstructed from\nground-truth depths cover the full 3D mesh. This allows us\nto characterize the beneﬁts of adding additional layers to the\nrepresentation. Table 1 reports the coverage (recall) of the\nground-truth at a threshold of 5cm. The left panels of Fig-\nure 8 show a breakdown of the precision and recall for the\nindividual layers of our model predictions along with the\nupper bounds achievable across a range of inlier thresholds.\nSince the room envelope is a large component of many\nscenes, we also analyze performance for objects (excluding\nthe envelope). Results summarized in Table 3 show that the\naddition of multiple depth layers signiﬁcantly increases re-\ncall with only a small drop in precision, and the addition of\noverhead EFT predictions boosts both precision and recall.\nAblation Study on Transformed Features. To further\ndemonstrate the value of the EFT module, we evaluate the\naccuracy of the overhead height map prediction while in-\ncrementally excluding features. We ﬁrst exclude channels\nthat correspond to the semantic segmentation network fea-\ntures and compare the relative pixel-level L1 error. We then\nexclude features from the depth prediction network, using\nonly RGB, frustum mask and best guess depth image. This\nbaseline corresponds to taking the prediction of the input\nview model as an RGB-D image and re-rendering it from\nthe virtual camera viewpoint. The L1 error increases re-\nspectively from 0.132 to 0.141 and 0.144, which show that\napplying the EFT to the whole CNN feature map outper-\nforms simple geometric transfer.\nComparison to the State-of-the-art.Finally, we compare\nthe scene reconstruction performance of our end-to-end ap-\nproach with the object-based Factored3D [45] method us-\ning their pre-trained weights, and converting voxel outputs\nto surface meshes using marching cubes. We evaluated\non 3960 examples from the SUNCG test set and compute\nprecision and recall on objects surfaces (excluding enve-\nlope). As Figure 8 shows, our method yields roughly 3x\nimprovement in recall and 2x increase in precision, pro-\nviding estimates which are both more complete and more\naccurate. Figure 7 highlights some qualitative differences.\nTo evaluate with an alternative metric, we voxelized scenes\nat 2.5cm3 resolution (shown in Figure 4). Using the voxel\nintersection-over-union metric, we see signiﬁcant perfor-\nmance improvements over Tulsiani et al. [47] (0.102 to\n0.310) on objects (see supplement for details).\nReconstruction on Real-world Images. Our network\nmodel is trained entirely on synthetically generated im-\nages [57]. We test the ability of the model to generalize\nto the NYUv2 dataset [26] via the promising comparison to\nTulsiani et al. [45] in Figure 6.\nWe additionally test our model on images from the Scan-\nNetv2 dataset [4]. The dataset contains RGB-D image se-\nquences taken in indoor scenes, as well as 3D reconstruc-\ntions produced by BundleFusion [5]. For each video se-\nquence from the 100 test scenes, we randomly sample 5%\nof frames, and manually select 1000 RGB images to com-\npare our algorithm to Tulsiani et al. [45]. We select images\nwhere the pose of the camera is almost perpendicular to the\ngravity orientation, the amount of motion blur is small, and\nthe image does not depict a close-up view of a single object.\nWe treat the provided 3D reconstructions within each view-\ning frustum as ground truth annotations. As summarized in\nTable 2, our approach has signiﬁcantly improved precision\nand recall to Tulsiani et al. [45].\n6. Conclusion\nOur novel integration of deep learning and perspective\ngeometry enables complete 3D scene reconstruction from\na single RGB image. We estimate multi-layer depth maps\nwhich model the front and back surfaces of multiple ob-\njects as seen from the input camera, as well as the room\nenvelope. Our epipolar feature transformer network geo-\nmetrically transfers input CNN features to estimate scene\ngeometry from virtual viewpoints, providing more complete\ncoverage of real-world environments. Experimental results\non the SUNCG dataset [41] demonstrate the effectiveness\nof our model. We also compare with prior work that pre-\ndicts voxel representations of scenes, and demonstrate the\nsigniﬁcant promise of our multi-view and multi-layer depth\nrepresentations for complete 3D scene reconstruction.\nAcknowledgements. This research was supported by NSF\ngrants IIS-1618806, IIS-1253538, CNS-1730158, and a\nhardware donation from NVIDIA.\n8\nReferences\n[1] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al. Shapenet:\nAn information-rich 3d model repository. arXiv preprint\narXiv:1512.03012, 2015. 3\n[2] Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single-\nimage depth perception in the wild. In Advances in Neural\nInformation Processing Systems (NeurIPS), pages 730–738,\n2016. 3\n[3] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese. 3d-r2n2: A uniﬁed approach\nfor single and multi-view 3d object reconstruction. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), 2016. 3\n[4] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nießner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. 3, 6, 8, 13\n[5] Angela Dai, Matthias Nießner, Michael Zollh ¨ofer, Shahram\nIzadi, and Christian Theobalt. Bundlefusion: Real-time\nglobally consistent 3d reconstruction using on-the-ﬂy surface\nreintegration. ACM Transactions on Graphics , 36(4):76a,\n2017. 8\n[6] Angela Dai, Charles Ruizhongtai Qi, and Matthias Nießner.\nShape completion using 3d-encoder-predictor cnns and\nshape synthesis. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n6545–6554, 2017. 3\n[7] Helisa Dhamo, Keisuke Tateno, Iro Laina, Nassir Navab, and\nFederico Tombari. Peeking behind objects: Layered depth\nprediction from a single image. Pattern Recognition Letters,\n2019. 3\n[8] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map\nprediction from a single image using a multi-scale deep net-\nwork. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), pages 2366–2374, 2014. 1, 3\n[9] Haoqiang Fan, Hao Su, and Leonidas Guibas. A point set\ngeneration network for 3d object reconstruction from a single\nimage. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 2463–2471,\n2017. 3\n[10] Valentin Gabeur, Jean-S ´ebastien Franco, Xavier Martin,\nCordelia Schmid, and Gregory Rogez. Moulding humans:\nNon-parametric 3d human shape estimation from single im-\nages. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), 2019. 3\n[11] Matheus Gadelha, Rui Wang, and Subhransu Maji. Multires-\nolution tree networks for 3d point cloud processing. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), September 2018. 2\n[12] Soumya Ghosh and Erik B Sudderth. Nonparametric learn-\ning for layered segmentation of natural images. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2272–2279, 2012. 3\n[13] Rohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab-\nhinav Gupta. Learning a predictable and generative vector\nrepresentation for objects. In Proceedings of the European\nConference on Computer Vision (ECCV) , pages 484–499.\nSpringer, 2016. 3\n[14] Saurabh Gupta, Pablo Arbel ´aez, Ross Girshick, and Jiten-\ndra Malik. Aligning 3d models to rgb-d images of cluttered\nscenes. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2015. 3\n[15] Christian H ¨ane, Shubham Tulsiani, and Jitendra Malik. Hi-\nerarchical surface prediction for 3d object reconstruction. In\nIEEE International Conference on 3D Vision (3DV) , pages\n412–420, 2017. 3\n[16] Phillip Isola and Ce Liu. Scene collaging: Analysis and syn-\nthesis of natural images with semantic layers. InProceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), pages 3048–3055, 2013. 3\n[17] Shahram Izadi, David Kim, Otmar Hilliges, David\nMolyneaux, Richard Newcombe, Pushmeet Kohli, Jamie\nShotton, Steve Hodges, Dustin Freeman, Andrew Davison,\net al. Kinectfusion: real-time 3d reconstruction and inter-\naction using a moving depth camera. In Proceedings of the\n24th annual ACM symposium on User interface software and\ntechnology, pages 559–568. ACM, 2011. 3\n[18] Hamid Izadinia, Qi Shan, and Steven M Seitz. Im2cad. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), 2017. 3\n[19] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. In Advances in Neural Infor-\nmation Processing Systems (NeurIPS), 2015. 4\n[20] Dinghuang Ji, Junghyun Kwon, Max McFarland, and Sil-\nvio Savarese. Deep view morphing. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 3\n[21] Huaizu Jiang, Erik Learned-Miller, Gustav Larsson, Michael\nMaire, and Greg Shakhnarovich. Self-supervised depth\nlearning for urban scene understanding. Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2018. 3\n[22] Abhishek Kar, Shubham Tulsiani, Joao Carreira, and Jiten-\ndra Malik. Category-speciﬁc object reconstruction from a\nsingle image. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR) , pages\n1966–1974, 2015. 3\n[23] Hiroharu Kato, Yoshitaka Ushiku, Tatsuya Harada, Andrew\nShin, Leopold Crestel, Hiroharu Kato, Kuniaki Saito, Kat-\nsunori Ohnishi, Masataka Yamaguchi, Masahiro Nakawaki,\net al. Neural 3d mesh renderer. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2018. 3\n[24] Chen-Hsuan Lin, Chen Kong, and Simon Lucey. Learn-\ning efﬁcient point cloud generation for dense 3d object re-\nconstruction. In AAAI Conference on Artiﬁcial Intelligence\n(AAAI), 2018. 3\n[25] Daniel Maturana and Sebastian Scherer. V oxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), pages 922–928, 2015. 3\n9\n[26] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob\nFergus. Indoor segmentation and support inference from\nrgbd images. In Proceedings of the European Conference\non Computer Vision (ECCV), 2012. 6, 8, 14\n[27] Andrea Nicastro, Ronald Clark, and Stefan Leutenegger. X-\nsection: Cross-section prediction for enhanced rgbd fusion.\nIn Proceedings of the IEEE International Conference on\nComputer Vision (ICCV), 2019. 3\n[28] Eunbyung Park, Jimei Yang, Ersin Yumer, Duygu Ceylan,\nand Alexander Berg. Transformation-grounded image gen-\neration network for novel 3d view synthesis. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), July 2017. 3\n[29] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classiﬁcation\nand segmentation. Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017. 2\n[30] Stephan R Richter and Stefan Roth. Matryoshka networks:\nPredicting 3d geometry via nested shape layers. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 1936–1944, 2018. 3\n[31] Gernot Riegler, Ali O Ulusoy, and Andreas Geiger. Octnet:\nLearning deep 3d representations at high resolutions. InPro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 6620–6629, 2017. 1\n[32] Lawrence G Roberts. Machine perception of three-\ndimensional solids. PhD thesis, Massachusetts Institute of\nTechnology, 1963. 3\n[33] Jason Rock, Tanmay Gupta, Justin Thorsen, JunYoung\nGwak, Daeyun Shin, and Derek Hoiem. Completing 3d\nobject shape from one depth image. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 2484–2493, 2015. 3\n[34] Samuel Rota Bul `o, Lorenzo Porzi, and Peter Kontschieder.\nIn-place activated batchnorm for memory-optimized training\nof dnns. In Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV), 2018. 12\n[35] Jonathan Shade, Steven Gortler, Li-wei He, and Richard\nSzeliski. Layered depth images. 1998. 4, 12\n[36] Daeyun Shin, Charless C. Fowlkes, and Derek Hoiem. Pix-\nels, voxels, and views: A study of shape representations for\nsingle view 3d object shape prediction. InProceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2018. 2, 3\n[37] Edward Smith, Scott Fujimoto, and David Meger. Multi-\nview silhouette and depth decomposition for high resolution\n3d object representation. In Advances in Neural Information\nProcessing Systems (NeurIPS), pages 6479–6489, 2018. 3\n[38] Edward J. Smith and David Meger. Improved adversarial\nsystems for 3d object generation and reconstruction. In 1st\nAnnual Conference on Robot Learning, CoRL 2017, Moun-\ntain View, California, USA, November 13-15, 2017, Proceed-\nings, pages 87–96, 2017. 3\n[39] Paul Smith, Tom Drummond, and Roberto Cipolla. Layered\nmotion segmentation and depth ordering by tracking edges.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 26(4):479–494, 2004. 3\n[40] Noah Snavely, Steven M Seitz, and Richard Szeliski. Mod-\neling the world from internet photo collections. Proceedings\nof the IEEE International Conference on Computer Vision\n(ICCV), 80(2):189–210, 2008. 3\n[41] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Mano-\nlis Savva, and Thomas Funkhouser. Semantic scene com-\npletion from a single depth image. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 1, 2, 3, 6, 8, 13, 14\n[42] Hang Su, Varun Jampani, Deqing Sun, Subhransu Maji,\nEvangelos Kalogerakis, Ming-Hsuan Yang, and Jan Kautz.\nSPLATNet: Sparse lattice networks for point cloud process-\ning. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 2530–2539,\n2018. 2\n[43] Hao Su, Fan Wang, Li Yi, and Leonidas Guibas. 3d-assisted\nimage feature synthesis for novel views of an object. arXiv\npreprint arXiv:1412.0003, 2014. 3\n[44] Deqing Sun, Erik B Sudderth, and Michael J Black. Layered\nsegmentation and optical ﬂow estimation over time. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 1768–1775, 2012. 3\n[45] Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei A\nEfros, and Jitendra Malik. Factoring shape, pose, and lay-\nout from the 2d image of a 3d scene. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2018. 2, 3, 6, 7, 8, 14\n[46] Shubham Tulsiani, Richard Tucker, and Noah Snavely.\nLayer-structured 3d scene inference via view synthesis. In\nProceedings of the European Conference on Computer Vi-\nsion (ECCV), 2018. 3, 12\n[47] Shubham Tulsiani, Tinghui Zhou, Alexei A Efros, and Ji-\ntendra Malik. Multi-view supervision for single-view re-\nconstruction via differentiable ray consistency. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2017. 3, 8\n[48] J. Y . A. Wang and E. H. Adelson. Representing moving im-\nages with layers. IEEE Transactions on Image Processing\n(TIP), 3(5):625–638, Sept. 1994. 3\n[49] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\nmodels from single rgb images.Proceedings of the European\nConference on Computer Vision (ECCV), 2018. 2, 3\n[50] Zhe Wang, Liyan Chen, Shaurya Rathore, Daeyun Shin,\nand Charless Fowlkes. Geometric pose affordance: 3d\nhuman pose with scene constraints. arXiv preprint\narXiv:1905.07718, 2019. 3\n[51] Jiajun Wu, Yifan Wang, Tianfan Xue, Xingyuan Sun, Bill\nFreeman, and Josh Tenenbaum. Marrnet: 3d shape recon-\nstruction via 2.5 d sketches. In Advances in Neural Informa-\ntion Processing Systems (NeurIPS), pages 540–550, 2017. 3\n[52] Jiajun Wu, Chengkai Zhang, Xiuming Zhang, Zhoutong\nZhang, William T Freeman, and Joshua B Tenenbaum.\nLearning 3D Shape Priors for Shape Completion and Recon-\nstruction. In Proceedings of the European Conference on\nComputer Vision (ECCV), 2018. 3\n[53] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3D\n10\nShapeNets: A deep representation for volumetric shapes. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , pages 1912–1920, 2015.\n3\n[54] Xinchen Yan, Jimei Yang, Ersin Yumer, Yijie Guo, and\nHonglak Lee. Perspective transformer nets: Learning single-\nview 3d object reconstruction without 3d supervision. InAd-\nvances in Neural Information Processing Systems (NeurIPS),\npages 1696–1704, 2016. 3\n[55] Bo Yang, Stefano Rosa, Andrew Markham, Niki Trigoni, and\nHongkai Wen. 3d object dense reconstruction from a single\ndepth view. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence (TPAMI), 2018. 3\n[56] Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Josh\nTenenbaum, Bill Freeman, and Jiajun Wu. Learning to re-\nconstruct shapes from unseen classes. In Advances in Neu-\nral Information Processing Systems (NeurIPS), pages 2263–\n2274, 2018. 3\n[57] Yinda Zhang, Shuran Song, Ersin Yumer, Manolis Savva,\nJoon-Young Lee, Hailin Jin, and Thomas Funkhouser.\nPhysically-based rendering for indoor scene understanding\nusing convolutional neural networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2017. 6, 7, 8, 12\n[58] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G.\nLowe. Unsupervised learning of depth and ego-motion from\nvideo. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2017. 3\n[59] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely. Stereo magniﬁcation: Learning view\nsynthesis using multiplane images. In ACM Transactions on\nGraphics (SIGGRAPH), 2018. 3, 12\n[60] Chuhang Zou, Ruiqi Guo, Zhizhong Li, and Derek Hoiem.\nComplete 3d scene parsing from single rgbd image. Interna-\ntional Journal of Computer Vision (IJCV), 2018. 3\n[61] Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, and\nDerek Hoiem. 3d-prnn: Generating shape primitives with\nrecurrent neural networks. In Proceedings of the IEEE In-\nternational Conference on Computer Vision (ICCV) , pages\n900–909, 2017. 3\n11\nAppendix\nA. System Overview\nWe provide an overview of our 3D reconstruction system\nand additional qualitative examples in our supplementary\nvideo (see project website).\nB. Training Data Generation\nAs we describe in Section 5.1 of our paper, we generate\nthe target multi-layer depth maps by performing multi-hit\nray tracing on the ground-truth 3D mesh models. If an ob-\nject instance is completely occluded (i.e. not visible at all\nfrom the ﬁrst-layer depth map), it is ignored in the subse-\nquent layers. The Physically-based Rendering [57] dataset\nignores objects in “person” and “plant” categories, so those\ncategories are also ignored when we generate our depth\nmaps. The complete list of room envelope categories (ac-\ncording to NYUv2 mapping) are as follows: wall, ﬂoor,\nceiling, door, ﬂoor mat, window, curtain, blinds, picture,\nmirror, ﬁreplace, roof, and whiteboard. In our experiments,\nall room envelope categories are merged into a single “back-\nground” category. In Figure 9, we provide a layer-wise 3D\nvisualization of our multi-layer depth representation. Fig-\nure 11 illustrates our surface precision-recall metrics.\nC. Representing the Back Surfaces of Objects\nWithout the back surfaces, ground truth depth layers\n( ¯D1,3,5) cover only 82% of the scene geometry inside the\nviewing frustum (vs. 92% including frontal surfaces — re-\nfer to Table 1 in our paper for full comparison). Figure 10(a)\nvisualizes ¯D1,3,5, without the back surfaces. This repre-\nsentation, layered depth image (LDI) [35], was originally\ndeveloped in the computer graphics community [35] as an\nalgorithm for rendering textured depth images using paral-\nlax transformation. Works based on prediction of LDI or\nits variants [46, 59] therefore do not represent the invisible\nback surfaces of objects. Prediction of back surfaces en-\nables volumetric inference in our epipolar transformation.\nD. Multi-layer Depth Prediction\nSee Figure 17 for network parameters of our multi-layer\ndepth prediction model. All batch normalization layers have\nmomentum 0.005, and all activation layers are Leaky Re-\nLUs layers with α = 0 .01. We use In-place Activated\nBatchNorm [34] for all of our batch normalization layers.\nWe trained the network for 40 epochs.\nE. Multi-layer Semantic Segmentation\nSee Figure 18 for network parameters of multi-layer se-\nmantic segmentation. We construct a binary mask for all\nFigure 9: Layer-wise illustration of our multi-layer depth\nrepresentation in 3D. Table 1 in our paper reports an empir-\nical analysis which shows that the ﬁve-layer representation\n( ¯D1,2,3,4,5) covers 92% of the scene geometry inside the\nviewing frustum.\nFigure 10: Illustration of ground-truth depth layers. (a, b):\n2.5D depth representation cannot accurately encode the ge-\nometry of surfaces which are nearly tangent to the viewing\ndirection. (b): We model both the front and back surfaces\nof objects as seen from the input camera. (c): The tangent\nsurfaces are sampled more densely in the additional virtual\nview (dark green). Table 3 in our paper shows the effect\nof augmenting the frontal predictions with the virtual view\npredictions.\nforeground objects, and deﬁne segmentation mask Ml as\nall non-background pixels at layer l. As mentioned in sec-\ntion 3.1, D1 and D2 have the same segmentation due to\nsymmetry, so we only segment layers 1 and 3. The purpose\nof the foreground object labels is to be used as a supervisory\nsignal for feature extraction Fseg, which is used as input to\nour Epipolar Feature Transformer Networks.\n12\nFigure 11: Illustration of our 3D precision-recall metrics.\nTop: We perform a bidirectional surface coverage evalu-\nation on the reconstructed triangle meshes. Bottom: The\nground truth mesh consists of all 3D surfaces within the\nviewing frustum and in front of the room envelope. We take\nthe union of the predicted meshes from different views in\nworld coordinates. This allows us to perform a layer-wise\nevaluation (e.g. Figure 8 in our paper).\nScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNet\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNetScanNet\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nFigure 12: Evaluation of 3D reconstruction on the Scan-\nNet [4] dataset, where green regions are detected objects\nand pink regions are ground truth.\nF. Virtual-view Prediction\nSee Figure 20 and 21 for network parameters of our\nvirtual-view height map prediction and segmentation mod-\nels. The height map prediction network is trained to mini-\nmize foreground pixel losses. At test time, the background\nmask predicted by the segmentation network is used to zero\nout the ﬂoor pixels. The ﬂoor height is assumed to be zero\nin world coordinates. An alternate approach is minimizing\nboth foreground and background losses and thus allowing\nthe height map predictor to implicitly segment the ﬂoor by\npredicting zeros. We experimented with both architectures\nand found the explicit segmentation approach to perform\nbetter.\nFigure 13: V olumetric evaluation of our predicted multi-\nlayer depth maps on the SUNCG [41] dataset.\n0.0 0.2 0.4 0.6 0.8 1.0\nVoxel IoU\n0\n200\n400\n600\n800Number of examples\nObject-based Reconstruction (Tulsiani, 2018)\n0.0 0.2 0.4 0.6 0.8 1.0\nVoxel IoU\n0\n200\n400\n600\n800\nOurs\nFigure 14: Distribution of voxel intersection-over-union on\nSUNCG (excluding room layouts). We observe that object-\nbased reconstruction is sensitive to detection failure and\nmisalignment on thin structures.\nG. Voxelization of Multi-layer Depth Predic-\ntion\nGiven a 10m3 voxel grid of resolution 400 (equivalently,\n2.5cm3) with a bounding box ranging from (-5,-5,-10) to\n(5,5,0) in camera space, we project the center of each voxel\ninto the predicted depth maps. If the depth value for the\nprojected voxel falls in the ﬁrst object interval (D1,D2) or\nthe occluded object interval (D3,D4), the voxel is marked\noccupied. We evaluate our voxelization against the SUNCG\nground truth object meshes inside the viewing frustum, vox-\n13\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n [Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018][Tulsiani, 2018]\n OursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOursOurs\nFigure 15: Evaluation of 3D scene reconstruction on the\nNYUv2 [26] dataset.\nelized using the Binvox software which implements z-buffer\nbased carving and parity voting methods. We also voxelize\nthe predicted Factored3D [45] objects (same meshes evalu-\nated in Figure 8 of our paper) using Binvox under the same\nsetting as the ground truth. We randomly select 1800 ex-\namples from the test set and compute the intersection-over-\nunion of all objects in the scene. In addition to Figure 4\nof our paper, Figure 13 shows a visualization of our voxels,\ncolored according to the predicted semantic labeling. Fig-\nure 14 shows a histogram of voxel intersection-over-union\nvalues.\nH. Predictions on NYU and SUNCG\nFigures 16 and 15 show additional 3D scene reconstruc-\ntion results. We provide more visualizations of our network\noutputs and error maps on the SUNCG dataset in the last\nfew pages of the supplementary material.\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInputInput\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\n 3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction3D Reconstruction\nFigure 16: Evaluation of 3D scene reconstruction on the\nSUNCG [41] dataset.\n14\n  \nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 48, 240, 320)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 48, 240, 320)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 768, 15, 20)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 15, 20)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 15, 20)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConcat\n(B, 1152, 30, 40)\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nYdepth\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 48, 240, 320)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 48, 240, 320)\nBN ReLU\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nFdepth\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 1, 240, 320)\nD1Fdepth\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 1, 240, 320)\nD2Fdepth\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 1, 240, 320)\nD3Fdepth\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 1, 240, 320)\nD4Fdepth\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 32, 240, 320)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 1, 240, 320)\nD5Fdepth\nInput  RGB Image\n(B, 3, 240, 320)\nDilated convolution\nConvolution\nFigure 17: Network architecture for multi-layer depth prediction. The horizontal arrows in the network represent skip\nconnections. This ﬁgure, along with following ﬁgures, is best viewed in color and on screen.\n  \nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 240, 320)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 240, 320)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 768, 15, 20)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 15, 20)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 15, 20)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConcat\n(B, 1152, 30, 40)\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 384, 30, 40)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 64, 60, 80)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 120, 160)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 240, 320)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 240, 320)\nBN ReLU\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nFseg\nInput  RGB Image\n(B, 3, 240, 320)\nDilated convolution\nConvolution\nConv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 80, 240, 320)\nS\nS1 = S[:, 0:40]\nS3 = S[:, 40:80]\nFigure 18: Network architecture for multi-layer semantic segmentation network. (Best viewed in color and on screen)\n  \nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 64, 240, 320)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2Fdepth\n(B, 48, 240, 320)\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 128, 120, 160)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 256, 60, 80)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 512, 30, 40)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 1024, 15, 20)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 1\nStride: 1\nPadding: 0 \n(B, 2048, 5, 8)\nAvgPool\nKernel: (5, 8)\nYdepth\n(B, 2048)\nFC\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 1024)\nBN ReLUReshape\n(B, 768, 15, 20)\nConcat\n(B, 3072)\nFC\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 3)\n(tx, tz, σ))\nDilated convolution\nConvolution\nFigure 19: Network architecture for virtual camera pose proposal network. (Best viewed in color and on screen)\n15\n  \nConv\nKernel: 5\nDilation: 4\nStride: 1\nPad: 10 \n(B, 64, 304, 304)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 304, 304)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 768, 19, 19)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 19, 19)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 19, 19)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConcat\n(B, 1024, 38, 38)\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 304, 304)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 304, 304)\nBN ReLU\nConcat\n(B, 384, 76, 76)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 192, 152, 152)\nDilated convolution\nConvolution\nConv\nKernel: 3\nDilation: 1\nStride: 1\nPad: -1 \n(B, 1, 300, 300)\nVG\n(B, 232, 300, 300)\nV\nFigure 20: Network architecture for virtual view surface prediction network. (Best viewed in color and on screen)\n  \nConv\nKernel: 5\nDilation: 4\nStride: 1\nPad: 10 \n(B, 64, 304, 304)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 304, 304)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU\nMaxPool\nKernel: 2\nStride: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 768, 19, 19)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 19, 19)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 768, 19, 19)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConcat\n(B, 1024, 38, 38)\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 256, 38, 38)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 3\nDilation: 2\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU Conv\nKernel: 3\nDilation: 1\nStride: 1 \n(B, 128, 76, 76)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 152, 152)\nBN ReLU\nBilinear\nUp-sample\nScale: 2\nConv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 304, 304)\nBN ReLU Conv\nKernel: 5\nDilation: 4\nStride: 1 \n(B, 64, 304, 304)\nBN ReLU\nConcat\n(B, 384, 76, 76)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 1152, 30, 40)\nConcat\n(B, 192, 152, 152)\nDilated convolution\nConvolution\nConv\nKernel: 3\nDilation: 1\nStride: 1\nPad: -1 \n(B, 40, 300, 300)\nVSG\n(B, 232, 300, 300)\nVS\nFigure 21: Network architecture for virtual view segmentation network. (Best viewed in color and on screen)\n16\n           Input RGB Image\n17\n           Input RGB Image\n18\n           Input RGB Image\n19\n           Input RGB Image\n20\n           Input RGB Image\n21\n            Input RGB Image\n22\n           Input RGB Image\n23\n           Input RGB Image\n24\n           Input RGB Image\n25\n           Input RGB Image\n26\n           Input RGB Image\n27\n           Input RGB Image\n28",
  "topic": "Epipolar geometry",
  "concepts": [
    {
      "name": "Epipolar geometry",
      "score": 0.8995461463928223
    },
    {
      "name": "Artificial intelligence",
      "score": 0.8021791577339172
    },
    {
      "name": "Computer vision",
      "score": 0.7394270300865173
    },
    {
      "name": "Computer science",
      "score": 0.7042427659034729
    },
    {
      "name": "View synthesis",
      "score": 0.6155535578727722
    },
    {
      "name": "RGB color model",
      "score": 0.5334622859954834
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5259739756584167
    },
    {
      "name": "Transformer",
      "score": 0.4304535984992981
    },
    {
      "name": "Image-based modeling and rendering",
      "score": 0.4186493158340454
    },
    {
      "name": "3D reconstruction",
      "score": 0.41382458806037903
    },
    {
      "name": "Image warping",
      "score": 0.41046133637428284
    },
    {
      "name": "Rendering (computer graphics)",
      "score": 0.36455681920051575
    },
    {
      "name": "Image (mathematics)",
      "score": 0.23092791438102722
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ],
  "cited_by": 7
}