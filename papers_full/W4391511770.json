{
  "title": "Transformers in health: a systematic review on architectures for longitudinal data analysis",
  "url": "https://openalex.org/W4391511770",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4201765470",
      "name": "Clauirton A. Siebra",
      "affiliations": [
        "University of Geneva",
        "Universidade Federal da Paraíba"
      ]
    },
    {
      "id": "https://openalex.org/A4225829849",
      "name": "Mascha Kurpicz-Briki",
      "affiliations": [
        "Bern University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2080491908",
      "name": "Katarzyna Wac",
      "affiliations": [
        "University of Geneva"
      ]
    },
    {
      "id": "https://openalex.org/A4201765470",
      "name": "Clauirton A. Siebra",
      "affiliations": [
        "Universidade Federal da Paraíba"
      ]
    },
    {
      "id": "https://openalex.org/A4225829849",
      "name": "Mascha Kurpicz-Briki",
      "affiliations": [
        "Bern University of Applied Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2080491908",
      "name": "Katarzyna Wac",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6605755270",
    "https://openalex.org/W4210551435",
    "https://openalex.org/W3016187590",
    "https://openalex.org/W3125915762",
    "https://openalex.org/W4229455170",
    "https://openalex.org/W3015173574",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4388319941",
    "https://openalex.org/W4210762108",
    "https://openalex.org/W4206681774",
    "https://openalex.org/W3181868159",
    "https://openalex.org/W4221140999",
    "https://openalex.org/W3209901185",
    "https://openalex.org/W3118299366",
    "https://openalex.org/W6636364444",
    "https://openalex.org/W3186197713",
    "https://openalex.org/W2469314752",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3004227146",
    "https://openalex.org/W3197379538",
    "https://openalex.org/W4310645210",
    "https://openalex.org/W4306955484",
    "https://openalex.org/W4200440059",
    "https://openalex.org/W4383315874",
    "https://openalex.org/W3109919947",
    "https://openalex.org/W4210847455",
    "https://openalex.org/W2607170871",
    "https://openalex.org/W3089168780",
    "https://openalex.org/W4226275002",
    "https://openalex.org/W4221106857",
    "https://openalex.org/W4380320058",
    "https://openalex.org/W4226203366",
    "https://openalex.org/W3002855918",
    "https://openalex.org/W3125218497",
    "https://openalex.org/W4221163971",
    "https://openalex.org/W3160137267",
    "https://openalex.org/W3166996592",
    "https://openalex.org/W3186350470",
    "https://openalex.org/W3125666747",
    "https://openalex.org/W4200209389",
    "https://openalex.org/W2899876413",
    "https://openalex.org/W4308216456",
    "https://openalex.org/W3108032709",
    "https://openalex.org/W2972856209",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2990822734",
    "https://openalex.org/W3127869043",
    "https://openalex.org/W4385763767",
    "https://openalex.org/W4225319493",
    "https://openalex.org/W4312220150",
    "https://openalex.org/W4212850805",
    "https://openalex.org/W3093599560",
    "https://openalex.org/W4205765055",
    "https://openalex.org/W2909240409",
    "https://openalex.org/W3133650345",
    "https://openalex.org/W4200623485",
    "https://openalex.org/W4226325015",
    "https://openalex.org/W4225851481"
  ],
  "abstract": "Abstract Transformers are state-of-the-art technology to support diverse Natural Language Processing (NLP) tasks, such as language translation and word/sentence predictions. The main advantage of transformers is their ability to obtain high accuracies when processing long sequences since they avoid the vanishing gradient problem and use the attention mechanism to maintain the focus on the information that matters. These features are fostering the use of transformers in other domains beyond NLP. This paper employs a systematic protocol to identify and analyze studies that propose new transformers’ architectures for processing longitudinal health datasets, which are often dense, and specifically focused on physiological, symptoms, functioning, and other daily life data. Our analysis considered 21 of 456 initial papers, collecting evidence to characterize how recent studies modified or extended these architectures to handle longitudinal multifeatured health representations or provide better ways to generate outcomes. Our findings suggest, for example, that the main efforts are focused on methods to integrate multiple vocabularies, encode input data, and represent temporal notions among longitudinal dependencies. We comprehensively discuss these and other findings, addressing major issues that are still open to efficiently deploy transformers architectures for longitudinal multifeatured healthcare data analysis.",
  "full_text": "Vol.:(0123456789)\nArtificial Intelligence Review (2024) 57:32\nhttps://doi.org/10.1007/s10462-023-10677-z\n1 3\nTransformers in health: a systematic review on architectures \nfor longitudinal data analysis\nClauirton A. Siebra1,2 · Mascha Kurpicz‑Briki3 · Katarzyna Wac1\nPublished online: 3 February 2024 \n© The Author(s) 2024\nAbstract\nTransformers are state-of-the-art technology to support diverse Natural Language Process-\ning (NLP) tasks, such as language translation and word/sentence predictions. The main \nadvantage of transformers is their ability to obtain high accuracies when processing long \nsequences since they avoid the vanishing gradient problem and use the attention mecha-\nnism to maintain the focus on the information that matters. These features are fostering \nthe use of transformers in other domains beyond NLP. This paper employs a systematic \nprotocol to identify and analyze studies that propose new transformers’ architectures for \nprocessing longitudinal health datasets, which are often dense, and specifically focused on \nphysiological, symptoms, functioning, and other daily life data. Our analysis considered \n21 of 456 initial papers, collecting evidence to characterize how recent studies modified or \nextended these architectures to handle longitudinal multifeatured health representations or \nprovide better ways to generate outcomes. Our findings suggest, for example, that the main \nefforts are focused on methods to integrate multiple vocabularies, encode input data, and \nrepresent temporal notions among longitudinal dependencies. We comprehensively discuss \nthese and other findings, addressing major issues that are still open to efficiently deploy \ntransformers architectures for longitudinal multifeatured healthcare data analysis.\nKeywords Transformers · Deep learning · Health datasets · Longitudinal analysis\n * Clauirton A. Siebra \n clauirton.dealbuquerque@unige.ch\n Mascha Kurpicz-Briki \n mascha.kurpicz@bfh.ch\n Katarzyna Wac \n katarzyna.wac@unige.ch\n1 Quality of Life Technologies Lab, University of Geneva, 1227 Carouge, Switzerland\n2 Informatics Center, Federal University of Paraiba, João Pessoa 58055-000, Brazil\n3 Applied Machine Intelligence Research Group, Bern University of Applied Sciences, Höheweg 80, \n2502 Biel/Bienne, Switzerland\n C. A. Siebra et al.\n1 332 Page 2 of 39\n1 Introduction\nLongitudinal data implies continuous assessments repeated over time. This type of data is \ncommon in the health area, and its major advantage is its capacity to separate cohort and \ntemporal effects in the context of the analyses (Diggle et al. 2002). For example, longitu-\ndinal data is part of clinical studies that follow a group of patients with diabetes over five \nyears to track changes in their blood sugar levels and complications. Longitudinal data con-\ntrast with cross-sectional data in which a single outcome is measured for each individual. \nAn example of cross-sectional study may determine the prevalence of hypertension among \nadults living in a specific metropolitan area, by collecting data at a single point in time and \nproviding a snapshot of the population’s hypertension status, rather than following all the \nindividuals over time. Thus, longitudinal data analysis can generate important conclusions \nfor health personnel from a temporal perspective. For example, the study of Zhao et  al. \n(2019) relies on longitudinal Electronic Health Records (EHR) and genetic data to cre-\nate a model for 10-years cardiovascular disease event prediction. Similarly, Severson et al. \n(2021) employed longitudinal data collected for up to seven years to develop a Parkinson’s \ndisease progression model for intra-individual and inter-individual variability and medica-\ntion effects. Perveen et al. (2020) aimed to create models that provide predictions concern-\ning the future condition of pre-diabetic individuals. They exploited sequences of clinical \nmeasurements obtained from longitudinal data from a sample of patients. According to all \nthese studies, prognostic modeling techniques are important decision support tools to iden-\ntify a prior patients’ health status and characterize progression patterns. In other words, \nthey support the health personnel by predicting future health conditions that could guide \nthe implementation of preventive and adequate interventions.\nThese and other recent research efforts have in common the use of machine learning \ntechniques. The main example is the family of deep learning recurrent neural networks \n(RNNs), specially designed to provide a tractable solution to handle longitudinal data \n(Mao and Sejdić 2022). RNNs support tasks such as sequence classification, anomaly \ndetection, decision-making, and status prediction. These tasks rely on identifying tem-\nporal patterns and modeling nonstationary dynamics of human contexts (e.g., physi-\ncal, physiological, mental, social, and environmental), providing a way to understand \ncomplex time variations and dependencies. The literature brings several derivations \nof RNNs already employed in the health area. For example, long short-term memory \n(LSTM) networks are a type of RNN capable of learning order dependence in long \nsequence prediction problems. Guo et al. (2021) used LSTM models to predict future \ncardiovascular health levels based on previous measurements from longitudinal elec-\ntronic health record (EHR) data. Gated Recurrent Units (GRU) are derivations of RNN \nthat use gates to control the flow of information, deciding what information should be \npassed to the output. GRU was used, for example, for early detection of post-surgical \ncomplications using longitudinal EHR data (Chen et al. 2021a). Bidirectional RNNs \ncan analyze longitudinal data in both directions, and they were used, for example, to \ndetect medical events (e.g., adverse drug events) in EHR (Jagannatha and Yu 2016). \nThis type of RNNs offers advantages in the health domain due to their ability to cap-\nture data from past and future time intervals. For example, consider the heart rate (HR) \nanomaly detection task. While unidirectional RNNs only consider data points from the \npast to detect an eventual problem, bidirectional RNNs consider both past and future \nheart rate measurements for each data point in an HR sequence. Thus, this approach \nallows a better understanding of the temporal context and dynamics of the patient’s \nTransformers in health: a systematic review on architectures…\n1 3\nPage 3 of 39 32\nheart rate. Some studies also use different RNN derivations in the same problem (e.g., \nlongitudinal prediction modeling of Alzheimer’s disease) to identify the best strategy \nin terms of accuracy (Tabarestani et al. 2019).\nRNNs have a strong ability to deal with longitudinal data since their recurrent \narchitecture can remember past information and utilize it to make predictions at future \ntime steps. Therefore, RNNs can model the temporal dependencies between parts of \nthe longitudinal data and understand how they evolve over time. However, they can-\nnot keep up with context and content for longer sequences. The main reason is that \nRNNs suffer from the vanishing gradient problem since long-term information must \nsequentially travel through all RNN units before generating results. Thus, such infor -\nmation will likely vanish by being multiplied many times by small values. RNN-like \nnetworks, such as LSTM and GRU, consider this problem, but their more complex \narchitectures still present sequential paths from older past units until the final one. For \nexample, mHealth applications daily assess different multifeature longitudinal data of \ntheir users, which generate multi-longitudinal sequences of health data (Wac 2016). In \nfive years, the longitudinal data sequence will have about 1825 timesteps. According \nto experimental results (Culurciello 2018), RNNs are good for remembering sentences \nin the order of hundreds but not thousands of timesteps. Moreover, this sequential flow \nthrough RNN units also brings performance problems since RNNs must process data \nsequentially. Thus, they cannot employ parallel computing hardware and graphics pro-\ncessing units (GPU) in training and inference.\nTransformers (Vaswani et al. 2017) are a recent type of deep neural network focused \non analyzing sequences. Their architecture allows the processing of entire sequences in \nparallel. Consequently, it is possible to scale the speed and capacity of such processing \ncompared to previous RNN-like approaches. Moreover, transformers introduced the atten-\ntion mechanism, which considers the relationship between attributes, irrespective of where \nthey are placed in a sequence. This mechanism allows tracking of the relations between \nattributes across long sequences in both forward and reverse directions. While transformers \nwere originally conceived to cover traditional problems of the Natural Language Process-\ning (NLP) area, such as text classification and named entity recognition, their high per -\nformance in dealing with sequential data encouraged the adaptation of this architecture to \nother areas that involve the analysis of longitudinal data, such as digital health.\nThis paper uses a systematic review protocol to identify and analyze studies that pro-\nposed adaptations for transformers’ architectures so they can handle longitudinal health \ndata. This protocol contains a set of research questions that guide the analysis of these \napproaches regarding their architectures, input vocabulary, aims, positional embedding \nimplementations, explainability, and other technical aspects. Moreover, this analysis also \nallows the identification of trends in the area, main limitations, and opportunities for \nresearch directions. Thus, our main contribution is to consolidate a body of knowledge that \nsupports advances in longitudinal health data analysis using transformers.\n2  Transformers background\nThis section summarizes the main concepts of transformers intending to enable a bet-\nter understanding of this review. These concepts are also used to formulate the research \nquestions of the research protocol (Sect. 3 ).\n C. A. Siebra et al.\n1 332 Page 4 of 39\n2.1  Encode‑decode architectures\nThe original transformer architecture (Fig.  1) is composed of two modules: an encoder \n(Fig. 1, left) and a decoder (Fig.  1, right). The basic unit of an encoder (Nx) has two sub-\nmodules: a multi-head attention (composed of multiple self-attention layers), followed by \na fully connected network (Feed Forward). Normalization layers and residual connections \nare also used in both sub-modules to stabilize the network over training. The encoder func-\ntion is to extract features from input sequences. To that end, the complete sequence is par -\nallelly processed at once. Each token of the sequence flows through its own path inside \nthe architecture but maintains dependencies on the paths of the other tokens. This strat-\negy enriches each token with contextual information from the whole sentence. Encoder \nunits (Nx) can be stacked on each other according to the task. For example, Bidirectional \nFig. 1  The original transformer \nencoder-decoder architecture \n(Vaswani et al. 2017). The left-\nside block represents the encoder, \nwhile the right-side block repre-\nsents the decoder\n\nTransformers in health: a systematic review on architectures…\n1 3\nPage 5 of 39 32\nEncoder Representations from Transformers (BERT) uses a stack of 12 encoders in its \nbasic version (Devlin et al. 2018).\nThe decoder and encoder architectures are very similar, and decoder units (Nx) can also \nbe stacked on each other. However, the decoder outputs one token at a time since each \noutput token becomes part of the next decoder input (auto-regressive process). Over this \nprocess, the vector of features from the encoder supports the decoder in focusing on the \nappropriate positions of the input sequence. We suggest the original paper “Attention is All \nYou Need” (Vaswani et al. 2017) for details about these components.\nTransformers can be classified into three types according to the modules that they \nimplement: encoder-only (only the encoder module is implemented), decoder-only (only \nthe decoder module is implemented), and encoder-decoder (the complete architecture is \nimplemented). Apart from this classification, architectural designs can modify one or more \nlayers according to the target task.\n2.2  Vocabulary\nTransformers were initially designed to support the solution of natural language processing \nproblems. Thus, their inputs and outputs are tokens (words) that compose a vocabulary. For \nexample, the illustrated Softmax layer (Fig. 1) produces probabilities over an output vocab-\nulary during a language translation task. In NLP problems, this vocabulary corresponds \nto the lexicon of a language such as English or French. Similarly, programming language \ncode generation using transformers (Svyatkovskiy et al. 2020) employs the tokens of the \nprogramming language as vocabulary. Observe that vocabularies are neither compulsory \nnor necessarily composed of textual tokens. For example, the study in Bao et al. (2021) \nshows that patches of an original image are employed as visual tokens. Therefore, like nat-\nural language, images are represented as sequences of discrete tokens obtained by an image \ntokenizer. Transformers that use health images as input are out of the scope of this paper. A \nreview on this subject can be seen in He et al. (2022).\n2.3  Input embedding\nThe input embedding layer is a type of lookup table that contains vectorial representations \nof input data (e.g., each term of the vocabulary). This layer is essential because transform-\ners process vectors of continuous values like any other machine learning algorithm. There \nare several proposals for input embeddings, which can be classified into context-independ-\nent (traditional) and context-dependent (contextualized) embeddings (Wang et  al. 2020). \nWhile the former produces unique and distinct representations for each token without con-\nsidering its context; the latter learns different embeddings for the same token according \nto its context (Fig.  2). Transformers also allow each sequential input to contain multiple \nembeddings. This approach is essentially a kind of early fusion (sum or concatenation) of \nembeddings. The papers discussed in our review bring several examples in such a direction.\n2.4  Positional encoding\nThe positional encoding layer adds a positional vector to each set of inputs assessed simul-\ntaneously. This step is essential since transformers process all the inputs in parallel, unlike \nRNN or LTSM approaches, where inputs are fed in sequence. While these techniques do \n C. A. Siebra et al.\n1 332 Page 6 of 39\nnot require any specific positional strategy because they already understand the sequence \nof the inputs, transformers need this additional information. The use of sequential indexes \nis the simplest strategy to generate positional encodings. It works well if the number \nof sequential inputs is low. However, it becomes problematic as the number of inputs \nincreases since the high values of the latter positional encodings may dominate the initial \nvalues, distorting the final results. A simple solution is to convert the encoding values as a \nfraction of length, i.e., index/w, where w is the number of inputs. However, several appli-\ncations do not know this w value a priori. Strategies such as frequency-based positional \nencodings (Vaswani et al. 2017) avoid this issue. The study in Dufter (2021) brings a com-\nprehensive discussion about positional encoding.\n2.5  Target task\nTransformers are used to different tasks, which usually affect how their architectures are \ndesigned. Some tasks, such as question answering and text translation, are specific for nat-\nural language processing. Apart from that, tasks that can use longitudinal data are:\n• Classification of sequences: This task usually employs architectures that have the soft-\nmax as the activation function of their final layer. Such a function returns probabilities \nover pre-defined classes according to the input sequence (Prakash et al. 2021).\n• Language modeling: This is the traditional strategy used to train transformers models. \nThe aim is to fit a model to a corpus, which normally is domain specific (Yang et al. \n2022). This strategy is also useful for inputting missing data on health records (Liu \net al. 2023).\n• Prediction: Given a sequence of tokens, the aim is to predict the next token of this \nsequence. Thus, architectures are designed to only attend to the left context (tokens on \nthe left of the current position). The prognostic of a disease is an example of this task \n(Florez et al. 2021).\n2.6  Training strategies\nThe original training process of transformers is conducted in two phases. First, they are \npretrained using strategies such as Masked Language Modeling (MLM) and Next Sentence \nPrediction (NSP) (Devlin 2018). The transformer model is trained to predict masked words \nFig. 2  Difference between context-independent and dependent embeddings. The left case shows that each \nword has only one entry in the lookup table. On the right, the same token can have multiple entries accord-\ning to its context (neighborhood tokens indicate if orange is a noun or an adjective)\nTransformers in health: a systematic review on architectures…\n1 3\nPage 7 of 39 32\nin the former strategy. In the latter strategy, the model must predict whether two sentences \nfollow each other. After the pretraining, the model is fine-tuned to a specific task, replacing, \nfor example, part of the language model with additional linear layers (traditional machine \nlearning training). However, transformer architectures can also be designed to avoid the \npretraining phase. In this case, they directly learn using backward propagation, such as the \narchitecture described in Dong et al. (2021) that predicts early signs of generalized anxiety \ndisorder and depression.\n2.7  Validation and comparisons\nA common approach to validate the results obtained using transformers is to compare such \nresults with outcomes of other machine learning methods. This approach was found in \nalmost all the reviewed papers, as discussed later in this paper (Sect.  4.5). Indeed, longi-\ntudinal data analysis in health does not have a “gold standard” machine learning method \nthat could be used as a comparisons baseline. The choice of this depends on the specific \nresearch question, the nature of the health data, and the goals of the analysis. A useful \nway to verify the effectiveness of transformers is by comparing their results with outcomes \ngenerated by clinicians. However, only two papers (Li et  al. 2020; Rasmy et  al. 2021) \nemployed clinicians during the validation process. Section  4.5 discusses this topic in more \ndetail.\n3  Research method\nA systematic review protocol, as stated by Kitchenham (2004), was used as the research \nmethod in this paper. The steps of this method follow the schema illustrated in Fig. 3.\nAccording to Kitchenham, the formulation of research questions is the first and most \nimportant activity of the protocol definition. We have defined such questions considering \nfour different perspectives. Demographical questions focus on metadata aspects of the \npaper. These questions are:\n• DemRQ1: When was the paper published (year)?\n• DemRQ2: Where was the paper published (journal or conference name and impact fac-\ntor)?\nFig. 3  Systematic review schema applied\n C. A. Siebra et al.\n1 332 Page 8 of 39\n• DemRQ3: What is the main research group/university?\n• DemRQ4: Which is the objective and scientific contribution?\nInput questions focus on the format and organization of the input data. These ques-\ntions are:\n• InpRQ1: Which is the input vocabulary and how was it created?\n• InpRQ2: Which is the data unit and frequency (periodic or aperiodic)?\n• InpRQ3: Are static attributes also employed?\n• InpRQ4: Which is the strategy used to implement the positional encoding?\n• InpRQ5: How are these attributes embedded?\nArchitectural questions characterize the organization of the network layers and pos-\nsible adaptations regarding the traditional transformer architecture (Fig. 1 ):\n• ArcRQ1: Which is the architectural type (Encoder-only, Decoder-only, Encoder-\nDecoder)?\n• ArcRQ2: How are the Nx modules (see Fig. 1 ) organized?\n• ArcRQ3: How does the proposed architectural design support the learning objective \n(task)?\nEvaluation questions focus on the training process and evaluation aspects of the \ntrained model:\n• EvaRQ1: What are the characteristics of the dataset used for evaluation (e.g., size)?\n• EvaRQ2: What is the pretraining strategy?\n• EvaRQ3: What is the fine-tuning strategy?\n• EvaRQ4: Which are the evaluation criteria?\n• EvaRQ5: Are there comparisons to other approaches?\nExplainability question tries to identify if the approach presents ways to explain its \nresults.\n• ExpRQ1: Is the interpretability/explainability of the model discussed?\nThe next step is the search string definition. We relied on general terms associated \nwith the research questions to compose such a string (Kitchenham 2004). This strategy \navoids bias and low covering. After a stage of string calibration, where we tried sev -\neral terms’ combinations, the resultant string was: (transformer) AND (“deep learning” \nOR “neural network” OR “machine learning”) AND (health OR medical OR patient). \nWe defined four datasets to use this string (ScienceDirect, IEEE Xplore Digital Library, \nACM Digital Library, and PubMed), and three stages for paper selection, which are:\n• Stage 1: Automatic match process using the search string. Only the title and abstract \nwere considered as the search space for this match. Moreover, we only considered \nfull papers written in English and dated from 2018 to 2023.\n• Stage 2: Manual title and abstract screening of the papers that resulted from the pre-\nvious stage. The aim was to identify papers that are related to our research area and \nTransformers in health: a systematic review on architectures…\n1 3\nPage 9 of 39 32\nhave the potential to answer our research questions. This stage only considered pri-\nmary studies and dropped out duplicate papers.\n• Stage 3: Remaining papers from the previous stage were fully analyzed to extract the \ndata that could answer the research questions. Papers that did not contain enough infor-\nmation were dropped out.\nThe temporal search range was defined from 2018 to 2023 since the studies about \ntransformers were initiated after the seminal paper of Vaswani et  al. (2017), released in \nDecember 2017. Two reviews separately conducted the paper selection over stages 1 and \n2 to avoid bias. A third reviewer participated in cases of disagreements, looking for a \nconsensus.\n4  Results\nThis section first discusses the selection process conducted in this review. Then, we con-\nsolidate the results obtained in the four sets of research questions (demographical, input, \narchitectural, evaluation, and explainability), emphasizing their main remarks.\n4.1  Selection process\nThe selection of papers based on search string (stage 1) returned 456 papers. After analyz-\ning their title and abstract, 59 papers were chosen for a more detailed analysis (stage 2). \nWe selected 21 papers (stage 3) from this set to compose our study. Most of the 38 papers \ndiscarded during stage 3 are related to NLP (14 papers) and image-based (6 papers) appli-\ncations in health. However, their approaches do not process longitudinal multifeature data. \nThe next paragraphs discuss some of the discarded papers, emphasizing the rationale of \nour decisions and better characterizing the scope of this review.\nPapers related to NLP discuss mechanisms to process textual health documents (e.g., \nEHR notes). As we could expect, since transformers come from the NLP area, the ini-\ntial use of this technology in health was focused on tasks such as identifying similar sen-\ntences (Mahajan et al. 2020), named entity recognition (Falissard et al. 2022), summariza-\ntion (Chen et al. 2020), and classification based on text analysis. For example, the work in \nMahajan et al. (2020) relied on ClinicalBERT (Huang et al.2019), a pre-trained domain-\nspecific transformer-based language model for identifying semantic textual similarity in the \nclinical domain and redundant information in clinical notes. The work in Falissard et al. \n(2022) focused on automatically recognizing ICD-10 medical entities from the French nat-\nural language using transformers. The work in Chen et al. (2020) relied on a BERT-based \nstructure to build a diagnoses-extractive summarization model for hospital information \nsystems. Regarding classification based on text analysis, the work in Shibly et al. (2020) \ncreated a transfer learning system by fine-tuning the BERT language model. The aim was \nto identify the right doctor, in terms of clinical expertise, who should receive the prelimi-\nnary textual diagnosis description.\nPapers that present image-based approaches investigate the use of transformers for \nmedical image analysis. For example, the work of Liu et al. (2021a) investigates the use \nof transformer layers to optimize the extracted features of breast cancer tumor images. \nThe work of Fu et al. (2022) uses a transformer-encoded Generative Adversarial Network \n(transGAN) to reconstruct low-dose PET (L-PET) images into high-quality full-dose PET \n C. A. Siebra et al.\n1 332 Page 10 of 39\n(F-PET). W-Transformer (Yao et al. 2022) and xViTCOS (Mondal et al. 2021) are other \nexamples of transformer-based works focused on health image analysis. While the for -\nmer integrates convolutional and transformers layers to detect spinal deformities in X-ray \nimages, the latter uses a transfer learning technique (pre-trained transformer model) for \nCOVID-19 screening tests that rely on chest radiography.\nApart from papers of these two groups, other papers that use transformers, for example, \nto analyze types of sounds (e.g., cough sounds for disease detection (Yan et al. 2022)) and \ndefine new health corpora to train transformers (Wang et  al. 2019), were also discarded \nsince they do not consider multifeature longitudinal health data. This means simultane-\nous data streams (e.g., physiological, physical, psychological, social, or contextual) that are \nassessed over long periods in daily life.\n4.2  Demographical research questions\nThe 21 remaining papers are temporally distributed according to their year of publication \n(DemRQ1) as follows: three papers in 2020, fourteen papers in 2021, three papers in 2022, \nand one in 2023. This temporal distribution emphasizes that no paper was identified in the \ninitial two years of our search (2018 and 2019). This fact suggests that the transformers \ntechnology had a maturation time until research groups identified its potential for analyzing \nother than NLP longitudinal health data.\nThe quality of the publication vehicles (DemRQ2) is a factor that indicates the impor -\ntance of the technology. Our analysis identified that most of these 21 papers were pub-\nlished in standing journals with high-impact factors, such as the IEEE Transactions on \nNeural Networks and Learning Systems (IF = 14.25) (Rao et al. 2022a, b), IEEE Journal \nof Biomedical and Health Informatics (IF = 7.021) (Rao et al. 2022a, b; Meng et al. 2021; \nDarabi et al. 2020; Li et al. 2023a, b), Scientific Reports (IF = 4.996) (Li et al. 2020; Zeng \net  al. 2022), Nature Digital Medicine (IF = 11.653) (Rasmy et  al. 2021), and Journal of \nBiomedical Informatics (IF = 8.000) (Li et al. 2021). Similarly, papers were also published \nin important conferences such as the ACM SIGKDD Conference on Knowledge Discovery \nand Data Mining (IF = 7.02) (Ren et al. 2021), AAAI Conference on Artificial Intelligence \n(IF = 3.4) (Prakash et al. 2021), International Conference on Machine Learning and Appli-\ncations (IF = 3.844) (Fouladvand et al. 2021; Dong et al. 2021), and International Sympo-\nsium on Computer-Based Medical Systems (IF = 1.040) (Florez et al. 2021).\nThe identification of several papers from the same research group (DemRQ3) shows \nthe ongoing efforts and technological developments rather than a paper resulting from a \none-off, isolated study. We found only one case in our analysis. The University of Oxford \nreleased the BEHRT model in 2020 (Li et al. 2020) and advanced this model in two works \npublished in 2022 (Rao et al. 2022a, b) and another in 2023 (Li et al. 2023a, b). The Gradu-\nate Institute of Biomedical Electronics and Bioinformatics at the National Taiwan Univer -\nsity used transformer models for two disease pattern retrieval and classification in 2021 \n(Boursalie et al. 2021) and prediction of postoperative mortality in 2022 (Chen et al. 2022). \nHowever, our analysis did not consider this latter application since it does not use longitu-\ndinal data. DemRQ3 was also useful to attest to the strong interdisciplinarity of this type \nof research. Many papers involve authors that are from some health-related department/\nfaculty such as Medicine (Li et al 2020; Li et al 2021; Rasmy et al. 2021), Women’s and \nReproductive Health (Rao et al. 2022a, b), Radiologic Sciences (Meng et al. 2021), and \nBiomedical Informatics (Boursalie et al. 2021). The most interesting example is the study \nin Fouladvand et  al. (2021), with authors from the Biomedical Informatics, Computer \nTransformers in health: a systematic review on architectures…\n1 3\nPage 11 of 39 32\nScience, Internal Medicine, Pharmaceutical Sciences, Biostatistics, Psychiatry, Family and \nCommunity Medicine departments. In this case, the involvement of interdisciplinary teams \nis related to the research complexity that involves aspects of opioid use disorder.\nFinally, Table  1 summarizes our findings regarding the learning objective (task) and \napplication domain (DemRQ4). This table shows that prediction is the most common \nlearning task, while the scientific contributions are mainly focused on:\n• Designing domain-specific vocabularies (e.g., diagnosis codes for EHR) and their \nembeddings using simple operations such as sum or concatenation (Li et al 2020; Rao \net al. 2022a; Florez et al. 2021; Meng et al. 2021; Rasmy et al. 2021).\n• Including temporal semantic elements, such as temporal distance between inputs (Bour-\nsalie et al. 2021; An et al. 2022), artificial time tokens as part of the vocabulary, (Pang \net al. 2021), time reference for an event from defined anchor date (Prakash et al. 2021), \nand special representations for duration of visits and interval between visits (Peng et al. \n2021).\n• Conducting a more complex input data preprocessing, using, for example, LSTM (Fou-\nladvand et al. 2021) or a pre-transformer (Chen et al. 2021b) module, including spa-\ntial dependences (Li et al. 2021), creating hierarchical input structures (Ye et al. 2020; \nLi et al. 2023a, b), filtering relevant information (Shome 2021), using self-supervised \nlearning to augment the input value (Dong et al. 2021, and joining categorical and tex-\ntual information (Darabi et al. 2020).\n• Modifying the learning algorithm to simultaneously analyze different data streams \n(Fouladvand et al. 2021), employing an attention mechanism based on probabilities (Li \net al. 2021), or proposing new forms of pre-training (Zeng et al. 2022; Ren et al. 2021).\nThese scientific contributions are better discussed along this paper.\n4.3  Transformers input research questions\nTable 2 summarizes the answers to the input research questions. The second column (Lon-\ngitudinal inputs) indicates the vocabulary (lexicon) used in each approach (InpRQ1). \nMany papers (Li et al. 2020; Rao et al. 2022a, Florez et al. 2021, Pang et al. 2021, Prakash \net al. 2021) use standardized categorical codes of diagnosis (e.g., ICD), medications, and \nother health elements as part of their vocabulary. Some papers mix categorical and con-\ntinuous data (Li et al. 2023a, b; Rao et al. 2022b). In this case, they apply a categorization \nprocess to transform the continuous data into tokens of a vocabulary. Works that assess \ndata from sensors (Shome 2021; Dong et al. 2021; An et al. 2022] do not use the concept \nof vocabulary since their training strategies directly use the raw data as input. A third type \nof input representation strategy uses special modules to learn their input (FCNN (Chen \net al. 2021b), DTW (Li et al. 2021)).\nThe third column represents the longitudinal unit, which aggregates the data assessed at \neach timestep (InpRQ2). Many studies use the idea of visits as longitudinal unit, and they \nare aperiodic (Li et al. 2020; Darabi et al. 2020). For aperiodic units, for example, the work \nin Boursalie et al. (2021) proposes concatenating the time between assessments (elapsed \ntime) in the unit encode. Proposals based on sensors (Shome 2021; Dong et al. 2021) have \neach data capture cycle as their longitudinal unit, which is mostly periodic.\n C. A. Siebra et al.\n1 332 Page 12 of 39\nTable 1  Learning tasks and main contribution\nReferences Learning objective Scientific contribution\nLi et al. (2020) Prediction\n(Next diagnosis)\nAdaptation of BERT (Devlin et al. 2018) to cover EHR elements (diagnosis codes)\nRao et al. (2022a) Prediction\n(Heart failure)\nExtends the previous model (Li et al. 2020) so it considers further EHR elements\nFlorez et al. (2021) Prediction (Prognostic) Transformer architecture (without pretraining) to cover EHR elements (e.g., diagnosis codes)\nMeng et al. (2021) Prediction (Depression) Extends the input code embeddings for bidirectional sequential learning\nFouladvand et al. (2021) Binary classification (Opioid disorder) Data preprocessing using LSTM models. Simultaneous analysis of multiple types of healthcare data \nstreams\nBoursalie et al. (2021) Prediction\n(Next diagnosis)\nInclusion of elapsed time between longitudinal units (e.g., medical visits) embeddings\nChen et al. (2021b) Prediction\n(Critical care)\nUse of a first transformer model for unsupervised embedding of the disease concept into a second trans-\nformer\nLi et al. (2021) Prediction (Influenza propagation) Inclusion of spatial dependency, using curve similarity, as part of the embedding. Adaptation of attention \nmechanism for a probabilistic mode\nYe et al. (2020) Prediction\n(Disease risk)\nAdditional modules to model the hierarchical structure of EHR data and extract long- (transformer \nencoder) and short-term dependences (convolutional layer)\nRasmy et al. (2021) Prediction\n(Disease risk)\nAdaptation of BERT (Devlin et al. 2018) to cover EHR elements (e.g., diagnosis codes)\nZeng et al. (2022) Prediction\n(Next diagnosis)\nProposal of unsupervised pretraining that relies on the Next Visit Prediction and Categorical Prediction \napproaches\nShome (2021) Classification (Physical activity) Use of a depthwise separable residual feature extractor network for input preprocessing\nDong et al. (2021) Prediction\n(Mental disease)\nGraphs as input. Integrates a contrastive self-supervised learning method such that the bag classifier can be \nlearned using semi-supervision\nDarabi et al. (2020) Prediction\n(Medical events)\nUse of two input distributed representation for a patient based on text and medical codes\nLi et al. (2023a, b) Prediction\n(Risk of health issues)\nUse of hierarchical structures to allow extracting associations from longer sequences\nPang et al. (2021) Prediction\n(Clinical events)\nIncorporating temporal information using artificial time tokens as part of the input vocabulary\nTransformers in health: a systematic review on architectures…\n1 3\nPage 13 of 39 32\nTable 1  (continued)\nReferences Learning objective Scientific contribution\nRen et al. (2021) Classification and regression (diabetes \nand hypertension)\nProposal of new forms of pre-training to handle data insufficiency, data incompleteness and short sequence \nproblems using a time-aware transformer architecture\nPrakash et al. (2021) Classification\n(Rare disease)\nExtension of Med-BERT by including context embedding and temporal reference embedding, together \nwith a novel adaptive loss function to handle the class imbalance\nRao et al. (2022b) Prediction\n(Treatment risks)\nUse of an unsupervised learning for richer feature extraction, which capture both static and temporal medi-\ncal history variables\nAn et al. (2022) Prediction\n(Mortality)\nLearn the personalized irregular temporal patterns of medical events\nPeng et al. (2021) Prediction\n(Diagnosis)\nUse of neural ordinary differential equation to handle both irregular intervals between a patient’s visits with \nadmitted timestamps and length of stay in each visit\n C. A. Siebra et al.\n1 332 Page 14 of 39\nTable 2  Summary of input research questions\nReferences Longitudinal inputs\n(lexicon = Γ)\nLongitudinal unit (t) and \nfrequency\nStatic attributes Positional encoding Input embedding\nLi et al. (2020) Diagnosis code (D) Patient visits Vp = list of D. \nAperiodic\n– f(t) = sinusoidal function. t \nis the Vp sequential value\nEVp ← ΓVp ⊕  AgeVp ⊕ f(t) ⊕ A/B \nsegment\nRao et al. (2022a) Diagnosis (D) and medica-\ntion (M) codes\nPatient visits Vp = list of \nD ∪ list of M. Aperiodic\n– – EVp ← ΓVp ⊕ Age-in-\nmonthsVp ⊕  YearVp\nFlorez et al. (2021) Diagnosis code (D) Patient admission Ap = list \nof D. List of fix size |D|-\ndimension. Uses padding \nto complete. Aperiodic\n– f (t) = sinusoidal function, t \nis the Vp sequential value\nEAp ← hot_vector(ΓAp) ⊕ f(t)\nMeng et al. (2021) Diagnosis (D), dCPT (C), \nmedication (M), and \naTopic features (T) codes\nPatient visits Vp = list of \nD ∪ list of C ∪ list of \nM ∪ list of T. Aperiodic\nGender f(t) = sinusoidal function, t \nis the Vp sequential value\nEVp ← ΓVp ⊕ f(t) ⊕ A/B seg-\nment ⊕  AgeVp ⊕ Gender\nFouladvand et al. (2021) Diagnosis (D) and medica-\ntion (M) codes\nPatient visits Vp = list of \nD ∪ list of M. Periodic \n(monthly)\nbAge, Gender f(t) = month ∈  [monthmin, \n monthmax]. t is the Vp \nsequential value\nED ← LSTM(D) ⊕ f(t)\nEM ← LSTM(M) ⊕ f(t)\nEVp ←  ED ©  EM\nBoursalie et al. (2021) Diagnosis (D) and medica-\ntion (M) codes\nPatient visits Vp = list of \nD ∪ list of M ∪ elapsed \ntime. Aperiodic\ncAge, Gender f(t) = t, where t is the Vp \nsequential value\nEΔ ← Δt(Vpt, Vpt−1)\nEVp ← (D © M © EΔ)Vp ⊕ f(t)\nChen et al. (2021b) FCNN-1 (e15 attributes) \nand FCNN-2 (dNL \nfeatures)\nPatient visits \nVp = FCNN1 ∪ FCNN2. \nAperiodic\n– f(t) = sinusoidal function. t \nis the Vp sequential value\nEVp ← ΓVp ⊕ f(t) ⊕ A/B segment\nLi et al. (2021) dDTW (number of patient) Patient counts Cp = DTW \n(patient counts). Periodic \n(weekly)\n– f (t) = sinusoidal function. t \nis the Cp sequential value\nECp ← ΓCp ⊕ f(t)\nYe et al. (2020) Diagnosis code (D) Patient visits \nVp = HAM(list of D). \nAperiodic\n– flong = sinusoidal function, \nfshort = conv1d layer\nEVp ← ΓVp ⊕ flong(Vp) ⊕ fshort\n(Vp)\nRasmy et al. (2021) Diagnosis code (D) Patient visits Vp = list of D. \nAperiodic\n– f(t) = t, where t is the Vp \nsequential value\ng = codes priority order \n(1..n)\nEVp ← ΓVp ⊕ f(t) ⊕ g(ΓVp)\nTransformers in health: a systematic review on architectures…\n1 3\nPage 15 of 39 32\nTable 2  (continued)\nReferences Longitudinal inputs\n(lexicon = Γ)\nLongitudinal unit (t) and \nfrequency\nStatic attributes Positional encoding Input embedding\nZeng et al. (2022) Diagnosis (D), procedure \n(P), medication (M), and \nvisit type (VT) codes\nPatient visits Vp = list \nof D ∪ list of P list of \nM ∪ VT Aperiodic\ncAge, Gender f(t) = visit date EVp ← Maxpool(ΓVp © f(t)) © \nAge © Gender\nShome (2021) No lexicon. Raw data from \naccelerometer and gyro-\nscope sensors\nSensor readings S. Periodic \n(continuous)\n– f (t) = continuous timestep Et ← Set of Residual blocks \n(S,f(t))\nDong et al. (2021) No lexicon. eDaily life data \nusing sensors\nSensor readings S. Periodic \n(daily)\n– f (t) = visit date Et ← GCL(Graph(S © f(t)))\nDarabi et al. (2020) Diagnosis (D), procedure \n(P), medication (M) \ncodes, and medical notes\nPatient visits Vp = list \nof D ∪ list of P list of \nM ∪ VT ∪ medical notes. \nAperiodic\ncDemographics (age, \ngender, race, etc.)\nf(t) = sinusoidal function. t \nis the Vp sequential value\nEVp ←  Ecodes ©  Enotes © Demo-\ngraphics\nEcodes ← Encoder(ΓVp ⊕ f(t))\nEnotes ← Summarizer(NL \nVp) ⊕ f(t)\nLi et al. (2023a, b) Diagnosis (D), Medica-\ntion (M), Procedure (P), \nExaminations (E), Blood \npressure (BP), Drinking \nstatus (DS), Smoking \nstatus (SS), and body \nmass index (BMI)\nPatient visits Vp = list of \npatient records (D ∪ M ∪ \nP ∪ E ∪ BP ∪ DS ∪ SS ∪ B\nMI). Aperiodic\n– f(t) = sinusoidal function. t \nis the Vp sequential value\nTsw-i ← Transform \n(ΓVp ⊕ Age ⊕ A/B ⊕ f(t))|sw|, \nwhere |sw|= slide windows \nlength\nEVp ←  Tsl-1 © … ©  Tsl-n\nPang et al. (2021) Diagnosis (D), Medication \n(M), and Procedures (P)\nPatient visit Vp = list of \nD ∪ list of M ∪ list of P. \nAperiodic\n– Redefined. Implementation \nof Time  (Temb) and Age \n (Aemb) embeddings\nEVp ← ΓVp ⊕  Temb ⊕  Aemb\nRen et al. (2021) Examinations (E) Patient visit Vp = list of E. \nAperiodic\n– f (week) = sinusoidal func-\ntion, week index as input\nEVp ← ΓVp ⊕ f (week)\nPrakash et al. (2021) Diagnosis (D), Treatment \n(T), and Procedures (P)\nPatient visit Vp = list of \nD ∪ list of M ∪ list of P. \nAperiodic\n– f (t) = Vp time reference \nregarding an anchor date\nEVp ← ΓVp ⊕ Type(ΓVp) index \n(Vp) ⊕ f(t)\n C. A. Siebra et al.\n1 332 Page 16 of 39\nTable 2  (continued)\nReferences Longitudinal inputs\n(lexicon = Γ)\nLongitudinal unit (t) and \nfrequency\nStatic attributes Positional encoding Input embedding\nRao et al. (2022b) Diagnosis (D), Medication \n(M), and Blood pressure \n(BP)\nPatient visit Vp = list \nof D ∪ list of M ∪ BP. \nAperiodic\nGender, region, smoking f(t) = t, where t is the Vp \nsequential value\nEVp ← ΓVp ⊕  AgeVp ⊕ Year \nVp ⊕ f(t) © (gender © region \n© smoking)\nAn et al. (2022) No lexicon. Raw data of \nvital signals and examina-\ntions. Medications are \nlater included\nSensor readings S = vital \nsignal (VS) ∪ examina-\ntions (E). Aperiodic\n– fvs = interval between vital \nsignals,  fexa = interval \nbetween examinations\nEvs ← vital signal ⊕ fvs\nEE ← vital signal ⊕ fexa\nPeng et al. (2021) Diagnosis (D) Patient visit Vp = list of D. \nAperiodic\n– Redefined using \nf1 = fODE(Vp-start, \nVp-end), f f2 = fODE \n(Vp1,Vp2)\nMO ← Medical ontology\nEoe ← OntologyEncoder (MO, \nΓVp)\nEVp ←  Eoe ⊕ f1 ⊕ f2\na Examples of topic features are words such as “pain”, “fracture”, or any part of the body\nb These static features are only used in the final layers, after the transformer module\nc Attributes only used in the first visit (t = 1)\nd CPT = Current Procedural Terminology; © = concatenation operation; ⊕  = summing operation; NL = natural language; FCNN = Fully connected neural network; \nDTW = Dynamic Time Warping algorithm, HAM = Hierarchical Attention Module; GCL = Graph Convolution Layer\ne Systolic and diastolic blood pressure, heart rate, SpO2, respiratory rate, body temperature, height, weight, pain index, Glasgow coma slace, eye response, verbal response, \nmotor, and motor response\nf ODE = Ordinary differential equations\nTransformers in health: a systematic review on architectures…\n1 3\nPage 17 of 39 32\nThe fourth column indicates whether the approaches consider static attributes \n(InpRQ3). The most common are age and gender, used at the beginning of each patient \nsequence (Boursalie et al. 2021; Rao et al. 2022b) or directly in the last layer of the archi-\ntecture (Fouladvand et al. 2021). However, some works use the attribute age as a resource \nto improve the sequential semantic notion (Li et al. 2020; Rao et al. 2022a). Thus, they \nsupport the positional encoding task. Our review shows that diverse concepts and union \nof concepts are used as positional encode (InpRQ4). For example, the sinusoidal function \n(Florez et al. 2021; Li et al. 2021; Darabi et al. 2020), which is used in the original trans-\nformer paper (Devlin et al. 2018), simple sequential values (Rasmy et al. 2021; Li et al. \n2023a, b; Rao et al. 2022b), and calendar dates (Zeng et al. 2022; Dong et al. 2021). A/B \nsegments are encodings used as an additional semantic layer to distinguish two adjacent \nlongitudinal units (Li et al. 2020; Meng et al. 2021; Chen et al. 2021b). For example, infor-\nmation related to each patient visit alternately receives the segments A and B. However, the \nadvantages of its use to improve the representation of positional encodings are unclear in \nthe literature. There are also slight variations. For example, the work in Ren et al. (2021) \nuses the number of weeks as the input parameter of the sinusoidal function rather than \nsimple sequential values. On the other hand, some papers completely redefine the idea of \npositional encoding. The work in Pang et al. (2021) defines two embeddings, one for rep-\nresenting the idea of continuous time using age as basis  (Aemb) and another for cyclic time \nusing the calendar data as basis  (Temb). The work in Peng et al. (2021) uses two ordinary \ndifferential equations (ODE) to represent visit durations given their initial and final time \nand the interval between such visits. According to Peng et al. (2021), ODEs are particularly \ninteresting in handling arbitrary time gaps between observations.\nThe sixth column indicates how the input information is embedded (InpRQ5). As dis-\ncussed in the previous section, the contribution of most papers focused on these embedding \nfunctions. For example, the work in Fouladvand et al. (2021) uses the sum of two LSTM \nnetwork outputs, which have the diagnosis and medication longitudinal data as input. \nThese inputs are also summed to the months when they were assessed. The work in Bour -\nsalie et al. (2021) first calculates the elapsed time between two visits (longitudinal unit) \nand concatenates this value to the diagnosis and medication codes. The final embedding is \nsummed to the positional encoding. The approach in Shome (2021) uses the results of a set \nof depthwise separable residual feature extractor networks. This set receives the concat-\nenation of the assessed sensorial data and the timestep when such data were assessed. The \nwork presented in Darabi et al. (2020) considers two different input modalities. Firstly, the \nencoding of the sum of categorial inputs and positional encoding. Secondly, the summari-\nzation of medical notes, also with its positional encoding. The final embedding is given by \nconcatenating these two information and demographics data. Hierarchies of transformers \n(Pang et al. 2021) are also used to create clusters of sequential data according to a slid-\ning window. A pre-transformer handles each of these clusters, and all the results are con-\ncatenated and used as the input of the main architectural transformer. Another interesting \napproach is proposed in Peng et al. (2021), which uses medical ontologies to augment the \nsequential information of patients. Thus, this approach uses an ontology encoder to map \nthe ontology information to a vector.\n4.4  Architectural research questions\nTable 3 shows that encode-only is the most common architectural type (ArcRQ1) found in \nour analysis.\n C. A. Siebra et al.\n1 332 Page 18 of 39\nThis result may be expected since the best-known transformer example (BERT) also fol-\nlows the encode-only approach. Thus, the authors were motivated by the results obtained \nfor BERT. However, it is interesting to analyze why other works employed different archi-\ntectural types rather than the encode-only approach. The proposals in Florez et al. (2021), \nPrakash et al. (2021), and Boursalie et al. (2021) construct models that do not need pre-\ntraining. Moreover, they are interested in producing step-by-step iterative outcomes, where \nthe previous output is used as input for the current process. This feature requires autore-\ngressive architectures, such as the decode-only discussed in Sect. 2.1. The work in Li et al. \n(2021) employs an enc/dec architecture since its problem requires a pre-analysis of long \ntime-series sequences at once (encode function) and the use of this analysis to support \nmulti-step-ahead time-series predictions in a generative style (decoder function). The pro-\nposal in Pang et al. (2021) uses a simple decoder to implement a second learning objective \n(Visit type Prediction—VTP) to boost further the performance of the encoder learning, \nwhich relies on the traditional masked language modelling (MLM) as its learning strategy. \nIn An et al. (2022), the authors use end/dec architectures to derive an aware contextual fea-\nture representation of inputs. The result is a list of temporal features generated one by one \naccording to the autoregressive decoder style.\nThe next step is to understand how the Nx module of each approach differs \n(ArcRQ2) from the original design illustrated in Fig.  1. Part of the works (Li et  al. \n2020; Rao et al. 2022a; Meng et al. 2021; Rasmy et al. 2021; Zeng et al. 2022; Shome \n2021; Darabi et al. 2020; Li et al. 2023a, b; Rao et al. 2022b; Peng et al. 2021) basically \nfollow the encoder stage of the original transformer architecture (Fig.  1), including a \nfinal fully connected layer for classification/prediction. This layer uses activation func-\ntions such as Sigmoid (Li et al. 2020; Rao et al. 2022a) and Softmax (Florez et al. 2021; \nZeng et al. 2022). Other works follow (Florez et al. 2021; Boursalie et al. 2021; Prakash \net al. 2021) the decoder stage of this architecture, while the proposal in An et al. (2022) \nrelies on both stages. While these proposals did not present significant changes in the \nNx module, the remaining works conducted diverse modifications (Fig.  4). The work \nin Dong et al. (2021) includes a layer called Prediction Bag Label, which is a specific \ncomponent to the problem of predicting classes for groups of graphs. The work in Chen \net al. (2021b) uses two transformers since the first transformer generates an embedding \nof diseases, which is used as input of the second transformer to augment its classifica-\ntion accuracy. The work in Fouladvand et al. (2021) proposed three main modifications. \nFirstly, it does not use residual connections. Secondly, the multi-head attention block is \nmodified to generate attention weights between different input streams. As these streams \nwere mixed, the architecture also includes reconstruction layers to redefine the original \nstreams. The work in Li et al. (2021) uses the encoder-decoder transformer architecture. \nTable 3  Architectural type of reviewed papers\nTransformer\nEncode-only Decode-only Enc/Dec\nLi et al. (2020), Rao et al. (2022a), Meng \net al. (2021), Fouladvand et al. (2021), Chen \net al. (2021b), Ye et al. (2020), Rasmy et al. \n(2021), Zeng et al. (2022), Shome (2021), \nDong et al. (2021), Darabi et al. (2020), Li \net al. (2023a, b), Ren et al. (2021), Rao et al. \n(2022b), Peng et al. (2021)\nFlorez et al. (2021), Boursalie \net al. (2021), Prakash et al. \n(2021)\nLi et al. (2021), Pang \net al. (2021), An \net al. (2022)\nTransformers in health: a systematic review on architectures…\n1 3\nPage 19 of 39 32\nFig. 4  Schema of the proposed modifications in the Nx module\n C. A. Siebra et al.\n1 332 Page 20 of 39\nHowever, the encoder is modified to use a probabilistic attention mechanism with a \nconvolutional neural network, while the decoder uses this same mechanism with a full \nattention layer. The work in Ye et  al. (2020) concatenates the results of a traditional \ntransformer with the results of a 1-dimensional convolutional layer (conv). The hierar -\nchical attention mechanism (TAM) layer receives this result, generating a dense diag-\nnostic embedding for each longitudinal unit. The work in Pang et  al. (2021) uses the \ncontextualized embedding generated by the encoder to support a new learning process \nconducted by the decoder module, called visit type prediction. Finally, the work in Ren \net al. (2021) proposes a new multi-head attention module to handle irregular time inter -\nvals. This architecture uses a fully connected layer (multilayer perceptron—MLP) to \ncapture the time span information, which relies on position encoding outputs. The other \nlayers account for integrating such temporal information into the health data.\nMany works that follow the traditional architecture (Li et al. 2020; Rao et al. 2022a; \nFlorez et al. 2021; Meng et al. 2021; Rasmy et al. 2021; Zeng et al. 2022; Shome 2021; \nDarabi et  al. 2020; Prakash et  al. 2021) include a classification/prediction layer as \nthe way of conducting, for example, the fine-tuning process in their specific domains. \nMeanwhile, the other 14 works presented more complex modifications. Thus, we tried \nto identify the motivations for their design decisions (ArcRQ3). The work in Foulad-\nvand et al. (2021) combines each stream of each type of health data (e.g., diagnosis and \nmedication codes) inside the transformer module, rather than previously concatenating \nor summing such streams and dealing with them as a unique input stream. According to \nthe authors, this approach facilitates exploring associations within and between these \npatients’ data streams. The work in Chen et al. (2021b) considers the relation between \ndiseases as fundamental to predict care tasks. Thus, the use of its first transformer is \njustified to extract features of this relation that enhance the predictions of the second \ntransformer. Li et al. (2021) argue that their modifications avoid the quadratic computa-\ntion of self-attention, its memory bottleneck in stacking layers for long inputs, and the \nspeed plunge in predicting long outputs. The work in Ye et al. (2020) intends to better \nlearn the long and short dependencies among longitudinal data. Thus, it uses two com-\nponents, which are a traditional transformer and a 1-dimensional convolutional layer. \nThe work in Dong et al. (2021) relies on graphs as input. Thus, the specialization of its \nfinal prediction layer is a natural design step for the learning process.\nThe hierarchical approach in Li et al. (2023a, b) relies on the classical divide-and-\nconquer metaphor to analyze long sequences. However, transformers do not theoreti-\ncally have this limitation regarding their sequence size. Thus, the main reason is the \nhypothesis that hierarchical information clusters improve the learning process and, \nthus, the accuracy of the model. As the authors emphasize, “medical records naturally \nhave stronger correlation when they are closer in time” (Li et  al. 2023a, b). In Pang \net al. (2021), the visit type prediction learning strategy takes advantage of the available \nsemantics for augmenting the information gain of the transformer. Indeed, the type of \nvisit is a type of information that is not used in other approaches. At last, the support \nfor temporal notions has motivated some approaches to conducting their modifications, \nwhich affect the task processing in the following way:\n• The values of intervals between visits are included as tokens. However, they are not \npart of the vocabulary, and the decoder layers are thus modified to analyze the event-\npairs (interval time, visit information) rather than only each token (Boursalie et al. \n2021).\nTransformers in health: a systematic review on architectures…\n1 3\nPage 21 of 39 32\n• The standard equations of the transformer self-attention are modified to consider the \nirregular time interval between visits. This information is separately calculated and \nthen integrated into the visit information (Ren et al. 2021).\n• Traditional transformers are specifically used to create temporal representations \nemploying time-aware attention weights. An interesting aspect is the use of a trans-\nformer for each data type (lab tests and vital signs). A later hierarchical feature fusion \nmodel is then used to integrate such temporal data with the prescription patient infor -\nmation (An et al. 2022).\n• The approach in Peng et al. (2021) also uses the transformer to learn temporal notions. \nIn this case, the admission intervals and length of sequential visits of a patient. Like the \nprevious case, the transformer is not modified. Rather, the proposal incorporates the \ninformation regarding admission intervals and visit lengths to the transformer input to \nsupport its learning objective.\nBased on this analysis, the transformer literature does not present a concrete strategy to \nrepresent this temporal notion. This is one of the main issues of this type of architecture, as \ndetailed later in this paper.\n4.5  Evaluation questions\nTable 4 summarizes the results regarding the evaluation questions. The data characteriza-\ntion (EvaRQ1) column shows that most of the datasets present a high number of sam-\nples. This high number is essential for approaches that require a pretraining stage. We also \nincluded the data dimension (dim) used for each approach between square brackets. For \nexample, the work of Li et  al. (2020) has only one dimension represented by diagnosis \ncodes. Approaches that do not require this stage and rely on mobile data (Shome 2021; \nDong et al. 2021), for example, present fewer samples. The third column indicates which \nproposals use pretraining strategies and identifies these strategies (EvaRQ2). Masked Lan-\nguage Model (MLM) is the main strategy used for pretraining. However, we also found \nsome variations. For example, the work in Li et al. (2023a, b) implements the Bootstrap \nyour own latent (BYOL) strategy with MLM (Li et al. 2023a, b). BYOL trains two net-\nworks (online and target), which are augmented separately. The idea is to minimize the \nmean squared loss between the output of the online predictor and the target projector. In \nPang et al. (2021), the visit time prediction (VTP) is concurrently conducted with MLM, \nusing a different semantical content that can provide gains to the learning process.\nThe fourth column characterizes the fine-tuning strategies (EvaRQ3) when they are \nemployed. These strategies use traditional layers for prediction or classification, which use \nsigmoid or softmax as the activation function. A particular strategy (Positive unlabeled \n(PU)-learning) is proposed in Prakash et  al. (2021) aimed at handling the class imbal-\nance. According to this strategy, the training data comprises only positive and unlabeled \ninstances, whereas unlabeled examples include both positive and negative classes. When \nfine-tuning is not used, the proposals usually employ the traditional backward propaga-\ntion as the learning mechanism (Florez et al. 2021; Fouladvand et al. 2021; Li et al. 2021; \nShome 2021; Dong et al. 2021; An et al. 2022; Peng et al. 2021). The unique exception is \nthe work in Boursalie et al. (2021), which uses predictions for the random subset of ele-\nments masked as the final training (Boursalie et al. 2021).\nThe evaluation criteria (EvaRQ4) of the proposals are indicated in the fifth column, \nwhich emphasizes the preference of such proposals for the Area Under the Curve-Receiver \n C. A. Siebra et al.\n1 332 Page 22 of 39\nTable 4  Summary of the evaluation research questions\nRefs Dataset characterization, \nDimensionality\nPretraining strategy Fine-tuning strategy bEvaluation Criteria c,dComparison to other \napproaches\nLi et al. (2020) Clinical Practice Research \nDatalink (CPRD), EHR, \n1.6 million patients \n[dim = 1]\nMasked language model \n(MLM)\nDisease prediction in the \nnext visit, next 6 months \nvisit and next 12 months \nvisit\nAverage precision score \n(APS), AUC-ROC\n–\nRao et al. (2022a) Clinical Practice Research \nDatalink (CPRD), EHR, \n100 thousand patients\n[dim = 2]\nMasked language model \n(MLM)\nPrediction of heart failure \nusing specific dataset in \nthis domain\nAUC-ROC\nAUPRC\nRETAINEX\nFlorez et al. (2021) Medical Information Mart \nfor Intensive Care III \n(MIMIC-III), 9,537 \npatients, and InCor from \nHeart Institute, Univer-\nsity of Sao Paulo, 89,000 \npatients\n[dim = 1]\n– – Recall@k, Precision@k, \nAUC-ROC\nLIG-Doctor, LSTM, GRU, \nDoctorAI\nMeng et al. (2021) EHR, 43,967 patients\n[dim = 4]\nMasked language model \n(MLM)\nDepression prediction in \nwindows of two weeks \nand one year\nAUC-ROC, AUPRC Dipole, MiME * , HCET, and \nBERHT\nFouladvand et al. (2021) IBM MarketScan Com-\nmercial Claims32 data-\nbase, 392,492 patients \n[dim = 2]\n– – Precision, F1-score, AUC-\nROCss\nOriginal transformer model, \nLSTM, LR, RF, and SVM\nBoursalie et al. (2021) Dataset from four Canadian \nhospitals, 66,906 patients \n[dim = 2]\n– – Precision, Recall Med-BERT\nChen et al. (2021b) a1,019,437 visits from \nTaiwan hospitals, EHR\n[dim = N/A]\nPretraining BERT using \nfree text medical records\nPrediction of any critical \nevent within three days \nafter an emergency\nAUC-ROC Specific prediction models. \nSee references in Chen \net al. (2021b)\nTransformers in health: a systematic review on architectures…\n1 3\nPage 23 of 39 32\nTable 4  (continued)\nRefs Dataset characterization, \nDimensionality\nPretraining strategy Fine-tuning strategy bEvaluation Criteria c,dComparison to other \napproaches\nLi et al. (2021) Epidemiologic datasets \n(Japan-prefectures), US-\nHHS and US-census\n[dim = N/A]\n– – RMSE, MAE, PCC AR, GAR (GAR), ARIMA, \nRNN, DA-RNN\nYe et al. (2020) Three real world EHR \ndatasets (12,320, 11,240, \nand 9540 patients)\n[dim = 1]\n– – AUC-ROC, Precision, \nRecall, F1Score\nSVN, LR, RF, LSTM, \nGRU, Dipole-, Dipole, \nRetain, RetainEx, t-LSTM, \nTimeline\nRasmy et al. (2021) Cerner and Truven EHR \ndataset, 28 million \npatients\n[dim = 1]\nMLM + prediction of \nprolonged length of stay \nin hospital\nPrediction of heart failure \namong patients with \ndiabetes, and pancreatic \ncancer onset prediction\nAUC-ROC GRU, Bi-GRU, RETAIN, \nLR, RF\nZeng et al. (2022) Partner for Kids (PFK) \nclaims data (model pre-\ntraining) with 1,881,020; \nsuicide claims (suicide \nprediction) with 79,350 \npatients; asthma claims \n(asthma exacerbation \nprediction) with 22,862 \npatients; PFK-2013 with \n160,339; and MIMIC-3 \n(validation of knowl-\nedge transfer) with 7537 \npatients\n[dim = 4]\nNext Visit Prediction \n(NVP) objective and Cat-\negorial Prediction (CP) \nobjective functions\nSuicide and asthma \nexacerbation predictions \nusing specific datasets for \neach case\nAUC-ROC LR Sparse, LR Dense, Doc-\ntorAI, Dipole, TransE\n(Shome 2021) UCI HAR (30 users) and \nWISDM datasets (51 \nusers) [dim = N/A]\n– – Confusion matrices –\n C. A. Siebra et al.\n1 332 Page 24 of 39\nTable 4  (continued)\nRefs Dataset characterization, \nDimensionality\nPretraining strategy Fine-tuning strategy bEvaluation Criteria c,dComparison to other \napproaches\nDong et al. (2021) Mobile sensing dataset col-\nlected from around 1,300 \nparticipants in the wild \n[dim = 4]\n– – F1-score\nAUC-ROC\nAUPRC\nGCN, GIN, GRU, Deep Set, \nSet Tfmer\nDarabi et al. (2020) MIMIC-III ICU dataset, \n38,597 patients [dim = 4]\nPretrained BERT model \ninitialized from \nBioBERT\nBinary task for readmission \n(in 30 days) and mortality \npredictions\nAUPRC\nAUC-ROC\nSeveral methods, such as \nClinicalBert and MCE (see \n[44] for complete list)\n(Li et al. (2023a, b) Clinical Practice Research \nDatalink (CPRD). \n4,063,811 patients \n[dim = 8]\nMLM and BYOL (Boot-\nstrap your own latent)\nPredict the 5-year risk of \nheart failure, diabetes, \nchronic kidney disease\nAUROC\nAUPRC\nBEHRT and Med-BERT\nPang et al. (2021) Columbia University Irving \nMedical Center-New \nYork Presbyterian Hospi-\ntal, 2.5 M [dim = 3]\nMLM and VTP (Visit Type \nPrediction\nPrediction of hospitaliza-\ntion, death, new heart \nfailure (HF) diagnosis, \nand HF readmission\nAUC-ROC\nAUPRC\nAblation\nBEHRT and Med-BERT\nRen et al. (2021) Hospital in Beijing, \npregnant women with \nhypertension (2,872) \nand diabetes (10,080) \n[dim = 1]\nSimilarity prediction, \nmasked prediction, and \nreasonability check\nSigmoid classifier for \ngestational diabetes and \nhypertension prediction, \npregnancy outcome, and \nrisk period\nAccuracy, precision, recall, \nF1-score, and AUC. \nAblation\nLSTM, Transformer, \nRETAIN, T-LSTM, \nDipole, HiTANet\nPrakash et al. (2021) Proprietary Symphony \nHealth’s IDV® dataset \nwith 3,670 XLHe and \n263,187 unlabeled \npatients [dim = 3]\nMLM-based approach \nusing\nXLH diagnosis Positive \nunlabeled (PU)-learning \nbased approach\nAUPRC LSTM and Med-BERT\nRao et al. (2022b) 6,777,845 CPRD patients. \n[dim = 3]\nMasked EHR modeling, \nwith variants for tempo-\nral and static features\nRisk of treatment Sum absolute error (SAE) LR, TMLE, BART, TAR-\nNET, Dragonnet\nTransformers in health: a systematic review on architectures…\n1 3\nPage 25 of 39 32\nTable 4  (continued)\nRefs Dataset characterization, \nDimensionality\nPretraining strategy Fine-tuning strategy bEvaluation Criteria c,dComparison to other \napproaches\nAn et al. (2022) Subset of 10,000 MIMIC-\nIII and MIMIC-IV \npatients [dim = N/A]\n– – Precision, recall, F1-score, \nand AUC \nSeveral methods, such as \nTimeLinet and GRUD. \nSee An et al. (2022) for \ncomplete list\nPeng et al. (2021) MIC II (7,499) and \nMIMIC-III (73,452) \n[dim = 1]\n– – Accuracy@k RETAIN, Dipole, GRAM, \nKAME, MMORE\na The number of patients is not clear\nb Area Under the Precision-Recall Curve (AUPRC), Area Under the Curve-Receiver Operating Characteristic (AUC-ROC), Root Mean Squared Error (RMSE), Mean Abso-\nlute Error (MAE), Pearsons Correlation (PCC)\nc Linear Regression (LR), Random Forest (RF), support vector machine (SVM), Autoregressive (AR), Global Autoregression (GAR), Autoregressive Integrated Moving Aver-\nage (ARIMA), Recurrent Neural Network (RNN), Graph Convolution Network (GCN), Graph Isomorphic Network (GIN), gated recurrent neural network (GRU)\nd References for the methods indicated in this column can be found in the papers that used such methods (references in the first column)\ne XLH means Xlinked hypophosphatemia, a rare disease that affects bones, muscles, and teeth due to the excessive loss of phosphate\n C. A. Siebra et al.\n1 332 Page 26 of 39\nOperating Characteristic (AUC-ROC). Indeed, this performance measurement is interest-\ning because it visually tells how much the model can distinguish between classes (degree \nor measure of separability) at various threshold settings. Finally, the sixth column shows \nthat comparative analysis is a frequent way to validate the approaches (EvaRQ5). How -\never, different from the AUC-ROC, which is almost a default performance measurement, \nthe approaches use diverse techniques for this analysis.\n4.6  Explainability question\nWhile the explainability/interpretability of results is desirable for inductive systems, such \na feature is compulsory for health support systems. Indeed, previous works (Shortliffe and \nSepúlveda 2018; Amann et  al. 2020) show that this lack of explainability, which causes \nlegal and ethical uncertainties, is one of the main barriers that impede the advance of \nmachine learning techniques in the health domain. In this context, we could expect dis-\ncussions about the implementation and evaluation (EvaRQ6) of explanainability in the \nreviewed works. The following schema (Table  5) summarizes our findings regarding the \nuse of explainability in the reviewed papers.\nWe defined four groups. The first group lists references that do not consider explainabil-\nity in their research. The second group contains references that agree about the importance \nof explainability. Such a topic is indicated as one of their future research directions (Fou-\nladvand et al. 2021; Li et al. 2021; Zeng et al. 2022; An et al. 2022). The third group lists \nreferences that exclusively rely on the attention weights to associate input features with \nresults. For example, the works (Meng et al. 2021) and (Ye et al. 2020) employ attention \nweights to relate symptoms to disease codes. Thus, they evaluate the explanations using a \nquantitative analysis of such weights. The works (Li et al. 2020) and (Rasmy et al. 2021) \nalso rely on attention weights, but they use a visualization tool (Vig 2019) that supports \nand augments the analysis of such weights. For example, this tool allows detecting model \nbias, locating relevant attention heads, and linking neurons to model behavior.\nThe last group also relies on attention weights but uses additional techniques. For \nexample, the work in Rao et al. (2022a) employs perturbation-based techniques (Ivanovs \net al. 2021) to show the importance of different contexts in prediction. These techniques \nare model-agnostic and not exclusive to transformers since they only perturb the input \nand observe changes in the output. Unlike model-agnostic methods, model-specific strate-\ngies, which take advantage of the particularities of neural network architectures, were not \nidentified in our review. This last group brings two works (Dong et al. 2021; Peng et al. \n2021) that show an interesting trend of combining inductive architectures with symbolic \napproaches to augment the explainability power. The work in Dong et al. (2021) also uses \nattention weights to show the importance of the input elements. However, such inputs are \ngiven in the form of graphs that relate concepts of the domain. Thus, this approach aug-\nments the explainability of the attention mechanism since it relies on the attention weights \nassigned for each graph instance rather than only weights between inputs and outcomes. \nThe work in Peng et al. (2021) shows that its model learns interpretable representations \naccording to the structure of an ontology given as input. Thus, it is possible to derive more \ninterpretable embeddings of medical codes. However, capturing all relevant knowledge \nwithin an ontology can be difficult. For example, some nuances, tacit knowledge, or rap-\nidly evolving information may be challenging to be represented accurately. Moreover, if the \nontology presents biased data, inconsistent or incorrect definitions, relationships, or con-\ncepts, it can lead to errors in the interpretable representations.\nTransformers in health: a systematic review on architectures…\n1 3\nPage 27 of 39 32\nTable 5  Use of explainability in the reviewed papers\nDo not consider explainability Consider explainability as research \ndirections\nUnique use of attention weights Use of additional techniques\nFlorez et al. (2021), Boursalie et al. \n(2021), Chen et al. (2021b), Shome \n2021), Darabi et al. (2020), Li et al. \n(2023a, b), Pang et al. (2021), Ren et al. \n(2021), Prakash et al. (2021), Rao et al. \n(2022b)\nFouladvand et al. (2021), Li et al. (2021), \nZeng et al. (2022), An et al. (2022)\nLi et al. (2020), Meng et al. (2021), Ye \net al. (2020), Rasmy et al. (2021)\nRao et al. (2022a), Dong et al. (2021), \nPeng et al. (2021)\n C. A. Siebra et al.\n1 332 Page 28 of 39\n5  Discussion\nThis section relies on the results of our review to identify important issues that limit the \nability of transformers in handling longitudinal health data. Therefore, we summarize such \nissues and discuss initial efforts that we have identified to address them.\n5.1  Data diversity\nThis review shows that clinical-based sparse electronic health records are the main multi-\nfeatured longitudinal data source for transformers in the health domain (Sect.  4.3). How-\never, only a few classes of features (e.g., medication and diagnosis codes) have been con-\nsidered at the same time thus far. An interesting exception is the work in Li et al. (2023a, \nb), which uses several health data types (diagnosis, medication, procedure, examinations, \nblood pressure, drinking status, smoking status, and body mass index). However, such data \nare part of the same vocabulary, and thus, the learning process cannot explore the particu-\nlar semantics of each. In other words, the model does not know if a token represents, for \nexample, a medication or a diagnosis. A different approach explicitly indicates this type, \nsuch as in Prakash et al. (2021). This additional semantics can buster the efficiency and \neffectiveness of the learning process, similar to natural language processing, when the \nmodel knows that a token represents a noun or an adjective.\nIn this context, the taxonomy for health data proposed by Mayo et al. (2017) shows that \nhealth records are mainly composed of Clinician-reported (ClinRO—evaluations from a \ntrained professional) and performance-reported outcomes (PerfRO—e.g., tests of walking, \ndexterity, and cognition). However, these outcomes are usually complemented with other \ntypes of daily-life assessments using technology-reported outcomes (TechRO, e.g., weara-\nbles). Thus, longitudinal health data can be modelled with a set of vocabularies beyond \nthe simple use of diagnoses and medication (Li et al. 2020; Rao et al. 2022a; Fouladvand \net al. 2021; Boursalie et al. 2021). Approaches that employ multiple vocabularies (Meng \net al. 2021) usually sum or concatenate their inputs to generate a unique data stream (Li \net al. 2020; Rao et al. 2022a). A different approach is proposed in Fouladvand et al. (2021), \nwhich considers distinct feature streams and their combination is conducted during the cal-\nculation of the attention weights. This approach seems to better explore the relationship \nbetween the feature types. However, its complexity is exponential regarding the number of \nstreams since the model needs to conduct more operations and maintain the results of these \noperations in memory for the next steps.\nWe are currently investigating alternatives to address this problem. According to the \nrepresentation proposed by many of the approaches of our review, the history of visits of \neach patient (Vp) is represented as in Eq. (1), where CLS and SEP are the start  and sepa-\nrate special words,  vp\ni represents each visit i of patient p, and n is the total number of visits \nof p.\nIn Eq.  (2), visits are composed of m words (diagnosis codes  di) that are part of a \nunique vocabulary D. Then, the challenge is to extend the representation in (2) towards \n(1)V p ={ CLS,v1\np, SEP,v2\np, SEP,… ,vn\np, SEP}\n(2)vi\np = /braceleft.s1d1,d2,…,dm\n/braceright.s1, where/bracketleft.s1d1..dm\n/bracketright.s1∈ D\nTransformers in health: a systematic review on architectures…\n1 3\nPage 29 of 39 32\nmultifeature inputs from different vocabularies (e.g., clinical examinations (E), diagnoses \n(D), and treatments (T), such as in (3):\nThis representation uses a new special word SEP’ to set different positions inside vis-\nits for words of different vocabularies. However, this approach brings implications to the \narchitecture since the word/sentence/document concept is broken. Thus, this approach may \nalso have implications for the model’s accuracy. Another possible strategy to overcome \nthe multiple vocabulary representation is to use a similar idea than segment embeddings \nto distinguish elements of different vocabularies. This approach seems more natural, and a \nsimilar strategy is already explored in Prakash et al. (2021). The following schema (Fig. 5) \nillustrates this idea using an example.\nAccording to this strategy, a further embedding called “Vocab_type” could be added to \nthe other inputs, including the semantics of each vocabulary to the final embedding. How -\never, this approach generates an overload of information in the final embedding. Thus, we \nmust evaluate the real predictive value of the embeddings to eliminate or represent them \ndifferently.\n5.2  Temporal handling\nTable 2 shows that most approaches use the same positional encoding principles to pro-\nvide the notion of position (or order) to input tokens. While such encoding works well \nfor textual data, since a text is just a homogeneous sequence of sentences (or words), they \nrepresent a limitation to modeling clinical data. By depicting diagnoses as words, each visit \nas a sentence, and a patient’s entire medical history as a document, this representation does \nnot allow to include the notion of temporal (variable and unpredicted) distance between \nvisits and, consequently, diagnoses or any other concept represented inside the visits. Some \npapers of our review (Rao et al. 2022a; Boursalie et al. 2021), for example, define the posi-\ntion as a continuous crescent numeric stamp for each visit. However, this simple strategy \nbecomes problematic for long periods since the high values of latter positional embeddings \n(3)\nvj\np =\n/braceleft.s1\ne1,e2,…,eh, SEP�,d1,d2,…,dm , SEP�,t1,t2,…,tw , SEP�/braceright.s1\n,\nwhere /bracketleft.s1e1..eh\n/bracketright.s1∈ E,/bracketleft.s1d1..dm\n/bracketright.s1∈ D,/bracketleft.s1t1..tw\n/bracketright.s1∈ T\nFig. 5  Possible representation for different input vocabularies (e.g., diagnoses D, examinations E, treat-\nments T)\n C. A. Siebra et al.\n1 332 Page 30 of 39\nmay dominate the initial values, distorting final results. Moreover, it does not capture the \nnotion of time distance between clinical events.\nIn this context, we have observed a trend in incorporating temporal notions, such as \nthe inclusion of temporal distances as part of the input (Boursalie et al. 2021; An et al. \n2022). While the approach in An et al. (2022) only sums the temporal and input represen-\ntations, the approach in Boursalie et al. (2021) includes this distance as a token that is not \npart of a vocabulary. The work in Pang et  al. (2021) handles this limitation by creating \nan artificial vocabulary for temporal distances (e.g., token LT to temporal distance longer \nthan 365 days). However, this approach may lose precision. The approach in Peng et al. \n(2021) augments this temporal notion since it also considers the temporal length of vis-\nits rather than uniquely the distance between visits. Therefore, these two pieces of infor -\nmation (length of visit and distance between current and previous visits) are added to the \ninput content representation (numeric medical codes). Another alternative is to explicitly \nadapt the positional encoding for a date/time encoding. For example, the work in Ren et al. \n(2021) uses a continuous value of weeks as input of the positional encoding.\nApart from strategies representing the temporal notion, longitudinal health data present \nissues such as sparse and irregular time assessment intervals. According to Li et al. (2023a, \nb) and corroborated with our review, while self-attention is a flexible and efficient learn-\ning mechanism for longitudinal analysis, its ability to interpret temporal distance between \nsparse, irregular data has yet to be investigated. Thus, time representation and modifica-\ntions in the attention mechanisms may need to be conducted jointly.\n5.3  Continuous numerical data\nWhile several works follow the original ideas of transformers (vocabulary definition, pre-\ntraining, and fine-tuning) using categorical data as input, other proposals use numeric val-\nues as input. The need to handle such data is mainly derived from the current trend of using \nmobile health technology—mHealth (e.g., wearables) to assess multifeature longitudinal \nhealth data. For example, the works (Shome 2021) and (Dong et al. 2021) are examples in \nsuch a direction, while the taxonomy of Mayo et al. (2017) for health data already consid-\ners such a technology (TechROs) as a source of health information.\nCurrent approaches that use continuous numerical data do not take advantage of the \noriginal transformers’ training methods since they use the traditional three steps (forward \npropagation, calculation of the loss function, backward propagation) neural network learn-\ning process. A research line is the creation of vocabularies that relies on such continuous \ndata. For example, the work in Li et al. (2023a, b) mixes standard vocabularies to repre-\nsent tokens of diagnosis (ICD-10) and medications (British National Formulary—BNF) \nwith defined vocabularies for continuous values. For example: “For continuous values, we \nincluded systolic pressure, diastolic pressure, and BMI within range 80 to 200 mmHg, 50 \nto 140 mmHg, and 16 to 50 kg/m 2, respectively. Afterwards, we categorized systolic and \ndiastolic pressure into bins with a 5-mmHg step size (e.g., 80–85 mmHg). BMI was pro-\ncessed the same way with a step size 1 kg/m 2.” (Li et al. 2023a, b). According to Siebra \net al. (2022), the number of categories affects the accuracy of the predictions and can cre-\nate imbalanced clusters. Thus, the specification of the intervals must consider the clusters \nbalance and use of standard feature-specific categories. For example, the heart rate fea-\nture is usually categorized into five zones regarding the level of physical activities: recov -\nery/easy, aerobic/base, tempo, lactate threshold, and anaerobic (Shortliffe and Sepúlveda \nTransformers in health: a systematic review on architectures…\n1 3\nPage 31 of 39 32\n2018). If such a categorization creates imbalanced clusters, such clusters could be merged \nor divided to avoid overfitting issues in the learning process.\n5.4  Validation\nWe observed that most contributions are associated with defining ways to encode the input \ndata rather than significant redefinitions of the transformer architecture. In other words, the \nproposals are focused on how to encode multifeatured data so the transformers’ equations \ncan process such data. This focus on input embeddings becomes evident when we ana-\nlyze the last column (Input Embedding) of Table 2, which shows diverse combinations and \nforms to embed the input data. However, some design decisions are not clear in the sur -\nveyed works. For example, proposals such as (Li et al. 2020) and (Chen et al. 2021b) use \nA/B segment embeddings to provide extra information to differentiate codes in adjacent \nlongitudinal units. The validation of this strategy is hard without the execution of experi-\nmental analysis. Some works (Pang et al. 2021; Ren et al. 2021; Li et al. 2023a, b; Pang \net al. 2021; Ren et al. 2021; Prakash et al. 2021; Rao et al. 2022b; An et al. 2022; Peng \net al. 2021) use ablation analysis to demonstrate the influence of specific architectural ele-\nments on task accuracy. This analysis is very interesting since it supports, for example, \nsimplifying the approach when we identify elements (e.g., A/B segment or the use of SEP \nspecial token) that are not significantly contributing to the learning process.\nIn general, transformers suffer from a lack of benchmarks. Each work uses its own data-\nsets, which are mostly not public. Even the use of the AUC-ROC measurement, which \nis a trend among the works, presents some problems. According to the authors of Meng \net al. (2021), for example, the AUC-ROC definition in Li et al. (2020) was nonstandard, \nmaking it difficult to compare their results with other studies. In common, works that have \ncompared their approaches with traditional deep learning techniques (Florez et al. 2021; \nFouladvand et al. 2021; Ye et al. 2020; Rasmy et al. 2021) agree on the superior perfor -\nmance of transformer-based approaches. Only two papers (Li et  al. 2020; Rasmy et  al. \n2021) involved clinicians during the validation process. Li et al. (2020) identified a rate of \n76% of overlapping between clinical researchers and automatic decisions regarding the top \n10 closest diseases to each of the 87 most common diseases. In the second paper (Rasmy \net  al. 2021), clinical experts were involved to verify the reliability of the semantic rela-\ntions between diseases identified by the models. Apart from these simple cases, none of \nthe approaches were adopted or presented follow-up papers that show the application or \nvalidations of the proposals in real scenarios. Indeed, Table  4 suggests that performance \ncomparisons are only conducted using other computational approaches. This inexistence \nof such advanced evaluation processes may be associated with the need for regulations, as \ndiscussed in the previous comments. In other words, the technology transition from aca-\ndemia to the market requires well-defined regulatory compliances to guide such a process.\n5.5  Explainability and ethics\nOur review shows that several transformer architectures provide and evaluate approaches \nfor explainability, aiming to engender trust with the healthcare professionals and pro-\nvide transparency to the decision-making process. However, such approaches still need \nto evolve to be used in clinical practice. Moreover, in the mid-term, AI regulations make \nexplainability of the decision models a requirement for so-called high-risk AI applications/\n C. A. Siebra et al.\n1 332 Page 32 of 39\ndomains, such as health. An example of such regulations is the European Union Artificial \nIntelligence Act (EU AI Act), which will regulate the use of AI in the EU and enforce the \nuse of explainable models (Panigutti et al. 2023). However, as identified in our review, the \ncurrent approaches for explainability mostly rely on identifying the importance of input \nfeatures to the models’ outcomes (predictions or classifications). Such approaches are still \nraising several ethical considerations (Ghassemi et al. 2021) since they cannot, for exam-\nple, identify when AI models inherit biases in the data used for their training or assist in \nmitigating and ensuring that the generated models provide equitable and fair healthcare \noutcomes. Moreover, explainability strategies must also be extended to characterize better \nliability regarding potential errors, which is a critical aspect for legal and ethical reasons in \nclinical practice (Naik et al. 2022).\nApproaches that address these and other ethical questions may employ neuro-symbolic \nstrategies, which integrate symbolic knowledge (e.g., ontologies, knowledge graphs) into \nthe inductive reasoning process. These strategies can be designed to incorporate explicit \nethical rules and principles, as well as be programmed with ethical guidelines to guide \ntheir decision-making and prevent them from making unethical choices. The study of \nDong et  al. (2021), for example, represents the first step in this direction as it relies on \nthe weights assigned for each graph. In other words, this process indicates the concepts \nand relations (part of a graph) that are important to the modeling of outcomes. However, \nresearch advances are still required in this area since ethical considerations regarding the \nuse of AI in health must imply multiple, complex and evolving requirements, including \ndata privacy, transparency, accountability, fairness, and more, and addressing these con-\ncerns requires a multi-faceted approach that extends beyond the choice of the transformer \narchitectures, or an AI model or a strategy of applying it.\n5.6  A guideline for pragmatic development\nWe relied on the knowledge obtained in this review to create a guideline that considers \npragmatic requirements (e.g., data diversity, temporal handling, continuous values, and \nexplainability generation) that represent strong barriers to using the transformer-based \napproaches in real scenarios. Then, we map proposals identified in this review to handle \neach requirement (Fig. 6), discussing their weaknesses and benefits.\nThe first requirement is the inclusion of data diversity (R1). Our review showed that \nseveral proposals are based on a unique vocabulary, such as for diagnosis codes (Li et al. \n2020; Florez et al. 2021). There are two main directions to increase this diversity:\n• At the level of vocabulary and embeddings (R1a): This approach defines different \nvocabularies and one or more embeddings to indicate the vocabulary of each token. \nThe use of different vocabularies impacts the size of the unit of analysis (e.g., visit), \nwhich tends to contain several tokens of different vocabularies. The work of Li et al. \n(2023a, b) proposed the use of a hierarchy of transformers to handle these long \nsequences. While this “divide-and-conquer” approach can cover the complexity of long \nsequences, it increases the computational complexity given the number of transformers \nused to handle each part of this hierarchy.\n• At the level of the internal transformer architecture (R1b): In this case, the architecture \nformulation is modified to receive different data streams (Fouladvand et al. 2021). The \nmain advantage is that outputs capture the associations between input streams, repre-\nsented as attention weights between different tokens across such streams. This approach \nTransformers in health: a systematic review on architectures…\n1 3\nPage 33 of 39 32\npreserves the original semantics of each stream while the network units handle them. \nHowever, the weakness of this approach is the exponential complexity regarding the \nnumber of streams since each stream must interact with all other streams. As the work \nof Fouladvand et al. (2021) only demonstrated this approach using two data streams, we \ncannot attest its adequacy when more streams are used.\nThe second requirement is associated with the use of continuous data since transformers \nare based on discrete vocabularies (R2). Two main approaches can be used in this situation:\n• At the vocabulary level (R2a): This case relies on categorizing continuous values to \ncreate a vocabulary for each of their types. As discussed by Siebra et al. (2022), while \nthis process is simple, it presents the loss of granularity as a drawback.\n• At the level of input of one of the transformer’s layers (R2b): In this case, pre-pro-\ncessing strategies can create a feature vector that is integrated into one of the layers of \nthe transformers or even in its input. Shome (2021) follows this approach using a set \nof CNN-based blocks to create a feature vector as input of a transformer architecture. \nThis approach has as the main advantage the use of CNN as a feature extractor mod-\nule since this CNN ability is already recognized in the machine learning area. Several \nCNN-based blocks can be part of this architecture, and skipping connections enable the \nnetwork to learn negligible weights for the layers that are not relevant and do not con-\ntribute to overall accuracy. Thus, the number of blocks is not an essential hyperparam-\nFig. 6  Visual schema emphasizing the architectural components affected by the requirements implementa-\ntion according to different approaches\n C. A. Siebra et al.\n1 332 Page 34 of 39\neter to tune. However, this architecture was exclusively designed to handle continuous \nvalues. The inclusion of no continuous data can generate a problem of temporal scale \nsince the architecture must deal with data from different temporal granularities.\nThe third requirement is associated with temporal handling since transformers only \nrepresent the notion of sequence (R3). We identified three different approaches to imple-\nment such a requirement:\n• At the level of embeddings (R3a): This is the most common approach, and it is \nimplemented using adaptations of the positional encoding (Florez et al. 2021; Fou-\nladvand et  al. 2021). While implementing this approach is simple, with minimal \nchanges in the original transformer’s architecture, its expressiveness is poor since it \ncannot explicitly represent temporal distances between clinical events.\n• At the vocabulary level (R3b): As proposed by Boursalie et al. (2021), the temporal \ndistance between patients’ visits could be part of the input data. As an advantage, \nthe input representation is simple since temporal distances between two patient vis-\nits are considered as input tokens. However, temporal distances are infinite numeric \nvalues rather than categorical values. This representation must be modified since \ndistances must be employed as tokens that are part of a discrete vocabulary. Moreo-\nver, the architecture must also be modified, as exemplified by Boursalie et al. (2021), \nso it can analyze the main content (patient visit information) together with its dis-\ntance to other visits. This approach increases the complexity of the training process, \nrequiring a high amount and diversity of data.\n• At the level of internal transformer architecture (R3c): This approach considers that \ntemporal information is directly included in one of the layers of the architecture. The \nwork of Peng et al. (2021) is an example of such a direction. This approach increases \nthe expressiveness of representations since both the distance between visits and the \nlength of each visit can be integrated into the learning process. However, the form as \nthis information is integrated into the architecture can affect accuracy. For example, \nPeng et al. (2021) implemented this integration using an “Add & Normalize” layer. \nThus, the semantics are mixed before reaching the core transformer components.\nThe fourth requirement concerns using the Masked Language Model (MLM) as the \nmain strategy for pretraining (R4). The changes to handle the three previous require-\nments may invalidate the standard MLM process and parameters. Thus, other strategies \nor modifications should be considered. For example:\n• The literature usually employs a 15% probability of masking in the MLM process. \nWhile this value works as a default value, it is not an absolute consensus (Wettig \n2022). Moreover, using more than one vocabulary can create other needs that affect \nthis probability.\n• The representation of temporal notions and data diversity has the advantage of \nincreasing the expressiveness of the models. On the other hand, such expressive-\nness also increases the complexity of the learning process. Therefore, this scenario \nrequires further strategies to buster this process. One of these strategies is to modify \na percentage of the words with randomly chosen words from the vocabulary. This \naction is similar to inserting noise into the learning process, which creates more \nrobust results (Li et al. 2020).\nTransformers in health: a systematic review on architectures…\n1 3\nPage 35 of 39 32\nThe fifth requirement is related to the generation of explanations (R5). The \napproaches found for explainability can be implemented at two levels:\n• At the level of weights (R5a): This simple method creates visual descriptions regard-\ning the weight values of the network. While simple, it is also restricted since the main \ninformation is only the strongness of the relationships between tokens.\n• At the level of external knowledge (R5b): This approach is more complex, and current \nproposals only implement simple versions of this solution to integrate symbolic knowl-\nedge into the learning process (Dong et  al 2021; Peng et  al. 2021). These solutions \nrepresent inputs in the form of knowledge graphs or ontologies. While such methods \nare well-established in the artificial intelligence community, they present limitations for \nlongitudinal representations (Siebra and Wac 2022).\nA final remark is about the integrity of the architecture. The efficacy of transformers \ncomes from their architecture. Thus, adaptations should minimally affect its structure or be \nvery well justified and validated. Otherwise, such adaptations may raise several side effects \nthat are hard to understand.\n6  Conclusion\nThere is a huge amount of knowledge codified in the health datasets (e.g., EHRs), derived \nfrom the experience of a large number of experts for several years. As we show in this \npaper, current transformer models rely on such knowledge to make conclusions that are \nimpossible or very hard to derive by humans due to the amount and complexity of relations \ninvolved. The approaches discussed in this review try to demonstrate a future where this \nability can be leveraged accurately as a decision-support tool for healthcare experts. As the \nuse of transformers to analyze multifeatured longitudinal health data is recent, we have not \nidentified a convergence regarding aspects such as positional encoding, input embedding, \nor training strategies using categorical or numerical values. Differently, we have identified \nstudies for temporal handling and explanation as the two main research trends in this area. \nTemporal handling is a compulsory requirement for the health domain and the inexistence \nof such ability is a barrier for the use of the transformer technology in real applications. \nSimilarly, explainability is also becoming a compulsory requirement for deep learning \nmodels, according to the upcoming AI regulations. Indeed, the explainability for trans-\nformers models and their results are in the initial stage, and this area requires strategies \nbeyond the simple analysis of attention weights. These open questions are opportunities for \nresearch directions, which must mainly consider replicable forms to compare and justify \ntheir designs. To the best of our knowledge, this is the first review that analyzes proposals \nthat adapt the transformers technology for longitudinal health data. Other reviews focused \non general aspects of transformers (Lin et al. 2022), time series (Wen et al. 2022) and com-\nputational vision (Khan et al. 2022; Liu et al. 2021b). We have also used comprehensive \nlanguage, avoiding as much as possible the use of complex equations, such as in Lin et al. \n(2022), so this text could be an important reference for research groups that work within \nthe boundaries of interdisciplinary health informatics research.\n C. A. Siebra et al.\n1 332 Page 36 of 39\nAuthor contributions CS: conceptualization, methodology, investigation (execution of review) and writing \n(original draft). MK: investigation (execution of review) and writing (review & editing), KW: supervision, \ninvestigation (execution of review) and writing (review & editing).\nFunding Open access funding provided by University of Geneva. This project has received funding from \nthe European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-\nCurie grand agreement H2020-MSCA-IF-2020- 101024693.\nDeclarations \nCompeting interests The authors declare that they have no known competing financial interests or personal \nrelationships that could have appeared to influence the work reported in this paper.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAmann J et al (2020) Explainability for artificial intelligence in healthcare: a multidisciplinary perspective. \nBMC Med Inform Decis Mak 20(310):1–9\nAn Y, Liu Y, Chen X, Sheng Y, Hošovský A (2022) TERTIAN: clinical endpoint prediction in ICU via \ntime-aware transformer-based hierarchical attention network. Comput Intell Neurosci 4207940:1–13\nBao H, Dong L, Wei F (2021) Beit: Bert pre-training of image transformers, arXiv preprint arXiv: 2106. \n08254\nBoursalie O, Samavi R, Doyle TE (2021) Decoder transformer for temporally-embedded health outcome \npredictions. In: 20th IEEE International conference on machine learning and applications (ICMLA), \npp 1461–1467\nChen YP, Chen YY, Lin JJ, Huang CH, Lai F (2020) Modified bidirectional encoder representations from \ntransformers extractive summarization model for hospital information systems based on character-level \ntokens (AlphaBERT): development and performance evaluation. JMIR Med Inform 8(4):e17787\nChen D et  al. (2021a) Early detection of post-surgical complications using time-series electronic health \nrecords. In: AMIA summits on translational science proceedings, pp 152–160\nChen YP, Lo YH, Lai F, Huang CH (2021b) Disease concept-embedding based on the self-supervised \nmethod for medical information extraction from electronic health records and disease retrieval: algo-\nrithm development and validation study. J Med Internet Res 23(1):e25113\nChen PF et al (2022) Predicting postoperative mortality with deep neural networks and natural language \nprocessing: model development and validation. JMIR Med Inform 10(5):e38241\nCulurciello E (2018) The fall of RNN/LSTM, towards data science. https:// towar dsdat ascie nce. com/ the- fall- \nof- rnn- lstm- 2d159 4c74c e0. Accessed 26 July 2023\nDarabi S, Kachuee M, Fazeli S, Sarrafzadeh M (2020) Taper: time-aware patient ehr representation. IEEE J \nBiomed Health Inform 24(11):3268–3275\nDevlin J, Chang MW, Lee K, Toutanova K (2018) Bert: pre-training of deep bidirectional transformers for \nlanguage understanding. arXiv preprint arXiv: 1810. 04805\nDiggle P, Diggle PJ, Heagerty P, Liang KY, Zeger S (2002) Analysis of longitudinal data. Oxford University \nPress, Oxford\nDong G, Tang M, Cai L, Barnes LE, Boukhechba M (2021) Semi-supervised graph instance transformer for \nmental health inference. In: 20th IEEE International conference on machine learning and applications \n(ICMLA), pp 1221–1228\nDufter P, Schmitt M, Schutze H (2021) Position information in transformers: an overview, arXiv preprint \narXiv: 2102. 11090.\nTransformers in health: a systematic review on architectures…\n1 3\nPage 37 of 39 32\nFalissard L, Morgand C, Ghosn W, Imbaud C, Bounebache K, Rey G (2022) Neural translation and auto-\nmated recognition of ICD-10 medical entities from natural language: model development and perfor -\nmance assessment. JMIR Med Inform 10(4):e26353\nFlorez AY, Scabora L, Eler DM, Rodrigues JF (2021) APEHR: automated prognosis in electronic health \nrecords using multi-head self-attention. In: IEEE 34th international symposium on computer-based \nmedical systems (CBMS), pp 277–282\nFouladvand S et al (2021) Identifying opioid use disorder from longitudinal healthcare data using a multi-\nstream transformer. In: AMIA annual symposium proceedings. American Medical Informatics Asso-\nciation, pp 476–485\nFu Y et al (2022) A resource-efficient deep learning framework for low-dose brain PET image reconstruc-\ntion and analysis. In: IEEE 19th International symposium on biomedical imaging (ISBI), pp 1–5\nGhassemi M, Oakden-Rayner L, Beam AL (2021) The false hope of current approaches to explainable arti-\nficial intelligence in health care. Lancet Digit Health 3(11):e745–e750\nGuo A, Beheshti R, Khan YM, Langabeer JR, Foraker RE (2021) Predicting cardiovascular health tra-\njectories in time-series electronic health records with LSTM models. BMC Med Inform Decis Mak \n21(1):1–10\nHe K et al (2022) Transformers in medical image analysis: a review, arXiv preprint arXiv: 2202. 12165\nHuang K, Altosaar J, Ranganath R (2019) ClinicalBERT: modeling clinical notes and predicting hospital \nreadmission. arXiv preprint arXiv: 1904. 05342\nIvanovs M, Kadikis R, Ozols K (2021) Perturbation-based methods for explaining deep neural networks: \na survey. Pattern Recogn Lett 150:228–234\nJagannatha AN, Yu H (2016) Bidirectional RNN for medical event detection in electronic health records. \nIn: Proceedings of the conference. Association for Computational Linguistics, North American \nChapter. Meeting, vol 2016, pp 473–482\nKhan S, Naseer M, Hayat M, Zamir SW, Khan FS, Shah M (2022) Transformers in vision: a survey. \nACM Comput Surv (CSUR) 54(10s):1–41\nKitchenham B (2004) Procedures for performing systematic reviews. Keele University, Keele, vol 33, pp \n1–26\nLi Y et al (2020) BEHRT: transformer for electronic health records. Sci Rep 10(1):1–12\nLi L, Jiang Y, Huang B (2021) Long-term prediction for temporal propagation of seasonal influenza \nusing Transformer-based model. J Biomed Inform 122:103894\nLi Y et  al (2023a) Hi-BEHRT: hierarchical transformer-based model for accurate prediction of clini-\ncal events using multimodal longitudinal electronic health records. IEEE J Biomed Health Inform \n27(2):1106–1117\nLi T et al (2023b) Time-distance vision transformers in lung cancer diagnosis from longitudinal com-\nputed tomography. Med Imaging 12464:221–230\nLin T, Wang Y, Liu X, Qiu X (2022) A survey of transformers AI Open (In press)\nLiu Y, Yang Y, Jiang W, Wang T, Lei B (2021a) 3d deep attentive u-net with transformer for breast \ntumor segmentation from automated breast volume scanner. In: 43rd Annual international confer -\nence of the IEEE Engineering in Medicine & Biology Society, pp 4011–4014\nLiu Y et al (2021b) A survey of visual transformers, arXiv preprint arXiv: 2111. 06091\nLiu L, Liu S, Zhang L, To XV, Nasrallah F, Chandra SS (2023) Cascaded multi-modal mixing trans-\nformers for alzheimer’s disease classification with incomplete data. Neuroimage 277:120267\nMahajan D et al (2020) Identification of semantically similar sentences in clinical notes: Iterative inter -\nmediate training using multi-task learning. JMIR Med Inform 8(11):e22508\nMao S, Sejdić E (2022) A review of recurrent neural network-based methods in computational physiol-\nogy. In: IEEE transactions on neural networks and learning systems\nMayo NE, Figueiredo S, Ahmed S, Bartlett SJ (2017) Montreal accord on patient-reported outcomes \n(pros) use series–paper 2: terminology proposed to measure what matters in health. J Clin Epide-\nmiol 89:119–124\nMeng Y, Speier W, Ong MK, Arnold CW (2021) Bidirectional representation learning from transformers \nusing multimodal electronic health record data to predict depression. IEEE J Biomed Health Inform \n25(8):3121–3129\nMondal AK, Bhattacharjee A, Singla P, Prathosh AP (2021) xViTCOS: explainable vision transformer \nbased COVID-19 screening using radiography. IEEE J Transl Eng Health Med 10:1–10\nNaik N, Hameed BM, Shetty DK, Swain D, Shah M, Paul R et al (2022) Legal and ethical consideration \nin artificial intelligence in healthcare: who takes responsibility? Front Surg 9:266\nPang C, Jiang X, Kalluri KS, Spotnitz M, Chen R, Perotte A, Natarajan K (2021) CEHR-BERT: incorpo-\nrating temporal information from structured EHR data to improve prediction tasks. In: Proceedings \nof machine learning for health, pp 239–260\n C. A. Siebra et al.\n1 332 Page 38 of 39\nPanigutti C, Hamon R, Hupont I, Fernandez Llorca D, Fano Yela D, Junklewitz H et al (2023). The role \nof explainable AI in the context of the AI Act. In: Proceedings of the 2023 ACM conference on \nfairness, accountability, and transparency, pp 1139–1150\nPeng X, Long G, Shen T, Wang S, Jiang J (2021) Sequential diagnosis prediction with transformer and \nontological representation. In: Proceedings of the IEEE International conference on data mining, pp \n489–498\nPerveen S, Shahbaz M, Saba T, Keshavjee K, Rehman A, Guergachi A (2020) Handling irregularly sam-\npled longitudinal data and prognostic modeling of diabetes using machine learning technique. IEEE \nAccess 8:21875–21885\nPrakash PKS, Chilukuri S, Ranade N, Viswanathan S (2021) RareBERT: transformer architecture \nfor rare disease patient identification using administrative claims. Proc AAAI Conf Artif Intell \n35(1):453–460\nRao S et al (2022a) An explainable transformer-based deep learning model for the prediction of incident \nheart failure. IEEE J Biomed Health Inform 26(7):3362–3372\nRao S et  al (2022b) Targeted-BEHRT: deep learning for observational causal inference on longitudi-\nnal electronic health records. IEEE Trans Neural Netw Learn Syst. https:// doi. org/ 10. 1109/ TNNLS. \n2022. 31838 64\nRasmy L, Xiang Y, Xie Z, Tao C, Zhi D (2021) Med-BERT: pretrained contextualized embeddings on large-\nscale structured electronic health records for disease prediction. NPJ Digit Med 4(1):1–13\nRen H, Wang J, Zhao WX, Wu N (2021) Rapt: pre-training of time-aware transformer for learning robust \nhealthcare representation. In: Proceedings of the 27th ACM SIGKDD conference on knowledge dis-\ncovery & data mining, pp. 3503–3511\nSeverson K et al (2021) Discovery of Parkinson’s disease states and disease progression modelling: a longi-\ntudinal data study using machine learning. Lancet Digital Health 3(9):e555–e564\nShibly MMA, Tisha TA, Islam MK, Uddin MM (2020) Transfer learning in classifying prescriptions and \nkeyword-based medical notes. In: the 22nd International conference on information integration and \nweb-based applications & services, pp. 82–90\nShome D (2021) RestHAR: residual feature learning transformer for human activity recognition from multi-\nsensor data. In: 8th International conference on soft computing & machine intelligence (ISCMI), pp. \n181–185\nShortliffe EH, Sepúlveda MJ (2018) Clinical decision support in the era of artificial intelligence. J Am Med \nAssoc 320:2199–2200\nSiebra C, Matias I, Wac K (2022) Behavioral data categorization for transformers-based models in digital \nhealth. In: 2022 IEEE-EMBS International conference on biomedical and health informatics (BHI), \nIoannina, Greece, pp 01–04\nSvyatkovskiy A, Deng SK, Fu S, Sundaresan N (2020) Intellicode compose: code generation using trans-\nformer. In: the 28th ACM joint meeting on European software engineering conference and symposium \non the foundations of software engineering, pp 1433–1443\nTabarestani S et al (2019) Longitudinal prediction modeling of Alzheimer disease using recurrent neural \nnetworks. In: 2019 IEEE EMBS international. Conference on biomedical & health informatics (BHI), \npp 1–4\nVaswani A et al (2017) Attention is all you need. In: Advances in neural information processing systems, \nvol 30\nVig J (2019) A multiscale visualization of attention in the transformer model. In: Proceedings of the 57th \nannual meeting of the Association for Computational Linguistics: system demonstrations, pp. 37–42\nWac K (2016) mQoL: experimental methodology for longitudinal, continuous quality of life assessment via \nunobtrusive, context-rich mobile computing in situ. In: The International Society for Quality-of-Life \nStudies Conference (ISQOLS 2016)\nWang X et al (2019) Assessing depression risk in Chinese microblogs: a corpus and machine learning meth-\nods. In: 2019 IEEE International conference on healthcare informatics (ICHI), pp 1–5\nWang C, Nulty P, Lillis D (2020) A comparative study on word embeddings in deep learning for text classi-\nfication. In: Proceedings of the 4th International conference on natural language processing and infor -\nmation retrieval, pp 37–46\nWen Q, Zhou T, Zhang C, Chen W, Ma Z, Yan J, Sun L (2022) Transformers in time series: a survey, arXiv \npreprint arXiv: 2202. 07125\nYan T, Meng H, Liu S, Parada-Cabaleiro E, Ren Z, Schuller BW (2022) Convoluational transformer with \nadaptive position embedding for Covid-19 detection from cough sounds. In: 2022 IEEE International \nconference on acoustics, speech and signal processing (ICASSP), pp 9092–9096\nYang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C et al (2022) A large language model for \nelectronic health records. NPJ Digit Med 5(1):194\nTransformers in health: a systematic review on architectures…\n1 3\nPage 39 of 39 32\nYao Y, Yu W, Gao Y, Dong J, Xiao Q, Huang B, Shi Z (2022) W-Transformer: accurate Cobb angles estima-\ntion by using a transformer-based hybrid structure. Med Phys 49(5):3246–3262\nYe M, Luo J, Xiao C, Ma F (2020) Lsan: modeling long-term dependencies and short-term correlations \nwith hierarchical attention for risk prediction. In: 29th ACM International conference on information \n& knowledge management, pp 1753–1762\nZeng X, Linwood SL, Liu C (2022) Pretrained transformer framework on pediatric claims data for popula-\ntion specific tasks. Sci Rep 12(1):1–13\nZhao J et al (2019) Learning from longitudinal data in electronic health record and genetic data to improve \ncardiovascular event prediction. Sci Rep 9(1):1–10\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7988001108169556
    },
    {
      "name": "Transformer",
      "score": 0.7540109753608704
    },
    {
      "name": "ENCODE",
      "score": 0.54758220911026
    },
    {
      "name": "Sentence",
      "score": 0.5099345445632935
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4848845899105072
    },
    {
      "name": "Natural language processing",
      "score": 0.4258885383605957
    },
    {
      "name": "Machine translation",
      "score": 0.42499592900276184
    },
    {
      "name": "Data science",
      "score": 0.386104553937912
    },
    {
      "name": "Machine learning",
      "score": 0.3674030601978302
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}