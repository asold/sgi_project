{
  "title": "Advancement in Bangla Sentiment Analysis: A Comparative Study of Transformer-Based and Transfer Learning Models for E-commerce Sentiment Classification",
  "url": "https://openalex.org/W4388441063",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3145148775",
      "name": "Zishan Ahmed",
      "affiliations": [
        "American International University-Bangladesh"
      ]
    },
    {
      "id": "https://openalex.org/A4377963853",
      "name": "Shakib Sadat Shanto",
      "affiliations": [
        "American International University-Bangladesh"
      ]
    },
    {
      "id": "https://openalex.org/A2520738284",
      "name": "Akinul Islam Jony",
      "affiliations": [
        "American International University-Bangladesh"
      ]
    },
    {
      "id": "https://openalex.org/A3145148775",
      "name": "Zishan Ahmed",
      "affiliations": [
        "American International University-Bangladesh"
      ]
    },
    {
      "id": "https://openalex.org/A4377963853",
      "name": "Shakib Sadat Shanto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2520738284",
      "name": "Akinul Islam Jony",
      "affiliations": [
        "American International University-Bangladesh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2019759670",
    "https://openalex.org/W2168493061",
    "https://openalex.org/W4281255712",
    "https://openalex.org/W4294234212",
    "https://openalex.org/W4366958820",
    "https://openalex.org/W4281394526",
    "https://openalex.org/W2493841484",
    "https://openalex.org/W4226278482",
    "https://openalex.org/W4206081398",
    "https://openalex.org/W4362564252",
    "https://openalex.org/W2787665061",
    "https://openalex.org/W2963944267",
    "https://openalex.org/W3157164164",
    "https://openalex.org/W4214840481",
    "https://openalex.org/W3112665949",
    "https://openalex.org/W3022782624",
    "https://openalex.org/W4281689302",
    "https://openalex.org/W4290714600",
    "https://openalex.org/W6785402801",
    "https://openalex.org/W3155743827",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2611554154",
    "https://openalex.org/W2981851313",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3024912642",
    "https://openalex.org/W2973508239",
    "https://openalex.org/W4362564106",
    "https://openalex.org/W3197066695",
    "https://openalex.org/W2902736485",
    "https://openalex.org/W4287888679",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3168820831",
    "https://openalex.org/W2964306294",
    "https://openalex.org/W2946870453",
    "https://openalex.org/W3209731161",
    "https://openalex.org/W4220799641"
  ],
  "abstract": "Background: As a direct result of the Internet's expansion, the quantity of information shared by Internet users across its numerous platforms has increased. Sentiment analysis functions at a higher level when there are more available perspectives and opinions. However, the lack of labeled data significantly complicates sentiment analysis utilizing Bangla natural language processing (NLP). In recent years, nevertheless, due to the development of more effective deep learning models, Bangla sentiment analysis has improved significantly. Objective: This article presents a curated dataset for Bangla e-commerce sentiment analysis obtained solely from the \"Daraz\" platform. We aim to conduct sentiment analysis in Bangla for binary and understudied multiclass classification tasks. Methods: Transfer learning (LSTM, GRU) and Transformers (Bangla-BERT) approaches are compared for their effectiveness on our dataset. To enhance the overall performance of the models, we fine-tuned them. Results: The accuracy of Bangla-BERT was highest for both binary and multiclass sentiment classification tasks, with 94.5% accuracy for binary classification and 88.78% accuracy for multiclass sentiment classification. Conclusion: Our proposed method performs noticeably better classifying multiclass sentiments in Bangla than previous deep learning techniques. Keywords: Bangla-BERT, Deep Learning, E-commerce, NLP, Sentiment Analysis",
  "full_text": "Journal of\n \nInformation Systems Engineering\n \nand Business Intelligence\n \nVol.\n9\n, No.\n2\n, October \n2023\n \nAvailable online at: \nhttp://e\n-\njournal.unair.ac.id/index.php/JISEBI\n \n \nISSN 2443\n-\n2555\n \n(online) \n2598\n-\n6333 (print)\n \n©\n \n2023\n \nThe Authors. Published by Universitas Airlangga. \n \nThis is an open access article under the CC BY license (\nhttp://creativecommons.org/licenses/by/4.0/\n)\n \ndoi: http://dx.doi.org/10.20473/jisebi.\n9\n.\n2\n.\n181\n-\n194\n \n \nAdvancement in Bangla Sentiment Analysis: A Comparative \nStudy of Transformer\n-\nBased and Transfer Learning Models \nfor E\n-\ncommerce Sentiment Classification\n \nZishan Ahmed\n \n1)\n \n, \nShakib Sadat Shanto\n \n2\n)\n \n, \nAkinul Islam Jony\n \n3\n)\n*\n \n \n1\n)2)\n3)\nDepartment of \nComputer Science, \nAmerican International University \n-\n \nBangladesh\n, Dhaka, Bangladesh\n \n \n1)\n \nzishanahmed599@gmail.com\n, \n2\n)\n \nshakibsss080@gmail.com\n, \n3\n)\n \nakinul@aiub.edu\n \n \n \nAbstract\n \n \n \nBackground:\n \nAs a direct result of the Internet's expansion, the quantity of information shared by Internet users across its \nnumerous platforms has increased. Sentiment analysis functions at a higher level when there are more available perspectives \nand opinions. However, the lack of labeled data significantly complicates sentiment analysis utilizing Bangla natural languag\ne \nproce\nssing (NLP). In recent years, nevertheless, due to the development of more effective deep learning models, Bangla \nsentiment analysis has improv\ned significantly. \n \nObjective:\n \nThis article presents a curated dataset for Bangla e\n-\ncommerce sentiment analysis obtained solely from the \"Daraz\" \nplatform. We aim to conduct sentiment analysis in Bangla for binary and understudied multiclass classification tasks. \n \nMethods:\n \nTransfer learning (LSTM, GRU) and Transformers (Bangla\n-\nBERT) approaches are compared for their effectiveness \non our dataset. To enhance the overall performance of the models, we fine\n-\ntuned them. \n \nResults:\n \nThe accuracy of Bangla\n-\nBERT was highest for both binary and multiclass sentiment classification tasks, with 94.5% \naccuracy for binary classification and 88.78% accuracy for multiclass sentiment classification. \n \nConclusion:\n \nOur proposed method performs noticeably better classifying multiclass sentiments in Bangla than previous deep \nlearning techniques.\n \n \nKeywords:\n \nBangla\n-\nBERT\n, \nDeep Learning,\n \nE\n-\ncommerce,\n \nNLP,\n \nSentiment Analysis\n \n \nArticle history:\n \nReceived \n26\n \nJune\n \n20\n23\n, first decision \n22\n \nSeptember\n \n20\n23\n, accepted \n3\n \nOctober\n \n20\n23\n, available online 28 October 20\n23\n \n \nI.\n \nI\nNTRODUCTION\n \n \nSentiment refers to individuals' general attitude toward a particular interaction \n[1]\n. Sentiment Analysis involves \ncomprehending people's opinions. A text's sentiment may be either positive or negative \n[2]\n.\n \nOn occasion, a neutral \nviewpoint is also considered for classification. In recent years, the field of natural language processing (NLP) has \ngiven an important focus on sentiment analysis\n \n[3, 4]\n.\n \nDue to the rapid development of e\n-\ncommerce platforms, it is \nnow essential for businesses to understand the opinions that customers have regarding a product through customer \nreviews. Monitoring public sentiment and evaluating user feedback on well\n-\nknown e\n-\ncommerce websites such as \n'Daraz,' 'Bikroy,' and 'Chaldal.com' are examples of the diverse applications of a powerful Bangla sentiment analysis \nsystem \n[5, 6]\n. Companies can use sentiment analysis to obtain insight into customer satisfaction, identify areas for \ndevelopment, and make data\n-\ndriven decisions to improve their products and services. \n \nHowever, sentiment analysis comes across unique challenges when it comes to languages with limited resources, \nsuch as Bangla \n[7]\n. The lack of resources, such as annotated datasets and language parsers, limits the accurate analysis \nof sentiments in Bangla text. Despite these challenges, recent advancements in machine learning and deep learning \nmethods have shown promise in overcomin\ng these difficulties \n[8]\n. In recent years, the field of Bangla sentiment \nanalysis has witnessed significant advancements by utilizing various deep learning methods, in particular RNN\n-\nbased \nmodels like GRU\n, \nLSTM and several transformer\n-\nbased techniques like BERT \n[9, 10]\n.\n \nIn this study, we annotated a dataset of approximately one thousand pure Bangla comments collected from the \npopular e\n-\ncommerce platform \n“\nDaraz\n”\n \nin both binary (positive, negative) and multiclass (very positive, positive, \nnegative, and very negative) categories.\n \nWe collected the user reviews from “Daraz” since it has a wide range of \nproducts accessible\n \ncompared to the other e\n-\ncommerce websites\n, and so the collected reviews contain greater \n \n*\n \nCorresponding author\n \nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n182\n \n \nvariations of user emotion.\n \nThe dataset consists of 1,000 pure Bangla comments on multiple product types. All the \ncomments were collected and annotated manually. \nThere are\n \n2 attributes or features: Comment and Sentiment. \nUsing \nour curated dataset, we then assess user sentiment across binary and multiclass classes.\n \nBinary and multiclass \nclassifications were implemented and compared to highlight performance differences across deep learning \narchitectures. The analysis specifically aimed to evaluate the models'\n \nability to represent the linguistic complexity and \ngrammatical nuances of the Bangla language under both binary and multiclass settings.\n \nThis study addresses the need \nfor more Bangla\n-\nspecific research and resources in sentiment analysis.\n \nSeveral prior works have applied transformers \nlike BERT and Bangla\n-\nBERT for binary sentiment classification in Bangla [17,18]\n. They show good accuracy in \nbinary classification but do not explore multiclass sentiment analysis.\n \nSimilarly, \nRNN\n-\nbased transfer learning \napproach\nes like LSTM and GRU have also been commonly applied for Bangla sentiment analysis\n \n[11,12,13]\n.\n \nThese \nstudies are mostly limited to a binary classification of positive vs. negative sentiment.\n \nAdditionally, there aren't many \nresearch that compare the effectiveness of models based on transfer learning and transformers in the same context. \nSo,\n \nt\nhis study aims to compare the performance of the popular RNN\n-\nbased models GRU and LSTM with the highly \neffective Transformer\n-\nbased model Bangla\n-\nBERT designed specificall\ny for the Bangla language in classifying both \nbinary and multiclass user sentiments. The research objective also includes improving the accuracy of existing \nmulticlass sentiment classification algorithms for the Bangla language using deep learning techniqu\nes.\n \nT\nhis study \ncontributes by the c\nuration of a \nwell\n-\nbalanced Bangla e\n-\ncommerce dataset for both Binary and Multiclass \nsentiment classification\n, which is made publicly available for further research and improvements. \nThis paper provides \na c\nomparative performance analysis of fine\n-\ntuned Transfer learning models (GRU, LSTM) and Transformer based \nmethod (Bangla\n-\nBERT).\n \nFinally, we present an i\nmproved accuracy of existing multiclass Bangla sentiment \nclassification.\n \nThe paper consists of the following sections: Section 2, or the literature review section, presents a comprehensive \nreview of existing studies on sentiment analysis in the Bangla language and e\n-\ncommerce. \nS\nection \n3 \nis divided into \nt\nhree\n \nparts: The dataset \nd\nescription, which covers data collection, labeling, preprocessing, and analysis, \nthen \nthe \nExperiment\n \nsection\n, which includes embedding techniques, transfer learning\n-\nbased models, transformer\n-\nbased \nmodels, and the optimization of hyperparameters\n. Finally, \nE\nvaluation measures are discussed within this section as \nwell. \nS\nection\n \n4\n \npresents the findings of the experiments, with separate evaluations for both multiclass and binary \nmodels. \nS\nection \n5 \noffers insights and implications drawn from the results, as well as a\n \ndiscussion of comparison and \nlimitation of used approaches\n. Finally, \nSection 6, or \nthe Conclusion section, summarizes the study's key findings and \noutlines potential directions for future research in the domain of Bangla sentiment analysis for e\n-\ncommerc\ne.\n \nII.\n \nL\nITERATURE  \nR\nEVIEW \n \nThe classification of sentiment in Bangla texts using the deep learning method has been a research priority. Deep \nlearning approaches generally outperform conventional machine learning methods \n[8]\n.\n \nIn their work,\n \nt\nhe authors \npresent a framework for analyzing sentiments in Bangla\n-\nwritten texts \n[11]\n. They construct a classification model using \nBangla comments. A neural network variant, Convolutional Neural Network, generates the model. The classification \naccuracy of the classifier model is 99.87%, which is 6.87% higher than the current state\n-\nof\n-\nthe\n-\na\nrt Bangla sentiment \nclassifier. \n \nSpecifically, deep recurrent models are frequently used for Bangla sentiment classification tasks among the deep \nlearning techniques \n[12]\n. The authors propose a technique based on deep learning to classify Bangla restaurant reviews \n[13]\n. They employed a dataset consisting of 8,435 reviews written in native Bangla, the majority of which were either \npositive or negative. They compared their method to other machine learning techniques. Their proposed LSTM model \nhad the highest accuracy at 9\n1.35 percent. In their study, the authors created a system to categorize online food reviews \nbased primarily on positive and negative user sentiment labels \n[14]\n. They gathered over a thousand Bangla Food \nreviews from various online resources, including Foodpanda, HungryNaki, Shohoz Food, and Pathao Food. They \npreprocessed the data and tested machine learning techniques using Count Vectors, TF\n-\nIDF, and N\n-\ngram. Amo\nng all \nmachine learning and deep learning techniques, LSTM, a deep learning model with word2sequence feature extraction, \nprovided the highest accuracy at 90.89 percent. After refining a massive dataset, the authors used a set of 6,600 data \nfor sentiment an\nalysis in Bangla \n[15]\n. Their study's objective was to develop a sentiment classification framework to \nevaluate the performance of various deep learning models with various parameter calibration combinations. With an \naccuracy of 94%, the proposed LSTM model with advanced layers\n \nattained superior performance in resolving the \nsentiment polarity of the targeted entities. In their research, the authors compiled a dataset of 10,000 cricket\n-\nrelated \ncomments in Bangla, categorizing them as positive, negative, or neutral \n[16]\n. Then, for the vectorization of each word, \nthey used the word embedding method, and their proposed system using LSTM achieved an accuracy of 95% higher \nthan the accuracy of all previous methods.\n \nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n183\n \n \nThe use of Transformers models to categorize Bangla sentiments has also gained popularity among researchers. In \ntheir study, the authors employed Bangla\n-\nBert to classify sentiment on binary classes \n[17]\n. Their approach \nsignificantly outperforms all embeddings and sentiment classification algorithms for binary classes. In their research, \nthe authors classify Bangla fake news using Bangla\n-\nBERT, Sentiment Analysis on Bengali News Comments, and \nCross\n-\nlingual\n \nSentiment Analysis in Bengali \n[18]\n. The authors fine\n-\ntune multilingual transformer models for \nclassification tasks on Bangla text in several areas, such as sentiment analysis, emotion detection, news categorization, \nand authorship attribution \n[19]\n. On six benchmark datasets, they achieve cutting\n-\nedge outcomes, outperforming the \nprior findings by 5\n-\n29% accuracy in various tasks. In this study, the authors employ various deep learning techniques \nto perform 2\n-\nclass and 3\n-\nclass sentiment analysis on Ba\nngla text \n[20]\n. BERT\n-\nGRU performed the best, achieving an \naccuracy of 60% on their 3\n-\nclass dataset and 71% on their 2\n-\nclass dataset.\n \nAccording to our background study, insufficient Bangla datasets with balanced annotations for \nE\n-\ncommerce product \nreviews make it difficult to compare and classify binary and multiclass sentiments for business understanding and \nother use cases. This study aims to create a dataset of e\n-\ncommerce product reviews and compare the performance of \nTransfer \nlearning models and Transformer\n-\nbased models in classifying user sentiment across binary and multiclass \ncategories.\n \nIII.\n \nM\nETHOD\nS\n \n \n \nFig. 1 Proposed System Design\n \n \nFig. 1 \ndepicts the system architecture of our study, highlighting the various phases of our research. We initially \ngathered comments \nby creating\n \na relevant dataset, which were then labeled for binary and multiclass classification \ntasks. The comments were then subjected to a phase of preprocessing to ensure data quality, providing a set of clean \ncomments suitable for analysis with both classificati\non approaches. Next, the dataset was divided into training, testing, \nand validation subsets to facilitate a reli\nable evaluation of models. We utilized advanced word embedding techniques \nto enhance the semantic representation of the comments. Then, transfer learning and transformer\n-\nbased models were \nused to train and assess the efficacy of both classification approac\nhes. Finally, we compared the outcomes of the \nvarious models.\n \n \nA.\n \nDataset Description\n \nIn our research, we collected 1000 pure Bangla comments from 'Daraz.' For our study, we collected data, manually \nannotated the dataset, preprocessed it, and then analyzed it. Each of these procedures was essential in helping us \nextract useful information f\nrom the dataset, thus allowing us to look more closely at how they worked. \n \n \n1)\n \nData Collection\n \nOnline purchasing in Bangladesh has increased dramatically over the past several years, leading to the explosive \nexpansion of the country's e\n-\ncommerce sector. In an ever\n-\nchanging environment, a firm's ability to read and respond \n\nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n184\n \n \nto customer sentiments and views is crucial for success. In order to effectively utilize the insights provided by our \ncustomers, we undertook a tedious data\n-\ngathering approach to gather a sizable collection of Bangla reviews. Our data \ncollection process wa\ns designed to ensure a diverse representation of product types within the dataset. Recognizing \nthe importance of capturing a broad range of customer experiences across various product categories, we made \nconcerted efforts to collect comments from a wide va\nriety of product types available on the e\n-\ncommerce platform.\n \nWe concentrated our data gathering efforts on 'Daraz,' a well\n-\nknown e\n-\ncommerce site in Bangladesh, to ensure the \naccuracy and clarity of the information we collected. Daraz's massive user base and wide selection of items made it a \ngreat place to get authen\ntic, representative reviews written in Bangla. We chose this platform to record the vast \nspectrum of user and consumer sentiments and experiences.\n \nCarefully, we read every single one of the thousands of customer reviews that were posted on Daraz. We took great \ncare only to include reviews written entirely in pure Bangla since we wanted to be sure that the language used truly \nreflected the sentiments \nand experiences of those who speak Bangla as their native tongue. We conveyed the \ncomplexity and subtlety of customer comments thanks to Bangla's emphasis on purity.\n \nWe used rigorous criteria in the selection process to ensure that the reviews we collected were genuine and of high \nquality. \nThe criteria \nw\nere\n: \ng\niving\n \nextra weight to reviews that avoided spammy or repetitive language\n, collecting \ncomments that\n \nexpressed themselves clearly and comprehensively\n, and collecting comments from a wide range of \nproduct types such as fashion, electronics, household stuff, clothing, beauty products, etc\n.\n \nWe focused our study on a \nhigh\n-\nquality dataset that accurately reflected the var\nied opinions and sentiments of Bangla\n-\nspeaking consumers by \neliminating irrelevant and low\n-\nquality reviews\n.\n \n2)\n \nData Labelling\n \nThe accurate labeling of the collected dataset is an essential stage in sentiment analysis, as it sets the groundwork \nfor training effective machine learning models. In our study, we assigned sentiment labels to reviews based on the \nthoughts and sentiments\n \nconveyed in each comment. The classification process consisted of four categories: positive, \nvery positive, negative, and very negative, capturing a wide range of user sentiments.\n \nFor binary classification, we merged the positive and very positive categories into a single label, 'Positive.' \nSimilarly, the negative and very negative categories were merged under the label 'Negative.' This consolidation \nsimplified the classification ta\nsk and aligned with the dataset's objective of distinguishing between positive and \nnegative sentiments.\n \nWe assigned each review's sentiment labels ourselves to ensure consistency and dependability in our labeling.\n \nWe, \nthe authors of this study, were involved in annotating the dataset. As we, the authors, are all native Bengali speakers, \nthe contextual meaning of the comments was properly understandable to us, which made the annotation process more \nprecise. The anno\ntated comments were cross\n-\nchecked by each author to avoid inconsistencies or errors. This process \nof cross\n-\nchecking helps ensure the quality and accuracy of the annotations and promotes consistency among the \nannotators.\n \nWe initially labeled the dataset acc\nording to the multiclass labeling format. As annotators, we used the \nbelow cr\niteria fo\nr labeling each comment:\n \nTABLE \n1\n \nC\nRITERIA FOR \nL\nABELLING \nC\nOMMENTS\n \nComment Type\n \nCriteria\n \nPositive\n \nConveys mild satisfaction, praise, and appreciation\n \nNegative\n \nConveys mild dissatisfaction, anger, negative emotions, disappointment\n \nVery Positive\n \nConveys a strong degree of positivity, praise, endorsement, and enthusiasm\n \nVery Negative\n \nConveys strong degree of negativity, slang, harsh language, condemnation\n \n \nThe criteria shown in Table 1 were used by \nthe \nannotators while annotating the comments.\n \nSince\n \neach of \no\nur \ncollected\n \ncomments\n \nh\neld \na distinctive tone that represented a particular\n \nsentiment,\n \nthe \nmanual labeling \nwith \na set\n \nof \ncommon criteria\n \nmade\n \nthe labeling more precise\n. \nThis strategy allowed us to analyze the content of each comment in \ndetail, discern the underlying sentiments, and designate labels accordingly. By manually labeling the dataset, we \nmaintained high control and precision in capturing the reviewers' intended \nemotions and thoughts.\n \nThe table 2 shows \nthe labelling examples for both multiclass and binary classification process\n.\n \nFig. 2 demonstrates the distribution of the collected comments in the preceding graph. As evidenced by the data, \nwe maintained relatively equal\n-\nsized comment categories during the data collection phase. The process involved \ncollecting equal comments for ea\nch sentiment category, including positive, very positive, negative, and very negative. \nWhile collecting the comments, we made sure comments from all kinds of sentiments were equally taken. By ensuring \na proportionate representation of sentiments, we aimed \nto reduce any potential bias resulting from an overabundance \nof one sentiment category. \n \nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n185\n \n \nTABLE \n2\n \nC\nOMMENT \nL\nABELLING \nS\nAMPLES\n \nOriginal Comment \n \nEnglish Translation\n \nMulticlass Sentiment\n \nBinary Sentiment\n \nমধু\n \nআলহামদুিল\u0000াহ\n \n\u0000মাটামু\u0000ট\n \nভােলা\n \nলাগেছ \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nThe Honey feels pretty good \nAlhamdulillah \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \n      \nPositive\n \n      \nPositive\n \nঅেনক\n \nউপকার\n \nপা\u0000\u0000\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n, \n\u0000সলার\n \nন\u0000াচারাল\n \nআইেটম\n \nিদেয়\n \nবািনেয়েছন\n \nকথাটা\n \nসিত\u0000\n, \n\u0000ানটাও\n \nঅেনক\n \nসু\u0000র।\n \nI am getting a lot of benefits \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n, it is true that the seller has \nmade it with \nnatural items, the \nsmell is also very nice.\n \nVery Positive\n \nPositive\n \nছিব\n \n\u0000দেখ\n \nমেন\n \nকেরিছলাম\n \nএকট\u0000\n \nবড়\n \nহেব\n।\n \nফ\u0000ান\u0000ট\n \n\u0000ছাট\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n।\n \nLooking at the picture, I \nthought it would be a little bigger. \nThe fan is small \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n.\n \nNegative\n \nNegative\n \nখুবই\n \nখারাপ\n \nঅিভ\u0000তা\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nদারাজ মেলর \u0000থেক \nিকনলাম িক\u0000 খাবােরর সােথ সাবােনর \n\u0000 ঁেড়া এক ব\u0000 আশা কিরিন ।\n \nVery bad experience \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nbought from \nDaraz mall but didn't expect a box \nof soap powder with the food.\n \nVery Negative\n \nNegative\n \n \n \n \n(a) \nDistribution of multiclass comments\n \n(b)\n \nDistribution of\n \nbinary class\n \ncomments\n \n \nFig\n. \n2\n \nThe \ndistribution\n \nof\n \nboth\n \ncategories \n(\na) \nMulticlass and \n(b) \nBinary\n \ncomments\n \nin the collected dataset\n \n3)\n \nData Preprocessing\n \n \nThe preprocessing of the collected dataset was a key step in getting the text data ready for the next step, which was \nthe classification of sentiment. First, we had to get rid of punctuation, numbers, emoji, pictorial icons, and letters that \ncould have add\ned more\n \ncomplexity\n \nto the classification of sentiment. By getting rid of these parts that were not \nimportant, we aimed to improve the text data and focus on the most important information for research.\n \nInitially, sentiment classification was done with separate lists of stop words. These stop\n-\nword lists consisted of \ncommonly used words that do not convey significant sentiment or category\n-\nspecific details. To ensure the \neffectiveness of stop\n-\nword filtering\n, we employed a novel approach. Instead of only using existing stop\n-\nword lists, we \nmade our own by finding the words that were used the least in the dataset. With this method, we could include domain\n-\nspecific stop words that might not be on general lists. \nThis process made our filtering process more accurate.\n \nAfter stop words were removed, the stemming technique was used to get similar words with different endings back \nto their basic forms. This method lowers the number of dimensions in the dataset and \nensures that words with similar \nmeanings are looked at as a single unit.\n \nTABLE \n3\n \nS\nAMPLES OF \nM\nINOR \nR\nEVIEWS\n \nMinor Reviews\n \nEnglish Translation\n \nকাটার\n \nমােনর\n \nCutting quality\n \nশািড়টার\n \nসুতা\n \n \nSaree thread\n \nচশমাটা\n \nভাই\n \n \nThe glasses are brother\n \nব\u0000াগ\n \nিছেলা\n \nThere \nwas bag\n \n\u0000মে\u0000স\n \nিনঃসে\u0000েহ\n \nThe mattress, of course\n \n \nMoreover, minor reviews that did not substantially contribute to the aggregate sentiment or category classification \nwere eliminated from the dataset.\n \nThe minor reviews are those that had \nless than 3 words in them and provided no \nprecise sentiment.\n \nThey could also be referred to as short or inconclusive reviews.\n \nThis step intended to eliminate \nnoise and ensure that the dataset only contained reviews that provided meaningful insights and representative \n\nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n186\n \n \nexpressions of sentiment. Eliminating \nminor\n \nreviews improved the quality and dependability of the dataset, ensuring \nthat the subsequent analysis was centered on reviews that contained considerable data.\n \nT\nable\n \n3 \nshows a few examples \nof \nsome minor reviews\n. \nThe resulting preprocessed, few examples of sentiment\n-\nclassification\n-\nready comments are \ndisplayed in Table\n \n4\n.\n \nTABLE \n4\n \nP\nREPROCESSED \nC\nOMMENT \nS\nAMPLES\n \nOriginal Comment\n \nEnglish Translation of \nOriginal \nComment\n \nCleaned Comment\n \nEnglish Translation of \nCleaned Comment\n \nএক\n \nকথাই\n \nঅসাধারণ\n \nএকটা\n \nশািড়\n♥\n♥\n♥\nযা\n \nবলার\n \nমেতা\n \nকথা\n \nসােথ\n \nকােজ\n \n১০০\n% \nিমল\n \nআেছ\n \nআপনার\n \nএই\n \nচাইেল\n \n\u0000চাখ\n \nব\u0000\n \nকের\n \nিব\u0000ােসর\n \nসােথ\n \nশািড়\n \nটা\n \nিনেত\n \nপাের\n \nধন\u0000বাদ\n♥\n♥\n \nধারাজ\n \nধন\u0000বাদ\n♥\n♥\n \n\u0000সইলার\n \nOne word is a \nwonderful \nsaree\n♥\n♥\n♥\n \nwhich is 100% \nmatching with words and \nactions, if you want you can \nclose your eyes and take the \nsaree with faith. Thank \nyou\n♥\n♥\n \nThank you very \nmuch\n♥\n♥\n \nS\nelle\nr\n \n \nএক\n \nকথাই\n \nঅসাধারণ\n \nএকটা\n \nশািড়\n \nধন\u0000বাদ\n \n \n\u0000সইলার\n \nOne word is a \nwonderful saree \nthanks \nSeller\n \nকম\n \nদােম\n \nভােলা\n \nএকটা\n \n\u0000\u0000াডা\u0000।\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nA good product at a low \nprice.\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nকম\n \nদােম\n \nভােলা\n \nএকটা\n \n\u0000\u0000াডা\u0000\n \nA good product at a \nlow price\n \nদাম\n \nঅনুযায়ী\n \nঅেনক\n \nভােলা\n \nিছেলা\n, \nআ\u0000জ\n \nহােত\n \n\u0000পলাম\n,\n \n১১\n.\n১১\n \n\u0000ত\n \nঅড\u0000ার\n \nিছেলা\n।\n \nমা\u0000\n \n৪৮০\n/\n-\n \nটাকা\n \nপরেলা\n \n২\n \nটা\n\u0000\n\u0000\n\u0000\n\u0000\n \nধন\u0000বাদ\n \n\u0000সলার\n \n\u0000ক\n।\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nবািলস\n \nমাথায়\n \nিদেয়\n \nিরিভউ\n \nিদ\u0000\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nIt was very good \naccording to the price, got it \ntoday.\n \nThe order was on 11.11. \nOnly Tk 480/\n-\n \nwore 2 \n\u0000\n\u0000\n\u0000\n\u0000\n \nThank you seller.\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nI am \ngiving a review with a pillow \non my head \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nদাম\n \nঅনুযায়ী\n \nঅেনক\n \nভােলা\n \nিছেলা\n, \nআ\u0000জ\n \nহােত\n \n\u0000পলাম ধন\u0000বাদ\n \n\u0000সলার\n \n\u0000ক\n \nIt was very good \naccording to the price, \nI received it today, \nthanks to the seller\n \nআলহামদুিল\u0000াহ\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \n\u0000মাজা\n \n\u0000েলা\n \nপছ\u0000\n \nহইেছ\n, \nমােন\n \n\u0000যমন\n \n\u0000দেখ\n \nঅড\u0000ার\n \nকরিছ\n \n\u0000তমিন\n \n\u0000পেয়িছ\n।\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \n\u0000কিরয়া\n \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nAlhamdulillah \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n, I like \nthe socks. I mean, I received \nit as I ordered it. \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nThank you \n\u0000\n\u0000\n\u0000\n\u0000\n\u0000\n \nআলহামদুিল\u0000াহ\n \n\u0000মাজা পছ\u0000\n \nহইেছ \n\u0000যমন\n \nঅড\u0000ার\n \nকরিছ\n \n\u0000তমিন\n \n\u0000পেয়িছ \n\u0000কিরয়া\n \nAlhamdulillah, I \nlike the socks, I \nreceived them \nas \nordered, thank you\n \n4)\n \nData Analysis\n \nIn this section, we delve into the analysis of the preprocessed data to gain valuable insights into customer sentiments \nand preferences. We examine various aspects of the dataset, including sentiment \ndistribution, comment lengths, and \nlexical diversity.\n \n \n \n \n(a)\n \nData statistics for multiclass comments\n \n(b)\n \nData statistics for\n \nbinary class\n \ncomments\n \nFig\n.\n \n3 Data\n \nstatistics for\n \nboth category (\na) \nMulticlass and \n(b) \nBinary\n \ncomments \nin\n \nthe preprocessed dataset\n \n \nIn \nFig.\n \n3 the data statistics for both multiclass and binary class comments are visualized. \nHere, the total number of \ncomments is represented by \n‘T\notal \nD\nocuments\n’\n. \nThe total number of words in each sentiment class is represented by \n‘T\notal \nW\nords\n’\n, and the total number of unique words is represented by \n‘U\nnique \nW\nords\n’\n. \nUnique words \nmean\n \nthe \nnumber count of distinct words from the total words\n \nof each sentiment class\n.\n \nIt is observed in \nFig. 3 \nthat the total word count is higher for the very negative and very positive classes in the \nmulticlass sentiment classification. In contrast, the negative and positive classes exhibit lower counts. In the binary \nclassification, the total word count for both\n \nclasses \nare\n \ncomparable.\n \n\nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n187\n \n \nAnother significant observation is that the number of unique words is lower than the total word count in each class \nfor both classifications. This reduced lexical diversity can be beneficial when dealing with misspelled or inappropriate \nwords present in th\ne corpus. Fewer unique words make it easier for models to handle such variations in the text and \ncontribute to more efficient computation and memory utilization by reducing the dimensionality of the vector space \nrepresentation. However, it is worth noting \nthat the very positive and positive classes have fewer unique words than \nthe very negative and negative classes. This limited lexical diversity may introduce ambiguity, particularly if multiple \nwords are assigned to the same vector representation.\n \n \n \nFig\n.\n \n4 Length\n-\nfrequency distribution of the preprocessed dataset\n \n \nThe analysis of the comment lengths relative to the number of comments, as illustrated in \nFig. 4\n, reveals an \ninteresting distribution pattern. The majority of the comments fall within the range of two to nine words in length. \nHowever, there needs to be more lengthy comments in the dataset\n \nbecause lengthier comments would have added \nmore \nword variation\n \nto the dataset and lessened the number of minor comments\n \nafter preprocessing.\n \nIn summary, the data analysis \nafter preprocessing \ndemonstrates that the\n \npreprocessed\n \ndataset primarily consists of \ncomments with shorter lengths, ranging from two to nine words. The presence of significantly fewer lengthy comments \nsuggests a tendency for concise expressions among users.\n \nThough general and domain\n-\nspecific stop\n-\nword removal \nmade most comments shorter in length, they conveyed the clear sentiment of the reviewer.\n \n \nB.\n \nExperiment\n \nThis experiment's primary objective is to compare the performance of Transfer Learning\n-\nbased and Transformer\n-\nbased models for classifying binary \nand multiclass sentiment in Bangla using fine\n-\ntuning. We conducted experiments \nusing Google Collaboratory, which is extensively used for developing deep learning applications. We used \nTensorFlow == 2.12.0, Keras == 2.12.0, and Transformers == 4.30.2 in ord\ner to develop our deep\n-\nlearning models.\n \nAfter preprocessing the dataset and removing small reviews, the final dataset contains a total of 979 cleaned reviews.\n \nWe split the preprocessed dataset into training, test, and validation datasets.\n \nThe training set \nshould be the largest split \nas it is used to actually train the models. A bigger training set allows the models to learn more robustly and generalize \nbetter. 7\n2\n%\n \n(704 reviews)\n \nfor training ensures sufficient data for this purpose.\n \nThe validation set is used during training \nto tune hyperparameters and evaluate model performance on data not seen during training.\n \nThe validation set contains \n18% (177 reviews)\n, \nwhich\n \nis a reasonable size \n-\n \nlarge enough to get a good estimate of model generalization but not \nso large it significantly reduces the training data.\n \nThe test set is used only for the final evaluation of the fully trained \nmodel. It should be a representation of real\n-\nworld unseen data.\n \nThe test set contains 10% (98 reviews) of total reviews\n.\n \n10% is a commonly used ratio for testing in machine learning\n \nor deep learning\n.\n \nUsing\n \nan approximate of\n \n70/20/10 \nratio is a commonly accepted rule of thumb for splitting datas\nets into train/validation/test. So\n,\n \nthe percentages are \naligned with standard practices.\n \n \n1)\n \nEmbedding\n \nWord embedding is a metric for language modeling and feature learning in neural networks that converts textual \nwords into dense, low\n-\ndimensional vectors. The Word2vec embedding approach is used to extract the feature. \nWord2Vec has achieved considerable suc\ncess in sentiment analysis in several languages, including Bengali \n[21, 22]\n \n2)\n \nTransfer learning Models\n \nIn our study, we use RNN\n-\nbased Transfer learning models GRU and LSTM. A form of recurrent neural network \n(RNN) layer called the GRU (Gated Recurrent Unit) layer is ideally suited for processing sequential input, such as \ntext data \n[23, 24]\n.\n \nLong\n-\nterm dependencies in sequential data are captured by the LSTM (Long Short\n-\nTerm Memory) \n\nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n188\n \n \nlayer, a form of recurrent neural network (RNN) layer that is created to address the vanishing gradient problem \n[25, \n26]\n \nEach word in the input set is first mapped to a dense vector representation in the model's first embedding layer. The \nvocabulary words ' low\n-\ndimensional representations (embeddings) are learned and assigned at this layer. Next comes \nthe Bidirectional GRU L\nayer for GRU and the Bidirectional LSTM layer for the LSTM approach. The model may \nconsider the context of both the words that come before and after the input sequence because of the bidirectional GRU \nlayer \n[27, 28]\n. The bidirectional long short\n-\nterm memory (LSTM) layer is a recurrent layer that employs three separate \ngates (input gate, forget gate, and output gate) to record long\n-\nterm associations and retain information in a memory \ncell \n[16, 29]\n. The output of the GRU and LSTM layers is subjected to an application of a dense layer with 24 units \nand the ReLU activation function. This layer receives the output from the GRU and LSTM layers and extracts \nadditional features and non\n-\nlinear transformati\nons. The output from the previous dense layer must be flattened since \nthe following dense layer requires a one\n-\ndimensional tensor. The last dense layer uses a softmax activation function \nwith two units for binary and four for multiclass, depending on the n\number of emotion categories. Constructing a \nprobability distribution across the categories calculates the probability of each sentiment category for a given input. \nThe multiclass capable loss function of sparse categorical cross\n-\nentropy was used to create \nthis model. The Adam \noptimizer is used in gradient descent.\n \n3)\n \nTransformer\n-\nbased Model\n \nWe employed the Bangla\n-\nBERT transformer\n-\nbased model, which was pre\n-\ntrained with 2.18 billion tokens \n[18, 30]\n \nbecause of BERT's recent success in various NLP tasks \n[31, 32]\n.\n \nA language model for the Bangla language called Bangla\n-\nBERT (Bidirectional Encoder Representations from \nTransformers) is based on the Transformer architecture \n[17]\n. The Bangla\n-\nBERT model's design is similar to the \noriginal BERT model; however, Bangla\n-\nBERT was \ntrained on a sizable corpus of Bangla text to capture language\n-\nspecific patterns and semantic representations. In order to handle words that are not recognized and gather details at a \nsubword level, Bangla text is broken down into WordPieces, which are sma\nller units of words. The Transformer layers, \nor several self\n-\nattention and feed\n-\nforward neural networks, comprise the model. Each layer engages in self\n-\nattention, \nwhich enables the model to focus on different elements of the input sequence and recognize co\nntextual relationships \nbetween words. The model can evaluate the relative weights of various words in the input sequence, capturing both \nlocal and global dependencies, thanks to the self\n-\nattention mechanism within each Transformer layer. The input \nembeddin\ngs are subjected to positional encoding to convey the positioning information of the words in the sequence \nand help the model comprehend the sequential order of the words.\n \n4)\n \nHyperparameters\n \nHyperparameters are beneficial for determining any model's network architecture and key characteristics, including \nthe number of layers and nodes. It is also crucial for network training and how it is trained based on learning rates, \nbatch size, and dropou\nt rates \n[33]\n. The hyperparameter settings for the proposed study are displayed in Table \n5\n. The \nentire hyperparameter space was explored to determine the optimal value of each hyperparameter. As demonstrated\n \nin Table \n5\n, \nthese \nmodels for our proposed study are trained with optimal hyperparameter values.\n \nTABLE \n5\n \nH\nYPERPARAMETER \nS\nETTINGS\n \nHyperparameters\n \nHyperparameter Space \n \nOptimal Value \n \nBatch Size\n \n16,32,64,128\n \n64\n \nEmbedding Dimension\n \n8,16,32,64,128,256\n \n128\n \nDropout Rate\n \n0.1,0.15,0.2,0.25,0.30,0.35,0.40,0.45,0.5\n \n0.30\n \nOptimizer\n \nSGD, Adam\n \nAdam\n \nLearning Rate\n \n0.6,0.3,0.1,0.01,0.001,0.0001\n \n0.0001\n \nEpochs\n \n10,15,20,25,30\n \n10\n \n \nC.\n \nEvaluation Measures\n \nDeep \nlearning model performance is measured using evaluation metrics. We evaluated the models based on their \naccuracy, precision, recall, and F1 score.\n \n        \n \nAccuracy \n[34]\n \nis a performance metric that evaluates how well a machine learning model performs. Which is \ndefined as:\n \nAccuracy\n=\nNumber\n \nof\n \ncorrect\n \npredictions\nTotal\n \nnumber\n \nof\n \npredictions\n \n(1)\n \n \nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n189\n \n \n        \n \nPrecision \n[34]\n \nquantifies the ratio of true positives (accurately predicted positive occurrences) to the total \nnumber of cases that fell into the positive category. Which is defined as:\n \n \nPrecision\n \n=\nTrue\n \nPositives\nTrue\n \nPositives\n+\nFalse\n \nPositives\n \n(2)\n \n    \nRecall \n[34]\n \ncalculates how many positive actual depictions, according to our criteria, are marked as positive (true \npositive). Which is defined as:\n \n \nRecall\n \n=\nTrue\n \nPositives\nTrue\n \nPositives\n+\nFalse\n \nNegatives\n \n(3)\n \n \n    \nThe harmonic mean of accuracy and recall, or F1\n-\nScore \n[34]\n, is widely used as a single statistic to evaluate a model's \nefficacy. The F1\n-\nscore is defined as:\n \n \nF1\n−\nScore\n \n=\n2\n \n×\n(\nPrecision\n \n×\nRecall\n)\nPrecision\n+\nRecall\n \n(\n4\n)\n \n \nIV.\n \nR\nESULT\nS\n \nA.\n \nModel Evaluations for Multiclass Classification\n \nThe result comparison of various deep learning approaches with different evaluation metrics of Multiclass sentiment \nclassification is shown in Table \n6\n.\n \nTABLE \n6\n \nM\nULTICLASS \nS\nENTIMENT \nC\nLASSIFICATION \nR\nESULTS\n \nApproach \n \nAccuracy \n \nPrecision \n \nRecall\n \nF1\n-\nScore\n \nGRU\n \n84.7\n \n87.6\n \n84.8\n \n86.1\n \nLSTM\n \n85.0\n \n82.0\n \n83.7\n \n85.5\n \nBangla\n-\nBERT\n \n88.78\n \n88.78\n \n88.68\n \n89.77\n \n \n \n \n \n \n \n \nFig. \n5 Performance\n \nComparison of deep learning approaches for \nmulticlass\n \nclassification\n \n \nIn terms of multiclass sentiment classification, \nBangla\n-\nBERT outperformed the other models, as shown in Table\n \n6\n \nand \nFig. 5\n. The accuracy of 88.78% demonstrated that the classification of sentiment categories was accurate. \nAdditionally, Bangla\n-\nBERT's accuracy of 88.78% demonstrated its ability to classify instances accurately as having \nthe desired sentiment.\n \nMoreover, Bangla\n-\nBERT's recall rate of 88.68% indicates that it effectively identifies instances belonging to each \nsentiment category (positive, negative, very positive, and very negative). The F1\n-\nscore of 89.77% attained by Bangla\n-\nBERT demonstrates its su\nperior performance in balancing precision and recall. While the Bangla\n-\nBERT model \nachieved an accuracy of 88.78%, the LSTM and GRU models only achieved an accuracy of 85% and 84.7%, \nrespectively, with inferior precision, recall, and F1\n-\nscore. \n \n84,7\n87,6\n84,8\n86,1\n85\n82\n83,7\n85,5\n88,78\n88,78\n88,68\n89,77\n78\n80\n82\n84\n86\n88\n90\n92\nAccuracy\nPrecision\nRecall\nF1-Score\nGRU\nLSTM\nBangla-BERT\nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n190\n \n \nResults indicate that when accuracy, precision, recall, and F1\n-\nscore are compared, Bangla\n-\nBERT outperforms GRU \nand LSTM models. Bangla\n-\nBERT has proved to be the most effective model for sentiment classification across \nmultiple classes for our dataset.\n \n \n \nFig. \n6\n \nTraining and Validation Accuracy Curve for Bangla\n-\nBERT (Multiclass)\n \n \nFig. 6 \ndepicts the accuracy of training and validation across various epochs. The number of epochs is displayed on \nthe x\n-\naxis, while \nthe accuracy is displayed on the y\n-\naxis.\n \nThe training accuracy line begins at a lower number and progressively increases with each epoch as the model \nlearns and becomes better at its task. As epochs progress, the training accuracy line gradually approaches 1.0. This \ntrend demonstrates that the mo\ndel becomes proficient at predicting the correct labels for training data.\n \nOn the other hand, the validation accuracy line begins at a lower number, which is typically lower than the training \naccuracy, because it measures how well the model performs on validation data that it has never seen before. Initially, \nthe accuracy of the \nvalidation improves at the same rate as the accuracy of the training, but it gradually increases. The \nvalidation accuracy line peaks near 0.90, and this result indicates that the model is accurate in predicting sentiment \nclasses.\n \nB.\n \nModel Evaluations for Binary Classification\n \nThe result comparison of various deep learning approaches with different evaluation metrics of Binary sentiment \nclassification is shown in Table \n7\n.\n \nTABLE \n7\n \nB\nINARY \nS\nENTIMENT \nC\nLASSIFICATION \nR\nESULTS\n \nApproach \n \nAccuracy \n \nPrecision \n \nRecall\n \nF1\n-\nScore\n \nGRU\n \n90.97\n \n91.06\n \n91.10\n \n90.97\n \nLSTM\n \n91.67\n \n92.01\n \n91.88\n \n91.67\n \nBangla\n-\nBERT\n \n94.5\n \n94.42\n \n94.49\n \n94.44\n \n \n \n \n \n \n \nFig. 7 Performance \nComparison of deep learning approaches for Binary classification\n \n90,97\n91,06\n91,1\n90,97\n91,67\n92,01\n91,88\n91,67\n94,5\n94,42\n94,49\n94,44\n89\n90\n91\n92\n93\n94\n95\nAccuracy\nPrecision\nRecall\nF1-Score\nGRU\nLSTM\nBangla-BERT\nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n191\n \n \n \nBased on the information in the Table 7 and the Fig. 7, Bangla\n-\nBERT has the highest overall performance. It exhibits \n94.5 percent accuracy, 94.4 percent precision, 94.4 percent recall, and 94.4 percent F1 score. This result indicates that \nit excels at accu\nrately identifying and categorizing Binary sentiments in our data set.\n \nGRU and LSTM also demonstrate excellent performance but lag slightly behind Bangla\n-\nBERT. LSTM performs \nadmirably with 91.67% accuracy, 92.01% precision, 91.88% recall, and 91.67% F1\n-\nscore. F1\n-\nscore, precision, \naccuracy, and recall all add up to 90.97 perce\nnt for GRU.\n \nAs the results demonstrate, Bangla\n-\nBERT performs better than GRU and LSTM at accurately categorizing \nemotions. Bangla\n-\nBERT provides superior sentiment categorization in terms of accuracy, precision, recall, and F1 \nscore owing to its superior capacity to co\nmprehend intricate patterns and dependencies within Bangla text.\n \n \n \nFig.\n \n8\n \nTraining and Validation Accuracy Curve for\n \nBangla\n-\nBERT\n \n(\nBinary class\n)\n \n \nFig. 8 \nillustrates training and validation accuracy across multiple epochs for sentiment classification of binary \nclasses using Bangla\n-\nBERT. As epochs progress, training accuracy increases in an ascending pattern. Furthermore, it \nachieves its highest point near 1\n. This trend demonstrates that the model performs exceptionally well at accurately \npredicting labels for training data.\n \nThe validation accuracy, on the other hand, begins at a lower level and then increases over time. The validation \naccuracy curve ends close to 0.95, demonstrating that the model can accurately predict unseen labels.\n \nV.\n \nD\nISCUSSION\n \nA.\n \nInsights and Implications\n \nThe acquired results provide valuable insights into sentiment classification in the context of the e\n-\ncommerce \nindustry and the Bangla language. The superiority of Bangla\n-\nBERT in both multiclass and binary sentiment \nclassification highlights its effectivene\nss in accurately categorizing sentiments and capturing intricate patterns in \nBangla text. The high accuracy, recall, precision, and F1\n-\nscore attained by Bangla\n-\nBERT emphasize its ability to \nunderstand and interpret sentiment expressions across different ca\ntegories. These findings underline the importance \nof leveraging advanced transformer\n-\nbased models for sentiment analysis tasks, enabling businesses to understand \ncustomer sentiments comprehensively, enhance product review analysis and make informed decisio\nns to improve \ncustomer experiences in the dynamic e\n-\ncommerce landscape.\n \nCompared to the few studies conducted on multiclass sentiment analysis in Bangla using deep learning, the efficacy \nof our proposed approach with Bangla\n-\nBERT is significantly \nimproved\n. In our dataset, Bangla\n-\nBERT classified \nmulticlass sentiments with 88.78% accuracy, which is higher than previously proposed deep learning methods \n[15,35,36\n]\n.\n \nOur findings revealed substantial improvements in categorizing sentiments into two categories. By applying transfer \nlearning and transformers methods on our dataset, we \ncompare \nthe categorization accuracy of Bangla text as positive \nor negative. Bangle\n-\nBERT was the most accurate method for classifying binary sentiment in Bangla texts, with an \naccuracy of 94.5%.\n \nB.\n \nComparison and Limitations\n \nThere are some noteworthy advantages of Bangla\n-\nBERT and some drawbacks of RNN\n-\nbased sequential models for \nthe task of classifying sentiments expressed in the Bangla language. First\nly,\n \nGRU and LSTM are recurrent neural \nnetwork architectures that process sequence data sequentially. This limits their ability to model long\n-\nrange \n\nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n192\n \n \ndependencies in long text sequences compared to BERT's transformer architecture.\n \nRNN\n-\nbased models like GRU\n \nand \nLSTM can suffer from vanishing gradient problems during training, which can degrade \nperformance.\n \nIn contrast, \nBERT uses attention, which alleviates this issue.\n \nSelf\n-\nattention in BERT allows explicitly modeling relationships \nbetween non\n-\nconsecutive words. \nRNNs implicitly simulate such relationships through their hidden state.\n \nBangla\n-\nBERT is pre\n-\ntrained on a large Bangla corpus, allowing it to have better\n \nBangla\n \nlanguage understanding. \nHowever, \nGRU and LSTM lack this pre\n-\ntrained initialization.\n \nBERT tokenizes text into WordPieces, which can better handle \nmisspellings or unseen words compared\n \nto word\n-\nlevel tokenization used in GRU\n \nor \nLSTM.\n \nOverall, BERT’s\n \ntransformer architecture, pre\n-\ntraining, WordPiece tokenization, bidirectional encoding, and higher parameter capacity \nallow it to learn better representations of text for sentiment analysis. The limitations of RNN \nsequentiality\n, lack of \npre\n-\ntraining, word\n-\nlevel tokenization, and smaller size likely contributed to lower performance compared to BERT. \nThe results validate \nBangla\n-\nBERT's superiority for the Bangla \nsentiment analysis\n \ntask\n \nover\n \nto GR\nU and LSTM\n.\n \nVI.\n \nC\nONCLUSION\n \n    \nThis study makes significant contributions to Bangla sentiment analysis in the e\n-\ncommerce domain. In response to \nthe insufficiency of labeled data in the Bangla language, we have compiled \na well\n-\nbalanced\n \ndataset for Bangla e\n-\ncommerce sentiment analysis using the \"Daraz\" platform. Our experiments contrasted transfer learning (LSTM, GRU) \nand transformer\n-\nbased (Bangla\n-\nBERT) sentiment classification approaches. Our results demonstrate that Bangla\n-\nBERT perform\ns superiorly in binary and multiclass sentiment classification. With an accuracy of 94.50% for binary \nclassification and 88.78% for multiclass sentiment classification, Bangla\n-\nBERT has demonstrated its ability to classify \nsentiments accurately and capture \nthe nuances of Bangla text.\n \n    \nRegarding future works, there are numerous avenues to investigate. The expansion of the dataset can improve the \nefficacy and generalizability of sentiment analysis models. In addition, investigating additional transformer\n-\nbased \nmodels and fine\n-\ntuning t\nechniques may yield even better outcomes. Exploring ensemble techniques that incorporate \nmultiple models could also be advantageous for enhancing sentiment classification accuracy. In addition, investigating \nsentiment analysis in other domains and integrat\ning domain\n-\nspecific knowledge could yield valuable insights for \nspecialized applications.\n \n    \nThis study establishes the groundwork for advanced sentiment analysis in Bangla for the e\n-\ncommerce industry. This \nresearch's findings can aid businesses in comprehending and capitalizing on customer sentiments to improve their \nproducts, services, and overa\nll customer experiences.\n \n \nAuthor Contributions:\n \nZishan \nAhmed\n: \nConceptualization,\n \nProgramming,\n \nData Curation,\n \nMethodology, Writing \n-\nOriginal Draft. \nShakib Sadat\n \nShanto\n: Conceptualization,\n \nProgramming\n,\n \nData Curation,\n \nMethodology,\n \nWriting \n-\nOriginal Draft\n. \nAkinul Islam\n \nJony\n:\n \nConceptualization\n, \nMethodology,\n \nWriting \n-\nReview & Editing\n, Supervision\n, Data \nCuration\n.\n \n \nAll authors have read and \nagreed to the published version of the manuscript.\n \n \nFunding:\n \nThis research received no specific grant from any funding agency.\n \n \nConflicts of Interest:\n \nThe authors declare no conflict of interest.\n \n \nData Availability:\n \nThe data that support the findings of this study are openly available in\n \n \nhttps://github.com/shakib\n-\nsadat/Bangla\n-\nE\n-\ncommerce\n-\nDataset\n \n \nInformed Consent:\n \nThere were no human subjects.\n \n \nAnimal Subjects:\n \nThere were no animal subjects. \n \n \nORCID\n: \n \nZishan Ahmed\n:\n \nhttps://orcid.org/0009\n-\n0004\n-\n9598\n-\n917X\n \nShakib Sadat Shanto\n: \nhttps://orcid.org/0009\n-\n0009\n-\n8798\n-\n9010\n \nAkinul Islam Jony: \nhttps://orcid.org/0000\n-\n0002\n-\n2942\n-\n6780\n \nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n193\n \n \nR\nEFERENCES\n \n[1]\n \nW. Medhat, A. Hassan, and H. Korashy, \"Sentiment analysis algorithms and applications: A survey,\" \nAin Shams engineering journal\n, vol. 5, \nno. 4, pp. 1093\n-\n1113, 2014.\n \n[2]\n \nC. O. Alm, D. Roth, and R. Sproat, \"Emotions from text: machine learning for text\n-\nbased emotion prediction,\" in \nProceedings of human \nlanguage technology conference and conference on empirical methods in natural language processing\n, pp. 579\n-\n586\n, \n2005\n.\n \n[3]\n \nA. Adak, B. Pradhan, and N. Shukla, \"Sentiment analysis of customer reviews of food delivery services using deep learning and\n \nexplainable \nartificial intelligence: Systematic review,\" \nFoods\n, vol. 11, no. 10, 1500, 2022.\n \n[4]\n \nA. Iqbal, R. Amin, J. Iqbal, R. Alroobaea, A. Binmahfoudh, and M. Hussain, \"Sentiment Analysis of Consumer Reviews Using Deep\n \n     \nLearning,\" \nSustainability\n, vol. 14, no. 17, 10844, 2022.\n \n[5]\n \nS. Zulfiker, A. Chowdhury, D. Roy, S. Datta, and S. Momen, \"Bangla E\n-\nCommerce Sentiment Analysis Using Machine Learning Approach,\" \nin \n4th International Conference on Sustainable Technologies for Industry 4.0 (STI)\n, pp. 1\n-\n5\n, \n2022\n.\n \n[6]\n \nM.J. Hossain, D.D. Joy, S. Das, and R. Mustafa, \"Sentiment Analysis on Reviews of E\n-\ncommerce Sites Using Machine Learning Algorithms,\" \nin \nInternational Conference on Innovations in Science, Engineering and Technology (ICISET)\n, pp. 522\n-\n527\n, \n2022\n.\n \n[7]\n \nK.A. Hasan, S. Islam, G. M\n.\nE\n. \nElahi, and M.N. Izhar, \"Sentiment recognition from \nBangla\n \ntext,\" in \nTechnical Challenges and Design Issues \nin Bangla Language Processing\n, pp. 315\n-\n327\n, \n2013\n.\n \n[8]\n \nO. Sen et al., \"Bangla Natural Language Processing: A \ncomprehensive analysis of classical, machine learning, and deep learning based \nmethods\n,\" \nIEEE Access\n, vol. 10, pp. 38999\n-\n39044, 2022.\n \n[9]\n \nN.R. Bhowmik, M. Arifuzzaman, and M.R.H. Mondal, \"Sentiment analysis on Bangla text using extended lexicon dictionary and dee\np \nlearning algorithms,\" \nArray\n, vol. 13, 100123, 2022.\n \n[10]\n \nM.R. Khan, S.N. Rahmatullah, M.F. Islam, A.R.M. Kamal, and M.A. Hossain, \"Sentiment \na\nnalysis of COVID\n-\n19 \nv\naccination in Bangla \nl\nanguage with \ncode\n-\nmixed text from social media\n,\" in \n12th International Conference on Electrical and Computer Engineering (ICECE)\n, pp. \n76\n-\n79\n, \n2022\n.\n \n[11]\n \nM.H. Alam, M.M. Rahoman, and M.A.K. Azad, \"Sentiment analysis for Bangla sentences using convolutional neural network,\" in \n20th \nInternational Conference of Computer and Information Technology (ICCIT)\n, pp. 1\n-\n6\n, \n2017\n.\n \n[12]\n \nA. Hassan, M.R. Amin, A.K. Al Azad, and N. Mohammed, \"Sentiment analysis on bangla and \nRomanized\n \nBangla\n \ntext using deep recurrent \nmodels,\" in \nInternational Workshop on Computational Intelligence (IWCI)\n, pp. 51\n-\n56\n, \n2016\n.\n \n[13]\n \nE. Hossain, O. Sharif, M.M. Hoque, and I.H. Sarker, \"Sentilstm: a deep learning approach for sentiment analysis of restaurant\n \nreviews,\" in \nInternational Conference on Hybrid Intelligent Systems\n, pp. 193\n-\n203\n, \n2020\n.\n \n[14]\n \nM.I.H. Junaid, F. Hossain, U.S. Upal, A. Tameem, A. Kashim, and A. Fahmin, \"Bangla Food Review Sentimental Analysis using Mac\nhine \nLearning,\" in \nIEEE 12th Annual Computing and Communication Workshop and Conference (CCWC),\n \npp. 0347\n-\n0353\n, \n2022\n.\n \n[15]\n \nA. Ahmed and M.A. Yousuf, \"Sentiment analysis on Bangla text using long short\n-\nterm memory (LSTM) recurrent neural network,\" in \nProceedings of International Conference on Trends in Computational and Cognitive Engineering: Proceedings of TCCE 2020, \npp. 181\n-\n192\n, \n2020\n.\n \n[16]\n \nM.F. Wahid, M.J. Hasan, and M.S. Alom, \"Cricket sentiment analysis from Bangla text using recurrent neural network with long \nshort term \nmemory model,\" in \nInternational Conference on Bangla Speech and Language Processing (ICBSLP)\n, pp. 1\n-\n4\n, \n2019\n.\n \n[17]\n \nN.J. Prottasha et al., \"Transfer learning for sentiment analysis using BERT based supervised fine\n-\ntuning,\" \nSensors\n, vol. 22, no. 11, 4157, \n2022.\n \n[18]\n \nM. Kowsher, A.A. Sami, N.J. Prottasha, M.S. Arefin, P.K. Dhar, and T. Koshiba, \"Bangla\n-\nBERT: transformer\n-\nbased efficient model for \ntransfer learning and language understanding,\" \nIEEE Access\n, vol. 10, pp. 91855\n-\n91870, 2022.\n \n[19]\n \nT. Alam, A. Khan, and F. Alam, \"Bangla text classification using transformers,\" arXiv preprint arXiv:.04446, 2020.\n \n[20]\n \nK.I. Islam, M.S. Islam, and M.R. Amin, \"Sentiment analysis in Bengali via transfer learning using multilingual BERT,\" in \n23rd International \nConference on Computer and Information Technology (ICCIT),\n \npp. 1\n-\n5\n, \n2020\n.\n \n[21]\n \nT. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient estimation of word representations in vector space,\" arXiv preprint \narXiv:. 2013.\n \n[22]\n \nM. Al\n-\nAmin, M.S. Islam, and S.D. Uzzal, \"Sentiment \nanalysis of Bengali comments with Word2Vec and sentiment information of words,\" \nin \nI\nnternational \nConference on Electrical, Computer \na\nnd Communication Engineering \n(ECCE), \npp. 186\n-\n190\n, 2017\n.\n \n[23]\n \nY. Santur, \"Sentiment analysis based on gated recurrent unit,\" in \nInternational Artificial Intelligence and Data Processing Symposium (IDAP)\n, \npp. 1\n-\n5, 2019\n.\n \n[24]\n \nK. Cho et al., \"Learning phrase representations using RNN encoder\n-\ndecoder for statistical machine translation,\" arXiv preprint arXiv:. 2014.\n \n[25]\n \nG. Murthy, S. R. Allu, B. Andhavarapu, M. Bagadi, and M. Belusonti, \"Text based sentiment analysis using LSTM,\" \nInt. J. Eng. Res. Tech. \nRes\n, vol. 9, no. 5, \npp. 299\n-\n303, \n2020.\n \n[\n26]\n \nZ. Jin, Y.\n \nYang, and Y. Liu, \"Stock \nclosing price prediction based on sentiment analysis and LSTM,\" \nNeural Computing and Applications\n, \nvol. 32, pp. 9713\n-\n9729, 2020.\n \n[27]\n \nR. Rahman, S. A. Hasan, and F. A. Rubel, \"Identifying Sentiment and Recognizing Emotion from Social Media Data in Bangla Lang\nuage,\" \nin \n12th International Conference on Electrical and Computer Engineering (ICECE),\n \npp. 36\n-\n39\n, 2022\n.\n \n[28]\n \nM.M. Abdelgwad, T.H.A. Soliman, A.I. Taloba,\n \nand \n \nM.F\n. \nFarghaly, \"Arabic aspect\n \nbased sentiment analysis using bidirectional GRU based \nmodels,\" \nJournal of King Saud University\n-\nComputer and Information Sciences\n, vol. 34, no. 9, pp. 6652\n-\n6662, 2022.\n \n[29]\n \nA.A. Sharfuddin, M. N. Tihami, and M. S. Islam, \"A deep recurrent neural network with bilstm model for sentiment classificati\non,\" in \nInternational conference on Bangla speech and language processing (ICBSLP),\n \npp. 1\n-\n4\n, 2018\n.\n \n[30]\n \nA. Bhattacharjee et al., \"BanglaBERT: Language model pre\n-\ntraining and benchmarks for low\n-\nresource language understanding evaluation in \nBangla,\" arXiv preprint \narXiv:.00204, 2021.\n \n[31]\n \nJ.D. M.W.C. Kenton and L.K. Toutanova, \"\nBERT\n: Pre\n-\ntraining of deep bidirectional transformers for language understanding,\" in \nProceedings of naacL\n-\nHLT\n, 2019\n.\n \nAhmed\n, \nShanto\n,\n \n&\n \nJony\n \n \n \nJournal of Information Systems Engineering and Business Intelligence\n,\n \n2023\n, \n9\n \n(\n2\n), \n181\n-\n194\n \n \n194\n \n \n[32]\n \nA. Zhao and Y. Yu, \"Knowledge\n-\nenabled BERT for aspect\n-\nbased sentiment analysis,\" \nKnowledge\n-\nBased Systems\n, vol. 227\n,\n \n107220, 2021.\n \n[33]\n \nG.I. Diaz, A. Fokoue\n-\nNkoutche, G. Nannicini, and H. Samulowitz, \"An effective algorithm for hyperparameter optimization of neural \nnetworks,\" \nIBM Journal of Research and Development\n, vol. 61, no. 4/5, pp. 1\n-\n9\n, \n2017.\n \n[34]\n \nR. Ahuja, A. Chug, S. Kohli, S. Gupta, and P. Ahuja, \"The impact of features extraction on the sentiment analysis,\" \nProcedia Computer \nScience\n, vol. 152, pp. 341\n-\n348, 2019.\n \n[35]\n \nE.A.E. Lucky, M.M.H. Sany, M. Keya, S.A. Khushbu, and S.R.H. Noori, \"An attention on sentiment analysis of child abusive publ\nic \ncomments towards bangla text and ml,\" in \n12th international conference on computing communication and networking technologies \n(ICCCNT),\n \npp. 1\n-\n6\n, 2021\n.\n \n[36]\n \nM. Rahman, M.R.A. Talukder, L.A. Setu, and A.K. Das, \"A \ndynamic strategy for classifying sentiment from Bengali text by utilizing \nword2vector model\n,\" \nJournal of Information Technology Research\n, vol. 15, no. 1, pp. 1\n-\n17, 2022.\n \n \nPublisher’s Note: \nPublisher stays neutral with regard to jurisdictional claims in published maps and institutional \naffiliations.\n \n \n ",
  "topic": "Bengali",
  "concepts": [
    {
      "name": "Bengali",
      "score": 0.9717767238616943
    },
    {
      "name": "Sentiment analysis",
      "score": 0.8102209568023682
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7835361957550049
    },
    {
      "name": "Computer science",
      "score": 0.781693696975708
    },
    {
      "name": "Natural language processing",
      "score": 0.6019946336746216
    },
    {
      "name": "Transfer of learning",
      "score": 0.5355987548828125
    },
    {
      "name": "Multiclass classification",
      "score": 0.519556999206543
    },
    {
      "name": "Binary number",
      "score": 0.4634195864200592
    },
    {
      "name": "Deep learning",
      "score": 0.4519897401332855
    },
    {
      "name": "Machine learning",
      "score": 0.4217241704463959
    },
    {
      "name": "The Internet",
      "score": 0.4191932678222656
    },
    {
      "name": "Support vector machine",
      "score": 0.255362331867218
    },
    {
      "name": "Mathematics",
      "score": 0.088786780834198
    },
    {
      "name": "World Wide Web",
      "score": 0.08520251512527466
    },
    {
      "name": "Arithmetic",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I103434671",
      "name": "American International University-Bangladesh",
      "country": "BD"
    }
  ],
  "cited_by": 19
}