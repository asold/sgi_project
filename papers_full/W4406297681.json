{
    "title": "Comparative Performance Evaluation of Multimodal Large Language Models, Radiologist, and Anatomist in Visual Neuroanatomy Questions",
    "url": "https://openalex.org/W4406297681",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A4208594912",
            "name": "Yasin Celal Güneş",
            "affiliations": [
                "Kırıkkale University"
            ]
        },
        {
            "id": "https://openalex.org/A3015387060",
            "name": "Mehmet Ülkir",
            "affiliations": [
                "Hacettepe University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4381838921",
        "https://openalex.org/W4398774978",
        "https://openalex.org/W4394768383",
        "https://openalex.org/W4387880237",
        "https://openalex.org/W4362608470",
        "https://openalex.org/W4388078979",
        "https://openalex.org/W4386770320",
        "https://openalex.org/W4390775136",
        "https://openalex.org/W4392407790",
        "https://openalex.org/W4391738137",
        "https://openalex.org/W4392702964",
        "https://openalex.org/W1830758076",
        "https://openalex.org/W3013294478",
        "https://openalex.org/W4390484920",
        "https://openalex.org/W4324130227",
        "https://openalex.org/W4319332969",
        "https://openalex.org/W4385850946",
        "https://openalex.org/W4405081677",
        "https://openalex.org/W3188437627",
        "https://openalex.org/W4392906075",
        "https://openalex.org/W4392621058",
        "https://openalex.org/W4390580666"
    ],
    "abstract": "This study examined the performance of four different multimodal Large Language Models (LLMs)—GPT4-V, GPT-4o, LLaVA, and Gemini 1.5 Flash—on multiple-choice visual neuroanatomy questions, comparing them to a radiologist and an anatomist. The study employed a cross-sectional design and evaluated responses to 100 visual questions sourced from the Radiopaedia website. The accuracy of the responses was analyzed using the McNemar test. According to the results, the radiologist demonstrated the highest performance with an accuracy rate of 90%, while the anatomist achieved an accuracy rate of 67%. Among the multimodal LLMs, GPT-4o performed the best, with an accuracy rate of 45%, followed by Gemini 1.5 Flash at 35%, ChatGPT4-V at 22%, and LLaVA at 15%. The radiologist significantly outperformed both the anatomist and all multimodal LLMs (p",
    "full_text": "Uludağ Üniversitesi Tıp Fakültesi Dergisi  \nJournal of Uludağ University Medical Faculty  \n50 (3) 551-556, 2024 \nDOI: https://doi.org/10.32708/uutfd.1568479 \n551 \n \nORIGINAL RESEARCH \n \nComparative Performance Evaluation of Multimodal Large \nLanguage Models, Radiologist, and Anatomist in Visual \nNeuroanatomy Questions \n \nYasin Celal GÜNEŞ1, Mehmet ÜLKİR2 \n \n1  Kirikkale Yuksek Ihtisas Hospital, Department of Radiology, Kirikkale, Türkiye . \n2  Hacettepe University Faculty of. Medicine, Department of Anatomy, Ankara, Türkiye. \n \nABSTRACT \nThis study examined the performance of four different multimodal Large Language Models (LLMs) —GPT4-V, GPT -4o, LLaVA, and \nGemini 1.5 Flash —on multiple-choice visual neuroanatomy questions, comparing them to a radiologist and an anatomist. The study \nemployed a cross-sectional design and evaluated responses to 100 visual questions sourced from the Radiopaedia website. The accuracy of \nthe responses was analyzed using the McNemar test. According to the results, the radiologist demonstrated the highest performance with an  \naccuracy rate of 90%, while the anatomist achieved an accuracy rate of 67%. Among the multimodal LLMs, GPT -4o performed the best, \nwith an accuracy rate of 45%, followed by Gemini 1.5 Flash at 35%, ChatGPT4-V at 22%, and LLaVA at 15%. The radiologist significantly \noutperformed both the anatomist and all multimodal LLMs (p<0.001). GPT -4o significantly outperformed GPT4 -V and LLaVA (p<0 .001), \nbut no significant difference was found between GPT -4o and Gemini 1.5 Flash (p=0.123). However, Gemini 1.5 Flash showed significant \nsuperiority over LLaVA (p<0.001) and also demonstrated a statistically significant difference compared to GPT4 -V (p=0.004). This study \nhighlights the significant performance gap between multimodal LLMs and medical professionals. While multimodal LLMs hold grea t \npotential in the medical field, they have not yet reached the level of accuracy of medical experts in correctly  identifying neuroanatomical \nregions. \nKeywords: Neuroanatomy. Large language models. GPT-4o. Gemini 1.5 Flash. \n \nÇok Modlu Büyük Dil Modelleri, Bir Radyolog ve Bir Anatomistin Görsel Nöroanatomi Sorularındaki Karşılaştırmalı  \nPerformans Değerlendirmesi \n \nÖZET \nBu çalışma, dört farklı çok modlu Büyük Dil Modeli'nin (GPT4 -V, GPT-4o, LLaVA, Gemini 1.5 Flash) görsel nöroanatomi çoktan seçmeli \nsorularındaki performansını, bir radyolog ve bir anatomistle karşılaştırarak incelemiştir. Kesitsel bir araştırma dizayn ına dayanan çalışmada, \nRadiopaedia web sitesinden alınan 100 görsel soruya verilen yanıtlar değerlendirilmiştir. Yanıtların doğruluğu McNemar testi kullanılarak \nanaliz edilmiştir. Sonuçlara göre, radyolog %90 doğruluk oranı ile en yüksek performansı sergil erken, anatomist %67 doğruluk oranı elde \netmiştir. Çok modlu LLM'ler arasında en iyi performansı %45 doğruluk oranı ile GPT -4o göstermiştir; onu %35 ile Gemini 1.5 Flash, %22 \nile ChatGPT4-V ve %15 ile LLaVA takip etmiştir. Radyolog, hem anatomiste hem de t üm çok modlu LLM'lere kıyasla anlamlı derecede \nüstün bir performans sergilemiştir (p<0.001). GPT -4o, GPT4-V ve LLaVA'ya kıyasla anlamlı derecede daha iyi bir performans göstermiş \n(p<0.001), ancak Gemini 1.5 Flash ile arasında anlamlı bir fark gözlenmemişti r (p=0.123). Bununla birlikte, Gemini 1.5 Flash, LLaVA'ya \nkarşı anlamlı bir üstünlük sağlamış (p<0.001) ve GPT4 -V ile karşılaştırıldığında da istatistiksel olarak anlamlı bir fark ortaya çıkmıştır \n(p=0.004). Bu çalışma, çok modlu LLM'ler ile tıbbi uzmanlar arasındaki belirgin performans farkını ortaya koymaktadır. Çok modlu LLM'ler \ntıp alanında büyük bir potansiyel vaat etse de, nöroanatomik bölgeleri doğru bir şekilde tanımlama konusunda henüz tıbbi uzma nların \ndoğruluk seviyesine ulaşamamaktadırlar. \nAnahtar Kelimeler: Nöroanatomi. Büyük dil modelleri. GPT-4o. Gemini 1.5 Flash. \n \nDate Received: October 16, 2024 \nDate Accepted: January 02, 2025 \n \nDr. Yasin Celal GÜNEŞ \nKırıkkale Yüksek İhtisas Hastanesi,  \nRadyoloji Department,  \nKırıkkale, Türkiye. \nPhone:+90 506  242  20  72 \nE-mail: gunesyasincelal@gmail.com \n \nAuthors’ ORCID Information: \nYasin Celal GÜNEŞ: 0000-0001-7631-854X \nMehmet ÜLKİR: 0000-0001-5615-8913 \nArtificial Intelligence (AI) tools known as large \nlanguage models (LLMs) are trained to process and \ngenerate text at a level that closely resembles human \nabilities. One of the competencies of LLMs is their \nability to respond to inquiries, translate text, \nparaphrase, and summarize after processing various \ninputs\n1. The release of  GPT-4 in March 2023 was \nsignificant for multimodal LLMs. GPT -4, also known \nas Generative Pre- Training Transformer -4th series \nY.C. Güneş ve M. Ulkir \n552 \nwith Vision (GPT4 -V), introduced advanced image \nevaluation capabilities2.  \nGPT4-V and Large Language -and-Vision Assistant \n(LLaVA) are multimodal LLMs with image analysis \ncapabilities that allow them to tackle more complex \nsituations by combining language and visual \ninformation (3).  The latest multimodal LLMs, GPT -\n4o and Gemini 1.5 Flash, were released in May 2024 4-\n5. \nLLMs serve as a valuable resource for medical \nprofessionals, providing rapid access to \ncomprehensive information on anatomy, surgical \ntechniques, and postoperative care. Furthermore, \nLLMs can create interactive quizzes and educational \ntools that allow students to evaluate their skills and \nreceive instant feedback\n6. Accurately identifying \nneuroanatomical landmarks is essential for \nradiologists to diagnose pathologies and for surgeons \nto perform neurosurgical and endovascular procedures \neffectively7. \nRecently, numerous arti cles have discussed the \npotential applications of LLMs in medical fields such \nas dermatology, pediatrics, radiology, anatomy, \notolaryngology, and forensic science\n8-13. Most \nprevious studies have focused on the integration of \nLLMs with only text -based capab ilities. However, \nwith the development of multimodal LLMs, visual \ndata can now be evaluated to accurately diagnose \npathologies in photos, interpret radiology images, and \nsolve board examinations\n14-16.  \nDespite advancements in multimodal LLMs, to our \nknowledge, there are no studies evaluating the \nperformance of these models on visual neuroanatomy \nmultiple choice questions (MCQs). The aim of this \nstudy is to investigate and compare the performance of \nradiologists, anatomists, and multimodal LLMs on \nvisual neuroanatomy MCQs covering spatial anatomy \nand various radiological images. \nMaterial and Method \nStudy design \nThis cross- sectional observational study compared \nmultimodal LLMs (GPT4 -V, GPT -4o, LLaVA, \nGemini 1.5 Flash), and the responses of radiologists \nand ana tomists in solving visual neuroanatomy \nMCQs. The study did not require ethics committee \napproval as it relied solely on open -access published \nonline MCQs and and did not involve any human \nsubjects or identifiable patient information.  This study \nfollowed th e Standards for Reporting Diagnostic \nAccuracy Studies (STARD) and the Checklist for \nArtificial Intelligence in Medical Imaging (CLAIM)\n17-\n18. \nData collection \nRadiopaedia provides publicly available multiple -\nchoice questions (MCQs) assessing knowledge of \ncross-sectional anatomy on its website. In this study, \nwe utilized a comprehensive dataset of 3,904 MCQs \nspanning various body systems. Each question had 4 –\n6 choices with one correct answer and included both \ntext-based and visual questions, available on the \nRadiopaedia website (Courtesy of Dr. Frank Gaillard; \naccessed September 2023; URL: \nhttps://radiopaedia.org/questions). \nAmong these, 964 questions were specifically related \nto the central nervous system (CNS), and within these \nCNS questions, 347 included ass ociated images. From \nthis subset, we identified 166 questions focusing on \nanatomical topics, particularly neuroanatomy. We \nrandomly selected 100 anatomy -related questions \nfrom these 166 using a computer -generated random \nnumber sequence to ensure a represen tative sample.  \nThis selection aimed to include a balanced distribution \nof questions covering both spatial anatomy (visual \nquestions involving anatomical structures without \nimaging modalities) and radiological image \ninterpretation (requiring analysis of ima ges from \nmodalities such as MRI and CT). \nAmong the chosen 100 questions, 55 were non-\ncontrast MRI scans (55%), 18 were non -contrast CT \nscans (18%),  and 27 were spatial anatomy questions \n(27%). All selected questions are listed in \nSupplementary Material 1, and the workflow of the \nstudy is detailed in Figure 1. \n \n \n \nFigure 1.  \nDemonstration of workflow of study. MCQs: multiple-\nchoice questions, GPT4-V: Generative Pre-Training \nTransformer-4th series with Vision, LLaVA: Large \nLanguage-and-Vision Assistant, GPT-4o: Generative \nPre-Training Transformer-4th series omni, MRI: \nMagnetic resonance imaging, CT: Computed \ntomography \n\nLanguage Models and Experts in Neuroanatomy \n553 \nPrompt design and performance evaluation for \nmultimodal LLMs \nWe initiated the input prompt in our study design as: \n\"I am working on a neuroanatomy quiz and will \nprovide you visual cases and multiple -choice \nquestions. Act like a medical professor, please indicate \nthe correct answer. There is only one correct answer.\" \nThis prompt was presented in September 2024 on two \ndistinct platforms with default hyperparameters, \nOpenAI’s GPT4 -V (https://chat.openai.com) and \nLLaVA (\nhttps://llava-vl.github.io) by radiologist \n(Y.C.G.). Subsequently, same prompt was presented \nin September 2024 on two distinct platform with \nidentical parameters, OpenAI’s GPT -4o \n(https://chat.openai.com) and Google’s Gemini 1.5 \nFlash \n(https://deepmind.google/technologies/gemini/flash) \n(Figure 2). \n \n \nFigure 2.  \nIllustration of prompt and answer of LLM \n \nThe visual questions were sequentially added to the \nsame chat session. Each multimodal LLMs was \npresented in 100 questions, and responses were \nrecorded. Multimodal LLMs were not pretrained with \na specific command or question set for this study. \nEach questi on was posed in a single chat session, \nwithout opening a new chat tab for individual \ninquiries. Radiologist (Y.C.G.) and anatomist (M.Ü) \njointly evaluated the multimodal LLMs’ answers \naccording to the correct answer list provided by \nRadiopaedia either correct (1) or incorrect (0). \nRadiologist and anatomist performance evaluation \nBoard-certified (EDiR) radiologist (Y.C.G.) and \nanatomist (M.Ü.), each with 6 years of experience, \nindependently assessed the visual questions using their \nown computers.  Upon compl etion of questions, they \nevaluated each other's answers according to the \ncorrect answer list provided by Radiopaedia either \ncorrect (1) or incorrect (0). \nStatistical analysis \nBasic descriptive statistics, including counts and \npercentages, were employed to analyze the \nperformance of GPT4 -V, GPT -4o, LLaVA, Gemini \n1.5 Flash, radiologists, and anatomists. McNemar’s \ntest was utilized to compare the proportions of correct \nresponses among these groups. All statistical analyses \nwere conducted using SPSS 26.0, with statistical \nsignificance defined as p<0.05. \nResults \nA total of 100 visual neuroanatomy MCQs were \nincluded in the study. The radiologist correctly \nanswered 90% (90/100 questions), surpassing the \nanatomist who scored 67% (67/100 questions). GPT -\n4o responded accurately to 45% (45/100 questions), \nfollowed by Gemini 1.5 Flash with 35% (35/100 \nquestions), GPT4-V with 22% (22/100 questions), and \nLLaVA with 15% (15/100 questions) (Table I, Figure \n3).  \n \nTable I. Diagnostic accuracy and classification by \nquestion types \n  Accuracy \n(MRI) \nAccuracy \n(CT) \nAccuracy \n(Spatial) \nTotal \nAccuracy \nRadiologist \n96,3% 100 70.4% 90.0% \n(53/55) (18/18) (19/27) (90/100) \nAnatomist \n63.6% 44.4% 88.9% 67.0% \n(35/55) (8/18) (24/27) (67/100) \nGPT4-V \n12.7% 27.8% 37.0% 22.0% \n(7/55) (5/18) (10/27) (22/100) \nLLaVA \n10.9% 16.7% 22.2% 15.0% \n(6/55) (3/18) (6/27) (15/100) \nGemini 1.5 \nFlash \n36.3% 44.4% 25.9% 35.0% \n(20/55) (8/18) (7/27) (35/100) \nGPT-4o \n43.6% 16.7% 66.7% 45% \n(24/55) (3/18) (18/27) (45/100) \nGPT4-V: Generative Pre- Training Transformer- 4th series with \nVision, LLaVA: Large Language -and Vision Assistant, GPT -4o: \nGenerative Pre- Training Transformer- 4 t h  s e r i e s  o m n i ,  M R I :  n o n-\ncontrast magnetic resonance imaging, C+MRI: contrast enhanced \nmagnetic res onance imaging, CT: non -contrast computed \ntomography, DSA: digital subtraction angiography. \n \n \n \n \n \n\nY.C. Güneş ve M. Ulkir \n554 \n \nFigure 3.  \nDemonstration of accuracy of multimodal LLMs, \nradiologist and anatomist. LLMs: Large Language \nModels, GPT4-V: Generative Pre-Training \nTransformer-4th series with Vision, LLaVA: Large \nLanguage-and-Vision Assistant, GPT-4o: Generative \nPre-Training Transformer-4th series omni. \n \nComparatively, the radiologist demonstrated \nsignificantly higher diagnostic accuracy than the \nanatomist (p=0.008). Both medical professionals \noutperformed the multimodal LLMs (p<0.05). \nAmong the multimodal LLMs, GPT -4o exhibited the \nhighest rate of correct responses. Its performance \nsignificantly surpassed that of  GPT4-V and LLaVA \n(p<0.001), while showing no significant difference \ncompared to Gemini 1.5 Flash (p=0.123). \nFurthermore, the Gemini 1.5 Flash demonstrated a \nsignificant superiority over LLaVA and GPT4 -V \n(p<0.05). GPT4-V correctly answered more questions \nthan LLaVA, which was statistically significant \n(p=0.016) (Table II). \n \nTable II. Comparison of diagnostic accuracy of \nmultimodal LLMs, radiologist and \nanatomist with p -values obtained from \nMcNemar’s Test \n \nRadiolog\nist \nAnatomi\nst \nGPT4-\nV LLaVA GPT-\n4o \nGemini 1.5 \nFlash \nRadiologi\nst - <0.001 <0.001 <0.001 <0.001 <0.001 \nAnatomist <0.001 - <0.001 <0.001 0.015 0.002 \nGPT4-V <0.001 <0.001 - 0.016 0.001 0.004 \nLLAVa <0.001 <0.001 0.016 - <0.001 <0.001 \nGPT-4o <0.001 0.015 0.001 <0.001 - 0.123 \nGemini \n1.5 Flash <0.001 0.002 0.004 <0.001 0.123 - \n \nRadiologists demonstrated the highest performance \nacross all question types except for spatial anatomy \nquestions. In spatial anatomy questions, anatomists \nexhibited the highest accuracy rate (88.9%) compared \nto radiologists (70.4%). Among the LLMs, GPT -4o \nachieved the highest success rate (66.7%) in spatial \nanatomy questions. \nDiscussion and Conclusion \nIn our study, radiologists exhibited superior \nperformance in answering visual neuroanatomy \nquestions, significantly outperforming anatomists and \nmultimodal LL Ms (p <0.001). Anatomists showed \nbetter performance than multimodal LLMs (p<0.05). \nGPT-4o outperformed other multimodal LLMs \n(p<0.05), exception of Gemini 1.5 Flash (p=0.123). \nThe superior performance of these two multimodal \nLLMs compared to GPT -4V and LLaV A may be \nattributed to their more recent training with larger and \nmore advanced datasets.  Although GPT -4V \ndemonstrated the highest performance among the \nmultimodal LLMs in our study, its accuracy of 45% \nindicates that these models currently lack sufficient  \nproficiency in visual neuroanatomy. This underscores \nthe need for further development and training of \nLLMs with specialized medical image datasets. \nNotably, radiologists showed the highest performance \nin questions involving radiological evaluation, \nwhereas anatomists provided more correct answers, \nparticularly in spatial anatomy questions. The \nperformance differences between radiologists and \nanatomists can be attributed to the fact that the \nmajority of questions were related to sectional \nanatomy through ra diological imaging methods. The \nhigher performance of anatomists in spatial anatomy \nquestions may be due to their exposure to a greater \nnumber of spatial and non- spatial anatomy questions \nduring their training. This study suggests that \nanatomists should re ceive more training in sectional \nanatomy based on radiological imaging during their \neducation. \nThere are studies in the literature evaluating the \nperformance of large language models in anatomy \nquestions. Bolgova et al. conducted a study assessing \nChatGPT 3.5's performance in answering text -based \nmultiple-choice questions (MCQs) across various \nanatomical regions\n19. Out of a total of 325 questions, \nChatGPT 3.5 successfully answered 44.1% of them. \nSpecifically focusing on neuroanatomy questions \npertaining to the head and neck region, it achieved an \napproximate success rate of 48.8% out of 50 \nquestions\n19. Ilgaz et al.'s study revealed that both \nChatGPT 3.5 and Google Bard performed below 50% \naccuracy in answering text -based non-spatial anatomy \nquestions. Furthe rmore, the study found no \nstatistically significant difference in ChatGPT 3.5's \nperformance between anatomy questions asked in \nTurkish and English\n12.  \nStudies have highlighted the utility of LLMs in \nanatomy education. Lee indicated that integrating \nChatGPT into anatomy education could improve \n\nLanguage Models and Experts in Neuroanatomy \n555 \nefficacy and students' engagement in the subject. \nHowever, concerns were raised regarding ChatGPT's \ntendency to generate hallucinations and provide \ninaccurate responses 20. Mogaliet al. showcased \nChatGPT's potential as an online anatomy tutor 21. \nSimilarly, Totlis et al. demonstrated the effectiveness \nof GPT 4 in generating and addressing various types \nof anatomy -related questions for learning purposes 22. \nThe low performance of the multimodal language \nmodels (LLMs) in recognizing neuroanatomical \nregions in our study precludes their consideration as a \nreliable standalone source for visual neuroanatomy \neducation. \nRecent advancements in multimodal LLMs, driven by \nthe development of visual evaluation of images that \nhave led to the creation of models tailored to the \nhealthcare domain, such as LLaVA and CLIP 23. \nHowever, most studies evaluating the performance of \nmultimodal LLMs have primarily focused on X-rays24. \nThe inclusion of images from different radiological \nmodalities in our study may pose a challenge for \nmultimodal LLMs in providing accurate answers. It is \nnecessary to conduct studies utilizing different \nradiological modalities in order to demonstrate the \nefficacy of multimodal LLMs in clinical settings. \nZhu et al.  demonstrated that GPT -4V was able to \naccurately diagnose medical conditions with a 77% \naccuracy rate when given visual USMLE -style \nquestions25. Howeve r, when patient history was \nremoved, the accuracy rate dropped to 19.54% This \nsuggests that the model relies heavily on patient \nhistory to make accurate diagnoses. Node et al. found \nthat the model's accuracy varied depending on the type \nof question, with i mage-based questions being more \nchallenging in answering questions from the \notolaryngology board certification exam26. The correct \nanswer rate was 30.4% when only text was provided, \nbut increased to 41.3% when images were also \nincluded26. \nNakao et al.  tested GPT4 -V's ability to recognize \nimages in the Japanese National Medical Licensing \nExamination16. The model was able to correctly \nanswer 68% of image -based questions and 72% of \ntext-based questions. It is noteworthy that there was \nno significant difference  in the model's performance \non image -based versus text -based questions. In \ncontrast to Nakao et al.'s study, our study \ndemonstrated that both GPT -4o (45.3%) and GPT4- V \n(22.6%) performed lower in visual neuroanatomy \nquestions. We believe that the differences in clinical \nhistory and prompts may have caused these varying \nperformances among studies. Overall, these studies \nsuggest that multimodal LLMs like GPT4 -V have the \npotential to be useful tools in radiology, but may \nrequire further development to reach their full \npotential in the future. Moreover, the Gemini 1.5 \nFlash demonstrated comparable performance to GPT \nmodels, which may indicate the remarkable potential \nfor further development in this field. \nThere are few studies comparing the diagnostic \nperformance of GPT4-V and LLaVA in visual images, \nand these studies are primarily related to melanoma. \nCirone et al. demonstrated that GPT4 -V outperformed \nLLaVA in all evaluated aspects, achieving an overall \naccuracy of 85%, whereas LLaVa achieved 45%. \nGPT4-V consistently provided detailed descriptions of \nrelevant features of melanoma 14. Similarly, Akrout et \nal. also found that GPT4 -V performed better than \nLLaVA across all assessed features of melanoma 27. \nOur study is also consistent with these studies \nregarding GPT4 -V has better performance than \nLLaVA regarding image interpretation. \nLimitations \nAlthough our study makes a significant contribution to \nthe comparison between multimodal LLMs and \nmedical professionals, it has some limitations. Firstly, \nthe number of visual questions in the study is limited, \nwhich may not fully capture the complexity of \nneuroanatomy. A larger set of questions could provide \na more accurate assessment of multimodal LLMs' \ncompetence. Secondly, the use of different modalities \nin the study provides heterogeneous information about \nmultimodal LLMs' performance, but future studies \nshould test the performance of multimodal LLMs \nseparately for each radiological modality and visual \nanatomy question to gain a more nuanced \nunderstanding. Thirdly, the small sample size, \nconsisting of only one radiologist and one anatomist, \nmay limit the generalizability of the findings. \nIncluding a larger cohort with varying levels of \nexperience could provide more comprehensive \ninsights.  Lastly, the choice of prompt using t he role-\nplay technique may have influenced multimodal \nLLMs' performance. Prompts made using zero -shot \nand few -shot techniques could provide more detailed \ninformation about multimodal LLMs' performance in \nfuture studies. \nIn conclusion, t his study provides v aluable insights \ninto the comparative performance of multimodal \nLLMs and medical professionals in visual \nneuroanatomy assessment. While multimodal LLMs \ndemonstrate potential, they are not yet capable of \naccurately identifying neuroanatomical regions. \nFurther research and development are necessary to \nbridge the gap between the capabilities of LLMs and \nhuman expertise regarding neuroanatomical \nknowledge. \nFikir ve tasarım: Y.C.G., M.Ü.; Veri toplama ve \nişleme: Y.C.G., M.Ü.; Analiz ve verilerin \nyorumlanması: Y. C.G., M.Ü.; Makalenin önemli \nbölümlerinin yazılması: Y.C.G., M.Ü. \n \n \nY.C. Güneş ve M. Ulkir \n556 \nEthics Committee Approval Information: \nSince this study was conducted using publicly available internet \ndata and the images did not contain patient information, the study \ndid not require  an ethics committee. The study was conducted in \naccordance with the Standards for Reporting Studies on Diagnostic \nAccuracy (STARD) and the Checklist for Artificial Intelligence in \nMedical Imaging (CLAIM). \n \nResearcher Contribution Statement: \nIdea and desig n: Y.C.G., M.Ü.; Data collection and processing: \nY.C.G., M.Ü.; Analysis and interpretation of data: Y.C.G., M.Ü.; \nWriting of significant parts of the article: Y.C.G., M.Ü.; \nSupport and Acknowledgement Statement: \nThe authors used ChatGPT 4o (September 2024 Release; OpenAI; \nhttps://chat.openai.com/) to review the grammar and English \ntranslation. The content of the publication is the sole responsibility \nof the authors, who reviewed and edited it as they deemed \nnecessary. \nThe authors would like to thank Juliette Hancox, Image Licensing \nManager at Radiopaedia.org, for permission to use the images on \nthe website. \nConflict of Interest Statement: \nThe authors of the article have no conflict of interest declarations. \nReferences \n1. Clusmann J, Kolbinger FR, Muti HS, Carrero ZI, Eckardt JN, \nLaleh NG, Löffler CML, Schwarzkopf SC, Unger M, \nVeldhuizen GP, Wagner SJ, Kather JN (2023) The future \nlandscape of large language models in medicine. Commun Med \n(Lond) 3:141. https://doi.org/10.1038/s43856-023-00370-1 \n2. GPT-4 is OpenAI’s most advanced system, producing safer and \nmore useful responses. OpenAI.https://openai.com/gpt-4 / GPT-\n4V(ision) System Card. OpenAI. Accessed Date Accessed \n3. Liu H, Li C, Wu Q, Lee YJ (2024) Visual instruction tuning. \nAdv Neural Inf Process Syst 36 \n4. https://deepmind.google/technologies/gemini/flash/. Accessed \nDate Accessed \n5. https://openai.com/index/hello-gpt-4o/. Accessed Date \nAccessed \n6. Kuang Y -R, Zou M -X, Niu H -Q, Zheng B -Y , Zhang T -L, \nZheng B-W (2023) ChatGPT encounters multiple opportunities \nand challenges in neurosurgery. Int J Surg 109:2886 -2891. \nhttps://doi.org/doi: 10.1097/JS9.0000000000000571 \n7. Gunes YC, Camur E, Cesur T (2024) Correspondence on \n‘Evaluation of ChatGPT in knowledge of newly evolving  \nneurosurgery: middle meningeal artery embolization for \nsubdural hematoma management’by Koester et al. J \nNeurointerv Surg \n8. Andykarayalar R, Surapaneni KM (2024) ChatGPT in \nPediatrics: Unraveling Its Significance as a Clinical Decision \nSupport Tool. Indian Pediatr 61:357-358 \n9. Dinis-Oliveira RJ, Azevedo RM (2023) ChatGPT in forensic \nsciences: a new Pandora’s box with advantages and challenges \nto pay attention. Forensic Sci Res 8:275- 279. \nhttps://doi.org/doi: 10.1093/fsr/owad039 \n10. Elkassem AA, Smith AD ( 2023) Potential use cases for \nChatGPT in radiology reporting. American Journal of \nRoentgenology 221:373-376. \nhttps://doi.org/10.2214/AJR.23.29198 \n11. Ferreira AL, Chu B, Grant -Kels JM, Ogunleye T, Lipoff JB \n(2023) Evaluation of ChatGPT dermatology response s to \ncommon patient queries. JMIR dermatol 6:e49280. \nhttps://doi.org/doi: 10.2196/49280 \n12. Ilgaz HB, Çelik Z (2023) The significance of artificial \nintelligence platforms in anatomy education: an experience \nwith ChatGPT and google bard. Cureus 15:e45301. \nhttps://doi.org/doi: 10.7759/cureus.45301 \n13. Langlie J, Kamrava B, Pasick LJ, Mei C, Hoffer ME (2024) \nArtificial intelligence and ChatGPT: An otolaryngology \npatient's ally or foe? Am J Otolaryngol 45:104220. \nhttps://doi.org/doi: 10.1016/j.amjoto.2024.104220 \n14. Cirone K, Akrout M, Abid L, Oakley A (2024) Assessing the \nutility of multimodal large language models (GPT -4 vision and \nlarge language and vision assistant) in identifying melanoma \nacross different skin tones. JMIR dermatol 7:e55508. \nhttps://doi.org/doi: 10.2196/55508 \n15. Deng J, Heybati K, Shammas -Toma M (2024) When vision \nmeets reality: Exploring the clinical applicability of GPT-4 with \nvision. Clin Imaging 108:110101. https://doi.org/doi: \n10.1016/j.clinimag.2024.110101 \n16. Nakao T, Miki S, Nakamura Y , Kikuchi T, Nomura Y , Hanaoka \nS, Yoshikawa T, Abe O (2024) Capability of GPT-4V (ision) in \nthe Japanese National Medical Licensing Examination: \nEvaluation Study. JMIR Med Educ 10:e54393. \nhttps://doi.org/doi: 10.2196/54393 \n17. Bossuyt PM, Reitsma JB, Bruns DE, Gatsonis CA, Glasziou PP, \nIrwig L, Lijmer JG, Moher D, Rennie D, De Vet HC (2015) \nSTARD 2015: an updated list of essential items for reporting \ndiagnostic accuracy studies. Radiology 277:826 -832. \nhttps://doi.org/doi: 10.1136/bmj.h5527 \n18. Mongan J, Moy L, Kahn CE, Jr. (2020) Checklist for Artificial \nIntelligence in Medical Imaging (CLAIM): A Guide for Authors \nand Reviewers. Radiol Artif Intell 2:e200029. \nhttps://doi.org/10.1148/ryai.2020200029 \n19. Bolgova O, Shypilova I, Sankova L, Mavrych V (2023) Ho w \nWell Did ChatGPT Perform in Answering Questions on \nDifferent Topics in Gross Anatomy? EJMED 5:94- 100. \nhttps://doi.org/doi: 10.24018/ejmed.2023.5.6.1989 \n20. Lee H (2023) The rise of ChatGPT: Exploring its potential in \nmedical education. Anat Sci Educ. htt ps://doi.org/doi: \n10.1002/ase.2270 \n21. Mogali SR (2024) Initial impressions of ChatGPT for anatomy \neducation. Anat Sci Educ 17:444 -447. https://doi.org/doi: \n10.1002/ase.2261 \n22. Totlis T, Natsis K, Filos D, Ediaroglou V , Mantzou N, Duparc \nF, Piagkou M (202 3) The potential role of ChatGPT and \nartificial intelligence in anatomy education: a conversation with \nChatGPT. Surg Radiol Anat 45:1321-1329 \n23. Li C, Wong C, Zhang S, Usuyama N, Liu H, Yang J, Naumann \nT, Poon H, Gao J (2024) Llava-med: Training a large language-\nand-vision assistant for biomedicine in one day. Adv Neural Inf \nProcess Syst 36. https://doi.org/doi: \n10.48550/arXiv.2306.00890 \n24. Monajatipoor M, Rouhsedaghat M, Li LH, Jay Kuo C-C, Chien \nA, Chang K -W (2022) Berthop: An effective vision- and-\nlanguage model for chest x -ray disease diagnosis. International \nConference on Medical Image Computing and Computer -\nAssisted Intervention:725 -734. https://doi.org/doi: \n10.48550/arXiv.2108.04938 \n25. Zhu L, Mou W, Lai Y , Chen J, Lin S, Xu L, Lin J, Guo Z, Yang \nT, Lin A (2024) Step into the era of large multimodal models: A \npilot study on ChatGPT -4V (ision)‘s ability to interpret \nradiological images. Int J Surg:10.1097. https://doi.org/doi: \n10.1097/JS9.0000000000001359 \n26. Noda M, Ueno T, Koshu R, Takaso Y , Shimada MD, Saito C, \nSugimoto H, Fushiki H, Ito M, Nomura A (2024) Performance \nof GPT -4V in Answering the Japanese Otolaryngology Board \nCertification Examination Questions: Evaluation Study. JMIR \nMed Educ 10:e57054. https://doi.org/doi: 10.2196/57054 \n27. Akrout M, Cirone KD, Vender R (2024) Evaluation of Vision \nLLMs GTP -4V and LLaVA for the Recognition of Features \nCharacteristic of Melanoma. J Cutan Med Surg 28:98 -99. \nhttps://doi.org/doi: 10.1177/12034754231220934 "
}