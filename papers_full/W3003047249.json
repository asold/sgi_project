{
  "title": "Migrating from partial least squares discriminant analysis to artificial neural networks: a comparison of functionally equivalent visualisation and feature contribution tools using jupyter notebooks",
  "url": "https://openalex.org/W3003047249",
  "year": 2020,
  "authors": [
    {
      "id": null,
      "name": "Mendez, Kevin M.",
      "affiliations": [
        "Edith Cowan University"
      ]
    },
    {
      "id": null,
      "name": "Broadhurst, David I.",
      "affiliations": [
        "Edith Cowan University"
      ]
    },
    {
      "id": null,
      "name": "Reinke, Stacey N.",
      "affiliations": [
        "Edith Cowan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4388297464",
    "https://openalex.org/W2911964244",
    "https://openalex.org/W2143283561",
    "https://openalex.org/W2802842185",
    "https://openalex.org/W2193923663",
    "https://openalex.org/W1976251851",
    "https://openalex.org/W2338241819",
    "https://openalex.org/W1507985183",
    "https://openalex.org/W2950871538",
    "https://openalex.org/W2154631167",
    "https://openalex.org/W2006820723",
    "https://openalex.org/W2126728600",
    "https://openalex.org/W1971034820",
    "https://openalex.org/W2066860102",
    "https://openalex.org/W2056073190",
    "https://openalex.org/W2200140537",
    "https://openalex.org/W1833005471",
    "https://openalex.org/W2158863190",
    "https://openalex.org/W2070723789",
    "https://openalex.org/W1994471338",
    "https://openalex.org/W2089181989",
    "https://openalex.org/W2787894218",
    "https://openalex.org/W6602347711",
    "https://openalex.org/W2076161502",
    "https://openalex.org/W2142316424",
    "https://openalex.org/W2342249984",
    "https://openalex.org/W2980773467",
    "https://openalex.org/W2972529175",
    "https://openalex.org/W2988932415",
    "https://openalex.org/W2106100979",
    "https://openalex.org/W2169053895",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2897185348",
    "https://openalex.org/W2950985821",
    "https://openalex.org/W6792841710",
    "https://openalex.org/W1987972238",
    "https://openalex.org/W2054625992",
    "https://openalex.org/W2124911115",
    "https://openalex.org/W1992483596",
    "https://openalex.org/W4235243453",
    "https://openalex.org/W1987336542",
    "https://openalex.org/W2073503722",
    "https://openalex.org/W2079529928",
    "https://openalex.org/W1974086876",
    "https://openalex.org/W1908991493",
    "https://openalex.org/W1554663460",
    "https://openalex.org/W1480376833",
    "https://openalex.org/W20683381",
    "https://openalex.org/W3103145119",
    "https://openalex.org/W1698155719",
    "https://openalex.org/W3150545588",
    "https://openalex.org/W2384495648",
    "https://openalex.org/W2953300457",
    "https://openalex.org/W2603713028",
    "https://openalex.org/W2101234009"
  ],
  "abstract": "Abstract Introduction Metabolomics data is commonly modelled multivariately using partial least squares discriminant analysis (PLS-DA). Its success is primarily due to ease of interpretation, through projection to latent structures, and transparent assessment of feature importance using regression coefficients and Variable Importance in Projection scores. In recent years several non-linear machine learning (ML) methods have grown in popularity but with limited uptake essentially due to convoluted optimisation and interpretation. Artificial neural networks (ANNs) are a non-linear projection-based ML method that share a structural equivalence with PLS, and as such should be amenable to equivalent optimisation and interpretation methods. Objectives We hypothesise that standardised optimisation, visualisation, evaluation and statistical inference techniques commonly used by metabolomics researchers for PLS-DA can be migrated to a non-linear, single hidden layer, ANN. Methods We compared a standardised optimisation, visualisation, evaluation and statistical inference techniques workflow for PLS with the proposed ANN workflow. Both workflows were implemented in the Python programming language. All code and results have been made publicly available as Jupyter notebooks on GitHub. Results The migration of the PLS workflow to a non-linear, single hidden layer, ANN was successful. There was a similarity in significant metabolites determined using PLS model coefficients and ANN Connection Weight Approach. Conclusion We have shown that it is possible to migrate the standardised PLS-DA workflow to simple non-linear ANNs. This result opens the door for more widespread use and to the investigation of transparent interpretation of more complex ANN architectures.",
  "full_text": "Vol.:(0123456789)1 3\nMetabolomics (2020) 16:17 \nhttps://doi.org/10.1007/s11306-020-1640-0\nORIGINAL ARTICLE\nMigrating from partial least squares discriminant analysis to artificial \nneural networks: a comparison of functionally equivalent visualisation \nand feature contribution tools using jupyter notebooks\nKevin M. Mendez1  · David I. Broadhurst1  · Stacey N. Reinke1 \nReceived: 30 November 2019 / Accepted: 13 January 2020 / Published online: 21 January 2020 \n© The Author(s) 2020\nAbstract\nIntroduction Metabolomics data is commonly modelled multivariately using partial least squares discriminant analysis (PLS-\nDA). Its success is primarily due to ease of interpretation, through projection to latent structures, and transparent assessment \nof feature importance using regression coefficients and Variable Importance in Projection scores. In recent years several \nnon-linear machine learning (ML) methods have grown in popularity but with limited uptake essentially due to convoluted \noptimisation and interpretation. Artificial neural networks (ANNs) are a non-linear projection-based ML method that share \na structural equivalence with PLS, and as such should be amenable to equivalent optimisation and interpretation methods.\nObjectives We hypothesise that standardised optimisation, visualisation, evaluation and statistical inference techniques \ncommonly used by metabolomics researchers for PLS-DA can be migrated to a non-linear, single hidden layer, ANN.\nMethods We compared a standardised optimisation, visualisation, evaluation and statistical inference techniques workflow \nfor PLS with the proposed ANN workflow. Both workflows were implemented in the Python programming language. All \ncode and results have been made publicly available as Jupyter notebooks on GitHub.\nResults The migration of the PLS workflow to a non-linear, single hidden layer, ANN was successful. There was a similarity \nin significant metabolites determined using PLS model coefficients and ANN Connection Weight Approach.\nConclusion We have shown that it is possible to migrate the standardised PLS-DA workflow to simple non-linear ANNs. \nThis result opens the door for more widespread use and to the investigation of transparent interpretation of more complex \nANN architectures.\nKeywords Metabolomics · Partial least squares · Artificial neural networks · Machine learning · Jupyter · Variable \nimportance in projection\n1 Introduction\nWithin a biological system, metabolite concentrations are \nhighly interdependent (Dunn et al. 2011). As such, the use-\nfulness of multivariate data analysis in metabolomics stems \nfrom the need to extract biological information from inher -\nently complex covariant data, where metabolite interac-\ntion is as important as individual changes in concentration. \nHistorically, partial least squares (PLS), a.k.a. projection to \nlatent structures (Wold 1975 ; Wold et al. 1993), has been \nthe standard multivariate machine learning (ML) method \nused to construct predictive models to classify metabolite \nprofiles. The underlying theory of PLS, and its utility to \nmetabolomics, has been documented many times (Geladi \nand Kowalski 1986; Gromski et al. 2015; Wold et al. 1993, \n2001). A key benefit of PLS is the ability to visualise (via a \nElectronic supplementary material The online version of this \narticle (https ://doi.org/10.1007/s1130 6-020-1640-0) contains \nsupplementary material, which is available to authorized users.\n * David I. Broadhurst \n d.broadhurst@ecu.edu.au\n * Stacey N. Reinke \n stacey.n.reinke@ecu.edu.au\n1 Centre for Integrative Metabolomics & Computational \nBiology, School of Science, Edith Cowan University, \nJoondalup 6027, Australia\n K. M. Mendez et al.\n1 317 Page 2 of 15\nlatent variable score plot) the projected metabolomic rela-\ntionship (clustering) between individual samples before \nclassification.\nThere are many machine learning (ML) alternatives to \nPLS, several of which have been applied to metabolomics \ndata. The most popular include support vector machines \n(Steinwart and Christmann, 2008), random forests (Breiman \n2001), and artificial neural networks (Bishop 1995; Wilkins \net al. 1994); however, despite coexisting for a similar length \nof time, none of these methods have gained the popular -\nity of PLS. A survey of publications listed on the Web of \nScience using the keywords metabolite*, metabolom* or \nmetabonom* reveals that up to and including 2018, 2224 \npublications list the use of PLS as a key term, whereas the \nalternatives were listed  < 500 times (combined number). \nThe key to the popularity of PLS over alternative methods \ncan be distilled into a single word—interpretability. His-\ntorically, the primary aim of machine learning (ML) has \nbeen accurate prediction, not statistical inference (Mendez \net al. 2019a). As such, methods for statistically interpreting \neither the similarities between each individual metabolite \nprofile, or the importance of individual metabolites across \nmultiple samples, have been a secondary consideration. The \nability for PLS to visualise and infer statistical confidence \nintervals upon the latent relationships within and between \nsample classes, together with the fact that a PLS model can \nbe reduced to a simple linear regression (and thus exposed \nto multiple well established post-hoc statistical tests), means \nthat it sits alone as an effective hybrid prediction-inference \nalgorithm for high dimensional data (Eriksson et al. 2013; \nWold 1975; Wold et al. 1993).\nArtificial neural networks (ANNs) are also of particu-\nlar interest because in their simplest form, as with PLS, \nthey can be considered as a combination of dimensional-\nity reduction and multiple linear regression. In fact, for a \nlinear ANN, with a single hidden layer, the only difference \nbetween ANN and PLS is the manner in which the constitu-\nent model parameters are optimised (Fig.  1). ANNs can be \ngenerally considered a projection-based method which share \na structural equivalence with PLS (Mendez et al. 2019a). \nWith non-linear ANNs the projection to latent structures  \nethos is preserved but now non-linear, rather than linear, \nlatent structures can be modelled.\nANNs were first applied to metabolomic profiling ca. \n1992 by Goodacre et al. (1992). At that time, due to lack of \ncompute power and poor software availability, ANNs were \nvery slow to train and considered difficult to interpret. As \nsuch, by the early 2000s they had been widely disregarded \nand relegated to an intellectual curiosity not considered able \nto provide meaningful biological insight (Goodacre 2003). \nWith recent advancements in computational power, the \navailability of easily accessible yet powerful open-source \npackages (e.g. TensorFlow and PyTorch), and the general \nsuccess within industry and other research fields, the rein-\ntroduction of ANNs warrants renewed investigation. We \nrecently showed that ANNs have similar predictive ability to \nPLS across multiple diverse metabolomics data sets (Men-\ndez et al. 2019c ). However, within the domain of metabo-\nlomics, if ANNs are to become a truly viable alternative \nto PLS it will be necessary to develop similar standardised \nand robust methods for data visualisation, evaluation, and \nstatistical inference (Mendez et al. 2019a).\nRecently, the increased availability of well curated open-\nsource software libraries, particularly from R and Python \nprogramming communities, has increased the availability \nand utility of many ML methods, including ANNs. Moreo-\nver, the massive increase in available computer power has \nreduced compute times such that methods previously intrac-\ntable due to computational expense, such as bootstrap confi-\ndence intervals (Efron 1988), have enabled non-parametric \nstatistical inference to be derived for previously considered \nuninterpretable ‘black box’ methods. This opens the door \nfor the development of an ANN framework comparable to \nthat of PLS-DA.\nThe aim of this study is to migrate the standardised \noptimisation, visualisation, evaluation, and statistical \ninference techniques commonly used in a PLS-DA binary \nclassification over to a non-linear, single hidden layer, \nANN algorithm, and then conduct a direct comparison of \nutility. We provide two functionally equivalent workflows \n(PLS-DA vs. ANN) implemented using the Python pro-\ngramming language, and presented as open-access Jupyter \nNotebooks (https ://cimcb .githu b.io/Metab Proje ction Viz/). \nThe workflows were applied to two previously published \nmetabolomics datasets by Chan et al. (2016) and Ganna et al. \n(2016), but are written to be used with any data set suit-\nably formatted following previous guidelines (Mendez et al. \n2019b). Both workflows include cross-validated hyperpa-\nrameter optimisation, latent variable projection scores plots, \nclassification evaluation using receiver operator character -\nistic curves, bootstrap resampling for statistical inference \nof feature contribution and generalisability of prediction \nmetrics.\n2  Methods\n2.1  Partial least squares discriminant analysis \n(PLS‑DA)\nPLS-DA (Wold 1975; Wold et al. 1993) is a widely used \nmultivariate ML algorithm used for classifying and inter -\npreting metabolomics data, especially applicable when the \nnumber of metabolites (independent variables) is much \nlarger than the number of data points (samples). PLS uses \nthe projection to latent space approach to model the linear \nMigrating from partial least squares discriminant analysis to artificial neural networks:…\n1 3\nPage 3 of 15 17\ncovariance structure between two matrices (X  and Y ). \nIf the X  matrix is thought of as a set of N  data points in \nM-dimensional space (where, N  = number of samples, and   \nM = number of metabolites), and Y is a binary vector (length \nN) describing the class of each samples (e.g. case = 1 and \ncontrol = 0), and if we consider the algorithm geometrically, \nthe PLS algorithm rotates and projects X into a lower K \ndimensional space (typically K  = 2 or 3), represented by \nthe scores matrix T , such that discrimination (covariance) \nbetween the two labelled groups in the subspace is maxim-\nised (Eriksson et al. 2013). For this study, PLS-DA models \nwas optimised using the iterative SIMPLS algorithm (de \nJong, 1993). T can be derived from X using Eq. (1), where \nW, the X-weight matrix, describes how the X-variables are \nFig. 1  Illustration of an ANN \nas a regression model. a \nNetwork representation of a \n2-layer ANN. b Representation \nof a 2-layer ANN with linear \nactivation functions, as a set of \nequations, simplified to a linear \nregression model\n\n K. M. Mendez et al.\n1 317 Page 4 of 15\nlinearly combined, or geometrically rotated, to form the \nscore vectors, t1 t2 … tK .\nThe predicted classification (Y*) can then be calculated \nfrom T using Eq. (2 ), where C is the Y-weights matrix \ndescribing how the Y vector is rotated to map to the covari-\nance described by T.\nThese matrix equations, Eq. (1) and Eq. (2), can be com-\nbined and simplified to a single linear regression, Eq. (3 ), \nwhere BPLS is a vector of coefficient values.\nThis matrix equation, Eq. (3 ), can also be described as \na single linear regression in standard form, Eq. (4 ), where \n/u1D6FD0 .../u1D6FDN is a vector of linear coefficients.\n2.1.1  PLS‑DA optimisation\nThe optimal number of latent variables, K , is determined \nsuch that the T matrix is just sufficient to accurately describe \nthe underlying latent structure in X but not so large as to also \nmodel random correlation and produce a model that is a poor \nclassification tool for new X-data (see cross-validation in \nSect. 3.4). In machine learning terminology any parameter \nwhich is used to define a model’s structure, or an optimisa-\ntion algorithm characteristic, is known as a hyperparameter. \nThus, the number of latent variables is the single PLS-DA \nhyperparameter.\n2.1.2  PLS‑DA evaluation\nIn order to provide some level of independent model evalu-\nation it is common practice to split the source data set into \ntwo parts: training set and test set (typically, 2/3 training \nand 1/3 test). Once the optimal number of latent variables \nhas been determined using the training data only ( \n/u1D417/u1D42D/u1D42B/u1D41A/u1D422/u1D427 and \n/u1D418/u1D42D/u1D42B/u1D41A/u1D422/u1D427 ), the resulting model, /u1D418∗ = /u1D417/u1D401/u1D40F/u1D40B/u1D412 , is then indepen-\ndently evaluated by applying the test data ( /u1D417/u1D42D/u1D41E/u1D42C/u1D42D ; suitably \ntransformed and scaled) to the model, /u1D418∗\n/u1D413/u1D41E/u1D42C/u1D42D= /u1D417/u1D42D/u1D41E/u1D42C/u1D42D/u1D401/u1D40F/u1D40B/u1D412 . \nA measure of the predictive ability of the model can then be \ncalculated by comparing the training prediction \n(/u1D418∗\n/u1D42D/u1D42B/u1D41A/u1D422/u1D427) to \nthe expected training outcome (Y train), and the test predic-\ntion (Ytest\n*    ) to the expected test outcome (Ytest).\n(1)/u1D413=/u1D417/u1D416\n(2)/u1D418∗ = /u1D413/u1D402�\n/u1D418∗ = /u1D413/u1D402�\n/u1D418∗ = /u1D417/u1D416/u1D402�\n(3)/u1D418∗ = /u1D417/u1D401/u1D40F/u1D40B/u1D412\n(4)y∗ = /u1D6FD0 + /u1D6FD1x1 + /u1D6FD1x2 +…+/u1D6FDM xM\nWhile true effectiveness of a model can only be assessed \nusing test data (Westerhuis et al. 2008; Xia et al. 2013), for \nsmall data sets it is dangerous to use a single random data \nsplit as the only means of model evaluation, as the random \ntest data set may not accurately represent the training data \nset (Mendez et al. 2019c). An alternative is to use bootstrap \nresampling. Bootstrap resampling is a method for calculating \nconfidence intervals using random sampling with replace-\nment (DiCiccio and Efron 1996 ; Efron 1981 , 2000). The \ntheoretical details of this methodology are beyond the scope \nof this paper. Briefly, this technique allows the accurate esti-\nmation of the sampling distribution of almost any statis -\ntic using repeated random sampling. Each random sample \nselects ~ 2/3 of the data points (called the in-bag sample) \nleaving ~ 1/3 (the out-of-bag sample).\nBootstrapping can be used to calculate confidence meas-\nurements for the evaluating the optimal ML model con-\nfiguration for a given metabolomics data set (Broadhurst \nand Kell 2006; Mendez et al. 2019b; Xia et al. 2013). A \nmodel with fixed hyperparameter values is retrained on \ndata, randomly sampled with replacement (in-bag), and then \nevaluated on the unused data (out-of-bag) for r  resamples \n(typically r = 100). The predicted outcome from each in-\nbag bootstrap resample as well as other outputs, including \nthe predicted outcome, latent scores, latent loadings, and \nfeature contribution metrics are stored after each resam-\npling. The out-of-bag prediction of classification is also \nstored, as this can be considered an unbiased estimate of the \nmodel’s performance when shown new data. Using these \nstored outputs, 95% confidence intervals are calculated using \nthe commonly-used bias-corrected and accelerated (BCa) \nmethod; this method adjusts the percentiles to account for \nthe bias and skewness in the bootstrap distribution (Efron \n1987). Following bootstrap resampling, a measure of gener-\nalised prediction of each model is calculated as the median \nand 95% confidence intervals of the in-bag and out-of-bag \npredictions.\n2.1.3  PLS‑DA visualisation\nFor a given PLS-DA model it is common practice to visu-\nalise the projection of X  into the latent variable space to \nprovide a generalised understanding of the metabolomic \nrelationship (clustering) between individual samples before \nclassification. For this, the scores matrix, T , described in \nEq. (1 ), can be represented as a scatter plot (scores plot) \nsuch that each axis of the plot represents a column of the \nT-matrix. For example, a scatter plot of  t\n1 vs.  t2 will repre-\nsent the projections of X onto the first two latent variables \n(i.e. each data point represents a projection of a given sam-\nple’s metabolite profile). It is in this latent variable space \nthat one would expect to see different metabotypes cluster. \nThe associated weight vectors (columns of W ) can also be \nMigrating from partial least squares discriminant analysis to artificial neural networks:…\n1 3\nPage 5 of 15 17\nvisualised individually and interpreted as an indication of \nhow the X-variables are linearly combined to create each \nscore vector, Eq. (5).\nFor a single optimised model, latent scores plots can \nbe generated for training, cross-validation, and test X-data \nsets independently. This is a useful method for determin-\ning if overtraining has occurred (see supplementary Jupyter \nNotebooks).\n2.1.4  PLS‑DA variable contribution\nFor PLS-DA, there are two common methods used to esti-\nmate variable contribution. First, as discussed, a PLS-DA \nmodel can be reduced to a single multiple linear regression, \nEq. (3 ), thus feature contribution can be inferred directly \nfrom the model’s regression coefficients, B\nPLS. Second, for \nmore of a focus on the importance of the X-variables on the \nlatent projection, the variable influence on projection (VIP) \nscores can be calculated using Eq. (6 ) (Favilla et al. 2013). \nVIP is the weighted,\n,w 2\ni combination of the sum of squares \nof Y explained by each latent variable, SSY i , normalised to \nthe cumulative sum of square, SSYcum,\nwhere M is the total number of metabolites, and K is the \ntotal number of latent variables.\nThe average VIP score is equal to 1 because the sum of \nsquares of all VIP scores is equal to the number of variables \nin X. Thus, if all X-variables have the same contribution to \nthe model, they will have a VIP score equal to 1. VIP scores \nlarger than 1 indicate the most relevant variables. Bootstrap \nresampling (Sect. 2.1.2) can be applied to calculate 95% \nconfidence intervals for both the B\nPLS coefficient values and \nVIP scores, from which estimates of significant contribution \nto the model can be determined.\n2.2  Artificial neural network (ANN)\nANNs consist of layered weighted networks of intercon-\nnected mathematical operators (neurons). The most preva -\nlent ANN is the feed-forward neural network. Here, each \nneuron acts as a weighted sum of the outputs of the previous \nlayer (or input data) transformed by an activation function \n(typically linear or logistic function). This is described in \nEq. (7), using notation from Fig.  1a, where \ntj is the output \nt1 = w0,1 + w1,1x1 + w2,1x2 +…+w M ,1xM\nt2 = w0,2 + w1,2x2 + w2,2x2 +…+w M ,2xM\n…\ntK = w0,K + w1,K x1 + w2,K x2 +…+w M ,K xM\n(5)\n(6)/u1D415/u1D408/u1D40F=\n�\nM ×\n∑K\ni=1w2\ni × SSYi\nSSYcum\nfor the j th neuron in the hidden layer, f0 is the activation \nfunction, x is a vector of input variables  (x1,  x2, …,  xM), w i,j \nis the weight from input variable,  xi, to the neuron, and w0,j \nis a constant offset value.\nA neuron with a linear activation function connected to \nmultiple input variables is mathematically equivalent to \na linear regression with multiple independent variables, \nEq. (8), where  w\n0,j …  wN,j is a vector of linear coefficients.\nA neuron with a logistic activation function,  f0 (), is \nequivalent to the multivariate logistic regression describe \nin Eq. (9).\nAn ANN with a single linear hidden layer and a single \nlinear output neuron is mathematically equivalent to a PLS-\nDA model (Fig.  1). Replacing all the linear neurons with \nlogistic neurons in the two-layer ANN results in a complex \nnon-linear projection-based discriminant model. For this \nstudy, we use a two-layer ANN with logistic activation func-\ntions in both layers.\n2.2.1  ANN optimisation\nDuring ANN training, the interconnection weights between \neach layer of neurons are optimised using an iterative algo-\nrithm known as back-propagation. This algorithm has been \ndescribed in detail elsewhere (Bishop 1995). The effec-\ntiveness of this optimisation method is dependent on a set \nof hyperparameters.  A two-layer feedforward ANN has 5 \nhyperparameters: 1 parameter to determine the model struc-\nture, the number of neurons in the hidden layer (equivalent \nto number of latent variables) and 4 parameters that char -\nacterise the learning process. These determine the rate and \nmomentum of traversing local error gradients (specifically \nlearning rate, momentum, and decay of the learning rate \nover time) and the number of times the back-propagation \nis applied to the ANN (the number of training epochs). For \nthis study, preliminary explorative analysis indicated that \nhyperparameters: momentum, decay, epochs could be set to \na constant value (0.5, 0 and 400 respectively) with little vari-\nation on performance. This reduced the number of tuneable \nhyperparameters to: (i) the number of neurons in the hidden \nlayer, and (ii) the learning rate.\n(7)\ntj = f0\n/parenleft.s4\nw 0,j +\nM/uni2211.s1\ni=1\nw i,j × xi\n/parenright.s4\n(8)tj = w 0,j + w 1,jx1 + w 2,jx2 + ⋯+ w M ,jxM\n(9)tj = 1\n1 + e\n−\n�\nw 0,j+∑M\ni=1 w i,j×xi\n�\n K. M. Mendez et al.\n1 317 Page 6 of 15\n2.2.2  ANN evaluation\nModel evaluation using a test set and model evaluation \nusing bootstrap resampling is identical to that described in \nSect. 2.1.2. except replacing the PLS-DA prediction,  Y\n*, \nwith the ANN equivalent.\n2.2.3  ANN visualisation\nFor an equivalent representation of the PLS-DA projection \nto latent space, we provide a projection to neuron space. \nEach hidden neuron represents a transformed weighted \nsum of the X-variables (Eq.  7). Thus, for each pairwise \ncombination of neurons, plotting the weighted sum before \ntransformation provides a similar means to PLS-DA for \nvisualising and interpreting any clustering between indi-\nvidual samples before classification. Similarly, associ-\nated weight vectors can also be visualised individually \nand interpreted as an indication of how the X-variables \nare linearly combined to create each neuron scores vector \nbefore transformation.\n2.2.4  ANN variable contribution\nFor ANN, several variable contribution metrics have \nbeen proposed (Olden et al. 2004); however, the two most \ncomparable metrics to the PLS-DA B\nPLS coefficients and \nVIP scores are the Connection Weight Approach (CWA) \n(Olden and Jackson 2002) and Garson’s Algorithm (GA) \n(Garson 1991), respectively. Similar to B\nPLS, for a two-\nlayer ANN with linear activation functions (Fig.  1b), fea-\nture contribution can be inferred directly from a model’s \nlinear coefficients, B\nANN, as shown in Eq. (10), where C \nis the weights for the hidden-output layer, and W  is the \nweights for the input-hidden layer.\nThis equation can be used to calculate variable con-\ntribution for two-layer non-linear ANNs, renamed as \nCWA, and describes relative (and directional) metabolite \ncontribution.\nWhile VIP may not be directly applied to non-linear \nANNs, a similar measure of weighted absolute relative con-\ntribution of each metabolite per neuron can be calculated \nusing Garson’s Algorithm (Garson 1991). First, absolute \nCWA \ni,jvalues are calculated across the network by multi-\nplying each neuron input weight, w i,j, to the corresponding \noutput weight,cjand converting to an absolute value.\n(10)/u1D402/u1D416/u1D400= /u1D401/u1D400/u1D40D/u1D40D= /u1D402/u1D416\n(11)/uni007C.x/uni007C.x/uni007C.xCWA i,j\n/uni007C.x/uni007C.x/uni007C.x= /uni007C.x/uni007C.x/uni007C.xwi,j× cj\n/uni007C.x/uni007C.x/uni007C.x\nSecond, as shown in Eq. (12), for each hidden neuron the \ntotal absolute connection weight value is calculated, where \nM is the total number of metabolites.\nThen, the overall contribution for each input variable, \nGAi, is calculated as shown in Eq. (13), where K is the total \nnumber of hidden layer neurons.\nUnlike VIP there is no general threshold of importance \nfor Garson’s Algorithm, so we propose using the average GA \nscore as a comparable equivalent to indicate metabolites of \nimportance in the model.\n2.3  Computational workflow\nThe standard workflow for the PLS visualisation and inter -\npretation, and the proposed equivalent ANN visualisation \nand interpretation is described in Fig.  2. Both the PLS-DA \nand ANN workflows were implemented in the Python pro-\ngramming language using a package called ‘cimcb’ (https  \n://githu b.com/CIMCB /cimcb ) developed by the authors. \nThis package contains tools for the analysis and visuali-\nsation of untargeted and targeted metabolomics data. The \npackage is based on existing well curated open-source pack-\nages (including numpy (Kristensen and Vinter, 2010), scipy \n(Virtanen et al. 2019), bokeh (Bokeh Development Team \n2018), keras (Chollet 2015), pandas (McKinney 2010), \nscikit-learn (Pedregosa et al. 2011) , and Theano (Theano \nDevelopment Team2016)). It utilises these packages through \nhelper functions specifically designed to simplify the appli-\ncation to metabolomics data, following guidelines previously \ndescribed (Mendez et al. 2019b).\nEach step of the respective PLS-DA and ANN workflow \nis described in detail in the associated Jupyter Notebook \nfile (included in supplementary material and https ://cimcb  \n.githu b.io/Metab Proje ction Viz/). The method of embedding \nexplanatory text within functional code and visualisations \nfollows previously published guidelines (Mendez et al. \n2019b). The generic workflow is now briefly described.\n2.3.1  Prepare data\nFor an adequate comparison of visualisation and interpre-\ntation methods, across PLS and ANN, it was important \nthat identical data were used in both models. The X  matrix \nof metabolite concentrations, and associated Y  vector of \n(12)\n/uni007C.x/uni007C.x\n/uni007C.x\nCWA j\n/uni007C.x/uni007C.x\n/uni007C.x\n=\nM/uni2211.s1\ni=1\n/uni007C.x/uni007C.x\n/uni007C.x\nCWA i,j\n/uni007C.x/uni007C.x\n/uni007C.x\n(13)GA i =\nK�\nj=1\n⎛\n⎜\n⎜⎝\n��\n�\nCWA i,j\n��\n�\n���CWA j\n���\n⎞\n⎟\n⎟⎠\nMigrating from partial least squares discriminant analysis to artificial neural networks:…\n1 3\nPage 7 of 15 17\nclassification labels (case = 1, control = 0) were extracted \nfrom the excel spreadsheet. Metabolites in X  were \nincluded for modelling if they had a QC relative standard \ndeviation  (RSDQC) < 20% and < 10% missing data (Broad-\nhurst et al. 2018). The datasets were split using a ratio of \n2:1 (2/3 training, 1/3 test) using stratified random selec-\ntion. After splitting the data into training and test sets, the \ncolumns of X were natural log transformed, mean centred, \nand scaled to unit variance with missing values imputed \nusing k-nearest neighbour prior to modelling following \nstandard protocols for metabolomics (Broadhurst and Kell \n2006). The means and standard deviations calculated from \nthe training set were applied to scale the test set data.\n2.3.2  Hyperparameter optimisation\nFor both PLS-DA and ANN algorithms the optimal hyper -\nparameter values were determined using 5-fold cross-vali-\ndation (CV) with 10 Monte Carlo repartitions (Broadhurst \nand Kell 2006; Hastie et al. 2009; Xia et al. 2013). For \nthe PLS-DA workflow, a linear search was used to opti-\nmise the number of latent variables (1 to 6). For the ANN \nworkflow, a grid search was used to optimise the number \nof neurons (2 to 6) and the learning rate (0.001 to 1). The \noptimal hyperparameter values were determined by evalu -\nating plots of R\n2 and Q 2 statistics. Two plots were gener -\nated: (i) a standard R 2and Q2 plot against hyperparameter \nvalues, and (ii) an alternative plot of /uni007C.x/uni007C.xR2 − Q 2 /uni007C.x/uni007C.xvs.Q2  . Using \nthe later plot, the optimal hyperparameter was selected at \nthe point of inflection of the outer convex hull. The area \nunder the receiver operating characteristic curve (AUC) \nis a recommended alternative non-parametric measure of \nclassification performance (Szymańska et al. 2012), thus \nequivalent plots of AUC \nFull and AUC cv metrics are also \ngenerated for comparison.\n2.3.3  Permutation test\nFollowing hyperparameter optimisation, a permutation test \nwas applied to the optimal model configuration. In a per -\nmutation test, the expected outcome label is randomised \n(permuted), and the model with fixed hyperparameter \nvalues is subsequently trained and evaluated (Lindgren \net al. 1996). For both PLS-DA and ANN, this process was \nrepeated (n = 100) using fivefold CV to construct a dis-\ntribution of the permuted model statistics. While R\n2and \nQ2 statistics are commonly used in permutation testing \n(Eriksson et al. 2013), AUC Full and AUC cv metrics were \nalso included for ANNs, given its common usage as a \nmeasure of non-linear classification performance.\nFig. 2  Data analysis workflow. Flowchart of the data analysis workflow \nused for the PLS and ANN methods. Arrows identify the figure corre-\nsponding to the respective workflow step\n K. M. Mendez et al.\n1 317 Page 8 of 15\n2.3.4  Model evaluation using test set\nAs previously described in Sect. 2.1.2, the measure of the \npredictive ability of the model using a test set is calculated \nby comparing the training score ( \n/u1D418∗\n/u1D42D/u1D42B/u1D41A/u1D422/u1D427 ) to the expected out-\ncome (Ytrain) classification, and the test score ( /u1D418∗\n/u1D42D/u1D41E/u1D42C/u1D42D ) to the \nexpected outcome (Y test) classification. This is visualised \nusing three plots:\n1. A violin plot that shows the distribution of the predicted \nscore, by outcome, for the training and test set.\n2. A probability density plot that shows the distribution of \nthe predicted score, by outcome, for the training and test \nset via overlapping probability density functions.\n3. A receiver operator characteristic (ROC) curve of the \ntraining and test sets.\n2.3.5  Model evaluation using bootstrap resampling\nModel evaluation using bootstrap resampling is described \nin Sect. 2.1.2. Following bootstrap resampling (n = 100), a \nmeasure of generalised prediction of each model is calcu-\nlated and visualised using the protocol described in 2.3.4, \nexcept this time presenting the 95% confidence intervals of \nthe 100 in-bag and out-of-bag predictions.\n2.3.6  Model visualisation: scores plot & weights plot\nPairwise latent variable scores plots and associated weight \nvector plots are also provided. The scores plots are similar \nin construction to those generated during hyperparameter \noptimisation, except they are based on the in-bag and out-\nof-bag scores averaged across repeated prediction for each \nsample (aggregate score). 95% confidence intervals for each \nclass are calculated using standard parametric methods. The \n95% confidence intervals for each weight vector plots were \nconstructed using the distribution of each weight variable \nacross the 100 bootstrap resampled models. Any metabo-\nlite weight with a confidence interval crossing the zero line \n(coloured blue) are considered non-significant to the latent \nvariable (or neuron).\n2.3.7  Variable contribution plots\nThe B\nPLS coefficients and VIP scores for the PLS models \nwere calculated using the methods described in Sect. 2.1.4. \nThe CWA and Garson scores were calculated for the ANNs \nusing the methods described in Sect. 2.2.4. There metrics \nwere also applied to all 100 models of each type generated \nduring the bootstrap resampling. Variable contribution \nplots were constructed. The 95% confidence intervals for \neach vector plots were calculated using the distribution of \neach variable’s metric across the 100 bootstrap resampled \nmodels. Any metabolite weight with a confidence interval \ncrossing the zero line are considered non-significant to the \nlatent variable (or neuron).\nThe variable contribution metrics for each model type \nwas compared and contrasted through visual inspection \nof a scatter plots of B\nPLS vs. CWA ANN and of VIP PLS vs. \nGarsonANN scores, and by calculating the associated Pear -\nson’s correlation coefficient.\n3  Results\n3.1  Datasets\nIn this study, a previously published dataset by Chan et al. \n(2016) was used to illustrate the standardised PLS work -\nflow and the proposed equivalent ANN workflow. This \nurine nuclear magnetic resonance (NMR) dataset, com-\nprised of 149 metabolites, is publicly available on Metabo-\nlomics Workbench (Study ID: ST0001047). For the work \ndescribed herein a binary classification was performed: \ngastric cancer (n = 43) vs. healthy controls (n = 40).\nThe computational libraries developed for this study \nrequire data to be converted to a standardised format using \nthe tidy data framework (Wickham, 2014). This stand-\nardised format has been previously described (Mendez \net al. 2019b, 2019c), and allows for the efficient reuse of \nthese workflows for other studies. To demonstrate this, \nwe include the application of the identical workflows and \nvisualisation techniques to a second previously published \ndataset (Ganna et al. 2016) as a supplementary document. \nThis plasma liquid chromatography-mass spectrometry \n(LC–MS) dataset, comprised of 189 named metabo-\nlites, is publicly available on MetaboLights (Study ID: \nMTBLS90), and for this study, samples were split into \ntwo classes by sex: males (n  = 485) and females (n = 483). \nThis dataset did not report QC measurements and therefore \nthe data cleaning step was unable to be performed.\nFollowing data cleaning, for the urine NMR gastric \ncancer data set 52 metabolites were included in data mod-\nelling (case = 43 vs. control = 40). Figures  3, 4, 5 and 6 \n(and Supplementary Figs. S1-2) show the optimisation, \nvisualisation, evaluation and statistical inference for the \nPLS-DA compared to the ANN algorithms. Similar plots \nare provided in supplementary documentation for the \nplasma LC–MS data set (males = 485 vs. females = 483). \nAll 4 workflows are also available as interactive Jupyter \nnotebooks (https ://cimcb .githu b.io/Metab Proje ction Viz/), \neither to be downloaded or to be run in the cloud through \nmybinder.org. See Mendez et al. (2019b) for guidance.\nMigrating from partial least squares discriminant analysis to artificial neural networks:…\n1 3\nPage 9 of 15 17\n3.2  Model optimisation\nUsing the = /uni007C.x/uni007C.xR2 − Q 2 /uni007C.x/uni007C.xvs.Q2  plot, both the number of latent \nvariables (LV  = 2; Fig.  3b) and ANN hyperparameters \n(learning rate = 0.03 & hidden neurons = 2; Fig.  3d) were \nclearly interpretable. These findings were verified using per-\nmutation testing (Supplementary Fig.  1).\n3.3  Model evaluation and visualisation\nStrategies for model evaluation and visualisation were \nsuccessfully transferred from PLS-DA to ANNs. For both \nexample data sets the ANN model performed slightly bet-\nter than the PLS-DA for both the training and test data sets \n(Fig. 4). Both models somewhat overtrained despite rigorous \ncross-validation. For the PLS-DA model the AUC \nTrain = 0.97 \nand the AUC  Test = 0.89. For the ANN model the AUC  \nTrain = 1.00 and AUC Test = 0.90. Bootstrap remodelling also \nshowed similar results. The PLS-DA model had an in-bag \narea under the ROC curve (AUC) with 95% CI of 0.92–0.99. \nSimilarly, the ANN produced an in-bag AUC with 95% \nCI of 0.95–0.99. The out-of-bag predictions showed that \nboth models overtrained with out-of-bag AUC 95% CI of \n0.72–0.98 (PLS-DA) and 0.77–1.00 (ANN). The bootstrap \nprojections confirmed these findings and illustrated that the \nmodels were still able to project significant mean differences \nbetween classes, for both the in-bag and out-bag projections \n(Fig. 5).\n3.4  Model inference\nFeature contribution was determined by calculating boot-\nstrap confidence intervals for the model coefficients B\nPLS \n(or equivalent CWA ANN) and of the VIP PLS (or equivalent \nGarson ANN). Across the two models, B PLS and CWA ANN \nshowed a high degree of correlation (Fig.  6a; Pearson’s \nr = 0.85, p = 2.8 × 10−15). Twenty-three metabolites signif-\nicantly contributed to the PLS-DA model and 25 metabo-\nlites significantly contributed to the ANN model, with an \noverlap of 17 metabolites being significant in both models \n(Fig.  6a). The VIP\nPLS  and Garson ANN values showed a \nreduced, but still significant, degree of correlation with \neach other (Fig.  6b; Pearson’s r = 0.75, p = 1.33 × 10\n−10). \nBased on median values alone (Fig.  6b), 12 metabolites \nFig. 3  Hyperparameter optimisation. Plots of R2 and Q2 statistics; red \ncircle, optimal hyperparameter value(s). a & c Standard R 2 and Q 2 \nvs hyperparameter values plot for PLS and ANN, respectively. Solid \nline, R2; dashed line, Q 2. b & d The alternate /uni007C.x/uni007C.xR2 − Q 2 /uni007C.x/uni007C.xvs.Q2  plot for \nPLS and ANN, respectively. The optimal hyperparameters shown in \npanel c were identified using the plot in panel d \n K. M. Mendez et al.\n1 317 Page 10 of 15\nwere deemed as “important” across both models and \nan additional 12 metabolites were “important” in one, \nbut not both models. When taking into consideration \nbootstrapped confidence intervals (Fig.  6d) VIP\nPLS and \nGarson ANN  yielded 7 and 8 “important” metabolites, \nrespectively. Six metabolites deemed “important” by Gar -\nsonANN were also deemed important by VIP PLS. Although \nmathematical calculations for variable contribution were \ndifferent for the two models, Fig.  6 shows that the overall \nvisualisation strategy was transferrable.\nFig. 4  Visualisations of model \nevaluation. Predicted scores \n(train and test) split into the \nrespective binary classification, \nvisualised in three different \nways. a, b Violin plots; c, d \nprobability distribution function \n(pdf) plots. Red, healthy con-\ntrols (control); blue, gastric can-\ncer (case). e, f ROC curves with \n95% CIs derived from 100 itera-\ntions of bootstrap resampling. \nGreen line predicted scores for \ntraining set; green 95% CIs, IB \npredictions; yellow line, predic-\ntion scores for test set; yellow \n95% CIs, OOB predictions. \nPLS-DA AUC \nTrain = 0.97, AUC \nTest = 0.89, AUC IB = 0.92–0.99, \nAUC OOB = 0.72–0.98. ANN \nAUC Train = 1.00, AUC Test = 0.90, \nAUC IB = 0.95–0.99,  \nAUC OOB = 0.77–1.00\n\nMigrating from partial least squares discriminant analysis to artificial neural networks:…\n1 3\nPage 11 of 15 17\n4  Discussion\nThe migration of the PLS-DA optimisation, evaluation, and \ninterpretation workflow to a single hidden layer ANN was suc-\ncessful. The strategy for visualising hyperparameter optimisa-\ntion was adapted to the \n/uni007C.x/uni007C.xR2 − Q 2 /uni007C.x/uni007C.xvs.Q2  plot (Fig. 3c–d) and \nreadily employable to both model types. Not only did it allow \nfor simultaneous interpretation of 2 hyperparameters (ANNs), \nbut it provides an alternate interpretation strategy for PLS-DA \noptimisation if the standard R\n2 and Q2 vs hyperparameter value \nplot is ambiguous. Model evaluation and projection (scores) \nplots were directly transferrable from PLS-DA to ANNs. Pro-\njecting the neuron weights (in place of latent variables) before \nthe transfer function allows for a comparative and clear visual \nFig. 5  Bootstrap projection \n(scores) plots. Projection plots \nshow LV2 vs LV1 for PLS and \nNeuron 2 vs Neuron 1 for ANN. \na, b projected scores of the \nmedian IB; c, d projected scores \nfor median OOB; e, f median \nIB and median OOB scores \noverlaid. Red, healthy control \n(control); blue, gastric cancer \n(case). Inner ellipses, 95% CI of \nthe mean; outer ellipses, 95% CI \nof the population. Solid lines, \nIB predictions; dashed lines, \nOOB predictions\n\n K. M. Mendez et al.\n1 317 Page 12 of 15\nFig. 6  Variable contribution. Visualisation of variable contribu-\ntion for PLS (coefficients and VIP) and ANN (CWA and Garson’s \nalgorithm). a Scatterplot of ANN CWA  vs. B PLS, Pearson’s r = 0.85 \n(p-value = 2.79e−15). b Scatterplot of GarsonANN vs. VIPPLS, Pearson’s \nr = 0.75 (p-value = 1.33e−10). Dashed lines at respective “importance” \ncut-off: GarsonANN = 0.038, VIPPLS = 1.00. c Median (and 95% CI) \nBPLS (left) and ANN CWA  (right). Blue, contribution not significant \nbased on 95% CIs; red, contribution significant based on 95% CIs. d \nMedian (and 95% CI) VIP\nPLS (left) and GarsonANN (right)\nMigrating from partial least squares discriminant analysis to artificial neural networks:…\n1 3\nPage 13 of 15 17\ndisruption of sample similarity. The bootstrap resampling/\nremodelling enabled both the PLS-DA and ANN models’ pre-\ndictions to be interpreted with statistical rigor. Both models \nhad similar performance, but as described (and expected) in \nthe bootstrap projections (Fig. 5) and loadings (Supplementary \nFig. S2).\nCWA  and Garson provided suitable variable contribution \nmetrics for the ANN model. The surprising similarity between \nB\nPLS and CWA ANN, and VIPPLS and GarsonANN indicates the \nvalidity of both CWA ANN and GarsonANN as methods of deter-\nmining feature importance. These findings are validated by the \nsecond study (supplementary documentation). It is important \nto note that no one ML method will be superior for identifying \nthe most biological plausible metabolites. The high level of \noverlap between comparable variable contribution methods, \nin these results, suggest that deviations are likely random false \ndiscoveries due to lack of power (as reflected in the 95% CIs \nare how close they are to the zero line). As the cut-off for both \nVIP and Garson\nANN are not statistically justified limits (Tran \net al. 2014), we recommend opting for BPLS for PLS and CWA \nANNfor ANN, and using the 95% CI from bootstrap resampling \nto determine statistically significant metabolites.\nAs a side note, it is worth discussing two additional \npoints. First, there is an advantage of using bootstrap resa-\nmpled predictions and projections once the optimal hyper -\nparameters are fixed. This is particularly important if the \nsample size is small and there may be large differences in \nresults depending on how the samples are split into training \nand test sets. The out-of-bag predictions provide an unbi-\nased estimate of model performance, and the averaged out-\nof-bag projections a more realistic estimate of generalised \nclass-based cluster similarity. Bootstrapping can also aid \nin preventing false discoveries regarding metabolite sig-\nnificance, as the resulting 95% CIs will identify metabolites \nwith unstable contributions to the model. Second, model \noutcomes and resulting interpretations can affected by the \nquality of the input data. We have previously shown that PLS \nand ANNs show similar predictive ability, when using the \nsame input data, and that sample size is an important deter-\nminant of model stability (Mendez et al. 2019c). However, \nto our knowledge, an extensive comparison of different data \ncleaning (Broadhurst et al. 2018 ), pre-treatment (van den \nBerg et al. 2006), and imputation (Di Guida et al. 2016; Do \net al. 2018) procedure options has not been performed for \nANNs. As such, individual users should consider and test \nthese effects prior to modelling their own data.\n5  Conclusion and future perspectives\nWe have shown that for binary discrimination using \nmetabolomics data it is possible to migrate the workflow \nfrom PLS-DA to a single hidden layer non-linear ANN. \nFor the two presented examples the ANN does not perform \nany better than PLS-DA, and based on coefficient plots \nthere is very similar feature contribution. However, these \nresults show that ANNs can be evaluated alongside PLS-\nDA for any data set (using the provided Jupyter notebooks \nit is possible to evaluate any binary classification data set \nprovided it is formatted appropriately before uploading). \nIf a highly non-linear relation should arise, then ANN may \nbe a better approach to PLS. This remains to be proven.\nMore importantly these results open the door to investigat-\ning more complex models. As discussed previously (Mendez \net al. 2019a), an area of increasing interest to the metabolomics \ncommunity is multi-block data integration (e.g. multi-omic or \nmulti-instrument). Currently, methods employed are based on \nhierarchical application of multiple linear projection models. \nFor example, OnPLS (Löfstedt and Trygg, 2011; Reinke et al. \n2018) is a combinatorial amalgamation of multiple PLS mod-\nels, and Mixomics (Rohart et al. 2017) is a stepwise integration \nof canonical correlation analysis and sparse PLS. The inherent \nflexibility of ANN architecture allows complex relationships \nto be combined into a single model. It may be possible to build \nan ANN to combine multiple data blocks into a single model \nwithout resorting to over-simplified data concatenation. For \nthese types of models to be useful will be necessary to incorpo-\nrate feature importance, and interpretable visualisation strate-\ngies. The work presented here is a first step to applying statisti-\ncal rigor and interpretability to more complex ANN models.\nAcknowledgements This work was partly funded through an Austral-\nian Research Council funded LIEF grant (LE170100021).\nAuthors contributions All authors conceived of the idea. KMM and \nDIB developed the software. KMM wrote the manuscript. DIB and \nSNR edited the manuscript.\nData availability The metabolomics and metadata used in this paper \nwere retrieved Metabolomics Workbench (https ://www.metab olomi  \ncswor kbenc h.org/) Study ID: ST0001047, and from Metabolights (https \n://www.ebi.ac.uk/metab oligh ts/) study identifier: MTBLS90. This data \nwere converted from the original data format to a clean format compliant \nwith the Tidy Data framework, this is available at the CIMCB GitHub \nproject page: https ://githu b.com/CIMCB /Metab Proje ction Viz.\nSoftware availability All software developed for this paper is available \nat the CIMCB GitHub project page: https ://githu b.com/CIMCB .\nCompliance with ethical standards \nConflicts of interest The authors have no disclosures of potential con-\nflicts of interest related to the presented work.\nHuman and animal rights No research involving human or animal par-\nticipants was performed in the construction of this manuscript.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \n K. M. Mendez et al.\n1 317 Page 14 of 15\nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creat iveco mmons .org/licen ses/by/4.0/.\nReferences\nBishop, C. M. (1995). Neural networks for pattern recognition. New \nYork, United States of America: Oxford University Press.\nBokeh Development Team (2018). Bokeh: Python library for interac-\ntive visualization. https ://bokeh .pydat a.org/en/lates t/\nBreiman, L. (2001). Random forests. Machine Learning, 45, 5–32.\nBroadhurst, D. I., & Kell, D. B. (2006). Statistical strategies for avoid-\ning false discoveries in metabolomics and related experiments. \nMetabolomics, 2, 171–196.\nBroadhurst, D., Goodacre, R., Reinke, S. N., Kuligowski, J., Wilson, I. \nD., Lewis, M. R., et al. (2018). Guidelines and considerations for \nthe use of system suitability and quality control samples in mass \nspectrometry assays applied in untargeted clinical metabolomic \nstudies. Metabolomics, 14, 72.\nChan, A. W., Mercier, P., Schiller, D., Bailey, R., Robbins, S., Eurich, \nD. T., et al. (2016). (1)H-NMR urinary metabolomic profiling for \ndiagnosis of gastric cancer. British Journal of Cancer, 114, 59–62.\nChollet, F. (2015). Keras. https ://keras .io/\nde Jong, S. (1993). SIMPLS: An alternative approach to partial least \nsquares regression. Chemometrics and Intelligent Laboratory Sys-\ntems, 18, 251–263.\nDi Guida, R., Engel, J., Allwood, J. W., Weber, R. J. M., Jones, M. \nR., Sommer, U., et al. (2016). Non-targeted UHPLC-MS metabo-\nlomic data processing methods: A comparative investigation of \nnormalisation, missing value imputation, transformation and scal-\ning. Metabolomics, 12, 93.\nDiCiccio, T. J., & Efron, B. (1996). Bootstrap confidence intervals. \nStatistical Science, 11, 189–212.\nDo, K. T., Wahl, S., Raffler, J., Molnos, S., Laimighofer, M., Adamski, \nJ., et al. (2018). Characterization of missing values in untargeted \nMS-based metabolomics data and evaluation of missing data han-\ndling strategies. Metabolomics, 14, 128.\nDunn, W. B., Broadhurst, D. I., Atherton, H. J., Goodacre, R., & Grif-\nfin, J. L. (2011). Systems level studies of mammalian metabo-\nlomes: the roles of mass spectrometry and nuclear magnetic \nresonance spectroscopy. Chemical Society Reviews, 40, 387–426.\nEfron, B. (1981). Nonparametric estimates of standard error—the jack-\nknife, the bootstrap and other methods. Biometrika, 68, 589–599.\nEfron, B. (1987). Better bootstrap confidence intervals. Journal of the \nAmerican Statistical Association, 82, 171–185.\nEfron, B. (1988). Bootstrap confidence—intervals—good or bad. Psy-\nchological Bulletin, 104, 293–296.\nEfron, B. (2000). The bootstrap and modern statistics. Journal of the \nAmerican Statistical Association, 95, 1293–1296.\nEriksson, L., Byrne, T., Johansson, E., Trygg, J., & Vikström, C. \n(2013). Multi- and megavariate data analysis: basic principles \nand applications (3rd ed.). Malmö, Sweden: Umetrics Academy.\nFavilla, S., Durante, C., Vigni, M. L., & Cocchi, M. (2013). Assessing \nfeature relevance in NPLS models by VIP. Chemometrics and \nIntelligent Laboratory Systems, 129, 76–86.\nGanna, A., Fall, T., Salihovic, S., Lee, W., Broeckling, C. D., Kumar, \nJ., et al. (2016). Large-scale non-targeted metabolomic profiling \nin three human population-based studies. Metabolomics, 12, 4.\nGarson, G. D. (1991). Interpreting neural network connection weights. \nAI Expert, 6, 47–51.\nGeladi, P., & Kowalski, B. R. (1986). Partial least-squares regression: \na tutorial. Analytica Chimica Acta, 185, 1–17.\nGoodacre, R. (2003). Explanatory analysis of spectroscopic data using \nmachine learning of simple, interpretable rules. Vibrational Spec-\ntroscopy, 32, 33–45.\nGoodacre, R., Kell, D. B., & Bianchi, G. (1992). Neural networks and \nolive oil. Nature, 359, 594–594.\nGromski, P. S., Muhamadali, H., Ellis, D. I., Xu, Y., Correa, E., Turner, \nM. L., et al. (2015). A tutorial review: Metabolomics and partial \nleast squares-discriminant analysis–a marriage of convenience or \na shotgun wedding. Analytica Chimica Acta, 879, 10–23.\nHastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of Sta-\ntistical Learning (2nd ed.). New York, United States of America: \nSpringer.\nKristensen, M.R.B. and Vinter, B. (2010) Numerical Python for scal-\nable architectures, Proceedings of the Fourth Conference on Par-\ntitioned Global Address Space Programming Model, Association \nfor Computing Machinery, pp. 1–9.\nLindgren, F., Hansen, B., Karcher, W., Sjöström, M., & Eriksson, L. \n(1996). Model validation by permutation tests: Applications to \nvariable selection. Journal of Chemometrics, 10, 521–532.\nLöfstedt, T., & Trygg, J. (2011). OnPLS—a novel multiblock method \nfor the modelling of predictive and orthogonal variation. Journal \nof Chemometrics, 25, 441–455.\nMcKinney, W. (2010) Data Structures for Statistical Computing in \nPython. Proceedings of the 9th Python in Science Conference , \n445, 51–56.\nMendez, K. M., Broadhurst, D. I., & Reinke, S. N. (2019a). The appli-\ncation of artificial neural networks in metabolomics: A historical \nperspective. Metabolomics, 15, 142.\nMendez, K. M., Pritchard, L., Reinke, S. N., & Broadhurst, D. I. \n(2019b). Toward collaborative open data science in metabolomics \nusing Jupyter Notebooks and cloud computing. Metabolomics,  \n15, 125.\nMendez, K. M., Reinke, S. N., & Broadhurst, D. I. (2019c). A com-\nparative evaluation of the generalised predictive ability of eight \nmachine learning algorithms across ten clinical metabolomics \ndata sets for binary classification. Metabolomics, 15, 150.\nOlden, J. D., & Jackson, D. A. (2002). Illuminating the “black box”: a \nrandomization approach for understanding variable contributions \nin artificial neural networks. Ecological Modelling, 154, 135–150.\nOlden, J. D., Joy, M. K., & Death, R. G. (2004). An accurate compari-\nson of methods for quantifying variable importance in artificial \nneural networks using simulated data. Ecological Modelling, 178, \n389–397.\nPedregosa, F., Varoquaux, l., Gramfort, A., Michel, V., Thirion, B., Gri-\nsel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Van-\nderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., \nDuchesnay, E., (2011). Scikit-learn: machine learning in Python. \nThe Journal of Machine Learning Research, 12, 2825–2830.\nReinke, S. N., Galindo-Prieto, B., Skotare, T., Broadhurst, D. I., \nSinghania, A., Horowitz, D., et al. (2018). OnPLS-based multi-\nblock data integration: A multivariate approach to interrogat-\ning biological interactions in asthma. Analytical Chemistry, 90, \n13400–13408.\nRohart, F., Gautier, B., Singh, A., & Lê Cao, K.-A. (2017). mixOm-\nics: An R package for ‘omics feature selection and multiple data \nintegration. PLOS Computational Biology, 13, e1005752.\nSteinwart, I., & Christmann, A. (2008). Support Vector Machines. New \nYork, United States of America: Springer.\nMigrating from partial least squares discriminant analysis to artificial neural networks:…\n1 3\nPage 15 of 15 17\nSzymańska, E., Saccenti, E., Smilde, A. K., & Westerhuis, J. A. (2012). \nDouble-check: Validation of diagnostic statistics for PLS-DA \nmodels in metabolomics studies. Metabolomics, 8, 3–16.\nTheano Development Team (2016) Theano: A Python framework for \nfast computation of mathematical expressions. arXiv:1605.02688.\nTran, T. N., Afanador, N. L., Buydens, L. M. C., & Blanchet, L. (2014). \nInterpretation of variable importance in partial least squares with \nsignificance multivariate correlation (sMC). Chemometrics and \nIntelligent Laboratory Systems, 138, 153–160.\nvan den Berg, R. A., Hoefsloot, H. C. J., Westerhuis, J. A., Smilde, A. \nK., & van der Werf, M. J. (2006). Centering, scaling, and transfor-\nmations: improving the biological information content of metabo-\nlomics data. BMC Genomics, 7, 142.\nVirtanen, P., Gommers, R., Oliphant, T., Haberland, M., Reddy, T., \nCournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, \nJ., Walt, S., Brett, M., Wilson, J., Millman, K., Mayorov, N., Nel-\nson, A., Jones, E., Kern, R., Larson, E. and SciPy 1.0 Contributors \n(2019) SciPy 1.0—Fundamental algorithms for scientific comput-\ning in Python. arXiv:1907.10121.\nWesterhuis, J. A., Hoefsloot, H. C. J., Smit, S., Vis, D. J., Smilde, A. \nK., van Velzen, E. J. J., et al. (2008). Assessment of PLSDA cross \nvalidation. Metabolomics, 4, 81–89.\nWickham, H. (2014). Tidy data. Journal of Statistical Software, 59, \n1–23.\nWilkins, M. F., Morris, C. W., & Boddy, L. (1994). A comparison \nof Radial Basis Function and backpropagation neural networks \nfor identification of marine phytoplankton from multivariate flow \ncytometry data. Computer Applications in the Biosciences, 10, \n285–294.\nWold, H. (1975). Path models with latent variables: The NIPALS \napproach (pp. 307–357). Quantitative sociology: Elsevier.\nWold, S., Johansson, E., & Cocchi, M. (1993). PLS: Partial least \nsquares projections to latent structures, 3D QSAR in drug design: \nTheory. Kluwer/Escom, Dordrecht, The Netherlands: Methods \nand Applications.\nWold, S., Sjöström, M., & Eriksson, L. (2001). PLS-regression: A basic \ntool of chemometrics. Chemometrics and Intelligent Laboratory \nSystems, 58, 109–130.\nXia, J., Broadhurst, D. I., Wilson, M., & Wishart, D. S. (2013). Transla-\ntional biomarker discovery in clinical metabolomics: An introduc-\ntory tutorial. Metabolomics, 9, 280–299.\nPublisher’s Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Linear discriminant analysis",
  "concepts": [
    {
      "name": "Linear discriminant analysis",
      "score": 0.7292628288269043
    },
    {
      "name": "Partial least squares regression",
      "score": 0.6107162833213806
    },
    {
      "name": "Artificial neural network",
      "score": 0.5968363881111145
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5962992906570435
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5491436123847961
    },
    {
      "name": "Visualization",
      "score": 0.525958776473999
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5212905406951904
    },
    {
      "name": "Computer science",
      "score": 0.4632182717323303
    },
    {
      "name": "Machine learning",
      "score": 0.3221699893474579
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I12079687",
      "name": "Edith Cowan University",
      "country": "AU"
    }
  ]
}