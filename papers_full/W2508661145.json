{
  "title": "Larger-Context Language Modelling with Recurrent Neural Network",
  "url": "https://openalex.org/W2508661145",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2097009564",
      "name": "Tian Wang",
      "affiliations": [
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2115080942",
      "name": "Kyunghyun Cho",
      "affiliations": [
        "Courant Institute of Mathematical Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1996430422",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2139666201",
    "https://openalex.org/W6908809",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2152311353",
    "https://openalex.org/W2129250947",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W581956982",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2197913429",
    "https://openalex.org/W2126209950",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2141440284",
    "https://openalex.org/W1689711448",
    "https://openalex.org/W2296712013",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1985258458",
    "https://openalex.org/W1847088711",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W2950726992",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2182417295",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W1571227886",
    "https://openalex.org/W4302400662",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2964267515"
  ],
  "abstract": "In this work, we propose a novel method to incorporate corpus-level discourse information into language modelling.We call this larger-context language model.We introduce a late fusion approach to a recurrent language model based on long shortterm memory units (LSTM), which helps the LSTM unit keep intra-sentence dependencies and inter-sentence dependencies separate from each other.Through the evaluation on four corpora (IMDB, BBC, Penn TreeBank, and Fil9), we demonstrate that the proposed model improves perplexity significantly.In the experiments, we evaluate the proposed approach while varying the number of context sentences and observe that the proposed late fusion is superior to the usual way of incorporating additional inputs to the LSTM.By analyzing the trained larger-context language model, we discover that content words, including nouns, adjectives and verbs, benefit most from an increasing number of context sentences.This analysis suggests that larger-context language model improves the unconditional language model by capturing the theme of a document better and more easily.* Recently, (Ji et al., 2015) independently proposed a similar approach.",
  "full_text": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1319–1329,\nBerlin, Germany, August 7-12, 2016.c⃝2016 Association for Computational Linguistics\nLarger-Context Language Modelling with Recurrent Neural Network∗\nTian Wang\nCenter for Data Science\nNew York University\nt.wang@nyu.edu\nKyunghyun Cho\nCourant Institute of Mathematical Sciences\nand Center for Data Science\nNew York University\nkyunghyun.cho@nyu.edu\nAbstract\nIn this work, we propose a novel method to\nincorporate corpus-level discourse infor-\nmation into language modelling. We call\nthis larger-context language model. We in-\ntroduce a late fusion approach to a recur-\nrent language model based on long short-\nterm memory units (LSTM), which helps\nthe LSTM unit keep intra-sentence depen-\ndencies and inter-sentence dependencies\nseparate from each other. Through the\nevaluation on four corpora (IMDB, BBC,\nPenn TreeBank, and Fil9), we demonstrate\nthat the proposed model improves per-\nplexity signiﬁcantly. In the experiments,\nwe evaluate the proposed approach while\nvarying the number of context sentences\nand observe that the proposed late fusion\nis superior to the usual way of incorporat-\ning additional inputs to the LSTM. By an-\nalyzing the trained larger-context language\nmodel, we discover that content words, in-\ncluding nouns, adjectives and verbs, bene-\nﬁt most from an increasing number of con-\ntext sentences. This analysis suggests that\nlarger-context language model improves\nthe unconditional language model by cap-\nturing the theme of a document better and\nmore easily.\n1 Introduction\nThe goal of language modelling is to estimate the\nprobability distribution of various linguistic units,\ne.g., words, sentences (Rosenfeld, 2000). Among\nthe earliest techniques were count-based n-gram\nlanguage models which intend to assign the prob-\nability distribution of a given word observed af-\n∗Recently, (Ji et al., 2015) independently proposed a sim-\nilar approach.\nter a ﬁxed number of previous words. Later Ben-\ngio et al. (2003) proposed feed-forward neural\nlanguage model, which achieved substantial im-\nprovements in perplexity over count-based lan-\nguage models. Bengio et al. showed that this neu-\nral language model could simultaneously learn the\nconditional probability of the latest word in a se-\nquence as well as a vector representation for each\nword in a predeﬁned vocabulary.\nRecently recurrent neural networks have be-\ncome one of the most widely used models in lan-\nguage modelling (Mikolov et al., 2010). Long\nshort-term memory unit (LSTM, Hochreiter and\nSchmidhuber, 1997) is one of the most common\nrecurrent activation function. Architecturally, the\nmemory state and output state are explicitly sep-\narated by activation gates such that the vanish-\ning gradient and exploding gradient problems de-\nscribed in Bengio et al. (1994) is avoided. Moti-\nvated by such gated model, a number of variants\nof RNNs (e.g. Cho et al. (GRU, 2014b), Chung\net al. (GF-RNN, 2015)) have been designed to eas-\nily capture long-term dependencies.\nWhen modelling a corpus, these language mod-\nels assume the mutual independence among sen-\ntences, and the task is often reduced to as-\nsigning a probability to a single sentence. In\nthis work, we propose a method to incorporate\ncorpus-level discourse dependency into neural lan-\nguage model. We call this larger-context lan-\nguage model. It models the inﬂuence of con-\ntext by deﬁning a conditional probability in the\nform of P(wn|w1:n−1,S), where w1,...,w n are\nwords from the same sentence, and S represents\nthe context which consists a number of previous\nsentences of arbitrary length.\nWe evaluated our model on four different cor-\npora (IMDB, BBC, Penn TreeBank, and Fil9).\nOur experiments demonstrate that the proposed\nlarger-context language model improve perplex-\n1319\nity for sentences, signiﬁcantly reducing per-word\nperplexity compared to the language models with-\nout context information. Further, through Part-Of-\nSpeech tag analysis, we discovered that content\nwords, including nouns, adjectives and verbs, ben-\neﬁt the most from increasing number of context\nsentences. Such discovery led us to the conclu-\nsion that larger-context language model improves\nthe unconditional language model by capturing the\ntheme of a document.\nTo achieve such improvement, we proposed a\nlate fusion approach, which is a modiﬁcation to\nthe LSTM such that it better incorporates the dis-\ncourse context from preceding sentences. In the\nexperiments, we evaluated the proposed approach\nagainst early fusion approach with various num-\nbers of context sentences, and demonstrated the\nlate fusion is superior to the early fusion approach.\nOur model explores another aspect of context-\ndependent recurrent language model. It is novel\nin that it also provides an insightful way to feed\ninformation into LSTM unit, which could beneﬁt\nall encoder-decoder based applications.\n2 Statistical Language Modelling with\nRecurrent Neural Network\nGiven a document D = (S1,S2,...,S L) which\nconsists of Lsentences, statistical language mod-\nelling aims at computing its probability P(D).\nIt is often assumed that each sentence in the\nwhole document is mutually independent from\neach other:\nP(D) ≈\nL∏\nl=1\nP(Sl). (1)\nWe call this probability (before approximation) a\ncorpus-level probability. Under this assumption of\nmutual independence among sentences, the task of\nlanguage modelling is often reduced to assigning\na probability to a single sentence P(Sl).\nA sentence Sl = ( w1,w2,...,w Tl ) is a\nvariable-length sequence of words or tokens. By\nassuming that a word at any location in a sentence\nis largely predictable by preceding words, we can\nrewrite the sentence probability into\nP(S) =\nTl∏\nt=1\np(wt|w<t), (2)\nwhere w<t denotes all the preceding words. We\ncall this a sentence-level probability.\nThis rewritten probability expression can be ei-\nther directly modelled by a recurrent neural net-\nwork (Mikolov et al., 2010) or further approxi-\nmated as a product of n-gram conditional proba-\nbilities such that\nP(S) ≈\nTl∏\nt=1\np(wt|wt−1\nt−(n−1)), (3)\nwhere wt−1\nt−(n−1) = (wt−(n−1),...,w t−1). The\nlatter is called n-gram language modelling.\nA recurrent language model is composed of two\nfunctions–transition and output functions. The\ntransition function reads one word wt and updates\nits hidden state such that\nht = φ(wt,ht−1) , (4)\nwhere h0 is an all-zero vector. φ is a recurrent\nactivation function. For more details on widely-\nused recurrent activation units, we refer the reader\nto (Jozefowicz et al., 2015; Greff et al., 2015).\nAt each timestep, the output function computes\nthe probability over all possible next words in the\nvocabulary V. This is done by\np(wt+1 = w′|wt\n1) ∝exp (gw′(ht)) . (5)\ngis commonly an afﬁne transformation:\ng(ht) =Woht + bo,\nwhere Wo ∈R|V |×d and bo ∈R|V |.\nThe whole model is trained by maximizing the\nlog-likelihood of a training corpus often using\nstochastic gradient descent with backpropagation\nthrough time (see, e.g., Rumelhart et al., 1988).\nThis conventional approach to statistical lan-\nguage modelling often treats every sentence in a\ndocument to be independent from each other This\nis often due to the fact that downstream tasks, such\nas speech recognition and machine translation, are\ndone sentence-wise. In this paper, we ask how\nstrong an assumption this is, how much impact this\nassumption has on the ﬁnal language model qual-\nity and how much gain language modelling can get\nby making this assumption less strong.\nLong Short-Term Memory Here let us brieﬂy\ndescribe a long short-term memory unit which is\nwidely used as a recurrent activation function φ\n(see Eq. (4)) for language modelling (see, e.g.,\nGraves, 2013).\n1320\nA layer of long short-term memory (LSTM)\nunit consists of three gates and a single memory\ncell. They are computed by\nit =σ(Wixt + Uiht−1 + bi)\not =σ(Woxt + Uoht−1 + bo)\nft =σ(Wf xt + Uf ht−1 + bf ) ,\nwhere σ is a sigmoid function. xt is the input at\ntime t. The memory cell is computed by\nct = ft ⊙ct−1 + it ⊙tanh (Wcx + Ucht−1 + bc) ,\nwhere ⊙is an element-wise multiplication. This\nadaptive leaky integration of the memory cell al-\nlows the LSTM to easily capture long-term depen-\ndencies in the input sequence.\nThe output, or the activation of this LSTM layer,\nis then computed by ht = ot ⊙tanh(ct).\n3 Larger-Context Language Modelling\nIn this paper, we aim not at improving the\nsentence-level probability estimation P(S) (see\nEq. (2)) but at improving the corpus-level prob-\nability P(D) from Eq. (1) directly. One thing we\nnoticed at the beginning of this work is that it is not\nnecessary for us to make the assumption of mutual\nindependence of sentences in a corpus. Rather,\nsimilarly to how we model a sentence probability,\nwe can loosen this assumption by\nP(D) ≈\nL∏\nl=1\nP(Sl|Sl−1\nl−n), (6)\nwhere Sl−1\nl−n = (Sl−n,Sl−n+1,...,S l−1). n de-\ncides on how many preceding sentences each con-\nditional sentence probability conditions on, sim-\nilarly to what happens with a usual n-gram lan-\nguage modelling.\nFrom the statistical modelling’s perspective, es-\ntimating the corpus-level language probability in\nEq. (6) is equivalent to build a statistical model\nthat approximates\nP(Sl|Sl−1\nl−n) =\nTl∏\nt=1\np(wt|w<t,Sl−1\nl−n), (7)\nsimilarly to Eq. (2). One major difference from the\nexisting approaches to statistical language mod-\nelling is that now each conditional probability of\na next word is conditioned not only on the preced-\ning words in the same sentence, but also on the\nn−1 preceding sentences.\nA conventional, count-based n-gram language\nmodel is not well-suited due to the issue of data\nsparsity. In other words, the number of rows in the\ntable storing n-gram statistics will explode as the\nnumber of possible sentence combinations grows\nexponentially with respect to both the vocabulary\nsize, each sentence’s length and the number of\ncontext sentences.\nEither neural or recurrent language modelling\nhowever does not suffer from this issue of data\nsparsity. This makes these models ideal for mod-\nelling the larger-context sentence probability in\nEq. (7). More speciﬁcally, we are interested in\nadapting the recurrent language model for this.\nIn doing so, we answer two questions in the\nfollowing subsections. First, there is a question\nof how we should represent the context sentences\nSl−1\nl−n. We consider two possibilities in this work.\nSecond, there is a large freedom in how we build a\nrecurrent activation function to be conditioned on\nthe context sentences. We also consider two alter-\nnatives in this case.\n3.1 Context Representation\nA sequence of preceding sentences can be repre-\nsented in many different ways. Here, let us de-\nscribe two alternatives we test in the experiments.\nThe ﬁrst representation is to simply bag all the\nwords in the preceding sentences into a single vec-\ntor s ∈[0,1]|V |. Any element of s corresponding\nto the word that exists in one of the preceding sen-\ntences will be assigned the frequency of that word,\nand otherwise 0. This vector is multiplied from\nleft by a matrix P which is tuned together with all\nthe other parameters: p = Ps.We call this repre-\nsentation p a bag-of-words (BoW) context.\nSecond, we try to represent the preceding con-\ntext sentences as a sequence of bag-of-words.\nEach bag-of-word sj is the bag-of-word represen-\ntation of thej-th context sentence, and they are put\ninto a sequence (sl−n,..., sl−1). Unlike the ﬁrst\nBoW context, this allows us to incorporate the or-\nder of the preceding context sentences.\nThis sequence of BoW vectors are read by\na recurrent neural network which is separately\nfrom the one used for modelling a sentence (see\nEq. (4).) We use LSTM units as recurrent acti-\nvations, and for each context sentence in the se-\nquence, we get zt = φ(xt,zt−1) , for t = l −\nn,...,l −1. We set the last hidden state zl−1 of\nthis context recurrent neural network as the con-\n1321\ntext vector p.\nAttention-based Context Representation The\nsequence of BoW vectors can be used in a bit dif-\nferent way from the above. Instead of a unidi-\nrectional recurrent neural network, we ﬁrst use a\nbidirectional recurrent neural network to read the\nsequence. The forward recurrent neural network\nreads the sequence as usual in a forward direction,\nand the reverse recurrent neural network in the op-\nposite direction. The hidden states from these two\nnetworks are then concatenated for each context\nsentence in order to form a sequence of annotation\nvectors (zl−n,..., zl−1).\nUnlike the other approaches, in this case, the\ncontext vector p differs for each word wt in the\ncurrent sentence, and we denote it bypt. The con-\ntext vector pt for the t-th word is computed as the\nweighted sum of the annotation vectors:\npt =\nl−1∑\nl′=l−n\nαt,l′zl′,\nwhere the attention weight αt,l′ is computed by\nαt,l′ = exp score (zl′,ht)∑l−1\nk=l−n exp score (zk,ht)\n.\nht is the hidden state of the recurrent language\nmodel of the current sentence from Eq. (5). The\nscoring function score(zl′,ht) returns a relevance\nscore of the l′-th context sentence w.r.t. ht.\n3.2 Conditional LSTM\nEarly Fusion Once the context vector p is com-\nputed from the npreceding sentences, we need to\nfeed this into the sentence-level recurrent language\nmodel. One most straightforward way is to simply\nconsider it as an input at every time step such that\nx = E⊤wt + Wpp,\nwhere E is the word embedding matrix that trans-\nforms the one-hot vector of the t-th word into a\ncontinuous word vector. We call this approach an\nearly fusion of the context.\nLate Fusion In addition to this approach, we\npropose here a modiﬁcation to the LSTM such\nthat it better incorporates the context from the pre-\nceding sentences (summarized by pt.) The ba-\nsic idea is to keep dependencies within the sen-\ntence being modelled ( intra-sentence dependen-\ncies) and those between the preceding sentences\n(a) Early Fusion\n(b) Late Fusion\nFigure 1: Proposed fusion methods\nand the current sent (inter-sentence dependencies)\nseparately from each other.\nWe let the memory cell ct of the LSTM to\nmodel intra-sentence dependencies. This simply\nmeans that there is no change to the existing for-\nmulation of the LSTM.\nThe inter-sentence dependencies are reﬂected\non the interaction between the memory cell ct,\nwhich models intra-sentence dependencies, and\nthe context vector p, which summarizes the npre-\nceding sentences. We model this by ﬁrst comput-\ning the amount of inﬂuence of the preceding con-\ntext sentences as\nrt = σ(Wr (Wpp) +Wrct + br) .\nThis vector rt controls the strength of each of the\nelements in the context vector p. This amount\nof inﬂuence from the n preceding sentences is\ndecided based on the currently captured intra-\nsentence dependency structures and the preceding\nsentences.\nThis controlled context vector rt ⊙(Wpp) is\nused to compute the output of the LSTM layer:\nht = ot ⊙tanh (ct + rt ⊙(Wpp)) .\nThis is illustrated in Fig. 1 (b).\nWe call this approach a late fusion, as the ef-\nfect of the preceding context is fused together with\n1322\nthe intra-sentence dependency structure in the later\nstage of the recurrent activation.\nLate fusion is a simple, but effective way to\nmitigate the issue of vanishing gradient in corpus-\nlevel language modelling. By letting the context\nrepresentation ﬂow without having to pass through\nsaturating nonlinear activation functions, it pro-\nvides a linear path through which the gradient for\nthe context ﬂows easily.\n4 Related Work\nContext-dependent Language Model This\npossibility of extending a neural or recurrent\nlanguage modeling to incorporate larger context\nwas explored earlier. Especially, (Mikolov and\nZweig, 2012) proposed an approach, called\ncontext-dependent recurrent neural network\nlanguage model, very similar to the proposed\napproach here. The basic idea of their approach\nis to use a topic distribution, represented as a\nvector of probabilities, of previous nwords when\ncomputing the hidden state of the recurrent neural\nnetwork each time.\nThere are three major differences in the pro-\nposed approach from the work by Mikolov and\nZweig (2012). First, the goal in this work is\nto explicitly model preceding sentences to bet-\nter approximate the corpus-level probability (see\nEq. (6)) rather than to get a better context of the\ncurrent sentence. Second, Mikolov and Zweig\n(2012) use an external method, such as latent\nDirichlet allocation (Blei et al., 2003) or latent se-\nmantics analysis (Dumais, 2004) to extract a fea-\nture vector, whereas we learn the whole model, in-\ncluding the context vector extraction, end-to-end.\nThird, we propose a late fusion approach which\nis well suited for the LSTM units which have re-\ncently been widely adopted many works involv-\ning language models (see, e.g., Sundermeyer et al.,\n2015). This late fusion is later shown to be supe-\nrior to the early fusion approach.\nDialogue Modelling with Recurrent Neural\nNetworks A more similar model to the pro-\nposed larger-context recurrent language model is\na hierarchical recurrent encoder decoder (HRED)\nproposed recently by Serban et al. (2015). The\nHRED consists of three recurrent neural networks\nto model a dialogue between two people from the\nperspective of one of them, to which we refer as a\nspeaker. If we consider the last utterance of the\nspeaker, the HRED is a larger-context recurrent\nlanguage model with early fusion.\nAside the fact that the ultimate goals differ (in\ntheir case, dialogue modelling and in our case,\ndocument modelling), there are two technical dif-\nferences. First, they only test with the early fusion\napproach. We show later in the experiments that\nthe proposed late fusion gives a better language\nmodelling quality than the early fusion. Second,\nwe use a sequence of bag-of-words to represent the\npreceding sentences, while the HRED a sequence\nof sequences of words. This allows the HRED to\npotentially better model the order of the words in\neach preceding sentence, but it increases computa-\ntional complexity (one more recurrent neural net-\nwork) and decreases statistical efﬁcient (more pa-\nrameters with the same amount of data.)\nSkip-Thought Vectors Perhaps the most simi-\nlar work is the skip-thought vector by Kiros et al.\n(2015). In their work, a recurrent neural network\nis trained to read a current sentence, as a sequence\nof words, and extract a so-called skip-thought vec-\ntor of the sentence. There are two other recurrent\nneural networks which respectively model preced-\ning and following sentences. If we only con-\nsider the prediction of the following sentence, then\nthis model becomes a larger-context recurrent lan-\nguage model which considers a single preceding\nsentence as a context.\nAs with the other previous works we have dis-\ncussed so far, the major difference is in the ulti-\nmate goal of the model. Kiros et al. (2015) fully\nfocused on using their model to extract a good,\ngeneric sentence vector, while in this paper we\nare focused on obtaining a good language model.\nThere are less major technical differences. First,\nthe skip-thought vector model conditions only on\nthe immediate preceding sentence, while we ex-\ntend this to multiple preceding sentences. Second,\nsimilarly to the previous works by Mikolov and\nZweig (2012), the skip-thought vector model only\nimplements early fusion.\nNeural Machine Translation Neural machine\ntranslation is another related approach (Forcada\nand ˜Neco, 1997; Kalchbrenner and Blunsom,\n2013; Cho et al., 2014b; Sutskever et al., 2014;\nBahdanau et al., 2014). In neural machine transla-\ntion, often two recurrent neural networks are used.\nThe ﬁrst recurrent neural network, called an en-\ncoder, reads a source sentence, represented as a\nsequence of words in a source language, to form\na context vector, or a set of context vectors. The\n1323\nother recurrent neural network, called a decoder,\nthen, models the target translation conditioned on\nthis source context.\nThis is similar to the proposed larger-context re-\ncurrent language model, if we consider the source\nsentence as a preceding sentence in a corpus. The\nmajor difference is in the ultimate application, ma-\nchine translation vs. language modelling, and\ntechnically, the differences between neural ma-\nchine translation and the proposed larger-context\nlanguage model are similar to those between the\nHRED and the larger-context language model.\nContext-Dependent Question-Answering Mod-\nels Context-dependent question-answering is a\ntask in which a model is asked to answer a ques-\ntion based on the facts from a natural language\nparagraph. The question and answer are often for-\nmulated as ﬁlling in a missing word in a query\nsentence (Hermann et al., 2015; Hill et al., 2015).\nThis task is closely related to the larger-context\nlanguage model we proposed in this paper in the\nsense that its goal is to build a model to learn\np(qk|q<k,q>k,D), (8)\nwhere qk is the missing k-th word in a query Q,\nand q<k and q>k are the context words from the\nquery. D is the paragraph containing facts about\nthis query. It is explicitly constructed so that the\nquery qdoes not appear in the paragraph D.\nIt is easy to see the similarity between Eq. (8)\nand one of the conditional probabilities in the\nr.h.s. of Eq. (7). By replacing the context sen-\ntences Sl−1\nl−n in Eq. (7) with Din Eq. (8) and con-\nditioning wt on both the preceding and follow-\ning words, we get a context-dependent question-\nanswering model. In other words, the pro-\nposed larger-context language model can be used\nfor context-dependent question-answering, how-\never, with computational overhead. The overhead\ncomes from the fact that for every possible answer\nthe conditional probability completed query sen-\ntence must be evaluated.\n5 Experimental Settings\n5.1 Models\nThere are six possible combinations of the pro-\nposed methods. First, there are two ways of rep-\nresenting the context sentences; (1) bag-of-words\n(BoW) and (2) a sequence of bag-of-words (Se-\nqBoW), from Sec. 3.1. There are two separate\nways to incorporate the SeqBoW; (1) with atten-\ntion mechanism (ATT) and (2) without it. Then,\nthere are two ways of feeding the context vector\ninto the main recurrent language model (RLM);\n(1) early fusion (EF) and (2) late fusion (LF), from\nSec. 3.2. We will denote them by\n1. RLM-BoW-EF- n\n2. RLM-SeqBoW-EF- n\n3. RLM-SeqBoW-ATT-EF-n\n4. RLM-BoW-LF- n\n5. RLM-SeqBoW-LF- n\n6. RLM-SeqBoW-ATT-LF-n\nn denotes the number of preceding sentences to\nhave as a set of context sentences. We test four\ndifferent values of n; 1, 2, 4 and 8.\nAs a baseline, we also train a recurrent language\nmodel without any context information. We refer\nto this model by RLM. Furthermore, we also re-\nport the result with the conventional, count-based\nn-gram language model with the modiﬁed Kneser-\nNey smoothing with KenLM (Heaﬁeld et al.,\n2013).\nEach recurrent language model uses 1000\nLSTM units and is trained with Adadelta (Zeiler,\n2012) to maximize the log-likelihood; L(θ) =\n1\nK\n∑K\nk=1 log p(Sk|Sk−1\nk−n). We early-stop training\nbased on the validation log-likelihood and report\nthe perplexity on the test set using the best model\naccording to the validation log-likelihood.\nWe use only those sentences of length up to 50\nwords when training a recurrent language model\nfor the computational reason. For KenLM, we\nused all available sentences in a training corpus.\n5.2 Datasets\nWe evaluate the proposed larger-context language\nmodel on three different corpora. For detailed\nstatistics, see Table 1.\nIMDB Movie Reviews A set of movie reviews\nis an ideal dataset to evaluate many different\nsettings of the proposed larger-context language\nmodels, because each review is highly likely of a\nsingle theme (the movie under review.) A set of\nwords or the style of writing will be well deter-\nmined based on the preceding sentences.\nWe use the IMDB Movie Review Corpus\n(IMDB) prepared by Maas et al. (2011).1 This cor-\npus has 75k training reviews and 25k test reviews.\n1http://ai.stanford.edu/˜amaas/data/\nsentiment/\n1324\n(a) IMDB (b) Penn Treebank\n(c) BBC (d) Fil9\nFigure 2: Corpus-level perplexity on (a) IMDB, (b) Penn Treebank, (c) BBC and (d) Fil9. The count-\nbased 5-gram language models with Kneser-Ney smoothing respectively resulted in the perplexities of\n110.20, 148, 127.32 and 65.21, and are not shown here.\nWe use the 30k most frequent words in the training\ncorpus for recurrent language models.\nBBC Similarly to movie reviews, each new ar-\nticle tends to convey a single theme. We use the\nBBC corpus prepared by Greene and Cunningham\n(2006).2 Unlike the IMDB corpus, this corpus\ncontains news articles which are almost always\nwritten in a formal style. By evaluating the pro-\nposed approaches on both the IMDB and BBC\ncorpora, we can tell whether the beneﬁts from\nlarger context exist in both informal and formal\nlanguages. We use the 10k most frequent words in\nthe training corpus for recurrent language models.\nBoth with the IMDB and BBC corpora, we did\nnot do any preprocessing other than tokenization.3\nPenn Treebank We evaluate a normal recurrent\nlanguage model, count-based n-gram language\nmodel as well as the proposed RLM-BoW-EF- n\nand RLM-BoW-LF-nwith varying n = 1,2,4,8\non the Penn Treebank Corpus. We preprocess the\n2http://mlg.ucd.ie/datasets/bbc.html\n3https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ntokenizer/tokenizer.perl\ncorpus according to (Mikolov et al., 2011) and use\na vocabulary of 10k words from the training cor-\npus.\nFil9 Fil9 is a cleaned Wikipedia corpus, consist-\ning of approximately 140M tokens, and is pro-\nvided on Matthew Mahoney’s website.4 We tok-\nenized the corpus and used the 44k most frequent\nwords in the training corpus for recurrent language\nmodels.\n6 Results and Analysis\nCorpus-level Perplexity We evaluated the mod-\nels, including all the proposed approaches (RLM-\n{BoW,SeqBoW}-{ATT,∅}-{EF,LF}-n), on the\nIMDB corpus. In Fig. 2 (a), we see three ma-\njor trends. First, RLM-BoW, either with the\nearly fusion or late fusion, outperforms both the\ncount-based n-gram and recurrent language model\n(LSTM) regardless of the number of context sen-\ntences. Second, the improvement grows as the\nnumber nof context sentences increases, and this\nis most visible with the novel late fusion. Lastly,\n4http://mattmahoney.net/dc/textdata\n1325\n(i) RLM-BoW-LF\n(ii) RLM-SeqBoW-ATT-LF\n(a) IMDB (b) BBC (b) Penn Treebank\nFigure 3: Perplexity per POS tag on the (a) IMDB, (b) BBC and (c) Penn Treebank corpora.\nwe see that the RLM-SeqBoW does not work\nwell regardless of the fusion type (RLM-SeqBow-\nEF not shown), while the attention-based model\n(RLM-SeqBow-ATT) outperforms all the others.\nAfter observing that the late fusion clearly\noutperforms the early fusion, we evaluated\nonly RLM- {BoW,SeqBoW}-{ATT}-LF-n’s on\nthe other two corpora.\nOn the other two corpora, PTB and BBC,\nwe observed a similar trend of RLM-SeqBoW-\nATT-LF-n and RLM-BoW-LF- n outperforming\nthe two conventional language models, and that\nthis trend strengthened as the numbernof the con-\ntext sentences grew. We also observed again that\nthe RLM-SeqBoW-ATT-LF outperforms RLM-\nSeqBoW-LF and RLM-BoW in almost all the\ncases.\nObserving the beneﬁt of RLM-SeqBoW-ATT-\nLF, we evaluated only such model on Fil9 to val-\nidate its performance on large corpus. Similar to\nthe results on all three previous corpora, we con-\ntinue to observe the advantage of RLM-SeqBoW-\nATT-LF-non Fil9 corpus.\nFrom these experiments, the beneﬁt of allow-\ning larger context to a recurrent language model is\nclear, however, with the right choice of the context\nrepresentation (see Sec. 3.1) and the right mech-\nanism for feeding the context information to the\nrecurrent language model (see Sec. 3.2.) In these\nexperiments, the sequence of bag-of-words repre-\nsentation with attention mechanism, together with\nthe late fusion was found to be the best choice in\nall four corpora.\nOne possible explanation on the failure of the\nSeqBoW representation with a context recurrent\nneural network is that it is simply difﬁcult for the\ncontext recurrent neural network to compress mul-\ntiple sentences into a single vector. This difﬁculty\nin training a recurrent neural network to com-\npress a long sequence into a single vector has been\nobserved earlier, for instance, in neural machine\ntranslation (Cho et al., 2014a). Attention mech-\nanism, which was found to avoid this problem\nin machine translation (Bahdanau et al., 2014), is\nfound to solve this problem in our task as well.\nPerplexity per Part-of-Speech Tag Next, we\nattempted at discovering why the larger-context\nrecurrent language model outperforms the uncon-\nditional one. In order to do so, we computed the\nperplexity per part-of-speech (POS) tag.\nWe used the Stanford log-linear part-of-speech\ntagger (Stanford POS Tagger, Toutanova et al.,\n2003) to tag each word of each sentence in the cor-\npora.5 We then computed the perplexity of each\nword and averaged them for each tag type sepa-\nrately. Among the 36 POS tags used by the Stan-\nford POS Tagger, we looked at the perplexities of\nthe ten most frequent tags (NN, IN, DT, JJ, RB,\nNNS, VBZ, VB, PRP, CC), of which we combined\nNN and NNS into a new tag Noun and VB and\nVBZ into a new tag Verb.\nWe show the results using the RLM-BoW-\nLF and RLM-SeqBoW-ATT-LF on three corpora–\nIMDB, BBC and Penn Treebank– in Fig. 3. We\nobserve that the predictability, measured by the\nperplexity (negatively correlated), grows most for\nnouns (Noun) and adjectives (JJ) as the number\nof context sentences increases. They are followed\nby verbs (Verb). In other words, nouns, adjec-\n5http://nlp.stanford.edu/software/\ntagger.shtml\n1326\nIMDB BBC Penn TreeBank Fil9\n# Sentences # Words # Sentences # Words # Sentences # Words # Sentences # Words\nTraining 930,139 21M 37,207 890K 42,068 888K 6,619,098 115M\nValidation 152,987 3M 1,998 49K 3,370 70K 825,919 14M\nTest 151,987 3M 2,199 53K 3,761 79K 827,416 14M\nTable 1: Statistics of IMDB, BBC, Penn TreeBank and Fil9.\ntives and verbs are the ones which become more\npredictable by a language model given more con-\ntext. We however noticed the relative degradation\nof quality in coordinating conjunctions (CC), de-\nterminers (DT) and personal pronouns (PRP).\nIt is worthwhile to note that nouns, adjectives\nand verbs are open-class, content, words, and con-\njunctions, determiners and pronouns are closed-\nclass, function, words (see, e.g., Miller, 1999).\nThe functions words often play grammatical roles,\nwhile the content words convey the content of a\nsentence or discourse, as the name indicates. From\nthis, we may carefully conclude that the larger-\ncontext language model improves upon the con-\nventional, unconditional language model by cap-\nturing the theme of a document, which is reﬂected\nby the improved perplexity on “content-heavy”\nopen-class words (Chung and Pennebaker, 2007).\nIn our experiments, this came however at the ex-\npense of slight degradation in the perplexity of\nfunction words, as the model’s capacity stayed\nsame (though, it is not necessary.)\nThis observation is in line with a recent ﬁnd-\ning by Hill et al. (2015). They also observed sig-\nniﬁcant gain in predicting open-class, or content,\nwords when a question-answering model, includ-\ning humans, was allowed larger context.\n7 Conclusion\nIn this paper, we proposed a method to improve\nlanguage model on corpus-level by incorporating\nlarger context. Using this model results in the im-\nprovement in perplexity on the IMDB, BBC, Penn\nTreebank and Fil9 corpora, validating the advan-\ntage of providing larger context to a recurrent lan-\nguage model.\nFrom our experiments, we found that the se-\nquence of bag-of-words with attention is better\nthan bag-of-words for representing the context\nsentences (see Sec. 3.1), and the late fusion is\nbetter than the early fusion for feeding the con-\ntext vector into the main recurrent language model\n(see Sec. 3.2). Our part-of-speech analysis re-\nvealed that content words, including nouns, adjec-\ntives and verbs, beneﬁt most from an increasing\nnumber of context sentences. This analysis sug-\ngests that larger-context language model improves\nperplexity because it captures the theme of a doc-\nument better and more easily.\nTo explore the potential of such a model, there\nare several aspects in which more research needs\nto be done. First, the four datasets we used in this\npaper are relatively small in the context of lan-\nguage modelling, therefore the proposed larger-\ncontext language model should be evaluated on\nlarger corpora. Second, more analysis, beyond the\none based on part-of-speech tags, should be con-\nducted in order to better understand the advantage\nof such larger-context models. Lastly, it is impor-\ntant to evaluate the impact of the proposed larger-\ncontext models in downstream tasks such as ma-\nchine translation and speech recognition.\nAcknowledgments\nThis work is done as a part of the course DS-GA\n1010-001 Independent Study in Data Science at\nthe Center for Data Science, New York Univer-\nsity. KC thanks Facebook, Google (Google Fac-\nulty Award 2016) and NVidia (GPU Center of Ex-\ncellence 2015–2016).\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. arXiv\npreprint arXiv:1409.0473 .\nYoshua Bengio, R ´ejean Ducharme, Pascal Vin-\ncent, and Christian Janvin. 2003. A neural prob-\nabilistic language model. The Journal of Ma-\nchine Learning Research 3:1137–1155.\nYoshua Bengio, Patrice Simard, and Paolo Fras-\nconi. 1994. Learning long-term dependencies\nwith gradient descent is difﬁcult. Neural Net-\nworks, IEEE Transactions on 5(2):157–166.\nDavid M Blei, Andrew Y Ng, and Michael I Jor-\ndan. 2003. Latent dirichlet allocation. the Jour-\nnal of machine Learning research 3:993–1022.\n1327\nKyunghyun Cho, Bart van Merrienboer, Dzmitry\nBahdanau, and Yoshua Bengio. 2014a. On\nthe properties of neural machine translation:\nEncoder-decoder approaches. In Eighth Work-\nshop on Syntax, Semantics and Structure in Sta-\ntistical Translation (SSST-8).\nKyunghyun Cho, Bart Van Merri ¨enboer, Caglar\nGulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Yoshua Bengio. 2014b.\nLearning phrase representations using rnn\nencoder-decoder for statistical machine transla-\ntion. arXiv preprint arXiv:1406.1078 .\nCindy Chung and James W Pennebaker. 2007. The\npsychological functions of function words. So-\ncial communication pages 343–359.\nJunyoung Chung, Caglar Gulcehre, Kyunghyun\nCho, and Yoshua Bengio. 2015. Gated feedback\nrecurrent neural networks. In Proceedings of\nthe 32nd International Conference on Machine\nLearning (ICML-15). pages 2067–2075.\nSusan T Dumais. 2004. Latent semantic analysis.\nAnnual review of information science and tech-\nnology 38(1):188–230.\nMikel L Forcada and Ram ´on P ˜Neco. 1997. Re-\ncursive hetero-associative memories for transla-\ntion. In Biological and Artiﬁcial Computation:\nFrom Neuroscience to Technology , Springer,\npages 453–462.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nDerek Greene and P ´adraig Cunningham. 2006.\nPractical solutions to the problem of diagonal\ndominance in kernel document clustering. In\nProc. 23rd International Conference on Ma-\nchine learning (ICML’06). ACM Press, pages\n377–384.\nKlaus Greff, Rupesh Kumar Srivastava, Jan\nKoutn´ık, Bas R Steunebrink, and J ¨urgen\nSchmidhuber. 2015. Lstm: A search space\nodyssey. arXiv preprint arXiv:1503.04069 .\nKenneth Heaﬁeld, Ivan Pouzyrevsky, Jonathan H.\nClark, and Philipp Koehn. 2013. Scalable mod-\niﬁed Kneser-Ney language model estimation.\nIn Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics .\nSoﬁa, Bulgaria, pages 690–696.\nKarl Moritz Hermann, Tom ´aˇs Ko ˇcisk`y, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. 2015. Teach-\ning machines to read and comprehend. arXiv\npreprint arXiv:1506.03340 .\nFelix Hill, Antoine Bordes, Sumit Chopra, and\nJason Weston. 2015. The goldilocks prin-\nciple: Reading children’s books with ex-\nplicit memory representations. arXiv preprint\narXiv:1511.02301 .\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation\n9(8):1735–1780.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris\nDyer, and Jacob Eisenstein. 2015. Docu-\nment context language models. arXiv preprint\narXiv:1511.03962 .\nRafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. 2015. An empirical exploration of\nrecurrent network architectures. In Proceedings\nof the 32nd International Conference on Ma-\nchine Learning (ICML-15). pages 2342–2350.\nNal Kalchbrenner and Phil Blunsom. 2013. Recur-\nrent continuous translation models. In EMNLP.\npages 1700–1709.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S Zemel, Antonio Torralba, Raquel Ur-\ntasun, and Sanja Fidler. 2015. Skip-thought vec-\ntors. arXiv preprint arXiv:1506.06726 .\nAndrew L Maas, Raymond E Daly, Peter T\nPham, Dan Huang, Andrew Y Ng, and Christo-\npher Potts. 2011. Learning word vectors for\nsentiment analysis. In Proceedings of the\n49th Annual Meeting of the Association for\nComputational Linguistics: Human Language\nTechnologies-Volume 1. Association for Com-\nputational Linguistics, pages 142–150.\nTomas Mikolov, Martin Karaﬁ ´at, Lukas Bur-\nget, Jan Cernock `y, and Sanjeev Khudanpur.\n2010. Recurrent neural network based lan-\nguage model. In INTERSPEECH 2010, 11th\nAnnual Conference of the International Speech\nCommunication Association, Makuhari, Chiba,\nJapan, September 26-30, 2010 . pages 1045–\n1048.\nTom´aˇs Mikolov, Stefan Kombrink, Luk´aˇs Burget,\nJan Honza ˇCernock`y, and Sanjeev Khudanpur.\n2011. Extensions of recurrent neural network\nlanguage model. In Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2011 IEEE Interna-\ntional Conference on. IEEE, pages 5528–5531.\n1328\nTomas Mikolov and Geoffrey Zweig. 2012. Con-\ntext dependent recurrent neural network lan-\nguage model. In SLT. pages 234–239.\nGeorge A Miller. 1999. On knowing a word. An-\nnual Review of psychology 50(1):1–19.\nRonald Rosenfeld. 2000. Two decades of statis-\ntical language modeling: where do we go from\nhere. In Proceedings of the IEEE.\nDavid E Rumelhart, Geoffrey E Hinton, and\nRonald J Williams. 1988. Learning represen-\ntations by back-propagating errors. Cognitive\nmodeling 5:3.\nIulian V Serban, Alessandro Sordoni, Yoshua\nBengio, Aaron Courville, and Joelle Pineau.\n2015. Hierarchical neural network generative\nmodels for movie dialogues. arXiv preprint\narXiv:1507.04808 .\nMartin Sundermeyer, Hermann Ney, and Ralf\nSchluter. 2015. From feedforward to recur-\nrent lstm neural networks for language model-\ning. Audio, Speech, and Language Processing,\nIEEE/ACM Transactions on 23(3):517–529.\nIlya Sutskever, Oriol Vinyals, and Quoc VV Le.\n2014. Sequence to sequence learning with neu-\nral networks. In Advances in neural information\nprocessing systems. pages 3104–3112.\nKristina Toutanova, Dan Klein, Christopher D\nManning, and Yoram Singer. 2003. Feature-\nrich part-of-speech tagging with a cyclic depen-\ndency network. In Proceedings of the 2003\nConference of the North American Chapter of\nthe Association for Computational Linguistics\non Human Language Technology-Volume 1. As-\nsociation for Computational Linguistics, pages\n173–180.\nMatthew D Zeiler. 2012. Adadelta: An adap-\ntive learning rate method. arXiv preprint\narXiv:1212.5701 .\n1329",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9187650680541992
    },
    {
      "name": "Computer science",
      "score": 0.8783919215202332
    },
    {
      "name": "Treebank",
      "score": 0.866824746131897
    },
    {
      "name": "Language model",
      "score": 0.7400389909744263
    },
    {
      "name": "Natural language processing",
      "score": 0.7056927680969238
    },
    {
      "name": "Artificial intelligence",
      "score": 0.677442729473114
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6706788539886475
    },
    {
      "name": "Sentence",
      "score": 0.6282568573951721
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4986395835876465
    },
    {
      "name": "Context model",
      "score": 0.4844205677509308
    },
    {
      "name": "Noun",
      "score": 0.44373437762260437
    },
    {
      "name": "Word (group theory)",
      "score": 0.44156554341316223
    },
    {
      "name": "Artificial neural network",
      "score": 0.40448471903800964
    },
    {
      "name": "Linguistics",
      "score": 0.2266378402709961
    },
    {
      "name": "Parsing",
      "score": 0.11355903744697571
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I57206974",
      "name": "New York University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36672615",
      "name": "Courant Institute of Mathematical Sciences",
      "country": "US"
    }
  ]
}