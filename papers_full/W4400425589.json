{
  "title": "From large language models to small logic programs: building global explanations from disagreeing local post-hoc explainers",
  "url": "https://openalex.org/W4400425589",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2976661471",
      "name": "Andrea Agiollo",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2078959474",
      "name": "Luciano Cavalcante Siebert",
      "affiliations": [
        "Delft University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2040248433",
      "name": "Pradeep K. Murukannaiah",
      "affiliations": [
        "Delft University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A9189865",
      "name": "Andrea Omicini",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2976661471",
      "name": "Andrea Agiollo",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A9189865",
      "name": "Andrea Omicini",
      "affiliations": [
        "University of Bologna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964236337",
    "https://openalex.org/W4205458180",
    "https://openalex.org/W3019166713",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3089472875",
    "https://openalex.org/W3033076904",
    "https://openalex.org/W2972203331",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W2926587947",
    "https://openalex.org/W4298181573",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4285798540",
    "https://openalex.org/W4386431516",
    "https://openalex.org/W4391645809",
    "https://openalex.org/W4223959013",
    "https://openalex.org/W4381996778",
    "https://openalex.org/W4387619784",
    "https://openalex.org/W4323903890",
    "https://openalex.org/W4284967716",
    "https://openalex.org/W4200260440",
    "https://openalex.org/W2036166268",
    "https://openalex.org/W2295416969",
    "https://openalex.org/W3008655042",
    "https://openalex.org/W2944670321",
    "https://openalex.org/W3194462780",
    "https://openalex.org/W2962772482",
    "https://openalex.org/W2891503716",
    "https://openalex.org/W3139145960",
    "https://openalex.org/W2536885573",
    "https://openalex.org/W2963399068",
    "https://openalex.org/W2618851150",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W3187467055",
    "https://openalex.org/W3090395639",
    "https://openalex.org/W1519043595",
    "https://openalex.org/W3132191748",
    "https://openalex.org/W2979200397",
    "https://openalex.org/W4221167913",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3035422918",
    "https://openalex.org/W3098839135",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W3154507271",
    "https://openalex.org/W4385571890",
    "https://openalex.org/W2562564313",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W2803187616",
    "https://openalex.org/W4285295903",
    "https://openalex.org/W4285201554",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W2990283237",
    "https://openalex.org/W3186876638",
    "https://openalex.org/W3179262290",
    "https://openalex.org/W7047491838",
    "https://openalex.org/W4385412194",
    "https://openalex.org/W2999905431",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3134751001"
  ],
  "abstract": "Abstract The expressive power and effectiveness of large language models (LLMs) is going to increasingly push intelligent agents towards sub-symbolic models for natural language processing (NLP) tasks in human–agent interaction. However, LLMs are characterised by a performance vs. transparency trade-off that hinders their applicability to such sensitive scenarios. This is the main reason behind many approaches focusing on local post-hoc explanations, recently proposed by the XAI community in the NLP realm. However, to the best of our knowledge, a thorough comparison among available explainability techniques is currently missing, as well as approaches for constructing global post-hoc explanations leveraging the local information. This is why we propose a novel framework for comparing state-of-the-art local post-hoc explanation mechanisms and for extracting logic programs surrogating LLMs. Our experiments—over a wide variety of text classification tasks—show how most local post-hoc explainers are loosely correlated, highlighting substantial discrepancies in their results. By relying on the proposed novel framework, we also show how it is possible to extract faithful and efficient global explanations for the original LLM over multiple tasks, enabling explainable and resource-friendly AI techniques.",
  "full_text": "Vol.:(0123456789)\nAutonomous Agents and Multi-Agent Systems (2024) 38:32\nhttps://doi.org/10.1007/s10458-024-09663-8\nFrom large language models to small logic programs: \nbuilding global explanations from disagreeing local post‑hoc \nexplainers\nAndrea Agiollo1 · Luciano Cavalcante Siebert2 · Pradeep K. Murukannaiah2 · \nAndrea Omicini1\nAccepted: 19 June 2024 / Published online: 8 July 2024 \n© The Author(s) 2024\nAbstract\nThe expressive power and effectiveness of large language models (LLMs) is going to \nincreasingly push intelligent agents towards sub-symbolic models for natural language \nprocessing (NLP) tasks in human–agent interaction. However, LLMs are characterised by \na performance vs. transparency trade-off that hinders their applicability to such sensitive \nscenarios. This is the main reason behind many approaches focusing on local post-hoc \nexplanations, recently proposed by the XAI community in the NLP realm. However, to the \nbest of our knowledge, a thorough comparison among available explainability techniques \nis currently missing, as well as approaches for constructing global post-hoc explanations \nleveraging the local information. This is why we propose a novel framework for comparing \nstate-of-the-art local post-hoc explanation mechanisms and for extracting logic programs \nsurrogating LLMs. Our experiments—over a wide variety of text classification tasks—\nshow how most local post-hoc explainers are loosely correlated, highlighting substantial \ndiscrepancies in their results. By relying on the proposed novel framework, we also show \nhow it is possible to extract faithful and efficient global explanations for the original LLM \nover multiple tasks, enabling explainable and resource-friendly AI techniques.\nKeywords Natural language processing · Post-hoc explanations · Symbolic knowledge \nextraction · EXplainable AI · Resource-friendly AI\n * Andrea Agiollo \n andrea.agiollo@unibo.it\n Luciano Cavalcante Siebert \n L.CavalcanteSiebert@tudelft.nl\n Pradeep K. Murukannaiah \n P.K.Murukannaiah@tudelft.nl\n Andrea Omicini \n andrea.omicini@unibo.it\n1 Dipartimento di Informatica - Scienza e Ingegneria (DISI), Alma Mater Studiorum-Università di \nBologna, Cesena, Italy\n2 Delft University of Technology, Delft, The Netherlands\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 2 of 33\n1 Introduction\nLarge language models (LLMs) represent the de-facto solution for dealing with complex \nnatural language processing (NLP) tasks such as sentiment analysis [1], question answering \n[2], and many others [3]. The ever-increasing popularity of such data-driven approaches is \nlargely due to their uncanny performance improvements against human counterparts over \ntasks such as grammar acceptability of a sentence [4] and text translation [5]. In this con-\ntext, the foreseeable future of intelligent agent systems is likely to be deeply intertwined \nwith LLMs. Intelligent agents exploiting NLP-enabled process for human-agent interaction \nas well as for inter-agent communication within complex Multi-Agent Systems (MASs) are \ngoing to become more and more popular [6, 7]. Natural language explanations are funda-\nmental for improving agents expressiveness and explaining agent actions, beliefs, and rea-\nsoning [8], as well as argumentation and negotiation processes [9, 10]. However, leaning \non LLMs—and Neural Networks (NNs) for NLP tasks in general—brings about a novel \nlayer of complexity, requiring full comprehensibility of sub-symbolic components. Agent’s \nobservable behaviour should be understandable at every step, which requires the explaina-\nbility of the sub-symbolic mechanisms in charge of the interaction—both with humans and \nagent-to-agent. Substantial reliance on LLMs does not make explanation extraction easy, \nas the LLM decision process is far from being transparent, given the complexity of popular \narchitectures such as BERT [11], GPT [12], and T5 [13]. While powerful and empirically \nreliable, those models suffer from a performance vs. transparency trade-off [14, 15].\nLLMs are black-box models, as they rely on the optimisation of their numerical sub-\nsymbolical components, which are mostly unreadable by humans. Mechanisms are then \nneeded that could make the reasoning process of LLM black-boxes somehow observable \nand understandable by humans. To this aim, a few different explainability approaches have \nbeen recently proposed, which mostly focus on Local Post-hoc Explainer (LPE) mecha-\nnisms. An LPE represents a popular solution to explain the reasoning process by highlight-\ning how different portions of the input sample impact differently the produced output, by \nassigning a relevance score to each input component. These approaches apply to single \ninstances of input sample—they are local—and to optimised LLM—they are post-hoc. \nWhile popular, such approaches do not give information about the general reasoning prin-\nciple of the underlying LLM, as they cannot produce a global view. Moreover, despite a \nbroad variety of LPE approaches, the state of the art lacks a fair comparison among them. \nA common trend for proposals of novel explanation mechanisms is to highlight its advan-\ntages through a set of tailored experiments. This hinders comparison fairness, making it \nvery difficult to identify the best approach for explanations of NLP models, or even to \ndetermine whether a best approach exists.\nThis is why in the following we present a framework for comparing several well-known \nLPE mechanisms for text classification in NLP—first introduced in [16]. Aiming at obtain-\ning comparison fairness, we rely on the aggregation of the local explanations obtained by \neach local post-hoc explainer into a set of global impact scores. The scores identify the set \nof concepts that best describe the underlying NLP model from the perspective of each LPE. \nThe concepts, along with their aggregated impact scores, are then compared for each LPE \nagainst other LPE counterparts. The comparison between the aggregated global impact \nscores rather than the single explanations is justified by the locality of LPE approaches. \nIndeed, it is reasonable for local explanations of different LPEs to differ somehow, depend-\ning on the approach design, therefore making it complex to compare the quality of two \nLPEs over the same sample. However, it is also expected for the aggregated global impacts \nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 3 of 33 32\nto be aligned between different LPEs as they are applied to the same NN, which leverages \nthe same set of relevant concepts for its inference. Therefore, when comparing the aggre-\ngated impact scores of different LPEs, we expect them to be correlated—at least up to a \ncertain extent.\nSubsequently, given the lack of available global post-hoc explainers, we consider ena-\nbling the extraction of global explanations from the output of a single LPE. Here, we refer \nto global as expressing the totality of knowledge acquired by the LLM at training time, thus \nbeing representative of the full reasoning principles of the LLM at hand. We extend the \nLPE comparison framework first introduced in [16] leveraging a a neuro-symbolic process \n[17–21] to build global explanations from LPEs. In this context, we aim at extracting the \nLLM knowledge under the form of a logic program equivalent to the sub-symbolic model \nat hand, similarly to [22, 23]. More in detail, the presented knowledge extraction frame-\nwork—namely, Global Explanations from Local Post-hoc Explainers (GELPE)—relies on \nthe output of a single LPE to identify the set of most relevant components in sentences, and \noptimise a transparent-by-design—such as Classification and Regression Tree (CART)—\nsurrogate model to mimic the LLM predictions. Once the transparent model is optimised, \nan equivalent logic program is extracted from the model, allowing for the inspection of the \nglobal reasoning process of the LLM. Figure 1 summarises the GELPE’s working process. \nWhile simple, this approach represents up to our knowledge the first mechanism for build-\ning global explanations of LLMs for text classification accounting for the available local \nexplanations. As such, the proposed approach is likely to represent a desirable tool for the \ntrustworthy and explainable AI community, as it allows opening LLM black-boxes while \nkeeping the explanations complexity bounded. Identifying small and efficient surrogate \nprograms over several tasks, the proposed framework enables the deployment of intelligent \ntechniques over resource-constrained environments where LLMs represent a limited solu-\ntion [24, 25].\nWe test the proposed framework over a large set of text classification domains, ranging \nfrom simple scenarios—e.g., spam text classification [26, 27]—to challenging tasks such as \nthe Moral Foundation Twitter Corpus (MFTC) [28]. Possibly surprisingly, our experiments \nshow how the explanations of different LPEs are far from being correlated, highlighting \nhow explanation quality is highly dependent on the chosen eXplainable Artificial Intel-\nligence (xAI) approach and the respective scenario at hand. There are huge discrepancies \nFig. 1  Overview of the GELPE extraction process. The LLM belonging to a smart agent is examined by a \nsingle LPE mechanism, generating a set of relevant lemmas (see Eq.  16 for more details). The set of rele-\nvant lemmas, along with the LLM’s predictions for a set of available inputs, are used to optimise the CART \nmodel to mimic the LLM behaviour. Thereafter, the CART model can be converted quickly into a logic \nprogram equivalent to the starting LLM.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 4 of 33\nin the results of different state-of-the-art local explainers, each of which identifies a set of \nrelevant concepts that largely differs from the others—at least in terms of relative impact \nscores. These results highlight the fragility of xAI approaches for NLP, caused mainly by \nthe complexity of large NN models, their inclination to extreme fitting of data and the lack \nof sound techniques for comparing xAI mechanisms. Notably, the proposed experiments \nalso highlights how GELPE enables the extraction of reliable surrogate logic programs \nfrom LLMs with high fidelity over a broad set of datasets. The extracted knowledge is not \nonly faithful to the original model, but also quite simple, as the complexity of the logic \nprogram is kept bounded depending on the number of relevant lemmas selected. Through-\nout our experimental evaluation, we analyse the computation requirements of the proposed \nextraction process and the efficiency of the extracted logic program. Numerical results \nhighlight the efficiency of the extracted surrogate model, improving over the original LLM \nin terms of required processing time and consumed energy. The results show how the pro-\nposed framework enables the deployment of intelligent solutions over resource-constrained \nenvironments via identifying transparent surrogate models. Also, we highlight that leverag-\ning on LLMs to tackle a learning task in NLP does not always represent the best option, as \nalternative equivalent solutions that are simple, small and transparent can be available [29, \n30].\nContributions: We summarise our contributions as follows:\n• We present the first framework for comparing explanations obtained leveraging differ -\nent LPEs over LLMs. The proposed scheme is designed to assert the correlation level \nof LPEs over a broad set of input sentences.\n• We test the correlation performance of seven different LPEs over nine different NLP \ndatasets, showcasing how state-of-the-art LPEs strongly disagree.\n• We present GELPE, the first framework for extracting global explanations from the \noutput of LPE processes, enabling the extraction of logic rules from LLMs.\n• We study the performance of GELPE considering its fidelity with respect to LLMs, the \ncomplexity of the extracted rules and its achievable efficiency improvements, showcas-\ning encouraging results.\nOrganization: Sect. 2 discusses the basic concepts of available explanation mechanisms in \nNLP, along with the required discussion between local and global explanations. Section  3 \npresents the methodology used in this paper for comparing LPE mechanisms and building \nglobal explanations from LPE’s outputs. The experimental evaluation of our methodology \nis made available in Sect.  4, in which we first focus on the comparison between the avail-\nable LPEs in Sect.  4.3, while Sect.  4.4 presents the knowledge extraction results. Conse-\nquently, Sect.  5 discusses the limitations of the proposed methodology, whereas Sect.  6 \nconcludes the paper with some insight into the possible extensions of our work.\nGlossary: Table 1 summarises notations used in the article.\n2  Background: explanation mechanisms in NLP\nThe set of explanations extraction mechanisms available in the xAI community are often \ncategorised along two main axis [31, 32]: (i) local against global explanations, and (ii) \nself-explaining against post-hoc approaches. In the former context, local identifies the \nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 5 of 33 32\nset of explainability approaches that given a single input produce an explanation of the \nreasoning process followed by the NN model to output its prediction for the given input \n[33]. In contrast, global explanations aim at expressing the reasoning process of the \nNN model as a whole [34, 35]. Given the complexity of the NN models leveraged for \ntackling most NLP tasks, it is worth noticing how there is a significant lack of global  \nexplainability systems, whereas a variety of local xAI approaches are available [36, 37].\nAbout the latter aspect, we define post-hoc as those set of explainability approaches \nwhich apply to an already optimised black-box model for which it is required to obtain \nsome sort of insight [38]. Therefore, a post-hoc approach requires additional operations \nto be performed after that the model outputs its predictions [39]. Conversely, inher -\nently explainable—self-explaining—mechanisms aim at building a predictor having a \ntransparent reasoning process by design—e.g., CART [40]. Therefore, a self-explaining \napproach can be seen as generating the explanation along with its prediction, using the \ninformation emitted by the model as a result of the prediction process [39].\nIn the context of local post-hoc explanation approaches, a popular solution in NLP \nis to explain the reasoning process by highlighting how different portions of the input \nsample impact differently the produced output, by assigning a relevance score to each \ninput component. The relevance score is then highlighted by using some saliency map \nto ease the visualisation of the obtained explanation. Therefore, it is also common for \nlocal post-hoc explanations to be referred to as saliency approaches, as they aim at high-\nlighting salient components.\nTable 1  Summary of glossary. Acronym Definition\nNN Neural Network\nNLP Natural Language Processing\nxAI eXplainable Artificial Intelligence\nLPE Local Post-hoc explainer\nMFTC Moral Foundation Twitter Corpus\nLLM Large Language Model\nCART Classification And Regression Tree\nGELPE Global Explanations from Local Post-hoc Explainers\nSHAP SHapley Additive exPlanations\nBLM Black Lives Matter\nALM All Lives Matter\nBLT Baltimore protests\nDAV hate speech and offensive language\nELE 2016 presidential election\nMT MeToo movement\nSND hurricane Sandy\nGS Gradient Sensitivity analysis\nGI Gradient × Input\nLRP Layer-wise Relevance Propagation\nLAT Layer-wise Attention Tracing\nLIME Local Interpretable Model-agnostic Explanations\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 6 of 33\n3  Methodology\nIn this section, we present our methodology for comparing LPE mechanisms and build-\ning global explanations from LPE’s outputs. We first overview the proposed approach in \nSect. 3.1. Subsequently, the set of LPE mechanisms adopted in our experiments are pre-\nsented in Sect.  3.2, and the aggregation approaches leveraged to obtain global impact \nscores from LPE outputs are described in Sect.  3.3. In Sect.  3.4 we present the metrics \nused to identify the correlation between LPEs. Finally, in Sect. 3.5 we propose GELPE as a \nnovel methodology to build global explanations of LLMs on top of LPE’s outputs.\n3.1  Overview\nMeasuring different LPE approaches over single local explanations is a complex task. This \nis why we first consider measuring how much LPEs correlate with each other over a set \nof fixed samples. The underlying assumption of our framework is that various LPE tech-\nniques aim at explaining the same NN model used for prediction. Therefore, while expla-\nnations may differ over local samples, one could reasonably assume that reliable LPEs \nwhen applied over a vast set of samples—sentences or set of sentences—should converge \nto similar (correlated) results. Indeed, the underlying LLM considers being relevant for its \ninference always the same set of concepts—lemmas. A lack of correlation between differ -\nent LPE mechanisms would hint to the existence of a conflict among the set of concepts \nthat each explanation mechanism considers as relevant for the LLM—thus making at least \none, if not all, of the explanations unreliable.\nWe first analyse the correlation between a set of LPEs over the same pool of samples, \nand define /u1D716NN as a LPE technique applied to a NN model at hand. Being local, /u1D716NN is \napplied to the single input sample xi , producing as output one impact score for each com-\nponent (token) of the input sample lk  . Throughout the remainder of the paper, we consider \nlk  to be the lemmas corresponding to the input components. Mathematically, we define \nthe output impact score for a single token or its corresponding lemma as j/parenleft.s1lk , /u1D716NN (xi)/parenright.s1 . \nDepending on the given /u1D716NN , the corresponding impact score j may be associated with a \nsingle label, making j a scalar value, or with a set of labels, making j a vector—one sca-\nlar value for each label. To enable the comparison between different LPE, we define the \naggregated impact scores of a LPE mechanism over a NN model and a set of samples S as \n/u1D716NN(S) . In our framework we obtain /u1D716NN(S) aggregating /u1D716NN (xi) for each xi ∈ S using an \naggregation operation A—mathematically:\nBy defining a correlation metric C , we obtain from Eq.  1 the following for describing the \ncorrelation between two LPE techniques:\nwhere /u1D716NN and /u1D716/uni2032.var\nNN are two LPE techniques applied to the same NN model.\nThe aggregated explanations /u1D716NN(S) obtained from LPE’s outputs can also be leveraged \nas a starting point for building transparent surrogate models of the original LLM, as they \nhighlight the impact of each lemma or token in the LLM decision process. Constructing \na transparent surrogate model allows for extracting explanations of the global reasoning \n(1)/u1D716NN (S) = A/parenleft.s1/braceleft.s1/u1D716NN (xi) for each xi ∈ S/braceright.s1/parenright.s1.\n(2)\nC\n/parenleft.s1\n/u1D716NN (S), /u1D716�\nNN (S)\n/parenright.s1\n= C\n/parenleft.s1\nA\n/parenleft.s1/braceleft.s1\n/u1D716NN (xi) for each xi ∈ S\n/braceright.s1/parenright.s1\n,\nA/parenleft.s1/braceleft.s1/u1D716�\nNN (xi) for each xi ∈ S/braceright.s1/parenright.s1/parenright.s1\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 7 of 33 32\nprocess of the black-box LLM, enabling knowledge extraction, model debugging, and \ninteraction with a human user or other intelligent agents. To this extent, we here propose \nGELPE as a novel framework for constructing a logic program—represented as a set of \nsequential propositional rules—that mimics the LLM behaviour starting from a set of \nlocally relevant lemmas /u1D716NN(S) , extracted using a single LPE. More in detail, GELPE relies \non transparent-by-design models such as CART optimised over the LLM outputs, rather \nthan the dataset considered.\nWe rely on CART models as they represent one of the easiest and most reliable \napproaches to identify human-readable rules—under the form of trees—from complex \nstructured data. In summary, the optimisation of CART models involves selecting input \nvariables and split points on those variables until a suitable tree is constructed. The selec-\ntion of which input variables and split points to use is performed using a greedy algorithm \naiming at minimising a given cost function. Finally, the tree construction process ends \nusing a predefined stopping criterion, such as a minimum number of training instances \nassigned to each leaf node of the tree. The set of tree-structured rules extracted using \nCART can be easily translated into a list of sequential, human-readable expressions that \ncontain logic expressions over the input variables, by extracting one rule for each leaf used \nin the CART model. Therefore, CART represents a very popular solution for extracting \nexplanations from fuzzy data or black-box classifiers, trying to mimic their outputs. How -\never, a thorough background on CART models is out of the scope of this paper and we \nrefer interested readers to [40].\nSince CART relies on structured—usually tabular—data to perform optimisation and \ninference, we convert the input sentences into a binary format, expressing the presence or \nabsence of relevant lemmas and their combinations. The binarised input is used to optimise \nthe underlying CART model, from which it is possible to extract the equivalent logic pro-\ngram P . Mathematically, we represent the knowledge extraction procedure as:\nwhere H identifies the transparent-by-design models used to extract the explanations logic \nprogram P , bin/u1D716NN (S) represents the binarization process used to convert the sentence xi into \na corresponding binary vector of lemmas occurences and NN (xi) identifies the output of \nthe LLM when fed with input sentence xi.\n3.2  Local post‑hoc explanations\nIn our framework, we consider seven different LPE approaches for extracting local expla-\nnations j/parenleft.s1lk , /u1D716NN (xi)/parenright.s1 from an input sentence xi and the trained LLM—identified as NN . \nThe seven LPEs are selected in order to represent as faithfully as possible the state-of-the-\nart of xAI approaches in NLP. Subsequently, we briefly describe each of the seven selected \nLPEs. However, a detailed analysis of these LPEs is out of the scope of this paper and we \nrefer interested readers to [33, 39, 41].\n3.2.1  Gradient sensitivity analysis (GS)\nThe Gradient Sensitivity Analysis (GS) probably represents the simplest approach for \nassigning relevance scores to input components. GS relies on computing gradients over \n(3)P = H/braceleft.s1(bin/u1D716NN (S)(xi), NN (xi)) ∀ xi ∈ S/braceright.s1,\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 8 of 33\ninputs components as \n/u1D6FFf/u1D70Fm\n(xi)\n/u1D6FFxi,k\n , which represents the derivative of the output with respect to \nthe the kth component of xi . Following this approach local impact scores of an input com-\nponent can be thus defined as:\nwhere f/u1D70Fm\n(xi) represents the predicted probability distribution of an input sequence xi over \na target class /u1D70Fm . While simple, GS has been shown to be an effective approach for under -\nstanding approximate input components relevance. However, this approach suffers from a \nvariety of drawbacks, mainly linked with its inability to define negative contributions of \ninput components for a specific prediction—i.e., negative impact scores.\n3.2.2  Gradient × input (GI)\nAiming at addressing few of the limitations affecting GS, the Gradient × Input (GI) \napproach defines the relevance scores assignment as GS multiplied—element-wise—with \nxi,k  [42]. Therefore, mathematically speaking, GI impact scores are defined as:\nwhere notation follows the one of Eq. 4. Being very similar to GS, GI also inherits most of \nits limitations.\n3.2.3  Layer‑wise relevance propagation (LRP)\nBuilding on top of gradient-based relevance scores mechanisms—such as GS and GI—, \nLayer-wise Relevance Propagation (LRP) proposes a novel mechanism relying on conser -\nvation of relevance scores across the layers of the NN at hand. Indeed, LRP relies on the \nfollowing assumptions: (i) NN can be decomposed into several layers of computation; (ii) \nthere exists a relevance score R (l)\nd  for each dimension z(l)\nd  of the vector z(l) obtained as the \noutput of the lth layer of the NN; and (iii) the total relevance scores across dimensions \nshould propagate through all layers of the NN model, mathematically:\nwhere, f (x) represents the predicted probability distribution of an input sequence x , and \nL the number of layers of the NN at hand. Moreover, LRP defines a propagation rule for \nobtaining R (l)\nd  from R (l+1) . However, the derivation of the propagation rule is out of the \nscope of this paper, thus we refer interested readers to [43, 44]. In our experiments we con-\nsider as impact scores the relevance scores of the input layer, namely j/parenleft.s1lk,/u1D716NN (xi)/parenright.s1= R (1)\nd .\n(4)j/parenleft.s1lk , /u1D716NN (xi)/parenright.s1=\n/u1D6FFf/u1D70Fm\n(xi)\n/u1D6FFxi,k\n,\n(5)j/parenleft.s1lk , /u1D716NN (xi)/parenright.s1= xi,k ⋅\n/u1D6FFf/u1D70Fm\n(xi)\n/u1D6FFxi,k\n,\n(6)f(x)=\n/uni2211.s1\nd∈L\nR (L)\nd =\n/uni2211.s1\nd∈L−1\nR (L−1)\nd = ⋯ =\n/uni2211.s1\nd∈1\nR (1)\nd ,\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 9 of 33 32\n3.2.4  Layer‑wise attention tracing (LAT)\nSince LLMs rely heavily on self-attention mechanisms [45], recent efforts propose to iden-\ntify input components relevance scores analysing solely the relevance scores of attentions \nheads of LLM models, introducing Layer-wise Attention Tracing (LAT) [46, 47]. Building \non top of LRP, LAT proposes to redistribute the inner relevance scores R (l) across dimen-\nsions using solely self-attention weights. Therefore, LAT defines a custom redistribution \nrule as:\nwhere, h corresponds to the attention head index, while a(h) are the corresponding learnt \nweights of the attention head and k is such that i is input for neuron k. Similarly to \nLRP, we here consider as impact scores the relevance scores of the input layer, namely \nj/parenleft.s1lk ,/u1D716NN (xi)/parenright.s1= R (1).\n3.2.5  Integrated gradient (HESS)\nMotivated by the shortcomings of previously proposed gradient-based relevance score \nattribution mechanisms—such as GS and GI—, Sundararajan et al. [48] propose a novel \nIntegrated Gradient approach. The proposed approach aims at explaining the input sample \ncomponents relevance by integrating the gradient along some trajectory of the input space, \nwhich links some baseline value x/uni2032.var\ni to the sample under examination xi . Therefore, the rel-\nevance score of the kth input component of the input sample xi is obtained following\nwhere xi,k  represents the kth component of the input sample xi . By integrating the gradient \nalong an input space trajectory, the authors aim at addressing the locality issue of gradient \ninformation. In our experiments we refer to the Integrated Gradient approach as HESS, as \nfor its implementation we rely on the integrated hessian library available for hugging face \nmodels.1\n3.2.6  SHapley additive exPlanations (SHAP)\nSHapley Additive exPlanations (SHAP) relies on Shapley values to identify the contribu-\ntion of each component of the input sample toward the final prediction distribution. The \nShapley value concept derives from game theory, where it represents a solution for a coop-\nerative game, found assigning a distribution of a total surplus generated by the players \ncoalition. SHAP computes the impact of an input component as its marginal contribution \ntoward a label /u1D70Fm , computed deleting the component from the input and evaluating the out-\nput discrepancy. Firstly defined for explaining simple NN models [36], in our experiments \n(7)R (l)\ni =\n/uni2211.s1\nk\n/uni2211.s1\nh\na(h) R (l+1)\nk,h ,\n(8)j\n/parenleft.s1\nlk ,/u1D716NN (xi)\n/parenright.s1\n=\n/parenleft.s2\nxi,k − x�\ni,k\n/parenright.s2\n⋅ /uni222B.dsp\n1\na =0\n/u1D6FFf(x�\ni + t ⋅ (xi − x�\ni))\n/u1D6FFxi,k\ndt,\n1 https:// github. com/ suinl eelab/ path_ expla in.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 10 of 33\nwe leverage the extension of SHAP supporting transformer models such as BERT [49], \navailable in the SHAP python library.2\n3.2.7  Local interpretable model‑agnostic explanations (LIME)\nSimilarly to SHAP, Local Interpretable Model-agnostic Explanations (LIME) relies on \ninput sample perturbation to identify its relevant components. Here, the predictions of the \nNN at hand are explained via learning an explainable surrogate model [37]. In detail, in \norder to obtain its explanations LIME constructs a set of samples from the perturbation \nof the input observation under examination. The constructed samples are considered to be \nclose to the observation to be explained from a geometric perspective, thus considering \nsmall perturbation of the input. The explainable surrogate model is then trained over the \nconstructed set of samples, obtaining the corresponding local explanation. Given an input \nsentence, we here consider obtaining its perturbed version via words—or tokens—removal \nand words substitution. In our experiments, we rely on the already available LIME python \nlibrary.3\n3.3  Aggregating local explanations\nOnce local explanations of the NN model are obtained for each input sentence, we aggre-\ngate them to obtain a global list of concept impact scores. Before aggregating the local \nimpact scores, we convert the words composing local explanations into their correspond-\ning lemmas-i.e., concepts-to avoid issues when aggregating different words expressing the \nsame concept-e.g., hate and hateful. As no bullet-proof solution exists for the aggregation \nof different impact scores, we adopt four different approaches in our experiments, namely: \nSum  A simple summation operation is leveraged to obtain the aggregated \nscore for each lemma. While simple this aggregation approach is effec-\ntive when dealing with additive impact scores such as SHAP values. \nHowever, it suffers from lemma frequency issues, as it tends to overes-\ntimate frequent lemmas with average low impact scores. Global impact \nscores are here defined as J(lk ,/u1D716NN )= ∑N\ni=1 j�lk ,/u1D716NN\n�xi\n�� . Therefore, \nwe define A as \nAbsolute sum  Here we sum the absolute values of the local impact scores—rather \nthan their true values—to increase the awareness of global impact \nscores towards lemmas having both high positive and high negative \nimpact over some sentences. Mathematically, we obtain aggregated \nscores as J(lk ,/u1D716NN )= ∑N\ni=1 �j�lk ,/u1D716NN\n�xi\n��� . \n(9)A/parenleft.s1/braceleft.s1/u1D716NN (xi) for each xi ∈ S/braceright.s1/parenright.s1=\n/braceleft.s4N/uni2211.s1\ni=1\nj/parenleft.s1lk , /u1D716NN\n/parenleft.s1xi\n/parenright.s1/parenright.s1for each l k ∈ S\n/braceright.s4\n.\n3 https:// github. com/ marco tcr/ lime.\n2 https:// github. com/ slund berg/ shap.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 11 of 33 32\nAverage  Similar to the sum operation, here we obtain aggregated scores aver -\naging local impact scores, thus avoiding possible overshooting issues \narising when dealing with very frequent lemmas. Mathematically, we \ndefine J(lk ,/u1D716NN )= 1\nN ⋅ ∑N\ni=1 j�lk ,/u1D716NN\n�xi\n��\n . \nAbsolute average  Similarly to absolute sum, here we average absolute values of local \nimpact scores for better-managing lemmas with a skewed impact \nas well as tackling frequency issues. Global impact scores are here \ndefined as J(lk ,/u1D716NN )= 1\nN ⋅ ∑N\ni=1 �j�lk ,/u1D716NN\n�xi\n��� . \n Since the selection of the aggregation mechanism may influence the correlation between \ndifferent LPEs, in our experiments we analyse LPEs correlation over the same aggregation \nscheme. Moreover, we also analyse how aggregation impacts the impact scores correla-\ntion over the same LPE, highlighting how leveraging the absolute value of impact score is \nhighly similar to adopting its true value—see Sect. 4.3.2.\n3.4  Comparing explanations\nEach aggregated global explanation J depends on a corresponding label /u1D70Fm since LPEs pro-\nduce either a scalar impact value for a single /u1D70Fm or a vector of impact scores for each /u1D70Fm . \nTherefore, recalling Sect. 3.3, we can define the set of aggregated global scores depending \non the label they refer to as following:\nJ/u1D70Fm\n/parenleft.s1/u1D716NN, S/parenright.s1\n represents a distribution of impact scores over the set of lemmas—i.e., con-\ncepts—available in the samples set for a specific label. To compare the distributions of \nimpact scores extracted using two LPEs—i.e., J/u1D70Fm\n/parenleft.s1/u1D716NN, S/parenright.s1\n and J/u1D70Fm\n/parenleft.s1/u1D716/uni2032.var\nNN, S/parenright.s1\n—we use Pear-\nson correlation, which is defined as the ratio between the covariance of two variables and \nthe product of their standard deviations, and it measures their level of linear correlation. \nThe selected correlation metric is applied to the normalised impact scores. Indeed, dif-\nferent LPEs produce impact scores that may differ relevantly in terms of their magnitude. \nNormalising the impact scores, we map impact scores to a fixed interval, allowing for a \ndirect comparison of J/u1D70Fm\n over different /u1D716NN . Mathematically, we refer to the normalised \nglobal impact scores as ‖J/u1D70Fm\n‖ . Therefore, we define the correlation score between two sets \nof global impact scores for a single label as:\n(10)A/parenleft.s1/braceleft.s1/u1D716NN (xi) for each xi ∈ S/braceright.s1/parenright.s1=\n/braceleft.s4N/uni2211.s1\ni=1\n/uni007C.varj/parenleft.s1lk , /u1D716NN\n/parenleft.s1xi\n/parenright.s1/parenright.s1/uni007C.varfor each l k ∈ S\n/braceright.s4\n.\n(11)A/parenleft.s1/braceleft.s1/u1D716NN (xi) for each xi ∈ S/braceright.s1/parenright.s1=\n/braceleft.s4\n1\nN ⋅\nN/uni2211.s1\ni=1\nj/parenleft.s1lk , /u1D716NN\n/parenleft.s1xi\n/parenright.s1/parenright.s1for each l k ∈ S\n/braceright.s4\n.\n(12)A/parenleft.s1/braceleft.s1/u1D716NN (xi) for each xi ∈ S/braceright.s1/parenright.s1=\n/braceleft.s4\n1\nN ⋅\nN/uni2211.s1\ni=1\n/uni007C.varj/parenleft.s1lk , /u1D716NN\n/parenleft.s1xi\n/parenright.s1/parenright.s1/uni007C.varfor each l k ∈ S\n/braceright.s4\n.\n(13)J/u1D70Fm\n/parenleft.s1\n/u1D716NN , S\n/parenright.s1\n=\n/braceleft.s1\nJ\n/parenleft.s1\nlk, /u1D716NN\n/parenright.s1\n/uni007C.var/u1D70Fm for each lk ∈ S\n/braceright.s1\n.\n(14)\n/u1D70C\n�\n‖J/u1D70Fm\n�\n/u1D716NN , S\n�\n‖, ‖J/u1D70Fm\n�\n/u1D716�\nNN , S\n�\n‖\n�\n= /u1D70C\n�\n‖\n�\nJ\n�\nlk, /u1D716NN\n�\n�/u1D70Fm for each lk ∈ S\n�\n‖,\n‖�J�lk, /u1D716�\nNN\n��/u1D70Fm for each lk ∈ S�‖�\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 12 of 33\nwhere /u1D70C refers to the Pearson correlation used to compare couples of J/u1D70Fm\n/parenleft.s1/u1D716NN, S/parenright.s1\n . Through-\nout our analysis we experimented with similar correlation metrics, such as Spearman cor -\nrelation and simple vector distance—similarly to [50]—, obtaining similar results. There-\nfore, to avoid redundancy we here show only the Pearson correlation results. Throughout \nour experiments, we consider a simple min-max normalisation process, scaling the scores \nto the range [0, 1].\nAs we aim at obtaining a measure of similarity between LPEs applied over the same set \nof samples, we can average the correlation scores /u1D70C obtained for each label /u1D70Fm over the set \nof labels T  . Therefore, we mathematically define the correlation score of two LPEs, putting \ntogether Eqs. 13, 2 and 14 as:\nwhere M is the total number of labels, belonging to T .\n3.5  GELPE: global explanations from LPEs\nAlthough useful, local explanations are limited, as they do not highlight the general reasoning \nprinciple of the underlying model, but rather focus solely on relevant input components for a \nspecific prediction. Aiming at overcoming such limitations, we here present GELPE as the \nfirst—up to our knowledge—framework for extracting global explanations from LPEs. Rely-\ning on LPE outputs, GELPE allows for the adoption of reliable local extraction mecanisms, \nwhile extending their impact to the global reasoning process of the black-box model. Figure 1 \npresents an overview of GELPE’s working process.\nThe aggregated explanations /u1D716NN(S) obtained from a single LPE’s output are leveraged as \na starting point for building a transparent surrogate model of the original LLM. GELPE relies \non transparent-by-design models such as CART optimised over the LLM outputs, rather than \nthe dataset considered. As described in Eq. 3, during the optimisation process of the CART \nmodel, input sentences are converted into a binary format, expressing the presence or absence \nof relevant lemmas and their combinations. In order to convert a sentence xi into its binary \nformat, we consider the K most valuable lemmas for each class identified during the aggrega-\ntion process presented in Sect. 3.3. The K most valuable lemmas are the ones with the high-\nest aggregated impact scores over a set of sample sentences for a single LPE mechanism. To \navoid relying only on keywords, and accounting instead for more complex constructs, we also \nconsider the set of skipgrams built from the combination of the single K most valuable lem-\nmas. In this context, skipgrams define co-occurences of relevant lemmas over a span of lim-\nited tokens sequences [51]. With such a procedure we build a set of valuable lemmas and \nsequences L defined as:\nwhere L i represents the lemma in the ith position of the sorted lemmas list—in terms of \nrelevance—, and (L i, … , L j) represent the concatenation of two or more lemmas. Once the \nset of most relevant lemmas and corresponding sequences L are available, we can define \nthe binarized version of an input sentence as the binary vector that identify the presence or \nabsence of each lemma and sequence in the considered sentence. Mathematically, the bina-\nrisation function can be defined as the following:\n(15)C�/u1D716NN(S),/u1D716�\nNN(S)\n� = 1\nM ⋅\nM�\nm=1\n/u1D70C�‖J/u1D70Fm\n�/u1D716NN,S�‖,‖J/u1D70Fm\n�/u1D716�\nNN,S�‖�\n(16)L = {(L i), (L i, L j), (L i, L j, L k ), …∀ i,j, k ∈ K},\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 13 of 33 32\nwhere x i,j represent the components—i.e., tokens or lemmas—of the input sentence xi , \nskip(xi,j−n , … , xi,j) the corresponding skipgrams built from the last n input components, and \n1 represents the indicator function, being equal to 1 if the lemma/skipgram belongs to L \nand 0 otherwise. Finally, || represents the concatenation operation between vectors. As an \nexample, consider the input sentence the dog is an animal with four legs and the set of \nmost relevant lemmas extracted by a given LPE to be L ={ animal , face, legs} . Then the \ncorresponding binarised version of the input sentence is shown in Fig. 2, where the + sym-\nbol is used to identify the concatenation of two relevant lemmas inside a sentence—i.e., \nlemma1 + lemma2 can be interpreted as lemma1 followed by lemma2.\nThe binarised input is used to optimise the underlying CART model, from which it is \npossible to extract the equivalent logic program P—see Eq. 3. P is extracted by identify -\ning one rule for each leaf used in the CART model optimised over the LLM outputs. The \nobtained logic program P represents an explanation of the black-box LLM in the form \nof a set of sequential propositional rules containing lemmas, sequences of lemmas, and \nnegations thereof. Extracted rules are sequential, meaning that each propositional rule \napplies if and only if the previous ones were not valid. As GELPE relies on the CART \nmodel, the extracted rules can only identify the presence or absence of a specific set of \nkeywords and sequences, which represents a limitation of such approach. However, vary -\ning the value of K and the length and expressiveness of the skipgram construction process, \nthe GELPE extraction procedure can be tuned to consider sequences of lemmas as com-\nplex as it is needed to fit well the LLM reasoning process. To keep the complexity of the \nextraction process under control, throughout our experiments we consider relying at most \non (2,5)-skipgrams—i.e., building sequences of lemmas of length at most two which are \ncontained over the span of five input tokens. An example of the GELPE extracted knowl-\nedge, along with the analysis of its correctness is made available in Sect. 4.4.3.\n4  Experiments\nIn this section we present the setup and results of our experiments. More in detail, we first \nanalyse the set of datasets used in our experimental evaluation in Sect.  4.1, along with the \nmodel training details and its obtained performance in Sect. 4.2. We then focus on the com-\nparison between the available LPEs, showing the correlation between their explanations in \n(17)xbin = bin/u1D716NN (S)(xi)=1(x i,j ∈ L) /uni007C.var/uni007C.var1(skip(xi,j−n , … , xi,j)∈L )∀ j ∈ xi,\nFig. 2  Sentence binarization approach in GELPE.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 14 of 33\nSect. 4.3. Section 4.4 presents the knowledge extraction results, analysing the performance \nof the knowledge extractor model, along with the complexity of the extracted knowledge. \nFinally, we analyse the efficiency of the knowledge extraction model, showcasing the \nimprovements in terms of time and energy consumption over the LLM counterpart. The \nsource code of our framework and experiments is publicly available.4\n4.1  Datasets\nIn our experiments, we aim at analysing the correlation among different LPEs and the fea-\nsibility of global knowledge extraction from LLM over a large set of scenarios. Therefore, \nwe consider an heterogeneous set of datasets targetting text classification tasks, ranging \nfrom easy to complex setups. More in detail, we consider targetting the SMS [26] and \nYOUTUBE [27] spam classification datasets as easy setups, having two highly separable \nclasses. Here, each sample represents a text—either obtained from text messages or from \ncomment posted in the comments section of youtube videos—manually labeled as spam \nor legitimate (ham). Although available, the metadata information—such as the author’s \nname and publication date—is not used. As a slightly more complex setup, we consider \nthe TREC [52] dataset, containing 4,965 labeled questions. In this context, each sample \nrepresents a question belonging to one of six classes—i.e., Abbreviation, Entity, Descrip-\ntion, Human, Location, Numeric-value—to be semantically classified. Finally, as a com-\nplex setup we select the MFTC datasets as the target classification task. The MFTC dataset \nis composed of 35,108 tweets—sentences—, which can be considered as a collection of \ndifferent datasets. Each split of MFTC corresponds to a different context. Here, tweets cor-\nresponding to the dataset samples are collected following a certain event or target. As an \nexample, tweets belonging to the Black Lives Matter (BLM) split were collected during the \nperiod of Black Lives Matter protests in the US. The list of all MFTC subjects considered \nin our experiments is the following: (i) All Lives Matter (ALM), (ii) Black Lives Matter \n(BLM), (iii) Baltimore protests (BLT), (iv) 2016 presidential election (ELE), (v) MeToo \nmovement (MT), (vi) hurricane Sandy (SND). Each tweet in MFTC is labelled, follow -\ning the same moral theory, with one or more of the following 11 moral values: (i) care/\nharm, (ii) fairness/cheating, (iii) loyalty/betrayal, (iv) authority/subversion, (v) purity/deg-\nradation, (vi) non-moral. Ten of the 11 available moral values are obtained as a moral con-\ncept and its opposite expression—e.g., fairness refers to the act of supporting fairness and \nequality, while cheating refers to the act of refraining from exploiting others. Given moral-\nity subjectivity, each tweet is labelled by multiple annotators, and the final moral labels are \nobtained via majority voting.\nAs the size of each dataset represents a relevant component to take into account, Table 2 \nreports the number of sentences belonging to each dataset. Throughout our experiments \nwe use 70% of the samples belonging to the dataset as the training set, in which LLMs are \nTable 2  Size of the considered datasets.\nSMS YOUTUBE TREC ALM BLM BLT ELE MT SND\nNumber of samples 5574 2403 4965 4424 5257 5593 5358 4891 4591\n4 https:// github. com/ AndAg io/ SKE_ NLP.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 15 of 33 32\ntrained, and both local and global explanations are fitted. The remaining 30% of samples \nis kept for testing the LLM performance as well as the quality of both local and global \nexplanations.\n4.2  Model training\nThe SMS, YOUTUBE, and TREC datasets represent standard multi-class single-label \nclassification tasks. Therefore, we tackle the classification task over those datasets using \na standard cross entropy loss [53]. Meanwhile, tackling MFTC we follow state-of-the-art \napproaches for dealing with morality classification task [54, 55]. Thus, we treat the moral-\nity classification problem as a multi-class multi-label classification task, using a binary \ncross entropy loss [53]. Differently from recent approaches, we here do not rely on the \nsequential training paradigm for the MFTC datasets, but rather train each model solely \non the MFTC split at hand. Indeed, in our experiments, we do not aim at obtaining strong \ntransferability between domains, but rather we focus on analysing LPEs behaviour.\nFor all datasets we leverage BERT as the LLM to be optimised [11], and define one \nNN model for each dataset, optimising its parameters over the 70% of samples, leaving \nthe remaining 30% for testing purposes. We leverage the pre-trained bert-base-uncased \nmodel—available in the Hugging Face python library 5—as the starting point of our train-\ning process. Each model is trained using the standard Stochastic Gradient Descent (SGD) \noptimization procedure for 3 epochs, a learning rate of 5 × 10−5 , a batch size of 16 and a \nmaximum sequence length of 64. We keep track of the macro F1-score for each model \nto identify its performance over the test samples. Table  3 shows the performance of the \ntrained BERT model.\n4.3  Local post‑hoc explainers comparison\nWe analyse the extent to which different LPEs are aligned in their process of identifying \nimpactful concepts for the underlying NN model. With this aim, we train a BERT model \nover a specific dataset (following the approach described in Sect.  4.2) and compute the \npairwise correlation C/parenleft.s1/u1D716NN(S), /u1D716�\nNN(S)\n/parenright.s1 (as described in Sect. 3) for each pair of LPEs in the \nselected set. To avoid issues caused by model overfitting over the training set, which would \nrender explanations unreliable, we apply each /u1D716NN over the test set of the selected dataset.\nTable 3  BERT performance over considered datasets.\nSMS YOUTUBE TREC ALM BLM BLT ELE MT SND\nF1 score 98.71% 95.81% 97.18% 63.04% 82.59% 64.51% 63.14% 52.16% 56.85%\n5 https:// github. com/ huggi ngface.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 16 of 33\n4.3.1  Local post‑hoc explainers disagreement\nUsing the pairwise correlation values we construct the correlation matrices shown in \nFigs. 3 and 4, which highlight how there exist a very weak correlation score between most \nLPEs over different datasets. Here, it is interesting to notice how few specific couples or \nclusters of LPEs exist which highly correlate with each other. For example, GS, GI, and \nLRP show moderate-to-high correlation score, mainly due to their reliance on computing \nthe gradient of the prediction to identify impactful concepts. However, this is not the case \nfor all LPE couples relying on similar approaches. For example, GI and gradient integra-\ntion—HESS in the matrices—show little to no correlation, although they both are gradient-\nbased approach for producing local explanations. Similarly, SHAP and LIME show no cor-\nrelation even if they both rely on input perturbation and are considered the state-of-the-art.\nFigures 3 and 4 highlight how the vast majority of LPE pairs show very-small-to-no \ncorrelation at all, exposing how the selected approaches actually disagree. Interestingly \nenough, disagreement between LPEs holds true for every dataset studied in our analyses, \nno matter the complexity or simplicity of the learning task and the samples considered. \nFig. 3  C/parenleft.s1/u1D716NN(S), /u1D716�\nNN(S)\n/parenright.s1\n using average aggregation as A over the SMS (left) and YOUTUBE (right) data-\nset.\nFig. 4  C/parenleft.s1/u1D716NN(S), /u1D716�\nNN(S)\n/parenright.s1\n using average aggregation as A over the ALM (left) and BLM (right) dataset.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 17 of 33 32\nThis finding represents a fundamental result of our study, as it demonstrates how no \naccordance exists between LPEs even when they are applied to the same model and data-\nset, even on very simple classification tasks such as the one represented by the SMS data-\nset. The reason behind the large discrepancies among LPE might be various, but mostly \nbear down to the following:\n• Few of the LPEs considered in the literature do not represent reliable solutions for iden-\ntifying the reasoning principles of LLMs.\n• Each of the uncorrelated LPEs highlights a different set or subset of reasoning princi-\nples of the underlying model.\nTherefore, our results show how complex it is to identify a set of fair and reliable metrics to \nspot the best LPE or even reliable LPEs, as they seem to gather uncorrelated explanations. \nSimilar results to the ones shown in Figs. 3 and 4 are obtained for all datasets and are made \navailable at https:// github. com/ AndAg io/ GELPE.\nFig. 5  C/parenleft.s1/u1D716NN(S), /u1D716�\nNN(S)\n/parenright.s1\n using different aggregations over the ALM dataset.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 18 of 33\n4.3.2  Aggregation affects correlation\nSince our LPE correlation metric is dependent on A , we here analyse how the selection of \ndifferent aggregation strategies impacts the correlation between LPEs. To understand the \nimpact of A on C , we plot the correlation matrices for a single dataset, varying the aggrega-\ntion approach, thus obtaining the four correlation matrices shown in Fig. 5.\nFrom Figureds 5c, d one could notice the strong correlation between different LPEs. \nThis seems to be in contrast with the results found in Sect. 4.3. However, the reason behind \nthe strong correlation achieved when relying on summation aggregation is not caused by \nthe actual correlation between explanations, but rather on the susceptibility of summation \nto tokens frequency. Indeed, since the summation aggregation approaches do not take into \naccount the occurrence frequency of lemmas in S , they tend to overestimate the relevance \nof popular concepts. Intuitively, using this aggregations, a rather impactless lemma appear-\ning 5000 times would obtain a global impact higher than a very impactful lemma appearing \nonly 10 times. These results highlight the importance of relying on average based aggrega-\ntion approaches when considering to construct global explanations from the LPE outputs.\nFigure  5 also points out how leveraging the absolute value of LPEs incurs in higher \ncorrelation scores. The reason behind this is to be found in the impact scores distributions. \nWhile true local impact scores are distributed over the set of real numbers ℝ , computing \nthe absolute value of local impacts j shifts their distribution to ℝ+ , shrinking possible dif-\nferences between positive and negative scores. Moreover, LPE outputs rely much more \nheavily on scoring positive contributions using positive impact scores, and typically give \nless focus to negative impact scores. Therefore, the output of LPEs is generally unbalanced \ntowards positive impact scores, making negative impact scores mostly negligible.\n4.3.3  LPEs visualization examples\nThe results obtained over various LPEs when considering several input sentences identify \na large discrepancy between the available LPE approaches. To better visualize the quarrel \nbetween LPEs, we here consider to visualize the output of LPE explanations over few of \nthe sentences belonging to the considered datasets in Figs.  6 and reffig:lpespsalmspssingle\nFig. 6  Example of LPEs influence scores over the sentence achieving the lowest (left) and highest (right) \ncorrelation of LPEs in the SMS dataset.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 19 of 33 32\nspssentence. More in detail, we plot the LPEs relevance scores for each token over the sen-\ntences of the dataset used in our experiments. Generally speaking, higher scores identify \nthe most relevant tokens for the LLM prediction, while low scores identify non relevant \ntokens. Negative scores are assigned to the tokens that negatively influence the prediction \nfor a specific class, thus identifying the tokens that should drift the prediction towards a \ndifferent class.\nFigure 6 shows the LPEs scores over two sentences of the SMS dataset. The left-side \nplot is obtained for a sentence where LPEs are far from being correlated, thus highlight-\ning the quarrel between LPEs and confirming the findings of Sect.  4.3.1. The difference \nin LPEs influence scores is evident across most tokens, with each LPE considering as the \nmost relevant tokens several different candidates—e.g., SHAP focuses on anti, GS focuses \non invest, LRP focuses on in, etc. On the other hand, the right-side plot is obtained for \na sentence where LPEs are slightly correlated, thus showing somewhat an agreement \nbetween most LPEs. However, even considering sentences where LPEs generally agree, \nit is possible to notice how few approaches are far from being perfectly adherent to the \nmajority of LPEs. For example, SHAP and LIME assign an almost zero influence score \nto all tokens, while other LPEs tend to produce non-negligible scores. Similar results are \nobtained for the ALM dataset and shown in Fig. 7. However, for the ALM dataset, the disa-\ngreement among LPEs is evident even when selecting the sentence achieving the highest \nLPEs correlation (right-side plot). Similar results to the ones shown in Figs.  6 and 7 are \nobtained for all sentences in each dataset considered, and made available at https:// github. \ncom/ AndAg io/ GELPE.\n4.4  Knowledge extraction\nWe here analyse if and to what extent it is possible to extract a knowledge base repre-\nsenting the trained LLM from each LPE, and how much these are aligned in their pro-\ncess of explaining the underlying NN model. With this aim, we rely on the GELPE global \nexplainer construction process presented in Sect.  3.5, extracting a set of rules representing \nthe LLM decision process for each dataset at hand. As the building process is dependent on \nthe number of most impactful lemmas, we consider varying the hyperparameter K to select \nthe top-K relevant lemmas for each class. After the relevant lemmas are selected from a \nFig. 7  Example of LPEs influence scores over the sentence achieving the lowest (left) and highest (right) \ncorrelation of LPEs in the ALM dataset.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 20 of 33\ngiven LPE, we construct the skipgrams of relevant lemmas as the set of skipgrams occur -\nring in the training set that are composed from relevant lemmas only. Skipgrams are con-\nsidered to extend the capabilities of the extraction process to consider sequences of relevant \nconcepts rather than blindly focusing only on single tokens. Once the relevant lemmas and \nskipgrams are available, we consider converting the samples of the training set into binary \nvectors describing the presence or absence of each lemma and skipgram. We optimise the \nCART model on the binary vectors representing the training samples and extract the cor -\nresponding knowledge from the tree as a set of ordered propositional rules. The extracted \nrules are sequential, meaning that one rule applies if and only if the previous rules were \nnot successful in identifying the relevant prediction. To avoid incurring in an unbearable \nnumber of propositional clauses—that would hinder the utility of the knowledge extraction \nprocess—we limit the depth of the CART model to be:\nwhere Λ represents the number of total relevant lemmas and skipgrams identified from \nthe LPE, /uni007C.varY/uni007C.var represents the number of classes of the classification task at hand, and /u1D707 rep-\nresents an hyperparameter that we set to /u1D707= 5 empirically. Throughout the remainder of \n(18)depth = /u1D707⋅ Λ\nK ∗ /uni007C.varY/uni007C.var,\nTable 4  Fidelity of the extracted \nknowledge w.r.t. to the original \nBERT model over the SMS \ndataset.\n† Identifies the best LPE over a single K value, while the bold row(s) \nidentify the overall best LPE\nLPEs K\n50 (%) 100 (%) 150 (%) 200 (%) 250 (%)\nGI 87.00 87.60 90.20† 91.80† 91.60†\nGS 87.40† 87.80† 89.80 90.40 91.60†\nLAT 87.40† 87.40 89.00 89.60 91.00\nLRP 86.60 86.40 86.60 87.80 90.80\nSHAP 86.40 86.60 86.60 86.40 86.40\nHESS 86.20 86.40 86.80 86.80 86.40\nLIME 86.20 86.20 86.20 86.60 86.80\nTable 5  Fidelity of the \nextracted knowledge w.r.t. to the \noriginal BERT model over the \nYOUTUBE dataset.\n† Identifies the best LPE over a single K value, while the bold row(s) \nidentify the overall best LPE\nLPEs K\n50 (%) 100 (%) 150 (%) 200 (%) 250 (%)\nGI 69.20 72.40 72.40 76.00 76.80\nGS 69.20 72.40 72.40 78.00 76.40\nLAT 66.00 64.40 70.80 80.00 84.40\nLRP 65.20 65.20 68.80 70.00 70.00\nSHAP 43.20 75.20 80.80 80.80 80.40\nHESS 82.40 87.60 86.40 88.80 87.20\nLIME 88.00† 92.00† 94.00† 93.20† 92.80†\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 21 of 33 32\nthis paper, we consider leveraging the average operation as the aggregation function A , as \nit represents the least biased aggregation process. However, we also experiment with other \naggregation functions, such as sum, absolute sum, and absolute average, obtaining similar \nTable 6  Fidelity of the extracted \nknowledge w.r.t. to the original \nBERT model over the BLT \ndataset.\n† Identifies the best LPE over a single K value, while the bold row(s) \nidentify the overall best LPE\nLPEs K\n50 (%) 100 (%) 150 (%) 200 (%) 250 (%)\nGI 92.64 92.70 93.18 93.12 92.76\nGS 93.60 92.28 93.18 93.24 92.82\nLAT 90.19 91.74 92.28 92.34 92.46\nLRP 92.28 93.12 93.00 93.48 92.88\nSHAP 95.69 94.14 94.14 94.14 94.14\nHESS 93.72 93.84 93.48 93.48 93.60\nLIME 95.27† 95.27† 95.09† 95.09† 95.09†\nTable 7  Fidelity of the extracted \nknowledge w.r.t. to the original \nBERT model over the ELE \ndataset.\n†  identifies the best LPE over a single K value, while the bold row(s) \nidentify the overall best LPE\nLPEs K\n50 (%) 100 (%) 150 (%) 200 (%) 250 (%)\nGI 68.51 72.00 74.67 75.92 76.23\nGS 68.95 73.93 74.74† 74.67 76.79†\nLAT 58.93 61.36 66.77 67.45 69.14\nLRP 72.81 74.74† 75.36 75.48 75.79\nSHAP 67.64 68.70 69.51 70.50 70.50\nHESS 68.33 73.80 74.05 74.11 74.11\nLIME 73.61† 73.93 74.49 76.60† 76.73\nTable 8  Fidelity of the extracted \nknowledge w.r.t. to the original \nBERT model over the SND \ndataset.\n† Identifies the best LPE over a single K value, while the bold row(s) \nidentify the overall best LPE\nLPEs K\n50 (%) 100 (%) 150 (%) 200 (%) 250 (%)\nGI 45.39 58.39 60.20 59.84 61.73\nGS 46.48 57.59 59.84 61.00† 62.60†\nLAT 38.63 45.53 49.82 49.89 57.30\nLRP 40.02 49.67 59.98 59.62 61.00\nSHAP 57.01 57.23 57.23 57.23 57.23\nHESS 60.49 60.13† 58.75 58.53 58.61\nLIME 61.15† 60.06 60.28† 60.20 60.13\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 22 of 33\nresults. Therefore, in order to avoid redundancy we here show only the average aggregation \nresults.\n4.4.1  Knowledge fidelity\nTo asses the performance of the proposed knowledge extraction process from LPEs, we \nmeasure the fidelity of the predictions obtained using the propositional rules against the \ncorresponding LLM predictions. The fidelity metric measures the percentage of instances \nin which the propositional rules predictions and model predictions are equivalent, thus \nmeasuring the accuracy of the knowledge extraction process. Since, GELPE relies on the \noutput of a single LPE mechanism to produce the logic program equivalent to the LLM \nat hand, we compare the fidelity performance of GELPE over all the LPEs presented in \nSect. 3.2. Tables 4 and 5 present the fidelity of the GELPE extraction process over the SMS \nand YOUTUBE datasets. In those simple scenarios, the proposed approach extracts a set of \naccurate rules, representing with high fidelity the decision process of the underlying LLM. \nUsing GELPE, we enable the extraction of simple and easy to understand rules from the \ncomplex black-box model.\nOver more complex datasets, the performance of the extracted knowledge using GELPE \nvaries depending on the dataset at hand. Table 6 shows the fidelity of GELPE over the BLT \ndataset, where the explanation model achieves up to 95.09% fidelity. Meanwhile, Tables  7 \nand 8 presents the fidelity results over the ELE and SND datasets respectively, where the \nproposed GELPE extraction seems to struggle to achieve high fidelity values. This is due \nto the underlying complexity of the dataset at hand. For some tasks—e.g. YOUTUBE, \nBLT—, considering the most relevant lemmas and their skipgram combinations is suffi-\ncient, while others—e.g. ELE, SND—require a more complex understanding of the inner \nsentence constructs.\nAs expected, increasing the number of relevant lemmas K considered to optimise \nGELPE results in higher fidelity, as the underlying CART model takes into account a \nbroader set of meaningful features. However, increasing K over a certain threshold results \nin an unbearable rules complexity and in smaller fidelity gains. The increment on rule \ncomplexity also hiders the understandability of the extracted explanation, representing a \nfundamental concept to take into account. This phenomenon is clearly shown in Tables  7 \nand 8, where the fidelity grows up to 20% when K ranges from 50 to 250.\nInterestingly, the disagreement between different LPEs seems to affect also the per -\nformance of the obtained global explainer model. Fidelity results highlight that GELPE \nexplanations obtained from highly correlated LPEs such as GI and GS achieve comparable \nperformance level. Meanwhile, propositional rules obtained from uncorrelated LPEs result \nin different fidelity level. While expected, such a behaviour represents a useful finding as it \nallows for the identification of more reliable LPEs, as the ones that results in a higher level \nof fidelity—e.g., LIME in most scenarios.\n4.4.2  Knowledge complexity\nThe ideal extraction process is required to output a set of sequential propositional rules \nthat is as faithful as possible w.r.t. the underlying LLM. However, the dimensionality of \nthe extracted program should be kept small to limit the complexity burden of the anal-\nysis process. An overly complex knowledge base would not be useful for analysing the \nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 23 of 33 32\nTable 9  Complexity of the \nextracted knowledge over the \nYOUTUBE dataset.\nL represents the length of the obtained explanation—i.e., the number \nof clauses—, while C represents the cumbersomeness—i.e., the aver -\nage number of atoms in each clause. The bold row(s) identify the over-\nall simplest LPE\nLPEs K\n50 100 150 200 250\nGI L = 30 L = 41 L = 40 L = 40 L = 66\nC = 6.73 C = 7.71 C = 7.65 C = 7.05 C = 10.38\nGS L = 30 L = 41 L = 42 L = 37 L = 73\nC = 6.63 C = 7.71 C = 7.79 C = 7.51 C = 10.59\nLAT L = 20 L = 43 L = 116 L = 75 L = 64\nC = 5.65 C = 6.42 C = 10.28 C = 8.84 C = 11.23\nLRP L = 37 L = 46 L = 36 L = 32 L = 48\nC = 6.51 C = 7.15 C = 7.17 C = 7.09 C = 7.81\nSHAP L=1 4 L=2 5 L=3 4 L=3 6 L=3 3\nC = 5.21 C = 7.04 C = 7.62 C = 7.78 C = 7.61\nHESS L = 48 L = 53 L = 55 L = 39 L = 52\nC = 9.67 C = 10.36 C = 12.05 C = 11.67 C = 11.88\nLIME L = 32 L = 56 L = 57 L = 60 L = 68\nC = 6.72 C = 9.29 C = 9.42 C = 11.30 C = 13.01\nTable 10  Complexity of the \nextracted knowledge over the \nELE dataset.\nL represents the length of the obtained explanation—i.e., the number \nof clauses—, while C represents the cumbersomeness—i.e., the aver -\nage number of atoms in each clause. The bold row(s) identify the over-\nall simplest LPE\nK LPEs\n50 100 150 200 250\nGI L = 430 L = 363 L = 353 L = 273 L = 296\nC = 19.78 C = 19.68 C = 18.21 C = 15.76 C = 15.29\nGS L = 422 L = 335 L = 350 L = 269 L = 369\nC = 19.58 C = 19.54 C = 18.27 C = 15.61 C = 17.94\nLAT L = 639 L = 487 L = 373 L = 379 L = 360\nC = 20.52 C = 19.93 C = 19.11 C = 20.00 C = 20.47\nLRP L = 390 L = 433 L = 391 L = 375 L = 364\nC = 19.85 C = 22.08 C = 18.45 C = 18.19 C = 17.93\nSHAP L=1 6 L=1 5 L=1 6 L=1 6 L=1 6\nC = 4.06 C = 3.93 C = 4.06 C = 4.06 C = 4.06\nHESS L = 17 L = 64 L = 71 L = 71 L = 72\nC = 4.12 C = 7.84 C = 8.04 C = 8.03 C = 8.08\nLIME L = 64 L = 68 L = 71 L = 130 L = 131\nC = 7.75 C = 7.94 C = 8.06 C = 10.72 C = 10.76\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 24 of 33\ninner working principle of the explained LLM, as it would be mostly impossible to be pro-\ncessed by a human interpreter. To assess the complexity of the extracted knowledge, we \nconsider tracking the length of the logic program and its cumbersomeness. In this context, \nthe length L represents the number of clauses in the obtained explanation, while the cum-\nbersomeness C represents the average number of atoms in each clause. L and C represent \ntwo fundamental parameters for describing the complexity of the extracted logic program. \nLengthier programs are more complex to read and may result in the reader getting lost. On \nthe other hand, a higher cumbersomeness translates directly into longer rules, which are \nby default more complex to understand, as human users are generally more susceptible to \ncomplex multi-variable reasoning. Moreover, longer rules are generally more specific, as \nthey require linking multiple input variables—and possibly their interactions—to a specific \noutput label. Therefore, when long rules are extracted it possibly means that the LLM sig-\nnals a specific behavior over a specific input. This phenomenon can translate directly into \nthe identification of bias issues, overfitting problems and much more.\nFor each dataset considered we keep track of L and C and analyse their variability over \neach LPE and K value. Tables  9 and 10 show the complexity of the GELPE output over \nthe YOUTUBE and ELE dataset respectively. The results highlight the relevant difference \nin terms of required complexity to extract reliable explanations when dealing with simple \nor complex classification tasks. Both L and C are kept small for each LPE and K combina-\ntion over the YOUTUBE dataset, while still being able to reach high fidelity (see Table  5). \nMeanwhile, the ELE moral classification task requires to consider higher values of L and C \nin order to achieve a satisfactory level of fidelity (see Table 7).\nTable 9 also highlights a dependency between the complexity of the extracted explana-\ntions and the parameter K . In the vast majority of cases, the higher K produces a more \ncomplex global explanation program, usually characterized by a higher number of clauses \nL and a larger number of atoms for each clause C. This is expected, since a higher value \nFig. 8  Logic program P obtained from the GELPE extraction process when leveraging LIME as LPE and \nK = 50 on the YOUTUBE dataset.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 25 of 33 32\nof K identifies a broader set of relevant lemmas considered during the optimization of the \nCART explainer, thus increasing the number of features available to construct proposi-\ntional clauses. However, it is interesting to notice how the almost-linear dependency on K \naffects more C than L, since L can be bounded during the CART optimization process via \npruning. The increased complexity of the obtained explanation represents a fundamental \naspect to take into account when considering leveraging GELPE, as we need for the expla-\nnations to be bounded in complexity for them to be human-readable. The limitation of the \nCART depth—see Eq.  18—represents an helping tool from this perspective, as it allows \nto keep the complexity of the explainer under control in complex setup, such as the ELE \ndataset. This phenomenon can be seen in Table  10, where the complexity of the extracted \nexplanations remains stable over K . However, depth limitation is not drawback free, as it \nhinders the achievement of high fidelity values.\n4.4.3  Knowledge visualisation\nWe visualise the logic programs obtained from the knowledge extraction process to \nanalyse their correctness and understandability. Figure  8 shows the logic program P \nobtained from the GELPE extraction process when leveraging LIME as LPE and K = 50 \non the YOUTUBE dataset. The extracted knowledge is characterised by a manageable \ncomplexity, having a small number of relatively short clauses. In this context, the sum-\nmation symbol + is used to identify the concatenation of two relevant lemmas inside a \nsentence—lemma1 + lemma2 can be interpreted as lemma1 followed by lemma2. More-\nover, we remind that the extracted rules are sequential, meaning that each propositional \nrule applies if an only if the previous rules did not. For example, in Fig.  8 the last rule, \nspecifing that the message is spam, is valid only if all the previous 31 rules did not \nmatch a class output. Interestingly, the extracted knowledge also shows some relevant \nproperties, such as the identification of spam comments as those containing certain \nFig. 9  Logic program P obtained from the GELPE extraction process when leveraging SHAP as LPE and \nK = 100 on the BLM dataset.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 26 of 33\nhyperlinks (org  lemma), subscription related lemmas (sub and subscribe), as well as \ngrammatical errors (suscribe rather than subscribe and withing rather than within).\nFigure  9 shows the extracted knowledge when GELPE is used with SHAP and \nK = 100 over the BLM dataset. Here, it is also possible to notice relevant concepts \nbeing extracted from the LLM decision process. For example, the proposed extraction \nprocess allows to identify that the combination of keywords obey and rape result in \nthe text being considered as harmful, as well as the keyword murder. Meanwhile, the \nsequence standing + injustice along with the justice keyword identify that the sentiment \nis fairness. Finally, since the extracted rules are sequential, the loyalty fact at the end \nof the program serves as the default prediction whenever none of the extracted rules \napplies. These results highlight the goodness of the proposed GELPE framework than \nenables the extraction of meaningful logic rules from the LLM reasoning principle with \nhigh fidelity.\n4.4.4  Resource effeciency\nThe proposed GELPE framework allows for the extraction of sequential propositional rules \nfrom LLM starting from LPEs outputs. In an ideal scenario, the logic program obtained \nas a result of the GELPE process contains a handful of simple—i.e., short—clauses. The \nexecution of such simple program—surrogate of the original LLM model—requires few \ncomputational power, as it does not rely on complex operations such as convolutions that \nrequire GPUs or hardware-specific solutions. However, the complexity of the GELPE \noutput can grow quickly depending on the set of considered lemmas and skipgrams, thus \nhindering its efficiency. Therefore, it is fundamental to assess the ability of the proposed \nGELPE framework to produce a resource-friendly surrogate model of the original LLM. \nTo this end, we consider measuring the time and energy efficiency of the original LLM \nmodel against few of the logic programs obtained using GELPE. More in detail, we con-\nsider running the original BERT model both in a GPU enabled scenario—using a Tesla \nV100S-PCIE with 32GB of RAM—and a CPU only scenario—using an Intel(R) Xeon(R) \nGold 6226R CPU @ 2.90GHz. We rely on the pyJoules 6 library for measuring the energy \nconsumption and latency of both LLM and logic program executions. pyJoules is a soft-\nware toolkit relying on (i) the Intel “Running Average Power Limit” (RAPL) 7 technology \nTable 11  Resource efficiency comparison of BERT against GELPE for each dataset.\nModel\nDataset SMS YOUTUBET RECA LM BLMB LT ELEM TS ND\nBERTGPU\nt = 0.017s t = 0.009s t = 0.008s t = 0.006s t = 0.006s t = 0.006s t = 0.006s t = 0.007s t = 0.006s\nE = 2.841J E = 2.350J E = 0.987J E = 1.181J E = 1.209J E = 1.481J E = 1.196J E = 1.961J E = 1.148J\nBERTCP U\nt = 0.047s t = 0.066s t = 0.023s t = 0.027s t = 0.028s t = 0.029s t = 0.026s t = 0.049s t = 0.026s\nE = 5.008J E = 7.893J E = 2.421J E = 2.940J E = 3.037J E = 3.141J E = 2.906J E = 5.576J E = 2.719J\nSHAP50\nt = 0.009s t = 0.004s t = 0.004s t = 0.008s t = 0.011s t = 0.005s t = 0.006s t = 0.008s t = 0.008s\nE = 0.574J E = 0.223J E = 0.269J E = 0.492J E = 0.595J E = 0.307J E = 0.383J E = 0.456J E = 0.490J\nLIME50\nt = 0.004s t = 0.004s t = 0.010s t = 0.021s t = 0.026s t = 0.005s t = 0.015s t = 0.020s t = 0.013s\nE = 0.208J E = 0.260J E = 0.592J E = 1.189J E = 1.455J E = 0.283J E = 0.891J E = 1.121J E = 0.777J\nSHAP250\nt = 0.010s t = 0.012s t = 0.019s t = 0.032s t = 0.035s t = 0.018s t = 0.025s t = 0.026s t = 0.034s\nE = 0.556J E = 0.694J E = 1.115J E = 1.736J E = 1.968J E = 1.015J E = 1.403J E = 1.432J E = 1.859J\nLIME250\nt = 0.016s t = 0.034s t = 0.084s t = 0.144s t = 0.235s t = 0.024s t = 0.087s t = 0.106s t = 0.078s\nE = 0.866J E = 1.789J E = 4.696J E = 7.990J E = 13.047J E = 1.388J E = 4.808J E = 5.797J E = 4.364J\nFor each dataset, we highlight in blue the most energy efficient model, in brown the least energy efficient \none, in green the quickest model and in red the slowest one\n6 https:// github. com/ power api- ng/ pyJou les.\n7 https:// power api. org/ refer ence/ formu las/ rapl/.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 27 of 33 32\nto estimate power consumption of the CPU, RAM and integrated GPU devices; and on \n(ii) the Nvidia “Nvidia Management Library” 8 technology to measure energy consump-\ntion of Nvidia GPU devices. Therefore, pyJoules represents a reliable solution to measure \nthe energy footprint of a host machine during the execution of a piece of Python code. \nWe consider comparing the BERT efficiency performance against the most faithful logic \nprogram—i.e., the one obtained with LIME as LPE—and against the simplest one—i.e., \nthe one obtained with SHAP as LPE. For each LPE, we consider two setups, having the \nlowest and highest value of K—i.e., K = 50 and K = 250 , respectively. The logic programs \nobtained from GELPE from each LPE are run using only the CPU device. We keep track of \nthe average time t  required to infer the prediction over a single sample and the correspond-\ning average energy consumed E . Table 11 shows the obtained results over all datasets.\nThe obtained results highlight how over simple setups such as SMS and YOUTUBE, \nthe surrogate model obtained using GELPE always outperforms the BERT counterpart. \nThis is due to the small task complexity, enabling the proposed framework to extract a \nsmall set of simple clauses to mimic the model behaviour. Indeed, the efficiency of the \nlogic program obtained is proportional to the complexity of the clauses to be analysed to \nachieve a prediction. Meanwhile, over more complex setups, such as the ELE dataset, in \nwhich GELPE outputs a large set of long clauses, it is possible to outperform the BERT \ncounterpart only when considering a small value of K . However, noticeably it is always \npossible to find a surrogate logic model obtained via GELPE representing a more efficient \nsolution than running the LLM model over the CPU. These results highlight the advan-\ntage of leveraging a simple rule-based approach over sub-symbolic models when hardware \nacceleration is not available. As such, the proposed model represents a feasible solution for \nthose scenarios where the deployment setup is composed of resource-constrained devices, \nsuch as embedded devices and micro-controllers. In this scenarios, running the original \nLLM would not be acceptable, due to latency and memory issues, while GELPE’s output \nresults in a resource efficient transparent program that is easily deployable. Therefore, the \nobtained results show that the GELPE surrogate model does not represent just an explain-\nable and transparent twin of the LLM original model, but also an efficient one.\n5  Discussion and limitations\nFidelity vs. efficiency trade-off The set of experiments proposed in Sect.  4 highlights how \nit is possible to identify a relevant logic program surrogate of the original LLM achieving \nhigh fidelity and efficiency for some scenarios. However, generally speaking there exists \nan intrinsic trade-off between the achievable fidelity of the surrogate logic program and \nits resource-efficiency improvements. Indeed, Table  11 highlights how resource efficiency \ngains are usually achievable whenever small logic programs are enforced using a small set \nof relevant lemmas—i.e., small K values. However, these small programs do not attain the \nbest achievable fidelity. Consider for example the YOUTUBE dataset, where GELPE rely-\ning on LIME with K = 50 achieves 88% fidelity, against the best fidelity of 94% achieved \nwith K = 150 . On the other hand, logic programs extracted using large set of relevant lem-\nmas—e.g., K = 250—usually achieve higher fidelity, while being less effective in reducing \nthe resource consumption. Therefore, it is possible to identify the fidelity vs. efficiency \n8 https:// devel oper. nvidia. com/ nvidia- manag ement- libra ry- nvml.\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 28 of 33\ntrade-off as one of the limitations of the proposed approach. However, while this trade-\noff exists, it is fundamental to note that it is relevant only whenever hardware accelera-\ntion—e.g., using GPUs—is available. Indeed, even the largest logic programs—which are \nexpected to be the most faithful—extracted with GELPE performs similarly—from the \nresource-efficiency perspective—to the LLM at hand whenever it runs on CPU only (see \nTable  11). Moreover, the application of knowledge extraction mechanisms is generally \nconsidered in those scenarios where model opacity is a no-go. Therefore, we consider the \ntrade-off between the achievable fidelity and the resource-efficiency to not apply in those \nscenarios where available hardware is limited or whenever transparency represents the \nmost important feature, thus rendering the trade-off less relevant.\nBERT and other LLMs Throughout our investigation we consider BERT as the tar -\nget LLM architecture. Indeed, BERT represent the first large NN model—comprising 340 \nmillions weights—which targets NLP and that is trained on large corpus of data collected \nfrom the web, namely the BooksCorpus dataset and a dump of the English Wikipedia of \nthe time. We rely on BERT as it allows for the quick implementation of all LPE approaches \navailable in the state-of-the-art. Indeed, few of these approaches require access to the inner \nmechanisms building the NN model to produce their explanations, thus being not applica-\nble to closed source models such as the GPT family. The full focus on BERT represents a \nlimitation of the proposed work, as the behaviour of BERT may differ significantly from \nother LLMs. Therefore, we consider the analysis of the application of our methodology to \nseveral different LLMs—wherever possible—as a future extension of this work. Moreo-\nver, we note that larger models—such as GPT or Llama—might exhibit some emergent \nproperties not appearing in the adopted BERT model [56]. The emergent properties may \nsomehow cause different results to be achieved employing the same methodology proposed \nin this paper, thus requiring further investigation. Indeed, emergent properties clash with \nmodel interpretability, thus making larger LLMs even more complex to analyse and inspect \nusing available LPEs. Therefore, it is reasonable to expect an even larger lack of correla-\ntion amongst available LPEs over larger LLMs caused by the inherent fuzzy nature of their \nemergent properties which is difficult to analyse from a single or a few examples.\nOn the LLM reasoning principles From the very first proposal of LLMs, the research \ncommunity has largely explored and speculated on their ability to reason over complex \nconcepts. However, the definition of the LLMs reasoning capabilities and their limitations \nrepresents an open research question in the literature. Indeed, there is no definitive proof on \nthe extent to which LLMs can process complex concepts incorporating human-like logical \nreasoning behaviour. Therefore, we here feel the need to stress that in this paper, whenever \nwe refer to the reasoning principles of LLMs we consider the process by which the model \nelaborates the textual information given, without assuming any human-like reasoning capa-\nbility from the LLM. Accordingly, the explanations extracted using GELPE mimic the \ninformation elaboration process of the LLM, rather than conjecturing the LLM’s logical \nreasoning capabilities. Therefore, the reasoning process carried out in the logic program \nmay be profoundly different to the reasoning capabilities of LLMs and rather represent the \nlogic grounding of how information is elaborated sub-symbolically by the LLM.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 29 of 33 32\n6  Conclusions and future work\nAs intelligent agents are going to increasingly rely on LLMs for smooth interaction with \nhumans and other agents, a fundamental issue for intelligent MASs is to open the LLM \nblack-boxes, enabling explanation of their inner reasoning principles. However, xAI tech-\nniques for NLP still suffer several issues, linked with the heterogeneity of available local \nexplanation techniques and the lack of robust global explanation processes. Inspired by \nthese limitations, we propose a novel approach for enabling a fair comparison among state-\nof-the-art local post-hoc explanation mechanisms, aiming at identifying the extent to which \ntheir extracted explanations correlate. We rely on a novel framework for extracting and \ncomparing global impact scores from local explanations obtained from LPEs, and apply \nsuch a framework over several text classification datasets, ranging from simple to complex \ntasks. Our experiments show how most LPEs explanations are far from being mutually \ncorrelated when LPEs are applied over a large set of input samples. These results highlight \nwhat we called the “quarrel” among state-of-the-art local explainers, highlighting the cur -\nrent fragility of xAI approaches for NLP. The disagreement is apparently caused by each \nof them focusing on a different set or subset of relevant concepts, or imposing a different \ndistribution on top of them. Furthermore, we propose a novel approach to construct global \nexplanations—under the form of logic programs—of the original LLM starting from the \nLPE outputs. We test the global explanation extraction approach—namely GELPE—over \na broad set of scenarios, highlighting its fidelity against the sub-symbolic model and the \nsimplicity of the extracted knowledge. Moreover, we analyse the efficiency of the extracted \nlogic programs, showing how it is possible to extract a logic program that is equivalent to \nthe original LLM and is faster and less energy wasteful in scenarios where hardware accel-\neration is not available. Therefore, our experiments show how the extraction process can \nbe leveraged to enable the deployment of NLP applications to resource-constrained envi-\nronments, such as embedded devices and microcontrollers. These findings also highlights \nhow—for some learning tasks—leveraging LLMs might represents an over complication, \nas it is possible to achieve similar performance using simple and small logic programs.\nFuture work is likely to include the application of the proposed methodology to a broad \nrange of state-of-the-art LLMs, starting from Llama and other available open-source archi-\ntectures, aiming at showing if—and to what extent—the findings of this paper apply to \nmodels different from BERT. Similarly, we intend to extend the analysis on the trade-off  \nbetween the achievable fidelity and efficiency of the surrogate logic programs extracted \nusing GELPE. Finally, although the proposed framework is applied to the NLP realm, it \nrepresents a useful starting point for analysing the relevance of LPEs in different domains, \nsuch as computer vision [57, 58], graph processing [59–61] and many more.\nAcknowledgements This work was partially supported by (i) PNRR—M4C2—Investimento 1.3, Parte-\nnariato Esteso PE00000013—“FAIR—Future Artificial Intelligence Research”—Spoke 8 “Pervasive AI”, \nfunded by the European Commission under the NextGenerationEU programme; (ii) the “ENGINES — \nENGineering INtElligent Systems around intelligent agent technologies” project funded by the Italian MUR \nprogram ”PRIN 2022” under grant number 20229ZXBZM; and (iii) the CHIST-ERA IV project “Expecta-\ntion”—CHIST-ERA-19-XAI-005—, co-funded by EU and the Italian MUR (Ministry for University and \nResearch).\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 30 of 33\nFunding Open access funding provided by Alma Mater Studiorum - Università di Bologna within the \nCRUI-CARE Agreement.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\n 1. Zhang, L., Wang, S., & Liu, B. (2018). Deep learning for sentiment analysis: A survey. Wiley Interdis-\nciplinary Reviews: Data Mining and Knowledge Discovery. https:// doi. org/ 10. 1002/ widm. 1253\n 2. Hao, T., Li, X., He, Y., Wang, F. L., & Qu, Y. (2022). Recent progress in leveraging deep learning \nmethods for question answering. Neural Computing and Applications, 34(4), 2765–2783. https:// doi. \norg/ 10. 1007/ s00521- 021- 06748-3\n 3. Otter, D. W., Medina, J. R., & Kalita, J. K. (2021). A survey of the usages of deep learning for natural \nlanguage processing. IEEE Transactions on Neural Networks and Learning Systems, 32(2), 604–624. \nhttps:// doi. org/ 10. 1109/ TNNLS. 2020. 29796 70\n 4. Warstadt, A., Singh, A., & Bowman, S. R. (2019). Neural network acceptability judgments. Trans-\nactions of the Association for Computational Linguistics, 7, 625–641. https:// doi. org/ 10. 1162/ tacl_a_ \n00290\n 5. Stahlberg, F. (2020). Neural machine translation: A review. Journal of Artificial Intelligence Research, \n69, 343–418. https:// doi. org/ 10. 1613/ jair.1. 12007\n 6. Lazaridou, A., & Baroni, M. (2020). Emergent multi-agent communication in the deep learning era. \nCoRR arXiv: 2006. 02419\n 7. Kocaballi, A. B., Berkovsky, S., Quiroz, J. C., Laranjo, L., Tong, H. L., Rezazadegan, D., Briatore, A., \n& Coiera, E. (2019). The personalization of conversational agents in health care: Systematic review. \nJournal of Medical Internet Research, 21(11), 15360. https:// doi. org/ 10. 2196/ 15360\n 8. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tompson, J., Mordatch, I., \nChebotar, Y., Sermanet, P., Jackson, T., Brown, N., Luu, L., Levine, S., Hausman, K., & Ichter, B. \n(2022). Inner monologue: Embodied reasoning through planning with language models. In K. Liu, D. \nKulic, J. Ichnowski (Eds.), Conference on robot learning (CoRL 2022). Proceedings of machine learn-\ning research (vol. 205, pp. 1769–1782). PMLR. https:// proce edings. mlr. press/ v205/ huang 23c/ huang \n23c. pdf\n 9. Cheng, M., Wei, W., & Hsieh, C. (2019). Evaluating and enhancing the robustness of dialogue sys-\ntems: A case study on a negotiation agent. In J. Burstein, C. Doran, T. Solorio (Eds.), 2019 conference \nof the North American chapter of the Association for Computational Linguistics: Human Language \nTechnologies (NAACL-HLT 2019) (vol. 1 (Long and Short Papers), pp. 3325–3335). Association for \nComputational Linguistics. https:// doi. org/ 10. 18653/ V1/ N19- 1336\n 10. Glaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L., \nChadwick, M.J., Thacker, P., Campbell-Gillingham, L., Uesato, J., Huang, P., Comanescu, R., Yang, \nF., See, A., Dathathri, S., Greig, R., Chen, C., Fritz, D., Elias, J.S., Green, R., Mokrá, S., Fernando, N., \nWu, B., Foley, R., Young, S., Gabriel, I., Isaac, W., Mellor, J., Hassabis, D., Kavukcuoglu, K., Hen-\ndricks, L.A., & Irving, G. (2022). Improving alignment of dialogue agents via targeted human judge-\nments. CoRR arXiv: 2209. 14375\n 11. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of deep bidirectional \ntransformers for language understanding. In Proceedings of the 2019 conference of the North Ameri-\ncan chapter of the Association for Computational Linguistics: Human Language Technologies (vol. 1 \n(Long and Short Papers), pp. 4171–4186). Association for Computational Linguistics. https:// doi. org/ \n10. 18653/ v1/ N19- 1423\n 12. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., \nSastry, G., & Askell, A. (2020). Language models are few-shot learners. Advances in Neural Informa-\ntion Processing Systems, 33, 1877–1901.\nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 31 of 33 32\n 13. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., & Liu, P. J. \n(2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of \nMachine Learning Research, 21(1), 5485–5551.\n 14. Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the dangers of stochastic \nparrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, \naccountability, and transparency (pp. 610–623). https:// doi. org/ 10. 1145/ 34421 88. 34459 22\n 15. Zini, J. E., & Awad, M. (2022). On the explainability of natural language processing deep models. \nACM Computing Surveys, 55(5), 1–31. https:// doi. org/ 10. 1145/ 35297 55\n 16. Agiollo, A., Siebert, L. C., Murukannaiah, P. K., & Omicini, A. (2023). The quarrel of local post-hoc \nexplainers for moral values classification in natural language processing. In Explainable and transpar-\nent AI and multi-agent systems. Lecture notes in computer science (Chapter 6, vol. 14127, pp. 97–115). \nSpringer. https:// doi. org/ 10. 1007/ 978-3- 031- 40878-6_6\n 17. Ciatto, G., Sabbatini, F., Agiollo, A., Magnini, M., & Omicini, A. (2024). Symbolic knowledge extrac-\ntion and injection with sub-symbolic predictors: A systematic literature review. ACM Computing Sur-\nveys, 56(6), 161–116135. https:// doi. org/ 10. 1145/ 36451 03\n 18. Kautz, H. A. (2022). The third AI summer: AAAI Robert S. Engelmore Memorial Lecture. AI Maga-\nzine, 43(1), 93–104. https:// doi. org/ 10. 1609/ AIMAG. V43I1. 19122\n 19. Agiollo, A., Rafanelli, A., Magnini, M., Ciatto, G., & Omicini, A. (2023). Symbolic knowledge injec-\ntion meets intelligent agents: QoS metrics and experiments. Autonomous Agents and Multi-Agent Sys-\ntems, 37(2), 27–12730. https:// doi. org/ 10. 1007/ s10458- 023- 09609-6\n 20. Agiollo, A., & Omicini, A. (2023). Measuring trustworthiness in neuro-symbolic integration. In Pro -\nceedings of the 18th conference on computer science and intelligence systems. Annals of computer sci-\nences and information systems (vol. 35, pp. 1–10). https:// doi. org/ 10. 15439/ 2023F 6019\n 21. Agiollo, A., Rafanelli, A., & Omicini, A. (2022). Towards quality-of-service metrics for symbolic \nknowledge injection. In WOA 2022—23rd Workshop “From Objects to Agents”. CEUR workshop pro-\nceedings (vol. 3261, pp. 30–47). Sun SITE Central Europe, RWTH Aachen University. http:// ceur- ws. \norg/ Vol- 3261/ paper3. pdf\n 22. Calegari, R., & Federico, S. (2023). The PSyKE technology for trustworthy artificial intelligence. In \nXXI international conference of the Italian Association for Artificial Intelligence, AIxIA 2022, Udine, \nItaly, November 28–December 2, 2022, Proceedings (vol. 13796, pp. 3–16). https:// doi. org/ 10. 1007/ \n978-3- 031- 27181-6_1\n 23. Sabbatini, F., Ciatto, G., Calegari, R., & Omicini, A. (2022). Symbolic knowledge extraction from \nopaque ML predictors in PSyKE: Platform design & experiments. Intelligenza Artificiale, 16(1), \n27–48. https:// doi. org/ 10. 3233/ IA- 210120\n 24. Sarkar, S., Babar, M. F., Hassan, M. M., Hasan, M., & Santu, S. K. K. (2023). Exploring challenges \nof deploying BERT-based NLP models in resource-constrained embedded devices. CoRR arXiv: 2304. \n11520\n 25. Agiollo, A., & Omicini, A. (2021). Load classification: A case study for applying neural networks in \nhyper-constrained embedded devices. Applied Sciences. https:// doi. org/ 10. 3390/ app11 24119 57. Spe-\ncial Issue “Artificial Intelligence and Data Engineering in Engineering Applications”\n 26. Almeida, T. A., Hidalgo, J. M. G., & Yamakami, A. (2011). Contributions to the study of SMS spam \nfiltering: new collection and results. In M. R. B. Hardy, & F. W. Tompa (Eds.), Proceedings of the \n2011 ACM symposium on document engineering (pp. 259–262). ACM. https:// doi. org/ 10. 1145/ 20346 \n91. 20347 42\n 27. Alberto, T. C., Lochter, J. V., & Almeida, T. A. (2015). TubeSpam: Comment spam filtering on You-\nTube. In T. Li, L. A. Kurgan, V. Palade, R. Goebel, A. Holzinger, K. Verspoor, & M. A. Wani, (Eds.), \n14th IEEE international conference on machine learning and applications (ICMLA 2015) (pp. 138–\n143). IEEE. https:// doi. org/ 10. 1109/ ICMLA. 2015. 37\n 28. Hoover, J., Portillo-Wightman, G., Yeh, L., Havaldar, S., Davani, A. M., Lin, Y., Kennedy, B., Atari, \nM., Kamel, Z., & Mendlen, M. (2020). Moral foundations Twitter corpus: A collection of 35k tweets \nannotated for moral sentiment. Social Psychological and Personality Science, 11(8), 1057–1071. DOI: \nhttps://doi.org/10.1177/194855061987662\n 29. Bayhaqy, A., Sfenrianto, S., Nainggolan, K., & Kaburuan, E. R. (2018). Sentiment analysis about \ne-commerce from tweets using decision tree, k-nearest neighbor, and Naïve Bayes. In 2018 Interna-\ntional Conference on Orange Technologies (ICOT) (pp. 1–6). https:// doi. org/ 10. 1109/ ICOT. 2018. \n87057 96\n 30. Singh, J., & Tripathi, P. (2021). Sentiment analysis of twitter data by making use of svm, random forest \nand decision tree algorithm. In 2021 10th IEEE international conference on communication systems \nand network technologies (CSNT) (pp. 193–198). https:// doi. org/ 10. 1109/ CSNT5 1715. 2021. 95096 79\n Autonomous Agents and Multi-Agent Systems (2024) 38:32\n32 Page 32 of 33\n 31. Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A survey of \nmethods for explaining black box models. ACM Computing Surveys (CSUR), 51(5), 93–19342. https:// \ndoi. org/ 10. 1145/ 32360 09\n 32. Adadi, A., & Berrada, M. (2018). Peeking inside the black-box: A survey on explainable artificial \nintelligence (XAI). IEEE Access, 6, 52138–52160. https:// doi. org/ 10. 1109/ ACCESS. 2018. 28700 52\n 33. Luo, S., Ivison, H., Han, S. C., & Poon, J. (2021). Local interpretations for explainable natural lan-\nguage processing: A survey. CoRR arXiv: 2103. 11072\n 34. Hailesilassie, T. (2016). Rule extraction algorithm for deep neural networks: A review. International \nJournal of Computer Science and Information Security, 14(7), 376–381.\n 35. Ibrahim, M., Louie, M., Modarres, C., & Paisley, J. (2019). Global explanations of neural networks: \nMapping the landscape of predictions. In Proceedings of the 2019 AAAI/ACM conference on AI, ethics, \nand society (pp. 279–287).\n 36. Lundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpreting model predictions. Advances \nin Neural Information Processing Systems, 30, 66.\n 37. Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). Why should I trust you?”: Explaining the predictions \nof any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge \ndiscovery and data mining (pp. 1135–1144). https:// doi. org/ 10. 18653/ v1/ N16- 3020\n 38. Madsen, A., Reddy, S., & Chandar, S. (2022). Post-hoc interpretability for neural NLP: A survey. ACM \nComputing Surveys, 55(8), 1–42. https:// doi. org/ 10. 1145/ 35465 77\n 39. Danilevsky, M., Qian, K., Aharonov, R., Katsis, Y., Kawas, B., & Sen, P. (2020). A survey of the state \nof explainable AI for natural language processing. In Proceedings of the 1st conference of the Asia-\nPacific chapter of the Association for Computational Linguistics and the 10th international joint con-\nference on natural language processing (pp. 447–459). Association for Computational Linguistics.\n 40. Loh, W.-Y. (2014). Fifty years of classification and regression trees. International Statistical Review, \n82(3), 329–348. https:// doi. org/ 10. 1111/ insr. 12016\n 41. Samek, W., Montavon, G., Lapuschkin, S., Anders, C. J., & Müller, K.-R. (2021). Explaining deep \nneural networks and beyond: A review of methods and applications. Proceedings of the IEEE, 109(3), \n247–278. https:// doi. org/ 10. 1109/ JPROC. 2021. 30604 83\n 42. Kindermans, P.-J., Hooker, S., Adebayo, J., Alber, M., Schütt, K.T., Dähne, S., Erhan, D., & Kim, B. \n(2019). The (un)reliability of saliency methods. In Explainable AI: Interpreting, explaining and visual-\nizing deep learning (pp. 267–280). Springer. https:// doi. org/ 10. 1007/ 978-3- 030- 28954-6_ 14\n 43. Ali, A., Schnake, T., Eberle, O., Montavon, G., Müller, K.-R., & Wolf, L. (2022). XAI for transform-\ners: Better explanations through conservative propagation. In International conference on machine \nlearning (pp. 435–451). PMLR.\n 44. Bach, S., Binder, A., Montavon, G., Klauschen, F., Müller, K.-R., & Samek, W. (2015). On pixel-wise \nexplanations for non-linear classifier decisions by layer-wise relevance propagation. PLoS ONE, 10(7), \n0130140.\n 45. Tay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., & Zheng, C. (2021). Synthesizer: Rethinking \nself-attention for transformer models. In Proceedings of the 38th international conference on machine \nlearning. Proceedings of machine learning research (vol. 139, pp. 10183–10192). PMLR.\n 46. Abnar, S., & Zuidema, W. (2020). Quantifying attention flow in transformers. In Proceedings of the \n58th annual meeting of the Association for Computational Linguistics (pp. 4190–4197). Association \nfor Computational Linguistics. https:// doi. org/ 10. 18653/ v1/ 2020. acl- main. 385\n 47. Wu, Z., Nguyen, T.-S., & Ong, D. C. (2020). Structured self-attention weights encode semantics in \nsentiment analysis. In Proceedings of the third blackbox NLP workshop on analyzing and interpreting \nneural networks for NLP (pp. 255–264). Association for Computational Linguistics. https:// doi. org/ 10. \n18653/ v1/ 2020. black boxnlp- 1. 24\n 48. Sundararajan, M., Taly, A., & Yan, Q. (2017). Axiomatic attribution for deep networks. In Interna-\ntional conference on machine learning. Proceedings of machine learning research (vol. 70, pp. 3319–\n3328). PMLR. http:// proce edings. mlr. press/ v70/ sunda raraj an17a/ sunda raraj an17a. pdf\n 49. Kokalj, E., Škrlj, B., Lavrač, N., Pollak, S., & Robnik-Šikonja, M. (2021). BERT meets shapley: \nExtending SHAP explanations to transformer-based classifiers. In Proceedings of the EACL hackashop \non news media content analysis and automated report generation (pp. 16–21).\n 50. Liscio, E., Araque, O., Gatti, L., Constantinescu, I., Jonker, C. M., Kalimeri, K., & Murukannaiah, P. \nK. (2023). What does a text classifier learn about morality? An explainable method for cross-domain \ncomparison of moral rhetoric. In A. Rogers, J. Boyd-Graber, & N. Okazaki (Eds.) Proceedings of the \n61st annual meeting of the Association for Computational Linguistics, vol. 1: Long Papers (pp. 14113–\n14132). Association for Computational Linguistics. https:// doi. org/ 10. 18653/ v1/ 2023. acl- long. 789\n 51. Nguyen, T. H., & Grishman, R. (2016). Modeling skip-grams for event detection with convolutional \nneural networks. In J. Su, X. Carreras, & K. Duh (Eds.), 2016 Conference on empirical methods in \nAutonomous Agents and Multi-Agent Systems (2024) 38:32 \n Page 33 of 33 32\nnatural language processing (EMNLP 2016) (pp. 886–891). The Association for Computational Lin-\nguistics. https:// doi. org/ 10. 18653/ V1/ D16- 1085\n 52. Li, X., & Roth, D. (2002). Learning question classifiers. In 19th International conference on computa-\ntional linguistics (COLING 2002), Taipei, Taiwan. https:// aclan tholo gy. org/ C02- 1150\n 53. Zhang, Z., & Sabuncu, M. (2018). Generalized cross entropy loss for training deep neural networks \nwith noisy labels. In Advances in neural information processing systems (vol. 31). Curran Associates, \nInc..\n 54. Kiesel, J., Alshomary, M., Handke, N., Cai, X., Wachsmuth, H., & Stein, B. (2022). Identifying the \nhuman values behind arguments. In Proceedings of the 60th annual meeting of the Association for \nComputational Linguistics (vol. 1: Long Papers, pp. 4459–4471). https:// doi. org/ 10. 18653/ v1/ 2022. \nacl- long. 306\n 55. Alshomary, M., Baff, R. E., Gurcke, T., & Wachsmuth, H. (2022). The moral debater: A study on the \ncomputational generation of morally framed arguments. In Proceedings of the 60th Annual Meeting of \nthe Association for Computational Linguistics (vol. 1: Long Papers, pp. 8782–8797). Association for \nComputational Linguistics. https:// doi. org/ 10. 18653/ v1/ 2022. acl- long. 601\n 56. Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, \nD., Metzler, D., & Chi, E. H. (2022). Emergent abilities of large language models. Transactions on \nMachine Learning Research, 2022, 66.\n 57. Buhrmester, V., Münch, D., & Arens, M. (2021). Analysis of explainers of black box deep neural net-\nworks for computer vision: A survey. Machine Learning & Knowledge Extraction, 3(4), 966–989. \nhttps:// doi. org/ 10. 3390/ make3 040048\n 58. Agiollo, A., Ciatto, G., & Omicini, A. (2021). Shallow2Deep: Restraining neural networks opacity \nthrough neural architecture search. In Explainable and transparent AI and multi-agent systems. Third \ninternational workshop, EXTRAAMAS 2021. Lecture notes in computer science (vol. 12688, pp. \n63–82). Springer. https:// doi. org/ 10. 1007/ 978-3- 030- 82017-6_5\n 59. Jaume, G., Pati, P., Bozorgtabar, B., Foncubierta, A., Anniciello, A.M., Feroce, F., Rau, T., Thiran, J., \nGabrani, M., & Goksel, O. (2021). Quantifying explainers of graph neural networks in computational \npathology. In IEEE conference on computer vision and pattern recognition, CVPR 2021, Virtual, June \n19–25, 2021, (pp. 8106–8116). Computer Vision Foundation/IEEE. https:// doi. org/ 10. 1109/ CVPR4 \n6437. 2021. 00801\n 60. Agiollo, A., & Omicini, A. (2022). GNN2GNN: Graph neural networks to generate neural networks. \nIn J. Cussens, & K. Zhang (Eds.) Uncertainty in artificial intelligence. Proceedings of machine learn-\ning research (vol. 180, pp. 32–42). ML Research Press. https:// proce edings. mlr. press/ v180/ agiol lo22a. \nhtml\n 61. Agiollo, A., Bardhi, E., Conti, M., Lazzeretti, R., Losiouk, E., & Omicini, A. (2023). GNN4IFA: Inter-\nest flooding attack detection with graph neural networks. In 2023 IEEE 8th European symposium on \nsecurity and privacy (EuroS &P) (pp. 615–630). IEEE Computer Society. https:// doi. org/ 10. 1109/ \nEuroS P57164. 2023. 00043\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6910438537597656
    },
    {
      "name": "Post hoc",
      "score": 0.5794949531555176
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.5134259462356567
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.483808696269989
    },
    {
      "name": "Realm",
      "score": 0.46521949768066406
    },
    {
      "name": "Language model",
      "score": 0.4377070665359497
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4155849814414978
    },
    {
      "name": "Data science",
      "score": 0.34302574396133423
    },
    {
      "name": "Computer security",
      "score": 0.22095927596092224
    },
    {
      "name": "Political science",
      "score": 0.13325709104537964
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Dentistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9360294",
      "name": "University of Bologna",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I98358874",
      "name": "Delft University of Technology",
      "country": "NL"
    }
  ]
}