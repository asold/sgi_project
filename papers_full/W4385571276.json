{
  "title": "Sequential Integrated Gradients: a simple but effective method for explaining language models",
  "url": "https://openalex.org/W4385571276",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5009775628",
      "name": "Joseph Enguehard",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221141050",
    "https://openalex.org/W2982567551",
    "https://openalex.org/W2917128112",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W3085380432",
    "https://openalex.org/W4298061300",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2346578521",
    "https://openalex.org/W3035503910",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W4287125421",
    "https://openalex.org/W2898694742",
    "https://openalex.org/W2951306478",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3212603771",
    "https://openalex.org/W2980282514"
  ],
  "abstract": "Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of language models, we also propose to replace the baseline token “pad” with the trained token “mask”. While being a simple improvement over the original IG method, we show on various models and datasets that SIG proves to be a very effective method for explaining language models.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 7555–7565\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nSequential Integrated Gradients: a simple but effective method for\nexplaining language models\nJoseph Enguehard\nBabylon Health\nSkippr\njoseph@skippr.com\nAbstract\nSeveral explanation methods such as Integrated\nGradients (IG) can be characterised as path-\nbased methods, as they rely on a straight line\nbetween the data and an uninformative baseline.\nHowever, when applied to language models,\nthese methods produce a path for each word\nof a sentence simultaneously, which could lead\nto creating sentences from interpolated words\neither having no clear meaning, or having a sig-\nnificantly different meaning compared to the\noriginal sentence. In order to keep the meaning\nof these sentences as close as possible to the\noriginal one, we propose Sequential Integrated\nGradients (SIG), which computes the impor-\ntance of each word in a sentence by keeping\nfixed every other words, only creating interpo-\nlations between the baseline and the word of\ninterest. Moreover, inspired by the training pro-\ncedure of several language models, we also pro-\npose to replace the baseline token \"pad\" with\nthe trained token \"mask\". While being a sim-\nple improvement over the original IG method,\nwe show on various models and datasets that\nSIG proves to be a very effective method for\nexplaining language models.1\n1 Introduction\nLanguage models such as BERT (Devlin et al.,\n2018) have demonstrated to be effective on various\ntasks, for instance on sentiment analysis (Hoang\net al., 2019), machine translation (Zhu et al., 2020),\ntext summarization (Liu, 2019) or intent classifi-\ncation (Chen et al., 2019). However, with the in-\ncreased performance and usage of such models,\nthere has been a parallel drive to develop methods\nto explain predictions made by these models. In-\ndeed, BERT and its variations are complex models\nwhich do not allow a user to easily understand why\na certain prediction has been produced. On the\nother hand, it is important to be able to explain a\n1An implementation of this work can be found at https:\n//github.com/josephenguehard/time_interpret\nFigure 1: Comparison between IG, DIG, and our\nmethod: SIG. While DIG improves on IG by creating\ndiscretized paths between the data and the baseline, it\ncan produce sentences with a different meaning com-\npared to the original one. Our method tackles this issue\nby fixing every word to their true value except one, and\nmoving the remaining word along a straight path (SIG)\nmodel’s predictions, especially when this model is\nused to make high-stake decisions, or when there\nis a risk of a discriminating bias, for instance when\ndetecting hate speech on social media (Sap et al.,\n2019).\nAs a result, developing effective methods to ex-\nplain not only language models, but also machine\nlearning models in general, has recently gained sig-\nnificant attention. Many different methods have\ntherefore been proposed such as: LIME (Ribeiro\net al., 2016), Grad*Inp (Shrikumar et al., 2016), In-\ntegrated Gradients (IG) (Sundararajan et al., 2017),\nDeepLift (Shrikumar et al., 2017) or GradientShap\n(Lundberg and Lee, 2017). Among these methods,\nsome can be characterised as path-based, which\n7555\nmeans that they rely on a straight line between the\ndata and an uninformative baseline. For instance,\nIG computes gradients on interpolated points along\nsuch a path, while DeepLift and GradientShap can\nbe seen as approximations of IG (Ancona et al.,\n2017; Lundberg and Lee, 2017).\nWhile these methods aim to be used on any type\nof models and data, some have been tailored to\nthe specificity of language models. For instance,\nSanyal and Ren (2021) challenge the use of contin-\nuous paths on a word embedding space which is\ninherently discrete. They propose as a result Dis-\ncretized Integrated Gradient (DIG), which replaces\nthe continuous straight path with a discretized one,\nwhere interpolated points are words.\nIn our work, we suggest another potential is-\nsue when applying path-based explanation meth-\nods on language models. These models are usu-\nally designed to be used on individual or multiple\nsentences, in order to perform for instance senti-\nment analysis or question answering. However,\na path-based method applied on such models cre-\nates straight lines between each word and a base-\nline simultaneously. When interpolated points are\ngrouped together to form a sentence, this sentence\ncould have a very different meaning compared with\nthe original one.\nAs a result, we propose a simple method to allevi-\nate this potential issue: computing the importance\nof each word in a sentence or a text by keeping\nfixed every other word and only creating interpola-\ntions between the baseline and the word of interest.\nAfter computing the importance of each word in\nthis way, we normalise these attributions across\nthe sentence or text we aim to explain. We call\nthis method Sequential Integrated Gradients (SIG),\nas, although we focus in this work on language\nmodels, such a method could be used on any se-\nquential modelling. We also propose to use the\ntoken \"mask\" as a baseline, when possible, as its\nembedding has been trained to replace part of sen-\ntences when training language models. As a result,\nour method follows closely the training procedure\nof these models.\n2 Method\nSIG formulation Let’s define a language model\nas a function F(x) : Rm×n →R. The input x\nis here modelled as a sequence of mwords, each\nhaving nfeatures. These features are usually con-\nstructed by an embedding layer. We denote xi the\nith word of a sentence (or of a text, depending on\nthe input of the model), and xij the jth feature of\nthe ith word. The output of F is a value in R, which\nis, in our experiments, a measure of the sentiment\nfor a given sentence. We now define the baseline\nfor each word xi as xi = (x1,..., <mask>,..., xm).\nThe baseline is therefore identical to x except at\nthe ith position, where the word xi is replaced by\nthe embedding of the word \"mask\"2, a token used\nin many language model to replace part of the sen-\ntence during training. Moreover, we use the nota-\ntion xi instead of xi as xi corresponds to an entire\nsentence, not to be mistaken with a single word\nlike xi.\nIn this setting, we keep the baseline as similar to\nthe original sentence as possible, only changing the\nword of interest. This method of explaining a word\nis also kept similar to the way these language mod-\nels are usually pre-trained, by randomly masking\npart of sentences.\nLet’s now define our Sequential Integrated Gra-\ndients (SIG) method. For a word xi and a feature j,\nSIG is defined as:\nSIGij(x) := (xij −xij)×\n∫1\n0\n∂F(xi + α×(x −xi))\n∂xij\ndα\nSimilar to the original IG (Sundararajan et al.,\n2017), we compute the gradient of F along a\nstraight line between xi and x for each word xi,\nthe main difference being that the baseline differs\nfor each word. Also similar to the original IG, we\napproximate in practice the integral with Riemann\nsummation.\nFinally, we compute the overall attribution of a\nword by computing the sum over the feature dimen-\nsion j, and normalising the result:\nSIGi(x) :=\n∑\nj SIGij\n||SIG||\nAxioms satisfied by SIG The original Integrated\nGradients method satisfies a few axioms that are\nconsidered desirable for any explanation methods\nto have. Among these axioms, SIG follows imple-\nmentation invariance, which states that attributions\nshould be identical if two models are functionally\nequivalent. Moreover, SIG follows completeness\n2Certain language models, such as GPT-2 (Radford et al.,\n2019), do not have a \"mask\" token. A \"pad\" token should be\ntherefore used for such models.\n7556\nMethod DistilBERT RoBERTa BERT\nLO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓\nGrad*Inp -0.412 0.112 0.375 -0.199 0.0760 0.426 -0.263 0.0923 0.439\nDeepLift -0.624 0.170 0.271 -0.261 0.0932 0.408 -0.244 0.0898 0.438\nGradientShap -1.32 0.303 0.258 -0.896 0.261 0.314 -0.622 0.219 0.388\nIG -1.96 0.445 0.151 -1.44 0.405 0.226 -0.981 0.345 0.352\nDIG -1.69 0.384 0.167 -0.824 0.263 0.278 -0.777 0.287 0.345\nSIG -2.02 0.473 0.0992 -1.62 0.440 0.216 -1.19 0.392 0.312\nTable 1: Comparison of SIG with several feature attribution methods on three language models fine-tuned on the\nSST2 dataset. For ↑metrics, the higher the better, while for ↓ones, the lower the better.\nMethod DistilBERT RoBERTa BERT\nLO ↓ Comp↑ Suff↓ LO ↓ Comp↑ Suff↓ LO ↓ Comp↑ Suff↓\nGrad*Inp -0.153 0.0766 0.209 -0.0892 0.0432 0.300 -0.291 0.0887 0.298\nDeepLift -0.269 0.117 0.159 -0.124 0.0557 0.269 -0.285 0.0701 0.366\nGradientShap -0.832 0.289 0.137 -0.606 0.204 0.144 -0.874 0.172 0.308\nIG -1.50 0.534 0.0428 -1.35 0.441 0.0327 -1.58 0.302 0.224\nDIG -0.779 0.304 0.133 -0.663 0.186 0.108 -1.06 0.207 0.232\nSIG -1.95 0.564 0.00409 -1.37 0.404 -3.31E-05 -2.12 0.364 0.124\nTable 2: Comparison of SIG with several feature attribution methods on three language models fine-tuned on the\nIMDB dataset.\nin a specific way: for each word xi, we have the\nfollowing result:\n∑\nj\nSIGij(x) =F(x) −F(xi)\nThis means that for each word, the sum of its\nattribution across all features j is equal to the differ-\nence between the output of the model as x and at\nits corresponding baseline xi. However, it does not\nentail that ∑\nij SIGij(x) =F(x) −F(x), where x\nwould be an overall baseline filled with <mask>.\nMoreover, this last axiom entail another one\ncalled sensitivity, which here means that if, for\na certain word, the input x has the same influence\non the output of F as its corresponding baseline xi,\nthen ∑\nj SIGij(x) = 0.\nFinally, we show in Appendix A that SIG pre-\nserves symmetry for each word on the embedding\ndimension, but that this axiom is not true in general.\nUsing mask instead of pad as a baseline We\npropose in this study to replace, as the baseline,\nthe commonly used \"pad\" token with the \"mask\"\ntoken, on language models having such token. This\nseems to go against the intuition that the baseline\nshould be uninformative, as \"mask\" is a trained\ntoken. To support the usage of \"mask\", we argue\nthat, because <PAD> (denoting the embedding of\n\"pad\") is untrained, it could be arbitrarily close to\nsome words, and far from others. Oh the other\nhand, <MASK> has been trained to replace random\nwords, making it ideally as close to one word as to\nany other.\nAnother way to see it is to compare it with im-\nages. It is natural for images to choose the baseline\nas a black image, as this baseline has no informa-\ntion. However, there is no such guarantee in NLP.\nFor instance, the embedding of \"pad\": <0, 0, 0,\n. . . , 0>could perfectly be very close to an em-\nbedding of a word with a specific meaning, which\nwould harm the explanations. On the other hand,\n<MASK> has been trained to replace any word, and\ntherefore seems more suited to be the baseline.\n3 Experiments\n3.1 Experiments design\nWe evaluate SIG against various explanation meth-\nods by closely following the experimental setup of\nSanyal and Ren (2021). As such, we use the follow-\ning language models: BERT (Devlin et al., 2018),\nDistilBERT (Sanh et al., 2019) and RoBERTa (Liu\n7557\nMethod DistilBERT RoBERTa BERT\nLO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓\nGrad*Inp -0.257 0.0681 0.315 -0.121 0.0617 0.363 -0.438 0.143 0.438\nDeepLift -0.332 0.101 0.260 -0.163 0.0804 0.348 -0.452 0.123 0.450\nGradientShap -0.452 0.237 0.212 -0.389 0.194 0.299 -0.715 0.204 0.438\nIG -0.540 0.341 0.163 -0.787 0.354 0.242 -1.19 0.307 0.410\nDIG -0.487 0.273 0.181 -0.426 0.223 0.286 -1.05 0.293 0.414\nSIG -0.533 0.331 0.134 -0.869 0.361 0.251 -1.52 0.390 0.349\nTable 3: Comparison of SIG with several feature attribution methods on three language models fine-tuned on the\nRotten Tomatoes dataset.\nsteps LO ↓ Comp↑ Suff↓ Delta Time\nIG 50 -0.981 0.345 0.352 0.304 t\nSIG 50 -1.19 0.392 0.312 4.82 N ×t\nIG 250 -0.999 0.352 0.355 0.055 t′\nIG 10 ×N -0.998 0.351 0.352 0.066 N ×t′/ 25\nSIG 10 -1.14 0.373 0.322 4.93 N ×t′/ 25\nTable 4: Comparison of IG and SIG with different num-\nbers of interpolations on BERT fine-tuned on the SST2\ndataset.\ntand t′ represent the amount of time to calculate IG\nwith 50 and 250 steps respectively, and N represents the\nnumber of words on the input data (for instance in one\nsentence). On the SST2 dataset, we have an average of:\nN ≈25 words per sentence.\nOn top of the table, we compare IG and SIG using a\nfixed number of steps. On the bottom of the table, we\ncompare IG with 250 steps against SIG with 10 steps.\nSince N ≈25, we have N ×t′ / 25 ≈t′. For a fairer\ncomparison, we also compare IG with a variable num-\nber of steps: 10 ×N for each sentence, against SIG\nwith 10 steps. These two methods have the same time\ncomplexity.\nDelta is defined as ∑\nij Attrij (x)−(F(x)−F(x)). Con-\ntrary to IG, SIG has a high delta value, as in general∑\nij SIGij (x) ̸= F(x) −F(x).\net al., 2019). We also use the following datasets:\nSST2 (Socher et al., 2013), IMDB (Maas et al.,\n2011) and Rotten Tomatoes (Pang and Lee, 2005),\nwhich classify sentences into positive or negative\nsentiments or reviews. Moreover, we use the Hug-\ngingFace library to recover processed data and pre-\ntrained models (Wolf et al., 2019).\nFollowing (Sanyal and Ren, 2021), we use the\nfollowing evaluation metrics: Log-Odds (Shriku-\nmar et al., 2017), Comprehensiveness (DeYoung\net al., 2019) and Sufficiency (DeYoung et al., 2019).\nThese metrics mask the top or bottom 20 % impor-\ntant features, according to an attribution method,\nand measure by how much the prediction of the\nlanguage model changes using this masked data,\ncompared with the original one. For more details\non these metrics, please see Sanyal and Ren (2021).\nFinally, we use the following feature attribu-\ntion methods to compare our methods against:\nGrad*Inp (Shrikumar et al., 2016), Integrated\nGradients (Sundararajan et al., 2017), DeepLift\n(Shrikumar et al., 2017), GradientShap (Lundberg\nand Lee, 2017) and Discretized IG (DIG) (Sanyal\nand Ren, 2021) using the GREEDY heuristics.\nMoreover, as in Sanyal and Ren (2021), we use\n50 interpolation steps for all methods expect from\nDIG, for which we use 30 steps.\n3.2 Results\nComparison with other feature attribution\nmethods We present of Tables 1, 2 and 3 a com-\nparison of the performance of SIG with the attri-\nbution methods listed in 3.1. We observe that SIG\nsignificantly outperforms all other methods across\nmost datasets and language models we used. This\ntends to confirm that the change of overall meaning\nof a sentence by combining interpolations simul-\ntaneously is an important issue which needs to be\ntackled.\nComparison between IG and DIG Although\nresults in Sanyal and Ren (2021) show that DIG\noutperforms other methods, including IG, this is\nnot the case when using \"mask\" as a token. This re-\nsult seems to undermine the intuition of Sanyal and\nRen (2021) that the discrete nature of the embed-\nding space is an important factor when explaining\na language model. We also show in Appendix C\nthat the requirement of having a monotonic path,\nstressed by Sanyal and Ren (2021), is not neces-\nsary.\n7558\nMethod Example\nIG “a well-made and often lovelydepictionof themysteriesof friendship.\nSIG “a well-madeand oftenlovelydepiction of the mysteries of friendship.\nIG “ a hideous ,confusingspectacle ,onethat may well putthenail in the coffin of any future rice adaptations.”\nSIG “a hideous, confusingspectacle ,onethat may well put the nail in the coffin of any futurericeadaptations.”\nIG \"this is junkfood cinema at itsgreasiest.\"\nSIG \"this is junkfood cinema at its greasiest.\"\nIG \"a remarkable179-minutemeditationon the nature of revolution.\"\nSIG \"a remarkable179-minutemeditationon the nature of revolution.\"\nTable 5: Examples of attributions on several sentences of the SST2 dataset. The underlined bold tokens represent\nthe most important token in the sentence, while bold tokens represent the top 20 % tokens in the sentence, according\nto each attribution method.\nChoice of the baseline token We also provide\nin Appendix B results using \"pad\" as a baseline.\nComparison between Tables 1, 2 and 3 on one\nhand, and Tables 6, 7, 8 on the other hand show\nthat IG greatly improves using the \"mask\" token\nas a baseline. This seems to confirm our intuition\nof using this token instead of \"pad\". Moreover,\nSIG performs similarly using either token, which\ndemonstrates the robustness of this method across\nthese two baseline tokens.\nTime complexity of SIG One important draw-\nback of SIG is its time complexity, which is de-\npendent on the number of words in the input data.\nIn Table 4, we compare the original IG with SIG,\nusing different numbers of steps. We define tand\nt′ as the time complexity of computing IG with\nrespectively 50 and 250 steps, and N the number\nof words in the input data. This table shows that,\nalthough reducing the number of steps results in\na decrease of performance, SIG with 10 steps still\nperforms better than both IG with 250 steps and\nIG with 10 ×N steps, while having the same time\ncomplexity.\nMoreover, as noted in Sanyal and Ren (2021),\nusing IG with a large number of steps decreases\nDelta = ∑\nij IGij(x) −(F(x) −F(x)), while not\nimproving performance. As a result, when comput-\ning attributions on long sentences or large texts, we\nrecommend using SIG with a reduced number of\nsteps instead of IG.\nComparison of IG and SIG on several exam-\nples We provide on Table 5 several examples of\nexplained sentences, using IG and SIG. Both meth-\nods tend to agree on short sentences, while more\ndisagreements appear on larger ones. For each\nexample, we display in underlined bold the most\nimportant token, and in bold the top 20 % most\nimportant tokens, according to each method.\n4 Conclusion\nIn this work, we have defined an attribution method\nspecific to text data: Sequential Integrated Gradi-\nents (SIG). We have shown that SIG yields signif-\nicantly better results than the original Integrated\nGradients (IG), as well as other methods specific\nto language models, such as Discretized Integrated\nGradients (DIG). This suggests that keeping the\nmeaning of interpolated sentences close to the orig-\ninal one is key to producing good explanations. We\nhave also shown that, although SIG can be compu-\ntationally intensive, reducing the number of inter-\npolations still yields better results than IG with a\ngreater number of interpolations.\nWe have also highlighted in this work the benefit\nof using the token \"mask\" as a baseline, instead\nof \"pad\". Although SIG seems to be robust across\nboth tokens, this is especially important when us-\ning IG, as it significantly improves the quality of\nexplanations. Using the trainable token \"mask\" is\nindeed closer to the training procedure of language\nmodels, and should yield better interpolations as\na result. We recommend therefore using this to-\nken as a baseline, when possible, when explaining\npredictions made by a language model.\nMoreover, while this study was conducted on\nbidirectional language models such as BERT, SIG\ncould also be used on auto-regressive models such\nas GPT-2 (Radford et al., 2019), by iteratively com-\nputing the attribution of a token, while keeping\nprevious tokens fixed, and masking future tokens if\nany has been already computed.\n7559\nLimitations\nWe see two main limitations of this work. The\nfirst one concerns the diversity of the language\nmodels and datasets used. BERT, DistilBERT and\nRoBERTa have similar architecture, and SST2,\nIMDB and Rotten Tomatoes are datasets designed\nto evaluate the sentiment of English text. It would\ntherefore be interesting to validate the robustness\nof our results on more diverse languages, tasks\nand language models. In this short paper, we de-\ncided for brevity to follow the experiment design\nof Sanyal and Ren (2021), while being aware of its\ninherent limitations.\nThe second limitation of this work concerns the\ntime complexity of SIG. As it needs to compute ex-\nplanations for each word individually, this method\ncan become very computationally expensive when\napplied on large text data. To alleviate this issue,\nwe first made it possible to compute gradients in\nparallel, using an internal batch size similar to how\nCaptum (Kokhlikyan et al., 2020) implemented\nthe Integrated Gradients method. Secondly, as dis-\ncussed in 3.2, it is possible to reduce the number of\ninterpolated points, which makes the computation\nfaster while retaining better performance than the\noriginal IG.\nIn this work, we ran our experiments on a ma-\nchine with 16 CPUs, and one Nvidia Tesla T4 GPU.\nWith this setting, computing SIG on SST2 and Rot-\nten Tomatoes takes around one hour for each model.\nOn the larger IMDB, computing SIG, on 2000 ran-\ndomly sampled inputs, takes around 5 days for\nBERT and RoBERTa, and 2 days for DistilBERT.\nEthics Statement\nThe methods presented in this work aim to explain\nlanguage models, and can as such present ethical is-\nsues related to this task. Discriminating biases can\nindeed be present in text data on which a language\nmodel is trained, and such a model can acquire\nand propagate these biases (Sap et al., 2019). As\nthe presented methods aim to explain a language\nmodel without additional knowledge, these meth-\nods could also display discriminating biases learnt\nby a language model.\nMoreover, common explanation methods such\nas Integrated Gradients has proved to be prone to\nadversarial attacks (Dombrowski et al., 2019), and\ncan be misleading when used on out of sample data\n(Slack et al., 2021). There is no reason to believe\nour methods would be more robust compared to\nexisting methods such as IG.\nThe proposed methods can also be characterised\nas gradient-based, as they rely on computing gradi-\nents on the input data, an uninformative baseline,\nor on interpolated points between them. As noted\nby (Mittelstadt et al., 2019), such methods are only\nlocal and may not give a clear explanation of the\nmodel globally.\nAcknowledgement\nThe author would like to thank Vitalii Zhelezniak\nfor his thoughtful comments and suggestions, in-\ncluding using the \"mask\" token as a baseline. We\nalso thank Anthony Hu for his detailed initial re-\nview of this paper.\nReferences\nMarco Ancona, Enea Ceolini, Cengiz Öztireli, and\nMarkus Gross. 2017. Towards better understand-\ning of gradient-based attribution methods for deep\nneural networks. arXiv preprint arXiv:1711.06104.\nQian Chen, Zhu Zhuo, and Wen Wang. 2019. Bert\nfor joint intent classification and slot filling. arXiv\npreprint arXiv:1902.10909.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C Wallace. 2019. Eraser: A benchmark to\nevaluate rationalized nlp models. arXiv preprint\narXiv:1911.03429.\nAnn-Kathrin Dombrowski, Maximillian Alber, Christo-\npher Anders, Marcel Ackermann, Klaus-Robert\nMüller, and Pan Kessel. 2019. Explanations can be\nmanipulated and geometry is to blame. Advances in\nNeural Information Processing Systems, 32.\nMickel Hoang, Oskar Alija Bihorac, and Jacobo Rouces.\n2019. Aspect-based sentiment analysis using BERT.\nIn Proceedings of the 22nd Nordic Conference on\nComputational Linguistics, pages 187–196, Turku,\nFinland. Linköping University Electronic Press.\nNarine Kokhlikyan, Vivek Miglani, Miguel Martin,\nEdward Wang, Bilal Alsallakh, Jonathan Reynolds,\nAlexander Melnikov, Natalia Kliushkina, Carlos\nAraya, Siqi Yan, and Orion Reblitz-Richardson. 2020.\nCaptum: A unified and generic model interpretability\nlibrary for pytorch.\nYang Liu. 2019. Fine-tune bert for extractive summa-\nrization. arXiv preprint arXiv:1903.10318.\n7560\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nScott M Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. Advances\nin neural information processing systems, 30.\nDaniel D Lundstrom, Tianjian Huang, and Meisam\nRazaviyayn. 2022. A rigorous study of integrated\ngradients method and extensions to internal neuron\nattributions. In International Conference on Machine\nLearning, pages 14485–14508. PMLR.\nAndrew Maas, Raymond E Daly, Peter T Pham, Dan\nHuang, Andrew Y Ng, and Christopher Potts. 2011.\nLearning word vectors for sentiment analysis. In\nProceedings of the 49th annual meeting of the associ-\nation for computational linguistics: Human language\ntechnologies, pages 142–150.\nBrent Mittelstadt, Chris Russell, and Sandra Wachter.\n2019. Explaining explanations in ai. In Proceedings\nof the conference on fairness, accountability, and\ntransparency, pages 279–288.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. arXiv preprint cs/0506075.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \" why should i trust you?\" explaining\nthe predictions of any classifier. In Proceedings of\nthe 22nd ACM SIGKDD international conference on\nknowledge discovery and data mining, pages 1135–\n1144.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nSoumya Sanyal and Xiang Ren. 2021. Discretized in-\ntegrated gradients for explaining language models.\narXiv preprint arXiv:2108.13654.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A Smith. 2019. The risk of racial bias in\nhate speech detection. In Proceedings of the 57th\nannual meeting of the association for computational\nlinguistics, pages 1668–1678.\nAvanti Shrikumar, Peyton Greenside, and Anshul Kun-\ndaje. 2017. Learning important features through\npropagating activation differences. In International\nconference on machine learning, pages 3145–3153.\nPMLR.\nAvanti Shrikumar, Peyton Greenside, Anna Shcherbina,\nand Anshul Kundaje. 2016. Not just a black box:\nLearning important features through propagating acti-\nvation differences. arXiv preprint arXiv:1605.01713.\nDylan Slack, Anna Hilgard, Himabindu Lakkaraju, and\nSameer Singh. 2021. Counterfactual explanations\ncan be manipulated. Advances in Neural Information\nProcessing Systems, 34:62–75.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. In Interna-\ntional conference on machine learning, pages 3319–\n3328. PMLR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020.\nIncorporating bert into neural machine translation.\narXiv preprint arXiv:2002.06823.\nA On the symmetry-preserving axiom of\nSequential Integrated Gradients\nThis section is divided into two parts. First, we\nshow that SIG preserves symmetry for each word\nalong the embedding dimension. Second, we pro-\nvide a counterexample to show that symmetry does\nnot hold in general.\nSymmetry of SIGi Let us use the same notations\nas in Section 2. We want to compute the attribution\nof a word xi on a model F, using the baseline xi.\nLet’s define the function:\nFi(x) :=F(x1,..., x,..., xm)\nFi corresponds to F where only the ith word is\nnot fixed. Here, x corresponds to a word, not a\nsentence.\nFor such a function, SIG corresponds to the reg-\nular IG method: the baseline is < mask > and\nSIG constructs a straight line between this base-\nline and xi. As a result, if Fi is symmetric on two\nembedding features j1 and j2, SIG preserves this\nsymmetry: SIGij1 (x) =SIGij2 (x).\n7561\nNon symmetry of SIG The fact that SIG does\nnot preserve symmetry in general is due to the\nchoice of the baseline. As a counterexample, let’s\ndefine a language F which takes as an input two\nwords x1 and x2. This language model is moreover\nsymmetric: F(x1,x2) =F(x2,x1).\nHere, the original IG method would preserve\nthe symmetry: as the baseline is (<mask>,<mask>),\nwhen x1 = x2, we have IG(x)1 = IG(x)2. How-\never, SIG doesn’t preserve the symmetry due to\nits baseline: we would have: x1 = (<mask>,x2)\nand x2 = (x1,<mask>). As a result, SIG(x)1 =\nSIG(x)2 only if x1 = x2 = <mask>.\nB Additional results using the \"pad\"\ntoken\nWe present in this section results using the \"pad\"\ntoken instead of the \"mask\" one. These results\nfor the three datasets: SST2, IMDB and Rotten\nTomatoes can be found respectively on Tables 6, 7\nand 8.\nWhen using the \"pad\" token as a baseline, SIG\nseems to perform similarly compared with using\nthe \"mask\" one, while other methods perform sig-\nnificantly worse. This demonstrates both the need\nto use \"mask\" as a token, and the robustness of the\nSIG method across different baselines.\nC Challenge of the monotonic assumption\nof the path\nSanyal and Ren (2021) stipulate that the path be-\ntween a baseline and an input needs to be mono-\ntonic to allow approximating the integral in IG\nusing Riemann summation. However, while this is\ntrue for a Riemann integral, it is also possible to\napproximate the Riemann–Stieltjes integral, which\nis a generalisation of Riemann integral, and does\nnot need a monotonic path. We define the Rie-\nmann–Stieltjes integral of f : [a,b] →R as:\n∫b\nx=a\nf(x) dg(x)\nwhere g : [0,1] →[a,b] designates a path. Let\nus define a partition over [0,1] as tk such as 0 ≤\nt1 ≤... ≤tn ≤1. We can then approximate the\nintegral with the sum:\nn−1∑\ni=0\nf(g(ci)) ×[g(ti+1) −g(ti)]\nwhere ci ∈[ti,ti+1]. As such, while the parti-\ntion ti, i∈{1,...,n }needs to be monotonic, the\nfunction g does not need to have this constraint. As\na result, we could define a path-based IG method\nas:\nIGγ(x)i :=\n∫\nγ\n∂F(x)\n∂xi\ndxi\nwhere γis not necessarily monotonic.\n(Lundstrom et al., 2022) provide more insights\non this topic, and in particular show that the imple-\nmentation invariance, completeness and sensitivity\naxioms hold for non-monotonic paths.\nFor this reason, we decided not to include a com-\nbination of DIG and SIG in this study. However, an\nimplementation of this method and the correspond-\ning results can be found in the repository published\nwith this paper.\n7562\nMethod DistilBERT RoBERTa BERT\nLO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓\nGrad*Inp -0.402 0.112 0.375 -0.318 0.085 0.398 -0.454 0.092 0.439\nDeepLift -0.196 0.053 0.489 -0.270 0.0784 0.439 -0.283 0.061 0.463\nGradientShap -0.753 0.191 0.328 -0.514 0.146 0.386 -0.471 0.146 0.425\nIG -0.954 0.251 0.273 -0.726 0.227 0.315 -0.658 0.235 0.398\nDIG -1.222 0.310 0.237 -0.812 0.249 0.287 -0.879 0.292 0.374\nSIG -1.993 0.466 0.108 -1.346 0.398 0.244 -1.30 0.393 0.331\nTable 6: Comparison of SIG with several baselines on three language models fine-tuned on the SST2 dataset. For ↑\nmetrics, the higher the better, while for ↓ones, the lower the better.\nMethod DistilBERT RoBERTa BERT\nLO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓\nGrad*Inp -0.189 0.082 0.209 -0.216 0.047 0.315 -0.654 0.087 0.299\nDeepLift -0.032 -0.005 0.515 -0.149 0.031 0.374 -0.519 0.027 0.465\nGradientShap -0.315 0.117 0.302 -0.351 0.110 0.213 -0.622 0.088 0.358\nIG -0.474 0.186 0.201 -0.499 0.169 0.114 -0.577 0.117 0.288\nDIG -0.812 0.297 0.153 -0.626 0.187 0.099 -0.971 0.192 0.229\nSIG -2.157 0.585 0.0062 -0.856 0.291 0.0207 -1.96 0.352 0.152\nTable 7: Comparison of SIG with several baselines on three language models fine-tuned on the IMDB dataset.\nMethod DistilBERT RoBERTa BERT\nLO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓ LO ↓ Comp ↑ Suff ↓\nGrad*Inp -0.152 0.068 0.315 -0.211 0.062 0.363 -0.806 0.143 0.438\nDeepLift -0.077 0.017 0.372 -0.198 0.056 0.370 -0.457 0.076 0.474\nGradientShap -0.326 0.147 0.250 -0.264 0.103 0.348 -0.697 0.161 0.429\nIG -0.424 0.208 0.190 -0.360 0.151 0.312 -0.795 0.201 0.414\nDIG -0.501 0.257 0.184 -0.346 0.153 0.310 -1.06 0.267 0.416\nSIG -0.753 0.378 0.109 -0.771 0.318 0.266 -1.55 0.360 0.393\nTable 8: Comparison of SIG with several baselines on three language models fine-tuned on the Rotten Tomatoes\ndataset.\n7563\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection at the end of the paper, before references\n□\u0013 A2. Did you discuss any potential risks of your work?\nIn the introduction and limitation sections\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nI created my own and used code from https://github.com/INK-USC/DIG\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Open source\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nIn the appendix\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nIn the experiment section\nC □\u0013 Did you run computational experiments?\nIn the experiment section\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nThe models used are pre-trained, standard language models. The computation budget and infrastruc-\nture used is discussed in the ethics statement.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n7564\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. The is no training and hyperparameter search\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nIt is just a single run - the results are not stochastic\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n7565",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.79461669921875
    },
    {
      "name": "Sentence",
      "score": 0.7912575602531433
    },
    {
      "name": "Security token",
      "score": 0.7312546372413635
    },
    {
      "name": "Word (group theory)",
      "score": 0.709533154964447
    },
    {
      "name": "Language model",
      "score": 0.6445637941360474
    },
    {
      "name": "Meaning (existential)",
      "score": 0.6319375038146973
    },
    {
      "name": "Baseline (sea)",
      "score": 0.6021593809127808
    },
    {
      "name": "Path (computing)",
      "score": 0.5726634860038757
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.5657005906105042
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5549814105033875
    },
    {
      "name": "Natural language processing",
      "score": 0.5245952010154724
    },
    {
      "name": "Linguistics",
      "score": 0.14966166019439697
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Psychology",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 14
}