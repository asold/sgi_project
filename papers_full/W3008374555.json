{
  "title": "MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers",
  "url": "https://openalex.org/W3008374555",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1496033914",
      "name": "Wang Wen-hui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2389670735",
      "name": "Wei, Furu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097573093",
      "name": "Dong Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222425276",
      "name": "Bao, Hangbo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1979826969",
      "name": "Yang Nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102363648",
      "name": "Zhou, Ming",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2962753370",
    "https://openalex.org/W2911803042",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2972437349",
    "https://openalex.org/W2952468927",
    "https://openalex.org/W2963350559",
    "https://openalex.org/W2970049541",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2888482885",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2890166583",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2920812691",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W2953109491",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2997666887",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2889518897",
    "https://openalex.org/W2970454332",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2924902521",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2980404057",
    "https://openalex.org/W2806120502",
    "https://openalex.org/W2969601108",
    "https://openalex.org/W2561238782",
    "https://openalex.org/W2970597249"
  ],
  "abstract": "Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.",
  "full_text": "MINI LM: Deep Self-Attention Distillation for\nTask-Agnostic Compression of Pre-Trained Transformers\nWenhui Wang Furu Wei Li Dong Hangbo Bao Nan Yang Ming Zhou\nMicrosoft Research\n{wenwan,fuwei,lidong1,t-habao,nanya,mingzhou}@microsoft.com\nAbstract\nPre-trained language models (e.g., BERT (Devlin\net al., 2018) and its variants) have achieved re-\nmarkable success in varieties of NLP tasks. How-\never, these models usually consist of hundreds of\nmillions of parameters which brings challenges\nfor Ô¨Åne-tuning and online serving in real-life ap-\nplications due to latency and capacity constraints.\nIn this work, we present a simple and effective ap-\nproach to compress large Transformer (Vaswani\net al., 2017) based pre-trained models, termed\nas deep self-attention distillation. The small\nmodel (student) is trained by deeply mimicking\nthe self-attention module, which plays a vital\nrole in Transformer networks, of the large model\n(teacher). SpeciÔ¨Åcally, we propose distilling the\nself-attention module of the last Transformer layer\nof the teacher, which is effective and Ô¨Çexible for\nthe student. Furthermore, we introduce the scaled\ndot-product between values in the self-attention\nmodule as the new deep self-attention knowledge,\nin addition to the attention distributions (i.e., the\nscaled dot-product of queries and keys) that have\nbeen used in existing works. Moreover, we show\nthat introducing a teacher assistant (Mirzadeh\net al., 2019) also helps the distillation of large\npre-trained Transformer models. Experimental\nresults demonstrate that our monolingual model1\noutperforms state-of-the-art baselines in different\nparameter size of student models. In particular, it\nretains more than 99% accuracy on SQuAD 2.0\nand several GLUE benchmark tasks using 50%\nof the Transformer parameters and computations\nof the teacher model. We also obtain competitive\nresults in applying deep self-attention distillation\nto multilingual pre-trained models.\nCorrespondence to: Furu Wei <fuwei@microsoft.com>.\n1The code and models are publicly available at https://\naka.ms/minilm.\n1. Introduction\nLanguage model (LM) pre-training has achieved remarkable\nsuccess for various natural language processing tasks (Pe-\nters et al., 2018; Howard & Ruder, 2018; Radford et al.,\n2018; Devlin et al., 2018; Dong et al., 2019; Yang et al.,\n2019; Joshi et al., 2019; Liu et al., 2019). The pre-trained\nlanguage models, such as BERT (Devlin et al., 2018) and its\nvariants, learn contextualized text representations by predict-\ning words given their context using large scale text corpora,\nand can be Ô¨Åne-tuned with additional task-speciÔ¨Åc layers to\nadapt to downstream tasks. However, these models usually\ncontain hundreds of millions of parameters which brings\nchallenges for Ô¨Åne-tuning and online serving in real-life\napplications for latency and capacity constraints.\nKnowledge distillation (Hinton et al., 2015; Romero et al.,\n2015) (KD) has been proven to be a promising way to com-\npress a large model (called the teacher model) into a small\nmodel (called the student model), which uses much fewer\nparameters and computations while achieving competitive\nresults on downstream tasks. There have been some works\nthat task-speciÔ¨Åcally distill pre-trained large LMs into small\nmodels (Tang et al., 2019; Turc et al., 2019b; Sun et al.,\n2019a; Aguilar et al., 2019). They Ô¨Årst Ô¨Åne-tune the pre-\ntrained LMs on speciÔ¨Åc tasks and then perform distillation.\nTask-speciÔ¨Åc distillation is effective, but Ô¨Åne-tuning large\npre-trained models is still costly, especially for large datasets.\nDifferent from task-speciÔ¨Åc distillation, task-agnostic LM\ndistillation mimics the behavior of the original pre-trained\nLMs and the student model can be directly Ô¨Åne-tuned on\ndownstream tasks (Tsai et al., 2019; Sanh et al., 2019; Jiao\net al., 2019; Sun et al., 2019b).\nPrevious works use soft target probabilities for masked lan-\nguage modeling predictions or intermediate representations\nof the teacher LM to guide the training of the task-agnostic\nstudent. DistilBERT (Sanh et al., 2019) employs a soft-label\ndistillation loss and a cosine embedding loss, and initializes\nthe student from the teacher by taking one layer out of two.\nBut each Transformer layer of the student is required to\nhave the same architecture as its teacher. TinyBERT (Jiao\net al., 2019) and MOBILE BERT (Sun et al., 2019b) utilize\narXiv:2002.10957v2  [cs.CL]  6 Apr 2020\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nmore Ô¨Åne-grained knowledge, including hidden states and\nself-attention distributions of Transformer networks, and\ntransfer these knowledge to the student model layer-to-layer.\nTo perform layer-to-layer distillation, TinyBERT adopts a\nuniform function to determine the mapping between the\nteacher and student layers, and uses a parameter matrix to\nlinearly transform student hidden states. MOBILE BERT\nassumes the teacher and student have the same number of\nlayers and introduces the bottleneck module to keep their\nhidden size the same.\nIn this work, we propose the deep self-attention distillation\nframework for task-agnostic Transformer based LM distil-\nlation. The key idea is to deeply mimic the self-attention\nmodules which are the fundamentally important compo-\nnents in the Transformer based teacher and student models.\nSpeciÔ¨Åcally, we propose distilling the self-attention module\nof the last Transformer layer of the teacher model. Com-\npared with previous approaches, using knowledge of the\nlast Transformer layer rather than performing layer-to-layer\nknowledge distillation alleviates the difÔ¨Åculties in layer map-\nping between the teacher and student models, and the layer\nnumber of our student model can be more Ô¨Çexible. Further-\nmore, we introduce the scaled dot-product between values\nin the self-attention module as the new deep self-attention\nknowledge, in addition to the attention distributions (i.e., the\nscaled dot-product of queries and keys) that has been used\nin existing works. Using scaled dot-product between self-\nattention values also converts representations of different\ndimensions into relation matrices with the same dimensions\nwithout introducing additional parameters to transform stu-\ndent representations, allowing arbitrary hidden dimensions\nfor the student model. Finally, we show that introducing a\nteacher assistant (Mirzadeh et al., 2019) helps the distilla-\ntion of large pre-trained Transformer based models and the\nproposed deep self-attention distillation can further boost\nthe performance.\nWe conduct extensive experiments on downstream NLP\ntasks. Experimental results demonstrate that our monolin-\ngual model outperforms state-of-the-art baselines in dif-\nferent parameter size of student models. SpeciÔ¨Åcally, the\n6-layer model of 768 hidden dimensions distilled from\nBERTBASE is 2.0√ófaster, while retaining more than99% ac-\ncuracy on SQuAD 2.0 and several GLUE benchmark tasks.\nMoreover, our multilingual model distilled from XLM-RBase\nalso achieves competitive performance with much fewer\nTransformer parameters.\n2. Preliminary\nMulti-layer Transformers (Vaswani et al., 2017) have been\nthe most widely-used network structures in state-of-the-art\npre-trained models. In this section, we present a brief intro-\nduction to the Transformer network and the self-attention\nmechanism, which is the core component of the Transformer.\nWe also present the existing approaches on knowledge distil-\nlation for Transformer networks, particularly in the context\nof distilling a large Transformer based pre-trained model\ninto a small Transformer model.\n2.1. Input Representation\nTexts are tokenized to subword units by WordPiece (Wu\net al., 2016) in BERT (Devlin et al., 2018). For example, the\nword ‚Äúforecasted‚Äù is split to ‚Äúforecast‚Äù and ‚Äú##ed‚Äù, where\n‚Äú##‚Äù indicates the pieces are belong to one word. A spe-\ncial boundary token [SEP] is used to separate segments\nif the input text contains more than one segment. At the\nbeginning of the sequence, a special token [CLS] is added\nto obtain the representation of the whole input. The vec-\ntor representations ({xi}|x|\ni=1) of input tokens are computed\nvia summing the corresponding token embedding, absolute\nposition embedding, and segment embedding.\n2.2. Backbone Network: Transformer\nTransformer (Vaswani et al., 2017) is used to encode contex-\ntual information for input tokens. The input vectors {xi}|x|\ni=1\nare packed together intoH0 = [x1,¬∑¬∑¬∑ ,x|x|]. Then stacked\nTransformer blocks compute the encoding vectors as:\nHl = Transformerl(Hl‚àí1), l‚àà[1,L] (1)\nwhere Lis the number of Transformer layers, and the Ô¨Ånal\noutput is HL = [hL\n1 ,¬∑¬∑¬∑ ,hL\n|x|]. The hidden vector hL\ni is\nused as the contextualized representation of xi. Each Trans-\nformer layer consists of a self-attention sub-layer and a fully\nconnected feed-forward network. Residual connection (He\net al., 2016) is employed around each of the two sub-layers,\nfollowed by layer normalization (Ba et al., 2016).\nSelf-Attention In each layer, Transformer uses multiple\nself-attention heads to aggregate the output vectors of the\nprevious layer. For the l-th Transformer layer, the output of\na self-attention head AOl,a, a‚àà[1,Ah] is computed via:\nQl,a = Hl‚àí1WQ\nl,a, Kl,a = Hl‚àí1WK\nl,a, Vl,a = Hl‚àí1WV\nl,a\n(2)\nAl,a = softmax(\nQl,aK‚ä∫\nl,a‚àödk\n) (3)\nAOl,a = Al,aVl,a (4)\nwhere the previous layer‚Äôs output Hl‚àí1 ‚ààR|x|√ódh is lin-\nearly projected to a triple of queries, keys and values using\nparameter matrices WQ\nl,a,WK\nl,a,WV\nl,a ‚ààRdh√ódk , respec-\ntively. Al,a ‚ààR|x|√ó|x|indicates the attention distributions,\nwhich is computed by the scaled dot-product of queries\nand keys. Ah represents the number of self-attention heads.\ndk √óAh is equal to the hidden dimension dh in BERT.\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTransformer Block L\nTransformer Block 3\nTransformer Block 2\n‚Ä¶\nTransformer Block 4\nùë•1 ùë•2 ùë•5ùë•3 ùë•4\n‚Ä¶\nQueries\n(‚Ñùùê¥‚Ñé√ó ùë• √óùëëùëò)\n‚Ä¶\nKeys\n(‚Ñùùê¥‚Ñé√ó ùë• √óùëëùëò)\n‚Ä¶\nTeacher Last Layer\nSelf-Attention Vectors\nValues-Values\nScaled Dot-Product\n(‚Ñùùê¥‚Ñé√ó ùë• √ó|ùë•|)\nQueries-Keys\nScaled Dot-Product\n(‚Ñùùê¥‚Ñé√ó ùë• √ó|ùë•|)\nAttention \nTransfer\nQueries-Keys\nScaled Dot-Product\n(‚Ñùùê¥‚Ñé√ó ùë• √ó|ùë•|)\nValue-Relation\nTransfer\nValues-Values\nScaled Dot-Product\n(‚Ñùùê¥‚Ñé√ó ùë• √ó|ùë•|)\n‚Ä¶\n‚Ä¶\n‚Ä¶\nLast Layer\nSelf-Attention Vectors\nTransformer Block M\nTransformer Block 3\nTransformer Block 2\n‚Ä¶\nStudent\nValues\n(‚Ñùùê¥‚Ñé√ó ùë• √óùëëùëò)\nQueries\n(‚Ñùùê¥‚Ñé√ó ùë• √óùëëùëò\n‚Ä≤\n)\nKeys\n(‚Ñùùê¥‚Ñé√ó ùë• √óùëëùëò\n‚Ä≤\n)\nValues\n(‚Ñùùê¥‚Ñé√ó ùë• √óùëëùëò\n‚Ä≤\n) ùë•1 ùë•2 ùë•5ùë•3 ùë•4\n(KL-Divergence)\n(KL-Divergence)\nTransformer Block 1\nTransformer Block 1\nFigure 1.Overview of Deep Self-Attention Distillation. The student is trained by deeply mimicking the self-attention behavior of the last\nTransformer layer of the teacher. In addition to the self-attention distributions, we introduce the self-attention value-relation transfer to\nhelp the student achieve a deeper mimicry. Our student models are named as MINI LM.\n2.3. Transformer Distillation\nKnowledge distillation (Hinton et al., 2015; Romero et al.,\n2015) is to train the small student model S on a transfer\nfeature set with soft labels and intermediate representations\nprovided by the large teacher model T. Knowledge distil-\nlation is modeled as minimizing the differences between\nteacher and student features:\nLKD =\n‚àë\ne‚ààD\nL(fS(e),fT (e)) (5)\nWhere Ddenotes the training data, fS(¬∑) and fT (¬∑) indicate\nthe features of student and teacher models respectively, L(¬∑)\nrepresents the loss function. The mean squared error (MSE)\nand KL-divergence are often used as loss functions.\nFor Transformer based LM distillation, soft target probabili-\nties for masked language modeling predictions, embedding\nlayer outputs, self-attention distributions and outputs (hid-\nden states) of each Transformer layer of the teacher model\nare used as features to help the training of the student. Soft\nlabels and embedding layer outputs are used in DistillBERT.\nTinyBERT and MOBILE BERT further utilize self-attention\ndistributions and outputs of each Transformer layer. For\nMOBILE BERT, the student is required to have the same\nnumber of layers as its teacher to perform layer-to-layer dis-\ntillation. Besides, bottleneck and inverted bottleneck mod-\nules are introduced to keep the hidden size of the teacher\nand student are also the same. To transfer knowledge layer-\nto-layer, TinyBERT employs a uniform-function to map\nteacher and student layers. Since the hidden size of the\nstudent can be smaller than its teacher, a parameter matrix\nis introduced to transform the student features.\n3. Deep Self-Attention Distillation\nFigure 1 gives an overview of the deep self-attention dis-\ntillation. The key idea is three-fold. First, we propose to\ntrain the student by deeply mimicking the self-attention\nmodule, which is the vital component in the Transformer,\nof the teacher‚Äôs last layer. Second, we introduce transfer-\nring the relation between values (i.e., the scaled dot-product\nbetween values) to achieve a deeper mimicry, in addition\nto performing attention distributions (i.e., the scaled dot-\nproduct of queries and keys) transfer in the self-attention\nmodule. Moreover, we show that introducing a teacher as-\nsistant (Mirzadeh et al., 2019) also helps the distillation of\nlarge pre-trained Transformer models when the size gap\nbetween the teacher model and student model is large.\n3.1. Self-Attention Distribution Transfer\nThe attention mechanism (Bahdanau et al., 2015) has been a\nhighly successful neural network component for NLP tasks,\nwhich is also crucial for pre-trained LMs. Some works show\nthat self-attention distributions of pre-trained LMs capture\na rich hierarchy of linguistic information (Jawahar et al.,\n2019; Clark et al., 2019). Transferring self-attention distri-\nbutions has been used in previous works for Transformer\ndistillation (Jiao et al., 2019; Sun et al., 2019b; Aguilar et al.,\n2019). We also utilize the self-attention distributions to help\nthe training of the student. SpeciÔ¨Åcally, we minimize the\nKL-divergence between the self-attention distributions of\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 1.Comparison with previous task-agnostic Transformer based LM distillation approaches.\nApproach Teacher Model Distilled Knowledge Layer-to-Layer\nDistillation\nRequirements on\nthe number of\nlayers of students\nRequirements on\nthe hidden\nsize of students\nDistillBERT BERT BASE\nSoft target probabilities\nEmbedding outputs ‚úì\nTinyBERT BERT BASE\nEmbedding outputs\nHidden states\nSelf-Attention distributions\n‚úì\nMOBILEBERT IB-BERTLARGE\nSoft target probabilities\nHidden states\nSelf-Attention distributions\n‚úì ‚úì ‚úì\nMINILM BERT BASE\nSelf-Attention distributions\nSelf-Attention value relation\nthe teacher and student:\nLAT = 1\nAh|x|\nAh‚àë\na=1\n|x|‚àë\nt=1\nDKL(AT\nL,a,t ‚à•AS\nM,a,t) (6)\nWhere |x|and Ah represent the sequence length and the\nnumber of attention heads. Land M represent the number\nof layers for the teacher and student. AT\nL and AS\nM are the\nattention distributions of the last Transformer layer for the\nteacher and student, respectively. They are computed by the\nscaled dot-product of queries and keys.\nDifferent from previous works which transfer teacher‚Äôs\nknowledge layer-to-layer, we only use the attention maps\nof the teacher‚Äôs last Transformer layer. Distilling attention\nknowledge of the last Transformer layer allows more Ô¨Çexi-\nbility for the number of layers of our student models, avoids\nthe effort of Ô¨Ånding the best layer mapping.\n3.2. Self-Attention Value-Relation Transfer\nIn addition to the attention distributions, we propose using\nthe relation between values in the self-attention module\nto guide the training of the student. The value relation is\ncomputed via the multi-head scaled dot-product between\nvalues. The KL-divergence between the value relation of\nthe teacher and student is used as the training objective:\nVRT\nL,a = softmax(\nVT\nL,aVT‚ä∫\nL,a‚àödk\n) (7)\nVRS\nM,a = softmax(\nVS\nM,aVS‚ä∫\nM,a‚àö\nd‚Ä≤\nk\n) (8)\nLVR = 1\nAh|x|\nAh‚àë\na=1\n|x|‚àë\nt=1\nDKL(VRT\nL,a,t ‚à•VRS\nM,a,t) (9)\nWhere VT\nL,a ‚ààR|x|√ódk and VS\nM,a ‚ààR|x|√ód‚Ä≤\nk are the values\nof an attention head in self-attention module for the teacher‚Äôs\nand student‚Äôs last Transformer layer.VRT\nL ‚ààRAh√ó|x|√ó|x|\nand VRS\nM ‚ààRAh√ó|x|√ó|x|are the value relation of the last\nTransformer layer for teacher and student, respectively.\nThe training loss is computed via summing the attention\ndistribution transfer loss and value-relation transfer loss:\nL= LAT + LVR (10)\nIntroducing the relation between values enables the stu-\ndent to deeply mimic the teacher‚Äôs self-attention behavior.\nMoreover, using the scaled dot-product converts vectors of\ndifferent hidden dimensions into the relation matrices with\nthe same size, which allows our students to use more Ô¨Çex-\nible hidden dimensions and avoids introducing additional\nparameters to transform the student‚Äôs representations.\n3.3. Teacher Assistant\nFollowing Mirzadeh et al. (2019), we introduce a teacher\nassistant (i.e., intermediate-size student model) to further\nimprove the model performance of smaller students.\nAssuming the teacher model consists ofL-layer Transformer\nwith dh hidden size, the student model has M-layer Trans-\nformer with d‚Ä≤\nh hidden size. For smaller students (M ‚â§1\n2 L,\nd‚Ä≤\nh ‚â§1\n2 dh), we Ô¨Årst distill the teacher into a teacher assistant\nwith L-layer Transformer and d‚Ä≤\nh hidden size. The assistant\nmodel is then used as the teacher to guide the training of the\nÔ¨Ånal student. The introduction of a teacher assistant bridges\nthe size gap between teacher and smaller student models,\nhelps the distillation of Transformer based pre-trained LMs.\nMoreover, combining deep self-attention distillation with\na teacher assistant brings further improvements for smaller\nstudent models.\n3.4. Comparison with Previous Work\nTable 1 presents the comparison with previous ap-\nproaches (Sanh et al., 2019; Jiao et al., 2019; Sun et al.,\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 2. Comparison between the publicly released 6-layer models with 768 hidden size distilled from BERTBASE. We compare task-\nagnostic distilled models without task-speciÔ¨Åc distillation and data augmentation. We report F1 for SQuAD 2.0, and accuracy for other\ndatasets. The GLUE results of DistillBERT are taken from Sanh et al. (2019). We report the SQuAD 2.0 result by Ô¨Åne-tuning their\nreleased model3. For TinyBERT, we Ô¨Åne-tune the latest version of their public model4 for a fair comparison. The results of our Ô¨Åne-tuning\nexperiments are an average of 4 runs for each task.\nModel #Param SQuAD2 MNLI-m SST-2 QNLI CoLA RTE MRPC QQP Average\nBERTBASE 109M 76.8 84.5 93.2 91.7 58.9 68.6 87.3 91.3 81.5\nDistillBERT 66M 70.7 79.0 90.7 85.3 43.6 59.9 87.5 84.9 75.2\nTinyBERT 66M 73.1 83.5 91.6 90.5 42.8 72.2 88.4 90.6 79.1\nMINILM 66M 76.4 84.0 92.0 91.0 49.2 71.5 88.4 91.0 80.4\n2019b). MOBILE BERT proposes using a specially de-\nsigned inverted bottleneck model, which has the same model\nsize as BERTLARGE, as the teacher. The other methods uti-\nlize BERTBASE to conduct experiments. For the knowledge\nused for distillation, our method introduces the scaled dot-\nproduct between values in the self-attention module as the\nnew knowledge to deeply mimic teacher‚Äôs self-attention be-\nhavior. TinyBERT and MOBILE BERT transfer knowledge\nof the teacher to the student layer-to-layer. MOBILE BERT\nassumes the student has the same number of layers as its\nteacher. TinyBERT employs a uniform strategy to deter-\nmine its layer mapping. DistillBERT initializes the student\nwith teacher‚Äôs parameters, therefore selecting layers of the\nteacher model is still needed. MINI LM distills the self-\nattention knowledge of the teacher‚Äôs last Transformer layer,\nwhich allows the Ô¨Çexible number of layers for the students\nand alleviates the effort of Ô¨Ånding the best layer mapping.\nStudent hidden size of DistillBERT and MOBILE BERT is\nrequired to be the same as its teacher. TinyBERT uses a\nparameter matrix to transform student hidden states. Using\nvalue relation allows our students to use arbitrary hidden\nsize without introducing additional parameters.\n4. Experiments\nWe conduct distillation experiments in different parameter\nsize of student models, and evaluate the distilled models on\ndownstream tasks including extractive question answering\nand the GLUE benchmark.\n4.1. Distillation Setup\nWe use the uncased version of BERT BASE as our teacher.\nBERTBASE (Devlin et al., 2018) is a 12-layer Transformer\nwith 768 hidden size, and 12 attention heads, which contains\nabout 109M parameters. The number of heads of attention\ndistributions and value relation are set to 12 for student\nmodels. We use documents of English Wikipedia 2 and\nBookCorpus (Zhu et al., 2015) for the pre-training data,\nfollowing the preprocess and the WordPiece tokenization\n2Wikipedia version: enwiki-20181101.\nof Devlin et al. (2018). The vocabulary size is 30,522. The\nmaximum sequence length is512. We use Adam (Kingma &\nBa, 2015) with Œ≤1 = 0.9, Œ≤2 = 0.999. We train the 6-layer\nstudent model with 768 hidden size using 1024 as the batch\nsize and 5e-4 as the peak learning rate for 400,000 steps.\nFor student models of other architectures, the batch size\nand peak learning rate are set to 256 and 3e-4, respectively.\nWe use linear warmup over the Ô¨Årst 4,000 steps and linear\ndecay. The dropout rate is 0.1. The weight decay is 0.01.\nWe also use an in-house pre-trained Transformer model\nin the BERTBASE size as the teacher model, and distill it\ninto 12-layer and 6-layer student models with 384 hidden\nsize. For the 12-layer model, we use Adam (Kingma & Ba,\n2015) with Œ≤1 = 0.9, Œ≤2 = 0.98. The model is trained\nusing 2048 as the batch size and 6e-4 as the peak learning\nrate for 400,000 steps. The batch size and peak learning\nrate are set to 512 and 4e-4 for the 6-layer model. The\nrest hyper-parameters are the same as above BERT based\ndistilled models.\nFor the training of multilingual MINI LM models, we use\nAdam (Kingma & Ba, 2015) with Œ≤1 = 0.9, Œ≤2 = 0.999.\nWe train the 12-layer student model using 256 as the batch\nsize and 3e-4 as the peak learning rate for 1,000,000 steps.\nThe 6-layer student model is trained using 512 as the batch\nsize and 6e-4 as the peak learning rate for 400,000 steps.\nWe distill our student models using 8 V100 GPUs with\nmixed precision training. Following Sun et al. (2019a)\nand Jiao et al. (2019), the inference time is evaluated on\nthe QNLI training set with the same hyper-parameters. We\nreport the average running time of 100 batches on a single\nP100 GPU.\n4.2. Downstream Tasks\nFollowing previous language model pre-training (Devlin\net al., 2018; Liu et al., 2019) and task-agnostic pre-trained\nlanguage model distillation (Sanh et al., 2019; Jiao et al.,\n2019; Sun et al., 2019b), we evaluate our distilled models\non the extractive question answering and GLUE benchmark.\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 3. Comparison between student models of different architectures distilled from BERTBASE. M and d‚Ä≤\nh indicate the number of layers\nand hidden dimension of the student model. TA indicates teacher assistant5. The Ô¨Åne-tuning results are averaged over 4 runs.\nArchitecture #Param Model SQuAD 2.0 MNLI-m SST-2 Average\nM=6;d‚Ä≤\nh=384 22M\nMLM-KD (Soft-Label Distillation) 67.9 79.6 89.8 79.1\nTinyBERT 71.6 81.4 90.2 81.1\nMINILM 72.4 82.2 91.0 81.9\nMINILM (w/ TA) 72.7 82.4 91.2 82.1\nM=4;d‚Ä≤\nh=384 19M\nMLM-KD (Soft-Label Distillation) 65.3 77.7 88.8 77.3\nTinyBERT 66.7 79.2 88.5 78.1\nMINILM 69.4 80.3 90.2 80.0\nMINILM (w/ TA) 69.7 80.6 90.6 80.3\nM=3;d‚Ä≤\nh=384 17M\nMLM-KD (Soft-Label Distillation) 59.9 75.2 88.0 74.4\nTinyBERT 63.6 77.4 88.4 76.5\nMINILM 66.2 78.8 89.3 78.1\nMINILM (w/ TA) 66.9 79.1 89.7 78.6\nTable 4.The number of Embedding (Emd) and Transformer (Trm)\nparameters, and inference time for different models.\n#Layers Hidden\nSize\n#Param\n(Emd)\n#Param\n(Trm)\nInference\nTime\n12 768 23.4M 85.1M 93.1s (1.0 √ó)\n6 768 23.4M 42.5M 46.9s (2.0 √ó)\n12 384 11.7M 21.3M 34.8s (2.7 √ó)\n6 384 11.7M 10.6M 17.7s (5.3 √ó)\n4 384 11.7M 7.1M 12.0s (7.8 √ó)\n3 384 11.7M 5.3M 9.2s (10.1 √ó)\nExtractive Question Answering Given a passage P, the\ntask is to select a contiguous span of text in the passage by\npredicting its start and end positions to answer the question\nQ. We evaluate on SQuAD 2.0 (Rajpurkar et al., 2018),\nwhich has served as a major question answering benchmark.\nFollowing BERT (Devlin et al., 2018), we pack the question\nand passage tokens together with special tokens, to form the\ninput: ‚Äú[CLS] Q[SEP] P [SEP]\". Two linear output\nlayers are introduced to predict the probability of each token\nbeing the start and end positions of the answer span. The\nquestions that do not have an answer are treated as having\nan answer span with start and end at the [CLS] token.\nGLUE The General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2019) consists of nine\nsentence-level classiÔ¨Åcation tasks, including Corpus of Lin-\nguistic Acceptability (CoLA) (Warstadt et al., 2018), Stan-\nford Sentiment Treebank (SST) (Socher et al., 2013), Mi-\ncrosoft Research Paraphrase Corpus (MRPC) (Dolan &\nBrockett, 2005), Semantic Textual Similarity Benchmark\n(STS) (Cer et al., 2017), Quora Question Pairs (QQP) (Chen\net al., 2018), Multi-Genre Natural Language Inference\n(MNLI) (Williams et al., 2018), Question Natural Language\nInference (QNLI) (Rajpurkar et al., 2016), Recognizing Tex-\ntual Entailment (RTE) (Dagan et al., 2006; Bar-Haim et al.,\n2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and\nWinograd Natural Language Inference (WNLI) (Levesque\net al., 2012). We add a linear classiÔ¨Åer on top of the [CLS]\ntoken to predict label probabilities.\n4.3. Main Results\nPrevious works (Sanh et al., 2019; Sun et al., 2019a; Jiao\net al., 2019) usually distill BERTBASE into a 6-layer student\nmodel with 768 hidden size. We Ô¨Årst conduct distillation\nexperiments using the same student architecture. Results on\nSQuAD 2.0 and GLUE dev sets are presented in Table 2.\nSince MOBILE BERT distills a specially designed teacher\nwith the inverted bottleneck modules, which has the same\nmodel size as BERT LARGE, into a 24-layer student using\nthe bottleneck modules, we do not compare our models\nwith MOBILE BERT. MINI LM outperforms DistillBERT3\nand TinyBERT4 across most tasks. Our model exceeds the\ntwo state-of-the-art models by3.0+% F1 on SQuAD 2.0 and\n5.0+% accuracy on CoLA. We present the inference time for\nmodels in different parameter size in Table 4. Our 6-layer\n768-dimensional student model is 2.0√ófaster than original\nBERTBASE, while retaining more than 99% performance on\na variety of tasks, such as SQuAD 2.0 and MNLI.\nWe also conduct experiments for smaller student models.\nWe compare MINI LM with our implemented MLM-KD\n3The public model of DistillBERT is obtained from https:\n//github.com/huggingface/transformers/tree/\nmaster/examples/distillation\n4We use the 2nd version TinyBERT fromhttps://github.\ncom/huawei-noah/Pretrained-Language-Model/\ntree/master/TinyBERT\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 5. Effectiveness of self-attention value-relation (Value-Rel)\ntransfer. The Ô¨Åne-tuning results are averaged over 4 runs.\nArchitecture Model SQuAD2 MNLI-m SST-2\nM=6;d‚Ä≤h=384 MINILM 72.4 82.2 91.0\n-Value-Rel 71.0 80.9 89.9\nM=4;d‚Ä≤h=384 MINILM 69.4 80.3 90.2\n-Value-Rel 67.5 79.0 89.2\nM=3;d‚Ä≤h=384 MINILM 66.2 78.8 89.3\n-Value-Rel 64.2 77.8 88.3\n(knowledge distillation using soft target probabilities for\nmasked language modeling predictions) and TinyBERT,\nwhich are trained using the same data and hyper-parameters.\nThe results on SQuAD 2.0, MNLI and SST-2 dev sets are\nshown in Table 3. MINI LM outperforms soft label distil-\nlation and our implemented TinyBERT on the three tasks.\nDeep self-attention distillation is also effective for smaller\nmodels. Moreover, we show that introducing a teacher as-\nsistant5 is also helpful in Transformer based pre-trained LM\ndistillation, especially for smaller models. Combining deep\nself-attention distillation with a teacher assistant achieves\nfurther improvement for smaller student models.\n4.4. Ablation Studies\nWe do ablation tests on several tasks to analyze the contribu-\ntion of self-attention value-relation transfer. The dev results\nof SQuAD 2.0, MNLI and SST-2 are illustrated in Table 5,\nusing self-attention value-relation transfer positively con-\ntributes to the Ô¨Ånal results for student models in different\nparameter size. Distilling the Ô¨Åne-grained knowledge of\nvalue relation helps the student model deeply mimic the self-\nattention behavior of the teacher, which further improves\nmodel performance.\nWe also compare different loss functions over values in the\nself-attention module. We compare our proposed value re-\nlation with mean squared error (MSE) over the teacher and\nstudent values. An additional parameter matrix is introduced\nto transform student values if the hidden dimension of the\nstudent is smaller than its teacher. The dev results on three\ntasks are presented in Table 6. Using value relation achieves\nbetter performance. SpeciÔ¨Åcally, our method brings about\n1.0% F1 improvement on the SQuAD benchmark. More-\nover, there is no need to introduce additional parameters for\nour method. We have also tried to transfer the relation be-\ntween hidden states. But we Ô¨Ånd the performance of student\nmodels are unstable for different teacher models.\nTo show the effectiveness of distilling self-attention knowl-\n5The teacher assistant is only introduced for the model\nMINI LM (w/ TA). The model MINI LM in different tables is di-\nrectly distilled from its teacher model.\nTable 6. Comparison between different loss functions: KL-\ndivergence over the value relation (the scaled dot-product between\nvalues) and mean squared error (MSE) over values. A parameter\nmatrix is introduced to transform student values to have the same\ndimensions as the teacher values (Jiao et al., 2019). The Ô¨Åne-tuning\nresults are an average of 4 runs for each task.\nArchitecture Model SQuAD2 MNLI-m SST-2\nM=6;d‚Ä≤h=384 MINILM 72.4 82.2 91.0\nValue-MSE 71.4 82.0 90.8\nM=4;d‚Ä≤h=384 MINILM 69.4 80.3 90.2\nValue-MSE 68.3 80.1 89.9\nM=3;d‚Ä≤h=384 MINILM 66.2 78.8 89.3\nValue-MSE 65.5 78.4 89.3\nedge of the teacher‚Äôs last Transformer layer, we compare\nour method with layer-to-layer distillation. We transfer the\nsame knowledge and adopt a uniform strategy as in Jiao\net al. (2019) to map teacher and student layers to perform\nlayer-to-layer distillation. The dev results on three tasks are\npresented in Table 7. MINI LM achieves better results. It\nalso alleviates the difÔ¨Åculties in layer mapping between the\nteacher and student. Besides, distilling the teacher‚Äôs last\nTransformer layer requires less computation than layer-to-\nlayer distillation, results in faster training speed.\n5. Discussion\n5.1. Better Teacher Better Student\nWe report the results of MINI LM distilled from an in-house\npre-trained Transformer model following UNILM (Dong\net al., 2019; Bao et al., 2020) in the BERT BASE size. The\nteacher model is trained using similar pre-training datasets\nas in RoBERTa BASE (Liu et al., 2019), which includes\n160GB text corpora from English Wikipedia, BookCor-\npus (Zhu et al., 2015), OpenWebText6, CC-News (Liu et al.,\n2019), and Stories (Trinh & Le, 2018). We distill the teacher\nmodel into 12-layer and 6-layer models with 384 hidden\nsize using the same corpora. The 12x384 model is used as\nthe teacher assistant to train the 6x384 model. We present\nthe dev results of SQuAD 2.0 and GLUE benchmark in\nTable 8, the results of MINI LM are signiÔ¨Åcantly improved.\nThe 12x384 MINI LM achieves 2.7√óspeedup while per-\nforms competitively better than BERTBASE in SQuAD 2.0\nand GLUE benchmark datasets.\n5.2. MINI LM for NLG Tasks\nWe also evaluate MINI LM on natural language generation\ntasks, such as question generation and abstractive sum-\nmarization. Following Dong et al. (2019), we Ô¨Åne-tune\n6skylion007.github.io/OpenWebTextCorpus\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 7. Comparison between distilling knowledge of the teacher‚Äôs last Transformer layer and layer-to-layer distillation. We adopt a\nuniform strategy as in Jiao et al. (2019) to determine the mapping between teacher and student layers. The Ô¨Åne-tuning results are an\naverage of 4 runs for each task.\nArchitecture Model SQuAD 2.0 MNLI-m SST-2 Average\nM=6;d‚Ä≤\nh=384 MINILM 72.4 82.2 91.0 81.9\n+Layer-to-Layer Distillation 71.6 81.8 90.6 81.3\nM=4;d‚Ä≤\nh=384 MINILM 69.4 80.3 90.2 80.0\n+Layer-to-Layer Distillation 67.6 79.9 89.6 79.0\nM=3;d‚Ä≤\nh=384 MINILM 66.2 78.8 89.3 78.1\n+Layer-to-Layer Distillation 64.8 77.7 88.6 77.0\nTable 8. The results of MINI LM distilled from an in-house pre-trained Transformer model (BERT BASE size, 12-layer Transformer,\n768-hidden size, and 12 self-attention heads) on SQuAD 2.0 and GLUE benchmark. We report our 12-layera and 6-layerb models with\n384 hidden size. The Ô¨Åne-tuning results are averaged over 4 runs.\nModel #Param SQuAD2 MNLI-m SST-2 QNLI CoLA RTE MRPC QQP Average\nBERTBASE 109M 76.8 84.5 93.2 91.7 58.9 68.6 87.3 91.3 81.5\nMINILMa 33M 81.7 85.7 93.0 91.5 58.5 73.3 89.5 91.3 83.1\nMINILMb (w/ TA) 22M 75.6 83.3 91.5 90.5 47.5 68.8 88.9 90.6 79.6\nTable 9.Question generation results of our 12-layera and 6-layerb\nmodels with 384 hidden size on SQuAD 1.1. The Ô¨Årst block\nfollows the data split in Du & Cardie (2018), while the second\nblock is the same as in Zhao et al. (2018). MTR is short for\nMETEOR, RG for ROUGE, and B for BLEU.\n#Param B-4 MTR RG-L\n(Du & Cardie, 2018) 15.16 19.12 -\n(Zhang & Bansal, 2019) 18.37 22.65 46.68\nUNILMLARGE 340M 22.78 25.49 51.57\nMINILMa 33M 21.07 24.09 49.14\nMINILMb (w/ TA) 22M 20.31 23.43 48.21\n(Zhao et al., 2018) 16.38 20.25 44.48\n(Zhang & Bansal, 2019) 20.76 24.20 48.91\nUNILMLARGE 340M 24.32 26.10 52.69\nMINILMa 33M 23.27 25.15 50.60\nMINILMb (w/ TA) 22M 22.01 24.24 49.51\nMINI LM as a sequence-to-sequence model by employing a\nspeciÔ¨Åc self-attention mask.\nQuestion Generation We conduct experiments for the\nanswer-aware question generation task (Du & Cardie, 2018).\nGiven an input passage and an answer, the task is to gen-\nerate a question that asks for the answer. The SQuAD 1.1\ndataset (Rajpurkar et al., 2016) is used for evaluation. The\nresults of MINI LM, UNILMLARGE and several state-of-the-\nart models are presented in Table 9, our 12x384 and 6x384\ndistilled models achieve competitive performance on the\nquestion generation task.\nAbstractive Summarization We evaluate MINI LM\non two abstractive summarization datasets, i.e.,\nXSum (Narayan et al., 2018), and the non-anonymized\nversion of CNN/DailyMail (See et al., 2017). The\ngeneration task is to condense a document into a concise\nand Ô¨Çuent summary, while conveying its key information.\nWe report ROUGE scores (Lin, 2004) on the datasets.\nTable 10 presents the results of MINI LM, baseline, several\nstate-of-the-art models and pre-trained Transformer models.\nOur 12x384 model outperforms BERT based method\nBERTS UMABS (Liu & Lapata, 2019) and the pre-trained\nsequence-to-sequence model MASS BASE (Song et al.,\n2019) with much fewer parameters. Moreover, our 6x384\nMINI LM also achieves competitive performance.\n5.3. Multilingual MINI LM\nWe conduct experiments on task-agnostic knowledge dis-\ntillation of multilingual pre-trained models. We use the\nXLM-RBase7 (Conneau et al., 2019) as the teacher and distill\nthe model into 12-layer and 6-layer models with 384 hidden\nsize using the same corpora. The 6x384 model is trained\nusing the 12x384 model as the teacher assistant. Given\nthe vocabulary size of multilingual pre-trained models is\nmuch larger than monolingual models (30k for monolin-\ngual BERT, 250k for XLM-R), soft-label distillation for\nmultilingual pre-trained models requires more computation.\nMINI LM only uses the deep self-attention knowledge of\nthe teacher‚Äôs last Transformer layer. The training speed\n7We use the v0 version of XLM-RBase in our distillation and\nÔ¨Åne-tuning experiments.\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 10. Abstractive summarization results of our 12-layera and 6-layerb models with 384 hidden size on CNN/DailyMail and XSum.\nThe evaluation metric is the F1 version of ROUGE (RG) scores.\nModel #Param CNN/DailyMail XSum\nRG-1 RG-2 RG-L RG-1 RG-2 RG-L\nLEAD -3 40.42 17.62 36.67 16.30 1.60 11.95\nPTRNET (See et al., 2017) 39.53 17.28 36.38 28.10 8.02 21.72\nBottom-Up (Gehrmann et al., 2018) 41.22 18.68 38.34 - - -\nUNILMLARGE (Dong et al., 2019) 340M 43.08 20.43 40.34 - - -\nBARTLARGE (Lewis et al., 2019a) 400M 44.16 21.28 40.90 45.14 22.27 37.25\nT511B (Raffel et al., 2019) 11B 43.52 21.55 40.69 - - -\nMASSBASE (Song et al., 2019) 123M 42.12 19.50 39.01 39.75 17.24 31.95\nBERTS UMABS (Liu & Lapata, 2019) 156M 41.72 19.39 38.76 38.76 16.33 31.15\nT5BASE (Raffel et al., 2019) 220M 42.05 20.34 39.40 - - -\nMINI LMa 33M 42.66 19.91 39.73 40.43 17.72 32.60\nMINI LMb (w/ TA) 22M 41.57 19.21 38.64 38.79 16.39 31.10\nTable 11. Cross-lingual classiÔ¨Åcation results of our 12-layera and 6-layerb multilingual models with 384 hidden size on XNLI. We\nreport the accuracy on each of the 15 XNLI languages and the average accuracy. Results of mBERT, XLM-100 and XLM-R Base are\nfrom Conneau et al. (2019).\nModel #Layers #Hiddenen fr es de el bg ru tr ar vi th zh hi sw ur Avg\nmBERT 12 768 82.1 73.8 74.3 71.1 66.4 68.9 69.0 61.6 64.9 69.5 55.8 69.3 60.0 50.4 58.0 66.3\nXLM-100 16 1280 83.2 76.7 77.7 74.0 72.7 74.1 72.7 68.7 68.6 72.9 68.9 72.5 65.6 58.2 62.4 70.7\nXLM-RBase 12 768 84.6 78.4 78.9 76.8 75.9 77.3 75.4 73.2 71.5 75.4 72.5 74.9 71.1 65.2 66.5 74.5\nMINILMa 12 384 81.5 74.8 75.7 72.9 73.0 74.5 71.3 69.7 68.8 72.1 67.8 70.0 66.2 63.3 64.2 71.1\nMINILMb (w/ TA) 6 384 79.2 72.3 73.1 70.3 69.1 72.0 69.1 64.5 64.9 69.0 66.0 67.8 62.9 59.0 60.6 68.0\nTable 12. The number of Transformer (Trm) and Embedding\n(Emd) parameters for different multilingual pre-trained models\nand our distilled models.\nModel #Layers Hidden\nSize #Vocab #Param\n(Trm)\n#Param\n(Emd)\nmBERT 12 768 110k 85M 85M\nXLM-15 12 1024 95k 151M 97M\nXLM-100 16 1280 200k 315M 256M\nXLM-RBase 12 768 250k 85M 192M\nMINILMa 12 384 250k 21M 96M\nMINILMb 6 384 250k 11M 96M\nof MINI LM is much faster than soft-label distillation for\nmultilingual pre-trained models.\nWe evaluate the student models on cross-lingual natural\nlanguage inference (XNLI) benchmark (Conneau et al.,\n2018) and cross-lingual question answering (MLQA) bench-\nmark (Lewis et al., 2019b).\nXNLI Table 11 presents XNLI results of our distilled stu-\ndents and several pre-trained LMs. Following Conneau et al.\n(2019), we select the best single model on the joint dev\nset of all the languages. We present the number of Trans-\nformer and embedding parameters for different multilingual\npre-trained models and our distilled models in Table 12.\nMINI LM achieves competitive performance on XNLI with\nmuch fewer Transformer parameters. Moreover, the 12x384\nMINI LM compares favorably with mBERT (Devlin et al.,\n2018) and XLM (Lample & Conneau, 2019) trained on the\nMLM objective.\nMLQA Table 13 shows cross-lingual question answering\nresults. Following Lewis et al. (2019b), we adopt SQuAD\n1.1 as training data and use MLQA English development\ndata for early stopping. The 12x384 MINI LM performs\ncompetitively better than mBERT and XLM. Our 6-layer\nMINI LM also achieves competitive performance.\n6. Related Work\n6.1. Pre-trained Language Models\nUnsupervised pre-training of language models (Peters et al.,\n2018; Howard & Ruder, 2018; Radford et al., 2018; Devlin\net al., 2018; Baevski et al., 2019; Song et al., 2019; Dong\net al., 2019; Yang et al., 2019; Joshi et al., 2019; Liu et al.,\n2019; Lewis et al., 2019a; Raffel et al., 2019) has achieved\nsigniÔ¨Åcant improvements for a wide range of NLP tasks.\nEarly methods for pre-training (Peters et al., 2018; Radford\net al., 2018) were based on standard language models. Re-\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 13. Cross-lingual question answering results of our 12-layera and 6-layerb multilingual models with 384 hidden size on MLQA.\nWe report the F1 and EM (exact match) scores on each of the 7 MLQA languages. Results of mBERT and XLM-15 are taken from Lewis\net al. (2019b). ‚Ä† indicates results of XLM-RBase taken from Conneau et al. (2019). We also report our Ô¨Åne-tuned results (‚Ä°) of XLM-RBase.\nModel #Layers #Hidden en es de ar hi vi zh Avg\nmBERT 12 768 77.7 / 65.2 64.3 / 46.6 57.9 / 44.3 45.7 / 29.8 43.8 / 29.7 57.1 / 38.6 57.5 / 37.357.7 / 41.6\nXLM-15 12 1024 74.9 / 62.4 68.0 / 49.8 62.2 / 47.6 54.8 / 36.3 48.8 / 27.3 61.4 / 41.8 61.1 / 39.661.6 / 43.5\nXLM-RBase‚Ä† 12 768 77.8 / 65.3 67.2 / 49.7 60.8 / 47.1 53.0 / 34.7 57.9 / 41.7 63.1 / 43.1 60.2 / 38.062.9 / 45.7\nXLM-RBase‚Ä° 12 768 80.3 / 67.4 67.0 / 49.2 62.7 / 48.3 55.0 / 35.6 60.4 / 43.7 66.5 / 45.9 62.3 / 38.364.9 / 46.9\nMINILMa 12 384 79.4 / 66.5 66.1 / 47.5 61.2 / 46.5 54.9 / 34.9 58.5 / 41.3 63.1 / 42.1 59.0 / 33.863.2 / 44.7\nMINILMb (w/ TA) 6 384 75.5 / 61.9 55.6 / 38.2 53.3 / 37.7 43.5 / 26.2 46.9 / 31.5 52.0 / 33.1 48.8 / 27.353.7 / 36.6\ncently, BERT (Devlin et al., 2018) proposes to use a masked\nlanguage modeling objective to train a deep bidirectional\nTransformer encoder, which learns interactions between left\nand right context. Liu et al. (2019) show that very strong\nperformance can be achieved by training the model longer\nover more data. Joshi et al. (2019) extend BERT by masking\ncontiguous random spans. Yang et al. (2019) predict masked\ntokens auto-regressively in a permuted order.\nTo extend the applicability of pre-trained Transformers for\nNLG tasks. Dong et al. (2019) extend BERT by utilizing spe-\nciÔ¨Åc self-attention masks to jointly optimize bidirectional,\nunidirectional and sequence-to-sequence masked language\nmodeling objectives. Raffel et al. (2019) employ an encoder-\ndecoder Transformer and perform sequence-to-sequence\npre-training by predicting the masked tokens in the encoder\nand decoder. Different from Raffel et al. (2019), Lewis et al.\n(2019a) predict tokens auto-regressively in the decoder.\n6.2. Knowledge Distillation\nKnowledge distillation has proven a promising way to com-\npress large models while maintaining accuracy. It transfers\nthe knowledge of a large model or an ensemble of neural\nnetworks (teacher) to a single lightweight model (student).\nHinton et al. (2015) Ô¨Årst propose transferring the knowledge\nof the teacher to the student by using its soft target distri-\nbutions to train the distilled model. Romero et al. (2015)\nintroduce intermediate representations from hidden layers\nof the teacher to guide the training of the student. Knowl-\nedge of the attention maps (Zagoruyko & Komodakis, 2017;\nHu et al., 2018) is also introduced to help the training.\nIn this work, we focus on task-agnostic knowledge dis-\ntillation of large pre-trained Transformer based language\nmodels. There have been some works that task-speciÔ¨Åcally\ndistill the Ô¨Åne-tuned language models on downstream tasks.\nTang et al. (2019) distill Ô¨Åne-tuned BERT into an extremely\nsmall bidirectional LSTM. Turc et al. (2019a) initialize the\nstudent with a small pre-trained LM during task-speciÔ¨Åc dis-\ntillation. Sun et al. (2019a) introduce the hidden states from\nevery klayers of the teacher to perform knowledge distilla-\ntion layer-to-layer. Aguilar et al. (2019) further introduce\nthe knowledge of self-attention distributions and propose\nprogressive and stacked distillation methods. Task-speciÔ¨Åc\ndistillation requires to Ô¨Årst Ô¨Åne-tune the large pre-trained\nLMs on downstream tasks and then perform knowledge\ntransfer. The procedure of Ô¨Åne-tuning large pre-trained LMs\nis costly and time-consuming, especially for large datasets.\nFor task-agnostic distillation, the distilled model mimics the\noriginal large pre-trained LM and can be directly Ô¨Åne-tuned\non downstream tasks. In practice, task-agnostic compres-\nsion of pre-trained LMs is more desirable. MiniBERT (Tsai\net al., 2019) uses the soft target distributions for masked\nlanguage modeling predictions to guide the training of the\nmultilingual student model and shows its effectiveness on\nsequence labeling tasks. DistillBERT (Sanh et al., 2019)\nuses the soft label and embedding outputs of the teacher to\ntrain the student. TinyBERT (Jiao et al., 2019) andMOBILE -\nBERT (Sun et al., 2019b) further introduce self-attention\ndistributions and hidden states to train the student. MOBILE -\nBERT employs inverted bottleneck and bottleneck modules\nfor teacher and student to make their hidden dimensions\nthe same. The student model of MOBILE BERT is required\nto have the same number of layers as its teacher to per-\nform layer-to-layer distillation. Besides, MOBILE BERT\nproposes a bottom-to-top progressive scheme to transfer\nteacher‚Äôs knowledge. TinyBERT uses a uniform-strategy to\nmap the layers of teacher and student when they have dif-\nferent number of layers, and a linear matrix is introduced to\ntransform the student hidden states to have the same dimen-\nsions as the teacher. TinyBERT also introduces task-speciÔ¨Åc\ndistillation and data augmentation for downstream tasks,\nwhich brings further improvements.\nDifferent from previous works, our method employs the self-\nattention distributions and value relation of the teacher‚Äôs last\nTransformer layer to help the student deeply mimic the self-\nattention behavior of the teacher. Using knowledge of the\nlast Transformer layer instead of layer-to-layer distillation\navoids restrictions on the number of student layers and the\neffort of Ô¨Ånding the best layer mapping. Distilling rela-\ntion between self-attention values allows the hidden size of\nstudents to be more Ô¨Çexible and avoids introducing linear\nmatrices to transform student representations.\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\n7. Conclusion\nIn this work, we propose a simple and effective knowledge\ndistillation method to compress large pre-trained Trans-\nformer based language models. The student is trained\nby deeply mimicking the teacher‚Äôs self-attention modules,\nwhich are the vital components of the Transformer networks.\nWe propose using the self-attention distributions and value\nrelation of the teacher‚Äôs last Transformer layer to guide the\ntraining of the student, which is effective and Ô¨Çexible for\nthe student models. Moreover, we show that introducing a\nteacher assistant also helps pre-trained Transformer based\nLM distillation, and the proposed deep self-attention distil-\nlation can further boost the performance. Our student model\ndistilled from BERTBASE retains high accuracy on SQuAD\n2.0 and the GLUE benchmark tasks, and outperforms state-\nof-the-art baselines. The deep self-attention distillation can\nalso be applied to compress pre-trained models in larger\nsize. We leave it as our future work.\nReferences\nAguilar, G., Ling, Y ., Zhang, Y ., Yao, B., Fan, X., and Guo,\nE. Knowledge distillation from internal representations.\nCoRR, abs/1910.03723, 2019. URL http://arxiv.\norg/abs/1910.03723.\nBa, L. J., Kiros, J. R., and Hinton, G. E. Layer normalization.\nCoRR, abs/1607.06450, 2016. URL http://arxiv.\norg/abs/1607.06450.\nBaevski, A., Edunov, S., Liu, Y ., Zettlemoyer, L., and Auli,\nM. Cloze-driven pretraining of self-attention networks.\narXiv preprint arXiv:1903.07785, 2019.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate. In\n3rd International Conference on Learning Representa-\ntions, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings, 2015.\nBao, H., Dong, L., Wei, F., Wang, W., Yang, N., Liu,\nX., Wang, Y ., Piao, S., Gao, J., Zhou, M., and Hon,\nH.-W. Unilmv2: Pseudo-masked language models for\nuniÔ¨Åed language model pre-training. arXiv preprint\narXiv:2002.12804, 2020.\nBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., and Giampic-\ncolo, D. The second PASCAL recognising textual entail-\nment challenge. In Proceedings of the Second PASCAL\nChallenges Workshop on Recognising Textual Entailment,\n01 2006.\nBentivogli, L., Dagan, I., Dang, H. T., Giampiccolo, D.,\nand Magnini, B. The Ô¨Åfth pascal recognizing textual\nentailment challenge. In In Proc Text Analysis Conference\n(TAC‚Äô09, 2009.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia,\nL. Semeval-2017 task 1: Semantic textual similarity-\nmultilingual and cross-lingual focused evaluation. arXiv\npreprint arXiv:1708.00055, 2017.\nChen, Z., Zhang, H., Zhang, X., and Zhao, L. Quora ques-\ntion pairs. 2018.\nClark, K., Khandelwal, U., Levy, O., and Manning, C. D.\nWhat does BERT look at? an analysis of bert‚Äôs attention.\nCoRR, abs/1906.04341, 2019. URL http://arxiv.\norg/abs/1906.04341.\nConneau, A., Lample, G., Rinott, R., Williams, A., Bowman,\nS. R., Schwenk, H., and Stoyanov, V . Xnli: Evaluating\ncross-lingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), 2018.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V .,\nWenzek, G., Guzm√°n, F., Grave, E., Ott, M., Zettlemoyer,\nL., and Stoyanov, V . Unsupervised cross-lingual repre-\nsentation learning at scale. CoRR, abs/1911.02116, 2019.\nURL http://arxiv.org/abs/1911.02116.\nDagan, I., Glickman, O., and Magnini, B. The pas-\ncal recognising textual entailment challenge. In Pro-\nceedings of the First International Conference on Ma-\nchine Learning Challenges: Evaluating Predictive Un-\ncertainty Visual Object ClassiÔ¨Åcation, and Recognizing\nTextual Entailment, MLCW‚Äô05, pp. 177‚Äì190, Berlin, Hei-\ndelberg, 2006. Springer-Verlag. ISBN 3-540-33427-0,\n978-3-540-33427-9. doi: 10.1007/11736790_9. URL\nhttp://dx.doi.org/10.1007/11736790_9.\nDevlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:\npre-training of deep bidirectional transformers for lan-\nguage understanding. CoRR, abs/1810.04805, 2018.\nDolan, W. B. and Brockett, C. Automatically construct-\ning a corpus of sentential paraphrases. In Proceedings\nof the Third International Workshop on Paraphrasing\n(IWP2005), 2005.\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y .,\nGao, J., Zhou, M., and Hon, H.-W. UniÔ¨Åed language\nmodel pre-training for natural language understanding\nand generation. In 33rd Conference on Neural Informa-\ntion Processing Systems (NeurIPS 2019), 2019.\nDu, X. and Cardie, C. Harvesting paragraph-level question-\nanswer pairs from wikipedia. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July 15-20,\n2018, Volume 1: Long Papers, pp. 1907‚Äì1917, 2018.\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nGehrmann, S., Deng, Y ., and Rush, A. Bottom-up ab-\nstractive summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pp. 4098‚Äì4109, Brussels, Belgium,\nOctober-November 2018. Association for Computational\nLinguistics. URL https://www.aclweb.org/\nanthology/D18-1443.\nGiampiccolo, D., Magnini, B., Dagan, I., and Dolan, B. The\nthird PASCAL recognizing textual entailment challenge.\nIn Proceedings of the ACL-PASCAL Workshop on Textual\nEntailment and Paraphrasing, pp. 1‚Äì9, Prague, June 2007.\nAssociation for Computational Linguistics. URLhttps:\n//www.aclweb.org/anthology/W07-1401.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016,\nLas Vegas, NV , USA, June 27-30, 2016 , pp. 770‚Äì778,\n2016.\nHinton, G. E., Vinyals, O., and Dean, J. Distilling the knowl-\nedge in a neural network. CoRR, abs/1503.02531, 2015.\nURL http://arxiv.org/abs/1503.02531.\nHoward, J. and Ruder, S. Universal language model Ô¨Åne-\ntuning for text classiÔ¨Åcation. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pp. 328‚Äì339, Mel-\nbourne, Australia, July 2018. Association for Compu-\ntational Linguistics. URL https://www.aclweb.\norg/anthology/P18-1031.\nHu, M., Peng, Y ., Wei, F., Huang, Z., Li, D., Yang, N.,\nand Zhou, M. Attention-guided answer distillation for\nmachine reading comprehension. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018, pp. 2077‚Äì2086, 2018. URL https:\n//www.aclweb.org/anthology/D18-1232/.\nJawahar, G., Sagot, B., and Seddah, D. What does BERT\nlearn about the structure of language? In Proceedings\nof the 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pp. 3651‚Äì\n3657, 2019. URL https://www.aclweb.org/\nanthology/P19-1356/.\nJiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L.,\nWang, F., and Liu, Q. Tinybert: Distilling BERT for natu-\nral language understanding. CoRR, abs/1909.10351, 2019.\nURL http://arxiv.org/abs/1909.10351.\nJoshi, M., Chen, D., Liu, Y ., Weld, D. S., Zettlemoyer,\nL., and Levy, O. Spanbert: Improving pre-training\nby representing and predicting spans. arXiv preprint\narXiv:1907.10529, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochas-\ntic optimization. In 3rd International Conference on\nLearning Representations, San Diego, CA, 2015. URL\nhttp://arxiv.org/abs/1412.6980.\nLample, G. and Conneau, A. Cross-lingual language model\npretraining. Advances in Neural Information Processing\nSystems (NeurIPS), 2019.\nLevesque, H., Davis, E., and Morgenstern, L. The winograd\nschema challenge. In Thirteenth International Confer-\nence on the Principles of Knowledge Representation and\nReasoning, 2012.\nLewis, M., Liu, Y ., Goyal, N., Ghazvininejad, M., Mo-\nhamed, A., Levy, O., Stoyanov, V ., and Zettlemoyer, L.\nBART: Denoising sequence-to-sequence pre-training for\nnatural language generation, translation, and comprehen-\nsion. arXiv preprint arXiv:1910.13461, 2019a.\nLewis, P. S. H., Oguz, B., Rinott, R., Riedel, S., and\nSchwenk, H. MLQA: evaluating cross-lingual extrac-\ntive question answering. CoRR, abs/1910.07475, 2019b.\nURL http://arxiv.org/abs/1910.07475.\nLin, C.-Y . ROUGE: A package for automatic evalua-\ntion of summaries. In Text Summarization Branches\nOut: Proceedings of the ACL-04 Workshop, pp. 74‚Äì81,\nBarcelona, Spain, July 2004. Association for Compu-\ntational Linguistics. URL https://www.aclweb.\norg/anthology/W04-1013.\nLiu, Y . and Lapata, M. Text summarization with pretrained\nencoders. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language\nProcessing, pp. 3730‚Äì3740, Hong Kong, China, 2019.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692, 2019.\nMirzadeh, S., Farajtabar, M., Li, A., and Ghasemzadeh,\nH. Improved knowledge distillation via teacher assistant:\nBridging the gap between student and teacher. CoRR,\nabs/1902.03393, 2019. URL http://arxiv.org/\nabs/1902.03393.\nNarayan, S., Cohen, S. B., and Lapata, M. Don‚Äôt give me the\ndetails, just the summary! topic-aware convolutional neu-\nral networks for extreme summarization. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 1797‚Äì1807, Brussels, Belgium,\nOctober-November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/D18-1206. URL https:\n//www.aclweb.org/anthology/D18-1206.\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nPeters, M., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep contextualized\nword representations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pp. 2227‚Äì2237, New\nOrleans, Louisiana, June 2018. Association for Computa-\ntional Linguistics. URL http://www.aclweb.org/\nanthology/N18-1202.\nRadford, A., Narasimhan, K., Salimans, T., and\nSutskever, I. Improving language understand-\ning by generative pre-training. 2018. URL\nhttps://s3-us-west-2.amazonaws.\ncom/openaiassets/research-covers/\nlanguage-unsupervised/\nlanguageunderstandingpaper.pdf.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a uniÔ¨Åed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.\nSQuAD: 100,000+ questions for machine comprehen-\nsion of text. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Processing,\npp. 2383‚Äì2392, Austin, Texas, November 2016. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/D16-1264. URL https://www.aclweb.org/\nanthology/D16-1264.\nRajpurkar, P., Jia, R., and Liang, P. Know what you don‚Äôt\nknow: Unanswerable questions for SQuAD. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne, Aus-\ntralia, July 15-20, 2018, Volume 2: Short Papers , pp.\n784‚Äì789, 2018.\nRomero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta,\nC., and Bengio, Y . Fitnets: Hints for thin deep nets.\nIn 3rd International Conference on Learning Repre-\nsentations, ICLR 2015, San Diego, CA, USA, May 7-\n9, 2015, Conference Track Proceedings , 2015. URL\nhttp://arxiv.org/abs/1412.6550.\nSanh, V ., Debut, L., Chaumond, J., and Wolf, T. Distilbert,\na distilled version of BERT: smaller, faster, cheaper and\nlighter. CoRR, abs/1910.01108, 2019. URL http://\narxiv.org/abs/1910.01108.\nSee, A., Liu, P. J., and Manning, C. D. Get to the point:\nSummarization with pointer-generator networks. In Pro-\nceedings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npp. 1073‚Äì1083, Vancouver, Canada, July 2017. Asso-\nciation for Computational Linguistics. doi: 10.18653/\nv1/P17-1099. URL https://www.aclweb.org/\nanthology/P17-1099.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nProceedings of the 2013 conference on empirical methods\nin natural language processing, pp. 1631‚Äì1642, 2013.\nSong, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y . Mass:\nMasked sequence to sequence pre-training for language\ngeneration. arXiv preprint arXiv:1905.02450, 2019.\nSun, S., Cheng, Y ., Gan, Z., and Liu, J. Patient knowledge\ndistillation for BERT model compression. InProceedings\nof the 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International Joint\nConference on Natural Language Processing, EMNLP-\nIJCNLP 2019, Hong Kong, China, November 3-7, 2019,\npp. 4322‚Äì4331, 2019a.\nSun, Z., Yu, H., Song, X., Liu, R., Yang, Y ., and Zhou,\nD. Mobilebert: Task-agnostic compression of bert by\nprogressive knowledge transfer, 2019b. URL https:\n//openreview.net/pdf?id=SJxjVaNKwB.\nSzegedy, C., Vanhoucke, V ., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer\nvision. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition , pp. 2818‚Äì2826,\n2016. doi: 10.1109/cvpr.2016.308.\nTang, R., Lu, Y ., Liu, L., Mou, L., Vechtomova, O., and Lin,\nJ. Distilling task-speciÔ¨Åc knowledge from BERT into\nsimple neural networks. CoRR, abs/1903.12136, 2019.\nURL http://arxiv.org/abs/1903.12136.\nTrinh, T. H. and Le, Q. V . A simple method for common-\nsense reasoning. ArXiv, abs/1806.02847, 2018.\nTsai, H., Riesa, J., Johnson, M., Arivazhagan, N., Li, X.,\nand Archer, A. Small and practical BERT models for\nsequence labeling. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference on\nNatural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pp. 3630‚Äì3634,\n2019.\nTurc, I., Chang, M., Lee, K., and Toutanova, K. Well-read\nstudents learn better: The impact of student initialization\non knowledge distillation. CoRR, abs/1908.08962, 2019a.\nURL http://arxiv.org/abs/1908.08962.\nTurc, I., Chang, M., Lee, K., and Toutanova, K. Well-read\nstudents learn better: The impact of student initialization\non knowledge distillation. CoRR, abs/1908.08962, 2019b.\nURL http://arxiv.org/abs/1908.08962.\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit,\nJ., Jones, L., Gomez, A. N., Kaiser, ≈Å., and\nPolosukhin, I. Attention is all you need. In\nAdvances in Neural Information Processing Sys-\ntems 30 , pp. 5998‚Äì6008. Curran Associates, Inc.,\n2017. URL http://papers.nips.cc/paper/\n7181-attention-is-all-you-need.pdf .\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. GLUE: A multi-task benchmark and\nanalysis platform for natural language understanding. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=rJ4km2R5t7.\nWarstadt, A., Singh, A., and Bowman, S. R. Neu-\nral network acceptability judgments. arXiv preprint\narXiv:1805.12471, 2018.\nWilliams, A., Nangia, N., and Bowman, S. A broad-\ncoverage challenge corpus for sentence understanding\nthrough inference. In Proceedings of the 2018 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pp. 1112‚Äì\n1122, New Orleans, Louisiana, June 2018. Associa-\ntion for Computational Linguistics. doi: 10.18653/\nv1/N18-1101. URL https://www.aclweb.org/\nanthology/N18-1101.\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M.,\nMacherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey,\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,\nL., Gouws, S., Kato, Y ., Kudo, T., Kazawa, H., Stevens,\nK., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J.,\nRiesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes,\nM., and Dean, J. Google‚Äôs neural machine translation\nsystem: Bridging the gap between human and machine\ntranslation. CoRR, abs/1609.08144, 2016. URL http:\n//arxiv.org/abs/1609.08144.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,\nR., and Le, Q. V . XLNet: Generalized autoregressive\npretraining for language understanding. In 33rd Confer-\nence on Neural Information Processing Systems (NeurIPS\n2019), 2019.\nZagoruyko, S. and Komodakis, N. Paying more atten-\ntion to attention: Improving the performance of con-\nvolutional neural networks via attention transfer. In\n5th International Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings , 2017. URL https:\n//openreview.net/forum?id=Sks9_ajex.\nZhang, S. and Bansal, M. Addressing semantic drift in ques-\ntion generation for semi-supervised question answering.\nCoRR, abs/1909.06356, 2019.\nZhao, Y ., Ni, X., Ding, Y ., and Ke, Q. Paragraph-\nlevel neural question generation with maxout pointer\nand gated self-attention networks. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natural\nLanguage Processing, pp. 3901‚Äì3910, Brussels, Bel-\ngium, October-November 2018. Association for Com-\nputational Linguistics. URL https://www.aclweb.\norg/anthology/D18-1424.\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. In Proceedings of the\nIEEE international conference on computer vision , pp.\n19‚Äì27, 2015.\nA. GLUE Benchmark\nThe summary of datasets used for the General Language Un-\nderstanding Evaluation (GLUE) benchmark8 (Wang et al.,\n2019) is presented in Table 14.\nWe present the dataset statistics and metrics of SQuAD\n2.09 (Rajpurkar et al., 2018) in Table 15.\nB. Fine-tuning Hyper-parameters\nExtractive Question Answering For SQuAD 2.0, the\nmaximum sequence length is 384 and a sliding window\nof size 128 if the lengths are longer than 384. For the 12-\nlayer model distilled from our in-house pre-trained model,\nwe Ô¨Åne-tune 3 epochs using 48 as the batch size and 4e-5 as\nthe peak learning rate. The rest distilled models are trained\nusing 32 as the batch size and 6e-5 as the peak learning rate\nfor 3 epochs.\nGLUE The maximum sequence length is 128 for the\nGLUE benchmark. We set batch size to 32, choose learning\nrates from {2e-5, 3e-5, 4e-5, 5e-5} and epochs from {3, 4,\n5} for student models distilled from BERTBASE. For student\nmodels distilled from our in-house pre-trained model, the\nbatch size is chosen from { 32, 48}. We Ô¨Åne-tune several\ntasks (CoLA, RTE and MRPC) with longer epochs (up to10\nepochs), which brings slight improvements. For the12-layer\nmodel, the learning rate used for CoLA, RTE and MRPC\ntasks is 1.5e-5.\n8https://gluebenchmark.com/\n9http://stanford-qa.com\nMINI LM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\nTable 14.Summary of the GLUE benchmark.\nCorpus #Train #Dev #Test Metrics\nSingle-Sentence Tasks\nCoLA 8.5k 1k 1k Matthews Corr\nSST-2 67k 872 1.8k Accuracy\nSimilarity and Paraphrase Tasks\nQQP 364k 40k 391k Accuracy/F1\nMRPC 3.7k 408 1.7k Accuracy/F1\nSTS-B 7k 1.5k 1.4k Pearson/Spearman Corr\nInference Tasks\nMNLI 393k 20k 20k Accuracy\nRTE 2.5k 276 3k Accuracy\nQNLI 105k 5.5k 5.5k Accuracy\nWNLI 634 71 146 Accuracy\nTable 15.Dataset statistics and metrics of SQuAD 2.0.\n#Train #Dev #Test Metrics\n130,319 11,873 8,862 Exact Match/F1\nC. SQuAD 2.0\nQuestion Generation For the question generation task,\nwe set batch size to 32, and total length to 512. The maxi-\nmum output length is 48. The learning rates are 3e-5 and\n8e-5 for the 12-layer and 6-layer models, respectively. They\nare both Ô¨Åne-tuned for 25 epochs. We also use label smooth-\ning (Szegedy et al., 2016) with rate of 0.1. During decod-\ning, we use beam search with beam size of 5. The length\npenalty (Wu et al., 2016) is 1.3.\nAbstractive Summarization For the abstractive summa-\nrization task, we set batch size to 64, and the rate of label\nsmoothing to 0.1. For the CNN/DailyMail dataset, the total\nlength is 768 and the maximum output length is 160. The\nlearning rates are 1e-4 and 1.5e-4 for the 12-layer and 6-\nlayer models, respectively. They are both Ô¨Åne-tuned for 25\nepochs. During decoding, we set beam size to 5, and the\nlength penalty to 0.7. For the XSum dataset, the total length\nis 512 and the maximum output length is 48. The learning\nrates are 1e-4 and 1.5e-4 for the12-layer and 6-layer models,\nrespectively. We Ô¨Åne-tune 30 epochs for the 12-layer model\nand 50 epochs for the 6-layer model. During decoding, we\nuse beam search with beam size of 5. The length penalty is\nset to 0.9.\nCross-lingual Natural Language Inference The maxi-\nmum sequence length is 128 for XNLI. We Ô¨Åne-tune 5\nepochs using 128 as the batch size, choose learning rates\nfrom {3e-5, 4e-5, 5e-5, 6e-5}.\nCross-lingual Question Answering For MLQA, the\nmaximum sequence length is 512 and a sliding window\nof size 128 if the lengths are longer than 512. We Ô¨Åne-tune\n3 epochs using 32 as the batch size. The learning rates are\nchosen from {3e-5, 4e-5, 5e-5, 6e-5}.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7301260232925415
    },
    {
      "name": "Computer science",
      "score": 0.5807275772094727
    },
    {
      "name": "Distillation",
      "score": 0.5274059176445007
    },
    {
      "name": "Task (project management)",
      "score": 0.5085588693618774
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49942779541015625
    },
    {
      "name": "Machine learning",
      "score": 0.33462852239608765
    },
    {
      "name": "Engineering",
      "score": 0.1602400839328766
    },
    {
      "name": "Chemistry",
      "score": 0.15734702348709106
    },
    {
      "name": "Electrical engineering",
      "score": 0.1571348011493683
    },
    {
      "name": "Chromatography",
      "score": 0.12190067768096924
    },
    {
      "name": "Voltage",
      "score": 0.07759404182434082
    },
    {
      "name": "Systems engineering",
      "score": 0.061500757932662964
    }
  ],
  "institutions": [],
  "cited_by": 628
}