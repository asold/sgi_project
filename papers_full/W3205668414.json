{
  "title": "Distilling Relation Embeddings from Pretrained Language Models",
  "url": "https://openalex.org/W3205668414",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2588925643",
      "name": "Ushio, Asahi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225875417",
      "name": "Camacho-Collados, Jose",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225610128",
      "name": "Schockaert, Steven",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2897038520",
    "https://openalex.org/W2998696444",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W3152884768",
    "https://openalex.org/W1654905138",
    "https://openalex.org/W2068737686",
    "https://openalex.org/W2978491132",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2467240462",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W1535654961",
    "https://openalex.org/W3139080614",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W3009958360",
    "https://openalex.org/W1660519191",
    "https://openalex.org/W2899286282",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2950976310",
    "https://openalex.org/W3099178230",
    "https://openalex.org/W2964242916",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2803986565",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965058444",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W3102226577",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2952391587",
    "https://openalex.org/W3098613713",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W4289678533",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1965605789",
    "https://openalex.org/W2962724755",
    "https://openalex.org/W2963219906",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2963958374",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W2903760398",
    "https://openalex.org/W3137573489",
    "https://openalex.org/W1852412531",
    "https://openalex.org/W2155157567",
    "https://openalex.org/W3125506016",
    "https://openalex.org/W2963239938",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2610751324",
    "https://openalex.org/W2608204223",
    "https://openalex.org/W3103136066",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W1593045043",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W4206214875",
    "https://openalex.org/W3100404639",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2739638526",
    "https://openalex.org/W3154903254",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2098801107",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W2005603639",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2795302121",
    "https://openalex.org/W3117576675",
    "https://openalex.org/W2460442863",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W3099386342",
    "https://openalex.org/W115166160",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2964350953",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2963768750",
    "https://openalex.org/W2751143192",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2135964261",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2962769333",
    "https://openalex.org/W2963201227",
    "https://openalex.org/W4288265479",
    "https://openalex.org/W4302044537",
    "https://openalex.org/W2950401678",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3035102548",
    "https://openalex.org/W2123143128",
    "https://openalex.org/W4316339712",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2987249037",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3174082608",
    "https://openalex.org/W4231510805",
    "https://openalex.org/W2250533418",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W2963176474",
    "https://openalex.org/W3173777717"
  ],
  "abstract": "Pre-trained language models have been found to capture a surprisingly rich\\namount of lexical knowledge, ranging from commonsense properties of everyday\\nconcepts to detailed factual knowledge about named entities. Among others, this\\nmakes it possible to distill high-quality word vectors from pre-trained\\nlanguage models. However, it is currently unclear to what extent it is possible\\nto distill relation embeddings, i.e. vectors that characterize the relationship\\nbetween two words. Such relation embeddings are appealing because they can, in\\nprinciple, encode relational knowledge in a more fine-grained way than is\\npossible with knowledge graphs. To obtain relation embeddings from a\\npre-trained language model, we encode word pairs using a (manually or\\nautomatically generated) prompt, and we fine-tune the language model such that\\nrelationally similar word pairs yield similar output vectors. We find that the\\nresulting relation embeddings are highly competitive on analogy (unsupervised)\\nand relation classification (supervised) benchmarks, even without any\\ntask-specific fine-tuning. Source code to reproduce our experimental results\\nand the model checkpoints are available in the following repository:\\nhttps://github.com/asahi417/relbert\\n",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9044–9062\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n9044\nDistilling Relation Embeddings from Pre-trained Language Models\nAsahi Ushio and Jose Camacho-Collados and Steven Schockaert\nCardiff NLP, School of Computer Science and Informatics\nCardiff University, United Kingdom\n{UshioA,CamachoColladosJ,SchockaertS1}@cardiff.ac.uk\nAbstract\nPre-trained language models have been found\nto capture a surprisingly rich amount of lexical\nknowledge, ranging from commonsense prop-\nerties of everyday concepts to detailed factual\nknowledge about named entities. Among oth-\ners, this makes it possible to distill high-quality\nword vectors from pre-trained language mod-\nels. However, it is currently unclear to what ex-\ntent it is possible to distillrelation embeddings,\ni.e. vectors that characterize the relationship\nbetween two words. Such relation embeddings\nare appealing because they can, in principle,\nencode relational knowledge in a more ﬁne-\ngrained way than is possible with knowledge\ngraphs. To obtain relation embeddings from a\npre-trained language model, we encode word\npairs using a (manually or automatically gen-\nerated) prompt, and we ﬁne-tune the language\nmodel such that relationally similar word pairs\nyield similar output vectors. We ﬁnd that\nthe resulting relation embeddings are highly\ncompetitive on analogy (unsupervised) and re-\nlation classiﬁcation (supervised) benchmarks,\neven without any task-speciﬁc ﬁne-tuning.1\n1 Introduction\nOne of the most widely studied aspects of word\nembeddings is the fact that word vector differences\ncapture lexical relations (Mikolov et al., 2013a).\nWhile not being directly connected to downstream\nperformance on NLP tasks, this ability of word em-\nbeddings is nonetheless important. For instance,\nunderstanding lexical relations is an important pre-\nrequisite for understanding the meaning of com-\npound nouns (Turney, 2012). Moreover, the ability\nof word vectors to capture semantic relations has\nenabled a wide range of applications beyond NLP,\nincluding ﬂexible querying of relational databases\n(Bordawekar and Shmueli, 2017), schema match-\n1Source code to reproduce our experimental results and the\nmodel checkpoints are available in the following repository:\nhttps://github.com/asahi417/relbert\ning (Fernandez et al., 2018), completion and re-\ntrieval of Web tables (Zhang et al., 2019), ontology\ncompletion (Bouraoui and Schockaert, 2019) and\ninformation retrieval in the medical domain (Ar-\nguello Casteleiro et al., 2020). More generally,\nrelational similarity (or analogy) plays a central\nrole in computational creativity (Goel, 2019), le-\ngal reasoning (Ashley, 1988; Walton, 2010), on-\ntology alignment (Raad and Evermann, 2015) and\ninstance-based learning (Miclet et al., 2008).\nGiven the recent success of pre-trained language\nmodels (Devlin et al., 2019; Liu et al., 2019; Brown\net al., 2020), we may wonder whether such mod-\nels are able to capture lexical relations in a more\nfaithful or ﬁne-grained way than traditional word\nembeddings. However, for language models (LMs),\nthere is no direct equivalent to the word vector\ndifference. In this paper, we therefore propose a\nstrategy for extracting relation embeddings from\npre-trained LMs, i.e. vectors encoding the relation-\nship between two words. On the one hand, this\nwill allow us to gain a better understanding of how\nwell lexical relations are captured by these models.\nOn the other hand, this will also provide us with a\npractical method for obtaining relation embeddings\nin applications such as the ones mentioned above.\nSince it is unclear how LMs store relational\nknowledge, rather than directly extracting relation\nembeddings, we ﬁrst ﬁne-tune the LM, such that re-\nlation embeddings can be obtained from its output.\nTo this end, we need a prompt, i.e. a template to\nconvert a given word pair into a sentence, and some\ntraining data to ﬁne-tune the model. To illustrate\nthe process, consider the word pair Paris-France.\nAs a possible input to the model, we could use\na sentence such as “The relation between Paris\nand France is <mask>\". Note that our aim is to\nﬁnd a strategy that can be applied to any pair of\nwords, hence the way in which the input is repre-\nsented needs to be sufﬁciently generic. We then\nﬁne-tune the LM such that its output corresponds\n9045\nto a relation embedding. To this end, we use a\ncrowdsourced dataset of relational similarity judge-\nments that was collected in the context of SemEval\n2012 Task 2 (Jurgens et al., 2012). Despite the\nrelatively small size of this dataset, we show that\nthe resulting ﬁne-tuned LM allows us to produce\nhigh-quality relation embeddings, as conﬁrmed in\nour extensive evaluation in analogy and relation\nclassiﬁcation tasks. Importantly, this also holds for\nrelations that are of a different nature than those\nin the SemEval dataset, showing that this process\nallows us to distill relational knowledge that is en-\ncoded in the pre-trained LM, rather than merely\ngeneralising from the examples that were used for\nﬁne-tuning.\n2 Related Work\nProbing LMs for Relational Knowledge Since\nthe introduction of transformer-based LMs, a large\nnumber of works have focused on analysing the\ncapabilities of such models, covering the extent to\nwhich they capture syntax (Goldberg, 2019; Saphra\nand Lopez, 2019; Hewitt and Manning, 2019; van\nSchijndel et al., 2019; Jawahar et al., 2019; Tenney\net al., 2019), lexical semantics (Ethayarajh, 2019;\nBommasani et al., 2020; Vulic et al., 2020), and var-\nious forms of factual and commonsense knowledge\n(Petroni et al., 2019; Forbes et al., 2019; Davison\net al., 2019; Zhou et al., 2020; Talmor et al., 2020;\nRoberts et al., 2020), among others. The idea of\nextracting relational knowledge from LMs, in par-\nticular, has also been studied. For instance, Petroni\net al. (2019) use BERT for link prediction. To this\nend, they use a manually deﬁned prompt for each\nrelation type, in which the tail entity is replaced\nby a <mask> token. To complete a knowledge\ngraph triple such as (Dante, born-in, ?) they create\nthe input “Dante was born in <mask>” and then\nlook at the predictions of BERT for the masked\ntoken to retrieve the correct answer. It is notable\nthat BERT is thus used for extracting relational\nknowledge without any ﬁne-tuning. This clearly\nshows that a substantial amount of factual knowl-\nedge is encoded in the parameters of pre-trained\nLMs. Some works have also looked at how such\nknowledge is stored. Geva et al. (2020) argue that\nthe feed-forward layers of transformer-based LMs\nact as neural memories, which would suggest that\ne.g. “the place where Dante is born” is stored as\na property of Florence. Dai et al. (2021) present\nfurther evidence of this view. What is less clear,\nthen, is whether relations themselves have an ex-\nplicit representation, or whether transformer mod-\nels essentially store a propositionalised knowledge\ngraph. The results we present in this paper sug-\ngest that common lexical relations (e.g. hypernymy,\nmeronymy, has-attribute), at least, must have some\nkind of explicit representation, although it remains\nunclear how they are encoded.\nAnother notable work focusing on link pre-\ndiction is (Bosselut et al., 2019), where GPT is\nﬁne-tuned to complete triples from commonsense\nknowledge graphs, in particular ConceptNet (Speer\net al., 2017) and ATOMIC (Sap et al., 2019). While\ntheir model was able to generate new knowledge\ngraph triples, it is unclear to what extent this is\nachieved by extracting commonsense knowledge\nthat was already captured by the pre-trained GPT\nmodel, or whether this rather comes from the abil-\nity to generalise from the training triples. For the\nConceptNet dataset, for instance, Jastrz˛ ebski et al.\n(2018) found that most test triples are in fact minor\nvariations of training triples. In this paper, we also\nrely on ﬁne-tuning, which makes it harder to de-\ntermine to what extent the pre-trained LM already\ncaptures relational knowledge. We address this con-\ncern by including relation types in our evaluation\nwhich are different from the ones that have been\nused for ﬁne-tuning.\nUnsupervised Relation Discovery Modelling\nhow different words are related is a long-standing\nchallenge in NLP. An early approach is DIRT (Lin\nand Pantel, 2001), which encodes the relation be-\ntween two nouns as the dependency path connect-\ning them. Their view is that two such depen-\ndency paths are similar if the sets of word pairs\nwith which they co-occur are similar. Hasegawa\net al. (2004) cluster named entity pairs based on\nthe bag-of-words representations of the contexts in\nwhich they appear. Along the same lines, Yao et al.\n(2011) proposed a generative probabilistic model,\ninspired by LDA (Blei et al., 2003), in which re-\nlations are viewed as latent variables (similar to\ntopics in LDA). Turney (2005) proposed a method\ncalled Latent Relational Analysis (LRA), which\nuses matrix factorization to learn relation embed-\ndings based on co-occurrences of word pairs and\ndependency paths. Matrix factorization is also used\nin the Universal Schema approach from Riedel et\nal. (Riedel et al., 2013), which jointly models the\ncontexts in which words appear in a corpus with a\ngiven set of relational facts.\n9046\nThe aforementioned works essentially represent\nthe relation between two words by summarising\nthe contexts in which these words co-occur. In\nrecent years, a number of strategies based on dis-\ntributional models have been explored that rely\non similar intuitions but go beyond simple vec-\ntor operations of word embeddings.2 For instance,\nJameel et al. (2018) introduced a variant of the\nGloVe word embedding model, in which relation\nvectors are jointly learned with word vectors. In\nSeVeN (Espinosa-Anke and Schockaert, 2018) and\nRELATIVE (Camacho-Collados et al., 2019), rela-\ntion vectors are computed by averaging the embed-\ndings of context words, while pair2vec (Joshi et al.,\n2019) uses an LSTM to summarise the contexts in\nwhich two given words occur, and Washio and Kato\n(2018) learn embeddings of dependency paths to\nencode word pairs. Another line of work is based\non the idea that relation embeddings should facil-\nitate link prediction, i.e. given the ﬁrst word and\na relation vector, we should be able to predict the\nsecond word (Marcheggiani and Titov, 2016; Si-\nmon et al., 2019). This idea also lies at the basis of\nthe approach from Soares et al. (2019), who train\na relation encoder by ﬁne-tuning BERT (Devlin\net al., 2019) with a link prediction loss. However, it\nshould be noted that they focus on learning relation\nvectors from individual sentences, as a pre-training\ntask for applications such as few-shot relation ex-\ntraction. In contrast, our focus in this paper is on\ncharacterising the overall relationship between two\nwords.\n3 RelBERT\nIn this section, we describe our proposed relation\nembedding model (RelBERT henceforth). To ob-\ntain a relation embedding for given a word pair\n(h,t), we ﬁrst convert it into a sentence s, called\nthe prompt. We then feed the prompt through the\nLM and average the contextualized embeddings\n(i.e. the output vectors) to get the relation embed-\nding of (h,t). These steps are illustrated in Figure 1\nand explained in more detail in the following.\n3.1 Prompt Generation\nManual Prompts A basic prompt generation\nstrategy is to rely on manually created templates,\n2Interestingly, Roller and Erk (2016) showed that the direct\nconcatenation of distributional word vectors in isolation can\neffectively identify Hearst Patterns (Hearst, 1992).\nFigure 1: Pipeline to transform the word pair (h,t) to\nthe relation embedding x.\nwhich has proven effective in factual knowledge\nprobing (Petroni et al., 2019) and text classiﬁca-\ntion (Schick and Schütze, 2021; Tam et al., 2021;\nLe Scao and Rush, 2021), among many others. To\ntest whether manually generated templates can be\neffective for learning relation embeddings, we will\nconsider the following ﬁve templates:\n1. Today, I ﬁnally discovered the relation between [h] and\n[t] : [h] is the <mask> of [t]\n2. Today, I ﬁnally discovered the relation between [h] and\n[t] : [t] is [h]’s <mask>\n3. Today, I ﬁnally discovered the relation between [h] and\n[t] : <mask>\n4. I wasn’t aware of this relationship, but I just read in the\nencyclopedia that [h] is the <mask> of [t]\n5. I wasn’t aware of this relationship, but I just read in the\nencyclopedia that [t] is [h]’s <mask>\nwhere <mask> is the LM’s mask token, and [h]\nand [t] are slots that are ﬁlled with the head word\nhand tail word tfrom the given word pair (h,t)\nrespectively. The main intuition is that the template\nshould encode that we are interested in the relation-\nship between hand t. Moreover, we avoid minimal\ntemplates such as “[h] is the <mask> of [t]”, as\nLMs typically perform worse on such short inputs\n(Bouraoui et al., 2020; Jiang et al., 2020).\nLearned Prompts The choice of prompt can\nhave a signiﬁcant impact on an LM’s performance.\nSince it is difﬁcult to generate manual prompts in\na systematic way, several strategies for automated\ngeneration of task-speciﬁc prompts have been pro-\nposed, e.g. based on mining patterns from a cor-\npus (Bouraoui et al., 2020), paraphrasing (Jiang\net al., 2020), training an additional LM for tem-\nplate generation (Haviv et al., 2021; Gao et al.,\n2020), and prompt optimization (Shin et al., 2020;\nLiu et al., 2021). In our work, we focus on the\nlatter strategy, given its conceptual simplicity and\nits strong reported performance on various bench-\nmarks. Speciﬁcally, we consider AutoPrompt (Shin\net al., 2020) and P-tuning (Liu et al., 2021). Note\nthat both methods rely on training data. We will\nuse the same training data and loss function that\n9047\nwe use for ﬁne-tuning the LM; see Section 3.2.\nAutoPrompt initializes the prompt as a ﬁxed-\nlength template:\nT = (z1,...,z π,[h],zπ+1,...,z π+τ,\n[t],zπ+τ+1,...,z π+τ+γ) (1)\nwhere π, τ, γ are hyper-parameters which deter-\nmine the length of the template. The tokens of the\nform zi are called trigger tokens. These tokens are\ninitialized as <mask>. The method then iteratively\nﬁnds the best token to replace each mask, based on\nthe gradient of the task-speciﬁc loss function.3\nP-tuning employs the same template initializa-\ntion as AutoPrompt but its trigger tokens are newly\nintroduced special tokens with trainable embed-\ndings ˆe1:π+τ+γ, which are learned using a task-\nspeciﬁc loss function while the LM’s weights are\nfrozen.\n3.2 Fine-tuning the LM\nTo ﬁne-tune the LM, we need training data and a\nloss function. As training data, we assume that,\nfor a number of different relation types r, we have\naccess to examples of word pairs (h,t) that are\ninstances of that relation type. The loss function is\nbased on the following intuition: the embeddings\nof word pairs that belong to the same relation type\nshould be closer together than the embeddings of\npairs that belong to different relations. In particular,\nwe use the triplet loss from Schroff et al. (2015) and\nthe classiﬁcation loss from Reimers and Gurevych\n(2019), both of which are based on this intuition.\nTriplet Loss We draw a triplet from the relation\ndataset by selecting an anchor pair a= (ha,ta), a\npositive example p= (hp,tp) and a negative exam-\nple n= (hn,tn), i.e. we select word pairs a,p,n\nsuch that aand pbelong to the same relation type\nwhile nbelongs to a different relation type. Let\nus write xa, xp, xn for the corresponding relation\nembeddings. Each relation embedding is produced\nby the same LM, which is trained to make the dis-\ntance between xa and xp smaller than the distance\nbetween xa and xn. Formally, this is accomplished\nusing the following triplet loss function:\nLt = max\n(\n0,∥xa −xp∥−∥xa −xn∥+ ε\n)\n3We note that in most implementations of AutoPrompt the\nvocabulary to sample trigger tokens is restricted to that of the\ntraining data. However, given the nature of our training data\n(i.e., pairs of words and not sentences), we consider the full\npre-trained LM’s vocabulary.\nwhere ε> 0 is the margin and ∥·∥ is the l2 norm.\nClassiﬁcation Loss Following SBERT (Reimers\nand Gurevych, 2019), we use a classiﬁer to predict\nwhether two word pairs belong to the same relation.\nThe classiﬁer is jointly trained with the LM using\nthe negative log likelihood loss function:\nLc = −log(g(xa,xp)) −log(1 −g(xa,xn))\nwhere\ng(u,v) =sigmoid(W ·[u ⊕v ⊕|v −u|]T)\nwith W ∈R3×d, u,v ∈Rd, |·| the element-wise\nabsolute difference, and ⊕concatenation.\n4 Experimental Setting\nIn this section, we explain our experimental setting\nto train and evaluate RelBERT.\n4.1 RelBERT Training\nDataset We use the platinum ratings from Se-\nmEval 2012 Task 2 (Jurgens et al., 2012) as our\ntraining dataset for RelBERT. This dataset cov-\ners 79 ﬁne-grained semantic relations, which are\ngrouped in 10 categories. For each of the 79 re-\nlations, the dataset contains a typicality score for\na number of word pairs (around 40 on average),\nindicating to what extent the word pair is a proto-\ntypical instance of the relation. We treat the top 10\npairs (i.e. those with the highest typicality score)\nas positive examples of the relation, and the bot-\ntom 10 pairs as negative examples. We use 80% of\nthese positive and negative examples for training\nRelBERT (i.e. learning the prompt and ﬁne-tuning\nthe LM) and 20% for validation.\nConstructing Training Triples We rely on three\ndifferent strategies for constructing training triples.\nFirst, we obtain triples by selecting two positive\nexamples of a given relation type (i.e. from the top-\n10 pairs) and one negative example (i.e. from the\nbottom-10 pairs). We construct 450 such triples per\nrelation. Second, we construct triples by using two\npositive examples of the relation and one positive\nexample from another relation (which is assumed\nto correspond to a negative example). In particu-\nlar, for efﬁciency, we use the anchors and positive\nexamples of the other triples from the same batch\nas negative examples (while ensuring that these\n9048\nFigure 2: Batch augmentation where the original batch\nwith msamples is augmented with2m(m−1) samples.\ntriples are from different relations). Figure 2 il-\nlustrates this idea. Note how the effective batch\nsize thus increases quadratically, while the num-\nber of vectors that needs to be encoded by the LM\nremains unchanged. In our setting, this leads to\nan additional 13500 triples per relation. Similar\nin-batch negative sampling has been shown to be\neffective in information retrieval (Karpukhin et al.,\n2020; Gillick et al., 2019). Third, we also construct\ntraining triples by considering the 10 high-level\ncategories as relation types. In this case, we choose\ntwo positive examples from different relations that\nbelong to the same category, along with a positive\nexample from a relation from a different category.\nWe add 5040 triples of this kind for each of the 10\ncategories.\nTraining RelBERT training consists of two\nphases: prompt optimization (unless a manually\ndeﬁned prompt is used) and language model ﬁne-\ntuning. First we optimize the prompt over the\ntraining set with the triplet loss Lt while the pa-\nrameters of the LM are frozen. Subsequently, we\nﬁne-tune the LM with the resulting prompt, using\nthe sum of the triplet loss Lt and the classiﬁca-\ntion loss Lc over the same training set. We do\nnot use the classiﬁcation loss during the prompt\noptimisation, as that would involve training the\nclassiﬁer while optimizing the prompt. We select\nthe best hyper-parameters of the prompting meth-\nods based on the ﬁnal loss over the validation set.\nIn particular, when manual prompts are used, we\nchoose the best template among the ﬁve candidates\ndescribed in Section 3.1. For AutoPrompt and P-\ntuning, we consider all combinations ofπ∈{8,9},\nτ ∈{1,2}, γ ∈{1,2}. We use RoBERTa (Liu\net al., 2019) as our main LM, where the initial\nweights were taken from the roberta-large\nmodel checkpoint shared by the Huggingface trans-\nformers model hub (Wolf et al., 2020). We use the\nAdam optimizer (Kingma and Ba, 2014) with learn-\ning rate 0.00002, batch size 64 and we ﬁne-tune the\nmodel for 1 epoch. For AutoPrompt, the top-50 to-\nkens are considered and the number of iterations is\nset to 50. In each iteration, one of the input tokens\nis re-sampled and the loss is re-computed across\nthe entire training set.4 For P-tuning, we train the\nweights that deﬁne the trigger embeddings (i.e. the\nweights of the input vectors and the parameters of\nthe LSTM) for 2 epochs. Note that we do not tune\nRelBERT on any task-speciﬁc training or valida-\ntion set. We thus use the same relation embeddings\nacross all the considered evaluation tasks.\n4.2 Evaluation Tasks\nWe evaluate RelBERT on two relation-centric tasks:\nsolving analogy questions (unsupervised) and lexi-\ncal relation classiﬁcation (supervised).\nAnalogy Questions We consider the task of solv-\ning word analogy questions. Given a query word\npair, the model is required to select the relationally\nmost similar word pair from a list of candidates.\nTo solve this task, we simply choose the candi-\ndate whose RelBERT embedding has the highest\ncosine similarity with the RelBERT embedding of\nthe query pair. Note that this task is completely\nunsupervised, without the need for any training or\ntuning. We use the ﬁve analogy datasets that were\nconsidered by Ushio et al. (2021): the SAT analo-\ngies dataset (Turney et al., 2003), the U2 and U4\nanalogy datasets, which were collected from an ed-\nucational website5, and datasets that were derived6\nfrom BATS (Gladkova et al., 2016) and the Google\nanalogy dataset (Mikolov et al., 2013b). These ﬁve\ndatasets consist of tuning and testing fragments.\nIn particular, they contain 37/337 (SAT), 24/228\n(U2), 48/432 (U4), 50/500 (Google), and 199/1799\n(BATS) questions for validation/testing. As there\nis no need to tune RelBERT on task-speciﬁc data,\nwe only use the test fragments. For SAT, we will\nalso report results on the full dataset (i.e. the testing\nfragment and tuning fragment combined), as this\nallows us to compare the performance with pub-\nlished results. We will refer to this full version of\nthe SAT dataset as SAT†.\n4We should note that AutoPrompt takes considerably\nlonger than any other components of RelBERT. More details\non experimental training times are included in the appendix.\n5https://englishforeveryone.org/\nTopics/Analogies.html\n6In particular, they were converted into the same format of\nmultiple-choice questions as the other datasets.\n9049\nBLESS CogALex EV ALution K&H+N ROOT09\nRandom 8,529/609/3,008 2,228/3,059 - 18,319/1,313/6,746 4,479/327/1,566\nMeronym 2,051/146/746 163/224 218/13/86 755/48/240 -\nEvent 2,657/212/955 - - - -\nHypernym 924/63/350 255/382 1,327/94/459 3,048/202/1,042 2,232/149/809\nCo-hyponym 2,529/154/882 - - 18,134/1,313/6,349 2,222/162/816\nAttribute 1,892/143/696 - 903/72/322 - -\nPossession - - 377/25/142 - -\nAntonym - 241/360 1,095/90/415 - -\nSynonym - 167/235 759/50/277 - -\nTable 1: Number of instances for each relation type across training/validation/test sets of all lexical relation classi-\nﬁcation datasets.\nLexical Relation Classiﬁcation We consider the\ntask of predicting which relation a given word\npair belongs to. To solve this task, we train\na multi-layer perceptron (MLP) which takes the\n(frozen) RelBERT embedding of the word pair\nas input. We consider the following widely-\nused multi-class relation classiﬁcation benchmarks:\nK&H+N (Nec¸ sulescu et al., 2015), BLESS (Ba-\nroni and Lenci, 2011), ROOT09 (Santus et al.,\n2016b), EV ALution (Santus et al., 2015), and\nCogALex-V Subtask 2 (Santus et al., 2016a). Ta-\nble 1 shows the size of the training, validation\nand test sets for each of the relation classiﬁca-\ntion dataset. The hyperparameters of the MLP\nclassiﬁer are tuned on the validation set of each\ndataset. Concretely, we tune the learning rate from\n[0.001,0.0001,0.00001] and the hidden layer size\nfrom [100,150,200]. CogALex-V only has test-\ning fragments so for this dataset we employ the\ndefault conﬁguration of Scikit-Learn (Pedregosa\net al., 2011), which uses a 100-dimensional hidden\nlayer and is optimized using Adam with a learning\nrate of 0.001. These datasets focus on the follow-\ning lexical relations: co-hyponymy (cohyp), hyper-\nnymy (hyp), meronymy (mero), possession (poss),\nsynonymy (syn), antonymy (ant), attribute (attr),\nevent, and random (rand).\n4.3 Baselines\nAs baselines, we consider two standard word em-\nbedding models: GloVe (Pennington et al., 2014)\nand FastText (Bojanowski et al., 2017), where word\npairs are represented by the vector difference of\ntheir word embeddings (diff ).7 For the classiﬁca-\ntion experiments, we also consider the concatena-\n7Vector difference is the most common method for encod-\ning relations, and has been shown to be the most reliable in\nthe context of word analogies (Hakami and Bollegala, 2017).\ntion of the two word embeddings ( cat) and their\nelement-wise multiplication 8 (dot). We further-\nmore experiment with two pre-trained word pair\nembedding models: pair2vec (Joshi et al., 2019)\n(pair) and RELATIVE (Camacho-Collados et al.,\n2019) (rel). For these word pair embeddings, as\nwell as for RelBERT, we concatenate the embed-\ndings from both directions, i.e. (h,t) and (t,h).\nFor the analogy questions, two simple statistical\nbaselines are included: the expected random perfor-\nmance and a strategy based on point-wise mutual\ninformation (PMI) Church and Hanks (1990). In\nparticular, the PMI score of a word pair is com-\nputed using the English Wikipedia, with a ﬁxed\nwindow size of 10. We then choose the candidate\npair with the highest PMI as the prediction. Note\nthat this PMI-based method completely ignores the\nquery pair. We also compare with the published\nresults from Ushio et al. (2021), where a strategy\nis proposed to solve analogy questions by using\nLMs to compute an analogical proportion score.\nIn particular, a four-word tuple (a,b,c,d ) is en-\ncoded using a custom prompt and perplexity based\nscoring strategies are used to determine whether the\nword pair (a,b) has the same relation as the word\npair (c,d). Finally, for the SAT† dataset, we com-\npare with the published results from GPT-3 (Brown\net al., 2020), LRA (Turney, 2005) and SuperSim\n(Turney, 2013); for relation classiﬁcation we re-\nport the published results of the LexNet (Shwartz\net al., 2016) and SphereRE (Wang et al., 2019) re-\nlation classiﬁcation models, taking the results from\nthe latter publication. We did not reproduce these\nlatter methods in similar conditions as our work,\nand hence they are not fully comparable. More-\n8Multiplicative features have been shown to provide consis-\ntent improvements for word embeddings in supervised relation\nclassiﬁcation tasks (Vu and Shwartz, 2018).\n9050\nover, these approaches are a different nature, as\nthe aim of our work is to provide universal relation\nembeddings instead of task-speciﬁc models.\n5 Results\nIn this section, we present our main experimental\nresults, testing the relation embeddings learned by\nRelBERT on analogy questions (Section 5.1) and\nrelation classiﬁcation (Section 5.2).\n5.1 Analogy Questions\nTable 2 shows the accuracy on the analogy bench-\nmarks. The RelBERT models substantially outper-\nform the baselines on all datasets, except for the\nGoogle analogy dataset.9 Comparing the different\nprompt generation approaches, we can see that, sur-\nprisingly, the manual prompt consistently outper-\nforms the automatically-learned prompt strategies.\nOn SAT†, RelBERT outperforms LRA, which\nrepresents the state-of-the-art in the zero-shot set-\nting, i.e. in the setting where no training data from\nthe SAT dataset is used. RelBERT moreover out-\nperforms GPT-3 in the few-shot setting, despite not\nusing any training examples. In contrast, GPT-3\nencodes a number of training examples as part of\nthe prompt.\nIt can furthermore be noted that the other two re-\nlation embedding methods (i.e. pair2vec and REL-\nATIVE) perform poorly in this unsupervised task.\nThe analogical proportion score from Ushio et al.\n(2021) also underperforms RelBERT, even when\ntuned on dataset-speciﬁc tuning data.\n5.2 Lexical Relation Classiﬁcation\nTable 3 summarizes the results of the lexical rela-\ntion classiﬁcation experiments, in terms of macro\nand micro averaged F1 score. The RelBERT mod-\nels achieve the best results on all datasets except\nfor BLESS and K&H+N, where the performance\nof all models is rather close. We observe a particu-\nlarly large improvement over the word embedding\nand SotA models on the EV ALution dataset. When\ncomparing the different prompting strategies, we\nagain ﬁnd that the manual prompts perform sur-\nprisingly well, although the best results are now\nobtained with learned prompts in a few cases.\n9The Google analogy dataset has been shown to be biased\ntoward word similarity and therefore to be well suited to word\nembeddings (Linzen, 2016; Rogers et al., 2017).\nModel SAT† SAT U2 U4 Google BATS\nRandom 20.0 20.0 23.6 24.2 25.0 25.0\nPMI 23.3 23.1 32.9 39.1 57.4 42.7\nLRA 56.4 - - - - -\nSuperSim 54.8 - - - - -\nGPT-3 (zero)53.7 - - - - -\nGPT-3 (few)65.2* - - - - -\nRELATIVE 24.9 24.6 32.5 27.1 62.0 39.0\npair2vec 33.7 34.1 25.4 28.2 66.6 53.8\nGloVe 48.9 47.8 46.5 39.8 96.0 68.7\nFastText 49.7 47.8 43.0 40.7 96.6 72.0\nAnalogical Proportion Score\n·GPT-2 41.4 35.9 41.2 44.9 80.4 63.5\n·BERT 32.6 32.9 32.9 34.0 80.8 61.5\n·RoBERTa 49.6 42.4 49.1 49.1 90.8 69.7\nAnalogical Proportion Score (tuned)\n·GPT-2 57.8* 56.7* 50.9* 49.5* 95.2* 81.2*\n·BERT 42.8* 41.8* 44.7* 41.2* 88.8* 67.9*\n·RoBERTa 55.8* 53.4* 58.3* 57.4* 93.6* 78.4*\nRelBERT\n·Manual 69.5 70.6 66.2 65.3 92.4 78.8\n·AutoPrompt 61.0 62.3 61.4 63.0 88.2 74.6\n·P-tuning 54.0 55.5 58.3 55.8 83.4 72.1\nTable 2: Test accuracy (%) on analogy datasets. Re-\nsults marked with * are not directly comparable, as\nthey use a subset or the entire dataset to tune the model.\nResults in bold represent the best accuracy excluding\nthose marked with *. Underlined results show the best\naccuracy over all the models. Results in italics were\ntaken from the original papers.\n6 Analysis\nTo better understand how relation embeddings are\nlearned, in this section we analyze the model’s\nperformance in more detail.\n6.1 Training Data Overlap\nIn our main experiments, RelBERT is trained using\nthe SemEval 2012 Task 2 dataset. This dataset con-\ntains a broad range of semantic relations, including\nhypernymy and meronymy relations. This raises\nan important question: Does RelBERT provide us\nwith a way to extract relational knowledge from the\nparameters of the pre-trained LM, or is it learning\nto construct relation embeddings from the triples\nin the training set? What is of particular interest is\nwhether RelBERT is able to model types of rela-\ntions that it has not seen during training. To answer\nthis question, we conduct an additional experiment\nto evaluate RelBERT on lexical relation classiﬁca-\ntion, using a version that was trained without the re-\nlations from the Class Inclusion category, which is\nthe high-level category in the SemEval dataset that\n9051\nModel BLESS CogALexV EV ALution K&H+N ROOT09\nmacro micro macro micro macro micro macro micro macro micro\nGloVe\ncat 92.9 93.3 42.8 73.5 56.9 58.3 88.8 94.9 86.3 86.5\ncat+dot 93.1 93.7 51.9 79.2 55.9 57.3 89.6 95.1 88.8 89.0\ncat+dot+pair 91.8 92.6 56.4 81.1 58.1 59.6 89.4 95.7 89.2 89.4\ncat+dot+rel 91.1 92.0 53.2 79.2 58.4 58.6 89.3 94.9 89.3 89.4\ndiff 91.0 91.5 39.2 70.8 55.6 56.9 87.0 94.4 85.9 86.3\ndiff+dot 92.3 92.9 50.6 78.5 56.5 57.9 88.3 94.8 88.6 88.9\ndiff+dot+pair91.3 92.2 55.5 80.2 56.0 57.4 88.0 95.5 89.1 89.4\ndiff+dot+rel 91.1 91.8 52.8 78.6 56.9 57.9 87.4 94.6 87.7 88.1\nFastText\ncat 92.4 92.9 40.7 72.4 56.4 57.9 88.1 93.8 85.7 85.5\ncat+dot 92.7 93.2 48.5 77.4 56.7 57.8 89.1 94.0 88.2 88.5\ncat+dot+pair 90.9 91.5 53.0 79.3 56.1 58.2 88.3 94.3 87.7 87.8\ncat+dot+rel 91.4 91.9 50.6 76.8 57.9 59.1 86.9 93.5 87.1 87.4\ndiff 90.7 91.2 39.7 70.2 53.2 55.5 85.8 93.3 85.5 86.0\ndiff+dot 92.3 92.9 49.1 77.8 55.2 57.4 86.5 93.6 88.5 88.9\ndiff+dot+pair90.0 90.8 53.9 79.0 55.8 57.8 86.6 94.2 87.7 88.1\ndiff+dot+rel 90.6 91.3 53.6 78.2 57.1 58.0 86.3 93.4 86.9 87.4\nRelBERT\nManual 91.7 92.1 71.2 87.0 68.4 69.6 88.0 96.2 90.9 91.0\nAutoPrompt 91.9 92.4 68.5 85.1 69.5 70.5 91.3 97.1 90.0 90.3\nP-tuning 91.3 91.8 67.8 84.9 69.1 70.2 88.5 96.3 89.8 89.9\nSotA LexNET - 89.3 - - - 60.0 - 98.5 - 81.3\nSphereRE - 93.8 - - - 62.0 - 99.0 - 86.1\nTable 3: Macro/micro F1 score (%) for lexical relation classiﬁcation.\nBLESS CogALex EV AL K&H+N ROOT09\nrand 93.7 (+0.3) 94.3 (-0.2) - 97.9 (+0.2) 91.2 (-0.1)\nmero 89.8 (+1.4) 72.9 (+2.7) 69.2 (+1.9) 74.5 (+5.4) -\nevent 86.5 (-0.3) - - - -\nhyp 94.1 (+0.8) 60.9 (-0.7) 61.7 (-1.5) 93.5 (+5.0) 83.0 (-0.4)\ncohyp 96.4 (+0.3) - - 97.8 (+1.2) 97.4 (-0.5)\nattr 92.6 (+0.3) - 84.7 (+1.6) - -\nposs - - 67.1 (-0.2) - -\nant - 76.8 (-2.6) 81.3 (-0.9) - -\nsyn - 49.9 (-0.6) 53.6 (+2.7) - -\nmacro 92.2 (+0.5) 71.0 (-0.2) 69.3 (+0.9) 90.9 (+2.9) 90.5 (-0.4)\nmicro 92.5 (+0.4) 86.9 (-0.1) 70.2 (+0.6) 97.2 (+1.0) 90.7 (-0.3)\nTable 4: Per-class F1 score of RelBERT trained without\nhypernymy relations and the absolute difference with\nrespect to the original model (parentheses), along with\nthe macro and micro averaged F1 for each dataset (%).\nincludes the hypernymy relation. Hypernymy is of\nparticular interest, as it can be found across all the\nconsidered lexical relation classiﬁcation datasets,\nwhich is itself a reﬂection of its central importance\nin lexical semantics. In Table 4, we report the dif-\nference in performance compared to the original\nRelBERT model (i.e. the model that was ﬁne-tuned\non the full SemEval training set). As can be seen,\nthe overall changes in performance are small, and\nthe new version actually outperforms the original\nRelBERT model on a few datasets. In particular,\nhypernymy is still modelled well, conﬁrming that\nRelBERT is able to generalize to unseen relations.\nModel Google BATS\nMor Sem Mor Sem Lex\nFastText 95.4 98.1 90.4 71.1 33.8\nManual 89.8 95.8 87.0 66.2 75.1\nAutoPrompt 90.5 85.1 85.3 59.8 68.0\nP-tuning 87.4 78.1 82.9 60.9 61.8\nTable 5: Test accuracy for the high-level categories of\nBATS and Google, comparing FastText and RelBERT.\nAs a further analysis, Table 5 shows a break-\ndown of the Google and BATS analogy results,\nshowing the average performance on each of the\ntop-level categories from these datasets.10 While\nRelBERT is outperformed by FastText on the mor-\nphological relations, it should be noted that the\ndifferences are small, while such relations are of a\nvery different nature than those from the SemEval\ndataset. This conﬁrms that RelBERT is able to\nmodel a broad range of relations, although it can\nbe expected that better results would be possible\nby including task-speciﬁc training data into the\nﬁne-tuning process (e.g. including morphological\nrelations for tasks where such relations matter).\n10A full break-down showing the results for individual rela-\ntions is provided in the appendix.\n9052\nTarget Nearest Neighbors\nbarista:coffee baker:bread, brewer:beer, bartender:cocktail, winemaker:wine, bartender:drink, baker:cake\nbag:plastic bottle:plastic, bag:leather, container:plastic, box:plastic, jug:glass, bottle:glass\nduck:duckling chicken:chick, pig:piglet, cat:kitten, ox:calf, butterﬂy:larvae, bear:cub\ncooked:raw raw:cooked, regulated:unregulated, sober:drunk, loaded:unloaded, armed:unarmed, published:unpublished\nchihuahua:dog dachshund:dog, poodle:dog, terrier:dog, chinchilla:rodent, macaque:monkey, dalmatian:dog\ndog:dogs cat:cats, horse:horses, pig:pigs, rat:rats, wolf:wolves, monkey:monkeys\nspy:espionage pirate:piracy, robber:robbery, lobbyist:lobbying, scout:scouting, terrorist:terrorism, witch:witchcraft\nTable 6: Nearest neighbors of selected word pairs, in terms of cosine similarity between RelBERT embeddings.\nCandidate word pairs are taken from the RELATIVE pair vocabulary.\n6.2 Language Model Comparison\nFigure 3 compares the performance of RelBERT\nwith that of the vanilla pre-trained RoBERTa model\n(i.e. when only the prompt is optimized). As can be\nseen, the ﬁne-tuning process is critical for achiev-\ning good results. In Figure 3, we also compare the\nperformance of our main RelBERT model, which is\nbased on RoBERTa, with versions that were instead\ninitialized with BERT (Devlin et al., 2019) and\nALBERT (Lan et al., 2019).11 RoBERTa clearly\noutperforms the other two LMs, which is in accor-\ndance with ﬁndings from the literature suggesting\nthat RoBERTa captures more semantic knowledge\n(Li et al., 2020; Warstadt et al., 2020).\n6.3 Qualitative Analysis\nTo give further insight into the nature of RelBERT\nembeddings, Table 6 shows the nearest neighbors\nof some selected word pairs from the evaluation\ndatasets. To this end, we computed RelBERT re-\nlation vectors for all pairs in the Wikipedia pre-\ntrained RELATIVE vocabulary (over 1M pairs).12\nThe neighbors are those word pairs whose Rel-\nBERT embedding has the highest cosine similarity\nwithin the full pair vocabulary. As can be seen,\nthe neighbors mostly represent word pairs that are\nrelationally similar, even for morphological rela-\ntions (e.g. dog:dogs), which are not present in the\nSemEval dataset. A more extensive qualitative anal-\nysis, including a comparison with RELATIVE, is\nprovided in the appendix.\n7 Conclusion\nWe have proposed a strategy for learning relation\nembeddings, i.e. vector representations of pairs of\nwords which capture their relationship. The main\n11We used bert-large-cased and albert-\nxlarge-v1 from the HuggingFace model hub.\n12https://github.com/pedrada88/relative\nFigure 3: Test accuracy (%) on analogy dataset of the\nvanilla RoBERTa model (i.e. without ﬁne-tuning) and\nvariants of RelBERT with different language models.\nEach variant uses the best manual prompt based on the\nSemEval tuning data.\nidea is to ﬁne-tune a pre-trained language model us-\ning the relational similarity dataset from SemEval\n2012 Task 2, which covers a broad range of seman-\ntic relations. In our experimental results, we found\nthe resulting relation embeddings to be of high qual-\nity, outperforming state-of-the-art methods on sev-\neral analogy and relation classiﬁcation benchmarks.\nAmong the models tested, we obtained the best re-\nsults with RoBERTa, when using manually deﬁned\ntemplates for encoding word pairs. Importantly, we\nfound that high-quality relation embeddings can\nbe obtained even for relations that are unlike those\nfrom the SemEval dataset, such as morphological\nand encyclopedic relations. This suggests that the\nknowledge captured by our relation embeddings\nis largely distilled from the pre-trained language\nmodel, rather than being acquired during training.\nAcknowledgements\nJose Camacho-Collados acknowledges support\nfrom the UKRI Future Leaders Fellowship scheme.\n9053\nReferences\nMercedes Arguello Casteleiro, Julio Des Diz, Nava\nMaroto, Maria Jesus Fernandez Prieto, Simon Pe-\nters, Chris Wroe, Carlos Sevillano Torrado, Diego\nMaseda Fernandez, and Robert Stevens. 2020. Se-\nmantic deep learning: Prior knowledge and a type of\nfour-term embedding analogy to acquire treatments\nfor well-known diseases. JMIR Med Inform, 8(8).\nKevin D Ashley. 1988. Arguing by analogy in law: A\ncase-based model. In Analogical Reasoning, pages\n205–224. Springer.\nMarco Baroni and Alessandro Lenci. 2011. How we\nBLESSed distributional semantic evaluation. InPro-\nceedings of the GEMS 2011 Workshop on GEometri-\ncal Models of Natural Language Semantics, pages 1–\n10, Edinburgh, UK. Association for Computational\nLinguistics.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent dirichlet allocation. Journal of Ma-\nchine Learning Research, 3:993–1022.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nRishi Bommasani, Kelly Davis, and Claire Cardie.\n2020. Interpreting pretrained contextualized repre-\nsentations via reductions to static embeddings. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4758–\n4781.\nRajesh Bordawekar and Oded Shmueli. 2017. Using\nword embedding to enable semantic queries in re-\nlational databases. In Proceedings of the 1st Work-\nshop on Data Management for End-to-End Machine\nLearning, pages 1–4.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4762–4779.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 7456–\n7463.\nZied Bouraoui and Steven Schockaert. 2019. Auto-\nmated rule base completion as bayesian concept in-\nduction. In Proceedings of the Thirty-Third AAAI\nConference on Artiﬁcial Intelligence , pages 6228–\n6235.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Proceedings of the Annual Conference on\nNeural Information Processing Systems.\nJose Camacho-Collados, Luis Espinosa-Anke, Jameel\nShoaib, and Steven Schockaert. 2019. A latent vari-\nable model for learning distributional relation vec-\ntors. In Proceedings of IJCAI.\nKenneth Church and Patrick Hanks. 1990. Word as-\nsociation norms, mutual information, and lexicogra-\nphy. Computational linguistics, 16(1):22–29.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, and Furu\nWei. 2021. Knowledge neurons in pretrained trans-\nformers. arXiv preprint arXiv:2104.08696.\nJoe Davison, Joshua Feldman, and Alexander M Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing, pages 1173–\n1178.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLuis Espinosa-Anke and Steven Schockaert. 2018.\nSeVeN: Augmenting word embeddings with unsu-\npervised relation vectors. In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 2653–2665, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, pages 55–65.\nRaul Castro Fernandez, Essam Mansour, Abdul-\nhakim A Qahtan, Ahmed Elmagarmid, Ihab Ilyas,\nSamuel Madden, Mourad Ouzzani, Michael Stone-\nbraker, and Nan Tang. 2018. Seeping semantics:\nLinking datasets using word embeddings for data\ndiscovery. In 2018 IEEE 34th International Confer-\nence on Data Engineering, pages 989–1000.\n9054\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.\nDo neural language representations learn physical\ncommonsense? In Proceedings of the 41th An-\nnual Meeting of the Cognitive Science Society, pages\n1753–1759.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nMor Geva, Roei Schuster, Jonathan Berant, and\nOmer Levy. 2020. Transformer feed-forward\nlayers are key-value memories. arXiv preprint\narXiv:2012.14913.\nDaniel Gillick, Sayali Kulkarni, Larry Lansing,\nAlessandro Presta, Jason Baldridge, Eugene Ie, and\nDiego Garcia-Olano. 2019. Learning dense repre-\nsentations for entity retrieval. In Proceedings of\nthe 23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL) , pages 528–537, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAnna Gladkova, Aleksandr Drozd, and Satoshi Mat-\nsuoka. 2016. Analogy-based detection of morpho-\nlogical and semantic relations with word embed-\ndings: what works and what doesn’t. InProceedings\nof the NAACL Student Research Workshop, pages 8–\n15, San Diego, California. Association for Computa-\ntional Linguistics.\nAshok Goel. 2019. Computational design, analogy,\nand creativity. In Computational Creativity, pages\n141–158. Springer.\nYoav Goldberg. 2019. Assessing bert’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nHuda Hakami and Danushka Bollegala. 2017. Com-\npositional approaches for representing relations be-\ntween words: A comparative study. Knowledge-\nBased Systems, 136:172–182.\nTakaaki Hasegawa, Satoshi Sekine, and Ralph Grish-\nman. 2004. Discovering relations among named\nentities from large corpora. In Proceedings of the\n42nd Annual Meeting of the Association for Compu-\ntational Linguistics, pages 415–422.\nAdi Haviv, Jonathan Berant, and Amir Globerson.\n2021. BERTese: Learning to speak to BERT. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 3618–3623, Online.\nAssociation for Computational Linguistics.\nMarti A. Hearst. 1992. Automatic acquisition of hy-\nponyms from large text corpora. In COLING 1992\nVolume 2: The 14th International Conference on\nComputational Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word represen-\ntations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4129–4138.\nShoaib Jameel, Zied Bouraoui, and Steven Schockaert.\n2018. Unsupervised learning of distributional rela-\ntion vectors. In Annual Meeting of the Association\nfor Computational Linguistics, pages 23–33.\nStanislaw Jastrz˛ ebski, Dzmitry Bahdanau, Seyedarian\nHosseini, Michael Noukhovitch, Yoshua Bengio,\nand Jackie Chi Kit Cheung. 2018. Commonsense\nmining as knowledge base completion? a study on\nthe impact of novelty. In Proceedings of the Work-\nshop on Generalization in the Age of Deep Learning,\npages 8–16.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nMandar Joshi, Eunsol Choi, Omer Levy, Daniel Weld,\nand Luke Zettlemoyer. 2019. pair2vec: Composi-\ntional word-pair embeddings for cross-sentence in-\nference. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 3597–3608, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDavid Jurgens, Saif Mohammad, Peter Turney, and\nKeith Holyoak. 2012. SemEval-2012 task 2: Mea-\nsuring degrees of relational similarity. In *SEM\n2012: The First Joint Conference on Lexical and\nComputational Semantics – Volume 1: Proceedings\nof the main conference and the shared task, and Vol-\nume 2: Proceedings of the Sixth International Work-\nshop on Semantic Evaluation (SemEval 2012), pages\n356–364, Montréal, Canada. Association for Com-\nputational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6769–\n6781, Online. Association for Computational Lin-\nguistics.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. ALBERT: A lite BERT for self-supervised\n9055\nlearning of language representations. In Interna-\ntional Conference on Learning Representations.\nTeven Le Scao and Alexander M Rush. 2021. How\nmany data points is a prompt worth? arXiv e-prints,\npages arXiv–2103.\nChuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu,\nXuhui Zhou, and Shane Steinert-Threlkeld. 2020.\nLinguistically-informed transformations (LIT): A\nmethod for automatically generating contrast sets.\nIn Proceedings of the Third BlackboxNLP Workshop\non Analyzing and Interpreting Neural Networks for\nNLP, pages 126–135, Online. Association for Com-\nputational Linguistics.\nDekang Lin and Patrick Pantel. 2001. DIRT - discov-\nery of inference rules from text. In Proceedings of\nthe 7th Iternational Conference on Knowledge Dis-\ncovery and Data Mining, pages 323–328.\nTal Linzen. 2016. Issues in evaluating semantic spaces\nusing word analogies. In Proceedings of the 1st\nWorkshop on Evaluating Vector-Space Representa-\ntions for NLP , pages 13–18, Berlin, Germany. As-\nsociation for Computational Linguistics.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. GPT\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nDiego Marcheggiani and Ivan Titov. 2016. Discrete-\nstate variational autoencoders for joint discovery and\nfactorization of relations. Transactions of the Asso-\nciation for Computational Linguistics, 4:231–244.\nLaurent Miclet, Sabri Bayoudh, and Arnaud Delhay.\n2008. Analogical dissimilarity: deﬁnition, algo-\nrithms and two experiments in machine learning.\nJournal of Artiﬁcial Intelligence Research , 32:793–\n824.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013a. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems, pages 3111–3119.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013b. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies , pages 746–751, Atlanta,\nGeorgia. Association for Computational Linguistics.\nSilvia Nec¸ sulescu, Sara Mendes, David Jurgens, Núria\nBel, and Roberto Navigli. 2015. Reading between\nthe lines: Overcoming data sparsity for accurate clas-\nsiﬁcation of lexical relationships. In Proceedings\nof the Fourth Joint Conference on Lexical and Com-\nputational Semantics, pages 182–192, Denver, Col-\norado. Association for Computational Linguistics.\nFabian Pedregosa, Gaël Varoquaux, Alexandre Gram-\nfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron\nWeiss, Vincent Dubourg, Jake Vanderplas, Alexan-\ndre Passos, David Cournapeau, Matthieu Brucher,\nMatthieu Perrot, and Édouard Duchesnay. 2011.\nScikit-learn: Machine learning in python. Journal\nof Machine Learning Research, 12(85):2825–2830.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nElie Raad and Joerg Evermann. 2015. The role of anal-\nogy in ontology alignment: A study on LISA. Cog-\nnitive Systems Research, 33:1–16.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nSebastian Riedel, Limin Yao, Andrew McCallum, and\nBenjamin M. Marlin. 2013. Relation extraction with\nmatrix factorization and universal schemas. In Pro-\nceedings of the 2013 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n74–84.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5418–5426.\nAnna Rogers, Aleksandr Drozd, and Bofang Li. 2017.\nThe (too many) problems of analogical reasoning\nwith word vectors. In Proceedings of the 6th\nJoint Conference on Lexical and Computational Se-\nmantics (*SEM 2017) , pages 135–148, Vancouver,\nCanada. Association for Computational Linguistics.\n9056\nStephen Roller and Katrin Erk. 2016. Relations such\nas hypernymy: Identifying and exploiting hearst pat-\nterns in distributional vectors for lexical entailment.\nIn Proceedings of the 2016 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2163–2172, Austin, Texas. Association for Compu-\ntational Linguistics.\nEnrico Santus, Anna Gladkova, Stefan Evert, and\nAlessandro Lenci. 2016a. The CogALex-V shared\ntask on the corpus-based identiﬁcation of seman-\ntic relations. In Proceedings of the 5th Workshop\non Cognitive Aspects of the Lexicon (CogALex - V) ,\npages 69–79, Osaka, Japan. The COLING 2016 Or-\nganizing Committee.\nEnrico Santus, Alessandro Lenci, Tin-Shing Chiu, Qin\nLu, and Chu-Ren Huang. 2016b. Nine features\nin a random forest to learn taxonomical seman-\ntic relations. In Proceedings of the Tenth Inter-\nnational Conference on Language Resources and\nEvaluation (LREC’16), pages 4557–4564, Portorož,\nSlovenia. European Language Resources Associa-\ntion (ELRA).\nEnrico Santus, Frances Yung, Alessandro Lenci, and\nChu-Ren Huang. 2015. EV ALution 1.0: an evolving\nsemantic dataset for training and evaluation of distri-\nbutional semantic models. In Proceedings of the 4th\nWorkshop on Linked Data in Linguistics: Resources\nand Applications, pages 64–69, Beijing, China. As-\nsociation for Computational Linguistics.\nMaarten Sap, Ronan Le Bras, Emily Allaway, Chan-\ndra Bhagavatula, Nicholas Lourie, Hannah Rashkin,\nBrendan Roof, Noah A Smith, and Yejin Choi. 2019.\nAtomic: An atlas of machine commonsense for if-\nthen reasoning. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, pages 3027–3035.\nNaomi Saphra and Adam Lopez. 2019. Understanding\nlearning dynamics of language models with SVCCA.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 3257–3267.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 255–269, Online. Association for Com-\nputational Linguistics.\nFlorian Schroff, Dmitry Kalenichenko, and James\nPhilbin. 2015. Facenet: A uniﬁed embedding for\nface recognition and clustering. In Proceedings of\nthe IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 815–823.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nVered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.\nImproving hypernymy detection with an integrated\npath-based and distributional method. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2389–2398, Berlin, Germany. Associa-\ntion for Computational Linguistics.\nÉtienne Simon, Vincent Guigue, and Benjamin Pi-\nwowarski. 2019. Unsupervised information extrac-\ntion: Regularizing discriminative approaches with\nrelation distribution losses. In Proceedings of the\n57th Conference of the Association for Computa-\ntional Linguistics, pages 1378–1387.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching the\nblanks: Distributional similarity for relation learn-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2895–2905.\nRobert Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artiﬁcial Intelligence.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nDerek Tam, Rakesh R Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving\nand simplifying pattern exploiting training. arXiv\npreprint arXiv:2103.11955.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you learn\nfrom context? probing for sentence structure in con-\ntextualized word representations. In Proceeding of\nthe 7th International Conference on Learning Rep-\nresentations (ICLR).\nPeter D. Turney. 2005. Measuring semantic similarity\nby latent relational analysis. In Proceedings of the\nNineteenth International Joint Conference on Artiﬁ-\ncial Intelligence, pages 1136–1141.\nPeter D Turney. 2012. Domain and function: A dual-\nspace model of semantic relations and compositions.\nJournal of Artiﬁcial Intelligence Research , 44:533–\n585.\nPeter D. Turney. 2013. Distributional semantics be-\nyond words: Supervised learning of analogy and\nparaphrase. Transactions of the Association for\nComputational Linguistics, 1:353–366.\n9057\nPeter D. Turney, Michael L. Littman, Jeffrey Bigham,\nand Victor Shnayder. 2003. Combining independent\nmodules in lexical multiple-choice problems. In Re-\ncent Advances in Natural Language Processing III ,\npages 101–110.\nAsahi Ushio, Luis Espinosa-Anke, Steven Schockaert,\nand Jose Camacho-Collados. 2021. BERT is to NLP\nwhat AlexNet is to CV: Can Pre-Trained Language\nModels Identify Analogies? In Proceedings of the\nJoint Conference of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5831–5837, Hong Kong,\nChina. Association for Computational Linguistics.\nTu Vu and Vered Shwartz. 2018. Integrating multiplica-\ntive features into supervised distributional methods\nfor lexical entailment. In Proceedings of the Seventh\nJoint Conference on Lexical and Computational Se-\nmantics, pages 160–166, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nIvan Vulic, Edoardo Maria Ponti, Robert Litschko,\nGoran Glavas, and Anna Korhonen. 2020. Prob-\ning pretrained language models for lexical semantics.\nIn Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7222–7240.\nDouglas Walton. 2010. Similarity, precedent and argu-\nment from analogy. Artiﬁcial Intelligence and Law,\n18(3):217–246.\nChengyu Wang, Xiaofeng He, and Aoying Zhou. 2019.\nSphereRE: Distinguishing lexical relations with hy-\nperspherical relation embeddings. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1727–1737, Florence,\nItaly. Association for Computational Linguistics.\nAlex Warstadt, Yian Zhang, Xiaocheng Li, Haokun\nLiu, and Samuel R. Bowman. 2020. Learning which\nfeatures matter: RoBERTa acquires a preference for\nlinguistic generalizations (eventually). In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n217–235, Online. Association for Computational\nLinguistics.\nKoki Washio and Tsuneaki Kato. 2018. Filling missing\npaths: Modeling co-occurrences of word pairs and\ndependency paths for recognizing lexical semantic\nrelations. In Annual Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics, pages 1123–1133.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nLimin Yao, Aria Haghighi, Sebastian Riedel, and An-\ndrew McCallum. 2011. Structured relation discov-\nery using generative models. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1456–1466.\nLi Zhang, Shuo Zhang, and Krisztian Balog. 2019. Ta-\nble2vec: Neural word and entity embeddings for ta-\nble population and retrieval. In Proceedings of the\n42nd International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval ,\npages 1029–1032.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence , pages\n9733–9740.\nA Additional Experimental Results\nIn this section, we show additional experimental\nresults that complement the main results of the\npaper.\nA.1 Vanilla LM Comparison\nWe show comparisons of versions of RelBERT\nwith optimized prompt with/without ﬁnetuning.\nFigure 4 shows the absolute accuracy drop from\nRelBERT (i.e. the model with ﬁne-tuning) to the\nvanilla RoBERTa model (i.e. without ﬁne-tuning)\nwith the same prompt. In all cases, the accuracy\ndrop for the models without ﬁne-tuning is substan-\ntial.\nA.2 Comparison with ALBERT & BERT\nWe use RoBERTa in our main experiments and\nhere we train RelBERT with ALBERT and BERT\ninstead, and evaluate them on both of the analogy\nand relation classiﬁcation tasks. Table 7 shows the\naccuracy on the analogy questions, while Table 8\nshows the accuracy on the relation classiﬁcation\ntask. In both tasks, we can conﬁrm that RoBERTa\n9058\nFigure 4: Test accuracy drop of the vanilla models with-\nout ﬁne-tuning (measured in terms of absolute percent-\nage points in comparison with RelBERT) on analogy\ndatasets.\nachieves the best performance within the LMs, by\na relatively large margin in most cases.\nA.3 Word Embeddings\nTable 9 shows additional results of word embed-\ndings on analogy test together with RelBERT re-\nsults. We concatenate the RELATIVE and pair2vec\nvectors with the word vector difference. However,\nthis does not lead to better results.\nB Experimental Details and Model\nConﬁgurations\nIn this section, we explain models’ conﬁguration in\nthe experiments, and details on RelBERT’s training\ntime.\nB.1 Prompting Conﬁguration\nTable 10 shows the best prompt conﬁguration based\non the validation loss for the SemEval 2012 Task 2\ndataset in our main experiments using RoBERTa.\nB.2 MLP Conﬁguration in Relation\nClassiﬁcation\nTable 11 shows the best hyperparameters in the val-\nidation set of the MLPs for relation classiﬁcation.\nB.3 Training Time\nTraining a single RelBERT model with a custom\nprompt takes about half a day on two V100 GPUs.\nAdditionally, to achieve prompt by AutoPrompt\nModel SAT† SAT U2 U4 Google BATS\nALBERT\n·Manual 34.2 35.0 42.1 42.8 78.0 64.9\n·AutoPrompt 34.0 35.0 36.4 33.8 25.0 30.5\n·P-tuning 32.4 32.6 33.8 33.6 35.0 37.8\nBERT\n·Manual 40.6 40.9 41.2 43.5 78.2 67.9\n·AutoPrompt 36.4 36.5 36.8 35.4 51.6 43.5\n·P-tuning 38.0 38.0 38.2 37.0 56.6 45.3\nRoBERTa\n·Manual 69.5 70.6 66.2 65.3 92.4 78.8\n·AutoPrompt 61.0 62.3 61.4 63.0 88.2 74.6\n·P-tuning 54.0 55.5 58.3 55.8 83.4 72.1\nTable 7: Test accuracy (%) of ALBERT, BERT, and\nRoBERTa on analogy datasets.\ntechnique takes about a week on a single V100,\nwhile P-tuning takes 3 to 4 hours, also on a single\nV100.\nC Implementation Details of\nAutoPrompt\nAll the trigger tokens are initialized by mask tokens\nand updated based on the gradient of a loss function\nLt. Concretely, let us denote the loss value with\ntemplate T as Lt(T). The candidate set for the jth\ntrigger is derived by\n˜Wj = top-k\nw∈W\n[\neT\nw∇jLt(T)\n]\n(2)\nwhere the gradient is taken with respect to jth trig-\nger token and ew is the input embedding for the\nword w. Then we evaluate each token based on the\nloss function as\nzj = argmin\nw∈˜Wj\n[\nLt(rep(T,j,w ))\n]\n(3)\nwhere rep(T,j,w ) replaces the jth token in T by\nwand j is randomly chosen. We ignore any can-\ndidates that do not improve current loss value to\nfurther enhance the prompt quality.\nD Additional Analysis\nIn this section, we analyze our experimental re-\nsults based on prediction breakdown and provide\nan extended qualitative analysis.\nD.1 Prediction Breakdown\nTable 12 shows a detailed break-down of the BATS\nresults.\n9059\nModel BLESS CogALexV EV ALution K&H+N ROOT09\nmacro micro macro micro macro micro macro micro macro micro\nALBERT\nManual 86.2 87.1 54.9 81.1 62.6 62.2 82.6 91.7 86.4 86.8\nAutoPrompt 88.4 88.9 42.2 75.6 56.0 56.4 87.1 94.8 84.4 85.1\nP-tuning 90.1 90.6 44.9 73.1 58.2 59.7 90.2 95.9 85.9 85.9\nBERT\nManual 90.9 91.2 65.2 83.4 67.8 68.3 91.6 97.6 90.1 90.4\nAutoPrompt 90.3 90.7 40.6 75.8 60.4 59.5 90.2 97.2 86.6 86.1\nP-tuning 87.6 88.0 52.7 79.2 61.9 63.3 86.2 95.1 85.2 85.3\nRoBERTa\nManual 91.7 92.1 71.2 87.0 68.4 69.6 88.0 96.2 90.9 91.0\nAutoPrompt91.9 92.4 68.5 85.1 69.5 70.5 91.3 97.1 90.0 90.3\nP-tuning 91.3 91.8 67.8 84.9 69.1 70.2 88.5 96.3 89.8 89.9\nTable 8: Macro/micro F1 score (%) for lexical relation classiﬁcation of ALBERT, BERT, and RoBERTa.\nModel SAT† SAT U2 U4 Google BATSGloVe\ndiff 48.9 47.8 46.5 47.8 96.0 68.7\ndiff+rel 45.9 40.4 46.9 35.4 87.6 67.3\ndiff+pair 35.1 33.8 29.4 30.6 78.0 56.3\nFastText\ndiff 49.7 47.8 43.0 40.7 96.6 72.0\ndiff+rel 37.3 35.9 39.5 35.6 85.8 67.5\ndiff+pair 33.4 33.5 27.2 28.7 75.4 52.1\nRelBERT\nManual 69.5 70.6 66.2 65.3 92.4 78.8\nAutoPrompt 61.0 62.3 61.4 63.0 88.2 74.6\nP-tuning 54.0 55.5 58.3 55.8 83.4 72.1\nTable 9: Test accuracy (%) on analogy datasets (SAT† refers to the full SAT dataset).\nModel Prompt π τ γ template type\nBERT\nManual - - - 3\nAutoPrompt 8 2 3 -\nP-tuning 8 2 2 -\nALBERT\nManual - - - 4\nAutoPrompt 8 3 3 -\nP-tuning 8 2 3 -\nRoBERTa\nManual - - - 4\nAutoPrompt 9 2 2 -\nP-tuning 9 3 2 -\nTable 10: Best prompting conﬁguration.\nD.2 Qualitative Analysis\nTables 13 shows the nearest neighbors of a number\nof selected word pairs, in terms of their RelBERT\nand RELATIVE embeddings. In both cases cosine\nsimilarity is used to compare the embeddings and\nthe pair vocabulary of the RELATIVE model is\nused to determine the universe of candidate neigh-\nbors.\nThe results for the RelBERT embeddings show\ntheir ability to capture a wide range of relations. In\nmost cases the neighbors make sense, despite the\nfact that many of these relations are quite different\nfrom those in the SemEval dataset that was used\nfor training RelBERT. The results for RELATIVE\nare in general much noisier, suggesting that REL-\nATIVE embeddings fail to capture many types of\nrelations. This is in particular the case for the mor-\nphological examples, although various issues can\nbe observed for the other relations as well.\n9060\nModel Data Manual AutoPrompt P-tuning\nALBERT\nBLESS (1e-4, 150) (1e-3, 200) (1e-4, 150)\nCogA (1e-3, 100) (1e-3, 100) (1e-3, 100)\nEV AL (1e-4, 100) (1e-3, 200) (1e-4, 100)\nK&H (1e-4, 150) (1e-4, 150) (1e-4, 200)\nROOT (1e-5, 200) (1e-4, 100) (1e-4, 150)\nBERT\nBLESS (1e-4, 200) (1e-3, 100) (1e-3, 100)\nCogA (1e-3, 100) (1e-3, 100) (1e-3, 100)\nEV AL (1e-5, 150) (1e-3, 200) (1e-3, 100)\nK&H (1e-4, 200) (1e-4, 200) (1e-3, 150)\nROOT (1e-5, 100) (1e-3, 150) (1e-4, 150)\nRoBERTa\nBLESS (1e-5, 200) (1e-3, 100) (1e-4, 200)\nCogA (1e-3, 100) (1e-3, 100) (1e-3, 100)\nEV AL (1e-4, 150) (1e-5, 100) (1e-5, 200)\nK&H (1e-3, 200) (1e-3, 150) (1e-5, 200)\nROOT (1e-5, 100) (1e-5, 200) (1e-3, 200)\nTable 11: Best MLP conﬁguration for the relation clas-\nsiﬁcation experiment. Each entry shows the learning\nrate and hidden layer size. Note that CogALex uses the\ndefault conﬁguration due to the lack of validation set.\n9061\nRelation FastText Manual AutoPrompt P-tuning\nEncyclopedic\nUK city:county 33.3 28.9 28.9 40.0\nanimal:shelter 44.4 88.9 77.8 84.4\nanimal:sound 80.0 86.7 82.2 75.6\nanimal:young 53.3 62.2 64.4 51.1\ncountry:capital 82.2 37.8 17.8 35.6\ncountry:language 93.3 51.1 55.6 51.1\nmale:female 88.9 60.0 55.6 62.2\nname:nationality 60.0 73.3 51.1 40.0\nname:occupation 86.7 75.6 75.6 77.8\nthings:color 88.9 97.8 88.9 91.1\nLexical\nantonyms:binary 26.7 64.4 68.9 77.8\nantonyms:gradable 44.4 88.9 93.3 88.9\nhypernyms:animals 44.4 91.1 80.0 55.6\nhypernyms:misc 42.2 71.1 60.0 64.4\nhyponyms:misc 31.1 55.6 55.6 48.9\nmeronyms:member 44.4 68.9 48.9 53.3\nmeronyms:part 31.1 77.8 71.1 55.6\nmeronyms:substance 26.7 75.6 66.7 53.3\nsynonyms:exact 17.8 80.0 71.1 66.7\nsynonyms:intensity 28.9 77.8 64.4 53.3\nMorphological\nadj+ly 95.6 84.4 88.9 82.2\nadj+ness 100.0 97.8 93.3 97.8\nadj:comparative 100.0 97.8 100.0 91.1\nadj:superlative 97.8 100.0 93.3 100.0\nnoun+less 77.8 100.0 97.8 100.0\nover+adj 75.6 84.4 80.0 82.2\nun+adj 60.0 97.8 91.1 97.8\nverb 3pSg:v+ed 100.0 75.6 84.4 68.9\nverb inf:3pSg 100.0 93.3 91.1 84.4\nverb inf:v+ed 100.0 91.1 91.1 88.9\nverb inf:v+ing 100.0 97.8 97.8 95.6\nverb v+ing:3pSg 97.8 82.2 68.9 68.9\nverb v+ing:v+ed 97.8 86.7 82.2 84.4\nverb+able 97.8 93.3 80.0 84.4\nverb+er 95.6 100.0 95.6 95.6\nverb+ment 95.6 77.8 77.8 62.2\nverb+tion 84.4 77.8 66.7 68.9\nnoun:plural 78.7 87.6 88.8 69.7\nre+verb 75.6 62.2 86.7 66.7\nTable 12: Break-down of BATS results per relation type.\n9062\nCategory Target Nearest Neighbors RelBERT Nearest Neighbors RELATIVE\nCommonsense\nbarista:coffee baker:bread, brewer:beer, bartender:cocktail, winemaker:wine, bartender:drink, baker:cake venue:bar, restaurant:kitchen, restaurant:grill, nightclub:open, pub:bar, night:concert\nrestaurant:waitress restaurant:waiter, diner:waitress, bar:bartender, hospital:nurse, courthouse:clerk, ofﬁce:clerk coincidentally:ﬁrst, ironically:ﬁrst, ironically:name, notably:three, however:new, instance:character\ncar:garage car:pit, plane:hangar, auto:garage, baby:crib, yacht:harbour, aircraft:hangar shelter:house, elevator:building, worker:mine, worker:factory, plane:hangar, horse:stable\nice:melt snow:melt, glacier:melt, ice:drift, crust:melt, polar ice:melt, ice:thaw glacier:melt, snow:melt, water:freeze, crack:form, ice:surface, ice:freeze\ndolphin:swim squid:swim, salmon:swim, shark:swim, ﬁsh:swim, horse:run, frog:leap ﬁsherman:ﬁsh, ﬁsherman:catch, must:protect, diver:underwater, dog:human, scheme:make\nﬂower:fragrant orchid:fragrant, cluster:fragrant, jewel:precious, jewel:valuable, soil:permeable, vegetation:abundant ﬂower:greenish, ﬂower:white, ﬂower:yellowish, ﬂower:creamy, ﬂower:pale yellow, ﬂower:arrange\ncoconut:milk coconut:oil, goat:milk, grape:juice, palm:oil, olive:oil, camel:milk dry:powder, mix:sugar, candy:chocolate, cook:fry, butter:oil, milk:coffee\nbag:plastic bottle:plastic, bag:leather, container:plastic, box:plastic, jug:glass, bottle:glass tube:glass, bottle:plastic, typically:plastic, frame:steel, shoe:leather, wire:metal\nduck:duckling chicken:chick, pig:piglet, cat:kitten, ox:calf, butterﬂy:larvae, bear:cub adult:young, worker:queen, queen:worker, bird:ﬂy, chick:adult, female:mat\nGender man:woman men:women, male:female, father:mother, boy:girl, hero:heroine, king:queen man:boy, woman:child, child:youth, ofﬁcer:crew, bride:groom, child:teen\nAntonymy cooked:raw raw:cooked, regulated:unregulated, sober:drunk, loaded:unloaded, armed:unarmed, published:unpublished annual:biennial, raw:cook, herb:subshrub, aquatic:semi, shrub:small, fry:grill\nnormal:abnormal ordinary:unusual, usual:unusual, acceptable:unacceptable, stable:unstable, rational:irrational, legal:illegal acute:chronic, mouse:human, negative:positive, fat:muscle, cell:tissue, motor:sensory\nMeronymy\nhelicopter:rotor helicopter:rotor blades, helicopter:wing, bicycle:wheel, motorcycle:wheel, airplane:engine, plane:engine aircraft:engine, engine:crankshaft, landing gear:wheel, engine:camshaft, rotor:blade, aircraft:wing\nbat:wing butterﬂy:wing, eagle:wing, angel:wing, cat:paw, lion:wings, ﬂy:wing mouse:tail, dog:like, dragon:like, human:robot, leopard:spot, cat:like\nbeer:alcohol wine:alcohol, cider:alcohol, soda:sugar, beer:liquor, beer:malt, lager:alcoho steel:carbon, cider:alcohol, humidity:average, rate:average, household:non, consume:beer\noxygen:atmosphere helium:atmosphere, hydrogen:atmosphere, nitrogen:atmosphere, methane:atmosphere, carbon:atmosphere carbon dioxide:atmosphere, cloud:atmosphere, methane:atmosphere, nitrogen:soil, gas:atmosphere\nHypernymy\nchihuahua:dog dachshund:dog, poodle:dog, terrier:dog, chinchilla:rodent, macaque:monkey, dalmatian:dog julie:katy, tench:pike, catﬁsh:pike, sunﬁsh:perch, salmonid:salmon, raw:marinate\npelican:bird toucan:bird, pufﬁn:bird, egret:bird, peacock:bird, grouse:bird, pigeon:bird drinking:contaminate, drinking:source, pelican:distinctive, boiling:pour, aquifer:table, fresh:source\ntennis:sport hockey:sport, soccer:sport, volleyball:sport, cricket:sport, golf:sport, football:sport hockey:sport, golf:sport, badminton:sport, boxing:sport, rowing:sport, volleyball:sport\nMorphology\ndog:dogs cat:cats, horse:horses, pig:pigs, rat:rats, wolf:wolves, monkey:monkeys shepherd:dog, landrace:breed, like:dog, farm:breed, breed:animal, captive:release\ntall:tallest strong:strongest, short:shortest, smart:smartest, weak:weakest, big:biggest, small:smallest rank:world, summit:nato, redistricting:district, delegation:congress, debate:congress\nspy:espionage pirate:piracy, robber:robbery, lobbyist:lobbying, scout:scouting, terrorist:terrorism, witch:witchcraft group:call, crime:criminal, action:involve, cop:police, action:one, group:make\nTable 13: Nearest neighbors of selected word pairs, in terms of cosine similarity between RelBERT embeddings. Candidate word pairs are taken from the RELATIVE pair\nvocabulary.",
  "topic": "Relation (database)",
  "concepts": [
    {
      "name": "Relation (database)",
      "score": 0.8100299835205078
    },
    {
      "name": "Computer science",
      "score": 0.8085300922393799
    },
    {
      "name": "Word (group theory)",
      "score": 0.7265751361846924
    },
    {
      "name": "Natural language processing",
      "score": 0.6676978468894958
    },
    {
      "name": "ENCODE",
      "score": 0.6486563086509705
    },
    {
      "name": "Language model",
      "score": 0.6294687986373901
    },
    {
      "name": "Artificial intelligence",
      "score": 0.612614095211029
    },
    {
      "name": "Analogy",
      "score": 0.6023989915847778
    },
    {
      "name": "Code (set theory)",
      "score": 0.5846940875053406
    },
    {
      "name": "Task (project management)",
      "score": 0.493403822183609
    },
    {
      "name": "Encoding (memory)",
      "score": 0.435271292924881
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.1574993133544922
    },
    {
      "name": "Linguistics",
      "score": 0.13804161548614502
    },
    {
      "name": "Programming language",
      "score": 0.11372897028923035
    },
    {
      "name": "Data mining",
      "score": 0.09917590022087097
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}