{
  "title": "Devignet: High-Resolution Vignetting Removal via a Dual Aggregated Fusion Transformer with Adaptive Channel Expansion",
  "url": "https://openalex.org/W4393148449",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3029425806",
      "name": "Sheng-hong Luo",
      "affiliations": [
        "University of Macau"
      ]
    },
    {
      "id": "https://openalex.org/A3179930206",
      "name": "Xuhang Chen",
      "affiliations": [
        "University of Macau",
        "Huizhou University",
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2115667793",
      "name": "Weiwen Chen",
      "affiliations": [
        "University of Macau",
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4280806207",
      "name": "Zinuo Li",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology",
        "University of Macau"
      ]
    },
    {
      "id": "https://openalex.org/A2102895376",
      "name": "Shuqiang Wang",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2634090761",
      "name": "Chi-Man Pun",
      "affiliations": [
        "University of Macau"
      ]
    },
    {
      "id": "https://openalex.org/A3179930206",
      "name": "Xuhang Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2115667793",
      "name": "Weiwen Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4280806207",
      "name": "Zinuo Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102895376",
      "name": "Shuqiang Wang",
      "affiliations": [
        "Shenzhen Institutes of Advanced Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6675893205",
    "https://openalex.org/W2025328853",
    "https://openalex.org/W6747510817",
    "https://openalex.org/W6810161528",
    "https://openalex.org/W2989899624",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1640745651",
    "https://openalex.org/W2468596194",
    "https://openalex.org/W2139851675",
    "https://openalex.org/W3002804594",
    "https://openalex.org/W2566376500",
    "https://openalex.org/W2950335850",
    "https://openalex.org/W2150721269",
    "https://openalex.org/W3134649899",
    "https://openalex.org/W4386301796",
    "https://openalex.org/W4385764641",
    "https://openalex.org/W3160809836",
    "https://openalex.org/W3107113662",
    "https://openalex.org/W3111901135",
    "https://openalex.org/W1144003762",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2054814429",
    "https://openalex.org/W4320858091",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W6754146604",
    "https://openalex.org/W2943838036",
    "https://openalex.org/W1987443165",
    "https://openalex.org/W2122742285",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4230472795",
    "https://openalex.org/W3035731588",
    "https://openalex.org/W4288359455",
    "https://openalex.org/W4312938066",
    "https://openalex.org/W2998154526",
    "https://openalex.org/W3174792937",
    "https://openalex.org/W4231697575",
    "https://openalex.org/W3172619439",
    "https://openalex.org/W4225506672",
    "https://openalex.org/W3121661546",
    "https://openalex.org/W2780108394",
    "https://openalex.org/W4312812783"
  ],
  "abstract": "Vignetting commonly occurs as a degradation in images resulting from factors such as lens design, improper lens hood usage, and limitations in camera sensors. This degradation affects image details, color accuracy, and presents challenges in computational photography. Existing vignetting removal algorithms predominantly rely on ideal physics assumptions and hand-crafted parameters, resulting in the ineffective removal of irregular vignetting and suboptimal results. Moreover, the substantial lack of real-world vignetting datasets hinders the objective and comprehensive evaluation of vignetting removal. To address these challenges, we present VigSet, a pioneering dataset for vignetting removal. VigSet includes 983 pairs of both vignetting and vignetting-free high-resolution (over 4k) real-world images under various conditions. In addition, We introduce DeVigNet, a novel frequency-aware Transformer architecture designed for vignetting removal. Through the Laplacian Pyramid decomposition, we propose the Dual Aggregated Fusion Transformer to handle global features and remove vignetting in the low-frequency domain. Additionally, we propose the Adaptive Channel Expansion Module to enhance details in the high-frequency domain. The experiments demonstrate that the proposed model outperforms existing state-of-the-art methods. The code, models, and dataset are available at https://github.com/CXH-Research/DeVigNet.",
  "full_text": "Devignet: High-Resolution Vignetting Removal via a Dual Aggregated Fusion\nTransformer with Adaptive Channel Expansion\nShenghong Luo1*, Xuhang Chen123*, Weiwen Chen12, Zinuo Li12,\nShuqiang Wang2†, Chi-Man Pun1†\n1University of Macau\n2Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences\n3Huizhou University\nAbstract\nVignetting commonly occurs as a degradation in images re-\nsulting from factors such as lens design, improper lens hood\nusage, and limitations in camera sensors. This degradation af-\nfects image details, color accuracy, and presents challenges\nin computational photography. Existing vignetting removal\nalgorithms predominantly rely on ideal physics assumptions\nand hand-crafted parameters, resulting in the ineffective re-\nmoval of irregular vignetting and suboptimal results. More-\nover, the substantial lack of real-world vignetting datasets\nhinders the objective and comprehensive evaluation of vi-\ngnetting removal. To address these challenges, we present\nVigSet, a pioneering dataset for vignetting removal. VigSet\nincludes 983 pairs of both vignetting and vignetting-free\nhigh-resolution (over 4k) real-world images under various\nconditions. In addition, We introduce DeVigNet, a novel\nfrequency-aware Transformer architecture designed for vi-\ngnetting removal. Through the Laplacian Pyramid decom-\nposition, we propose the Dual Aggregated Fusion Trans-\nformer to handle global features and remove vignetting in\nthe low-frequency domain. Additionally, we propose the\nAdaptive Channel Expansion Module to enhance details in\nthe high-frequency domain. The experiments demonstrate\nthat the proposed model outperforms existing state-of-the-\nart methods. The code, models, and dataset are available at\nhttps://github.com/CXH-Research/DeVigNet.\nIntroduction\nVignetting is a common optical degradation that results in a\ngradual decrease in brightness toward the edges of an image.\nIt occurs due to multiple factors such as lens characteristics,\nfilter presence, aperture settings, focal length settings, etc.\nSome may confuse the difference between Low-Light Im-\nage Enhancement (LLIE) and vignetting removal. LLIE fo-\ncuses on enhancing the overall brightness of images cap-\ntured in low-light conditions. Its goal is to improve visibil-\nity, reduce noise, and enhance contrast in dark regions. On\nthe other hand, vignetting removal specifically addresses the\nuneven light projection effects in specific regions of an im-\nage, typically towards the edges. Its purpose is to correct this\n*These authors contributed equally.\n†Corresponding Author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n(a) Input\n (b) RIVC\n (c) SIVC\n(d) ELGAN\n (e) Ours\n (f) Target\nFigure 1: The visual results display the effects of different\nmethods on the input (a) with vignetting, including the tra-\nditional methods (b) and (c), the LLIE model (d), and Devi-\ngNet (e). Our model effectively removes vignetting.\neffect and restore a more uniform brightness across the im-\nage. Therefore, these two tasks serve distinct purposes and\naim to enhance different aspects of image quality.\nThere are mathematical and prior-based methods avail-\nable for vignetting removal (Zheng et al. 2008, 2013; Lopez-\nFuentes, Oliver, and Massanet 2015). Nevertheless, these ap-\nproaches have limitations. These approaches ideally assume\nthat the optical center is located at the center of the image,\nwhich may not be valid in real-world scenarios. Moreover,\nthese methods can demonstrate bias under certain conditions\nand frequently necessitate extensive parameter adjustments\nto achieve optimal performance. In addition, these param-\neters are highly sensitive to high-resolution images, often\nleading to inferior outcomes. Another significant challenge\narises from the absence of ground truth in the datasets used\nfor evaluation, which contributes to subjective assessments\nof the experimental results.\nTo tackle the issue of vignetting, we introduce a dataset\nnamed VigSet. The dataset comprises 983 pairs of images\ncaptured under various environmental and lighting condi-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4000\ntions. Each pair consists of an image captured under op-\ntimal lighting conditions, without vignetting, and its cor-\nresponding vignetting-free ground truth image. Addition-\nally, we present DeVigNet, a network that employs Dual\nAggregated Fusion Transformer and Adaptive Channel Ex-\npansion for vignetting removal. We utilize the Laplacian\nPyramid (Burt and Adelson 1987; Liang, Zeng, and Zhang\n2021; Li et al. 2023a,b) to decompose the image into high-\nfrequency and low-frequency components (You et al. 2022).\nThis decomposition enhances vignetting removal by im-\nage sharpening and edge enhancement. We introduce the\nDual Aggregated Fusion Transformer for vignetting removal\nin the low-frequency component, as it primarily contains\nsmoother color information that represents the overall color\nand distribution. Additionally, the high-frequency compo-\nnent utilizes the Adaptive Channel Expansion Module for\nhandling color edges, texture details, and specific color char-\nacteristics. Our experimental results demonstrate the state-\nof-the-art performance of our method. In addition, Figure 1\nprovides an intuitive indication of the favorable outcomes\nattained by DeVigNet. The main contributions of this article\nare as follows:\n• We present VigSet, the first vignetting dataset that in-\ncludes high-resolution vignetting images along with the\ncorresponding vignetting-free ground truth. VigSet aims\nto alleviate the current scarcity of vignetting datasets by\nproviding a substantial number of samples accompanied\nby accurate ground truth information.\n• We propose DeVigNet, a network that is based on\nthe Dual Aggregated Fusion Transformer and Adaptive\nChannel Expansion. It represents the first learning-based\nmodel for high-resolution vignetting removal.\n• Quantitative and qualitative experiments demonstrate\nthat DeVigNet outperforms state-of-the-art methods on\nvignetting datasets.\nRelated Work\nVignetting Removal\nA limited number of studies in the field of traditional\nvignetting removal has proposed methods that are based\non mathematical principles, statistical analysis, and prior\nknowledge. SIVC (Zheng et al. 2013) utilizes the symme-\ntry properties of semicircular tangential gradients and RG\ndistributions to estimate the optical center and correct vi-\ngnetting. Goldman et al. assume that the vignetting in the\nimage exhibits radial symmetry around its center (Goldman\n2010). RIVC (Lopez-Fuentes, Oliver, and Massanet 2015),\naddresses vignetting removal through the minimization of\nthe log-intensity entropy.\nLow-Light Image Enhancement\nTraditional methods (Wang et al. 2013; Guo, Li, and Ling\n2016; Jobson, Rahman, and Woodell 1997; Cai et al. 2017;\nFu et al. 2016, 2015) for Low-Light Image Enhancement\noften refers to the Retinex theory or histogram equalization.\nRecently, the utilization of these learning-based ap-\nproaches gain traction as a prevalent solution for enhanc-\ning low-light images. Several widely recognized datasets\nare utilized by these learning-based methods. For instance,\nWei et al. propose a Low-Light dataset (LOL) (Wei et al.\n2018) containing pairs of low/normal-light images. The\nMIT-Adobe FiveK dataset (Bychkovsky et al. 2011) com-\nprises 5,000 photos that are manually annotated. To ensure\nthe highest quality, the dataset underwent retouching by a\nteam of 5 trained photographers, rendering it well-suited for\nsupervised learning in the context of LLIE. In terms of effec-\ntive methods, certain technologies also made contributions.\nWang et al. propose Uformer (Wang et al. 2022) employ-\ning both local and global dependencies to restore images.\nThe KinD (Zhang, Zhang, and Guo 2019) method proposed\nby Zhang et al. can be trained and achieves impressive re-\nsults without the need for explicitly defining a ground truth\ndataset. Liu et al. (Liu et al. 2021) present a retinex-based\nnetwork that is both lightweight and efficient. Guo et al. en-\nables end-to-end training without the need for reference im-\nages (Guo et al. 2020). Li et al. (Li, Guo, and Loy 2021)\nintroduce an adaptive LLIE network that can operate under\ndiverse lighting conditions without dependence on paired or\nunpaired training data. Lim et al. (Lim and Kim 2020) pro-\npose a method that can independently recover global illumi-\nnation and local details from the original input, and gradu-\nally merge them in the image space. Jiang et al. (Jiang et al.\n2021) put forward unsupervised learning into the realm of\nLLIE using GAN.\nDataset\nIn our research on vignetting removal, we encounter limi-\ntations in the existing datasets available for evaluating the\nperformance of these methods. These datasets often consist\nof low-quality images or lack the necessary characteristics\nfor effectively assessing vignetting removal algorithms. Un-\nfortunately, at present, there is no accessible dataset exclu-\nsively designed for vignetting removal that provides reliable\nground truth for objective evaluation.\nConsequently, we introduce VigSet, the first high-\nresolution dataset that offers a comprehensive collection of\npaired vignetting and ground truth images, specifically de-\nsigned for vignetting removal. It consists of 983 pairs of\nphotos captured by DSLR camera and two mobile phones.\nWhat distinguishes VigSet from other vignetting datasets is\nits exceptional diversity, substantial quantity, and most no-\ntably, the inclusion of accurate ground truth. This ground\ntruth information serves as a reliable reference for evalu-\nating the effectiveness and performance of vignetting re-\nmoval algorithms. Furthermore, VigSet stands out as a high-\nresolution dataset, with images boasting an impressive mean\nresolution of 5340 ×3697. This high-resolution characteris-\ntic enables researchers and practitioners to conduct detailed\nanalyses and evaluations of vignetting removal techniques.\nEquipment for Data Collection\nVigSet employs a variety of three distinct capture devices:\nFujifilm X-T4, ONE PLUS 10PRO, and iPhone SE.\nIn well-lit situations, vignetting is generally not notice-\nable. Therefore, we use an ND filter to reduce illumination\nand capture photos with vignetting. The center light, which\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4001\npasses through an ND filter, travels a shorter distance com-\npared to the light at the edges. This difference in distance\ncontributes to the occurrence of vignetting.\n(a)\n (b)\n (c)\nFigure 2: (a) represents the visual depiction of our data ac-\nquisition equipment. (b) is the image showcasing of ND fil-\nter. and (c) illustrates the data collection process conducted\nby us under optimal lighting conditions along with a preview\nafter the installation of the ND filter.\nAdditionally, to ensure that there is no displacement be-\ntween each pair of photos and to enhance data diversity, it\nis crucial to avoid dynamic objects such as swaying tree\nleaves, moving vehicles, and glass reflections on specific\nobjects that are susceptible to motion. Therefore, we select\nmultiple distinct indoor environments for the data collection.\nTo reduce device vibration caused by manual camera shut-\nter presses, we use remote shutter control to capture steps 2\nand 4, as illustrated in Figure 2 (c).\n(a)\n (b)\nFigure 3: (a) represents the image without vignetting. (b)\nrepresents the image with vignetting.\nData Collection and Processing\nFigure 2 (a) provides a preview of our experimental config-\nuration designed for capturing VigSet. The process of col-\nlecting data is facilitated by adjusting the white ND value\ncontroller located on the camera ND filter, as depicted in\nFigure 2 (c). During the data collection process, we capture\nimages from real-world scenes utilizing experimental equip-\nment. We use a tripod to stabilize the camera and control un-\nrelated variables. The steps of the data collection process are\nas follows.\n1. The camera settings, such as focal length, aperture, ex-\nposure, and ISO, are set to fixed values.\n2. A vignetting-free image is captured with an ND value of\n0, as shown in Figure 3 (a)\n3. Continuously modify the value of the ND filter until no-\nticeable vignetting becomes visible in the image.\n4. An image with vignetting is captured, as shown in Fig-\nure 3 (b)\nWe exclude images exhibiting obvious motion distortion or\nlack of focus. Moreover, we conduct group reviews of these\nphotographs to identify and remove any outliers and dupli-\ncates.\nMethodology\nWe propose a multi-frequency network based on Dual Ag-\ngregated Fusion Transformer with Adaptive Channel Ex-\npansion to exploit the features of images at various scales.\nSpecifically, the structure of DeVigNet, as illustrated in Fig-\nure 4, includes three major components: The Dual Aggre-\ngated Fusion Transformer (DAFT), the Adaptive Channel\nExpansion Module (ACEM), and the Hierarchical Channel\nAttention Module (HCAM). In the upcoming sections, we\nwill introduce these components of DeVigNet and the learn-\ning criteria.\nDual Aggregated Fusion Transformer\nThe Dual Aggregated Fusion Transformer (DAFT) is a\nneural network designed specifically for handling low-\nfrequency information in images. As the foundational archi-\ntecture, the Fusion Transformer employs multiple attention\nmechanisms to capture various points of focus, enabling the\nmodel to effectively incorporate both local and global in-\nformation within its representation. Fusion Transformer sig-\nnificantly boosts the expressive capacity of the Transformer\nnetwork. Figure 4 illustrates the location of these four fu-\nsion transformers, wherein each transformer comprises two\nmodules. Each module contains 1, 2, 3, or 4 Transformer\nBlocks (Dosovitskiy et al. 2021) from left to right. The out-\nput of the first module is passed on to the second module,\nand the final output is obtained by adding the output of the\nsecond module to the output of the first module.\nInspired by (Cun, Pun, and Shi 2020), the Aggregation\nNode is designed as integration operations on the input fea-\ntures. By aggregating the semantic information extracted by\nthe fusion transformer, a richer and more comprehensive\nglobal feature representation can be obtained. This Structure\nfacilitates an improved understanding of the image’s struc-\nture, texture, and global properties by the model in low fre-\nquency, leading to enhancing vignetting removal.\nAdaptive Channel Expansion Module\nWhile the primary cause of vignetting is low-frequency\ncolor information, some edge information of the images re-\nmains. Therefore, motivated by U-Net (Ronneberger, Fis-\ncher, and Brox 2015), we propose a structure that inte-\ngrates Adaptive Contrast Enhancement Module (ACEM)\nand Laplacian pyramid reconstruction for optimal vignetting\nremoval results in high-frequency.\nACEM is a lightweight module that does not employ any\nactivation functions, and it is proven that there will be no\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4002\nX8\nX8\n2I\nHierarchical Channel Attention Module\nRGB Aggregation Fusion Transformer\nAdaptive Channel Expansion Module\nConv 1X1 MAS Aggregation\nTransforming\nS SimpleGate\nInner-product Attention\nDconv 3X3\nNorm\nS\nCONV\nDCONV\nSCA\nCONV\nNorm\nCONV\nS\nCONV\nHCAM ACEM\nX8\nOutput Image\n1 n\n^\n−I\n1\n^\nI\nn\n^\nI\nnI\n1I\nDual Aggregated Fusion Transformer\nFigure 4: The overall architecture of DeVigNet comprises the Dual Aggregated Fusion Transformer, Adaptive Channel Ex-\npansion Module, and Hierarchical Channel Attention Module (HCAM). Initially, the input image is decomposed into its high-\nfrequency and low-frequency components using a Laplacian pyramid. Subsequently, the low-frequency component of the image\nundergoes processing with DAFT and HCAM to capture global information. Next, ACEM is employed in the reconstruction of\nthe high-frequency component of the Laplacian pyramid to enhance edge information. Finally, a vignetting-free image result is\nproduced.\ndecrease in performance. Inspired by (Ba, Kiros, and Hinton\n2016), the beginning section of ACEM is LayerNorm, which\nimproves stability and reduces computational overhead. The\nfollowing convolutional layers capture feature information\nat varying scales. Inspired by (Chen et al. 2022), the Simpli-\nfied Channel Attention (SCA) and SimpleGate techniques\nare utilized to enhance network performance. SimpleGate\ndivides the feature maps into two channel dimensions and\nmultiplies them together, leading to a reduction in computa-\ntional load and complexity. The formula is as follows:\nSimpleGate(X, Y) =X ⊙ Y, (1)\nwhere X and Y are feature maps of the same size. Simpli-\nfied Channel Attention (SCA) can be considered as a stream-\nlined adaptation of Channel Attention (CA). It captures the\nessence of CA by retaining only two vital components: ag-\ngregating global information and enabling channel-wise in-\nteraction. SCA can be described as :\nSCA(X) = X ∗ Wpool (X). (2)\nW denotes a fully connected layer. pool represents the\nglobal average pooling procedure which combines spatial\ndata into channels.∗indicates a channel-wise multiplication.\nHierarchical Channel Attention Module\nInspired by (Wang et al. 2023), The Hierarchical Channel\nAttention Module (HCAM) module is dedicated to the hi-\nerarchical fusion of features and the acquisition of learn-\nable correlations across diverse layers. The primary func-\ntion of HCAM is to compute and apply attention weights\nto the input feature map, resulting in the refinement and\nenhancement of vignetting features. Initially, HCAM per-\nforms a transformation on the input I ∈ RH×W×3 to\nyield RN×H×W×C, N= 3, from successive layers. Subse-\nquently, the Q (query), K (key), and V (value) features are\nextracted using convolutional layers. Subsequently, the Q,\nK, and V features are extracted using convolutional layers.\nIn this module, Inner-Product Attention plays a crucial\nrole in calculating attention scores by computing the dot\nproduct between the query and the key. HCAM adaptively\nfuses features from different hierarchical levels by con-\nducting weighted operations between values and attention\nscores. The corresponding output feature Rout can be de-\nscribed as:\nRout = W1×1LA(Q, K, V) +Rin , (3)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4003\nMethods LOL MIT-Adobe FiveK\nPSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓ PSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓\nInput 7.77 0.20 99.80 0.42 12.26 0.61 55.98 0.22\nLIME 16.76 0.44 30.59 0.31 13.30 0.75 52.12 0.09\nMSRCR 13.17 0.45 52.71 0.33 13.31 0.75 50.82 0.12\nNPE 16.97 0.47 32.89 0.31 17.38 0.79 31.22 0.10\nWV-SRIE 11.86 0.49 65.56 0.24 18.63 0.84 26.27 0.08\nPM-SRIE 12.28 0.51 63.28 0.23 19.70 0.84 23.42 0.07\nJieP 12.05 0.51 64.34 0.22 18.64 0.84 26.42 0.07\nRetinexNet 16.77 0.42 32.02 0.38 12.51 0.69 52.73 0.20\nKID 17.65 0.77 31.40 0.12 16.20 0.79 35.16 0.11\nDSLR 14.98 0.60 48.90 0.27 20.24 0.83 22.45 0.10\nELGAN 17.48 0.65 34.47 0.23 16.00 0.79 36.37 0.09\nRUAS 16.40 0.50 39.11 0.19 17.91 0.84 33.12 0.08\nZero_DCE 14.86 0.56 47.07 0.24 15.93 0.77 36.36 0.12\nZero_DCE++ 14.75 0.52 45.94 0.22 14.61 0.42 39.25 0.16\nUformer 18.55 0.73 28.91 0.23 21.92 0.87 17.91 0.06\nOurs 21.33 0.76 19.30 0.16 23.10 0.84 15.43 0.16\nTable 1: Quantitative results on LOL and MIT-Adobe FiveK datasets in terms of PSNR, SSIM, LPIPS and MAE. The top two\nresults are marked in bold and underline.\nwhere LA represents Layer Attention, W1×1 indicates Con-\nvolution 1 × 1, LA can be written as:\nLA( ˆQ, ˆK, ˆV) = ˆVsoftmax( ˆQˆK/α), (4)\nLearning Criteria\nDuring the training phase, we employ two loss functions,\nnamely Mean Squared Error (L MSE ) and Structural Sim-\nilarity Index Measure (L SSIM ) (Wang et al. 2004), which\noffer significant advantages in preserving the intricate de-\ntails of the image. The mathematical expressions for these\nlosses are as follows:\nLtotal = LMSE + λ ∗ LSSIM , (5)\nThe weight of LSSIM is empirically set to 0.4.\nExperiments\nExperiment Settings\nIn order to ensure the highest level of fidelity and reliabil-\nity in our experiments, we make the deliberate decision to\nexclusively use the VigSet dataset for vignetting removal.\nMoreover, we also conduct experiments in the field of LLIE\nto demonstrate the versatility of DeVigNet in global illumi-\nnance adjustment.\nIn our experiments, we leverage the following datasets:\nVigSet: This dataset contains 983 image pairs. We allo-\ncate 803 pairs for training purposes and reserve the remain-\ning 180 pairs for testing.\nMIT-Adobe FiveK(Bychkovsky et al. 2011) & LOL-\nv1 (Wei et al. 2018): For these datasets, we adopt the exper-\nimental settings delineated in (Wang et al. 2023), ensuring\nconsistency with prior research.\nEvaluation Metrics We evaluate the quality of the\nenhanced images using four widely adopted metrics:\nPeak Signal-to-Noise Ratio (PSNR), Structural Similarity\n(SSIM), Mean Absolute Error (MAE). In addition, to bet-\nter align with human perceptual conditions, we utilize the\nLearned Perceptual Image Patch Similarity (LPIPS). These\nmetrics are commonly used in the assessment of image\nenhancement and low-level computer vision tasks. Higher\nPSNR and SSIM scores indicate improved visual quality,\nwhile lower MAE and LPIPS suggest better accuracy in rep-\nresenting the original image.\nImplementation Details We employ PyTorch to imple-\nment our model and conduct experiments using the NVIDIA\nA40 GPU. We utilize the Adam optimizer, adhering to its\ndefault parameter settings. During the training process, all\nmodels are trained at a resolution of 512 × 512 and subse-\nquently tested at different resolutions. Additionally, we set\nthe batch size to 1 and the learning rate to 1e − 4.\nComparisons with State-of-the-Art\nWe perform a comprehensive evaluation of state-of-the-art\nvignetting removal methods on the VigSet dataset. The com-\npared vignetting methods include SIVC (Zheng et al. 2008)\nand RIVC (Lopez-Fuentes, Oliver, and Massanet 2015).\nThese methods serve as benchmarks for evaluating the per-\nformance of our proposed vignetting removal technique.\nFurthermore, we also conduct additional experiments\nwith state-of-the-art LLIE methods. We compare DeVigNet\nwith traditional LLIE methods such as LIME (Guo, Li, and\nLing 2016), MSRCR (Jobson, Rahman, and Woodell 1997),\nand NPE (Wang et al. 2013). Additionally, we evaluate\nthe performance of learning-based LLIE methods, includ-\ning WV-SRIE (Fu et al. 2016), PM-SRIE (Fu et al. 2015),\nUformer (Wang et al. 2022), KID (Zhang, Zhang, and Guo\n2019), DSLR (Lim and Kim 2020), ELGAN (Jiang et al.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4004\nMethods 512 × 512 1024 × 1024 2048 × 2048\nPSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓ PSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓ PSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓\nInput 12.08 0.59 60.04 0.18 12.08 0.58 60.04 0.18 12.08 0.58 60.04 0.19\nRIVC 13.08 0.59 55.17 0.18 13.08 0.59 55.17 0.18 13.08 0.59 55.17 0.19\nSIVC 14.65 0.62 43.71 0.17 14.65 0.61 43.69 0.17 14.65 0.60 43.70 0.18\nLIME 12.42 0.41 51.29 0.41 12.42 0.39 51.29 0.40 12.41 0.37 51.32 0.42\nMSRCR 11.20 0.39 60.43 0.44 11.20 0.37 60.43 0.45 11.20 0.35 60.44 0.46\nNPE 15.72 0.51 38.60 0.33 15.72 0.49 38.59 0.32 15.72 0.47 38.60 0.33\nWV-SRIE 18.84 0.60 26.67 0.22 18.84 0.58 26.67 0.23 18.84 0.56 26.67 0.24\nPM-SRIE 19.45 0.66 24.81 0.16 19.45 0.64 24.81 0.17 19.45 0.62 24.81 0.19\nJieP 18.93 0.58 26.33 0.24 18.93 0.55 26.32 0.25 18.93 0.54 26.32 0.27\nKID 14.73 0.71 44.01 0.18 14.74 0.71 43.95 0.22 14.73 0.71 44.00 0.31\nDSLR 19.37 0.65 24.10 0.16 19.37 0.64 24.07 0.20 19.35 0.62 24.10 0.29\nELGAN 16.32 0.73 37.77 0.10 16.32 0.72 37.76 0.11 16.31 0.72 37.77 0.12\nRUAS 15.54 0.60 36.93 0.22 15.54 0.57 36.92 0.24 15.54 0.56 36.92 0.25\nZero-DCE 16.28 0.58 34.77 0.26 16.28 0.57 34.77 0.26 16.28 0.55 34.78 0.26\nZero-DCE++ 16.82 0.55 32.31 0.20 16.82 0.52 32.32 0.21 16.81 0.51 32.34 0.23\nUformer 20.95 0.77 21.32 0.19 20.60 0.77 22.80 0.25 20.67 0.77 22.69 0.28\nOurs 22.96 0.79 15.82 0.09 22.94 0.78 15.84 0.11 22.94 0.77 15.85 0.13\nTable 2: Quantitative results on VigSet in terms of PSNR, SSIM, LPIPS and MAE. The top two results are marked in bold and\nunderline.\nAblation Study 512 × 512 1024 × 1024 2048 × 2048\nPSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓ PSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓ PSNR ↑ SSIM ↑ MAE ↓ LPIPS ↓\nInput 12.08 0.59 60.04 0.18 12.08 0.58 60.04 0.18 12.08 0.58 60.04 0.19\nOurs w/ Depth=3 16.04 0.69 32.44 0.16 16.04 0.69 32.44 0.17 16.04 0.70 32.44 0.19\nOurs w/ Depth=4 13.94 0.66 42.62 0.21 13.94 0.67 42.63 0.23 13.94 0.68 42.63 0.22\nOurs w/o ACEM 19.66 0.76 23.77 0.11 19.66 0.74 23.78 0.14 19.65 0.74 23.79 0.17\nOurs w/o DAFT 18.25 0.73 28.75 0.14 18.25 0.72 28.75 0.15 18.25 0.72 28.75 0.16\nDHAN w/ ACEM 21.84 0.75 17.73 0.20 20.95 0.74 19.96 0.21 21.13 0.74 19.25 0.23\nOurs 22.96 0.79 15.82 0.09 22.94 0.78 15.84 0.11 22.94 0.77 15.85 0.13\nTable 3: Quantitative results of the ablation study on three datasets in different resolutions. The top two results are marked in\nbold and underline.\n(a) Input\n (b) ELGAN\n (c) UFormer\n (d) Zero-DCE\n(e) RIVC\n (f) SIVC\n (g) Ours\n (h) Target\nFigure 5: Visual comparison of various Vignetting Removal and LLIE methods on the VigSet dataset is presented. The figure\nclearly illustrates the presence of noticeable vignetting in methods (b), (f), and (e). Color degradation or distortion issues are\napparent in methods (c) and (d).\n2021), JieP (Cai et al. 2017), RUAS (Liu et al. 2021), Zero-\nDCE (Guo et al. 2020), and Zero-DCE++ (Li, Guo, and Loy\n2021).\nQualitative results, as shown in Figure 5 demonstrate that\nour method exhibits significant superiority over others in\nthe vignetting dataset. Our results display enhanced image\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4005\n(a) Input\n (b) ELGAN\n (c) UFormer\n (d) Zero-DCE\n(e) LIME\n (f) NPE\n (g) Ours\n (h) Target\nFigure 6: A visual comparison of different LLIE methods is conducted on the MIT-Adobe FiveK dataset. (b), (e), and (f) exhibit\noverexposure in the entire image. Both (d) and (e) exhibit either blurriness or distortion, respectively.\nclarity and more prominent vignetting removal. Moreover,\nour method achieves competitiveness on the LLIE dataset as\nshown in Figure 6.\nIn terms of quantitative results, by comparing our pro-\nposed method with these state-of-the-art vignetting and\nLLIE methods as displayed in Table 1 and Table 2, we can\nassess its effectiveness and performance in relation to ex-\nisting methods. This comprehensive evaluation provides in-\nsights into the strengths and weaknesses of different ap-\nproaches and helps to advance the field of vignetting re-\nmoval.\nConventional methods heavily depend on assumptions\nbased on physics, which are not consistently accurate. Ad-\nditionally, current LLIE methods are not well-suited for ad-\ndressing the issue of vignetting. Therefore, based on the data\nfrom Table 2, it is evident that traditional methods and LLIE\nmethods exhibit substantial disparities compared to other\nmethods in multiple metrics, particularly in terms of MAE.\nAdditionally, we conducted supplementary quantitative ex-\nperiments on a LLIE dataset, as presented in Table 1. The\nresults illustrate the strong performance of our method in\nLLIE. Besides, with the advancement of current technology,\ncapturing high-resolution images become an essential rou-\ntine. DeVigNet stands out as the most efficient approach in\nachieving superior results in vignetting removal across vari-\nous resolutions when compared to other existing methods.\nAblation Studies\nWe maintain a similar parameter count across all combina-\ntions to ensure a fair comparison. DAFT and ACEM are re-\nmoved respectively using the comment-out method in the\nexperiments. As shown in Table 3, we conduct ablation stud-\nies evaluating various components of our model at differ-\nent image resolutions. These components include varying\nthe depth of the Laplacian pyramid, as well as ablating the\nACEM and DAFT modules.\nAs the depth of the Laplacian pyramid increases, the mag-\nnitude of the low-frequency component decreases. This re-\nduction directly impacts the quality of vignetting removal,\nespecially with respect to preserving low-frequency color in-\nformation.\nDAFT plays a pivotal role in capturing global color infor-\nmation within the low-frequency component of an image.\nTherefore, models lacking DAFT suffer performance degra-\ndation due to the loss of global color context.\nMeanwhile, ACEM is primarily utilized to retain edge\ndetails during image reconstruction, focusing on the high-\nfrequency portion. Removing ACEM causes models to lose\ntextural information in the reconstruction phase, thereby\ndeteriorating performance due to the absence of high-\nfrequency context.\nAdditionally, we replace DAFT with DHAN (Cun, Pun,\nand Shi 2020), the results are shown in Table 3. Accordingly,\nDAFT and ACEM are responsible for the low-frequency and\nhigh-frequency components, respectively. While each com-\nponent contributes individually to improved vignetting re-\nmoval performance, their combined effect yields optimal re-\nsults by leveraging complementary global and local context.\nConclusion\nIn this paper, we introduce Vigset, the first large-scale high-\nresolution vignetting removal dataset with ground truth im-\nages. Vigset comprises 983 pairs of images captured under\ndifferent lighting conditions and in various scenes. Addition-\nally, we propose a novel method called DeVigNet, specif-\nically designed for vignetting removal on this dataset. It\nincludes three components: The Dual Aggregated Fusion\nTransformer, the Adaptive Channel Expansion Module and\nthe Hierarchical Channel Attention Module. By utilizing the\nLaplacian pyramid, DeVigNet performs vignetting removal\non the color information in the high-frequency and low-\nfrequency domains of the image, thereby achieving optimal\nresults. DeVigNet effectively eliminates vignetting effects in\nimages, demonstrating superior performance compared to\nexisting methods in terms of both quality and quantity for\nvignetting removal.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4006\nAcknowledgements\nThis work was supported in part by the Science and\nTechnology Development Fund, Macau SAR, under Grant\n0087/2020/A2 and Grant 0141/2023/RIA2, in part by the\nNational Natural Science Foundations of China under Grant\n62172403, in part by the Distinguished Young Scholars\nFund of Guangdong under Grant 2021B1515020019, in part\nby the Excellent Young Scholars of Shenzhen under Grant\nRCYX20200714114641211.\nReferences\nBa, J. L.; Kiros, J. R.; and Hinton, G. E. 2016. Layer nor-\nmalization. arXiv preprint arXiv:1607.06450.\nBurt, P. J.; and Adelson, E. H. 1987. The Laplacian pyramid\nas a compact image code. In Readings in computer vision,\n671–679. Elsevier.\nBychkovsky, V .; Paris, S.; Chan, E.; and Durand, F. 2011.\nLearning photographic global tonal adjustment with a\ndatabase of input/output image pairs. In CVPR, 97–104.\nCai, B.; Xu, X.; Guo, K.; Jia, K.; Hu, B.; and Tao, D. 2017.\nA joint intrinsic-extrinsic prior model for retinex. In Pro-\nceedings of the IEEE international conference on computer\nvision, 4000–4009.\nChen, L.; Chu, X.; Zhang, X.; and Sun, J. 2022. Simple\nbaselines for image restoration. In European Conference on\nComputer Vision, 17–33. Springer.\nCun, X.; Pun, C.-M.; and Shi, C. 2020. Towards ghost-free\nshadow removal via dual hierarchical aggregation network\nand shadow matting gan. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, 10680–10687.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. ICLR.\nFu, X.; Liao, Y .; Zeng, D.; Huang, Y .; Zhang, X.-P.; and\nDing, X. 2015. A probabilistic method for image enhance-\nment with simultaneous illumination and reflectance esti-\nmation. IEEE Transactions on Image Processing, 24(12):\n4965–4977.\nFu, X.; Zeng, D.; Huang, Y .; Zhang, X.-P.; and Ding, X.\n2016. A weighted variational model for simultaneous re-\nflectance and illumination estimation. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 2782–2790.\nGoldman, D. B. 2010. Vignette and exposure calibration\nand compensation. IEEE transactions on pattern analysis\nand machine intelligence, 32(12): 2276–2288.\nGuo, C.; Li, C.; Guo, J.; Loy, C. C.; Hou, J.; Kwong, S.;\nand Cong, R. 2020. Zero-reference deep curve estima-\ntion for low-light image enhancement. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 1780–1789.\nGuo, X.; Li, Y .; and Ling, H. 2016. LIME: Low-light image\nenhancement via illumination map estimation. IEEE TIP,\n26(2): 982–993.\nJiang, Y .; Gong, X.; Liu, D.; Cheng, Y .; Fang, C.; Shen, X.;\nYang, J.; Zhou, P.; and Wang, Z. 2021. Enlightengan: Deep\nlight enhancement without paired supervision. IEEE TIP,\n30: 2340–2349.\nJobson, D.; Rahman, Z.; and Woodell, G. 1997. A multiscale\nretinex for bridging the gap between color images and the\nhuman observation of scenes. IEEE Transactions on Image\nProcessing, 6(7): 965–976.\nLi, C.; Guo, C.; and Loy, C. C. 2021. Learning to enhance\nlow-light image via zero-reference deep curve estimation.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 44(8): 4225–4238.\nLi, Z.; Chen, X.; Pun, C.-M.; and Cun, X. 2023a. High-\nResolution Document Shadow Removal via A Large-Scale\nReal-World Dataset and A Frequency-Aware Shadow Eras-\ning Net. InProceedings of the IEEE/CVF International Con-\nference on Computer Vision (ICCV), 12449–12458.\nLi, Z.; Chen, X.; Wang, S.; and Pun, C.-M. 2023b. A Large-\nScale Film Style Dataset for Learning Multi-frequency\nDriven Film Enhancement. In Proceedings of the Thirty-\nSecond International Joint Conference on Artificial Intelli-\ngence, IJCAI-23, 1160–1168.\nLiang, J.; Zeng, H.; and Zhang, L. 2021. High-resolution\nphotorealistic image translation in real-time: A laplacian\npyramid translation network. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 9392–9400.\nLim, S.; and Kim, W. 2020. Dslr: Deep stacked laplacian\nrestorer for low-light image enhancement. IEEE TMM, 23:\n4272–4284.\nLiu, R.; Ma, L.; Zhang, J.; Fan, X.; and Luo, Z. 2021.\nRetinex-inspired unrolling with cooperative prior architec-\nture search for low-light image enhancement. In CVPR,\n10561–10570.\nLopez-Fuentes, L.; Oliver, G.; and Massanet, S. 2015. Re-\nvisiting image vignetting correction by constrained mini-\nmization of log-intensity entropy. In Advances in Computa-\ntional Intelligence: 13th International Work-Conference on\nArtificial Neural Networks, IWANN 2015, Palma de Mal-\nlorca, Spain, June 10-12, 2015. Proceedings, Part II 13 ,\n450–463. Springer.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net:\nConvolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, 234–241. Springer.\nWang, S.; Zheng, J.; Hu, H.-M.; and Li, B. 2013. Natu-\nralness Preserved Enhancement Algorithm for Non-Uniform\nIllumination Images. IEEE Transactions on Image Process-\ning, 22(9): 3538–3548.\nWang, T.; Zhang, K.; Shen, T.; Luo, W.; Stenger, B.; and\nLu, T. 2023. Ultra-high-definition low-light image enhance-\nment: a benchmark and transformer-based method. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence ,\nvolume 37, 2654–2662.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4007\nWang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P.\n2004. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image process-\ning, 13(4): 600–612.\nWang, Z.; Cun, X.; Bao, J.; and Liu, J. 2022. Uformer:\nA general u-shaped transformer for image restoration. In\nCVPR, 17683–17693.\nWei, C.; Wang, W.; Yang, W.; and Liu, J. 2018. Deep retinex\ndecomposition for low-light enhancement. In BMVC.\nYou, S.; Lei, B.; Wang, S.; Chui, C. K.; Cheung, A. C.; Liu,\nY .; Gan, M.; Wu, G.; and Shen, Y . 2022. Fine perceptive\ngans for brain mr image super-resolution in wavelet domain.\nIEEE transactions on neural networks and learning systems.\nZhang, Y .; Zhang, J.; and Guo, X. 2019. Kindling the dark-\nness: A practical low-light image enhancer. In ACMMM,\n1632–1640.\nZheng, Y .; Lin, S.; Kang, S. B.; Xiao, R.; Gee, J. C.; and\nKambhamettu, C. 2013. Single-Image Vignetting Correc-\ntion from Gradient Distribution Symmetries. IEEE Trans\nPattern Anal Mach Intell, 35(6): 1480–1494.\nZheng, Y .; Yu, J.; Kang, S. B.; Lin, S.; and Kambhamettu, C.\n2008. Single-image vignetting correction using radial gra-\ndient symmetry. In 2008 IEEE Conference on Computer\nVision and Pattern Recognition, 1–8. IEEE.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4008",
  "topic": "Vignetting",
  "concepts": [
    {
      "name": "Vignetting",
      "score": 0.9360730648040771
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.6014097929000854
    },
    {
      "name": "Fusion",
      "score": 0.4761386215686798
    },
    {
      "name": "Transformer",
      "score": 0.4660545289516449
    },
    {
      "name": "Computer science",
      "score": 0.42104005813598633
    },
    {
      "name": "Physics",
      "score": 0.12108564376831055
    },
    {
      "name": "Engineering",
      "score": 0.11028677225112915
    },
    {
      "name": "Voltage",
      "score": 0.10490244626998901
    },
    {
      "name": "Electrical engineering",
      "score": 0.09036615490913391
    },
    {
      "name": "Optics",
      "score": 0.08907830715179443
    },
    {
      "name": "Art",
      "score": 0.0594085156917572
    },
    {
      "name": "Lens (geology)",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204512498",
      "name": "University of Macau",
      "country": "MO"
    },
    {
      "id": "https://openalex.org/I93477617",
      "name": "Huizhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145761",
      "name": "Shenzhen Institutes of Advanced Technology",
      "country": "CN"
    }
  ],
  "cited_by": 37
}