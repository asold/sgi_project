{
  "title": "Empower Entity Set Expansion via Language Model Probing",
  "url": "https://openalex.org/W3035458998",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2124706810",
      "name": "Yun-Yi Zhang",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2465159399",
      "name": "Jiaming Shen",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2223914299",
      "name": "Jingbo Shang",
      "affiliations": [
        "University of Illinois Urbana-Champaign",
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2103606203",
      "name": "Jiawei Han",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3134710324",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2250727976",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2284650288",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2117510361",
    "https://openalex.org/W2295058825",
    "https://openalex.org/W2009510620",
    "https://openalex.org/W3102679845",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2103296194",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2809189384",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2969539057",
    "https://openalex.org/W2404527150",
    "https://openalex.org/W2068737686",
    "https://openalex.org/W2949765915",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3103593722",
    "https://openalex.org/W2951113467",
    "https://openalex.org/W2980384207",
    "https://openalex.org/W2104987630",
    "https://openalex.org/W2593560537",
    "https://openalex.org/W2604165577",
    "https://openalex.org/W2777203405",
    "https://openalex.org/W2188138540",
    "https://openalex.org/W2962762512",
    "https://openalex.org/W2970427399",
    "https://openalex.org/W1975497117"
  ],
  "abstract": "Entity set expansion, aiming at expanding a small seed entity set with new entities belonging to the same semantic class, is a critical task that benefits many downstream NLP and IR applications, such as question answering, query understanding, and taxonomy construction. Existing set expansion methods bootstrap the seed entity set by adaptively selecting context features and extracting new entities. A key challenge for entity set expansion is to avoid selecting ambiguous context features which will shift the class semantics and lead to accumulative errors in later iterations. In this study, we propose a novel iterative set expansion framework that leverages automatically generated class names to address the semantic drift issue. In each iteration, we select one positive and several negative class names by probing a pre-trained language model, and further score each candidate entity based on selected class names. Experiments on two datasets show that our framework generates high-quality class names and outperforms previous state-of-the-art methods significantly.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8151–8160\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n8151\nEmpower Entity Set Expansion via Language Model Probing\nYunyi Zhang1, Jiaming Shen1, Jingbo Shang2, Jiawei Han1\n1 University of Illinois at Urbana-Champaign, IL, USA\n2 University of California San Diego, CA, USA\n1{yzhan238, js2, hanj}@illinois.edu 2 jshang@ucsd.edu\nAbstract\nEntity set expansion, aiming at expanding a\nsmall seed entity set with new entities belong-\ning to the same semantic class, is a critical\ntask that beneﬁts many downstream NLP and\nIR applications, such as question answering,\nquery understanding, and taxonomy construc-\ntion. Existing set expansion methods boot-\nstrap the seed entity set by adaptively select-\ning context features and extracting new enti-\nties. A key challenge for entity set expansion\nis to avoid selecting ambiguous context fea-\ntures which will shift the class semantics and\nlead to accumulative errors in later iterations.\nIn this study, we propose a novel iterative set\nexpansion framework that leverages automati-\ncally generated class names to address the se-\nmantic drift issue. In each iteration, we select\none positive and several negative class names\nby probing a pre-trained language model, and\nfurther score each candidate entity based on\nselected class names. Experiments on two\ndatasets show that our framework generates\nhigh-quality class names and outperforms pre-\nvious state-of-the-art methods signiﬁcantly.\n1 Introduction\nEntity set expansion aims to expand a small set\nof seed entities (e.g., {“ United States”, “China”,\n“Canada”}) with new entities (e.g., “United King-\ndom”, “Australia”) belonging to the same semantic\nclass (i.e., Country). The entities so discovered\nmay beneﬁt a variety of NLP and IR applications,\nsuch as question answering (Wang et al., 2008),\nquery understanding (Hua et al., 2017), taxonomy\nconstruction (Shen et al., 2018a), and semantic\nsearch (Xiong et al., 2017; Shen et al., 2018b).\nMost existing entity set expansion methods boot-\nstrap the initial seed set by iteratively selecting\ncontext features (e.g., co-occurrence words (Pantel\net al., 2009), unary patterns (Rong et al., 2016),\nand coordinational patterns (Mamou et al., 2018)),\nEntities Hearst Pattern\n[NP0] such as [NP1], [NP2], and [NP3]{USA, China, Canada}\n[MASK] such as USA, China, and Canada\nClass-probing Query\nClass Name Entity Hearst Pattern\n[NP0], [NP1], or other [NP2]countries Canada\nEntity-probing Query\nCanada, [MASK], or other countries\nRetrieved Class Names\ncountries\nstates\nlarge countries\n…\ncities\n…\nRetrieved Entities\nJapan\nUnited Kingdom\nMexico\n…\nToronto\n…\nLanguage Model\n(e.g. BERT/XLNet)\nFigure 1: Examples of class-probing and entity-\nprobing queries generated based on Hearst patterns.\nwhile extracting and ranking new entities. A key\nchallenge to set expansion is to avoid selecting am-\nbiguous patterns that may introduce erroneous en-\ntities from other non-target semantic classes. Take\nthe above class Country as an example, we may\nﬁnd some ambiguous patterns like “* located at”\n(which will match more general Location enti-\nties) and “match against *” (which may be asso-\nciated with entities in the Sports Club class).\nFurthermore, as bootstrapping is an iterative pro-\ncess, those erroneous entities added at early iter-\nations may shift the class semantics, leading to\ninferior expansion quality at later iterations. Ad-\ndressing such “semantic drift” issue without requir-\ning additional user inputs (e.g., mutually exclusive\nclasses (Curran et al., 2007) and negative example\nentities (Jindal and Roth, 2011)) remains an open\nresearch problem.\nIn this study, we propose to empower entity\nset expansion with class names automatically gen-\nerated from pre-trained language models (Peters\net al., 2018; Devlin et al., 2019; Yang et al., 2019).\nIntuitively, knowing the class name is “country”,\ninstead of “state” or “city”, can help us identify\nunambiguous patterns and eliminate erroneous en-\ntities like “Europe” and “New York”. Moreover, we\ncan acquire such knowledge (i.e., positive and nega-\ntive class names) by probing a pre-trained language\n8152\nmodel automatically without relying on human an-\nnotated data.\nMotivated by the above intuition, we propose a\nnew iterative framework for entity set expansion\nthat consists of three modules: (1) The ﬁrst, class\nname generation module, constructs and submits\nclass-probing queries (e.g., “[MASK ] such as USA,\nChina, and Canada.” in Fig. 1) to a language model\nfor retrieving a set of candidate class names. (2)\nThe second, class name ranking module, builds\nan entity-probing query for each candidate class\nname and retrieves a set of entities. The similarity\nbetween this retrieved set and the current entity\nset serves as a proxy for the class name quality,\nbased on which we rank all candidate class names.\nAn unsupervised ensemble technique (Shen et al.,\n2017) is further used to improve the quality of ﬁ-\nnal ranked list from which we select one best class\nname and several negative class names. (3) The\nthird, class-guided entity selection module, scores\neach entity conditioned on the above selected class\nnames and adds top-ranked entities into the cur-\nrently expanded set. As better class names may\nemerge in later iterations, we score and rank all en-\ntities (including those already in the expanded set)\nat each iteration, which helps alleviate the semantic\ndrift issue.\nContributions. In summary, this study makes the\nfollowing contributions: (1) We propose a new set\nexpansion framework that leverages class names\nto guide the expansion process and enables ﬁltra-\ntion of the entire set in each iteration to resolve the\nsemantic drift issue; (2) we design an automatic\nclass name generation algorithm that outputs high-\nquality class names by dynamically probing pre-\ntrained language models; and (3) experiments on\ntwo public datasets from different domains demon-\nstrate the superior performance of our approach\ncompared with state-of-the-art methods.\n2 Background\nIn this section, we provide background on language\nmodels and deﬁne the entity set expansion problem.\n2.1 Language Model\nA standard language model (LM) inputs a word se-\nquence w = [w1, w2, . . . , wn] and assigns a prob-\nability P(w) to the whole sequence. Recent stud-\nies (Peters et al., 2018; Devlin et al., 2019; Yang\net al., 2019) found that language models, simply\ntrained for next word or missing word prediction,\ncan generate high quality contextualized word rep-\nresentations which beneﬁt many downstream appli-\ncations. Speciﬁcally, these language models will\noutput an embedding vector for each word appear-\nance in a speciﬁc context that is usually the en-\ntire sentence where the target word occurs, rather\nthan just words appearing before the target word.\nTherefore, we can also view a LM as a model that\ninputs a word sequence w and outputs a probabil-\nity P(wi) = P(wi|w1, . . . , wi−1, wi+1, . . . , wn)\nto any position 1 ≤ i ≤ n. Currently, Devlin\net al. (2019) propose BERT and train the language\nmodel with two objectives: (1) a cloze-ﬁlling ob-\njective which randomly substitutes some words\nwith a special [MASK ] token in the input sentence\nand forces LM to recover masked words, and (2)\na binary classiﬁcation objective that guides LM\nto predict whether one sentence directly follows\nanother (sentence). BERT leverages Transformer\n(Vaswani et al., 2017) architecture and is learned on\nEnglish Wikipedia as well as BookCorpus. More\nLM architectures are described in Section 5.\n2.2 Problem Formulation\nWe ﬁrst deﬁne some key concepts and then present\nour problem formulation.\nEntity. An entity is a word or a phrase that refers\nto a real-world instance. For example, “U.S.” refers\nto the country: United States.\nClass Name.A class name is a text representation\nof a semantic class. For instance, country could\nbe a class name for the semantic class that includes\nentities like “United States” and “China”.\nProbing Query. A probing query is a word se-\nquence containing one [MASK ] token. In this work,\nwe utilize Hearst patterns (Hearst, 1992) to con-\nstruct two types of probing queries: (1) A class-\nprobing query aims to predict the class name of\nsome given entities (e.g., “[MASK ] such as United\nStates and China”), and (2) an entity-probing query\naims to retrieve entities that ﬁt into the mask token\n(e.g., “countries such as [MASK ] and Japan”).\nProblem Formulation.Given a text corpus Dand\na seed set of user-provided entities, we aim to out-\nput a ranked list of entities that belong to the same\nsemantic class.\nExample 1. Given a seed set of three countries\n{“United States”, “China”, “Canada”}, we aim\nto return a ranked list of entities belonging to the\nsame country class such as “United Kingdom”,\n“Japan”, and “Mexico”.\n8153\n3 Class-Guided Entity Set Expansion\nWe introduce our class-guided entity set expansion\nframework in this section. First, we present our\nclass name generation and ranking modules in Sec-\ntions 3.1 and 3.2, respectively. Then, we discuss\nhow to leverage class names to guide the iterative\nexpansion process in Section 3.3.\n3.1 Class Name Generation\nThe class name generation module inputs a small\ncollection of entities and generates a set of can-\ndidate class names for these entities. We build\nthis module by automatically constructing class-\nprobing queries and iteratively querying a pre-\ntrained LM to obtain multi-gram class names.\nFirst, we notice that the class name genera-\ntion goal is similar to the hypernymy detection\ntask which aims to ﬁnd a general hypernym (e.g.,\n“mammal”) for a given speciﬁc hyponym (e.g.,\n“panda”). Therefore, we leverage the six Hearst pat-\nterns (Hearst, 1992)1, widely used for hypernymy\ndetection, to construct the class-probing query.\nMore speciﬁcally, we randomly select three en-\ntities in the current set as well as one Hearst pattern\n(out of six choices) to construct one query. For ex-\nample, we may choose entities {“China”, “India”,\n“Japan”} and pattern “ NPy such as NPa, NPb,\nand NPc” to construct the query “[ MASK ] such\nas China, India, and Japan”. By repeating such a\nrandom selection process, we can construct a set\nof queries and feed them into pre-trained language\nmodels to obtain predicted masked tokens which\nare viewed as possible class names.\nThe above procedure has one limitation—it can\nonly generate unigram class names. To obtain\nmulti-gram class names, we design a modiﬁed\nbeam search algorithm to iteratively query a pre-\ntrained LM. Speciﬁcally, after we query a LM for\nthe ﬁrst time and retrieve top K most likely words\n(for the masked token), we constructK new queries\nby adding each retrieved word after the masked\ntoken. Taking the former query “[ MASK ] such\nas China, India, and Japan” as an example, we\nmay ﬁrst obtain words like “countries”, “nations”,\nand then construct a new query “[MASK ] countries\nsuch as China, India, and Japan”. Probing the LM\nagain with this new query, we can get words like\n“Asian” or “large”, and obtain more ﬁne-grained\nclass names like “Asian countries” or “large coun-\n1For example, the pattern “NPy such as NPa” indicates\nthat noun phrase y is a hypernym of noun phrase a.\ntries”. We repeat this process for maximum three\ntimes and keep all generated class names that are\nnoun phrases2. As a result, for each Hearst pat-\ntern and randomly selected three entities from the\ncurrent set, we will obtain a set of candidate class\nnames. Finally, we use the union of all these sets as\nour candidate class name pool, denoted as C. Note\nthat in this module, we focus on the recall of can-\ndidate class name pool C, without considering its\nprecision, since the next module will further rank\nand select these class names based on the provided\ntext corpus.\n3.2 Class Name Ranking\nIn this module, we rank the above generated candi-\ndate class names to select one best class name that\nrepresents the whole entity set and some negative\nclass names used in the next module to ﬁlter out\nwrong entities. A simple strategy is to rank these\nclass names based on the number of times it has\nbeen generated in the previous module. However,\nsuch a strategy is sub-optimal because short uni-\ngram class names always appear more frequently\nthan longer multi-gram class names. Therefore, we\npropose a new method below to measure how well\neach candidate class name represents the entity set.\nFirst, we introduce a corpus-based similarity\nmeasure between an entity e and a class name c.\nGiven the class name c, we ﬁrst construct 6 entity-\nprobing queries by masking the hyponym term in\nsix Hearst patterns3, and query a pre-trained LM\nto obtain the set of six [MASK ] token embeddings,\ndenoted as Xc. Moreover, we use Xe to denote\nthe set of all contextualized representations of the\nentity e in the given corpus. Then, we deﬁne the\nsimilarity between e and c, as:\nMk(e, c) = 1\nk max\nX⊆Xe,|X|=k\n∑\nx∈X\nmax\nx′∈Xc\ncos(x, x′), (1)\nwhere cos(x, x′) is the cosine similarity between\ntwo vectors x and x′. The inner max operator\nﬁnds the maximum similarity between each occur-\nrence of e and the set of entity-probing queries\nconstructed based on c. The outer max operator\nidentiﬁes the top-k most similar occurrences of e\nwith the queries and then we take their average as\nthe ﬁnal similarity between the entity e and the\nclass name c. This measure is analogous to ﬁnding\n2Therefore, class names likes “and countries” and “, coun-\ntries” are ﬁltered out.\n3For example, a query for class name “countries” is\n“countries such as [MASK ]”.\n8154\nChina\nIndia\nCanada\nUnited States\nKorea\nCanada\nUnited States\nChina\nKorea\nChina\nIndia\nUnited States\nCanada\nIndia\n[MASK] such as \nUnited States, \nChina, and Canada\n[MASK] such as \nChina, India, and \nKorea\n[MASK] such as \nCanada, India, and \nUnited States\n…\nEurope\nUnited Kingdom\nJapan\n, countries\n…\ncountries\nstates\nlarge countries\nand states\ncountries\nnations\nAsian countries\ndeveloping \ncountries\ncountries\nstates\n…\n…\n…\n, nations\nlarge countries\ncommonwealth  \ncountries\n…\n, states\nCandidate \nClass Names\n……\ncommonwealth \ncountries\ndeveloping \ncountries\nnations\nAsian countries\nstates\ncountries\nlarge countries\n(1) Class Name Generation Module\nUnited \nStates\n… …\n0.765\n0.825\n…\n0.819\n0.728states\ncities\n…\ncountries\nlarge countries\nChina\n… …\n0.760\n0.861\n…\n0.848\n0.753states\nterritories\n…\nAsian countries\ncountries\n…\nRank list L 1\nCurrent Set E\nL | E |Rank list\n…\n(2) Class Name Ranking Module\nPositive Class \nName:\nNegative  \nClass Names:\n……\nterritories\nstates\ncities\ncountries\ncountries such as [MASK] \n{China, United States, Canada}\n+\ncountries, including [MASK] \n{China, India, Korea}\n+\n[MASK] and other countries  \n{United States, India, Korea}\n+\nQueries and sampled \nentities subsets\n…\n…\nJapan\nThailand\nSingapore\n…\nMalaysia\nSingapore\nEurope\nFilter\nFilter\nFilter\n…\nUnited Kingdom\nJapan\n…\nJapan\nThailand\nSingapore\n…\nMalaysia\nSingapore\n(3) Class-Guide Entity Selection Module\n……\nUnited Kingdom\nJapan\nSingapore\n…\n…\nSelected \nNew Entities\n…\n…\nFigure 2: Overview of one iteration in CGExpan framework.\nk best occurrences of entity e that matches to any\nof the probing queries of class c, and therefore it\nimproves the previous similarity measures that uti-\nlize only the context-free representations of entities\nand class names (e.g., Word2Vec).\nAfter we deﬁne the entity-class similarity score,\nwe can choose one entity in the current set and\nobtain a ranked list of candidate class names based\non their similarities with this chosen entity. Then,\ngiven an entity setE, we can obtain|E|ranked lists,\nL1, L2, . . . , L|E|, one for each entity in E. Finally,\nwe follow (Shen et al., 2017) and aggregate all\nthese lists to a ﬁnal ranked list of class names based\non the score s(c) = ∑|E|\ni=1\n1\nric\n, where ri\nc indicates\nthe rank position of class name c in ranked list Li.\nThis ﬁnal ranked list shows the order of how well\neach class name can represent the current entity set.\nTherefore, we choose the best one that ranks in the\nﬁrst position as the positive class , denoted as cp.\nAside from choosing the positive class name cp,\nwe also select a set of negative class names for the\ntarget semantic class to help bound its semantics.\nTo achieve this goal, we assume that entities in the\ninitial user-provided seed set E0 deﬁnitely belong\nto the target class. Then, we choose those class\nnames that rank lower than cp in all lists corre-\nsponding to entities in E0, namely {Li|ei ∈E0},\nand treat them as the negative class names. We\nrefer to this negative set of class names as CN and\nuse them to guide the set expansion process below.\n3.3 Class-Guided Entity Selection\nIn this module, we leverage the above selected\npositive and negative class names to help select\nnew entities to add to the set. We ﬁrst introduce\ntwo entity scoring functions and then present a new\nrank ensemble algorithm for entity selection.\nThe ﬁrst function utilizes the positive class name\ncp and calculates each entity ei’s score :\nscoreloc\ni = Mk(ei, cp), (2)\nwhere Mk is deﬁned in Eq. (1). We refer to this\nscore as a local score because it only looks at top-k\nbest occurrences in the corpus where the contextu-\nalized representation of entity ei is most similar to\nthe representation of class name cq.\nThe second scoring function calculates the sim-\nilarity between each candidate entity and existing\nentities in the current set, based on their context-\nfree representations. For each entity e, we use the\naverage of all its contextualized embedding vectors\nas its context-free representation, denoted as ve.\nGiven the current entity set E, we ﬁrst sample sev-\neral entities from E, denoted as Es, and calculate\nthe score for each candidate entity ei as:\nscoreglb\ni = 1\n|Es|\n∑\ne∈Es\ncos(vei , ve). (3)\nNote here we sample a small set Es (typically of\nsize 3), rather than using the entire set E. Since\nthe current entity set E may contain wrong entities\nintroduced in previous steps, we do not use all the\nentities in E and compute the candidate entity score\nonly once. Instead, we randomly select multiple\nsubsets of entities from the current set E, namely\nEs, obtain a ranked list of candidate entities for\neach sampled subset, and aggregate all ranked lists\nto select the ﬁnal entities. Such a sampling strategy\ncan reduce the effect of using wrong entities in E,\nas they are unlikely to be sampled multiple times,\nand thus can alleviate potential errors that are intro-\nduced in previous iterations. We refer to this score\nas a global score because it utilizes context-free\nrepresentations which better reﬂect entities’ over-\nall positions in the embedding space and measure\nthe entity-entity similarity in a more global sense.\nSuch a global score complements the above local\nscore and we use their geometric mean to ﬁnally\nrank all candidate entities:\nscorei =\n√\nscoreloc\ni ×scoreglb\ni . (4)\nAs the expansion process iterates, wrong entities\n8155\nmay be included in the set and cause semantic drift-\ning. We develop a novel rank ensemble algorithm\nthat leverages those selected class names to im-\nprove the quality and robustness of entity selection.\nFirst, we repeatedly sample Es (used for calculat-\ning scoreglb\ni in Eq. (3)) T times from current entity\nset E, and obtain T entity ranked lists {Rm}T\nm=1.\nSecond, we follow the class name ranking proce-\ndure in Section 3.2 to obtain |E|class ranked lists\n{Ln}|E|\nn=1, one for each entity ei ∈E. Note here\neach Ln is actually a ranked list over {cp}∪CN ,\nnamely the set of selected one positive class name\nand all negative class names. Intuitively, an entity\nbelonging to our target semantic class should sat-\nisfy two criteria: (1) it appears at the top positions\nin multiple entity ranked lists, and (2) within its cor-\nresponding class ranked list, the selected best class\nname cp should be ranked above any one of the\nnegative class name in CN . Combining these two\ncriteria, we deﬁne a new rank aggregation score as\nfollows:\nS(ei) =\nT∑\nt=1\n(\n1 (ei ∈E) +st(ei)\n)\n×1 (ri\ncp < min\nc′∈CN\nri\nc′), (5)\nwhere 1 (·) is an indicator function, ri\nc is the rank\nof class name c in entity ei’s ranked list Li\nc, and\nst(ei) the individual aggregation score of ei de-\nduced from the ranked list Rt, for which we test\ntwo aggregation methods: (1) mean reciprocal rank,\nwhere\nst(ei) = 1\nrt\ni\n(6)\nand rt\ni is the rank of entity ei in the t-th ranked list\nRt; and (2) the combination of scores (CombSUM),\nwhere\nst(ei) =\nscoret\ni −minej∈Rt scoret\nj\nmaxej∈Rt scoret\nj −minej∈Rt scoret\nj\n(7)\nis the ranking score of ei in the ranked list Rt after\nmin-max feature scaling.\nTo interpret Eq. 5, the ﬁrst summation term re-\nﬂects our criterion (1) and its inner indicator func-\ntion ensuring an entity in the current set E prone\nto have a large rank aggregation score if not been\nﬁltered out below. The second term reﬂects our cri-\nterion (2) by using an indicator function that ﬁlters\nout all entities which are more similar to a negative\nclass name than the positive class name. Note here\nwe calculate the aggregation score for all entities in\nDataset # Test Queries # Entities # Sentences\nWiki 40 33K 1.50M\nAPR 15 76K 1.01M\nTable 1: Datasets statistics\nthe vocabulary list, including those already in the\ncurrent set E, and it is possible that some entity in\nE will be ﬁltered out because it has 0 value in the\nsecond term. This makes a huge difference compar-\ning with previous iterative set expansion algorithms\nwhich all assume that once an entity is included in\nthe set, it will stay in the set forever. Consequently,\nour method is more robust to the semantic drifting\nissue than previous studies.\nSummary. Starting with a small seed entity set, we\niteratively apply the above three modules to obtain\nan entity ranked list and add top-ranked entities into\nthe set. We repeat the whole process until either (1)\nthe expanded set reaches a pre-deﬁned target size\nor (2) the size of the set does not increase for three\nconsecutive iterations. Notice that, by setting a\nlarge target size, more true entities belonging to the\ntarget semantic class will be selected to expand the\nset, which increases the recall, but wrong entities\nare also more likely to be included, which decreases\nthe precision. However, as the output of the set\nexpansion framework is a ranked list, the most\nconﬁdent high-quality entities will still be ranked\nhigh in the list.\n4 Experiments\n4.1 Experiment Setup\nDatasets. We conduct our experiments on two\npublic benchmark datasets widely used in previous\nstudies (Shen et al., 2017; Yan et al., 2019): (1)\nWiki, which is a subset of English Wikipedia arti-\ncles, and (2) APR, which contains all news articles\npublished by Associated Press and Reuters in 2015.\nFollowing the previous work, we adopt a phrase\nmining tool, AutoPhrase (Shang et al., 2018), to\nconstruct the entity vocabulary list from the corpus,\nand select the same 8 semantic classes for the Wiki\ndataset as well as 3 semantic classes for the APR\ndataset. Each semantic class has 5 seed sets and\neach seed set contains 3 entities. Table 1 summa-\nrizes the statistics for these datasets.\nCompared methods. We compare the following\ncorpus-based entity set expansion methods.\n1. Egoset (Rong et al., 2016): This is a multi-\nfaceted set expansion system using context fea-\ntures and Word2Vec embeddings. The original\n8156\nMethods Wiki APR\nMAP@10 MAP@20 MAP@50 MAP@10 MAP@20 MAP@50\nEgoset (Rong et al., 2016) 0.904 0.877 0.745 0.758 0.710 0.570\nSetExpan (Shen et al., 2017) 0.944 0.921 0.720 0.789 0.763 0.639\nSetExpander (Mamou et al., 2018) 0.499 0.439 0.321 0.287 0.208 0.120\nCaSE (Yu et al., 2019b) 0.897 0.806 0.588 0.619 0.494 0.330\nMCTS (Yan et al., 2019) 0.980 ∇ 0.930∇ 0.790∇ 0.960∇ 0.900∇ 0.810∇\nCGExpan-NoCN 0.968 0.945 0.859 0.909 0.902 0.787\nCGExpan-NoFilter 0.990 0.975 0.890 0.979 0.962 0.892\nCGExpan-Comb 0.991 0.974 0.895 0.983 0.984 0.937\nCGExpan-MRR 0.995 0.978 0.902 0.992 0.990 0.955\nTable 2: Mean Average Precision on Wiki and APR. “∇” means the number is directly from the original paper.\nframework aims to expand the set in multiple\nfacets. Here we treat all expanded entities as in\none semantic class due to little ambiguity in the\nseed set.\n2. SetExpan (Shen et al., 2017): This method iter-\natively selects skip-gram context features from\nthe corpus and develops a rank ensemble mech-\nanism to score and select entities.\n3. SetExpander (Mamou et al., 2018): This method\ntrains different embeddings based on different\ntypes of context features and leverages addi-\ntional human-annotated sets to build a classiﬁer\non top of learned embeddings to predict whether\nan entity belongs to the set.\n4. CaSE (Yu et al., 2019b): This method combines\nentity skip-gram context feature and embedding\nfeatures to score and rank entities once from the\ncorpus. The original paper has three variants\nand we use the CaSE-W2V variant since it is\nthe best model claimed in the paper.\n5. MCTS (Yan et al., 2019): This method boot-\nstraps the initial seed set by combing the Monte\nCarlo Tree Search algorithm with a deep simi-\nlarity network to estimate delayed feedback for\npattern evaluation and to score entities given\nselected patterns.\n6. CGExpan : This method is our proposed\nClass-Guided Set Expansion framework, using\nBERT (Devlin et al., 2019) as the pre-trained\nlanguage model. We include two versions of\nour full model, namely CGExpan-Comb and\nCGExpan-MRR, that use the combination of\nscore and mean reciprocal rank for rank aggre-\ngation, respectively.\n7. CGExpan-NoCN : An ablation of CGExpan that\nexcludes the class name guidance. Therefore, it\nonly incorporates the average BERT representa-\ntion to select entities.\n8. CGExpan-NoFilter : An ablation of CGExpan\nCGExpan vs. Other MAP@10 MAP@20 MAP@50\nvs. SetExpan 100% 94.5% 87.3%\nvs. CGExpan-NoFilter 100% 94.5% 58.2%\nvs. CGExpan-NoCN 100% 94.5% 70.9%\nTable 3: Ratio of seed entity set queries on which the\nﬁrst method reaches better or the same performance as\nthe second method.\nthat excludes the negative class name selection\nstep and uses only the single positive class name\nin the entity selection module.\nEvaluation Metric. We follow previous studies\nand evaluate set expansion results using Mean\nAverage Precision at different top K positions\n(MAP@K) as below:\nMAP@K = 1\n|Q|\n∑\nq∈Q\nAPK(Lq, Sq),\nwhere Q is the set of all seed queries and for each\nquery q, we use APK(Lq, Sq) to denote the tra-\nditional average precision at position K given a\nranked list of entities Lq and a ground-truth set Sq.\nImplementation Details. For CGExpan, we use\nBERT-base-uncased4 as our pre-trained LM. For\nparameter setting, in the class name generation\nmodule (Sec. 3.1), we take top-3 predicted tokens\nin each level of beam search and set the maximum\nlength of generated class names up to 3. When\ncalculating the similarity between an entity and a\nclass name (Eq. 1), we choose k = 5, and will later\nprovide a parameter study on k in the experiment.\nAlso, since MAP@K for K = 10, 20, 50 are typi-\ncally used for set expansion evaluations, we follow\nthe convention and choose 50 as the target set size\nin our experiments.5\n4In principle, other masked LMs such as RoBERTa and\nXLNet can also be used in our framework.\n5The code and data are available at https://github.\ncom/yzhan238/CGExpan\n8157\nMethods Wiki APR\nMAP@{10/20/50} MAP@{10/20/50}\nOracle-Full 0.991/0.976/0.891 1.000/1.000/0.964\nOracle-NoFilter 0.994/0.983/0.887 0.988/0.966/0.894\nCGExpan 0.995/0.978/0.902 0.992/0.990/0.955\nTable 4: Compared to oracle models knowing ground\ntruth class names, CGExpan automatically generates\nclass names and achieves comparative performances.\n4.2 Experiment Results\nOverall Performance. Table 2 shows the over-\nall performance of different entity set expansion\nmethods. We can see that CGExpan along with\nits ablations in general outperform all the base-\nlines by a large margin. Comparing with SetExpan,\nthe full model CGExpan achieves 24% improve-\nment in MAP@50 on the Wiki dataset and 49%\nimprovement in MAP@50 on the APR dataset,\nwhich veriﬁes that our class-guided model can re-\nﬁne the expansion process and reduce the effect\nof erroneous entities on later iterations. In addi-\ntion, CGExpan-NoCN outperforms most baseline\nmodels, meaning that the pre-trained LM itself\nis powerful to capture entity similarities. How-\never, it still cannot beat CGExpan-NoFilter model,\nwhich shows that we can properly guide the set\nexpansion process by incorporating generated class\nnames. Moreover, by comparing our full model\nwith CGExpan-NoFilter, we can see that negative\nclass names indeed help the expansion process by\nestimating a clear boundary for the target class and\nﬁltering out erroneous entities. Such an improve-\nment is particularly obvious on the APR dataset.\nThe two versions of our full model overall have\ncomparable performance, but CGExpan-MRR con-\nsistently outperforms CGExpan-Comb. To explain\nsuch a difference, empirically we observe that high-\nquality entities tend to rank high in most of the\nranked lists. Therefore, we use the MRR ver-\nsion for the rest of our experiment, denoted as\nCGExpan.\nFine-grained Performance Analysis. Table 3\nreports more ﬁne-grained comparison results be-\ntween two methods. Speciﬁcally, we calculate\nthe ratio of seed entity set queries (out of total\n55 queries) on which one method achieves better\nor the same performance as the other method. We\ncan see that CGExpan clearly outperforms SetEx-\npan and its two variants on the majority of queries.\nIn Table 4, we further compare CGExpan with\ntwo “oracle” models that have the access to ground\ntruth class names. Results show that CGExpan can\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c\nk\n/uni00000013/uni00000011/uni0000001b/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni0000001b\n/uni00000013/uni00000011/uni0000001c/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000015\n/uni00000013/uni00000011/uni0000001c/uni00000017\n/uni00000013/uni00000011/uni0000001c/uni00000019\n/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000013/uni00000030/uni00000024/uni00000033/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000056\n/uni00000030/uni00000024/uni00000033/uni00000023/uni00000014/uni00000013\n/uni00000030/uni00000024/uni00000033/uni00000023/uni00000015/uni00000013\n/uni00000030/uni00000024/uni00000033/uni00000023/uni00000018/uni00000013\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a/uni0000001b/uni0000001c\nk\n/uni00000013/uni00000011/uni0000001b/uni00000019\n/uni00000013/uni00000011/uni0000001b/uni0000001b\n/uni00000013/uni00000011/uni0000001c/uni00000013\n/uni00000013/uni00000011/uni0000001c/uni00000015\n/uni00000013/uni00000011/uni0000001c/uni00000017\n/uni00000013/uni00000011/uni0000001c/uni00000019\n/uni00000013/uni00000011/uni0000001c/uni0000001b\n/uni00000014/uni00000011/uni00000013/uni00000013/uni00000030/uni00000024/uni00000033/uni00000003/uni00000056/uni00000046/uni00000052/uni00000055/uni00000048/uni00000056\n/uni00000030/uni00000024/uni00000033/uni00000023/uni00000014/uni00000013\n/uni00000030/uni00000024/uni00000033/uni00000023/uni00000015/uni00000013\n/uni00000030/uni00000024/uni00000033/uni00000023/uni00000018/uni00000013\nFigure 3: Performance for different k values on Wiki\n(left) and APR (right).\nachieve comparative results as those oracle models,\nwhich indicates the high quality of generated class\nnames and effectiveness of CGExpan.\nParameter Study.In CGExpan, we calculate the\nsimilarity between an entity and a class name based\non its k occurrences that are most similar to the\nclass name (cf. Eq. (1)). Figure 3 studies how this\nparameter k would affect the overall performance.\nWe ﬁnd that the model performance ﬁrst increases\nwhen k increases from 1 to 5 and then becomes\nstable (in terms of MAP@10 and MAP@20) when\nk further increases to 10. Overall, we ﬁnd k = 5is\nenough for calculating entity-class similarity and\nCGExpan is insensitive to k as long as its value is\nlarger than 5.\n4.3 Case Studies\nClass Name Selection. Table 5 shows some re-\nsults of our class name ranking module for several\nqueries from different semantic classes in the Wiki\ndataset. We see that CGExpan is able to select\nthe correct class name and thus injects the correct\nsemantics in later entity selection module. More-\nover, as shown in the last column, CGExpan can\nidentify several negative class names that provide\na tight boundary for the target semantic class, in-\ncluding sports and competition for sport\nleague class, as well as city and country\nfor Chinese province class. These negative\nclass names help CGExpan avoid adding those re-\nlated but erroneous entities into the set.\nFrom Table 5 we can see that it happens when\nthe predicted positive class name is not exactly\nthe ground true class name in the original dataset.\nHowever, since we use both the generated class\nnames and currently expanded entities as guidance\nand select new entities according to the context fea-\ntures in the provided corpus, those imperfect class\nnames can still guide the set expansion process and\nperform well empirically.\nAlso, in principle, synonyms of the positive class\nname can be wrongly selected as negative class\nnames, which also happens but very rarely in our\nexperiments. However, since these synonyms con-\n8158\nSeed Entity Set Ground True Class Name Positive Class Name Negative Class Names\n{“Intel”, “Microsoft”, “Dell”} company company product, system, bank, ...\n{“United States”, “China”, “Canada”} country country state, territory, island, ...\n{“ESPNews”, “ESPN Classic”, “ABC”} tv channel television network program, sport, show, ...\n{“NHL”, “NFL”, “American league”} sports league professional league sport, competition, ...\n{“democratic”, “labor”, “tories”} party political party organization, candidate, ...\n{“Hebei”, “Shandong”, “Shanxi”} Chinese province chinese province city, country, state, ...\n{“tuberculossi”, “Parkinson’s disease”,\n“esophageal cancer”} disease chronic disease symptom, condition, ...\n{“Illinois”, “Arizona”, “California”} US state state county, country, ...\nTable 5: Class names generated for seed entity sets. The 2 nd column is the ground true class name in the original\ndataset. The 3rd and 4th columns are positive and negative class names predicted by CGExpan, respectively.\nsistently rank lower than the positive one for the\ninitial seeds based on the given corpus, they are\nindeed not good class names for this speciﬁc cor-\npus. Thus, misclassifying them will not have much\ninﬂuence on the performance of our model.\nEntity Selection. Table 6 shows expanded en-\ntity sets for two sample queries. After correctly\npredicting true positive class names and selecting\nrelevant negative class names, CGExpan utilizes\nthem to ﬁlter out those related but erroneous en-\ntities, including two TV shows in television\nnetwork class and three entities in political\nparty class. As a result, CGExpan can outper-\nform CGExpan-NoFilter.\n5 Related Work\nEntity Set Expansion.Traditional entity set ex-\npansion systems such as Google Sets (Tong and\nDean, 2008) and SEAL (Wang and Cohen, 2007,\n2008) typically submit a query consisting of seed\nentities to a general-domain search engine and ex-\ntract new entities from retrieved web pages. These\nmethods require an external search engine for on-\nline seed-oriented data collection, which can be\ncostly. Therefore, more recent studies propose to\nexpand the seed set by ofﬂine processing a corpus.\nThese corpus-based set expansion methods can be\ncategorized into two general approaches: (1) one-\ntime entity ranking which calculates entity distribu-\ntional similarities and ranks all entities once with-\nout back and forth reﬁnement (Mamou et al., 2018;\nYu et al., 2019b), and (2) iterative bootstrapping\nwhich aims to bootstrap the seed entity set by iter-\natively selecting context features and ranking new\nentities (Rong et al., 2016; Shen et al., 2017; Yan\net al., 2019; Zhu et al., 2019; Huang et al., 2020).\nOur method in general belongs to the later category.\nFinally, there are some studies that incorporate ex-\ntra knowledge to expand the entity set, including\nnegative examples (Curran et al., 2007; McIntosh\nand Curran, 2008; Jindal and Roth, 2011), semi-\nstructured web table (Wang et al., 2015), and ex-\nternal knowledge base (Yu et al., 2019a). Partic-\nularly, Wang et al. (2015) also propose to use a\nclass name to help expand the target set. However,\ntheir method requires a user-provided class name\nand utilizes web tables as additional knowledge,\nwhile our method can automatically generate both\npositive and negative class names and utilize them\nto guide the set expansion process.\nLanguage Model Probing.Traditional language\nmodels aim at assigning a probability for an in-\nput word sequence. Recent studies have shown\nthat by training on next word or missing word pre-\ndiction task, language models are able to gener-\nate contextualized word representations that bene-\nﬁt many downstream applications. ELMo (Peters\net al., 2018) proposes to learn a BiLSTM model\nthat captures both forward and backward contexts.\nBERT (Devlin et al., 2019) leverages the Trans-\nformer architecture and learns to predict randomly\nmasked tokens in the input word sequence and\nto classify the neighboring relation between pair\nof input sentences. Based on BERT’s philosophy,\nRoBERTa (Liu et al., 2019) conducts more care-\nful hyper-parameter tuning to improve the perfor-\nmance on downstream tasks. XLNet (Yang et al.,\n2019) further combines the ideas from ELMo and\nBERT and develops an autoregressive model that\nlearns contextualized representation by maximiz-\ning the expected likelihood over permutations of\nthe input sequence.\nAside from generating contextualized represen-\ntations, pre-trained language models can also serve\nas knowledge bases when being queried appropri-\nately. Petroni et al. (2019) introduce the language\nmodel analysis probe and manually deﬁne prob-\ning queries for each relation type. By submitting\nthose probing queries to a pre-trained LM, they\nshow that we can retrieve relational knowledge and\nachieve competitive performance on various NLP\ntasks. More recently, Bouraoui et al. (2020) further\nanalyze BERT’s ability to store relational knowl-\nedge by using BERT to automatically select high-\n8159\nSeed Entity Set CGExpan CGExpan-NoCN CGExpan-NoFilter\n1 “Pb” 1 “NBC” 1 “Pb”\n2 “ABC” 2 “CBS” 2 “Mtv”\n3 “CBS” 3 “Disney Channel” 3 “ABC”\n... ... ...\n35 “Telemundo” 35 “ESPN Radio”* 35 “MyNetworkTV”\n36 “Fox Sports Net” 36 “BBC America” 36 “ESPN2”\n37 “Dateline NBC” 37 “G4” 37 “the Today Show”*\n38 “Channel 4” 38 “Sirius Satellite Radio”* 38 “Access Hollywood”*\n39 “The History Channel” 39 “TNT” 39 “Cartoon Network”\n{“ESPN”,\n“Discovery Channel”,\n“Comedy Central”}\n... ... ...\n1 “republican” 1 “national party” 1 “republican”\n2 “likud” 2 “labour party” 2 “likud”\n3 “liberal democrats” 3 “gop establishment”* 3 “liberal democrats”\n... ... ...\n40 “komeito” 40 “republican jewish coalition”* 40 “young voters”*\n41 “centrist liberal democrats” 41 “british parliament”* 41 “bjp”\n42 “aipac”* 42 “tea party patriots”* 42 “religious”*\n43 “aam aadmi party” 43 “centrist liberal democrats” 43 “congress”*\n44 “ennahda” 44 “federal government”* 44 “lib dem”\n{“democratic party”,\n“republican party”,\n“labor party”}\n... ...\nTable 6: Expanded entity sets for two sample queries, with erroneous entities colored red and marked with a “*”.\nquality templates from text corpus for new relation\nprediction. Comparing with previous work, in this\npaper, we show that probing pre-trained language\nmodel works for entity set expansion task, and we\npropose a new entity set expansion framework that\ncombines corpus-independent LM probing with\ncorpus-speciﬁc context information for better ex-\npansion performance.\n6 Conclusions\nIn this paper, we propose a new entity set expan-\nsion framework that can use a pre-trained LM to\ngenerate candidate class names for the seed set,\nrank them according to the provided text corpus,\nand guide the entity selection process with the se-\nlected class names. Extensive experiments on the\nWiki and APR datasets demonstrate the effective-\nness of our framework on both class name predic-\ntion and entity set expansion. In the future, we\nplan to expand the method scope from expanding\nconcrete entity sets to more abstract concept sets.\nFor example, we may expand the set {“ machine\ntranslation”, “information extraction”, “syntactic\nparsing”} to acquire more NLP task concepts.\nAnother interesting direction is to generate a class\nname hierarchy via language model probing.\nAcknowledgments\nResearch was sponsored in part by US DARPA\nKAIROS Program No. FA8750-19-2-1004 and\nSocialSim Program No. W911NF-17-C-0099,\nNational Science Foundation IIS 16-18481, IIS\n17-04532, and IIS-17-41317, and DTRA HD-\nTRA11810026. Any opinions, ﬁndings, and con-\nclusions or recommendations expressed herein are\nthose of the authors and should not be interpreted\nas necessarily representing the views, either ex-\npressed or implied, of DARPA or the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for government\npurposes notwithstanding any copyright annotation\nhereon. We thank anonymous reviewers for valu-\nable feedback.\nReferences\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In AAAI.\nJames R. Curran, Tara Murphy, and Bernhard Scholz.\n2007. Minimising semantic drift with mutual exclu-\nsion bootstrapping. In PAACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nMarti A Hearst. 1992. Automatic acquisition of hy-\nponyms from large text corpora. In COLING.\nWen Hua, Zhongyuan Wang, Haixun Wang, Kai Zheng,\nand Xiaofang Zhou. 2017. Understand short texts by\nharvesting and analyzing semantic knowledge. In\nTKDE.\nJiaxin Huang, Yiqing Xie, Yu Meng, Jiaming Shen,\nYunyi Zhang, and Jiawei Han. 2020. Guiding\ncorpus-based set expansion by auxiliary sets gener-\nation and co-expansion. In WebConf.\n8160\nPrateek Jindal and Dan Roth. 2011. Learning from neg-\native examples in set-expansion. In ICDM.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nJonathan Mamou, Oren Pereg, Moshe Wasserblat,\nAlon Eirew, Yael Green, Shira Guskin, Peter Izsak,\nand Daniel Korat. 2018. Term set expansion based\nnlp architect by intel ai lab. In EMNLP.\nTara McIntosh and James R. Curran. 2008. Weighted\nmutual exclusion bootstrapping for domain indepen-\ndent lexicon and template acquisition. In ALTA.\nPatrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-\nMaria Popescu, and Vishnu Vyas. 2009. Web-scale\ndistributional similarity and entity set expansion. In\nEMNLP.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke S. Zettlemoyer. 2018. Deep contextualized\nword representations. In NAACL-HLT.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nXin Rong, Zhe Chen, Qiaozhu Mei, and Eytan Adar.\n2016. Egoset: Exploiting word ego-networks and\nuser-generated ontology for multifaceted set expan-\nsion. In WSDM.\nJingbo Shang, Jialu Liu, Meng Jiang, Xiang Ren,\nClare R. V oss, and Jiawei Han. 2018. Automated\nphrase mining from massive text corpora. TKDE.\nJiaming Shen, Zeqiu Wu, Dongming Lei, Jingbo\nShang, Xiang Ren, and Jiawei Han. 2017. Setex-\npan: Corpus-based set expansion via context feature\nselection and rank ensemble. In ECML/PKDD.\nJiaming Shen, Zeqiu Wu, Dongming Lei, Chao Zhang,\nXiang Ren, Michelle T. Vanni, Brian M. Sadler, and\nJiawei Han. 2018a. Hiexpan: Task-guided taxon-\nomy construction by hierarchical tree expansion. In\nKDD.\nJiaming Shen, Jinfeng Xiao, Xinwei He, Jingbo Shang,\nSaurabh Sinha, and Jiawei Han. 2018b. Entity set\nsearch of scientiﬁc literature: An unsupervised rank-\ning approach. In SIGIR.\nSimon Tong and Jeff Dean. 2008. System and meth-\nods for automatically creating lists. US Patent\n7,350,187.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nChi Wang, Kaushik Chakrabarti, Yeye He, Kris Gan-\njam, Zhimin Chen, and Philip A. Bernstein. 2015.\nConcept expansion using web tables. In WWW.\nRichard C. Wang and William W. Cohen. 2007.\nLanguage-independent set expansion of named en-\ntities using the web. In ICDM.\nRichard C. Wang and William W. Cohen. 2008. Itera-\ntive set expansion of named entities using the web.\nIn ICDM.\nRichard C. Wang, Nico Schlaefer, William W. Cohen,\nand Eric Nyberg. 2008. Automatic set expansion for\nlist question answering. In EMNLP.\nChenyan Xiong, Russell Power, and James P. Callan.\n2017. Explicit semantic ranking for academic\nsearch via knowledge graph embedding. In WWW.\nLingyong Yan, Xianpei Han, Le Sun, and Ben He.\n2019. Learning to bootstrap for entity set expansion.\nIn EMNLP.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurlPS.\nJifan Yu, Chenyu Wang, Gan Luo, Lei Hou, Juan-Zi Li,\nZhiyuan Liu, and Jie Tang. 2019a. Course concept\nexpansion in moocs with external knowledge and in-\nteractive game. In ACL.\nPuxuan Yu, Zhiqi Huang, Razieh Rahimi, and James D\nAllan. 2019b. Corpus-based set expansion with lex-\nical features and distributed representations. In SI-\nGIR.\nWanzheng Zhu, Hongyu Gong, Jiaming Shen, Chao\nZhang, Jingbo Shang, Suma Bhat, and Jiawei\nHan. 2019. Fuse: Multi-faceted set expansion\nby coherent clustering of skip-grams. ArXiv,\nabs/1910.04345.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.858890175819397
    },
    {
      "name": "Class (philosophy)",
      "score": 0.7246461510658264
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.665523886680603
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.6165469884872437
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5767983198165894
    },
    {
      "name": "Natural language processing",
      "score": 0.5645580291748047
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5521763563156128
    },
    {
      "name": "Entity linking",
      "score": 0.5342682003974915
    },
    {
      "name": "Information retrieval",
      "score": 0.512171745300293
    },
    {
      "name": "Language model",
      "score": 0.46774962544441223
    },
    {
      "name": "Task (project management)",
      "score": 0.41534435749053955
    },
    {
      "name": "Knowledge base",
      "score": 0.12413045763969421
    },
    {
      "name": "Programming language",
      "score": 0.1225976049900055
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 51
}