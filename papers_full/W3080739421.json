{
    "title": "Taming Pretrained Transformers for Extreme Multi-label Text Classification",
    "url": "https://openalex.org/W3080739421",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A3016980541",
            "name": "Chang Wei Cheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2591561200",
            "name": "Yu-Hsiang Fu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2169901301",
            "name": "Zhong Kai",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098367636",
            "name": "Yang Yiming",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222291380",
            "name": "Dhillon, Inderjit",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2921113176",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2951434086",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W2896214533",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2890634844",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2906963924",
        "https://openalex.org/W2939507640",
        "https://openalex.org/W2936191805",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W1940008012",
        "https://openalex.org/W2788125153",
        "https://openalex.org/W2951238624",
        "https://openalex.org/W2739996966",
        "https://openalex.org/W1834987204",
        "https://openalex.org/W2970449868",
        "https://openalex.org/W2165558283",
        "https://openalex.org/W2150385485",
        "https://openalex.org/W2970574105",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W21006490",
        "https://openalex.org/W2996064239",
        "https://openalex.org/W2114393161",
        "https://openalex.org/W2520348554",
        "https://openalex.org/W97540112",
        "https://openalex.org/W2773640334",
        "https://openalex.org/W2362855512",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W2155144632",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2461743311",
        "https://openalex.org/W2740731087",
        "https://openalex.org/W2941950312",
        "https://openalex.org/W2743021690",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2751120573",
        "https://openalex.org/W756166754",
        "https://openalex.org/W2068074736",
        "https://openalex.org/W2909544278",
        "https://openalex.org/W2963908579",
        "https://openalex.org/W2183087644",
        "https://openalex.org/W2744136723"
    ],
    "abstract": "We consider the extreme multi-label text classification (XMC) problem: given an input text, return the most relevant labels from a large label collection. For example, the input text could be a product description on Amazon.com and the labels could be product categories. XMC is an important yet challenging problem in the NLP community. Recently, deep pretrained transformer models have achieved state-of-the-art performance on many NLP tasks including sentence classification, albeit with small label sets. However, naively applying deep transformer models to the XMC problem leads to sub-optimal performance due to the large output space and the label sparsity issue. In this paper, we propose X-Transformer, the first scalable approach to fine-tuning deep transformer models for the XMC problem. The proposed method achieves new state-of-the-art results on four XMC benchmark datasets. In particular, on a Wiki dataset with around 0.5 million labels, the prec@1 of X-Transformer is 77.28%, a substantial improvement over state-of-the-art XMC approaches Parabel (linear) and AttentionXML (neural), which achieve 68.70% and 76.95% precision@1, respectively. We further apply X-Transformer to a product2query dataset from Amazon and gained 10.7% relative improvement on prec@1 over Parabel.",
    "full_text": "Taming Pretrained Transformers\nfor Extreme Multi-label Text Classification\nWei-Cheng Chang\nCarnegie Mellon University\nHsiang-Fu Yu\nAmazon\nKai Zhong\nAmazon\nYiming Yang\nCarnegie Mellon University\nInderjit S. Dhillon\nAmazon & UT Austin\nABSTRACT\nWe consider the extreme multi-label text classification (XMC) prob-\nlem: given an input text, return the most relevant labels from a large\nlabel collection. For example, the input text could be a product de-\nscription on Amazon.com and the labels could be product categories.\nXMC is an important yet challenging problem in the NLP commu-\nnity. Recently, deep pretrained transformer models have achieved\nstate-of-the-art performance on many NLP tasks including sen-\ntence classification, albeit with small label sets. However, naively\napplying deep transformer models to the XMC problem leads to\nsub-optimal performance due to the large output space and the label\nsparsity issue. In this paper, we propose X-Transformer, the first\nscalable approach to fine-tuning deep transformer models for the\nXMC problem. The proposed method achieves new state-of-the-art\nresults on four XMC benchmark datasets. In particular, on a Wiki\ndataset with around 0.5 million labels, the prec@1 ofX-Transformer\nis 77.28%, a substantial improvement over state-of-the-art XMC ap-\nproaches Parabel (linear) andAttentionXML (neural), which achieve\n68.70% and 76.95% precision@1, respectively. We further applyX-\nTransformer to a product2query dataset from Amazon and gained\n10.7% relative improvement on prec@1 over Parabel.\nCCS CONCEPTS\n• Computing methodologies →Machine learning ; Natural lan-\nguage processing ; • Information systems →Information retrieval .\nKEYWORDS\nTransformer models, eXtreme Multi-label text classification\nACM Reference Format:\nWei-Cheng Chang, Hsiang-Fu Yu, Kai Zhong, Yiming Yang, and Inderjit\nS. Dhillon. 2020. Taming Pretrained Transformers for Extreme Multi-label\nText Classification. In Proceedings of the 26th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining (KDD ’20), August 23–27, 2020, Virtual\nEvent, CA, USA. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/\n3394486.3403368\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nKDD ’20, August 23–27, 2020, Virtual Event, CA, USA\n© 2020 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-7998-4/20/08.\nhttps://doi.org/10.1145/3394486.3403368\n1 INTRODUCTION\nWe are interested in the Extreme multi-label text classification\n(XMC) problem: given an input text instance, return the most rele-\nvant labels from an enormous label collection, where the number\nof labels could be in the millions or more. One can view the XMC\nproblem as learning a score function f : X×Y→ R, that maps an\n(instance, label) pair (x, y)to a score f (x, y). The function f should\nbe optimized such that highly relevant(x, y)pairs have high scores,\nwhereas the irrelevant pairs have low scores. Many real-world ap-\nplications are in this form. For example, in E-commerce dynamic\nsearch advertising, x represents an item and y represents a bid\nquery on the market [20, 21]. In open-domain question answering,\nx represents a question and y represents an evidence passage con-\ntaining the answer [4, 11]. In the PASCAL Large-Scale Hierarchical\nText Classification (LSHTC) challenge,x represents an article and y\nrepresents a category of the Wikipedia hierarchical taxonomy [17].\nXMC is essentially a text classification problem on an industrial\nscale, which is one of the most important and fundamental topics in\nmachine learning and natural language processing (NLP) communi-\nties. Recently, deep pretrained Transformers, e.g., BERT [5], along\nwith its many successors such as XLNet [ 30] and RoBERTa [13],\nhave led to state-of-the-art performance on many tasks, such as\nquestion answering, part-of-speech tagging, information retrieval,\nand sentence classification with very few labels. Deep pretrained\nTransformer models induce powerful token-level and sentence-level\nembeddings that can be rapidly fine-tuned on many downstream\nNLP problems by adding a task-specific lightweight linear layer on\ntop of the transformer models.\nHowever, how to successfully apply Transformer models to XMC\nproblems remains an open challenge, primarily due to the extremely\nlarge output space and severe label sparsity issues. As a concrete\nexample, Table 1 compares the model size (in terms of the number\nof model parameters) and GPU memory usage, when applying a 24-\nlayer XLNet model to a binary classification problem (e.g., the MNLI\ndataset of GLUE [27]) versus its application to an XMC problem with\n1 million labels. Note that the classifier for the MNLI problem and\nXMC problem has a model size of 2K and 1025M, respectively. This\nmeans that the latter is a much harder problem than the former from\nthe model optimization point of view. Additionally, in attempting\nto solve the XMC problem, we run out of GPU memory even for a\nsingle example mini-batch update. Table 1 gives the details of the\nGPU memory usage in the training stages of one forward pass, one\nbackward pass and one optimization step, respectively.\nIn addition to the computational challenges, the large output\nspace in XMC is exacerbated by a severe label sparsity issue. The\nleft part of Figure 1 illustrates the “long-tailed” label distribution\narXiv:1905.02331v4  [cs.LG]  23 Jun 2020\nXLNet-large model (# params) (batch size, sequence length)=(1,128)\nproblem encoder classifier total load model +forward +backward +optimizer step\nGLUE (MNLI) 361 M 2 K 361 M 2169 MB 2609 MB 3809 MB 6571 MB\nXMC (1M) 361 M 1,025 M 1,386 M 6077 MB 6537 MB OOM OOM\nTable 1: On the left of are the model sizes (numbers of parameters) when applying the XLNet-large model to the MNLI problem\nvs. the XMC (1M) problem; on the right is the GPU memory usage (in megabytes) in solving the two problems, respectively.\nThe results were obtained on a recent Nvidia 2080Ti GPU with 12GB memory. OOM stands for out-of-memory.\nin the Wiki-500K data set [25]. Only 2% of the labels have more\nthan 100 training instances, while the remaining 98% are long-tail\nlabels with much fewer training instances. How to successfully\nfine-tune Transformer models with such sparsely labeled data is a\ntough question that has not been well-studied so far, to the best of\nour knowledge.\nFigure 1: On the left, Wiki-500K shows a long-tail distribution\nof labels. Only 2.1% of the labels have more than 100 training\ninstances, as indicated by the cyan blue regime. On the right\nis the clusters distribution after our semantic label indexing\nbased on different label representations; 99.4% of the clusters\nhave more than 100 training instances, which mitigates the\ndata sparsity issue for fine-tuning of Transformer models.\nInstead of fine-tuning deep Transformer models and dealing\nwith the bottleneck classifier layer, an alternative is to use a more\neconomical transfer learning paradigm as studied in the context\nof word2vec [15], ELMo [19], and GPT [22]. For instance, ELMo\nuses a (bi-directional LSTM) model pretrained on large unlabeled\ntext data to obtain contexualized word embeddings. When applying\nELMo on a downstream task, these word embeddings can be used as\ninput without adaptation. This is equivalent to freezing the ELMo\nencoder, and fine-tuning the downstream task-specific model on\ntop of ELMo, which is much more efficient in terms of memory as\nwell as computation. However, such a benefit comes at the price of\nlimiting the model capacity from adapting the encoder, as we will\nsee in the experimental results in Section 4.\nIn this paper, we propose X-Transformer, a new approach that\novercomes the aforementioned issues, with successful fine-tuning\nof deep Transformer models for the XMC problem. X-Transformer\nconsists of a Semantic Label Indexing component, a Deep Neural\nMatching component, and an Ensemble Ranking component. First,\nSemantic label Indexing (SLI) decomposes the original intractable\nXMC problem into a set of feasible sub-problems of much smaller\noutput space via label clustering, which mitigates the label sparsity\nissue as shown in the right part of Figure 1. Second, the Deep Neural\nMatching component fine-tunes a Transformer model for each of\nthe SLI-induced XMC sub-problems, resulting in a better mapping\nfrom the input text to the set of label clusters. Finally, the Ensemble\nRanking component is trained conditionally on the instance-to-\ncluster assignment and neural embedding from the Transformer,\nand is used to assemble scores derived from various SLI-induced\nsub-problems for further performance improvement.\nIn our experiments, the proposed X-Transformer achieves new\nstate-of-the-art results on four XMC benchmarks and leads to im-\nprovement on two real-would XMC applications. On a Wiki dataset\nwith a half million labels, the precision@1 ofX-Transformerreaches\n77.28%, a substantial improvement over the well-established hierar-\nchical label tree approachParabel [20] (i.e., 68.70%) and the compet-\ning deep learning method AttentionXML [32] (i.e., 76.95%). Further-\nmore, X-Transformeralso demonstrates great impact on the scalabil-\nity of deep Transformer models in real-world large applications. In\nour application of X-Transformer to Amazon Product2Query prob-\nlem that can be formulated as XMC, X-Transformer significantly\noutperforms Parabel too. The dataset, experiment code, models are\navailable: https://github.com/OctoberChang/X-Transformer.\n2 RELATED WORK AND BACKGROUND\n2.1 Extreme Multi-label Classification\nSparse Linear Models. To overcome computational issues, most\nexisting XMC algorithms use sparse TF-IDF features (or slight\nvariants), and leverage different partitioning techniques on the\nlabel space to reduce complexity. For example, sparse linear one-vs-\nall (OVA) methods such as DiSMEC [1], ProXML [2] and PPDSparse\n[31] explore parallelism to speed up the algorithm and reduce the\nmodel size by truncating model weights to encourage sparsity. OVA\napproaches are also widely used as building blocks for many other\napproaches, for example, in Parabel [20] and SLICE [7], linear OVA\nclassifiers with smaller output domains are used.\nThe efficiency and scalability of sparse linear models can be fur-\nther improved by incorporating different partitioning techniques\non the label spaces. For instance, Parabel [20] partitions the labels\nthrough a balanced 2-means label tree using label features con-\nstructed from the instances. Recently, several approaches are pro-\nposed to improve Parabel. Bonsai [9] relaxes two main constraints\nin Parabel: 1) allowing multi-way instead of binary partitionings\nof the label set at each intermediate node, and 2) removing strict\nbalancing constraints on the partitions. SLICE [7] considers build-\ning an approximate nearest neighbor (ANN) graph as an indexing\nstructure over the labels. For a given instance, the relevant labels\ncan be found quickly from the nearest neighbors of the instance\nvia the ANN graph.\nDeep Learning Approaches. Instead of using handcrafted TF-\nIDF features which are hard to optimize for different downstream\nXMC problems, deep learning approaches employ various neural\nnetwork architectures to extract semantic embeddings of the in-\nput text. XML-CNN [12] employs one-dimensional Convolutional\nneural networks along both sequence length and word embedding\ndimension for representing text input. As a follow-up, SLICE con-\nsiders dense embedding from the supervised pre-trainedXML-CNN\nmodels as the input to its hierarchical linear models. More recently,\nAttentionXML [32] uses BiLSTMs and label-aware attention as the\nscoring function, and performs warm-up training of the models\nwith hierarchical label trees. In addition, AttentionXML consider\nvarious negative sampling strategies on the label space to avoid\nback-propagating the entire bottleneck classifier layer.\n2.2 Transfer Learning Approaches in NLP\nRecently, the NLP community has witnessed a dramatic paradigm\nshift towards the “pre-training then fine-tuning” framework. One\nof the pioneering works is BERT [5], whose pre-training objectives\nare masked token prediction and next sentence prediction tasks.\nAfter pre-training on large-scale unsupervised corpora such as\nWikipedia and BookCorpus, the Transformer model demonstrates\nvast improvement over existing state-of-the-art when fine-tuned\non many NLP tasks such as the GLUE benchmark [ 27], named\nentity recognition, and question answering. More advanced vari-\nants of the pre-trained Transformer models include XLNet [30] and\nRoBERTa [13]. XLNet considers permutation language modeling as\nthe pre-training objective and two-stream self-attention for target-\naware token prediction. It is worth noting that the contextualized\ntoken embeddings extracted from XLNet also demonstrate compet-\nitive performance when fed into a task-specific downstream model\non large-scale retrieval problems. RoBERTa improves upon BERT\nby using more robust optimization with large-batch size update,\nand pre-training the model for longer till it truly converges.\nHowever, transferring the success of these pre-trained Trans-\nformer models on the GLUE text classification to the XMC problem\nis non-trivial, as we illustrated in Table 1. Before the emergence of\nBERT-type end-to-end fine-tuning, the canonical way of transfer\nlearning in NLP perhaps comes from the well-known Word2Vec [15]\nor GloVe [18] papers. Word2vec is a shallow two-layer neural net-\nwork that is trained to reconstruct the linguistic context of words.\nGLoVe considers a matrix factorization objective to reconstruct the\nglobal word-to-word co-occurrence in the corpus. A critical down-\nside of Word2vec and GloVe is that the pre-trained word embed-\ndings are not contextualized depending on the local surrounding\nword. ELMo [ 19] and GPT2 [ 22] instead present contextualized\nword embeddings by using large BiLSTM or Transformer models.\nAfter the models are pre-trained, transfer learning can be easily\ncarried out by feeding these extracted word embeddings as input\nto the downstream task-specific models. This is more efficient com-\npared to the BERT-like end-to-end additional fine-tuning of the\nencoder, but comes at the expense of losing model expressiveness.\nIn the experimental results section, we show that using fixed word\nembeddings from universal pre-trained models such as BERT is not\npowerful enough for XMC problems.\n2.3 Amazon Applications\nMany challenging problems at Amazon amount to finding relevant\nresults from an enormous output space of potential candidates: for\nexample, suggesting keywords to advertisers starting new cam-\npaigns on Amazon, predicting next queries a customer will type\nbased on the previous queries he/she typed. Here we discuss key-\nword recommendation system for Amazon Sponsored Products,\nas illustrations in Fig.2, and how it can be formulated as XMC\nproblems.\nKeyword recommendation system. Keyword Recommenda-\ntion Systems provide keyword suggestions for advertisers to create\ncampaigns. In order to maximize the return of investment for the\nadvertisers, the suggested keywords should be highly relevant to\ntheir products so that the suggestions can lead to conversion. An\nXMC model, when trained on an product-to-query dataset such as\nproduct-query customer purchase records, can suggest queries that\nare relevant to any given product by utilizing product information,\nlike title, description, brand, etc.\nFigure 2: keyword recommendation system\n3 PROPOSED METHOD: X-TRANSFORMER\n3.1 Problem Formulation\nMotivations. Given a training set D= {(xi , yi )|xi ∈X, yi ∈\n{0, 1}L, i = 1, . . . ,N }, extreme multi-label classification aims to\nlearn a scoring function f that maps an input (or instance) xi and\na label l to a score f (xi , l)∈ R. The function f should be optimized\nsuch that the score is high when yil = 1 (i.e., label l is relevant\nto instance xi ) and the score is low when yil = 0. A simple one-\nversus-all approach realizes the scoring function f as\nf (x, l)= wT\nl ϕ(x)\nwhere ϕ(x)represents an encoding and W = [w1, . . . ,wL]T ∈\nRL×d is the classifier bottleneck layer. For convenience, we further\ndefine the top-b prediction operator as\nfb (x)= Top-b\n\u0010\u0002\nf (x, 1), . . . ,f (x, L)\n\u0003\u0011\n∈{1, . . . ,L},\nwhere fb (x)is an index set containing the top-b predicted labels.\nAs we pointed out in Table 1, it is not only very difficult to fine-tune\nthe Transformer encoders ϕT (x;θ)together with the intractable\nclassifier layer W, but also extremely slow to compute the top-K\npredicted labels efficiently.\nFigure 3: The proposed X-Transformer framework. First, Semantic Label Indexing reduces the large output space. Transform-\ners are then fine-tuned on the XMC sub-problem that maps instances to label clusters. Finally, linear rankers are trained\nconditionally on the clusters and Transformer’s output in order to re-rank the labels within the predicted clusters.\nHigh-level Sketch. To this end, we propose X-Transformer as\na practical solution to fine-tune deep Transformer models on XMC\nproblems. Figure 3 summarizes our proposed framework.\nIn a nutshell, X-Transformer decomposes the intractable XMC\nproblem to a feasible sub-problem with a smaller output space,\nwhich is induced from semantic label indexing, which clusters the\nlabels. We refer to this sub-problem as the neural matcher of the\nfollowing form:\nд(x, k)= wT\nk ϕT (x), k = 1, . . . ,K (1)\nwhere K is the number of clusters which is significantly smaller\nthan the original intractable XMC problem of size O(L). Finally,\nX-Transformer currently uses a linear ranker that conditionally\ndepends on the embedding of transformer models and its top- b\npredicted clusters дb (x).\nf (x, l)=\n(\nσ\u0000д(x, cl ), h(x, l)\u0001, if cl ∈дb (x),\n−∞, otherwise. (2)\nHere cl ∈{1, . . . ,K}represents the cluster index of label l, д(x, cl )\nis the neural matcher realized by deep pre-trained Transformers,\nh(x, l)is the linear ranker, andσ()is a non-linear activation function\nto combine the final scores fromдand h. We now further introduce\neach of these three components in detail.\n3.2 Semantic Label Indexing\nInducing latent clusters with semantic meaning brings several ad-\nvantages to our framework. We can perform a clustering of labels\nthat can be represented by a label-to-cluster assignment matrix\nC ∈{0, 1}L×K where clk = 1 means label l belongs to cluster k.\nThe number of clusters K is typically set to be much smaller than\nthe original label space L. Deep Transformer models are fine-tuned\non the induced XMC sub-problem where the output space is of size\nK, which significantly reduces the computational cost and avoids\nthe label sparsity issue in Figure 1. Furthermore, the label clustering\nalso plays a crucial role in the linear ranker h(x, l). For example,\nonly labels within a cluster are used to construct negative instances\nfor training the ranker. In prediction, ranking is only performed\nfor labels within a few clusters predicted by our deep Transformer\nmodels.\nGiven a label representation, we cluster theL labels hierarchically\nto form a hierarchical label tree with K leaf nodes [7, 9, 20, 32]. For\nsimplicity, we consider binary balanced hierarchical trees [14, 20]\nas the default setting. Due to the lack of a direct and informative\nrepresentation of the labels, the indexing system for XMC may be\nnoisy. Fortunately, the instances in XMC are typically very informa-\ntive. Therefore, we can utilize the rich information of the instances\nto build a strong matching system as well as a strong ranker to\ncompensate for the indexing system.\nLabel embedding via label text. Given text information about\nlabels, such as a short description of categories in the Wikipedia\ndataset or search queries on the Amazon shopping website, we can\nuse this short text to represent the labels. In this work, we use a\npretrained XLNet [19] to represent the words in the label. The label\nembedding is the mean pooling of all XLNet word embeddings in\nthe label text. Specifically, the label embedding of label l is\nψtext-emb(l)= 1\n|text(l)|\nÕ\nw ∈text (l)\nϕxlnet(w)\nwhere ϕxlnet (w)is the hidden embedding of token w in label l.\nLabel embedding via embedding of positive instances. The\nshort text of labels may not contain sufficient information and is\noften ambiguous and noisy for some XMC datasets. Therefore we\ncan derive a label representation from embedding of its positive\ninstances. Specifically, the label embedding of label l is\nψpifa-tfidf(l)= vl /∥vl ∥, vl =\nÕ\ni:yil =1\nϕtf-idf(xi ), l = 1, . . . ,L,\nψpifa-neural(l)= vl /∥vl ∥, vl =\nÕ\ni:yil =1\nϕxlnet(xi ), l = 1, . . . ,L.\nWe refer to this type of label embedding as Positive Instance Feature\nAggregation (PIFA), which is used in recent state-of-the-art XMC\nmethods [7, 9, 20, 32]. Note that X-Transformer is not limited by\nthe above mentioned label representations; indeed in applications\nwhere labels encode richer meta information such as a graph, we\ncan use label representations derived from graph clustering and\ngraph convolution.\n3.3 Deep Transformer as Neural Matcher\nAfter Semantic Label Indexing (SLI), the original intractable XMC\nproblem morphs to a feasible XMC sub-problem with a much\nsmaller output space of size K. See Table 2 for the exact K that\nwe used for each XMC data set. Specifically, the deep Transformer\nmodel now aims to map each text instance to the assigned rel-\nevant clusters. The induced instance-to-cluster assignment ma-\ntrix is M = YC = [m1, . . . ,mi , . . . ,mN ]T ∈ {0, 1}N ×K where\nY ∈RN ×L is the original instance-to-label assignment matrix and\nC ∈RL×K is the label-to-cluster assignment matrix provided by\nthe SLI stage. The goal now becomes fine-tuning deep Transformer\nmodels д(x, k; W, θ)on {(xi , mi )|i = 1, . . . ,N }such that\nmin\nW, θ\n1\nNK\nNÕ\ni=1\nKÕ\nk=1\nmax \u00000, 1 − ˜Mik д(x, k; W, θ)\u00012, (3)\ns.t. д(x, k; W, θ)= wT\nk ϕtransformer(x),\nwhere ˜Mik = 2Mik −1 ∈{−1, 1}, W = [w1, . . . ,wK ]T ∈RK×d ,\nand ϕtransformer(x)∈ Rd is the embedding from the Transformers.\nWe use the squared-hinge loss in the matching objective (3) as it\nhas shown better ranking performance as shown in [31]. Next, we\ndiscuss engineering optimizations and implementation details that\nconsiderably improve training efficiency and model performance.\nPretrained Transformers. We consider three state-of-the-art\npre-trained Transformer-large-cased models (i.e., 24 layers with\ncase-sensitive vocabulary) to fine-tune, namely BERT [5], XLNet [30],\nand RoBERTa [13]. The instance embedding ϕ(x)is the \"[CLS]\"-like\nhidden states from the last layer of BERT, RoBERTa and XLNet.\nComputationally speaking, BERT and RoBERTa are similar while\nXLNet is nearly 1.8 times slower. In terms of performance on XMC\ntasks, we found RoBERTa and XLNet to be slightly better than\nBERT, but the gap is not as significant as in the GLUE benchmark.\nMore concrete analysis is available in Section 4.\nIt is possible to use Automatic Mixed Precision (AMP) between\nFloat32 and Float16 for model fine-tuning, which can considerably\nreduce the model’s GPU memory usage and training speed. How-\never, we used Float32 for all the experiments as our initial trials of\ntraining Transformers in AMP mode often led to unstable numerical\nresults for the large-scale XMC dataset Wiki-500K.\nInput Sequence Length. The time and space complexity of the\nTransformer scales quadratically with the input sequence length,\ni.e., O(T 2)[26], where T = len(x)is the number of tokenized sub-\nwords in the instance x. Using smaller T reduces not only the GPU\nmemory usage that supports using larger batch size, but also in-\ncreases the training speed. For example, BERT first pre-trains on\ninputs of sequence length 128 for 90% of the optimization, and the\nremaining 10% of optimization steps on inputs of sequence length\n512 [5]. Interestingly, we observe that the model fine-tuned with\nsequence length 128 v.s. sequence length 512 does not differ signif-\nicantly in the downstream XMC ranking performance. Thus, we\nfix the input sequence length to be T = 128 for model fine-tuning,\nwhich significantly speeds up the training time. It would be interest-\ning to see if we can bootstrap training the Transformer models from\nshorter sequence length and ramp up to larger sequence length\n(e.g., 32, 64, 128, 256), but we leave that as future work.\nFigure 4: Training rankers with the Teacher Forcing Nega-\ntives(TFN) strategy. For illustration, we have N = 6 instances, L = 20\nlabels, K = 4 label clusters, and M ∈{0, 1}6×4 denotes the instance-\nto-cluster assignment matrix. For example, Cluster 1 with the or-\nange color contains the first 5 labels. The nonzeros of the first col-\numn of M correspond to {x1, x2, x6 }, which are instances with at\nleast one positive label contained in Cluster 1. For each label in the\nfirst cluster, the ranker using Teacher Forcing Negatives (TFN) only\nconsiders these three instances. Matcher-aware Negatives (MAN)\nstrategy is introduced in Section 3.4 to further add improved hard\nnegatives to enhance the TFN strategy.\nBootstrapping Label Clustering and Ranking. After fine-\ntuning a deep Transformer model, we have powerful instance rep-\nresentation ϕtransformer(x)that can be used to bootstrap semantic\nlabel clustering and ranking. For label clustering, the embedding\nlabel l can be constructed by aggregating the embeddings of its\npositive instances. For ranking, the fine-tuned Transformer embed-\nding can be concatenated with the sparse TF-IDF vector for better\nmodeling power. See details in the ablation study Table 5.\n3.4 Ranking\nAfter the matching step, a small subset of label clusters is retrieved.\nThe goal of the ranker is to model the relevance between the in-\nstance and the labels from the retrieved clusters. Formally, given a\nlabel l and an instance x, we use a linear one-vs-all (OVA) classifier\nto parameterize the ranker h(x, l)= wT\nl ϕ(x)and train it with a\nbinary loss. For each label, naively estimating the weightswl based\non all instances {(xi , Yi, l )}N\ni=1 takes O(N ), which is too expensive.\nInstead, we consider two sampling strategies that only include hard\nnegative instances to reduce the computational complexity: Teacher\nForcing Negatives (TFN) and Matcher-aware Negatives (MAN).\nTeacher Forcing Negatives (TFN) . for each label l, we only\ninclude a subset of instances induced by the instance-to-cluster\nassignment matrix M = YC. In particular, in addition to the pos-\nitive instances corresponding to the l-th label, we only include\ninstances whose labels belong to the same cluster as the l-th label,\ni.e., {(xi , yi, l : i ∈{i : Mi, cl = 1}}. In Figure 4, we illustrate the\nTFN strategy with a toy example. As the first five labels belong to\nCluster 1, and only {x1, x2, x6}contain a positive label within this\ncluster, we only consider this subset of instances to train a binary\nclassifier for each of the first five labels.\nMatcher-aware Negatives (MAN) . The Teacher Forcing strat-\negy only includes negative instances which are hard from the\n“teacher”, i.e., the ground truth instance-to-clustering assignment\nmatrix M used to train our neural matcher. However,M is indepen-\ndent from the performance of our neural matcher. Thus, training\nranker with the TFN strategy alone might introduce an exposure\nbias issue, i.e., training-inference discrepancy. Instead, we also con-\nsider including matcher-aware hard negatives for each label. In\nparticular, we can use the instance-to-cluster prediction matrix\nˆM ∈{0, 1}N ×K from our neural matcher, where the nonzeros of\nthe i-th row of ˆM correspond to the top-b predicted clusters from\nдb (xi ). In practice, we observe that a combination of TFN and MAN\nyields the best performance, i.e., using M′ = YC + ˆM to include\nhard negatives for each label. See Table 5 for a detailed Ablation\nstudy.\nFor the ranker input representation, we not only leverage the\nTF-IDF features ϕtf-idf(x), but also exploit the neural embeddings\nϕneural(x)from either the pre-trained or fine-tuned Transformer\nmodel. After the ranker is trained, the final ranking scores are\ncomputed via (2). We can further ensemble the scores from different\nX-Transformer models, which are trained on different semantic-\naware label clusters or different pre-trained Transformer models\nsuch as BERT, RoBERTa and XLNet.\n4 EMPIRICAL RESULTS\nThe experiment code, including datasets and fine-tuned models are\npublicly available. 1\n4.1 Datasets and Preprocessing\nXMC Benchmark Data. We consider four multi-label text clas-\nsification data sets used in AttentionXML [32] for which we had\naccess to the raw text representation, namely Eurlex-4K, Wiki10-\n31K, AmazonCat-13K and Wiki-500K. Summary statistics of the\ndata sets are given in Table 2. We follow the training and test split\nof [32] and set aside 10% of the training instances as the validation\nset for hyperparameter tuning.\nAmazon Applications. We consider an internal Amazon data\nset, namely Prod2Query-1M, which consists of 14 million instances\n(products) and 1 million labels (queries) where the label is positive\nif a product is clicked at least once as a result of a search query. We\ndivide the data set into 12.5 million training samples, 0.8 million\nvalidation samples and 0.7 million testing samples.\n4.2 Algorithms and Hyperparameters\nComparing Methods. We compare our proposedX-Transformer\nmethod to the most representative and state-of-the-art XMC meth-\nods including the embedding-based AnnexML [24]; one-versus-all\nDiSMEC [1]; instance tree based PfastreXML [8]; label tree based\nParabel [20], eXtremeText[29], Bonsai [9]; and deep learning based\nXML-CNN [12], AttentionXML [32] methods. The results of all these\nbaseline methods are obtained from [32, Table 3]. For evaluation\nwith other XMC approaches that have not released their code or\nare difficult to reproduce, we have a detailed comparison in Table 6.\nEvaluation Metrics. We evaluate all methods with example-\nbased ranking measures including Precision@k ( k = 1, 3, 5) and\nRecall@k (k = 1, 3, 5), which are widely used in the XMC litera-\nture [3, 8, 20, 21, 23].\nHyperparameters. For X-Transformer, all hyperparameters are\nchosen from the held-out validation set. The number of clusters\n1https://github.com/OctoberChang/X-Transformer\nare listed in Table 2, which are consistent with the Parabel setting\nfor fair comparison. We consider the 24 layers cased models of\nBERT [5], RoBERTa [13], and XLNet [30] using the Pytorch imple-\nmentation from HuggingFace Transformers [28]2. For fine-tuning\nthe Transformer models, we set the input sequence length to be\n128 for efficiency, and the batch size per GPU to be 16 along with\ngradient accumulation step of 4, and use 4 GPUs per model. This\ntogether amounts to a batch size of 256 in total. We use Adam [10]\nwith linear warmup scheduling as the optimizer where the learn-\ning rate is chosen from {4, 5, 6, 8}×10−5. Models are trained until\nconvergence, which takes 1k, 1.4k, 20k, 50k optimization steps for\nEurlex-4K, Wiki10-31K, AmazonCat-13K, Wiki-500K, respectively.\n4.3 Results on Public XMC Benchmark Data\nTable 3 compares the proposed X-Transformer with the most repre-\nsentative SOTA XMC methods on four benchmark datasets. Follow-\ning previous XMC works, we focus on top predictions by presenting\nPrecision@k, where k = 1, 3, 5.\nThe proposed X-Transformer outperforms all XMC methods, ex-\ncept being slightly worse than AttentionXML in terms of P@3 and\nP@5 on the Wiki-500K dataset. We also compare X-Transformer\nagainst linear baselines using Parabel model with three different\ninput representations: (1) ϕpre-xlnet denotes pretrained XLNet em-\nbeddings (2) ϕtfidf denotes TF-IDF embeddings (3) ϕfnt-xlnet ⊕ϕtfidf\ndenotes finetuned XLNet embeddings concatenated with TF-IDF\nembeeddings. We clearly see that the performance of baseline (1)\nis significantly worse. This suggests that the ELMo-style transfer\nlearning, though efficient, is not powerful to achieve good perfor-\nmance for XMC problems. The performance of baseline (2) is similar\nto that of Parabel, while baseline (3) further improves performance\ndue to the use of fine-tuned XLNet embeddings.\nAttentionXML [32] is a very recent deep learning method that\nuses BiLSTM and label-aware attention layer to model the scoring\nfunction. They also leverage hierarchical label trees to recursively\nwarm-start the models and use hard negative sampling techniques\nto avoid using the entire classifier bottleneck layer. Some of the\ntechniques in AttentionXML are complementary to our proposed X-\nTransformer, and it would be interesting to see how X-Transformer\ncan be improved from those techniques.\n4.4 Results on Amazon Applications.\nRecall that the Amazon data consists of 12 million products and\n1 million queries along with product-query relevance. We treat\nqueries as output labels and product title as input. We use the default\nParabel method (using TFIDF features) as the baseline method and\nshow X-Transformer’s relative improvement of precision and recall\nover the baseline in Table 4.\n4.5 Ablation Study\nWe carefully conduct an ablation study of X-Transformer as shown\nin Table 5. We analyze theX-Transformer framework in terms of its\nfour components: indexing, matching, ranker input representation,\nand training negative-sampling training algorithm. The configu-\nration Index 9 represents the final best configuration as reported\n2https://github.com/huggingface/transformers\nDataset ntrn ntst |Dtrn| | Dtrn| L ¯L ¯n K\nEurlex-4K 15,449 3,865 19,166,707 4,741,799 3,956 5.30 20.79 64\nWiki10-31K 14,146 6,616 29,603,208 13,513,133 30,938 18.64 8.52 512\nAmazonCat-13K 1,186,239 306,782 250,940,894 64,755,034 13,330 5.04 448.57 256\nWiki-500K 1,779,881 769,421 1,463,197,965 632,463,513 501,070 4.75 16.86 8192\nTable 2: Data Statistics. ntrn , ntst refer to the number of instances in the training and test sets, respectively. |Dtrn|, |Dtst|refer\nto the number of word tokens in the training and test corpus, respectively. L is the number of labels, ¯L the average number of\nlabels per instance, ¯n the average number of instances per label, and K is the number of clusters. The four benchmark datasets\nare the same as AttentionXML [32] for fair comparison.\nMethods Prec@1 Prec@3 Prec@5 Methods Prec@1 Prec@3 Prec@5\nEurlex-4K Wiki10-31K\nAnnexML [24] 79.66 64.94 53.52 AnnexML [24] 86.46 74.28 64.20\nDiSMEC [1] 83.21 70.39 58.73 DiSMEC [1] 84.13 74.72 65.94\nPfastreXML [8] 73.14 60.16 50.54 PfastreXML [8] 83.57 68.61 59.10\nParabel [20] 82.12 68.91 57.89 Parabel [20] 84.19 72.46 63.37\neXtremeText[29] 79.17 66.80 56.09 eXtremeText[29] 83.66 73.28 64.51\nBonsai [9] 82.30 69.55 58.35 Bonsai [9] 84.52 73.76 64.69\nMLC2seq [16] 62.77 59.06 51.32 MLC2seq [16] 80.79 58.59 54.66\nXML-CNN [12] 75.32 60.14 49.21 XML-CNN [12] 81.41 66.23 56.11\nAttentionXML [32] 87.12 73.99 61.92 AttentionXML [32] 87.47 78.48 69.37\nϕpre-xlnet + Parabel 33.53 26.71 22.15 ϕpre-xlnet + Parabel 81.77 64.86 54.49\nϕtfidf + Parabel 81.71 69.15 58.11 ϕtfidf + Parabel 84.27 73.20 63.66\nϕfnt-xlnet ⊕ϕtfidf + Parabel 84.09 71.50 60.12 ϕfnt-xlnet ⊕ϕtfidf + Parabel 87.35 78.24 68.62\nX-Transformer 87.22 75.12 62.90 X-Transformer 88.51 78.71 69.62\nAmazonCat-13K Wiki-500K\nAnnexML [24] 93.54 78.36 63.30 AnnexML [24] 64.22 43.15 32.79\nDiSMEC [1] 93.81 79.08 64.06 DiSMEC [1] 70.21 50.57 39.68\nPfastreXML [8] 91.75 77.97 63.68 PfastreXML [8] 56.25 37.32 28.16\nParabel [20] 93.02 79.14 64.51 Parabel [20] 68.70 49.57 38.64\neXtremeText[29] 92.50 78.12 63.51 eXtremeText[29] 65.17 46.32 36.15\nBonsai [9] 92.98 79.13 64.46 Bonsai [9] 69.26 49.80 38.83\nMLC2seq [16] 94.26 69.45 57.55 MLC2seq [16] - - -\nXML-CNN [12] 93.26 77.06 61.40 XML-CNN [12] - - -\nAttentionXML [32] 95.92 82.41 67.31 AttentionXML [32] 76.95 58.42 46.14\nϕpre-xlnet + Parabel 80.96 63.92 50.72 ϕpre-xlnet + Parabel 31.83 20.24 15.76\nϕtfidf + Parabel 92.81 78.99 64.31 ϕtfidf + Parabel 68.75 49.54 38.92\nϕfnt-xlnet ⊕ϕtfidf + Parabel 95.33 82.77 67.66 ϕfnt-xlnet ⊕ϕtfidf + Parabel 75.57 55.12 43.31\nX-Transformer 96.70 83.85 68.58 X-Transformer 77.28 57.47 45.31\nTable 3: Comparing X-Transformeragainst state-of-the-art XMC methods on Eurlex-4K, Wiki10-31K, AmazonCat-13K, and Wiki-500K.\nThe baselines’ results are from [32, Table 3]. Note that MLC2seq and XML-CNN are not scalable on Wiki-500K. We also present\nlinear baselines ( Parabel) with three input representations. Specifically, ϕpre-xlnet denotes pre-trained XLNet embeddings, ϕtfidf\ndenotes TF-IDF embeddings, ϕfnt-xlnet ⊕ϕtfidf denotes fine-tuned XLNet embeddings concatenate with TF-IDF embeddings.\nPrecision Recall\nMethods @1 @5 @10 @1 @5 @10\nX-Transformer 10.7% 7.4% 6.6% 12.0% 4.9% 2.8%\nTable 4: Relative improvement over Parabel on the\nProd2Query data set.\nin Table 3. There are four takeaway messages from this ablation\nstudy, and we describe them in the following four paragraphs.\nRanker Representation and Training. Config. ID0, 1, 2 shows\nthe effect of input representation and training strategy for the rank-\ning. The benefit of using instance embedding from fine-tuned trans-\nformers can be seen from config. ID 0 to 1. In addition, from ID 1\nto 2, we observe that using Teacher Forcing Negatives (TFN) is not\nenough for training the ranker, as it could suffer from the exposure\nDataset Config. ID X-TransformerAblation Configuration Evaluation Metric\nindexing matching ranker input negative-sampling P@1 P@3 P@5 R@1 R@3 R@5\nEurlex-4K\n0 pifa-tfidf BERT ϕtfidf(x) TFN 83.93 70.59 58.69 17.05 42.08 57.14\n1 pifa-tfidf BERT ϕtfidf(x)⊕ϕneural(x) TFN 85.02 71.83 59.87 17.21 42.79 58.30\n2 pifa-tfidf BERT ϕtfidf(x)⊕ϕneural(x) TFN + MAN 85.51 72.95 60.83 17.32 43.45 59.21\n3 pifa-tfidf RoBERTa ϕtfidf(x)⊕ϕneural(x) TFN + MAN 85.33 72.89 60.79 17.32 43.39 59.16\n4 pifa-tfidf XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 85.07 72.75 60.69 17.25 43.29 59.01\n5 pifa-neural XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 84.81 72.39 60.38 17.19 42.98 58.70\n6 text-emb XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 85.25 72.76 60.20 17.29 43.25 58.54\n7 all XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 86.55 74.24 61.96 17.54 44.16 60.24\n8 pifa-neural all ϕtfidf(x)⊕ϕneural(x) TFN + MAN 85.92 73.43 61.53 17.40 43.69 59.86\n9 all all ϕtfidf(x)⊕ϕneural(x) TFN + MAN 87.22 75.12 62.90 17.69 44.73 61.17\nWiki-500K\n0 pifa-tfidf BERT ϕtfidf(x) TFN 69.52 49.87 38.71 22.30 40.62 48.65\n1 pifa-tfidf BERT ϕtfidf(x)⊕ϕneural(x) TFN 71.90 51.58 40.10 23.27 42.14 50.42\n2 pifa-tfidf BERT ϕtfidf(x)⊕ϕneural(x) TFN + MAN 74.68 53.64 41.50 24.56 44.26 52.50\n3 pifa-tfidf RoBERTa ϕtfidf(x)⊕ϕneural(x) TFN + MAN 75.40 54.32 42.06 24.85 44.93 53.30\n4 pifa-tfidf XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 75.45 54.50 42.24 24.81 45.00 53.44\n5 pifa-neural XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 76.34 55.50 43.04 25.15 45.88 54.53\n6 text-emb XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 74.12 52.85 40.53 24.18 43.30 50.98\n7 all XLNet ϕtfidf(x)⊕ϕneural(x) TFN + MAN 75.85 56.08 44.24 24.80 46.36 56.35\n8 pifa-neural all ϕtfidf(x)⊕ϕneural(x) TFN + MAN 77.44 56.84 44.37 25.61 47.18 56.55\n9 all all ϕtfidf(x)⊕ϕneural(x) TFN + MAN 77.28 57.47 45.31 25.48 47.82 57.95\nTable 5: Ablation study of X-Transformer on Eurlex-4K and Wiki-500K data sets. We outline four take away messages: (1) Config.\nID= {0, 1, 2}demonstrates better performance by using Matcher-aware Negatives (MAN) and Neural embedding for training\nthe rankers; (2) Config. ID = {2, 3, 4}suggests that, performance-wise, XLNet is similar to RoBERTa, and slightly better than\nBERT; (3) Config. ID= {4, 5, 6}manifests the importance of label clusters induced from different label representations. (4) Config.\nID={7, 8, 9}indicates the effect of ensembling various configuration of the models.\nbias of only using the ground truth clustering assignment, but ig-\nnores the hard negatives mistakenly produced by the Transformer\nmodels. Note that techniques such as adding Matcher-aware neg-\natives (MAN) from previous model’s prediction to bootstrap the\nnext level’s model training is also used in AttentionXML [32].\nDifferent Transformer Models. Next, we analyze how the\nthree different Transformer models (i.e., BERT, RoBERTa, XLNet)\naffect the performance, as shown in Config. ID 2, 3, 4. For Wiki-\n500K, we observe that the XLNet and RoBERTa are generally more\npowerful than the BERT models. On the other hand, such an ad-\nvantage is not clear for Eurlex-4K, possibly due to the nature of the\ndata set.\nLabel Representation for Clustering. The importance of dif-\nferent label representation for clustering is demonstrated in Config.\nID 4, 5, 6. For Eurlex-4K, we see that using label text embedding\nas representation (i.e. text-emb) leads to the strong performance\ncompared to pifa-tfidf (id 4) and pifa-neural (id 5). In contrast, pifa-\ntfidf becomes the best performing representation on the Wiki-500K\ndataset. This phenomenon could be due to the label text of Wiki-\n500K being more noisy compared to Eurlex-4K, which deteriorates\nthe label clustering results on Wiki-500K.\nEnsemble Ranking. Finally, we show the advantage of ensem-\nbing prediction from different models as shown in Config. ID7, 8, 9.\nFor Eurlex-4K, combining predictions from different label represen-\ntations (ID 7) is better than from different Transformer models (ID\n8). Combining all (ID 9) leads to our final model, X-Transformer.\n4.6 Cross-Paper Comparisons\nMany XMC approaches have been proposed recently. However, it\nis sometimes difficult to compare metrics directly from different pa-\npers. For example, the P@1 of Parabel on Wiki-500K is 59.34% in [7,\nTable 2] and 68.52% in [20, Table 2], but we see 68.70% in Table 3.\nThe inconsistency may be due to differences in data processing,\ninput representation, or other reasons. We propose an approach to\ncalibrate these numbers so that various methods can be compared\nin a more principled way. In particular, for each metricm(·), we use\nthe relative improvement over a common anchor method, which is\nset to be Parabel as it is widely used in the literature. For a compet-\ning method X with a metric m(X)on a data set reported in a paper,\nwe can compute the relative improvement over Parabel as follows:\nm(X)−m(Parabel)\nm(Parabel) ×100%, where m(Parabel)is the metric obtained\nby Parabel on the same data set in the same paper. Following the\nabove approach, we include a variety of XMC approaches in our\ncomparison. We report the relative improvement of various meth-\nods on two commonly used data sets, Eurlex-4K and Wiki-500K, in\nTable 6. We can clearly observe thatX-Transformer brings the most\nsignificant improvement over Parabel and SLICE.\n5 CONCLUSIONS\nIn this paper, we propose X-Transformer, the first scalable frame-\nwork to fine-tune Deep Transformer models that improves state-of-\nthe-art XMC methods on four XMC benchmark data sets. We fur-\nther applied X-Transformer to a real-life application, product2query\nprediction, showing significant improvement over the competitive\nlinear models, Parabel.\nEurlex-4K Wiki-500K\nMethod Source\nRelative Improvement\nMethod Source\nRelative Improvement\nover Parabel (%) over Parabel (%)\nPrec@1 Prec@3 Prec@5 Prec@1 Prec@3 Prec@5\nX-Transformer Table 3 +6.27% +9.08% +8.55% X-Transformer Table 3 +12.49% +15.94% +17.26%\nSLICE [7, Table 2] +4.27% +3.34% +3.11% SLICE [7, Table 2] +5.53% +7.02% +7.56%\nGLaS [6, Table 3] -5.18% -5.48% -5.34% GLaS [6, Table 3] +4.77% +3.37% +4.27%\nProXML [2, Table 5] +3.86% +2.90% +2.43% ProXML [2, Table 5] +2.22% +0.82% + 2.92%\nPPD-Sparse [20, Table 2] +1.92% +2.93% +2.92% PPD-Sparse [20, Table 2] +2.39% +2.33% + 2.88%\nSLEEC [9, Table 2] -3.53% -6.40% -9.04% SLEEC [9, Table 2] -29.84% -40.73% -45.08%\nTable 6: Comparison of Relative Improvement over Parabel. The relative improvement for each state-of-the-art (SOTA) method\nis computed based on the metrics reported from its original paper as denoted in the Source column.\nREFERENCES\n[1] Rohit Babbar and Bernhard Schölkopf. 2017. DiSMEC: distributed sparse ma-\nchines for extreme multi-label classification. In WSDM.\n[2] Rohit Babbar and Bernhard Schölkopf. 2019. Data scarcity, robustness and\nextreme multi-label classification. Machine Learning (2019), 1–23.\n[3] Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain.\n2015. Sparse local embeddings for extreme multi-label classification. In NIPS.\n[4] Wei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yiming Yang, and Sanjiv Ku-\nmar. 2020. Pre-training Tasks for Embedding-based Large-scale Retrieval. In\nInternational Conference on Learning Representations .\n[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics (NAACL) .\n[6] Chuan Guo, Ali Mousavi, Xiang Wu, Daniel N Holtmann-Rice, Satyen Kale,\nSashank Reddi, and Sanjiv Kumar. 2019. Breaking the Glass Ceiling for\nEmbedding-Based Classifiers for Large Output Spaces. In Advances in Neural\nInformation Processing Systems . 4944–4954.\n[7] Himanshu Jain, Venkatesh Balasubramanian, Bhanu Chunduri, and Manik Varma.\n2019. Slice: Scalable Linear Extreme Classifiers Trained on 100 Million Labels for\nRelated Searches. In Proceedings of the Twelfth ACM International Conference on\nWeb Search and Data Mining . ACM, 528–536.\n[8] Himanshu Jain, Yashoteja Prabhu, and Manik Varma. 2016. Extreme multi-\nlabel loss functions for recommendation, tagging, ranking & other missing label\napplications. In KDD.\n[9] Sujay Khandagale, Han Xiao, and Rohit Babbar. 2019. Bonsai-Diverse and Shallow\nTrees for Extreme Multi-label Classification. arXiv preprint arXiv:1904.08249\n(2019).\n[10] Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimiza-\ntion. In Proceedings of the International Conference on Learning Representations .\n[11] Kenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for\nweakly supervised open domain question answering. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics (ACL) .\n[12] Jingzhou Liu, Wei-Cheng Chang, Yuexin Wu, and Yiming Yang. 2017. Deep\nlearning for extreme multi-label text classification. In Proceedings of the 40th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. ACM, 115–124.\n[13] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer\nLevy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. RoBERTa: A\nRobustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692\n(2019).\n[14] Mikko I Malinen and Pasi Fränti. 2014. Balanced k-means for clustering. In Joint\nIAPR International Workshops on Statistical Techniques in Pattern Recognition (SPR)\nand Structural and Syntactic Pattern Recognition (SSPR) . Springer, 32–41.\n[15] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.\nDistributed representations of words and phrases and their compositionality. In\nAdvances in neural information processing systems . 3111–3119.\n[16] Jinseok Nam, Eneldo Loza Mencía, Hyunwoo J Kim, and Johannes Fürnkranz.\n2017. Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-\nlabel Classification. In NIPS.\n[17] Ioannis Partalas, Aris Kosmopoulos, Nicolas Baskiotis, Thierry Artieres, George\nPaliouras, Eric Gaussier, Ion Androutsopoulos, Massih-Reza Amini, and Patrick\nGalinari. 2015. LSHTC: A benchmark for large-scale text classification. arXiv\npreprint arXiv:1503.08581 (2015).\n[18] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. In EMNLP. 1532–1543.\n[19] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics (NAACL) .\n[20] Yashoteja Prabhu, Anil Kag, Shrutendra Harsola, Rahul Agrawal, and Manik\nVarma. 2018. Parabel: Partitioned label trees for extreme classification with\napplication to dynamic search advertising. In WWW.\n[21] Yashoteja Prabhu and Manik Varma. 2014. Fastxml: A fast, accurate and stable\ntree-classifier for extreme multi-label learning. In KDD.\n[22] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Im-\nproving language understanding by generative pre-training. (2018).\n[23] Sashank J Reddi, Satyen Kale, Felix Yu, Dan Holtmann-Rice, Jiecao Chen, and\nSanjiv Kumar. 2019. Stochastic Negative Mining for Learning with Large Output\nSpaces. In AISTATS.\n[24] Yukihiro Tagami. 2017. AnnexML: Approximate nearest neighbor search for\nextreme multi-label classification. In Proceedings of the 23rd ACM SIGKDD inter-\nnational conference on knowledge discovery and data mining . 455–464.\n[25] Manik Varma. 2019. The Extreme Classification Repository: Multi-label Datasets\n& Code. http://manikvarma.org/downloads/XC/XMLRepository.html.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\n[27] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R\nBowman. 2018. Glue: A multi-task benchmark and analysis platform for natural\nlanguage understanding. arXiv preprint arXiv:1804.07461 (2018).\n[28] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,\nAnthony Moi, Pierric Cistac, Tim Rault, R’emi Louf, Morgan Funtowicz, and Jamie\nBrew. 2019. HuggingFace’s Transformers: State-of-the-art Natural Language\nProcessing. ArXiv abs/1910.03771 (2019).\n[29] Marek Wydmuch, Kalina Jasinska, Mikhail Kuznetsov, Róbert Busa-Fekete, and\nKrzysztof Dembczynski. 2018. A no-regret generalization of hierarchical softmax\nto extreme multi-label classification. In NIPS.\n[30] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nand Quoc V Le. 2019. XLNet: Generalized Autoregressive Pretraining for Lan-\nguage Understanding. In NIPS.\n[31] Ian EH Yen, Xiangru Huang, Wei Dai, Pradeep Ravikumar, Inderjit Dhillon, and\nEric Xing. 2017. PPDsparse: A parallel primal-dual sparse method for extreme\nclassification. In KDD. ACM.\n[32] Ronghui You, Zihan Zhang, Ziye Wang, Suyang Dai, Hiroshi Mamitsuka, and\nShanfeng Zhu. 2019. AttentionXML: Label Tree-based Attention-Aware Deep\nModel for High-Performance Extreme Multi-Label Text Classification. In Ad-\nvances in Neural Information Processing Systems . 5812–5822."
}