{
    "title": "Head-Free Lightweight Semantic Segmentation with Linear Transformer",
    "url": "https://openalex.org/W4382240924",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1997810020",
            "name": "Dong Bo",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2097016312",
            "name": "Pichao Wang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2080195193",
            "name": "Fan Wang",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6730587030",
        "https://openalex.org/W2787091153",
        "https://openalex.org/W6799772243",
        "https://openalex.org/W3217112505",
        "https://openalex.org/W3180659539",
        "https://openalex.org/W2340897893",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6653267767",
        "https://openalex.org/W4225722315",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W3203105104",
        "https://openalex.org/W3042844210",
        "https://openalex.org/W6797578546",
        "https://openalex.org/W3216314363",
        "https://openalex.org/W2796438033",
        "https://openalex.org/W6795103355",
        "https://openalex.org/W2946942188",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3175515048",
        "https://openalex.org/W3186681406",
        "https://openalex.org/W3170544306",
        "https://openalex.org/W6735937681",
        "https://openalex.org/W3213792864",
        "https://openalex.org/W6796203388",
        "https://openalex.org/W6797784111",
        "https://openalex.org/W6730342312",
        "https://openalex.org/W6848325238",
        "https://openalex.org/W6741437728",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3213165621",
        "https://openalex.org/W4312977443",
        "https://openalex.org/W2964309882",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W4386071535",
        "https://openalex.org/W3165745140",
        "https://openalex.org/W2561196672",
        "https://openalex.org/W3213601271",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4313023779",
        "https://openalex.org/W3203701986",
        "https://openalex.org/W4313160444",
        "https://openalex.org/W4308909683",
        "https://openalex.org/W4312815172",
        "https://openalex.org/W3094728142",
        "https://openalex.org/W4287025584",
        "https://openalex.org/W2560023338",
        "https://openalex.org/W4286910290",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W2011191879",
        "https://openalex.org/W3034175346",
        "https://openalex.org/W4287324101",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W3177248696",
        "https://openalex.org/W3122239467"
    ],
    "abstract": "Existing semantic segmentation works have been mainly focused on designing effective decoders; however, the computational load introduced by the overall structure has long been ignored, which hinders their applications on resource-constrained hardwares. In this paper, we propose a head-free lightweight architecture specifically for semantic segmentation, named Adaptive Frequency Transformer (AFFormer). AFFormer adopts a parallel architecture to leverage prototype representations as specific learnable local descriptions which replaces the decoder and preserves the rich image semantics on high-resolution features. Although removing the decoder compresses most of the computation, the accuracy of the parallel structure is still limited by low computational resources. Therefore, we employ heterogeneous operators (CNN and vision Transformer) for pixel embedding and prototype representations to further save computational costs. Moreover, it is very difficult to linearize the complexity of the vision Transformer from the perspective of spatial domain. Due to the fact that semantic segmentation is very sensitive to frequency information, we construct a lightweight prototype learning block with adaptive frequency filter of complexity O(n) to replace standard self attention with O(n^2). Extensive experiments on widely adopted datasets demonstrate that AFFormer achieves superior accuracy while retaining only 3M parameters. On the ADE20K dataset, AFFormer achieves 41.8 mIoU and 4.6 GFLOPs, which is 4.4 mIoU higher than Segformer, with 45% less GFLOPs. On the Cityscapes dataset, AFFormer achieves 78.7 mIoU and 34.4 GFLOPs, which is 2.5 mIoU higher than Segformer with 72.5% less GFLOPs. Code is available at https://github.com/dongbo811/AFFormer.",
    "full_text": "Head-Free Lightweight Semantic Segmentation with Linear Transformer\nBo Dong*, Pichao Wang†, Fan Wang\nAlibaba Group\n{bo.dong.cst, pichaowang}@gmail.com; fan.w@alibaba-inc.com\nAbstract\nExisting semantic segmentation works have been mainly fo-\ncused on designing effective decoders; however, the com-\nputational load introduced by the overall structure has long\nbeen ignored, which hinders their applications on resource-\nconstrained hardwares. In this paper, we propose a head-free\nlightweight architecture specifically for semantic segmenta-\ntion, named Adaptive Frequency Transformer (AFFormer).\nAFFormer adopts a parallel architecture to leverage proto-\ntype representations as specific learnable local descriptions\nwhich replaces the decoder and preserves the rich image\nsemantics on high-resolution features. Although removing\nthe decoder compresses most of the computation, the accu-\nracy of the parallel structure is still limited by low com-\nputational resources. Therefore, we employ heterogeneous\noperators (CNN and Vision Transformer) for pixel embed-\nding and prototype representations to further save compu-\ntational costs. Moreover, it is very difficult to linearize the\ncomplexity of the vision Transformer from the perspective\nof spatial domain. Due to the fact that semantic segmenta-\ntion is very sensitive to frequency information, we construct a\nlightweight prototype learning block with adaptive frequency\nfilter of complexity O(n) to replace standard self atten-\ntion with O(n2). Extensive experiments on widely adopted\ndatasets demonstrate that AFFormer achieves superior accu-\nracy while retaining only 3M parameters. On the ADE20K\ndataset, AFFormer achieves 41.8 mIoU and 4.6 GFLOPs,\nwhich is 4.4 mIoU higher than Segformer, with 45% less\nGFLOPs. On the Cityscapes dataset, AFFormer achieves 78.7\nmIoU and 34.4 GFLOPs, which is 2.5 mIoU higher than\nSegformer with 72.5% less GFLOPs. Code is available at\nhttps://github.com/dongbo811/AFFormer.\nIntroduction\nSemantic segmentation aims to partition an image into sub-\nregions (collections of pixels) and is defined as a pixel-\nlevel classification task (Long, Shelhamer, and Darrell 2015;\nChen et al. 2018; Strudel et al. 2021) since Fully Con-\nvolutional Networks (FCN) (Long, Shelhamer, and Darrell\n2015). It has two unique characteristics compared to image\n*Work done during an internship at Alibaba Group.\n†Corresponding author; work done at Alibaba Group, and now\naffiliated with Amazon Prime Video.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n0 256 512 76810241280153617922048\n0\n50\n100\n150\n200\n250\n300\n350\n400FLOPs\nInput Scale\n PSPNet\n DeepLabV3+\n SegFormer\n AFFormer\n2.1x 1.8x\n2.1x\n2.5x\n3.0x\n3.6x\n4.4x\n5.3x\n11.3x\n5.6x\n81.0\n78.0\n75.0\n44.0\n40.0\n36.0\n+4.4 mIoU\n…….+2.5 mIoU\n…….\nSegFormer AFFormerInput Scale\nFLOPs\nADE20K Cityscapes\nFigure 1: Left: Computational complexity under differ-\nent input scales. Segformer (Xie et al. 2021) significantly\nreduces the computational complexity compared to tra-\nditional methods, such as PSPNet (Zhao et al. 2017)\nand DeepLabV3+ (Chen et al. 2018) which have mo-\nbilenetV2 (Sandler et al. 2018) as backbone. However, Seg-\nformer still has a huge computational burden for higher\nresolutions. Right: AFFormer achieves better accuracy on\nADE20K and Cityscapes datasets with significantly lower\nFLOPs.\nclassification: pixel-wise dense prediction and multi-class\nrepresentation, which is usually built upon high-resolution\nfeatures and requires a global inductive capability of im-\nage semantics, respectively. Previous semantic segmenta-\ntion methods (Zhao et al. 2017; Chen et al. 2018; Strudel\net al. 2021; Xie et al. 2021; Cheng, Schwing, and Kirillov\n2021; Yuan et al. 2021b) focus on using the classification\nnetwork as backbone to extract multi-scale features, and de-\nsigning a complicated decoder head to establish the rela-\ntionship between multi-scale features. However, these im-\nprovements come at the expense of large model size and\nhigh computational cost. For instance, the well-known PSP-\nNet (Zhao et al. 2017) using light-weight MobilenetV2 (San-\ndler et al. 2018) as backbone contains 13.7M parameters and\n52.2 GFLOPs with the input scale of512×512. The widely-\nused DeepLabV3+ (Chen et al. 2018) with the same back-\nbone requires 15.4M parameters and 25.8 GFLOPs. The in-\nherent design manner limits the development of this field\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n516\nand hinders many real-world applications. Thus, we raise the\nfollowing question: can semantic segmentation be as simple\nas image classification?\nRecently vision Transformers (ViTs) (Liu et al. 2021;\nXie et al. 2021; Lee et al. 2022) have shown great poten-\ntial in semantic segmentation, however, they face the chal-\nlenges of balancing performance and memory usage when\ndeployed on ultra-low computing power devices. Standard\nTransformers has computational complexity ofO(n2) in the\nspatial domain, where n is the input resolution. Existing\nmethods alleviate this situation by reducing the number of\ntokens (Wang et al. 2021; Ren et al. 2022) or sliding win-\ndows (Liu et al. 2021; Yuan et al. 2021a), but they introduce\nlimited reduction on computational complexity and even\ncompromise global or local semantics for the segmentation\ntask. Meanwhile, semantic segmentation as a fundamental\nresearch field, has extensive application scenarios and needs\nto process images with various resolutions. As shown in\nFigure 1, although the well-known efficient Segformer (Xie\net al. 2021) achieves a great breakthrough compared to PSP-\nNet and DeepLabV3+, it still faces a huge computational\nburden for higher resolutions. At the scale of 512 × 512,\nalthough Segformer is very light compared to PSPNet and\nDeepLabV3+, it is almost twice as expensive as ours (8.4\nGFLOPs vs 4.6 GFLOPs); at the scale of 2048 ×2048, even\n5x GFLOPs is required (384.3 GFLOPs vs 73.2 GFLOPs).\nThus, we raise another question: can we design an efficient\nand lightweight Transformer network for semantic segmen-\ntation in ultra-low computational scenarios?\nThe answers to above two questions are affirmative. To\nthis end, we propose a head-free lightweight semantic seg-\nmentation specific architecture, named Adaptive Frequency\nTransformer (AFFormer). Inspired by the properties that\nViT maintains a single high-resolution feature map to keep\ndetails (Dosovitskiy et al. 2021) and the pyramid structure\nreduces the resolution to explore semantics and reduce com-\nputational cost (He et al. 2016; Wang et al. 2021), AF-\nFormer adopts a parallel architecture to leverage the proto-\ntype representations as specific learnable local descriptions\nwhich replace the decoder and preserves the rich image se-\nmantics on high-resolution features. The parallel structure\ncompresses the majority of the computation by removing\nthe decoder, but it is still not enough for ultra-low compu-\ntational resources. Moreover, we employ heterogeneous op-\nerators for pixel embedding features and local description\nfeatures to save more computational costs. A Transformer-\nbased module named prototype learning (PL) is used to\nlearn the prototype representations, while a convolution-\nbased module called pixel descriptor (PD) takes pixel em-\nbedding features and the learned prototype representations\nas inputs, transforming them back into the full pixel embed-\nding space to preserve high-resolution semantics.\nHowever, it is still very difficult to linearize the complex-\nity of the vision Transformer from the perspective of spatial\ndomain. Inspired by the effects of frequency on classifica-\ntion tasks (Rao et al. 2021; Wang et al. 2020), we find that\nsemantic segmentation is also very sensitive to frequency\ninformation. Thus, we construct a lightweight adaptive fre-\nquency filter of complexityO(n) as prototype learning to re-\nplace the standard self attention withO(n2). The core of this\nmodule is composed of frequency similarity kernel, dynamic\nlow-pass and high-pass filters, which capture frequency in-\nformation that is beneficial to semantic segmentation from\nthe perspectives of emphasizing important frequency com-\nponents and dynamically filtering frequency, respectively.\nFinally, the computational cost is further reduced by sharing\nweights in high and low frequency extraction and enhance-\nment modules. We also embed a simplified depthwise con-\nvolutional layer in the feed-forward network (FFN) layer to\nenhance the fusion effect, reducing the size of the two matrix\ntransformations.\nWith the help of parallel heterogeneous architecture and\nadaptive frequency filter, we use only one convolutional\nlayer as classification layer (CLS) for single-scale feature,\nachieving the best performance and making semantic seg-\nmentation as simple as image classification. We demonstrate\nthe advantages of the proposed AFFormer on three widely-\nused datasets: ADE20K, Cityscapes and COCO-stuff. With\nonly 3M parameters, AFFormer significantly outperforms\nthe state-of-the-art lightweight methods. On ADE20K, AF-\nFormer achieves 41.8 mIoU with 4.6 GFLOPs, outperform-\ning Segformer by 4.4 mIoU, while reducing GFLOPs by\n45%. On Cityscapes, AFFormer achieves 78.7 mIoU and\n34.4 GFLOPs, which is 2.5 mIoU higher than Segformer,\nwith 72.5% less GFLOPs. Extensive experimental results\ndemonstrate that it is possible to apply our model in compu-\ntationally constrained scenarios, which still maintaining the\nhigh performance and robustness across different datasets.\nRelated Work\nSemantic Segmentation\nSemantic segmentation is regarded as a pixel classifica-\ntion task (Xu et al. 2017; Xie et al. 2021). In the last two\nyears, new paradigms based on visual Transformers have\nemerged, which enable mask classification via queries or\ndynamic kernels (Zhang et al. 2021; Li et al. 2022; Cheng\net al. 2022). For instance, Maskformer (Cheng, Schwing,\nand Kirillov 2021) learns an object query and converts it\ninto an embedding of masks. Mask2former (Cheng et al.\n2022) enhances the query learning with a powerful multi-\nscale masked Transformer (Zhu et al. 2021). K-Net (Zhang\net al. 2021) adopts dynamic kernels for masks generation.\nMaskDINO (Li et al. 2022) brings object detection to se-\nmantic segmentation, further improving query capabilities.\nHowever, all above methods are not suitable for low comput-\ning power scene due to the high computational cost of learn-\ning efficient queries and dynamic kernels. We argue that the\nessence of these paradigms is to update pixel semantics by\nreplacing the whole with individual representations. There-\nfore, we leverage pixel embeddings as a specific learnable\nlocal description that extracts image and pixel semantics and\nallows semantic interaction.\nEfficient Vision Transformers\nThe lightweight solution of vision Transformer mainly fo-\ncuses on the optimization of self attention, including follow-\ning ways: reducing the token length (Wang et al. 2021; Xie\n517\nDC-FFN\nAFF\nAdd & Norm\nRestoring\n(i)\nClustering\n(iii) Pixel Descriptor (PD)\n(ii) Prototype Learning (  )    \nPositional \nEncodings\nAdd & Norm\n  \n \n  \n \n  \n  \nPL\nClusteringPD\n          \nCLS\nStem\nPixel Classification\nPL\nClusteringPD\n          \nPL\nClusteringPD\n          \nPL\nClusteringPD\n          \n…\n…\n…\n…  \n \n \n                    \nSharing\nAFF\nDC-FFN\nStem\nAdaptive Frequency Filter\nDepthwise\nFeed-Forward Network\nTwo Convolutional Layers\nCLS\nA Convolutional Layer\n     \nFigure 2: An Overview of Adaptive Frequency Transformer (AFFormer). We first displays the overall structure of parallel\nheterogeneous network. Specifically, the feature F after patch embedding is first clustered to obtain the prototype feature G,\nso as to construct a parallel network structure, which includes two heterogeneous operators. A Transformer-based module\nas prototype learning to capture favorable frequency components in G, resulting prototype representation G′. Finally G′ is\nrestored by a CNN-based pixel descriptor, resulting F′ for the next stage.\net al. 2021) and using local windows (Liu et al. 2021; Yuan\net al. 2021a). PVT (Wang et al. 2021) performs spatial com-\npression on keys and values through spatial reduction, and\nPVTv2 (Wang et al. 2022) further replaces the spatial re-\nduction by pooling operation, but many details are lost in\nthis way. Swin (Liu et al. 2021; Yuan et al. 2021a) signifi-\ncantly reduce the length of the token by restricting self at-\ntention to local windows, while these against the global na-\nture of Transformer and restrict the global receptive field. At\nthe same time, many lightweight designs (Chen et al. 2022;\nMehta and Rastegari 2022) introduce Transformers in Mo-\nbileNet to obtain more global semantics, but these meth-\nods still suffer from the square-level computational com-\nplexity of conventional Transformers. Mobile-Former (Chen\net al. 2022) combines the parallel design of MobileNet (San-\ndler et al. 2018) and Transformer (Dosovitskiy et al. 2021),\nwhich can achieve bidirectional fusion performance of local\nand global features far beyond lightweight networks such as\nMobileNetV3. However, it only uses a very small number\nof tokens, which is not conducive to semantic segmentation\ntasks.\nMethod\nIn this section, we introduce the lightweight parallel hetero-\ngeneous network for semantic segmentation. The basic in-\nformation is first provivided on the replacement of semantic\ndecoder by parallel heterogeneous network. Then, we intro-\nduce the modeling of pixel descriptions and semantic fre-\nquencies. Finally, the specific details and the computational\noverhead of parallel architectures are discussed.\nParallel Heterogeneous Architecture\nThe semantic decoder propagates the image semantics ob-\ntained by the encoder to each pixel and restores the lost de-\ntails in downsampling. A straightforward alternative is to\nextract image semantics in high resolution features, but it\nintroduces a huge amount of computation, especially for vi-\nsion Transformers. In contrast, we propose a novel strategy\nto describe pixel semantic information with prototype se-\nmantics. For each stage, given a feature F ∈ RH×W×C,\nwe first initial a grid G ∈ Rh×w×C as a prototype of the\nimage, where each point in G acts as a local cluster center,\nand the initial state simply contains information about the\nsurrounding area. Here we use a 1 × C vector to represent\nthe local semantic information of each point. For each spe-\ncific pixel, because the semantics of the surrounding pixels\nare not consistent, there are overlap semantics between each\ncluster centers. The cluster centers are weighted initialized\nin its corresponding area α2, and the initialization of each\ncluster center is expressed as:\nG(s) =\nnX\ni=0\nwixi (1)\nwhere n = α × α, wi denotes the weight of xi, and α is\nset to 3. Our purpose is to update each cluster center s in\nthe grid G instead of updating the feature F directly. As\nh × w ≪ H × W, it greatly simplifies the computation.\nHere, we use a Transformer-based module as prototype\nlearning to update each cluster center, which containsL lay-\ners in total, and the updated center is denoted as G′(s). For\neach updated cluster center, we recover it by a pixel descrip-\ntor. Let F′\ni denote the recovered feature, which contains not\nonly the rich pixel semantics from F, but also the prototype\nsemantics collected by the cluster centers G′(s). Since the\ncluster centers aggregate the semantics of surrounding pix-\nels, resulting in the loss of local details, PD first models local\ndetails in F with pixel semantics. Specifically,F is projected\nto a low-dimensional space, establishing local relationships\n518\n200 175 150 125 100 75 50 25\n5\n10\n15\n20\n25\n30\n35\n40mIoU\nFilter Radius\nFiltered Image\nFigure 3: The effect of different frequency components on\nsemantic segmentation. We use the cut-edge method Seg-\nformer (Xie et al. 2021) to evaluate the impact of frequency\ncomponents on semantic segmentation on the widely used\nADE20K dataset (Zhou et al. 2017). The image is trans-\nformed into the frequency domain by a fast Fourier trans-\nform (Heideman, Johnson, and Burrus 1984), and high-\nfrequency information is filtered out using a low-pass op-\nerator with a radius. Removing high-frequency components\nat different levels results the prediction performance drops\nsignificantly.\nbetween pixels such that each local patch keeps a distinct\nboundary. Then G′(s) is embedded into F to restore to the\noriginal space feature F′ through bilinear interpolation. Fi-\nnally, they are integrated through a linear projection layer.\nPrototype Learning by Adaptive Frequency Filter\nMotivation Semantic segmentation is an extremely com-\nplex pixel-level classification task that is prone to category\nconfusion. The frequency representation can be used as a\nnew paradigm of learning difference between categories,\nwhich can excavate the information ignored by human vi-\nsion (Zhong et al. 2022; Qian et al. 2020). As shown in\nFigure 3, humans are robust to frequency information re-\nmoval unless the vast majority of frequency components are\nfiltered out. However, the model is extremely sensitive to\nfrequency information removal, and even removing a small\namount would result in significant performance degrada-\ntion. It shows that for the model, mining more frequency\ninformation can enhance the difference between categories\nand make the boundary between each category more clear,\nthereby improving the effect of semantic segmentation.\nSince feature F contains rich frequency features, each\ncluster center in the grid G also collects these frequency in-\nformation. Motivated by the above analysis, extracting more\nbeneficial frequencies in grid G helps to discriminate the\nattributes of each cluster. To extract different frequency fea-\ntures, the straightforward way is to transform the spatial do-\nmain features into spectral features through Fourier trans-\nform, and use a simple mask filter in the frequency domain\nto enhance or attenuate the intensity of each frequency com-\nponent of the spectrum. Then the extracted frequency fea-\nH Groups\nDynamic Low-pass Filters\nN Groups\n…\n          \n          \n…\nDynamic High-pass Filters\nWeight \nSharing\nFrequency \nAggregation\n \nFrequency Similarity Kernel\n       \nM Groups\nAggregation\nConvolution\nUpsample\nFigure 4: Structure of the adaptive frequency filter in pro-\ntotype learning. The prototype as learnable local descrip-\ntion utilizes frequency component similarity kernel to en-\nhance different components while combining efficient and\ndynamic low-pass and high-pass filters to capture more fre-\nquency information.\ntures are converted to the spatial domain by inverse Fourier\ntransform. However, Fourier transform and inverse trans-\nform bring in additional computational expenses, and such\noperators are not supported on many hardwares. Thus, we\ndesign an adaptive frequency filter block based on the vanilla\nvision Transformer from the perspective of spectral correla-\ntion to capture important high frequency and low frequency\nfeatures directly in the spatial domain. The core components\nare shown in Figure 4 and the formula F(·) is defined as:\nF(X) = ||Dfc\nh (X)||H\n|\n{z }\ncorr.\n+ ||Dlf\nm(X)||M + ||Dhf\nn (X)||N|\n{z }\ndynamic filters\n,\n(2)\nwhere Dfc\nh , Dlf\nm(X) and Dhf\nn (X) denote the frequency\nsimilarity kernel with H groups to achieve frequency com-\nponent correlation enhancement, dynamical low-pass filters\nwith M groups and dynamical high-pass filters with N\ngroups, respectively. || · ||denotes concatenation. It is worth\nnoting that these operators adopt a parallel structure to fur-\nther reduce the computational cost by sharing weights.\nFrequency Similarity Kernel (FSK) Different frequency\ncomponents distribute over in G, and our purpose is to se-\nlect and enhance the important components that helps se-\nmantic parsing. To this end, we design a frequency similar-\nity kernel module. Generally, this module is implemented\nby the vision Transformer. Given a feature X ∈ R(hw)×C,\nwith relative position encoding on G through a convolution\nlayer (Wu et al. 2021). We first use a fixed-size similarity\nkernel A ∈ RC/H×C/H to represent the correspondence be-\ntween different frequency components, and select the impor-\ntant frequency components by querying the similarity ker-\nnel. We treat it as a function transfer that computes the keys\nK and values V of frequency components through a linear\n519\nlayer, and normalizes the keys across frequency components\nby a Softmax operation. Each component integrates a simi-\nlarity kernel Ai,j, which is computed as:\nAi,j = ekiv⊤\nj /\nnX\nj=1\neki , (3)\nwhere ki represents the i-th frequency component in K,\nvj represents the j-th frequency component in V . We also\ntransform the input X into the query Q through a linear\nlayer, and obtain the component-enhanced output through\ninteractions on the fixed-size similarity kernel.\nDynamic Low-Pass Filters (DLF) Low-frequency com-\nponents occupy most of the energy in the absolute image and\nrepresent most of the semantic information. A low-pass fil-\nter allows signals below the cutoff frequency to pass, while\nsignals above the cutoff frequency are obstructed. Thus, we\nemploy typical average pooling as a low-pass filter. How-\never, the cutoff frequencies of different images are different.\nTo this end, we control different kernels and strides in multi-\ngroups to generate dynamic low-pass filters. Form-th group,\nwe have:\nDlf\nm(vm)) = B(Γs×s(vm)), (4)\nwhere B(·) represents bilinear interpolation and Γs×s de-\nnotes the adaptive average pooling with the output size of\ns × s.\nDynamic High-Pass Filters (DHF) High-frequency in-\nformation is crucial to preserve details in segmentation. As\na typical high-pass operator, convolution can filter out irrel-\nevant low-frequency redundant components to retain favor-\nable high-frequency components. The high-frequency com-\nponents determine the image quality and the cutoff fre-\nquency of the high-pass for each image is different. Thus,\nwe divide the valueV into N groups, resulting vn. For each\ngroup, we use a convolution layer with different kernels to\nsimulate the cutoff frequencies in different high-pass filters.\nFor the n-th group, we have:\nDhf\nn (vn)) = Λk×k(vn), (5)\nwhere Λk×k denotes the depthwise convolution layer with\nkernel size of k ×k. In addition, we use the Hadamard prod-\nuct of query and high-frequency features to suppress high\nfrequencies inside objects, which are noise for segmentation.\nFFN helps to fuse the captured frequency information, but\nowns a large amount of calculation, which is often ignored\nin lightweight designs. Here we reduce the dimension of the\nhidden layer by introducing a convolution layer to make up\nfor the missing capability due to dimension compression.\nDiscuss For the frequency similarity kernel, the computa-\ntional complexity is O(hwC2). The complexity of each dy-\nnamic high-pass filter is O(hwCk2), which is much smaller\nthan that of frequency similarity kernel. Since the dynamic\nlow-pass filter is implemented by adaptive mean pooling of\neach group, its complexity is aboutO(hwC). Therefore, the\ncomplexity of a module is linear with the resolution, which\nis advantageous for high resolution in semantic segmenta-\ntion.\nModel #Param. FLOPs mIoU\nFCN-8s 9.8M 39.6G 19.7\nPSPNet (MV2) 13.7M 52.2G 29.6\nDeepLabV3+ (MV2) 15.4M 25.8G 38.1\nDeepLabV3+ (EN) 17.1M 26.9G 36.2\nDeepLabV3+ (SV2) 16.9M 15.3G 37.6\nLite-ASPP 2.9M 4.4G 36.6\nR-ASPP 2.2M 2.8G 32.0\nLR-ASPP 3.2M 2.0G 33.1\nHRNet-W18-Small 4.0M 10.2G 33.4\nHR-NAS-A 2.5M 1.4G 33.2\nHR-NAS-B 3.9M 2.2G 34.9\nPVT-v2-B0 7.6M 25.0G 37.2\nTopFormer 5.1M 1.8G 37.8\nEdgeViT-XXS 7.9M 24.4G 39.7\nSegformer (LVT) 3.9M 10.6G 39.3\nSwin-tiny 31.9M 46G 41.5\nXcit-T12/16 8.4M 21.5G 38.1\nViT 10.2M 24.6G 37.4\nPVT-tiny 17.0M 33G 36.6\nSegformer 3.8M 8.4G 37.4\nAFFormer-tiny 1.6M(-58%) 2.8G(-67%) 38.7(+1.3)\nAFFormer-small 2.3M(-41%) 3.6G(-61%) 40.2(+2.8)\nAFFormer-base 3.0M(-21%) 4.6G(-45%) 41.8(+4.4)\nTable 1: Comparison to state of the art methods on\nADE20K with resolution at 512 × 512. Here we use\nthe Segformer as the baseline and report the per-\ncentage growth. MV2=MobileNetV2, EN=EfficientNet,\nSV2=ShuffleNetV2.\nExperiments\nImplementation Details\nWe validate the proposed AFFormer on three publicly\ndatasets: ADE20K (Zhou et al. 2017), Cityscapes (Cordts\net al. 2016) and COCO-stuff (Caesar, Uijlings, and Fer-\nrari 2018). We implement our AFFormer with the Py-\nTorch framework base on MMSegmentation toolbox (Open-\nMMLab 2020). Follow previous works (Cheng, Schwing,\nand Kirillov 2021), we use ImageNet-1k to pretrain our\nmodel. During semantic segmentation training, we employ\nthe widely used AdamW optimizer for all datasets to update\nthe model parameters. For fair comparisons, our training pa-\nrameters mainly follow the previous work (Xie et al. 2021).\nFor the ADE20K and Cityscapes datasets, we adopt the de-\nfault training iterations 160K in Segformer, where mini-\nbatchsize is set to 16 and 8, respectively. For the COCO-stuff\ndataset, we set the training iterations to 80K and the mini-\nbatch to 16. In addition, we implement data augmentation\nduring training by random horizontal flipping, random resiz-\ning with a ratio of 0.5-2.0, and random cropping. We eval-\nuate the results with mean Intersection over Union (mIoU)\nmetric.\nComparisons with Existing Works\nResults on ADE20K Dataset. We compare our AF-\nFormer with top-ranking semantic segmentation methods.\nFollowing the inference settings in (Xie et al. 2021), we test\nFLOPs at 512 × 512 resolution and show the single scale\n520\nModel #Param. FLOPs mIoU\nFCN 9.8M 317G 61.5\nPSPNet (MV2) 13.7M 423G 70.2\nDeepLabV3+ (MV2) 15.4M 555G 75.2\nSwiftNetRN 11.8M 104G 75.5\nEncNet 55.1M 1748G 76.9\nSegformer 3.8M 125G 76.2\nAFFormer-tiny 1.6M(-58%) 23.0G(-82%) 76.5(+0.3)\nAFFormer-small 2.3M(-41%) 26.2G(-79%) 77.6(+1.4)\nAFFormer-base 3.0M(-21%) 34.4G(-73%) 78.7(+2.5)\nTable 2: Comparison to state of the art methods on\nCityscapes val set. The FLOPs are test on the resolution of\n1024 × 2048. Meanwhile, we also report the percentage in-\ncrease compared to Segformer.\nModel size FLOPs mIoU\nSegformer 512 × 1024 17.7G 71.9\nAFFormer-base 512 × 1024 8.6G(-51.4%) 73.5(+1.6)\nSegformer 640 × 1280 31.5G 73.7\nAFFormer-base 640 × 1280 13.4G(-57.5%) 75.6(+1.9)\nSegformer 768 × 1536 51.7G 75.3\nAFFormer-base 768 × 1536 19.4G(-62.5%) 76.5(+1.2)\nSegformer 1024 × 2048 125G 76.2\nAFFormer-base 1024 × 2048 34.4G(-72.5%) 78.7(+2.5)\nTable 3: Speed-accuracy tradeoffs at different scales on\nCityscapes.\nModel #Param. FLOPs mIoU\nPSPNet (MV2) 13.7M 52.9G 30.1\nDeepLabV3+ (MV2) 15.4M 25.9G 29.9\nDeepLabV3+ (EN) 17.1M 27.1G 31.5\nLR-ASPP (MV3) – 2.37G 25.2\nAFFormer-base 3.0M 4.6G 35.1\nTable 4: Comparison to state of the art methods on COCO-\nstuff. We use a single-scale results at the input resolution of\n512 × 512. MV3=MobileNetV3.\nresults in Table 1. Our model AFFormer-base improves by\n5.2 mIoU under the same computing power consumption as\nLite-ASPP, reaching 41.8 mIoU. Meanwhile, by reducing\nthe number of layers and channels, we obtain AFFormer-tiny\nand AFFormer-small versions to adapt to different comput-\ning power scenarios. For the lightweight and efficient Seg-\nformer (8.4 GFLOPs),our base version (4.6 GFLOPs) also\ngain 4.4 mIoU using half the computing power and the tiny\nversion (2.4 GFLOPs) with only 33% the computing power\nimproving 1.3 mIoU. Only 1.8 GFLOPs are needed for the\nlighter topformer, but our base version has 2.1M less param-\neters (5.1M vs 3M) with 4.0 higher mIoU.\nResults on Cityscapes Dataset. Table 2 shows the results\nof our model and the cutting-edge methods on Cityscapes.\nAlthough the Segformer is efficient enough, due to its\nsquare-level complexity, we only use 30% of the compu-\ntational cost to reach 78.7 mIoU, which is 2.5 mIoU im-\nprovement with a 70% reduction in FLOPs. Meanwhile, we\nSetting #Param. FLOPs mIoU\nw/o PD 2.78G 2.98G 39.2\nw/o PL 0.42G 1.65G 19.5\nParallel 3.0G 4.6G 41.8\nTable 5: Ablation studies on the parallel structure.\nSetting #Param. FLOPs mIoU\nAll PD 0.6M 1.85G 27.4\nAll PL 3.6M 7.0G 41.6\nHeterogeneous 3.0M 4.6G 41.8\nTable 6: Ablation studies on heterogeneous architecture.\nreport the results at different high resolutions in Table 3. At\nthe short side of {512, 640, 768, 1024}, the computational\ncost of our model is 51.4%, 57.5%, 62.5% and 72.5% of\nthat of Segformer, respectively. Meanwhile, the mIoU are\nimproved by 1.6, 1.9, 1.2 and 2.5, respectively. The higher\nthe input resolution, the more advantageous of our model in\nboth computational cost and accuracy.\nResults on COCO-stuff Dataset. COCO-stuff dataset\ncontains a large number of difficult samples that collected\nin COCO. As show in Table 4, although complex decoders\n(e.g., PSPNet, DeepLabV3+) can achieve better results than\nLR-ASPP (MV3), they bring a lot of computational cost.\nOur model achieves an accuracy of 35.1 mIoU while only\ntaking 4.5 GFLOPs, achieving the best trade-off.\nAblation Studies\nAll the ablation studies are conducted on ADE20K dataset\nwith AFFormer-base unless otherwise specified.\nRationalization of Parallel Structures. Parallel architec-\nture is the key to removing the decoder head and ensuring\naccuracy and efficiency. We first adjust the proposed struc-\nture to a naive pyramid architecture (denoted as “w/o PD”)\nand a ViT architecture (denoted as “w/o PL”) to illustrate the\nadvantages of the parallel architecture. Specifically, the “w/o\nPD” means removing PD module and keeping only PL mod-\nule, while the “w/o PL” does the opposite. As shown in Ta-\nble 5, the setting “w/o PD” reduces 2.6 mIoU due to the lack\nof high-resolution pixel semantic information. The “w/o PL”\nstructure without the pyramid structure has a significant re-\nduction in accuracy due to few parameters and lack of rich\nimage semantic information. It also demonstrates that our\nparallel architecture can effectively combine the advantages\nof both architectures.\nAdvantages of Heterogeneous Structure. The purpose\nof the heterogeneous approach is to further reduce the com-\nputational overhead. The PL module is adopted to learn\nthe prototype representation in the clustered features, and\nthen use PD to combine the original features for restoration,\nwhich avoids direct calculation on the high-resolution origi-\nnal features and reduce the computational cost. It can be seen\nfrom Table 6 that when the parallel branch is adjusted to the\npixel description module (denote as “All PD”), which means\n521\nthat the prototype representation is learned by PD module.\nThe model size is only 0.6M, and the FLOPs are reduced by\n2.5G, but the accuracy is reduced by 14.3 mIoU. This is due\nto the PD lacks the ability to learn great prototype represen-\ntations. In contrast, after we replace the PD module with the\nPL module (denote as “All PL”), the FLOPs are increased\nby 2.4G, but there is almost no difference in accuracy. We\nbelieve that the PD module is actually only a simple way to\nrestore the learned prototype, and the relatively complex PL\nmodule saturates the model capacity.\nAdvantages of Adaptive Frequency Filter. We use two\ndatasets with large differences, including ADE20K and\nCityscapes, to explore the core components in adaptive fre-\nquency filter module. The main reason is that the upper limit\nof the ADE20K dataset is only 40 mIoU, while the upper\nlimit of the Cityscapes is 80 mIoU. The two datasets have\ndifferent degrees of sensitivity to different frequencies. We\nreport the benefits of each internal component in the Table 7.\nWe find that DHF alone outperforms DLF, especially on the\nCityscapes dataset by 2.6 mIoU, while FSK is significantly\nhigher than DLF and DHF on ADE20K. This shows that\nADE20K may be more inclined to an intermediate state be-\ntween high frequency and low frequency, while Cityscapes\nneeds more high frequency information. The combined ex-\nperiments show that the combination of the advantages of\neach component can stably improve the results of ADE20K\nand Cityscapes.\nFrequency Statistics Visualization. We first count the\ncharacteristic frequency distribution of different stages, as\nshown in Figure 5. It can be found that the curves of G2\nand F2 almost overlap, indicating that the frequencies after\nclustering are very similar to those in the original features.\nThe same goes for G3 and F3. Whereas, the learned proto-\ntype representation after frequency adaptive filtering signifi-\ncantly improves the contained frequency information. After\nPD restoration, different frequency components can be em-\nphasized in different stages. As shwon in Figure 6, we also\nanalyze the frequency effects of the core components in the\nAFF module. As expected, DLF and DHF show strong low-\npass and high-pass capabilities, respectively, as FSK does.\nAt the same time, we also found that the important frequency\ncomponents screened and enhanced by FSK are mainly con-\ncentrated in the high frequency part, but the frequency signal\nis more saturated than that of DHF. This also shows that the\nhigh-frequency component part is particularly important in\nthe semantic segmentation task, because it emphasizes more\non the boundary details and texture differences between ob-\njects. Meanwhile, according to the analysis in Table 7 (the\neffects of ADE20K and Cityscapes have been steadily im-\nproved), each core component has its own advantages, and\nthe AFF module shows strong robustness in various types\nand complex scenes.\nSpeed and Memory Costs. Meanwhile, we report the\nspeed on the Cityscapes in Table 8. We can find that the pro-\nposed model improves by 10 FPS and performs much better\nthan Segformer on such high-resolution Cityscapes images.\nSetting #Param. FLOPs ADE20K Cityscapes\nDLF 2.4M 3.6G 38.7 75.7\nDHF 2.6M 3.9G 39.3 78.3\nFSK 2.9M 4.2G 40.5 75.3\nDLF + DHF 2.7M 3.9G 41.1 77.8\nDLF + FSK 2.8M 4.2G 40.0 76.2\nDHF + FSK 2.9M 4.3G 41.2 77.3\nWhole 3.0M 4.6G 41.8 78.7\nTable 7: Ablation studies on frequency aware statistics.\nModel FPS mIoU\nSegformer 12 76.2\nAFFormer 22 78.7\nTable 8: The FPS is tested on a V100 NVIDIA GPU with a\nbatch size of 1 on the resolution of 1024x2048.\n  \n  \n  \n \n  \n \n  \n  \n  \n \n  \n \n Log amplitude\nFrequency\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n Log amplitude\n0 0.2 0.4 0.6 0.8 1.0 \nFrequency\n0 0.2 0.4 0.6 0.8 1.0 \nFrequency\n2.0\n3.0\n4.0\n5.0\n6.0\n7.0\n8.0\n Log amplitude\n1.0\n0.0\nFigure 5: Frequency analysis of stage-2 (left) and stage-3\n(right).\nDHF\nInput\nDLF\nFSK\nFrequency\n Log amplitude\nInput DHF\nDLF FSK\n-6.0\n-4.0\n-2.0\n0.0\n2.0\n4.0\n-6.0\n0 0.2 0.4 0.6 0.8 1.0 \nFigure 6: Frequency analysis of the core components in PL\nmodule.\nConclusion\nIn this paper, we propose a head-free lightweight semantic\nsegmentation specific architecture. The core is to learn the\nlocal representation of the clustered prototypes from the fre-\nquency perspective, instead of directly learning all the pixel\nembedding features. It removes the complex decoder while\nhaving linear complexity Transformer and realizes seman-\ntic segmentation as simple as regular classification. Experi-\nments demonstrate that the AFFormer owns powerful accu-\nracy and great stability at low computational cost.\n522\nAcknowledgements\nThis work was supported by Alibaba Group through Alibaba\nResearch Intern Program.\nReferences\nCaesar, H.; Uijlings, J.; and Ferrari, V . 2018. Coco-stuff:\nThing and stuff classes in context. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 1209–1218.\nChen, L.-C.; Zhu, Y .; Papandreou, G.; Schroff, F.; and\nAdam, H. 2018. Encoder-decoder with atrous separable con-\nvolution for semantic image segmentation. In Proceedings\nof the European conference on computer vision (ECCV) ,\n801–818.\nChen, Y .; Dai, X.; Chen, D.; Liu, M.; Dong, X.; Yuan, L.;\nand Liu, Z. 2022. Mobile-former: Bridging mobilenet and\ntransformer. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 5270–5279.\nCheng, B.; Misra, I.; Schwing, A. G.; Kirillov, A.; and Gird-\nhar, R. 2022. Masked-attention mask transformer for univer-\nsal image segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n1290–1299.\nCheng, B.; Schwing, A.; and Kirillov, A. 2021. Per-pixel\nclassification is not all you need for semantic segmenta-\ntion. Advances in Neural Information Processing Systems,\n34: 17864–17875.\nCordts, M.; Omran, M.; Ramos, S.; Rehfeld, T.; Enzweiler,\nM.; Benenson, R.; Franke, U.; Roth, S.; and Schiele, B.\n2016. The cityscapes dataset for semantic urban scene un-\nderstanding. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 3213–3223.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. ICLR.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770–778.\nHeideman, M.; Johnson, D.; and Burrus, C. 1984. Gauss\nand the history of the fast Fourier transform. IEEE ASSP\nMagazine, 1(4): 14–21.\nLee, Y .; Kim, J.; Willette, J.; and Hwang, S. J. 2022. MPViT:\nMulti-path vision transformer for dense prediction. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 7287–7296.\nLi, F.; Zhang, H.; Liu, S.; Zhang, L.; Ni, L. M.; Shum, H.-Y .;\net al. 2022. Mask DINO: Towards A Unified Transformer-\nbased Framework for Object Detection and Segmentation.\narXiv preprint arXiv:2206.02777.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\nLong, J.; Shelhamer, E.; and Darrell, T. 2015. Fully convo-\nlutional networks for semantic segmentation. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 3431–3440.\nMehta, S.; and Rastegari, M. 2022. Mobilevit: light-weight,\ngeneral-purpose, and mobile-friendly vision transformer.\nICLR.\nOpenMMLab. 2020. MMSegmentation: OpenMMLab Se-\nmantic Segmentation Toolbox and Benchmark. https://\ngithub.com/open-mmlab/mmsegmentation.\nQian, Y .; Yin, G.; Sheng, L.; Chen, Z.; and Shao, J. 2020.\nThinking in frequency: Face forgery detection by mining\nfrequency-aware clues. In European conference on com-\nputer vision, 86–103. Springer.\nRao, Y .; Zhao, W.; Zhu, Z.; Lu, J.; and Zhou, J. 2021. Global\nfilter networks for image classification. Advances in Neural\nInformation Processing Systems, 34: 980–993.\nRen, S.; Zhou, D.; He, S.; Feng, J.; and Wang, X. 2022.\nShunted Self-Attention via Multi-Scale Token Aggregation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 10853–10862.\nSandler, M.; Howard, A.; Zhu, M.; Zhmoginov, A.; and\nChen, L.-C. 2018. Mobilenetv2: Inverted residuals and lin-\near bottlenecks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 4510–4520.\nStrudel, R.; Garcia, R.; Laptev, I.; and Schmid, C. 2021.\nSegmenter: Transformer for semantic segmentation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 7262–7272.\nWang, H.; Wu, X.; Huang, Z.; and Xing, E. P. 2020.\nHigh-frequency component helps explain the generaliza-\ntion of convolutional neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 8684–8694.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2021. Pyramid vision trans-\nformer: A versatile backbone for dense prediction without\nconvolutions. InProceedings of the IEEE/CVF International\nConference on Computer Vision, 568–578.\nWang, W.; Xie, E.; Li, X.; Fan, D.-P.; Song, K.; Liang, D.;\nLu, T.; Luo, P.; and Shao, L. 2022. Pvt v2: Improved base-\nlines with pyramid vision transformer. Computational Vi-\nsual Media, 8(3): 415–424.\nWu, K.; Peng, H.; Chen, M.; Fu, J.; and Chao, H. 2021. Re-\nthinking and Improving Relative Position Encoding for Vi-\nsion Transformer. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 10033–\n10041.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient design\nfor semantic segmentation with transformers. Advances in\nNeural Information Processing Systems, 34: 12077–12090.\nXu, N.; Price, B.; Cohen, S.; and Huang, T. 2017. Deep\nimage matting. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2970–2979.\n523\nYuan, Y .; Fu, R.; Huang, L.; Lin, W.; Zhang, C.; Chen, X.;\nand Wang, J. 2021a. Hrformer: High-resolution vision trans-\nformer for dense predict. Advances in Neural Information\nProcessing Systems, 34: 7281–7293.\nYuan, Y .; Huang, L.; Guo, J.; Zhang, C.; Chen, X.; and\nWang, J. 2021b. OCNet: Object context for semantic\nsegmentation. International Journal of Computer Vision,\n129(8): 2375–2398.\nZhang, W.; Pang, J.; Chen, K.; and Loy, C. C. 2021. K-net:\nTowards unified image segmentation. Advances in Neural\nInformation Processing Systems, 34: 10326–10338.\nZhao, H.; Shi, J.; Qi, X.; Wang, X.; and Jia, J. 2017. Pyra-\nmid scene parsing network. InProceedings of the IEEE con-\nference on computer vision and pattern recognition, 2881–\n2890.\nZhong, Y .; Li, B.; Tang, L.; Kuang, S.; Wu, S.; and Ding, S.\n2022. Detecting Camouflaged Object in Frequency Domain.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 4504–4513.\nZhou, B.; Zhao, H.; Puig, X.; Fidler, S.; Barriuso, A.; and\nTorralba, A. 2017. Scene parsing through ade20k dataset. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, 633–641.\nZhu, X.; Su, W.; Lu, L.; Li, B.; Wang, X.; and Dai, J. 2021.\nDeformable detr: Deformable transformers for end-to-end\nobject detection. ICLR.\n524"
}