{
  "title": "MLIM: Vision-and-Language Model Pre-training with Masked Language and Image Modeling",
  "url": "https://openalex.org/W3202461379",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5041540752",
      "name": "Tarik Arici",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5077343004",
      "name": "Mehmet Saygın Seyfioğlu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5048282838",
      "name": "Tal Neiman",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5018343158",
      "name": "Yi Xu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5008915336",
      "name": "Son N. Tran",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5013670321",
      "name": "Trishul Chilimbi",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5082750097",
      "name": "Belinda Zeng",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5073502763",
      "name": "Ismail Tutar",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2969876226",
    "https://openalex.org/W3142330916",
    "https://openalex.org/W3000433013",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3016923549",
    "https://openalex.org/W2952020226",
    "https://openalex.org/W3035497460",
    "https://openalex.org/W4287025446",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3165938948",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W3018442911",
    "https://openalex.org/W3020257313"
  ],
  "abstract": "Vision-and-Language Pre-training (VLP) improves model performance for downstream tasks that require image and text inputs. Current VLP approaches differ on (i) model architecture (especially image embedders), (ii) loss functions, and (iii) masking policies. Image embedders are either deep models like ResNet or linear projections that directly feed image-pixels into the transformer. Typically, in addition to the Masked Language Modeling (MLM) loss, alignment-based objectives are used for cross-modality interaction, and RoI feature regression and classification tasks for Masked Image-Region Modeling (MIRM). Both alignment and MIRM objectives mostly do not have ground truth. Alignment-based objectives require pairings of image and text and heuristic objective functions. MIRM relies on object detectors. Masking policies either do not take advantage of multi-modality or are strictly coupled with alignments generated by other models. In this paper, we present Masked Language and Image Modeling (MLIM) for VLP. MLIM uses two loss functions: Masked Language Modeling (MLM) loss and image reconstruction (RECON) loss. We propose Modality Aware Masking (MAM) to boost cross-modality interaction and take advantage of MLM and RECON losses that separately capture text and image reconstruction quality. Using MLM + RECON tasks coupled with MAM, we present a simplified VLP methodology and show that it has better downstream task performance on a proprietary e-commerce multi-modal dataset.",
  "full_text": "MLIM: V ISION -AND -LANGUAGE MODEL PRE-TRAINING WITH\nMASKED LANGUAGE AND IMAGE MODELING\nA PREPRINT\nTarik Arici∗\nAmazon.com Inc\nSeattle, W A, USA\naricit@amazon.com\nMehmet Saygin Seyfioglu*\nAmazon.com Inc\nSeattle, W A, USA\nmseyfiog@amazon.com\nTal Neiman\nAmazon.com Inc\nSeattle, W A, USA\ntalneim@amazon.com\nYi Xu\nAmazon.com Inc\nSeattle, W A, USA\nyxaamzn@amazon.com\nSon Tran\nAmazon.com Inc\nSeattle, W A, USA\nsontran@amazon.com\nTrishul Chilimbi\nAmazon.com Inc\nSeattle, W A, USA\ntrishulc@amazon.com\nBelinda Zeng\nAmazon.com Inc\nSeattle, W A, USA\nzengb@amazon.com\nIsmail Tutar\nAmazon.com Inc\nSeattle, W A, USA\nismailt@amazon.com\nApril 16, 2022\nABSTRACT\nVision-and-Language Pre-training (VLP) improves model performance for downstream tasks that\nrequire image and text inputs. Current VLP approaches differ on (i) model architecture (especially\nimage embedders), (ii) loss functions, and (iii) masking policies. Image embedders are either deep\nmodels like ResNet or linear projections that directly feed image-pixels into the transformer. Typically,\nin addition to the Masked Language Modeling (MLM) loss, alignment-based objectives are used for\ncross-modality interaction, and RoI feature regression and classification tasks for Masked Image-\nRegion Modeling (MIRM). Both alignment and MIRM objectives mostly do not have ground truth.\nAlignment-based objectives require pairings of image and text and heuristic objective functions.\nMIRM relies on object detectors. Masking policies either do not take advantage of multi-modality or\nare strictly coupled with alignments generated by other models. In this paper, we present Masked\nLanguage and Image Modeling (MLIM) for VLP. MLIM uses two loss functions: Masked Language\nModeling (MLM) loss and image reconstruction (RECON) loss. We propose Modality Aware\nMasking (MAM) to boost cross-modality interaction and take advantage of MLM and RECON losses\nthat separately capture text and image reconstruction quality. Using MLM + RECON tasks coupled\nwith MAM, we present a simplified VLP methodology and show that it has better downstream task\nperformance on a proprietary e-commerce multi-modal dataset.\nKeywords Vision and language models, BERT, transformers, multimodal models, pre-training\n1 Introduction\nSince the emergence of transformer architectures (Vaswani et al ., 2017), and their first successful use in Natural\nLanguage Processing (NLP) tasks (Devlin et al., 2018), they have been used for vision tasks (Dosovitskiy et al., 2021;\nWang et al., 2021), and finally in vision-and-language tasks (Su et al ., 2020; Lu et al ., 2019; Li et al ., 2020). For\nvision-and-language pre-training, researchers have proposed i) diverse architectures, ii) loss functions, and iii) masking\npolicies. Model architectures employ different techniques to convert images into a suitable input sequence to the\ntransformer. Loss functions are designed for image-region modeling and cross-modality interaction via alignment\nfunctions between text and images. Masking strategies aim to avoid disrupting this alignment. Each of these techniques\nhas advanced the state of the art and provided valuable insights. However, it is unclear how to integrate all of them to\n∗These authors contributed equally.\nMLIM: Vision-and-Language Model Pre-training with Masked Language and Image ModelingA PREPRINT\nachieve a level of simplicity and accuracy for vision-and-language pre-training that is comparable to text transformer\npre-training using masked language modeling (MLM) loss and text-token masking.\nImage embedders convert an image to an embedding sequence. Image embedding sequences can be concatenated with\ntext token embedding sequences and forwarded to the transformer. Most VLP models use object detectors as image\nembedders. These object detectors are deep convolutional models trained on Visual Genome (Krishna et al., 2016) or\nImageNet classification (Russakovsky et al., 2015) datasets. Object detectors are limited to their original detection\ncategories and are heavyweight models. While end-to-end training can adapt these models, computational complexity\nor end-to-end depth of the VLP model (object detector + transformer) can be an issue for training or inference. Visual\ntransformers (ViLT) remove deep image-embedders and directly feed pixel-level inputs to the transformer via a linear\nprojection on image patches defined by a grid (Liu et al ., 2021). While ViLT significantly reduces computational\ncomplexity, it leaves image feature learning to the transformer layers, limiting interaction between text and image\nembeddings to later layers of the transformer. We propose using shallow CNN models as embedders to learn high-level\nfeatures for low-level pixel-data and enable better cross-modality interaction throughout the transformer layers.\nIn addition to MLM loss for language modeling, Masked Image-Region Modeling (MIRM) uses various loss functions\nfor image modeling. UNITER proposes loss functions that use RoI-features and predicted object-classes obtained\nfrom the object detector as target features and target labels for masked image regions (Chen et al., 2020). Another\ncategory of loss functions is alignment-based functions to encourage cross-modality interaction. Some alignment-based\nobjectives aim to match fused representations of images and text (e.g., Image Text Matching (ITM)). Other alignment\nobjectives operate on a finer resolution and aim to align text or image representations at a sequence position level via\ntransport plans. ITM tasks require positive and negative pairs. We believe that training using such pairs is a better fit for\ndownstream tasks and negative/positive definition might hurt generalization while pre-training. In addition, these loss\nfunctions do not use ground-truth labels, and instead rely on heuristics. We aim to design a pre-training methodology\nthat is as well-defined as MLM loss on text tokens: ground truth targets with loss functions that are not based on\nheuristics. We use the image reconstruction (RECON) task for Masked Image Modeling (MIM) with the image itself as\nthe target for this task.\nWhile text token masking is straightforward, masking for images modeling is not. Image regions detected by the object\ndetector are good candidates for masking as they define semantically significant regions in the image. However they do\nnot rely on ground truth, using predictions of the object detector and require heavyweight models. In addition, masking\ncan break cross-modal alignment if it is performed independently on text and image inputs. To avoid these requirements\nfor masking corresponding representations in both modalities, coarse alignments obtained using text generators are\nutilized (Zhuge et al., 2021). We avoid these problems by not using object detectors or alignment based objectives.\nIn this paper, we introduce Masked Language and Image Modeling (MLIM) for VLP pe-training. We use Transformers\n(Vaswani et al., 2017) as the main component in the model. We use a shallow-CNN based image embedder to embed\nimages and perform masking, and a CNN based image decoder to compute RECON loss. We use MLM loss to measure\ntext token prediction performance and RECON loss to measure image reconstruction performance. While MLM loss\nis only defined on masked text tokens, RECON loss is defined over all pixels of the image. Together, MLM and\nRECON losses capture text and image reconstruction performance. We do not use any alignment-based loss-function\nfor cross-modality interaction and instead use Modality-Aware Masking (MAM) to achieve this. MAM has three\noperating modes: (1) heavy image-masking, (2) heavy text-masking, (3) light image-masking and light text-masking.\nThe first two modes encourage information flow across modalities. Using an image embedder, we aim to bring image\nembeddings to a common embedding space with text token embeddings. We perform image masking on the image\nembedder outputs. MAM masks image and text inputs before the transformer, and MLM + RECON losses measure\nreconstructed text and image quality. We demonstrate the effectiveness of our VLP pre-training methodology on a\nproprietary e-commerce multi-modal dataset.\nOur contributions are summarized as follows:\n• We propose using image reconstruction (RECON) loss and show pre-training with MLM + RECON obviates\nthe need for alignment-based and image-region modeling based loss functions. This simplifies loss function\ndesign and does not require image-text pairings to create training instances.\n• We use a shallow CNN as an image embedder. Our image embedder is much more lightweight than deep\nmodels like ResNet, and image masking friendly. We show that a shallow CNN image embedder can bring\nimage pixels to higher level representations that foster interaction between image and text embeddings.\n• We show that MAM takes advantage of the two loss functions capturing text and image reconstruction quality\nvia masking policies that encourage cross-modal information flow.\n2\nMLIM: Vision-and-Language Model Pre-training with Masked Language and Image ModelingA PREPRINT\n2 Related Work\nRecently, transformer based model pre-training has been successfully applied in NLP tasks [elmo, bert gpt2, roberta,\nalbert, etc]. As an extension, multimodal models are proposed to learn representations from image-text pairs. Fine-\ntuning multimodal models achieve state–of-the-art (SoTA) on downstream tasks. Some of these models use two-stream\nstream architectures that use two single-modal transformers, and their outputs are fused by a third transformer [vilbert,\nlxmert]. On the other hand single-stream architectures input image and text embeddings into a transformer. Single-\nstream models have the potential to enable better cross-modality interaction since attention mechanisms can attend\nto both image and text. VL-BERT presents promising results for single-stream models. More work on VL tasks are\nproposed lately: VLP, ViLBERT, VL-BERT, ViLT offer single-stream models for generic understanding tasks such as\nVisual Question Answering (VQA) and image-text retrieval or generation tasks such as image captioning.\nObject detectors are commonly used as image embedders. Image embeddings are either region of interest (RoI) features\nobtained from the object detector (e.g., (Chen et al., 2020; Tan and Bansal, 2019; Li et al., 2020)) or output feature grids\nof the CNN backbone (Jiang et al., 2020; Nguyen et al., 2020). While using grid features instead of RoI-features falls\nbehind, it achieves similar performance with heavier CNN models (Huang et al., 2020). ViLT avoids using heavyweight\nimage embedders by directly embedding low-level pixel data with a single-layer projection and achieves similar results\nwith reduced complexity.\nMultiple loss functions are proposed for better learning from images. Some of these can be categorized under masked\nimage-region modeling (MIRM) tasks. RoI-feature regression tasks regresses the RoI-feature of the masked region with\nL2 loss. RoI-classification task learns the labels of masked regions with cross-entropy loss. UNITER (Chen et al., 2020)\nand LXMERT (Tan and Bansal, 2019) utilize object-detector features and its label-predictions as targets in MIRM\nlosses. Contrastive loss in the form of detecting randomly replaced RoI tags are proposed in Oscar (Li et al ., 2020).\nObject tags are also generated by the object detector. These MIRM tasks rely on object-detectors and pseudo-targets\n(i.e., RoI features and class-labels obtained from the detector), which may be wrong or irrelevant in the given context.\nAlignment-based loss functions are also commonly utilized (Zhuge et al., 2021; Li et al., 2020; Liu et al., 2021) for\ncross-modality interaction. These alignments either happen at modality level as in widely used Image Text Matching\n(ITM) task or at finer-grained levels as in Word-Region Alignment (WRA) via Optimal Transport (OT) (Chen et al.,\n2020). Alignment task requires pairings. While creating negative pairs for ITM is easy, how to retrieve these pairs at\ntrain-time and how to find good training pairs remains an issue. Also, WRA does not have ground truth.\nOur key differences from prior art is as follows: i) We simplify loss function design by using MLM and RECON losses\nboth of which have ground truths ii) We use MAM to force cross-modal information flow for better cross-modality\ninteraction and take advantage of MLM + RECON iii) We utilize a shallow CNN as image embedder, which is\nspecifically designed for image embedding and masking. Our image embedder can bring low-level pixel data to a\ncommon representation space with text-token embeddings for better cross-modality interaction2\n3 Masked Language and Image Modeling\n3.1 Model Overview\nThe model architecture of MLIM is given in Figure 1. Given an image-text pair, text is tokenized and embedded using\ntransformer’s word embeddings and word-position embeddings. Image is embedded using a shallow CNN model and\nimage-positional embeddings.\nSpecifically, image embedder is a CNN model with convolutional layers using 2D filters of kernel size 2x2 and stride\nof 2. Choosing kernel size in horizontal and vertical dimensions to be equal to the stride ensures each input will only\ncontribute to a single filter’s output. Therefore, image embeddings will correspond to non-overlapping pixel regions in\nthe original image-resolution and masking an embedding deletes all information captured from a region in the image.\nHence, prediction of masked region’s pixel data will be left to transformer’s self-attention layers to gather information\nfrom existing text and image embeddings. We observed that this helps with text-to-image information flow. Text is\ntokenized using transformer’s tokenizer (e.g. WordPiece for BERT). Word embeddings are obtained from transformer’s\nword embeddings. Both image and word embeddings have positional embeddings separately, which are added to the\nembeddings. Since we have separate positional embeddings for the two modalities, it is redundant to have modality\nembeddings. Therefore, we do not use any explicit modality embeddings (i.e., “segment embedding” in BERT).\n2We speculate low-level pixel embeddings can not directly interact with higher-level text-token embeddings. This might explain\nwhy ViLT did not work with BERT as the main transformer but it worked with Vision Transformer (Dosovitskiy et al., 2021) as the\nmain transformer.\n3\nMLIM: Vision-and-Language Model Pre-training with Masked Language and Image ModelingA PREPRINT\nWith equal probability \nMask image with prob=0.8, keep text \nMask text with prob=0.8, keep image \nMask both with prob=0.2\nImage Embedder\n1\nModality Aware\nMasking\nBert (Word) Embeddings\nMASK Embedding\nTransformer SEP Embedding\nImage DecoderMLM Head\nwomens \nMasked Language Modeling\nRECON Loss\n2 641 2\nS\nM\nPositional\nEmbeddings\nwomens lightweight breathable\nno show sock\n3 4 3 63\n2 MM 4 1 643M M\nbreathable show \nMasked Image Modeling\nMLM Loss\nM\n625 6\n6\nS\nFigure 1: Model overview. Model consists of word embeddings, MLM head, CNN-based image embedder, and\nCNN-based image decoder, and a multi-layer self-attention Transformer as the main component. MAM operates on\nword and image embeddings before inputting to the Transformer.\nImage decoder inputs transformer-outputs corresponding to image embeddings. Transformer outputs (1D vector\nsequences) are reshaped to 2D vector grids for deconvolutional filtering. Image decoder consists of cascaded decon-\nvolutional layers to bring 2D vector grids back to the original image resolution with three color channels. Output of\nthe decoder is a tensor of shape, i.e., (image_width, image_height, 3). Finally an element-wise sigmoid function is\napplied to the decoder output tensor to match pixel-intensity range (i.e., [0-1]) for each color channel.\nWe have two pre-training tasks: Masked Language Modeling (MLM) and image reconstruction (RECON) coupled with\nModality Aware Masking (MAM). MAM applies masking to both word and image embeddings. Both image and word\nmasking is realized by replacing an embedding with the embedding of [MASK]. This way transformer layers recognize\n[MASK]’s embedding as a special embedding that needs to be “filled in”, independent of the modality, by attending\nto other vectors in the layer inputs3. We do not mask positional embeddings, therefore they are added after masking.\nMAM operates on three modes as given in Figure 1. To pre-train, we apply MLM + RECON as a multi-loss objective\ngiven a mini-batch of image-text pairs.\n3.2 Pre-train Objectives for MLIM\nMasked Language Modeling (MLM): This task’s objective is to predict the masked words from available words and\nimage regions. We follow BERT for this task: two-layer MLP MLM head outputting logits over the vocabulary. MLM\nloss is negative log-likelihood for masked words. MAM determines the masking probability.\nMasked Image Modeling (MIM): This task’s objective is to reconstruct the full image from available words and image\nregions. Our RECON loss is an average of pixel-wise sum of squared errors (SSE).\nBoth tasks aim to reconstruct masked image and text inputs. While MLM task only reconstructs masked tokens, MIM\ntask reconstructs the full image. This is because image embedder outputs embeddings corresponding to input image\nregions which are lossy representations of the pixel data. Hence MIM task is better defined as reconstruction of masked\nand not-masked image regions, i.e., the whole image.\nWe do not have any other tasks, specifically no image-region feature prediction, image-region classification, or any\nalignment loss at modality or embedding sequence-position level.\n3We observed that using separate mask embeddings for the two modalities as in UNITER degrades the performance.\n4\nMLIM: Vision-and-Language Model Pre-training with Masked Language and Image ModelingA PREPRINT\nTable 1: PR AUC values on CM test dataset: Modality ablation study and the effect of MDO on fine-tuning\nModel Description PR AUC\nImage and text with MDO Pre-trained with RECON, MLM and MAM, fine-tuned with MDO 0.884\nImage and text without MDO Pre-trained with RECON, MLM and MAM, fine-tuned without MDO 0.864\nTable 2: PR AUC values on CM test dataset: Loss function ablation study and the effect of MAM\nModel Description PR AUC\nRECON + ITM + MLM + MAM Pre-trained with RECON, ITM, MLM and MAM 0.884\nRECON + MLM + MAM Pre-trained with RECON, MLM and MAM 0.884\nRECON + MLM + Naive Masking Pre-trained with RECON, MLM, and fixed masking prob of 0.2 0.873\nITM + MLM + MAM Pre-trained with ITM, MLM and MAM 0.855\nITM + MLM + Naive Masking Pre-trained with ITM, MLM, and fixed masking prob of 0.2 0.8\n4 Experiments\nWe use Amazon catalog data for pre-training. Amazon catalog includes items available online. These items have images\nand textual attributes. Catalog item images are dominantly single-item images and the text describes attributes of the\nitem. We sampled 6M items with their relevant attributes from our catalog for pre-training.\nWe evaluate MLIM on an Amazon internal dataset collected for the task of finding closely-matching (CM) item pairs.\nA pair of items is labeled as a match or a mismatch depending on the type of variation between the two items. Learning\nand predicting relationships is an essential task in our catalog systems and requires both image and text as input. CM\ndataset has 30K training examples and 10K test examples.\n4.1 Implementation Details\nFor all experiments we use Adam optimizer with a fixed learning rate of 8 ∗ 10−4. We resize images to 384 × 384\nresolution. We use pre-trained BERT model ( bert-large-uncased) from Huggingface (https://huggingface.co/,\n[n.d.]) with 24 stacked transformer blocks, 16 attention heads, and 1024 hidden state dimension as the transformer\nand continue training on Amazon catalog pre-training dataset. bert-large-uncased model has 336M parameters in\ntotal. Our image embedder and decoders are randomly initialized. Image embedder is a CNN with 7 layers and 200K\nparameters. Image decoder is a deconvolutional network with 10 layers and 2.8M parameters. Encoder output is a grid\nof 8 × 8, which is reshaped to a 1D sequence of length 64 for inputting to the transformer. We intentionally kept the\nencoder lightweight and decoder heavyweight.\n4.2 Results\n4.2.1 Fine-tuning for Pairwise Downstream Tasks\nWe fine-tune and evaluate our model on the CM downstream task. A pair of image-text inputs are concatenated using\nthe separator token. We remove the image decoder from the model-graph and do not apply any masking strategy for\nfine-tuning.\nAlthough fine-tuning VLP models on downstream tasks is mostly straightforward, VLP models offer new fine-tuning\ntricks. Since our CM task requires a pair of image+text inputs, we exploit this multi-modality by employing Modality\nDropout (MDO). MDO improves fine-tuning by randomly dropping one of the modalities. Similar to MAM, MDO\noperates in one of the three modes on a micro-batch: text-only, image-only, and image-text mode. In Table 1, we present\nPR AUC values. Fine-tuning with MDO further improves performance.\nWe present an ablation study for loss functions in Table 2. Using RECON loss instead of ITM loss improves PR AUC\nfrom 0.855 to 0.884. Using ITM loss together with MLM and RECON does not change the performance.\n4.2.2 Cross-Modality Interaction\nAs discussed before, cross-modality interaction in the transformer is a desired objective in VLP models. In this section,\nwe show evidence for cross-modality interaction in our proposed method. We present MLM and RECON losses on\nour test data under different inputting schemes. In Figure 2a, we show MLM loss computed on text inputs paired with\n5\nMLIM: Vision-and-Language Model Pre-training with Masked Language and Image ModelingA PREPRINT\n(a) Effect of images on MLM loss\n (b) Effect of text on RECON loss\nFigure 2: We use MLM and RECON loss on pre-training test-data to measure performance improvements by cross-\nmodal information flow\nrandom images sampled from the dataset, text inputs paired with no images (specifically gray images) and text inputs\npaired with their original images in the dataset for different text-token masking probabilities. MLM loss is reduced\nby using images, showing that the transformer uses information from the image inputs to achieve the MLM task. In\nFigure 2b, we show RECON loss computed on image inputs paired with random texts in the dataset, image inputs\npaired with empty texts and image inputs paired with their original texts in the dataset for different image-embedding\nmasking probabilities. RECON loss is reduced by using text, showing that the transformer uses information from the\ntext inputs to achieve the RECON task.\nWe note that random text inputs degrade RECON performance more than random images degrade MLM performance.\nThis might be implying that text-to-image information flow is more significant compared to image-to-text information\nflow.\n5 Conclusion\nIn this paper, we present Masked Language and Image Modeling (MLIM): a simplified VLP method using MLM and\nRECON losses and MAM. We simplify loss function design, propose a shallow image embedder to avoid heavyweight\nobject-detectors and present an image decoder to enable RECON loss. We believe VLP datasets ( e.g. e-commerce\ndatasets) are large enough to enable learning built-in image embedders during pre-training.\nWhile alignment-based loss functions are promising and help in learning contrastive features, finding good image-text\npairs (especially negative pairs) becomes an issue and makes pre-training rely on pairing techniques. On the other hand\nfiner-grained alignment objectives do not have ground truth.\nMasked Image-Region Modeling (MIRM) relies on RoI features and classes predicted by the object detector. Further-\nmore MIRM tasks aim to “fill in” masked regions. However RECON task aims to reconstruct the whole image.\nWe encourage work in built-in image embedders designed to be masking friendly, and in image decoders that are\ndesigned to get the best cross-modality interaction inside the transformer.\nAs a future work, we will optimize image embedder and decoder for public pre-training datasets which have more\ncomplexity (e.g., multiple objects and complex scenes) compared to e-commerce data.\nReferences\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020.\nUNITER: UNiversal Image-TExt Representation Learning. arXiv:1909.11740 [cs.CV]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. CoRR abs/1810.04805 (2018). arXiv:1810.04805 http://arxiv.org/\nabs/1810.04805\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An\nImage is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv:2010.11929 [cs.CV]\n6\nMLIM: Vision-and-Language Model Pre-training with Masked Language and Image ModelingA PREPRINT\nhttps://huggingface.co/. [n.d.]. Hugging Face.\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-BERT: Aligning Image Pixels\nwith Text by Deep Multi-Modal Transformers. arXiv:2004.00849 [cs.CV]\nHuaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen. 2020. In Defense of Grid Features\nfor Visual Question Answering. arXiv:2001.03615 [cs.CV]\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis,\nLi-Jia Li, David A. Shamma, Michael S. Bernstein, and Fei-Fei Li. 2016. Visual Genome: Connecting Language and\nVision Using Crowdsourced Dense Image Annotations. arXiv:1602.07332 [cs.CV]\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong,\nFuru Wei, Yejin Choi, and Jianfeng Gao. 2020. Oscar: Object-Semantics Aligned Pre-training for Vision-Language\nTasks. arXiv:2004.06165 [cs.CV]\nYang Liu, Alexandras Neophytou, Sunando Sengupta, and Eric Sommerlade. 2021. Cross-modal spectrum transforma-\ntion network for acoustic scene classification. In ICASSP 2021-2021 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 830–834.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. ViLBERT: Pretraining Task-Agnostic Visiolinguistic\nRepresentations for Vision-and-Language Tasks. arXiv:1908.02265 [cs.CV]\nDuy-Kien Nguyen, Vedanuj Goswami, and Xinlei Chen. 2020. MoVie: Revisiting Modulated Convolutions for Visual\nCounting and Beyond. arXiv:2004.11883 [cs.CV]\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual\nRecognition Challenge. arXiv:1409.0575 [cs.CV]\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic\nVisual-Linguistic Representations. arXiv:1908.08530 [cs.CV]\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers.\narXiv:1908.07490 [cs.CL]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. 2017. Transformer: Attention is all you need. Advances in Neural Information Processing Systems 30\n(jun 2017), 5998–6008. arXiv:1706.03762v5 http://arxiv.org/abs/1706.03762\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling\nShao. 2021. Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions.\narXiv:2102.12122 [cs.CV]\nMingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, and Ling Shao.\n2021. Kaleido-BERT: Vision-Language Pre-training on Fashion Domain. arXiv:2103.16110 [cs.CV]\n7",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7840233445167542
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5718772411346436
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.5558826327323914
    },
    {
      "name": "Masking (illustration)",
      "score": 0.5538747906684875
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.5430452823638916
    },
    {
      "name": "Language model",
      "score": 0.5167438387870789
    },
    {
      "name": "Image (mathematics)",
      "score": 0.49782848358154297
    },
    {
      "name": "Pixel",
      "score": 0.4757632613182068
    },
    {
      "name": "Transformer",
      "score": 0.46209707856178284
    },
    {
      "name": "Heuristic",
      "score": 0.45514681935310364
    },
    {
      "name": "Image quality",
      "score": 0.4387722909450531
    },
    {
      "name": "Computer vision",
      "score": 0.40101805329322815
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38549935817718506
    },
    {
      "name": "Linguistics",
      "score": 0.07318735122680664
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1311688040",
      "name": "Amazon (United States)",
      "country": "US"
    }
  ],
  "cited_by": 7
}