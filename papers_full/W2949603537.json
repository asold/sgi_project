{
    "title": "Analyzing the Structure of Attention in a Transformer Language Model",
    "url": "https://openalex.org/W2949603537",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2125219408",
            "name": "Jesse Vig",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2279044859",
            "name": "Yonatan Belinkov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2910243263",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2951528897",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2945767825",
        "https://openalex.org/W2962958286",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2610881169",
        "https://openalex.org/W2934842096",
        "https://openalex.org/W2964174820",
        "https://openalex.org/W2760327630",
        "https://openalex.org/W2946359678",
        "https://openalex.org/W2963651521",
        "https://openalex.org/W2890353432",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W2118463056",
        "https://openalex.org/W2799118206",
        "https://openalex.org/W2906152891",
        "https://openalex.org/W2799051177",
        "https://openalex.org/W2896667998",
        "https://openalex.org/W2964303116"
    ],
    "abstract": "The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.",
    "full_text": "Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 63–76\nFlorence, Italy, August 1, 2019.c⃝2019 Association for Computational Linguistics\n63\nAnalyzing the Structure of Attention in a Transformer Language Model\nJesse Vig\nPalo Alto Research Center\nMachine Learning and\nData Science Group\nInteraction and Analytics Lab\nPalo Alto, CA, USA\njesse.vig@parc.com\nYonatan Belinkov\nHarvard John A. Paulson School of\nEngineering and Applied Sciences and\nMIT Computer Science and\nArtiﬁcial Intelligence Laboratory\nCambridge, MA, USA\nbelinkov@seas.harvard.edu\nAbstract\nThe Transformer is a fully attention-based\nalternative to recurrent networks that has\nachieved state-of-the-art results across a range\nof NLP tasks. In this paper, we analyze\nthe structure of attention in a Transformer\nlanguage model, the GPT-2 small pretrained\nmodel. We visualize attention for individ-\nual instances and analyze the interaction be-\ntween attention and syntax over a large cor-\npus. We ﬁnd that attention targets different\nparts of speech at different layer depths within\nthe model, and that attention aligns with de-\npendency relations most strongly in the mid-\ndle layers. We also ﬁnd that the deepest layers\nof the model capture the most distant relation-\nships. Finally, we extract exemplar sentences\nthat reveal highly speciﬁc patterns targeted by\nparticular attention heads.\n1 Introduction\nContextual word representations have recently\nbeen used to achieve state-of-the-art perfor-\nmance across a range of language understanding\ntasks (Peters et al., 2018; Radford et al., 2018;\nDevlin et al., 2018). These representations are\nobtained by optimizing a language modeling (or\nsimilar) objective on large amounts of text. The\nunderlying architecture may be recurrent, as in\nELMo (Peters et al., 2018), or based on multi-head\nself-attention, as in OpenAI’s GPT (Radford et al.,\n2018) and BERT (Devlin et al., 2018), which are\nbased on the Transformer (Vaswani et al., 2017).\nRecently, the GPT-2 model (Radford et al., 2019)\noutperformed other language models in a zero-\nshot setting, again based on self-attention.\nAn advantage of using attention is that it can\nhelp interpret the model by showing how the\nmodel attends to different parts of the input (Bah-\ndanau et al., 2015; Belinkov and Glass, 2019).\nVarious tools have been developed to visualize\nattention in NLP models, ranging from attention\nmatrix heatmaps (Bahdanau et al., 2015; Rush\net al., 2015; Rockt ¨aschel et al., 2016) to bipartite\ngraph representations (Liu et al., 2018; Lee et al.,\n2017; Strobelt et al., 2018). A visualization tool\ndesigned speciﬁcally for multi-head self-attention\nin the Transformer (Jones, 2017; Vaswani et al.,\n2018) was introduced in Vaswani et al. (2017).\nWe extend the work of Jones (2017), by visu-\nalizing attention in the Transformer at three lev-\nels of granularity: the attention-head level, the\nmodel level, and the neuron level. We also adapt\nthe original encoder-decoder implementation to\nthe decoder-only GPT-2 model, as well as the\nencoder-only BERT model.\nIn addition to visualizing attention for individ-\nual inputs to the model, we also analyze attention\nin aggregate over a large corpus to answer the fol-\nlowing research questions:\n•Does attention align with syntactic depen-\ndency relations?\n•Which attention heads attend to which part-\nof-speech tags?\n•How does attention capture long-distance re-\nlationships versus short-distance ones?\nWe apply our analysis to the GPT-2 small pre-\ntrained model. We ﬁnd that attention follows de-\npendency relations most strongly in the middle\nlayers of the model, and that attention heads tar-\nget particular parts of speech depending on layer\ndepth. We also ﬁnd that attention spans the great-\nest distance in the deepest layers, but varies signif-\nicantly between heads. Finally, our method for ex-\ntracting exemplar sentences yields many intuitive\npatterns.\n64\n2 Related Work\nRecent work suggests that the Transformer im-\nplicitly encodes syntactic information such as de-\npendency parse trees (Hewitt and Manning, 2019;\nRaganato and Tiedemann, 2018), anaphora (V oita\net al., 2018), and subject-verb pairings (Goldberg,\n2019; Wolf, 2019). Other work has shown that\nRNNs also capture syntax, and that deeper layers\nin the model capture increasingly high-level con-\nstructs (Blevins et al., 2018).\nIn contrast to past work that measure a model’s\nsyntactic knowledge through linguistic probing\ntasks, we directly compare the model’s atten-\ntion patterns to syntactic constructs such as de-\npendency relations and part-of-speech tags. Ra-\nganato and Tiedemann (2018) also evaluated de-\npendency trees induced from attention weights in a\nTransformer, but in the context of encoder-decoder\ntranslation models.\n3 Transformer Architecture\nStacked Decoder: GPT-2 is a stacked decoder\nTransformer, which inputs a sequence of tokens\nand applies position and token embeddings fol-\nlowed by several decoder layers. Each layer ap-\nplies multi-head self-attention (see below) in com-\nbination with a feedforward network, layer nor-\nmalization, and residual connections. The GPT-2\nsmall model has 12 layers and 12 heads.\nSelf-Attention: Given an input x, the self-\nattention mechanism assigns to each tokenxi a set\nof attention weights over the tokens in the input:\nAttn(xi) = (αi,1(x),αi,2(x),...,α i,i(x)) (1)\nwhere αi,j(x) is the attention that xi pays to xj.\nThe weights are positive and sum to one. Attention\nin GPT-2 is right-to-left, soαi,j is deﬁned only for\nj ≤i. In the multi-layer, multi-head setting, αis\nspeciﬁc to a layer and head.\nThe attention weights αi,j(x) are computed\nfrom the scaled dot-product of the query vectorof\nxi and the key vectorof xj, followed by a softmax\noperation. The attention weights are then used to\nproduce a weighted sum of value vectors:\nAttention(Q,K,V ) = softmax(QKT\n√dk\n)V (2)\nusing query matrix Q, key matrix K, and value\nmatrix V, where dk is the dimension of K. In a\nmulti-head setting, the queries, keys, and values\nare linearly projected h times, and the attention\noperation is performed in parallel for each repre-\nsentation, with the results concatenated.\n4 Visualizing Individual Inputs\nIn this section, we present three visualizations of\nattention in the Transformer model: the attention-\nhead view, the model view, and the neuron view.\nSource code and Jupyter notebooks are avail-\nable at https://github.com/jessevig/\nbertviz, and a video demonstration can be\nfound at https://vimeo.com/339574955.\nA more detailed discussion of the tool is provided\nin Vig (2019).\n4.1 Attention-head View\nThe attention-head view (Figure 1) visualizes at-\ntention for one or more heads in a model layer.\nSelf-attention is depicted as lines connecting the\nattending tokens (left) with the tokens being at-\ntended to (right). Colors identify the head(s), and\nline weight reﬂects the attention weight. This view\nclosely follows the design of Jones (2017), but has\nbeen adapted to the GPT-2 model (shown in the\nﬁgure) and BERT model (not shown).\nFigure 1: Attention-head view of GPT-2 for layer 4,\nhead 11, which focuses attention on previous token.\nThis view helps focus on the role of speciﬁc at-\ntention heads. For instance, in the shown example,\nthe chosen attention head attends primarily to the\nprevious token position.\n65\nFigure 2: Model view of GPT-2, for same input as in\nFigure 1 (excludes layers 6–11 and heads 6–11).\n4.2 Model View\nThe model view (Figure 2) visualizes attention\nacross all of the model’s layers and heads for a\nparticular input. Attention heads are presented in\ntabular form, with rows representing layers and\ncolumns representing heads. Each head is shown\nin a thumbnail form that conveys the coarse shape\nof the attention pattern, following the small multi-\nples design pattern (Tufte, 1990). Users may also\nclick on any head to enlarge it and see the tokens.\nThis view facilitates the detection of coarse-\ngrained differences between heads. For example,\nseveral heads in layer 0 share a horizontal-stripe\npattern, indicating that tokens attend to the current\nposition. Other heads have a triangular pattern,\nshowing that they attend to the ﬁrst token. In the\ndeeper layers, some heads display a small number\nof highly deﬁned lines, indicating that they are tar-\ngeting speciﬁc relationships between tokens.\n4.3 Neuron View\nThe neuron view (Figure 3) visualizes how indi-\nvidual neurons interact to produce attention. This\nview displays the queries and keys for each to-\nken, and demonstrates how attention is computed\nfrom the scaled dot product of these vectors. The\nelement-wise product shows how speciﬁc neurons\ninﬂuence the dot product and hence attention.\nWhereas the attention-head view and the model\nview show what attention patterns the model\nlearns, the neuron view shows how the model\nforms these patterns. For example, it can help\nidentify neurons responsible for speciﬁc attention\npatterns, as illustrated in Figure 3.\n5 Analyzing Attention in Aggregate\nIn this section we explore the aggregate proper-\nties of attention across an entire corpus. We ex-\namine how attention interacts with syntax, and we\ncompare long-distance versus short-distance rela-\ntionships. We also extract exemplar sentences that\nreveal patterns targeted by each attention head.\n5.1 Methods\n5.1.1 Part-of-Speech Tags\nPast work suggests that attention heads in the\nTransformer may specialize in particular linguis-\ntic phenomena (Vaswani et al., 2017; Raganato\nand Tiedemann, 2018; Vig, 2019). We explore\nwhether individual attention heads in GPT-2 target\nparticular parts of speech. Speciﬁcally, we mea-\nsure the proportion of total attention from a given\nhead that focuses on tokens with a given part-of-\nspeech tag, aggregated over a corpus:\nPα(tag) =\n∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\nαi,j(x)·1pos(xj)=tag\n∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\nαi,j(x)\n(3)\nwhere tagis a part-of-speech tag, e.g.,NOUN, xis\na sentence from the corpus X, αi,j is the attention\nfrom xi to xj for the given head (see Section 3),\nand pos(xj) is the part-of-speech tag of xj. We\nalso compute the share of attention directed from\neach part of speech in a similar fashion.\n5.1.2 Dependency Relations\nRecent work shows that Transformers and recur-\nrent models encode dependency relations (Hewitt\nand Manning, 2019; Raganato and Tiedemann,\n2018; Liu et al., 2019). However, different mod-\nels capture dependency relations at different layer\ndepths. In a Transformer model, the middle layers\nwere most predictive of dependencies (Liu et al.,\n2019; Tenney et al., 2019). Recurrent models were\nfound to encode dependencies in lower layers for\nlanguage models (Liu et al., 2019) and in deeper\nlayers for translation models (Belinkov, 2018).\nWe analyze how attention aligns with depen-\ndency relations in GPT-2 by computing the pro-\nportion of attention that connects tokens that are\nalso in a dependency relation with one another. We\n66\nFigure 3: Neuron view for layer 8, head 6, which targets items in lists. Positive and negative values are colored blue\nand orange, respectively, and color saturation indicates magnitude. This view traces the computation of attention\n(Section 3) from the selected token on the left to each of the tokens on the right. Connecting lines are weighted\nbased on attention between the respective tokens. The arrows (not in visualization) identify the neurons that most\nnoticeably contribute to this attention pattern: the lower arrows point to neurons that contribute to attention towards\nlist items, while the upper arrow identiﬁes a neuron that helps focus attention on the ﬁrst token in the sequence.\nrefer to this metric as dependency alignment:\nDepAlα =\n∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\nαi,j(x)dep(xi,xj)\n∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\nαi,j(x)\n(4)\nwhere dep(xi,xj) is an indicator function that re-\nturns 1 if xi and xj are in a dependency relation\nand 0 otherwise. We run this analysis under three\nalternate formulations of dependency: (1) the at-\ntending token (xi) is the parent in the dependency\nrelation, (2) the token receiving attention ( xj) is\nthe parent, and (3) either token is the parent.\nWe hypothesized that heads that focus attention\nbased on position—for example, the head in Fig-\nure 1 that focuses on the previous token—would\nnot align well with dependency relations, since\nthey do not consider the content of the text. To dis-\ntinguish between content-dependent and content-\nindependent (position-based) heads, we deﬁne at-\ntention variability, which measures how attention\nvaries over different inputs; high variability would\nsuggest a content-dependent head, while low vari-\nability would indicate a content-independent head:\nVariabilityα =\n∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\n|αi,j(x) −¯αi,j|\n2 · ∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\nαi,j(x)\n(5)\nwhere ¯αi,j is the mean of αi,j(x) over all x∈X.\nVariabilityα represents the mean absolute de-\nviation1 of α over X, scaled to the [0,1] inter-\nval.2,3 Variability scores for three example atten-\ntion heads are shown in Figure 4.\n5.1.3 Attention Distance\nPast work suggests that deeper layers in NLP\nmodels capture longer-distance relationships than\nlower layers (Belinkov, 2018; Raganato and\nTiedemann, 2018). We test this hypothesis on\nGPT-2 by measuring the mean distance (in num-\nber of tokens) spanned by attention for each head.\nSpeciﬁcally, we compute the average distance be-\ntween token pairs in all sentences in the corpus,\nweighted by the attention between the tokens:\n¯Dα =\n∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\nαi,j(x) ·(i−j)\n∑\nx∈X\n|x|∑\ni=1\ni∑\nj=1\nαi,j(x)\n(6)\nWe also explore whether heads with more dis-\n1We considered using variance to measure attention vari-\nability; however, attention is sparse for many attention heads\nafter ﬁltering ﬁrst-token attention (see Section 5.2.3), result-\ning in a very low variance (due to αi,j(x) ≈0 and ¯αi,j ≈0)\nfor many content-sensitive attention heads. We did not use a\nprobability distance measure, as attention values do not sum\nto one due to ﬁltering ﬁrst-token attention.\n2The upper bound is 1 because the denominator is an\nupper bound on the numerator.\n3When computing variability, we only include the ﬁrstN\ntokens (N=10) of each x ∈X to ensure a sufﬁcient amount\nof data at each position i. The positional patterns appeared to\nbe consistent across the entire sequence.\n67\nFigure 4: Attention heads in GPT-2 visualized for an example input sentence, along with aggregate metrics com-\nputed from all sentences in the corpus. Note that the average sentence length in the corpus is 27.7 tokens. Left:\nFocuses attention primarily on current token position. Center: Disperses attention roughly evenly across all pre-\nvious tokens. Right: Focuses on words in repeated phrases.\nFigure 5: Proportion of attention focused on ﬁrst token,\nbroken out by layer and head.\npersed attention patterns (Figure 4, center) tend to\ncapture more distant relationships. We measure\nattention dispersion based on the entropy 4 of the\nattention distribution (Ghader and Monz, 2017):\nEntropyα(xi) = −\ni∑\nj=1\nαi,j(x)log(αi,j(x)) (7)\nFigure 4 shows the mean distance and entropy\nvalues for three example attention heads.\n5.2 Experimental Setup\n5.2.1 Dataset\nWe focused our analysis on text from English\nWikipedia, which was not included in the training\n4When computing entropy, we exclude attention to the\nﬁrst (null) token (see Section 5.2.3) and renormalize the re-\nmaining weights. We exclude tokens that focus over 90% of\nattention to the ﬁrst token, to avoid a disproportionate inﬂu-\nence from the remaining attention from these tokens.\nset for GPT-2. We ﬁrst extracted 10,000 articles,\nand then sampled 100,000 sentences from these ar-\nticles. For the qualitative analysis described later,\nwe used the full dataset; for the quantitative anal-\nysis, we used a subset of 10,000 sentences.\n5.2.2 Tools\nWe computed attention weights using the\npytorch-pretrained-BERT5 implemen-\ntation of the GPT-2 small model. We extracted\nsyntactic features using spaCy (Honnibal and\nMontani, 2017) and mapped the features from\nthe spaCy-generated tokens to the corresponding\ntokens from the GPT-2 tokenizer.6\n5.2.3 Filtering Null Attention\nWe excluded attention focused on the ﬁrst token\nof each sentence from the analysis because it was\nnot informative; other tokens appeared to focus\non this token by default when no relevant tokens\nwere found elsewhere in the sequence. On aver-\nage, 57% of attention was directed to the ﬁrst to-\nken. Some heads focused over 97% of attention\nto this token on average (Figure 5), which is con-\nsistent with recent work showing that individual\nattention heads may have little impact on over-\nall performance (V oita et al., 2019; Michel et al.,\n2019). We refer to the attention directed to the ﬁrst\ntoken as null attention.\n5https://github.com/huggingface/\npytorch-pretrained-BERT\n6In cases where the GPT-2 tokenizer split a word into\nmultiple pieces, we assigned the features to all word pieces.\n68\nFigure 6: Each heatmap shows the proportion of total attention directed to the given part of speech, broken out by\nlayer (vertical axis) and head (horizontal axis). Scales vary by tag. Results for all tags available in appendix.\nFigure 7: Each heatmap shows the proportion of total attention that originatesfrom the given part of speech, broken\nout by layer (vertical axis) and head (horizontal axis). Scales vary by tag. Results for all tags available in appendix.\n5.3 Results\n5.3.1 Part-of-Speech Tags\nFigure 6 shows the share of attention directed to\nvarious part-of-speech tags (Eq. 3) broken out by\nlayer and head. Most tags are disproportionately\ntargeted by one or more attention heads. For ex-\nample, nouns receive 43% of attention in layer 9,\nhead 0, compared to a mean of 21% over all heads.\nFor 13 of 16 tags, a head exists with an attention\nshare more than double the mean for the tag.\nThe attention heads that focus on a particular\ntag tend to cluster by layer depth. For example,\nthe top ﬁve heads targeting proper nouns are all in\nthe last three layers of the model. This may be due\nto several attention heads in the deeper layers fo-\ncusing on named entities (see Section 5.4), which\nmay require the broader context available in the\ndeeper layers. In contrast, the top ﬁve heads tar-\ngeting determiners—a lower-level construct—are\nall in the ﬁrst four layers of the model. This is con-\nsistent with previous ﬁndings showing that deeper\nlayers focus on higher-level properties (Blevins\net al., 2018; Belinkov, 2018).\nFigure 7 shows the proportion of attention di-\nrected from various parts of speech. The values\nappear to be roughly uniform in the initial lay-\ners of the model. The reason is that the heads in\nthese layers pay little attention to the ﬁrst (null) to-\nken (Figure 5), and therefore the remaining (non-\nnull) attention weights sum to a value close to\none. Thus, the net weight for each token in the\nweighted sum (Section 5.1.1) is close to one, and\nthe proportion reduces to the frequency of the part\nof speech in the corpus.\nBeyond the initial layers, attention heads spe-\ncialize in focusing attention from particular part-\nof-speech tags. However, the effect is less pro-\nnounced compared to the tags receiving attention;\nfor 7 out of 16 tags, there is a head that focuses\nattention from that tag with a frequency more than\ndouble the tag average. Many of these specialized\nheads also cluster by layer. For example, the top\nten heads for focusing attention from punctuation\nare all in the last six layers.\n5.3.2 Dependency Relations\nFigure 8 shows the dependency alignment scores\n(Eq. 4) broken out by layer. Attention aligns with\ndependency relations most strongly in the mid-\ndle layers, consistent with recent syntactic probing\nanalyses (Liu et al., 2019; Tenney et al., 2019).\nOne possible explanation for the low alignment\nin the initial layers is that many heads in these lay-\ners focus attention based on position rather than\ncontent, according to the attention variability (Eq.\n5) results in Figure 10. Figure 4 (left and center)\nshows two examples of position-focused heads\nfrom layer 0 that have relatively low dependency\nalignment7 (0.04 and 0.10, respectively); the ﬁrst\n69\nFigure 8: Proportion of attention that is aligned with dependency relations, aggregated by layer. The orange line\nshows the baseline proportion of token pairs that share a dependency relationship, independent of attention.\nFigure 9: Proportion of attention directed to various dependency types, broken out by layer.\nFigure 10: Attention variability by layer / head.\nHigh-values indicate content-dependent heads, and low\nvalues indicate content-independent (position-based)\nheads.\nhead focuses attention primarily on the current to-\nken position (which cannot be in a dependency re-\nlation with itself) and the second disperses atten-\ntion roughly evenly, without regard to content.\nAn interesting counterexample is layer 4, head\n11 (Figure 1), which has the highest depen-\ndency alignment out of all the heads (DepAl α =\n0.42)7 but is also the most position-focused\n(Variabilityα = 0.004). This head focuses atten-\ntion on the previous token, which in our corpus\nhas a 42% chance of being in a dependency rela-\n7Assuming relation may be in either direction.\ntion with the adjacent token. As we’ll discuss in\nthe next section, token distance is highly predic-\ntive of dependency relations.\nOne hypothesis for why attention diverges from\ndependency relations in the deeper layers is that\nseveral attention heads in these layers target very\nspeciﬁc constructs (Tables 1 and 2) as opposed to\nmore general dependency relations. The deepest\nlayers also target longer-range relationships (see\nnext section), whereas dependency relations span\nrelatively short distances (3.89 tokens on average).\nWe also analyzed the speciﬁc dependency types\nof tokens receiving attention (Figure 9). Sub-\njects (csubj, csubjpass, nsubj, nsubjpass) were\ntargeted more in deeper layers, while auxiliaries\n(aux), conjunctions (cc), determiners (det), ex-\npletives (expl), and negations (neg) were targeted\nmore in lower layers, consistent with previous\nﬁndings (Belinkov, 2018). For some other depen-\ndency types, the interpretations were less clear.\n5.3.3 Attention Distance\nWe found that attention distance (Eq. 6) is greatest\nin the deepest layers (Figure 11, right), conﬁrm-\ning that these layers capture longer-distance rela-\ntionships. Attention distance varies greatly across\nheads (SD = 3.6), even when the heads are in the\nsame layer, due to the wide variation in attention\nstructures (e.g., Figure 4 left and center).\n70\nFigure 11: Mean attention distance by layer / head (left), and by layer (right).\nFigure 12: Mean attention entropy by layer / head.\nHigher values indicate more diffuse attention.\nWe also explored the relationship between at-\ntention distance and attention entropy (Eq. 7),\nwhich measures how diffuse an attention pattern\nis. Overall, we found a moderate correlation ( r =\n0.61, p <0.001) between the two. As Figure 12\nshows, many heads in layers 0 and 1 have high en-\ntropy (e.g., Figure 4, center), which may explain\nwhy these layers have a higher attention distance\ncompared to layers 2–4.\nOne counterexample is layer 5, head 1 (Fig-\nure 4, right), which has the highest mean attention\ndistance of any head (14.2), and one of the low-\nest mean entropy scores (0.41). This head con-\ncentrates attention on individual words in repeated\nphrases, which often occur far apart from one an-\nother.\nWe also explored how attention distance re-\nlates to dependency alignment. Across all heads,\nwe found a negative correlation between the two\nquantities (r = −0.73,p < 0.001). This is con-\nsistent with the fact that the probability of two to-\nkens sharing a dependency relation decreases as\nthe distance between them increases 8; for exam-\n8This is true up to a distance of 18 tokens; 99.8% of de-\npendency relations occur within this distance.\nple, the probability of being in a dependency rela-\ntion is 0.42 for adjacent tokens, 0.07 for tokens at\na distance of 5, and 0.02 for tokens at a distance of\n10. The layers (2–4) in which attention spanned\nthe shortest distance also had the highest depen-\ndency alignment.\n5.4 Qualitative Analysis\nTo get a sense of the lexical patterns targeted by\neach attention head, we extracted exemplar sen-\ntences that most strongly induced attention in that\nhead. Speciﬁcally, we ranked sentences by the\nmaximum token-to-token attention weight within\neach sentence. Results for three attention heads\nare shown in Tables 1–3. We found other attention\nheads that detected entities (people, places, dates),\npassive verbs, acronyms, nicknames, paired punc-\ntuation, and other syntactic and semantic proper-\nties. Most heads captured multiple types of pat-\nterns.\n6 Conclusion\nIn this paper, we analyzed the structure of atten-\ntion in the GPT-2 Transformer language model.\nWe found that many attention heads specialize\nin particular part-of-speech tags and that different\ntags are targeted at different layer depths. We also\nfound that the deepest layers capture the most dis-\ntant relationships, and that attention aligns most\nstrongly with dependency relations in the middle\nlayers where attention distance is lowest.\nOur qualitative analysis revealed that the struc-\nture of attention is closely tied to the training ob-\njective; for GPT-2, which was trained using left-\nto-right language modeling, attention often fo-\ncused on words most relevant to predicting the\nnext token in the sequence. For future work, we\nwould like to extend the analysis to other Trans-\nformer models such as BERT, which has a bidi-\n71\nRank Sentence\n1 The Australian search and rescue service is provided by Aus S AR , which is part of the\nAustralian Maritime Safety Authority ( AM SA ).\n2 In 1925 , Bapt ists worldwide formed the Baptist World Alliance ( B W A ).\n3 The Oak dale D ump is listed as an Environmental Protection Agency Super fund site due\nto the contamination of residential drinking water wells with volatile organic compounds (\nV OC s ) and heavy metals .\nTable 1: Exemplar sentences for layer 10, head 10, which focuses attention from acronyms to the associated phrase.\nThe tokens with maximum attention are underlined; the attending token is bolded and the token receiving attention\nis italicized. It appears that attention is directed to the part of the phrase that would help the model choose thenext\nword piece in the acronym (after the token paying attention), reﬂecting the language modeling objective.\nRank Sentence\n1 After the two prototypes were completed , production began in Mar iet ta , Georgia , ...\n3 The ﬁctional character Sam Fisher of the Spl inter Cell video game series by Ubisoft was\nborn in Tow son , as well as residing in a town house , as stated in the novel izations ...\n4 Suicide bombers attack three hotels in Am man , Jordan , killing at least 60 people .\nTable 2: Exemplar sentences for layer 11, head 2, which focuses attention from commas to the preceding place\nname (or the last word piece thereof). The likely purpose of this attention head is to help the model choose the\nrelated place name that would follow the comma, e.g. the country or state in which the city is located.\nRank Sentence\n1 With the United States isolation ist and Britain stout ly refusing to make the ” continental\ncommitment ” to defend France on the same scale as in World War I , theprospects of Anglo\n- American assistance in another war with Germany appeared to be doubtful ...\n2 The show did receive a somewhat favorable review from noted critic Gilbert Se ld es in the\nDecember 15 , 1962 TV Guide : ” The whole notion on which The Beverly Hill bill ies is\nfounded is an encouragement to ignorance ...\n3 he Arch im edes won signiﬁcant market share in the education markets of the UK , Ireland\n, Australia and New Zealand ; the success of the Arch im edes in British schools was due\npartly to its predecessor the BBC Micro and later to the Comput ers for Schools scheme ...\nTable 3: Exemplar sentences for layer 11, head 10 which focuses attention from the end of a noun phrase to the\nhead noun. In the ﬁrst sentence, for example, the head noun is prospects and the remainder of the noun phrase is\nof Anglo - American assistance in another war withGermany. The purpose of this attention pattern is likely to\npredict the word (typically a verb) that follows the noun phrase, as the head noun is a strong predictor of this.\nrectional architecture and is trained on both token-\nlevel and sentence-level tasks.\nAlthough the Wikipedia sentences used in our\nanalysis cover a diverse range of topics, they all\nfollow a similar encyclopedic format and style.\nFurther study is needed to determine how attention\npatterns manifest in other types of content, such as\ndialog scripts or song lyrics. We would also like\nto analyze attention patterns in text much longer\nthan a single sentence, especially for new Trans-\nformer variants such as the Transformer-XL (Dai\net al., 2019) and Sparse Transformer (Child et al.,\n2019), which can handle very long contexts.\nWe believe that interpreting a model based on\nattention is complementary to linguistic probing\napproaches (Section 2). While linguistic prob-\ning precisely quantiﬁes the amount of information\nencoded in various components of the model, it\nrequires training and evaluating a probing clas-\nsiﬁer. Analyzing attention is a simpler process\nthat also produces human-interpretable descrip-\ntions of model behavior, though recent work casts\ndoubt on its role in explaining individual predic-\ntions (Jain and Wallace, 2019). The results of\nour analyses were often consistent with those from\nprobing approaches.\n7 Acknowledgements\nY .B. was supported by the Harvard Mind, Brain,\nand Behavior Initiative.\n72\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. InInternational Con-\nference on Learning Representations (ICLR).\nYonatan Belinkov. 2018. On Internal Language Rep-\nresentations in Deep Learning: An Analysis of Ma-\nchine Translation and Speech Recognition. Ph.D.\nthesis, Massachusetts Institute of Technology.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49–72.\nTerra Blevins, Omer Levy, and Luke Zettlemoyer.\n2018. Deep RNNs encode soft hierarchical syntax.\narXiv preprint arXiv:1805.04218.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G.\nCarbonell, Quoc V . Le, and Ruslan Salakhutdinov.\n2019. Transformer-XL: Attentive language models\nbeyond a ﬁxed-length context.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nHamidreza Ghader and Christof Monz. 2017. What\ndoes attention in neural machine translation pay at-\ntention to? In Proceedings of the Eighth Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 30–39.\nYoav Goldberg. 2019. Assessing BERT’s syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nMatthew Honnibal and Ines Montani. 2017. spaCy 2:\nNatural language understanding with Bloom embed-\ndings, convolutional neural networks and incremen-\ntal parsing. To appear.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. CoRR, abs/1902.10186.\nLlion Jones. 2017. Tensor2tensor transformer\nvisualization. https://github.com/\ntensorflow/tensor2tensor/tree/\nmaster/tensor2tensor/visualization.\nJaesong Lee, Joong-Hwi Shin, and Jun-Seok Kim.\n2017. Interactive visualization and manipulation\nof attention-based neural machine translation. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 17th Annual\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT).\nShusen Liu, Tao Li, Zhimin Li, Vivek Srikumar, Vale-\nrio Pascucci, and Peer-Timo Bremer. 2018. Visual\ninterrogation of attention-based models for natural\nlanguage inference and machine comprehension. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? arXiv\npreprint arXiv:1905.10650.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Technical re-\nport.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. Techni-\ncal report.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in Transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287–297. Association for Computational Linguis-\ntics.\nTim Rockt ¨aschel, Edward Grefenstette, Karl Moritz\nHermann, Tomas Kocisky, and Phil Blunsom. 2016.\nReasoning about entailment with neural attention.\nIn International Conference on Learning Represen-\ntations (ICLR).\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A neural attention model for abstractive sen-\ntence summarization. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing.\n73\nH. Strobelt, S. Gehrmann, M. Behrisch, A. Perer,\nH. Pﬁster, and A. M. Rush. 2018. Seq2Seq-Vis:\nA Visual Debugging Tool for Sequence-to-Sequence\nModels. ArXiv e-prints.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the Association for Computational\nLinguistics.\nEdward Tufte. 1990. Envisioning Information. Graph-\nics Press, Cheshire, CT, USA.\nAshish Vaswani, Samy Bengio, Eugene Brevdo, Fran-\ncois Chollet, Aidan N. Gomez, Stephan Gouws,\nLlion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki\nParmar, Ryan Sepassi, Noam Shazeer, and Jakob\nUszkoreit. 2018. Tensor2tensor for neural machine\ntranslation. CoRR, abs/1803.07416.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv preprint arXiv:1706.03762.\nJesse Vig. 2019. A multiscale visualization of atten-\ntion in the Transformer model. In Proceedings of\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Pa-\npers), pages 1264–1274. Association for Computa-\ntional Linguistics.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nThomas Wolf. 2019. Some additional experiments ex-\ntending the tech report ”Assessing BERTs syntactic\nabilities” by Yoav Goldberg. Technical report.\n74\nA Appendix\nFigures A.1 and A.2 shows the results from Fig-\nures 6 and 7 for the full set of part-of-speech tags.\n75\nFigure A.1: Each heatmap shows the proportion of total attention directed to the given part of speech, broken out\nby layer (vertical axis) and head (horizontal axis).\n76\nFigure A.2: Each heatmap shows the proportion of attention originating from the given part of speech, broken out\nby layer (vertical axis) and head (horizontal axis)."
}