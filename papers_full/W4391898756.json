{
  "title": "Combining Swin Transformer and Attention-Weighted Fusion for Scene Text Detection",
  "url": "https://openalex.org/W4391898756",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101795786",
      "name": "Xianguo Li",
      "affiliations": [
        "Tiangong University"
      ]
    },
    {
      "id": "https://openalex.org/A5102568142",
      "name": "Xingchen Yao",
      "affiliations": [
        "Tiangong University"
      ]
    },
    {
      "id": "https://openalex.org/A5056614770",
      "name": "Y. F. Liu",
      "affiliations": [
        "Tianjin University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2519818067",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W2998621280",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W3177052299",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W4312403814",
    "https://openalex.org/W6604054835",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2550687635",
    "https://openalex.org/W2784050770",
    "https://openalex.org/W2605982830",
    "https://openalex.org/W2605076167",
    "https://openalex.org/W2963353821",
    "https://openalex.org/W3035679705",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2962810613",
    "https://openalex.org/W2963647456",
    "https://openalex.org/W2991626090",
    "https://openalex.org/W2967615747",
    "https://openalex.org/W4214922754",
    "https://openalex.org/W6600459194",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W2074849287",
    "https://openalex.org/W2963516811",
    "https://openalex.org/W2785383245",
    "https://openalex.org/W2144554289",
    "https://openalex.org/W1988461287",
    "https://openalex.org/W2963299604",
    "https://openalex.org/W3034514377",
    "https://openalex.org/W3181016597",
    "https://openalex.org/W3012138014",
    "https://openalex.org/W4212820886",
    "https://openalex.org/W3184364189",
    "https://openalex.org/W4283692648",
    "https://openalex.org/W4293679656",
    "https://openalex.org/W4283802017",
    "https://openalex.org/W4383113591",
    "https://openalex.org/W3172799005",
    "https://openalex.org/W3102695566",
    "https://openalex.org/W3106250896"
  ],
  "abstract": "Abstract The existing text detection algorithms based on Convolutional Neural Networks (CNN) commonly have the problems of insufficient receptive fields and inadequate extraction of spatial positional information, which limit their ability to detect large-scale variation text instances, long-distance and wide-spaced text instances as well as effectively distinguish complex background textures. To address the above problems, in this paper, a scene text detection algorithm combining Swin Transformer and attention-weighted fusion is proposed. Firstly, an attention-weighted fusion (AWF) module is proposed, which embeds a modified coordinate attention module (CAM) in the feature pyramid network (FPN). This module learns spatial positional weights of foreground information in different-scale features while suppressing redundant background information. As a result, the fused features are more focused on the text regions, enhancing the localization ability for text regions and boundaries. Secondly, the window-based self-attention mechanism of the Swin Transformer is utilized to achieve global feature perception on the fused features of the pyramid network. This compensates for the insufficient receptive fields of CNN and enhances the representation capability of global contextual features, thereby further improving the performance of text detection. Experimental results demonstrate that the proposed algorithm achieves competitive performance on three public datasets, namely ICDAR2015, MSRA-TD500, and Total-Text, with F-measure reaching 87.9%, 91.4%, and 86.7%, respectively. Code is available at: https://github.com/xgli411/ST-AWFNet .",
  "full_text": "Neural Processing Letters (2024) 56:52\nhttps://doi.org/10.1007/s11063-024-11501-7\nCombining Swin Transformer and Attention-Weighted\nFusion for Scene Text Detection\nXianguo Li 1,2 · Xingchen Yao 1,2 · Yi Liu 2,3\nAccepted: 25 November 2023 / Published online: 17 February 2024\n© The Author(s) 2024\nAbstract\nThe existing text detection algorithms based on Convolutional Neural Networks (CNN)\ncommonly have the problems of insufﬁcient receptive ﬁelds and inadequate extraction of\nspatial positional information, which limit their ability to detect large-scale variation text\ninstances, long-distance and wide-spaced text instances as well as effectively distinguish\ncomplex background textures. To address the above problems, in this paper, a scene text\ndetection algorithm combining Swin Transformer and attention-weighted fusion is proposed.\nFirstly, an attention-weighted fusion (AWF) module is proposed, which embeds a modiﬁed\ncoordinate attention module (CAM) in the feature pyramid network (FPN). This module\nlearns spatial positional weights of foreground information in different-scale features while\nsuppressing redundant background information. As a result, the fused features are more\nfocused on the text regions, enhancing the localization ability for text regions and boundaries.\nSecondly, the window-based self-attention mechanism of the Swin Transformer is utilized\nto achieve global feature perception on the fused features of the pyramid network. This\ncompensates for the insufﬁcient receptive ﬁelds of CNN and enhances the representation\ncapability of global contextual features, thereby further improving the performance of text\ndetection. Experimental results demonstrate that the proposed algorithm achieves competitive\nperformance on three public datasets, namely ICDAR2015, MSRA-TD500, and Total-Text,\nwith F-measure reaching 87.9%, 91.4%, and 86.7%, respectively. Code is available at: https://\ngithub.com/xgli411/ST-AWFNet.\nKeywords Scene text detection · Swin transformer · Attention-weighted fusion · Global\nfeature perception\nB Yi Liu\nliuyi@tiangong.edu.cn\nXianguo Li\nlixianguo@tiangong.edu.cn\nXingchen Yao\nxingchenyao12@gmail.com\n1 School of Electronics and Information Engineering, Tiangong University, Tianjin 300387, China\n2 Tianjin Key Laboratory of Optoelectronic Detection Technology and System, Tianjin 300387,\nChina\n3 Center for Engineering Internship and Training, Tiangong University, Tianjin 300387, China\n123\n52 Page 2 of 22 X. Li et al.\n1 Introduction\nScene text detection is an important computer vision task aimed at accurately locating text\nregions in images in the form of bounding boxes, laying the foundation for subsequent text\nrecognition. Currently, scene text detection technology has demonstrated high application\nvalue in areas such as image and video understanding, intelligent transportation, intelligent\nsecurity, and visual impairment navigation, attracting signiﬁcant attention from the academic\nand industrial communities.\nTraditional text detection methods for scenes are based on manually designed low-level\nfeatures such as character lines and shapes for text detection, with poor generalization ability,\nespecially in complex natural scenes. The complex background and geometric layout of nat-\nural scene texts with extreme aspect ratios, wide spacing, and varying shapes make it more\nchallenging to precisely locate text regions. With the rapid development of deep learning,\nscene text detection methods have made signiﬁcant progress in detecting regular-shaped text\ninstances [1]. Among them, text detection algorithms based on CNN have become a research\nhotspot. Most advanced scene text detectors are built upon object detection or semantic seg-\nmentation frameworks, which can be mainly classiﬁed into two categories: regression-based\nand segmentation-based methods. Regression-based methods usually draw inspiration from\nthe ideas of object detection algorithms like Faster R-CNN [ 2] and SSD [ 3]. They directly\npredict the coordinates of text locations using an anchor regression mechanism, eliminating\nthe need for complex post-processing. However, due to the limited representation form of\ninstances (e.g., quadrilateral, axis-aligned rectangle, or rotating rectangle), this may lead to\nredundant background noise and overlapping, making it inﬂexible to cope with irregular-\nshaped instances. Although some improvements have been proposed in recent years to detect\nirregular-shaped text instances using modiﬁed object detection frameworks, the complex-\nity of these frameworks severely impacts model efﬁciency. Segmentation-based methods\nemploy pixel-level segmentation for images. Due to the scalability of pixel-level predictions,\nthese methods can adapt well to various instance shapes. However, most segmentation-based\nmethods require complex post-processing to combine pixel-level results into text lines and\npredict accurate text bounding boxes, leading to signiﬁcant computational overhead during\nprediction. To leverage the advantages of segmentation-based methods and simplify post-\nprocessing steps, researchers have conducted in-depth studies on this issue. Liao et al. [ 4]\nproposed a differentiable binarization method (DB), which integrates the binarization pro-\ncess into the segmentation network. This approach improves detection performance while\nachieving streamlined post-processing.\nAlthough the segmentation-based approaches are more advantageous for text detection\nof arbitrary shapes, the following problems still exist: (1) the large-scale variation of text\ninstances in the same image leads to missed detection (e.g., Fig. 1a); (2) complex back-\nground textures lead to false and missed detections (e.g., Fig. 1b); (3) wide-spaced text\ninstance suffers from excessive segmentation, where characters within the same text line are\ndisconnected at character gaps (e.g., Fig. 1c). The reasons for the above problems are mainly\ntwofold. On the one hand, the spatial positional information of multi-scale text instances has\nnot been fully explored, which makes the pyramid network easily lose the high-level seman-\ntic information of small-scale text instances when merging features. Meanwhile, large-scale\ntext instances cannot be fully represented on shallow layers, leading to poor detection ability\nfor instances with large-scale variation. On the other hand, due to the limited receptive ﬁeld\nof convolutional operators in CNN, global contextual information is insufﬁciently captured,\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 3 of 22 52\nFig. 1 Problems in Text Detection: a The large-scale variation of text instances leads to missed detection;\nb Complex background textures lead to false and missed detections; c Break at character gaps\nand the text instance lacks long-range dependencies, which makes it difﬁcult to distinguish\ntext instances from the complex background and wide-spaced instances.\nAttention mechanisms [ 5–8] have been widely applied and studied in various computer\nvision tasks in recent years. Attention mechanisms simulate the information processing of\nthe human visual perception system, enabling neural networks to focus on meaningful fore-\nground information. Huang et al. [ 9] proposed a Criss-Coss Attention method that captures\nlong-distance contextual features in both horizontal and vertical directions, achieving satis-\nfactory results in the ﬁeld of semantic segmentation. This approach has introduced a fresh\nperspective for text detection based on segmentation methods. On this basis, considering the\ncharacteristics and challenges of the text detection task, it is desirable to design an efﬁcient\nand reasonable attention mechanism for text detection, which can enhance the feature repre-\nsentation of foreground text information and assist the network in accurately detecting text\nregions.\nRelated studies have shown that the global information modeling feature of the Trans-\nformer [ 10] can make up for the defect of the insufﬁcient receptive ﬁeld and improve the\nperformance of detection tasks. Dosovitskiy et al. [ 11] introduced Vision Transformers (ViT)\nthat segment images into sequences and feed them into standard Transformers with position\nembeddings to generate global interactions between different components, achieving out-\nstanding performance on the ImageNet dataset. For scene text detection tasks, Raisi et al.\n[12] utilized CNN as the feature extraction network and employed Transformer as the encoder\nand decoder to design a scene text detector. Zeng et al. [ 13] proposed a hybrid architecture\nthat introduced a novel backbone network called FSNet and incorporates central Transformer\nblocks (CTBlock). This architecture combines the advantages of both CNN and Transformer,\nparticularly excelling in the detection of small scene text instances. However, when faced\nwith challenges such as large-scale variations of text instances, their detection performance\nstill needs improvement. Liu et al. [ 14] proposed the Swin Transformer, which restricted the\ncalculation of self-attention to local windows, and at the same time achieved cross-window\ninformation interaction through a shifting window strategy. By achieving global interactions\nwhile addressing the issue of insufﬁcient local attention focus in traditional vision Transform-\ners, the Swin Transformer demonstrates excellent performance in detection and segmentation\ntasks with relatively lower computational complexity.\nInspired by the above research, a novel segmentation-based text detection algorithm named\nSwin Transformer and Attention Weighted Fusion Network (ST-AWFNet) is proposed. The\ncontributions of this paper can be summarized in the following three aspects:\n123\n52 Page 4 of 22 X. Li et al.\n1. An attention-weighted fusion module is designed, which embeds a modiﬁed coordinate\nattention module sensitive to position information in the feature pyramid networks. This\nmodule learns spatial position weights of foreground information in different-scale fea-\ntures, enhancing the information representation of multi-scale features in the network.\nAs a result, it alleviates information loss in multi-scale feature fusion and improves the\nlocalization ability of text regions with large-scale variation.\n2. A global feature perception method is proposed. Speciﬁcally, the Swin Transformer\nBlock (STB) module is constructed to calculate the relationship between different pixel\npoints using the window self-attention mechanism in Swin Transformer, capturing valu-\nable information from a global perspective. This not only reduces the false and missed\ndetection rate of text instances under complex background texture but also makes the text\ninstances of long-range dependencies more closely, which strengthens the perception\nof global features of text instances and avoids over-segmentation of wide-spaced text\ninstances.\n3. Combining the local feature capture advantage of CNN with the global context modeling\nadvantage of Transformer, a new algorithm ST-AWFNet is proposed. It achieves com-\npetitive detection results on multi-oriented text dataset, long text dataset, and curved text\ndataset.\n2 Related Work\n2.1 Regression-Based Scene Text Detection\nInspired by general object detection frameworks, regression-based methods directly regress\nthe text bounding boxes for text detection. TextBoxes [ 15] modiﬁes the aspect ratios of anchor\nboxes based on the SSD algorithm and replaces 3 × 3 convolutions with 1 × 5 horizontal\nconvolutions. TextBoxes++ [16] regresses the vertices of a quadrilateral based on predeﬁned\nanchor points to accommodate multi-directional text instances. EAST [ 17] performs efﬁcient\npixel-level regression on text instances in an anchor-free manner. SegLink [ 18] splits a long\ntext line into small text segments with directions and uses links to reconnect adjacent text\nsegments into long text lines. While the aforementioned methods excel in detecting quadrilat-\neral text instances, they often struggle when confronted with irregularly shaped text instances\ndue to their limited ability to represent complex geometries. To this end, LOMO [ 19] applies\na coarse-to-ﬁne iterative strategy to quadrilateral text bounding boxes and realizes irregular\ntext detection by reconstructing the outline of irregular text instances by considering the\ngeometric properties of the text instances. ContourNet [ 20] uses scale insensitivity to model\nthe contour points of the text itself to detect curved text instances. Although these methods\nachieve the detection of irregular-shaped text instances, the frameworks are more complex\nand the advantages are not signiﬁcant compared to segmentation methods that are more\nsuitable for arbitrarily shaped text detection.\n2.2 Segmentation-Based Scene Text Detection\nCurrently, most segmentation-based methods for text detection are built upon fully con-\nvolutional networks (FCN) [ 21]. These methods generate text masks through pixel-level\nsemantic segmentation and employ post-processing techniques to obtain text bounding\ncurves, overcoming the limitations of regression-based methods in handling irregular-shaped\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 5 of 22 52\ntext instances. PixelLink [ 22] combines pixel-level classiﬁcation and link prediction by\npredicting the link state between each pixel and adjacent pixels, but it tends to encounter\n\"sticking\" issues for tight-spaced instances. PSENet [ 23] detects arbitrarily shaped text\ninstances by performing progressive scale expansion on the text kernel to generate the entire\ntext bounding boxes. PAN [ 24] designs a lightweight segmentation framework that accu-\nrately aggregates pixels belonging to the same text instance by predicting similarity vectors.\nCRAFT [ 25] detects text instances by clustering character boxes based on character afﬁni-\nties, achieving accurate detection of large-scale text instances with smaller receptive ﬁelds.\nHowever, its reliance on additional character-level annotations limits its practical applicabil-\nity. TextFuseNet [ 26] restructures Mask-RCNN, enabling the ﬁnal text detection to utilize\na fusion of three levels of feature representations, including characters, words, and global\nfeatures. The version with the best performance utilizes the heavy backbone (ResNet101),\nresulting in a large model size and parameter quantity. It severely limits inference speed and\npractical application of the model. Liao et al. [ 4] designed a differentiable binarization module\nto perform the binarization process in the segmentation network, which achieved streamlined\npost-processing while alleviating the problem of dense text detection stickiness, and signif-\nicantly enhanced the performance of text detection. DBNet++ [ 27] introduces an adaptive\nscale fusion module on the basis of DBNet, which improves the robustness of the model\nfor multi-scale text detection. Chen et al. [ 28] proposed a lightweight network FAST, which\nis constructed by Neural Architecture Search (NAS) with a post-processing step that uses\nGPU. FAST exhibits high inference speed. However, its capability of handling wide-spaced\nor extremely scale-differentiated text instances has room for improvement.\nTherefore, this paper builds upon the segmentation-based text detection algorithm and\nintegrates Swin Transformer with attention-weighted feature fusion to further enhance the\nperformance of text detection.\n3 Methodology\n3.1 Overview\nThe architecture of the proposed ST-AWFNet algorithm is illustrated in Fig. 2, which mainly\nconsists of four parts: feature extraction, attention-weighted feature fusion, global feature\nperception, and differentiable binary post-processing. Firstly, the input image is fed into the\nbackbone network, which employs a ResNet50 [ 29] with deformable convolution (DCN)\n[30] for feature extraction across shallow to deep layers. The incorporation of deformable\nconvolution enables the model to beneﬁt from ﬂexible perceptual ﬁelds. Secondly, to make\nfull use of the multi-scale information, the attention-weighted fusion (AWF) module is used\nfor the efﬁcient fusion of features at different scales. Thirdly, the fused features undergo the\nSwin Transformer Block (STB) module to achieve comprehensive global feature perception.\nThis module utilizes a unique window-based multi-head self-attention mechanism as well\nas a shifted window multi-head self-attention mechanism, both of which enhance the repre-\nsentation capability of global features. Finally, the detection results are obtained through the\napplication of the differentiable binarization post-processing method.\n123\n52 Page 6 of 22 X. Li et al.\nC1\n8× 4× 2×\nEi+1\nEi\nCoordinate Attention Module \nConv 3×3 Convolution\nSwin Transformer Block C Concat\nElement-wise Add\nFeature extraction Attention-weighted fusion Global perception Post-processing\nImage\nC2 C3 C4 C5 E2E3E4E5\nApproximate\nbinary map\nC\n2×\nN× Upsample with rate N\nFe\nSTB\nSTB\nC'i\nProbability \nmap\nThreshold \nmapConv\nCAM\nCAM\nCAM\nCAM\nCAMCAM\nConv Conv Conv\nDetection result\nFig. 2 The architecture of our proposed ST-AWFNet. The complete workﬂow includes four stages: (1) Feature\nextraction; (2) Attention-weighted fusion; (3) Global perception; (4) Post-processing\n3.2 Attention-Weighted Fusion\nIn scene text images, the localization of text regions and boundaries is constrained by the\ncapture of location information on feature maps at different scales. While utilizing feature\npyramid networks (FPN) [ 31] for cross-scale feature fusion, the limited acquisition of spatial\nlocation information results in incomplete expression of multi-scale text instances features.\nAs a consequence, this hampers the effective representation of text regions and boundaries.\nExisting methods usually introduce channel attention and spatial attention mechanisms to\nmake the network pay more attention to the information of text regions, thus bolstering target\nlocalization capabilities. However, these methods tend to overlook the signiﬁcance of posi-\ntional and long-distance dependencies. To capture multi-scale positional information and full\nimage context as well, inspired by Coordinate Attention [ 8] and Criss-Coss Attention [ 9],\nwe propose a modiﬁed Coordinate Attention Module (CAM). Two parallel one-dimensional\npooling operations, horizontal and vertical, are applied to avoid the loss of location informa-\ntion due to two-dimensional global pooling. By capturing intra-channel dependencies while\npreserving positional information, the CAM signiﬁcantly enhances text localization.\nThe structure of CAM is shown in Fig. 3. The speciﬁc implementation process is as\nfollows: Given an input feature map of size C × H × W , the feature is encoded using\nC\nInput\nX  Pooling\nY  Pooling\n/circlex\nConv \nConv Split\nC×H×W 256×H×W\nC×H×1\nC×1×W\nC×1×(W+H) C/16×1×(W+H)\nConv C×H×1\nC×1×W\nSigmoid MultiplicationConcat /circlexC\nOutput\nFig. 3 Structure of modiﬁed coordinate attention module\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 7 of 22 52\naverage pooling operations in the horizontal and vertical directions to obtain position-aware\nfeatures f h ∈ RC×H ×1 and f w ∈ RC×1×W .T a k i n gt h ec-th channel as an example, the\noutput can be formulated as:\nf h\nc (h) = 1\nW\n∑\n0≤i <W\nxc (h,i ) (1)\nf w\nc (w) = 1\nH\n∑\n0≤j <H\nxc (j ,w ) (2)\nwhere xc denotes the tensor of the input feature map on channel c. This operation allows the\nattention map to capture the channel dependencies in one direction while preserving the loca-\ntion information in the other direction. Next, the features from two directions are concatenated\nalong the spatial dimension and further processed with 1 × 1 convolution, batch normaliza-\ntion, and activation operations to obtain the attention feature map g ∈ R\nC/16×1×(H +W ).T h e\ncomputational process can be expressed as:\ng = δ\n(\nConv1\n([\nf h , f w\n]))\n(3)\nwhere [·,·] represents the concatenation operation in the spatial dimension, Conv 1(·) repre-\nsents the 1 × 1 convolution operation, and δ(·) represents the nonlinear activation function.\nSubsequently, the attention feature map g is split into gh ∈ RC/16×H ×1 and gw ∈ RC/16×1×W\nalong the spatial dimension, followed by the 1 × 1 convolution and sigmoid activation func-\ntion to obtain the location-aware attention weight maps uh ∈ RC×H ×1 and uw ∈ RC×1×W .\nThe calculation process can be represented as:\nuh = σ\n(\nConv1\n(\ngh\n))\n(4)\nuw = σ\n(\nConv1\n(\ngw))\n(5)\nwhere σ(·) denotes the sigmoid activation function. uh and uw are multiplied element-wise\nwith the input features to obtain weighted features. Lastly, a 1 × 1 convolution is employed to\ncomprehensively consolidate features across channels. This process ﬁne-tunes the weighted\nfeatures while optimizing channel output to 256, effectively reducing parameters. The output\nof the CAM module is obtained as:\ny\nc (i , j ) = Conv1\n(\nxc (i , j ) × uh\nc (i ) × uw\nc (j )\n)\n(6)\nThe structure of our proposed AWF module is depicted in the “Attention weighted fusion”\nsection in Fig. 2. By deploying the proposed CAM at each scale of the feature pyramid,\nour module effectively captures inter-channel correlations across diverse scales. It encodes\npositional information along the horizontal and vertical axes, generating corresponding posi-\ntional weights. These weights are then utilized to perform a weighted fusion of the features.\nTherefore, it effectively mines the spatial location information of multi-scale text instances,\nso that the fused features pay more attention to the text instances and the ability of text\ndetection is enhanced. In our work, we utilize the four layers of features {C\n2,C3,C4,C5}\nextracted from the ResNet50 architecture as input to the CAM module. This yields weighted\nfeatures {C\n′\n2,C′\n3,C′\n4,C′\n5}. Afterward, the top-down fusion method is employed to fuse the\nhigher-level feature maps after upsampling twice with the adjacent lower-level feature maps\nin an element-wise addition manner to obtain the fused E\ni. The calculation process can be\n123\n52 Page 8 of 22 X. Li et al.\nrepresented as:\nEi =\n{ C′\n5,i = 5\nUp×2(Ei +1) + C′\ni ,i = 2,3,4 (7)\nwhere Up ×2(·) denotes upsampling by a factor of 2 utilizing the nearest-neighbor interpo-\nlation. After that, a 3 × 3 convolutional operation is employed to further adjust the channel\ndimensions of the features { E5, E4, E3, E2} to 1/4 of their original size, i.e., 64, and then\nupsampled to match the size of the E ′\n2. The resulting feature maps are ultimately concate-\nnated along the channel dimension, yielding the ﬁnal output feature map, denoted as Eout.\nThe whole procedure can be described as:\nE ′\ni = Up×2i −2 (Conv3(Ei )),i = 2,3,4,5( 8 )\nEout = Cat\n(\nE ′\n2, E ′\n3, E ′\n4, E ′\n5\n)\n(9)\nwhere Conv 3(·) represents a 3 × 3 convolutional operation, Up ×2i −2 (·) represents upsam-\npling using the nearest-neighbor interpolation by a factor of 2 i−2,a n dC a t ( · ) represents\nconcatenation along the channel dimension. In this way, an effective fusion of multi-scale\nfeatures is achieved.\n3.3 Global Feature Perception\nIn order to adequately capture global contextual information, avoid background information\nsimilar to text morphology being misclassiﬁed as text, and prevent text instances with seman-\ntic associations from being over-segmented, we propose a global feature perception method.\nSpeciﬁcally, the STB module is constructed which uses window multi-head between instance\nand background or instance and instance in images to further complement and enhance the\nfeatures captured by CNN.\nThe STB module consists of 8 cascaded Swin Transformer Layers, as illustrated in Fig. 4.\nEvery two consecutive Swin Transformer Layers form a group, as depicted in Fig. 5,t h e y\nare composed of Window Multi-head Self-Attention (W-MSA), Shifted Window Multi-head\nSelf-Attention (SW-MSA), Multilayer Perceptron (MLP), and Layer Norm(LN) layer. The\nLN layer is conﬁgured before the MSA and MLP modules, and the residual connection is\nadded afterward.\nDue to the input data format requirement of the Swin Transformer Layer, for input features\nwith a scale of ( B, C, H, W ), it is necessary to be ﬂattened along width and height dimensions\nEout Fe\nSwin Transformer Layer\nFig. 4 Overall structure of STB module. Consisting of 8 cascaded Swin Transformer Layers, STB is used for\nperceiving global features\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 9 of 22 52\nFig. 5 Structure of two successive\nSwin Transformer Layers\nand transformed into the format of ( B, H × W , C) before being fed into the Swin Transformer\nLayer. Since the self-attention calculation in Swin Transformer Layer is window-based, the\nfeature map needs to be divided into several windows of the same size before calculation.\nThe speciﬁc implementation process is as follows: First, the data format is transformed from\n(B, H × W , C)t o( B, H, W , C). Then, the feature map is divided into windows to obtain the\nfeatures in the data format ( N × B, Z, Z, C), where Z indicates the size of each window, and\nN indicates the number of windows. The calculation formula for N is as follows:\nN = (H\n/\nZ ) × (W\n/\nZ ) (10)\nThe calculation of W-MSA is performed within each window to learn the dependencies\nbetween pixels, and it is calculated as follows:\nAtt(Q, K , V ) = Softmax( QK\nT\n√dk\n+ BP )V (11)\nwhere Q, K,a n d V denote the query vector, key vector, and value vector, respectively. dk\ndenotes the dimension of the key vector, and BP denotes the relative position encoding.\nCompared with the traditional global self-attention calculation, the computational com-\nplexity of window-based self-attention is substantially reduced. However, it lacks information\ninteraction between adjacent windows. To remedy this deﬁciency, SW-MSA utilizes a shift-\ning window strategy to shift the local window of W-MSA by ( Z/2, Z/2) pixels, and performs\nmulti-headed self-attention computation within the shifted windows, thereby realizing the\ninteraction of cross-window information. The sequenced feature E\nout is computed in the suc-\ncessive Swin Transformer Layer with W-MSA and SW-MSA to obtain the enhanced feature\nF\ne. The computation procedure is as follows:\nˆFl\ne = W-MSA(LN(Fl−1\ne ))+Fl−1\ne (12)\nFl\ne = MLP(LN( ˆFl−1\ne ))+ ˆFl−1\ne (13)\nˆFl+1\ne = SW-MSA(LN(Fl\ne ))+Fl\ne (14)\nFl+1\ne = MLP(LN( ˆFl+1\ne ))+ ˆFl+1\ne (15)\n123\n52 Page 10 of 22 X. Li et al.\nwhere l represents the current index of the Swin Transformer Layer. ˆFl\ne and Fl\np denote the\noutput results of the (S)W-MSA module and MLP module, respectively. The STB module\nis composed of 8 layers of Swin Transformer Layer, so when l equals 8, F\nl\ne represents the\noutput of the STB module.\n3.4 Post-processing\n3.4.1 Differentiable Binarization\nSegmentation-based methods typically require binarization of the segmented features. In this\npaper, a differentiable binarization method [ 4] is employed. Firstly, probability maps of the\ntext regions and threshold maps of the text boundaries are predicted from the feature maps.\nThen, based on the pixel relationship between the text regions and the text boundaries, an\napproximate binary map is calculated using the following formula:\n˜B\ni ,j = 1\n1 + e−k(Pi ,j −Ti ,j ) (16)\nwhere Pi,j and T i,j indicate the pixel values at the coordinate point ( i, j) of the probability map\nand threshold map predicted from the segmentation network, respectively. ˜Bi ,j represent the\npixel values at the coordinate point ( i, j) of the approximate binary map. k is a hyperparameter,\nwhich is set to 50. Since Eq. ( 16) can be considered as an approximation of a step function\nand is differentiable, this binarization method can be integrated into the network to participate\nin training and optimization, thereby aiding in generating more accurate prediction results.\n3.4.2 Label Generation\nDuring the training period, the probability maps, threshold maps, and approximate binary\nmaps predicted by the network need to be supervised by the corresponding ground truth labels.\nTo this end, we apply the V atti clipping algorithm [ 32] to the ground truth text polygon I\n(e.g., red polygon in Fig. 6) to obtain expanded I\ne and shrunk polygons Is (e.g., blue and\nFig. 6 The process of label generation. The initial polygon, expanded polygon, and shrunk polygon are repre-\nsented by red, blue, and green, respectively\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 11 of 22 52\ngreen polygons in Fig. 6), respectively. The pixel offsets are calculated as:\nDoffset = Area(P) × (1 − r 2)\nPerimeter(P) (17)\nwhere Area (·) means calculating the area of a polygon, Perimeter (·) means calculating its\nperimeter, and r stands for the scale factor, which is set to 0.4 empirically. Both the approx-\nimate binary map and the probability map have the same labels and are derived from Is.\nMoreover, the region between Ie and Is represents the text boundary. The label for the thresh-\nold map is obtained by calculating the distance from the boundary to the original polygon\nI.\n3.5 Loss Function\nThe loss function L, which is composed of three portions during network training and opti-\nmization, can be written as follows:\nL = Lbi + λ1 Lpr + λ2 Lth (18)\nwhere Lbi, Lpr,a n d Lth stand for approximate binary map loss, probability map loss, and\nthreshold map loss, respectively. λ1 and λ2 are weight coefﬁcients used to balance the sig-\nniﬁcance of the three losses. They are set to 5 and 10, respectively, based on the numerical\nvalues of the three losses.\nThere is a problem of imbalanced positive and negative samples in natural scene text\nimages, i.e., the text region accounts for a small proportion of the image. Consequently,\nnetwork training may exhibit a biased preference for background information, overlooking\nthe essential text region. To address this problem, L\nbi is calculated by the Dice coefﬁcient.\nDice loss is an extensively employed loss function in the ﬁeld of image segmentation. It has\nan advantage in dealing with problems such as sample imbalance and boundary-blurring,\nbecause it does not rely on the class probability prediction of pixels, but is more concerned\nwith the degree of overlap between prediction results and the true labels. The computational\nformula is as follows:\nL\nbi = 1 − 2 ∑ N\ni =1 (xi × yi )\n∑ N\ni =1 xi + ∑ N\ni =1 yi + ε\n(19)\nwhere xi and yi represent the prediction and true label value of the i-th pixel in the approximate\nbinary map. ε represents the smoothing item used to avoid division by zero and reduce\noverﬁtting. In the experiment, it is set to 1e-6.\nWe adopt the Binary Cross-Entropy Loss (BCE Loss) for Lpr, which is commonly used\nin image classiﬁcation tasks. At the same time, online hard example Mining(OHEM) [ 33]\nis employed to sample the hard negative samples to further alleviate the sample imbalance\nproblem. L\npr can be calculated as:\nLpr =\n∑\ni ∈SP\ny′\ni log x ′\ni +\n(\n1 − y′\ni\n)\nlog\n(\n1 − x ′\ni\n)\n(20)\nwhere Sp represents a set with a ratio of positive and negative samples of 1:3. x ′\ni and y′\ni refer\nto the prediction and true label value of the i-th pixel in the probability map, respectively.\n123\n52 Page 12 of 22 X. Li et al.\nLth is calculated by the L1 distance, which is used to assess the similarity between the\nprediction and the true label. The calculation formula is\nLth =\n∑\ni ∈Re\n⏐⏐ˆyi −ˆxi\n⏐⏐ (21)\nwhere ˆxi represents the prediction value of the probability map, ˆyi represents the true label\nof the threshold map. Re represents the set of pixels in the expanded text polygon Ie.\n4 Experiments\n4.1 Datasets\nICDAR 2017-ML T [34] serves as a large-scale multilingual text dataset for natural scenes.\nComprising text images captured from diverse scenes across nine languages, the dataset\nencompasses 7,200 training images, 1,800 validation images, and an additional 9,000 test\nimages. The text regions within these images are meticulously annotated using the coordinates\nof the four vertices of bounding quadrilaterals. Notably, the training and validation sets were\nused during the experiments to pre-train the model in this paper.\nICDAR 2015 [35] serves as a prominent benchmark dataset extensively employed for\nmulti-oriented text detection. It is composed of 1000 training images and 500 test images. The\nimages in this dataset exhibit diverse characteristics, including multi-scale, multi-directional,\nand blurred attributes, which collectively pose the primary challenges when utilizing this\ndataset. Furthermore, the text regions within the images are meticulously annotated with the\ncoordinates of the four vertices.\nMSRA-TD500 [36] is a long text detection dataset that encompasses both English and\nChinese text. It features a wide array of text instances characterized by large aspect ratios\nand signiﬁcant scale variations. The dataset comprises 300 training images and 200 test\nimages. Given the relatively limited number of training images within this dataset, a strategy\nis adopted to augment the training phase by incorporating an additional set of 400 images\nfrom the HUST-TR400 dataset [ 37], following established methodologies [ 4, 17].\nTo t a l - Te x t[38] is a dataset for scene text detection that encompasses curved text instances\nof arbitrary shapes. The dataset contains horizontal, multi-oriented, and curved text instances.\nIt comprises 1,255 images for training and an additional 300 images for testing. The back-\nground diversity and curved text instances features presented in this dataset are the difﬁculties\nin using this dataset.\n4.2 Implementation Details\nIn our experiments, the model undergoes a two-stage training process. Initially, it is pre-\ntrained on the combined training and validation sets of the ICDAR 2017-MLT dataset.\nSubsequently, ﬁne-tuning is conducted for a total of 1200 epochs on the three datasets of\nICDAR2015, MSRA-TD500, and Total-Text. During the training process, the batch size is\nset to 4, and the stochastic gradient descent (SGD) algorithm serves as the optimizer for the\nnetwork. To ensure adaptive learning, the Poly Learning Rate Policy is implemented. This\ndynamic adjustment of the learning rate follows the formula l\nr = l0 ×\n(\n1 − epoch\nmax _epoch\n) p\n,\nwith the initial learning rate l0 set to 0.001 and p set to 0.9.\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 13 of 22 52\nDuring the training phase, three distinct data augmentation strategies are applied to the\noriginal training images to enhance the generalization ability of the network. Speciﬁcally,\nthe following strategies are utilized: (1) Randomly rotating images within a range of ( − 10°,\n10°); (2) Randomly ﬂipping images; (3) Randomly cropping images to 640 × 640 pixels\nwhile preserving the integrity of text instances. In the inference phase, the segmentation\nnetwork initially predicts pixel-level segmentation results, and then the binary representation\nof text pixels is obtained. Subsequently, the quadrilateral or polygonal text bounding boxes\nare derived using the OpenCV . To ensure the accuracy of the detection, the height of the\ntest images is adjusted to 1152, 736, and 800 pixels for the ICDAR2015, MSRA-TD500,\nand Total-Text datasets, respectively, while maintaining the aspect ratio of the images. All\nexperiments are conducted on a singular GPU (GeForce RTX 3090) with Intel® Xeon(R)\nGold 6330 CPU @ 2.00 GHz. The software framework employed for these experiments is\nPyTorch 1.8.1.\n4.3 Ablation Study\n4.3.1 Effectiveness Analysis of AWF and STB\nTo validate the impact of AWF and STB on the performance of ST-AWFNet, ablation exper-\niments are carried out on both the ICDAR2015 dataset and the MSRA-TD500 dataset. The\nperformance of the proposed method is evaluated using three key metrics: Precision (P),\nRecall (R), and F-measure (F). In these ablation experiments, the window size Z within STB\nis consistently set to 5. The experimental results are shown in Table 1, where the ﬁrst row of\ndata represents the performance of the baseline model under the same experimental setup.\nAll experiments are ﬁne-tuned on the ICDAR2017-MLT pre-trained model.\nFrom the ﬁrst two sets of experimental results displayed in Table 1, it can be observed\nthat replacing the feature pyramid network (FPN) of the baseline model with AWF has\nthe following effects: On the ICDAR2015 dataset, while maintaining the same precision, it\nachieves 1% gain in recall and 0.5% increase in F-measure. On the MSRA-TD500 dataset,\nnotable improvements are observed in all three metrics. These results indicate that AWF is\ncapable of capturing the positional information of text regions on multi-scale feature maps,\nallowing effective integration of informative features from adjacent layers while suppressing\nredundant background information. Consequently, it enhances the ability of text detection\nand localization.\nTable 1 Results of ablation experiments on ICDAR2015 and MSRA-TD500. “Backbone” all use ResNet50\nwith DCN. “FPN” indicates the feature fusion method used in the baseline model\nBackbone FPN AWF STB ICDAR2015 MSRA-TD500\nP/% R/% F/% P/% R/% F/%\n✓✓ 90.6 82.3 86.3 90.9 81.1 85.7\n✓✓ 90.6 83.3 86.8 92.8 84.4 88.4\n✓✓ ✓ 89.2 84.9 87.0 94.5 85.6 89.8\n✓✓ ✓ 91.3 84.7 87.9 94.1 88.0 90.9\n123\n52 Page 14 of 22 X. Li et al.\nComparing the ﬁrst and third sets of experiments, it can be seen that the introduction of\nthe STB module has the following effects: On the ICDAR2015 dataset, the recall reaches\nits optimal value of 84.9%, which outperforms the baseline by 2.6%. On the ICDAR2015\ndataset, the recall reaches the optimal value of 84.9%, which is a 2.6% improvement over\nthe baseline model. On the MSRA-TD500 dataset, characterized by larger aspect ratios and\nnotable scale variations in the text instances, the improvements are even more pronounced.\nThe precision, recall and F-measure surpass the baseline model by 3.6%, 4.5%, and 4.1%\nrespectively. This experiment demonstrates that the global feature interaction capability of\nthe STB module brings a signiﬁcant performance improvement for the text detection task.\nAs seen in the ﬁrst and fourth sets of experiments, replacing FPN with AWF while intro-\nducing the STB module can signiﬁcantly improve the detection results. Our method brings\n1.6% and 5.2% gains in F-measure on the two datasets, respectively. This indicates that\nintegrating the two modules can jointly promote the text detection effect and enhance the\nrobustness of the model.\n4.3.2 Inﬂuence of Window Size ( Z)\nThe window size in the Swin Transformer directly affects the size of the receptive ﬁeld\nand consequently impacts the detection results. Therefore, experiments are conducted on the\ndatasets of ICDAR2015 and MSRA-TD500 for different window sizes, denoted as Z.G i v e n\nthat both the width and height of the feature maps input to the module measure 160 pixels,\nand to adhere to the divisibility principle, Z is assigned the values 5, 8, and 16, respectively.\nThe experimental outcomes are presented in Table 2.\nFor the ICDAR2015 dataset, the best results are obtained when Z equals 5, and as the\nwindow expands, accuracy progressively diminishes. This phenomenon is rooted in the pre-\nponderance of small to medium-scale text instances within this dataset. Employing smaller\nwindow sizes for partitioning the input features tends to yield superior outcomes in such cases.\nHowever, the MSRA-TD500 dataset, characterized by a greater presence of long and large-\nscale text instances, yields its ﬁnest results with Z set as 8. Smaller windows can preserve\nmore detailed information, while larger windows can capture a broader range of contextual\ninformation, but incur more location coding and attention computation, thus increasing the\ncomputational demand and training difﬁculty of the model. Taking into account the scale\ndistribution of the target instances in the datasets and resource limitations, we ultimately set\nZ as 5 and 8 for the ICDAR2015 and MSRA-TD500 datasets, respectively.\nTable 2 Results of different Z values on ICDAR2015 and MSRA-TD500\nZ ICDAR2015 MSRA-TD500\nP/(%) R/(%) F/(%) P/(%) R/(%) F/(%)\n5 91.3 84.7 87.9 94.1 88.0 90.9\n8 90.6 84.6 87.5 95.2 88.0 91.4\n16 90.3 84.6 87.4 93.6 87.6 90.5\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 15 of 22 52\n4.4 Comparisons with State-of-the-art Methods\nTo verify the robustness and effectiveness of the proposed ST-AWFNet method, the com-\nparative and analytical study is conducted against state-of-the-art methods from recent years\nusing three distinct datasets: the multi-oriented text dataset ICDAR2015, the long text dataset\nMSRA-TD500, and the curved text dataset Total-Text. Hyperparameter Z is conﬁgured as\n5, 8, and 5 for the respective datasets. It is worth noting that FPS is for reference only, as\ndifferent GPUs are adopted in different methods.\nEvaluation of multi-oriented text detection: The ICDAR2015 dataset presents chal-\nlenges in text detection due to the prevalence of numerous small-scale, low-resolution, and\nmulti-oriented text instances. As shown in Table 3, the proposed method achieves the best\nperformance in terms of F-measure, with a score of 87.9%. The precision also achieved\nthe second-best result. Compared to the classical segmentation-based methods PSENet [ 23]\nand PAN [ 24], the proposed ST-AWFNet outperforms them by 2.2% and 5% on F-measure,\nrespectively. In comparison to DBNet [ 4] and DBNet++ [ 27], the improved recall makes a\nhigher F-measure by 0.6%. Additionally, ST-AWFNet has advantages in F-measures com-\npared to FAST [ 28]. Qualitative detection results on ICDAR2015 are shown in Fig. 7a,\nrevealing the effectiveness of our method in tackling the above challenges, and further reduc-\ning the possibility of false positives in complex backgrounds.\nEvaluation of long text detection: The MSRA-TD500 dataset has the challenge of\nextreme aspect ratio and large variation in the scale of the text instances. As illustrated\nin Table 4, the proposed method achieves state-of-the-art results, with precision, recall, and\nF-measure reaching 95.2%, 88.0%, and 91.4%, respectively. Compared with the classical\nTable 3 Comparison results on ICDAR2015. The best and second-best results are highlighted in bold and\nunderlined respectively\nModel V enue ICDAR2015\nP/(%) R/(%) F/(%) FPS\nPSENet [ 23] CVPR 2019 84.5 86.9 85.7 1.6\nPAN [24] ICCV 2019 84.0 81.9 82.9 26.1\nContourNet [ 20] CVPR 2020 87.6 86.1 86.9 3.5\nDRRG [ 39] CVPR 2020 88.5 84.7 86.6 –\nDBNet [ 4] AAAI 2020 91.8 83.2 87.3 12\nFCENet [ 40] CVPR 2021 90.1 82.6 86.2 –\nOPMP [ 41] TMM 2021 89.1 85.5 87.3 1.4\nFAST [28] arXiv 2021 89.7 84.6 87.1 15.7\nDText [42] PR 2022 88.5 85.6 87.0 –\nKPN [ 43] TNNLS 2022 88.3 84.8 86.5 6.3\nTextDCT [44] TMM 2022 88.9 84.8 86.8 7.5\nDBNet++ [ 27] TPAMI 2022 90.9 83.9 87.3 10\nFS [ 45] TIP 2022 89.8 82.7 86.1 12.1\nLPAP [46] TOMM 2023 88.7 84.8 86.5 –\nLeafText [47] TMM 2023 88.9 82.3 86.1 –\nST-AWFNet (Ours) – 91.3 84.7 87.9 5.0\n123\n52 Page 16 of 22 X. Li et al.\nFig. 7 Comparison of qualitative detection results on ICDAR2015, MSRA-TD500, and Total-Text datasets.\nFrom left to right, the ground truth (blue bounding boxes), the ﬁnal detection results of our method (green\nbounding boxes), and binary maps predicted by the segmentation network in our work are shown\nPAN [ 24] based on segmentation methods, all three metrics have been improved substan-\ntially. When compared to DBNet [ 4] and DBNet++ [ 27], the proposed method surpasses them\nby 6.5% and 4.2% in F-measure, respectively. Moreover, our ST-AWFNet displays substan-\ntial advantages compared to the latest MixNet [ 13] method. Compared to other methods, our\nmethod also offers signiﬁcant advantages in detection accuracy while ensuring appropriate\ndetection speed. Qualitative detection results on MSRA-TD500 are shown in Fig. 7b, indi-\ncating the superiority and robustness of the proposed method in detecting multi-language\nlong straight text instances.\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 17 of 22 52\nTable 4 Comparison results on MSRA-TD500. The best and second-best results are highlighted in bold and\nunderlined respectively\nModel V enue MSRA-TD500\nP/(%) R/(%) F/(%) FPS\nPAN [24] CVPR 2019 84.4 83.8 84.1 30.2\nDRRG [ 39] CVPR 2020 88.1 82.3 85.1 –\nDBNet [ 4] AAAI 2020 91.5 79.2 84.9 32\nOPMP [ 41] TMM 2021 86.0 83.4 84.7 1.6\nFAST [28] arXiv 2021 92.1 83.0 87.3 56.8\nPCR [ 48] CVPR 2021 90.8 83.5 87.0 –\nDText [42] PR 2022 87.9 83.1 85.4 –\nDBNet++ [ 27] TPAMI 2022 91.5 83.3 87.2 29\nFS [ 45] TIP 2022 89.3 81.6 85.3 25.4\nLPAP[46] TOMM 2023 87.9 77.7 82.5 –\nLeafText [47] TMM 2023 92.1 83.8 87.8 –\nMixNet [ 13] arXiv 2023 90.7 88.1 89.4 –\nST-AWFNet (Ours) – 95.2 88.0 91.4 16.5\nEvaluation of curved text detection: Curved text presents greater challenges compared to\nquadrilateral text. To further demonstrate the applicability of the proposed method for curved\ntext detection, experiments are conducted on the Total-Text dataset. As shown in Table 5,t h e\nexperimental results indicate that the proposed method achieves the best F-measure of 86.7%.\nMoreover, there are improvements across all metrics in comparison to both DBNet [ 4]a n d\nDBNet++ [27], with the F-measure increasing by 2% and 0.7% respectively. Additionally, all\nthe metrics of ST-AWFNet are superior to TextFuseNet [ 26], which also uses ResNet50 as the\nbackbone. For the speed, our ST-AWFNet can also inference at an appropriate speed, which\nhas a degree of ascendancy compared to some previous methods. Qualitative detection results\non the Total-Text dataset are shown in Fig. 7c, where the contours of curved text instances\nare accurately ﬁtted, further conﬁrming the competitive performance of the proposed method\nin curved text detection.\nTo intuitively demonstrate the effectiveness of the proposed method in tackling chal-\nlenges such as missed detection of large-scale variation text instances, over-segmentation\nof wide-spaced text instances, and false detections in complex background texture, a\ndetailed comparative analysis is conducted on the MSRA-TD500, contrasting the DBNet\nand DBNet++ algorithms. As depicted in Fig. 8, from left to right, the ground truth labels,\nDBNet algorithm detection results, DBNet++ algorithm detection results, and the detection\nresults of our ST-AWFNet are shown. In the top row of images, it can be observed that\nboth DBNet and DBNet++ missed the detection of large-scale variation text instances, while\nthe proposed method successfully detects all the text instances. In the second row of images,\nalthough DBNet++ detects all the text instances, it erroneously segments vertically associated\ninstances as horizontal fragments. Conversely, our method successfully detects the correct\ntext bounding boxes, demonstrating its capability to capture long-distance dependencies and\ncontextual information. As can be seen from the third row of images, our method effectively\n123\n52 Page 18 of 22 X. Li et al.\nTable 5 Comparison results on Total-Text. The best and second-best results are highlighted in bold and under-\nlined respectively\nModel V enue Total-Text\nP/(%) R/(%) F/(%) FPS\nPSENet [ 23] CVPR 2019 84.0 80.0 80.9 3.9\nPAN [24] CVPR 2019 89.3 81.0 85.0 39.6\nContourNet [ 20] CVPR 2020 86.9 83.9 85.4 3.8\nDRRG [ 39] CVPR 2020 86.5 84.9 85.7 –\nDBNet [ 4] AAAI 2020 87.1 82.5 84.7 32\nTextFuseNet-R50 [26] ICJAI 2020 83.2 87.5 85.3 7.1\nFCENet [ 40] CVPR 2021 89.3 82.5 85.8 –\nOPMP [ 41] TMM 2021 88.5 82.9 85.6 1.4\nPCR [ 48] CVPR 2021 88.5 82.0 85.2 –\nKPN [ 43] TNNLS 2022 88.0 82.3 85.1 22.7\nTextDCT [44] TMM 2022 87.2 82.7 84.9 15.1\nDBNet++ [ 27] TPAMI 2022 88.9 83.2 86.0 28\nFS [ 45] TIP 2022 88.7 79.9 84.1 24.3\nLPAP[46] TOMM 2023 87.3 79.8 83.4 –\nST-AWFNet (Ours) – 89.8 83.8 86.7 17.4\navoids missed detections and over-segmentation of wide-spaced text instances in the hor-\nizontal direction. The results in the fourth row demonstrate that the proposed method can\ndifferentiate background pixels that resemble text texture and accurately detect small-scale\ntext instance in the upper right corner. Overall, from the comparison of the visualization\nresults, it is demonstrated that the proposed method exhibits higher performance in challeng-\ning scenes.\n4.5 Runtime Analysis\nIn the inference phase, the runtime of our method is primarily inﬂuenced by the network infer-\nence and post-processing steps. For different datasets, the running time also varies depending\non the number of text instances and the scale of the input image. We calculate the average\nrunning time of each image on the ICDRA2015, MSRA-TD500, and Total-Text datasets, as\ns h o w ni nT a b l e6. This indicates that our model can complete scene text detection within\nan appropriate runtime. Meanwhile, the FPS metrics of our model on the three datasets (as\nshown in Tables 3, 4, 5) further prove the efﬁciency of proposed method.\n5 Conclusion\nIn this paper, we propose a scene text detection algorithm ST-AWFNet combining Swin\nTransformer and attention-weighted fusion. Attention-weighted fusion module is designed\nin the feature fusion part to replace the traditional FPN, aiming to fully exploit location infor-\nmation of text regions in multi-scale features and enhance the capability of feature fusion.\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 19 of 22 52\nFig. 8 Comparison of qualitative detection results with Ground Truth, DBNet, and DBNet++ on MSRA-TD500\nTable 6 Runtime of our proposed\nmethod Dataset Image scale Time(ms)\nICDAR2015 1152 × 2048 198\nMSRA-TD500 736 × 736 61\nTotal-Text 800 × 800 66\nIn addition, leveraging the global modeling advantage of Swin Transformer, the algorithm\nlearns the correlations among different pixel information and captures the contextual depen-\ndencies. Experimental results on three public datasets demonstrate the competitiveness of\nthe proposed algorithm, effectively improving the ability to detect text instances with large-\nscale variation and wide spacing, as well as the ability to distinguish background pixels that\nare morphologically similar to the text instance. In future work, we will further investigate\nlightweight model design and post-processing methods to optimize the network model for\nimproved detection speed. Additionally, we will explore the integration of text recognition\ntechniques to achieve efﬁcient end-to-end text detection and recognition.\nAcknowledgements This research is supported by the Tianjin \"Project+Team\" Key Training Special Project\n(Grant No. XB202007), and the Science and Technology Support of Tianjin Key Research and the Development\nPlan Project (Grant No. 18YFZCGX00930). The authors truly appreciate the constructive criticism provided\nby anonymous reviewers.\n123\n52 Page 20 of 22 X. Li et al.\nAuthors’ Contributions XL contributed to the design of research methodology, data collection, and results\nanalysis. XY conducted experiments and drafted the manuscript. YL provided supervision and guidance\nthroughout the research process and revised the manuscript.\nFunding This research is supported by the Tianjin \"Project + Team\" Key Training Special Project (Grant\nNo. XB202007), and the Science and Technology Support of Tianjin Key Research and the Development Plan\nProject (Grant No. 18YFZCGX00930).\nData Availability The datasets used in this research are publicly available in reference [ 34–38].\nDeclarations\nConﬂict of interest All authors declare that they have no conﬂict of interest in this research.\nEthical Approval This work does not involve the participation of any human or animal subject.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\n1. Tian Z, Huang W, He T, He P , Qiao Y (2016) Detecting text in natural image with connectionist text\nproposal network. Computer vision—ECCV 2016. Springer International Publishing, Cham, pp 56–72\n2. Ren S, He K, Girshick R, Sun J (2017) Faster r-cnn: towards real-time object detection with region\nproposal networks. IEEE Trans Pattern Anal Mach Intell 39(6):1137–1149\n3. Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu CY , Berg AC (2016) Ssd: single shot multibox\ndetector. Computer vision—ECCV 2016. Springer International Publishing, Cham, pp 21–37\n4. Liao M, Wan Z, Yao C, Chen K, Bai X (2020) Real-time scene text detection with differentiable bina-\nrization. In: Proceedings of the AAAI conference on artiﬁcial intelligence, pp 11474–11481\n5. Hu J, Shen L, Sun G (2018) Squeeze-and-excitation networks. In: Proceedings of the IEEE conference\non computer vision and pattern recognition, pp 7132–7141\n6. Woo S, Park J, Lee JY , K I S (2018) Cbam: convolutional block attention module. In: 2018 European\nconference on computer vision (ECCV), pp 3–19\n7. Wang Q, Wu B, Zhu P , Li P , Zou W, Hu Q (2020) ECA-Net: efﬁcient channel attention for deep convo-\nlutional neural networks. In: Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp 11534–11542\n8. Hou Q, Zhou D, Feng J (2021) Coordinate attention for efﬁcient mobile network design. In: Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, pp 13713–13722\n9. Huang Z, Wang X, Huang L, Huang C, Wei Y , Liu W (2019) Ccnet: criss-cross attention for semantic\nsegmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pp 603–612\n10. V aswani A, Shazeer N, Parmar N, Parmar N, Uszkoreit J, Jones L, Gomez A, Kaiser Ł, Polosukhin I\n(2017) Attention is all you need. Advances in neural information processing systems, pp 5998–6008\n11. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D et al (2021) An image is worth 16 ×16 words:\ntransformers for image recognition at scale. arXiv preprint arXiv: 2010.11929\n12. Raisi Z, Y ounes G, Zelek J (2022) Arbitrary shape text detection using transformers. In: 2022 26th\nInternational Conference on Pattern Recognition (ICPR). IEEE, pp 3238–3245\n13. Zeng YX, Hsieh JW, Li X, Chang MC (2023) MixNet: toward accurate detection of challenging scene\ntext in the wild. arXiv preprint arXiv:2308.12817\n14. Liu Z, Lin Y , Cao Y , Hu H, Wei Y , Zhang Z, Lin S, Guo B (2021) Swin transformer: hierarchical\nvision transformer using shifted windows. In: Proceedings of the IEEE/CVF international conference on\ncomputer vision (ICCV), pp 10012–10022\n123\nCombining Swin Transformer and Attention-Weighted Fusion … Page 21 of 22 52\n15. Liao M, Shi B, Bai X, Wang X, Liu W (2017) Textboxes: a fast text detector with a single deep neural\nnetwork. In: Proceedings of the thirty-ﬁrst AAAI conference on artiﬁcial intelligence, pp 4161–4167\n16. Liao M, Shi B, Bai X (2018) Textboxes++: a single-shot oriented scene text detector. IEEE Trans Image\nProcess 27(8):3676–3690\n17. Zhou X, Yao C, Wen H, Wang Y , Zhou S, He W, Liang J (2017) East: an efﬁcient and accurate scene text\ndetector. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp\n5551–5560\n18. Shi B, Bai X, Belongie S (2017) Detecting oriented text in natural images by linking segments. In:\nProceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp 2550–2558\n19. Zhang C, Liang B, Huang Z, En M, Han J, Ding E, Ding X (2019) Look more than once: an accurate\ndetector for text of arbitrary shapes. In: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition (CVPR), pp 10552–10561\n20. Wang Y , Xie H, Zha ZJ, Xing M, Fu Z, Zhang Y (2020) Contournet: taking a further step toward accurate\narbitrary-shaped scene text detection. In: Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition (CVPR), pp 11753–11762\n21. Long J, Shelhamer E, Darrell T (2015) Fully convolutional networks for semantic segmentation. In:\nProceedings of the IEEE conference on computer vision and pattern recognition (CVPR), pp 3431–3440\n22. Deng D, Liu H, Li X, Cai D (2018) Pixellink: detecting scene text via instance segmentation. In: Pro-\nceedings of the thirty-second AAAI conference on artiﬁcial intelligence. AAAI Press, pp 6773–6780\n23. Wang W, Xie E, Li X, Hou W, Lu T, Y u G, Shao S (2019) Shape robust text detection with progressive\nscale expansion network. In: Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition (CVPR), pp 9336–9345\n24. Wang W, Xie E, Song X, Zang Y , Wang W, Lu T, Y u G, Shen C (2019) Efﬁcient and accurate arbitrary-\nshaped text detection with pixel aggregation network. In: Proceedings of the IEEE/CVF International\nconference on computer vision(ICCV), pp 8439–8448\n25. Baek Y , Lee B, Han D, Y un S, Lee H (2019) Character region awareness for text detection. In: Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp 9357–9366\n26. Ye J, Chen Z, Liu J, Du B (2020) TextFuseNet: scene text detection with richer fused features. Proc IJCAI\n20:516–522\n27. Liao M, Zou Z, Wan Z, Yao C, Bai X (2022) Real-time scene text detection with differentiable binarization\nand adaptive scale fusion. IEEE Trans Pattern Anal Mach Intell 45(1):919–931\n28. Chen Z, Wang J, Wang W, Chen G, Xie E, Lou P , Lu T (2021) FAST: faster arbitrarily-shaped text detector\nwith minimalist Kernel representation. arXiv preprint arXiv:2111.02394\n29. He K, Zhang X, Ren S, Sun J (2016) Deep residual learning for image recognition. In: Proceedings of\nthe IEEE conference on computer vision and pattern recognition (CVPR), pp 770–778\n30. Dai J, Qi H, Xiong Y , Li Y , Zhang G, Hu H, Wei Y(2017) Deformable convolutional networks. In:\nProceedings of the IEEE international conference on computer vision (ICCV), pp 764–773\n31. Lin T, Doll´ar P , Girshick R, He K, Hariharan B, Belongie S (2017) Feature pyramid networks for object\ndetection. In: Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR),\npp 936–944\n32. V atti BR (1992) A generic solution to polygon clipping. Commun ACM 35(7):56–63\n33. Shrivastava A, Gupta A, Girshick RB (2016) Training region-based object detectors with online hard\nexample mining. In: Proceedings of the IEEE Conference on computer vision and pattern recognition\n(CVPR), pp 761–769\n34. Nayef N, Yin F, Bizid I, Choi H, Feng Y , Karatzas D, Luo Z, Pal U, Rigaud C, Chazalon J, Khlif W,\nLuqman MM, Burie J, Liu C, Ogier J (2017) ICDAR2017 robust reading challenge on multi-lingual scene\ntext detection and script identiﬁcation-rrc-mlt. In: 2017 14th IAPR International Conference on document\nanalysis and recognition (ICDAR), pp 1454–1459\n35. Karatzas D, Gomez-Bigorda L, Nicolaou A, Ghosh S, Bagdanov A, Iwamura M, Matas J, Neumann L,\nChandrasekhar VR, Lu S, Shafait F, Uchida S, V alveny E (2015) Icdar 2015 competition on robust reading.\nIn: 2015 13th international conference on document analysis and recognition (ICDAR), pp 1156–1160\n36. Yao C, Bai X, Liu W, Ma Y , Tu Z (2012) Detecting texts of arbitrary orientations in natural images. In:\n2012 IEEE conference on computer vision and pattern recognition (CVPR), pp 1083–1090\n37. Yao C, Bai X, Liu W (2014) A uniﬁed framework for multi oriented text detection and recognition. Image\nProcess IEEE Trans 23(11):4737–4749\n38. Chng CK, Chan CS (2017) Total-text: a comprehensive dataset for scene text detection and recognition.\nIn: 14th IAPR International conference on document analysis and recognition (ICDAR), pp 935–942\n39. Zhang S, Zhu X, Hou J, Liu C, Yang C, Wang H, Yin X (2020) Deep relational reasoning graph network\nfor arbitrary shape text detection. In: Proceedings of the IEEE/CVF Conference on computer vision and\npattern recognition(CVPR), pp 9696–9705\n123\n52 Page 22 of 22 X. Li et al.\n40. Zhu Y , Chen J, Liang L, Kuang Z, Jin L, Zhang W (2021) Fourier contour embedding for arbitrary-shaped\ntext detection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n(CVPR), pp 3123–3131\n41. Zhang S, Liu Y , Jin L, Wei Z, Shen C (2020) OPMP: an omnidirectional pyramid mask proposal network\nfor arbitrary-shape scene text detection. IEEE Trans Multimed 23:454–467\n42. Cai Y , Liu YY , Shen C, Jin L, Li Y , Ergu D (2022) Arbitrarily shaped scene text detection with dynamic\nconvolution. Pattern Recognit 127:108608\n43. Zhang SX, Zhu X, Hou JB, Yang C, Yin XC (2022) Kernel proposal network for arbitrary shape text\ndetection. IEEE Trans Neural Netw Learn Syst 1–12\n44. Su Y , Shao Z, Zhou Y , Meng H, Zhu H, Liu B, Yao R (2022) Textdct: arbitrary-shaped text detection via\ndiscrete cosine transform mask. IEEE Trans Multimed 1–14\n45. Wang F, Xu X, Chen Y , Li X (2022) Fuzzy semantics for arbitrary-shaped scene text detection. IEEE\nTrans Image Process 32:1–12\n46. Fu Z, Xie H, Fang S, Wang Y , Xing M, Zhang Y (2023) Learning pixel afﬁnity pyramid for arbitrary-\nshaped text detection. ACM Trans Multimed Comput 19(1s):1–24\n47. Yang C, Chen M, Y uan Y , Wang Q (2023). Text growing on leaf. IEEE Trans Multimed 1–14\n48. Dai P , Zhang S, Zhang H, Cao C (2021) Progressive contour regression for arbitrary-shape scene text\ndetection. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition\n(CVPR), pp 7393–7402\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123",
  "topic": "Computational intelligence",
  "concepts": [
    {
      "name": "Computational intelligence",
      "score": 0.7219729423522949
    },
    {
      "name": "Transformer",
      "score": 0.5426905155181885
    },
    {
      "name": "Artificial intelligence",
      "score": 0.525974690914154
    },
    {
      "name": "Fusion",
      "score": 0.5063267946243286
    },
    {
      "name": "Computer science",
      "score": 0.5002760887145996
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47566717863082886
    },
    {
      "name": "Linguistics",
      "score": 0.14092576503753662
    },
    {
      "name": "Engineering",
      "score": 0.13871511816978455
    },
    {
      "name": "Electrical engineering",
      "score": 0.09566318988800049
    },
    {
      "name": "Philosophy",
      "score": 0.09481596946716309
    },
    {
      "name": "Voltage",
      "score": 0.06364616751670837
    }
  ]
}