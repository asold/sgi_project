{
    "title": "Optimizing the Clinical Application of Rheumatology Guidelines Using Large Language Models: A Retrieval-Augmented Generation Framework Integrating EULAR and ACR Recommendations",
    "url": "https://openalex.org/W4409353819",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2734168775",
            "name": "Alfredo Madrid-Garcia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226652655",
            "name": "Diego Benavent",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146583838",
            "name": "Chamaida Plasencia-Rodríguez",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3110542793",
            "name": "Zulema Rosales Rosado",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2724549067",
            "name": "Beatriz Merino-Barbancho",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2953279847",
            "name": "Dalifer Freites-Nuñez",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226652655",
            "name": "Diego Benavent",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4402585946",
        "https://openalex.org/W4386897457",
        "https://openalex.org/W4406179873",
        "https://openalex.org/W6851775633",
        "https://openalex.org/W1991839905",
        "https://openalex.org/W4404534210",
        "https://openalex.org/W4408108874",
        "https://openalex.org/W4402909345",
        "https://openalex.org/W4389685420",
        "https://openalex.org/W2949769604",
        "https://openalex.org/W4400902310",
        "https://openalex.org/W4405728682",
        "https://openalex.org/W4406813548",
        "https://openalex.org/W4406421570",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4389984066",
        "https://openalex.org/W6929646337",
        "https://openalex.org/W4409645678",
        "https://openalex.org/W4405030403",
        "https://openalex.org/W4399530612",
        "https://openalex.org/W6875657730",
        "https://openalex.org/W6929235096",
        "https://openalex.org/W4406542750",
        "https://openalex.org/W4391849320",
        "https://openalex.org/W4387800467",
        "https://openalex.org/W4405031432",
        "https://openalex.org/W6874886628",
        "https://openalex.org/W4409067446",
        "https://openalex.org/W4395050972",
        "https://openalex.org/W4392597393",
        "https://openalex.org/W4409157497",
        "https://openalex.org/W4406260843",
        "https://openalex.org/W4408225015",
        "https://openalex.org/W2003535046"
    ],
    "abstract": "Abstract Objectives Timely access to current rheumatology guidelines at the point of care is challenging. We aimed to develop and evaluate the first Retrieval-Augmented Generation (RAG) system specifically designed for adult rheumatology, integrating European Alliance of Associations for Rheumatology (EULAR) and American College of Rheumatology (ACR) guidelines to provide rheumatologists with timely, evidence-based recommendations at the point of care. Methods EULAR and ACR management guidelines were selected by rheumatologists based on their clinical relevance for decision making and processed. A RAG system was implemented using LangChain framework, voyage-3 embedding model, and a Qdrant vector database. To evaluate the system, ten questions per guideline were generated using ChatGPT 4.5 . Answers to these guideline-specific questions were subsequently produced by ChatGPT-o3-mini with context retrieval (RAG) and without (baseline). Performance was assessed by an LLM-as-a-judge ( Gemini 2.0 Flash ) using a 5-point Likert scale across five dimensions: relevance, factual accuracy, safety, completeness, and conciseness. The judge also determined preference between the RAG and baseline responses. Statistical significance was established using Wilcoxon signed-rank and Binomial tests. For further validation, two blinded rheumatologists independently evaluated a random sample of questions (15%). Results After agreement, 74 guidelines were included, and 740 evaluation questions were generated. Analysis revealed that the RAG system significantly outperformed the baseline system across all criteria (p&lt;0.001) in the LLM-as-a-judge evaluation. Manual evaluation by rheumatologists confirmed these findings (p&lt;0.001 for accuracy, safety, completeness). Furthermore, the RAG system was significantly preferred by the LLM-as-a-judge in 92.8% of comparisons (p&lt;0.001) and by the human evaluators in 71.2%-74.8% of comparisons (p&lt;0.001). Conclusion This study demonstrates the successful development and evaluation of a RAG system integrating extensive EULAR/ACR guidelines for adult rheumatology. The system significantly improves answer quality compared to a baseline LLM. This provides a robust foundation for reliable, AI-driven clinical decision support tools designed to enhance guideline adherence and evidence-based practice in rheumatology by providing clinicians with rapid, context-aware access to recommendations. Graphical abstract Key messages Large language models, combined with EULAR and ACR guidelines, may enhance rheumatology clinical decision support. Retrieval augmented generation (RAG) responses showed significantly greater accuracy, safety and completeness than baseline LLMs. RAG is a promising architecture for reducing hallucinations and providing grounded, reliable answers.",
    "full_text": " \nOptimizing the Clinical Application of Rheumatology \nGuidelines Using Large Language Models: A Retrieval-\nAugmented Generation Framework Integrating ACR \nand EULAR Recommendations \n \nAlfredo Madrid -García1,§,*, Diego Benavent 2,3,§, Beatriz Merino -\nBarbancho4,†,*, Dalifer Freites-Núnez5,† \n1 No affiliation \n2 Rheumatology Department, Hospital Universitari de Bellvitge, Barcelona, Spain \n3 Medical Department, Savana Research SL, Madrid, Spain \n4 Escuela Técnica Superior de Ingenieros de Telecomunicación. Universidad Politécnica de \nMadrid. Avenida Complutense, Madrid, Spain  \n5 Grupo de Patología Musculoesquelética. Hospital Clínico San Carlos. Instituto de \nInvestigación Sanitaria San Carlos (IdISSC), Madrid, Spain \n§ Have contributed equally \n† Have contributed equally \n \n* Corresponding author details:  \nAlfredo Madrid García, PhD. \ne-mail: alfredo.madrid.garcia@alumnos.upm.es \nBeatriz Merino Barbancho, PhD. \nUniversidad Politécnica de Madrid \nEscuela Técnica Superior de Ingenieros de Telecomunicación. Universidad Politécnica de \nMadrid, Avenida Complutense, 30, Madrid, 28040, Spain \ne-mail: bmerino@lst.tfo.upm.es \n \nORCiD \nAlfredo Madrid García: 0000-0002-1591-0467  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \nBeatriz Merino-Barbancho: 0000-0001-5070-4178 \nDiego Benavent: 0000-0001-9119-5330 \nDalifer Freites Nuñez: 0000-0002-0966-2778 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nAbstract (202 words) \nObjectives: \nTo develop and evaluate a Retrieval -Augmented Generation (RAG) system integrating \nEuropean Alliance of Associations for Rheumatology (EULAR) and American College of \nRheumatology (ACR) guidelines to provide rheumatologists with timely, evidence -based \nrecommendations at the point of care. \nMethods: \nEULAR and ACR and management guidelines were selected by rheumatologists according to \nrelevance to clinical decision making, processed, and chunked. A RAG system using LangChain \nframework, voyage-3 embedding model, and a Qdrant vector database was implemented. \nAnswers to 740 guideline-specific questions were generated by ChatGPT-o3-mini with context \nretrieval (RAG) and without (baseline). Performance was evaluated using an LLM -as-a-judge \n(Gemini 2.0 Flash) assessing factual accuracy, sa fety, completeness, faithfulness, and \npreference, with Wilcoxon signed-rank and Binomial tests for statistical significance. \nResults:  \nAfter agreement, 74 guidelines were included. The RAG-based system received consistently \nhigher or comparable medians than the baseline across all criteria, relevance, factual \naccuracy, safety, completeness and conciseness (p<0.001). Moreover, the RAG-based system \nwas significantly preferred by the LLM-judge in 92.8% of comparisons (p<0.001). \nConclusion:  \nThis study demonstrates the successful development and validation of a RAG system \nintegrating extensive ACR/EULAR guidelines. The system significantly improves answer \nquality compared to a baseline LLM, providing a promising foundation for reliable, AI -driven \nclinical decision support tools in rheumatology to enhance guideline adherence. \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nGraphical abstract \n \nKey words: Retrieval augmented generation, large language models, rheumatology, clinical \nguidelines, clinical decision support system, artificial intelligence \nKey messages:  \n● Large language models, combined with EULAR and ACR guidelines, may enhance \nrheumatology clinical decision support. \n● Retrieval augmented generation (RAG) responses showed significantly greater \naccuracy, safety and completeness than baseline LLMs. \n● RAG is a promising architecture for reducing hallucinations and providing grounded, \nreliable answers. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nIntroduction \nLarge language models (LLMs) have emerged as a key component of modern generative \nartificial intelligence (AI). These models, trained on extensive text corpora, leverage advanced \nnatural language processing (NLP) techniques to generate human -like text, positioning them \nas essential tools in various disciplines, including medicine [1–3] . \nOver the past years, LLMs have experienced growth in size, capability, availability, context \nwindow, and cost-efficiency. In parallel, advanced reasoning models integrating multimodal \ninputs and enhanced contextual awareness have emerged, further broadening the scope of \ntheir appl ications [4,5]. LLMs have demonstrated their capacity to generate coherent \nresponses to free -text queries, even when not explicitly trained for specific tasks. Their \nadaptability makes them attractive candidates for applications in clinical practice, ranging \nfrom differential diagnosis support to the synthesis of structured medical reports. Moreover, \nAgents integrating these advancements are now capable of performing sophisticated, \nautonomous tasks across diverse sectors [6]. \nThese advances have catalysed interest among medical professionals, particularly in fields \nrequiring extensive textual interpretation and with wide practice pattern variation, such as \nrheumatology [7]. To manage this inherent complexity and strive for standardized, evidence-\nbased care, the field places significant emphasis on the use of clinical guidelines that \nincorporate the diverse expertise and perspectives of multiple stakeholders, to promote \nconsistent decision -making. Therefore, these  clinical guidelines issued by entities like the \nEuropean Alliance of Associations for Rheumatology  (EULAR) and the  American College of \nRheumatology (ACR) play a key role in guiding complex treatment decisions. \nNevertheless, LLMs associated challenges remain. A key limitation of current LLMs is their \ntendency to generate \"hallucinations\"—plausible yet inaccurate or misleading information, a \nparticularly concerning issue in clinical settings where erroneous outputs could impact patient \nsafety [8]. Medical hallucinations pose a critical challenge by using plausible-sounding clinical \nlanguage and domain-specific terminology to hide errors in diagnostics, treatment planning, \nand test result interpretation—areas where mistakes can have immediate consequences [9]. \nTheir coherent logic makes these inaccuracies difficult to spot without expert scrutiny, and \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nunrecognized errors can delay proper treatment or misdirect care, especially in settings that \nincreasingly rely on AI-generated recommendations. \nRecent studies have demonstrated that while LLMs can assist in drafting patient information \nand generating preliminary clinical documentation, their outputs require meticulous human \nvalidation to ensure accuracy and reliability [10,11]. Within rheumatology, where diseases \nexhibit multifaceted clinical presentations, diagnostic and therapeutic decisions depend on \nsynthesizing diverse data sources—clinical history, laboratory findings, imaging, and patient-\nreported outcomes —making the pre cision of AI -generated recommendations even more \ncritical [12,13]. \nTo mitigate these challenges, advanced methodologies such as retrieval -augmented \ngeneration (RAG) have been proposed  [14]. RAG architectures enhance the accuracy of AI \nmodels by anchoring generative outputs to verified sources, thereby reducing the likelihood \nof hallucinations and improving clinical reliability [15,16]. Given these considerations, our \nstudy aims to develop and validate a novel RAG framework integrating ACR and EULAR \nrecommendations. This framework is designed to facilitate the automated extraction and \nsynthesis of key recommendations from complex guidel ine documents, aiding clinicians in \nmaking informed, evidence -based decisions. While clinical guidelines are essential for \nstandardizing care and translating research into practice, their effective adoption by clinicians \nfaces significant hurdles, includin g the time required to synthesize complex information. By \nproviding rapid, synthesized, and context -specific access to guideline recommendations, we \naim to bridge the gap between guideline availability and practical implementation, directly \ntackling the challenge of embedding evidence -based medicine into daily  clinical decision -\nmaking, while reinforcing both the quality and consistency of care.  \nPrimary objective \nThe primary objective of this study is to integrate ACR and EULAR rheumatology \nrecommendations into a RAG system enabling  clinicians to access timely, evidence -based \nrecommendations at the point of care. By seamlessly merging established guidelines with \nLLMs, we aim to enhance clinical decision -making and improve adherence to best practices \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nin rheumatology. Hence, the solution proposed is  intended to prove how RAG architectures \nand LLMs could assist the rheumatologist in its daily activities. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nRetrieval-augmented generation \nRAG is not a new concept [17]. However, only in recent years with the popularisation of LLMs, \nhas it been widely adopted as a means to enhance LLM performance by integrating external \nknowledge sources into the generation process  [18], RAG enables LLMs to access external \nknowledge bases and incorporate that information as context for generating grounded and \nfactual responses. This approach offers several advantages: it allows the integration of \nproprietary and private data without additional training, produces more grounded responses \nmitigating medic al hallucinations [9,19], and enables the use of up -to-date information \nbeyond the model’s training cutoff. Additionally, it enhances user trust by clearly indicating \nthe sources from which the information was extracted increasing user’s reliability. Various \nRAG architectures have been proposed in recent years, incorporating modifications to their \ndifferent components  [20]. A general RAG schema is shown in Figure 1  and contains the \nfollowing elements: \nA) Retriever \n1. Documents intended for storage in the vector database, which serve as \ncontextual support for enhancing LLM -generated responses, undergo \ncollection, processing, and chunking. The chunking step is crucial, as it \nfacilitates more accurate alignment between us er queries and relevant text \nsegments, thereby reducing noise and filtering out irrelevant information. \nThese text chunks are then converted into vector representations, \nsubsequently stored and indexed within the vector database for efficient \nretrieval. \n2. User queries submitted to the LLM are also embedded into vector \nrepresentations using the same embedding model employed for document \nembedding, ensuring consistency and improving matching accuracy. \n3. The vector representation of the user query is compared against vectors stored \nin the vector database through similarity search techniques, retrieving the k \nmost similar document chunks relevant to the query. \nB) Generator \n4. An enhanced prompt is constructed by combining the user query and the \nretrieved chunks from the vector database. This prompt is then provided to \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nthe LLM, enabling it to generate an informed and contextually grounded \nresponse. \nModifications to this schema leads to the different RAG architectures. However, all of them \nshare the same core idea: to create a module that runs before the LLM is called that gets \nrelevant textual data to enhance the answer. \nRAG performance is determined by multiple factors, including the type of retrieved \ndocuments, the extent to which the system accurately recalls relevant content, the number \nof retrieved documents appended to the prompt, and the prompt design itself, among others. \nRecent work shows that distracting documents degrade performance while well -aligned \ndocuments and prompt techniques can boost correctness and confidence [21]. For instance, \nrecently, Barnett at al. identified seven common failure points of RAG systems, underscoring \nthat every component —from data preprocessing to chunking, retrieval, and final answer \nextraction—must be carefully designed [22]. \nTo overcome some of these challenges different RAG architectures have been proposed in \nrecent years [23–25]. \nIn healthcare, RAG has gained considerable traction because it promotes equity, reliability, \nand personalization [14–16] Recently, a comprehensive introduction to RAG architecture in \nhealthcare along with a detailed explanation of its underlying mechanisms was published by \nNg et al  [14]  The authors highlighted how RAG can enhance both patient care (e.g., \npersonalized discharge summaries) and pharmaceutical research (e.g., clinical trial \nscreenings), demonstrating its versatility and impact across multiple medical domains. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nMethods \n1. Data selection \nTwo independent rheumatologists, DFN and DB, reviewed the full set of EULAR and ACR \nrecommendations to identify those most relevant to clinical decision -making within a \npotential rheumatology clinical support system decision used during daily consultations. The \nlist of available guidelines was extracted from EULAR [26] and ACR [27] websites, see \nSupplementary Excel Initial List of Recommendations . Initially, only the  Classification and \ndiagnosis criteria/response criteria and the Recommendations for management sections were \nconsidered. After agreement, the established consensus-based criteria followed for selecting \nthe guidelines were: \n● Classification criteria guidelines were excluded, as they were considered less relevant \nfor specialized rheumatological practice. \n● Guidelines targeting other healthcare professionals (e.g., nurses) or pediatric \npopulations were excluded, as the system was designed to support adult \nrheumatologist consultations. \n● To ensure clinical relevance and accuracy, only the most recent versions of each \nidentified guideline were included, and outdated recommendations were excluded. \nThe rationale was that new versions  incorporate the most pertinent information from \nearlier iterations. \nUpon completing the independent selection, the recommendations chosen by the two \nrheumatologists were compared, and any discrepancies were resolved by mutual agreement. \n2. Data collection and processing \nAfter selection, the agreed recommendations were retrieved in . PDF format (i.e., published \nscientific articles). Supplementary information from the recommendations was not retrieved. \nVarious parsing tools were evaluated based on their ability to accurately and efficiently \nextract text from PDFs, and subsequently convert it into a machine -readable format, see \nSupplementary Table 1 . Ultimately, LlamaParse was selected due to its performance, \ncustomizable options, and community support. The LlamaParse API was used, and the output \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nwas downloaded in markdown .md  format to preserve structural clarity and semantic \nsections. After download, a manual cleaning of the clinical recommendations was conducted: \nnon-essential sections (e.g., references, affiliation listings) were manually removed to \nminimize noise and eliminate redundancy, the headings and subheadings were reviewed, and \nthe tables and boxes were moved to the end.  To preserve the figures' information an \nadvanced reasoning model, ChatGPT o1, was employed to describe them. Prompt A was used, \nsee Table 1. \nThe description of the figures was attached at the end of the markdown .md file. Once the \nguidelines were cleaned, Prompt B was used to automatically generate a set of 10 questions \nper recommendation—designed to simulate realistic user interactions — that later could be \nused for evaluating the RAG architecture. ChatGPT 4.5 was employed for enhanced creativity. \nto simulate realistic user interactions  \nThe generated questions were manually reviewed by DFN and DB, those out of the scope of \nthe guideline were replaced with new ones (n = 14). \nMoreover, PubMed metadata for each recommendation was retrieved using PubMed API and \nE-utilities. Metadata included the PMID, title, publication year, DOI, society (i.e., EULAR, ACR), \nauthors, citation, first author, journal, create date, NIHMS ID, PMC, MeSH terms, and abstract. \nLangChain served as the default framework for both experimentation and the development \nof the RAG architecture. Text -splitting (i.e., chunking) was guided by Markdown \ndemarcations, ensuring a coherent segmentation of the guidelines -based on headings - for \nsubsequent embedding. The rationale was that information grouped under specific headings \nis thematically cohesive, which in turn enhances retrieval accuracy and contextual relevance \nin downstream tasks. UnstructuredMarkdownLoader was used for that purpose. Finally, the \nlength of the chunks was measured to ensure they fit within the context window. \n3. Embeddings, vectorstore deployment and data ingestion \nThe chunked text was transformed into embeddings —numerical vector representations of \nwords that capture their semantic properties in a continuous space —. The choice of the \nembedding model was guided by the MTEB (Massive Text Embedding Benchmark) \nleaderboard on Hugging Face, prioritizing models that demonstrated superior performance \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \non retrieval tasks. The final selection balanced factors such as accuracy, availability, domain \nrelevance, embedding dimensionality, cost, and requests per minute. Consequently, voyage-\n3 from Voyage AI—the second best performing embedding model in the MTEB and optimized \nfor general-purpose and multilingual retrieval quality—was chosen. This model has a context \nlength of 32,000 tokens and a 1024 embedding dimension. \nAll chunks, together with the PubMed metadata were subsequently vectorized using voyage-\n3 and stored in Qdrant vector database, a scalable vector database capable of storing, \nindexing, and retrieving the guidelines chunks according to semantic similarity [28,29]. \nThe vector database was deployed locally using Docker, enabling efficient retrieval of \nguideline segments. This setup ensured secure and rapid access without relying on external \ninfrastructure. \n4. Hybrid search \nThe RAG system was implemented via the LangChain framework. An initial similarity search \nwas performed based on the user query to retrieve the top k=4 most relevant chunks, \nleveraging the semantic embeddings stored in Qdrant. In this process, dense embeddings \nwere computed using voyage-3, while sparse embeddings were derived with BM25. Finally, \nonly chunks with a similarity score above 0.5 were retained. The distance metric for \nquantifying the similarity between the user’s questions and the vector embedding s was the \ncosine distance. \n5. Answer generation \nTo generate concise, evidence -informed answers, Prompt C  adapted from LangChain \ndocumentation was used. Moreover, a predefined schema was integrated into the answer \ngeneration step to present a list of sources including title, year and DOI, drawn from the \nretrieved guidelines. This schema ensured transparent attribution of the guidelines utilized in \nformulating the final answer, thus enhancing the trustworthiness of the system. \nOpenAI API was used to generate the answers, and the chosen model was ChatGPT o3-mini \n(i.e., LLM 1). The rationale was that utilizing a reasoning model could provide enhanced \nanswers both with and without the application of RAG. Additionally, costs were considered in \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nthe selection of the model, balancing performance with affordability. Temperature parameter \nwas not supported by this model. \n6. System testing and evaluation \nDifferent evaluation techniques have been proposed for RAG systems  [30]. To evaluate the \nperformance of the RAG system, the LLM -as-a-judge paradigm was used —where a LLM is \nprompted to act as a reviewer and quantitatively score the model-generated responses [31]. \nGemini 2.0 Flash  was the LLM used as a judge (i.e., LLM 2). The rationale behind choosing \nGemini 2.0 Flash  was based on its performance, reduced cost, and large context window, \nwhich was crucial to include all the required information. Moreover, LangSmith was used as \nthe observability framework to monitor and trace the entire RAG pipeline execution, \ncapturing intermediate steps such as document retrieval, prompt composition, and LLM \noutputs. \nThe evaluation was conducted in tree steps: \n1. Creation of answers with RAG: o3-mini (LLM1) was prompted, Prompt C, along with \nthe set of questions previously generated by ChatGPT-4.5. In this configuration, \nrelevant documents were retrieved and included as context in the prompt, allowing \nthe model to generate grounded responses based on external information. Therefore, \nthe input to the system consisted of the user question and the retrieved context. \n2. Creation of answers without RAG: o3-mini (LLM1) was prompted, Prompt D, with the \nsame set of ChatGPT-4.5 questions. No additional context was provided —responses \nrelied solely on the model’s internal knowledge and the final input consisted of the \nuser question. \n3. Evaluation: Gemini 2.0 Flash (LLM2) was prompted, Prompt E , for conducting the \nevaluation. The next elements were used as input: user question, original guideline, \nretrieved context, RAG system response (i.e., step 1) and the response without RAG \n(i.e., step 2). The evaluation rubric, which includes criteria for relevance, factual \naccuracy, safety, completeness, and conciseness, is presented in Table 2. \nFigure 2 shows the steps taken in this study. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \n7. Statistical analysis \nTo determine if the observed differences between the RAG system and the baseline (without \nRAG) were statistically significant, the paired nature of the evaluation data was used. \nDichotomous and categorical variables were summarized using proportions. Conti nuous \nvariables were summarized using the median and the first and third quartiles (Q1 –Q3). \nQuantitative scores (1 -5 ratings) for each evaluation criterion, Table 2 , were compared \nbetween the two systems using the non -parametric Wilcoxon Signed -Rank test, as it is \nsuitable for paired ordinal data and does not assume a normal distribution of score \ndifferences. Separate tests were planned for each evaluation criterion. \nFor the categorical preference data ('A' preferred, 'B' preferred, 'Comparable'), a Binomial \nTest was planned. This test compares the frequency of preference for the RAG system ('B') \nversus the baseline ('A'), excluding 'Comparable' responses, to determine  if one system was \nsignificantly preferred over the other against a null hypothesis of equal preference (50%). A \none-sided greater approach was followed, to determine if the RAG system was chosen \nsignificantly more often than the baseline. \nMoreover, the Wilcoxon Signed -Rank test was used to compare the 'completeness given \nretrieval' score (i.e., considering only the retrieved context) with the 'completeness overall' \nscore (i.e., considering the full guideline) to identify any significant difference. \nA significance level alpha of p < 0.05 was set to define statistical significance. All analyses were \nperformed using Python. \nDifferent technical alternatives were implemented and can be seen in the Supplementary \nText.  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nResults \nA total of 74 recommendations, 24 from ACR and 50 from EULAR, were included, see \nSupplementary Excel File List of Recommendations. The 740 associated questions generated \nto evaluate the RAG system, and reviewed by rheumatologists,  are available in the \nSupplementary Excel File Questions . These recommendations covered a wide range of \nrheumatic conditions. The most frequently represented categories were inflammatory \narthritis (including rheumatoid arthritis, psoriatic arthritis, and axial spondyloarthritis), with \n16 guidelines. Vasculitides were the target of 8 guidelines. Systemic lupus erythematosus, and \njuvenile idiopathic arthritis each accounted for 5 guidelines. Osteoarthritis was addressed in \n4 guidelines, encompassing general, hip, knee, and hand involvement, similar to crystal \narthropathies. Systemic conditions such as systemic sclerosis and antiphospholipid syndrome \nwere each the focus of 2 guidelines. Less frequent topics included particular diseases such as \nBechet’s disease, Still’s disease, Sjögren disease, fibromyalgia, familiar  mediterranean fever \nand macrophage activation syndrome. A subset of guidelines addressed broader aspects such \nas fatigue, reproductive health, vaccinations, comorbidities, or therapeutic strategies not \nspecific to a single disease category. \n \nQuantitative Evaluation \nA total of 740 question –answer pairs were evaluated across predefined quality dimensions \nusing a structured rubric, with Gemini 2.0 Flash serving as the LLM evaluator under the LLM-\nas-a-judge paradigm. \nStatistics demonstrating that RAG -based answers consistently outperformed baseline \nresponses, can be seen in Table 3.  \nThe RAG -based system (B) received consistently higher or comparable medians than the \nbaseline (A) across all criteria, with small p -values indicating statistically significant \ndifferences. Both systems scored at or near the upper boundary for Relevance, Sa fety, and \nConciseness, but B’s Factual Accuracy median reached 5.0 (interquartile range 5.0 –5.0), \ncontrasting with A’s slightly lower median of 5.0 (4.0 –5.0). Completeness (Overall) also \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nshowed a statistically significant improvement for B, even though both systems posted \nmedians around 4.0.  \nThe Faithfulness to retrieved context and completeness given retrieval median values for RAG-\nbased answers were 5.0 (5.0 -5.0). Furthermore, the binomial test on preference strongly \nfavored system B (92.8% vs. 7.2%), reinforcing the conclusion that the RAG -based approach \nyielded superior performance within this evaluation framework. All differences were \nstatistically significant (p<0.001). \nPreference-Based Evaluation \nIn addition to quantitative scoring, the evaluator was tasked with identifying the preferred \nanswer for each question (Prompt E). Among 614 pairwise comparisons with a definitive \npreference, the RAG-generated response was favored in 570 instances (92.8%), compared to \nonly 44 (7.2%) for the baseline model. This difference was statistically significant as per the \nbinomial test. The comparison confidence, according to the LLM, was high for 739 instances \n(99.9%). \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nDiscussion \n \nThis study demonstrates that a RAG system integrating EULAR and ACR guidelines improves \nanswer quality compared to a baseline model, offering enhanced factual accuracy, safety, and \ncompleteness with strong adherence to context. The RAG -based answers consis tently \noutperformed baseline responses across evaluated metrics, with significant increases in \nrelevance, factual accuracy, safety, completeness and conciseness. Additionally, the RAG -\ngenerated responses were strongly preferred by evaluators in 92.8% of co mparisons, \nhighlighting its reliability as an evidence-based clinical support tool in rheumatology. \nBuilding on these promising performance outcomes, we introduce the first RAG system \nspecifically designed to optimize adult rheumatology management While similar RAG \napproaches have been applied in other medical disciplines—such as hepatology, nephrology, \nand preoperative medicine —our system represents a novel, tailored application for \nrheumatology, with the potential to offer clinicians a specialized tool to optimize patient care. \nA recent scoping review identified 31 studies in which RAG was used in clinical decision \nsupport, healthcare education, and pharmacovigilance [32].  \nFor instance, in  Kresevic et al. [33],  the authors integrated the \" EASL recommendations on \ntreatment of hepatitis C: Final update of the series \" clinical guideline into a RAG framework \nusing GPT-4 Turbo to enhance clinical decision support. They transformed non -text sources \ninto text-based lists, standardized the guideline structure, and employed targeted prompt \nengineering, resulting in an accur acy of 99% compared to a baseline of 43%. Although the \nstudy focused on a single guideline, it demonstrate d that structured reformatting and \ndomain-specific prompts can markedly improve the outputs of large language models. \nOn the other hand, Miao et al. [34], showcased the integration of GPT-4 with the “KDIGO 2023 \nClinical Practice Guideline for the Evaluation and Management of Chronic Kidney Disease ” \nguideline to address specialized nephrology queries. They reformatted the text, developed a \ncustom ChatGPT model, and evaluated its performance in specialized nephrology queries. \nAlthough only one guideline was included, the study underscored RAG’s ability to generate \nup-to-date, contextually relevant answer s while reducing inaccuracies. Their findings \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nhighlight the importance of structured data integration and targeted prompt engineering to \nenhance factual precision in clinical settings.  \nA recent study in preoperative medicine developed and evaluated a GPT-4 based RAG model \nusing perioperative guidelines to assess patient fitness for surgery and generate preoperative \ninstructions [35]. The model outperformed human evaluators in predicting medical fitness for \nsurgery (96.4% vs 86.6%) and showed high accuracy (93.0%) in determining if a patient should \nbe seen by a nurse or doctor. The RAG system demonstrated high reproducibility, safety, and \nlower hallucination rates compared to some baseline LLMs. Although human evaluators were \nbetter at generating specific medication instructions, overall accuracy across various tasks \nwas comparable. This study reinforces the potential of RAG frameworks  to enhance clinical \ndecision-making by applying specific guidelines accurately, ensuring consistency, and \nimproving safety in complex medical workflows. \nThe clinical implications of this framework are significant. In a field as dynamic as \nrheumatology, where diagnostic criteria and treatment protocols continuously evolve, the \nability to rapidly and accurately interpret guideline recommendations could subst antially \nimprove patient outcomes. By providing concise, evidence-based summaries, this system has \nthe potential to reduce cognitive load, streamline decision -making, and enhance adherence \nto best practices. Moreover, this initiative aligns with broader healthcare digitalization efforts, \nleveraging AI and NLP technologies to optimize clinical workflows. When properly \nimplemented, LLMs can serve as invaluable tools for documentation, patient communication, \nand complex diagnostic reasoning, although their use  must be accompanied by rigorous \nvalidation to mitigate risks associated with algorithmic biases and outdated training data. \nIntegrating LLMs with a RAG framework represents a significant step forward in applying AI \nto rheumatology guideline interpretation.  \nFuture directions \nFuture directions may involve extending the current RAG approach through the development \nof specialized AI agents capable of dynamically interacting with both patients and healthcare \nprofessionals [6]. By leveraging iterative feedback loops, these agents could refine treatment \nrecommendations in real time, better adapt to nuanced clinical contexts, and ultimately \nimprove patient outcomes. They can also integrate multimodal data and personalized patient \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \ninformation for more precise decision -making. The synergy between AI agents, advanced \nlanguage models, and robust guideline frameworks are anticipated to substantially transform \nthe practice of rheumatology. \nFinally, other approaches such as GraphRAG [36] or other RAG improvements such as \nreranking, or contextual compression could be explored  [37]. However, we chose to test a \nsimplified RAG architecture to maintain a more focused scope for our initial experiments and \nestablish a clear baseline for future work.  \nLimitations \nThis study is not without limitations. To start with, some recommendations contained \nsupplementary data that were not integrated into the RAG system despite their potential \nrelevance. Another notable limitation of this study is the exclusion of classification criteria \nguidelines. While these criteria may be reconsidered for future primary -care–focused tools, \nthey were deemed to be less applicable to the specialized context of rheumatological \npractice.  \nMoreover, this research has been limited to clinical recommendations; however, \nincorporating additional data sources —such as rheumatology drug leaflets —could enhance \nthe functionality of a RAG system aimed at supporting rheumatologists in patient \nmanagement [38]. \nOn the other hand, recent work underscores that LLMs can unintentionally produce “device-\nlike” clinical decision support, thus falling under regulatory scrutiny as potential medical \ndevices [39]. Although current disclaimers assert, they are not for formal medical guidance, \nreal-world usage increasingly ventures into regulated territory. This situation underscores the \nneed for updated frameworks to ensure safety, accuracy, and equitable outcomes when LLM-\nbased tools are deployed. Fut ure developments should focus on refining model outputs to \nadhere to approved indications and clarifying regulatory lines between simple advisory tools \nand full -fledged medical devices. Such balanced oversight can prom ote innovation in \nrheumatology guideline implementation while safeguarding patient care. \nAlthough the standardization of clinical guidelines is not a novel concept  [40], a primary \nlimitation identified in this study was the absence of a uniform format and standardized \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nstructure within the guidelines and their accompanying tables. The variability in color codes, \nsymbols, and layout conventions across recommendations compounded the complexity of \nautomated processing, extraction, and interpretation. Establishing a uniform approach for \nguideline drafting and table notation could substantially improve data accuracy and \nconsistency, thereby enhancing the reliability of RAG methods. \nIn this study, the chunking process was carried out using markdown delimiters, specifically \nhashtags. This approach, although straightforward to implement, may adversely impact RAG. \nSince RAG is highly dependent on the integrity of contextual relationships  embedded in the \nsource text, the absence of hierarchical preservation can lead to diminished retrieval fidelity \nand, consequently, reduced overall performance. \nFinally, this study's reliance on an LLM -based evaluation paradigm for assessing the RAG \nsystem's outputs represents a notable limitation. Although this automated approach enables \nefficient, large-scale assessment, it lacks the nuanced clinical perspective provided by expert \nrheumatologists. Consequently, the findings may not fully capture the system's practical \nutility or safety in clinical settings, as subtle inaccuracies or contextual misinterpretations \nmight be overlooked. Future studies should incorpor ate human expert reviews to validate \nthese results and ensure clinical appropriateness. \nConclusion \nThis work demonstrates the successful integration of rheumatology-specific clinical guidelines \ninto a RAG system that could assist physicians at the point of care. To our knowledge, it is the \nfirst initiative in rheumatology to leverage such an extensive c ollection of guidelines, \nhighlighting its potential impact in advancing guideline -based practice. By consolidating \nmultiple recommendations into a single, AI -driven platform, our study sets a foundational \nframework for subsequent, more robust developments in RAG-based clinical decision support \nsystems.  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nAcknowledgements:  \nNot applicable \nGroup authorship list: \nNot applicable \nFunding: \nThis study did not receive funding. AMG covered the expenses for the API calls to the different \nservices \nAuthor approval: \nAll authors have seen and approved the manuscript. \nConflict of interest statement: \nDB has received payment honoraria for lectures, presentations, speakers bureaus,or support \nfor meeting attendance from AbbVie, Galapagos, Janssen, UCB, Pfizer, Novartis, support for \nattending meetings from UCB, Novartis and AbbVie. He works part-time as Advisor at Savana \nResearch, company on AI in medicine. AMG works at Roche as data scientist. \nData availability statement:  \nThe clinical guidelines used in this study are freely available. All LLM prompts are \ndocumented, and the code is available upon individual request \nFor further details or additional information, please contact the corresponding authors.  \nSupplementary File with Questions, Answers, and the Evaluation  contains the data used to \nevaluate the RAG system performance. \nEthics statement:  \nThis study did not require ethics approval, as it did not involve patient data or any information \ngoverned by the GDPR. All data employed in this research are publicly available, ensuring that \nno sensitive personal information was used. Moreover, the propo sed system is intended \nsolely as a support tool for rheumatologists, with final clinical decisions remaining the \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nexclusive responsibility of qualified physicians. Therefore, no formal ethical committee \napproval was required for this study as only publicly available, non -patient-specific guideline \ndocuments were utilized. \nReferences \n \n1  Benavent D, Madrid-García A. Large language models and rheumatology: are we there yet? \nRheumatol Adv Pract. Published Online First: 18 September 2024. doi: 10.1093/rap/rkae119 \n2  Mannstadt I, Mehta B. Large language models and the future of rheumatology: assessing \nimpact and emerging opportunities. Curr Opin Rheumatol. 2024;36:46–51. doi: \n10.1097/BOR.0000000000000981 \n3  Sequí-Sabater JM, Benavent D. Artificial intelligence in rheumatology research: what is it \ngood for? RMD Open. 2025;11:e004309. doi: 10.1136/rmdopen-2024-004309 \n4  Matarazzo A, Torlone R. A Survey on Large Language Models with some Insights on their \nCapabilities and Limitations. Published Online First: 3 January 2025. \n5  Zhao WX, Zhou K, Li J, et al. A Survey of Large Language Models. 2023. \n6  Wang W, Ma Z, Wang Z, et al. A Survey of LLM-based Agents in Medicine: How far are we \nfrom Baymax? Published Online First: 16 February 2025. \n7  Homik JE, Suarez-Almazor ME. Clinical guidelines: a must for rheumatology? Best Pract Res \nClin Rheumatol. 2000;14:649–61. doi: 10.1053/berh.2000.0105 \n8  Huang L, Yu W, Ma W, et al. A Survey on Hallucination in Large Language Models: Principles, \nTaxonomy, Challenges, and Open Questions. ACM Trans Inf Syst. 2025;43:1–55. doi: \n10.1145/3703155 \n9  Kim Y, Jeong H, Chen S, et al. Medical Hallucinations in Foundation Models and Their Impact \non Healthcare. Published Online First: 25 February 2025. \n10  Tam TYC, Sivarajkumar S, Kapoor S, et al. A framework for human evaluation of large \nlanguage models in healthcare derived from literature review. NPJ Digit Med. 2024;7:258. \ndoi: 10.1038/s41746-024-01258-7 \n11  Madrid-García A, Rosales-Rosado Z, Freites-Nuñez D, et al. Harnessing ChatGPT and GPT-4 for \nevaluating the rheumatology questions of the Spanish access exam to specialized medical \ntraining. Sci Rep. 2023;13:22129. doi: 10.1038/s41598-023-49483-6 \n12  Gossec L, Kedra J, Servy H, et al. EULAR points to consider for the use of big data in rheumatic \nand musculoskeletal diseases. Ann Rheum Dis. 2020;79:69–76. doi: 10.1136/annrheumdis-\n2019-215694 \n13  Madrid-García A, Merino-Barbancho B, Freites-Núñez D, et al. From Web to RheumaLpack: \nCreating a Linguistic Corpus for Exploitation and Knowledge Discovery in Rheumatology. \nComput Biol Med. 2024;179:108920. doi: 10.1016/j.compbiomed.2024.108920 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \n14  Ng KKY, Matsuba I, Zhang PC. RAG in Health Care: A Novel Framework for Improving \nCommunication and Decision-Making by Addressing LLM Limitations. NEJM AI. 2025;2. doi: \n10.1056/AIra2400380 \n15  Yang R, Ning Y, Keppo E, et al. Retrieval-augmented generation for generative artificial \nintelligence in health care. npj Health Systems. 2025;2:2. doi: 10.1038/s44401-024-00004-1 \n16  Liu S, McCoy AB, Wright A. Improving large language model applications in biomedicine with \nretrieval-augmented generation: a systematic review, meta-analysis, and clinical \ndevelopment guidelines. Journal of the American Medical Informatics Association. Published \nOnline First: 15 January 2025. doi: 10.1093/jamia/ocaf008 \n17  Lewis P, Perez E, Piktus A, et al. Retrieval-Augmented Generation for Knowledge-Intensive \nNLP Tasks. ArXiv. Published Online First: 22 May 2020. \n18  Gao Y, Xiong Y, Gao X, et al. Retrieval-Augmented Generation for Large Language Models: A \nSurvey. ArXiv. Published Online First: 18 December 2023. \n19  Huang L, Yu W, Ma W, et al. A Survey on Hallucination in Large Language Models: Principles, \nTaxonomy, Challenges, and Open Questions. ACM Trans Inf Syst. 2025;43:1–55. doi: \n10.1145/3703155 \n20  Li S, Stenzel L, Eickhoff C, et al. Enhancing Retrieval-Augmented Generation: A Study of Best \nPractices. Published Online First: 13 January 2025. \n21  Zhao S, Huang Y, Song J, et al. Towards Understanding Retrieval Accuracy and Prompt Quality \nin RAG Systems. Published Online First: 28 November 2024. \n22  Barnett S, Kurniawan S, Thudumu S, et al. Seven Failure Points When Engineering a Retrieval \nAugmented Generation System. Proceedings of the IEEE/ACM 3rd International Conference \non AI Engineering - Software Engineering for AI. New York, NY, USA: ACM 2024:194–9. \n23  Han H, Wang Y, Shomer H, et al. Retrieval-Augmented Generation with Graphs (GraphRAG). \nPublished Online First: 31 December 2024. \n24  Fan T, Wang J, Ren X, et al. MiniRAG: Towards Extremely Simple Retrieval-Augmented \nGeneration. Published Online First: 11 January 2025. \n25  Singh A, Ehtesham A, Kumar S, et al. Agentic Retrieval-Augmented Generation: A Survey on \nAgentic RAG. Published Online First: 15 January 2025. \n26  European Alliance of Associations for Rheumatology. EULAR Recommendations and \nInitiatives. 2025. \n27  American College of Rheumatology. Clinical Practice Guidelines. 2025. \n28  Taipalus T. Vector database management systems: Fundamental concepts, use-cases, and \ncurrent challenges. Cogn Syst Res. 2024;85. doi: 10.1016/j.cogsys.2024.101216 \n29  Han Y, Liu C, Wang P. A Comprehensive Survey on Vector Database: Storage and Retrieval \nTechnique, Challenge. Published Online First: 18 October 2023. \n30  de Lima RT, Gupta S, Berrospi C, et al. Know Your RAG: Dataset Taxonomy and Generation \nStrategies for Evaluating RAG Systems. Published Online First: 29 November 2024. \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \n31  Gu J, Jiang X, Shi Z, et al. A Survey on LLM-as-a-Judge. Published Online First: 23 November \n2024. \n32  Bunnell DJ, Bondy MJ, Fromtling LM, et al. Bridging AI and Healthcare: A Scoping Review of \nRetrieval-Augmented Generation—Ethics, Bias, Transparency, Improvements, and \nApplications. Published Online First: 1 April 2025. doi: 10.1101/2025.04.01.25325033 \n33  Kresevic S, Giuffrè M, Ajcevic M, et al. Optimization of hepatological clinical guidelines \ninterpretation by large language models: a retrieval augmented generation-based \nframework. NPJ Digit Med. 2024;7:102. doi: 10.1038/s41746-024-01091-y \n34  Miao J, Thongprayoon C, Suppadungsuk S, et al. Integrating Retrieval-Augmented Generation \nwith Large Language Models in Nephrology: Advancing Practical Applications. Medicina (B \nAires). 2024;60:445. doi: 10.3390/medicina60030445 \n35  Ke YH, Jin L, Elangovan K, et al. Retrieval augmented generation for 10 large language models \nand its generalizability in assessing medical fitness. NPJ Digit Med. 2025;8:187. doi: \n10.1038/s41746-025-01519-z \n36  Han H, Wang Y, Shomer H, et al. Retrieval-Augmented Generation with Graphs (GraphRAG). \nPublished Online First: 31 December 2024. \n37  Gao Y, Xiong Y, Gao X, et al. Retrieval-augmented generation for large language models: A \nsurvey. arXiv preprint arXiv:231210997. 2023. \n38  Madrid-García A, Merino-Barbancho B, Freites-Núñez D, et al. From Web to RheumaLpack: \nCreating a Linguistic Corpus for Exploitation and Knowledge Discovery in Rheumatology. \nComput Biol Med. 2024;179:108920. doi: 10.1016/j.compbiomed.2024.108920 \n39  Weissman GE, Mankowitz T, Kanter GP. Unregulated large language models produce medical \ndevice-like output. NPJ Digit Med. 2025;8:148. doi: 10.1038/s41746-025-01544-y \n40  Shiffman RN, Shekelle P, Overhage JM, et al. Standardized Reporting of Clinical Practice \nGuidelines: A Proposal from the Conference on Guideline Standardization. Ann Intern Med. \n2003;139:493–8. doi: 10.7326/0003-4819-139-6-200309160-00013 \n  \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nTables \n \nTable 1: Prompts used in this study \nPrompt Use \nPrompt A “Describe the following figure \n{figure}”.  \nPrompt B “Based on the clinical guidelines and recommendations \nprovided for Rheumatology, please generate 10 questions \nthat a physician might ask at the point of care \nregarding specific rheumatologic conditions. Ensure \nthat the questions are neutral, exploratory, and \nreflect genuine inquiries that a physician might \nrealistically have while seeking information. Do not \nexplicitly reference the clinical guideline or use \nphrases like \"according to this guideline.\" Each \nquestion should clearly mention the disease in question \nand avoid making implicit assumptions. \nPlease, find the clinical guideline below: \n{clinical_guideline}” \nPrompt C “You are an assistant for question-answering tasks. Use \nthe following pieces of retrieved context to answer the \nquestion.  \nIf you don't know the answer, just say that you don't \nknow. Use seven sentences maximum and keep the answer \nconcise.  \nQuestion: {question} \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nContext: {context} \nAnswer:” \nPrompt D “You are a rheumatologist specializing in question-\nanswering tasks. Answer concisely using no more than \nseven sentences. \nQuestion: {question}” \nPrompt E Role: You are an impartial AI Evaluation Assistant \nspecializing in clinical information accuracy and \nsafety, tasked with evaluating answers against a \nprovided clinical guideline. \n \nObjective: Evaluate and compare two answers (Answer A: \nBaseline LLM, Answer B: RAG LLM) provided for a \nspecific question. You have access to the full clinical \nguideline for verifying accuracy and safety. You must \nalso assess Answer B's faithfulness to the specific \ncontext snippet it was given during generation. \n \nInput Information: \n \n1.  Clinical Guideline Name: {guideline_name} \n2.  Question: {question} \n3.  Full Guideline Context: {full_guideline_context} \n4.  Retrieved Context (Provided *only* to RAG LLM for \ngenerating Answer B): {retrieved_context_str} \n5.  Answer A (Baseline LLM - Generated without \ncontext): {answer_baseline_llm} \n6.  Answer B (RAG LLM - Generated using ONLY the \n'Retrieved Context' above): {answer_rag_llm} \n \nEvaluation Instructions: \n \nCarefully analyze both Answer A and Answer B against \nthe `Full Guideline Context` and the specific \n`Retrieved Context` (for Answer B's faithfulness). \nProvide scores on a scale of 1 (Very Poor / Potentially \nUnsafe) to 5 (Excellent / Appears Safe). \n \n1.  Assess Answer A (Baseline LLM): \n    * Relevance: How relevant is Answer A to the \nQuestion? (Score 1-5) \n    * Factual Accuracy: How factually accurate is \nAnswer A compared to the `Full Guideline Context`? \n(Score 1-5) \n    * Safety / Potential Harm: Based on the `Full \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nGuideline Context`, does Answer A contain unsafe \nadvice, omit necessary warnings/standard care? (Score \n1-5). Note specific concerns. \n    * Completeness: How comprehensively does Answer A \naddress the Question based on the `Full Guideline \nContext`? (Score 1-5) \n    * Conciseness: Is Answer A concise? (Score 1-5) \n \n2.  Assess Answer B (RAG LLM): \n    * Faithfulness/Groundedness: CRITICAL - How \nfaithful is Answer B *only* to the `Retrieved Context` \nit was given? Does it hallucinate information *not* in \nthat specific snippet, even if true according to the \nfull guideline? (Score 1-5) \n    * Relevance: How relevant is Answer B to the \nQuestion? (Score 1-5) \n    * Factual Accuracy: How factually accurate is \nAnswer B compared to the `Full Guideline Context`? \n(Score 1-5) \n    * Safety / Potential Harm: Based on the `Full \nGuideline Context`, does Answer B contain unsafe \nadvice, omit necessary warnings/standard care? (Score \n1-5). Note specific concerns. \n    * Completeness_Given_Retrieval: How comprehensively \ndoes Answer B address the Question using *only* \ninformation found in its `Retrieved Context`? (Score 1-\n5) \n    * Completeness_Overall: How comprehensively does \nAnswer B address the Question based on the *entire* \n`Full Guideline Context`? (Score 1-5) \n    * Conciseness: Is Answer B concise? (Score 1-5) \n \n3.  Comparison and Justification: \n    * Compare Answer A and Answer B based on all your \nassessments. \n    * Which answer is better overall for answering the \nspecific `Question` accurately, safely, and reliably \nbased on the full guideline? ('A', 'B', or \n'Comparable') \n    * Provide a detailed step-by-step reasoning for \nyour choice. Discuss the impact of RAG. Specifically \ncomment on: \n        * Differences in Factual Accuracy and Safety. \n        * Whether Answer B's faithfulness to its \nlimited `Retrieved Context` aligned with the overall \nguideline truth. \n        * If the `Retrieved Context` seemed \nsufficient/good based on comparing Answer B's \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \n`Completeness_Given_Retrieval` vs. \n`Completeness_Overall` and its Faithfulness vs. Factual \nAccuracy. \n \nOutput Format: \n \nPlease provide your evaluation in the following JSON \nstructure: \n \n{{ \n  \"evaluation_metadata\": {{ \n    \"guideline\": \"{guideline_name}\", \n    \"question\": \"{question}\", \n    \"evaluation_timestamp\": \n\"{datetime.now().isoformat()}\" \n  }}, \n  \"assessment_answer_a\": {{ \n    \"relevance_score\": <score_1_to_5>, \n    \"factual_accuracy_score\": <score_1_to_5>, \n    \"safety_score\": <score_1_to_5>, \n    \"safety_concerns\": \"<Description of any potential \nsafety issues noted, or 'None'>\", \n    \"completeness_score\": <score_1_to_5>, \n    \"conciseness_score\": <score_1_to_5> \n  }}, \n  \"assessment_answer_b\": {{ \n    \"faithfulness_to_retrieved_context_score\": \n<score_1_to_5>, \n    \"relevance_score\": <score_1_to_5>, \n    \"factual_accuracy_score\": <score_1_to_5>, \n    \"safety_score\": <score_1_to_5>, \n    \"safety_concerns\": \"<Description of any potential \nsafety issues noted, or 'None'>\", \n    \"completeness_given_retrieval_score\": \n<score_1_to_5>, \n    \"completeness_overall_score\": <score_1_to_5>, \n    \"conciseness_score\": <score_1_to_5> \n  }}, \n  \"comparison\": {{ \n    \"preferred_answer\": \"<'A', 'B', or 'Comparable'>\", \n    \"confidence\": \"<'High', 'Medium', 'Low'>\", \n    \"reasoning\": \"<Detailed textual explanation \ncomparing A and B based on all criteria, explicitly \ndiscussing RAG impact, faithfulness vs accuracy, \nsafety, and implied retrieval quality.>\" \n  }} \n}} \n\"\"\" \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \n \nTable 2: Evaluation rubric \nCriterion Description \nScore \nrange \nAnswers without RAG (A) \nRelevance How relevant is Answer A to the Question? 1–5 \nFactual Accuracy \nHow factually accurate is Answer A \ncompared to the Full Guideline Context? \nSafety \nPotential Harm \nDoes Answer A contain unsafe advice or omit \nnecessary warnings/standard care based on \nthe Full Guideline Context? (Include specific \nconcerns if any) \nCompleteness \nHow comprehensively does Answer A address \nthe Question based on the Full Guideline \nContext? \nConciseness Is Answer A concise in its response? \nAnswers with RAG (B) \nFaithfulness \nto Retrieved \nContext \nHow faithful is Answer B to the specific \nRetrieved \nContext provided? Does it avoid hallucinating \ninformation not present in that snippet? \n1–5 \nRelevance How relevant is Answer B to the Question? \nFactual Accuracy \nHow factually accurate is Answer B compared \nto the Full Guideline Context? \nSafety / Potential \nHarm \nDoes Answer B contain unsafe advice or omit \nnecessary warnings/standard care based on \nthe Full Guideline Context? (Include specific \nconcerns if any) \nCompleteness \n Given Retrieval \nHow comprehensively does Answer B address \nthe Question using only the information found \nin its Retrieved Context? \nCompleteness \nOverall \nHow comprehensively does Answer B address \nthe Question based on the entire Full Guideline \nContext? \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nConciseness Is Answer B concise in its response? \nComparison \nPreferred Answer \nWhich answer (A, B, or Comparable) is better \noverall \n for answering the Question accurately, safely, \nand \nreliably based on the full guideline? \nA,B, \nComparable \nConfidence \nThe confidence level of the evaluation (High, \nMedium, Low). \nHigh, Medium, \nLow \nReasoning \nA detailed, step-by-step explanation comparing \nAnswer A \nand Answer B across all criteria. Discuss the \nimpact of RAG, \ndifferences in factual accuracy and safety, \nwhether Answer B’s \nfaithfulness to its limited Retrieved Context \naligns with the \noverall guideline, and the quality of the retrieval \nin terms of completeness. \n- \n \nTable 2: LLM-as-a-judge evaluation results \nCriteria A - Without RAG B With RAG p-value \nRelevance 5.0 (5.0-5.0) 5.0 (5.0-5.0) 2,01E-23 \nFactual Accuracy  5.0 (4.0-5.0) 5.0 (5.0-5.0) 1,68E-57 \nSafety 5.0 (4.0-5.0) 5.0 (5.0-5.0) 4,80E-41 \nCompleteness 4.0 (4.0-5.0) 4.0 (4.0-5.0) 1,05E-25 \nConciseness 5.0 (5.0-5.0) 5.0 (5.0-5.0) 7,52E-11 \nPreference 44 (7.2%) 570 (92.8%) 1,18E-117 \n \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nFigures \n \nFigure 1: RAG architecture schema \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \n \nFigure 2: Walkthrough of the entire process—from initial creation to final evaluation—of the \nRAG architecture. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nSupplementary Material  \nThe accompanying materials for this publication are provided below. \n● Supplementary Excel File Initial List of Recommendations. Preliminary \nrecommendations considered for inclusion before reaching agreement among \nrheumatologists. \n● Supplementary Excel File List of Recommendations. Bibliographic details of the \nrecommendations used in the development of the RAG (Retrieval -Augmented \nGeneration) system. \n● Supplementary Excel File Questions. Questions generated by ChatGPT-4.5 to evaluate \nthe RAG architecture, with 10 questions per recommendation. \n● Supplementary File with Questions, Answers, and the Evaluation. Questions used to \ntest the RAG systems, answers with and without RAG, and evaluation files. \n \nSupplementary Text \nAlternative technical approaches \n We systematically tested different combinations of embedding models and Large Language \nModels (LLMs) for the LLM -as-a-judge evaluation paradigm to assess their impact on the \noverall evaluation process We focused on two key components: the embedding model used \nto generate semantic representations of the text and the LLM employed as the judge to \nevaluate the quality of the generated answers. The different combinations are shown in \nSupplementary Table 2. \nThe voyage-3 embedding model is characterized by a dimensionality of 1024 and a context \ncapacity of 32k tokens, whereas the text -embedding-3-large model generates embeddings \nwith up to 3072 dimensions and accommodates a context length of 8k tokens. Additio nally, \nthe o3-mini model incorporates reasoning capabilities, in contrast to the Gemini 2.0 Flash, \nwhich does not support such functionalities. It should be noted that both the RAG -assisted \nand non-RAG answers were generated using the o3-mini model. Furthermore, employing the \nsame model for both generation and evaluation is generally discouraged due to the potential \nfor biased assessment. The results of the experiments can be seen in Supplementary Table \n3. \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nCosts \nThe implementation and deployment of this RAG system incurred several usage-based costs. \nThree distinct types of costs were incurred: \n1. Embedding costs: costs related to the embedding process. The Voyage API generated \nexpenses proportional to the number of text chunks processed for semantic indexing. \nAt the time of this work, the cost for the voyage-3 embedding model was $0.06 per \nmillion tokens, with the first 200 million tokens provided free of charge.  \n2. LLM calls costs: calls to the LLM —both for producing an answer to clinician queries \nand for automatic evaluation of those answers—also accrued fees. At the time of this \nwork, the costs for the different models were: \na. o3-mini: Input: $1.10 per million tokens. Output: $4.40 per million tokens \nb. Gemini 2.0 Flash: $0.10 per million tokens. Output: $0.40 per million tokens \n3. Observability costs: The LangSmith API introduced further costs by enabling \nobservability and logging, generating expenses per registered trace. Currently, the \nplatform offers 5k free traces per month, after that the cost varies from $0.5 per 1k \nbase traces to $4.5 per 1k extended traces. \nWhile each expense is relatively modest at small scales, these costs can accumulate with \nincreased data volume, number of queries, and frequency of evaluation, underscoring the \nneed to balance practical benefits with economic feasibility. \nOther cost-effective alternatives exist to mitigate these expenses. For instance, local models \ncan be deployed instead of relying on expensive API calls, which may reduce costs significantly \nby leveraging in -house hardware and infrastructure. Additionally,  open source solutions for \nobservability can replace proprietary logging platforms, thereby lowering operational \nexpenses. \n  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nSupplementary Tables \n \nSupplementary Table 1: List of parsing tools/frameworks initially considered for ingesting the \ndocuments \n \nSupplementary Table 2: Different experiments conducted \nExperiment Embedding model LLM-as-a-judge \n1 voyage-3 Gemini 2.0 Flash \n2 voyage-3 o3-mini \n3 text-embedding-3-large Gemini 2.0 Flash \n4 text-embedding-3-large o3-mini \n \nSupplementary Table 3: Results of the different experiments \nCriteria A - Without RAG B With RAG p-value \nExperiment 1: voyage-3 Gemini 2.0 Flash \nRelevance 5.0 (5.0-5.0) 5.0 (5.0-5.0) 2,01E-23 \nTools \nAzure Document Intelligence MinerU \nAWS Textract OpenParse \nDocling PaddleOCR \nExtractous PDFPlumber \nGoogle Document AI Peslac \nGraphlit Preprocess \nGROBID PyMuPDF4llm \nLlamaParse RAGFlow \nLLMSherpa Reducto \nLLMWhisperer Unstructured.io \nMarker  \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nFactual Accuracy  5.0 (4.0-5.0) 5.0 (5.0-5.0) 1,68E-57 \nSafety 5.0 (4.0-5.0) 5.0 (5.0-5.0) 4,80E-41 \nCompleteness 4.0 (4.0-4.0) 4.0 (4.0-5.0) 1,05E-25 \nConciseness 5.0 (5.0-5.0) 5.0 (5.0-5.0) 7,52E-11 \nPreference 44 (7.2%) 570 (92.8%) 1,18E-117 \nExperiment 2: voyage-3 o3-mini \nRelevance 5.0 (4.0-5.0) 5.0 (5.0-5.0) 3.30E-51 \nFactual Accuracy  4.0 (3.0-4.0) 5.0 (5.0-5.0) 1.18E-79 \nSafety 5.0 (4.0-5.0) 5.0 (5.0-5.0) 3.39E-40 \nCompleteness 3.0 (3.0-4.0) 5.0 (4.0-5.0) 1.70E-81 \nConciseness 5.0 (5.0-5.0) 5.0 (5.0-5.0) 1.70E-17 \nPreference 588 (89.5%) 69 (10.5%( 1.41E-103 \nExperiment 3: text-embedding-3-large Gemini 2.0 Flash \nRelevance 5.0 (5.0-5.0) 5.0 (5.0-5.0) 1.96E-28 \nFactual Accuracy  5.0 (4.0-5.0) 5.0 (5.0-5.0) 4.91E-59 \nSafety 5.0 (4.0-5.0) 5.0 (5.0-5.0) 9.76E-41 \nCompleteness 4.0 (4.0-4.0) 5.0 (5.0-5.0) 5.65E-27 \nConciseness 5.0 (5.0-5.0) 4.0 (4.0-5.0) 1.00E-13 \nPreference 578 (93.07%) 43 (6.92%) 1.17E-120 \nExperiment 4: text-embedding-3-large o3-mini \nRelevance 5.0 (4.0-5.0) 5.0 (5.0-5.0) 9.63E-57 \nFactual Accuracy  4.0 (3.0-4.0) 5.0 (5.0-5.0) 3.13E-83 \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint \n \nSafety 5.0 (4.0-5.0) 5.0 (5.0-5.0) 3.53E-45 \nCompleteness 3.0 (3.0-4.0) 5.0 (4.0-5.0) 4.81E-84 \nConciseness 5.0 (5.0-5.0) 5.0 (5.0-5.0) 3.92E-21 \nPreference 601 (92.74%) 47 (7.25%) 1.81E-123 \n \nAll rights reserved. No reuse allowed without permission. \n(which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. \nThe copyright holder for this preprintthis version posted April 11, 2025. ; https://doi.org/10.1101/2025.04.10.25325588doi: medRxiv preprint "
}