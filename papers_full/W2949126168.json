{
    "title": "Calibration, Entropy Rates, and Memory in Language Models",
    "url": "https://openalex.org/W2949126168",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2580462636",
            "name": "Braverman, Mark",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1896894975",
            "name": "Chen Xin-yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222581213",
            "name": "Kakade, Sham M.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221425244",
            "name": "Narasimhan, Karthik",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222931751",
            "name": "Zhang, Cyril",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1873694039",
            "name": "Zhang Yi",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W6328264",
        "https://openalex.org/W135467536",
        "https://openalex.org/W2010886500",
        "https://openalex.org/W2058195458",
        "https://openalex.org/W2962883166",
        "https://openalex.org/W2963951265",
        "https://openalex.org/W2047012355",
        "https://openalex.org/W2197913429",
        "https://openalex.org/W2964212410",
        "https://openalex.org/W1800356822",
        "https://openalex.org/W2079145130",
        "https://openalex.org/W2963748792",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W1501567504",
        "https://openalex.org/W2906625520",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W2508661145",
        "https://openalex.org/W2328111639",
        "https://openalex.org/W2012942264",
        "https://openalex.org/W149661194",
        "https://openalex.org/W2793273050",
        "https://openalex.org/W2792376130",
        "https://openalex.org/W1999965501",
        "https://openalex.org/W1618905105",
        "https://openalex.org/W2098824882",
        "https://openalex.org/W1549664537",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2787149435",
        "https://openalex.org/W2023943903",
        "https://openalex.org/W15592790",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2778817245",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2122602353",
        "https://openalex.org/W2964059481",
        "https://openalex.org/W2139716865",
        "https://openalex.org/W2888799392",
        "https://openalex.org/W1524012148"
    ],
    "abstract": "Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that state-of-the-art language models, including LSTMs and Transformers, are \\emph{miscalibrated}: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction.",
    "full_text": "Calibration, Entropy Rates, and Memory in Language Models\nMark Braverman∗ Xinyi Chen† Sham M. Kakade‡ Karthik Narasimhan§\nCyril Zhang¶ Yi Zhang∥\nAbstract\nBuilding accurate language models that capture meaningful long-term dependencies is a\ncore challenge in natural language processing. Towards this end, we present a calibration-\nbased approach to measure long-term discrepancies between a generative sequence model and\nthe true distribution, and use these discrepancies to improve the model. Empirically, we show\nthat state-of-the-art language models, including LSTMs and Transformers, are miscalibrated:\nthe entropy rates of their generations drift dramatically upward over time. We then provide\nprovable methods to mitigate this phenomenon. Furthermore, we show how this calibration-\nbased approach can also be used to measure the amount of memory that language models use\nfor prediction.\n1 Introduction\nRecent advances in language modeling have resulted in signiﬁcant breakthroughs on a wide variety of\nbenchmarks in natural language processing Dai et al. [2018], Gong et al. [2018], Takase et al. [2018].\nCapturing long-term dependencies has especially been a major focus, with approaches ranging\nfrom explicit memory-based neural networks Grave et al. [2016], Ke et al. [2018] to optimization\nimprovements aimed at stabilizing training Le et al. [2015], Trinh et al. [2018]. In this paper, we\naddress a basic question: how do the long-term dependencies in a language model’s generations\ncompare to those of the underlying language? Furthermore, if there are measurable discrepancies,\nthis leads to the question of whether and how we can use them to improve these models.\nStarting from Shannon’s seminal work that essentially introduced statistical language modeling\nShannon [1951], the most classical and widely studied long-term property of a language model\nis its entropy rate — the average amount of information contained per word, conditioned on the\npreceding words. A learned model provides an upper bound for the entropy rate of a language,\nvia its cross-entropy loss. The exponential of the entropy rate can be interpreted as the eﬀective\nsupport size of the distribution of the next word (intuitively, the average number of “plausible”\nword choices to continue a document), and the perplexity score of a model (the exponential of\nthe cross entropy loss) is an upper bound for this quantity. In state-of-the-art models trained on\n∗Princeton University, Computer Science Department, email: mbraverm@cs.princeton.edu\n†Google AI Princeton, email: xinyic@google.com\n‡University of Washington, Allen School of Computer Science and Engineering and Department of Statistics,\nemail: sham@cs.washington.edu\n§Princeton University, Computer Science Department, email: karthikn@cs.princeton.edu\n¶Princeton University, Computer Science Department, email: cyril.zhang@cs.princeton.edu\n∥Princeton University, Computer Science Department, email: y.zhang@cs.princeton.edu\n1\narXiv:1906.05664v1  [cs.CL]  11 Jun 2019\nModel Corpus Test ppl. EntRate\nAWD-LSTM [Merity et al., 2017] PTB 58.3 93.1\nCNN-LSTM [Jozefowicz et al., 2016] GBW 29.8 49.4\nTransformer [Vaswani et al., 2017b] GBW 28.1 34.7\nTransformer [Radford et al., 2019] WebText 23.7 61.2\nTable 1: Entropy rate degradations for generations from popular language models. State-of-the-art\nperformance is usually reported via perplexity (one-step prediction loss), but there is a striking\nblowup in the entropy rates of these models’ long-term generations.\nbillion-scale corpora, this number ranges between 10 and 30 Melis et al. [2017], Radford et al.\n[2019]. A natural diagnostic question, with which we begin our work, is whether the long-term\ngenerations of these models exhibit the same entropy rates as the underlying languages they are\nmodeling predictively.\nEmpirically, and perhaps surprisingly, it turns out that the entropy rate of generated text is\nsubstantially higher than the estimate for true text derived from the model’s one-step predictions.\nAs seen in Table 1 (see also Figure 1), this is true for both state-of-the-art LSTMs and Transformers\ntrained on a variety of datasets. As a timely example, the GPT-2 model Radford et al. [2019],\nthe object of much recent attention for its seemingly coherent and on-topic generations, suﬀers\na dramatic degradation in its entropy rate, from 23 .7 to 61 .2. We will defer the details of this\nexperiment to the supplementary material.\nThis empirical ﬁnding is notable since the neural attention- and memory-based techniques have\nbeen steadily improving on standard metrics like perplexity and, in some cases, even produce re-\nmarkably coherent text (often with some heuristics to reject poor generations). That the perplexity\nof generated text is so much higher than it is under the true distribution suggests that there are\nsigniﬁcant gaps in our current methodologies in accurately learning language models, particularly\nif we are interested in generating text that globally resembles the modeled language itself.\nOur contributions. The focus of this work is twofold: to improve generations based on any\nmeasurement mismatch on a long-term property of the model (e.g. the entropy rate), and to\nquantify the way a model’s predictions depend on the distant past. Central to both of these is a\ncalibration-based approach, which is utilized in statistics and other areas of machine learning Dawid\n[1982, 1985], Foster [1991], Zadrozny and Elkan [2002], Platt [1999], Guo et al. [2017], Niculescu-\nMizil and Caruana [2005].\nFirst, we show that, from a worst-case perspective, even an extremely accurate model (with ε\naverage KL divergence from the true distribution) may have generated text with a substantially\ndiﬀerent entropy rate as compared to the true distribution. Indeed, we show that this worst-case\nampliﬁcation may occur for a variety of long-term properties of a probabilistic language model; this\nis because the one-step KL divergence does not in general provide tight control over the expectation\nof a bounded function. The observed entropy rate ampliﬁcation (as seen in Table 1) demonstrates\nthat this is not only of theoretical concern. We then describe a calibration procedure to ﬁx this\nmismatch while simultaneously improving the perplexity of the language model. From a statistical\nperspective, the procedure is simple, and we discuss approaches to make it computationally eﬃcient.\nSecond, we provide a deﬁnition for long-term memory in language models as the mutual in-\nformation between the models predictions and the distant past in the input. We then provide\n2\nan upper bound on the amount of this mutual information using calibrated distributions (with a\nsingle-parameter exponent). This allows us to estimate the amount of context used by a language\nmodel as a function of the distance of past tokens from the current prediction timestep.\nWe perform empirical studies to accompany our theoretical results. We ﬁrst use the entropy rate\ncalibration algorithm to ﬁx an LSTM language model, resulting in a drop of around 20 perplexity\npoints in the generated text (so that the entropy rate of the model more accurately matches that of\nthe language itself). Then, we empirically estimate and compare the long-term memory of state-of-\nthe-art language models. Our insights point towards new ways of assessing (and ﬁxing) language\nmodels, especially in terms of their long-term properties, in a manner complementary to existing\nmetrics like perplexity.\n2 Related Work\nImproving language modeling with long-term dependencies. Recent approaches to im-\nproving language modeling have focused on several ways to better capture long-term dependencies,\nfrom using manually-deﬁned context representations Mikolov and Zweig [2012], Ji et al. [2015],\nWang and Cho [2016] or document-level topics Wang et al. [2017] to using LSTM recurrent neural\nnetworks with careful initialization Le et al. [2015], auxiliary loss signals Trinh et al. [2018] or\naugmented memory structures Grave et al. [2016], Ke et al. [2018]. More recent work has demon-\nstrated the applicability of Transformer networks Vaswani et al. [2017a] to the task, potentially\nside-stepping issues in training recurrent networks (e.g. vanishing/exploding gradients) and scaling\nto longer contexts Dai et al. [2018], Radford et al. [2018]. All these papers propose either architec-\ntural or optimization innovations to improve language model training. In contrast, we deﬁne and\nmeasure explicit long-term properties of language models and show that calibrating them correctly\ncan provide improvements to any black-box language model.\nInformation-theoretic approaches. While most language models aim to predict a distribution\nover the next token conditioned on the context, there have been alternative approaches relying on\ninformation-theoretic measures. Jost and Atwell [1994] propose a model which makes use of mutual\ninformation between word pairs to generate word sequences that retain longer-term dependencies.\nMcAllester [2018] propose a training objective based on mutual information for predictive modeling,\nand demonstrate its application for phoneme prediction. Clarkson and Robinson [1999] develop a\nhybrid metric using both perplexity and entropy rate, and show that it correlates better with a\ndownstream metric like word error rate. Such works propose alternative optimization objectives;\nin contrast, we show how to use information-theoretic measures to improve models with respect to\nexisting objectives like cross-entropy.\nMeasuring long-term statistics. Khandelwal et al. [2018] analyze LSTM-based language mod-\nels and empirically show that such models make use of a ﬁnite context for prediction. Lin and\nTegmark [2017] measure mutual information between any two symbols in human languages, and\nshow that it decays with distance, roughly following a power law distribution. Takahashi and\nTanaka-Ishii [2018] provide an upper bound for the entropy (character-level) of human languages\nby training neural language models with various context and data sizes and extrapolating to inﬁnity.\nWhile we also make use of measures like entropy and mutual information across longer contexts,\nour goal is to use these to better calibrate the language model and provably improve its perplexity.\n3\nCalibration and integral probability metrics.The idea of matching properties of the models’\npredictions to the empirical outcomes, in an online setting, goes back (at least) to the “prequential\nprinciple” of Dawid [1982, 1985], with subsequent work in online and game-theoretic settings Foster\n[1991], Vovk [2001], Kalai et al. [1999]. The idea of improving probability scores is also common in\nmachine learning Zadrozny and Elkan [2002], Platt [1999], Guo et al. [2017], Niculescu-Mizil and\nCaruana [2005]. The notion of examining the expectation of functions as a metric for the distance\nbetween two distributions sometimes goes under the name of integral probability metrics Mller\n[1997], Sriperumbudur et al. [2009], and this notion is becoming increasingly relevant again in\nunsupervised learning through the connections to GANs Mroueh and Sercu [2017]. In this work,\nwe directly focus on the KL divergence, where our use of calibration is largely based on basic facts\nabout exponential families Brown [1986].\n3 Preliminaries\nWe ﬁrst deﬁne some useful quantities for our analyses. Let Pr( W1,W2,...,W T) represent the true\nunderlying distribution over T length sequences of words, where the vocabulary is of size M. Let\nW1:T denote a random sequence of length T, with distribution Pr(W1:T). For clarity of exposition,\nwe assume that all sequences (i.e. sentences or documents or books) are of equal length T.\nFor any distributions Dand D′ over length-T sequences, recall that the entropy H(·), KL-\ndivergence, and entropy rate are, respectively, deﬁned by: H(D) := Ew1:T∼D\n[\nlog 1\nD(W1:T=w1:T)\n]\n,\nKL(D∥D ′) := Ew1:T∼D\n[\nlog D(W1:T=w1:T)\nD′(W1:T=w1:T)\n]\n, and EntRate(D) := 1\nTH(D). Let ˆPr(W1:T) denote\na learned distribution over sequences. In the typical sequential prediction setting, the probabilistic\nmodel is implicitly deﬁned by the conditional distributions Pr( Wt|W<t), which are typically eﬃ-\nciently computable. It is standard for such a language model to be trained to minimize the cross\nentropy objective:\nCE(Pr ∥ˆPr) := 1\nT E\nw1:T∼Pr\n[ T∑\nt=1\nlog 1\nˆPr(wt|w<t)\n]\n= 1\nT E\nw1:T∼Pr\n[\nlog 1\nˆPr(w1:T)\n]\n.\nNote that for an accurate language model, we would hope that: CE(Pr ∥ˆPr) ≈EntRate(ˆPr), i.e.\nthe entropy rate of the sequences generated under the learned model is nearly that of the cross\nentropy of the model (with respect to the true distribution Pr).\nThroughout, we assume that\n1\nTKL(Pr ∥ˆPr) = CE(Pr ∥ˆPr) −H(Pr) ≤ε (1)\nholds for some ε. In other words, the (unknown) ε measures the degree of sub-optimality of the\nlearned model, this ε is often referred to as the Bayes regret.\n4 Calibration and Entropy Rates\nIn this section, we assess the long-term properties of language models when generating text.\nSpeciﬁcally, we quantify the ampliﬁcation in the entropy rate of generations under an ε-accurate\n4\n0 100 200 300 400 500 600 700\nGeneration length t\n100\n60\n70\n80\n90\neH (LSTM)\n0 100 200 300 400 500 600 700\nGeneration length t\n20\n30\n40\n50\n60eH (Transformer)\nFigure 1: Entropy of the t-th generated word, conditioned on the past, for two popular language\nmodels, interpolating between model’s estimate for the language’s entropy (t= 1) and entropy rate\nof generations ( t →∞). A perfectly calibrated generative model would exhibit a time-invariant\nentropy rate (gray dotted lines). Left: LSTM trained on Penn Treebank. Right: GPT-2 Trans-\nformer.\nmodel (Eq. 1). We then provide a procedure to ﬁx this ampliﬁcation, without increasing the\nperplexity of the model. Proofs for all statements are provided in the supplementary material.\nFor generality, consider a function f : [M]T →R, deﬁned on T length sequences. Let the mean\nand variance of f under distribution Dbe denoted by µD(f) and σ2\nD(f)\nµD(f) := E\nw1:T∼D\n[f(w1:T)], σ 2\nD(f) := E\nw1:T∼D\n[(f(w1:T) −µD(f))2] .\n4.1 Error ampliﬁcation under our model\nIf our learned model ˆPr is accurate, we may hope that µPr(f) ≈µˆPr(f) i.e. that the expected value\nof f under the true distribution Pr is close to its expected value under our model. We can quantify\nthis gap as follows:\nLemma 4.1.(Pinsker’s Inequality Csiszar and K¨ orner [2011]) Suppose that for allw1:T, f(w1:T) ≤\nB. Then: ⏐⏐µPr(f) −µˆPr(f)\n⏐⏐≤B\n√\n2KL(Pr ∥ˆPr) .\nSince this holds for any bounded function, we can obtain the error ampliﬁcation of the entropy\nrate of ˆPr simply by choosing f = −log ˆPr.\nBefore we proceed, in order to rule out ampliﬁcation of this entropy rate due to arbitrarily\nsmall probabilities (which can blow up −log ˆPr), it is helpful to deﬁne the γ-mixture distribution\nas: D(γ) := (1 −γ)D+ γUni, where the Uni is the uniform distribution over all MT sequences.\nWe will then consider the model ˆPr\n(ε)\n, which has only a minor degradation in the cross entropy\ncompared to ˆPr, and, yet, may have a large ampliﬁcation in the entropy rate.\nCorollary 4.2. (Entropy rate ampliﬁcation under generations) Suppose the bound in equation 1\nholds. The ε-mixture distribution has KL bounded as:\n1\nTKL(Pr ∥ˆPr\n(ε)\n) ≤\n(\n1 + 1\nT\n)\nε.\nWe have that:\n|CE(Pr ∥ˆPr\n(ε)\n) −EntRate(Pr)|≤\n(\n1 + 1\nT\n)\nε ,and\n5\n|CE(Pr ∥ˆPr\n(ε)\n) −EntRate(ˆPr\n(ε)\n)|≤\n√\n2ε(T + 1)\n(\nlog M + log(1/ε)\nT\n)\n.\nThis bound shows that, in the worst case, even a small cross entropy may provide little control\nover the generations under our model (in terms of entropy rate). In fact, for ε= O( 1\nT) (which we\nmay hope is an accurate model), the bound is vacuous; the following remark shows this worst case\nbound is unimprovable, see the supplementary material.\nThe above theorems suggest that entropy rate ampliﬁcation is a theoretical possibility in the\nworst case, which our experiments show is in fact prevalent in pratice. These entropy rate ampliﬁ-\ncations are evident from the plots in Figure 1. Regardless of the text corpus or the language model,\nwe observe that the entropy rate under the model’s generations quickly increases with time, indi-\ncating that this is a persistent problem even for state-of-the-art language models while generating\ntext.\n4.2 Model calibration\nWe now describe a procedure to ﬁx this error ampliﬁcation. First, let us deﬁne a distribution ˆPrα\nsuch that:\nˆPrα(w1:T) = exp(αf(w1:T)) ·ˆPr(w1:T)\nZα\nwhere Zα =\n∑\nw1:T\nexp(αf(w1:T)) ·ˆPr(w1:T) .\nWe can then recover a calibrated model that does not suﬀer from error ampliﬁcation in f:\nLemma 4.3. (Calibration to f with model improvement) Suppose the variance of f is uniformly\nbounded in that there exists σ2\n+ such that the following holds for all α, σ2\nPrα(f) ≤σ2\n+ . Let α∗ =\nargminαCE(Pr ∥ˆPrα) . We have\nµPr(f) −µˆPrα∗(f) = 0, and CE(Pr ∥ˆPrα∗) ≤CE(Pr ∥ˆPr) −1\nT\n(µ(f) −µˆPr(f))2\n2σ2\n+\n.\nEntropy rate calibration. We can now apply the previous result to ﬁx the entropy rate ampli-\nﬁcation seen in Table 1. Note that it is trivial to avoid the entropy rate ampliﬁcation if we were\nallowed to degrade the quality of our model, in terms of perplexity (e.g. a unigram model does not\nhave this ampliﬁcation. However, we show that it is possible to match the entropy rate without\nhaving to sacriﬁce the quality of our model. In fact, we can both improve our model and more\naccurately match the entropy rate, by ﬁtting a family of one-parameter models.\nTheorem 4.4. (Entropy rate calibration) Suppose equation 1 holds. Algorithm 1 returns a ˆPrα∗\nsuch that: the following calibration property is satisﬁed:\nCE(Pr ∥ˆPrα∗) = EntRate(ˆPrα∗).\nFurthermore, ˆPrα∗ has entropy close to the true entropy rate as speciﬁed by:\n|EntRate(Pr) −EntRate(ˆPrα∗)|≤\n(\n1 + 1\nT\n)\nε,\n6\nAlgorithm 1(Ineﬃcient) Entropy Rate Calibration\n1: Input: Model ˆPr\n(ε)\n.\n2: Deﬁne a model class:\nˆPrα(w1:T) =\n(\nˆPr(w1:T)(ε)\n)1+α\n/Zα.\n3: Fit α∗: α∗= argminαCE(Pr ∥ˆPrα)\n4: Return ˆPrα∗\nand ˆPrα∗ is an improvement over the original model as characterized by:\nCE(Pr ∥ˆPrα∗) ≤CE(Pr ∥ˆPr\n(ε)\n) −1\n2\n\nCE(Pr ∥ˆPr\n(ε)\n) −EntRate(ˆPr)\nlog M + log(1/ε)\nT\n\n\n2\n.\nThis result shows that we simply need a single parameter αto deﬁne a new model class that is\na powered up version of our original model. Then, we can ﬁt this α to minimize the cross-entropy\nof the new model with respect to the true distribution Pr, in order to eliminate the entropy rate\nampliﬁcation.\nEven though this algorithm ﬁts only a single parameter, it is not easily implementable since it\nrequires an integration over sequences, at least in its exact form. One future direction would be to a\nsample based approach. This may be an interesting alternative to ideas like beam search Steinbiss\net al. [1994], Ortmanns and Ney [2000], Antoniol et al. [1995], which also aims to minimize a global\ncost function on sequences that is inconsistent with the token-level perplexity loss used to train the\nunderlying generative model.\nLookahead algorithms. In order to sidestep the computational issues of Algorithm 1, we pro-\nvide another simple approach based on what can be viewed as a “one-step” lookahead correction\n(Algorithm 2). Let ˆWt be a random variable with conditional distribution ˆPr(·|W<t). H(ˆWt+1|w≤t)\ndenotes the entropy of this conditional distribution, i.e.\nH(ˆWt+1|w≤t) = E\nwt+1∼ˆPr(·|w≤t)\n[\nlog 1\nˆPr(wt+1|w≤t)\n]\n.\nNote that H(ˆWt+1|w≤t) includes the word wt, so we require computing the entropy at time t+ 1\nwhen predicting Wt using a learned model.\nFor a conditional distribution, D(W1:T), let us deﬁne:\n¯µD= 1\nT\nT∑\nt=1\nE\nw<t∼Pr\nE\nwt∼D(·|w<t)\n[H(ˆWt+1|w≤t)]\nThus, ¯µD is the average of H(ˆWt+1|w≤t) with respect to a distribution which uses Dfor sampling\nthe last word Wt (at every timestep). Intuitively, the resulting model ˆPrα with a positive α would\nsuppress sampling words leading to larger entropy but rather encourage words that stablizes the\nentropy 1-step ahead in the future. Therefore, if our learned language model ˆPr was accurate, we\n7\nAlgorithm 2Local Entropy Rate Calibration\n1: Input: Model ˆPr\n(ε)\n, where ˆWt ∼ˆPr\n(ε)\n(·|W<t).\n2: Deﬁne a model class:\nˆPrα(w1:T) = ˆPα(w1) ˆPα(w2|w1) ...\nwhere\nˆPrα(wt|w<t) = ˆPr(wt|w<t) ·exp\n(\nα·H(ˆWt+1|w≤t)\n)\n/Zα.\n3: Fit α∗: α∗= argminαCE(Pr ∥ˆPrα)\n4: Return ˆPrα∗\nwould hope that: ¯ µPr ≈ ¯µˆPr . The following corollary shows that this is achievable, along with\nimproving the model’s perplexity.\nCorollary 4.5. Suppose Equation 1 holds. Then, Algorithm 2 returns a ˆPrα∗ such that:\n¯µPr −¯µˆPrα∗ = 0, and CE(Pr ∥ˆPrα∗) ≤CE(Pr ∥ˆPr\n(ε)\n) −1\n2\n( ¯µ−¯µˆPr\n(ε)\nlog M + log(1/ε)\nT\n)2\n.\nThis result provides us with Algorithm 2, which is computationally quite tractable. We ﬁrst use\nthe learned model ˆPr to deﬁne a new model classˆPrα, which scales ˆPr by an exponential distribution\nover the weighted 1-step lookahead entropyH(ˆWt+1|w≤t). Then, similar to Algorithm 1, we simply\nﬁt the single parameter αto minimize the cross-entropy of the new model with respect to Pr, which\nﬁxes the entropy ampliﬁcation in the resulting model ˆPrα. We observe this empirically in Figure 2\n– our calibration results in a perplexity drop of almost 20 points over long-term generations under\nan LSTM model. Model and implementation details are in the supplementary material.\nGenerations from a calibrated model.Table 2 provides sample generations from a calibrated\nTransformer model trained on the GBW dataset, compared to its original version. Qualitatively,\nthe calibrated generations: (1) are shorter and more concise, and (2) display a better grasp of\ndiscourse structure across sentences. More generations are provided in the supplementary material.\n5 Calibration and Memory\nDeﬁning a notion of memory in language models is challenging, and multiple equally sensible notions\nmay co-exist. Here we present our choice from ﬁrst principles. Let us say that ˆWt is a sample from\na model at time t, i.e. ˆWt ∼ˆPr(Wt|W<t). Let us also assume that W<t ∼Pr(W<t). We will deﬁne\nthe memory at gap τ as the mutual information between ˆWt and the distant past (those words\ngreater than τ steps ago) conditioned on the subsequence Wt−τ:t−1. Precisely,\nIτ := I(ˆWt; W<t−τ|Wt−τ:t−1) = H(ˆWt|Wt−τ:t−1) −H(ˆWt|W<t) ,\nwhere we are not explicitly denoting the t dependence in this deﬁnition 1.\n1While we may attempt to estimate Iτ for a given t, we can remove the t dependence by either deﬁning this\nquantity by with an average over t or by using appropriate stationarity assumptions. In our experiments, we average\nover t.\n8\nh\n0 20 40 60 80 100\nGeneration length t\n60\n70\n80\n90eH (LSTM)\noriginal\ncalibrated\nFigure 2: Eﬀect of calibrating an LSTM generative model with 1-step lookahead. Blue: entropy\ncurve from the setting of Figure 1. Green: entropy measurements after applying local calibration.\nIntuitively, It can be viewed as how much uncertainty (entropy) in the prediction Wt the model\nis able to reduce by utilizing the deep past W<t−τ in addition to the recent past Wt−τ:t−1.\nThe diﬃculty in estimating this mutual information is due to estimatingH(ˆWt|Wt−τ:t−1), which\nrequires the marginalized model ˆPr(Wt|Wt−τ:t−1). To (even approximately) marginalize a model\ndistribution ˆPr(Wt|W<t) over the deep past W<t−τ is statistically diﬃcult, since it requires the\naccess to a pool of samples of W<t that share an common recent past Wt−τ:t−1. Nevertheless,\nwe now show that it is possible to obtain an upper bound (which is computationally eﬃcient to\nestimate).\nUpper bounding mutual information using calibrated models. In the above, we were\nconsidering the mutual information between ˆWt and W<t−τ conditioned on Wt−τ:t−1. Let us now\nconsider a more general setting, where we have a distribution Pr( Z,Y,X ) where Z, Y, and X are\nrandom variables. We wil eventually consider Z,Y,X to be ˆWt,Wt−τ:t−1W<t−τ, respectively.\nFor distributions D(·|Y,X) and ˜D(·|Y,X) and for α∈R, deﬁne\nDα(Z|Y,X) := D(Z|Y,X) ·\n(\n˜D(Z|Y,X)\n)α\n/Zα.\nWe say that D(·|Y,X) is calibrated to ˜D(·|Y,X), if D= Dα=0 is unimprovable in that for all α\nCE(Pr ∥D) ≤CE(Pr ∥Dα) .\nNote this condition is achievable due to that calibrating a model to ˜D(·|Y,X) involves a one di-\nmensional (convex) estimation problem (over α).\n9\nOriginal model Calibrated model\nActual results could diﬀer materially from those in-\ndicated by these forward-looking statements as a re-\nsult of various important factors , including , without\nlimitation : changes in general economic and busi-\nness conditions , including more diﬃcult real estate\nenvironments ; [...174 tokens...] risks related to\ninvestigations by other companies ; inadequate infor-\nmation systems ; the impact of reduced availability of\n; * assumptions upon such companies using such as\nours to gauge CNET ’s ﬁnancial condition ; and other\nfactors .\nActual results could diﬀer materially from those indi-\ncated by these forward-looking statements as a result\nof a variety of factors , including but not limited to\n( i ) the risk that the tender oﬀer could close in one\nor more manner or at all ; ( ii ) risks associated with\nconducting business in foreign jurisdictions ; ( iii )\ndiﬃculties in combining some or all of the businesses\nunder one roof ; ( iv ) decreased demand for electric-\nity , natural gas and other energy products , including\nadverse eﬀects on the pricing of oil and natural gas ;\nand ( v ) the risks associated with doing business in-\nternationally .\nBluepoint Games , Inc. is a highly experienced and\nmulti-faceted publisher of licensed virtual worlds for\ngamers , developers and technology professionals .\n[...114 tokens...] James Upon , CEO of MyNet-\nSheltetWeb and the three previous Developers of\nMySQL . Based in Redwood City , California , Blue-\nMountain is the leader in franchise and game develop-\nment for the massively multiplayer online game .\nBluepoint Games , Inc. is a highly experienced licens-\ning , gaming and entertainment ﬁrm focused on de-\nveloping the next generation of casual games based on\nthe PlayStation ( R ) BRAVIA family of video game\nmachines for the North American market . Bluepoint\nis a wholly owned subsidiary of Bluehill ID Holdings\nL.P.\nTable 2: Sample generations from a calibrated, state-of-the-art Transformer model trained on the\nGBW corpus, seeded with preﬁxes of sentences (in italics) from the holdout validation set.\nTheorem 5.1.Suppose we have a model ˆPr(Z|X), and suppose ˜Z ∼˜Pr(·|X), where ˜Z is dependent\nonly on X Suppose that ˆPr is calibrated to ˜Pr. Then we have that:\nI( ˆZ; X|Y) ≤CE(Pr ∥˜Pr) −H( ˆZ|Y,X) , where:\nCE(Pr ∥˜Pr) = E\nY∼Pr\nE\nZ∼Pr(·|Y)\n[\nlog 1\n˜Pr(Z|Y)\n]\n.\nMemory estimation. We ﬁrst learn another ˜Wt ∼˜Pr(·|Wt−τ:t−1), and then calibrate ˆPr to ˜Pr.\nCorollary 5.2. Suppose ˆPr\ncal\n(·|W<t) is a model calibrated to ˜Pr(·|Wt−τ:t−1). For a random vari-\nable, ˆWcal\nt ∼ˆPr\ncal\n(·|W<t), we have that:\nI(ˆWcal\nt ; W<t−τ|Wt−τ:t−1) ≤CE(Pr ∥˜Pr) −H(ˆWcal\nt |W<t), where:\nCE(Pr ∥˜Pr) = E\nWt−τ:t∼Pr\n[\nlog 1\n˜Pr(Wt|Wt−τ:t−1)\n]\n.\nThis corollary gives us a means to eﬃciently provide upper bounds on the mutual information.\nThe key is that since ˜Pr is eﬃciently computable, we can directly estimate CE(Pr ||˜Pr) through\nMonte Carlo estimation. We measure the upper bounds on Iτ of a LSTM model with trained\nlimited-memory models ˜Pr (see details in the supplementary material) and report them in Figure 3.\nAs expected, the memory estimate gradually decays with longer τ, indicating that the models make\nmore use of the recent past to generate text.\n10\n5 10 15 20 25 30\nhistory length \n0.0\n0.2\n0.4\n0.6I  upper bound (nats)\nτ CE(Pr||˜Pr) Iτ upper bound α∗\n5 4.8144 0.6180 0.003515\n10 4.5258 0.4226 -0.01041\n15 4.4166 0.2678 -0.00447\n20 4.3347 0.2485 -0.02268\n25 4.2777 0.2274 -0.01814\n30 4.2408 0.2143 -0.02323\nFigure 3: Left: Plot of the upper bound on Iτ derived from calibrated models. Right: The\nmeasurements of the upper bound on mutual information, the cross entropy of the limited memory\nmodel ˜Pr as well as the optimal calibration coeﬃcient α∗for various time lengths τ. Details of the\nmodel used here can be found in the supplementary material.\n6 Conclusion\nWe have introduced a calibration-based approach to detect and provably correct the discrepancies\nbetween the long-term generations of language models and the true distributions they estimate\nsequentially. In particular, for state-of-the-art neural language models, we have observed large\ndegradations of the entropy rate under iterative generation, and a proposed ﬁrst-order correction\nwhich is both computationally tractable and eﬀective. Using the same calibration approach, we\nhave derived estimators for the amount of information extracted by these models from the deep\npast.\nAside from the empirical ﬁndings and improvements, we hope that this work will inspire a more\nprincipled line of discourse on the quality of long-term generations in language models. It remains\nan interesting open problem to study other ”future-aware” generation-improving heuristics (beam\nsearch, reverse language models, GANs) in this framework of calibration.\nAcknowledgments\nS. K. gratefully acknowledges funding from the Washington Research Foundation for Innovation in\nData-intensive Discover, the ONR award N00014-18-1-2247, and NSF Award CCF-1703574.\nReferences\nGiuliano Antoniol, Fabio Brugnara, Mauro Cettolo, and Marcello Federico. Language model rep-\nresentations for beam-search decoding. In 1995 International Conference on Acoustics, Speech,\nand Signal Processing, volume 1, pages 588–591. IEEE, 1995.\nL. D. Brown. Fundamentals of Statistical Exponential Families: With Applications in Statistical\nDecision Theory. Institute of Mathematical Statistics, Hayworth, CA, USA, 1986. ISBN 0-940-\n60010-2.\nPhilip Clarkson and Tony Robinson. Towards improved language model evaluation measures. In\nSixth European Conference on Speech Communication and Technology, 1999.\n11\nThomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-\nmunications and Signal Processing) . Wiley-Interscience, New York, NY, USA, 2006. ISBN\n0471241954.\nImre Csiszar and J´ anos K¨ orner.Information theory: coding theorems for discrete memoryless\nsystems. Cambridge University Press, 2011.\nZihang Dai, Zhilin Yang, Yiming Yang, William W Cohen, Jaime Carbonell, Quoc V Le, and\nRuslan Salakhutdinov. Transformer-xl: Language modeling with longer-term dependency. 2018.\nA. P. Dawid. The well-calibrated bayesian. Journal of the Am. Stat. Assoc , 77, 1982.\nA. P. Dawid. The impossibility of inductive inference. Journal of the Am. Stat. Assoc , 80, 1985.\nD. P. Foster. Prediction in the worst case. Annals of Statistics , 19, 1991.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: frequency-agnostic\nword representation. In Advances in Neural Information Processing Systems , pages 1341–1352,\n2018.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a\ncontinuous cache. arXiv preprint arXiv:1612.04426 , 2016.\nChuan Guo, Geoﬀ Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\nnetworks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 ,\npages 1321–1330. JMLR. org, 2017.\nYangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. Document context\nlanguage models. arXiv preprint arXiv:1511.03962 , 2015.\nUwe Jost and ES Atwell. Proposal for a mutual-information based language model. InProceedings of\nthe 1994 AISB Workshop on Computational Linguistics for Speech and Handwriting Recognition .\nAISB, 1994.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the\nlimits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\nE. Kalai, E. Lehrer, and R. Smorodinsky. Calibrated forecasting and merging.Games and Economic\nBehavior, 29, 1999.\nNan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris\nPal, and Yoshua Bengio. Sparse attentive backtracking: Temporal credit assignment through\nreminding. In Advances in Neural Information Processing Systems , pages 7651–7662, 2018.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. Sharp nearby, fuzzy far away: How neural\nlanguage models use context. In Proceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers) , volume 1, pages 284–294, 2018.\nQuoc V Le, Navdeep Jaitly, and Geoﬀrey E Hinton. A simple way to initialize recurrent networks\nof rectiﬁed linear units. arXiv preprint arXiv:1504.00941 , 2015.\n12\nHenry Lin and Max Tegmark. Critical behavior in physics and probabilistic formal languages.\nEntropy, 19(7):299, 2017.\nMitchell Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building a large annotated\ncorpus of english: The penn treebank. 1993.\nDavid McAllester. Information theoretic co-training. arXiv preprint arXiv:1802.07572 , 2018.\nG´ abor Melis, Chris Dyer, and Phil Blunsom. On the state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589 , 2017.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. Regularizing and Optimizing LSTM\nLanguage Models. arXiv preprint arXiv:1708.02182 , 2017.\nStephen Merity, Nitish Shirish Keskar, and Richard Socher. An Analysis of Neural Language\nModeling at Multiple Scales. arXiv preprint arXiv:1803.08240 , 2018.\nTomas Mikolov and Geoﬀrey Zweig. Context dependent recurrent neural network language model.\nIn 2012 IEEE Spoken Language Technology Workshop (SLT) , pages 234–239. IEEE, 2012.\nYoussef Mroueh and Tom Sercu. Fisher gan. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors,Advances in Neural Information Processing\nSystems 30 , pages 2513–2523. Curran Associates, Inc., 2017. URL http://papers.nips.cc/\npaper/6845-fisher-gan.pdf.\nAlfred Mller. Integral probability metrics and their generating classes of functions. Advances in\nApplied Probability, 29:429–443, 06 1997. doi: 10.2307/1428011.\nAlexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learn-\ning. In Proceedings of the 22nd international conference on Machine learning , pages 625–632.\nACM, 2005.\nStefan Ortmanns and Hermann Ney. Look-ahead techniques for fast beam search. Computer Speech\n& Language, 14(1):15–32, 2000.\nJohn C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized\nlikelihood methods. In Advances in Large Margin Classiﬁers , pages 61–74. MIT Press, 1999.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language un-\nderstanding by generative pre-training. URL https://s3-us-west-2. amazonaws. com/openai-\nassets/research-covers/languageunsupervised/language understanding paper. pdf, 2018.\nAlec Radford, Jeﬀ Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nClaude E Shannon. Prediction and entropy of printed english. Bell system technical journal , 30(1):\n50–64, 1951.\nBharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schlkopf, and Gert Lanckriet.\nOn integral probability metrics, phi-divergences and binary classiﬁcation. 01 2009.\n13\nVolker Steinbiss, Bach-Hiep Tran, and Hermann Ney. Improvements in beam search. In Third\nInternational Conference on Spoken Language Processing , 1994.\nShuntaro Takahashi and Kumiko Tanaka-Ishii. Cross entropy of neural language models at inﬁnitya\nnew bound of the entropy rate. Entropy, 20(11):839, 2018.\nSho Takase, Jun Suzuki, and Masaaki Nagata. Direct output connection for a high-rank language\nmodel. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Pro-\ncessing, pages 4599–4609, 2018.\nTrieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. Learning longer-term dependencies\nin rnns with auxiliary losses. arXiv preprint arXiv:1803.00144 , 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in Neural Information\nProcessing Systems, pages 5998–6008, 2017a.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. InAdvances in neural information\nprocessing systems, pages 5998–6008, 2017b.\nV. Vovk. Competitive on-line statistics. International Statistical Review, 69, 2001.\nTian Wang and Kyunghyun Cho. Larger-context language modelling with recurrent neural network.\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , volume 1, pages 1319–1329, 2016.\nWenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and\nLawrence Carin. Topic compositional neural language model. arXiv preprint arXiv:1712.09783 ,\n2017.\nBianca Zadrozny and Charles Elkan. Transforming classiﬁer scores into accurate multiclass proba-\nbility estimates, 2002.\n14\nA Proofs for Section 4\nProof. (of Lemma 4.1) We have\n⏐⏐⏐⏐⏐ E\nw1:T∼Pr\n[f(w1:T)] − E\nw1:T∼ˆPr\n[f(w1:T)]\n⏐⏐⏐⏐⏐ =\n⏐⏐⏐⏐⏐\n∑\nw1:T\n(\nPr(w1:T) −ˆPr(w1:T)\n)\nf(w1:T)\n⏐⏐⏐⏐⏐\n≤\n⏐⏐⏐Pr −ˆPr\n⏐⏐⏐\n1\nB\n≤\n√\n2KL(Pr ||ˆPr))B\nwhere we have used Holder’s and Pinsker’s inequalities Cover and Thomas [2006].\nProof. (of Corollary 4.2) First observe:\nlog 1\nˆPr\n(ε)\n(w1:T)\n≤log 1\n(1 −ε)ˆPr(w1:T)\n= log 1\nˆPr(w1:T)\n−log(1 −ε) ≤log 1\nˆPr(w1:T)\n+ ε\nand that:\nlog 1\nˆPr\n(ε)\n(w1:T)\n≤log MT\nε . (2)\nFor the ﬁrst claim, we have\n1\nTKL(Pr ||ˆPr\n(ε)\n) = 1\nT E\nw1:T∼Pr\n[\nlog Pr(w1:T)\nˆPr\n(ε)\n(w1:T)\n]\n≤(1 + 1\nT)ε.\nusing our assumption in Equation 1.\nFor the second claim, taking f = log 1\nˆPr\n(ε)\n(w1:T)\nwith Lemma 4.1, we have:\n⏐⏐⏐⏐CE(Pr ||ˆPr\n(ε)\n) −EntRate(ˆPr\n(ε)\n)\n⏐⏐⏐⏐ ≤ 1\nT\n√\n2KL(Pr ||ˆPr\n(ε)\n)\n⏐⏐⏐⏐⏐log 1\nˆPr\n(ε)\n⏐⏐⏐⏐⏐\n∞\n= 1\nT\n√\n2ε(T + 1) logMT\nε ,\nwhich completes the proof.\nProof. (of Lemma 4.3) By deﬁnition,\nCE(Pr ||ˆPrα) := CE(Pr ||ˆPr) −α\nT E\nw1:T∼Pr\n[f(w1:T)] + 1\nT log(Zα) ,\nwe have:\n∂CE(Pr ||ˆPrα)\n∂α = 1\nT\n(\n−µPr(f) + µˆPrα\n(f)\n)\nThe ﬁrst claim now follows from optimality of α∗.\n15\nFor the second claim,\n∂2CE(Pr ||ˆPrα)\n∂2α = 1\nT\n∂2 log(Zα)\n∂2α\n= 1\nT\n∂\n∂α\n∑\nw1:T f(w1:T) exp(αf(w1:T)) ·ˆPr(w1:T)\n∑\nw1:T exp(αf(w1:T)) ·ˆPr(w1:T)\n= 1\nTσ2\nˆPrα\n(f) = 1\nTσ2\nˆPrα\n(f) ≤σ2\n+\nT .\nBy Taylor’s theorem, we have:\nCE(Pr ||ˆPrα) ≤CE(Pr ||ˆPr) −α·1\nT\n(\nµPr(f) −µˆPrα\n(f)\n)\n+ α2\n2 ·σ2\n+\nT .\nTaking the the α which minimizes the upper bound, leads to the second claim.\nRemark A.1. (Sharpness) If ε ≥ 1\nT, then there exists a problem where the bound is sharp and\nEntRate(ˆPr) takes on the maximal value of O(log M). As an example, consider a model ˆPr, that\nstarts by generating words under the true distribution Pr and has a 1\nT probability of transitioning\ninto a mode in which it generates words uniformly at random thereafter.\nProof. (of Theorem 4.4) We can apply the previous lemma using\nf = log 1\nˆPr(w1:T)\n,\nand so our calibration condition implies:\n0 = µPr(f) −µˆPrα∗(f) = −\n(\nµPr(log ˆPr) −µˆPrα∗(log ˆPr)\n)\n.\nNow observe that:\nT ·CE(Pr ||ˆPrα∗) = µPr(−(1 + α∗) logˆPr + logZα∗) = −(1 + α∗)µPr(log ˆPr) + logZα∗\nand, similarly,\nT ·EntRate(ˆPrα∗) = −(1 + α∗)µˆPrα∗(log ˆPr) + logZα∗.\nThese imply:\nCE(Pr ||ˆPrα∗) −EntRate(ˆPrα∗) = −1\nT(1 + α∗)\n(\nµPr(log ˆPr) −µˆPrα∗(log ˆPr)\n)\n= 0 ,\nwhich completes the proof of the ﬁrst claim.\nThe proof of the second claim uses\nµ(f) −µˆPr(f) = T\n(\nCE(Pr ||ˆPr) −EntRate(ˆPr)\n)\n,\nand, by Equation 2,\nσ2\n+ ≤Tlog M + log(1/ε) ,\nwhich completes the proof.\n16\nNow we move on to the proof of Corollary 4.5.\nSuppose f(W≤t) be a function of W≤t. For a conditional distribution, D(W1:T), let us now\ndeﬁne:\n¯µD(f) = 1\nT\nT∑\nt=1\nE\nw<t∼Pr\nE\nwt∼ˆD(·|w<t)\n[f(w≤t)] .\nDeﬁne:\nˆPt,α(wt|w<t) := 1\nZα,t\nexp(αf(w≤t)) ·ˆPr(wt|w<t)\nand\nˆPrα(w1:T) := ˆP1,α(w1) ˆP2,α(w2|w1) ... .\nLemma A.1. Suppose f ≤σ2\n+. Let\nα∗= argmin\nα\nCE(Pr ||ˆPrα) .\nWe have that:\n¯µPr(f) −¯µˆPrα∗(f) = 0\nand that\nCE(Pr ||ˆPrα∗) ≤CE(Pr ||ˆPr) −(¯µ(f) −¯µˆPr(f))2\nσ2∗\n.\nProof. (sketch) The proof is identical to that of Lemma 4.3, with the addition of using linearity of\nexpectation.\nB Proofs for Section 5\nProof. (of Theorem 5.1) It is convenient to deﬁne the distribution:\nD(Z,Y,X ) = ˆPr(Z|X,Y ) ·Pr(Y,X) .\nWe then have:\nI( ˆZ; X|Y) = H( ˆZ|Y) −H( ˆZ|Y,X)\nby the deﬁntion of the mutual information.\nThe proof consists of showing that:\nH( ˆZ|Y) = EY,Z∼Dlog 1\nD(Z|Y) ≤CE(Pr ||˜Pr) .\nLet us take ˆPrα(Z|X,Y ) = ˆPr(Z|X,Y ) ·\n(\n˜Pr(Z|X)\n)α\n/Zα. The zero gradient condition for the\noptimality at α= 0 implies:\n0 = ∂CE(Pr ||ˆPrα)\n∂α\n⏐⏐⏐\nα=0\n= EX,Y∼Pr\n[\n−EZ∼Pr(·|X,Y) log ˜Pr(Z|Y) + EZ∼ˆPr(·|X,Y) log ˜Pr(Z|Y)\n]\n= −EY∼Pr[EZ∼Pr(·|Y) log ˜Pr(Z|Y) + EX,Y∼Pr[EZ∼ˆPr(·|X,Y) log ˜Pr(Z|Y)]\n= CE(Pr ||˜Pr) + EX,Y∼Pr[EZ∼ˆPr(·|X,Y) log ˜Pr(Z|Y)] .\n17\nThis implies:\nCE(Pr ||˜Pr) = EX,Y∼Pr[EZ∼ˆPr(·|X,Y) log 1\n˜Pr(Z|Y)\n]\n= EX,Y,Z∼Dlog 1\n˜Pr(Z|Y)\n= EY,Z∼Dlog 1\n˜Pr(Z|Y)\n≥ EY,Z∼Dlog 1\nD(Z|Y)\n= H( ˆZ|Y) ,\nwhere the last step uses the deﬁnition of ˆZ and Jensen’s inequality.\nC Experimental Details\nIn this section, we outline the experimental setups used to obtain the empirical results throughout\nthe paper. For the calibration and memory experiments (Table 1 row 1, Figure 1 (left), Figures 2, 3),\nour base model is a 3-layer LSTM with with 400 embedding dimension and 1150 hidden nodes. We\ntrain it on the Penn Treebank (PTB) corpus Marcus et al. [1993], following the setup of Merity\net al. [2017] and Merity et al. [2018] for 500 epochs using SGD with batch size 20 and BPTT length\n70. The trained base model achieves 64.3 validation perplexity and 58.3 test perplexity.\nThe limited-memory models ˜Pr(·|Wt−τ:t−1) used for the memory estimation in Section 5 share\nthe same architecture as our base model while, during training, the hidden states is re-initialized\nafter reading every τ tokens (τ takes value from {5,15,..., 30}).\nFinally, for the entropy rate measurements of larger-scale state-of-the-art language models (Ta-\nble 1 rows 2-4, Figure 1 (right)), we used the pretrained weights published alongside Jozefowicz et al.\n[2016], Radford et al. [2019] for rows 2 and 4, while we trained the model using the tensor2tensor\nframework. The model for row 2 is an LSTM with CNN-embedded inputs, trained on the Google\nBillion Words (GBW) corpus. The other two are Transformer Vaswani et al. [2017a] models trained\non GBW (row 3), and an proprietary corpus derived from a web crawl (WebText; row 4). For GPT-\n2, since the authors have not published training or validation data, we used the text of several New\nYork Times articles as a stand-in validation set; the cross entropy loss is comparable to that re-\nported on the validation set. The entropy rate ampliﬁcation plot in Figure 1 (bottom) corresponds\nto the setup from row 4.\nTo measure the conditional entropy after t generations, we measured the empirical conditional\nentropy of the t-th word over >500 independent generations, which were produced by the standard\nway of iteratively sampling from the next predicted conditional distribution, seeded with ground-\ntruth text up to >100 random points in the validation set. We used the entropy rate at t= 700\nas a proxy for the asymptotic limit in Table 1.\nD Additional Generation Samples\nIn this section, to provide a better sense of the qualitative eﬀect of calibration, we provide below\nsome additional generations, seeded by 10-token preﬁxes of the holdout (validation) sentences from\n18\nthe Google Billion Words dataset. Here, we used the model we trained for row 3 of Table 1. To\nidentify a failure mode for the uncalibrated model, we selected the seed preﬁxes which resulted in\nunusually long generations by the uncalibrated model.\nOriginal model Calibrated model\nActual results could differ materially from\nthose indicated by these forward-looking\nstatements as a result of numerous fac-\ntors including the risks associated with the\ntimely and eﬃcient completion and inte-\ngration of the Temporary Liquidity Guar-\nantee Department ’s supervision into the\ncommercial , open market , solar energy\n, energy eﬃciency , electric utility trans-\nmission , and water demands of residential\nand business customers , Comcast ’s abil-\nity to successfully implement its business\nplan , timing of completion of the acqui-\nsition and the eﬀectiveness of the eﬀorts\nand strategies involved in the integration\nof Rhapsody , timing of regulatory and\nclient approvals and availability of key en-\nhancements .\nActual results could differ materially from\nthose indicated by these forward-looking\nstatements as a result of a variety of fac-\ntors , including but not limited to ( i )\nthe risk that the tender oﬀer could close\nin one or more manner or at all ; ( ii )\nrisks associated with conducting business\nin foreign jurisdictions ; ( iii ) diﬃculties\nin combining some or all of the businesses\nunder one roof ; ( iv ) decreased demand\nfor electricity , natural gas and other en-\nergy products , including adverse eﬀects\non the pricing of oil and natural gas ; and\n( v ) the risks associated with doing busi-\nness internationally .\n19\nActual results could differ materially from\nthose indicated by these forward-looking\nstatements as a result of various important\nfactors , including , without limitation :\nchanges in general economic and business\nconditions , including more diﬃcult real\nestate environments ; declines in infor-\nmation technology spending ; continued\navailability of capital and government reg-\nulations ; changes in general economic and\nbusiness conditions ; the possibility that\nextended unemployment and healthcare\npolicies may change , or may reduce ac-\ncess to quality care services ; failure to ob-\ntain adequate and aﬀordable medications\n; changes in certain CME / CE product\nmix ; disruption in CME credit markets ;\nuncertainty of the outcomes of regulatory\ninvestigations of companies in which the\nCompany has an interest ; dependence on\nsuppliers for most of its products ; consol-\nidation among ﬁnancial institutions ; abil-\nity to attract and retain skilled personnel\n; changes in rapidly changing technology\nand regulatory environments ; arrogance\nand complacency among ﬁnancial analysts\n; the impact of competition ; inability to\nretain and motivate senior management ;\ndiﬃculties in the integration of acquired\nbusinesses ; the eﬀects of redundancy and\nloss of key employees ; litigation , includ-\ning claims and the challenge of insurance\npractices ; uncertainties relating to liti-\ngation ; risks related to investigations by\nother companies ; inadequate information\nsystems ; the impact of reduced availabil-\nity of ; * assumptions upon such compa-\nnies using such as ours to gauge CNET ’s\nﬁnancial condition ; and other factors .\nActual results could differ materially from\nthose indicated by such forward-looking\nstatements as a result of various important\nfactors , including those discussed in the\ncompany ’s periodic reports that are ﬁled\nwith the Securities and Exchange Com-\nmission and available on the SEC ’s web-\nsite at www.sec.gov.\n20\nActual results could differ materially from\nthose indicated by such forward-looking\nstatements as a result of a variety of fac-\ntors , including our ability to improve\nour liquidity . Among these factors are\nchanges in the general economy , changes\nin political and economic conditions ,\nchanges in interest rates , changes in tech-\nnology and implementation of regulatory\npolicies and legislation , the direction of\ninterest rates and changes in the bank-\ning industry , changes in loan prepay-\nment activity , changes in consumer pref-\nerences and consumer and business lend-\ning markets , legislation or public com-\npliance with applicable laws and regula-\ntions and changes in the business or reg-\nulatory environment . We caution you\nthat there are many uncertainties that\ncould cause actual results to diﬀer mate-\nrially from those indicated in the forward-\nlooking statements . Among them are the\nrisk factors that could cause results to dif-\nfer from those expressed in the forward-\nlooking statements . These factors include\n, but are not limited to : general economic\nand business conditions , including the ﬁ-\nnancial markets ; ﬂuctuations in interest\nrates ; government regulation of the ﬁnan-\ncial services industry and possible failures\n; planning assumptions and estimates ; po-\ntential funding requirements ; unexpected\nchanges in cost increases ( including good-\nwill impairment ) ; competition ; the po-\ntentially lengthy , protracted U.S. reces-\nsion ; and migratory consumer and busi-\nness conditions .\nActual results could differ materially from\nthose indicated by these forward-looking\nstatements as a result of various important\nfactors , including those discussed in the ”\nRisk Factors ” section of the Company ’s\nAnnual Report on Form 10-K for the most\nrecently ended ﬁscal year .\n21\nBluepoint Games , Inc. is a highly expe-\nrienced and multi-faceted publisher of li-\ncensed virtual worlds for gamers , devel-\nopers and technology professionals . The\ncompany is based in Vancouver , Canada\n. BlueKai ’s innovative games are dis-\ntributed by Devices EA , LLC , and Club\nPenguin . BlueKai owns and is the exclu-\nsive licensor of Scrabulous . BluetoothQ\nInteractive Inc. has acquired JoShear-\nSwain Media , LLC , a premier devel-\noper and publisher of community based\ngames for the handheld game device .\nFor further information , please visit :\nwww.netgear.com / ngcleveld . Sprint ’s\nfantasy game publisher and Web doing\nbusiness within the Entertainment Group\nis James Upon , CEO of MyNetShel-\ntetWeb and the three previous Develop-\ners of MySQL . Based in Redwood City\n, California , BlueMountain is the leader\nin franchise and game development for the\nmassively multiplayer online game .\nBluepoint Games , Inc. is a highly experi-\nenced gaming and entertainment company\nwith several renowned blockbuster fran-\nchises including PC , GameHouse ( ( R\n) ) GameHouse ( ( R ) ) , Heavenly Sword\n( ( TM ) ) , EverQuest ( R ) , Untold Story\n( TM ) and EverQuest ( R ) II . Through\nits wholly-owned subsidiary , Bluehill ID\n( R ) , the Bluehill ID logo and tagline are\nregistered trademarks of Bluehill ID Cor-\nporation and its subsidiaries in the U.S.\nand in other countries .\n22\nBluepoint Games , Inc. is a highly expe-\nrienced gaming , entertainment and mo-\nbile games company with a vertically in-\ntegrated portfolio including : games ( TM\n) , social network , mobile , casual games\n, MMORPG , production , distribution ,\nand licensing including its ﬂagship games ,\nSUIT and TIMMERIX ( TM ) , as well as\nits award-winning gaming , basketball and\nentertainment network . In order to cre-\nate a highly integrated , pure and socially\nresponsible Game ( R ) family , Bluepoint\nhas collaborated with Amplify Systems In-\nternational , Inc. on various titles for\nPlayStation ( R ) 2 , PLAYSTATION 3\n( R ) 5 , Wii ( TM ) 3 , PS3 , Wii ( TM\n) ( and PS3 titles ) as well as PC games\nfor PC , PSP , POOL , Wii ( TM ) ( and\nsuccessor title ) and IP ( R ) , in addition\nto its focused gaming , entertainment and\ncommunication services . BlueBay ’s ex-\nclusive licensee worldwide licensee of the\nBluepoint ( TM ) ZMFAO Gateway series\n, it is the world ’s leading portable gam-\ning , PC and mobile phone company . For\nmore information , see UNK , Inc. and\n” Oakpoint : ZWC ’s Community Health\nBusiness Development Center .\nBluepoint Games , Inc. is a highly expe-\nrienced licensing , gaming and entertain-\nment ﬁrm focused on developing the next\ngeneration of casual games based on the\nPlayStation ( R ) BRAVIA family of video\ngame machines for the North American\nmarket . Bluepoint is a wholly owned sub-\nsidiary of Bluehill ID Holdings L.P.\nBluepoint Games , Inc. is a highly expe-\nrienced , innovative entertainment sports\ngaming company whose products and ser-\nvices are used by some of the most rec-\nognized and respected names in the world\nof gaming including : Pokemon , Macau\n( Valve ) , Quattro , Super Smash Bros.\n, Good Neighbor Games , IGN Games ,\nVail Resorts , Kania ( Ocean Spray , Pem-\nberton and Roatenham ) , PURE Hold-\nings , TeenNick , National Amusements\n, SEGA Games , Cirrus ( Aircraft ) and\nwww.netapool.com.\nBluepoint Games , Inc. is a highly ex-\nperienced player in the growing genre of\ncasual games for both casual and active\ngaming enthusiasts . Bluepoint is an early\nstage Company with a signiﬁcant follow-\ning among youth and adults in Europe and\nthe United States with an impressive track\nrecord in global on-line gaming opportuni-\nties .\n23\nNursing Homes : Genworth ’s 2009 Cost\nof Care Survey , conducted by the Robert\nWood Johnson Foundation and released\ntoday , reveals the extent to which mem-\nbers of the U.S. population adheres to\npractices recommended since 1995 , in-\ncluding : a rolling three-hour ” Python for\nLife ” that fell asleep from 11 p.m. to 2\na.m. , sleep time from 11 p.m. to 3 a.m. ,\nspare time from 8 a.m. to 9 p.m. , and use\nof state-of-the art non-invasive technolo-\ngies . A remodeling and refurbishment of\nhospital facilities is underway as the na-\ntion ’s economy begins to gain momentum\n. Similar to the previous years , Thinking\nAbout Health - Hear how health plans are\nworking to address various congressional\nproposals to advance best practices in pa-\ntient care and provide greater accountabil-\nity , advocacy and transparency to con-\nsumers .\nNursing Homes : Genworth ’s 2009 Cost of\nCare Survey is based on interviews with\n516 family , friends and neighbors of in-\nsured and self-employed people conducted\nfrom Jan .\nNursing Homes : Genworth ’s 2009 Cost\nof Care Survey is based on a double-\nblind , randomized , double-blind ,\nplacebo-controlled survey which involved\nan assessment of the cost-eﬀectiveness of\nhealthcare associated with an adequate\ndiet and regular physical activity com-\npared to its managed-care counterparts .\nThe margin of error for this survey is + /\n- 3.3 percentage points at the 95 percent\nlevel of conﬁdence .\nNursing Homes : Genworth ’s 2009 Cost of\nCare Survey , conducted by Harris Inter-\nactive , performed signiﬁcantly worse than\na control group of its peers who provided\ncare but were not able to oﬀer health care\nto their employees .\nNursing Homes : Genworth ’s 2009 Cost\nof Care Survey , conducted by CareScout\n( R ) and published in the April 2009 is-\nsue , evaluated ﬁndings from the 10-year\n, nearly 900,000-member Specialty Health\nManagement Association ’s more than\n6,000 professionals living in the United\nStates .\nNursing Homes : Genworth ’s 2009 Cost\nof Care Survey includes a series of health\nand medical cost reports on more than\n100 home medical equipment and related\nproducts , including more than 3.9 million\nunits of durable medical equipment . IBC\n’s cost of more than $ 100 billion is a sig-\nniﬁcant portion of Medicare spending on\nhome health care .\nTable 3: More generations from a state-of-the-art Transformer model trained on GBW, seeded with\npreﬁxes of sentences from the holdout validation set.\n24"
}