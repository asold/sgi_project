{
  "title": "CharBERT: Character-aware Pre-trained Language Model",
  "url": "https://openalex.org/W3095771422",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1914390314",
      "name": "Ma, Wentao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2349293236",
      "name": "Cui, Yiming",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2983530483",
      "name": "Si, Chenglei",
      "affiliations": [
        "University of Maryland, College Park"
      ]
    },
    {
      "id": "https://openalex.org/A1966340437",
      "name": "Liu Ting",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1845590260",
      "name": "Wang Shijin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352985049",
      "name": "Hu Guoping",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963661177",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2767899794",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W2799074487",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W3023532425",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W2811010710",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2964185324",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W2550837020",
    "https://openalex.org/W2951815760",
    "https://openalex.org/W3177028045",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2963336393",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4302343710"
  ],
  "abstract": "Most pre-trained language models (PLMs) construct word representations at\\nsubword level with Byte-Pair Encoding (BPE) or its variations, by which OOV\\n(out-of-vocab) words are almost avoidable. However, those methods split a word\\ninto subword units and make the representation incomplete and fragile. In this\\npaper, we propose a character-aware pre-trained language model named CharBERT\\nimproving on the previous methods (such as BERT, RoBERTa) to tackle these\\nproblems. We first construct the contextual word embedding for each token from\\nthe sequential character representations, then fuse the representations of\\ncharacters and the subword representations by a novel heterogeneous interaction\\nmodule. We also propose a new pre-training task named NLM (Noisy LM) for\\nunsupervised character representation learning. We evaluate our method on\\nquestion answering, sequence labeling, and text classification tasks, both on\\nthe original datasets and adversarial misspelling test sets. The experimental\\nresults show that our method can significantly improve the performance and\\nrobustness of PLMs simultaneously. Pretrained models, evaluation sets, and code\\nare available at https://github.com/wtma/CharBERT\\n",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 39–50\nBarcelona, Spain (Online), December 8-13, 2020\n39\nCharBERT: Character-aware Pre-trained Language Model\nWentao Ma†, Yiming Cui‡†, Chenglei Si¶†, Ting Liu‡, Shijin Wang†§, Guoping Hu†\n†State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China\n‡Research Center for Social Computing and Information Retrieval (SCIR),\nHarbin Institute of Technology, Harbin, China\n§iFLYTEK AI Research (Hebei), Langfang, China\n¶University of Maryland, College Park, MD, USA\n†§{wtma,ymcui,clsi,sjwang3,gphu}@iflytek.com\n‡{ymcui,tliu}@ir.hit.edu.cn\nAbstract\nMost pre-trained language models (PLMs) construct word representations at subword level with\nByte-Pair Encoding (BPE) or its variations, by which OOV (out-of-vocab) words are almost\navoidable. However, those methods split a word into subword units and make the represen-\ntation incomplete and fragile. In this paper, we propose a character-aware pre-trained lan-\nguage model named CharBERT improving on the previous methods (such as BERT, RoBERTa)\nto tackle these problems. We ﬁrst construct the contextual word embedding for each token\nfrom the sequential character representations, then fuse the representations of characters and\nthe subword representations by a novel heterogeneous interaction module. We also propose\na new pre-training task named NLM (Noisy LM) for unsupervised character representation\nlearning. We evaluate our method on question answering, sequence labeling, and text classi-\nﬁcation tasks, both on the original datasets and adversarial misspelling test sets. The exper-\nimental results show that our method can signiﬁcantly improve the performance and robust-\nness of PLMs simultaneously. Pretrained models, evaluation sets, and code are available at\nhttps://github.com/wtma/CharBERT.\n1 Introduction\nUnsupervised pre-trained language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al.,\n2019) have achieved surprising results on multiple NLP benchmarks. These models are pre-trained over\nlarge-scale open-domain corpora to obtain general language representations and then ﬁne-tuned for spe-\nciﬁc downstream tasks. To deal with the large vocabulary, these models use Byte-Pair Encoding (BPE)\n(Sennrich et al., 2016) or its variations as the encoding method. Instead of whole words, BPE per-\nforms statistical analysis of the training corpus and split the words into subword units, a hybrid between\ncharacter- and word-level representation.\nEven though BPE can encode almost all the words in the vocabulary into WordPiece tokens with-\nout OOV words, it has two problems: 1) incomplete modeling: the subword representations may not\nincorporate the ﬁne-grained character information and the representation of the whole word; 2) fragile\nrepresentation: minor typos can drastically change the BPE tokens, leading to inaccurate or incomplete\nrepresentations. This lack of robustness severely hinders its applicability in real-world applications. We\nillustrate the two problems by the example in Figure 1. For a word like backhand, we can decompose\nits representation at different levels by a tree with a depth of 3: the complete word at the ﬁrst layer, the\nsubwords at the second layer, and the last characters. BPE only considers representations of subwords\non the second layer and misses the potentially useful information at the ﬁrst and last layer. Furthermore,\nif there is noise or typo in the characters (e.g., missing the letter ‘k’), the subwords and its number at the\nsecond layer will be changed at the same time. Models relying purely on these subword representations\nthus suffer from this lack of robustness.\nWe take the CoNLL-2003 NER development set as an example. Nearly 28% of the nouns words will\nbe split into more than one subword with BERT tokenizer. When we randomly remove a character from\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n40\nbackhand\nback hand\nb a c h a d\nbachand\nb ach and\na h a n d\nUHPRYH\u0003ņNŇ\nnk b c\nFigure 1: The internal structure tree of backhand, which has two subwords: back,hand. If the letter k is\nremoved, the subwords will change to b, ach, and.\nthe noun words in the dataset like the example in Figure 1, about 78% of the words will be tokenized\ninto completely different subwords, and 77% of the words have a different number of subwords.\nIf we focus on the leaf nodes in the example, we can ﬁnd that the difference of the two trees is only\none leaf. So we extend the pre-trained language models by integrating character information of words.\nThere are two challenges for character integration: 1) how to model character information for whole\nwords instead of subwords; 2) how to fuse the character representations with the subwords information\nin the original pre-trained models.\nWe propose a new pre-training method CharBERT (BERT can also be replaced by other pre-trained\nmodels like RoBERTa) to solve these problems. Instead of the traditional CNN layer for modeling the\ncharacter information, we use the context string embedding (Akbik et al., 2018) to model the word’s\nﬁne-grained representation. We use a dual-channel architecture for characters and original subwords and\nfuse them after each transformer block. Furthermore, we propose an unsupervised character learning\ntask, which injects noises into characters and trains the model to denoise and restores the original word.\nThe main advantages of our methods are: 1) character-aware: we construct word representations from\ncharacters based on the original subwords, which greatly complements the subword-based modeling. 2)\nrobustness: we improve not only the performance but also the robustness of the pre-trained model; 3)\nmodel-agnostic: our method is agnostic to the backbone PLM like BERT and RoBERTa, so that we can\nadapt it to any transformer-based PLM. In summary, our contributions in this paper are:\n•We propose a character-aware pre-training method CharBERT, which can enrich the word repre-\nsentation in PLMs by incorporating features at different levels of a word;\n•We evaluate our method on 8 benchmarks, and the results show that our method can signiﬁcantly\nimprove the performance compared to the strong BERT and RoBERTa baselines;\n•We construct three character attack test sets on three types of tasks. The experimental results indi-\ncate that our method can improve the robustness by a large margin.\n2 Related Work\nPre-trained Language Model. Early pre-trained language models (PLMs) like CoVe (McCann et al.,\n2017) and ELMo (Peters et al., 2018) are pre-trained with RNN-based models, which are usually used as\na part of the embedding layer in task-speciﬁc models. GPT (Radford et al., 2019a) used the transformer\ndecoder for language modeling by generative pre-training and ﬁne-tuned for various downstream tasks.\nBERT (Devlin et al., 2019) pre-trains the transformer encoder and uses self-supervised pre-training on\nthe larger corpus, achieving surprising results in multiple natural language understanding (NLU) bench-\nmarks. Other PLMs such as RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), ALBERT (Lan et\nal., 2019) and ELECTRA (Clark et al., 2019), improve on previous models with various improvements\non the model architectures, training methods or pre-training corpora.\nTo handle the large vocabularies in natural language corpora, most PLMs process the input sequence\nin subword units by BPE (Sennrich et al., 2016) instead of whole words, split a word into subwords by\nthe byte pair encoding compression algorithm. The size of BPE vocabulary usually ranges from 10K-\n100K subword units, most of which are Unicode characters. Radford et al. (2019b) introduce another\nimplementation that uses bytes instead of Unicode characters as the base subword units, allowing BPE\nto encode any input sequence without OOV words with a modest vocabulary size (50K).\n41\nCharacter Representation. Traditional language models employ a pre-deﬁned vocabulary of words,\nbut they cannot handle out-of-vocabulary words well. Character language models (CLMs) can mitigate\nthis problem by using a vocabulary of characters and modeling the character distribution for language\nmodeling (Sutskever et al., 2011). CLMs have been shown to perform competitively on various NLP\ntasks, such as neural machine translation (Lee et al., 2017) and sequence labeling (S ¸ahin and Steed-\nman, 2018; Akbik et al., 2018). Furthermore, character representation has also been used to construct\nword representation; for example, Peters et al. (2018) construct the contextual word representation with\ncharacter embeddings and achieve signiﬁcant improvement.\nAdversarial Attack. PLMs are fragile to adversarial attacks, where human-imperceptible perturbations\nadded to the original examples fool models to make wrong predictions. Jia and Liang (2017) and Si et\nal. (2020) show that state-of-the-art reading comprehension models can be fooled even with black-box\nattacks without accessing model parameters. Other white-box attacks (Alzantot et al., 2018; Ren et al.,\n2019; Jin et al., 2020; Zang et al., 2020) use gradients or model prediction scores to ﬁnd adversarial\nword substitutes as effective attacks. For character-level attacks, Belinkov and Bisk (2017) studied how\nsynthetic noise and noise from natural sources affect character-level machine translations. Ebrahimi\net al. (2018) investigated adversarial examples for character-level neural machine translation with a\nwhite-box adversary. To defend against character-level attacks, Pruthi et al. (2019) propose to place a\nword recognition model before the downstream classiﬁer to perform word spelling correction to combat\nspelling mistakes.\nHeterogeneous Representation Fusion. In our work, we need to fuse heterogeneous representations\nfrom two different sources. Similar modules have been applied before under different settings such as\nmachine reading comprehension (Seo et al., 2017; Yang et al., 2017) and pre-trained language mod-\nels (Zhang et al., 2019; Zhang et al., 2020). Different from these works, we design a two-step fusion\nmodule to fuse the character and subword representations by a interactive way, which can be extended to\nintegrate other information into language model (e.g. diacritics or external knowledge).\n3 Methodology\nIn this section, we present the overall framework of CharBERT and its submodules, including the model\narchitecture in Section 3.2, the character encoder in Section 3.3, the heterogeneous interaction module in\nSection 3.4, the new pre-training task in Section 3.5, and the ﬁne-tuning method in Section 3.6.\n3.1 Notations\nWe denote an input sequence as {w1,...,w i,...,w m}, where wi is a subword tokenized by BPE and m\nis the length of the sequence in subword-level. Each token wi is consisted of characters {ci\n1,...,c i\nni}and\nni is the subword’s length. We denote the length of input in character-level asN, where N = ∑m\ni=1 ni.\n3.2 Model Architecture\nAs shown in Figure 2, we use a dual-channel architecture to model the information from subwords and\ncharacters, respectively. Besides the transformer layers from the original pre-trained model like BERT,\nthe core modules of CharBERT are: 1) the character encoder, responsible for encoding the character\nsequence from the input tokens; 2) heterogeneous interaction, fuse the information from the two sources\nand construct new independent representations for them.\nWe model the input words as sequences of characters to catch the character information within and\namong subwords, a supplement for WordPiece embedding. The character-level representation is hetero-\ngeneous with subword-level representation from the embedding layer of pre-trained models as they come\nfrom different sources. However, they capture information at the different granularity and complement\neach other. In order to enable them to enrich each other effectively, we design a heterogeneous interac-\ntion module with two steps: 1) fuse: fuse the information from the dual-channel based on the CNN layer\n(Kim, 2014); 2) split: build new representations for each channel based on residual connection.\n42\n\u0001CLS] Wash It Away … mayAll refer\nBERT Embedding\nTransformer\n….\nTransformer\nTransformer\nMLM\nHeterogeneous Interaction\nToken Channel\nHeterogeneous Interaction\nHeterogeneous Interaction\nToken Repr. Char Repr.\nFeed Forward Feed Forward\nCNN \nFeed Forward & Act\nAdd & Norm Add & Norm\nNew Token Repr. New Char Repr.\nFeed Forward & Act\nNLM\nChar Channel\nHeterogeneous Interaction\nChar Channel\nChar Channel\nToken Channel\nToken Channel\nCharacter Encoder\nCharacter Encoder\nFusion\nDivide\nChar ChannelToken Channel\nW a s h r e\nWash\nf e\nrefer\nr\nGRU\n…\n…\nFigure 2: The neural architecture of CharBERT. The left part is the main structure of CharBERT based\non the original pre-trained models like BERT. The modules in the right part are the heart of CharBERT:\ncharacter encoder and heterogeneous interaction. (best viewed in color)\n3.3 Character Encoder\nIn this module, we need to form token-level embeddings with the input sentences as sequences of charac-\nters. We ﬁrst convert the sequences of tokens into characters and embed them into ﬁxed-size vectors. We\nthen apply a bidirectional GRU layer (Cho et al., 2014) to construct the contextual character embeddings,\nwhich can be formulated by\nei\nj = Wc ·ci\nj ; hi\nj(x) =Bi-GRU(ei\nj); (1)\nwhere Wc is the character embedding matrix , hi\nj is the representation for jth character in the ith token.\nWe apply the bi-GRU on the characters with a length of N for the whole input sequence instead of\na single token, building the representations from the characters within and among the subwords. To\nconstruct token-level embeddings, we concatenate the hidden of the ﬁrst and last character of the token.\nhi(x) = [hi\n1(x); hi\nni(x)] (2)\nwhere ni is the length of ith token and hi(x) is the token-level embedding from characters. The con-\ntextual character embeddings are derived by characters and can also catch the full word information by\nbi-GRU layers.\n3.4 Heterogeneous Interaction\nThe embeddings from characters and the original token-channel are fed into the same transformer layers\nin pre-trained models. The token and char representations are fused and split by the heterogeneous\ninteraction module after each transformer layer.\nIn the fusion step, the two representations are transformed by different fully-connected layers. Then\nthey are concatenated and fused by a CNN layer, which can be formulated by\nt′\ni(x) =W1 ∗ti(x) +b1 ; h′\ni(x) =W2 ∗hi(x) +b2 (3)\nwi(x) = [t′\ni(x); h′\ni(x)] ; mj,t = tanh(Wj\n3 ∗wt:t+sj−1 + bj\n3) (4)\nwhere ti(x) is the token representations, W, bare parameters, wt:t+sj−1 refers to the concatenation of\nthe embedding of (wt,...,wt+sj−1), sj is the window size of jth ﬁlter, and mis the fusion representation\nwith the dimension same with the number of ﬁlters.\n43\n[CLS] Wash It All Away is a song [SEP]\n[CLS] [MASK] It ALiL Awya is a sng [SEP]\n[CLS] [MASK] It AL ##wy a s ##ng [SEP]is##a##iL A\nCharBERT\nToken Repr. Char Repr.\nWash Away songALL\n0/0 1/0\n\u000eL D\\\u0010\\D \u0010R\nFigure 3: Character-aware language model pretraining. The MLM task is similar to the one in BERT,\nbut with lower mask probability (10%). The NLM task introduces the character noises by dropping,\nadding and swapping internal characters within the word and predicts the original whole word by the\nrepresentation from the character channel.\nIn the divide step, we transform the fusion representations by another fully connected layer with\nGELU activation layer (Hendrycks and Gimpel, 2016). We then use the residual connection to retain the\nrespective information from the two channels.\nmt\ni(x) =δ(W4 ∗mi(x) +b4) ; mh\ni (x) =δ(W5 ∗mi(x) +b5) (5)\nTi(x) = ti(x) +mt\ni(x) ; Hi(x) =hi(x) +mh\ni (x) (6)\nWhere δ is the activation function GELU, T and H is the new representations of the two channels. To\nprevent vanishing or exploding of gradients, a layer normalization (Ba et al., 2016) operation is applied\nafter the residual connection.\nBy the fusion step, the representations from the two channels can enrich each other. By the divide\nstep, they can keep their unique features from token and character, and learn the different representations\nin dual-channel by their own pre-training tasks.\n3.5 Unsupervised Character Pre-training\nTo learn the representation from the internal morphological feature within the words, we propose an\nunsupervised character pre-training task named noisy language modeling (NLM) for CharBERT. We\nintroduce some character noises into the words, and predict the original words by the representations\nfrom the character channel as shown in Figure 3.\nFollowing the previous work (Pruthi et al., 2019), we change the original character sequence by drop-\nping, adding, and swapping internal characters within the whole word. As the number of subwords may\nbe changed after introducing the noise, the objective of the pre-training tasks is to predict the whole\noriginal word instead of subwords. We construct a new word-level vocabulary as the prediction space\nH′\ni = δ(W6 ∗Hi + b5) ; p(Wj|Hi) = exp(linear(H′\ni)·Wj)∑S\nk=1 exp(linear(H′\ni)·Wk) (7)\nwhere linear(·) is a linear layer, Hi is the token representations from the character channel,Sis the size\nof the word-level vocabulary.\nSimilar to BERT, CharBERT also adopts masked language modeling (MLM) as the pre-training task\nfor the token channel. Different from NLM , MLM enables CharBERT to capture lexical and syntactic\ninformation in token-level. Note that, we only mask or replace the tokens without any character noise\nfor MLM. More details of the pre-training tasks can be found in Devlin et al. (2019).\n3.6 Fine-tuning for Speciﬁc Tasks\nMost of the natural language understanding tasks can be simply divided into two groups: token-level\ntasks like sequence labeling and sequence-level tasks, such as the text classiﬁcation tasks. For token-\nlevel tasks, we concatenate the ﬁnal output embeddings from the two channels in CharBERT as the input\n44\nSQuAD Text Classiﬁcation\nModels 1.1 2.0 CoLA MRPC QQP QNLI\nEM F1 EM F1 Corr Acc Acc Acc\nBERT (Devlin et al., 2019) 80.5 88.5 73.7 76.3 57.4 86.7 90.6 90.7\nCharBERT 82.9 89.9 75.7 78.6 59.1 87.8 91.0 91.7\nRoBERTa (Liu et al., 2019) 84.6 91.5 80.5 83.7 62.1 90.2 91.2 92.8\nXLNet (Yang et al., 2019) - - 80.2 - 60.2 88.2 91.4 91.7\nCharBERTRoBERTa 84.0 90.9 81.1 84.5 61.8 90.4 91.6 93.4\nTable 1: Experimental results of our model and previous strong pre-trained models under BERT base\nsetting on the dev set of Question Answering and Text Classiﬁcation tasks. We report exact match (EM)\nand F1 scores for SQuAD, Matthew’s correlation for CoLA, and accuracy for other tasks.\nfor ﬁne-tuning. For sequence-level tasks, most of the pre-trained models use the representation of a\nspecial token like [CLS] for prediction. In this paper, to adequately take advantage of the character- and\ntoken-level information in the sequence, we perform average over all the embeddings after concatenating\nthe representations from the two channels in the last layer of CharBERT for sequence level classiﬁcation.\n4 Experiments\nIn this section, we present the pre-training details of CharBERT and the ﬁne-tuning results on three kinds\nof tasks: question answering, sequence labeling, and text classiﬁcation. Furthermore, we construct three\ncharacter attack test set from those tasks and evaluate the robustness of CharBERT.\n4.1 Experimental Setup\nWe use BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) base as our main baseline, where the\nmodels consist of 12 transformer layers, with 768 hidden size and 12 attention heads. The vocabulary of\nBERT and RoBERTa contains 30K and 50K subword units respectively, and the total parameters of them\nare 110M and 125M. The size of additional parameters for BERT and RoBERTa is 5M, which means the\ncharacter channel is much smaller than the token channel in original pre-trained models. We change 15%\nof the input words for NLM and lower the mask probability from 15% to 10% in MLM task to avoid too\nmuch information loss in the sequence.\nWe use English Wikipedia (12G, 2,500M words) as our pre-training corpus and adopt the parameters\nof the pre-trained models to initialize the token channel of CharBERT. In the pre-training step, we set the\nlearning rate as 5e-5, batch size as 32, and pre-train CharBERT 320K steps. The word-level vocabulary\ncontains 30K words for NLM, and the size of the character vocabulary is 1000. We use 2 NVIDIA Tesla\nV100 GPUs, with 32GB memory and FP16 for pre-training, which is estimated to take 5 days. For ﬁne-\ntuning, we ﬁnd the following ranges of possible values work well on the downstream tasks, i.e., batch\nsize 16, learning rate: 3e-5, 2e-5, number of epochs ranging from 2 to 6.\nFor the optimizer, we use the same setting with the pre-trained model in the token channel like BERT\nand RoBERTa, both in pre-training and ﬁne-tuning steps. For experimental comparison, we mainly\ncompare CharBERT with previous state-of-the-art pre-trained models in BERTbase setting. We will also\npre-train CharBERT with pre-trained models in BERTlarge setting in the future.\n4.2 Results on Question Answering (SQuAD)\nThe Stanford Question Answering Dataset (SQuAD) task requires to extract the answer span from a\nprovided passage based on speciﬁed questions. We evaluate on two versions of the dataset: SQuAD\n1.1 (Rajpurkar et al., 2016) and SQuAD 2.0 (Rajpurkar et al., 2018). For any question in SQuAD 1.1,\nthere is always one or more answers in the corresponding passage. While for some questions in SQuAD\n2.0, there is no answer in the passage. In the ﬁne-tuning step for SQuAD, we concatenate the outputs\nfrom the character and token channel from CharBERT and use a classiﬁcation layer to predict whether\nthe token is a start or end position of the answer. For SQuAD 2.0, we use the probability on the token\n[CLS] as the results of no answer and search the best threshold for it.\n45\nQNLI CoNLL-2003 NER SQuAD 2.0\nModels Original Attack Original Attack Original Attack\nBERT 90.7 63.4 91.24 60.79 76.3 50.1\nAdvBERT 90.8 75.8 90.68 71.47 76.6 52.4\nBERT+WordRec 84.0 76.1 82.52 67.79 63.5 55.2\nCharBERT 91.7 80.1 91.81 76.14 78.6 56.3\nTable 2: Experimental results of robustness evaluation. We report accuracy for QNLI, F1-score for\nCoNLL-2003 NER and SQuAD 2.0. We construct the ‘Attack’ sets with ‘Original’ ones by introducing\nfour kinds of character-level noise.\nThe results are reported on Table 1. For comparable experiments, all of the results are reported by a\nsingle model without other tricks like data augmentation. We can ﬁnd that our character-aware models\n(CharBERT, CharBERTRoBERTa) outperform the baseline pre-trained models except for RoBERTa in\nSQuAD 1.1, which indicates the character information probably can not help the remaining questions.\n4.3 Results on Text Classiﬁcation\nWe select four text classiﬁcation tasks for evaluation: CoLA (Warstadt et al., 2019), MRPC (Dolan and\nBrockett, 2005), QQP, and QNLI (Wang et al., 2018). CoLA is a single-sentence task annotated with\nwhether it is a grammatical English sentence. MRPC is a similarity task consisted of sentence pairs auto-\nmatically extracted from online news sources, with human annotations for whether the sentences in pairs\nare semantically equivalent. QQP is a paraphrase task with a collection of question pairs from the com-\nmunity question-answering website Quora, annotated with whether a pair of questions are semantically\nequivalent. QNLI is an inference task consisted of question-paragraph pairs, with human annotations for\nwhether the paragraph sentence contains the answer.\nThe results are reported in Table 1. For the BERT based experiments, CharBERT signiﬁcantly out-\nperforms BERT in the four tasks. In the RoBERTa based part, the improvement becomes much weaker\nfor the stronger baseline. We ﬁnd that the improvement in text classiﬁcation is weaker than the other\ntwo kinds of tasks, which may be because the character information contributes more to token-level\nclassiﬁcation tasks like SQuAD and sequence labeling.\n4.4 Results on Sequence Labeling\nTo evaluate performance on token tagging tasks, we ﬁne-tune CharBERT on CoNLL-2003 Named Entity\nRecognition (NER) (Sang and De Meulder, 2003) and Penn Treebank POS tagging datasets. 1 CoNLL-\n2003 NER dataset consists of 300k words, which have been annotated as Person, Organization, Miscel-\nlaneous, Location,or Other. The POS tagging dataset comes from the Wall Street Journal (WSJ) portion\nof the Penn Treebank, containing 45 different POS tags and more than 1 million words. For ﬁne-tuning,\nwe feed the representations from the dual-channel of CharBERT into a classiﬁcation layer over the label\nset. Following the setting in BERT (Devlin et al., 2019), we use the hidden state corresponding to the\nﬁrst sub-token as input to the classiﬁer.\nThe results in reported in Table 3. We introduce two strong baselines Meta-BiLSTM (Bohnet et al.,\n2018) and Flair Embeddings (Akbik et al., 2018) in the two tasks for comparison. Our model (CharBERT,\nCharBERTRoBERTa) exceeds the baseline pre-trained models BERT and RoBERTa signiﬁcantly (p-value\n≤0.05), and we set new state-of-the-art results on the POS tagging dataset.\n4.5 Robustness Evaluation\nWe conduct the robustness evaluation on adversarial misspellings with BERT based models. Followed\nthe previous work (Pruthi et al., 2019), we use four kinds of character-level attack: 1) dropping: drop a\nrandom character within the word; 2) adding: add a random character into the word; 3) swapping: swap\ntwo adjacent characters within the word; 4) keyboard: replace a random internal char with a nearby char\n1https://catalog.ldc.upenn.edu/LDC2015T13\n46\nNER POS\nModels F1-score Accuracy\nMetaBiLSTM - 97.96\nFlair Embeddings 93.09 97.85\nBERT 91.24 97.93\nCharBERT 91.81 98.05\nRoBERTa 92.22 97.98\nCharRERTRoBERTa 92.49 98.09\nTable 3: Experimental results of our model\nand previous strong models on the test set of\nCoNLL-2003 NER and WSJ Postag datasets.\nSQuAD 2.0 NER QNLI QNLI-Att\nModels F1 F1 Acc Acc\nCharBERT 78.6 91.81 91.7 80.1\nw/o GRU 77.7 91.45 90.8 76.9\nw/o HI 76.8 91.28 90.9 77.7\nw/o NLM 78.3 91.69 91.4 68.3\nAdvBERT 77.4 91.03 90.7 75.8\nBERT 76.3 91.24 90.7 63.4\nTable 4: Experimental results of ablation study. Ad-\nvBERT can be considered as CharBERT without the\nthree modules but in the same pre-training setting.\non the keyboard. We only apply the attack perturbation on words with length no less than 4 and we\nrandomly select one of the four attacks to apply on each word.\nFor the evaluation tasks, we consider all the three types of tasks: questioning answering, sequence\nlabeling, and text classiﬁcation. That is different from the previous works on adversarial attack and\ndefense (Ebrahimi et al., 2018; Pruthi et al., 2019), which usually focus only on a speciﬁc task like\nmachine translation or text classiﬁcation. We select the SQuAD 2.0, CoNLL-2003 NER, and QNLI\ndatasets for the evaluation.\nFor the dev set in SQuAD 2.0, we only attack the words in questions. For CoNLL-2003 NER and\nQNLI, we attack all the words under the length constraint. In this set-up, we modify 51.86% of the words\nin QNLI, 49.38 % in CoNLL-2003 NER, and 22.97% words in SQuAD 2.0. We compare our CharBERT\nmodel with three baselines: 1) the original BERT model; 2) BERT model with adversarial training\n(AdvBERT), which is pre-trained by the same data and hyper-parameters with CharBERT; 3) BERT\nwith word recognition and pass-through back-off (BERT+WordRec), we use the pre-trained scRNN typo-\ncorrector from (Pruthi et al., 2019). All the inputs are ‘corrected’ by the typo-corrector and fed into a\ndownstream model. We replace any OOV word predicted by the typo-corrector with the original word\nfor better performance.\nThe results are reported in Table 2. The performance of BERT drops more than 30% on the mis-\nspelling test sets, which shows that BERT is brittle for the character misspellings attack. AdvBERT and\nBERT+WordRec have moderate improvement on the misspellings attack sets, compared to the BERT\nbaseline. We ﬁnd that the performance of BERT+WordRec has dropped signiﬁcantly in the original set\ndue to the error recall for normal words. In comparison, CharBERT has the least performance drop than\nthe other baselines under the character attacks, which denotes that our model is the most robust for the\nmisspellings attack in multiple tasks, while still achieving improvement on the original test sets at the\nsame time. Note that AdvBERT was pre-trained on the same data for the same number of training steps\nas our CharBERT model, except that AdvBERT does not have our proposed new methods. Thus, the\ncomparison between AdvBERT and CharBERT can highlight the advantages of our method.\n4.6 Ablation Study\nWe consider the three modules in CharBERT: character encoder, heterogeneous interaction, and the pre-\ntraining task NLM in the ablation experiments. For character encoder, we remove the GRU layer and use\nthe character embeddings as the character representation (w/o GRU). For the heterogeneous interaction\nmodule, we remove the whole module and the two channels have no interaction with each other in the\nmodel (w/o HI). For the pre-training tasks, we remove NLM and concatenate the representations from the\ntwo channels in CharBERT for MLM in the pre-training step (w/o NLM). At last, we also compare with\nthe two baseline models AdvBERT and BERT. We can consider AdvBERT as a fair baseline model with\nthe same weight initialization, training data and training steps as CharBERT, without our three proposed\nmodules. We select four tasks: SQuAD 2.0, CoNLL-2003 NER, QNLI, and QNLI with character attack\n(QNLI-Att) from the four parts of the experiments above for evaluation.\nWe can see the ablation results from Table 4. When we remove the GRU layer or heterogeneous\n47\nAll Subword Word90.0\n90.5\n91.0\n91.5\n92.0\n92.5\n93.0\n93.5\n94.0F1 score(%)91.23\n90.28\n93.1\n91.81\n90.96\n93.39\nBERT\nCharBERT\nFigure 4: The performances of\ndifferent parts in ConNLL-2003\nNER test set. ‘Subword’ means\nthe words will be split into more\nthan one subwords.\nI\nI\nthink\nthnik\nit\nit\n'\n'\ns\ns\nfair\nfar\nto\nto\ngive\ngie\nthem\ntjhem\na\na\nchance\ncahnce\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150\n0.175\n0.200Sensitivity\nBERT\nAdvBERT\nCharBERT\nFigure 5: The sensitivity result\nof a sample in CoNLL-2003 NER\ntest set. The words ‘think, fair,\ngive, them, chance’ are changed in\nthe attack set.\n88.0 88.5 89.0 89.5 90.0 90.5 91.0\nF1 score(%)\nChar\nSum\nToken\nBERT\nConcat\n88.69\n90.13\n90.23\n90.28\n90.39\nFigure 6: Embedding comparison\nwith LSTM on the CoNLL-2003\nNER task. All the embeddings are\ngenerated by the hidden of the last\nlayer.\ninteraction module, the performance drops signiﬁcantly in all the tasks. While we remove NLM in the\npre-training step, the model has a similar performance in SQuAD 2.0, NER, and QNLI tasks, but has a\nmuch worse performance in QNLI-Att set, which denotes that the pre-training task signiﬁcantly improves\nthe robustness of CharBERT. Furthermore, CharBERT (w/o NLM) still has a much better performance\nthan BERT, which means CharBERT has better robustness even without the pre-training task.\n5 Analysis\nIn this section, we conduct some experiments on CoNLL-2003 NER task with the test set to further\nanalyze the ‘incomplete modeling’ and ‘fragile representation’ problems. In the end, we compare the\ncontextual word embeddings generated by BERT and CharBERT with a feature-based method.\n5.1 Word vs. Subword\nTo ﬁnd out the effect of ‘incomplete modeling’ problem on the word representation, we divide all the\nwords in the dataset into ‘Word’ and ‘Subword’ groups by whether the word will be split into multiple\nsubwords. In fact, the ‘Subword’ group only has 17.8% of words but has 45.3% of named entities.\nThe results of BERT and CharBERT are in Figure 4. For the results of the same model in different\ngroups, we ﬁnd that the performance in ‘Subword’ group are signiﬁcantly lower than the ones in ‘Word’\ngroup, which indicates that the representations based on subwords may be insufﬁcient for the words. For\nthe results of different models in the same group, the improvement of CharBERT in ‘Subword’ group\nis 0.68%, which is much higher than that in ‘Word’ group (0.29%). That means the main improvement\ncomes from the ‘Subword’ part, and CharBERT can generate better representations for the words with\nmultiple subwords. In other words, CharBERT can alleviate the ‘incomplete modeling’ problem by\ncatching the information among different subwords with the GRU layer.\n5.2 Robustness Analysis\nIn this part, we further explore how the contextual word embeddings change over the character noise.\nSpeciﬁcally, we need to ﬁnd out whether the representations from CharBERT are more or less sensitive to\nchanges in character level. We deﬁne a metric to measure the sensitivity of pre-trained language models\nover a speciﬁc dataset\nS = 1\nm\n∑m\ni=1(−1\n2 cos(h(ti),hi(t′\ni)) + 0.5) (8)\nwhere cosis cosine similarity, m is the number of words in dataset, his the last hidden in the model, ti\nis the ith word in the set and t′\ni is the same word with character noise. In extreme cases, if a model is not\nsensitive at all to the character attack, the two vectors would be the same, yielding S=0.\nWe conduct the experiment with the original set and the set with character attacks. For the words with\nmultiple subwords, we use the hidden of the ﬁrst subword as the word embedding, which is consistent\n48\nwith the ﬁne-tuning setting. For example, we calculate the sensitivity for each word in the sentence in\nthe sample in Figure 5, and the average of the results is S.\nTo our surprise, the sensitivity results of the three models are: SBERT = 0.0612, SAdvBERT = 0.0407,\nSCharBERT = 0.0986, but the robustness of the three models is: BERT<AdvBERT <CharBERT (Section\n4.5), which means there is no signiﬁcant correlation between robustness and sensitivity. That is different\nfrom the previous work Pruthi et al. (2019), which shows word recognition models with low sensitivity\nare more robust. After observing the results of many samples, we ﬁnd that for the words without character\nnoise, the sensitivity of BERT and CharBERT have no distinct difference. While for the words with noise\nsuch as ‘think-thnik,’ ‘fair-far’ in the example, the sensitivity of CharBERT is much higher than BERT.\nOn the other hand, the sensitivity of AdvBERT is lower than BERT in most of the words.\nThat indicates CharBERT improves the robustness using a different way with adversarial training\n(AdvBERT). It may be because we use the representations of noisy words to predict the original word in\nNLM, but AdvBERT treats all the words in the same way in the pre-training step, which leads CharBERT\nto construct the representations for the noisy words in a different way. The result inspires us that, we can\nimprove the robustness of model directly by better representation for the noise, which is different from\nimproving the robustness by additional word recognition modules or adversarial training.\n5.3 Feature-based Comparison\nThe contextual word embeddings from pre-trained models are usually used as input features in task-\nspeciﬁc models. To explore whether the character information can enrich the word representation, we\nevaluate the contextual embedding generated by BERT and CharBERT. Following Devlin et al. (2019),\nwe use the same input representation as Section 4.4 without ﬁne-tuning any parameters of BERT or\nCharBERT. Those contextual embeddings are used as embedding features to a randomly initialized two-\nlayer 768-dimensional Bi-LSTM before the classiﬁcation layer. For CharBERT, we consider embeddings\nfrom three sources: token channel, character channel, sum, and concatenating of the two channels.\nThe results are reported in Figure 6. We ﬁnd that the embeddings from the token channel of CharBERT\nand BERT have similar performances, which denotes that the token channel retrains the information in\nBERT. The embeddings from the character channel have worse performance, which may be due to the\nfewer data and training steps for this part of parameters. When we concatenate the embeddings from the\ntoken and character channels, the model gets the best score. That indicates the character information can\nenrich the word embeddings, even with a lot fewer training data and steps.\n6 Conclusion\nIn this paper, we address the important limitations of current PLMs: incomplete modeling and lack of\nrobustness. To tackle these problems, we proposed a new pre-trained model CharBERT by injecting the\ncharacter-level information into PLMs. We construct the representations from characters by sequential\nGRU layers and use a dual-channel architecture for the subword and character. Furthermore, we pro-\npose a new pre-training task NLM for unsupervised character representation learning. The experimental\nresults show that CharBERT can improve both the performance and robustness of pre-trained models.\nIn the future, we will extend CharBERT to other languages to learn cross-lingual representations from\ncharacter information. We believe that CharBERT can bring even more improvements to morpholog-\nically rich languages like Arabic, where subwords cannot adequately capture the morphological infor-\nmation. On the other hand, we will extend CharBERT to defense other kinds of noise, e.g., word-level,\nsentence-level noise, to improve the robustness of PLMs comprehensively.\nAcknowledgement\nWe would like to thank all anonymous reviewers for their hard work on reviewing and providing valuable\ncomments on our paper. This work was supported by the National Natural Science Foundation of China\n(NSFC) via grant 61976072, 61632011, and 61772153.\n49\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018. Contextual string embeddings for sequence labeling. In\nProceedings of the 27th International Conference on Computational Linguistics, pages 1638–1649, Santa Fe,\nNew Mexico, USA, August. Association for Computational Linguistics.\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, M. Srivastava, and Kai-Wei Chang. 2018.\nGenerating natural language adversarial examples. In EMNLP.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nYonatan Belinkov and Yonatan Bisk. 2017. Synthetic and natural noise both break neural machine translation.\narXiv preprint arXiv:1711.02173.\nBernd Bohnet, Ryan McDonald, Gonc ¸alo Sim˜oes, Daniel Andor, Emily Pitler, and Joshua Maynez. 2018. Mor-\nphosyntactic tagging with a meta-BiLSTM model over context sensitive token encodings. In Proceedings of\nthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\n2642–2652, Melbourne, Australia, July. Association for Computational Linguistics.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. 2014. Learning phrase representations using rnn encoder-decoder for statistical machine\ntranslation. arXiv preprint arXiv:1406.1078.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019. Electra: Pre-training text\nencoders as discriminators rather than generators. In International Conference on Learning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186.\nWilliam B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In\nProceedings of the Third International Workshop on Paraphrasing (IWP2005).\nJavid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018. On adversarial examples for character-level neural machine\ntranslation. In Proceedings of the 27th International Conference on Computational Linguistics, pages 653–663,\nSanta Fe, New Mexico, USA, August. Association for Computational Linguistics.\nDan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415.\nRobin Jia and Percy Liang. 2017. Adversarial examples for evaluating reading comprehension systems. In\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2021–2031,\nCopenhagen, Denmark, September. Association for Computational Linguistics.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is bert really robust? a strong baseline for natural\nlanguage attack on text classiﬁcation and entailment. In AAAI.\nYoon Kim. 2014. Convolutional neural networks for sentence classiﬁcation. In Proceedings of EMNLP 2014,\npages 1746–1751. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations. In International Conference on\nLearning Representations.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann. 2017. Fully character-level neural machine translation with-\nout explicit segmentation. Transactions of the Association for Computational Linguistics, 5:365–378.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextual-\nized word vectors. In Advances in Neural Information Processing Systems, pages 6294–6305.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of NAACL-HLT-2018, pages 2227–\n2237. Association for Computational Linguistics.\n50\nDanish Pruthi, Bhuwan Dhingra, and Zachary C. Lipton. 2019. Combating adversarial misspellings with robust\nword recognition. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\npages 5582–5591, Florence, Italy, July. Association for Computational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2019a. Improving language understanding\nby generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019b. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8):9.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2383–2392, Austin, Texas, November. Association for Computational Linguistics.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable questions for\nSQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n2: Short Papers), pages 784–789, Melbourne, Australia, July. Association for Computational Linguistics.\nShuhuai Ren, Yihe Deng, Kun He, and W. Che. 2019. Generating natural language adversarial examples through\nprobability weighted word saliency. In ACL.\nG¨ozde G¨ul S ¸ahin and Mark Steedman. 2018. Character-level models versus morphology in semantic role labeling.\nIn Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 386–396.\nErik F Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent\nnamed entity recognition. arXiv preprint cs/0306050.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword\nunits. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), pages 1715–1725, Berlin, Germany, August. Association for Computational Linguistics.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention ﬂow for\nmachine comprehension.\nChenglei Si, Ziqing Yang, Yiming Cui, Wentao Ma, Ting Liu, and S. Wang. 2020. Benchmarking robustness of\nmachine reading comprehension models. arXiv preprint arXiv:2004.14004.\nIlya Sutskever, James Martens, and Geoffrey E Hinton. 2011. Generating text with recurrent neural networks. In\nProceedings of the 28th international conference on machine learning (ICML-11), pages 1017–1024.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355, Brus-\nsels, Belgium, November. Association for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2019. Neural network acceptability judgments. Trans-\nactions of the Association for Computational Linguistics, 7:625–641.\nZ. Yang, Bhuwan Dhingra, Y . Yuan, Junjie Hu, William W. Cohen, and R. Salakhutdinov. 2017. Words or\ncharacters? ﬁne-grained gating for reading comprehension. ArXiv.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xl-\nnet: Generalized autoregressive pretraining for language understanding. In Advances in neural information\nprocessing systems, pages 5754–5764.\nYuan Zang, Chenghao Yang, Fanchao Qi, Z. Liu, Meng Zhang, Qun Liu, and Maosong Sun. 2020. Word-level\ntextual adversarial attacking as combinatorial optimization. In ACL.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. ERNIE: Enhanced lan-\nguage representation with informative entities. InProceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1441–1451, Florence, Italy, July. Association for Computational Linguistics.\nZhuosheng Zhang, Yu-Wei Wu, Zhao Hai, Z. Li, Shuailiang Zhang, Xi Zhou, and Xiaodong Zhou. 2020.\nSemantics-aware bert for language understanding. In AAAI.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8713755011558533
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6935446858406067
    },
    {
      "name": "Language model",
      "score": 0.6529321670532227
    },
    {
      "name": "Natural language processing",
      "score": 0.6467792391777039
    },
    {
      "name": "Character (mathematics)",
      "score": 0.6353262662887573
    },
    {
      "name": "Word (group theory)",
      "score": 0.5824944972991943
    },
    {
      "name": "Security token",
      "score": 0.5717705488204956
    },
    {
      "name": "Construct (python library)",
      "score": 0.5449416637420654
    },
    {
      "name": "Question answering",
      "score": 0.49627476930618286
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4920346438884735
    },
    {
      "name": "Speech recognition",
      "score": 0.375305712223053
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I66946132",
      "name": "University of Maryland, College Park",
      "country": "US"
    }
  ]
}