{
    "title": "Semi-automatic Data Enhancement for Document-Level Relation Extraction with Distant Supervision from Large Language Models",
    "url": "https://openalex.org/W4389518811",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2100389215",
            "name": "Junpeng Li",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2988623467",
            "name": "Zixia Jia",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2158714742",
            "name": "Zilong Zheng",
            "affiliations": [
                "Beijing Academy of Artificial Intelligence"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4225353277",
        "https://openalex.org/W2228826686",
        "https://openalex.org/W4226031465",
        "https://openalex.org/W2952179106",
        "https://openalex.org/W4365601444",
        "https://openalex.org/W4386566720",
        "https://openalex.org/W4384662964",
        "https://openalex.org/W4385573954",
        "https://openalex.org/W3188999884",
        "https://openalex.org/W4353007316",
        "https://openalex.org/W4366735603",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385573505",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035053871",
        "https://openalex.org/W3101327207",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W3093891978",
        "https://openalex.org/W4389523710",
        "https://openalex.org/W4224909145",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4285240908",
        "https://openalex.org/W4311409687",
        "https://openalex.org/W4385571451",
        "https://openalex.org/W4385571437"
    ],
    "abstract": "Document-level Relation Extraction (DocRE), which aims to extract relations from a long context, is a critical challenge in achieving fine-grained structural comprehension and generating interpretable document representations. Inspired by recent advances in in-context learning capabilities emergent from large language models (LLMs), such as ChatGPT, we aim to design an automated annotation method for DocRE with minimum human effort. Unfortunately, vanilla in-context learning is infeasible for DocRE due to the plenty of predefined fine-grained relation types and the uncontrolled generations of LLMs. To tackle this issue, we propose a method integrating an LLM and a natural language inference (NLI) module to generate relation triples, thereby augmenting document-level relation datasets. We demonstrate the effectiveness of our approach by introducing an enhanced dataset known as DocGNRE, which excels in re-annotating numerous long-tail relation types. We are confident that our method holds the potential for broader applications in domain-specific relation type definitions and offers tangible benefits in advancing generalized language semantic comprehension.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5495–5505\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSemi-automatic Data Enhancement for Document-Level Relation\nExtraction with Distant Supervision from Large Language Models\nJunpeng Li∗, Zixia Jia∗, Zilong Zheng\u0000\nNational Key Laboratory of General Artificial Intelligence, BIGAI\n{lijunpeng,jiazixia,zlzheng}@bigai.ai\nhttps://github.com/bigai-nlco/DocGNRE\nAbstract\nDocument-level Relation Extraction (DocRE),\nwhich aims to extract relations from a long\ncontext, is a critical challenge in achieving fine-\ngrained structural comprehension and gener-\nating interpretable document representations.\nInspired by recent advances in in-context learn-\ning capabilities emergent from large language\nmodels (LLMs), such as ChatGPT, we aim to\ndesign an automated annotation method for\nDocRE with minimum human effort. Unfortu-\nnately, vanilla in-context learning is infeasible\nfor document-level Relation Extraction ( RE)\ndue to the plenty of predefined fine-grained re-\nlation types and the uncontrolled generations\nof LLMs. To tackle this issue, we propose a\nmethod integrating a Large Language Model\n(LLM) and a natural language inference (NLI)\nmodule to generate relation triples, thereby aug-\nmenting document-level relation datasets. We\ndemonstrate the effectiveness of our approach\nby introducing an enhanced dataset known as\nDocGNRE, which excels in re-annotating nu-\nmerous long-tail relation types. We are confi-\ndent that our method holds the potential for\nbroader applications in domain-specific rela-\ntion type definitions and offers tangible bene-\nfits in advancing generalized language semantic\ncomprehension.\n1 Introduction\nDocument-level Relation Extraction (DocRE) is a\ntask that focuses on extracting fine-grained rela-\ntions between entity pairs within a lengthy context\n(Yao et al., 2019; Nan et al., 2020; Wang et al.,\n2020; Zhou et al., 2021; Zhang et al., 2021; Ma\net al., 2023). The abundance of entity pairs in a\ndocument, coupled with a vast array of fine-grained\nrelation types, makes DocRE inherently more chal-\nlenging than sentence-level RE. The challenge is\nobserved not only in model learning but also in\nhuman annotations.\n∗Equal contributions. Authors ordered alphabetically.\n\u0000 Correspondence to Zilong Zheng <zlzheng@bigai.ai>.\nP17 P27 P150 P276 P170 P127 P749 P551 P54 P1366\nRelation\n0\n500\n1000\n1500\n2000\n2500Count\nDocGNRE (Ours)\nRe-DocRED\nDocRED\nFigure 1: Counts of relation types in different datasets.\nThe original document-level RE dataset Do-\ncRED (Yao et al., 2019) has been recognized for its\nfalse negative issue and subsequently re-annotated\nto address this concern by supplementing a sig-\nnificant number of relation triples. Notably, two\nrepresentative works, Huang et al. (2022) and Tan\net al. (2022b), have contributed to this re-annotation\nprocess. Huang et al. (2022) undertook manual an-\nnotation from scratch, employing two expert an-\nnotators to annotate 96 documents. On the other\nhand, Tan et al. (2022b) utilized pre-trained RE\nmodels in conjunction with manual revision to con-\nstruct Re-DocRED. Despite their contributions to\nsupplementing relations for DocRED, both meth-\nods have certain limitations. First, achieving com-\nplete manual annotation is challenging: each doc-\nument within this dataset contains an average of\n19.5 entities, requiring consideration of approxi-\nmately 37,000 candidate triples (including 97 re-\nlation types, including NULL). Second, the supple-\nmentary annotations are derived from the existing\ndata distribution: Tan et al. (2022b) first pre-trained\na RE model with distantly supervised data from\nDocRED, then utilized this model to predict triple\ncandidates. Such a process may introduce model\nbias, potentially resulting in the exclusion of sparse\nrelations that exist beyond the scope of the existing\n5495\ndata distribution. Figure 1 illustrates the counts of\nsome relation types across various datasets. It is\nevident that the supplementary annotations in Re-\nDocRED exhibit a distribution similar to that of the\nDocRED test set.\nIn light of the limitations inherent in prior an-\nnotation techniques and the imperative to enhance\nthe completeness of DocRE datasets, we propose\na novel approach aimed at augmenting the Re-\nDocRED dataset through the utilization of the pow-\nerful generalization capabilities of Large Language\nModels (LLMs). By leveraging our method, as\nshown in Figure 1, our revised test set, named\nDocGNRE, exhibits the advantage of re-annotating\na greater number of long-tail types, such as P276\nand P551.\nRecent studies have utilized GPT (Floridi and\nChiriatti, 2020; Chan, 2023) for various structural\nprediction tasks, such as named entity prediction\nand relation extraction (Dunn et al., 2022; Gutiér-\nrez et al., 2022; Wang et al., 2023; Liu et al., 2023;\nXu et al., 2023), as well as text classification label-\ning (Gilardi et al., 2023; Törnberg, 2023). Notably,\nresearchers such as Wan et al. (2023); Wadhwa\net al. (2023) have demonstrated the effectiveness\nof in-context learning by incorporating prompts\ncontaining suitable example demonstrations forRE\ntasks. However, it is worth noting that without ex-\nplicit instructions, GPT may generate uncontrolled\nrelations that do not align with predefined types.\nTherefore, recent methods only work in sentence-\nlevel RE and especially highlight one distinguished\nchallenge for LLM in-context learning: unable to\nfitting detailed instructions for long-context docu-\nments (Wadhwa et al., 2023).\nTo align GPT-generated relations and predefined\nrelation types, we first combine Natural Language\nInference (NLI) models (MacCartney, 2009) with\nGPT to solve zero-shot DocRE. The results show\nthat although GPT generations only hit partial\nground truth of Re-DocRED, it detects substantial\nexternal valid relation triples (details in Sec. 3.1).\nTherefore, we design a pipeline framework to fur-\nther complement the test set of Re-DocRED and\nautomatically generate distant training annotations\nby combining GPT and NLI modules. To verify\nthat we supplement many relation triples beyond\nthe scope of the original data distribution, we test\nprevious models in our DocGNRE test set. Addi-\ntionally, we train the state-of-the-art (SOTA) model\nusing our distant training dataset.\nOur contributions can be summarized as follows:\n\u0003 We conduct a quantitative analysis to evaluate\nthe performance of GPT in zero-shot document-\nlevel RE.\n\u0003 We propose a novel framework that integrates\nan NLI module and an LLM to automatically\ngenerate distant relation triples, supplementing\nexisting DocRE datasets.\n\u0003 We create an enhanced version of the Re-\nDocRED test set, named DocGNRE, with mini-\nmal human intervention, ensuring high quality.\nAdditionally, we augment the Re-DocRED train-\ning set by supplementing it with distant relation\ntriples automatically generated by our frame-\nwork1, referring to Tabel 1.\n2 LLM Enhanced Automatic Data\nGeneration\nOur approach consists of two main procedures:\nconstructing prompts for LLM to generate rela-\ntions triples as proposals and employing Natural\nLanguage Inference (NLI) models to align gener-\nated relations with predefined relation types. Fig. 2\nshows the whole framework. In the first procedure,\nwe observed that even though we imposed restric-\ntions on the entity and relation lists in the prompts,\nLLMs (both GPT-3.5 and GPT-4) still generated\ntriples that fell outside of our intended constraints.\nFurthermore, we found that the generated relations\nexpressed by contextual words in the document\nwere more accurate than those with the restricted\nrelation types. Based on these insights and to fully\nleverage the potential of LLMs, hereby generat-\ning more accurate relation triples, we removed the\nrestrictions of specific relation types for LLMs. In-\nstead, we subsequently utilize an NLI module to\nmap the generated relations to the predefined rela-\ntion types in the Re-DocRED dataset.\n2.1 GPT Results as Proposals\nWe select GPT-3.5 (gpt-3.5-turbo) as our LLM\nmodule, considering a balance between cost and\nperformance. Given that the original DocRED\ndataset provides an entity list for each document,\nwe constrain the responses of GPT to utilize only\nthe entities present in the provided list.\nPrompt Construction As shown in Figure 2,\nthe prompt consists of a generation demonstration\nand a specific context followed by a corresponding\n1Our dataset is publicly available at https://github.\ncom/bigai-nlco/DocGNRE.\n5496\nPrompt Construction\nPlease generate at least 20 triples \nthat are considered to be true with \nrespect to below concontext using only \ngiven entities from the entity list. \nPlease answer using triples in form of \n<'entity1', 'relation', 'entity2'>. \n'entity1' and 'entity2' are from the \nbelow entity list.\nContext: The Sound Barrier ( known in \nthe United States , as Breaking \nThrough the Sound Barrier and Breaking \nthe Sound Barrier ) is a 1952 British \nfilm directed by David Lean. …… ,\nit was his first for Alexander Korda \n's London Films following the break\nAnd Entity list: [Breaking Through the \nSound Barrier,  1952, David Lean, \nAlexander Korda, London Films, ……]\nFor example,\n<'Caribbean', 'part of', 'Atlantic'>,\n<'Atlantic', 'has part', 'Caribbean'>\nGPT-generated relation triples\nNLI Module\nFiltering\n<‘David Lean’, ‘worked for’, ‘London Films’>\n<‘London Films’, ‘was owned by’, ‘Alexander Korda’>\n<‘The Sound Barrier’, ‘is a’, ‘British film’>\n……\nA: “David Lean worked for London Films”\nPremise: B: “London Films was owned by Alexander Korda”……\nHypot\nhesis:\n1. “The person or organization for which sub. (David …) \nworks or worked is obj. (London …)”    <—   Employer\n3. “The owner of sub. is obj. ”       <—   Owned by\n2. “The person or organization for which sub. (London \n…) works or worked is obj. (David …)”  <—  Employer\n4. “sub. has obj. as their sibling”       <—   Sibling……<David …, Employer, London …>\n<David …, Sibling, London …>\n<London…, Owned by, David …>\n<London…, Employer, David …>\n……\n0.97\n0.22\n0.02\n0.01\n<London …, Owned by, Alexander…>\n<Alexander …, Sibling, London …>\n<Alexander…, Employer, London …>……\n0.88\n0.47\n0.01\n<\n‘David Lean’, \n‘Employer’, \n‘London Films’\n>\n<\n‘London Films’, \n‘Owned by’, \n‘Alexander Korda’\n>\n……\n……\nFigure 2: The automatic data generation framework and an exemplar document. The green triple in GPT-generated\nrelation triples will be filtered because its object entity is out of the given entity list.\nentity list. We notice that as the generated content\nby LLMs became longer, the accuracy decreased.\nTo mitigate this, we set “at least 20 triples” in the\ninitial prompt 2. To generate more additional triples,\nwe employ an iterative approach by feeding the\nprevious GPT answers as input while instructing\nGPT to “Please keep generating 20 more triples\nusing only the given entities from the entity list”.\nHowever, despite providing the entity list in the\nprompt, we observed that undesired triples with\nincorrect entity pairs still occurred. To address this,\nwe implemented a filtering process to remove these\nundesired triples. Consequently, all the remaining\ntriples are treated as proposals and later aligned\nusing the NLI module.\n2.2 NLI Module as an Annotator\nIn this procedure, our goal is to map the relations\ngenerated by GPT to predefined types. To achieve\nthis, a reasonable approach is to align the seman-\ntic meaning of relations. Therefore, we employ a\nNLI model, which has demonstrated effectiveness\nin assessing factual consistency (Honovich et al.,\n2022). The NLI model takes two sentences as in-\nput, typically referred to as the premise and the\nhypothesis. It assigns a score to each term, indi-\n2We choose the “20” number because it is a trade-off be-\ntween ensuring accuracy and the quantity of generated re-\nlations. We find that the first 20 or so relations generated by\nGPT-3.5 (gpt-3.5-turbo) exhibit a relatively promising level\nof quality. The “at least 20” could be replaced by “no more\nthan” and “a maximum of”. We discovered that GPT-3.5 gen-\nerates comparable numbers and quality of triples using all of\nthese expressions.\ncating whether it signifies entailment, neutrality,\nor contradiction. If the term “entailment” receives\nthe highest score, the model concludes that the two\nsentences are factually consistent.\nPremise and Hypothesis ConstructionIn our\nframework, we take each GPT-generated triple as\nthe premise and replace the relation in such triple\nwith a specific predefined relation type as the hy-\npothesis. Remember that our purpose is to map\neach GPT-generated relation proposal to a prede-\nfined relation type. Hence, we should enumerate\nthe hypothesis constructed by each specific type\nin the predefined set to calculate the entailment\nscores with the corresponding premise and choose\nthe ONE with the highest score. Moreover, we\nobserve that the GPT-generated relation may cor-\nrespond to an inverse predefined relation type. For\nexample, if the predefined relation set contains the\n“employee” type rather than “employer”, the GPT-\ngenerated triple “<David Lean, worked for, London\nFilms>” will correspond to “<London Films, em-\nployee, David Lean>” rather than “<David Lean,\nemployee, London Films>”. Therefore, for each\ngenerated relation proposal as a premise, we con-\nstruct 96∗2 = 192 possible hypotheses, where96 is\nthe size of the predefined relation set without NULL,\nand double means we change the subject and object\nentities for each predefined type. Specifically, take\na triple < e1, rgpt, e2 > generated from GPT as\nan example premise, given the predefined relation\nset {r1, r2, ...}, we construct candidate hypotheses\n{< e1, r1, e2 >, < e2, r1, e1 >, < e1, r2, e2 >, <\n5497\n# Doc # Ent # Tri\nTest\nRe-DocRED 500 9,779 17,448\nDocGNRE (Ours) 500 9,779 19,526\n∆ 0 0 2,078\nTrain\nRe-DocRED 3,053 59,359 85,932\nRe-DocRED+GPT 3,053 59,359 96,505\nRe-DocRED+more GPT 3,053 59,359 103,561\nTable 1: Comparation of relation statistics between Re-\nDocred and DocGNRE. The test set has been verified by\nhuman annotators. The GPT-generated triples (+GPT)\non the train set are distant.\ne2, r2, e1 >, ...}.\nBecause the NLI model is pretrained with nat-\nural language sentences, we convert the triples to\nnatural sentences.\n\u0003 Most of the GPT-generated relations are them-\nselves in natural language, so each triple’s subject\nentity, relation, and object entity are directly con-\ncatenated to get a natural sentence.\n\u0003 The predefined relation types typically are ab-\nstractive. To make the hypothesis precisely convey\nthe meaning of each relation type, we integrate the\ndescription of each relation type with subject and\nobject entities. Hypothesis for each relation type\ncan be found in Appendix C.\nEntail Scores fromNLI Model We use the\nT5-based NLI model3 in this paper for its powerful\ngeneralizability. T5-XXL (Raffel et al., 2020) is\na generative model, which identifies \"Entailment\"\nand \"No entailment\" by generating two sequences\nin the inference stage. We leverage the probabilities\nof such two sequences omitting the start and end\ntokens to calculate the entailment scores used for\nsorting predefined relation types. Details can be\nfound in Appendix A.\nPost Processing To ensure the high quality\nof newly produced relation triples, we ultimately\nretain those hypothesis triples that should satisfy\nall the following principles:\n\u0003 The entity types of subject and object entities\nsatisfy the type constraints of the relation.\n\u0003 Get the highest entailment scores.\n\u0003 Get the entailment scores of more than 0.6.\nNote that some of the GPT-generated relations\nmay be exactly those in the predefined set of rela-\ntion types. We do not need to map these generated\ntriples via our NLI module and just add them into\nthe final selected triples set.\n3https://huggingface.co/google/t5_xxl_true_\nnli_mixture\nThrough above procedures, we process each doc-\nument of the Re-DocRED train set to produce addi-\ntional distant relation triples. For the Re-DocRED\ntest set, after acquiring distant relation triples, each\ndistant triple will be conducted through human\nverification. Two annotators are asked to answer\nwhether the relation triples can be inferred accord-\ning to the provided documents. A third annotator\nwill resolve the conflicting annotations. Specifi-\ncally, we use Mechanical Turk for human annota-\ntions. In order to ensure that annotators possessed a\nsignificant level of qualification, prospective anno-\ntators were required to meet the following criteria:\n• “HIT Approval Rate(%) for all Requesters’ HITs”\n> 95.\n• “Number of HITs Approved” > 1000.\n• “Location” is one of {United States, Canada,\nGreat Britain, Australia, Singapore, Ireland, New\nZealand}.\nThe first two indicators are calculated by Mechani-\ncal Turk according to one’s historical performance\nand the last one aims to promise English profi-\nciency of the annotators. Finally, the acceptance\nrate of the NLI-selected relations in the test set is\n71.3%. We provide a more accurate and complete\ntest set with the addition of 2078 triples than Re-\nDocRED. Detailed statistics of our datasets can be\nfound in Table 1.\n3 Experiments\n3.1 Zero-shot Document-level RE\nOur framework can obviously be used to predict\ndocument-level relations directly. Therefore, in the\nfirst experiment, we explore the GPT performance\non the zero-shot document-levelRE. Table 2 shows\nthe results. As far as we know, we are the first to\nreport these results on document-level RE.\nWe have three observations based on Table 2: i)\nPure GPT-3.5 (without our NLI module) only\nhits rare ground truth.As aforementioned, GPT\ngenerates most relations expressed by natural lan-\nguage, which do not exactly match with the ground\ntruth, even though some of these relations represent\nthe same meaning as ground truth. So the exact-\nmatch F1 scores are unsatisfactory; ii)NLI module\ncan improve pure GPT performance.With NLI\nmodule mapping GPT answers to predefined types,\nGPT-3.5 (gpt-3.5-turbo) predicts a small portion\nof ground truth triples that are manually annotated\n(5.77 recall in the Re-DocRED test set). The reason\nmay be that we ask it to generate multiple relations\n5498\nDocRED Re-DocRED DocGNRE\nMethod P R F1 P R F1 P R F1\nGPT-3.5 Only 7.34 4.53 5.6 13.12 2.85 4.68 13.97 2.71 4.54\nGPT-3.5 + NLI (w/o. rel des) 13.9 10.29 11.82 23.57 6.14 9.74 42.91 9.9 16.2\nGPT-3.5 + NLI (w. rel des) 14.61 9.8 11.73 24.45 5.77 9.33 72.71 15.32 25.31\nTable 2: Results of zero-shot document-level RE. We test the three test sets: DocRED, Re-DocRED, and our\nDocGNRE. “rel des” means relation description. “P” and “R” refer to precision and recall respectively.\nDocGNRE (D) Re D - Re\nTrain set P R F1 R R\nDocRED† 90.3 27.83 42.55 31.04 0.91\nDocRED‡ 84.52 32.1 46.52 35.73 1.54\nRe-DocRED† 81.45 56.98 67.05 63.59 1.40\nRe-DocRED‡ 85.0 64.29 73.21 71.69 2.17\nDoc + GPT† 84.54 27.9 41.96 29.72 12.66\nDoc+ GPT‡ 84.07 34.86 49.2 37.22 15.08\nDoc + mGPT† 80.65 28.89 42.54 30.02 19.39\nDoc + mGPT‡ 79.29 36.31 49.75 38.09 21.4\nRe+ GPT† 83.66 57.62 68.24 62.87 13.53\nRe+ GPT‡ 84.92 63.86 72.9 70.0 12.29\nRe+ mGPT† 81.71 58.23 68.0 62.74 20.36\nRe+ mGPT‡ 80.93 66.98 73.29 72.36 21.86\nTable 3: Results of DREEAM model (Ma et al., 2023)\ntraining with different settings. All results are averaged\nby three runs. “mGPT” means “more GPT” that we carry\nout two iterative processes as mentioned in Sec. 2.1.\n“Doc” and “Re” are abbreviation of “DocRED” and “Re-\nDocRED”. \"D-Re\" refers to our supplementary triple\nset (the remaining triples obtained by removing triples\nof Re-DocRED from DocGNRE. †means BERT-base\n(Devlin et al., 2018) and ‡means RoBERTa-large (Liu\net al., 2019).\nat once by one prompt rather than enumerate all\nentity pairs to ask for relations one by one (which\nis too costly and time-consuming to execute for\nplenty of entities on document-level RE). But from\nhuman verification, the accuracy of NLI-selected\ntriples has been proven relatively high (72.71 pre-\ncision in our supplementary test set DocGNRE),\nwhich illustrates that most triples predicted by our\nframework are the supplementary of Re-DocRED;\niii) Relation descriptions can guideNLI to out-\nput expected relations.With relation descriptions\nto construct hypothesises, the performance is fur-\nther improved (25.31 vs. 16.2) in our DocGNRE.\n3.2 Training with Distant Triples\nWe test the SOTA document-levelRE model (Ma\net al., 2023) on our DocGNRE and retrain it with\nour distant training set. All experiment settings are\nthe same as Ma et al. (2023) except for training\ndata in the +GPT setting. Table 3 shows the results.\nWe can find that i) the recall of previous models\non our DocGNRE drops, which demonstrates the\ndifficult prediction on our supplementary test rela-\ntion triples when the model is only trained with the\ntraining set of DocRED or ReDocRED; ii) The re-\ncall scores on all the test sets are improved with\ndirectly supervised training on our training set\n(which exhibits the capability to predict additional\nground truth instances), even though our distantly\nsupervised data is somewhat noisy. Designing more\nadvanced methods to leverage our distant training\nset is taken in future work.\nIn addition, we conducted experiments using two\nother DocRE models, ATLOP (Zhou et al., 2021)\nand KD-DocRE (Tan et al., 2022a), by leveraging\ntheir officially provided code. Experimental results\nof ATLOP and KD-DocRE show a similar tendency\nto DREEAM. Detailed results are in Appendix B.\n4 Conclusion\nLLMs face challenges in extracting fine-grained\nrelations within lengthy contexts. To address this\nlimitation, we present a novel framework that in-\ntegrates an NLI module in this work. With our\nframework, we improve the performance of GPT\nin zero-shot document-level RE. Above all, our\nframework enhances the automatic data generation\ncapability with minimum human effort. We sup-\nplement the existing DocRE dataset, providing a\ncomplete test set DocGNRE and a distant training\nset. Given the inherent presence of false negative in-\nstances in numerous RE datasets, particularly those\nconstructed through a recommend-revise scheme\nor distant supervision, we believe our framework\npossesses a broad utility that extends to a wider\narray of datasets.\nLimitations\nThe limited generated length of LLMs causes the\nlimitation of our methods. There is a specific upper\nlimit on the number of relation triples that can be\ngenerated for each document. Therefore, our frame-\nwork is an excellent data supplement method rather\nthan a perfect zero-shot predictor.\n5499\nAcknowledgements\nThis work is supported in part by the National Key\nR&D Program of China (2021ZD0150200) and\nthe National Natural Science Foundation of China\n(62376031).\nReferences\nAnastasia Chan. 2023. Gpt-3 and instructgpt: techno-\nlogical dystopianism, utopianism, and “contextual”\nperspectives in ai ethics and industry. AI and Ethics,\n3(1):53–64.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAlexander Dunn, John Dagdelen, Nicholas Walker,\nSanghoon Lee, Andrew S Rosen, Gerbrand Ceder,\nKristin Persson, and Anubhav Jain. 2022. Struc-\ntured information extraction from complex scientific\ntext with fine-tuned large language models. arXiv\npreprint arXiv:2212.05238.\nLuciano Floridi and Massimo Chiriatti. 2020. Gpt-3: Its\nnature, scope, limits, and consequences. Minds and\nMachines, 30:681–694.\nFabrizio Gilardi, Meysam Alizadeh, and Maël Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nBernal Jiménez Gutiérrez, Nikolas McNeal, Clay Wash-\nington, You Chen, Lang Li, Huan Sun, and Yu Su.\n2022. Thinking about gpt-3 in-context learning\nfor biomedical ie? think again. arXiv preprint\narXiv:2203.08410.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the Second\nDialDoc Workshop on Document-grounded Dialogue\nand Conversational Question Answering, pages 161–\n175, Dublin, Ireland. Association for Computational\nLinguistics.\nQuzhe Huang, Shibo Hao, Yuan Ye, Shengqi Zhu,\nYansong Feng, and Dongyan Zhao. 2022. Does\nrecommend-revise produce reliable annotations? an\nanalysis on missing instances in docred. In Proceed-\nings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 6241–6252.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhengliang Liu, Xiaowei Yu, Lu Zhang, Zihao Wu,\nChao Cao, Haixing Dai, Lin Zhao, Wei Liu, Ding-\ngang Shen, Quanzheng Li, et al. 2023. Deid-gpt:\nZero-shot medical text de-identification by gpt-4.\narXiv preprint arXiv:2303.11032.\nYoumi Ma, An Wang, and Naoaki Okazaki. 2023.\nDREEAM: Guiding attention with evidence for im-\nproving document-level relation extraction. In Pro-\nceedings of the 17th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 1971–1983, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\nBill MacCartney. 2009. Natural language inference.\nStanford University.\nGuoshun Nan, Zhijiang Guo, Ivan Sekuli´c, and Wei Lu.\n2020. Reasoning with latent structure refinement for\ndocument-level relation extraction. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1546–1557.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nQingyu Tan, Ruidan He, Lidong Bing, and Hwee Tou\nNg. 2022a. Document-level relation extraction with\nadaptive focal loss and knowledge distillation. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, pages 1672–1681, Dublin, Ire-\nland. Association for Computational Linguistics.\nQingyu Tan, Lu Xu, Lidong Bing, Hwee Tou Ng, and\nSharifah Mahani Aljunied. 2022b. Revisiting docred\n– addressing the false negative problem in relation\nextraction. In Annual Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nPetter Törnberg. 2023. Chatgpt-4 outperforms experts\nand crowd workers in annotating political twitter\nmessages with zero-shot learning. arXiv preprint\narXiv:2304.06588.\nSomin Wadhwa, Silvio Amir, and Byron C Wallace.\n2023. Revisiting relation extraction in the era of large\nlanguage models. arXiv preprint arXiv:2305.05003.\nZhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying\nLiu, Haiyue Song, Jiwei Li, and Sadao Kurohashi.\n2023. Gpt-re: In-context learning for relation extrac-\ntion using large language models. arXiv preprint\narXiv:2305.02105.\nDifeng Wang, Wei Hu, Ermei Cao, and Weijian Sun.\n2020. Global-to-local neural networks for document-\nlevel relation extraction. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 3711–3721.\n5500\nShuhe Wang, Xiaofei Sun, Xiaoya Li, Rongbin Ouyang,\nFei Wu, Tianwei Zhang, Jiwei Li, and Guoyin Wang.\n2023. Gpt-ner: Named entity recognition via large\nlanguage models. arXiv preprint arXiv:2304.10428.\nXin Xu, Yuqi Zhu, Xiaohan Wang, and Ningyu Zhang.\n2023. How to unleash the power of large lan-\nguage models for few-shot relation extraction? arXiv\npreprint arXiv:2305.01555.\nYuan Yao, Deming Ye, Peng Li, Xu Han, Yankai Lin,\nZhenghao Liu, Zhiyuan Liu, Lixin Huang, Jie Zhou,\nand Maosong Sun. 2019. DocRED: A large-scale\ndocument-level relation extraction dataset. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 764–777,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nNingyu Zhang, Xiang Chen, Xin Xie, Shumin Deng,\nChuanqi Tan, Mosha Chen, Fei Huang, Luo Si, and\nHuajun Chen. 2021. Document-level relation ex-\ntraction as semantic segmentation. arXiv preprint\narXiv:2106.03618.\nWenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing\nHuang. 2021. Document-level relation extraction\nwith adaptive thresholding and localized context pool-\ning. In Proceedings of the AAAI conference on artifi-\ncial intelligence, volume 35, pages 14612–14620.\n5501\nDocGNRE (D) Re D - Re\nTrain set P R F1 R R\nDocRED† 87.9 28.6 43.2 31.9 0.9\nDocRED‡ 89.8 31.1 46.2 34.6 1.5\nDoc + GPT† 82.9 29.3 43.2 31.2 13.0\nDoc + GPT‡ 84.4 32.2 46.6 34.4 13.3\nDoc + mGPT† 79.6 30.2 43.8 31.5 19.54\nDoc + mGPT‡ 79.3 34.2 47.7 35.7 21.1\nTable 4: Results of ATLOP model (Zhou et al., 2021)\ntraining with different settings.†means BERT-base (De-\nvlin et al., 2018) and‡means RoBERTa-large (Liu et al.,\n2019).\nDocGNRE (D) Re D - Re\nTrain set P R F1 R R\nDocRED† 82.4 32.9 47.0 36.6 1.6\nDocRED‡ 76.9 37.8 50.3 41.9 2.7\nDoc + GPT† 74.3 36.1 48.5 38.3 17.9\nDoc + GPT‡ 77.1 37.4 50.4 39.7 18.8\nDoc + mGPT† 65.3 39.2 48.9 40.0 31.9\nDoc + mGPT‡ 68.8 41.2 51.6 42.5 30.8\nTable 5: Results of KD-DocRE model (Tan et al., 2022a)\ntraining with different settings.†means BERT-base (De-\nvlin et al., 2018) and‡means RoBERTa-large (Liu et al.,\n2019).\nA NLI Score\nWe choose the T5-based NLI model released by\nGoogle in this paper. As mentioned earlier, T5-\nXXL is a generative model. The model identifies\n“Entailment” and “No entailment” by generating\ntwo sequences in the inference stage. During infer-\nence, the sequence “<pad>_0</s>” identifies “No\nentailment”, and the sequence “<pad>1</s><pad>”\nidentifies “Entailment”. Because both the first to-\nkens are “<pad>”, and when the first three tokens\nhave been determined, the prediction of the last\ntoken has a high probability, so we do not consider\nthe first and last token when calculating the NLI\nscore. To obtain the NLI score, we obtain the logits\nof four subsequences (\"_0\", \"_</s>\", \"10\", \"1</s>\")\nand perform a softmax operation to obtain the corre-\nsponding probabilities. The score of \"_0\" sequence\ncorresponds to the score of “No entailment”. The\nscore of “1</s>” sequence corresponds to the score\nof “Entailment”. To further distinguish the NLI\nscore among the constructed triples, we subtract\nthe score of “No entailment” from the score of “En-\ntailment” to fuse the two scores as the final scores.\nB Model Results\nExperimental results of ATLOP and KD-DocRE\nare shown in Table 4 and Table 5 respectively.\nC Hypothesis Construction of Relation\nWe provide hypothesis construction of predefined\nrelations in DocRED and Re-DocRED with Wiki-\ndata Id and Name in Table 6.\n5502\nWikidata ID Name Hypothesis Construction\nP6 head of government The head of the executive power of the governmental body sub.\nis obj.\nP17 country The sovereign state of this item sub. is obj.\nP19 place of birth The birth location of the person, animal or fictional character sub.\nis obj.\nP20 place of death The death location of the person, animal or fictional character\nsub. is obj.\nP22 father The father of sub. is obj.\nP25 mother The mother of sub. is obj.\nP26 spouse The spouse of sub. is obj.\nP27 country of citizen-\nship\nobj. is a country that recognizes sub. as its citizen\nP30 continent obj. is the continent of which sub. is a part\nP31 instance of obj. is that class of which sub. is a particular example and member.\n(sub. typically an individual member with proper name label)\nP35 head of state obj. is the official with the highest formal authority in the coun-\ntry/state sub.\nP36 capital obj. is the primary city of the country/state sub.\nP37 official language obj. is the language designated as official by sub.\nP39 position held sub. currently or formerly holds the position or public office obj.\nP40 child sub. has obj. as their offspring son or daughter\nP50 author The main creator(s) of the written work sub. is(are) obj.\nP54 member of sports\nteam\nThe sports team or club that sub. represents or formerly repre-\nsented is obj.\nP57 director The director of this film, TV-series, stageplay or video game is\nobj.\nP58 screenwriter The author(s) of the screenplay or script for this work sub. is(are)\nobj.\nP69 educated at The educational institution attended by sub. is obj.\nP86 composer The person(s) who wrote the music sub. is(are) obj.\nP102 member of political\nparty\nThe political party of which this politician sub. is or has been a\nmember is obj.\nP108 employer The person or organization for which sub. works or worked is\nobj.\nP112 founded by The founder or co-founder of this organization, religion or place\nsub. is obj.\nP118 league The league in which the team or player sub. plays or has played\nin is obj.\nP123 publisher The organization or person responsible for publishing books,\nperiodicals, games or software sub. is obj.\nP127 owned by The owner of sub. is obj.\nP131 located in the ad-\nministrative territo-\nrial entity\nsub. is located on the territory of the following administrative\nentity obj.\nP136 genre The creative work sub.’s genre is obj.\nP137 operator The person or organization that operates the equipment, facility,\nor service sub. is obj.\nP140 religion The religion of a person, organization or religious building, or\nassociated with sub. is obj.\nP150 contains administra-\ntive territorial entity\nThe direct subdivisions of an administrative territorial entity sub.\nhas obj.\n5503\nWikidata ID Name Hypothesis Construction\nP155 follows The immediately prior item in some series of which sub. is part\nis obj.\nP156 followed by The immediately following item in some series of which sub. is\npart is obj.\nP159 headquarters loca-\ntion\nThe specific location where sub.’s headquarters is or has been\nsituated is obj.\nP161 cast member The actor performing live sub. for a camera or audience has obj.\nP162 producer The producer(s) of this film or music work sub. is(are) obj.\nP166 award received The award or recognition received by a person, organization or\ncreative work sub. is obj.\nP170 creator The maker of a creative work sub. is obj.\nP171 parent taxon The closest parent taxon of the taxon sub. is obj.\nP172 ethnic group sub.’s ethnicity is obj.\nP175 performer The performer involved in the performance or the recoding of the\nwork sub. is obj.\nP176 manufacturer The manufacturer or producer of the product sub. is obj.\nP178 developer The organization or person that developed sub. is obj.\nP179 series The series which contains sub. is obj.\nP190 sister city sub. and obj. are twin towns, sister cities, twinned municipalities\nP194 legislative body The legislative body governing sub. is obj.\nP205 basin country The country that have drainage to/from or border the body of\nwater sub. has obj.\nP206 located in or next to\nbody of water\nsub. is located in or next to body of water obj.\nP241 military branch The branch to which the military unit, award, office, or person\nsub. belongs is obj.\nP264 record label The brand and trademark associated with the marketing of subject\nmusic recordings and music videos sub. is obj.\nP272 production com-\npany\nThe company that produced this film, audio or performing arts\nwork sub. is obj.\nP276 location The location of the item, physical object or event sub. is within is\nobj.\nP279 subclass of All instances of sub. are instances of obj.\nP355 subsidiary The subsidiary of a company or organization sub. has obj.\nP361 part of obj. has part or parts sub.\nP364 original language of\nwork\nThe language in which the film or a performance work sub. was\noriginally created is obj.\nP400 platform The platform for which the work sub. has been developed or\nreleased / specific platform version fo the software sub. developed\nis obj.\nP403 mouth of the water-\ncourse\nThe body of water to which the watercourse sub. drains is obj.\nP449 original network The network(s) the radio or television show sub. was originally\naired on has obj.\nP463 member of The organization or club to which sub. belongs is obj.\nP488 chairperson The presiding member of the organization, group or body sub. is\nobj.\nP495 country of origin The country of origin of the creative work sub. is obj.\nP527 has part sub. has part or parts obj.\nP551 residence The place where the person sub. is, or has been, resident is obj.\nP569 date of birth The date on which sub. was born is obj.\nP570 date of death The date on which sub. died is obj.\n5504\nWikidata ID Name Hypothesis Construction\nP571 inception The date or point in time when the organization/subject sub. was\nfounded/created is obj.\nP576 dissolved, abol-\nished or demolished\nThe date or point in time on which the organization sub. was\ndissolved/disappeared or the building sub. demolished is obj.\nP577 publication date The data or point in time the work sub. is first published or\nreleased is obj.\nP580 start time The time the item sub. begins to exist or the statement sub. starts\nbeing valid is obj.\nP582 end time The time the item sub. ceases to exist or the statement sub. stops\nbeing valid is obj.\nP585 point in time The time and date sub. took place, existed or the statement sub.\nwas true is obj.\nP607 conflict The battles, wars or other military engagements in which the\nperson or item sub. participated is obj.\nP674 characters The characters which appear in sub. has obj.\nP676 lyrics by The author of song lyrics sub. is obj.\nP706 located on terrain\nfeature\nsub. is located on the specified landform obj.\nP710 participant The person, group of people or organization that actively\ntakes/took part in the event sub. has obj.\nP737 influenced by The person, idea sub. is informed by obj.\nP740 location of forma-\ntion\nThe location where the group or organization sub. was formed is\nobj.\nP749 parent organization The parent organization of the organization sub. is obj.\nP800 notable work The notable scientific, artistic or literary work, or other work of\nsignificance among sub.’s works is obj.\nP807 separated from sub. was founded or started by separating from identified object\nobj.\nP840 narrative location The narrative of the work sub. is set in the location obj.\nP937 work location The location where persons or organization sub. were actively\nparticipating in employment, business or other work is obj.\nP1001 applies to jurisdic-\ntion\nThe institution, law or public office sub. belongs to or has power\nover or applies to the country, state or municipality obj.\nP1056 product or material\nproduced\nThe material or product produced by the government agency,\nbusiness, industry, facility, or process sub. is obj.\nP1198 unemployment rate The portion of the workforce population that is not employed of\nsub. is obj.\nP1336 territory claimed by The administrative divisions that claim control of the given area\nsub. is obj.\nP1344 participant of The event that the person or the organization sub. was a partici-\npant in is obj.\nP1365 replaces The person or item sub. replaces obj.\nP1366 replaced by The person or item obj. replaces sub.\nP1376 capital of sub. is capital of obj.\nP1412 languages spoken,\nwritten or signed\nThe language(s) that the person sub. speaks or writes is obj.\nP1441 present in work The work in which the fictional entity or historical person sub. is\npresent is obj.\nP3373 sibling sub. has obj. as their sibling\nTable 6: Relation list, including Wikidata IDs, Names and Hypothesis Construction of relations\n5505"
}