{
  "title": "Learning to Prompt for Vision-Language Models",
  "url": "https://openalex.org/W3198377975",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5030812622",
      "name": "Kaiyang Zhou",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5075948339",
      "name": "Jingkang Yang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5005626854",
      "name": "Chen Change Loy",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A5100406050",
      "name": "Ziwei Liu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W12634471",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3182683290",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2145215286",
    "https://openalex.org/W2155904486",
    "https://openalex.org/W6678470764",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2963785020",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2901458284",
    "https://openalex.org/W2944828972",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W3177096435",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W4312651322",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W2100031962",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W2962714319",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2562153041",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W6678360021",
    "https://openalex.org/W3040002795",
    "https://openalex.org/W3108975329",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2947707615",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W4289639938",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W2962900737",
    "https://openalex.org/W2123024445",
    "https://openalex.org/W3091546937",
    "https://openalex.org/W2166049352",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2619749592",
    "https://openalex.org/W3152956381",
    "https://openalex.org/W3126960149",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W2124033848",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W24089286",
    "https://openalex.org/W4286906902",
    "https://openalex.org/W2964194231",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W2970018230",
    "https://openalex.org/W4296151220",
    "https://openalex.org/W3104911444",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W2950276680",
    "https://openalex.org/W4225922988",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3035058308",
    "https://openalex.org/W3183508146",
    "https://openalex.org/W2953937638"
  ],
  "abstract": null,
  "full_text": "Noname manuscript No.\n(will be inserted by the editor)\nLearning to Prompt for Vision-Language Models\nKaiyang Zhou · Jingkang Yang · Chen Change Loy · Ziwei Liu\nReceived: date / Accepted: date\nAbstract Large pre-trained vision-language models\nlike CLIP have shown great potential in learning repre-\nsentations that are transferable across a wide range of\ndownstream tasks. Diﬀerent from the traditional repre-\nsentation learning that is based mostly on discretized\nlabels, vision-language pre-training aligns images and\ntexts in a common feature space, which allows zero-\nshot transfer to a downstream task via prompting, i.e.,\nclassiﬁcation weights are synthesized from natural lan-\nguage describing classes of interest. In this work, we\nshow that a major challenge for deploying such mod-\nels in practice is prompt engineering, which requires\ndomain expertise and is extremely time-consuming—\none needs to spend a signiﬁcant amount of time on\nwords tuning since a slight change in wording could\nhave a huge impact on performance. Inspired by re-\ncent advances in prompt learning research in natural\nlanguage processing (NLP), we propose Context Op-\ntimization (CoOp), a simple approach speciﬁcally for\nadapting CLIP-like vision-language models for down-\nstream image recognition. Concretely, CoOp models a\nprompt’s context words with learnable vectors while the\nentire pre-trained parameters are kept ﬁxed. To han-\ndle diﬀerent image recognition tasks, we provide two\nKaiyang Zhou\nS-Lab, Nanyang Technological University, Singapore\nE-mail: kaiyang.zhou@ntu.edu.sg\nJingkang Yang\nS-Lab, Nanyang Technological University, Singapore\nE-mail: jingkang001@ntu.edu.sg\nChen Change Loy\nS-Lab, Nanyang Technological University, Singapore\nE-mail: ccloy@ntu.edu.sg\nZiwei Liu\nS-Lab, Nanyang Technological University, Singapore\nE-mail: ziwei.liu@ntu.edu.sg\nimplementations of CoOp: uniﬁed context and class-\nspeciﬁc context. Through extensive experiments on 11\ndatasets, we demonstrate that CoOp requires as few as\none or two shots to beat hand-crafted prompts with a\ndecent margin and is able to gain signiﬁcant improve-\nments over prompt engineering with more shots, e.g.,\nwith 16 shots the average gain is around 15% (with the\nhighest reaching over 45%). Despite being a learning-\nbased approach, CoOp achieves superb domain general-\nization performance compared with the zero-shot model\nusing hand-crafted prompts.\n1 Introduction\nA common approach for building state-of-the-art visual\nrecognition systems is to train vision models to predict\nfor a ﬁxed set of object categories using discrete la-\nbels (He et al., 2016; Dosovitskiy et al., 2021). From\na technical point of view, this is achieved by match-\ning image features—produced by a vision model like\nResNet (He et al., 2016) or ViT (Dosovitskiy et al.,\n2021)—with a ﬁxed set of weights that are seen as visual\nconcepts and initialized randomly. Although training\ncategories often have a textual form, such as “goldﬁsh”\nor “toilet paper,” they will be converted into discrete la-\nbels just for easing the computation of the cross-entropy\nloss, leaving the semantics encapsulated in texts largely\nunexploited. Such a learning paradigm limits visual\nrecognition systems to closed-set visual concepts, mak-\ning them unable to deal with new categories since ad-\nditional data are required for learning a new classiﬁer.\nRecently, vision-language pre-training such as\nCLIP (Radford et al., 2021) and ALIGN (Jia et al.,\n2021) has emerged as a promising alternative for vi-\nsual representation learning. The main idea is to align\narXiv:2109.01134v6  [cs.CV]  6 Oct 2022\n2 Kaiyang Zhou et al.\na [CLASS].a photo of [CLASS].a photo of a [CLASS].[V]1 [V]2… [V]M[CLASS].\n82.6880.8186.2991.83\nCaltech101PromptAccuracy\nDescribable Textures (DTD)a photo of a [CLASS].a photo of a [CLASS] texture.[CLASS] texture.[V]1 [V]2… [V]M[CLASS].\n39.8340.2542.3263.58\nPromptAccuracy\nFlowers102a photo of a [CLASS].a flower photo of a [CLASS].a photo of a [CLASS], a type of flower.[V]1 [V]2… [V]M[CLASS].\n60.8665.8166.1494.51\nPromptAccuracy\nEuroSATa photo of a [CLASS].a satellite photo of [CLASS].a centered satellite photo of [CLASS].[V]1 [V]2… [V]M[CLASS].\n24.1737.4637.5683.53\nPromptAccuracy(a) (b)\n(c) (d)\nFig. 1 Prompt engineering vs Context Optimization (CoOp). The former needs to use a held-out validation set for\nwords tuning, which is ineﬃcient; the latter automates the process and requires only a few labeled images for learning.\nimages and raw texts using two separate encoders—one\nfor each modality. For instance, both CLIP and ALIGN\nformulate the learning objective as a contrastive loss,\nwhich pulls together images and their textual descrip-\ntions while pushes away unmatched pairs in the feature\nspace. By pre-training at a large scale, models can learn\ndiverse visual concepts and can readily be transferred\nto any downstream task through prompting (Radford\net al., 2021; Jia et al., 2021; F¨ urst et al., 2021; Li et al.,\n2021; Singh et al., 2021; Yuan et al., 2021). In particu-\nlar, for any new classiﬁcation task one can ﬁrst synthe-\nsize the classiﬁcation weights by giving sentences de-\nscribing task-relevant categories to the text encoder,\nand then compare with image features produced by the\nimage encoder.\nWe observe that for pre-trained vision-language\nmodels, the text input, known as prompt, plays a key\nrole in downstream datasets. However, identifying the\nright prompt is a non-trivial task, which often takes a\nsigniﬁcant amount of time for words tuning—a slight\nchange in wording could make a huge diﬀerence in per-\nformance. For instance, for Caltech101 (Figure 1(a),\n2nd vs 3rd prompt), adding “a” before the class to-\nken brings more than 5% increase in accuracy. More-\nover, prompt engineering also requires prior knowledge\nabout the task and ideally the language model’s under-\nlying mechanism. This is exempliﬁed in Figure 1(b-d)\nwhere adding task-relevant context can lead to signiﬁ-\ncant improvements, i.e., “ﬂower” for Flowers102, “tex-\nture” for DTD and “satellite” for EuroSAT. Tuning the\nsentence structure could bring further improvements,\ne.g., putting “a type of ﬂower” after the class token for\nFlowers102, keeping only “texture” in the context for\nDTD, and adding “centered” before “satellite photo”\nfor EuroSAT. However, even with extensive tuning, the\nresulting prompts are by no means guaranteed to be\noptimal for these downstream tasks.\nInspired by recent prompt learning research in natu-\nral language processing (NLP) (Shin et al., 2020; Jiang\net al., 2020; Zhong et al., 2021), we propose a simple\napproach called Context Optimization (CoOp)1 to au-\ntomate prompt engineering, speciﬁcally for pre-trained\nvision-language models. Concretely, CoOp models a\nprompt’s context words with learnable vectors, which\ncould be initialized with either random values or pre-\ntrained word embeddings (see Figure 2). Two imple-\nmentations are provided to handle tasks of diﬀerent na-\ntures: one is based on uniﬁed context, which shares the\nsame context with all classes and works well on most\ncategories; while the other is based on class-speciﬁc con-\ntext, which learns a speciﬁc set of context tokens for\neach class and is found to be more suitable for some\nﬁne-grained categories. During training, we simply min-\nimize prediction errors using the cross-entropy loss with\nrespect to the learnable context vectors while keeping\nthe entire pre-trained parameters ﬁxed. The gradients\ncan be back-propagated all the way through the text\nencoder, distilling the rich knowledge encoded in the\nparameters for learning task-relevant context.\nTo demonstrate the eﬀectiveness of CoOp, we\nbenchmark on 11 datasets, which cover a diverse set\nof visual recognition tasks including classiﬁcation on\ngeneric objects, scenes, actions and ﬁne-grained cate-\ngories, as well as specialized tasks like recognizing tex-\ntures and satellite imagery. The results show that CoOp\neﬀectively turns pre-trained vision-language models\ninto data-eﬃcient visual learners, requiring as few as\none or two shots to beat hand-crafted prompts with a\ndecent margin. The performance can be further boosted\n1 CoOp is pronounced as /ku:p/.\nLearning to Prompt for Vision-Language Models 3\nby using more shots, e.g., with 16 shots the margin\nover hand-crafted prompts averages at around 15% and\nreaches over 45% for the highest. CoOp also outper-\nforms the linear probe model, which is known as a\nstrong few-shot learning baseline (Tian et al., 2020).\nFurthermore, CoOp demonstrates much stronger ro-\nbustness than the zero-shot model (which uses manual\nprompts) to domain shifts, despite being a learning-\nbased approach.\nIn summary, we make the following contributions:\n1. We present a timely study on the adaptation of re-\ncently proposed vision-language models in down-\nstream applications and identify a critical prob-\nlem associated with the deployment eﬃciency, i.e.,\nprompt engineering.\n2. To automate prompt engineering speciﬁcally for\npre-trained vision-language models, we propose a\nsimple approach based on continuous prompt learn-\ning and provide two implementations that can han-\ndle diﬀerent recognition tasks.\n3. We for the ﬁrst time show that the proposed prompt\nlearning-based approach outperforms both hand-\ncrafted prompts and the linear probe model in terms\nof downstream transfer learning performance and\nrobustness under domain shifts for large vision-\nlanguage models.\n4. We open-source our project at https://github.\ncom/KaiyangZhou/CoOp.\nWe hope the ﬁndings together with the open-source\ncode can inspire and facilitate future research on ef-\nﬁcient adaptation methods for large vision-language\nmodels—an emerging topic related to democratization\nof foundation models (Bommasani et al., 2021) i.e.,\nmaking them easier and cheaper to adapt for the wider\ncommunity.\n2 Related Work\n2.1 Vision-Language Models\nVision-language models have recently demonstrated\ngreat potential in learning generic visual representa-\ntions and allowing zero-shot transfer to a variety of\ndownstream classiﬁcation tasks via prompting (Rad-\nford et al., 2021; Jia et al., 2021; Zhang et al., 2020;\nSingh et al., 2021; Yuan et al., 2021).\nTo our knowledge, the recent developments in\nvision-language learning, particularly CLIP (Radford\net al., 2021) and ALIGN (Jia et al., 2021), are largely\ndriven by advances in the following three areas: i) text\nrepresentation learning with Transformers (Vaswani\net al., 2017), ii) large-minibatch contrastive representa-\ntion learning (Chen et al., 2020; He et al., 2020; H´ enaﬀ\net al., 2020), and iii) web-scale training datasets—CLIP\nbeneﬁts from 400 million curated image-text pairs while\nALIGN exploits 1.8 billion noisy image-text pairs.\nThe idea of mapping images and text onto a com-\nmon embedding space has been studied since nearly\na decade ago (Socher et al., 2013; Frome et al., 2013;\nElhoseiny et al., 2013), but with drastically diﬀerent\ntechnologies. For text features extraction, early work\nhas mainly utilized pre-trained word vectors (Socher\net al., 2013; Frome et al., 2013) or the hand-crafted\nTF-IDF features (Elhoseiny et al., 2013; Lei Ba et al.,\n2015). Matching images and text features has been for-\nmulated as metric learning (Frome et al., 2013), multi-\nlabel classiﬁcation (Joulin et al., 2016; Gomez et al.,\n2017), n-gram language learning (Li et al., 2017), and\nthe recently proposed captioning (Desai and Johnson,\n2021).\nOur work is orthogonal to recent research in vision-\nlanguage models, aiming to facilitate the adaptation\nand deployment of such models in downstream datasets.\n2.2 Prompt Learning in NLP\nKnowledge probing for large pre-trained language mod-\nels, formally deﬁned by Petroni et al. (2019) as “ﬁll-in-\nthe-blank” cloze tests, has recently sparked interest in\nprompt learning research in NLP (Shin et al., 2020;\nJiang et al., 2020; Li and Liang, 2021; Zhong et al.,\n2021; Lester et al., 2021; Gao et al., 2020; Liu et al.,\n2021b).\nThe basic idea of knowledge probing is to induce\npre-trained language models to generate answers given\ncloze-style prompts, which can beneﬁt a number of\ndownstream tasks, such as sentiment analysis. Jiang\net al. (2020) propose to generate candidate prompts\nthrough text mining and paraphrasing, and identify\nthe optimal ones that give the highest training accu-\nracy. Shin et al. (2020) introduce a gradient-based ap-\nproach, which searches for tokens with the largest gra-\ndient changes in the label likelihood.\nMost related to our work are continuous prompt\nlearning methods (Zhong et al., 2021; Li and Liang,\n2021; Lester et al., 2021) which optimize continuous\nvectors in the word embedding space. A drawback of\nsuch methods compared to searching discrete tokens\nis the lack of a clear way to visualize what “words”\nare learned for the vectors. We refer readers to Liu\net al. (2021a) for a comprehensive survey in the topic\nof prompt learning in NLP.\nIt is worth noting that we are the ﬁrst to ap-\nply prompt learning to the adaptation of large vision-\n4 Kaiyang Zhou et al.\n.[CLASS]\nbutterflypizza\n<latexit sha1_base64=\"i4zpmuRm7l6oli4lXwkT2OQ8Jsk=\">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>\n...\n<latexit sha1_base64=\"i4zpmuRm7l6oli4lXwkT2OQ8Jsk=\">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>\n...\ntext encoder\nimage encoder\ntextfeatures\nimagefeatures\nsimilarityscores\n<latexit sha1_base64=\"i4zpmuRm7l6oli4lXwkT2OQ8Jsk=\">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>\n...\n[V]1 [V]2\n<latexit sha1_base64=\"i4zpmuRm7l6oli4lXwkT2OQ8Jsk=\">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>\n... [V]M\n<latexit sha1_base64=\"i4zpmuRm7l6oli4lXwkT2OQ8Jsk=\">AAACFXicbVC7TgJBFJ31ifhCLW02ggkV2aVQSxIbS0zkkcCGzM7OwoR5bGbuasiGn7Cx0F+xM7bW/omlA2wh4ElucnLOvbn3njDhzIDnfTsbm1vbO7uFveL+weHRcenktG1UqgltEcWV7obYUM4kbQEDTruJpliEnHbC8e3M7zxSbZiSDzBJaCDwULKYEQxW6lb6o0iBqQxKZa/mzeGuEz8nZZSjOSj99CNFUkElEI6N6fleAkGGNTDC6bTYTw1NMBnjIe1ZKrGgJsjm907dS6tEbqy0LQnuXP07kWFhzESEtlNgGJlVbyb+5/VSiG+CjMkkBSrJYlGccheUO3vejZimBPjEEkw0s7e6ZIQ1JmAjWtoSiqUfMpFyYFo9TYs2Kn81mHXSrtf8q5p3Xy83qnloBXSOLlAV+egaNdAdaqIWIoijZ/SK3pwX5935cD4XrRtOPnOGluB8/QImmZ+d</latexit>\n...\nlearnable context\nairplane\nmaximize the score for the ground-truth class\nFig. 2 Overview of Context Optimization (CoOp). The main idea is to model a prompt’s context using a set of learnable\nvectors, which can be optimized through minimizing the classiﬁcation loss. Two designs are proposed: one is uniﬁed context,\nwhich shares the same context vectors with all classes; and the other is class-speciﬁc context, which learns for each class a\nspeciﬁc set of context vectors.\nlanguage models in computer vision—which we view as\nan important topic for democratizing foundation mod-\nels (Bommasani et al., 2021)—and justify that prompt\nlearning not only brings signiﬁcant improvements to\ncomputer vision tasks in terms of transfer learning per-\nformance but also produces robust models that can han-\ndle domain shifts.\n3 Methodology\n3.1 Vision-Language Pre-training\nWe brieﬂy introduce vision-language pre-training with\na particular focus on CLIP (Radford et al., 2021).\nOur approach is applicable to broader CLIP-like vision-\nlanguage models.\nModels CLIP consists of two encoders, one for im-\nages and the other for text. The image encoder aims to\nmap high-dimensional images into a low-dimensional\nembedding space. The architecture of the image en-\ncoder can take the form of a CNN like ResNet-50 (He\net al., 2016) or a ViT (Dosovitskiy et al., 2021). On the\nother hand, the text encoder is built on top of a Trans-\nformer (Vaswani et al., 2017) and aims to generate text\nrepresentations from natural language.\nSpeciﬁcally, given a sequence of words (tokens),\nsuch as “a photo of a dog,” CLIP ﬁrst converts each\none of the token (including punctuation) into a lower-\ncased byte pair encoding (BPE) representation (Sen-\nnrich et al., 2016), which is essentially a unique nu-\nmeric ID. The vocabulary size in CLIP is 49,152. To\nfacilitate minibatch processing, each text sequence is\nencompassed with the [SOS] and [EOS] tokens and\ncapped at a ﬁxed length of 77. After that, the IDs are\nmapped to 512-D word embedding vectors, which are\nthen passed on to the Transformer. Finally, the features\nat the [EOS] token position are layer normalized and\nfurther processed by a linear projection layer.\nTraining CLIP is trained to align the two embedding\nspaces learned for images and text respectively. Specif-\nically, the learning objective is formulated as a con-\ntrastive loss. Given a batch of image-text pairs, CLIP\nmaximizes the cosine similarity for matched pairs while\nminimizes the cosine similarity for all other unmatched\npairs. To learn diverse visual concepts that are more\ntransferable to downstream tasks, CLIP’s team collects\na large training dataset consisting of 400 million image-\ntext pairs.\nZero-Shot Inference Since CLIP is pre-trained to\npredict whether an image matches a textual descrip-\ntion, it naturally ﬁts zero-shot recognition. This is\nachieved by comparing image features with the classi-\nﬁcation weights synthesized by the text encoder, which\ntakes as input textual descriptions specifying classes of\ninterest. Formally, let f be image features extracted\nby the image encoder for an image x and {wi}K\ni=1 a\nset of weight vectors generated by the text encoder. K\ndenotes the number of classes and each wi is derived\nfrom a prompt that could have the form of “a photo of\na [CLASS].” where the class token is replaced by the\nspeciﬁc class name, such as “cat,” “dog” or “car.” The\nLearning to Prompt for Vision-Language Models 5\nprediction probability is then computed as\np(y= i|x) = exp(cos(wi,f)/τ)∑K\nj=1 exp(cos(wj ,f)/τ)\n, (1)\nwhere τ is a temperature parameter learned by CLIP\nand cos(·,·) denotes cosine similarity.\nCompared with the traditional classiﬁer learning\napproach where closed-set visual concepts are learned\nfrom random vectors, vision-language pre-training al-\nlows open-set visual concepts to be explored through\na high-capacity text encoder, leading to a broader se-\nmantic space and in turn making the learned represen-\ntations more transferable to downstream tasks.\n3.2 Context Optimization\nWe propose Context Optimization (CoOp), which\navoids manual prompt tuning by modeling context\nwords with continuous vectors that are end-to-end\nlearned from data while the massive pre-trained pa-\nrameters are frozen. An overview is shown in Figure 2.\nBelow we provide several diﬀerent implementations.\nUniﬁed Context We ﬁrst introduce the uniﬁed con-\ntext version, which shares the same context with all\nclasses. Speciﬁcally, the prompt given to the text en-\ncoder g(·) is designed with the following form,\nt = [V]1[V]2 ... [V]M [CLASS], (2)\nwhere each [V] m (m ∈{1,...,M }) is a vector with\nthe same dimension as word embeddings (i.e., 512 for\nCLIP), and M is a hyperparameter specifying the num-\nber of context tokens.\nBy forwarding a prompt t to the text encoder g(·),\nwe can obtain a classiﬁcation weight vector representing\na visual concept (still from the [EOS] token position).\nThe prediction probability is computed as\np(y= i|x) = exp(cos(g(ti),f)/τ)∑K\nj=1 exp(cos(g(tj),f)/τ)\n, (3)\nwhere the class token within each prompt ti is replaced\nby the corresponding word embedding vector(s) of the\ni-th class name.\nOther than placing the class token at the end of a\nsequence as in Equation (2), we can also put it in the\nmiddle like\nt = [V]1 ... [V]M\n2\n[CLASS][V]M\n2 +1 ... [V]M , (4)\nwhich increases ﬂexibility for learning—the prompt is\nallowed to either ﬁll the latter cells with supplementary\ndescriptions or cut oﬀ the sentence earlier by using a\ntermination signal such as full stop.\nClass-Speciﬁc Context Another option is to de-\nsign class-speciﬁc context (CSC) where context vectors\nare independent to each class, i.e., [V] i\n1[V]i\n2 ... [V]i\nM ̸=\n[V]j\n1[V]j\n2 ... [V]j\nM for i ̸= j and i,j ∈ {1,...,K }. As\nan alternative to uniﬁed context, we ﬁnd that CSC is\nparticularly useful for some ﬁne-grained classiﬁcation\ntasks.\nTraining is performed to minimize the standard clas-\nsiﬁcation loss based on the cross-entropy, and the gra-\ndients can be back-propagated all the way through the\ntext encoder g(·), making use of the rich knowledge en-\ncoded in the parameters to optimize the context. The\ndesign of continuous representations also allows full ex-\nploration in the word embedding space, which facili-\ntates the learning of task-relevant context.\n3.3 Discussion\nOur approach speciﬁcally addresses the emerging prob-\nlem of the adaptation of recently proposed large vision-\nlanguage models such as CLIP (Radford et al., 2021).\nThere are some diﬀerences that distinguish our ap-\nproach from the prompt learning methods developed in\nNLP for language models (e.g., GPT-3 (Brown et al.,\n2020)). First, the backbone architectures are clearly dif-\nferent for CLIP-like models and language models—the\nformer take both visual and textual data as input and\nproduce alignment scores used for image recognition,\nwhile the latter are tailored to handle textual data only.\nSecond, the pre-training objectives are diﬀerent: con-\ntrastive learning vs autoregressive learning. This would\nlead to diﬀerent model behaviors and thus require dif-\nferent module designs.\n4 Experiments\n4.1 Few-Shot Learning\nDatasets We select 11 publicly available image clas-\nsiﬁcation datasets used in CLIP: ImageNet (Deng\net al., 2009), Caltech101 (Fei-Fei et al., 2004), Oxford-\nPets (Parkhi et al., 2012), StanfordCars (Krause et al.,\n2013), Flowers102 (Nilsback and Zisserman, 2008),\nFood101 (Bossard et al., 2014), FGVCAircraft (Maji\net al., 2013), SUN397 (Xiao et al., 2010), DTD (Cim-\npoi et al., 2014), EuroSAT (Helber et al., 2019) and\nUCF101 (Soomro et al., 2012) (see Appendix A for their\nstatistics). These datasets constitute a comprehensive\nbenchmark, which covers a diverse set of vision tasks\nincluding classiﬁcation on generic objects, scenes, ac-\ntions and ﬁne-grained categories, as well as specialized\ntasks like recognizing textures and satellite imagery.\n6 Kaiyang Zhou et al.\nFig. 3 Main results of few-shot learning on the 11 datasets. Overall, CoOp eﬀectively turns CLIP into a strong\nfew-shot learner (solid lines), achieving signiﬁcant improvements over zero-shot CLIP (stars) and performing favorably against\nthe linear probe alternative (dashed lines). M denotes the context length. “end” or “mid” means putting the class token in\nthe end or middle. CSC means class-speciﬁc context.\nWe follow the few-shot evaluation protocol adopted in\nCLIP (Radford et al., 2021), using 1, 2, 4, 8 and 16\nshots for training respectively and deploying models in\nthe full test sets. The average results over three runs\nare reported for comparison.\nTraining Details CoOp has four versions: position-\ning the class token in the end or middle; uniﬁed con-\ntext vs CSC. Unless otherwise stated, ResNet-50 (He\net al., 2016) is used as the image encoder’s backbone\nand the number of context tokens M is set to 16. In-\nvestigations on other design choices are discussed in\nLearning to Prompt for Vision-Language Models 7\nSection 4.3. All models are built on top of CLIP’s open-\nsource code.2 CoOp’s context vectors are randomly ini-\ntialized by drawing from a zero-mean Gaussian distri-\nbution with standard deviation equal to 0.02. Training\nis done with SGD and an initial learning rate of 0.002,\nwhich is decayed by the cosine annealing rule. The max-\nimum epoch is set to 200 for 16/8 shots, 100 for 4/2\nshots, and 50 for 1 shot (except for ImageNet where\nthe maximum epoch is ﬁxed to 50). To mitigate explo-\nsive gradients observed in the early training iterations,\nwe use the warmup trick by ﬁxing the learning rate to\n1e−5, only during the ﬁrst epoch.\nBaseline Methods We compare CoOp with two\nbaseline methods. The ﬁrst is zero-shot CLIP, which\nis based on hand-crafted prompts. We follow the guide-\nline of prompt engineering introduced by Radford et al.\n(2021). For generic objects and scenes, “a photo of a\n[CLASS].” is adopted. For ﬁne-grained categories, task-\nrelevant context is added like “a type of pet” for Ox-\nfordPets and “a type of food” for Food101. When it\ncomes to specialized tasks such as recognizing textures\nin DTD, the prompt is customized as “[CLASS] tex-\nture.” where the class names are adjectives like “bub-\nbly” and “dotted.” See Appendix A for the details. The\nsecond baseline is the linear probe model. As suggested\nby Radford et al. (2021) and a recent study on few-shot\nlearning (Tian et al., 2020), training a linear classiﬁer\non top of high-quality pre-trained models’ features (like\nCLIP) can easily achieve performance that is on a par\nwith that of state-of-the-art few-shot learning methods,\nwhich are often much more sophisticated. We follow the\nsame training method used by Radford et al. (2021) to\ntrain the linear probe model.\nComparison with Hand-Crafted Prompts Fig-\nure 3 summarizes the results. Our default model is\nCLIP+CoOp with the class token positioned in the\nend. The two diﬀerent ways of positioning the class to-\nken achieve similar performance as their curves highly\noverlap. From the average performance displayed in the\ntop-left corner, we observe that CLIP+CoOp is a strong\nfew-shot learner, requiring only two shots on average to\nobtain a decent margin over zero-shot CLIP. Given 16\nshots for training, the average gap brought by CoOp\ncan be further increased to around 15%.\nFigure 4 ranks the absolute improvements obtained\nby CoOp at 16 shots over hand-crafted prompts. Huge\nimprovements are observed on specialized tasks namely\nEuroSAT and DTD where the increase in performance\nreaches over 45% and 20% respectively. The jumps\nin performance are also signiﬁcant (those more than\n2 https://github.com/openai/CLIP.\n0 10 20 30 40\nAbsolute improvement (%)\nFood101\nOxfordPets\nImageNet\nCaltech101\nSUN397\nFGVCAircraft\nUCF101\nStanfordCars\nDTD\nFlowers102\nEuroSAT\n-2.64\n+1.24\n+4.77\n+5.54\n+10.74\n+13.98\n+14.25\n+17.75\n+21.26\n+28.37\n+45.97\nCLIP + CoOp (M=16, end) vs. Zero-Shot CLIP\nFig. 4 Comparison with hand-crafted prompts.\n10%) on most ﬁne-grained datasets including Flow-\ners102, StanfordCars and FGVCAircraft, as well as\non scene and action recognition datasets (i.e., SUN397\n& UCF101). Since ImageNet is a challenging dataset\nthat contains 1,000 classes, the 4.77% improvement is\nalso noteworthy. In contrast, the increases on the two\nﬁne-grained datasets, OxfordPets and Food101, are less\nappealing.3 By digging into CLIP+CoOp’s curves on\nthese two datasets in Figure 3, we ﬁnd there is a loss\nof momentum in performance improvements even with\nmore shots used, seemingly an overﬁtting problem. A\npotential solution is to impose higher regularization like\nincreasing the weight decay. Nonetheless, the overall re-\nsults are strong enough to serve as evidence of CoOp’s\ncapability of learning task-relevant prompts in a data-\neﬃcient manner.\nComparison with Linear Probe CLIP In terms\nof the overall performance (Figure 3, top-left),\nCLIP+CoOp demonstrates clear advantages over the\nlinear probe model. The latter requires more than 4\nshots on average to match the zero-shot’s performance\nwhile CoOp’s average gain at 4 shots is already im-\npressive. It is also clear that the gaps in the extreme\nlow-data regime such as one or two shots are much\nlarger, suggesting that CoOp is much more eﬀective\nthan learning a linear classiﬁer from scratch for few-shot\nlearning. We also observe that the linear probe model\nis comparable to CLIP+CoOp on the two specialized\ntasks (DTD & EuroSAT) as well as on a couple of ﬁne-\ngrained datasets (Flowers102 & FGVCAircraft)—this\nis not too surprising as the pre-trained CLIP space has\nbeen proved powerful, making the linear probe model\na strong competitor. Nevertheless, CoOp’s CSC version\n3 We ﬁnd that the negative results on Food101, for learning-\nbased models including CoOp and linear probe, are caused by\nthe noisy training data with “intense colors and sometimes\nwrong labels” (Bossard et al., 2014).\n8 Kaiyang Zhou et al.\nTable 1 Comparison with zero-shot CLIP on robustness to distribution shift using diﬀerent vision backbones. M: CoOp’s\ncontext length.\nSource Target\nMethod ImageNet -V2 -Sketch -A -R\nResNet-50\nZero-Shot CLIP 58.18 51.34 33.32 21.65 56.00\nLinear Probe CLIP 55.87 45.97 19.07 12.74 34.86\nCLIP + CoOp ( M=16) 62.95 55.11 32.74 22.12 54.96\nCLIP + CoOp ( M=4) 63.33 55.40 34.67 23.06 56.60\nResNet-101\nZero-Shot CLIP 61.62 54.81 38.71 28.05 64.38\nLinear Probe CLIP 59.75 50.05 26.80 19.44 47.19\nCLIP + CoOp ( M=16) 66.60 58.66 39.08 28.89 63.00\nCLIP + CoOp ( M=4) 65.98 58.60 40.40 29.60 64.98\nViT-B/32\nZero-Shot CLIP 62.05 54.79 40.82 29.57 65.99\nLinear Probe CLIP 59.58 49.73 28.06 19.67 47.20\nCLIP + CoOp ( M=16) 66.85 58.08 40.44 30.62 64.45\nCLIP + CoOp ( M=4) 66.34 58.24 41.48 31.34 65.78\nViT-B/16\nZero-Shot CLIP 66.73 60.83 46.15 47.77 73.96\nLinear Probe CLIP 65.85 56.26 34.77 35.68 58.43\nCLIP + CoOp ( M=16) 71.92 64.18 46.71 48.41 74.32\nCLIP + CoOp ( M=4) 71.73 64.56 47.89 49.93 75.14\ncan beat the linear probe CLIP on the aforementioned\ndatasets, and moreover, shows much better potential\nwhen more shots become available. We later show that\nCoOp obtains much stronger performance than the lin-\near probe model in domain generalization.\nUniﬁed vs Class-Speciﬁc Context On average,\nusing uniﬁed context leads to better performance. In\nterms of when to apply CSC and when not to, we\nhave the following suggestions. For generic objects (Im-\nageNet & Caltech101), scenes (SUN397) and actions\n(UCF101), using uniﬁed context is clearly better. Uni-\nﬁed context also works better on some ﬁne-grained\ndatasets including OxfordPets and Food101, but on\nothers like StanfordCars, Flowers102 and FGVCAir-\ncraft the CSC version is preferred. CSC also yields\nbetter performance on the two specialized tasks, DTD\nand EuroSAT, at 16 shots in particular. However, CSC\nmostly underperforms uniﬁed context in challenging\nlow-data scenarios (fewer than 8 shots), which makes\nsense because CSC has more parameters than uniﬁed\ncontext and needs more data for training.\n4.2 Domain Generalization\nSince CoOp requires training on a speciﬁc data distri-\nbution, it risks learning spurious correlations that are\ndetrimental to generalization in unseen distributions\n(domains), as suggested in recent studies (Taori et al.,\n2020; Zhou et al., 2021). On the contrary, zero-shot\nCLIP is not tied to a speciﬁc data distribution and has\nexhibited strong robustness to distribution shifts (Rad-\nford et al., 2021). In this section, we aim to unveil how\nrobust CoOp is to distribution shifts, in comparison to\nzero-shot CLIP and the linear probe model.\nDatasets The source dataset is ImageNet. The\ntarget datasets are ImageNetV2 (Recht et al.,\n2019), ImageNet-Sketch (Wang et al., 2019),\nImageNet-A (Hendrycks et al., 2021b) and ImageNet-\nR (Hendrycks et al., 2021a), all of which have\ncompatible class names with ImageNet allowing\nseamless transfer for the prompts learned by CoOp.\nImageNetV2 is a reproduced test set using diﬀerent\nsources while following ImageNet’s data collection\nprocess. ImageNet-Sketch contains sketch images\nbelonging to the same 1,000 ImageNet classes. Both\nImageNet-A and -R contain 200 classes derived from a\nsubset of ImageNet’s 1,000 classes. The former consists\nof real-world adversarially ﬁltered images that cause\ncurrent ImageNet classiﬁers to produce low results,\nwhereas the latter features a rendition of the ImageNet\nclasses in diverse image styles such as paintings,\ncartoons and sculptures.\nLearning to Prompt for Vision-Language Models 9\n(a) Context length(b) Vision backbones\nFig. 5 Investigations on CoOp’s context length and various vision backbones.\nTable 2 Comparison with prompt engineering and prompt ensembling on ImageNet using diﬀerent vision backbones.\nMethod ResNet-50 ResNet-101 ViT-B/32 ViT-B/16\nPrompt engineering 58.18 61.26 62.05 66.73\nPrompt ensembling 60.41 62.54 63.71 68.74\nCoOp 62.95 66.60 66.85 71.92\nTable 3 Random vs manual initialization.\nAvg %\n[V]1[V]2[V]3[V]4 72.65\n“a photo of a” 72.65\nResults Table 1 summarizes the results (with a va-\nriety of vision backbones). It is surprising that CoOp\nenhances CLIP’s robustness to distribution shifts, de-\nspite the exposure to the source dataset. This suggests\nthat the learned prompts are also generalizable. More-\nover, it is interesting to see that using fewer context\ntokens leads to better robustness. In contrast, the lin-\near probe model obtains much worse results on these\ntarget datasets, exposing its weakness in domain gen-\neralization. In Appendix B, we provide the domain gen-\neralization results on DOSCO-2k (Zhou et al., 2022b),\na recently proposed benchmark focusing on contextual\ndomain shift.\n4.3 Further Analysis\nContext Length How many context tokens should\nbe used? And is it better to have more context tokens?\nThe results in Section 4.2 suggest having a shorter con-\ntext length beneﬁts domain generalization (probably\ndue to less overﬁtting as fewer parameters are learned).\nHere we study this hyperparameter for source datasets.\nSpeciﬁcally, we repeat experiments on the 11 datasets\nby varying the context length from 4 to 8 to 16. The\naverage results are shown in Figure 5(a), which indicate\nthat having more context tokens leads to better perfor-\nmance and that positioning the class token in the mid-\ndle gains more momentum with longer context length.\nTo sum up, there is no golden rule for selecting per-\nfect context length since one needs to balance between\nperformance and robustness to distribution shift.\nVision Backbones Figure 5(b) summarizes the re-\nsults on the 11 datasets using a variety of vision back-\nbones covering both CNNs and ViTs. The results are\nexpected: the more advanced the backbone, the better\nthe performance. The gap between CoOp and hand-\ncrafted prompts is signiﬁcant across all architectures.\nComparison with Prompt Ensembling The au-\nthors of CLIP (Radford et al., 2021) have suggested\nthat additional improvements can be obtained by\nensembling over multiple zero-shot classiﬁers gener-\nated using diﬀerent hand-crafted prompts, such as “a\nphoto of the large [CLASS].”, “a bad photo of the\n[CLASS].” and “a origami [CLASS].”, which reﬂect a\ndiﬀerent scale, view and abstraction respectively for an\nimage. We are interested to know whether the prompts\nlearned by CoOp can still maintain advantages when\ncompared with prompt ensembling. For fair compar-\nison, we use the select prompts from Radford et al.\n(2021), which have been extensively tuned on Ima-\ngeNet, to construct the ensemble classiﬁer. Table 2\nshows the comparison and justiﬁes the superiority of\n10 Kaiyang Zhou et al.\nCoOp. Given the potential of prompt ensembling, fu-\nture work could investigate how to improve CoOp from\nthe ensembling perspective.\nComparison with Other Fine-tuning Methods\nWe further compare CoOp with other ﬁne-tuning meth-\nods: i) ﬁne-tuning CLIP’s image encoder; ii) optimizing\na transformation layer added to the text encoder’s out-\nput; iii) optimizing a bias term added to the text en-\ncoder’s output. The results are shown in Table 5. Obvi-\nously, ﬁne-tuning the image encoder does not work well.\nAdding a transformation layer slightly improves upon\nthe zero-shot model. Adding a bias term shows promis-\ning results, but still largely underperforms CoOp, which\nsuggests that the gradients that went through the text\nencoder provide more useful information.\nInitialization We compare random initialization\nwith manual initialization. The latter uses the embed-\ndings of “a photo of a” to initialize the context vectors\nfor the 11 datasets. For fair comparison, we also set the\ncontext length to 4 when using random initialization.\nTable 3 suggests a “good” initialization does not make\nmuch diﬀerence. Though further tuning of the initial-\nization words might help, in practice we suggest using\nthe simple random initialization method.\nInterpreting the Learned Prompts is diﬃcult be-\ncause the context vectors are optimized in a continu-\nous space. We resort to an indirect way by searching\nwithin the vocabulary for words that are closest to the\nlearned vectors based on the Euclidean distance. Note\nthat CLIP (Radford et al., 2021) uses the BPE rep-\nresentation (Sennrich et al., 2016) for tokenization, so\nthe vocabulary includes subwords that frequently ap-\npear in text, such as “hu” (subsumed by many words\nlike “hug” and “human”). Table 4 shows the searched\nresults on some datasets. We observe that a few words\nare somewhat relevant to the tasks, such as “enjoyed”\nfor Food101, “ﬂuﬀy” and “paw” for OxfordPets, and\n“pretty” for DTD. But when connecting all the near-\nest words together, the prompts do not make much\nsense. We also observe that when using manual initial-\nization (like “a photo of a”), the nearest words for the\nconverged vectors are mostly the ones used for initial-\nization. We conjecture that the learned vectors might\nencode meanings that are beyond the existing vocabu-\nlary. Overall, we are unable to draw any ﬁrm conclusion\nbased on the observations because using nearest words\nto interpret the learned prompts could be inaccurate—\nthe semantics of the vectors is not necessarily correlated\nwith the nearest words.\n5 Conclusion, Limitations and Future Work\nLarge pre-trained vision-language models have shown\nsurprisingly powerful capabilities in diverse down-\nstream applications. However, these models, also called\nvision foundation models given their “critically central\nyet incomplete” nature (Bommasani et al., 2021), need\nto be adapted using automated techniques for better\ndownstream performance and eﬃciency.\nOur research provides timely insights on how CLIP-\nlike models can be turned into a data-eﬃcient learner\nby using prompt learning, and reveals that despite be-\ning a learning-based approach, CoOp performs much\nbetter in domain generalization than manual prompts.\nThe results serve as strong evidence that prompt learn-\ning has potential for large vision models. It is worth\nnoting that our paper presents the ﬁrst comprehensive\nstudy about adapting large vision models with prompt\nlearning.\nThough the performance is excellent, the results of\nCoOp are relatively diﬃcult to interpret, like other con-\ntinuous prompt learning methods in NLP. The experi-\nments also reveal that CoOp is sensitive to noisy labels\ngiven the weak performance on Food101.\nNevertheless, the simplicity of CoOp allows easy ex-\ntension for future work and there remain many inter-\nesting questions to explore, such as cross-dataset trans-\nfer (Zhou et al., 2022a) and test-time adaptation (Wang\net al., 2020). It would also be interesting to investi-\ngate more generic adaptation methods for mega-size vi-\nsion models (Jia et al., 2022; Bahng et al., 2022; Gao\net al., 2021). In summary, we hope the empirical ﬁnd-\nings and insights presented in this work could pave the\nway for future research on eﬃcient adaptation methods\nfor emerging foundation models, which is still a nascent\nresearch topic.\nAcknowledgements This work is supported by NTU NAP,\nMOE AcRF Tier 2 (T2EP20221-0033), and under the\nRIE2020 Industry Alignment Fund – Industry Collaboration\nProjects (IAF-ICP) Funding Initiative, as well as cash and in-\nkind contribution from the industry partner(s). Correspond-\ning author: Ziwei Liu (ziwei.liu@ntu.edu.sg).\nAppendix\nA Datasets Details\nThe detailed statistics of the 11 datasets, as well as the\nfour variants of ImageNet, are shown in Table 6. The hand-\ncrafted prompts used for zero-shot CLIP are also detailed\nin the table. For Caltech101, the “BACKGROUND Google”\nand “Faces easy” classes are discarded. For the video dataset,\nUCF101, the middle frame of each video is used as input to\nthe image encoder.\nLearning to Prompt for Vision-Language Models 11\nTable 4 The nearest words for each of the 16 context vectors learned by CoOp, with their distances shown in parentheses.\nN/A means non-Latin characters.\n# ImageNet Food101 OxfordPets DTD UCF101\n1 potd (1.7136) lc (0.6752) tosc (2.5952) boxed (0.9433) meteorologist (1.5377)\n2 that (1.4015) enjoyed (0.5305) judge (1.2635) seed (1.0498) exe (0.9807)\n3 ﬁlmed (1.2275) beh (0.5390) ﬂuﬀy (1.6099) anna (0.8127) parents (1.0654)\n4 fruit (1.4864) matches (0.5646) cart (1.3958) mountain (0.9509) masterful (0.9528)\n5 ,... (1.5863) nytimes (0.6993) harlan (2.2948) eldest (0.7111) fe (1.3574)\n6 ° (1.7502) prou (0.5905) paw (1.3055) pretty (0.8762) thof (1.2841)\n7 excluded (1.2355) lower (0.5390) incase (1.2215) faces (0.7872) where (0.9705)\n8 cold (1.4654) N/A bie (1.5454) honey (1.8414) kristen (1.1921)\n9 stery (1.6085) minute (0.5672) snuggle (1.1578) series (1.6680) imam (1.1297)\n10 warri (1.3055) ∼ (0.5529) along (1.8298) coca (1.5571) near (0.8942)\n11 marvelcomics (1.5638) well (0.5659) enjoyment (2.3495) moon (1.2775) tummy (1.4303)\n12 .: (1.7387) ends (0.6113) jt (1.3726) lh (1.0382) hel (0.7644)\n13 N/A mis (0.5826) improving (1.3198) won (0.9314) boop (1.0491)\n14 lation (1.5015) somethin (0.6041) srsly (1.6759) replied (1.1429) N/A\n15 muh (1.4985) seminar (0.5274) asteroid (1.3395) sent (1.3173) facial (1.4452)\n16 .# (1.9340) N/A N/A piedmont (1.5198) during (1.1755)\nTable 5 CoOp vs other ﬁne-tuning methods on ImageNet\n(w/ 16 shots). ∆: diﬀerence with the zero-shot model.\nImageNet ∆\nZero-shot CLIP 58.18 -\nLinear probe 55.87 -2.31\nFine-tuning CLIP’s image encoder 18.28 -39.90\nOptimizing transformation layer (text) 58.86 0.68\nOptimizing bias (text) 60.93 +2.75\nCoOp 62.95 +4.77\nB Results on DOSCO-2k\nDOSCO-2k The DOSCO (DOmain Shift in COntext)\nbenchmark (Zhou et al., 2022b) contains 7 image recogni-\ntion datasets, which cover a wide range of classiﬁcation prob-\nlems, such as generic object recognition, ﬁne-grained recog-\nnition on aircraft models, and action recognition. Unlike ex-\nisting domain generalization datasets where the domain la-\nbels are manually deﬁned and often limited to image style\nvariations, DOSCO-2k focuses on broader contextual domain\nshift, which is automatically detected by a neural network\npre-trained on the Places dataset (Zhou et al., 2017). Fol-\nlowing Zhou et al. (2022b), we use the 2k version where the\ntraining and validation splits in each dataset have 2,000 im-\nages in total (1,600 for training and 400 for validation).\nResults We study three methods’ domain generalization\nperformance on DOSCO-2k: CLIP, CoOp and CoCoOp (Zhou\net al., 2022a). All models are trained on the training set and\nthe checkpoints with the best validation performance are used\nfor ﬁnal test in unseen domains. Table 7 shows the results of\nfour diﬀerent architectures. It is clear that the two learn-\nable methods outperform the zero-shot method with a large\nmargin, despite having only a small number of parameters\nto tune. CoCoOp beats CoOp on 4 out of 7 datasets but\nCoOp’s average performance is higher. In summary, the re-\nsults suggest that eﬃcient adaptation methods like CoOp and\nCoCoOp have great potential in tackling transfer learning\nproblems.\nReferences\nBahng H, Jahanian A, Sankaranarayanan S, Isola P (2022)\nVisual prompting: Modifying pixel space to adapt pre-\ntrained models. arXiv preprint arXiv:220317274\nBommasani R, Hudson DA, Adeli E, Altman R, Arora S, von\nArx S, Bernstein MS, Bohg J, Bosselut A, Brunskill E,\net al. (2021) On the opportunities and risks of foundation\nmodels. arXiv preprint arXiv:210807258\nBossard L, Guillaumin M, Van Gool L (2014) Food-101–\nmining discriminative components with random forests. In:\nECCV\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhari-\nwal P, Neelakantan A, Shyam P, Sastry G, Askell A,\net al. (2020) Language models are few-shot learners. arXiv\npreprint arXiv:200514165\nChen T, Kornblith S, Norouzi M, Hinton G (2020) A sim-\nple framework for contrastive learning of visual represen-\ntations. In: ICML\nCimpoi M, Maji S, Kokkinos I, Mohamed S, Vedaldi A (2014)\nDescribing textures in the wild. In: CVPR\nDeng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009)\nImagenet: A large-scale hierarchical image database. In:\nCVPR\nDesai K, Johnson J (2021) Virtex: Learning visual represen-\ntations from textual annotations. In: CVPR\nDosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai\nX, Unterthiner T, Dehghani M, Minderer M, Heigold G,\nGelly S, et al. (2021) An image is worth 16x16 words:\nTransformers for image recognition at scale. In: ICLR\nElhoseiny M, Saleh B, Elgammal A (2013) Write a classiﬁer:\nZero-shot learning using purely textual descriptions. In:\nICCV\nFei-Fei L, Fergus R, Perona P (2004) Learning generative\nvisual models from few training examples: An incremen-\ntal bayesian approach tested on 101 object categories. In:\nCVPR-W\n12 Kaiyang Zhou et al.\nTable 6 Datasets statistics.\nDataset Classes Train Val Test Hand-crafted prompt\nImageNet 1,000 1.28M N/A 50,000 “a photo of a [CLASS].”\nCaltech101 100 4,128 1,649 2,465 “a photo of a [CLASS].”\nOxfordPets 37 2,944 736 3,669 “a photo of a [CLASS], a type of pet.”\nStanfordCars 196 6,509 1,635 8,041 “a photo of a [CLASS].”\nFlowers102 102 4,093 1,633 2,463 “a photo of a [CLASS], a type of ﬂower.”\nFood101 101 50,500 20,200 30,300 “a photo of [CLASS], a type of food.”\nFGVCAircraft 100 3,334 3,333 3,333 “a photo of a [CLASS], a type of aircraft.”\nSUN397 397 15,880 3,970 19,850 “a photo of a [CLASS].”\nDTD 47 2,820 1,128 1,692 “[CLASS] texture.”\nEuroSAT 10 13,500 5,400 8,100 “a centered satellite photo of [CLASS].”\nUCF101 101 7,639 1,898 3,783 “a photo of a person doing [CLASS].”\nImageNetV2 1,000 N/A N/A 10,000 “a photo of a [CLASS].”\nImageNet-Sketch 1,000 N/A N/A 50,889 “a photo of a [CLASS].”\nImageNet-A 200 N/A N/A 7,500 “a photo of a [CLASS].”\nImageNet-R 200 N/A N/A 30,000 “a photo of a [CLASS].”\nTable 7 Domain generalization results on DOSCO-2k, a recently proposed benchmark focusing on broader contextual domain\nshift. Among the three approaches, CoOp and its follow-up, CoCoOp, contain learnable components while CLIP here denotes\nthe zero-shot model. Both CoOp and CoCoOp use four learnable context tokens initialized with the word embeddings of “a\nphoto of a”. Bold denotes the best performance on each dataset for a speciﬁc architecture.\nP-Air P-Cars P-Ctech P-Ins P-Mam P-Pets P-UCF Avg\nResNet-50\nCLIP 16.1 56.1 86.7 62.7 59.7 84.0 60.6 60.9\nCoOp 22.1 60.7 89.4 66.3 61.6 83.8 69.2 64.7\nCoCoOp 20.1 59.8 90.4 67.9 63.8 87.6 69.1 65.5\nResNet-101\nCLIP 17.5 63.2 89.5 62.4 62.2 84.2 61.3 62.9\nCoOp 24.6 68.2 92.0 68.3 65.4 88.2 72.7 68.5\nCoCoOp 22.5 65.2 93.3 69.9 67.5 88.6 71.5 68.4\nViT-B/32\nCLIP 18.2 60.1 91.6 61.3 61.8 85.5 61.3 62.8\nCoOp 24.0 63.0 93.6 67.3 65.7 88.5 74.5 68.1\nCoCoOp 19.5 60.4 93.8 69.8 67.3 88.5 72.7 67.4\nViT-B/16\nCLIP 24.4 64.9 92.6 67.5 67.9 87.4 66.1 67.2\nCoOp 32.4 72.4 94.7 73.2 72.1 90.1 78.2 73.3\nCoCoOp 30.4 68.7 94.8 73.5 73.6 91.6 76.3 72.7\nFrome A, Corrado G, Shlens J, Bengio S, Dean J, Ranzato\nM, Mikolov T (2013) Devise: A deep visual-semantic em-\nbedding model. In: NeurIPS\nF¨ urst A, Rumetshofer E, Tran V, Ramsauer H, Tang F,\nLehner J, Kreil D, Kopp M, Klambauer G, Bitto-Nemling\nA, et al. (2021) Cloob: Modern hopﬁeld networks with in-\nfoloob outperform clip. arXiv preprint arXiv:211011316\nGao P, Geng S, Zhang R, Ma T, Fang R, Zhang Y, Li H, Qiao\nY (2021) Clip-adapter: Better vision-language models with\nfeature adapters. arXiv preprint arXiv:211004544\nGao T, Fisch A, Chen D (2020) Making pre-trained lan-\nguage models better few-shot learners. arXiv preprint\narXiv:201215723\nGomez L, Patel Y, Rusi˜ nol M, Karatzas D, Jawahar C (2017)\nSelf-supervised learning of visual features through embed-\nding images into text topic spaces. In: CVPR\nHe K, Zhang X, Ren S, Sun J (2016) Deep residual learning\nfor image recognition. In: CVPR\nHe K, Fan H, Wu Y, Xie S, Girshick R (2020) Momentum\ncontrast for unsupervised visual representation learning.\nIn: CVPR\nHelber P, Bischke B, Dengel A, Borth D (2019) Eurosat: A\nnovel dataset and deep learning benchmark for land use\nand land cover classiﬁcation. IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing\nH´ enaﬀ OJ, Srinivas A, Fauw JD, Razavi A, Doersch C, Es-\nlami SMA, van den Oord A (2020) Data-eﬃcient image\nrecognition with contrastive predictive coding. In: ICML\nHendrycks D, Basart S, Mu N, Kadavath S, Wang F, Dorundo\nE, Desai R, Zhu T, Parajuli S, Guo M, Song D, Steinhardt\nJ, Gilmer J (2021a) The many faces of robustness: A crit-\nical analysis of out-of-distribution generalization. ICCV\nLearning to Prompt for Vision-Language Models 13\nHendrycks D, Zhao K, Basart S, Steinhardt J, Song D (2021b)\nNatural adversarial examples. In: CVPR\nJia C, Yang Y, Xia Y, Chen YT, Parekh Z, Pham H, Le\nQV, Sung Y, Li Z, Duerig T (2021) Scaling up visual and\nvision-language representation learning with noisy text su-\npervision. In: ICML\nJia M, Tang L, Chen BC, Cardie C, Belongie S, Hariharan\nB, Lim SN (2022) Visual prompt tuning. arXiv preprint\narXiv:220312119\nJiang Z, Xu FF, Araki J, Neubig G (2020) How can we know\nwhat language models know? ACL\nJoulin A, Van Der Maaten L, Jabri A, Vasilache N (2016)\nLearning visual features from large weakly supervised data.\nIn: ECCV\nKrause J, Stark M, Deng J, Fei-Fei L (2013) 3d object repre-\nsentations for ﬁne-grained categorization. In: ICCV-W\nLei Ba J, Swersky K, Fidler S, et al. (2015) Predicting deep\nzero-shot convolutional neural networks using textual de-\nscriptions. In: ICCV\nLester B, Al-Rfou R, Constant N (2021) The power of\nscale for parameter-eﬃcient prompt tuning. arXiv preprint\narXiv:210408691\nLi A, Jabri A, Joulin A, van der Maaten L (2017) Learning\nvisual n-grams from web data. In: ICCV\nLi XL, Liang P (2021) Preﬁx-tuning: Optimizing continuous\nprompts for generation. arXiv preprint arXiv:210100190\nLi Y, Liang F, Zhao L, Cui Y, Ouyang W, Shao J, Yu F, Yan\nJ (2021) Supervision exists everywhere: A data eﬃcient\ncontrastive language-image pre-training paradigm. arXiv\npreprint arXiv:211005208\nLiu P, Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G (2021a)\nPre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. arXiv\npreprint arXiv:210713586\nLiu X, Zheng Y, Du Z, Ding M, Qian Y, Yang Z,\nTang J (2021b) Gpt understands, too. arXiv preprint\narXiv:210310385\nMaji S, Rahtu E, Kannala J, Blaschko M, Vedaldi A (2013)\nFine-grained visual classiﬁcation of aircraft. arXiv preprint\narXiv:13065151\nNilsback ME, Zisserman A (2008) Automated ﬂower classiﬁ-\ncation over a large number of classes. In: ICVGIP\nParkhi OM, Vedaldi A, Zisserman A, Jawahar C (2012) Cats\nand dogs. In: CVPR\nPetroni F, Rockt¨ aschel T, Lewis P, Bakhtin A, Wu Y, Miller\nAH, Riedel S (2019) Language models as knowledge bases?\nIn: EMNLP\nRadford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal\nS, Sastry G, Askell A, Mishkin P, Clark J, et al. (2021)\nLearning transferable visual models from natural language\nsupervision. In: ICML\nRecht B, Roelofs R, Schmidt L, Shankar V (2019) Do ima-\ngenet classiﬁers generalize to imagenet? In: ICML\nSennrich R, Haddow B, Birch A (2016) Neural machine trans-\nlation of rare words with subword units. In: ACL\nShin T, Razeghi Y, Logan IV RL, Wallace E, Singh S (2020)\nAutoprompt: Eliciting knowledge from language models\nwith automatically generated prompts. In: EMNLP\nSingh A, Hu R, Goswami V, Couairon G, Galuba W,\nRohrbach M, Kiela D (2021) Flava: A foundational\nlanguage and vision alignment model. arXiv preprint\narXiv:211204482\nSocher R, Ganjoo M, Sridhar H, Bastani O, Manning CD, Ng\nAY (2013) Zero-shot learning through cross-modal trans-\nfer. In: NeurIPS\nSoomro K, Zamir AR, Shah M (2012) Ucf101: A dataset of\n101 human actions classes from videos in the wild. arXiv\npreprint arXiv:12120402\nTaori R, Dave A, Shankar V, Carlini N, Recht B, Schmidt L\n(2020) Measuring robustness to natural distribution shifts\nin image classiﬁcation. In: NeurIPS\nTian Y, Wang Y, Krishnan D, Tenenbaum JB, Isola P (2020)\nRethinking few-shot image classiﬁcation: a good embed-\nding is all you need? In: ECCV\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,\nGomez AN, Kaiser  L, Polosukhin I (2017) Attention is all\nyou need. In: NeurIPS\nWang D, Shelhamer E, Liu S, Olshausen B, Darrell T (2020)\nTent: Fully test-time adaptation by entropy minimization.\narXiv preprint arXiv:200610726\nWang H, Ge S, Lipton Z, Xing EP (2019) Learning robust\nglobal representations by penalizing local predictive power.\nIn: NeurIPS\nXiao J, Hays J, Ehinger KA, Oliva A, Torralba A (2010) Sun\ndatabase: Large-scale scene recognition from abbey to zoo.\nIn: CVPR\nYuan L, Chen D, Chen YL, Codella N, Dai X, Gao J,\nHu H, Huang X, Li B, Li C, et al. (2021) Florence: A\nnew foundation model for computer vision. arXiv preprint\narXiv:211111432\nZhang Y, Jiang H, Miura Y, Manning CD, Langlotz CP\n(2020) Contrastive learning of medical visual represen-\ntations from paired images and text. arXiv preprint\narXiv:201000747\nZhong Z, Friedman D, Chen D (2021) Factual probing is\n[mask]: Learning vs. learning to recall. In: NAACL\nZhou B, Lapedriza A, Khosla A, Oliva A, Torralba A (2017)\nPlaces: A 10 million image database for scene recognition.\nIEEE transactions on pattern analysis and machine intel-\nligence 40(6):1452–1464\nZhou K, Liu Z, Qiao Y, Xiang T, Loy CC (2021) Domain\ngeneralization: A survey. arXiv preprint arXiv:210302503\nZhou K, Yang J, Loy CC, Liu Z (2022a) Conditional\nprompt learning for vision-language models. arXiv preprint\narXiv:220305557\nZhou K, Zhang Y, Zang Y, Yang J, Loy CC, Liu Z\n(2022b) On-device domain generalization. arXiv preprint\narXiv:220907521",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8053586483001709
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6463647484779358
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.5442741513252258
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5275190472602844
    },
    {
      "name": "Feature learning",
      "score": 0.48290300369262695
    },
    {
      "name": "Feature engineering",
      "score": 0.469016969203949
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.45874127745628357
    },
    {
      "name": "Machine learning",
      "score": 0.4470144808292389
    },
    {
      "name": "Natural language processing",
      "score": 0.4367196559906006
    },
    {
      "name": "Language model",
      "score": 0.42387789487838745
    },
    {
      "name": "Transfer of learning",
      "score": 0.4154379367828369
    },
    {
      "name": "Representation (politics)",
      "score": 0.410855770111084
    },
    {
      "name": "Deep learning",
      "score": 0.2970987856388092
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    }
  ],
  "cited_by": 2008
}