{
  "title": "A Comparison of Transformer and Recurrent Neural Networks on Multilingual Neural Machine Translation",
  "url": "https://openalex.org/W2809456172",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225981102",
      "name": "Lakew, Surafel M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2586592233",
      "name": "Cettolo, Mauro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2799855848",
      "name": "Federico, Marcello",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2222949842",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W2513073850",
    "https://openalex.org/W2950635152",
    "https://openalex.org/W287510790",
    "https://openalex.org/W2772120246",
    "https://openalex.org/W2555745756",
    "https://openalex.org/W2725082186",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2251743902",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2463396630",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2949509249",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2229833550",
    "https://openalex.org/W2574872930",
    "https://openalex.org/W2531207078"
  ],
  "abstract": "Recently, neural machine translation (NMT) has been extended to multilinguality, that is to handle more than one translation direction with a single system. Multilingual NMT showed competitive performance against pure bilingual systems. Notably, in low-resource settings, it proved to work effectively and efficiently, thanks to shared representation space that is forced across languages and induces a sort of transfer-learning. Furthermore, multilingual NMT enables so-called zero-shot inference across language pairs never seen at training time. Despite the increasing interest in this framework, an in-depth analysis of what a multilingual NMT model is capable of and what it is not is still missing. Motivated by this, our work (i) provides a quantitative and comparative analysis of the translations produced by bilingual, multilingual and zero-shot systems; (ii) investigates the translation quality of two of the currently dominant neural architectures in MT, which are the Recurrent and the Transformer ones; and (iii) quantitatively explores how the closeness between languages influences the zero-shot translation. Our analysis leverages multiple professional post-edits of automatic translations by several different systems and focuses both on automatic standard metrics (BLEU and TER) and on widely used error categories, which are lexical, morphology, and word order errors.",
  "full_text": "arXiv:1806.06957v2  [cs.CL]  20 Jun 2018\nA Comparison of T ransformer and Recurrent Neural Networks\non Multilingual Neural Machine T ranslation\nSurafel M. Lakew\nUniversity of Trento\nFondazione Bruno Kessler\nlakew@fbk.eu\nMauro Cettolo\nFondazione Bruno Kessler\ncettolo@fbk.eu\nMarcello Federico\nMMT Srl, Trento\nFondazione Bruno Kessler\nfederico@fbk.eu\nAbstract\nRecently , neural machine translation (NMT) has been extend ed to multilinguality , that is to han-\ndle more than one translation direction with a single system . Multilingual NMT showed compet-\nitive performance against pure bilingual systems. Notably , in low-resource settings, it proved to\nwork effectively and efﬁciently , thanks to shared represen tation space that is forced across lan-\nguages and induces a sort of transfer-learning. Furthermor e, multilingual NMT enables so-called\nzero-shot inference across language pairs never seen at tra ining time. Despite the increasing in-\nterest in this framework, an in-depth analysis of what a mult ilingual NMT model is capable of\nand what it is not is still missing. Motivated by this, our wor k (i) provides a quantitative and com-\nparative analysis of the translations produced by bilingua l, multilingual and zero-shot systems;\n(ii) investigates the translation quality of two of the curr ently dominant neural architectures in\nMT , which are the Recurrent and the Transformer ones; and (ii i) quantitatively explores how the\ncloseness between languages inﬂuences the zero-shot trans lation. Our analysis leverages mul-\ntiple professional post-edits of automatic translations b y several different systems and focuses\nboth on automatic standard metrics (BLEU and TER) and on wide ly used error categories, which\nare lexical, morphology , and word order errors.\n1 Introduction\nAs witnessed by recent machine translation evaluation camp aigns (IWSL T (Cettolo et al., 2017),\nWMT (Bojar et al., 2017)), in the past few years several model variants and training procedures have\nbeen proposed and tested in neural machine translation (NMT ). NMT models were mostly employed\nin conventional single language-pair settings, where the t raining process exploits a parallel corpus\nfrom a source language to a target language, and the inferenc e involves only those two languages in\nthe same direction. However, there have also been attempts t o incorporate multiple languages in the\nsource (Luong et al., 2015a; Zoph and Knight, 2016; Lee et al. , 2016), in the target (Dong et al., 2015),\nor in both sides like Firat et al. (2016) which combines a shar ed attention mechanism and multi-\nple encoder-decoder layers. Regardless, the simple approa ch proposed in Johnson et al. (2016) and\nHa et al. (2016) remains outstandingly effective: it relies on single “universal” encoder, decoder and\nattention modules, and manages multilinguality by introdu cing an artiﬁcial token at the beginning of the\ninput sentence to specify the requested target language.\nThe current NMT state-of-the-art includes the use of recurr ent neural networks, ini-\ntially introduced in Sutskever et al. (2014; Cho et al. (2014 ), convolutional neural networks, pro-\nposed by Gehring et al. (2017), and so-called transformer ne ural networks, recently proposed\nby V aswani et al. (2017). All of them implement an encoder-de coder architecture, suitable for sequence-\nto-sequence tasks like machine translation, and an attenti on mechanism (Bahdanau et al., 2014).\nBesides speciﬁc studies focusing on new architectures and m odules, like Luong et al. (2015b) that\nempirically evaluates different implementations of the at tention mechanism, the comprehension of\nwhat a model can learn and the errors it makes has been drawing much attention of the research\nThis work is licensed under a Creative Commons Attribution 4 .0 International License. License details:\nhttp://creativecommons.org/licenses/by/4.0/\ncommunity , as evidenced by the number of recent publication s aiming at comparing the behav-\nior of neural vs. phrase-based systems (Bentivogli et al., 2 016; T oral and S ´ anchez-Cartagena, 2017;\nBentivogli et al., 2018). However, understanding the capab ility of multilingual NMT models in general\nand zero-shot translation, in particular, has not been thor oughly analyzed yet. By taking the bilingual\nmodel as the reference, this work quantitatively analyzes t he translation outputs of multilingual and\nzero-shot models, aiming at answering the following resear ch questions:\n• How do bilingual, multilingual, and zero-shot systems comp are in terms of general translation qual-\nity? Is there any translation aspect better modeled by each s peciﬁc system?\n• How do Recurrent and Transformer architectures compare in t erms of general translation quality?\nIs there any translation aspect better modeled by each speci ﬁc system?\n• What is the impact of using related languages data in trainin g a zero-shot translation system for a\ngiven language pair?\nT o address these questions, we exploit the data collected in the IWSL T 2017 MT evaluation cam-\npaign (Cettolo et al., 2017) and made publicly available by t he organizers. The campaign was the ﬁrst\nfeaturing a multilingual shared MT task, spanning ﬁve langu ages (English, Dutch, German, Italian,\nand Romanian) and all their twenty possible translation dir ections. In addition to the ofﬁcial exter-\nnal single reference of the test sets, we can also rely on prof essional post-edits of the outputs of nine\nRomanian→ Italian and of nine Dutch → German participants’ systems. Hence, we exploit the availa bil-\nity of multiple Italian and German references to perform a th orough analysis for identifying, comparing\nand understanding the errors made by different neural syste m/architectures we are interested in; in par-\nticular, we consider pairs of both related languages (Roman ian→ Italian, Dutch → German) and unrelated\nlanguages (Romanian → German and Dutch → Italian). Furthermore, to explore the impact of using data\nfrom other related languages, French and Spanish are consid ered for training purposes as well, in par-\nticular for analyzing the behavior of zero-shot x→ Italian systems, x representing any source language\ndistant from Italian.\nIn the following sections, we begin with a brief review of rel ated work on quantitative analysis of\nMT tasks ( §2). Then, we give an overview of NMT ( §3) with a contrast between the Recurrent ( §3.1)\nand Transformer ( §3.2) approaches, and a summary on multilingual and zero-sho t translation ( §3.3).\nSection ( §4), describes the dataset and preprocessing pipeline ( §4.1), qualitative evaluation data ( §4.2),\nexperimental setting ( §4.3), models ( §4.4) and the evaluation methods ( §4.5). In Section ( §5), we analyze\nthe overall translation quality for related and unrelated l anguage directions. Before the summary and\nconclusion, we will focus on lexical, morphological and wor d-order error types for the ﬁne-grained\nanalysis ( §6).\n2 Related W ork\nRecent trends in NMT evaluation show that post-editing help s to identify and address the weakness\nof systems (Bentivogli et al., 2018). Furthermore, the use o f multiple post-edits in addition to the\nmanual reference is gaining more and more ground (Bentivogl i et al., 2016; Koehn and Knowles, 2017;\nT oral and S ´ anchez-Cartagena, 2017; Bentivogli et al., 201 8). For our investigation, we follow the error\nanalysis approach deﬁned in Bentivogli et al. (2018), where multiple post-edits are exploited in order\nto quantify morphological, lexical, and word order errors, a simpliﬁed error classiﬁcation with respect\nto that proposed in Vilar et al. (2006), which settles two add itional classes, namely missing and extra\nwords.\nThe ﬁrst work that compares bilingual, multilingual, and ze ro-shot systems comes from the IWSL T\n2017 evaluation campaign (Cettolo et al., 2017). The authors ana lyze the outputs of several systems\nthrough two human evaluation methods: direct assessment wh ich focuses on the generic assessment of\noverall translation quality , and post-editing which direc tly measures the utility of a given MT output to\ntranslators. Post-edits are also exploited to run a ﬁne-gra ined analysis of errors made by the systems. The\nmain ﬁndings are that (i) a single multilingual system is an e ffective alternative to a bunch of bilingual\nsystems, and that (ii) zero-shot translation is a viable sol ution even in low-resource settings. Motivated by\nthose outcomes, in this work we explore in more detail the pra ctical feasibility of multilingual and zero-\nshot approaches. In particular, we explore the beneﬁt of add ing training data involving related languages\nin a zero-shot setting and, in that framework, we compare the behavior of state-of-the-art Transformer\nand Recurrent NMT models.\n3 Neural Machine T ranslation\nA standard state-of-the-art NMT system comprises of an enco der, a decoder and an attention mecha-\nnism, which are all trained with maximum likelihood in an end -to-end fashion (Bahdanau et al., 2014).\nAlthough there are different variants of the encoder-atten tion-decoder based approach, this\nwork focuses on the Recurrent LSTM-based variant (Sutskeve r et al., 2014) and the Transformer\nmodel (V aswani et al., 2017).\nIn both the Recurrent and Transformer approaches, the encod er is purposed to cipher a source sentence\ninto hidden state vectors, whereas the decoder uses the last representation of the encoder to predict\nsymbols in the target language. In a broad sense, the attenti on mechanism improves the prediction\nprocess by deciding which portion of the source sentence to e mphasize at a time (Luong et al., 2015b).\nIn the following two subsections, we brieﬂy summarize the tw o architecture types.\n3.1 Recurrent NMT\nIn this case, the source words are ﬁrst mapped to vectors with which the encoder recurrent network is fed.\nWhen the <eos> (i.e. end of sentence) symbol is seen, the ﬁnal time step init ializes the decoder recurrent\nnetwork. At each time step of the decoding, the attention mec hanism is applied over the encoder hidden\nstates and combined with the current hidden state of the deco der to predict the next target word. Then,\nthe prediction is fed back to the decoder (i.e. input feeding ), to predict the next word, until the <eos>\nsymbol is generated (Sutskever et al., 2014; Cho et al., 2014 ).\n3.2 T ransformer NMT\nThe Transformer architecture works by relying on a self-att ention ( intra-attention) mechanism, remov-\ning all the recurrent operations that are found in the previo us approach.In other words, the attention\nmechanism is repurposed to compute the latent space represe ntation of both the encoder and the decoder\nsides. However, with the absence of recurrence, positional-encoding is added to the input and output\nembeddings. Similarly , as the time-step in a recurrent netw ork, the positional information provides the\nTransformer network with the order of input and output seque nces. In our work, we use the absolute\npositional encoding, but very recently the use of the relati ve positional information has been shown to\nimprove performance (Shaw et al., 2018). The model is organi zed as a stack of encoder-decoder net-\nworks that works in an auto-regressive way , using the previo usly generated symbol as input for the next\nprediction. Both the decoder and encoder can be composed of u niform layers, each built of two sub-\nlayers, i.e., a multi-head self-attention layer and a posit ion wise feed-forward network (FFN) layer. The\nmulti-head sub-layer enables the use of multiple attention functions with a similar cost of utilizing at-\ntention, while the FFN sub-layer is a fully connected networ k used to process the attention sublayers; as\nsuch, FFN applies two linear transformations on each positi on and a ReLU (V aswani et al., 2017).\n3.3 Multilingual NMT\nRecent efforts in multilingual NMT using a single encoder-d ecoder and an attention mecha-\nnism (Johnson et al., 2016; Ha et al., 2016) have shown to impr ove translation performance with minimal\ncomplexity . Multilingual NMT models can be trained with par allel corpora of several language pairs in\nmany-to-one, one-to-many , or many-to-many translation directions. The main idea that distinguishes\nmultilingual NMT training and inference from a single langu age pair NMT is that in preprocessing, a\nlanguage-ﬂag is appended to the source side of each segment pair. The ﬂag sp eciﬁes the target language\nthe source is paired with at training time. Moreover, it enab les a zero-shot inference by directing the\ntranslation to a target language never seen at training time paired with the source. In addition to reducing\nencoder-decoder embedding hidden encoder decoder batch\ntype size units depth depth size\nRecurrent LSTM 512 1024 4 4 128 seg\nTransformer Self-Attention 512 512 6 6 2048 tok\nT able 1: Hyper-parameters used to train Recurrent and Trans former models, unless differently speciﬁed.\ntraining and maintenance complexity of several single lang uage pair systems, the two main advantages\nof multilingual NMT is the performance gain for low-resourc e languages, and the possibility to perform\na zero-shot translation.\nHowever, the translations generated by multilingual and ze ro-shot systems have not been investigated\nin detail yet. This includes analyzing how the model behaves solely relying on a “language-ﬂag” as a\nway to redirect the inference. Recent works have shown that t he target language-ﬂag is weaker in a low-\nresource language setting (Lakew et al., 2017). Thus, in add ition to analyzing the behavior of bilingual\nand multilingual models, mainly , the zero-shot task requir es a careful investigation.\n4 Data and Experiments\n4.1 Datasets and preprocessing\nThe experimental setting comprises seven languages; for ea ch language pair, we use the ≈ 200,000 paral-\nlel sentences made publicly available by the IWSL T 2017 eval uation campaign (Cettolo et al., 2017),\npartitioned in training, development, and test sets. In the preprocessing pipeline, the raw data is\nﬁrst tokenized and cleaned by removing empty lines. Then, a s hared byte pair encoding (BPE)\nmodel (Sennrich et al., 2015) is trained using the union of th e source and target sides of the train-\ning data. The number of BPE segmentation rules is set to 8,000, following the suggestion of\nDenkowski and Neubig (2017) for experiments in small traini ng data condition. For the case of Trans-\nformer training, the internal sub-word segmentation (Wu et al., 2016) provided by the T ensor2T ensor\nlibrary1 is used. Note that prepending the “language-ﬂag” on the sour ce side of the corpus is speciﬁc to\nthe multilingual and zero-shot models.\n4.2 Evaluation data\nFor our investigation, we exploit the nine post-edits avail able from the IWSL T 2017 evaluation cam-\npaign. Post-editing regarded the bilingual, multilingual , and zero-shot runs of three different participants\nto the two tasks Dutch (Nl) → German (De) and Romanian (Ro) → Italian (It). Human evaluation was\nperformed on a subset ( 603 sentences) of the nine runs, involving professional transl ators. Details on\ndata preparation and the post-editing task can be found in Ce ttolo et al. (2017).\nThe translation directions we consider in this work are Nl/R o→ De and Nl/Ro → It. The choice of\nGerman and Italian as the target languages is motivated by (i) the availability of multiple post-edits for\nthe ﬁne-grained analysis and (ii) the possibility of varyin g the linguistic distance between the source\nand the target languages, allowing experimental conﬁgurat ions suitable to answer the research questions\nraised in Section §1.\nAs said, for Nl → De and Ro → It the human evaluation sets consist of 603 segments. Since post-editing\ninvolved only those two language pairs, for the other two dir ections considered in this work, namely\nNl→ It and Ro → De, we tried to exploit at best the available post-edits by lo oking for all and only the\nsegment pairs of the Nl → It and Ro → De tst 2017 sets for which the target side exactly matches (at least)\none of the segment pairs of the Ro → It and Nl → De human evaluation sets. This way , we were able to\nﬁnd 478 matches on the Italian sides and 444 on the German sides, which therefore become the sizes\nof the human evaluation sets of Ro → De and Nl → It, respectively , for which we can re-use the available\npost-edits.\n1 https://github.com/tensorﬂow/tensor2tensor\nModel #Directions System description\nNMT 1 F our pure bilingual models for the Nl → De/It and Ro → De/It directions\nM-NMT 20 Multilingual, trained on all directions in the set {En,De,Nl,It,Ro}\nZST 16 Zero-shot, trained as multilingual but removing also Nl ↔ De and It ↔ Ro data\nZST A 12 Zero-shot, trained as ZST but removing also De ↔ Ro and Nl ↔ It data\nZST B 16 Zero-shot, trained as ZST A but adding En ↔ Fr/Es data\nT able 2: The training setting of 4*bilingual, 1*multilingual, and 3*zero-shot systems.\nIt is worth to note that in general, the post-edits from the ev aluation campaign are not actual post-edits\nof MT outputs generated in our experiments, with some except ions discussed later, therefore they should\nrather be considered as multiple external references.\n4.3 T raining setting\nEach of the three system types, namely bilingual, multiling ual and zero-shot, is trained using both Recur-\nrent and Transformer architectures, with the proper traini ng data provided in the IWSL T 2017 evaluation\ncampaign. Meta training parameters were set in a preliminar y stage with the aim of maximizing the\nquality of each approach. Recurrent NMT experiments are car ried out using the open source OpenNMT -\npy2 (Klein et al., 2017), whereas the Transformer models are tra ined using the T ensor2T ensor toolkit.\nHence, we took the precaution of selecting the optimal train ing and inference parameters for both ap-\nproaches and toolkits. For instance, for our low-resource s etting characterized by a high data sparsity ,\nthe dropout (Srivastava et al., 2014) is set to 0.3 (Gal and Ghahramani, 2016) in Recurrent models and\nto 0.2 in Transformer models to prevent over-ﬁtting. Similarly , A dam (Kingma and Ba, 2014) optimizer\nwith an initial learning rate of either 0.001 (RNN) or 0.2 (Transformer) is used. If the perplexity does\nnot decrease on the validation set or the number of epochs is a bove 7, a learning rate decay of 0.7 is\napplied in the Recurrent case. For the Transformer case, the learning rate is increased linearly in the\nearly stages ( warmup\ntraining steps=16000); after that, it is decreased with an inverse square root of\ntraining step (V aswani et al., 2017). T able 1 summarizes the list of hyper-parameters.\n4.4 Models\nT o address the research questions listed in Section 1, we tra in ﬁve types of models using either the\nRecurrent or the Transformer approaches. All models are tra ined up to convergence, eventually the\nbest performing checkpoint on the dev set is selected. T able 2 summarizes the systems tested in our\nexperiments. As references, we consider four bilingual sys tems (in short NMT) trained on the following\ndirections: Nl → De/It and Ro → De/It. The ﬁrst term of comparison is a many-to-many multili ngual\nsystem (in short M-NMT) trained in all directions in the set {En,De,Nl,It,Ro}. Then, we test zero-shot\ntranslation (ZST) between related languages, namely Nl → De and Ro → It, by training a multilingual\nNMT without any data for these language pairs. W e also test ze ro-shot translation between unrelated\nlanguages (ZST\nA), namely Ro → De and Nl → It, by excluding parallel data between these languages.\nFinally , for the same unrelated zero-shot directions we als o train multi-lingual systems (ZST B) that\ninclude data related to Romanian and Italian, namely En ↔ Fr/Es.\n4.5 Evaluation methods\nSystems are compared in terms of BLEU (Papineni et al., 2002) (as implemented in multi-bleu.perl 3)\nand TER (Snover et al., 2006) scores, on the single reference s of the ofﬁcial IWSL T test sets.\nIn addition, two TER-based scores are reported, namely the m ultiple-reference TER (mTER) and a\nlemma-based TER (lmmTER), which are instead computed on the nine post-edits of the IWSL T 2017\nhuman evaluation set. In mTER, TER is computed by counting, f or each segment of the MT output,\nthe minimum number of edits across all the references and div iding by the average length of references.\n2 https://github.com/OpenNMT/OpenNMT -py\n3 A script from the Moses SMT toolkithttp://www .statmt.org/ moses\nDirection System Recurrent Transformer\nBLEU TER mTER lmmTER BLEU TER mTER lmmTER\nNl→ De\nNMT 18.05 64.61 23.70 20.60 18.37 63.74 27.95 23.86\nM-NMT 17.79 66.18 21.75 18.28 ↑ 19.95 61.90 23.62 20.05\nZST 17.06 65.73 26.35 22.29 ↑ 19.13 62.69 25.19 21.53\nRo→ It\nNMT 22.16 59.35 22.99 20.39 22.48 57.34 26.60 23.36\nM-NMT 21.69 59.50 21.12 18.46 ↑ 22.12 57.51 25.05 21.57\nZST 18.72 62.08 29.66 26.15 ↑ 21.29 59.08 26.93 23.33\nT able 3: Automatic scores on tasks involving related langua ges. BLEU and TER are computed on\ntest2017, while mTER and lmmTER are reported for human evaluation set s. Best scores of the Trans-\nformer model against the Recurrent are highlighted in bold, whereas arrow ↑ indicates statistically sig-\nniﬁcant differences ( p <0.05).\nlmmTER is computed similarly to mTER but looking for matches at the lemma level instead of surface\nforms. Signiﬁcance tests for all scores are reported using M ulteval (Clark et al., 2011) tool.\nSystems are also compared in terms of three well known and wid ely used error categories, that is\nlexical, morphological, and word order errors, exploiting TER and post-edits as follows. First, the\nMT outputs and the corresponding post-edits are lemmatized and POS-tagged; for that, we used\nParZu (Sennrich et al., 2013) for German and TreeT agger (Sch mid, 1994) for Italian. Then, the lem-\nmatized outputs are evaluated against the corresponding po st-edits via a variant of the tercom implemen-\ntation4 of TER: in addition to computing TER, the tool provides compl ete information about matching\nlemmas, as well as shift (matches after displacements), ins ertion, deletion, and substitution operations.\nSince for each lemma the tool keeps track of the correspondin g original word form and POS tag, we are\nable to measure the number of errors falling in the three erro r categories, following the scheme described\nin detail in Bentivogli et al. (2018).\n5 T ranslation Analysis\n5.1 Related languages\nFirst, we compare the bilingual (NMT), multilingual (M-NMT ), and zero-shot (ZST) systems on the\ntwo tasks Nl→ De and Ro→ It, implemented as either Recurrent or Transformer networks, in terms of\nautomatic metrics. As stated above, BLEU and TER exploit the ofﬁcial external reference of the whole\ntest sets, while mTER and lmmTER are utilize the multiple pos t-edits of the (smaller) IWSL T human\nevaluation test set. Scores are given in T able 3.\nLooking at the BLEU/TER scores, it is evident that Transform er performs better in all the three model\nvariants. In particular, for the multilingual and the zero- shot models, the gain is statistically signiﬁcant.\nOn the contrary , the mTER and lmmTER scores are better for the Recurrent architecture; in this case,\nthe outcome is misleading since the nine post-edits include those generated by correcting the outputs of\nthe three Recurrent systems. As such, the translations of th e Recurrent systems are rewarded over the\ntranslations produced by the Transformer systems, thus mak ing the comparison not fair.\nAs far as the models are compared, the bilingual one is the bes t in three out of four cases, the exception\nbeing the Transformer/Nl → De. Nonetheless, it is worth to note the good performance of t he multilin-\ngual model in terms of mTER and lmmTER. This result holds true in both Recurrent and Transformer\napproaches, regardless of the BLEU score. W e hypothesize th at the main reason behind this is the higher\nnumber of linguistic phenomena observed in training, thank s to the use of data from multiple languages,\nwhich makes the multilingual models more robust than the bilingual models.\n5.2 Unrelated languages\nIn unrelated language directions, our experimental settin g is aimed at evaluating the impact of source\nlanguage-relatedness with the target. Particularly , we focus on the zero-shot set up given its intrinsic\n4 A vailable at wit3.fbk.eu/show.php?release=2016-02&page=subjeval\nDirection System Recurrent Transformer\nBLEU TER mTER lmmTER BLEU TER mTER lmmTER\nRo→ De\nNMT 13.99 72.70 61.82 54.61 ↑ 16.52 66.71 55.68 48.44\nZST A 14.93 69.38 58.26 51.08 ↑ 16.46 66.88 54.72 48.25\nZST B 14.75 69.29 58.26 51.37 ↑ 16.55 67.18 55.29 48.03\nNl→ It\nNMT 18.88 63.79 58.79 52.16 ↑ 20.22 60.88 55.52 48.56\nZST A 18.77 62.97 58.80 51.32 ↑ 19.80 60.24 54.06 47.16\nZST B 18.87 62.40 57.34 50.17 ↑ 20.61 59.41 53.04 46.17\nT able 4: Evaluation results for the unrelated language dire ctions. BLEU and TER scores are computed\nwith single references, while mTER and lmmTER are computed w ith nine post-edits. Best scores of\nthe Transformer over the corresponding Recurrent architec tures are highlighted in bold, whereas arrow ↑\nindicates statistically signiﬁcant differences ( p <0.05).\ndifﬁculty , by taking the bilingual systems as references. T able 4 provides BLEU and TER based scores\nfor the Ro → De and Nl → It directions.\nConcerning the ZST A training condition, in one case (Recurrent Ro → De) it outstandingly allows to\noutperform the pure bilingual system, while in the other cas es there is no signiﬁcant difference between\nZST A and NMT , proving once again that zero-shot translation bui lt on the “language-ﬂag” of M-NMT\nis really effective (Johnson et al., 2016): in fact, at most a slight performance degradation is recorded as\nthe number of pairs used in training decreases (Lakew et al., 2017). Although gains are rather limited,\nadding training data involving Romance target languages (F rench and Spanish, ZST B) close to Italian\nimpacts as hoped: ZST B scores are in general better than both NMT and ZST A in Nl → It, while they\ndo not degrade with respect to ZST A in Ro → De.\nSimilarly to what is observed for related pairs (T able 3), th e Transformer architecture shows deﬁnitely\nhigher quality than the RNN one, conﬁrming the capability of the approach to infer unseen directions.\nThe overall outcomes from T ables 3 and 4 are: (i) multilingua l systems have the potential to effectively\nmodel the translation either in zero-shot or non zero-shot c onditions; (ii) zero-shot translation is a vi-\nable option to enable translation without training samples ; (iii) the Transformer is the best performing\napproach, particularly in the zero-shot directions.\nThe next section is devoted to a ﬁne-grained analysis of erro rs made by the various systems at hand, with\nthe aim of assessing the outcomes based on automatic metrics .\n6 Fine-grained Analysis\nFollowing the error classiﬁcation deﬁned in Section 4.5, no w we focus on lexical, morphological, and\nreordering error distributions to characterize the behavi or of the three types of models and the two\nsequence-to-sequence learning approaches considered in t his work.\nAs anticipated in the previous section, it is expected that s cores computed with reference to post-edits\npenalize Transformer over Recurrent systems because the ou tputs of the latter were post-edited, but\nnot those of the former. W e try to mitigate this bias by relyin g on the availability of multiple post-edits\nwhich likely allows to better match the Transformer runs tha n having a single reference would do. For the\nﬁne-grained analysis, we use instead the expedient of compu ting error distributions that are normalized\nwith respect to the error counts observed in a bilingual refe rence system. In the next two sections, the\nﬁne-grained analysis is reported for related and unrelated languages pairs, consecutively .\n6.1 Related languages\nT able 5 provides the distribution over the error types by the bilingual (NMT), multi-lingual (M-NMT),\nand zero-shot (ZST) models, implemented either with Recurr ent or Transformer architectures, for the\nNl→ De translation direction. W e also report, for each error typ e and M-NMT and ZST system, the\nobserved relative difference of errors with respect to the b ilingual reference model (NMT).\nConsidering each error category , we observe the same genera l trend for all systems: the lexical errors\nNl→ De Recurrent T ransformer\nNMT M-NMT ∆ NMT ZST ∆ NMT NMT M-NMT ∆ NMT ZST ∆ NMT\nLexical 77.29 69.65 -7.64 83.73 +6.44 76.47 64.83 -11.64 69.53 -6.94\nMorph 15.41 16.51 +1.10 19.1 +3.69 15.70 13.96 -1.74 14.13 -1.57\nReordering 5.53 3.14 -2.39 5.41 -0.12 6.20 4.97 -1.23 5.41 -0.79\nMorph. & Reo. 1.76 1.02 -0.74 1.61 -0.15 1.63 1.36 -0.27 1.53 -0.10\nT otal 100 90.31 -9.69 109.84 +9.84 100 85.12 -14.88 90.6 -9.40\nT able 5: Distribution of lexical, morphological, and reord ering error types from the two MT approaches.\nReported values are normalized with respect to the total err or count of the respective bilingual reference\nmodel (NMT). ∆ NMT are variations with respect to the bilingual reference mode ls (NMT).\nRo→ It Recurrent Transformer\nNMT M-NMT ∆ NMT ZST ∆ NMT NMT M-NMT ∆ NMT ZST ∆ NMT\nLexical 80.63 73.81 -6.82 102.79 +22.16 81.97 76.01 -5.96 84.12 +2.15\nMorph 12.33 12.86 +0.53 16.00 +3.67 11.49 11.79 +0.30 12.44 +0.95\nReordering 5.74 3.71 -2.03 6.09 +0.35 5.35 4.64 -0.71 4.81 -0.54\nMorph. & Reo. 1.30 1.15 -0.15 2.18 +0.88 1.19 1.09 -0.10 1.09 -0.10\nT otal 100 91.54 -8.46 127.07 +27.07 100 93.52 -6.48 102.45 +2.45\nT able 6: Distribution of the error types in the Ro → It direction for the Recurrent and Transformer ap-\nproaches. From the variation of errors that compare M-NMT an d ZST models with the bilingual refer-\nence (NMT), a larger margin of error is observed in case of Tra nsformer ZST model.\nRo→ It Recurrent Transformer\nNMT ZST A ∆ NMT ZST B ∆ NMT NMT ZST A ∆ NMT ZST B ∆ NMT\nLexical 80.63 108.27 +27.64 100.31 +19.68 81.97 82.11 +0.14 76.76 -5.21\nMorph 12.33 17.11 +4.78 17.23 +4.90 11.49 13.09 +1.60 11.59 +0.10\nReordering 5.74 6.20 +0.46 6.16 +0.42 5.35 5.18 -0.17 5.59 +0.24\nMorph. & Reo. 1.30 2.22 +0.92 2.30 +1.00 1.19 1.16 -0.03 1.02 -0.17\nT otal 100 133.81 +33.81 126 +26.00 100 101.53 +1.53 94.96 -5.04\nT able 7: Error distribution of ZST A and ZST B models for the Recurrent and Transformer variants.\nTransformer achieves the highest error reduction, particu larly in the ZST B model setting.\nrepresent by far the most frequent category (76-77%), follo wed by morphology (15- 16%) and reordering\n(3-6%) errors; cases of words whose morphology and position ing are both wrong, represent about 1-2%\nof the total errors. Beyond the similar error distributions , it is worth to note the variation of errors made\nby M-NMT and ZST models with respect to those of the NMT model: for the Recurrent architecture,\nthere is a decrease of 9.69 and an increase of 9.84 points, respectively . On the contrary , the Trans-\nformer architecture yields improvements for both models: t otal errors reduce by 14.88 and 9.40 points,\nrespectively . The result for the Transformer ZST system is p articularly valuable since the average error\nreduction comes from remarkable improvements across all er ror categories.\nFor the Ro → It direction, results are given in T able 6. Although to a diff erent extent, we observe a\npicture similar to that of Nl → De discussed above: lexical errors is the type of error commi tted to a\ngreater extent, multilingual models outperform their bili ngual correspondents (more for the Recurrent\nthan for the Transformer models), and ZST is competitive wit h bilingual NMT only if the Transformer\narchitecture is adopted.\nTraining under the zero-shot conditions ZST\nA and ZST B assume less training data available and per-\nmit to measure the impact of introducing additional paralle l data from related languages. W e considered\ntraining conditions ZST A and ZST B here to perform Ro → It zero shot translation and report the out-\ncomes in T able 7.\nRo→ De Recurrent Transformer\nNMT ZST A ∆ NMT ZST B ∆ NMT NMT ZST A ∆ NMT ZST B ∆ NMT\nLexical 79.18 74.42 -4.76 74.09 -5.09 79.21 79.11 -0.10 78.52 -0.69\nMorph 9.91 10.35 +0.44 10.07 0.16 9.92 10.05 +0.13 10.87 +0.95\nReordering 7.33 6.16 -1.17 6.16 -1.17 7.19 6.88 -0.31 7.22 +0.03\nMorph. & Reo. 3.58 3.47 -0.11 3.47 -0.11 3.68 3.52 -0.16 3.60 -0.08\nT otal 100 94.4 -5.60 93.79 -6.21 100 99.55 -0.45 100.21 +0.21\nT able 8: Error distribution of the bilingual (NMT), ZST A and ZST B model runs for the unrelated\nRo→ De direction. The Transformer moder shows the smallest sens itivity to the change in the number of\ntraining language pairs.\nResults show error counts for each condition normalized wit h respect to the corresponding bilingual\nreference models (NMT). The most interesting aspect comes f rom the fact that global variations in the\nnormalized error counts of the zero-shot translation can be here associated with the relatedness and va-\nriety of languages in the training data. As recently reporte d (Lakew et al., 2017), zero-shot performance\nof Recurrent models in a low resource setting seems highly as sociated with the number of languages\nprovided in the training data. This is also conﬁrmed by compa ring performance of Recurrent models\nacross the ZST (T able 6), ZST\nA and ZST B conditions. In particular, variations from the bilingual ref-\nerence model, show signiﬁcant degradation when some langua ge directions are removed (from +27.07 to\n+33.81) and a signiﬁcant improvement when two related langu ages are added (from +33.81 to +26.00).\nRemarkably , the Transformer zero-shot model seems less sen sitive to the removal or addition of lan-\nguages: actually a slight improvement is observed after rem oving Nl → It and De → Ro (ZST A), i.e.,\nfrom +2.45 to +1.53, followed by a large improvement when En → Fr/Es (ZST B) are added, i.e. from\n+1.53 to -5.04. Notice that the latter results outperform th e bilingual model. Overall, across all experi-\nments, we see slight changes in the distribution of errors ty pes. On the other hand, increases or drops of\nspeciﬁc error types with respect to the bilingual reference model show sharper differences across the dif-\nferent conditions. For instance, the best performing Trans former model (ZST B in T able 7) seems to gain\nover the reference bilingual systems only in terms of lexica l errors (-5.21). The zero-shot Transformer\nmodel trained under the ZST condition (T able 6) although glo bally worse than the bilingual reference,\nseems instead slightly better than the reference concernin g reordering error (-0.54), which account for\n5.35% of the total number of errors.\n6.2 Unrelated languages\nIn our second scenario, we evaluate the relative changes in t he error distribution for the unrelated lan-\nguage directions (Ro → De and Nl → It). This section complements the translation results repo rted in\nT able 4, analyzing the runs from the ZST\nA and ZST B models in a different manner.\nIn the Ro → De unrelated direction (T able 8), the Recurrent model shows a reduction in the error rate\nof 5.60 points (ZST A) and 6.21 points (ZST B) with respect to the bilingual (NMT) reference model,\nwhile for the Transformer no signiﬁcant differences are obs erved. These results conﬁrm what observed\nin the automatic evaluation on the reference translations ( T able 4). The gain observed by the Recurrent\nmodel on the ZST B condition is mainly on lexical (-4.76 points) and reorderi ng errors (-1.17 points) is\nprobably due to the poor performance of its bilingual counte rpart.\nAs far as the the Nl → It unrelated direction (T able 9) is concerned, both Recurre nt and Transformer\nZST models show to reduce the error counts over the bilingual reference model. Actually , a similar\ntrend occurs in Ro → De (T able 8), but with a relatively higher error reduction in case of the Trans-\nformer model. In particular, the Transformer model shows re ductions of − 2.29 points for ZST A and\n− 4.53% for ZST B, whereas for the Recurrent model the improvements are slig htly lower, namely − 1.11\n(ZST A) and − 4.14 (ZST B) points. Remarkably , both the Recurrent and Transformer m odels beneﬁt\nfrom additional training data related to Italian (compare Z ST A and ZST B).\nIn conclusion, we observe that error counts of the zero-shot models in unrelated directions can increase\nNl→ It Recurrent Transformer\nNMT ZST A ∆ NMT ZST B ∆ NMT NMT ZST A ∆ NMT ZST B ∆ NMT\nLexical 81.08 80.7 -0.38 78.79 -2.29 81.15 79.36 -1.79 77.48 -3.67\nMorph 8.47 9.03 +0.56 8.38 -0.09 9.01 9.2 +0.19 9.03 +0.02\nReordering 7.78 6.63 -1.15 6.32 -1.46 7.51 6.74 -0.77 6.51 -1.00\nMorph & Reo 2.67 2.54 -0.13 2.38 -0.29 2.34 2.41 +0.07 2.45 +0.11\nT otal 100 98.89 -1.11 95.86 -4.14 100 97.71 -2.29 95.47 -4.53\nT able 9: Error distribution of the bilingual (NMT), ZST A and ZST B model runs. ∆ NMT shows the\nrelative change in the error distribution of the zero-shot m odels with respect to the bilingual reference\nmodels.\n(T able 8) when compared to the bilingual model. However, in t he related language direction the most\ninteresting aspect is observed with the discount of error in the Nl → It direction (T able 9). In particular,\nthe ZST\nB zero-shot model showed >2.0% error reduction over the ZST A model. This gain is directly\nrelated to the newly introduced training data (i.e., Englis h↔ French/Spanish) in case of ZST B.\nSummary and Conclusions\nIn this work, we showed how bilingual, multilingual, and zer o-shot models perform in terms of overall\ntranslation quality , as well as the errors types produced by each system. Our analysis compared Recurrent\nmodels with the recently introduced Transformer architect ure. Furthermore, we explored the impact of\ngrouping related languages for a zero-shot translation tas k. In order to make the overall evaluation\nmore sound, BLEU and TER scores were complemented with mTER a nd lmmTER, leveraging multiple\nprofessional post-edits. Our investigation on the transla tion quality and the results of the ﬁne-grained\nanalysis shows that:\n• Multilingual models consistently outperform bilingual mo dels with respect to all considered error\ntypes, i.e., lexical, morphological, and reordering.\n• The Transformer approach delivers the best performing mult ilingual models, with a larger gain over\ncorresponding bilingual models than observed with RNNs.\n• Multilingual models between related languages achieve the best performance scores and relative\ngains over corresponding bilingual models.\n• When comparing zero-shot and bilingual models, relatednes s of the source and target languages\ndoes not play a crucial role.\n• The Transformer model delivers the best quality in all consi dered zero-shot condition and translation\ndirections.\nOur ﬁne-grained analysis looking at three types of errors (l exical, reordering, morphology) show sig-\nniﬁcant differences in the error distributions across the d ifferent translation directions, even when switch-\ning the source language with another source language of the s ame family . No particular differences in\nthe error distributions were observed across neural MT arch itectures (Recurrent vs. Transformer), while\nsome marked differences were observed when comparing bilin gual, multilingual, and zero-shot systems.\nA more in-depth analysis of these differences will be carrie d out in future work.\nAcknowledgements\nThis work has been partially supported by the EC-funded proj ects ModernMT (H2020 grant agreement\nno. 645487) and QT21 (H2020 grant agreement no. 645452). W e a lso gratefully acknowledge the\nsupport of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.\nReferences\n[Bahdanau et al.2014] Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio. 2014. Neural machine translation\nby jointly learning to align and translate. arXiv preprint arXiv:1409.0473 .\n[Bentivogli et al.2016] Luisa Bentivogli, Arianna Bisazza , Mauro Cettolo, and Marcello Federico. 2016. Neural\nversus phrase-based machine translation quality: a case st udy. arXiv preprint arXiv:1608.04631 .\n[Bentivogli et al.2018] Luisa Bentivogli, Arianna Bisazza , Mauro Cettolo, and Marcello Federico. 2018. Neural\nversus phrase-based mt quality: An in-depth analysis on eng lish–german and english–french. Computer Speech\n& Language , 49:52–70.\n[Bojar et al.2017] Ondˇ rej Bojar, Rajen Chatterjee, Christ ian Federmann, Yvette Graham, Barry Haddow , Shujian\nHuang, Matthias Huck, Philipp Koehn, Qun Liu, V arvara Logac heva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findi ngs of the 2017 conference on machine trans-\nlation (wmt17). In Proceedings of the Second Conference on Machine T ranslatio n, V olume 2: Shared T ask\nP apers, pages 169–214, Copenhagen, Denmark, September. Associat ion for Computational Linguistics.\n[Cettolo et al.2017] Mauro Cettolo, Marcello Federico, Lui sa Bentivogli, Jan Niehues, Sebastian St ¨ uker, Katsuhito\nSudoh, Koichiro Y oshino, and Christian Federmann. 2017. Ov erview of the IWSL T 2017 Evaluation Campaign.\nIn Proceedings of the 14th International W orkshop on Spoken La nguage T ranslation (IWSLT) , T okyo, Japan.\n[Cho et al.2014] Kyunghyun Cho, Bart V an Merri¨ enboer, Cagl ar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,\nHolger Schwenk, and Y oshua Bengio. 2014. Learning phrase re presentations using rnn encoder-decoder for\nstatistical machine translation. arXiv preprint arXiv:1406.1078 .\n[Clark et al.2011] Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A Smith. 2011. Better hypothesis testing\nfor statistical machine translation: Controlling for opti mizer instability. In Proceedings of the 49th Annual Meet-\ning of the Association for Computational Linguistics: Huma n Language T echnologies: short papers-V olume 2 ,\npages 176–181. Association for Computational Linguistics .\n[Denkowski and Neubig2017] Michael Denkowski and Graham Ne ubig. 2017. Stronger baselines for trustable\nresults in neural machine translation. arXiv preprint arXiv:1706.09733 .\n[Dong et al.2015] Daxiang Dong, Hua Wu, W ei He, Dianhai Y u, an d Haifeng W ang. 2015. Multi-task learning for\nmultiple language translation. In ACL (1) , pages 1723–1732.\n[Firat et al.2016] Orhan Firat, Kyunghyun Cho, and Y oshua Be ngio. 2016. Multi-way, multilingual neural machine\ntranslation with a shared attention mechanism. arXiv preprint arXiv:1601.01073 .\n[Gal and Ghahramani2016] Y arin Gal and Zoubin Ghahramani. 2 016. A theoretically grounded application of\ndropout in recurrent neural networks. In Advances in neural information processing systems , pages 1019–1027.\n[Gehring et al.2017] Jonas Gehring, Michael Auli, David Gra ngier, Denis Y arats, and Y ann N Dauphin. 2017.\nConvolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122 .\n[Ha et al.2016] Thanh-Le Ha, Jan Niehues, and Alexander W aib el. 2016. T oward multilingual neural machine\ntranslation with universal encoder and decoder. arXiv preprint arXiv:1611.04798 .\n[Johnson et al.2016] Melvin Johnson, Mike Schuster, Quoc V L e, Maxim Krikun, Y onghui Wu, Zhifeng Chen,\nNikhil Thorat, Fernanda V i´ egas, Martin W attenberg, Greg C orrado, et al. 2016. Google’s multilingual neural\nmachine translation system: Enabling zero-shot translati on. arXiv preprint arXiv:1611.04558 .\n[Kingma and Ba2014] Diederik Kingma and Jimmy Ba. 2014. Adam : A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980 .\n[Klein et al.2017] Guillaume Klein, Y oon Kim, Y untian Deng, Jean Senellart, and Alexander M Rush. 2017. Open-\nnmt: Open-source toolkit for neural machine translation. arXiv preprint arXiv:1701.02810 .\n[Koehn and Knowles2017] Philipp Koehn and Rebecca Knowles. 2017. Six challenges for neural machine trans-\nlation. arXiv preprint arXiv:1706.03872 .\n[Lakew et al.2017] Surafel M Lakew , Mattia A Di Gangi, and Mar cello Federico. 2017. Multilingual neural\nmachine translation for low resource languages. In CLiC-it 2017 4th Italian Conference on Computational\nlinguistics.\n[Lee et al.2016] Jason Lee, Kyunghyun Cho, and Thomas Hofman n. 2016. Fully character-level neural machine\ntranslation without explicit segmentation. arXiv preprint arXiv:1610.03017 .\n[Luong et al.2015a] Minh-Thang Luong, Quoc V Le, Ilya Sutske ver, Oriol V inyals, and Lukasz Kaiser. 2015a.\nMulti-task sequence to sequence learning. arXiv preprint arXiv:1511.06114 .\n[Luong et al.2015b] Minh-Thang Luong, Hieu Pham, and Christ opher D Manning. 2015b. Effective approaches to\nattention-based neural machine translation. arXiv preprint arXiv:1508.04025 .\n[Papineni et al.2002] Kishore Papineni, Salim Roukos, T odd W ard, and W ei-Jing Zhu. 2002. Bleu: a method for\nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for\ncomputational linguistics , pages 311–318. Association for Computational Linguistic s.\n[Schmid1994] Helmut Schmid. 1994. Probabilistic Part-of- Speech T agging Using Decision Trees. In Proceedings\nof the International Conference on New Methods in Language P rocessing, pages 44–49.\n[Sennrich et al.2013] Rico Sennrich, Martin V olk, and Gerol d Schneider. 2013. Exploiting Synergies Between\nOpen Resources for German Dependency Parsing, POS-tagging , and Morphological Analysis. In Proceedings\nof Recent Advances in Natural Language Processing , number September, pages 601–609.\n[Sennrich et al.2015] Rico Sennrich, Barry Haddow , and Alex andra Birch. 2015. Neural machine translation of\nrare words with subword units. arXiv preprint arXiv:1508.07909 .\n[Shaw et al.2018] Peter Shaw , Jakob Uszkoreit, and Ashish V a swani. 2018. Self-attention with relative position\nrepresentations. arXiv preprint arXiv:1803.02155 .\n[Snover et al.2006] Matthew Snover, Bonnie Dorr, Rich Schwa rtz, Linnea Micciulla, and John Makhoul. 2006. A\nstudy of translation edit rate with targeted human annotati on. In Proceedings of the Conference of the Associa-\ntion for Machine T ranslation in the Americas (AMTA) , Boston, US-MA, August.\n[Srivastava et al.2014] Nitish Srivastava, Geoffrey E Hint on, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. 2014. Dropout: a simple way to prevent neural network s from overﬁtting. Journal of machine learning\nresearch, 15(1):1929–1958.\n[Sutskever et al.2014] Ilya Sutskever, Oriol V inyals, and Q uoc V Le. 2014. Sequence to sequence learning with\nneural networks. In Advances in neural information processing systems , pages 3104–3112.\n[T oral and S´ anchez-Cartagena2017] Antonio T oral and V´ ıc tor M S´ anchez-Cartagena. 2017. A multifaceted\nevaluation of neural versus phrase-based machine translat ion for 9 language directions. arXiv preprint\narXiv:1701.02901 .\n[V aswani et al.2017] Ashish V aswani, Noam Shazeer, Niki Par mar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information\nProcessing Systems , pages 6000–6010.\n[V ilar et al.2006] David V ilar, Jia Xu, Luis Fernando dHaro, and Hermann Ney. 2006. Error analysis of statistical\nmachine translation output. In Proceedings of LREC , pages 697–702.\n[Wu et al.2016] Y onghui Wu, Mike Schuster, Zhifeng Chen, Quo c V Le, Mohammad Norouzi, W olfgang\nMacherey, Maxim Krikun, Y uan Cao, Qin Gao, Klaus Macherey, e t al. 2016. Google’s neural machine transla-\ntion system: Bridging the gap between human and machine tran slation. arXiv preprint arXiv:1609.08144 .\n[Zoph and Knight2016] Barret Zoph and Kevin Knight. 2016. Mu lti-source neural translation. arXiv preprint\narXiv:1601.00710 .",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7616056799888611
    },
    {
      "name": "Machine translation",
      "score": 0.6762838363647461
    },
    {
      "name": "Artificial neural network",
      "score": 0.6556814312934875
    },
    {
      "name": "Computer science",
      "score": 0.6442384123802185
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45123088359832764
    },
    {
      "name": "Natural language processing",
      "score": 0.33961841464042664
    },
    {
      "name": "Speech recognition",
      "score": 0.3285921514034271
    },
    {
      "name": "Engineering",
      "score": 0.12673553824424744
    },
    {
      "name": "Voltage",
      "score": 0.11895188689231873
    },
    {
      "name": "Electrical engineering",
      "score": 0.11867547035217285
    }
  ],
  "institutions": [],
  "cited_by": 63
}