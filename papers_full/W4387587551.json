{
  "title": "ConvNeXt-ST-AFF: A Novel Skin Disease Classification Model Based on Fusion of ConvNeXt and Swin Transformer",
  "url": "https://openalex.org/W4387587551",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2224235370",
      "name": "Shengnan Hao",
      "affiliations": [
        "North China University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105945378",
      "name": "Liguo Zhang",
      "affiliations": [
        "North China University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2053669043",
      "name": "Yanyan Jiang",
      "affiliations": [
        "Hebei Agricultural University"
      ]
    },
    {
      "id": "https://openalex.org/A2277169148",
      "name": "Jingkun Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2152248008",
      "name": "Zhanlin Ji",
      "affiliations": [
        "North China University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2117608616",
      "name": "Li Zhao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A227077636",
      "name": "Ivan Ganchev",
      "affiliations": [
        "University of Limerick"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4281631732",
    "https://openalex.org/W6802648153",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W4205189720",
    "https://openalex.org/W4323060005",
    "https://openalex.org/W3150038271",
    "https://openalex.org/W2983542513",
    "https://openalex.org/W4281670481",
    "https://openalex.org/W3216577131",
    "https://openalex.org/W3207421495",
    "https://openalex.org/W4318263565",
    "https://openalex.org/W2899425762",
    "https://openalex.org/W3022989514",
    "https://openalex.org/W4200322801",
    "https://openalex.org/W4221060026",
    "https://openalex.org/W4308889883",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2592160412",
    "https://openalex.org/W3208002538",
    "https://openalex.org/W2963125010",
    "https://openalex.org/W2890655382",
    "https://openalex.org/W3029477994",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W2125581910",
    "https://openalex.org/W3006349040",
    "https://openalex.org/W2911818805",
    "https://openalex.org/W3120946080",
    "https://openalex.org/W2407336108",
    "https://openalex.org/W2171197573",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4312657777",
    "https://openalex.org/W3094897602",
    "https://openalex.org/W2565639579",
    "https://openalex.org/W4319233950",
    "https://openalex.org/W4280531133",
    "https://openalex.org/W6720275808",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W3195373382",
    "https://openalex.org/W2990040069",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4312443924",
    "https://openalex.org/W6762718338",
    "https://openalex.org/W4298001327",
    "https://openalex.org/W2914959431",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4229003689",
    "https://openalex.org/W4310255585",
    "https://openalex.org/W3012614932",
    "https://openalex.org/W1565050238",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2471801048",
    "https://openalex.org/W4286910290",
    "https://openalex.org/W1686810756"
  ],
  "abstract": "Automatic classification of dermatological images is an important technology that assists doctors in performing faster and more accurate classification of skin diseases. Recently, convolutional neural networks (CNNs) and Transformer networks have been employed in learning respectively the local and global features of lesion images. However, existing works mainly focus on utilizing a single neural network for feature extraction, which limits the model classification performance. In order to tackle this problem, a novel fusion model, named ConvNeXt-ST-AFF, is proposed in this paper, by combining the strengths of ConvNeXt and Swin Transformer (ConvNeXt-ST in the model&#x2019;s name). In the proposed model, the pretrained ConvNeXt and Swin Transformer networks extract local and global features from images, which are then fused using Attentional Feature Fusion (AFF) submodules (AFF in the model&#x2019;s name). Additionally, in order to enhance the model&#x2019;s attention on the regions of skin lesions during training, an Efficient Channel Attention (ECA) module is incorporated into the ConvNeXt network. Moreover, the proposed model employs a denoising module to reduce the influence of artifacts and improve the image contrast. The results, obtained by experiments conducted on two datasets, demonstrate that the proposed ConvNeXt-ST-AFF model has higher classification ability, according to multiple evaluation metrics, compared to the original ConvNeXt and Swin Transformer, and other state-of-the-art classification models.",
  "full_text": " \nVOLUME XX, 2023 1 \nDate of publication xxxx 00, 0000, date of current version xxxx 00, 0000.  \nDigital Object Identifier 10.1109/ACCESS.2023.Doi Number \nConvNeXt-ST-AFF: A Novel Skin Disease \nClassification Model Based on Fusion of \nConvNeXt and Swin Transformer \nShengnan Hao 1, Liguo Zhang1, Yanyan Jiang2, Jingkun Wang3, Zhanlin Ji1,4, Li Zhao3* and \nIvan Ganchev4,5,6* \n1Department of Artificial Intelligence, North China University of Science and Technology, Tangshan 063009, China.  \n2Hebei Agricultural University, Baoding 071000, China. \n3Research Institute of Information Technology, Tsinghua University, Beijing 100080, China. \n4Telecommunications Research Centre (TRC), University of Limerick, Limerick, V94 T9PX, Ireland.  \n5Department of Computer Systems, University of Plovdiv â€œPaisii Hilendarskiâ€, Plovdiv 4000, Bulgaria.  \n6Institute of Mathematics and Informaticsâ€”Bulgarian Academy of Sciences, Sofia 1040, Bulgaria.  \nCorresponding author: zhaoli@tsinghua.edu.cn; ivan.ganchev@ul.ie (I.G.). \nThis publication has emanated from research conducted with the financial support of the National Key Research and Development  Program of China \nunder the Grant No. 2017YFE0135700, the Tsinghua Precision Medicine Foundation under the Grant No. 2022TS003, the Bulgarian N ational Science Fund \n(BNSF) under the Grant No. ĞšĞŸ-06-Ğ˜ĞŸ-ĞšĞ˜Ğ¢ĞĞ™/1 (ĞšP-06-IP-CHINA/1), and the Telecommunications Research Centre (TRC) of University of Limerick, \nIreland.  \nABSTRACT Automatic classification of dermatological images is an important technology that assists \ndoctors in achieving faster and more accurate classification of skin diseases. Recently, convolutional neural \nnetworks (CNNs) and Transformer networks have been employed in learning respectively the local and \nglobal features of lesion images. However, existing works mainly focus on utilizing a single neural network \nfor feature extraction, which limits the model classification performance. In order to tackle this problem, a \nnovel fusion model, named ConvNeXt-ST-AFF, is proposed in this paper , by combining the strengths of \nConvNeXt and Swin Transformer (ConvNeXt-ST in the model name) . In the proposed model , the \npretrained ConvNeXt and Swin Transformer networks extract local and global features from images, which \nare then fused using Attentional Feature Fusion (AFF) modules (AFF in the model name). Additionally, in \norder to enhance the model's attention on the regions of skin lesions during training, an Efficient Channel \nAttention (ECA) module is incorporated into the ConvNeXt network. Moreover, t he proposed model \nemploys a denoising module to reduce the influence of artifacts and improve the image contrast. The \nresults, obtained by experiments conducted on two datasets, demonstrate that the proposed ConvNeXt-ST-\nAFF model has higher classification ability, based on multiple evaluation metrics, compared to the original \nConvNeXt and Swin Transformer, and other state-of-the-art classification models. \nINDEX TERMS Skin disease classification ; image denoising; model fusion; attention; ConvNeXt; Swin \nTransformer \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nI. INTRODUCTION \nMalignant melanoma is a type of skin cancer characterized \nby abnormal growth of skin cells , [1]. If not diagnosed early, \nmalignant melanoma has a high mortality rate , [2]. \nAccording to data released by the World Health \nOrganization, in 2019 alone, there were 104,350 reported \ncases of skin cancer and 11,650 deaths in the United States , \n[3]. Research has shown that early treatment of malignant \nmelanoma can significantly reduce the mortality rate of \npatients, [4]. Dermoscopy is an essential tool used by \nphysicians for diagnosing skin cancer , [5]. It helps reduce \nthe reflection effect on the skin surface, providing doctors \nwith clearer and more detailed images of lesions. This \nenables dermatologists to observe deeper characteristics of \nthe affected areas. However, melanoma shares very similar \nfeatures with other skin diseases, and even experienced \ndermatologists can achieve an accuracy rate of only around \n75% in diagnosing melanoma using dermoscopy , [6]. \nTherefore, the diagnosis of melanoma remains a time -\nconsuming and error-prone process. \nIn recent years, researchers have proposed artificial \nintelligence (AI)-assisted diagnostic methods to help physicians \nachieve faster and more accurate diagnosis of melanoma , [7]. \nUnlike traditional methods, such as the 7-point checklist [8] and \nthe ABCD rule [9] that rely on color and shape of the lesion \narea for classification, the AI -assisted diagnostic methods \nutilize deep features extracted from images to classify skin \ndiseases, [10]. However, the extraction of deep features from \nimages faces the following challenges: (1) the contrast between \nnormal skin regions and lesion areas in dermoscopy images is \nlow, making it difficult for a neural network to focus on the \nskin lesion area during feature extraction; (2) dermoscopy \nimages often contain artifacts such as body hair and blood \nvessels, which greatly affect the extraction of meaningful \nfeatures; (3) the intra-class variation of lesion area features is \nhigh, while the inter -class similarity of lesion area features \nbetween different classes is also high, complicating the \nclassification process.  \nTo date , various methods have been proposed to address \nthese challenges, but the accuracy of classification is still not \nideal, [11]. In the field of AI -assisted diagnosis, convolutional \nneural networks (CNNs) and Transformer networks [12] are the \nmainstream feature extraction networks. CNN s traverse the \nfeature maps of images using convolutional kernels of different \nsizes to extract features from different positions of images. The \nCNN advantage lies in extracting local features of images, [13]. \nThe core module of the Transformer network is the multi-head \nself-attention module, which can capture global features of \nimages, based on global contextual information , [14, 15]. In \nrecent years , researchers have attempted to improve \nclassification accuracy through model fusion , [16]. However, \nexisting fusion methods mostly focus on fusing features \nextracted by different CNN networks [17], and do not \neffectively leverage both local and global features of the image. \nAdditionally, researchers often preprocess dermoscopy images \nbefore feature extraction to remove image artifacts and improve \ncontrast, [18]. To overcome the aforementioned problems, a \nnovel fusion model is  proposed here for  skin disease \nclassification. \nThe following are the main contributions of the paper: \n1) The incorporation of  an Efficient Channel Attention \n(ECA) module into the ConvNeXt network is proposed \nto enhance the network's focus on the lesion areas; \n2) During the feature extraction stage, the employment of \ntwo different networks ( ConvNeXt and Swin \nTransformer) is proposed  as two subnetworks train ed \nby means of transfer learning; \n3) The utilization of a Attentional Feature Fusion (AFF) \nmodule is proposed for feature fusion, allowing the \nmodel to dynamically allocate weights based on the \nimportance of input features. \nThe remaining structure of this paper is the following. \nSection 2 provides an overview of related work done in the \nfield of skin disease classification, including a brief summary of \nexisting models and their corresponding research outcomes. \nSection 3 presents a detailed description of the proposed model. \nSection 4 describes the experimental setup and results of the \nexperimental performance comparison of the proposed model  \nwith state-of-the-art models. Finally, Section 5 concludes the \npaper by summarizing the contributions of this study  and \nsetting up future directions for research.  \nII. RELATED WORK \nCNNs have been widely used for image feature extraction, \nleveraging different-sized convolutional kernels to capture local \nfeatures at different positions of the input image , [19]. With \nadvancements in neural network technology, CNN models , \nsuch as VGG [20], ResNet [21], DenseNet [22], EfficientNet \n[23], and ConvNeXt [24], have gained significant attention in \nimage classification. To focus more on the lesion areas, CNN \nmodels are often combined with attention mechanisms to \nimprove classification performance. For instance, Zhang et al. \n[25] designed an Attention Residual Learning (ARL) module \nthat combines residual learning and attention learning \nmechanisms to construct an Attention Residual Learning \nConvolutional Neural Network (ARL -CNN). The se authors  \nconducted experiments on the ISIC2017 dataset, demonstrating \nthat the ir model adapts to focusing on the skin lesion areas \nduring training. Wan et al. [26] proposed a Multi-Scale Long \nAttention Network (MSLANet) that fuses contextual \ninformation through three Long Attention Networks (LANets). \nAdditionally, MSLANet extracts multi -scale information \nthrough self-supervised learning. Their network achieved area \nunder the curve ( AUC) values of 93.7% and 92.4% on the \nISIC2017 and ISIC2020 datasets, respectively. However, CNN \nmodels are primarily adopted for extracting local features, \nmaking them less effective at capturing contextual information \nand long-range dependencies, thereby limiting their ability to \nextract global features effectively. \nIn recent years, Transformer networks, based on the multi -\nhead self -attention mechanism , have become popular for \nextracting global features by capturing global contextual \ninformation, [15]. Researchers have made various attempts to \nexplore the classification performance of Transformer networks \nin the field of skin disease classification. For instance, Ayas et \nal. [27] utilized the Swin Transformer model for skin disease \nclassification and introduced weighted cross -entropy loss to \naddress class imbalance. Their model achieved sensitivity and \nspecificity of 82.3% and 97.9%, respectively, on the ISIC2019 \ndataset. He et al. [14] designed a Fully Transformer Network \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \n(FTN) that learns long-range contextual information to improve \nthe baseline performance of CNNs in skin disease \nclassification. The se authors  introduced a Spatial Pyramid \nPooling (SPP) module in the multi -head attention (MHA) to \nreduce computational complexity and memory consumption. \nCai et al. [28] employed Vision Transformer (V IT) as the \nbackbone network for extracting deep image features and fused \nthe extracted features with patient metadata. The ir method \nachieved an accuracy of 93.81% on the ISIC2018 dataset. \nHowever, these Transformer -based models tend to capture \nglobal features while neglecting the consideration of local \nfeatures. \nTo effectively utilize different types of features, designing \nfeature fusion architectures is an effective approach that \nconsiders the feature extraction preferences of different \nnetworks to improve classification accuracy. In [29], Mahbod et \nal. proposed a Multi -Scale Multi-CNN Fusion (MSM -CNN) \nmethod for fusing features extracted by three CNN models on \nsix different scales of cropped images, achieving a balanced \nmulti-class accuracy of 86.2% on the ISIC2018 dataset. \nMaqsood et al. [17] presented a unified Computer-Aided \nDiagnosis (CAD) model for segmentation and classification of \nskin lesions. Their model fused features extracted by four pre-\ntrained CNN networks using a convolutional sparse image \ndecomposition fusion method and employed univariate \nmeasurement and Poisson distribution feature selection \nmethods to select the best classification features. The \nclassification accuracy of this method on the HAM10000, \nISIC2018, ISIC2019, and PH2 datasets was 98.57%, 98.62%, \n93.47%, and 98.98%, respectively. However, existing works \ntend to focus on designing feature fusion architectures based on \nCNN models, without effectively combining the local and \nglobal features of images. \nTherefore, combining the feature extraction advantages of \nboth CNNs and Transformers to effectively utilize the local and \nglobal features of images for skin disease classification is a \npromising research direction. \nIII. PROPOSED MODEL: ConvNeXt-ST-AFF \nIn contrast to the methods employed in [16], which fuse image \nfeatures extracted by VGG16, AlexNet, ResNet -18, and \nResNet-101, the presented research opts for the fusion of two \ndistinct types of feature extraction networks, namely CNNs and \nTransformers. This approach aims to effectively utilize both \nlocal and global image features for classification. Inspired by \nthe work presented in [13], we chose to merge the features \nextracted from each block of the ConvNeXt and Swin \nTransformer networks, rather than solely fusing the final \nfeatures. This strategy enables the Swin Transformer network to \ncontinually provide global feature information to the ConvNeXt \nnetwork. To address the limitations of fixed -weight fusion \nmethods like summation and concatenation, the proposed \nmodel employs an Attention Feature Fusion (AFF) module, \nallowing the network to dynamically adjust the weights of each \ninput feature during training. To further enhance the integration \nof local and global image features, a channel-shuffled operation \nis incorporated within the AFF module to improve information \ninteraction between different channel heights. Additionally, to \nmitigate class imbalance issues of the datasets utilized in the \nexperiments, data augmentation is applied to the images before \nnetwork training. Furthermore, denoising techniques are \nemployed to reduce image noise and enhance contrast in \ndermatoscopic images. Based on these considerations, the \nproposed ConvNeXt-ST-AFF model was elaborated with the \noverall structure illustrated in Figure 1.  \n \nInput\nStem\nConvNeXt block\nDims=dim\nConvNeXt block\nDims=dim*2\nConvNeXt block\nDims=dim*8\nConvNeXt block\nDims=dim*4\nDownsample\nDownsample\nDownsample\nLiner Embedding\nSwin transformer \nblock\nDims=dim\nSwin transformer \nblock\nDims=dim*2\nSwin transformer \nblock\nDims=dim*4\nSwin transformer \nblock\nDims=dim*8\nPatch Merging\nPatch Merging\nPatch Merging\nLayer Norm\nGlobal Average Pooling\nLiner\nPatch Partition\nDenoising \nmodule\nAFF\nAFF\nAFF\nAFF\nConvNeXt Swin \nTransformer\nClass\nHead\nFeature extraction module\n \nFIGURE 1. The overall structure of the proposed ConvNeXt-ST-AFF model. \n \nThe overall process of skin disease image classification can \nbe divided into three steps, each contributing to enhancing the \nmodel's capability to effectively classify skin disease images: \n1) The first step involves using a denoising module to \nremove artifacts and pseudo -features from the input \nskin disease image s, while simultaneously enhancing \ntheir contrast. This preprocessing step helps improve \nthe overall quality of the image s, making it easier for \nsubsequent modules to extract meaningful features. \n2) In the second step, each denoised image is fed into the \nfeature extraction module. In the proposed model, this \nmodule incorporates a Swin Transformer network and a \nConvNeXt network, both of which are pre -trained on \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nlarge-scale datasets. The Swin Transformer excels at \ncapturing global patterns and long-range dependencies \nin the image, while the ConvNeXt network focuses on \nextracting local features. By combining both global and \nlocal features, the proposed model gains a \ncomprehensive understanding of the skin disease \nimage. Additionally, AFF modules are utilized in this \nstep to fuse the features extracted by the Swin  \nTransformer and ConvNeXt networks effectively. \nThe AFF modules merge the local and global features, \nenabling the model to capture the synergistic \ninformation from both sources, leading to more \ndiscriminative feature representations. \n3) In t he final step, the fused features from the feature \nextraction module are fed into the classification head, \nwhere the final classification of the skin disease image \ntakes place. \nIn the following subsections, a detailed description of each \nmodule's functionality is separately provided. \nA. DENOISING MODULE \nThe presence of artifacts, such as body hair and blood vessels, \nin skin disease images  may have a negative impact on the \nmodel's classification performance [30]. Image denoising is an \neffective method for reducing artifacts in images , [31]. When \nusing traditional wavelet-based image denoising methods, it's \nnecessary to first determine the type of noise present in an \nimage, such as Gaussian noise or salt -and-pepper noise, and \nthen choose specific wavelets for denoising based on the noise \ntype. Additionally, parameter adjustments need to be made \nmanually for different types of images during the denoising \nprocess. \nIn recent years, neural network -based image denoising \nmethods have demonstrated excellent performance. The most \nsignificant difference between them and traditional methods lies \nin their generalization ability. Through extensive training on \nimage data, neural network methods automatically learn model \nparameters that adapt to different image data and noise \ndistributions during training, without relying on manual \nparameter tuning. This results in better generalization across \nvarious types of images and noise distributions, [32]. \nTherefore, in our study, we employed neural network-based \ndenoising methods. Specifically, we combined the design of \nRED-Net [33] to implement a smaller denoising network  that \nemploys an end-to-end training approach. It takes the original \nskin lesion image as input, trains the network to understand the \ndistribution of artifacts in skin lesion images, selectively \nreduces their impact, and ultimately outputs a denoised clean \nimage. As shown in Figure 2, the denoising module used in this \nstudy consists of an encoder, a decoder, and skip connections. \nThe encoder comprises five 2D convolutional blocks for feature \nextraction, while the decoder consists of five 2D transpose \nconvolutional blocks for recovering the denoised image. \nThe denoising module of the proposed model  employes \nsymmetric convolutional layers and transpose convolutional \nlayers to ensure consistency in the sizes of input and output \nimages. Specifically, convolutional layers serve as the encoder \nfor extracting image information. They are capable of learning \nthe essential information within the image and eliminating \nfeatures, including noise, that are irrelevant to network training. \nHowever, this process may result in the loss of certain image \ndetails. On the other hand, transpose convolutional layers \nprimarily focus on restoring fine details in the image. They \nupsample the low -resolution image output from the encoder, \nensuring that the recovered image maintains the same \nresolution as the original image. Inspired by U-Net and VGG-\nUNet [34], we have introduced skip connections between the \nsymmetric convolutional and transpose convolutional layers to \nbetter leverage shallow features containing more detailed \ninformation and deep features containing more semantic \ninformation for image recovery. Since the denoising module \nserves as a preprocessing step before image feature extraction, \nwe have intentionally avoided using any pooling operations \nwithin the denoising module. This is because pooling \noperations have the potential to discard valuable details from \nthe original image, resulting in the loss of critical information in \nthe recovered image. \nThe denoising module not only reduces the impact of \nartifacts in the image but also effectively enhances the contrast \nbetween lesion and non-lesion areas. \n2D Convolution\n(kernel_size=3)\n+\n2D Transposed \nConvolution\n(kernel_size=3)\n+\n+\nEncoder Decoder\nX Y\n2D Convolution\n(kernel_size=3)\n2D Convolution\n(kernel_size=3)\n2D Convolution\n(kernel_size=3)\n2D Convolution\nkernel_size=3)\n2D Transposed \nConvolution\n(kernel_size=3)\n2D Transposed \nConvolution\n(kernel_size=3)\n2D Transposed \nConvolution\n(kernel_size=3)\n2D Transposed \nConvolution\n(kernel_size=3)\n \nFIGURE 2. The denoising module structure.  \nB.  FEATURE EXTRACTION MODULE  \nThe feature extraction module of the proposed model utilizes \nConvNeXt and Swin Transformer as feature extraction \nnetworks. \nDue to the high cost of acquiring dermatological data, \ntraining a network from scratch solely using a dermatological \ndataset may result in insufficient training, leading to a lack of \ngeneralization ability. This problem can be addressed through \nthe application of transfer learning. \nTransfer learning is a machine learning method that \nleverages knowledge transferred from related learned tasks to a \nnew task, enabling a network to achieve higher performance \nwith limited training data , [3]. In the context of image \nclassification tasks, it is common to employ a network pre -\ntrained on a large-scale image dataset, such as ImageNet [35], \nand fine-tune it for the new task. \nMathematically, the operation of transfer learning is defined \nas per [17] as: \nğ‘‘ğ‘  = {(ğ‘¢1\nğ‘ , ğ‘£1\nğ‘ ), â€¦ , (ğ‘¢ğ‘–\nğ‘ , ğ‘£ğ‘–\nğ‘ ), â€¦ , (ğ‘¢ğ‘›\nğ‘  , ğ‘£ğ‘›\nğ‘ )} , (1) \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nwhere ğ‘‘ğ‘  represents the source domain whose learning function \nis defined as  ğ¿ğ‘ , ğ¿ğ‘‘ , (ğ‘¢ğ‘¥\n5, ğ‘£ğ‘¥\n5) âˆˆ ğœ“. \nThe operation for the target domain ğ‘‘ğ‘¡  is defined as: \nğ‘‘ğ‘¡ = {(ğ‘¢1\nğ‘¡ , ğ‘£1\nğ‘¡ ), â€¦ , (ğ‘¢ğ‘–\nğ‘¡ , ğ‘£ğ‘–\nğ‘¡ ), â€¦ , (ğ‘¢ğ‘›\nğ‘¡ , ğ‘£ğ‘›\nğ‘¡ )} , (2) \nwhere (ğ‘¥, ğ‘¦) represents the size of the training data  (ğ‘¥ â‰ª ğ‘¦), \nand ğ‘£ğ‘–\nğ‘   and ğ‘£ğ‘–\nğ‘¡  denote the labels of the training data. The \nlearning task for the target domain is defined in [17] as  \nğ¿ğ‘¡ , (ğ‘¢ğ‘¦\nğ‘¡ , ğ‘£ğ‘¦\nğ‘¡ ) âˆˆ ğœ“ . By employing this operation, all modified \nnetworks are trained on a specific dataset. The specific training \nprocess of transfer learning is illustrated in Figure 3. \nImageNet\nSource Data\nConvNeXt\n&\nSwin Transformer\nConvNeXt\n&\nSwin Transformer\nTarget Data\nSource Networks\nTarget  Networks\nFeature Layer\nTranfer Learning\n \nFIGURE 3. Illustration of model training using transfer learning. \n \n1) AFF SUBMODULE \nSkip connections are a crucial component of CNNs. They \nmerge low -level detail features with high -level semantic \nfeatures through summation or concatenation operations, \nenabling the network to obtain richer semantic representations, \n[36]. However, traditional summation- or concatenation-based \nfeature fusion methods can only assign fixed weights to \ndifferent features. \nTo enhance the feature fusion operation in skip connections, \nthe proposed model incorporates  Attentional Feature Fusion \n(AFF) modules [37]. The AFF module leverages the two input \nfeatures to generate dynamic fusion weights, allowing the \nnetwork to adaptively select features based on their importance.  \nThe specific implementation process of the Attention al \nFeature Fusion ( AFF) module , depicted in Figure 4 , is the \nfollowing. Firstly, the features ğ‘‹ and ğ‘Œ are combined through \nan initial feature fusion operation. The fused feature is then fed \ninto a Multi-Scale Channel Attention Module (MS -CAM) \nmodule, which generates a fusion weight ğ›¼ (ranging from 0 to \n1), used as the weight for feature ğ‘‹, while 1 âˆ’ ğ›¼ is used as the \nweight for feature ğ‘Œ . This weighted fusion allows adaptive \nfeature fusion based on the importance of features ğ‘‹ and ğ‘Œ. \nThrough the attention-based feature fusion process, the network \ncan dynamically adapt the weighted fusion of features ğ‘‹ and ğ‘Œ, \nthereby enhancing the performance of the model. \nThe calculation process for the AFF module is defined in \n[37] as follows: \nğ‘ = ğ›¼ âŠ— ğ‘‹ + (1 âˆ’ ğ›¼) âŠ— ğ‘Œ , (3) \nwhere ğ‘ âˆˆ ğ‘…ğ¶Ã—ğ»Ã—ğ‘Š  denotes the fused feature , âŠ— denotes an \nelement-wise multiplication, and  ğ›¼ =  ğ‘€(ğ‘‹ âŠ ğ‘Œ) denotes the \nfusion weight, where âŠ denotes the initial feature fusion and ğ‘€ \nrepresents the MS-CAM module, which plays a pivotal role in \naggregating local and global features. \nchannel_shuffle\nMS-CAM (1-a) Ã„ Xa Ã„ X\na 1-a\n+\nCat\nConv\nX Y\nZ\n \nFIGURE 4. The AFF structure. \n \nMS-CAM, shown in Figure 5,  utilizes point -wise \nconvolution with an 1x1 convolutional kernel to transform the \nchannel dimension of the feature map. This selection offers the \nadvantage of reducing the computational cost associated with \nusing different-scale convolutional kernels. Through the MS -\nCAM, the AFF module can effectively leverage information \nfrom different scales of the image and facilitate the effective \nfusion of local and global features.  \nThe MS -CAM module utilizes two branches to extract \nchannel attention weights. The first o ne employs Global \nAverage Pooling to extract global feature attention, while the \nsecond one directly uses Point-Wise Convolution (PWConv) to \nextract local feature attention. The local feature ğ¿(ğ‘‹) âˆˆ\nğ‘…ğ¶Ã—ğ»Ã—ğ‘Š is calculated as shown in [37]: \nğ¿(ğ‘‹) = â„¬ (ğ‘ƒğ‘Šğ¶ğ‘œğ‘›ğ‘£2 (ğ›¿(â„¬(ğ‘ƒğ‘Šğ¶ğ‘œğ‘›ğ‘£1(ğ‘‹))))), (4) \nwhere the convolution kernels for ğ‘ƒğ‘Šğ¶ğ‘œğ‘›ğ‘£1  and ğ‘ƒğ‘Šğ¶ğ‘œğ‘›ğ‘£2 \nare of size ğ¶/ğ‘Ÿ Ã— ğ¶ Ã— 1 Ã— 1 and ğ¶ Ã— ğ¶/ğ‘Ÿ Ã— 1 Ã— 1, respectively. \nGiven the local feature L(X) and the global feature g(X), the \nweight ğ‘ in MS-CAM is calculated as follows: \nğ‘ = ğ‘€(ğ‘‹) = ğœ(ğ¿(ğ‘‹) âŠ• ğ‘”(ğ‘‹)) , (5) \nwhere ğ‘€(ğ‘‹) âˆˆ ğ‘…ğ¶Ã—ğ»Ã—ğ‘Š  denotes the attention weights \ngenerated by the MS-CAM module and  âŠ• denotes the element \nsummation. \n2) SWIN TRANSFORMER SUBMODULE \nThe Swin Transformer [38] is utilized as a global feature \nextraction submodule for image processing by the proposed \nmodel. Figure 6 illustrates the structure of two consecutive \nSwin Transformer blocks, each consisting of a Windows Multi-\nHead Self -Attention (W -MSA) or Shifted Windows Multi -\nHead Self -Attention (SW -MSA) subblock, a Multi-Layer \nPerceptron (MLP)  subblock (shown in Figure 7), two \nLayerNorm (LN) normalization layers, a DropPath layer, and \ntwo skip connections. W -MSA and SW -MSA subblocks are \napplied alternately in two consecutive Swin Transformer blocks. \nThe computation al flow for the two consecutive Swin \nTransformer blocks is defined in [38] as: \nğ‘§Ë†ğ‘™ = ğ‘Š âˆ’ ğ‘€ğ‘†ğ´ (ğ¿ğ‘(ğ‘§ğ‘™âˆ’1)) + ğ‘§ğ‘™âˆ’1; (6) \nğ‘§ğ‘™ = ğ‘€ğ¿ğ‘ƒ (ğ¿ğ‘(ğ‘§Ë†ğ‘™)) + ğ‘§Ë†ğ‘™ ; (7) \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nGlobal Average Pooling\nPoint-wise Conv\nk=1ï¼Œs=1\nBatchNorm\nReLU\nBatchNorm\nBatchNorm\nReLU\nBatchNorm\nX\nC*H*W\nC*1*1\n(C/4)*1*1\nC*1*1 C*H*W\n+\nC*H*W\nSigmoid\na\nLocal \nfeature \nattention\nGlobal \nfeature \nattention\nPoint-wise Conv\nk=1ï¼Œs=1\nPoint-wise Conv\nk=1ï¼Œs=1\nPoint-wise Conv\nk=1ï¼Œs=1\n(C/4)*H*W\n1-a\n \nFIGURE 5. The MS-CAM structure. \n \nğ‘§Ë†ğ‘™+1 = ğ‘†ğ‘Š âˆ’ ğ‘€ğ‘†ğ´(ğ¿ğ‘(ğ‘§ğ‘™)) + ğ‘§ğ‘™ ; (8) \nğ‘§ğ‘™+1 = ğ‘€ğ¿ğ‘ƒ (ğ¿ğ‘(ğ‘§Ë†ğ‘™+1)) + ğ‘§Ë†ğ‘™+1, (9) \nwhere ğ‘§Ë†ğ‘™, ğ‘§ğ‘™ , ğ‘§Ë†ğ‘™+1 and zğ‘™+1 denote the output features of the W-\nMSA and MLP subblocks of the first block, and the output \nfeatures of the SW -MSA and MLP subblocks of the second \nblock, respectively. \nMLP\nW-MSA\nLayer Norm\nLayer Norm\nMLP\nSW-MSA\nLayer Norm\nLayer Norm\nDropPath DropPath\n+\n+\n+\n+\nBlock 1 Block 2\n \nFIGURE 6. The structure of two consecutive Swin Transformer blocks . \nLiner\nGeLU\nDropout\nLiner\nDropout\n4*dim dim\n \nFIGURE 7. The MLP structure. \n \nThe W-MSA subblock partitions the input feature map into \nmultiple non-overlapping windows and computes self-attention \nwithin each window. This design significantly reduces the \ncomputational complexity of the network. However, due to the \nwindow partitioning, the use of W-MSA can lead to \ninformation loss between windows. To address this issue, the \nSwin Transformer submodule employes a SW-MSA subblock. \nThe SW -MSA module incorporates an offset matrix ğµ âˆˆ\nğ‘…ğ‘€2Ã—ğ‘€2\n when calculating self-attention as follows [38]: \nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘› (ğ‘„, ğ¾, ğ‘‰) = ğ‘†ğ‘œğ‘“ğ‘¡ğ‘€ğ‘ğ‘¥ (ğ‘„ğ¾ğ‘‡ /âˆšğ‘‘ + ğµ)ğ‘‰, (10) \nwhere ğ‘„, ğ¾, ğ‘‰ âˆˆ ğ‘…ğ‘€2Ã—ğ‘‘  denote the query, key, and value \nmatrices, respectively,  ğ‘‘ denotes the dimension of the query \nand key, and ğ‘€2 denotes the number of patches in a window. \nThe values in matrix ğµ are obtained from the offset matrix ğµË† âˆˆ\nğ‘…(2ğ‘€âˆ’1)Ã—(2ğ‘€âˆ’1). \nThe SW-MSA subblock facilitates information propagation \nbetween windows. By combining the W -MSA and SW -MSA \nsubblocks, the Swin Transformer submodule is able to \nefficiently process large-scale images and effectively capture \nthe global features of the images. \n3) ConvNeXt SUBMODULE \nIn the feature extraction module of  the proposed model , a \nConvNeXt network [24] is used as a submodule for local \nfeature extraction from the skin disease images. The ConvNeXt \nnetwork consists of ConvNeXt blocks and Downsample blocks. \nAs illustrated in Figure 8a, each ConvNeXt block includes skip \nconnections, depthwise convolution layers, LN normalization \nlayers, MLP blocks, Layer Scale, and DropPath. The \ndownsample block structure is depicted in Figure 8c. \nECA\nDepthwise Conv2d\nK7ï¼Œs1ï¼Œp3\nLayer Norm\nMLP\nLayer Scale\nDepthwise Conv2d\nK7ï¼Œs1ï¼Œp3\nLayer Norm\nMLP\nLayer Scale\nDrop Path\n+\nï¼ˆaï¼‰ ï¼ˆbï¼‰\n+\nDrop Path\n Conv2d\nK2ï¼Œs2\nLayer Norm\nï¼ˆcï¼‰\n \nFIGURE 8. (a) The original ConvNeXt block structure; (b) The ConvNeXt \nblock structure used in the proposed model; (c) The down-sample block \nstructure. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nThe attention mechanism can enhance the network's focus on \nthe lesion regions during the training process, thereby \nimproving the model's performance, [39]. After comparing the \neffects of different attention modules, we ultimately chose to \nincorporate an Efficient Channel Attention (ECA) subblock [40] \ninto the original ConvNeXt block as to enhance its feature \nextraction capability. The structure of the ConvNeXt block , \nmodified in this fashion, is illustrated in Figure 8b.  \nECA employes a type of channel attention mechanism that \ngenerates corresponding weight values for each channel in the \nfeature map based on their importance. This allows the network \nto pay more attention to important feature channels. The \nimplementation process follows these steps: when the feature \nmap is input ted into ECA, it first undergoes global average \npooling. Then, ECA adaptively computes the size of the one -\ndimensional convolution kernel and generates the weights for \neach channel in the feature map. Finally, the normalized \nweights are multiplied with the original feature map to produce \nthe channel -weighted feature map. The ECA structure is \ndepicted in Figure 9. \nGlobalAvgPooling\nConv1d\nSigmoid\nX*a\na\nX\n \nFIGURE 9. The ECA structure. \n4) FEATURE FUSION \nDifferent feature extraction networks have different preferences \nfor extracting features. Combining features extracted from \nmultiple networks can improve the adaptability of the fused \nfeatures and thus enhance classification performance. Therefore, \nin the proposed model , ConvNeXt and Swin Transformer \nnetworks are selected as two feature extraction networks. The \nSwin Transformer network can establish long -range \ndependencies, effectively extracting global features from the \nimage. On the other hand, the ConvNeXt network traverses the \nfeature map of the input image with convolutional kernels to \nextract local features. To fully leverage the advantages of these \nnetworks, the feature extraction module of the proposed model \nutilizes a AFF submodule, allowing it to dynamically generate \ncorresponding weights based on the importance of the input \nfeatures. The weighted features are then fused to make full use \nof both local and global features for skin disease classification. \nThe specific fusion process of the AFF module follows these \nsteps. Firstly, the features extracted by the Swin Transformer \nnetwork are linearly projected to the shape of ğµ Ã— ğ¶ Ã— ğ» Ã— ğ‘Š. \nThey are then concatenated with the features extracted by the \nConvNeXt network. A 1x1 convolution is applied to transform \nthe channel-shuffled [41] feature map to the desired number of \nchannels. Finally, the fused feature map is input ted into MS-\nCAM to compute the weights. The weighted features are then \nseparately weighted and summed to obtain the fused feature. \nThrough the feature fusion of the AFF submodule, the \nproposed model can effectively utilize both local and global \nfeatures of the image s, thereby improving the classification \nperformance. \nC. DATA AUGMENTATION \nNeural networks require extensive training on annotated data to \nachieve high performance. However, acquiring skin disease \nimage data is costly, resulting in a relatively small number of \nimages in skin disease datasets, along with a severe class \nimbalance issue  [42]. This imbalance leads to lower \nclassification accuracy for classes with fewer images [43]. \nTherefore, before conducting model training, we employed data \naugmentation [44] to increase the diversity of image \ninformation in the utilized datasets. This helps mitigate the \nimpact of class imbalance on the model training results while \nenhancing the model's robustness and generalization \ncapabilities. The specific steps taken were the following: \n(1) The images in the training set were randomly cropped, \nand the cropped images were resized to 224 Ã— 224 pixels. This \nstep helped in focusing on relevant regions of the images while \nmaintaining a consistent input size;  \n(2) The RandAugment method [45] was applied to enhance \nthe augmented images further. More specifically, in order to \nincrease the diversity of the training set, each image underwent \ntwo randomly selected image augmentation s out of fifteen \navailable options, such as  histogram equalization, contrast \nadjustment, color inversion, random rotation, increasing \nexposure, and random horizontal (vertical) translation , etc . \nUtilization of these data augmentation techniques  allowed to \nincreases the diversity of the images in the training set, \neffectively mitigating overfitting and enhancing the model's  \nclassification performance. Additionally, these techniques \ncontributed to achieving a more stable training process, [46]. \nExperimentally, a fter applying these data augmentation  \ntechniques, we found o ut that the ConvNeXt values of all \nevaluation metrics were indeed significantly higher than those \nof ConvNeXt without augmentation. Therefore, in the main \nexperiments, described in the next section, we chose to use the \nexperimental results of ConvNeXt after applying data \naugmentation as the baseline for comparison. \nIV. EXPERIMENTS AND RESULTS  \nA. DATASETS \nIn the experiments, two datasets were used â€“ a private dataset \ncontaining acne images and the publicly available ISIC2019 \ndataset. \nThe acne dataset was provided by Peking Union Medical \nCollege Hospital and used with the consent of all participants. \nThe dataset contains 2,900 skin lesion images classified into six \nclasses (Figure 10): acne (AC), melasma (ME), Ota nevus (ON), \nrosacea (ROS), discoid lupus erythematosus (DLE), and \nseborrheic dermatitis (SD). All images and their corresponding \nlabels have undergone rigorous review by dermatologists. For \nconducting the experiments, this dataset was randomly split into \na training set and a testing set in an 8:2 ratio as shown in Table \n1. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nThe second dataset was the public ISIC2019 dataset [47]. It \nis a publicly available large -scale skin lesion image dataset, \nconsisting of 25,331 skin lesion images  containing images of \neight classes of skin diseases (Figure 11): actinic keratosis (AK), \nbasal cell carcinoma (BCC), benign keratosis (BKL), \ndermatofibroma (DF), melanoma (MEL), melanocytic nevus \n(NV), squamous cell carcinoma (SCC), and vascular lesion \n(VASC). For conducting the experiments, this dataset was also \nrandomly split into training and testing sets with an 8:2 ratio as \nshown in Table 2. \n \nï¼ˆaï¼‰\n ï¼ˆbï¼‰\n ï¼ˆcï¼‰\nï¼ˆdï¼‰\n ï¼ˆeï¼‰\n ï¼ˆfï¼‰\n \nFIGURE 10. Sample images of the private acne dataset: (a) Acne (AC); \n(b) Melasma (ME); (c) Rosacea (ROS); (d) Discoid lupus erythematosus \n(DLE); (e) Ota nevus (ON); (f) Seborrheic dermatitis (SD).  \n \nTABLE 1. SPLITTING THE PRIVATE ACNE DATASET INTO TRAINING AND \nTESTING SETS \nClasses Images in \ntraining set \nImages in \ntesting set \nTotal \nimages in \ndataset \nAC 1292 323 1615 \nME 320 80 400 \nROS 248 62 310 \nDLE 100 25 125 \nON 224 56 280 \nSD 136 34 170 \n \nï¼ˆaï¼‰\n ï¼ˆbï¼‰\n ï¼ˆcï¼‰\n ï¼ˆdï¼‰\nï¼ˆeï¼‰\n ï¼ˆfï¼‰\n ï¼ˆgï¼‰\n ï¼ˆhï¼‰\n \nFIGURE 11. Sample images of the public ISIC2019 dataset: (a) Actinic \nKeratosis (AK); (b) Basal Cell Carcinoma (BCC); (c) Benign Keratosis -like \nLesions (BKL); (d) Dermatofibroma (DF); (e) Melanoma (MEL);                               \n(f) Melanocytic Nevi (NV); (g) Squamous Cell Carcinoma (SCC);                           \n(h) Vascular Lesions (VASC). \n                    \nTABLE 2. SPLITTING THE PUBLIC ISIC2019 DATASET INTO TRAINING AND \nTESTING SETS \nClasses Images in \ntraining set \nImages in \ntesting set \nTotal images \nin dataset \nAK 694 173 867 \nBCC 2659 664 3323 \nBKL 2100 524 2624 \nDF 192 47 239 \nMEL 3618 904 4522 \nNV 10300 2575 12875 \nSCC 503 125 628 \nVASC 203 50 253 \n \nB. EXPERIMENTAL SETUP \nIn the model training, a hyperparameter initialization was \nperformed, as follows: (i) in the experiments on the ISIC2019 \ndataset, the initial learning rate was set to 0.01 and the number \nof epochs was set to 100 ; (ii) in the experiments on the acne \ndataset, the initial learning rate was set to 0.0005  and the \nnumber of epochs was set to 60. In both sets of experiments, a \ncosine annealing strategy was employed to reduce the learning \nrate during model training, with a minimum learning rate of 1e-\n6. The batch size was set to 16. All models were optimized \nusing the stochastic gradient descent (SGD) optimizer [39], \nwith a momentum of 0.9 and a weight decay of 0.0001. The \nloss function used was the cross-entropy loss function. \nThe experiments were conducted on a host machine running \nLinux 3.10.0-1062.el7.bclinux.x86_64 and equipped with an \nIntel(R) Xeon(R) Gold 6240 CPU @ 2.60GHz and an NVIDIA \nTesla V100S PCIe 32GB GPU. The CUDA version used was \n11.0, and the PyTorch version was 1.12.1. \nC. EVALUATION METRICS \nTo evaluate the performance of the proposed model in \ncomparison to state -of-the-art models used  for skin lesion \nimage classification, multiple metrics were used to achieve  \ncomprehensive assessment of the model classification \nperformance.  \nA useful tool for visualizing the model classification \nperformance is the confusion matrix. It presents a model's \nclassification results for each class in a tabular form. An \nexample of a simple confusion matrix is shown in Figure 12. \nPositive Negative\nPositive\nNegative\nTrue\nPositive\n(TP)\nTrue\nNegative\n(TN)\nFalse\nPositive\n(FP)\nFalse\nNegative\n(FN)\nPredicted Class\nActual Class\n \nFIGURE 12. A simple confusion matrix. \n \nIn Figure 12 , True Positive (TP)  represents the samples \nwhose true class is positive, and the model correctly identifies \nthem as positive , False Negative (FN) indicates the samples \nwhose true class is positive, but the model incorrectly identifies \nthem as negative , False Positive (FP) refers to the samples \nwhose true class is negative, but the model incorrectly identifies \nthem as positive , and  True Negative (TN) represents the \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nsamples whose true class is negative, and the model correctly \nidentifies them as negative. \nThe performance of multi-class classifiers can be evaluated \nusing various metrics [48], briefly described below. \nAccuracy (Acc), a.k.a. detection rate,  is the most \nstraightforward evaluation metric for classification tasks . I t \nrepresents the proportion of correctly classified samples out of \nthe total number of samples, as follows: \nAcc =\nğ‘‡ğ‘ƒ+ğ‘‡ğ‘\nğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘ . (11) \nPrecision (Pre), a.k.a. positive predictive value (PPV), refers \nto the proportion of true positive samples among the samples \nclassified as positive, as follows: \nPre =\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ . (12) \nRecall (Rec), a.k.a. sensitivity or true positive rate (TPR), is \nthe proportion of true positive samples out of all samples  \nclassified (truly or falsely) as positive, as follows: \nğ‘…ğ‘’ğ‘ =\nğ‘‡ğ‘ƒ\nğ‘‡ğ‘ƒ+ğ¹ğ‘ . (13) \nSpecificity (Spec), a.k.a. selectivity or  true negative rate  \n(TNR), is the proportion of true negative samples out of all \nactually negative samples, as follows: \nğ‘†ğ‘ğ‘’ğ‘ =\nğ‘‡ğ‘\nğ‘‡ğ‘+ğ¹ğ‘ƒ . (14) \nF1-score (F1) is the harmonic mean of precision and recall, \ncalculated as follows: \nğ¹1 =\n2Ã— precision Ã— recall \n precision + recall  . (15) \nThe higher the F1-score, the better balance between precision \nand recall.  \nD. SELECTION OF NEURAL NETWORKS FOR USE IN THE \nPROPOSED MODEL \nTables 3 and 4 present the training results , obtained on the \nISIC2019 and acne datasets, respectively, of six commonly \nused networks for image classification, including four CNNs \nand two Transformer networks (the median value of three \ndifferent training runs performed on each dataset is shown in \nthe tables) . By comparing the training results, it can be \nobserved that the ConvNeXt model performs the best among \nthe four CNNs, according to all evaluation metrics, except for \nrecall on the  ISIC2019 dataset. The Swin Transformer \ndemonstrates the best classification performance among the two \nTransformer networks on both datasets, according to all \nevaluation metrics.  \n \nTABLE 3. TRAINING RESULTS OF STATE-OF-THE-ART NETWORKS ON THE \nPUBLIC ISIC2019 DATASET \nClassifiers Evaluation Metrics (%) \nAcc Pre Rec Spec F1 \nResNet-50 [21] 90.34 88.67 84.76 98.19 86.58 \nEfficientNet-B4 \n[23] 89.47 85.60 83.61 98.07 84.44 \nDenseNet-121 \n[22] 86.23 80.29 81.35 97.52 80.76 \nConvNeXt [24] 90.65 90.41 84.40 98.25 87.17 \nPIT [49] 86.01 84.00 78.97 97.39 81.29 \nSwin \nTransformer \n[38] \n89.39 89.13 83.66 97.98 86.10 \n \nTABLE 4. TRAINING RESULTS OF STATE-OF-THE-ART NETWORKS ON THE \nPRIVATE ACNE DATASET \nClassification \nmethod \nEvaluation metrics (%) \nAcc Pre Rec Spec F1 \nResNet-50 [21] 88.10 80.39 76.79 97.34 78.00 \nEfficientNet-B4 \n[23] 78.28 67.14 59.00 94.44 61.20 \nDenseNet-121 \n[22] 86.72 80.18 76.25 97.03 77.30 \nConvNeXt [24] 90.51 85.65 82.37 97.81 83.52 \nPIT [49] 87.93 80.60 76.43 97.06 77.94 \nSwin \nTransformer \n[38] \n91.72 84.79 84.76 98.08 84.66 \n \nBased on these results, it was decided to use ConvNeXt and \nSwin Transformer as the feature extraction sub-networks in the \nproposed fusion model. \nE. ABLATION STUDY \nTables 5 and 6 present the results of each step of the ablation \nstudy performed with the proposed model on the ISIC2019 and \nacne datasets, respectively. In the first step, by adding the \ndenoising module to ConvNeXt, the values of all metrics were \nimproved, compared to those of ConvNeXt, except for \nprecision on the acne dataset. In the second step , by \nincorporating the ECA attention module alone into ConvNeXt, \nthe values of all metrics were improved, compared to those of \nConvNeXt, except for precision on the ISIC2019 dataset and \nfor recall, specificity, and F1-score on the acne dataset. In the \nthird step , by directly fusing the features extracted by \nConvNeXt and Swin Transformer, the values of all metrics \nwere improved, compared to those separately achieved by \nConvNeXt and Swin Transformer, except for precision w.r.t. \nConvNeXt on the ISIC2019 dataset. In the fourth step, by \nadding AFF modules to the combined scheme of ConvNeXt \nand Swin Transformer, further improvement was achieved on \nall metrics, compared to the previous step, except for recall, \nspecificity, and F1-score on the acne dataset. Finally, in the last \nstep, when using all modules, resulting the proposed \nConvNeXt-ST-AFF model, top values of all metrics were \nachieved, except for precision on the ISIC2019 dataset. \nTable 7 displays the classification accuracy of the proposed \nConvNeXt-ST-AFF model under different hyperparameter \nsettings. Due to hardware limitations, we conducted the testing \nin two steps. In the first step, with a fixed batch size of 16, we \nevaluated the impact of different initial learning rates on the \nmodel's classification performance. We experimented with \nthree commonly used initial learning rates: 0.01, 0.001, and \n0.0005. The experimental results in Table 7 demonstrate that \nthe proposed model perfo rms best on the ISIC2019  dataset \n(resp. private dataset), when using an initial learning rate of \n0.01 ( resp. 0.0005). In the second step , we set the  initial \nlearning rate to the optimal value, determined for the particular \ndataset in the previous step, and tested the influence of different \nbatch sizes on the model's  classification performance. The \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nresults indicate that when the batch size is set to 16, the model \nachieves its highest classification performance on both datasets. \n \nTABLE 5. ABLATION STUDY RESULTS ON THE PUBLIC ISIC2019 DATASET \nClassifiers Evaluation metrics (%) \nAcc Pre Rec Spec F1 \nConvNeXt 90.65 90.41 84.40 98.25 87.17 \nSwin \nTransformer 89.39 89.13 83.66 97.98 86.10 \nConvNeXt \n+ Denoising  91.47 91.31 84.77 98.36 87.68 \nConvNeXt + \nECA 91.40 89.90 85.09 98.39 87.27 \nSwin \nTransformer + \nConvNeXt \n91.31 89.49 85.35 98.40 87.27 \nSwin \nTransformer + \nConvNeXt + \nAFF \n91.76 91.07 85.91 98.43 88.26 \nSwin \nTransformer + \nConvNeXt + \nAFF + ECA + \nDenoising (i.e., \nConvNeXt-ST-\nAFF) \n92.16 90.96 87.08 98.54 88.83 \n \nTABLE 6. ABLATION STUDY RESULTS ON THE PRIVATE ACNE DATASET \nClassification \nmethod \nEvaluation Metrics (%) \nAcc Pre Rec Spec F1 \nConvNeXt 90.51 85.65 82.37 97.81 83.52 \nSwin \nTransformer 91.72 84.79 84.76 98.08 84.66 \nConvNeXt + \nDenoising 91.20 85.53 83.25 98.00 83.97 \nConvNeXt + \nECA 91.03 86.16 81.13 97.71 82.78 \nSwin \nTransformer \n+ ConvNeXt \n92.24 87.04 86.05 98.20 86.47 \nSwin \nTransformer \n+ ConvNeXt \n+ AFF \n92.41 88.38 84.98 98.19 86.16 \nSwin \nTransformer \n+ ConvNeXt \n+ AFF + \nECA + \nDenoising \n(i.e., \nConvNeXt-\nST-AFF) \n93.27 88.67 89.21 98.49 88.78 \n \nTABLE 7. THE CONVNEXT-ST-AFF CLASSIFICATION ACCURACY (%)                       \nUNDER DIFFERENT HYPERPARAMETER SETTINGS \nHyperparameter Value Datasets \nISIC2019 Acne \nInitial learning \nrate \n0.01 92.16 92.24 \n0.001 91.82 92.93 \n0.0005 90.30 93.27 \nBatch size 8 90.66 92.93 \n16 92.16 93.27 \n \nF. RESULTS ON PUBLIC ISIC2019 DATASET \nFigure 13 illustrates the classification performance of the \nproposed ConvNeXt-ST-AFF model on ISIC2019 dataset in the \nform of a confusion matrix , where t he number of correctly \nclassified images is represented by the diagonal values. \n \n \nFIGURE 13. The confusion matrix of the training results of the proposed \nConvNeXt-ST-AFF model on the public ISIC2019 dataset. \n \nBased on th is confusion matrix, further calculations were \nperformed to obtain the values of evaluation metrics for each \nclass, as shown in Table 8. As can be seen from this table, the \nproposed model performs especially well on the BCC, NV, and \nVASC classes, according to all metrics. However, for the AK \nand DF classes, the values of precision, recall, and F1-score are \nnot ideal due to the fact that these classes contain fewer images \nand images with higher similarity than the other classes. \n \nTABLE 8. TRAINING RESULTS OF THE PROPOSED CONVNEXT-ST-AFF \nMODEL ON THE PUBLIC ISIC2019 DATASET \nClass Evaluation Metrics (%) \nPre Rec Spec F1 \nAK 83.54 76.30 99.47 79.76 \nBCC 92.84 95.63 98.89 94.21 \nBKL 89.31 86.07 98.81 87.66 \nDF 97.37 78.72 99.98 87.06 \nMEL 89.93 84.96 97.93 87.37 \nNV 94.14 96.62 93.77 95.36 \nSCC 80.60 86.40 99.47 83.40 \nVASC 100.00 92.00 100.00 95.83 \nG. RESULTS ON PRIVATE ACNE DATASET \nFigure 1 4 presents the classification performance of the \nproposed ConvNeXt-ST-AFF model on the acne dataset in the \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \nform of a confusion matrix , where t he number of correctly \nclassified images is represented by the diagonal values. \n \nFIGURE 14. The confusion matrix of the training results of the proposed \nConvNeXt-ST-AFF model on the private acne dataset. \n \nBased on th is confusion matrix, further calculations were \nperformed to obtain the values of evaluation metrics for each \nclass, as shown in Table 9. As can be observed from this table, \nthe proposed model performs especially well on the AC, DLE, \nand ON classes. However, its performance for the SD class is \nnot ideal, because this class contains fewer images and images \nwith higher similarity than the other classes. \n \nTABLE 9. TRAINING RESULTS OF THE PROPOSED CONVNEXT-ST-AFF \nMODEL ON THE PRIVATE ACNE DATASET \nClass Evaluation Metrics (%) \nPre Rec Spec F1 \nAC 97.52 97.52 96.89 97.52 \nDLE 89.29 100.00 99.46 94.34 \nME 93.42 88.75 99.00 91.03 \nON 83.61 91.07 98.09 87.18 \nROS 88.89 90.32 98.65 89.60 \nSD 79.31 67.65 98.90 73.02 \nH. DISCUSSION \nTable 10 shows performance comparison of  the proposed \nmodel with state-of-the-art-models, based on their respective \nresults reported in the corresponding literature sources ( \"-\" in \nthe table indicates missing data in a source) . The proposed \nmodel clearly outperforms all other models, according to all \nevaluation metrics used, except for accuracy where the leader is \nthe model  described in [17]. Furthermore, compared to the \nlightweight models in [50] and [51], although the proposed \nmodel has a larger number of parameters, it has shown \nsignificant improvements in classification performance. \nV. CONCLUSION \nIn the presented study, we have effectively mitigated the impact \nof class imbalance on classification performance by employing \ndata augmentation. We also introduced an image denoising \nmodule for eliminating the artifacts, such as hair in \ndermoscopic images , and for enhancing the contrast. \nFurthermore, we proposed a model fusion approach with \nadaptive weights whose effectiveness has been validated. \n  \nTABLE 10. PERFORMANCE COMPARISON OF THE PROPOSED CONVNEXT-ST-\nAFF MODEL WITH STATE-OF-THE-ART-MODELS ON THE ISIC2019 DATASET.  \nModel \nEvaluation Metrics (%) Parameter \ncount \n(M) Acc Pre Rec Spec F1 \nWang et \nal. [50] 86.60 - - - - 66.7 \nHoang et \nal. [51] 82.56 - 82.56 97.51 - 1.8 \nKhan et al. \n[52] 85.35 82.61 73.30 - 77.68 - \nAyasha et \nal. [53] 88.00 86.00 80.00 - - - \nKhan et al. \n[54] 89.00 - - - - - \nMaqsood \net al. [17] 93.47 - 84.34 87.53 88.67 - \nSharafude\nen et al. \n[55] \n91.93 - 85.58 98.29 - - \nConvNeXt\n-ST-AFF \n(proposed \nmodel) \n92.16 90.96 87.08 98.54 88.83 178.5 \n \nThe proposed ConvNeXt-ST-AFF model has been tested on \nthe publicly available ISIC2019 dataset and a private acne \ndataset, showing excellent results on both . Moreover, the \ncomparison of  its results  on the ISIC2019 dataset  to those \nreported in the literature for state -of-the-art c lassification \nmodels clearly has demonstrated that the proposed model \noutperforms them, according to all evaluation metrics used, \nexcept for one of the models according to accuracy only. \nThe main limitation of this study lies in the increased \ncomputational burden resulting from the model fusion approach. \nIn the future, we plan to optimize the network architecture of \nthe proposed model  by incorporating the latest lightweight \nmodel design techniques , such as MobileViT  [56] and \nFasterNet [57], in order to reduce computational demands. \nAdditionally, we aim to further refine the denoising module to \nimprove its effectiveness in eliminating image artifacts and \nreducing the influence of irrelevant features on model training. \nREFERENCES \n[1] M. A. Khan, M. Sharif, T. Akram, S. A. C. Bukhari, and R. S. J. P. \nR. L. Nayak, \"Developed Newton -Raphson based deep features \nselection framework for skin lesion recognition,\" vol. 129, pp. \n293-303, 2020. \n[2] M. Nawaz et al., \"Skin cancer detection fr om dermoscopic images \nusing deep learning and fuzzy k â€means clustering,\" vol. 85, no. \n1, pp. 339-351, 2022. \n[3] M. A. Khan, T. Akram, Y. -D. Zhang, and M. J. P. R. L. Sharif, \n\"Attributes based skin lesion detection and recognition: A mask \nRCNN and transfer learning -based deep learning framework,\" vol. \n143, pp. 58-66, 2021. \n[4] M. E. Celebi, N. Codella, A. J. I. j. o. b. Halpern, and h. \ninformatics, \"Dermoscopy image analysis: overview and future \ndirections,\" vol. 23, no. 2, pp. 474 -478, 2019. \n[5] J. Lin et al., \"Evaluation of dermoscopic algorithm for seborrhoeic \nkeratosis: a prospective study in 412 patients,\" vol. 28, no. 7, pp. \n957-962, 2014. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \n[6] Q. Abbas, I. Garcia, and M. J. B. j. o. b. s. Rashid, \"Automatic \nskin tumour border detection for digital dermoscopy using a new \ndigital image analysis scheme,\" vol. 67, no. 4, pp. 177 -183, 2010. \n[7] Y. Xie, J. Zhang, Y. Xia, and C. J. I. t. o. m. i. Shen, \"A mutual \nbootstrapping model for automated skin lesion segmentation and \nclassification,\" vol. 39, no. 7, pp. 2482 -2493, 2020. \n[8] F. M. Walter et al., \"Using the 7-point checklist as a diagnostic aid \nfor pigmented skin lesions in general practice: a diagnostic \nvalidation study,\" vol. 63, no. 610, pp. e345 -e353, 2013. \n[9] J. D. Jensen, B. E. J. T. J. o. c. Elewski, and a. dermatology, \"The \nABCDEF rule: combining the â€œABCDE ruleâ€ and the â€œugly \nduckling signâ€ in an effort to improve patient self -screening \nexaminations,\" vol. 8, no. 2, p. 15, 2015.  \n[10] S. Dey, R. Roychoudhury, S. Malakar, and R. J. A. S. C. Sarkar, \n\"An optimized fuzzy ensemble of convolutional neural networks \nfor detecting tuberculosis from Chest X -ray images,\" vol. 114, p. \n108094, 2022. \n[11] M. A. Khan  et al. , \"An integrated framework of skin lesion \ndetection and recognition through saliency method and optimal \ndeep neural network features selection,\" vol. 32, pp. 15929 -15948, \n2020. \n[12] A. Dosovitskiy  et al. , \"An image is worth 16x16 words: \nTransformers for image recognition at scale,\" 2020.  \n[13] T. Wang  et al. , \"O -Net: a novel framework with deep fusion of \nCNN and transformer for simultaneous segmentation and \nclassification,\" vol. 16, p. 876065, 2022.  \n[14] X. He, E.-L. Tan, H. Bi, X. Zhang, S. Zhao, and B. J. M. I. A. Lei, \n\"Fully transformer network for skin lesion analysis,\" vol. 77, p. \n102357, 2022. \n[15] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. J. \nA. c. s. Shah, \"Transformers in vision: A survey,\" vol. 54, no. 10s, \npp. 1-41, 2022. \n[16] A. Mahbod et al., \"Fusing fine -tuned deep features for skin lesion \nclassification,\" vol. 71, pp. 19-29, 2019. \n[17] S. Maqsood and R. J. N. N. DamaÅ¡eviÄius, \"Multiclass skin lesion \nlocalization and classification using deep learning based features \nfusion and selection framework for smart healthcare,\" vol. 160, pp. \n238-258, 2023. \n[18] K. Ali, Z. A. Shaikh, A. A. Khan, and A. A. J. N. I. Laghari, \n\"Multiclass skin cancer classification using EfficientNets â€“a first \nstep towards preventing skin cancer,\" vol. 2, no. 4, p. 100034, \n2022. \n[19] A. Khamparia, P. K. Singh, P. Rani, D. Samanta, A. Khann a, and \nB. J. T. o. E. T. T. Bhushan, \"An internet of health things â€driven \ndeep learning framework for detection and classification of skin \ncancer using transfer learning,\" vol. 32, no. 7, p. e3963, 2021.  \n[20] K. Simonyan and A. J. a. p. a. Zisserman, \"Very deep \nconvolutional networks for large -scale image recognition,\" 2014. \n[21] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for \nimage recognition,\" in Proceedings of the IEEE conference on \ncomputer vision and pattern recognition , Las Vegas, NV, USA, \n27-30 June 2016, pp. 770-778. \n[22] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, \n\"Densely connected convolutional networks,\" in Proceedings of \nthe IEEE conference on computer vision and pattern recognition , \nHonolulu, HI, USA, 21-26 July 2017, pp. 4700 -4708. \n[23] M. Tan and Q. Le, \"Efficientnet: Rethinking model scaling for \nconvolutional neural networks,\" in International conference on \nmachine learning , Long Beach, CA, USA, 10 -15 June 2019, pp. \n6105-6114: PMLR. \n[24] Z. Liu, H. Mao, C. -Y. Wu, C. Feichtenhofer, T. Darrell, and S. \nXie, \"A convnet for the 2020s,\" in Proceedings of the IEEE/CVF \nconference on computer vision and pattern recognition , New \nOrleans, LA, USA, 18-24 June 2022, pp. 11976 -11986. \n[25] J. Zhang, Y. Xie, Y. Xia, and C. J. I. t. o. m. i. Shen, \"Attention \nresidual learning for skin lesion classification,\" vol. 38, no. 9, pp. \n2092-2103, 2019. \n[26] Y. Wan, Y. Cheng, and M. J. A. I. Shao, \"MSLANet: multi -scale \nlong attention network for skin lesion classification,\" vol. 53, no. \n10, pp. 12580-12598, 2023. \n[27] S. J. N. C. Ayas and Applications, \"Multiclass skin lesion \nclassification in dermoscopic images using swin transformer \nmodel,\" vol. 35, no. 9, pp. 6713 -6722, 2023. \n[28] G. Cai, Y. Zhu, Y. Wu, X. Jiang, J. Ye, and D. J. T. V. C. Yang, \n\"A multimodal transformer to fuse images and metadata for skin \ndisease classification,\" pp. 1-13, 2022. \n[29] A. Mahbod et al., \"Transfer learning using a multi -scale and multi-\nnetwork ensemble for skin lesion classification,\" vol. 193, p. \n105475, 2020. \n[30] M. Nawaz  et al. , \"Melanoma segmentation: A framework of \nimproved DenseNet77 and UNET convolutional neural network,\" \nvol. 32, no. 6, pp. 2137-2153, 2022. \n[31] W. Wu, M. Chen, Y. Xiang, Y. Zhang, and Y. J. I. I. P. Yang, \n\"Recent progress in image denoising: A training strategy \nperspective,\" 2023. \n[32] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. J. I. t. o. i. p. Zhang, \n\"Beyond a gaussian denoiser: Residual learning of deep cnn for \nimage denoising,\" vol. 26, no. 7, pp. 3142 -3155, 2017. \n[33] X.-J. Mao, C. Shen, and Y. -B. J. a. p. a. Yang, \"Image restoration \nusing convolutional auto -encoders with symmetric skip \nconnections,\" 2016. \n[34] V. Rajinikanth, S. Kadry, R. DamaÅ¡eviÄius, D. Sankaran, M. A. \nMohammed, and S. Chander, \"Skin melanoma segmentation using \nVGG-UNet with Adam/SGD optimizer: a study,\" in 2022 Third \nInternational Conference on Intelligent Computing \nInstrumentation and Control Technologies (ICICICT) , Kannur, \nIndia, 11-12 August 2022, pp. 982-986: IEEE. \n[35] J. Deng, W. Dong, R. Socher, L. -J. Li, K. Li, and L. Fei -Fei, \n\"Imagenet: A large -scale hierarchical image database,\" in 2009 \nIEEE conference on computer vision and pattern recognition , \nMiami, FL, 20-25 June 2009, pp. 248 -255: Ieee. \n[36] T.-Y. Lin, P. DollÃ¡r, R. Girshick, K. He, B. Hariharan, and S. \nBelongie, \"Feature pyramid networks for object detection,\" in \nProceedings of the IEEE conference on computer vision and \npattern recognition , Honolulu, HI, USA, 21 -26 July 2017, pp. \n2117-2125. \n[37] Y. Dai, F. Gieseke, S. Oehmcke, Y. Wu, and K. Barnard, \n\"Attentional feature fusion,\" in Proceedings of the IEEE/CVF \nwinter conference on applications of computer vision , Waikoloa, \nHI, USA, 3-8 January 2021, pp. 3560 -3569. \n[38] Z. Liu  et al. , \"Swin transformer: Hierarchical vision transformer \nusing shifted windows,\" in Proceedings of the IEEE/CVF \ninternational conference on computer vision , Montreal, QC, \nCanada, 10-17 October 2021, pp. 10012-10022. \n[39] Z.-H. Zhou and Z.-H. Zhou, Ensemble learning. Springer, 2021. \n[40] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, \"ECA -Net: \nEfficient channel attention for deep convolutional neural \nnetworks,\" in Proceedings of the IEEE/CVF conference on \ncomputer vision and pattern recognition , Seattle, WA, USA, 13 -\n19 June 2020, pp. 11534 -11542. \n[41] X. Zhang, X. Zhou, M. Lin, and J. Sun, \"Shufflenet: An extremely \nefficient convolutional neural network for mobile devices,\" in \nProceedings of the IEEE conference on computer vision and \npattern recognition , Salt Lake City, UT, USA, 18 -23 June 2018, \npp. 6848-6856. \n[42] O. O. Abayomi -Alli, R. Damasevicius, S. Misra, R. Maskeliunas, \nA. J. T. J. o. E. E. Abayomi -Alli, and C. Sciences, \"Malignant skin \nmelanoma detection using image augmentation by oversamplingin \nnonlinear lower-dimensional embedding manifold,\" vol. 29, no. 8, \npp. 2600-2614, 2021. \n[43] Z. Qin, Z. Liu, P. Zhu, Y. J. C. M. Xue, and P. i. Biomedicine, \"A \nGAN-based image synthesis method for skin lesion classification,\" \nvol. 195, p. 105568, 2020. \n[44] F. Perez, C. Vasconcelos, S. Avila, and E. Valle, \"Data \naugmentation for skin lesion analysis,\" in OR 2.0 Context -Aware \nOperating Theaters, Computer Assisted Robotic Endoscopy, \nClinical Image-Based Procedures, and Skin Image Analysis: First \nInternational Workshop, OR 2.0 2018, 5th International \nWorkshop, CARE 2018, 7th International Workshop, CLIP 2018, \nThird International Workshop, ISIC 2018, Held in Conjunction \nwith MICCAI 2018, Granada, Spain, September 16 and 20, 2018, \nProceedings 5 , Spain, Granada, 16 -20 September 2018, pp. 303 -\n311: Springer. \n[45] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, \"Randaugment: \nPractical automated data augmentation with a reduced search \nspace,\" in Proceedings of the IEEE/CVF conference on computer \nvision and pattern recognition workshops , Seattle, WA, USA, 14 -\n19 June 2020, pp. 702-703. \n[46] C. Shorten and T. M. J. J. o. b. d. Khoshgoftaar, \"A survey on \nimage data augmentation for deep learning,\" vol. 6, no. 1, pp. 1 -\n48, 2019. \n[47] Y. Li and L. J. S. Shen, \"Skin lesion analysis towards melanoma \ndetection using deep learning network,\" vol. 18, no. 2, p. 556, \n2018. \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n \n8 VOLUME XX, 2023 \n[48] D. L. Olson and D. Delen, Advanced data mining techniques . \nSpringer Science & Business Media, 2008.  \n[49] B. Heo, S. Yun, D. Han, S. Chun, J. Choe, and S. J. Oh, \n\"Rethinking spatial dimensions of vision transformers,\" in \nProceedings of the IEEE/CVF International Conference on \nComputer Vision, Montreal, QC, Canada, 10 -17 October 2021, pp. \n11936-11945. \n[50] Y. Wang, Y. Wang, J. Cai, T. K. Lee, C. Miao, and Z. J. J. M. I. \nA. Wang, \"Ssd -kd: A self -supervised diverse knowledge \ndistillation method for lightweight skin lesion classification using \ndermoscopic images,\" vol. 84, p. 102693, 2023.  \n[51] L. Hoang, S. -H. Lee, E. -J. Lee, and K. -R. J. A. S. Kwon, \n\"Multiclass skin lesion classification using a novel lightweight \ndeep learning framework for smart healthcare,\" vol. 12, no. 5, p. \n2677, 2022. \n[52] M. A. Khan, T. Akram, M. Sharif, S. Kadry, Y. J. C. Nam, \nMaterials, and Continua, \"Computer Decision Support System for \nSkin Cancer Localization and Classification,\" vol. 68, no. 1, 2021.  \n[53] A. H. Jui, S. Sharnami, and A. Islam, \"A CNN Based Approach to \nClassify Skin Cancers using Transfer Learning,\" in 2022 25th \nInternational Conference on Computer and Information \nTechnology (ICCIT) , New York City, USA, 4 March 2022, pp. \n1063-1068: IEEE. \n[54] M. Attique Khan, M. Sharif, T. Akram, S. Kadry, and C. H. J. I. J. \no. I. S. Hsu, \"A two â€stream deep neural network â€based \nintelligent system for complex skin cancer types classification,\" \nvol. 37, no. 12, pp. 10621 -10649, 2022. \n[55] M. J. M. T. Sharafudeen and Applications, \"Detecting skin lesions \nfusing handcrafted features in image network ensembles,\" vol. 82, \nno. 2, pp. 3155-3175, 2023. \n[56] S. Mehta and M. J. a. p. a. Rastegari, \"Mobilevit: light -weight, \ngeneral-purpose, and mobile-friendly vision transformer,\" 2021.  \n[57] J. Chen  et al. , \"Run, Don't Walk: Chasing Higher FLOPS for \nFaster Neural Networks,\" in Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition , \nVancouver, BC, Canada, 17-24 June 2023, pp. 12021 -12031. \n \nSHENGNAN HAO received the B.S. degree from \nthe North China University of Science and \nTechnology, China, and the M.S. degree from the \nBeijing University of Technology, China, in 1996 and \n2009, respectively. She joined the North China \nUniversity of Science and Technology, in 1996, and \nbecame an Associate Professor, in 2009. Her current \nresearch interests include complex  systems, impulsive \nsystems, and stochastic control. \n \nLIGUO ZHANG was born in 199 8. He received a \nbachelor's degree  North China University of Science \nand Technology, in 20 20. He is currently pursuing \nthe masterâ€™s degree with the North China University \nof Science and Technology. His research interests \ninclude machine vision and algorithm research.  \n \n \n \n \n \nYANYAN JIANG was born in 1982. She received a \nbachelor's degree from Zhengzhou University in \n2012 and is currently pursuing a master's degree at \nHebei Agricultural University. Her research interests \ninclude Water pollution remediation and treatment.  \n \n \n \n \n \n \nJINGKUN WANG is currently a research assistant at \nTsinghua University, Beijing, China. She received her \nmasterâ€™s degree in industrial economics from Inner \nMongolia University of Technology in 2018. Her \ncurrent research interests include healthcare \nblockchain, Internet of Things and big data . \n \n \n \nZHANLIN JI (Member, IEEE) received his MEng \nand PhD degrees from Dublin City University and \nUniversity of Limerick in 2006 and 2010, \nrespectively. He is a Professor at the North China \nUniversity of Science and Technology, China, and \na Researcher of the Telecommunications Research \nCentre (TRC), University of Limerick, Ireland.  \nProf. Jiâ€™s research interests include the Ubiquitous \nConsumer Wireless World (UCWW), Internet of \nThings (IoT), cloud computing, big data \nmanagement, and data mining. He has authored/co -\nauthored 100+ research papers in refereed journals and conferences.  \n \nLI ZHAO received a B.S. degree and a Ph.D. \ndegree from Tsinghua University, Beijing, China, \nin 1997 and 2002, respectively. He currently \nserves as an Associate Professor at the Research \nInstitute of Information Technology, Tsinghua \nUniversity, China. His current research interests \ninclude mobile computing, Internet of Things \n(IoT), e -health systems, intelligent transportation \nsystems (ITS), home networking, machine \nlearning, and digital multimedia . \n \n \nIVAN GANCHEV (SM, IEEE) is an \nInternational Telecommunications Union (ITU -T) \nInvited Expert and an Institution of Engineering \nand Technology (IET) Invited Lecturer, currently \naffiliated with University of Limerick, Ireland, \nand University of Plovdiv â€œPaisii Hilendarskiâ€ and \nIMI-BAS, Bulgaria. He received his doctoral and \nengineering ( summa cum laude ) degrees from \nSaint-Petersburg University of \nTelecommunications in 1995 and 1989, \nrespectively. Prof. Ganchev participated in  more \nthan 40 international and national research projects. He has served on the \nTPC of more than 390 prestigious international \nconferences/symposia/workshops, and has authored/co -authored 1 \nmonographic book, 3 textbooks, 4 edited books, and more than 300 research \npapers in refereed international journals, books, and conference proceedings. \nProf. Ganchev is on the editorial board of and has served as a Guest Editor for \nmultiple international journals . \n \nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3324042\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7609404921531677
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7040430903434753
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6686685681343079
    },
    {
      "name": "Transformer",
      "score": 0.6514995098114014
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5515881180763245
    },
    {
      "name": "Feature extraction",
      "score": 0.5078595280647278
    },
    {
      "name": "Deep learning",
      "score": 0.49731066823005676
    },
    {
      "name": "Artificial neural network",
      "score": 0.41278210282325745
    },
    {
      "name": "Machine learning",
      "score": 0.3274282217025757
    },
    {
      "name": "Engineering",
      "score": 0.0931042730808258
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}