{
  "title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages",
  "url": "https://openalex.org/W4385570522",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5009189077",
      "name": "Ayyoob ImaniGooghari",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014957657",
      "name": "Peiqin Lin",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universit√§t M√ºnchen"
      ]
    },
    {
      "id": "https://openalex.org/A5032918483",
      "name": "Amir Hossein Kargaran",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universit√§t M√ºnchen"
      ]
    },
    {
      "id": "https://openalex.org/A5087749069",
      "name": "Silvia Severini",
      "affiliations": [
        "Ludwig-Maximilians-Universit√§t M√ºnchen"
      ]
    },
    {
      "id": "https://openalex.org/A5022974084",
      "name": "Masoud Jalili Sabet",
      "affiliations": [
        "Ludwig-Maximilians-Universit√§t M√ºnchen"
      ]
    },
    {
      "id": "https://openalex.org/A5019973471",
      "name": "Nora Kassner",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universit√§t M√ºnchen"
      ]
    },
    {
      "id": "https://openalex.org/A5104101020",
      "name": "Chunlan Ma",
      "affiliations": [
        "Munich Center for Machine Learning"
      ]
    },
    {
      "id": "https://openalex.org/A5027452875",
      "name": "Helmut Schmid",
      "affiliations": [
        "Ludwig-Maximilians-Universit√§t M√ºnchen"
      ]
    },
    {
      "id": "https://openalex.org/A5051693368",
      "name": "Andr√© F. T. Martins",
      "affiliations": [
        null,
        "Instituto Superior T√©cnico",
        "Ludwig-Maximilians-Universit√§t M√ºnchen",
        "Instituto de Telecomunica√ß√µes"
      ]
    },
    {
      "id": "https://openalex.org/A5030615769",
      "name": "Fran√ßois Yvon",
      "affiliations": [
        "Centre National de la Recherche Scientifique",
        "Institut Syst√®mes Intelligents et de Robotique",
        "Sorbonne Universit√©"
      ]
    },
    {
      "id": "https://openalex.org/A5071144367",
      "name": "Hinrich Sch√ºtze",
      "affiliations": [
        "Munich Center for Machine Learning",
        "Ludwig-Maximilians-Universit√§t M√ºnchen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2759088880",
    "https://openalex.org/W4367692612",
    "https://openalex.org/W4385724599",
    "https://openalex.org/W3152788712",
    "https://openalex.org/W2983577274",
    "https://openalex.org/W4287890953",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3118036215",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W4294959075",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W2847467100",
    "https://openalex.org/W2726119835",
    "https://openalex.org/W4226395792",
    "https://openalex.org/W2805974756",
    "https://openalex.org/W3154301048",
    "https://openalex.org/W4385571607",
    "https://openalex.org/W4281762188",
    "https://openalex.org/W3200011783",
    "https://openalex.org/W4221165891",
    "https://openalex.org/W3213418658",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W2807280702",
    "https://openalex.org/W4229014795",
    "https://openalex.org/W3176765167",
    "https://openalex.org/W2120120683",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W3029164262",
    "https://openalex.org/W4226091929",
    "https://openalex.org/W4287024328",
    "https://openalex.org/W4287890112",
    "https://openalex.org/W2996822578",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W630532510"
  ],
  "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, ‚Äúhelp‚Äù from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should notlimit NLP to a small fraction of the world‚Äôs languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 1082‚Äì1117\nJuly 9-14, 2023 ¬©2023 Association for Computational Linguistics\nGlot500:\nScaling Multilingual Corpora and Language Models to 500 Languages\nAyyoob Imani‚àó1,2, Peiqin Lin‚àó1,2, Amir Hossein Kargaran1,2, Silvia Severini1,\nMasoud Jalili Sabet1, Nora Kassner1,2, Chunlan Ma1,2,\nHelmut Schmid1, Andr√© F. T. Martins3,4,5, Fran√ßois Yvon6 and Hinrich Sch√ºtze1,2\n1CIS, LMU Munich, Germany 2Munich Center for Machine Learning (MCML), Germany\n3Instituto Superior T√©cnico (Lisbon ELLIS Unit)4Instituto de Telecomunica√ß√µes\n5Unbabel 6Sorbonne Universit√©, CNRS, ISIR, France\n{ayyoob, linpq, amir, silvia}@cis.lmu.de\nAbstract\nThe NLP community has mainly focused on\nscaling Large Language Models (LLMs)ver-\ntically, i.e., making them better for about 100\nlanguages. We instead scale LLMshorizon-\ntally: we create, through continued pretraining,\nGlot500-m, an LLM that covers 511 predom-\ninantly low-resource languages. An impor-\ntant part of this effort is to collect and clean\nGlot500-c, a corpus that covers these 511 lan-\nguages and allows us to trainGlot500-m. We\nevaluateGlot500-monfivediversetasksacross\nthese languages. We observe large improve-\nments for both high-resource and low-resource\nlanguages compared to an XLM-R baseline.\nOur analysis shows that no single factor ex-\nplains the quality of multilingual LLM rep-\nresentations. Rather, a combination of fac-\ntors determines quality including corpus size,\nscript, ‚Äúhelp‚Äù from related languages and the\ntotal capacity of the model. Our work ad-\ndresses an important goal of NLP research: we\nshould not limit NLP to a small fraction of the\nworld‚Äôs languages and instead strive to support\nas many languages as possible to bring the ben-\nefits of NLP technology to all languages and\ncultures. Code, data and models are available\nat https://github.com/cisnlp/Glot500.\n1 Introduction\nTheNLPcommunityhasmainlyfocusedonscaling\nLarge Language Models (LLMs)vertically, i.e.,\ndeepeningtheirunderstandingofhigh-resourcelan-\nguages by scaling up parameters and training data.\nWhile this approach has revolutionized NLP, the\nachievements are largely limited to high-resource\nlanguages. Examplesof‚Äúvertical‚ÄùLLMsareGPT3\n(Brown et al., 2020), PaLM (Chowdhery et al.,\n2022) and Bloom (BigScience et al., 2022). In this\npaper, we createGlot500-m, a model that instead\nfocuses on scaling multilingual LLMshorizontally,\ni.e.,scalingtoalargenumberoflanguagesthegreat\n*Equal contribution.\nmajority of which is low-resource. As LLMs are\nessentialforprogressinNLP,lackofLLMssupport-\ninglow-resourcelanguagesisaseriousimpediment\ntobringingNLPto allof theworld‚Äôslanguagesand\ncultures. Our goal is to address this need with the\ncreation of Glot500-m.1\nExisting multilingual LLMs support only about\n100(Conneauetal.,2020)outofthe7000languages\nof the world. These supported languages are the\nones for which large amounts of training data are\navailable through projects such as Oscar (Su√°rez\net al., 2019) and the Wikipedia dumps.2 Following\nSiddhantetal.(2022),werefertothe100languages\ncovered by XLM-R (Conneau et al., 2020) ashead\nlanguagesand to the remaining languages astail\nlanguages. This terminology is motivated by the\nskewed distribution of available data per language:\nfor the best-resourced languages there are huge\ncorpora available, but for the long tail of languages,\nonly small corpora exist. This is a key problem we\naddress: the availability of data for tail languages\nis limited compared to head languages. As a result,\ntail languages have often been ignored by language\ntechnologies (Joshi et al., 2020).\nAlthough there exists some work on machine\ntranslation for a large number of tail languages\n(Costa-juss√† et al., 2022; Bapna et al., 2022), ex-\nisting LLMs for tail languages are limited to a\nrelatively small number of languages (Wang et al.,\n2019; Alabi et al., 2022; Wang et al., 2022). In this\npaper,weaddressthisgap. Ourworkhasthreeparts.\n(i) Corpus collection. We collectGlot2000-c, a\ncorpus covering thousands of tail languages. (ii)\nModel training. Using Glot500-c, a subset of\nGlot2000-c,wetrain Glot500-m,anLLMcovering\n511 languages. (iii)Validation. We conduct an\nextensive evaluation of the quality ofGlot500-m‚Äôs\n1In concurrent work, Adebara et al. (2022) train a multilin-\ngualmodelfor517Africanlanguagesona42gigabytecorpus,\nbut without making the model available.\n2https://dumps.wikimedia.org/\n1082\nrepresentations of tail languages on a diverse suite\nof tasks.\nInmoredetail, corpuscollection considersthree\nmajor sources: websites that are known to publish\ncontent in specific languages, corpora with clas-\nsified multilingual content and datasets published\nin specific tail languages. The resulting dataset\nGlot2000-c comprises 700GB in 2266 languages\ncollected from‚âà150 sources. After cleaning and\ndeduplication, we create the subsetGlot500-c, con-\nsisting of 511 languages and 534language-scripts\n(where we define a language-script as a combina-\ntion of ISO 639-33 and script) to trainGlot500-m.\nOur criterion for including a language-script in\nGlot500-cis that it includes more than 30,000 sen-\ntences.\nModeltraining. Totrain Glot500-m,weemploy\nvocabulary extension and continued pretraining.\nXLM-R‚Äôs vocabulary is extended with new tokens\ntrained onGlot500-c. We then perform continued\npretraining of XLM-R with the MLM objective\n(Devlin et al., 2019).\nValidation. We comprehensively evaluate\nGlot500-m on a diverse suite of natural language\nunderstanding, sequence labeling and multilingual\ntasksforhundredsoflanguages. Theresultsdemon-\nstrate thatGlot500-m performs better than XLM-\nR-B (XLM-R-base) for tail languages by a large\nmarginwhileperformingcomparably(orbetter)for\nhead languages.\nPrevious work on multilinguality has been hin-\ndered by the lack of LLMs supporting a large num-\nber of languages. This limitation has led to studies\nbeing conducted in settings dissimilar from real-\nworld scenarios. For example, Dufter and Sch√ºtze\n(2020) use synthetic language data. And the curse\nof multilinguality has been primarily studied for\na set of high-resource languages (Conneau et al.,\n2020). By creatingGlot500-m, we can investigate\nthese issues in a more realistic setting. We make\ncode, data and trained models available to foster\nresearch by the community on how to include hun-\ndreds of languages that are currently ill-served by\nNLP technology.\nContributions. (i) We train the multilingual\nmodel Glot500-m on a600GB corpus, covering\nmore than 500 diverse languages, and make it pub-\nlicly available athttps://github.com/cisnlp/\nGlot500. (ii) We collect and cleanGlot500-c, a\ncorpus that covers these diverse languages and al-\n3https://iso639-3.sil.org/code_tables/639\nlows us to trainGlot500-m, and will make as much\nofitpubliclyavailableaspossible. (iii)Weevaluate\nGlot500-monpseudoperplexityandonfivediverse\ntasks across these languages. We observe large im-\nprovements for low-resource languages compared\nto an XLM-R baseline. (iv) Our extensive analysis\nshows that no single factor explains the quality of\nmultilingual LLM representations. Rather, a com-\nbination of factors determines quality including\ncorpus size, script, ‚Äúhelp‚Äù from related languages\nand the total capacity of the model. (v) Our work\naddresses an important goal of NLP research: we\nshould not limit NLP to a relatively small number\nof high-resource languages and instead strive to\nsupport as many languages as possible to bring the\nbenefits of NLP to all languages and cultures.\n2 Related Work\nTraining multilingual LLMs using the masked lan-\nguage modeling (MLM) objective is effective to\nachieve cross-lingual representations (Devlin et al.,\n2019; Conneau et al., 2020). These models can be\nfurther improved by incorporating techniques such\nasdiscriminativepre-training(Chietal.,2022)and\ntheuseofparalleldata(Yangetal.,2020;Chietal.,\n2021). However, this primarily benefits a limited\nset of languages with large corpora.\nRecent research has attempted to extend exist-\ning LLMs to languages with limited resources.\nWang et al. (2019) propose vocabulary extension;\nEbrahimi and Kann (2021) investigate adaptation\nmethods, including MLM and Translation Lan-\nguageModel(TLM)objectivesandadapters;Alabi\netal.(2022)adaptXLM-Rto17Africanlanguages;\nWang et al. (2022) expand language models to\nlow-resource languages using bilingual lexicons.\nAlternatively, parameter-efficient fine-tuning\nadapts pre-trained models to new languages by\ntraining a small set of weights effectively (Zhao\netal.,2020;Pfeifferetal.,2021;Anselletal.,2022).\nPfeiffer et al. (2022) address the ‚Äúcurse of multilin-\nguality‚Äù by sharing a part of the model among all\nlanguagesandhavingseparatemodulesforeachlan-\nguage. We show that the common perception that\nmultilingualityincreasesasweaddmorelanguages,\nuntil, fromsomepoint, itstartsdecreasing, isnaive.\nThe amount of available data per language and the\nsimilarity between languages also play important\nroles (¬ß6.8).\nAnother approach trains LLMs from scratch for\nalimitednumberoftaillanguages;e.g.,AfriBERTa\n1083\n(Oguejietal.,2021a)andIndicNLPSuite(Kakwani\netal.,2020)areLLMsfor11Africanlanguagesand\n11 Indic languages. In concurrent work, Adebara\net al. (2022) train a multilingual model for 517\nAfrican languages on a 42 GB corpus, but without\nmaking the model available and with an evaluation\non a smaller number of languages than ours.\nClosely related to our work on corpus creation,\nBapna et al. (2022) and Costa-juss√† et al. (2022)\nalsocreateNLPresourcesforalargenumberoftail\nlanguages. They train a language identifier model\nandextracttextualdatafortaillanguagesfromlarge-\nscale web crawls. This approach is effective, but\nit requires significant computational resources and\nnative speakers for all tail languages. This is hard\nto do outside of large corporations. Bapna et al.\n(2022) have not made their data available. Costa-\njuss√† et al. (2022) have only released a portion of\ntheir data in around 200 languages.\nA key benefit of ‚Äúhorizontally‚Äù scaled multilin-\ngual LLMs is transfer from high- to low-resource\nlanguages. Ourevaluationsuggeststhat Glot500-m\nexcels at this, but this is not the main focus of our\npaper. There is a large body of work on crosslin-\ngual transfer: (Artetxe and Schwenk, 2019; Imani-\nGooghari et al., 2022; Lauscher et al., 2020; Con-\nneau et al., 2020; Turc et al., 2021; Fan et al., 2021;\nSeverini et al., 2022; Choenni and Shutova, 2022;\nWang et al., 2023), inter alia.\n3 Glot2000-c\n3.1 Data Collection\nOne of the major challenges in developing NLP\ntechnologies for tail languages is the scarcity of\nhigh-qualitytrainingdata. Inthiswork,wepropose\na lightweight methodology that is easily replicable\nfor academic labs. We identify tail language data\npreviouslypublishedbyresearchers,publishersand\ntranslators and then crawl or download them. By\ncrawling a few websites and compiling data from\naround 150 different datasets, we amass more than\n700GB of text in 2266 languages. We will refer\nto these sources of data asdata sources. Our data\ncovers many domains, including religious texts,\nnews articles and scientific papers. Some of the\ndata sources are high-quality, verified by native\nspeakers, translators and linguists. Others are less\nreliable such as web crawls and Wikipedia dumps.\nIt is therefore necessary to clean the data. For a list\nof data sources, see ¬ßC.\n3.2 Language-Scripts\nSomelanguagesarewritteninmultiplescripts;e.g.,\nTajik is written in both Cyrillic and Arabic scripts.\nSome data sources indicate the script, but others\neither do not or provide mixed text in multiple\nscripts. We detect the script for each sentence and\ntreat each language-script as a separate entity.\n3.3 Ngram LMs and Language Divergence\nWe train a 3-gram character-level language model\nùëÄùëñ for each language-script ùêøùëñ, using KenLM\n(Heafield, 2011). We refer to the perplexity calcu-\nlated for the corpus of languageùêøùëñ using language\nmodel ùëÄùëó as PP(ùëÄùëó,ùêøùëñ). Similar to Gamallo\net al. (2017), we define a perplexity-based diver-\ngence measure of languagesùêøùëñ and ùêøùëó as:\nDùêøùëñ ,ùêøùëó = max (PP(ùëÄùëó,ùêøùëñ),PP(ùëÄùëñ,ùêø ùëó))\nWe useDto filter out noisy data in ¬ß3.4 and study\nthe effect of similar languages in LLM training in\n¬ß6.7 and ¬ß6.8. For more details, see ¬ßA.\n3.4 Data Cleaning\nTo remove noise, we use chunk-level and corpus-\nlevel filters.\nWhile some sources are sentence-split, others\nprovide multiple sentences (e.g., a paragraph) as\none chunk. Chunk-level filters process each chunk\noftextfromadatasourceasaunit,withoutsentence-\nsplitting. Some chunk-level filters are based on the\nnotion of word: we use white space tokenization\nwhenpossibleandotherwiseresorttosentencePiece\n(KudoandRichardson,2018)trainedbyCosta-juss√†\net al. (2022).\nAs chunk-level filters, we employ thesentence-\nlevel filtersSF1‚ÄìSF5 from BigScience ROOTS\n(Lauren√ßon et al., 2022).\nSF1Character repetition. If the ratio of repeated\ncharacters is too high, it is likely that the sentence\nhas not enough textual content.\nSF2 Word repetition. A high ratio of repeated\nwords indicates non-useful repetitive content.\nSF3 Special characters. Sentences with a high\nratio of special characters are likely to be crawling\nartifacts or computer code.\nSF4Insufficientnumberofwords. Sincetraining\nlanguage models requires enough context, very\nsmall chunks of text are not useful.\nSF5Deduplication. Iftwosentencesareidentical\nafter eliminating punctuation and white space, one\nis removed.\n1084\nlangsscriptssents‚Äômedian s‚Äô\nGlot2000-c 2266 35 2.3B 8K\nGlot500-c 511 30 1.5B 120K\nCosta-juss√† et al. (2022) 134 - 2.4B 3.3M\nBapna et al. (2022) 1503 - 1.7B 25K\nTable 1: Statistics forGlot2000-c, Glot500-c and ex-\nisting multilingual datasets: number of languages,\nscripts, sentences‚Äô and median number of sentences‚Äô\nper language-script.\nIn the rest of the paper, we refer to a chunk as\na sentence‚Äô. A sentence‚Äô can consist of a short\nsegment, a complete sentence or a chunk (i.e.,\nseveral sentences).\nCorpus-level filtersdetect if the corpus of a\nlanguage-script is noisy; e.g., the corpus is in an-\nother language or consists of non-meaningful con-\ntent such as tabular data. We employ filters CF1\nand CF2.\nCF1 In case ofmismatch between language\nand script, the corpus is removed; e.g., Chinese\nwritten in Arabic is unlikely to be Chinese.\nCF2 Perplexity mismatch. For each language-\nscript L1, we find its closest language-script L2:\nthe language-script with the lowest perplexity di-\nvergence (¬ß3.3). If L1 and L2 are not in the same\ntypological family, we check L1/L2 manually and\ntakeappropriateactionsuchasremovingthecorpus\n(e.g., if it is actually English) or correcting the ISO\ncode assigned to the corpus.\n3.5 Training Data: Glot500-c\nAmong the 2000+ language-scripts that we col-\nlected data for, after cleaning, most have too little\ndata for pretraining LLMs. It is difficult to quan-\ntify the minimum amount needed for pretraining.\nTherefore,wepickarelativelyhigh‚Äúsafe‚Äùthreshold,\n30,000sentences‚Äô, forinclusionoflanguage-scripts\ninmodeltraining. Thisallowsustotrainthemodel\neffectively and cover many low-resource languages.\nTable 1 givesGlot500-c statistics. See ¬ßB for a\nlist of language-scripts. We trainGlot500-m on\nGlot500-c; note that whileGlot500-c focuses on\ntail languages, it contains some data in head lan-\nguages which we include inGlot500-mtraining to\nprevent catastrophic forgetting.\nWe divide the corpus for each language into\ntrain/dev/test, reserving 1000 sentences‚Äô each for\ndev and test and using the rest for train. We pick\n1000 parallel verses if we have a Bible translation\nXLM-R-B XLM-R-L Glot500-m\nModel Size 278M 560M 395M\nVocab Size 250K 250K 401K\nTransformer Size 86M 303M 86M\nTable 2: Model sizes.Glot500-m and XLM-R-B have\nthe same transformer size, butGlot500-m has a larger\nvocabulary, resulting in an overall larger model.\nand add 500 each to test and dev. These parallel\nverses convey identical meanings and facilitate\ncrosslingual evaluation. We pretrain the model\nusing only the training data.\n4 Glot500-m\n4.1 Vocabulary Extension\nTo extend XLM-R‚Äôs vocabulary, we use Sentence-\nPiece(KudoandRichardson,2018)withaunigram\nlanguage model (Kudo, 2018) to train a tokenizer\nwith a vocabulary size of 250K onGlot500-c. We\nsample data from different language-scripts accord-\ning to a multinomial distribution, withùõº=.3. The\namount we sample for head languages is the same\nastaillanguageswiththelowestamount;thisfavors\ntail languages ‚Äì head languages are already well\nlearned by XLM-R. We merge the obtained tokens\nwith XLM-R‚Äôs vocabulary. About 100K new to-\nkens were in fact old tokens, i.e., already part of\nXLM-R‚Äôs vocabulary. We take the probabilities\nof the (genuinely) new tokens directly from Sen-\ntencePiece. After adding the 151K new tokens to\nXLM-R‚Äôs vocabulary (which has size 250K), the\nvocabulary size of Glot500-m is 401K.\nWe could also calculate probabilities of existing\nand new tokens over a mixture of original XLM-R\ntraining corpus andGlot500-c(Chung et al., 2020).\nFor head languages, the percentage of changed\ntokens using the new tokenizer compared to the\noriginal tokenizer ranges from 0.2% to 50%. How-\never, we found no relationship between percentage\nof changed tokens and change in performance on\ndownstream tasks. Thus, there was little effect of\ntokenization in our experiments.\n4.2 Continued Pretraining\nWe createGlot500-m by continued pretraining of\nXLM-R-B with the MLM objective. The opti-\nmizer used is Adam with betas (0.9, 0.999). Initial\nlearning rate: 5e-5. Each training step contains\na batch of 384 training samples randomly picked\nfrom all language-scripts. The sampling strategy\nacross language-scripts is the same as for vocabu-\n1085\n|head| |tail| measure (%)\nSentence Retrieval Tatoeba 70 28 Top10 Acc.\nSentence Retrieval Bible 94 275 Top10 Acc.\nText Classification 90 264 F1\nNER 89 75 F1\nPOS 63 28 F1\nRoundtrip Alignment 85 288 Accuracy\nTable 3: Evaluation tasks and measures. |head|/|tail|:\nnumber of head/tail language-scripts\nlary extension (¬ß4.1). We save checkpoints every\n10K steps and select the checkpoint with the best\naverage performance on downstream tasks by early\nstopping. Table2liststhesizesofXLM-R-B,XLM-\nR-Land Glot500-m. Exceptforalargervocabulary\n(¬ß4.1), Glot500-m has the same size as XLM-R-B.\nWetrain Glot500-monaserverwitheightNVIDIA\nRTX A6000 GPUs for two weeks.\nSimilartoXLM-R,weconcatenatesentences‚Äôof\na language-script and feed them as a stream to the\ntokenizer. The resulting output is then divided into\nchunks of 512 tokens and fed to the model.\n5 Experimental Setup\nFor most tail languages, there are no manually\nlabeledevaluationdata. Wethereforeadoptamixed\nevaluation strategy: based partly on human labels,\npartly on evaluation methods that are applicable\nto many languages without requiring gold data.\nTable 3 lists all our evaluation tasks.\nPerplexity Following Salazar et al. (2020), we\ncalculate pseudoperplexity (PPPL) over the held-\nout test set. PPPL is based on masking tokens\none-by-one (not left to right). Salazar et al. (2020)\ngive evidence that PPPL is a better measure of\nlinguistic acceptability compared to standard left-\nto-right perplexity.\nRoundtrip Alignment For assessing the quality\nof multilingual representations for a broad range of\ntail languages without human gold data, we adopt\nroundtrip evaluation (Dufter et al., 2018). We first\nword-align sentences‚Äô in a parallel corpus based on\nthemultilingualrepresentationsofanLLM.Wethen\nstartfromaword ùë§inasentence‚Äôinlanguage-script\nL1, follow the alignment links to its translations in\nlanguage-script L2, then the alignment links from\nL2 to L3 and so on, until in the end we follow\nalignment links back to L1. If this ‚Äúroundtrip‚Äù gets\nus back toùë§, then it indicates that the LLM has\nsimilar representations for the meaning ofùë§ in\nlanguage-scripts L1, L2, L3, etc. In other words,\nthe cross-lingual quality of representations is high.\nVice versa, failure to get back toùë§is a sign of poor\nmultilingual representations.\nWe use SimAlign (Jalili Sabet et al., 2020) and\nalignonthesub-wordlevelontheBiblepartoftest,\nbased on the representations of the LLM computed\nby transformer layer 8 as suggested in the original\npaper. We use intersection symmetrization: each\nword in a sentence‚Äô is aligned to at most one word\nin the other sentence‚Äô.\nAs evaluation measure we compute the percent-\nage of roundtrips that were successes, i.e., the\nroundtrip starts atùë§in L1 and returns back toùë§.\nForeachlanguage-scriptintest,werandomlyselect\nthree language-scripts as intermediate points L2,\nL3, L4. Since the intermediate points influence\nthe results, we run the experiment five times with\ndifferentintermediatepointsandreporttheaverage.\nAll models are evaluated with the same five sets of\nthree intermediate language-scripts.\nSequence Labeling We consider two sequence\nlabeling tasks: Named Entity Recognition (NER)\nand Part-Of-Speech (POS) tagging. We use the\nWikiANN dataset (Pan et al., 2017) for NER and\nversion v2.11 of Universal Dependencies (UD)\n(de Marneffe et al., 2021) for POS. Since training\ndata does not exist for some languages, we finetune\non English (with early stopping based on dev) and\nevaluatezero-shottransferonalllanguagescovered\nby WikiANN/UD. We set the learning rate to 2e-5\nwith Adam.\nSentence Retrieval Following (Hu et al., 2020),\nwe use up to 1000 English-aligned sentences‚Äô from\nTatoeba (Artetxe and Schwenk, 2019) to evaluate\nSentRetr (sentence retrieval). We also use 500\nEnglish-aligned sentences‚Äô from the Bible part of\ntest. We find nearest neighbors using cosine sim-\nilarity based on the average word embeddings in\nlayerùëô = 8 ‚Äì following Jalili Sabet et al. (2020) ‚Äì\nand compute top10 accuracy. For fair comparison\nand because the architectures are the same, we do\nnot optimize the hyperparameterùëôforGlot500-m\nand XLM-R-B.\nText Classification We evaluate on Taxi1500\n(Ma et al., 2023). It provides gold data for text\nclassification with six classes in a large number\nof language-scripts of whichGlot500-m supports\n354. We finetune on English (with early stopping\non dev) and evaluate zero-shot on test of the target\nlanguage-script. Learning rate: 2e-5, batch size:\n1086\n16 (following Ma et al. (2023)).\n6 Experiments\nIn this section, we discuss aggregate results. For\ndetailed results, see ¬ßD and ¬ßE.\n6.1 Results\nTable 4 gives results. Glot500-m outperforms\nXLM-R-B on all tasks for both head and tail\nlanguage-scripts, except for POS on head. That\nGlot500-moutperforms XLM-R-B is expected for\ntail language-scripts (i.e., those not covered by\nXLM-R). For these language-scripts the improve-\nment margin is large. Outperformance may seem\ncounterintuitive for head language-scripts (those\ncoveredbyXLM-R)since Glot500-mhasthesame\nnumber of (non-embedding) parameters as XLM-\nR-B. Since the number of covered languages has\ngreatly increased, leaving less capacity per lan-\nguage, we might expect underperformance. There\nare a few possible explanations. First, XLM-R may\nbe undertrained, and the inclusion of more head\nlanguage training data may improve their repre-\nsentations. Second, having more languages may\nimprove multilinguality by allowing languages to\nsynergize and enhance each other‚Äôs representations\nand cross-lingual transfer. Third, there are lan-\nguages similar to head languages among the tail\nlanguages, which in turn aids head languages.\nThe gap betweenGlot500-m and the baselines\nfor tail language-scripts in sequence labeling is\nsmaller. These tasks do not require as deep an\nunderstanding of language and thus transfer from\nheadtotaillanguage-scriptsiseasierthroughshared\ntokens.\nGlot500-m also outperforms XLM-R-L for tail\nlanguage-scripts (all tasks) and head language-\nscripts (3 tasks). This suggests that scaling up\nsize is not the only way for improvements. We can\nalsoimprovethequalityofmultilingualLLMrepre-\nsentations by increasing the number of languages.\n6.2 Language Coverage\nTable 5 comparesGlot500-m vs. XLM-R-B on\npseudoperplexity. For fair comparison we use\nword-level normalization. For 69 head language-\nscripts, Glot500-munderperformsXLM-R-B.This\nis expected asGlot500-m‚Äôs training data is small\nfortheselanguage-scripts. Glot500-moutperforms\nXLM-R-B for 420 tail language-scripts.\nThere are eight tail language-scripts for which\n0.400\n0.500\n0.600\n0.700\n0.800\n0 20 40 60\nhead tail\nSentence Retrieval Tatoeba\n0.100\n0.200\n0.300\n0.400\n0.500\n0.600\n0 20 40 60\nhead tail\nSentence Retrieval Bible\nepochs\n0.500\n0.600\n0.700\n0.800\n0 20 40 60\nhead tail\nPOS\nepochs\n0.500\n0.550\n0.600\n0.650\n0 20 40 60\nhead tail\nNER\nFigure 1: Progression of training for sentence retrieval\nand sequence labeling. x-axis: epochs/10K. The im-\nprovement is fast in the beginning for tail languages,\nthen gets slower and and reaches a plateau. This pattern\nis partially observed for head languages.\nGlot500-m performs worse than XLM-R-B. Five\nare tail languages with a similar head lan-\nguage where the two share a macro-language:\nekk/Standard Estonian (est/Estonian), aln/Gheg\nAlbanian (sqi/Albanian), nob/Norwegian Bokmal\n(nor/Norwegian),hbs/Serbo-Croatian(srp/Serbian),\nlvs/Standard Latvian (lav/Latvian). Since XLM-\nR-B‚Äôs pretraining corpus is large for the five head\nlanguages,itsperformanceisgoodfortheclosetail\nlanguages.\nThe other three languages all have a unique\nscript: sat/Santali (Ol Chiki script), div/Dhivehi\n(Thaana script), iku/Inuktitut (Inuktitut syllabics).\nFor these languages, XLM-R-B‚Äôs tokenizer returns\nmany UNK tokens since it is not trained on these\nscripts, resulting in an unreasonably optimistic esti-\nmate of pseudoperplexity by our implementation.\nGlot500-m‚Äôs token-level normalized pseudoper-\nplexity ranges from 1.95 for lhu/Lahu to 94.4 for\ntok/Toki Pona. The average is 13.5, the median\n10.6. We analyze the five language-scripts with\nthe highest pseudoperplexity: tok_Latn, luo_Latn,\nacm_Arab, ach_Latn, and teo_Latn.\ntok/TokiPonaisaconstructedlanguage. Accord-\ning to Wikipedia: ‚ÄúEssentially identical concepts\ncan be described by different words as the choice\nrelies on the speaker‚Äôs perception and experience.‚Äù\nThis property can result in higher variability and\nhigher perplexity.\nacm/MesopotamianArabiccontainsalargenum-\nber of tweets in raw form. This may result in\ndifficult-to-predict tokens in test.\nluo/Luo, ach/Acoli and teo/Teso are related\nNilotic languages spoken in Kenya, Tanzania,\nUganda and South Sudan. Their high perplex-\n1087\ntail head all\nXLM-R-B XLM-R-L Glot500-mXLM-R-B XLM-R-L Glot500-mXLM-R-B XLM-R-L Glot500-m\nPseudoperplexity 304.2 168.6 12.2 12.5 8.4 11.8 247.8 136.4 11.6\nSentence Retrieval Tatoeba 32.6 33.6 59.8 66.2 71.1 75.0 56.6 60.4 70.7\nSentence Retrieval Bible 7.4 7.1 43.2 54.2 58.3 59.0 19.3 20.1 47.3\nText Classification 13.7 13.9 46.6 51.3 60.5 54.7 23.3 25.8 48.7\nNER 47.5 51.8 60.7 61.8 66.0 63.9 55.3 59.5 62.4\nPOS 41.7 43.5 62.3 76.4 78.4 76.0 65.8 67.7 71.8\nRoundtrip Alignment 2.6 3.1 4.5 3.4 4.1 5.5 2.8 3.3 4.7\nTable 4: Evaluation of XLM-R base and large (XLM-R-B and XLM-R-L) andGlot500-mon pseudoperplexity and\nsix multilingual tasks across 5 seeds. Each number is an average over head, tail and all language-scripts. See ¬ßD, ¬ßE\nfor results per task and language-script.Glot500-m outperforms XLM-R-B in all tasks for head (except for POS)\nand tail language-scripts and XLM-R-L for tail language-scripts. Best result per row/column group in bold.\nhead tail\nGlot500-m is better 37 420\nXLM-R-B is better 69 8\nTable 5: Pseudoperplexity Glot500-m vs XLM-R-B.\nGlot500-m‚Äôs worse performance on head can be at-\ntributed to smaller training corpora and the relative diffi-\ncultyoflearningfivetimesmorelanguageswiththesame\nnumberof(non-embedding)parameters. Glot500-mper-\nforms better on almost all tail language-scripts. ¬ß6.2\ndiscusses the eight exceptions.\nity could be related to the fact that they are tonal\nlanguages, but the tones are not orthographically\nindicated. Another possible explanation is that\nthe training data is dominated by one subcorpus\n(Jehova‚Äôs Witnesses) whereas the test data are dom-\ninated by PBC. There are orthographic differences\nbetweenthetwo,e.g.,‚Äúdong‚Äù(JW)vs.‚Äúdo≈ã‚Äù(PBC)\nfor Acoli. These three languages are also spoken\noveralargeareaincountrieswithdifferentstandard\nlanguages, which could increase variability.\nOur analysis is not conclusive. We note however\nthat the gap between the three languages and the\nnext most difficult languages in terms of pseudop-\nerplexity is not large. So maybe Luo, Acoli and\nTeso are simply (for reasons still to be determined)\nlanguages that have higher perplexity than others.\n6.3 Training Progression\nTo analyze the training process, we evaluate\nGlot500-m on sequence labeling and SentRetr at\n10,000-step intervals. Figure 1 shows that perfor-\nmanceimprovesrapidlyattheonsetoftraining,but\nthen the rate of improvement slows down. This\ntrendisparticularlypronouncedfortaillanguagesin\nSentRetr. In comparison, sequence labeling is rela-\ntivelystraightforward,withthebaseline(XLM-R-B,\nepoch 0) achieving high performance by correctly\ntransferringprevalentclassessuchas verbandnoun\nthrough shared vocabulary, resulting in a smaller\nimprovement of Glot500-m vs. XLM-R-B.\nFor SentRetr, we observe larger improvements\nfor the Bible than for Tatoeba. This is likely due to\nthehigherproportionofreligiousdatain Glot500-c,\ncompared to XLM-R‚Äôs training data (i.e., CC100).\nThe average performance on downstream tasks\npeaks at 480K steps. We have taken a snapshot of\nGlot500-m at this stage and released it.\n6.4 Analysis across Language-Scripts\nTo analyze the effect of language-scripts, we select\nfive tail language-scripts each with the largest and\nsmallestgainwhencomparing Glot500-mvs.XLM-\nR-B for SentRetr and sequence labeling.\nTable 6 shows thatGlot500-m improves lan-\nguages with scripts not covered by XLM-R (e.g.,\ndiv/Dhivehi, Thaana script, see ¬ß6.2) by a large\nmarginsinceXLM-Rsimplyregardstheuncovered\nscripts as unknown tokens and cannot compute\nmeaningful representations for the input. The large\namount of data we collected inGlot500-c also\ncontributes to the improvement for tail languages,\ne.g., for tat_Cyrl (Tatar) in SentRetr Tatoeba and\nmlt_Latn (Maltese) in POS. See ¬ß6.7 for a detailed\nanalysis of the effect of corpus size.\nOntheotherhand, Glot500-machievesjustcom-\nparable or even worse results for some language-\nscripts. We see at least three explanations. (i)\nAs discussed in ¬ß6.2, some tail languages (e.g.,\nnob/Norwegian Bokmal) are close to a head lan-\nguage (e.g., nor/Norwegian), soGlot500-mhas no\nadvantage over XLM-R-B. (ii) A language is at the\nlow end of our corpus size range (i.e., 30,000 sen-\ntences‚Äô). Example: xav_Latn, Xav√°nte. (iii) Some\nlanguages are completely distinct from all other\nlanguages inGlot500-c, thus without support from\nany similar language. An example is mau_Latn,\nHuautla Mazatec. Glot500-m has a much harder\n1088\nlanguage-script XLMRGlot500 gain language-script XLMRGlot500 gain\nhigh end\nSentRetr Tatoeba\ntat CTatar 10.3 70.3 60.0\nSentRetr Bible\nuzn CNorthern Uzbek 5.4 87.0 81.6\nnds LLow German 28.8 77.1 48.3 crs LSeselwa Creole 7.4 80.6 73.2\ntuk LTurkmen 16.3 63.5 47.3 srn LSranan Tongo 6.8 79.8 73.0\nile LInterlingue 34.6 75.6 41.0 uzb CUzbek 6.2 78.8 72.6\nuzb CUzbek 25.2 64.5 39.3 bcl LCentral Bikol 10.2 79.8 69.6\nlow end\ndtp LKadazan Dusun 5.6 21.1 15.5 xav LXav√°nte 2.2 5.0 2.8\nkab LKabyle 3.7 16.4 12.7 mauLHuautla Mazatec 2.4 3.6 1.2\npamLPampanga 4.8 11.0 6.2 ahk LAkha 3.0 3.2 0.2\nlvs LStandard Latvian 73.4 76.9 3.5 aln LGheg Albanian 67.8 67.6 -0.2\nnob LBokm√•l 93.5 95.7 2.2 nob LBokm√•l 82.8 79.2 -3.6\nhigh end\nNER\ndiv TDhivehi 0.0 50.9 50.9\nPOS\nmlt LMaltese 21.3 80.3 59.0\nche CChechen 15.3 61.2 45.9 sah CYakut 21.9 76.9 55.0\nmri LMaori 16.0 58.9 42.9 smeLNorthern Sami 29.6 73.6 44.1\nnan LMin Nan 42.3 84.9 42.6 yor LYoruba 22.8 64.2 41.4\ntgk CTajik 26.3 66.4 40.0 quc LK‚Äôiche‚Äô 28.5 64.1 35.6\nlow end\nzea LZeeuws 68.1 67.3 -0.8 lzh HLiterary Chinese 11.7 18.4 6.7\nvol LVolap√ºk 60.0 59.0 -1.0 nap LNeapolitan 47.1 50.0 2.9\nminLMinangkabau 42.3 40.4 -1.8 hywAWestern Armenian 79.1 81.1 2.0\nwuuHWu Chinese 28.9 23.9 -5.0 kmrLNorthern Kurdish 73.5 75.2 1.7\nlzh HLiterary Chinese 15.7 10.3 -5.4 aln LGheg Albanian 54.7 51.2 -3.5\nTable6: Resultsfor five taillanguage-scriptseachwith thelargest(high end)and smallest (lowend) gainGlot500-m\nvs. XLM-R-B for four tasks.Glot500-m‚Äôs gain over XLM-R-B is large at the high end and small or slightly negative\nat the low end. L = Latin, C = Cyrillic, H = Hani, A = Armenian, T = Thaana\nlang-script XLM-R-B Glot500-m gain\nuig_Arab head 45.8 56.2 10.4\nuig_Latn tail 9.8 62.8 53.0\nhin_Deva head 67.0 76.6 9.6\nhin_Latn tail 13.6 43.2 29.6\nuzb_Latn head 54.8 67.6 12.8\nuzb_Cyrl tail 6.2 78.8 72.6\nkaa_Cyrl tail 17.6 73.8 56.2\nkaa_Latn tail 9.2 43.4 34.2\nkmr_Cyrl tail 4.0 42.4 38.4\nkmr_Latn tail 35.8 63.0 27.2\ntuk_Cyrl tail 13.6 65.0 51.4\ntuk_Latn tail 9.6 66.2 56.6\nTable 7: Sentence Retrieval Bible performance of\nGlot500-m and XLM-R-B for six languages with two\nscripts: Uighur (uig), Hindi (hin), Uzbek (uzb), Kara-\nKalpak (kaa), Northern Kurdish (kmr), Turkmen (tuk).\nGlot500-m clearly outperforms XLM-R-B with large\ndifferences for tail language-scripts.\ntime learning good representations in these cases.\n6.5 Languages with Multiple Scripts\nTable 7 compares SentRetr performance XLM-R-B\nvs. Glot500-m for six languages with two scripts.\nUnsurprisingly,XLM-Rperformsmuchbetterfora\nlanguage-script it was pretrained on (‚Äúhead‚Äù) than\non one that it was not (‚Äútail‚Äù). We can improve\nthe performance of a language, even surpassing the\nlanguage-script covered by XLM-R, if we collect\nenough data for its script not covered by XLM-R.\nForlanguageswithtwoscriptsnotcoveredbyXLM-\nR,the performanceis betterfor thescript forwhich\nwe collect a larger corpus. For example, kaa_Cyrl\n(Kara-Kalpak)hasaboutthreetimesasmuchdataas\nkaa_Latn. Thisexplainswhykaa_Cyrloutperforms\nkaa_Latn by 30%.\nDufterandSch√ºtze(2020)foundthat, aftertrain-\ningamultilingualmodelwithtwoscriptsforEnglish\n(naturalEnglishand‚ÄúfakeEnglish‚Äù),themodelper-\nformed well at zero-shot transfer if the capacity of\nthe model was of the right size (i.e., not too small,\nnottoolarge). Ourexperimentswithrealdatashow\nthecomplexityoftheissue: evenifthereisa‚Äúright‚Äù\nsize for an LLM that supports both full acquisition\nof languages and multilingual transfer, this size is\ndifficulttodetermineanditmaybedifferentfordif-\nferent language pairs in a large horizontally scaled\nmodel like Glot500-m.\n6.6 Analysis across Language Families\nTable8comparesSentRetrperformance Glot500-m\nvs. XLM-R-B for seven language families that have\nten or more language-scripts inGlot500-c. We\nassign languages to families based on Glottolog.4\nGenerally,XLM-Rhasbetterperformancethemore\nlanguage-scripts from a language family are rep-\nresented in its training data; e.g., performance is\nbetter for indo1319 and worse for maya1287. The\nresults suggest thatGlot500-m‚Äôs improvement over\n4http://glottolog.org/glottolog/family\n1089\nfamily |ùêøùê∫| |ùêøùëã| XLM-R-B Glot500-m gain\nindo1319 91 50 41.5 61.4 19.9\natla1278 69 2 5.5 45.2 39.6\naust1307 53 6 13.7 47.0 33.2\nturk1311 22 7 20.1 62.9 42.8\nsino1245 22 2 7.6 38.9 31.3\nmaya1287 15 0 3.8 20.3 16.4\nafro1255 12 5 13.0 34.3 21.4\nTable 8: Average Sentence Retrieval Bible performance\nofGlot500-mandXLM-R-Bforsevenlanguagefamilies.\nThe difference in coverage of a family byGlot500-m\nvs. XLM-R-B is partially predictive of the performance\ndifference. |ùêøùê∫|/|ùêøùëã|: numberoflanguage-scriptsfrom\nfamily covered by Glot500-m/XLM-R.\nlang-script Glot+1 Glot500-m\nrug_Latn, Roviana 51.0 49.0\nyan_Latn, Mayangna/Sumo 46.4 31.8\nwbm_Latn, Wa/Va 49.6 46.4\nctd_Latn, Tedim Chin 47.4 59.4\nquh_Latn, Southern Quechua 33.4 56.2\ntat_Cyrl, Tatar 58.8 67.2\nTable9: PerformanceonSentenceRetrievalBibleofcon-\ntinued pretraining on just one language-script (Glot+1)\nvs. onGlot500-c (Glot500-m). Glot500-m underper-\nforms on the top three and outperforms on the bottom\nthree. Our explanation is that the second group is sup-\nported by closely related languages inGlot500-c; e.g.,\nfor Southern Quechua (quh),Glot500-m also covers\ncloselyrelatedCuzcoQuechua(quz). Forthefirstgroup\nthis is not the case; e.g., the Wa language (wbm) has no\nclose relative in Glot500-c.\nXLM-R is the larger, the better our training corpus\nGlot500-c‚Äôs coverage is of a family.\n6.7 Effect of Amount of Training Data\nWeexaminecorrelationbetweenpretrainingcorpus\nsize andGlot500-m zero-shot performance. We\nfocus on SentRetr Bible (¬ß5) since it supports the\nmostheadandtaillanguages. WefindthatPearson‚Äôs\nùëü = .34, i.e., corpus size and performance are\nmoderately, but clearly correlated. We suspect that\nthe correlation is not larger because, in addition\nto corpus size of languageùëô itself, corpus size of\nlanguages closely related toùëôis also an important\nfactor(see¬ß6.4forasimilarfindingforNorwegian).\nWe therefore also compute Pearson‚Äôsùëübetween (i)\nperformance of languageùëôon SentRetr Bible and\n(ii)jointcorpussizeof ùëôandits ùëònearestneighbors\n(according to perplexity divergence, ¬ß3.3). In this\ncase, Pearson‚Äôsùëü = .44 (for bothùëò = 3 and ùëò = 4),\nindicating that the corpus size of nearest neighbor\nlanguages does play a role.\n6.8 Support through Related Languages\nBuildingon¬ß6.7,thereisanotherwaywecaninves-\ntigatethepositiveeffectofcloselyrelatedlanguages\non performance: We can compare performance\n(again on SentRetr Bible) of continued pretraining\non just one language (we refer to this model as\nGlot+1) vs. on all 511 languages represented in\nGlot500-c (i.e., Glot500-m). Table 9 presents re-\nsults for six language-scripts selected from various\nlanguagefamiliesandsuggeststhatsomelanguages\ndo not receive support from related languages (top\nthree). In that case, Glot+1 can fully concentrate\non learning the isolated language and does better\nthan Glot500-c. Other languages (bottom three)\ndo receive support from related languages. For\nexample, SouthernQuechua(quh)seemstoreceive\nsupport inGlot500-m from closely related Cuzco\nQuechua(quz),resultingin Glot500-moutperform-\ning Glot+1.\n7 Conclusion and Future Work\nWecollectanddata-clean Glot500-c,alargecorpus\nofhundredsofusuallyneglectedtail(i.e., long-tail)\nlanguages and createGlot500-m, an LLM that is\ntrained onGlot500-c and covers these languages.\nWe evaluateGlot500-m on six tasks that allow us\nto evaluate almost all languages. We observe large\nimprovementsforbothheadandtaillanguagescom-\nparedtoXLM-R.Ouranalysisshowsthatnosingle\nfactor fully explains the quality of the representa-\ntion of a language in a multilingual model. Rather,\na combination of factors is important, including\ncorpus size, script, ‚Äúhelp‚Äù from related languages\nand the total capacity of the model.\nThis work is the first to create a language model\non a dataset of several hundreds of gigabytes and\nto make it publicly available for such a large and di-\nverse number of low-resource languages. In future\nresearch, we would like to train larger models to\nfurther investigate the effect of model size, distill\nhighly multilingual models for resource-efficient\ndeployment, explore alternatives to continued pre-\ntraining and use models for more tail language\ndownstream tasks.\nLimitations\n(1) We did not perform any comprehensive hy-\nperparameter search, which would have further\nconsolidated our results. This decision was made\nduetothehighcostoftrainingmultiplemodels. (2)\nComparedtocurrentverylargemodels, Glot500-m\n1090\nis comparatively small. (3) Although we have tried\nto minimize the amount of noise in our data, some\nnoise is still present.\nEthics Statement\nThere are two issues worth mentioning in regards\nto this project. First, it was not feasible for us\nto thoroughly examine the content of the data for\nall languages, thus we cannot confirm the absence\nof discrimination based on factors such as race or\nsexuality. The data was solely utilized as a textual\ncorpus, and the content should not be interpreted\nasanendorsementbyourteam. Ifthemodelissub-\nsequently utilized for generation, it is possible that\nthe training data may be reflected in the generated\noutput. However,addressingpotentialbiaseswithin\nthe data is an area for future research. Second, it\nis important to note that while the data sources\nutilized in this study do not explicitly prohibit the\nreuse of data for research purposes, some sources\ndo have copyright statements indicating that such\nuseispermissiblewhileothersdonot. Additionally,\ncertain sources prohibit the redistribution of data.\nAs such, data from these sources is omitted from\nthe published version of Glot2000-c.\nAcknowledgements\nWe would like to thank Renhao Pei, Yihong Liu,\nVerena Blaschke, and the anonymous reviewers.\nThis work was funded by the European Research\nCouncil (grants #740516 and #758969) and EU‚Äôs\nHorizon Europe Research and Innovation Actions\n(UTTER, contract 101070631).\nReferences\nSolomon Teferra Abate, Michael Melese, Martha Yi-\nfiru Tachbelie, Million Meshesha, Solomon Ati-\nnafu, Wondwossen Mulugeta, YaregalAssabie, Hafte\nAbera, Binyam Ephrem, Tewodros Abebe, Wondim-\nagegnhue Tsegaye, Amanuel Lemma, Tsegaye An-\ndargie, and Seifedin Shifaw. 2018. Parallel corpora\nfor bi-lingual English-Ethiopian languages statisti-\ncal machine translation. InProceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 3102‚Äì3111, Santa Fe, New Mexico, USA.\nAssociation for Computational Linguistics.\nAhmedAbdelali,HamdyMubarak,YounesSamih,Sabit\nHassan, and Kareem Darwish. 2021. QADI: Arabic\ndialectidentificationinthewild. In Proceedingsofthe\nSixthArabicNaturalLanguageProcessingWorkshop ,\npages 1‚Äì10, Kyiv, Ukraine (Virtual). Association for\nComputational Linguistics.\nKathrein Abu Kwaik, Motaz Saad, Stergios Chatzikyri-\nakidis, and Simon Dobnik. 2018. Shami: A corpus\nof Levantine Arabic dialects. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nIfe Adebara, AbdelRahim Elmadany, Muhammad\nAbdul-Mageed, and Alcides Alcoba Inciarte. 2022.\nSERENGETI: Massively multilingual language mod-\nels for Africa.arXiv preprint arXiv:2212.10785.\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia\nKreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter,\nDietrichKlakow, PeterNabende, ErnieChang, Tajud-\ndeen Gwadabe, Freshia Sackey, Bonaventure F. P.\nDossou, Chris Emezue, Colin Leong, Michael Beuk-\nman, Shamsuddeen Muhammad, Guyo Jarso, Oreen\nYousuf,AndreNiyongaboRubungo,GillesHacheme,\nEric Peter Wairagala, Muhammad Umair Nasir, Ben-\njamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade\nAbbott, Mohamed Ahmed, Millicent Ochieng, An-\nuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi,\nFatoumata Ouoba Kabore, Godson Kalipe, Derguene\nMbaye, Allahsera Auguste Tapo, Victoire Memd-\njokam Koagne, Edwin Munkoh-Buabeng, Valen-\ncia Wagner, Idris Abdulmumin, Ayodele Awokoya,\nHappy Buzaaba, Blessing Sibanda, Andiswa Bukula,\nand Sam Manthalu. 2022. A few thousand transla-\ntions go a long way! leveraging pre-trained models\nfor African news translation. InProceedings of the\n2022 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 3053‚Äì3070,\nSeattle,UnitedStates.AssociationforComputational\nLinguistics.\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola\nAdebonojo, Adesina Ayeni, Mofe Adeyemi, Ayo-\ndele Esther Awokoya, and Cristina Espa√±a-Bonet.\n2021. The effect of domain and diacritics in Yoruba‚Äì\nEnglishneuralmachinetranslation. In Proceedingsof\nMachine Translation Summit XVIII: Research Track,\npages 61‚Äì75, Virtual. Association for Machine Trans-\nlation in the Americas.\nRodrigo Agerri, Xavier G√≥mez Guinovart, German\nRigau, and Miguel Anxo Solla Portela. 2018. De-\nveloping new linguistic resources and tools for the\nGalician language. InProceedings of the Eleventh\nInternationalConferenceonLanguageResourcesand\nEvaluation(LREC2018) , Miyazaki, Japan.European\nLanguage Resources Association (ELRA).\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius\nMosbach, and Dietrich Klakow. 2022. Adapting pre-\ntrained language models to African languages via\nmultilingual adaptive fine-tuning. InProceedings of\nthe 29th International Conference on Computational\nLinguistics, pages 4336‚Äì4349, Gyeongju, Republic\nof Korea. International Committee on Computational\nLinguistics.\n1091\nIsraa Alsarsour, Esraa Mohamed, Reem Suwaileh, and\nTamer Elsayed. 2018. DART: A large dataset of di-\nalectal Arabic tweets. InProceedings of the Eleventh\nInternationalConferenceonLanguageResourcesand\nEvaluation(LREC2018) , Miyazaki, Japan.European\nLanguage Resources Association (ELRA).\nAntonios Anastasopoulos, Alessandro Cattelan, Zi-\nYi Dou, Marcello Federico, Christian Federmann,\nDmitriyGenzel,FransciscoGuzm√°n,JunjieHu,Mac-\nduffHughes,PhilippKoehn,RosieLazar,WillLewis,\nGraham Neubig, Mengmeng Niu, Alp √ñktem, Eric\nPaquin, Grace Tang, and Sylwia Tur. 2020. TICO-19:\nthe translation initiative for COvid-19. InProceed-\nings of the 1st Workshop on NLP for COVID-19\n(Part 2) at EMNLP 2020, Online. Association for\nComputational Linguistics.\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan\nVuliƒá.2022. Composablesparsefine-tuningforcross-\nlingual transfer. InProceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1778‚Äì1796,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nMikel Artetxe and Holger Schwenk. 2019. Massively\nmultilingualsentenceembeddingsforzero-shotcross-\nlingual transfer and beyond. Transactions of the\nAssociation for Computational Linguistics, 7:597‚Äì\n610.\nNiyati Bafna. 2022. Empirical models for an indic\nlanguage continuum.\nMarta Ba√±√≥n, Pinzhen Chen, Barry Haddow, Kenneth\nHeafield,HieuHoang,MiquelEspl√†-Gomis,MikelL.\nForcada, Amir Kamran, Faheem Kirefu, Philipp\nKoehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere,\nGema Ram√≠rez-S√°nchez, Elsa Sarr√≠as, Marek Strelec,\nBrianThompson,WilliamWaites,DionWiggins,and\nJaume Zaragoza. 2020. ParaCrawl: Web-scale acqui-\nsition of parallel corpora. InProceedings of the 58th\nAnnualMeetingoftheAssociationforComputational\nLinguistics, pages 4555‚Äì4567, Online. Association\nfor Computational Linguistics.\nMarta Ba√±√≥n, Miquel Espl√†-Gomis, Mikel L. For-\ncada, Cristian Garc√≠a-Romero, Taja Kuzman, Nikola\nLjubesic, Rik van Noord, Leopoldo Pla Sempere,\nGemaRam√≠rez-S√°nchez,PeterRupnik,V√≠tSuchomel,\nAntonio Toral, Tobias van der Werff, and Jaume\nZaragoza. 2022. Macocu: Massive collection and\ncuration of monolingual and bilingual data: focus\non under-resourced languages. InProceedings of the\n23rd Annual Conference of the European Associa-\ntion for Machine Translation, EAMT 2022, Ghent,\nBelgium, June 1-3, 2022, pages 301‚Äì302. European\nAssociation for Machine Translation.\nAnkurBapna,IsaacCaswell,JuliaKreutzer,OrhanFirat,\nDaan van Esch, Aditya Siddhant, Mengmeng Niu,\nPallavi Baljekar, Xavier Garcia, Wolfgang Macherey,\net al. 2022. Building machine translation systems\nfor the next thousand languages. arXiv preprint\narXiv:2205.03983.\nWorkshop BigScience, :, Teven Le Scao, Angela Fan,\nChristopher Akiki, Ellie Pavlick, Suzana Iliƒá, Daniel\nHesslow, Roman Castagn√©, Alexandra Sasha Luc-\ncioni, Fran√ßois Yvon, Matthias Gall√©, Jonathan\nTow, Alexander M. Rush, Stella Biderman, Albert\nWebson, Pawan Sasanka Ammanamanchi, Thomas\nWang, Beno√Æt Sagot, Niklas Muennighoff, Albert Vil-\nlanova del Moral, Olatunji Ruwase, Rachel Bawden,\nStas Bekman, Angelina McMillan-Major, Iz Belt-\nagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pe-\ndro Ortiz Suarez, Victor Sanh, Hugo Lauren√ßon,\nYacine Jernite, Julien Launay, Margaret Mitchell,\nColin Raffel, Aaron Gokaslan, Adi Simhi, Aitor\nSoroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers,\nAriel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris Emezue, Christopher Klamm, Colin Leong,\nDanielvanStrien,DavidIfeoluwaAdelani,Dragomir\nRadev, Eduardo Gonz√°lez Ponferrada, Efrat Lev-\nkovizh, Ethan Kim, Eyal Bar Natan, Francesco\nDe Toni, G√©rard Dupont, Germ√°n Kruszewski, Gi-\nada Pistilli, Hady Elsahar, Hamza Benyamina, Hieu\nTran,IanYu,IdrisAbdulmumin,IsaacJohnson,Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse\nDodge, Jian Zhu, Jonathan Chang, J√∂rg Frohberg,\nJoseph Tobing, Joydeep Bhattacharjee, Khalid Al-\nmubarak,KimboChen,KyleLo,LeandroVonWerra,\nLeon Weber, Long Phan, Loubna Ben allal, Lu-\ndovic Tanguy, Manan Dey, Manuel Romero Mu√±oz,\nMaraimMasoud,Mar√≠aGrandury,Mario≈†a≈°ko,Max\nHuang,MaximinCoavoux,MayankSingh,MikeTian-\nJian Jiang, Minh Chien Vu, Mohammad A. Jauhar,\nMustafa Ghaleb, Nishant Subramani, Nora Kassner,\nNurulaqilla Khamis, Olivier Nguyen, Omar Espe-\njel, Ona de Gibert, Paulo Villegas, Peter Henderson,\nPierre Colombo, Priscilla Amuok, Quentin Lhoest,\nRheza Harliman, Rishi Bommasani, Roberto Luis\nL√≥pez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo,\nSebastianNagel,ShamikBose,ShamsuddeenHassan\nMuhammad, Shanya Sharma, Shayne Longpre, So-\nmaieh Nikpoor, Stanislav Silberberg, Suhas Pai, Syd-\nney Zink, Tiago Timponi Torrent, Timo Schick, Tris-\ntan Thrush, Valentin Danchev, Vassilina Nikoulina,\nVeronika Laippala, Violette Lepercq, Vrinda Prabhu,\nZaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin\nHeinzerling, Chenglei Si, Davut Emre Ta≈üar, Eliz-\nabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee,\nAbheesht Sharma, Andrea Santilli, Antoine Chaffin,\nArnaud Stiegler, Debajyoti Datta, Eliza Szczechla,\nGunjan Chhablani, Han Wang, Harshit Pandey, Hen-\ndrik Strobelt, Jason Alan Fries, Jos Rozen, Leo\nGao, Lintang Sutawika, M Saiful Bari, Maged S.\nAl-shaibani, Matteo Manica, Nihal Nayak, Ryan Tee-\nhan, SamuelAlbanie, ShengShen, SrulikBen-David,\nStephen H. Bach, Taewoon Kim, Tali Bers, Thibault\nFevry, Trishala Neeraj, Urmish Thakker, Vikas Rau-\nnak, Xiangru Tang, Zheng-Xin Yong, Zhiqing Sun,\nShaked Brody, Yallow Uri, Hadar Tojarieh, Adam\nRoberts, Hyung Won Chung, Jaesung Tae, Jason\nPhang, Ofir Press, Conglong Li, Deepak Narayanan,\nHatim Bourfoune, Jared Casper, Jeff Rasley, Max\nRyabinin, Mayank Mishra, Minjia Zhang, Moham-\nmad Shoeybi, Myriam Peyrounette, Nicolas Pa-\ntry, Nouamane Tazi, Omar Sanseviero, Patrick von\n1092\nPlaten, Pierre Cornette, Pierre Fran√ßois Lavall√©e,\nR√©miLacroix,SamyamRajbhandari,SanchitGandhi,\nShaden Smith, St√©phane Requena, Suraj Patil, Tim\nDettmers,AhmedBaruwa,AmanpreetSingh,Anasta-\nsia Cheveleva, Anne-Laure Ligozat, Arjun Subramo-\nnian,Aur√©lieN√©v√©ol,CharlesLovering,DanGarrette,\nDeepak Tunuguntla, Ehud Reiter, Ekaterina Takta-\nsheva, Ekaterina Voloshina, Eli Bogdanov, Genta In-\ndra Winata, Hailey Schoelkopf, Jan-Christoph Kalo,\nJekaterina Novikova, Jessica Zosa Forde, Jordan\nClive, Jungo Kasai, Ken Kawamura, Liam Hazan,\nMarineCarpuat,MirunaClinciu,NajoungKim,New-\nton Cheng, Oleg Serikov, Omer Antverg, Oskar\nvan der Wal, Rui Zhang, Ruochen Zhang, Sebas-\ntian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana\nShavrina, Thomas Scialom, Tian Yun, Tomasz Lim-\nisiewicz, Verena Rieser, Vitaly Protasov, Vladislav\nMikhailov, Yada Pruksachatkun, Yonatan Belinkov,\nZachary Bamberger, Zdenƒõk Kasner, Alice Rueda,\nAmandaPestana,AmirFeizpour,AmmarKhan,Amy\nFaranak, Ana Santos, Anthony Hevia, Antigona Unl-\ndreaj,ArashAghagol,ArezooAbdollahi,AychaTam-\nmour, Azadeh HajiHosseini, Bahareh Behroozi, Ben-\njamin Ajibade, Bharat Saxena, Carlos Mu√±oz Ferran-\ndis, Danish Contractor, David Lansky, Davis David,\nDouwe Kiela, Duong A. Nguyen, Edward Tan, Emi\nBaylor, Ezinwanne Ozoani, Fatima Mirza, Frankline\nOnoniwu, Habib Rezanejad, Hessie Jones, Indrani\nBhattacharya, Irene Solaiman, Irina Sedenko, Isar\nNejadgholi, Jesse Passmore, Josh Seltzer, Julio Bo-\nnis Sanz, Livia Dutra, Mairon Samagaio, Maraim\nElbadri, Margot Mieskes, Marissa Gerchick, Martha\nAkinlolu, Michael McKenna, Mike Qiu, Muhammed\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Ra-\njani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel,\nRan An, Rasmus Kromann, Ryan Hao, Samira Al-\nizadeh, Sarmad Shubber, Silas Wang, Sourav Roy,\nSylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le,\nYoyoYang, ZachNguyen, AbhinavRameshKashyap,\nAlfredo Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Singh, Benjamin\nBeilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag\nJain, Chuxin Xu, Cl√©mentine Fourrier, Daniel Le√≥n\nPeri√±√°n, Daniel Molano, Dian Yu, Enrique Manjava-\ncas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec,\nImane Bello, Ishani Dash, Jihyun Kang, John Giorgi,\nJonas Golde, Jose David Posada, Karthik Rangasai\nSivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shin-\nzato,MadeleineHahndeBykhovetz,MaikoTakeuchi,\nMarcP√†mies,MariaACastillo,MariannaNezhurina,\nMario S√§nger, Matthias Samwald, Michael Cullan,\nMichael Weinberg, Michiel De Wolf, Mina Mihalj-\ncic, Minna Liu, Moritz Freidank, Myungsun Kang,\nNatasha Seelam, Nathan Dahlberg, Nicholas Michio\nBroad, Nikolaus Muellner, Pascale Fung, Patrick\nHaller, Ramya Chandrasekhar, Renata Eisenberg,\nRobert Martin, Rodrigo Canalli, Rosaline Su, Ruisi\nSu, Samuel Cahyawƒ≥aya, Samuele Garda, Shlok S\nDeshmukh, Shubhanshu Mishra, Sid Kiblawi, Si-\nmon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan\nSchweter,SushilBharati,TanmayLaud,Th√©oGigant,\nTomoya Kainuma, Wojciech Kusa, Yanis Labrak,\nYash Shailesh Bajaj, Yash Venkatraman, Yifan Xu,\nYingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan\nYe, Mathilde Bras, Younes Belkada, and Thomas\nWolf. 2022. BLOOM: a 176b-parameteropen-access\nmultilingual language model.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nJos√©Camacho-Collados,ClaudioDelliBovi,Alessandro\nRaganato, and Roberto Navigli. 2016. A large-scale\nmultilingual disambiguation of glosses. InProceed-\nings of the Tenth International Conference on Lan-\nguage Resources and Evaluation (LREC‚Äô16), pages\n1701‚Äì1708, Portoro≈æ, Slovenia. European Language\nResources Association (ELRA).\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM:\nAninformation-theoreticframeworkforcross-lingual\nlanguage model pre-training. InProceedings of the\n2021ConferenceoftheNorthAmericanChapterofthe\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3576‚Äì3588, Online.\nAssociation for Computational Linguistics.\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma,\nBo Zheng, Saksham Singhal, Payal Bajaj, Xia Song,\nXian-Ling Mao, Heyan Huang, and Furu Wei. 2022.\nXLM-E: Cross-lingual language model pre-training\nvia ELECTRA. InProceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 6170‚Äì6182,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nRochelle Choenni and Ekaterina Shutova. 2022. Inves-\ntigating language relationships in multilingual sen-\ntenceencodersthroughthelensoflinguistictypology.\nComputational Linguistics, 48(3):635‚Äì672.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways.arXiv preprint\narXiv:2204.02311.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020. Improving multilingual models\nwith language-clustered vocabularies. InProceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4536‚Äì4546, Online. Association for Computational\nLinguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\n1093\ncross-lingual representation learning at scale. InPro-\nceedings of the 58th Annual Meeting of the Associa-\ntionforComputationalLinguistics ,pages8440‚Äì8451,\nOnline. Association for Computational Linguistics.\nMarta R Costa-juss√†, James Cross, Onur √áelebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation.arXiv preprint\narXiv:2207.04672.\nMarie-Catherine de Marneffe, Christopher D. Manning,\nJoakim Nivre, and Daniel Zeman. 2021. Universal\ndependencies. ComputationalLinguistics,47(2):255‚Äì\n308.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. InProceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171‚Äì4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPhilipp Dufter and Hinrich Sch√ºtze. 2020. Identifying\nelements essential for BERT‚Äôs multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4423‚Äì4437, Online. Association for Computa-\ntional Linguistics.\nPhilipp Dufter, Mengjie Zhao, Martin Schmitt, Alexan-\nder Fraser, and Hinrich Sch√ºtze. 2018. Embedding\nlearning through multilingual concept induction. In\nProceedingsofthe56thAnnualMeetingoftheAssoci-\nationforComputationalLinguistics(Volume1: Long\nPapers), pages 1520‚Äì1530, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJonathan Dunn. 2020. Mapping languages: the corpus\nof global language use.Lang. Resour. Evaluation,\n54(4):999‚Äì1018.\nEberhard,DavidM.,GaryF.Simons,andCharlesD.Fen-\nnig(eds.).2022. Ethnologue: Languagesoftheworld.\ntwenty-fifth edition.\nAbteen Ebrahimi and Katharina Kann. 2021. How\nto adapt your pretrained multilingual model to 1600\nlanguages. InProceedingsofthe59thAnnualMeeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguageProcessing(Volume1: LongPapers),pages\n4555‚Äì4567, Online. Association for Computational\nLinguistics.\nMahmoud El-Haj. 2020. Habibi - a multi dialect multi\nnational Arabic song lyrics corpus. InProceedings\nof the Twelfth Language Resources and Evaluation\nConference, pages 1318‚Äì1326, Marseille, France.\nEuropean Language Resources Association.\nMahmoud El-Haj, Paul Rayson, and Mariam Aboelezz.\n2018. Arabic dialect identification in the context of\nbivalency and code-switching. In Proceedings of\nthe Eleventh International Conference on Language\nResources and Evaluation (LREC 2018), Miyazaki,\nJapan. European Language Resources Association\n(ELRA).\nAngelaFan,ShrutiBhosale,HolgerSchwenk,ZhiyiMa,\nAhmedEl-Kishky,SiddharthGoyal,MandeepBaines,\nOnurCelebi,GuillaumeWenzek,VishravChaudhary,\nNamanGoyal,TomBirch,VitaliyLiptchinsky,Sergey\nEdunov,MichaelAuli,andArmandJoulin.2021. Be-\nyondenglish-centricmultilingualmachinetranslation.\nJ. Mach. Learn. Res., 22:107:1‚Äì107:48.\nPablo Gamallo, Jose Ramom Pichel, and I√±aki Alegria.\n2017. A perplexity-based method for similar lan-\nguages discrimination. InProceedings of the Fourth\nWorkshop on NLP for Similar Languages, Varieties\nand Dialects (VarDial), pages 109‚Äì114, Valencia,\nSpain. Association for Computational Linguistics.\nDirk Goldhahn, Thomas Eckart, and Uwe Quasthoff.\n2012. Building large monolingual dictionaries at\nthe leipzig corpora collection: From 100 to 200\nlanguages. InProceedingsoftheEighthInternational\nConference on Language Resources and Evaluation,\nLREC2012,Istanbul,Turkey,May23-25,2012 ,pages\n759‚Äì765. European Language Resources Association\n(ELRA).\nSantiago G√≥ngora, Nicol√°s Giossa, and Luis Chiruzzo.\n2021. Experiments on a Guarani corpus of news\nand social media. InProceedings of the First Work-\nshoponNaturalLanguageProcessingforIndigenous\nLanguages of the Americas, pages 153‚Äì158, Online.\nAssociation for Computational Linguistics.\nSantiago G√≥ngora, Nicol√°s Giossa, and Luis Chiruzzo.\n2022. Can we use word embeddings for enhancing\nGuarani-Spanish machine translation? InProceed-\nings of the Fifth Workshop on the Use of Computa-\ntionalMethodsintheStudyofEndangeredLanguages ,\npages127‚Äì132,Dublin,Ireland.AssociationforCom-\nputational Linguistics.\nThamme Gowda, Zhao Zhang, Chris Mattmann, and\nJonathanMay.2021. Many-to-Englishmachinetrans-\nlation tools, data, and pretrained models. InProceed-\ningsofthe59thAnnualMeetingoftheAssociationfor\nComputationalLinguisticsandthe11thInternational\nJoint Conference on Natural Language Processing:\nSystem Demonstrations, pages 306‚Äì316, Online. As-\nsociation for Computational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Samin Mubasshir, Yuan-Fang Li, Yong-\nBin Kang, M. Sohel Rahman, and Rifat Shahriyar.\n2021. Xl-sum: Large-scale multilingual abstrac-\ntive summarization for 44 languages. InFindings\nof the Association for Computational Linguistics:\nACL/ƒ≤CNLP 2021, Online Event, August 1-6, 2021,\nvolumeACL/ƒ≤CNLP2021of FindingsofACL ,pages\n4693‚Äì4703. Association for Computational Linguis-\ntics.\n1094\nKenneth Heafield. 2011. KenLM: Faster and smaller\nlanguage model queries. InProceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages\n187‚Äì197, Edinburgh, Scotland. Association for Com-\nputational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME:Amassivelymultilingualmulti-task\nbenchmarkforevaluatingcross-lingualgeneralisation.\nIn Proceedings of the 37th International Conference\non Machine Learning, volume 119 ofProceedings\nof Machine Learning Research, pages 4411‚Äì4421.\nPMLR.\nAyyoob ImaniGooghari, Silvia Severini, Masoud\nJalili Sabet, Fran√ßois Yvon, and Hinrich Sch√ºtze.\n2022. Graph-based multilingual label propagation\nfor low-resource part-of-speech tagging. InProceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 1577‚Äì1589,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nMasoud Jalili Sabet, Philipp Dufter, Fran√ßois Yvon,\nand Hinrich Sch√ºtze. 2020. SimAlign: High quality\nword alignments without parallel training data using\nstatic and contextualized embeddings. InFindings\nof the Association for Computational Linguistics:\nEMNLP2020,pages1627‚Äì1643,Online.Association\nfor Computational Linguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. InProceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282‚Äì6293, Online. Association for Computational\nLinguistics.\nDivyanshuKakwani,AnoopKunchukuttan,SatishGolla,\nGokul N.C., Avik Bhattacharyya, Mitesh M. Khapra,\nandPratyushKumar.2020. IndicNLPSuite: Monolin-\ngual corpora, evaluation benchmarks and pre-trained\nmultilingual language models for Indian languages.\nIn Findings of the Association for Computational\nLinguistics: EMNLP2020,pages4948‚Äì4961,Online.\nAssociation for Computational Linguistics.\nFajri Koto and Ikhwan Koto. 2020. Towards computa-\ntional linguistics in Minangkabau language: Studies\non sentiment analysis and machine translation. In\nProceedings of the 34th Pacific Asia Conference on\nLanguage, Information and Computation, pages 138‚Äì\n148, Hanoi, Vietnam. Association for Computational\nLinguistics.\nJuliaKreutzer,IsaacCaswell,LisaWang,AhsanWahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Beno√Æt Sagot, Clara Rivera,\nAnnette Rios, Isabel Papadimitriou, Salomey Osei,\nPedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias M√ºller, Andr√© M√ºller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyakeni,\nJamshidbek Mirzakhalov, Tapiwanashe Matangira,\nColinLeong,NzeLawson,SnehaKudugunta,Yacine\nJernite, Mathias Jenny, Orhan Firat, Bonaventure\nF. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine √áabuk Ballƒ±, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2022. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Compu-\ntational Linguistics, 10:50‚Äì72.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. InProceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 66‚Äì75,\nMelbourne,Australia.AssociationforComputational\nLinguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66‚Äì71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-\ntacharyya. 2018. The IIT Bombay English-Hindi\nparallel corpus. InProceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation(LREC2018) , Miyazaki, Japan.European\nLanguage Resources Association (ELRA).\nHugo Lauren√ßon, Lucile Saulnier, Thomas Wang,\nChristopherAkiki,AlbertVillanovadelMoral,Teven\nLe Scao, Leandro Von Werra, Chenghao Mou, Ed-\nuardo Gonz√°lez Ponferrada, Huu Nguyen, et al. 2022.\nThe BigScience ROOTS Corpus: A 1.6 TB Compos-\nite Multilingual Dataset. InThirty-sixth Conference\non Neural Information Processing Systems Datasets\nand Benchmarks Track.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuliƒá, and\nGoranGlava≈°.2020. Fromzerotohero: Onthelimita-\ntions of zero-shot language transfer with multilingual\nTransformers. InProceedingsofthe2020Conference\non Empirical Methods in Natural Language Process-\ning(EMNLP),pages4483‚Äì4499,Online.Association\nfor Computational Linguistics.\nColinLeong,JoshuaNemecek,JacobMansdorfer,Anna\nFilighera, Abraham Owodunni, and Daniel White-\nnack. 2022. Bloom library: Multimodal datasets in\n300+ languages for a variety of downstream tasks. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, Decem-\nber 7-11, 2022, pages 8608‚Äì8621. Association for\nComputational Linguistics.\n1095\nChunlan Ma, Ayyoob ImaniGooghari, Haotian Ye,\nEhsaneddin Asgari, and Hinrich Sch√ºtze. 2023.\nTaxi1500: A multilingual dataset for text classifi-\ncation in 1500 languages.\nMartin Majli≈°. 2011. W2C ‚Äì web to corpus ‚Äì corpora.\nLINDAT/CLARIAH-CZdigitallibraryattheInstitute\nof Formal and Applied Linguistics (√öFAL), Faculty\nof Mathematics and Physics, Charles University.\nJamshidbek Mirzakhalov, Anoop Babu, Duygu Ataman,\nSherzod Kariev, Francis Tyers, Otabek Abduraufov,\nMammad Hajili, Sardana Ivanova, Abror Khaytbaev,\nAntonio Laverghetta Jr., Bekhzodbek Moydinboyev,\nEsra Onal, Shaxnoza Pulatova, Ahsan Wahab, Orhan\nFirat, and Sriram Chellappan. 2021. A large-scale\nstudy of machine translation in Turkic languages. In\nProceedings of the 2021 Conference on Empirical\nMethodsinNaturalLanguageProcessing ,pages5876‚Äì\n5890, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nSteven Moran, Christian Bentz, Ximena Gutierrez-\nVasques, Olga Pelloni, and Tanja Samardzic. 2022.\nTeDDi sample: Text data diversity sample for lan-\nguage comparison and multilingual NLP. InPro-\nceedings of the Thirteenth Language Resources and\nEvaluation Conference, pages 1150‚Äì1158, Marseille,\nFrance. European Language Resources Association.\nMakoto Morishita, Jun Suzuki, and Masaaki Nagata.\n2020. JParaCrawl: A large scale web-based English-\nJapanese parallel corpus. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 3603‚Äì3609, Marseille, France. European\nLanguage Resources Association.\nToshiakiNakazawa,HideyaMino,IsaoGoto,RajDabre,\nShohei Higashiyama, Shantipriya Parida, Anoop\nKunchukuttan, Makoto Morishita, Ond≈ôej Bojar,\nChenhui Chu, Akiko Eriguchi, Kaori Abe, Yusuke\nOda, and Sadao Kurohashi. 2022. Overview of the\n9th workshop on Asian translation. InProceedings\nof the 9th Workshop on Asian Translation, pages\n1‚Äì36, Gyeongju, Republic of Korea. International\nConference on Computational Linguistics.\nToshiaki Nakazawa, Hideki Nakayama, Chenchen Ding,\nRaj Dabre, Shohei Higashiyama, Hideya Mino, Isao\nGoto, Win Pa Pa, Anoop Kunchukuttan, Shantipriya\nParida, Ond≈ôej Bojar, Chenhui Chu, Akiko Eriguchi,\nKaori Abe, Yusuke Oda, and Sadao Kurohashi. 2021.\nOverview of the 8th workshop on Asian translation.\nInProceedings of the 8th Workshop on Asian Trans-\nlation (WAT2021), pages 1‚Äì45, Online. Association\nfor Computational Linguistics.\nGraham Neubig. 2011. The Kyoto free translation task.\nhttp://www.phontron.com/kftt.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021a.\nSmall data? no problem! exploring the viability\nof pretrained multilingual language models for low-\nresourced languages. InProceedings of the 1st Work-\nshoponMultilingualRepresentationLearning ,pages\n116‚Äì126, Punta Cana, Dominican Republic. Associa-\ntion for Computational Linguistics.\nKelechi Ogueji, Yuxin Zhu, and Jimmy Lin. 2021b.\nSmall data? no problem! exploring the viability\nof pretrained multilingual language models for low-\nresourced languages. InProceedings of the 1st Work-\nshoponMultilingualRepresentationLearning ,pages\n116‚Äì126.\nChester Palen-Michel, June Kim, and Constantine Lig-\nnos. 2022. Multilingual open text release 1: Public\ndomain news in 44 languages. InProceedings of\nthe Thirteenth Language Resources and Evaluation\nConference, pages 2080‚Äì2089, Marseille, France.\nEuropean Language Resources Association.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman,KevinKnight,andHengJi.2017. Cross-lingual\nname tagging and linking for 282 languages. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1946‚Äì1958, Vancouver, Canada.\nAssociation for Computational Linguistics.\nJonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James\nCross, Sebastian Riedel, and Mikel Artetxe. 2022.\nLifting the curse of multilinguality by pre-training\nmodular transformers. InProceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3479‚Äì3495, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJonasPfeiffer,IvanVuliƒá,IrynaGurevych,andSebastian\nRuder. 2021. UNKs everywhere: Adapting multilin-\ngual language models to new scripts. InProceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 10186‚Äì10203,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nRobertsRozisandRaivisSkadi n, ≈°.2017. TildeMODEL\n-multilingualopendataforEUlanguages. In Proceed-\ningsofthe21stNordicConferenceonComputational\nLinguistics, pages 263‚Äì265, Gothenburg, Sweden.\nAssociation for Computational Linguistics.\nHassan Sajjad, Ahmed Abdelali, Nadir Durrani, and\nFahim Dalvi. 2020. AraBench: Benchmarking di-\nalectal Arabic-English machine translation. InPro-\nceedingsofthe28thInternationalConferenceonCom-\nputational Linguistics, pages 5094‚Äì5107, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nJulianSalazar,DavisLiang,ToanQ.Nguyen,andKatrin\nKirchhoff. 2020. Masked language model scoring.\n1096\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n2699‚Äì2712, Online. Association for Computational\nLinguistics.\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun,\nHongyu Gong, and Francisco Guzm√°n. 2021. Wiki-\nMatrix: Mining 135M parallel sentences in 1620\nlanguage pairs from Wikipedia. InProceedings of\nthe 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main\nVolume, pages 1351‚Äì1361, Online. Association for\nComputational Linguistics.\nSilvia Severini, Ayyoob Imani, Philipp Dufter, and Hin-\nrichSch√ºtze.2022. Towardsabroadcoveragenamed\nentity resource: A data-efficient approach for many\ndiverse languages.arXiv preprint arXiv:2201.12219.\nAditya Siddhant, Ankur Bapna, Orhan Firat, Yuan Cao,\nMiaXuChen,IsaacCaswell,andXavierGarcia.2022.\nTowards the next 1000 languages in multilingual ma-\nchine translation: Exploring the synergy between su-\npervisedandself-supervisedlearning. arXivpreprint\narXiv:2201.03110.\nAnil Kumar Singh. 2008. Named entity recognition\nfor south and south East Asian languages: Taking\nstock. InProceedings of the ƒ≤CNLP-08 Workshop\non Named Entity Recognition for South and South\nEast Asian Languages.\nPedro Javier Ortiz Su√°rez, Beno√Æt Sagot, and Laurent\nRomary.2019. Asynchronouspipelineforprocessing\nhuge corpora on medium to low resource infrastruc-\ntures. In 7th Workshop on the Challenges in the\nManagement of Large Corpora (CMLC-7). Leibniz-\nInstitut f√ºr Deutsche Sprache.\nJ√∂rgTiedemann.2012. Paralleldata,toolsandinterfaces\nin opus. InProceedings of the Eight International\nConference on Language Resources and Evaluation\n(LREC‚Äô12), Istanbul, Turkey. European Language\nResources Association (ELRA).\nIulia Turc, Kenton Lee, Jacob Eisenstein, Ming-Wei\nChang, and Kristina Toutanova. 2021. Revisiting the\nprimacy of english in zero-shot cross-lingual transfer.\nCoRR, abs/2106.16171.\nHai Wang, Dian Yu, Kai Sun, Jianshu Chen, and Dong\nYu. 2019. Improving pre-trained multilingual model\nwith vocabulary expansion. InProceedings of the\n23rd Conference on Computational Natural Lan-\nguage Learning (CoNLL), pages 316‚Äì327, Hong\nKong,China.AssociationforComputationalLinguis-\ntics.\nMingyang Wang, Heike Adel, Lukas Lange, Jannik\nStr√∂tgen, and Hinrich Sch√ºtze. 2023. NLNDE at\nsemeval-2023 task 12: Adaptive pretraining and\nsource language selection for low-resource multi-\nlingual sentiment analysis.CoRR, abs/2305.00090.\nXinyiWang,SebastianRuder,andGrahamNeubig.2022.\nExpanding pretrained models to thousands more lan-\nguages via lexicon-based adaptation. InProceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages863‚Äì877,Dublin,Ireland.AssociationforCom-\nputational Linguistics.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm√°n, Ar-\nmand Joulin, and Edouard Grave. 2020a. Ccnet:\nExtracting high quality monolingual datasets from\nwebcrawldata. In ProceedingsofThe12thLanguage\nResources and Evaluation Conference, LREC 2020,\nMarseille, France, May 11-16, 2020, pages 4003‚Äì\n4012. European Language Resources Association.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm√°n, Ar-\nmand Joulin, and Edouard Grave. 2020b. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. InProceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n4003‚Äì4012, Marseille, France. European Language\nResources Association.\nLintingXue,NoahConstant,AdamRoberts,MihirKale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trainedtext-to-texttransformer. InProceedingsof\nthe 2021 Conference of the North American Chapter\noftheAssociationforComputationalLinguistics: Hu-\nman Language Technologies, pages 483‚Äì498, Online.\nAssociation for Computational Linguistics.\nJian Yang, Shuming Ma, Dongdong Zhang, Shuangzhi\nWu, Zhoujun Li, and Ming Zhou. 2020. Alternating\nlanguage modeling for cross-lingual pre-training. In\nProceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 9386‚Äì9393.\nRodolfo Zevallos, John Ortega, William Chen, Richard\nCastro, N√∫ria Bel, Cesar Toshio, Renzo Venturas,\nHilarioAradiel,andNelsiMelgarejo.2022. Introduc-\ning QuBERT: A large monolingual corpus and BERT\nmodel for Southern Quechua. InProceedings of the\nThird Workshop on Deep Learning for Low-Resource\nNatural Language Processing, pages 1‚Äì13, Hybrid.\nAssociation for Computational Linguistics.\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and\nHinrich Sch√ºtze. 2020. Masking as an efficient alter-\nnative to finetuning for pretrained language models.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2226‚Äì2241, Online. Association for Computa-\ntional Linguistics.\nA N-grams LMs and Language\nDivergence\nPerplexity and Language Divergence.Perplexity\nmeasures how well a model predicts a sample test\ndata. Assuming a test data contains sequences of\n1097\ncharacters ùëÜ= ùëê‚Ñé1,ùëê‚Ñé2,¬∑¬∑¬∑ ,ùëê‚Ñéùëá, perplexity (PP)\nofùëÜgivenann-gramcharacterlevellanguagemodel\nùëÄis computed as follows:\nPP(ùëÜ,ùëÄ )=\nùëá\nÓµ™Óµ´‚àö ùëá/‚àöÔ∏Å‚àöÔ∏Çoductdi‚àöÔ∏Ñ‚àöÔ∏Ålay.Ô£∂\nùë°=1\n1\nP (ùëê‚Ñéùë° |ùëê‚Ñéùë°‚àí1\n1\n) (1)\nwhere P (ùëê‚Ñéùë° |ùëê‚Ñéùë°‚àí1\n1\n)is computed as by dividing\nthe observed frequency (ùê∂) of ùëê‚Ñéùë°‚àí1\n1 ùëê‚Ñéùëñ by the\nobserved frequency ofùëê‚Ñéùë°‚àí1\n1 in ùëÄtraining data:\nP\n(\nùëê‚Ñéùë° |ùëê‚Ñéùë°‚àí1\n1\n)\n=\nùê∂(ùëê‚Ñéùë°‚àí1\n1 ùëê‚Ñéùë°\n)\nùê∂(ùëê‚Ñéùë°‚àí1\n1\n) (2)\nGiventhedefinitionofperplexity,wecandetermine\nhow well a trained language model on languageùêø1\npredicts the test text of languageùêø2 and vice-versa.\nThedivergencebetweentwolanguagesiscomputed\nwith the maximum of the perplexity values in both\ndirections. Two reasons lead to the use ofmax:\nfirst, a symmetrical divergence is required, and\nsecond, languages differ in their complexity, so\none direction of computing perplexity may result\nin a much lower perplexity than another. Thus,\ncomparing perplexity results becomes difficult. As\nan example, the Kuanua language (ksd_Latn) has\nshort words and a simple structure, which results\nin 3‚àígram models getting lower perplexity on its\ntext compared to other languages. The lower the\nperplexity the smaller the divergence between lan-\nguages. The divergence (D) between languageùêøùëñ\nand ùêøùëó with trained language models ofùëÄùêøùëß and\ntest texts ofùëÜùêøùëß , whereùêøùëß is the corresponding\nlanguage, computed as follows:\nDùêøùëñ,ùêøùëó = max(PP(ùëÜùêøùëñ ,ùëÄùêøùëó ),PP(ùëÜùêøùëó ,ùëÄùêøùëñ )) (3)\nRuns and Data.The data used to train and test\nthe character level n-gram models is the same data\nused for the training and testing of theGlot500-m.\nThe training of the models was limited to100,000\nsentences‚Äô per language-script. We use KenLM\nlibrary (Heafield, 2011) to build n-gram models.\nThis library uses an interpolated modified Kneser-\nNey smoothing for estimating the unseen n-grams.\nOur evaluation has been performed over 7 n-gram\nmodels (3 ‚â§ùëõ‚â§9).\nBaseline and Evaluation.Language family trees\nwere used as a baseline for evaluating the diver-\ngence measures of the proposed approach. We\nobtained language family tree data from Ethno-\nlogue online version (Eberhard et al., 2022). For\neachlanguage,thefamilytreefollowsthegeneralor-\nder from largest typological language family group\nto smallest. There is only one family tree for each\nlanguage in the baseline data. Nodes in the family\ntree represent typological language family groups.\nEach node only has one parent, so if a node is\ncommon in the family tree of two languages, its\nparent is also common. We evaluate our perplex-\nity method on the following binary classification\ntask: Do the majority of a languageùêøùëß‚Äôsùëònearest\nneighbors belong to the same typological language\nfamily group asùêøùëß? Assuming languagesùêøùëñ and\nùêøùëó, with the following family trees:\nùëáùêøùëñ : 1 ‚Üí 2 ‚Üí 3 ‚Üí 4 ‚Üí 5 ‚Üí 6\nùëáùêøùëó : 1 ‚Üí 2 ‚Üí 7 ‚Üí 8\nThese 2 languages belong to the same typological\nfamily group with family tree levels ofùëô ‚àà{1,2},\nbut not with family tree levels ofùëô = 3 and higher.\nResult. When it comes to language families, the\nmajority of studies only refer to the largest typo-\nlogical language family group (levelùëô = 1). Here,\nwe also assess our methodology for other levels.\nThe results of classification accuracy for3‚àígram\nmodel, ùëò ‚àà{1,3,7,13,21}and ùëô ‚àà{1,2,3,max}\nareshowninTable10. Incaseswherethemaximum\nlevel of a tree is less than theùëôparameter, the max-\nimum level for that language is used. Languages\nwithout a family or no other family member in our\ndata are excluded. We only report the3‚àígram\nmodel results as it gets the best results in most\nconfigurations among other n-gram models. With\nincreasing ùëô, the accuracy decreases, since more\nlanguages fall outside the same typological family.\nAsùëòincreases,theaccuracydecreases,becauselan-\nguages with faraway neighbors are being included\nbut the number of languages in the language typo-\nlogical group family will remain the same. There\nare times when languages have a lot of loan words\nfromotherlanguagesbecauseofgeologicalproxim-\nity or historical reasons (e.g, colonization), which\nmakesthemsimilartothelanguagestheyborrowed\nwordsfrominourmethod. Howevertheyarediffer-\nent when it comes to their typological families and\nourmethodfailsinthesecases. Aymara(Macrolan-\nguage: aym_Latn) and Quechua (Macrolanguage:\nque_Latn), for example, had a great deal of contact\nand influence on each other, but they do not belong\nto the same typological group. As well, some of\nthe typological families are not that large, which\nmakes our results worse whenùëòincreases. This is\n1098\nthe case, for instance, of the Tarascan typological\nfamily which only has two members.\nmodel ùëô ùëò accuracy (%)\n3-gram 1 1 84 .45\n3-gram 1 3 75 .77\n3-gram 1 7 69 .08\n3-gram 1 13 62 .75\n3-gram 1 21 55 .33\n3-gram 2 1 79 .75\n3-gram 2 3 67 .63\n3-gram 2 7 59 .49\n3-gram 2 13 51 .36\n3-gram 2 21 42 .68\n3-gram 3 1 75 .05\n3-gram 3 3 60 .22\n3-gram 3 7 49 .55\n3-gram 3 13 38 .34\n3-gram 3 21 29 .84\n3-gram max 1 59 .31\n3-gram max 3 36 .89\n3-gram max 7 18 .81\n3-gram max 13 6 .87\n3-gram max 21 2 .89\nTable 10: Detecting the typological relatedness of lan-\nguage with n-gram divergence: (Eq. 3);ùëô: level of\ntypological language family group;ùëò: number of near-\nest language neighbors.\nB Languages\nThe list of languages used to trainGlot500-mwith\nthe amount of available data for each language is\navailable in Tables 11, 12 and 13.\nOn Macrolanguages The presence of language\ncodes that are supersets of other language codes\nwithin datasets is not uncommon (Kreutzer et al.,\n2022). This issue becomes more prevalent in ex-\ntensive collections. Within the ISO 639-3 standard,\nthese languages are referred to as macrolanguages.\nWhen confronted with macrolanguages, if it is not\nfeasibletoascertainthespecificindividuallanguage\ncontainedwithinadataset,themacrolanguagecode\nis retained. Consequently, it is possible that in\nGlot2000-cand Glot500-cboth the corpora for the\nmacrolanguage and its individual languages have\nbeen included.\nC List of data sources\nThe datasets and repositories used in this project\ninvolve: AI4Bharat,5 AIFORTHAI-LotusCorpus,6\nAdd (El-Haj et al., 2018), AfriBERTa (Ogueji\net al., 2021b), AfroMAFT (Adelani et al., 2022;\nXue et al., 2021), Anuvaad,7 AraBench (Sajjad\net al., 2020), AUTSHUMATO,8 Bloom (Leong\net al., 2022), CC100 (Conneau et al., 2020;\nWenzek et al., 2020a), CCNet (Wenzek et al.,\n2020b), CMU_Haitian_Creole,9 CORP.NCHLT,10\nClarin,11 DART (Alsarsour et al., 2018), Earth-\nlings (Dunn, 2020), FFR,12 Flores200 (Costa-juss√†\net al., 2022), GiossaMedia (G√≥ngora et al., 2022,\n2021), Glosses (Camacho-Collados et al., 2016),\nHabibi (El-Haj, 2020), HinDialect (Bafna, 2022),\nHornMT,13 IITB (Kunchukuttan et al., 2018), In-\ndicNLP (Nakazawa et al., 2021), Indiccorp (Kak-\nwanietal.,2020),isiZulu, 14 JParaCrawl(Morishita\net al., 2020), KinyaSMT,15 LeipzigData (Goldhahn\net al., 2012), Lindat,16 Lingala_Song_Lyrics,17\nLyrics,18 MC4 (Raffel et al., 2020), MTData\n(Gowda et al., 2021), MaCoCu (Ba√±√≥n et al.,\n2022), Makerere MT Corpus,19 Masakhane com-\nmunity,20 Mburisano_Covid,21 Menyo20K (Ade-\nlani et al., 2021), Minangkabau corpora (Koto\nand Koto, 2020), MoT (Palen-Michel et al.,\n2022), NLLB_seed (Costa-juss√† et al., 2022),\nNart/abkhaz,22 OPUS (Tiedemann, 2012), OS-\nCAR (Su√°rez et al., 2019), ParaCrawl (Ba√±√≥n\net al., 2020), Parallel Corpora for Ethiopian Lan-\n5https://ai4bharat.org/\n6https://github.com/korakot/corpus/releases/\ndownload/v1.0/AIFORTHAI-LotusCorpus.zip\n7https://github.com/project-anuvaad/\nanuvaad-parallel-corpus\n8https://autshumato.sourceforge.net/\n9http://www.speech.cs.cmu.edu/haitian/text/\n10https://repo.sadilar.org/handle/20.500.12185/\n7\n11https://www.clarin.si/\n12https://github.com/bonaventuredossou/ffr-v1/\ntree/master/FFR-Dataset\n13https://github.com/asmelashteka/HornMT\n14https://zenodo.org/record/5035171\n15https://github.com/pniyongabo/kinyarwandaSMT\n16https://lindat.cz/faq-repository\n17https://github.com/espoirMur/songs_lyrics_\nwebscrap\n18https://lyricstranslate.com/\n19https://zenodo.org/record/5089560\n20https://github.com/masakhane-io/\nmasakhane-community\n21https://repo.sadilar.org/handle/20.500.12185/\n536\n22https://huggingface.co/datasets/Nart/abkhaz_\ntext\n1099\nLanguage-Script |Sent| Family HeadLanguage-Script |Sent| Family HeadLanguage-Script |Sent| Family Head\nhbs_Latn 63411156 indo1319 vec_Latn 514240 indo1319 swh_Latn 95776 atla1278 yes\nmal_Mlym 48098273 drav1251 yesjpn_Jpan 510722 japo1237 yes alt_Cyrl 95148 turk1311\naze_Latn 46300705 yes lus_Latn 509250 sino1245 rmn_Grek 94533 indo1319\nguj_Gujr 45738685 indo1319 yescrs_Latn 508755 indo1319 miq_Latn 94343 misu1242\nben_Beng 43514870 indo1319 yeskqn_Latn 507913 atla1278 kaa_Cyrl 88815 turk1311\nkan_Knda 41836495 drav1251 yesndo_Latn 496613 atla1278 kos_Latn 88603 aust1307\ntel_Telu 41580525 drav1251 yessnd_Arab 488730 indo1319 yes grn_Latn 87568\nmlt_Latn 40654838 afro1255 yue_Hani 484700 sino1245 lhu_Latn 87255 sino1245\nfra_Latn 39197581 indo1319 yestiv_Latn 483064 atla1278 lzh_Hani 86035 sino1245\nspa_Latn 37286756 indo1319 yeskua_Latn 473535 atla1278 ajp_Arab 83297 afro1255\neng_Latn 36122761 indo1319 yeskwy_Latn 473274 atla1278 cmn_Hani 80745 sino1245 yes\nfil_Latn 33493255 aust1307 yes hin_Latn 466175 indo1319 gcf_Latn 80737 indo1319\nnob_Latn 32869205 indo1319 iku_Cans 465011 rmn_Cyrl 79925 indo1319\nrus_Cyrl 31787973 indo1319 yeskal_Latn 462430 eski1264 kjh_Cyrl 79262 turk1311\ndeu_Latn 31015993 indo1319 yestdt_Latn 459818 aust1307 rng_Latn 78177 atla1278\ntur_Latn 29184662 turk1311 yesgsw_Latn 449240 indo1319 mgh_Latn 78117 atla1278\npan_Guru 29052537 indo1319 yesmfe_Latn 447435 indo1319 xmv_Latn 77896 aust1307\nmar_Deva 28748897 indo1319 yesswc_Latn 446378 atla1278 ige_Latn 77114 atla1278\npor_Latn 27824391 indo1319 yesmon_Latn 437950 mong1349 rmy_Latn 76991 indo1319\nnld_Latn 25061426 indo1319 yesmos_Latn 437666 atla1278 srm_Latn 76884 indo1319\nara_Arab 24524122 yes kik_Latn 437228 atla1278 bak_Latn 76809 turk1311\nzho_Hani 24143786 yes cnh_Latn 436667 sino1245 gur_Latn 76151 atla1278\nita_Latn 23539857 indo1319 yesgil_Latn 434529 aust1307 idu_Latn 75106 atla1278\nind_Latn 23018106 aust1307 yespon_Latn 434522 aust1307 yom_Latn 74818 atla1278\nell_Grek 22033282 indo1319 yesumb_Latn 431589 atla1278 tdx_Latn 74430 aust1307\nbul_Cyrl 21823004 indo1319 yeslvs_Latn 422952 indo1319 mzn_Arab 73719 indo1319\nswe_Latn 20725883 indo1319 yessco_Latn 411591 indo1319 cfm_Latn 70227 sino1245\nces_Latn 20376340 indo1319 yesori_Orya 410827 yes zpa_Latn 69237 otom1299\nisl_Latn 19547941 indo1319 yesarg_Latn 410683 indo1319 kbd_Cyrl 67914 abkh1242\npol_Latn 19339945 indo1319 yeskur_Latn 407169 indo1319 yes lao_Laoo 66966 taik1256 yes\nron_Latn 19190217 indo1319 yesdhv_Latn 405711 aust1307 nap_Latn 65826 indo1319\ndan_Latn 19174573 indo1319 yesluo_Latn 398974 nilo1247 qub_Latn 64973 quec1387\nhun_Latn 18800025 ural1272 yeslun_Latn 395764 atla1278 oke_Latn 64508 atla1278\ntgk_Cyrl 18659517 indo1319 nzi_Latn 394247 atla1278 ote_Latn 64224 otom1299\nsrp_Latn 18371769 indo1319 yesgug_Latn 392227 tupi1275 bsb_Latn 63634 aust1307\nfas_Arab 18277593 yes bar_Latn 387070 indo1319 ogo_Latn 61901 atla1278\nceb_Latn 18149215 aust1307 bci_Latn 384059 atla1278 abn_Latn 61830 atla1278\nheb_Hebr 18128962 afro1255 yeschk_Latn 380596 aust1307 ldi_Latn 61827 atla1278\nhrv_Latn 17882932 indo1319 yesroh_Latn 377067 indo1319 ayr_Latn 61570 ayma1253\nglg_Latn 17852274 indo1319 yesaym_Latn 373329 ayma1253 gom_Deva 61140 indo1319\nfin_Latn 16730388 ural1272 yesyap_Latn 358929 aust1307 bba_Latn 61123 atla1278\nslv_Latn 15719210 indo1319 yesssw_Latn 356561 atla1278 aln_Latn 60989 indo1319\nvie_Latn 15697827 aust1305 yesquz_Latn 354781 quec1387 leh_Latn 59944 atla1278\nmkd_Cyrl 14717004 indo1319 yessah_Cyrl 352697 turk1311 ban_Latn 59805 aust1307\nslk_Latn 14633631 indo1319 yestsn_Latn 350954 atla1278 ace_Latn 59333 aust1307\nnor_Latn 14576191 indo1319 yeslmo_Latn 348135 indo1319 pes_Arab 57511 indo1319 yes\nest_Latn 13600579 yes ido_Latn 331239 arti1236 skg_Latn 57228 aust1307\nltz_Latn 12997242 indo1319 abk_Cyrl 321578 abkh1242 ary_Arab 56933 afro1255\neus_Latn 12775959 yes zne_Latn 318871 atla1278 hus_Latn 56176 maya1287\nlit_Latn 12479626 indo1319 yesquy_Latn 311040 quec1387 glv_Latn 55641 indo1319\nkaz_Cyrl 12378727 turk1311 yeskam_Latn 310659 atla1278 fat_Latn 55609 atla1278\nlav_Latn 12143980 indo1319 yesbbc_Latn 310420 aust1307 frr_Latn 55254 indo1319\nbos_Latn 11014744 indo1319 yesvol_Latn 310399 arti1236 mwn_Latn 54805 atla1278\nepo_Latn 8737198 arti1236 yes wal_Latn 309873 gong1255 mai_Deva 54687 indo1319\ncat_Latn 8648271 indo1319 yes uig_Arab 307302 turk1311 yes dua_Latn 53392 atla1278\ntha_Thai 7735209 taik1256 yes vmw_Latn 306899 atla1278 dzo_Tibt 52732 sino1245\nukr_Cyrl 7462046 indo1319 yeskwn_Latn 305362 atla1278 ctd_Latn 52135 sino1245\ntgl_Latn 7411064 aust1307 yes pam_Latn 303737 aust1307 nnb_Latn 52041 atla1278\nsin_Sinh 7293178 indo1319 yes seh_Latn 300243 atla1278 sxn_Latn 51749 aust1307\ngle_Latn 7225513 indo1319 yes tsc_Latn 298442 atla1278 mps_Latn 50645 tebe1251\nhin_Deva 7046700 indo1319 yesnyk_Latn 297976 atla1278 mny_Latn 50581 atla1278\nkor_Hang 6468444 kore1284 yeskmb_Latn 296269 atla1278 gkp_Latn 50549 mand1469\nory_Orya 6266475 indo1319 zai_Latn 277632 otom1299 kat_Latn 50424 kart1248\nurd_Arab 6009594 indo1319 yesgym_Latn 274512 chib1249 bjn_Latn 49068 aust1307\nswa_Latn 5989369 yes bod_Tibt 273489 sino1245 acr_Latn 48886 maya1287\nsqi_Latn 5526836 indo1319 yes nde_Latn 269931 atla1278 dtp_Latn 48468 aust1307\nbel_Cyrl 5319675 indo1319 yes fon_Latn 268566 atla1278 lam_Latn 46853 atla1278\nafr_Latn 5157787 indo1319 yes ber_Latn 264426 bik_Latn 46561\nnno_Latn 4899103 indo1319 nbl_Latn 259158 atla1278 poh_Latn 46454 maya1287\ntat_Cyrl 4708088 turk1311 kmr_Latn 256677 indo1319 phm_Latn 45862 atla1278\nTable 11: List of languages used to train Glot500-m (Part I).\n1100\nLanguage-Script |Sent| Family HeadLanguage-Script |Sent| Family HeadLanguage-Script |Sent| Family Head\nast_Latn 4683554 indo1319 guc_Latn 249044 araw1281 hrx_Latn 45716 indo1319\nmon_Cyrl 4616960 mong1349 yesmam_Latn 248348 maya1287 quh_Latn 45566 quec1387\nhbs_Cyrl 4598073 indo1319 nia_Latn 247406 aust1307 hyw_Cyrl 45379 indo1319\nhau_Latn 4368483 afro1255 yes nyn_Latn 241992 atla1278 rue_Cyrl 45369 indo1319\nsna_Latn 4019596 atla1278 cab_Latn 240101 araw1281 eml_Latn 44630 indo1319\nmsa_Latn 3929084 yes top_Latn 239232 toto1251 acm_Arab 44505 afro1255\nsom_Latn 3916769 afro1255 yes tog_Latn 231969 atla1278 tob_Latn 44473 guai1249\nsrp_Cyrl 3864091 indo1319 yes mco_Latn 231209 mixe1284 ach_Latn 43974 nilo1247\nmlg_Latn 3715802 yes tzh_Latn 230706 maya1287 vep_Latn 43076 ural1272\nzul_Latn 3580113 atla1278 pms_Latn 227748 indo1319 npi_Deva 43072 indo1319\narz_Arab 3488224 afro1255 wuu_Hani 224088 sino1245 tok_Latn 42820 arti1236\nnya_Latn 3409030 atla1278 plt_Latn 220413 aust1307 sgs_Latn 42467 indo1319\ntam_Taml 3388255 drav1251 yesyid_Hebr 220214 indo1319 yes lƒ≥_Latn 42447 indo1319\nhat_Latn 3226932 indo1319 ada_Latn 219427 atla1278 myv_Cyrl 42147 ural1272\nuzb_Latn 3223485 turk1311 yes iba_Latn 213615 aust1307 tih_Latn 41873 aust1307\nsot_Latn 3205510 atla1278 kek_Latn 209932 maya1287 tat_Latn 41640 turk1311\nuzb_Cyrl 3029947 turk1311 koo_Latn 209375 atla1278 lfn_Latn 41632 arti1236\ncos_Latn 3015055 indo1319 sop_Latn 206501 atla1278 cgg_Latn 41196 atla1278\nals_Latn 2954874 indo1319 kac_Latn 205542 sino1245 ful_Latn 41188 atla1278\namh_Ethi 2862985 afro1255 yes qvi_Latn 205447 quec1387 gor_Latn 41174 aust1307\nsun_Latn 2586011 aust1307 yes cak_Latn 204472 maya1287 ile_Latn 40984 arti1236\nwar_Latn 2584810 aust1307 kbp_Latn 202877 atla1278 ium_Latn 40683 hmon1336\ndiv_Thaa 2418687 indo1319 ctu_Latn 201662 maya1287 teo_Latn 40203 nilo1247\nyor_Latn 2392359 atla1278 kri_Latn 201087 indo1319 kia_Latn 40035 atla1278\nfao_Latn 2365271 indo1319 mau_Latn 199134 otom1299 crh_Cyrl 39985 turk1311\nuzn_Cyrl 2293672 turk1311 scn_Latn 199068 indo1319 crh_Latn 39896 turk1311\nsmo_Latn 2290439 aust1307 tyv_Cyrl 198649 turk1311 enm_Latn 39809 indo1319\nbak_Cyrl 2264196 turk1311 ina_Latn 197315 arti1236 sat_Olck 39614 aust1305\nilo_Latn 2106531 aust1307 btx_Latn 193701 aust1307 mad_Latn 38993 aust1307\ntso_Latn 2100708 atla1278 nch_Latn 193129 utoa1244 cac_Latn 38812 maya1287\nmri_Latn 2046850 aust1307 ncj_Latn 192962 utoa1244 hnj_Latn 38611 hmon1336\nhmn_Latn 1903898 pau_Latn 190529 aust1307 ksh_Latn 38130 indo1319\nasm_Beng 1882353 indo1319 yestoj_Latn 189651 maya1287 ikk_Latn 38071 atla1278\nhil_Latn 1798875 aust1307 pcm_Latn 187594 indo1319 sba_Latn 38040 cent2225\nnso_Latn 1619354 atla1278 dyu_Latn 186367 mand1469 zom_Latn 37013 sino1245\nibo_Latn 1543820 atla1278 kss_Latn 185868 atla1278 bqc_Latn 36881 mand1469\nkin_Latn 1521612 atla1278 afb_Arab 183694 afro1255 bim_Latn 36835 atla1278\nhye_Armn 1463123 indo1319 yesurh_Latn 182214 atla1278 mdy_Ethi 36370 gong1255\noci_Latn 1449128 indo1319 quc_Latn 181559 maya1287 bts_Latn 36216 aust1307\nlin_Latn 1408460 atla1278 new_Deva 181427 sino1245 gya_Latn 35902 atla1278\ntpi_Latn 1401844 indo1319 yao_Latn 179965 atla1278 ajg_Latn 35631 atla1278\ntwi_Latn 1400979 atla1278 ngl_Latn 178498 atla1278 agw_Latn 35585 aust1307\nkir_Cyrl 1397566 turk1311 yes nyu_Latn 177483 atla1278 kom_Cyrl 35249 ural1272\npap_Latn 1360138 indo1319 kab_Latn 176015 afro1255 knv_Latn 35196\nnep_Deva 1317291 indo1319 yestuk_Cyrl 175769 turk1311 giz_Latn 35040 afro1255\nazj_Latn 1315834 turk1311 xmf_Geor 174994 kart1248 hui_Latn 34926 nucl1709\nbcl_Latn 1284493 aust1307 ndc_Latn 174305 atla1278 kpg_Latn 34900 aust1307\nxho_Latn 1262364 atla1278 yes san_Deva 165616 indo1319 yes zea_Latn 34426 indo1319\ncym_Latn 1244783 indo1319 yesnba_Latn 163485 atla1278 aoj_Latn 34349 nucl1708\ngaa_Latn 1222307 atla1278 bpy_Beng 162838 indo1319 csy_Latn 34126 sino1245\nton_Latn 1216118 aust1307 ncx_Latn 162558 utoa1244 azb_Arab 33758 turk1311 yes\ntah_Latn 1190747 aust1307 qug_Latn 162500 quec1387 csb_Latn 33743 indo1319\nlat_Latn 1179913 indo1319 yes rmn_Latn 162069 indo1319 tpm_Latn 33517 atla1278\nsrn_Latn 1172349 indo1319 cjk_Latn 160645 atla1278 quw_Latn 33449 quec1387\newe_Latn 1161605 atla1278 arb_Arab 159884 afro1255 yes rmy_Cyrl 33351 indo1319\nbem_Latn 1111969 atla1278 kea_Latn 158047 indo1319 ixl_Latn 33289 maya1287\nefi_Latn 1082621 atla1278 mck_Latn 157521 atla1278 mbb_Latn 33240 aust1307\nbis_Latn 1070170 indo1319 arn_Latn 155882 arau1255 pfl_Latn 33148 indo1319\norm_Latn 1067699 yes pdt_Latn 155485 indo1319 pcd_Latn 32867 indo1319\nhaw_Latn 1062491 aust1307 her_Latn 154827 atla1278 tlh_Latn 32863 arti1236\nhmo_Latn 1033636 pidg1258 gla_Latn 152563 indo1319 yes suz_Deva 32811 sino1245\nkat_Geor 1004297 kart1248 yes kmr_Cyrl 151728 indo1319 gcr_Latn 32676 indo1319\npag_Latn 983637 aust1307 mwl_Latn 150054 indo1319 jbo_Latn 32619 arti1236\nloz_Latn 964418 atla1278 nav_Latn 147702 atha1245 tbz_Latn 32264 atla1278\nfry_Latn 957422 indo1319 yes ksw_Mymr 147674 sino1245 bam_Latn 32150 mand1469\nmya_Mymr 945180 sino1245 yesmxv_Latn 147591 otom1299 prk_Latn 32085 aust1305\nnds_Latn 944715 indo1319 hif_Latn 147261 indo1319 jam_Latn 32048 indo1319\nrun_Latn 943828 atla1278 wol_Latn 146992 atla1278 twx_Latn 32028 atla1278\nTable 12: List of languages used to train Glot500-m (Part II).\n1101\nLanguage-Script |Sent| Family HeadLanguage-Script |Sent| Family HeadLanguage-Script |Sent| Family Head\npnb_Arab 899895 indo1319 sme_Latn 146803 ural1272 nmf_Latn 31997 sino1245\nrar_Latn 894515 aust1307 gom_Latn 143937 indo1319 caq_Latn 31903 aust1305\nfij_Latn 887134 aust1307 bum_Latn 141673 atla1278 rop_Latn 31889 indo1319\nwls_Latn 882167 aust1307 mgr_Latn 138953 atla1278 tca_Latn 31852 ticu1244\nckb_Arab 874441 indo1319 ahk_Latn 135068 sino1245 yan_Latn 31775 misu1242\nven_Latn 860249 atla1278 kur_Arab 134160 indo1319 xav_Latn 31765 nucl1710\nzsm_Latn 859947 aust1307 yes bas_Latn 133436 atla1278 bih_Deva 31658\nchv_Cyrl 859863 turk1311 bin_Latn 133256 atla1278 cuk_Latn 31612 chib1249\nlua_Latn 854359 atla1278 tsz_Latn 133251 tara1323 kjb_Latn 31471 maya1287\nque_Latn 838486 sid_Latn 130406 afro1255 hne_Deva 31465 indo1319\nsag_Latn 771048 atla1278 diq_Latn 128908 indo1319 wbm_Latn 31394 aust1305\nguw_Latn 767918 atla1278 srd_Latn 127064 zlm_Latn 31345 aust1307\nbre_Latn 748954 indo1319 yes tcf_Latn 126050 otom1299 tui_Latn 31161 atla1278\ntoi_Latn 745385 atla1278 bzj_Latn 124958 indo1319 ifb_Latn 30980 aust1307\npus_Arab 731992 indo1319 yes udm_Cyrl 121705 ural1272 izz_Latn 30894 atla1278\nche_Cyrl 728201 nakh1245 cce_Latn 120636 atla1278 rug_Latn 30857 aust1307\npis_Latn 714783 indo1319 meu_Latn 120273 aust1307 aka_Latn 30704 atla1278\nkon_Latn 685194 chw_Latn 119751 atla1278 pxm_Latn 30698 book1242\noss_Cyrl 683517 indo1319 cbk_Latn 118789 indo1319 kmm_Latn 30671 sino1245\nhyw_Armn 679819 indo1319 ibg_Latn 118733 aust1307 mcn_Latn 30666 afro1255\niso_Latn 658789 atla1278 bhw_Latn 117381 aust1307 ifa_Latn 30621 aust1307\nnan_Latn 656389 sino1245 ngu_Latn 116851 utoa1244 dln_Latn 30620 sino1245\nlub_Latn 654390 atla1278 nyy_Latn 115914 atla1278 ext_Latn 30605 indo1319\nlim_Latn 652078 indo1319 szl_Latn 112496 indo1319 ksd_Latn 30550 aust1307\ntuk_Latn 649411 turk1311 ish_Latn 111814 atla1278 mzh_Latn 30517 mata1289\ntir_Ethi 649117 afro1255 naq_Latn 109747 khoe1240 llb_Latn 30480 atla1278\ntgk_Latn 636541 indo1319 toh_Latn 107583 atla1278 hra_Latn 30472 sino1245\nyua_Latn 610052 maya1287 ttj_Latn 106925 atla1278 mwm_Latn 30432 cent2225\nmin_Latn 609065 aust1307 nse_Latn 105189 atla1278 krc_Cyrl 30353 turk1311\nlue_Latn 599429 atla1278 hsb_Latn 104802 indo1319 tuc_Latn 30349 aust1307\nkhm_Khmr 590429 aust1305 yesami_Latn 104559 aust1307 mrw_Latn 30304 aust1307\ntum_Latn 589857 atla1278 alz_Latn 104392 nilo1247 pls_Latn 30136 otom1299\ntll_Latn 586530 atla1278 apc_Arab 102392 afro1255 rap_Latn 30102 aust1307\nekk_Latn 582595 ural1272 vls_Latn 101900 indo1319 fur_Latn 30052 indo1319\nlug_Latn 566948 atla1278 mhr_Cyrl 100474 ural1272 kaa_Latn 30031 turk1311\nniu_Latn 566715 aust1307 djk_Latn 99234 indo1319 prs_Arab 26823 indo1319 yes\ntzo_Latn 540262 maya1287 wes_Latn 98492 indo1319 san_Latn 25742 indo1319 yes\nmah_Latn 534614 aust1307 gkn_Latn 97041 atla1278 som_Arab 14199 afro1255 yes\ntvl_Latn 521556 aust1307 grc_Grek 96986 indo1319 uig_Latn 9637 turk1311 yes\njav_Latn 516833 aust1307 yes hbo_Hebr 96484 afro1255 hau_Arab 9593 afro1255 yes\nTable 13: List of languages used to train Glot500-m (Part III).\n1102\nguages (Abate et al., 2018), Phontron (Neubig,\n2011), QADI (Abdelali et al., 2021), Quechua-IIC\n(Zevallos et al., 2022), SLI_GalWeb.1.0 (Agerri\net al., 2018), Shami (Abu Kwaik et al., 2018),\nStanford NLP,23 StatMT,24 TICO (Anastasopou-\nlos et al., 2020), TIL (Mirzakhalov et al., 2021),\nTatoeba,25 TeDDi(Moranetal.,2022),Tilde(Rozis\nand Skadin, ≈°, 2017), W2C (Majli≈°, 2011), WAT\n(Nakazawa et al., 2022), WikiMatrix (Schwenk\net al., 2021), Wikipedia,26 Workshop on NER for\nSouth and South East Asian Languages (Singh,\n2008), XLSum (Hasan et al., 2021).\nD Results for Each Task and Language\nWe report the detailed results for all tasks and\nlanguagesinTable14(SentenceRetrievalTatoeba),\n15,16(SentenceRetrievalBible),17(NER),and18\n(POS), 19, 20 (Text Classification), 21, 22 (Round\nTrip Alignment).\nE Perplexity Results for all Languages\nPerplexity number for all languages is presented in\nTable 23, Table 24, and Table 25.\n23https://nlp.stanford.edu/\n24https://statmt.org/\n25https://tatoeba.org/en/\n26https://huggingface.co/datasets/wikipedia\n1103\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nafr_Latn 71.9 76.5 81.1 heb_Hebr 76.3 84.1 76.0 pam_Latn 4.8 5.6 11.0amh_Ethi 35.1 37.5 44.6 hin_Deva 73.8 88.8 85.6 pes_Arab 83.3 86.6 87.6ara_Arab 59.2 66.8 64.2 hrv_Latn 79.6 85.6 89.8 pms_Latn 16.6 12.6 54.5arz_Arab 32.5 47.8 63.5 hsb_Latn 21.5 23.0 53.6 pol_Latn 82.6 89.6 82.4ast_Latn 59.8 59.8 87.4 hun_Latn 76.1 81.8 69.2 por_Latn 91.0 92.1 90.1aze_Latn 62.6 78.3 79.9 hye_Armn 64.6 40.0 83.2 ron_Latn 86.0 89.1 82.8bel_Cyrl 70.0 80.5 81.4 ido_Latn 25.7 28.8 57.6 rus_Cyrl 89.6 91.6 91.5ben_Beng 54.1 68.2 69.4 ile_Latn 34.6 41.9 75.6 slk_Latn 73.2 80.6 75.9bos_Latn 78.5 82.2 92.4 ina_Latn 62.7 66.2 91.4 slv_Latn 72.1 78.0 77.0bre_Latn 10.3 10.9 19.9 ind_Latn 84.3 90.2 88.8 spa_Latn 85.5 89.0 88.9bul_Cyrl 84.4 88.3 86.7 isl_Latn 78.7 84.5 84.0 sqi_Latn 72.2 81.4 84.7cat_Latn 72.8 73.9 78.7 ita_Latn 81.3 84.7 86.4 srp_Latn 78.1 85.0 90.0cbk_Latn 33.2 36.0 49.4 jpn_Jpan 74.4 80.8 72.6 swe_Latn 90.4 92.4 89.7ceb_Latn 15.2 15.0 41.3 kab_Latn 3.7 3.0 16.4 swh_Latn 30.3 34.6 44.1ces_Latn 71.1 81.3 75.1 kat_Geor 61.1 79.1 67.7 tam_Taml 46.9 42.3 66.4cmn_Hani 79.5 84.8 85.6 kaz_Cyrl 60.3 69.9 72.3 tat_Cyrl 10.3 10.3 70.3csb_Latn 21.3 20.2 40.3 khm_Khmr 41.1 45.0 52.5 tel_Telu 58.5 50.4 67.9cym_Latn 45.7 45.7 55.7 kor_Hang 73.4 84.3 78.0 tgl_Latn 47.6 54.2 77.1dan_Latn 91.9 93.9 91.5 kur_Latn 24.1 28.5 54.1 tha_Thai 56.8 39.4 78.1deu_Latn 95.9 94.7 95.0 lat_Latn 33.6 48.0 42.8 tuk_Latn 16.3 14.8 63.5dtp_Latn 5.6 4.7 21.1 lfn_Latn 32.5 35.9 59.3 tur_Latn 77.9 85.4 78.4ell_Grek 76.2 84.1 80.2 lit_Latn 73.4 76.8 65.6 uig_Arab 38.8 58.3 62.6epo_Latn 64.9 68.5 74.3 lvs_Latn 73.4 78.9 76.9 ukr_Cyrl 77.1 88.3 83.7est_Latn 63.9 68.6 69.1 mal_Mlym 80.1 84.4 83.8 urd_Arab 54.4 34.3 80.9eus_Latn 45.9 54.4 52.7 mar_Deva 63.5 81.2 77.9 uzb_Cyrl 25.2 32.2 64.5fao_Latn 45.0 42.7 82.4 mhr_Cyrl 6.5 5.8 34.9 vie_Latn 85.4 87.9 87.0fin_Latn 81.9 85.8 72.3 mkd_Cyrl 70.5 83.9 81.4 war_Latn 8.0 6.5 26.2fra_Latn 85.7 85.8 86.0 mon_Cyrl 60.9 77.3 77.0 wuu_Hani 56.1 47.4 79.7fry_Latn 60.1 62.4 75.1 nds_Latn 28.8 29.0 77.1 xho_Latn 28.9 31.7 56.3gla_Latn 21.0 21.2 41.9 nld_Latn 90.3 91.8 91.8 yid_Hebr 37.3 51.8 74.4gle_Latn 32.0 36.9 50.8 nno_Latn 70.7 77.8 87.8 yue_Hani 50.3 42.3 76.3glg_Latn 72.6 75.8 77.5 nob_Latn 93.5 96.5 95.7 zsm_Latn 81.4 87.4 91.8gsw_Latn 36.8 31.6 69.2 oci_Latn 22.9 23.2 46.9\nTable 14: Top10 accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Sentence Retrieval Tatoeba.\n1104\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nace_Latn 4.4 4.6 53.4 iba_Latn 14.4 13.6 66.0 pan_Guru 43.2 59.4 48.8ach_Latn 4.4 3.2 40.0 ibo_Latn 5.0 3.0 30.4 pap_Latn 12.4 9.2 72.4acr_Latn 2.6 3.4 25.4 ifa_Latn 4.4 4.4 39.2 pau_Latn 4.4 4.0 29.8afr_Latn 76.8 77.2 69.4 ifb_Latn 4.8 3.6 36.6 pcm_Latn 13.6 10.4 66.8agw_Latn 5.8 3.0 36.0 ikk_Latn 3.0 3.2 50.6 pdt_Latn 9.2 8.6 68.6ahk_Latn 3.0 2.6 3.2 ilo_Latn 6.2 3.6 55.0 pes_Arab 69.4 72.2 80.8aka_Latn 5.0 4.2 57.0 ind_Latn 82.6 80.4 72.2 pis_Latn 6.4 5.0 57.2aln_Latn 67.8 72.4 67.6 isl_Latn 62.6 73.6 66.0 pls_Latn 5.0 4.0 34.4als_Latn 51.4 48.0 55.8 ita_Latn 75.4 73.6 70.0 plt_Latn 26.6 28.0 59.8alt_Cyrl 12.6 9.0 50.8 ium_Latn 3.2 3.0 24.8 poh_Latn 3.4 2.4 15.2alz_Latn 4.6 3.8 34.6 ixl_Latn 4.0 3.0 18.4 pol_Latn 79.2 79.8 63.8amh_Ethi 35.4 43.2 52.8 izz_Latn 2.8 2.8 25.6 pon_Latn 5.6 4.4 21.6aoj_Latn 5.0 3.0 20.4 jam_Latn 6.6 4.4 67.8 por_Latn 81.6 79.8 76.6arb_Arab 7.0 7.8 14.6 jav_Latn 25.4 33.2 47.4 prk_Latn 3.6 2.2 49.8arn_Latn 4.8 4.0 28.4 jpn_Jpan 65.0 71.8 64.2 prs_Arab 79.4 78.6 88.8ary_Arab 2.8 4.0 15.2 kaa_Cyrl 17.6 24.8 73.8 pxm_Latn 3.2 3.2 24.0arz_Arab 5.4 4.8 24.8 kaa_Latn 9.2 9.8 43.4 qub_Latn 4.6 3.6 43.4asm_Beng 26.2 40.6 66.6 kab_Latn 3.4 2.4 20.6 quc_Latn 3.6 2.8 24.8ayr_Latn 4.8 4.8 52.8 kac_Latn 3.6 3.2 26.4 qug_Latn 4.8 3.6 50.8azb_Arab 7.4 6.8 72.4 kal_Latn 3.4 3.6 23.2 quh_Latn 4.6 4.4 56.2aze_Latn 71.0 78.6 73.0 kan_Knda 51.2 67.6 50.2 quw_Latn 6.2 4.6 49.2bak_Cyrl 5.4 6.4 65.2 kat_Geor 54.2 61.4 51.4 quy_Latn 4.6 4.6 61.4bam_Latn 3.4 3.6 60.2 kaz_Cyrl 61.4 73.0 56.8 quz_Latn 4.8 4.2 68.0ban_Latn 9.0 9.8 33.0 kbp_Latn 2.6 2.6 36.0 qvi_Latn 4.4 3.4 46.8bar_Latn 13.4 12.8 40.8 kek_Latn 5.0 3.4 26.4 rap_Latn 3.2 3.2 25.6bba_Latn 3.8 3.4 36.8 khm_Khmr 28.4 42.6 47.6 rar_Latn 3.2 3.0 26.6bbc_Latn 7.8 7.4 57.2 kia_Latn 4.0 5.6 33.2 rmy_Latn 6.8 5.8 34.6bci_Latn 4.4 3.6 13.2 kik_Latn 3.2 2.8 53.4 ron_Latn 72.2 69.6 66.6bcl_Latn 10.2 11.2 79.8 kin_Latn 5.0 5.0 59.4 rop_Latn 4.6 3.4 46.0bel_Cyrl 67.2 72.8 55.8 kir_Cyrl 54.8 70.2 66.6 rug_Latn 3.6 3.4 49.0bem_Latn 6.6 5.4 58.2 kjb_Latn 4.0 3.8 29.6 run_Latn 5.4 6.4 54.6ben_Beng 46.4 52.8 53.4 kjh_Cyrl 11.0 7.8 53.8 rus_Cyrl 75.8 74.6 71.2bhw_Latn 4.4 6.0 47.8 kmm_Latn 4.8 3.8 42.6 sag_Latn 6.0 4.4 52.4bim_Latn 4.2 2.8 52.2 kmr_Cyrl 4.0 4.2 42.4 sah_Cyrl 6.2 4.6 45.8bis_Latn 7.0 4.6 48.6 kmr_Latn 35.8 40.4 63.0 san_Deva 13.8 14.2 27.2bod_Tibt 2.0 1.8 33.2 knv_Latn 2.8 2.2 9.0 san_Latn 4.6 3.8 9.8bqc_Latn 3.4 3.0 39.2 kor_Hang 64.0 71.6 61.2 sba_Latn 2.8 2.8 37.6bre_Latn 17.6 23.4 32.8 kpg_Latn 5.2 3.8 51.8 seh_Latn 6.4 4.8 74.6bts_Latn 6.0 5.0 56.4 krc_Cyrl 9.2 10.2 63.0 sin_Sinh 44.8 56.6 45.0btx_Latn 11.0 9.0 59.6 kri_Latn 2.8 2.8 62.8 slk_Latn 75.2 72.8 63.6bul_Cyrl 81.2 78.0 76.4 ksd_Latn 7.0 5.4 42.6 slv_Latn 63.6 64.6 51.8bum_Latn 4.8 3.6 38.0 kss_Latn 2.2 2.4 6.0 sme_Latn 6.8 6.2 47.8bzj_Latn 7.8 4.0 75.0 ksw_Mymr 1.6 2.0 31.8 smo_Latn 4.4 3.4 36.0cab_Latn 5.8 4.6 17.4 kua_Latn 4.8 5.4 43.8 sna_Latn 7.0 3.6 43.0cac_Latn 3.6 3.0 14.8 lam_Latn 4.6 3.6 27.4 snd_Arab 52.2 64.6 66.6cak_Latn 3.4 3.4 21.4 lao_Laoo 31.4 52.8 49.6 som_Latn 22.2 29.0 33.0caq_Latn 3.2 4.4 30.2 lat_Latn 52.2 57.8 49.6 sop_Latn 5.2 4.2 31.2cat_Latn 86.6 81.0 76.4 lav_Latn 74.2 78.0 58.8 sot_Latn 6.0 4.8 52.2cbk_Latn 31.8 35.6 54.6 ldi_Latn 5.4 4.4 25.2 spa_Latn 81.2 78.8 80.0cce_Latn 5.2 4.6 51.8 leh_Latn 5.6 4.0 58.2 sqi_Latn 58.2 58.2 63.4ceb_Latn 14.2 12.6 68.0 lhu_Latn 2.0 2.0 5.0 srm_Latn 4.0 3.2 32.4ces_Latn 75.2 75.8 58.0 lin_Latn 6.6 5.4 65.4 srn_Latn 6.8 5.2 79.8cfm_Latn 4.6 4.0 46.8 lit_Latn 74.4 71.6 62.4 srp_Cyrl 83.0 87.0 81.2che_Cyrl 3.4 3.4 14.0 loz_Latn 6.8 4.6 49.2 srp_Latn 85.0 87.2 81.2chk_Latn 5.4 4.2 41.2 ltz_Latn 9.8 10.0 73.8 ssw_Latn 4.8 8.4 47.0chv_Cyrl 4.6 4.2 56.0 lug_Latn 4.6 4.0 49.4 sun_Latn 22.4 25.4 43.0ckb_Arab 4.0 4.8 47.2 luo_Latn 6.4 4.4 40.8 suz_Deva 3.6 3.4 34.2cmn_Hani 39.2 40.8 41.8 lus_Latn 3.8 3.8 54.4 swe_Latn 79.8 79.8 78.0cnh_Latn 4.8 4.2 55.6 lzh_Hani 25.0 31.4 63.4 swh_Latn 47.8 48.8 66.4crh_Cyrl 8.8 11.2 75.2 mad_Latn 7.6 4.4 44.4 sxn_Latn 4.8 4.8 25.8crs_Latn 7.4 5.2 80.6 mah_Latn 4.8 4.2 35.6 tam_Taml 42.8 56.8 52.0csy_Latn 3.8 5.0 50.0 mai_Deva 6.4 9.6 59.2 tat_Cyrl 8.2 6.2 67.2ctd_Latn 4.2 5.4 59.4 mal_Mlym 49.4 62.6 56.8 tbz_Latn 2.6 2.6 28.0ctu_Latn 2.8 2.8 21.6 mam_Latn 3.8 3.2 12.8 tca_Latn 2.4 3.2 15.4cuk_Latn 5.0 3.4 22.2 mar_Deva 66.2 69.0 74.8 tdt_Latn 6.2 5.0 62.2cym_Latn 38.8 46.0 42.4 mau_Latn 2.4 2.4 3.6 tel_Telu 44.4 57.2 42.6dan_Latn 71.6 73.2 63.2 mbb_Latn 3.0 3.4 33.6 teo_Latn 5.8 3.4 26.0deu_Latn 78.8 80.6 66.6 mck_Latn 5.2 3.6 57.4 tgk_Cyrl 4.6 4.2 71.2djk_Latn 4.6 4.0 40.4 mcn_Latn 6.0 4.2 39.2 tgl_Latn 61.0 60.6 78.6dln_Latn 5.2 4.8 66.4 mco_Latn 2.6 2.6 7.0 tha_Thai 30.0 37.0 45.4\nTable 15: Top10 accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Sentence Retrieval Bible (Part I).\n1105\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\ndtp_Latn 5.4 4.2 24.2 mdy_Ethi 2.8 2.4 31.6 tih_Latn 5.2 4.4 51.6dyu_Latn 4.2 2.4 50.2 meu_Latn 5.6 4.4 52.0 tir_Ethi 7.4 6.2 43.4dzo_Tibt 2.2 2.0 36.4 mfe_Latn 9.0 6.8 78.6 tlh_Latn 7.8 6.4 72.4efi_Latn 4.4 4.2 54.0 mgh_Latn 5.2 3.4 23.6 tob_Latn 2.2 3.0 16.8ell_Grek 52.6 53.8 48.6 mgr_Latn 4.0 4.4 57.6 toh_Latn 4.0 4.0 47.2enm_Latn 39.8 39.2 66.0 mhr_Cyrl 6.6 5.4 48.0 toi_Latn 4.2 4.4 47.4epo_Latn 64.6 59.8 56.2 min_Latn 9.4 6.2 29.0 toj_Latn 4.2 4.0 15.6est_Latn 72.0 75.6 56.4 miq_Latn 4.4 4.4 47.4 ton_Latn 4.2 3.8 22.4eus_Latn 26.2 28.4 23.0 mkd_Cyrl 76.6 72.6 74.8 top_Latn 3.4 3.6 8.0ewe_Latn 4.6 3.0 49.0 mlg_Latn 29.0 28.4 66.0 tpi_Latn 5.8 4.4 58.0fao_Latn 24.0 28.4 73.4 mlt_Latn 5.8 5.2 50.4 tpm_Latn 3.6 3.0 39.6fas_Arab 78.2 80.4 89.2 mos_Latn 4.2 3.6 42.8 tsn_Latn 5.4 3.6 41.8fij_Latn 3.8 3.0 36.4 mps_Latn 3.2 3.2 21.6 tso_Latn 5.6 5.0 50.8fil_Latn 60.4 64.4 72.0 mri_Latn 4.2 3.8 48.4 tsz_Latn 5.6 3.2 27.0fin_Latn 75.6 75.0 53.8 mrw_Latn 6.0 4.4 52.2 tuc_Latn 2.6 2.6 31.4fon_Latn 2.6 2.0 33.4 msa_Latn 40.0 40.2 40.6 tui_Latn 3.6 3.2 38.0fra_Latn 88.6 86.8 79.2 mwm_Latn 2.6 2.6 35.8 tuk_Cyrl 13.6 15.8 65.0fry_Latn 27.8 27.4 44.0 mxv_Latn 3.0 3.4 8.8 tuk_Latn 9.6 9.6 66.2gaa_Latn 3.8 3.4 47.0 mya_Mymr 20.2 27.8 29.4 tum_Latn 5.2 4.6 66.2gil_Latn 5.6 3.6 36.8 myv_Cyrl 4.6 4.0 35.0 tur_Latn 74.4 74.8 63.2giz_Latn 6.2 4.0 41.0 mzh_Latn 4.6 3.2 36.2 twi_Latn 3.8 3.0 50.0gkn_Latn 4.0 3.4 32.2 nan_Latn 3.2 3.2 13.6 tyv_Cyrl 6.8 7.0 46.6gkp_Latn 3.0 3.2 20.4 naq_Latn 3.0 2.2 25.0 tzh_Latn 6.0 5.2 25.8gla_Latn 25.2 26.6 43.0 nav_Latn 2.4 2.8 11.2 tzo_Latn 3.8 3.8 16.6gle_Latn 35.0 38.6 40.0 nbl_Latn 9.2 11.8 53.8 udm_Cyrl 6.0 5.0 55.2glv_Latn 5.8 3.6 47.4 nch_Latn 4.4 3.0 21.4 uig_Arab 45.8 63.6 56.2gom_Latn 6.0 4.6 42.8 ncj_Latn 4.6 3.0 25.2 uig_Latn 9.8 11.0 62.8gor_Latn 3.8 3.0 26.0 ndc_Latn 5.2 4.6 40.0 ukr_Cyrl 66.0 63.4 57.0grc_Grek 17.4 23.8 54.8 nde_Latn 13.0 15.2 53.8 urd_Arab 47.6 47.0 65.0guc_Latn 3.4 2.6 13.0 ndo_Latn 5.2 4.0 48.2 uzb_Cyrl 6.2 7.4 78.8gug_Latn 4.6 3.2 36.0 nds_Latn 9.6 8.4 43.0 uzb_Latn 54.8 60.8 67.6guj_Gujr 53.8 71.2 71.4 nep_Deva 35.6 50.6 58.6 uzn_Cyrl 5.4 5.4 87.0gur_Latn 3.8 2.8 27.0 ngu_Latn 4.6 3.4 27.6 ven_Latn 4.8 4.2 47.2guw_Latn 4.0 3.4 59.4 nia_Latn 4.6 3.2 29.4 vie_Latn 72.8 71.0 57.8gya_Latn 3.6 3.0 41.0 nld_Latn 78.0 75.8 71.8 wal_Latn 4.2 5.4 51.4gym_Latn 3.6 3.8 18.0 nmf_Latn 4.6 4.6 36.6 war_Latn 9.8 6.6 43.4hat_Latn 6.0 4.2 68.2 nnb_Latn 3.6 3.2 42.0 wbm_Latn 3.8 2.4 46.4hau_Latn 28.8 36.0 54.8 nno_Latn 58.4 67.2 72.6 wol_Latn 4.6 4.4 35.8haw_Latn 4.2 3.4 38.8 nob_Latn 82.8 85.2 79.2 xav_Latn 2.2 2.4 5.0heb_Hebr 25.0 26.0 21.8 nor_Latn 81.2 84.2 86.2 xho_Latn 10.4 16.2 40.8hif_Latn 12.2 16.4 39.0 npi_Deva 50.6 70.8 76.6 yan_Latn 4.2 3.4 31.8hil_Latn 11.0 10.8 76.2 nse_Latn 5.2 5.0 54.8 yao_Latn 4.4 3.8 55.2hin_Deva 67.0 72.8 76.6 nso_Latn 6.0 4.2 57.0 yap_Latn 4.0 4.0 24.0hin_Latn 13.6 16.0 43.2 nya_Latn 4.0 4.6 60.2 yom_Latn 4.8 3.6 42.2hmo_Latn 6.4 4.4 48.2 nyn_Latn 4.4 4.2 51.8 yor_Latn 3.4 3.6 37.4hne_Deva 13.4 14.8 75.0 nyy_Latn 3.0 3.0 25.6 yua_Latn 3.8 3.4 18.2hnj_Latn 2.8 2.8 54.2 nzi_Latn 3.2 3.0 47.2 yue_Hani 17.2 14.0 24.0hra_Latn 5.2 4.6 52.2 ori_Orya 42.6 62.0 57.0 zai_Latn 6.2 4.2 38.0hrv_Latn 79.8 81.8 72.6 ory_Orya 31.4 47.0 55.2 zho_Hani 40.4 40.2 44.4hui_Latn 3.8 3.0 28.0 oss_Cyrl 4.2 3.6 54.8 zlm_Latn 83.4 78.4 87.0hun_Latn 76.4 78.2 56.2 ote_Latn 3.6 2.4 18.0 zom_Latn 3.6 3.4 50.2hus_Latn 3.6 3.2 17.6 pag_Latn 8.0 5.0 61.2 zsm_Latn 90.2 91.0 83.0hye_Armn 30.8 33.0 75.2 pam_Latn 8.2 7.0 49.8 zul_Latn 11.0 16.0 49.0\nTable 16: Top10 accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Sentence Retrieval Bible (Part II).\n1106\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nace_Latn 33.4 38.9 44.2 heb_Hebr 51.5 56.5 49.0 ori_Orya 31.4 27.6 31.0afr_Latn 75.6 78.3 76.7 hin_Deva 67.0 71.1 69.4 oss_Cyrl 33.7 39.2 52.1als_Latn 60.7 61.4 80.0 hrv_Latn 77.2 78.9 77.3 pan_Guru 50.0 50.5 48.1amh_Ethi 42.2 40.9 45.4 hsb_Latn 64.0 69.0 71.2 pms_Latn 71.2 74.9 75.9ara_Arab 44.7 48.7 56.1 hun_Latn 76.2 79.8 75.9 pnb_Arab 57.0 64.6 65.8arg_Latn 73.6 74.6 77.2 hye_Armn 50.8 61.7 54.8 pol_Latn 77.5 81.2 78.1arz_Arab 48.3 52.5 57.4 ibo_Latn 40.8 42.8 58.6 por_Latn 77.8 81.2 78.6asm_Beng 53.2 64.4 64.2 ido_Latn 61.6 78.6 77.8 pus_Arab 37.4 39.9 41.4ast_Latn 78.1 82.8 84.5 ilo_Latn 55.3 65.3 77.1 que_Latn 59.1 55.2 66.8aym_Latn 40.8 38.7 47.1 ina_Latn 54.7 63.4 58.0 roh_Latn 52.6 55.7 60.3aze_Latn 62.4 69.2 66.1 ind_Latn 49.0 54.1 56.6 ron_Latn 74.8 79.9 74.2bak_Cyrl 35.1 49.3 59.4 isl_Latn 69.1 77.2 72.1 rus_Cyrl 63.8 70.0 67.6bar_Latn 55.2 58.6 68.4 ita_Latn 77.3 81.2 78.7 sah_Cyrl 47.3 49.7 74.2bel_Cyrl 74.2 78.7 74.3 jav_Latn 58.4 61.2 55.8 san_Deva 36.9 37.3 35.8ben_Beng 65.3 75.8 71.6 jbo_Latn 18.0 26.3 27.8 scn_Latn 49.9 54.8 65.8bih_Deva 50.7 57.1 58.7 jpn_Jpan 19.7 20.6 17.2 sco_Latn 80.9 81.8 85.6bod_Tibt 2.5 3.0 31.6 kan_Knda 56.9 60.8 58.4 sgs_Latn 42.5 47.4 62.7bos_Latn 74.0 74.3 74.2 kat_Geor 65.5 69.5 68.3 sin_Sinh 52.2 57.0 57.8bre_Latn 59.1 63.9 63.3 kaz_Cyrl 43.7 52.7 50.0 slk_Latn 75.0 81.7 78.5bul_Cyrl 76.8 81.6 77.2 khm_Khmr 43.3 46.2 40.6 slv_Latn 79.4 82.2 80.1cat_Latn 82.2 85.4 83.7 kin_Latn 60.5 58.4 67.1 snd_Arab 41.2 46.6 41.8cbk_Latn 54.6 54.0 54.1 kir_Cyrl 44.2 46.9 46.7 som_Latn 55.8 55.5 58.2ceb_Latn 55.1 57.8 53.8 kor_Hang 49.1 58.5 50.9 spa_Latn 72.8 73.3 72.8ces_Latn 77.6 80.8 78.3 ksh_Latn 41.3 48.3 58.7 sqi_Latn 74.0 74.4 76.6che_Cyrl 15.4 24.6 60.9 kur_Latn 58.8 65.0 69.6 srp_Cyrl 59.7 71.4 66.4chv_Cyrl 52.9 51.6 75.9 lat_Latn 70.7 79.2 73.8 sun_Latn 42.0 49.7 57.7ckb_Arab 33.1 42.6 75.5 lav_Latn 73.4 77.1 74.0 swa_Latn 65.6 69.0 69.6cos_Latn 54.3 56.4 56.0 lƒ≥_Latn 36.9 41.6 46.6 swe_Latn 71.8 75.9 69.7crh_Latn 44.3 52.4 54.7 lim_Latn 59.9 64.7 71.8 szl_Latn 58.2 56.7 67.6csb_Latn 55.1 54.2 61.2 lin_Latn 37.4 41.3 54.0 tam_Taml 55.0 57.9 55.2cym_Latn 57.9 60.1 59.7 lit_Latn 73.4 77.0 73.5 tat_Cyrl 40.7 47.7 68.0dan_Latn 81.5 84.2 81.7 lmo_Latn 68.8 68.4 71.3 tel_Telu 47.4 52.5 46.0deu_Latn 74.3 78.6 75.7 ltz_Latn 47.4 55.8 69.1 tgk_Cyrl 24.7 38.3 68.5diq_Latn 37.8 43.3 53.1 lzh_Hani 15.6 21.6 11.8 tgl_Latn 71.0 74.7 75.1div_Thaa 0.0 0.0 51.1 mal_Mlym 61.0 63.3 61.3 tha_Thai 4.2 1.6 3.2ell_Grek 73.7 78.6 72.8 mar_Deva 60.2 63.4 60.7 tuk_Latn 45.6 50.7 59.7eml_Latn 32.9 36.1 40.8 mhr_Cyrl 44.3 48.3 63.1 tur_Latn 74.9 79.3 76.1eng_Latn 82.7 84.5 83.3 min_Latn 42.9 46.2 41.8 uig_Arab 44.0 50.9 48.0epo_Latn 63.8 71.8 68.0 mkd_Cyrl 74.5 80.4 73.3 ukr_Cyrl 75.2 76.3 74.2est_Latn 72.2 78.5 73.5 mlg_Latn 54.9 54.3 57.9 urd_Arab 51.2 57.8 74.5eus_Latn 59.0 62.0 58.0 mlt_Latn 43.2 48.3 73.3 uzb_Latn 70.6 76.2 75.1ext_Latn 36.9 47.1 46.1 mon_Cyrl 72.4 74.3 66.9 vec_Latn 59.0 63.3 66.4fao_Latn 61.1 70.8 72.4 mri_Latn 14.2 18.3 53.5 vep_Latn 59.8 59.3 71.3fas_Arab 44.6 58.0 51.2 msa_Latn 62.3 70.4 65.8 vie_Latn 68.5 77.8 71.3fin_Latn 75.5 79.1 75.2 mwl_Latn 42.6 47.5 45.3 vls_Latn 68.1 73.6 73.7fra_Latn 77.2 79.8 76.0 mya_Mymr 51.3 53.4 55.5 vol_Latn 59.2 55.6 59.2frr_Latn 45.4 46.8 54.8 mzn_Arab 36.4 43.1 44.9 war_Latn 61.9 61.4 66.1fry_Latn 74.3 79.0 77.5 nan_Latn 46.2 51.4 82.1 wuu_Hani 29.4 54.0 25.1fur_Latn 44.9 50.1 56.4 nap_Latn 53.0 53.9 55.7 xmf_Geor 40.2 40.0 62.6gla_Latn 55.5 61.4 63.5 nds_Latn 62.4 66.7 77.1 yid_Hebr 47.6 52.5 50.3gle_Latn 70.8 74.6 72.2 nep_Deva 63.2 66.4 62.7 yor_Latn 42.2 40.1 63.1glg_Latn 80.2 81.1 79.4 nld_Latn 80.1 83.6 80.8 yue_Hani 24.8 30.3 22.6grn_Latn 40.0 42.3 54.7 nno_Latn 76.6 80.4 78.0 zea_Latn 65.2 67.4 68.6guj_Gujr 61.0 61.9 59.8 nor_Latn 76.5 80.1 76.7 zho_Hani 24.2 28.8 23.4hbs_Latn 61.1 57.2 61.5 oci_Latn 65.3 67.8 70.1\nTable 17: F1 of XLM-R-B, XLM-R-L, and Glot500-m on NER.\n1107\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nafr_Latn 88.7 89.3 87.5 hbo_Hebr 38.9 45.7 54.2 pol_Latn 84.7 85.4 82.4ajp_Arab 62.9 67.3 69.7 heb_Hebr 68.0 69.2 67.2 por_Latn 88.6 89.8 88.2aln_Latn 53.5 60.4 52.3 hin_Deva 71.3 75.3 70.3 quc_Latn 28.9 29.3 62.4amh_Ethi 64.5 66.2 66.1 hrv_Latn 85.9 86.2 85.5 ron_Latn 83.9 85.7 80.6ara_Arab 68.5 69.7 65.4 hsb_Latn 71.5 74.4 83.6 rus_Cyrl 89.1 89.7 88.7bam_Latn 25.4 23.5 40.8 hun_Latn 82.6 82.7 81.2 sah_Cyrl 20.3 22.8 76.8bel_Cyrl 86.2 86.2 86.0 hye_Armn 85.2 86.5 84.0 san_Deva 18.3 28.6 26.1ben_Beng 82.8 83.8 83.8 hyw_Armn 78.5 82.5 80.4 sin_Sinh 57.7 60.1 54.7bre_Latn 61.6 66.6 60.7 ind_Latn 83.5 84.1 82.7 slk_Latn 85.6 85.8 84.4bul_Cyrl 89.1 88.9 88.1 isl_Latn 84.2 85.1 82.8 slv_Latn 78.5 79.1 75.9cat_Latn 86.7 87.9 86.3 ita_Latn 88.3 89.6 87.3 sme_Latn 29.8 31.5 73.7ceb_Latn 49.3 49.5 66.4 jav_Latn 73.2 76.7 74.1 spa_Latn 88.5 89.0 88.0ces_Latn 85.0 85.4 84.4 jpn_Jpan 17.3 32.2 31.7 sqi_Latn 81.4 82.9 77.9cym_Latn 65.5 67.0 64.4 kaz_Cyrl 77.3 79.1 75.9 srp_Latn 86.1 86.6 85.3dan_Latn 90.7 91.0 90.2 kmr_Latn 73.1 78.2 75.5 swe_Latn 93.5 93.7 92.1deu_Latn 88.4 88.4 87.9 kor_Hang 53.7 53.4 53.1 tam_Taml 76.1 76.9 75.0ell_Grek 87.3 87.0 85.4 lat_Latn 75.0 80.3 72.4 tat_Cyrl 45.0 48.8 70.1eng_Latn 96.3 96.5 96.0 lav_Latn 86.0 86.3 83.5 tel_Telu 85.0 85.0 82.2est_Latn 86.1 86.4 83.1 lƒ≥_Latn 48.1 48.6 76.8 tgl_Latn 72.7 74.8 74.7eus_Latn 71.3 73.7 61.8 lit_Latn 84.1 84.6 81.1 tha_Thai 46.0 54.7 56.7fao_Latn 77.0 80.6 89.2 lzh_Hani 14.1 23.1 23.0 tur_Latn 72.9 74.0 70.7fas_Arab 71.8 74.2 71.5 mal_Mlym86.9 86.7 84.4 uig_Arab 68.2 70.2 68.9fin_Latn 85.2 85.7 80.8 mar_Deva 83.0 85.2 80.8 ukr_Cyrl 85.9 86.3 84.8fra_Latn 86.7 87.3 85.4 mlt_Latn 21.0 21.9 79.5 urd_Arab 61.0 68.2 62.0gla_Latn 57.4 61.8 60.2 myv_Cyrl 39.7 38.6 65.7 vie_Latn 70.9 72.2 67.1gle_Latn 65.5 68.7 64.4 nap_Latn 52.8 17.0 63.6 wol_Latn 25.6 25.5 61.6glg_Latn 83.7 86.4 82.6 nds_Latn 58.0 67.3 77.2 xav_Latn 8.4 5.3 14.0glv_Latn 27.5 29.5 52.7 nld_Latn 88.5 88.8 88.2 yor_Latn 21.7 21.4 63.9grc_Grek 62.0 68.1 73.1 nor_Latn 88.1 88.9 88.0 yue_Hani 31.5 42.0 40.9grn_Latn 8.9 7.8 19.8 pcm_Latn 47.3 50.1 57.1 zho_Hani 28.6 42.4 43.1gsw_Latn 48.7 55.9 80.3\nTable 18: F1 of XLM-R-B, XLM-R-L, and Glot500-m on POS.\n1108\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nace_Latn 15 25 60 iba_Latn 30 35 56 ote_Latn 6 5 36ace_Latn 15 25 60 iba_Latn 30 35 56 ote_Latn 6 5 36ach_Latn 9 8 34 ibo_Latn 8 6 51 pag_Latn 22 21 52acr_Latn 10 8 46 ifa_Latn 12 12 47 pam_Latn 20 18 41afr_Latn 54 64 57 ifb_Latn 14 11 48 pan_Guru 53 65 59agw_Latn 11 13 54 ikk_Latn 11 7 47 pap_Latn 31 36 55ahk_Latn 5 5 24 ilo_Latn 15 13 52 pau_Latn 12 10 41aka_Latn 11 7 48 ind_Latn 62 66 63 pcm_Latn 25 28 46aln_Latn 44 51 49 isl_Latn 50 60 49 pdt_Latn 17 20 53als_Latn 45 51 50 ita_Latn 57 68 61 pes_Arab 60 70 64alt_Cyrl 25 23 54 ium_Latn 6 7 53 pis_Latn 13 13 57alz_Latn 13 11 34 ixl_Latn 10 7 33 pls_Latn 6 7 41amh_Ethi 42 49 43 izz_Latn 9 6 41 plt_Latn 30 51 50aoj_Latn 12 9 41 jam_Latn 15 14 55 poh_Latn 16 8 48arb_Arab 27 55 45 jav_Latn 44 54 49 pol_Latn 53 63 47arn_Latn 9 8 46 jpn_Jpan 56 66 56 pon_Latn 10 8 50ary_Arab 16 27 40 kaa_Cyrl 35 49 59 por_Latn 61 67 57arz_Arab 28 49 39 kab_Latn 8 7 30 prk_Latn 6 6 51asm_Beng 44 53 53 kac_Latn 7 8 44 prs_Arab 62 67 65ayr_Latn 11 9 53 kal_Latn 9 7 33 pxm_Latn 9 9 43azb_Arab 19 17 55 kan_Knda 53 63 59 qub_Latn 13 10 55aze_Latn 56 64 61 kat_Geor 55 60 57 quc_Latn 9 7 45bak_Cyrl 17 19 57 kaz_Cyrl 53 64 56 qug_Latn 13 8 59bam_Latn 7 7 46 kbp_Latn 5 5 35 quh_Latn 11 10 56ban_Latn 21 24 46 kek_Latn 6 9 45 quw_Latn 13 10 48bar_Latn 31 42 45 khm_Khmr 51 64 59 quy_Latn 12 11 57bba_Latn 6 6 42 kia_Latn 7 7 39 quz_Latn 11 8 56bci_Latn 9 8 28 kik_Latn 7 6 40 qvi_Latn 9 8 59bcl_Latn 28 27 51 kin_Latn 17 9 50 rap_Latn 8 7 50bel_Cyrl 56 67 54 kir_Cyrl 55 63 60 rar_Latn 8 9 48bem_Latn 13 14 43 kjb_Latn 7 9 48 rmy_Latn 16 12 47ben_Beng 53 65 60 kjh_Cyrl 15 19 50 ron_Latn 60 70 60bhw_Latn 11 11 47 kmm_Latn 8 6 46 rop_Latn 10 10 50bim_Latn 7 7 47 kmr_Cyrl 8 8 44 rug_Latn 7 7 55bis_Latn 13 12 57 knv_Latn 7 6 44 run_Latn 16 9 49bqc_Latn 7 7 36 kor_Hang 59 70 60 rus_Cyrl 60 66 61bre_Latn 30 49 36 kpg_Latn 9 10 57 sag_Latn 9 11 42bts_Latn 18 17 56 krc_Cyrl 25 22 56 sah_Cyrl 10 9 52btx_Latn 23 26 53 kri_Latn 7 9 52 sba_Latn 7 6 41bul_Cyrl 61 70 57 ksd_Latn 10 11 53 seh_Latn 11 8 47bum_Latn 9 9 43 kss_Latn 5 5 23 sin_Sinh 54 66 59bzj_Latn 18 14 56 ksw_Mymr 5 5 53 slk_Latn 56 63 56cab_Latn 9 8 41 kua_Latn 12 12 45 slv_Latn 59 66 61cac_Latn 10 10 47 lam_Latn 5 8 28 sme_Latn 10 12 43cak_Latn 7 8 53 lao_Laoo 56 66 64 smo_Latn 8 7 51caq_Latn 7 7 47 lat_Latn 56 64 50 sna_Latn 13 11 42cat_Latn 53 64 48 lav_Latn 54 66 55 snd_Arab 54 64 57cbk_Latn 43 47 57 ldi_Latn 8 9 28 som_Latn 32 45 33cce_Latn 13 9 47 leh_Latn 13 10 44 sop_Latn 12 8 32ceb_Latn 28 30 49 lhu_Latn 6 6 30 sot_Latn 11 8 45ces_Latn 50 65 53 lin_Latn 10 7 49 spa_Latn 61 69 60cfm_Latn 8 8 55 lit_Latn 54 66 53 sqi_Latn 57 68 60che_Cyrl 11 6 20 loz_Latn 10 10 48 srm_Latn 10 9 53chv_Cyrl 8 7 52 ltz_Latn 22 30 52 srn_Latn 10 9 53cmn_Hani 53 62 56 lug_Latn 16 9 45 srp_Latn 55 67 56cnh_Latn 7 8 56 luo_Latn 12 10 39 ssw_Latn 14 17 40crh_Cyrl 22 31 57 lus_Latn 11 7 52 sun_Latn 40 47 47crs_Latn 14 17 61 lzh_Hani 46 55 55 suz_Deva 15 13 53csy_Latn 9 7 52 mad_Latn 23 28 56 swe_Latn 60 66 56ctd_Latn 9 8 56 mah_Latn 6 6 42 swh_Latn 47 59 56ctu_Latn 15 14 51 mai_Deva 34 39 59 sxn_Latn 11 8 46cuk_Latn 15 7 44 mal_Mlym 56 64 60 tam_Taml 56 61 60cym_Latn 46 51 48 mam_Latn 10 6 31 tat_Cyrl 21 28 64dan_Latn 51 62 50 mar_Deva 55 63 60 tbz_Latn 6 6 43deu_Latn 56 65 53 mau_Latn 5 5 6 tca_Latn 5 5 47djk_Latn 12 10 46 mbb_Latn 11 7 48 tdt_Latn 16 13 56dln_Latn 10 5 52 mck_Latn 15 10 41 tel_Telu 55 65 60dtp_Latn 9 8 39 mcn_Latn 13 9 43 teo_Latn 12 8 26dyu_Latn 6 8 52 mco_Latn 6 7 28 tgk_Cyrl 10 7 55dzo_Tibt 6 5 55 mdy_Ethi 6 7 47 tgl_Latn 48 60 56\nTable 19: F1 of XLM-R-B, XLM-R-L, and Glot500-m on Text Classification (Part I).\n1109\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nefi_Latn 10 9 50 meu_Latn 15 11 52 tha_Thai 56 67 61ell_Grek 37 47 54 mfe_Latn 16 14 61 tih_Latn 11 11 56eng_Latn 74 75 68 mgh_Latn 10 6 35 tir_Ethi 23 27 48enm_Latn 46 56 65 mgr_Latn 14 12 46 tlh_Latn 30 26 59epo_Latn 53 63 53 mhr_Cyrl 14 10 43 tob_Latn 6 9 52est_Latn 62 68 53 min_Latn 27 37 50 toh_Latn 11 8 41eus_Latn 28 33 22 miq_Latn 7 7 48 toi_Latn 14 10 40ewe_Latn 9 9 52 mkd_Cyrl 65 69 61 toj_Latn 12 11 42fao_Latn 33 41 55 mlg_Latn 32 51 48 ton_Latn 6 7 47fas_Arab 62 68 62 mlt_Latn 12 11 49 top_Latn 11 10 25fij_Latn 8 7 51 mos_Latn 7 8 41 tpi_Latn 11 13 55fil_Latn 47 56 53 mps_Latn 11 12 54 tpm_Latn 9 8 47fin_Latn 57 66 56 mri_Latn 9 8 47 tsn_Latn 11 8 45fon_Latn 5 6 49 mrw_Latn 15 18 41 tsz_Latn 10 10 45fra_Latn 57 66 57 msa_Latn 43 49 46 tuc_Latn 7 9 50fry_Latn 31 34 37 mwm_Latn 5 6 50 tui_Latn 8 8 49gaa_Latn 5 6 43 mxv_Latn 8 8 24 tuk_Latn 23 26 53gil_Latn 9 8 44 mya_Mymr 45 52 54 tum_Latn 12 12 49giz_Latn 9 10 49 myv_Cyrl 11 7 47 tur_Latn 55 66 56gkn_Latn 8 7 40 mzh_Latn 7 9 45 twi_Latn 9 6 46gkp_Latn 5 6 35 nan_Latn 6 6 30 tyv_Cyrl 19 18 54gla_Latn 28 43 42 naq_Latn 8 7 42 tzh_Latn 12 13 42gle_Latn 37 53 40 nav_Latn 7 9 25 tzo_Latn 13 11 41glv_Latn 10 12 38 nbl_Latn 20 26 46 udm_Cyrl 10 11 51gom_Latn 10 13 39 nch_Latn 10 8 39 ukr_Cyrl 61 67 56gor_Latn 17 15 50 ncj_Latn 7 9 43 urd_Arab 59 65 59guc_Latn 8 6 42 ndc_Latn 13 13 40 uzb_Latn 49 59 56gug_Latn 11 7 44 nde_Latn 20 26 46 uzn_Cyrl 13 17 57guj_Gujr 57 67 63 ndo_Latn 13 9 40 ven_Latn 10 8 43gur_Latn 6 6 47 nds_Latn 16 15 42 vie_Latn 57 65 55guw_Latn 11 9 49 nep_Deva 56 61 61 wal_Latn 15 9 41gya_Latn 5 5 39 ngu_Latn 8 10 50 war_Latn 19 21 41gym_Latn 10 7 47 nia_Latn 11 9 47 wbm_Latn 7 6 52hat_Latn 11 10 59 nld_Latn 50 59 55 wol_Latn 11 9 40hau_Latn 34 40 47 nmf_Latn 9 7 36 xav_Latn 10 10 40haw_Latn 8 7 41 nnb_Latn 11 8 46 xho_Latn 23 32 48heb_Hebr 16 31 41 nno_Latn 49 56 57 yan_Latn 7 7 46hif_Latn 22 37 42 nob_Latn 54 60 55 yao_Latn 10 8 43hil_Latn 26 31 60 nor_Latn 53 63 55 yap_Latn 8 8 46hin_Deva 54 70 57 npi_Deva 53 62 61 yom_Latn 13 9 35hmo_Latn 14 13 53 nse_Latn 17 10 45 yor_Latn 11 7 51hne_Deva 32 40 59 nso_Latn 11 7 48 yua_Latn 12 10 39hnj_Latn 8 7 55 nya_Latn 12 10 56 yue_Hani 52 61 54hra_Latn 10 7 49 nyn_Latn 16 7 38 zai_Latn 16 14 40hrv_Latn 56 63 56 nyy_Latn 8 8 34 zho_Hani 55 68 55hui_Latn 9 7 43 nzi_Latn 5 7 40 zlm_Latn 59 70 64hun_Latn 62 69 53 ori_Orya 54 65 60 zom_Latn 11 9 50hus_Latn 7 10 39 ory_Orya 55 64 61 zsm_Latn 61 64 63hye_Armn 60 68 60 oss_Cyrl 6 6 47 zul_Latn 24 35 52\nTable 20: F1 of XLM-R-B, XLM-R-L, and Glot500-m on Text Classification (Part II).\n1110\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nace_Latn 2.50 2.83 4.56 hye_Armn 2.32 3.25 4.91 pam_Latn 2.85 3.52 4.46ach_Latn 3.13 4.02 5.60 hye_Latn 2.34 2.98 2.44 pan_Guru 2.11 2.73 4.11acr_Latn 2.01 2.46 2.51 iba_Latn 2.77 3.85 6.01 pap_Latn 3.12 3.85 5.46afr_Latn 3.17 3.66 5.46 ibo_Latn 2.05 2.43 4.33 pau_Latn 2.67 3.09 4.09agw_Latn 2.51 2.80 4.09 ifa_Latn 1.81 2.40 3.45 pcm_Latn 3.81 4.44 6.47ahk_Latn 1.11 1.23 1.22 ifb_Latn 2.22 2.58 3.28 pdt_Latn 2.41 3.33 5.11aka_Latn 3.38 4.50 6.48 ikk_Latn 1.75 2.29 3.83 pes_Arab 2.66 3.91 4.81aln_Latn 4.06 4.92 7.39 ilo_Latn 3.06 3.87 6.24 pis_Latn 1.91 2.32 4.42als_Latn 3.92 4.85 6.32 ind_Latn 4.06 5.00 7.60 pls_Latn 2.14 2.57 4.02alt_Cyrl 2.91 3.36 5.32 isl_Latn 4.40 5.22 7.07 plt_Latn 3.74 3.99 6.82alz_Latn 3.78 4.89 5.94 ita_Latn 3.55 4.02 6.18 poh_Latn 0.92 1.10 1.87amh_Ethi 3.04 3.10 4.87 ium_Latn 2.00 2.27 3.46 pol_Latn 3.94 5.20 5.12amh_Latn 1.41 1.76 1.70 ixl_Latn 1.62 1.94 2.14 pon_Latn 3.53 4.51 5.18aoj_Latn 1.77 1.97 3.22 izz_Latn 1.65 2.06 3.12 por_Latn 3.61 4.35 6.12arb_Arab 1.07 1.47 2.40 jam_Latn 2.77 3.06 3.59 prk_Latn 2.10 2.70 5.40arn_Latn 2.40 2.79 4.51 jav_Latn 3.10 3.67 5.21 prs_Arab 3.54 4.28 6.92ary_Arab 0.86 1.10 2.43 jpn_Jpan 3.62 4.39 4.07 pxm_Latn 1.76 2.15 3.40arz_Arab 0.83 1.14 2.52 kaa_Cyrl 2.99 3.91 5.45 qub_Latn 2.48 2.97 4.24asm_Beng 2.82 2.47 5.21 kaa_Latn 2.34 2.96 3.64 quc_Latn 1.87 2.45 2.77ayr_Latn 2.61 3.09 3.93 kab_Latn 2.51 3.08 3.14 qug_Latn 2.44 2.99 5.34azb_Arab 2.57 3.16 4.96 kac_Latn 1.66 2.17 3.34 quh_Latn 2.91 3.46 5.43aze_Cyrl 2.76 3.26 3.62 kal_Latn 3.00 3.90 4.73 quw_Latn 2.89 3.50 5.62aze_Latn 4.24 5.04 8.00 kan_Knda 2.58 3.18 4.05 quy_Latn 2.69 3.15 5.51bak_Cyrl 2.20 2.38 4.35 kan_Latn 1.62 2.08 1.81 quz_Latn 3.33 3.89 6.07bam_Latn 3.56 4.29 5.73 kat_Geor 4.06 4.99 5.53 qvi_Latn 2.82 3.42 4.89ban_Latn 2.26 2.74 3.37 kaz_Cyrl 3.82 4.56 5.31 rap_Latn 1.31 1.61 2.31bar_Latn 3.11 3.81 3.84 kbp_Latn 1.47 1.65 3.32 rar_Latn 1.83 2.22 3.27bba_Latn 2.43 2.80 4.16 kek_Latn 1.91 2.45 2.70 rmy_Latn 2.85 3.68 4.83bbc_Latn 3.02 3.85 5.22 khm_Khmr 1.57 1.70 2.82 ron_Latn 3.33 4.00 4.99bci_Latn 2.81 3.18 3.30 kia_Latn 2.92 3.27 4.69 rop_Latn 1.60 2.08 3.46bcl_Latn 3.78 4.61 8.06 kik_Latn 2.28 2.73 4.38 rug_Latn 2.56 2.95 3.60bel_Cyrl 3.73 4.91 6.46 kin_Latn 2.67 3.26 4.19 run_Latn 3.33 3.98 6.82bem_Latn 3.06 3.77 5.69 kir_Cyrl 4.54 4.35 6.36 rus_Cyrl 4.20 5.05 7.38ben_Beng 3.29 3.07 4.99 kjb_Latn 2.42 3.03 3.27 sag_Latn 2.92 3.52 5.17bhw_Latn 2.91 3.47 5.16 kjh_Cyrl 3.13 3.81 5.39 sah_Cyrl 2.31 3.01 4.98bim_Latn 2.54 3.29 4.12 kmm_Latn 2.52 3.30 3.73 san_Deva 2.48 2.20 3.64bis_Latn 2.59 2.96 4.68 kmr_Cyrl 2.31 2.76 4.30 san_Latn 1.54 2.23 2.35bod_Tibt 0.54 3.39 2.43 kmr_Latn 3.75 4.19 5.70 sba_Latn 1.88 2.24 3.86bqc_Latn 2.44 3.16 4.61 knv_Latn 1.27 1.53 2.09 seh_Latn 3.44 4.20 4.94bre_Latn 3.32 3.87 3.79 kor_Hang 2.76 3.99 4.89 sin_Sinh 2.55 3.60 3.44bts_Latn 4.06 4.92 7.99 kor_Latn 0.92 2.40 0.90 slk_Latn 4.65 5.06 6.43btx_Latn 3.23 3.88 5.59 kpg_Latn 2.80 3.12 5.77 slv_Latn 3.11 4.32 5.23bul_Cyrl 3.56 4.67 5.88 krc_Cyrl 2.85 3.66 4.90 sme_Latn 2.70 3.35 4.40bum_Latn 3.22 3.73 4.89 kri_Latn 1.90 2.52 5.07 smo_Latn 2.26 2.72 4.34bzj_Latn 1.65 2.43 4.48 ksd_Latn 2.82 3.28 5.42 sna_Latn 2.89 3.39 5.32cab_Latn 2.16 2.63 2.98 kss_Latn 0.99 1.09 1.49 snd_Arab 3.12 3.92 5.30cac_Latn 1.51 1.74 2.86 ksw_Mymr 0.95 1.46 4.18 som_Latn 3.15 3.40 4.17cak_Latn 1.86 2.18 3.24 kua_Latn 4.25 4.92 7.31 sop_Latn 2.80 3.55 4.23caq_Latn 2.20 2.94 3.66 lam_Latn 2.41 3.09 4.03 sot_Latn 3.49 4.31 6.96cat_Latn 3.76 4.04 5.24 lao_Laoo 2.61 3.21 4.39 spa_Latn 3.71 4.21 5.86cbk_Latn 3.12 3.64 4.34 lat_Latn 4.65 5.51 7.44 sqi_Latn 4.07 5.07 6.50cce_Latn 2.96 3.40 4.86 lav_Latn 3.35 4.56 6.45 srm_Latn 1.75 1.96 3.23ceb_Latn 3.45 4.13 5.10 ldi_Latn 3.41 3.94 4.29 srn_Latn 3.40 3.86 5.98ces_Latn 4.33 5.27 7.75 leh_Latn 2.73 3.66 5.28 srp_Cyrl 6.48 6.50 10.24cfm_Latn 2.69 3.18 4.52 lhu_Latn 1.43 1.61 1.36 srp_Latn 4.16 5.06 6.31che_Cyrl 2.50 3.02 3.17 lin_Latn 1.78 2.73 4.61 ssw_Latn 3.27 4.02 5.72chk_Hani 4.88 6.75 7.08 lit_Latn 4.69 5.66 7.07 sun_Latn 2.98 3.69 4.61chk_Latn 3.20 3.94 5.36 loz_Latn 3.35 3.91 6.03 suz_Deva 1.68 1.66 2.82chv_Cyrl 2.25 2.77 4.79 ltz_Latn 3.73 3.99 5.16 swe_Latn 4.77 4.76 7.09ckb_Arab 2.38 3.15 3.86 lug_Latn 2.84 3.50 5.59 swh_Latn 4.05 4.99 7.27ckb_Latn 2.11 2.57 3.35 luo_Latn 3.34 4.09 4.90 sxn_Latn 2.08 2.54 3.06cmn_Hani 3.24 4.57 5.22 lus_Latn 2.43 2.99 5.20 tam_Latn 2.59 3.08 2.56cnh_Latn 2.17 2.75 3.62 lzh_Hani 3.21 5.56 5.47 tam_Taml 3.09 3.77 5.74crh_Cyrl 3.14 3.79 6.77 mad_Latn 2.65 3.29 4.45 tat_Cyrl 2.13 2.62 4.03crs_Latn 2.63 3.46 4.88 mah_Latn 2.95 3.59 4.92 tbz_Latn 1.62 2.03 4.22csy_Latn 2.58 3.02 4.25 mai_Deva 1.79 2.02 3.86 tca_Latn 1.29 1.56 2.77ctd_Latn 2.94 3.61 4.65 mal_Latn 2.67 3.36 2.71 tdt_Latn 3.20 3.48 5.06ctu_Latn 1.89 2.31 2.40 mal_Mlym 3.19 4.13 4.76 tel_Telu 2.87 3.78 3.98cuk_Latn 2.20 2.87 3.09 mam_Latn 1.84 2.20 2.22 teo_Latn 3.37 4.18 4.29cym_Latn 3.11 3.78 3.85 mar_Deva 3.87 5.13 5.65 tgk_Cyrl 2.63 3.29 6.11dan_Latn 4.06 5.03 6.94 mau_Latn 1.60 1.78 1.12 tgl_Latn 3.22 3.35 5.16deu_Latn 4.85 5.19 7.28 mbb_Latn 2.25 2.56 3.51 tha_Thai 1.50 2.72 4.10djk_Latn 2.07 2.46 3.53 mck_Latn 3.34 4.06 5.09 tih_Latn 2.21 2.89 4.57dln_Latn 3.89 4.89 5.23 mcn_Latn 3.74 4.42 5.60 tir_Ethi 1.90 1.93 4.03dtp_Latn 2.05 2.28 3.04 mco_Latn 1.42 1.63 1.69 tlh_Latn 3.02 3.52 5.71dyu_Latn 2.75 3.32 5.29 mdy_Ethi 1.36 1.26 2.89 tob_Latn 1.42 1.84 2.00dzo_Tibt 0.39 2.51 2.03 meu_Latn 3.26 3.79 5.10 toh_Latn 2.17 2.90 4.41\nTable 21: Accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Round Trip Alignment (Part I).\n1111\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nefi_Latn 2.55 3.25 6.23 mfe_Latn 3.61 4.19 6.26 toi_Latn 3.19 4.10 4.31ell_Grek 2.79 3.38 4.77 mgh_Latn 2.78 3.28 3.48 toj_Latn 1.43 1.84 2.25eng_Latn 4.02 4.49 6.39 mgr_Latn 3.32 4.06 6.39 ton_Latn 2.01 2.64 3.63enm_Latn 3.77 4.60 7.19 mhr_Cyrl 2.75 3.28 5.32 top_Latn 1.56 2.16 2.19epo_Latn 4.01 4.83 5.88 min_Latn 2.62 3.05 3.78 tpi_Latn 2.44 2.71 5.96est_Latn 4.34 5.24 8.21 miq_Latn 2.23 3.13 4.12 tpm_Latn 2.79 3.39 4.67eus_Latn 3.12 3.80 4.19 mkd_Cyrl 3.99 4.54 7.37 tsn_Latn 2.82 3.12 4.63ewe_Latn 2.22 2.67 4.74 mlg_Latn 3.34 3.81 6.33 tso_Latn 2.40 3.05 5.00fao_Latn 3.85 4.62 5.75 mlt_Latn 2.94 3.57 4.87 tsz_Latn 2.68 3.14 4.20fas_Arab 4.54 4.48 7.00 mos_Latn 2.71 3.24 4.25 tuc_Latn 1.43 1.83 2.36fij_Latn 2.81 3.17 4.94 mps_Latn 1.50 1.65 3.05 tui_Latn 2.47 2.83 4.53fil_Latn 3.26 3.92 4.80 mri_Latn 2.81 3.44 5.49 tuk_Cyrl 2.74 3.68 4.33fin_Latn 4.06 5.19 6.03 mrw_Latn 2.69 3.24 4.58 tuk_Latn 2.43 3.23 4.74fon_Latn 1.63 1.89 3.70 msa_Latn 3.17 3.50 5.38 tum_Latn 3.41 4.13 6.15fra_Latn 3.19 3.97 5.08 mwm_Latn 1.74 1.99 3.20 tur_Latn 5.18 4.86 7.45fry_Latn 3.36 3.99 4.52 mxv_Latn 1.75 2.11 2.31 twi_Latn 3.05 4.06 6.70gaa_Latn 2.74 3.26 6.01 mya_Mymr 1.54 1.53 2.46 tyv_Cyrl 2.31 2.83 3.33gil_Latn 2.76 3.20 4.50 myv_Cyrl 2.90 3.42 4.46 tzh_Latn 2.16 2.50 3.08giz_Latn 3.00 3.43 5.40 mzh_Latn 2.62 3.02 4.10 tzo_Latn 2.01 2.29 2.77gkn_Latn 1.93 2.07 3.31 nan_Latn 1.99 2.51 2.56 udm_Cyrl 2.90 3.48 4.72gkp_Latn 1.88 2.25 3.40 naq_Latn 2.42 3.15 4.41 uig_Arab 2.58 3.11 3.61gla_Latn 2.90 3.48 3.61 nav_Latn 1.75 2.10 2.71 uig_Latn 2.26 2.76 3.79gle_Latn 3.52 4.24 4.49 nbl_Latn 3.09 3.87 4.85 ukr_Cyrl 5.71 5.96 7.47glv_Latn 2.76 3.38 4.45 nch_Latn 2.18 2.74 3.32 urd_Arab 1.88 2.88 3.96gom_Latn 3.05 3.59 4.40 ncj_Latn 2.64 3.40 3.69 urd_Latn 2.29 2.97 3.03gor_Latn 2.26 2.73 3.71 ndc_Latn 3.32 3.85 6.67 uzb_Cyrl 2.73 3.26 7.24grc_Grek 1.11 2.00 2.93 nde_Latn 4.00 4.60 6.05 uzb_Latn 3.32 3.98 5.91guc_Latn 1.46 1.80 2.23 ndo_Latn 3.21 3.85 5.61 uzn_Cyrl 2.61 3.06 5.86gug_Latn 2.60 3.23 4.70 nds_Latn 2.98 3.69 4.70 ven_Latn 2.96 3.64 5.34guj_Gujr 3.18 4.15 4.38 nep_Deva 3.02 2.97 6.31 vie_Latn 3.99 4.48 6.69gur_Latn 2.14 2.59 3.22 ngu_Latn 1.86 2.34 3.39 wal_Latn 2.87 3.65 4.24guw_Latn 2.18 2.54 4.56 nia_Latn 2.75 3.47 3.24 war_Latn 3.04 3.74 5.43gya_Latn 1.94 2.25 4.63 nld_Latn 2.81 3.63 4.90 wbm_Latn 2.44 2.86 6.53gym_Latn 1.44 1.78 2.63 nmf_Latn 3.30 4.27 5.05 wol_Latn 3.47 4.48 6.10hat_Latn 3.21 3.64 6.39 nnb_Latn 2.46 3.14 4.08 xav_Latn 0.87 1.03 1.12hau_Latn 3.69 4.24 6.31 nno_Latn 3.90 4.61 7.41 xho_Latn 3.61 4.27 5.90haw_Latn 2.25 2.63 3.55 nob_Latn 3.88 4.81 5.83 yan_Latn 2.95 3.35 5.59heb_Hebr 1.85 2.41 3.92 nor_Latn 3.31 4.14 5.82 yao_Latn 2.01 2.66 3.87hif_Latn 2.90 3.43 3.60 npi_Deva 3.29 3.30 5.93 yap_Latn 2.86 3.41 3.45hil_Latn 2.92 3.48 4.88 nse_Latn 3.29 4.06 5.74 yom_Latn 3.25 4.00 5.17hin_Deva 3.39 3.80 5.13 nso_Latn 3.06 3.92 5.51 yor_Latn 2.24 2.68 3.88hin_Latn 2.94 3.20 4.77 nya_Latn 2.76 3.19 5.96 yua_Latn 2.04 2.26 2.86hmo_Latn 2.43 2.70 6.12 nyn_Latn 2.77 3.50 5.59 yue_Hani 2.37 3.19 2.95hne_Deva 2.48 2.53 4.95 nyy_Latn 2.21 2.74 2.95 zai_Latn 3.22 3.76 5.21hnj_Latn 2.14 2.53 4.28 nzi_Latn 2.09 2.70 4.20 zho_Hani 2.77 4.38 5.03hra_Latn 3.32 3.86 5.19 ori_Orya 2.73 2.77 3.92 zlm_Latn 4.39 5.15 7.54hrv_Latn 4.14 5.24 7.02 ory_Orya 3.27 3.20 4.39 zom_Latn 3.65 4.45 5.36hui_Latn 1.84 2.10 3.47 oss_Cyrl 2.20 2.52 5.85 zsm_Latn 4.49 5.07 8.83hun_Latn 4.54 4.10 5.62 ote_Latn 1.89 2.23 2.66 zul_Latn 3.67 4.39 5.44hus_Latn 1.70 2.00 2.42 pag_Latn 2.93 3.44 4.56\nTable 22: Accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Round Trip Alignment (Part II).\n1112\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nsrd_Latn 87.2 66.6 5.4 aka_Latn 86.7 74.1 14.2 dyu_Latn 68.5 27.4 10.2ben_Beng 5.2 3.7 7.2 mon_Latn 288 282.4 33.7 nyy_Latn 628.5 198.3 18.0ajp_Arab 74.6 34.0 44.8 gor_Latn 89.8 140.7 8.8 tzh_Latn 320.3 82.8 4.7tdx_Latn 688.4 716.4 16.0 kjb_Latn 110.8 81.1 16.2 hne_Deva 80.1 60.3 9.1tpm_Latn 99.9 90.2 17.9 lhu_Latn 44.7 12.3 2.0 bel_Cyrl 3.4 2.5 5.3grc_Grek 10.1 10.4 3.4 bos_Latn 6.1 3.4 7.9 szl_Latn 46.4 30.2 3.1sxn_Latn 469.2 148.3 14.5 lmo_Latn 48.4 25.9 6.1 ksh_Latn 340.3 227.6 19.9cos_Latn 52.1 22.8 13.3 mwn_Latn 697.8 543.8 30.7 pcd_Latn 61.2 40.8 13.2tlh_Latn 53.6 46.3 11.1 aym_Latn 1084.6 727.8 14.5 ada_Latn 100 78.5 9.5sid_Latn 1003.6 782.3 34.5 aoj_Latn 95.1 53.7 7.4 pxm_Latn 101.3 120.7 2.7jam_Latn 213.3 195.2 15.8 est_Latn 7.7 4.0 22.1 xho_Latn 32.5 9.4 16.7ban_Latn 40.8 76.1 16.1 bre_Latn 12.9 3.7 12.3 kaa_Cyrl 72.9 29.2 8.8kin_Latn 544.1 203.2 6.6 bsb_Latn 74.5 45.1 7.6 kea_Latn 754.2 525.3 13.4rop_Latn 150.7 93.4 8.4 yua_Latn 246.8 55.1 4.6 teo_Latn 587.1 271.7 62.0alz_Latn 511.9 145.6 47.7 hrv_Latn 7.4 4.9 9.7 tsc_Latn 726.3 501.1 17.0kwy_Latn 598.8 514.4 30.5 jav_Latn 20.2 4.4 22 hin_Deva 7.4 3.1 10yor_Latn 109.1 55.9 11.0 mai_Deva 42.9 48.8 6.0 ekk_Latn 7 3.8 11.8lao_Laoo 4.2 4.4 3.8 tyv_Cyrl 104.1 104.4 7.3 umb_Latn 920 838.8 17.4aze_Latn 5.6 3.6 5.4 afb_Arab 68.7 44.4 55.9 tam_Taml 7.2 2.3 9.8mya_Mymr 6.9 2.7 6.3 twi_Latn 178.9 66.7 17.9 toi_Latn 988.7 246.5 20.9ssw_Latn 345.7 108.4 20.2 sme_Latn 293 368.2 6.5 kon_Latn 463.7 418.9 16.3lus_Latn 493.5 131.2 16.4 yom_Latn 468 240.7 43.1 che_Cyrl 266.4 127.6 5.7krc_Cyrl 120.1 63.2 9.3 tob_Latn 115 78.8 7.2 gaa_Latn 109.3 33.3 13.5hbo_Hebr 6.3 3.6 5.6 mxv_Latn 69.8 29.7 5.0 tzo_Latn 246.5 54.3 7.0mgr_Latn 737.8 254.2 33.0 ron_Latn 4.4 2.9 10.4 mon_Cyrl 5.8 3.4 8.6crh_Cyrl 138.6 86.3 5.2 ile_Latn 67.9 40.1 5.7 cuk_Latn 211.5 72.1 32.0ara_Arab 10.1 6.3 18.8 cce_Latn 468.3 123.5 22.5 ces_Latn 4.4 3.1 11.6mar_Deva 7.5 4.6 11.2 uzn_Cyrl 402.4 138.7 5.2 rmy_Latn 288.2 349.8 25.0nba_Latn 638.8 675.1 14.6 ibg_Latn 897.3 807.3 21.8 phm_Latn 914.5 678.5 11.6mny_Latn 568.9 492.5 38.7 hat_Latn 228 113.3 14.0 glv_Latn 240.2 182.3 9.4run_Latn 817.5 218.5 16.9 fij_Latn 377.3 96 12.8 diq_Latn 256.6 120.5 13.4rus_Cyrl 3.3 2.3 4.5 kbp_Latn 34.6 24.5 7.1 poh_Latn 62.8 68.9 3.8hbs_Latn 4.5 2.6 6 mlt_Latn 223 162.2 10.3 oss_Cyrl 121.8 58.7 5.1lug_Latn 489 197.5 13.1 kjh_Cyrl 209.8 88.8 16.4 san_Deva 20.5 12.4 15.5pls_Latn 91.7 98.9 6.9 ndo_Latn 892.3 178.1 21.1 ote_Latn 127.8 71.2 8.0hif_Latn 21.6 46.7 13.5 rar_Latn 458.1 50.2 12.1 her_Latn 776 707.3 31.6tll_Latn 244.6 161 24.3 ell_Grek 3.4 2.6 5.9 efi_Latn 256.8 47 11.5crs_Latn 782.2 146.5 7.4 tvl_Latn 634.1 378.5 7.1 idu_Latn 117.7 90.9 12.0rng_Latn 656.6 606.8 11.7 toj_Latn 287.1 113.6 9.6 hye_Armn 3.6 4.4 3.8cjk_Latn 530.8 419.6 24.0 ikk_Latn 67.8 49.5 8.6 gcf_Latn 450.8 292.4 5.5seh_Latn 917.8 230 11.2 ory_Orya 6.1 2.8 6.3 pus_Arab 12.9 7.5 12.7rug_Latn 260.9 214.2 5.4 nor_Latn 5 2.8 8.5 sgs_Latn 119.2 124.7 10.5hau_Latn 14.5 7.1 17.2 enm_Latn 43.1 31.0 36.6 mbb_Latn 177.1 138 4.2uzb_Latn 5.6 3.6 5.8 arz_Arab 17.5 1.5 6.8 som_Arab 7.2 3.1 9.3bim_Latn 142.2 97.3 11.3 bem_Latn 706.9 219.9 27.1 hsb_Latn 109.6 103.6 5.2vep_Latn 218.1 111.5 6.1 gkp_Latn 33.1 30.2 12.7 ary_Arab 32.7 4.6 26slv_Latn 7.8 4.9 26.9 guj_Gujr 6.2 3.6 6.5 hmo_Latn 509.3 77.7 10.9azj_Latn 5.3 3.3 5.1 tbz_Latn 39.2 40.4 8.4 quw_Latn 177.8 157.7 26.1cac_Latn 51.4 39.3 7.0 ven_Latn 268.3 62 9.4 pag_Latn 923.5 232.4 25.8npi_Deva 8.6 4.9 7.3 crh_Latn 151 70.9 6.5 ber_Latn 639.1 981.4 21.3lin_Latn 377.3 96.6 15.3 xmv_Latn 593.2 491.4 19.4 chk_Latn 766.9 151.6 19.1zom_Latn 238.7 176.2 22.8 slk_Latn 4 2.9 11.2 kan_Knda 7.2 2.8 8.9kmr_Cyrl 140.6 56.7 4.1 zne_Latn 854.7 658.4 48.8 loz_Latn 895 113.7 27.8acm_Arab 113.6 74.0 81 cgg_Latn 565.7 454.4 12.4 tih_Latn 247.6 151.3 4.9fin_Latn 4.2 3.1 21.7 vie_Latn 7.6 3.1 16.4 mfe_Latn 767.9 255.4 10.1rmn_Grek 108.9 76.8 3.3 amh_Ethi 8.9 5.3 7.5 tel_Telu 6.5 4.0 7.9wls_Latn 334.9 207.9 4.0 nyu_Latn 926.2 479.2 9.3 ina_Latn 26.9 17.1 7.2hun_Latn 5.1 3.3 25.1 suz_Deva 63.4 76.4 2.5 isl_Latn 7.9 4.9 16.7lƒ≥_Latn 98.8 55.1 5.9 tuc_Latn 108.9 80.8 7.6 tsz_Latn 990.6 199.7 14.2quh_Latn 279 176.6 16.5 lub_Latn 670.8 577.5 23.8 ori_Orya 5.2 3.0 4.7yap_Latn 507.3 195.9 10.6 epo_Latn 10.8 5.2 21 tat_Latn 168.4 65.5 6.9abk_Cyrl 122.6 89.5 20.1 ksw_Mymr 16.6 7.5 4.6 arg_Latn 29.2 13.6 7.2cmn_Hani 10.4 5.0 9.8 mwl_Latn 69.1 35.6 4.9 kia_Latn 132.4 126.8 18.5csb_Latn 112.8 59.4 6.1 cak_Latn 101.7 46.1 5.4 afr_Latn 12.2 7.8 19.2nbl_Latn 137.7 19.6 13.9 bar_Latn 124.7 108.9 14.4 myv_Cyrl 97.7 153.3 8.5ndc_Latn 1188.5 374.6 19.4 asm_Beng 6 3.8 5 bik_Latn 170.4 60.3 13.7oci_Latn 41.2 24.4 8.3 grn_Latn 199.3 141.6 10.3 ltz_Latn 39.7 165.1 10.9fao_Latn 84.2 35.6 5.5 tso_Latn 506.1 115.2 13.2 iso_Latn 236.2 222.4 8.7tui_Latn 126.1 127 20.6 nso_Latn 656.3 153.4 9.1 ewe_Latn 198 54.6 20.0xav_Latn 21.4 15.9 5.7 bum_Latn 282.8 91.5 22.1 als_Latn 7.6 2.5 6.4\nTable 23: Perplexity of all languages covered by Glot500-m (Part I).\n1113\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nswc_Latn 39.2 22.5 13.2 top_Latn 589.2 89.6 23.5 hin_Latn 11.1 22.1 11.9deu_Latn 4.4 3.6 10.2 bin_Latn 278.1 169.8 13.3 eng_Latn 5.7 4.0 7.5caq_Latn 185.9 129 21.6 chw_Latn 778.9 645.8 33.9 hus_Latn 134.6 68.2 5.3ceb_Latn 63.1 53.1 2.1 hyw_Cyrl 268.5 233.5 6.3 urh_Latn 236.8 211.5 11.4nia_Latn 280.3 85.5 7.5 kor_Hang 7.2 2.6 11 mkd_Cyrl 4.3 3.1 6.2urd_Arab 8.3 5.3 8.7 btx_Latn 463 163.1 19.3 wbm_Latn 58.9 47.3 13.6niu_Latn 600.1 437.5 10.1 srn_Latn 609.3 137.2 12.6 kwn_Latn 1053.6 753.2 32.0mrw_Latn 320.8 174.9 7.6 llb_Latn 555.6 589.8 41.1 guc_Latn 432.6 117.8 9.4bul_Cyrl 3.9 3.6 6.8 cbk_Latn 129.5 60.4 11.6 quc_Latn 270.7 83.9 5.6pau_Latn 333.7 147.3 7.2 bcl_Latn 270 60.1 12.5 nds_Latn 112.5 161.1 7.4tha_Thai 10.8 2.9 14.6 csy_Latn 198.3 152.5 21.7 ind_Latn 8.5 5.4 17.1ilo_Latn 786.7 184.4 13.8 ctd_Latn 249.2 166.1 11.6 nde_Latn 56.7 21.5 12.1kss_Latn 90.4 13.2 11.2 plt_Latn 10.8 3.6 5.7 kua_Latn 1104.8 191.2 13.4zai_Latn 719.4 212.5 10.4 smo_Latn 235.7 55.6 7.0 nch_Latn 705.1 166.4 11.2guw_Latn 267.7 65.5 6.9 kab_Latn 744.5 203.5 24.3 por_Latn 5.1 3.9 9.3kbd_Cyrl 175.7 94.4 9.1 gom_Deva 82.8 48.4 9.0 jpn_Jpan 7.9 3.9 10dln_Latn 238.8 207.8 7.5 ukr_Cyrl 3.1 2.9 5.9 spa_Latn 4.6 3.5 7.8war_Latn 200.9 110.7 2.3 ast_Latn 27.5 18.6 4.8 knv_Latn 129 78.3 5.8tca_Latn 70.4 49 6.0 lvs_Latn 4.8 2.7 5.7 agw_Latn 150.1 73.4 16.3iku_Cans 2.2 1.9 5.8 rmn_Cyrl 624.3 513.1 8.7 ige_Latn 181.1 105.2 11.9bjn_Latn 41.3 17.6 11.4 kir_Cyrl 7.7 2.9 11.9 dua_Latn 232.8 152.2 19.1ngu_Latn 918 110.9 13.4 pfl_Latn 152 101.3 11.3 ogo_Latn 131.3 129.7 31.1kmr_Latn 68 4.6 10.6 bqc_Latn 102.7 71.1 26.5 bas_Latn 410.4 437.7 16.7tgl_Latn 7.9 4.4 8.9 yid_Hebr 7.6 4.8 5.1 bpy_Beng 20 21.4 2.9eus_Latn 10.7 6.2 37.3 fil_Latn 9.2 2.3 9.9 lfn_Latn 60.4 51 6.9hra_Latn 212.1 177.7 54.3 nap_Latn 81.7 39.6 10.5 ton_Latn 116 65.2 2.8lue_Latn 839.2 627.4 19.8 heb_Hebr 6.7 4.9 13.5 lim_Latn 66.8 43.5 11.4pol_Latn 4.5 2.7 10.6 sba_Latn 75.7 81.8 6.0 lav_Latn 4.2 2.2 6.6leh_Latn 476.5 253.9 26.2 ifa_Latn 371.9 266.1 6.0 bih_Deva 27.6 16.1 5.0lat_Latn 15.3 3.7 24.5 ami_Latn 1070.7 710.2 29.2 gym_Latn 509.6 66.3 17.0div_Thaa 1.6 1.5 3.5 gil_Latn 763.5 161.3 15.7 ish_Latn 144.9 134 11.6min_Latn 105 39.7 3.9 djk_Latn 360.4 93.4 13.4 zea_Latn 69.6 27.5 8.7ctu_Latn 177.4 37.9 4.5 new_Deva 36.1 29.8 4.5 aln_Latn 3.9 2.3 12.7tur_Latn 9.1 4.1 29.5 bam_Latn 74.5 23.7 46.8 gcr_Latn 352.9 314.7 7.5dhv_Latn 509 435.8 11.8 wol_Latn 236.4 158.3 32.0 kal_Latn 377.2 370.9 8.3lua_Latn 706 784.5 21.7 alt_Cyrl 140.7 50.9 9.3 dan_Latn 6 3.6 13.1rmy_Cyrl 488.1 389.3 9.3 kri_Latn 87.6 35.8 8.6 tah_Latn 363 330.9 4.8zpa_Latn 476.1 550.1 13.6 kom_Cyrl 93.4 57 4.9 kik_Latn 205.8 55.5 12.1gom_Latn 405.7 282.9 27.9 sah_Cyrl 99.9 91.1 4.5 vmw_Latn 828.8 434.8 17.8dtp_Latn 166.4 78.7 5.5 mzh_Latn 132.8 133.4 9.6 eml_Latn 283.4 144.9 6.6fra_Latn 4.1 2.8 6.9 sna_Latn 316.6 331.1 16.4 sco_Latn 28.1 15.5 9.8cat_Latn 4.1 2.2 7.3 bzj_Latn 264.7 75.8 10.9 kac_Latn 189.9 76.3 17.9xmf_Geor 71.2 72.3 3.8 nld_Latn 5.7 4.5 12 ttj_Latn 865.2 509.5 15.5ixl_Latn 53 29.6 4.2 gug_Latn 626.9 141.6 8.4 lun_Latn 720.1 565.6 31.9ckb_Arab 72.2 80.6 6.0 yue_Hani 17.8 10.6 10.8 sot_Latn 269.1 122.4 8.1ahk_Latn 44.8 9.1 2.1 fry_Latn 16.1 15.4 17.2 mau_Latn 199.7 13.6 8.4sag_Latn 491.4 68.7 11.1 jbo_Latn 132.3 187.1 9.0 yan_Latn 134.4 108.4 31.4qug_Latn 505 135.2 13.7 iba_Latn 529.3 87 16.6 ido_Latn 79.8 24.2 7.1nyn_Latn 834.8 236.9 16.8 nya_Latn 319.6 256.8 12.7 rmn_Latn 968.8 1062.8 22.9koo_Latn 481.3 321.6 13.8 tat_Cyrl 99.8 116 4.1 sat_Olck 1.4 1.2 4.6uig_Arab 8.1 2.4 5.5 nzi_Latn 113.7 47.4 12.5 mad_Latn 132.7 90.2 7.9kam_Latn 225.9 155.7 10.3 wal_Latn 492.7 120.3 18.1 hil_Latn 366 38.7 9.6gkn_Latn 248 74.6 9.4 pdt_Latn 417.7 143 13.3 khm_Khmr 4.8 3.2 4.5twx_Latn 1209.8 978.2 15.5 apc_Arab 74.8 42.2 37.2 fon_Latn 71.8 27 10.4skg_Latn 665.4 624.1 15.8 mdy_Ethi 65.7 68.4 5.4 ngl_Latn 664.9 518.3 15.9arb_Arab 4.1 2.1 6 rue_Cyrl 18.7 11.4 4.5 tcf_Latn 224.5 225.4 6.9mco_Latn 295 37.6 4.6 azb_Arab 194.1 141.8 4.8 gur_Latn 86.2 39 17.9sqi_Latn 6.2 2.1 8.4 bci_Latn 129.6 95.6 8.7 qvi_Latn 863.4 91.5 12.3cnh_Latn 496 154.4 16.3 kmm_Latn 193.3 164.9 20.2 izz_Latn 95.5 78.5 5.5sin_Sinh 7.5 5.4 9.8 bak_Cyrl 99 79 5.3 kur_Arab 90.3 76.3 5.7kmb_Latn 564.8 465.8 15.6 miq_Latn 347.4 198.9 23.6 hbs_Cyrl 3.7 2.3 4.3vol_Latn 78.4 67.7 2.4 kaa_Latn 94.2 100.6 7.3 ach_Latn 488.8 114.6 77.3msa_Latn 8.2 26.1 15 bod_Tibt 8.8 4.0 6.3 wuu_Hani 35.9 16.8 11.7bba_Latn 75.5 65.5 16.3 glg_Latn 5.9 4.6 9.2 quz_Latn 804.5 269.4 12.2tgk_Latn 11.9 11.7 7.5 tum_Latn 516.4 168.3 10.2 tok_Latn 592.4 423 94.5tiv_Latn 912.3 716.3 29.3 bbc_Latn 787.9 203.7 13.6 bis_Latn 727.1 47.7 10.7hmn_Latn 60.9 52.5 8.8 kek_Latn 126.4 40.6 4.3 fur_Latn 196.5 142.8 7.7swh_Latn 12.6 5.8 24.4 ace_Latn 81.5 54 6.4 ium_Latn 36.6 33.1 7.2pis_Latn 563.2 64.7 9.7 pam_Latn 59.6 276.7 28.2 nse_Latn 771.7 292.3 13.7mzn_Arab 50 34.3 6.3 fas_Arab 8 4.1 14.1 zul_Latn 36.3 10.1 21.7\nTable 24: Perplexity of all languages covered by Glot500-m (Part II).\n1114\nLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-mLanguage-Script XLM-R-B XLM-R-L Glot500-m\nbts_Latn 205.7 204.5 8.8 tsn_Latn 264.7 137.8 12.5 orm_Latn 23.4 8.6 16gla_Latn 11.5 12.7 7.2 pon_Latn 928.4 181.9 19.2 luo_Latn 699.4 258.5 85.1kat_Latn 36.4 24.8 18.3 nmf_Latn 297.6 310.6 44.9 pcm_Latn 38.3 169.6 3.6uig_Latn 188.8 173.9 15.2 ajg_Latn 147.1 149.5 22.6 nnb_Latn 364.1 95 28.6kat_Geor 6 3.9 6.4 tir_Ethi 28.3 15.7 4.4 kaz_Cyrl 4.3 5.4 9.6mlg_Latn 10.9 4.4 7.6 bhw_Latn 411.2 126.2 21.6 dzo_Tibt 8.5 3.3 5.7arn_Latn 382.7 96.7 17.6 mhr_Cyrl 122.9 168.4 5.8 sun_Latn 23.6 11.9 17tuk_Latn 456.7 197.8 5.8 swe_Latn 4.8 3.5 12.7 vec_Latn 40.6 21.1 9.2vls_Latn 97.7 39.6 9.7 scn_Latn 117 64.9 7.8 ayr_Latn 261.1 237.6 27.7hyw_Armn 15.8 9.1 4.3 udm_Cyrl 356.7 224.9 6.7 oke_Latn 209.2 220.1 13.0que_Latn 447.9 536.1 11.9 ifb_Latn 246.3 177.9 5.1 kur_Latn 14.2 6.8 10.3snd_Arab 13.2 4.1 19.5 naq_Latn 136.8 60.2 15.7 mgh_Latn 680 272.8 23.7giz_Latn 81.9 82.9 37.7 zlm_Latn 5.6 3.3 4.6 tgk_Cyrl 181.3 153 4.5ita_Latn 4.5 3.3 7.2 hrx_Latn 478.1 679.1 14.9 sop_Latn 607.5 228.2 29.5qub_Latn 283.2 312.7 9.4 lzh_Hani 70 58 21.8 mos_Latn 272.6 118.3 13.2nav_Latn 228.5 126.5 5.2 pap_Latn 674.4 149.3 18.1 rap_Latn 36.1 31.1 2.8kqn_Latn 825.9 686.6 17.5 cfm_Latn 235.1 155 14.0 prk_Latn 69.4 45.9 7.1toh_Latn 758.3 216.6 19.6 chv_Cyrl 122.5 73.8 5.4 uzb_Cyrl 236.2 138.4 4.9mah_Latn 314.7 81.8 17.3 tdt_Latn 641.9 78.6 9.7 tog_Latn 821.1 777.7 13.4wes_Latn 144.6 103.9 14.3 pan_Guru 4.4 2.5 4.3 mal_Mlym 5 3.7 6.2nob_Latn 6.8 4.0 9.5 pms_Latn 83.6 46.2 3.6 nyk_Latn 1182.6 914.2 16.5ext_Latn 68.3 38.2 8.1 roh_Latn 243.5 170 7.0 quy_Latn 949.7 320.2 14.5lam_Latn 233.7 160.8 21.6 prs_Arab 6.8 3.5 4.8 abn_Latn 245.2 272.5 8.7mwm_Latn 44.8 53.1 7.1 tuk_Cyrl 277.4 86.3 6.7 mcn_Latn 120.7 129.7 43.6kpg_Latn 165.9 122.6 15.1 srm_Latn 257.5 74.5 12.3 nep_Deva 8.8 6.3 10hau_Arab 5.3 3.0 8.1 gsw_Latn 288.2 181.2 22.3 gle_Latn 10.5 3.7 9.8ksd_Latn 150 154.9 7.7 fat_Latn 192.3 149 17.6 cab_Latn 1216.7 155.6 15.4zsm_Latn 12.2 2.9 22.7 ldi_Latn 394.8 107.1 38.2 mps_Latn 75.2 55.2 17.4hui_Latn 209.9 177 10.0 kos_Latn 470.7 485.7 27.0 pnb_Arab 51.8 30.8 7.1cym_Latn 8.2 4.8 11.2 acr_Latn 155.7 90.7 5.8 swa_Latn 11.4 6.4 20srp_Latn 10.9 7.9 13.3 mri_Latn 63 59.5 8.7 hnj_Latn 88.3 92.5 11.3bak_Latn 347.1 211 7.5 frr_Latn 117.6 101 9.5 haw_Latn 63.5 66.7 7.4zho_Hani 20.7 5.9 31.3 mck_Latn 369.3 164.8 24.7 tpi_Latn 891.8 67.8 8.8nno_Latn 9.9 12.7 10.4 pes_Arab 5.5 3.1 5.3 ncj_Latn 1019 136.2 13.7gya_Latn 31 24.3 16.5 san_Latn 94.4 96.8 12.0 som_Latn 14.1 6.9 22.2ibo_Latn 77.1 90.1 8.5 yao_Latn 738.9 162.4 13.8 mam_Latn 132.7 62.4 6.1meu_Latn 380.2 158.5 26.7 srp_Cyrl 7.4 4.5 8.4 lit_Latn 4.4 2.5 10.6ncx_Latn 1084.7 948.5 14.6 ful_Latn 104 105.6 13.1\nTable 25: Perplexity of all languages covered by Glot500-m (Part III).\n1115\nACL 2023 Responsible NLP Checklist\nA For every submission:\n‚ñ°\u0013 A1. Did you describe the limitations of your work?\nsection ‚ÄôLimitation‚Äô\n‚ñ°\u0013 A2. Did you discuss any potential risks of your work?\nsection ‚ÄòEthics Statement‚Äô\n‚ñ°\u0013 A3. Do the abstract and introduction summarize the paper‚Äôs main claims?\nAbstract and section 1\n‚ñ°\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB ‚ñ°\u0013 Did you use or create scientiÔ¨Åc artifacts?\nsection 3.3, section 4, appendix c\n‚ñ°\u0013 B1. Did you cite the creators of artifacts you used?\nsection 3.3, section 4, appendix c\n‚ñ°\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nsection ‚ÄòEthics Statement‚Äô\n‚ñ°\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciÔ¨Åed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nsection ‚ÄòEthics Statement‚Äô\n‚ñ°\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiÔ¨Åes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSince our work deals with millions of sentences in hundreds of languages, it was impossible for us to\ncheck the content. We leave it as a future work\n‚ñ°\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nsection 3.1, appendix a, appendix c\n‚ñ°\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiÔ¨Åcant, while on small test sets they may not be.\nsection 5\nC ‚ñ°\u0013 Did you run computational experiments?\nsection 4.2\n‚ñ°\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nsection 4.2\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1116\n‚ñ°\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nsection 5\n‚ñ°\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nsection 5. For continued pretraining, it is a single run due to computational resource limitation. For\ndownstream task evaluation, it is multilple runs across 5 seeds.\n‚ñ°\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nsection 3.3\nD ‚ñ°\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n‚ñ° D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n‚ñ° D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants‚Äô demographic\n(e.g., country of residence)?\nNo response.\n‚ñ° D3. Did you discuss whether and how consent was obtained from people whose data you‚Äôre\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n‚ñ° D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n‚ñ° D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n1117",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6726465225219727
    },
    {
      "name": "Computational linguistics",
      "score": 0.5460593104362488
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5195103883743286
    },
    {
      "name": "Natural language processing",
      "score": 0.5093199014663696
    },
    {
      "name": "Linguistics",
      "score": 0.4847121238708496
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4268163740634918
    },
    {
      "name": "Scaling",
      "score": 0.4173814058303833
    },
    {
      "name": "Mathematics",
      "score": 0.15642660856246948
    },
    {
      "name": "Philosophy",
      "score": 0.14302152395248413
    },
    {
      "name": "Physics",
      "score": 0.06690940260887146
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4403386549",
      "name": "Munich Center for Machine Learning",
      "country": null
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universit√§t M√ºnchen",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4387152517",
      "name": "Instituto Superior T√©cnico",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210120471",
      "name": "Instituto de Telecomunica√ß√µes",
      "country": "PT"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210150358",
      "name": "Institut Syst√®mes Intelligents et de Robotique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I39804081",
      "name": "Sorbonne Universit√©",
      "country": "FR"
    }
  ],
  "cited_by": 10
}