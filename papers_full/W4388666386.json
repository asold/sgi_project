{
  "title": "Orientation Cues-Aware Facial Relationship Representation for Head Pose Estimation via Transformer",
  "url": "https://openalex.org/W4388666386",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100624965",
      "name": "Hai Liu",
      "affiliations": [
        "Central China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2101963880",
      "name": "Cheng Zhang",
      "affiliations": [
        "Central China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2146910251",
      "name": "Yong-jian Deng",
      "affiliations": [
        "Beijing University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1971233859",
      "name": "Tingting Liu",
      "affiliations": [
        "Hubei University"
      ]
    },
    {
      "id": "https://openalex.org/A2105790825",
      "name": "Zhaoli Zhang",
      "affiliations": [
        "Central China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2997145757",
      "name": "Youâ€Fu Li",
      "affiliations": [
        "City University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4212907698",
    "https://openalex.org/W3015671815",
    "https://openalex.org/W2330274280",
    "https://openalex.org/W3111785105",
    "https://openalex.org/W2946709531",
    "https://openalex.org/W3131309013",
    "https://openalex.org/W3112693297",
    "https://openalex.org/W2142727917",
    "https://openalex.org/W2589255576",
    "https://openalex.org/W2990347595",
    "https://openalex.org/W2883284130",
    "https://openalex.org/W2087681821",
    "https://openalex.org/W2949662773",
    "https://openalex.org/W1896788142",
    "https://openalex.org/W2088028039",
    "https://openalex.org/W1643185111",
    "https://openalex.org/W2215711223",
    "https://openalex.org/W2737644856",
    "https://openalex.org/W2957744218",
    "https://openalex.org/W4226087396",
    "https://openalex.org/W3175906877",
    "https://openalex.org/W3162748909",
    "https://openalex.org/W2923153064",
    "https://openalex.org/W4308233968",
    "https://openalex.org/W3119574651",
    "https://openalex.org/W2022566595",
    "https://openalex.org/W4386076337",
    "https://openalex.org/W2927589841",
    "https://openalex.org/W2963644257",
    "https://openalex.org/W4213198672",
    "https://openalex.org/W4210726167",
    "https://openalex.org/W3183842024",
    "https://openalex.org/W3034552680",
    "https://openalex.org/W2963377935",
    "https://openalex.org/W3092537075",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W4313153644",
    "https://openalex.org/W2963076818",
    "https://openalex.org/W2964093990",
    "https://openalex.org/W2921491036",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3139434170",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6809665764",
    "https://openalex.org/W3195286673",
    "https://openalex.org/W3203925315",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W6769955919",
    "https://openalex.org/W2047875689",
    "https://openalex.org/W2964014798",
    "https://openalex.org/W6778161298",
    "https://openalex.org/W2998505840",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W4299802238",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3028400540",
    "https://openalex.org/W4221146106",
    "https://openalex.org/W3104792420"
  ],
  "abstract": "Head pose estimation (HPE) is an indispensable upstream task in the fields of human-machine interaction, self-driving, and attention detection. However, practical head pose applications suffer from several challenges, such as severe occlusion, low illumination, and extreme orientations. To address these challenges, we identify three cues from head images, namely, critical minority relationships, neighborhood orientation relationships, and significant facial changes. On the basis of the three cues, two key insights on head poses are revealed: 1) intra-orientation relationship and 2) cross-orientation relationship. To leverage two key insights above, a novel relationship-driven method is proposed based on the Transformer architecture, in which facial and orientation relationships can be learned. Specifically, we design several orientation tokens to explicitly encode basic orientation regions. Besides, a novel token guide multi-loss function is accordingly designed to guide the orientation tokens as they learn the desired regional similarities and relationships. Experimental results on three challenging benchmark HPE datasets show that our proposed TokenHPE achieves state-of-the-art performance. Moreover, qualitative visualizations are provided to verify the effectiveness of the token-learning methodology.",
  "full_text": " \n \n \nOrientation Cues-aware Facial Relationship \nRepresentation for Head Pose Estimation via \nTransformer \n \nï€ Abstractâ€”Head pose estimation (HPE) is an indispensable \nupstream task in the fields of human -machine interaction, self -\ndriving, and attention detection. However,  practical head pose \napplications suffer from several challenges, such  as severe \nocclusion, low illumination, and extreme orientations. To address \nthese challenges, we identify three cues from head images, namely, \ncritical minority relationships , neighborhood orientation \nrelationships, and significant facial changes. On the basis of the \nthree cues, two key insights on head poses are revealed: 1) i ntra-\norientation relationship and 2) c ross-orientation relationship. To \nleverage two key insights above, a novel relationship -driven \nmethod is proposed based on the Transformer archi tecture, in \nwhich facial and orientation relationships can be learned. \nSpecifically, we design several orientation tokens to explicitly \nencode basic orientation regions. Besides, a  novel token guide \nmulti-loss function is accordingly designed to guide the orientation \ntokens as they learn the desired regional similarities and \nrelationships. Experiment al results  on three challenging \nbenchmark HPE datasets show that our proposed TokenHPE  \nachieves state -of-the-art performance. Moreover, qualitative \nvisualizations are provided to verify the effectiveness of the token-\nlearning methodology. \nIndex Terms â€”Head pose estimation,  attention mechanism,  \nrelationship perception, deep learning, Transformer. \nI. INTRODUCTION \nEAD pose estimation (HPE) is a popular research area \nin image processing [1-3] and an indispensable \nupstream task in human -machine interaction [4-7], \ndriver assistance [8], virtual reality [9, 10] , and \n  \nattention detection [11]. In the past few years, the accuracy of \nHPE has been considerably improved in terms of utilizing extra \nfacial landmark information [12, 13] , extra RGB -depth \ninformation [14-17], extra temporal information  [18], stage -\nwise regression strategy [19], multitask learning [20, 21], and \nalternative parametrizations of orientation  [22-27]. However, \nseveral challenges still exist for practical application where \nocclusion, unstable illumination and extreme orientations  are \nubiquitous. \n \nFig. 1. Existing challenges on head pose estimation, including (a) â€“(b) serious \nocclusions, (c)â€“(d) poor illumination, and (e)â€“(f) extreme orientations. Some or \neven most of the facial parts are missing in these scenarios, resulting in \ndifficulties for HPE. \n \nA. Challenges \nCurrently, convolutional neural networks (CNNs) have \nbecome prevalent on computer vision tasks, and they are widely \nadopted on HPE. CNN -based HPE methods [19, 24, 27 -29] \nhave achieved impressive performanc e due to the powerful \nabilities of CNNs on representing superficial vi sual patterns. \nNevertheless, the intrinsic r elationships of head orientations \nand facial parts are usually neglected. A possible reason is that \nthese relationships are theoretically difficult to learn by existing \nCNN architectures, which are based on pattern driven learning. \nIn normal and easy -to-predict scenarios, highly accurate head \npose predictions can be achieved by detecting facial patterns \nthrough CNNs. However, in some challenging scenarios  (Fig. \n1), such as severe occlusions, poor illumination, and extreme \nOcclusion\nLow \nillumination\nExtreme\norientations\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nSunglasses\nHand\nHai Liu, Senior Member IEEE, Cheng Zhang*, Student Member IEEE, Yongjian Deng, Member IEEE,  \nTingting Liu, Member IEEE, Zhaoli Zhang, Senior Member IEEE, You-Fu Li*, Fellow, IEEE, \nH \nThis work was sup ported in part by the National Key R&D Program of \nChina under Grant 2021YFC3340802, the National Natural Science \nFoundation of China under Grant (62277041, 62211530433, 62177018, \n62173286, 62077020 , 62005092, 62203024, 92167102), and in part by the \nResearch Grants Council of Hong Kong (Project No. CityU11213420 and \nCityU11206122). Paper no. TIP-29310-2023.R2. (*Corresponding author: \nCheng Zhang, Youfu Li) \nHai Liu , Cheng Zhang, Zhaoli Zhang are  with the National Engineering \nResearch Center for E -Learning, Central  China Normal University, Wuhan \n430079, China. (email: hailiu0204@ccnu.edu.cn, zc2021@mails.ccnu.edu.cn, \nzl.zhang@ccnu.edu.cn). \nYongjian Deng is with College of Computer Science, Beijing University of \nTechnology, Beijing, China. (Email: yjdeng@bjut.edu.cn) \nTingting Liu is School of Education, Hubei University, No. 368 Youyi Road, \nWuhan 430062, Hubei, China, and also with Department of Mechanical \nEngineering, City University of Hong Kong, Kowloon, Hong Kong. (E -mail: \ntliu@hubu.edu.cn) \nYou-Fu Li is with the Department of Mechanical Engineering, City \nUniversity of Hong Kong, Kowloon, Hong Kong. (E-mail: \nmeyfli@cityu.edu.hk). \nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n  \n \norientations, many remarkable facial parts are missing because \nof occlusion or low light, which is devastating for existing \nCNN-based methods that highly depend on facial patterns for \nprediction. Consequently, the few remaining facial parts and \ntheir geometric  relationships must be leverage d to achieve \nrobust and high -accuracy prediction. Furthermore, the latent \nrelationships of neighborhood orientations also can be exploited \nwhen facial part missing happens in the current orientation. \nRecently, there are a stream of researches that explore \nTransformer-architecture as an alternative to CNN layers  in \ntheir models and achieve compelling performance, nevertheless \nthe main structures are still a CNN-style, leaving the long-range \nand semantic relationships virtually intact.  Therefore, how to \nleverage the head orientation and facial part relationships is \nconsiderably attractive on the research topic of high accuracy \nand robust HPE.  \nB. Observation and Insights \nFor the purpose  of utilizing the facial and orientational \ninformation to facilitate HPE, i n this study, we identify three \ncues by careful observation. First, critical minority \nrelationships of facial parts exist, and they can determine the \norientation of a head pose despite possible occlusions and \nmissing facial parts. For example, as shown in Fig. 1(a), if a \npersonâ€™s eye is occluded, then the head orientation can be  \ndetermined by the geometric spatial relationships of the \nremaining facial parts, such as ear, nose, and the outline of the \nface. In other words, by ingeniously leveraging the semant ic \nrelationships of the few remaining facial parts, accurate \nprediction can be achieved despite severe facial part missing. \nThis important cue is defined as critical minority relationships. \nSecond, a local similarity in neighbored orientation regions \nexists. As shown in the right part of Fig. 2, the facial \nappearances in neighbored orientations are similar, which \nindicates that neighborhood orientation information can be \nleveraged to improve accuracy. In a local orientational region, \nhead poses and their co rresponding latent facial characteristics \nenjoy high semantic similarities. Therefore, the neighborhood \norientations contribute latent semantic information to the \ncentral orientation. Prediction can be facilitated by taking the \nneighborhood orientation inf ormation, which is defined as \nneighborhood orientation relationships . Third, several \nsignificant facial part changes  are observed  in specific \norientations. For example, two facial regions can be \ndistinguished by a significant facial part change, such as th e \nappearance/disappearance of eye on one side, appearance of the \nnostril, and overlapping of the nose and mouth. The set of head \norientations can be partitioned into several highly similar local \nfacial regions by these significant facial part changes. Generally, \nwe find three cues ( critical minority relationships , \nneighborhood orientation relationships , and significant facial \npart changes) that veiled in head poses, which are necessary for \nefficient HPE on all scenarios.  \nFurthermore, on the basis of the three cues, we reveal two \ninsights in head poses, namely, the intra-orientation relationship \nand cross-orientation relationship. We argue that the two novel \ninsights on facial and orientational relationships are curial for \nefficient HPE from a different relationship -driven learning \nparadigm. The proposed two key insi ghts are introduced as \nfollows. \n \nFig. 2. Illustration of (a) intra-orientation relationship and (b) cross-orientation \nrelationship. The critical minority relationships in a single image are deduced \nby self -attention among visual tokens, the neighborhood orientation \ninformation is encoded in orientation tokens, and their relationships are \ndeduced by self -attention among orientation tokens. Finally, the intra - and \ncross-orientation relationships are exchanged by cross attention.  \n \nKey insight I:  Intra-orientation relationship . There exists \ncritical minority relationship in a specific head orientation (a \nsingle head image). The few facial parts and their relationships \nwithin a head image defined as i ntra-orientation relationship s \nare crucial for prediction and more robust and reliable than \nmerely superficial visual patterns. Figure 2(a) provides an \nillustration of the i ntra-orientation relationship . As can be \nobserved, a single image has many informativ e facial patterns, \nbut only the core facial parts and their relationships are \ndeterminative for prediction. On the basis of critical minority \nrelationship learning, detriments of facial part missing from \nocclusion or poor illumination can be greatly alleviated.  \nKey insight I I: Cross -orientation relationship.  We argue \nthat the vicinal and symmetric orientation characteristics are \ninformative to the central orientation due to their high \nsimilarities. This property is defined as c ross-orientation \nrelationship, because interrelated orientational features are \nlearned and for prediction. As show in Fig. 2(b), the attention is \ndistributed to the vicinal orientational regions, allowing a larger \nreception field than in a single image to collect more \norientational information for prediction. However, this general \nrelationship cannot be encoded by CNN because of its inherent \narchitecture. Therefore, the cross-orientation relationships have \nkept as untapped treasures that are hardly leveraged by previous \nworks on HPE. \nGiven the aforementioned key insights on head pose images, \nthe question is how to design a model that can utilize this \nheuristic knowledge. The traditional CNN architecture cannot \neasily learn these relationships. By contrast, the Transformer \narchitecture c an effectively address this drawback of CNN. \nRecently, Vision Transformer (ViT) [30] emerged as a new \nchoice for various computer vision tasks. The Transformer \narchitecture is known for its extraordinary ability to learn long-\ndistance, high-level relation ships between image patches. \nTherefore, using Transformer to learn the intra-orientation \nrelationship is reasonable . Moreover, cross-orientation \n(a) Intra orientation relationship (b) Cross orientation relationship\nCritical minority relationship Neighborhood orientation relationship\nCross attention\nSelf attention Self attentionï– ï–\nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n  \n \nrelationships can be well-represented by learnable tokens in the \nTransformer.  \nC. Contributions \nInspired by the  two key insights and Transformer â€™s \nproperties, this study  proposes TokenHPE, a method that can \ndiscover and leverage intra-orientation and cross -orientation \nrelationships via the Transformer architecture. The proposed \nmethod can discover facial part geometric relationships via self-\nattention among visual tokens, and the orientation tokens can \nencode the characteristics of the neighborhood orientation \nregions. These relationships between visual and orientation \ntokens are learned by TokenHPE from abundant synthetic data. \nThe learned information is encoded into the orientation tokens, \nwhich can be visualized by vector similarities. In addition , a \nspecial token guide multi -loss function is constructed to help \nthe orientation token learn the general information. Notice that \nalthough currently there are several Transformer-based \napproaches for HPE, the superior properties of Transformer \narchitecture than CNN architecture, which is the capability to \nreveal the long -range and semantic relationships of the input \ntoken sequences, has not been exploited. The gap between them \nand our method is that they utilize the Transformer encoder \nlayers as a supplementary module for the main CNN structure, \nwhile we take Transformer blocks as our  core design . \nSpecifically, we divide the input image into patches, \nconsidering them as visual tokens that contains semantic \ninformation analogous to â€œwordsâ€ in natural language \nprocessing. Then, we proposed several learnable orientation \ntokens that represent the orientation knowledge to interact with \nthe visual tokens in the Transformer blocks via attention \nmechanism. Overall, our main contributions can be summarized \nas follows: \nâ— Three cues are derived on head images, including critical \nminority relationships , neighborhood orientation \nrelationships, and  significant facial part changes . \nFurthermore, to leverage our findings and cope with \nchallenging scenarios, a novel token-learning model based \non Transformer for HPE is presented. \nâ— We reveal two key insights in he ad poses , namely, the \nintra-orientation relationship and the cross-orientation \nrelationship. Several learnable orientation tokens are \ndesigned to encode the general information  of c ross-\norientation relationship s. Moreover, a novel token guide \nmulti-loss function is designed to train the model.  \nâ— Experiments are conducted on three benchmark HPE \ndatasets. Results show TokenHPE achieves state -of-the-\nart perfo rmance with a novel token -learning concept \ncompared with its existing CNN -based counterparts. \nBesides, we conduct abundant visualizations to illustrate \nthe effectiveness of the proposed orientation tokens.  \nThe remainder of this article is organized as follows. In \nSection II, we review the head pose estimation -related works. \nSection III introduces the proposed model for head pose angle \ninference and experimental results are provided in Section IV. \nIn Section V, we conclude this study. \nII. RELATED WORKS \nA. Head Pose Estimation \nGenerally, the traditional HPE models can be classified into \nthree kinds, such as Euler angle regression (EAR)-based models \n[19, 29, 31, 32] , extra information-utilized (EIU) models [20, \n21, 33 -35], and alternative parametrizations of orientation  \n(APO) model [23-26]. For the EAR-based head pose estimation \nmethod, three Euler angles need to be regressed progressively. \nThe paradigm in early studies was to consider HPE as a \nregression problem [3, 19, 29, 36]. Abate et al. [36] proposed a \nweb-shaped model algorithm to encode the pose of the face and \nthen appl ied regression algorithms to predict the pose of the \nface. Recently, CNNs have been adopted for HPE and remained \ndominant for many years because convolution can efficiently \nreveal the visual patterns on human faces. Ruiz et al. [29] was \nthe first to propose an end-to-end method which independently \npredicts three Euler angles by using a multi-loss network based \nCNN. In [19], Yang et al.  proposed FSA -Net, a novel \narchitecture that consists of progressive stage fusions and fine-\ngrained spatial structures. The spatial informatio n can be  \npreliminarily learned by setting a learnable or fixed importance \nover the spatial location. However, becaues of the incapacity of \nCNN to learn the relationships among visual patterns, further \nfacial part relationships are not explored in this category. \nFor the EIU approaches, extra facial information is exploited \nto facilitate angle estimation. With graph neural network (GCN) \nbeing generalized to various natural language processing (NLP) \nand computer vision tasks [37-41], Xin et al. [33] proposed a \nGCN-based method which learns through the facial landmark \ngraph. However, the precision of the model depends largely on \nthe precision of the additional landmark detector. Kazemi et al. \n[12] proposed a general framework based on gradient boosting \nfor learning an ensemble of regression trees that optimizes the \nsum of square error loss and naturally handles missing or \npartially labelled landmarks. Wu et al. [20] proposed a multi -\ntask approach named SynergyNet t hat predicts complete 3D \nfacial geometry. Through synergistic learning of 3D landmarks \nand 3D morphable models (3DMM) parameters, improved \nperformance is achieved by the collaborative contribution. In \nthese methods, facial part relationships can be learned  from \nlandmarks or other extra information. However, many manual \nannotations are required for training, which is laborious and \ninefficient. \nFor the APO models, Euler angles representation are usually \nsubstituted with with other representations. Most contributions \nto HPE in recent years have focused on alternative \nparametrizations of head orientation because traditional Euler \nangle labels inevitably have some problems at specific \norientations. Geng et al.  [26] proposed a multivariate label \ndistribution as a substitute of Euler angles. In this manner , \ninaccutate manual annnotation can be alleviated and the \noriginal label is softened, making the training easy. In [25], a \nvector-based head pose representation is proposed which  \nhandles the issue of discontinuity of Euler angl e annnotation. \nRecently, Hepel et al.  [24] proposed a rotation matrix -based \nrepresentation for HPE. In this way, the ambiguity problem was \nperfectly resolved by full pose regression  based on  rotation \nmatrices. In our previous work [22], the head orientations are \nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n  \n \nrepresented by matrix Fisher distribution  based on rotation \nmatrix, which greatly avoids the ambiguity problem of Euler \nangle labels. Furthermore, in [27], the characteristics of head \npose image varitations in different directions are revealed and \nleveraged by constructing the anisotropic angle distributions. \nAlthough these methods have achieved impressive results, the \nintrinsic facial and orientational relationships are not fully \nexploited. \nB. Vision Transformer and Its Applications \nViT is a variant model of Transformer [42], which is \noriginally u tilized in NLP. In ViT model, an input image is \ndivided into patches and projected into 1D vectors called tokens. \nThese visual patches can be viewed as words. In addition, a \nlearnable class token is concatenated similarly to the original \nTransformer. The success of ViT quickly focused researchersâ€™ \nattention to applying the Transformer architecture in various \nvision tasks, including fine -grained classification [43-45], \nobject detection [46], facial expression recognition [47], human \npose estimation [48], and image segmentation [49]. Li et al. [48] \nproposed the utilization of learnable tokens to represent each \nhuman keypoint entity  on the  basis of prior knowledge . \nThrough this sensible token construction , constraint cues and \nviusal cues are expicitly learned and incorporated through the \nTransformer architecture.  Dhingra [32] preliminarily utilized \nTransformer encoder after a CNN backbone for HPE. However, \nTransformerâ€™s ability to learn  the semantic relationships was \nnot fully exploited. In [50], Cordonnier et al.  provided a \ntheoretical explanation of the long-distance information learned \nin Transformer. Based on their theoretical foundation, we \nbelieve tha t the intra - and cross -orientation relationship \nrevealed in this work can be learned based on Transformer \narchitecture. Specifically, the intra orientation relationship can \nbe learn ed by attention mechanism , and cross-orientation \ninformation can be encoded into learnable orientation tokens. \nIII. PROPOSED TOKENHPE MODEL \nIn this section, the overview of the proposed TokenHPE is \nprovided first. Second, the details of the four parts of the model \nare elaborated. Lastly, the supplementary architecture details of \nthe TokenHPE are provided. \nA. Architecture Overview \nOur methodâ€™s overview is show n in Fig. 3. The TokenHPE \nmodel comprises four parts. The first one is v isual token \nconstruction, where the input image is transformed into visual \ntokens through multiple appr oaches. The second part is \norientation token construction. We provide two strategies to \nconstruct orientation tokens based on our finding on head image \npanoramic overview. The third part is the Transformer module, \nwherein the relationships of facial parts and orientation \ncharacteristics in the basic regions are learned by the \nTransformer mechanism. The fourth part i s t oken learning-\nbased prediction. A novel token guide multi -loss that can help \nthe orientation tokens encode general information is also \nintroduced in this part. \n \nFig. 3. Pipeline of our TokenHPE model.  \n \nB. Visual Token Construction \nIn this part, an original input RGB image is transformed into \nvisual tokens. We provide three options to obtain the visual \ntokens: by patch division of the origi nal image (Option 1), by  \nextracting feature maps from a CNN  (Option 2), and by  \nselecting the tokens from a ViT backbone  (Option 3) [30]. For \nOption 1, suppose we have an input image ğ¼ of size ğ»Ã—ğ‘ŠÃ—\nğ¶. The image is divided into patches with patch size ğ‘ƒâ„ Ã—ğ‘ƒğ‘¤. \neach patch is subsequently resized into a 1 -dimensional vector \nof size ğ‘ƒâ„ Ã—ğ‘ƒğ‘¤ Ã—ğ¶. Linear projection is applied  to obtain a \nvisual token. This operation is expressed as: \nğ‘‚:ğ‘â†’ğ‘ âˆˆâ„ğ‘‘, (1) \nwhere ğ‘ refers to a 1D patch vector and ğ‘  is a visual token with \nFeature \nExtractorOrientation tokens\n1 2 3\n k\nMLP head\nPredicted rotation matrix\nTransformer blocks\nVisual tokens\nOrientation tokens\nPositional embedding\nLinear projection\nRotation matrix\nFeature \nmaps\nAttention map between orientation tokens\nHeatmap visualization between visual tokens\nBlock 1 Block M\nBlock 1 Block M\nLayer Norm\nMulti-head\nSelf Attention\nLayer Norm\nLinear Projection\nTanh(â€¢)\nLinear Projection\nGroundtruth rotation matrix \nVisual tokens\nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \na dimension of ğ‘‘. For Option 2, the output of the CNN extractor \nis a set of feature maps with a size of ğ»Ã—ğ‘ŠÃ—ğ¶â€². The \nremaining operation s are similar to th ose in  Option 1. For \nOption 3, the visual tokens can be simply selected from the \noutput of a Transformer backbone. \nGiven that  spatial relationships are essential for accurate \nHPE, positional embedding, ğ‘ğ‘œğ‘ , is added to the visual tokens  \nto reserve spatial relationships, which can be expressed as: \n[ğ‘£ğ‘–ğ‘ ğ‘¢ğ‘ğ‘™]={ğ‘ 1 +ğ‘ğ‘œğ‘ ,ğ‘ 2 +ğ‘ğ‘œğ‘ ,â‹¯,ğ‘ ğ‘› +ğ‘ğ‘œğ‘ }, (2) \nwhere ğ‘› is the number of patches. Then, we obtain n 1D vectors \nsymbolically presented by [visual] tokens. \nC. Orientation Token Construction \nBasic orientation region partition ing. The cross-\norientation relationship information is encoded into learnable \norientation tokens. To construct the orientation tokens, the  \npanoramic overview is divided into several basic orientation \nregions. Within a spec ific orientation region, the orientations \nhave high similarities on head pose characteristics.  \nThe significant facial part change angle threshold can be \nobserved by calculating the cosine similarities between the \nfeature maps generated from the feature ex tractor in different \nhead pose images. As shown in Fig. 4, the cosine similarity is \nrelatively lower when significant facial part change  happens, \nsuch as the appearance/disappearance of eye on one side and \nappearance/disappearance of ear. For example, when  the pitch \nangle is constantly at 0Â° and the yaw angle moves from âˆ’90Â° to \n0Â°, two main significant facial part change s are marked by \nvariation on cosine similarities from 84.67% to 87.78% and \nthen to 83.54%. Therefore, three basic orientation regions on \nthe left central part can be defined according to the discontinuity \non neighborhood cosine similarities. Following this rule, the \nremaining basic orientation regions can be easily defined.  \n \nFig. 4. Illustration of significant facial part change on neighbored orientations \nmeasured by cosine similarity scores, which are denoted as â€œSâ€. \n \nBased on quantitative results on cosine similarities of \nneighborhood head pose images, we introduce two partitioning \nstrategies, as  shown in Fig.  5. In Strategy I, the panoramic \noverview is divided into nine  basic orientation regions \naccording to the appearance of the eyes and the overlapping of \nthe nose and mouth. In yaw direction, we set 60Â° and âˆ’60Â° as \nthe division degree because of the appearance (or disappearance) \nof eyes. In pitch direction, we set 30Â° and âˆ’30Â° as the division \ndegree because of the appearance (or disappearance) of the \nnostril and the overlapping of nose and mouth. As such, the nine \nbasic orientation regions in strategy I are: (0) upper left, (1) top, \n(2) upper right, (3) middle left, (4) middle, (5) middle right, (6) \nbottom left, (7) bottom, and (8) Bottom right. As depicted in the \nleft part of Fig. 5, head poses in the same region are similar, and \nthe opposite head poses are symmetric.  In Strategy II , the \npanoramic overview is divided into 11 regions , with a fine -\ngrained partition in the yaw direction.  we divide the yaw \ndirection in a finer-granularity because the significant f acial \npart changes are complex when pitch angle is little. As shown \nin the right part of Fig. 5, in this partition strategy, the middle \narea of the panoramic overview is divided into five basic \nregions. The division degree is  set as  60Â° because of the \ncomplete disappearance of eye. We set 20Â° as the other division \ndegree for the start of the disappearance of eye. Therefore, when \nthe pitch angle is within âˆ’30Â° and 30Â° , the basic orientation \nregions are as follows: (3) middle left 1, (4) middle left 2, (5) \nmiddle, (6) middle right 1, and (7) middle right 2. \n \nFig. 5. Construction of orientation tokens. We discover that the head pose \npanoramic overview can be roughly divided into several basic orientation \nregions according the neighbor image similarities. As the division granularity \nvaries, the number of basic orientation regions also varies. \n \nOrientation token. After the quantitative analysis on Basic \norientation region partitioning , we construct the same number \nof d dimensional learnable vectors  to represent k basic \norientation regions . These vectors are symbolized as [dir] \ntokens. Then, the [visual] tokens are concatenated with the [dir] \ntokens as the input of Transformer  blocks. After that , the \nprocessed [dir] tokens are chosen as the output of Transformer. \nD. Transformer Blocks \nInputted with the [visual] and [dir] tokens, the Transformer \nblocks learn s the relationships  among facial parts and head \norientations. The Transformer is constructed  by stacking M \nidentical unit blocks. Each block comprises a multi-head self-\nattention (MSA) module and a multi-layer perception (MLP) \nmodule, with a layer norm (LN) operation and skip connection \nadded between the two modules. Self-attention (SA) is defined \nas: \nğ‘†ğ´(ğ‘…ğ‘¡)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘…ğ‘¡ğ‘Šğ‘„(ğ‘…ğ‘¡ğ‘Šğ¾)ğ‘‡\nâˆšğœƒ\n)(ğ‘…ğ‘¡ğ‘Šğ‘‰), (3) \nwhere ğ‘Šğ‘„,ğ‘Šğ¾,and ğ‘Šğ‘‰ âˆˆâ„ğ‘‘Ã—ğ‘‘ represent the query matrix, the \nkey matrix, and the value matrix. ğ‘…ğ‘¡ is the output of the t-th \nTransformer layer. Î¸ is part of the scaling factor 1 âˆšğœƒâ„ . In SA, \ns equals the dimension d of the tokens. MSA is an extension of \n-90 -75 -60 -45 -30 -15 0 15 30 45 60 75 90\n0\n15\n30\n60\n90\n-15\n-30\n-60\n-90\nS=75.80%\nS=82.36%\nS=87.78%\nS=82.52%\nS=86.80%\nS=84.67% S=83.56%S=81.18%\nPitch /Degree (Â°)\nYaw/Degree (Â°)\nS=80.39%S=83.54%\nStrategy I: Nine basic orientation regions Strategy II: Eleven basic orientation regions\n0 1 2 3 4 5 6 7 8 0 1 2 4 5 6 8 9 10\nNine orientation tokens\n3 7\nEleven orientation tokens\nMiddle\nUpper left\nBottom left\nTop\nUpper right\nMiddle left\nMiddle right\nBottom\nBottom right\nBottom left\nBottom right\nUpper left\nTop\nUpper right\nMiddle left 1\nMiddle left 2\nMiddle\nMiddle right 1\nMiddle right 2\nBottom\nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nSA with h self-attention operations, which are named heads. In \nMSA, Î¸ is typically set as ğ‘‘ â„â„ . Thus, MSA can be formulated \nas: \nğ‘€ğ‘†ğ´(ğ‘…ğ‘¡)=[ğ‘†ğ´1(ğ‘…ğ‘¡);ğ‘†ğ´2(ğ‘…ğ‘¡);â‹¯;ğ‘†ğ´â„(ğ‘…ğ‘¡)]ğ‘Šğ‘ƒ, (4) \nwhere ğ‘Šğ‘ƒ âˆˆâ„(â„âˆ™ğ‘ )Ã—ğ‘‘. After defining MSA, the operations of a \nTransformer block can be expressed as: \nğ‘…Ìƒğ‘¡âˆ’1 =ğ‘€ğ‘†ğ´(ğ¿ğ‘(ğ‘…ğ‘¡âˆ’1))+ğ‘…ğ‘¡âˆ’1, (5) \nğ‘…ğ‘¡ =ğ‘€ğ¿ğ‘ƒ(ğ¿ğ‘(ğ‘…Ìƒğ‘¡âˆ’1))+ğ‘…Ìƒğ‘¡âˆ’1. (6) \nThe MLP module is constructed by two linear projections, \nwith a Tanh(â€¢) activation function and dropout operations in \nbetween. \nAfter the last Transformer layer, the [dir] tokens are selected \nas the output of Transformer , whereas the [visual] tokens are \nnot used in the following steps. Therefore, the output of M \nTransformer blocks is denoted as {ğ‘…1\nğ‘€,ğ‘…2\nğ‘€,â‹¯,ğ‘…ğ‘˜\nğ‘€}, where k is \nthe number of [dir] tokens. \nE. Token Learning-based Prediction \nSuppose a set of orientation tokens outputted by part three in \nour model is denoted as {ğ‘…1\nğ‘€,ğ‘…2\nğ‘€,â‹¯,ğ‘…ğ‘˜\nğ‘€}, where k is the \nnumber of orientation tokens. The orientation tokens need to be \ntransformed to rotation matrices for training and prediction. We \nadopt sim ilar transformation strategy as used in [24]. The \ntransformation is elaborated as follows.  \nFirst, a linear projection is applied to ğ‘…ğ‘–\nğ‘€ to obtain a 6D \nrepresentation of head pose. Next, the Gram â€“Schmidt process \nis applied to generate the 9D rotation matrix. This \ntransformation is formulated as: \nğ´Ì‚ğ‘– =ğ¹ğºğ‘†(ğ‘Šğ‘…ğ‘–\nğ‘€), (7) \nwhere W is a linear projection matrix, and ğ´Ì‚ğ‘– is the predicted \nrotation angle matrix of the i-th basic orientation region. ğ¹ğºğ‘†(â€¢) \ndenotes the Gramâ€“Schmidt process that can be expressed as: \nğ¹ğºğ‘†(ğ‘1,ğ‘2)=[ğ‘1 ğ‘2 ğ‘3], (8) \nwhere ğ‘1,ğ‘2 âˆˆâ„3 are 3D column ve ctors of a rotation matrix. \nğ‘ğ‘– is 3D column vector of the rotation matrix defined as: \n{\n \n \n \n ğ‘1 = ğ‘1\nâ€–ğ‘1â€–,                    \nğ‘¢2 =ğ‘2 âˆ’(ğ‘1 âˆ™ğ‘2)ğ‘1,\nğ‘2 = ğ‘¢2\nâ€–ğ‘¢2â€–,                    \nğ‘3 =ğ‘1 Ã—ğ‘2.                \n(9) \nA set of rotation matrices {ğ´Ì‚1,ğ´Ì‚2,â‹¯,ğ´Ì‚ğ‘˜} can be generated \nby the transformation above, where k is the number of \norientation tokens. \nTo obtain the final prediction rotation matrix, \n{ğ´Ì‚1,ğ´2,â‹¯,ğ´Ì‚ğ‘˜} is concatenated and flattened as the input of the \nMLP head, which can be formulated as: \nğ´Ì‚=ğ¹ğºğ‘†(ğ‘Š2(tanh(ğ‘Š1 âˆ™ğ´Ìƒ+ğ‘1))+ğ‘2), (10) \nwhere ğ´Ìƒâˆˆâ„9âˆ™ğ‘˜ is a vector of flattened rotation matrices. ğ‘Šğ‘– \nand ğ‘ğ‘– are the weight matrix and bias vector of the MLP module, \nrespectively. In the training stage, the intermediate rotation \nmatrices and the final prediction rotation matrix are used for \ncalculating the loss for back propagation. In the prediction stage, \nonly the prediction rotation matrix ğ´Ì‚ is outputted as the model \nprediction. \nF. Total Loss Function \nThe prediction of the proposed TokenHPE is a rotation \nmatrix representation denoted as ğ´Ì‚ . Suppose that the \ngroundtruth rotation matrix is A. The geodesic distance is used \nas the loss between two 3D rotations, similar to that used in [24]. \nThe geodesic distance loss is formulated as: \nğ¿ğ‘”(ğ´,ğ´Ì‚)=cosâˆ’1(ğ‘¡ğ‘Ÿ(ğ´ğ´Ì‚ğ‘‡)âˆ’1\n2 ). (11) \nOrientation token loss. Information can be encoded into the \norientation tokens through the orientation token loss. It is \ndefined as a mean squared error, which is formulated as: \nğ¿ğ‘‚ğ‘Ÿğ‘– =âˆ‘ğ•€(ğ´,ğ‘–)âˆ™ğ¿ğ‘”(ğ´,ğ´Ì‚ğ‘–)\nğ‘˜\nğ‘–=1\n, (12) \nwhere ğ‘˜ is the number of basic orientation regions, ğ´ is the \nground truth rotation matrix, ğ´Ì‚ğ‘– is the predicted rotation matrix, \nand ğ•€(ğ´,ğ‘–) is an identity function that determines if a ground \ntruth head p ose lies in the i-th basic region. ğ•€(ğ´,ğ‘–) can be \nwritten as: \nğ•€(ğ´,ğ‘–)={1, ğ‘–ğ‘“ ğ´ ğ‘–ğ‘› ğ‘Ÿğ‘’ğ‘”ğ‘–ğ‘œğ‘› ğ‘–,\n0, ğ‘–ğ‘“ ğ´ ğ‘›ğ‘œğ‘¡ ğ‘–ğ‘› ğ‘Ÿğ‘’ğ‘”ğ‘–ğ‘œğ‘› ğ‘–. (13) \nPrediction loss. The predictions from the orientation tokens \nare aggregated to form the  final prediction of our model. This \nis optimized by the prediction loss, which is formulated as: \nğ¿ğ‘ğ‘Ÿğ‘’ğ‘‘ =ğ¿ğ‘”(ğ´,ğ´Ì‚), (14) \nwhere ğ´Ì‚ is the prediction of the model. \nOverall loss. The overall loss consists of the orientation \ntoken loss and the prediction loss. It is formulated as: \nğ¿ğ‘œğ‘£ğ‘’ğ‘Ÿğ‘ğ‘™ğ‘™ =ğ›¾ğ¿ğ‘ğ‘Ÿğ‘’ğ‘‘ +(1âˆ’ğ›¾)ğ¿ğ‘‚ğ‘Ÿğ‘–, (15) \nwhere ğ›¾ is a hyperparameter that balances prediction loss and \norientation token loss. \nG. Network Parameters \nTo obtain the visual tokens, three options aforementioned \npreviously can be utilized. A few extra CNN layers can \nefficiently extract low-level superficial features. In our different \nversions, a feature extractor is added, or the raw image patches \nare manipulated directly. In the version added with a feature  \nextractor, many low-level features are utilized for prediction. In \nOption 2, we adopt the widely used stem-net, which can quickly \ndownsample the feature map into 1/4 input resolution in a very \nshallow convolutional structure. In Option 3, we adopt ViT -\nB/16 as the feature extractor for a tradeoff between model size \nand performance. The outputs of ViT are the visual tokens that \ncan be directly used in the second part of the proposed model.  \nOption 3 is set by default in our TokenHPE model if not \nspecially mentioned. \nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nIV. EXPERIMENTAL RESULTS AND DISCUSSION \nA. Generally Setting \n1) Evaluation Metrics: Two common evaluation metrics are \nselected to validate the performance of the comparing methods. \nIt includes the mean absolute errors of Euler angles (MAE), and \nmean absolute errors of vectors (MAEV). For the MAE, it is \nusually assumed that pose angles are known. Namely, the Euler \nangles {ğ‘¦ğ‘ğ‘¤,ğ‘ğ‘–ğ‘¡ğ‘â„,ğ‘Ÿğ‘œğ‘™ğ‘™} of an image  are considered as the \nground-truth. The symbols ğ‘¦ğ‘ğ‘¤,ğ‘ğ‘–ğ‘¡ğ‘â„,and ğ‘Ÿğ‘œğ‘™ğ‘™  represent \npitch, yaw, and roll angle , respectively. The predicted set of \nEuler angles from a model is denoted as  {ğ‘¦ğ‘ğ‘¤Ì‚,ğ‘ğ‘–ğ‘¡ğ‘â„Ì‚,ğ‘Ÿğ‘œğ‘™ğ‘™Ì‚ }. \nThen, MAE is defined as: \nğ‘€ğ´ğ¸=1\n3(|ğ‘¦ğ‘ğ‘¤âˆ’ğ‘¦ğ‘ğ‘¤Ì‚|+|ğ‘ğ‘–ğ‘¡ğ‘â„âˆ’ğ‘ğ‘–ğ‘¡ğ‘â„Ì‚|+|ğ‘Ÿğ‘œğ‘™ğ‘™âˆ’ğ‘Ÿğ‘œğ‘™ğ‘™Ì‚ |).(16) \nWe adopt M AE as an evaluation metric. However, because \nthis metric is unreliable, the MAEV results are given at the same \ntime for a more accurate measurement of the models. \nFor the MAEV, it is usually based on rotation matrix \nrepresentation. For an image, suppose  that the groundtruth \nrotation angle matrix is ğ´=[ğ‘1,ğ‘2,ğ‘3], where ğ‘ğ‘– is a 3D \nvector that indicates a spatial direction. The predicted rotation \nmatrix from a model is denoted as ğ´Ì‚=[ğ‘Ì‚1,ğ‘Ì‚2,ğ‘Ì‚3]. MAEV can \nbe formulated as: \nğ‘€ğ´ğ¸ğ‘‰=1\n3âˆ‘â€–ğ‘ğ‘– âˆ’ğ‘Ì‚ğ‘–â€–1\n3\nğ‘–=1\n. (17) \n2) Datasets: Three datasets are employed in our experiments \nas listed below. \na) BIWI dataset  [51]: It  includes 15, 678 images of 20 \nindividuals (14 individuals are males and the rest are \nfemales) with 4 of whom recorded twice ., a RGB-depth \nimage (640Ã—480 pixels) , and the corresponding head \npose annotation are recorded for each video frame. The \nhead pose range covers about Â± 60Â°  pitch and Â± 75Â°  yaw. \nThe 3D location of the head and its Euler angle  are \nprovided as the ground truth labels of each frame. \nb) AFLW2000 dataset [52]: It contains 2000 images  that \nhave been annotated with 68 -point 3D facial landmarks  \nat image -level. The dataset is typically adopted as the \nevaluation benchmark of 3D facial landmark detection \ntask. The head poses in this dataset are diverse and \nalways difficult to be detected by traditional CNN-based \nface detectors. Notice that the 2D landmark annotations \nare discarded in the dataset because some of the data do \nnot have complete landmark points, as mentioned in the \noriginal paper. \nc) 300W-LP dataset [52]: It  is an expanded version of  \n300W dataset, which collects multiple alignment \ndatabases with 68 landmarks, including IBUG, \nXM2VTS, LFPW, AFW, and  HELEN. With 300W  \ndataset, 300W-LP adopts the proposed face profiling to \ngenerate about 61k samples across large poses. The \ndataset is usually employed as the training set for HPE. \nB. Training Details \nTraining. In our experiments, the TokenHPE is trained end-\nto-end. The batch size is set as 64, and ğ›¾ is set to 0.65 by default. \nWe train the proposed TokenHPE model for 120 epochs. The \nlearning rate is initialized as 0.000 1, which is further decayed \nby a factor of 10 at the 30th and 60th epochs. \nInitialization. All the images are resized into 240Ã—240 pixels. \nA random crop is then applied to make the input image size \n224Ã—224 pixels. Our method is implemented with the Pytorch \ntoolbox with a single TITAN V GPU. All the parameters in our \nmodel are trained with random initialization. \nComputational time: The training time is about six hours on \nGPU. In the inference stage , our model can inference in real \ntime on GPU at over 400 fps. When ran on CPU, the inference \nspeed is about 10 fps. \nC. Compare to State-of-the-art \nWe compare our proposed TokenHPE with 13 state-of-the-\nart (SOTA) methods, including Euler angle regression methods \n(HopeNet, FSA-Net, FAN)), extra information -utilized \nmethods (3DDFA, Dlib, EVA -GCN, img2pose, SynergyNet), \nand alternative parametrizations of orientation  methods \n(QuatNet, TriNet, 6DRep Net). In our two experiments, we \nfollow the convention by FSA -Net [19]. We conduct \nexperiments on two versions of our model: TokenHPE -N with \nnine basic orientation regions and TokenHPE -E with eleven \nbasic orientation regions.  TokenHPE-E is our standard model  \nreferred as TokenHPE. \nFirstly, we follow the conventional protocol 1 [19], in which \nthe models are all trained on the 300W-LP dataset and tested on \nAFLW2000 and BIWI datasets. Tables I and II show the results \nof the first experiment. An extra column is added to indicate \nwhich methods are free from extra annotation for fair \ncomparison. Results show that the proposed TokenHPE  is on \npar with SOTA methods on AFLW2000 dataset and achieves \nSOTA results on MAEV on BIWI dataset. Among the \ncompared methods, HopeNet [29] is normally considered the \nbaseline of HPE.  Compared with it in Table I, our model \nachieves a 24.8% decrease in MAE and a 12.7% decrease in \nMAEV, which shows the high accuracy of our method. Xia et \nal.[28] proposed a method that applies an affine transformation \nto simplify the input and combines landmarks information into \na CNN feature extractor, leading to 4.20% improvement from \nbaseline. TriNet [25] is a vector-based model, in which the head \npose is represented by vectors instead of Euler angles to solve \nthe discontinuity problem. The MAE is 0.69 lower than the \nbaseline. A new MAEV metric is also introduced. We adopt this \nmetric for our comparison. Compared with TriNet, our method \nobtains a lower MAEV value, which indicates that the proposed \nrelationship-learning approach  has the potential to achieve \nSOTA performance. Some extra information -utilized methods \n(e.g., SynergyNet, img2pose) are also compared in Table I. \nFAN, Dlib, SynergyNet and img2pose, which are better known \nto perform landmarks prediction, are not specially designed for \nHPE but can be readily modified for HPE as a downstream task. \nEVA-GCN [33] is a facial landmark graph -based method , \nwhich takes the detected landmark graph as the input The GCN \ncan learn the landmark relationships for HPE  thus the model \nresult has an impressive improvement. SynergyNet is a multi-\nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \ntask model , and  HPE is a subtask. The model is trained by \nsynergistic learning. Therefore, abundant information, \nincluding 3DMM parameters and 3D landmarks, is utilized to \nenhance the p erformance. Img2pose [21] achieves the best \nresult among annotation utilized methods by applying the six \ndegress of freedom 3D face pose estimation. HeadPosr achieves \nexcellent performance by introducing a Transformer encoder to \na CNN backbone , which is buil t on the CNN learning \nmethodology. In general, c ompared to other methods that \nmainly based on CNN and  its variants, our model is the only \nTransformer-based token learning method, thus has a s tronger \nability to learn the facial relationships and the orientation \ncharacteristics in the basic regions.  Therefore, even on the \nchallenging AFLW2000 dataset that has many difficult-to-\npredict images, our method still outperforms the majority of the \ncompared methods by a large margin. The excellent \nperformance verifies the orientation learning capacity of the \nproposed TokenHPE. \nAfterward, the authors follow the protocol 2 in [19], in which \nthe model performace is evaluated on the BIWI dataset  alone \nwith a  ramdom 7:3 separation for training and testing. \nExperiments are conducted on the TokenHPE and the compared \nSOTA methods with protocol 2. R esults shown in Table III \ndemostates the proposed  TokenHPE outperforms other \nmethods by a large margin both on MAE and three Euler angles. \n6DRepNet [24] uses the rotation matrix representation with a \nCNN backbone. Compared to 6DRepNet, our TokenHPE can \nlearn the general regional informat ion and facail relationships \nthrough Transformer architecture, resulting in a 6.39% drop on \nMAE. The similar results on two experiments show that our \nmethod is robust and stable, and its impressive performance is \nindependent from the training and testing datasets. \nD. Ablation Study \nIn this section, we conduct ablation study on different \nsegment of our TokenHPE model. The models are trained on \n300W-LP dataset and tested on AFLW2000 dataset by default \nif there is no explicit declaration. \n1) On the token guide multi -loss function . The proposed \nmodel is trained by a token guide multi -loss function. We \nconduct ablation study on the orientation to ken loss and \nprediction loss, which is controlled by Î³.  When ğ›¾ is set to 1 .0, \nthe model is trained solely on the prediction loss, meaning the \nmodel learns the basic orientation regions by itself. As the value \nof ğ›¾ decreases, orientation token loss plays an increasingly \nimportant role in helping the model learn the orientation \ninformation. Since the final prediction head an indispensable \npart in our method, the Î³ only can be set to a very small value \nnear zero but cannot be zero. Preliminarily, we remove each \ncomponent of the multi-loss individually and evaluate the \nimportance. In setting I, s ince the prediction loss is \nindispensable, we set it to a very small weight (5%) thus the \norientation loss is predominant.  \n \nTABLE I: COMPARISON WITH SOTA METHODS ON THE AFLW2000 DATASET WITH PROTOCOL 1.  \n \nTABLE II: COMPARISON SOTA METHODS ON THE BIWI DATASET WITH PROTOCOL 1.  \n \n \n \n \n \n \n \n \n \n \n \nHPE Models \nw/o \nAnnotation  \ninformation \nVector errors Prediction errors (Â°) \nFront Down Left MAEV Roll Yaw Pitch MAE \n3DDFA [52] No 18.52 39.05 30.57 29.38 28.43 4.71 27.05 20.08 \nDlib [12] No 14.31 28.51 26.56 23.13 22.83 8.50 11.25 14.19 \nFAN [13] No - - - - 8.71 6.36 12.3 9.12 \nEVA-GCN [33] No - - - - 4.11 4.46 5.34 4.64 \nSynergyNet [20] No - - - - 2.55 3.42 4.09 3.35 \nimg2pose [21] No - - - - 3.28 3.43 5.03 3.91 \nHopeNet [29] Yes 7.50 5.98 7.07 6.85 6.13 5.31 7.12 6.20 \nXia et al. [28] Yes - - - - 6.50 3.99 7.32 5.94 \nFSA-Net [19] Yes 7.35 6.22 6.75 6.77 4.78 4.96 6.34 5.36 \nLwPosr [31] Yes - - - - 4.88 4.80 6.38 5.35 \nHeadPosr EH64 [32] Yes - - - - 4.30 4.64 5.84 4.92 \nQuatNet [23] Yes - - - - 3.92 3.97 5.62 4.50 \nTriNet [25] Yes 6.52 5.67 5.78 5.99 4.04 4.20 5.77 4.67 \nTokenHPE-N (ours) Yes 6.97 5.21 6.16 6.11 4.29 4.53 5.73 4.85 \nTokenHPE-E (ours) Yes 6.82 5.10 6.01 5.98 4.08 4.36 5.54 4.66 \nHPE models \nw/o \nAnnotation  \ninformation \nVector errors Prediction errors (Â°) \nFront Down Left MAEV Roll Yaw Pitch MAE \nHopeNet [29] Yes 8.68 6.73 7.65 7.69 3.72 6.01 5.89 5.20 \nFSA-Net [19] Yes 7.22 5.96 6.03 6.40 3.07 4.56 5.21 4.28 \nQuatNet [23] Yes - - - - 2.94 4.01 5.49 4.15 \nTriNet [25] Yes 6.57 5.46 5.57 5.86 4.11 3.05 4.76 3.97 \nEVA-GCN [33] No - - - - 2.98 4.01 4.78 3.92 \nHeadPosr EH64 [32] Yes - - - - 2.69 3.37 5.44 3.83 \nWHENet [53] Yes - - - - 3.06 3.99 4.39 3.81 \nTokenHPE-E (ours) Yes 6.23 5.17 5.41 5.60 2.71 3.95 4.51 3.72 \nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \n \nTABLE III. COMPARISON WITH SOTA METHODS ON BIWI DATASET WITH \nPROTOCOL 2.  \n \nTABLE IV.  \nABLATION STUDY ON THE TOKEN GUIDE MULTI-LOSS FUNCTION.  \nSetting ğ¿ğ‘ğ‘Ÿğ‘’ğ‘‘ ğ¿ğ‘œğ‘Ÿğ‘– #Regions=9 #Regions=11 \nMAE âˆ† MAE âˆ† \nAppr. only ğ¿ğ‘œğ‘Ÿğ‘–  ï 5.33 -9.90% 5.26 -12.88% \nOnly ğ¿ğ‘ğ‘Ÿğ‘’ğ‘‘ ï  4.99 -2.89% 4.87 -4.51% \nJoint ï ï 4.85 - 4.66 - \n \nTABLE V.  \nEFFECT OF ORIENTATION TOKENS. \n#[dir] \nTokens \nGuidance of \norientation \ninformation \nPrediction errors (Â°) \nMAEV Roll Yaw Pitch MAE \nNone No 4.44 5.01 5.79 5.08 6.48 \n9 No 4.38 4.97 5.61 4.99 6.36 \n9 Yes 4.29 4.53 5.73 4.85 6.11 \n11 No 4.33 4.63 5.67 4.87 6.14 \n11 Yes 4.08 4.36 5.54 4.66 5.98 \n \nTABLE VI.  \nEFFECT OF THE FEATURE EXTRACTOR. \nFeature extractor Prediction errors (Â°) MAEV Roll Yaw Pitch MAE \nNone 5.11 4.96 6.07 5.38 6.65 \nCNN 4.48 5.71 4.68 4.96 6.04 \nViT 4.08 4.36 5.54 4.66 5.98 \nResults presented in Table IV shows that bot h sub-losses are \nsignificant for model performance.  Furthermore, we set a \nsequence of ğ›¾ values for a thorough investigation of the \ncontribution of two sub -loss to the model performance. The \nexperimental results are shown in Fig. 6. When ğ›¾ decreases, \nMAE initially decreases then increases. The best result is \nobtained when ğ›¾ is set to 0.6.  This situation indicates that the \ntoken guide loss indeed helps the model encode the basic \norientation regions. As ğ›¾ decreases, the flexibility of the model \nis constrained, resulting in poor performance.  When Î³ is set to \n0.6, where prediction loss and or ientation token loss jointly \ncontribute to the overall loss, the model reaches the best \nperformance. \n2) On the orientation tokens. Since the number of orientation \ntokens is derived from the partitioning strategies which are \nrigorously defined, we evaluate  their effectiveness on three \nsettings. In the first setting, we removed all orientation tokens \nand leave a learnable token similar to the [cls] token in ViT for \nprediction. In the second and third settings, we use nine tokens \n(Strategy I) and eleven tokens (Strategy II), respectively. When \nthere is no guidance of orientation information, the model is \ntrained only with the prediction loss thus the reginal orientation \ncharacters are not learned in this scheme. Results in Table V \nshow that nine orientation tokens and eleven tokens bring 5.7% \nand 7.7% improvement on the MAEV, 4.5% and 8.2% \nimprovement on MAE compared to the version that has no \norientation token. Besides, the guidance of orientation \ninformation brings 2.8% and 4.3% improvement on MAE in \nnine and eleven token settings compared to the control groups \nthat don not have the orientation information from the \norientation loss. Overall, the quantitative results verify the \neffectiveness of the proposed token guide multi -loss and the \ncontribution of orientation tokens. \n3) Feature extractor . Since the visual tokens are generated \nfrom the feature extractor, the performance of the model \npartially depends on it. Therefore, we conduct experiments on \nCNN and ViT  feature extractors to reveal the extent to which \nperformance is affected by the feature extractor. As shown in \nTable VI, we test three versions with  or without  feature \nextractors. Results show that feature extractor improves \nperformance to a specific extent compared with the version \nwithout a  feature extrac tor. The model with ViT feature \nextractor has the best performance.  This mainly contribute s to \nthe better capability of Transformer to encode semantic visual \ninformation than CNNs. \nE. Discussion \n1) Positional embedding. Different from classification tasks, \nspatial relationships play an important role in HPE. Given that \nthe self-attention operation is positionally invariant, normally, \n2D sine positional embedding is added to reserve the spatial \nrelationships for computer vision tasks. Therefore, we conduct \nexperiments on our TokenHPE model with three positional \nembedding types (i.e., no positional embedding, learnable \npositional embedding, and 2D sine positional embedding)  to \ninvestigate the effect of positional embedding . As shown in \nTable VII, the model wit h 2D sine positional embedding \ndemonstrates the best performance. The learnable positional \nembedding version has a low prediction accuracy. The model \nwithout positional embedding performs the worst. Therefore, \nfixed positional embedding is important for a model to learn the \nfacial part relationships. Meanwhile, the absence of positional \nembedding results in the loss of spatial geometric relationships \nbetween visual tokens. \nTABLE VII.  \nRESULTS OF DIFFERENT POSITIONAL EMBEDDING STRATEGIES.  \n \nPositional embedding Prediction errors (Â°) \nRoll Yaw Pitch MAE \nNone 4.39 4.51 7.11 5.33 \nLearnable 4.33 4.63 5.67 4.87 \n2D sine 4.08 4.36 5.54 4.66 \n \n2) Transformer block parameters . Transformer parameters \nhave effect in a certain extent on model performance. Therefore, \nwe investigate different options of Transformer block \nparameters. For comparison,  only the  investigated parameter \nvaries while the others are set to default configuration (token \ndimension of 64, GELU activation function, and 8 heads in \nMSA). The experimental results are shown in Table VIII. The \nbest dimension of token is 128, the best activation function is \nTanh, and best number of heads is 12. \nHPE models Prediction errors (Â°) \nRoll Yaw Pitch MAE \nFSA-Net [19] 3.60 2.89 4.29 3.60 \nXia et al. [28] 3.09 2.39 4.92 3.47 \nFDN [54] 2.88 3.00 3.98 3.29 \nHopenet [29] 3.00 3.29 3.39 3.23 \nTriNet [25] 2.44 2.93 3.04 2.80 \n6DRepNet [24] 2.36 2.69 2.92 2.66 \nTokenHPE-E (ours) 2.01 2.28 3.01 2.49 \nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \n \nFig. 6. Effect of the token guide multi -loss function. The hyperparameter Î³ \nbalances the importance of prediction loss and the orientation token loss. When \nÎ³ is set to 1.0, the model is solely trained with prediction loss, while when Î³ \napproaches zero, the model is solely trained with orientation token loss.  \n3) Number of Transformer blocks . Different number s of \nTransformer blocks are evaluated to check their effect on HPE. \nThe results are shown  in Fig. 7. As the number of blocks \nincreases, the MAE first decreases then increases. When the \nnumber of blocks is small, the model has less capacity to learn \nthe complicated f acial relationships. When the number of \nblocks is too large, the model is diffi cult to converge, thus \nresulting in the increase of MAE. The best result is achieved \nwhen the number of blocks is set to 12. \n \nTABLE VIII: ABLATION STUDY ON TRANSFORMER BLOCK \nHYPERPARAMETERS. \n Prediction errors (Â°) MAEV Roll Yaw Pitch MAE \nActivation function in P module \nTanh 4.08 4.36 5.54 4.66 5.99 \nReLU 4.28 4.43 5.71 4.80 6.01 \nGELU 4.19 4.35 5.63 4.72 5.98 \nToken dimension \n386 4.22 4.45 5.64 4.77 6.00 \n128 4.08 4.36 5.54 4.66 5.98 \n64 4.29 4.54 5.70 4.85 6.04 \nNumber of heads in MSA \n16 4.25 4.42 5.62 4.76 5.99 \n12 4.08 4.36 5.54 4.66 5.98 \n8 4.32 4.46 5.77 4.85 6.05 \n \n \nFig. 7. Effect of the number of Transformer blocks. \nF. Visualizations \nWe visualize the inference  details to investigate how the \nTokenHPE explicitly utilizes orientation tokens to find the \nfacial part relationships and orientation characteristics in the \nbasic regions. Notice that  on most common  images, our \nproposed TokenHPE exhibits similar behaviors and all images \nin Figs. 8-12 are randomly chosen from the AFLW 2000 dataset \nin order to visualize the details.  \n \nFig. 8. Heatmap visualization of three models, namely, HopeNet (left), \n6DRepNet (middle), and our proposed model (right)  in challenging scenarios, \nincluding occlusion and extreme orientation, occlusion, low illumination.  The \nred-color areas mean that the model provides high attention to these facial parts.  \n1) Visualization in challenging scenarios : To confirm that \nour model can learn critical minorit y facial part relationships  \nand tackle challenging scenarios , we use Grad -CAM [55] to \nvisualize the attention of head pose predictions in a challenging \nsubset of AFLW2000 . Two representative methods (HopeNet \nand 6DRepNet) are adopted for a comparison with our proposed \nmodel. As Fig . 8 shows, our method can learn the crucial \nminority relationships of facial parts, such as the eyes, nose, and \nears in challenging scenarios ( e.g., occlusion, extreme \norientation, low illumination) where some facial parts are \nmissing and hard to estimate. In these scenarios, the compared \nmethods performed poorly when abundant facial information is \nmissed. Row 2 indicates that our method can deduce the spatial \nlocation of the eyes to achieve accurate prediction compared \nwith the other methods that only attend to the facial parts that \nappear. As shown on Row 4, our method presents an impressive \ncapability to reveal the symmetric relationships of the face even \nthough the entire left side of the face is dark  due to low \nillumination. On Row 5, the attention heatmaps show that our \nmethod can find the critical minority relationships (nose, eyes, \nand ears)  in the most cha llenging scenario . In summary, the \nheatmap visualization proves that our method can learn facial \npart relationships and can deduce the spatial relationships of \nfacial parts to mitigate the obstacles in challenging scenarios. \n \nFig. 9. Cosine similarity mat rix between the learned orientation tokens. (a) \nStrategy I: nine basic  orientation regions. (b) Strategy II: eleven basic \norientation regions.  \n2) Similarity matrix of orientation tokens: We visualize the \ncosine similarities of the orientation tokens. As shown in Fig. 9, \nthe neighbor ed orientation tokens are highly similar. The \norientation tokens that represent symmetric facial regions have \nhigher similarity scores than the tokens that represent the other \nunrelated regions. Therefore, the results of the similarity matrix \nverify that the general information and the cross -orientation \nValue of\nDegree (Â°)\n#Regions=9 #Regions=11\nOnlyOnly \nLoss weight Loss weight\nValue of OnlyOnly \nNo. of blocks\nDegree (Â°)\nTokenHPE (ours)\nHopeNet 6DRepNet\nLow illumination\nExtreme orientation\nOcclusion/missing\nChallenging scenarios\nOcclusion/missing\nLow illumination\nOcclusion/missing\nOcclusion/missing\nExtreme orientation\nOcclusion/missing\n0 1 2 3 4 5 6 7 8\n012345678 0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n#Regions=9 #Regions=11\n0 1 2 3 4 5 6 7 8 9 10\n012345678910\n-0.6\n-0.4\n-0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nrelationships are learned by the orientation tokens. \n \nFig. 10. Cosine similarity matrix between orientation tokens during model training. The orientation information is learned gradually by the orientation tokens. \n \n \nFig. 11. Heatmap visualization in different Transformer blocks of the \nTokenHPE model. Arrows and circles indicate the crucial facial parts to which \nthe model pays attention for the head pose prediction. \n3) Orientation token learning during training: We calculate \nthe cosine similarity between the orientation tokens in different \ntraining epochs. As Fig. 1 0 shows, in early stages, no distinct \nrelationship is learned by the orientation tokens. As the training \nepochs increase, general information is learned gradually by the \norientation token s. The orientation relationships can be \nobserved in the later training epochs. In partition strategy I (nine \nbasic orientation regions), take the middle left (region 3) \norientation token in the 30 th epoch for example. The similarity \nscores are higher in its neighborhood regions (upper left (region \n0), bottom left (region 6)) and spatial symmetric regions, such \nas middle right (region 5). Similar results can be observed when \nthe number of basic orientation regions is set to eleven. \nVisualization of orientation token learning in the training stage \nvalidates that general orientation information and the cross-\norientation relationships can be learned by the orientation \ntokens. \n4) Region informa tion learned by orientation tokens : The \nattention maps of orientation tokens are visualized in Fig. 12. It \ncan be observed that  in shallow blocks, each orientation token \npays similar attention to the rest in order to construct the global \nperception of the image. By contrast, in deeper blocks , each \norientation token pays most attention on its neighborhood \nregion tokens and spatial symmetric tokens to yield the final \nprediction. As indicated in Fig. 12, at the deeper Transformer \nblocks, the attention score is  higher between neighbor regions \n(the diagonal) and symmetric regions, such as regions 0 and 2, \nregions 3 and 5, and regions 6 and 8. In Fig. 12, the attention \nscore is higher in regions 3, 4, 6, and 7, indicating that the \npredicted head pose has more prob ability in the left â€“bottom \ndirection, similar to the groundtruth. Therefore, from the \nvisualization shown in Fig. 12, we can conclude that our model \nhas the ability to encode the cross -orientation relationships of \nthe basic regional orientation characteris tics, including \nneighborhood similarities and symmetric properties. \n#Regions=9\n#Iteration=1\n#Regions=11 Orientation token process with the iteration number increasing (TokenHPE)\n1 2 3 4 5 6 7 80\n#Iteration=5 #Iteration=10 #Iteration=15 #Iteration=20 #Iteration=30\n12345678 0\n1 2 3 4 5 6 7 80 9 10\n12345678 0910\n1 2 3 4 5 6 7 801 2 3 4 5 6 7 801 2 3 4 5 6 7 80\n1 2 3 4 5 6 7 80 9 10 1 2 3 4 5 6 7 80 9 10 1 2 3 4 5 6 7 80 9 10 1 2 3 4 5 6 7 80 9 10 1 2 3 4 5 6 7 80 9 10\n1 2 3 4 5 6 7 80 1 2 3 4 5 6 7 80\nHeatmap visualization in different Transformer blocks\n(a) (b) (c) (d) (e) (f) (g)\nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \n \nFig. 12. Attention maps of orientation tokens of our TokenHPE model with the \nTransformer blocks number increasing. \nV. CONCLUSION \nIn this work, we proposed  an orientation cues-aware facial \nrelationship representation learning method for head pose \nestimation. We revealed intra -orientation relationships and \ncross-orientation relationships on head images. To leverage \nthese significant properties of head images, Transformer \narchitecture was utilized to learn intra-orientation relationships, \nand several orientation tokens were designed to encode cross -\norientation relationships according to panoramic overview \npartitions. The experimental results show ed that TokenHPE \nachieves state-of-the-art performance and is capable to resolve \nthe challenges of low illumination, occlusion, and extreme \norientations. Besides, the success of TokenHPE reveals the \nsignificance of facial and orientational relationships for head \npose estimation, which have been  ignored in previous \nresearches. Moreover, we hope this initial work can inspire \nfurther research on token -learning methods for HPE and other \nhead related fields, such as attention detection, facial expression \nrecognition, and gaze estimation. \nREFERENCES \n[1] A. F. Abate, C. Bisogni, A. Castiglione, and M. Nappi, \"Head pose \nestimation: An extensive survey on recent techniques and applications,\"  \nPattern Recognition, vol. 127, p. 108591, 2022. \n[2] P. Barra, S. Barra, C. Bisogni, M. De Marsico, and M. Nappi, \"Web-shaped \nmodel for head pose estimation: An approach for best exemplar selection,\" \nIEEE Transactions on Image Processing, vol. 29, pp. 5457-5468, 2020. \n[3] V. Drouard, R. Horaud, A. Deleforge, S. Ba, and G. Evangelidis, \"Robust \nhead-pose estimation based on partially -latent mixture of linear \nregressions,\" IEEE Transactions on Image Processing, vol. 26, pp. 1428-\n1440, 2017. \n[4] J. Liu, Z. Wang , H. Qin, K. Xu, B. Ji, and H. Liu, \"Free -Head Pose \nEstimation under Low-Resolution Scenarios,\" in 2020 IEEE International \nConference on Systems, Man, and Cybernetics (SMC) , 2020, pp. 2277 -\n2283. \n[5] Y. Wang, W. Liang, J. Shen, Y. Jia, and L. -F. Yu, \"A deep coarse-to-fine \nnetwork for head pose estimation from synthetic data,\" Pattern \nRecognition, vol. 94, pp. 196-206, 2019. \n[6] C. Bisogni, M. Nappi, C. Pero, and S. Ricciardi, \"FASHE: A fractal based \nstrategy for head pose estimation,\" IEEE Transactions on Im age \nProcessing, vol. 30, pp. 3192-3203, 2021. \n[7] W. Y. Hsu and C. J. Chung, \"A Novel Eye Center Localization Method for \nHead Poses With Large Rotations,\" IEEE Transactions on Image \nProcessing, vol. 30, pp. 1369-1381, 2021. \n[8] E. Murphy-Chutorian, A. Doshi, and M. M. Trivedi, \"Head pose estimation \nfor driver assistance systems: A robust algorithm and experimental \nevaluation,\" in 2007 IEEE intelligent transportation systems conference , \n2007, pp. 709-714. \n[9] A. Kumar, A. Alavi, and R. Chellappa, \"Kepler: Ke ypoint and pose \nestimation of unconstrained faces by learning efficient h -cnn regressors,\" \nin IEEE International Conference on Automatic Face and Gesture \nRecognition, 2017, pp. 258-265. \n[10] D. Bicho, P. GirÃ£ o, J. Paulo, L. Garrote, U. J. Nunes, and P. Pei xoto, \n\"Markerless Multi -View-based Multi -User Head Tracking System for \nVirtual Reality Applications,\" in 2019 IEEE International Conference on \nSystems, Man and Cybernetics (SMC), 2019, pp. 2645-2652. \n[11] E. Chong, N. Ruiz, Y. Wang, Y. Zhang, A. Rozga, and  J. M. Rehg, \n\"Connecting gaze, scene, and attention: Generalized attention estimation \nvia joint modeling of gaze and scene saliency,\" in Proceedings of the \nEuropean conference on computer vision (ECCV), 2018, pp. 383-398. \n[12] V. Kazemi and J. Sullivan, \"O ne millisecond face alignment with an \nensemble of regression trees,\" in Proceedings of the IEEE Conference on \nComputer Vision and Pattern Recognition, 2014, pp. 1867-1874. \n[13] A. Bulat and G. Tzimiropoulos, \"How far are we from solving the 2d & 3d \nface alignment problem?(and a dataset of 230,000 3d facial landmarks),\" \nin Proceedings of the IEEE International Conference on Computer Vision, \n2017, pp. 1021-1030. \n[14] S. S. Mukherjee and N. M. Robertson, \"Deep head pose: Gaze -direction \nestimation in multimodal video,\" IEEE Transactions on Multimedia, vol. \n17, pp. 2094-2107, 2015. \n[15] M. Martin, F. Van De Camp, and R. Stiefelhagen, \"Real time head model \ncreation and head pose estimation on consumer depth cameras,\" in \nProceedings of The 2nd International Confere nce on 3D Vision (3DV) , \n2014, pp. 641-648. \n[16] G. Fanelli, T. Weise, J. Gall, and L. V. Gool, \"Real time head pose \nestimation from consumer depth cameras,\" in Joint pattern recognition \nsymposium, 2011, pp. 101-110. \n[17] G. P. Meyer, S. Gupta, I. Frosio, D. Reddy, and J. Kautz, \"Robust model-\nbased 3d head pose estimation,\" in Proceedings of the IEEE International \nConference on Computer Vision, 2015, pp. 3649-3657. \n[18] J. Gu, X. Yang, S. De Mello, and J. Kautz, \"Dynamic facial analysis: From \nbayesian filtering to recurrent neural network,\" in Proceedings of the IEEE \nConference on Computer Vision and Pattern Recognition, 2017, pp. 1548-\n1557. \n[19] T. Yang, Y. Chen, Y. Lin, and Y. Chuang, \"Fsa-net: Learning fine-grained \nstructure aggregation for head pose estimation from a single image,\" in \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern \nRecognition, 2019, pp. 1087-1096. \n[20] C. Wu, Q. Xu, and U. Neumann, \"Synergy between 3dmm and 3d \nlandmarks for accurate 3d facial geometry,\" in 2021 International \nConference on 3D Vision (3DV), 2021, pp. 453-463. \n[21] V. Albiero, X. Chen, X. Yin, G. Pang, and T. Hassner, \"img2pose: Face \nalignment and detection via 6dof, face pose estimation,\" in Proceedings of \nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , \n2021, pp. 7617-7627. \n[22] H. Liu, S. Fang, Z. Zhang, D. Li, K. Lin, and J. Wang, \"MFDNet: \nCollaborative poses perception and matrix Fisher distribution for head pose \nestimation,\" IEEE Transactions on Multimedia, vol. 24, pp. 2449 -2460, \n2021. \n[23] H. Hsu, T. Wu, S. Wan, W. Wong, and C. Lee, \"Quatnet: Quaternion-based \nHead Pose Estimation with Multiregression Loss,\" IEEE Transactions on \nMultimedia, vol. 21, pp. 1035-1046, 2018. \n[24] T. Hempel, A. A. Abdelrahman, and A. Al -Hamadi, \"6D Rotation \nRepresentation For Unconstrained Head Pose Estimation,\" arXiv preprint \narXiv:2022.12555, 2022. \n(a)\n(b) (c)\n(d) (e)\n(f) (g)\nNeighborhood \nsimilarities \nSymmetric \nproperties\nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \n[25] Z. Cao, Z. Chu, D. Liu, and Y. C hen, \"A vector -based representation to \nenhance head pose estimation,\" in Proceedings of the IEEE/CVF Winter \nConference on Applications of Computer Vision, 2021, pp. 1188-1197. \n[26] X. Geng and Y. Xia, \"Head pose estimation based on multivariate label \ndistribution,\" in Proceedings of the IEEE Conference on Computer Vision \nand Pattern Recognition, 2014, pp. 1837-1842. \n[27] C. Zhang, H. Liu, et al. , \"TokenHPE: Learning Orientation Tokens for \nEfficient He ad Pose Estimation via Transformers,\" presented at the \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern \nRecognition (CVPR), Vancouver Canada, 2023. \n[28] J. Xia, L. Cao, G. Zhang, and J. Liao, \"Head pose estimation in the wild \nassisted by facial landmarks based on convolutional neural networ ks,\" \nIEEE Access, vol. 7, pp. 48470-48483, 2019. \n[29] N. Ruiz, E. Chong, and J. M. Rehg, \"Fine -grained head pose estimation \nwithout keypoints,\" in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition Workshops, 2018, pp. 2074-2083. \n[30] D. Alexey, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. \nUnterthiner, et al. , \"An image is worth 16x16 words: Transformers for \nimage recognition at scale,\" arXiv preprint arXiv:2010.11929, 2020. \n[31] N. Dhingra, \"LwPosr: Lightweight Efficient Fine Grained Head Pose \nEstimation,\" in Proceedings of the IEEE/CVF Winter Conference on \nApplications of Computer Vision, 2022, pp. 1495-1505. \n[32] N. Dhingra, \"HeadPosr: End-to-end Trainable Head Pose Estimation using \nTransformer Encoders,\" in 2021 16th IE EE International Conference on \nAutomatic Face and Gesture Recognition (FG 2021), 2021, pp. 1-8. \n[33] M. Xin, S. Mo, and Y. Lin, \"Eva-gcn: Head pose estimation based on graph \nconvolutional networks,\" in Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition, 2021, pp. 1462-1471. \n[34] J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou, \"Retinaface: \nSingle-shot multi-level face localisation in the wild,\" in Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, \npp. 5203-5212. \n[35] R. Ranjan, V. M. Patel, and R. Chellappa, \"Hyperface: A deep multi -task \nlearning framework for face detection, landmark localization, pose \nestimation, and gender recognition,\" IEEE transactions on pattern analysis \nand machine intelligence, vol. 41, pp. 121-135, 2017. \n[36] A. F. Abate, P. Barra, C. Pero, and M. Tucci, \"Head pose estimation by \nregression algorithm,\" Pattern Recognition Letters, vol. 140, pp. 179-185, \n2020. \n[37] T. N. Kipf and M. Welling, \"Semi -supervised classification with graph \nconvolutional networks,\" arXiv preprint arXiv:1609.02907, 2016. \n[38] Y. Deng, H. Chen, H. Liu, and Y. Li, \"A Voxel Graph CNN for Object \nClassification With Event Cameras,\" in Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, 2022, pp. 1172-\n1181. \n[39] S. Yan, Y. Xiong, and D. Lin, \"Spatial temporal graph convolutional \nnetworks for skeleton -based action recognition,\" in Thirty-second AAAI \nconference on artificial intelligence, 2018. \n[40] L. Ge, Z. Ren, Y. Li, Z. Xue, Y. Wang, J. Cai , et al., \"3d hand shape and \npose estimation from a single rgb image,\" in Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition , 2019, pp. \n10833-10842. \n[41] J.-X. Zhong, N. Li, W. Kong, S. Liu, T . H. Li, and G. Li, \"Graph \nconvolutional label noise cleaner: Train a plug -and-play action classifier \nfor anomaly detection,\" in Proceedings of the IEEE/CVF Conference on \nComputer Vision and Pattern Recognition, 2019, pp. 1237-1246. \n[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \net al. , \"Attention is all you need,\" Advances in Neural Information \nProcessing Systems, vol. 30, 2017. \n[43] J. He, J. Chen, S. Liu, A. Kortylewski, C. Yang, Y. Bai , et al., \"TransFG: \nA Transformer Architecture for Fine-grained Recognition,\" in Proceedings \nof the AAAI Conference on Artificial Intelligence, 2022, pp. 852-860. \n[44] W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, et al., \"Pyramid Vision \nTransformer: A Versatile Backbone for Dense Predicti on Without \nconvolutions,\" in Proceedings of the IEEE/CVF International Conference \non Computer Vision, 2021, pp. 568-578. \n[45] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, et al., \"Swin transformer: \nHierarchical vision transformer using shifted windows,\" in Proceedings of \nthe IEEE/CVF International Conference on Computer Vision , 2021, pp. \n10012-10022. \n[46] H. Zhang, F. Li, S. Liu, L. Zhang, H. Su, J. Zhu , et al., \"Dino: Detr with \nimproved denoising anchor boxes for end -to-end object detection,\" arXiv \npreprint arXiv: 2203.03605, 2022. \n[47] F. Xue, Q. Wang, and G. Guo, \"Transfer: Learning relation -aware facial \nexpression representations with transformers,\" in Proceedings of the \nIEEE/CVF International Conference on Computer Vision, 2021, pp. 3601-\n3610. \n[48] Y. Li, S. Zhang, Z. Wang, S. Yang, W. Yang, S.-T. Xia, et al., \"Tokenpose: \nLearning keypoint tokens for human pose estimation,\" in Proceedings of \nthe IEEE/CVF International Conference on Computer Vision , 2021, pp. \n11313-11322. \n[49] Y. Wang, Z. Xu, X. Wang, C. Shen, B. Cheng, H. Shen, et al., \"End-to-end \nvideo instance segmentation with transformers,\" in Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, \npp. 8741-8750. \n[50] J. B. Cordonnier, A. Loukas, and M. Jaggi, \"On the relationship between \nself-attention and convolutional layers,\" arXiv preprint arXiv:1911.03584, \n2019. \n[51] G. Fanelli, M. Dantone, J. Gall, A. Fossati, and L. Van Gool, \"Random \nforests for real time 3d face analysis,\" International Journal of Computer \nVision (IJCV), vol. 101, pp. 437-458, 2013. \n[52] X. Zhu, Z. Lei, X. Liu, H. Shi, and S. Z. Li, \"Face alignment across large \nposes: A 3d solution,\" in Proceedings of the IEEE Conference on Computer \nVision and Pattern Recognition (CVPR), 2016, pp. 146-155. \n[53] Y. Zhou and J. Gregson, \"Whenet: Real -time fine-grained estimation for \nwide range head pose,\" arXiv preprint arXiv:2005.10353, 2020. \n[54] H. Zhang, M. Wang, Y. Liu, and Y. Yuan, \"FDN: Feature decoupling \nnetwork for head pose estimation,\" in Proceedings of the AAAI conference \non artificial intelligence, 2020, pp. 12789-12796. \n[55] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. \nBatra, \"Grad-cam: Visual explanations from deep networks via gradient -\nbased localization,\" in Proceedings of the IEEE International Conference \non Computer Vision, 2017, pp. 618-626. \n \nHai LIU  (S12-M14) received the M.S. degree in \napplied mathematics from Huazhong University of \nScience and Technology (HUST),  Wuhan, China, in \n2010, and the Ph. D. degree in pattern recognition and \nartificial intelligence from the same university, in 2014.  \nSince June 2017, he has been an Assistant Professor \nwith the Faculty of Artificial Intelligence in Education, \nCentral China Normal University, Wuhan. From 2023 \nto 2024, he was selected as â€œChina - European \nCommission Talent Programmeâ€ under National \nNatural Science Foundation of China (NSFC). He was a senior researcher with \nUCL Interaction Centre, University College London, London, United Kingdom, \nwhere he was host by the Professor Sriram Subramanian. He has authored more \nthan 100 peer reviewed articles in international journals from multiple domains. \nMore than 20 articles are selected as the ESI highly cited articles, and eight \npapers were selected as the hot papers. His current research interests include \nhuman pose estimation, gaze tracking, head pose estimation, facial expression \nrecognition, deep learning, artificial intelligence, self-regulated learning.  \nDr. Liu has been freq uently serving as a reviewer for more than six \ninternational journals including the IEEE Transactions on Multimedia, IEEE \nTranslations on Industrial Informatics, IEEE Transactions on Knowledge and \nData Engineering. He is also a Communication Evaluation Expert for the NSFC \nfrom 2016 to present. He won the first prize of Science and Technology \nProgress Award by the Hubei Province of China in 2020. \n \nCheng Zhang  (Student Member IEEE)  is currently \nworking toward the B.E. degree in artificial intelligence \nfrom Central China Normal University, Wuhan, China. \nHis research interests include deep learning, object \ndetection and pattern recognition. \n \n \n \n \nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n \n \nYongjian Deng  (Member IEEE)  received the Ph.D. \ndegree from the City University of Hong Kong in 2021. \nHe is currently an Assistant Professor in the College of \nComputer Science, Beijing University of Technology. \nHis research interests include pattern recognition and \nmachine learning with event cameras. \n \n \n \n \nTingting Liu  (Member IEEE) received the M.S. \ndegree in natural language processing from Huazhong \nUniversity of Science and Technology, in 2014, Ph.D \ndegree in education information technology from \nCentral China Normal University (CCNU), Wuhan, \nChina, in 2019.  \nShe joined Hubei University, Wuhan, in 2020, and \nis currently an Assistant Professor with the School of \nEducation. During November 2022- November 2024, \nshe was selected as a visiting scholar in School of \nComputer Science, City University of Hong Kong, Hong Kong, China. where \nshe was hosted by the Professor Youfu Li. Her current research interests include \nlearning behavior analysis, human pose estimation, label distribution learning \nand graph neural network.  \nDr. Liu has been frequently serving as a Reviewer for several international \njournals including the IEEE Transactions on Knowledge and Data Engineering, \nIEEE Transactions on Neural Networks and Learning S ystems, IEEE \nTransactions on Multimedia. She is also a Communication Evaluation Expert \nfor the National Natural Science Foundation of China from 2020. \n \nZhaoli Zhang (Senior Member, IEEE) received the \nM.S. degree in Computer Science from Central \nChina Normal University, Wuhan, China, in 2004, \nand the Ph.D. degree in Computer Science from \nHuazhong University of Science and Technology \nin 2008.  \nHe is currently a professor in the National \nEngineering Research Center for e -learning, \nCentral China Normal University, Wuhan, China. \nHis research interests include self -regulated \nlearning, human -computer interaction, deep \nlearning, image processing, knowledge services \nand software engineering. He is a member of IEEE and CCF (China Computer \nFederation). \n \nYou-Fu LI (SM01-FELLOW21) is a Professor in the \nDepartment of Mechanical Engineering, City \nUniversity of Hong Kong. He received the Ph.D. \ndegree in robotics from the Department of Engineering \nScience, University of Oxford, U.K. in 1993. He was a \nresearch staff from the Computer Science Department \nat the University of Wales, Aberystwyth, UK, from \n1993 to 1995. His research interests include robot \nsensing, robot vision, and attitude estimation. In these \nareas, Professor Li has published over 150 papers in \nrefereed international journals. \nProfessor Li has served as an Associate Editor for the IEEE \nTRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING \nand is currently an Associate Editor for the IEEE ROBOTICS AND \nAUTOMATION MAGAZINE. He is an Editor for the IEEE Robotics and \nAutomation Society Conference Editorial Board and the IEEE Conference on \nRobotics and Automation. \nThis article has been accepted for publication in IEEE Transactions on Image Processing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TIP.2023.3331309\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.760257363319397
    },
    {
      "name": "Pose",
      "score": 0.7518492937088013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6774073243141174
    },
    {
      "name": "Orientation (vector space)",
      "score": 0.5903555750846863
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5407161712646484
    },
    {
      "name": "Computer vision",
      "score": 0.5330219268798828
    },
    {
      "name": "Security token",
      "score": 0.5160512328147888
    },
    {
      "name": "Transformer",
      "score": 0.46878373622894287
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.41191551089286804
    },
    {
      "name": "Mathematics",
      "score": 0.13783308863639832
    },
    {
      "name": "Engineering",
      "score": 0.09089004993438721
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I40963666",
      "name": "Central China Normal University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I37796252",
      "name": "Beijing University of Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I75900474",
      "name": "Hubei University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I168719708",
      "name": "City University of Hong Kong",
      "country": "HK"
    }
  ],
  "cited_by": 132
}