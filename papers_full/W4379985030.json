{
  "title": "Image Super-Resolution Using Dilated Window Transformer",
  "url": "https://openalex.org/W4379985030",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101880665",
      "name": "Soobin Park",
      "affiliations": [
        "Hanyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2099594421",
      "name": "Yong-Suk Choi",
      "affiliations": [
        "Hanyang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6794906783",
    "https://openalex.org/W3179869055",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6756040250",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3034595214",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2947376905",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W4312960790",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W6795435739",
    "https://openalex.org/W2102166818",
    "https://openalex.org/W1930824406",
    "https://openalex.org/W2121927366",
    "https://openalex.org/W2739757502",
    "https://openalex.org/W2192954843",
    "https://openalex.org/W3178925107",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W1791560514",
    "https://openalex.org/W2047920195",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6754405603",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W3204971388",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W2964101377",
    "https://openalex.org/W6752237900",
    "https://openalex.org/W2954930822",
    "https://openalex.org/W6797790494",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W6788135285",
    "https://openalex.org/W6811232167",
    "https://openalex.org/W6794345597",
    "https://openalex.org/W6779778204",
    "https://openalex.org/W2747898905",
    "https://openalex.org/W6779248606",
    "https://openalex.org/W4319300717",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W6845283504",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W6838529300",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W6696085341",
    "https://openalex.org/W6796568838",
    "https://openalex.org/W6810863823",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W6782282802",
    "https://openalex.org/W54257720",
    "https://openalex.org/W3174531399",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4290714341",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W2899771611",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3169612303",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3096739052",
    "https://openalex.org/W4226335785",
    "https://openalex.org/W2805163084",
    "https://openalex.org/W3038857985",
    "https://openalex.org/W4386083034",
    "https://openalex.org/W4302306054",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2891158090",
    "https://openalex.org/W3175544090",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W3101659800"
  ],
  "abstract": "Transformer-based networks using attention mechanisms have shown promising results in low-level vision tasks, such as image super-resolution (SR). Specifically, recent studies that utilize window-based self-attention mechanisms have exhibited notable advancements in image SR. However, window-based self-attention, results in a slower expansion of the receptive field, thereby restricting the modeling of long-range dependencies. To address this issue, we introduce a novel dilated window transformer, namely DWT, which utilizes a dilation strategy. We employ a simple yet efficient dilation strategy that enlarges the window by inserting intervals between the tokens of each window to enable rapid and effective expansion of the receptive field. In particular, we adjust the interval between the tokens to become wider as the layers go deeper. This strategy enables the extraction of local features by allowing interaction between neighboring tokens in the shallow layers while also facilitating efficient extraction of global features by enabling interaction between not only adjacent tokens but also distant tokens in the deep layers. We conduct extensive experiments on five benchmark datasets to demonstrate the superior performance of our proposed method. Our DWT surpasses the state-of-the-art network of similar sizes by a PSNR margin of 0.11dB to 0.27dB on the Urban100 dataset. Moreover, even when compared to state-of-the-art network with about 1.4 times more parameters, DWT achieves competitive results for both quantitative and visual comparisons.",
  "full_text": "Received 7 May 2023, accepted 1 June 2023, date of publication 9 June 2023, date of current version 20 June 2023.\nDigital Object Identifier 10.1 109/ACCESS.2023.3284539\nImage Super-Resolution Using Dilated Window\nTransformer\nSOOBIN PARK\n 1 AND YONG SUK CHOI\n2\n1Department of Artificial Intelligence, Hanyang University, Seoul 04763, South Korea\n2Department of Computer Science, Hanyang University, Seoul 04763, South Korea\nCorresponding author: Yong Suk Choi (cys@hanyang.ac.kr)\nThis work was supported in part by the National Research Foundation of Korea (NRF) Grant funded by the Korean Government [Ministry\nof Science and Information and Communication Technology (MSIT)] under Grant 2018R1A5A7059549 and Grant 2020R1A2C1014037;\nand in part by the Institute of Information and Communications Technology Planning and Evaluation (IITP) Grant funded by the Korean\nGovernment (MSIT), Artificial Intelligence Graduate School Program, Hanyang University, under Grant 2020-0-01373.\nABSTRACT Transformer-based networks using attention mechanisms have shown promising results in\nlow-level vision tasks, such as image super-resolution (SR). Specifically, recent studies that utilize window-\nbased self-attention mechanisms have exhibited notable advancements in image SR. However, window-\nbased self-attention, results in a slower expansion of the receptive field, thereby restricting the modeling\nof long-range dependencies. To address this issue, we introduce a novel dilated window transformer,\nnamely DWT, which utilizes a dilation strategy. We employ a simple yet efficient dilation strategy that\nenlarges the window by inserting intervals between the tokens of each window to enable rapid and effective\nexpansion of the receptive field. In particular, we adjust the interval between the tokens to become wider\nas the layers go deeper. This strategy enables the extraction of local features by allowing interaction\nbetween neighboring tokens in the shallow layers while also facilitating efficient extraction of global\nfeatures by enabling interaction between not only adjacent tokens but also distant tokens in the deep layers.\nWe conduct extensive experiments on five benchmark datasets to demonstrate the superior performance of\nour proposed method. Our DWT surpasses the state-of-the-art network of similar sizes by a PSNR margin\nof 0.11dB to 0.27dB on the Urban100 dataset. Moreover, even when compared to state-of-the-art network\nwith about 1.4 times more parameters, DWT achieves competitive results for both quantitative and visual\ncomparisons.\nINDEX TERMS Image super-resolution, self-attention mechanism, transformer, window-based self-\nattention.\nI. INTRODUCTION\nImage super-resolution (SR) is a well-known problem in low-\nlevel vision tasks, which aims to reconstruct a high-resolution\n(HR) image from its low-resolution (LR) counterpart. Recent\nadvancements in deep learning have enabled image SR using\ndeep convolutional neural networks such as residual learn-\ning [1], [2], dense blocks [3], attention mechanisms [4], [5],\n[6], and adversarial learning [7], [8], [9]. Due to improved\nresults, convolutional neural networks have become the de\nfacto standard for this field.\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Charalambos Poullis\n.\nIn recent years, inspired by the remarkable success\nof Transformer [10] in the field of natural language\nprocessing, several researchers have attempted to adopt\ntransformer-based networks for high-level vision tasks with\npromising results across high-level vision tasks, such as\nimage classification [11], [12], [13], object detection [14],\n[15], [16], and dense prediction [17], [18]. Following\nthe success of this approach, researchers have also intro-\nduced transformer-based networks for low-level vision tasks,\nincluding SR [19], [20], [21]. In particular, SwinIR [21],\nwhich adopts window-based self-attention of Swin Trans-\nformer [15], has achieved breakthrough performance in the\nSR task.\n60028\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ VOLUME 11, 2023\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nFIGURE 1. SR results of SwinIR. SwinIR fails to accurately restore textures even for images with self-repeating patterns. These findings indicate that\nSwinIR cannot utilize the complete global information of the image.\nHowever, window-based self-attention leads to slower\ngrowth of receptive field, which limits the potential of\nmodeling long-range dependencies [16]. Window-based self-\nattention is effective at capturing local context, but it falls\nshort in capturing global context. This limitation affects the\nimage quality reconstructed by SR networks using window-\nbased self-attention. As shown in Fig. 1, SwinIR restores\nunclear textures even for images with repetitive patterns.\nDue to the lack of global information, SwinIR cannot utilize\ninformation of similar patterns located at a farther distance.\nTo solve this problem, some recent studies have achieved\noutstanding performance compared to SwinIR by proposing\npre-training methods on large-scale datasets [22] such as Ima-\ngeNet [23] or by combining CNN-based channel attention [4]\nand window-based self-attention to use their complementary\nadvantages [24]. However, these gains have come at the cost\nof additional large-scale training data and a greater number\nof parameters compared to SwinIR.\nIn this work, we propose a dilated window transformer\n(DWT) that complements the limitations of SwinIR with-\nout introducing additional training data and parameters. Our\nDWT introduces two types of window-based multi-head self-\nattention blocks, named the window attention block (WAB)\nand the dilated window attention block (DWAB). We adopt a\nstructure that alternates between WAB and DWAB. The WAB\nis responsible for extracting local features using standard\nwindow attention, while the DWAB uses a dilation strategy\nto extract global features. Similar to dilated convolution [25],\nDWAB places intervals between the tokens in each win-\ndow to expand the receptive field. In contrast to SwinIR’s\nwindow-based self-attention, which only interacts with adja-\ncent tokens at all layers, we use the dilation strategy in an\neffective way to allow each token to interact with tokens that\nare farther away as the layers become deeper. As a result, our\nDWT effectively utilizes both local and global context when\nrestoring images. Experimental results show that our DWT\nachieves better performance than the state-of-the-art models\nof similar sizes on five benchmark datasets.\nTo summarize, the main contributions of our DWT are as\nfollows:\n• We introduce a DWT, which leverages a dilation strategy\nto effectively extract both local and global features, address-\ning the limitations of window-based self-attention.\n• We propose a dilation strategy that is adopted in the\nDWAB of our DWT. This strategy efficiently widens the\nreceptive field, allowing for improved modeling of long-\nrange dependencies.\n• By conducting extensive experiments, we demonstrate\nthat our DWT achieves promising results compared to other\nstate-of-the-art methods, while the number of parameters and\ncomputational cost are competitive in comparison with con-\nventional methods. The superior performance of our model\nhighlights the effectiveness of our proposed dilation strategy\nfor image SR.\nThe rest of the paper is organized as follows.\nIn Section II, we summarize the related work. Then,\nwe describe our proposed method in Section III. Section IV\npresents the experimental results and analysis. Finally, con-\nclusions are drawn in Section V.\nII. RELATED WORK\nA. IMAGE SUPER-RESOLUTION\nSince the introduction of SRCNN [26], [27], which applied\nthe deep convolutional neural network to image SR for the\nfirst time, various deep neural networks with different designs\nhave been proposed. Kim et al. [2] utilized residual learn-\ning to build a deeper network and speed up convergence.\nLedig et al. [7] introduced adversarial learning to improve the\ntexture details of the reconstructed images. Several methods\nusing attention mechanisms, such as channel attention [5],\n[28] and non-local attention [6], [29] have significantly\nimproved the performance of the SR task. In addition, net-\nworks using recurrent neural networks [6], [30] and graph\nneural networks [31] have also been proposed. Recently,\ntransformer-based networks [19], [32] have been applied to\nthe SR task and have shown remarkable performance. These\nnetworks take advantage of their ability to model long-range\ndependencies to improve the quality of reconstructed\nimages.\nVOLUME 11, 2023 60029\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nFIGURE 2. (a) Overall architecture of DWT. The input image goes through a shallow feature extraction module, and the shallow features are fed into\ndeep feature extraction module to extract deep features. Finally, the shallow features and deep features are fused by a skip connection, and an image\nreconstruction module is utilized to generate the SR result. (b) The inner structure of two successive multi-head self-attention blocks. The WAB and the\nDWAB are alternately applied in a pair of multi-head self-attention blocks.\nB. VISION TRANSFORMER\nInspired by the success of Transformer [10] in machine\ntranslation tasks, transformer-based networks have been\nintroduced in computer vision community. Dosovitskiy et\nal. [11] introduced VIT, which was the first application\nof a transformer architecture in computer vision to non-\noverlapping medium-sized image patches. The ability of\ntransformer-based networks to model long-range dependen-\ncies through self-attention has shown impressive performance\nin high-level vision tasks, such as image classification [11],\n[12], [13], [33], object detection [14], [15], [34], [35], [36],\nand dense prediction [17], [18], [37], [38]. Transformer-based\narchitectures have also been applied to low-level vision tasks.\nChen et al. [19] proposed a standard transformer architec-\nture called IPT for various low-level vision tasks, includ-\ning image SR, denoising, and deraining. Liang et al. [21]\nproposed SwinIR, which introduced window-based self-\nattention of the Swin Transformer [15] instead of standard\nself-attention, leading to tremendous growth in the SR task.\nHowever, SwinIR has a structural limitation that it cannot\nmodel long-range dependencies in input images, despite its\nadvantage of efficient local feature extraction using window-\nbased self-attention. Zhang et al. [39] proposed an attention\nretractable transformer named ART for low-level vision tasks\nto compensate for the limitations of dense attention used in\nwindow-based self-attention. ART introduced a sparse atten-\ntion strategy similar to the dilation strategy we proposed.\nIn this paper, we propose a DWT that uses a dilation strategy\nmore efficiently than ART to successfully extract both local\nand global features for improved performance in image SR.\nIII. PROPOSED METHOD\nA. MOTIVATION\nSwinIR [21], which applied the Swin Transformer archi-\ntecture [15], has demonstrated the significant potential of\ntransformer-based networks in image SR. SwinIR extracts\ndeep features using window-based self-attention and shifted\nwindow-based self-attention, which demonstrates robust\ncapabilities in local feature extraction.\nHowever, SwinIR’s window-based self-attention has a\nstructural limitation that falls short of capturing global con-\ntext. This is due to the use of a smaller and slowly growing\nreceptive field in comparison to the full-sized receptive field\nused in standard self-attention. This limitation leads to serious\ndefects that cannot produce high-quality output images in\nimage SR. For instance, SwinIR restores incorrect textures,\neven for images with self-repeating patterns, as shown in\nFig. 1. This phenomenon indicates that SwinIR does not fully\nleverage the global information of the image, and extracting\nboth local and global information is important to achieve\nbetter performance in image SR. Thus, effectively utilizing\nglobal information to reconstruct HR images may overcome\nthe limitations of SwinIR. Based on this motivation, we pro-\npose a DWT, which can effectively enlarge the receptive field.\nB. THE OVERALL ARCHITECTURE\nAs shown in Fig. 2(a), our DWT consists of three mod-\nules, including shallow feature extraction, deep feature\nextraction, and image reconstruction. Given a LR image\nIL R∈ RH×W×Cin (H, W, and Cin are the image height,\nwidth, and the input channel number, respectively), we apply\na single 3 × 3 convolutional layer HSF (·) to obtain shallow\nfeatures FSF ∈ RH×W×C as:\nFSF = HSF (IL R), (1)\nwhere C is the channel number of the feature. According\nto [40], a convolutional stem can result in more reliable\noptimization. Additionally, it can effectively map an input\nimage from a low-dimensional space to a high-dimensional\nspace. Then, deep features FDF ∈ RH×W×C are extracted as:\nFDF = HDF (FSF ), (2)\n60030 VOLUME 11, 2023\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nFIGURE 3. Illustration of two types of attention blocks in RTG. The sizes of the baby picture and window are 16× 16 and 4× 4, respectively. A small\nsquare represents one pixel, and pixels of the same color belong to the same window. The dilation value depends on the depth of the network: smaller\nvalues for shallow layers, and larger values for deep layers. A larger dilation value encourages the model to capture long-range dependencies.\nwhere HDF (·) is a deep feature extraction module consisting\nof M residual transformer groups (RTG) and one 3 × 3 con-\nvolutional layer. The intermediate features of the deep feature\nextraction module are sequentially extracted as:\nFi = HRTGi (Fi−1), i = 1, 2, . . . ,M,\nFDF = HConv(FM ), (3)\nwhere HRT Gi (·) denotes the i-th RTG and HCon v(·) denotes\nthe last convolutional layer at the end of the deep feature\nextraction module. This last convolutional layer can bring\nthe inductive biases into the transformer-based network and\nlead to better aggregation of deep features [21]. Shallow\nfeatures FSF and deep features FDF are fused by a long\nskip connection and passed through the image reconstruction\nmodule HRec (·) to generate a HR image IS Ras:\nISR = HRec(FSF + FDF ). (4)\nSpecifically, we use the sub-pixel convolutional layer [41] to\nupscale the feature. We optimize our model parameters with\nL1 pixel loss, which is known to be effective in image SR.\nC. RESIDUAL TRANSFORMER GROUP (RTG)\nAs shown in Fig. 2(a), the RTG is a residual group consisting\nof N pairs of multi-head self-attention blocks and one 3 ×\n3 convolutional layer. For the i-th RTG, it is formulated as:\nFi,j = HMHSABij (Fi,j−1), j = 1, 2, . . . ,N,\nFi,out = HConv(Fi,N ) + Fi,0, (5)\nwhere HM H S ABi j(·) is the j-th pair of multi-head self-\nattention blocks in the i-th RTG. Following [21], at the end\nof the RTG, we employ a single 3 × 3 convolutional layer\nHCon v(·) and the residual connection is also added.\nD. SUCCESSIVE MULTI-HEAD SELF-ATTENTION BLOCKS\nWe introduce two types of window-based multi-head self-\nattention blocks: WAB and DWAB. Commonly, window-\nbased self-attention proceeds as follows. Given an input fea-\nture of size H × W × C, it is first partitioned into H\nM × W\nM\nnon-overlapping windows of size M × M. Note that we treat\neach pixel as a token so that our DWT can learn pixel-level\ninformation. Then, self-attention is calculated separately for\neach window. For a local window feature X ∈ RM2×C , the\nquer y, key , and value metrics Q, K , and V are computed\nby linear projection as:\nQ = X WQ, K = X WK , V = X WV , (6)\nwhere WQ, WK , and WV denote the weight metrics for linear\nprojection. Then, the attention matrix is computed by the\nwindow-based self-attention as:\nAttention(Q, K, V ) = SoftMax(QK T /\n√\nd + B)V, (7)\nwhere d is the dimension of the quer y/key and B is the\nlearnable relative positional encoding.\nAs shown in Fig. 2(b), WAB and DWAB are alternately\napplied in a pair of successive multi-head self-attention\nblocks. Both WAB and DWAB are based on the window-\nbased self-attention of the Swin Transformer [15]. The key\ndifference between the two lies in the dilation strategy\nVOLUME 11, 2023 60031\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nemployed in DWAB. Different from SwinIR, we use the dila-\ntion strategy with the shifted window mechanism in DWAB\nto obtain a wider receptive field. As shown in Fig. 3, for\nWAB, every M × M token of each window is adjacent.\nIn contrast, for DWAB, M × M tokens of each window\nare sampled with a dynamic interval size. Therefore, while\nWAB can extract local features through interactions with\nadjacent tokens, DWAB can extract global features through\ninteractions with tokens that are further away. With the dila-\ntion strategy, consecutive multi-head self-attention blocks are\ncomputed as:\nˆxl = W-MSA(LN(xl−1)) + xl−1,\nxl = MLP(LN( ˆxl )) + ˆxl ,\nˆxl+1 = DW-MSA(LN(xl )) + xl ,\nxl+1 = MLP(LN(ˆxl+1)) + ˆxl+1, (8)\nwhere ˆxl and xl denote the output feature of the (D)W-\nMSA and the MLP for l-th attention block, respectively. MLP\ndenotes a multi-layer perceptron that has two fully-connected\nlayers with GELU activation function between them, and\nLN denotes the layer normalization. W-MSA and DW-MSA\ndenote window-based multi-head self-attention and dilated\nwindow-based multi-head self-attention, respectively.\n1) WINDOW ATTENTION BLOCK (WAB)\nAs shown in Fig. 3, in a WAB, multi-head self-attention is\ncomputed within non-overlapping windows. Each token can\ninteract with neighboring M × M tokens, including itself.\n2) DILATED WINDOW ATTENTION BLOCK (DWAB)\nIn this section, we elaborate on the key design element of\nDWT, the DWAB.\nInspired by dilated convolution [25], we introduce a dila-\ntion strategy. Dilated convolution is a type of convolution that\nenlarges the kernel by inserting holes between the kernel ele-\nments. In a similar method, we employ a dilation strategy with\nthe shifted window mechanism to even-numbered attention\nblocks in every RTG. Similar to SwinIR [21], DWAB utilizes\na shifted window mechanism for cross-window connections.\nAs illustrated in Fig. 3, the dilation value depends on the depth\nof the network and the input image size. The dilation value\nindicates the interval between the tokens of each window.\nThus, in DWAB, as the dilation value increases, the tokens\nwithin each window interact with tokens located at a greater\ndistance. In the first two RTGs, we set the dilation value as\n1 to sufficiently extract local features in the early stages of\ndeep feature extraction. Therefore, the DWAB of the first two\nRTGs is exactly the same as the shifted window-based self-\nattention of SwinIR [21]. In the third and fourth RTGs, DWT\nextracts M × M tokens for a window from the 1\n4 area of the\nshifted input image, while in the last two RTGs, DWT extracts\nM ×M tokens for a window from the entire area of the shifted\ninput image. The dilation value increases with depth of the\nnetwork, allowing each token to interact with a wider area as\nthe layers become deeper.\nIn Fig. 3, we illustrate an example where the height and\nwidth of the input image are both 16 and the window size\nis set to 4 × 4. For this example, the dilation values of the\nheight and width are computed as 16\n2 × 1\n4 = 2 in the third\nand fourth RTGs, while in the last two RTGs, the dilation\nvalues of the height and width are computed as 16\n4 = 4. The\nproposed DWAB enables the receptive field to be widened\nfaster and more efficiently than the standard shifted window-\nbased self-attention. Specifically, local features are extracted\nin the shallow layers through interactions between adjacent\ntokens, while global features are extracted in the deep layers\nby interacting with neighboring and distant tokens.\nIn summary, our DWAB provides an effective means of\nwidening the receptive field and improving local and global\nfeature extraction.\nE. DIFFERENCES FROM RELATED WORK\nIn this section, we provide a detailed comparison between our\nproposed DWT and the ART [39] introduced in Section II.\nART is an attention retractable transformer that uses sparse\nattention, which is similar to our dilation strategy. However,\nthere are several differences between these methods. We com-\npare the differences in two aspects.\n1) WINDOW SIZE IN THE ATTENTION BLOCK\nIn our proposed DWT, the window size in DWAB is always\nfixed to 16 × 16, irrespective of the input image size. How-\never, in ART’s sparse attention block (SAB), the window\nsize varies depending on the input image size. ART uses a\nfixed interval size of 4, meaning that as the input image size\nincreases, the window size also increases, leading to a higher\ncomputational cost. For example, if the height and width of\nthe input image are both 160, the window size in SAB of ART\nis 40 × 40, while the window size in DWAB of our DWT is\n16 × 16.\n2) DESIGN OF THE ATTENTION BLOCK\nAs explained in Section III-D2, our proposed DWT employs\na simple yet efficient dilation strategy to gradually expand\nthe receptive field. In contrast to ART, which has a fixed\ninterval size for all layers, our DWT increases the dilation\nvalue as the layers become deeper, allowing for a wider\narea where dilation is applied. As a result, our DWT can\nextract both local and global features efficiently in the shallow\nand deep layers, respectively. We provide a detailed analy-\nsis of the effectiveness of our proposed dilation strategy in\nSection IV-B.\nIV. EXPERIMENT AND ANALYSIS\nA. EXPERIMENTAL SETUP\n1) IMPLEMENTATION DETAILS\nFor DWT implementation, the RTG number is set to 6. Both\nthe WAB and DWAB number of each RTG are set to 3 and\nattention head is set to 6. Therefore, each RTG is composed\nof three pairs of multi-head self-attention blocks. Since the\n60032 VOLUME 11, 2023\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nFIGURE 4. LAM comparison results. The LAM results represent the importance of each pixel in the input LR image with respect to the SR results of the\npatch marked with a red box [42]. The higher DI provided below the LAM results indicates a wider range of pixels used.\nTABLE 1. Ablation study on the design of DWAB (×2 SR).\nDWAB uses shifted window mechanism, we adopt a masking\nstrategy in DWAB to restrict self-attention between the non-\nadjacent areas, similar to the Swin Transformer [15] and\nSwinIR [21]. The channel number for all the modules except\nthe image reconstruction module is set to 180, while in the\nimage reconstruction module, it is set to 64. All convolutional\nlayers in DWT have 3 × 3 kernel, stride of length 1, and\npadding of length 1, so the height and width of the feature map\nremain the same as the input size before upsampling. In [22]\nand [24], the authors showed the effectiveness of using a large\nwindow size. Therefore, we set the window size to 16 × 16.\nTo ensure fair comparison, we also provide a smaller version\nof DWT, which we refer to as DWT-S. In DWT-S, we set the\nwindow size to 8 × 8, while keeping the other settings the\nsame as DWT.\n2) DATASETS\nFollowing previous work [21], [39], we use DF2K as\nthe training dataset, which consists of 800 images from\nDIV2K [1] and 2560 images from Flicker2K [48].\nWe evaluate our model on Set5 [43], Set14 [44],\nBSD100 [45], Urban100 [46], and Manga109 [47] datasets.\n3) EVALUATION METRICS\nFor evaluation of the SR result, we use PSNR and SSIM [49]\ncomputed on the Y channel of the YCbCr color space. The\nPSNR and SSIM are commonly used full-reference image\nquality assessment (FR-IQA) metrics for image SR. These\nmetrics evaluate the fidelity of the reconstructed image, with\nhigher values indicating better image fidelity. In addition,\nfor the comparison of the perceptual quality, we also utilize\nno-reference image quality assessment (NR-IQA) metrics,\nNIQE [50] and BRISQUE [51]. A lower value of both NIQE\nand BRISQUE indicates higher perceptual quality.\n4) TRAINING DETAILS\nWe generate LR images by downsampling the ground truth\nimages using the ‘‘bicubic’’ method in MATLAB. During\nthe training phase, we randomly crop the LR images into\ninput patches of size 64 × 64 and apply data augmentation\nVOLUME 11, 2023 60033\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nTABLE 2. FR-IQA results comparison with numerous state-of-the-art SR methods. The best and the second-best values are highlighted with red and blue,\nrespectively.\nTABLE 3. Model resource comparison with numerous transformer-based SR methods (×4 SR). Input size is 3× 160 × 160 for Mult-Adds calculation.\ntechniques such as horizontal flip and random rotation. How-\never, during the evaluation phase, the input image size is not\nfixed. Therefore, we employ a reflection padding strategy\non the input image to ensure that the number of windows is\nalways an integer. We use a mini-batch size of 32 and train\nfor a total of 500K iterations, with the learning rate initialized\nat 2e-4 and reduced by half at [250K,400K,450K,475K]. For\n×3 and ×4 SR, we initialize the model with pre-trained ×2\nSR model weights and reduce both the iterations for each\nlearning rate decay and total iterations by half. We adopt\nthe Adam [52] optimizer with β1 = 0.9, β2 = 0.999 and\nzero weight decay to optimize our model. DWT is imple-\nmented on the PyTorch [53] framework with 4 NVIDIA\nRTXA5000 GPUs.\nB. ABLATION STUDY\n1) EFFECTIVENESS OF DWAB DESIGN\nAs described in Section III-D2, we use the dilation strategy\nto extract both local and global features effectively. In our\nDWAB, the dilation value increases as the layers become\ndeeper, resulting in a wider receptive field.\nTo demonstrate the effectiveness of our proposed dilation\nstrategy, we conduct an ablation study. We compare the\ndynamic dilation value strategy, which gradually increases\nthe dilation value and expands the area where dilation is\napplied as the layers become deeper, with the strategy that\nuses a fixed dilation value for all layers. In the fixed dilation\nstrategy, the dilation values of the height and width are set to\nH\nM , and W\nM , respectively for all DWAB, the same as the fifth\nand sixth RTGs in Fig. 3.\nWe evaluate the quantitative performance of both strategies\non five benchmark datasets for ×2 SR. These results are\nshown in Table 1. The results demonstrate that the strategy\nof gradually increasing the dilation value yields better perfor-\nmance on all benchmark datasets. This implies that, in order\nto achieve good performance in SR, extracting both local and\nglobal features are important and the design of our DWT\nmakes it possible to achieve this.\nC. LAM RESULTS COMPARISON\nWe propose a dilation strategy to exploit global information\nin image reconstruction by gradually expanding the receptive\nfield.\n60034 VOLUME 11, 2023\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nTABLE 4. NR-IQA results comparison with numerous transformer-based SR methods. The top three values are highlighted with red, blue and purple,\nrespectively.\nTo analyze whether our dilation strategy works as intended,\nwe use LAM [42]. LAM is a sophisticated attribution method\nfor SR that identifies the input pixels that significantly affect\nthe SR results and quantifies the results into a diffusion\nindex (DI) that evaluates the extraction and utilization of\ninformation from the LR image. A LAM result with a higher\nDI means more pixels are involved in restoring images in\na specific area. Fig. 4 shows the LAM results (DI) and SR\nresults (PSNR/SSIM) for SwinIR, ART, and our DWT. In the\nLAM results, the contribution areas are illustrated in red.\nAs we can see, among the three models, our DWT has the\nhighest DI, PSNR, and SSIM values, and achieves better\nvisual results. Furthermore, we can observe that the red area\nof the DWT’s LAM result is more widely spread than that of\nother models (Fig. 4). This means that DWT can leverage a\nwider range of information than SwinIR and ART. Therefore,\nthese results demonstrate the efficiency of our DWT, which\ncan effectively utilize both local and global information to\nimprove SR performance.\nD. QUANTITATIVE COMPARISON\n1) FR-IQA RESULTS\nTable 2 presents the FR-IQA results comparison between\nour proposed DWT and other state-of-the-art methods,\nincluding EDSR [1], RCAN [4], NLSA [29], SwinIR [21],\nEDT [22], ART-S [39], and ART [39]. As illustrated in\nTable 2, our DWT achieves the best or second-best per-\nformance across all scale factors. Especially, DWT shows\ngreater performance improvement in SSIM metric than in\nPSNR. Since SSIM is a metric that considers the human\nvisual perception system, these results imply that our\nDWT generates higher quality images in terms of human\nperception.\nWe also provide a comparison of the parameter numbers\nand Mult-Adds for transformer-based networks in Table 3.\nThe Mult-Adds are calculated assuming a 3 ×160×160 input\nsize for ×4 SR. As indicated in Table 3, ART has about\n1.4 times more parameters and computational cost than our\nDWT. However, DWT exhibits superior or competitive per-\nformance than ART. The small version of DWT, DWT-S, also\nshows competitive results with less or similar computational\ncost compared to SwinIR and ART-S. When compared to\nstate-of-the-art models of similar sizes, with the exception\nof ART, DWT outperforms all of them in terms of PSNR\nand SSIM. Specifically, while DWT and ART-S have simi-\nlar computational cost, DWT surpasses ART-S by a PSNR\nmargin of up to 0.11dB to 0.27dB on the Urban100 dataset.\nFurthermore, in the ×4 SR results of the Urban100, it can be\nobserved that DWT achieves superior performance not only\ncompared to ART-S but also to ART. The Urban100 dataset\nexhibits a higher disparity in performance compared to other\ndatasets due to its abundance of repeated patterns. These char-\nacteristics of the Urban100 are advantageous for our dilation\nstrategy that enable access distant features. Considering the\nfact that the resolution of the Urban100 is relatively higher\nthan that of Set5 and Set14, it can be inferred that our model\nis much more efficient than ART.\nAll of these results collectively demonstrate the effective-\nness of the DWT, which achieves superior performance with\na competitive number of parameters and acceptable compu-\ntational cost.\n2) NR-IQA RESULTS\nFor a more comprehensive comparison between our DWT\nand other transformer-based networks, we also provide\nNR-IQA results comparison in Table 4. The goal of NR-IQA\nis to estimate the perceptual quality of the image rather\nthan the image fidelity. However, it is debatable whether the\nNR-IQA metrics precisely reflect human perceptual qual-\nity [54], [55]. Hence, we use these two metrics solely\nVOLUME 11, 2023 60035\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nFIGURE 5. Visual comparison (x4) with numerous state-of-the-art SR methods on Set14 and Urban100 datasets. The red boxed areas have been cropped\nfrom the results and enlarged for better visibility.\nas a rough reference. Unlike the FR-IQA results, the\nNR-IQA results show inconsistent performance across meth-\nods, datasets, and scales. However, our DWT shows com-\npetitive performance in ×4 SR results, particularly in terms\nof the BRISUQE. Especially, unlike the FR-IQA results, the\nBRISQUE of DWT on the Manga109 dataset is the best at all\nscales. Although DWT is not the best in both scores, as can\nbe seen from the visual comparison results in the Fig. 5 and\nFig. 6, our DWT shows impressive results in terms of human\nvisual perception.\n60036 VOLUME 11, 2023\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nFIGURE 6. Visual comparison (×4) with numerous state-of-the-art SR methods on Urban100 dataset. The red boxed areas have been cropped from the\nresults and enlarged for better visibility.\nE. VISUAL COMPARISON\nWe also provide visual comparison with state-of-the-art\nmethods in Fig. 5 and Fig. 6. These results demonstrate that\nDWT generates clearer textures and restores high-frequency\ndetails, leading to sharper edges when compared to other\nmethods.\nIn particular, the Urban100 dataset’s ‘‘img_004’’,\n‘‘img_024’’, and ‘‘img_073’’ images serve as excellent exam-\nples that highlight the strengths of our DWT. We observe that\nthese images contain comparable patterns that can be used as\npoints of reference when restoring the areas marked with a\nred box. Despite containing repetitive patterns in the image,\nmost other methods struggle to recover clear structures and\ntend to generate blurry outcomes. In comparison, our DWT\nrecovers more details while reducing blurring artifacts. For\n‘‘img_004’’, DWT utilizes information from both neighbor-\ning and distant regions through its dilation strategy to produce\nresults almost identical to the original. We can find similar\nbehavior on ‘‘img_024’’ in the Urban100 dataset. The red\nboxed area in ‘‘img_024’’ is composed of repeated vertical\nlines. However, it can be observed that SwinIR generates\nblurry results by failing to accurately restore most of the\nvertical lines. In contrast, DWT is able to restore the vertical\nlines relatively sharply. The result of ‘‘image_073’’ also\nhighlights that DWT restores the building’s windows more\nclearly than other models. Overall, our findings demonstrate\nthe superiority of the proposed dilation strategy in producing\nhigh-quality SR results.\nV. CONCLUSION\nIn this paper, we propose a novel dilated window transformer,\nDWT, for image SR that aims to address the limitations of\nwindow-based self-attention. Without introducing additional\ncomputational cost, we employ a dilation strategy to expand\nthe receptive field more quickly and effectively. This sim-\nple yet efficient strategy enables our DWT to extract both\nlocal and global features, leading to improved performance\nin image SR. Extensive experiments under numerous bench-\nmark datasets show the effectiveness of our proposed DWT.\nNotably, DWT records the state-of-the-art SR performance in\nVOLUME 11, 2023 60037\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\nterms of both quantitative and qualitative evaluations with a\ncompetitive number of parameters and reasonable computa-\ntional cost.\nREFERENCES\n[1] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, ‘‘Enhanced deep residual\nnetworks for single image super-resolution,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit. Workshops (CVPRW), Jul. 2017, pp. 1132–1140.\n[2] J. Kim, J. K. Lee, and K. M. Lee, ‘‘Accurate image super-resolution using\nvery deep convolutional networks,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2016, pp. 1646–1654.\n[3] Y. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu, ‘‘Residual dense net-\nwork for image super-resolution,’’ in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit., Jun. 2018, pp. 2472–2481.\n[4] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, ‘‘Image super-\nresolution using very deep residual channel attention networks,’’ in Proc.\nEur. Conf. Comput. Vis. (ECCV), 2018, pp. 286–301.\n[5] T. Dai, J. Cai, Y. Zhang, S. Xia, and L. Zhang, ‘‘Second-order attention\nnetwork for single image super-resolution,’’ in Proc. IEEE/CVF Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jun. 2019, pp. 11057–11066.\n[6] D. Liu, B. Wen, Y. Fan, C. C. Loy, and T. S. Huang, ‘‘Non-local recurrent\nnetwork for image restoration,’’ in Proc. Adv. Neural Inf. Process. Syst.,\nvol. 31, 2018, pp. 1680–1689.\n[7] C. Ledig, L. Theis, F. Huszár, J. Caballero, A. Cunningham, A. Acosta,\nA. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, ‘‘Photo-realistic\nsingle image super-resolution using a generative adversarial network,’’\nin Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017,\npp. 105–114.\n[8] X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, Y. Qiao, and C. C. Loy,\n‘‘ESRGAN: Enhanced super-resolution generative adversarial networks,’’\nin Proc. Eur. Conf. Comput. Vis. (ECCV) Workshops, 2018, pp. 63–79.\n[9] X. Wang, L. Xie, C. Dong, and Y. Shan, ‘‘Real-ESRGAN: Training real-\nworld blind super-resolution with pure synthetic data,’’ in Proc. IEEE/CVF\nInt. Conf. Comput. Vis. Workshops (ICCVW), Oct. 2021, pp. 1905–1914.\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ in Proc. Adv.\nNeural Inf. Process. Syst., vol. 30, 2017, pp. 5998–6008.\n[11] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, ‘‘An image is worth 16×16 words: Trans-\nformers for image recognition at scale,’’ 2020, arXiv:2010.11929.\n[12] A. Vaswani, P. Ramachandran, A. Srinivas, N. Parmar, B. Hechtman,\nand J. Shlens, ‘‘Scaling local self-attention for parameter efficient visual\nbackbones,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2021, pp. 12889–12899.\n[13] Y. Li, K. Zhang, J. Cao, R. Timofte, and L. Van Gool, ‘‘LocalViT: Bringing\nlocality to vision transformers,’’ 2021, arXiv:2104.05707.\n[14] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, ‘‘End-to-end object detection with transformers,’’ in\nProc. 16th Eur. Conf. Comput. Vis. (ECCV), Glasgow, U.K. Cham,\nSwitzerland: Springer, Aug. 2020, pp. 213–229.\n[15] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n‘‘Swin transformer: Hierarchical vision transformer using shifted win-\ndows,’’ in Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 9992–10002.\n[16] Z. Xia, X. Pan, S. Song, L. E. Li, and G. Huang, ‘‘Vision transformer\nwith deformable attention,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2022, pp. 4784–4793.\n[17] R. Ranftl, A. Bochkovskiy, and V. Koltun, ‘‘Vision transformers for dense\nprediction,’’ inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), Oct. 2021,\npp. 12159–12168.\n[18] H. Cao, Y. Wang, J. Chen, D. Jiang, X. Zhang, Q. Tian, and M. Wang,\n‘‘Swin-Unet: Unet-like pure transformer for medical image segmentation,’’\nin Computer Vision—ECCV 2022 Workshops, Tel Aviv, Israel. Cham,\nSwitzerland: Springer, Oct. 2022, pp. 205–218.\n[19] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, ‘‘Pre-trained image processing transformer,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2021,\npp. 12294–12305.\n[20] J. Cao, Y. Li, K. Zhang, J. Liang, and L. Van Gool, ‘‘Video super-resolution\ntransformer,’’ 2021, arXiv:2106.06847.\n[21] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, ‘‘SwinIR:\nImage restoration using Swin transformer,’’ in Proc. IEEE/CVF Int. Conf.\nComput. Vis. Workshops (ICCVW), Oct. 2021, pp. 1833–1844.\n[22] W. Li, X. Lu, S. Qian, J. Lu, X. Zhang, and J. Jia, ‘‘On efficient transformer-\nbased image pre-training for low-level vision,’’ 2021, arXiv:2112.10175.\n[23] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘ImageNet:\nA large-scale hierarchical image database,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2009, pp. 248–255.\n[24] X. Chen, X. Wang, J. Zhou, Y. Qiao, and C. Dong, ‘‘Activating more pixels\nin image super-resolution transformer,’’ 2022, arXiv:2205.04437.\n[25] F. Yu and V. Koltun, ‘‘Multi-scale context aggregation by dilated convolu-\ntions,’’ 2015, arXiv:1511.07122.\n[26] C. Dong, C. C. Loy, K. He, and X. Tang, ‘‘Image super-resolution using\ndeep convolutional networks,’’ IEEE Trans. Pattern Anal. Mach. Intell.,\nvol. 38, no. 2, pp. 295–307, Feb. 2016.\n[27] C. Dong, C. C. Loy, K. He, and X. Tang, ‘‘Learning a deep convolutional\nnetwork for image super-resolution,’’ in Proc. 13th Eur. Conf. Comput. Vis.\n(ECCV), Zurich, Switzerland. Cham, Switzerland: Springer, Sep. 2014,\npp. 184–199.\n[28] B. Niu, W. Wen, W. Ren, X. Zhang, L. Yang, S. Wang, K. Zhang, X. Cao,\nand H. Shen, ‘‘Single image super-resolution via a holistic attention net-\nwork,’’ in Proc. 16th Eur. Conf. Comput. Vis. (ECCV), Glasgow, U.K.\nCham, Switzerland: Springer, Aug. 2020, pp. 191–207.\n[29] Y. Mei, Y. Fan, and Y. Zhou, ‘‘Image super-resolution with non-local\nsparse attention,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2021, pp. 3516–3525.\n[30] Y. Tai, J. Yang, and X. Liu, ‘‘Image super-resolution via deep recursive\nresidual network,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jul. 2017, pp. 2790–2798.\n[31] S. Zhou, J. Zhang, W. Zuo, and C. C. Loy, ‘‘Cross-scale internal graph\nneural network for image super-resolution,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., vol. 33, 2020, pp. 3499–3509.\n[32] J. Yoo, T. Kim, S. Lee, S. H. Kim, H. Lee, and T. H. Kim, ‘‘Enriched\nCNN-transformer feature aggregation networks for super-resolution,’’ in\nProc. IEEE/CVF Winter Conf. Appl. Comput. Vis. (WACV), Jan. 2023,\npp. 4945–4954.\n[33] B. Wu, C. Xu, X. Dai, A. Wan, P. Zhang, Z. Yan, M. Tomizuka, J. Gonzalez,\nK. Keutzer, and P. Vajda, ‘‘Visual transformers: Token-based image repre-\nsentation and processing for computer vision,’’ 2020, arXiv:2006.03677.\n[34] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou,\n‘‘Training data-efficient image transformers & distillation through atten-\ntion,’’ in Proc. Int. Conf. Mach. Learn., 2021, pp. 10347–10357.\n[35] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, ‘‘Deformable\nDETR: Deformable transformers for end-to-end object detection,’’ 2020,\narXiv:2010.04159.\n[36] X. Chu, Z. Tian, Y. Wang, B. Zhang, H. Ren, X. Wei, H. Xia, and C. Shen,\n‘‘Twins: Revisiting the design of spatial attention in vision transformers,’’\nin Proc. Adv. Neural Inf. Process. Syst., vol. 34, 2021, pp. 9355–9366.\n[37] G. Huang, Y. Wang, K. Lv, H. Jiang, W. Huang, P. Qi, and S. Song, ‘‘Glance\nand focus networks for dynamic visual recognition,’’ IEEE Trans. Pattern\nAnal. Mach. Intell., vol. 45, no. 4, pp. 4605–4621, Apr. 2023.\n[38] W. Wang, E. Xie, X. Li, D. Fan, K. Song, D. Liang, T. Lu, P. Luo, and\nL. Shao, ‘‘Pyramid vision transformer: A versatile backbone for dense\nprediction without convolutions,’’ in Proc. IEEE/CVF Int. Conf. Comput.\nVis. (ICCV), Oct. 2021, pp. 548–558.\n[39] J. Zhang, Y. Zhang, J. Gu, Y. Zhang, L. Kong, and X. Yuan, ‘‘Accu-\nrate image restoration with attention retractable transformer,’’ 2022,\narXiv:2210.01427.\n[40] T. Xiao, M. Singh, E. Mintun, T. Darrell, P. Dollár, and R. Girshick,\n‘‘Early convolutions help transformers see better,’’ in Proc. Adv. Neural\nInf. Process. Syst., vol. 34, 2021, pp. 30392–30400.\n[41] W. Shi, J. Caballero, F. Huszár, J. Totz, A. P. Aitken, R. Bishop,\nD. Rueckert, and Z. Wang, ‘‘Real-time single image and video super-\nresolution using an efficient sub-pixel convolutional neural network,’’ in\nProc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , Jun. 2016,\npp. 1874–1883.\n[42] J. Gu and C. Dong, ‘‘Interpreting super-resolution networks with local attri-\nbution maps,’’ in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n(CVPR), Jun. 2021, pp. 9195–9204.\n[43] M. Bevilacqua, A. Roumy, C. Guillemot, and M.-L.-A. Morel, ‘‘Low-\ncomplexity single-image super-resolution based on nonnegative neighbor\nembedding,’’ in Proc. Brit. Mach. Vis. Conf., 2012, pp. 135.1–135.10.\n60038 VOLUME 11, 2023\nS. Park, Y. S. Choi: Image SR Using Dilated Window Transformer\n[44] R. Zeyde, M. Elad, and M. Protter, ‘‘On single image scale-up using sparse-\nrepresentations,’’ in Proc. 7th Int. Conf. Curves Surf., Avignon, France.\nBerlin, Germany: Springer, 2012, pp. 711–730.\n[45] D. Martin, C. Fowlkes, D. Tal, and J. Malik, ‘‘A database of human\nsegmented natural images and its application to evaluating segmentation\nalgorithms and measuring ecological statistics,’’ in Proc. 8th IEEE Int.\nConf. Comput. Vision. (ICCV), vol. 2, Jul. 2001, pp. 416–423.\n[46] J. Huang, A. Singh, and N. Ahuja, ‘‘Single image super-resolution from\ntransformed self-exemplars,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit. (CVPR), Jun. 2015, pp. 5197–5206.\n[47] Y. Matsui, K. Ito, Y. Aramaki, A. Fujimoto, T. Ogawa, T. Yamasaki,\nand K. Aizawa, ‘‘Sketch-based Manga retrieval using manga109 dataset,’’\nMultimedia Tools Appl., vol. 76, no. 20, pp. 21811–21838, Oct. 2017.\n[48] R. Timofte, E. Agustsson, L. Van Gool, M.-H. Yang, and L. Zhang,\n‘‘NTIRE 2017 challenge on single image super-resolution: Methods and\nresults,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. Workshops\n(CVPRW), Jul. 2017, pp. 1110–1121.\n[49] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, ‘‘Image quality\nassessment: From error visibility to structural similarity,’’ IEEE Trans.\nImage Process., vol. 13, no. 4, pp. 600–612, Apr. 2004.\n[50] A. Mittal, R. Soundararajan, and A. C. Bovik, ‘‘Making a ‘completely\nblind’ image quality analyzer,’’ IEEE Signal Process. Lett., vol. 20, no. 3,\npp. 209–212, Nov. 2012.\n[51] A. Mittal, A. K. Moorthy, and A. C. Bovik, ‘‘Referenceless image spatial\nquality evaluation engine,’’ in Proc. 45th Asilomar Conf. Signals, Syst.\nComput., vol. 38, Nov. 2011, pp. 53–54.\n[52] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\n2014, arXiv:1412.6980.\n[53] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin,\nA. Desmaison, L. Antiga, and A. Lerer, ‘‘Automatic differentiation in\nPyTorch,’’ in Proc. NIPS Workshop Autodiff, 2017.\n[54] J. W. Soh, G. Y. Park, J. Jo, and N. I. Cho, ‘‘Natural and realistic single\nimage super-resolution with explicit natural manifold discrimination,’’ in\nProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), Jun. 2019,\npp. 8114–8123.\n[55] A. Lugmayr, M. Danelljan, and R. Timofte, ‘‘NTIRE 2020 challenge\non real-world image super-resolution: Methods and results,’’ in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit. Workshops (CVPRW),\nJun. 2020, pp. 2058–2076.\nSOOBIN PARK was born in Republic of Korea,\nin 1996. She received the B.S. degree in IT\nconvergence from Hanyang University, Seoul,\nSouth Korea, in 2022, where she is currently pursu-\ning the M.S. degree with the Department of Arti-\nficial Intelligence. Her research interests include\ncomputer vision and image super-resolution.\nYONG SUK CHOIwas born in Republic of Korea,\nin 1969. He received the B.S., M.S., and Ph.D.\ndegrees in computer science from Seoul National\nUniversity, Seoul, South Korea, in 1993, 1995, and\n2000, respectively. From 1997 to 2001, he was\nwith the Telecommunication Research Laboratory,\nSamsung Electronics Company. In 2001, he joined\nHanyang University, Seoul, where he is currently\na Professor with the Department of Computer\nScience and Engineering. His research interests\ninclude ontology, knowledge-based systems, computer vision, dialogue gen-\neration systems, social media analysis and visualization, and multi-modal.\nVOLUME 11, 2023 60039",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8211102485656738
    },
    {
      "name": "Dilation (metric space)",
      "score": 0.7052401304244995
    },
    {
      "name": "Transformer",
      "score": 0.6097179651260376
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5660269856452942
    },
    {
      "name": "Feature extraction",
      "score": 0.5301255583763123
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.4802394509315491
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.4622255563735962
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4196486473083496
    },
    {
      "name": "Computer vision",
      "score": 0.3661049008369446
    },
    {
      "name": "Voltage",
      "score": 0.12489911913871765
    },
    {
      "name": "Machine learning",
      "score": 0.116679847240448
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4575257",
      "name": "Hanyang University",
      "country": "KR"
    }
  ],
  "cited_by": 7
}