{
  "title": "On negative results when using sentiment analysis tools for software engineering research",
  "url": "https://openalex.org/W2571312737",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2902075811",
      "name": "Robbert Jongeling",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2103697348",
      "name": "Proshanta Sarkar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102467351",
      "name": "Subhajit Datta",
      "affiliations": [
        "Singapore University of Technology and Design"
      ]
    },
    {
      "id": "https://openalex.org/A2047031519",
      "name": "Alexander Serebrenik",
      "affiliations": [
        "Eindhoven University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2902075811",
      "name": "Robbert Jongeling",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103697348",
      "name": "Proshanta Sarkar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102467351",
      "name": "Subhajit Datta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047031519",
      "name": "Alexander Serebrenik",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3144363571",
    "https://openalex.org/W4231371247",
    "https://openalex.org/W2159730313",
    "https://openalex.org/W1512798931",
    "https://openalex.org/W2020800417",
    "https://openalex.org/W2023110617",
    "https://openalex.org/W2103562041",
    "https://openalex.org/W2037789405",
    "https://openalex.org/W2076019529",
    "https://openalex.org/W2061419337",
    "https://openalex.org/W1967432327",
    "https://openalex.org/W2141850512",
    "https://openalex.org/W2213700299",
    "https://openalex.org/W4251494148",
    "https://openalex.org/W2061554433",
    "https://openalex.org/W4232691406",
    "https://openalex.org/W2150290224",
    "https://openalex.org/W2167804243",
    "https://openalex.org/W2083824326",
    "https://openalex.org/W2158814886",
    "https://openalex.org/W1573641422",
    "https://openalex.org/W1983175439",
    "https://openalex.org/W2141012982",
    "https://openalex.org/W1988762048",
    "https://openalex.org/W4231947862",
    "https://openalex.org/W2043223258",
    "https://openalex.org/W2032959067",
    "https://openalex.org/W2133990480",
    "https://openalex.org/W2147199018",
    "https://openalex.org/W2040043446",
    "https://openalex.org/W4235169531",
    "https://openalex.org/W2498120566",
    "https://openalex.org/W2125886624",
    "https://openalex.org/W2063456204",
    "https://openalex.org/W2145456657",
    "https://openalex.org/W1968904436",
    "https://openalex.org/W2095163973",
    "https://openalex.org/W2010484422",
    "https://openalex.org/W2127898555",
    "https://openalex.org/W3144129431",
    "https://openalex.org/W1999830936",
    "https://openalex.org/W2000471262",
    "https://openalex.org/W2009682804",
    "https://openalex.org/W4249127983",
    "https://openalex.org/W2060872734",
    "https://openalex.org/W222426624",
    "https://openalex.org/W2166706824",
    "https://openalex.org/W2126574721",
    "https://openalex.org/W2146649871",
    "https://openalex.org/W2013594612",
    "https://openalex.org/W2033403400",
    "https://openalex.org/W4245902261",
    "https://openalex.org/W1523794535",
    "https://openalex.org/W2085782462",
    "https://openalex.org/W1996551380",
    "https://openalex.org/W3150619302",
    "https://openalex.org/W2113533445",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W285841728",
    "https://openalex.org/W1964045860",
    "https://openalex.org/W1987425720",
    "https://openalex.org/W4239946314",
    "https://openalex.org/W1971875656",
    "https://openalex.org/W1590495275",
    "https://openalex.org/W2545778708",
    "https://openalex.org/W2065849599",
    "https://openalex.org/W2004142432",
    "https://openalex.org/W4239343171",
    "https://openalex.org/W2105916773",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W2022204871",
    "https://openalex.org/W2288177242",
    "https://openalex.org/W1974384549",
    "https://openalex.org/W2143177027",
    "https://openalex.org/W2097726431",
    "https://openalex.org/W2009543464",
    "https://openalex.org/W2949709688",
    "https://openalex.org/W1565420920",
    "https://openalex.org/W3105265400",
    "https://openalex.org/W1521626219",
    "https://openalex.org/W1972034440",
    "https://openalex.org/W2165044314",
    "https://openalex.org/W2005311637",
    "https://openalex.org/W2202473840",
    "https://openalex.org/W1998470619",
    "https://openalex.org/W1989479444",
    "https://openalex.org/W2147801848",
    "https://openalex.org/W2025287599",
    "https://openalex.org/W1444168786",
    "https://openalex.org/W2963790016",
    "https://openalex.org/W2252197266",
    "https://openalex.org/W3105551542",
    "https://openalex.org/W2074637729",
    "https://openalex.org/W2079972013",
    "https://openalex.org/W2018851841",
    "https://openalex.org/W2020703761",
    "https://openalex.org/W2085989833",
    "https://openalex.org/W1972386298",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W193524605",
    "https://openalex.org/W1743243001",
    "https://openalex.org/W1574454071"
  ],
  "abstract": null,
  "full_text": "Empir Software Eng (2017) 22:2543–2584\nDOI 10.1007/s10664-016-9493-x\nOn negative results when using sentiment analysis tools\nfor software engineering research\nRobbert Jongeling 1 ·Proshanta Sarkar 2 ·\nSubhajit Datta 3 ·Alexander Serebrenik 1\nPublished online: 10 January 2017\n© The Author(s) 2017. This article is published with open access at Springerlink.com\nAbstract Recent years have seen an increasing attention to social aspects of software engi-\nneering, including studies of emotions and sentiments experienced and expressed by the\nsoftware developers. Most of these studies reuse existing sentiment analysis tools such as\nSENTI STRENGTH and NLTK. However, these tools have been trained on product reviews\nand movie reviews and, therefore, their results might not be applicable in the software engi-\nneering domain. In this paper we study whether the sentiment analysis tools agree with the\nsentiment recognized by human evaluators (as reported in an earlier study) as well as with\neach other. Furthermore, we evaluate the impact of the choice of a sentiment analysis tool\non software engineering studies by conducting a simple study of differences in issue reso-\nlution times for positive, negative and neutral texts. We repeat the study for seven datasets\n(issue trackers and S\nTACK OVERFLOW questions) and different sentiment analysis tools and\nobserve that the disagreement between the tools can lead to diverging conclusions. Finally,\nwe perform two replications of previously published studies and observe that the results of\nthose studies cannot be confirmed when a different sentiment analysis tool is used.\nCommunicated by: Richard Paige, Jordi Cabot and Neil Ernst\n/envelopebackAlexander Serebrenik\na.serebrenik@tue.nl\nRobbert Jongeling\nr.m.jongeling@alumnus.tue.nl\nProshanta Sarkar\nproshant.cse@gmail.com\nSubhajit Datta\nsubhajit.datta@acm.org\n1 Eindhoven University of Technology, Eindhoven, The Netherlands\n2 IBM India Private Limited, Kolkata, India\n3 Singapore University of Technology and Design, Singapore, Singapore\n2544 Empir Software Eng (2017) 22:2543–2584\nKeywords Sentiment analysis tools · Replication study · Negative results\n1 Introduction\nSentiment analysis is “the task of identifying positive and negative opinions, emotions, and\nevaluations” (Wilson et al. 2005). Since its inception sentiment analysis has been subject\nof an intensive research effort and has been successfully applied e.g., to assist users in\ntheir development by providing them with interesting and supportive content (Honkela et al.\n2012), predict the outcome of an election (Tumasjan et al. 2010) or movie sales (Mishne\nand Glance 2006). The spectrum of sentiment analysis techniques ranges from identifying\npolarity (positive or negative) to a complex computational treatment of subjectivity, opinion\nand sentiment (Pang and Lee 2007). In particular, the research on sentiment polarity analysis\nhas resulted in a number of mature and publicly available tools such as S\nENTI STRENGTH\n(Thelwall et al. 2010), Alchemy,1 Stanford NLP sentiment analyser (Socher et al. 2013)a n d\nNLTK (Bird et al. 2009).\nIn recent times, large scale software development has become increasingly social. With\nthe proliferation of collaborative development environments, discussion between developers\nare recorded and archived to an extent that could not be conceived before. The availability of\nsuch discussion materials makes it easy to study whether and how the sentiments expressed\nby software developers influence the outcome of development activities. With this back-\nground, we apply sentiment polarity analysis to several software development ecosystems\nin this study.\nSentiment polarity analysis has been recently applied in the software engineering context\nto study commit comments in GitHub (Guzman et al. 2014), GitHub discussions related to\nsecurity (Pletea et al. 2014), productivity in Jira issue resolution (Ortu et al. 2015), activity\nof contributors in Gentoo (Garcia et al. 2013), classification of user reviews for mainte-\nnance and evolution (Panichella et al. 2015) and evolution of developers’ sentiments in the\nopenSUSE Factory (Rousinopoulos et al. 2014). It has also been suggested when assess-\ning technical candidates on the social web (Capiluppi et al. 2013). Not surprisingly, all\nthe aforementioned software engineering studies with the notable exception of the work\nby Panichella et al. (2015), reuse the existing sentiment polarity tools, e.g., (Pletea et al.\n2014) and (Rousinopoulos et al. 2014) use NLTK, while (Garcia et al. 2013; Guzman and\nBruegge 2013;G u z m a ne ta l . 2014; Novielli et al. 2015) and (Ortu et al. 2015) opted for\nS\nENTI STRENGTH . While the reuse of the existing tools facilitated the application of the sen-\ntiment polarity analysis techniques in the software engineering domain, it also introduced\na commonly recognized threat to validity of the results obtained: those tools have been\ntrained on non-software engineering related texts such as movie reviews or product reviews\nand might misidentify (or fail to identify) polarity of a sentiment in a software engineering\nartefact such as a commit comment (Guzman et al. 2014; Pletea et al. 2014).\nTherefore, in this paper we focus on sentiment polarity analysis (Wilson et al. 2005)a n d\ninvestigate to what extent are the software engineering results obtained from sentiment anal-\nysis depend on the choice of the sentiment analysis tool. We recognize that there are multiple\nways to measure outcomes in software engineering. Among them, time to resolve a partic-\nular defect, and/or respond to a particular query are relevant for end users. Accordingly, in\n1http://www.alchemyapi.com/products/alchemylanguage/sentiment-analysis/\nEmpir Software Eng (2017) 22:2543–2584 2545\nthe different data-sets studied in this paper, we have taken such resolution or response times\nto reflect the outcomes of our interest.\nFor the sake of simplicity, from here on, instead of “existing sentiment polarity analysis\ntools” we talk about the “sentiment analysis tools”. Specifically, we aim at answering the\nfollowing questions:\n– RQ1: To what extent do different sentiment analysis tools agree with emotions of\nsoftware developers?\n– RQ2: To what extent do results from different sentiment analysis tools agree with each\nother?\nWe have observed disagreement between sentiment analysis tools and the emotions of soft-\nware developers but also between different sentiment analysis tools themselves. However,\ndisagreement between the tools does not ap r i o r i mean that sentiment analysis tools might\nlead to contradictory results in software engineering studies making use of these tools. Thus,\nwe ask\n– RQ3: Do different sentiment analysis tools lead to contradictory results in a software\nengineering study?\nWe have observed that disagreement between the tools might lead to contradictory results\nin software engineering studies. Therefore, we finally conduct replication studies in order\nto understand:\n– RQ4: How does the choice of a sentiment analysis tool affect validity of the previously\npublished results?\nThe remainder of this paper is organized as follows. The next section outlines the sen-\ntiment analysis tools we have considered in this study. In Section 3 we study agreement\nbetween the tools and the results of manual labeling, and between the tools themselves, i.e.,\nRQ1 and RQ2. In Section 4 we conduct a series of studies based on the results of different\nsentiment analysis tools. We observe that conclusions one might derive using different tools\ndiverge, casting doubt on their validity (RQ3). While our answer to RQ3 indicates that the\nchoice of a sentiment analysis tool might affect validity of software engineering results, in\nSection 5 we perform replication of two published studies answering RQ4 and establishing\nthat conclusions of previously published works cannot be reproduced when a different sen-\ntiment analysis tool is used. Finally, in Section 6 we discuss related work and conclude in\nSection 7.\nSource code and data used to obtain the results of this paper has been made available.\n2\n2 Sentiment Analysis T ools\n2.1 T ool Selection\nTo perform the tool evaluation we have decided to focus on open-source tools. This require-\nment excludes such commercial tools as Lymbix\n3 Sentiment API of MeaningCloud 4 or\n2http://ow.ly/HvC5302N4oK\n3http://www.lymbix.com/supportcenter/docs\n4https://www.meaningcloud.com/developer/sentiment-analysis\n2546 Empir Software Eng (2017) 22:2543–2584\nGetSentiment.5 Furthermore, we exclude tools that require training before they can be\napplied such as LibShortText (Yu et al. 2013) or sentiment analysis libraries of popular\nmachine learning tools such as RapidMiner or Weka. Finally, since the software engineering\ntexts that have been analyzed in the past can be quite short (JIRA issues, S TACK OVER -\nFLOW questions), we have chosen tools that have already been applied either to software\nengineering texts (S ENTI STRENGTH and NLTK) or to short texts such as tweets (Alchemy\nor Stanford NLP sentiment analyser).\n2.2 Description of T ools\n2.2.1 SENTI STRENGTH\nSENTI STRENGTH is the sentiment analysis tool most frequently used in software engineer-\ning studies (Garcia et al. 2013;G u z m a ne ta l . 2014; Novielli et al. 2015;O r t ue ta l .2015).\nMoreover, S ENTI STRENGTH had the highest average accuracy among fifteen Twitter senti-\nment analysis tools (Abbasi et al. 2014). S ENTI STRENGTH assigns an integer value between\n1 and 5 for the positivity of a text, p and similarly, a value between −1a n d −5f o rt h e\nnegativity, n.\nInterpretation In order to map the separate positivity and negativity scores to a senti-\nment (positive, neutral or negative) for an entire text fragment, we follow the approach by\nThelwall et al. ( 2012). A text is considered positive when p + n> 0, negative when\np + n< 0, and neutral if p =− n and p< 4. Texts with a score of p =− n and p ≥ 4a r e\nconsidered having an undetermined sentiment and are removed from the datasets.\n2.2.2 Alchemy\nAlchemy provides several text processing APIs, including a sentiment analysis API which\npromises to work on very short texts (e.g., tweets) as well as relatively long texts (e.g., news\narticles).\n6 The sentiment analysis API returns for a text fragment a status, a language, a\nscore and a type. The score is in the range [−1, 1],t h e type is the sentiment of the text and is\nbased on the score. For negative scores, the type is negative, conversely for positive scores,\nthe type is positive. For a score of 0, the type is neutral. The status reflects the analysis\nsuccess and it is either “OK” or “ERROR”.\nInterpretation We ignore texts with status “ERROR” or a non-English language. For the\nremaining texts we consider them as being negative, neutral or positive as indicated by the\nreturned type.\n2.2.3 NLTK\nNLTK has been applied in earlier software engineering studies (Pletea et al. 2014;\nRousinopoulos et al. 2014). NLTK uses a simple bag of words model and returns for each\n5https://getsentiment.3scale.net/\n6http://www.alchemyapi.com/products/alchemylanguage/sentiment-analysis\nEmpir Software Eng (2017) 22:2543–2584 2547\ntext three probabilities: a probability of the text being negative, one of it being neutral and\none of it being positive. To call NLTK, we use the API provided at text-processing.com. 7\nInterpretation If the probability score for neutral is greater than 0.5, the text is considered\nneutral. Otherwise, it is considered to be the other sentiment with the highest probability\n(Pletea et al. 2014).\n2.2.4 Stanford NLP\nThe Stanford NLP parses the text into sentences and performs a more advanced grammatical\nanalysis as opposed to a simpler bag of words model used, e.g., in NLTK. Indeed, Socher\net al. argue that such an analysis should outperform the bag of words model on short texts\n(Socher et al. 2013). The Stanford NLP breaks down the text into sentences and assigns\neach a sentiment score in the range [0, 4], where 0 is very negative, 2 is neutral and 4 is\nvery positive. We note that the tool may have difficulty breaking the text into sentences\nas comments sometimes include pieces of code or e.g. URLs. The tool does not provide a\ndocument-level score.\nInterpretation To determine a document-level sentiment we compute −2∗#0−#1+#3+\n2 ∗#4, where #0 denotes the number of sentences with score 0, etc.. If this score is negative,\nneutral or positive, we consider the text to be negative, neutral or positive, respectively.\n3 Agreement Between Sentiment Analysis T ools\nIn this section we address RQ1 and RQ2, i.e., to what extent do the different sentiment\nanalysis tools described earlier, agree with emotions of software developers and to what\nextent do different sentiment analysis tools agree with each other. To perform the evaluation\nwe use the manually labeled emotions dataset (Murgia et al. 2014).\n3.1 Methodology\n3.1.1 Manually-Labeled Software Engineering Data\nAs the “golden set” we use the data from a developer emotions study by Murgia et al.\n(2014). In this study, four evaluators manually labeled 392 comments with emotions “joy”,\n“love”, “surprise”, “anger”, “sadness” or “fear”. Emotions “joy” and“love” are taken as\nindicators of positive sentiments and “anger”, “sadness” and “fear”—of negative sentiment.\nWe exclude information about the “surprise” sentiment, since surprises can be, in general,\nboth positive and negative depending on the expectations of the speaker.\nWe focus on consistently labeled comments. We consider the comment as positive if at\nleast three evaluators have indicated a positive sentiment and no evaluator has indicated\nnegative sentiments. Similarly, we consider the comment as negative if at least three evalua-\ntors have indicated a negative sentiment and no evaluator has indicated positive sentiments.\nFinally, a text is considered as neutral when three or more evaluators have neither indicated\na positive sentiment nor a negative sentiment.\n7API docs for NLTK sentiment analysis: http://text-processing.com/docs/sentiment.html\n2548 Empir Software Eng (2017) 22:2543–2584\nUsing these rules we can conclude that 265 comments have been labeled consistently:\n19 negative, 41 positive and 205 neutral. The remaining 392 − 265 = 127 comments from\nthe study Murgia et al. ( 2014) have been labeled with contradictory labels e.g. “fear” by one\nevaluator and “joy” by another.\n3.1.2 Evaluation Metrics\nSince more than 77 % of the comments have been manually labeled as neutral, i.e., the\ndataset is unbalanced, traditional metrics such as accuracy might be misleading (Batista\net al. 2000): indeed, accuracy of the straw man sentiment analysis predicting “neutral” for\nany comment can be easily higher than of any of the four tools. Therefore, rather than\nreporting accuracy of the approaches we use the Weighted kappa (Cohen 1968)a n dt h e\nAdjusted Rand Index (ARI) (Hubert and Arabie 1985; Santos and Embrechts 2009). For the\nsake of completeness we report the F-measures for the three categories of sentiments.\nKappa is a measure of interrater agreement. As recommended by Bakeman and Gottman\n(Bakeman and Gottman 1997, p. 66) we opt for the weighted kappa (κ) since the sentiments\ncan be seen as ordered, from positive through neutral to negative, and disagreement between\npositive and negative is more “severe” than between positive and neutral or negative and\nneutral. Our weighting scheme, also following the guidelines of Bakeman and Gottman,\nis shown in Table 1. We follow the interpretation of κ as advocated by Viera and Garrett\n(Viera and Garrett 2005) since it is more fine grained than, e.g., the one suggested by Fleiss\net al. ( 2003, p. 609). We say that the agreement is less than chance if κ ≤ 0, slight if\n0.01 ≤ κ ≤ 0.20, fair if 0.21 ≤ κ ≤ 0.40, moderate if 0 .41 ≤ κ ≤ 0.60, substantial if\n0.61 ≤ κ ≤ 0.80 and almost perfect if 0.81 ≤ κ ≤ 1. To answer the first research question\nwe look for the agreement between the tool and the manual labeling; to answer the second\none—for agreement between two tools.\nARI measures the correspondence between two partitions of the same data. Similarly to\nthe Rand index (Rand 1971), ARI evaluates whether pairs of observations (comments) are\nconsidered as belonging to the same category (sentiment) rather than on whether observa-\ntions (comments) have been assigned to correct classes (sentiment). As opposed to the Rand\nindex, ARI corrects for the possibility that pairs of observations have been put in the same\ncategory by chance. The expected value of ARI ranges for independent partitions is 0. The\nmaximal value, obtained e.g., for identical partitions is 1, the closer the value of ARI to 1 the\nbetter the correspondence between the partitions. To answer the first research question we\nlook for the correspondence between the partition of the comments into positive, neutral and\nnegative groups provided by the tool and the partition based on the manual labeling. Simi-\nlarly, to answer the second research question we look for correspondence between partition\nof the comments into positive, neutral and negative groups provided by different tools.\nFinally, F-measure, introduced by Lewis and Gale ( 1994) based on the earlier E-measure\nof Van Rijsbergen ( 1979, p. 128), is the harmonic mean of the precision and recall. Recall\nthat precision in the classification context is the ratio of true positives\n8 and all entities pre-\ndicted to be positive, while recall is the ratio of true positives and all entities known to be\npositive. The symmetry between precision and recall, false positives and false negatives,\ninherent in the F-measure makes it applicable both when addressing RQ1 and when address-\ning RQ2. We report the F-measure separately for the three classes: neutral, positive and\nnegative.\n8Here “positive” is not related to the positive sentiment.\nEmpir Software Eng (2017) 22:2543–2584 2549\nTa b l e 1 Weighting scheme for\nthe weighted kappa computation positive neutral negative\npositive 0 1 2\nneutral 1 0 1\nnegative 2 1 0\n3.2 Results\nNone of the 265 consistently labeled comments produce S ENTI STRENGTH results with\np =− n and p ≥ 4. Three comments produce the “ERROR” status with Alchemy;\nthose comments have been excluded from consideration. We exclude those comments from\nconsideration and report κ and ARI for 262 comments.\nResults obtained both for RQ1 and for RQ2 are summarized in Table 2 . Detailed confu-\nsion matrices relating the results of the tools and the manual labeling as well as results of\ndifferent tools to each other are presented in Appendix A.\n3.3 Discussion\nOur results clearly indicate that the sentiment analysis tools do not agree with the manual\nlabeling and neither do they agree with each other.\nRQ1 As can be observed from Table 2 both κ and ARI show that the tools are quite far\nfrom agreeing with the manual labeling: κ is merely fair, and ARI is low. NLTK scores best,\nfollowed by S ENTI STRENGTH , and both perform better than Alchemy and Stanford NLP.\nEven when focusing solely on the positive and the negative sentiment, the F-values suggest\nthat improving the F-value for the negative sentiments tends to decrease the F-value for the\npositive ones, and vice versa.\nRQ2 Values of κ and ARI obtained when different tools have been compared are even\nlower when compared to the results of the agreement with the manual labeling. The highest\nTa b l e 2 Agreement of sentiment analysis tools with the manual labeling and with each other\nF\nTools κ ARI neu pos neg\nNLTK vs. manual 0.33 0.21 0.76 0.53 0.31\nSENTI STRENGTH vs. manual 0.31 0.13 0.73 0.47 0.35\nAlchemy vs. manual 0.26 0.07 0.53 0.54 0.23\nStanford NLP vs. manual 0.20 0.11 0.48 0.53 0.20\nNLTK vs. S ENTI STRENGTH 0.22 0.08 0.64 0.45 0.33\nNLTK vs. Alchemy 0.20 0.09 0.52 0.60 0.44\nNLTK vs. Stanford NLP 0.12 0.05 0.48 0.42 0.47\nSENTI STRENGTH vs. Alchemy 0.07 0.07 0.56 0.55 0.38\nSENTI STRENGTH vs. Stanford NLP −0.14 0.00 0.51 0.33 0.35\nAlchemy vs. Stanford NLP 0.25 0.05 0.41 0.43 0.58\n2550 Empir Software Eng (2017) 22:2543–2584\nvalue of κ, 0.25, has been obtained for Alchemy and Stanford NLP, and is only fair. Agree-\nment between NLTK and S ENTI STRENGTH is, while also only fair, the second highest one\namong the six possible pairs in Table 2.\nTo illustrate the reasons for the disagreement between the tools and the manual labeling\nas well as between the tools themselves we discuss a number of example comments.\nExample 1 Our first example is a developer describing a clearly undesirable behavior\n(memory leak) in Apache UIMA. The leak, however, has been fixed; the developer confirms\nthis and thanks the community.\nTo test this I used an aggregate AE with a CAS multiplier that declared getCasIn-\nstancesRequired()=5. If this AE is instantiated and run in a loop with earlier code it\neats up roughly 10MB per iteration. No such leak with the latest code. Thanks!\nDue to presence of the expression of gratitude, the comment has been labeled as “love” by\nall four participants of the Murgia’s study. We interpret this as a clear indication of the posi-\ntive sentiment. However, none of the tools is capable of recognizing this: S\nENTI STRENGTH\nlabels the comment as being neutral, NLTK, Alchemy and Stanford NLP—as being nega-\ntive. Indeed, for instance Stanford NLP believes the first three sentences to be negative (e.g.,\ndue to presence of “No”), and while it correctly recognizes the last sentence as positive, this\nis not enough to change the evaluation of the comment as the whole.\nExample 2 The following comment from Apache Xerces merely describes an action that\nhas taken place (“committed a patch”).\nD.E. Veloper\n9 committed your patch for Xerces 2.6.0. Please verify.\nThree out of four annotators do not recognize presence of emotion in this comment and\nwe interpret this as the comment being neutral. However, keyword-based sentiment anal-\nysis tools might wrongly identify presence of sentiment. For instance, in SentiWordNet\n(Baccianella et al. 2010) the verb “commit”, in addition to neutral meanings (e.g., perpe-\ntrate an act as in “commit a crime”) has several positive meanings (e.g., confer a trust upon,\n“I commit my soul to God” or cause to be admitted when speaking of a person to an insti-\ntution, “he was committed to prison”). In a similar way, the word “patch”, in addition to\nneutral meanings, has negative meanings (e.g.,, sewing that repairs a worn or torn hole or\na piece of soft material that covers and protects an injured part of body). Hence, it should\ncome as no surprise that some sentiment analysis tools identify this comment as positive,\nsome other as negative and finally, some as neutral.\nThese examples show that in order to be successfully applied in the software engineering\ncontext, sentiment analysis tools should become aware of the peculiarities of the software\nengineering domain: e.g., that words “commit” and “patch” are merely technical terms and\ndo not express sentiment. Our observation concurs with the challenge Novielli et al. (2015)\nhas recognized in sentiment detection in the social programming ecosystem such as S\nTACK\nOVERFLOW .\n9To protect the privacy of the project participants we do not disclose their names.\nEmpir Software Eng (2017) 22:2543–2584 2551\nTa b l e 3 Agreement of groups of tools with the manual labeling ( n—the number of comments the tools agree\nupon)\nF\nTools n κ ARI neu pos neg\nNLTK, S ENTI STRENGTH 138 0.65 0.51 0.89 0.78 0.56\nNLTK, Alchemy 134 0.46 0.24 0.73 0.69 0.47\nNLTK, Stanford NLP 122 0.43 0.23 0.71 0.74 0.40\nSENTI STRENGTH , Alchemy 133 0.50 0.27 0.76 0.71 0.43\nSENTI STRENGTH , Stanford NLP 109 0.53 0.34 0.78 0.83 0.39\nAlchemy, Stanford NLP 130 0.36 0.19 0.49 0.79 0.31\nNLTK, S ENTI STRENGTH , Alchemy 88 0.68 0.49 0.84 0.84 0.58\nNLTK, S ENTI STRENGTH , Stanford NLP 71 0.72 0.52 0.85 0.91 0.55\nSENTI STRENGTH , Alchemy, Stanford NLP 74 0.59 0.38 0.73 0.91 0.41\nNLTK, Alchemy, Stanford NLP 75 0.55 0.28 0.68 0.83 0.52\nNLTK, S ENTI STRENGTH , Alchemy, Stanford NLP 53 0.72 0.50 0.80 0.93 0.57\n3.4 A Follow-up Study\nGiven the disagreement between different sentiment analysis tools, we wonder whether\nfocusing only on the comments where the tools agree with each other, would result in a\nbetter agreement with the manual labeling. Clearly, since the tools tend to disagree, such\na focus reduces the number of comments that can be evaluated. However, it is ap r i o r i\nnot clear whether a better agreement can be expected with the manual labeling. Thus, we\nhave conducted a follow-up study: for every group of tools we consider only comments on\nwhich the tools agree, and determine κ, ARI and the F-measures with respect to the manual\nlabeling.\nResults of the follow up study are summarized in Table 3. As expected, the more tools we\nconsider the less comments remain. Recalling that in our previous evaluation 262 comments\nhave been considered, only 52.6 % remain if agreement between two tools is required. For\nfour tools slightly more than 20 % of the comments remain. We also see that focusing on\nthe comments where the tools agree improves the agreement with the manual labeling both\nin terms of κ and in terms of ARI. The F-measures follow, in general, the same trend. This\nmeans a trade-off should be sought between the number of comments the tools agree upon\nand the agreement with the manual labeling.\n3.5 Threats to V alidity\nAs any empirical evaluation, the study presented in this section is subject to threats to\nvalidity:\n– Construct validity might have been threatened by our operationalization of senti-\nment polarity via emotion, recorded in the dataset by Murgia et al. ( 2014)( c f .t h e\nobservations of Novielli et al. ( 2015)).\n– Internal validity of our evaluation might have been affected by the exact ways tools\nhave been applied and the interpretation of the tools’ output as indication of sentiment,\n2552 Empir Software Eng (2017) 22:2543–2584\ne.g., calculation of a document-level sentiment as −2 ∗ #0 − #1 + #3 + 2 ∗ #4 for\nStanford NLP. Another threat to internal validity stems form the choice of the evaluation\nmetrics: to reduce this threat we report several agreement metrics (ARI, weighted κ and\nF-measures) recommended in the literature.\n– External validity of this study can be threatened by the fact that only one dataset has\nbeen considered and by the way this dataset has been constructed and evaluated by\nMurgia et al. (2014). To encourage replication of our study and evaluation of its external\nvalidity we make publicly available both the source code and the data used to obtain\nthe results of this paper.\n10\n3.6 Summary\nWe have observed that the sentiment analysis tools do not agree with the manual labeling\n(RQ1) and neither do they agree with each other ( RQ2).\n4 Impact of the Choice of Sentiment Analysis T ool\nIn Section 3 we have seen that not only is the agreement of the sentiment analysis tools\nwith the manual labeling limited, but also that different tools do not necessarily agree with\neach other. However, this disagreement does not necessarily mean that conclusions based\non application of these tools in the software engineering domain are affected by the choice\nof the tool. Therefore, we now address RQ3 and discuss a simple set-up of a study aiming\nat understanding differences in response times for positive, neutral and negative texts.\n4.1 Methodology\nWe study whether differences can be observed between response times (issue resolution\ntimes or question answering times) for positive, neutral and negative texts in the context of\naddressing RQ3. We do not claim that the type of comment (positive, neutral or negative)\nis the main factor influencing response time: indeed, certain topics might be more popular\nthan others and questions asked during the weekend might lead to higher resolution times.\nHowever, if different conclusions are derived for the same dataset when different sentiment\nanalysis tools are used, then we can conclude that the disagreement between sentiment\nanalysis tools affects validity of conclusions in the software engineering domain.\nRecent studies considering sentiment in software engineering data tend to include addi-\ntional variables, e.g., sentiment analysis has been recently combined with politeness analysis\n(Danescu-Niculescu-Mizil et al. 2013) to study issue resolution time (Destefanis et al. 2016;\nOrtu et al. 2015). To illustrate the impact of the choice of sentiment analysis tool on the\nstudy outcome in presence of other analysis techniques, we repeat the response time study\nbut combine sentiment analysis with politeness analysis.\n4.1.1 Sentiment Analysis T ools\nBased on the answers to RQ1 and RQ2 presented in Section 3.3 we select S ENTI STRENGTH\nand NLTK to address RQ3. Indeed, NLTK scores best when compared to the manual\n10http://ow.ly/HvC5302N4oK\nEmpir Software Eng (2017) 22:2543–2584 2553\nTa b l e 4 Descriptive statistics of\nresolution/response type Mean Std Dev Median\nAndroid 79.58 143.19 9\nGnome 267.03 1.33 26.94\nSO 21.53 131.32 0.13\nASF 96.57 255.44 4.16\nlabelling, followed by S ENTI STRENGTH , and both perform better than Alchemy and Stan-\nford NLP. Agreement between NLTK and S ENTI STRENGTH , while also only fair, is still\nthe second highest one among the six possible pairs in Table 2.\nMoreover, we also repeat each study on the subset of texts where NLTK and S EN -\nTI STRENGTH agree. Indeed, Table 3 shows that these tools agree upon the largest subset\nof comments, achieving at the same time the highest among the two-tool combinations κ,\nARI and the F-measure for neutral and negative class. We also observe that further improve-\nment of the evaluation metrics is possible but at cost of significant drop in the number of\ncomments.\n4.1.2 Datasets\nWe study seven different datasets: titles of issues of the A NDROID issue tracker, descriptions\nof issues of the A NDROID issue tracker, titles of issues of the Apache Software Foundation\n(ASF) issue tracker, descriptions of issues of the ASF issue tracker, descriptions of issues\nof the G\nNOME issue tracker, titles of the G NOME -related S TACK OVERFLOW questions and\nbodies of the G NOME -related S TACK OVERFLOW questions. As opposed to the A NDROID\ndataset, G NOME issues do not have titles. To ensure validity of our study we have opted\nfor five datasets collected independently by other researchers (A NDROID Issue Tracker\ndescriptions and titles, G NOME Issue Tracker descriptions, ASF Issue Tracker descriptions\nand titles) and two dataset derived by us from a well-known public data source (G NOME -\nRelated S TACK OVERFLOW question titles and bodies). All datasets are publicly available\nfor replication purposes. 11 The descriptive statistics of the resolution/response times from\nthese data-sets are given in Table 4.\nANDROID Issue Tracker A dataset of 20,169 issues from the A NDROID issue tracker was\npart of the mining challenge of MSR 2012 (Shihab et al. 2012). Excluding issues without a\nclosing date, as well as those with bug status “duplicate”, “spam” or “usererror”, results in\nthe dataset with 5,216 issues.\nWe analyze the sentiment of the issue titles and descriptions. Five issues have an undeter-\nmined description sentiment. We remove these issues from further analysis on the titles and\nthe descriptions. To measure the response time, we calculate the time difference in seconds\nbetween the opening ( openedDate) and closing time ( closedOn)o fa ni s s u e .\nGNOME Issue Tracker The G NOME project issue tracker dataset containing 431,863\nissues was part of the 2009 MSR mining challenge. 12 Similarly to the A NDROID dataset,\nwe have looked only at issues with a value for field bug status of resolved.I nt o t a l\n11http://ow.ly/HvC5302N4oK\n12http://msr.uwaterloo.ca/msr2009/challenge/msrchallengedata.html\n2554 Empir Software Eng (2017) 22:2543–2584\n367,877 have been resolved. We analyze the sentiment of the short descriptions of the issues\n(short desc) and calculate the time difference in seconds between the creation and closure\nof each issue. Recall that as opposed to the A NDROID dataset, G NOME issues do not have\ntitles.\nGNOME -Related S TAC K OVERFLOW Discussions We use the StackExchange online\ndata explorer13 to obtain all S TACK OVERFLOW posts created before May 20, 2015, tagged\ngnome and having an accepted answer. For all 410 collected posts, we calculate the time\ndifference in seconds between the creation of the post and the creation of the accepted\nanswer. Before applying a sentiment analysis tool we remove HTML formatting from the\ntitles and bodies of posts. In the results, we refer to the body of a post as its description.\nASF Issue Tracker We use a dataset containing data from the ASF issue tracking system\nJIRA . This dataset was collected by Ortu et al. (2015) and contains 701,002 issue reports.\nWe analyze the sentiments of the titles and the descriptions of 95,667 issue reports that have\na non-null resolved date, a resolved status and the resolution value being Fixed.\n4.1.3 Politeness Analysis\nSimilarly to sentiment analysis classifying texts into positive, neutral and negative, polite-\nness analysis classifies texts into polite, neutral and impolite. In our work we use the\nStanford politeness API\n14 based on the work of Danescu-Niculescu-Mizil et al. ( 2013).\nAs opposed to sentiment analysis tools such as S ENTI STRENGTH and NLTK, the Stan-\nford politeness API has been evaluated on software engineering data: S TACK OVERFLOW\nquestions and answers.\nGiven a textual fragment the Stanford politeness API returns a politeness score ranging\nbetween 0 (impolite) and 1 (polite) with 0.5 representing the “ideal neutrality”. To discretize\nthe score into polite, neutral and impolite we apply the Stanford politeness API to the seven\ndatasets above. It turns out that the politeness scores of the majority of comments are low:\nthe median score is 0.314, the mean score is 0.361 and the third quartile (Q3) is 0.389. We\nuse the latter value to determine the neutrality range. We say therefore that the comments\nscoring between 0.389 and 0.611 = 1 − 0.389 are neutral; comments scoring lower than\n0.389 are impolite and comments scoring higher than 0.611 are polite.\n4.1.4 Statistical Analysis\nTo answer our research questions we need to compare distributions of response times corre-\nsponding to different groups of issues. We conduct two series of studies. In the first series of\nstudies we compare the distributions of the response times corresponding to positive, neutral\nand negative questions/issues. In the second series we also consider politeness and compare\nthe distributions of the response times corresponding to nine groups obtained through all\npossible combinations of sentiment (positive, neutral and negative) and politeness (polite,\nneutral and impolite).\n13http://data.stackexchange.com/\n14https://github.com/sudhof/politeness\nEmpir Software Eng (2017) 22:2543–2584 2555\nTraditionally, a comparison of multiple groups follows a two-step approach: first, a\nglobal null hypothesis is tested, then multiple comparisons are used to test sub-hypotheses\npertaining to each pair of groups. The first step is commonly carried out by means of\nANOV A or its non-parametric counterpart, the Kruskal-Wallis one-way analysis of vari-\nance by ranks. The second step uses the t-test or the rank-based Wilcoxon-Mann-Whitney\ntest (Wilcoxon 1945), with correction for multiple comparisons, e.g., Bonferroni correction\n(Dunn 1961;S h e s k i n2007). Unfortunately, the global test null hypothesis may be rejected\nwhile none of the sub-hypotheses are rejected, or vice versa (Gabriel 1969). Moreover,\nsimulation studies suggest that the Wilcoxon-Mann-Whitney test is not robust to unequal\npopulation variances, especially in the case of unequal sample sizes (Brunner and Munzel\n2000; Zimmerman and Zumbo 1992). Therefore, one-step approaches are preferred: these\nshould produce confidence intervals which always lead to the same test decisions as the\nmultiple comparisons. We use the ˜T-procedure (Konietschke et al. 2012) for Tukey-type\ncontrasts (Tukey 1951), the probit transformation and the traditional 5 % family error rate\n(cf. Vasilescu et al. 2013;W a n ge ta l . 2014).\nThe results of the ˜T-procedure are a series of probability estimates p(a,b) with the\ncorresponding p-values, where a and b are representing the distributions being compared.\nThe probability estimate p(a,b) is interpreted as follows: if the corresponding p-value\nexceeds 5 % then no evidence has been found for difference in response times correspond-\ning to categories a and b. If, however, the corresponding p-value does not exceed 5 % and\np(a,b) > 0.5 then response times in category b tends to be larger than those in category a.\nFinally, if the corresponding p-value does not exceed 5 % and p(a,b) < 0.5 then response\ntimes in category a tends to be larger than those in category b.\nWe opt for comparison of distributions rather than a more elaborate statistical modeling\n(cf. Ortu et al. 2015) since it allows for an easy comparison of the results obtained for\ndifferent tools.\n4.1.5 Agreement Between the Results\nRecall that sentiment analysis tools induce partition of the response times into categories.\nFor every pair of values (a, b) the ˜T-procedure indicates one of the three following out-\ncomes: > (response times in category a tends to be larger than those in category b), <\n(response times in category b tends to be larger than those in category a)o r∥ (no evidence\nhas been found for difference in response times corresponding to categories a and b). We\nstress that we refrain from interpreting lack of evidence for difference as evidence for lack\nof difference, i.e., we do not claim the distributions of response times corresponding to cat-\negories a and b are the same but merely that we cannot find evidence that these distributions\nare not the same. Hence, we also use ∥ (incomparable) rather than = (equal).\nTo compare the tools we therefore need to assess the agreement between the results\nproduced by the ˜T-procedure for partitions induced by different tools.\nExample 3 Let ˜T-procedure report “pos < neu”, “pos < neg” and “neu < neg” for partitions\ninduced by Tool1, “pos < neu”, “pos < neg” and “neu ∥ neg” for partitions induced by\nTool2, and “pos > neu”, “pos > neg” and “neu ∥ neg” for partitions induced by Tool3.\nThen, we would like to say that Tool1 agrees more with Tool2 than with Tool3, and Tool2\nagrees more with Tool3 than with Tool1.\nUnfortunately, traditional agreement measures such as discussed in Section 3.1.2 are\nno longer applicable since the number of datapoints (pairs of categories) is small: 3 for\n2556 Empir Software Eng (2017) 22:2543–2584\nsentiment and 36 for the sentiment-politeness combination. Hence, we propose to count the\npairs of categories (a, b) such that the ˜T-procedure produces the same result for partitions\ninduced by both tools (so called observed agreement).\nExample 4 For Example 3 we observe that Tool1 and Tool2 agree on two pairs, Tool1 and\nTool3 agree on zero pairs, and Tool2 and Tool3 agree on one pair.\nWe believe, however, that a disagreement between claims “response times in category\na tends to be larger than those in category b” and “response times in category b tends to\nbe larger than those in category a” is more severe than between claims “response times in\ncategory a tends to be larger than those in category b” and “no evidence has been found\nfor difference in response times corresponding to categories a and b”. One possible way to\naddress this concern would be to associate different kinds of disagreement with different\nweights: this is an approach taken, e.g., by the weighted κ (Cohen 1968). However, the\nchoice of specific weights might appear arbitrary.\nHence, when reporting disagreement between the tools (cf. Tables 6 and 8 below) we\nreport different kinds of disagreement separately, i.e., we report four numbers x−y−z−w,\nwhere\n– x is the number of pairs for which the tools agree about the relation between the\nresponse times ( >> or <<),\n– y is the number of pairs for which the tools agree about the lack of such a relation ( ∥∥),\n– z is the number of pairs when one of the tools has established the relation and another\none did not ( ∥ >, ∥ <, < ∥ or > ∥),\n– w is the number of pairs when the tools have established different relations (<> or\n><).\nExample 5 Example 3, continued. We report agreement between Tool1 and Tool2 as 2 −\n0 − 0 − 1, between Tool1 and Tool3 as 0 − 0 − 1 − 2, and between Tool2 and Tool3 as\n0 − 1 − 0 − 2.\n4.2 Results\nResults of our study are summarized in Table 5. For the sake of readability the relations\nfound are aligned horizontally. For each dataset and each tool we also report the number of\nissues/questions recognized as negative, neutral or positive.\nWe observe that NLTK and S ENTI STRENGTH agree only on one relation for the\nANDROID , i.e., that issues with the neutral sentiment tend to be resolved more slowly than\nissues formulated in a more positive way. We also observe that for G NOME and ASF the\ntools agree that the issues with the neutral sentiment are resolved faster than issues with\nthe positive sentiment, i.e., the results for G\nNOME and ASF are opposite from those for\nANDROID .\nFurther inspection reveals that differences between NLTK and S ENTI STRENGTH led to\nrelations “neu > neg” and “neg > pos” to be discovered in A NDROID issue descriptions only\nby one of the tools and not by the other. In the same way, “pos > neg” on the ASF descrip-\ntions data can be found only by S ENTI STRENGTH . It is also surprising that while “pos >\nneg” has been found for the ASF titles data both by NLTK and by S ENTI STRENGTH ,i t\ncannot be found when one restricts the attention to the issues where the tools agree. Finally,\nEmpir Software Eng (2017) 22:2543–2584 2557\nTa b l e 5 Comparison of NLTK and S ENTI STRENGTH . Thresholds for statistical significance: 0.05 ( ∗), 0.01\n(∗∗), 0.001 ( ∗∗∗). Exact p-values are indicated as subscripts; 0 indicates that the p-value is too small to be\ncomputed precisely. For the sake of readability we omit pairs for which no evidence has been found for\ndifferences in response times\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\nneg-neu-pos neg-neu-pos neg-neu-pos\nANDROID\ntitle 1,230-3,588-398 1,417-3,415-384 396-2,381-36\n∅∅∅\ndescr 2,690-1,657-869 1,684-2,435-1,182a 893-712-299\nneu > neg∗∗∗\n2.79×10−8 neu > neg∗\n2.54×10\n−2\nneu > pos∗∗\n5.55×10−3 neu > pos∗∗\n9.72×10\n−3 neu > pos∗∗∗\n7.53×10\n−5\nneg > pos∗∗∗\n6.32×10\n−4 neg > pos∗\n3.81×10\n−2\nGNOME\ndescr 54,032-291,906-20,380 58,585-293,226-14,507 16,829-24,2780-1,785\nneg > neu∗∗∗\n0 neg > neu∗∗∗\n0\nneg > neu∗∗∗\n0\npos > neu∗∗∗\n0\npos > neu∗∗∗\n0\npos > neu∗∗∗\n0\npos > neg∗∗∗\n0\nneg > pos∗∗∗\n0\nSTACK OVERFLOW\ntitle 84-285-41 53-330-27 16-240-8\n∅∅∅\ndescr 249-71-90 90-183-137 62-35-42\n∅ neg > pos∗\n3.46×10−2 ∅\nASF\ntitle 19,367-67,948-8,348 b 24,141-62,016-9,510 6,450-44,818-1,106\npos > neu∗∗∗\n0 pos > neu∗∗\n3.71×10−3\npos > neg∗∗∗\n0 pos > neg∗∗∗\n2.60×10−12\ndescrc 30,339-42,540-13,129d 29,021-41,043-15,971e 10,989-20,940-3,814\nneg > neu∗∗∗\n0 neg > neu∗∗∗\n0\nneg > neu∗∗∗\n0\npos > neu∗∗∗\n0\npos > neu∗∗∗\n0\npos > neu∗∗∗\n0\npos > neg∗∗∗\n5.32×10−13 pos > neg∗∗∗\n5.12×10\n−13\naSentiment of 5 issues was “undetermined”.\nbThe tool reported an error for 4 issues.\nc9,620 empty descriptions where not included in this analysis.\ndThe tool reported an error for 39 issues.\neSentiment of 12 issues was “undetermined”.\ncontradictory results have been obtained for G NOME issue descriptions: while the NLTK-\nbased analysis suggests that the positive issues are resolved more slowly than the negative\nones, the S ENTI STRENGTH -based analysis suggests the opposite.\nOverall, the agreement between NLTK, S ENTI STRENGTH and NLTK ∩ SEN -\nTI STRENGTH reported as described in Section 4.1.5 is summarized in Table 6.\nNext we perform a similar study by including the politeness information. Table 7 sum-\nmarizes the findings for A NDROID . Observe that not a single relation could have been\n2558 Empir Software Eng (2017) 22:2543–2584\nTa b l e 6 Agreement between NLTK, S ENTI STRENGTH and NLTK ∩ SENTI STRENGTH . See Section 4.1.5\nfor the explanation of the x − y − z − w notation\nNLTK vs. NLTK vs. SENTI STRENGTH vs.\nSENTI STRENGTH NLTK ∩ SENTI STRENGTH NLTK ∩ SENTI STRENGTH\nANDROID\ntitle 0 − 3 − 0 − 00 − 3 − 0 − 00 − 3 − 0 − 0\ndescr 1 − 0 − 2 − 02 − 0 − 1 − 02 − 0 − 1 − 0\nGNOME\ndesc 2 − 0 − 0 − 12 − 0 − 1 − 02 − 0 − 1 − 0\nSTACK OVERFLOW\ntitle 0 − 3 − 0 − 00 − 3 − 0 − 00 − 3 − 0 − 0\ndesc 0 − 2 − 1 − 00 − 3 − 0 − 00 − 2 − 1 − 0\nASF\ntitle 1 − 1 − 1 − 00 − 1 − 2 − 01 − 1 − 1 − 0\ndesc 2 − 0 − 1 − 02 − 0 − 1 − 03 − 0 − 0 − 0\nestablished both by NLTK and by S ENTI STRENGTH . Results for G NOME ,S TACK OVER -\nFLOW and ASF are presented in Tables 18, 19 and 20 in the appendix. Agreement is\nsummarized in Table 8: including politeness increases the number of categories to be com-\npared to nine, and therefore, the number of possible category pairs to 9∗8\n2 = 36. Table 8\nsuggests that while the tools tend to agree on the relation or lack thereof between most of\nthe category pairs, the differences between the tools account for the differences in the rela-\ntions observed in up to 30 % (11/36) of the pairs. Still, differences between the tools leading\nto contradictory results is relatively rare (two cases in G\nNOME , one in ASF titles and one\nin ASF descriptions), the differences tend to manifest as a relation being discovered when\nonly one of the tools is used.\n4.3 Discussion\nOur results suggest the choice of the sentiment analysis tool affects the conclusions one\nmight derive when analysing differences in the response times, casting doubt on the valid-\nity of those conclusions. We conjecture that the same might be observed for any kind of\nsoftware engineering studies dependent on off-the-shelf sentiment analysis tools. A more\ncareful sentiment analysis for software engineering texts is therefore needed: e.g., one might\nconsider training more general purpose machine learning tools such as Weka (Hall et al.\n2009) or RapidMiner\n15 on software engineering data.\nA similar approach has been recently taken by Panichella et al. (2015)t h a th a v eu s e d\nWeka to train a Naive Bayes classifier on 2090 App Store and Google Play review sentences.\nIndeed, both dependency of sentiment analysis tools on the domain (Gamon et al. 2005)a n d\nthe need for text-analysis tools specifically targeting texts related to software engineering\n(Howard et al. 2013) have been recognized in the past.\n15https://rapidminer.com/solutions/sentiment-analysis/\nEmpir Software Eng (2017) 22:2543–2584 2559\nTa b l e 7 Comparison of NLTK and S ENTI STRENGTH in combination with politeness for the A NDROID\ndatasets. Thresholds for statistical significance: 0.05 ( ∗), 0.01 ( ∗∗), 0.001 ( ∗∗∗). Exact p-values are indicated\nas subscripts. Results for G NOME ,S TACK OVERFLOW and ASF are presented in Tables 18, 19 and 20 in the\nappendix\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\ntitle\nneg neu pos neg neu pos neg neu pos\nimp 948 2872 268 1077 2729 279 297 1935 18\nneu 245 693 120 315 652 89 86 432 17\npol 37 23 10 22 32 16 13 14 1\n∅∅ — a\ndescr\nneg neu pos neg neu pos neg neu pos\nimp 262 220 41 218 236 68 118 110 7\nneu 562 530 144 470 515 251 211 229 46\npol 1866 907 684 996 1594 863 564 373 246\nneg.neu > pos.pol∗∗\n1.40×10−3\nneg.pol > pos.pol∗\n4.55×10\n−2\nneu.imp > neg.pol∗\n4.63×10\n−2\nneu.imp > pos.pol∗∗\n7.20×10\n−3\nneu.neu > neg.imp∗\n4.23×10−2\nneu.neu > neg.pol∗∗∗\n1.19×10\n−5 neu.neu > neg.pol∗\n3.89×10\n−2\nneu.neu > pos.pol∗\n3.91×10\n−2 neu.neu > pos.pol∗∗\n3.14×10\n−3\nneu.pol > neg.pol∗∗∗\n8.19×10−4\nanparcomp could not run due to insufficient data points\n4.4 Threats to V alidity\nValidity of the conclusions derived might have been threatened by the choice of the data as\nwell by the choice of the statistical machinery.\nTo reduce the threats related to the data, we have opted for seven different but similar\ndatasets: the S TACK OVERFLOW dataset contains information about questions and answers,\nANDROID ,G NOME and ASF—information about issues. We expect the conclusions above\nto be valid at least for other issue trackers and software engineering question & answer plat-\nforms. For A\nNDROID ,G NOME and ASF we have reused data collected by other researchers\n(Shihab et al. (2012), Bird 16 and Ortu et al. (2015), respectively). We believe the threats\nassociated with noise in these datasets are limited as they have been extensively used in the\nprevious studies: e.g., Asaduzzaman et al. (Asaduzzaman et al. ) and Martie et al. (Martie\net al. ) used the A\nNDROID dataset, Linstead and Baldi ( 2009)u s e dt h eG NOME dataset, and\nOrtu et al. (2015) used the ASF dataset. The only dataset we have collected ourselves is the\nSTACK OVERFLOW dataset, and indeed the usual threats related to completeness of the data\n(questions can be removed) apply. Furthermore, presence of machine-generated text, e.g.,\nerror messages, stack traces or source code, might have affected our results.\n16http://msr.uwaterloo.ca/msr2009/challenge/msrchallengedata.html\n2560 Empir Software Eng (2017) 22:2543–2584\nTa b l e 8 Agreement between NLTK, S ENTI STRENGTH and NLTK ∩ SENTI STRENGTH (politeness\ninformation included). See Section 4.1.5 for the explanation of the x − y − z − w notation\nNLTK vs. NLTK vs. SENTI STRENGTH vs.\nSENTI STRENGTH NLTK ∩ SENTI STRENGTH NLTK ∩ SENTI STRENGTH\nANDROID\ntitle 0 − 36 − 0 − 0— a — a\ndescr 0 − 30 − 6 − 01 − 30 − 5 − 01 − 30 − 5 − 0\nGNOME\ndesc 14 − 13 − 7 − 21 0 − 15 − 11 − 01 0 − 18 − 8 − 0\nSTACK OVERFLOW\ntitle 0 − 28 − 0 − 0b — c — c\ndesc 0 − 33 − 3 − 0— c — c\nASF\ntitle 1 − 24 − 10 − 10 − 31 − 5 − 00 − 27 − 9 − 0\ndesc 25 − 3 − 7 − 12 3 − 5 − 8 − 02 3 − 4 − 9 − 0\nanparcomp could not run on the results of NLTK ∩ SENTI STRENGTH due to insufficient data points.\nbSince the S TACK OVERFLOW dataset is relatively small, not all sentiment/politeness combinations are\npresent in the dataset.\ncFocus on questions where NLTK and S ENTI STRENGTH agree reduces the number of combinations present\nmaking comparing NLTK ∩ SENTI STRENGTH and NLTK not possible. Idem for S ENTI STRENGTH .\nSimilarly, to reduce the threats related to the choice of the statistical machinery we opt for\nthe ˜T-approach (Konietschke et al. 2012) that has been successfully applied in the software\nengineering context (Dajsuren et al. 2013;L ie ta l .2014; Sun et al. 2015; Vasilescu et al.\n2013; Vasilescu et al. 2013;W a n ge ta l . 2014;Y ue ta l .2016).\n5 Implications on Earlier Studies\nIn this section we consider RQ4: while the preceding discussion indicates that the choice\nof a sentiment analysis tool might affect validity of software engineering results, in this\nsection we investigate whether this is indeed the case by performing replication studies\n(Shull et al. 2008) for two published examples. Since our goal is to understand whether the\neffects observed in the earlier studies hold when a different sentiment analysis tool is used,\nwe opt for dependent or similar replications (Shull et al. 2008). In dependent replications\nthe researchers aim at keeping the experiment the same or very similar to the original one,\npossibly changing the artifact being studied.\n5.1 Replicated Studies\nWe have chosen to replicate two previous studies conducted as part of the 2014 MSR min-\ning challenge: both studies use the same dataset of 90 GitHub projects (Gousios 2013).\nThe dataset includes information from the top-10 starred repositories in the most popular\nprogramming languages and is not representative of GitHub as a whole\n17.\n17http://ghtorrent.org/msr14.html\nEmpir Software Eng (2017) 22:2543–2584 2561\nThe first paper we have chosen to replicate is the one by Pletea et al. ( 2014). In this\npaper the authors apply NLTK to GitHub comments and discussions, and conclude that\nsecurity-related discussions on GitHub contain more negative emotions than non-security\nrelated discussions. Taking the blame, the fourth author of the current manuscript has also\nco-authored the work by Pletea et al. ( 2014).\nThe second paper we have chosen to replicate is the one by Guzman et al. (2014). The\nauthors apply S\nENTI STRENGTH to analyze the sentiment of GitHub commit comments and\nconclude that comments written on Mondays tend to contain a more negative sentiment than\ncomments written on other days. This study was the winner of the MSR 2014 challenge.\n5.2 Replication Approach\nWe aim at performing the exact replication of the studies chosen with one notable deviation\nfrom the original work: we apply a different sentiment analysis tool to each study. Since\nthe original study of Pletea et al. uses NLTK, we intend to apply S\nENTI STRENGTH in the\nreplication; since Guzman et al. use S ENTI STRENGTH , we intend to apply NLTK. However,\nsince the exact collections of comments used in the original studies were no longer available,\nwe had to recreate the datasets ourselves. This lead to minor differences with the number\nof comments we have found as opposed to those reported in the original studies. Hence, we\nreplicate each study twice: first applying the same tool as in the original study to a slightly\ndifferent data, second applying a different sentiment analysis tool to the same data as in the\nfirst replication.\nWe hypothesize that the differences between applying the same tool to slightly differ-\nent datasets would be small. However, we expect that we might get different, statistically\nsignificant, results in these studies when using a different sentiment analysis tool.\n5.2.1 Pletea et al.\nPletea et al. distinguish between comments and discussions, collections of comments per-\ntaining to an individual commit or pull request. Furthermore, the authors distinguish\nbetween security-related and non-security related comments/discussions, resulting in eight\ndifferent categories of texts. The original study has found that for commits comments,\ncommit discussions, pull request comments and pull request discussions, the negativity for\nsecurity related texts is higher that for other texts. Comparison of the sentiment recognition\nusing a sentiment analysis tool (NLTK) with 30 manually labeled security-related commit\ndiscussions were mixed. Moreover, it has been observed that the NLTK results were mostly\nbipolar, having both strong negative and strong positive components. Based on this obser-\nvations the authors suggest that the security-related discussions are more emotional than\nnon-security related ones.\nIn our replication of this study we present a summary of the distribution of the sentiments\nfor commits and pull requests, recreating Tables 2 and 3 from the original study. In order\nto do this, we also need to distinguish security-related texts and other texts, i.e., we repli-\ncate Table 1 from the paper. We extend the original comparison with the manually labeled\ndiscussions by including the results obtained by S\nENTI STRENGTH .\n5.2.2 Guzman et al.\nIn this study, the authors have focused on commit comments and studied differences\nbetween the sentiment of commit comments written at different days of week and times of\n2562 Empir Software Eng (2017) 22:2543–2584\nTa b l e 9 Identification of security-related comments and discussions results\nType Comments Discussions\nCommits Pletea et al. (2014) Security 2689 (4.43 %) 1809 (9.84 %)\nTotal 60658 18380\nCurrent study Before elimination Security 2509 (4.13 %) 1706 (9.28 %)\nTotal 60658 18377\nExcluded S ENTI STRENGTH 93 2\nExcluded NLTK 0 1\nFor further analysis Security 2509 (4.14 %) 1689 (9.21 %)\nTotal 60649 18344\nPletea et al. ( 2014) Security 2689 (4.43 %) 1809 (9.84 %)\nTotal 60658 18380\nCurrent study Before elimination Security 1801 (3.28 %) 1091 (11.36 %)\nTotal 54892 9601\nExcluded S ENTI STRENGTH 11 6\nExcluded NLTK 5 0\nFor further analysis Security 1800 (3.28 %) 1081 (11.28 %)\nTotal 54886 9585\nday, belonging to projects in different programming languages, created by teams distributed\nover different continents and “starred”, i.e., approved, by different number of GitHub users.\nWe replicate the studies pertaining to differences between comments based on day and\ntime of their creation and programming language of the project. We do not replicate the\nstudy related to the geographic distribution of the authors because the mapping of devel-\nopers to continents has been manually made by Guzman et al. and was not present in the\noriginal dataset.\n5.3 Replication Results\nHere we present the results of replicating both studies.\n5.3.1 Pletea et al.\nWe start the replication by creating Table 9, which corresponds to Table 1 from the paper\nby Pletea et al. We have rerun the division using the keyword list as included in the orig-\ninal paper. As explained above, we have found slightly different numbers of comments\nand discussions in each category. Most notably we find 180 less security-related comments\nin commits. However, the percentages of security and non-security related comments and\ndiscussions are similar.\nTo ensure validity of the comparison between NLTK and S\nENTI STRENGTH we have\napplied both tools to comments and discussions. On several occasions the tools reported\nan error. We have decided to exclude those cases to ensure that further analysis applies to\nexactly the same comments and discussions. Hence, in Table 9 we also report the numbers\nof comments and discussions excluded.\nEmpir Software Eng (2017) 22:2543–2584 2563\nTa b l e 1 0 Commits sentiment analysis statistics. The largest group per study is typeset in boldface\nType Negative Neutral Positive\nDiscussions Pletea et al. ( 2014) Security 72.52 % 10.88 % 16.58 %\nNLTK Rest 52.28 % 20.37 % 25.33 %\nCurrent study Security 70.16 % 12.79 % 17.05 %\nNLTK Rest 52.89 % 21.50 % 25.61 %\nCurrent study Security 30.66 % 42.92 % 26.40 %\nSENTI STRENGTH Rest 24.13 % 43.92 % 31.94 %\nComments Pletea et al. ( 2014) Security 55.59 % 23.42 % 20.97 %\nNLTK Rest 46.94 % 26.58 % 26.47 %\nCurrent study Security 55.96 % 22.88 % 21.16 %\nNLTK Rest 46.89 % 26.61 % 26.50 %\nCurrent study Security 32.60 % 46.95 % 20.44 %\nSENTI STRENGTH Rest 22.30 % 50.74 % 26.95 %\nNext we apply NLTK and S ENTI STRENGTH to analyze the sentiment of comments and\ndiscussions. Tables 10 and 11 present the results Tables 2 and 3 of the original paper, respec-\ntively, and extend them by including results of NLTK and S ENTI STRENGTH on the current\nstudy dataset from Table 9. Inspecting Tables 10 and 11 we observe that the values obtained\nwhen using NLTK are close to those reported by Pletea et al., while S ENTI STRENGTH\nproduces very different results. Indeed, NLTK indicates that comments and discussions,\nsubmitted via commits or via pull requests, are predominantly negative, while according to\nSENTI STRENGTH neutral is the predominant classification.\nDespite those differences, the original conclusion of Pletea et al. still holds: whether we\nconsider comments or discussions, commits or pull requests, percentage of negative texts\namong security related texts is higher than among non-security related texts.\nFinally, in Table 4 Pletea et al. consider thirty security-related commit discussions and\ncompare evaluation of the security relevance and sentiment as determined by the tools with\nTa b l e 1 1 Pull Requests sentiment analysis statistics. The largest group per study is typeset in boldface\nType Negative Neutral Positive\nDiscussions Pletea et al. ( 2014) Security 81.00 % 5.52 % 13.47 %\nNLTK Rest 69.58 % 11.98 % 18.42 %\nCurrent study Security 77.61 % 7.03 % 15.36 %\nNLTK Rest 67.43 % 13.82 % 18.76 %\nCurrent study Security 30.80 % 45.51 % 23.68 %\nSENTI STRENGTH Rest 24.15 % 51.17 % 24.67 %\nComments Pletea et al. ( 2014) Security 59.83 % 19.09 % 21.06 %\nNLTK Rest 50.16 % 26.12 % 23.70 %\nCurrent study Security 59.67 % 18.83 % 21.50 %\nNLTK Rest 49.81 % 26.45 % 23.74 %\nCurrent study Security 25.66 % 51.22 % 23.11 %\nSENTI STRENGTH Rest 18.14 % 62.87 % 18.97 %\n2564 Empir Software Eng (2017) 22:2543–2584\nTa b l e 1 2 Case study results. Strength of the human-labeled sentiments has been labeled by Pletea et al. on a 5-star scale (Pletea et al. 2014)\nSec.\nrelevance\nDiscussion\n(Commit ID)\n# sec. key-\nwords\nSec. rel-\nevance\n(human)\nNLTK neu-\ntral (%)\nNLTK neg-\native (%)\nNLTK\npositive\n(%)\nNLTK\nresult\nS\nENTI\nSTRENGTH\nresult\nSentiment\n(human)\nHigh 535033 6 Yes 16.5 42.9 57.0 pos neutral neg(*)\n256855 4 Yes 17.1 84.2 15.7 neg neutral neg(*)\n455971 6 Yes 19.1 84.3 15.6 neg neutral neutral\n131473 5 Yes 21.4 45.8 54.2 pos neg neg(*****)\n253685 4 No 20.4 59.1 40.8 neg neutral pos(*)\n370765 5 Yes 20.0 65.0 34.9 neg neutral pos(***)\n59082 4 No 19.8 76.4 23.5 neg neutral neg(*)\n157981 11 Yes 23.9 58.8 41.1 neg neutral neg(***)\n391963 9 Yes 16.7 71.9 28.0 neg neutral pos(****)\n272987 4 Yes 22.4 41.6 58.3 pos pos neg(*)\nMedium 15128 1 No 20.6 71.3 28.6 neg neutral neutral\n396099 1 No 18.8 74.0 26.0 neg neg neg(****)\n132779 1 No 30.6 76.4 23.5 neg pos neutral\n295686 1 No 23.9 70.7 29.3 neg neutral pos(*)\n541007 1 Partial 37.7 71.7 28.2 neg neg neg(*)\n199287 1 Partial 18.9 76.4 23.5 neg neutral neg(*)\n461318 1 Yes 15.0 75.0 24.9 neg neutral neg(*)\n509384 1 Partial 33.4 67.3 32.7 neg neutral neutral\n338681 1 No 29.9 75.5 24.4 neg pos neg(*)\n511734 1 No 17.6 79.4 20.5 neg pos pos(***)\nLow 364215 1 No 41.4 44.1 55.8 pos neg neg(*)\n274571 1 Partial 30.1 46.5 53.4 pos pos neg(**)\n47639 1 Yes 19.3 38.6 61.3 pos neutral pos(*****)\n277765 1 No 27.0 45.2 54.7 pos pos pos(*)\n6491 1 No 37.6 29.6 70.4 pos neutral neutral\n130367 1 No 15.4 43.6 56.3 pos pos pos(*)\n189623 1 No 57.9 35.8 64.1 neutral neutral pos(***)\n41379 1 Partial 30.9 26.1 73.8 pos pos pos(***)\n456580 1 No 26.6 46.6 53.3 pos pos pos(***)\n52122 1 No 17.6 46.3 53.6 pos neutral pos(*****)\nEmpir Software Eng (2017) 22:2543–2584 2565\nFig. 1 Emotion score average per project, using S ENTI STRENGTH (Guzman et al. 2014)\nthe decisions performed by the human evaluator. The discussions have been selected based\non the number of security keywords found: ten discussions labeled as “high” have been\nrandomly selected from the top 10 % discussions with the highest number of security key-\nwords found, “middle” from the middle 10 % and “low” from the bottom 10 % of all\nsecurity-related discussions.\nTable 12 extends Table 4 (Pletea et al. 2014) by adding a column with the results of S\nEN -\nTI STRENGTH . Asterisks indicate the strength of the sentiment as perceived by the human\nevaluator.\nBy inspecting Table 12 we observe that NLTK agrees with the human evaluator in 14\ncases out of 30; S ENTI STRENGTH —in 13 cases out of 30 but the tools agree with each other\nonly in 9 cases. We can therefore conclude that replacing NLTK by S ENTI STRENGTH did\naffect the conclusion of the original study: results of the agreement with the manual labeling\nare still mixed.\nWe also observe that both for NLTK and for S\nENTI STRENGTH agreement in the “high”\nsecurity group is lower than in the “low” security group.\nMoreover, Pletea et al. have been observed that the NLTK results were mostly bipolar,\nhaving both strong negative and strong positive components, suggesting that security-related\ndiscussions are more emotional. This observation is not supported by S\nENTI STRENGTH\nthat classifies 17 out of 30 discussions as neutral.\n5.3.2 Guzman et al.\nWe classified all 60658 commit comments in the MSR 2014 challenge dataset (Gousios\n2013)u s i n gN L T K .\nIn the original paper by Guzman et al. (2014) the authors claim to have analyzed 60425\ncommit comments, on the one hand, to have focused on comments of all projects having\nmore than 200 comments, on the other. However, when replicating this study and consider-\ning comments of projects having more than 200 comments we have obtained merely 50133\ncomments, more then ten thousand comments less than in the original study. Therefore, to be\nas close as possible to the original study we have decided to include all commit comments\nin the dataset which produced 233 comments more than in the original study.\n2566 Empir Software Eng (2017) 22:2543–2584\nFig. 2 Proportion of positive, neutral and negative commit comments per project, using S ENTI STRENGTH\n(replication)\nGuzman et al. start by considering six projects with the highest number of commit\ncomments: Jquery, Rails, CraftBukkit, Diaspora, MaNGOS and TrinityCore. The authors\npresent two charts to show the average sentiment score in those six projects and the propor-\ntions of negative, neutral and positive sentiments in commit comments. We replicate their\nstudy twice: first of all, using the same tool used by the authors (S\nENTI STRENGTH ), and\nthen using an alternative tool (NLTK).\nFigs. 2 and 3 show the replication of the study of the average sentiment score in the six\nprojects. The original figure from the work of Guzman et al. is shown in Fig. 1. Comparing\nFig. 1 with Fig. 2 we observe that while the exact values of the averages are lower in the\nreplication, the relative order of the projects is almost the same. Indeed, Rails is the most\npositive project, followed by MaNGOS and then the close values of Diaspora and Trinity-\nCore, followed by Jquery and at last CraftBukkit. Differences between Figs. 1 and 3 are\nmore pronounced. Indeed, the average emotion score is more negative than in the original\nstudy for each project. Moreover, while Jquery and CraftBukkit are still the most negative\nprojects, Rails is no longer positive or even least negative.\nNext we consider proportions of negative, neutral and positive sentiments. The original\nfigure from the work of Guzman et al. is shown in Fig. 4, while Figs. 5 and 6 show the\nresults of our replications. NLTK replication (Fig. 6) shows a larger proportion of negative\ncommit comments than in the original paper (Fig. 4), which shows a larger proportion of\nnegative commit comments than the S\nENTI STRENGTH replication (Fig. 5).\nTables 13–15 contain the results from replicating the studies done in the study by\nGuzman et al. As above, we replicate those studies twice: using the same tool used by the\nauthors (S\nENTI STRENGTH ), and then using an alternative tool (NLTK).\nIn contrast to S ENTI STRENGTH , NLTK outputs scores between 0 and 1 for negative,\nneutral and positive to indicate the probability of each sentiment. In the original paper, the\nSENTI STRENGTH scores are mapped to an integer in the range [−5, −1) for negative texts,\nEmpir Software Eng (2017) 22:2543–2584 2567\nFig. 3 Emotion score average per project, using NLTK (replication)\n0 for neutral texts and in the range (1, 5] for positive texts. In addition, negative scores were\nmultiplied by 1.5 to account for the less frequent occurrence of negativity in human texts.\nFig. 4 Proportion of positive, neutral and negative commit comments per project, using S ENTI STRENGTH\n(Guzman et al. 2014)\n2568 Empir Software Eng (2017) 22:2543–2584\nFig. 5 Proportion of positive, neutral and negative commit comments per project, using S ENTI STRENGTH\n(replication)\nTherefore, when using NLTK we apply a transformation to create numbers in the same\nrange according to the following formula:\nsentiment score =\n⎧\n⎨\n⎩\n(((neg − 0.5) ∗ (−6)) − 2) ∗ 1.5i f neg\n0i f neutral\n((pos − 0.5) ∗ 6) + 2i f pos\nFig. 6 Proportion of positive, neutral and negative commit comments per project, using NLTK (replication)\nEmpir Software Eng (2017) 22:2543–2584 2569\nTa b l e 1 3 Emotion score average grouped by programming language\nLang Guzman et al. (2014) Current study\nSENTI STRENGTH Com S ENTI STRENGTH NLTK\nCom Mean SD Mean SD Med IQR Mean SD Med IQR\nC 6257 0.023 1.716 6277 −0.217 1.746 0.000 2.000 −1.834 3.095 −3.256 4.491\nC++ 16930 0.017 1.725 16983 −0.031 1.765 0.000 4.000 1.017 2.959 0.000 5.953\nJava 4713 −0.144 1.736 4712 −0.282 1.887 0.000 4.000 −1.753 3.106 −3.191 4.460\nPython 2128 −0.018 1.711 2133 −0.182 1.709 0.000 2.000 −1.636 3.079 −3.093 4.395\nRuby 15257 0.002 1.714 15355 −0.034 1.794 0.000 4.000 1.243 3.117 0.000 6.293\nThe formula maps numbers from the range given by NLTK to the range used by S EN -\nTI STRENGTH as well as multiplies negative comments by 1.5, as done in the study by\nGuzman et al.\nWe stress that we do not compare the sentiment values obtained using NLTK with those\nobtained using S ENTI STRENGTH . Rather we compare sentiment values obtained for dif-\nferent groups of comments using the same tool and the same data set, and then observe\n(dis)agreement between the conclusions made. In Tables 13–15 we replicate the sentiment\nscores grouped by programming language, weekday and time of the day. The original study\nreports the mean and the standard deviation. However, the mean can be unreliable (Vasilescu\net al. 2011) and, therefore, we also report the median and the interquartile range IQR,\nQ\n3 − Q1.\nGuzman et al. report that “Java projects tend to have a slightly more negative score than\nprojects implemented in other languages”. As can be seen from Table 13, when the same\ntool (S ENTI STRENGTH ) has been applied to our data set a similar conclusion can be made.\nThis is, however, not the case when NLTK has been applied: Table 13 shows a lower average\nemotion score for the C programming language than for Java. Also the median score for C\nis lower than for Java. We can therefore say that validity of this conclusion is not affected\nby the data set but is affected by the choice of the sentiment analysis tool.\nFurthermore, Guzman et al. report that the observation about Java has been statistically\nconfirmed and that the statistical tests on the remaining programming languages (C, C++,\nTa b l e 1 4 Emotion score average grouped by weekday\nDay Guzman et al. (2014) Current study\nSENTI STRENGTH Com S ENTI STRENGTH NLTK\nCom Mean SD Mean SD Med IQR Mean SD Med IQR\nMon 9517 -0.043 1.732 9533 −0.148 1.790 0.000 4.000 −1.316 3.047 0.000 6.199\nTue 9319 0.005 1.712 9389 −0.089 1.766 0.000 4.000 −1.344 3.079 0.000 6.218\nWed 9730 0.008 1.716 9748 −0.117 1.797 0.000 4.000 −1.372 3.100 0.000 6.292\nThu 9538 0.001 1.728 9561 −0.116 1.791 0.000 4.000 −1.357 3.073 0.000 6.226\nFri 9076 −0.016 1.739 9152 −0.075 1.791 0.000 4.000 −1.347 3.082 0.000 6.256\nSat 6701 −0.027 1.688 6722 − 0.073 1.788 0.000 4.000 −1.326 3.066 0.000 6.264\nSun 6544 0.022 1.717 6544 −0.123 1.774 0.000 4.000 −1.381 3.081 0.000 6.245\n2570 Empir Software Eng (2017) 22:2543–2584\nTa b l e 1 5 Emotion score average grouped by time of the day\nDay Guzman et al. (2014) Current study\nSENTI STRENGTH Com S ENTI STRENGTH NLTK\nCom Mean SD Mean SD Med IQR Mean SD Med IQR\nmorning 12714 0.001 1.730 12750 −0.112 1.777 0.000 4.000 −1.398 3.062 0.000 6.234\nafternoon 19809 0.004 1.717 19859 −0.089 1.764 0.000 4.000 −1.326 3.076 0.000 6.235\nevening 16584 −0.023 1.721 16634 −0.102 1.794 0.000 4.000 −1.323 3.085 0.000 6.261\nnight 11318 −0.016 1.713 11415 −0.142 1.820 0.000 4.000 −1.370 3.077 0.000 6.246\nJavaScript, PHP, Python and Ruby) did not yield significant results. The statistical test used\nis the Wilcoxon rank sum test. The authors compare seven programming languages and\nreport that the corresponding p-values are less or equal to 0.002. We conjecture that the\nBonferroni correction for multiple comparisons has been applied since 0 .05/21 ≃ 0.0024.\nWhen replicating this study we first of all exclude projects developed in languages\nother than the seven languages considered in the original study, and keep 55405 commit\ncomments. Next we compare distributions corresponding to different programming lan-\nguages. A more statistically sound procedure would have been the ˜T-procedure discussed in\nSection 4.1.4. However, in order to keep our replication as close as possible to the original\nstudy, we also perform a series of pairwise Wilcoxon tests with the Bonferroni correction.\nIn the replication with S\nENTI STRENGTH we observe that (1) the claim that Java has\nmore negative score than other languages is not confirmed ( p-value for the (Java, C) pair\nis 0.6552) and (2) lack of statistically significant relation between other programming lan-\nguages is not confirmed either (e.g., p-value for (C,C++) with the two-sided alternative is\n6.9 × 10\n−12). Similarly, in the replication with NLTK neither of the claims of the original\nstudy can be confirmed.\nConsider next the study of the sentiments grouped by the weekday. Guzman et al. report\nthat comments on Monday were more negative than comments on the other days. Simi-\nlarly to the study of programming languages, Table 14 suggests that a similar conclusion\ncan be derived if S\nENTI STRENGTH is used but is no longer the case for NLTK. In fact,\nthe mean NLTK score for Monday is the least negative. The median values both for S EN -\nTI STRENGTH and for NLTK are 0 for all the days suggesting no difference can be found.\nThen Guzman et al. have performed a statistical analysis and compared Monday against\neach of the other days. This analysis “confirmed that commit comments were more negative\non Monday than on Sunday, Tuesday, and Wednesday (p -value ≤ 0.015). We replicated this\nstudy with S\nENTI STRENGTH and observed that p ≤ 0.015 for Tuesday, Friday and Satur-\nday. We can conclude that while the exact days have not been confirmed, at least we still can\nsay that commit comments on Monday are more negative than those on some other days .\nUnfortunately, even a weaker conclusion cannot be confirmed if NLTK has been used: p\nexceeds the 0.015 for all days (in fact, p ≥ 0.72 for all days).\nFinally, Table 15 shows that NLTK evaluates the comments made in the afternoon as\nslightly more negative than comments in the evening, in contrast to S\nENTI STRENGTH that\nindicates the afternoon comments as the most positive, or at least the least negative ones.\nWe could not replicate those results neither for S\nENTI STRENGTH nor for NLTK.\nEmpir Software Eng (2017) 22:2543–2584 2571\n5.4 Discussion\nWhen replicating the study of Pletea et al. we confirm the original observation that secu-\nrity comments or discussions are more often negative than the non-security comments or\ndiscussions. We also observe that the when compared with the manually labeled security dis-\ncussions both tools produce mixed results. However, we could not find evidence supporting\nthe suggestion that security-related discussions are more emotional.\nWhen trying to replicate the results of Guzman et al. we could not derive the same con-\nclusion when a different tool has been used. The only conclusion we could replicate when\nthe same tool has been used is that the commit comments on Monday are more negative\nthan those on some other days , which is a weakened form of the original claim. Recently\nIslam and Zibran ( 2016) have performed a similar study of the differences between emo-\ntions expressed by developers during different times and days of a week. Similarly to\nGuzman et al. Islam and Zibran have studied commit messages and used S\nENTI STRENGTH ;\nas opposed Guzman et al. they have considered 50 projects with the highest number of com-\nmits from the Boa dataset (Dyer et al. 2013) rather than the 2014 MSR mining challenge\ndataset of 90 GitHub projects (Gousios 2013). In sharp contrast with the work of Guzman\net al. no significant differences have been found in the developers’ emotions in different\ntimes and days of a week.\nOur replication studies show that validity of conclusions of the previously published\npapers such as the ones by Pletea et al. ( 2014) and Guzman et al. ( 2014) should be ques-\ntioned and ideally reassessed when (or if) a sentiment analysis tool will become available\nspecifically targeting software engineering domain.\n5.5 Threats to V alidity\nAs any empirical study the current replications are subject to threats to validity. Since we\nhave tried to follow the methodology presented in the papers being replicated as closely as\npossible, we have also inherited some of the threats to validity of those papers, e.g., that\nthe dataset under consideration is not representative for GitHub as a whole. Furthermore,\nwe had to convert the NLTK scores to the [−5, 5] scale and this conversion might have\nintroduced additional threats to validity. Finally, we are aware that the pairwise Wilcoxon\ntest as done in Section 5.3.2 might not be the preferred approach from the statistical point of\nview: this is why a more advanced statistical technique has been used in Section 4.H o w e v e r ,\nto support the comparative aspects of replication in Section 5.3.2 we present the results\nexactly in the same way as in the original work (Guzman et al. 2014).\n6 Related Work\nThis paper builds on our previous work (Jongeling et al. 2015). The current submission\nextends it by reporting on a follow-up study (Section 3.3), replication of two recent studies\n(Section 5) as well presenting a more elaborate discussion of the related work below.\n6.1 Sentiment Analysis in Large T ext Corpora\nAs announced in the Manifesto for Agile Software Development (Beck et al. 2001), the\ncentrality of developer interaction in large scale software development has come to be\nincreasingly recognized in recent times (Datta et al. 2012;S c h r¨oter et al. 2012). Today,\n2572 Empir Software Eng (2017) 22:2543–2584\nsoftware development is influenced in myriad ways by how developers talk, and what\nthey talk about. With distributed teams developing and maintaining many software systems\ntoday (Cataldo and Herbsleb 2008), developer interaction is facilitated by collaborative\ndevelopment environments that capture details of discussion around development activities\n(Costa et al. 2011). Mining such data offers an interesting opportunity to examine\nimplications of the sentiments reflected in developer comments.\nSince its inception, sentiment analysis has become a popular approach towards classify-\ning text documents by the predominant sentiment expressed in them (Pang et al. 2002). As\npeople increasingly express themselves freely in online media such as the microblogging\nsite Twitter, or in product reviews on Web marketplaces such as Amazon, rich corpora of\ntext are available for sentiment analysis. Davidov et al., have suggested a semi-supervised\napproach for recognizing sarcastic sentences in Twitter and Amazon (Davidov et al. 2010).\nAs sentiments are inherently nuanced, a major challenge in sentiment analysis is to dis-\ncern the contextual meaning of words. Pak and Patrick suggest an automated and language\nindependent method for disambiguating adjectives in Twitter data (Pak and Paroubek 2010)\nand Agarwal et al., have proposed an approach to correctly identify the polarity of tweets\n(Agarwal et al. 2011). Mohammad, Kiritchenko, and Xiaodan report the utility of using\nsupport vector machine (SVM) base classifiers while analyzing sentiments in tweets\n(Mohammad et al. 2013). Online question and answer forums such as Yahoo! Answers are\nalso helpful sources for sentiment mining data (Kucuktunc et al. 2012).\n6.2 Sentiment Analysis Application in Software Engineering\nThe burgeoning field of tools, methodologies, and results around sentiment analysis have\nalso impacted how we examine developer discussion. Goul et al. examine how require-\nments can be extracted from sentiment analysis of app store reviews (Goul et al. 2012).\nThe authors conclude that while sentiment analysis can facilitate requirements engineer-\ning, in some cases algorithmic analysis of reviews can be problematic (Goul et al. 2012).\nUser reviews of a software system in operation can offer insights into the quality of the sys-\ntem. However given the unstructured nature of review comments, it is often hard to reach\na clear understanding of how well a system is functioning. A key challenge comes from\n“... different sentiment of the same sentence in different environment”. To work around this\nproblem, Leopairote et al. propose a methodology that combines lists of positive and neg-\native sentiment words with rule based classification (Leopairote et al. 2013). Mailing lists\noften characterize large, open source software systems as different stakeholders discuss their\nexpectations as well as disappointments from the system. Analyzing the sentiment of such\ndiscussions can be an important step towards a deeper understanding of the corresponding\necosystem. Tourani et al. seek to identify distress or happiness in a development team by\nanalyzing sentiments in Apache mailing lists (Tourani et al. 2014). The study concludes\nthat developer and user mailing lists carry similar sentiments, though differently focused;\nand automatic sentiment analysis techniques need to be tuned specifically to the software\nengineering context (Novielli et al. 2015). Impact of the sentiment on issue resolution time,\nsimilar to RQ3 discussed in Section 4, have also been considered in the literature (Garcia\net al. 2013;O r t ue ta l .2015).\nAs mentioned earlier, developer interaction data captured by collaborative development\nenvironments are fertile grounds for analyzing sentiments. There are recent trends around\ndesigning emotion aware environments that employ sentiment analysis and other techniques\nto discern and visualize health of a development team in real time (Vivian et al. 2015).\nEmpir Software Eng (2017) 22:2543–2584 2573\nLatest studies have also explored the symbiotic relationship between collaborative software\nengineering and different kinds of task based emotions (Dewan 2015).\n6.3 Sentiment Analysis T ools\nAs already mentioned in the introduction, application of sentiment analysis tools to software\nengineering texts has been studied in a series of recent publications (Garcia et al. 2013;\nGuzman et al. 2014; Guzman and Bruegge 2013; Novielli et al. 2015;O r t ue ta l .2015;\nPanichella et al. 2015; Pletea et al. 2014; Rousinopoulos et al. 2014)\nWith the notable exception of the work of Panichella et al. ( 2015) that trained their\nown classifier on manually labeled software engineering data, all other works have reused\nthe existing sentiment analysis tools. As such reuse of those tools introduced a commonly\nrecognized threat to validity of the results obtained: those tools have been trained on non-\nsoftware engineering related texts such as movie reviews or product reviews and might\nmisidentify (or fail to identify) polarity of a sentiment in a software engineering artefact\nsuch as a commit comment (Guzman et al. 2014; Pletea et al. 2014).\nIn our previous work (Jongeling et al. 2015) and in the current submission we perform\na series of quantitative analyses aiming at evaluation whether the choice of the sentiment\nanalysis tool can affect the validity of the software engineering results. A complementary\napproach to evaluating the applicability of sentiment analysis tools to software engineer-\ning data has been followed by Novielli et al. ( 2015) that performed a qualitative analysis\nof S\nTACK OVERFLOW posts and compared the results of S ENTI STRENGTH with those\nobtained by manual evaluation.\nBeyond the discussion of sentiment analysis tools observations similar to those we made\nhave been made in the past for software metric calculators (Barkmann et al. 2009) and code\nsmell detection tools (Fontana et al. 2011). Similarly to our findings, disagreement between\nthe tools was observed.\n6.4 Replications and Negative Results\nThis paper builds on our previous work (Jongeling et al. 2015). The current submission\nextends it by reporting on replication of two recent studies (Section 5). There is an enduring\nconcern about the lack of replication studies in empirical software engineering: “Replica-\ntion is not supported, industrial cases are rare ... In order to help the discipline mature,\nwe think that more systematic empirical evaluation is needed” (Tonella et al. 2007). The\nchallenges around replication studies in empirical software engineering have been iden-\ntified by Mende (2010). de Magalh ˜aes et al. analyzed 36 papers reporting empirical and\nnon-empirical studies related to replications in software engineering and concluded that not\nonly do we need to replicate more studies in software engineering, expansion of “specific\nconceptual underpinnings, definitions, and process considering the particularities” are also\nneeded (de Magalh ˜aes et al. 2014). Recent studies have begun to address this replication\ngap (Sfetsos et al. 2012; Greiler et al. 2015).\nOne of the most important benefits of replication studies center around the possibility\nof arriving at negative results. Although negative results have been widely reported and\nregarded in different fields of computing since many years (Pritchard 1984; Fuhr and Muller\n1987), its importance is being reiterated in recent years (Giraud-Carrier and Dunham 2011).\nBy carefully and objectively examining what went wrong in the quest for expected outcome,\nthe state-of-art and practice can be enhanced (Lindsey 2011;T ¨aht 2014). We believe the\nresults reported in this paper can aid such enhancement.\n2574 Empir Software Eng (2017) 22:2543–2584\n7 Conclusions\nIn this paper we have studied the impact of the choice of a sentiment analysis tool when\nconducting software engineering studies. We have observed that not only do the tools con-\nsidered not agree with the manual labeling, but also they do not agree with each other, that\nthis disagreement can lead to diverging conclusions and that previously published results\ncannot be replicated when different sentiment analysis tools are used.\nOur results suggest a need for sentiment analysis tools specially targeting the soft-\nware engineering domain. Moreover, going beyond the specifics of the sentiment analysis\ndomain, we would like to encourage the researchers to reuse ideas rather than tools.\nAcknowledgments We are very grateful to Alessandro Murgia and Marco Ortu for making their datasets\navailable for our study, and to Bogdan Vasilescu and reviewers of ICSME 2015 for providing feedback on\nthe preliminary version of this manuscript.\nOpen Access This article is distributed under the terms of the Creative Commons Attribution 4.0 Inter-\nnational License ( http://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution,\nand reproduction in any medium, provided you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons license, and indicate if changes were made.\nAppendix A: Agreement of Sentiment Analysis T ools with the Manual\nLabeling and with each other\nTable 16 presents the confusion matrices corresponding to Table 2. Similarly, Table 17\npresents the confusion matrices corresponding to Table 3.\nTa b l e 1 6 Confusion matrices corresponding to Table 2\n⇓ pos neu neg ⇓ pos neu neg\nNLTK Manual SENTI STRENGTH Manual\npos 26 27 4 pos 30 53 3\nneu 6 128 1 neu 10 126 7\nneg 9 47 14 neg 1 23 9\nAlchemy Manual Stanford NLP Manual\npos 31 39 3 pos 20 13 1\nneu 3 74 1 neu 11 67 1\nneg 7 89 15 neg 10 122 17\nNLTK S\nENTI STRENGTH NLTK Alchemy\npos 32 21 4 pos 39 6 12\nneu 34 89 12 neu 21 55 59\nneg 20 33 17 neg 13 17 40\nNLTK Stanford NLP SENTI STRENGTH Alchemy\npos 19 16 22 pos 44 13 29\nneu 9 51 75 neu 26 62 55\nneg 6 12 52 neg 3 3 27\nS\nENTI STRENGTH Stanford NLP Alchemy Stanford NLP\npos 20 22 44 pos 23 16 34\nn e u 1 35 77 3 n e u 6 3 24 0\nneg 1 0 32 neg 5 31 75\nEmpir Software Eng (2017) 22:2543–2584 2575\nTa b l e 1 7 Confusion matrices corresponding to Table 3\nNLTK and Manual NLTK and Manual\nSENTI STRENGTH pos neu neg , Alchemy pos neu neg\npos 23 8 1 pos 23 14 2\nneu 4 85 0 neu 2 53 0\nneg 0 10 7 neg 3 24 13\nNLTK and Manual Alchemy and Manual\nStanford NLP pos neu neg S ENTI STRENGTH , pos neu neg\npos 16 3 0 pos 26 17 1\nneu 3 48 0 neu 2 59 1\nneg 5 34 13 neg 1 18 8\nS\nENTI STRENGTH Manual Alchemy Manual\nand Stanford NLP pos neu neg and Stanford NLP pos neu neg\npos 17 3 0 pos 19 4 0\nneu 3 53 1 neu 1 30 1\nneg 1 23 8 neg 5 56 14\nNLTK, Alchemy Manual NLTK, Stanford NLP Manual\nand S\nENTI STRENGTH pos neu neg and S ENTI STRENGTH pos neu neg\npos 21 5 1 pos 15 1 0\nneu 2 43 0 neu 2 37 0\nneg 0 9 7 neg 0 10 6\nAlchemy, Stanford NLP Manual NLTK, Alchemy Manual\nand S\nENTI STRENGTH pos neu neg and Stanford NLP pos neu neg\npos 16 1 0 pos 15 2 0\nneu 1 29 1 neu 1 23 0\nneg 1 18 7 neg 3 19 12\nall tools Manual\npos neu neg\npos 14 1 0\nneu 1 22 0\nneg 0 9 6\nAppendix B: Comparison of NLTK and S ENTI STRENGTH\nin Combination with Politeness\nTables 18, 19 and 20 are similar to Table 7 and are provided for the sake of completeness.\nTa b l e 1 8 Comparison of NLTK and S ENTI STRENGTH in combination with politeness for the G NOME\ndataset. Thresholds for statistical significance: 0.05 ( ∗), 0.01 ( ∗∗), 0.001 ( ∗∗∗). Exact p-values are indicated\nas subscripts. 0 indicates that the p-value is too small to be computed precisely\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\ndescr\nneg neu pos neg neu pos neg neu pos\nimp 43702 260570 15306 48835 259271 11472 14105 219444 1111\nneu 9945 30794 4883 9513 33227 2882 2627 22958 617\npol 385 542 191 237 728 153 97 378 57\nneg.imp > neu.imp\n∗∗∗\n0 neg.imp > neu.imp∗∗∗\n0\nneg.imp > neu.imp∗∗∗\n0\n2576 Empir Software Eng (2017) 22:2543–2584\nTa b l e 1 8 (continued)\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\nneg.neu > neg.imp∗∗∗\n0 neg.neu > neg.imp∗∗∗\n0\nneg.neu > neu.imp∗∗∗\n0\nneg.neu > neu.imp∗∗∗\n0\nneg.neu > neu.imp∗∗∗\n0\nneg.neu > pos.imp∗∗\n1.59×10−3 neg.neu > pos.imp∗∗∗\n0\nneg.pol > neu.imp∗∗∗\n1.62×10−8 neg.pol > neu.imp∗∗∗\n9.54×10\n−14 neg.pol > neu.imp∗∗\n2.16×10\n−3\nneg.pol > pos.imp∗∗∗\n5.23×10\n−4\nneu.neu > neg.imp∗∗∗\n0 neu.neu > neg.imp∗∗∗\n0\nneu.neu > neg.imp∗\n1.16×10−2\nneu.neu > neg.neu∗∗\n1.65×10−3\nneg.neu > neu.neu∗∗∗\n6.78×10\n−8\nneu.neu > neu.imp∗∗∗\n0 neu.neu > neu.imp∗∗∗\n0\nneu.neu > neu.imp∗∗∗\n0\nneu.neu > pos.imp∗∗∗\n0\nneu.neu > pos.imp∗∗∗\n0\nneu.pol > neg.imp∗∗∗\n1.59×10−5\nneu.pol > neu.imp∗∗∗\n0 neu.pol > neu.imp∗∗∗\n0\nneu.pol > neu.imp∗∗∗\n0\nneu.pol > pos.imp∗∗∗\n4.95×10−5\npos.imp > neg.imp∗∗∗\n0 neg.imp > pos.imp∗∗∗\n0\npos.imp > neu.imp∗∗∗\n0\npos.imp > neu.imp∗∗∗\n0\npos.imp > neu.imp∗∗∗\n0\npos.neu > neg.imp∗∗∗\n0\npos.neu > neg.imp∗∗∗\n1.9×10−7 pos.neu > neg.imp∗\n3.29×10−2\npos.neu > neg.neu∗∗∗\n1.6×10−7\npos.neu > neg.pol∗\n1.35×10−2\npos.neu > neu.imp∗∗∗\n0 pos.neu > neu.imp∗∗∗\n0\npos.neu > neu.imp∗∗∗\n0\npos.neu > neu.neu∗\n1.54×10−2\npos.neu > pos.imp∗∗∗\n0 pos.neu > pos.imp∗∗∗\n0\npos.pol > neg.imp∗∗∗\n5.29×10−4\npos.pol > neu.imp∗∗∗\n2.22×10−16 pos.pol > neu.imp∗∗∗\n2.34×10\n−6 pos.pol > neu.imp∗∗∗\n5.2×10\n−5\nTa b l e 1 9 Comparison of NLTK and S ENTI STRENGTH in combination with politeness for the S TACK\nOVERFLOW datasets. Thresholds for statistical significance: 0.05 ( ∗). 0.01 ( ∗∗), 0.001 ( ∗∗∗). Exact p-values\nare indicated as subscripts. 0 indicates that the p-value is too small to be computed precisely\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\ntitle\nneg neu pos neg neu pos neg neu pos\nimp 61 244 29 43 270 21 11 203 5\nn e u 1 93 71 21 05 53 5 3 43\np o l 440053 030\nneutral.polite > pos.impolite∗∗∗\n0\ndescr\nneg neu pos neg neu pos neg neu pos\ni m p 3 37 4 1 22 48 1 14 0\nEmpir Software Eng (2017) 22:2543–2584 2577\nTa b l e 1 9 (continued)\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\nneu 38 20 9 15 32 20 10 8 2\npol 178 44 77 63 127 109 41 23 40\nneg.neutral > pos.impolite∗∗∗\n2,37×10−4\nneg.polite > pos.impolite∗\n4,87×10\n−2\npos.polite > pos.impolite∗∗\n5,82×10\n−3\nTa b l e 2 0 Comparison of NLTK and S ENTI STRENGTH in combination with politeness for the ASF datasets.\nThresholds for statistical significance: 0.05 ( ∗). 0.01 ( ∗∗), 0.001 ( ∗∗∗). Exact p-values are indicated as\nsubscripts. 0 indicates that the p-value is too small to be computed precisely\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\ntitle\nneg neu pos neg neu pos neg neu pos\nimp 15690 55726 5819 19228 50437 7573 5216 37083 733\nneu 3527 11988 2404 4799 11265 1856 1195 7583 340\npol 150 234 125 114 314 81 39 152 33\nneg.imp > neg.neu\n∗∗\n6.51×10−3\nneg.imp > neu.neu∗∗\n6.05×10−3\nneu.imp > neg.neu∗∗\n5.97×10\n−3\nneu.neu > neg.neu∗\n1.29×10−2\nneg.neu > neu.neu∗\n2.9×10−2\npos.imp > neg.imp∗∗∗\n1.55×10−10\npos.imp > neg.neu∗∗∗\n8.81×10−4 pos.imp > neg.neu∗∗∗\n7.53×10\n−4\npos.imp > neu.imp∗∗∗\n0\npos.imp > neu.neu∗∗∗\n0\npos.neu > neg.imp∗\n1.73×10−2\npos.neu > neg.neu∗\n3.14×10−2\npos.neu > neu.imp∗∗∗\n3.04×10−4\npos.neu > neu.neu∗∗∗\n6.62×10\n−6\ndescr\nneg neu pos neg neu pos neg neu pos\nimp 5293 10291 1881 5553 9595 2346 1937 5816 358\nneu 9505 16709 4357 10357 15205 5008 3501 8425 1048\npol 15493 15433 6872 13041 16161 8586 5530 6646 2401\nab c\nneg.imp > neu.imp∗∗\n1.06×10−3\nneg.neu > neg.imp∗\n2.92×10\n−2 neg.neu > neg.imp∗\n3.36×10\n−2\nneg.neu > neu.imp∗∗∗\n0 neg.neu > neu.imp∗∗∗\n6.23×10−6 neg.neu > neu.imp∗∗∗\n7.57×10\n−14\nneg.neu > neu.neu∗∗∗\n9.43×10−7 neg.neu > neu.neu∗∗∗\n4.84×10\n−7\nneg.pol > neg.imp∗∗∗\n0 neg.pol > neg.imp∗∗∗\n0\nneg.pol > neg.imp∗∗∗\n0\nneg.pol > neg.neu∗∗∗\n0 neg.pol > neg.neu∗∗∗\n0\nneg.pol > neg.neu∗∗∗\n0\nneg.pol > neu.imp∗∗∗\n0 neg.pol > neu.imp∗∗∗\n0\nneg.pol > neu.imp∗∗∗\n0\n2578 Empir Software Eng (2017) 22:2543–2584\nTa b l e 2 0 (continued)\nNLTK SENTI STRENGTH NLTK ∩ SENTI STRENGTH\nneg.pol > neu.neu∗∗∗\n0 neg.pol > neu.neu∗∗∗\n0\nneu.pol > neg.pol∗∗\n2.49×10−3\nneg.pol > neu.pol∗∗∗\n0 neu.pol > neg.pol∗∗\n2.49×10−3\nneg.pol > pos.imp∗∗∗\n0 neg.pol > pos.imp∗∗∗\n4.56×10−10\nneg.pol > pos.neu∗∗∗\n0 neg.pol > pos.neu∗∗∗\n8.89×10−6\nneu.neu > neu.imp∗∗∗\n2.83×10\n−5 neu.neu > neu.imp∗\n2.34×10\n−2 neu.neu > neu.imp∗\n1.53×10\n−2\nneu.pol > neg.imp∗∗∗\n0 neu.pol > neg.imp∗∗∗\n0\nneu.pol > neg.imp∗∗∗\n0\nneu.pol > neg.neu∗∗∗\n0 neu.pol > neg.neu∗∗∗\n0\nneu.pol > neg.neu∗∗∗\n6.2×10−13\nneu.pol > neu.imp∗∗∗\n0 neu.pol > neu.imp∗∗∗\n0\nneu.pol > neu.imp∗∗∗\n0\nneu.pol > neu.neu∗∗∗\n0 neu.pol > neu.neu∗∗∗\n0\nneu.pol > neu.neu∗∗∗\n0\nneu.pol > pos.imp∗∗∗\n2.79×10−9 neu.pol > pos.imp∗∗∗\n0\nneu.pol > pos.neu∗∗∗\n3.99×10−14 neu.pol > pos.neu∗∗∗\n7.07×10\n−14\npos.imp > neg.imp∗∗\n1.91×10\n−3\npos.imp > neu.imp∗∗∗\n1.82×10−4 pos.imp > neu.imp∗∗∗\n2.06×10\n−6 pos.imp > neu.imp∗∗\n2.89×10\n−3\npos.imp > neu.neu∗\n1.38×10\n−2\npos.neu > neg.imp∗\n2.06×10\n−2 pos.neu > neg.imp∗∗∗\n0 pos.neu > neg.imp∗∗∗\n2.03×10−9\npos.neu > neg.neu∗∗∗\n1.84×10\n−13 pos.neu > neg.neu∗∗∗\n3.49×10\n−4\npos.neu > neu.imp∗∗∗\n2.24×10−13 pos.neu > neu.imp∗∗∗\n0 pos.neu > neu.imp∗∗∗\n0\npos.neu > neu.neu∗∗∗\n1.7×10−5 pos.neu > neu.neu∗∗∗\n0 pos.neu > neu.neu∗∗∗\n8.22×10−15\npos.pol > neg.imp∗∗∗\n0 pos.pol > neg.imp∗∗∗\n0\npos.pol > neg.imp∗∗∗\n0\npos.pol > neg.neu∗∗∗\n0\npos.pol > neg.neu∗∗∗\n0\npos.pol > neg.neu∗∗∗\n0\npos.pol > neg.pol∗∗∗\n2.45×10−12 pos.pol > neg.pol∗\n4.21×10\n−2\npos.pol > neu.imp∗∗∗\n0 pos.pol > neu.imp∗∗∗\n0\npos.pol > neu.imp∗∗∗\n0\npos.pol > neu.neu∗∗∗\n0 pos.pol > neu.neu∗∗∗\n0\npos.pol > neu.neu∗∗∗\n0\npos.pol > neu.pol∗∗∗\n1.54×10−12 pos.pol > neu.pol∗∗\n1.24×10\n−3 pos.pol > neu.pol∗∗∗\n1.79×10\n−6\npos.pol > pos.imp∗∗∗\n0 pos.pol > pos.imp∗∗∗\n0\npos.pol > pos.imp∗\n1.57×10−2\npos.pol > pos.neu∗∗∗\n0 pos.pol > pos.neu∗∗∗\n0\npos.pol > pos.neu∗\n3.06×10−2\na Sentiment of 174 descriptions could not been determined.\nb Sentiment of 183 descriptions could not been determined.\ncSentiment of 81 descriptions could not been determined.\nReferences\nAbbasi A, Hassan A, Dhar M (2014) Benchmarking Twitter sentiment analysis tools. In: International\nConference on Language Resources and Evaluation. ELRA, Reykjavik, Iceland, pp 823–829\nAgarwal A, Xie B, V ovsha I, Rambow O, Passonneau R (2011) Sentiment Analysis of Twitter Data. In:\nProceedings of the Workshop on Languages in Social Media, LSM ’11, pp 30–38. Association for\nComputational Linguistics, Stroudsburg, PA, USA. http://dl.acm.org/citation.cfm?id=2021109.2021114\nAsaduzzaman M, Bullock MC, Roy CK, Schneider KA Bug introducing changes: A case study with android.\nIn: Lanza et al. [43], pp 116–119. doi: 10.1109/MSR.2012.6224267\nBaccianella S, Esuli A, Sebastiani F (2010) SentiWordNet 3.0: an enhanced lexical resource for sentiment\nanalysis and opinion mining. In: Proceedings of the Seventh Conference on International Language\nResources and Evaluation (LREC’10). European Language Resources Association (ELRA), Valletta,\nMalta. http://www.lrec-conf.org/proceedings/lrec2010/pdf/769\nPaper.pdf\nEmpir Software Eng (2017) 22:2543–2584 2579\nBakeman R, Gottman JM (1997) Observing interaction: an introduction to sequential analysis. Cambridge\nUniversity Press. https://books.google.nl/books?id=CMj2SmcijhEC\nBarkmann H, Lincke R, L ¨owe W (2009) Quantitative evaluation of software quality metrics in open-\nsource projects. In: IEEE International Workshop on Quantitative Evaluation of large-scale Systems and\nTechnologies, pp 1067–1072\nBatista GEAPA, Carvalho ACPLF, Monard MC (2000) Applying one-sided selection to unbalanced datasets.\nIn: Mexican International Conference on Artificial Intelligence: Advances in Artificial Intelligence.\nSpringer-Verlag, London, UK, UK, pp 315–325\nBeck K, Beedle M, van Bennekum A, Cockburn A, Cunningham W, Fowler M, Grenning J, Highsmith\nJ, Hunt A, Jeffries R, Kern J, Marick B, Martin RC, Mellor S, Schwaber K, Sutherland J, Thomas\nD (2001) Manifesto for agile software development http://agilemanifesto.org/principles.html Last\naccessed: October 14, 2015\nBird S, Loper E, Klein E (2009) Natural language processing with Python. O’Reilly Media Inc\nBrunner E, Munzel U (2000) The Nonparametric Behrens-Fisher Problem: Asymptotic Theory and a Small-\nSample Approximation. Biometrical Journal 42(1):17–25\nCapiluppi A, Serebrenik A, Singer L (2013) Assessing technical candidates on the social web. Software.\nIEEE 30(1):45–51. doi:10.1109/MS.2012.169\nCataldo M, Herbsleb JD (2008) Communication networks in geographically distributed software develop-\nment. In: Proceedings of the 2008 ACM conference on Computer supported cooperative work, CSCW\n’08, pp 579–588. ACM, New York, NY , USA. doi:10.1145/1460563.1460654\nCohen J (1968) Weighted kappa: Nominal scale agreement with provision for scaled disagreement or partial\ncredit. Psychol Bull 70(4):213–220\nCosta JM, Cataldo M, de Souza CR (2011) The scale and evolution of coordination needs in large-scale\ndistributed projects: implications for the future generation of collaborative tools. In: Proceedings of the\n2011 annual conference on Human factors in computing systems, CHI ’11, pp 3151–3160. ACM, New\nYork, NY, USA. doi:10.1145/1978942.1979409\nDajsuren Y, van den Brand MGJ, Serebrenik A, Roubtsov S (2013) Simulink models are also software: Mod-\nularity assessment. In: Proceedings of the 9th International ACM Sigsoft Conference on Quality of Soft-\nware Architectures, QoSA ’13, pp 99–106. ACM, New York, NY , USA. doi:10.1145/2465478.2465482\nDanescu-Niculescu-Mizil C, Sudhof M, Jurafsky D, Leskovec J, Potts C (2013) A computational approach\nto politeness with application to social factors. In: Proceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2013, 4-9 August 2013, Sofia, Bulgaria, V olume 1:\nLong Papers, pp 250–259. The Association for Computer Linguistics. http://aclweb.org/anthology/P/\nP13/P13-1025.pdf\nDatta S, Sindhgatta R, Sengupta B (2012) Talk versus work: characteristics of developer collaboration on\nthe Jazz platform. In: Proceedings of the ACM international conference on Object oriented program-\nming systems languages and applications, OOPSLA ’12, pp 655–668. ACM, New York, NY , USA.\ndoi:10.1145/2384616.2384664\nDavidov D, Tsur O, Rappoport A (2010) Semi-supervised recognition of sarcastic sentences in Twitter and\nAmazon. In: Proceedings of the Fourteenth Conference on Computational Natural Language Learning,\nCoNLL ’10, pp. 107–116. Association for Computational Linguistics, Stroudsburg, PA, USA. http://dl.\nacm.org/citation.cfm?id=1870568.1870582\nde Magalh ˜aes CVC, da Silva FQB, Santos RES (2014) Investigations about replication of empirical studies\nin software engineering: Preliminary findings from a mapping study. In: Proceedings of the 18th Inter-\nnational Conference on Evaluation and Assessment in Software Engineering, EASE ’14, pp 37:1–37:10.\nACM, New York, NY , USA. doi: 10.1145/2601248.2601289\nDestefanis G, Ortu M, Counsell S, Swift S, Marchesi M, Tonelli R (2016) Peer J Comput Sci 2(e73):1–35.\ndoi:10.7717/peerj-cs.73\nDewan P (2015) Towards Emotion-Based Collaborative Software Engineering. In: 2015 IEEE/ACM 8th\nInternational Workshop on Cooperative and Human Aspects of Software Engineering (CHASE), pp 109–\n112. doi:10.1109/CHASE.2015.32\nDunn OJ (1961) Multiple comparisons among means. J Am Stat Assoc 56(293):52–64\nDyer R, Nguyen HA, Rajan H, Nguyen TN (2013) Boa: A language and infrastructure for analyzing ultra-\nlarge-scale software repositories. In: Proceedings of the 2013 International Conference on Software\nEngineering, ICSE ’13, pp 422–431. IEEE Press, Piscataway, NJ, USA. http://dl.acm.org/citation.cfm?\nid=2486788.2486844\nFleiss JL, Levin B, Paik MC (2003) Statistical methods for rates and proportions, 3rd edn. Wiley Series in\nProbability and Statistics. Wiley, Hoboken, NJ\nFontana FA, Mariani E, Morniroli A, Sormani R, Tonello A (2011) An experience report on using code smells\ndetection tools. In: ICST Workshops, pp 450–457. IEEE\n2580 Empir Software Eng (2017) 22:2543–2584\nFuhr N, Muller P (1987) Probabilistic search term weighting - some negative results. In: Proceedings of\nthe 10th Annual International ACM SIGIR Conference on Research and Development in Information\nRetrieval, SIGIR ’87, pp 13–18. ACM, New York, NY , USA. doi:10.1145/42005.42007\nGabriel KR (1969) Simultaneous test procedures—some theory of multiple comparisons. Ann Math Stat\n40(1):224–250\nGamon M, Aue A, Corston-Oliver S, Ringger E (2005) Pulse: Mining customer opinions from free text.\nIn: Proceedings of the 6th International Conference on Advances in Intelligent Data Analysis, IDA’05.\nSpringer-Verlag, Berlin, Heidelberg, pp 121–132. doi: 10.1007/11552253 12\nGarcia D, Zanetti MS, Schweitzer F (2013) The role of emotions in contributors activity: A case study on the\nGentoo community. In: International Conference on Cloud and Green Computing, pp 410–417\nGiraud-Carrier C, Dunham MH (2011) On the importance of sharing negative results. Sigkdd Explor. Newsl.\n12(2):3–4. doi: 10.1145/1964897.1964899\nGoul M, Marjanovic O, Baxley S, Vizecky K (2012) Managing the Enterprise Business Intelligence App\nStore: Sentiment Analysis Supported Requirements Engineering. In: 2012 45th Hawaii International\nConference on System Science (HICSS), pp 4168–4177. doi: 10.1109/HICSS.2012.421\nGousios G (2013) The GHTorrent dataset and tool suite. In: Proceedings of the 10th Working Conference\non Mining Software Repositories, MSR’13, pp 233–236. http://dl.acm.org/citation.cfm?id=2487085.\n2487132\nGreiler M, Herzig K, Czerwonka J (2015) Code ownership and software quality: A replication study. In:\nProceedings of the 12th Working Conference on Mining Software Repositories, MSR ’15, pp. 2–12.\nIEEE Press, Piscataway, NJ, USA. http://dl.acm.org.library.sutd.edu.sg:2048/citation.cfm?id=2820518.\n2820522\nGuzman E, Az ´ocar D, Li Y (2014) Sentiment analysis of commit comments in GitHub: An empirical study.\nIn: MSR, pp 352–355, ACM, New York, NY , USA\nGuzman E, Bruegge B (2013) Towards emotional awareness in software development teams. In: Joint\nMeeting on Foundations of Software Engineering, pp 671–674, ACM, New York, NY , USA\nHall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten IH (2009) The Weka data mining software:\nAn upyear. SIGKDD Explor Newsl 11(1):10–18\nHonkela T, Izzatdust Z, Lagus K (2012) Text mining for wellbeing: Selecting stories using semantic and\npragmatic features. In: Artificial Neural Networks and Machine Learning, Part II, LNCS, vol 7553.\nSpringer, pp 467–474\nHoward MJ, Gupta S, Pollock LL, Vijay-Shanker K (2013) Automatically mining software-based,\nsemantically-similar words from comment-code mappings. In: Zimmermann T, Penta MD, Kim S (eds)\nMSR, pp 377–386. IEEE Computer Society\nHubert L, Arabie P (1985) Comparing partitions. J Classif 2(1):193–218. doi: 10.1007/BF01908075\nIslam MR, Zibran MF (2016) Towards understanding and exploiting developers’ emotional variations in\nsoftware engineering. In: 2016 IEEE 14th International Conference on Software Engineering Research,\nManagement and Applications (SERA), pp 185–192. doi:10.1109/SERA.2016.7516145\nJongeling R, Datta S, Serebrenik A (2015) Choosing your weapons: On sentiment analysis tools for software\nengineering research. In: ICSME, pp 531–535. IEEE. doi:10.1109/ICSM.2015.7332508\nKonietschke F, Hothorn LA, Brunner E (2012) Rank-based multiple test procedures and simultaneous\nconfidence intervals. Electronic Journal of Statistics 6:738–759\nKucuktunc O, Cambazoglu BB, Weber I, Ferhatosmanoglu H (2012) A Large-scale Sentiment Analysis for\nYahoo! Answers. In: Proceedings of the Fifth ACM International Conference on Web Search and Data\nMining, WSDM ’12, pp 633–642. ACM, New York, NY , USA. doi: 10.1145/2124295.2124371\nLanza M, Di Penta M, Xie T (2012) (eds.): 9th IEEE Working Conference of Mining Software Repositories,\nMSR 2012, June 2-3, 2012, Zurich, Switzerland. IEEE Computer Society. http://ieeexplore.ieee.org/xpl/\nmostRecentIssue.jsp?punumber=6220358\nLeopairote W, Surarerks A, Prompoon N (2013) Evaluating software quality in use using user reviews\nmining. In: 2013 10th International Joint Conference on Computer Science and Software Engineering\n(JCSSE), pp 257–262. doi:10.1109/JCSSE.2013.6567355\nLewis DD, Gale W A (1994) A sequential algorithm for training text classifiers. In: Proceedings of the 17th\nAnnual International ACM SIGIR Conference on Research and Development in Information Retrieval,\nSIGIR ’94, pp. 3–12. Springer-Verlag New York, Inc., New York, NY , USA. http://dl.acm.org.dianus.\nlibr.tue.nl/citation.cfm?id=188490.188495\nLi TH, Liu R, Sukaviriya N, Li Y, Yang J, Sandin M, Lee J (2014) Incident ticket analytics for it application\nmanagement services. In: 2014 IEEE International Conference on Services Computing (SCC), pp 568–\n574. doi:10.1109/SCC.2014.80\nEmpir Software Eng (2017) 22:2543–2584 2581\nLindsey MR (2011) What went wrong?: Negative results from V oIP service providers. In: Proceedings of\nthe 5th International Conference on Principles, Systems and Applications of IP Telecommunications,\nIPTcomm ’11, pp 13:1–13:3. ACM, New York, NY , USA. doi: 10.1145/2124436.2124453\nLinstead E, Baldi P (2009) Mining the coherence of GNOME bug reports with statistical topic models. In:\nGodfrey MW, Whitehead J (eds) Proceedings of the 6th International Working Conference on Mining\nSoftware Repositories, MSR 2009 (Co-located with ICSE), Vancouver, BC, Canada, May 16–17, 2009,\nProceedings, pp 99–102. IEEE Computer Society. doi:10.1109/MSR.2009.5069486\nMartie L, Palepu VK, Sajnani H, Lopes CV Trendy bugs: Topic trends in the android bug reports. In: Lanza\net al. [43], pp 120–123. doi:10.1109/MSR.2012.6224268\nMende T (2010) Replication of defect prediction studies: Problems, pitfalls and recommendations. In: Pro-\nceedings of the 6th International Conference on Predictive Models in Software Engineering, PROMISE\n’10, pp 5:1–5:10. ACM, New York, NY , USA. doi: 10.1145/1868328.1868336\nMishne G, Glance NS (2006) Predicting movie sales from blogger sentiment. In: AAAI Spring Symposium:\nComputational Approaches to Analyzing Weblogs, pp 155–158\nMohammad SM, Kiritchenko S, Zhu X (2013) NRC-Canada: Building the State-of-the-Art in Sentiment\nAnalysis of Tweets. arXiv:1308.6242[cs]\nMurgia A, Tourani P, Adams B, Ortu M (2014) Do developers feel emotions? an exploratory analysis of\nemotions in software artifacts. In: MSR, pp 262-271, ACM, New York, NY , USA\nNovielli N, Calefato F, Lanubile F (2015) The challenges of sentiment detection in the social programmer\necosystem. In: Proceedings of the 7th International Workshop on Social Software Engineering, SSE\n2015, pp 33–40. ACM, New York, NY , USA. doi:10.1145/2804381.2804387\nOrtu M, Adams B, Destefanis G, Tourani P, Marchesi M, Tonelli R (2015) Are bullies more productive?\nempirical study of affectiveness vs. issue fixing time. In: MSR\nOrtu M, Destefanis G, Adams B, Murgia A, Marchesi M, Tonelli R (2015) The JIRA repository dataset:\nUnderstanding social aspects of software development. In: Proceedings of the 11th International Con-\nference on Predictive Models and Data Analytics in Software Engineering, PROMISE ’15, pp 1:1–1:4.\nACM, New York, NY , USA. doi: 10.1145/2810146.2810147\nOrtu M, Destefanis G, Kassab M, Counsell S, Marchesi M, Tonelli R (2015) Would you mind fixing\nthis issue? - an empirical analysis of politeness and attractiveness in software developed using agile\nboards. In: Lassenius C, Dingsøyr T, Paasivaara M (eds) Agile Processes, in Software Engineering,\nand Extreme Programming - 16th International Conference, XP 2015, Helsinki, Finland, May 25–29,\n2015, Proceedings, Lecture Notes in Business Information Processing, vol 212. Springer, pp 129–140.\ndoi:10.1007/978-3-319-18612-2\n11\nPak A, Paroubek P (2010) Twitter Based System: Using Twitter for Disambiguating Sentiment Ambiguous\nAdjectives. In: Proceedings of the 5th International Workshop on Semantic Evaluation, SemEval ’10, pp.\n436–439. Association for Computational Linguistics, Stroudsburg, PA, USA. http://dl.acm.org/citation.\ncfm?id=1859664.1859761\nPang B, Lee L (2007) Opinion mining and sentiment analysis. Found Trends Inf Retr 2(1-2):1–135\nPang B, Lee L, Vaithyanathan S (2002) Thumbs Up?: sentiment classification using machine learning\ntechniques. In: Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Pro-\ncessing - V olume 10, EMNLP ’02, pp 79–86. Association for Computational Linguistics, Stroudsburg,\nPA, USA. doi:10.3115/1118693.1118704\nPanichella S, Sorbo AD, Guzman E, Visaggio CA, Canfora G, Gall HC (2015) How can I improve my app?\nclassifying user reviews for software maintenance and evolution. In: ICSME. IEEE, pp 281–290\nPletea D, Vasilescu B, Serebrenik A (2014) Security and emotion: Sentiment analysis of security discussions\non GitHub. In: MSR. ACM, New York, NY, USA, pp 348-351. doi:10.1145/2597073.2597117\nPritchard P (1984) Some negative results concerning prime number generators. Commun ACM 27(1):53–57.\ndoi:10.1145/69605.357970\nRand WM (1971) Objective criteria for the evaluation of clustering methods. J Am Stat Assoc 66(336):846–850\nRousinopoulos AI, Robles G, Gonz ´alez-Barahona JM (2014) Sentiment analysis of Free/Open Source\ndevelopers: preliminary findings from a case study. Revista Eletr ˆonica de Sistemas de Informac ¸ ˜ao\n13(2):6:1–6:21\nSantos JM, Embrechts M (2009) On the use of the adjusted rand index as a metric for evaluating supervised\nclassification. In: International Conference on Artificial Neural Networks, LNCS, vol 5769. Springer,\npp 175–184\nSchr¨oter A, Aranda J, Damian D, Kwan I (2012) To talk or not to talk: factors that influence communication\naround changesets. In: Proceedings of the ACM 2012 conference on Computer Supported Cooperative\nWork, CSCW ’12, pp 1317–1326. ACM, New York, NY, USA. doi: 10.1145/2145204.2145401\nSfetsos P, Adamidis P, Angelis L, Stamelos I, Deligiannis I (2012) Investigating the impact of personal-\nity and temperament traits on pair programming: a controlled experiment replication. In: 2012 Eighth\nInternational Conference on the Quality of Information and Communications Technology (QUATIC),\npp 57–65. doi: 10.1109/QUATIC.2012.36\n2582 Empir Software Eng (2017) 22:2543–2584\nSheskin DJ (2007) Handbook of parametric and nonparametric statistical procedures, 4 edn. Chapman &\nHall\nShihab E, Kamei Y, Bhattacharya P (2012) Mining challenge 2012: the Android platform. In: MSR, pp 112–\n115\nShull FJ, Carver JC, Vegas S, Juristo N (2008) The role of replications in empirical software engineering.\nEmpir Softw Eng 13(2):211–218. doi:10.1007/s10664-008-9060-1\nSocher R, Perelygin A, Wu J, Chuang J, Manning CD, Ng A, Potts C (2013) Recursive deep models\nfor semantic compositionality over a sentiment treebank. In: Empirical Methods in Natural Language\nProcessing, pp 1631–1642. Ass. for Comp. Linguistics\nSun X, Li B, Leung H, Li B, Li Y (2015) MSR4SM: Using topic models to effectively mining software repos-\nitories for software maintenance tasks. Inf Softw Technol 66:1–12. doi:10.1016/j.infsof.2015.05.003 .\nhttp://www.sciencedirect.com/science/article/pii/S0950584915001007\nT¨aht D (2014) The value of repeatable experiments and negative results: - a journey through the\nhistory and future of aqm and fair queuing algorithms. In: Proceedings of the 2014 ACM SIG-\nCOMM Workshop on Capacity Sharing Workshop, CSWS ’14, pp 1–2. ACM, New York, NY , USA.\ndoi:10.1145/2630088.2652480\nThelwall M, Buckley K, Paltoglou G (2012) Sentiment strength detection for the social web. J Am Soc Inf\nSci Technol 63(1):163–173\nThelwall M, Buckley K, Paltoglou G, Cai D, Kappas A (2010) Sentiment in short strength detection informal\ntext. J Am Soc Inf Sci Technol 61(12):2544–2558\nTonella P, Torchiano M, Du Bois B, Syst ¨a T (2007) Empirical studies in reverse engineering: State of the art\nand future trends. Empir Softw Eng 12(5):551–571. doi: 10.1007/s10664-007-9037-5\nTourani P, Jiang Y, Adams B (2014) Monitoring sentiment in open source mailing lists: exploratory study on\nthe apache ecosystem. In: Proceedings of 24th Annual International Conference on Computer Science\nand Software Engineering, CASCON ’14, pp 34–44. IBM Corp., Riverton, NJ, USA. http://dl.acm.org/\ncitation.cfm?id=2735522.2735528\nTukey JW (1951) Quick and dirty methods in statistics, part II, Simple analysis for standard designs. In:\nAmerican Society for Quality Control, pp 189–197\nTumasjan A, Sprenger TO, Sandner PG, Welpe IM (2010) Predicting elections with twitter: What 140 charac-\nters reveal about political sentiment. In: International AAAI Conference on Weblogs and Social Media,\npp 178–185\nvan Rijsbergen CJ (1979) Information Retrieval, 2nd edn. Butterworth-Heinemann, Newton, MA, USA\nVasilescu B, Filkov V, Serebrenik A (2013) StackOverflow and GitHub: associations between software\ndevelopment and crowdsourced knowledge. In: 2013 International Conference on Social Computing\n(SocialCom), pp 188–195. doi: 10.1109/SocialCom.2013.35\nVasilescu B, Serebrenik A, van den Brand MGJ (2011) By no means: a study on aggregating software metrics.\nIn: Concas G, Tempero ED, Zhang H, Penta MD (eds) Proceedings of the 2nd International Workshop\non Emerging Trends in Software Metrics, WETSoM 2011, Waikiki, Honolulu, HI, USA, May 24, 2011.\nACM, pp 23–26. doi:10.1145/1985374.1985381\nVasilescu B, Serebrenik A, Goeminne M, Mens T (2013) On the variation and specialisation of work-\nload – a case study of the Gnome ecosystem community. Empir Softw Eng 19(4):955–1008.\ndoi:10.1007/s10664-013-9244-1\nViera AJ, Garrett JM (2005) Understanding interobserver agreement: the kappa statistic. Fam Med\n37(5):360–363\nVivian R, Tarmazdi H, Falkner K, Falkner N, Szabo C (2015) The development of a dashboard tool for visu-\nalising online teamwork discussions. In: Proceedings of the 37th International Conference on Software\nEngineering - V olume 2, ICSE ’15, pp 380–388. IEEE Press, Piscataway, NJ, USA. http://dl.acm.org/\ncitation.cfm?id=2819009.2819070\nWang S, Lo D, Vasilescu B, Serebrenik A (2014) EnTagRec: An enhanced tag recommendation system for\nsoftware information sites. In: ICSME. IEEE, pp 291–300\nWilcoxon F (1945) Individual comparisons by ranking methods. Biom Bull 1(6):80–83\nWilson T, Wiebe J, Hoffmann P (2005) Recognizing contextual polarity in phrase-level sentiment analysis.\nIn: Human Language Technology and Empirical Methods in Natural Language Processing. Association\nfor Computational Linguistics, Stroudsburg, PA, USA, pp 347–354\nYu HF, Ho CH, Juan YC, Lin CJ (2013) Libshorttext: A library for short-text classification and analysis.\nTech. rep., Technical Report. http://www.csie.ntu.edu.tw/\n∼cjlin/papers/libshorttext.pdf\nYu Y, Wang H, Yin G, Wang T (2016) Reviewer recommendation for pull-requests in github: What\ncan we learn from code review and bug assignment? Inf Softw Technol 74:204–218. doi: 10.1016/\nj.infsof.2016.01.004. http://www.sciencedirect.com/science/article/pii/S0950584916000069\nZimmerman DW, Zumbo BD (1992) Parametric alternatives to the Student t test under violation of normality\nand homogeneity of variance. Percept Mot Skills 74(31):835–844\nEmpir Software Eng (2017) 22:2543–2584 2583\nRobbert Jongeling is a consultant at ALTEN Technology in the Netherlands. He received a MSc degree in\nComputer Science and Engineering from Eindhoven University of Technology. After graduation in March of\n2016, he has started his career in software design and engineering. His research interests include empirical\nsoftware engineering.\nProshanta Sarkar is an application developer at IBM India Pvt. Ltd.; He received the M.Tech degree in\nComputer science and Engineering from Heritage Institute of Technology, India. He has more than 2 years\nof experience in roles of application developer across several client engagements in the design, development,\nand deployment of large scale enterprise software systems. His research interests include empirical software\nengineering, cognitive computing and BlockChain.\n2584 Empir Software Eng (2017) 22:2543–2584\nSubhajit Datta is currently a lecturer at the Singapore University of Technology and Design. He has more\nthan 17 years of experience in software design, development, research, and teaching at various organizations\nin the United States of America, India, and Singapore. He is the author of the books Software Engineer-\ning: Concepts and Applications (Oxford University Press, 2010) and Metrics- Driven Enterprise Software\nDevelopment (J. Ross Publishing, 2007), which are widely used by students and practitioners. His research\ninterests include software architecture, empirical software engineering, social computing, and big data. Sub-\nhajit received the PhD degree in computer science from the Florida State University. More details about his\nbackground and interest are available at www.dattas.net.\nAlexander Serebrenik (PhD, K.U. Leuven, Belgium 2003; MSc, Hebrew University, Israel, 1999) is asso-\nciate professor software evolution at Eindhoven University of Technology. He has co-authored a book\n“Evolving Software Systems” (Springer Verlag, 2014), more than 100 scientific papers and articles. He is\nand was the chair of the steering committee chair, general chair and program chair of several conferences in\nthe area of software maintenance and evolution. His research pertains both to technical and social aspects of\nsoftware evolution.",
  "topic": "Sentiment analysis",
  "concepts": [
    {
      "name": "Sentiment analysis",
      "score": 0.8411142826080322
    },
    {
      "name": "Computer science",
      "score": 0.6749935150146484
    },
    {
      "name": "Reuse",
      "score": 0.5954771637916565
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5852826237678528
    },
    {
      "name": "Software",
      "score": 0.5795514583587646
    },
    {
      "name": "Data science",
      "score": 0.48334166407585144
    },
    {
      "name": "Social media",
      "score": 0.4414651393890381
    },
    {
      "name": "Product (mathematics)",
      "score": 0.4175940752029419
    },
    {
      "name": "Software engineering",
      "score": 0.3655303418636322
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3042861223220825
    },
    {
      "name": "World Wide Web",
      "score": 0.2963574528694153
    },
    {
      "name": "Engineering",
      "score": 0.15251898765563965
    },
    {
      "name": "Waste management",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}