{
  "title": "A Measure-Theoretic Characterization of Tight Language Models",
  "url": "https://openalex.org/W4385571048",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5104294805",
      "name": "Li Du",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5087863947",
      "name": "Lucas Torroba Hennigen",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5102825308",
      "name": "Tiago Pimentel",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5029033876",
      "name": "Clara Meister",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A5052467896",
      "name": "Jason Eisner",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5061951606",
      "name": "Ryan Cotterell",
      "affiliations": [
        "ETH Zurich",
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3171737424",
    "https://openalex.org/W2991240545",
    "https://openalex.org/W357046174",
    "https://openalex.org/W2119179558",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4317897818",
    "https://openalex.org/W3197754201",
    "https://openalex.org/W4255871774",
    "https://openalex.org/W1990005915",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W4211192810",
    "https://openalex.org/W4236626873",
    "https://openalex.org/W2950858167",
    "https://openalex.org/W1582706191",
    "https://openalex.org/W4298443330",
    "https://openalex.org/W4205537173",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W2061048794",
    "https://openalex.org/W2139250372",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W2122092249",
    "https://openalex.org/W1547322272",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4229706427",
    "https://openalex.org/W1613162937",
    "https://openalex.org/W2253795368",
    "https://openalex.org/W1564629734",
    "https://openalex.org/W2752779325",
    "https://openalex.org/W2963706817",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W4234891673",
    "https://openalex.org/W1520882901",
    "https://openalex.org/W3142894125",
    "https://openalex.org/W2111041233",
    "https://openalex.org/W1549364818",
    "https://openalex.org/W1491175367",
    "https://openalex.org/W2054125330",
    "https://openalex.org/W2039829163",
    "https://openalex.org/W3100715140"
  ],
  "abstract": "Li Du, Lucas Torroba Hennigen, Tiago Pimentel, Clara Meister, Jason Eisner, Ryan Cotterell. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 9744–9770\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nA Measure-Theoretic Characterization of Tight Language Models\nLi Du6 Lucas Torroba Hennigen@ Tiago PimentelD\nClara MeisterQ Jason Eisner6 Ryan CotterellQ\n6Johns Hopkins University @MIT\nDUniversity of Cambridge QETH Zürich\nleodu@cs.jhu.edu lucastor@mit.edu tp472@cam.ac.uk\nclara.meister@inf.ethz.ch jason@cs.jhu.edu ryan.cotterell@inf.ethz.ch\nAbstract\nLanguage modeling, a central task in natural\nlanguage processing, involves estimating a\nprobability distribution over strings. In most\ncases, the estimated distribution sums to 1\nover all ﬁnite strings. However, in some patho-\nlogical cases, probability mass can “leak” onto\nthe set of inﬁnite sequences. In order to char-\nacterize the notion of leakage more precisely,\nthis paper offers a measure-theoretic treatment\nof language modeling. We prove that many\npopular language model families are in fact\ntight, meaning that they will not leak in this\nsense. We also generalize characterizations of\ntightness proposed in previous works.\n1 Introduction\nLanguage modeling is a core task in natural lan-\nguage processing. As canonically deﬁned, lan-\nguage modeling involves estimating a distribution\nover the set of strings over a given alphabet. If the\nalphabet is the set of words in a language, 1 then\na language model can be thought of as a distribu-\ntion over the language’s sentences. Since Shannon\n(1948), language modeling has been used to es-\ntimate statistical properties of language and has\nbecome essential for computational linguistics re-\nsearch (Hale, 2001; Meister et al., 2021). Further,\nit is also central to a wide range of natural language\nprocessing applications, whether as a source model\nin a noisy channel architecture (Weaver, 1955; Je-\nlinek, 1976), as a way of learning better represen-\ntations of language (Peters et al., 2018), or, more\nrecently, for prompted generation (Brown et al.,\n2020), where the distribution deﬁned by a language\nmodel is employed in tasks like question-answering\n(Petroni et al., 2019), style transfer (Reif et al.,\n2022), and even sequence tagging (Liu et al., 2022).\nMore formally, a language model is typically de-\nﬁned to be a distribution over the countably inﬁnite\n1Or perhaps alphabetic symbols or subwords; see, e.g.,\nBostrom and Durrett (2020).\nset Σ∗of all (ﬁnite) strings (Booth and Thompson,\n1973).2 However, it has been shown that some\nclasses of autoregressive language models have pa-\nrameter settings in which the generative process\nterminates with probability < 1. Welleck et al.\n(2020) discuss this issue for recurrent neural net-\nwork language models. Models whose generative\nprocess may fail to terminate are called non-tight\n(Chi, 1999, who discussed non-tight PCFGs). If\nan autoregressive language model is non-tight, it\nmay generate inﬁnite sequences and MCMC algo-\nrithms over such models will not mix to the correct\ndistribution.\nIt is here that a subtlety arises: the set of inﬁnite\nsequences is uncountably inﬁnite. Properly treat-\ning a distribution over this sample space requires\na modicum of measure theory.3 To clarify the sit-\nuation, we review the measure-theoretic treatment\nof distributions over inﬁnite sequences. We then\nmake use of a termination symbol EOS to deﬁne a\nrandom variable whose value can be a string, i.e.,\nan element of Σ∗, or an inﬁnite sequence. In a tight\nlanguage model, this random variable has probabil-\nity 1 of being a string and hence ﬁnite.\nBeyond offering a measure-theoretic formaliza-\ntion, our paper also demonstrates how tightness\nrelates to the Borel–Cantelli lemmata, simplifying\na recent result by Meister et al. (2022). To conclude\nour paper, we analyze several language modeling\narchitectures and give conditions on their tightness.\nWe demonstrate that n-gram language models—\nand more generally, language models deﬁned by\nstochastic ﬁnite-state automata—can be non-tight,\nand we give a simple necessary and sufﬁcient con-\ndition for tightness in terms of the inverse of the au-\ntomaton’s transition matrix. This builds on a known\n2Recall that Σ∗def\n= ⋃\nn Σn where for n≥0, Σn is the set\nof strings of length n. The ∗is the Kleene closure operator.\n3Indeed, our treatment resolves an imprecision present in\nthe literature. For instance, Welleck et al. (2020) discusses the\nprobability of inﬁnite sequences despite using the canonical\ndeﬁnition of a language model as a distribution over Σ∗.\n9744\nresult due to Lehmann (1977). We also discuss\nwhen neural language models become non-tight.\nWe prove that Transformer-based language models\n(Vaswani et al., 2017; Brown et al., 2020) are al-\nways tight and that recurrent neural language mod-\nels are always tight when they employ a bounded\nactivation function. However, we also exhibit a\nrecurrent neural network (RNN) language model\nwith a ReLU activation (Nair and Hinton, 2010)\nthat is non-tight in a simpler construction than the\none offered by Chen et al. (2018). As a byproduct,\nwe also generalize and strengthen the results given\nby Welleck et al. (2020), who give a sufﬁcient con-\ndition for tightness of recurrent neural language\nmodels in terms of the norm of the hidden state.\n2 Motivating Examples\nLet Σ be an alphabet, i.e., a ﬁnite set of sym-\nbols, and let EOS /∈Σ be a distinguished end-of-\nsequence symbol. Let Σ\ndef\n= Σ∪{EOS }. A string of\nlength n≥0 is a ﬁnite sequence x= x1x2 ...x n\nwhere each xt ∈Σ. By convention, we say that\nxn+1 = EOS , although xn+1 is not an element of\nthe sequence x. For any integer 1 ≤t≤n+ 1, we\nwrite x<t for the preﬁx x1x2 ···xt−1.\nWe now begin to distinguish between “language\nmodels” and “sequence models.” As is traditional\nin the NLP literature, we henceforth deﬁne a lan-\nguage model to be a probability distribution over\nthe countable set Σ∗of all strings (see Def. 3.4).\nIt is popular to specify such a distribution in terms\nof its conditional probabilities ¯p(xt |x<t).\nDeﬁnition 2.1. An autoregressive sequence\nmodel (ASM) is a conditional probability distri-\nbution ¯p(xt |x<t) where xt ∈Σ and x<t ∈Σ\n∗\n.\nIf ¯pis an ASM, then we deﬁne a non-negative\nfunction p over Σ∗ by p(x)\ndef\n= ∏n+1\nt=1 ¯p(xt |\nx<t) = ¯p(EOS |x) ∏n\nt=1 ¯p(xt |x<t), where n\ndenotes the length of x.\nBut is pa language model? Not always, since\nas we will see below, it is possible for p(Σ∗)\ndef\n=∑\nx∈Σ∗p(x) <1. Of course this “bad” case never\nhappens if the ASM’s conditional probabilities are\nderived from a known LM, in which case psimply\nrecovers that LM.4 In this case clearly p(Σ∗) = 1.\n4That is, suppose the ASM’s conditional probabilities\nmatch the conditional probabilities of some known language\nmodel p0: that is, p0(Xt = xt |X1 ...X t−1 = x<t) =\n¯p(xt |x<t) whenever the former conditional probability is\nwell-deﬁned under the language model p0, i.e., whenever\nxt ∈Σ and x<t ∈Σ∗with p0(X1 ...X t−1 = x<t) > 0.\nIt follows that if p(Σ∗) <1, then the ASM’s con-\nditional probabilities do not match the conditional\nprobabilities of any language model p0.\nWe now exhibit such a “bad” ASM. Although\nthe conditional probability distributions ¯p(·| x<t)\neach sum to 1 over Σ, they fail to combine into a\nmodel pthat sums to 1 over Σ∗(i.e., a language\nmodel).\nExample 2.2 (non-tight bigram model). Consider\nthe bigram model deﬁned in Fig. 1a over the alpha-\nbet Σ = {a,b}. Under this model, any ﬁnite string\nthat contains the symbol b will have probability\n0, since ¯p(EOS |b) = ¯p(a |b) = 0 . This im-\nplies p(Σ∗) = ∑∞\nn=0 p(an) = ∑∞\nn=0(0.7)n·0.1 =\n0.1\n1−0.7 = 1\n3 <1. ■\nExample 2.3 (tight bigram model). In contrast, in\nFig. 1b, obtained from Ex. 2.2 by changing the arcs\nfrom the b state, p(Σ∗) = 1. See App. B for details\nof this calculation. ■\nEx. 2.2 above conﬁrms that the autoregressive\nformulation does not necessarily yield pthat is a\nvalid distribution over Σ∗.\nBut if pis not a language model, what is it? It is\nintuitive to suspect that, in a model withp(Σ∗) <1,\nthe remainder of the probability mass “leaks” to\ninﬁnite sequences, i.e., the generative process may\ncontinue forever with probability > 0. We will\nmake this intuition formal in §3. By analogy with\nChi and Geman (1998) and Cohen and Johnson\n(2013), we refer to such models as non-tight.5\nThe non-tightness of Ex. 2.2 is related to the fact\nthat the probability of EOS is 0 at some states, in\ncontrast to Ex. 2.3. However, requiring ¯p(EOS |\nx<t) >0 for all preﬁxes x<t is neither necessary\nnor sufﬁcient to ensure tightness. It is notnecessary\nbecause one can, for example, construct an ASM in\nwhich ¯p(EOS |x<t) = 0.1 when tis even but = 0\notherwise. Such a model generates only odd-length\nstrings but is tight. It is not sufﬁcient because of\nthe following example, in which ¯p(EOS |x<t) is\nalways positive but decays so rapidly toward 0 that\nthe generative process might continue forever.\nExample 2.4 (non-tight RNN). Consider an RNN\nover a small alphabet Σ = {a,EOS }with the fol-\nThen by the chain rule of probability, p(x) = p0(x) for each\nx ∈Σ∗. Thus p= p0, so pis a language model.\n5In Chi and Geman (1998) and Cohen and Johnson (2013),\na PCFG is non-tight if its generative process may not terminate,\nand consequently the total probability of all ﬁnite trees is less\nthan 1.\n9745\n¯p(a |BOS) 1\n¯p(a |a) 0.7\n¯p(b |a) 0.2\n¯p(EOS |a) 0.1\n¯p(b |b) 1\n¯p(EOS |EOS) 1\na\nb\nEOS\nBOS\na/1 a/0.7\nb/1\nEOS/1\nb/0.2\nEOS\n/0.1\n(a) Non-tight 2-gram model.\n¯p(a |BOS) 1\n¯p(a |a) 0.7\n¯p(b |a) 0.2\n¯p(EOS |a) 0.1\n¯p(b |b) 0.9\n¯p(EOS |b) 0.1\n¯p(EOS |EOS) 1\na\nb\nEOS\nBOS\na/1 a/0.7\nb/0.9\nEOS/0.1\nEOS/1\nb/0.2\nEOS\n/0.1\n(b) Tight 2-gram model.\nFigure 1: Tight and non-tight bigram models, expressed as Mealy machines (see §5.1). Transitions with conditional\nprobability of 0 are omitted. The termination probability at a state is represented by an EOS arc from that state.\nlowing hidden state recurrence:\nh0 = 0, h t = ReLU(ht−1 + 1). (1)\nIn this case, the hidden state admits a closed-form\nexpression ht = t∈R. Setting the (1-dimensional)\nembedding of the alphabet to beva = 1 and vEOS =\n0, we arrive at\n¯p(EOS |x<t) = softmax(vaht,vEOS ht)EOS\n= e0·t\ne1·t+e0·t = 1\net+1 >0. (2)\nThe EOS probability is always strictly positive, but\nThm. 4.7 shows that this sequence model is non-\ntight. Numerically, p(Σ∗) ≈0.702 <1. ■\nOn the other hand, an ASM may be tight after all\nif the probability ofEOS decays more slowly—even\nwhen it still approaches 0.\nExample 2.5 (tight RNN). Consider again an RNN\nover the alphabet Σ = {a,EOS }with the following\nrecurrence using softplus activation:6\nh1 = 0, h t = log(exp(ht−1) + 1). (3)\nStarting from h1 = 0 = log 1, a simple induction\nargument shows that\nht = log(exp log(t−1) + 1) = logt. (4)\nAgain, setting va = 1 and vEOS = 0, we arrive at\n¯p(EOS |x<t) = softmax(vaht,vEOS ht)EOS (5)\n= e0·log t\ne1·log t+e0·log t = 1\nt+1 >0.\nThis decays slowly to 0: limt→∞¯p(EOS |x<t) =\n0, but since ∑∞\nt=1 ¯p(EOS |x<t) = ∞, Prop. 4.3\nbelow implies that this ASM is tight. ■\nFinally, we illustrate the peril of not treating\ndistributions over uncountable sets carefully.\n6We use softplus instead of ReLU to simplify arithmetics.\nExample 2.6 (inﬁnite coin toss). Consider the in-\nﬁnite independent fair coin toss model, where we\naim to place a distribution over {H,T}∞, the un-\ncountable set of inﬁnite sequences of {H,T}. Intu-\nitively, such a distribution corresponds to an ASM\nin which for all x<t, ¯p(H |x<t) = ¯p(T |x<t) = 1\n2\nand ¯p(EOS |x<t) = 0 . Clearly, each individual\ninﬁnite sequence over {H,T}should be assigned\nprobability (1\n2 )∞ = 0. Without a formal founda-\ntion, one may arrive at the following paradox:\n1 = p({H,T}∞) = p\n(⋃\nω∈{H,T}∞{ω}\n)\n(6)\n=\n∑\nω∈{H,T}∞\np({ω}) =\n∑\nω∈{H,T}∞\n0 ?= 0. ■\nTogether, these examples suggest that one must\ntake care to characterize tightness. And, to the au-\nthors’ surprise, it does not appear as if such a care-\nful characterization yet exists in the NLP literature.\n3 The Language Model Measure\nIn this section, we rigorously characterize the kind\nof distribution induced by an ASM. As mentioned\nearlier, an ASM can lose probability mass to the set\nof inﬁnite sequences, Σ∞. However, Σ∞, unlike\nΣ∗, is uncountable, and it is due to this fact that we\nneed to work explicitly with the measure-theoretic\nformulation of probability.\n3.1 Measure-Theoretic Background\nThe goal of measure-theoretic probability is to as-\nsign probability to subsets of an outcome space Ω.\nFor instance, in Ex. 2.6, Ω = {H,T}∞. However,\nin the course of the study of measure theory, it has\nbecome clear that for many common Ω, it is impos-\nsible to assign probabilities in a way that satisﬁes\na set of reasonable desiderata.7 Consequently, the\n7Measure theory texts commonly discuss such desiderata\nand the dilemmas that comes with them. See, e.g., Chapter\n7 in Tao (2016), Chapter 3 in Royden (1988) or Chapter 3 in\nBillingsley (1995). We also give an example in Thm. 3.5.\n9746\nstandard approach to probability theory resorts to\nonly assigning probability to certain “nice” sub-\nsets of Ω, which are referred to as events or mea-\nsurable subsets, as in the theory of integration or\nfunctional analysis. The set of measurable subsets\nis commonly denoted as F(Def. 3.1), and a proba-\nbility measure P : F→ [0,1] is the function that\nassigns a probability to each measurable subset. As\nit turns out, the following simple and reasonable\nrequirements imposed on Fand P are enough to\nrigorously discuss probability (Kolmogorov, 1933).\nDeﬁnition 3.1. Let P(Ω) be the powerset of Ω.\nThen F⊆P (Ω) is called a σ-algebra (or σ-ﬁeld)\nover Ω if the following conditions hold:\n1) Ω ∈F,\n2) if E ∈F, then its complement Ec ∈F,\n3) if E1,E2,... is a ﬁnite or inﬁnite sequence of\nsets in F, then ⋃\nnEn ∈F.\nIf Fis a σ-algebra over Ω, we call the tuple (Ω,F)\na measurable space.\nA measurable space guarantees that operations\non countably many sets are always valid, and hence\npermits the following deﬁnition.\nDeﬁnition 3.2. A probability measure P over a\nmeasurable space (Ω,F) is a function P : F→\n[0,1] such that\n1) P(Ω) = 1,\n2) if E1,E2,... is a ﬁnite or inﬁnite sequence\nof disjoint sets in F, then P(⋃\nnEn) =∑\nnP(En).\nIn this case we call (Ω,F,P) a probability space.\nNote that it assigns measure only to the sets in F;\nother sets are said to be non-measurable.\n3.2 Sequence Models\nAs we saw in §2, sampling successive symbols\nfrom a non-tight ASM has probability >0 of con-\ntinuing forever. Hence, we hope to regard the ASM\nas deﬁning a probability space over Ω = Σ∗∪Σ∞,\nwhere Σ∞denotes the set of inﬁnite strings8 over\nΣ. Note that this set Ω is uncountable whenever\n|Σ|≥ 2. We will ﬁrst need to turn it into a measur-\nable space by deﬁning an appropriate σ-algebra.\nThis type of distribution is more general than a\nlanguage model, which takes Ω to be the set Σ∗of\nﬁnite strings. To distinguish the two, we refer to a\ndistribution over Σ∗∪Σ∞as a sequence model.\n8We will use the phrase “inﬁnite string” in this paper when\nit is natural to do so, e.g., in the context ofΣ∗∪Σ∞. However,\nthis is nonstandard terminology: in computer science, string\ngenerally refers to a ﬁnite object.\nDeﬁnition 3.3. A sequence model is a probability\nmeasure P over the set Σ∗∪Σ∞.\nIntuitively (we will make this precise later), the\nevent Σ∞⊂Σ∗∪Σ∞in Def. 3.3 represents non-\ntermination of the generating process, i.e., it at-\ntempts to generate an inﬁnitely long sequence. If\nthis never happens, we have a language model.\nDeﬁnition 3.4. A language model is a probabil-\nity measure P over just Σ∗. Equivalently, it is a\nsequence model P such that P(Σ∞) = 0.\nOur goal in the rest of this section is to rigorously\nconstruct a sequence model P that encodes the\nconditional probabilities of a given ASM. Since the\nASM speciﬁes conditional distributions over the\naugmented alphabet Σ, we ﬁrst use it to construct\na probability measure P over a measurable space\n(Σ\n∞\n,σ(C)). We then derive our sequence model\nP from P as the probability measure of a random\nvariable Xin a measurable space (Σ∗∪Σ∞,σ(C)).\nThe σ-algebras σ(C) and σ(C) will be built below.\n3.3 Pre-Measure\nAs mentioned in §3.1, it is often impossible to mea-\nsure the probability of every single subset ofΩ. For\nexample, in the inﬁnite coin toss model in Ex. 2.6,\nwe might begin by reasonably assigning probability\n0 to each individual sequence ω∈{H,T}∞. Unfor-\ntunately, it is then impossible to assign probability\nto every subset of {H,T}∞; we must restrict our\nmeasurable space to a strict subset of P(Ω), where\nP() is the powerset operator.\nTheorem 3.5. Assuming the Axiom of Choice and\nthe Continuum Hypothesis, there exists no proba-\nbility measure P over ({H,T}∞,P({H,T}∞)) such\nthat P({ω}) = 0 for each ω∈{H,T}∞.\nProof. This is a direct consequence of Ulam\n(1930). See App. C.1.1 for a discussion. ■\nWe will address this with well-known methods.\nA versatile theorem of Carathéodory provides a\nnatural way to construct a probability space for\nsequences, in which preﬁx probabilities are well-\ndeﬁned. We ﬁrst review two needed deﬁnitions.\nDeﬁnition 3.6. A⊆P (Ω) is called an algebra\n(or ﬁeld) over Ω if\n1) Ω ∈A,\n2) if E ∈A, then Ec ∈A,\n3) if E1,E2 ∈A, then E1 ∪E2 ∈A.\nDeﬁnition 3.7. Let Abe an algebra over some\nset Ω. A probability pre-measure over (Ω,A) is a\nfunction P0 : A→ [0,1] such that\n9747\n1) P0(Ω) = 1,\n2) if E1,E2,... is a (countable) sequence of dis-\njoint sets in Awhose (countable) union is also\nin A, then P0(∪∞\nn=1En) = ∑∞\nn=1 P0(En).\nNote that the only difference between a σ-\nalgebra (Def. 3.1) and an algebra is that condition 3\nis weakened from countable to ﬁnite, and the only\ndifference between a probability measure (Def. 3.2)\nand a pre-measure is that the latter is deﬁned with\nrespect to an algebra instead of a σ-algebra.\nThe idea behind Carathéodory’s Extension The-\norem is that there is often a simple construction\nof an algebra Aover Ω such that there is a natural\nway to deﬁne a probability pre-measure. One can\nthen extend this probability pre-measure to a proba-\nbility measure that is both minimal and unique in a\nprecise sense. For example, the standard Lebesgue\nmeasure over the the real line can be constructed\nin this way. For our case of inﬁnite sequences, we\nwill ﬁrst construct an algebra over Ω = Σ\n∞\nfor\nsome alphabet Σ. Then, assuming we are given an\nASM ¯pover Σ, we can associate the algebra with\na pre-measure that is consistent with ¯p. We will\nmake use of the following deﬁnition to construct\nthe algebra:\nDeﬁnition 3.8. Given any set H ⊆Σ\nk\n, deﬁne its\ncylinder set (of rank k) to be\nC(H)\ndef\n=\n{\nxω : x∈H,ω∈Σ\n∞}\n(7)\nIn essence, a cylinder set of rank kis the set of\ninﬁnite strings that share their k-preﬁx with some\nstring x ∈H ⊆Σ\nk\n. For a length- k string x =\nx1 ···xk, the rank-kcylinder set C(x)\ndef\n= C({x})\nis the set of all inﬁnite strings preﬁxed by x.9 We\ndenote the collection of all rank- k cylinder sets\nby Ck\ndef\n=\n{\nC(H) : H ∈P(Σ\nk\n)\n}\nand deﬁne C\ndef\n=\n⋃∞\nk=1 Ck to be the collection of all cylinder sets\nover Ω.10\nLemma 3.9. C⊂P (Ω) is an algebra over Ω =\nΣ\n∞\n.\nProof. See App. C.1.2. ■\nWe are now ready to deﬁne the pre-measure P0\nfor the cylinder algebra C. Given an ASM ¯pand\nany set C(H) ∈C, let\nP0(C(H))\ndef\n= ∑\nx∈H ¯p(x) (8)\n9This type of cylinder set, i.e., one that is generated by a\nsingleton, is also called a thin cylinder.\n10Observe that C1 ⊂C2 ⊂C3 ⊂··· .\nwhere, denoting the length of xby k,\n¯p(x)\ndef\n= ∏k\nt=1 ¯p(xt |x<t). (9)\nWe conﬁrm in Prop. C.2 that P0 is well-deﬁned\neven though the cylinder set C(H) may also arise\nas C(H′) where H′̸= H.11\nLemma 3.10. P0 is a pre-measure over C.\nProof. See App. C.1.2. ■\n3.4 Extension of Pre-Measure\nWe have now gathered all the ingredients needed\nto state Carathéodory’s theorem.\nTheorem 3.11 (Carathéodory’s Extension Theo-\nrem). Given an algebra Aover some set Ω and a\nprobability pre-measure P0 : A→ [0,1], there ex-\nists a probability space (Ω,F,P) such that A⊂F\nand P|A= P0. Furthermore, the σ-algebra Fde-\npends only on Aand is minimal and unique—thus\nwe may denote it by σ(A)—and the probability\nmeasure P is unique.\nProof Sketch. See App. C.2.1. ■\nApplying Carathéodory’s extension theorem to\nour cylinder algebra Cand pre-measure P0, we see\nthat there exists a probability space (Σ\n∞\n,σ(C),P)\nover Σ\n∞\nthat agrees with the ASM¯p’s probabilities.\nIt is a fair question to ask what kinds of sets are\nnon-measurable under this construction; we discuss\nthis in App. C.2.2.\n3.5 A String-Valued Random Variable\nHaving constructed the probability space\n(Σ\n∞\n,σ(C),P), we now demonstrate how to use\nit to induce a probability space over Σ∗∪Σ∞as\nrequired by Def. 3.3. We will achieve this through\nthe use of a random variable.\nDeﬁnition 3.12 (random variable). A mapping X :\nΩ →S between two measurable spaces (Ω,F)\nand (A,G) is an (A,G)-valued random variable,\nor a measurable mapping, if, for all B ∈G,\nX−1(B)\ndef\n= {ω∈Ω : X(ω) ∈B}∈F . (10)\nTo construct a random variable that takes values\nin Σ∗∪Σ∞, Def. 3.12 requires us to ﬁrst construct a\nσ-algebra over Σ∗∪Σ∞. We will do so in a similar\n11For example, in the inﬁnite coin toss model, C(H) =\nC({HH,HT}).\n9748\nfashion as we constructed (Σ\n∞\n,C). Given H ⊆\nΣk, deﬁne a rank-kcylinder set in Σ∗∪Σ∞to be\nC(H)\ndef\n= {xω : x∈H,ω∈Σ∗∪Σ∞}. (11)\nLet Ck be the set of all rank-kcylinder sets. Deﬁne\nC\ndef\n= ∪∞\nk=1Ck. Then, σ(C) is a σ-algebra by the\nsame reasoning as in Lemma 3.9 and Thm. 3.11.\nWe can now deﬁne the random variable X by12\nX(ω) =\n{\nω<k if ωk is the ﬁrst EOS in ω\nω otherwise (if EOS /∈ω) (12)\nwhere ω∈Σ\n∞\n. We claim that X is well-deﬁned:\nProposition 3.13. The function X :\n(Σ\n∞\n,σ(C)) → (Σ∗ ∪Σ∞,σ(C)) deﬁned in\nEq. (12) is a measurable mapping.\nProof. See App. C.3. ■\nAny measurable function induces a probability\nmeasure on the output space, called the pushfor-\nward measure (cf. §2.4 in Tao, 2011), given by\nP(X ∈E)\ndef\n= P(X−1(E)). (13)\nOne can check that P, deﬁned using P, is indeed\na probability measure on (Σ∗∪Σ∞,σ(C)) and\nhence (Σ∗∪Σ∞,σ(C),P) is a probability space.\nWe have therefore shown that, given any ASM,\nwe can construct an associated sequence model as\ndeﬁned in Def. 3.3.\nUnder the formulation of a probability space to-\ngether with a random variable, useful probability\nquantities arise naturally and intuitively. In particu-\nlar, when x∈Σ∗is a ﬁnite string, we have\nP(X = x)\ndef\n= P(X ∈{x}) = p(x) (14)\nwith the deﬁnition of pfrom §2. Additionally, as\nwe will show in the next section, the probability\nof the set of inﬁnite strings P(X ∈Σ∞) is the\nprobability of generating an inﬁnite string.13\nDeriving EOS As an aside, the preceding section\nallows us to motivate the EOS token in ASM as a\nconstruct that emerges naturally. Speciﬁcally, for\nany x∈Σ∗, rearranging Eq. (14):\n¯p(EOS |x) = P(X=x)\n¯p(x) = P(X=x)\nP(X∈C(x)) (15a)\n12In this deﬁnition, the position k≤∞ of the ﬁrst EOS —a\nstopping time—is itself a random variable.\n13An important detail left out in this discussion is that both\nthe singleton set {x}and Σ∞need to be measurable in (Σ∗∪\nΣ∞,σ(C)) for the above to make sense. This is addressed by\nProp. C.7 and Prop. C.8.\n= P(X = x|X ∈C(x)) (15b)\nwhere we have used ¯p(x) = P(C(x)) =\nP(X−1(C(x))) = P(X ∈C(x)). This means\nthat the EOS probability in an ASM emerges as\nthe conditional probability that, given that we must\ngenerate a string with a preﬁx x∈Σ∗, the string is\nexactly x.\n4 Characterizing Tightness\nBeyond the measure-theoretic formalization, a goal\nof this paper is to provide an exact characterization\nof tightness in ASMs. The results presented in\nthis section generalize Lemma 3.2 in Welleck et al.\n(2020). First, we consider the event\nAk\ndef\n= {ω∈Σ\n∞\n: ωk = EOS } (16)\nin the probability space (Σ\n∞\n,σ(C),P). Intuitively,\nAk is the event that an EOS symbol appears at posi-\ntion kin the string. Note that under this deﬁnition\nthe Ak are not disjoint. For example, the string\nω= ab EOS c EOS dd ··· lives in the intersection\nof A3 and A5 since EOS appears at both position 3\nand position 5. Using Eq. (16), we can express the\nevent consisting of all ﬁnite strings as ⋃∞\nk=1 Ak. It\nfollows that we can express the event of an inﬁnite\nstring as (⋃∞\nk=1 Ak)c = ⋂∞\nk=1 Ac\nk. Thus, using the\nrandom variable X, we can express the probability\nof generating an inﬁnite string as\nP(X ∈Σ∞) = P(X−1(Σ∞)) (17a)\n= P(⋂∞\nk=1 Ac\nk) . (17b)\nHence, we can now formalize the notion of tight-\nness, which we have introduced in §2 and Def. 3.4.\nDeﬁnition 4.1. A sequence model P is said to be\ntight if P(X ∈Σ∞) = 0, in which case it is also\na language model (cf. Prop. C.9). Otherwise, we\nsay that it is non-tight.\nNote that the deﬁnition of Ak only uses a\nstring’sk-preﬁx, and hence is a cylinder set of rank\nk. Recalling that the cylinder sets are measurable\nand so are the sets countably generated by them,\nwe see that both the event consisting of all ﬁnite\nstrings and the event consisting of all inﬁnite\nstrings are measurable. Thus, P(∪∞\nk=1Ak) and\nP(∩∞\nk=1Ac\nk) are well deﬁned.\n4.1 A Lower Bound Result\nWe have characterized tightness in terms of the\nprobability of a speciﬁc event P(∩∞\nk=1Ac\nk), a quan-\ntity we now seek to determine.\n9749\nLemma 4.2. If ∑∞\nn=2 P\n(\nAn |∩n−1\nm=1Ac\nm\n)\n= ∞,\nthen P(∩∞\nm=1Ac\nm) = 0.\nProof. See App. D. ■\nUsing Lemma 4.2, we can derive the following\nuseful sufﬁcient condition for a sequence model\nderived from an ASM to be tight. It applies when\nthe probability of EOS does not decay too rapidly\nwith the length of the preﬁx.\nProposition 4.3. If ¯p(EOS | x) ≥ f(t) for all\nt ≥ 1,x ∈ Σt−1, and ∑∞\nt=1 f(t) = ∞, then\nP(∩∞\nk=1Ac\nk) = 0. In other words, P is tight.\nProof. See App. D.2. ■\nThis test implies tightness for all of the tight exam-\nples in §2, but not for the non-tight ones. Note that\nthe lower-bounding function f depends only on the\nlength of the preﬁx, not its content. f does not\nhave to be monotonic—in the case of the even/odd\nexample from §2, it is not.\n4.2 The Borel–Cantelli Lemmata\nIt turns out that Prop. 4.3 admits a converse state-\nment in which we can prove a similar property of\n¯p by assuming that the model is tight. To prove\nthis result, we will use a fundamental inequality\nfrom probability theory—the Borel–Cantelli lem-\nmata. The Borel–Cantelli lemmata are useful for\nour purposes because they relate the probability\nmeasure of sets of the form ⋂∞\nn=0 An or ⋃∞\nn=0 An\nto a series ∑∞\nn=0 pn. We will only state the lem-\nmata here without supplying their proofs;14 how-\never, we point out that Lemma 4.2 can be viewed as\na parallel statement to the Borel–Cantelli lemmata\nand one can prove the lemmata using a very similar\nproof (cf. proof of Thm 2.3.7 in Durrett, 2019).\nConcretely, given a sequence of events{An}∞\nn=1\nin some probability space, the Borel–Cantelli lem-\nmata are statements about the event\n{An i.o.}\ndef\n= ⋂∞\nm=1\n⋃∞\nn=mAn (18)\nwhere i.o. stands for “inﬁnitely often.” Intuitively,\n{An i.o.}is the set of outcomes that appear in\ninﬁnitely many sets in the collection {An}∞\nn=1\n(hence the name). We will not use Borel–Cantelli\ndirectly, but they offer a probabilistic proof of a key\nresult (Cor. 4.6) which will in turn lead to the de-\nsired statement about tightness. We formally state\nthe ﬁrst and second Borel–Cantelli lemmata below.\n14See §2.3 in Durrett (2019) or §4 in Billingsley (1995)\ninstead.\nLemma 4.4 (Borel–Cantelli I). If∑∞\nn=1 P(An) <∞, then P(An i.o.) = 0.\nLemma 4.5 (Borel–Cantelli II). If∑∞\nn=1 P(An) = ∞, then P(An i.o.) = 1, provided\nthat {An}is a sequence of independent events.\nUsing the Borel–Cantelli lemmata, we can prove\nthe following useful fact.\nCorollary 4.6. Given a sequence{pn}where pn ∈\n[0,1). Then,\n∏∞\nn=1(1 −pn) = 0 ⇐⇒∑∞\nn=1 pn = ∞. (19)\nProof. See App. D.3. ■\nWe now turn to proving a more general version\nof Prop. 4.3, which would imply its converse. First,\nwe deﬁne the following quantity\n˜pEOS (t)\ndef\n= P(At |Ac\n1 ∩···∩ Ac\nt−1) (20)\nwhich can be viewed as the EOS probability at step\nt, given that EOS was not generated at any earlier\nstep. In Eq. (48a) in App. D.2, we show that, when\n˜pEOS (t) is deﬁned, it has the same value as\n˜pEOS (t) =\n∑\nω∈Σt−1 ¯p(ω)¯p(EOS |ω)∑\nω∈Σt−1 ¯p(ω) . (21)\nWe can now completely characterize the tightness\nof an ASM with the following theorem.\nTheorem 4.7 (Proposition 2.4 in Meister et al.,\n2022). An ASM is tight if and only if ˜pEOS (t) = 1\nfor some tor ∑∞\nt=1 ˜pEOS (t) = ∞.\nProof. See App. D.4. The proof uses Cor. 4.6,\nwhich accounts for the form of the condition. ■\nWe remark that Thm. 4.7 is a generalization\nof Prop. 4.3 since if ˜pEOS (t) is lower-bounded by\nf(t) whose series diverges, its own series would\nalso diverge. However, since ˜pEOS (t) involves\nthe computation of a partition function in its\ndenominator, it can be intractable to calculate (Lin\net al., 2021). Hence, Prop. 4.3 will be our main\ntool for determining tightness.\nFinally, we note that Thm. 4.7 generalizes claims\nin previous work. For example, Welleck et al.\n(2020) require f(t) = c >0 for some constant\nc to determine tightness. Hence, their bound is\nnot helpful in determining the tightness in either\nEx. 2.4 or Ex. 2.5, because the EOS probability\ncan be arbitrarily small in both cases. Applying\nProp. 4.3, we see that (1) the ASM in Ex. 2.4 is\nnon-tight, because the series ∑∞\nt=1\n1\net+1 is conver-\ngent, and (2) the ASM in Ex. 2.5 is tight, since the\nseries ∑∞\nt=1\n1\nt+1 is divergent.\n9750\n5 Analysis of Common Language Models\nWe now put into practice the foundations built up\nin the previous sections and discuss the tightness\nof several classes of ASMs.\n5.1 Stochastic Finite-State Language Models\nLanguage modeling based on n-grams has been\nhistorically inﬂuential in NLP (Jurafsky and Mar-\ntin, 2009, Ch. 4). However, as Fig. 1 illustrates,\nn-gram language models are speciﬁc cases of the\nmore general stochastic ﬁnite-state language mod-\nels (Vidal et al., 2005). Tightness is more naturally\ncharacterized in this more general setting, as it\nturns out. We begin with a linear-algebraic deﬁni-\ntion of stochastic ﬁnite-state language models—or,\nmore precisely, sequence models, since in this pa-\nper we do not consider the non-tight ones to be\nlanguage models.\nDeﬁnition 5.1. A Q-state stochastic ﬁnite-\nstate sequence model (SFSSM) is a quadruple(\nΣ,s,{P(a)}a∈Σ,t\n)\n, where Σ is an alphabet of\nsymbols, P(a) ∈RQ×Q\n≥0 is a symbol-speciﬁc tran-\nsition matrix for a∈Σ,15 s∈RQ\n≥0 is a vector of\ninitial state probabilities, and t∈RQ\n≥0 is a vector\nof termination probabilities, i.e., probabilities of\ngenerating EOS in each state.16 We further require\nthat ∑Q\nq=1 sq = 1 and that tq + ∑Q\nq′=1 Pqq′ = 1\nfor all 1 ≤q ≤Q, where P\ndef\n= ∑\na∈Σ P(a) is the\ntransition sum matrix.\nGiven an SFSSM\n(\nΣ,s,{P(a)}a∈Σ,t\n)\n, the\nprobability of a string x∈Σ∗is deﬁned by\n¯p(x1 ···xn) = s⊤(∏n\nt=1 P(xt))\nt. (22)\nDeﬁnition 5.2. A state qof an SFSSM (1 ≤q ≤\nQ) is accessible if there is a positive-probability\npath to q from some state rwith sr > 0; it is co-\naccessible if there is a positive-probability path\nfrom qto some state rwith tr >0. It is useful if it\nis both accessible and co-accessible, i.e.,qappears\non some positive-probability accepting path.\nDef. 5.2 allows a simple characterization of tight\nSFSSMs, namely Thm. 5.3, and a straightforward\nproof of Cor. 5.4.17\n15For simplicity, we have disallowedε-transitions.\n16We use Qto denote the number of states as Qis the tradi-\ntional notation for the set of states in a ﬁnite-state automaton.\n17Cor. 5.4 is a special case of Chi and Geman (1998), who\nshowed that MLE estimates of PCFGs are tight.\nTheorem 5.3. An SFSSM is tight iff all accessible\nstates are also co-accessible.\nProof. See App. E.1.1. ■\nCorollary 5.4. Maximum likelihood estimates of\nn-gram models based on some corpus are tight.\nProof. See App. E.1.1. ■\nIn fact, we can express the termination proba-\nbility of an SFSSM in simple linear algebra terms.\nDeﬁnition 5.5. Trimming an SFSSM means re-\nmoving its non-useful (useless) states to obtain a\nsubstochastic ﬁnite-state sequence model.18 This\ndoes not affect the string probabilities (22). Re-\nmoving the non-useful states means removing their\nrows and columns fromP as well as their rows from\nsand t, yielding possibly smaller P′,s′and t′.\nTheorem 5.6. Let P′be the transition sum matrix\nof a trimmed substochastic FSSM. Then I −P′is\ninvertible and P(X ∈Σ∗) = s′⊤(I−P′)−1t′≤1.\nProof. See App. E.1.2. ■\nThe well-known matrix inversion formula used\nabove ﬁnds the total weight of all accepting paths\nin any weighted graph (Tarjan, 1981). 19 The\nformula can be seen as a special case of Lehmann’s\n(1977) algebraic path algorithm.\n5.2 Transformer Language Models\nWe now prove that all Transformer language mod-\nels are tight. Key to our proof of the tightness of\nvarious neural architectures, including the Trans-\nformer, is the following basic fact in topology.\nTheorem 5.7. Let X be a compact topological\nspace and Y be any topological space. If f : X →\nY is continuous, then f(X) ⊆Y is also compact.\nProof. See App. E.2. ■\nTo address the variable-length nature of mod-\nern deep NLP models, we will mathematically\nabstract them as a function on vector tuples, 20\nf :\n(\nRd)+ →\n(\nRd)+, that is length-preserving\nin the sense that f\n(\nRt×d)\n⊆\n(\nRt×d)\nfor all t> 0.\n18We use the term substochastic rather than stochastic here\nbecause the trimmed model satisﬁes t′\nq + ∑Q′\nq′=1 P′\nqq′ ≤1,\nbut might no longer achieve equality as required by Def. 5.1.\n19This is assuming the total weight is ﬁnite (which we\nguarantee by substochasticity) and the matrix is invertible\n(which we guarantee by trimming)\n20Here\n(\nRd)+\nis the set of nonempty tuples of vectors inRd.\nThis is formally the disjoint union (coproduct) ∐\nt∈Z>0 Rt×d.\n9751\nIntuitively, this deﬁnition is saying that f is a func-\ntion that maps a nonempty vector tuple {vi}t\ni=1 to\nanother vector tuple {hi}t\ni=1 of the same length,\nf(v1,..., vt) = (h1,..., ht) ∈Rt×d, (23)\nwhere vi ∈Rd is commonly the embedding of\nthe input symbol xi. In particular, we can take the\nfunction f :\n(\nRd)+ →\n(\nRd)+ to be the function\ndeﬁned by a stack of Transformer layers. This\nsetup will help us state the following.\nLemma 5.8. Let f :\n(\nRd)+ →\n(\nRd)+ be the func-\ntion deﬁned by a ﬁnite number of Transformer lay-\ners (e.g., nlayers) with any continuous activation\nfunction. Given a compact set K ⊂Rd. Then,\nthere exists a compact set K′⊂Rd such that for\nevery t∈Z>0,\nf\n(\nKt)\n⊆\n(\nK′)t. (24)\nProof. See App. E.2. ■\nRecall that a Transformer language model—or\nmore precisely, a Transformer ASM—deﬁnes the\nconditional probabilities using the softmax trans-\nformation\n¯p(xt+1 |x≤t) =\nexp(u⊤\nxt+1ht)∑\ny∈Σ exp(u⊤y ht) (25)\nwhere ux ∈Rd is the output symbol embedding of\nx∈Σ and ht is deﬁned from the input embeddings\nof x≤t via Eq. (23). Using Lemma 5.8, together\nwith the ﬁniteness of the vocabulary Σ and the con-\ntinuity of the softmax transformation (25), readily\nyields our main result on Transformers.\nTheorem 5.9. The autoregressive sequence model\ndeﬁned by any (ﬁxed-depth) Transformer is tight.\nProof. See App. E.2. ■\n5.3 Recurrent Neural Language Models\nRecall that the hidden state of an RNN is typically\ndeﬁned by the recurrence\nht = σ(Wvt + Uht−1 + b) (26)\nwhere vt ∈Rd is the embedding of the input sym-\nbol xt, as above, and σ(·) is some activation func-\ntion (Elman, 1990). The conditional probabilities\nare usually deﬁned in the same way as Eq. (25).\nUsing Thm. 5.7 and the same strategy of proof as\nin Thm. 5.9, one can also easily prove the tightness\nof any RNN ASM with bounded activations (e.g.,\ntanh or sigmoid). However, as we saw in Ex. 2.4,\nan unbounded activation function (e.g., ReLU) can\nindeed lead to non-tightness by making the proba-\nbility of EOS decay too fast. The condition derived\nin Thm. 4.7 precisely determines how fast such\ndecay can be without losing the tightness of the\nlanguage model. Below, we generalize this result\nas well as Lemma 3.2 of Welleck et al. (2020), and\nshow that if the norm of the activations eventually\ngrows sub-logarithmically, the RNN is still tight.\nProposition 5.10. Given an RNN ASM over Σ.\nAgain let the output symbol vector be ux ∈Rd\nfor x ∈Σ, and set k\ndef\n= sup x∈Σ ∥ux −uEOS ∥2.\nAdditionally, for each t> 0, let ∥ˆht∥2 be the max-\nimum attainable hidden state norm for any con-\ntext x ∈Σt. Such a sequence model is tight if\nk∥ˆht∥2 ≤log tfor all sufﬁciently large t.\nProof. See App. E.3. ■\nThis result is weaker than Thm. 5.9 because in\nan RNN, unlike a Transformer, the depth of the\ncomputation graph grows with the sequence length.\n6 Conclusion\nThis paper presents a measure-theoretic treatment\nof language modeling and its tightness. Practi-\ncal implications of our results include determining\nwhen sampling from an autoregressive sequence\nmodel is guaranteed to terminate and whether\nMCMC algorithms over such models will mix to\nthe correct distribution.\nTo this end, we ﬁrst deﬁned various components\nof language modeling in measure-theoretic termi-\nnology. This in turn allows us to understand the por-\ntion of probability mass allocated to inﬁnite-length\nstrings. Importantly, this presentation formalizes\na deﬁnition of sequence modeling under which\nthe probability of producing an inﬁnite-length se-\nquence is non-zero; while today’s models are often\ncapable of producing such strings, previously there\nwas no rigorous treatment of this case.\nIndeed, such a deﬁnition is useful when consider-\ning a number of neural architectures (e.g., a simple\nRNN as in Elman, 1990) and language generation\nsystems (e.g., the distribution induced by nucleus\nsampling; Holtzman et al., 2020). In particular, we\nshowed that perhaps the most commonly-used NLP\narchitecture, the Transformer language model, is\nindeed a language model—a tight distribution over\nﬁnite strings—a property that had been called into\nquestion by previous work.\n9752\nLimitations\nOur discussion in this paper leaves out the consider-\nation of computability of measures over languages.\nSpeciﬁcally, we note that there exist works on com-\nputable measure theory developed in the context\nof theoretical computer science (de Leeuw et al.,\n1956) and probabilistic programming languages\n(Roy, 2011). Additional machinery needs to be\ndeveloped in order for a proper treatment and we\nleave this for future work.\nAnother notable limitation is that we exclusively\nfocused on the autoregressive production of lan-\nguage. Importantly, our formalism might not be\ncompatible with other models of language produc-\ntion such as those induced by a PCFG.\nFinally, our proofs of Thm. 5.9 and Prop. 5.10\nexploit the strictly positive property of the softmax\nfunction. Importantly, they do not apply to models\nwith sparse distributions (Martins and Astudillo,\n2016; Peters et al., 2019; Martins, 2021).\nEthics\nThere are no ethical implications of this paper to\nthe best knowledge of the authors.\nAcknowledgments\nWe thank Holden Lee for helpful discussion and\nsuggestions, Anej Svete for help with the graphics\nand Chu-Cheng Lin and Xinyan Velocity Yu for\nhelpful comments. LD is partially supported by\nthe Johns Hopkins Mathematical Institute for Data\nScience (MINDS) Fellowship. Finally, we thank\nthe students of the LLM course at ETH Zürich (263-\n5354-00L) for carefully reading this paper as part\nof their lecture notes, and in particular, Valentin\nBieri for making a valuable remark.\nReferences\nSheldon Axler. 2020. Measure, Integration & Real\nAnalysis. Springer International Publishing.\nPatrick Billingsley. 1995. Probability and Measure, 3rd\nedition. Wiley.\nDavid Blackwell and Persi Diaconis. 1996. A non-\nmeasurable tail set. Statistics, probability and game\ntheory: Papers in honor of David Blackwell, 30:1–5.\nTaylor L. Booth and Richard A. Thompson. 1973. Ap-\nplying probability measures to abstract languages.\nIEEE Transactions on Computers , C-22(5):442–\n450.\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 4617–4624, Online.\nAssociation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901.\nYining Chen, Sorcha Gilroy, Andreas Maletti, Jonathan\nMay, and Kevin Knight. 2018. Recurrent neural\nnetworks as weighted language recognizers. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 2261–2271, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nZhiyi Chi. 1999. Statistical properties of probabilistic\ncontext-free grammars. Computational Linguistics,\n25(1):131–160.\nZhiyi Chi and Stuart Geman. 1998. Estimation of prob-\nabilistic context-free grammars. Computational Lin-\nguistics, 24(2):299–305.\nShay B. Cohen and Mark Johnson. 2013. The effect\nof non-tightness on Bayesian estimation of PCFGs.\nIn Proceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1033–1041, Soﬁa, Bulgaria.\nAssociation for Computational Linguistics.\nK. de Leeuw, E. F. Moore, C. E. Shannon, and\nN. Shapiro. 1956. COMPUTABILITY BY PROB-\nABILISTIC MACHINES , Annals of Mathematics.\nStudies, no. 34, pages 183–212. Princeton Univer-\nsity Press.\nRick Durrett. 2019. Probability: Theory and Examples,\n5th edition. Cambridge Series in Statistical and Prob-\nabilistic Mathematics. Cambridge University Press.\nJeffrey L. Elman. 1990. Finding structure in time. Cog-\nnitive Science, 14(2):179–211.\nGerald B. Folland. 1999. Real Analysis: Modern Tech-\nniques and Their Applications, 2nd edition. Wiley.\nCharles M. Grinstead and J. Laurie Snell. 1997. Intro-\nduction to Probability , 2nd revised edition. Ameri-\ncan Mathematical Society.\n9753\nJohn Hale. 2001. A probabilistic Earley parser as a psy-\ncholinguistic model. In Second Meeting of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learn-\ning Representations.\nRoger A. Horn and Charles R. Johnson. 2012. Matrix\nAnalysis, 2nd edition. Cambridge University Press.\nFrederick Jelinek. 1976. Continuous speech recogni-\ntion by statistical methods. Proceedings of the IEEE,\n64(4):532–556.\nDan Jurafsky and James Martin. 2009.Speech and Lan-\nguage Processing: An Introduction to Natural Lan-\nguage Processing, Computational Linguistics, and\nSpeech Recognition, 2 nd edition. Pearson Prentice\nHall.\nA. N. Kolmogorov. 1933. Grundbegriffe der\nWahrscheinlichkeitsrechnung. Springer.\nDaniel J. Lehmann. 1977. Algebraic structures for\ntransitive closure. Theoretical Computer Science ,\n4(1):59–76.\nChu-Cheng Lin. 2022. On Expressiveness, Inference,\nand Parameter Estimation of Discrete Sequence\nModels. Ph.D. thesis, Johns Hopkins University.\nChu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R.\nGormley, and Jason Eisner. 2021. Limitations of au-\ntoregressive models and their alternatives. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n5147–5173, Online. Association for Computational\nLinguistics.\nChu-Cheng Lin and Arya D. McCarthy. 2022. On\nthe uncomputability of partition functions in energy-\nbased sequence models. In International Confer-\nence on Learning Representations.\nTianyu Liu, Yuchen Jiang, Nicholas Monath, Ryan Cot-\nterell, and Mrinmaya Sachan. 2022. Autoregressive\nstructure prediction with language models. In Find-\nings of the Association for Computational Linguis-\ntics: EMNL 2022 , Abu Dhabi, United Arab Emi-\nrates. Association for Computational Linguistics.\nAndre Martins and Ramon Astudillo. 2016. From soft-\nmax to sparsemax: A sparse model of attention and\nmulti-label classiﬁcation. In Proceedings of The\n33rd International Conference on Machine Learn-\ning, volume 48 of Proceedings of Machine Learning\nResearch, pages 1614–1623, New York, New York,\nUSA. PMLR.\nAndré F. T. Martins. 2021. Reconciling the discrete-\ncontinuous divide: Towards a mathematical theory\nof sparse communication. CoRR, abs/2104.00755.\nClara Meister, Tiago Pimentel, Patrick Haller, Lena\nJäger, Ryan Cotterell, and Roger Levy. 2021. Re-\nvisiting the uniform information density hypothesis.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n963–980, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nClara Meister, Tiago Pimentel, Gian Wiher, and Ryan\nCotterell. 2022. Locally typical sampling. Transac-\ntions of the Association for Computational Linguis-\ntics.\nJames R. Munkres. 2000. Topology, 2nd edition. Pren-\ntice Hall, Inc.\nVinod Nair and Geoffrey E. Hinton. 2010. Rectiﬁed\nlinear units improve restricted Boltzmann machines.\nIn Proceedings of the 27th International Conference\non International Conference on Machine Learning ,\npages 807–814, Madison, WI, USA.\nMark-Jan Nederhof and Giorgio Satta. 2006. Estima-\ntion of consistent probabilistic context-free gram-\nmars. In Proceedings of the Human Language Tech-\nnology Conference of the NAACL, Main Conference,\npages 343–350, New York City, USA. Association\nfor Computational Linguistics.\nJohn C. Oxtoby. 1980. Measure and Category: A Sur-\nvey of the Analogies between Topological and Mea-\nsure Spaces. Springer New York.\nBen Peters, Vlad Niculae, and André F. T. Martins.\n2019. Sparse sequence-to-sequence models. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1504–\n1519, Florence, Italy. Association for Computational\nLinguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nEmily Reif, Daphne Ippolito, Ann Yuan, Andy Co-\nenen, Chris Callison-Burch, and Jason Wei. 2022. A\nrecipe for arbitrary text style transfer with large lan-\nguage models. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers) , pages 837–848,\n9754\nDublin, Ireland. Association for Computational Lin-\nguistics.\nDaniel M. Roy. 2011. Computability, Inference and\nModeling in Probabilistic Programming. Ph.D. the-\nsis, Massachusetts Institute of Technology, USA.\nAAI0823858.\nHalsey L. Royden. 1988. Real Analysis , 3 rd edition.\nPrentice-Hall.\nClaude E. Shannon. 1948. A mathematical theory\nof communication. Bell System Technical Journal ,\n27:623–656.\nTerence Tao. 2011.An Introduction to Measure Theory.\nAmerican Mathematical Society.\nTerence Tao. 2016. Analysis II: Third Edition . Texts\nand Readings in Mathematics. Springer Singapore.\nRobert E. Tarjan. 1981. Fast algorithms for solving\npath problems. Journal of the Association for Com-\nputing Machinery, 28(3):594–614.\nStanisław Ulam. 1930. Zur masstheorie in der allge-\nmeinen mengenlehre. Fundamenta Mathematicae ,\n16(1):140–150.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\nE. Vidal, F. Thollard, C. de la Higuera, F. Casacu-\nberta, and R.C. Carrasco. 2005. Probabilistic ﬁnite-\nstate machines - part i. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 27(7):1013–\n1025.\nWarren Weaver. 1955. Translation. In William N.\nLocke and A. Donald Boothe, editors, Machine\nTranslation of Languages, pages 15–23. MIT Press.\nReprinted from a memorandum written by Weaver\nin 1949.\nSean Welleck, Ilia Kulikov, Jaedeok Kim,\nRichard Yuanzhe Pang, and Kyunghyun Cho.\n2020. Consistency of a recurrent language model\nwith respect to incomplete decoding. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n5553–5568, Online. Association for Computational\nLinguistics.\n9755\nA Related Work\nThe issue of tightness has been studied extensively in the context of probabilistic context-free grammars\n(PCFG; Chi and Geman, 1998; Chi, 1999; Cohen and Johnson, 2013), although Chi (1999) refers to\nnon-tight models as improper. Speciﬁcally, Chi (1999) gave algorithms for determining the tightness of\na PCFG by formalizing a PCFG as a branching process. Chi (1999) further proved that any maximum-\nlikelihood estimator yields a tight PCFG. Several previous works study the ability of language models to\nplace probability mass on inﬁnite-length strings (Booth and Thompson, 1973; Nederhof and Satta, 2006;\nChen et al., 2018; Welleck et al., 2020), where they refer to the non-tight language models asinconsistent.\nIn some cases, this behavior can be attributed to the discrepancy between the language model itself and\nthe distribution induced by a (possibly stochastic) decoding algorithm: the decoder may have a lower\nprobability of generating the EOS token. For example, on the tight bigram model of Ex. 2.3, a greedy\ndecoder will always generate a and never EOS . Yet in other examples, it is the model itself that leaks\nprobability mass to inﬁnite-length strings, i.e., it may be non-tight, which is the problem we focus on in\nthis work, providing a characterization of tightness. Notably, the conditions we propose are more general\nthan those of Welleck et al. (2020).\nSeveral other works consider the limitations of common neural network architectures for modeling\ndistributions over ﬁnite sequences (strings), albeit focusing speciﬁcally on other attributes, such as their\ncomputational complexity for problems like equivalence or undecidability (Chen et al., 2018; Lin et al.,\n2021; Lin and McCarthy, 2022; Lin, 2022). In contrast, this work provides a formal treatment of language\nmodels by enlarging the sample space to Σ∗∪Σ∞, although to ensure tightness, Σ∞ must receive\nprobability 0. Such deﬁnitions are not uncommon in probability theory. For example, while the Wiener\nprocess (i.e., the standard Brownian motion) is a distribution over all functions, the deﬁnition ensures that\nthe set of discontinuous functions is assigned probability 0 (Durrett, 2019, Ch. 7).\nMeister et al. (2022) similarly address the notion of a language model as a distribution over inﬁnite\nsequences by casting such models as stochastic processes. They use this framing in order to motivate\ndecoding, without providing comprehensive measure-theoretic foundations of such distributions.\nB Details for Motivating Ex. 2.3\nHere, we lay out the steps to calculate P(Σ∗) from Fig. 1b:\nP(Σ∗) =\n∞∑\nn=0\n(\nP(an+1) +\n∞∑\nm=0\nP(an+1bm+1)\n)\n(27a)\n=\n∞∑\nn=0\n(\n1 ·(0.7)n ·0.1 +\n∞∑\nm=0\n1 ·(0.7)n ·0.2 ·(0.9)m ·0.1\n)\n(27b)\n=\n∞∑\nn=0\n(0.7)n ·\n(\n0.1 + 0.2 ·\n( ∞∑\nm=0\n(0.9)m\n)\n·0.1\n)\n(27c)\n=\n∞∑\nn=0\n(0.7)n ·\n(\n0.1 + 0.2 · 1\n1 −0.9 ·0.1\n)\n(27d)\n=\n∞∑\nn=0\n(0.7)n ·0.3 = 0.3\n1 −0.7 = 1 (27e)\nC Measure Theory Details\nC.1 Proofs and Details in §3.3\nC.1.1 Details of Thm. 3.5\nTheorem 3.5. Assuming the Axiom of Choice and the Continuum Hypothesis, there exists no probability\nmeasure P over ({H,T}∞,P({H,T}∞)) such that P({ω}) = 0 for each ω∈{H,T}∞.\n9756\nThis theorem is an impossibility of measure theorem. Generally speaking, the existence of a\nnon-measurable set implies some form of impossibility of measure. The most famous example of\nnon-measurable sets are Vitali sets, which exist given the Axiom of Choice. Vitali’s 1905 construction\nis typically described in introductory texts on measure theory (Royden, 1988; Billingsley, 1995; Axler,\n2020). The existence of Vitali sets shows that it is impossible to deﬁne a probability measure that\nsatisﬁes translational invariance on the measurable space\n(\n[0,1),P([0,1))\n)\n. Thus, to achieve translational\ninvariance, Lebesgue measure uses a σ-algebra smaller than P([0,1)), in which the Vitali sets are\namong the non-measurable sets. However, the translational invariance desideratum is not relevant to our\nspace of discrete sequences. A theorem by Ulam (1930) reveals a deeper reason that some sets must be\nnon-measurable. We shall state the theorem below as given in Oxtoby (1980) and omit its proof. We refer\ninterested readers to Chapter 5 in Oxtoby (1980), which contains an accessible proof and an excellent\ndiscussion of the theorem including its generalizations and historical context.\nTheorem C.1 (Ulam, 1930). Assuming the Axiom of Choice, a ﬁnite measure µdeﬁned for all subsets of\na set X of cardinality ℵ1 vanishes identically [that is, equals zero for all subsets] if it is equal to zero for\nevery one-element subset.\nIn the statement above, ℵ1 denotes the cardinality of the ﬁrst uncountable ordinal. We can see that\nThm. 3.5 is a straightforward consequence of Thm. C.1.\nProof of Thm. 3.5. Recall that card({H,T}∞) = 2ℵ0. Assuming the Continuum Hypothesis, 2ℵ0 = ℵ1,\nand hence by Thm. C.1, such a measure is uniformly 0, and hence cannot be a probability measure. ■\nC.1.2 Other Proofs in §3.3\nLemma 3.9. C⊂P (Ω) is an algebra over Ω = Σ\n∞\n.\nProof. First, Ω ∈Csince it is a cylinder set of rank 0 or indeed of any rank k: Ω = C(Σ\nk\n) ∈Ck ⊂C.\nSecond, Cis closed under complements: given a cylinder set of rank k, that is, C(H) where H ⊆Σ\nk\n, its\ncomplement\n(\nC(H)\n)c = C\n(\nΣ\nk\n\\H\n)\nis also a cylinder set of rank k. Finally, Cis closed under union:\nthe union of cylinder sets of ranks k1 ≤k2 is a cylinder set of rank k2, since both can be regarded as\ncylinder sets of rank k2. Hence, Cis an algebra over Ω. ■\nProposition C.2. P0 as deﬁned in Eq. (8) is a well-deﬁned function.\nProof. Suppose a cylinder set arises in two ways, C(H1) = C(H2), where H1 ⊆Σ\nk1\nand H2 ⊆Σ\nk2\n.\nWe must show ∑\nx∈H1 ¯p(x) = ∑\nx′∈H2 ¯p(x′). Without loss of generality, assume that k1 ≤k2. The\ndeﬁnition of C(H2) (Def. 3.8) implies that H2 consists of all length-k2 preﬁxes of strings in C(H2). But\nC(H2) = C(H1), so the deﬁnition of C(H1) (Def. 3.8) implies that its length- k2 preﬁxes are exactly\nthe strings of the form xywhere x ∈H1,y ∈Σ\nk2−k1\n. Hence we can write H2 in terms of H1 as\nH2 = {xy: x∈H1,y∈Σ\nk2−k1\n}. Thus\n∑\nx′∈H2\n¯p(x′) =\n∑\nx∈H1\n∑\ny∈Σk2−k1\n¯p(xy) =\n∑\nx∈H1\n¯p(x) (28)\nwhere the last equality is true because ¯pis deﬁned by the locally normalized product (9). ■\nLemma 3.10. P0 is a pre-measure over C.\nFor the proof of Lemma 3.10, we will mostly follow the proof of Thm 2.3 in Billingsley (1995), with\nthe exception of invoking the Tychonoff theorem directly. This proof depends on the following lemma,\nwhich is Example 2.10 in Billingsley (1995). We repeat the statement and proof here for the reader’s\nconvenience.\nLemma C.3. Let P0 be a ﬁnitely additive probability pre-measure overCsuch that, given a decreasing\nsequence of sets A1 ⊃A2 ⊃··· in Cwhere ⋂∞\nn=1 An = ∅, limn→∞P0(An) = 0 . Then, P0 is also\ncountably additive over C.\n9757\nProof. Let {An}be a sequence of disjoint sets in Csuch that A = ⋃\nnAn ∈ C. Then, deﬁning\nBn = ⋃\nm>nAm, we see that B1 ⊃B2 ⊃··· and ⋂\nnBn = ∅. Notice that\nA= A1 ∪B1 = A1 ∪A2 ∪B2 = ··· = A1 ∪···∪ An ∪Bn (29)\nfor any nand hence by ﬁnite additivity of P0\nP0(A) = P0(A1) + ··· + P0(An) + P0(Bn) (30)\nor equivalently\nP0(A1) + ··· + P0(An) = P0(A) −P0(Bn). (31)\nSince, Bn ↓∅ implies that P0(Bn) ↓0 by assumption, taking the limits on both sides of Eq. (31) yields\n∑\nn\nP0(An) = lim\nn→∞\n∑\ni≤n\nP0(Ai) = P0(A) − lim\nn→∞\nP0(Bn) = P0(A) (32)\nwhich shows countable additivity. ■\nWe also recall the Tychonoff theorem.21\nTheorem C.4 (Tychonoff). Let {Xα}α∈J be an indexed family of compact topologies. Then, their product\ntopology ∏\nα∈J Xα is also compact.\nWe can now give the proof for Lemma 3.10.\nProof of Lemma 3.10. We ﬁrst show that P0 is ﬁnitely additive over C. Let C(H1) and C(H2) be two\ndisjoint cylinder sets. By Prop. C.2, we can assume they are of the same rank without loss of generality.\nThen,\nC(H1) ∪C(H2) =\n⋃\nx∈H1\n{xω: ω∈Σ\n∞\n}∪\n⋃\nx∈H2\n{xω: ω∈Σ\n∞\n} (33a)\n=\n⋃\nx∈H1∪H2\n{xω: ω∈Σ\n∞\n} (H1 and H2 equal rank and disjoint) (33b)\n= C(H1 ∪H2) (33c)\nwhich leads to\nP0(C(H1) ∪C(H2)) = P0(C(H1 ∪H2)) =\n∑\nx∈H1∪H2\n¯p(x) = P0(C(H1)) + P0(C(H2)). (34a)\nHence, P0 is ﬁnitely additive.\nNow, equip Σ with the discrete topology. Since Σ is ﬁnite, it is compact under the discrete topology\nand so is Σ\n∞\nby Thm. C.4. Then, by properties of the product topology over discrete ﬁnite spaces, all\ncylinder sets in Σ\n∞\nare compact. To apply Lemma C.3, let C1 ⊃C2 ⊃··· be a decreasing sequence of\ncylinder sets with empty intersection. Suppose to the contrary that P0 (⋂\nnCn) >0. This would imply\nthat all Cn are nonempty (any of these being empty would result in a measure 0). However, by Cantor’s\nintersection theorem22, ⋂\nnCn is nonempty, contradicting the assumption. Hence, P0 (⋂\nnCn) = 0, and\nby Lemma C.3, P0 is countably additive. ■\n21See §37 in Munkres (2000) for a detailed and well-written treatise.\n22Cantor’s intersection theorem states that a decreasing sequence of nonempty compact sets have a nonempty intersection. A\nversion of this result in introductory real analysis is the Nested Interval Theorem.\n9758\nC.2 Details in §3.4\nC.2.1 Carathéodory’s Extension Theorem\nTheorem 3.11 (Carathéodory’s Extension Theorem). Given an algebra Aover some set Ω and a prob-\nability pre-measure P0 : A→ [0,1], there exists a probability space (Ω,F,P) such that A⊂F and\nP|A= P0. Furthermore, the σ-algebra Fdepends only on Aand is minimal and unique—thus we may\ndenote it by σ(A)—and the probability measure P is unique.\nProof Sketch. First, construct an outer measure by approximation with countable coverings. Then, show\nthat the collection of sets that is measurable with respect to this outer measure is a σ-algebra Fthat\ncontains A. Finally, restricting the outer measure to thisσ-algebra, one is then left with a probability space.\nTo show minimality, one can show thatFis contained in any σ-algebra that contains A. Uniqueness is\ngiven by applying Dynkin’sπ-λtheorem (Theorem 3.2 in Billingsley, 1995).\nGreat care must be taken in each step involved in the outline above. To address these is well beyond the\nscope of this paper and we refer reader to the many excellent texts with a proof of this theorem, such as\nChapter 12 in Royden (1988) and Chapter 11 in Billingsley (1995). ■\nC.2.2 The Space of Non-measurable Sets\nNon-measurable sets are, in general, difﬁcult to ﬁnd. Even when we can exhibit such sets, they tend to\nbe very abstract and counter-intuitive. Vitali’s and Bernstein’s sets are two prominent examples for the\nLebesgue measure. Blackwell and Diaconis (1996) offers a construction of a non-measurable set in the\ncylinder σ-algebra.23\nAs another approach to understand this better, we can consider how our collectionσ(C) of all measurable\nsets, i.e., our σ-algebra, is constructed from our algebra Cof cylinder sets (as opposed to simply knowing\nfrom Carathéodory’s Extension Theorem that it exists). Concretely, as in §1.6 in Folland (1999), we\ncan intuitively consider the following process to build from the collection of cylinder setsC, which is a\ncountable collection, all the way up to its generated σ-algebra, whose cardinality is unknown just yet:\n• Let C0 = C,\n• Let C1 be the set that includes all countable unions of sets in C0 or the complements of such,\n• Repeat this process to build Cn for every n∈N.\nOne might then take the union ⋃\nn∈N Cn of this increasing sequence of collections of sets, and ask if it\nis the same as σ(C). In general, the answer is no (as one might expect if one is familiar with the Borel\nHierarchy). However, we can obtain σ(C) if we perform this construction for every countable ordinal α.\nAbbreviating the operation in the second step above as δ, i.e., C1 = δ(C0), and letting ω1 be the collection\nof all countable ordinals,24 we can deﬁne\nCα =\n{\nδ(Cβ) if α= β+ 1 for some β ∈ω1,⋃\nβ∈ω1:β<αCβ otherwise. (35)\nThis will give us the desired set as follows:\nProposition C.5 (Proposition 1.23, Folland, 1999). σ(C) = ⋃\nα∈ω1 Cα.\nNext, we recall the following basic fact from cardinality theory.\nProposition C.6 (Proposition 0.14, Folland, 1999). If card(A) ≤2ℵ0 and card(Xα) ≤2ℵ0 for all α∈A,\nthen card\n(⋃\nα∈AXα\n)\n≤2ℵ0.\nNoting that card(ω1) ≤ 2ℵ0 and card(C) = ℵ0, we can conclude that card(σ(C)) ≤ 2ℵ0 from\nProp. C.5 and Prop. C.6. In other words, the cardinality of σ(C) is at most that of the continuum, and since\ncard\n(\nP(Σ\n∞\n)\n)\n= 22ℵ0\n(= ℶ2), σ(C) is, in terms of cardinality, an almost negligible subset of P(Σ\n∞\n)!\nThat is, most subsets in Σ\n∞\nare non-measurable—though explicit examples have rarely been constructed\n23The following assumes basic familiarity with the theory of ordinal numbers. Readers without such background may skip to\nthe last paragraph for conclusion.\n24ω1 is, in fact, the same as the ﬁrst uncountable ordinal. Its existence (and hence the existence of the collection of all\ncountable ordinals) can be guaranteed by exhibiting a well-ordered uncountable set using the Axiom of Choice.\n9759\n(Blackwell and Diaconis, 1996). App. C.3 below establishes that common subsets of Σ\n∞\nthat we work\nwith are measurable.\nC.3 Proofs in §3.5\nProposition 3.13. The function X : (Σ\n∞\n,σ(C)) →(Σ∗∪Σ∞,σ(C)) deﬁned in Eq. (12) is a measurable\nmapping.\nProof. To show that X is measurable, it sufﬁces to show the measurability of preimages of a generating\nset25 of the σ-algebra σ(C) on Σ∗∪Σ∞. Such a generating set is formed by the thin cylinders C(x)\ndef\n=\nC({x}) for x∈Σ∗. (Recall that cylinders in Σ∗∪Σ∞are deﬁned by Eq. (11).) Given x∈Σ∗:\nX−1(C(x)) =X−1({xω: ω∈Σ∗∪Σ∞}) (36a)\n=X−1({xω: ω∈Σ∗}) ∪X−1({xω: ω∈Σ∞}) (36b)\n=\n( ⋃\nω∈Σ∗\nC(xωEOS )\n)\n∪\n(\nC(x) ∩\n∞⋂\nk=1\nAc\nk\n)\n(36c)\nNote that the set Ak above, deﬁned by Eq. (16), is a cylinder of Σ\n∞\n, representing the event of terminating\nby step k. Then, from the derivation above, we can see thatX−1(C(x)) is formed by countable operations\nover measurable sets (cylinders) of Σ\n∞\n, and is hence measurable. So X is a measurable function. ■\nProposition C.7. In measure space (Σ∗∪Σ∞,σ(C)), {x}is measurable for all x∈Σ∗.\nProof. We will show that{x}= C(x) \\⋃\na∈Σ C(xa) and hence is measurable. By deﬁnition in Eq. (11),\nfor any x∈Σ∗,\nC(x) = {xω: ω∈Σ∗∪Σ∞} (37a)\n= {xω: ω∈Σ∗}∪{xω: ω∈Σ∞} (37b)\nwhere\n{xω: ω∈Σ∗}= {x}∪\n⋃\na∈Σ\n{xaω: ω∈Σ∗} (38a)\nand\n{xω: ω∈Σ∞}=\n⋃\na∈Σ\n{xaω: ω∈Σ∞}. (39)\nSo\nC(x) = {x}∪\n⋃\na∈Σ\n(\n{xaω: ω∈Σ∗}∪{xaω: ω∈Σ∞}\n)\n(40a)\n= {x}∪\n⋃\na∈Σ\nC(xa) (40b)\nwhere the union is disjoint. This implies {x}= C(x) \\⋃\na∈Σ C(xa) as desired. ■\nProposition C.8. In the measure space (Σ∗∪Σ∞,σ(C)), Σ∞is measurable.\nProof. First, Σ∗∪Σ∞is the entire outcome space, which is measurable by the deﬁnition of σ-algebra.\nNotice that\nΣ∞= (Σ∗∪Σ∞) \\\n⋃\nx∈Σ∗\n{x}. (41)\nSince each {x}in the above is measurable by Prop. C.7 and Σ∗is a countable set, Σ∞is then measurable.\n■\n25A set Gis said to be a generating set of a σ-algebra Fif Fis the smallest σ-algebra that contains G.\n9760\nThe measurability of Σ∞in (Σ∗∪Σ∞,σ(C)) (Prop. C.8) was assumed by our deﬁnition of tightness\n(Def. 4.1). As we have also established that each {x}is measurable (Prop. C.7), we can give an alternative\ncharacterization.\nProposition C.9. A sequence model (Σ∗∪Σ∞,σ(C),P) is tight if and only if ∑\nx∈Σ∗P({x}) = 1.\nProof. We deﬁned a sequence model to be tight if and only ifP(Σ∞) = 0 (Def. 4.1). By Propositions C.7\nand C.8, we can write\n1 = P(Σ∗∪Σ∞) = P(Σ∞) + P(Σ∗) (ﬁnite additivity) (42a)\n= P(Σ∞) +\n∑\nx∈Σ∗\nP({x}). (countable additivity) (42b)\nHence, a sequence model is tight if and only if ∑\nx∈Σ∗P({x}) = 1. ■\nD Proofs on Characterizing Tightness (§4)\nD.1 Proof of Lemma 4.2\nThe result below is stated without proof as Exercise 4.3.5 in Durrett (2019).\nLemma 4.2. If ∑∞\nn=2 P\n(\nAn |∩n−1\nm=1Ac\nm\n)\n= ∞, then P(∩∞\nm=1Ac\nm) = 0.\nProof. First, recall an elementary inequality that for x> 0,\nx−1 ≥log x ⇔ 1 −x≤log 1\nx. (43)\nNote that P(∩n\nm=1Ac\nm) >0 for any n, for otherwise the conditional probabilities would be undeﬁned. Let\npn\ndef\n= P(∩n\nm=1Ac\nm). Then we have that pn >0 for all n, and\n∞=\n∞∑\nn=2\nP(An |∩n−1\nm=1Ac\nm) (44a)\n=\n∞∑\nn=2\n1 −P(Ac\nn |∩n−1\nm=1Ac\nm) (44b)\n= lim\nN→∞\nN∑\nn=2\n1 −P(Ac\nn |∩n−1\nm=1Ac\nm) (44c)\n≤ lim\nN→∞\nN∑\nn=2\nlog 1/P(Ac\nn |∩n−1\nm=1Ac\nm) (by Eq. (43)) (44d)\n= lim\nN→∞\nN∑\nn=2\nlog P(∩n−1\nm=1Ac\nm)\nP(∩n\nm=1Acm) (44e)\n= lim\nN→∞\nN∑\nn=2\nlog pn−1\npn\n(44f)\n= lim\nN→∞\nN∑\nn=2\n(log pn−1 −log pn) (44g)\n= lim\nN→∞\n(log p1 −log pN) (44h)\n= log p1 − lim\nN→∞\nlog pN (44i)\nwhich implies that\nlim\nN→∞\nlog pN = −∞ (45a)\n9761\n⇔ lim\nN→∞\npN = 0 (45b)\n⇔ lim\nN→∞\nP(∩N\nm=1Ac\nm) = 0 (45c)\n⇔ P(∩∞\nm=1Ac\nm) = 0. (by continuity of measure) (45d)\n■\nD.2 Proof of Prop. 4.3\nProposition 4.3. If ¯p(EOS |x) ≥f(t) for all t≥1,x∈Σt−1, and ∑∞\nt=1 f(t) = ∞, then P(∩∞\nk=1Ac\nk) =\n0. In other words, P is tight.\nProof. In the proof, we rename the index tto nto match the usual presentation of the Borel-Cantelli\nlemmata. We are given that ¯p(EOS |x) ≥f(n) for all x∈Σn−1. To apply Lemma 4.2, we observe that\nAn ∩(Ac\n1 ∩···∩ Ac\nn−1) ={ω∈Σ\n∞\n: ωn = EOS }∩\n(n−1⋂\ni=1\n{ω∈Σ\n∞\n: ωi ̸= EOS }\n)\n(46a)\n={ω∈Σ\n∞\n: ω= EOS ,∀i<n, ω̸= EOS } (46b)\n={ω∈Σ\n∞\n: ω’s ﬁrstEOS is at position n} (46c)\nand similarly\nAc\n1 ∩···∩ Ac\nn−1 = {ω∈Σ\n∞\n: there is no EOS in ω’s ﬁrstn−1 positions} (47)\nSetting G\ndef\n= {ωEOS : ω∈Σn−1}⊂ Σ\nn\n, we get\nP(An |Ac\n1 ∩···∩ Ac\nn−1) = P(An ∩(Ac\n1 ∩···∩ Ac\nn−1))\nP(Ac\n1 ∩···∩ Ac\nn−1) (48a)\n= P(C(G))\nP(C(Σn−1)) (deﬁnition of G) (48b)\n=\n∑\nω∈Σn−1 ¯p(EOS |ω)¯p(ω)∑\nω∈Σn−1 ¯p(ω) (by Eq. (8)) (48c)\n≥\n∑\nω∈Σn−1 f(n)¯p(ω)∑\nω∈Σn−1 ¯p(ω) (deﬁnition of f(n)) (48d)\n= f(n)\n∑\nω∈Σn−1 ¯p(ω)∑\nω∈Σn−1 ¯p(ω) (48e)\n= f(n). (48f)\nSince ∑∞\nn=1 f(n) = ∞and hence ∑∞\nn=2 f(n) = ∞, the above inequality shows that the condition\nof Lemma 4.2 holds. Hence by Lemma 4.2, the event of a string never terminating, i.e., ∩∞\nk=1Ac\nk, has\nprobability measure P(∩∞\nk=1Ac\nk) = 0.\nIn summary, if the EOS probability of a language model is lower-bounded at ever steps by the terms of\na divergent series, then the event that this language model terminates has probability 1. ■\nD.3 Proof of Cor. 4.6\nTo show Cor. 4.6, we ﬁrst show the following simple consequence of Borel–Cantelli.\nCorollary D.1. If P(An i.o.) = 1, then ∑∞\nn=1 P(An) = ∞.\nProof. Suppose to the contrary that ∑∞\nn=1 P(An) < ∞, then, by Borel–Cantelli I (Lemma 4.4),\nP(An i.o.) = 0, which contradicts the assumption. Hence, ∑∞\nn=1 P(An) = ∞.\n■\nCor. 4.6 below is also stated without proof as Exercise 4.3.4 in Durrett (2019).\n9762\nCorollary 4.6. Given a sequence {pn}where pn ∈[0,1). Then,\n∏∞\nn=1(1 −pn) = 0 ⇐⇒∑∞\nn=1 pn = ∞. (19)\nProof. We can use a product measure to construct a sequence of independent events{An}∞\nn=1 such that\nP(An) = pn. (The product measure ensures independence.) Then, by deﬁnition in Eq. (18),\n{An i.o.}c =\n∞⋃\nm=1\n⋂\nn≥m\nAc\nn (49)\nSo,\n1 −P(An i.o.) = P\n\n⋃\nm\n⋂\nn≥m\nAc\nn\n\n (50a)\n= lim\nm→∞\nP\n\n⋂\nn≥m\nAc\nn\n\n (50b)\n= lim\nm→∞\n∏\nn≥m\nP(Ac\nn) (An are independent by construction) (50c)\n= lim\nm→∞\n∏\nn≥m\n(1 −pn) (50d)\n(⇒): Assume ∏∞\nn=1(1 −pn) = 0. Then, for any m,\n0 =\n∏\nn≥1\n(1 −pn) =\n\n ∏\n1≤n<m\n(1 −pn)\n\n\n  \n>0\n\n∏\nn≥m\n(1 −pn)\n\n (51)\nSo it must the case that, for any m, ∏\nn≥m(1 −pn) = 0. Therefore,\n1 −P(An i.o.) = lim\nm→∞\n∏\nn≥m\n(1 −pn) = 0 (52)\nwhich implies P(An i.o.) = 1. Cor. D.1 implies that ∑∞\nn=1 pn = ∞.\n(⇐): Assume ∑∞\nn=1 pn = ∞. Then by Borel–Cantelli II (Lemma 4.5), P(An i.o.) = 1 which implies\n0 = 1 −P(An i.o.) = lim\nm→∞\n∏\nn≥m\n(1 −pn) (53)\nObserve that\n{∏\nn≥m(1 −pn)\n}\nm\nis a non-decreasing sequence in m; to see this, note that as mgrows\nlarger we multiply strictly fewer values (1 −pn) ∈(0,1]. However, since we know the sequence is\nnon-negative and tends to 0, it follows that for any m, we have\n∏\nn≥m\n(1 −pn) = 0. (54)\nIt follows that, for any m, we have\n∞∏\nn=1\n(1 −pn) =\n∏\nn<m\n(1 −pn)\n∏\nn≥m\n(1 −pn)\n  \n=0\n=\n∏\nn<m\n(1 −pn) ·0 = 0. (55)\n■\n9763\nD.4 Proof of Thm. 4.7\nTheorem 4.7 (Proposition 2.4 in Meister et al., 2022). An ASM is tight if and only if ˜pEOS (t) = 1 for some\ntor ∑∞\nt=1 ˜pEOS (t) = ∞.\nProof. Recall the deﬁnition of ˜pEOS , as previously deﬁned in Eq. (20), is\n˜pEOS (t)\ndef\n= P(At |Ac\n1 ∩···∩ Ac\nt−1). (56)\nCase 1. Suppose that ˜pEOS (t) <1 for all t. Consider the termination probability again:\nP\n(∞⋂\nt=1\nAc\nt\n)\n= lim\nT→∞\nP\n(T⋂\nt=1\nAc\nt\n)\n(57a)\n= lim\nT→∞\nT∏\nt=1\nP(Ac\nt |Ac\n1 ∩···∩ Ac\nt−1) (57b)\n= lim\nT→∞\nT∏\nt=1\n(1 −˜pEOS (t)) (57c)\n=\n∞∏\nt=1\n(1 −˜pEOS (t)). (57d)\nIn the above, we have assumed that P(Ac\n1 ∩···∩ Ac\nt) > 0 for all t, which is true by assumption that\n˜pEOS (t) <1.. Hence, by Cor. 4.6, Eq. (57d) is 0 if and only if ∑\nt ˜pEOS (t) = ∞.\nCase 2. If ˜pEOS (t) = 1 is true for some t= t0, then P(Ac\n1 ∩···∩ Ac\nt0) = 0 and hence P(⋂∞\nt=1 Ac\nt) = 0\nand such a language model is guaranteed to terminate at t0. ■\nE Proofs for Analyses of Common Language Models (§5)\nE.1 Proofs for FSSMs (§5.1)\nE.1.1 Proofs for Stochastic FSSMs\nTheorem 5.3. An SFSSM is tight iff all accessible states are also co-accessible.\nProof. We refer to a state qas initial if sq >0 and as ﬁnal if tq >0. (These are sometimes called source\nand sink states.) We prove each direction of the theorem in turn:\n(⇒): Assume the SFSSM is tight. Let q be an accessible state. Since the SFSSM has at least one\npositive-probability path from an initial state, there is a positive probability of reachingqduring generation.\nIf there were no positive-probability path fromqto a ﬁnal state, then the SFSSM would never terminate on\nthe occasions when it reached q, contradicting the assumption of tightness. Hence qmust be co-accessible.\n(⇐): Assume that all accessible states are co-accessible. We construct a Markov chain whose states are\nthe SFSSM’s accessible statesQA ⊆{1,...,Q }together with an EOS state. In this Markov chain, the\ninitial probability of qis given bysq when q∈QA and by 0 when q= EOS ; the transition probability from\nqto q′is given by Pqq′ when q,q′∈QA, by tq when q ∈QA and q′= EOS , by 1 when q = q′= EOS ,\nand by 0 otherwise. The probability that the Markov chain is in state q ∈QA after tsteps equals the\nprobability that the SFSSM is in stateqafter tsteps (note that the SFSSM never reaches any stateq /∈QA).\nThe probability that it is in state EOS after tsteps equals the probability that the SFSSM has terminated\nafter ≤tsteps.\nClearly EOS is an absorbing state of the Markov chain, meaning that once the Markov chain reaches\nthis state, it never leaves. A fundamental result on ﬁnite-state Markov chains (Grinstead and Snell, 1997,\nTheorem 11.3) is that if every state can reach an absorbing state, then with probability 1, the chain reaches\nan absorbing state (“is absorbed”) in ﬁnite time. Every state can in fact reach EOS , by coaccessibility\nof QA. This further implies that EOS is the only absorbing state (as an absorbing state cannot reach any\n9764\nother state). So by the result cited above, the Markov chain reaches EOS with probability 1 in ﬁnite time.\nConsequently, the SFSSM terminates after ﬁnitely many steps with probability 1; that is, the SFSSM is\ntight. ■\nCorollary 5.4. Maximum likelihood estimates of n-gram models based on some corpus are tight.\nProof. The SFSSM for an n-gram model has states that correspond to (n−1)-grams and transitions\nthat correspond to characters (unigrams), as illustrated by Fig. 1. When the SFSSM’s probabilities are\nestimated with MLE, the accessible states are (n−1)-grams that have appeared in some string in the\ncorpus. Such states must also be co-accessible so that they can generate the rest of that string. Hence, by\nThm. 5.3, this SFSSM is tight. ■\nE.1.2 Proofs for Substochastic FSSMs\nTo prove Thm. 5.6, we will make use of the following useful lemma.\nLemma E.1. Let P′be the transition sum matrix of a trimmed substochastic FSSM. Then ρ(P′) < 1\nwhere ρ(·) denotes the spectral radius.\nProof. To begin with, we wish to apply the following result, which connects the row sums of a matrix to\nits spectral radius. Below, Mn denotes the set of n×nmatrices, and |||A|||∞= max1≤i≤n\n∑n\nj=1 |Aij|\ndenotes the operator ∞-norm.\nProposition E.2 (§6.2.P8, Horn and Johnson, 2012). For any A∈Mn, ρ(A) ≤|||A|||∞. Additionally, if\nAis irreducible and not all absolute row sums of Aare equal, then ρ(A) <|||A|||∞.\nHowever, the transition sum matrix P of a substochastic FSSM may be reducible, whereas the irre-\nducibility condition in Prop. E.2 cannot be dropped. Hence, we need to “decompose” P′in a way that\nrecovers irreducibility. We use the Frobenius normal form (also known as irreducible normal form) to\nachieve this.\nProposition E.3 (§8.3.P8, Horn and Johnson, 2012). Let A ∈Mn be non-negative. Then, either Ais\nirreducible or there exists a permutation matrix Π such that\nΠ⊤AΠ =\n\n\nA1 ∗\n...\n0 Ak\n\n (58)\nis block upper triangular, and each diagonal block is irreducible (possibly a 1 ×1 zero matrix). This\nis called a Frobenius normal form (or irreducible normal form) of A. Additionally, λ(A) = λ(A1) ∪\n···∪ λ(Ak) where λ(·) denotes the set of eigenvalues of a matrix.\nNotice that the permutation in the Frobenius normal form merely renumbers the states of the trimmed\nFSSM. We may check that as a result, the termination probability given in Thm. 5.6 is unchanged:26\n(Π⊤s′)⊤(Π⊤P′Π)k(Π⊤t′) = (s′⊤Π)(Π⊤P′kΠ)(Π⊤t′) = s′⊤P′kt′ (59)\nHence, with an appropriate renumbering, we may assume without loss of generality that P is already\ngiven in Frobenius normal form\nP′=\n\n\nP′\n1 ∗\n...\n0 P ′\nk\n\n (60)\nwhere each P′\ni is irreducible.\nSince the transition sum matrix P′of a trimmed substochastic FSSM is a substochastic matrix, each P′\ni\nis also substochastic. In fact, each P′\ni is strictly substochastic, meaning that there is at least one row that\n26The equalities here use the fact that the inverse of a permutation matrix Π is its transpose: Π Π⊤= I.\n9765\nsums to less than 1. To see this, suppose to the contrary that there is a stochastic P′\ni. Since the FSSM is\ntrimmed, every state is both accessible and co-accessible. Being accessible implies that there is a positive\nprobability of reaching every state in P′\ni. However, the stochasticity of P′\ni forces the corresponding t′\nentries to be 0. Hence, none of these states can transition to EOS , meaning that they’re not co-accessible,\ncontradicting the assumption. Hence, every P′\ni is strictly substochastic. Then, either all row sums of\nP′\ni are less than 1 (in which case |||P′\ni|||∞ < 1) or some row sums are 1 and some are less than 1 (in\nwhich case |||P′\ni|||∞= 1 and P′has unequal absolute row sums). In either case, Prop. E.2 implies that\nρ(P′\ni) <1, for all 1 ≤i≤k.\nFinally, the last sentence of Prop. E.3 entails that ρ(P′) = max {ρ(P′\n1),...,ρ (P′\nk)}. Since each\nρ(P′\ni) <1, we have ρ(P′) <1. ■\nTheorem 5.6. Let P′be the transition sum matrix of a trimmed substochastic FSSM. Then I −P′is\ninvertible and P(X ∈Σ∗) = s′⊤(I −P′)−1t′≤1.\nProof. By Lemma E.1, ρ(P′) <1, in which case I−P′is invertible and the Neumann series∑∞\nk=0 P′k =\nI + P′+ P′2 + ··· converges to (I −P′)−1 (Horn and Johnson, 2012, §5.6). Thus\nP(Σ∗) =\n∞∑\nk=0\nP(Σk) =\n∞∑\nk=0\ns′⊤P′kt′= s′⊤\n(∞∑\nk=0\nP′k\n)\nt′= s′⊤(I −P′)−1t′. (61)\n■\nE.2 Proofs for Transformer Result (§5.2)\nAgain, the following theorem is well-known:\nTheorem 5.7. Let X be a compact topological space and Y be any topological space. If f : X →Y is\ncontinuous, then f(X) ⊆Y is also compact.\nProof. Let {Uα}α∈Abe any open cover of f(X). By continuity, f−1(Uα) ⊂X is open for any α∈A,\nand hence {f−1(Uα)}α∈Ais also an open cover ofX. By the compactness of X, there is a ﬁnite sub-cover\n{f−1(Uαi)}n\ni=1, in which case {Uαi}n\ni=1 forms a ﬁnite sub-cover for f(X). ■\nLemma 5.8. Let f :\n(\nRd)+ →\n(\nRd)+ be the function deﬁned by a ﬁnite number of Transformer layers\n(e.g., nlayers) with any continuous activation function. Given a compact set K ⊂Rd. Then, there exists\na compact set K′⊂Rd such that for every t∈Z>0,\nf\n(\nKt)\n⊆\n(\nK′)t. (24)\nNote. We make use of the following notations in the proof below:△t−1 = {y∈Rt : y≥0,1⊤y= 1}\ndenotes the (t−1)-dimensional simplex; Br(z) = {v ∈Rn : dist(z,v) < r}denotes the open ball\ncentered at zwith radius r; Adenotes the closure of set A.\nProof. Let K0 = K. In an autoregressive transformer, each of the nlayers consists of two blocks: a\nself-attention block and a feedforward block. We will use induction on the 2nblocks to build up compact\nsets K1,K2,...,K 2n that contain the output vectors of these respective blocks, and then take K′= K2n.\nThe self-attention block is a function on (Rd)+ →(Rd)+. So, let t∈Z>0 be arbitrary and consider\nany sequence of input vectors (v1,..., vt) such that for all i, vi ∈K0. Denote the output vectors of the\nattention block with (v′\n1,..., v′\nt). By deﬁnition of attention, each output vector v′\nj = ∑t\ni=1 α(j)\ni vi where\nα(j) ∈△t−1 are the attention weight vectors obtained through the softmax function. Compact sets in\nRd are bounded (by the Heine–Borel theorem), and hence there exists M >0 such that K0 ⊆BM(0).\nNoting that the norm function ∥·∥ on Rd is convex, we have the following\n∥v′\nj∥=\n\nt∑\ni=1\nα(j)\ni vi\n (62a)\n9766\n≤\nt∑\ni=1\nα(j)\ni ∥vj∥ (∗)\n≤\nt∑\ni=1\nα(j)\ni M = M (62b)\nwhere (∗) results from Jensen’s inequality. Eq. (62b) shows that each of the output vectors v′\nj lies in\nBM(0) which is compact. Hence, setting K1 = BM(0), we have shown that, for any t ∈Z>0, the\nattention block maps Kt\n0 into Kt\n1.\nNote that we cannot use Thm. 5.7 here because the attention block deﬁnes a different function on\nRt×d →Rt×d for each t, and Thm. 5.7 only implies that there exists a separate length-dependent output\ncompact set Kt ⊂Rt×d for each t, which is different from this lemma’s statement.\nThe feedforward function is a continuous function on Rd →Rd, and therefore, by Thm. 5.7, maps its\ninput compact set K1 to an output compact set, which we call K2.\nFinally, residual connections and layer norms are also continuous functions acting on each of the input\nvectors, and hence by the same reasoning would also preserve compactness.\nNow we can use induction and show that there exist compact setsK3,K4,...,K 2n−1,K2n where K2n\ncontains the output set of the ﬁnal layer. Set K′= K2n and we have proven the statement. ■\nTheorem 5.9. The autoregressive sequence model deﬁned by any (ﬁxed-depth) Transformer is tight.\nProof. Given the Transformer, there exists a ﬁxed compact set Kthat will contain all inputs vi ∈Rd to\nthe ﬁrst layer. This is true because each vi is the sum of a word embedding, which falls in a ﬁnite set\nsince Σ is ﬁnite, and a position embedding, which lies in the compact set [−1,1]d. Hence, by Lemma 5.8,\nthere exists a ﬁxed compact set K′that contains all output embedding vectors (regardless of how long the\nsequence is).\nThe ﬁnal output probability is given by a multiplication with the word embedding matrix followed\nby the softmax function as in Eq. (25). This process amounts to composing two continuous functions.\nIn particular, we can extract the EOS probability as a continuous R-valued function gEOS : K′ →\n(0,1) (neither 0 or 1 is in the range of the softmax function). By continuity of gEOS and Thm. 5.7,\nK′′ def\n= gEOS (K′) ⊆(0,1) is compact. Since K′′is compact, and hence closed, inf K′′ ∈K′′. Thus\ninf K′′∈(0,1) and in particular inf K′′>0. Therefore, taking ϵ= inf K′′, we have shown that the EOS\nprobability of a Transformer is bounded below by some ϵ> 0 (regardless of the length of the sequence).\nHence, by Prop. 4.3, any Transformer ASM is tight and thus deﬁnes a language model. ■\nE.3 Proofs for RNN Result (§5.3)\nProposition 5.10. Given an RNN ASM over Σ. Again let the output symbol vector be ux ∈Rd for x∈Σ,\nand set k\ndef\n= supx∈Σ ∥ux −uEOS ∥2. Additionally, for each t> 0, let ∥ˆht∥2 be the maximum attainable\nhidden state norm for any context x ∈Σt. Such a sequence model is tight if k∥ˆht∥2 ≤log tfor all\nsufﬁciently large t.\nProof. Let Xt(ω) be the random variable that is equal to the tth token in an outcome ω∈Ω. Also let hx\nbe the hidden representation of the RNN after processing some ﬁnite list of tokens x∈Σ∗. Further, let\nux ∈Rd be the output embedding of x∈Σ, Then for any t∈N and any x∈Σt, we have:\nP(Xt+1 = EOS |X≤t = x) = exp u⊤\nEOS hx∑\ny∈Σ exp u⊤y hx\n(63a)\n= 1∑\ny∈Σ exp u⊤y hx /exp u⊤\nEOS hx\n(63b)\n= 1\n1 + ∑\ny∈Σ exp(uy −uEOS )⊤hx\n(63c)\n9767\n≥ 1\n1 + ∑\ny∈Σ exp (∥uy −uEOS ∥2∥hx∥2) (Cauchy–Schwarz) (63d)\n≥ 1\n1 + ∑\ny∈Σ exp(k∥hx∥2) (63e)\n= 1\n1 + |Σ|exp(k∥hx∥2) (63f)\nNow deﬁne ∥ˆht∥2\ndef\n= supx∈Σt ∥hx∥2. We then have that ∀t∈N and ∀x∈Σt:\nP(Xt+1 = EOS |X≤t = x) ≥ 1\n1 + |Σ|exp(k∥ˆht∥2)\n(64)\nNow, by Prop. 4.3, we have that if∑∞\nt=0\n1\n1+|Σ|exp(k∥ˆht∥2) diverges, then the language model is tight.\nWe will show that this condition holds if ∃N ∈N such that ∀t≥N, k∥ˆht∥2 ≤log t.\nFirst, note that limt→∞1\nt\n1+|Σ|t\n1 = limt→∞1\nt + |Σ|= |Σ|∈ (0,∞). Hence, by the limit comparison\ntest, since ∑∞\nt=1\n1\nt diverges, this means ∑∞\nt=1\n1\n1+|Σ|t must also diverge.\nNow, suppose there exists N such that that k∥ˆht∥2 ≤log tfor all t≥N. This implies that for t≥N\nwe have 1\n1+|Σ|exp(k∥ˆht∥2) ≥ 1\n1+|Σ|t, which combined with the above and the comparison test, implies that\n∑∞\nt=N\n1\n1+|Σ|exp(k∥ˆht∥2) diverges. This in turn means that ∑∞\nt=0\n1\n1+|Σ|exp(k∥ˆht∥2) diverges.\nHence, if k∥ˆht∥2 ≤log tfor all sufﬁciently large t(that is, for all t≥N), then the RNN ASM is tight\nand thus deﬁnes a language model.\n■\n9768\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\n7\n□\u0017 A2. Did you discuss any potential risks of your work?\nOur paper provides theoretical analysis of language models. We do not expect potential risks as a\nresult of this work.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nNo response.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNo response.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNo response.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNo response.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNo response.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNo response.\nC □\u0017 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNo response.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n9769\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNo response.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNo response.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNo response.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n9770",
  "topic": "Measure (data warehouse)",
  "concepts": [
    {
      "name": "Measure (data warehouse)",
      "score": 0.6757961511611938
    },
    {
      "name": "Computer science",
      "score": 0.5649520754814148
    },
    {
      "name": "Computational linguistics",
      "score": 0.548856258392334
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.526785671710968
    },
    {
      "name": "Natural language processing",
      "score": 0.3999935984611511
    },
    {
      "name": "Linguistics",
      "score": 0.35208168625831604
    },
    {
      "name": "Philosophy",
      "score": 0.18528318405151367
    },
    {
      "name": "Data mining",
      "score": 0.1475248634815216
    },
    {
      "name": "Physics",
      "score": 0.08529368042945862
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    }
  ]
}