{
  "title": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-World Multi-Turn Dialogue",
  "url": "https://openalex.org/W4393153123",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2100915114",
      "name": "Songhua Yang",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2112640115",
      "name": "Han-jie Zhao",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5102543288",
      "name": "Senbin Zhu",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2129876892",
      "name": "Guangyu Zhou",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2108325679",
      "name": "Hongfei Xu",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2151214976",
      "name": "Yu-xiang Jia",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2401931038",
      "name": "Hongying Zan",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2100915114",
      "name": "Songhua Yang",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2112640115",
      "name": "Han-jie Zhao",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5102543288",
      "name": "Senbin Zhu",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2129876892",
      "name": "Guangyu Zhou",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2108325679",
      "name": "Hongfei Xu",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2151214976",
      "name": "Yu-xiang Jia",
      "affiliations": [
        "Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2401931038",
      "name": "Hongying Zan",
      "affiliations": [
        "Zhengzhou University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3124687886",
    "https://openalex.org/W4285294723",
    "https://openalex.org/W6853797351",
    "https://openalex.org/W4366327625",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6851960618",
    "https://openalex.org/W3025935268",
    "https://openalex.org/W4302305863",
    "https://openalex.org/W6741002519",
    "https://openalex.org/W4378765257",
    "https://openalex.org/W6840334356",
    "https://openalex.org/W4372283945",
    "https://openalex.org/W4366198844",
    "https://openalex.org/W6852754783",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4362598952",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4297676396",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W4361020491",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4387928881",
    "https://openalex.org/W4378499145",
    "https://openalex.org/W4381586770",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3162532547",
    "https://openalex.org/W4378770815",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4377009978",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4366327277",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4385572634",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4389520259"
  ],
  "abstract": "Recent advances in Large Language Models (LLMs) have achieved remarkable breakthroughs in understanding and responding to user intents. However, their performance lag behind general use cases in some expertise domains, such as Chinese medicine. Existing efforts to incorporate Chinese medicine into LLMs rely on Supervised Fine-Tuning (SFT) with single-turn and distilled dialogue data. These models lack the ability for doctor-like proactive inquiry and multi-turn comprehension and cannot align responses with experts' intentions. In this work, we introduce Zhongjing, the first Chinese medical LLaMA-based LLM that implements an entire training pipeline from continuous pre-training, SFT, to Reinforcement Learning from Human Feedback (RLHF). Additionally, we construct a Chinese multi-turn medical dialogue dataset of 70,000 authentic doctor-patient dialogues, CMtMedQA, which significantly enhances the model's capability for complex dialogue and proactive inquiry initiation. We also define a refined annotation rule and evaluation criteria given the unique characteristics of the biomedical domain. Extensive experimental results show that Zhongjing outperforms baselines in various capacities and matches the performance of ChatGPT in some abilities, despite the 100x parameters. Ablation studies also demonstrate the contributions of each component: pre-training enhances medical knowledge, and RLHF further improves instruction-following ability and safety. Our code, datasets, and models are available at https://github.com/SupritYoung/Zhongjing.",
  "full_text": "Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language\nModel through Expert Feedback and Real-World Multi-Turn Dialogue\nSonghua Yang*, Hanjie Zhao*, Senbin Zhu, Guangyu Zhou,\nHongfei Xu, Yuxiang Jia†, Hongying Zan\nZhengzhou University, Henan, China\n{suprit,hjzhao zzu,ygdzzx5156,zhougyzzu,hfxunlp}@foxmail.com, {ieyxjia, iehyzan}@zzu.edu.cn\nAbstract\nRecent advances in Large Language Models (LLMs) have\nachieved remarkable breakthroughs in understanding and re-\nsponding to user intents. However, their performance lag be-\nhind general use cases in some expertise domains, such as\nChinese medicine. Existing efforts to incorporate Chinese\nmedicine into LLMs rely on Supervised Fine-Tuning (SFT)\nwith single-turn and distilled dialogue data. These models\nlack the ability for doctor-like proactive inquiry and multi-\nturn comprehension and cannot align responses with experts’\nintentions. In this work, we introduce Zhongjing, the first\nChinese medical LLaMA-based LLM that implements an\nentire training pipeline from continuous pre-training, SFT,\nto Reinforcement Learning from Human Feedback (RLHF).\nAdditionally, we construct a Chinese multi-turn medical di-\nalogue dataset of 70,000 authentic doctor-patient dialogues,\nCMtMedQA, which significantly enhances the model’s capa-\nbility for complex dialogue and proactive inquiry initiation.\nWe also define a refined annotation rule and evaluation crite-\nria given the unique characteristics of the biomedical domain.\nExtensive experimental results show that Zhongjing outper-\nforms baselines in various capacities and matches the perfor-\nmance of ChatGPT in some abilities, despite the 100x pa-\nrameters. Ablation studies also demonstrate the contributions\nof each component: pre-training enhances medical knowl-\nedge, and RLHF further improves instruction-following abil-\nity and safety. Our code, datasets, and models are available at\nhttps://github.com/SupritYoung/Zhongjing.\nIntroduction\nRecently, significant progress has been made with LLMs,\nexemplified by ChatGPT (OpenAI 2022) and GPT-4 (Ope-\nnAI 2023), allowing them to understand and respond to var-\nious questions and even outperform humans in a range of\ngeneral areas. Although openai remains closed, Open-source\ncommunity swiftly launched high performing LLMs such as\nLLaMA (Touvron et al. 2023), Bloom (Scao et al. 2022),\nFalcon (Almazrouei et al. 2023) etc. To bridge the gap in\nChinese adaptability, researchers also introduced more pow-\nerful Chinese models (Cui, Yang, and Yao 2023a; Du et al.\n*These authors contributed equally.\n†Corresponding author\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n最近⼿⾜出现红斑并伴有脱⽪，\n已经持续了5个⽉了。这个问题\n是在我注射新冠疫苗后出现的...\n根据您描述的症状和病史，可\n能是由于新冠疫苗接种引起的\n过敏反应导致了⼿⾜红斑和脱\n⽪... 我需要了解⼀下您最近的\n⽤药情况以及过敏史\n我最近⼝服的药物主要是开瑞\n坦、⽩芍总苷和美能，⽬前仍\n在持续使⽤...\n注意到您最近开始接受奥⻢珠单抗治疗，\n并且停⽤了雷公藤，改为甲氨蝶呤。这些\n药物的使⽤可能对您的⽪肤病症状产⽣⼀\n定的影响....您可以考虑使⽤卡泊三醇软膏\n(达⼒⼠)进⾏外⽤治疗，每天使⽤两次。\n这种药膏在改善⽪肤红斑和脱⽪⽅⾯具有\n⼀定的效果。您对此有什么疑问吗？\n...\nFigure 1: An example of multi-turn Chinese medical consul-\ntation dialogues, relies heavily on LLM’s proactive inquiry.\n2022; Zhang et al. 2022). However, despite the stellar perfor-\nmance of these general LLMs across many tasks, their per-\nformance in specific professional fields, such as the biomed-\nical domain, is often limited due to a lack of domain ex-\npertise (Zhao et al. 2023). With its intricate and specialized\nknowledge, the biomedical domain demands high accuracy\nand safety for the successful development of LLMs (Singhal\net al. 2023a). Despite the challenges, medical LLMs hold\nenormous potential, offering value in diagnosis assistance,\nconsultations, drug recommendations, etc. In the realm of\nChinese medicine, some medical LLMs have been proposed\n(Li et al. 2023; Zhang et al. 2023; Xiong et al. 2023).\nHowever, these works are totally dependent on SFT. Han\net al. (2021) and Zhou et al. (2023) indicated that almost\nall knowledge is learned during pre-training, which is the\ncritical phase in accumulating knowledge, and RLHF can\nguide models to recognize their capability boundaries and\nenhance instruction-following ability (Ramamurthy et al.\n2022). Over-reliance on SFT may result in overconfident\ngeneralization, the model essentially rote-memorizes the an-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19368\nswers rather than understanding and reasoning the inherent\nknowledge. Moreover, previous dialogue datasets primarily\nfocus on single-turn dialogue, overlooking the process in au-\nthentic doctor-patient dialogues that usually need multi-turn\ninteractions and are led by doctors who will initiate inquiries\nfrequently to understand the condition.\nTo address these limitations, we propose Zhongjing1, the\nfirst Chinese medical LLM based on LLaMA, implement-\ning the entire pipeline from continuous pre-training, SFT to\nRLHF. Furthermore, we construct a Chinese multi-turn med-\nical dialogue dataset, CMtMedQA, based on real doctor-\npatient dialogues, comprising about 70,000 Q&A, covering\n14 departments. It also contains numerous proactive inquiry\nstatements to stimulate model. An example of multi-turn\nmedical dialogue is illustrated in Figure 1, only by relying\non frequent proactive inquiries can a more accurate medical\ndiagnosis be given.\nSpecifically, the construction of our model is divided into\nthree stages. First, we collect a large amount of real medi-\ncal corpus and conduct continuous pre-training based on the\nZiya-LLaMA model (Zhang et al. 2022), resulting in a base\nmodel with a medical foundation in the next SFT stage, in-\ntroducing four types of instruction datasets for training the\nmodel: single-turn medical dialogue data, multi-turn medi-\ncal dialogue data (CMtMedQA), natural language process-\ning task data, and general dialogue data. The aim is to en-\nhance the model’s generalization and understanding abili-\nties and to alleviate the problem of catastrophic forgetting\n(Aghajanyan et al. 2021). In the RLHF stage, we establish a\nset of detailed annotation rules and invited six medical ex-\nperts to rank 20,000 sentences produced by the model. These\nannotated data are used to train a reward model based on the\nprevious medical base model. Finally, we use the Proximal\nPolicy Optimization (PPO) algorithm (Schulman et al. 2017)\nto guide the model to align with the expert doctors’ intents.\nAfter extensive training and optimization, we successfully\ndeveloped Zhongjing, a robust Chinese medical LLM. Uti-\nlizing an extended version of previously proposed annota-\ntion rules (Wang et al. 2023a; Zhang et al. 2023), we eval-\nuated the performance of our model on three dimensions of\ncapability and nine specific abilities, using GPT-4 or human\nexperts. The experimental results show that our model sur-\npasses other open-source Chinese medical LLM in all capac-\nity dimensions. Due to the alignment at the RLHF stage, our\nmodel also makes a substantial improvement in safety and\nresponse length. Remarkably, it matched ChatGPT’s perfor-\nmance in some areas, despite having only 1% of its param-\neters. Moreover, the CMtMedQA dataset significantly bol-\nsters the model’s capability in dealing with complex multi-\nturn dialogue and initiating proactive inquiries.\nThe main contributions of this paper are as follows:\n• We develop a novel Chinese medical LLM,\nZhongjing. This is the first model to implement the full\npipeline training from pre-training, SFT, to RLHF.\n• We build CMtMedQA, a multi-turn medical dia-\nlogue dataset, based on 70,000 real instances from 14 med-\n1In homage to the renowned ancient Chinese medical scientist\nZhongjing Zhang, we named our model “Zhongjing”.\nical departments, including many proactive doctor inquiries.\n• We establish an improved annotation rule and as-\nsessment criteria for medical LLMs, customizing a stan-\ndard ranking annotation rule for medical dialogues, which\nwe apply to evaluation, spanning three capacity dimensions\nand nine distinct abilities.\n• We conduct multiple experiments on two benchmark\ntest datasets. Our model exceeds the previous top Chinese\nmedical model in all dimensions and matches ChatGPT in\nspecific fields.\nRelated Work\nLarge Language Models\nThe remarkable achievements of Large Language Models\n(LLMs) such as ChatGPT (OpenAI 2022) and GPT-4 (Ope-\nnAI 2023) have received substantial attention, sparking a\nnew wave in AI. Although OpenAI has not disclosed their\ntraining strategies or weights, the rapid emergence of open-\nsource LLMs like LLaMA (Touvron et al. 2023), Bloom\n(Scao et al. 2022), and Falcon (Almazrouei et al. 2023)\nhas captivated the research community. Despite their ini-\ntial limited Chinese proficiency, efforts to improve their\nChinese adaptability have been successful through train-\ning with large Chinese datasets. Chinese LLaMA and Chi-\nnese Alpaca (Cui, Yang, and Yao 2023b) continuously pre-\ntrained and optimized with Chinese data and vocabulary.\nZiya-LLaMA (Zhang et al. 2022) completed the RLHF pro-\ncess, enhancing instruction-following ability and safety. In\naddition, noteworthy attempts have been made to build pro-\nficient Chinese LLMs from scratch (Du et al. 2022; Sun et al.\n2023a).\nLLMs in Medical Domain\nLarge models generally perform suboptimally in the\nbiomedical field that require complex knowledge and high\naccuracy. Researchers have made significant progress, such\nas MedAlpaca (Han et al. 2023) and ChatDoctor (Yunxi-\nang et al. 2023), which employed continuous training, Med-\nPaLM (Singhal et al. 2023a), and Med-PaLM2 (Singhal\net al. 2023b), receiving favorable expert reviews for clinical\nresponses. In the Chinese medical domain, some efforts in-\nclude DoctorGLM (Xiong et al. 2023), which used extensive\nChinese medical dialogues and an external medical knowl-\nedge base, and BenTsao (Wang et al. 2023a), utilizing only a\nmedical knowledge graph for dialogue construction. Zhang\net al. (2023); Li et al. (2023) proposed HuatuoGPT and a\n26-million dialogue dataset, achieving better performance\nthrough a blend of distilled and real data for SFT and us-\ning ChatGPT for RLHF.\nMethods\nThis section explores the construction of Zhongjing, span-\nning three stages: continuous pre-training, SFT, and RLHF -\nwith the latter encompassing data annotation, reward model,\nand PPO. Each step is discussed sequentially to mirror the\nresearch workflow. The comprehensive method flowchart is\nshown in Figure 2.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19369\nOpen Source\nZiya-LLaMA Medical\nBase Model\nMedical\nBase Chat\nModel\nZhongjing\n-LLaMA\nPediatri\ncs KG\nObstetric\ns KG\nCMeKG\nCardiolo\ngy KG\nKnowledge Graph\nChatMed CMtMedQA\nQ&A\nHealth\nRecords\nMedical\nRecords\nClinical\nReports\nReal Medical Data\nContinuous Pre-training\nPretrain Datasets\nSupervised Fine Tuning\nPrivate Proposed\nChatMed CMCQA\nSingle-round Dialogue\nCMtMedQA\nMulti-round Dialogue\nMedical\nTextbook\nMedical\nWiki Data\nMedical Text\nMedical NLP Task\nCMeCC\nMRG\nNER CMeIE\nIR ...\nGeneral Dialogue\nInquiry Self-cognition\nKnowledge Medicine ...\nMedical Base\nDialogue Ability\nMedical Base\nDialogue Ability\nReliability\nSafety\nMedical Base\nDialogue Ability\nReliability\nSafety\nInstruction Datasets\nReward\nModel\nPPO\nHuman Ranking\nDataset\nReinforcement Learningfrom Human Feedback\nHow should herpes \nzoster be treated?\nExplain herpes \nzoster...\nHerpes zoster \nis caused by \nvirus..\n.\nPlease use\nantibiotic...\nUsing antiviral \nmedications...\n>\n =\n×\nSorry, I don’t know...>\n>\n = >\nMedical Base\nDialogue Ability\nFigure 2: The overall flowchart of constructing Zhongjing. Ticks, crosses, and question marks beneath the upper rectangles\nsignify the ability model currently possesses, lacks, or likely absents, respectively.\nContinuous Pre-training\nHigh-quality pre-training corpus can greatly improve the\nperformance of LLM and even break the scaling laws to\nsome extent (Gunasekar et al. 2023). Given the complex-\nity and wide range of the medical field, we emphasize both\ndiversity and quality. The medical field is full of knowledge\nand expertise, requires a thorough education similar to that\nof a qualified physician. Sole reliance on medical textbooks\nis insufficient as they only offer basic theoretical knowledge.\nIn real-world scenarios, understanding specific patient con-\nditions and informed decision-making requires medical ex-\nperience, professional insight, and even intuition.\nTo ensure the diversity of the medical corpus, we collect\na variety of real medical text data from multiple channels,\nincluding open-source data, proprietary data, and crawled\ndata, including medical textbooks, electronic health records,\nclinical diagnosis records, real medical consultation dia-\nlogues, and other types. These datasets span various depart-\nments and aspects within the medical domain, providing the\nmodel with a wealth of medical knowledge. The statistics\nof pre-training data are shown in Table 1. After corpus shuf-\nfling and pre-training based on Ziya-LLaMA, a base medical\nmodel was eventually obtained.\nConstruction of Multi-turn Dialogue Dataset\nDuring the construction of our Q&A data, we give spe-\ncial attention to the role of multi-turn dialogues. To en-\nsure the authenticity of the data, all dialogue data is sourced\nfrom real-world doctor-patient interactions. However, the re-\nsponses of real doctors are often very concise and in a dif-\nferent style. The direct use of these data for SFT may re-\nduce the fluency and completeness of the model responses.\nSome studies suggest that queries should be diverse enough\nto ensure the generalization and robustness of the model,\nwhile maintaining a uniform tone in responses (Wei et al.\n2022; Zhou et al. 2023). Therefore, we introduce the self-\ninstruct method (Wang et al. 2023c; Peng et al. 2023), nor-\nmalizing the doctor’s responses into a uniform, professional,\nand friendly response style, yet the original and diverse\nuser queries are preserved. Besides, some too overly con-\ncise single-turn dialogues are expanded into multi-turn di-\nalogue data. Subsequently, an external medical knowledge\ngraph CMeKG (Ao and Zan 2019) is used to check the ac-\ncuracy and safety of medical knowledge mentioned in the\ndialogue. We design a KG-Instruction collaborative filtering\nstrategy, which extracts the medical entity information from\nCMeKG and inserts them into an instruction to assist in fil-\ntering low-quality data. Both self-instruct methods are based\non GPT-3.5-turbo API. Finally, we construct a Chinese med-\nical multi-turn Q&A dataset, CMtMeQA, which contains\nabout 70,000 multi-turn dialogues and 400,000 conversa-\ntions. The distribution of the medical departments in the\ndataset is shown in Figure 3. It covers 14 medical depart-\nments and over 10 medical Q&A scenarios, such as disease\ndiagnosis, medication advice, health consultation, medical\nknowledge, etc. All data are subject to strict de-identification\nprocessing to protect patient’s privacy.\nSupervised Fine-Tuning\nSFT is the crucial stage in imparting the model with dialogue\ncapabilities. With high-quality doctor-patient dialogue data,\nthe model can effectively invoke the medical knowledge ac-\ncumulated during pre-training, thereby understanding and\nresponding to users’ queries. Relying excessively on dis-\ntilled data from GPT, tends to mimic their speech patterns,\nand may lead to a collapse of inherent capabilities rather\nthan learning substantive ones (Gudibande et al. 2023; Shu-\nmailov et al. 2023). Although substantial distilled data can\nrapidly enhance conversational fluency, medical accuracy is\nparamount. Hence, we avoid using solely distilled data. We\nemploy four types of data in the SFT stage:\nSingle-turn Medical Dialogue Data: Incorporating both\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19370\nDataset Type Department Size\nMedical Books Textbook Multiple 20MB\nChatMed Q&A Multiple 436MB\nCMtMedQA Q&A Multiple 158MB\nMedical Wiki Wiki Data Multiple 106MB\nCMeKG Knowledge Base Multiple 28MB\nPediatrics KG Knowledge Base Pediatrics 5MB\nObstetrics KG Knowledge Base Obstetrics 7MB\nCardiology KG Knowledge Base Cardiology 8MB\nHospital Data\nHealth Record Multiple 73MB\nClinical Report Multiple 140MB\nMedical Record Multiple 105MB\nTable 1: Medical pre-training data statistics and sources, all\ndata are from real medical scenarios.\nsingle and multi-turn medical data is effective. Zhou et al.\n(2023) demonstrated that a small amount of multi-turn dia-\nlogue data is sufficient for the model’s multi-turn capabili-\nties. Thus, we add more single-turn medical dialogue from\nZhu and Wang (2023) as a supplementary, and the final fine-\ntuning data ratio between single-turn and multi-turn data is\nabout 7:1.\nMulti-turn Medical Dialogue Data: CMtMedQA is the\nfirst large-scale multi-turn Chinese medical Q&A dataset\nsuitable for LLM training, which can significantly enhance\nthe model’s multi-turn Q&A capabilities. Covers 14 medical\ndepartments and 10+ scenarios, including numerous proac-\ntive inquiry statements, prompting the model to initiate med-\nical inquiries, an essential feature of medical dialogues.\nMedical NLP Task Instruction Data: Broad-ranges of\ntasks can improve the zero-shot generalization ability of the\nmodel (Sanh et al. 2022). To avoid overfitting to medical\ndialogue tasks, we include medical-related NLP task data\n(Zhu et al. 2023), all converted into an instruction dialogue\nformat, thus improving its generalization capacity.\nGeneral Medical-related Dialogue Data: To prevent\ncatastrophic forgetting of prior general dialogue abilities af-\nter incremental training (Aghajanyan et al. 2021), we also\ninclude some general dialogue or partially related to medical\ntopics. This not only mitigates forgetting but also enhances\nthe model’s understanding of the medical domain. These di-\nalogues also contain modifications relating to the model’s\nself-cognition.\nReinforcement Learning from Human Feedback\nAlthough pre-training and SFT accumulate medical knowl-\nedge and guide dialogue capabilities, the model may still\ngenerate untruthful, harmful, or unfriendly responses. In\nmedical dialogues, this can lead to serious consequences.\nWe utilize RLHF, a strategy aligned with human objects, to\nreduce such responses (Ouyang et al. 2022). As pioneers in\napplying RLHF in Chinese medical LLMs, we establish a\nrefined ranking annotation rule, train a reward model using\nFigure 3: Statistics on the distribution of medical depart-\nments in CMtMedQA.\n20,000 ranked sentences by six annotators, and align train-\ning through the PPO algorithm combined with the reward\nmodel.\nHuman Feedback for Medicine: Given the unique na-\nture of medical dialogues, we develop detailed ranking an-\nnotation rules inspired by (Li et al. 2023; Zhang et al. 2023).\nThe standard covers three dimensions of capacity: safety,\nprofessionalism, fluency, and nine specific abilities (Ta-\nble 2). Annotators assess model-generated dialogues across\nthese dimensions in descending priority. The annotation data\ncome from 10,000 random samples from the training set\nand an additional 10,000 data pieces, in order to train the\nmodel in both in-distribution and out-of-distribution scenar-\nios. Each dialogue is segmented into individual turns for\nseparate annotation, ensuring consistency and coherence. To\npromote the efficiency of annotation, we develop an simple-\nyet-efficient annotation platform.2 All annotators are medi-\ncal post-graduates or clinical physicians and are required to\nindependently rank the K answers generated by the model\nfor a question in a cross-annotation manner. If two anno-\ntators’ orders disagree, it will be decided by a third-party\nmedical expert.\nReinforcement Learning: Finally, we use the annotated\nranking data to train the reward model (RM). The RM takes\nthe medical base model as a starting point, leveraging its\nfoundational medical ability, while the model after the SFT,\nhaving learned excessive chat abilities, may cause interfer-\nence with the reward task. The RM adds a linear layer to\nthe original model, taking a dialogue pair(x, y) as input and\noutputs a scalar reward value reflecting the quality of the\ninput dialogue. The objective of RM is to minimize the fol-\nlowing loss function:\nL(θ) =− 1\n\u0000K\n2\n\u0001 E(x,yh,yl)∈D [log (σ (rθ(x, yh) − rθ(x, yl)))]\n2https://github.com/SupritYoung/RLHF-Label-Tool\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19371\nDimension Ability Explanation\nSafety\nAccuracy Must provide scientific, accurate medical knowledge, especially in scenarios such as disease diag-\nnosis, medication suggestions; must admit ignorance for unknown knowledge\nSafety Must ensure patient safety; must refuse to answer information or suggestions that may cause harm\nEthics Must adhere to medical ethics while respecting patient’s choices; refuse to answer if in violation\nProfessionalism\nComprehension Must accurately understand the patient’s questions and needs to provide relevant answers and sug-\ngestions\nClarity Must clearly and concisely explain complex medical knowledge so that patients can understand\nInitiative Must proactively inquire about the patient’s condition and related information when needed\nFluency\nCoherence Answers must be semantically coherent, without logical errors or irrelevant information\nConsistency Answers must be consistent in style and content, without contradictory information\nWarm Tone Answering style must maintain a friendly, enthusiastic attitude; cold or overly brief language is\nunacceptable\nTable 2: Medical question-answering ranking annotation criteria, divided into 3 capability dimensions and 9 specific abilities\nwith their explanations. The importance is ranked from high to low; if two abilities conflict, the more important is prioritized.\nwhere rθ denotes the reward model, and θ is generated\nparameter. E(x,yh,yl)∈D denotes the expectation over each\ntuple (x, yh, yl) in the manually sorted dataset D, where x\nis the input, and yh, yl are the outputs marked as “better”\nand “worse”.\nWe set the number of model outputs K = 4 and use\nthe trained RM to automatically evaluate the generated dia-\nlogues. We find that for some questions beyond the model’s\ncapability, all K responses generated by the model may\ncontain incorrect information, these incorrect answers will\nbe manually modified to responses like “I’m sorry, I don’t\nknow...” to improve the model’s awareness of its ability\nboundaries. For the reinforcement learning, we adopt the\nPPO algorithm (Schulman et al. 2017). PPO is an efficient\nreinforcement learning algorithm that can use the evaluation\nresults of the reward model to guide the model’s updates,\nthus further aligning the model with experts’ intentions.\nExperiments and Evaluation\nTraining Details\nOur model is based on Ziya-LLaMA-13B-v1 3, a general\nChinese LLM with 13 billion parameters, trained based\non LLaMA. Training is performed on 4 A100-80G GPUs\nusing parallelization, leveraging low-rank adaptation (lora)\nparameter-efficient tuning method (Hu et al. 2022) dur-\ning non-pretraining stages. This approach is implemented\nthrough the transformers 4 and peft 5 libraries. To balance\ntraining costs, we employ fp16 precision with ZeRO-2 (Ra-\njbhandari et al. 2020), gradient accumulation strategy, and\nlimit the length of a single response (including history) to\n4096. AdamW optimizer (Loshchilov and Hutter 2019), a\n3https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1\n4https://huggingface.co/docs/transformers/\n5https://github.com/huggingface/peft\n0.1 dropout, and a cosine learning rate scheduler are used.\nWe reserve 10% of the training set for validation, saving\nthe best checkpoints as the final model. To maintain training\nstability, we halve loss during gradient explosion and decay\nlearning rate. The final hyper-parameters for each stage, af-\nter multiple adjustments, are presented in Appendix 6. The\nlosses for all training stages successfully converged within\nan effective range.\nBaselines\nTo comprehensively evaluate our model, we select a series of\nLLMs with different parameter scales as baselines for com-\nparison, including general and medical LLMs.\nChatGPT (OpenAI 2022): A renowned LLM with ap-\nproximately 175B parameters. Although not specifically\ntrained for the medical field, it has shown impressive per-\nformance in medical dialogue tasks.\nZiya-LLaMA (Zhang et al. 2022): A fully trained Chi-\nnese general LLM, which also serves as the base model for\nours, is used to compare performance improvements.\nBenTsao (Wang et al. 2023a): A Chinese medical LLM\nbased on Chinese-LLaMA (Cui, Yang, and Yao 2023b), and\nfine-tuned on an 8k medical dialogue dataset.\nDoctorGLM (Xiong et al. 2023): A large-scale Chinese\nmedical model based on ChatGLM-6B (Du et al. 2022), fine-\ntuning on a large amount of medical instruction dataset.\nHuatuoGPT (Zhang et al. 2023): Previous best Chi-\nnese medical LLM implemented based on Bloomz-7b1-mt\n(Muennighoff et al. 2022). This model was fine-tuned on\nan extensive medical dialogue dataset (Li et al. 2023) using\nSFT and RLHF using GPT for feedback.\n6Refer to Table 3 (In Appendix): Prompt template with GPT-\n4 for evaluation. Our appendix is available at https://arxiv.org/abs/\n2308.03549v2\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19372\n49%\n100%75%50%25%0%\nBenTsao\nDoctorGLM\nZiya-LLaMA\nHuatuoGPT\nChatGPT\nOurs Win Tie Ours Loss\n24% 27%\n52% 26% 22%\n71% 10% 19%\n80% 12% 8%\n95% 4%1%\n(a) Evaluation results for professionalism and fluency.\n32%\n100%75%50%25%0%\nBenTsao\nDoctorGLM\nZiya-LLaMA\nHuatuoGPT\nChatGPT 29% 39%\n68% 13% 19%\n54% 16% 30%\n83% 7% 10%\n99% 1% (b) Evaluation results for safety.\nFigure 4: Experimental results on the CMtMedQA test dataset for multi-turn evaluation. All models are versions as of June 11.\n40%\n100%75%50%25%0%\nBenTsao\nDoctorGLM\nZiya-LLaMA\nHuatuoGPT\nChatGPT 27% 33%\n51% 20% 29%\n56% 13% 31%\n78% 9% 13%\n94% 4% 2%\n(a) Evaluation results for professionalism and fluency.\n26%\n100%75%50%25%0%\nBenTsao\nDoctorGLM\nZiya-LLaMA\nHuatuoGPT\nChatGPT 28% 46%\n65% 12% 23%\n44% 18% 38%\n81% 8% 11%\n97% 2%1% (b) Evaluation results for safety.\nFigure 5: Experimental results on Huatuo26M-test for single-turn evaluation, other settings are same as in Figure 4.\nEvaluation\nBenchmark Test Datasets We conduct experiments on\nthe CMtMedQA and huatuo-26M (Zhang et al. 2023) test\ndatasets, respectively, to evaluate the single-turn and multi-\nturn dialogue capabilities of the Chinese medical LLM.\nWhen building CMtMedQA, we set aside an additional 1000\nunseen dialogue data set during the training process as the\ntest set, CMtMedQA-test. To assess the safety of model, test\nset also contains 200 deliberately aggressive, ethical or in-\nductive medical-related queries. For the latter, huatuo26M-\ntest (Li et al. 2023) is a single-turn Chinese medical dialogue\ndataset containing 6000 questions and standard answers.\nEvaluation Metrics Evaluation of medical dialogue qual-\nity is a multifaceted task. We define a model evaluation\nstrategy including three-dimensional and nine-capacity, de-\nscribed in Table 2 to compare Zhongjing with various base-\nlines. For identical questions answered by different models,\nwe assess them on safety, professionalism, and fluency di-\nmensions, using win, tie, and loss rates of our model as met-\nrics. Evaluation integrates both human and AI components.\nDue to domain expertise (Wang et al. 2023b), only human\nexperts are responsible for evaluating the safety, ensuring\naccurate, safe, and ethical implications of all the medical en-\ntities or phrases mentioned. For simpler professionalism and\nfluency dimensions, we leverage GPT-4 (Zheng et al. 2023;\nChiang et al. 2023; Sun et al. 2023b) for scoring to conserve\nhuman resources. Given that these abilities are interrelated,\nwe evaluate professionalism and fluency together. Evalua-\ntion instruction templates are detailed in the Appendix.7\nResults\nThe experimental results on the two test sets are shown in\nFigures 4 and 5. The results indicate that Zhongjing achieves\nexcellent performance in both single-turn and multi-turn dia-\nlogues and across all three ability dimensions, surpassing the\nbaseline models in most cases. The following are our main\nobservations and conclusions from the experimental results:\nOur model surpasses the previous best model.\nZhongjing outperforms the previous best model, Hu-\natuoGPT, in all dimensions. Although it utilized a much\nlarger scale of fine-tuning instructions compared to our\nmodel. We attribute this primarily to the pre-training and\nRLHF stages, which instilled foundational knowledge and\nboundary awareness in the model.\nExceptional Multi-turn Dialogue Proficiency. The\namalgamation of professionalism and fluency, encapsulat-\n7See Table 4 (In Appendix): Parameter settings for each training\nphase\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19373\nSafety Prof. Fluency Length\n0%\n25%\n50%\n75%\n100%\n81%\n14%\n5%\n56%\n27%\n17%\n69%\n11%\n20%\n91%\n9%\nw/ RLHF/PT Win Tie w/o RLHF/PT Win\nSafety Prof. Fluency Length\n54%\n14%\n32%\n5%\n8%\n87%\n16%\n15%\n69% 66%\n34%\nFigure 6: The ablation experiment results (left: pre-training\n(PT), right: RLHF), w , w/o refer to the models with and\nwithout PT or RLHF.\ning the model’s multi-turn dialogue aptitude, signifies a\npivotal evaluation criterion. The results distinctly indicate\nZhongjing’s superior performance over all baselines except\nChatGPT, a feat attributable to the novel multi-turn dialogue\ndataset, CMtMedQA, that we meticulously curated.\nImportance of instruction scale. BenTsao, trained on 6k\ndatasets, performs the worst, indicating that the instruction\nscale remains a crucial factor for model capabilities.\nDistilled data lead to poor performance. Our model,\nsimilar to DoctorGLM in parameter size and instruction\nscale, significantly outperforms it. We believe this is mainly\nbecause DoctorGLM relies too heavily on distilled data ob-\ntained through the self-instruct method during training.\nCustomized training can significantly improve domain\ncapabilities. Comparison with the base model Ziya-LLaMA\nreveals that Zhongjing is significantly superior in medical\ncapabilities, reinforcing the effectiveness of targeted fine-\ntuning as a strategy to enhance domain abilities.\nThe scaling law still holds.Although our model achieves\nsome improvement in medical capabilities, it could only\nhold its ground against the ultra-large parameter model\nChatGPT in most cases, even falling behind in safety. This\nshows that parameter size continues to be a significant factor\nin model scale.\nAblation Study\nTo investigate the contribution of continuous pre-training\nand RLHF to the performance of Zhongjing, we conduct\na series of ablation experiments on the CMtMedQA test\ndataset. We adopt the evaluation strategy described in Ta-\nble 2 to compare the performance of Zhongjing with and\nwithout pre-training and RLHF. In addition to evaluating\nthe three main capability dimensions, safety, professional-\nism, and fluency, we also specifically focus on the change in\nresponse text length, a more intuitive metric of the amount\nof information. The Results in Figure 6, demonstrate that\nthe model has been enhanced in all capacities to different\nextents. As shown in Figure 6 (left), with the help of PT in\nthe medical corpus, Zhongjing achieves much better perfor-\nmance across all aspects, especially in “Professional”. This\nindicates the importance of CPT to incorporate more med-\nical knowledge. As for another, the improvements in safety\nand response length are the most significant, further demon-\nstrating that the RLHF phase can align medical LLM with\nmedical experts, reducing dangerous and toxic responses\nand improving the quality and information of the output. The\nimprovements in fluency and professionalism are relatively\nsmall, probably because the previous model already has high\nmedical performance. In summary, these ablation experi-\nments reveal the importance of PT and RLHF in the training\nof medical LLMs, providing valuable experience and guid-\nance for future research and applications in this field.\nCase Study\nIn the case study section, we select a challenging question\nthat not only involves multi-turn dialogue and proactive in-\nquiry, but also requires the model to have a deep understand-\ning of medical capabilities. The answers to the four base-\nline models are listed in the Appendix. 8 From the results,\nwe can observe that BenTsao’s output is too brief with lim-\nited information; DoctorGLM’s answer, though containing\nsome information, still offers limited help to the question;\nHuatuoGPT provides more detailed medical advice but in-\ncorrectly gives a diagnosis and medication recommendation\nwithout initiating an active inquiry. On the other hand, Chat-\nGPT’s output, although detailed and relatively safe, lacks\nthe diagnostic advice expected from a medical professional.\nIn contrast, Zhongjing’s response demonstrates a complete\ninquiry-answer process.\nThrough this example, the advantages of our model in\nhandling complex and deep questions become evident. Not\nonly accurately identifies potential causes (such as allergic\ndermatitis or drugeruption), but also provides specific ad-\nvice, such as stopping the use of medications that might\nexacerbate allergic reactions, switching to other anti-allergy\nmedications, etc. All of this fully showcases its professional\ncapabilities and practical value.\nConclusion and Limitations\nIn this work, we propose Zhongjing, the first comprehen-\nsive Chinese medical LLM that implements entire train-\ning pipelines from PT, SFT to RLHF, outperforming the\nbaseline LLMs, additional experiments highlight the signif-\nicance of PT and RLHF for medical field. We also construct\na large-scale Chinese multi-turn medical dialogue dataset,\nCMtMedQA. Despite these achievements, we recognize the\nlimitations of the model. Zhongjing cannot guarantee accu-\nracy in all its responses. Due to the serious consequences\nthat can arise from inaccurate data in medical field, we\nstrongly suggest that users exercise caution when dealing\nwith generated information and seek advice from experts.\nIn the future, we will focus on improving safety, integrat-\ning more real-world data, and incorporating multimodal in-\nformation for a more holistic and accurate medical service.\nErroneous medical suggestions and decisions can have seri-\nous consequences. How to eliminate the hallucination prob-\nlem in medical LLM, and how to further align with hu-\nman experts remains a problem worth studying. Despite this,\nZhongjing remains mainly a research tool rather than a re-\nplacement for professional medical consultation.\n8See Appendix: Table 5 and Table 6\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19374\nAcknowledgments\nThe authors thank anonymous reviewers for their insightful\ncomments. This work is mainly supported by the Key Pro-\ngram of the Natural Science Foundation of China (NSFC)\n(Grant No. U23A20316). Hongfei Xu acknowledges the\nsupport of the National Natural Science Foundation of China\n(Grant No. 62306284), China Postdoctoral Science Founda-\ntion (2023M743189), and the Natural Science Foundation of\nHenan Province (Grant No. 232300421386).\nReferences\nAghajanyan, A.; Gupta, A.; Shrivastava, A.; Chen, X.;\nZettlemoyer, L.; and Gupta, S. 2021. Muppet: Massive\nMulti-task Representations with Pre-Finetuning. In Pro-\nceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, 5799–5811. Online and\nPunta Cana, Dominican Republic: Association for Compu-\ntational Linguistics.\nAlmazrouei, E.; Alobeidli, H.; Alshamsi, A.; Cappelli, A.;\nCojocaru, R.; Debbah, M.; Goffinet, E.; Heslow, D.; Lau-\nnay, J.; Malartic, Q.; et al. 2023. Falcon-40B: an open large\nlanguage model with state-of-the-art performance.\nAo, Y .; and Zan. 2019. Preliminary Study on the Construc-\ntion of Chinese Medical Knowledge Graph. Journal of Chi-\nnese Information Processing, 33(10): 9.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; et al.\n2023. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023).\nCui, Y .; Yang, Z.; and Yao, X. 2023a. Efficient and Ef-\nfective Text Encoding for Chinese LLaMA and Alpaca.\narXiv:2304.08177.\nCui, Y .; Yang, Z.; and Yao, X. 2023b. Efficient and Effec-\ntive Text Encoding for Chinese LLaMA and Alpaca. arXiv\npreprint arXiv:2304.08177.\nDu, Z.; Qian, Y .; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and\nTang, J. 2022. GLM: General Language Model Pretraining\nwith Autoregressive Blank Infilling. In Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), 320–335. Dublin, Ire-\nland: Association for Computational Linguistics.\nGudibande, A.; Wallace, E.; Snell, C.; Geng, X.; Liu, H.;\nAbbeel, P.; Levine, S.; and Song, D. 2023. The false\npromise of imitating proprietary llms. ArXiv preprint,\nabs/2305.15717.\nGunasekar, S.; Zhang, Y .; Aneja, J.; Mendes, C. C. T.;\nDel Giorno, A.; Gopi, S.; Javaheripi, M.; Kauffmann, P.;\nde Rosa, G.; Saarikivi, O.; et al. 2023. Textbooks Are All\nYou Need. ArXiv preprint, abs/2306.11644.\nHan, T.; Adams, L. C.; Papaioannou, J.-M.; Grundmann, P.;\nOberhauser, T.; L ¨oser, A.; Truhn, D.; and Bressem, K. K.\n2023. MedAlpaca–An Open-Source Collection of Medi-\ncal Conversational AI Models and Training Data. ArXiv\npreprint, abs/2304.08247.\nHan, X.; Zhang, Z.; Ding, N.; Gu, Y .; Liu, X.; Huo, Y .; Qiu,\nJ.; Yao, Y .; Zhang, A.; Zhang, L.; et al. 2021. Pre-trained\nmodels: Past, present and future. AI Open, 2: 225–250.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2022. LoRA: Low-Rank Adapta-\ntion of Large Language Models. In The Tenth International\nConference on Learning Representations, ICLR 2022, Vir-\ntual Event, April 25-29, 2022. OpenReview.net.\nLi, J.; Wang, X.; Wu, X.; Zhang, Z.; Xu, X.; Fu, J.; Ti-\nwari, P.; Wan, X.; and Wang, B. 2023. Huatuo-26M, a\nLarge-scale Chinese Medical QA Dataset. arXiv preprint\narXiv:2305.01526.\nLoshchilov, I.; and Hutter, F. 2019. Decoupled Weight De-\ncay Regularization. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nMuennighoff, N.; Wang, T.; Sutawika, L.; Roberts, A.;\nBiderman, S.; Scao, T. L.; Bari, M. S.; Shen, S.; Yong,\nZ.-X.; Schoelkopf, H.; et al. 2022. Crosslingual gen-\neralization through multitask finetuning. ArXiv preprint,\nabs/2211.01786.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nOpenAI, T. 2022. Chatgpt: Optimizing language models for\ndialogue. OpenAI.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nPeng, B.; Li, C.; He, P.; Galley, M.; and Gao, J. 2023. In-\nstruction tuning with gpt-4. ArXiv preprint, abs/2304.03277.\nRajbhandari, S.; Rasley, J.; Ruwase, O.; and He, Y . 2020.\nZero: Memory optimizations toward training trillion param-\neter models. In SC20: International Conference for High\nPerformance Computing, Networking, Storage and Analysis,\n1–16. IEEE.\nRamamurthy, R.; Ammanabrolu, P.; Brantley, K.; Hessel,\nJ.; Sifa, R.; Bauckhage, C.; Hajishirzi, H.; and Choi, Y .\n2022. Is reinforcement learning (not) for natural language\nprocessing?: Benchmarks, baselines, and building blocks\nfor natural language policy optimization. ArXiv preprint,\nabs/2210.01241.\nSanh, V .; Webson, A.; Raffel, C.; Bach, S. H.; Sutawika,\nL.; Alyafeai, Z.; Chaffin, A.; Stiegler, A.; Le Scao, T.; Raja,\nA.; et al. 2022. Multitask Prompted Training Enables Zero-\nShot Task Generalization. InICLR 2022-Tenth International\nConference on Learning Representations. OpenReview.net.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; Castagn´e, R.; Luccioni, A. S.; Yvon, F.; Gall´e, M.; et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. ArXiv preprint, abs/2211.05100.\nSchulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and\nKlimov, O. 2017. Proximal policy optimization algorithms.\nArXiv preprint, abs/1707.06347.\nShumailov, I.; Shumaylov, Z.; Zhao, Y .; Gal, Y .; Paper-\nnot, N.; and Anderson, R. 2023. The Curse of Recursion:\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19375\nTraining on Generated Data Makes Models Forget. ArXiv\npreprint, abs/2305.17493.\nSinghal, K.; Azizi, S.; Tu, T.; Mahdavi, S. S.; Wei, J.; Chung,\nH. W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.;\net al. 2023a. Large language models encode clinical knowl-\nedge. Nature, 1–9.\nSinghal, K.; Tu, T.; Gottweis, J.; Sayres, R.; Wulczyn,\nE.; Hou, L.; Clark, K.; Pfohl, S.; Cole-Lewis, H.; Neal,\nD.; et al. 2023b. Towards expert-level medical question\nanswering with large language models. ArXiv preprint,\nabs/2305.09617.\nSun, T.; Zhang, X.; He, Z.; Li, P.; Cheng, Q.; Yan, H.; Liu,\nX.; Shao, Y .; Tang, Q.; Zhao, X.; Chen, K.; Zheng, Y .; Zhou,\nZ.; Li, R.; Zhan, J.; Zhou, Y .; Li, L.; Yang, X.; Wu, L.; Yin,\nZ.; Huang, X.; and Qiu, X. 2023a. MOSS: Training Conver-\nsational Language Models from Synthetic Data.\nSun, Z.; Shen, Y .; Zhou, Q.; Zhang, H.; Chen, Z.; Cox,\nD.; Yang, Y .; and Gan, C. 2023b. Principle-driven self-\nalignment of language models from scratch with minimal\nhuman supervision. ArXiv preprint, abs/2305.03047.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. ArXiv preprint, abs/2302.13971.\nWang, H.; Liu, C.; Xi, N.; Qiang, Z.; Zhao, S.; Qin, B.; and\nLiu, T. 2023a. Huatuo: Tuning llama model with chinese\nmedical knowledge. ArXiv preprint, abs/2304.06975.\nWang, P.; Li, L.; Chen, L.; Zhu, D.; Lin, B.; Cao, Y .; Liu, Q.;\nLiu, T.; and Sui, Z. 2023b. Large language models are not\nfair evaluators. ArXiv preprint, abs/2305.17926.\nWang, Y .; Kordi, Y .; Mishra, S.; Liu, A.; Smith, N. A.;\nKhashabi, D.; and Hajishirzi, H. 2023c. Self-Instruct: Align-\ning Language Models with Self-Generated Instructions. In\nProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\n13484–13508. Toronto, Canada: Association for Computa-\ntional Linguistics.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2022. Finetuned Lan-\nguage Models are Zero-Shot Learners. InThe Tenth Interna-\ntional Conference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nXiong, H.; Wang, S.; Zhu, Y .; Zhao, Z.; Liu, Y .; Wang,\nQ.; and Shen, D. 2023. Doctorglm: Fine-tuning your\nchinese doctor is not a herculean task. ArXiv preprint,\nabs/2304.01097.\nYunxiang, L.; Zihan, L.; Kai, Z.; Ruilong, D.; and You, Z.\n2023. Chatdoctor: A medical chat model fine-tuned on llama\nmodel using medical domain knowledge. ArXiv preprint,\nabs/2303.14070.\nZhang, H.; Chen, J.; Jiang, F.; Yu, F.; Chen, Z.; Li, J.; Chen,\nG.; Wu, X.; Zhang, Z.; Xiao, Q.; et al. 2023. HuatuoGPT,\ntowards Taming Language Model to Be a Doctor. ArXiv\npreprint, abs/2305.15075.\nZhang, J.; Gan, R.; Wang, J.; Zhang, Y .; Zhang, L.; Yang,\nP.; Gao, X.; Wu, Z.; Dong, X.; He, J.; Zhuo, J.; Yang, Q.;\nHuang, Y .; Li, X.; Wu, Y .; Lu, J.; Zhu, X.; Chen, W.; Han, T.;\nPan, K.; Wang, R.; Wang, H.; Wu, X.; Zeng, Z.; and Chen, C.\n2022. Fengshenbang 1.0: Being the Foundation of Chinese\nCognitive Intelligence. CoRR, abs/2209.02970.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou,\nY .; Min, Y .; Zhang, B.; Zhang, J.; Dong, Z.; et al. 2023.\nA survey of large language models. ArXiv preprint,\nabs/2303.18223.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;\nZhuang, Y .; Lin, Z.; Li, Z.; Li, D.; Xing, E.; et al. 2023. Judg-\ning LLM-as-a-judge with MT-Bench and Chatbot Arena.\nArXiv preprint, abs/2306.05685.\nZhou, C.; Liu, P.; Xu, P.; Iyer, S.; Sun, J.; Mao, Y .; Ma, X.;\nEfrat, A.; Yu, P.; Yu, L.; et al. 2023. Lima: Less is more for\nalignment. arXiv preprint arXiv:2305.1120.\nZhu, W.; and Wang, X. 2023. ChatMed: A Chinese Medical\nLarge Language Model. https://github.com/michael-wzhu/\nChatMed.\nZhu, W.; Wang, X.; Zheng, H.; Chen, M.; and Tang, B. 2023.\nPromptCBLUE: A Chinese Prompt Tuning Benchmark for\nthe Medical Domain. arXiv preprint arXiv:2310.14151.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19376",
  "topic": "Turn-taking",
  "concepts": [
    {
      "name": "Turn-taking",
      "score": 0.4901193082332611
    },
    {
      "name": "Computer science",
      "score": 0.46323129534721375
    },
    {
      "name": "Turn (biochemistry)",
      "score": 0.4157855808734894
    },
    {
      "name": "Psychology",
      "score": 0.25034666061401367
    },
    {
      "name": "Communication",
      "score": 0.1510198414325714
    },
    {
      "name": "Chemistry",
      "score": 0.08417674899101257
    },
    {
      "name": "Conversation",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I38877650",
      "name": "Zhengzhou University",
      "country": "CN"
    }
  ]
}