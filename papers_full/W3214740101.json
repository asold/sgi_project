{
  "title": "Generative Chemical Transformer: Neural Machine Learning of Molecular Geometric Structures from Chemical Language via Attention",
  "url": "https://openalex.org/W3214740101",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2562158691",
      "name": "Kim Hyunseung",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2555671000",
      "name": "Na, Jonggeol",
      "affiliations": [
        "Ewha Womans University"
      ]
    },
    {
      "id": "https://openalex.org/A2753751221",
      "name": "Lee, Won Bo",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2110791536",
    "https://openalex.org/W1508604947",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2038702914",
    "https://openalex.org/W1999638776",
    "https://openalex.org/W2989615256",
    "https://openalex.org/W3043096321",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2901476322",
    "https://openalex.org/W3043969542",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2963445908",
    "https://openalex.org/W2991736596",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W2985931096",
    "https://openalex.org/W2956961449",
    "https://openalex.org/W2786308452",
    "https://openalex.org/W2910636130",
    "https://openalex.org/W2763220183",
    "https://openalex.org/W2747592475",
    "https://openalex.org/W2995724889",
    "https://openalex.org/W3025593963",
    "https://openalex.org/W4253105429",
    "https://openalex.org/W3116865743",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W1757990252",
    "https://openalex.org/W2887447356",
    "https://openalex.org/W1988037271",
    "https://openalex.org/W2022476850",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2567534979",
    "https://openalex.org/W3160789623",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W4233151585",
    "https://openalex.org/W2910135751",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4391602018",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3193917007",
    "https://openalex.org/W2188365844",
    "https://openalex.org/W4240795200",
    "https://openalex.org/W4241677786",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2951670304",
    "https://openalex.org/W4232186742",
    "https://openalex.org/W2786722833",
    "https://openalex.org/W2977044154",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W4242836807",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2769836604",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Discovering new materials better suited to specific purposes is an important issue in improving the quality of human life. Here, a neural network that creates molecules that meet some desired multiple target conditions based on a deep understanding of chemical language is proposed (generative chemical Transformer, GCT). The attention mechanism in GCT allows a deeper understanding of molecular structures beyond the limitations of chemical language itself which cause semantic discontinuity by paying attention to characters sparsely. The significance of language models for inverse molecular design problems is investigated by quantitatively evaluating the quality of the generated molecules. GCT generates highly realistic chemical strings that satisfy both chemical and linguistic grammar rules. Molecules parsed from the generated strings simultaneously satisfy the multiple target properties and vary for a single condition set. These advances will contribute to improving the quality of human life by accelerating the process of desired material discovery.",
  "full_text": "1 \n \n \n \nGenerative Chemical Transformer: Neural Machine Learning of \nMolecular Geometric Structures from Chemical Language via \nAttention \n \nHyunseung Kimâ€ , Jonggeol Naâ€¡,*, Won Bo Leeâ€ ,* \n \nâ€ School of Chemical and Biological Engineering, Seoul National University, Gwanak-ro 1, Gwanak-\ngu, Seoul 08826, Republic of Korea \nâ€¡Department of Chemical Engineering and Materials Science, Graduate Program in System Health \nScience and Engineering, Ewha Womans University, Seoul 03760, Republic of Korea \n \nCorrespondence and requests for materials should be addressed to \nJ.N. (email: jgna@ewha.ac.kr) or W.B.L. (email: wblee@snu.ac.kr). \n  \n2 \n \nABSTRACT: \nDiscovering new materials better suited to specific purposes is an important issue in improving \nthe quality of human life . Here, a neural network that creates molecules that meet some desired \nconditions based on a deep understanding of chemical language is proposed (Generative Chemical \nTransformer, GCT). The attention mechanism in GCT allows a deeper understanding of molecular \nstructures beyond the limitations of chemical language itself  which cause semantic discontinuity  by \npaying attention to characters sparsely. It is investigated that the significance of language models for \ninverse molecular design problems by quantitatively evaluating the quality of the generated molecules. \nGCT generates highly realistic chemical strings that satisfy both chemical and linguistic grammar rules. \nMolecules parsed from generated strings simultaneously satisfy the multiple target properties and vary \nfor a single condition set.  These advances will contribute to improving the quality of human life by \naccelerating the process of desired material discovery. \n  \n3 \n \nâ–  INTRODUCTION \nMaterial discovery is a research field that searches for new materials that fit specific purposes. \nSearching molecular  structures that simultaneously satisfy the multiple desired target  conditions \nrequires a high level of expert knowledge and considerable time and cost, since the chemical space is \nvast; the number of organic molecules less than 500 Da exceeds  1060.1 For this reason, inferring the \ndesired molecules using artificial intelligence with transcendental learning can help to accelerate the \nprocess of material discovery. \nIn recent years, attempts have been made to utilize artificial neural networks in chemistry fields \nwhere the relationship between molecular structure and physical properties is complex. To learn the \nmolecular structures in neural nets, the molecular structure must be expressed as structured data. \nMolecules are made up of atoms and connecting  bonds, so they are similar to  graphs. However, \ntechniques for expressing molecular structures in the form of strings also have been studied to facilitate \nthe construction and utilization of molecular information as a database, since the form of string data is \nmore convenient to handle than the form of graph data.2-6 The Simplified Molecular-Input Line-Entry \nSystem (SMILES)4-6 developed in the 1980s  is the most popular artificial language used to express \nmolecular structure s in detail. Similar to natural language, SMILES also has an arrangement of \ncharacters according to grammar and context. \nSeveral approaches have demonstrated that Natural Language Processing (NLP) models are \napplicable to inverse molecular design problems by generating molecules expressed in chemical \nlanguage via a language model  to select a character that follows the currently generated string, 7-11 a \nlanguage model combined with a Variational Autoencoder (V AE)12 to compress molecular information \ninto latent space and re-sample the latent code to create various strings,13,14 a language model combined \nwith a Generative Adversarial Network (GAN)15 to create strings from noise, 16 or a language model \ncombined with reinforcement learning to reward a natural character that follows the currently \ngenerated string.17 However, why language model-based molecular generators work well has not been \n4 \n \ncomprehensively analyzed. \nAn important point in material discovery is to search the molecular structures that meet multiple \ndesired target conditions. The desired molecules can be discovered in 2 steps by applying additional \noptimization or navigation process to the generative model: Bayesian optimization,13,18 particle swarm \noptimization,19 genetic optimization,20-22 or Monte Carlo tree search .23-25 Another method is to use \nconditional models which create molecules with the given target conditions in 1  step. The latter can \nshorten the time consumed to discover the desired molecules and directly control the molecules to be \ngenerated. For this reason, attempts have been made to directly input conditions to a recurrent neural \nnetwork ( Conditional Recurrent Neural Network, cRNN26). Unlike generative models, since a \nRecurrent Neural Network (RNN) is  based on one -to-one matching of input and output , there is a \nlimitation in using it to infer the various molecular candidates with a single condition set; here, the \ngenerative models refer to models that can output various results by decoding latent codes sampled \nfrom the distribution of latent variables (latent space). \nHere, Generative Chemical Transformer (GCT), which embeds Transformer27â€”a state-of-the-art \narchitecture that became a breakthrough for NLP problems by using an attention mechanism28â€”into a \nconditional variational generative model is proposed. From the point of view of data recognition and \nprocessing, GCT is close to a conditional variational generator that embodies the language recognition \nability of the attention mechanism in Transformer. To use GCT for material discovery, it is intended to \ntake advantage of both the high -performance language model and the conditional generative model. \nGCT is analyzed by quantitatively evaluating the generated molecules and investigate the significance \nof language models for inverse molecular design problems. It is shown that a deep understanding of \nthe molecular geometric structure learned from chemical language by paying attention to each \ncharacter in chemical strings sparsely helps to generate chemical strings satisfying the chemical \nvalance rule and syntax of the chemical language (grammar understanding). The strings are parsed into \nhighly realistic molecules (context understanding). Additionally, it is demonstrated that the conditional \n5 \n \nvariational generator, which is the skeleton of GCT, helps to generate molecules that satisfy multiple \npreconditions simultaneously (conditional generator) and varies for a single precondition set \n(variational generator). In addition, the autoencoder, a substructure of GCT, makes the molecular size \ncontrollable. \n \nâ–  MATERIALS AND METHODS \n \n \nFigure 1. Structural limitation of language forms. (a) An example of a non -Hamiltonian graph. A Hamiltonian \ngraph has a path that passes through all the points in the graph only once, and a non -Hamiltonian graph does \nnot have such a path . (b) An example of a non -Hamiltonian molecular graph and its SMILES string: 4 -(2-\naminopropyl)-2-methoxyphenol. Each atom is labelled with a circled number.  Different colors refer to different \nbranches. (c) In natural language, words that are semantically close within a sentence are not always structurally \n\n6 \n \nclose within a sentence. \nOvercoming Structural Limits of Language via Attention. \nIt is expected that introducing an attention mechanism to language-based inverse molecular design \nproblems can help networks understand the geometric structures of molecules beyond the limitations \nof chemical language itself. SMILES, a chemical language, represents molecules as one-dimensional \ntext. It is a powerful language since a one -SMILES string is converted into one exact molecule . \nUnfortunately, however, since most molecules are non -Hamiltonian graphs, it is self -evident that \nsemantic discontinuity occurs whenever a branch in a molecule is translated into a one -dimensional \nstring (Figure 1a). SMILES distinguishes each branch with open and close parentheses and creates a \ngap between the distance of two characters within the string and the distance of the corresponding two \natoms in the molecular graph. An example is shown in Figure 1b. Even though atom2 and atom13 in \nFigure 1b are geometrically adjacent, they are far from each other  in the string . In other words, the \nlonger the branch is, the harder it is to imagine the molecular structure by reading the chemical string \nin order (similar to memory cells). A similar phenomenon occurs in natural language (Figure 1c). The \nlonger the sentence is, the larger the gap between the words within the sentence and the semantic \nsimilarity. Unfortunately, in chemical language, there are more areas where semantic discontinuity \noccurs. In the field of NLP, by introducing an attention mechanism to this problem, language models \nare able to pay sparse attention to semantically related parts; it is a departure from the traditional way \nof perceiving context in the order of sentences (memory cells). Transformer is an architecture involving \nan attention mechanism in the form of a neural net work, and it became a breakthrough in cognitive \nability. This study was started with the expectation that the attention mechanism could help structural \nunderstanding beyond the semantic discontinuity of chemical language. The results and discussion for \nthis problem will be covered in a later section. \n \nSMILES Language. \n7 \n \nSMILES expresses the atoms, bonds, and rings that make u p molecules as a string. An atom is \nrepresented by the alphabet of the element symbol, and a bond is represented by a single bond ( -), \ndouble bonds (=), and triple bonds (#), depending on the type. In general, a bond that can be easily \ninferred through the atoms or ring structure of the surrounding atom is omitted. The notation of \nhydrogen is also omitted in SMILES string if single bond hydrogen can be explicitly inferred by the \nchemical valence rule, however, single bond hydrogen can be clearly indicated by using [H] if the bond \nis implicit. For charged atoms, where the number of hydrogen bonds cannot be determined explicitly, \natoms and formal charges are written together in brackets [ ]. The beginning and end of each ring are \nexpressed with the same digit, and the pair must be correct; if a ring is open, it must be closed. The \natoms present in the  aromatic ring are written in lowercase, while the atoms outside  the ring  are \ncapitalized. The branches in molecules are indicated by opening and closing parentheses (see Figure \n1b). A more detailed description of SMILES is in the ref. 4-6. \n \nTokenization and Token Embedding. \nTo input SMILES string to language models, the process of tokenizing by semantic units is necessary. \nThe SmilesPE29 tokenizer was used to tokenize the SMILES strings included in the training data of \nMolecular Sets benchmarking platforms  (MOSES).30 In total, 28 types of tokens are used: 4 special \ntokens (<unknown>, <pad>, <sos>, and <eos>), 13 atom tokens (<C>, <c>, <O>, <o>, <N>, <n>, <F>, \n<S>, <s>, <Cl>, <Br>, <[nH]>, and <[H]>), 3 bond tokens (<->, <=>, and <#>), 2 branch tokens (<(> \nand <)>), and 6 ring tokens (<1>, <2>, <3>, <4>, <5>, and <6>). Note that tokens related to charged \natom (e.g. <[O-]>, <[n+]>) and tokens related to stereochemistry (e.g. </> , < \\>) were not considered \nas they are not covered by MOSES database. Each token that constitutes the SMILES string is one-hot \nencoded in 28 dimensions and embedded in 512 dimensions. The condition of GCT is also embedded \nin 512 dimensions. \n \n8 \n \nAttention Mechanism. \nThe attention mechanism is the core of Transformer's language cogniti on abilities. The attention \nmechanism allows Transformer to self -learn which token of the input string is better to focus on to \nperform a given task better. The attention mechanism uses three vectors: the query ğ‘„, the key ğ¾, and \nthe value ğ‘‰. The attention mechanism calculates the similarity between Q and all keys in K, and the \ncalculated similarity is multiplied by the value corresponding to the key to calculate the attention scores. \nThe scale-dot attention used in the Transformer is the same as eq 1 27: \nAttention(ğ‘„,ğ¾,ğ‘‰)=Softmax(ğ‘„ğ¾ğ‘‡\nâˆšğ‘‘ğ‘˜\n)ğ‘‰                                                                 (1) \nwhere ğ‘‘ğ‘˜ is the dimension of ğ¾ and ğ‘‘ğ‘˜ must correspond to the dimension ğ‘‘ğ‘ of ğ‘„. \nThe Transformer uses multi-head attention instead of single-head attention (eq 2)27: \nMultiHead(ğ‘„,ğ¾,ğ‘‰)=Concat(â„ğ‘’ğ‘ğ‘‘1,â€¦,â„ğ‘’ğ‘ğ‘‘â„)ğ‘Šğ‘‚                           (2) \nğ‘¤â„ğ‘’ğ‘Ÿğ‘’ â„ğ‘’ğ‘ğ‘‘ğ‘– =Attention(ğ‘„ğ‘Šğ‘–\nğ‘„,ğ¾ğ‘Šğ‘–\nğ¾,ğ‘‰ğ‘Šğ‘–\nğ‘‰) \nWhere ğ‘Šğ‘–\nğ‘„ âˆˆâ„ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—ğ‘‘ğ‘˜, ğ‘Šğ‘–\nğ¾ âˆˆâ„ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—ğ‘‘ğ‘˜, ğ‘Šğ‘–\nğ‘‰ âˆˆâ„ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™Ã—ğ‘‘ğ‘˜,ğ‘Šğ‘–\nğ‘‚ âˆˆâ„â„ğ‘‘ğ‘£Ã—ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™  and \nğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™ =ğ‘‘ğ‘˜ Ã—â„=ğ‘‘ğ‘£ Ã—â„ . Here, â„  is the number of multi -head and ğ‘‘ğ‘£  is the dimension of ğ‘‰ . \nEach head (â„ğ‘’ğ‘ğ‘‘ğ‘–) calculates an attention score between ğ‘„ and  ğ¾ from different viewpoints using \nthe different weights belonging to each head (ğ‘Šğ‘–\nğ‘„, ğ‘Šğ‘–\nğ¾, and ğ‘Šğ‘–\nğ‘‰). \n \nGenerative Chemical Transformer. \nWe designed GCT, an architecture that embeds Transformerâ€”one of the most advanced language \nmodelsâ€”into a Conditional Variational Autoencoder (cV AE),31 which creates molecules with target \nproperties based on a deep understanding of chemical language. Transformer, the core of GCT's \nlanguage recognition ability, is mainly used as a Neural Machine Translator (NMT). It consists of an \n9 \n \nencoder and a decoder (Figure 2a). The encoder receives a sentence to be translated and understands \nthe received sentence through self -attention. Then, the processed information from sentence \ncomprehension is passed to the decoder. The decoder iteratively selects the next token that will follow \nthe translated sentence up to this point, referring to the information received from the encoder and the \nsentences translated up to the previous step; if there is no translated sentence at the beginning of \ntranslation, the special token â€˜start of sentence <sos>â€™ is used. The decoder uses the input information \nto iteratively select the next token that will follow the translated sentence up to the previous step. \nFinally, the translation ends when the decoder selects a special token 'end of sentence <eos>'. \nGCT is a structure that inserts a low -dimensional conditional Gaussian latent space between the \nencoder and the decoder of the Pre-Layer Normalization (Pre-LN) Transformer.32 (Figure 2b). Pre-\nLN Transformer is a modified version of the original (Post-Layer Normalization, Post -LN) \nTransformer. The combination of language models and variational autoencoders is vulnerable to \nposterior collapse.33 A complete solution to posterior collapse has yet to be identified ; however, it is \nknown that Kullback-Leibler divergence ( KL) annealing can alleviate this problem .34 Since KL  \nannealing (KLA) controls the gradient size, adopting Pre-LN Transformerâ€”designed to stabilize the \ngradient flow of the (Post-LN) Transformerâ€”can facilitate KLA manipulation. The loss function of \nGCT is as follows: \nğ¿(âˆ…,ğœƒ;ğ‘¥ğ‘’ğ‘›ğ‘,ğ‘¥<ğ‘¡,ğ‘¥ğ‘¡,ğ‘)=ğ‘˜ğ‘¤ğ·ğ¾ğ¿(ğ‘âˆ…(ğ‘§|ğ‘¥ğ‘’ğ‘›ğ‘,ğ‘)âˆ¥ğ‘(ğ‘§|ğ‘) )âˆ’ğ¸ğ‘âˆ…(ğ‘§|ğ‘¥ğ‘’ğ‘›ğ‘,ğ‘)[logğ‘”ğœƒ(ğ‘¥ğ‘¡|ğ‘§,ğ‘¥<ğ‘¡,ğ‘)]                  (3) \nwhere ğ·ğ¾ğ¿(âˆ™)  is the KL divergence, and ğ¸[âˆ™]  is the expectation of âˆ™ . ğ‘âˆ…  is a parameterized \nencoder function, ğ‘”âˆ…  is a parameterized decoder function (generator), ğ‘(âˆ™|ğ‘)  is a conditional \nGaussian prior. Here, âˆ… , ğœƒ , ğ‘¥ğ‘’ğ‘›ğ‘ , ğ‘¥<ğ‘¡ , ğ‘§ , ğ‘¥ğ‘¡ , ğ‘ , ğ‘˜ğ‘¤  are the parameter set of the encoder, the \nparameter set of the decoder, the input of the encoder, the input of the decoder, the latent variables, the \nreconstruction target, the conditions, and the weight for KLA, respectively. The encoder and decoder \neach consist of six Pre-LN Transformer blocks. Each block has dimensions of 512 and 8-head attention, \nand the dimension of the feed -forward block is 2,048. The Gaussian latent space is designed in 128  \n10 \n \ndimensions. \nThe self-attention block of the  encoder obtains the concatenated array of the SMILES string and \nthree different properties: the octanol-water partition coefficient (logP), the topological Polar Surface \nArea (tPSA), and the Quantitative Estimate of Drug-likeness (QED).35 The encoder-decoder attention \nblock in the decoder obtains the concatenated array of latent code and condition (three properties), and \nthe self-attention block in the decoder obtains only the SMILES string. In the training phase, GCT \nperforms the task of r econstructing the SMILES string â€”input through the encoder â€”by referring to \nthe given hints on the molecular properties. In this process, the low -dimensional latent space acts as \nthe model's bottleneck to find as much meaningful information that can be restored to the decoder as \npossible by exploiting the limited information passed through the bottleneck. Then, meaningful latent \nvariables for molecular structures and properties are represented in  the low-dimensional continuous \nlatent space. In the infere nce phase, a sampled latent code and target properties are input into the \nlearned decoder, and the decoder selects the next tokens iteratively through a 4-beam search; which is \na kind of tree search method. \nA dropout rate of GCT is 0.3 applied. Learning is conducted by the Adam optimizer.36 The initial \nlearning rate is 10-4. The expansion rate of the momentum is 0.9 and the expansion rate of the adaptive \nterm is 0.98. Two methods are applied to schedule the learning rate  (GCT-WarmUP and GCT-SGDR \nin Table 1). One is to use the warm-up scheduler (eq 4) 27: \nğœ‚=3 ğ‘‘ğ‘šğ‘œğ‘‘ğ‘’ğ‘™\nâˆ’0.5 âˆ™min (ğ‘ ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡\nâˆ’0.5 ,ğ‘ ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘\nâˆ’1.5)    ( 4) \nwhere, ğœ‚ means learning rate, ğ‘ ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡ means current training step, and ğ‘ ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘ means warm-\nup steps. ğ‘ ğ‘¤ğ‘ğ‘Ÿğ‘šğ‘¢ğ‘  is set to  100,000. The other is to use Stochastic Gradient Descent with warm \nRestart (SGDR) of one epoch cycle (eq 5) 37: \nğœ‚= ğœ‚ğ‘šğ‘–ğ‘› +0.5(ğœ‚ğ‘šğ‘ğ‘¥ âˆ’ğœ‚ğ‘šğ‘–ğ‘›)(1+ğ‘ğ‘œğ‘ (\nğ‘ ğ‘ğ‘¢ğ‘Ÿğ‘Ÿğ‘’ğ‘›ğ‘¡\nğ‘ ğ‘ğ‘¦ğ‘ğ‘™ğ‘’\nğœ‹))   (5) \n11 \n \n  where, ğœ‚ğ‘šğ‘–ğ‘›   means minimum learning rate, ğœ‚ğ‘šğ‘ğ‘¥  means maximum learning rate, ğ‘ ğ‘ğ‘¦ğ‘ğ‘™ğ‘’  is \nlearning rate scheduling step cycle. Here, ğœ‚ğ‘šğ‘–ğ‘› , ğœ‚ğ‘šğ‘ğ‘¥, and ğ‘ ğ‘ğ‘¦ğ‘ğ‘™ğ‘’ are set to 0, 0.0001, and 1-epoch \nsteps, respectively. KL annealing was applied to increase ğ‘˜ğ‘¤ from 0.02 to 0.50 at 0.02 intervals per \nepoch for a total of 25 epochs. Other details of GCT are provided in SI.  \n12 \n \n \nFigure 2. De novo molecular generation via GCT. (a) Transformer model for NMT: an example of translating \nLatin into English. It iteratively selects the next English word by referring to the Latin sentence and the English \nsentence translated up to the previous step. (b) In the process of learning to reproduce the input chemical \nformula, GCT learns the molecular structure and three different properties: logP, tPSA, and QED. It represents \nthe information of molecular structure and properties in the latent space during t he learning process. (c) The \ntrained GCT generates a de novo molecule that satisfies the target properties by decoding the molecular \ninformation sampled from the latent space and the given preconditions.  \n  \n\n13 \n \nDatasets & Benchmark. \nThe GCT model was trained and benchmarked using a database of MOSES benchmarking platforms. \nThe MOSES database is a benchmarking dataset for drug discovery created by sampling molecules \nfrom the ZINC is Not Commercial ( ZINC) database38â€”composed of commercially available \ncompoundsâ€”that satisfy specific conditions: a weight in the range from 250 to 350 daltons, number \nof rotatable bonds is not greater than 7, not containing charged atoms or atoms other than C, N, S, O, \nF, Cl, Br, and H or cycles longer than 8 atoms. The MOSES database consists of training samples (1.7 \nM), test samples (176 k), and scaffold test samples (176 k), which have scaffolds that never appear in \nthe training samples. It is also designed to closely match the distribution between the datasets. The \nthree additional properties (logP, tPSA, and QED) computed from RDKit39 are used for GCT learning. \nIn general, the quality of network training can be evaluated by measuring how different the model's \npredicted and the actual label s are. However, for molecular generative models, the small mean loss \ndoes not guarantee that the generative model performs well because the art ifacts in the generated \nmolecules, which are not observed in the mean loss measurement, may not fit the chemical valence \nrule or may make the molecules unrealistic. For this reason, the quality of the generated molecules \nneeds to be checked against the following criteria: \nï‚ How plausible are the generated molecules? \nï‚ Do the generated molecules satisfy the target properties? \nï‚ Can multiple candidates be generated for a single precondition set? \nï‚ Can de novo molecules be created in a short time? \nIn total, 30,000 SMILES strings are generated by the trained GCT model and evaluated by MOSES \nbenchmarking score metrics (Table 1). In addition to relatively simple scores such as the validity, \nuniqueness, internal diversity, filters, and novelty, the MOSES benchmarking platform also provides \nmetrics that can measure similarity with reference molecules such as  the Similarity to a Nearest \n14 \n \nNeighbor (SNN),30 FrÃ©chet ChemNet Distance (FCD),40 Fragment similarity (Frag), 30 and Scaffold \nsimilarity (Scaf).30 \nThe SNN score is calculated via eq 6: \nSNN(ğº,ğ‘…)= 1\n|ğº| âˆ‘ max\nğ‘šğ‘…âˆˆğ‘…\nğ‘‡(ğ‘šğº,ğ‘šğ‘…)\nğ‘šğºâˆˆğº\n                                                                    (6) \nwhere ğº  and ğ‘…  refer to the set of molecules generated and reference molecules, respectively , ğ‘š \nstands for Morgan fingerprints,41 and ğ‘‡(ğ´,ğµ) stands for the Tanimoto similarity42 between set ğ´ and \nset ğµ. \nThe FCD uses activation of the penultimate layer in ChemNet and is designed to predict bioactivity. \nIt calculates the difference in the distributions between ğº and ğ‘… via eq 7: \nFCD(ğº,ğ‘…)= â€–ğœ‡ğº âˆ’ğœ‡ğ‘…â€–2 +ğ‘‡ğ‘Ÿ(âˆ‘ +\nğº\nâˆ‘ âˆ’\nğ‘…\n2(âˆ‘ âˆ‘\nğ‘…ğº\n)\n1/2\n )                                      (7) \nwhere ğœ‡ is the mean, âˆ‘  is the covariance, and ğ‘‡ğ‘Ÿ(âˆ™) is the trace operator. \nThe Frag score is calculated via eq 8: \nFrag(ğº,ğ‘…)= \nâˆ‘ (ğ‘ğ‘“(ğº)âˆ™ğ‘ğ‘“(ğ‘…))ğ‘“âˆˆğ¹\nâˆšâˆ‘ ğ‘ğ‘“\n2(ğº)ğ‘“âˆˆğ¹ âˆšâˆ‘ ğ‘ğ‘“\n2(ğ‘…)ğ‘“âˆˆğ¹\n                                                                   (8) \nwhere ğ¹ is the set of 58,315 unique BRICS fragments, 43 and ğ¶ğ‘“(ğ´) is the frequency with which \nfragment ğ‘“âˆˆğ¹ appears in the molecules in set ğ´. \nThe Scaf score is calculated via eq 9: \nScaf(ğº,ğ‘…)= \nâˆ‘ (ğ‘ğ‘ (ğº)âˆ™ğ‘ğ‘ (ğ‘…))ğ‘ âˆˆğ‘†\nâˆšâˆ‘ ğ‘ğ‘ 2(ğº)ğ‘ âˆˆğ‘† âˆšâˆ‘ ğ‘ğ‘ 2(ğ‘…)ğ‘ âˆˆğ‘†\n                                                                    (9) \nwhere ğ‘† is the set of 448,854 unique Bemis-Murcko scaffolds44 and ğ¶ğ‘ (ğ´) is the frequency at which \nscaffold ğ‘ âˆˆğ‘† appears in the molecules in set A. \n  \n15 \n \nCondition Sampling. \nThe properties considered in this problem are logP, tPSA, and QED. A three-dimensional histogram \nwas derived after dividing each property into 1,000 equal sections between the maximum and \nminimum values in the training data. Then, the cells were sampled ac cording to the probability that \ndata samples exist in each cell; here, the probability is the number of samples in that cell out of the \ntotal samples. Next, uniform noise was added at the center value of the cell to create condition sets for \nthe 30,000 mol ecules to be generated; the sizes of the uniform noise for logP -axis, tPSA-axis, and \nQED-axis are applied to not exceed the size of the cell sides in each axis direction. \n \nLatent Variable Sampling. \nAs mentioned earlier, the dimension of latent variables are set to 128; 128-number of latent variables. \nHowever, since the number of tokens constituting a SMILES string is various for each molecule, the \nsequence length of the latent variables is applied differently each time ; â„128Ã—ğ‘ ğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘’_ğ‘™ğ‘’ğ‘›ğ‘”ğ‘¡â„. Here, \nthe sequence length means the number of tokens constituting the SMILES string. The sequence length \nused for each molecular generation was sampled from a normal distribution. The mean and variance  \nof the normal distribution were derived from the number of tokens constituting the SMILES strings in \nthe MOSES training dataset. After the sequence length is determined, the values of the latent variables \nare sampled from the standard normal distribution.  \n16 \n \nâ–  RESULTS AND DISCUSSION \nTable 1 Comparison of the MOSES benchmarking results \n   GCT (ours) MOSES Reference Models \n   GCT-\nWarmUp \nGCT-\nSGDR \nVAE9,30 AAE30,45 Char \nRNN9,30 \nLatent \nGAN16,30 \nJTN \n-VAE30,46 \nValiditya â†‘  0.9853 0.9916 0.9767 \nÂ±0.0012 \n0.9368Â± \n0.0341 \n0.9748 \nÂ±0.0264 \n0.8966 \nÂ±0.0029 \n1.0Â±0.0 \nUnique@1kb â†‘  1.0 0.998 1.0Â±0.0 1.0Â±0.0 1.0Â±0.0 1.0Â±0.0 1.0Â±0.0 \nUnique@10kc â†‘  0.9981 0.9797 0.9984 \nÂ±0.0005 \n0.9973 \nÂ±0.002 \n0.9994 \nÂ±0.0003 \n0.9968 \nÂ±0.0002 \n0.9996 \nÂ±0.0003 \nIntDivd â†‘  0.8531 0.8458 0.8558 \nÂ±0.0004 \n0.8557 \nÂ±0.0031 \n0.8562 \nÂ±0.0005 \n0.8565 \nÂ±0.0007 \n0.8551 \nÂ±0.0034 \nFilterse â†‘  0.9956 0.9982 0.9970 \nÂ±0.0002 \n0.9960 \nÂ±0.0006 \n0.9943 \nÂ±0.0034 \n0.9735 \nÂ±0.0006 \n0.9760 \nÂ±0.0016 \nNoveltyf â†‘  0.8144 0.6756 0.6949 \nÂ±0.0069 \n0.7931 \nÂ±0.0285 \n0.8419 \nÂ±0.0509 \n0.9498 \nÂ±0.0006 \n0.9143 \nÂ±0.0058 \nSNNg â†‘ Test 0.6179 0.6513 0.6257 \nÂ±0.0005 \n0.6081 \nÂ±0.0043 \n0.6015 \nÂ±0.0206 \n0.5371 \nÂ±0.0004 \n0.5477 \nÂ±0.0076 \nTestSF 0.5771 0.5990 0.5783 \nÂ±0.0008 \n0.5677 \nÂ±0.0045 \n0.5649 \nÂ±0.0142 \n0.5132 \nÂ±0.0002 \n0.5194 \nÂ±0.007 \nFCDh â†“ Test 0.4017 0.7980 0.0990 \nÂ±0.0125 \n0.5555 \nÂ±0.2033 \n0.0732 \nÂ±0.0247 \n0.2968 \nÂ±0.0087 \n0.3954 \nÂ±0.0234 \nTestSF 0.8031 0.9949 0.5670 \nÂ±0.0338 \n1.0572 \nÂ±0.2375 \n0.5204 \nÂ±0.0379 \n0.8281 \nÂ±0.0117 \n0.9382 \nÂ±0.0531 \nFragi â†‘ Test 0.9973 0.9922 0.9994 \nÂ±0.0001 \n0.9910 \nÂ±0.0051 \n0.9998 \nÂ±0.0002 \n0.9986 \nÂ±0.0004 \n0.9965 \nÂ±0.0003 \nTestSF 0.9952 0.8562 0.9984 \nÂ±0.0003 \n0.9905 \nÂ±0.0039 \n0.9983 \nÂ±0.0003 \n0.9972 \nÂ±0.0007 \n0.9947 \nÂ±0.0002 \nScafj â†‘ Test 0.8905 0.8562 0.9386 \nÂ±0.0021 \n0.9022 \nÂ±0.0375 \n0.9242 \nÂ±0.0058 \n0.8867 \nÂ±0.0009 \n0.8964 \nÂ±0.0039 \nTestSF 0.0921 0.0551 0.0588 \nÂ±0.0095 \n0.0789 \nÂ±0.009 \n0.1101 \nÂ±0.0081 \n0.1072 \nÂ±0.0098 \n0.1009 \nÂ±0.0105 \n \nMOSES benchmarking was performed on 30,000 SMILES strings generated by GCT. Then, the scores obtained \nfrom GCT were compared with the scores of other generative models provided by the MOSES benchmarking \nplatform: VAE, AAE, CharRNN, LatentGAN, and JTN-VAE. Ten indicators were used to evaluate the quality of \nthe resulting molecules (a -j). aThe ratio of valid SMILES strings. bThe ratio of unique samples out of 1,000  \ngenerated molecules. cThe ratio of unique samples out of 10,000  generated molecules. dThe internal diversity \nof the generated molecules. eThe ratio of passing through toxic substance filters. fThe ratio of generated \nmolecules that do not exist in the training data. gThe average Tanimoto similarity for all generated molecules; \nthe similarity  between a generated molecule and the most similar molecule among the reference data  is \ncalculated. hMeasurement of the distance between the generated molecules and the reference molecules using \nthe activation of the penultimate layer of the ChemNet bioactivity  prediction model. iThe cosine similarity \nbetween the frequency in which the BRICS appear in the reference data and the frequency appearing in the \ngenerated molecules.  jThe cosine similarity between the frequency in which specific scaffolds appear in the \n17 \n \nreference data and the frequency appearing in the generated molecules.  The up arrow means that a higher \nvalue is better, and the down arrow means that a lower value is better. \n \nHow Plausible Are the Generated Molecules? \nTo evaluate whether the generated SMILES strings represent plausible molecular structures, analysis \nfrom two perspectives is required. The first analysis is whether the generated SMILES strings can \ngenerate valid molecular graphs , in other words, whether t he generated SMILES strings satisfy both \nthe chemical valence rule and the syntax of the SMILES language. From the benchmarking results, it \nwas found that more than 98.5% of the generated SMILES strings are valid; GCT-WarmUp shows the \nvalidity of 98.5% and GCT-SGDR shows the validity of 99.2%. It is the highest value among language-\nbased models (Table 1a). This ability depends on how well the generative machine can understand the \ngeometry of molecules through SMILES strings. To determine the character (corresponding to atom, \nbond, or branch) followed by the given chemical string that satisfies the chemical valence rule and the \ngrammar of the SMILES language, the geometry of the molecules (connectivity of each atom and the \nbranches) present in the string must be understood.  It seems that the attention mechanism applied to \nGCT helps the neural network to understand the grammar of chemical language beyond the semantic \ndiscontinuity of the SMILES language. It tends to be consis tent with the results (visualized example \nof attention score for diproxadol) shown in ref 47. \n Figure 3 shows the results for two extreme examples of how to pay attention to the characters \nwithin the SMILES string. Atom1, atom2, and atom13 in Figure 3a are located close to each other in \nthe molecular graphs but far away from each other within the SMILES string. Although only a SMILES \nstring was provided to GCT, it is recognized that atom1, atom2, and atom13 are related to each other \n(â™ ); Figure 3b shows that GCT-SGDR recognizes the relationship between atom2 and atom22, 23 (â™¦). \nIt also recognizes atoms corresponding to a particular branch (â™£) and recognizes the ring type of branch \n(â™¥). Each attention -head recognizes the molecular structure according to different viewp oints. In \n18 \n \naddition to the attention -heads visualized in Figure 3a and Figure 3b, the other attention -heads are \nshown in Figure S2. In summary, the attention mechanism applied to GCT seems to help GCT to \nrecognize the molecular structure hidden in the one-dimensional text. This claim is consistent with the \nclaim of ref. 47 regarding the role of attention mechanism. \nThe second analysis is how similar the molecular structures parsed from the valid SMILES strings \nare similar to real molecules. Four metrics can measure this: the SNN, FCD, Frag, and Scaf. Four \nmetrics were evaluated on two test sets present in MOSES: Test set and TestSF set. Molecules \nconstituting the test set have the same scaffolds of molecules constituting t he train set. On the other \nhand, TestSF consists of molecules including scaffolds that are not included in the train set. Except for \nSNN (Test) and SNN (TestSF) of GCT-SGDR, and Scaf (TestSF) of GCT-WarmUp, V AE shows better \nscores than GCT. However, compared to the scores of other reference models, it doesnâ€™t seem that the \nscores of GCT are poor. SNN (Test) and SNN (TestSF) of GCT-WarmUp are higher than the score of \nall the other reference models except V AE. FCD (Test) of GCT-WarmUp is better than AAE and FCD \n(TestSF) of GCT-WarmUp is better than FCD (TestSF) of AAE, LatentGAN, and JTN-V AE. Frag (Test) \nand Frag (TestSF) of GCT-WarmUp is higher than Frag (Test) and Frag (TestSF) of AAE and JTN -\nV AE. Scaf (Test) of GCT-WarmUp is higher than LatentGAN and Scaf (TestSF) of GCT-WarmUp is \nhigher than V AE and AAE. \n  \n19 \n \n \nFigure 3. Visualized examples for self-attention scores of an attention head in encoder blocks. (a) Visualization \nresults of the fourth head in the second encoder block for 4-(2-aminopropyl)-2-methoxyphenol. (b) Visualization \nresults of the third head in the second encoder block for 5,6 -bis(p-methoxyphenyl)-3-methyl-1,2,4-triazine. \nThese are two extreme  cases where the atom adjacent to atom2 in each molecular graph (atom13 in a and \natom23 in b is far away in each SMILES string.  The yellow lines in the black box at the bottom of the figure \nshow which tokens were given a high attention score from the query vector by attention. The higher the attention \nscore, the opaquer the yellow line. Each color in the SMILES string and the molecular graph represents each \nbranch of the SMILES string separated with parentheses. The visualization scheme is borrowed from ref. 48. \n \nDo the Generated Molecules Satisfy the Target Properties? \nGCT generates molecular structures that satisfy multiple target properties . Figure 4a-c are the \nresults of comparing the properties of 30,000 molecules generated from GCT (calculated from RDKit) \nand target properties (preconditions given in GCT). Since logP and tPSA are physical properties \ndirectly related to the molecular structure, it is possible to generate a molecular structure corresponding \nto the target property based on an understanding of the molecular structure. However, the QED is an \nartificial index designed to determine the likeness to drugs quantitatively through geometric averages \n\n20 \n \nof eight different properties, so it is relatively difficult for the QED; this phenomenon is also found \nwith cRNNs. The absolute mean errors between the target conditions for each property and the \nproperties of the generated molecule are 0.177 (logP), 2.923 (tPSA), and 0.035 (QED). \nThe length of the generated SMILES string depends on the length of the latent code  since GCT \nhas an autoencoder (AE) structure; it is trained to reconstruct information input into the encoder. In the \ntraining phase, the length of the latent code appears equal to the length of the string input into the \nencoder and the length of the string output from the decoder; in fact, these are slightly different \ndepending on whether the <sos> and <eos> tokens are used in the input and output design; however, \nthe nature of the string is not different (see SI). In the inference phase, the length of the input latent \ncode and the length of the generated SMILES string did not match perfectly and GCT does not learn \nthe distribution of sequence lengths (Figure 4d). However, it seems that the length of the generated \nSMILES string which is related to the size of the molecule can be manipulated to some extent by \nadjusting the length of the latent code. \nTo check whether the multiple given target properties are satisfied simultaneously, the properties \nof generated molecules were compared to the 10  precondition sets that were sampled from the \ndistribution of training data  (Figure 4e-g). The conditional mode l, which is a skeleton of GCT , \ngenerates molecules that simultaneously satisfy multiple target properties  well. Furthermore, the \nvariational generator in GCT makes it possible to generate various molecules under the same \nprecondition set (Figure S3-12). \n21 \n \n \nFigure 4 Comparison of target properties (preconditions given to GCT) and properties of 30,000  generated \nmolecules. (a)-(c), The x-axis refers to the target property, and the y-axis refers to the property of the generated \nmolecule. (d), The x-axis refers to the length of the latent code, and the y -axis refers to the number of tokens \nconstituting the generated SMILES string. (e)-(g), 10 precondition sets randomly sampled from the MOSES data \ndistribution (red line) and properties of the generated molecules (blue dot s). (h), The length of the latent code \nrandomly sampled from the MOSES data distribution (red marker) and the number of tokens constituting the \ngenerated molecule (blue dots). \n \nCan De Novo Molecules Be Created? \nWhether a generative model can create de novo molecules is an important criterion that \ndetermines its applicability for material discovery. The novelty score refers to the probability of \ngenerating a new molecule that does not exist in the training data (Table 1f). Note that o nly a high \nnovelty score does not guarantee that it is a good generator  since odd and unrealistic molecules can \nincrease the novelty score. Hence, the novelty score should be used in conjunction with indicators to \n\n22 \n \nevaluate whether the generated molecules are realistic. Figure 5 shows a scatter plot of each modelâ€™s \nnovelty score and v alidity score. The dotted line is a linear regression of the reference model scores \n(V AE, AAE, CharRNN, GAN, JTN-V AE). Interestingly, for all reference models, it is observed that \nthe novelty score decreases as the SNN score increases. Conversely, the higher the novelty score, the \nlower the SNN score. This means that it is not easy to create new molecules that are similar to real \nmolecules. However, it can be con firmed that GC Ts (GCT-WarmUp, GCT-SGDR, GCT-Exp1~3) \ngenerate new and similar molecules better than the reference models; Here, GCTs mean trained models \nusing different hyperparameters or different learning rate schedules. The detailed conditions and scores \nfor each GCT are summarized in Table 2. \n \nFigure 5 Scatter plot of novelty score and similarity of a nearest neighbor score. \n \nTable 2 Experiments on hyperparameter tuning and learning rate scheduler type  \n  GCT-Exp1 \n(baseline) \nGCT \n-WarmUp \nGCT \n-Exp2 \nGCT \n-Exp3 \nGCT \n-Exp4 \nGCT \n-SGDR \nTraining Conditions \n Epochs 25 25 25 25 50 25 \n KLA weight ( ğ‘˜ğ‘¤ ) \n[start:step:end] \n[0:0.02:0.5] [0:0.02:0.5] [0:0.02:0.5] [0:0.01:0.25] [0:0.01:0.5] [0:0.02:0.5] \n Learning rate (lr) Baseline lr Baseline lr \nÃ—ğŸ‘ (= eq. 4) \nBaseline lr \nÃ—ğŸ“ \nBaseline lr Baseline lr eq. 5 \n Learning rate \nschedular \nWarm-Up Warm-Up Warm-Up Warm-Up Warm-Up SGDR \nMOSES Benchmarks \n Validity 0.9757 0.9853 0.9808 \n \n0.9692 0.9813 0.9916 \n\n23 \n \n Unique@1k 1.0 1.0 1.0 1.0 1.0 0.998 \n Unique@10k 0.9977 0.9981 0.9977 0.9983 0.9986 0.9797 \n IntDiv 0.8465 0.8531 0.8531 0.8541 0.8491 0.8458 \n Filters 0.9955 0.9956 0.9950 0.9944 0.9943 0.9982 \n Noveltyf 0.8019 0.8144 0.8043 0.8114 0.8572 0.6756 \n SNN (Test) 0.6204 0.6179 0.6206 0.6081 0.6131 0.6513 \n SNN (TestSF) 0.5784 0.5771 0.5791 0.5693 0.5744 0.5990 \n FCD (Test) 0.4181 0.4017 0.3813 0.3883 0.7180 0.7980 \n FCD (TestSF) 0.8560 0.8031 0.8411 0.7855 1.221 0.9949 \n Frag (Test) 0.9977 0.9973 0.9980 0.9979 0.9944 0.9922 \n Frag (TestSF) 0.9957 0.9952 0.9956 0.9960 0.9910 0.8562 \n Scaf (Test) 0.9021 0.8905 0.8935 0.8644 0.8893 0.8562 \n Scaf (TestSF) 0.0883 0.0921 0.1039 0.1049 0.0969 0.0551 \n \nThe model using the SGDR (GCT -SGDR) shows lower novelty and higher validity than the \nmodels using the warm-up scheduler (GCT-WarmUp). A scheduler that cyclically reduces the learning \nrate has a loss in reducing the KL divergence term of the V AE loss function, however, it has a benefit \nin reducing the reconstruction  error term. 49 It seems that the SGDR scheduler, a kind of cyclic \nannealing scheduler, makes GCT-SGCR have high validity and low novelty. \nThe time taken per molecule generation was 507 ms in the environment of an 8C/16T CPU and \nan NVIDIA GTX 1080 Ti and 440 ms with a 12C/24T and an NVIDIA Tesla T4. \n \nLimitations. \nNot all properties of molecules used for drug discovery were considered, and only properties of drug \nmolecules covered by the MOSES dataset were considered; charged atoms and stereochemistry are not \nconsidered and these are limitations of this study. Furthermore, it is hard to extrapolate outside of the \nproperty window GCT was trained on since V AE, which is a model that learns the distribution of data \nand generates data by sampling the latent variables from the learned distribution of latent variables , \ncannot learn the distribution of data properly for regions where there are no data samples or for sparse \nregions. For this reason, the target properties are not satisfied relatively well for precondition #10 in \nFigure 4e-h; low QED area with few data samples (see Figure 4c). \n \n24 \n \nâ–  CONCLUSIONS \nIn this study, a GCT architecture that embeds Transformer â€”a language model that has been a \nbreakthrough in the field of NLP using  an attention mechanismâ€”into a conditional variational \ngenerator is proposed. The trained GCT can generate SMILES strings that meet the desired conditions \nbased on a deep understanding of chemical language. It learns molecular structures and three different \nproperties as a form of language: the logP, tPSA, and QED. Quantitative evaluations were performed \nby scoring molecules converted from the generated SMILES strings. In this process, the characteristics \nof the indicators (the SNN, FCD, Frag, and Scaf) that measure the plausibility of the molecules were \nanalyzed, and the limitations were discussed. The performance of GCT was benchmarked by  the \nMOSES benchmarking platform. By analyzing the results, it is demonstrated that GCT can utilize both \nthe advantages of a language model and a conditional variational generator. The conclusions obtained \nare summarized as follows: \nï‚ (1) The attention mechanism in GCT helps to deeply understand the geometric structure \nof molecules beyond the limitations of chemical language semantic  discontinuity \nresulting from converting a non -Hamiltonian molecular graph to a one -dimensional \nstring by paying sparse attention to chemical formulas. \nï‚ A deep understanding of chemical language makes the generated SMILES strings ( 2) \nsatisfy the syntax of SMILES language, (3) satisfy the chemical rules, and (4) are realistic. \nï‚ The conditional variational generator in GCT makes the generated molecules (5) satisfy \nmultiple target properties simultaneously and (6) vary. \nï‚ The AE structure of GCT (7) makes the molecular size controllable. \nï‚ GCT (8) creates de novo molecules that have never been seen in the training process, and \n(9) creates a molecule in hundreds of milliseconds. \nWell-trained GCT (GCT-WarmUp) generates valid SMILES strings with 98.5% probability. 84.1% \n25 \n \nof the generated SMILES strings were new molecules that had never been learned, and their similarity \nwith real molecules was 0.681 (their SNN score was 0.681). It is difficult to create a new molecule that \nis similar to the pattern of existing real molecules but has never been seen before. Among the compared \nmodels, GCT showed the best performance in making new molecules that are similar to real molecules. \nAdditionally, the generated molecules satisfied multiple target properties simultaneously, and the mean \nabsolute errors for the three different properties were 0.177 (logP), 2.923 (tPSA), and 0.035 (QED). In \naddition, it was confirmed that GCT can control the molecular size; the averaged difference in the \nnumber of generated SMILES tokens compared to the given length of latent code is 0.332. Molecular \ngeneration took 507 ms per mole cule on a personal computer. Furthermore, conditions applicable to \nGCT can be configured differently as needed, and GCT can be extended to Transformer -based \narchitectures such as BERT,50 GPT,51 and T552.  Considering the time required versus the advantages \nlisted above, it is expecte d that our proposed model can contribute to accelerating the process of \nmaterial discovery. \n26 \n \nâ–  ASSOCIATED CONTENT \nSupporting Information \nThe Supporting Information is available free of charge on the ACS Publications website at DOI: \nDetails of the structure of GCT, input and output data structure, and generation results for the 10-\nprecondition set (PDF) \n \nâ–  AUTHORS INFORMATION \nCorresponding Authors \nJonggeol Na â€” Department of Chemical Engineering and Materials Science, Graduate Program in \nSystem Health Science and Engineering, Ewha Womans University, Seoul 03760, Republic of Korea; \nE-mail: jgna@ewha.ac.kr \nWon Bo Lee â€” School of Chemical and Biological Engineering, Seoul National University, Gwanak-\nro 1, Gwanak-gu, Seoul 08826, Republic of Korea; E-mail: wblee@snu.ac.kr \n \nNotes \nThe authors declare no competing financial interest.  \n \nâ–  Data and Software Availability \nThe datasets used for learning and evaluation of GCT were based on the MOSES benchmarking \nplatform at https://github.com/molecularsets/moses,30 released under the MIT license.53 Three physical \nproperties were calculated from RDKit. The curated data are available at \nhttps://github.com/Hyunseung-Kim/molGCT. \nGCT was implemented by modifying the public code under Apache License 2.0 implementing \nTransformer (https://github.com/SamLynnEvans/Transformer54). The Python code implementing GCT \nis available at https://github.com/Hyunseung-Kim/molGCT. \n \nâ–  ACKNOWLEDGMENTS \nThis research was supported by the National Research Foundation of Korea (NRF) grant funded by \nKorean Government through the Ministry of Science and ICT  (MSIT) (NRF-2018M3D1A1058633, \nNRF-2019R1A2C1085081, and NRF-2021R1C1C1012031). \n27 \n \nâ–  REFERENCES \n1. Virshup, A. M.; Contreras-GarcÃ­ a, J.; Wipf, P.; Yang, W.; Beratan, D. N. Stochastic Voyages \ninto Uncharted Chemical Space Produce a Representative Library of All Possible Drug-Like \nCompounds. J. Am. Chem. Soc. 2013, 135, 7296-7303. \n2. SMARTS â€“ Language for Describing Molecular Patterns.  \nhttps://www.daylight.com/dayhtml/doc/theory/theory.smarts.html, [Online; accessed 4 -June-\n2021] \n3. Heller, S. R.; McNaught, A.; Pletnev, I.; Stein, S; Tchekhovskoi, D. InChI, the IUPAC \nInternational Chemical Identifier. J. Cheminf. 2015, 7. \n4. Weininger, D. SMILES, A Chemical Language and Information System. 1. Introduction to \nMethodology and Encoding Rules. J. Chem. Inf. Model. 1988, 28, 31-36. \n5. Weininger, D.; Weininger, A.; Weininger, J. L. SMILES. 2. Algorithm for Generation of \nUnique SMILES Notation. J. Chem. Inf. Model. 1989, 29, 97-101. \n6. Weininger, D. SMILES. 3. DEPICT. Graphical Depiction of Chemical Structures. J. Chem. \nInf. Model. 1990, 30, 237-243. \n7. ArÃºs-Pous, J.; Johansson, S.; Prykhodko, O.; Bjerrum, E. J.; Tyrchan, C.; Reymond, J. -L.; \nChen, H.; Engkvist, O. Randomized SMILES Strings Improve the Quality of Molecular \nGenerative Models. J. Cheminf. 2019, 11. \n8. Amabilino, S.; PogÃ¡ny, P.; Pickett, S. D.; Green, D. V . S. Guidelines for Recurrent Neural \nNetwork Transfer Learning-Based Molecular Generation of Focused Libraries. J. Chem. Inf. \nModel. 2020, 60, 5699-5713. \n9. Segler, M. H. S.; Kogej, T.; Tyrchan, C.; Waller, M. P. Generating Focused Molecule Libraries \nfor Drug Discovery with Recurrent Neural Networks. ACS Cent. Sci. 2018, 4, 120-131. \n28 \n \n10. Winter, R.; Montanari, F.; NoÃ©, F.; Clevert, D. A. Learning Continuous and Data-Driven \nMolecular Descriptors by Translating Equivalent Chemical Representations. Chem. Sci., 2019, \n10, 1692-1701. \n11. Gao, K.; Nguyen, D. D.; Tu, M.; Wei, G. W. Generative Network Complex for the Automated \nGeneration of Drug-Like Molecules. J. Chem. Inf. Model., 2020, 60, 5682-5698. \n12. Kingma, D. P.; Welling, M. Auto-Encoding Variational Bayes. ArXiv 2014, 1312.6114. \n13. GÃ³mez-Bombarelli, R.; Wei, J. N.; Duvenaud, D.; HernÃ¡ndezLobato, J. M.; SÃ¡nchez -\nLengeling, B.; Sheberla, D.; AguileraIparraguirre, J.; Hirzel, T. D.; Adams, R. P.; Aspuru -\nGuzik, A. Automatic Chemical Design Using a Data-Driven Continuous Representation of \nMolecules. ACS Cent. Sci. 2018, 4, 268âˆ’276. \n14. Blaschke, T.; Olivecrona, M.; Engkvist, O.; Bajorath, J.; Chen, H. Application of Generative \nAutoencoder in De Novo Molecular Design. Mol. Inf. 2018, 37, 170123-1700123. \n15. Goodfellow, I. J.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde -Farley, D.; Ozair, S.;  \nCourville, A.; Bengio, Y . Generative Adversarial Networks. ArXiv 2014, 1406.2661. \n16. Prykhodko, O.; Johansson, S. V .; Kotsias, P. C.; ArÃºs-Pous, J.; Bjerrum, E. J.; Engkvist, O.; \nChen, H. A De Novo Molecular Generation Method Using Latent V ector Based Generative \nAdversarial Network. J. Cheminf. 2019, 11, 1-13.  \n17. Olivecrona, M.; Blaschke, T.; Engkvist, O.; Chen, H. Molecular De-Novo Design Through \nDeep Reinforcement Learning. J. Cheminf. 2017, 9, 1-14. \n18. Griffiths, R. R.; HernÃ¡ndez-Lobato, J. M. Constrained Bayesian Optimization for Automatic \nChemical Design Using Variational Autoencoders. Chem. Sci. 2020, 11, 577-586. \n19. Winter, R.; Montanari, F.; Steffen, A.; Briem, H.; NoÃ©, F.; Clevert, D. A. Efficient Multi-\nObjective Molecular Optimization in a Continuous Latent Space. Chem. Sci. 2019, 10, 8016-\n29 \n \n8024. \n20. Janet, J. P.; Chan, L.; Kulik, H. J. Accelerating Chemical Discovery with Machine Learning: \nSimulated Evolution of Spin Crossover Complexes with an Artificial Neural Network. J. Phys. \nChem. Lett. 2018, 9, 1064-1071. \n21. Moosavi, S. M.; Chidambaram, A.; Talirz, L.; Haranczyk, M.; Stylianou, K. C.; Smit, B. \nCapturing Chemical Intuition in Synthesis of Metal-Organic Frameworks. Nat. comm. 2019, \n10, 1-7. \n22. Nigam, A.; Friederich, P.; Krenn, M.; Aspuru-Guzik, A. Augmenting Genetic Algorithms with \nDeep Neural Networks for Exploring the Chemical Space. ArXiv 2020, 1909.11655. \n23. Yang, X.; Zhang, J.; Yoshizoe, K.; Terayama, K.; Tsuda, K. ChemTS: An Efficient P ython \nLibrary for De Novo Molecular Generation. Sci. Technol. Adv. Mater. 2017, 18, 972-976. \n24. Segler, M. H. S.; Preuss, M.; Waller, M. P. Planning Chemical Syntheses with Deep Neural \nNetworks and Symbolic AI. Nature 2018, 555, 604-610. \n25. Zhang, X.; Zhang, K.; Lee, Y. Machine Learning Enabled Tailor-Made Design of Application-\nSpecific Metalâ€“Organic Frameworks. ACS Appl. Mater. Interfaces 2020, 12, 734-743. \n26. Kotsias, P. C.; ArÃºs -Pous, J.; Chen, H.; Engkvist, O.; Tyrchan, C.; Bjerrum, E. J. Direct \nSteering of De Novo Molecular G eneration with Descriptor Conditional Recurrent Neural \nNetworks. Nat. Mach. Intell. 2020, 2, 254-265. \n27. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.;  \nPolosukhin, I. Attention Is All You Need. In proceedings of the 31st International Conference \non NIPS. 2017, 6000-6010. \n28. Bahdanau, D.; Cho, K. H.; Bengio, Y.  Neural Machine Translation by Jointly Learning to \nAlign and Translate. In proceedings of 3rd ICLR 2015. \n30 \n \n29. Li, X.; Fourches, D. SMILES Pair Encoding: A Data -Driven Substructure Tokenization \nAlgorithm for Deep Learning. Chemarxiv 2020, 12339368.v1. \n30. Polykovskiy1, D.; Zhebrak1, A.; Sanchez-Lengeling, B.; Golovanov, S.; Tatanov, O.; Belyaev, \nS.; Kurbanov, R.; Artamonov, A.; Aladinskiy1, V .; Veselov, M.; Kadurin1, A.; Johansson, S.; \nChen, H.; Nikolenko, S.; Aspuru -Guzik, A.; Zhavoronkov, A. Molecular Sets (MOSES): A \nBenchmarking Platform for Molecular Generation Models. Front. pharmacol. 2020, 11. \n31. Sohn, K.; Lee, H.; Yan, X.; Learning Structured Output Representation Using Deep \nConditional Generative Models. Adv. Neural Inf. Process Syst. 2015, 28, 3483-3491. \n32. Xiong, R.; Yang, Y .; He, D.; Zheng, K.; Zheng, S.; Xing, C.; Zhang, H.; Lan, Y .; Wang, L.; \nLiu, T.-L. On Layer Normalization in the Transformer Architecture. In proceedings of ICML \n2020, 10524-10533. \n33. He, J.; Spokoyny, D.; Neubig, G.; Berg-Kirkpatrick, T. Lagging Inference Networks and \nPosterior Collapse in Variational Autoencoders. In proceedings of 7th ICLR 2019. \n34. Bowman, S. R.; Vilnis, L.; Vinyals, O.; Dai, A. M.; Jozefowicz, R.; Bengio, S. Generating \nSentences from a Continuous Space. ArXiv 2015, 1511.06349. \n35. Bickerton, G. R.; Paolini, G. V.; Besnard, J.; Muresan, S.; Hopkins, A. L. Quantifying the \nChemical Beauty of Drugs. Nat. Chem. 2012, 4, 90-98. \n36. Kingma, D. P.; Ba, J. Adam: A Method for Stochastic Optimization. In  proceedings of 3rd \nICLR 2015. \n37. Loshchilov, I.; Hutter, F. SGDR: Stochastic Gradient Descent with Warm Restarts. In  \nproceedings of 5th ICLR 2017. \n38. Sterling, T.; Irwin, J. J. ZINC 15 â€“ Ligand Discovery for Everyone. J. Chem. Inf. Model.   \n2015, 55, 2324-2337. \n39. Landrum, G. RDKit: Open-Source Cheminformatics Software (2019); https://www.rdkit.org/. \n31 \n \n[Online; accessed 8-June-2021] \n40. Preuer, K.; Renz, P.; Unterthiner, T.; Hochreiter, S.; Klambauer, G. FrÃ©chet ChemNet \nDistance: A Metric for Generative Models for Molecules in Drug Discovery. J. Chem. Inf. \nModel. 2018, 58, 1736-1741. \n41. Rogers, D.; Hahn, M. Extended -Connectivity Fingerprints. J. Chem. Inf. Model.  2010, 50, \n742-754. \n42. Tanimoto, T. T. Elementary Mathematical Theory of Classification and Prediction; IBM \nInternal Report, 1958. \n43. Degen, J.; Wegscheid-Gerlach, C.; Zaliani, A.; Rarey, M. On the Art of Compiling and Using \n'Drug-Like' Chemical Fragment Spaces. ChemMedChem 2008, 3, 1503-1507. \n44. Bemis, G. W.; Murcko, M. A. The Properties of Known Drugs. 1. Molecular Frameworks. J. \nMed. Chem. 1996, 39, 2887-2893. \n45. Kadurin, A.; Aliper, A.; Kazennov, A.; Mamoshina, P.; Vanhaelen, Q.; Khrabrov, K.; \nZhavoronkov, A. The Cornucopia of Meaningful L eads: Applying Deep Adversarial \nAutoencoders for New Molecule Development in Oncology. Oncotarget 2017, 8, 10883 -\n10890. \n46. Jin, W.; Barzilay, R; Jaakkola, T. Junction Tree Variational Autoencoder for Molecular Graph \nGeneration. In Proceedings of ICML 2018, 2323-2332.  \n47. Dollar, O.; Joshi, N.; Beck, D. A.; & Pfaendtner, J., Attention-Based Generative Models for \nDe Novo Molecular Design. Chem. Sci. 2021, 12, 8362-8372. \n48. Vig, J. A Multiscale Visualization of Attention in the Transformer Model. ArXiv 2019, \n1906.05714. \n49. Fu, H.; Li, C.; Liu, X.; Gao, J.; Celikyilmaz, A.; Carin, L. Cyclical Annealing Schedule: A \nSimple Approach to Mitigating KL Vanishing. ArXiv 2019, 1903.10145. \n32 \n \n50. Devlin, J.; Chang, M.-W.; Lee, K.; Toutanova, K. BERT: Pre-Training of Deep Bidirectional \nTransformers for Language Understanding. ArXiv 2019, 1810.04805. \n51. Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; \nShyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan, T.; \nChild, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; \nLitwin, M.; Gray, S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, \nI.; Amodei, D. Language Models Are Few-Shot Learners. ArXiv 2020, 2005.14165.. \n52. Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena, M.; Zhou, Y .; Li, W.; Liu P. \nJ. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. J. Mach. \nLearn. Res. 2020, 21, 1-67. \n53. MIT License; https://opensource.org/licenses/MIT [Online; accessed 8-June-2021] \n54. Lynn-Evans, S. Transformer (2019); \nhttps://github.com/SamLynnEvans/Transformer/commits?author=SamLynnEvans [Online; \naccessed 8-June-2021] \n  \n33 \n \n \n \nSupporting Information \nfor manuscript \n \nGenerative chemical Transformer: Neural machine learning of \nmolecular geometric structures from chemical language via attention \n \nHyunseung Kimâ€ , Jonggeol Naâ€¡,*, Won Bo Leeâ€ ,* \n \nâ€ School of Chemical and Biological Engineering, Seoul National University, Gwanak -ro 1, Gwanak-\ngu, Seoul 08826, Republic of Korea \nâ€¡Department of Chemical Engineering and Materials Science, Graduate Program in System Health \nScience and Engineering, Ewha Womans University, Seoul 03760, Republic of Korea \n \nCorrespondence and requests for materials should be addressed to  \nJ.N. (email: jgna@ewha.ac.kr) or W.B.L. (email: wblee@snu.ac.kr). \n \n34 \n \nThe architecture of generative chemical Transformer \n \nFigure S1. The architecture of generative chemical Transformer. \n \nFigure S1 shows the network architecture of generative chemical Transformer (GCT). Since GCT \nis based on the structure of conditional variational autoencoder (cV AE)1, it is trained in the form of \nreconstructing information that enters the encoder through the decoder. The encoder receives a \nTok1\n \nTokN\nInput  S ILES\n       \n    ( , )\nlog \nt SA\n ED\n on ition\n  sos  \nTok1\n \nTokN\nInput  S ILES\nlog \nt SA\n ED\n on ition\nTok1\n \nTokN\n  eos  \n utput  S ILES\nN N \n\n35 \n \ntokenized SMILES string and three different properties: the logP, tPSA, and QED. First, the tokenized \nSMILES string is one-hot encoded for 26 types of tokens: <unknown>, <pad>, <C>, <c>, <O>, <o>, \n<N>, <n>, <F>, <S>, <s>, <Cl>, <Br>, <[nH]>, <[H]>, <->, <=>, <#>, <(>, <)>, <1>, <2>, <3>, <4>, \n<5>, <6>. The maximum length of the tokenized string is set to 80. The empty part is filled with <pad> \ntoken. Each token is then embedded into 512 dimensions  which is the same as the model dimension. \nThe conditions entered in the encoder are also embedded in 512 dimensions and concatenated in the \nembedded SMILES string; the conditions are placed in front and the SMILES string is attached to the \nback. The dimension entered the encoder block is then â„(3+80)Ã—512. \nThe language model applied to GCT is pre -layer normalization (Pre-LN) Transformer2. The \nencoder has six Pre-LN encoder blocks. The encoder has 8 multi -head attention, each head has 64 \ndimensions. Information on the properties and molecular structures leaving the Encoder is represented \nto the 128 -dimensional conditional Gaussian latent space.  Latent code sampled from latent space  is \nconcatenated with the target condition; the conditions are placed in front and the latent code is attached \nto the back . Then, the concatenated array expanded to 512 dimensions is entered into the encoder-\ndecoder attention block of the decoder. \nThe decoder has two inputs; One is the concatenated array described above and the other input is \nthe SMILES string generated up to the previous step.  The latter is entered in the self -attention block \nof decoder, after one -hot encoding for 28 types of tokens: <unknown>, <sos>, <eos>, <pad>, <C>, \n<c>, <O>, <o>, <N>, <n>, <F>, <S>, <s>, <Cl>, <Br>, <[nH]>, <[H]>, <->, <=>, <#>, <(>, <)>, <1>, \n<2>, <3>, <4>, <5>, <6> . The information entered in this way passes throu gh six pre -LN decoder \nblocks and exits the decoder. The array that leaves the decoder is reduced to the same dimension as the \none-hot encoded tokens, and then softmax function is taken to select the next one token that will come \nafter the string entered in the decoder's self-attention block. \nThe <sos> and <eos> tokens are only added to t he SMILES string which is input and output to \nthe decoder; the SMILES string input to the encoder does not have the <sos> and <eos> tokens. There \n36 \n \nare two reasons why we designed it like this. The first is that the dimension of conditions is constant, \nso if the SMILES string is put behind the conditions, the location of the beginning and end of the \nSMILES string within the concatenated input array can be recognizable. The second is the irrationality \nthat can occur during latent code sampling. Using tokens with the <sos> and <eos> can cause problems \nwhen sampling the latent code of the <sos> and <eos> token location; the location where the <sos> \nappears is always constant and it cannot be randomly sampled . For this reason, the <sos> and <eos> \nare not used in the SMILES string entered in the encoder. \nThe loss function of GCT training is given as follows: \nğ¿(âˆ…,ğœƒ;ğ‘¥ğ‘’ğ‘›ğ‘,ğ‘¥<ğ‘¡,ğ‘¥ğ‘¡,ğ‘)=âˆ’ğ‘˜ğ‘¤ğ·ğ¾ğ¿(ğ‘âˆ…(ğ‘§|ğ‘¥ğ‘’ğ‘›ğ‘,ğ‘)âˆ¥ğ‘(ğ‘§|ğ‘) )+ğ¸ğ‘âˆ…(ğ‘§|ğ‘¥ğ‘’ğ‘›ğ‘,ğ‘)[logğ‘”ğœƒ(ğ‘¥ğ‘¡|ğ‘§,ğ‘¥<ğ‘¡,ğ‘)]                  (1) \nwhere âˆ…, ğœƒ, ğ‘¥ğ‘’ğ‘›ğ‘, ğ‘¥<ğ‘¡, ğ‘§, ğ‘¥ğ‘¡, and ğ‘ are the parameter set of the encoder, the parameter set of the \ndecoder, the input of the encoder, the input of the decoder, the latent variables, the target to reconstruct, \nand the conditions, respectively. ğ·ğ¾ğ¿(âˆ™) is KL divergence, and ğ¸[âˆ™] is the expectation of âˆ™. ğ‘âˆ… is \nparameterized encoder function , ğ‘”âˆ…  is parameterize d decoder function (generator), and ğ‘(âˆ™|ğ‘)  is \nconditional Gaussian prior. ğ‘˜ğ‘¤ is the weight for KL annealing. The first term in Equation (1) is a term \nthat forces the distribution of the encoderâ€™s output to follow the conditional Gaussian prior. The second \nterm is a term to reconstruct the target t hat will come after the string entered in the decoder's self -\nattention block using ğ‘§,ğ‘, and ğ‘¥<ğ‘¡. Learning was carried out in teacher forcing manner using a mask. \n \n  \n37 \n \nVisualization of attention hea s \n \nFigure S2. Visualize  examples for self -attention scores of an attention hea  in \nenco er blocks  for 4-(2-aminopropyl)-2-methoxyphenol. (a) Visualize  attention \nscores for all attention hea s. (b) Visualize  results of the sixth hea  in the secon  \nenco er layer. \n \n\n38 \n \nFigure S 2a shows the visualized results of all attention heads in encoder block for 4 -(2-\naminopropyl)-2-methoxyphenol; note that Figure 3a in the main text shows the visualiz ed results of \nthe fourth head in the second encoder layer for 4 -(2-aminopropyl)-2-methoxyphenol. Each attention-\nhead recognizes the molecular structure according to different viewpoints. However, attention between \natom 1, atom2, and atom 13, which is intuitively easy to be interpreted, is found in many attention \nheads. Even attention heads that don't have this connection seem to understand the molecular structure \nin different ways. Figure S2b shows the visualized results of the sixth head in the second enconder \nlayer for 4 -(2-aminopropyl)-2-methoxyphenol. It seems to recognize the connection  between atom1 \nand atom2 (â˜…). And it seems to recognize that atom4, 5, 6, and 7 are connected (â–²).  And it seems to \nrecognize that the red branch is related to atom3 (â—). \n \n10-precon ition set test \nTable S1 10-precon ition set (Figure 4 of main text) \nSet # #1 #2 #3 #4 #5 #6 #7 #8 #9 #10 \nlogP 4.074 3.370 3.645 3.588 2.398 2.865 1.969 3.379 3.263 2.518 \ntPSA 20.26 53.00 68.00 50.51 29.72 47.93 43.19 38.38 75.36 85.80 \nQED 0.772 0.907 0.798 0.713 0.840 0.802 0.834 0.905 0.793 0.677 \n \nTable S1 shows the details of precondition sets used in the test shown in Figure 4 of the main text. \nThe sets were determined by dividing 1,000 intervals between the minimum and maximum values for \neach property, drawing a three-dimensional histogram, and random sampling from the probability that \ntraining samples exist in each cell. \nFigures S3-12 show the randomly sampled molecules that are generated with each precondition set. \n \n39 \n \n \nFigure S3. Generate  molecules at the given precon ition set #1. The string below the \nmolecule image is the S ILES string. The four numbers below S ILES are actual log , \nt SA,  ED, and the number of tokens, respectively. \n \n \nFigure S4. Generate  molecules at the given precon ition set #2.  \n\n40 \n \n \n \nFigure S5. Generate  molecules at the given precon ition set #3. \n \n \n \nFigure S6. Generate  molecules at the given precon ition set #4. \n\n41 \n \n \nFigure S7. Generate  molecules at the given precon ition set #5. \n \n \n \nFigure S8. Generate  molecules at the given precon ition set #6. \n \n\n42 \n \n \nFigure S9. Generate  molecules at the given precon ition set #7. \n \n \n \nFigure S10. Generate  molecules at the given precon ition set #8. \n \n\n43 \n \n \nFigure S11. Generate  molecules at the given precon ition set #9. \n \n \n \nFigure S12. Generate  molecules at the given precon ition set #10. \n \n\n44 \n \nReferences \n1. Sohn, K.; Lee, H.; Yan, X.; Learning Structured Output Representation Using Deep \nConditional Generative Models. Adv. Neural Inf. Process Syst. 2015, 28, 3483-3491. \n2. Xiong, R.; Yang, Y .; He, D.; Zheng, K.; Zheng, S.; Xing, C.; Zhang, H.; Lan, Y .; Wang, L.; \nLiu, T.-L. On Layer Normalization in the Transformer Architecture. In proceedings of ICML \n2020, 10524-10533. \n ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7026835083961487
    },
    {
      "name": "Generative grammar",
      "score": 0.6656504273414612
    },
    {
      "name": "Computer science",
      "score": 0.5563002228736877
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47254228591918945
    },
    {
      "name": "Machine learning",
      "score": 0.38616037368774414
    },
    {
      "name": "Natural language processing",
      "score": 0.35242724418640137
    },
    {
      "name": "Engineering",
      "score": 0.15627822279930115
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I138925566",
      "name": "Ewha Womans University",
      "country": "KR"
    }
  ],
  "cited_by": 66
}