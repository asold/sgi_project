{
  "title": "Latin BERT: A Contextual Language Model for Classical Philology",
  "url": "https://openalex.org/W3088026279",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4288916105",
      "name": "Bamman, David",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4304037033",
      "name": "Burns, Patrick J.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3021530909",
    "https://openalex.org/W2994980856",
    "https://openalex.org/W2898879711",
    "https://openalex.org/W2123520200",
    "https://openalex.org/W2956118678",
    "https://openalex.org/W2994335404",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3089023236",
    "https://openalex.org/W1979327299",
    "https://openalex.org/W3087823004",
    "https://openalex.org/W3040858410",
    "https://openalex.org/W360953099",
    "https://openalex.org/W2561607149",
    "https://openalex.org/W2740782137",
    "https://openalex.org/W2946676565",
    "https://openalex.org/W2979684565",
    "https://openalex.org/W2938224028",
    "https://openalex.org/W1560937015",
    "https://openalex.org/W3088650759",
    "https://openalex.org/W2964105617",
    "https://openalex.org/W2804120589",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2250949227",
    "https://openalex.org/W3034071222",
    "https://openalex.org/W2969505328",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W3088517387",
    "https://openalex.org/W2605341992",
    "https://openalex.org/W2953307569",
    "https://openalex.org/W3009095382",
    "https://openalex.org/W2991461215",
    "https://openalex.org/W3025326423",
    "https://openalex.org/W2170087572",
    "https://openalex.org/W2899381163",
    "https://openalex.org/W2788784374",
    "https://openalex.org/W1591646873",
    "https://openalex.org/W2019135282",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W3026380955",
    "https://openalex.org/W3211504873",
    "https://openalex.org/W2618692808",
    "https://openalex.org/W2270364989",
    "https://openalex.org/W2969873034",
    "https://openalex.org/W1986975620"
  ],
  "abstract": "We present Latin BERT, a contextual language model for the Latin language, trained on 642.7 million words from a variety of sources spanning the Classical era to the 21st century. In a series of case studies, we illustrate the affordances of this language-specific model both for work in natural language processing for Latin and in using computational methods for traditional scholarship: we show that Latin BERT achieves a new state of the art for part-of-speech tagging on all three Universal Dependency datasets for Latin and can be used for predicting missing text (including critical emendations); we create a new dataset for assessing word sense disambiguation for Latin and demonstrate that Latin BERT outperforms static word embeddings; and we show that it can be used for semantically-informed search by querying contextual nearest neighbors. We publicly release trained models to help drive future work in this space.",
  "full_text": "Latin BERT:\nA Contextual Language Model for Classical Philology\nDavid Bamman\nSchool of Information\nUniversity of California, Berkeley\nPatrick J. Burns\nDepartment of Classics\nUniversity of Texas at Austin\nAbstract\nWe present Latin BERT, a contextual language model for the Latin language, trained on 642.7 million words\nfrom a variety of sources spanning the Classical era to the 21st century. In a series of case studies, we illustrate\nthe affordances of this language-speciﬁc model both for work in natural language processing for Latin and in using\ncomputational methods for traditional scholarship: we show that Latin BERT achieves a new state of the art for\npart-of-speech tagging on all three Universal Dependency datasets for Latin and can be used for predicting missing\ntext (including critical emendations); we create a new dataset for assessing word sense disambiguation for Latin and\ndemonstrate that Latin BERT outperforms static word embeddings; and we show that it can be used for semantically-\ninformed search by querying contextual nearest neighbors. We publicly release trained models to help drive future\nwork in this space.\n1 Introduction\nThe rise of contextual language models (Peters et al., 2018; Devlin et al., 2019) has transformed the space of natural\nlanguage processing, raising the state of the art for a variety of tasks—including parsing (Mrini et al., 2019), word\nsense disambiguation (Huang et al., 2019) and coreference resolution (Joshi et al., 2020)—and enabling new ones\naltogether. While BERT was initially released with a model for English along with a multilingual model trained on\naggregated data in 104 languages (mBERT), subsequent work has trained language-speciﬁc models for Chinese (Cui\net al., 2019), French (Martin et al., 2020; Le et al., 2019), Russian (Kuratov and Arkhipov, 2019), Spanish (Cañete\net al., 2020) and at least 14 other languages, demonstrating substantial improvements for several NLP tasks compared\nto an mBERT baseline (Nozza et al., 2020). In this work, we contribute to this growing space of language-speciﬁc\nmodels by training a BERT model for the Latin language.\nIn many ways, research in Latin (and Classics in general) has been at the forefront of computational research for\nhistorical languages (Berti, 2019). Some of the earliest digitized corpora in the humanities concerned Latin works\n(Roberto Busa’s Index Thomisticus of the works of Thomas Aquinas); and one of the ﬂagships of early work in the\ndigital humanities, the Perseus Project (Crane, 1996; Smith et al., 2000), is structured around a large-scale digital\nlibrary of primarily Classical texts. At the speciﬁc intersection of Classics and NLP, Latin has been the subject of\nseveral dependency treebanks (Bamman and Crane, 2006; Haug and Jøhndal, 2008; Passarotti and Dell’Orletta, 2010;\nCecchini et al., 2020) and other lexico-semantic resources (Mambrini et al., 2020; Short, 2020), and is the focus of\nmuch work on individual components of the NLP pipeline, including lemmatizers, part-of-speech taggers, and mor-\nphological analyzers, among others (for overviews, see McGillivray (2014) and Burns (2019)). This work on corpus\ncreation and annotation as well as the development of NLP tools has enabled literary-critical work on problems relevant\nto historical-language texts, including uncovering instances of intertextuality in Classical texts (Coffee et al., 2012;\nMoritz et al., 2016; Coffee, 2018) and stylometric research on genre and authorship (Dexter et al., 2017; Chaudhuri\net al., 2019; Köntges, 2020; Storey and Mimno, 2020).\nResearch in Latin has also made use more speciﬁcally of recent advancements in word embeddings, primarily using\nstatic word representations such as word2vec (Mikolov et al., 2013) to drive work in this space. In the context of NLP,\nthis work includes using lemmatized word embeddings on synonym tasks (Sprugnoli et al., 2019) as well as using\na variety of embeddings strategies to improve the tasks of lemmatization and POS tagging (Sprugnoli et al., 2020)\n1\narXiv:2009.10053v1  [cs.CL]  21 Sep 2020\nand in particular to improve cross-temporal and cross-generic performance on these tasks (Celano et al., 2020; Bacon\net al., 2020; Straka et al., 2020). Bloem et al. (2020) look at the performance of Latin word embedding models in\nlearning domain-speciﬁc sentence representations, speciﬁcally sentences taken from Neo-Latin philosophical texts. In\nliterary critical contexts, Bjerva and Praet (2015, 2016) and Manjavacas et al. (2019) have used embeddings to model\nintertextuality and allusive text reuse. Distributional semantics has received attention elsewhere in Classics, including\nthe work of Rodda et al. (2019), which uses an Ancient Greek vector space model to explore semantic variation in\nHomeric formulae. It should also be noted that Latin is often included in large, multilingual NLP studies (Ammar\net al., 2016); the existence of Vicipaedia, a Latin-language version of Wikipedia, has led to the language’s inclusion\nin published multilingual embedding collections, including FastText (Grave et al., 2018) and mBERT (Devlin et al.,\n2019).\nIn this work, we expand on this existing focus on word representations to build a new BERT-based contextual\nlanguage model for Latin, trained on 642.7 million tokens from a range of sources, spanning the Classical era through\nthe present. Our work makes the following contributions:\n• We openly publish a new BERT model for Latin, trained on a dataset of 642.7 million tokens.\n• We demonstrate new state-of-the-art performance for Latin POS tagging on all three Universal Dependency\ndatasets.\n• We create a new dataset for assessing word sense disambiugation in Latin, using data from theLatin Dictio-\nnary of Lewis and Short (1879).\n• We illustrate the affordances of Latin BERT for applications in NLP and digital Classics with four case\nstudies, including text inﬁlling and ﬁnding contextual nearest neighbors.\nCode and data to support this work can be found at https://github.com/dbamman/latin-bert.\n2 Corpus\nSource Tokens\nCorpus Thomisticum 14.1M\nInternet Archive 561.1M\nLatin Library 15.8M\nPatrologia Latina 29.3M\nPerseus 6.5M\nLatin Wikipedia 15.8M\nTotal 642.7M\nTable 1: Corpus for Latin BERT.\nContextual language models demand large corpora for pre-\ntraining: English (Devlin et al., 2019) and Spanish (Cañete\net al., 2020), for example, are trained on 3 billion words,\nwhile the French CamemBERT model is trained on 32.7 bil-\nlion (Martin et al., 2020). While Latin is a historical language\nand is comparatively less resourced than modern languages,\nthere are extant works written in the language covering a time\nperiod of over twenty-two centuries—from 200 BCE to the\npresent—resulting in a wealth of textual data (Stroh, 2007;\nLeonhardt, 2013). In order to capture this variation in us-\nage, we leverage data from several sources: texts from the\nPerseus Project, which primarily covers the Classical era; the\nLatin Library, which covers the full chronological scope of\nthe language; the Patrologia Latina, which covers ecclesiastical writers from the 3rd century to the 13th century CE;\nthe Corpus Thomisticum, which covers the (voluminous) writings of Thomas Aquinas; Latin Wikipedia (Vicipaedia),\nwhich contains articles on a wide variety of subjects, including contemporary subjects likeStar Wars, Serena Williams,\nand Wikipedia itself, written in Latin; and texts from the Internet Archive (IA), which contain a total of 1 billion words\nspanning works published between roughly 200 BCE and 1922 CE (Bamman and Smith, 2011). The Internet Archive\ntexts are OCR’d scans of books and contain varying OCR quality; in order to use only data with reasonable quality,\nwe retain only those books where at least 40% of tokens are present in a vocabulary derived from born-digital texts.\nTable 1 presents a summary of the corpus and its individual components; since the texts from the Internet Archive are\nnoisier than the other subcorpora, we uniformly upsample all non-IA texts to train on a balance of approximately 50%\nIA texts and 50% non-IA texts.\nWe tokenize all texts using the same Latin-speciﬁc tokenization methods in the Classical Language Toolkit (John-\nson, 2020), both for delimiting sentences and tokenizing words; the CLTK word tokenizer segments enclitics from\ntheir adjoining word so that arma virumque cano (“I sing of arms and the man”) is tokenized into [ arma], [virum],\n2\n[-que], [cano]. Since BERT operates on subtokens rather than word tokens, we learn a Latin-speciﬁc WordPiece tok-\nenizer using the tensor2tensor library from this training data, with a resulting vocabulary size of 32,895 subword types.\nThis method tokenizes audentes fortuna iuvat (“fortune favors the daring”) into the sequence [audent, es], [fortuna],\n[iuvat] in order for representations to be learned for subword units rather than for the much larger space of all possible\ninﬂectional variants of a word, which substantially reduces the representation space for highly inﬂected languages like\nLatin. In the experiments that follow, we generate a BERT representation for a token comprised of multiple WordPiece\nsubtokens (such as audentes above) by averaging its component subtoken representations.\n3 Training\nOur Latin BERT model contains 12 layers and a hidden dimensionality of 768; we pre-train it with whole word\nmasking using tensorﬂow on a TPU for one million steps. Training took approximately 5 days on a TPU v2, and\ncost $540 on Google Cloud (at $4.50 per TPU v2 hour). We set the maximum sequence length to 256 WordPiece\ntokens and trained with a batch size of 256. Details on other hyperparameter settings can be found in the Appendix.\nWe convert the resulting tensorﬂow checkpoint into a BERT model that can be used by the HuggingFace library; the\ntrained model and code to support it can be found at https://github.com/dbamman/latin-bert.\n4 Analyses\nA Latin-speciﬁc contextual language model offers a range of affordances for work both in Classics NLP (improving\nthe state of the art for various NLP tasks for the domain of Latin) and in using computational methods to inform\ntraditional scholarly analysis. We present four case studies to illustrate these possibilities: improving POS tagging for\nLatin (yielding a new state-of-the-art for the UD datasets); predicting editorial reconstructions of texts; disambiguating\nLatin word senses in context; and enabling contextual nearest neighbor queries—that is, ﬁnding speciﬁc passages\ncontaining words that are used in similar contexts to a given query.\n4.1 POS tagging\nBERT has been shown to learn representations that encode many aspects of the traditional NLP pipeline, including\nPOS tagging, parsing, and coreference resolution (Tenney et al., 2019; Hewitt and Manning, 2019). To explore the\ndegree to which Latin BERT can be useful for the individual stages in NLP, we focus on POS tagging. POS tagging is\nan important component in much work in Latin NLP, providing the scaffolding for dependency parsing and detecting\ntext reuse (Moritz et al., 2016), and providing a focus for the 2020 EvaLatin shared task (Sprugnoli et al., 2020).\nTo understand how contextual representations in BERT capture morphosyntactic information, we can examine\na case study of the ambiguity present in the frequent word form cum, a homograph with two distinct meanings: it\nis used both as a preposition appearing with a nominal in the ablative case (meaning with) and as a subordinating\nconjunction (meaning when, because, although, etc.). To illustrate the degree to which raw BERT representations\nnaturally encode this distinction, we sample 100 sentences containing cum as a preposition (ADP) and 100 sentences\nwith it as a subordinating conjunction (SCONJ) from the Index Thomisticus Treebank (Passarotti and Dell’Orletta,\n2010), run all sentences through Latin BERT, and generate a representation of each instance of cum as the ﬁnal layer\nof BERT at that token position (each cum is therefore represented as a 768-dimensional vector). In order to visualize\nthe results in two dimensions, we then carry out dimensionality reduction using t-SNE (Maaten and Hinton, 2008).\nFigure 1 illustrates the result: the representations ofcum as a preposition and as a subordinating conjunction are nearly\nperfectly separable, indicating that the distinction between the use of cum corresponding to these parts of speech\nis inherent in its contextual representation within BERT without any further training necessary to tailor it to POS\ntagging.1\n1Two of the instances of cum/ADP that cluster with SCONJ are in the collocation cum hoc; this collocation appears 17 times in the ITT data\nand in 14 of these instances cum is labelled SCONJ. The third instance of cum/ADP that clusters with SCONJ (Subdit autem, qui cum in forma Dei\nesset, non rapinam arbitratus est esse se aequalem Deo.) is mislabelled in the data; accordingly, it is in the correct cluster. As far as the instances of\ncum/SCONJ that cluster with ADP, one is followed by an ablative noun (cum actu), which perhaps affects its representation; the only other instance\nof cum actu in the ITT data has cum labelled as ADP. The second instance (Ergo cum aliae formae sint simplices, multo fortius anima. ) provides\nno clue as to its misclassiﬁcation.\n3\n-20\n-10\n0\n10\n20\n-10 0 10\nADP\nSCONJ\nremanent autem hujusmodi formae intelligibiles in \nintellectu possibili, cum actu non intelligit …\nvirtutes autem activae indigent et ad hoc, et ad \nsubveniendum aliis, cum quibus convivendum \nest\nanima, cum sit forma corporis, unitur corpori secundum suum esse\net cum hoc, ex lumine intellectualium \nsubstantiarum, illuminatur intellectus eius \nad eadem agenda …\nFigure 1: Part of speech distinctions for cum as preposition (ADP) vs. subordinating conjunction (SCONJ), along\nwith examples of each class.\nWhile this case study provides a measure of face validity on the ability of Latin BERT to meaningfully distinguish\nmajor sense distinctions for a frequent word—without explicitly being trained to do so—we can also test its capacity\nto be used for the speciﬁc task of POS tagging. To do so, we draw on three different dependency treebanks annotated\nwith morphosyntactic information: the Perseus Latin Treebank (Bamman and Crane, 2006, Perseus), containing works\nfrom the Classical period (18,184 training tokens); the Index Thomisticus Treebank (Passarotti and Dell’Orletta, 2010,\nIITB), containing works by Thomas Aquinas (293,305 training tokens); and the PROIEL treebank (Haug and Jøhndal,\n2008, PROIEL), containing both Classical and Medieval works (172,133 training tokens). We build a POS tagger by\nadding a linear transformation and softmax operation on top of the pre-trained Latin BERT model, and allowing all of\nthe parameters within the model to be ﬁne-tuned during training. For ITTB and PROIEL, early stopping was assessed\non performance on development data (lack of improvement after 10 iterations), while the Perseus model (which has\nno development split due to its size) was trained for a ﬁxed number of 5 epochs.\nWe compare performance with several alternatives. First, to contextualize performance with static word repre-\nsentations, we also train 200-dimensional static word2vec embeddings (Mikolov et al., 2013) for Latin using the\nsame training data as Latin BERT, and use these as trainable word representations in a bidirectional LSTM ( static\nembeddings below); to compare performance with a similar model that uses the multilingual mBERT—trained simul-\ntaneously on different versions of Wikipedia in many different languages—we report test accuracy from Straka et al.\n(2019). And to provide context from several other static systems at the the 2018 Shared Task on universal depen-\ndency parsing (which includes a subtask on POS tagging on these datasets), we report test accuracies from Smith et al.\n(2018), Straka (2018) and Boros et al. (2018).\nAs can be seen, Latin BERT generates a new state of art for POS tagging on these datasets, with the most dramatic\nimprovement coming in its performance on the small Perseus dataset (an improvement of 4.6 absolute points).\n4.2 Text inﬁlling\nOne of BERT’s primary training objectives ismasked language modeling—randomly selecting a word from an input\nsentence and predicting it from representations of the surrounding words. This inductive bias makes it a natural model\n4\nMethod Perseus PROIEL ITTB\nLatin BERT 94.3 98.2 98.8\nStraka et al. (2019) 90.0 97.2 98.4\nSmith et al. (2018) 88.7 96.2 98.3\nStraka (2018) 87.6 96.8 98.3\nStatic embeddings 87.8 95.2 97.7\nBoros et al. (2018) 85.7 94.6 97.7\nTable 2: POS tagging results.\nfor the task of text inﬁlling (Zhu et al., 2019; Assael et al., 2019; Donahue et al., 2020), in which a model is tasked\nwith predicting a word (or sequence of words) that has been elided in some context.\nPrevious work has largely used synthetic evaluations for this task—both for predicting missing words in English\nand Ancient Greek (Assael et al., 2019)—by randomly masking a word in a complete sentence and attempting to\npredict it; to more closely align this task with the scholarly practice of textual criticism, in which an editor reconstructs\na text that has been corrupted or is otherwise illegible, we create an evaluation set by exploiting orthographic markers\nof emendation—speciﬁcally, the angle brackets (< and >) typically used to mark “words ... added to the transmitted\ntext by conjecture or from a parallel source” (West, 1973, 80). In the following sentence, for example, an editor notes\nthat the word ter is a conjecture by surrounding it in angle brackets:\npopulus romanus <ter> cum carthaginiensibus dimicavit.2\nWe build a dataset of textual emendations by searching all texts in the Latin Library for single words at least two\ncharacters long surrounded by brackets in sentences ranging between 10 and 100 words in length. In order to ensure\nthat the emendation does not appear in BERT’s training data, we ﬁrst removed all sentences with single words in angle\nbrackets from the training data prior to training BERT, and removed any evaluation sentence whose 5-gram centered\non the conjecture appeared in the training data (since emended text may appear in other versions of the text without\nexplicit markers of the emendation). In the example above, if the 5-gram populus romanus ter cum carthaginiensibus\nappeared in the training data, we would exclude this example from evaluation.\nThe resulting evaluation dataset contains 2,205 textual emendations. We ﬁnd that Latin BERT is able to reconstruct\nthe human-judged emendation 33.1% of the time; in 62.2% of cases, the human emendation is in the top 10 predictions\nranked by their probability, and in 74.0% of cases, it is within the top 50. Table 3 illustrates several examples of\nsuccesses and failures for this task.\nLeft context Prediction Emendation Right context\npraetorius qui bello civili partes pompei se-\ncutus mori maluit quam superstes esse rei\npublicae publicae servienti.\nhanno et mago qui secundo primo punico bello cornelium consulem aput li-\nparas ceperunt.\ntiberis infestiore quam priore anno anno impetu inlatus urbi duos pontes, aediﬁcia\nmulta maxime circa ﬂumentanam portam\neuertit.\nbrachium enim tuum non dominus domini dixisset, si non dominum patrem et\ndominum ﬁlium intellegi vellet.\npostquam dederat universitati parem dig-\nnamque faciem, spiritum desuper, quo\npariter\nomnes omnia animarentur, inmisit.\nTable 3: Examples of inﬁlling textual emendations.\n2“The Roman people fought against the Carthaginians <three times>,” Ampelius,Liber Memorialis 46.\n5\nWord Probability\nsecundo 0.451\nprimo 0.385\ntertio 0.093\naltero 0.018\nprimi 0.012\npriore 0.012\nquarto 0.005\nsecundi 0.004\nprimum 0.002\nsuperiore 0.002\nTable 4: Candidate words ﬁlling “hanno et mago\nqui ___ punico bello cornelium consulem aput li-\nparas ceperunt,” ranked by their probability.\nIn addition to providing a single best prediction for a miss-\ning word, language models such as BERT produce a probabil-\nity distribution over the entire vocabulary, and we can use those\nprobabilities to generate a ranked list of candidates. Table 4 below\npresents one such list of ranked candidates to ﬁll the missing word\nin “hanno et mago qui ___ punico bello cornelium consulem aput\nliparas ceperunt” (“Hanno and Mago, who captured the consul\nCornelius at Lipari in the [blank] Punic War”); while Latin BERT\npredicts secundo as its best guess, the human emendation ofprimo\nalso ranks highly. This is especially interesting as it reﬂects to\nsome degree the editor’s decision-making process. According to\nEduard Wölfﬂin’s 1854 Teubner edition, Karl Halm addedprimo\nto a section of Ampelius’sLiber Memorialis on Carthaginian gen-\nerals and kings. His addition to the text clearly places the battle\nat Lipari during which Cornelius (i.e. Gnaeus Cornelius Scipio\nAsina) was captured as having taken place during the First Punic\nWar. Yet the collocation of Hanno et Mago (at least in the extant\ntexts available for use as training data; e.g. Livy 23.41, 28.1; Val. Max. 7.2 ext 16; Sil. Pun. 16.674) is more\nclosely associated with the Second Punic War and it is perhaps the case that Ampelius has misreported the subject of\nthis sentence. So with respect to contextual semantics, the left context connotes “Second Punic War” and the right\ncontext connotes “First Punic War.” Accordingly, Halm must draw on external, historical information (i.e. the dating\nof Lipari) to establish what semantic context alone cannot. Latin BERT, on the other hand, unable to draw on external\ninformation, makes a reasonable suggestion based on what is reported in Ampelius’s text and how his words relate to\nother texts.\n4.3 Word Sense Disambiguation\nMany word forms in Latin have multiple senses, both at the level of homographs (words derived from distinct lemmas,\nsuch as est, which can be derived both from the verbedo (“to eat”) and the far more commonsum (“to be”) and within\na single dictionary entry as well—the word in, for example, is a preposition that denotes both location within (where\nit appears with nouns in the ablative case) and movement towards (where it appears with nouns in the accusative\ncase). Most ambiguous words exhibit sense variation without direct impact on the morphosyntactic structure of the\nsurrounding sentence (where, for example, the word vir can refer to “a man” generally and also the more speciﬁc\n“husband”).\nWithin NLP, word sense disambiguation provides a mechanism for assigning a word in context to a choice of sense\nfrom a pre-deﬁned sense inventory. Broad-coverage WSD system are typically evaluated on annotated datasets such as\nthose in English from the Senseval and SemEval competitions (Raganato et al., 2017), where human annotators select\nthe appropriate sense for a word given its context of use within a sentence. While these datasets exist for languages\nlike English, they have yet to be created for Latin.\n0\n10000\n20000\n30000\n40000\n0 10 20 30\nlength\ncount\nFigure 2: Distribution of citation lengths in Lewis and Short.\nIn order to explore the affordances of BERT for\nword sense disambiguation, we create a new WSD\ndataset for Latin by mining sense examples from the\nLatin Dictionary of Lewis and Short (1879), which\nprovides both a sense inventory for Latin words\nand a set of attestations of those senses primarily\nin Classical authors. Figure 3 provides one such\nexample of the dictionary entry for the preposition\nin within this dictionary, illustrating its ﬁrst sense\nalong with attestations of its use. As ﬁgure 2 illus-\ntrates, the majority of example sentences are frag-\nmentary in nature, with 55% of them having a length\nfewer than 5 words. We build a dataset from this\nsource by selecting dictionary headwords that have\nat least two distinct major senses (denoted by “I.”\n6\nand “II.” typographically) that are supported by at least 10 example sentences, where each sentence is longer than 5\nwords to provide enough context for disambiguation. We transform the problem into a balanced binary classiﬁcation\nfor each headword by only selecting the ﬁrst two major senses, and balancing the number of examples for each sense\nto be equal. This results in a ﬁnal dataset comprised of 8,354 examples for a total of 201 dictionary headwords. We\ndivide this data into training (80%), development (10%) and test splits (10%) for each individual headword, preserving\nbalance across all splits.\nFigure 3: Selection from the Lewis and Short entry for in (Perseus Digital Library).\nWe evaluate Latin BERT on this dataset by ﬁne-tuning a separate model for each dictionary headword; the number\nof training instances per headword ranges from 16 (8 per sense) to 192 (96 per sense); 59% of headwords have 24\nor fewer training examples. For comparison, we also present the results of a 200-dimensional bidirectional LSTM\nwith static word embeddings as input. For both models, we determine the number of epochs to train on based on\nperformance on development data, and report accuracy on held-out test data.\nTable 5 presents these results: while random choice would result in 50% accuracy on this balanced dataset, static\nembeddings achieve an overall accuracy of 67.3%. Even when presented with only a few training examples, however,\nLatin BERT is able to learn meaningful sense distinctions, yielding a 75.4% accuracy (an absolute improvement of 8.1\npoints over a non-contextual model).\nMethod Accuracy\nLatin BERT 75.4%\nStatic embeddings 67.3%\nRandom 50.0%\nTable 5: WSD results.\nWhile the absolute performance of these models is naturally not as strong as those for POS tagging, this reﬂects the\ndifﬁculty of word sense disambiguation as a task (where, for comparison, it is only recently that BERT-based models\nin English have been able to demonstrate signiﬁcant improvements over a most frequent sense baseline (Huang et al.,\n2019)). This experimental design also points the way for similar work in other Classical languages like Ancient\nGreek, which have similar lexica (such as the LSJ Greek-English Lexicon (Liddell et al., 1996)) that contain a variety\nof examples for each dictionary sense. And while we focus in this evaluation on citation examples longer than ﬁve\nwords, this work could be signiﬁcantly expanded to include far more training and evaluation examples by retrieving\nfull-text examples from the fragmentary citations.\n4.4 Contextual Nearest Neighbors\nOne ﬁnal case study that we can examine is the use of contextual word embeddings to ﬁnd similar passages to a query.\nWhile static word embeddings like word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014) allow for\nthe comparison of nearest neighbors, similarity in those models is only scoped over word types, and not over speciﬁc\ninstances of those words as tokens in context. Finding similar tokens in context would enable a range of applications in\ndigital Classics, including discovering instances of intertextuality (where, for example, the contextual representations\n7\nfor tokens in Ovid’sarma gravi numero violentaque bella parabam / edere(“I was getting ready to publish something\nin a serious meter about arms and violent wars”) may bear some similarity to Vergil’sarma virumque cano), surfacing\nexamples for pedagogy (where an instructor may want to ﬁnd examples of ablative absolute constructions in extant\nLatin texts by ﬁnding passages that are similar tohis verbis dictis(“with these words having been said”)), or suggesting\nto the textual critic additional apposite “parallel” passages when reconstructing texts. We illustrate the potential here by\ngenerating Latin BERT representations for 16 million tokens of primarily Classical Latin texts, and ﬁnding the nearest\nneighbor for a query token representation. While some work using BERT has calculated sentence-level similarity\nusing the representations for the starting [CLS] token (Qiao et al., 2019), we ﬁnd that comparing individual tokens\nwithin sentences yields much greater precision, since similar subphrases may be embedded within longer sentences\nthat are otherwise dissimilar.\nTables 6 and 7 present the contextual nearest neighbor results for two queries: tokens most similar toin in gallia est\nomnis divisa in partes tres (“The entirety of Gaul is divided into three parts”), and tokens most similar toaudentes in\naudentes fortuna iuvat. The most similar tokens to the ﬁrst query example within the rest of the corpus not only capture\nthe speciﬁc morphological constraints of this sense of in appearing with a noun in the accusative case (denoting into\nrather than within) but also broadly capture the more speciﬁc subsense of division into smaller components—including\ndivision into “parts” (partis, partes), “districts” (pagos), “provinces” (provincias), units of measurement (e.g. uncias)\nand “kinds” (genera).\nCosine Text Citation\n0.835 ager romanus primum divisus in partis tris, a quo tribus appellata titiensium ... Varro, Ling.\n0.834 in ea regna duodeviginti dividuntur in duas partes. Solin.\n0.833 gallia est omnis divisa in partes tres, quarum unam incolunt belgae, aliam ... Caes., BGall.\n0.824 is pagus appellabatur tigurinus; nam omnis civitas helvetia in quattuor pagos\ndivisa est.\nCaes., BGall.\n0.820 ea pars, quam africam appellavimus, dividitur in duas provincias, veterem et\nnovam, discretas fossa ...\nPlin. HN\n0.817 eam distinxit in partes quatuor. Erasmus, Ep.\n0.812 hereditas plerumque dividitur in duodecim uncias, quae assis appellatione con-\ntinentur.\nJustinian, Inst.\nTable 6: Most similar tokens to “in” in gallia est omnis divisa in partes tres.\nThe most similar tokens to audentes in audentes fortuna iuvat include other versions of the same phrase with\nlexical variation ( audentis fortuna iuvat, audaces fortuna iuvat) and instances of intertextuality ( audentes forsque\ndeusque iuvat). This example in particular not only illustrates the ability of BERT to capture meaningful similarities\nbetween different instances of the same word (such as in in the ﬁrst example), but also between words that exhibit\nsemantic similarity in spite of surface differences (audentes, audaces and audentis).\nCosine Text Citation\n0.926 audentes forsque deusque iuvat. Ov., Fast.\n0.864 audentis fortuna iuvat, piger ipse sibi opstat. Sen., Ep.\n0.846 audentes in tela ruunt. Vida, Scacchia Ludus\n0.840 audentes facit amissae spes lapsa salutis, succurruntque duci Vida, Scacchia Ludus\n0.837 ... audentis fortuna iuuat.’ haec ait, et cum se uersat ... Verg.,Aen.\n0.815 cedentes urget totas largitus habenas liuius acer equo et turmis ... Sil.\n0.809 sors aequa merentes respicit. Stat., Theb.\n0.801 nec jam pugnantes pro caesare didius audax hortari poterat, nec in ... May, Supp. Pharsaliae\n0.800 et alibi idem dixit,”audaces fortuna iuvat, piger sibiipsi obstat.” Albertanus of Brescia\n0.796 quae saeua repente uictores agitat leto iouis ira sinistri? Stat., Theb.\nTable 7: Most similar tokens to “audentes” in audentes fortuna iuvat.\n8\n5 Conclusion\nIn this work, we present the ﬁrst contextual language model for a historical language, training a BERT-based model\nfor Latin on 642.7 million tokens originally written over a span of 22 centuries (from 200 BCE to today). This large-\nscale language model has proven valuable for a range of applications, including speciﬁc subtasks in the NLP pipeline\n(including POS tagging and word sense disambiguation) and has the potential to be instrumental in driving forward\ntraditional scholarship by providing estimates of word probabilities to aid in the work of textual emendation and by\noperationalizing a new form of semantic similarity in contextual nearest neighbors.\nWhile this work presents Latin BERT and illustrates its usefulness with several case studies, there are a variety\nof directions this work can lead. One productive area of research within BERT-based NLP has been in designing\nprobing experiments to tease apart what the different layers of BERT have learned about linguistic structure; while the\nBERT representations used as features for several aspects of this work come from the single ﬁnal layer, this line of\nresearch may shed light on which layers are more appropriate for speciﬁc tasks (including the best representations for\nﬁnding comparable passages). While we focus on the tasks of POS tagging and word sense disambiguation in NLP,\ncontextual language models have shown dramatic performance improvements for a range of components in the NLP\npipeline, including syntactic parsing and named entity recognition. Given the availability of labeled datasets for these\ntasks, this is a direction worth pursuit. And ﬁnally, we only begin to address the ways in which speciﬁcally contextual\nmodels of language can inform traditional scholarship in Classics; while textual criticism and intertextuality represent\ntwo major areas of research where such models can be useful, there are many other potentially fruitful areas where\nbetter representation of the contextual meaning of words can be helpful, including Classical lexicography and literary\ncritical applications other than intertextuality detection. We leave it to future work to explore these new directions.\nAcknowledgments\nThe research reported in this article was supported by a grant from the National Endowment for the Humanities\n(HAA256044-17), along with resources provided by the Google Cloud Platform. The authors would also like to\nacknowledge the Quantitative Criticism Lab and the Institute for the Study of the Ancient World Library for their\nsupport.\nReferences\nWaleed Ammar, George Mulcaire, Yulia Tsvetkov, Guillaume Lample, Chris Dyer, and Noah A. Smith. 2016. Mas-\nsively multilingual word embeddings. arXiv preprint arXiv:1602.01925.\nYannis Assael, Thea Sommerschield, and Jonathan Prag. 2019. Restoring ancient text using deep learning: a case\nstudy on Greek epigraphy. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n6368–6375, Hong Kong, China. Association for Computational Linguistics.\nGeoff Bacon, Clayton Marr, and David Mortensen. 2020. Data-driven choices in neural part-of-speech tagging for\nLatin. In Proceedings of 1st Workshop on Language Technologies for Historical and Ancient Languages , pages\n111–113, Marseille.\nDavid Bamman and Gregory Crane. 2006. The design and use of a Latin dependency treebank. In Proceedings of the\nFifth Workshop on Treebanks and Linguistic Theories (TLT2006), pages 67–78, Prague. ÚFAL MFF UK.\nDavid Bamman and David Smith. 2011. Extracting two thousand years of Latin from a million book library. Journal\nof Computing and Cultural Heritage (JOCCH), 5(1).\nMonica Berti. 2019. Digital Classical Philology: Ancient Greek and Latin in the Digital Revolution . De Gruyter,\nBerlin.\nJohannes Bjerva and Raf Praet. 2015. Word embeddings pointing the way for late antiquity. In Proceedings of the\n9th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities , pages\n53–57.\n9\nJohannes Bjerva and Raf Praet. 2016. Rethinking intertextuality through a word-space and social network approach—\nthe case of Cassiodorus. Journal of Data Mining and Digital Humanities.\nJelke Bloem, Maria Chiara Parisi, Martin Reynaert, Yvette Oortwijn, Arianna Betti, Clayton Marr, and David\nMortensen. 2020. Distributional semantics for Neo-Latin. In Proceedings of 1st Workshop on Language Tech-\nnologies for Historical and Ancient Languages, pages 84–93, Marseille.\nTiberiu Boros, Stefan Daniel Dumitrescu, and Ruxandra Burtica. 2018. NLP-cube: End-to-end raw text processing\nwith neural networks. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to\nUniversal Dependencies, pages 171–179, Brussels, Belgium. Association for Computational Linguistics.\nPatrick J. Burns. 2019. Building a text analysis pipeline for Classical languages. In Digital Classical Philology:\nAncient Greek and Latin in the Digital Revolution, pages 159–176. De Gruyter, Berlin.\nJosé Cañete, Gabriel Chaperon, Rodrigo Fuentes, and Jorge Pérez. 2020. Spanish pre-trained BERT model and\nevaluation data. In to appear in PML4DC at ICLR 2020.\nFlavio Massimiliano Cecchini, Timo Korkiakangas, and Marco Passarotti. 2020. A new Latin treebank for Universal\nDependencies: Charters between ancient Latin and Romance languages. In Proceedings of the 12th Conference\non Language Resources and Evaluation (LREC 2020) , pages 933–942, Marseille. European Language Resources\nAssociation (ELRA).\nGiuseppe G. A. Celano, Clayton Marr, and David Mortensen. 2020. A gradient boosting–Seq2Seq system for Latin\nPOS tagging and lemmatization. In Proceedings of 1st Workshop on Language Technologies for Historical and\nAncient Languages, pages 119–123, Marseille.\nPramit Chaudhuri, Tathagata Dasgupta, Joseph P. Dexter, and Krithika Iyer. 2019. A small set of stylometric features\ndifferentiates Latin prose and verse. Digital Scholarship in the Humanities, 34(4):716–729.\nNeil Coffee. 2018. An Agenda for the Study of Intertextuality.Transactions of the American Philological Association,\n148(1):205–223.\nNeil Coffee, Jean-Pierre Koenig, Shakthi Poornima, Roelant Ossewaarde, Christopher Forstall, and Sarah Jacobson.\n2012. Intertextuality in the digital age. Transactions of the American Philological Association, 142(2):383–422.\nGregory Crane. 1996. Building a digital library: The Perseus Project as a case study in the humanities. InProceedings\nof the ﬁrst ACM international conference on Digital libraries, pages 3–10.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019. Pre-training with\nwhole word masking for Chinese BERT. arXiv preprint arXiv:1906.08101.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.\nJoseph P. Dexter, Theodore Katz, Nilesh Tripuraneni, Tathagata Dasgupta, Ajay Kannan, James A. Brofos, Jorge\nA. Bonilla Lopez, Lea A. Schroeder, Adriana Casarez, Maxim Rabinovich, Ayelet Haimson Lushkov, and Pramit\nChaudhuri. 2017. Quantitative criticism of literary relationships. Proceedings of the National Academy of Sciences,\n114(16):E3195–E3204.\nChris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to ﬁll in the blanks. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics , pages 2492–2501. Association for\nComputational Linguistics.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. 2018. Learning word vectors\nfor 157 languages. In Proceedings of the International Conference on Language Resources and Evaluation (LREC\n2018).\n10\nDag T. T. Haug and Marius L. Jøhndal. 2008. Creating a parallel treebank of the old Indo-European Bible translations.\nIn Proceedings of the Second Workshop on Language Technology for Cultural Heritage Data (LaTeCH 2008) ,\nMarrakesh.\nJohn Hewitt and Christopher D Manning. 2019. A structural probe for ﬁnding syntax in word representations. In Pro-\nceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers), pages 4129–4138.\nLuyao Huang, Chi Sun, Xipeng Qiu, and Xuanjing Huang. 2019. GlossBERT: BERT for word sense disambigua-\ntion with gloss knowledge. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n3509–3514, Hong Kong, China. Association for Computational Linguistics.\nKyle P Johnson. 2020. The Classical Language Toolkit. http://cltk.org/.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and Omer Levy. 2020. SpanBERT:\nImproving pre-training by representing and predicting spans. Transactions of the Association for Computational\nLinguistics, 8:64–77.\nYuri Kuratov and Mikhail Arkhipov. 2019. Adaptation of deep bidirectional multilingual transformers for Russian\nlanguage. arXiv preprint arXiv:1905.07213.\nThomas Köntges. 2020. Measuring philosophy in the ﬁrst thousand years of Greek literature. Digital Classics Online,\n6(2):1–23.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen,\nBenoît Crabbé, Laurent Besacier, and Didier Schwab. 2019. FlauBERT: Unsupervised language model pre-training\nfor French. arXiv preprint arXiv:1912.05372.\nJürgen Leonhardt. 2013. Latin: Story of a World Language. Harvard University Press, Cambridge, Massachusetts.\nCharles T. Lewis and Charles Short, editors. 1879. A Latin Dictionary. Clarendon Press, Oxford.\nHenry George Liddell, Robert Scott, Henry Stuart Jones, and Robert McKenzie, editors. 1996. A Greek-English\nLexicon, 9th edition. Oxford University Press, Oxford.\nLaurens van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. Journal of machine learning\nresearch, 9(Nov):2579–2605.\nFrancesco Mambrini, Flavio Massimiliano Cecchini, Greta Franzini, Eleonora Litta, Marco Carlo Passarotti, and Paolo\nRuffolo. 2020. LiLa: Linking Latin. Risorse linguistiche per il latino nel Semantic Web. Umanistica Digitale, 4(8).\nEnrique Manjavacas, Brian Long, and Mike Kestemont. 2019. On the Feasibility of Automated Detection of Allusive\nText Reuse. arXiv preprint arXiv:1905.02973.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Suárez, Yoann Dupont, Laurent Romary, Éric de la Clergerie,\nDjamé Seddah, and Benoît Sagot. 2020. CamemBERT: a tasty French language model. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pages 7203–7219. Association for Computational\nLinguistics.\nBarbara McGillivray. 2014. Methods in Latin Computational Linguistics. Brill, Leiden.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efﬁcient estimation of word representations in\nvector space. ICLR.\nMaria Moritz, Andreas Wiederhold, Barbara Pavlek, Yuri Bizzoni, and Marco Büchler. 2016. Non-literal text reuse\nin historical texts: An approach to identify reuse transformations and its application to Bible reuse. In Proceedings\nof the 2016 Conference on Empirical Methods in Natural Language Processing , pages 1849–1859, Austin, Texas.\nAssociation for Computational Linguistics.\n11\nKhalil Mrini, Franck Dernoncourt, Trung Bui, Walter Chang, and Ndapa Nakashole. 2019. Rethinking self-attention:\nTowards interpretability in neural parsing. arXiv preprint arXiv:1911.03875.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020. What the [MASK]? making sense of language-speciﬁc BERT\nmodels. arXiv preprint arXiv:2003.02912.\nMarco Passarotti and Felice Dell’Orletta. 2010. Improvements in Parsing the Index Thomisticus Treebank. Revision,\nCombination and a Feature Model for Medieval Latin. In Proceedings of the Seventh International Conference\non Language Resources and Evaluation (LREC’10) , Valletta, Malta. European Language Resources Association\n(ELRA).\nJeffrey Pennington, Richard Socher, and Christopher Manning. 2014. GloVe: Global vectors for word representation.\nIn Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) , pages\n1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer.\n2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers),\npages 2227–2237, New Orleans, Louisiana. Association for Computational Linguistics.\nYifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the behaviors of BERT in ranking.\narXiv preprint arXiv:1904.07531.\nAlessandro Raganato, Jose Camacho-Collados, and Roberto Navigli. 2017. Word sense disambiguation: A uniﬁed\nevaluation framework and empirical comparison. InProceedings of the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Volume 1, Long Papers, pages 99–110, Valencia, Spain. Association\nfor Computational Linguistics.\nMartina Astrid Rodda, Philomen Probert, and Barbara McGillivray. 2019. Vector space models of Ancient Greek word\nmeaning, and a case study on Homer. TAL, 60(3):63–87.\nWilliam M. Short. 2020. Latin WordNet. https://latinwordnet.exeter.ac.uk/.\nAaron Smith, Bernd Bohnet, Miryam de Lhoneux, Joakim Nivre, Yan Shao, and Sara Stymne. 2018. 82 treebanks, 34\nmodels: Universal dependency parsing with multi-treebank models. InProceedings of the CoNLL 2018 Shared Task:\nMultilingual Parsing from Raw Text to Universal Dependencies, pages 113–123, Brussels, Belgium. Association for\nComputational Linguistics.\nDavid A. Smith, Jeffrey A. Rydberg-Cox, and Gregory Crane. 2000. The Perseus Project: A digital library for the\nhumanities. Literary & Linguistic Computing, 15(1):15–25.\nRachele Sprugnoli, Marco Passarotti, Flavio M Cecchini, Matteo Pellegrini, Clayton Marr, and David Mortensen.\n2020. Overview of the EvaLatin 2020 evaluation campaign. In Proceedings of 1st Workshop on Language Tech-\nnologies for Historical and Ancient Languages, pages 105–110, Marseille.\nRachele Sprugnoli, Marco Passarotti, and Giovanni Moretti. 2019. Vir is to Moderatus as Mulier is to Intemperans:\nLemma embeddings for Latin. In Proceedings of the Sixth Italian Conference on Computational Linguistics, Bari,\nItaly.\nGrant Storey and David Mimno. 2020. Like Two Pis in a Pod: Author Similarity Across Time in the Ancient Greek\nCorpus. Journal of Cultural Analytics.\nMilan Straka. 2018. UDPipe 2.0 prototype at CoNLL 2018 UD shared task. In Proceedings of the CoNLL 2018\nShared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 197–207, Brussels, Belgium.\nAssociation for Computational Linguistics.\nMilan Straka, Jana Straková, and Jan Haji ˇc. 2019. Evaluating contextualized embeddings on 54 languages in POS\ntagging, lemmatization and dependency parsing. arXiv preprint arXiv:1908.07448.\n12\nMilan Straka, Jana Straková, Clayton Marr, and David Mortensen. 2020. UDPipe at EvaLatin 2020: Contextualized\nEmbeddings and Treebank Embeddings. In Proceedings of 1st Workshop on Language Technologies for Historical\nand Ancient Languages, pages 124–129, Marseille.\nWilfried Stroh. 2007. Latein ist tot, es lebe Latein!: kleine Geschichte einer grossen Sprache. List, Berlin.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In Proceedings\nof the 57th Annual Meeting of the Association for Computational Linguistics , pages 4593–4601, Florence, Italy.\nAssociation for Computational Linguistics.\nMartin Litchﬁeld West. 1973.Textual Criticism and Editorial Technique Applicable to Greek and Latin Texts. Teubner,\nStuttgart.\nWanrong Zhu, Zhiting Hu, and Eric Xing. 2019. Text inﬁlling. arXiv preprint arXiv:1901.00158.\n13\nAppendix\nattention probs dropout prob 0.1\ndirectionality bidi\nhidden act gelu\nhidden dropout prob 0.1\nhidden size 768\ninitializer range 0.02\nintermediate size 3072\nmax position embeddings 512\nnum attention heads 12\nnum hidden layers 12\npooler fc size 768\npooler num attention heads 12\npooler num fc layers 3\npooler size per head 128\npooler type ﬁrst token transform\ntype vocab size 2\nvocab size 32900\ntrain batch size 256\nmax seq length 256\nlearning rate 1e-4\nmasked lm prob 0.15\ndo whole word mask True\nTable 8: Latin BERT training hyperparameters.\n14",
  "topic": "Philology",
  "concepts": [
    {
      "name": "Philology",
      "score": 0.7979565262794495
    },
    {
      "name": "Linguistics",
      "score": 0.5289894342422485
    },
    {
      "name": "Computer science",
      "score": 0.42810940742492676
    },
    {
      "name": "Natural language processing",
      "score": 0.39464902877807617
    },
    {
      "name": "Philosophy",
      "score": 0.25519609451293945
    },
    {
      "name": "Sociology",
      "score": 0.16873425245285034
    },
    {
      "name": "Feminism",
      "score": 0.08482468128204346
    },
    {
      "name": "Gender studies",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 28
}