{
  "title": "OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on OffRoad environments",
  "url": "https://openalex.org/W3174634301",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5030725689",
      "name": "Anukriti Singh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5011222429",
      "name": "Kartikeya Singh",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5014252229",
      "name": "P. B. Sujit",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2916743882",
    "https://openalex.org/W3135360951",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3003922739",
    "https://openalex.org/W3203732962",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W3157525179",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2743627947",
    "https://openalex.org/W2024139303",
    "https://openalex.org/W3117459969",
    "https://openalex.org/W3014795891",
    "https://openalex.org/W3109808990",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2768282280"
  ],
  "abstract": "We present OffRoadTranSeg, the first end-to-end framework for semi-supervised segmentation in unstructured outdoor environment using transformers and automatic data selection for labelling. The offroad segmentation is a scene understanding approach that is widely used in autonomous driving. The popular offroad segmentation method is to use fully connected convolution layers and large labelled data, however, due to class imbalance, there will be several mismatches and also some classes may not be detected. Our approach is to do the task of offroad segmentation in a semi-supervised manner. The aim is to provide a model where self supervised vision transformer is used to fine-tune offroad datasets with self-supervised data collection for labelling using depth estimation. The proposed method is validated on RELLIS-3D and RUGD offroad datasets. The experiments show that OffRoadTranSeg outperformed other state of the art models, and also solves the RELLIS-3D class imbalance problem.",
  "full_text": "OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on\nOffRoad environments\nAnukriti Singh1, Kartikeya Singh 1, and P.B. Sujit\nAbstract— We present OffRoadTranSeg, the ﬁrst end-to-end\nframework for semi-supervised segmentation in unstructured\noutdoor environment using transformers and automatic data\nselection for labelling. The offroad segmentation is a scene\nunderstanding approach that is widely used in autonomous\ndriving. The popular offroad segmentation method is to use\nfully connected convolution layers and large labelled data,\nhowever, due to class imbalance, there will be several mis-\nmatches and also some classes may not be detected. Our\napproach is to do the task of offroad segmentation in a semi-\nsupervised manner. The aim is to provide a model where\nself supervised vision transformer is used to ﬁne-tune of-\nfroad datasets with self-supervised data collection for labelling\nusing depth estimation. The proposed method is validated\non RELLIS-3D and RUGD offroad datasets. The experiments\nshow that OffRoadTranSeg outperformed other state of the\nart models, and also solves the RELLIS-3D class imbalance\nproblem.\nI. I NTRODUCTION\nAn autonomous outdoor navigation [1], [2] system for\nunstructured environment [3] can be built using semantic\nsegmentation network. It has applications in areas like in-\nspection, exploration, rescue, and reconnaissance. But scene\nsemantic segmentation in off-road environments is challeng-\ning due to absence of a deﬁned boundaries between classes\nand terrain structure. This leads to challenges like class im-\nbalance when algorithms are deployed in real environments.\nHowever, class imbalance problem is highly environment\nspeciﬁc and becomes more severe when its within a dataset,\nlike in RELLIS-3D [4]. This makes semantic segmentation in\noff-road classes challenging as compared to urban environ-\nments. Presently, Convolutional Neural Networks (CNNs) [5]\nin semantic segmentation [6] have achieved state-of-the-art\nresults for unstructured outdoor environments like RELLIS-\n3D. However, CNNs require dense annotated datasets, which\nis costly and takes time specially in off-road environments\nwhere classes that are only visible in few frames can be\nimportant for autonomous decision making and navigation\nplanning.\nCNNs typically fail to produce accurate masks when the\ncontent of the images are complicated. For example in\nRELLIS-3D off-road dataset Jiang et al. [4] used HRNet\nfor semantic segmentation. They successfully segmented\nclasses like sky, grass, tree, etc. but there were some ex-\nceptions like log, water, and building where the mIOU\nwas 0. Guan et al. [7] and Viswanath et al. [8] performed\nsegmentation after classifying groups of terrain classes based\non traversal regions. [7] used group attention network with\nThe authors are with the IISER Bhopal. ∗ Equal contribution.\ntransformers for segmentation and [8] used color clustering.\nHowever, since both of them pooled classes they did not have\nto face the challenge of class imbalance. Recently, it was\nproposed to use transformers for vision based tasks [9] like\nsemantic segmentation [10], [11], [12] after their success in\nNatural Language Processing(NLP). Jin et al. [13] showed\nhow transformers based model can perform better for se-\nmantic segmentation on two datasets: RUGD and Cityscape.\nAlthough, their method was completely supervised and re-\nquired all the images in the dataset to be annotated. Caron\net al. [14] proposed DINO to use self supervised pretraining\non vision transformers, which can beneﬁt segmentation as\nwell as classiﬁcation tasks in vision. DINO is pretrained on\nImageNet dataset and uses knowledge distillation [15].\nMain Results:\nAnnotations for off road datasets is particularly challeng-\ning since the boundaries are mixed and not well deﬁned,\nit takes a lot of time and increases stress leading to misla-\nbelling. In this paper, we present a novel semi-supervised\napproach for offroad semantic segmentation. The approach\ninvolves ﬁne-tuning the DINO network with no labelled\ndata and evaluating it with the automatic selected data.\nOur method is general and is particularly designed for un-\nstructured outdoor environments. Semi-supervised sequence\nsegmentation requires to provide the mask of the ﬁrst image\nin the sequence and the network segments the subsequent\nframes based on that. We instead propose to do an automatic\ndata selection, where the selected images are labelled instead\nof just the ﬁrst frame. In summary, the main contributions\nof our proposed method are :\n• A novel model for semi-supervised semantic segmen-\ntation in unstructured outdoor environment using self\nattention in vision transformers.\n• Completely solving class imbalance problem by ob-\ntaining mIoU value for all the classes in RELLIS-3D\ndataset. We also test our model on other offroad datasets\nlike RUGD and obtain IoU values for multiple classes\nbetter than those reported in RUGD [16].\n• Signiﬁcantly reducing the amount of labelled data re-\nquired for training and evaluating using an algorithm\nto automatically select the most diverse set of images\nper sequence. We used less than 25 annotated images\nduring training.\nWe evaluate our method on RELLIS-3D [4] which\ncontains 20 classes and 13, 556 images and RUGD\n[16] dataset with 24 classes and over 29, 000 images.\narXiv:2106.13963v1  [cs.CV]  26 Jun 2021\nSelf-Supervised Vision Transformer\nInput image\nAttention heads\nAutomatic labels selection\nSelected labels Segmented output\n(Ra)\nSemi-Supervised \nSegmentation\nFig. 1. Our framework consists of two stages: Fine-tuning self supervised vision transformer to get the attention maps and automatically selecting labelled\nimages for evaluating the network for semi supervised semantic segmentation\nOffRoadTranSeg outperforms the networks used in [4]\nand [16] in the semi supervised fashion.\nII. R ELATED WORK\nA. Semi-supervised semantic segmentation\nSegmentation is a widely studied area using convolutional\nneural network as well as the present growing transformer\nnetworks. The semantic segmentation tasks can be super-\nvised, semi supervised and self supervised. A number of\narchitectures have been proposed to perform segmentation,\nlike PSPnet [17], HRNETV2+OCR [18], BiSeNetV2 [19]\nin CNNs and DETR [12], SETR [11] in transformer based\nnetworks. However, the CNNs based network performs well\nin urban navigation where the roads and structure is deﬁned,\nthey lack in performance in offroad unstructured areas due\nto undeﬁned, overlapping boundaries and convoluted area.\nB. Transformers\nTransformers were initially proposed for their use in\nNatural Language Processing (NLP). With their increasing\ngood performance, transformers were then introduced in\ncomputer vision with CNNs, making a hybrid model in\nDETR [12]. Later ViT [9] was proposed which proved to be\na successful transformer network for vision tasks like object\nclassiﬁcation. Transformer based network have beneﬁts over\npurely CNN architectures since they have high accuracy\nwith less computation time for training. TrSeg[20] used\ntransformer based network for segmentation in urban area\ndataset like cityscape and offroad dataset RUGD , however\ntheir network performs supervised segmentation. DINO is\na self supervised network which predicts attention in the\nimages and can also be used in semantic segmentation. Our\nnetwork is based on DINO, which makes segmentation in\nunstructured environments semi supervised. It has explicit\ninformation about the semantic segmentation of an image,\nwhich does not emerge as clearly with supervised ViTs, nor\nwith convnets.\nC. Dataset\nNavigation is an important part in perception module of\nurban and unstructured offroad regions. Latest datasets on\noffroad are RELLIS-3D and RUGD which are especially\ntailored for semantic segmentation in a variety of natural,\nunstructured semi-urban areas. RELLIS-3D is the latest\ndataset which is hugely derived from RUGD and has addi-\ntionally added unique terrain classes which aren’t present\nin RUGD . Both these datasets have rich ontology and large\nannotations of their ground truths. RELLIS-3D dataset also\nincludes 3-D lidar data and their annotations. We use both\nthese latest datasets to evaluate the performance of our\nproposed framework.\nIII. O FFROAD TRAN SEG\nThe proposed OffRoadTranSeg architecture is illus-\ntrated in Fig. 2 and it contains two major component : Self-\nSupervised Transformer and Automatic Data Selection. With\nOffRoadTranSeg off-terrain unstructured environment is\nsemantically segmented using self supervised attention maps\nand automatic data selection. Our framework is motivated by\nDINO[14] a state of the art self supervised attention mech-\nanism based on student teacher transformer network which\nperformed well on ImageNet dataset and DA VIS-2017 video\ninstance segmentation benchmark [21]. This framework takes\nthe advantage of attention in transformers and addresses\nthe class imbalance problem which was introduced with\nRELLIS-3D dataset in semantic segmentation of offroad\nimages. The details of our OffRoadTranSeg model for\nsemi supervised semantic segmentation will be described in\nthe following subsections.\nA. Transformer\nOffRoadTranSeg is based on a fully transformer-\nbased architecture, partially borrowed from knowledge\ndistillation[15] and self-supervised approaches used in\nDINO. Knowledge distillation has two network: student\nnetwork and a teacher network. The student network SΘs is\ntrained to match the output of a given teacher network SΘt,\nparameterised by Θs and Θt respectively. When given an\ninput image x, probability distribution Ps and Pt from both\nthe networks are calculated as an output over the dimension\nK. The output of the network, probability P is normalized\n(a) Raw Image\n (b) Ground Truth (c) HRNet (d) Ours\nRELLIS-3D\n(a) Raw Image\n (b) Ground Truth\n (c) TrSeg\n (d) OursRUGD\nsky treebush rubblelog grassbarrier gravelbuilding pole\nFig. 2. Both the columns highlight the raw image, ground truth, performance of SOTA algorithm and our approach on RELLIS-3D and RUGD dataset.\nWe observe that OffRoadTranSeg is able to segment all the classes of the image as present in the raw image.\nIoU Distribution for RELLIS-3D onOffRoadTranSeg\nSky Bush Building Log Grass Person Tree Asphalt Rubble Mud Fence Puddle Concrete Barrier Vehicle Object Pole\n97% 96% 92% 96% 92% 40% 82% 97% 77% 97% 88% 92% 98% 90% 98% 77% 87%\nIoU Distribution for RELLIS-3D on HRNet\n98% 78% 1% 0% 93% 89% 91% 67% 77% 96% 88% 64% 92% 67% 42% 50% 49%\nTABLE I\nCLASS -WISE IOU DISTRIBUTION FOR RELLIS-3D DATASET ON OffRoadTranSeg AND HRN ET.\nClass Dirt RockbedSand Grass Log Bicycle Tree Mulch Pole Fence Bush Sky Sign VehicleRock ConcreteBridge AsphaltGravel building\nIoU 88% 94% 92% 97% 89% 91% 82% 86% 94% 90% 76% 96% 88% 72% 86% 88% 91% 96% 89% 93%\nTABLE II\nCLASS -WISE IOU DISTRIBUTION FOR RUGD DATASET ON OffRoadTranSeg .\nClass Dirt RockbedSand Grass Log Bicycle Tree Mulch Pole Fence Bush Sky Sign VehicleRock ConcreteBridge Asphalt Gravel building\nIoU 0% 10% 27% 77% 10% - 83% 45% 19% 45% 27% 81% 11% 60% 9% 83% 0% 13% 38% 73%\nTABLE III\nCLASS -WISE IOU DISTRIBUTION FOR RUGD DATASET TAKEN FROM THEIR BENCHMARK METHOD 4(R ESNET50+U PER NET).\nusing a softmax function described below:\nPs(x)(i) = exp(SΘs(x)(i)/Ts)∑K\nk=1 exp(SΘs(x)(k)/Ts)\n(1)\nWhere the temperature parameter Ts > 0 controls the\nsharpness of the output distribution. The similar equation\nholds true for probability distribution Pt. In DINO both\nthe student and the teacher architecture S share the same\nnetwork with different set of parameters, Θs and Θt. This\nself supervised vision transformer network has two types of\nbackbone architectures f, ViT[9] and ResNet-50[22] and a\nprojection head h. For unstructured terrain image dataset\nwe use both these networks as backbone. The projection\nhead h : g = hf, which works best for this network\nconsists of a three layer multi-layer perceptron with 2048\nhidden dimension followed by l2 normalization and a weight\nnormalized fully connected layer. Also, The DINO network\ndoes not have batch normalizations (BN) by default.\nB. Automatic Data selection (ADS)\nSelf-supervised monocular depth estimation(SDE) [23]\na type of label selection method which makes the process\nof selecting the image to annotate automatic. Recently,\nmanually labelling data is starting to get replaced with self\nsupervised learning. Segmentation is tightly connected with\ndepth estimation. The leverage of this is taken to improve\nsemantic segmentation in unstructured environments with\nself supervised depth estimation. Labelling images in\noffroad is particularly hard because it requires ﬁne grained\nsegmentation. With this approach we come up with a way\nwhich requires comparatively less number of annotated\nimages and improve the performance of segmentation. Self\nsupervised depth estimation is used as a pretext task for\nour offroad data, which has N number of total samples and\nNs are selected for annotation. Here we denote, R as total\nnumber images in the data, RA as the subset of images\nselected for annotation and RU as the un-selected images\nsubset. Initially, all data is un-selected and hence, R= RU\nand selected images for annotation is empty. For the\nselection of images we ﬁrst do diversity sampling, to make\nsure most diverse set of images are selected and which cover\ndifferent scenes in off-trail image datasets. The set of images\nchoosen for annotation should be diverse enough so that they\nrepresent the entire dataset. The intermediate layer of SDE\nnetwork calculates features ΦSDE\nj over which we calculate\nL2 distance and do iterative farthest point sampling. At\nstep t, for each of the samples at that point, we choose the\nimage from RU which has the largest distance to the current\nannotation set RA. The selected sample of images RA is\niteratively expanded by moving one image at a time from\nRU to RA until the required number of images are collected:\ni= arg max\nIiϵRU\nmin\nIiϵRA\nΦSDE\ni − ΦSDE\nj\n (2)\nThe next selection criteria is uncertainty sampling. When\nthe model is trained on RA, the images with high uncertain-\nties near the decision boundary are favoured.For uncertainty\nsampling, we need to train and update the model with RA.\nIt is inefﬁcient to repeat this every time a new image is\nadded. To make it more efﬁcient, the selection is divided\ninto T steps and the model is trained T times. In each step t,\nthe required number of images are selected and moved from\nunselected subset to selected images for annotation. With\nuncertainty Sampling the aim is to select difﬁcult samples,\ni.e., samples in RU that the model trained on the current\nRA cannot handle well since the dataset of offroad is very\ndiverse and uncertain. The equation is then updated :\ni= arg max\nIiϵRU\nmin\nIiϵRA\nΦSDE\ni − ΦSDE\nj\n− λEE(i) (3)\nUsing automatic data selection we get a set of selected\nannotated data. To perform semi-supervised sequence seg-\nmentation a full mask of the object(s) of interest in the\nﬁrst frame of a video sequence has to be given and the\nnetwork produce the segmentation mask for that object(s) in\nthe subsequent frames. We leverage automatic data selection\nin semi supervised video segmentation, where instead of\ngiving annotation of only the ﬁrst frame we select the most\ndiversed frames nt from a sequence and divide it in batches.\nIV. E XPERIMENTS AND RESULTS\nIn this section we provide the implementation details\nof OffRoadTranSeg and describe the experiments per-\nformed to highlight the two main components of our ap-\nproach.\nA. Implementation Details\n1) Dataset:: We evaluate our OffRoadTranSeg on\ntwo recent off-road datasets RELLIS-3D and RUGD .\nRELLIS-3D dataset consists of 20 classes and 6,235 la-\nbelled images. The original resolution of the image is\n1920 × 1600 but for training purposes we downscale the\nimage to 512 × 512. With RELLIS-3D dataset we segment\nunique classes like rubble, mud, and man-made barriers\nwhich weren’t present in RUGD .\nArch mIoU Split\nViT small 88% Ra\nViT base 72% Ra\nViT tiny 66% Ra\nResNet50 36% Ra\nTABLE IV\nPERFORMANCE OF OffRoadTranSeg USING DIFFERENT\nARCHITECTURES .\nFramework mIoU\nOffRoadTranSeg 88%\nHRNet 67%\nFPN 87%\nTABLE V\nMEAN -IOUS FOR RELLIS-3D DATASET.\nRUGD dataset has 7,456 labelled images, that is for every\nﬁfth frame in the sequence. It has 24 different classes\nincluding categories like sign, rock and bridge. The original\nsize of the image is 688 × 550. Among these thousands\nof images We use the automatic data selection algorithm to\nselect the most diverse set of images and provide only their\nannotation for semi-supervised video sequence segmentation\nin both these datasets.\n2) Training Details: :We follow DINO [14] and ﬁne tune\nthe network on RELLIS-3D without labels. The training\nis completely unsupervised and is performed with AdamW\noptimizer[24] and 5 batch size using the ViT-8 model.\nThe output patch tokens are then evaluated on two offroad\ndatasets RELLIS-3D and RUGD for semantic segmenta-\ntion. For the semi supervised video sequence segmentation\ninstead of providing label of only the ﬁrst frame in the\nsequence, we run ADS for each sequence to select the\nmost diversed images and provide their label. For ADS we\nfollow [23], it has a slimmed network architecture with a\nResNet50 encoder and fewer decoder channels. We iterate\nthe number of selected sample for labelling as 25 and 744.\nFor both subsets, a student depth error is calculated when\nthe student depth network is trained from scratch for 4k and\n20k iterations.\nB. Ablation studies\nTo evaluate the performance of our proposed\nOffRoadTranSeg approach we calculate the individual\nclass IoU on RELLIS-3D dataset and compare it with\nthe the convolutional network based network HRNet.\nTable I shows that OffRoadTranSeg solves the class\nimbalance problem by giving the IoU value of 96% which\nwas 0% in CNN based network HRNet as provided in the\nRELLIS-3D benchmark. The comparison also shows that\nFramework mIoU\nOffRoadTranSeg 89%\nTABLE VI\nMEAN -IOUS FOR RUGD DATASET.\n(a) (b)\nFig. 3. (a) Training parameters for loss degradation of using different backbones in DINO transformer network in RELLIS-3D dataset. (b) Representation\nof mIoUs obtained from only transformer network and transformer with ADS.\n         RELLIS-3D\n      OursTransformer    Ground TruthRaw\nb1\nbn\nbn         \nRUGD\nb1\nFig. 4. Segmentation results: b1 represents the outputs from batch 1 of during semi supervised segmentation. Whereas, bn represents the outputs from\nthe last batch.\nwith our approach we get state of the art IoU values in\nmajority of the categories. We evaluate OffRoadTranSeg\non one another dataset, RUGD and the get the values for all\ndifferent categories as shown in table II compared with table\nIII. Figure 2 shows the visible differences in the performance\nof different networks. The RELLIS-3D dataset raw image\nhas 7 classes and HRNet is only able to segment 4\nof those classes whereas with OffRoadTranSeg we\nsegment all classes including log, rubble and barrier. The\nmean-IoU(mIoU) value for the two datasets in table V and\ntable VI shows that using transformer and automatic data\nselection can give the state of the art values in segmentation\nof unstructured outdoor environments. We also analyze\nhow different backbones of the self-supervised transformer\nnetwork impact the segmentation performance in the offroad\ndatasets by using four different architectures ViTsmall,\nViTbase, ViTtiny and ResNet − 50. We ﬁne-tune off-road\nRELLIS-3D dataset in an unsupervised fashion using\nvision transformer architectures and CNN architecture. The\nperformance is shown in table IV. Using our data selection\napproach with four different architectures we see that\nViTsmall gives the best performance with 88% mIoU and\non the other hand with ResNet-50 has 36% mIoU value.\nFigure 3 summarizes these results with different number of\nepochs in a form of graph.\nWe run ADS algorithm on each image sequence and select\nthe best set of 25 images in RELLIS-3D and RUGD .\nFor semi-supervised video segmentation instead of providing\nthe label for only the ﬁrst image in the sequence, we\nprovide label masks for all these 25 images and divide\nthe sequence into 25 batches. Since, these selected images\nare well diversed with all the features we see signiﬁcant\nimprovement in segmentation of offroad images. We show\nthe comparison in ﬁgure 4 where b1 represents ﬁrst batch\nof the sequence and bn represents the last batch. We used\nthe transformer based network DINO to evaluate video\nsegmentation on RELLIS-3D and RUGD , however, when\nthe label for only the ﬁrst image of the sequence is provided\nwe see good performance in b1 but the performance is poor as\nwe proceed to the last batch bn of the sequence. It shows that\nintroducing ADS algorithm with transformer based network\ncan signiﬁcantly improve the segmentation even for the last\nbatch, for example the last batch of the RUGD dataset in the\nﬁgure successfully segments log with grass and bush.\nV. C ONCLUSION\nIn this work, we have explored transformer based approach\nfor semi-supervised segmentation in unstructured outdoor en-\nvironments along with handling the class imbalance problem\nin two most famous offroad datasets. We show how the\nscene layout information present in the features of vision\ntransformer can be used for semi-supervised segmentation\nin offroad datasets. Annotation is an important part of the\nsegmentation task in off-trail environments because the per-\nformance of the network depends on how precisely the data\nis labelled. But on the same hand labelling an offroad dataset\nis particularly challenging because of undeﬁned boundaries.\nHence in our approach, we propose to use an Automatic Data\nSelection algorithm based on depth estimation which selects\nthe most feature rich images. This signiﬁcantly reduces\nthe number of annotated images needed while training and\nevaluating with increase in performance. The experiment\nresults show that by introducing OffRoadTranSeg we\nget state-of-the-art results in unstructured environments. But\nour approach also has some limitations, it requires a human\nin loop to annotate the images after getting the most diversed\nset of images selected for semi-supervised segmentation, for\nfuture work the labelling process in between the loop can be\nautomated which will further reduce the chances inaccuracy.\nREFERENCES\n[1] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benen-\nson, U. Franke, S. Roth, and B. Schiele, “The cityscapes dataset for\nsemantic urban scene understanding,” in Proc. of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2016.\n[2] H. A. Alhaija, S. K. Mustikovela, L. Mescheder, A. Geiger, and\nC. Rother, “Augmented reality meets computer vision: Efﬁcient data\ngeneration for urban driving scenes,” International Journal of Com-\nputer Vision, vol. 126, no. 9, pp. 961–972, 2018.\n[3] M. J. Procopio, J. Mulligan, and G. Grudic, “Learning terrain segmen-\ntation with classiﬁer ensembles for autonomous robot navigation in\nunstructured environments,” Journal of Field Robotics, vol. 26, no. 2,\npp. 145–175, 2009.\n[4] P. Jiang, P. Osteen, M. Wigness, and S. Saripalli, “Rellis-3d dataset:\nData, benchmarks and analysis,” arXiv preprint arXiv:2011.12954,\n2020.\n[5] Y . Lecun, L. Bottou, Y . Bengio, and P. Haffner, “Gradient-based\nlearning applied to document recognition,” Proceedings of the IEEE,\nvol. 86, no. 11, pp. 2278–2324, 1998.\n[6] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks\nfor semantic segmentation,” in 2015 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2015, pp. 3431–3440.\n[7] T. Guan, D. Kothandaraman, R. Chandra, and D. Manocha, “Ganav:\nGroup-wise attention network for classifying navigable regions in\nunstructured outdoor environments,”arXiv preprint arXiv:2103.04233,\n2021.\n[8] K. Viswanath, K. Singh, P. Jiang, S. Saripalli, et al., “Offseg: A seman-\ntic segmentation framework for off-road driving,”IEEE Conference on\nAutomation Science and Engineering, 2021.\n[9] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\n[10] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Trans-\nformer for semantic segmentation,” arXiv preprint arXiv:2105.05633,\n2021.\n[11] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr, et al., “Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers,” arXiv preprint\narXiv:2012.15840, 2020.\n[12] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable\ndetr: Deformable transformers for end-to-end object detection,” arXiv\npreprint arXiv:2010.04159, 2020.\n[13] Y . Jin, D. Han, and H. Ko, “Trseg: Transformer for semantic segmen-\ntation,” Pattern Recognition Letters, vol. 148, pp. 29–35, 2021.\n[14] M. Caron, H. Touvron, I. Misra, H. J ´egou, J. Mairal, P. Bojanowski,\nand A. Joulin, “Emerging properties in self-supervised vision trans-\nformers,” arXiv preprint arXiv:2104.14294, 2021.\n[15] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a\nneural network,” arXiv preprint arXiv:1503.02531, 2015.\n[16] M. Wigness, S. Eum, J. G. Rogers, D. Han, and H. Kwon, “A\nrugd dataset for autonomous navigation and visual perception in\nunstructured outdoor environments,” in 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS). IEEE, 2019,\npp. 5000–5007.\n[17] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing\nnetwork,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017, pp. 2881–2890.\n[18] J. Wang, K. Sun, T. Cheng, B. Jiang, C. Deng, Y . Zhao, D. Liu,\nY . Mu, M. Tan, X. Wang, et al., “Deep high-resolution representation\nlearning for visual recognition,” IEEE transactions on pattern analysis\nand machine intelligence, 2020.\n[19] C. Yu, C. Gao, J. Wang, G. Yu, C. Shen, and N. Sang, “Bisenet\nv2: Bilateral network with guided aggregation for real-time semantic\nsegmentation,” arXiv preprint arXiv:2004.02147, 2020.\n[20] Y . Jin, D. Han, and H. Ko, “Trseg: Transformer for semantic\nsegmentation,” Pattern Recognition Letters , vol. 148, pp. 29–\n35, 2021. [Online]. Available: https://www.sciencedirect.com/science/\narticle/pii/S016786552100163X\n[21] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbel ´aez, A. Sorkine-Hornung,\nand L. V . Gool, “The 2017 davis challenge on video object segmen-\ntation,” 2018.\n[22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning\nfor image recognition,” CoRR, vol. abs/1512.03385, 2015. [Online].\nAvailable: http://arxiv.org/abs/1512.03385\n[23] L. Hoyer, D. Dai, Y . Chen, A. K ¨oring, S. Saha, and L. Van Gool,\n“Three ways to improve semantic segmentation with self-supervised\ndepth estimation,” arXiv preprint arXiv:2012.10782, 2020.\n[24] I. Loshchilov and F. Hutter, “Fixing weight decay regularization in\nadam,” 2018.",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.8344703912734985
    },
    {
      "name": "Computer science",
      "score": 0.7625152468681335
    },
    {
      "name": "Artificial intelligence",
      "score": 0.703243613243103
    },
    {
      "name": "Transformer",
      "score": 0.6284303665161133
    },
    {
      "name": "Labeled data",
      "score": 0.5173827409744263
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5086022615432739
    },
    {
      "name": "Supervised learning",
      "score": 0.5006697177886963
    },
    {
      "name": "Semi-supervised learning",
      "score": 0.4396517276763916
    },
    {
      "name": "Machine learning",
      "score": 0.4336444139480591
    },
    {
      "name": "Class (philosophy)",
      "score": 0.4173169434070587
    },
    {
      "name": "Task (project management)",
      "score": 0.4163588285446167
    },
    {
      "name": "Engineering",
      "score": 0.07859143614768982
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}