{
  "title": "SBAT: Video Captioning with Sparse Boundary-Aware Transformer",
  "url": "https://openalex.org/W3034593503",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2040484645",
      "name": "Tao Jin",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2110765125",
      "name": "Siyu Huang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100106610",
      "name": "Ming Chen",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2144910405",
      "name": "Yingming Li",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2177285464",
      "name": "Zhongfei Zhang",
      "affiliations": [
        "Binghamton University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2904452845",
    "https://openalex.org/W2556388456",
    "https://openalex.org/W4299341000",
    "https://openalex.org/W2584992898",
    "https://openalex.org/W2905145027",
    "https://openalex.org/W2425121537",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W2914587137",
    "https://openalex.org/W2563399268",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2984862483",
    "https://openalex.org/W2139501017",
    "https://openalex.org/W2970212637",
    "https://openalex.org/W2302086703",
    "https://openalex.org/W2962990649",
    "https://openalex.org/W2164290393",
    "https://openalex.org/W2560346187",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2951390634",
    "https://openalex.org/W2607119937",
    "https://openalex.org/W2970060478",
    "https://openalex.org/W2558834163"
  ],
  "abstract": "In this paper, we focus on the problem of applying the transformer structure to video captioning effectively. The vanilla transformer is proposed for uni-modal language generation task such as machine translation. However, video captioning is a multimodal learning problem, and the video features have much redundancy between different time steps. Based on these concerns, we propose a novel method called sparse boundary-aware transformer (SBAT) to reduce the redundancy in video representation. SBAT employs boundary-aware pooling operation for scores from multihead attention and selects diverse features from different scenarios. Also, SBAT includes a local correlation scheme to compensate for the local information loss brought by sparse operation. Based on SBAT, we further propose an aligned cross-modal encoding scheme to boost the multimodal interaction. Experimental results on two benchmark datasets show that SBAT outperforms the state-of-the-art methods under most of the metrics.",
  "full_text": "SBAT: Video Captioning with Sparse Boundary-Aware Transformer\nTao Jin1 , Siyu Huang2, Ming Chen3, Yingming Li1\u0003, Zhongfei Zhang4\n1College of Information Science & Electronic Engineering, Zhejiang University, China\n2Baidu Research, China\n3Alibaba Group, China\n4Department of Computer Science, Binghamton University, USA\nfjint zju,yingmingg@zju.edu.cn, huangsiyu@baidu.com, black.cm@alibaba-inc.com,\nzzhang@binghamton.edu\nAbstract\nIn this paper, we focus on the problem of apply-\ning the transformer structure to video captioning ef-\nfectively. The vanilla transformer is proposed for\nuni-modal language generation task such as ma-\nchine translation. However, video captioning is a\nmultimodal learning problem, and the video fea-\ntures have much redundancy between different time\nsteps. Based on these concerns, we propose a novel\nmethod called sparse boundary-aware transformer\n(SBAT) to reduce the redundancy in video repre-\nsentation. SBAT employs boundary-aware pooling\noperation for scores from multihead attention and\nselects diverse features from different scenarios.\nAlso, SBAT includes a local correlation scheme to\ncompensate for the local information loss brought\nby sparse operation. Based on SBAT, we further\npropose an aligned cross-modal encoding scheme\nto boost the multimodal interaction. Experimen-\ntal results on two benchmark datasets show that\nSBAT outperforms the state-of-the-art methods un-\nder most of the metrics.\n1 Introduction\nRecently, the combination of vision and language attracts\nmore and more attention [You et al., 2016; Pan et al., 2017;\nAntol et al., 2015; Li et al., 2019]. Video captioning is a\nvaluable but challenging task in this topic, where the goal is\nto generate text descriptions for video data directly. The dif-\nﬁculties of video captioning mainly lie in the modeling of\ntemporal dynamics and the fusion of multiple modalities.\nEncoder-decoder structures are widely used in video cap-\ntioning [Shen et al., 2017; Aafaqet al., 2019; Peiet al., 2019;\nWang et al., 2019; Gan et al., 2017 ]. In general, the en-\ncoder learns multiple types of features from raw video data.\nThe decoder utilizes these features to generate words. Most\nencoder-decoder structures are built upon the long short-term\nmemory (LSTM) unit, however, LSTM has two main draw-\nbacks. First, LSTM-based decoder does not allow a parallel\nprediction of words at different time steps, since its hidden\n\u0003Corresponding author.\nDescription: someone is slicing a tomato\nFigure 1: An example of redundancy between video frames.\nstate is computed based on the previous one. Second, LSTM-\nbased encoder has insufﬁcient capacity to capture the long-\nrange temporal correlations.\nTo tackle these issues,[Chen et al., 2018] and [Zhou et al.,\n2018] proposed to replace LSTM with transformer for video\nunderstanding. Speciﬁcally, [Chen et al., 2018] used multi-\nple transformer-based encoders to encode video features and\na transformer-based decoder to generate descriptions. Simi-\nlarly, [Zhou et al., 2018] utilized transformer for dense video\ncaptioning, [Zhou et al., 2018] utilized a transformer-based\nencoder to detect action proposals and described them simul-\ntaneously with a transformer-based decoder. Different from\nLSTM, the self-attention mechanism in transformer corre-\nlates the features at any two time steps, enabling the global\nassociation of features. However, the vanilla transformer is\nlimited in processing video features with much temporal re-\ndundancy like the example in Fig. 1. In addition, the cross-\nmodal interaction between different modalities is ignored in\nthe existing transformer-based methods.\nMotivated by the above observations, we propose a novel\nmethod named sparse boundary-aware transformer (SBAT)\nto improve the transformer-based encoder and decoder ar-\nchitectures for video understanding. In the encoder, we em-\nploy sparse attention mechanism to better capture the global\nand local dynamic information by solving the redundancy be-\ntween consecutive video frames. Speciﬁcally, to capture the\nglobal temporal dynamics, we divide all the time steps into\nnchunks according to the gradient values of attention logits\nand select ntime steps with top-ngradient values. To capture\nthe local temporal dynamics, we implement self-attention be-\ntween r adjacent time steps. In the decoder, we also em-\nploy the boundary-aware strategy for encoder-decoder multi-\nhead attention. In addition, we implement cross-modal sparse\nattention following the self-attention layer to align multi-\nmodal features along temporal dimension. We conduct ex-\ntensive empirical studies on two benchmark video caption-\ning datasets. The quantitative, qualitative and ablation exper-\nimental results comprehensively reveal the effectiveness of\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n630\nour proposed methods.\nThe main contributions of this paper are three-folded:\n(1) We propose the sparse boundary-aware transformer\n(SBAT) to improve the vanilla transformer. We use boundary-\naware pooling operation following the preliminary scores of\nmultihead attention and select the features of different scenar-\nios to reduce the redundancy.\n(2) We develop a local correlation scheme to compen-\nsate for the local information loss brought by sparse opera-\ntion. The scheme can be implemented synchronously with\nthe boundary-aware strategy.\n(3) We further propose a cross-modal encoding scheme to\nalign the multimodal features along the temporal dimension.\n2 Related Work\nAs a popular variant of RNN, LSTM is widely used in exist-\ning video captioning methods. [Venugopalanet al., 2015] uti-\nlized LSTM to encode video features and decode words.[Yao\net al., 2015] integrated the attention mechanism into video\ncaptioning, where the encoded features are given different at-\ntention weights according to the queries of decoder. [Hori et\nal., 2017] further proposed a two-level attention mechanism\nfor video captioning. The ﬁrst level focuses on different time\nsteps, and the second level focuses on different modalities.\n[Long et al., 2018] and [Jin et al., 2019b] detected local at-\ntributes and used them as supplementary information. [Jin\net al., 2019a] introduced cross-modal correlation into atten-\ntion mechanism. Recently, [Chen et al., 2018] proposed to\nreplace LSTM with transformer in video captioning models.\nHowever, directly using transformer for video captioning has\nseveral drawbacks, i.e., the redundancy of video features and\nthe lack of multimodal interaction modeling. In this paper,\nwe propose a novel approach called sparse boundary-aware\ntransformer (SBAT) to address these problems.\n3 Transformer-based Video Captioning\nTransformer [Vaswani et al., 2017] is originally proposed for\nmachine translation. Due to the effectiveness and scalability,\ntransformer is employed in many other tasks including video\ncaptioning. A simple illustration of transformer-based video\ncaptioning model is shown in Fig. 2(a). The encoder and\ndecoder both consist of multihead attention blocks and feed-\nforward neural network.\n3.1 Encoder\nDifferent from the uni-modal inputs of machine translation,\nthe inputs of video captioning are typically multimodal. As\nshown in Fig. 2(a), two separate encoders process image and\nmotion features, respectively. We use I 2RTi\u0002d and M 2\nRTm\u0002dto denote the image and motion features, respectively.\nHere we take the process of image encoding as an example.\nThe self-attention layer is formulated as\nSelfAttention(I) = MultiHead(I;I;I ) (1)\nMultiHead(I;I;I ) = Cat(head1;:::; headh)W1 (2)\nwhere ”Cat” denotes concatenation operation, W1 2Rd\u0002d is\na trainable variable. Multihead attention is a special variant\nof attention, where each head is calculated as\nheadi = Attention(IWQ\ni ;IW K\ni ;IW V\ni ) (3)\nwhere WQ\ni , WK\ni , and WV\ni 2Rd\u0002 d\nh are also trainable vari-\nables, “Attention” denotes scaled dot-product attention:\nAttention(Q;K;V ) = softmax(QKT\np\nd\n)V (4)\nwhere dis dimension of Qand K. We adopt residual connec-\ntion and layer normalization after the self-attention layer:\nx= LayerNorm(I+ SelfAttention(I)) (5)\nEvery self-attention layer is followed by a feed-forward layer\n(FFN) that employs non-linear transformation:\nFFN(x) = max(0;xW2 + b2)W3 + b3 (6)\nI\n0\n= LayerNorm(x+ FFN(x)) (7)\nwhere W2 2Rd\u00024d, b2 2R4d, W3 2R4d\u0002d, and b3 2Rd\nare trainable variables. The encoded image features I\n0\nis the\noutput of an encoder block. The encoded motion features M\n0\nare calculated in the same way.\n3.2 Decoder\nThe decoder block consists of self-attention layer, enc-dec\nattention layer, and feed-forward layer. In the self-attention\nlayer, the word embeddings of different time steps associate\nwith each other, and we take the output features as queries.\nIn the enc-dec multihead attention layer, the query ﬁrst as-\nsociates image and motion features to get two context vec-\ntors respectively, then generates the words. The feed-forward\nlayer in decoder is the same as Eqns. 6 and 7. We also adopt\nresidual connection and layer normalization after all the lay-\ners of the decoder.\nSpeciﬁcally, we use E 2RTe\u0002d to denote the embeddings\nof target words. To predict the word yte at time step te, the\nself-attention layer is formulated as\nE\n0\n<te = LayerNorm(E<te + SelfAttention(E<te)) (8)\nwhere E<te 2R(te\u00001)\u0002d denotes the word embeddings of\ntime steps less than te. The enc-dec attention layer is:\nIte = MultiHead(E\n0\nte-1;I\n0\n;I\n0\n) (9)\nMte = MultiHead(E\n0\nte-1;M\n0\n;M\n0\n) (10)\nFollowing [Hori et al., 2017], we employ a hierarchical atten-\ntion layer for Ite and Mte:\nVte=LayerNorm(E\n0\nte-1 +MultiHead(E\n0\nte-1;Cte;Cte)) (11)\nCte = [Ite;Mte] (12)\nV\n0\nte = LayerNorm(Vte + FFN(Vte)) denotes the output of\nfeed-forward layer. We calculate the probability distributions\nof words as:\nPr(ytejy<te;I\n0\n;M\n0\n) = softmax(WpV\n0\nte + bp) (13)\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n631\nImage\n Motion\nSelf-\nAttention\nSelf-\nAttention\nFeed-\nForward\nFeed-\nForward\nSelf-\nAttention\nEnc-Dec\nAttention\nFeed-\nForward\nImage\n Motion\nSp-Self-\nAttention\nSp-Self-\nAttention\nFeed-\nForward\nFeed-\nForward\nSelf-\nAttention\nSp-Enc-Dec\nAttention\nFeed-\nForward\nSp-CM\nAttention\nSp-CM\nAttention\nFeed-\nForward\nFeed-\nForward\n(a) Vanilla Tranformer (b) SBAT \nN x N x N x N x\nFigure 2: (a) is the overall framework of Vanilla Transformer. It consists of multihead attention mechanism and feed-forward neural network.\nThe features of different modalities are processed separately and the queries of decoder associate these features to generate words.N denotes\nthe number of stacked blocks. (b) is the architecture of SBAT. It introduces sparse boundary-aware strategy (Sp) into all the multihead\nattention blocks in encoder and decoder. In addition, we learn cross-modal interaction after the ﬁrst feed-forward layer in an encoder block.\nwhere I\n0\nand M\n0\ndenote the encoded video features. The op-\ntimization goal is to minimize the cross-entropy loss function\ndeﬁned as accumulative loss from all the time steps:\nL= \u0000\nTeX\nte=1\nlog Pr(y\u0003\ntejy\u0003\n<te;I\n0\n;M\n0\n) (14)\nwhere y\u0003\nte denotes the ground-truth word at time step te.\n4 Sparse Boundary-Aware Attention\nConsidering the redundancy of video features, it is not appro-\npriate to compute attention weights using vanilla multihead\nattention. To solve the problem, we introduce a novel sparse\nboundary-aware strategy into the multihead attention. In Sec-\ntion 4.1, we introduce the sparse boundary-aware strategy.\nIn Section 4.2, we provide the analysis of sparse boundary-\naware strategy. In Section 4.3, we introduce the local corre-\nlation attention which compensates for the local information\nloss. In Section 4.4, we introduce an aligned cross-modal en-\ncoding scheme based on SBAT.\n4.1 Sparse Boundary-Aware Pooling\nWe employ sparse boundary-aware strategy following the\nscaled dot-product attention logits. Speciﬁcally, the original\nlogits are calculated as follows:\nP = QKT\np\nd\n(15)\nwhere Q 2RTq\u0002d and K 2RTk\u0002d denote the query and\nkey, respectively; d represents the dimension of Q and K.\nWe utilize Pi;j to represent the associated result of Qi 2Rd\nand Kj 2Rd. The discrete ﬁrst derivative of P in the second\ndimension is obtained as follows:\nP\n0\ni;j =\n\u001a\njPi;jj j = 0\njPi;j \u0000Pi;j\u00001j j 6= 0 (16)\nFor time step iof the query, we choose top-nvalues in P\n0\ni,\nsince the boundary of two scenarios always has high gradient\nvalue.\nH(P;n)i;j =\n(\nPi;j P\n0\ni;j \u0015ci\n\u00001 P\n0\ni;j <ci\n(17)\nwhere ci is the n-th largest value of P\n0\ni. We implement\nsoftmax function for the processed H(P;n).\nFurthermore, to keep the time steps with large original log-\nits, we deﬁne P\u0003\ni;j to replace P\n0\ni;j:\nP\u0003\ni;j = \u000bP\n0\ni;j + (1\u0000\u000b)Pi;j (18)\nP\n0\ni;j is a special variant of P\u0003\ni;j when \u000b= 1.\n4.2 Theoretical Analysis of Boundary-Aware\nPooling\nSuppose we randomly choose one time step of Q 2RTq\u0002d\nas query q 2Rd, the query q associates K 2RTk\u0002d at all\nthe time steps. The logits of scaled dot-product attention are\n[p1;p2;:::;p Tk] 2RTk. We calculate the attention weight of\neach time step as:\na\f = exp(p\f)\nPTk\ntk=1 exp(ptk)\n(19)\nTo the best of our knowledge, there are about3-5 scenarios\non average in a ten-second video clip at a coarse granularity,\nlike the example in Fig. 1. One-second clip usually con-\ntains 25 frames. Therefore, most frames in the same scenario\nare redundant. Existing methods sample the video to a ﬁxed\nnumber of frames or directly reduce the frame rate. Although\nsuch methods are effective to some extent, there is still much\nredundancy in the scenarios that have a large number of time\nsteps. The total attention weights of the scenarios with fewer\ntime steps may be inﬂuenced. Speciﬁcally, we divide Tk time\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n632\nsteps into two groups. The scenario one occupies T1 time\nsteps, the remaining T2 time steps belong to scenario two.\nSuppose that the features of different time steps in the same\nscenario are the same, we obtain the total weights of two sce-\nnarios as follows:\nAs\r = T\rexp(ps\r)\nP2\no=1 Toexp(pso)\n;\r 2f1; 2g (20)\nwhere As\r denotes the total weight of scenario\r, ps\r denotes\nthe associated logit. Suppose the query is related to scenario\ntwo (ps1 < ps2 ) and T1 > T2, the ratio of T1 to T2 may\ninﬂuence the total attention weights (A s1 and As2 ) of two\nscenarios.\nMore concretely, we assume that T1 is 0:75Tk and T2 is\n0:25Tk. As1 and As2 are calculated as:\nAs1 = 3 exp(ps1 )\n3 exp(ps1 ) + exp(ps2 ) (21)\nAs2 = exp(ps2 )\n3 exp(ps1 ) + exp(ps2 ) (22)\nif we apply sparse boundary-aware pooling strategy (P\n0\ni;j) for\nthe logits and sample one time step in each scenario. Both\nAs1 and As2 are transformed and the weight of scenario two\nobviously increases.\nA\n0\ns1 = exp(ps1 )\nexp(ps1 ) + exp(ps2 ) <As1 (23)\nA\n0\ns2 = exp(ps2 )\nexp(ps1 ) + exp(ps2 ) >As2 (24)\nHowever, when the query is related to scenario one (ps1 >\nps2 ). It is not appropriate to reduce the proportion of sce-\nnario one. Therefore, we deﬁne P\u0003\ni;j to replace P\n0\ni;j and select\nnot only the boundaries of scenarios, but also the time steps\nwith large original logits. Speciﬁcally, the number of selected\nsteps is n, we sample two boundaries in the two scenarios and\nthe remaining n\u00002 time steps belong to scenario one. A\n0\ns1 is\nobtained as:\nA\n0\ns1 = (n\u00001) exp(ps1 )\n(n\u00001) exp(ps1 ) + exp(ps2 ) (25)\nfor the increase from As1 in Eqn. 21 to A\n0\ns1 in Eqn. 25, we\njust need to ensure that n\u00001 >3.\nWhen the video clip has more than two scenarios, we also\ndivide them into two groups. One has the scenarios with\nlarger logits, the other has the remaining scenarios. The above\nanalysis of two scenarios is approximately applicable in this\nsituation.\n4.3 Local Correlation\nSince we employ sparse boundary-aware strategy for the\nattention logits, the local information between consecutive\nframes is ignored. We develop a local correlation scheme\nbased on the multihead attention to compensate for the in-\nformation loss. Formally, the original logits P are obtained\nfollowing Eqn. 16. The correlation scheme is\nHcorr(P;n)i;j =\n\u001a\nPi;j ji\u0000jj\u0014 r\n\u00001 ji\u0000jj>r (26)\nwhere r denotes the maximum distance of two frames and\nthe correlation size is 2r. In practice, the local correlation\nand boundary-aware correlation are utilized simultaneously.\n4.4 Cross-Modal Scheme\nExisting methods deal with different modalities separately\nin the encoder and ignore the interaction between different\nmodalities. Here, we propose an aligned cross-modal scheme\nbased on sparse boundary-aware attention. We divide the\nvideo into a ﬁxed number of video chunks and then extract\nimage and motion features from these chunks at the same in-\ntervals. Therefore, the feature vectors at the same step are\nextracted from the same video chunk. We directly apply\nour sparse boundary-aware attention to the aligned features.\nWhen the query is image modality, the key is motion modal-\nity, vice versa. Taking the former situation as an example,\nwe compute the results of vanilla and boundary-aware cross-\nmodal attentions as follows:\nCM-Attention(I;M ) = MultiHead(I;M;M ) (27)\nSp-CM-Attention(I;M ) = Sp-MultiHead(I;M;M )\n(28)\nwhere CM denotes cross-modal.\n5 Video Captioning with SBAT\nWe introduce the encoder-decoder structure combined with\nour sparse boundary-aware attention for video captioning. As\nshown in Fig. 2(b), we replace all the vanilla multihead at-\ntention blocks with boundary-aware attention blocks, except\nfor the self-attention block for target word embeddings. Dif-\nferent from the original structure, an additional cross-modal\nattention layer is adopted following the self-attention layer in\nthe encoder. In the decoder, we also introduce the boundary-\naware attention into the enc-dec attention layer, but we set \u000b\nto 0 in Eqn. 18 and do not use local correlation.\n6 Experimental Methodology\n6.1 Datasets and Metrics\nWe evaluate SBAT on two benchmark video captioning\ndatasets, MSVD [Chen and Dolan, 2011] and MSR-VTT [Xu\net al., 2016]. Both the datasets are provided by Microsoft\nResearch, and a series of state-of-the-art methods have been\nproposed based on these datasets in recent years. MSVD con-\ntains 1970 video clips and each video clip is about 10 to 25\nseconds long and annotated with about 40 English sentences.\nMSR-VTT is larger than MSVD with 10000 YouTube video\nclips in total and each clip is annotated with 20 English sen-\ntences. We follow the commonly used protocol in the previ-\nous work and evaluate methods under four standard metrics\nincluding BLEU, ROUGE, METEOR, and CIDEr.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n633\nMethod MSVD MSR-VTT\nBLEU4 ROUGE METEOR CIDEr BLEU4 ROUGE METEOR CIDEr\nVanilla Transformer 51.4 69.7 34.6 86.4 40.9 60.4 28.5 48.9\nSBAT (w/o CM) 52.4 71.2 35.0 87.0 42.0 60.8 28.5 50.1\nSBAT (w/o Local) 53.5 72.3 35.2 88.9 41.9 61.0 28.4 50.5\nSBAT (Sample) 51.3 71.9 35.2 88.6 42.3 61.0 28.7 51.0\nSBAT 53.1 72.3 35.3 89.5 42.9 61.5 28.9 51.6\nTable 1: Evaluation results of our proposed methods. Note that we reproduce the results of Vanilla Transformer (TVT [Chen et al., 2018]).\nDue to different learning rate strategy, our implementation achieves better performances than the original TVT on MSR-VTT.\n6.2 Data Preprocessing\nWe extract image features and motion features of video data.\nFor image features, we sample video data to 80 frames\nand use the pre-trained Inception-ResNet-v2 [Szegedy et al.,\n2017] model to obtain the activations from the penultimate\nlayer. For motion features, we divide the raw video data into\nvideo chunks centered on the sampled frames and use the pre-\ntrained I3D [Carreira and Zisserman, 2017 ] model to obtain\nthe activations from the last convolutional layer. We imple-\nment a mean-pooling operation along the temporal dimension\nto get the motion features. On MSR-VTT, we also employ\nglove embeddings of the auxiliary video category labels to\nfacilitate feature encoding.\n6.3 Experimental Details\nThe hidden size is set to 512 for all the multihead attention\nmechanisms. The numbers of heads and attention blocks are\n8 and 4, respectively. The value of \u000bis set to 0:8 in the en-\ncoder and 0 in the decoder. In the training phase, we use\nAdam [Kingma and Ba, 2014] algorithm to optimize the loss\nfunction. The learning rate is initially set to 0:0001. If the\nCIDEr on validation set does not improve over10 epochs, we\nchange the learning rate to 0:00002. The batch size is set to\n32. In the testing phase, we use the beam-search method with\na beam-width of 5 to generate words. We use the pre-trained\nword2vec embeddings to initialize the word vectors. Each\nword is represented as a 300-dimension vector.\nFigure 3: Effect of \u000bon MSR-VTT. We show the relative results on\nMETEOR and CIDEr. Speciﬁcally, we set \u000b= 0as the baseline.\n7 Experimental Results\n7.1 Impact of Sparse Boundary-Aware Attention\nWe ﬁrst evaluate the effectiveness of different variants of\nSBAT, as shown in Table 1. Vanilla Transformer and SBAT\ndenote the models in Fig. 2(a) and (b). SBAT (w/o CM) de-\nnotes the model without aligned cross-modal attention. SBAT\n(w/o Local) denotes the model without local correlation in\nthe encoder. SBAT (Sample) denotes the model with equidis-\ntant sampling for all the time steps, rather than our boundary-\naware operation.\nIn Table 1, Vanilla Transformer achieves relatively bad re-\nsults on both datasets. However, when we adopt boundary-\naware or equidistant sampling strategies in the multihead at-\ntentions, the performances are obviously improved. SBAT\nwith boundary-aware attention, local correlation, and aligned\ncross-modal interaction achieves promising results under all\nthe metrics. The comparison between SBAT (w/o CM) and\nSBAT shows that the cross-modal interaction provides useful\ncues for generating words. The comparison between SBAT\n(w/o Local) and SBAT shows that the local correlation can\nmake up the loss of local information. Comparing SBAT and\nSBAT (Sample), although equidistant sampling reduces the\nfeature redundancy to some extent, the ratio between different\nscenarios is not considered, while SBAT solves this problem\neffectively.\n7.2 Comparison of P\n0\nand P\nTo evaluate the impact ofP\u0003 and ﬁnd an appropriate ratio be-\ntween P\n0\nand P, we adjust the value of \u000bin Eqn. 18 based\non SBAT. The experimental results are shown in Fig. 3. Note\nthat we only adjust the value of \u000b in the encoder, and the\nvalue of \u000b in the decoder is always 0. We observe that P\u0003\nwith \u000b = 0:8 achieves the best performances on both ME-\nTEOR and CIDEr. In addition, only using original logits P\n(\u000b = 0) shows the worst performances, indicating that our\nproposed boundary-aware strategy P\n0\nis a signiﬁcant boost\nfor the transformer-based video captioning model.\n7.3 Comparison with State-of-the-art\nTable 2 shows the results of different methods on MSVD and\nMSR-VTT. For a fair comparison, we compare SBAT with\nthe methods which also use image features and motion fea-\ntures. The comparison methods include TVT [Chen et al.,\n2018], MGSA [Chen and Jiang, 2019 ], Dense Cap [Shen et\nal., 2017 ], MARN [Pei et al., 2019], GRU-EVE [Aafaq et\nal., 2019 ], POS-CG [Wang et al., 2019], SCN [Gan et al.,\n2017]. In Table 2, SBAT shows better or competitive per-\nformances compared with the state-of-the-art methods. On\nMSR-VTT, SBAT outperforms TVT, MGSA, Dense Cap,\nMARN, GRU-EVE, POS-CG on all the metrics. In partic-\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n634\nVanilla Transformer: two teams are playing football\nSBAT: a player is playing cricket\nGT: men are playing cricket\nVanilla Transformer: a woman is slicing an apple\nSBAT: a woman is slicing a tomato\nGT: someone is slicing a tomato\nVanilla Transformer: a woman is showing her nails\nSBAT: a woman is showing how to apply makeup\nGT: a woman is showing how to do her makeup\nVanilla Transformer: a group of people are playing sports\nSBAT: two men are wrestling\nGT: two men are wrestling\nFigure 4: Some qualitative results of the video clips on the test sets of MSR-VTT and MSVD. We provide the ground-truth description and\nthe generated descriptions of Vanilla Transformer and SBAT for each video clip.\nDataset Method B R M C\nMSR-\nVTT\nTVT 40.1 61.1 28.2 47.7\nMGSA 42.4 - 27.6 47.5\nDense Cap 41.4 61.1 28.3 48.9\nMARN 40.4 60.7 28.1 47.1\nGRU-EVE 38.3 60.7 28.4 48.1\nPOS-CG 42.0 61.1 28.1 49.0\nSBAT 42.9 61.5 28.9 51.6\nMSVD\nTVT 53.2 - 35.2 86.8\nSCN 51.1 - 33.5 77.7\nMARN 48.6 71.9 35.1 92.2\nGRU-EVE 47.9 71.5 35.0 78.1\nPOS-CG 52.5 71.3 34.1 88.7\nSBAT 53.1 72.3 35.3 89.5\nTable 2: Evaluation results of video captioning, where B, R, M, C\ndenote BLEU4, ROUGE, METEOR, CIDEr, respectively.\nular, SBAT achieves 51.6% on CIDEr, making an improve-\nment of 2.6% over POS-CG. On MSVD, SBAT outperforms\nSCN, GRU-EVE, POS-CG on all the metrics and has a better\noverall performance than TVT and MARN.\n7.4 Visualization of Attention Mechanism\nTo further illustrate the effectiveness of SBAT, we conduct a\ncase study and visualize the attention distributions of SBAT\nand Vanilla Transformer. In Fig. 5, we take a video clip for\nexample. Note that we only visualize the weights of image\nmodality for convenience, and we do not show the local at-\ntention weights. Fig. 5(a) shows that the attention weights of\nVanilla Transformer are dispersed and Vanilla Transformer\nhas a poor ability to detect the boundary of different scenar-\nios. While Fig. 5(b) shows that (1) SBAT has more sparse\nattention weights than Vanilla Transformer; (2) SBAT accu-\nrately detects the scenario boundaries.\n7.5 Qualitative Results\nFig. 4 shows several qualitative examples. We compare the\ndescriptions generated by Vanilla Transformer, SBAT, and\n(a) Vanilla Transformer (b) SBAT (No Local Weights)\nFigure 5: Visualization of attention mechanism. (a) and (b) denote\nVanilla Transformer and SBAT, respectively.xand yaxes both de-\nnote continuous video frames. The generated descriptions of two\nmethods are both “a man is shooting a gun”.\nground truth (GT). With the help of redundancy reduction and\na better usage of global and local information, SBAT gener-\nates more accurate descriptions that are close to GT.\n8 Conclusion\nIn this paper, we have proposed a new method called sparse\nboundary-aware transformer (SBAT) for video captioning.\nSpeciﬁcally, we have proposed sparse boundary-aware strat-\negy for improving the attention logits in vanilla transformer.\nCombined with local correlation and cross-modal encoding,\nSBAT can effectively reduce the feature redundancy and\ncapture the global-local video information. The quantita-\ntive, qualitative, and ablation experiments on two benchmark\ndatasets have demonstrated the advantage of SBAT.\nAcknowledgments\nThis work is supported in part by Science and Technology\nInnovation 2030 –“New Generation Artiﬁcial Intelligence”\nMajor Project No.(2018AAA0100904), National Key R&D\nProgram of China (No. 2018YFB1403600), NSFC (No.\n61672456, 61702448, U19B2043), Artiﬁcial Intelligence Re-\nsearch Foundation of Baidu Inc., the funding from HIKVision\nand Horizon Robotics, and ZJU Converging Media Comput-\ning Lab.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n635\nReferences\n[Aafaq et al., 2019] Nayyer Aafaq, Naveed Akhtar, Wei Liu,\nSyed Zulqarnain Gilani, and Ajmal Mian. Spatio-temporal\ndynamics and semantic attribute enriched visual encoding\nfor video captioning. In CVPR, 2019.\n[Antol et al., 2015] Stanislaw Antol, Aishwarya Agrawal, Ji-\nasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zit-\nnick, and Devi Parikh. Vqa: Visual question answering. In\nICCV, 2015.\n[Carreira and Zisserman, 2017] Joao Carreira and Andrew\nZisserman. Quo vadis, action recognition? a new model\nand the kinetics dataset. In CVPR, 2017.\n[Chen and Dolan, 2011] David L Chen and William B\nDolan. Collecting highly parallel data for paraphrase eval-\nuation. In ACL, 2011.\n[Chen and Jiang, 2019] Shaoxiang Chen and Yu-Gang Jiang.\nMotion guided spatial attention for video captioning,\n2019.\n[Chen et al., 2018] Ming Chen, Yingming Li, Zhongfei\nZhang, and Siyu Huang. Tvt: Two-view transformer net-\nwork for video captioning. In ACML, 2018.\n[Gan et al., 2017] Zhe Gan, Chuang Gan, Xiaodong He,\nYunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence\nCarin, and Li Deng. Semantic compositional networks for\nvisual captioning. In CVPR, 2017.\n[Hori et al., 2017] Chiori Hori, Takaaki Hori, Teng-Yok Lee,\nZiming Zhang, Bret Harsham, John R Hershey, Tim K\nMarks, and Kazuhiko Sumi. Attention-based multimodal\nfusion for video description. In ICCV, 2017.\n[Jin et al., 2019a] Tao Jin, Siyu Huang, Yingming Li, and\nZhongfei Zhang. Low-rank hoca: Efﬁcient high-order\ncross-modal attention for video captioning. arXiv preprint\narXiv:1911.00212, 2019.\n[Jin et al., 2019b] Tao Jin, Yingming Li, and Zhongfei\nZhang. Recurrent convolutional video captioning with\nglobal and local attention. Neurocomputing, 2019.\n[Kingma and Ba, 2014] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization. arXiv\npreprint arXiv:1412.6980, 2014.\n[Li et al., 2019] Xiangpeng Li, Jingkuan Song, Lianli Gao,\nXianglong Liu, Wenbing Huang, Xiangnan He, and\nChuang Gan. Beyond rnns: Positional self-attention with\nco-attention for video question answering. In AAAI, 2019.\n[Long et al., 2018] Xiang Long, Chuang Gan, and Gerard\nde Melo. Video captioning with multi-faceted attention.\nTACL, 2018.\n[Pan et al., 2017] Yingwei Pan, Ting Yao, Houqiang Li, and\nTao Mei. Video captioning with transferred semantic at-\ntributes. In CVPR, 2017.\n[Pei et al., 2019] Wenjie Pei, Jiyuan Zhang, Xiangrong\nWang, Lei Ke, Xiaoyong Shen, and Yu-Wing Tai.\nMemory-attended recurrent network for video captioning.\nIn CVPR, 2019.\n[Shen et al., 2017] Zhiqiang Shen, Jianguo Li, Zhou Su,\nMinjun Li, Yurong Chen, Yu-Gang Jiang, and Xiangyang\nXue. Weakly supervised dense video captioning. InCVPR,\n2017.\n[Szegedy et al., 2017] Christian Szegedy, Sergey Ioffe, Vin-\ncent Vanhoucke, and Alexander A Alemi. Inception-v4,\ninception-resnet and the impact of residual connections on\nlearning. In AAAI, 2017.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NIPS, 2017.\n[Venugopalan et al., 2015] Subhashini Venugopalan, Mar-\ncus Rohrbach, Jeffrey Donahue, Raymond Mooney,\nTrevor Darrell, and Kate Saenko. Sequence to sequence-\nvideo to text. In ICCV, 2015.\n[Wang et al., 2019] Bairui Wang, Lin Ma, Wei Zhang, Wen-\nhao Jiang, Jingwen Wang, and Wei Liu. Controllable video\ncaptioning with pos sequence guidance based on gated fu-\nsion network. In ICCV, 2019.\n[Xu et al., 2016] Jun Xu, Tao Mei, Ting Yao, and Yong Rui.\nMsr-vtt: A large video description dataset for bridging\nvideo and language. In CVPR, 2016.\n[Yao et al., 2015] Li Yao, Atousa Torabi, Kyunghyun Cho,\nNicolas Ballas, Christopher Pal, Hugo Larochelle, and\nAaron Courville. Describing videos by exploiting tempo-\nral structure. In ICCV, 2015.\n[You et al., 2016] Quanzeng You, Hailin Jin, Zhaowen\nWang, Chen Fang, and Jiebo Luo. Image captioning with\nsemantic attention. In CVPR, 2016.\n[Zhou et al., 2018] Luowei Zhou, Yingbo Zhou, Jason J\nCorso, Richard Socher, and Caiming Xiong. End-to-\nend dense video captioning with masked transformer. In\nCVPR, 2018.\nProceedings of the Twenty-Ninth International Joint Conference on Artiﬁcial Intelligence (IJCAI-20)\n636",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.8317116498947144
    },
    {
      "name": "Computer science",
      "score": 0.8241583704948425
    },
    {
      "name": "Transformer",
      "score": 0.6904498338699341
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5452786087989807
    },
    {
      "name": "Redundancy (engineering)",
      "score": 0.5154755711555481
    },
    {
      "name": "Modal",
      "score": 0.42798662185668945
    },
    {
      "name": "Pooling",
      "score": 0.4233781695365906
    },
    {
      "name": "Sparse approximation",
      "score": 0.4148733913898468
    },
    {
      "name": "Machine learning",
      "score": 0.3631529211997986
    },
    {
      "name": "Speech recognition",
      "score": 0.3519899249076843
    },
    {
      "name": "Voltage",
      "score": 0.16493752598762512
    },
    {
      "name": "Image (mathematics)",
      "score": 0.08012944459915161
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ]
}