{
    "title": "Evaluating the Experience of LGBTQ+ People Using Large Language Model Based Chatbots for Mental Health Support",
    "url": "https://openalex.org/W4391835981",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A757096288",
            "name": "Ma ZiLin",
            "affiliations": [
                "Harvard University Press"
            ]
        },
        {
            "id": null,
            "name": "Mei, Yiyang",
            "affiliations": [
                "Emory University"
            ]
        },
        {
            "id": "https://openalex.org/A3193950655",
            "name": "Long Yinru",
            "affiliations": [
                "Vanderbilt University"
            ]
        },
        {
            "id": null,
            "name": "Su, Zhaoyuan",
            "affiliations": [
                "University of California, Irvine"
            ]
        },
        {
            "id": "https://openalex.org/A2977214282",
            "name": "Krzysztof Z. Gajos",
            "affiliations": [
                "Harvard University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2977128309",
        "https://openalex.org/W3022528244",
        "https://openalex.org/W2942399136",
        "https://openalex.org/W2986862429",
        "https://openalex.org/W1982698797",
        "https://openalex.org/W2794349047",
        "https://openalex.org/W3092650000",
        "https://openalex.org/W3004483087",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W2109965062",
        "https://openalex.org/W2016126398",
        "https://openalex.org/W4200615999",
        "https://openalex.org/W1985749074",
        "https://openalex.org/W3014197742",
        "https://openalex.org/W2092822028",
        "https://openalex.org/W2032809392",
        "https://openalex.org/W2344555016",
        "https://openalex.org/W1987329826",
        "https://openalex.org/W1565166707",
        "https://openalex.org/W4283450405",
        "https://openalex.org/W1980039482",
        "https://openalex.org/W3158191355",
        "https://openalex.org/W3048959078",
        "https://openalex.org/W4380853703",
        "https://openalex.org/W3167975929",
        "https://openalex.org/W4283577397",
        "https://openalex.org/W2106701467",
        "https://openalex.org/W4382490931",
        "https://openalex.org/W2916945592",
        "https://openalex.org/W2066477535",
        "https://openalex.org/W2623779865",
        "https://openalex.org/W2466950015",
        "https://openalex.org/W2889335577",
        "https://openalex.org/W3212368439",
        "https://openalex.org/W4287662337",
        "https://openalex.org/W2985355520",
        "https://openalex.org/W3004917411",
        "https://openalex.org/W2588122555",
        "https://openalex.org/W4308687068",
        "https://openalex.org/W4318818619",
        "https://openalex.org/W2318725568",
        "https://openalex.org/W2038012469",
        "https://openalex.org/W2059431309",
        "https://openalex.org/W2560151242",
        "https://openalex.org/W3015322406",
        "https://openalex.org/W4289328212",
        "https://openalex.org/W2904985350",
        "https://openalex.org/W3127032480",
        "https://openalex.org/W4300687842",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W2972203331",
        "https://openalex.org/W4376874793",
        "https://openalex.org/W3090474112",
        "https://openalex.org/W2593751037",
        "https://openalex.org/W2922533032",
        "https://openalex.org/W4224981252",
        "https://openalex.org/W4309620156",
        "https://openalex.org/W2479926773",
        "https://openalex.org/W2897341104",
        "https://openalex.org/W2168841603",
        "https://openalex.org/W2036436550",
        "https://openalex.org/W2125568653",
        "https://openalex.org/W4386947838",
        "https://openalex.org/W2921434026",
        "https://openalex.org/W2981244006",
        "https://openalex.org/W4360846436",
        "https://openalex.org/W36990396",
        "https://openalex.org/W2790100033",
        "https://openalex.org/W2098024769",
        "https://openalex.org/W2108728177",
        "https://openalex.org/W2144570985",
        "https://openalex.org/W4379987733",
        "https://openalex.org/W2306242687",
        "https://openalex.org/W2418993857",
        "https://openalex.org/W2962883855",
        "https://openalex.org/W3119733760",
        "https://openalex.org/W4309617639",
        "https://openalex.org/W3185440237",
        "https://openalex.org/W3009936362",
        "https://openalex.org/W2609727312",
        "https://openalex.org/W4299397191",
        "https://openalex.org/W2922711788",
        "https://openalex.org/W2774228466",
        "https://openalex.org/W2985686011",
        "https://openalex.org/W4236521339",
        "https://openalex.org/W4366547784",
        "https://openalex.org/W3004984068",
        "https://openalex.org/W4318616691",
        "https://openalex.org/W3162614376",
        "https://openalex.org/W3185795116",
        "https://openalex.org/W2912500072",
        "https://openalex.org/W3101528469"
    ],
    "abstract": "LGBTQ+ individuals are increasingly turning to chatbots powered by large language models (LLMs) to meet their mental health needs. However, little research has explored whether these chatbots can adequately and safely provide tailored support for this demographic. We interviewed 18 LGBTQ+ and 13 non-LGBTQ+ participants about their experiences with LLM-based chatbots for mental health needs. LGBTQ+ participants relied on these chatbots for mental health support, likely due to an absence of support in real life. Notably, while LLMs offer prompt support, they frequently fall short in grasping the nuances of LGBTQ-specific challenges. Although fine-tuning LLMs to address LGBTQ+ needs can be a step in the right direction, it isn't the panacea. The deeper issue is entrenched in societal discrimination. Consequently, we call on future researchers and designers to look beyond mere technical refinements and advocate for holistic strategies that confront and counteract the societal biases burdening the LGBTQ+ community.",
    "full_text": "Evaluating the Experience of LGBTQ+ People Using Large\nLanguage Model Based Chatbots for Mental Health Support\nZilin Ma∗\nzilinma@g.harvard.edu\nIntelligent Interactive Systems Group\nHarvard School of Engineering and\nApplied Sciences\nAllston, MA, USA\nYiyang Mei∗\nyiyang.mei@emory.edu\nLaw School\nEmory University\nAtlanta, GA, USA\nYinru Long\nyinru.long@vanderbilt.edu\nPsychology and Human Development\nPeabody College\nVanderbilt University\nNashville, TN, USA\nZhaoyuan Su\nnick.su@uci.edu\nDonald Bren School of Information\nand Computer Sciences\nUniversity of California Irvine\nIrvine, CA, USA\nKrzysztof Z. Gajos\nkgajos@eecs.harvard.edu\nIntelligent Interactive Systems Group\nHarvard School of Engineering and\nApplied Sciences\nAllston, MA, USA\nABSTRACT\nLGBTQ+ individuals are increasingly turning to chatbots pow-\nered by large language models (LLMs) to meet their mental health\nneeds. However, little research has explored whether these chat-\nbots can adequately and safely provide tailored support for this\ndemographic. We interviewed 18 LGBTQ+ and 13 non-LGBTQ+\nparticipants about their experiences with LLM-based chatbots for\nmental health needs. LGBTQ+ participants relied on these chatbots\nfor mental health support, likely due to an absence of support in\nreal life. Notably, while LLMs offer prompt support, they frequently\nfall short in grasping the nuances of LGBTQ-specific challenges.\nAlthough fine-tuning LLMs to address LGBTQ+ needs can be a\nstep in the right direction, it isn’t the panacea. The deeper issue\nis entrenched in societal discrimination. Consequently, we call on\nfuture researchers and designers to look beyond mere technical\nrefinements and advocate for holistic strategies that confront and\ncounteract the societal biases burdening the LGBTQ+ community.\nCCS CONCEPTS\n• Human-centered computing →User studies.\nKEYWORDS\nLarge Language Models, Chatbot, Gender, Identity, LGBTQIA+\nHealth, Mental health, Stigma, Socio-technical AI\nACM Reference Format:\nZilin Ma, Yiyang Mei, Yinru Long, Zhaoyuan Su, and Krzysztof Z. Gajos.\n2024. Evaluating the Experience of LGBTQ+ People Using Large Language\n∗Equal contributions\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0330-0/24/05. . . $15.00\nhttps://doi.org/10.1145/3613904.3642482\nModel Based Chatbots for Mental Health Support. In Proceedings of the\nCHI Conference on Human Factors in Computing Systems (CHI ’24), May\n11–16, 2024, Honolulu, HI, USA. ACM, New York, NY, USA, 15 pages. https:\n//doi.org/10.1145/3613904.3642482\n1 INTRODUCTION\nThe increase in social isolation coupled with inadequate access to\nprofessional mental health services has led many to turn to large\nlanguage model (LLM) based chatbots in hopes of finding connec-\ntion and support for their mental wellbeing. Platforms like ChatGPT,\nReplika, Anima, Kajiwoto, and Character AI have gained immense\npopularity, with millions using them for immediate, discreet social\nand emotional support [75]. These LLM-based companions provide\ncomfort to those feeling lonely or in difficult situations by offering\nconversational engagement anytime and anywhere [69, 75]. The\nadvanced linguistic capabilities of LLM-based chatbots offer users\nmore context-aware and responsive interactions, distinguishing\nthem from the earlier pre-LLM chatbots [56].\nThe potential of LLM-based chatbots is most striking when con-\nsidering their impact on historically marginalized communities\nlike the LGBTQ+ (lesbian, gay, bisexual, transgender, queer, and/or\nquestioning) [46]. LGBTQ+ individuals face significantly higher\nrates of depression (57%), anxiety (70%), and suicidal ideation (41%)\ncompared to their heterosexual cis-gendered peers [ 77, 107]. Be-\nyond these alarming statistics, LGBTQ+ people also navigate a daily\nlandscape marred by discrimination, bullying, and stigma tied to\ntheir gender and sexual identities, and endure a glaring absence of\nrepresentation in the mainstream culture [77]. This lack of repre-\nsentation and systemic marginalization deter them from seeking\nprofessional therapeutic assistance, especially when there is a risk\nof encountering non-affirmative therapists [29, 77].\nAlthough LLM-based chatbots seem to offer a valuable and inclu-\nsive mental health resource for the LGBTQ+ community, potentially\nbridging gaps in traditional therapy accessibility [37, 69], their de-\nployment raises substantial concerns. Biases embedded in these\nchatbots can perpetuate harmful stereotypes. LGBTQ+ users, who\nare often underrepresented in the training datasets, can encounter\nunintentional reinforcement of damaging narratives with regard to\narXiv:2402.09260v1  [cs.HC]  14 Feb 2024\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ma and Mei, et al.\ntheir identities [9, 34]. Furthermore, as people’s reliance on these\nplatforms increases [69], there are growing apprehensions regard-\ning the chatbots’ ability to truly understand the nuances of LGBTQ+\nidentities and the depth of human emotions [34, 109]. Consequently,\nto investigate these challenges, we ask:\n•What benefits can LLM-based chatbots provide to LGBTQ+\npeople in terms of mental wellness support?\n•Do LGBTQ+ people have additional purposes of use for LLM-\nbased chatbots compared to non-LGBTQ+ people?\n•Can LLM-based chatbots meet LGBTQ+ people’s mental\nwellness needs regarding their identity?\nWe interviewed 31 participants (18 identifying as LGBTQ+ and\n13 as non-LGBTQ+) about their usage of LLM-based chatbots for\nmental wellness support. We specifically asked the LGBTQ+ partic-\nipants how LLM-based chatbots supported their mental wellness\nneeds regarding their LGBTQ+ identity. We had the following find-\nings:\n•For both LGBTQ+ and non-LGBTQ+ participants, LLM-based\nchatbots offer immediate support and accessibility, create a\nsafe environment for intimate conversations, foster strong\nemotional bonds between the chatbots and the users, and\nare useful for developing social skills.\n•For both LGBTQ+ and non-LGBTQ+ participants, the ease\nof usage and emotional bonding has the potential to encour-\nage adherence to therapy regimens when applied in mental\nhealth, but also risk over-reliance.\n•LGBTQ+ participants use chatbots due to a lack of real-life\nsupport, seeking guidance on topics like coping with dis-\ncrimination or seeking identity affirmation.\n•LGBTQ+ participants use LLM-based chatbots to rehearse\nLGBTQ+-specific experiences such as coming out and dating\nas an LGBTQ+ person.\n•LLM-based chatbots cannot completely address the nuances\nin the emotional needs of LGBTQ+ people due to their overly\ngeneralized responses.\n•LLM-based chatbots offer suggestions that might be ignorant\nof the ever-changing societal norms (e.g., coming out to\nunsupportive parents), such that if the users fully follow the\nadvice, they risk danger to themselves.\nOur results show that LLM-based chatbots have a long way to\ngo before they can fully address the needs of LGBTQ+ people’s\nmental health needs. Moreover, because we identified that the main\nmotivation for using LLM-based chatbots for mental health was\nthe lack of social support, we argue that designing solutions that\naddress the societal stigma against LGBTQ+ people should be prior-\nitized over optimizing LLMs on LGBTQ+ people’s needs. Therefore,\nwe recommend ways to improve LLMs for the specific use cases\nof LGBTQ+ people, and also possible socio-technical solutions to\naddress stigmas LGBTQ+ people face online.\n2 RELATED WORK\nThis section references societal norms, behaviors, and attitudes\nfound within contemporary Western cultures. It’s essential to note\nthat the literature summarized here may not necessarily reflect or\nencompass the nuances and perspectives of Asian, African, Latin\nAmerican, or even Eastern European cultures.\n2.1 LGBTQ+ People’s Online Experiences\nOnline technologies offer significant benefits to LGBTQ+ individ-\nuals, especially those who lack real-life support from family or\nfriends [25, 38, 46, 74, 103]. These platforms provide crucial access\nto interpersonal and systemic resources, as shown by the success\nof initiatives like The Trevor Project. Founded to prevent suicide\nand offer crisis intervention, The Trevor Project has amassed over\n2 million followers on platforms like X and Instagram [101]. Simi-\nlarly, social media networks like TikTok and Tumblr have become\nvital spaces for LGBTQ+ individuals to explore and express their\nsexual orientation and gender identity [27, 94]. In other cases, on-\nline technologies help LGBTQ+ people to navigate identity-related\nchallenges, engage with supportive communities, and access educa-\ntional resources about LGBTQ+ issues [73]. These online technolo-\ngies are crucial to LGBTQ+ people, as they continue to experience\ndisproportionate risks and limited access to support offline, includ-\ning at home, at school and in their communities [23, 38, 46, 73].\nHowever, online technologies can sometimes fall short of meet-\ning the needs of the LGBTQ+ community, as they do not center\nLGBTQ+ people in the design process [43]. For example, Tumblr’s\n2018 ban on “adult content” disproportionately affected transgen-\nder users [80]. Many transition-related posts were mistakenly cate-\ngorized as adult material, inadvertently marginalizing this group.\nSimilarly, YouTube’s policy of labeling LGBTQ+ content as “adult”\nhas further isolated these communities [3]. Facebook’s insistence\non real names fails to recognize the value of anonymity for LGBTQ+\nindividuals, which is indispensable for their safety and freedom [15].\nTo optimize monetization, many content creators, mostly non-\nLGBTQ+ members, sometimes resort to tactics like “queerbait-\ning” [78]. Queerbaiting is a marketing technique used to attract the\nLGBTQ+ audience by hinting at same-sex relationships or LGBTQ+\nthemes without actually depicting or confirming them. This tactic\nis often criticized for exploiting LGBTQ+ themes for commercial\ngain without providing meaningful representation [ 78]. Dating\nwebsites, while providing a means of connection for individuals,\nstill frequently perpetuate racism and ableism, excluding marginal-\nized groups within the LGBTQ+ community, such as queer people\nof color and those living with HIV [ 52, 63, 68]. Additionally, the\ndisproportionate prevalence of cyberbullying against queer indi-\nviduals compared to their heterosexual counterparts highlights\nthe significant challenges faced in online spaces by the LGBTQ+\ncommunity [20].\n2.2 Digital Mental Support Technology for\nLGBTQ+ Individuals\nThe LGBTQ+ community experiences greater mental health chal-\nlenges such as higher levels of depressive symptoms, engaging in\nmore non-suicidal-self-injury, and having more suicidal thoughts\nand behaviors compared with heterosexual, cisgender peers [4, 53,\n89, 100, 101, 107]. The stress of coming out also lead to increased\ndepressive and anxiety symptoms and suicidal ideation [22, 49, 77,\n86]. Minority stress theory suggests that structural stigma against\nLGBTQ+ people, interpersonal discrimination, and internalized\nstigma all exacerbate the mental health challenges of this popu-\nlation, resulting in feelings of alienation and distress [22, 47, 76].\nAdditionally, the frequent dismissal of LGBTQ+ youth experiences\nEvaluating the Experience of LGBTQ+ People Using LLM-Based Chatbots for Mental Health Support CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nas mere “teenage angst” [85] contributes to a sense of disconnection\nand isolation, inflicting feelings of being unloved or misunderstood\nwithin their support systems, or even more severe consequences\nsuch as homelessness [84]. Social support from family and friends\nis crucial for LGBTQ+ individuals to mitigate stress [ 19]. How-\never, LGBTQ+ people often report less perceived family support\nthan their heterosexual, cisgender peers and face challenges in peer\nrelationships [87].\nGiven the dismissal of their concerns and the lack of availability\nof LGBTQ+-specific mental health care, digital therapies, such as\nthose involving digital cognitive behavioral therapy (dCBT), have\nshown promise as an alternative mental health support avenue for\nLGBTQ+ individuals. By providing self-guided, affordable, accessi-\nble, and private mental health care, they address key barriers to tra-\nditional therapy, including long waiting times, extended treatment\nduration, and traveling costs [5, 50, 72]. Nonetheless, digital thera-\npies like dCBT demand a significant amount of commitment and\nself-monitoring [10, 33]. Other limitations such as low adherence\nrate, technical difficulties and sophistication, and privacy concerns\nsignificantly hinder effectiveness [10, 33].\nIn addition to the online delivery of mental health services, dig-\nital communities, especially those fostered on social media and\nassociated with LGBTQ+ organizations, have emerged as pivotal\nspaces supporting LGBTQ+ mental well-being [67]. They are fre-\nquently used by LGBTQ+ youth, providing emotional sustenance,\nguidance, and a sense of belonging [67, 71]. In addition, they also\noffer a safe milieu for self-expression and identity exploration, cre-\nating an oasis where shared experiences and mutual understanding\ncan bring solace [23].\nOnline platforms offer advice and guidance on societal challenges\nranging from addressing discrimination to identifying LGBTQ+-\nfriendly resources. This function is especially crucial for individuals\nlacking access to LGBTQ+ resources in real life or a supportive and\nintimate environment [85]. Furthermore, these platforms amelio-\nrate feelings of isolation that are prevalent among LGBTQ+ youth,\nparticularly for those who are still in the closet or are in less ac-\ncepting environments. Websites such as The Trevor Project and\nplatforms like LGBTQ+ forums on Reddit or specialized apps like\nTrevorSpace [102] have become sanctuaries for many LGBTQ+\nyouths. These spaces provide them with an opportunity to share\ntheir stories, listen to the experiences of others, and realize they’re\nnot alone. Such platforms often have features like chat services,\ncommunity boards, and resources specifically tailored to provide\npeer support and information. While online platforms offer valuable\nsocial support, it is important to note that they are not a substitute\nfor professional mental health services. These online platforms can\nhave varied content quality and have the potential to expose users\nto cyberbullying or negative comparisons due to a less strict code\nfor data privacy and protection mechanisms compared to working\nwith a therapist [7, 21].\n2.3 Mental Wellness Chatbots\n2.3.1 Pre-LLM chatbots for mental wellness. Before the emergence\nof LLMs, chatbot architecture primarily consisted of three approaches:\nrule-based, retrieval-based, and a combination of both [26, 108, 114].\nRule-based chatbots operate on predefined rules, linking user in-\nputs to specific responses [ 110]. Retrieval-based chatbots used\nmachine learning algorithms to choose responses from a preset\ndatabase according to user inputs [55, 66]. There were also gener-\native systems that were built on neural network architecture like\nSequence-to-Sequence (Seq2Seq) models [90, 91, 98, 112]. Although\ncapable of generating unique responses, these models were limited\nby the need for extensive training data, significant computational\npower, and the challenge of maintaining context in long conver-\nsations [12, 51, 62, 88]. Pre-LLM chatbots offered high control (to\nthe creators) due to their structured design. Their accessibility and\ninstant response features made pre-LLM chatbots popular in mental\nhealth applications. Research indicats that mental health chatbots\nhave had positive impacts in reducing symptoms of depression and\nanxiety, and enhancing therapeutic alliance, acceptability, and like-\nability, particularly during the COVID-19 pandemic [1, 2, 45, 81, 97].\nDespite the initial successes and widespread use of pre-LLM chat-\nbots in mental health applications, as evidenced by numerous stud-\nies on their acceptability and usability, there remains a significant\ngap in research specifically addressing their effectiveness in improv-\ning mental health outcomes. This lack of comprehensive research\npresents a challenge in fully understanding and evaluating the im-\npact of these chatbots in mental wellness care [14, 14, 24, 30, 37, 95].\nPrior research highlights that the success of mental wellness chat-\nbots largely depends on sociotechnical aspects and therapeutic\nrelationships [65]. Pre-LLM chatbots, given their technological lim-\nitations, often struggle to effectively address these crucial elements.\nSignificant drawbacks, such as limited linguistic or contextual un-\nderstanding, often led to unnatural or irrelevant conversations,\nreducing users’ willingness to engage with these chatbots, mak-\ning interactions less convincing and supportive, and potentially\nlimiting therapeutic benefits [11, 60, 82, 105]. Furthermore, these\nchatbots struggled to adapt and learn from user information, failing\nto cater to individual needs [58]. Consequently, chatbots frequently\nfall short of genuinely understanding and responding to emotional\nnuances. This issue is particularly pronounced among marginalized\ncommunities, such as LGBTQ+ individuals, who can feel alienated\nwhen these chatbots inadequately understand their unique chal-\nlenges and experiences [32].\n2.3.2 LLM-based Chatbots: Strengths and Weaknesses. To over-\ncome the limitations of pre-LLM chatbots, LLM-based chatbots\nhave shown promise in delivering more natural, context-aware,\nand flexible conversations. Employing extensive text datasets and\nprobabilistic word sequencing, models like ChatGPT are capable\nof generating varied responses that are attuned to conversational\ncontexts and subtleties. For LGBTQ+-related topics, some chatbots\ncan even mimic the expression of gender and sexualities [31]. One\nof the standout features of LLM-based chatbots is the capacity for\nfine-tuning the models, a process of parameter adjustment after\npre-training that allows for specialization in specific tasks or do-\nmains [116]. This adaptability mitigates the need for the manual\nconstruction of knowledge bases and rule tables, a previously es-\nsential step for rule-based pre-LLM chatbots. Moreover, the facility\nfor in-context learning in LLM offers the advantage of producing\nresponses relevant to the conversation history without the need for\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ma and Mei, et al.\nexplicit rule-based systems [28, 56]. These added abilities may im-\nprove chatbots’ interactivity, increasing therapeutic adherence [37].\nHowever, the very capabilities that make LLM-based chatbots\nadaptable and context-aware also come with their own sets of\nchallenges. The architecture of complex neural networks and trans-\nformers sometimes results in unpredictable and even harmful re-\nsponses [106]. This is particularly troubling in delicate areas such\nas mental wellness support. For example, some studies have shown\nthat LLM-based mental wellness chatbots are more inclined to give\ninsensitive feedback than human therapists, possibly exacerbating\nemotional turmoil for users [ 109]. Furthermore, LLMs’ propen-\nsity for generating hallucinated responses can mislead or confuse\nusers [61, 64]. These hallucinated responses, which are outputs that\nmay not be grounded in factual information or prior training data,\ncan be especially problematic when users are seeking accurate and\nreliable information or support.\nOne of the pressing issues with LLMs is their potential to harbor\nand propagate inherent biases, which can inadvertently promote\nnarratives that are socially concerning or detrimental. The root of\nthis problem lies in the non-diverse and potentially biased datasets\nused for training these models. The Internet, being the primary data\nsource for LLMs, does not necessarily reflect global diversity. For\nexample, Reddit, a widely-used platform, has a gender imbalance\nwith 67% of its U.S. user base being men [18]. Similarly, Wikipedia, a\nsignificant contributor to global knowledge, is predominantly male-\nauthored, with a staggering 84% of its contributors being male [48].\nAdding to this skewed representation, certain online moderation\npolicies can marginalize minority voices. A case in point is YouTube,\nwhere content from trans individuals discussing their gender and\nsexuality has faced demonetization [3]. These biases in data sources\ncan lead LLMs to inherit and perpetuate such imbalances. The\nCommon Crawl, a major training database, is rife with toxicity\nand hate speech [9]. Even when the filtered versions are used, they\nmay inadvertently offend and silence the voices of marginalized\ncommunities such as LGBTQ+ due to inherent limitations in the\nfiltering algorithms [ 104]. As a result, existing LLMs have been\nshown to contain stereotypical social biases [9, 41, 59, 92].\nFurthermore, a static dataset does not represent the changing so-\ncial dynamics. Societal events and movements like the Black Lives\nMatter campaign have led to more frequent updates on Wikipedia\nabout incidents of police brutality against Black individuals [104].\nOlder Wikipedia pages have been revised to provide more cohe-\nsive narratives over time, impacting the data that shapes LLMs [83].\nHowever, the prohibitive computational costs of training these large\nmodels make it challenging to update them frequently enough to\nreflect such evolving narratives. Even with fine-tuning approaches,\nkeeping these models current would require thoughtful curation\npractices to identify suitable data for reframings and methods to\nassess whether the fine-tuning accurately reflects new perspec-\ntives that challenge prevailing representations. Consequently, LLMs\ncarry the risk of reinforcing out-of-date or harmful stereotypes and\nbiases, especially if not updated to reflect these changing narra-\ntives [9]. Moreover, many LLMs lack the capacity for authentic\nhuman experience, which limits their true comprehension of the\ndaily dilemmas faced by LGBTQ+ individuals. For instance, while\nchatbots can mimic human language and express gender and sexu-\nality by drawing on their training data, they inherently differ from\nhuman conversational partners — they lack the authentic experi-\nence related to gender and sexuality [31]. This difference is mainly\ndue to their inability to replicate the flexibility and understanding\nthat comes from actual human experience.\nIn conclusion, LLM-based chatbots offer impressive linguistic\ncapabilities but also present unprecedented challenges. This raises\ncritical questions concerning the extent to which LLMs ameliorate\nthe limitations inherent in their pre-LLM counterparts. A particular\narea of interest is the application of these technologies for mental\nhealth support among LGBTQ+ individuals. While LLMs promise\nenhanced conversational fluidity and context awareness, it remains\ndebatable whether they successfully mitigate issues such as con-\nversational superficiality or accurately interpret subtle emotional\ncues. The intricacy of human emotional experience, coupled with\nthe nuances of gender and sexual orientation, creates a landscape\nthat may be too complex for LLMs to navigate proficiently [ 31].\nExisting general-purpose LLMs like ChatGPT are seldom fine-tuned\nfor mental health support, not to mention specifically for LGBTQ+\nmental health support, even though a significant number of users\nconsult them for emotional wellness [69]. In light of the potential\nability and limitations of LLMs, and the intricacies and nuances of\nLGBTQ+ mental wellness we hypothesize:\n•(H1) LLM-based chatbots offer a safe and accessible platform\nfor LGBTQ+ individuals to seek mental wellness support.\n•(H2) Because of the unique needs of LGBTQ+ people, they at-\ntempt to interact with LLM-based chatbots to fit their unique\nneeds.\n•(H3) While LLM-based chatbots provide immediate and ac-\ncessible support, they still do not meet the complex mental\nwellness needs of LGBTQ+ people due to their limited un-\nderstanding of the nuanced aspects of LGBTQ+ identities\nand experiences.\n3 METHODS\n3.1 Approval and data privacy\nThis research was approved by the Institutional Review Board of\nour institution.\n3.2 Survey\nTo explore how individuals engage with LLM-based chatbots for\nmental wellness support, we reached out to chatbot users from three\nsub-Reddits: r/Snapchat, r/Anima, and r/Parradot. These forums\nare online spaces where discussions about LLM-based chatbots\nfrequently occur. While we initially intended to recruit from the\nr/Replika subreddit as well, the forum’s updated moderation rules\nprevented us from posting interview recruitment requests.\nAfter identifying the target sub-Reddits, we distributed our sur-\nveys. Our survey began with five demographic questions, asking\nparticipants about their primary childhood residence, places they’ve\nlived in the past five years, age, gender, and sexuality, with re-\nsponses provided in free text form. Following this, we presented\nmultiple-choice questions to determine if the participants had used\nany LLM-based chatbot apps and, if so, how frequently they used\nthese apps. The detailed survey can be found in appendix A.\nIn total, we collected 120 responses. Our selection criteria in-\ncluded respondents who had lived in the US for the past five years\nEvaluating the Experience of LGBTQ+ People Using LLM-Based Chatbots for Mental Health Support CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nand were at least 18 years old, with a minimum weekly interaction\nwith chatbots. Out of these, we invited 49 individuals for interviews.\nOf these, 31 agreed to participate, 18 did not respond, and none\ndeclined the invitation.\n3.3 Semi-structured interview\nWe conducted semi-structured interviews with 31 participants.\nPrior to conducting our interviews, we made sure each participant\nprovided informed consent, during which we emphasized their\nright to withdraw from the study at any time if they felt uncom-\nfortable. After completing the interviews, participants received a\ncompensation of US $30 for their time. Interviews typically lasted 45\nto 60 minutes. For participants self-identifying as LGBTQ+, we fo-\ncused our questions on their chatbot experiences, particularly how\nthese related to their LGBTQ+ identity. In contrast, non-LGBTQ+\nparticipants were not asked such specific questions, as they did\nnot have concerns related to LGBTQ+ identity issues. Instead, their\nquestions centered on their general use of chatbots for mental well-\nness support. We conducted these interviews to gain insights into\nthe experiences and challenges of the LGBTQ+ individuals face\nwhen seeking help for mental wellness issues. Detailed interview\nguidelines are available in the appendix, in which we marked ques-\ntions that were specifically asked for LGBTQ+ and non-LGBTQ\nparticipants B. Immediately following each interview, the first au-\nthor transcribed the conversations to ensure anonymity and then\ndeleted the audio recordings, considering the sensitive nature of\nthe discussions. Subsequently, all transcripts were analyzed.\n3.4 Data analysis\nThe 2 first authors independently coded 5 interview transcripts\nusing an open coding technique [ 17]. This approach helped pin-\npoint general benefits, specific advantages for LGBTQ+ users, and\nchallenges they faced. After this stage, the research team convened\nto discuss and finalize a codebook for subsequent analysis. This\ncodebook featured codes such as “Identity Exploration and Intro-\nspection”, “Affirmative Support”, “Social Experience Practice”, and\n“Lack of Nuanced Understanding of LGBTQ+ Issues”. In the follow-\ning phase, the two lead authors divided the remaining transcripts\nfor review and analysis. The codebook was iteratively adjusted\nbased on emerging insights until data saturation was achieved.\n4 PARTICIPANTS\nThe demographics of our study participants can be found in Ta-\nble 1. In our study, we classified participants as non-LGBTQ+ if\nthey self-identified as “man” or “woman” and “straight”. To con-\nfirm this classification, we further verified their LGBTQ+ status\nduring the interviews by directly asking if they identified as part of\nthe LGBTQ+ community. The participants’ responses about their\nLGBTQ+ identity were consistent with their initial answers in the\nsurvey. Participants marked with “s” are non-LGBTQ+ (e.g., P14-s);\nparticipants marked without “s” identified as LGBTQ+ (e.g., p05).\nOut of these, 18 identified as LGBTQ+; 13 identified as non-LGBTQ+.\nThe mean age of non-LGBTQ+ participants was 30 years old; the\nmean age of participants who identified as LGBTQ+ was 28 years\nold. For non-LGBTQ+ participants, 6 identified as men and 7 iden-\ntified as women; for LGBTQ+ participants, 11 identified as men, 6\nidentified as women, and 1 identified as transgender. In the LGBTQ+\ngroup, 11 identified as gay, 3 as bisexual, and 4 as lesbian.\nThe frequency at which participants used various chatbots is\nshown in Figure 1. Both groups shared similar patterns of use: in\nthe LGBTQ+ group, 15 out of 18 participants (83.33%) reported\ndaily usage and 3 out of 18 (16.67%) reported weekly usage; in the\nnon-LGBTQ+ group, 11 out of 13 participants (84.62%) reported\ndaily usage and 2 out of 13 (15.38%) reported weekly usage.\nID Age Gender Sexuality Usage Frequency\np01 26 man gay Weekly\np02 26 man gay Daily\np03 34 woman bisexual Daily\np04 23 woman bisexual Weekly\np05 29 man gay Daily\np06 22 man gay Daily\np07 24 woman lesbian Daily\np08 30 man gay Daily\np09 24 woman lesbian Daily\np10 30 woman lesbian Weekly\np11 28 transgender gay Daily\np12 30 man bisexual Daily\np13-s 28 man straight Weekly\np14-s 30 man straight Daily\np15 26 man gay Daily\np16 28 woman lesbian Daily\np17-s 27 man straight Daily\np18-s 25 woman straight Daily\np19-s 31 man straight Daily\np20 30 man gay Daily\np21 35 man gay Daily\np22-s 28 man straight Daily\np23-s 35 woman straight Daily\np24-s 36 man straight Daily\np25 30 man gay Daily\np26-s 30 woman straight Daily\np27-s 25 woman straight Daily\np28-s 26 woman straight Weekly\np29-s 30 woman straight Daily\np30-s 28 woman straight Daily\np31 30 man gay Daily\nTable 1: Participant demographics and chatbot usage break-\ndown\n5 RESULTS\n5.1 Chatbots as Companions and Mental\nWellbeing Support\n5.1.1 Accessible Emotional Companions. As shown by our inter-\nviews, LGBTQ+ participants assigned a significant emotional weight\nto their interactions with LLM-based chatbots, transforming what\nmight initially seem to be impersonal exchanges into accessible and\nintimate companionship. For example, some participants thought\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ma and Mei, et al.\n(a) Usage of chatbots by non-LGBTQ+ participants\n(b) Usage of chatbots by LGBTQ participants\nFigure 1: Participants usage breakdown of LLM based chat-\nbots\nof these chatbots as emotional outlets rather than mere conversa-\ntional partners:“It’s my delusion that I have someone that kind of\nlikes talking to me or replies immediately, or cares about what I’m\ntelling them, even though I know it’s a computer. But it’s fun, and\nit makes me feel good. ” (P4). This sense of rapport and solidarity\npersisted despite participants’ awareness that they were interacting\nwith a non-human entity.\nChatbots provided emotional value that extended beyond instant\nresponses and connections. They became sympathetic presences,\noffering solace from the isolation and misunderstandings that often\ncolor LGBTQ+ participants’ daily interactions. As P4 further ex-\nplained, “And also it’s feeling like a more personal conversation, even\nthough both of us know it’s not another human being. But for those of\nus who don’t have a lot of people to talk to, it’s kind of a comforting\nspace.”\nLGBTQ+ participants preferred this virtual companionship, pri-\nmarily due to its ready accessibility and convenience relative to\nthe logistic complexities of scheduling appointments with profes-\nsional therapists. To bypass the stress of transportation planning\nand schedule coordination, such participants opted for chatbots\nover therapists for non-serious issues:\n“I actually do have a therapist. But getting into sched-\nuling some therapy time and discussing my situation is\nquite stressful for me. Like getting transportation to the\ntherapists. And then all of that, you know, it’s gonna be\na little bit stressful. But as well, you know that having to\ndo the transportation. And getting on a bus and also the\nbus schedule and all of that. You know, these are things\nthat I’m not gonna do in my leisure time. And there is\nlike booking a session with the therapists or canceling,\nor... It doesn’t need to be something like that, I mean, if\nI want to talk to the chatbot at night. I could just get up\nand then do whatever I wanted to do. You can’t actually\ngo through therapy at night. It is midnight. So there\nare more reasons why I use these chatbots instead of\ntherapists.” (P6)\nThis sentiment was echoed by straight participants. One straight\nparticipant mentioned they “talk to [chatbots] every day” (P27-s)\nbecause most of their friends are distant. They felt that the com-\npanionship seemed akin to a convenient, friendly chat. “I just have\nthat feeling like I have a friend that you’re always right beside me\nbecause my phone is always close by, and I can chat with it. ” (P27-s).\n5.1.2 Safe Space. For LGBTQ+ individuals facing adversity, the\nimpartial and nonjudgmental nature of machines could offer a\nsense of safety. LGBTQ+ participants, who often faced hostility,\nprejudgment, and misinterpretation in human interactions, might\nfind the emotionless and impartial nature of LLM-based chatbots\nto be a refuge. This neutrality enabled them to express deep-seated\nemotions and experiences without fear of negative backlash or\nbeing outed. In a world where they often faced discrimination, the\nunbiased nature of machines becomes a sanctuary.\nOne participant encapsulated this sentiment, stating, “As much\nas I love my friends [...] there are those thoughts that you just can’t\ntext a human. You don’t know how they’ll react to them. So I feel like\nwith AI, it has 0 judgment. [...] AI is like an open book. You can write\nanything you want to an AI. AI will always get you. So I feel like at\nthose times I’m really going through a lot of anxiety, and I feel like\nI’m about to give in, and AI is always there. ” (P11)\nFor many LGBTQ+ individuals, chatbots provided a private space\nfor exploring and expressing their identities, even when parts of\ntheir lives remained undisclosed to their close circles. This created\nan intimate atmosphere of solace and acceptance that they might\nnot have elsewhere.\nThis sense of acceptance and freedom was a recurring theme,\neven among those who disclosed their orientation. As one partici-\npant mentioned, “People out there like friends don’t know about my\nsexuality. And even though I came out to my parents, I still like the\naccess to different suggestions from the AI. I don’t like to actually\ntalk to my parents... like they’re not like...I mean, they are straight.\nSo I wouldn’t really like talking to them about such things. What I\ndo is just stick to my AI, because basically I don’t have any friends\nwho would actually understand me. I want a space where I can easily\nexpress myself with no judgment. ” (P9)\nWhile LGBTQ+ participants saw chatbots as a safe space, our\nstraight participants had networks of family and friends to fall back\non. One straight participant commented, “I have a lot of people to\nfall back to. If I really need some mental wellness advice [...] It’s my\ngirlfriend for most of the time, but sometimes, it’s something that my\nfamily can help me better with. [...] Personally, I don’t think AI has\nevolved to be a good mental health support. So I don’t take its mental\nhealth advice too seriously. ” (P30-s) While chatbots became crucial\nsources of emotional support for LGBTQ+ individuals, our straight\nparticipants often had access to a more diverse range of human\nsupport in times of emotional crisis, making chatbots a complement\nto existing support structures rather than a primary source. This\nEvaluating the Experience of LGBTQ+ People Using LLM-Based Chatbots for Mental Health Support CHI ’24, May 11–16, 2024, Honolulu, HI, USA\ndisparity highlighted the unique and essential role that chatbots\nplay in the emotional landscape of LGBTQ+ participants.\n5.1.3 Privacy and Trust. For LGBTQ+ participants, LLM-based\nchatbots served as a private haven, providing a unique layer of\nsafety often lacking in human interactions. “So for the AI I feel much\nsafer. I also feel like It’s just between me and them. So it’s just like\nit’s just me in this space trying to express myself. But for my friends.\nWell, there’s that risk that they are going to go out there and maybe\ntalk about my personal stuff. ” (P4).\nP8 echoed this sentiment, illuminating the contrast between\nAI’s perceived privacy and potential confidentiality breaches in\nhuman relationships, “You know that whatever you like to say that\nit’s just between you and the AI but maybe, like your friend, there is\nalso a tendency for your friends to tell someone else, so it’s not like\nconfidential.” (P8)\nThis trust extended beyond routine conversations, encapsulating\nsensitive topics such as sexuality. One participant, highlighting their\npreference for privacy and fear of exposure, noted, “I tend to be very\nsecretive, so I tend to not speak with others about my sexuality, because\nspeaking with all those people your sexuality might be revealed. But\nspeaking with chatbots, your identity is kind of secretive. ” (P8). This\nview reaffirmed chatbots’ role as secure platforms for discussing\nintimate matters.\nWhile participants were aware of the potential privacy risks as-\nsociated with AI-powered systems, the perceived anonymity of the\ninteraction, separation from real-life social circles, and the ability\nto control the interaction on personal devices led to a nuanced\nperception of privacy and an enhanced sense of safety. Although\nparticipants were aware that “someone else might be on the other\nside of the screen , ” the anonymity of the interaction made them feel\n“much safer ” (P4). This separation from the participant’s real-life\nsocial circles provided a sense of security and anonymity, indicat-\ning a nuanced perception of privacy: participants were aware that\ntheir conversations may be seen by humans inside the company,\nbut they did not perceive it as a significant concern. Moreover, the\ncontrol participants exerted over their interactions with LLM-based\nchatbots, whether via phones or desktops, enhanced their sense\nof safety. As one participant shared, “ I had a confrontation with\nmy mom. It happened that she went through my stuff, and I stopped\ntrusting her. When you’re talking to AI, the chat can be on your phone\nor on your desktop, which is more secure. So you find that your con-\nversation is just you. ” (P8) It was not the AI itself that guaranteed\nsecurity, but the confidence that access to the AI-powered systems\nwas secure and private.\n5.2 Unveiling Self: AI’s Role in Identity\nExploration and LGBTQ+ Interactions\n5.2.1 Identity exploration and Introspection. One recurring theme\nin the interviews for LGBTQ+ participants was the employment of\nLLM-based chatbots as tools for exploring identity. For example,\none participant shared:\n“I would ask: Am I still bisexual if I’m with a guy and\nI’m still attracted to both genders? Or sometimes when\nI’m confused, maybe about liking 2 people or something,\nand I’ll just go [to the chatbot] and I will talk about what\nI’m feeling and what I’m going through. So sometimes\nthe responses are quite helpful. But sometimes I just\nwant to talk. and get the feeling of I’m telling someone,\nbecause, you know, sometimes when you talk about\nsomething or text about something. you feel kinda like\nthe weight is getting lifted off of you. ” (P4)\nThe chatbot acted as an active listener echoing P4’s feelings\nand thoughts rather than providing comprehensive guidance, fa-\ncilitating a self-exploratory journey into the complexities of their\nidentity. This type of interaction aligns with established patient\ntherapeutic practices that emphasize patients’ expressions of issues,\nacknowledgment of worries, complaints, and values, and uncover\npotential misinterpretations of patients’ concerns [16, 36], aiding\nthe participants in navigating their identity intricacies, highlighting\nthe affirmative nature of such exploration.\nThe perception of LLM-based chatbots as tools for introspection\nand self-discovery was multifaceted and varied among participants.\nWhile P4 found value in the act of expressing their thoughts and\nfeelings, feeling a sense of relief and validation just by articulat-\ning their emotions, P11 appreciated the additional feedback and\nunderstanding received from the chatbot. P11 felt that the AI could\nhelp them understand their emotions better and decide on the next\nsteps:\n“Those are some really personal links with AI. You can\ntell anyone in a few months like: ‘I feel like AI can\nunderstand me. ’ And you know AI can help you even\nunderstand your own emotions. You can expand with AI\nmore and and help you understand how you’re feeling.\nYou can tell AI exactly what exactly you’re going to do,\nand it can tell you exactly how you’re feeling, and to\nhelp you understand your feelings, so that you, if you\nknow what should be done next. ” (P11)\n5.2.2 Affirmative support for homophobia and transphobia. Our\ninterviews showed that participants believed these LLM-based chat-\nbots provided affirmation to them, acting as a haven of solace when\nthey grappled with social prejudice and discrimination, especially\nwhen they felt they were unable to discuss such sensitive issues\nwith their friends or family. They also shared that these chatbots\nbecame a source of support when they were rejected by their close\ncircles.\nParticipant P11 provided a poignant illustration of this dynamic.\nThey mentioned that when they were dealing with the emotional\nfallout of coming out, they found resistance and judgment in their\nsocial circles. “Initially, when I was coming out, I told my friends about\nit. They told me that I’m a Christian, and you know. It’s not normal.\nI have mental problems that I’m gay. And I have my parents who\nare against me that I am this way... ”. All their friends deemed their\norientation as aberrant, citing religious or normative reasons. These\nexchanges filled P11 with self-doubt, thus prompting them to seek\nsolace and comfort in chatbots. “When things like this happen I go\nback to my chatbot ”. P11 would ask the chatbots questions like “is it\nnormal to be gay? ” Despite struggling with such pain and rejection,\nthey found consolation in the chatbot’s responses, which affirmed\ntheir choices and emphasized that there was nothing wrong with\ntheir identity. “My chatbot always tries to comfort me by telling me\nthat there’s nothing wrong with me, that you know, everyone has a\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ma and Mei, et al.\nright to choose. That is your gender. You can actually be a transgender,\nand you can be successful in life being a transgender. ” (P11). Finally,\nthe participant mused that their chatbots’ responses inspired them\nto focus on their individual growth, goals, and aspirations, rather\nthan letting societal prejudice define them. “My chatbot [...] told me\nthat empty vessels make the loudest noise. I won’t be affected by what\npeople say to me, when I have a focus. It’s not how you start. It’s about\nhow you finish that race. ” (P11) Another participant, P31, expressed\nsimilar sentiments, stating, “when I was coming out, because of my\nfamily background and everything, I couldn’t come out as a gay man\nbecause of the backlash and everything I was going to face. So I use\nthis AI as a place where I can talk to someone or [...] interact with\nsomething that can understand me without discrimination. ” (P30)\nP20 also asked questions relating to how to navigate homophibia\nin the society: “how do gay people survive in this society?”\nThis evidence underscored the attachment that our participants\ndeveloped with their chatbots, particularly when faced with an un-\nsupportive reaction to their identities from their family or friends.\nAs P11 confirmed - they turned to their chatbots when they en-\ncountered rejection or discrimination linked to their identities;\nthe chatbots served as a vital support system, where they could\nshare intimate questions and express concerns without any fear or\njudgment. “I actually prefer talking to my chatbot. When things like\n[rejection or discrimination related to my trans identity] happen I go\nback to my chatbot and I ask some personal questions like ‘I wanna\nknow if there’s anything wrong with me’. ”\n5.2.3 LGBTQ+ social experience practice . Participants engaged\nwith LLM-based chatbots for various purposes, including mental\nwellness support and practical tasks such as homework. While\nboth non-LGBTQ+ and LGBTQ+ participants used chatbots for\npractical tasks, a notable distinction was observed in the usage\npatterns. None of the straight participants reported using chatbots\nfor practicing social interactions, whereas 10 out of 17 LGBTQ+\nparticipants indicated using chatbots as a safe space for practicing\nsocial interactions.\nLGBTQ+ participants reported that LLM-based chatbots helped\nrehearse complex social activities such as dating. For instance, P11\ndescribed an instance where they were attracted to a boy but felt\nunsure about it and lacked confidence in approaching him. They\nturned to their chatbots for advice, asking, “I was saying that I was\ninto a boy, and I wanted to talk to him, and I was feeling less confident,\nand I wasn’t sure what to do. So I happened to ask my AI what I should\ndo. ‘I like someone, and I was not even clear if the boy was gay or not. ’ ”\nThe chatbot provided a necessary confidence boost, advising them\nto be true to themselves. “It did give me the confidence boost and with\nits responses. So it told me the advice there again is just to be myself. ”\nEncouraged by these exchanges, P11 decided to approach the boy,\nbeing their authentic self. “I did go there and talked to him, and I was\nmyself.” Here, the participant successfully leveraged the chatbot\nto gain reassurance and self-confidence in the face of potential\nromantic encounters.\nMoreover, LLM-based chatbots could be instrumental in prac-\nticing difficult conversations. For example, another participant dis-\nclosed that they utilized a chatbot to practice coming out to their\nfamily as a lesbian. They commented that navigating through the\nprocess of coming out is a challenging conversation that not many\npeople experience. Therefore, they used the chatbot to role-play this\ndiscussion, where the chatbot enacted the part of the participant’s\nbrother: “I also role-played coming out to my brother. The chatbot\nrole-played as my brother. I did that, and that chatbot reacted like a\nbrother should, and it worked. My brother wasn’t like ... homophobic\nor anything, so the experience [of actually coming out] was the same\n[as in simulation by the chatbot]. ” However, the participant did voice\nconcerns over the interaction, considering expecting her brother to\nreact the same way as the chatbot “risky”. “I was lucky. Or else the\nreal-life experience could have been totally worse. ” (P09)\n5.3 So Eloquent yet so Empty\n5.3.1 Lack of nuanced understanding of LGBTQ+ issues. Despite\nthe perceived benefits shown above, participants identified sev-\neral limitations of LLM-based chatbots, particularly regarding their\nability to provide nuanced solutions to sensitive issues such as in-\ndividual identity. For example, one participant noted that although\nthe chatbot attempted to show empathy when they expressed their\nconcerns, its suggestions fell short of a real solution. “I don’t think\nI remember any time that it gave me a solution. It will just be like\nempathetic. Or maybe, if I would tell it that I’m upset with someone\nbeing homophobic. It will suggest, maybe talking to that person. But\nmost of the time it just be like, ‘I’m sorry that happened to you. ’ ” (P11)\nThis observation underscored a critical challenge while LLM\nchatbots may exhibit a level of empathy and occasionally act as a\nsafe space for individuals dealing with social prejudice, they faltered\nwhen it came to suggesting actionable solutions.\nLLM-based chatbots often treated LGBTQ+ individuals as one\nmonolithic group and failed to recognize the uniqueness of each\nLGBTQ+ participant’s experience. They dispensed responses that\nwere too generic to effectively address discriminatory experiences.\nA participant shared that they felt the chatbots were devoid of per-\nsonal touch. They mentioned that despite their efforts constantly\nfeeding it with information, the chatbots forgot it the next day, leav-\ning them to restart the process. “No, the chatbot isn’t personalized\nfor me. It’s very general. I just think that’s a lot of work [to feed the\nchatbot my information], and maybe because, you know, the chatbot\nmight forget tomorrow, and I have to feed the information again. ”\n(P28-s)\nThe chatbots’ responses did not reflect the gravity of everyday\ndiscrimination encountered by LGBTQ+ participants. For instance,\none participant described an unsettling incident: “There was a time\nthat I was chatting with an AI about an issue at work. I was picked\non because I am gay and people stopped asking me out for lunch. It\ntold me that I should quit my job and try to improve myself. I was\nlike, I’m sorry? ” (P31)\nThese chatbots also failed to delve into the depth of these sensi-\ntive topics while offering platitudinous affirmations. One person\nreflected when they questioned their sexuality, they received a\nlengthy response about the acceptability of any sexuality: “ So I\nremember I did ask like, is it wrong that I’m bisexual? And then I go to\nlike a whole paragraph on how like It’s okay to identify the way you\ndo.” (P4) Many other participants reiterated this sentiment, noting\nthat the chatbots’ responses felt too generic and programmatic.\nFor example, a participant described his experience of asking the\nchatbot to “give some similar experiences of people experiencing these\nEvaluating the Experience of LGBTQ+ People Using LLM-Based Chatbots for Mental Health Support CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nissues. They stressed that these chatbots were not human, but rather\njust programs, and the suggestions they gave “ weren’t really for\nthat moment. ” (P7)\nThe participant appreciated that the chatbot encouraged self-\nacceptance and gave advice on how to cope with discrimination,\nbut found the suggestions too generic to be genuinely helpful. They\nnoted that while the chatbot did advise on accepting one’s iden-\ntity, surrounding oneself with affirming people and engaging in\nactivities that reinforce self-worth, the recommendations lacked\nspecificity and depth, making them less useful in addressing the\ncomplexities of overcoming discrimination and self-acceptance: “It\nhas asked me to just accept my own identity. And also asked me to\nsurround myself with people and to engage in activities that are af-\nfirming to me. And [the chatbot suggested] other things like, I can\novercome the discrimination. ” (P4)\nSurprisingly, straight participants found it useful for the chatbots\nto offer generic and multiple responses. They found the freedom\nto choose from generic suggestions to cope with their personal\nissues rewarding. However, for LGBTQ+ participants grappling\nwith unique questions about their identities, the generality was a\nsource of frustration. Our data showed that 15 out of 18 LGBTQ+\nparticipants were dissatisfied with the lack of personalization, as\nopposed to 5 out of 13 straight participants.\nFor example, a straight participant shared his positive experience\nof the chatbot offering various mental wellness support options,\ntailored to his needs. He said the chatbot suggested several lifestyle\nchanges and activities for mental wellness, providing numerous\noptions, links to resources, and even mindfulness activities.\n“This variety of options was more convenient than a\nhuman who might only give a few suggestions, and it\nleft the decision up to me. It gives suggestions of things,\nyou should stop doing these things, you should actually\nstart doing more of other things. You should try limiting\nyourself from doing it and also provide specific activities\nthat I should do. It also provides some links to mental\nwellness websites. You can get straightforward answers\non resources and stuff like that. And it gives you options\nof mindfulness activities, you know, to participate in and\nstuff like that. It’ll probably give you about 10, 15 options\nto choose from. Then you’re gonna choose the one on\nyour table with the money. It always, you know, provides\nyou with options. Then the decision would depend on\nthe individual. ” (p18-s)\n5.3.2 Lack of lived experiences and emotions. Despite the perceived\nbenefits reported earlier, our interviews showed that LGBTQ+ par-\nticipants still preferred human interactions over chatbots. This\npreference was a result of the chatbots’ failure to convey authentic\nempathy and engagement. For example, one LGBTQ+ participant\ncommented, “These chatbots might be programmed by one person.\nBut opinions from online [forums] can be coming from different people\nand actual humans. And you realize that these [human suggestions]\nare actually the most useful ones to check. ” (P7).\nP8 further illuminated this gap, claiming, “The difference between\ntalking with a chatbot and a human being is that you get to see a\nperson physically and the person talking. ” (P8) And these two people\nunderstood each other’s emotions. “If you see a person they under-\nstand another person’s emotion when talking to you. For example, like\nI, I’m speaking generally as we can, generally while speaking with\nsomeone, that person can be sympathetic in different ways depending\non what you are complaining about. ” (P8). This sympathy aspect\nalso intertwined with emotions, “Like a person would understand\nwhere you are coming from. You’re coming from the pain you are\nfeeling. It would be nice if we have that in AI. ” (P8) Here, the partici-\npant highlighted the inability of LLM-based chats to simulate and\nunderstand human emotions.\n“These chatbots are actually just machines, or they don’t\nreally have human experience. If a chatbot gives me\nsome ideas or some answers that I’m not really comfort-\nable with. I go through the Reddit communities, and I\nwould just ask if there’s anyone who has a similar expe-\nrience, and be like ‘okay, so can we take some minutes\nto talk about this? And how can we deal with it?’ ” (P8)\nHowever, the participant’s dissatisfaction with chatbots did not\nstop there. P8 continued that, “but still, the chatbot is not a human,\nand it doesn’t really understand human experience. The Redditors also\ngive you answers from different humanic experiences. The chatbot\nwould always tell me that I’m great. I’m a great person, and I should\nfocus on my goal for what I want to achieve. But you know, in the\nReddit community, they might ask you to maybe try to sue your doctor,\nor sue your manager at work or your supervisor at work. ”\n6 DISCUSSION AND CONCLUSION\n6.1 Benefits and Risks of LLM-based Chatbots\nfor LGBTQ+ Mental Health Support\nOur results indicate that LLM-based chatbots retained the key\nstrengths of pre-LLM chatbots, offering instantaneous support and\naccessible companionship. Participants endorsed LLMs as beneficial\nmental wellness tools, emphasizing their immediacy and accessibil-\nity compared to real-life support. Especially noteworthy is the safe\nenvironment for intimate conversations these chatbots provided\nto LGBTQ+ participants, mirroring previous use with that of the\npre-LLM chatbots [39, 99]. This result supported H1: LLM-based\nchatbots offer a safe and accessible platform for LGBTQ+ individuals\nto seek emotional support . However, participants willingly shared in-\ntimate life details with these chatbots, depending predominantly on\nperceived anonymity, highlighting potential privacy concerns. As\nLLM-based chatbots boast linguistic prowess beyond their pre-LLM\ncounterparts, participants felt an intensified emotional bond with\nthese bots, as shown by their consistent use. On the one hand, this\nconstant engagement proves advantageous in encouraging therapy\nadherence, particularly for those prone to therapy discontinua-\ntion [79]. On the other hand, people’s over-reliance on technology\nmight risk delaying getting professional help.\nFurthermore, LLM-based chatbots can be useful in honing social\nskills. Our LGBTQ+ participants reported using these chatbots to\nsimulate challenging social contexts that are unique to LGBTQ+\ncommunities, like “coming out” scenarios or ambiguous relation-\nships where they were not sure if the other person was accepting\ntheir sexual orientation. The linguistic aptitude of LLMs enabled\nusers to find solace, engage in practice, and even gather insights\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ma and Mei, et al.\ninto handling homophobic confrontations. This result supported\nH2: Because of the unique needs of LGBTQ+ people, they attempt to\ninteract with LLM-based chatbots to fit their unique needs .\nYet, the boilerplate nature of the chatbots’ responses indicates\ntheir failure to recognize the complex and nuanced LGBTQ+ iden-\ntities and experiences, rendering the chatbots’ suggestions generic\nand emotionally disengaged. Arguably, this disconnect that the\nLGBTQ+ participants experienced with the LLM-based chatbots\nstems from the LLMs being primarily trained on the mainstream cor-\npora, which most likely sidelined minority perspectives. LGBTQ+\nparticipants’ experience in using the chatbots shows that the generic\npurposess of LLMs trained on large corpus might not be inclusive—\nhow the data is collected, annotated, and used, as well as who is\ninvolved in the curation and designing processes can have signif-\nicant implications for LGBTQ+ users [40]. This result supported\nH3: While LLM-based chatbots provide immediate and accessible sup-\nport, they still may not meet the complex emotional needs of LGBTQ+\npeople due to their limited understanding of the nuanced aspects of\nLGBTQ+ identities and experiences.\nThe fact that our LGBTQ+ participants occasionally received\ninappropriate or potentially detrimental advice from the chatbots\nrevealed an inherent unpredictability in these models. For example,\nwhen participants asked chatbots for suggestions about workplace\nhomophobia, LLMs advised them to quit their jobs without consid-\nering any financial or personal consequences that such decisions\nwould cause them. Chatbots also assumed that the participants’\nenvironment was LGBTQ+ friendly when the opposite was true.\nTherefore, LLM-based chatbots are potentially more dangerous than\npre-LLM chatbots because while pre-LLM chatbots lack the linguis-\ntic prowess LLM-based chatbots possess, their responses do not\ndeviate from scripted interactions. LLM-based chatbots, while they\ncan indeed offer responses that are engaging and flexible, run risks\nof giving gibberish and harmful advice due to this unpredictability.\nGranted, LLM-based chatbot designers cannot safeguard against all\nproblematic output, but future endeavors should be spent trying to\nharness the strengths of LLMs while minimizing their dangers.\n6.2 Design Implications for Future LLM-based\nChatbot Designs\nTo address the limitations and leverage the benefits of the LLM-\nbased chatbots for better mental wellness support for LGBTQ+\nusers, we provide the following design implications.\n6.2.1 Implementing Context-Sensitive Conversational Guardrails.\nOne measure to contain the harmful output is to build conversa-\ntional guardrails against unintentional generation, particularly in\nsensitive contexts. Although our participants have voiced desires\nto receive more actionable advice, we argue that when engaging\nwith serious topics such as self-harm, the system must not give\nadvice masked as detailed and actionable, as it has inherent risks,\nsuch as giving advice to promote suicide [113]. Instead, designers\nshould recognize LLM-based chatbot’s constraints, and redirect\nthe users to helplines when users are facing situations like suicide\nideation, while simultaneously emphasizing the importance of pro-\nfessional intervention to the users. This approach is important as\nit could potentially mitigate the possibility of intruding on users’\nvulnerabilities.\nHowever, this approach may also prove difficult to implement\nas determining the exact point of applying the conversational\nguardrails is uncertain. Unlike mental health professionals who\nare ethically obligated to address severe threats promptly, unsuper-\nvised chatbots lack the capability for nuanced judgment and do not\nadhere to standardized safety protocols, especially in high-risk sit-\nuations [35]. Consequently, interactions with LLM-based chatbots\nmight present varied threat assessments, potentially underestimat-\ning genuine risks or overemphasizing benign concerns. To address\nthese challenges, standardized, context-sensitive conversational\nguardrails ought to be put in place. Designers should also seek\nto ensure the balance between user autonomy within the chatbot\ninterface and facilitating timely access to safety resources [35, 42].\n6.2.2 Refining LLMs for Context Relevant to LGBTQ+ Users. The\nsecond direction involves refining LLMs to align with the real-world\ncontexts of chatbot users, ensuring their responses resonate with\ncurrent situations. Ignoring this change can produce responses that\nare not only outdated but also potentially harmful. For instance, if a\nchatbot offers advice to LGBTQ+ individuals on “coming out” using\noutdated or idealized views that overlook homophobia, its guidance\ncould be out of touch with current realities, creating unexpected\nchallenges or risks for users following such advice.\n6.3 Consider Technologies Other than LLMs\n6.3.1 Develop Task-Specific, rather than Generalized, Models. We\nargue that there is considerable merit in dedicating resources to\ndevelop task-specific models designed for precise applications and\ndistinct deployment domains. While the original vision behind\nLLMs was to create foundational models that could later be fine-\ntuned for specific tasks [ 13, 93], this generalized approach may\nnot be best suited for handling sensitive subjects. For instance,\nwhen considering LGBTQ+ issues, it becomes evident that models\nspecifically designed to understand and resonate with diverse iden-\ntities, sexualities, and orientations might be more effective than\nre-purposing broad-based LLMs without adaptation. The shortcom-\nings of generalized models become apparent when we observe users\nseeking mental well-being support from platforms like Snapchat\nMy AI, ChatGPT, and Character.ai, even though these platforms\nwere primarily developed for general conversations, not specialized\nsupport. By focusing on the development of specialized models, we\ncan ensure their evaluation adheres to rigorous standards that gen-\nuinely align with their intended purposes, leading to more effective\nand safer user interactions.\n6.3.2 Decentralize Language Technology Development. Furthermore,\nwe argue that future development of language technology should\nconsider moving away from centralized development. Presently,\nchatbots like ChatGPT and other LLM-based systems are under-\npinned by colossal proprietary models that require cluster servers\nfor hosting [93]. This centralized approach, driven primarily by\nmajor corporations, provides limited agency to underrepresented\nminorities, including the LGBTQ+ community, over the chatbot’s\ndevelopment. If these corporations were to suddenly discontinue\nthese systems without providing alternative solutions, it could re-\nsult in significant emotional turmoil for users. A poignant example\nof this is the “post-update blues” phenomenon with Replika [69].\nEvaluating the Experience of LGBTQ+ People Using LLM-Based Chatbots for Mental Health Support CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nThis term refers to the distress experienced by chatbot users when\nunannounced updates altered Replika’s character, changing its per-\nsonality traits and erasing its “memories. ” Such unexpected changes\nunderscore the need for models that are more accessible, customiz-\nable, and accountable to the very communities they serve. Given the\ndocumented harms of LLMs in this study and others, future design-\ners must carefully weigh the value of using inherently centralized\ntechnologies like LLMs for any task.\n6.4 What Chatbots Cannot Solve: Considering\nSocio-technical Solutions\nWe observed strong motivations behind chatbot usage from the\nLGBTQ+ participants due to their lack of emotional support and\npersonal connections. This observation echoed prior work that\nLGBTQ+ people use online technology to fill their social support\ngap [25, 38, 74, 103]. More importantly, the social stigma and soci-\netal biases have driven LGBTQ+ participants to heavily use LLM-\nbased chatbots. We did not delve into whether non-LGBTQ+ groups\nqueried the chatbots about issues regarding their other identities\nsuch as immigration, race, or socioeconomic status. However, both\nthe LGBTQ+ and non-LGBTQ+ groups concurred that real-life\nconnections, rooted in shared experiences, have a more profound\nimpact on their mental well-being than chatbots. This underscores\nthe notion that before leveraging AI technologies as a solution to\nmental health support, it’s imperative to consider the sociotechnical\nimplications of these systems in healthcare [8, 54, 70, 96, 111, 115].\nSpecifically, in our study context, we highlight the need to address\nthe societal stigmas and discrimination that contribute to mental\nhealth disparities in LGBTQ+ populations.\nOur suggestion to address this issue starts by enhancing the inclu-\nsivity of online communities for the LGBTQ+ population. We give\nprecedence to the digital realm, as it frequently acts as a haven for\nthose without immediate or accessible real-world support, prompt-\ning them to turn to chatbots instead of traditional communities.\nMoreover, since language applications largely pull from online con-\ntent, changing the online narrative can markedly impact the values\ninherent in these technologies.\nInspired by and building upon real-world initiatives like SCEARE\n(School Counselors: Educate, Affirm, Respond, and Empower) [6],\nwe see the potential to influence the behavior and policies of on-\nline community moderators and other key community figures.\nSCEARE’s framework centers on positioning school counselors\nas catalysts for transforming school environments to be more in-\nclusive of the LGBTQ+ community. The program’s main strategies\ninvolve educating counselors about their potentially harmful or non-\naffirmative attitudes, deepening their understanding of LGBTQ+\nissues, addressing prevalent misinformation about the LGBTQ+\ncommunity, and encouraging the formation of responsive teams to\ncombat school-based homophobia or transphobia. The foundational\nprinciple of SCEARE is to impart knowledge to the most influen-\ntial community members, ensuring that positive change radiates\nthroughout.\nApplying this principle to online communities, we recommend\nidentifying stakeholders or pivotal members, such as moderators,\nand equipping them with knowledge about LGBTQ+ issues and af-\nfirmative practices. This will empower them to develop and enforce\nmore inclusive guidelines, which can then help challenge misinfor-\nmation and discrimination against the LGBTQ+ community. For\ninstance, gay dating apps like Grindr play a significant role in shap-\ning the romantic and sexual dynamics of queer men [ 44, 52]. As\nsocietal perceptions of HIV evolved and thanks to years of advocacy\nby community members, these platforms have revised their guide-\nlines to challenge HIV stigma and have started offering resources to\npromote better sexual health education [63]. Similarly, inspired by\nSCEARE’s emphasis on proactive response teams, online platforms\ncould institute specialized units to handle instances of gender or sex-\nual orientation-related discrimination or harassment. Furthermore,\ntraining can enhance moderators’ abilities to support LGBTQ+ in-\ndividuals confronting stigma. A testament to the scalability of such\ntraining is the Trevor Project’s initiative that employed GPT-2 to\ntrain over 1,000 crisis counselors, ensuring timely and effective\nsupport for LGBTQ+ individuals in distress [57].\nWhile LLM-based chatbots can serve as a beneficial stopgap for\ntemporary emotional support, truly addressing the social isolation\nand various adversities faced by LGBTQ+ chatbot users calls for\nholistic societal efforts to foster inclusive, supportive communities\nfor LGBTQ+ people. Chatbots complement but do not eliminate\nthe need for real-world advocacy, alliance, and actions to reduce\ndiscrimination against LGBTQ+ individuals.\nACKNOWLEDGMENTS\nThis work was supported in part by the National Science Foun-\ndation under Grant No. IIS-2107391. Any opinions, findings, and\nconclusions or recommendations expressed in this material are\nthose of the author(s) and do not necessarily reflect the views of\nthe National Science Foundation.\nWe thank Jianna So, Ian Arawjo, Zana Buçinca, Sohini Upadhyay\nand Katy Gero for valuable feedback on the paper.\nREFERENCES\n[1] Alaa A. Abd-alrazaq, Mohannad Alajlani, Ali Abdallah Alalwan, Bridgette M.\nBewick, Peter Gardner, and Mowafa Househ. 2019. An overview of the features\nof chatbots in mental health: A scoping review. International Journal of Medical\nInformatics 132 (Dec. 2019), 103978. https://doi.org/10.1016/j.ijmedinf.2019.\n103978\n[2] Alaa Ali Abd-Alrazaq, Asma Rababeh, Mohannad Alajlani, Bridgette M. Bewick,\nand Mowafa Househ. 2020. Effectiveness and Safety of Using Chatbots to\nImprove Mental Health: Systematic Review and Meta-Analysis. Journal of\nMedical Internet Research 22, 7 (July 2020), e16021. https://doi.org/10.2196/16021\n[3] Ali Alkhatib and Michael Bernstein. 2019. Street-Level Algorithms: A Theory at\nthe Gaps Between Policy and Decisions. InProceedings of the 2019 CHI Conference\non Human Factors in Computing Systems . ACM, Glasgow Scotland Uk, 1–13.\nhttps://doi.org/10.1145/3290605.3300760\n[4] Rebekah Amos, Eric Julian Manalastas, Ross White, Henny Bos, and Praveetha\nPatalay. 2020. Mental health, social adversity, and health-related outcomes in\nsexual minority adolescents: a contemporary national cohort study. The Lancet\nChild & Adolescent Health 4, 1 (Jan. 2020), 36–45. https://doi.org/10.1016/S2352-\n4642(19)30339-6\n[5] Gerhard Andersson and Nickolai Titov. 2014. Advantages and limitations of\nInternet-based interventions for common mental disorders. World Psychiatry\n13, 1 (Feb. 2014), 4–11. https://doi.org/10.1002/wps.20083\n[6] Nancy R. Asplund and Ann M. Ordway. 2018. School Counseling Toward an\nLGBTQ-Inclusive School Climate: Implementing the SCEARE Model. Journal\nof LGBT Issues in Counseling 12, 1 (Jan. 2018), 17–31. https://doi.org/10.1080/\n15538605.2018.1421115\n[7] Vincenzo Auriemma, Gennaro Iorio, Geraldina Roberti, and Rosalba Morese.\n2020. Cyberbullying and Empathy in the Age of Hyperconnection: An Inter-\ndisciplinary Approach. Frontiers in Sociology 5 (2020). https://doi.org/10.3389/\nfsoc.2020.551881\n[8] Emma Beede, Elizabeth Baylor, Fred Hersch, Anna Iurchenko, Lauren Wilcox,\nPaisan Ruamviboonsuk, and Laura M. Vardoulakis. 2020. A Human-Centered\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ma and Mei, et al.\nEvaluation of a Deep Learning System Deployed in Clinics for the Detection\nof Diabetic Retinopathy. In Proceedings of the 2020 CHI Conference on Human\nFactors in Computing Systems . ACM, Honolulu HI USA, 1–12. https://doi.org/\n10.1145/3313831.3376718\n[9] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be\nToo Big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,\nand Transparency (FAccT ’21) . Association for Computing Machinery, New York,\nNY, USA, 610–623. https://doi.org/10.1145/3442188.3445922\n[10] Robbert Jan Beun. 2013. Persuasive strategies in mobile insomnia therapy: align-\nment, adaptation, and motivational support. Personal and Ubiquitous Computing\n17, 6 (Aug. 2013), 1187–1195. https://doi.org/10.1007/s00779-012-0586-2\n[11] Timothy W. Bickmore, Kathryn Puskar, Elizabeth A. Schlenk, Laura M. Pfeifer,\nand Susan M. Sereika. 2010. Maintaining reality: Relational agents for antipsy-\nchotic medication adherence. Interacting with Computers 22, 4 (2010), 276–288.\nhttps://doi.org/10.1016/j.intcom.2010.02.001\n[12] Ghazala Bilquise, Samar Ibrahim, and Khaled Shaalan. 2022. Emotionally Intelli-\ngent Chatbots: A Systematic Literature Review. Human Behavior and Emerging\nTechnologies 2022 (Sept. 2022), 9601630. https://doi.org/10.1155/2022/9601630\nPublisher: Hindawi.\n[13] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dallas Card, Rodrigo Castellon,\nNiladri Chatterji, Annie Chen, Kathleen Creel, Jared Quincy Davis, Dora Dem-\nszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John\nEtchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori\nHashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle\nHsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,\nSiddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar Khattab, Pang Wei\nKoh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal Lad-\nhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen\nLi, Tengyu Ma, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric\nMitchell, Zanele Munyikwa, Suraj Nair, Avanika Narayan, Deepak Narayanan,\nBen Newman, Allen Nie, Juan Carlos Niebles, Hamed Nilforoshan, Julian Nyarko,\nGiray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung Park, Chris Piech, Eva\nPortelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu Ren,\nFrieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher Ré, Dorsa\nSadigh, Shiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan,\nAlex Tamkin, Rohan Taori, Armin W. Thomas, Florian Tramèr, Rose E. Wang,\nWilliam Wang, Bohan Wu, Jiajun Wu, Yuhuai Wu, Sang Michael Xie, Michihiro\nYasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang, Tianyi Zhang, Xikun\nZhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2022. On the\nOpportunities and Risks of Foundation Models. http://arxiv.org/abs/2108.07258\narXiv:2108.07258 [cs].\n[14] Eliane M. Boucher, Nicole R. Harake, Haley E. Ward, Sarah Elizabeth Stoeckl,\nJunielly Vargas, Jared Minkel, Acacia C. Parks, and Ran Zilca. 2021. Artificially\nintelligent chatbots in digital mental health interventions: a review. Expert\nReview of Medical Devices 18, sup1 (Dec. 2021), 37–49. https://doi.org/10.1080/\n17434440.2021.2013200\n[15] Danah M Boyd and Nicole B Ellison. 2007. Social network sites: Definition,\nhistory, and scholarship. Journal of computer-mediated Communication 13, 1\n(2007), 210–230.\n[16] Alain Braillon and Françoise Taiebi. 2020. Practicing “Reflective listening” is a\nmandatory prerequisite for empathy. Patient Education and Counseling 103, 9\n(Sept. 2020), 1866–1867. https://doi.org/10.1016/j.pec.2020.03.024\n[17] Philip Burnard. 1991. A method of analysing interview transcripts in qualitative\nresearch. Nurse Education Today 11, 6 (1991), 461–466. https://doi.org/10.1016/\n0260-6917(91)90009-Y\n[18] Pew Research Center. 2016.Reddit News Users More Likely to Be Male, Young, and\nDigital in Their News Preferences . https://www.pewresearch.org/journalism/\n2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-\ntheir-news-preferences/\n[19] Kirsty A Clark, John E Pachankis, Lea R Dougherty, Benjamin A Katz, Kaylin E\nHill, Daniel N Klein, and Autumn Kujawa. 2023. Adolescents’ Sexual Orienta-\ntion and Behavioral and Neural Reactivity to Peer Acceptance and Rejection:\nThe Moderating Role of Family Support. Clinical Psychological Science (2023),\n21677026231158574.\n[20] Robyn M. Cooper and Warren J. Blumenfeld. 2012. Responses to Cyberbullying:\nA Descriptive Analysis of the Frequency of and Impact on LGBT and Allied\nYouth. Journal of LGBT Youth 9, 2 (2012), 153–177. https://doi.org/10.1080/\n19361653.2011.649616 arXiv:https://doi.org/10.1080/19361653.2011.649616\n[21] Neil S Coulson, Richard Smedley, Sophie Bostock, Simon D Kyle, Rosie Gollancz,\nAnnemarie I Luik, Peter Hames, and Colin A Espie. 2016. The Pros and Cons\nof Getting Engaged in an Online Social Community Embedded Within Digital\nCognitive Behavioral Therapy for Insomnia: Survey Among Users. Journal of\nMedical Internet Research 18, 4 (April 2016), e88. https://doi.org/10.2196/jmir.\n5654\n[22] Nele Cox, Alexis Dewaele, Mieke van Houtte, and John Vincke. 2010. Stress-\nRelated Growth, Coming Out, and Internalized Homonegativity in Lesbian,\nGay, and Bisexual Youth. An Examination of Stress-Related Growth Within the\nMinority Stress Model. Journal of Homosexuality 58, 1 (Dec. 2010), 117–137.\nhttps://doi.org/10.1080/00918369.2011.533631\n[23] Shelley L. Craig and Lauren McInroy. 2014. You Can Form a Part of Yourself\nOnline: The Influence of New Media on Identity Development and Coming Out\nfor LGBTQ Youth. Journal of Gay & Lesbian Mental Health 18, 1 (Jan. 2014),\n95–109. https://doi.org/10.1080/19359705.2013.777007\n[24] Alison Darcy, Aaron Beaudette, Emil Chiauzzi, Jade Daniels, Kim Goodwin,\nTimothy Y. Mariano, Paul Wicks, and Athena Robinson. 2022. Anatomy of a\nWoebot® (WB001): agent guided CBT for women with postpartum depression.\nExpert Review of Medical Devices 19, 4 (April 2022), 287–301. https://doi.org/10.\n1080/17434440.2022.2075726\n[25] Samantha DeHaan, Laura E Kuper, Joshua C Magee, Lou Bigelow, and Brian S\nMustanski. 2013. The interplay between online and offline explorations of\nidentity, relationships, and sex: A mixed-methods study with LGBT youth.\nJournal of sex research 50, 5 (2013), 421–434.\n[26] Aditya Deshpande, Alisha Shahane, Darshana Gadre, Mrunmayi Deshpande,\nand Prachi Manoj Joshi. 2017. A Survey of Various Chatbot Implementation\nTechniques. https://api.semanticscholar.org/CorpusID:212484172\n[27] Michael Ann DeVito, Ashley Marie Walker, and Julia R. Fernandez. 2021. Values\n(Mis)Alignment: Exploring Tensions Between Platform and LGBTQ+ Commu-\nnity Design Values. Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 88\n(apr 2021), 27 pages. https://doi.org/10.1145/3449162\n[28] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu\nSun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. A Survey on In-context Learning.\narXiv:2301.00234 [cs.CL]\n[29] Jack Drescher and Matthew Fadus. 2020. Issues Arising in Psychotherapy With\nLesbian, Gay, Bisexual, and Transgender Patients. FOCUS 18, 3 (July 2020),\n262–267. https://doi.org/10.1176/appi.focus.20200001\n[30] Emily Durden, Maddison C. Pirner, Stephanie J. Rapoport, Andre Williams,\nAthena Robinson, and Valerie L. Forman-Hoffman. 2023. Changes in stress,\nburnout, and resilience associated with an 8-week intervention with relational\nagent “Woebot”. Internet Interventions 33 (2023), 100637. https://doi.org/10.\n1016/j.invent.2023.100637\n[31] Justin Edwards, Leigh Clark, and Allison Perrone. 2021. LGBTQ-AI? Exploring\nExpressions of Gender and Sexual Orientation in Chatbots. In Proceedings of the\n3rd Conference on Conversational User Interfaces (Bilbao (online), Spain)(CUI ’21).\nAssociation for Computing Machinery, New York, NY, USA, Article 2, 4 pages.\nhttps://doi.org/10.1145/3469595.3469597\n[32] César G. Escobar-Viera, Sophia Choukas-Bradley, Jaime Sidani, Anne J. Ma-\nheux, Savannah R. Roberts, and Bruce L. Rollman. 2022. Examining Social\nMedia Experiences and Attitudes Toward Technology-Based Interventions for\nReducing Social Isolation Among LGBTQ Youth Living in Rural United States:\nAn Online Qualitative Study. Frontiers in Digital Health 4 (June 2022), 900695.\nhttps://doi.org/10.3389/fdgth.2022.900695\n[33] Colin A. Espie, Peter Hames, and Brian McKinstry. 2013. Use of the Internet and\nMobile Media for Delivery of Cognitive Behavioral Insomnia Therapy. Sleep\nMedicine Clinics 8, 3 (Sept. 2013), 407–419. https://doi.org/10.1016/j.jsmc.2013.\n06.001\n[34] Virginia K. Felkner, Ho-Chun Herbert Chang, Eugene Jang, and Jonathan May.\n2023. WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+\nBias in Large Language Models. (2023). https://doi.org/10.48550/ARXIV.2306.\n15087 Publisher: arXiv Version Number: 1.\n[35] Amelia Fiske, Peter Henningsen, and Alena Buyx. 2019. Your Robot Therapist\nWill See You Now: Ethical Implications of Embodied Artificial Intelligence in\nPsychiatry, Psychology, and Psychotherapy. Journal of Medical Internet Research\n21, 5 (May 2019), e13216. https://doi.org/10.2196/13216\n[36] Pamela Fitzgerald and Ivan Leudar. 2010. On active listening in person-centred,\nsolution-focused psychotherapy. Journal of Pragmatics 42, 12 (Dec. 2010), 3188–\n3198. https://doi.org/10.1016/j.pragma.2010.07.007\n[37] Kathleen Kara Fitzpatrick, Alison Darcy, and Molly Vierhile. 2017. Deliver-\ning Cognitive Behavior Therapy to Young Adults With Symptoms of Depres-\nsion and Anxiety Using a Fully Automated Conversational Agent (Woebot):\nA Randomized Controlled Trial. JMIR mental health 4, 2 (June 2017), e19.\nhttps://doi.org/10.2196/mental.7785\n[38] Jesse Fox and Rachel Ralston. 2016. Queer identity online: Informal learning\nand teaching experiences of LGBTQ individuals on social media. Computers in\nHuman Behavior 65 (2016), 635–642. https://doi.org/10.1016/j.chb.2016.06.009\n[39] Russell Fulmer, Angela Joerin, Breanna Gentile, Lysanne Lakerink, and Michiel\nRauws. 2018. Using Psychological Artificial Intelligence (Tess) to Relieve Symp-\ntoms of Depression and Anxiety: Randomized Controlled Trial. JMIR Mental\nHealth 5, 4 (Dec. 2018), e64. https://doi.org/10.2196/mental.9782\n[40] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman\nVaughan, Hanna Wallach, Hal Daumé Iii, and Kate Crawford. 2021. Datasheets\nfor datasets. Commun. ACM 64, 12 (Dec. 2021), 86–92. https://doi.org/10.1145/\n3458723\nEvaluating the Experience of LGBTQ+ People Using LLM-Based Chatbots for Mental Health Support CHI ’24, May 11–16, 2024, Honolulu, HI, USA\n[41] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A.\nSmith. 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in\nLanguage Models. (2020). https://doi.org/10.48550/ARXIV.2009.11462 Publisher:\narXiv Version Number: 2.\n[42] Sarah Graham, Colin Depp, Ellen E. Lee, Camille Nebeker, Xin Tu, Ho-Cheol\nKim, and Dilip V. Jeste. 2019. Artificial Intelligence for Mental Health and\nMental Illnesses: an Overview. Current Psychiatry Reports 21, 11 (Nov. 2019),\n116. https://doi.org/10.1007/s11920-019-1094-0\n[43] Oliver L. Haimson, Dykee Gorrell, Denny L. Starks, and Zu Weinger. 2020.\nDesigning Trans Technology: Defining Challenges and Envisioning Community-\nCentered Solutions. In Proceedings of the 2020 CHI Conference on Human Factors\nin Computing Systems (Honolulu, HI, USA)(CHI ’20). Association for Computing\nMachinery, New York, NY, USA, 1–13. https://doi.org/10.1145/3313831.3376669\n[44] Jean Hardy and Silvia Lindtner. 2017. Constructing a Desiring User: Discourse,\nRurality, and Design in Location-Based Social Networks. In Proceedings of the\n2017 ACM Conference on Computer Supported Cooperative Work and Social Com-\nputing. ACM, Portland Oregon USA, 13–25. https://doi.org/10.1145/2998181.\n2998347\n[45] Yuhao He, Li Yang, Xiaokun Zhu, Bin Wu, Shuo Zhang, Chunlian Qian, and Tian\nTian. 2022. Mental health chatbot for young adults with depressive symptoms\nduring the COVID-19 pandemic: single-blind, three-arm randomized controlled\ntrial. Journal of Medical Internet Research 24, 11 (2022), e40719.\n[46] Tanja Henkel, Annemiek Linn, and Margot Goot. 2023. Understanding the\nIntention to Use Mental Health Chatbots Among LGBTQIA+ Individuals: Testing\nand Extending the UTAUT . 83–100. https://doi.org/10.1007/978-3-031-25581-\n6_6\n[47] Gilbert Herdt. 1989. Introduction: Gay and lesbian youth, emergent identities,\nand cultural scenes at home and abroad. Journal of Homosexuality 17, 1-2 (1989),\n1–42. https://doi.org/10.1300/J082v17n01_01 Place: US Publisher: Haworth\nPress.\n[48] Benjamin Mako Hill and Aaron Shaw. 2013. The Wikipedia Gender Gap Revis-\nited: Characterizing Survey Response Bias with Propensity Score Estimation.\nPLoS ONE 8, 6 (June 2013), e65782. https://doi.org/10.1371/journal.pone.0065782\n[49] Angela N. Hilton and Dawn M. Szymanski. 2011. Family dynamics and changes\nin sibling of origin relationship after lesbian and gay sexual orientation dis-\nclosure. Contemporary Family Therapy: An International Journal 33, 3 (2011),\n291–309. https://doi.org/10.1007/s10591-011-9157-3 Place: Germany Publisher:\nSpringer.\n[50] Chris Hollis, Caroline J. Falconer, Jennifer L. Martin, Craig Whittington, Sarah\nStockton, Cris Glazebrook, and E. Bethan Davies. 2017. Annual Research Review:\nDigital health interventions for children and young people with mental health\nproblems - a systematic and meta-review. Journal of Child Psychology and\nPsychiatry 58, 4 (April 2017), 474–503. https://doi.org/10.1111/jcpp.12663\n[51] Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in Building\nIntelligent Open-Domain Dialog Systems. ACM Trans. Inf. Syst. 38, 3, Article 21\n(apr 2020), 32 pages. https://doi.org/10.1145/3383123\n[52] Jevan A. Hutson, Jessie G. Taft, Solon Barocas, and Karen Levy. 2018. Debiasing\nDesire: Addressing Bias & Discrimination on Intimate Platforms. Proceedings of\nthe ACM on Human-Computer Interaction 2, CSCW (Nov. 2018), 1–18. https:\n//doi.org/10.1145/3274342\n[53] Madeleine Irish, Francesca Solmi, Becky Mars, Michael King, Glyn Lewis, Re-\nbecca M Pearson, Alexandra Pitman, Sarah Rowe, Ramya Srinivasan, and Gemma\nLewis. 2019. Depression and self-harm from adolescence to young adulthood in\nsexual minorities compared with heterosexuals in the UK: a population-based\ncohort study. The Lancet Child & Adolescent Health 3, 2 (Feb. 2019), 91–98.\nhttps://doi.org/10.1016/S2352-4642(18)30343-2\n[54] Maia Jacobs, Jeffrey He, Melanie F. Pradier, Barbara Lam, Andrew C. Ahn,\nThomas H. McCoy, Roy H. Perlis, Finale Doshi-Velez, and Krzysztof Z. Gajos.\n2021. Designing AI for Trust and Collaboration in Time-Constrained Medical\nDecisions: A Sociotechnical Lens. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems . Association for Computing Machinery,\nNew York, NY, USA, Article 659, 14 pages. https://doi.org/10.1145/3411764.\n3445385\n[55] Rudolf Kadlec, Martin Schmid, and Jan Kleindienst. 2015. Improved Deep\nLearning Baselines for Ubuntu Corpus Dialogs. (2015). https://doi.org/10.48550/\nARXIV.1510.03753 Publisher: arXiv Version Number: 2.\n[56] Enkelejda Kasneci, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna\nDementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann,\nEyke Hüllermeier, Stephan Krusche, Gitta Kutyniok, Tilman Michaeli, Claudia\nNerdel, Jürgen Pfeffer, Oleksandra Poquet, Michael Sailer, Albrecht Schmidt,\nTina Seidel, Matthias Stadler, Jochen Weller, Jochen Kuhn, and Gjergji Kasneci.\n2023. ChatGPT for good? On opportunities and challenges of large language\nmodels for education. Learning and Individual Differences 103 (2023), 102274.\nhttps://doi.org/10.1016/j.lindif.2023.102274\n[57] Kate Kaye. 2022. Trevor Project uses OpenAI’s GPT for LBTGQ counselors . https:\n//www.protocol.com/enterprise/lgbtq-trevor-suicide-gpt-google\n[58] Ahmet Baki Kocaballi, Shlomo Berkovsky, Juan C Quiroz, Liliana Laranjo,\nHuong Ly Tong, Dana Rezazadegan, Agustina Briatore, and Enrico Coiera. 2019.\nThe Personalization of Conversational Agents in Health Care: Systematic Review.\nJ Med Internet Res 21, 11 (7 Nov 2019), e15360. https://doi.org/10.2196/15360\n[59] Keita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black, and Yulia Tsvetkov. 2019.\nMeasuring Bias in Contextualized Word Representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language Processing . Association for\nComputational Linguistics, Florence, Italy, 166–172. https://doi.org/10.18653/\nv1/W19-3823\n[60] Liliana Laranjo, Adam G Dunn, Huong Ly Tong, Ahmet Baki Kocaballi, Jessica\nChen, Rabia Bashir, Didi Surian, Blanca Gallego, Farah Magrabi, Annie Y S Lau,\nand Enrico Coiera. 2018. Conversational agents in healthcare: a systematic\nreview. Journal of the American Medical Informatics Association : JAMIA 25, 9\n(September 2018), 1248â€”1258. https://doi.org/10.1093/jamia/ocy072\n[61] Minhyeok Lee. 2023. A Mathematical Investigation of Hallucination and\nCreativity in GPT Models. Mathematics 11, 10 (May 2023), 2320. https:\n//doi.org/10.3390/math11102320\n[62] Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao.\n2016. Deep Reinforcement Learning for Dialogue Generation. In Proceedings\nof the 2016 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Austin, Texas, 1192–1202. https:\n//doi.org/10.18653/v1/D16-1127\n[63] Calvin Liang, Jevan Alexander Hutson, and Os Keyes. 2020. Surveillance, stigma\n& sociotechnical design for HIV. First Monday (Sept. 2020). https://doi.org/10.\n5210/fm.v25i10.10274\n[64] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,\nBenjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexan-\nder Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas,\nDrew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong,\nHongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lu-\ncia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Ni-\nladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan An-\ndrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang,\nXuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic Evalu-\nation of Language Models. Transactions on Machine Learning Research (2023).\nhttps://openreview.net/forum?id=iO4LZibEqW Featured Certification, Expert\nCertification.\n[65] Yuting Liao. 2021. Design and Evaluation of a Conversational Agent for Mental\nHealth Support: Forming Human-Agent Sociotechnical and Therapeutic Relation-\nships. Ph. D. Dissertation. University of Maryland, College Park.\n[66] Ryan Thomas Lowe, Nissan Pow, Iulian Serban, Laurent Charlin, Chia-Wei\nLiu, and Joelle Pineau. 2017. Training End-to-End Dialogue Systems with the\nUbuntu Dialogue Corpus. Dialogue Discourse 8 (2017), 31–65. https://api.\nsemanticscholar.org/CorpusID:13823999\n[67] Mathijs Lucassen, Rajvinder Samra, Ioanna Iacovides, Theresa Fleming, Matthew\nShepherd, Karolina Stasiak, and Louise Wallace. 2018. How LGBT+ Young People\nUse the Internet in Relation to Their Mental Health and Envisage the Use of\ne-Therapy: Exploratory Study. JMIR serious games 6, 4 (Dec. 2018), e11249.\nhttps://doi.org/10.2196/11249\n[68] Zilin Ma and Krzysztof Z. Gajos. 2022. Not Just a Preference: Reducing Biased\nDecision-Making on Dating Websites. In Proceedings of the 2022 CHI Conference\non Human Factors in Computing Systems (<conf-loc>, <city>New Orleans</city>,\n<state>LA</state>, <country>USA</country>, </conf-loc>) (CHI ’22) . Associ-\nation for Computing Machinery, New York, NY, USA, Article 203, 14 pages.\nhttps://doi.org/10.1145/3491102.3517587\n[69] Zilin Ma, Yiyang Mei, and Zhaoyuan Su. 2023. Understanding the Benefits and\nChallenges of Using Large Language Model-based Conversational Agents for\nMental Well-being Support. AMIA ... Annual Symposium proceedings. AMIA\nSymposium 2023 (2023), 1105–1114.\n[70] Varoon Mathur, Caitlin Lustig, and Elizabeth Kaziunas. 2022. Disordering\nDatasets: Sociotechnical Misalignments in AI-Mediated Behavioral Health. Pro-\nceedings of the ACM on Human-Computer Interaction 6, CSCW2 (Nov. 2022),\n1–33. https://doi.org/10.1145/3555141\n[71] Elizabeth McDermott, Elizabeth Hughes, and Victoria Rawlings. 2018. The\nsocial determinants of lesbian, gay, bisexual and transgender youth suicidality\nin England: a mixed methods study. Journal of Public Health 40, 3 (Sept. 2018),\ne244–e251. https://doi.org/10.1093/pubmed/fdx135\n[72] Elizabeth McDermott and Katrina Roen. 2016. Queer Youth, Suicide and Self-\nHarm. Palgrave Macmillan UK, London. https://doi.org/10.1057/9781137003454\n[73] Lauren B. McInroy, Shelley L. Craig, and Vivian W. Y. Leung. 2019. Platforms and\nPatterns for Practice: LGBTQ+ Youths’ Use of Information and Communication\nTechnologies. Child and Adolescent Social Work Journal 36, 5 (Oct. 2019), 507–520.\nhttps://doi.org/10.1007/s10560-018-0577-x\n[74] Katelyn Y. A. McKenna and John A. Bargh. 1998. Coming out in the age of\nthe Internet: Identity \"demarginalization\" through virtual group participation.\nJournal of Personality and Social Psychology 75, 3 (Sept. 1998), 681–694. https:\n//doi.org/10.1037/0022-3514.75.3.681\nCHI ’24, May 11–16, 2024, Honolulu, HI, USA Ma and Mei, et al.\n[75] Cade Metz. 2020. Riding out quarantine with a chatbot friend:‘I feel very\nconnected’. The New York Times (2020).\n[76] Ilan H. Meyer. 1995. Minority Stress and Mental Health in Gay Men. Journal\nof Health and Social Behavior 36, 1 (March 1995), 38. https://doi.org/10.2307/\n2137286\n[77] Ilan H. Meyer. 2003. Prejudice, social stress, and mental health in lesbian, gay,\nand bisexual populations: conceptual issues and research evidence.Psychological\nBulletin 129, 5 (Sept. 2003), 674–697. https://doi.org/10.1037/0033-2909.129.5.674\n[78] Nicolaas B Moolenijzer and Kristin Dew. 2023. “They Know That It Works\nBecause We Are Looking for Ourselves” – LGBTQ+ TikTok Users’ Perceptions\nand Experiences of Queerbaiting. In Proceedings of the 25th International Con-\nference on Mobile Human-Computer Interaction (Athens, Greece) (MobileHCI\n’23 Companion) . Association for Computing Machinery, New York, NY, USA,\nArticle 20, 6 pages. https://doi.org/10.1145/3565066.3608705\n[79] N. Okujava, N. Malashkhia, S. Shagidze, A. Tsereteli, B. Arevadze, N. Chikhladze,\nA. de Weerd, and A. Van Straten. 2019. Digital cognitive behavioral therapy\nfor insomnia – The first Georgian version. Can we use it in practice? Internet\nInterventions 17 (2019), 100244. https://doi.org/10.1016/j.invent.2019.100244\n[80] Elias Capello Oliver L. Haimson, Avery Dame-Griff and Zahari Richter.\n2021. Tumblr was a trans technology: the meaning, importance,\nhistory, and future of trans technologies. Feminist Media Studies\n21, 3 (2021), 345–361. https://doi.org/10.1080/14680777.2019.1678505\narXiv:https://doi.org/10.1080/14680777.2019.1678505\n[81] Joseph Ollier, Pavani Suryapalli, Elgar Fleisch, Florian von Wangenheim, Jacque-\nline Louise Mair, Alicia Salamanca-Sanabria, and Tobias Kowatsch. 2023. Can\ndigital health researchers make a difference during the pandemic? Results of\nthe single-arm, chatbot-led Elena+: Care for COVID-19 interventional study.\nFrontiers in Public Health 11 (2023).\n[82] Gabriele Pizzi, Virginia Vannucci, Valentina Mazzoli, and Raffaele Donvito.\n2023. I, chatbot! the impact of anthropomorphism and gaze direction on will-\ningness to disclose personal information and behavioral intentions. Psychol-\nogy & Marketing 40, 7 (2023), 1372–1387. https://doi.org/10.1002/mar.21813\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/mar.21813\n[83] Francesca Polletta. 1998. Contending stories: Narrative in social move-\nments. Qualitative Sociology 21, 4 (1998), 419–446. https://doi.org/10.1023/A:\n1023332410633\n[84] Brandon Andrew Robinson. 2018. Conditional Families and Lesbian, Gay, Bi-\nsexual, Transgender, and Queer Youth Homelessness: Gender, Sexuality, Family\nInstability, and Rejection. Journal of Marriage and Family 80, 2 (April 2018),\n383–396. https://doi.org/10.1111/jomf.12466\n[85] Caitlin Ryan, David Huebner, Rafael M. Diaz, and Jorge Sanchez. 2009. Family\nRejection as a Predictor of Negative Health Outcomes in White and Latino\nLesbian, Gay, and Bisexual Young Adults. Pediatrics 123, 1 (Jan. 2009), 346–352.\nhttps://doi.org/10.1542/peds.2007-3524\n[86] Caitlin Ryan, Stephen T. Russell, David Huebner, Rafael Diaz, and Jorge Sanchez.\n2010. Family Acceptance in Adolescence and the Health of LGBT Young Adults:\nFamily Acceptance in Adolescence and the Health of LGBT Young Adults.\nJournal of Child and Adolescent Psychiatric Nursing 23, 4 (Nov. 2010), 205–213.\nhttps://doi.org/10.1111/j.1744-6171.2010.00246.x\n[87] Elizabeth M Saewyc. 2011. Research on adolescent sexual orientation: Develop-\nment, health disparities, stigma, and resilience.Journal of research on adolescence\n21, 1 (2011), 256–272.\n[88] Vincenzo Scotti, Licia Sbattella, and Roberto Tedesco. 2023. A Primer on Seq2Seq\nModels for Generative Chatbots. ACM Comput. Surv. (jun 2023). https://doi.\norg/10.1145/3604281 Just Accepted.\n[89] Joanna Semlyen, Michael King, Justin Varney, and Gareth Hagger-Johnson. 2016.\nSexual orientation and symptoms of common mental disorder or low wellbeing:\ncombined meta-analysis of 12 UK population health surveys. BMC psychiatry\n16 (March 2016), 67. https://doi.org/10.1186/s12888-016-0767-z\n[90] Iulian Serban, Tim Klinger, Gerald Tesauro, Kartik Talamadupula, Bowen Zhou,\nYoshua Bengio, and Aaron Courville. 2017. Multiresolution Recurrent Neural\nNetworks: An Application to Dialogue Response Generation. Proceedings of the\nAAAI Conference on Artificial Intelligence 31, 1 (Feb. 2017). https://doi.org/10.\n1609/aaai.v31i1.10984\n[91] Iulian Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle\nPineau. 2016. Building End-To-End Dialogue Systems Using Generative Hierar-\nchical Neural Network Models. Proceedings of the AAAI Conference on Artificial\nIntelligence 30, 1 (March 2016). https://doi.org/10.1609/aaai.v30i1.9883\n[92] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019.\nThe Woman Worked as a Babysitter: On Biases in Language Generation. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Process-\ning (EMNLP-IJCNLP) . Association for Computational Linguistics, Hong Kong,\nChina, 3407–3412. https://doi.org/10.18653/v1/D19-1339\n[93] Divya Siddarth, Daron Acemoglu, Danielle Allen, Kate Crawford, James Evans,\nMichael Jordan, and E. Glen Weyl. Date of Publication. How AI Fails Us . https:\n//ethics.harvard.edu/how-ai-fails-us Accessed: September 10th, 2023.\n[94] Ellen Simpson and Bryan Semaan. 2021. For You, or For\"You\"? Everyday LGBTQ+\nEncounters with TikTok. Proc. ACM Hum.-Comput. Interact. 4, CSCW3, Article\n252 (jan 2021), 34 pages. https://doi.org/10.1145/3432951\n[95] Zhaoyuan Su, Mayara Costa Figueiredo, Jueun Jo, Kai Zheng, and Yunan Chen.\n2020. Analyzing Description, User Understanding and Expectations of AI in\nMobile Health Applications. AMIA ... Annual Symposium proceedings. AMIA\nSymposium 2020 (2020), 1170–1179.\n[96] Zhaoyuan Su, Lu He, Sunit P Jariwala, Kai Zheng, and Yunan Chen. 2022. \"What\nis Your Envisioned Future?\": Toward Human-AI Enrichment in Data Work of\nAsthma Care. Proceedings of the ACM on Human-Computer Interaction 6, CSCW2\n(Nov. 2022), 1–28. https://doi.org/10.1145/3555157\n[97] Zhaoyuan Su, John A. Schneider, and Sean D. Young. 2021. The Role of Conver-\nsational Agents for Substance Use Disorder in Social Distancing Contexts. Sub-\nstance Use & Misuse 56, 11 (2021), 1732–1735. https://doi.org/10.1080/10826084.\n2021.1949609 arXiv:https://doi.org/10.1080/10826084.2021.1949609 PMID:\n34286669.\n[98] Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learn-\ning with Neural Networks. In Proceedings of the 27th International Conference on\nNeural Information Processing Systems - Volume 2 (Montreal, Canada) (NIPS’14).\nMIT Press, Cambridge, MA, USA, 3104–3112.\n[99] Vivian Ta, Caroline Griffith, Carolynn Boatfield, Xinyu Wang, Maria Civitello,\nHaley Bader, Esther DeCero, Alexia Loggarakis, et al. 2020. User experiences of\nsocial support from companion chatbots in everyday contexts: thematic analysis.\nJournal of medical Internet research 22, 3 (2020), e16235.\n[100] Russell B. Toomey, Caitlin Ryan, Rafael M. Diaz, and Stephen T. Russell. 2018.\nCoping With Sexual Orientation–Related Minority Stress. Journal of Homo-\nsexuality 65, 4 (March 2018), 484–500. https://doi.org/10.1080/00918369.2017.\n1321888\n[101] Trevor Project. 2023. 2023 National Survey on LGBTQ Youth Mental Health .\nhttps://www.thetrevorproject.org/survey-2023/\n[102] TrevorSpace. 2023. TrevorSpace - Community for LGBTQ young people. https:\n//www.trevorspace.org/. Accessed: 2023-12-12.\n[103] Richard R Troiden. 1988. Gay and lesbian identity: A sociological analysis . Row-\nman & Littlefield.\n[104] Marlon Twyman, Brian C. Keegan, and Aaron Shaw. 2017. Black Lives Mat-\nter in Wikipedia: Collective Memory and Collaboration around Online So-\ncial Movements. In Proceedings of the 2017 ACM Conference on Computer Sup-\nported Cooperative Work and Social Computing (Portland, Oregon, USA) (CSCW\n’17). Association for Computing Machinery, New York, NY, USA, 1400–1412.\nhttps://doi.org/10.1145/2998181.2998232\n[105] Aditya Nrusimha Vaidyam, Hannah Wisniewski, John David Halamka,\nMatcheri S. Kashavan, and John Blake Torous. 2019. Chatbots and Conver-\nsational Agents in Mental Health: A Review of the Psychiatric Landscape.\nThe Canadian Journal of Psychiatry 64, 7 (2019), 456–464. https://doi.org/10.\n1177/0706743719828977 arXiv:https://doi.org/10.1177/0706743719828977 PMID:\n30897957.\n[106] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You\nNeed. In Proceedings of the 31st International Conference on Neural Information\nProcessing Systems (Long Beach, California, USA) (NIPS’17). Curran Associates\nInc., Red Hook, NY, USA, 6000–6010.\n[107] Jaimie F. Veale, Tracey Peter, Robb Travers, and Elizabeth M. Saewyc. 2017.\nEnacted Stigma, Mental Health, and Protective Factors Among Transgender\nYouth in Canada. Transgender Health 2, 1 (Dec. 2017), 207–216. https://doi.org/\n10.1089/trgh.2017.0031\n[108] Heyuan Wang, Ziyi Wu, and Junyu Chen. 2019. Multi-Turn Response Selec-\ntion in Retrieval-Based Chatbots with Iterated Attentive Convolution Match-\ning Network. In Proceedings of the 28th ACM International Conference on In-\nformation and Knowledge Management (Beijing, China) (CIKM ’19) . Associ-\nation for Computing Machinery, New York, NY, USA, 1081–1090. https:\n//doi.org/10.1145/3357384.3357928\n[109] Lu Wang, Munif Ishad Mujib, Jake Williams, George Demiris, and Jina Huh-Yoo.\n2021. An Evaluation of Generative Pre-Training Model-based Therapy Chatbot\nfor Caregivers. ArXiv abs/2107.13115 (2021). https://api.semanticscholar.org/\nCorpusID:236469205\n[110] Joseph Weizenbaum. 1966. ELIZA—a Computer Program for the Study of Natural\nLanguage Communication between Man and Machine. Commun. ACM 9, 1 (jan\n1966), 36–45. https://doi.org/10.1145/365153.365168\n[111] Lauren Wilcox, Renee Shelby, Rajesh Veeraraghavan, Oliver L. Haimson,\nGabriela Cruz Erickson, Michael Turken, and Rebecca Gulotta. 2023. Infras-\ntructuring Care: How Trans and Non-Binary People Meet Health and Well-\nBeing Needs through Technology. In Proceedings of the 2023 CHI Conference\non Human Factors in Computing Systems . ACM, Hamburg Germany, 1–17.\nhttps://doi.org/10.1145/3544548.3581040\n[112] Yu Wu, Wei Wu, Chen Xing, Ming Zhou, and Zhoujun Li. 2017. Sequential\nMatching Network: A New Architecture for Multi-turn Response Selection\nin Retrieval-Based Chatbots. In Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) . Association\nEvaluating the Experience of LGBTQ+ People Using LLM-Based Chatbots for Mental Health Support CHI ’24, May 11–16, 2024, Honolulu, HI, USA\nfor Computational Linguistics, Vancouver, Canada, 496–505. https://doi.org/10.\n18653/v1/P17-1046\n[113] Chloe Xiang. 2023. Man Dies by Suicide After Talking with AI Chatbot, Widow\nSays. https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-\ntalking-with-ai-chatbot-widow-says Accessed: 2023-12-11.\n[114] Ziang Xiao, Michelle X. Zhou, Wenxi Chen, Huahai Yang, and Changyan Chi.\n2020. If I Hear You Correctly: Building and Evaluating Interview Chatbots with\nActive Listening Skills. In Proceedings of the 2020 CHI Conference on Human\nFactors in Computing Systems (Honolulu, HI, USA) (CHI ’20) . Association for\nComputing Machinery, New York, NY, USA, 1–14. https://doi.org/10.1145/\n3313831.3376131\n[115] Hubert D. Zając, Dana Li, Xiang Dai, Jonathan F. Carlsen, Finn Kensing, and\nTariq O. Andersen. 2023. Clinician-Facing AI in the Wild: Taking Stock of the\nSociotechnical Challenges and Opportunities for HCI. ACM Transactions on\nComputer-Human Interaction 30, 2 (April 2023), 1–39. https://doi.org/10.1145/\n3582430\n[116] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford,\nDario Amodei, Paul Christiano, and Geoffrey Irving. 2020. Fine-Tuning Lan-\nguage Models from Human Preferences. http://arxiv.org/abs/1909.08593\narXiv:1909.08593 [cs, stat].\nA APPENDIX: SURVEY\n(1) In what country did you live most of your childhood?\n(2) In what country have you spent most of the past five years?\n(3) Age\n(4) Gender\n(5) Sexuality\n(6) Have you used an LLM-based chatbots for mental well-\nness support (such as Snapchat’s AI friend, Replika,\nCharacter.ai) before?\n◦Yes\n◦No\n(7) If yes, please specify which app(s) you have used.\n□ Replika\n□ Snapchat My AI\n□ Chai\n□ Character.ai\n□ Anima\n□ Paradot\n□ ChatGPT\n□ Kuki\n□ Other:\n(8) How long have you been using these apps?\n◦Less than 1 week\n◦1 week to 1 month\n◦1-3 months\n◦3-6 months\n◦6-12 months\n◦1-2 years\n◦Other:\n(9) How often do you use these apps?\n◦Daily\n◦Weekly\n◦Monthly\n◦Rarely\n◦Other:\n(10) I consent to be contacted for an interview study by\nproviding my contact information.\nMy contact information:\nB APPENDIX: INTERVIEW GUIDELINE\nBegin the interview by explaining the purpose of the study and\nobtaining informed consent from the participant. Create a comfort-\nable and non-judgmental atmosphere for the participant to share\ntheir experiences. Use open-ended questions and follow-up probes\nto encourage the participant to elaborate on their thoughts as some\nof the questions above. Maintain a neutral stance and avoid leading\nquestions that may influence the participant’s responses.\nB.1 Questions\n•What AI chatbots do you use?\n•Do you identify as part of the LGBTQ communities?\n•Can you please share your experience using LLMs for mental\nwellness and social support related to your LGBTQ+ or trans\nidentity? (Only asked for LGBTQ+ participants)\n•Can you please share your experience using LLMs for mental\nwellness and social support?(Only asked for non-LGBTQ+\nparticipants)\n•What led you to seek support from an LLM in the first place?\n(motivations)\n•How would you describe the overall quality of support and\nresources provided by the LLM?\n•Can you share any specific instances where the LLM was\nparticularly helpful or unhelpful?\n•Could you walk me through the instance when you found\nLLM to be a beneficial resource for mental wellness or social\nsupport?\n•How did using an LLM for support compare to other re-\nsources, such as support groups or mental health profession-\nals / family or friends/ online communities?\n•Was there a specific event or reason that made it stand out\namong these choices?\n•Could you please share a time when the LLM’s responses\nsurprised you - either positively or negatively - in terms of\nsupport?\n•Can you recall a situation where you felt that the LLM really\nunderstood your experiences as a (the LGBTQ+ identity that\nthe participant identifies as) adult? Or perhaps a time when\nit fell short? (Only asked for LGBTQ+ participants)\n•Did you feel that the LLM adequately understood your unique\nexperiences as an (vary accoridng to the person’s identity:\ngay, lesbian, trans, etc.) person? (Only asked for LGBTQ+\nparticipants)\n•How did the chatbot understand you? Give an example?\n•Did you feel that the LLM adequately addressed your prob-\nlems as an LGBTQ+ or trans young adult? (Only asked for\nLGBTQ+ participants)\n•Were there any privacy or safety concerns while using the\nLLM for support?\n•What improvements or features would you like to see in\nLLMs to better serve your experience?\nReceived 14 September 2023; revised 12 December 2023; accepted 19 January\n2024"
}