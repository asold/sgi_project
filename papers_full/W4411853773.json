{
  "title": "Scalable evaluation framework for retrieval augmented generation in tobacco research using large Language models",
  "url": "https://openalex.org/W4411853773",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5058407390",
      "name": "Sherif Elmitwalli",
      "affiliations": [
        "University of Bath"
      ]
    },
    {
      "id": "https://openalex.org/A2631574720",
      "name": "John Mehegan",
      "affiliations": [
        "University of Bath"
      ]
    },
    {
      "id": "https://openalex.org/A3086465880",
      "name": "Sophie Braznell",
      "affiliations": [
        "University of Bath"
      ]
    },
    {
      "id": "https://openalex.org/A2808006052",
      "name": "Allen Gallagher",
      "affiliations": [
        "University of Bath"
      ]
    },
    {
      "id": "https://openalex.org/A5058407390",
      "name": "Sherif Elmitwalli",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2631574720",
      "name": "John Mehegan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3086465880",
      "name": "Sophie Braznell",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2808006052",
      "name": "Allen Gallagher",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4400530529",
    "https://openalex.org/W4391847187",
    "https://openalex.org/W4309139087",
    "https://openalex.org/W4384388099",
    "https://openalex.org/W4405081690",
    "https://openalex.org/W4393015002",
    "https://openalex.org/W4404832535",
    "https://openalex.org/W3172148746",
    "https://openalex.org/W4409177398",
    "https://openalex.org/W4317898419",
    "https://openalex.org/W4401857375",
    "https://openalex.org/W4401213083",
    "https://openalex.org/W4404783449",
    "https://openalex.org/W4401132613",
    "https://openalex.org/W4396812188",
    "https://openalex.org/W4403307008",
    "https://openalex.org/W4411119660",
    "https://openalex.org/W4393033483",
    "https://openalex.org/W4404181464",
    "https://openalex.org/W3200098776",
    "https://openalex.org/W4393001808",
    "https://openalex.org/W4404969987",
    "https://openalex.org/W4399932275",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4406141733",
    "https://openalex.org/W3195865861",
    "https://openalex.org/W3039883906",
    "https://openalex.org/W4367849532",
    "https://openalex.org/W4389833065",
    "https://openalex.org/W4393870852",
    "https://openalex.org/W4406596702",
    "https://openalex.org/W6860710830",
    "https://openalex.org/W4396832709",
    "https://openalex.org/W4399785296",
    "https://openalex.org/W4403232958",
    "https://openalex.org/W4399028724",
    "https://openalex.org/W4392487838",
    "https://openalex.org/W4406477144",
    "https://openalex.org/W4404852772",
    "https://openalex.org/W4405143935"
  ],
  "abstract": "Abstract Retrieval-augmented generation (RAG) systems show promise in specialized knowledge domains, but the tobacco research field lacks standardized assessment frameworks for comparing different large language models (LLMs). This gap impacts public health decisions that require accurate, domain-specific information retrieval from complex tobacco industry documentation. To develop and validate a tobacco domain-specific evaluation framework for assessing various LLMs in RAG systems that combines automated metrics with expert validation. Using a Goal-Question-Metric paradigm, we evaluated two distinct LLM architectures in RAG configurations: Mixtral 8 × 7B and Llama 3.1 70B. The framework incorporated automated assessments via GPT-4o alongside validation by three tobacco research specialists. A domain-specific dataset of 20 curated queries assessed model performance across nine metrics including accuracy, domain specificity, completeness, and clarity. Our framework successfully differentiated performance between models, with Mixtral 8 × 7B significantly outperformed Llama 3.1 70B in accuracy (8.8/10 vs. 7.55/10, p &lt; 0.05) and domain specificity (8.65/10 vs. 7.6/10, p &lt; 0.05). Case analysis revealed Mixtral’s superior handling of industry-specific terminology and contextual relationships. Hyperparameter optimization further improved Mixtral’s completeness from 7.1/10 to 7.9/10, demonstrating the framework’s utility for model refinement. This study establishes a robust framework specifically for evaluating LLMs in tobacco research RAG systems, with demonstrated potential for extension to other specialized domains. The significant performance differences between models highlight the importance of domain-specific evaluation for public health applications. Future research should extend this framework to broader document corpora and additional LLMs, including commercial models.",
  "full_text": "Scalable evaluation framework for \nretrieval augmented generation \nin tobacco research using large \nLanguage models\nSherif Elmitwalli, John Mehegan, Sophie Braznell & Allen Gallagher\nRetrieval-augmented generation (RAG) systems show promise in specialized knowledge domains, \nbut the tobacco research field lacks standardized assessment frameworks for comparing different \nlarge language models (LLMs). This gap impacts public health decisions that require accurate, \ndomain-specific information retrieval from complex tobacco industry documentation. To develop \nand validate a tobacco domain-specific evaluation framework for assessing various LLMs in RAG \nsystems that combines automated metrics with expert validation. Using a Goal-Question-Metric \nparadigm, we evaluated two distinct LLM architectures in RAG configurations: Mixtral 8 × 7B and \nLlama 3.1 70B. The framework incorporated automated assessments via GPT-4o alongside validation \nby three tobacco research specialists. A domain-specific dataset of 20 curated queries assessed model \nperformance across nine metrics including accuracy, domain specificity, completeness, and clarity. Our \nframework successfully differentiated performance between models, with Mixtral 8 × 7B significantly \noutperformed Llama 3.1 70B in accuracy (8.8/10 vs. 7.55/10, p < 0.05) and domain specificity (8.65/10 \nvs. 7.6/10, p < 0.05). Case analysis revealed Mixtral’s superior handling of industry-specific terminology \nand contextual relationships. Hyperparameter optimization further improved Mixtral’s completeness \nfrom 7.1/10 to 7.9/10, demonstrating the framework’s utility for model refinement. This study \nestablishes a robust framework specifically for evaluating LLMs in tobacco research RAG systems, \nwith demonstrated potential for extension to other specialized domains. The significant performance \ndifferences between models highlight the importance of domain-specific evaluation for public health \napplications. Future research should extend this framework to broader document corpora and \nadditional LLMs, including commercial models. \nKeywords Retrieval-Augmented generation, Domain-Specific information retrieval, AI evaluation, Goal-\nQuestion-Metric framework, Large Language models, Expert validation\nThe rapid advancement of artificial intelligence (AI) and natural language processing (NLP) technologies has \nprofoundly transformed the landscape of information retrieval and knowledge management1. These innovations \nare particularly impactful in fields that require the efficient processing and utilization of vast, domain-specific \nknowledge. One such critical domain is tobacco-related research, which encompasses a wide array of topics, \nincluding public health, regulatory policies, industry practices, and scientific studies on the health impacts of \ntobacco use2. The effective extraction of relevant information from the extensive and complex corpus of tobacco-\nrelated documents—ranging from peer-reviewed publications and industry reports to legal frameworks and \npublic health guidelines—is essential for researchers, policymakers, and public health professionals. These \nchallenges are compounded by the need to synthesize diverse data formats, understand the nuanced language \nused across public health and industry contexts, and ensure accuracy in summarizing both quantitative and \nqualitative information3. Addressing these issues requires the development of more sophisticated AI-driven \napproaches tailored to the evolving demands of tobacco research4,5.\nIn recent years, machine learning (ML) has shown considerable promise in addressing some of these \nchallenges within tobacco research. The applications of ML in this domain include smoking cessation support, \nsocial media content analysis, and predictive modelling of tobacco-related health outcomes 6,7. Despite these \nadvances, traditional information retrieval methods often fall short in handling the nuanced and context-\ndependent nature of queries within this highly specialized field 8. These limitations can lead to incomplete or \nTobacco Control Research Group, Department for Health, University of Bath, Bath, UK. email: se606@bath.ac.uk\nOPEN\nScientific Reports |        (2025) 15:22760 1| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports\n\nirrelevant results, which in turn can hinder effective decision-making and policy development, as shown 10. As \nthe field of tobacco research continues to evolve rapidly, there is a pressing need for more advanced tools that can \nkeep pace with the growing body of knowledge and provide accurate, timely insights9.\nNeural models, particularly those powered by deep learning, have begun to address some of these limitations. \nThe availability of large datasets and advancements in computational power have enabled the development of \nsophisticated models capable of processing and interpreting complex information 10. However, these models \noften require substantial data and computational resources and may still struggle to fully capture the semantic \ndepth of queries in domains where the knowledge base is both vast, constantly evolving and nuanced. In such \nfields, the ability to integrate and update information in real-time is critical, making traditional static models \nincreasingly inadequate for the task11.\nTo overcome these challenges, RAG systems have emerged as promising solutions. RAG systems combine \nthe generative capabilities of large language models (LLMs) with the precision of domain-specific knowledge \nbases, enabling them to retrieve and synthesize information more effectively than traditional retrieval methods \ndo. By leveraging the strengths of both retrieval and generation, RAG systems can provide more accurate and \ncontextually relevant responses to complex queries, making them particularly useful in specialized domains \nsuch as tobacco research, where the need for accurate information is high12,13.\nDespite the potential of RAG systems, there is a significant gap in the literature concerning their evaluation \nand optimization in domain-specific contexts. Most existing studies on RAG systems have focused on open-\ndomain question-answering tasks, where the challenges of domain specificity and contextual relevance are \nless pronounced14. The application of RAG systems to specialized domains such as tobacco research remains \nunderexplored, and there is a need for more systematic approaches to evaluate their performance in these \ncontexts. Traditional evaluation metrics, which are often designed for general-purpose models, may not fully \ncapture the nuanced requirements of specialized domains. Moreover, manual evaluation processes are not only \ntime-consuming and labour-intensive but also subject to inconsistencies, making them less reliable for ongoing \noptimization efforts15,16.\nTo address these challenges, our research introduces a novel framework for evaluating and optimizing RAG \nsystems tailored specifically to the domain of tobacco research. This framework leverages advanced language \nmodels not only for information retrieval and generation but also as tools for systematically evaluating the RAG \nsystem itself. A key component of this approach is the development of human-verified reference responses, which \nserve as the gold reference for consistently evaluating the performance of the core LLMs within the RAG system. \nThe evaluation framework is further enhanced by the integration of a Goal-Question-Metric (GQM) approach, \nwhich systematically assesses the RAG system across various dimensions, including relevance, accuracy, \ncompleteness, and scalability. By establishing these reference responses and applying the GQM methodology, we \naim to create a reliable benchmark that allows for the rigorous comparison and optimization of different RAG \nconfigurations, ensuring that the system meets the specific needs of tobacco research.\nGQM provides a structured framework for defining and analysing measurement goals. It operates on \nthree hierarchical levels: at the conceptual level, the goal defines the purpose of measurement from specific \nperspectives; at the operational level, questions are formulated to characterize the assessment process; and at \nthe quantitative level, metrics are identified to provide measurable answers to these questions. The evaluation \nframework employs nine carefully selected metrics that address critical aspects of information retrieval and \ngeneration in tobacco research. These metrics are essential because tobacco research demands high standards \nof accuracy when dealing with regulatory requirements and industry practices, completeness in covering \ncomplex relationships between scientific evidence and policy implications, and precision in using domain-\nspecific terminology. Clear and coherent presentation of information is vital for policy decisions, while context \nawareness ensures appropriate interpretation of industry documentation. Adaptability and domain specificity \nare particularly important given the diverse nature of tobacco research queries, from scientific assessments to \nmarket analyses. Together, these metrics provide a comprehensive evaluation framework that aligns with the \nspecific needs of tobacco research applications.\nIn fields such as software development, GQM has been crucial for understanding system performance, \nidentifying improvement strategies, and creating objective evaluation criteria17. For example, in software quality \nassessment, researchers use GQM to define goals such as improving software reliability, formulating questions \nabout code complexity and error rates, and developing metrics to quantitatively measure these aspects. Its \nadaptability makes GQM particularly powerful in complex domains where traditional evaluation methods may \nfall short, offering a flexible yet rigorous approach to assessment that can be tailored to the specific nuances of \ndifferent research contexts18.\nTo address the gaps identified in domainspecific RAG evaluation, the work is structured around three \nresearch questions:\nRQ1: How do Mixtral 8 × 7B and Llama 3.1 70B differ in their ability to retrieve and generate accurate, \ncontextaware responses to tobacco research queries that involve industry-specific terminology, regulatory \nconcepts, and scientific information?\nRQ2: Which of the nine-evaluation metrics (relevance, accuracy, completeness, clarity, conciseness, \ncoherence, context awareness, adaptability, domain specificity) show the largest performance gaps between these \ntwo model architectures?\nRQ3: To what extent can hyperparameter optimization enhance the performance of a domainspecific RAG \nconfiguration in tobacco research?\nAnswering these questions, this paper makes four key contributions:\n 1. The extension of the GoalQuestionMetric paradigm to RAG systems by defining a structured, tobaccotai -\nlored mapping of goals, questions, and quantitative metrics, thereby advancing theory in AI evaluation.\nScientific Reports |        (2025) 15:22760 2| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\n 2. The development and public release of a goldstandard dataset of 20 specialistvalidated tobacco query–re -\nsponse pairs, enabling reproducible, expertgrounded benchmarking of RAG configurations and providing a \nvaluable resource for tobacco researchers to understand how language models interpret industry documen-\ntation.\n 3. We provide the first headtohead comparison of Mixtral 8 × 7B versus Llama 3.1 70B in a tobacco domain \nRAG setting, demonstrating statistically significant performance differences across all nine metrics.\n 4. We demonstrate how hyperparameter tuning (embedding model, chunk size/overlap, retrieval count) can \nmeaningfully boost RAG performance offering practical guidance for system refinement.\nFinally, we show that our evaluation framework is modelagnostic and readily adaptable to other specialized fields \n(e.g. healthcare, legal research, environmental policy), supporting scalable, transparent, and rigorous assessment \nof RAG systems across diverse domains. The framework’s design enables efficient evaluation of any number of \nLLMs without modification to the core methodology.\nRelated work\nChallenges in domainspecific information retrieval\nThe challenges in domain-specific information retrieval (IR) are multifaceted, including the need for context \nsensitivity, semantic understanding, and the ability to process vast amounts of specialized information. Rateria \nand Singh noted that traditional search techniques, which lack these capabilities, often fail to retrieve pertinent \ndata efficiently in large-scale enterprises where extensive textual information is shared across corporate \nrepositories and intranet websites19. Wu et al.20 showed that domainspecific IR is often undermined by acronyms, \nabbreviations and typographical errors common in realworld corpora, which traditional stringmatching \nmethods fail to address. They introduced Smash, a dynamic programming–based similarity measure that boosts \nmax and mean Fscores by 23.5% and 110.8%, respectively, without relying on predefined synonym rules. This \nwork underscores the need for robust, domaintailored stringmatching techniques to improve retrieval precision \nin specialized IR tasks. Tamine and Goeuriot highlighted that despite the explosive growth and accessibility \nof medical information on the internet leading to increased research activity, current medical search systems \nexhibit low levels of performance, especially when tackling complex tasks such as searching for diagnoses or \ntreatments21. These shortcomings in string-matching and context sensitivity motivate the exploration of neural, \nretrieval‐augmented approaches, as surveyed in the next subsection.\nLarge language models and RAG systems\nWhile domain-specific IR methods struggle with semantic ambiguity and scale, LLMs - especially when \ncombined with retrieval - hold promise. Recent advancements in LLMs have revolutionized the field of artificial \nintelligence, offering unprecedented performance in various applications. Y ang et al. emphasized that LLMs \ndemonstrate remarkable abilities in understanding, language synthesis, and commonsense reasoning, often \nachieving humanlike performance 22. In the queryfocused summarization domain, Zhang et al. 23 propose a \nnovel knowledgeintensive approach that reframes QFS (Query-Focused Summarization) as a twostage task: a \nretrieval module that identifies potentially relevant documents from a largescale corpus based on the input \nquery, and a summarization controller that leverages an LLM with a tailored prompt to produce comprehensive, \nqueryrelevant summaries. Evaluated on a newly created dataset with humanannotated relevance labels, their \nmethod outperforms traditional approaches—especially in scenarios where no preselected document set is \navailable. Building on this foundation, Wiratunga et al. demonstrated the potential of RAG systems in legal \nquestionanswering tasks, highlighting how the integration of casebased reasoning can enrich LLM queries \nwith contextually relevant cases 24. Despite impressive RAG capabilities, systematic, domain-aware evaluation \nremains underdeveloped.\nEvaluation frameworks for language models\nAs LLMs and RAG systems continue to evolve and play vital roles in research and daily use, their evaluation \nbecomes increasingly critical. Chang et al. emphasized the need for comprehensive evaluation methods that \nassess LLMs not only at the task level but also at the societal level to better understand their potential risks and \nimpacts25. Standard IR/NLG (Natural Language Generation) metrics such as ROUGE often fail to reflect true \nquality in domainspecific summarization tasks, as evidenced by a recent evaluation of five LLMs on clinical \ndialogues: ROUGE ranked a finetuned transformer highest, yet clinician ratings and the UniEval metric both \nfavored ChatGPT, with UniEval showing strong correlation with expert scores 26. This underscores the need \nfor domainaware evaluation frameworks that integrate specialized automated measures and humanintheloop \nassessment26. There is a growing recognition of the need for more nuanced and context-aware evaluation \nmethodologies. The authors provided an extensive review of evaluation methods for LLMs, focusing on three \nkey dimensions: what to evaluate, where to evaluate, and how to evaluate. Their work highlighted the importance \nof considering a wide range of evaluation tasks, from general natural language processing to specialized \napplications in medicine, ethics, and education. Although these frameworks address general IR/NLG quality, \nthey rarely target the ethical and terminological demands of tobacco research.\nApplications in tobacco research\nThe optimization of AI systems for domain-specific applications, particularly in fields such as tobacco research, \npresents unique challenges and opportunities. Fu et al. highlighted that machine learning represents a powerful \ntool that could advance research and policy decision-making in tobacco control. However, the effective application \nof these technologies requires careful adaptation to the specific needs and constraints of the domain27.\nScientific Reports |        (2025) 15:22760 3| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nWilson and Cook discussed domain adaptation techniques, offering approaches for handling situations where \na network is trained on labelled data from a source domain and unlabelled data from a related but different target \ndomain. This is particularly relevant in specialized fields such as tobacco research, where labelled data may be \nscarce, but large amounts of unlabelled domain-specific data are available28.\nMoreover, the field of tobacco research presents a unique set of challenges for information retrieval and \ndocument analysis. For example, Legg et al. highlighted the tobacco industry’s long history of attempting to \ninfluence scientific research and public perception, underscoring the need for robust and unbiased information \nretrieval systems that can effectively navigate the complex landscape of tobacco-related documents29.\nIn addition, the application of AI in healthcare and specialized research domains raises significant ethical \nconcerns. In regulated domains such as tobacco research, system trustworthiness hinges not just on retrieval \naccuracy but on fairness, accountability, transparency and ethics (FATE). A recent systematic review of FATE in \ninformation retrieval develops clear definitions, surveys approaches and evaluation methodologies and proposes \ntaxonomies to quantify each dimension of trustworthiness30.\nIn the context of social media and health information dissemination, Singhal et al. explored the ethical use of \nAI and machine learning, focusing on the principles of fairness, accountability, transparency, and ethics (FATE). \nTheir work underscored the importance of developing computational methods and approaches that ensure the \nresponsible application of AI in health-related contexts 31. These studies underscore both the potential and the \nunresolved need for domain-specific evaluation.\nResearch gaps and opportunities\nDespite a growing body of literature on AI technologies in specialized domains, several critical gaps remain \nin the context of tobacco research. First, there is a lack of domainspecific evaluation frameworks for RAG \nsystems: existing metrics and methodologies often fail to capture the nuanced characteristics of highly \nspecialized tobaccorelated corpora. Second, although machine learning methods have demonstrated potential \nin tobacco control studies, the systematic application and assessment of RAG systems within this field remain \nunderexplored32. Third, ethical considerations—particularly fairness, accountability, transparency and bias \nmitigation—are insufficiently integrated into tobaccofocused information retrieval pipelines, leaving a risk of \nperpetuating industrydriven misinformation29,31. Fourth, there is an absence of costeffective yet robust evaluation \nmethodologies tailored to RAG performance in domainspecific settings, making it difficult for researchers and \npractitioners to balance resource constraints with rigorous assessment.\nAddressing these research gaps is essential for advancing both methodological innovation and the responsible \nuse of AI in tobacco research. In this study, we therefore develop a comprehensive, costefficient evaluation \nframework designed specifically for RAG systems analysing tobaccorelated documents; conduct a comparative \nanalysis of leading language models within a RAG setup; optimize retrieval and summarization configurations \nto enhance accuracy and mitigate bias; and propose a generalizable deployment framework for RAG applications \nin tobacco research.\nMethods\nOverview\nThis methodology outlines a detailed approach to evaluating and optimizing RAG systems for tobacco-related \ndocument analysis; addresses the complex challenges of domain-specific information retrieval in tobacco \nresearch; and emphasizes cost effectiveness, robustness, and adherence to ethical guidelines. The methodology \nconsists of four main phases, each designed to systematically build a robust and reliable evaluation framework, \nas shown in Fig. 1.\n 1. The first phase centred on identifying a single, representative document to serve as the foundation for the \nstudy. After a thorough evaluation of potential sources, the Philip Morris International (PMI) Integrated \nReport 2023 was selected for its comprehensive and structured presentation of topics critical to tobacco \nresearch from the PMI33. This document covers a wide range of essential themes, including business perfor-\nmance, industry strategies, product details, regulatory developments, and health impacts, making it an ideal \nbasis for developing domain-specific queries and responses. The selection process was guided by a rigorous \nset of criteria to ensure the suitability of the document for the study’s goals. A tobacco domain specialist con-\nducted an in-depth review of potential candidates, assessing each document for content diversity, specificity, \nand relevance to tobacco research. Particular emphasis was placed on the depth and detail provided by the \ndocument, ensuring that it could support the development of complex and nuanced queries. Additionally, \nthe consistency and clarity of the document’s language were evaluated to ensure that it could facilitate precise \nretrieval and generation tasks within the RAG system.\n 2. The second phase involves generating and refining queries and reference responses focused on tobacco-relat-\ned topics, ensuring that they align with the context and content of the chosen dataset, guided by the capabili-\nties of GPT-4o, a language model developed by OpenAI, and domain expertise. We employ a comprehensive \nquery generation strategy that produces a diverse set of questions across various tobacco research topics, \nguided by prompt engineering and expert input. A taxonomy categorizes these queries by topic, type, and \ncomplexity, ensuring a balanced evaluation dataset. This iterative process ensures that the reference respons-\nes are accurate, unbiased, and suitable for benchmarking the RAG system.\n 3. The third phase constitutes the core of our evaluation process. Here, we implement two parallel RAG con -\nfigurations, one using Mixtral 8 × 7B and another using Llama 3.1 70B as the base language model. These \nsystems process the generated queries, and their outputs are subjected to a multifaceted evaluation. This eval-\nuation employs an innovative approach using the GPT-4o as an automated assessor, guided by a Goal-Ques-\nScientific Reports |        (2025) 15:22760 4| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\ntion-Metric framework. The automated assessment is calibrated and validated through strategic human ex-\npert reviews, ensuring reliability and accuracy in the evaluation process.\n 4. The fourth phase is based on initial evaluation results; we conduct hyperparameter optimization to maximize \nthe retrieval accuracy. We adjusted parameters such as the embedding model, chunk size, and retrieval count \nthrough a grid search, testing each configuration on the gold reference queries. The optimal configuration, \nidentified within the best performing model, was re-evaluated to confirm improvements across metrics such \nas relevance and domain specificity. This optimized setup provided a reliable and efficient configuration for \ntobacco-related information retrieval, establishing a benchmark for future model assessments in the RAG \nframework.\nPreparation and data collection\nThe foundation of our RAG system evaluation lies in the careful selection and preparation of a relevant and \nrepresentative tobacco-related document. The PMI 2023 Integrated Report offers a robust foundation for \nevaluating the RAG system’s ability to handle complex, domain-specific queries. The report not only offers \nextensive insights into these topics but also provides detailed and specific language that is essential for testing \nthe RAG system’s ability to handle domain-specific queries effectively. Although we did not assemble a broad \ncorpus that includes peer-reviewed scientific literature and public health guidelines, we address potential biases \nassociated with the use of industry-produced materials. Our approach includes an independent review of the \ncontent by domain specialists in tobacco research, who critically assess the information to mitigate the risk \nof biased or misleading representations. This document poses these challenges, which is why it was chosen. \nFurthermore, we disclose our document choice transparently to ensure that readers are aware of the source of \nour data and can appropriately interpret our findings within this context.\nThe pre-processed PMI report was prepared for efficient retrieval via the Chroma vector database. We defined \na schema for storing document embeddings and metadata and implemented a scalable indexing strategy tailored \nto the single-source setup. This design enables rapid and accurate retrieval of information while ensuring that \nour RAG system can robustly address queries relevant to the document’s content.\nThrough this focused approach to document selection, including a critical review by domain specialists \nto assess potential biases and accuracy, we establish a well-defined and transparent foundation for evaluating \nour RAG system. While narrower in scope than a full corpus, our methodology ensures clarity, precision, and \ncontextual accuracy in the complex domain of tobacco research. The inclusion of a thorough review process \nensures that the information drawn from industry-produced materials is critically examined to mitigate risks of \nbias or misleading representations.\nFig. 1. RAG system evaluation framework.\n \nScientific Reports |        (2025) 15:22760 5| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nQuery generation and reference response creation\nIn this step, we developed a precise set of queries and responses tailored to the content of the PMI Report, \nguided by the expertise of a tobacco domain specialist. The process begins with the primary domain specialist \n(PS) selecting topics and defining categories of queries on the basis of the document’s key areas. The PS provided \nguidance on the appropriate number of queries for each topic to ensure balanced representation across essential \nthemes: industry strategies/practices, business/market performance, tobacco/nicotine products, scientific \nresearch/health effects, and policy/regulation.\nUsing these guidelines, an initial set of 20 query‒response pairs was generated via GPT-4o within the \nRAG system. The primary specialist (PS) carefully reviewed these pairs for accuracy and alignment with the \ntobacco document, refining them by removing redundant or irrelevant content and adjusting the text to better \nreflect the document’s details. During this review, it became evident that the “policy/regulation” category was \nunderrepresented due to limited content in the document on this topic. To address this, the PS introduced \n“language/terminology” as a replacement focus area, ensuring the inclusion of an underexplored dimension \nof the document. This iterative process enhanced the diversity and relevance of the questions while addressing \ncontent gaps. The refined queries underwent further rounds of expert validation to finalize a comprehensive, \naccurate, and domain-specific reference set. Once this initial review was completed, additional queries were \nadded on certain topics to round out the coverage, ensuring that each topic was represented with multiple well-\ndefined queries.\nAfter the PS finalized a comprehensive set of 20 query‒response pairs, the gold reference set underwent \na rigorous secondary review by two additional domain specialists. These specialists used tracked changes to \nfurther refine the responses, focusing on factual accuracy, consistency with the PMI Report, and appropriate \nterminology. This multistage review process allowed for collaborative refinements and ensured that the gold \nreference set met the highest standards of quality and domain specificity. The iterative validation by multiple \nspecialists not only reinforced the reliability of the reference set but also minimized the risk of bias or inaccuracies, \nestablishing a robust benchmark for evaluating the RAG system’s performance.\nTable 1 illustrates the four-stage iterative workflow by which the gold reference of 20 query–response pairs \nwas developed and validated. In the first round, the Primary Tobacco Specialist (PS) drafted an initial set of 14 \npairs, removing overly broad queries and aligning responses with the PMI 2023 Integrated Report. In the second \nround, the PS expanded and refined this set to 17 pairs by introducing underrepresented topic areas—such as \nindustry terminology—and tightening factual accuracy. The third round involved a final PS review to rephrase \ncomplex responses for clarity and ensure full alignment with the source document, resulting in a complete set of \n20 pairs. Finally, two additional specialists conducted a collaborative review (Round 4), using tracked changes to \nresolve remaining discrepancies, standardize terminology, and confirm factual precision.\nThis structured, multireview process guaranteed that each query–response pair met rigorous domainspecific \nstandards. By progressively narrowing focus—from broad topic coverage to precise wording and factual \ncorrectness—the table underscores how expert validation and iterative refinement combined to produce a \nreliable, biasmitigated benchmark for evaluating RAG system outputs in tobacco research.\nModel selection criteria\nOur evaluation focused on comparing two distinct LLM architectures: Mixtral 8 × 7B and Llama 3.1 70B. \nThese models were selected based on several key criteria relevant to real-world deployment in tobacco research \ncontexts:\n 1. Architectural diversity: Mixtral 8 × 7B employs a mixture-of-experts architecture that theoretically enables \nmore efficient domain specialization, while Llama 3.1 70B represents a traditional dense transformer archi-\ntecture with larger parameter count. This contrast allows us to examine whether architectural differences \nimpact domain-specific performance.\n 2. Open accessibility: Both models are openly available, ensuring our framework and findings can be repro -\nduced and extended by other researchers. This accessibility is critical for developing evaluation standards in \npublic health domains where transparency is essential.\n 3. Resource considerations: Our selection balanced performance potential with computational efficiency. While \ncommercial models like GPT-4o were used as evaluation tools in our framework, they would be prohibitively \nexpensive for deployment as the core RAG model in many research settings. Our framework deliberately \nRound Reviewer Actions taken Example of changes Outcome\n1 Primary Tobacco \nSpecialist\nDefined topics, specified query numbers, \nconducted initial review, removed redundancies, \nmodified responses for specificity.\nRemoved overly broad queries, ensured responses were \ntobacco specific.\nProduced initial set of 14 \npairs.\n2 Primary Tobacco \nSpecialist\nAdded queries, refined responses for accuracy and \nspecificity.\nIntroduced new queries in underrepresented areas, adjusted \nresponses to better align with the PMI document.\nExpanded and refined set of \n17 pairs.\n3 Primary Tobacco \nSpecialist\nFinalized the initial gold reference by modifying \nor rejecting responses.\nRephrased complex responses for clarity, ensured responses \nwere factually precise and document aligned.\nPrepared set of 20 \nquery-response pairs for \nsubsequent specialist review.\n4 Second and Third \nTobacco Specialist Assessed the gold reference pairs Used tracked changes to revise each query-response pairs \naccording to the reference document\nPrepared a revised final set \nof 20 query-response pairs.\nTable 1. Evaluation rounds used to develop the gold standard reference.\n \nScientific Reports |        (2025) 15:22760 6| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nfocuses on more accessible open-source models that tobacco researchers with varying resource constraints \ncould feasibly implement.\n 4. Parameter scale comparison: The significant difference in parameter count (Mixtral’s 8 × 7B architecture \nversus Llama’s 70B parameters) enables investigation of whether larger models necessarily provide better \nperformance in specialized domains.\n 5. Proof of concept approach: This study demonstrates a proof of concept for our evaluation framework using \ntwo representative models with different architectures. The framework itself is designed to be model-ag -\nnostic and can be extended to evaluate any number of LLMs, whether open-source or commercial, without \nmodification to the core methodology. It’s important to note that GPT-4o serves a distinct role in our frame-\nwork as an automated evaluation tool rather than as one of the evaluated models. This separation maintains \nthe integrity of our assessment process while leveraging advanced language understanding capabilities that is \nassociated with human experts validated GPT-4o’s assessments to ensure reliability for consistent evaluation \nacross model outputs.\n 6. Future extensibility: The framework can incorporate any capable LLM—opensource, commercial (e.g. GPT-\n4o or its successors), or future releases—can be slotted into either the retrieval/generation or evaluation com-\nponent without altering the core GQM methodology, ensuring seamless adaptation as new models emerge.\nRAG system evaluation\nThe RAG system evaluation begins with the setup of two parallel configurations using Mixtral 8 × 7B and Llama \n3.1 70B as base language models. Mixtral 8 × 7B, a mixture-of-experts model known for efficiency and task-\nspecific performance, and Llama 3.1 70B, a larger model with broad general language understanding. Moreover, \nthe Mixtral 8 × 7B model is particularly advantageous for scenarios requiring rapid inference and low latency, \nmaking it ideal for real-time applications where efficiency is crucial 34. In contrast, Llama 3.1 70B’s extensive \ntraining on diverse datasets equips it with a robust understanding of nuanced language and complex queries, \nenabling it to generate more contextually relevant and coherent responses across a wide range of topics 35. This \ncomparison aims to explore the trade-offs between specialized architecture and general capability in domain-\nspecific applications such as tobacco research. This dual-configuration approach allows us to compare the \nperformance of these models within the RAG framework, providing valuable insights into their respective \nstrengths and weaknesses in handling tobacco-related queries. To ensure a fair comparison, we implement \nconsistent retrieval mechanisms and prompt templates across both configurations, isolating the impact of the \nbase language model on overall system performance.\nFor both configurations, we set the temperature parameter to 0.7. This relatively low temperature provides \na good balance between deterministic outputs and a small degree of variability that can be beneficial in natural \nlanguage tasks, particularly in a domain-specific context such as tobacco research, where consistency is crucial but \nsome flexibility in language generation can enhance the naturalness of responses36. With our RAG configurations \nin place, we proceed to the response generation phase. Each query from our carefully curated set (gold reference) \nis processed through both RAG configurations, generating responses that adhere to a standardized format. This \nformat includes the query and response text.\nThe core of our evaluation process lies in the automated assessment of these generated responses, leveraging \nthe advanced capabilities of the GPT-4o. We developed a comprehensive evaluation prompt for the GPT-4o, \nincorporating the GQM approach to ensure a multifaceted assessment. For each RAG-generated response, GPT-\n4o evaluates nine metrics on a 0–10 scale.\nWe selected nine evaluation metrics—relevance, accuracy, completeness, clarity, conciseness, coherence, \ncontext awareness, adaptability, and domain specificity—to align with the GQM) paradigm and to capture \nboth general IR quality and the specialized demands of tobacco research. Relevance and accuracy ensure that \neach response directly addresses the query intent and faithfully represents source facts; completeness, clarity, \nand conciseness balance thoroughness with readability; coherence and context awareness gauge logical flow \nand sensitivity to regulatory or historical context; and adaptability and domain specificity measure the system’s \nflexibility across subdomains and its grasp of tobaccospecific terminology and concepts.\nThis automated evaluation generates a detailed report for each response, complete with scores for each \nmetric, qualitative feedback highlighting strengths and weaknesses, and suggestions for improvement. This \napproach allows for a comprehensive and consistent evaluation, which would be impractical to achieve through \nmanual human assessment alone.\nTo ensure the reliability of our automated evaluation process, we implement a human evaluation calibration \nstep. We evaluated responses for review by human specialists in tobacco research. These specialists assess the \nsame metrics as the automated system does, allowing us to compare human evaluations with GPT-4o evaluations. \nThis comparison serves multiple purposes: it helps us assess the reliability of the automated evaluation process, \nidentify any systematic biases or discrepancies, and calibrate the GPT-4o evaluation prompt if necessary. This \nhuman-in-the-loop approach enhances the credibility of our evaluation framework for greater accuracy.\nTo rigorously compare the performance of the Mixtral 8 × 7B and Llama 3.1 70B LLMs, we employed a \npaired t-test to analyse their overall performance across multiple evaluation metrics. We assessed each model’s \nperformance via nine criteria: relevance, accuracy, completeness, clarity, conciseness, coherence, context \nawareness, adaptability, and domain specificity, as shown in Table 2. For each query in our dataset, both models \ngenerated responses that were then evaluated via a standardized scoring system ranging from 0 to 10 for each \nmetric. We calculate an aggregate performance score by averaging these individual metric scores. The paired \nt-test allowed us to account for the matched nature of our data, where each query was processed by both models \nunder identical conditions. We set our significance level (α) at 0.05 to determine statistical significance. The \nt-test examined whether there was a statistically significant difference between the aggregate performance scores \nScientific Reports |        (2025) 15:22760 7| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nof the two models. A p-value less than 0.05 would indicate a meaningful performance difference, whereas a \np-value of 0.05 or greater would suggest no statistically significant distinction between the models.\nThe comprehensive nature of our evaluation process, which combines automated assessment, human \ncalibration, statistical analysis, and efficiency considerations, allows us to gain a holistic understanding of \neach RAG configuration’s performance in the context of tobacco research. This multifaceted approach not \nonly enables us to determine which base language model performs better within the RAG framework but also \nprovides detailed insights into specific areas of strength and weakness. These insights are invaluable for the \nsubsequent optimization and fine-tuning phase, guiding our efforts to enhance the system’s performance.\nMoreover, this evaluation framework is designed to be adaptable and scalable. As new language models or \nRAG techniques emerge, they can be easily incorporated into this evaluation process, allowing for continuous \nassessment and improvement of our system. The detailed metrics and analysis also provide a solid foundation for \ncommunicating results to stakeholders, whether they are researchers, policymakers, or technical teams, ensuring \ntransparency and facilitating informed decision-making in the deployment and use of RAG systems in tobacco \nresearch.\nHyperparameter optimization\nFollowing the initial RAG evaluation, we performed hyperparameter tuning on the LLM that achieved the \nhighest aggregate score (the winning LLM). This allowed us to concentrate our compute budget on the most \npromising architecture while fully illustrating our optimization procedure. We targeted four retrievalrelated \nhyperparameters—embedding model, chunk size, retrieval count, and chunk overlap [37]—and conducted a \ngrid search38 across a predefined range of values.\nFor each hyperparameter configuration, the responses generated by the RAG system on the test queries are \nevaluated via the same automated and statistical approach. This allows us to identify the optimal hyperparameter \nvalues that maximize various performance metrics, such as relevance, accuracy and completeness, based on our \nevaluation methodology.\nThe precise hyperparameters and ranges tested are noted in the results section, where we also report on \nthe optimal configuration identified through this process. Our goal is to enhance information retrieval and \ngeneration abilities according to the standards of domain-specific evaluation. The selection of hyperparameters \nfor optimization was guided by both theoretical considerations and empirical observations from previous \nstudies on RAG systems. The embedding model choice directly impacts the quality of document representation \nand retrieval, with “all-MiniLM-L6-v2” and “paraphrase-MiniLM-L6-v2” selected on the basis of their strong \nperformance in sentence embedding tasks39. Chunk size is crucial for balancing context preservation and retrieval \ngranularity; we explored sizes of 100, 300, and 500 tokens, covering a range that allows for both fine-grained \nretrieval and sufficient context capture. Chunk overlap percentages (0, 25, 50) were included to investigate \ntheir effect on maintaining context continuity between chunks, potentially improving retrieval relevance for \nqueries that span chunk boundaries. The number of retrieved chunks (1, 3, 5) was chosen to explore the trade-\noff between providing sufficient context and introducing noise; these values are based on common practices \nin the RAG literature 40). By systematically exploring these hyperparameters, we aim to optimize the RAG \nsystem’s performance specifically for the nuances of tobacco research documents, balancing retrieval accuracy, \ncomputational efficiency, and the unique contextual requirements of the domain. All scripts, configuration files, \nand the full implementation of the RAG evaluation framework are publicly available at our GitHub repository43.\nResults\nInitial RAG system evaluation\nStatistical analysis\nWe conducted a statistical analysis to compare the performance of the Mixtral 8 × 7B and Llama 3.1 70B \nconfigurations across the evaluation metrics. Paired t-tests revealed a statistically significant difference in \nperformance between the two configurations across most metrics, with Mixtral consistently outperforming \nLlama (p < 0.05 for all key metrics). Compared with Llama, Mixtral achieved a mean performance improvement \nof 0.52 points per metric. The most significant differences were observed in accuracy (Mixtral: 8.8 vs. Llama: \n7.55, mean difference: 1.25) and domain specificity (Mixtral: 8.65 vs. Llama: 7.6, mean difference: 1.05). These \nMetric Description\nRelevance Does the response directly address the query’s intent?\nAccuracy Is the information provided factually correct and free from errors?\nCompleteness Does the response provide a comprehensive and thorough answer?\nClarity Is the response easy to understand and free from ambiguity?\nConciseness Is the response concise and avoids unnecessary redundancy?\nCoherence Does the response flow logically and maintain a consistent narrative?\nContext Awareness Does the response demonstrate understanding of the broader context and relevant information?\nAdaptability Can the response be adapted to different query formats or contexts?\nDomain Specificity Does the response exhibit knowledge and expertise in the tobacco research domain?\nTable 2. RAG system evaluation metrics.\n \nScientific Reports |        (2025) 15:22760 8| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nresults highlight Mixtral’s superior ability to provide precise, context-aware information tailored to tobacco \nresearch.\nOverall performance comparison\nThe gold reference queries were processed through both systems, and their responses were evaluated using our \nGPT-4o-based automated assessment framework. The evaluation criteria are defined in Table  2, and the mean \nscores for each metric are presented in Table  3. These results indicate that the Mixtral 8 × 7B configuration \nconsistently outperformed the Llama 3.1 70B configuration across nearly all evaluated metrics.\nTable 3 reports the mean scores (0–10) for each evaluation metric and the aggregate performance of the two \nRAG configurations. Mixtral 8 × 7B consistently outperformed Llama 3.1 70B across almost all metrics, with the \nlargest gaps observed in Accuracy and Domain Specificity (both p < 0.05, paired ttest).\nAs shown in Table 3, Mixtral 8 × 7B achieves its greatest advantages in Accuracy (mean + 1.25) and Domain \nSpecificity (mean + 1.05), indicating superior handling of tobaccospecific terminology and factual precision. \nEven where Llama slightly outperforms Mixtral on Conciseness (–0.35), the overall performance gap remains \nstatistically significant (p < 0.05). These results underscore Mixtral’s stronger domain-tailored capabilities within \nthe RAG framework.\nThe Mixtral 8 × 7B configuration demonstrated strong performance in relevance (mean score 8.2), accuracy \n(8.8), clarity (8.85), and domain specificity (8.65). The largest performance gaps between the two configurations \nwere observed in Domain Specificity (1.05 difference) and Accuracy (1.25 difference), suggesting that Mixtral \n8 × 7B was better able to leverage domain-specific knowledge and provide more precise information in the \ncontext of tobacco research.\nWe analysed performance via different query categories to gain deeper insights into the strengths and \nweaknesses of each configuration. Figure  2 presents a radar chart comparing the performance of both \nconfigurations across the nine metrics, which are evaluated on six main query categories: Industry strategies/\npractices, business/market performance, tobacco/nicotine products, science/health effects, policy/regulation, \nand language/terminology.\nThe radar chart shows that Mixtral 8 × 7B outperforms Llama 3.1 70B across nearly all metrics, with the most \nsignificant differences observed in accuracy and domain specificity. These findings suggest that the Mixtral 8 × \n7B mixture may provide a better grasp of the complex industry-specific knowledge relevant to tobacco research.\nCase analysis of model performance differences\nTo provide deeper insights into the performance differences between Mixtral 8 × 7B and Llama 3.1 70B, we \nconducted detailed case analyses of specific queries where performance diverged significantly. Three representative \ncases highlight the key distinctions in how these models handle tobacco domain-specific information:\nCase 1 Terminology precision in product categories.\nFor the query “Explain PMI’s product portfolio categorization, ” Mixtral achieved significantly higher accuracy \nscores (9.2 vs. 7.1). Examining the responses reveals that Mixtral correctly identified all four official product \ncategories used by PMI (combustible products, heated tobacco products, e-vapor products, and oral products) \nwith precise terminology matching the document. In contrast, Llama’s response introduced inconsistent \nterminology, referring to “heat-not-burn devices” rather than the document-specific “heated tobacco products” \nterminology, and incorrectly grouped certain products across categories. This demonstrates Mixtral’s superior \nprecision in maintaining domain-specific terminology.\nCase 2 Scientific context integration.\nWhen asked “What scientific studies does PMI reference regarding IQOS?“ , both models retrieved relevant \ninformation, but Mixtral (scoring 8.7 vs. Llama’s 6.9 on completeness) more effectively integrated scientific \ncontext. Mixtral accurately connected PMI’s scientific assessment program to specific study types and regulatory \nsubmissions, while Llama provided a more generalized summary without capturing the hierarchical relationship \nMetric Mixtral 8 × 7B Llama 3.1 70B Δ (Mixtral–Llama)\nRelevance 8.20 7.75 + 0.45\nAccuracy 8.80 7.55 + 1.25\nCompleteness 7.10 6.60 + 0.50\nClarity 8.85 8.65 + 0.20\nConciseness 7.95 8.30 –0.35\nCoherence 8.65 8.35 + 0.30\nContext Awareness 7.75 7.15 + 0.60\nAdaptability 7.60 6.90 + 0.70\nDomain Specificity 8.65 7.60 + 1.05\nOverall Score 8.17 7.65 + 0.52\nTable 3. Mean metric scores and overall performance for mixtral 8 × 7B versus Llama 3.1 70B (n = 20 queries).\n \nScientific Reports |        (2025) 15:22760 9| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nbetween PMI’s scientific framework and specific studies. This illustrates Mixtral’s enhanced ability to maintain \nrelational context in complex scientific information.\nCase 3 Regulatory information accuracy.\nFor regulatory-focused queries such as “Explain PMI’s regulatory environment in key markets, ” Mixtral \ndemonstrated superior context awareness (8.5 vs. 6.8). Mixtral correctly identified specific regulatory frameworks \nmentioned in the document, including the EU Tobacco Products Directive and FDA PMTA process, while Llama \nproduced some factually incorrect statements about regulatory statuses in specific countries not supported by \nthe document. This case underscores the critical importance of factual precision in regulatory contexts, where \neven minor misrepresentations can lead to significant consequences.\nThese cases demonstrate that Mixtral’s superior performance stems primarily from its enhanced ability to \nmaintain terminology precision, integrate complex relational information, and avoid factual errors in specialized \ndomains - capabilities particularly valuable in tobacco research where regulatory and scientific precision is \nessential.\nHyperparameter optimization\nFollowing the comparative evaluation, we focused our hyperparameter optimization efforts on the Mixtral \n8 × 7B configuration for several strategic reasons. First, Mixtral demonstrated statistically significant superior \nperformance across key metrics (p < 0.05), particularly in accuracy (8.8 vs. 7.55) and domain specificity (8.65 vs. \n7.6), making it the more promising candidate for further improvement. Second, this focused approach allowed \nus to conduct a more thorough exploration of the parameter space within our computational constraints, \nrather than splitting resources across both models. Third, Mixtral’s mixture-of-experts architecture proved \nparticularly well-suited to tobacco domain specificity, suggesting greater potential for optimization gains. This \nstrategic decision to optimize the better-performing model enabled us to achieve substantial improvements in \ncompleteness (+ 0.8) and overall performance (+ 0.14) as detailed in the following analysis.\nTo enhance the performance of the Mixtral 8 × 7B configuration, we optimized key hyperparameters \ninfluencing retrieval and synthesis quality: the embedding model, chunk size, chunk overlap, and number of \nretrieved chunks. We tested ‘all-MiniLM-L6-v2’ and ‘paraphrase-MiniLM-L6-v2’ for semantic effectiveness, \nadjusted the chunk size and overlap to balance context and granularity, and varied the number of chunks \nretrieved to ensure sufficient context without adding noise.\nUsing a grid search methodology, we tested multiple combinations of these parameters on a predefined test \nset of queries, assessing each configuration against our evaluation metrics. The optimal configuration identified \nwas “all-MiniLM-L6-v2” as the embedding model, a chunk size of 300 tokens, no chunk overlap, and the \nretrieval of three chunks per query. This configuration achieved the best balance between relevance, accuracy, \nFig. 2. Radar chart comparing the performance of Mixtral 8 × 7B and Llama 3.1 70B using the gold reference.\n \nScientific Reports |        (2025) 15:22760 10| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nand completeness, demonstrating its suitability for handling the nuanced demands of tobacco research while \nmaintaining computational efficiency.\nFigure 3 illustrates the grid search process conducted to optimize hyperparameters for the Mixtral 8 × 7B \nconfiguration. Each point in the grid represents a unique combination of parameters tested, including embedding \nmodels, chunk sizes, chunk overlap percentages, and the number of retrieved chunks. The highlighted region \nindicates the parameter set that yielded the highest average performance across metrics such as relevance, \naccuracy, and completeness. The results show a clear trend where moderate chunk sizes (e.g., 300 tokens) and no \noverlap produced the best outcomes, balancing context preservation and retrieval granularity.\nTable 4 highlights the performance improvements achieved through hyperparameter optimization of the \nMixtral 8 × 7B configuration. The optimized setup, which used the “all-MiniLM-L6-v2” embedding model, a \nchunk size of 300 tokens, no overlap, and three retrieved chunks, demonstrated gains across most evaluation \nmetrics compared with the initial configuration. Notably, completeness showed the greatest improvement (+ \n0.8 points), indicating a more thorough synthesis of information. The relevance (+ 0.35 points) and adaptability \n(+ 0.3 points) also improved, highlighting the system’s enhanced ability to retrieve and respond to complex, \ndomain-specific queries. While minor decreases in accuracy (−0.10 points) and clarity (−0.05 points) were \nMetric Initial Optimized Improvement\nRelevance 8.20 8.55 + 0.35\nAccuracy 8.80 8.70 −0.10\nCompleteness 7.10 7.90 + 0.80\nClarity 8.85 8.80 −0.05\nConciseness 7.95 8.10 + 0.15\nCoherence 8.65 8.75 + 0.10\nContext Awareness 7.75 7.80 + 0.05\nAdaptability 7.60 7.90 + 0.30\nDomain Specificity 8.65 8.70 + 0.05\nOverall 8.17 8.36 + 0.14\nTable 4. Performance comparison of the initial and optimized mixtral 8 × 7B configurations.\n \nFig. 3. Hyperparameter optimization based on grid search, showing overall scores vs. (top left) chunk sizes, \n(top right) chunk overlap percentages, (bottom left) the number of retrieved chunks, and (bottom right) \nembedding models.\n \nScientific Reports |        (2025) 15:22760 11| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nobserved, these trade-offs were minimal and did not significantly impact overall performance. The optimized \nconfiguration’s overall score increased from 8.17 to 8.36, underscoring the effectiveness of the parameter \nadjustments in refining the RAG system for tobacco research applications.\nThe observed improvements suggest that the optimized hyperparameters enhance the system’s ability to \nretrieve and synthesize relevant information from the tobacco domain corpus, as illustrated in Fig. 4.\nThese findings provide a robust basis for understanding the comparative performance of the evaluated RAG \nconfigurations. The implications of these results are explored in the following section.\nCase analysis of optimization impact\nTo illustrate qualitatively how hyperparameter tuning improves our RAG pipeline, we selected three representative \nqueries—covering scientific context integration, product categorization, and regulatory compliance—and \ntracked changes in three key metrics (Completeness, Relevance, and Domain Specificity) on a 0–10 scale.\nCase 1 For the query “What scientific studies does PMI reference regarding IQOS?“ , the pre-optimization sys-\ntem (chunk size = 100 tokens) returned fragmented study details without linking them to PMI’s broader scientif-\nic framework. For example, it listed individual toxicology studies but failed to connect them to PMI’s systematic \nassessment program. After increasing the chunk size to 300 tokens, the model produced a cohesive synthesis that \nconnected preclinical studies, clinical trials, and post-market surveillance within PMI’s regulatory submission \nframework. This change raised the Completeness score from 7.1 to 7.9, reflecting better preservation of scientific \ncontext and methodological relationships.\nCase 2 When asked “Explain PMI’s product portfolio categorization, ” the original setup (retrieval count = 1 \nchunk) yielded an answer focused solely on heated tobacco products, omitting other categories. Post-optimiza-\ntion (retrieval count = 3 chunks), the response comprehensively covered all product lines: combustible products \n(cigarettes), heated tobacco products (IQOS), e-vapor products (VEEV), and oral products (ZYN), along with \ntheir respective market positions and regulatory statuses. This broader context boosted the Relevance score from \n8.2 to 8.55, demonstrating improved topic coverage and strategic context.\nCase 3 In response to “Describe PMI’s regulatory compliance measures, ” the pre-optimization output mixed \nterminology across different regulatory frameworks due to small chunks and insufficient context. After opti -\nmization, the system maintained consistent terminology throughout: accurately distinguishing between FDA \nPMTA (Premarket Tobacco Application) requirements for novel products, EU TPD (Tobacco Products Direc-\ntive) compliance for heated tobacco products, and ICH (International Council for Harmonisation standards) \nguidelines for toxicological assessments. The enhanced parameter configuration preserved Domain Specificity \nFig. 4. Radar plot for the original and optimized Mixtral performance.\n \nScientific Reports |        (2025) 15:22760 12| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\nat 8.7 while increasing Adaptability from 7.6 to 7.9, showing improved handling of complex regulatory relation-\nships.\nTogether, these three cases demonstrate how our optimized parameters—300-token chunk size, three-passage \nretrieval, and the all-MiniLM-L6-v2 embedding model—work in concert to enhance both the completeness and \nprecision of RAG outputs in tobacco research applications. The improvements in scientific context integration, \nproduct portfolio coverage, and regulatory terminology consistency align with the quantitative gains reported \nin Table 4, validating our hyperparameter optimization approach for domain-specific information retrieval and \ngeneration tasks.\nDiscussion\nOur evaluation framework provides comprehensive insights into the performance of two RAG configurations \nin the context of tobacco research. Our headtohead evaluation of Mixtral 8 × 7B and Llama 3.1 70B in a \ntobaccofocused RAG framework highlights clear architectural tradeoffs with practical implications. Mixtral’s \nmixtureofexperts routing delivers superior accuracy (8.8 vs. 7.55, p < 0.05) and domain specificity (8.65 vs. 7.60, \np < 0.05) by steering tobaccocentric tokens through specialist subnetworks, preserving precise terminology (e.g., \nCase 1’s productcategory labels) and capturing regulatory relationships (Case 3). In contrast, Llama’s monolithic \n70 billionparameter model produces more concise text but occasionally overgeneralizes or drifts factually, as \nseen in Case 2’s scientific context integration. Modest hyperparameter tuning of Mixtral—adjusting embedding \nmodel, chunk size/overlap, and retrieval count—boosted completeness by + 0.8 and overall score by + 0.14, \ndemonstrating that even small parameter tweaks can meaningfully sharpen performance in specialized domains. \nThese findings underscore the importance of matching model architecture—expert routing versus dense \nmodeling—to domain requirements and suggest that tobacco researchers should prioritize mixtureofexperts \nmodels when precision and terminology fidelity are paramount.\nSome limitations remain. Our evaluation relies on GPT-4o as an automated assessor, which may introduce \nbias despite calibration against three human specialists. Furthermore, our singlesource design around the PMI \n2023 Integrated Report enabled tightly controlled benchmarking and streamlined goldstandard construction \nbut limits generalizability. Expanding to peerreviewed articles, policy guidelines or alternative industry reports \nwould enrich coverage yet demands substantial additional curation, expert calibration and alignment of our \nGQM metrics—efforts that can quickly become unwieldy. A more conservative, phased expansion—adding one \ndocument type at a time and revalidating queries and reference responses—offers a practical path forward, \nbalancing the benefits of broader coverage against the overhead of maintaining a reliable, wellevaluated standard. \nThrough this cautious roadmap, future work can extend our framework to diverse document corpora, additional \nLLMs (opensource and commercial), and emerging RAG variants, ensuring its robustness and adaptability to \nevolving domain needs.\nConclusion\nThis study introduces a robust, domainspecific framework for evaluating retrievalaugmented generation \nsystems in tobacco research. Leveraging the GQM paradigm, we map highlevel goals to concrete metrics, \nrelease a publicly available goldstandard dataset of 20 specialistvalidated query–response pairs, and deliver the \nfirst headtohead comparison of Mixtral 8 × 7B versus Llama 3.1 70B in this field. Our results demonstrate that \nMixtral’s mixtureofexperts architecture consistently surpasses Llama in accuracy and domain specificity, while \nLlama yields slightly more concise outputs. We further show that targeted hyperparameter optimization can \nmeaningfully boost completeness and overall performance, offering practical guidance for refining RAG systems \nin specialized domains.\nWhile our singlesource design around the PMI 2023 Integrated Report enabled tightly controlled \nbenchmarking33, it limits broader generalizability. Future work will expand the corpus to include peerreviewed \narticles, policy guidelines, and alternative industry reports in a phased manner; incorporate additional \nopensource and commercial LLMs; and explore realworld deployment scenarios. Moreover, although we used \npaired ttests for our twomodel comparison, the framework supports repeatedmeasures ANOV A for robust \nmultimodel evaluation. Finally, the design is readily adaptable to nextgeneration RAG paradigms—such as \nGraphRAG41 and AgenticRAG 42 —ensuring a consistent benchmark as architectures evolve. By providing a \ntransparent, scalable toolkit for domaintailored RAG evaluation, this work equips researchers and policymakers \nto deploy RAG systems effectively in tobacco control and other specialized fields requiring precise, contextaware \ninformation retrieval.\nData availability\nThe datasets generated and analysed during the current study are available in the Harvard Dataverse repository, \nhttps://doi.org/10.7910/DVN/GVGVMP, and the complete codebase is hosted at our GitHub repository [43].\nReceived: 25 February 2025; Accepted: 4 June 2025\nReferences\n 1. van Schaik, T. A. & Pugh, B. A field guide to automatic evaluation of llm-generated summaries. InProceedings of the 47th \nInternational ACM SIGIR Conference on Research and Development in Information Retrieval 2024 Jul 10 (pp. 2832–2836).\n 2. Elmitwalli, S., Mehegan, J., Wellock, G., Gallagher, A. & Gilmore, A. Topic prediction for tobacco control based on COP9 tweets \nusing machine learning techniques. Plos One. 19 (2), e0298298 (2024).\nScientific Reports |        (2025) 15:22760 13| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\n 3. Fitzpatrick, I., Bertscher, A. & Gilmore, A. B. Identifying misleading corporate narratives: the application of linguistic and \nqualitative methods to commercial determinants of health research. PLOS Global Public. Health. 2 (11), e0000379 (2022).\n 4. Hambarde, K. A. & Proenca, H. Information retrieval: recent advances and beyond. IEEE Access. 11, 76581–76604 (2023).\n 5. Guțu, B. M. & Popescu, N. Exploring data analysis methods in generative models: from Fine-Tuning to RAG implementation. \nComputers 13 (12), 327 (2024).\n 6. Elmitwalli, S. & Mehegan, J. Sentiment analysis of COP9-related tweets: a comparative study of pre-trained models and traditional \ntechniques. Front. Big Data. 7, 1357926 (2024).\n 7. Elmitwalli, S., Mehegan, J., Gallagher, A. & Alebshehy, R. Enhancing sentiment and intent analysis in public health via fine-tuned \nlarge Language models on tobacco and e-cigarette-related tweets. Front. Big Data. 7, 1501154 (2024).\n 8. Mateos, P . & Bellogín, A. A systematic literature review of recent advances on context-aware recommender systems. Artif. Intell. \nRev. 58 (1), 1–53 (2025).\n 9. Demner-Fushman, D., Elhadad, N. & Friedman, C. Natural language processing for health-related texts. InBiomedical informatics: \nComputer applications in health care and biomedicine 2021 Jun 1 241–272 (Springer International Publishing, ).\n 10. Upadhyay, R. & Viviani, M. Enhancing health information retrieval with RAG by prioritizing topical relevance and factual \naccuracy. Discover Comput. 28 (1), 27 (2025).\n 11. Siriwardhana, S. et al. Improving the domain adaptation of retrieval augmented generation (RAG) models for open domain \nquestion answering. Trans. Association Comput. Linguistics. 11, 1–7 (2023).\n 12. Fan, W . et al. A survey on rag meeting llms: Towards retrieval-augmented large language models. InProceedings of the 30th ACM \nSIGKDD Conference on Knowledge Discovery and Data Mining 2024 Aug 25 (pp. 6491–6501).\n 13. Xu, S., Chen, M. & Chen, S. Enhancing retrieval-augmented generation models with knowledge graphs: Innovative practices \nthrough a dual-pathway approach. InInternational Conference on Intelligent Computing. Aug 1 (pp. 398–409). Singapore: \nSpringer Nature Singapore. (2024).\n 14. Han, R. et al. Rag-qa arena: evaluating domain robustness for long-form retrieval augmented question answering. ArXiv Preprint \nArXiv:2407.13998. Jul 19. (2024).\n 15. Soto-Jiménez, F ., Martínez-Velásquez, M., Chicaiza, J., Vinueza-Naranjo, P . & Bouayad-Agha, N. RAG-based question-answering \nsystems for closed-domains: Development of a prototype for the pollution domain. InIntelligent Systems Conference 2024 Jul 31 \n(pp. 573–589). Cham: Springer Nature Switzerland.\n 16. Mayfield, J. et al. On the evaluation of machine-generated reports. InProceedings of the 47th International ACM SIGIR Conference \non Research and Development in Information Retrieval. Jul 10 (pp. 1904–1915). (2024).\n 17. Goktas, P . Ethics, transparency, and explainability in generative Ai decision-making systems: A comprehensive bibliometric study. \nJ. Decis. Syst. 11, 1–29 (2024 Oct).\n 18. Zhang, B. et al. Evaluating Large Language Models with Enterprise Benchmarks. InProceedings of the 2025 Conference of the \nNations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: \nIndustry Track) 2025 Apr (pp. 485–505).\n 19. Rateria, S., Singh, S. & Transparent Low resource, and Context-Aware information retrieval from a closed domain knowledge base. \nIEEE Access. 12, 44233–44243 (2024).\n 20. Wu, J. et al. Dealing with Acronyms, Abbreviations, and Typos in Real-World Entity Matching. Proceedings of the VLDB \nEndowment. ;17(12):4104-16. (2024).\n 21. Tamine, L. & Goeuriot, L. Semantic information retrieval on medical texts: research challenges, survey, and open issues. ACM \nComput. Surv. (CSUR). 54 (7), 1–38 (2021).\n 22. Y ang, J. et al. Harnessing the power of Llms in practice: A survey on Chatgpt and beyond. ACM Trans. Knowl. Discovery Data. 18 \n(6), 1–32 (2024).\n 23. Zhang, W . et al. Beyond relevant documents: A knowledge-intensive approach for query-focused summarization using large \nlanguage models. InInternational Conference on Pattern Recognition Springer, Cham. (2025) (pp. 89–104).\n 24. Wiratunga, N. et al. CBR-RAG: case-based reasoning for retrieval augmented generation in LLMs for legal question answering. \nInInternational Conference on Case-Based Reasoning 2024 Jun 24 (pp. 445–460). Cham: Springer Nature Switzerland.\n 25. Chang, Y . et al. A survey on evaluation of large Language models. ACM Trans. Intell. Syst. Technol. 15 (3), 1–45 (2024).\n 26. Fraile Navarro, D. et al. Expert evaluation of large Language models for clinical dialogue summarization. Sci. Rep. 15 (1), 1195 \n(2025).\n 27. Fu, R. et al. Machine learning applications in tobacco research: a scoping review. Tob. Control. 32 (1), 99–109 (2023).\n 28. Wilson, G. & Cook, D. J. A survey of unsupervised deep domain adaptation. ACM Trans. Intell. Syst. Technol. (TIST). 11 (5), 1–46 \n(2020).\n 29. Legg, T., Clift, B. & Gilmore, A. B. Document analysis of the foundation for a Smoke-Free world’s scientific outputs and activities: \na case study in contemporary tobacco industry agnogenesis. Tob. Control. 33 (4), 525–534 (2024).\n 30. Bernard, N. & Balog, K. A systematic review of fairness, accountability, transparency, and ethics in information retrieval. ACM \nComput. Surveys. 57 (6), 1–29 (2025).\n 31. Singhal, A., Neveditsin, N., Tanveer, H. & Mago, V . Toward fairness, accountability, transparency, and ethics in AI for social media \nand health care: scoping review. JMIR Med. Inf. 12 (1), e50048 (2024).\n 32. Lopez, I. et al. Clinical entity augmented retrieval for clinical information extraction. Npj Digit. Med. 8 (1), 45 (2025).\n 33. Philip Morris International. PMI Integrated Report 2023 [Internet]. 2023 [cited 2024 Oct 12]. Available from:  h t t p s :  / / w w w .  p m i . c o  \nm / r e s o  u r c e s  / d o c s /  d e f a u l  t - s o u r  c e / i r  2 0 2 3 - d  o c u m e n  t s / p m i  - i n t e g r a t e d - r e p o r t - 2 0 2 3 . p d f\n 34. Jiang, A. Q. et al. Mixtral of experts. ArXiv Preprint ArXiv:2401.04088. Jan 8. (2024).\n 35. Wang, J. et al. Evaluating Large Language Models on Academic Literature Understanding and Review: An Empirical Study among \nEarly-stage Scholars. InProceedings of the 2024 CHI Conference on Human Factors in Computing Systems. May 11 (pp. 1–18). \n(2024).\n 36. Liu, Y . et al. Are LLMs good at structured outputs? A benchmark for evaluating structured output capabilities in LLMs. Inf. Process. \nManag. 61 (5), 103809 (2024).\n 37. Njeh, C., Nakouri, H. & Jaafar, F . Enhancing rag-retrieval to improve llms robustness and resilience to hallucinations. InInternational \nConference on Hybrid Artificial Intelligence Systems 2024 Oct 9 (pp. 201–213). Cham: Springer Nature Switzerland.\n 38. Quan, S. J. Comparing hyperparameter tuning methods in machine learning based urban Building energy modeling: A study in \nChicago. Energy Build. 317, 114353 (2024).\n 39. Soudani, H., Kanoulas, E. & Hasibi, F . Fine tuning vs. retrieval augmented generation for less popular knowledge. InProceedings of \nthe 2024 Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific \nRegion 2024 Dec 8 (pp. 12–22).\n 40. Alawwad, H. A., Alhothali, A., Naseem, U., Alkhathlan, A. & Jamal, A. Enhancing textual textbook question answering with large \nLanguage models and retrieval augmented generation. Pattern Recogn. 162, 111332 (2025).\n 41. Hussien, M. M. et al. Rag-based explainable prediction of road users behaviors for automated driving using knowledge graphs and \nlarge Language models. Expert Syst. Appl. 265, 125914 (2025).\n 42. Jang, J., Li, W . S. & AU-RAG Agent-based Universal Retrieval Augmented Generation. InProceedings of the 2024 Annual \nInternational ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region 2024 \nDec 8 (pp. 2–11).\nScientific Reports |        (2025) 15:22760 14| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/\n 43. El-Mitwalli, S. tobacco-RAG-evaluation: a domain-specific evaluation framework for LLMs in tobacco research [Internet]. 2024 \n[cited 2025 May 2]. Available from:  h t t p s :   /  / g i t h u  b . c o  m / s h e r  i f e l m  i t w a l  l  i / t o b a  c  c o -   r a g - e v a l u a t i o n\nAuthor contributions\nS.E., J.M., A.G. and S.B. conceptualized the study and developed the methodology. S.E. and J.M. implemented \nthe evaluation framework and conducted the experiments. S.E. and J.M. contributed to the hyperparameter op-\ntimization process and statistical analyses. S.B., J.M. and A.G. curated the gold reference dataset and performed \nthe human validation assessments. A.G. supervised the study. S.E. wrote the main manuscript text, and all au -\nthors (S.E., J.M., S.B., and A.G.) reviewed, edited, and approved the final manuscript.\nFunding\nThis research was supported by Bloomberg Philanthropies as part of the Bloomberg Initiative to Reduce Tobacco \nUse (www.bloomberg.org). The funders had no role in study design, data collection and analysis, decision to \npublish, or preparation of the manuscript.\nDeclarations\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to S.E.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give \nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and \nindicate if changes were made. The images or other third party material in this article are included in the article’s \nCreative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy \nof this licence, visit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025 \nScientific Reports |        (2025) 15:22760 15| https://doi.org/10.1038/s41598-025-05726-2\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.696710467338562
    },
    {
      "name": "Terminology",
      "score": 0.6135352849960327
    },
    {
      "name": "Scalability",
      "score": 0.6099235415458679
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5766015648841858
    },
    {
      "name": "Documentation",
      "score": 0.5107309818267822
    },
    {
      "name": "Metric (unit)",
      "score": 0.4633610248565674
    },
    {
      "name": "CLARITY",
      "score": 0.4600726068019867
    },
    {
      "name": "Public domain",
      "score": 0.4588821828365326
    },
    {
      "name": "Completeness (order theory)",
      "score": 0.42768755555152893
    },
    {
      "name": "Information retrieval",
      "score": 0.41430583596229553
    },
    {
      "name": "Data science",
      "score": 0.39743196964263916
    },
    {
      "name": "Data mining",
      "score": 0.37165361642837524
    },
    {
      "name": "Machine learning",
      "score": 0.3597337603569031
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33358699083328247
    },
    {
      "name": "Database",
      "score": 0.11975133419036865
    },
    {
      "name": "Mathematics",
      "score": 0.08457261323928833
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Theology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I51601045",
      "name": "University of Bath",
      "country": "GB"
    }
  ],
  "cited_by": 1
}