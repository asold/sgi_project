{
  "title": "A Term Weighted Neural Language Model and Stacked Bidirectional LSTM Based Framework for Sarcasm Identification",
  "url": "https://openalex.org/W3118913631",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2098275340",
      "name": "Aytuğ Onan",
      "affiliations": [
        "Izmir Kâtip Çelebi University"
      ]
    },
    {
      "id": "https://openalex.org/A2222374179",
      "name": "Mansur Alp Tocoglu",
      "affiliations": [
        "Manisa Celal Bayar University"
      ]
    },
    {
      "id": "https://openalex.org/A2098275340",
      "name": "Aytuğ Onan",
      "affiliations": [
        "Izmir Kâtip Çelebi University"
      ]
    },
    {
      "id": "https://openalex.org/A2222374179",
      "name": "Mansur Alp Tocoglu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2989947381",
    "https://openalex.org/W2529281176",
    "https://openalex.org/W2979750067",
    "https://openalex.org/W2011432097",
    "https://openalex.org/W2024011160",
    "https://openalex.org/W2232443784",
    "https://openalex.org/W6683738474",
    "https://openalex.org/W2964301648",
    "https://openalex.org/W4312789226",
    "https://openalex.org/W2991113277",
    "https://openalex.org/W2964236337",
    "https://openalex.org/W2250480277",
    "https://openalex.org/W3010507974",
    "https://openalex.org/W2250489604",
    "https://openalex.org/W2038634595",
    "https://openalex.org/W2251379416",
    "https://openalex.org/W2007734915",
    "https://openalex.org/W2513138008",
    "https://openalex.org/W6726282261",
    "https://openalex.org/W2397798297",
    "https://openalex.org/W2547868665",
    "https://openalex.org/W3000333714",
    "https://openalex.org/W2512532697",
    "https://openalex.org/W2990662920",
    "https://openalex.org/W2942326195",
    "https://openalex.org/W2997460077",
    "https://openalex.org/W2971879956",
    "https://openalex.org/W6636510571",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W6758994088",
    "https://openalex.org/W2125109223",
    "https://openalex.org/W2248116437",
    "https://openalex.org/W2782127670",
    "https://openalex.org/W2981668055",
    "https://openalex.org/W2974335209",
    "https://openalex.org/W2923528470",
    "https://openalex.org/W2910215611",
    "https://openalex.org/W2119615570",
    "https://openalex.org/W2997026866",
    "https://openalex.org/W2565145181",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2131774270",
    "https://openalex.org/W6731175626",
    "https://openalex.org/W2512470422",
    "https://openalex.org/W4240187290",
    "https://openalex.org/W2952230511",
    "https://openalex.org/W2135731857",
    "https://openalex.org/W2952072734",
    "https://openalex.org/W2962809001",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2906461268",
    "https://openalex.org/W2913108690",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2565637284",
    "https://openalex.org/W2158899491"
  ],
  "abstract": "Sarcasm identification on text documents is one of the most challenging tasks in natural language processing (NLP), has become an essential research direction, due to its prevalence on social media data. The purpose of our research is to present an effective sarcasm identification framework on social media data by pursuing the paradigms of neural language models and deep neural networks. To represent text documents, we introduce inverse gravity moment based term weighted word embedding model with trigrams. In this way, critical words/terms have higher values by keeping the word-ordering information. In our model, we present a three-layer stacked bidirectional long short-term memory architecture to identify sarcastic text documents. For the evaluation task, the presented framework has been evaluated on three-sarcasm identification corpus. In the empirical analysis, three neural language models (i.e., word2vec, fastText and GloVe), two unsupervised term weighting functions (i.e., term-frequency, and TF-IDF) and eight supervised term weighting functions (i.e., odds ratio, relevance frequency, balanced distributional concentration, inverse question frequency-question frequency-inverse category frequency, short text weighting, inverse gravity moment, regularized entropy and inverse false negative-true positive-inverse category frequency) have been evaluated. For sarcasm identification task, the presented model yields promising results with a classification accuracy of 95.30%.",
  "full_text": "Received December 8, 2020, accepted January 4, 2021, date of publication January 6, 2021, date of current version January 13, 2021.\nDigital Object Identifier 10.1 109/ACCESS.2021.3049734\nA Term Weighted Neural Language Model and\nStacked Bidirectional LSTM Based Framework\nfor Sarcasm Identification\nAYTUG ONAN\n1 AND MANSUR ALP TOÇOĞLU\n2\n1Department of Computer Engineering, Faculty of Engineering and Architecture, İzmir Katip Çelebi University, 35620 İzmir, Turkey\n2Department of Software Engineering, Faculty of Technology, Manisa Celal Bayar University, 45140 Manisa, Turkey\nCorresponding author: Aytug Onan (aytug.onan@ikcu.edu.tr)\nABSTRACT Sarcasm identiﬁcation on text documents is one of the most challenging tasks in natural\nlanguage processing (NLP), has become an essential research direction, due to its prevalence on social\nmedia data. The purpose of our research is to present an effective sarcasm identiﬁcation framework on social\nmedia data by pursuing the paradigms of neural language models and deep neural networks. To represent text\ndocuments, we introduce inverse gravity moment based term weighted word embedding model with trigrams.\nIn this way, critical words/terms have higher values by keeping the word-ordering information. In our model,\nwe present a three-layer stacked bidirectional long short-term memory architecture to identify sarcastic\ntext documents. For the evaluation task, the presented framework has been evaluated on three-sarcasm\nidentiﬁcation corpus. In the empirical analysis, three neural language models (i.e., word2vec, fastText and\nGloVe), two unsupervised term weighting functions (i.e., term-frequency, and TF-IDF) and eight supervised\nterm weighting functions (i.e., odds ratio, relevance frequency, balanced distributional concentration, inverse\nquestion frequency-question frequency-inverse category frequency, short text weighting, inverse gravity\nmoment, regularized entropy and inverse false negative-true positive-inverse category frequency) have been\nevaluated. For sarcasm identiﬁcation task, the presented model yields promising results with a classiﬁcation\naccuracy of 95.30%.\nINDEX TERMS Sarcasm identiﬁcation, term weighting, neural language model, bidirectional long\nshortterm memory.\nI. INTRODUCTION\nSocial networks and microblogging websites have been\ncommonly used for exchanging observations, thoughts, feed-\nback and comments. Much of the information posted on\nthese sites is based on ﬁgurative devices, such as, metaphor,\noxymoron, hyperbole, puns, irony, satire and sarcasm [1].\nSarcasm is a type of nonliteral and ﬁgurative language, where\npeople may use words with a positive literal meaning to\nexpress their negative feelings, and, conversely, words with\nnegative meanings can be used to imply positive feelings.\nSarcasm can be described as a cutting, ironic remark designed\nto convey disdain or ridicule [2]. Sentiment analysis (also\nknown as, opinion mining) is one of the main tasks in natural\nlanguage processing, in which perceptions, observations,\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Wenbing Zhao\n.\nfeelings, opinions or judgments have been derived about\na speciﬁc subject. For individual decision makers, busi-\nness organizations and governments, sentiment recognition\ncan be essential [3]. It can be of great beneﬁt to govern-\nments, decision support systems and individuals to recognize\npublic opinions on policies, products and organizations [4].\nSarcastic expressions can shift the meaning orientation of text\ndocuments from positive to negative, or vice versa. For text\ndocuments with sarcasm, text utterances conveyed and the\nseverity of the individual using sarcasm can be absolutely the\nopposite. Consequently, the predictive efﬁciency of sentiment\nclassiﬁcation schemes can be diminished if sarcasm cannot\nbe properly handled. Sarcasm identiﬁcation on text docu-\nments is one of the most challenging tasks in natural language\nprocessing, has become an important research direction due\nto its prevalence on social networks and microblogging\nwebsites.\nVOLUME 9, 2021 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 7701\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nSarcasm identiﬁcation methods can be broadly divided\ninto three classes, as lexicon based approaches, rule based\napproaches and machine learning based approaches [5]. The\nlexicon-based approaches identify sarcasm in text docu-\nments with the use of a dictionary of words and sentences,\ncomprising unigrams, bigrams, trigrams, and phrases. The\nrule-based approaches identify sarcasm with use of syntactic,\nsemantic and stylistic properties of the sentence, such as,\nphrase patterns and lexical structures [6]. Machine learning\nbased approaches identify sarcasm with the construction of\nprediction models. The machine learning based approaches\ncan be further classiﬁed into three classes, as unsuper-\nvised learning, semi-supervised learning and hybrid learning\nschemes [5].\nThe identiﬁcation of an efﬁcient text representation\nscheme is an essential stage for NLP tasks. The conventional\ntext representation scheme utilized in text categorization\nis bag-of-words scheme, in which syntax, word rankings\nand grammar rules of text are not taken into consideration.\nThis scheme is unable to capture semantic relationships\nbetween the components of text documents. In addition,\nit gives a sparse representation of data with high-dimensional\nfeature space [7]. Recent research contributions indicate that\nprocessing text documents using neural language models\ncan signiﬁcantly boost the predictive performance in natural\nlanguage processing (NLP) tasks, including sentiment anal-\nysis, linguistic modelling and machine translation [8]. Neural\nlanguage models have been employed in recent years to learn\ntext representation, due to the efﬁciency of word embed-\nding schemes learned from large unstructured text data [9].\nWord embedding schemes make it possible to represent\ntext documents with less dimension in a more efﬁcient\nway, thereby removing the sparsity and high dimensionality\nproblems experienced in bag-of-words scheme [10]. The\nconventional word embedding schemes, such as word2vec,\nview text documents as bags of embedded words, and obtain\na sentence or document representation by simply adding or\naveraging every sequence term obtained via word embed-\nding. Term weighting schemes are one of the most common\npre-processing steps in text classiﬁcation. Term weighting\nschemes have been utilized to assign appropriate weight\nvalue for each term/word [11]. Based on the consideration\nof category information, term weighting schemes can be\ndivided into two classes, as unsupervised term weighting\nschemes and supervised term weighting schemes [12]. The\nunsupervised term weighting schemes do not consider cate-\ngory information to assign weight for terms, whereas the\nsupervised term weighting schemes utilize the category\ninformation of a term from the training data. Unsuper-\nvised term weighting schemes include term-frequency and\nTF-IDF, and supervised term weighting schemes include\nodds ratio, relevance frequency, balanced distributional\nconcentration, inverse question frequency-question frequency-\ninverse category frequency, short text weighting, inverse\ngravity moment, regularized entropy and inverse false\nnegative-true positive-inverse category frequency. Recent\nempirical analyzes in NLP indicate that term weighted\nneural language models can boost predictive performance\non supervised and unsupervised NLP tasks [13]. In addition,\nconventional word embedding schemes ignore word-order\ninformation, which can be crucial to capture sentiment orien-\ntation of text documents.\nFor text mining applications, deep neural networks, such\nas, convolutional neural networks, recurrent neural networks,\nlong short-term memory and gated recurrent units, have\nbeen successfully utilized. The conventional long short-term\nmemory architecture consist of a single hidden LSTM\nlayer, followed by a feedforward output layer. The stacked\nbi-directional long short-term memory is an extended archi-\ntecture to the conventional LSTM, where multiple hidden\nLSTM layers with multiple memory cells have been conﬁg-\nured. In this way, addition of layers can enhance the levels of\nabstraction of input data.\nIn our proposed scheme, a term weighted neural language\nmodel and deep neural network based framework has been\npresented for sarcasm identiﬁcation. To obtain an efﬁcient\ntext representation scheme, we introduce inverse gravity\nmoment based term weighted word embedding model with\ntrigram features. In this way, critical words and terms have\nhigher values, while keeping the word ordering information.\nThe term weighted neural language model has been integrated\nto a three-layer stacked bidirectional long-short term memory\narchitecture to identify sarcasm on text documents. For the\nevaluation task, the presented framework has been evalu-\nated on three corpus. In the empirical analysis, three neural\nlanguage models (i.e., word2vec, fastText and GloVe), two\nunsupervised term weighting functions (i.e., term-frequency,\nand TF-IDF) and eight supervised term weighting func-\ntions (i.e., odds ratio, relevance frequency, balanced distri-\nbutional concentration, inverse question frequency-question\nfrequency-inverse category frequency, short text weighting,\ninverse gravity moment, regularized entropy and inverse\nfalse negative-true positive-inverse category frequency) have\nbeen evaluated. The proposed scheme has been empiri-\ncally compared with ﬁve deep neural network architectures\n(i.e., convolutional neural network, recurrent neural network,\nlong short-term memory, gated recurrent unit, and bidirec-\ntional LSTM).\nTo summarize, many research contributions on NLP have\nbeen dedicated to neural language models, motivated by\nthe predictive performance of word embedding schemes [2],\n[3], [14], [15]. In order to obtain an efﬁcient word embed-\nding scheme, term weighting functions can be utilized in\nconjunction with neural language models so that reasonable\nweight values can be assigned to each term. In addition, word\nordering information can be critical to the performance of\nlanguage modeling. Though term weighting and word-order\ninformation are critical on NLP tasks, the number of works\ndedicated to the use of term weighted word embedding\nschemes is very limited. In addition, stacked bidirectional\nlong short term memory yields promising results on senti-\nment analysis and sarcasm identiﬁcation [16]. Motivated\n7702 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nby the use of term weighted neural language modeling in\ntext representation to enhance predictive performance and\npromising performance of stacked bidirectional long short\nterm memory, this paper seeks to introduce a sarcasm iden-\ntiﬁcation framework based on term weighting and stacked\nbidirectional LSTM.\nThe main contributions of our proposed scheme can be\nsummarized as follows:\n•A novel text representation scheme based on inverse\ngravity moment based term weighted word embedding model\nwith trigram has been presented. Word embedding schemes\nhave been frequently utilized for sentiment analysis and\nsarcasm identiﬁcation. To the best of our knowledge, this\nis the ﬁrst study on sarcasm identiﬁcation, which employs\nterm weighted word embedding and word order information\nto capture informative terms and obtain an efﬁcient scheme\nfor text representation.\n•This is the ﬁrst comprehensive analysis of unsupervised\nand supervised term weighted neural language models on\nsarcasm identiﬁcation.\n• Deep learning architectures, such as, convolutional\nneural networks, long short term memory, have been\nsuccessfully employed for sentiment analysis and sarcasm\nidentiﬁcation. In our model, we present a stacked three-layer\nbidirectional long short term memory architecture for\nsarcasm identiﬁcation. This is the ﬁrst comprehensive anal-\nysis on sarcasm identiﬁcation, in which different number of\nlayers in stacked bidirectional LSTM have been analyzed.\nThe rest of this paper is structured as follows. Section II\npresents the literature review sarcasm identiﬁcation,\nSection III presents the research objectives, Section IV brieﬂy\ndiscusses the theoretical foundations of the study. Section V\nintroduces the proposed sarcasm identiﬁcation framework.\nSection VI presents experimental procedure, results and\ndiscussions. Finally, Section VII presents the concluding\nremarks of the study.\nII. RELATED WORK\nThere are several studies in the area of natural language\nprocessing, using conventional machine learning approaches,\nneural language models, and deep learning architectures. This\nsection brieﬂy addresses earlier research contributions on\nsarcasm identiﬁcation.\nA. RELATED WORK ON CONVENTIONAL MACHINE\nLEARNING APPROACHES\nThe study by González-Ibánez et al.[17] is one of the main\nstudies in the ﬁeld of NLP dedicated to use of machine\nlearning techniques on sarcasm identiﬁcation. In this study,\npredictive performances of several linguistic feature sets\n(i.e., unigrams, presence of dictionary-based lexical and\npragmatic factors, and frequency of dictionary-based lexical\nand pragmatic factors) have been evaluated on a sarcasm\ncorpus obtained from Twitter in conjunction with two conven-\ntional supervised learners (namely, support vector machines\nand logistic regression). In another study conducted by\nReyes et al. [18], feature sets based on structural ambi-\nguity, morph syntactic ambiguity, semantic ambiguity,\npolarity, unexpectedness, and emotional scenarios have\nbeen evaluated to process ﬁgurative language. Similarly,\nBarbieri et al. [19] considered frequency-based features,\nwritten and spoken style uses, intensity of adjectives\nand adverbs, punctuation marks, emoticons for the task\nof sarcasm identiﬁcation. In another study conducted\nby Kunneman et al. [20], the relatively good reliability\nof the user-generated hashtags on Twitter has been\nevaluated as golden labels for sarcasm identiﬁcation.\nRajadesingan et al. [5] proposed a behavioral modeling\nscheme for detection of sarcasms. Bouazizi and Ohtsuki [21]\nintroduced a pattern-based approach for sarcasm identiﬁ-\ncation on Twitter. In this scheme, four main feature sets\n(i.e., sentiment-related features, punctuation-related features,\nsyntactic and semantic features, and pattern-based features)\nhave been evaluated in conjunction with several conven-\ntional machine-learning algorithms, such as, random forests,\nsupport vector machines, k-nearest neighbor algorithm and\nmaximum entropy. The empirical analysis revealed that the\npattern-based feature sets can yield promising results for the\ntask of sarcasm identiﬁcation, with a classiﬁcation accuracy\nof 83.1%. In another study, Mishra et al. [22] evaluated\nlexical, implicit incongruity based features, explicit incon-\ngruity based features, textual features, simple gaze based\nfeatures and complex gaze based features for sarcasm iden-\ntiﬁcation. In addition to structural features, Sulis et al.[23]\nhave taken into account cognitive and lexical features\nfor sarcasm identiﬁcation. Another study conducted by\nMukherjee and Bala [24] employed Naïve Bayes and fuzzy\nc-means clustering algorithms for sarcasm identiﬁcation\non microblogging platforms. Recently, Sundararajan and\nPalanisamy [25] presented an effective sarcasm identiﬁcation\nframework based on multi-rule ensemble feature selection\nscheme.\nB. RELATED WORK ON DEEP LEARNING BASED\nAPPROACHES\nDeep neural networks have been successfully employed in\nNLP tasks, including sarcasm identiﬁcation. For instance,\nGhosh and Veale [26] introduced a deep learning based\narchitecture for sarcasm identiﬁcation based on convolutional\nneural network, long short-term memory and deep neural\nnetwork. The empirical analysis on Twitter messages indi-\ncated that the presented architecture without dropout outper-\nforms conventional classiﬁers (such as, recursive support\nvector machines) and deep learning architectures (such\nas, long short-term memory) with an F-measure of 0.92.\nNaz et al.[27] employed convolutional neural network based\narchitecture for sarcasm identiﬁcation task. In this scheme,\nconvolution layer has processed input matrix containing\nTwitter messages. Then, maximum-pooling layer, drop-out\nlayer, and ﬁnally, fully connected layer has been employed to\nclassify text documents as either sarcastic or non-sarcastic.\nIn another study, Kumar et al. [16] presented a sarcasm\nVOLUME 9, 2021 7703\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nidentiﬁcation framework based on bidirectional long\nshort-term memory with convolution neural network. In the\npresented architecture, GloVe (global vectors for word repre-\nsentation) has been utilized to construct semantic word\nembeddings. The feature maps generated by semantic atten-\ntion based bidirectional long short-term memory architec-\nture and auxiliary feature sets based on punctuation have\nbeen merged into the convolutional neural network. The\nempirical analysis on two balanced/unbalanced corpus from\nTwitter revealed that the presented scheme can outperform\nthe conventional deep neural networks, with a classiﬁcation\naccuracy of 97.87%. In another study, Onan [3] examined\nthe predictive performance of word embedding schemes on\nsarcasm identiﬁcation of Twitter messages. In this regard,\nfour neural language models (i.e., LDA2vec, word2vec,\nfastText and GloVe) have been taken into consideration.\nAside from word embedding based feature sets, tradi-\ntional lexical, pragmatic, implicit incongruity and explicit\nincongruity based feature sets have been evaluated. The\npredictive performance of feature sets and word embedding\nschemes have been evaluated in conjunction with convolu-\ntional neural network. Ren et al.[28] introduced a weighted\nword embedding scheme, which combines balanced distri-\nbutional concentration term weighting and N-gram model\nfor text sentiment classiﬁcation. Another study conducted\nby Mehndiratta and Soni [29] presented a comprehensive\nanalysis of neural language models and deep neural networks\non sarcasm identiﬁcation. In this study, word2vec, GloVe\nand fastText have been utilized to represent text documents.\nThe predictive performance of word embedding schemes\nhave been evaluated in conjunction with convolutional neural\nnetworks, long short-term memory and hybrid architecture\nbased on bidirectional long short-term memory and convolu-\ntional neural network. The empirical results indicated that\nthe presented hybrid scheme could outperform the other\nconventional architectures. In a similar way, Mehndiratta\nand Soni [30] examined the effects of hyperparameters on\ndeep learning based sarcasm identiﬁcation. Ren et al. [28]\npresented a multi-level memory network for capturing the\nfeatures of sarcasm expressions using sentiment semantics.\nRecently, Jain et al. [16] presented a deep learning based\nframework for sarcasm identiﬁcation in English and Hindi\nlanguage. To extract semantic feature vectors, pre-trained\nGloVe model has been utilized. To process text documents,\nbidirectional long short term memory architecture with\nsoft-attention mechanism has been introduced.\nTo sum up, existing machine learning based sarcasm\nidentiﬁcation models generally concentrate on seeking an\neffective feature set to obtain an efﬁcient recognition\nmodel [17], [18], [23]. In addition, existing deep learning\nbased models for sarcasm identiﬁcation mainly concentrate\non the conﬁguration of an efﬁcient deep neural network\narchitecture [15], [16], [28]. Existing deep learning based\nschemes have not fully considered term weighting and word\nordering in neural language based modelling, even though\nthey can be crucial to improve the predictive performance of\nsarcasm identiﬁcation models. To deal with the problem, this\npaper presents an improved term weighted neural language\nmodel.\nIII. RESEARCH OBJECTIVES\nThe aim of this study is to comprehensively evaluate the\npredictive performance of term weighted neural language\nmodels and deep learning architectures on sarcasm identi-\nﬁcation and to develop an efﬁcient sarcasm identiﬁcation\nframework. The main research objectives of the study are as\nfollows:\n1. Investigate the performance of term weighting schemes\non neural language models for sarcasm identiﬁcation:\nIn order to do so, two unsupervised term weighting functions\n(i.e., term-frequency, and TF-IDF) and eight supervised term\nweighting functions (i.e., odds ratio, relevance frequency,\nbalanced distributional concentration, inverse question\nfrequency-question frequency-inverse category frequency,\nshort text weighting, inverse gravity moment, regularized\nentropy and inverse false negative-true positive-inverse cate-\ngory frequency) have been evaluated in conjunction with\nthree word embedding schemes (i.e., word2vec, fastText and\nGloVe).\n2. Examine the predictive performance of different aggre-\ngation functions for word embedding schemes. For the objec-\ntive, we have analyzed the performance of average pooling\nand maximum pooling aggregation functions.\n3. Determine the effect of word ordering information on\nneural language models: We have analyzed three N-gram\nmodels (i.e., unigram, bigram and trigram) in conjunction\nwith term weighed word embedding schemes.\n4. Evaluate the predictive performance of deep neural\nnetworks on sarcasm identiﬁcation. In this regard, conven-\ntional deep neural networks (such as, convolutional neural\nnetwork, recurrent neural network, long short-term memory,\ngated recurrent unit, and bidirectional LSTM) have been\nevaluated. In addition, the predictive performance of layers\n(2-layered, 3-layered and 4-layered) on stacked bidirectional\nlong short-term memory has been evaluated for sarcasm iden-\ntiﬁcation. A stacked LSTM architecture has been proposed.\n5. Design an efﬁcient sarcasm identiﬁcation framework,\nwhich integrates term weighted word embedding, N-gram\nword-order information and stacked three-layer bidirectional\nlong short-term memory architecture.\nIV. THEORETICAL FOUNDATIONS\nThis section brieﬂy presents the theoretical foundations of the\nstudy. Namely, neural language models utilized to represent\ntext documents, term weighting functions utilized in word\nembedding schemes and conventional deep neural network\narchitectures have been presented.\nA. NEURAL LANGUAGE MODELS\nNeural language models provide robust representation of text\ndocuments with semantic properties by fewer manual prepro-\ncessing. Dense representation of the vectors gives promising\n7704 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nresults in the processing of natural language tasks. In this\nstudy, we have considered three well-known word embedding\nschemes (i.e., word2vec, fastText, and GloVe). The rest of this\nsection brieﬂy presents the neural language models employed\nin the empirical analysis.\n1) Word2vec\nThe word2vec model is an artiﬁcial neural network based\nword embedding scheme, which consists of the input layer,\nthe output layer and the hidden layer [31].It aims to learn\nthe embedding of words by determining the probability\nthat a particular word will be surrounded by other words.\nThe model has two basic architectures, skip-gram (SG) and\ncontinuous-bag-of words (CBOW). The CBOW architecture\ndeﬁnes the target word by taking the content of each word\nas input; SG architecture, on the other hand, predicts the\nwords surrounding the target word by taking the target word\nas input. The CBOW architecture can work properly with a\nsmall amount of data.\n2) FastText\nAnother efﬁcient way to get word embeddings from text\ndocuments is the fastText model. Every word is deﬁned in that\nmodel by dividing the character into n-grams. Word vectors\nare constructed for each N-gram in the training set. The\nfastText scheme provides a more efﬁcient word embedding\nrepresentation for morphologically rich languages and rare\nwords [32].\n3) GloVe\nThe global vectors (GloVe) is a word2vec-based represen-\ntation scheme to effectively learn word embeddings from\ntext documents. The model incorporates local context-based\nword2vec model learning with a factorization of the global\nmatrix [33]. In the model, probability ratios of words are\nalso taken into consideration to calculate the error function.\nWords, which are closely observed in the text document and\nare likely to be used together in the learning cycle are more\nrelevant than other terms.\nB. TERM WEIGHTING FUNCTIONS\nIn the conventional neural language models, such as,\nword2vec, the equivalent weights were allocated to each word\nin the sentence and the word embedding was acquired by\nthe mean of word embeddings. Recent empirical analyzes\nin NLP indicate that term weighted neural language models\ncan boost predictive performance on supervised and unsuper-\nvised NLP tasks [34]. In this paper, we have considered two\nunsupervised and eight supervised term weighting schemes.\nThe remained of the section brieﬂy outlies term weighting\nfunctions employed in the empirical analysis.\nTerm weighting schemes can be mainly divided into\ntwo categories as unsupervised term weighting schemes\nand supervised term weighting schemes [12]. The unsuper-\nvised term weighting schemes do not use category infor-\nmation to allocate weight to terms, whereas the supervised\nterm weighting schemes use category information from the\ntraining data for a term.\nLet N denote a total number of documents in the corpus,\ntf denote the term frequency indicating the number of times\na particular term has been encountered in the document and\ndf denote the number of documents in which particular term\nhas been encountered at least once. Term frequency (tf)\nis an unsupervised term weighting scheme to compute\nweight value wdi,tj for term t j in document d i, as given by\nEquation 1 [11]:\nwdi,tj =tf (1)\nTerm frequency-inverse document frequency (TF-IDF) is\nanother unsupervised term weighting scheme on information\nretrieval and text mining. Term frequency represents the rela-\ntive frequency of a word t in a text document and inverse\ndocument frequency scales with the number of documents.\nTF-IDF weighting scheme can be computed as given by\nEquation 2:\nwdi,tj =tf ∗log\n(N\ndf\n)\n(2)\nOdds ratio (OR) is a supervised term weighting scheme,\nwhich can be computed as given by Equation 3 [35]:\nOR =log\n(tp ∗tn\nfp ∗fn\n)\n(3)\nwhere tp denotes true positives, tn denotes true negatives,\nfp denotes false positives and fn denotes false negatives.\nRelevance frequency (RF) is another supervised term\nweighting scheme. In this scheme, the ratio of number of\npositive category documents consisting of the word to the\nnumber of negative category documents containing the word\nhas been considered to compute weight values, as given by\nEquation 4 [35]:\nRF =log\n(\n2 + tp\nMax(1,fn)\n)\n(4)\nBalanced distributional concentration (bdc) is another super-\nvised term weighting scheme based on entropy (Wang et al.,\n2015). Balanced distributional concentration test term t’s\ndiscriminating power based on its distribution in different\ncategories (c i). Balanced distributional concentration can be\ncomputed, as given by Equation 5 [36]:\nbdc =1 − BHt\nlog(K) (5)\nBHt =−\nK∑\ni=1\np(t/ci)∑K\ni=1 p(t/ci) log\n(\np(t/ci)\n∑K\ni=1 p(t/ci)\n)\n(6)\nwhere K denotes total number of categories in the training\ndata and p(t/ci) denotes the probability of term t in\ncategory ci.\nInverse question frequency-question frequency-inverse\ncategory frequency (IQF-QF-ICF) is another supervised term\nVOLUME 9, 2021 7705\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nweighting scheme for short text classiﬁcation, which can be\ncomputed as given by Equation 7 [34]:\niqf −qf −icf =log\n( N\ntp +fn\n)\n∗log (tp +1)∗log\n(K\ncf +1\n)\n(7)\nwhere cf denotes the number of categories that have at least\none document in which t has been encountered.\nShort text weighting (SW) is another supervised term\nweighting scheme for short text classiﬁcation [37]. Short text\nweighting scheme can be computed as given by Equation 8:\nW\n(\ntij\n)\n= tfij +1\n∑|T |\nj=1 tfij +|T |\n∗log\n(\n1 + tp\nfp +fn +1\n)\n(8)\nwhere j denotes term present in document i, which contains\n|T |terms and tf ij denotes the frequency of it.\nInverse gravity moment (IGM) is another supervised\nterm weighting scheme based on class-speciﬁc gravity [35].\nInverse gravity moment based weight value for a term t i has\nbeen computed as given by Equations 9 and 10:\nW\n(\ntij\n)\n=1 +λ∗IGM(ti) (9)\nIGM (ti)= fi1\n∑K\nr=1 fir ∗r\n(10)\nwhere λis the parameter which has been set according to [12]\nand fir denotes term’s frequency in category r.\nRegularized entropy (RE) is another supervised term\nweighting scheme, which seeks to ﬁnd a balanced weighting\nscheme for terms by measuring term distribution. Regular-\nized entropy can be computed as given by Equation 11 [11]:\nRE =b +(1 −b)∗(1 −h), where b∈[0,1] (11)\nh =−p +∗log\n(\np+)\n−p−∗log\n(\np−)\n(12)\np+=− tp/(tp +fp)\ntp\ntp+fp + tp\nfn+tn\n(13)\np−=− fn/(fn +tn)\ntp\ntp+fp + tp\nfn+tn\n(14)\nInverse false negative-true positive-inverse category\nfrequency (IFN-TP-ICF) is another recently proposed\nsupervised term weighting scheme based on inverse\nquestion frequency-question frequency-inverse category\nfrequency [11]. IFN-TP-ICF can be computed as given by\nEquation 15:\nifn −tp −icf =log\n(N−+1\nfn +1\n)\n∗log (tp +1)∗log\n(K\ncf +1\n)\n(15)\nwhere cf denotes the number of categories that have at least\none document in which t has been encountered and N−\ndenotes the number of documents in the negative category.\nC. DEEP LEARNING ARCHITECTURES\nIn the ﬁeld of machine learning, deep learning is one of\nthe branches, in which data will be automatically processed\nin multiple processing layers through a linear or non-linear\ntransformation. Conventional machine learning methods\nproduce features usually through a signiﬁcant amount of\nmanual intervention, which takes a great deal of study. Deep\nlearning has the potential to automatically extract features and\ncan effectively reduce time with high predictive performance.\nDeep learning architectures have achieved remarkably good\npredictive performance on tasks, including drug event detec-\ntion [38], document classiﬁcation [39], sentiment classiﬁca-\ntion [27], [40], [41], and irony identiﬁcation [13]. The rest of\nthis section brieﬂy presents the deep learning architectures\nemployed in the empirical analysis.\n1) CONVOLUTIONAL NEURAL NETWORKS\nConvolutional neural networks (CNN) are deep neural\nnetwork based architectures which process data using a\ngrid-based topology, whereby a special form of mathemat-\nical process referred as convolution has been employed.\nTypical CNN architecture consists of input layer, output layer\nand hidden layers. The hidden layers include convolutional\nlayers, pooling layers and fully connected layers. In convolu-\ntion layers, feature maps were obtained through convolution\noperation from the input data. The activation functions were\nused along with feature maps to add nonlinearity to the archi-\ntecture. The outputs of neuron clusters have been integrated\nvia pooling layers. This reduced the spatial size of function\nspaces, and improved models’ ability to tolerate overﬁtting.\nMaximum pooling has been employed in pooling layer. The\nfully connected layers are used to get the ﬁnal architectural\noutput [37].\n2) RECURRENT NEURAL NETWORKS\nThe recurrent neural network (RNN) is a type of deep neural\nnetworks used for sequential data processing [42]. In RNN,\na directed graph is generated by the connections between\nneurons. RNN can use its internal state to process input\nsequences, which makes it suitable for NLP tasks [13].\nIn RNN, each output is calculated by executing the same\nfunction over each instance of the sequence repeatedly. In this\nway, the performance was calculated based on all the previous\ncalculations. The length of the time steps in RNN architecture\nwas calculated based on the length of the input.\n3) LONG SHORT-TERM MEMORY NETWORK\nLong short-term memory network (LSTM) is a RNN-based\ndeep neural network architecture, in which forget gates\nhave been employed to eliminate exploding or vanishing\ngradient problem. Unlike traditional recurrent neural network\narchitectures, LSTM allows error back propagation through\nthe limited amount of time steps [43]. Typical LSTM unit\ncontains a cell and three types of gates, i.e. an input gate,\nan output gate, and a forget gate. The cell decides which\n7706 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 1. The general structure ofproposed sarcasm identification framework.\ninformation should be stored and when the units should\naccess the information determined based on the open and\nclose operations at the gates [44].\n4) GATED RECURRENT UNIT\nGated recurrent unit (GRU) is another RNN-based deep\nneural network architecture [45]. As mentioned in advance,\nLSTM architecture efﬁciently solves the vanishing gradient\nproblem of RNN and the long-torm memory weight can be\nwell retained. However, LSTM has a complex architecture\nwhich involves high number of calculations. Typical GRU\narchitecture replaces the forget and input gate of LSTM archi-\ntecture with an update gate. GRU is a simpliﬁed LSTM based\narchitecture with reduced tensor calculations [45].\n5) BIDIRECTIONAL LONG SHORT-TERM MEMORY\nIn sequence classiﬁcation tasks, a summary vector was\nconstructed by reading a series of inputs from left to right\nby considering only the left context. For NLP activities,\nboth the left and the right context of a word/term can be\ncritical. In this regard, bidirectional recurrent neural network\nbased architectures have been introduced [46]. The principle\nof bidirectional LSTM is to read the training data in two\ntime directions and train the neural network. In bidirectional\nLSTM, the prediction has been done by concatenating the\nleft and right summary vectors. Since bidirectional LSTM\ntakes both the left and right context information, it can yield\nhigher predictive performance compared to unidirectional\ndeep neural architectures [43].\nV. PROPOSED SARCASM IDENTIFICATION FRAMEWORK\nThe proposed sarcasm identiﬁcation framework consists of\ntwo main phases: term-weighted word embedding scheme\nand stacked three layer bidirectional long short memory based\nprocessing of text documents. The general structure of the\nproposed sarcasm identiﬁcation framework is summarized\nin Figure 1.\nA. TERM WEIGHTED TRI-GRAM REPRESENTATION\nMODEL\nNeural language models are a kind of unsupervised text\nrepresentation, which provide dense representation of text\ndocuments based on a large amount of corpus. Neural\nlanguage models provide dense vector representations with\nless manual processing. The text representation obtained in\ntraditional neural language models, such as word2vec, fast-\nText, and GloVe, cannot express speciﬁcally the important\ndegree of each word in the sequence. Term weighing schemes\ncan assign appropriate weight values for each term/word\nof a text sequence. To obtain an efﬁcient text representa-\ntion scheme which integrates neural language models, term\nweighting schemes and N-gram models, we have made\ncomprehensive empirical analysis with three neural language\nmodels (i.e., word2vec, fastText and GloVe), ten aforemen-\ntioned term weighting schemes, and three N-gram models\n(i.e., unigram, bigram and trigram). To concatenate the\nfeature vectors, two aggregation functions (i.e., averaging\nand maximum pooling) have been taken into consideration.\nIn this way, 180 different text representation conﬁgurations\nhave been obtained. The detailed empirical results with these\nconﬁgurations have been presented in Section VI. Since\nthe highest predictive performance of text representation for\nsarcasm has been identiﬁed based on term weighted neural\nlanguage model with trigram model, the rest of this section\nbrieﬂy presents this scheme. In Figure 2, the general struc-\nture of the term weighted tri-gram representation has been\nillustrated.\nVOLUME 9, 2021 7707\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 2. The general structure of the term weighted tri-gram representation model.\nIn term weighted tri-gram text representation scheme,\na weighted text representation has been obtained for each\nn-gram (namely, for n =1, n =2, and n =3). Based on text\nrepresentation of individual n-gram models, the representa-\ntion scheme for sentence or document has been obtained by\nconcatenating the weighted representations. In this scheme,\nthe weight value (w ′\ni) of each token (x i) has been initialized\nbased on Inverse gravity moment (IGM), as given by Equa-\ntion 16 and the normalized weight value (w i) can be computed\nas given by Equation 17:\nw′\ni =IGM(xi) (16)\nwi = ew′\ni\n∑L\ni=1 ew′\ni\n(17)\nwhere L denotes the number of tokens in a text sequence.\nAs illustrated in Figure 2, term weighted tri-gram text\nrepresentation scheme keeps word ordering information. For\nterm weighted tri-gram model by averaging scheme (v average),\nthe representation can be computed as given by Equations 18,\n19 and 20:\nvaverage =concat(vweightedaverage ,vtrigramaverage ) (18)\nvweightedaverage =w1.v1 +w2.v2 +···+ wL .vL (19)\nFIGURE 3. Long-short term memory (LSTM) unit.\nvtrigramaverage =wtrigram,1\n∑n\ni=1 wi.vi\n+wtrigram,2\n∑n+1\ni=2 wi.vi\n+wtrigram,3\n∑n+2\ni=3 wi.vi (20)\nFor term weighted tri-gram model by the maximum pooling\n(vmaxpooling), the representation can be computed as given by\nEquations 21, 22, 23 and 24:\nvmaxpooling =concat(vweightedmaximum ,vtrigrammaximum ) (21)\nvtrigrammaximum =maxpooling(w1.v1,w2.v2,..., wL .vL ) (22)\nvtrigrammaximum =wtrigram,1.vmax (1,n)\n+wtrigram,2.vmax (2,n +1)\n+wtrigram,3.vmax (3,n +2) (23)\nvmax (i,j)=maxpooling(wi.vi,wi+1.vi+1,..., wj.vj)\n(24)\n7708 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 4. The general structure of the three-layer stacked bidirectional LSTM.\nB. THREE-LAYER STACKED BIDIRECTIONAL LSTM\nIn this section, three-layer stacked bidirectional long\nshort-term memory (LSTM) model for sarcasm iden-\ntiﬁcation has been presented. Figure 4 illustrates the\ngeneral structure of the three-layer stacked bidirectional\nLSTM architecture. In this scheme, we initially perform\nweighted word embedding scheme based on inverse gravity\nmoment based term weighted word embedding model with\nn-grams to represent text documents. The obtained weighted\nword-embedding scheme has been taken as an input to the\nstacked bi-directional LSTM architecture. In this architec-\nture, contextual information to represent text documents have\nbeen further extracted and the text documents have been\nprocessed. Then, a softmax layer to identify the class label,\nas either sarcastic or non-sarcastic, has concatenated the\noutputs of ﬁnal hidden layer.\nBased on the empirical analysis with various number of\nlayers, we have introduced a three-layer stacked bidirectional\nLSTM architecture to process text documents. As mentioned\nin advance, bidirectional architectures can outperform the\nunidirectional architectures [42]. In addition, stacked archi-\ntectures can yield higher predictive performance owing to\nobtaining richer contextual information about both past and\nfuture data sequences. The higher number of upper layers\ndeployed in the stacked LSTM architectures can provide a\ndetailed contextual information regarding the data, hereby\nproviding an enhanced prediction model.\nThe general structure for the long-short term memory unit\nhas been illustrated in Figure 3. Each node of the hidden\nlayers presented in Figure 4 implements the main function-\nality of long-short term memory unit depicted in Figure 3.\nAs illustrated in Figure 4, for a particular time sequence T,\nthe input data has been given to the hidden layers in the\nforward direction to obtain information regarding the earlier\nstages. In addition, the input sequence has been given to the\nhidden layers in the reverse direction to obtain information\nregarding the future stages. To identify detailed contextual\ninformation, units of hidden layers in the higher hierarchies\nof the model take outputs from the lower layers of the\nconﬁguration.\nThe hidden state information (a t ) for the ﬁrst forward layer\nhas been identiﬁed as given by Equation 25:\nit (a) =σ(Wi(a)at−1 +Ui(a)xt +bi(a)) (25)\nft (a) =σ(Wf (a)at−1 +Uf (a)xt +bf (a)) (26)\not (a) =σ(Wo(a)at−1 +Uo(a)xt +bo(a)) (27)\nVOLUME 9, 2021 7709\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nTABLE 1. Classification accuracies obtained by conventional representation schemes and the proposed schemes on three corpus.\nTABLE 2. Classification accuracies obtained by word2vec unigram model on ‘‘Sarcasm Corpus 1’’ .\nut (a) =tanh(Wu(a)at−1 +Uu(a)xt +bu(a)) (28)\nCt (a) =it (a)◦ut (a) +ft (a)◦Ct−1(a) (29)\nat =ot (a)◦tanh(Ct (a)) (30)\nThe hidden state information (b t ) for the second forward layer\nhas been identiﬁed as given by Equation 31:\nit (b) =σ(Wi(b)bt−1 +Ui(b)at +bi(b)) (31)\nft (b) =σ(Wf (b)bt−1 +Uf (b)at +bf (b)) (32)\not (b) =σ(Wo(b)bt−1 +Uo(b)at +bo(b)) (33)\nut (b) =tanh(Wu(b)bt−1 +Uu(b)at +bu(b)) (34)\nCt (b) =it (b)◦ut (b) +ft (b)◦Ct−1(b) (35)\nbt =ot (b)◦tanh(Ct (b)) (36)\nThe hidden state information (c t ) for the third forward layer\nhas been identiﬁed as given by Equation 37:\nit (c) =σ(Wi(c)ct−1 +Ui(c)bt +bi(c)) (37)\nft (c) =σ(Wf (c)ct−1 +Uf (c)bt +bf (c)) (38)\not (c) =σ(Wo(c)ct−1 +Uo(c)bt +bo(c)) (39)\nut (c) =tanh(Wu(c)ct−1 +Uu(c)bt +bu(c)) (40)\nCt (c) =it (c)◦ut (c) +ft (c)◦Ct−1(c) (41)\nct =ot (c)◦tanh(Ct (c)) (42)\n7710 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nTABLE 3. Classification accuracies obtained by word2vec bigram model on ‘‘Sarcasm Corpus 1’’ .\nTABLE 4. Classification accuracies obtained by word2vec trigram model on ‘‘Sarcasm Corpus 1’’ .\nThe hidden state information (d t ) for the ﬁrst backward layer\nhas been identiﬁed as given by Equation 43:\nit (d) =σ(Wi(d)dt+1 +Ui(d)xt +bi(d)) (43)\nft (d) =σ(Wf (d)dt+1 +Uf (d)xt +bf (d)) (44)\not (d) =σ(Wo(d)dt+1 +Uo(d)xt +bo(d)) (45)\nut (d) =tanh(Wu(d)dt+1 +Uu(d)xt +bu(d)) (46)\nCt (d) =it (d)◦ut (d) +ft (d)◦Ct−1(d) (47)\ndt =ot (d)◦tanh(Ct (d)) (48)\nThe hidden state information (e t ) for the second backward\nlayer has been identiﬁed as given by Equation 49:\nit (e) =σ(Wi(e)et+1 +Ui(e)dt +bi(e)) (49)\nft (e) =σ(Wf (e)et+1 +Uf (e)dt +bf (e)) (50)\not (e) =σ(Wo(e)et+1 +Uo(e)dt +bo(e)) (51)\nut (e) =tanh(Wu(e)et+1 +Uu(e)dt +bu(e)) (52)\nCt (e) =it (e)◦ut (e) +ft (e)◦Ct−1(e) (53)\net =ot (e)◦tanh(Ct (e)) (54)\nThe hidden state information (g t ) for the third backward layer\nhas been identiﬁed as given by Equation 55:\nit (g) =σ(Wi(g)gt+1 +Ui(g)et +bi(g)) (55)\nft (g) =σ(Wf (g)gt+1 +Uf (g)et +bf (g)) (56)\not (g) =σ(Wo(g)gt+1 +Uo(g)et +bo(g)) (57)\nVOLUME 9, 2021 7711\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nTABLE 5. Classification accuracies obtained by GloVe unigram model on ‘‘Sarcasm Corpus 1’’ .\nTABLE 6. Classification accuracies obtained by GloVe bigram model on ‘‘Sarcasm Corpus 1’’ .\nut (g) =tanh(Wu(g)gt+1 +Uu(g)et +bu(g)) (58)\nCt (g) =it (g)◦ut (g) +ft (g)◦Ct−1(g) (59)\ngt =ot (g)◦tanh(Ct (g)) (60)\nFor a particular time step t, the output has been obtained\nby concatenating hidden state information (c t ) for the third\nforward layer and the hidden state information (g t ) for the\nthird backward layer, as illustrated in Equation 61:\not =U(O)ct +W (O)gt +b(O) (61)\nAfter obtaining the output for the ﬁnal time step, softmax\nlayer has been employed to obtain the ﬁnal class label for the\nframework, as given by Equation 63, where y′corresponds to\nthe predicted class label for a particular instance:\np(y (X) =softmax(W (s)OK +b(s)) (62)\ny′=argmaxyp(y (X) (63)\nVI. EXPERIMENTAL ANALYSIS\nTo examine the predictive performance of the proposed\ndeep learning based framework on sarcasm identiﬁcation,\nan extensive empirical analysis has been performed. This\nsection presents three corpus utilized in the empirical anal-\nysis, experimental procedure and the experimental results.\nA. CORPUS\nThe empirical analysis has been conducted on three-sarcasm\nidentiﬁcation corpus. The ﬁrst corpus (referred as, ‘‘Sarcasm\n7712 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nTABLE 7. Classification accuracies obtained by GloVe trigram model on ‘‘Sarcasm Corpus 1’’ .\nTABLE 8. Classification accuracies obtained by fastText unigram model on ‘‘Sarcasm Corpus 1’’ .\nCorpus 1’’) is our own collected corpus of Twitter messages.\nIn the dataset collection, the methodology outlined in [17]\nhas been adopted. Self-annotated Tweets of Twitter users\nwere used to create the sarcasm dataset with sarcastic\nand non-sarcastic tweets. Twitter posts with ‘‘sarcasm’’ or\n‘‘ironic’’ hashtags are taken as sarcastic tweets, whereas\nhashtags on positive and negative emotions are consid-\nered non-sarcastic tweets. We received about 40,000 tweets\nwritten in English in this way. Twitter4J, an open-source\nJava library for the use of the Twitter Streaming API, was\nused to gather data collection. Automatic ﬁltering was used\nto delete duplicated messages, retweets, vague, meaningless\nand redundant posts [3]. Each tweet has been annotated\nmanually using a single-class label, either as sarcastic or as\nnon-sarcastic. We received about 15,000 sarcastic messages,\nand about 24,000 non-sarcastic messages. Our ﬁnal corpus\nincludes Twitter messages of 15,000 sarcastic tweets and\n15,000 non-sarcastic tweets, in order to provide a balanced\ncorpus. We have followed the structure provided in [1] to\npre-process our corpus. First, on the corpus, tokenization\nwas used to separate tweets into tokens, such as words and\npunctuation marks. Twokenize tool was used to manage the\ntokenization process. Unnecessary items produced by Twok-\nenize were removed at the end of the tokenization process.\nThe second corpus utilized in the empirical analysis\nis ‘‘Sarcasm version 2’’ dataset. It is a publicly acces-\nsible dataset. It is a balanced subset of Internet Argument\nCorpus (IAC) [47]. The dataset’s content is text from some\nVOLUME 9, 2021 7713\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nTABLE 9. Classification accuracies obtained by fastText bigram model on ‘‘Sarcasm Corpus 1’’ .\nTABLE 10. Classification accuracies obtained by fastText trigram model on ‘‘Sarcasm Corpus 1’’ .\nquote-response pairs annotated for the inclusion of sarcasm\nin them. The three key types of quotes in the dataset are\nGeneric (Gen) which are common quote-response sentences\nwith sarcasm, Rhetorical Questions (RQ) are the questions\nthat imply a sarcastic statement, and Hyperbole (Hyp) are\nthe questions that represent real-life exaggeration. The corpus\nhas 4,712 instances, in which 3,260 are of the Generic\nkind, while 582 and 850 are of the form of Hyperbole and\nRhetorical Questions, respectively.\nThe third corpus utilized in the empirical analysis is ‘‘The\nNews headline dataset for Sarcasm detection’’ [48]. The\nSarcasm Detection headline news dataset is a dataset that is\ncreated by collecting news from two major websites, i.e. The\nOnion, which provided the sarcastic version of different\nevents that occurred across the globe, and HuffPost provided\nthe non-sarcastic version of the same news in order to\npreserve the data set balance. The News headline dataset for\nSarcasm detection corpus consists of 26,709 instances with\napproximately 14,500 sarcastic and 12,000 non-sarcastic\nnews documents.\nB. EXPERIMENTAL PROCEDURE\nTo evaluate the predictive performance of algorithms, classi-\nﬁcation accuracy (ACC) is utilized as the evaluation measure.\nIn the empirical analysis, three neural language models\n(i.e., word2vec, fastText and GloVe), two unsupervised term\nweighting functions (i.e., term-frequency, and TF-IDF) and\neight supervised term weighting functions (i.e., odds ratio,\nrelevance frequency, balanced distributional concentration,\ninverse question frequency-question frequency-inverse\n7714 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 5. Main effects plots for accuracy values in terms of models, term weighting schemes and aggregation functions.\ncategory frequency, short text weighting, inverse gravity\nmoment, regularized entropy and inverse false negative-true\npositive-inverse category frequency) have been evaluated.\nThe proposed scheme has been empirically compared\nwith ﬁve deep neural network architectures (i.e., convo-\nlutional neural network, recurrent neural network, long\nshort-term memory, gated recurrent unit, and bidirectional\nLSTM).\nWe used Tensorﬂow and Keras to implement and train the\narchitectures that are based on deep learning used in empir-\nical analysis. We used hyperparameter search algorithms for\neach model to get maximum predictive efﬁciency from each\ndeep learning model. For this, optimization of hyperparame-\nters based on Bayesian optimization using Gaussian method\nwas used. For deep neural network conﬁgurations, the cross\nentropy loss has been utilized.\nFor word embedding schemes, we have employed contin-\nuous skip-gram and continuous bag of words (CBOW)\nschemes, with vector sizes of 200 and 300. For word2vec and\nfastText models, skip-gram scheme yields higher predictive\nperformance with a vector size of 300. For GloVe word\nembedding scheme, vector size of 300 has been taken. Hence,\nthe empirical results for these conﬁgurations have been listed\nin Section 6.3. For the corpus training, 80% of data has been\ndedicated as the training set, while the rest of data has been\ndedicated as the testing set.\nC. EXPERIMENTAL RESULTS\nIn the empirical analysis, predictive performance of different\nword embedding models, term weighting schemes, aggre-\ngation functions and N-gram models have been evaluated\nin conjunction with eight deep neural network architectures.\nIn Table 1, classiﬁcation accuracies obtained by conventional\nrepresentation schemes and deep neural architectures have\nbeen compared with the proposed scheme on three-sarcasm\ncorpus.\nWith respect to the predictive performance of tradi-\ntional text representation schemes mentioned in Table 1,\nfastText outperforms the other two word embedding\nschemes (i.e., word2vec and GloVe). For text representation,\nthe highest predictive performance has been achieved by\nthe proposed term weighted tri-gram representation model,\nwhich is based on inverse gravity moment based weighted\nfastText with trigram by maximum pooling aggregation.\nIn the empirical analysis, ﬁve conventional deep neural\nnetworks (i.e., convolutional neural networks, recurrent\nneural networks, long short-term memory, gated recurrent\nunit and bidirectional long short-term memory) has been\nVOLUME 9, 2021 7715\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 6. Main effects plots for accuracy values in terms of deep neural architectures, datasets and N-gram models.\ntaken into consideration. In addition, the empirical results\nof the stacked bidirectional long short-term memory archi-\ntecture with different number of layers have been evaluated.\nRecurrent neural network and its variants (i.e., LSTM and\nGRU) outperform convolutional neural network for sarcasm\nidentiﬁcation. Long short-term memory and gated recurrent\nunit architectures yield higher predictive compared to recur-\nrent neural network. Long short-term memory and gated\nrecurrent unit architectures yield similar results. Bidirectional\nlong short-term memory architecture outperforms unidirec-\ntional deep neural network architectures. With respect to the\npredictive performance of stacked architectures, the highest\npredictive performances have been achieved by three layer\nstacked bidirectional long short-term memory architecture.\nFor three-sarcasm corpus evaluated in the empirical analysis,\nthe highest predictive performances have been achieved by\nterm weighted tri-gram representation model in conjunc-\ntion with proposed stacked bidirectional LSTM with three\nlayers.\nTo analyze the effect of term weighting schemes and\nN-gram models on neural language models,\nTables 2-4 present empirical results for word2vec unigram\nmodel, word2vec bigram model and word2vec trigram\nmodel, respectively. Tables 5-7 presents empirical results\nfor GloVe unigram model, GloVe bigram model and GloVe\ntrigram model, respectively. Tables 8-10 present empirical\nresults for fastText unigram model, fastText bigram model\nand fastText trigram model, respectively.\nThe ﬁrst concern of the empirical analysis is to investi-\ngate the performance of term weighting schemes on neural\nlanguage models. As it can be observed from the empir-\nical results listed in Tables 2-10, eight supervised term\nweighting schemes (i.e., odds ratio, relevance frequency,\nbalanced distributional concentration, inverse question\nfrequency-question frequency-inverse category frequency,\nshort text weighting, inverse gravity moment, regularized\nentropy and inverse false negative-true positive-inverse\ncategory frequency) outperform two unsupervised term\nweighting functions (i.e., term-frequency, and TF-IDF)\nfor sarcasm identiﬁcation. The highest predictive perfor-\nmances have been obtained by inverse gravity moment\nbased term weighting scheme. The second highest predictive\nperformances have been obtained by regularized entropy\nbased term weighting scheme, which is followed by\nIFN-TP-ICF scheme. Term weighting schemes in conjunc-\ntion with different neural language models (i.e., word2vec,\nfastText and GloVe) exhibit similar predictive performance\npatterns.\n7716 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 7. Confidence intervals for compared deep neural network architectures.\nThe second concern of the empirical analysis is to examine\nthe predictive performance of different aggregation func-\ntions for word embedding schemes. Regarding the predictive\nperformance of two aggregation functions taken into consid-\neration, maximum pooling aggregation function outperforms\nthe average-pooling scheme for the empirical results listed\nin Tables 2-10.\nThe third concern of the empirical analysis is to\nexamine the effect of word ordering information on neural\nlanguage models. Three N-gram models (i.e., unigram,\nbigram and trigram) have been evaluated in conjunction\nwith term-weighted word embedding schemes. The empirical\nanalysis reveals that word ordering information can enhance\nthe predictive performance of text representation schemes for\nsarcasm identiﬁcation. Trigram based models have achieved\nthe highest predictive performances. In addition, bigram\nbased models generally yield higher predictive performance\ncompared to the unigram based text representation schemes.\nThe fourth concern of the empirical analysis is to evaluate\nthe predictive performance of deep neural networks. The\ncomprehensive empirical results listed in Tables 2-10 indicate\nthat the stacked bidirectional long short-term memory archi-\ntectures outperform the conventional deep neural networks\nfor sarcasm identiﬁcation. The empirical results indicate\nthat the proposed sarcasm identiﬁcation framework based on\nterm weighted word embedding, N-gram word order infor-\nmation and stacked three-layer bidirectional long short-term\nmemory can yield promising results for sarcasm recogni-\ntion. The highest value among the compared conﬁgurations\non ‘‘Sarcasm Corpus 1’’ has been achieved by fastText\ntrigram based conﬁguration with inverse gravity moment\nbased weighting with maximum pooling aggregation, with a\nclassiﬁcation accuracy of 95.30%.\nTo summarize the main ﬁndings of the empirical results\non three-sarcasm corpus, Figures 5-6 present the main effects\nplots for the compared factors. As outlined in the empirical\nresults, fastText model outperforms GloVe and word2vec\nschemes. Inverse gravity moment term weighting scheme\noutperforms the other supervised and unsupervised term\nweighting schemes. Maximum pooling aggregation function\nyields higher predictive performance compared to the aver-\naging scheme. As illustrated in Figure 6, stacked bidirectional\narchitectures outperform unidirectional deep neural network\nbased architectures. Regarding the predictive performance of\ncompared schemes on different sarcasm datasets, the highest\naverage predictive performance values have been achieved\nby ‘‘Sarcasm Corpus 1’’. ‘‘News Headline Corpus’’ has\nachieved the second highest predictive performance values\nVOLUME 9, 2021 7717\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 8. Confidence intervals for compared term weighting schemes.\nand ‘‘Sarcasm Corpus V2’’ has obtained the third highest\nvalues.\nTABLE 11. One-way ANOVA test results.\nTo examine the statistical signiﬁcance of the empir-\nical results, we have performed one-way ANOV A test in\nMinitab statistical analysis software. The results for the\none-way ANOV A test of overall results obtained by the\nconventional clustering methods, word embedding based\nschemes and the proposed scheme have been presented\nin Table 11, where DF, SS, MS, F and P denote degrees\nof freedom. According to the statistical analysis results\npresented in Table 11, there are statistically meaningful\ndifferences between the predictive performance values of\ncompared conﬁgurations (deep neural network architectures,\ndatasets, models, term weighting schemes, aggregation func-\ntions and N-gram models) (p<0,0001).\nIn Figures 7-10, the conﬁdence intervals for the average\nvalues of predictive performance values achieved by the\ncompared conﬁgurations for a conﬁdence level of 95%\nhave been presented. Based on the statistical signiﬁcance of\npredictive performance values, Figure 7 is divided into two\nregions, denoted by red dashed line. As it can be observed\nfrom Figure 7, the proposed three layer stacked bidirectional\nlong short-term memory architecture has been deployed in\nanother region of the interval plot, which indicates that the\npredictive performances obtained by this architecture are\nstatistically signiﬁcant. In a similar way, Figure 8 indicates\nthe interval plots for term weighting schemes. The higher\npredictive performance values obtained by supervised term\nweighting schemes are statistically signiﬁcant.\nIn Figure 9, conﬁdence interval plots for compared aggre-\ngation functions have been given. As it can observed from\nFigure 9, the higher average predictive performance values\nobtained by maximum pooling aggregation function are\nstatistically signiﬁcant. In Figure 10, conﬁdence interval\nplots for compared N-gram models have been presented.\nAs illustrated in Figure 10, the plot has been divided into three\nregions denoted by red dashed lines. Figure 10 indicates that\n7718 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 9. Confidence intervals for compared aggregation functions.\nthe higher predictive performances obtained by trigram based\nmodels are statistically signiﬁcant.\nD. DISCUSSIONS\nBased on the empirical analysis on neural language models,\nterm weighting schemes, aggregation function, deep neural\nnetwork architectures and N-gram models, several insights\nfollow:\n•Neural language models can be effectively utilized to\nrepresent text documents in NLP tasks, including sentiment\nanalysis and sarcasm identiﬁcation. The conventional neural\nlanguage models take equivalent weight values for each word\nin the sentence and take the average of values to obtain\nword embedding. The empirical analysis on term weighting\nschemes in conjunction with neural language models indi-\ncate that term weighted neural language models can enhance\nthe predictive performance of conventional neural language\nmodels. The empirical results are in line with recent\nresearch contributions on supervised and unsupervised NLP\ntasks [3], [34].\n•Regarding the predictive performance of term weighting\nschemes taken into consideration, supervised term weighing\nschemes yield higher predictive performances compared to\nthe unsupervised term weighting schemes. The unsupervised\nterm weighting schemes do not use category information to\nassign weight to terms whereas the supervised term weighting\nschemes use category information for a term from the training\ndata. The empirical results indicate that the utilization of\ncategory information can yield more promising weight values\nfor terms. These empirical results are in line with recent\ncomprehensive analysis on term weighting schemes for text\nclassiﬁcation [11].\n•Regarding the predictive performance of deep neural\nnetwork architectures taken into consideration, stacked bidi-\nrectional long short-term memory based architectures outper-\nform the conventional deep neural networks (such as,\nconvolutional neural networks, gated recurrent units, and\nrecurrent neural networks) for sarcasm identiﬁcation.\n•Regarding the predictive performance of neural language\nmodels in conjunction with different N-gram models,\nthe highest predictive performance values obtained by trigram\nscheme. The empirical results indicate that the utilization of\nword ordering information in word embedding schemes can\nyield better text representation schemes with higher contex-\ntual information.\n•Several limitations characterize the research. In the\npresented scheme, word embedding schemes in conjunction\nwith weighting functions and N-gram models have been\nVOLUME 9, 2021 7719\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nFIGURE 10. Confidence intervals for compared N-gram models.\nevaluated for sarcasm identiﬁcation task. For NLP tasks,\nfeature engineering can inﬂuence the predictive performance\nof sarcasm identiﬁcation frameworks. For sarcasm identiﬁ-\ncation, linguistic feature sets have been frequently utilized.\nThe linguistic feature sets include conventional lexical, prag-\nmatic, implicit incongruity and explicit incongruity based\nfeatures [22]. Hence, different linguistic feature sets may\nbe combined by neural language based models to further\nenhance the predictive performance of the proposed scheme.\nVII. CONCLUSION\nThis work presents a deep learning based framework for\nsarcasm identiﬁcation. We propose a three-layer stacked\nbidirectional long short-term memory architecture to iden-\ntify sarcastic text documents. To effectively represent text\ndocuments, term weighted tri-gram representation model\nhas been introduced. We present inverse gravity moment\nbased term weighted word embedding scheme. In addition,\nthe weighted word embedding model has been combined\nwith trigram model to keep word ordering information. The\nproposed framework has been evaluated on three sarcasm\nidentiﬁcation corpus. The empirical results indicate that the\npresented three-layer stacked bidirectional long short-term\nmemory architecture can yield higher predictive performance\ncompared to the conventional deep neural network archi-\ntectures. In addition, the empirical results indicate that\nthe presented word-embedding scheme outperforms the\nconventional word embedding schemes. The highest value\namong the compared conﬁgurations on ‘‘Sarcasm Corpus\n1’’ has been achieved by fastText trigram based conﬁgu-\nration with inverse gravity moment based weighting with\nmaximum pooling aggregation, with a classiﬁcation accuracy\nof 95.30%.\nREFERENCES\n[1] M. del Pilar Salas-Zárate, G. Alor-Hernández, J. L. Sánchez-Cervantes,\nM. A. Paredes-Valverde, J. L. García-Alcaraz, and R. Valencia-García,\n‘‘Review of english literature on ﬁgurative language applied to social\nnetworks,’’Knowl. Inf. Syst., vol. 62, no. 6, pp. 2105–2137, Jun. 2020, doi:\n10.1007/s10115-019-01425-3.\n[2] A. Joshi, V . Tripathi, K. Patel, P. Bhattacharyya, and M. Carman, ‘‘Are\nword embedding-based features useful for sarcasm detection?’’ 2016,\narXiv:1610.00883. [Online]. Available: https://arxiv.org/abs/1610.00883\n[3] A. Onan, ‘‘Two-stage topic extraction model for bibliometric data anal-\nysis based on word embeddings and clustering,’’ IEEE Access, vol. 7,\npp. 145614–145633, 2019, doi: 10.1109/access.2019.2945911.\n[4] E. Fersini, E. Messina, and F. A. Pozzi, ‘‘Sentiment analysis: Bayesian\nensemble learning,’’ Decis. Support Syst., vol. 68, pp. 26–38, Dec. 2014.\n10.1016/j.dss.2014.10.004.\n7720 VOLUME 9, 2021\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\n[5] A. Rajadesingan, R. Zafarani, and H. Liu, ‘‘Sarcasm detection on Twitter:\nA behavioral modeling approach,’’ in Proc. 8th ACM Int. Conf. Web Search\nData Mining, 2015, pp. 97–106.\n[6] S. K. Bharti, K. S. Babu, and S. K. Jena, ‘‘Parsing-based sarcasm sentiment\nrecognition in Twitter data,’’ in Proc. IEEE/ACM Int. Conf. Adv. Social\nNetw. Anal. Mining, Aug. 2015, pp. 1373–1380.\n[7] G. Hackeling, Mastering Machine Learning With Scikit-Learn. New York,\nNY , USA: Packt Publishing, 2017.\n[8] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and\nP. Kuksa, ‘‘Natural language processing (almost) from scratch,’’ J. Mach.\nLearn. Res., vol. 12, no. 8, pp. 2493–2537, 2011.\n[9] D. Shen, G. Wang, W. Wang, M. R. Min, Q. Su, Y . Zhang, C. Li,\nR. Henao, and L. Carin, ‘‘Baseline needs more love: On simple word-\nembedding-based models and associated pooling mechanisms,’’ 2018,\narXiv:1805.09843. [Online]. Available: https://arxiv.org/abs/1805.09843\n[10] S. Yıldırım and T. Yıldız, ‘‘Türkçe için karşılaştırmalı metin sınıﬂandırma\nanalizi,’’ Pamukkale Üniversitesi Mühendislik Bilimleri Dergisi, vol. 24,\nno. 5, pp. 879–886, 2018.\n[11] J. C. Martineau and T. Finin, ‘‘Delta TFIDF: An improved feature space for\nsentiment analysis,’’ in Proc. 3rd Int. AAAI Conf. Weblogs Social Media,\n2009, pp. 258–261.\n[12] S. S. Samant, N. L. Bhanu Murthy, and A. Malapati, ‘‘Improving term\nweighting schemes for short text classiﬁcation in vector space model,’’\nIEEE Access, vol. 7, pp. 166578–166592, 2019.\n[13] L. Zhang, S. Wang, and B. Liu, ‘‘Deep learning for sentiment analysis:\nA survey,’’Wiley Interdiscipl. Rev. Data Mining Knowl. Discovery, vol. 8,\nno. 4, p. 1253, 2018.\n[14] J. Acosta, N. Lamaute, M. Luo, E. Finkelstein, and C. Andreea, ‘‘Sentiment\nanalysis of twitter messages using Word2vec,’’ in Proc. Student-Fac. Res.,\nvol. 7, 2017, pp. 1–7.\n[15] D. Ghosh, W. Guo, and S. Muresan, ‘‘Sarcastic or not: Word embeddings to\npredict the literal or sarcastic meaning of words,’’ in Proc. Conf. Empirical\nMethods Natural Lang. Process., 2015, pp. 1003–1012.\n[16] D. Jain, A. Kumar, and G. Garg, ‘‘Sarcasm detection in mash-up\nlanguage using soft-attention based bi-directional LSTM and feature-rich\nCNN,’’ Appl. Soft Comput., vol. 91, Jun. 2020, Art. no. 106198, doi:\n10.1016/j.asoc.2020.106198.\n[17] R. González-Ibánez, S. Muresan, and N. Wacholder, ‘‘Identifying sarcasm\nin Twitter: A closer look,’’ in Proc. 49th Annu. Meeting Assoc.\nComput. Linguistics Hum. Lang. Technol. Short Papers, vol. 2, 2011,\npp. 581–586.\n[18] A. Reyes, P. Rosso, and D. Buscaldi, ‘‘From humor recognition to irony\ndetection: The ﬁgurative language of social media,’’ Data Knowl. Eng.,\nvol. 74, pp. 1–12, Apr. 2012, doi: 10.1016/j.datak.2012.02.005.\n[19] F. Barbieri, H. Saggion, and F. Ronzano, ‘‘Modelling sarcasm in Twitter, a\nnovel approach,’’ in Proc. 5th Workshop Comput. Approaches Subjectivity,\nSentiment Social Media Anal., 2014, pp. 50–58.\n[20] F. Kunneman, C. Liebrecht, M. van Mulken, and A. van den Bosch,\n‘‘Signaling sarcasm: From hyperbole to hashtag,’’ Inf. Process. Manage.,\nvol. 51, no. 4, pp. 500–509, Jul. 2015.\n[21] M. Bouazizi and T. Otsuki Ohtsuki, ‘‘A pattern-based approach for sarcasm\ndetection on Twitter,’’ IEEE Access, vol. 4, pp. 5477–5488, 2016, doi:\n10.1109/access.2016.2594194.\n[22] A. Mishra, D. Kanojia, S. Nagar, K. Dey, and P. Bhattacharyya,\n‘‘Harnessing cognitive features for sarcasm detection,’’ 2017,\narXiv:1701.05574. [Online]. Available: https://arxiv.org/abs/1701.05574\n[23] E. Sulis, D. I. H. Farías, P. Rosso, V . Patti, and G. Ruffo, ‘‘Figurative\nmessages and affect in Twitter: Differences between #irony, #sarcasm\nand #not,’’ Knowl.-Based Syst., vol. 108, pp. 132–143, Sep. 2016, doi:\n10.1016/j.knosys.2016.05.035.\n[24] S. Mukherjee, ‘‘Sarcasm detection in microblogs using Naïve Bayes and\nfuzzy clustering,’’ Technol. Soc., vol. 48, pp. 19–27, Feb. 2017.\n[25] K. Sundararajan and A. Palanisamy, ‘‘Multi-rule based ensemble feature\nselection model for sarcasm type detection in Twitter,’’ Comput. Intell.\nNeurosci., vol. 2020, pp. 1–17, Jan. 2020, doi: 10.1155/2020/2860479.\n[26] A. Ghosh and T. Veale, ‘‘Fracking sarcasm using neural network,’’ in Proc.\n7th Workshop Comput. Approaches Subjectivity, Sentiment Social Media\nAnal., 2016, pp. 161–169.\n[27] F. Naz, M. Kamran, W. Mehmood, W. Khan, M. S. Alkatheiri,\nA. S. Alghamdi, and A. A. Alshdadi, ‘‘Automatic identiﬁcation of sarcasm\nin tweets and customer reviews,’’ J. Intell. Fuzzy Syst., vol. 37, no. 5,\npp. 6815–6828, 2019, 10.3233/jifs-190596.\n[28] H. Ren, Z. Zeng, Y . Cai, Q. Du, Q. Li, and H. Xie, ‘‘A weighted word\nembedding model for text classiﬁcation,’’ in Proc. Int. Conf. Database Syst.\nAdv. Appl.Berlin, Germany: Springer, 2019, pp. 419–434.\n[29] P. Mehndiratta and D. Soni, ‘‘Identiﬁcation of sarcasm in textual data:\nA comparative study,’’J. Data Inf. Sci., vol. 4, no. 4, pp. 56–83, Dec. 2019,\ndoi: 10.2478/jdis-2019-0021.\n[30] P. Mehndiratta and D. Soni, ‘‘Identiﬁcation of sarcasm using word embed-\ndings and hyperparameters tuning,’’ J. Discrete Math. Sci. Cryptogr.,\nvol. 22, no. 4, pp. 465–489, May 2019.\n[31] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‘‘Efﬁcient estimation of\nword representations in vector space,’’ 2013, arXiv:1301.3781. [Online].\nAvailable: http://arxiv.org/abs/1301.3781\n[32] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, ‘‘Enriching word\nvectors with subword information,’’ 2016, arXiv:1607.04606. [Online].\nAvailable: http://arxiv.org/abs/1607.04606\n[33] J. Pennington, R. Socher, and C. Manning, ‘‘Glove: Global vectors for\nword representation,’’ in Proc. Conf. Empirical Methods Natural Lang.\nProcess. (EMNLP), 2014, pp. 1532–1543.\n[34] V . Gupta, A. K. Saw, P. P. Talukdar, and P. Netrapalli, ‘‘Unsupervised\ndocument representation using partition word-vectors averaging,’’ in Proc.\n7th Int. Conf. Learn. Represent., 2018, pp. 1–28.\n[35] X. Quan, L. Wenyin, and B. Qiu, ‘‘Term weighting schemes for question\ncategorization,’’ IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5,\npp. 1009–1021, May 2011.\n[36] T. Wang, Y . Cai, H.-F. Leung, Z. Cai, and H. Min, ‘‘Entropy-\nbased term weighting schemes for text categorization in VSM,’’ in\nProc. IEEE 27th Int. Conf. Tools Artif. Intell. (ICTAI), Nov. 2015,\npp. 325–332.\n[37] I. Alsmadi and G. K. Hoon, ‘‘Term weighting scheme for short-text\nclassiﬁcation: Twitter corpuses,’’ Neural Comput. Appl., vol. 31, no. 8,\npp. 3819–3831, Aug. 2019, doi: 10.1007/s00521-017-3298-8.\n[38] B. Fan, W. Fan, C. Smith, and H. S. Garner, ‘‘Adverse drug event\ndetection and extraction from open data: A deep learning approach,’’\nInf. Process. Manage., vol. 57, no. 1, Jan. 2020, Art. no. 102131, doi:\n10.1016/j.ipm.2019.102131.\n[39] A. Elnagar, R. Al-Debsi, and O. Einea, ‘‘Arabic text classiﬁcation using\ndeep learning models,’’ Inf. Process. Manage., vol. 57, no. 1, Jan. 2020,\nArt. no. 102121, doi: 10.1016/j.ipm.2019.102121.\n[40] A. Abdi, S. M. Shamsuddin, S. Hasan, and J. Piran, ‘‘Deep learning-\nbased sentiment classiﬁcation of evaluative text based on multi-feature\nfusion,’’ Inf. Process. Manage., vol. 56, no. 4, pp. 1245–1259, Jul. 2019,\ndoi: 10.1016/j.ipm.2019.02.018.\n[41] M. Song, H. Park, and K.-S. Shin, ‘‘Attention-based long short-term\nmemory network using sentiment lexicon embedding for aspect-level\nsentiment analysis in korean,’’ Inf. Process. Manage., vol. 56, no. 3,\npp. 637–653, May 2019, doi: 10.1016/j.ipm.2018.12.005.\n[42] X. Li and X. Wu, ‘‘Constructing long short-term memory based deep recur-\nrent neural networks for large vocabulary speech recognition,’’ in Proc.\nIEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), Apr. 2015,\npp. 4520–4524.\n[43] C. Li, Z. Bao, L. Li, and Z. Zhao, ‘‘Exploring temporal representations\nby leveraging attention-based bidirectional LSTM-RNNs for multi-modal\nemotion recognition,’’ Inf. Process. Manage., vol. 57, no. 3, May 2020,\nArt. no. 102185.\n[44] L. M. Rojas-Barahona, ‘‘Deep learning for sentiment analysis,’’ Lang.\nLinguistics Compass, vol. 10, no. 12, pp. 701–719, Dec. 2016, doi:\n10.1111/lnc3.12228.\n[45] K. Cho, B. V . Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, Y . Bengio, J. Chung, C. Gulcehre, K. Cho, and Y . Bengio,\n‘‘Learning phrase representations using RNN encoder-decoder for statis-\ntical machine translation,’’ in Empirical Evaluation Of Gated Recurrent\nNeural Networks On Sequence Modeling. New York, NY , USA: ACL,\n2014.\n[46] M. Schuster and K. K. Paliwal, ‘‘Bidirectional recurrent neural networks,’’\nIEEE Trans. Signal Process., vol. 45, no. 11, pp. 2673–2681, Nov. 1997,\ndoi: 10.1109/78.650093.\n[47] S. Oraby, V . Harrison, L. Reed, E. Hernandez, E. Riloff, and\nM. Walker, ‘‘Creating and characterizing a diverse corpus of\nsarcasm in dialogue,’’ 2017, arXiv:1709.05404. [Online]. Available:\nhttps://arxiv.org/abs/1709.05404\n[48] R. Mishra. (2018). The News Headline Dataset for Sarcasm\nDetection. [Online]. Available: https://www.kaggle.com/rmisra/\nnewsheadlines-dataset-for-sarcasm-detection\nVOLUME 9, 2021 7721\nA. Onan, M. A. Toçoğlu: Term Weighted Neural Language Model and Stacked Bidirectional LSTM\nAYTUG ONAN received the B.S. degree in\ncomputer engineering from the İzmir University\nof Economics, Turkey, in 2010, and the M.Sc. and\nPh.D. degrees in computer engineering from Ege\nUniversity, Turkey, in 2013 and 2016, respectively.\nHe has been an Associate Professor of Computer\nEngineering with İzmir Katip Celebi University,\nİzmir, Turkey, since April 2019. His research inter-\nests include machine learning and text mining.\nMANSUR ALP TOÇOĞLU received the B.Sc.\ndegree in software engineering and the M.Sc.\ndegree in artiﬁcial intelligent systems from the\nIzmir University of Economics, İzmir, Turkey,\nin 2008 and 2013, respectively, and the Ph.D.\ndegree in computer engineering from Dokuz\nEylül University, İzmir. He has been working as\nan Assistant Professor with the Software Engi-\nneering Department, Manisa Celal Bayar Univer-\nsity, Manisa, Turkey. His research interest includes\ninformation extraction from text using machine learning.\n7722 VOLUME 9, 2021",
  "topic": "Sarcasm",
  "concepts": [
    {
      "name": "Sarcasm",
      "score": 0.9313814640045166
    },
    {
      "name": "Trigram",
      "score": 0.7450090646743774
    },
    {
      "name": "Computer science",
      "score": 0.7273358702659607
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6867170929908752
    },
    {
      "name": "Weighting",
      "score": 0.5890290141105652
    },
    {
      "name": "Natural language processing",
      "score": 0.5830695629119873
    },
    {
      "name": "tf–idf",
      "score": 0.5281762480735779
    },
    {
      "name": "Term (time)",
      "score": 0.5232338309288025
    },
    {
      "name": "Language model",
      "score": 0.5225428938865662
    },
    {
      "name": "Word2vec",
      "score": 0.4637869596481323
    },
    {
      "name": "Identification (biology)",
      "score": 0.4386729598045349
    },
    {
      "name": "Word embedding",
      "score": 0.43388330936431885
    },
    {
      "name": "Speech recognition",
      "score": 0.3871801793575287
    },
    {
      "name": "Machine learning",
      "score": 0.3717970848083496
    },
    {
      "name": "Linguistics",
      "score": 0.17068448662757874
    },
    {
      "name": "Embedding",
      "score": 0.13174575567245483
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Irony",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Radiology",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}