{
  "title": "Supervised Transformer Network for Efficient Face Detection",
  "url": "https://openalex.org/W2951064206",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1931409288",
      "name": "Chen Dong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143809558",
      "name": "Hua, Gang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103600719",
      "name": "Wen, Fang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1911566220",
      "name": "Sun Jian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2155893237",
    "https://openalex.org/W182571476",
    "https://openalex.org/W1530262073",
    "https://openalex.org/W2156539399",
    "https://openalex.org/W2951829713",
    "https://openalex.org/W1970456555",
    "https://openalex.org/W204612701",
    "https://openalex.org/W1555563476",
    "https://openalex.org/W2034025266"
  ],
  "abstract": "Large pose variations remain to be a challenge that confronts real-word face detection. We propose a new cascaded Convolutional Neural Network, dubbed the name Supervised Transformer Network, to address this challenge. The first stage is a multi-task Region Proposal Network (RPN), which simultaneously predicts candidate face regions along with associated facial landmarks. The candidate regions are then warped by mapping the detected facial landmarks to their canonical positions to better normalize the face patterns. The second stage, which is a RCNN, then verifies if the warped candidate regions are valid faces or not. We conduct end-to-end learning of the cascaded network, including optimizing the canonical positions of the facial landmarks. This supervised learning of the transformations automatically selects the best scale to differentiate face/non-face patterns. By combining feature maps from both stages of the network, we achieve state-of-the-art detection accuracies on several public benchmarks. For real-time performance, we run the cascaded network only on regions of interests produced from a boosting cascade face detector. Our detector runs at 30 FPS on a single CPU core for a VGA-resolution image.",
  "full_text": "Supervised Transformer Network for Eﬃcient\nFace Detection\nDong Chen, Gang Hua, Fang Wen, and Jian Sun\nMicrosoft Research\n{doch,ganghua,fangwen,jiansun}@microsoft.com\nAbstract. Large pose variations remain to be a challenge that con-\nfronts real-word face detection. We propose a new cascaded C onvolution-\nal Neural Network, dubbed the name Supervised Transformer N etwork,\nto address this challenge. The ﬁrst stage is a multi-task Reg ion Proposal\nNetwork (RPN), which simultaneously predicts candidate fa ce regions\nalong with associated facial landmarks. The candidate regi ons are then\nwarped by mapping the detected facial landmarks to their can onical po-\nsitions to better normalize the face patterns. The second st age, which is\na RCNN, then veriﬁes if the warped candidate regions are vali d faces or\nnot. We conduct end-to-end learning of the cascaded network , including\noptimizing the canonical positions of the facial landmarks . This super-\nvised learning of the transformations automatically selec ts the best scale\nto diﬀerentiate face/non-face patterns. By combining feat ure maps from\nboth stages of the network, we achieve state-of-the-art det ection accura-\ncies on several public benchmarks. For real-time performan ce, we run the\ncascaded network only on regions of interests produced from a boosting\ncascade face detector. Our detector runs at 30 FPS on a single CPU core\nfor a VGA-resolution image.\n1 Introduction\nAmong the various factors that confront real-world face detect ion, large pose\nvariations remain to be a big challenge. For example, the seminal Viola- Jones [1]\ndetector works well for near-frontal faces, but become much le ss eﬀective for\nfaces in poses that are far from frontal views, due to the weakne ss of the Haar\nfeatures on non-frontal faces.\nThere were abundant works attempted to tackle with large pose va riations\nunder the regime of the boosting cascade advocated by Viola and Jo nes [1]. Most\nof them adopt a divide-and-conquer strategy to build a multi-view fa ce detector.\nSome works [2–4] proposed to train a detector cascade for each v iew and combine\ntheir results of all detectors at the test time. Some other works [5 –7] proposed to\nﬁrst estimate the face pose and then run the cascade of the corr esponding face\npose to verify the detection. The complexity of the former approa ch increases\nwith the number of pose categories, while the accuracy of the latte r is prone to\nthe mistakes of pose estimation.\nPart-based model oﬀers an alternative solution [8–10]. These dete ctors are\nﬂexible and robust to both pose variation and partial occlusion, sinc e they can\narXiv:1607.05477v1  [cs.CV]  19 Jul 2016\n2 Dong Chen, Gang Hua, Fang Wen, Jian Sun\nreliably detect the faces based on some conﬁdent part detections . However, these\nmethods always require the target face to be large and clear, which is essential\nto reliably model the parts.\nOther works approach to this issue by using more sophisticated inva riant\nfeatures other than Haar wavelets, e.g., HOG [8], SIFT [9], multiple channel fea-\ntures [11], and high-level CNN features [12]. Besides these model-ba sed methods,\nShen et al. [13] proposed to use an exemplar-based method to detect faces b y\nimage retrieval, which achieved state-of-the-art detection accu racy.\nIt has been shown in recent years that a face detector trained en d-to-end\nusing DNN can signiﬁcantly outperforms previous methods [10, 14]. H owever,\nto eﬀectively handle the diﬀerent variations, especially pose variatio ns, it often\nrequires a DNN with lots of parameters, inducing high computational cost. To\naddress the conﬂicting challenge, Li et al. [15] proposed a cascade DNN archi-\ntecture at multiple resolutions. It quickly rejects the background regions in the\nlow resolution stages, and carefully evaluates the challenging candid ates in the\nhigh resolution stage.\nHowever, the set of DNNs in Li et al. [15] are trained sequentially, instead\nof end-to-end, which may not be desirable. In contrast, we propo se a new cas-\ncade Convolutional Neural Network that is trained end-to-end. T he ﬁrst stage\nis a multi-task Region Proposal Network (RPN), which simultaneously propos-\nes candidate face regions along with associated facial landmarks. I nspired by\nChen et al. [16], we jointly conduct face detection and face alignment, since fac e\nalignment is helpful to distinguish faces/non-faces patterns.\nDiﬀerent from Li et al. [15], this network is calculated on the original reso-\nlution to better leverage more discriminative information. The alignme nt step\nwarps each candidate face region to a canonical pose, which maps t he facial land-\nmarks into a set of canonical positions. The aligned candidate face r egion is then\nfed into the second-stage network, a RCNN [17], for further veriﬁ cation. Note we\nonly keep the K face candidate regions with top responses in a local neighbor-\nhood from the RPN. In other words, those Non-top K regions are s uppressed.\nThis helps increase detection recall.\nInspired by previous work [18], which revealed that joint features f rom dif-\nferent spatial resolutions or scales will improve accuracy. We conc atenate the\nfeature maps from the two cascaded networks together to form an architecture\nthat is trained end-to-end, as shown in Figure 1. Note in the learning process,\nwe treat the set of canonical positions also as parameters, which a re learnt in\nthe end-to-end learning process.\nNote that the canonical positions of the facial landmarks in the align ed face\nimage and the predicted facial landmarks in the candidate face regio n jointly\ndeﬁnes the transform from the candidate face region. In the end -to-end training,\nthe training of the ﬁrst-stage RPN to predict facial landmarks is als o supervised\nby annotated facial landmarks in each true face regions. We hence call our net-\nwork a Supervised Transformer Network. These two characteris tics diﬀerentiate\nour model from the Spatial Transformer Network [19] because a) the Spatial\nSupervised Transformer Network for Eﬃcient Face Detection 3\nSupervised \nTransformer \nfc \nRPN \nNet\nRCNN \nNet\nScore 1\nShape\nRCNN Feature\nScore 2\nRCNN FeatureRPN Feature\n128\n128\n256\nSo/g332max Loss 1\nL2 Loss \nSo/g332max Loss 2\nRPN \nFeature\nFig. 1. Illustration of the structure of our Supervised Transforme r Network.\nTransformer Network conducts regression on the transformat ion parameters di-\nrectly, and b) it is only supervised by the ﬁnal recognition objective .\nThe proposed Supervised Transformer Network can eﬃciently run on the G-\nPU. However, in practice, the CPU is still the only choice in most situat ions.\nTherefore, we propose a region-of-interest (ROI) convolution s cheme to make the\nrun-time of the Supervised Transformer Network to be more eﬃcie nt. It ﬁrst uses\na conventional boosting cascade to obtain a set of face candidate areas. Then, we\ncombine these regions into irregular binary ROI mask. All DNN operat ions (in-\ncluding convolution, ReLU, pooling, and concatenation) are all proc essed inside\nthe ROI mask, and hence signiﬁcantly reduce the computation.\nOur contributions are: 1) we proposed a new cascaded network na med Super-\nvised Transformer Network trained end-to-end for eﬃcient face detection; 2) we\nintroduced the supervised transformer layer, which enables to lea rn the optimal\ncanonical pose to best diﬀerentiate face/non-face patterns; 3 ) we introduced a\nNon-top K suppression scheme, which can achieve better recall wit hout sacri-\nﬁcing precision; 4) we introduced a ROI convolution scheme. It spee ds up our\ndetector 3x on CPU with little recall drop.\nOur face detector outperformed the current best performing a lgorithms on\nseveral public benchmarks we evaluated, with real-time performan ce at 30 frames\nper second with VGA resolution.\n2 Network Architecture\n2.1 Overview\nIn this section, we will introduce the architecture of our proposed cascade net-\nwork. As illustrated in Figure 1, the whole architecture consists of t wo stages.\nThe ﬁrst stage is a multi-task Region Proposal Network (RPN). It p roduces a\nset of candidate face regions along with associated facial landmark s. We conduct\nNon-top K suppression to only keep the candidate face regions with responses\nranked in the top K in a local neighborhood.\n4 Dong Chen, Gang Hua, Fang Wen, Jian Sun\nThe second stage starts with a Supervised Transformer layer, an d then a\nRCNN to further verify if a face region is a true face or not. The tra nsformer\nlayer takes the facial landmarks and the candidate face regions, t hen warp the\nface regions into a canonical pose by mapping the detected facial la ndmarks into\na set of canonical positions. This explicitly eliminates the eﬀect of rot ation and\nscale variation according to the facial points.\nTo make this clear, the geometric transformation are uniquely dete rmined by\nthe facial landmarks and the canonical positions. In our cascade n etwork, both\nthe prediction of the facial landmarks and the canonical positions a re learned\nin the end-to-end training process. We call it a Supervised Transfo rmer layer,\nas it receives supervision from two aspects. On one hand, the learn ing of the\nprediction model of the facial landmarks are supervised by the ann otated ground-\ntruth facial landmarks. On the other hand, the learning of both th e canonical\npositions and the prediction model of the facial landmarks both are supervised\nby the ﬁnal classiﬁcation objective.\nTo make a ﬁnal decision, we concatenate the ﬁne-grained feature from the\nsecond-stage RCNN network and the global feature from the ﬁrs t-stage RPN\nnetwork. The concatenated features are then put into a fully con nected layer to\nmake the ﬁnal face/non-face arbitration. This concludes the who le architecture\nof our proposed cascade network.\n2.2 Multi-task RPN\nThe design of the multi-task RPN is inspired by the JDA detector [16], w hich val-\nidated that face alignment is helpful to distinguish faces/non-face s. Our method\nis very straight forward. We use a RPN to simultaneous detect face s and as-\nsociated facial landmarks. Our method is very similar to the work [20], except\nthat our regression target is facial landmark locations, instead of bounding box\nparameters.\n2.3 The supervised transformer layer\nIn this section, we describe the detail of the supervised transfor mer layer. As\nwe know, similarity transformation was widely used in face detection a nd face\nrecognition task to eliminate scale and rotation variation. The commo n practice\nis to train a prediction model to detect the facial landmarks, and th en warp\nthe face image to a canonical pose by mapping the facial landmarks t o a set of\nmanually speciﬁed canonical locations.\nThis process at least has two drawbacks: 1) one needs to manually s et the\ncanonical locations. Since the canonical locations determines the s cale and oﬀ-\nset of rectiﬁed face images, it often takes many try-and-errors to ﬁnd a rela-\ntively good setting. This is not only time-consuming, but also suboptim al. 2)\nThe learning of the prediction model for the facial landmark is super vised by\nthe ground-truth facial landmark points. However, labeling groun d-truth facial\nlandmarks is a highly subjective process and hence prone to introdu cing noise.\nSupervised Transformer Network for Eﬃcient Face Detection 5\nWe propose to learn both the canonical positions and the prediction of the\nfacial landmarks end-to-end from the network with additional sup ervision in-\nformation from the classiﬁcation objective of the RCNN using end-t o-end back\npropagation. Speciﬁcally, we use the following formula to deﬁne a simila rity\ntransformation, i.e.,\n[\n¯xi − m¯x\n¯yi − m¯y\n]\n=\n[\na b\n−b a\n][\nxi − mx\nyi − my\n]\n, (1)\nwhere xi, y i are the detected facial landmarks, ¯ xi, ¯yi are the canonical positions,\nm∗ is the mean value of the corresponding variables, e.g., mx = 1\nN\n∑xi, N is\nthe number of facial landmarks, a and b are parameters of similarity transforms.\nWe found that this two parameters model is equivalent to the tradit ional four\nparameters, but much simpler in derivation and avoid problems of num erical\ncalculation. After some straightforward mathematical derivation , we can obtain\nthe least squares solution of the parameters, i.e.,\na = c1\nc3\nb = c2\nc3\n.\n(2)\nwhere\nc1 =\n∑\n((¯xi − m¯x)(xi − mx) + (¯yi − m¯y)(yi − my))\nc2 =\n∑\n((¯xi − m¯x)(yi − my) − (¯yi − m¯y)(xi − mx))\nc3 =\n∑(\n(xi − mx)2 + ( yi − my)2)\n.\n(3)\nAfter obtaining the similarity transformation parameters, we can o btain the\nrectiﬁed image ¯I given the original image I, using ¯I(¯x, ¯y) = I(x, y ). Each point\n(¯x, ¯y) in the rectiﬁed image can be mapped back to the original image space\n(x, y ) by\nx = a\na2 + b2 (¯x − m¯x) − b\na2 + b2 (¯y − m¯y) + mx\ny = b\na2 + b2 (¯x − m¯x) + a\na2 + b2 (¯y − m¯y) + my.\n(4)\nSince x and y may not be integers, bilinear interpolation is always used to obtain\nthe value of I(x, y ). Therefore, we can calculate the derivative by the chain rule\n∂L\n∂a =\n∑\n{¯x,¯y}\n∂L\n∂ ¯I(¯x, ¯y)\n∂ ¯I(¯x, ¯y)\n∂a =\n∑\n{¯x,¯y}\n∂L\n∂ ¯I(¯x, ¯y)\n∂I(x, y )\n∂a\n=\n∑\n{¯x,¯y}\n∂L\n∂ ¯I(¯x, ¯y)\n(∂I(x, y )\n∂x\n∂x\n∂a + ∂I(x, y )\n∂y\n∂y\n∂a\n)\n=\n∑\n{¯x,¯y}\n∂L\n∂ ¯I(¯x, ¯y)\n(\nIx\n∂x\n∂a + Iy\n∂y\n∂a\n)\n(5)\n6 Dong Chen, Gang Hua, Fang Wen, Jian Sun\nwhere L is the ﬁnal classiﬁcation loss and ∂L\n∂ ¯I(¯x,¯y) is the gradient signals back\npropagated from the RCNN network. The Ix and Iy are horizontal and vertical\ngradient of the original image\nIx = βy(I(xr, y b) − I(xl, y b)) + (1 − βy)(I(xr, y t) − I(xl, y t))\nIy = βx(I(xr, y b) − I(xr, y t)) + (1 − βx)(I(xl, y b) − I(xl, y t)). (6)\nHere we use a bilinear interpolation, βx = x−⌊x⌋ and βy = y−⌊y⌋. xl = ⌊x⌋, x r =\nxl + 1 , y t = ⌊y⌋, y b = yt + 1 are the left, right, top, bottom integer boundary of\npoint ( x, y ). Similarly, we can obtain the derivative of other parameters. Finally ,\nwe can obtain the gradient of the canonical positions of the facial la ndmarks,\ni.e., ∂L\n∂¯xi\nand ∂L\n∂¯yi\n. And the gradient with respect to the detected facial landmarks:\n∂L\n∂xi\nand ∂L\n∂yi\n. Please refer to the supplementary material for more detail.\nThe proposed Supervised Transformer layer is put between of the RPN and\nRCNN networks. In the end-to-end training, it automatically adjus ts the canon-\nical positions and guiding the detection of the facial landmarks such that the\nrectiﬁed image is more suitable for face/non-face classiﬁcation. We will further\nillustrate this in the experiments.\n2.4 Non-top K suppression\nIn RCNN [20, 17] based object detection, after the region propos als, non-maximum\nsuppression (NMS) is always adopted to reduce the region candidat e number for\neﬃciency. However, the candidate with highest conﬁdence score m ay be rejected\nby the later stage RCNN. Decreasing the NMS overlap threshold will b ring in\nlots of useless candidates. This will make subsequent RCNN slow. Our idea is to\nkeep K candidate regions with highest conﬁdence for each potentia l face, since\nthese samples are more promising for RCNN classiﬁer. In the experim ents part\nwe will demonstrate that we can eﬀectively improve the recall with th e proposed\nNon-top K Suppression.\nreceptive ﬁeld receptive\ntype relationship ﬁeld size\nconv1 (7 × 7, 2) 2k+5 85\nmax pool (2 × 2, 2) 2k 40\nconv 2a (1 × 1, 1) k 20\nconv 2b (3 × 3, 1) k+2 20\nmax pool (2 × 2, 2) 2k 18\ninception 3a k+4 9\ninception 3b k+4 5\nTable 1. RPN network structure\nSupervised Transformer Network for Eﬃcient Face Detection 7\n2.5 Multi-granularity feature combination\nSome works have revealed that joint features from diﬀerent spat ial resolutions\nor scales will improve accuracy [18]. The most straight-forward way may be\ncombining several RCNN networks with diﬀerent input scales. Howev er, this\napproach will obviously increase the computation complexity signiﬁca ntly.\nIn our end-to-end network, the details of the RPN network struc ture is shown\nin Table 1. There are 3 convolution and 2 inception layers in our RPN net work.\nTherefore, we can calculate that its receptive ﬁeld size is 85. While th e target\nface size is 36 ∼ 72 pixels. Therefore, our RPN takes advantage of the surround-\ning contextual information around face regions. On the other han d, the RCNN\nnetwork focuses more on the rotation and scale variation ﬁne grain ed detail in\nthe inner face region. So we concatenate these two features in an end-to-end\ntraining architecture, which makes the two parts more complement ary. Experi-\nments demonstrate that this kind of joint feature can signiﬁcantly improve the\nface detection accuracy. Besides, the proposed method is much m ore eﬃcient.\n3 The ROI convolution\n3.1 Motivation\nAs a practical face detection algorithm, real-time performance is v ery important.\nHowever, the heavy computation incurred at test phase using DNN -based models\noften make them impractical in real-world systems. That is the reas on why\ncurrent DNN-based models heavily rely on a high-end GPU to increase the run-\ntime performance. However, high-end GPU is not often available in co mmodity\ncomputing system, so most often, we still need to run the DNN mode l with a\nCPU. However, even using a high-end CPU with highly optimized code, it is still\nabout 4 times slower than the runtime speed on a GPU [21]. More import antly,\nfor portable devices, such as phones and tablets, mostly have low- end CPUs\nonly, it is necessary to accelerate the test-phase performance o f DNNs.\nIn a typical DNN, the convolutional layers are the most computatio nally ex-\npensive and often take up about more than 90% of the time in runtime . There\nwere some works attempted to reduce the computational complex ity of convolu-\ntion layer. For example, Jaderberg et al. [22] applied a sparse decomposition to\nreconstruct the convolutional ﬁlters. Some other works [23, 24] assume that the\nconvolutional ﬁlters are approximately low-rank along certain dimen sions, and\ncan be approximately decomposed into a series of smaller ﬁlters. Our detector\nmay also beneﬁt from these model compression techniques.\nNevertheless, we propose a more practical approach to accelera te the runtime\nspeed of our proposed Supervised Transformer Network for fac e detection. Our\nmain idea is to use a conventional cascade based face detector to q uickly reject\nnon-face regions and obtain a binary ROI mask. The ROI mask has the same\nsize as the input. The background area is represented by 0 and the face area\nis represented by 1. The DNN convolution is only computed within the r egion\nmarked as 1, ignoring all other regions. Because most regions did no t participate\n8 Dong Chen, Gang Hua, Fang Wen, Jian Sun\nT\nT\nT\nF\nF\nF\nFace\nNon-face \n36~72\n72~144\n144~288\nFace Size:\nCascade Prefilter\nFace CandidatesInput\nROI mask\nFig. 2. Illustration of the ROI mask\nin the calculation, we can greatly reduce the amount of computation in the\nconvolution layers.\nWe want to emphasize that our method is diﬀerent to those RCNN bas ed\nalgorithm [17, 25] which treated each candidate region independent ly. In those\nmodels, features in the overlap subregions will be calculated repeat edly. Instead,\nwe use the ROI masks, so that diﬀerent samples can share the feat ure in the\noverlapping area. It eﬀectively reduces the computational cost b y further avoid-\ning repeated operations. Meanwhile, in the following section, we will int roduce\nthe implementation details of our ROI convolution. Similar to Caﬀe [26], w e\nalso take advantage of the matrix multiplication in the BLAS library to o btain\nalmost a linear speedup.\n3.2 Implementation details\nCascade pre-ﬁlter . As shown in Figure 2, we use a cascade detector as a pre-\nﬁlter. It is basically a variant of the Volia-Jones’s detector [1], but it h as more\nweak classiﬁers and is trained with more data. Our boosted classiﬁer is consisted\nof 1000 weak classiﬁers. Diﬀerent form [1], we adopted a boosted fe rn [27] as\nthe weaker classiﬁer, since a fern is more powerful than using a sing le Haar\nfeature based decision stump, and more eﬃcient than boosted tre e on CPUs.\nFor completeness, we brieﬂy describe our implementation.\nEach fern contains 8 binary nodes. The splitting function is to compa re the\ndiﬀerence of two image pixel values in two diﬀerent locations with a thr eshold,\ni.e.,\nsi =\n{\n1 p(x1i , y 1i ) − p(x2i , y 2i ) < θ i\n0 otherwise (7)\nwhere p is the image patch. The patch size is ﬁxed to 32 in our experiments. Th e\n(x1i , y 1i , x 2i , y 2i , θ i) are fern parameters learned from training data. Each fern\nsplits the data space into 2 8 = 256 partitions. We use a Real-Boost algorithm\nfor the cascade classiﬁcation learning. In each space partition, th e classiﬁcation\nSupervised Transformer Network for Eﬃcient Face Detection 9\nscore is computed as\n1\n2 log\n(∑\n{i∈piece ⋂yi=1} wi\n∑\n{i∈piece ⋂yi=0} wi\n)\n, (8)\nwhere the enumerator and denominator are the sum of the weights of positive\nand negative samples in the space partition, respectively.\nThe ROI mask . After we obtain some candidate face regions, we will group\nthem according to their sizes. The maximum size is twice larger than th e min-\nimum size in each group. Since the smallest face size can be detected b y the\nproposed DNN based face detector is 36 × 36 pixels, the ﬁrst group contains the\nface size between 36 to 72 pixels. While the second ground contains t he face size\nbetween 72 to 144, and so on (as shown in Figure 2).\nIt should be noted that, beginning from the second group, we need to down-\nsample the image, such that the candidate face size in the image is alwa ys main-\ntained between 36 to 72 pixels. Besides, in order to retain some of th e background\ninformation, we will double the side length of each candidate. But the side length\nwill not exceed the receptive ﬁeld size (85) of the following DNN face d etector.\nFinally, we set the ROI mask according to the sizes and positions of th e candidate\nboxes in each group.\nWe use this grouping strategy for two reasons. First, when there is a face\nalmost ﬁlling the whole image, we do not have to deal with the full origina l\nimage size. Instead, it will be down-sampled to a quite small resolution , so we\ncan more eﬀectively reduce the computation cost. Secondly, since the following\nDNN detector only need to handle twice the scale variation, this is indu ces a\ngreat advantage when compared with the RPN in [20], which needs to h andle\nall scale changes. This advantage allows us to use a relatively cheape r network\nfor the DNN-based detection.\nBesides, such a sparse pyramid structure will only increase about 3 3% ( 1\n22 +\n1\n42 + 1\n82 · · · ≈1\n3 ) computation cost when compared with the computational cost\nat the base scale.\nDetails of the ROI convolution . There are several ways to implement the\nconvolutions eﬃciently. Currently, the most popular method is to tr ansform the\nconvolutions into a matrix multiplication. As described in [28] and impleme nted\nin Caﬀe [26], this can be done by ﬁrstly reshaping the ﬁlter tensor into a matrix\nF with dimensions CK2 × N, where C and N are input and output channel\nnumbers, and K is the ﬁlter width/height.\nWe can subsequently gather a data matrix by duplicating the original input\ndata into a matrix D with dimensions WH × CK2, W and H are output width\nand height. The computation can then be performed with a single mat rix mul-\ntiplication to form an output matrix O = DF with dimension WH × N. This\nmatrix multiplication can be eﬃciently calculated with optimized linear alge bra\nlibraries such as BLAS.\nOur main idea in ROI convolution is to only calculate the area marked as\n1 ( a.k.a, the ROI regions), while skipping other regions. According to the RO I\nmask, we only duplicate the input patches whose centers are marke d as 1. So\n10 Dong Chen, Gang Hua, Fang Wen, Jian Sun\nROI Conv \nROI Conv ROI Conv \nROI Conv \nMax Pool \nMax Pool \nHalf Sample \nHalf Sample \nFig. 3. Illustration of the ROI convolution.\nthe input data become a matrix D′ with dimensions M × CK2, where M is\nthe number of non-zero entries in the ROI mask. Similarly, we can the n use\nmatrix multiplication to obtain the output O′ = D′F with dimension M ×CK2.\nFinally, we put each row of O′ to the corresponding channel of the output.\nThe computation complexity of ROI convolution is MCK 2N. Therefore, we can\nlinearly decrease the computation cost according to the mask spar sity.\nAs illustrated in Figure 3, we only apply the ROI convolution in the test\nphase. We replace all convolution layers into ROI convolution layers. After a\nmax pooling, the size of the input will be halved. So we also half sample th e\nROI mask, such that their size can be matched. The original DNN det ector\ncan run at 50 FPS on GPU and 10 FPS on CPU for a VGA image. With ROI\nconvolution, it can speed up to 30 FPS on CPU with little accuracy loss.\n4 Experiments\nIn this section, we will experimentally validate the proposed method. We col-\nlected about 400 K face images from the web with various variations as positive\ntraining samples. These images are exclusive from FDDB [29], AFW [8] an d\nPASCAL [30] datasets. We labeled all faces with 5 facial points (two e yes cen-\nter, nose tip, and two mouth corners). For the negative training s amples, we\nuse the Coco database [31]. This dataset has pixel level annotation s of various\nobjects, including people. Therefore, we covered all person area s with random\ncolor blocks, and ensure that no samples are drawn from those colo red regions\nin these images. We use more than 120 K images (including 2014 training and\nvalidation data) for the training. Some sample images are shown in Fig. 4.\nWe use GoogleNet in both the RPN and RCNN networks. The network s truc-\nture is similar to that in FaceNet [32], but we cut all the convolution ker nel\nnumber in half for eﬃciency. Moreover, we only include two inception la yers in\nRPN network (as shown in Table 1) and the input size of RCNN network is 64.\nSupervised Transformer Network for Eﬃcient Face Detection 11\nFig. 4. Illustration of our negative training sample. We covered al l person area with\nrandom color blocks in Coco [31] dataset and ensured that no p ositive training samples\nare drawn from these regions in these images.\nIn order to avoid the initialization problem and improve the convergen ce\nspeed, we ﬁrst train the RPN network from random without the RCN N net-\nwork. After the predicted facial landmarks are largely correct, w e add the RCNN\nnetwork and perform end-to-end training together. For evaluat ion, we use three\nchallenging public datasets, i.e., FDDB [29], AFW [8] and PASCAL faces [30].\nAll these three datasets are widely used as face detection benchm ark. We em-\nploy the Intersection over Union (IoU) as the evaluation metric and ﬁx the IoU\nthreshold to 0.5.\n4.1 Learning canonical position\nIn this part, we verify the eﬀect of the Supervised Transformatio n in ﬁnding\nthe best canonical position. We intentionally initialize the Supervised T ransfor-\nmation with three inappropriate canonical positions according to th ree settings,\nrespectively, i.e., too large, too small, or with oﬀset. Then we perform the end-\nto-end training and record the canonical points position after 10K , 100K, 500K\niterations.\nAs shown in Fig. 5, each row shows the canonical positions movement for\none kind of initializations. We also place the image warp result besides its corre-\nsponding canonical points. We can observe that, for these three diﬀerent kinds\nof initializations, they all eventually converge to a very close position setting af-\nter 500K iterations. It demonstrated that the proposed Superv ised Transformer\nmodule is robust to the initialization. It automatically adjusts the can onical\npositions such that the rectiﬁed image is more suitable for face/non -face classi-\nﬁcation.\n4.2 Ablative evaluation of various network components\nAs discussed in Sec. 2, our end-to-end cascade network is consist ed of four no-\ntable parts, i.e., the multi-task RPN, the Supervised Transformer, the multi-\ngranularity feature combination, and non-top K suppression. The former three\nwill aﬀect the network structure of training, while the last one only a ppear in\nthe test phase.\nIn order to separately study the eﬀect of each part, we conduct an ablative\nstudy by removing one or more parts from our network structure and evaluate\n12 Dong Chen, Gang Hua, Fang Wen, Jian Sun\nTest Sample \nInit Iter 10K Iter 100K Iter 500K\nSmall:\nlarge:\nbiased:\nFig. 5. Results of learning canonical positions.\nMulti-task RPN N N Y Y Y Y\nSupervised Transformer / / N Y N Y\nFeature combination N Y N N Y Y\nRecall Rate 85.6% 88.0% 87.1% 88.3% 88.8% 89.6%\nTable 2. Evaluation of the eﬀect of three parts in training architect ure.\nthe new network with the same training and testing data. When remo ving the\nmulti-task RPN, it means that we directly regress the face rectang le similar\nto [20], instead of facial points. Without the Supervised Transform er layer, we\nsimply replace it with a standard similarity transformation without tra ining with\nback propagation. Without the feature combination component me ans that we\ndirectly use the output of the RCNN features to make the ﬁnial dec ision. In\nthe case that we removed multi-task RPN, there will be no facial poin ts for\nSupervised Transformation or conventional similarity transforma tion. In this\nsituation, we directly resize the face patch into 64 × 64 and fed it into a RCNN\nnetwork.\nThere are 6 diﬀerent ablative settings in total. We perform end-to- end train-\ning with the same training samples for all settings, and evaluate the r ecall rate\non the FDDB dataset when the false alarm number is 10. We manually review\nthe face detection results and add 67 unlabeled faces in the FDDB da taset to\nmake sure all the false alarms are true. As shown in Table 2, multi-tas k RPN, Su-\npervised Transformer, and feature combination will bring about 1% , 1%, and 2%\nrecall improvement respectively. Besides, these three parts are complementary,\nremove any one part will cause a recall drop.\nIn the training phase, in order to increase the variation of training s amples,\nwe randomly select K positive/negative samples from each image for t he RCNN\nnetwork. However, in the test phase, we need to balance the reca ll rate with\neﬃciency. Next, we will compare the proposed non-top K suppress ion with NMS\nin the testing phase,\nWe present a sample visual result of RPN, NMS and non-top K suppre ssion\nin Fig. 6. We keep the same number of candidates for both NMS and No n-top K\nsuppression ( K = 3 in the visual result). We found that NMS tend to include too\nmuch noisy low conﬁdence candidates. We also compare the PR curve s of using\nSupervised Transformer Network for Eﬃcient Face Detection 13\n−2.372092.418474.92762.64211−1.46713−2.761166.7945311.897313.043612.42827.481430.388981\n−0.617633.021992.14106−2.08364 0.66207211.348815.085114.196414.346910.2533.59056−1.96867\n−1.941667.690499.425259.382244.72838−2.30704 1.3725811.326115.798413.912413.160610.33723.98692\n−1.2281\n4.1829214.129815.477314.715612.87184.97851−2.3907\n−2.386136.5305412.220613.774312.20086.40494\n−1.39011\n−2.967620.559887\n4.714314.35514.955213.230611.10676.67951−0.962566 −2.028194.020955.716694.34942\n−2.28893−1.375283.2196310.492812.049613.618810.07335.26462\n−2.902123.218066.00997.201154.38735−0.717036\n−2.85219−2.52554−2.03158−2.12082−0.897877−1.49078 −2.78802−2.23297−2.97399\n−2.49201−2.88542−2.58914−2.2714−1.95483\n−2.77331\n−2.4941−2.82915−2.97661\n−0.8554853.484252.33645\n0.783903\n0.9747623.197721.68617\n−1.95694 −0.755279\n5.486048.111417.55691\n5.40102−1.59994\n−2.214274.30633\n10.721311.6586\n8.523575.23913\n2.176447.652867.378967.403267.43012\n−0.207047\n0.14890110.3071\n9.531719.5237511.03577.62308−2.94538\n1.221216.950738.130436.996326.43452\n0.101984\n0.76426810.2705\n11.13369.2857911.874\n7.97314\n−2.32836 −2.06352\n2.13546.127366.104233.16044−2.80971\n−2.569865.806229.8303512.09949.31753.45199\n−2.47942\n−1.84718\n2.646913.30051\n0.435778\n−2.64819−1.19839−1.832790.0853313−1.35725\n−2.88089−2.86741−2.4571−2.08966 −1.23536−2.62529−0.304019−0.928625\n−2.79757−2.47557−2.78879 −1.12208−0.8054730.531686−0.973129−1.94769−1.95185\n−2.84192−2.20664\n15.7984\n15.4773\n0.531686\n−1.94769\n−1.95483\n−2.03158\n−2.20664\n−2.80971\n 15.798415.085114.3469\n15.477314.955214.7156\n0.5316860.0853313\nRPN NMS Non-top K Suppression 0 5 10 15 200.85\n0.86\n0.87\n0.88\n0.89\n0.9\nFalse positive\nTrue positive\n \n \nAll Candidates (89.6%)\nNMS (88.7%)\nNon−top 3 Supp (89.3%)\nNon−top 5 Supp (89.4%)\nFig. 6. Comparison of NMS and Non-Top K Suppression\nPre-ﬁlter Threshold N/A 0 1 2 3\nROI Mask Sparsity N/A 31.3% 27.1% 10.6% 5.7%\nPre-ﬁlter Time (ms) 0 12.1 12.0 12.0 11.9\nRPN Time (ms) 98.2 (100%) 33.9 (34 .5%) 24.2 (24 .6%) 11.0 (11 .2%) 8.1 (8 .2%)\nRCNN Time (ms) 9.0 9.3 8.7 9.1 9.3\nTotal Time (ms) 107.2 55.3 44.8 32.1 29.3\nRecall Rate 89.3% 89.2% 89.0% 88.7% 88.1%\nTable 3. Various results demonstrating the eﬀects of ROI convolutio n.\nall candidates, NMS, and non-top K suppression. Our non-top K su ppression is\nvery close to using all candidates, and achieved consistently bette r results than\nNMS under the same number of candidates.\n4.3 The eﬀect of ROI convolution\nIn this section, we will validate the acceleration performance of the proposed ROI\nconvolution algorithm. We train the Cascade pre-ﬁlter with the same training\ndata. By adjusting the classiﬁcation threshold of the Cascade re- ﬁlter, we can\nobtain the ROI masks in diﬀerent areas. Therefore, we can strike f or the right\nbalance between speed and accuracy.\nWe conduct the experiments on the FDDB database. We resized all im ages\nto 1 .5 times of the original size, the resulting average photos resolution is ap-\nproximately 640 × 480. We evaluate the ROI mask sparsity, run-time speed 1 of\neach part, and the recall rate when the false alarm number is 10 und er diﬀerent\npre-ﬁlter threshold. We also compare with the standard network w ithout ROI\nconvolution. Non-top K ( K = 3) suppression is adopted in all settings to make\nRCNN network more eﬃciency.\nTable 3 shows the average ROI mask sparsity, testing speed of eac h part,\nand recall rate of each setting. Comparing the second row with the fourth row,\nit proves that we can linearly decrease the computation cost accor ding to the\nmask sparsity. The last two rows show the recall rate and average test time of\ndiﬀerent settings. The original DNN detector can run at 10 FPS on C PU for a\nVGA image. With ROI convolution, it can speed up to 30 FPS on CPU. We c an\nachieve about 3 times speed up with only 0 .6% recall rate drop.\n1 All experiments use a single thread on an Intel i7-4770K CPU\n14 Dong Chen, Gang Hua, Fang Wen, Jian Sun\n0 40 80 120 160 2000.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFalse positive\nTrue positive\nFDDB\n \n Ours\nFaceness\nDP2MFD\nDDFD\nCascadeCNN\nYan et al.\nACF−multiscale\nPico\nHeadHunter\nJoint Cascade\nBoosted Exemplar\nSURF−frontal\nSURF−multiview\nPEP−Adapt\nXZJY\nZhu et al.\nSegui et al.\nKoestinger et al.\nLi et al.\nJain et al.\nSubburaman et al.\nViola−Jones\nMikolajczyk et al.\nKienzle et al.\nPASCAL faces AFW\nFig. 7. Comparison with state-of-the-arts on the FDDB [29], AFW [8] and PASCAL\nfaces [30] datasets.\n\u0001\r \u0000B \u0001\u000e \u0001\r \u0000C \u0001\u000e \u0001\r \u0000D \u0001\u000e\nFig. 8. Qualitative face detection results on (a) FDDB [29], (b) AFW [8], (c) PASCAL\nfaces [30] datasets.\n4.4 Comparing with state-of-the-art\nWe conduct face detection experiments on three benchmark data sets. On the\nFDDB dataset, we compare with all public methods [33, 8, 34, 35, 9, 3 6–40, 35,\n10, 41, 42]. We regress the annotation ellipses with 5 facial points an d ignore\n67 unlabeled faces to make sure all false alarms are true. On the AFW and\nPASCAL faces datasets, we compare with (1) deformable part bas ed methods,\ne.g. structure model [30] and Tree Parts Model (TSM) [8]; (2) cascade -based\nmethods, e.g. Headhunter [4]; (3) commercial system, e.g. face.com, Face++\nand Picasa. We learn a global regression from 5 facial points to face rectangles\nto match the annotation for each dataset, and use toolbox from [4 ] for the\nevaluation. Fig. 8 shows that our method outperforms all previous methods by\na considerable margin.\n5 Conclusion and future work\nIn this paper, we proposed a new Supervised Transformer Networ k for face de-\ntection. The superior performance on three challenge datasets s hows its ability\nto learn the optimal canonical positions to best distinguish face/no n-face pat-\nterns. We also introduced a ROI convolution, which speeds up our de tector 3x\non CPU with little recall drop. Our future work will explore how to enha nce the\nROI convolution so that it does not incur additional drops in recall.\nSupervised Transformer Network for Eﬃcient Face Detection 15\nReferences\n1. Viola, P., Jones, M.: Rapid object detection using a boost ed cascade of simple\nfeatures. In: Computer Vision and Pattern Recognition, 200 1. CVPR 2001. Pro-\nceedings of the 2001 IEEE Computer Society Conference on. Vo lume 1., IEEE\n(2001) 511–518\n2. Li, S.Z., Zhu, L., Zhang, Z., Blake, A., Zhang, H., Shum, H. : Statistical learning of\nmulti-view face detection. In: European Conference on Comp uter Vision. (2002)\n67–81\n3. Wu, B., Ai, H., Huang, C., Lao, S.: Fast rotation invariant multi-view face detection\nbased on real adaboost. In: Automatic Face and Gesture Recog nition. (2004) 79–84\n4. Mathias, M., Benenson, R., Pedersoli, M., Van Gool, L.: Fa ce detection without\nbells and whistles. In: European Conference on Computer Vis ion. (2014) 720–735\n5. Viola, M., Jones, M.J., Viola, P.: Fast multi-view face de tection. In: TR2003-96.\n(2003)\n6. Huang, C., Ai, H., Li, Y., Lao, S.: Vector boosting for rota tion invariant multi-view\nface detection. In: Computer Vision, 2005. ICCV 2005. Tenth IEEE International\nConference on. Volume 1. (Oct 2005) 446–453 Vol. 1\n7. Huang, C., Ai, H., Li, Y., Lao, S.: High-performance rotat ion invariant multiview\nface detection. Pattern Analysis and Machine Intelligence , IEEE Transactions on\n29(4) (April 2007) 671–686\n8. Zhu, X., Ramanan, D.: Face detection, pose estimation, an d landmark localization\nin the wild. In: Computer Vision and Pattern Recognition (CV PR), 2012 IEEE\nConference on, IEEE (2012) 2879–2886\n9. Li, H., Hua, G., Lin, Z., Brandt, J., Yang, J.: Probabilist ic elastic part model for\nunsupervised face detector adaptation. In: The IEEE Intern ational Conference on\nComputer Vision (ICCV). (2013)\n10. Yang, S., Luo, P., Loy, C.C., Tang, X.: From facial parts r esponses to face detection:\nA deep learning approach. In: Proceedings of the IEEE Intern ational Conference\non Computer Vision. (2015) 3676–3684\n11. Dollar, P., Appel, R., Belongie, S., Perona, P.: Fast fea ture pyramids for object\ndetection. Pattern Analysis and Machine Intelligence, IEE E Transactions on 36(8)\n(Aug 2014) 1532–1545\n12. Yang, B., Yan, J., Lei, Z., Li, S.Z.: Convolutional chann el features for pedestrian,\nface and edge detection. CoRR abs/1504.07339 (2015)\n13. Shen, X., Lin, Z., Brandt, J., Wu, Y.: Detecting and align ing faces by image\nretrieval. In: Computer Vision and Pattern Recognition (CV PR), 2013 IEEE Con-\nference on. (June 2013) 3460–3467\n14. Farfade, S.S., Saberian, M.J., Li, L.J.: Multi-view fac e detection using deep con-\nvolutional neural networks. In: Proceedings of the 5th ACM o n International Con-\nference on Multimedia Retrieval. ICMR ’15, New York, NY, USA , ACM (2015)\n643–650\n15. Li, H., Lin, Z., Shen, X., Brandt, J., Hua, G.: A convoluti onal neural network\ncascade for face detection. In: Computer Vision and Pattern Recognition (CVPR),\n2015 IEEE Conference on. (June 2015) 5325–5334\n16. Chen, D., Ren, S., Wei, Y., Cao, X., Sun, J.: Joint cascade face detection and align-\nment. In: Proceedings of the European Conference on Compute r Vision (ECCV).\n(2014)\n17. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich f eature hierarchies for accu-\nrate object detection and semantic segmentation. In: Compu ter Vision and Pattern\nRecognition (CVPR), 2014 IEEE Conference on. (June 2014) 58 0–587\n16 Dong Chen, Gang Hua, Fang Wen, Jian Sun\n18. Bell, S., Zitnick, C.L., Bala, K., Girshick, R.: Inside- outside net: Detecting ob-\njects in context with skip pooling and recurrent neural netw orks. arXiv preprint\narXiv:1512.04143 (2015)\n19. Jaderberg, M., Simonyan, K., Zisserman, A., et al.: Spat ial transformer networks.\nIn: Advances in Neural Information Processing Systems. (20 15) 2008–2016\n20. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towa rds real-time object detec-\ntion with region proposal networks. In: Advances in Neural I nformation Processing\nSystems. (2015) 91–99\n21. Vanhoucke, V., Senior, A., Mao, M.Z.: Improving the spee d of neural networks\non cpus. In: Deep Learning and Unsupervised Feature Learnin g Workshop, NIPS\n2011. (2011)\n22. Liu, B., Wang, M., Foroosh, H., Tappen, M., Penksy, M.: Sp arse convolutional\nneural networks. In: Computer Vision and Pattern Recogniti on (CVPR), 2015\nIEEE Conference on. (June 2015) 806–814\n23. Jaderberg, M., Vedaldi, A., Zisserman, A.: Speeding up c onvolutional neural net-\nworks with low rank expansions. In: Proceedings of the Briti sh Machine Vision\nConference, BMVA Press (2014)\n24. Zhang, X., Zou, J., Ming, X., He, K., Sun, J.: Eﬃcient and a ccurate approximations\nof nonlinear convolutional networks. In: Computer Vision a nd Pattern Recognition\n(CVPR), 2015 IEEE Conference on. (June 2015) 1984–1992\n25. Zhang, C., Zhang, Z.: Improving multiview face detectio n with multi-task deep\nconvolutional neural networks. In: Applications of Comput er Vision (WACV), 2014\nIEEE Winter Conference on. (March 2014) 1036–1041\n26. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J ., Girshick, R., Guadar-\nrama, S., Darrell, T.: Caﬀe: Convolutional architecture fo r fast feature embedding.\nIn: Proceedings of the ACM International Conference on Mult imedia, ACM (2014)\n675–678\n27. Ozuysal, M., Fua, P., Lepetit, V.: Fast keypoint recogni tion in ten lines of code.\nIn: Computer Vision and Pattern Recognition, 2007. CVPR’07 . IEEE Conference\non, Ieee (2007) 1–8\n28. Chellapilla, K., Puri, S., Simard, P.: High performance convolutional neural net-\nworks for document processing. In: Tenth International Wor kshop on Frontiers in\nHandwriting Recognition, Suvisoft (2006)\n29. Jain, V., Learned-Miller, E.: Fddb: A benchmark for face detection in uncon-\nstrained settings. Technical Report UM-CS-2010-009, Univ ersity of Massachusetts,\nAmherst (2010)\n30. Yan, J., Zhang, X., Lei, Z., Li, S.Z.: Face detection by st ructural models. Image\nand Vision Computing 32(10) (2014) 790–799\n31. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ ar, P.,\nZitnick, C.L.: Microsoft coco: Common objects in context. I n: Computer Vision–\nECCV 2014. Springer (2014) 740–755\n32. Schroﬀ, F., Kalenichenko, D., Philbin, J.: Facenet: A un iﬁed embedding for face\nrecognition and clustering. In: Proceedings of the IEEE Con ference on Computer\nVision and Pattern Recognition. (2015) 815–823\n33. Wu, B., Ai, H., Huang, C., Lao, S.: Fast rotation invarian t multi-view face detec-\ntion based on real adaboost. In: Automatic Face and Gesture R ecognition, 2004.\nProceedings. Sixth IEEE International Conference on. (May 2004) 79–84\n34. Shen, X., Lin, Z., Brandt, J., Wu, Y.: Detecting and align ing faces by image\nretrieval. In: Computer Vision and Pattern Recognition (CV PR), 2013 IEEE Con-\nference on. (June 2013) 3460–3467\nSupervised Transformer Network for Eﬃcient Face Detection 17\n35. Li, H., Lin, Z., Brandt, J., Shen, X., Hua, G.: Eﬃcient boo sted exemplar-based\nface detection. In: Computer Vision and Pattern Recognitio n (CVPR), 2014 IEEE\nConference on. (June 2014) 1843–1850\n36. Li, J., Zhang, Y.: Learning surf cascade for fast and accu rate object detection. In:\nComputer Vision and Pattern Recognition (CVPR), 2013 IEEE C onference on.\n(June 2013) 3468–3475\n37. Jain, V., Learned-Miller, E.: Online domain adaptation of a pre-trained cascade\nof classiﬁers. In: Computer Vision and Pattern Recognition (CVPR), 2011 IEEE\nConference on, IEEE (2011) 577–584\n38. Subburaman, V.B., Marcel, S.: Fast bounding box estimat ion based face detection.\nIn: ECCV, Workshop on Face Detection: Where we are, and what n ext? (2010)\n39. Mikolajczyk, K., Schmid, C., Zisserman, A.: Human detec tion based on a prob-\nabilistic assembly of robust part detectors. In: Computer V ision-ECCV 2004.\nSpringer (2004) 69–82\n40. Yan, J., Lei, Z., Wen, L., Li, S.: The fastest deformable p art model for object\ndetection. In: Computer Vision and Pattern Recognition (CV PR), 2014 IEEE\nConference on. (June 2014) 2497–2504\n41. Ranjan, R., Patel, V.M., Chellappa, R.: A deep pyramid de formable part model\nfor face detection. In: Biometrics Theory, Applications an d Systems (BTAS), 2015\nIEEE 7th International Conference on, IEEE (2015) 1–8\n42. Farfade, S.S., Saberian, M.J., Li, L.J.: Multi-view fac e detection using deep con-\nvolutional neural networks. In: Proceedings of the 5th ACM o n International\nConference on Multimedia Retrieval, ACM (2015) 643–650",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7577126026153564
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7184167504310608
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6675006151199341
    },
    {
      "name": "Boosting (machine learning)",
      "score": 0.5812019109725952
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5616142153739929
    },
    {
      "name": "Transformer",
      "score": 0.5461283326148987
    },
    {
      "name": "Face detection",
      "score": 0.5414716005325317
    },
    {
      "name": "Cascade",
      "score": 0.5248477458953857
    },
    {
      "name": "Face (sociological concept)",
      "score": 0.4486718475818634
    },
    {
      "name": "Interpretability",
      "score": 0.43530574440956116
    },
    {
      "name": "Video Graphics Array",
      "score": 0.4261210262775421
    },
    {
      "name": "Preprocessor",
      "score": 0.4213871955871582
    },
    {
      "name": "Computer vision",
      "score": 0.3717792332172394
    },
    {
      "name": "Facial recognition system",
      "score": 0.3593694567680359
    },
    {
      "name": "Software",
      "score": 0.06733021140098572
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 18
}