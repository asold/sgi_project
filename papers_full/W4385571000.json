{
  "title": "Numeric Magnitude Comparison Effects in Large Language Models",
  "url": "https://openalex.org/W4385571000",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2143724091",
      "name": "Raj Shah",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3012034874",
      "name": "Vijay Marupudi",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5091983129",
      "name": "Reba Koenen",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4366302145",
      "name": "Khushi Bhardwaj",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2196079146",
      "name": "Sashank Varma",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2032895136",
    "https://openalex.org/W3163028255",
    "https://openalex.org/W2111540192",
    "https://openalex.org/W2033565928",
    "https://openalex.org/W2014481741",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2962817854",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W2478527802",
    "https://openalex.org/W3183248212",
    "https://openalex.org/W2319824271",
    "https://openalex.org/W2950645060",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W2106119216",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W2114066201",
    "https://openalex.org/W2132200584",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2065990977",
    "https://openalex.org/W2797426066",
    "https://openalex.org/W3081886688",
    "https://openalex.org/W2161463579",
    "https://openalex.org/W3134642945",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W4307429315",
    "https://openalex.org/W2010598940",
    "https://openalex.org/W2555527003",
    "https://openalex.org/W2071347285",
    "https://openalex.org/W2067998532",
    "https://openalex.org/W1970258217",
    "https://openalex.org/W2001569942",
    "https://openalex.org/W2053138397"
  ],
  "abstract": "Large Language Models (LLMs) do not differentially represent numbers, which are pervasive in text. In contrast, neuroscience research has identified distinct neural representations for numbers and words. In this work, we investigate how well popular LLMs capture the magnitudes of numbers (e.g., that 4<5) from a behavioral lens. Prior research on the representational capabilities of LLMs evaluates whether they show human-level performance, for instance, high overall accuracy on standard benchmarks. Here, we ask a different question, one inspired by cognitive science: How closely do the number representations of LLMscorrespond to those of human language users, who typically demonstrate the distance, size, and ratio effects? We depend on a linking hypothesis to map the similarities among the model embeddings of number words and digits to human response times. The results reveal surprisingly human-like representations across language models of different architectures, despite the absence of the neural circuitry that directly supports these representations in the human brain. This research shows the utility of understanding LLMs using behavioral benchmarks and points the way to future work on the number of representations of LLMs and their cognitive plausibility.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 6147–6161\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nNumeric Magnitude Comparison Effects in Large Language Models\nRaj Sanjay Shah, Vijay Marupudi, Reba Koenen, Khushi Bhardwaj, Sashank Varma\nGeorgia Institute of Technology\n{rajsanjayshah, vijaymarupudi, rkoenen3, khushi.bhardwaj, varma}@gatech.edu\nAbstract\nLarge Language Models (LLMs) do not dif-\nferentially represent numbers, which are per-\nvasive in text. In contrast, neuroscience re-\nsearch has identiﬁed distinct neural represen-\ntations for numbers and words. In this work,\nwe investigate how well popular LLMs cap-\nture the magnitudes of numbers (e.g., that 4 <\n5) from a behavioral lens. Prior research on\nthe representational capabilities of LLMs eval-\nuates whether they show human-level perfor-\nmance, for instance, high overall accuracy on\nstandard benchmarks. Here, we ask a differ-\nent question, one inspired by cognitive science:\nHow closely do the number representations of\nLLMs correspond to those of human language\nusers, who typically demonstrate the distance,\nsize, and ratio effects? We depend on a linking\nhypothesis to map the similarities among the\nmodel embeddings of number words and dig-\nits to human response times. The results reveal\nsurprisingly human-like representations across\nlanguage models of different architectures, de-\nspite the absence of the neural circuitry that di-\nrectly supports these representations in the hu-\nman brain. This research shows the utility of\nunderstanding LLMs using behavioral bench-\nmarks and points the way to future work on the\nnumber of representations of LLMs and their\ncognitive plausibility.\n1 Introduction\nHumans use symbols – number words such as\n“three” and digits such as “3” – to quantify the\nworld. How humans understand these symbols has\nbeen the subject of cognitive science research for\nhalf a century. The dominant theory is that people\nunderstand number symbols by mapping them to\nmental representations, speciﬁcally magnitude rep-\nresentations (Moyer and Landauer, 1967). This is\ntrue for both number words (e.g., “three”) and dig-\nits (e.g., “3”). These magnitude representations are\norganized as a “mental number line” (MNL), with\nnumbers mapped to points on the line as shown in\nFigure 1d. Cognitive science research has revealed\nthat this representation is present in the minds of\nyoung children (Ansari et al., 2005) and even non-\nhuman primates (Nieder and Miller, 2003). Most\nof this research has been conducted with numbers\nin the range 1-9, in part, because corpus studies\nhave shown that 0 belongs to a different distribu-\ntion (Dehaene and Mehler, 1992) and, in part, be-\ncause larger numbers require parsing place-value\nnotation (Nuerk et al., 2001), a cognitive process\nbeyond the scope of the current study.\nEvidence for this proposal comes from magni-\ntude comparison tasks in which people are asked\nto compare two numbers (e.g., 3 vs. 7) and judge\nwhich one is greater (or lesser). Humans have con-\nsistently exhibited three effects that suggest recruit-\nment of magnitude representations to understand\nnumbers: the distance effect, the size effect, and the\nratio effect (Moyer and Landauer, 1967; Merkley\nand Ansari, 2010). We review the experimental evi-\ndence for these effects, shown in Figure 1, in LLMs.\nOur behavioral benchmarkingapproach shifts the\nfocus from what abilities LLMs have in an absolute\nsense to whether they successfully mimic human\nperformance characteristics. This approach can\nhelp differentiate between human tendencies cap-\ntured by models and the model behaviors due to\ntraining strategies. Thus, the current study bridges\nbetween Natural Language Processing (NLP), com-\nputational linguistics, and cognitive science.\n1.1 Effects of Magnitude Representations\nPhysical quantities in the world, such as the bright-\nness of a light or the loudness of a sound, are en-\ncoded as logarithmically scaled magnitude repre-\nsentations (Fechner, 1860). Research conducted\nwith human participants and non-human species\nhas revealed that they recruit many of the same\nbrain regions, such as the intra-parietal sulcus, to\ndetermine the magnitude of symbolic numbers (Bil-\nlock and Tsou, 2011; Nieder and Dehaene, 2009).\n6147\nFigure 1: The input types, LLMs, and effects in this study. The three effects are depicted in an abstract manner in\nsub-ﬁgures (a), (b), (c).\nThree primary magnitude representation effects\nhave been found using the numerical comparison\ntask in studies of humans. First, comparisons show\na distance effect: The greater the distance |x −y|\nbetween the numbers x vs. y, the faster the com-\nparison (Moyer and Landauer, 1967). Thus, peo-\nple compare 1 vs. 9 faster than 1 vs. 2. This is\nshown in abstract form in Figure 1a. This effect\ncan be explained by positing that people possess an\nMNL. When comparing two numbers, they ﬁrst lo-\ncate each number on this representation, determine\nwhich one is “to the right”, and choose that number\nas the greater one. Thus, the farther the distance\nbetween the two points, the easier (and thus faster)\nthe judgment.\nSecond, comparisons show a size effect: Given\ntwo comparisons of the same distance (i.e., of the\nsame value for |x −y|), the smaller the numbers,\nthe faster the comparison (Parkman, 1971). For\nexample, 1 vs. 2 and 8 vs. 9 both have the same\ndistance (i.e., |x −y|= 1), but the former involves\nsmaller numbers and is therefore the easier (i.e.,\nfaster) judgment. The size effect is depicted in ab-\nstract form in Figure 1b. This effect also references\nthe MNL, but a modiﬁed version where the points\nare logarithmically compressed, i.e., the distance\nfrom 1 to x is proportional to log(x); see Figure\n1d. To investigate if a logarithmically compressed\nnumber line is also present in LLMs, we use mul-\ntidimensional scaling (Ding, 2018) on the cosine\ndistances between number embeddings.\nThird, comparisons show a ratio effect: The\ntime to compare two numbers x vs. y is a decreas-\ning function of the ratio of the larger number over\nthe smaller number, i.e., max(x,y)\nmin(x,y) (Halberda et al.,\n2008). This function is nonlinear, as depicted in\nabstract form in Figure 1c. Here, we assume that\nthis function is a negative exponential, though other\nfunctional forms have been proposed in the cogni-\ntive science literature. The ratio effect can also be\nexplained by the logarithmically compressed MNL\ndepicted in Figure 1d.\nThese three effects — distance, size, and ratio —\nhave been replicated numerous times in studies of\nhuman adults and children, non-human primates,\nand many other species (Cantlon, 2012; Cohen Ka-\ndosh et al., 2008). The MNL model in Figure 1d\naccounts for these effects (and many others in the\nmathematical cognition literature). Here, we use\nLLMs to evaluate a novel scientiﬁc hypothesis: that\nthe MNL representation of the human mind is latent\nin the statistical structure of the linguistic environ-\nment, and thus learnable. Therefore, there is less\nneed to posit pre-programmed neural circuitry to\nexplain magnitude effects.\n1.2 LLMs and Behavioral Benchmarks\nModern NLP models are pre-trained on large\ncorpora of texts from diverse sources such as\nWikipedia (Wikipedia contributors, 2004) and the\nopen book corpus (Zhu et al., 2015). LLMs like\nBERT (Devlin et al., 2018), ROBERTA (Liu et al.,\n2019) and GPT-2 (Radford et al., 2019) learn con-\ntextual semantic vector representations of words.\n6148\nThese models have achieved remarkable success\non NLP benchmarks (Wang et al., 2018). They\ncan perform as well as humans on a number of\nlanguage tests such as semantic veriﬁcation (Bha-\ntia and Richie, 2022) and semantic disambiguation\n(Lake and Murphy, 2021).\nMost benchmarks are designed to measure the\nabsolute performance of LLMs, with higher accu-\nracy signaling “better” models. Human or superhu-\nman performance is marked by exceeding certain\nthresholds. Here, we ask not whether LLMs can\nperform well or even exceed human performance at\ntasks, but whether they show the sameperformance\ncharacteristics as humans while accomplishing the\nsame tasks. We call these behavioral benchmarks.\nThe notion of behavioral benchmarks requires mov-\ning beyond accuracy (e.g., scores) as the dominant\nmeasure of LLM performance.\nAs a test case, we look at the distance, size, and\nratio effects as behavioral benchmarks to determine\nwhether LLMs understand numbers as humans do,\nusing magnitude representations. This requires a\nlinking hypothesisto map measures of human per-\nformance to indices of model performance. Here,\nwe map human response times on numerical com-\nparison tasks to similarity computations on number\nword embeddings.\n1.3 Research Questions\nThe current study investigates the number repre-\nsentations of LLMs and their alignment with the\nhuman MNL. It addresses ﬁve research questions:\n1. Which LLMs, if any, capture the distance,\nsize, and ratio effects exhibited by humans?\n2. How do different layers of LLMs vary in ex-\nhibiting these effects?\n3. How do model behaviors change when using\nlarger variants (more parameters) of the same\narchitecture?\n4. Do the models show implicit numeration\n(\"four\" = \"4\"), i.e., do they exhibit these ef-\nfects equally for all number symbol types or\nmore for some types (e.g., digits) than others\n(e.g., number words)?\n5. Is the MNL representation depicted in Figure\n1d latent in the representations of the models?\n2 Related Work\nResearch on the numerical abilities of LLMs fo-\ncuses on several aspects of mathematical reasoning\n(Thawani et al., 2021), such as magnitude com-\nparison, numeration (Naik et al., 2019; Wallace\net al., 2019), arithmetic word problems (Burns\net al., 2021; Amini et al., 2019), exact facts (Lin\net al., 2020), and measurement estimation (Zhang\net al., 2020). The goal is to improve performance\non application-driven tasks that require numerical\nskills. Research in this area typically attempts to\n(1) understand the numerical capabilities of pre-\ntrained models and (2) propose new architectures\nthat improve numerical cognition abilities (Geva\net al., 2020; Dua et al., 2019).\nOur work also focuses on the ﬁrst research di-\nrection: probing the numerical capabilities of pre-\ntrained models. Prior research by Wallace et al.\n(2019) judges the numerical reasoning of various\ncontextual and non-contextual models using dif-\nferent tests (e.g., ﬁnding the maximum number in\na list, ﬁnding the sum of two numbers from their\nword embeddings, decoding the original number\nfrom its embedding). These tasks have been pre-\nsented as evaluation criteria for understanding the\nnumerical capabilities of models. Spithourakis and\nRiedel (2018) change model architectures to treat\nnumbers as distinct from words. Using perplexity\nscore as a proxy for numerical abilities, they argue\nthat this ability reduces model perplexity in neu-\nral machine translation tasks. Other work focuses\non ﬁnding numerical capabilities through building\nQA benchmarks for performing discrete reasoning\n(Dua et al., 2019). Most research in this direction\ncasts different tasks as proxies of numerical abili-\nties of NLP systems (Weiss et al., 2018; Dua et al.,\n2019; Spithourakis and Riedel, 2018; Wallace et al.,\n2019; Burns et al., 2021; Amini et al., 2019).\nAn alternative approach by Naik et al. (2019)\ntests multiple non-contextual task-agnostic embed-\nding generation techniques to identify the failures\nin models’ abilities to capture the magnitude and\nnumeration effects of numbers. Using a systematic\nfoundation in cognitive science research, we build\nupon their work in two ways: we (1) use contextual\nembeddings spanning a wide variety of pre-training\nstrategies, and (2) evaluate models by comparing\ntheir behavior to humans. Our work looks at num-\nbers in an abstract sense, and is relevant for the\ngrounding problem studied in artiﬁcial intelligence\nand cognitive science (Harnad, 2023).\n3 Experimental Design\nThe literature lacks adequate experimental studies\ndemonstrating magnitude representations of num-\n6149\nModel Category SizeBase LargeBERT (Devlin et al., 2018)Encoder 110M 340MRoBERTA (Liu et al., 2019)Encoder 125M 355MXLNET (Yang et al., 2019)Auto-regressive Encoder 110M 340MGPT-2 (Radford et al., 2019)Auto-regressive Decoder 117M 345MT5 (Raffel et al., 2019)Encoder 110M 335MBART (Lewis et al., 2020)Encoder-Decoder 140M 406M\nTable 1: Popular Language Models\nbers in LLMs from a cognitive science perspective.\nThe current study addresses this gap. We propose a\ngeneral methodology for mapping human response\ntimes to similarities computed over LLM embed-\ndings. We test for the three primary magnitude\nrepresentation effects described in section 1.1.\n3.1 Linking Hypothesis\nIn studies with human participants, the distance,\nsize, and ratio effects are measured using reaction\ntime. Each effect depends on the assumption that\nwhen comparing which of two numbers x and y\nis relatively easy, humans are relatively fast, and\nwhen it is relatively difﬁcult, they are relatively\nslow. The ease or difﬁculty of the comparison is a\nfunction of x and y: |x −y|for the distance effect,\nmin(x, y) for the size effect, and max(x,y)\nmin(x,y) for the\nratio effect. LLMs do not naturally make reaction\ntime predictions. Thus, we require a linking hy-\npothesis to estimate the relative ease or difﬁculty of\ncomparisons for LLMs. Here we adopt the simple\nassumption that the greater the similarity of two\nnumber representations in an LLM, the longer it\ntakes to discriminate them, i.e., to judge which one\nis greater (or lesser).\nWe calculate thesimilarity of two numbers based\non the similarity of their vector representations.\nSpeciﬁcally, the representation of a number for a\ngiven layer of a given model is the vector of acti-\nvation across its units. There are many similarity\nmetrics for vector representations (Wang and Dong,\n2020): Manhattan, Euclidean, cosine, dot product,\netc. Here, we choose a standard metric in distribu-\ntional semantics: the cosine of the angle between\nthe vectors (Richie and Bhatia, 2021). This reason-\ning connects an index of model function (i.e., the\nsimilarity of the vector representations of two num-\nbers) to a human behavioral measure (i.e., reaction\ntime). Thus, the more similar the two representa-\ntions are, the less discriminable they are from each\nother, and thus the longer the reaction time to select\none over the other.\n3.2 Materials\nFor these experiments, we utilized three formats\nfor number representations in LLMs: lowercase\nnumber words, mixed-cased number words (i.e.,\nthe ﬁrst letter is capitalized), and digits. These for-\nmats enable us to explore variations in input tokens\nand understand numeration in models. Below are\nexamples of the three input types:\n• \"one\", \"two\", \"three\", \"four\" ... \"nine\"\n• \"One\", \"Two\", \"Three\", \"Four\" ... \"Nine\"\n• \"1\", \"2\", \"3\", \"4\" ... \"9\"\nAs noted in the Introduction, prior studies of the dis-\ntance, size and ratio effects in humans have largely\nfocused on numbers ranging from 1 to 9. Our input\ntypes are not-affected by tokenization methods as\nthe models under consideration have each input as\na separate token.\n3.3 Large Language Models - Design Choices\nModern NLP models are pre-trained on a large\namount of unlabeled textual data from a diverse set\nof sources. This enables LLMs to learn contextu-\nally semantic vector representations of words. We\nexperiment on these vectors to evaluate how one\nspeciﬁc dimension of human knowledge - number\nsense - is captured in different model architectures.\nWe use popular large language models from Hug-\ngingface’s Transformers library (Wolf et al., 2020)\nto obtain vector representations of numbers in dif-\nferent formats. Following the work by Min et al.\n(2021) to determine popular model architectures,\nwe select models from three classes of architectural\ndesign: encoder models (e.g., BERT (Devlin et al.,\n2018)), auto-regressive models (e.g., GPT-2 (Rad-\nford et al., 2019)), and encoder-decoder models\n(e.g., T5 (Raffel et al., 2019)). The ﬁnal list of\nmodels is provided in Table 1.\nOperationalization: We investigate the three\nnumber magnitude effects as captured in the repre-\nsentations of each layer of the six models for the\nthree number formats. For these experiments, we\nconsider only the obtained hidden layer outputs for\nthe tokens corresponding to the input number word\ntokens. We ignore the special preﬁx and sufﬁx to-\nkens of models (e.g., the [cls] token in BERT) for\nuniformity among different architectures. For the\nT5-base model, we use only the encoder to obtain\nmodel embedding. All models tested use a simi-\nlar number of model parameters (around 110-140\nmillion parameters). For our studies, we arbitrarily\nchoose the more popular BERT uncased variant as\n6150\nopposed to the cased version. We compare the two\nmodels in Appendix section A.2 for a complete\nanalysis, showing similar behaviors in the variants.\nModel size variations for the same architecture are\nconsidered in the Appendix section A.1 to show\nthe impact of model size on the three effects.\n4 Magnitude Representation Effects in\nLLMs\n4.1 The Distance Effect\nLayer T5 BART RoB XLNETBERTGPT-2 Avg.\n1 0.974 0.965 0.954 0.967 0.979 0.937 0.963\n2 0.984 0.959 0.959 0.951 0.983 0.940 0.963\n3 0.973 0.957 0.961 0.960 0.955 0.937 0.957\n4 0.956 0.964 0.977 0.962 0.956 0.923 0.957\n5 0.941 0.951 0.976 0.948 0.982 0.931 0.955\n6 0.972 0.916 0.966 0.942 0.991 0.932 0.953\n7 0.967 0.960 0.967 0.943 0.990 0.930 0.959\n8 0.945 0.969 0.954 0.923 0.977 0.931 0.950\n9 0.950 0.978 0.945 0.920 0.967 0.929 0.948\n10 0.933 0.958 0.928 0.926 0.923 0.931 0.933\n11 0.924 0.975 0.968 0.951 0.926 0.930 0.946\n12 0.920 0.956 0.854 0.934 0.890 0.931 0.914\nTable 2: Distance Effect: Averaged (across the three\nnumber formats) R2 values of different LLMs for dif-\nferent layers when ﬁtting a linear function. RoB:\nRoberta-base model, BERT: uncased variant.\nLLMs\\Input LC MC Digits Avg.\nT5 0.986 0.937 0.936 0.953\nBART 0.942 0.951 0.983 0.959\nRoBERTa 0.945 0.943 0.964 0.951\nXLNET 0.888 0.965 0.979 0.944\nBERT (uncased) 0.976 0.944 0.960\nGPT-2 0.906 0.904 0.986 0.932\nTotal Averages\nacross models 0.941 0.946 0.965 0.950\nTable 3: Distance Effect: Averaged (across layers) R2\nvalues of different LLMs on the three numbers when\nﬁtting a linear function. LC: Lowercase number words,\nMC: Mixed-case number words.\nRecall that the distance effect is that people are\nslower (i.e., ﬁnd it more difﬁcult) to compare num-\nbers the closer they are to each other on the MNL.\nWe use the pipeline depicted in Figure 1 to inves-\ntigate if LLM representations are more similar to\neach other if the numbers are closer on the MNL.\nEvaluation of the distance effect in LLMs is done\nby ﬁtting a straight line (a + bx) on the cosine sim-\nilarity vs. distance plot. We ﬁrst perform two oper-\nations on these cosine similarities: (1) We average\nthe similarities across each distance (e.g., the point\nat distance 1 on the x-axis represents the average\nsimilarity of 1 vs. 2, 2 vs. 3, ..., 8 vs. 9). (2)\nWe normalize the similarities to be in the range [0,\n1]. These decisions allow relative output compar-\nisons across different model architectures, which\nis not possible using the raw cosine similarities of\neach LLM. To illustrate model performance, the\ndistance effects for the best-performing layer in\nterms of R2 values for BART are shown in Figure\n2 for the three number formats. The highR2 values\nindicate a human-like distance effect.\nLayer T5 BART RoB XLNETBERTGPT-2 Avg.\n1 0.756 0.651 0.494 0.602 0.617 0.466 0.597\n2 0.685 0.637 0.507 0.551 0.783 0.653 0.636\n3 0.744 0.697 0.503 0.492 0.834 0.574 0.641\n4 0.726 0.677 0.519 0.493 0.871 0.478 0.627\n5 0.665 0.685 0.610 0.54 0.783 0.528 0.635\n6 0.670 0.692 0.586 0.563 0.757 0.539 0.635\n7 0.701 0.634 0.613 0.585 0.823 0.539 0.649\n8 0.705 0.687 0.567 0.591 0.870 0.532 0.659\n9 0.697 0.757 0.581 0.566 0.877 0.541 0.670\n10 0.727 0.694 0.622 0.555 0.905 0.533 0.672\n11 0.729 0.756 0.734 0.602 0.911 0.547 0.713\n12 0.703 0.702 0.744 0.662 0.889 0.550 0.708\nTable 4: Size Effect: Averaged (across inputs) R2 val-\nues of different LLMs on different input layers when\nﬁtting a linear function. RoB: Roberta-base model,\nBERT: uncased variant.\nAll of the models show strong distance effects\nfor all layers, as shown in Table 2, and for all num-\nber formats, as shown in Table 3. Interestingly,\nLLMs are less likely to reveal the distance effect\nas layer count increases (Table 2). For example,\nlayer one results in the strongest distance effect\nwhile layer twelve is the least representative of the\ndistance effect. With respect to number format,\npassing digits as inputs tended to produce stronger\ndistance effects than passing number words (Table\n3); this pattern was present for four of the six LLMs\n(i.e., all but T5 and BERT).\n4.2 The Size Effect\nThe size effect holds for comparisons of the same\ndistance (e.g., for a distance of 1, these include 1\nvs. 2, 2 vs. 3, ..., 8 vs. 9). Among these compar-\nisons, those involving larger numbers (e.g., 8 vs.\n9) are made more slowly (i.e., people ﬁnd them\nmore difﬁcult) than those involving smaller num-\nbers (e.g., 1 vs. 2). That larger numbers are harder\nto differentiate than smaller numbers aligns with\nthe logarithmically compressed MNL depicted in\n6151\nFigure 2: Distance effect for the best-performing layer (9th layer) for the BART model\nFigure 1d. This study evaluates whether a given\nLLM shows a size effect on a given layer for num-\nbers of a given format by plotting the normalized\ncosine similarities against the size of the compari-\nson, deﬁned as the minimum of the two numbers\nbeing compared. For each minimum value (points\non the x-axis), we average the similarities for all\ncomparisons to form a single point (vertical com-\npression). We then ﬁt a straight line (ax + b) over\nthe vertically compressed averages (blue line in\nFigure 3) to obtain the R2 values (scores). To il-\nlustrate model performance, the size effects for the\nbest-performing layer of the BERT-uncased model\n(in terms of R2 values) are shown in Figure 3. Sim-\nilar to the results for the distance effect, the high\nR2 values indicate a human-like size effect.\nInterestingly, Table 4 generally shows an increas-\ning trend in the layer-wise capability of capturing\nthe size effect across the six LLMs. This is opposite\nto the trend observed across layers for the distance\neffect. Table 5 shows that using digits as the input\nvalues yields signiﬁcantly better R2 values than the\nother number formats. In fact, this is the only num-\nber format for which the models produce strong\nsize effects. However, the vertical compression of\npoints fails to capture the spread of points across\nthe y-axis for each point on the x-axis. This spread,\na limitation of the size effect analysis, is captured\nin the ratio effect (section 4.3).\nLLMs\\Input LC MC Digits Avg.\nT5 0.702 0.539 0.886 0.709\nBART 0.614 0.568 0.885 0.689\nRoBERTa 0.520 0.466 0.783 0.59\nXLNET 0.500 0.408 0.793 0.567\nBERT (uncased) 0.803 0.851 0.827\nGPT-2 0.434 0.332 0.853 0.54\nTotal Averages\nacross models 0.596 0.519 0.842 0.654\nTable 5: Size Effect: Averaged (across layers) R2 val-\nues of different LLMs on the three number formats\nwhen ﬁtting a linear function. LC: Lowercase number\nwords, MC: Mixed-case number words.\n4.3 The Ratio Effect\nLLMs\\Input LC MC Digits Avg.\nT5 0.852 0.756 0.868 0.826\nBART 0.786 0.833 0.897 0.838\nRoBERTa 0.714 0.747 0.746 0.736\nXLNET 0.729 0.761 0.901 0.797\nBERT (uncased) 0.906 0.757 0.831\nGPT-2 0.686 0.758 0.681 0.709\nTotal Averages\nacross models 0.779 0.793 0.808 0.789\nTable 6: Ratio Effect: Averaged (across layers) R2\nvalues of different LLMs on different number formats\nwhen ﬁtting a negative exponential function. LC: Low-\nercase number words, MC: Mixed-case number words.\nLayer T5 BART RoB XLNETBERTGPT-2 Avg.\n1 0.850 0.820 0.756 0.868 0.837 0.735 0.811\n2 0.865 0.837 0.745 0.828 0.878 0.755 0.819\n3 0.846 0.861 0.725 0.820 0.853 0.738 0.807\n4 0.847 0.859 0.739 0.822 0.820 0.659 0.791\n5 0.851 0.847 0.805 0.825 0.847 0.695 0.812\n6 0.880 0.821 0.800 0.816 0.883 0.703 0.817\n7 0.867 0.811 0.795 0.810 0.883 0.698 0.811\n8 0.824 0.849 0.780 0.780 0.880 0.702 0.803\n9 0.806 0.852 0.780 0.746 0.861 0.705 0.791\n10 0.785 0.821 0.720 0.754 0.779 0.704 0.760\n11 0.755 0.849 0.666 0.781 0.769 0.702 0.754\n12 0.731 0.834 0.516 0.717 0.687 0.708 0.699\nTable 7: Ratio Effect: Averaged (across number for-\nmats) R2 values of different LLMs on different in-\nput layers when ﬁtting a negative exponential function.\nRoB: Roberta-base model, BERT: uncased variant.\nThe ratio effect in humans can be thought of as\nsimultaneously capturing both the distance and size\neffects. Behaviorally, the time to compare x vs. y\nis a decreasing function of the ratio of the larger\nnumber over the smaller number, i.e., of max(x,y)\nmin(x,y) .\nIn fact, the function is nonlinear as depicted in\nFigure 1c. For the LLMs, we plot the normalized\ncosine similarity vs. max(x,y)\nmin(x,y) . To each plot, we ﬁt\nthe negative exponential function a ∗e−bx + c and\n6152\n2 4 6 8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nR\n2\n= 0.936\nNumber words\ndistance = 1\ndistance = 2\ndistance = 3\ndistance = 4\ndistance = 5\ndistance = 6\ndistance = 7\ndistance = 8\nF itted Line\n2 4 6 8\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nR\n2\n= 0.886\nDigits\ndistance = 1\ndistance = 2\ndistance = 3\ndistance = 4\ndistance = 5\ndistance = 6\ndistance = 7\ndistance = 8\nF itted Line\nminimum of the two numbers com ared\nNormalized Cosine Similarity\nFigure 3: Size effect for the best-performing layer for the BERT model (layer 11).\n1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nR\n2\n= 0.827\nLowercase  number w rds\n1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nR\n2\n= 0.874\nMixedcase number w rds\n1 2 3 4 5 6 7 8 9\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nR\n2\n= 0.882\nDigits\nmax  f the tw  numbers c mpared/min  f the tw  numbers c mpared\nN rmalized C sine Similarity\nFigure 4: Ratio effect for the best-performing layer for the BART model (layer 3).\nevaluate the resulting R2. To illustrate model per-\nformance, Figure 4 shows the ratio effects for the\nbest-ﬁtting layer of the BART model for the three\nnumber formats. As observed with the distance\nand size effect, the high R2 values of the LLMs\nindicate a human-like ratio effect in the models.\nLLMs\\Input LC MC Digits Avg.\nT5 0.489 0.526 0.410 0.475\nBART 0.676 0.714 0.678 0.690\nRoBERTa 0.520 0.597 0.587 0.568\nXLNET 0.622 0.620 0.622 0.621\nBERT (uncased) 0.312 0.423 0.368\nGPT-2 0.566 0.513 0.828 0.636\nTotal Averages\nacross models 0.531 0.547 0.591 0.560\nTable 8: Averaged (across layers) correlations when\ncomparing MDS values with Log101 to Log109 for\ndifferent LLMs. LC: Lowercase number words, MC:\nMixed-case number words.\n4.4 Multidimensional Scaling\nAlong with the three magnitude effects, we also\ninvestigate whether the number representations of\nLLMs are consistent with the human MNL. To do\nso, we utilize multidimensional scaling (Borg and\nGroenen, 2005; Ding, 2018). MDS offers a method\nfor recovering the latent structure in the matrix\nof cosine (dis)similarities between the vector rep-\nresentations of all pairs of numbers (for a given\nLLM, layer, and number format). It arranges each\nnumber in a space of N dimensions such that the\nLayer T5 BART RoB XLNETBERTGPT-2 Avg.\n1 0.686 0.679 0.602 0.595 0.739 0.526 0.638\n2 0.271 0.693 0.763 0.734 0.704 0.669 0.639\n3 0.374 0.657 0.772 0.704 0.456 0.685 0.608\n4 0.385 0.728 0.489 0.621 0.425 0.663 0.552\n5 0.476 0.733 0.597 0.707 0.448 0.615 0.596\n6 0.540 0.739 0.571 0.598 0.465 0.608 0.587\n7 0.687 0.696 0.250 0.677 0.445 0.665 0.570\n8 0.529 0.624 0.594 0.591 0.189 0.624 0.525\n9 0.544 0.718 0.691 0.566 0.400 0.671 0.598\n10 0.502 0.624 0.697 0.563 0.394 0.613 0.566\n11 0.195 0.708 0.602 0.543 -0.013 0.675 0.451\n12 0.509 0.677 0.186 0.557 -0.239 0.615 0.384\nTable 9: Averaged (across inputs) correlations of differ-\nent LLMs on different model layers when comparing\nMDS values with Log101 to Log109. RoB: Roberta-\nbase model, BERT: uncased variant.\nNumberT5 BART RoB XLNETBERTGPT-2Avg.\n1 0.01 0.00 0.02 0.00 0.02 0.00 0.01\n2 0.10 0.17 0.15 0.17 0.09 0.12 0.13\n3 0.07 0.05 0.07 0.10 0.06 0.10 0.07\n4 0.05 0.04 0.05 0.05 0.03 0.05 0.04\n5 0.17 0.09 0.07 0.05 0.20 0.05 0.11\n6 0.02 0.04 0.08 0.02 0.06 0.04 0.04\n7 0.09 0.08 0.11 0.04 0.20 0.06 0.10\n8 0.04 0.01 0.08 0.01 0.09 0.05 0.05\n9 0.40 0.08 0.17 0.18 0.44 0.17 0.24\nTable 10: Residual analysis on MDS outputs in 1 di-\nmension on the base variants of the model. RoB:\nRoberta-base model, BERT: uncased variant.\ndistance between each pair of points is consistent\nwith the cosine dissimilarity between their vector\n6153\nrepresentations.\nWe ﬁx N = 1to recover the latent MNL repre-\nsentation for each LLM, layer, and number format.\nFor each solution, we anchor the point for \"1\" to\nthe left side and evaluate whether the resulting vi-\nsualization approximates the log compressed MNL\nas shown in Figure 1d. To quantify this approxi-\nmation, we calculate the correlation between the\npositions of the numbers 1 to 9 in the MDS solution\nand the expected values (log(1) to log (9)) of the\nhuman MNL; see Table 8. All inputs have similar\ncorrelation values. Surprisingly, GPT-2 with digits\nas the number format (and averaged across all lay-\ners) shows a considerably higher correlation with\nthe log-compressed MNL than all other models and\nnumber formats. The average correlation between\nlatent model number lines and the log compressed\nMNL decreases over the 12 layers; see Table 9.\nWe visualize the latent number line of GPT-2\nby averaging the cosine dissimilarity matrix across\nlayers and number formats, submitting this to MDS,\nand requesting a one-dimensional solution; see Fig-\nure 5. This representation shows some evidence\nof log compression, though with a few exceptions.\nOne obvious exception is the right displacement of\n2 away from 1. Another is the right displacement\nof 9 very far from 8.\nTo better understand if this is a statistical arti-\nfact of GPT-2 or a more general difference between\nnumber understanding in humans versus LLMs, we\nperform a residual analysis comparing positions on\nthe model’s number line to those on the human\nMNL. We choose the digits number format, esti-\nmate the latent number line representation averaged\nacross the layers of each model, and compute the\nresidual between the position of each number in\nthis representation compared to the human MNL.\nThis analysis is presented in Table 10. For 1, all\nmodels show a residual value of less than 0.03. This\nmakes sense given our decision to anchor the latent\nnumber lines to 1 on the left side. The largest resid-\nuals are for 2 and 9, consistent with the anomalies\nnoticed for the GPT-2 solution in Figure 5. These\nanomalies are a target for future research. We note\nhere that 2 is often privileged even in languages\nsuch as Piraha and Mundurucu that have very lim-\nited number of word inventories(Gordon, 2004;\nPica et al., 2004). Further note that 9 has special\nsigniﬁcance as a “bargain price numeral” in many\ncultures, a fact that is often linguistically marked\n(Pollmann and Jansen, 1996).\n0.0 0.2 0.4 0.6 0.8 1.0\n−0.04\n−0.02\n0.00\n0.02\n0.04\n1\n3\n4\n5\n6\n7\n2\n8\n9\nModel: GPT2, A veraged pairwise dis ances\nFigure 5: MDS visualization on averaged distances of\nthe GPT-2 model for all number formats and layers.\n4.5 Ablation studies: Base vs Large Model\nVariants\nWe investigate changes in model behaviors when\nincreasing the number of parameters for the same\narchitectures. We use the larger variants of each of\nthe LLMs listed in Table 1. The detailed tabular\nresults of the behaviors are presented in Appendix\nsection A.1; see Tables 11, 12, and 13. Here, we\nsummarize key takeaways from the ablation stud-\nies:\n• The distance and ratio effects of the large vari-\nants of models align with human performance\ncharacteristics. Similar to the results for the\nbase variants, the size effect is only observed\nwhen the input type is digits.\n• We observe the same decreasing trendin the\nlayer-wise capability of capturing thedistance\neffect, ratio effect, and the MDS correlation\nvalues in the Large variants of LLMs as ob-\nserved in the base variants. The increasing\ntrend in the layer-wise capability of the size\neffect is not observed in the Larger LLMs.\n• Residual analysis shows high deviation for\nthe numbers \"2\", \"5\", and \"9\"; which is in line\nwith our observations for the base variations.\n5 Conclusion\nThis paper investigates the performance character-\nistics in various LLMs across numerous conﬁgura-\ntions, looking for three number-magnitude compar-\nison effects: distance, size, and ratio. Our results\nshow that LLMs show human-like distance and ra-\ntio effects across number formats. The size effect\nis also observed among models for the digit num-\nber format, but not for the other number formats,\nshowing that LLMs do not completely capture nu-\nmeration. Using MDS to scale down the pairwise\n(dis)similarities between number representations\nproduces varying correspondences between LLMs\nand the logarithmically compressed MNL of hu-\nmans, with GPT-2 showing the highest correlation\n(using digits as inputs). Our residual analysis ex-\nhibits high deviation from expected outputs for the\nnumbers 2, 5, 9 which we explain through patterns\n6154\nobserved in previous linguistics studies. The be-\nhavioral benchmarking of the numeric magnitude\nrepresentations of LLMs presented here helps us\nunderstand the cognitive plausibility of the rep-\nresentations the models learn. Our results show\nthat LLM pre-training allows models to approx-\nimately learn human-like behaviors for two out\nof the three magnitude effects without the need to\nposit explicit neural circuitry. Future work on build-\ning pre-trained architectures to improve numerical\ncognition abilities should also be evaluated using\nthese three effects.\n6 Limitations\nLimitations to our work are as follows: (1) We only\nstudy the three magnitude effects for the number\nword and digit denotations of the numbers 1 to\n9. The effects for the number 0, numbers greater\nthan 10, decimal numbers, negative numbers, etc.\nare beyond the scope of this study. Future work\ncan design behavioral benchmark for evaluating\nwhether LLMs shows these effects for these other\nnumber classes. (2) The mapping of LLM behav-\niors to human behaviors and effects might vary for\neach effect. Thus, we might require a different link-\ning hypothesis for each such effect. (3) We only\nuse the models built for English tasks and do not\nevaluate multi-lingual models. (4) We report and\nanalyze aggregated scores across different dimen-\nsions. There can be some information loss in this\naggregation. (5) Our choice of models is limited\nby certain resource constraints. Future works can\nexplore the use of other foundation / super-large\nmodels (1B parameters +) and API-based models\nlike GPT3 and OPT3. (6) The behavioral analysis\nof this study is one-way: we look for human per-\nformance characteristics and behaviors in LLMs.\nFuture research can utilize LLMs to discover new\nnumerical effects and look for the corresponding\nperformance characteristics in humans. This could\nspur new research in cognitive science. (7) The\nresults show similar outputs to low dimensional hu-\nman output and show that we do not need explicit\nneural circuitry for number understanding. We do\nnot suggest models actually are humanlike in how\nthey process numbers.\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable\nmath word problem solving with operation-based\nformalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 2357–2367, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDaniel Ansari, Nicolas Garcia, Elizabeth Lucas, Kath-\nleen Hamon, and Bibek Dhital. 2005. Neural corre-\nlates of symbolic number processing in children and\nadults. Neuroreport, 16(16):1769–1773.\nSudeep Bhatia and Russell Richie. 2022. Transformer\nnetworks of human conceptual knowledge. Psycho-\nlogical Review, pages No Pagination Speciﬁed–No\nPagination Speciﬁed.\nVincent A. Billock and Brian H. Tsou. 2011. To honor\nFechner and obey Stevens: Relationships between\npsychophysical and neural nonlinearities. Psycho-\nlogical Bulletin, 137:1–18.\nI. Borg and P.J.F. Groenen. 2005. Modern Multidimen-\nsional Scaling: Theory and Applications. Springer.\nCollin Burns, Saurav Kadavath, Akul Arora, Steven\nBasart, Eric Tang, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring mathematical problem solv-\ning with the MATH dataset. CoRR, abs/2103.03874.\nJessica F. Cantlon. 2012. Math, monkeys, and the\ndeveloping brain. Proceedings of the National\nAcademy of Sciences , 109(supplement_1):10725–\n10732.\nRoi Cohen Kadosh, Jan Lammertyn, and Veronique\nIzard. 2008. Are numbers special? An overview\nof chronometric, neuroimaging, developmental and\ncomparative studies of magnitude representation.\nProgress in Neurobiology, 84(2):132–147.\nStanislas Dehaene and Jacques Mehler. 1992. Cross-\nlinguistic regularities in the frequency of number\nwords. Cognition, 43(1):1–29.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nCody S. Ding. 2018. Fundamentals of Applied Multidi-\nmensional Scaling for Educational and Psychologi-\ncal Research. Springer International Publishing.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\n6155\nGustav Theodor Fechner. 1860. Elements of psy-\nchophysics. 1.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. CoRR, abs/2004.04487.\nPeter Gordon. 2004. Numerical cognition with-\nout words: Evidence from amazonia. Science,\n306(5695):496–499.\nJustin Halberda, Michèle M. M. Mazzocco, and Lisa\nFeigenson. 2008. Individual differences in non-\nverbal number acuity correlate with maths achieve-\nment. Nature, 455(7213):665–668.\nStevan Harnad. 2023. Symbol grounding problem.\nBrenden M. Lake and Gregory L. Murphy. 2021. Word\nmeaning in minds and machines. Psychological re-\nview.\nM. Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. ArXiv, abs/1910.13461.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of\nPre-Trained Language Models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 6862–\n6868, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, M. Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692.\nRebecca Merkley and Daniel Ansari. 2010. Using\neye tracking to study numerical cognition: The case\nof the ratio effect. Experimental Brain Research,\n206(4):455–460.\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar\nSainz, Eneko Agirre, Ilana Heintz, and Dan Roth.\n2021. Recent advances in natural language process-\ning via large pre-trained language models: A survey.\nCoRR, abs/2111.01243.\nRobert S. Moyer and Thomas K. Landauer. 1967. Time\nrequired for judgements of numerical inequality.Na-\nture, 215(5109):1519–1520.\nAakanksha Naik, Abhilasha Ravichander, Carolyn\nRose, and Eduard Hovy. 2019. Exploring numeracy\nin word embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3374–3380, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAndreas Nieder and Stanislas Dehaene. 2009. Repre-\nsentation of Number in the Brain. Annual Review of\nNeuroscience, 32(1):185–208.\nAndreas Nieder and Earl K. Miller. 2003. Coding of\nCognitive Magnitude: Compressed Scaling of Nu-\nmerical Information in the Primate Prefrontal Cortex.\nNeuron, 37(1):149–157.\nHans-Christoph Nuerk, Ulrich Weger, and Klaus\nWillmes. 2001. Decade breaks in the mental num-\nber line? Putting the tens and units back in different\nbins. Cognition, 82(1):B25–B33.\nJohn M. Parkman. 1971. Temporal aspects of digit and\nletter inequality judgments. Journal of Experimen-\ntal Psychology, 91(2):191–205.\nPierre Pica, Cathy Lemer, Ve’ronique Izard, and Stanis-\nlas Dehaene. 2004. Exact and approximate arith-\nmetic in an amazonian indigene group. Science,\n306(5695):499–503.\nThijs Pollmann and Carel Jansen. 1996. The language\nuser as an arithmetician. Cognition, 59(2):219–237.\nAlec Radford, Jeff Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language mod-\nels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nRussell Richie and Sudeep Bhatia. 2021. Similarity\nJudgment Within and Across Categories: A Com-\nprehensive Model Comparison. Cognitive Science,\n45(8):e13030.\nGeorgios P. Spithourakis and Sebastian Riedel. 2018.\nNumeracy for language models: Evaluating and im-\nproving their ability to predict numbers. CoRR,\nabs/1805.08154.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021. Representing numbers in NLP: a sur-\nvey and a vision. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 644–656, Online. Asso-\nciation for Computational Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know\nnumbers? probing numeracy in embeddings. CoRR,\nabs/1909.07940.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Black-\nboxNLP@EMNLP.\nJiapeng Wang and Yihong Dong. 2020. Measurement\nof text similarity: A survey. Information, 11(9).\n6156\nGail Weiss, Yoav Goldberg, and Eran Yahav. 2018.\nOn the practical computational power of ﬁnite\nprecision rnns for language recognition. CoRR,\nabs/1805.04908.\nWikipedia contributors. 2004. Wikipedia, the free en-\ncyclopedia.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. CoRR, abs/1906.08237.\nXikun Zhang, Deepak Ramachandran, Ian Tenney,\nYanai Elazar, and Dan Roth. 2020. Do language em-\nbeddings capture scales? In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020,\npages 4889–4896, Online. Association for Computa-\ntional Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In The IEEE International Con-\nference on Computer Vision (ICCV).\nA Appendix\nA.1 Variants in Large Language Models\nInputs\\Effects\nAveraged\nDistance\nEffect\nAveraged\nSize\nEffect\nAveraged\nRatio\nEffect\nAveraged MDS\nCorrelation\nvalues\nLowercase\nnumber words0.909 0.587 0.730 0.593\nMixedcase\nnumber words0.933 0.514 0.749 0.460\nDigits 0.930 0.678 0.707 0.548\nTotal\nAverages 0.927 0.595 0.727 0.534\nTable 11: Averaged distance effect, size effect, ratio\neffect, and the MDS correlation values for the different\ninput types of the models.\nFor the models in Table1, we show the three\neffects for the larger variants. The variants have the\nsame architectures and training methodologies as\ntheir base variants but more parameters ( thrice the\nnumber of parameters). The in-depth results for the\nNumberT5 BART RoB XLNETBERTGPT-2Avg.\n1 0.04 0.01 0.01 0.01 0.01 0.00 0.01\n2 0.09 0.17 0.09 0.16 0.07 0.12 0.12\n3 0.02 0.09 0.04 0.07 0.03 0.10 0.06\n4 0.02 0.07 0.03 0.04 0.03 0.07 0.04\n5 0.12 0.07 0.13 0.17 0.16 0.02 0.11\n6 0.20 0.06 0.06 0.05 0.10 0.02 0.08\n7 0.17 0.09 0.09 0.07 0.12 0.02 0.09\n8 0.22 0.09 0.05 0.06 0.09 0.03 0.09\n9 0.15 0.19 0.25 0.36 0.25 0.14 0.22\nTable 12: Residual analysis on MDS outputs in 1 di-\nmension on the large variants of the models. RoB:\nRoberta-base model, BERT: uncased variant.\nLayer\\Effects\nAveraged\nDistance\nEffect\nAveraged\nSize\nEffect\nAveraged\nRatio\nEffect\nAveraged MDS\nCorrelation\nvalues\n1 0.967 0.647 0.825 0.643\n2 0.963 0.549 0.718 0.557\n3 0.964 0.587 0.736 0.584\n4 0.968 0.622 0.765 0.544\n5 0.962 0.632 0.763 0.423\n6 0.958 0.641 0.774 0.483\n7 0.957 0.591 0.752 0.526\n8 0.956 0.608 0.753 0.550\n9 0.956 0.599 0.773 0.625\n10 0.944 0.612 0.766 0.610\n11 0.938 0.608 0.742 0.526\n12 0.923 0.604 0.726 0.557\n13 0.939 0.659 0.739 0.538\n14 0.944 0.656 0.755 0.562\n15 0.940 0.645 0.751 0.500\n16 0.933 0.611 0.741 0.509\n17 0.934 0.567 0.730 0.550\n18 0.933 0.580 0.723 0.505\n19 0.919 0.559 0.690 0.527\n20 0.900 0.557 0.671 0.535\n21 0.867 0.558 0.644 0.553\n22 0.854 0.571 0.664 0.524\n23 0.829 0.509 0.633 0.484\n24 0.805 0.508 0.622 0.414\nTable 13: Averaged distance effect, size effect, ratio\neffect, and MDS correlation values for the 24 layers of\nthe models.\nLLMs\\Input LC MC Digits Avg.\nT5 0.961 0.957 0.974 0.964\nBART 0.892 0.957 0.845 0.898\nRoBERTa 0.893 0.959 0.946 0.933\nXLNET 0.924 0.952 0.855 0.910\nBERT (uncased) 0.837 0.969 0.903\nGPT-2 0.946 0.934 0.987 0.956\nTotal Averages\nacross models 0.909 0.933 0.930 0.927\nTable 14: Distance Effect: Averaged (across layers)R2\nvalues of different Larger variants ofLLMs on differ-\nent input types when ﬁtting a linear function. LC: Low-\nercase number words, MC: Mixedcase number words.\nthree effects are presented in tables 14, 16, 15, 17,\n18, and 19. We also present the MDS correlation\nvalues in the same manner as done for base variants;\nsee tables 20 and 21.\nGiven the large layer count for these model vari-\n6157\nLLMs\\Input LC MC Digits Avg.\nT5 0.720 0.730 0.840 0.763\nBART 0.697 0.644 0.380 0.574\nRoBERTa 0.468 0.267 0.677 0.471\nXLNET 0.533 0.448 0.510 0.497\nBERT (uncased) 0.635 0.712 0.674\nGPT-2 0.467 0.358 0.950 0.592\nTotal Averages\nacross models 0.587 0.514 0.678 0.595\nTable 15: Size Effect: Averaged (across layers) R2 val-\nues of different Larger variants ofLLMs on different\ninput types when ﬁtting a linear function. LC: Lower-\ncase number words, MC: Mixedcase number words.\nLayer T5 BART RoB XLNETBERTGPT-2 Avg.\n1 0.978 0.948 0.968 0.972 0.978 0.959 0.967\n2 0.977 0.958 0.962 0.976 0.967 0.940 0.963\n3 0.977 0.970 0.931 0.979 0.977 0.951 0.964\n4 0.976 0.948 0.972 0.984 0.968 0.959 0.968\n5 0.975 0.944 0.950 0.981 0.976 0.947 0.962\n6 0.973 0.919 0.950 0.978 0.975 0.952 0.958\n7 0.979 0.911 0.968 0.974 0.958 0.952 0.957\n8 0.981 0.892 0.953 0.977 0.973 0.959 0.956\n9 0.983 0.875 0.967 0.974 0.980 0.959 0.956\n10 0.980 0.857 0.947 0.967 0.957 0.958 0.944\n11 0.984 0.847 0.931 0.944 0.964 0.959 0.938\n12 0.990 0.828 0.865 0.920 0.974 0.959 0.923\n13 0.990 0.953 0.901 0.865 0.968 0.959 0.939\n14 0.990 0.933 0.935 0.874 0.975 0.957 0.944\n15 0.988 0.919 0.945 0.858 0.972 0.959 0.940\n16 0.977 0.900 0.941 0.854 0.966 0.957 0.933\n17 0.974 0.899 0.944 0.883 0.948 0.955 0.934\n18 0.978 0.897 0.946 0.892 0.930 0.957 0.933\n19 0.951 0.882 0.938 0.874 0.913 0.957 0.919\n20 0.947 0.885 0.900 0.857 0.858 0.956 0.900\n21 0.932 0.879 0.887 0.808 0.740 0.957 0.867\n22 0.927 0.858 0.927 0.789 0.668 0.957 0.854\n23 0.859 0.827 0.889 0.862 0.579 0.957 0.829\n24 0.872 0.825 0.867 0.808 0.502 0.954 0.805\nTable 16: Distance Effect: Averaged (across inputs)R2\nvalues of different Larger variants ofLLMs for differ-\nent layers when ﬁtting a linear function. RoB: Roberta-\nbase model, BERT: uncased variant.\nants and the multiple tables, we also present a sum-\nmarized view of the results in tables 11, 12, 13.\nA.2 Cased vs Uncased BERT\nThe behavioral differences between the cased and\nuncased variants of the BERT architecture are\nshown in TableA.2. Despite different preprocess-\ning paradigms, both models have similar perfor-\nmance characteristics. The only visible distinction\nis the higher correlation values for the cased ver-\nsion when compared to the uncased version of the\nLayer T5 BART RoB XLNETBERTGPT-2 Avg.\n1 0.785 0.800 0.591 0.630 0.608 0.467 0.647\n2 0.794 0.763 0.275 0.666 0.198 0.597 0.549\n3 0.894 0.709 0.379 0.665 0.214 0.661 0.587\n4 0.922 0.719 0.465 0.661 0.345 0.620 0.622\n5 0.940 0.721 0.550 0.634 0.387 0.563 0.632\n6 0.925 0.606 0.426 0.644 0.661 0.584 0.641\n7 0.912 0.441 0.360 0.603 0.636 0.594 0.591\n8 0.923 0.399 0.460 0.548 0.726 0.595 0.608\n9 0.915 0.354 0.435 0.541 0.750 0.599 0.599\n10 0.923 0.329 0.546 0.553 0.727 0.593 0.612\n11 0.924 0.362 0.458 0.574 0.727 0.601 0.608\n12 0.890 0.351 0.512 0.543 0.728 0.601 0.604\n13 0.864 0.801 0.467 0.468 0.757 0.595 0.659\n14 0.837 0.861 0.452 0.436 0.751 0.600 0.656\n15 0.805 0.796 0.480 0.454 0.741 0.597 0.645\n16 0.761 0.683 0.449 0.436 0.739 0.597 0.611\n17 0.692 0.550 0.391 0.423 0.746 0.598 0.567\n18 0.743 0.520 0.453 0.423 0.747 0.594 0.580\n19 0.633 0.512 0.435 0.391 0.788 0.594 0.559\n20 0.583 0.513 0.448 0.373 0.828 0.596 0.557\n21 0.523 0.532 0.512 0.345 0.847 0.592 0.558\n22 0.432 0.546 0.633 0.350 0.874 0.588 0.571\n23 0.356 0.455 0.491 0.316 0.846 0.590 0.509\n24 0.335 0.444 0.634 0.250 0.801 0.584 0.508\nTable 17: Size Effect: Averaged (across inputs)R2 val-\nues of different Larger variants ofLLMs for different\nlayers when ﬁtting a linear function. RoB: Roberta-\nbase model, BERT: uncased variant.\nLLMs\\Input LC MC Digits Avg.\nT5 0.868 0.816 0.833 0.839\nBART 0.767 0.838 0.478 0.694\nRoBERTa 0.672 0.686 0.725 0.694\nXLNET 0.617 0.649 0.711 0.659\nBERT (uncased) 0.786 0.732 0.759\nGPT2 0.669 0.720 0.767 0.718\nTotal Averages\nacross models 0.730 0.749 0.707 0.718\nTable 18: Ratio Effect: Averaged (across layers) R2\nvalues of different Larger variants ofLLMs on differ-\nent input types when ﬁtting a negative exponential func-\ntion. LC: Lowercase number words, MC: Mixedcase\nnumber words.\nmodel.\nA.3 Impact of Distance effect and Size effect\nin Ratio effect scores\nWhen interpreting LLM ﬁndings on the ratio effect,\nwe observe that they are dominated by the distance\neffect as compared to the size effect. We observe\nthe same decreasing trend in averaged results over\ninput types in layers; see Table 7 (column: Total\nAverages). The impact of layer-wise trends can be\nquantiﬁed using regression with the distance effect\n6158\nLayer T5 BART RoB XLNETBERTGPT-2 Avg.\n1 0.868 0.837 0.803 0.881 0.829 0.733 0.825\n2 0.803 0.740 0.529 0.873 0.657 0.708 0.718\n3 0.792 0.798 0.573 0.875 0.602 0.775 0.736\n4 0.828 0.782 0.722 0.868 0.667 0.725 0.765\n5 0.860 0.823 0.716 0.863 0.664 0.652 0.763\n6 0.878 0.811 0.671 0.836 0.765 0.680 0.774\n7 0.898 0.686 0.669 0.818 0.735 0.704 0.752\n8 0.896 0.657 0.726 0.797 0.722 0.716 0.753\n9 0.910 0.658 0.714 0.792 0.838 0.729 0.773\n10 0.915 0.639 0.718 0.774 0.818 0.729 0.766\n11 0.921 0.640 0.583 0.745 0.835 0.725 0.742\n12 0.917 0.638 0.518 0.691 0.868 0.724 0.726\n13 0.920 0.836 0.538 0.593 0.820 0.728 0.739\n14 0.937 0.764 0.679 0.585 0.837 0.724 0.755\n15 0.931 0.715 0.772 0.546 0.822 0.722 0.751\n16 0.915 0.713 0.762 0.514 0.815 0.726 0.741\n17 0.904 0.684 0.747 0.492 0.836 0.718 0.730\n18 0.907 0.666 0.728 0.497 0.815 0.728 0.723\n19 0.778 0.617 0.754 0.464 0.807 0.720 0.690\n20 0.754 0.613 0.717 0.450 0.775 0.720 0.671\n21 0.692 0.600 0.723 0.435 0.699 0.716 0.644\n22 0.679 0.605 0.802 0.459 0.715 0.721 0.664\n23 0.637 0.587 0.730 0.478 0.651 0.716 0.633\n24 0.592 0.559 0.767 0.486 0.624 0.703 0.622\nTable 19: Ratio Effect: Averaged (across inputs) R2\nvalues of different Larger variants ofLLMs for differ-\nent layers when ﬁtting a negative exponential function.\nRoB: Roberta-base model, BERT: uncased variant.\nLLMs\\Input LC MC Digits Avg.\nT5 0.572 0.127 0.408 0.369\nBART 0.677 0.546 0.515 0.580\nRoBERTa 0.669 0.573 0.473 0.572\nXLNET 0.498 0.373 0.465 0.445\nBERT (uncased) 0.519 0.541 0.530\nGPT2 0.623 0.624 0.888 0.711\nTotal Averages\nacross models 0.593 0.460 0.548 0.534\nTable 20: Averaged (across layers) correlation values\nwhen comparing MDS values with Log101 to Log109\nfor Large variants ofdifferent LLMs. LC: Lowercase\nnumber words, MC: Mixedcase number words.\nand size effect as inputs (column: Total Averages;\ntables 2, 4) and the ratio effect (column: Total Av-\nerages; Table4) as output. Importantly, the distance\neffect averages are statistically signiﬁcant predic-\ntors of ratio effect averages; see Table 23). These\nresults provide a superﬁcial view of the impact of\ndistance and size effect in the ratio effect scores\nbecause of the aggregation performed at different\nlevels of the study.\nLayer T5 BART RoB XLNETBERTGPT-2Avg.\n1 0.675 0.633 0.731 0.590 0.542 0.689 0.643\n2 0.249 0.662 0.461 0.649 0.555 0.767 0.557\n3 0.251 0.673 0.522 0.689 0.662 0.707 0.584\n4 0.156 0.682 0.698 0.674 0.353 0.703 0.544\n5 0.059 0.518 0.493 0.686 0.065 0.719 0.423\n6 0.219 0.471 0.411 0.533 0.535 0.729 0.483\n7 0.569 0.421 0.558 0.549 0.367 0.688 0.526\n8 0.578 0.413 0.540 0.690 0.385 0.695 0.550\n9 0.581 0.710 0.594 0.546 0.598 0.720 0.625\n10 0.495 0.716 0.531 0.487 0.710 0.718 0.610\n11 0.286 0.691 0.404 0.495 0.576 0.702 0.526\n12 0.481 0.682 0.304 0.466 0.708 0.700 0.557\n13 0.387 0.605 0.533 0.394 0.588 0.721 0.538\n14 0.483 0.672 0.538 0.383 0.574 0.718 0.562\n15 0.486 0.386 0.596 0.241 0.586 0.705 0.500\n16 0.485 0.454 0.689 0.140 0.591 0.692 0.509\n17 0.536 0.677 0.617 0.163 0.588 0.719 0.550\n18 0.259 0.562 0.651 0.251 0.602 0.704 0.505\n19 0.458 0.750 0.583 0.077 0.599 0.694 0.527\n20 0.463 0.545 0.652 0.246 0.585 0.718 0.535\n21 0.362 0.526 0.653 0.524 0.554 0.700 0.553\n22 0.402 0.522 0.656 0.247 0.596 0.719 0.524\n23 -0.019 0.466 0.649 0.490 0.600 0.720 0.484\n24 -0.051 0.473 0.652 0.476 0.205 0.726 0.414\nTable 21: Averaged (across inputs) correlation val-\nues of the Large variants of different LLMs on dif-\nferent model layers when comparing MDS values with\nLog101 to Log109. RoB: Roberta-base model, BERT:\nuncased variant.\nVariant Effect LC MC Digits Avg.\nUncased\nDistance 0.976 0.944 0.960\nSize 0.803 0.851 0.827\nRatio 0.906 0.757 0.831\nMDS (Corr.) 0.312 0.423 0.386\nCased\nDistance 0.958 0.980 0.890 0.943\nSize 0.664 0.691 0.918 0.758\nRatio 0.854 0.880 0.866 0.867\nMDS (Corr.) 0.621 0.553 0.487 0.554\nTable 22: Behavioral differences between the cased and\nuncased variants of the BERT architecture. LC: Lower-\ncase number words, MC: Mixed-case number words.\nVariant Coef. Std. Error t Stat P-value\nIntercept -0.916 0.531 -1.722 0.119\nBase Distance Effect 1.953 0.452 4.3140.001⊙\nSize Effect -0.228 0.188 -1.212 0.256\nIntercept -0.188 0.075 -2.491 0.0.021\nLarge Distance Effect 0.700 0.117 5.9970.000⊕\nSize Effect 0.447 0.124 3.612 0.001⊙\nTable 23: Impact of layer-wise trends of distance and\nsize effect on the ratio effect;⊙indicates statistical sig-\nniﬁcance with p-value less that 0.01, ⊕indicates statis-\ntical signiﬁcance with p-value less that 0.00001\n6159\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 6\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract and Section 1. Section 1.2 lays the foundation for the research questions and 1.3 describes\nthe problems we tackle.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0017 Did you use or create scientiﬁc artifacts?\nLeft blank.\n□\u0017 B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nSection 3, 4.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNot applicable. Left blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n6160\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nThe experimental design and the operationalization of experiments are given in sections 3 and 4.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 3.3\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n6161",
  "topic": "Cognition",
  "concepts": [
    {
      "name": "Cognition",
      "score": 0.5890668630599976
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5033709406852722
    },
    {
      "name": "Computer science",
      "score": 0.5020310878753662
    },
    {
      "name": "Ask price",
      "score": 0.4699335992336273
    },
    {
      "name": "Contrast (vision)",
      "score": 0.41928359866142273
    },
    {
      "name": "Psychology",
      "score": 0.35267823934555054
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3395366966724396
    },
    {
      "name": "Cognitive science",
      "score": 0.32526540756225586
    },
    {
      "name": "Neuroscience",
      "score": 0.15100997686386108
    },
    {
      "name": "Economy",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 3
}