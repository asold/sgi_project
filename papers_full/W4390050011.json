{
  "title": "Tomato maturity recognition with convolutional transformers",
  "url": "https://openalex.org/W4390050011",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2187009071",
      "name": "Asim Khan",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2162667643",
      "name": "Taimur Hassan",
      "affiliations": [
        "Abu Dhabi University"
      ]
    },
    {
      "id": "https://openalex.org/A3099891075",
      "name": "Muhammad Shafay",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3211344481",
      "name": "Israa Fahmy",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A433153394",
      "name": "Naoufel Werghi",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5092634331",
      "name": "Seneviratne Mudigansalage",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2188268779",
      "name": "Irfan Hussain",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2187009071",
      "name": "Asim Khan",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2162667643",
      "name": "Taimur Hassan",
      "affiliations": [
        "Abu Dhabi University"
      ]
    },
    {
      "id": "https://openalex.org/A3099891075",
      "name": "Muhammad Shafay",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A3211344481",
      "name": "Israa Fahmy",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A433153394",
      "name": "Naoufel Werghi",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5092634331",
      "name": "Seneviratne Mudigansalage",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2188268779",
      "name": "Irfan Hussain",
      "affiliations": [
        "Khalifa University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2990718515",
    "https://openalex.org/W2082538881",
    "https://openalex.org/W1815665843",
    "https://openalex.org/W3116981493",
    "https://openalex.org/W4389635773",
    "https://openalex.org/W2778455075",
    "https://openalex.org/W2962860144",
    "https://openalex.org/W2950800384",
    "https://openalex.org/W2193145675",
    "https://openalex.org/W3044607523",
    "https://openalex.org/W2999847126",
    "https://openalex.org/W2896736480",
    "https://openalex.org/W3035016091",
    "https://openalex.org/W3046188730",
    "https://openalex.org/W2953151638",
    "https://openalex.org/W3011638763",
    "https://openalex.org/W3094319938",
    "https://openalex.org/W4298148810",
    "https://openalex.org/W4249669273",
    "https://openalex.org/W4285263288",
    "https://openalex.org/W2006810925",
    "https://openalex.org/W3034495444",
    "https://openalex.org/W2045720376",
    "https://openalex.org/W2010729636",
    "https://openalex.org/W2470368200",
    "https://openalex.org/W4286461669",
    "https://openalex.org/W3083395297",
    "https://openalex.org/W4280595758",
    "https://openalex.org/W4226072349",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2993182889",
    "https://openalex.org/W3004299129",
    "https://openalex.org/W2968952932",
    "https://openalex.org/W3011977658",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3211592992",
    "https://openalex.org/W4223970271",
    "https://openalex.org/W4307111147",
    "https://openalex.org/W4309718291",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3098093879",
    "https://openalex.org/W2954996726",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2501369945",
    "https://openalex.org/W3047372165",
    "https://openalex.org/W2564734427",
    "https://openalex.org/W3039712305",
    "https://openalex.org/W6600137863",
    "https://openalex.org/W3171398643",
    "https://openalex.org/W6603394118",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2962767316",
    "https://openalex.org/W3092238363",
    "https://openalex.org/W3176923149",
    "https://openalex.org/W3106250896",
    "https://openalex.org/W3014641072",
    "https://openalex.org/W3047032303",
    "https://openalex.org/W4206566023",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2806070179",
    "https://openalex.org/W4226061908"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports\nTomato maturity recognition \nwith convolutional transformers\nAsim Khan 1,2, Taimur Hassan 3, Muhammad Shafay 2,4, Israa Fahmy 2,4, Naoufel Werghi 2,4, \nSeneviratne Mudigansalage 1,2 & Irfan Hussain 1,2*\nTomatoes are a major crop worldwide, and accurately classifying their maturity is important for many \nagricultural applications, such as harvesting, grading, and quality control. In this paper, the authors \npropose a novel method for tomato maturity classification using a convolutional transformer. The \nconvolutional transformer is a hybrid architecture that combines the strengths of convolutional \nneural networks (CNNs) and transformers. Additionally, this study introduces a new tomato dataset \nnamed KUTomaData, explicitly designed to train deep-learning models for tomato segmentation \nand classification. KUTomaData is a compilation of images sourced from a greenhouse in the UAE, \nwith approximately 700 images available for training and testing. The dataset is prepared under \nvarious lighting conditions and viewing perspectives and employs different mobile camera sensors, \ndistinguishing it from existing datasets. The contributions of this paper are threefold: firstly, the \nauthors propose a novel method for tomato maturity classification using a modular convolutional \ntransformer. Secondly, the authors introduce a new tomato image dataset that contains images of \ntomatoes at different maturity levels. Lastly, the authors show that the convolutional transformer \noutperforms state-of-the-art methods for tomato maturity classification. The effectiveness of the \nproposed framework in handling cluttered and occluded tomato instances was evaluated using two \nadditional public datasets, Laboro Tomato and Rob2Pheno Annotated Tomato, as benchmarks. \nThe evaluation results across these three datasets demonstrate the exceptional performance of our \nproposed framework, surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of \nmean average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated Tomato, \nrespectively. This work can potentially improve the efficiency and accuracy of tomato harvesting, \ngrading, and quality control processes.\nPlants play a pivotal role in meeting global food demands. Among the most widely consumed vegetables are \ntomatoes, with annual production surpassing 180 million tons for the past 7  years1. Commercially, tomatoes are \ntypically harvested during the mature ripening stage. This practice is primarily due to their firmness, extended \nshelf-life, and the potential to turn red after being removed from the  plant2. The decision to harvest at this stage \nis primarily influenced by consumer preferences for fresh tomatoes, particularly their colour and  texture3 and \nthe need to minimize potential damage during transportation and other supply chain-related activities.\nIn academic research, the role of technology in optimizing agricultural practices is highly emphasized. A \nparticular area of interest for scholars lies in the detection and classification of crops, where deep learning and \nimage-processing techniques are utilized. Furthermore, automation in agriculture can enhance the working \nconditions of farmers and agricultural workers, who often face musculoskeletal disorders. The introduction of \nrobots for crop monitoring and harvesting has proven highly beneficial, leading to significant improvements in \nproduction profits. These benefits are realized by streamlining the harvesting process, enhancing crop quality \nand yield, and reducing labour costs. These advantages have spurred extensive research over the past few decades, \nparticularly on robotic technology’s improvements and potential applications in agriculture. Whether referred \nto as “precision agriculture” or “low-impact farming” , this approach forms an integral part of a broader shift  \nwithin the agricultural industry. Additionally, advancements in computer vision can significantly enhance the \nagricultural sector by increasing efficiency and accuracy in various tasks, such as crop assessment and harvesting.\nMachine learning (ML) methodologies significantly automate processes such as categorising plant diseases, \nfruit maturity grading, and automated harvesting  methods4,5. ML tools aid in monitoring plant health and \nOPEN\n1Department of Mechanical Engineering, Khalifa University, Abu Dhabi, UAE. 2Khalifa University Center for \nRobotics and Autonomous Systems (KUCARS), Khalifa University, Abu Dhabi, UAE. 3Department of Electrical, \nComputer and Biomedical Engineering, Abu Dhabi University, Abu Dhabi, UAE. 4Department of Electrical \nEngineering and Computer Science, Khalifa University, Abu Dhabi, UAE. *email: Irfan.Hussain@ku.ac.ae\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\npredicting potential abnormalities at early  stages6. Over the years, various ML models have been developed, \nincluding artificial neural networks and support vector machines (SVM) 7.\nWith the advent of deep learning (DL), several new models such as  VGG8, R-FCN9, Faster R-CNN10, and \n SSD11; have been introduced, providing fundamental frameworks to perform object detection and recognition \ntasks. Some of these methodologies find application in agricultural automation systems, aiding in identifying \nand classifying crops and their diseases. Notably, the advent of DL has led to promising results and methods in \nthe agricultural domain. Advancements in deep learning have made it possible to employ convolutional neural \nnetworks (CNNs) in tasks such as fruit classification and yield estimation. For instance, Faster R-CNN10 has been \nutilized for apple  detection12, and YOLO has been applied to detect  mangoes13. Sun et al.14 proposed an enhanced \nversion of the Faster R-CNN model, which demonstrated improved performance in detecting and identifying \nvarious parts of tomatoes, achieving a mean average precision (mAP) score of 90.7% for the recognition of \ntomato flowers, unripened tomatoes, and ripe tomatoes. The optimized model exhibited a noteworthy reduction \nof approximately 79% in memory requirements, suggesting the use of memory optimization techniques, such as \nparameter reduction methods or model compression techniques. In another study, Liu et al.15 proposed a novel \ntomato detection model based on  YOLOv316. Their model, which utilized a new bounding mechanism instead of \nconventional rectangular bounding boxes, enhanced the F1 score by 65%. Zhifeng et al.17 improved the YOLOv3-\ntiny model for ripe tomato identification, which achieved a 12% improvement over its conventional counterpart \nin terms of the F1 score. While detection models can identify and localize fruit regions within candidate scans, \nthey often struggle to capture the contours and shapes of the fruits accurately. Segmentation methods can address \nthis limitation by providing detailed information about fruit shapes and sizes through pixel-wise mask output. \nFor instance, as demonstrated by Yu et al.18, the Mask R-CNN model was employed to successfully identify ripe \nstrawberries, particularly those difficult to distinguish due to overlapping. Similarly, Kang et al.19 employed the \nMobile-DasNet model combined with a segmentation network to identify fruits, achieving accuracies of 90% \nand 82% for the respective tasks.\nRipeness is a critical factor in the quality and marketability of tomatoes. Traditionally, ripeness is assessed by \nhuman inspectors, who visually examine the tomatoes for colour, firmness, and other characteristics. However, \nthis manual process is time-consuming, labour-intensive, and subjective. Early studies used simple features, such \nas the average RGB value of a tomato image, to classify ripeness.\nTargeted fruit harvesting refers to the selective picking of ripe fruits, a complex task due to the unpredictable \nnature of crops and outdoor conditions. A vital example of this complexity is seen with tomatoes. They are a \nstaple food crop widely grown worldwide but present a unique segmentation challenge due to their occlusion \nwith leaves and stems, making it difficult to determine their ripeness. This is the reason for creating a new dataset \nthat helps resolve these issues and provides a better perspective of tomato segmentation in complex environ -\nments. Introducing the KUTomaDATA dataset, with approximately 700 images obtained from greenhouses in \nAl Ajban, Abu Dhabi, United Arab Emirates, the authors address the pressing need for a comprehensive and \ndiverse collection of tomato images to tackle real-life challenges in tomato farming. One of the novel features of \nKUTomaDATA lies in its representation of three distinct types of tomatoes: green, half-ripe, and fully ripe. This \ndivision into ripening stages, comprising “Fully Ripened” , “Half Ripened” , and “Un-ripened” tomatoes, provides \na more nuanced and comprehensive dataset for researchers and practitioners. This dataset offers a unique and \nvaluable resource for the computer vision community.\nIn this research, the authors present a novel framework for the real-time segmentation of tomatoes and deter-\nmining their maturity levels under diverse lighting and occlusion conditions. Here, our primary objective is to \nautomate the process of tomato harvesting, potentially resulting in enhanced efficiency and reduced agricultural \nexpenses. In addition to improving the harvesting process, accurately assessing tomato ripeness at the pixel \nlevel could also have other benefits. For example, it may allow for more precise sorting and grading of tomatoes, \nresulting in higher-quality final products. This could be particularly important for producers who export their \ntomatoes to different markets, as quality standards vary widely among countries.\nIn summary, this research can potentially bring about a paradigm shift in the harvesting and grading of \ntomatoes, which could have profound implications for the agricultural sector. By enhancing productivity and \nimplementing stringent quality control measures, farmers may have the opportunity to boost their profitability \nwhile satisfying the increasing market demand for premium, environmentally friendly agricultural products. \nThe main contributions of this study are outlined below: \n1. The proposed approach provides a modular feature extraction and decoding method that separates the \nsegmentation architecture, commonly referred to as the “meta-architecture” , as illustrated in Fig. 1.\n2. Introducing a new dataset known as KUTomaData, captured under various Lighting, Occlusion, and Ripe-\nness conditions from indoor glasshouse farms. Hence, this dataset provides many challenges to solve, giving \nit an edge over the existing datasets available to the research community.\n3. The proposed model is constrained via the L t loss function, enabling it to extract tomato regions from can-\ndidate scans that depict various textural, contextual, and semantic differences. Moreover, the L t loss function \nalso ensures that the proposed model, at the inference stage, can objectively recognize different maturity \nstages of the tomatoes, irrespective of the scan attributes, for their effective cultivation.\n4. The proposed trained model is highly versatile and can be integrated into a mobile robot system designed for \ngreenhouse farming. This integration would enable the robot to accurately detect and identify the maturity \nlevel of tomatoes in real time, which could significantly improve the efficiency and productivity of the farm-\ning process.\n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nThe remainder of the paper is organized as follows: “Methods” delivers an in-depth discussion of the proposed \nmethod. Section “ Datasets” explores datasets. Section “ Experiments” offers insights into the experiments and \nexperimental procedures utilized. “ Results” covers the evaluation results, “Ablation study” covers the ablation \nstudy, and “Discussion” delves into a detailed discussion of the proposed framework. “Limitations” lists some of \nthe limitations. Finally, “Conclusions” concludes the paper.\nRelated work\nIn this section, the authors highlight recent advances in precision agriculture proposed to assist farmers in \neffectively increasing their crop production, with a particular emphasis on  tomatoes20. To effectively organize the \nexisting literature, the authors have categorized the methods into two groups: one group focuses on employing \nconventional techniques to enhance existing agricultural workflows, while the other group leverages modern \ncomputer vision schemes to enhance agricultural growth in terms of productivity, disease detection, and moni-\ntoring in natural farm  environments21\nTraditional methods in precision agriculture\nTomatoes are widely grown crops that have been the focus of many agricultural studies. Traditional approaches \nto improving tomato harvesting encompass various methods and principles for better managing these fruits \nagainst pests and diseases. These methods ultimately enhance overall agricultural productivity. Moreover, the \nevolution of these methods over the years has refined the foundation of traditional agricultural practices. Some \nof the standard methods proposed to improve agricultural workflows include.\nCrop rotation is a strategic agricultural practice that involves the sequential cultivation of different crops \nacross multiple seasons. Its purpose is to mitigate the negative impact of pests and diseases that specifically target \ncertain crops while simultaneously improving soil fertility and overall crop  yield22. Intercropping is a farming \ntechnique that involves cultivating two or more crops together in the same field  concurrently23. This method \noptimizes land utilization, promotes biodiversity, reduces the incidence of pests and diseases, and enhances soil \nfertility through nutrient complementarity. Conventional irrigation methods encompass various systems such \nas flood, furrow, and sprinkler irrigation. These systems ensure a regulated water supply to crops, facilitating \nFigure 1.  An architectural diagram of the proposed framework for tomato maturity level recognition and \ngrading. The proposed framework consists of the transformer, encoder, and decoder blocks. The input scan is \ninitially passed to the transformer and encoder block. Across the transformer end, the input scan is divided into \na set of image patches, against which the positional embeddings are computed. These positional embeddings and \nlinear projections of the image patches are combined and are passed to the t-layered transformer block, which \ngenerates the projectional features to differentiate tomato grades. Similarly, the latent feature representations \nare computed from the input scan using the residual and shape preservation blocks at the encoder block. These \nlatent space representations are then fused with the projectional features of the transformer end to boost the \nseparation between different tomato grades. Finally, the decoder block removes extraneous elements through \nrescaling and max un-pooling operations, resulting in accurate segmentation and grading of tomato maturity \nlevels..\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\ntheir optimal growth and  development24 Furthermore, traditional agricultural practices have heavily relied on \napplying organic fertilizers, including crop residues, compost, and manure, to enhance soil fertility and provide \nessential nutrients to plants. These natural fertilizers contribute to long-term soil health and foster sustainable \nagricultural  practices25. Mechanical tillage involves using ploughs, harrows, and other machinery to prepare \nthe soil for  planting26. It serves multiple purposes, such as weed control, improved seedbed conditions, and \nincorporated nutrients into the soil. However, it is essential to note that mechanical tillage can also result in soil \nerosion and degradation. Conventional pest and disease management methods predominantly rely on chemical \npesticides and fungicides to control insects, weeds, and plant diseases. These methods aim to safeguard crops \nfrom damage and promote optimal growth. However, concerns have been raised regarding their potential adverse \nimpacts on the environment and human  health27. Acknowledging the strengths and limitations of these conven-\ntional agricultural practices is crucial to exploring opportunities for improvement and advancement in the field.\nModern computer vision methods for precision agriculture\nDeep learning methods have recently attracted a lot of interest and have been increasingly utilized for the precise \nidentification of tomato diseases and growth monitoring. Similarly, CNNs have also been utilized for tomato \nfertilization and disease  detection28. These methods, built upon neural networks, are used to analyze large-scale \ndatasets and derive insightful patterns for the precise detection and monitoring of tomatoes. Sherafati et al. 29 \nproposed a framework for assessing the ripeness of tomatoes from RGB images. Sladojevic et al.28 utilized trans-\nfer learning to detect and classify tomato diseases. They achieved accurate disease classification by fine-tuning \na pre-trained CNN network using a tomato disease dataset. Khan et al. 30 proposed a DeepLens Classification \nand Detection Model (DCDM) to classify healthy and unhealthy fruit trees and vegetable plant leaves using \nself-collected data and PlantVillage  dataset31. Their experiments achieved an impressive 98.78% accuracy in \nreal-time diagnosis of plant leaf diseases. Zheng et al. 32 presented a  YOLOv433 detector to determine tomato \nripeness. In contrast, Xu et al. 34 utilized Mask R-CNN35 to differentiate between tomato stems and fruit. Rong \net al.36 presented a framework based on YOLACT++37 for tomato identification. However, this model could not \ndetermine the tomatoes’ ripeness due to the limited capability of the YOLACT++ framework in capturing and \nanalyzing colour and textural features indicative of tomato ripeness. The YOLACT++ model primarily focuses \non instance segmentation and object detection tasks without incorporating specific features or mechanisms to \nassess the ripeness of the tomatoes. As a result, the model’s performance in accurately determining the ripeness \nlevel of the tomatoes was not satisfactory.\nIncorporating semantic or instance segmentation models in agriculture can revolutionise how crops are \nassessed and harvested. While segmentation tasks are intricate, they offer the ability to identify objects and \nextract their semantic information at the pixel level. Such capabilities have become increasingly important for \nrobots used in crop harvesting, where the first step is to detect, classify, and segment crops using computer \nvision  methods38,39. For example, Liu et al. 40 employed  UNet41 to extract maize tassel. The authors achieved a \nhigh accuracy of 98.10% and demonstrated the potential of using semantic segmentation for plant phenotyping.\nMoreover, various studies have shown that using transformer models, such as  ViT42, has improved the rec-\nognition of  crops43. Likewise, transformers-based detection models have shown promising results in leaf disease \ndetection and assessing the appearance quality of crops such as  strawberries 44–46. Chen et al. 47 used a Swin \n transformer48 for detecting and counting wine grape bunch clusters in a non-destructive and efficient manner. \nRemarkably, their proposed approach achieved high recognition accuracy even in partial occlusions and overlap-\nping fruit clusters. Utilizing advanced computer vision techniques in agriculture can significantly enhance the \neffectiveness and precision of crop assessment and harvesting, ultimately boosting productivity and sustainability \nwithin the  industry49.\nMethods\nThe precise segmentation of tomato maturity levels is important in various agricultural applications, such as \nharvesting, grading, and quality control. To address this challenge, we propose a novel framework that lever -\nages advanced techniques, including encoder and transformer blocks, to process input scans effectively. The \ntransformer block within the proposed model is derived from  ViT43, and CMSA is the same as the multi-headed \nself-attention block in  ViT43. In contrast to standard ViT variants, our approach involves the utilization of three \ntransformer encoders arranged in a cascaded manner. This enables the generation of attentional characteristics, \nwhich are subsequently combined with convolutional features to extract various development stages of tomatoes \neffectively.\nInitially, the input image is passed to the encoder and transformer blocks. The latent feature representations \nare computed from the input image using the residual and shape preservation blocks at the encoder block. Simi-\nlarly, the input image is divided into n number of image patches at the transformer end, against which n  posi-\ntional embeddings are computed. These positional embeddings and linear projections of the image patches are \ncombined and are passed to the t-layered transformer block to generate the projectional features via a contextual \nmulti-head self-attention mechanism to differentiate between different tomato grades. We want to mention that \nthe distinctive aspect of the transformer model (used in the proposed scheme) as compared to the conventional \nViT is the number of stacked transformer blocks. In the original Vision Transformer (ViT) architecture, the ViT \nmodel consists of a stack of 8 identical transformer blocks. However, within the proposed scheme, we used only \n3 stacked transformer blocks. The reason for using 3 stacked transformer blocks is because we achieved optimal \ntrade-off between performance and computational complexity with this configuration toward recognizing dif -\nferent maturity stages of tomatoes. Adding more transformer blocks can increase further the performance of \nthe proposed system but at the expense of adding excessive computational cost which we avoided by the cur -\nrent model design choice. Finally, the decoder block removes extraneous elements through rescaling and max \n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nun-pooling operations, resulting in accurate segmentation and grading of tomato maturity levels. The subsequent \nsections provide a comprehensive overview of each block within the proposed framework:\nTransformer block\nThe proposed model incorporates a transformer block composed of t encoders. Empirically, t is set to 3, giving \nrise to encoders T-1, T-2, and T-3, which are cascaded together to generate pt . Initially, the input image x  is \npartitioned into non-overlapping, square-shaped patches denoted by xpǫRPx PxCh , where P indicates the resolu-\ntion of xp determined by the equation P =\n√\nRCx\nnp  . Here, np represents the total number of patches. The positional \nembeddings x e\ni corresponding to patch xp\ni are then generated, i.e., xeǫRPx P xCh . Subsequently, the flattened projec-\ntions, i.e., fp (x e\ni ) , are computed. In a similar manner, the linear projection for patch xp\ni  , denoted as lt(x p\ni ) , is \nobtained. Both fp (x e\ni ) and lt(x p\ni ) are resized to l  dimensions, and the sequenced embeddings for patch xp\ni  are \ncomputed by adding lt(x p\ni ) to fp (x e\ni ) , i.e., q i = lt(x p\ni ) + fp (x e\ni ) . By repeating this process for all the np patches, the \ncombined projections, qo , are generated, expressed as follows:\nOr\nThis process allows the model to capture spatial information from the image and create a representation that \nthe transformer block can further process. The next step involves passing the combined projections qo to T-1, \nwhere each head j normalises qo j to produce ´qjo . Then, ´qjo is decomposed into a query ( Q j ), key ( K j ), and value \n( V j ) pairs using learn-able weights, with Q j =´qjow q , K =´qjow k , and V =´qjow v . The contextual self-attention at \nhead j (i.e., A j ) is then computed by combining Q j and K j through scaled dot product, and their resulting scores \nare merged with V j.\nThis computation is expressed below:\nThe soft-max function σ is applied element-wise to the output of the scaled dot product in each head. Fur -\nthermore, the contextual self-attention maps from all the leaders are concatenated to produce the contextual \nmulti-head self-attention distribution ϕCMSA( ´qo) , which is given by:\nThis process enables the model to capture relationships and dependencies within the input patches. In addi-\ntion to this, the contextual multi-head self-attention distribution ϕCMSA( ´qo) is combined with qo , and the \nresulting embeddings are normalised and fed into the normalised feedforward block, which generates the T-1 \nlatent projections (pT 1).\nThis process aims to generate more powerful and informative representations of the input data, which subse-\nquent components in the model can further process. After applying the learnable feed-forward function φf (:) , \nthe resultant embeddings are normalised and passed through the normalised feedforward block to generate \nT-1 latent projections (pT 1) . These projections are then passed to T-2, which produces pT 2 similarly. pT 2 is then \npassed to the T-3 encoder, which generates pT 3 projections. Here, pt = pT 3 . These projections are fused with fe \nto produce fd  . Finally, fd  is passed to the decoder block to extract the instances of tomato objects.\nEncoder\nThe encoder block in E is responsible for creating the latent feature distribution fe (x ) from the input tomato \nimages xǫRRxCXCh , where R represents rows, C  represents columns, and Ch represents channels of x . Unlike \ntraditional pre-trained networks, E ’s encoder comprises five levels. (E-1 to E-5), each with three to four shape \npreservation and residual blocks. These blocks empower the encoder to generate precise contextual and semantic \nrepresentations of the targeted items during image decomposition while concurrently producing distinct feature \nmaps. The encoder consists of 11 shape preservation blocks (SPBs) and five residual blocks (RBs), each with \nfour convolutions, four batch normalisations (BNs), two ReLUs for SPBs, and three convolutions, three BNs, \ntwo ReLUs, and one max pooling for RBs. The encoder’s learned latent features ( fe ), after being fine-tuned, are \neffective in distinguishing the maturity level of one tomato from another. However, they may also produce false \npositives when differentiating between occluded regions of tomato objects, as their features are highly correlated. \nTo mitigate this issue, the authors convolve fe with the transformer projections pt to enhance the distinction of \ninter-class distributions. The resulting fused feature representations fd = fe ∗pt enhance similarities between fe \nand pt , suppressing heterogeneous representations and significantly reducing false positives. fe is convolved with \npt to produce fd  , forwarded to the decoder. Convolution is a mathematical operation transforming one sequence \nusing another, often termed an image, signal, or feature vector as the first input, and a filter as the  second50. In \nthe expression fd = fe ∗pt , pt acts as a filter transforming the fe feature vector to yield fd  . These fused features \nthen pass to the decoder, reconstructing the input image with segmented tomatoes.\n(1)qo =[ lt(x p\n0 );(lt(x p\n1 ));... ;lt(x p\nn p −1 )]+[ fp (x e\n0 );fp (x e\n1 );... ;fp (x e\nn p −1 )],\n(2)qo =[ q0;q1;... ;qnp − 1)]:\n(3)A j(´qjo;Q j, Kj, V j) = σ\n(\nQ jK T\nj√\nl\n)\nV j,\n(4)ϕCMSA( ´qo) =[ A0(´qo\nj;Q0;K0;V0);A1(´qo\nj;Q0;K0;V0);... ;Ah− 1(´qjh−1 ;Qh− 1;Kh− 1;Vh− 1)]\n(5)pT1 = φf((ϕCMSA (´q0) + qo)´) + (ϕCMSA (´q0) + qo)´\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nDecoder\nThe decoder block comprises several components that work together to segment tomato objects. It consists of 11 \nmaximum unpooling layers, five rescaling layers, and a softmax layer. The unpooling layer plays a crucial role in \nrecovering the spatial information lost during encoding. These layers help restore the original size and shape of \nthe segmented objects. Each rescaling layer has a convolutional layer, batch normalization, and ReLU activation. \nSkip connections are also established between the encoder and decoder blocks to address the degradation prob-\nlem that can occur during the segmentation of tomato objects. These connections enable the flow of information \nfrom earlier layers in the network to later layers. By doing so, the network can utilize low-level features from the \nencoder to refine and enhance the segmentation results in the decoder. Following the successful segmentation \nprocess, a softmax layer is applied. This layer assigns each pixel in the segmented image to one of the tomato \nobject categories based on its estimated maturity level. The softmax function computes the probability distribu-\ntion over the categories, ensuring that each pixel is assigned to the most appropriate category. In conclusion, the \nproposed framework leverages the strengths of the encoder, transformer, and decoder blocks to achieve precise \nsegmentation and grading of tomato maturity levels. The model efficiently collects spatial information, captures \nrelationships among input patches, and enhances the differentiation between different tomato grades by utilizing \nlearned latent features, contextual multi-head self-attention processes, and feature representation fusion. The \ndecoder block refines the segmentation results and generates precise classifications with its unpooling layers, \nrescaling layers, and skip connections.\nProposed Lt loss function\nDuring the training phase, the model is constrained by the proposed loss function, referred to as L t , which \nidentifies and extracts tomato objects from input images. The L t loss function comprises two components: Ls1 \nand Ls2 . By integrating these sub-objectives into the loss function, the model can be trained and subjected to a \nmore extensive array of potential network defects. This approach proves particularly useful when dealing with an \nimbalanced distribution of background and foreground pixels in the input scan, as it often leads to significantly \nsmaller defect regions than the background region. In such cases, Ls1 effectively minimises errors at the pixel \nlevel, enabling the model to perform segmentation tasks despite the imbalanced distribution of pixels.\nHowever, attaining convergence through Ls1 presents challenges due to the possibility of the gradient of \nLs1 to overshoot when the predicted logits and ground truths have smaller values. To mitigate this issue, Ls2 is \nintroduced into the L t loss function, allowing the model to converge even when dealing with smaller values of \npredicted logits and ground truths. Moreover, the balance between Ls1 and Ls2 within L t is controlled by the \nhyperparameters β1 and β2 . Mathematically, the objective functions can be expressed as follows:\nwhere\nand\nThe notation used in the context is as follows: T sei, j denotes the ground truth label for the ith sample belong-\ning to the jth tomato classes, namely full ripe, half ripe, and green. p (Li, jse,τ ) indicates the predicted probability \ndistribution obtained from the output logit Li, jse ,τ for the ith sample and jth net defects category. This probability \ndistribution is generated using the softmax function, and τ is a temperature constant used to soften the prob-\nabilities, ensuring robust learning of tomato classes. bs signifies the batch size. c  se represents the total number \nof classes, corresponding to the different tomato maturity levels considered.\nInformed consent\nThis study does not involve any human. In this study, plants were not directly used or cultivated.\nDatasets\nThis study leverages three different datasets, namely KUTomaData, Laboro  Tomato51, and  Rob2Pheno52, to \naddress various aspects of the research. Each dataset serves a specific role in contributing to the overall objec -\ntives of the study. Below, the authors provide detailed explanations for the characteristics and purposes of each \ndataset employed in this investigation.\nKUTomaData\nThis dataset was collected from greenhouses in Al Ajban, Abu Dhabi, United Arab Emirates, and we have named \nit KUTomaData. This dataset consists of approximately 700 images. The participants used mobile phone cameras \nto capture imagery from these greenhouses. The dataset encompasses three distinct types of tomatoes: green, \nhalf-ripe, and fully ripe. The ripening stages are classified into three categories:\n(6)L t = β1 L s1 + β2 L s2 ,\n(7)L s1 = 1\nbs\nbs−1�\ni=0\n\n1 −\n2 �cse−1\nj=0 T se\ni,jp(Lse,τ\ni,j )\n�cse−1\nj=0\n�\n(T se\ni,j)2 + p(Lse,τ\ni,j )2\n�\n\n,\n(8)L s2 =− 1\nbs\nbs−1∑\ni=0\ncse−1∑\nj=0\nT se\ni,j log(p(Lse,τ\ni,j )).\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nFully ripened\nThis category represents tomatoes that have reached their optimal ripeness and are ready to be harvested. They \nexhibit a uniform red colouration, with at least 90% of the tomato’s surface filled with red colour.\nHalf ripened\nTomatoes in this category are in a transitional ripening stage. They appear greenish and require more time to \nripen fully. Typically, these tomatoes are red on 30–89% of their surface.\nUn-ripened\nThis category encompasses tomatoes in the early ripening stages. They are predominantly green or white, with \noccasional small patches of red. These tomatoes have less than 30% of their surface filled with red colour.\nThe authors included images with varying hues, textures, and occlusion backdrops to ensure the dataset accu-\nrately mirrored real-world conditions. The complexity of the dataset is heightened by the diverse backgrounds of \nthe images, which exhibit varying densities and hues of tomatoes and leaves. This variability in the background \ncomposition adds intricacy to the dataset, making it more challenging and representative of real-world scenarios. \nThe other challenging factors, such as complex environments, different lighting conditions, occlusion, and varia-\ntions in tomato maturity levels and densities, were deliberately incorporated to ensure that the dataset accurately \nrepresents most real-world situations.\nThe images presented in Fig.  2 provide a visual presentation of the complexity of the dataset, with intricate \nbackdrops for each tomato category and diverse illuminations and stages in most images. This comprehensive \nand challenging dataset is suitable for training and testing the model’s performance under realistic conditions.\nLaboro Tomato: instance  segmentation51\nThe Laboro Tomato dataset is a valuable collection of images that provides an in-depth exploration of the growth \nstages of tomatoes as they undergo the ripening process. With a total of 1005 images, the dataset comprises 743 \nimages for training and 262 images for testing. The dataset is curated to cater specifically to object detection and \ninstance segmentation tasks, making it highly suitable for our research area. One notable aspect of the Laboro \nTomato dataset is the inclusion of two distinct subsets of tomatoes, which are categorized based on size. This \ncategorization adds an additional dimension to the dataset, allowing researchers to investigate the impact of \ntomato size on the performance of object detection and instance segmentation models.\nTo ensure the dataset’s diversity and real-world relevance, the images were captured using two separate cam-\neras, each with its unique resolution and image quality. The usage of different cameras introduces variations in \nimage characteristics, such as colour rendition and sharpness, which can challenge the performance of computer \nvision models and better simulate real-world scenarios.\nRob2Pheno annotated  tomato52\nAfonso et al.52 conducted a research study focused on tomato fruit detection and counting in greenhouses using \ndeep learning techniques. For this purpose, they utilized the Rob2Pheno Tomato dataset, which comprises \nRGB-D images of tomato plants captured in a production greenhouse setting. The images in this dataset were \nacquired using real-sense cameras, which can capture both colour information and depth data. This additional \ndepth information offers a three-dimensional perspective of the scene, providing valuable spatial context to the \ndataset.\nMoreover, the Rob2Pheno Tomato dataset includes object instance-level ground truth annotations of the \nfruit. These annotations precisely identify the location and boundaries of individual tomato fruits within the \nimages. Regarding data volume, the dataset consists of 710 images for training purposes and 284 images for \ntesting purposes. Data augmentation methods were applied during the training phase to enhance the dataset’s \ndiversity and improve the generalization ability of the models.\nThis paper presents a novel segmentation approach to extract and grade tomato maturity levels using RGB \nimages acquired under various lighting and occlusion conditions. Upon understanding the textures of the tomato \nplant, the proposed framework isolates the critical parts of the tomato fruit, such as the colour, shape, and size \nof tomatoes. The block diagram of the proposed framework is shown in Fig.  1, where the authors can observe \nthat it is composed of an encoder, transformer, and decoder blocks.\nExperiments\nThe proposed framework was tested using a dataset from a nearby greenhouse farm in Ajban, Abu Dhabi, UAE. \nThe dataset comprises time-linked frames that can be employed to identify tomatoes at different maturity levels. \nThe authors employed meticulous manual annotations using the Matlab data annotations tool to ensure accurate \nand reliable annotations. Skilled participants used mobile phone cameras to capture tomato images from the \ngreenhouses, and each image was then carefully annotated to identify the ripening stage and other relevant attrib-\nutes. This annotation process guarantees high-quality and precise labelling, making KUTomaDATA suitable for \nvarious computer vision tasks. Specifically, the authors annotated approximately 700 images of the KUTomaData \ndataset for three maturity levels, i.e., Unripped, Half ripened and Full ripened tomatoes, and Table  1 indicates \nthe number of occurrences of each class in this dataset.\nTo ensure model robustness, 75% of the annotated images were used for training, whereas the remaining 25% \nwas allocated for validation and testing. During the training phase, the number of epochs and the batch size were \nset to 200 and 16, respectively. After each epoch, the trained model was evaluated against the validation dataset. \nThe loss and mIoU curves are presented in Fig. 3.\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nNumerous experiments were carried out to assess the proposed method’s effectiveness. One of these experi-\nments involved using a test set to assess the model’s ability to make accurate predictions under various lighting \nconditions, occlusion levels, and viewing angles. The segmentation quality was evaluated by calculating the \nDice coefficient and the mean intersection over union (mIoU). These metrics assessed the accuracy and overlap \nbetween the predicted segmentation masks and the ground truth annotations. The Dice coefficient and mean \nIoU are valuable metrics in gauging the performance and quality of segmentation algorithms, offering comple-\nmentary insights into the correctness and overlap of the segmentation results. To evaluate models using mAP , \nwe employ a method where we build bounding boxes from semantic segmentation ground truths and compute \nminimum bounding rectangles around segmented objects. This technique ensures the creation of bounding \nboxes precisely fitting the geometry of segmented objects by determining the smallest rectangle containing the \nentire object. Bounding box limits are determined by finding the least and highest row and column indices where \nFigure 2.  The dataset of tomato images contains samples of tomatoes captured in different stages of ripeness \nand under varying lighting conditions and occlusion. The images in the dataset are organized into three \ncolumns. The first column showcases unripened tomatoes, the second column shows half-ripe and unripened \ntomatoes, and the third column presents fully-ripened tomatoes with some half-ripened and some unripened \ntomatoes. This division allows for clear differentiation and visual representation of the different ripeness stages \nof the tomatoes in the dataset.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nthe segmentation mask is ‘True’ , accurately depicting identified regions in space by closely following segmented \nobject contours. We extend this approach to evaluate both the bounding boxes of the model’s output and the \nmAP from the bounding boxes of the ground truths and the test dataset. In this work, we compute mAP scores \ndirectly from segmentation masks rather than bounding boxes (detections). For each segmentation mask, we \nextract its minimum and maximum extents to obtain x, y, width, and height information, allowing us to fit a \nbounding box around the mask. Additionally, we calculate the average confidence scores of each mask pixel and \nuse the mask label, average confidence score, and the fitted bounding box (derived from the segmentation mask) \nto compute the mAP score. As the mAP score is directly derived from the segmentation mask, the quality of the \nsegmentation mask significantly influences the computed mAP score.\nExperimental setup\nThe suggested framework has been trained on a system comprising a Core i9-10940 processor running at 3.30 \nGHz, with 128 GB of RAM, and a single NVIDIA Quadro RTX 6000 GPU. The GPU has the CUDA toolkit \nversion 11.0 and cuDNN version 7.5. The development of the proposed model was carried out using Python \n3.7.9 and TensorFlow 2.1.0. During the training process, the model was trained for 200 epochs, each consisting \n(a) Loss curves for Proposed Model (Our), UNet, PSPNet, andS egNet.\n(b) Accuracy curves for Proposed Model( Our), UNet, PSPNeta nd SegNet.\nFigure 3.  Sub-figures (a) and (b) depict the loss and accuracy curves, respectively, for several network models \nduring both training and validation stages. The models include the proposed model (Our), UNet, PSPNet, and \nSegNet.\nTable 1.  The proposed model was trained, tested, and evaluated using the KUTomaData dataset, which \nconsists of images for each class: unripened tomatoes, half-ripened tomatoes, and full-ripened tomatoes. The \nrespective number of occurrence in all the images is mentioned here.\nClass label No. of occurrences\nUnripened tomato 3557\nHalf ripen tomato 196\nFull ripen tomato 724\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nof 512 iterations. The ADADELTA optimizer was employed, utilizing default values for the learning rate (1.00) \nand decay rate (0.95).\nData augmentation\nDeep convolutional neural network (DCNN) models typically require a substantial number of training images \nto achieve high accuracy in predicting ground truth labels. However, there are instances where certain classes \nmay have limited images, posing a challenge in effectively training the model. Data augmentation techniques \nare employed to augment the available images and expand the training dataset to tackle this issue. In our study, \nthe authors employed data augmentation techniques, as described  in53, to generate additional variations from \nthe existing images for classes with limited samples, particularly for maturity-level classes. These augmenta-\ntion techniques include blurriness, rotation, horizontal and vertical flipping, horizontal and vertical shearing, \nand adding noise. Figure 4 illustrates an example of image augmentation. By incorporating this technique, the \nauthors increased the number of images in our dataset, thereby enhancing the model’s robustness during the \ntraining phase of the CNN.\nResults\nIn this section, the authors present both qualitative and quantitative results of our experiments, evaluating the \nperformance of each model using several key metrics, including intersection over union ( µIoU), dice coefficient \n( µDC), mean average precision (mAP), and area under the curve (AUC). These evaluation metrics provide \ncomprehensive insights into the effectiveness and accuracy of the models in various aspects.\nIn the following section, an explanation of the theoretical aspects related to network selection is provided. \nThis sub-section aims to provide a comprehensive understanding of how network selection was done underlying \nthe principles involved in the process.\nComparison with conventional segmentation models\nFigure  5 shows tomatoes in a cluttered and occluded environment where the difficulty lies in detecting the \nunripened tomatoes within same-coloured leaves. This presents a scenario where mobile robots can capture \nthe image and identify the tomatoes. The authors thoroughly assess the proposed framework on the collected \ndataset. Furthermore, the authors also report its comparative evaluation with state-of-the-art segmentation \nmodels. Figure 5 shows the cluttered situation in an indoor greenhouse where multiple tomato vines can be seen. \nMoreover, the qualitative evaluation of the proposed architecture and its comparison with the state-of-the-art \nsegmentation models (such as  SegFormer54,  PSPNet55,  SegNet56 and U-Net41) on the dataset is presented in Fig. 5.\nTable 2 represents the quantitative performance of the proposed framework compared to the state-of-the-art \nnetworks. It can be seen that the proposed model outperforms the other models in terms of evaluation metrics. \nThe proposed incremental instance segmentation scheme was compared with various popular transformer, scene \nparsing, encoder-decoder, and fully convolutional-based models, such as  SegFormer54,  SegNet56, U-Net41, and \nFigure 4.  Here are some examples of data augmentation techniques: (a) original image, (b) random brightness, \n(c) horizontal flip, (d) random rotation, (e) salt and pepper, (f) speckle effect, (f) vertical variation, and (f) zoom \nvariation.\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\n PSPNet55. As shown in Table 2, this table compares the performance metrics of four different models (Proposed \n(Our), SegFormer, SegNet, U-Net, and PSPNet) on the task of tomato segmentation.\nThe compared metrics are F1 Score, dice coefficient, mean Intersection over Union (IoU), and class-wise IoU. \nThe dice coefficient is a statistical measure of the overlap between two sets of data—in this case, the predicted \nand actual tomato segmentation masks. The Mean IoU measures how well the model can accurately segment \nFigure 5.  The authors compared the proposed framework with the best existing models to evaluate how well it \nwould work. Here, the raw test images from our dataset are displayed in Column 1, the ground truth labels are \ndisplayed in Column 2, the results of the proposed framework are displayed in Column 3, and those of PSPNet, \nSegNet, and UNet are displayed in Columns 4–6, respectively.\nTable 2.  The performance of the proposed framework was compared to state-of-the-art frameworks using the \nKUTomaData dataset. Significant values are in [bold]. The leading results are highlighted in bold, while the \nsecond-best scores are underlined.\nModel µDC µIoU\nClasswise IoU\nUnriped Half-riped Fully riped\nProposed (Our) 0.7685 0.6241 0.7395 0.6028 0.3262\nSegFormer54  0.7297  0.5745  0.6391  0.3969  0.2800\nSegNet56 0.5728  0.4104  0.6288 0.0001 0.0002\nUNet41 0.5475 0.3769 0.5422 0.0016 0.0017\nPSPNet55 0.5504 0.3797 0.5123 0.0320 0.0001\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nthe tomato regions in the images. The class-wise IoU shows the IoU score for each of the three tomato ripeness \nclasses: unripe, half-ripe, and fully-ripe. The results show that the proposed model outperforms other models in \nall metrics, achieving a Dice coefficient of 0.7326 and a mean IoU of 0.6641. The proposed model also achieves \nhigher class-wise IoU scores for all three tomato ripeness classes, indicating that it is better at accurately seg-\nmenting each class.\nThe proposed model outperformed SegFormer, SegNet, U-Net, and PSPNet models across all metrics. The \nSegNet and U-Net models exhibited significantly poorer performance, achieving the lowest scores in all metrics. \nTheir Dice coefficients were 0.5728 and 0.5475, and mean IoU values were merely 0.4104 and 0.3769, respectively. \nThe data suggest that the proposed model is highly effective in accurately segmenting tomato regions in images, \noutperforming other commonly used segmentation models for this particular task. In addition to quantitative \nevaluations, a qualitative comparison was performed between the proposed convolutional transformer segmenta-\ntion framework and other existing segmentation models. The results, illustrated in Fig. 5, demonstrate that while \nall the examined segmentation models successfully localize tomato data through masks, substantial variation \nexists in the quality of the generated masks across different methods. Notably, our proposed framework exhibits \nexceptional accuracy in producing precise tomato masks.\nMoreover, when considering the extraction of tomatoes at various maturity levels, the capabilities of the \nproposed convolutional transformer model become evident. Our framework stands out due to its distinctive \nability to generate shape-preserving embeddings and to effectively leverage self-attention projections. This unique \nattribute enables the framework to achieve effective segmentation, even in the presence of occluded tomato data, \nsurpassing the performance of state-of-the-art methods in this domain.\nQuantitative evaluations\nTable 2 presents a quantitative comparison of different models based on various evaluation metrics for tomato \nsegmentation. These metrics include the Dice Coefficient, Mean IoU (Intersection over Union), and Classwise \nIoU (IoU for different tomato ripeness classes). The first row represents the proposed model, labelled as “Our” , \nwhich achieved a remarkably high Dice Coefficient of 0.7326 and a mean IoU of 0.6641. The Classwise IoU \nvalues for the “Unripened” , “Half-Ripened” , and “Fully Ripened” classes are also noteworthy, with IoU scores \nof 0.7395, 0.6028, and 0.3262, respectively. Comparing the proposed model to other state-of-the-art models, a \nDice Coefficient of 0.6602 and a mean IoU of 0.5745. However, its Classwise IoU scores for all three ripeness \nclasses are lower than the proposed model’s. The SegNet model obtained a Dice Coefficient of 0.5728 and a mean \nIoU of 0.4104. Its Classwise IoU scores for the “Unripened” and “Half-Ripened” classes are higher than those of \nother models, but it performs poorly for the “Fully Ripened” class. The UNet model achieved a Dice Coefficient \nof 0.5475 and a mean IoU of 0.3769. Similar to SegNet, it demonstrates better performance for the “Unripened” \nand “Half-Ripened” classes but struggles with the “Fully Ripened” class.\nFinally, the PSPNet model obtained a Dice Coefficient of 0.5504 and a Mean IoU of 0.3797. Its Classwise IoU \nscores for the “Unripened” and “Half-Ripened” classes are relatively higher, but it performs poorly for the “Fully \nRipened” class. Overall, the proposed model outperforms the other models regarding the Dice Coefficient, mean \nIoU, and Classwise IoU for different tomato ripeness classes. The results highlight the effectiveness and superiority \nof the proposed model in accurately segmenting tomatoes of varying ripeness levels.\nQualitative evaluations\nFigure 6 presents a rigorous qualitative assessment of the proposed framework alongside state-of-the-art meth-\nods, primarily focusing on the accuracy of tomato segmentation. The objective is to comprehensively evaluate \nthe performance of the proposed framework against existing approaches when dealing with real-world scenarios.\nThe quantitative analysis of these models is shown in Table 3.\nIn Column (A) of Fig.  6, the ground truth annotations are visually overlaid on the corresponding actual \nimages. A distinctive colour scheme is employed to signify different maturity grades: cyan for fully-ripened \ntomatoes, pink for half-ripened tomatoes, and yellow for unripe tomatoes. This column is a reliable reference \nfor assessing the expected quality of segmentation. Column (B) showcases the exceptional results of the pro-\nposed convolutional transformer model. The segmentation outcomes achieved by the framework demonstrate its \nremarkable efficacy in accurately classifying and segmenting tomatoes of three maturity grades, even in scenarios \nwith challenging factors such as occlusion and variable lighting conditions. Columns (C) to (H) provide a meticu-\nlous comparative analysis of other state-of-the-art methods, namely  SETR57,  Segformer54,  DeepFruits58,  COS59, \n CWD60, and  DLIS61. Each column represents a distinct method, illustrating the segmentation results attained by \nthe respective approaches. This thorough evaluation facilitates a meticulous examination and meaningful com-\nparisons of the techniques, leading to the identification of the most effective segmentation model for tomatoes.\nIt is also evident from Fig. 6 that the proposed framework consistently outperforms state-of-the-art methods \nin accurately extracting tomatoes of different maturity grades. The segmentation results obtained by the proposed \nmethod exhibit superior accuracy, robustness, and the ability to precisely classify and delineate fully-riped, \nhalf-riped, and unripe tomatoes, even in challenging conditions. Conversely, the qualitative analysis of alterna-\ntive methods reveals varying performance levels, with specific approaches struggling to delineate the distinct \nmaturity grades accurately.\nAblation study\nThe authors conduct an ablation study in this section to pinpoint the optimal hyperparameters and backbone \nnetworks that yield the most favourable outcomes across various datasets. The first set of ablation experiments \nfocused on identifying optimal β parameters that produce the best recognition performance of the proposed \nframework. The second set of experiments aimed to identify the optimal network backbone. Several backbone \n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\narchitectures were evaluated and compared to discern the architecture that yielded maximum accuracy and \nquality segmentation. The objective of the third series of experiments was to determine the optimal value for \nthe parameter τ . By varying τ and evaluating the model’s performance, the authors established the threshold that \nmaximized detection accuracy while minimizing false positives and negatives. The fourth ablation experiment \naimed to identify the optimal loss function for the proposed model by comparing it to other state-of-the-art loss \nfunctions, including soft nearest neighbour loss, focal Tversky loss, dice-entropy loss, and conventional cross-\nentropy loss. The fifth series of ablation experiments was related to comparing the segmentation performance \nof the proposed model against state-of-the-art networks.\nOptimal β values in Lt\nThe first set of ablation experiments aimed to determine the optimal hyper-parameters β1,2 in the L t loss func-\ntion, which would result in the best segmentation performance across different datasets. To explore this, the \nauthors varied the value of β1 from 0.1 to 0.9 in increments of 0.2. For each β1 value, the authors calculated β2 \nas β2 = 1 − β1 . Subsequently, the proposed model was trained using each combination of β1 and β2 . During the \ninference stage, the model’s segmentation performance for each combination was evaluated across the datasets, \nutilizing mAP scores as the evaluation metric (as shown in Table 4).\nThe results revealed that the proposed framework performs better when assigning a higher weight to β1 , par-\nticularly with a value of 0.9 in this specific instance. For example, with β1 = 0.9 and β2 = 0.1 , the proposed model \nachieved mAP scores of 0.5814, 0.6542, and 0.6639 across the three datasets: KUTomaData, Laboro Tomato, and \nRob2Pheno Annotated Tomato respectively. Based on these findings, a combination of β1 = 0.9 and β2 = 0.2 was \nselected for subsequent experiments to train the proposed model. This choice of hyperparameters was deemed \noptimal based on the earlier evaluations and resulted in favourable model performance.\nOptimal encoder backbone\nThe second set of ablation experiments aimed to determine the optimal network backbone for segmenting and \ndetecting tomato objects. The model has been designed to effectively integrate with several convolutional neural \nnetwork (CNN) backbones for the encoder. To achieve this, the authors integrated various pre-trained mod-\nels, including  HRNet62, Lite-HRNet63, EfficientNet-B464, DenseNet-20165, and ResNet-10166, into the proposed \nmodel. The authors then compared their performance against the proposed backbone specifically designed for \ntomato object detection and segmentation. The results obtained from the conducted experiments are displayed \nin Table 5. Upon examining Table 5, the proposed encoder outperformed the state-of-the-art models, surpass -\ning them by 3.22%, 2.51%, 3.67%, and 0.56% in terms of µIoU, µDC, mAP , and AUC scores, respectively, on the \nKUTomaData dataset.\nMoreover, when considering the Laboro dataset, the proposed framework exhibited performance improve-\nments of 2.27%, 1.60%, 2.61%, and 2.25% in terms of µIoU, µDC, mAP , and AUC scores, respectively. Similarly, \non the Rob2Pheno dataset, the proposed model achieved gains of 3.68%, 2.50%, 3.71%, and 1.25% in µIoU, µ\nTable 3.  Quantitative evaluation of the proposed framework with state-of-the-art methods in terms of µIoU, \nµDC, mAP , and AUC scores across KUTomaData, Laboro, and Rob2Pheno datasets.\nDataset Method µIoU µDC mAP AUC \nKUTomaData\nProposed 0.6241 0.7685 0.5814 0.7381\nSETR57 0.5923 0.7439 0.5382 0.7069\nSegFormer54 0.5745 0.7297 0.5176 0.6843\nDeepFruits58 0.4668 0.6364 0.4368 0.6209\nCOS59 0.4837 0.6520 0.4562 0.5968\nCWD60 0.5096 0.6751 0.4739 0.6027\nDLIS61 0.5485 0.7084 0.5173 0.6391\nLaboro\nProposed 0.6946 0.8197 0.6542 0.7419\nSETR57 0.6529 0.7900 0.6083 0.7346\nSegFormer54 0.6387 0.7795 0.5856 0.7068\nDeepFruits58 0.5162 0.6809 0.4602 0.5834\nCOS59 0.5243 0.6879 0.4728 0.5812\nCWD60 0.5576 0.7159 0.5116 0.6185\nDLIS61 0.5865 0.7393 0.5394 0.6527\nRob2Pheno\nProposed 0.7341 0.8466 0.6639 0.8253\nSETR57 0.6856 0.8134 0.6204 0.7261\nSegFormer54 0.6738 0.8151 0.6325 0.7524\nDeepFruits58 0.5967 0.7474 0.5315 0.6403\nCOS59 0.6149 0.7615 0.5628 0.6752\nCWD60 0.6424 0.7822 0.5935 0.7124\nDLIS61 0.6573 0.7932 0.6176 0.7492\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nFigure 6.  Qualitative evaluation of the proposed framework alongside state-of-the-art methods to extract \ndifferent maturity grades of tomatoes under occlusion and variable lighting conditions. Column (A) represents \nthe ground truth overlaid on the actual image, where cyan represents ripe tomatoes, pink represents half-ripe \ntomatoes, and yellow highlights unripe tomatoes. Column (B) shows the outcome of the proposed method, \nwhile Columns (C—H) display the qualitative analysis for  SETR57,  Segformer54,  DeepFruits58,  COS59,  CWD60 \nand  DLIS61, respectively.\nTable 4.  The objective is to determine the optimal values of β1,2 that yield the highest segmentation \nperformance. Significant values are in [bold].\nKUTomaData β1 0.1 0.3 0.5 0.7 0.9\nβ2\n0.1 – – – – 0.5814\n0.3 – – – 0.5352 –\n0.5 – – 0.4836 – –\n0.7 – 0.4582 – – –\n0.9 0.4031 – – – –\nLaboro β1 0.1 0.3 0.5 0.7 0.9\nβ2\n0.1 – – – – 0.6542\n0.3 – – – 0.5894 –\n0.5 – – 0.5351 – –\n0.7 – 0.4953 – – –\n0.9 0.4406 – – – –\nRob2Pheno β1 0.1 0.3 0.5 0.7 0.9\nβ2\n0.1 – – – – 0.6639\n0.3 – – – 0.6074 –\n0.5 – – 0.5692 – –\n0.7 – 0.5106 – – –\n0.9 0.5739 – – – –\n15\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nDC, mAP , and AUC scores, respectively. These notable performance improvements can be attributed to utilizing \na novel butterfly structure in the proposed encoder backbone. In contrast to traditional encoders, the proposed \nencoder can maintain the high-resolution features of the candidate input by summing feature maps across each \ndepth in a butterfly manner via upsampling and downsampling the kernel sizes as needed. Moreover, each block \nwithin the proposed network consists of custom identity blocks (IB), hierarchical decomposition blocks (HDB), \nand shape-preservation blocks (SPB). These blocks refine the attention of the model so that it only focuses on the \ndefected regions, irrespective of the scan’s textural and contextual attributes. The model acquires the ability to \nextract distinctive latent characteristics from the input images by adding this integration, resulting in improved \nperformance in tomato object segmentation and classification tasks. This advancement outperforms the capa -\nbilities of current cutting-edge models, such as  HRNet62, Lite-HRNet63, EfficientNet-B464, DenseNet-20165, and \nResNet-10166. It is important to note that while the proposed scheme is computationally expensive compared to \nLite-HRNet63, its superior detection performance justified its selection for generating distinct feature representa-\ntions in the subsequent experiments. This decision was driven by the primary objective of achieving the highest \npossible detection performance.\nDetermining the optimal temperature constant\nIn the proposed L t loss function, the temperature constant ( τ ) serves as a hyperparameter that softens the target \nprobabilities. Using a higher value of τ , the model becomes more receptive to recognising tomato object segmen-\ntation and detection regardless of the input imagery characteristics. This softening effect enhances the detection \nand segmentation performance by enabling the model to comprehend the target probabilities more broadly.\nIn the fourth set of ablation experiments, the authors aimed to determine the optimal value for τ to extract \ntomato objects accurately. To achieve this, the authors varied the value of τ from 1 to 2.5 in increments of 0.5 \nwithin the L t loss function while training the proposed model across each dataset. After completing the train-\ning process, in the inference stage, the authors assessed the performance of the proposed framework in tomato \nobject segmentation and detection on each dataset. The outcomes of these evaluations are showcased in the \nprovided Table 6. From Table 6, it can be observed that increasing the value of τ from 1 to 1.5 led to a significant \nperformance boost across all four datasets. For instance, on the KUTomaData dataset, the proposed framework \nachieved performance improvements of 4.12% in terms of µIoU, 3.21% in terms of µDC, 1.65% in terms of mAP , \nand 1.25% in terms of AUC scores. Similarly, on the Laboro dataset, it achieved performance improvements \nof 2.87% in µIoU, 2.03% in µDC, 3.58% in mAP , and 1.88% in AUC scores. Furthermore, experiments on the \nRob2Pheno Annotated dataset showed performance improvements of 1.88% in µIoU, 1.26% in µDC, 2.12% in \nmAP , and 1.85% in AUC scores.\nIt is important to note that increasing τ does not always result in performance improvements. When the \nauthors increased the value of τ from 1.5 to 2 and from 2 to 2.5, the proposed framework’s effectiveness deterio-\nrated. This decline in performance can be attributed to the fact that when τ exceeds a certain threshold, it loses \nits ability to accurately differentiate between logits representing different categories, such as green, half-ripen \nand fully-ripen and the background, within the input imagery.\nTable 5.  To identify the most suitable backbone network for performing tomato object detection and \nsegmentation tasks across all datasets, a comprehensive evaluation was conducted. The models were evaluated \nusing an input size of 540 × 640 × 3.\nDataset Backbone µIoU µDC mAP AUC Params\nKUTomaData\nProposed 0.6241 0.7685 0.5814 0.7381 52.4M\nLite-HRNet63 0.5753 0.7304 0.5365 0.7168 7.43M\nHRNetv262 0.5916 0.7434 0.5447 0.7325 52.1M\nEfficientNetB464 0.5324 0.6948 0.5102 0.6762 22.3M\nDenseNet-20165 0.5672 0.7238 0.5283 0.6831 85.6M\nResNet-10166 0.5384 0.6999 0.4951 0.6676 84.2M\nLaboro\nProposed 0.6946 0.8197 0.6542 0.7498 54.3M\nLite-HRNet63 0.6653 0.7990 0.6173 0.7208 8.94M\nHRNetv262 0.6719 0.8037 0.6281 0.7483 51.2M\nEfficientNetB464 0.6582 0.7938 0.6017 0.7219 23.6M\nDenseNet-20165 0.6625 0.7969 0.6246 0.7394 87.5M\nResNet-10166 0.6503 0.7880 0.6128 0.7153 86.5M\nRob2Pheno\nProposed 0.7341 0.8466 0.6639 0.8253 58.3M\nLite-HRNet63 0.6824 0.8112 0.6143 0.8029 9.72M\nHRNetv262 0.6973 0.8216 0.6268 0.8128 53.7M\nEfficientNetB464 0.6782 0.8082 0.6042 0.7953 22.1M\nDenseNet-20165 0.6856 0.8134 0.6194 0.8058 86.4M\nResNet-10166 0.6675 0.8006 0.5918 0.7942 85.3M\n16\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nConsidering the optimal detection results achieved with τ = 1.5 for the proposed framework on each dataset, \nthe authors chose to train the model with τ = 1.5 for the remaining experiments. This selection ensures consist-\nent and effective performance throughout the subsequent experimentation.\nOptimal loss function\nThe fifth set of ablation experiments focused on analysing the performance of the proposed model when trained \nusing the L t loss function compared to other state-of-the-art loss functions. These include the soft nearest neigh-\nbor loss function ( Lsn)67, the focal Tversky loss function ( L ft)68, the dice-entropy loss function ( Lde)69, and the \nconventional cross-entropy loss function ( Lce ). The results of these experiments are summarised in Table 7. From \nTable 7, it is evident that the proposed model, trained using the L t loss function, outperformed its counterparts \ntrained with state-of-the-art loss functions across all datasets. For instance, on the KUTomaData dataset, the L t \nloss function resulted in a performance improvement of 2.16% in terms of µIoU, 1.66% in terms of µDC, 2.39% \nin terms of mAP , and 2.25% in terms of AUC scores. Similarly, on the Laboro dataset, the L t loss function led \nto a performance improvement of 3.25% in terms of µIoU, 2.30% in terms of µDC, 5.58% in terms of mAP , and \n4.81% in terms of AUC scores.\nFurthermore, on the Rob2Pheno Annotated dataset, it yielded a performance improvement of 1.23% in terms \nof µIoU, 0.82% in terms of µDC, 1.16% in terms of mAP , and 1.45% in terms of AUC scores.\nThese performance improvements can be attributed to the proposed L t loss function, which leverages both \ncontextual and semantic differences within the underwater scans, effectively allowing the model to recognise \nTable 6.  To identify the optimal temperature constant ( τ ) for achieving the best tomato detection \nperformance across each dataset, the authors employed the proposed backbone encoder to generate latent \nfeatures using various values of τ.\nDataset τ µIoU µDC mAP AUC \nKUTomaData\n1 0.5829 0.7364 0.5649 0.7256\n1.5 0.6241 0.7685 0.5814 0.7381\n2 0.5783 0.7328 0.5627 0.7169\n2.5 0.5596 0.7176 0.5345 0.6812\nLaboro\n1 0.6528 0.7899 0.6013 0.7293\n1.5 0.6946 0.8197 0.6542 0.7419\n2 0.6659 0.7994 0.6184 0.7156\n2.5 0.6407 0.7810 0.6025 0.6924\nRob2Pheno\n1 0.7026 0.8253 0.6284 0.8068\n1.5 0.7341 0.8466 0.6639 0.8253\n2 0.7153 0.8340 0.6427 0.7976\n2.5 0.6938 0.8192 0.6265 0.7782\nTable 7.  To determine the optimal loss function with the best tomato object detection performance across all \nfour datasets, the authors used the proposed backbone encoder to generate the latent features when the model \nwas constrained using different loss functions. Moreover, (P) indicates theproposed L t loss function.\nDataset Loss Function µIoU µDC mAP AUC \nKUTomaData\nL t (P) 0.6241 0.7685 0.5814 0.7381\nLce 0.5516 0.7110 0.5143 0.6836\nLsn67 0.6025 0.7519 0.5575 0.7156\nL ft68 0.5773 0.7320 0.5164 0.7052\nLde69 0.5962 0.7470 0.5389 0.7109\nLaboro\nL t (P) 0.6946 0.8197 0.6542 0.7419\nLce 0.6183 0.7641 0.5423 0.6537\nLsn67 0.6621 0.7967 0.5984 0.6938\nL ft68 0.6358 0.7773 0.5667 0.6726\nLde69 0.6496 0.7875 0.5793 0.6893\nRob2Pheno\nL t (P) 0.7341 0.8466 0.6639 0.8253\nLce 0.6793 0.8090 0.6128 0.7641\nLsn67 0.6946 0.8197 0.6315 0.7869\nL ft68 0.7104 0.8306 0.6447 0.8076\nLde69 0.7218 0.8384 0.6523 0.8108\n17\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\ntomato objects regardless of input image characteristics. Consequently, the authors employed the L t loss function \nfor the remaining experiments to train the proposed model for tomato object extraction across all four datasets.\nTransformer encoder analysis\nIn this subsection, we conduct a comprehensive ablation study focused on removing the transformer encoder \ncomponent from our proposed network architecture. We aim to investigate in detail the impact of the transformer \nencoder in our fully convolutional pipeline. By systematically evaluating the model’s performance with and \nwithout the transformer encoder, we aim to clarify its crucial role in enhancing the feature extraction capabilities \nof the proposed model for our specific task.\nFrom Table 8, it is evident that the model using the transformer encoder showed notable performance gains \nacross KUTomaData, Laboro, and Rob2Pheno. More specifically, the mean Dice Coefficient (mDC) and mean \nIntersection over Union (mIoU) scores increased with the addition of the transformer block. For instance, using \nthe transformer, the mDC increased by 2.86% on KUTomaData, and the mIoU improved by 4.13%. Compara-\nble patterns were noted in the Rob2Pheno and Laboro datasets. These findings support our theory that adding \ntransformer-based designs to fully convolutional pipelines improves the model’s capacity to identify complex \nlinkages and patterns in the input. Improved semantic segmentation across datasets results from the transformer’s \nattention mechanisms, which are essential for precise object segmentation.\nDiscussion\nThe proposed framework presents a novel approach for tomato maturity level segmentation and classification \nusing RGB scans acquired under various lighting and occlusion conditions. The experimental analysis demon -\nstrates the framework’s effectiveness in segmenting and grading tomatoes based on colour, shape, and size. The \nproposed framework addresses the challenges associated with harvesting ripe tomatoes using mobile robots in \nreal-world scenarios. These challenges include occlusion caused by leaves and branches and the colour similarity \nbetween tomatoes and the surrounding foliage during fruit development. The existing literature lacks a sufficient \nexplanation of these tomato recognition challenges, necessitating the development of new approaches. To over-\ncome these challenges, a novel framework is introduced in this paper, leveraging a convolutional transformer \narchitecture for autonomous tomato recognition and grading. The framework is designed to handle tomatoes \nwith varying occlusion levels, lighting conditions, and ripeness stages. It offers a promising solution for efficient \ntomato harvesting in complex and diverse natural environments. An essential contribution of this work is the \nintroduction of the KUTomaData dataset, specifically curated for training deep learning models for tomato \nsegmentation and classification. KUTomaData comprises images collected from greenhouses across the UAE. \nThe dataset encompasses diverse lighting conditions, viewing perspectives, and camera sensors, making it unique \ncompared to existing datasets. The availability of KUTomaData fills a gap in the deep learning community by \nproviding a dedicated resource for tomato-related research. The proposed framework’s performance was evalu-\nated against two additional public datasets: Laboro Tomato and Rob2Pheno Annotated Tomato. These datasets \nwere used to benchmark the framework’s ability to extract cluttered and occluded tomato instances from RGB \nscans, comparing its performance against state-of-the-art models. The evaluation results demonstrated excep -\ntional performance, with the proposed framework outperforming the state-of-the-art models, including  SETR57, \n Segformer54,  DeepFruits58,  COS59,  CWD60, and  DLIS61, by a significant margin. A series of ablation experiments \nwere conducted to enhance the model’s effectiveness. The initial experiments focused on optimizing hyperpa -\nrameters to improve performance. Subsequently, different network backbones were compared in the second set \nof experiments to identify the architecture that achieved accurate and high-quality segmentation. The fourth set \nof experiments determined the optimal value for the parameter τ , balancing detection accuracy and minimizing \nfalse positives and negatives. The fifth set of investigations comprehensively evaluated the proposed model’s per-\nformance, considering accuracy, segmentation quality, computational efficiency, and robustness in challenging \nscenarios. The initial ablation experiments aimed to find the optimal hyperparameters β1,2 in the L t loss function \nfor achieving the best segmentation performance across different datasets. Varying β1 from 0.1 to 0.9 and calculat-\ning β2 = 1 − β1 , the model was trained and evaluated using different combinations of these values. The results \ndemonstrated that assigning a higher weight to β1 , particularly 0.9, led to superior performance. For example, \nwith β1 = 0.9 and β2 = 0.1 , the model achieved high mAP scores on the KUTomaData, Laboro Tomato, and \nRob2Pheno Annotated Tomato datasets. Based on these findings, the combination of β1 = 0.9 and β2 = 0.2 was \nchosen as the optimal hyperparameter choice for subsequent model training, resulting in favourable performance. \nVarious pre-trained models were integrated into the proposed framework for tomato object segmentation and \nTable 8.  Comparison of model variants with and without transformer encoder across datasets.\nDataset Variant µIoU µDC\nKUTomaData\nProposed (with transformer block) 0.6241 0.7685\nProposed (without transformer block) 0.5938 0.7486\nLaboro\nProposed (with transformer block) 0.6946 0.8197\nProposed (without transformer block) 0.6572 0.7931\nRob2Pheno\nProposed (with transformer block) 0.7341 0.8466\nProposed (without transformer block) 0.7059 0.8275\n18\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\ndetection in the ablation experiments and backbone analysis. The performance of these models was compared \nagainst the proposed backbone, designed explicitly for this task. The results, summarized in Table  5, clearly \ndemonstrate the superiority of the proposed encoder backbone. Compared to state-of-the-art models such as \nHRNet, Lite-HRNet, EfficientNet-B4, DenseNet-201, and ResNet-101, the proposed backbone achieved nota-\nble improvements across different evaluation metrics. On the KUTomaData dataset, it outperformed existing \nmodels by 3.22%, 2.51%, 3.67%, and 0.56% in terms of µIoU, µDC, mAP , and AUC scores, respectively. Similar \nperformance gains were observed on the Laboro and Rob2Pheno datasets, with improvements ranging from 1.60 \nto 3.68% in various evaluation metrics. These significant improvements can be attributed to integrating a novel \nbutterfly structure in the encoder backbone, incorporating distinctive SPB, IB, and HDB blocks. This integration \nenables the model to extract unique latent characteristics from input images, improving performance in tomato \nobject segmentation and classification tasks. Despite the higher computational cost compared to Lite-HRNet, \nthe selection of the proposed scheme was justified by its superior detection performance. The primary objective \nof achieving the highest possible detection performance drove this decision. Integrating the butterfly structure \nand distinctive blocks enables the model to capture essential features and accurate tomato object delineation.\nThe proposed L t loss function incorporates a temperature constant ( τ ) as a hyperparameter to soften target prob-\nabilities, improving tomato object segmentation and detection. Adjusting τ makes the model more receptive to \nrecognizing tomato objects, independent of input imagery characteristics. This softening effect allows the model \nto comprehend target probabilities better, resulting in enhanced performance. Varying τ from 1 to 2.5 during \ntraining, experiments revealed that increasing τ from 1 to 1.5 led to significant performance improvements across \ndatasets. For instance, on the KUTomaData dataset, improvements of 4.12% in µIoU, 3.21% in µDC, 1.65% in \nmAP , and 1.25% in AUC scores were achieved. However, performance declined when τ exceeded 1.5, indicating \na reduced ability to differentiate between object categories. Based on optimal results with τ = 1.5 , subsequent \nexperiments used this value to balance model receptiveness and accurate classification and segmentation of \ntomato objects. In the fifth set of experiments, conducted with the optimal loss function, the performance of \nthe proposed model trained with the L t loss function was compared against other state-of-the-art loss functions, \nincluding Lsn , L ft , Lde , and Lce . The results, summarized in Table 7, clearly demonstrated the superiority of the \nproposed model trained with the L t loss function across alldatasets. On the KUTomaData dataset, the L t loss \nfunction achieved improvements of 2.16% in µIoU, 1.66% in µDC, 2.39% in mAP , and 2.25% in AUC scores \ncompared to other loss functions. Similarly, on the Laboro dataset, the L t loss function outperformed the alter-\nnatives, resulting in enhancements of 3.25% in µIoU, 2.30% in µDC, 5.58% in mAP , and 4.81% in AUC scores. \nFurthermore, on the Rob2Pheno Annotated dataset, the L t loss function delivered improvements of 1.23% in µ\nIoU, 0.82% in µDC, 1.16% in mAP , and 1.45% in AUC scores. Overall, the proposed framework demonstrates \npromising results in segmenting and grading tomatoes based on their maturity levels. The experimental analysis \nvalidates the effectiveness of the proposed method and highlights its superiority over existing approaches. The \nframework’s robustness to various challenging scenarios and its computational efficiency makes it a valuable \ntool for assessing tomato quality in greenhouse farming.\nLimitations\nIn this section, the authors discuss the limitations of the proposed framework and our dataset, along with poten-\ntial solutions to mitigate them.\nLimitations of the proposed framework\nThe first limitation of the framework is its inability to generate small masks for extremely occluded, cluttered, \nor rarely observed small-sized tomatoes. To address this limitation, a practical approach is to incorporate mor-\nphological opening operations as a post-processing step to enhance the quality of small masks. This technique \ncould improve the framework’s performance in segmenting such challenging instances.\nThe second limitation of the proposed framework lies in its generation of false masks for highly complex and \noccluded tomato objects. Although the produced masks are of decent quality and outperform state-of-the-art \nmethods (as demonstrated in Fig. 5), this limitation can still be mitigated by employing more sophisticated seg-\nmentation loss functions, such as dice or IoU loss, as objective functions. By utilizing these functions, the model \ncan be constrained to preserve the exact shape of segmented objects, thus reducing the generation of false masks.\nFinally, the third limitation of the proposed framework is its potential to generate pixel-level false positives. \nThis limitation can be overcome by incorporating morphological blob opening operations as a post-processing \nstep, which can effectively eliminate small false positives and improve the overall accuracy of the framework.\nIn conclusion, While the proposed framework has certain limitations, they can be addressed by integrating \nappropriate post-processing steps and using more advanced segmentation loss functions during training. Con-\nsidering these solutions, the framework can enhance its ability to segment occluded, cluttered, accurately, and \nrarely observed objects, establishing itself as a more robust solution for tomato object detection.\nLimitations of the proposed dataset\nThe proposed dataset has the following limitations. Firstly, the tomato dataset may exhibit limited diversity \nregarding varieties, growth stages, and lighting conditions. This narrow scope of variation poses a potential \ndrawback, as it may result in overfitting the model to the specific characteristics of the dataset. Consequently, \nthe model’s ability to generalize to different scenarios could be compromised.\nSecondly, the tomato dataset may contain minor annotation errors, such as inaccurate masking of tomatoes or \nmislabeling of instances. These errors can affect the model’s performance, making it challenging to achieve high \naccuracy. To mitigate this limitation, it is essential to thoroughly evaluate and validate all labelled data before \nutilizing it for training the proposed model.\n19\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\nLastly, the proposed dataset may primarily cover a specific domain, such as a greenhouse, and may not be \nsuitable for applications in other open-field testing scenarios. This limited domain coverage can restrict the \napplicability of models trained solely on this dataset. To address this limitation, it is advisable to incorporate \nopen-field data during training to ensure the models are more adaptable to diverse environments.\nBy acknowledging and addressing these limitations, the authors can enhance the quality and applicability \nof the dataset, ultimately facilitating the development of more robust and versatile models for tomato object \ndetection and segmentation.\nConclusions\nThis study introduces a novel convolutional transformer-based segmentation and a new dataset of tomato images \nobtained from greenhouse farms in Al Ajban, Abu Dhabi, UAE. The KUTomaData dataset encompasses images \ncaptured under different environmental conditions, including varying light conditions, weather patterns, and \nstages of plant growth. These factors introduce complexity and challenges for segmentation models in accurately \nidentifying and distinguishing different components of tomato plants. The availability of such a dataset is crucial \nfor developing more precise segmentation models in the robotic harvesting industry, aiming to enhance field effi-\nciency and productivity. the authors qualitatively assessed and compared our proposed architecture with  SETR57, \n SegFormer54,  DeepFruits58,  COS59,  CWD60 and  DLIS61. The results demonstrate the superiority of the proposed \nmodel across all metrics. It outperformed in terms of µIoU, µDC, mAP , and AUC across the KUTomaData, \nLaboro and Rob2Pheno datasets. The results are presented in Table  3. Moreover, the proposed model exhibits \nhigher class-wise IoU scores for all three tomato ripeness classes, indicating its effectiveness in accurately seg -\nmenting each class. This work contributes substantially to the computer vision and machine learning community \nby providing a new dataset that facilitates developing and testing segmentation models specifically designed for \nagricultural purposes. Furthermore, it emphasizes the importance of ongoing research and progress in precision \nagriculture. In conclusion, the proposed framework and the accompanying KUTomaData dataset contribute to \ntomato recognition and maturity level classification. The framework addresses the challenges associated with \ntomato harvesting in real-world scenarios, while the dataset provides a dedicated resource for training and bench-\nmarking deep learning models. The exceptional performance demonstrated by the proposed framework across \nmultiple datasets validates its effectiveness and superiority over existing approaches. Future research can focus \non further enhancing the framework’s capabilities and exploring its applicability in other agricultural domains.\nData availibility\nThe data that support the findings of this study are available from ASPIRE, Abu Dhabi, but restrictions apply to \nthe availability of these data, which were used under license for the current study and so are not publicly avail-\nable. Data are, however, available from the authors upon reasonable request and with permission of ASPIRE, \nAbu Dhabi.\nReceived: 1 August 2023; Accepted: 15 December 2023\nReferences\n 1. Quinet, M. et al. Tomato fruit development and metabolism. Front. Plant Sci. 10, 1554 (2019).\n 2. Bapat, V . A. et al. Ripening of fleshy fruit: Molecular insight and the role of ethylene. Biotechnol. Adv. 28, 94–107 (2010).\n 3. Oltman, A., Jervis, S. & Drake, M. Consumer attitudes and preferences for fresh market tomatoes. J. Food Sci.  79, S2091–S2097 \n(2014).\n 4. Sangbamrung, I., Praneetpholkrang, P . & Kanjanawattana, S. A novel automatic method for cassava disease classification using \ndeep learning. J. Adv. Inf. Technol. 11, 241–248 (2020).\n 5. Septiarini, A. et al. Maturity grading of oil palm fresh fruit bunches based on a machine learning approach. In 2020 Fifth Interna-\ntional Conference on Informatics and Computing (ICIC), 1–4 (IEEE, 2020).\n 6. Emuoyibofarhe, O. et al. Detection and classification of cassava diseases using machine learning. Int. J. Comput. Sci. Softw. Eng.  \n8(7), 166–176 (2019).\n 7. Huang, S. et al. Applications of support vector machine (SVM) learning in cancer genomics. Cancer Genom. Proteom. 15, 41–51 \n(2018).\n 8. Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition. In International Conference \non Learning Representations (2015).\n 9. Dai, J., Li, Y ., He, K. & Sun, J. R-fcn: Object detection via region-based fully convolutional networks. https://  doi. org/ 10. 48550/ \nARXIV . 1605. 06409 (2016).\n 10. Ren, S., He, K., Girshick, R. & Sun, J. Faster r-cnn: Towards real-time object detection with region proposal networks. Adv. Neural \nInf. Process. Syst. 28, 25 (2015).\n 11. Liu, W . et al. Ssd: Single shot multibox detector. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The \nNetherlands, October 11–14, 2016, Proceedings, Part I 14, 21–37 (Springer, 2016).\n 12. Fu, L., Majeed, Y ., Zhang, X., Karkee, M. & Zhang, Q. Faster r-cnn-based apple detection in dense-foliage fruiting-wall trees using \nrgb and depth features for robotic harvesting. Biosys. Eng. 197, 245–256 (2020).\n 13. Shi, R., Li, T. & Y amaguchi, Y . An attribution-based pruning method for real-time mango detection with yolo network. Comput. \nElectron. Agric. 169, 105214 (2020).\n 14. Sun, J. et al. Detection of key organs in tomato based on deep migration learning in a complex background. Agriculture  8, 196 \n(2018).\n 15. Liu, J. & Wang, X. Tomato diseases and pests detection based on improved yolo v3 convolutional neural network. Front. Plant Sci. \n11, 898 (2020).\n 16. Redmon, J. & Farhadi, A. Y olov3: An incremental improvement. arXiv (2018).\n 17. Xu, Z.-F ., Jia, R.-S., Sun, H.-M., Liu, Q.-M. & Cui, Z. Light-yolov3: Fast method for detecting green mangoes in complex scenes \nusing picking robots. Appl. Intell. 50, 4670–4687 (2020).\n 18. Yu, Y ., Zhang, K., Y ang, L. & Zhang, D. Fruit detection for strawberry harvesting robot in non-structural environment based on \nmask-rcnn. Comput. Electron. Agric. 163, 104846 (2019).\n20\nVol:.(1234567890)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\n 19. Kang, H. & Chen, C. Fruit detection, segmentation and 3d visualisation of environments in apple orchards. Comput. Electron. \nAgric. 171, 105302 (2020).\n 20. Hasan, M., Tanawala, B. & Patel, K. J. Deep learning precision farming: Tomato leaf disease detection by transfer learning. In \nProceedings of 2nd International Conference on Advanced Computing and Software Engineering (ICACSE) (2019).\n 21. Dhanya, V . et al. Deep learning based computer vision approaches for smart agricultural applications. Artif. Intell. Agric. 20, 20 \n(2022).\n 22. Francis, C. Crop rotations. In Encyclopedia of Soils in the Environment (ed. Hillel, D.) 318–322 (Elsevier, 2005). https:// doi. org/ 10. \n1016/ B0- 12- 348530- 4/ 00253-8.\n 23. Vlaiculescu, A. & Varrone, C. Chapter 14—sustainable and eco-friendly alternatives to reduce the use of pesticides. In Pesticides \nin the Natural Environment (eds Singh, P . et al.) 329–364 (Elsevier, 2022). https:// doi. org/ 10. 1016/ B978-0- 323- 90489-6. 00014-8.\n 24. Mitchell, A. R. & Van Genuchten, M. T. Flood irrigation of a cracked soil. Soil Sci. Soc. Am. J. 57, 490–497 (1993).\n 25. Tahat, M. M., Alananbeh, M. K., Othman, A. Y . & Leskovar, I. D. Soil health and sustainable agriculture. Sustainability  12, 25. \nhttps:// doi. org/ 10. 3390/ su121 24859 (2020).\n 26. Reicosky, D. & Allmaras, R. Advances in tillage research in north American cropping systems. J. Crop. Prod. 8, 75–125 (2003).\n 27. Strand, J. F . Some agrometeorological aspects of pest and disease management for the 21st century. Agric. For. Meteorol. 103, 73–82 \n(2000).\n 28. Sladojevic, S., Arsenovic, M., Anderla, A., Culibrk, D. & Stefanovic, D. Deep neural networks based recognition of plant diseases \nby leaf image classification. Comput. Intell. Neurosci. 2016, 25 (2016).\n 29. Sherafati, A., Mollazade, K., Saba, M. K. & Vesali, F . Tomatoscan: An android-based application for quality evaluation and ripening \ndetermination of tomato fruit. Comput. Electron. Agric. 200, 107214 (2022).\n 30. Khan, A., Nawaz, U., Ulhaq, A. & Robinson, R. W . Real-time plant health assessment via implementing cloud-based scalable \ntransfer learning on aws deeplens. PLoS One 15, 1–23. https:// doi. org/ 10. 1371/ journ al. pone. 02432 43 (2020).\n 31. Xu, H. Plantvillage disease classification challenge-color images (2018).\n 32. Zheng, T., Jiang, M., Li, Y . & Feng, M. Research on tomato detection in natural environment based on rc-yolov4. Comput. Electron. \nAgric. 198, 107029 (2022).\n 33. Bochkovskiy, A., Wang, C.-Y . & Liao, H.-Y . M. Y olov4: Optimal speed and accuracy of object detection (2020). arXiv: 2004. 10934.\n 34. Xu, P . et al. Visual recognition of cherry tomatoes in plant factory based on improved deep instance segmentation. Comput. Electron. \nAgric. 197, 106991 (2022).\n 35. He, K., Gkioxari, G., Dollár, P . & Girshick, R. Mask r-cnn (2018). arXiv: 1703. 06870.\n 36. Rong, J., Dai, G. & Wang, P . A peduncle detection method of tomato for autonomous harvesting. Complex Intell. Syst. 20, 1–15 \n(2021).\n 37. Bolya, D., Zhou, C., Xiao, F . & Lee, Y . Y olact: Better real-time instance segmentation. arXiv: 1912. 06218 (arXiv preprint) (2019).\n 38. Arad, B. et al. Development of a sweet pepper harvesting robot. J. Field Robot. 37, 1027–1039 (2020).\n 39. Xiong, Y ., Ge, Y ., Grimstad, L. & From, P . J. An autonomous strawberry-harvesting robot: Design, development, integration, and \nfield evaluation. J. Field Robot. 37, 202–224 (2020).\n 40. Liu, C., Li, H., Su, A., Chen, S. & Li, W . Identification and grading of maize drought on rgb images of uav based on improved u-net. \nIEEE Geosci. Remote Sens. Lett. 18, 198–202 (2020).\n 41. Ronneberger, O., Fischer, P . & Brox, T. U-net: Convolutional networks for biomedical image segmentation. In Medical Image \nComputing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, \n2015, Proceedings, Part III 18, 234–241 (Springer, 2015).\n 42. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on \nLearning Representations (ICLR) (2021).\n 43. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv: 2010. 11929 (arXiv pre-\nprint) (2020).\n 44. Wang, J. et al. Swingd: A robust grape bunch detection model based on swin transformer in complex vineyard environment. \nHorticulturae 7, 492 (2021).\n 45. Zheng, H., Wang, G. & Li, X. Swin-mlp: A strawberry appearance quality identification method by swin transformer and multi-\nlayer perceptron. J. Food Meas. Charact. 16, 2789–2800 (2022).\n 46. Guo, Y ., Lan, Y . & Chen, X. Cst: Convolutional swin transformer for detecting the degree and types of plant diseases. Comput. \nElectron. Agric. 202, 107407 (2022).\n 47. Lu, S. et al. Swin-transformer-yolov5 for real-time wine grape bunch detection. Remote Sens. 14, 25. https:// doi. org/ 10. 3390/ rs142 \n25853 (2022).\n 48. Liu, Z. et al. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International \nConference on Computer Vision, 10012–10022 (2021).\n 49. Javaid, M., Haleem, A., Singh, R. P . & Suman, R. Enhancing smart farming through the applications of agriculture 40 technologies. \nInt. J. Intell. Netw. 3, 150–164 (2022).\n 50. TensorFlow Authors. Tensorflow conv1d documentation (Y ear of Access).\n 51. Laboro tomato: Instance segmentation dataset. https:// github. com/ labor oai/ Labor oToma to (2020). Accessed 15 Jun 2023.\n 52. Afonso, M. et al. Tomato fruit detection and counting in greenhouses using deep learning. Front. Plant Sci. 11, 20. https:// doi. org/ \n10. 3389/ fpls. 2020. 571299 (2020).\n 53. Shorten, C. & Khoshgoftaar, T. M. A survey on image data augmentation for deep learning. J. Big Data 6, 1–48 (2019).\n 54. Xie, E. et al. Segformer: Simple and efficient design for semantic segmentation with transformers. Adv. Neural. Inf. Process. Syst.  \n34, 12077–12090 (2021).\n 55. Zhao, H., Shi, J., Qi, X., Wang, X. & Jia, J. Pyramid scene parsing network. In 2017 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 6230–6239. https:// doi. org/ 10. 1109/ CVPR. 2017. 660 (IEEE Computer Society, Los Alamitos, CA, USA, \n2017).\n 56. Badrinarayanan, V ., Kendall, A. & Cipolla, R. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. \nIEEE Trans. Pattern Anal. Mach. Intell. 39, 2481–2495 (2017).\n 57. Zheng, S. et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In IEEE Interna-\ntional Conference on Computer Vision and Pattern Recognition (CVPR) (2021).\n 58. Sa, I. et al. Deepfruits: A fruit detection system using deep neural networks. Sensors 20, 2 (2016).\n 59. Fukuda, M. et al. Central object segmentation by deep learning for fruits and other roundish objects. ArXiv (2020).\n 60. Cicco, M. D. et al. Automatic model based dataset generation for fast and accurate crop and weeds detection. IEEE/RSJ IROS \n(2017).\n 61. Ni, X. et al. Deep learning image segmentation and extraction of blueberry fruit traits associated with harvestability and yield. \nNat. Hortic. Res. 20, 25 (2020).\n 62. Wang, J. et al. Deep high-resolution representation learning for visual recognition (2020). arXiv: 1908. 07919.\n 63. Yu, C. et al. Lite-hrnet: A lightweight high-resolution network (2021). arXiv:  2104. 06403.\n 64. Tan, M. & Le, Q. V . Efficientnet: Rethinking model scaling for convolutional neural networks. arXiv: abs/ 1905. 11946 (2019).\n 65. Huang, G., Liu, Z., van der Maaten, L. & Weinberger, K. Q. Densely connected convolutional networks (2018). arXiv: 1608. 06993.\n 66. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition (2015). arXiv: 1512. 03385.\n21\nVol.:(0123456789)Scientific Reports |        (2023) 13:22885  | https://doi.org/10.1038/s41598-023-50129-w\nwww.nature.com/scientificreports/\n 67. Frosst, N., Papernot, N. & Hinton, G. Analyzing and improving representations with the soft nearest neighbor loss. In International \nConference on Machine Learning, 2012–2020 (PMLR, 2019).\n 68. Abraham, N. & Khan, N. M. A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation. In IEEE \n16th International Symposium on Biomedical Imaging (ISBI) (2019).\n 69. Raja, H., Hassan, T., Akram, M. U. & Werghi, N. Clinically verified hybrid deep learning system for retinal ganglion cells aware \ngrading of glaucomatous progression. IEEE Trans. Biomed. Eng. 68, 2140–2151 (2020).\nAcknowledgements\nThis research is supported by ASPIRE, the technology program management pillar of Abu Dhabi’s Advanced \nTechnology Research Council (ATRC), under the ASPIRE project “ Aspire Research Institute for Food Security in \nthe Drylands ” within Theme 1.4. We also acknowledge Khalifa University Center for Robotics and Autonomous \nSystems (KUCARS) for their facilities and labs.\nAuthor contributions\nA.K. wrote the main manuscript draft, reviewed the experiements and manuscript. T.H. reviewed the manuscript \nand supervised the experiments. M.S. prepared the Figures (Figs. 1, 2, 4, 5  and 6) and reviewed the manuscript. \nI.F . curated data and reviewed the manuscript. N.W . reviewed the manuscript. S.M. acquired the funds and \nreviewed the manuscript. I.H. Supervised the entire project and reviewed the manuscript.\nCompeting interests \nD.W . is founder and shareholder of MIRICO Ltd. R.I., M.J., B.K., and D.R. declare no potential conflict of interest.\nAdditional information\nCorrespondence and requests for materials should be addressed to I.H.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5269190669059753
    },
    {
      "name": "Maturity (psychological)",
      "score": 0.4238853454589844
    },
    {
      "name": "Transformer",
      "score": 0.41212114691734314
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3519544303417206
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3208204507827759
    },
    {
      "name": "Engineering",
      "score": 0.10158297419548035
    },
    {
      "name": "Electrical engineering",
      "score": 0.07650214433670044
    },
    {
      "name": "Psychology",
      "score": 0.06969320774078369
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    }
  ]
}