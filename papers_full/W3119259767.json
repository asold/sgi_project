{
  "title": "Disentangling Representations of Text by Masking Transformers",
  "url": "https://openalex.org/W3119259767",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2904482350",
      "name": "Xiongyi Zhang",
      "affiliations": [
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A4201925681",
      "name": "Jan-Willem van de Meent",
      "affiliations": [
        "University of Amsterdam",
        "Universidad del Noreste"
      ]
    },
    {
      "id": "https://openalex.org/A2441726348",
      "name": "byron wallace",
      "affiliations": [
        "Universidad del Noreste"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2753738274",
    "https://openalex.org/W2613332842",
    "https://openalex.org/W1691728462",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2250243742",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2570431255",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W2785615365",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2989701728",
    "https://openalex.org/W2963813662",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2964238855",
    "https://openalex.org/W2617566453",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W4245255589",
    "https://openalex.org/W2931212643",
    "https://openalex.org/W3104223418",
    "https://openalex.org/W4297812996",
    "https://openalex.org/W2460474657",
    "https://openalex.org/W2785519580",
    "https://openalex.org/W2785961484",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2964212550",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2904405987",
    "https://openalex.org/W2982111970",
    "https://openalex.org/W4297813615",
    "https://openalex.org/W2962917899",
    "https://openalex.org/W2964127395",
    "https://openalex.org/W2888161220",
    "https://openalex.org/W2902476877",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W2903538854",
    "https://openalex.org/W2950095160",
    "https://openalex.org/W2963226019",
    "https://openalex.org/W2950662112",
    "https://openalex.org/W3104100020",
    "https://openalex.org/W3020268419",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W2927085091",
    "https://openalex.org/W2977235550",
    "https://openalex.org/W2963104724",
    "https://openalex.org/W2775844704",
    "https://openalex.org/W2945445411",
    "https://openalex.org/W1882958252",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2970211217",
    "https://openalex.org/W2994934025",
    "https://openalex.org/W4391602018"
  ],
  "abstract": "Representations from large pretrained models such as BERT encode a range of features into monolithic vectors, affording strong predictive accuracy across a range of downstream tasks. In this paper we explore whether it is possible to learn disentangled representations by identifying existing subnetworks within pretrained models that encode distinct, complementary aspects. Concretely, we learn binary masks over transformer weights or hidden units to uncover subsets of features that correlate with a specific factor of variation; this eliminates the need to train a disentangled model from scratch for a particular task. We evaluate this method with respect to its ability to disentangle representations of sentiment from genre in movie reviews, toxicity from dialect in Tweets, and syntax from semantics. By combining masking with magnitude pruning we find that we can identify sparse subnetworks within BERT that strongly encode particular aspects (e.g., semantics) while only weakly encoding others (e.g., syntax). Moreover, despite only learning masks, disentanglement-via-masking performs as well as — and often better than —previously proposed methods based on variational autoencoders and adversarial training.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 778–791\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n778\nDisentangling Representations of Text by Masking Transformers\nXiongyi Zhang\nNortheastern University\nzhang.xio@northeastern.edu\nJan-Willem van de Meent\nUniversity of Amsterdam\nNortheastern University\nj.vandemeent@northeastern.edu\nByron C. Wallace\nNortheastern University\nb.wallace@northeastern.edu\nAbstract\nRepresentations from large pretrained models\nsuch as BERT encode a range of features into\nmonolithic vectors, affording strong predictive\naccuracy across a range of downstream tasks.\nIn this paper we explore whether it is possible\nto learn disentangled representations by iden-\ntifying existing subnetworks within pretrained\nmodels that encode distinct, complementary\naspects. Concretely, we learn binary masks\nover transformer weights or hidden units to un-\ncover subsets of features that correlate with\na speciﬁc factor of variation; this eliminates\nthe need to train a disentangled model from\nscratch for a particular task. We evaluate this\nmethod with respect to its ability to disentan-\ngle representations of sentiment from genre in\nmovie reviews, toxicity from dialect in Tweets,\nand syntax from semantics. By combining\nmasking with magnitude pruning we ﬁnd that\nwe can identify sparse subnetworks within\nBERT that strongly encode particular aspects\n(e.g., semantics) while only weakly encoding\nothers (e.g., syntax). Moreover, despite only\nlearning masks, disentanglement-via-masking\nperforms as well as — and often better than —\npreviously proposed methods based on varia-\ntional autoencoders and adversarial training.\n1 Introduction and Motivation\nLarge pretrained models such as ELMo (Peters\net al., 2018), BERT (Devlin et al., 2019), and XL-\nNet (Yang et al., 2019) have come to dominate mod-\nern NLP. Such models rely on self-supervision over\nlarge datasets to learn general-purpose representa-\ntions of text that achieve strong predictive perfor-\nmance across a spectrum of downstream tasks (Liu\net al., 2019). A downside of such learned represen-\ntations is that it is not obvious what information\nthey encode, which hinders model robustness and\ninterpretability. The opacity of embeddings pro-\nduced by models such as BERT has motivated NLP\nresearch on designing probing tasks as a means of\nuncovering the properties of input texts that are\nMask Weight\nMask Hidden\n*\n*\n=\n=\nW\nW\nM\nMh h’\nh h’\nInput X\nor\nLower layers\nAdditional \nlayers\nAdditional \nlayers\nFigure 1: Masking weights and hidden activations in\nBERT. We show a linear layer with weights W, inputs\nh, and outputs h′. We learn a mask for each disentan-\ngled factor, which is either applied to the weightsW or\nto intermediate representations h.\nencoded in learned representations (Rogers et al.,\n2020; Linzen et al., 2019; Tenney et al., 2019).\nIn this paper we investigate whether we can un-\ncover disentangled representations from pretrained\nmodels. That is, rather than mapping inputs onto a\nsingle vector that captures arbitrary combinations\nof features, our aim is to extract a representation\nthat factorizes into distinct, complementary proper-\nties of inputs. Explicitly factorizing representations\naids interpretability, in the sense that it becomes\nmore straightforward to determine which factors of\nvariation inform predictions in downstream tasks.\nA general motivation for learning disentangled\nrepresentations is to try and minimize — or at least\nexpose — model reliance on spurious correlations,\ni.e., relationships between (potentially sensitive)\nattributes and labels that exist in the training data\nbut which are not causally linked (Kaushik et al.,\n2020). This is particularly important for large pre-\ntrained models like BERT, as we do not know what\nthe representations produced by such models en-\ncode. Here, learning disentangled representations\nmay facilitate increased robustness under distribu-\ntional shifts by capturing a notion of invariance: If\nsyntactic changes do not affect the representation\nof semantic features (and vice versa) then we can\nhope to learn models that are less sensitive to any\nincidental correlations between these factors.\n779\nAs one example that we explore in this paper,\nconsider the task of identifying Tweets that contain\nhate speech (Founta et al., 2018). Recent work\nshows that models trained over Tweets annotated\non a toxicity scale exhibit a racial bias: They have\na tendency to over-predict that Tweets written by\nusers who self-identify as Black are “toxic”, owing\nto the use of African American Vernacular English\n(AA VE; Sap et al. 2019). In principal, disentangled\nrepresentations would allow us to isolate relevant\nsignal from irrelevant or spurious factors (such as,\nin this case, the particular English dialect used),\nwhich might in turn reveal and allow us to mitigate\nunwanted system biases, and increase robustness.\nTo date, most research on disentangled represen-\ntations has focused on applications in computer vi-\nsion (Locatello et al., 2019b; Kulkarni et al., 2015;\nChen et al., 2016; Higgins et al., 2017), where\nthere exist comparatively clear independent factors\nof variation such as size, position, and orientation,\nwhich have physical grounding and can be formal-\nized in terms of actions of symmetry subgroups\n(Higgins et al., 2018). A challenge in learning dis-\nentangled representations of text is that it is less\nclear which factors of variation should admit invari-\nance. Still, we may hope to disentangle particular\nproperties for certain applications — e.g., protected\ndemographic information (Elazar and Goldberg,\n2018) — and there are general properties of lan-\nguage that we might hope to disentangle, e.g., syn-\ntax and semantics (Chen et al., 2019).\n2 Methods\nWe are interested in learning a disentangled rep-\nresentation that maps inputs x(text) onto vectors\nz(a) and z(b) that encode two distinct factors of\nvariation. To do so, we will learn two sets of masks\nM(a) and M(b) that can be applied to either the\nweights or the intermediate representations in a pre-\ntrained model (in our case, BERT). We estimate\nonly the mask parameters and do not ﬁnetune the\nweights of the pretrained model.\nTo learn M(a) and M(b), we assume access to\ntriplets (x0,x1,x2) in which x0 and x1 are similar\nwith respect to aspect abut dissimilar with respect\nto aspect b, whereas x0 and x2 are similar with\nrespect to aspect bbut dissimilar with respect to\naspect a. In some of our experiments (e.g., when\ndisentangling sentiment from genre in movie re-\nviews) we further assume that we have access to\nclass labels y(a) ∈{0,1}and y(b) ∈{0,1}for as-\npects of interest. In such cases, we build triplets\nusing these labels, deﬁning (x0,x1,x2) such that\ny(a)\n0 = y(a)\n1 ̸= y(a)\n2 and y(b)\n0 = y(b)\n2 ̸= y(b)\n1 .\n2.1 Masking Weights and Hidden Activations\nFigure 1 illustrates the two forms of masking that\nwe consider in our approach (we depict only a sin-\ngle linear layer of the model). Hereh= (h(a),h(b))\nare input activations, W are the weights in the\npretrained model,1 and h′= (h′(a),h′(b)) are out-\nput activations. We augment each layer of the\noriginal network with two (binary) masks M =\n(M(a),M(b)), applied in one of two ways:\n1. Masking Weights Here masks M(a) and\nM(b) have the same shape as weights W, and out-\nputs are computed using the masked weights tensor\nh′= h·(W ◦M). (1)\n2. Masking Hidden Activations In this case\nmasks M(a) and M(b) have the same shape as\nthe intermediate (hidden) activations h(a) and h(b).\nOutput activations are computed by applying the\noriginal weights W to masked inputs\nh′= (h◦M) ·W. (2)\nIn both methods, we follow (Zhao et al., 2020)\nand only mask the last several layers of BERT,\nleaving bottom layers unchanged.2\n2.2 Triplet Loss\nTo learn masks, we assume that we have access\nto supervision in the form of triplets, as intro-\nduced above. Passing (x0,x1,x2) through our\nmodel yields two representations for each instance:\n(z(a)\n0 ,z(b)\n0 ),(z(a)\n1 ,z(b)\n1 ),(z(a)\n2 ,z(b)\n2 ), for which we\ndeﬁne the losses\nL(a)\ntrp = max\n(\n∥z(a)\n0 −z(a)\n1 ∥−∥z(a)\n0 −z(a)\n2 ∥+ α,0\n)\n,\nL(b)\ntrp = max\n(\n∥z(b)\n0 −z(b)\n2 ∥−∥z(b)\n0 −z(b)\n1 ∥+ α,0\n)\n,\nLtrp = 1\n2\n(\nL(a)\ntrp + L(b)\ntrp\n)\n. (3)\nHere αis a hyperparameter specifying a margin for\nthe loss, which we set to α= 2in all experiments.\n1We omit the bias term, which we do not mask.\n2We mask the last six layers, which seems to work well.\n780\n2.3 Supervised Loss\nIn some settings we may have access to more direct\nforms of supervision. For example, when learning\nrepresentations for the genre and sentiment in a\nmovie review, we have explicit class labelsy(a) and\ny(b) for each aspect. To exploit such supervision\nwhen available, we add classiﬁcation layers C(a)\nand C(b) and deﬁne classiﬁcation losses\nL(a)\ncls = CrossEntropy\n(\nC(a)(z(a)),y(a)\n)\n, (4)\nL(b)\ncls = CrossEntropy\n(\nC(b)(z(b)),y(b)\n)\n, (5)\nLcls = 1\n2\n(\nL(a)\ncls + L(b)\ncls\n)\n. (6)\n2.4 Disentanglement Loss\nTo ensure that the two aspect representations are\ndistinct, we encourage the masks to overlap as little\nas possible. To achieve this we add a term in the\nloss for each layer l∈L\nLovl = 1\n|L|\n∑\nl∈L\n∑\ni,j\n1(M(a)\ni,j +M(b)\ni,j >1). (7)\n2.5 Binarization and Gradient Estimation\nThe ﬁnal loss of our model is\nL= λtrp ·Ltrp + λovl ·Lovl (+λcls ·Lcls). (8)\nWe parenthetically denote the classiﬁcation loss,\nwhich we only include when labels are available.\nWe minimize this loss to estimateM (and classiﬁer\nparameters), keeping the pretrained BERT weights\nﬁxed. Because the loss is not differentiable with re-\nspect to a binary mask, we learn continuous masks\nM that are binarized during the forward pass by\napplying a threshold τ, a global hyperparameter,\nM∗\nij =\n{\n1 if Mij ≥τ\n0 if Mij <τ . (9)\nWe then use a straight-through estimator (Hinton\net al., 2012; Bengio et al., 2013) to approximate\nthe derivative, which is to say that we evaluate the\nderivative of the loss with respect to the continuous\nmask M at the binarized values M = M∗,\nM = M −η ∂L\n∂M\n⏐⏐⏐⏐\nM=M∗\n. (10)\nIMDB\nSentiment / Genre Drama Horror\nOriginal Positive 41.2 7.4\nDataset Negative 20.0 31.4\nOur Train Positive 42.5 7.5\n(Correlated) Negative 7.5 42.5\nOur Test Positive 25.0 25.0\n(Uncorrelated) Negative 25.0 25.0\nTwitter\nToxicity / Race Black White\nOriginal Toxic 15.4 16.9\nDataset Non-toxic 20.8 46.9\nTrain Toxic 42.5 7.5\n(Correlated) Non-toxic 7.5 42.5\nTest Toxic 25.0 25.0\n(Uncorrelated) Non-toxic 25.0 25.0\nTable 1: Percentage of each class in the original dataset,\nand in the two subsets we sampled, one correlated train-\ning set and one uncorrelated test set. We train models\non the former and test on the latter. This is meant to as-\nsess the robustness of models to shifts in spurious cor-\nrelations that might exist in training data.\n3 Experiments\nWe conduct a series of experiments to evaluate\nthe degree to which the proposed masking strat-\negy achieves disentanglement, as compared to ex-\nisting methods for disentanglement in NLP. As a\nﬁrst illustrative example, we consider a corpus of\nmovie reviews, in which sentiment is correlated\nwith ﬁlm genre (3.1). We treat this as a proxy for\na spurious correlation, and evaluate the robustness\nof the models to shifts in conditional probabilities\nof one attribute (sentiment) given another (genre).\nWe then consider a more consequential example:\nHate speech classiﬁcation on Twitter (3.2). Prior\nwork (Sap et al., 2019) has shown that models ex-\nploit a spurious correlation between “toxicity” and\nAfrican American Vernacular English (AA VE); we\naim to explicitly disentangle these factors in ser-\nvice of fairness. We evaluate whether the model is\nable to achieve equalized odds, a commonly used\nfairness metric. Finally, following prior work, we\ninvestigate disentangling semantics from syntax\n(insofar as this is possible) in Section 3.3.\n3.1 Disentangling Sentiment From Genre\nExperimental Setup In this experiment we as-\nsume a setting in which each data point xhas both\na ‘main’ labelyand a secondary (possibly sensi-\n781\nAvg. (Sent) Worst (Sent) Avg. (Genre) Worst (Genre)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9Accuracy\nDRLST\nDRLST-BERT\nUntuned\nFinetuned\nAdversarial\nMasked Weights\nMasked Hidden\nFigure 2: Average and worst main task performance\nacross sentiment/genre combinations. Masked variants\n(proposed in this paper) are cross-hatched. Large gaps\nbetween average and worst performance for a model\nsuggest that it is using the non-target attribute when\nmaking predictions for the main task.\ntive) attribute z. We are interested in evaluating the\ndegree to which explicitly disentangling representa-\ntions corresponding to these may afford robustness\nto shifts in the conditional distribution of ygiven z.\nAs a convenient, illustrative dataset with which to\ninvestigate this, we use a set of movie reviews from\nIMDB (Maas et al., 2011) in which each review\nhas both a binary sentiment label and a genre label.\nWe pick the two genres of movies that exhibit a\nstrong correlation with review sentiment: Drama\n(reviews tend to be positive) and Horror (negative),\nexcluding reviews corresponding to other genres\nand the (small) set of instances that belong to both\ngenres. To investigate robustness to shifts in cor-\nrelations between zand ywe sampled two subsets\nfrom the training set such that in the ﬁrst sentiment\nand genre are highly correlated, while in the second\nthey are uncorrelated. We report the correlations\nbetween these variables in the two subsets in Table\n1. We train models on the correlated subset, and\nthen evaluate them on the uncorrelated set.\nWe compare the proposed masking approaches\nto several baselines. Untuned is a dense classiﬁ-\ncation layer on top of BERT representations (with-\nout ﬁnetuning). In the ﬁnetuned variant we omit\nmasks and instead minimize the loss with respect to\nBERT weights. In the adversarial model we adopt\n‘adversarial debiasing’: In addition to minimizing\nloss on the main task, we train an adversarial clas-\nsiﬁer to predict the non-target attribute, and the en-\ncoder is trained to mitigate the adversaries’ ability\nto do so. We implement this via gradient-reversal\n(Ganin and Lempitsky, 2015). We also compare\nto two variational autoencoder baselines: DRLST\n(John et al., 2019) is a V AE model with multi-task\nloss and adversarial loss; andDRLST-BERTis the\nsame model, except we use BERT as the encoder\nSentiment ↑ Leakage (Genre) ↓\nDRLST 62.1 59.0\nDRLST-BERT 67.5 66.3\nUntuned 82.3 81.5\nFinetuned 87.5 85.5\nAdversarial 86.8 80.3\nMasked Weights 88.0 72.0\nMasked Hidden 88.0 79.0\nTable 2: Performance on the main task of sentiment\nanalysis and gender information leakage. The DRLST\nbaselines perform poorly on the main task; the pro-\nposed masking approaches have achieve comparable re-\nsults for sentiment, and expose less genre information.\nin place of a GRU (Cho et al., 2014).\nLeakage of the Non-target Attribute We evalu-\nate the degree to which representations “leak” non-\ntarget information. Following (Elazar and Gold-\nberg, 2018), we ﬁrst train the model to predict the\nmain task label on the correlated dataset. Then\nwe ﬁx the encoder and train a single layer MLP\non the uncorrelated dataset to probe the learned\nrepresentations for the non-target attribute. Be-\ncause this probe is trained and tested on only un-\ncorrelated data, it cannot simply learn the main\ntask and exploit the correlation. We report results\nfor our proposed masking models and baselines\nin Table 2. We also report the results with genre\nclassiﬁcation as the main task and sentiment as\nthe protected attributes in the Appendix (Section\nA.1). The DRLST baselines generally underper-\nform, which translates to low leakage numbers but\nalso poor performance on the main task. Compared\nto the baselines, our masking variants perform com-\nparably with respect to predicting the main task\nlabel, but do so with less leakage.\nWorst Group Performance In addition to non-\ntarget attribute leakage, we measure how mod-\nels perform on the main task for each subgroup:\n(Positive, Drama), (Positive, Horror), (Negative,\nDrama), and (Negative, Horror). Because the dis-\ntribution of the four groups is unequal in the train\nset, we expect that models will perform better on\nattribute combinations that are over-represented\nin this set, and worse on those that are under-\nrepresented, suggesting that the model is implicitly\nexploiting the correlation between these attributes.\nWe report both the average and worst perfor-\nmance on the four subgroups; the latter is a proxy to\nmeasure robustness when subgroup compositions\n782\nMasked Weights Masked Hidden Adversarial Finetuned\nSentimentGenre\nFigure 3: t-SNE projection of sentiment and genre representations of different models. Marker colors denote\nsentiment (blue for positive and yellow for negative); marker shapes denote genre (×for drama and •for horror).\nIn the upper row we expect the points of the same color to be clustered together, but not the points with the same\nmarker shapes, and for the lower row we expect the points of the same marker shapes to be clustered together, but\nnot those of the same colors.\nshift between the train to the test set. Figure 2 plots\nthe results. We observe that the masking variants\nrealize similar average performance as the base-\nlines, but consistently outperform these in terms\nof worst performance. This indicates that the pro-\nposed variants rely less on the correlation between\nthe two attributes when predicting the main label.\nQualitative Evaluation In Figure 3 we plot t-\nSNE visualizations (Maaten and Hinton, 2008) of\nthe representations induced by different models.\nIf the representations are disentangled as desired,\ninstances with different sentiment will be well sep-\narated, while those belonging to different genres\nwithin each sentiment will not be separated. Sim-\nilarly, for genre representations, instances of the\nsame genre should co-locate, but clusters should\nnot reﬂect sentiment. No method perfectly realizes\nthese criteria, but the proposed masking approaches\nachieve better results than do the two baselines. For\ninstance, in the embeddings from the adversarial\n(Sentiment) and ﬁnetuned (Sentiment), instances\nthat have negative sentiment but different genres\n(•and ×) are separated, indicating that these senti-\nment representations still carry genre information.\n3.2 Disentangling Toxicity from Dialect\nExperimental Setup In this experiment we eval-\nuate models on a more consequential task: Detect-\ning hate speech in Tweets (Founta et al., 2018).\nPrior work (Sap et al., 2019) has shown that ex-\nisting hate speech datasets exhibit a correlation\nbetween African American Vernacular English\n(AA VE) and toxicity ratings, and that models\ntrained on such datasets propagate these biases.\nThis results in Tweets by Black individuals being\nmore likely to be predicted as “toxic”. Factorizing\nrepresentations of Tweets into dialectic and toxicity\nsubvectors could ameliorate this problem.\nWe use (Founta et al., 2018) as a dataset for this\ntask. This comprises 100k Tweets, each with a\nlabel indicating whether the Tweet is considered\ntoxic, and self-reported information about the au-\nthor. We focus on the self-reported race informa-\ntion. Speciﬁcally, we subset the data to include\nonly users who self-reported as being either white\nor Black. The idea is that Tweets from Black indi-\nviduals will sometimes use AA VE, which in turn\ncould be spuriously associated with ‘toxicity’.\nSimilar to the above experiment, we sampled\ntwo subsets of the data such that in the ﬁrst the (an-\nnotated) toxicity and self-reported race are highly\ncorrelated, while in the second they are uncorre-\nlated (see Table 1). We train models on the corre-\nlated subset, and evaluate them on the uncorrelated\nset. This setup is intended to measure the extent\nto which models are prone to exploiting (spurious)\ncorrelations, and whether and which disentangle-\nment methods render models robust to these.\nLeakage of Race Information We evaluate the\ndegree to which representations of Tweets “leak”\ninformation about the (self-reported) race of their\nauthors using the same method as above, and report\n783\nToxicity ↑ Leakage (Race) ↓\nDRLST 66.4 86.6\nDRLST-BERT 68.0 90.0\nUntuned 68.2 76.8\nFinetuned 70.4 93.0\nAdversarial 70.2 67.0\nMasked Weights 70.4 59.8\nMasked Hidden 71.4 63.5\nTable 3: Performance on the main task of toxicity pre-\ndiction, and leakage of race information. Compared to\nthe baselines, the proposed approaches achieve perfor-\nmance competitive with or better than baselines, while\nminimizing leakage of the protected attribute.\nresults in Table 3. We observe that the proposed\nmasking variants perform comparably to baselines\nwith respect to predicting the toxicity label, but\nleak considerably less information pertaining to the\nsensitive attribute (race).\nFairness Implications In addition to the degree\nto which representations encode race information,\nwe are interested in how the model performs on\ninstances comprising (self-identiﬁed) Black and\nwhite individuals, respectively. More speciﬁcally,\nwe can measure the True Positive Rate (TPR)\nand the True Negative Rate (TNR) on these sub-\ngroups, which in turn inform equalized odds, a\nstandard metric used in the fairness literature.\nWe report the TPR and TNR of each model\nachieved over white and Black individuals, respec-\ntively, as well as the difference across the two\ngroups in Figure 4. We observe that the proposed\nmodel variants achieve a smaller TPR and TNR gap\nacross the two races (see rightmost subplots), indi-\ncating that performance is more equitable across\nthe groups, compared to baselines.\n3.3 Disentangling Semantics from Syntax\nExperimental Setup As a ﬁnal experiment, we\nfollow prior work in attempting to disentangle\nsemantic from syntactic information encoded in\nlearned (BERT) representations of text. Because\nwe have proposed exploiting triplet-loss, we ﬁrst\nconstruct triplets (x0,x1,x2) such that x0 and x1\nare similar semantically but differ in syntax, while\nx0 and x2 are syntactically similar but encode dif-\nferent semantic information. We follow prior work\n(Chen et al., 2019; Ravfogel et al., 2020) in deriving\nthese triplets. Speciﬁcally, we obtain x0,x1 from\nthe ParaNMT-50M (Wieting and Gimpel, 2018)\ndataset. Here x1 is obtained by applying back-\ntranslation to x0, i.e., by translating x0 from En-\nglish to Czech and then back into English. To\nderive x2 we keep all function words (from a list\nintroduced in Ravfogel et al. 2020) in x0, and re-\nplace content words by masking each in turn, run-\nning the resultant input forward through BERT, and\nrandomly selecting one of the top predictions (that\ndiffers from the original word) as a replacement.\nWe compare our disentanglement-via-masking\nstrategies against models that represent state-of-the-\nart approaches to disentangling syntax and seman-\ntics. In particular, we compare against VGV AE\n(Chen et al., 2019), though we implement this on\ntop of BERT-base to allow fair comparison. Fol-\nlowing prior work that has used triplet loss for\ndisentanglement, we also compare against a model\nin which we ﬁnetune BERT using the same triplet\nloss that we use to train our model, but in which we\nupdate all model parameters (as opposed to only\nestimating mask parameters).\nTo evaluate learned representations with respect\nto the semantic and syntactic information that they\nencode, we evaluate them on four tasks. Two of\nthese depend predominantly on semantic informa-\ntion, while the other two depend more heavily on\nsyntax.3 For the semantics tasks we use: (i) A\nword content (WC) (Conneau et al., 2018) task in\nwhich we probe sentence representations to assess\nwhether the corresponding sentence contains a par-\nticular word; and (ii) A semantic textual similarity\n(STS) benchmark (Nakov et al., 2013), which in-\ncludes human provided similarity scores between\npairs of sentences. We evaluate the former in terms\nof accuracy; for the latter (a ranking task) we use\nSpearman correlation. To evaluate whether rep-\nresentations encode syntax, we use: (i) A task in\nwhich the aim is to predict the length of the longest\npath in a sentence’s parse tree from its embedding\n(Depth) (Conneau et al., 2018); and (ii) A task in\nwhich we probe sentence representations for the\ntype of their top constituents immediately below\nthe Snode (TopConst).4\nFigure 5 shows the signed differences be-\ntween the performance achieved on semantics- and\nsyntax-oriented tasks by BERT embeddings (we\nmean-pool over token embeddings) and the ‘syn-\ntax’ representations from the disentangled models\nconsidered (see the Appendix for the analogous\nplot for the ‘semantics’ representations in ﬁgure\n3This is a (very) simpliﬁed view of ‘semantics’ / ‘syntax’.\n4See (Conneau et al., 2018) for more details regarding WC,\nDepth, and TopConst tasks.\n784\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive RateTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to recognize “toxic” comments from white users\nMore likely to recognize “toxic” comments from Black users\nMore likely to incorrectly flag tweets from Black users as “toxic”  \nMore likely to incorrectly flag tweets from white users as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive RateTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to recognize “toxic” comments from white users\nMore likely to recognize “toxic” comments from Black users\nMore likely to incorrectly flag tweets from Black users as “toxic”  \nMore likely to incorrectly flag tweets from white users as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive RateTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to recognize “toxic” comments from white users\nMore likely to recognize “toxic” comments from Black users\nMore likely to incorrectly flag tweets from Black users as “toxic”  \nMore likely to incorrectly flag tweets from white users as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive RateTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to recognize “toxic” comments from white users\nMore likely to recognize “toxic” comments from Black users\nMore likely to incorrectly flag tweets from Black users as “toxic”  \nMore likely to incorrectly flag tweets from white users as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive RateTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to recognize “toxic” comments from white users\nMore likely to recognize “toxic” comments from Black users\nMore likely to incorrectly flag tweets from Black users as “toxic”  \nMore likely to incorrectly flag tweets from white users as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive RateTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to recognize “toxic” comments from white users\nMore likely to recognize “toxic” comments from Black users\nMore likely to incorrectly flag tweets from Black users as “toxic”  \nMore likely to incorrectly flag tweets from white users as “toxic”  \nΔ White - BlackΔ White - Black\nTrue Positive Rate\nTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTrue Positive Rate\nTrue Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nWhite White\nBlack Black\nTrue Positive Rate True Negative Rate\nTPR TNR\nWhiteWhite\nBlackBlack\nTPR Diﬀ TNR Diﬀ\nWorse on Black\n Worse On White\nMore likely to \nrecognize “toxic” \ncomments from \nwhite users\nMore likely to \nrecognize “toxic” \ncomments from \nBlack users\nMore likely to \nincorrectly flag \ntweets from Black \nusers as “toxic”  \nMore likely to \nincorrectly flag \ntweets from white \nusers as “toxic”  \nΔ White - BlackΔ White - Black\nTrue Positive Rate\nTrue Negative Rate\nFigure 4: True Positive and True Negative Rates achieved on white and Black individuals, respectively, and the\n(signed) difference between these (rightmost subplots). The proposed masked variants (cross-hatched) are more\nequitable in performance, while other methods tend to over-predict Tweets written by Black individuals as “toxic\".\nA.2). Ideally, syntax embeddings would do well\non the syntax-oriented tasks (Depth and TopCon)\nand poorly on the semantic tasks (WC and STS).\nWith respect to syntax-oriented tasks, the proposed\nmasking methods outperform BERT base repre-\nsentations, as well as the alternative disentangled\nmodels considered. These methods also consid-\nerably reduce performance on semantics-oriented\ntasks, as we would hope. We emphasize that this is\nachieved only via masking, and without modifying\nthe underlying model weights.\n3.4 Identifying Sparse Disentangled\nSub-networks for Semantic and Syntax\nWe next assess if we are able to identify sparse dis-\nentangled subnetworks by combining the proposed\nmasking approaches with magnitude pruning (Han\net al., 2015a). Speciﬁcally, we use the loss func-\ntion deﬁned in Equation 8 to ﬁnetune BERT for k\niterations, and prune weights associated with them\nsmallest magnitudes after training. We then initial-\nize masks to the sparse sub-networks identiﬁed in\nthis way, and continue reﬁning these masks via the\ntraining procedure proposed above. We compare\nthe resultant sparse network to networks similarly\npruned (but not masked). Speciﬁcally, for the latter\nwe consider: Standard magnitude tuning applied\nto BERT, without additional tuning (Pruned + Un-\ntuned), and a method in which after magnitude\npruning we resume ﬁnetuning of the subnetwork\nuntil convergence, using the aforementioned loss\nfunction (Pruned + Finetuned).\nWe compare the performance achieved on the\nsemantic and syntax tasks by the subnetworks iden-\ntiﬁed using the above strategies at varying levels of\nsparsity, namely after pruning: {0, 20%, 40%, 60%,\n80%, 85%, 90%, 95%} of weights.5 We report full\nresults in Appendix Figure A.3, but here observe\nthat combining the proposed masking strategy with\nmagnitude pruning consistently yields represen-\ntations of semantics that perform comparatively\nstrongly on the semantics-oriented tasks (STS,\nWC), even at very high levels of sparsity; these\nsemantics representations also perform compara-\ntively poorly on the syntax-oriented tasks (Depth,\nTopCon), as one would hope. Similarly, syntax rep-\nresentations perform poorly on semantics-oriented\ntasks, and outperform alternatives on the syntax-\noriented tasks. In sum, this experiment suggests\nthat we are indeed able to identify sparse disentan-\ngled subnetworks via masking.\n4 Related Work\nDisentangled and structured representations of\nimages. The term disentangled representations\nhas been used to refer to a range of methods with\ndiffering aims. Much of the initial focus in this\nspace was on learning representations of images,\nin which certain dimensions correspond to inter-\npretable factors of variation (Kulkarni et al., 2015;\n5Technically, in the Pruned + Masked Weights method,\nreﬁning the masks may change subnetwork sparsity, but em-\npirically we ﬁnd this to change the sparsity only slightly (∼1%\nin all of our experiments).\n785\n          WC↓              STS↓.            Depth↑         TopCon↑\nRepresentation Type: Syntax\nSemantic Task Syntax Task\nVGVAE\nFinetuned\nMasked Weight\nMasked Hidden\nFigure 5: Differences between performances achieved\nvia BERT embeddings and the disentangled model vari-\nants considered on semantics-oriented (WC, STS) and\nsyntax-oriented (Depth, TopCon) tasks compared with\nBERT embeddings. We plot this difference with re-\nspect to the syntax embeddings induced by the models.\nHiggins et al., 2017; Chen et al., 2016). In the\ncontext of variational autoencoders (Kingma and\nWelling, 2014; ?) this motivated work that evalu-\nates to what extent such representations can re-\ncover a set of ground-truth factors of variation\nwhen learned without supervision (Eastwood and\nWilliams, 2018; Kim and Mnih, 2018; Chen et al.,\n2018). Other work has investigated representations\nwith the explicit motivation of fairness (Locatello\net al., 2019a; Creager et al., 2019), which disentan-\nglement may help to facilitate.\nDisentangling representations in NLP. Com-\npared to vision, there has been relatively little work\non learning disentangled representations of text.\nMuch of the prior work on disentanglement for\nNLP that does exist has focused on using such rep-\nresentations to facilitate controlled generation, e.g.,\nmanipulating sentiment (Larsson et al., 2017).\nA related notion is that of style transfer, for ex-\nample, separating style from content in language\nmodels Shen et al. (2017); Mir et al. (2019). There\nhas also been prior work on learning representa-\ntions of particular aspects to facilitate domain adap-\ntation (Zhang et al., 2017), and aspect-speciﬁc in-\nformation retrieval (Jain et al., 2018). Esmaeili et al.\n(2019) focus on disentangling user and item repre-\nsentations for product reviews. Moradshahi et al.\n(2019) combine BERT with Tensor-Product Rep-\nresentations to improve its transferability across\ndifferent tasks. Recent work has proposed learning\ndistinct vectors coding for semantic and syntac-\ntic properties of text (Chen et al., 2019; Ravfogel\net al., 2020); these serve as baseline models in our\nexperiments.\nFinally, while not explicitly framed in terms of\ndisentanglement, efforts to ‘de-bias’ representa-\ntions of text are related to our aims. Some of this\nwork has used adversarial training to attempt to re-\nmove sensitive information (Elazar and Goldberg,\n2018; Barrett et al., 2019).\nNetwork pruning. A ﬁnal thread of relevant\nwork concerns selective pruning of neural networks.\nThis has often been done in the interest of model\ncompression (Han et al., 2015a,b). Recent intrigu-\ning work has considered pruning from a different\nperspective: Identifying small subnetworks — win-\nning ‘lottery tickets’ (Frankle and Carbin, 2019) —\nthat, trained in isolation with the right initialization,\ncan match the performance of the original networks\nfrom which they were extracted. Very recent work\nhas demonstrated that winning tickets exist within\nBERT (Chen et al., 2020).\n5 Discussion\nWe have presented a novel perspective on learning\ndisentangled representations for natural language\nprocessing in which we attempt to uncover existing\nsubnetworks within pretrained transformers (e.g.,\nBERT) that yield disentangled representations of\ntext. We operationalized this intuition via a mask-\ning approach, in which we estimate only binary\nmasks over weights or hidden states within BERT,\nleaving all other parameters unchanged. We demon-\nstrated that — somewhat surprisingly — we are\nable to achieve a level of disentanglement that of-\nten exceeds existing approaches (e.g., a varational\nauto-encoder on top of BERT), which have the ben-\neﬁt of ﬁnetuning all model parameters.\nOur experiments demonstrate the potential ben-\neﬁts of this approach. In Section 3.1 we showed\nthat disentanglement via masking can yield repre-\nsentations that are comparatively robust to shifts\nin correlations between (potentially sensitive) at-\ntributes and target labels. Aside from increasing\nrobustness, ﬁnding sparse subnetworks that induce\ndisentangled representations constitutes a new di-\nrection to pursue in service of providing at least one\ntype of model interpretability for NLP. Finally, we\nnote that sparse masking (which does not mutate\nthe underlying transformer parameters) may offer\n786\nefﬁciency advantages over alternative approaches.\n6 Acknowledgements\nThis work was supported by that National Science\nFoundation (NSF), grant 1901117.\nReferences\nMaria Barrett, Yova Kementchedjhieva, Yanai Elazar,\nDesmond Elliott, and Anders Søgaard. 2019. Adver-\nsarial removal of demographic attributes revisited.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 6330–\n6335, Hong Kong, China. Association for Computa-\ntional Linguistics.\nYoshua Bengio, Nicholas Léonard, and Aaron\nCourville. 2013. Estimating or propagating gradi-\nents through stochastic neurons for conditional com-\nputation. ArXiv preprint, abs/1308.3432.\nMingda Chen, Qingming Tang, Sam Wiseman, and\nKevin Gimpel. 2019. A multi-task approach for dis-\nentangling syntax and semantics in sentence repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 2453–2464, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTian Qi Chen, Xuechen Li, Roger B. Grosse, and\nDavid Duvenaud. 2018. Isolating sources of disen-\ntanglement in variational autoencoders. InAdvances\nin Neural Information Processing Systems 31: An-\nnual Conference on Neural Information Processing\nSystems 2018, NeurIPS 2018, December 3-8, 2018,\nMontréal, Canada, pages 2615–2625.\nTianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia\nLiu, Yang Zhang, Zhangyang Wang, and Michael\nCarbin. 2020. The lottery ticket hypothesis for pre-\ntrained BERT networks. In Advances in Neural\nInformation Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nXi Chen, Yan Duan, Rein Houthooft, John Schul-\nman, Ilya Sutskever, and Pieter Abbeel. 2016. In-\nfogan: Interpretable representation learning by in-\nformation maximizing generative adversarial nets.\nIn Advances in Neural Information Processing Sys-\ntems 29: Annual Conference on Neural Informa-\ntion Processing Systems 2016, December 5-10, 2016,\nBarcelona, Spain, pages 2172–2180.\nKyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder–decoder ap-\nproaches. In Proceedings of SSST-8, Eighth Work-\nshop on Syntax, Semantics and Structure in Statisti-\ncal Translation, pages 103–111, Doha, Qatar. Asso-\nciation for Computational Linguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nElliot Creager, David Madras, Jörn-Henrik Jacobsen,\nMarissa A. Weis, Kevin Swersky, Toniann Pitassi,\nand Richard S. Zemel. 2019. Flexibly fair represen-\ntation learning by disentanglement. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 1436–1445. PMLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nCian Eastwood and Christopher K. I. Williams. 2018.\nA framework for the quantitative evaluation of disen-\ntangled representations. In 6th International Confer-\nence on Learning Representations, ICLR 2018, Van-\ncouver, BC, Canada, April 30 - May 3, 2018, Con-\nference Track Proceedings. OpenReview.net.\nYanai Elazar and Yoav Goldberg. 2018. Adversarial\nremoval of demographic attributes from text data.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n11–21, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nBabak Esmaeili, Hongyi Huang, Byron C. Wallace, and\nJan-Willem van de Meent. 2019. Structured neural\ntopic models for reviews. In The 22nd International\nConference on Artiﬁcial Intelligence and Statistics,\nAISTATS 2019, 16-18 April 2019, Naha, Okinawa,\nJapan, volume 89 of Proceedings of Machine Learn-\ning Research, pages 3429–3439. PMLR.\nAntigoni Founta, Constantinos Djouvas, Despoina\nChatzakou, Ilias Leontiadis, Jeremy Blackburn, Gi-\nanluca Stringhini, Athena Vakali, Michael Siriv-\nianos, and Nicolas Kourtellis. 2018. Large scale\ncrowdsourcing and characterization of twitter abu-\nsive behavior. In Proceedings of the International\nAAAI Conference on Web and Social Media , vol-\nume 12.\n787\nJonathan Frankle and Michael Carbin. 2019. The lot-\ntery ticket hypothesis: Finding sparse, trainable neu-\nral networks. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nYaroslav Ganin and Victor S. Lempitsky. 2015. Unsu-\npervised domain adaptation by backpropagation. In\nProceedings of the 32nd International Conference\non Machine Learning, ICML 2015, Lille, France,\n6-11 July 2015 , volume 37 of JMLR Workshop\nand Conference Proceedings , pages 1180–1189.\nJMLR.org.\nSong Han, Huizi Mao, and William J Dally. 2015a.\nDeep compression: Compressing deep neural net-\nworks with pruning, trained quantization and huff-\nman coding. ArXiv preprint, abs/1510.00149.\nSong Han, Jeff Pool, John Tran, and William J. Dally.\n2015b. Learning both weights and connections for\nefﬁcient neural network. In Advances in Neural\nInformation Processing Systems 28: Annual Con-\nference on Neural Information Processing Systems\n2015, December 7-12, 2015, Montreal, Quebec,\nCanada, pages 1135–1143.\nIrina Higgins, David Amos, David Pfau, Sebastien\nRacaniere, Loic Matthey, Danilo Rezende, and\nAlexander Lerchner. 2018. Towards a Deﬁnition\nof Disentangled Representations. ArXiv preprint ,\nabs/1812.02230.\nIrina Higgins, Loïc Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir\nMohamed, and Alexander Lerchner. 2017. beta-vae:\nLearning basic visual concepts with a constrained\nvariational framework. In 5th International Con-\nference on Learning Representations, ICLR 2017,\nToulon, France, April 24-26, 2017, Conference\nTrack Proceedings. OpenReview.net.\nGeoffrey Hinton, Nitsh Srivastava, and Kevin Swersky.\n2012. Neural networks for machine learning. Cours-\nera, video lectures, 264(1).\nSarthak Jain, Edward Banner, Jan-Willem van de\nMeent, Iain J. Marshall, and Byron C. Wallace. 2018.\nLearning disentangled representations of texts with\napplication to biomedical abstracts. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4683–4693,\nBrussels, Belgium. Association for Computational\nLinguistics.\nVineet John, Lili Mou, Hareesh Bahuleyan, and Olga\nVechtomova. 2019. Disentangled representation\nlearning for non-parallel text style transfer. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics , pages 424–434,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nDivyansh Kaushik, Eduard H. Hovy, and\nZachary Chase Lipton. 2020. Learning the differ-\nence that makes A difference with counterfactually-\naugmented data. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nHyunjik Kim and Andriy Mnih. 2018. Disentangling\nby factorising. In Proceedings of the 35th Inter-\nnational Conference on Machine Learning, ICML\n2018, Stockholmsmässan, Stockholm, Sweden, July\n10-15, 2018, volume 80 of Proceedings of Machine\nLearning Research, pages 2654–2663. PMLR.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In 2nd International\nConference on Learning Representations, ICLR\n2014, Banff, AB, Canada, April 14-16, 2014, Con-\nference Track Proceedings.\nTejas D. Kulkarni, William F. Whitney, Pushmeet\nKohli, and Joshua B. Tenenbaum. 2015. Deep con-\nvolutional inverse graphics network. In Advances in\nNeural Information Processing Systems 28: Annual\nConference on Neural Information Processing Sys-\ntems 2015, December 7-12, 2015, Montreal, Quebec,\nCanada, pages 2539–2547.\nMaria Larsson, Amanda Nilsson, and Mikael Kåge-\nbäck. 2017. Disentangled representations for ma-\nnipulation of sentiment in text. ArXiv preprint ,\nabs/1712.10066.\nTal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and\nDieuwke Hupkes, editors. 2019. Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP . Association\nfor Computational Linguistics, Florence, Italy.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nFrancesco Locatello, Gabriele Abbati, Thomas Rain-\nforth, Stefan Bauer, Bernhard Schölkopf, and\nOlivier Bachem. 2019a. On the fairness of dis-\nentangled representations. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 14584–14597.\nFrancesco Locatello, Stefan Bauer, Mario Lucic, Gun-\nnar Rätsch, Sylvain Gelly, Bernhard Schölkopf, and\nOlivier Bachem. 2019b. Challenging common as-\nsumptions in the unsupervised learning of disentan-\ngled representations. In Proceedings of the 36th In-\nternational Conference on Machine Learning, ICML\n2019, 9-15 June 2019, Long Beach, California, USA,\nvolume 97 of Proceedings of Machine Learning Re-\nsearch, pages 4114–4124. PMLR.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\n788\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nLaurens van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(Nov):2579–2605.\nRemi Mir, Bjarke Felbo, Nick Obradovich, and Iyad\nRahwan. 2019. Evaluating style transfer for text. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 495–504,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMehrad Moradshahi, Hamid Palangi, Monica S Lam,\nPaul Smolensky, and Jianfeng Gao. 2019. Hubert\nuntangles bert to improve transfer across nlp tasks.\nArXiv preprint, abs/1910.12647.\nPreslav Nakov, Sara Rosenthal, Zornitsa Kozareva,\nVeselin Stoyanov, Alan Ritter, and Theresa Wilson.\n2013. SemEval-2013 task 2: Sentiment analysis in\nTwitter. In Second Joint Conference on Lexical and\nComputational Semantics (*SEM), Volume 2: Pro-\nceedings of the Seventh International Workshop on\nSemantic Evaluation (SemEval 2013) , pages 312–\n320, Atlanta, Georgia, USA. Association for Com-\nputational Linguistics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nShauli Ravfogel, Yanai Elazar, Jacob Goldberger, and\nYoav Goldberg. 2020. Unsupervised distillation of\nsyntactic information from contextualized word rep-\nresentations. In Proceedings of the Third Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP , pages 91–106, Online.\nAssociation for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019. The risk of racial bias\nin hate speech detection. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1668–1678, Florence,\nItaly. Association for Computational Linguistics.\nTianxiao Shen, Tao Lei, Regina Barzilay, and Tommi S.\nJaakkola. 2017. Style transfer from non-parallel text\nby cross-alignment. In Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, De-\ncember 4-9, 2017, Long Beach, CA, USA , pages\n6830–6841.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-\n50M: Pushing the limits of paraphrastic sentence em-\nbeddings with millions of machine translations. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 451–462, Melbourne, Australia.\nAssociation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 5754–5764.\nYuan Zhang, Regina Barzilay, and Tommi Jaakkola.\n2017. Aspect-augmented adversarial networks for\ndomain adaptation. Transactions of the Association\nfor Computational Linguistics, 5:515–528.\nMengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hin-\nrich Schütze. 2020. Masking as an efﬁcient alter-\nnative to ﬁnetuning for pretrained language models.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2226–2241, Online. Association for Computa-\ntional Linguistics.\n789\nA Appendix\nA.1 Additional IMDB Results\nIn Table A.1 we report results treating genre clas-\nsiﬁcation as the main task, and sentiment as the\n‘protected’ attribute.\nGenre ↑ Leakage (Sentiment) ↓\nDRLST 65.6 61.1\nDRLST-BERT 71.4 70.3\nUntuned 81.5 82.3\nFinetuned 87.3 86.0\nAdversarial 85.0 75.5\nMasked Weights 87.0 73.0\nMasked Hidden 85.0 79.0\nTable A.1: Performance of models when treating genre\nprediction as the main task and sentiment as the ‘pro-\ntected’ variable. The DRLST baselines perform poorly\non the main task. The proposed masking methods\nachieve performance comparable to other baselines,\nwhile encoding less of the sentiment information.\nA.2 Distribution of Learned Masks Across\nBERT Layers\nHere we inspect the subnetworks (i.e., the weights\nor hidden activations that are not masked) uncov-\nered by our model, which may provide insights re-\ngarding where pretrained (masked) language mod-\nels encode different sorts of linguistic information.\nFigure A.1 shows the distributions of the two types\nof masks (weights and hidden activations, respec-\ntively) over the layers within BERT for for the se-\nmantics/syntax tasks. We observe that the learned\n‘semantic’ mask zeros out fewer elements at higher\nlayers in the network, while the ‘syntax’ mask\nprefers to keep non-zero entries in lower layers.\nThis suggests that semantic information may be\ncaptured mostly in higher layers of BERT, while\nsyntactic information may be encoded in lower\nlayers, consistent with observations in prior work\n(Tenney et al., 2019).\nA.3 Semantic Representation Performance\n(vs. BERT)\nWe show the signed differences between the perfor-\nmance achieved on semantics- and syntax-oriented\ntasks by BERT embeddings (we mean-pool over\ntoken embeddings) and the ‘semantic’ representa-\ntions from the disentangled models in ﬁgure A.2.\n7 8 9 10 11 12\nBERT Layer\n0.0\n0.1\n0.2\n0.3\n0.4Percentage of Non-zero Masks\nSemantic\nSyntax\n(a) Masking Weights\n7 8 9 10 11 12\nBERT Layer\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12Percentage of Non-zero Masks\nSemantic\nSyntax\n(b) Masking Hidden Activations\nFigure A.1: Non-zero (unmasked) elements of the se-\nmantic and syntax subnetworks induced by masking\nweights (a) and hidden representations / activations (b).\n          WC↑              STS↑.           Depth↓           TopCon↓  \nRepresentation Type: Semantic\nVGVAE\nFinetuned\nMasked Weight\nMasked Hidden\nSemantic Task Syntax Task\nFigure A.2: Differences between the performances\nachieved via BERT embeddings and the disentangled\nmodel variants considered on semantics-oriented (WC,\nSTS) and syntax-oriented (Depth, TopCon) tasks com-\npared with BERT embeddings. We plot this difference\nwith respect to the semantic embeddings induced by the\nmodels.\n790\nRepresentation: \nSemantic\nRepresentation: \nSyntax\nFigure A.3: Model performance as a function of the degree of pruning. The x-axis corresponds to the subnetwork\nsparsities (percent of weights dropped), while the y axes are performance measures — accuracy for all tasks\nexcept for STS, where we report Pearson’s correlation. We compare the performance of models trained on the\nsemantic (top) and syntax representations (bottom) learned by the disentangling strategies considered, after pruning\nto varying levels of sparsity.\nLayers Masked Sentiment ↑ Leakage (Genre) ↓\nLast 3 86.3 73.5\nLast 6 88.0 72.0\nLast 9 88.0 72.8\nAll 12 87.5 72.3\nTable A.2: Performance of model (Masking Weights)\nwhen masking the last 3, 6, 9 or 12 layers of BERT.\nα Sentiment ↑ Leakage (Genre) ↓\n0.5 83.8 74.8\n1.0 86.3 73.0\n2.0 88.0 72.0\n5.0 87.8 70.3\nTable A.3: Performance of model (Masking Weights)\nwhen choosing different αfor the triplet loss.\nA.4 Model performance with iterative\nmagnitude pruning\nWe report full results of combining our method\nwith magnitude pruning to uncover sparse sub-\nnetworks in Figure A.3. We compare our method\nto several alternative pruning strategies: Standard\nmagnitude tuning applied to BERT, without addi-\ntional tuning (Pruned + Untuned), and a method in\nwhich after magnitude pruning we resume ﬁnetun-\ning of the subnetwork for a ﬁxed number of steps,\nusing the aforementioned loss function (Pruned +\nFinetuned).\nA.5 Additional Experiments:Perturbation\nStudy of Hyper-parameters\nWe report model performance when masking dif-\nferent number of layers of BERT (Table A.2) and\nwhen choosing different values for α(Table A.3).\nA.6 Model performance with varying degree\nof correlation in the training set\nWe report the comparison of our model (Masking\nWeights) with two baselines (Finetuned and Ad-\nversarially trained BERT) with varying degree of\ncorrelation in the training set. The task is senti-\nment classiﬁcation and we control the correlation\nbetween sentiment and genre into 3 different set-\ntings: strong, moderate and weak (if any) correla-\ntion. We report the results in Table A.4. Our model\nsigniﬁcantly outperforms the baselines when the\ncorrelation is strong, and the advantage begins to\ndiminish as the correlation becomes weaker, as we\nwould expect.\n791\nFinetuned Adversarial Masking Weights\nStrong Correlation: 15% of \"Drama\" Reviews are Positive\nAvg.(Sentiment) 84.3 84.5 87.8\nWorst (Sentiment) 69.3 72.0 77.0\nModerate Correlation: 25% of \"Drama\" Reviews are Positive\nAvg.(Sentiment) 86.8 86.5 86.3\nWorst (Sentiment) 72.0 76.8 80.5\nNo Correlation: 50% of \"Drama\" Reviews are Positive\nAvg.(Sentiment) 88.5 88.0 86.8\nWorst (Sentiment) 86.0 87.0 85.3\nTable A.4: Performance of our model (Masking Weights) compared to baseline models with varying degree of\ncorrelation in the training set.",
  "topic": "ENCODE",
  "concepts": [
    {
      "name": "ENCODE",
      "score": 0.7936798334121704
    },
    {
      "name": "Computer science",
      "score": 0.7867403030395508
    },
    {
      "name": "Transformer",
      "score": 0.5980914235115051
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5454207062721252
    },
    {
      "name": "Natural language processing",
      "score": 0.5054483413696289
    },
    {
      "name": "Discriminative model",
      "score": 0.4814402461051941
    },
    {
      "name": "Syntax",
      "score": 0.47335028648376465
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4597284197807312
    },
    {
      "name": "Masking (illustration)",
      "score": 0.4287717342376709
    },
    {
      "name": "Programming language",
      "score": 0.10832288861274719
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}