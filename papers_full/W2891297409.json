{
  "title": "Code-switched Language Models Using Dual RNNs and Same-Source Pretraining",
  "url": "https://openalex.org/W2891297409",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2122846389",
      "name": "Saurabh Garg",
      "affiliations": [
        "Indian Institute of Technology Bombay"
      ]
    },
    {
      "id": "https://openalex.org/A2624127707",
      "name": "Tanmay Parekh",
      "affiliations": [
        "Indian Institute of Technology Bombay"
      ]
    },
    {
      "id": "https://openalex.org/A2088806615",
      "name": "Preethi Jyothi",
      "affiliations": [
        "Indian Institute of Technology Bombay"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2114569717",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2034585809",
    "https://openalex.org/W2098355803",
    "https://openalex.org/W2099808146",
    "https://openalex.org/W2094655846",
    "https://openalex.org/W2153433699",
    "https://openalex.org/W2407467516",
    "https://openalex.org/W2149778059",
    "https://openalex.org/W2798348125",
    "https://openalex.org/W2123162330",
    "https://openalex.org/W2767289498",
    "https://openalex.org/W2084543186",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W2884096449",
    "https://openalex.org/W2807157666",
    "https://openalex.org/W2031292349",
    "https://openalex.org/W2950304420"
  ],
  "abstract": "This work focuses on building language models (LMs) for code-switched text. We propose two techniques that significantly improve these LMs: 1) A novel recurrent neural network unit with dual components that focus on each language in the code-switched text separately 2) Pretraining the LM using synthetic text from a generative model estimated using the training data. We demonstrate the effectiveness of our proposed techniques by reporting perplexities on a Mandarin-English task and derive significant reductions in perplexity.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3078–3083\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n3078\nCode-switched Language Models\nUsing Dual RNNs and Same-Source Pretraining\nSaurabh Garg∗ Tanmay Parekh∗\nIndian Institute of Technology, Bombay\n{saurabhgarg,tanmayb,pjyothi}@cse.iitb.ac.in\nPreethi Jyothi\nAbstract\nThis work focuses on building language mod-\nels (LMs) for code-switched text. We pro-\npose two techniques that signiﬁcantly improve\nthese LMs: 1) A novel recurrent neural net-\nwork unit with dual components that focus on\neach language in the code-switched text sep-\narately 2) Pretraining the LM using synthetic\ntext from a generative model estimated using\nthe training data. We demonstrate the effec-\ntiveness of our proposed techniques by report-\ning perplexities on a Mandarin-English task\nand derive signiﬁcant reductions in perplexity.\n1 Introduction\nCode-switching is a widespread linguistic phe-\nnomenon among multilingual speakers that in-\nvolves switching between two or more languages\nin the course of a single conversation or within a\nsingle sentence (Auer, 2013). Building speech and\nlanguage technologies to handle code-switching\nhas become a fairly active area of research and\npresents a number of interesting technical chal-\nlenges (C ¸ etinoglu et al., 2016). Language mod-\nels for code-switched text is an important prob-\nlem with implications to downstream applications\nsuch as speech recognition and machine transla-\ntion of code-switched data. A natural choice for\nbuilding such language models would be to use\nrecurrent neural networks (RNNs) (Mikolov et al.,\n2010), which yield state-of-the-art language mod-\nels in the case of monolingual text. In this work,\nwe explore mechanisms that can signiﬁcantly im-\nprove upon such a baseline when applied to code-\nswitched text. Speciﬁcally, we develop two such\nmechanisms:\n•We alter the structure of an RNN unit to in-\nclude separate components that focus on each\n∗Joint ﬁrst authors\nlanguage in code-switched text separately while\ncoordinating with each other to retain contex-\ntual information across code-switch boundaries.\nOur new model is called a Dual RNN Language\nModel (D-RNNLM), described in Section 2.\n•We propose using same-source pretraining –\ni.e., pretraining the model using data sampled\nfrom a generative model which is itself trained\non the given training data – before training the\nmodel on the same training data (see Section 3).\nWe ﬁnd this to be a surprisingly effective strat-\negy.\nWe study the improvements due to these tech-\nniques under various settings (e.g., with and with-\nout access to monolingual text in the candidate\nlanguages for pretraining). We use perplexity as\na proxy to measure the quality of the language\nmodel, evaluated on code-switched text in English\nand Mandarin from the SEAME corpus. Both the\nproposed techniques are shown to yieldsigniﬁcant\nperplexity improvements (up to 13% relative)over\ndifferent baseline RNNLM models (trained with a\nnumber of additional resources). We also explore\nhow to combine the two techniques effectively.\nRelated Work: Adel et al. (2013) was one of\nthe ﬁrst works to explore the use of RNNLMs\nfor code-switched text. Many subsequent works\nexplored the use of external sources to enhance\ncode-switched LMs, including the use of part-of-\nspeech (POS) tags, syntactic and semantic fea-\ntures (Yeh et al., 2010; Adel et al., 2014, 2015)\nand the use of machine translation systems to gen-\nerate synthetic text (Vu et al., 2012). Prior work\nhas also explored the use of interpolated LMs\ntrained separately on monolingual texts (Bhuvana-\ngiri and Kopparapu, 2010; Imseng et al., 2011;\nLi et al., 2011; Baheti et al., 2017). Linguis-\ntic constraints governing code-switching have also\n3079\nOut\nLSTM (L1)\n0\n0\n LSTM (L0)\n#0\n #1\nEmb0\nEmb1\n0\n0\nτ2\nb2\nOut\nLSTM (L1)\n0\n0\n LSTM (L0)\n#0\n #1\nEmb0\nEmb1\n0\n0\nτ1\nb1\nFigure 1: Illustration of the dual RNNLM (see the text for\na detailed description). The highlighted left-to-right path (in\ngreen) indicates the ﬂow of state information, when b1 = 0\nand b2 = 1(corresponding to token τ1 belonging to language\nL0 and τ2 belonging to L1). The highlighted bottom-to-top\npath (in orange) indicates the inputs and outputs.\nbeen used as explicit priors to model when peo-\nple switch from one language to another. Fol-\nlowing this line of enquiry, (Chan et al., 2004)\nused grammar rules to model code-switching; (Li\nand Fung, 2013, 2014) incorporated syntactic con-\nstraints with the help of a code-switch boundary\nprediction model; (Pratapa et al., 2018) used a lin-\nguistically motivated theory to create grammati-\ncally consistent synthetic code-mixed text.\n2 Dual RNN Language Models\nTowards improving the modeling of code-\nswitched text, we introduce Dual RNN Language\nModels (D-RNNLMs). The philosophy behind D-\nRNNLMs is that two different sets of neurons will\nbe trained to (primarily) handle the two languages.\n(In prior work (Garg et al., 2018), we applied sim-\nilar ideas to build dual N-gram based language\nmodels for code-switched text.)\nAs shown in Figure 1, the D-RNNLM consists\nof a “Dual LSTM cell” and an input encoding\nlayer. The Dual LSTM cell, as the name indi-\ncates, has two long short-term memory (LSTM)\ncells within it. The two LSTM cells are desig-\nnated to accept input tokens from the two lan-\nguages L0 and L1 respectively, and produce an\n(unnormalized) output distribution over the tokens\nin the same language. When a Dual LSTM cell is\ninvoked with an input tokenτ, the two cells will be\ninvoked sequentially. The ﬁrst (upstream) LSTM\ncell corresponds to the language that τ belongs to,\nand gets τ as its input. It passes on the resulting\nstate to the downstream LSTM cell (which takes\na dummy token as input). The unnormalized out-\nputs from the two cells are combined and passed\nthrough a soft-max operation to obtain a distribu-\ntion over the union of the tokens in the two lan-\nguages. Figure 1 shows a circuit representation\nof this conﬁguration, using multiplexers (shaded\nunits) controlled by a selection bit bi such that the\nith token τi belongs to Lbi .\nThe input encoding layer also uses multiplexers\nto direct the input token to the upstream LSTM\ncell. Two dummy tokens #0 and #1 are added\nto L0 and L1 respectively, to use as inputs to the\ndownstream LSTM cell. The input tokens are en-\ncoded using an embedding layer of the network\n(one for each language), which is trained along\nwith the rest of the network to minimize a cross-\nentropy loss function.\nThe state-update and output functions of the\nDual LSTM cell can be formally described as fol-\nlows. It takes as input (b,τ) where bis a bit and\nτ is an input token, as well as a state vector of the\nform (h0,h1) corresponding to the state vectors\nproduced by its two constituent LSTMs. Below\nwe denote the state-update and output functions of\nthese two LSTMs as Hb(τ,h) and Ob(τ,h) (for\nb= 0,1):\nH((b,τ),(h0,h1)) = (h′\n0,h′\n1) where\n(h′\n0,h′\n1) =\n{\n(H0(τ,h0),H1(#1,h′\n0)) if b= 0\n(H0(#0,h′\n1),H1(τ,h1)) if b= 1\nO((b,τ),(h0,h1)) = softmax(o0,o1) where\n(o0,o1) =\n{\n(O0(τ,h0),O1(#1,h′\n0)) if b= 0\n(O0(#0,h′\n1),O1(τ,h1)) if b= 1.\nNote that above, the inputs to the downstream\nLSTM functions H1−b and O1−b are expressed in\nterms of h′\nb which is produced by the upstream\nLSTM.\n3 Same-Source Pretraining\nBuilding robust LMs for code-switched text is\nchallenging due to the lack of availability of\nlarge amounts of training data. One solution is\nto artiﬁcially generate code-switched to augment\nthe training data. We propose a variant of this\napproach – called same-source pretraining – in\nwhich the actual training data itself is used to train\na generative model, and the data sampled from this\nmodel is used to pretrain the language model.\nSame-source pretraining can leverage powerful\ntraining techniques for generative models to train a\n3080\nTrain Dev Test\n# Utterances 74,927 9,301 9,552\n# Tokens 977,751 131,230 114,546\n# English Tokens 316,726 30,154 50,537\n# Mandarin Tokens661,025 101,076 64,009\nTable 1: Statistics of data splits derived from SEAME.\nlanguage model. We note that the generative mod-\nels by themselves are typically trained to minimize\na different objective function (e.g., a discrimina-\ntion loss) and need not perform well as language\nmodels.∗\nOur default choice of generative model will\nbe an RNN (but see the end of this paragraph).\nTo complete the speciﬁcation of same-source pre-\ntraining, we need to specify how it is trained\nfrom the given data. Neural language models\ntrained using the maximum likelihood training\nparadigm tend to suffer from the exposure bias\nproblem during inference when the model gener-\nates a text sequence by conditioning on previous\ntokens that may never have appeared during train-\ning. Scheduled sampling (Bengio et al., 2015) can\nhelp bridge this gap between the training and in-\nference stages by using model predictions to syn-\nthesize preﬁxes of text that are used during train-\ning, rather than using the actual text tokens. A\nmore promising alternative to generate text se-\nquences was recently proposed by Yu et al. (2017)\nwhere sequence generation is modeled in a genera-\ntive adversarial network (GAN) based framework.\nThis model – referred to as “SeqGAN” – consists\nof a generator RNN and a discriminator network\ntrained as a binary classiﬁer to distinguish between\nreal and generated sequences. The main innova-\ntion of SeqGAN is to train the generative model\nusing policy gradients (inspired by reinforcement\nlearning) and use the discriminator to determine\nthe reward function. We experimented with using\nboth na¨ıve and scheduled sampling based training;\nusing SeqGAN was a consistently better choice (5\npoints or less in terms of test perplexities) com-\npared to these two sampling methods. As such,\nwe use SeqGAN as our training method for the\ngenerator. We also experiment with replacing the\nRNN with a Dual RNN as the generator in the Se-\nqGAN training and observe small but consistent\nreductions in perplexity.\n∗In our experiments, we found the preplexity measures\nfor the generative models to be an order of magnitude larger\nthan that of the LMs we construct.\n4 Experiments and Results\nDataset Preparation: For our experiments, we\nuse code-switched text from the SEAME cor-\npus (Lyu et al., 2010) which contains conversa-\ntional speech in Mandarin and English. Since\nthere is no standardized task based on this corpus,\nwe construct our own training, development and\ntest sets using a random 80-10-10 split. Table 1\nshows more details about our data sets. (Speakers\nwere kept disjoint across these datasets.)\nEvaluation Metric: We use token-level per-\nplexity as the evaluation metric where tokens are\nwords in English and characters in Mandarin.\nThe SEAME corpus provides word boundaries for\nMandarin text. However, we used Mandarin char-\nacters as individual tokens since a large proportion\nof Mandarin words appeared very sparsely in the\ndata. Using Mandarin characters as tokens helped\nalleviate this issue of data sparsity; also, applica-\ntions using Mandarin text are typically evaluated\nat the character level and do not rely on having\nword boundary markers (Vu et al., 2012).\nOutline of Experiments: Section 4.1 will ex-\nplore the beneﬁts of both our proposed techniques\n– (1) using D-RNNLMs and (2) using text gen-\nerated from SeqGAN for pretraining – in isolation\nand in combination. Section 4.2 will introduce two\nadditional resources (1) monolingual text for pre-\ntraining and (2) a set of syntactic features used as\nadditional input to the RNNLMs that further im-\nprove baseline perplexities. We show that our pro-\nposed techniques continue to outperform the base-\nlines albeit with a smaller margin. All these per-\nplexity results have been summarized in Table 2.\n4.1 Improvements Over the Baseline\nThis section focuses only on the numbers listed\nin the ﬁrst two columns of Table 2. The Base-\nline model is a1-layer LSTM LM with512 hidden\nnodes, input and output embedding dimensionality\nof 512, trained using SGD with an initial learning\nrate of 1.0 (decayed exponentially after 80 epochs\nat a rate of 0.98 till 100 epochs) The develop-\nment and test set perplexities using the baseline\nare 89.60 and 74.87, respectively.\nThe D-RNNLM is a 1-layer language model\nwith each LSTM unit having 512 hidden nodes.\nThe training paradigm is similar to the above-\nmentioned setting for the baseline model.†We see\n†D-RNNLMs have a few additional parameters. How-\n3081\nw/o syntactic features with syntactic features\nw/o mono. data with mono. data w/o mono. data with mono. data\nDev Test Dev Test Dev Test Dev Test\nBaseline 89.60 74.87 74.06 61.66 81.87 68.23 71.04 59.00\nD-RNNLM 88.68 72.29 72.41 60.73 81.01 66.26 70.83 59.04\nWith RNNLM SeqGAN 79.16 65.96 72.51 60.56 77.30 63.75 68.43 55.71\nWith D-RNNLM SeqGAN 78.63 65.41 72.33 60.30 77.19 63.63 67.79 55.60\nTable 2: Development set and test set perplexities using RNNLMs and D-RNNLMs with various pretraining strategies.\nconsistent improvements in test perplexity when\ncomparing a D-RNNLM with an RNNLM (i.e.\n74.87 drops to 72.29).‡\nNext, we use text generated from a SeqGAN\nmodel to pretrain the RNNLM. §We use our best\ntrained RNNLM baseline as the generator within\nSeqGAN. We sample 157,440 sentences (with\na ﬁxed sentence length of 20) from the Seq-\nGAN model; this is thrice the amount of code-\nswitched training data. We ﬁrst pretrain the base-\nline RNNLM with this sampled text, before train-\ning it again on the code-switched text. This gives\nsigniﬁcant reductions in test perplexity, bringing it\ndown to 65.96 (from 74.87).\nFinally, we combine both our proposed tech-\nniques by replacing the generator with our best-\ntrained D-RNNLM within SeqGAN. Although\nthere are other ways of combining both our pro-\nposed techniques, e.g. pretraining a D-RNNLM\nusing data sampled from an RNNLM SeqGAN,\nwe found this method of combination to be most\neffective. We see modest but consistent improve-\nments with D-RNNLM SeqGAN over RNNLM\nSeqGAN in Table 2, further validating the utility\nof D-RNNLMs.\n4.2 Using Additional Resources\nWe employed two additional resources to further\nimprove our baseline models. First, we used\nmonolingual text in the candidate languages to\npretrain the RNNLM and D-RNNLM models. We\nused transcripts from the Switchboard corpus¶for\nEnglish; AIShell ∥ and THCHS30 ∗∗ corpora for\never, increasing the capacity of an RNNLM to exactly match\nthis number makes its test perplexity worse; RNNLM with\n720 hidden units gives a development set perplexity of 91.44\nand 1024 hidden units makes it 91.46.\n‡Since D-RNNLMs use language ID information, we\nalso trained a baseline RNNLM with language ID features;\nthis did not help reduce the baseline test perplexities. In fu-\nture work, we will explore alternate LSTM-based models that\nincorporate language ID information (Chandu et al., 2018)\n§To implement SeqGAN, we use code from https://\ngithub.com/LantaoYu/SeqGAN.\n¶http://www.openslr.org/5/\n∥http://www.openslr.org/33/\n∗∗http://www.openslr.org/18/\nMandarin monolingual text data. This resulted in\na total of ≈3.1 million English tokens and ≈2.5\nmillion Mandarin tokens. Second, we used an\nadditional set of input features to the RNNLMs\nand D-RNNLMs that were found to be useful for\ncode-switching in prior work (Adel et al., 2014).\nThe feature set included part-of-speech (POS) tag\nfeatures and Brown word clusters (Brown et al.,\n1992), along with a language ID feature. We ex-\ntracted POS tags using the Stanford POS-tagger††\nand we clustered the words into 70 classes using\nthe unsupervised clustering algorithm by Brown\net al. (1992) to get Brown cluster features.\nThe last six columns in Table 2 show the util-\nity of using either one of these resources or both\nof them together (shown in the last two columns).\nThe perplexity reductions are largest (compared to\nthe numbers in the ﬁrst two columns) when com-\nbining both these resources together. Interestingly,\nall the trends we observed in Section 4.1 still hold.\nD-RNNLMs still consistently perform better than\ntheir RNNLM counterparts and we obtain the best\noverall results using D-RNNLM SeqGAN.\n5 Discussion and Analysis\nEng-EngEng-ManMan-EngMan-Man\nRNNLM 133.18 157.18 2617.28 34.98\nD-RNNLM 140.37 151.38 2452.16 32.89\nMono RNNLM101.61 181.28 2510.48 30.00\nMono D-RNNLM101.66 156.44 2442.81 29.64\nRNNLM SeqGAN120.28 154.44 2739.85 30.40\nD-RNNLM SeqGAN120.26 149.68 2450.85 30.60\nTable 3: Decomposed perplexities on the development set\non all four types of tokens from various models.\nTable 3 shows how the perplexities on the de-\nvelopment set from six of our prominent mod-\nels decompose into the perplexities contributed by\nEnglish tokens preceded by English tokens (Eng-\nEng), Eng-Man, Man-Eng and Man-Man tokens.\nThis analysis reveals a number of interesting ob-\nservations. 1) The D-RNNLM mainly improves\nover the baseline on the “switching tokens”, Eng-\n††https://nlp.stanford.edu/software/\ntagger.shtml\n3082\nMan and Man-Eng. 2) The RNNLM with mono-\nlingual data improves most over the baseline on\n“the monolingual tokens”, Eng-Eng and Man-\nMan, but suffers on the Eng-Man tokens. The D-\nRNNLM with monolingual data does as well as\nthe baseline on the Eng-Man tokens and performs\nbetter than “Mono RNNLM” on all other tokens.\n3) RNNLM SeqGAN suffers on the Man-Eng to-\nkens, but helps on the rest; in contrast, D-RNNLM\nSeqGAN helps on all tokens when compared with\nthe baseline.\nSeqGAN-RNNLM SeqGAN-DLM\nBigram 25.57 31.33\nTrigram 75.88 83.86\nQuadgram 137 .98 145 .71\nTable 4: Percentage of new n-grams generated.\nAs an additional measure of the quality of text\ngenerated by RNNLM SeqGAN and D-RNNLM\nSeqGAN, in Table 4, we measure the diversity\nin the generated text by looking at the increase\nin the number of unique n-grams with respect to\nthe SEAME training text. D-RNNLM SeqGAN is\nclearly better at generating text with larger diver-\nsity, which could be positively correlated with the\nperplexity improvements shown in Table 2.\nWhile we do not claim same-source pretraining\nmay be an effective strategy in general, we show\nit is useful in low training-data scenarios. Even\nwith only 1\n16 th of the original SEAME training\ndata used for same-source pretraining, develop-\nment and test perplexities are reduced to 84.45 and\n70.59, respectively (compared to 79.16 and 65.96\nusing the entire training data).\n6 Conclusion\nD-RNNLMs and same-source pretraining pro-\nvide signiﬁcant perplexity reductions for code-\nswitched LMs. These techniques may be of more\ngeneral interest. Leveraging generative models to\ntrain LMs is potentially applicable beyond code-\nswitching; D-RNNLMs could be generalized be-\nyond LMs, e.g. speaker diarization. We leave\nthese for future work to explore.\n7 Acknowledgments\nThe authors thank the anonymous reviewers for\ntheir helpful comments and suggestions. The last\nauthor gratefully acknowledges ﬁnancial support\nfrom Microsoft Research India (MSRI) for this\nproject.\nReferences\nHeike Adel, Dominic Telaar, Ngoc Thang Vu, Katrin\nKirchhoff, and Tanja Schultz. 2014. Combining re-\ncurrent neural networks and factored language mod-\nels during decoding of code-switching speech. In\nProceedings of Interspeech.\nHeike Adel, Ngoc Thang Vu, Katrin Kirchhoff, Do-\nminic Telaar, and Tanja Schultz. 2015. Syntactic\nand semantic features for code-switching factored\nlanguage models. Proceedings of IEEE Transac-\ntions on Audio, Speech, and Language Processing ,\n23(3):431–440.\nHeike Adel, Ngoc Thang Vu, Franziska Kraus, Tim\nSchlippe, Haizhou Li, and Tanja Schultz. 2013. Re-\ncurrent neural network language modeling for code\nswitching conversational speech. In Proceedings of\nICASSP, pages 8411–8415. IEEE.\nPeter Auer. 2013. Code-switching in conversation:\nLanguage, interaction and identity. Routledge.\nAshutosh Baheti, Sunayana Sitaram, Monojit Choud-\nhury, and Kalika Bali. 2017. Curriculum design for\ncode-switching: Experiments with language iden-\ntiﬁcation and language modeling with deep neural\nnetworks. In Proceedings of ICON, pages 65–74.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Proceedings of NIPS, pages 1171–1179.\nK Bhuvanagiri and Sunil Kopparapu. 2010. An ap-\nproach to mixed language automatic speech recog-\nnition. Proceedings of Oriental COCOSDA, Kath-\nmandu, Nepal.\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language.\nComputational linguistics, 18(4):467–479.\n¨Ozlem C ¸ etinoglu, Sarah Schulz, and Ngoc Thang Vu.\n2016. Challenges of computational processing of\ncode-switching. Proceedings of EMNLP, page 1.\nJoyce YC Chan, PC Ching, Tan Lee, and Helen M\nMeng. 2004. Detection of language boundary in\ncode-switching utterances by bi-phone probabilities.\nIn Proceedings of International Symposium on Chi-\nnese Spoken Language Processing, pages 293–296.\nIEEE.\nKhyathi Chandu, Thomas Manzini, Sumeet Singh, and\nAlan W Black. 2018. Language informed modeling\nof code-switched text. In Proceedings of the Third\nWorkshop on Computational Approaches to Linguis-\ntic Code-Switching, pages 92–97.\nSaurabh Garg, Tanmay Parekh, and Preethi Jyothi.\n2018. Dual language models for code mixed speech\nrecognition. In Proceedings of Interspeech.\n3083\nDavid Imseng, Herv ´e Bourlard, Mathew Magimai\nDoss, and John Dines. 2011. Language dependent\nuniversal phoneme posterior estimation for mixed\nlanguage speech recognition. In Proceedings of\nICASSP, pages 5012–5015.\nYing Li and Pascale Fung. 2013. Improved mixed lan-\nguage speech recognition using asymmetric acoustic\nmodel and language model with code-switch inver-\nsion constraints. In Proceedings of ICASSP, pages\n7368–7372. IEEE.\nYing Li and Pascale Fung. 2014. Language modeling\nwith functional head constraint for code switching\nspeech recognition. In Proceedings of EMNLP.\nYing Li, Pascale Fung, Ping Xu, and Yi Liu. 2011.\nAsymmetric acoustic modeling of mixed language\nspeech. In Proceedings of ICASSP , pages 5004–\n5007.\nDau-Cheng Lyu, Tien-Ping Tan, Eng-Siong Chng, and\nHaizhou Li. 2010. An analysis of a Mandarin-\nEnglish code-switching speech corpus: SEAME.\nProceedings of Age, 21:25–8.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proceed-\nings of Interspeech.\nAdithya Pratapa, Gayatri Bhat, Monojit Choudhury,\nSunayana Sitaram, Sandipan Dandapat, and Kalika\nBali. 2018. Language modeling for code-mixing:\nThe role of linguistic theory based synthetic data. In\nProceedings of ACL, volume 1, pages 1543–1553.\nNgoc Thang Vu, Dau-Cheng Lyu, Jochen Weiner, Do-\nminic Telaar, Tim Schlippe, Fabian Blaicher, Eng-\nSiong Chng, Tanja Schultz, and Haizhou Li. 2012.\nA ﬁrst speech recognition system for Mandarin-\nEnglish code-switch conversational speech. In Pro-\nceedings of ICASSP, pages 4889–4892. IEEE.\nChing Feng Yeh, Chao Yu Huang, Liang Che Sun, and\nLin Shan Lee. 2010. An integrated framework for\ntranscribing Mandarin-English code-mixed lectures\nwith improved acoustic and language modeling. In\nProceedings of Chinese Spoken Language Process-\ning (ISCSLP), 2010 7th International Symposium\non, pages 214–219. IEEE.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In Proceedings of AAAI, pages\n2852–2858.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.956081748008728
    },
    {
      "name": "Computer science",
      "score": 0.737667441368103
    },
    {
      "name": "Code (set theory)",
      "score": 0.6062806248664856
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5957748293876648
    },
    {
      "name": "Focus (optics)",
      "score": 0.5835297107696533
    },
    {
      "name": "Language model",
      "score": 0.5816637277603149
    },
    {
      "name": "Mandarin Chinese",
      "score": 0.5719003677368164
    },
    {
      "name": "Natural language processing",
      "score": 0.5233237743377686
    },
    {
      "name": "Dual (grammatical number)",
      "score": 0.5138223767280579
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4871532618999481
    },
    {
      "name": "Task (project management)",
      "score": 0.4870658218860626
    },
    {
      "name": "Source code",
      "score": 0.4413769543170929
    },
    {
      "name": "Speech recognition",
      "score": 0.4181459844112396
    },
    {
      "name": "Generative grammar",
      "score": 0.41130414605140686
    },
    {
      "name": "Artificial neural network",
      "score": 0.3914813697338104
    },
    {
      "name": "Programming language",
      "score": 0.28505778312683105
    },
    {
      "name": "Linguistics",
      "score": 0.14943557977676392
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}