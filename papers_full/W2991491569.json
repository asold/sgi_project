{
  "title": "Unsupervised Domain Adaptation of Language Models for Reading Comprehension",
  "url": "https://openalex.org/W2991491569",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3034638261",
      "name": "Nishida Kosuke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2886602696",
      "name": "Nishida Kyosuke",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4290478021",
      "name": "Saito, Itsumi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3196208883",
      "name": "Asano Hisako",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2754968486",
      "name": "Tomita Junji",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2962874939",
    "https://openalex.org/W2963898943",
    "https://openalex.org/W3090047754",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W2734823783",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2158108973",
    "https://openalex.org/W2963506049",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W2963928014",
    "https://openalex.org/W2551396370",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2124033848",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963619912",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2991231696",
    "https://openalex.org/W2976193123",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W2970928735",
    "https://openalex.org/W2557764419"
  ],
  "abstract": "This study tackles unsupervised domain adaptation of reading comprehension (UDARC). Reading comprehension (RC) is a task to learn the capability for question answering with textual sources. State-of-the-art models on RC still do not have general linguistic intelligence; i.e., their accuracy worsens for out-domain datasets that are not used in the training. We hypothesize that this discrepancy is caused by a lack of the language modeling (LM) capability for the out-domain. The UDARC task allows models to use supervised RC training data in the source domain and only unlabeled passages in the target domain. To solve the UDARC problem, we provide two domain adaptation models. The first one learns the out-domain LM and in-domain RC task sequentially. The second one is the proposed model that uses a multi-task learning approach of LM and RC. The models can retain both the RC capability acquired from the supervised data in the source domain and the LM capability from the unlabeled data in the target domain. We evaluated the models on UDARC with five datasets in different domains. The models outperformed the model without domain adaptation. In particular, the proposed model yielded an improvement of 4.3/4.2 points in EM/F1 in an unseen biomedical domain.",
  "full_text": "arXiv:1911.10768v2  [cs.CL]  21 May 2020\nUnsupervised Domain Adaptation of Language Models\nfor Reading Comprehension\nKosuke Nishida, Kyosuke Nishida, Itsumi Saito, Hisako Asan o, Junji T omita\nNTT Media Intelligence Laboratories, NTT Corporation\n1-1 Hikarinooka Y okosuka-Shi, Kanagawa, Japan\nkosuke.nishida.ap@hco.ntt.co.jp\nAbstract\nThis study tackles unsupervised domain adaptation of readi ng comprehension (UDARC). Reading comprehension (RC) is a t ask to learn\nthe capability for question answering with textual sources . State-of-the-art models on RC still do not have general lin guistic intelligence;\ni.e., their accuracy worsens for out-domain datasets that a re not used in the training. W e hypothesize that this discrep ancy is caused by\na lack of the language modeling (LM) capability for the out-d omain. The UDARC task allows models to use supervised RC trai ning\ndata in the source domain and only unlabeled passages in the t arget domain. T o solve the UDARC problem, we provide two doma in\nadaptation models. The ﬁrst one learns the out-domain LM and in-domain RC task sequentially . The second one is the propos ed model\nthat uses a multi-task learning approach of LM and RC. The mod els can retain both the RC capability acquired from the super vised data\nin the source domain and the LM capability from the unlabeled data in the target domain. W e evaluated the models on UDARC wi th\nﬁve datasets in different domains. The models outperformed the model without domain adaptation. In particular, the pro posed model\nyielded an improvement of 4.3/4.2 points in EM/F1 in an unsee n biomedical domain.\nKeywords: Reading Comprehension, Domain Adaptation, Unsupervised L earning\n1. Introduction\nReading comprehension (RC) is a task to acquire a capabil-\nity of understanding natural language for question answer-\ning with textual sources. It has seen signiﬁcant progress\nsince the release of numerous datasets such as SQuAD\n(Rajpurkar et al., 2016) and the rise of the deep neural\nmodels such as BiDAF (Seo et al., 2017). Recently, ﬁne-\ntuning of pre-trained language models (LM) such as BER T\n(Devlin et al., 2019) has achieved state-of-the-art perfor -\nmance in many NLP tasks including RC.\nHowever, such state-of-the-art models still do not have gen -\neral linguistic intelligence; e.g., their accuracy is sens itively\naffected by the difference in the distribution between the\ntraining and evaluation datasets, such as in the domains\nof textual sources. This discrepancy becomes an issue in\na real-world scenario. For instance, a business intending\nto introduce an RC application for a service must create\ntens of thousands of (passage, query, answer) tuples per\ndomain of the service (Y ogatama et al., 2019). However,\nsuch a large annotation to create training data costs much\nmoney and takes up a lot of time. When the domain entails\npersonal information or expert knowledge, even crowd-\nsourcing can not be used to create the training data.\nThis study tackles a task, called Unsupervised Domain\nAdaptation of Reading Comprehension (UDARC). T a-\nble 1 shows the task setting of UDARC. The model can use\nthe (passage, query, answer) tuples of the source domain for\ntraining. In addition, the model can use unlabeled passages\nfor training in the target domain. Annotated corpora with\nQA pairs in the target domain can not be used for training.\nIn addition to the interest in the domain discrepancy, we\nthink that this setting is a natural one in real-world scenar -\nios, where it is assumed that the RC application provider\nhas documents that can be used as knowledge sources for\nRC in the target domain (e.g., technical documents about a\nproduct). UDARC thus enables people who have such doc-\nTraining Evaluation\nInput Output Input Output\nPassage Query Answer Passage Query Answer\nSource ✓ ✓ ✓\nT arget ✓ ✓ ✓ ✓\nT able 1: T ask setting of unsupervised domain adaptation\nfor reading comprehension (UDARC). Only unlabeled pas-\nsages can be used for training in the target domain. In terms\nof QA pairs, the task requires zero-shot domain adaptation.\numents in the target domain but have no RC training data\nfor the domain to introduce RC applications. This scenario\nalso applies to low-resource languages, where the availabi l-\nity of RC training data is limited.\nW e hypothesize that the poor performance of in-domain\nmodels for out-domains is caused by a lack of LM ca-\npability for out-domains. That is, we can improve the\nquestion answering accuracy, without the RC data for the\nout-domains, by training the language model with textual\nsources about the out-domains in an unsupervised fashion.\nIn this study, we introduce a no-adaptation baseline model.\nIt transfers a BER T model ﬁne-tuned with the source do-\nmain RC dataset to the target domain without domain adap-\ntation. As a natural unsupervised domain adaptation ap-\nproach, we investigate a sequential model. It adapts the\nlanguage model of BER T with the unlabeled passages in the\ntarget domain, and then it ﬁne-tunes BER T with the source\ndomain RC dataset.\nMoreover, we propose the multi-task learning approach of\nLM in the target domain and RC in the source domain.\nIt is more promising because the model avoids forgetting\nabout the target domain while ﬁne-tuning. This study in-\nvestigates the feasibility of UDARC and the effectiveness\nof these models on various domain datasets. Our main con-\ntributions are as follows.\n• W e tackled unsupervised domain adaptation of read-\ning comprehension (UDARC). T o solve UDARC, we\nhypothesize that the ability to understand the out-\ndomain passages can be learned without supervised\nannotation for RC.\n• W e propose a multi-task learning approach of the LM\nin the target domain and the RC in the source domain.\nThis approach retains both knowledge of RC in the\nsource domain and knowledge of LM in the target do-\nmain without forgetting. W e use BER T as the back-\nbone model.\n• W e evaluated three models on UDARC with ﬁve\ndatasets in different domains. The domain adapta-\ntion models outperformed the no-adaptation model.\nIn particular, the proposed model (adapted from the\nWikipedia domain to a biomedical domain) yielded\nthe best performance, with a 4.3/4.2 point gain in\nEM/F1 over the model without domain adaptation.\n• W e thoroughly investigated cases in which UDARC\nis effective. W e found that the task is promising\nwhen the target domain is unseen in the training of\nthe source domain and the pre-training of the language\nmodel and when the training data in the target domain\nare insufﬁcient to acquire the RC and LM capabilities.\n2. Problem Formulation\nHere, we will focus on unsupervised domain adaptation of\nextractive RC, which is the most popular problem formula-\ntion of RC. The UDARC task is deﬁned as follows.\nPRO BL E M 1. Let IS be instances of 3-tuples (passage,\nquery, answer span) in the source domain, and IT be unla-\nbeled passages in the target domain. The task of UDARC is\nto accurately answer the query (extract the correct answer-\nspan from an input passage) in the target domain by training\nan extractive RC model with IS and IT .\n3. Related W ork\n3.1. Reading Comprehension\nGolub et al. (2017) and W ang et al. (2019) also tackled\nUDARC. Their approaches are to create pseudo QA pairs\nfor training in the target domain. They use the answer ex-\ntraction model to ﬁnd a potential answer from the passage\nand the query generation model to create the query given\nthe potential answer and the passage. Our approach is dif-\nferent from theirs because, according to our hypothesis, we\ncan acquire knowledge in the target domain without RC\ntraining. An advantage of our approach is the low com-\nputational cost due to the lack of a need for the training of\nthe Sequence-to-Sequence model to generate the query.\nThe MRQA 2019 shared task has a similar motivation as\nours (Fisch et al., 2019). Its goal is to generalize to new\ntest distributions and be robust to test-time perturbation s.\nThe task is on six in-domain training data and twelve out-\ndomain evaluation data (a many-to-many setting). Our mo-\ntivation is to acquire the ability to understand out-domain\npassages from unlabeled passages and the ability to answer\na query from annotated RC training data for domain adap-\ntation (a one-to-one setting). This motivation correspond s\nto a real-world scenario. The shared task resulted in the\ncertiﬁcation of the dependence of RC performance on the\nbackbone LM capability. This result supports the effective -\nness of our approach to improve LM for domain adaptation.\nRC is essentially classiﬁed into four types: extractive\n(Rajpurkar et al., 2016), multiple-choice (Lai et al., 2017 ),\ngenerative (Nguyen et al., 2016), and cloze-style\n(Hermann et al., 2015). Although this study focused\non extractive RC, we believe that the concept of our model\ncan be applied to other types of RC.\nMany RC datasets have been published recently, but\nthe ones for closed domains are limited in number.\nThere are datasets, for example, in biomedical do-\nmain ( ˇSuster and Daelemans, 2018), scientiﬁc domain\n(Clark et al., 2018; Johannes W elbl and Gardner, 2017),\nand software domain (Dhingra et al., 2017). The limited\nnumber is one reason for our developing UDARC. Despite\nthe demand for RC in closed domains, which requires\nexpert knowledge, the annotation cost of the dataset makes\nit difﬁcult to create the dataset.\n3.2. Domain Adaptation\nUnsupervised domain adaptation is a task to adapt the\nmodel to the target domain with labeled source data and\nunlabeled target data. While supervised domain adapta-\ntion with labeled target data ﬁrst trains the model in the\nsource domain and then adapts it to the target domain, un-\nsupervised domain adaptation takes another approach. In\nNLP , Ziser and Reichart (2019) uses a two-step algorithm;\nthe model ﬁrst learns the representations and then learns\nthe classiﬁcation. Miller (2019) uses the multi-task learn -\ning approach of the classiﬁcation and the feature selection\nof structural correspondence learning (Blitzer et al., 200 6).\nHere, Ganin et al. (2016) trains the representation learnin g\nand the classiﬁcation jointly with a domain-adversarial ne u-\nral network.\nHowever, UDARC can not use standard unsupervised do-\nmain adaptation methods. As shown in T able 1, an instance\nis divided into an input (e.g., a (passage, query) pair in RC)\nand output (e.g., an answer in RC). Unsupervised domain\nadaptation allows inputs to be used in the target domain\nwithout outputs. In RC, neither the output nor the queries\nin the inputs can be used for training.\nZero-shot learning is a task to adapt the model so that it can\npredict unseen classes in the training (Socher et al., 2013) .\nIn particular, the recently proposed zero-shot domain adap -\ntation (Y ang and Hospedales, 2015; Peng et al., 2018) can\nnot use the inputs for the training of domain adaptation.\nUDARC can be interpreted as a kind of zero-shot domain\nadaptation, because it can not use queries in the inputs for\nthe training.\nZero-shot domain adaptation is a more challenging task\nthan unsupervised domain adaptation, and there are few\nstudies on it. Peng et al. (2018) allows task-irrelevant\ndata in the target domain. This is similar to our set-\nting, but their task is classiﬁcation of images, which is\nrather different from extractive RC, which is a special\nBERT\nRC output\nRC instances\nfrom source\n(a) No-adaptation\nBERT\nLM output\nLM instances\nfrom target\nBERT\nRC output\nRC instances\nfrom source\n(b) Sequential\nLM output\nLM instances\nfrom target\nRC output\nRC instances\nfrom source\nShared TRMs\nLM-speciﬁ c TRMs RC-speciﬁ c TRMs\n(c) Multi-T ask\nFigure 1: Overview of the three models. The LM instances are f rom the target domain, and the RC instances are from the\nsource domain. “TRMs” means transformer layers.\ncase of sequence labeling for text. The results of other\nstudies have limitations when they are used for UDARC.\nY ang and Hospedales (2015) and Mancini et al. (2019) hy-\npothesize that the training data are from multiple domains.\nIshii et al. (2019) suppose there is prior knowledge about\nwhat factors cause the differences between the source and\ntarget data distributions.\n4. Methods\nThis section introduces three models: the baseline model\nwithout domain adaptation, the sequential domain adapta-\ntion model, and the proposed model using multi-task learn-\ning. W e used a pre-trained BER T base model as the back-\nbone architecture for the three models. It was trained\nwith large-scale corpora: BookCorpus (800M words)\n(Zhu et al., 2015) and English Wikipedia (2,500M words).\nIt has achieved state-of-the-art performance on various\nRC datasets. See (Devlin et al., 2019) for the details of\nBER Tbase . Note that the models can use other pre-trained\nLM consisting of stacked layers as their backbone architec-\nture.\n4.1. No-adaptation Baseline\nThis baseline model is a BER T ﬁne-tuned with an RC\ndataset in the source domain. It does not consider domain\nadaptation, so its performance corresponds to the lower\nbound of UDARC.\nFigure 1 (a) shows an overview of the model. W e\nfollow the ﬁne-tuning setup for extractive RC, as in\n(Devlin et al., 2019). W e add a linear layer for extractive\nRC on top of the BER T layers, where its output dimension\nis two. The ﬁrst dimension represents a score that the to-\nken is the start of the answer span, and the other dimension\nrepresents the end. The input sequence is [‘[CLS]’; query;\n‘[SEP]’; passage; ‘[SEP]’], where ‘[CLS]’ and ‘[SEP]’ are\nspecial tokens and ‘;’ means concatenation.\n4.2. Sequential Model\nThis model ﬁrst adapts the pre-trained BER T with unsuper-\nvised passages in the target domain. Then, it ﬁne-tunes the\nadapted BER T with the RC dataset of the source domain.\nFigure 1 (b) shows an overview of the model. For the ﬁrst\nunsupervised adaptation, a linear layer for LM is added\nat top of the BER T layers. The BER T including the LM\noutput layer is trained (with the sentences in the unsuper-\nvised passages) in the same manner as in the pre-training\nof BER T , i.e., by using masked language modeling (MLM)\nand next sentence prediction (NSP). The input sequence is\n[‘[CLS]’; sentences 1; ‘[SEP]’; sentences 2 ; ‘[SEP]’]. Af-\nter the adaptation, another linear layer for RC is added, and\nthe BER T including the RC output layer is ﬁne-tuned in the\nsame manner as in the no-adaptation baseline.\nNote that we may conduct domain adaptation in reverse\norder, i.e., ﬁrst build an RC model in the source domain\nand then adapt it to the target domain. However, this order\ncauses catastrophic forgetting of the capability of ﬁnding\nan answer span. The sequential model follows the pipeline\nstrategy of the previous unsupervised domain adaptation\nwork (Ziser and Reichart, 2019). Their model learns rep-\nresentations ﬁrst and classiﬁcation after that, but their t ask\nis not RC and they do not use a pre-trained language model.\n4.3. Proposed Multi-T ask Model\nW e consider that the sequential model forgets the knowl-\nedge in the target domain while it is being ﬁne-tuned with\nRC training data. Moreover, it is known that multi-task\nﬁne-tuning of the pre-trained language models outperforms\nsingle-task ﬁne-tuning (Liu et al., 2019a). For this reason ,\nwe propose a multi-task learning approach. Multi-task\nlearning adds an RC linear layer and LM linear layer to the\ntop of the BER T layers. The model uses the RC linear layer\nfor RC instances and the LM linear layer for LM instances\nas the output layer. Figure 1 (c) shows an overview of the\nmodel.\nOur multi-task learning approach uses the following two\ntechniques.\n4.3.1. Using Shared and Speciﬁc T ransformer Layers\nFirst, we use the top- n Transformer layers (TRMs) for the\ntasks separately as task-speciﬁc RC or LM layers. The\ntask-speciﬁc layers are cloned and initialized from the ori g-\ninal pre-trained BER T base layers. The other layers are com-\nmonly shared by the tasks. The RC instances pass through\nthe shared layers, the RC-speciﬁc TRMs, and the RC out-\nput layer. The LM instances pass through the shared layers,\nthe LM-speciﬁc TRMs, and the LM output layer. This idea\nfollows T enney et al. (2019)’s observation that basic syn-\ntactic information (e.g., part-of-speech tagging) is capt ured\nAlgorithm 1 Multi-task learning approach\nInput: source RC instances IS, target LM instances IT ,\nnum. of steps N, the RC training ratio k\n1: for all i in 1,· · ·, Ndo\n2: Select mini-batch bS ∼ IS\n3: Train the shared layers, RC-speciﬁc layers, and RC\noutput layer with bS.\n4: if i%k == 0then\n5: Select mini-batch bT ∼ IT\n6: Train the shared layers, LM-speciﬁc layers, and\nLM output layer with bT .\n7: end if\n8: end for\nin lower layers and high-level semantic information (e.g.,\ncoreference labeling) are captured in higher layers. W e con -\nsider that the basic syntactic information is the common\nfeatures between tasks, and the high-level semantic infor-\nmation is closely related to the output of the task. There-\nfore, the LM instances in the target domain should use the\nshared TRMs in order to capture the basic syntactic infor-\nmation in the target domain. The LM instances should not\nshare the higher layers of BER T with the RC instances, be-\ncause the way it uses the high-level semantic information is\ndifferent from in RC.\n4.3.2. One-Segment LM T raining\nSecond, we preprocess each LM instance as a sequence\nwith one segment. That is, each LM instance is [‘[CLS]’;\n‘[LM]’; passage; ‘[SEP]’], and the segment ids are a zero\nvector. Therefore, NSP is not used for training of our multi-\ntask model. This intends to prevent the learning of the\nsegment interaction in NSP from disturbing the learning of\nquery-passage interaction, because the two segments only\ninteract in the RC training. ‘[LM]’ is a special token mean-\ning that the instance is an LM instance.\n4.3.3. T raining Procedure\nW e perform the RC training and LM training alternately.\nThe RC training is performed k times as many times as the\nLM training. The algorithm is shown in Algorithm 1.\nNote that the model size and computational time in the eval-\nuation are the same as in the original BER T , because the\nLM-speciﬁc layers are not used for evaluation. In the train-\ning, the computational time remains about the same, be-\ncause the training for each mini-batch is the same as in the\noriginal BER T .\n5. Experiments\n5.1. Dataset\nW e evaluated the UDARC task on various datasets. The\ntraining data in the source domain should be large and cover\na wide range of topics. Moreover, the test data in the target\ndomain should be from a closed domain. Here, we selected\nﬁve datasets from different domains. T able 2 shows the\nstatistics. W e used the development data for the evaluation\nbecause some of the datasets do not include test data. Note\nthat not all the unlabeled passages were used. The number\nof used unlabeled passages depended on the experimental\ndataset domain # training # dev. # unlabeled\ndata data passages\nSQuAD Wikipedia 87599 10570 19047\nNewsQA news 107064 5988 95933\nBioASQ biomedical 0 1504 55148\nDuoRC movie 69524 15591 5137\nNatural HTML 104071 12836 12222Questions Wikipedia\nT able 2: Statistics of the datasets used in the experiments.\nsetup (the number of ﬁne-tuning epochs and the value of k)\nand the size of the training dataset.\nSQuAD1.1 is an RC dataset from Wikipedia\n(Rajpurkar et al., 2016). W e used this dataset as the\nsource domain or target domain. W e used the passages\nof the training data as the unlabeled passages when the\ndataset was in the target domain.\nNewsQA is an RC dataset from CNN news\n(Trischler et al., 2017). W e used this dataset as the\nsource domain or target domain. W e used the CNN\nnews scripts (Hermann et al., 2015) as unlabeled passages\nsimilar to the data collection of NewsQA.\nBioASQ is a biomedical semantic indexing and ques-\ntion answering challenge 1 (Tsatsaronis et al., 2015). The\nMRQA 2019 shared task preprocesses this dataset for ex-\ntractive RC; we used the MRQA version of this dataset.\nThe training data of this dataset is not provided in the\nMRQA 2019 shared task. W e used this dataset only for the\ntarget domain. W e collected the unlabeled passages from\nthe abstracts of PubMed articles.\nOur main interest in the experiments is the performance of\nthe unsupervised domain adaptation models on BioASQ.\nThis is because the BioASQ domain is not fully covered\nby the source domain (Wikipedia or news) or by the BER T\npre-training (BookCorpus and Wikipedia).\nDuoRC is an RC dataset in the movie domain\n(Saha et al., 2018). DuoRC provides parallel movie plots\nfrom Wikipedia and IMDb. In the experiment, we used the\nParaphraseRC task in DuoRC, where each query is created\non a Wikipedia movie article, and each passage is collected\nfrom an IMDb article corresponding to the same movie.\nThe ParaphraseRC task is difﬁcult because it is designed\nto contain a large number of queries with low lexical over-\nlap between queries and their corresponding passages. W e\nused this dataset only for the target domain and the passages\nof the training data as the unlabeled passages.\nThe movie domain is different from the source domains,\nwhereas the BER T pre-training covers many stories in\nBookCorpus. Therefore, we think that the pre-trained\nBER T has acquired knowledge of language modeling for\nthis dataset.\nNatural Questions (NQ) is an RC dataset containing pas-\nsages from Wikipedia written in HTML format, where\n1 T ask 7b, Biomedical Semantic QA, is held with ECML\nPKDD 2019. See http://BioASQ.org/ .\nTrain w ./ Domain T arget\nSQuAD Adaptation NewsQA BioASQ DuoRC NQ\nStandard RC in T arget 41.5/56.0 — 20.2/27.2 58.9/72.2\nNo-adaptation ✓ 35.2/50.7 41.1/53.6 24.5/33.0 44.4/57.5\nSequential ✓ ✓ 35.2/51.0 44.5/57.1 25.4/33.8 —\nMulti-T ask ✓ ✓ 35.9/51.4 45.4 /57.8 25.5 /34.1 43.8/56.7\nT able 3: Results when the SQuAD was the source dataset. EM is o n the left and F1 is on the right in each cell. The top row\nis the supervised training in the target domain, so it is the e xpected upper bound of UDARC. The Standard RC in BioASQ\nis empty, due to the lack of training data. The sequential mod el for NQ is empty. NSP of the BER T pre-training can not be\napplied to HTML with a lot of HTML tags (e.g., List and T able).\nTrain w ./ Domain T arget\nNewsQA Adaptation SQuAD BioASQ DuoRC NQ\nStandard RC in T arget 80.9/88.4 — 20.2/27.2 58.9/72.2\nNo-adaptation ✓ 59.8/73.9 34.5/48.3 22.5/31.2 39.0/52.7\nSequential ✓ ✓ 59.7/75.3 36.6/ 50.4 23.7/32.7 —\nMulti-T ask ✓ ✓ 60.6/75.8 36.8 /50.3 23.8/32.3 42.0/56.2\nT able 4: Results when the NewsQA was the source dataset.\neach passage is given as a sequence of words and HTML\ntags (Kwiatkowski et al., 2019). W e used the preprocessed\ndataset for extractive RC in the MRQA 2019 shared task\nfor the training and evaluation. W e used the passages of the\ntraining data in the original NQ as the unlabeled passages.\nAlthough the domain is the same as that of SQuAD and is\nalso covered by the BER T pre-training, we used this dataset\nto conﬁrm whether our model can adapt to HTML format\nwithout supervised RC data on the HTML format.\n5.2. Experimental Setup\nW e compared three models, no-adaptation, sequential, and\nmulti-task on the above datasets.\nW e used the PyT orch implementation of BER T 2 . W e\ntrained the models on four NVIDIA T esla P100 GPUs. The\noptimizer was Adam (Kingma and Ba, 2014). The warm-\nup proportion was 0.1 and the learning rate was 0.00005.\nThe batch size was 32. There were three epochs. The in-\nput length was 384. Sequences longer than the input length\nwere truncated with a stride length of 128. The other hy-\nperparameters followed those of BER T base . The ratio of\nRC training to LM training k is 10. The number of task-\nspeciﬁc layers n is 3. The hyperparameters are ﬁxed in all\nof the ﬁne-tunings for RC. The hyperparameters of the pre-\ntraining in the sequential model follow the default setting s\nof the implementation, except that the input length is 512,\nwhich is the longest case.\nW e evaluated the answer prediction in terms of exact match\n(EM) and partial match (F1). These are ofﬁcial metrics of\nSQuAD.\n5.3. Results\nUnder what condition is UDARC effective? T able 3\n(the source domain is Wikipedia from SQuAD) and T able\n4 (news from NewsQA) show the performance of the mod-\nels for the target domain. The “Standard RC in T arget” row\n2 https://github.com/huggingface/pytorch-transformers\nlists the results in the standard RC (non-UDARC) setting,\nwhere each model was trained and evaluated in the target\ndomain, so these are expected to be the upper bounds. First,\nwe discuss each target-only dataset separately.\nBioASQ. BioASQ is of main interest in our experiments.\nThe two domain adaptation models outperformed the no-\nadaptation baseline in both the source domain settings.\nThe proposed model improved on the no-adaptation model\ntrained in SQuAD by 4.3/4.2 points.\nBioASQ is in a domain that is not included in the BER T\npre-training corpora. The results showed that the UDARC\nframework is effective in adapting to unseen domains.\nMoreover, this result conﬁrms that our hypothesis behind\nUDARC is correct; i.e., the ability to understand the out-\ndomain passages can be learned from a non-annotated cor-\npus and the ability to answer the query can be learned\nfrom annotated RC training data, even though BER T is pre-\ntrained with very large corpora.\nDuoRC. All unsupervised domain adaptation models out-\nperformed the model trained with the supervised dataset\nin the target domain 3. This surprising result is caused by\nthe difﬁculty of the ParaphraseRC task of DuoRC. It has\nlow lexical overlap between queries and their correspond-\ning passages, and the passages are rather long and com-\nplicated. As a result, the training data are too difﬁcult for\nlearning the RC capability. W e think UDARC is promising\nwhen it is difﬁcult to acquire the LM and RC capabilities\nwith the supervised datasets.\nNatural Questions. W e compared four models (No-\nadaptation / Multi-T ask with the source domain of SQuAD /\nNewsQA) in the target domain of NQ. The results indicated\nthat unsupervised adaptation from SQuAD to NQ did not\n3 The performance of the supervised model was 20.2/27.2,\nwhich is similar to the performance (19.7/27.6) reported in the\noriginal paper.\nNewsQA BioASQ DuoRC NQ\nStandard RC 80.9/88.4\nSequential 81.2/88.6 80.6/88.4 81.0/88.4 —\nMulti-T ask 81.1/88.5 81.1/88.5 80.7/88.3 80.9/88.4\nT able 5: Results for when the source and evaluation\ndatasets were SQuAD. The model was adapted to each tar-\nget dataset. The top row is the no-adaptation model trained\nonly with the SQuAD dataset.\nSQuAD BioASQ DuoRC NQ\nStandard RC 41.5/56.0\nSequential 41.8/56.9 42.0/57.1 42.7/58.0 —\nMulti-T ask 42.6/57.6 42.0/57.0 42.8/57.8 42.3/57.4\nT able 6: Results for when the source and the evaluation\ndatasets were NewsQA. The model was adapted to each tar-\nget dataset. The top row is the no-adaptation model trained\nonly with the NewsQA dataset.\nimprove accuracy; on the other hand, unsupervised adap-\ntation from NewsQA to NQ was effective. These results\ncan be interpreted as meaning that the multi-task approach\ntrained in the news domain as the source successfully\nadapted to the Wikipedia domain. However, the proposed\nmodel trained with plain text failed to adapt to HTML for-\nmat. W e consider that the adaptation to the HTML format\nis a more challenging task than domain adaptation. Here,\nthe task design of the language modeling remains as future\nwork to understand text in HTML format, such as under-\nstanding of the dependencies among segments separated by\nHTML tags.\nWhat is the performance of the three models? Here, let\nus discuss the model performances shown in T able 3 and\nT able 4. Except for NQ, the domain adaptation models out-\nperformed the no-adaptation baseline. In terms of the EM\nmetric, the multi-task model outperformed the sequential\nmodel for all source/target settings. This tendency showed\nthe possibility that the sequential model forgets the out-\ndomain knowledge while it is being ﬁne-tuned. However,\nthere was no statistical signiﬁcance between the two mod-\nels. W e observed that the sequential model was as effective\nas the multi-task model. It is worth pre-training BER T with\nthe unlabeled target domain data after pre-training in the\ngeneral domain. This ﬁnding can be applied to other NLP\ntasks.\nIn comparison to the related work, W ang et al. (2019) only\nrefers to the experiments with BER T in the setting from\nSQuAD to NewsQA. The improvement of their domain\nadaptation method over BER T ﬁne-tuned in the source do-\nmain is 0.6/0.5 points, which is comparable to our 0.7/0.7\npoint improvement, though their pseudo QA generation ap-\nproach requires more computational cost.\nDoes domain adaption hurt the performance in the\nsource domain? W e evaluated the drop in performance\nin the source domain due to the domain adaptation. T able\n500 1000 2000 5000 10000 20000\nNumber of Unlabeled Passages\n52\n53\n54\n55\n56\n57\n58\n59\n60F1\nFigure 2: Performance (F1) of the proposed model in adap-\ntation from SQuAD to BioASQ in terms of the number of\nunlabeled passages. The proposed model was trained with\nk times fewer LM instances than RC instances. The right-\nmost coordinate of each line is the maximum number of\nused LM instances determined by k, the number of epochs,\nand the number of training data.\n5 and T able 6 show the results. The “Standard RC” row is\nthe model trained only in the source domain.\nSurprisingly, the sequential model and the multi-task mode l\ntend to outperform the standard RC training in the source\ndomain. In addition to the discrepancy between domains,\na discrepancy also exists between the training samples and\nthe evaluation samples. W e consider that the domain adap-\ntation has the effect of generalization, so the model over-\ncomes the sampling discrepancy. UDARC requires no ad-\nditional supervised data for training, so the framework of\nUDARC can be easily extended to the standard RC task in\nwhich we can expect an improvement in performance.\nHow many unlabeled passages are required? W e eval-\nuated the performance of the proposed model in terms of\nthe number of unlabeled passages. The experiments were\non adaptation from SQuAD to BioASQ, because this set-\nting showed the largest improvements and BioASQ is the\nmain focus of UDARC. Figure 2 shows the results.\nThe results show that the proposed model outperformed the\nno-adaptation baseline even when there were only 500 un-\nlabeled passages. The gain was 3.7/3.3 points in EM/F1. In\nterms of the RC-LM ratio k, k = 10was preferred, but we\nshould note that in some cases, k = 100 was preferred in\nthe pilot experiments. The biomedical domain is far from\nthe pre-training corpus of BER T , so we consider that the\nmoderate frequency of LM training steps is larger than in\nother domains included in the corpora. W e found that the\nperformance does not always increase as more unlabeled\npassages come to be used, though the best performance is\nwith the full 26280 passages.\nWhat domain is preferred as the source domain? T o\nevaluate the preference about the source domain under the\nsame conditions, we equalized the number of training data\nin the source domain. Figure 3 and Figure 4 shows the per-\nformance of the proposed model with BioASQ and DuoRC\nas the target dataset.\nOn BioASQ, the proposed model adapted from SQuAD\noutperformed the model adapted from NewsQA. In con-\ntrast, on DuoRC, the performance was on par. Therefore,\nthe performance of the proposed model depends on the se-\nlection of the source domain, but the preferred source do-\nmain cannot as yet be identiﬁed.\n5000 10000 20000 40000 80000\nNumber of RC Instances\n30\n35\n40\n45\n50\n55\n60F1\nFigure 3: Performance (F1) of the proposed model in adap-\ntation from SQuAD and NewsQA to BioASQ versus num-\nber of source training data.\n5000 10000 20000 40000 80000\nNumber of RC Instances\n20.0\n22.5\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0F1\nFigure 4: Performance (F1) of the proposed model in adap-\ntation from SQuAD and NewsQA to DuoRC versus number\nof source training data.\nHow much source data are required? Figure 3 and Fig-\nure 4 show the performance in terms of the number of\nsource training data. The results indicate that more traini ng\ndata results in higher performance. In particular, the im-\nprovement grows rapidly until 10000 instances, after which\nit becomes slower. This result coincides with the observa-\ntion of Y ogatama et al. (2019). They showed that tens of\nthousands of training data are required to ﬁne-tune BER T .\nW e consider that the required number of source data for\nUDARC shows the same tendency as in the standard RC\ntask.\n6. Conclusion and Future W ork\nThis paper studied UDARC to adapt an RC model to the\ntarget domain without any annotated data in the target do-\nmain.\nThis is the ﬁrst study to focus on the unsupervised domain\nadaptation for acquiring the ability to answer the question\nfrom the RC task in the source domain and the ability to\nunderstand the out-domain passages from the LM task in\nthe target domain. This approach is different from the re-\nlated work, which generates the pseudo QA pairs for the\ntraining. W e described two unsupervised domain adapta-\ntion models using BER T . In addition, the proposed model\nreduces the forgetting of out-domain knowledge while it is\nbeing ﬁne-tuned.\nW e evaluated the two models and the no-adaptation model\non the ﬁve datasets in different domains. As a result, the\ndomain adaptation models outperformed the no-adaptation\nmodel especially well when the target domain was not con-\ntained in the source domain or the BER T pre-training cor-\npora. The proposed model (adapted from the Wikipedia\ndomain to the biomedical domain) yielded the best perfor-\nmance, with a 4.3/4.2 points gain in EM/F1 over the no-\nadaptation model. W e believe that this study sheds light on\nthe importance and feasibility of UDARC. Our experiments\nalso showed that the UDARC framework has the potential\nto outperform a model trained with supervised datasets in\nthe target domain when it is difﬁcult to acquire the LM and\nRC capabilities from the supervised datasets.\nPre-trained language models (Devlin et al., 2019;\nRadford et al., 2019; Y ang et al., 2019; Liu et al., 2019b)\nand the ﬁne-tuned models achieved state-of-the-art perfor -\nmance in many NLP tasks, including RC. W e believe that\nthis study considering unsupervised domain adaptation\nwith BER T will foster great contributions to various ﬁelds\nof NLP that have not been the subject of previous work in\nunsupervised domain adaptation.\n7. Bibliographical References\nBlitzer, J., McDonald, R., and Pereira, F . (2006). Domain\nadaptation with structural correspondence learning. In\nEMNLP, pages 120–128.\nClark, P ., Cowhey, I., Etzioni, O., Khot, T ., Sabharwal, A.,\nSchoenick, C., and T afjord, O. (2018). Think you have\nsolved question answering? Try ARC, the AI2 reasoning\nchallenge. arXiv preprint arXiv:1803.05457.\nDevlin, J., Chang, M.-W ., Lee, K., and T outanova, K.\n(2019). BER T: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT,\npages 4171–4186.\nDhingra, B., Mazaitis, K., and Cohen, W . W . (2017).\nQuasar: Datasets for question answering by search and\nreading. arXiv preprint arXiv:1707.03904.\nFisch, A., T almor, A., Seo, M., Choi, E., and Chen, D.\n(2019). MRQA 2019 shared task: Evaluating general-\nization in reading comprehension. In MRQA@EMNLP,\npages 1–13.\nGanin, Y ., Ustinova, E., Ajakan, H., Germain, P .,\nLarochelle, H., Laviolette, F ., Marchand, M., and Lem-\npitsky, V . (2016). Domain-adversarial training of neural\nnetworks. JMLR, 17(1):2096–2030.\nGolub, D., Huang, P .-S., He, X., and Deng, L. (2017).\nT wo-stage synthesis networks for transfer learning in\nmachine comprehension. In EMNLP, pages 835–844.\nHermann, K. M., Kocisky, T ., Grefenstette, E., Espeholt, L. ,\nKay, W ., Suleyman, M., and Blunsom, P . (2015). T each-\ning machines to read and comprehend. In NIPS, pages\n1693–1701.\nIshii, M., T akenouchi, T ., and Sugiyama, M. (2019). Zero-\nshot domain adaptation based on attribute information.\nIn ACML, pages 473–488.\nJohannes W elbl, N. F . L. and Gardner, M. (2017). Crowd-\nsourcing multiple choice science questions. In W-\nNUT@EMNLP, pages 94–106.\nKingma, D. P . and Ba, J. (2014). Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nKwiatkowski, T ., Palomaki, J., Redﬁeld, O., Collins, M.,\nParikh, A., Alberti, C., Epstein, D., Polosukhin, I.,\nDevlin, J., Lee, K., et al. (2019). Natural questions:\na benchmark for question answering research. TACL,\n7:453–466.\nLai, G., Xie, Q., Liu, H., Y ang, Y ., and Hovy, E. (2017).\nRACE: Large-scale reading comprehension dataset from\nexaminations. In EMNLP, pages 785–794.\nLiu, X., He, P ., Chen, W ., and Gao, J. (2019a). Multi-task\ndeep neural networks for natural language understand-\ning. In ACL, pages 4487–4496.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\n(2019b). RoBER T a: A robustly optimized bert pretrain-\ning approach. arXiv preprint arXiv:1907.11692.\nMancini, M., Bul ` o, S. R., Caputo, B., and Ricci, E. (2019).\nAdagraph: Unifying predictive and continuous domain\nadaptation through graphs. In CVPR, pages 6568–6577.\nMiller, T . (2019). Simpliﬁed neural unsupervised domain\nadaptation. In NAACL-HLT, pages 414–419.\nNguyen, T ., Rosenberg, M., Song, X., Gao, J., Tiwary,\nS., Majumder, R., and Deng, L. (2016). MS MARCO:\nA human generated machine reading comprehension\ndataset. In CoCo@NIPS.\nPeng, K.-C., Wu, Z., and Ernst, J. (2018). Zero-shot deep\ndomain adaptation. In ECCV, pages 764–781.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. (2019). Language models are unsupervised\nmultitask learners. T echnical Report OpenAi.\nRajpurkar, P ., Zhang, J., Lopyrev, K., and Liang, P . (2016).\nSQuAD: 100,000+ questions for machine comprehen-\nsion of text. In EMNLP, pages 2383–2392.\nSaha, A., Aralikatte, R., Khapra, M. M., and Sankara-\nnarayanan, K. (2018). DuoRC: T owards complex lan-\nguage understanding with paraphrased reading compre-\nhension. In ACL, pages 1683–1693.\nSeo, M., Kembhavi, A., Farhadi, A., and Hajishirzi, H.\n(2017). Bidirectional attention ﬂow for machine com-\nprehension. In ICLR.\nSocher, R., Ganjoo, M., Manning, C. D., and Ng, A.\n(2013). Zero-shot learning through cross-modal transfer.\nIn NIPS, pages 935–943.\nˇSuster, S. and Daelemans, W . (2018). CliCR: A dataset of\nclinical case reports for machine reading comprehension.\nIn NAACL-HLT, pages 1551–1563.\nT enney, I., Das, D., and Pavlick, E. (2019). BER T redis-\ncovers the classical nlp pipeline. In ACL, pages 4593–\n4601.\nTrischler, A., W ang, T ., Y uan, X., Harris, J., Sordoni, A.,\nBachman, P ., and Suleman, K. (2017). NewsQA: A ma-\nchine comprehension dataset. In Rep4NLP@ACL, pages\n191–200.\nTsatsaronis, G., Balikas, G., Malakasiotis, P ., Partalas, I.,\nZschunke, M., Alvers, M. R., W eissenborn, D., Krithara,\nA., Petridis, S., Polychronopoulos, D., Almirantis, Y .,\nPavlopoulos, J., Baskiotis, N., Gallinari, P ., Artieres,\nT ., Ngonga, A., Heino, N., Gaussier, E., Barrio-Alvers,\nL., Schroeder, M., Androutsopoulos, I., and Paliouras,\nG. (2015). An overview of the BIOASQ large-scale\nbiomedical semantic indexing and question answering\ncompetition. BMC Bioinformatics, 16:138.\nW ang, H., Gan, Z., Liu, X., Liu, J., Gao, J., and\nW ang, H. (2019). Adversarial domain adaptation for\nmachine reading comprehension. In EMNLP-IJCNLP,\npages 2510–2520.\nY ang, Y . and Hospedales, T . (2015). Zero-shot domain\nadaptation via kernel regression on the grassmannian.\nDIFF-CV@BMVC.\nY ang, Z., Dai, Z., Y ang, Y ., Carbonell, J., Salakhutdinov,\nR. R., and Le, Q. V . (2019). XLNet: Generalized au-\ntoregressive pretraining for language understanding. In\nNeurIPS, pages 5754–5764.\nY ogatama, D., d’Autume, C. d. M., Connor, J., Kocisky,\nT ., Chrzanowski, M., Kong, L., Lazaridou, A., Ling, W .,\nY u, L., Dyer, C., and Blunsom, P . (2019). Learning and\nevaluating general linguistic intelligence. arXiv preprint\narXiv:1901.11373 .\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun,\nR., T orralba, A., and Fidler, S. (2015). Aligning books\nand movies: T owards story-like visual explanations by\nwatching movies and reading books. In ICCV, pages 19–\n27.\nZiser, Y . and Reichart, R. (2019). T ask reﬁnement learn-\ning for improved accuracy and stability of unsupervised\ndomain adaptation. In ACL, pages 5895–5906.",
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.6699318885803223
    },
    {
      "name": "Computer science",
      "score": 0.6689919829368591
    },
    {
      "name": "Domain adaptation",
      "score": 0.5340104699134827
    },
    {
      "name": "Reading (process)",
      "score": 0.5001509189605713
    },
    {
      "name": "Natural language processing",
      "score": 0.4993455410003662
    },
    {
      "name": "Comprehension",
      "score": 0.4992556571960449
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.46434149146080017
    },
    {
      "name": "Reading comprehension",
      "score": 0.4596559405326843
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4434441328048706
    },
    {
      "name": "Linguistics",
      "score": 0.4293658137321472
    },
    {
      "name": "Psychology",
      "score": 0.3229821026325226
    },
    {
      "name": "Programming language",
      "score": 0.09081372618675232
    },
    {
      "name": "Mathematics",
      "score": 0.06966951489448547
    },
    {
      "name": "Neuroscience",
      "score": 0.051214247941970825
    },
    {
      "name": "Philosophy",
      "score": 0.044206082820892334
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 15
}