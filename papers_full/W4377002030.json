{
  "title": "Focused Decoding Enables 3D Anatomical Detection by Transformers",
  "url": "https://openalex.org/W4377002030",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4314230983",
      "name": "Bastian Wittmann",
      "affiliations": [
        "University of Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2004672545",
      "name": "Fernando Guillermo Navarro Navarro",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2230803424",
      "name": "Suprosanna Shit",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2746842118",
      "name": "Bjoern Menze",
      "affiliations": [
        "University of Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4294974914",
    "https://openalex.org/W4214627427",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4283072464",
    "https://openalex.org/W2083760363",
    "https://openalex.org/W188807991",
    "https://openalex.org/W3202823735",
    "https://openalex.org/W3207601934",
    "https://openalex.org/W2592605422",
    "https://openalex.org/W2983943451",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W4312587667",
    "https://openalex.org/W2912564039",
    "https://openalex.org/W3112701542",
    "https://openalex.org/W4221163766",
    "https://openalex.org/W4312312588",
    "https://openalex.org/W2665641376",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W2900082295",
    "https://openalex.org/W2150066425",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3037156242",
    "https://openalex.org/W4221146106",
    "https://openalex.org/W2979508243",
    "https://openalex.org/W3098085362",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2412453619",
    "https://openalex.org/W2897297550",
    "https://openalex.org/W2413073178",
    "https://openalex.org/W3161665451",
    "https://openalex.org/W2963849369",
    "https://openalex.org/W2168747910",
    "https://openalex.org/W2601564443",
    "https://openalex.org/W4226013992",
    "https://openalex.org/W3146455718",
    "https://openalex.org/W1573412753",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W4295214535",
    "https://openalex.org/W3095440947",
    "https://openalex.org/W2169561439",
    "https://openalex.org/W639708223",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Detection Transformers represent end-to-end object detection approaches based on a Transformer encoder-decoder architecture, exploiting the attention mechanism for global relation modeling. Although Detection Transformers deliver results on par with or even superior to their highly optimized CNN-based counterparts operating on 2D natural images, their success is closely coupled to access to a vast amount of training data. This, however, restricts the feasibility of employing Detection Transformers in the medical domain, as access to annotated data is typically limited. To tackle this issue and facilitate the advent of medical Detection Transformers, we propose a novel Detection Transformer for 3D anatomical structure detection, dubbed Focused Decoder. Focused Decoder leverages information from an anatomical region atlas to simultaneously deploy query anchors and restrict the crossattention’s field of view to regions of interest, which allows for a precise focus on relevant anatomical structures. We evaluate our proposed approach on two publicly available CT datasets and demonstrate that Focused Decoder not only provides strong detection results and thus alleviates the need for a vast amount of annotated data but also exhibits exceptional and highly intuitive explainability of results via attention weights. Our code is available at &lt;a href='https://github.com/bwittmann/transoar'&gt;https://github.com/bwittmann/transoar&lt;/a&gt;",
  "full_text": "Journal of Machine Learning for Biomedical Imaging 2023:003 vol. 2, pp. 72–95 Submitted 10/2022\nPublished 02/2023\nFocused Decoding Enables\n3D Anatomical Detection by Transformers\nBastian Wittmann bastian.wittmann@uzh.ch\nDepartment of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland\nFernando Navarro fernando.navarro@tum.de\nDepartment of Informatics, Technical University of Munich, Munich, Germany\nSuprosanna Shit suprosanna.shit@tum.de\nDepartment of Informatics, Technical University of Munich, Munich, Germany\nBjoern Menze bjoern.menze@uzh.ch\nDepartment of Quantitative Biomedicine, University of Zurich, Zurich, Switzerland\nAbstract\nDetection Transformers represent end-to-end object detection approaches based on a Trans-\nformer encoder-decoder architecture, exploiting the attention mechanism for global relation\nmodeling. Although Detection Transformers deliver results on par with or even superior to\ntheir highly optimized CNN-based counterparts operating on 2D natural images, their suc-\ncess is closely coupled to access to a vast amount of training data. This, however, restricts\nthe feasibility of employing Detection Transformers in the medical domain, as access to\nannotated data is typically limited. To tackle this issue and facilitate the advent of medi-\ncal Detection Transformers, we propose a novel Detection Transformer for 3D anatomical\nstructure detection, dubbed Focused Decoder. Focused Decoderleverages information from\nan anatomical region atlas to simultaneously deploy query anchors and restrict the cross-\nattention’s field of view to regions of interest, which allows for a precise focus on relevant\nanatomical structures. We evaluate our proposed approach on two publicly available CT\ndatasets and demonstrate that Focused Decodernot only provides strong detection results\nand thus alleviates the need for a vast amount of annotated data but also exhibits excep-\ntional and highly intuitive explainability of results via attention weights. Our code is\navailable at https://github.com/bwittmann/transoar.\nKeywords: Anatomical Structure Localization, Attention, CT, Deep Learning, Detection\nTransformer, Explainable AI\n1. Introduction\nOver time, object detection has firmed its position as a fundamental task in computer vi-\nsion (Lin et al., 2014; Shao et al., 2019; Geiger et al., 2012). In medical imaging, however,\nobject detection remains heavily under-explored, as the primary focus lies on the domi-\nnant discipline of semantic segmentation. This can be mostly traced back to the fact that\nmany clinically relevant tasks require voxel-wise predictions. Object detection, on the other\nhand, yields object-specific bounding boxes as output. Even though these bounding boxes\nthemselves are of relevancy for object-level diagnostic decision-making (Baumgartner et al.,\n2021), efficient medical image retrieval and sorting (Criminisi et al., 2010), streamlining\ncomplex medical workflows (Gauriau et al., 2015), and robust quantification (Tong et al.,\n©2023 Wittmann, Navarro, Shit, and Menze. License: CC-BY 4.0\nhttps://melba-journal.org/2023:003\nFocused Decoding Enables 3D Anatomical Detection by Transformers\n2019), they can be additionally utilized to increase the performance of many medically rel-\nevant downstream tasks, such as semantic segmentation (Navarro et al., 2022; Liang et al.,\n2019), image registration (Samarakoon et al., 2017), or lesions detection (Mamani et al.,\n2017). In this context, the bounding boxes generated by the preliminary detection step\nallow the downstream task to focus solely on regions that are likely to contain relevant\ninformation, which not only introduces a strong inductive bias but also improves the com-\nputation efficiency and unifies the sizes of regions for further analysis. In the specific case of\nsemantic segmentation, the estimated bounding boxes are utilized to crop organ-wise RoIs\nfrom the original CT images. Subsequently, these organ-wise RoIs are used to train organ-\nspecific segmentation networks allowing the individual segmentation networks to solely learn\nhighly specialized organ-specific feature representations. This results in a binary instead of\na multi-class semantic segmentation problem. However, since this hypothesis only holds if\nthe detection algorithm yields precise results, special attention must be paid towards the\ndevelopment of medical detection algorithms.\nCurrently, most state-of-the-art architectures operating on 2D natural images exploit\nthe global relation modeling capability of Vision Transformers. These Vision Transformers\ncan generally be divided into two categories, namely Vision Transformer Backbones and\nDetection Transformers. While Detection Transformers represent complete end-to-end ob-\nject detection pipelines with an encoder-decoder structure similar to the original machine\ntranslation Transformer (Vaswani et al., 2017), Vision Transformer Backbones solely utilize\nthe Transformer’s encoder for feature refinement. To obtain competitive results, object\ndetectors typically combine variants of Vision Transformer Backbones with two-stage (Ren\net al., 2015) or multi-stage approaches (Chen et al., 2019) based on convolutional neural\nnetworks (CNN).\nAlthough the success of Vision Transformer Backbones has shown that the global re-\nlation modeling capability introduced by the attention operation is clearly beneficial, the\nnovel concept of Detection Transformers was unable to produce competitive performances\nand, therefore, mostly seen as an alternate take on object detection, particularly suitable\nfor set prediction. However, as time evolved, modifications (Zhu et al., 2021; Li et al., 2022;\nLiu et al., 2022a) of the original Detection Transformer (Carion et al., 2020) narrowed the\nperformance gap to their highly optimized and intensively researched CNN-based counter-\nparts until a Detection Transformer, called DINO (Zhang et al., 2022), was finally able to\nachieve state-of-the-art results on the COCO benchmark.\nDespite the successful employment of Detection Transformers for computer vision tasks\nprocessing 2D natural images, the feasibility of adapting Detection Transformers for medical\ndetection tasks remains largely unexplored. The faltering progress of Detection Transform-\ners in the medical domain can be mainly attributed to the absence of large-scale annotated\ndatasets, which are crucial for the successful employment of Detection Transformers (Doso-\nvitskiy et al., 2020). This is because their attention modules require long training schedules\nto learn sparse, focused attention weights due to their shortage of inductive biases com-\npared to the convolutional operation. In this regard, large-scale datasets are essential to\navoid overfitting. Therefore, current medical object detectors still rely predominantly on\nCNN-based approaches.\nIn order to accelerate the advent of Detection Transformers to the medical domain,\nsystematic research focusing on their adaption and thus overcoming the limitations im-\n73\nWittmann, Navarro, Shit, and Menze\nposed by small-scale datasets has to be conducted. Many clinically relevant detection tasks\nsuch as organs-at-risk or vertebrae detection rely on a well-defined anatomical field of view\n(FoV) (Schoppe et al., 2020). This implies that approximate relative and absolute posi-\ntions of anatomical structures of interest remain consistent throughout the whole dataset.\nFor example, the right lung always appears above the liver in the top right region. We\nargue that Detection Transformers should bear an immense performance potential for these\nwell-defined FoV anatomical structure detection tasks by presenting three major arguments:\n1. The Detection Transformers’ learned positional embeddings, also called query em-\nbeddings, should be especially beneficial in the context of the positional consistency\nassumption of well-defined FoV detection tasks.\n2. The concept of relation modeling inherent to Detection Transformers should allow the\nmodel to capture relative positional inter-dependencies among anatomical structures\nvia self-attention among object queries.\n3. Visualization of attention weights allows for exceptional and accessible explainability\nof results, which is a crucial aspect of explainable artificial intelligence in medicine.\nTherefore, this work aims to pave the path for Detection Transformers, solving the\ntask of 3D anatomical structure detection. We propose a novel Detection Transformer,\ndubbed Focused Decoder, that alleviates the need for large-scale annotated datasets by\nreducing the complexity of the 3D detection task. To this end, Focused Decoderbuilds\nupon the positional consistency assumption of well-defined FoV detection tasks by exploiting\nanatomical constraints provided by an anatomical region atlas. Focused Decoderutilizes\nthese anatomical constraints to restrict the cross-attention’s FoV and reintroduce the well-\nknown concept of anchor boxes in the form of query anchors. We prove that Focused\nDecoder drastically outperforms two Detection Transformer baselines (Carion et al., 2020;\nZhu et al., 2021) and performs comparably to the CNN-based detector RetinaNet (Jaeger\net al., 2020) while having a significantly lower amount of trainable parameters and increased\nexplainability of results. We summarize our contribution as follows:\n• To the best of our knowledge, we are the first to successfully leverage 3D Detection\nTransformers for 3D anatomical structure detection.\n• We adapt two prominent 2D Detection Transformers, DETR and Deformable DETR,\nfor 3D anatomical structure detection.\n• We introduceFocused Decoder, a novel Detection Transformer producing strong detec-\ntion results for well-defined FoV detection tasks by exploiting anatomical constraints\nprovided by an anatomical region atlas to deploy query anchorsand restrict the cross-\nattention’s FoV.\n• We identify attention weights as a crucial aspect of explainable artificial intelligence\nin medicine and demonstrate that Focused Decoder improves the explainability of\nresults.\n• We compare performances of DETR, Deformable DETR, and Focused Decoderwith\nCNN-based approaches represented by RetinaNet on two publicly available computed\ntomography (CT) datasets and ensure direct comparability of results.\n74\nFocused Decoding Enables 3D Anatomical Detection by Transformers\n2. Related Works\nSince Focused Decoderbuilds upon prior work, we discuss related work on 2D Detection\nTransformers processing natural images followed by 3D medical detection algorithms. We\npay special attention to detectors featured in this work.\n2.1 2D Detection Transformers\nDETR (Carion et al., 2020) laid the foundations for Detection Transformers. DETR adopts\nthe machine translation Transformer’s (Vaswani et al., 2017) encoder-decoder architecture\nto create a streamlined end-to-end object detection pipeline. However, in contrast to the\nmachine translation Transformer, DETR predicts the final set of bounding boxes in parallel\nrather than in an autoregressive manner. DETR employs a ResNet-based backbone to\ngenerate a low resolution representation of the input image, which is subsequently flattened\nand refined in N Transformer encoder blocks. The refined input sequence is in the next step\nforwarded to the decoder’s cross-attention module to refine a set of object queries. After\nthe object queries have been refined in N Transformer decoder blocks, they are fed into a\nclassification and a bounding box regression head, resulting in predicted bounding boxes and\nclass scores. During training, a Hungarian algorithm enforces one-to-one matches, which are\nparticularly suitable for set prediction tasks, based on a set of weighted criteria. Although\nDETR’s simple design achieves promising 2D detection results on par with a Faster R-CNN\nbaseline, it suffers from high computational complexity, low performance on small objects,\nand slow convergence.\nDeformable DETR (Zhu et al., 2021) improved DETR’s detection performance while\nsimultaneously reducing the model’s computational complexity and training time by in-\ntroducing the deformable attention mechanism. The concept of deformable attention has\nbeen derived from the concept of deformable convolution (Dai et al., 2017), which increases\nthe modeling ability of CNNs by leveraging learned sampling offsets from the original grid\nsampling locations. The deformable attention module utilizes the learned sparse offset sam-\npling strategy introduced by deformable convolutions by allowing an object query to solely\nattend to a small fixed set of sampled key points and combines it with the relation modeling\nability of the attention operation.\nIt is worth mentioning that many Detection Transformer variants tried to improve\nDETR’s initial concept over time. For example, Efficient DETR (Yao et al., 2021) eliminated\nDETR’s need for an iterative object query refinement process, Conditional DETR (Meng\net al., 2021) introduced a conditional cross-attention module, DAB-DETR (Liu et al., 2022a)\nupdated the object query formulation to represent stronger spatial priors, DN-DETR (Li\net al., 2022) presented a denoising training strategy, and DINO (Zhang et al., 2022) com-\nbined and improved important aspects like denoising training, query initialization, and box\nprediction.\n2.2 3D Medical Detection\nEven though 3D medical detection is a longstanding topic in medical image analysis (Cri-\nminisi et al., 2009, 2010), most research currently focuses on the more dominant discipline\nof semantic segmentation (Isensee et al., 2021; Navarro et al., 2021, 2019). This is because\n75\nWittmann, Navarro, Shit, and Menze\nmost clinically relevant tasks require voxel-vise predictions. Therefore, prior work on deep\nlearning-based 3D medical detection remains relatively limited.\n2.2.1 CNN-Based Detectors\nApart from individual experiments with different detectors, such as 3D Faster R-CNN (Xu\net al., 2019), most research focused on the CNN-based detector Retina U-Net (Jaeger et al.,\n2020; Baumgartner et al., 2021). Retina U-Net builds upon the one-stage detector Reti-\nnaNet (Lin et al., 2017) and modifies its architecture for 3D medical detection. Retina\nU-Net’s main contribution is represented by the introduction of a segmentation proxy task.\nThe segmentation proxy task allows the use of voxel-level annotations present in most med-\nical datasets as an additional highly detailed supervisory signal. For detecting objects at\ndifferent scales, Retina U-Net’s backbone, given by a modified feature pyramid network\n(FPN), forwards multi-level feature maps to the bounding box regression and classification\nhead networks. Since each voxel of these multi-level feature maps is assigned to a set of 27\nlevel-specific anchor boxes varying in scale, the bounding box regression head predicts posi-\ntion and size offsets, while the classification head predicts their corresponding class scores.\nAs Retina U-Net, therefore, represents a dense one-stage detection scheme, it utilizes non-\nmaximum suppression to reduce duplicates during inference. Retina U-Net is also featured\nin the automated medical object detection pipeline nnDetection (Baumgartner et al., 2021).\nFollowing nnU-Net’s (Isensee et al., 2021) agenda, nnDetection adapts itself without any\nmanual intervention to arbitrary medical detection problems while achieving results on par\nwith or superior to the state-of-the-art. Furthermore, SwinFPN (Wittmann et al., 2022)\nexperimented with Vision Transformer Backbones by incorporating 3D Swin Transformer\nblocks (Liu et al., 2022b) in Retina U-Net’s architecture, following recent trends in medical\nsemantic segmentation (Hatamizadeh et al., 2022b,a).\n2.2.2 Detection Transformers\nAlthough some studies experimented with Detection Transformers operating on 2D medical\ndata (Prangemeier et al., 2020; Shen et al., 2021), only a few attempts tried to adapt them\nfor 3D medical detection tasks. Spine-Transformer (Tao et al., 2022), for example, leverages\nDETR for sphere-based vertebrae detection. To this end, Spine-Transformer augments\nDETR with skip connections and additional learnable positional encodings. In contrast\nto Focused Decoder, however, Spine-Transformer’s concept relies on data of arbitrary FoV.\nRelationformer (Shit et al., 2022) successfully utilizes Deformable DETR to detect small-\nscale blood vessels from 3D voxel-level segmentations. The authors introduce an additional\nrelation-token and demonstrate that Relationformer achieves state-of-the-art performances\nfor multi-domain image-to-graph generation tasks, such as blood vessel graph generation.\n3. Methods\nThis section builds our rationale for the proposed Detection Transformer Focused Decoder.\nFocused Decoderexplicitly takes advantage of the fact that approximate relative and ab-\nsolute positions of labeled anatomical structures contained in datasets of well-defined FoV\nare consistent, which we try to ensure in an abundant and necessary preprocessing step.\n76\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nFigure 1: Detailed structure of the FPN. The FPN can generally be divided into three\nsections: the down-sampling branch (down, C0 - C5), the up-sampling branch\n(up), and the final output projection (out, P2 - P5).\nWe, therefore, draw parallels to the well-known concept of anatomical atlases (Hohne et al.,\n1992). Based on this positional consistency assumption, we first determine for each dataset\nFoV-specific anatomical region atlases containing regions of interest (RoI) that comprise\nlabeled anatomical structures. Subsequently, we place in each structure- or class-specific\nRoI uniformly spaced query anchors and assign a dedicated object query to each of them,\nresulting in two levels of abstraction. Therefore, Focused Decoder’sobject queries are not\nonly assigned to individual RoIs but also to query anchors located inside RoIs. This al-\nlows Focused Decoderto restrict the object queries’ FoV for cross-attention to solely voxel\nwithin their respective RoI, simplify matching during training and inference, and overcome\npatient-to-patient variability by generating diverse class-specific predictions enforced by\nquery anchors.\nIn the following, we introduce the FPN feature extraction backbone and the atlas gener-\nation process. Finally, we present the novel concept ofquery anchorsand essential aspects of\nFocused Decoder, including its architecture, the focused cross-attention module, the concept\nof relative offset prediction, its matching strategy, and its loss function.\n3.1 FPN Feature Extraction Backbone\nFollowing DETR, Focused Decoder relies on a feature extraction backbone to create a\nlower resolution representation of the input CT image x ∈ RH×W×D, which mitigates\nthe high computational complexity of the attention operation. This feature extraction\nbackbone is depicted in Fig. 1 and is given, similar to Retina U-Net, by an FPN. It\nshould be mentioned that although the feature extraction backbone serves as a CNN-\nbased feature encoder, we refrain from referring to it as an encoder to avoid confusion\nbetween the Transformer’s encoder and the FPN. The FPN consists of a CNN-based down-\nsampling branch (down), an up-sampling branch (up), and a final output projection (out).\n77\nWittmann, Navarro, Shit, and Menze\nFigure 2: Concept of query anchors. We place 27 query anchorsof class-specific median size\nat uniformly spaced positions in each RoI. Focused Decoderpredicts relative po-\nsition and size offsets with regard to fixed query anchors. Exemplary predictions\ncorresponding to the upper left query anchor of the left adrenal gland pushing\nthe position offset restriction to its limit are visualized in red. Regions of possible\nquery-specific final center positions are indicated in blue.\nThe up-sampling branch incorporates the down-sampled multi-level feature maps {xCl}5\nl=2,\nwhere xCl ∈ R24·2l×H/2l×W/2l×D/2l\n, via lateral connections based on 1 × 1 × 1 convolu-\ntions and combines them with up-sampled feature maps of earlier stages. A final out-\nput projection acts as an additional refinement stage, fixing the channel dimension to\ndhidden. Therefore, the FPN outputs a set of refined multi-level feature maps {xPl }5\nl=2,\nwhere xPl ∈ Rdhidden×H/2l×W/2l×D/2l\n, encoding semantically strong information by leverag-\ning the top-down inverted high- to low-level information flow. Even thoughFocused Decoder\nsolely processes the feature map xP2, we require multi-level feature maps for our experi-\nments. This is because our experiments rely on the same feature extraction backbone to\nensure maximum comparability of results (see Fig. 4 and Table 5).\n3.2 Atlas Generation\nIn this paper, we generate custom atlases and refer to them as anatomical region atlases.\nRelying on the positional consistency assumption, we determine class-specific RoIs, describ-\ning the minimum volumes in which all instances of a particular anatomical structure are\nlocated. A small subset of RoIs is shown in Fig. 2 (left). Additionally, we estimate for\neach labeled anatomical structure the median, minimum, and maximum bounding box size,\nwhich will be necessary for the query anchor generation process and the concept of relative\noffset prediction. Given that the test set should only be used to estimate the final perfor-\nmance of the model, we estimate RoIs and bounding box sizes solely based on instances\ncontained in the training and validation sets.\nSince medical CT datasets typically contain different labeled anatomical structures,\nFoVs are inconsistent across datasets. Therefore, we generate dataset-specific anatomical\n78\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nregion atlases. In general, however, existing anatomical atlases could be adjusted to fit the\ndatasets at use based on a few anatomical landmarks (Potesil et al., 2013; Xu et al., 2016).\n3.3 Query Anchor Generation\nDue to dataset-specific inconsistencies and the substantial amount of patient-to-patient\nvariability with regard to positions and sizes of anatomical structures, it is often difficult to\nensure that datasets consist of images of exactly the same FoV. This is reflected in the fact\nthat, on average, RoIs occupy 25 times larger volumes than anatomical structures. There-\nfore, Focused Decoderfurther subdivides RoIs by assigning to each class-specific RoI in our\nanatomical region atlas 27 fixed class-specificquery anchorsin the format (cx, cy, cz, h, w, d).\nWe denote query anchors by qanchors ∈ R#classes×27×6. While the spatial locations of query\nanchors are defined by uniformly spaced positions in their corresponding RoIs, their sizes\nare governed by the class-specific median bounding box sizes contained in the anatomical\nregion atlas. Generated query anchors associated with the left adrenal gland’s RoI are\nshown in detail in Fig. 2 (right).\n3.4 Focused Decoder\n3.4.1 Architecture\nThe architecture of Focused Decoderand its iterative refinement process are shown in Fig. 3.\nFocused Decoderrepresents a lightweight Detection Transformer consisting solely of N de-\ncoder blocks, omitting the Transformer’s encoder completely. The Transformer’s encoder\nhas the task of modeling relations between elements of the input sequence via its self-\nattention module. In 3D, this task is highly complex due to long input sequences that\narise from flattening high resolution 3D feature maps. Therefore, we argue that omitting\nthe Transformer’s encoder is a sound design choice when training on small-scale medical\ndatasets. Following this hypothesis, Focused Decoderforwards the input sequence given by\nthe flattened feature map xP2 directly to the decoder.\nFocused Decoderiteratively refines a set of object queries in N stacked Focused Decoder\nblocks (see Fig. 3 (right)). We assign an individual object query to each generatedquery an-\nchor, which results in qobjects ∈ R#classes×27×dhidden, containing class-specific object queries.\nBy assigning object queries to unique spatial locations in the form of query anchors, we\ndemystify their associated learned positional embeddings, called query embeddings, and\nencourage diverse class-specific predictions, overcoming the issue of patient-to-patient vari-\nability. Besides the focused cross-attention module, the general structure of the Focused\nDecoder block remains similar to DETR’s decoder. The main components of the Focused\nDecoder block (see Fig. 3 (left)) are represented by a self-attention module followed by a\nfocused cross-attention moduleand a two-layer feedforward network (FFN), which utilizes a\nReLU non-linearity in between its two linear layers. The FFN’s in- and output dimensions\ncorrespond to the object queries’ dhidden. Its hidden dimension is represented by dFFN.\nWhile the self-attention module aims to encode strong positional inter-dependencies among\nobject queries, the focused cross-attention module matches the input sequence to object\nqueries and thus regulates the influence of individual feature map voxels for prediction\n79\nWittmann, Navarro, Shit, and Menze\nFigure 3: ( Left): Focused Decoder’s detailed architecture. The input sequence is given\nby the flattened feature map of the FPN stage P2. Focused Decodermakes use\nof class-specific object queries by assigning object queries to 27 query anchors\nlocated inside each class-specific RoI (see color coding). Object queries are not\nonly associated with query anchors but also with learned positional embeddings,\ncalled query embeddings. ( Right): Iterative refinement process in N stacked\nFocused Decoderblocks. Object queries are initialized with zeros. Refined object\nqueries are forwarded to subsequent Focused Decoderblocks.\nvia attention. Subsequently, the FFN facilitates richer feature representations. We employ\nadditional residual connections and layer normalization operations to increase gradient flow.\nThe classification and bounding box regression head networks are attached to the last\nFocused Decoderblock and process the refined object queries (see Fig. 3 (left)). Given that\nobject queries are already preassigned to specific classes, the classification head predicts\nclass-specific confidence scores, resulting in a binary classification task. The classification\nhead is given by a simple linear layer, whereas the bounding box regression head is repre-\n80\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nsented by a more complex three-layer FFN, which predicts relative position and size offsets\nwith regard to query anchors. The hidden dimension of the bounding box regression head\ncorresponds to the object queries’ dhidden. Eventually, we combine these relative offset pre-\ndictions with their corresponding query anchors and return the predicted bounding boxes\ntogether with the class-specific confidence scores, generating overall 27 candidate predictions\nper class.\nSince the attention operation is known to be permutation invariant, valuable information\nabout the spatial location of voxels would be dismissed. To tackle this issue, a positional\nencoding consisting of sine and cosine functions of different frequencies tries to encode\ninformation regarding the absolute position of voxels directly in the input sequence.\n3.4.2 Focused Cross-Attention\nAt the core of Focused Decoderlies the focused cross-attention moduleforcing class-specific\nobject queries to focus primarily on the structures of interest and their close proximity\ncontained in RoIs of our anatomical region atlas. To this end, the focused cross-attention\nmodule leverages an attention mask M ∈ R#queries×#voxels to restrict the object queries’\nFoV for cross-attention. This restriction of attention drastically simplifies the relation\nmodeling task, as we solely have to learn dependencies between object queries and relevant\nvoxels located in their respective RoIs. Equation (1) demonstrates the concept of masked\nattention, exploited in the focused cross-attention module.\nMaskedAttn(Q, K, V) = softmax( QKT\n√dhidden\n+ M)V (1)\nHere, the key and value sequences K and V are derived from the input sequence, while the\nobject queries contribute to the query sequence Q. Masking is done by adding the attention\nmask M onto the raw attention weightsQKT /√dhidden. The attention mask M contains the\nvalue −inf for voxels outside and the value 0 for voxels inside the object queries’ respective\nRoIs. By adding −inf, we nullify the importance of attention weights outside of RoIs for\nprediction. This is because attention weights corresponding to the value −inf end up in\nextremely flat areas of the softmax function, which in turn kills gradient flow.\n3.4.3 Relative Offset Prediction\nFocused Decoderleverages query anchors to take advantage of the concept of relative offset\nprediction. To this end, we extend the bounding box regression head with a tanh nonlin-\nearity, allowing object queries to modify their query anchors’ fixed center positions and\nsizes for prediction (see Fig. 3 (left)). However, these modifications are restricted. To coun-\nteract overlap and facilitate diverse predictions, center position offsets are restricted to a\nmaximum value, resulting in distinct query-specific regions of possible final center positions\ncovering the RoI completely (see Fig. 2 (right)). In contrast to position offset restrictions,\nwhich are derived from the sizes of RoIs, we restrict the allowed size offsets to the minimum\nand maximum bounding box sizes contained in our anatomical region atlas.\n81\nWittmann, Navarro, Shit, and Menze\n3.4.4 Matching\nSince Focused Decoderpredicts 27 eligible candidates per class, matching boils down to find-\ning the most suitable candidate among 27. To train the head networks in a meaningful way,\nwe generate dynamic confidence labels, solely relying on the normalized generalized inter-\nsection over union (GIoU) between all class-specific query anchors and their corresponding\nground truth objects. This results in dynamic confidence labels of 1 for predictions corre-\nsponding to query anchorswith the highest GIoU per class and dynamic confidence labels of\n0 for predictions corresponding to query anchors with the lowest GIoU per class. Based on\nthese dynamic confidence labels, we match predictions with the highest dynamic confidence\nlabels per class to the ground truth objects. We subsequently forward matched predictions\nand the dynamic confidence labels to the loss function.\n3.4.5 Loss Function\nEquation (2) expresses Focused Decoder’sgeneral loss function, where N corresponds to\nthe number of Focused Decoderblocks. Following Deformable DETR, λcls, λGIoU, and λℓ1\ncorrespond to 2, 2, and 5.\nL =\nNX\nn=1\n(λcls · Ln\nBCE + λGIoU · Ln\nGIoU + λℓ1 · Ln\nℓ1) (2)\nWe utilize a binary cross-entropy loss LBCE between predicted confidence scores and dy-\nnamic confidence labels determined during matching to facilitate diverse predictions during\ninference. For bounding box regression, two loss functions, namely a scale-invariant GIoU\nloss LGIoU and an ℓ1 loss Lℓ1, are combined. The bounding box regression loss functions\nsolely consider matched predictions and thus only optimize predictions having dynamic con-\nfidence labels of 1. Following DETR, we additionally forward predictions generated based\non outputs of earlier Focused Decoderblocks (see Fig. 3 (right)) to the final loss function,\nresulting in an auxiliary loss for intermediate supervision.\n3.4.6 Inference\nDuring inference, the class-specific confidence scores predicted by the classification head\nindicate the most suitable candidates. Therefore, we simply select predictions with the\nhighest confidence scores per class to represent the output.\n4. Experiments and Results\nIn this section, we determine Focused Decoder’sperformance for 3D anatomical structure\ndetection and compare it to two Detection Transformer baselines and a RetinaNet vari-\nant adopted from the state-of-the-art detection pipeline nnDetection. Subsequently, we\ndemonstrate the excellent explainability of Focused Decoder’sresults and critically assess\nthe importance of its design choices.\n82\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nFigure 4: Overview of compared detectors. All featured architectures utilize the same FPN\nfeature extraction backbone. Detection Transformers are depicted in red, while\nCNN-based traditional approaches are shown in yellow.\n4.1 Experimental Setup\nFirst, it should be mentioned that comparability of performances is of utmost importance\nand, due to the lack of 3D anatomical structure detection benchmarks, challenging to\nachieve. Therefore, we adapt and directly integrate detectors featured in this work into\nthe same detection and training pipeline for a fair and reproducible comparison. For ex-\nample, all featured detectors were trained until convergence on a single Quadro RTX 8000\nGPU using the same AdamW optimizer, step learning rate scheduler, data augmentation\ntechniques, and FPN feature extraction backbone. In addition, we tried to keep the config-\nurations of featured Detection Transformers as similar as possible. We use the same head\nnetworks, set N to three, dhidden to 384, and dFFN to 1024. By doing so, we ensure maximal\ncomparability of results. Precise information about training details, hyperparameters, and\nindividual model configurations is available athttps://github.com/bwittmann/transoar.\nTo facilitate understanding of our experimental setup, Fig. 4 depicts an overview of all fea-\ntured detectors. Next, we briefly introduce the Detection Transformer baselines, which we\nadapted for 3D anatomical structure detection, and our RetinaNet variant.\n4.1.1 DETR\nDETR acts as our first Detection Transformer baseline. Besides some additional minor\nadjustments, the general structure and configuration of our DETR variant remain essentially\nunchanged. Following the original DETR’s configuration, we set the number of object\nqueries, which are in contrast to Focused Decodernot preassigned to specific classes, to 100.\nWhile the Hungarian algorithm is responsible for matching during training, we return the\nhighest-scoring predictions per class during inference.\n4.1.2 Deformable DETR\nTo evaluate Focused Decoderproperly, we introduce Deformable DETR as an additional\nDetection Transformer baseline. Deformable DETR generates its input sequence by flat-\ntening and combining the multi-level feature maps of the stages P2 to P5 into a combined\ninput sequence and refines, similar to DETR, a set of 100 object queries. Deformable DETR\nemploys the same matching strategies during inference and training as DETR.\n83\nWittmann, Navarro, Shit, and Menze\n4.1.3 RetinaNet\nAs a representative of CNN-based detectors, we adopt Retina U-Net from nnDetection\nwith minor modifications. Since the automated detection pipeline nnDetection developed\naround Retina U-Net has been shown to deliver state-of-the-art results for numerous medical\ndetection tasks, we extract nnDetection’s generated hyperparameters and refine them via a\nbrief additional hyperparameter search. In addition, we omit Retina U-Net’s segmentation\nproxy task, converting it to a RetinaNet variant. This not only enforces comparability\nwith other detectors featured in this work but also drastically reduces training times. We\njustify this decision as experiments 1 have shown that the segmentation proxy task leads\nto extremely minor and hence negligible performance benefits for 3D anatomical structure\ndetection.\n4.2 Datasets\nDue to the lack of 3D anatomical structure detection benchmarks, we conduct experiments\non two CT datasets of well-defined FoV that were originally developed for the task of\nsemantic segmentation, namely the VISCERAL anatomy benchmark (Jimenez-del Toro\net al., 2016) and the AMOS22 challenge (Ji et al., 2022). We transform their voxel-wise\nannotations into bounding boxes and class labels in the dataloader. Dataset-specific spatial\nsizes, approximated resolutions, and sizes of the respective training, validation, and test\nsets are along with the total number of subjects reported in Table 1.\nTable 1: Final dataset properties after preprocessing.\nDataset Size Resolution (in mm) Train / Val / Test # of subjects\nVISCERAL 160 × 160 × 256 2 .5 × 2.1 × 2.7 120 / 20 / 12 152\nAMOS22 256 × 256 × 128 1 .0 × 0.7 × 3.5 117 / 20 / 18 155\n4.2.1 VISCERAL Anatomy Benchmark\nThe VISCERAL anatomy benchmark contains segmentations of 20 major anatomical struc-\ntures. The CT images of the silver corpus subset are used as training data, while the gold\ncorpus subset is split into two halves to create the validation and test sets.\n4.2.2 AMOS22 Challenge\nThe multi-modality abdominal multi-organ segmentation challenge of 2022, short AMOS22,\nprovides CT images with voxel-level annotations of 15 abdominal organs. We divide the\nCT images of the challenge’s first stage into the training, validation, and test sets.\n4.2.3 Preprocessing\nWe utilize the same preprocessing approach for both datasets. First, the raw CT images\nand labels are transformed to a uniform orientation represented by the RAS, short for ’right,\nanterior, superior’, axis code. We subsequently crop CT images to foreground structures.\n1. We provide experiments with the segmentation proxy task incorporated in all featured detectors in our\nGitHub repository. To activate the segmentation proxy task, set flag use seg proxy loss to True in the\nrespective config file.\n84\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nBy doing so, we additionally try to ensure datasets consisting of images with a similar\nFoV, which is necessary to determine meaningful class-specific RoIs. In the next step, the\nCT images and their labels are resized to a fixed spatial size. This increases fairness and\nfurther reduces the detection task’s complexity by compensating for variations in patient\nbody size. To compensate for incompletely labeled CT images, we discard CT images based\non a specified label threshold, allowing for a sufficient amount of data while simultaneously\nensuring fairly clean datasets. Additionally, we completely omit partially labeled CT images\nin the test sets to provide a valid performance estimate.\n4.3 Metric\nThe metric commonly used to evaluate object detection algorithms is the mean average\nprecision (mAP). The mAP metric, described in (3), is defined as the mean of a selected\nsubset of average precision (AP) values. We report mAP coco, which is calculated based on\nAP values evaluated at IoU thresholds T = {0.5, 0.55, ..., 0.95 }.\nmAPcoco = 1\n|T|\nX\nt∈T\nAPt (3)\nTo determine detection performance related to structure size, we introduce mAPS\ncoco, mAPM\ncoco,\nand mAPL\ncoco. To this end, we categorize classes based on the volume occupancy of their\nmedian bounding boxes into the subsets S (small), M (medium), and L (large), which rely\non the volume occupancy ranges of [0.0%, 0.5%), [0.5%, 5.0%), and [5.0%, 100%], respec-\ntively. Subsequently, we reevaluate mAP coco restricted to solely classes in the individual\nsubsets. The dataset-specific subsets determined based on the above reported volume oc-\ncupancy ranges are shown in Table 2. It should be mentioned that although both datasets\nhave anatomical structures in common, they might be assigned to different subsets due to\nvarying FoVs. The difference in FoV across both datasets can be observed in Fig. 7 (left).\nTable 2: Size-specific subsets.\nVISCERAL\nS pancreas, gall bladder, urinary bladder, trachea, thyroid gland,\nfirst lumbar vertebra, adrenal glands\nM spleen, aorta, sternum, kidneys, psoas major, rectus abdominis\nL liver, lungs\nAMOS\nS esophagus, adrenal glands, prostate/uterus\nM spleen, kidneys, gall bladder, aorta, postcava, pancreas, duodenum,\nurinary bladder\nL liver, stomach\n4.4 Quantitative Results\nTable 3 lists quantitative results of all detectors featured in this work. Essentially, our\nfindings remain consistent across both datasets. Focused Decoder, possessing the least\namount of trainable parameters, significantly outperforms the Detection Transformer base-\nlines, namely DETR and Deformable DETR, and performs close to the state-of-the-art\ndetector RetinaNet.\n85\nWittmann, Navarro, Shit, and Menze\nTable 3: Quantitative results estimated on the VISCERAL anatomy benchmark and the\nAMOS22 challenge. We compare Focused Decoderto the baseline architectures\nDETR, Deformable DETR, and RetinaNet. Focused Decoderperforms best on\nlarge structures, while RetinaNet dominates on small structures.\nDataset Model #params ↓ mAPcoco ↑ mAPS\ncoco ↑ mAPM\ncoco ↑ mAPL\ncoco ↑\nVISCERAL\nRetinaNet 52.8M 41.37* 20.99* 47.02 78.74\nDETR 46.5M 27.35 11.32 28.10 67.88\nDef. DETR 54.8M 33.06 14.52 35.09 76.39\nFocused Decoder 41.8M 39.22 18.33 43.59 81.70*\nAMOS22\nRetinaNet 52.1M 30.38 16.92* 32.37 44.38\nDETR 43.7M 16.47 4.24 15.68 32.96\nDef. DETR 53.4M 26.59 8.93 30.32 43.08\nFocused Decoder 42.6M 29.83 10.30 34.01 47.97*\n* denotes statistically significant difference between Focused Decoder and RetinaNet; p-values < 0.05.\nThis demonstrates that a naive adaption of 2D Detection Transformers to the medi-\ncal domain is definitely not sufficient to overcome the problem of data scarcity. However,\nFocused Decoder’sstrong detection performances reveal that reducing the complexity of\nthe relation modeling and thus the 3D detection task alleviates the need for large-scale\nannotated datasets, converting Detection Transformers into competitive and lightweight\n(approximately 20% fewer parameters compared to RetinaNet) 3D medical detection algo-\nrithms. Focused Decodernot only performs close to or on par with its CNN-based coun-\nterpart but also provides the most accurate predictions for larger structures. On the other\nhand, RetinaNet outperforms Focused Decoderon smaller structures, overcoming the De-\ntection Transformer’s well-known shortcoming of poor small object detection performance,\nwhich is subject to open research.\nEven though the overall detection performance on the VISCERAL anatomy benchmark\nturns out to be stronger, Focused Decodermanages to narrow the gap to RetinaNet on\nthe AMOS22 challenge. The generally stronger performances on the VISCERAL anatomy\nbenchmark can be attributed to the absence of symmetrical and easy to detect structures\nsuch as the lungs or the rectus abdominis in the AMOS22 challenge. The narrowed perfor-\nmance gap, however, primarily results from the difference in FoV between both datasets.\nThis is because the reduced FoV of the AMOS22 challenge (see Fig. 7 (left)) leads to an\nartificial increase in structure size, which is most beneficial for Focused Decoder.\nTo estimate the statistical significance of the performance differences between Focused\nDecoder and RetinaNet, we determined p-values using the Wilcoxon signed-rank test based\non bootstrapped subsets. We indicate statistically significant performance differences in\nTable 3 (see footnote).\n4.5 Explainability of Focused Decoder’s Predictions\nExplainability of results is of exceptional importance in the context of medical image anal-\nysis. While identifying reasons behind predictions produced by CNN-based detectors is\noften cumbersome and requires additional engineering susceptible to errors, Focused De-\ncoder’s attention weights facilitate the accessibility of explainable results. Therefore, we\n86\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nparticularly probe into intra-anatomical and inter-anatomical explainability by analyzing\ncross- and self-attention weights.\nFigure 5: ( Left): Focused Decoder’s cross-attention weights (red) corresponding to six\nselected anatomical structures (black) of the VISCERAL anatomy benchmark.\nUpon closer inspection, one can identify the actual class-specific RoIs, restricting\nthe cross-attention’s FoV. The class-specific RoIs are especially visible for small\nand hence hard to detect structures, as the model’s uncertainty is reflected in less\nprecise cross-attention weights. (Right): Comparison of explainability of results.\nWe compare the explainability of Focused Decoder’s, DETR’s, and RetinaNet’s\npredictions corresponding to the urinary bladder (first row) and the left adrenal\ngland (second row).\n4.5.1 Intra-Anatomical Explainability\nFocused Decoder’scross-attention weights between the input sequence and object queries\ncorresponding to selected anatomical structures are visualized in Fig. 5 (left). Since cross-\nattention weights indicate the importance of voxels for prediction, one can conclude that\nFocused Decoder’spredictions primarily rely on structures of interest and the context in\ntheir proximity, even for extremely small objects such as the adrenal glands.\nTo highlight Focused Decoder’sintra-anatomical explainability of results, we addition-\nally compare its cross-attention weights to cross-attention weights generated by DETR\nand gradient-weighted class activation maps corresponding to RetinaNet’s predictions in\nFig. 5 (right). These gradient-weighted activation maps were generated using the estab-\nlished Grad-CAM (Selvaraju et al., 2017) algorithm, backpropagating to the FPN’s output\nfeature maps. Focused Decoder’scross-attention weights provide the best explainability\nof results. In contrast, DETR’s cross-attention weights and RetinaNet’s class activations\n87\nWittmann, Navarro, Shit, and Menze\nare scattered over large parts of the CT image, complicating explainability and possibly\nindicating memorization of dataset-specific covariates.\n4.5.2 Inter-Anatomical Explainability\nWhile cross-attention weights indicate the importance of voxels for prediction, self-attention\nweights reveal the inter-dependencies among the class-specific object queries, providing\ninter-anatomical explainability. We visualize self-attention weights in Fig. 6 to prove the\nhypothesis that Focused Decodercaptures meaningful inter-dependencies among anatomical\nstructures.\nFigure 6: Focused Decoder’sself-attention weights between class-specific object queries cor-\nresponding to structures of the VISCERAL anatomy benchmark. We averaged\nattention weights of object queries corresponding to the same class to provide a\nconcise visualization.\nWe observe the trend of high inter-dependencies among neighboring structures. Object\nqueries corresponding to the liver (first row), for example, attend primarily to structures\nin the liver’s proximity, such as the gall bladder, the right lung, the right kidney, and the\nright psoas major, whereas object queries corresponding to the spleen (second row) attend\nprimarily to the pancreas, the left lung, the left kidney, and the left adrenal gland.\n4.6 Qualitative Results\nQualitative results in the form of predicted bounding boxes of the two best-performing\narchitectures, RetinaNet and Focused Decoder, are compared to the ground truth in Fig. 7.\nOne can observe that Focused Decoderexhibits exceptional detection performance on larger\nstructures such as the liver or the aorta, while RetinaNet remains superior in detecting small\nstructures such as the adrenal glands.\n88\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nFigure 7: Qualitative results. We compare RetinaNet’s predictions (red) and Focused\nDecoder’s predictions (yellow) to the ground truth (green) on the VISCERAL\nanatomy benchmark (first row) and the AMOS22 challenge (second row).\n4.7 Ablation Studies\nIn this subsection, we investigate the importance of Focused Decoder’sdesign choices by\nconducting detailed ablations on the validation set of the VISCERAL anatomy benchmark.\nTable 4 displays mAP coco values produced by Focused Decoder’sbase configuration (first\nrow) and three additional configurations (second to fourth row), omitting different design\nchoices.\nTable 4: Ablation on Focused Decoder’smain design choices.\nRestriction Anchors Queries per class mAPcoco ↑ ∆\n✓ ✓ 27 37.78 −\n✓ 27 36.63 -1.15\n✓ 1 35.08 -2.70\n✓ 27 32.03 -5.75\nTo evaluate the impact of our proposed query anchors, we completely omit them in the\nsecond configuration, resulting in an mAPcoco decrease of 1.15. This proves the importance\nof demystifying object queries and their associated query embeddings by assigning them to\nprecise spatial locations and thus generating diverse class-specific predictions, overcoming\nthe issue of patient-to-patient variability. The configuration presented in the third row\nadditionally reduces the amount of object queries per class from 27 to one. Based on the\nmAPcoco reduction of 2.70, we argue that having only one object query per class fails to\ncover all class-specific object variations and hence leads to performance decreases. Next,\nwe deactivate the focused cross-attention module’srestriction to RoIs in the fourth row. As\nexpected, detection performance drastically diminishes, which is reflected in an mAP coco\n89\nWittmann, Navarro, Shit, and Menze\ndelta of -5.75. The result of this ablation study repeatedly demonstrates the necessity of\nsimplifying the relation modeling task to achieve competitive detection performances.\nSince the FPN outputs a set of refined multi-level feature maps {xPl }5\nl=2, we experiment\nwith different input sequences represented by flattened feature maps of different resolutions\nand report our findings in Table 5.\nTable 5: Ablation on input feature maps.\nStage Feature map resolution mAPcoco ↑ ∆\nP2 40 × 40 × 64 37.78 −\nP3 20 × 20 × 32 36.97 -0.81\nP4 10 × 10 × 16 33.72 -4.07\nP5 5 × 5 × 8 29.96 -7.82\nOne can observe that feature maps of higher resolution are clearly beneficial for detection\nperformance, as they contain more fine-grained details, which in turn leads to more precise\nbounding box estimations. However, the performance improvement saturates as the feature\nmap resolutions increase.\nFinally, we prove that omitting the Transformer’s encoder leads to more precise de-\ntections when the amount of available training data is strictly limited. To this end, we\nexperiment with DETR and Deformable DETR configurations omitting the encoder and\nreport the results in Table 6.\nTable 6: Impact of the Transformer’s encoder.\nModel Encoder mAPcoco ↑ AP50 ↑ AP75 ↑\nDETR 25.94 61.49 17.46\nDETR ✓ 23.32 59.11 14.42\nDef DETR 30.95 67.07 24.35\nDef DETR ✓ 29.26 65.31 19.44\nEvaluation of mAPcoco values presented in Table 6 leads to the conclusion that the Trans-\nformer’s encoder is disruptive to detection performance. This supports the hypothesis that\nthe encoder’s intricate relation modeling task would require an extreme amount of training\nepochs and hence a large amount of annotated data to capture meaningful dependencies\namong the input sequence elements.\n5. Limitations\nWe would like to particularly highlight the limitations of Focused Decoder’sdesign, which\ncan be primarily attributed to the assumption of well-defined FoVs. Although this assump-\ntion is reasonable to make from a clinical standpoint, it also poses a significant challenge to\nFocused Decoder’srobustness with regard to CT images of varying FoVs. This is because\ndrastic shifts in FoV may lead to anatomical structures being located partially outside of\nRoIs, which in turn may hinder object queries to encode meaningful information via our\nproposed focused cross-attention module. However, it should be emphasized that the FoV\n90\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nof CT images could always be adjusted to fit the FoVs of our datasets by adopting our\npreprocessing step or based on registration via a few anatomical landmarks.\nIt is also worth mentioning that Focused Decoderpredicts by design for each anatomical\nstructure contained in our anatomical region atlases exactly one final bounding box. There-\nfore, the complete absence of anatomical structures, for example after radical nephrectomy,\nwould remain unnoticed.\n6. Outlook and Conclusion\nThis work lays the foundations of 3D medial Detection Transformers by introducing Fo-\ncused Decoder, a lightweight alternative to CNN-based detectors, which not only exhibits\nexceptional and highly intuitive explainability of results but also demonstrates the best\ndetection performances on large structures. Focused Decoder’simpressive performances on\nlarge structures already conclusively indicate the immense potential of Detection Trans-\nformers for medical applications.\nBased on results from our experiments, we recommend the use of Focused Decoderfor\nanatomical structure detection tasks when the priority lies on the analysis of large structures\nor the explainability of results. However, we strongly believe that with increasing sizes of\nannotated medical datasets, Detection Transformers will eventually outperform CNN-based\narchitectures on medical detection tasks in all metrics, resulting in a complete shift from\nCNN- to Transformer-based architectures.\nWe encourage future work to further explore Focused Decoder’sparameters, overcome\nits limitations, and address its inferior performance on small structures by investigating the\ninfluence of the fixed FoV of CT images, which results in drastic size differences between\nanatomical structures (the right lung, for example, occupies magnitudes more space com-\npared to the urinary bladder in CT images of fixed FoV). Future work should, therefore,\nalso focus on exploring approaches operating on uniformly sized RoIs and hence dynamic\nspatial resolutions. This would in turn lead to uniform structure sizes, possibly allowing\nFocused Decoderto overcome the issue of relative scale between anatomical structures.\nAcknowledgments\nThis work has been supported in part by the TRABIT under the EU Marie Sklodowska-\nCurie Program (Grant agreement ID: 765148) and in part by the DCoMEX (Grant agree-\nment ID: 956201). The work of Bjoern Menze was supported by a Helmut-Horten-Foundation\nProfessorship.\nEthical Standards\nThe work follows appropriate ethical standards in conducting research and writing the\nmanuscript, following all applicable laws and regulations regarding treatment of animals or\nhuman subjects.\n91\nWittmann, Navarro, Shit, and Menze\nConflicts of Interest\nWe declare we don’t have conflicts of interest.\nReferences\nMichael Baumgartner, Paul F J¨ ager, Fabian Isensee, and Klaus H Maier-Hein. nnDetection:\nA self-configuring method for medical object detection. In Proc. MICCAI, pages 530–539,\n2021.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-end object detection with transformers. In Proc. ECCV,\npages 213–229, 2020.\nKai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen\nFeng, Ziwei Liu, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. Hybrid\ntask cascade for instance segmentation. In Proc. IEEE/CVF CVPR, pages 4974–4983,\n2019.\nAntonio Criminisi, Jamie Shotton, and Stefano Bucciarelli. Decision forests with long-\nrange spatial context for organ localization in CT volumes. In Proc. MICCAI, pages\n69–80, 2009.\nAntonio Criminisi, Jamie Shotton, Duncan Robertson, and Ender Konukoglu. Regression\nforests for efficient anatomy detection and localization in CT studies. In Proc. MCV\nWorkshop, pages 106–117, 2010.\nJifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei.\nDeformable convolutional networks. In Proc. IEEE ICCV, pages 764–773, 2017.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain\nGelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers\nfor image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\nRomane Gauriau, R´ emi Cuingnet, David Lesage, and Isabelle Bloch. Multi-organ local-\nization with cascaded global-to-local regression and shape prior. MedIA, 23(1):70–83,\n2015.\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving?\nthe KITTI vision benchmark suite. In Proc. IEEE/CVF CVPR, pages 3354–3361, 2012.\nAli Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang\nXu. Swin UNETR: Swin transformers for semantic segmentation of brain tumors in mri\nimages. arXiv preprint arXiv:2201.01266, 2022a.\nAli Hatamizadeh et al. UNETR: Transformers for 3d medical image segmentation. In Proc.\nIEEE/CVF WACV, pages 574–584, 2022b.\n92\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nKarl Heinz Hohne, Michael Bomans, Martin Riemer, Rainer Schubert, Ulf Tiede, and\nWerner Lierse. A volume-based anatomical atlas. IEEE CGA, 12(04):73–77, 1992.\nFabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Petersen, and Klaus H Maier-Hein.\nnnU-Net: a self-configuring method for deep learning-based biomedical image segmenta-\ntion. Nature methods, 18(2):203–211, 2021.\nPaul F Jaeger, Simon AA Kohl, Sebastian Bickelhaupt, Fabian Isensee, Tristan Anselm\nKuder, Heinz-Peter Schlemmer, and Klaus H Maier-Hein. Retina U-Net: embarrassingly\nsimple exploitation of segmentation supervision for medical object detection. In Proc\nML4H, pages 171–183. PMLR, 2020.\nYuanfeng Ji, Haotian Bai, Jie Yang, Chongjian Ge, Ye Zhu, Ruimao Zhang, Zhen\nLi, Lingyan Zhang, Wanling Ma, Xiang Wan, et al. AMOS: a large-scale abdomi-\nnal multi-organ benchmark for versatile medical image segmentation. arXiv preprint\narXiv:2206.08023, 2022.\nOscar Jimenez-del Toro, Henning M¨ uller, Markus Krenn, Katharina Gruenberg, Abdel Aziz\nTaha, Marianne Winterstein, Ivan Eggel, Antonio Foncubierta-Rodr´ ıguez, Orcun Goksel,\nAndr´ as Jakab, et al. Cloud-based evaluation of anatomical structure segmentation and\nlandmark detection algorithms: VISCERAL anatomy benchmarks. IEEE TMI, 35(11):\n2459–2475, 2016.\nFeng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-DETR:\nAccelerate DETR training by introducing query denoising. In Proc. IEEE/CVF CVPR,\npages 13619–13627, 2022.\nShujun Liang, Fan Tang, Xia Huang, Kaifan Yang, Tao Zhong, Runyue Hu, Shangqing\nLiu, Xinrui Yuan, and Yu Zhang. Deep-learning-based detection and segmentation of or-\ngans at risk in nasopharyngeal carcinoma computed tomographic images for radiotherapy\nplanning. European radiology, 29(4):1961–1967, 2019.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll´ ar, and C Lawrence Zitnick. Microsoft COCO: common objects in context. In\nProc. ECCV, pages 740–755, 2014.\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ ar. Focal loss for\ndense object detection. In Proc. IEEE ICCV, pages 2980–2988, 2017.\nShilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei\nZhang. DAB-DETR: Dynamic anchor boxes are better queries for DETR. arXiv preprint\narXiv:2201.12329, 2022a.\nZe Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video\nSwin transformer. In Proc. IEEE/CVF CVPR, pages 3202–3211, 2022b.\nGabriel Efrain Humpire Mamani, Arnaud Arindra Adiyoso Setio, Bram van Ginneken, and\nColin Jacobs. Organ detection in thorax abdomen CT using multi-label convolutional\nneural networks. In Proc Medical Imaging: Computer-Aided Diagnosis, pages 287–292,\n2017.\n93\nWittmann, Navarro, Shit, and Menze\nDepu Meng, Xiaokang Chen, Zejia Fan, Gang Zeng, Houqiang Li, Yuhui Yuan, Lei Sun, and\nJingdong Wang. Conditional DETR for fast training convergence. In Proc. IEEE/CVF\nCVPR, pages 3651–3660, 2021.\nFernando Navarro, Suprosanna Shit, Ivan Ezhov, Johannes Paetzold, Andrei Gafita, Jan C\nPeeken, Stephanie E Combs, and Bjoern H Menze. Shape-aware complementary-task\nlearning for multi-organ segmentation. In Proc. MLMI, pages 620–627, 2019.\nFernando Navarro, Christopher Watanabe, Suprosanna Shit, Anjany Sekuboyina, Jan C\nPeeken, Stephanie E Combs, and Bjoern H Menze. Evaluating the robustness of self-\nsupervised learning in medical imaging. arXiv preprint arXiv:2105.06986, 2021.\nFernando Navarro, Guido Sasahara, Suprosanna Shit, Ivan Ezhov, Jan C Peeken,\nStephanie E Combs, and Bjoern H Menze. A unified 3D framework for organs at\nrisk localization and segmentation for radiation therapy planning. arXiv preprint\narXiv:2203.00624, 2022.\nVaclav Potesil, Timor Kadir, and Michael Brady. Learning new parts for landmark local-\nization in whole-body CT scans. IEEE TMI, 33(4):836–848, 2013.\nTim Prangemeier, Christoph Reich, and Heinz Koeppl. Attention-based transformers for\ninstance segmentation of cells in microstructures. In Proc. IEEE BIBM, pages 700–707,\n2020.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time\nobject detection with region proposal networks. In Proc. NeurIPS, pages 91–99, 2015.\nPrasad N Samarakoon, Emmanuel Promayon, and Celine Fouard. Light random regression\nforests for automatic multi-organ localization in CT images. In Proc IEEE ISBI, pages\n371–374, 2017.\nOliver Schoppe, Chenchen Pan, Javier Coronel, Hongcheng Mai, Zhouyi Rong, Mihail Ivili-\nnov Todorov, Annemarie M¨ uskes, Fernando Navarro, Hongwei Li, Ali Ert¨ urk, et al. Deep\nlearning-enabled multi-organ segmentation in whole-body mouse scans. Nature commu-\nnications, 11(1):1–14, 2020.\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi\nParikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via\ngradient-based localization. In Proc. IEEE ICCV, pages 618–626, 2017.\nShuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li,\nand Jian Sun. Objects365: A large-scale, high-quality dataset for object detection. In\nProc. IEEE/CVF CVPR, pages 8430–8439, 2019.\nZhiqiang Shen, Rongda Fu, Chaonan Lin, and Shaohua Zheng. COTR: convolution in\ntransformer network for end to end polyp detection. In Proc. INFOCOM, pages 1757–\n1761, 2021.\n94\nFocused Decoding Enables 3D Anatomical Detection by Transformers\nSuprosanna Shit, Rajat Koner, Bastian Wittmann, Johannes Paetzold, Ivan Ezhov, Hong-\nwei Li, Jiazhen Pan, Sahand Sharifzadeh, Georgios Kaissis, Volker Tresp, et al. Re-\nlationformer: A unified framework for image-to-graph generation. arXiv preprint\narXiv:2203.10202, 2022.\nRong Tao, Wenyong Liu, and Guoyan Zheng. Spine-transformers: Vertebra labeling and\nsegmentation in arbitrary field-of-view spine CTs via 3D transformers.MedIA, 75:102258,\n2022.\nYubing Tong, Jayaram K Udupa, Dewey Odhner, Caiyun Wu, Stephen J Schuster, and\nDrew A Torigian. Disease quantification on PET/CT images without explicit object\ndelineation. MedIA, 51:169–183, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. InProc. NeurIPS,\npages 5998–6008, 2017.\nBastian Wittmann, Suprosanna Shit, Fernando Navarro, Jan C Peeken, Stephanie E Combs,\nand Bjoern H Menze. SwinFPN: Leveraging vision transformers for 3d organs-at-risk\ndetection. In Proc. MIDL, 2022.\nXuanang Xu, Fugen Zhou, Bo Liu, Dongshan Fu, and Xiangzhi Bai. Efficient multiple organ\nlocalization in CT image using 3D region proposal network. IEEE TMI, 38(8):1885–1898,\n2019.\nZhoubing Xu, Christopher P Lee, Mattias P Heinrich, Marc Modat, Daniel Rueckert, Se-\nbastien Ourselin, Richard G Abramson, and Bennett A Landman. Evaluation of six\nregistration methods for the human abdomen on clinically acquired CT. IEEE TBME,\n63(8):1563–1572, 2016.\nZhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient DETR: improving end-to-end\nobject detector with dense prior. arXiv preprint arXiv:2104.01318, 2021.\nHao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-\nYeung Shum. Dino: DETR with improved denoising anchor boxes for end-to-end object\ndetection. arXiv preprint arXiv:2203.03605, 2022.\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable\nDETR: Deformable transformers for end-to-end object detection. In Proc. ICLR, 2021.\n95",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.816874623298645
    },
    {
      "name": "Decoding methods",
      "score": 0.6848391890525818
    },
    {
      "name": "Transformer",
      "score": 0.6781038045883179
    },
    {
      "name": "Encoder",
      "score": 0.6331303119659424
    },
    {
      "name": "Object detection",
      "score": 0.5535191893577576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.47806957364082336
    },
    {
      "name": "Architecture",
      "score": 0.4666898846626282
    },
    {
      "name": "Computer vision",
      "score": 0.3561822175979614
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.31217873096466064
    },
    {
      "name": "Algorithm",
      "score": 0.11831235885620117
    },
    {
      "name": "Engineering",
      "score": 0.07779750227928162
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}