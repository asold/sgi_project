{
    "title": "Learning to Create and Reuse Words in Open-Vocabulary Neural Language Modeling",
    "url": "https://openalex.org/W2963899393",
    "year": 2017,
    "authors": [
        {
            "id": "https://openalex.org/A2108388156",
            "name": "Kazuya Kawakami",
            "affiliations": [
                "University of Oxford",
                "DeepMind (United Kingdom)"
            ]
        },
        {
            "id": "https://openalex.org/A2107310219",
            "name": "Chris Dyer",
            "affiliations": [
                "DeepMind (United Kingdom)",
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A297118547",
            "name": "Phil Blunsom",
            "affiliations": [
                "DeepMind (United Kingdom)",
                "University of Oxford"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W627498308",
        "https://openalex.org/W2154099718",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2127836646",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W4302375066",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W196214544",
        "https://openalex.org/W2963573053",
        "https://openalex.org/W2126377586",
        "https://openalex.org/W2409027918",
        "https://openalex.org/W1517590677",
        "https://openalex.org/W4293714597",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2963304263",
        "https://openalex.org/W1993378086",
        "https://openalex.org/W2963735467",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W2571859396",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W1975690018",
        "https://openalex.org/W2949563612",
        "https://openalex.org/W2962686221",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2138660131",
        "https://openalex.org/W1810943226"
    ],
    "abstract": "Fixed-vocabulary language models fail to account for one of the most characteristic statistical facts of natural language: the frequent creation and reuse of new word types. Although character-level language models offer a partial solution in that they can create word types not attested in the training corpus, they do not capture the “bursty” distribution of such words. In this paper, we augment a hierarchical LSTM language model that generates sequences of word tokens character by character with a caching mechanism that learns to reuse previously generated words. To validate our model we construct a new open-vocabulary language modeling corpus (the Multilingual Wikipedia Corpus; MWC) from comparable Wikipedia articles in 7 typologically diverse languages and demonstrate the effectiveness of our model across this range of languages.",
    "full_text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1492–1502\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1137\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1492–1502\nVancouver, Canada, July 30 - August 4, 2017.c⃝2017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-1137\nLearning to Create and Reuse Words in\nOpen-Vocabulary Neural Language Modeling\nKazuya Kawakami♠ Chris Dyer♣ Phil Blunsom♠♣\n♠Department of Computer Science, University of Oxford, Oxford, UK\n♣DeepMind, London, UK\n{kazuya.kawakami,phil.blunsom}@cs.ox.ac.uk,cdyer@google.com\nAbstract\nFixed-vocabulary language models fail to\naccount for one of the most characteristic\nstatistical facts of natural language: the fre-\nquent creation and reuse of new word types.\nAlthough character-level language models\noffer a partial solution in that they can cre-\nate word types not attested in the training\ncorpus, they do not capture the “bursty” dis-\ntribution of such words. In this paper, we\naugment a hierarchical LSTM language\nmodel that generates sequences of word to-\nkens character by character with a caching\nmechanism that learns to reuse previously\ngenerated words. To validate our model\nwe construct a new open-vocabulary lan-\nguage modeling corpus (the Multilingual\nWikipedia Corpus; MWC) from compara-\nble Wikipedia articles in 7 typologically\ndiverse languages and demonstrate the ef-\nfectiveness of our model across this range\nof languages.\n1 Introduction\nLanguage modeling is an important problem in nat-\nural language processing with many practical ap-\nplications (translation, speech recognition, spelling\nautocorrection, etc.). Recent advances in neural\nnetworks provide strong representational power\nto language models with distributed representa-\ntions and unbounded dependencies based on recur-\nrent networks (RNNs). However, most language\nmodels operate by generating words by sampling\nfrom a closed vocabulary which is composed of\nthe most frequent words in a corpus. Rare tokens\nare typically replaced by a special token, called\nthe unknown word token, ⟨UNK ⟩. Although ﬁxed-\nvocabulary language models have some important\npractical applications and are appealing models\nfor study, they fail to capture two empirical facts\nabout the distribution of words in natural languages.\nFirst, vocabularies keep growing as the number of\ndocuments in a corpus grows: new words are con-\nstantly being created (Heaps, 1978). Second, rare\nand newly created words often occur in “bursts”,\ni.e., once a new or rare word has been used once in\na document, it is often repeated (Church and Gale,\n1995; Church, 2000).\nThe open-vocabulary problem can be solved\nby dispensing with word-level models in favor\nof models that predict sentences as sequences of\ncharacters (Sutskever et al., 2011; Chung et al.,\n2017). Character-based models are quite success-\nful at learning what (new) word forms look like\n(e.g., they learn a language’s orthographic conven-\ntions that tell us that sustinated is a plausible En-\nglish word and bzoxqir is not) and, when based on\nmodels that learn long-range dependencies such as\nRNNs, they can also be good models of how words\nﬁt together to form sentences.\nHowever, existing character-sequence models\nhave no explicit mechanism for modeling the fact\nthat once a rare word is used, it is likely to be used\nagain. In this paper, we propose an extension to\ncharacter-level language models that enables them\nto reuse previously generated tokens (§2). Our\nstarting point is a hierarchical LSTM that has been\npreviously used for modeling sentences (word by\nword) in a conversation (Sordoni et al., 2015), ex-\ncept here we model words (character by character)\nin a sentence. To this model, we add a caching\nmechanism similar to recent proposals for caching\nthat have been advocated for closed-vocabulary\nmodels (Merity et al., 2017; Grave et al., 2017).\nAs word tokens are generated, they are placed in\nan LRU cache, and, at each time step the model\ndecides whether to copy a previously generated\nword from the cache or to generate it from scratch,\ncharacter by character. The decision of whether\n1492\nto use the cache or not is a latent variable that\nis marginalised during learning and inference. In\nsummary, our model has three properties: it creates\nnew words, it accounts for their burstiness using\na cache, and, being based on LSTM s over word\nrepresentations, it can model long range dependen-\ncies.\nTo evaluate our model, we perform ablation ex-\nperiments with variants of our model without the\ncache or hierarchical structure. In addition to stan-\ndard English data sets (PTB and WikiText-2), we\nintroduce a new multilingual data set: the Multi-\nlingual Wikipedia Corpus (MWC), which is con-\nstructed from comparable articles from Wikipedia\nin 7 typologically diverse languages (§3) and show\nthe effectiveness of our model in all languages (§4).\nBy looking at the posterior probabilities of the gen-\neration mechanism (language model vs. cache) on\nheld-out data, we ﬁnd that the cache is used to gen-\nerate “bursty” word types such as proper names,\nwhile numbers and generic content words are gen-\nerated preferentially from the language model (§5).\n2 Model\nIn this section, we describe our hierarchical char-\nacter language model with a word cache. As is\ntypical for RNN language models, our model uses\nthe chain rule to decompose the problem into incre-\nmental predictions of the next word conditioned on\nthe history:\np(w) =\n|w|∏\nt=1\np(wt |w<t).\nWe make two modiﬁcations to the traditional\nRNN language model, which we describe in turn.\nFirst, we begin with a cache-less model we call\nthe hierarchical character language model (HCLM;\n§2.1) which generates words as a sequence of char-\nacters and constructs a “word embedding” by en-\ncoding a character sequence with an LSTM (Ling\net al., 2015). However, like conventional closed-\nvocabulary, word-based models, it is based on an\nLSTM that conditions on words represented by\nﬁxed-length vectors.1\nThe HCLM has no mechanism to reuse words\nthat it has previously generated, so new forms will\n1The HCLM is an adaptation of the hierarchical recurrent\nencoder-decoder of Sordoni et al. (2015) which was used\nto model dialog as a sequence of actions sentences which\nare themselves sequences of words. The original model was\nproposed to compose words into query sequences but we use\nit to compose characters into word sequences.\nonly be repeated with very low probability. How-\never, since the HCLM is not merely generating\nsentences as a sequence of characters, but also seg-\nmenting them into words, we may add a word-\nbased cache to which we add words keyed by the\nhidden state being used to generate them (§2.2).\nThis cache mechanism is similar to the model pro-\nposed by Merity et al. (2017).\nNotation. Our model assigns probabilities to se-\nquences of words w = w1,...,w |w|, where |w|is\nthe length, and where each word wi is represented\nby a sequence of characters ci = ci,1,...,c i,|ci|of\nlength |ci|.\n2.1 Hierarchical Character-level Language\nModel (HCLM)\nThis hierarchical model satisﬁes our linguistic intu-\nition that written language has (at least) two differ-\nent units, characters and words.\nThe HCLM consists of four components, three\nLSTMs (Hochreiter and Schmidhuber, 1997):\na character encoder, a word-level context en-\ncoder, and a character decoder (denoted LSTMenc,\nLSTMctx, and LSTMdec, respectively), and a soft-\nmax output layer over the character vocabulary.\nFig. 1 illustrates an unrolled HCLM.\nSuppose the model reads word wt−1 and pre-\ndicts the next word wt. First, the model reads the\ncharacter sequence representing the word wt−1 =\nct−1,1,...,c t−1,|ct−1| where |ct−1|is the length\nof the word generated at time t −1 in charac-\nters. Each character is represented as a vector\nvct−1,1 ,..., vct−1,|ct−1| and fed into the encoder\nLSTMenc . The ﬁnal hidden state of the encoder\nLSTMenc is used as the vector representation of\nthe previously generated word wt−1,\nhenc\nt = LSTMenc(vct−1,1 ,..., vct−1,|ct|).\nThen all the vector representations of words\n(vw1 ,..., vw|w|) are processed with a context\nLSTMctx . Each of the hidden states of the con-\ntext LSTMctx are considered representations of the\nhistory of the word sequence.\nhctx\nt = LSTMctx(henc\n1 ,..., henc\nt )\nFinally, the initial state of the decoder LSTM\nis set to be hctx\nt and the decoder LSTM reads a\nvector representation of the start symbol v⟨S⟩and\ngenerates the next word wt+1 character by charac-\nter. To predict the j-th character in wt, the decoder\n1493\nP  o  k  é  m  o  n  </s> \nThe Pokémon Company International (formerly Pokémon  USA Inc.), a subsidiary of Japan's Pokémon Co., oversees all Pokémon licensing …\nC  o  m  p  a  n  y  </s> …. (      f     o     r    m    e    r    l    y    </s> \nCache rt\n. . . . . . . .\n<s> P o k é m o n\nP o k é m o n </s> \nhenc \nt\nhctx \nt\nwt−1\nwt\np(Pok´emo() = λtplm (Pok´emo() + (1− λt)pptr (Pok´emo() \nut\nλtpptr (Pok´emo() plm (Pok´emo() \nFigure 1: Description of Hierarchical Character Language Model with Cache.\nLSTM reads vector representations of the previous\ncharacters in the word, conditioned on the context\nvector hctx\nt and a start symbol.\nhdec\nt,j = LSTMdec(vct,1 ,..., vct,j−1 ,hctx\nt ,v⟨S⟩).\nThe character generation probability is deﬁned\nby a softmax layer for the corresponding hidden\nrepresentation of the decoder LSTM .\np(ct,j |w<t,ct,<j) = softmax(Wdechdec\nt,j + bdec)\nThus, a word generation probability from\nHCLM is deﬁned as follows.\nplm(wt |w<t) =\n|ct|∏\nj=1\np(ct,j |w<t,ct,<j)\n2.2 Continuous cache component\nThe cache component is an external memory struc-\nture which store Kelements of recent history. Sim-\nilarly to the memory structure used in Grave et al.\n(2017), a word is added to a key-value memory\nafter each generation of wt. The key at position\ni∈[1,K] is ki and its value mi. The memory slot\nis chosen as follows: if the wt exists already in the\nmemory, its key is updated (discussed below). Oth-\nerwise, if the memory is not full, an empty slot is\nchosen or the least recently used slot is overwritten.\nWhen writing a new word to memory, the key is\nthe RNN representation that was used to generate\nthe word (ht) and the value is the word itself (wt).\nIn the case when the word already exists in the\ncache at some position i, the ki is updated to be\nthe arithmetic average of ht and the existing ki.\nTo deﬁne the copy probability from the cache\nat time t, a distribution over copy sites is deﬁned\nusing the attention mechanism of Bahdanau et al.\n(2015). To do so, we construct a query vector (rt)\nfrom the RNN’s current hidden stateht,\nrt = tanh(Wqht + bq),\nthen, for each element iof the cache, a ‘copy score,’\nui,t is computed,\nui,t = vTtanh(Wuki + rt).\nFinally, the probability of generating a word via\nthe copying mechanism is:\npmem(i|ht) = softmaxi(ut)\npptr(wt |ht) =pmem(i|ht)[mi = wt],\nwhere [mi = wt] is 1 if the ith value in memory\nis wt and 0 otherwise. Since pmem deﬁnes a distri-\nbution of slots in the cache, pptr translates it into\nword space.\n2.3 Character-level Neural Cache Language\nModel\nThe word probability p(wt |w<t) is deﬁned as a\nmixture of the following two probabilities. The ﬁrst\n1494\none is a language model probability, plm(wt |w<t)\nand the other is pointer probability ,pptr(wt |w<t).\nThe ﬁnal probability p(wt |w<t) is\nλtplm(wt |w<t) + (1−λt)pptr(wt |w<t),\nwhere λt is computed by a multi-layer perceptron\nwith two non-linear transformations using ht as its\ninput, followed by a transformation by the logistic\nsigmoid function:\nγt = MLP(ht), λ t = 1\n1 −e−γt\n.\nWe remark that Grave et al. (2017) use a clever trick\nto estimate the probability, λt of drawing from the\nLM by augmenting their (closed) vocabulary with\na special symbol indicating that a copy should be\nused. This enables word types that are highly pre-\ndictive in context to compete with the probability of\na copy event. However, since we are working with\nan open vocabulary, this strategy is unavailable in\nour model, so we use the MLP formulation.\n2.4 Training objective\nThe model parameters as well as the character pro-\njection parameters are jointly trained by maximiz-\ning the following log likelihood of the observed\ncharacters in the training corpus,\nL= −\n∑\nlog p(wt |w<t).\n3 Datasets\nWe evaluate our model on a range of datasets, em-\nploying preexisting benchmarks for comparison to\nprevious published results, and a new multilingual\ncorpus which speciﬁcally tests our model’s perfor-\nmance across a range of typological settings.\n3.1 Penn Tree Bank (PTB)\nWe evaluate our model on the Penn Tree Bank.\nFor fair comparison with previous works, we fol-\nlowed the standard preprocessing method used\nby Mikolov et al. (2010). In the standard prepro-\ncessing, tokenization is applied, words are lower-\ncased, and punctuation is removed. Also, less fre-\nquent words are replaced by unknown an token\n(UNK),2 constraining the word vocabulary size to\nbe 10k. Because of this preprocessing, we do not\nexpect this dataset to beneﬁt from the modeling\ninnovations we have introduced in the paper. Fig.1\nsummarizes the corpus statistics.\n2When the unknown token is used in character-level model,\nit is treated as if it were a normal word (i.e. UNK is the\nTrain Dev Test\nCharacter types 50 50 48\nWord types 10000 6022 6049\nOOV rate - 0.00% 0.00%\nWord tokens 0.9M 0.1M 0.1M\nCharacters 5.1M 0.4M 0.4M\nTable 1: PTB Corpus Statistics.\n3.2 WikiText-2\nMerity et al. (2017) proposed the WikiText-2 Cor-\npus as a new benchmark dataset.3 They pointed out\nthat the preprocessed PTB is unrealistic for real lan-\nguage use in terms of word distribution. Since the\nvocabulary size is ﬁxed to 10k, the word frequency\ndoes not exhibit a long tail. The wikiText-2 corpus\nis constructed from 720 articles. They provided\ntwo versions. The version for word level language\nmodeling was preprocessed by discarding infre-\nquent words. But, for character-level models, they\nprovided raw documents without any removal of\nword or character types or lowercasing, but with\ntokenization. We make one change to this corpus:\nsince Wikipedia articles make extensive use of char-\nacters from other languages; we replaced character\ntypes that occur fewer than 25 times were replaced\nwith a dummy character (this plays the role of the\n⟨UNK ⟩token in the character vocabulary). Tab. 2\nsummarizes the corpus statistics.\nTrain Dev Test\nCharacter types 255 128 138\nWord types 76137 19813 21109\nOOV rate - 4.79% 5.87%\nWord tokens 2.1M 0.2M 0.2M\nCharacters 10.9M 1.1M 1.3M\nTable 2: WikiText-2 Corpus Statistics.\n3.3 Multilingual Wikipedia Corpus (MWC)\nLanguages differ in what word formation processes\nthey have. For character-level modeling it is there-\nfore interesting to compare a model’s performance\nsequence U, N, and K). This is somewhat surprising modeling\nchoice, but it has become conventional (Chung et al., 2017).\n3http://metamind.io/research/the-\nwikitext-long-term-dependency-language-\nmodeling-dataset/\n1495\nacross languages. Since there is at present no stan-\ndard multilingual language modeling dataset, we\ncreated a new dataset, the Multilingual Wikipedia\nCorpus (MWC), a corpus of the same Wikipedia\narticles in 7 languages which manifest a range of\nmorphological typologies. The MWC contains En-\nglish (EN), French (FR), Spanish (ES), German\n(DE), Russian (RU), Czech (CS), and Finnish (FI).\nTo attempt to control for topic divergences across\nlanguages, every language’s data consists of the\nsame articles. Although these are only comparable\n(rather than true translations), this ensures that the\ncorpus has a stable topic proﬁle across languages.4\nConstruction & Preprocessing We constructed\nthe MWC similarly to the WikiText-2 corpus. Arti-\ncles were selected from Wikipedia in the 7 target\nlanguages. To keep the topic distribution to be\napproximately the same across the corpora, we ex-\ntracted articles about entities which explained in\nall the languages. We extracted articles which ex-\nist in all languages and each consist of more than\n1,000 words, for a total of 797 articles. These cross-\nlingual articles are, of course, not usually transla-\ntions, but they tend to be comparable. This ﬁltering\nensures that the topic proﬁle in each language is\nsimilar. Each language corpus is approximately the\nsame size as the WikiText-2 corpus.\nWikipedia markup was removed with WikiEx-\ntractor,5 to obtain plain text. We used the\nsame thresholds to remove rare characters in the\nWikiText-2 corpus. No tokenization or other nor-\nmalization (e.g., lowercasing) was done.\nStatistics After the preprocessing described\nabove, we randomly sampled 360 articles. The\narticles are split into 300, 30, 30 sets and the ﬁrst\n300 articles are used for training and the rest are\nused for dev and test respectively. Table 3 summa-\nrizes the corpus statistics.\nAdditionally, we show in Fig. 2 the distribution\nof frequencies of OOV word types (relative to the\ntraining set) in the dev+test portions of the corpus,\nwhich shows a power-law distribution, which is\nexpected for the burstiness of rare words found in\nprior work. Curves look similar for all languages\n(see Appendix A).\n4The Multilingual Wikipedia Corpus (MWC) is avail-\nable for download from http://k-kawakami.com/\nresearch/mwc\n5https://github.com/attardi/\nwikiextractor\nFigure 2: Histogram of OOV word frequencies in\nthe dev+test part of the MWC Corpus (EN).\n4 Experiments\nWe now turn to a series of experiments to show\nthe value of our hierarchical character-level cache\nlanguage model. For each dataset we trained the\nmodel with LSTM units. To compare our results\nwith a strong baseline, we also train a model with-\nout the cache.\nModel Conﬁguration For HCLM and HCLM\nwith cache models, We used 600 dimensions for\nthe character embeddings and theLSTMs have 600\nhidden units for all the experiments. This keeps the\nmodel complexity to be approximately the same as\nprevious works which used an LSTM with 1000\ndimension. Our baseline LSTM have 1000 dimen-\nsions for embeddings and reccurence weights.\nFor the cache model, we used cache size 100\nin every experiment. All the parameters includ-\ning character projection parameters are randomly\nsampled from uniform distribution from −0.08\nto 0.08. The initial hidden and memory state of\nLSTMenc and LSTMctx are initialized with zero.\nMini-batches of size 25 are used for PTB experi-\nments and 10 for WikiText-2, due to memory lim-\nitations. The sequences were truncated with 35\nwords. Then the words are decomposed to charac-\nters and fed into the model. A Dropout rate of 0.5\nwas used for all but the recurrent connections.\nLearning The models were trained with the\nAdam update rule (Kingma and Ba, 2015) with\na learning rate of 0.002. The maximum norm of\nthe gradients was clipped at 10.\nEvaluation We evaluated our models with bits-\nper-character (bpc) a standard evaluation metric\n1496\nChar. Types Word Types OOV rate Tokens Characters\nTrain Valid Test Train Valid Test Valid Test Train Valid Test Train Valid Test\nEN 307 160 157 193808 38826 35093 6.60% 5.46% 2.5M 0.2M 0.2M 15.6M 1.5M 1.3M\nFR 272 141 155 166354 34991 38323 6.70% 6.96% 2.0M 0.2M 0.2M 12.4M 1.3M 1.6M\nDE 298 162 183 238703 40848 41962 7.07% 7.01% 1.9M 0.2M 0.2M 13.6M 1.2M 1.3M\nES 307 164 176 160574 31358 34999 6.61% 7.35% 1.8M 0.2M 0.2M 11.0M 1.0M 1.3M\nCS 238 128 144 167886 23959 29638 5.06% 6.44% 0.9M 0.1M 0.1M 6.1M 0.4M 0.5M\nFI 246 123 135 190595 32899 31109 8.33% 7.39% 0.7M 0.1M 0.1M 6.4M 0.7M 0.6M\nRU 273 184 196 236834 46663 44772 7.76% 7.20% 1.3M 0.1M 0.1M 9.3M 1.0M 0.9M\nTable 3: Summary of MWC Corpus.\nfor character-level language models. Following the\ndeﬁnition in Graves (2013), bits-per-character is\nthe average value of −log2 p(wt |w<t) over the\nwhole test set,\nbpc = −1\n|c|log2 p(w),\nwhere |c|is the length of the corpus in characters.\n4.1 Results\nPTB Tab. 4 summarizes results on the PTB\ndataset.6 Our baseline HCLM model achieved\n1.276 bpc which is better performance than the\nLSTM with Zoneout regularization (Krueger et al.,\n2017). And HCLM with cache outperformed the\nbaseline model with 1.247 bpc and achieved com-\npetitive results with state-of-the-art models with\nregularization on recurrence weights, which was\nnot used in our experiments.\nExpressed in terms of per-word perplexity (i.e.,\nrather than normalizing by the length of the corpus\nin characters, we normalize by words and expo-\nnentiate), the test perplexity on HCLM with cache\nis 94.79. The performance of the unregularized\n2-layer LSTM with 1000 hidden units on word-\nlevel PTB dataset is 114.5 and the same model\nwith dropout achieved 87.0. Considering the fact\nthat our character-level models are dealing with\nan open vocabulary without unknown tokens, the\nresults are promising.\nWikiText-2 Tab. 5 summarizes results on the\nWikiText-2 dataset. Our baseline, LSTM achieved\n1.803 bpc and HCLM model achieved 1.670 bpc.\nThe HCLM with cache outperformed the baseline\nmodels and achieved 1.500 bpc. The word level\nperplexity is 227.30, which is quite high compared\nto the reported word level baseline result 100.9\n6Models designated with a * have more layers and more\nparameters.\nMethod Dev Test\nCW-RNN (Koutnik et al., 2014) - 1.46\nHF-MRNN (Mikolov et al., 2012) - 1.41\nMI-RNN (Wu et al., 2016) - 1.39\nME n-gram (Mikolov et al., 2012) - 1.37\nRBN (Cooijmans et al., 2017) 1.281 1.32\nRecurrent Dropout (Semeniuta et al., 2016) 1.338 1.301\nZoneout (Krueger et al., 2017) 1.362 1.297\nHM-LSTM (Chung et al., 2017) - 1.27\nHyperNetwork (Ha et al., 2017) 1.296 1.265\nLayerNorm HyperNetwork (Ha et al., 2017) 1.281 1.250\n2-LayerNorm HyperLSTM (Ha et al., 2017)* - 1.219\n2-Layer with New Cell (Zoph and Le, 2016)* - 1.214\nLSTM (Our Implementation) 1.369 1.331\nHCLM 1.308 1.276\nHCLM with Cache 1.266 1.247\nTable 4: Results on PTB Corpus (bits-per-\ncharacter). HCLM augmented with a cache obtains\nthe best results among models which have approx-\nimately the same numbers of parameter as single\nlayer LSTM with 1,000 hidden units.\nwith LSTM with ZoneOut and Variational Dropout\nregularization (Merity et al., 2017). However, the\ncharacter-level model is dealing with 76,136 types\nin training set and 5.87% OOV rate where the word\nlevel models only use 33,278 types without OOV\nin test set. The improvement rate over the HCLM\nbaseline is 10.2% which is much higher than the\nimprovement rate obtained in the PTB experiment.\nMethod Dev Test\nLSTM 1.758 1.803\nHCLM 1.625 1.670\nHCLM with Cache 1.480 1.500\nTable 5: Results on WikiText-2 Corpus .\nMultilingual Wikipedia Corpus (MWC)\nTab. 6 summarizes results on the MWC dataset.\nSimilarly to WikiText-2 experiments, LSTM\n1497\nis strong baseline. We observe that the cache\nmechanism improve performance in every lan-\nguages. In English, HCLM with cache achieved\n1.538 bpc where the baseline is 1.622 bpc. It\nis 5.2% improvement. For other languages,\nthe improvement rates were 2.7%, 3.2%, 3.7%,\n2.5%, 4.7%, 2.7% in FR, DE, ES, CS, FI, RU\nrespectively. The best improvement rate was\nobtained in Finnish.\n5 Analysis\nIn this section, we analyse the behavior of proposed\nmodel qualitatively. To analyse the model, we com-\npute the following posterior probability which tell\nwhether the model used the cache given a word and\nits preceding context. Let zt be a random variable\nthat says whether to use the cache or the LM to gen-\nerate the word at time t. We would like to know,\ngiven the text w, whether the cache was used at\ntime t. This can be computed as follows:\np(zt |w) =p(zt,wt |ht,cachet)\np(wt |ht,cachet)\n= (1 −λt)pptr(wt |ht,cachet)\np(wt |ht,cachet) ,\nwhere cachet is the state of the cache at time t. We\nreport the average posterior probability of cache\ngeneration excluding the ﬁrst occurrence of w,\np(z|w).\nTab. 7 shows the words in the WikiText-2 test\nset that occur more than 1 time that are most/least\nlikely to be generated from cache and character\nlanguage model (words that occur only one time\ncannot be cache-generated). We see that the model\nuses the cache for proper nouns: Lesnar, Gore, etc.,\nas well as very frequent words which always stored\nsomewhere in the cache such as single-token punc-\ntuation, the, and of. In contrast, the model uses the\nlanguage model to generate numbers (which tend\nnot to be repeated): 300, 770 and basic content\nwords: sounds, however, unable, etc. This pattern\nis similar to the pattern found in empirical distri-\nbution of frequencies of rare words observed in\nprior wors (Church and Gale, 1995; Church, 2000),\nwhich suggests our model is learning to use the\ncache to account for bursts of rare words.\nTo look more closely at rare words, we also in-\nvestigate how the model handles words that oc-\ncurred between 2 and 100 times in the test set, but\nfewer than 5 times in the training set. Fig. 3 is a\nscatter plot of p(z|w) vs the empirical frequency\nin the test set. As expected, more frequently re-\npeated words types are increasingly likely to be\ndrawn from the cache, but less frequent words show\na range of cache generation probabilities.\nFigure 3: Average p(z |w) of OOV words in test\nset vs. term frequency in the test set for words\nnot obsered in the training set. The model prefers\nto copy frequently reused words from cache com-\nponent, which tend to names (upper right) while\ncharacter level generation is used for infrequent\nopen class words (bottom left).\nTab. 8 shows word types with the highest and\nlowest average p(z | w) that occur fewer than\n5 times in the training corpus. The pattern here\nis similar to the unﬁltered list: proper nouns are\nextremely likely to have been cache-generated,\nwhereas numbers and generic (albeit infrequent)\ncontent words are less likely to have been.\n6 Discussion\nOur results show that the HCLM outperforms a\nbasic LSTM. With the addition of the caching\nmechanism, the HCLM becomes consistently more\npowerful than both the baseline HCLM and the\nLSTM. This is true even on the PTB, which\nhas no rare or OOV words in its test set (because\nof preprocessing), by caching repetitive common\nwords such as the. In true open-vocabulary settings\n(i.e., WikiText-2 and MWC), the improvements are\nmuch more pronounced, as expected.\nComputational complexity. In comparison with\nword-level models, our model has to read and gen-\nerate each word character by character, and it also\nrequires a softmax over the entire memory at ev-\nery time step. However, the computation is still\nlinear in terms of the length of the sequence, and\nthe softmax over the memory cells and character\n1498\nEN FR DE ES CS FI RU\ndev test dev test dev test dev test dev test dev test dev test\nLSTM 1.793 1.736 1.669 1.621 1.780 1.754 1.733 1.667 2.191 2.155 1.943 1.913 1.942 1.932\nHCLM 1.683 1.622 1.553 1.508 1.666 1.641 1.617 1.555 2.070 2.035 1.832 1.796 1.832 1.810\nHCLM with Cache 1.591 1.538 1.499 1.467 1.605 1.588 1.548 1.498 2.010 1.984 1.754 1.711 1.777 1.761\nTable 6: Results on MWC Corpus (bits-per-character).\nWord p(z | w) ↓ Word p(z | w) ↑\n. 0.997 300 0.000\nLesnar 0.991 act 0.001\nthe 0.988 however 0.002\nNY 0.985 770 0.003\nGore 0.977 put 0.003\nBintulu 0.976 sounds 0.004\nNerva 0.976 instead 0.005\n, 0.974 440 0.005\nUB 0.972 similar 0.006\nNero 0.967 27 0.009\nOsbert 0.967 help 0.009\nKershaw 0.962 few 0.010\nManila 0.962 110 0.010\nBoulter 0.958 Jersey 0.011\nStevens 0.956 even 0.011\nRifenburg 0.952 y 0.012\nArjona 0.952 though 0.012\nof 0.945 becoming 0.013\n31B 0.941 An 0.013\nOlympics 0.941 unable 0.014\nTable 7: Word types with the highest/lowest av-\nerage posterior probability of having been copied\nfrom the cache while generating the test set. The\nprobability tells whether the model used the cache\ngiven a word and its context. Left: Cache is\nused for frequent words (the, of ) and proper nouns\n(Lesnar, Gore). Right: Character level generation\nis used for basic words and numbers.\nvocabulary are much smaller than word-level vo-\ncabulary. On the other hand, since the recurrent\nstates are updated once per character (rather than\nper word) in our model, the distribution of opera-\ntions is quite different. Depending on the hardware\nsupport for these operations (repeated updates of\nrecurrent states vs. softmaxes), our model may be\nfaster or slower. However, our model will have\nfewer parameters than a word-based model since\nmost of the parameters in such models live in the\nword projection layers, and we use LSTMs in place\nof these.\nNon-English languages. For non-English lan-\nguages, the pattern is largely similar for non-\nEnglish languages. This is not surprising since\nmorphological processes may generate forms that\nare related to existing forms, but these still have\nWord p(z | w) ↓ Word p(z | w) ↑\nGore 0.977 770 0.003\nNero 0.967 246 0.037\nOsbert 0.967 Lo 0.074\nKershaw 0.962 Pitcher 0.142\n31B 0.941 Poets 0.143\nKirby 0.935 popes 0.143\nCR 0.926 Yap 0.143\nSM 0.924 Piso 0.143\nimpedance 0.923 consul 0.143\nBlockbuster 0.900 heavyweight 0.143\nSuperfamily 0.900 cheeks 0.154\nAmos 0.900 loser 0.164\nSteiner 0.897 amphibian 0.167\nBacon 0.893 squads 0.167\nﬁlters 0.889 los 0.167\nLim 0.889 Keenan 0.167\nSelfridge 0.875 sculptors 0.167\nﬁlter 0.875 Gen. 0.167\nLockport 0.867 Kipling 0.167\nGermaniawerft 0.857 Tabasco 0.167\nTable 8: Same as Table 7, except ﬁltering for word\ntypes that occur fewer than 5 times in the training\nset. The cache component is used as expected even\non rare words: proper nouns are extremely likely\nto have been cache-generated, whereas numbers\nand generic content words are less likely to have\nbeen; this indicates both the effectiveness of the\nprior at determining whether to use the cache and\nthe burstiness of proper nouns.\nslight variations. Thus, they must be generated by\nthe language model component (rather than from\nthe cache). Still, the cache demonstrates consistent\nvalue in these languages.\nFinally, our analysis of the cache on English\ndoes show that it is being used to model word\nreuse, particularly of proper names, but also of\nfrequent words. While empirical analysis of rare\nword distributions predicts that names would be\nreused, the fact that cache is used to model frequent\nwords suggests that effective models of language\nshould have a means to generate common words as\nunits. Finally, our model disfavors copying num-\nbers from the cache, even when they are available.\nThis suggests that it has learnt that numbers are not\ngenerally repeated (in contrast to names).\n1499\n7 Related Work\nCaching language models were proposed to ac-\ncount for burstiness by Kuhn and De Mori (1990),\nand recently, this idea has been incorporated to\naugment neural language models with a caching\nmechanism (Merity et al., 2017; Grave et al., 2017).\nOpen vocabulary neural language models have\nbeen widely explored (Sutskever et al., 2011;\nMikolov et al., 2012; Graves, 2013, inter alia). At-\ntempts to make them more aware of word-level\ndynamics, using models similar to our hierarchical\nformulation, have also been proposed (Chung et al.,\n2017).\nThe only models that are open vocabulary lan-\nguage modeling together with a caching mech-\nanism are the nonparametric Bayesian language\nmodels based on hierarchical Pitman–Yor pro-\ncesses which generate a lexicon of word types us-\ning a character model, and then generate a text\nusing these (Teh, 2006; Goldwater et al., 2009;\nChahuneau et al., 2013). These, however, do not\nuse distributed representations on RNNs to capture\nlong-range dependencies.\n8 Conclusion\nIn this paper, we proposed a character-level lan-\nguage model with an adaptive cache which selec-\ntively assign word probability from past history\nor character-level decoding. And we empirically\nshow that our model efﬁciently model the word\nsequences and achieved better perplexity in every\nstandard dataset. To further validate the perfor-\nmance of our model on different languages, we\ncollected multilingual wikipedia corpus for 7 typo-\nlogically diverse languages. We also show that our\nmodel performs better than character-level models\nby modeling burstiness of words in local context.\nThe model proposed in this paper assumes the\nobservation of word segmentation. Thus, the model\nis not directly applicable to languages, such as Chi-\nnese and Japanese, where word segments are not\nexplicitly observable. We will investigate a model\nwhich can marginalise word segmentation as latent\nvariables in the future work.\nAcknowledgements\nWe thank the three anonymous reviewers for their\nvaluable feedback. The third author acknowledges\nthe support of the EPSRC and nvidia Corporation.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. ICLR.\nVictor Chahuneau, Noah A. Smith, and Chris Dyer.\n2013. Knowledge-rich morphological priors for\nbayesian language models. In Proc. NAACL.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2017. Hierarchical multiscale recurrent neural net-\nworks. In Proc. ICLR.\nKenneth W Church. 2000. Empirical estimates of adap-\ntation: the chance of two Noriegas is closer to p/2\nthan p2. In Proc. COLING.\nKenneth W Church and William A Gale. 1995. Poisson\nmixtures. Natural Language Engineering 1(2):163–\n190.\nTim Cooijmans, Nicolas Ballas, César Laurent, Ça ˘glar\nGülçehre, and Aaron Courville. 2017. Recurrent\nbatch normalization. In Proc. ICLR.\nSharon Goldwater, Thomas L Grifﬁths, and Mark John-\nson. 2009. A Bayesian framework for word segmen-\ntation: Exploring the effects of context. Cognition\n112(1):21–54.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving neural language models with a con-\ntinuous cache. In Proc. ICLR.\nAlex Graves. 2013. Generating sequences with\nrecurrent neural networks. arXiv preprint\narXiv:1308.0850 .\nDavid Ha, Andrew Dai, and Quoc V Le. 2017. Hyper-\nnetworks. In Proc. ICLR.\nHarold Stanley Heaps. 1978. Information retrieval:\nComputational and theoretical aspects . Academic\nPress, Inc.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation 9(8):1735–\n1780.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proc. ICLR.\nJan Koutnik, Klaus Greff, Faustino Gomez, and Juer-\ngen Schmidhuber. 2014. A clockwork RNN. In\nProc. ICML.\nDavid Krueger, Tegan Maharaj, János Kramár, Moham-\nmad Pezeshki, Nicolas Ballas, Nan Rosemary Ke,\nAnirudh Goyal, Yoshua Bengio, Hugo Larochelle,\nAaron Courville, et al. 2017. Zoneout: Regulariz-\ning rnns by randomly preserving hidden activations.\nIn Proc. ICLR.\nRoland Kuhn and Renato De Mori. 1990. A cache-\nbased natural language model for speech recogni-\ntion. IEEE transactions on pattern analysis and ma-\nchine intelligence 12(6):570–583.\n1500\nWang Ling, Tiago Luís, Luís Marujo, Ramón Fernan-\ndez Astudillo, Silvio Amir, Chris Dyer, Alan W\nBlack, and Isabel Trancoso. 2015. Finding function\nin form: Compositional character models for open\nvocabulary word representation. In Proc. EMNLP.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proc. ICLR.\nTomas Mikolov, Martin Karaﬁát, Lukas Burget, Jan\nCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proc. In-\nterspeech.\nTomáš Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSon Le, Stefan Kombrink, and Jan Cernocky.\n2012. Subword language modeling with neu-\nral networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf).\nStanislau Semeniuta, Aliaksei Severyn, and Erhardt\nBarth. 2016. Recurrent dropout without memory\nloss. In Proc. COLING.\nAlessandro Sordoni, Yoshua Bengio, Hossein Vahabi,\nChristina Lioma, Jakob Grue Simonsen, and Jian-\nYun Nie. 2015. A hierarchical recurrent encoder-\ndecoder for generative context-aware query sugges-\ntion. In Proc. CIKM.\nIlya Sutskever, James Martens, and Geoffrey E Hin-\nton. 2011. Generating text with recurrent neural net-\nworks. In Proc. ICML.\nYee Whye Teh. 2006. A hierarchical Bayesian lan-\nguage model based on Pitman-Yor processes. In\nProc. ACL.\nYuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Ben-\ngio, and Ruslan R Salakhutdinov. 2016. On multi-\nplicative integration with recurrent neural networks.\nIn Proc. NIPS.\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578 .\nA Corpus Statistics\nFig. 4 show distribution of frequencies of OOV\nword types in 6 languages.\n1501\nFR DE \nES CS \nFI RU \nFigure 4: Histogram of OOV word frequencies in MWC Corpus in different languages.\n1502"
}