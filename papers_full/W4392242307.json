{
  "title": "INTRUSION DETECTION MODEL BASED ON IMPROVED TRANSFORMER",
  "url": "https://openalex.org/W4392242307",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2777199020",
      "name": "Svitlana Gavrylenko",
      "affiliations": [
        "National Technical University \"Kharkiv Polytechnic Institute\""
      ]
    },
    {
      "id": "https://openalex.org/A5093538214",
      "name": "Vadym Poltoratskyi",
      "affiliations": [
        "National Technical University \"Kharkiv Polytechnic Institute\""
      ]
    },
    {
      "id": "https://openalex.org/A2231374370",
      "name": "Alina Nechyporenko",
      "affiliations": [
        "Technical University of Applied Sciences Wildau"
      ]
    },
    {
      "id": "https://openalex.org/A2777199020",
      "name": "Svitlana Gavrylenko",
      "affiliations": [
        "National Technical University \"Kharkiv Polytechnic Institute\""
      ]
    },
    {
      "id": "https://openalex.org/A5093538214",
      "name": "Vadym Poltoratskyi",
      "affiliations": [
        "National Technical University \"Kharkiv Polytechnic Institute\""
      ]
    },
    {
      "id": "https://openalex.org/A2231374370",
      "name": "Alina Nechyporenko",
      "affiliations": [
        "Technical University of Applied Sciences Wildau"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3100813509",
    "https://openalex.org/W4315431955",
    "https://openalex.org/W2748771977",
    "https://openalex.org/W2100537916",
    "https://openalex.org/W1977838479",
    "https://openalex.org/W3083112336",
    "https://openalex.org/W4226378558",
    "https://openalex.org/W4389334156",
    "https://openalex.org/W2606697812",
    "https://openalex.org/W3160455160",
    "https://openalex.org/W2293634267",
    "https://openalex.org/W3043116777",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4226530256",
    "https://openalex.org/W4389981728"
  ],
  "abstract": "The object of the study is the process of identifying the state of a computer network. The subject of the study are the methods of identifying the state of computer networks. The purpose of the paper is to improve the efficacy of intrusion detection in computer networks by developing a method based on transformer models. The results obtained. The work analyzes traditional machine learning algorithms, deep learning methods and considers the advantages of using transformer models. A method for detecting intrusions in computer networks is proposed. This method differs from known approaches by utilizing the Vision Transformer for Small-size Datasets (ViTSD) deep learning algorithm. The method incorporates procedures to reduce the correlation of input data and transform data into a specific format required for model operations. The developed methods are implemented using Python and the GOOGLE COLAB cloud service with Jupyter Notebook. Conclusions. Experiments confirmed the efficiency of the proposed method. The use of the developed method based on the ViTSD algorithm and the data preprocessing procedure increases the model's accuracy to 98.7%. This makes it possible to recommend it for practical use, in order to improve the accuracy of identifying the state of a computer system.",
  "full_text": "Advanced Information Systems. 2024. Vol. 8, No. 1 ISSN 2522-9052 \n94 \nMethods of information systems protection \nUDC 004.732.056   doi: https://doi.org/10.20998/2522-9052.2024.1.12 \n \nSvitlana Gavrylenko1, Vadym Poltoratskyi 1, Alina Nechyporenko2 \n \n1 National Technical University “Kharkiv Polytechnic Institute”, Kharkiv, Ukraine \n2 Technical University of Applied Sciences Wildau, Wildau, Germany \n \nINTRUSION DETECTION MODEL BASED ON IMPROVED TRANSFORMER \n \nAbstract.  The object of the study is the process of identifying the state of a computer network.  The subject of the study \nare the methods of identifying the state of computer networks. The purpose of the paper is to improve the efficacy of intrusion \ndetection in computer networks by developing a method based on transformer models. The results obtained . The work \nanalyzes traditional machine learning algorithms, deep learning methods and considers the advantages of using transformer \nmodels. A method for detecting intrusions in computer networks is proposed . This method differs from known approaches \nby utilizing the Vision Transformer for Small -size Datasets (ViTSD ) deep learning algorithm. The method incorporates \nprocedures to reduce the correlation of input data and transform data into a specific format required for model operations. \nThe developed methods are implemented using Python and the GOOGLE COLAB cloud ser vice with Jupyter Notebook. \nConclusions. Experiments confirmed the efficiency of the proposed method. The use of the developed method based on the \nViTSD algorithm and the data preprocessing procedure increases the model's accuracy to 98.7%.  This makes it possible to \nrecommend it for practical use, in order to improve the accuracy of identifying the state of a computer system. \nKeywords:  data preprocessing ; machine learning ; neural networks ; RNN; SVM; KNN; transformer models ; vision \ntransformer. \n \nIntroduction \nModern society depends more and more on existing \ncomputer technologies. At the same time, cyber threat \nstatistics show a significant increase in the number of \nintrusions into computer systems and networks. \nAccording to recent statistics, cyber -attacks are \nbecoming more frequent and sophisticated, with an \nestimated 1.4 million new threats emerging every day in \n2023. The cost of a successful cyber -attack can range \nfrom a few hundred dollars to millions of dollars, \ndepending on the type of attack and the targeted \norganization. Some of the most common cyber threats \ninclude malware, phishing scams, ransomware, and data \nbreaches. \nDynamic changes in the rate of appearance of new \nclasses and families of malicious software, methods of \nspreading and destruction of anti -virus systems \ncomplicate the possibility of detecting malicious \nsoftware and do not satisfy the existing requirements for \nthe speed and accuracy of detecting malicious software. \nMoreover, protection of computer systems and \nnetworks from cyber-attacks has become an increasingly \nurgent problem in recent years.  Although improved \nsecurity features have been added to most systems, there \nare still a large number of vulnerabilities, including data \nmodification or destruction, unauthorized access to \nsystems and information, etc.  In this scenario, intrusion \ndetection systems (IDS) play a key role in protecting the \nnetwork and accurately identifying attacks on the system.  \nLiterature review and problem statement.  \nComputer network intrusion detection systems are based \non the use of machine learning (ML) methods  [1, 2 ]. \nTraditional machine learning algorithms such as decision \ntrees [3], Random Forest [4], K nearest neighbors [5], and \nBayesian methods , recommendation system [ 6] and \nNeural Networks [ 7] have been successfully used in \nintrusion detection systems for many years. The different \nensemble methods, based on meta-algorithms boosting, \nbagging and stacking are popular too [8]. However, they \nmay not be sufficient for processing of high-dimensional \nbig data generated by modern networks.  This is because \nthese methods are not always able to effectively extract \nuseful features from the data. Deep learning is becoming \nincreasingly popular in the area of intrusion detection  \nsystem. It has to do with deep learning algorithms are \nbetter suited for processing big data and complex features \nthan traditional machine learning algorithms. Deep \nlearning-based intrusion detection models commonly use \nmodels such as convolutional neural networks  (CNN) \n[9], recurrent neural networks (RNN)  [10], long -short-\nterm memory networks (LSTM)  [11], and generative \nadversarial networks (GAN). \nRecurrent Neural Networks (RNNs) are one of the \nmost efficient methods for processing sequential data  \n[12]. They inclu de a feedback loop and transmit the \ncurrent state of the model for the computation of its \nsubsequent state. During network training, gradients may \nexponentially decrease or increase over time, leading to \neither vanishing or exploding gradients.  This makes it \ndifficult to train the model and update the weights. \nMoreover, for input sequences of sufficient length, \nrecurrent neural networks become inefficient. In such \nmodels, words at the end of the sequence have a greater \ninfluence on the result, since the ini tial information has \nalready passed through recurrent transformations many \ntimes. Since important information can contain both at \nthe beginning and in the middle of a sentence, it is \nimportant to be able to save it until the end of the network \ncalculations. \nTherefore, despite significant progress in intrusion \ndetection using machine learning and deep learning, three \nkey challenges remain: long model training time, \nunbalanced datasets, and the low efficiency of multiclass \n©   Gavrylenko S., Poltoratskyi V., Nechyporenko A., 2024 \nISSN 2522-9052 Сучасні інформаційні системи. 2024. Т. 8, № 1 \n95 \nclassification. Thus, the improvemen t and development \nof new methods for computer network identification are \nrelevant tasks that require further research. In additional, \nthere are many challenges to designing a reliable, \nefficient, and accurate IDS, especially when dealing with \nhigh-dimensional anomalous data with subtle and \nunpredictable attacks. \nThe aim and objectives of the study.  This paper \nscrutinizes the possibility of using ViT and ViTSD \nmodels to detect intrusions into computer networks. \nViTSD models are designed for small datasets, usually \ninclude regularization and data augmentation \nmechanisms, which makes them more robust with a \nlimited number of training examples. \nThe object of the study is the process of identifying \nthe state of the computer system and network. \nThe subject of the study are methods of identifying \nthe state of computer system and networks. \nThe purpose of this study is to develop a method for \ndetecting intrusions in computer networks base d on \nVision Transformer (ViT) and Improved Vision \nTransformer for Small-Size Datasets (ViTSD). \nTo achieve this , it is necessary to complete the \nfollowing tasks: \n− analyse input data and perform their data \npreprocessing; \n− develop a method for increasing the accuracy of \nidentifying the state of computer systems and network \nthrough the use of transformer model; \n− develop program ViT and ViTSD models; \n− analyse an impact of different machine learning \nmethods on the model performance. \nResearch materials and methods \nTo enhance the performance of deep learning-based \nmodels, an attention mechanism has been developed. The \nfundamental idea behind the attention mechanism is to \nallow the model to learn the importa nce of different \ntokens in a sequence based on context, rather than \nassigning a fixed weight to each token. Thus, the model \ncan learn  how to pay attention to words that are more \nsignificant for the result, regardless of their position in \nthe sentence. \nImplementation of the attention mechanism consists \nof the following steps: \n• initially, each element of the input sequence \n(e.g., a word or token) is transformed into a vector \nrepresentation using embedding technology  – mapping \nelements into a continuous space of continuous vectors. \nThen, weights are calculated for each element of the input \nsequence and the importance of each input element for \nthe given context is determined. These weights are called \n\"attention weights\" a nd are calculated based on the \nsimilarity between the current item and all other items in \nthe input sequence; \n• the next step involves normalizing the relevance \nscores so that their sum equals one; \n• then the relevance scores and the corresponding \nvalues of th e input vectors are multiplied. The resulting \nmatrix will have the same dimensions as the original \nmatrix of word embeddings, but the values of the \nembedding vectors will take into account the context in \nwhich the word is situated. The obtained contextual \nvector (Scaled Dot -Product Attention) serves as a \ncompressed representation of the input data, considering \ntheir importance within this context.  This contextual \nvector can be used for further data processing, such as \nclassification, translation, or text generation. \nAlso, the attention mechanism makes it possible to \npartially interpret the decisions made by the neural \nnetwork and understand which parts of the input data are \nmore important. \nIn the models, usually several \"heads of attention\" \nare used, each of  which calculates its own value of \nattention (multi-head attention). This allows the model to \nfocus on different aspects of the data and to detect \ndifferent relationships between sequence elements and to \nhandle sequences of different lengths. \nTransformers are one of the most successful \napplications of the attention mechanism. The key feature \nof the architecture is that recurrent layers have been \ncompletely removed from the network, leaving only \nattention mechanisms and auxiliary layers: data \nnormalization l ayers and fully connected layers. This \nallowed for parallel processing of sequences, handling all \ntokens simultaneously rather than token by token. \nTransformer models consist of two parts: the \nencoder and the decoder (Fig. 1). \n \nFig. 1. Transformer model \n \nThe encoder is responsible for converting the input \ndata into an internal multidimensional representation and \nconsists of several identical blocks (in the original \narchitecture 6). Each block consists of two layers. The \nfirst layer is a  multi-head self-attention mechanism, and \nthe second is an ordinary fully connected layer. Residual \n\nAdvanced Information Systems. 2024. Vol. 8, No. 1 ISSN 2522-9052 \n96 \nconnections [13] are employed around each of the two \nsub-layers, followed by layer normalization [14]. In other \nwords, the output of each sublayer can be found as \nOutput = LayerNorm(x + Sublayer(x)),          (1) \nwhere x is input data, LayerNorm is a data normalization \nfunction, Sublayer( x) is a function implemented by the \nsublayer itself. To facilitate these residual connections, \nall sublayers in the model, as well as embedding layers, \ngenerate outputs of size dmodel = 512. \nThe decoder also consists of several identical \ndecoding units. In addition to the two sublayers in each \nencoder layer, the decoder inserts a third sublayer that \napplies multi-head attention to the output of the encoder \nstack. \nSimilar to the encoder, residual connections around \neach of the sublayers are used, followed by layer \nnormalization. The self-attention sub-layer in the decoder \nstack is also modified to prevent information from future \npositions influencing the current position. This \nmodification, along with shifting the output embeddings \nby one position, ensures that the prediction for position i \ndepends only on the known outputs at positions less than \ni. The internal representation of the encoder and decoder \nlayers is shown in Fig. 2. \n \nFig. 2. Internal representation  \nof the encoder and decoder layers \n \nModel transformers are used in computer network \nintrusion detection systems (Intrusion Detection \nSystems, IDS) to process and analyze network traffic \ndata and identify potential anomalies or intrusions. They \nare effective in analyzing network traffic, including data \npackets, metadata, and logs. Transformer models are able \nto detect such types of attacks as DDoS attacks, SQL \ninjections, buffer overflow attacks, and others. They can \nhelp in segmenting network traffic into different \ncategories such as normal traffic, attacks, port scans, etc. \nThis allows for the analysis of each category separately \nand taking actions accordingly based on the detected \nthreats. In addition, in IDS systems, transformers can be \napplied to analyze text data, such as event logs, and \nidentify pot ential attacks and incidents based on text \ninformation. They are also useful in predicting future \nthreats and intrusions based on analysis of historical data \nand current activity. Model transformers can be trained \nto perform several tasks simultaneously. F or example, \nthey can predict the next network event and \nsimultaneously determine whether the event is an \nanomaly. \nThe use of different types of transformer models \ndepends on the specific requirements and characteristics \nof the task and requires prior adaptation to intrusion \ndetection systems (IDS).  \nSequence-to-Sequence Transformer models are \ntypically used to process  text sequences and can be \nadapted to process network logs or other security-related \ntext data for anomaly detection.  \nTime Series Transformer models, specially \ndesigned for working with time series, can be applied in \nIDS to process time stamps and network  events. \nAttention-based models with attention mechanisms can \nbe useful for detecting unusual patterns in network \ntraffic.  \nThe attention mechanism allows the model to focus \non key elements of the sequence. Transformer \nAutoencoders models can be used to st udy the \nrepresentation of \"normal\" traffic and detect anomalies in \ndeviations from this representation.  \nIf the input data includes different types of \ninformation (for example, text logs and numerical \nmetrics), then Multimodal Transformers can handle them \neffectively. Using pre-trained transformer models such as \nBERT, GPT, etc. can improve training with small \namounts of data when using Transfer Learning with \nTransformers models.  \nIf the structure of the network is important, then the \nuse of Graph Transforme rs models is effective. Such \nmodels work with graphs and can be used to process and \nanalyze network topology.  \nFor image classification, it is effective to use the \nVision Transformer (ViT)  [15] model and its improved \nVision Transformer for Small -Size Datasets (ViTSD)  \n[16] model, which uses the method of image tokenization \nwith shifted patches (Shifted Patch Tokenization, SPT). \nThis allows capturing more spatial relationships between \npixels. \nSince Transformer models are trained on labeled \ndata, including normal network traffic and attack data, \nthey serve as an effective tool for continuous network \nmonitoring and detecting potential intrusions. However, \nthe use of Transformer models requires signif icant \ncomputational power and a large amount of data for \neffective training.  Moreover, the effectiveness of \nTransformer models in intrusion detection systems \ndepends on the nature of the data, the quality of the \ntraining data, and the specific detection task that needs to \nbe addressed. \nDevelopment of an intrusion detection method \nbased on improved transformer \nThis work uses the UNSW-NB 15 data set as input, \nwhich was developed at the Australian Cyber Security \nCenter (ACCS) Cyber Range Laboratory and contain s \ninformation about normal network operation and during \nsynthetic intrusions . UNSW -NB15 has 45 features and \nrepresents nine main groups of attacks generated by the \nIXIA tool: PerfectStorm Analysis, Backdoors, DoS, \nExploits, Fuzzers, Generic, Reconnaissance , Shellcode \nand Worms. \nAn important step in building a model is data \npreprocessing, which can significantly improve its \nperformance. It includes cleaning scaling, balancing and \nfeature extraction. \nThe analysis of the input data founded a significant \nnumber of related features (Fig. 3)[17]. \n\nISSN 2522-9052 Сучасні інформаційні системи. 2024. Т. 8, № 1 \n97 \n \nFig. 3. Correlation matrix \n \nThe presence of correlated features negatively \naffects the quality of the model. However, correlated \nfeatures can contain useful information about the \ndependence between variables. When removing one of \nthem, the model loses access to this information and may \nbecome less informative. To solve this problem, we \npreviously proposed a special procedure [17]. If there are \nfeatures that exhibit a correlation higher than a specified \nthreshold (e.g., 90%), they are processed using Principal \nComponent Analysis (PCA). To achieve this, we create \ndata frames with the two features that have the maximum \ncorrelation and then apply the Principal Component \nAnalysis (PCA) method. We turn each set into a new \nfeature. After the formation of new feature, we delete the \nold ones and add new feature to the main data set.  The \nprocess is repeated until the specified stop criteria are \nmet. Using the proposed procedure made it possible to \nreduce the number of signs to 31 \nThe ViT and ViTSD models work with images \nwhere each pixel is represented by three channel values. \nTherefore, in this paper was proposed a procedure fo r \nconverting tabular input data into a special image format, \nwhich is necessary for the models to work. After loading \nthe data set, we normalize d the values from 0 to 255, \naccording to the RGB format. Further, from the initial \ndata, we form N arrays of size k x k, where k is the number \nof features and duplicate three times. Using the above \nprocedure, the raw data of UNSW-NB 15 is transformed \ninto a structure (2744, 31, 31, 3) . As result we got 2744 \n\"images\" of size 31x31x3. \nTo investigate the effectiveness of ViT and ViTSD \nmodels, as well as the proposed feature correlation \nreduction procedure, software models were developed in \nthe Python environment using Google Colab. The quality \nof the model was estimated by accuracy, training and \ntesting time of model (Tabl. 1). \n \nTable 1 – Indicators of quality of ViT and ViTSD models with and without  \nusing of the feature correlation reduction procedure \nQuality indicators \nUsing the method PCA Without prior data preprocessing \nViT ViTSD ViT ViTSD \nAccuracy 0.973 0.987 0.761 0.952 \nTraining time, s 1056 1176 816 817 \nTesting time, s 20 14 20 10 \n \nAs you can see in the table, applying the proposed \nfeature correlation reduction procedure for the ViT \nmethod improved the accuracy of the model from \n0.761% to 0.973 % with the same testing time . For the \nViTSD model, the accuracy also increased by 3.5 %, \nalthough the testing time also increased. \n\nAdvanced Information Systems. 2024. Vol. 8, No. 1 ISSN 2522-9052 \n98 \nTo evaluate the quality of the proposed Vision \nTransformer and ViTSD intrusion detection models, \nclassical models based on SVM and KNN methods were \ninvestigated. \nA comparative analysis of the results of the \ndeveloped intrusion detection models based on ViT and \nViTSD and classical models based on SVM and KNN \nmethods is given in Tabl. 2. \n \nTable 2 – Results of machine and deep learning models using the data preprocessing procedure \nQuality indicators Accuracy Training time, s Recognition time, s \nWith the use of data \npreprocessing procedure \nSVM 0.910 107 21 \nKNN 0.933 0.06 10 \nViT 0.973 1056 20 \nViTSD 0.987 1176 14 \nWithout using the data \npreprocessing procedure \nSVM 0.908 128 18 \nKNN 0.931 0.02 12 \nViT 0.761 816 20 \nViTSD 0.952 817 10 \n \nAs you can see in the table, the best accuracy is \nachieved by using the Vision Transformer For Small -\nSize Datasets model with preprocessing of the input data \nusing the proposed feature space reduction procedure. \nConclusions \nAs part of the research, an analysis of various \napproaches to improving the quality of computer network \nintrusion detection models was performed. Traditional \nmachine learning algorithms and deep learning methods \nhave been analysed, and the advantages of us ing \ntransformer models have been considered. The efficiency \nof using the ViT and ViTSD models was proved. \nTo enhance the performance of the model, a \nprocedure for reducing the correlation of input data has \nbeen proposed, achieved through the recursive \napplication of Principal Component Analysis (PCA). \nThis has led to a reduction in the size of input data and \ndecrease training time for the model. \nThe procedure for converting tabular raw data into \na special image format, which is necessary for the \noperation o f deep learning models Vision Transformer \n(ViT) and Vision Transformer for Small -size Datasets \n(ViTSD), has been developed. \nA method for detecting intrusions into computer \nnetworks is proposed, which differs from known ones by \nusing the Vision Transformer for Small -size Datasets \n(ViTSD) deep learning algorithm and a special procedure \nfor reducing the correlation of the input data and \nconverting tabular output data into a special image \nformat. \nThe UNSW-NB 15 set, which was developed in the \nCyber Range Labora tory of the Australian Cyber \nSecurity Center (ACCS) and contains information about \nthe normal functioning of the network and during \nsynthetic intrusions, was used as the source data. \nTo investigate the effectiveness of the proposed \nmethods, software models were developed in the Python \nenvironment using Google Colab.  The performance of \nthe model was evaluated using accuracy, model training \nand testing time. A comparative analysis of the proposed \nmethod and classic models based on the SVM and KNN \nmethods was performed. \nIt was found that the use of the developed method \nbased on the ViTSD  algorithm and data preprocessing \nprocedure led to an increase in the model's accuracy to \n98.7%, which is 22.6% better compared to the ViT \nmethod. \nREFERENCES \n1. Gavrylenko, S., Chelak, V. and Hornostal, O. (2020), “Researc h of Intelligent Data Analysis Methods for Identification of \nComputer System State”, Proceedings of the 30th International Scientific Symposium Metrology and Metrology Assurance \n(MMA), Sozopol, Bulgaria, pp. 1–5, doi: https://doi.org/10.1109/MMA49863.2020.9254252  \n2. Semenov, S., Mozhaiev, O., Kuchuk, N., Mozhaiev, M., Tiulieniev, S., Gnusov, Y., Yevstrat, D., Chyrva, Y. and Kuchuk, H. \n(2022), “Devising a procedure for defining the general criteria of abnormal behavior of a computer system based on the \nimproved criterion of uniformity of input data samples”, Eastern-European Journal of Enterprise Technologies , vol. 6(4), \npp. 40-49, doi: https://doi.org/10.15587/1729-4061.2022.269128 \n3. Bhupendra I., Anamika Y.  and Atul, S. (2017) , “Decision Tree Based Intrusion Detection System for NSL -KDD Dataset, \nConference: International Conference on Information and Communication Technology for Intelligent Systems (ICTIS 2017), \ndoi:  https://doi.org/10.1007/978-3-319-63645-0_23   \n4. Zhang, J., Zulkernine, M. and Haque , A. (2008), “Random -Forests-Based Network Intrusion Detection Systems”, IEEE \nTransactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews),  vol. 38, no. 5, pp. 649 –659, doi: \nhttps://doi.org/10.1109/TSMCC.2008.923876 \n5. Liao, Y. and Vemuri, R. (2002), “Use of K -Nearest Neighbor classifier for intrusion detection”, Computers and Security , \nVolume 21, Issue 5, pp. 439–448, doi: https://doi.org/10.1016/S0167-4048(02)00514-X \n6. Meleshko, Ye., Drieiev, O., Yakymenko, M. and Lysytsia , D. (2020), “Developing a model of the dynamics of states of a \nrecommendation system under conditions of profile injection attacks”, Eastern-European Journal of Enterprise Technologies, \nvol. 4, no 4(106), pp. 14–24,. doi: https://doi.org/10.15587/1729-4061.2020.209047 \nISSN 2522-9052 Сучасні інформаційні системи. 2024. Т. 8, № 1 \n99 \n7. Yaloveha, V., Podorozhniak, A. and Kuchuk, H. (2022), “Convolutional neural network hyperparameter optimization applied \nto land cover classification”, Radioelectronic and Computer Systems , vol. 1(2022), pp. 115 –128, doi: \nhttps://doi.org/10.32620/REKS.2022.1.09 \n8. Gavrylenko, S. and Hornostal, O.  (2023), “Application of heterogeneous ensembles in problems of computer system state \nidentification”, Advanced Information System, vol. 7, no. 4, рр. 5–12, doi: https://doi.org/10.20998/2522-9052.2023.4.01 \n9. Wei, W. (2017), “Malware Traffic Classification Using Convolutional Neural Network for Representation Learning ”, Recent \nAdvances in Machine Learning and Applications,  doi: https://doi.org/10.1109/ICOIN.2017.7899588  \n10. Ashfaq, M. (2021), “HCRNNIDS: Hybrid Convolutional Recurrent Neural Network -Based Network Intrusion Detection \nSystem. Processes”, doi: https://doi.org/10.3390/pr9050834  \n11. Laghrissi, F., Douzi, S. and Douzi, K. (2021) , ”Intrusion detection systems using long short -term memory (LSTM)”,  J Big \nData, vol. 8(65), doi: https://doi.org/10.3390/pr9050834 \n12. Sak, H., Senior, A. and Beaufays, F. (2014), “Long Short-Term Memory Recurrent Neural Network Architectures for Large \nScale Acoustic  Modeling”, Interspeech, pp. 338–342, doi: https://doi.org/10.21437/Interspeech.2014-80 \n13. Khan, T., Alhussein, M., Aurangzeb, V., Arsalan, K., Naqvi, S. and Nawaz, S. (2020) , “Residual Connection-Based Encoder \nDecoder Network (RCED -Net) for R etinal Vessel Segmentation”,  IEEE Access , vol. 8, pp. 131257 –131272, doi: \nhttps://doi.org/10.1109/ACCESS.2020.3008899  \n14. Ba, J., Kiros, J. and Hinton, G. (2016), “Layer Normalization”, arXiv, doi: https://doi.org/10.48550/arXiv.1607.06450   \n15. Dosovitskiy A., Beyer L., Kolesnikov A., Weissenborn D., Zhai X., Unterthiner T., Mostafa Dehghani, Minderer M., Heigold \nG., Sylvain Gelly, Uszkoreit,  J. and Houlsby , N. (2021) , “An Image is Worth 16x16 Words: Transformers for Image \nRecognition at Scale”, Conference paper at ICLR, arXiv, arXiv:2010.11929, doi: https://doi.org/10.48550/arXiv.2010.11929 \n16. Seung, L., Seunghyun, L. and Byung, S. (2021) , “Vision Transformer for Small -Size Datasets”, arXiv, arXiv:2112.13492v1 \n[cs.CV], 27 Dec 2021, doi: https://doi.org/10.48550/arXiv.2112.13492 \n17. Gavrylenko, S. and Poltoratsky i, V. (2023) , “Method of increasing the efficiency of data classification at the account of \nreducing the correlation of the sign ”,  Control, Navigation and Communication Systems , No.  4 (74), pp.  71–75, doi: \nhttps://doi.org/10.26906/SUNZ.2023.4.070 \n \nНадійшла (received) 22.11.2023 \nПрийнята до друку (accepted for publication) 06.02.2024 \nВІДОМОСТІ ПРО АВТОРІВ/ ABOUT THE AUTHORS \nГавриленко Світлана Юріївна  –доктор технічних наук, професорка, професорка кафедри комп’ютерної інженерії та \nпрограмування, Національний технічний університет \"Харківський політехнічний інститут\", Харків, Україна;  \nSvitlana Gavrylenko  – Doctor of Technical Sciences, Professor, Professor of Computer Engineering and Programming \nDepartment, National Technical University \"Kharkiv Polytechnic Institute\", Kharkiv, Ukraine; \ne-mail: gavrilenko08@gmail.com; ORCID ID: https://orcid.org/0000-0002-6919-0055; \nScopus ID: https://www.scopus.com/authid/detail.uri?authorId=57189042150. \nПолторацький Вадим Олександрович – магістрант кафедри комп’ютерної інженерії та програмування, Національний \nтехнічний університет \"Харківський політехнічний інститут\", Харків, Україна;  \nVadym Poltoratskyi  – master's student of Computer Engineering and Programming Department, National Technical \nUniversity \"Kharkiv Polytechnic Institute\", Kharkiv, Ukraine; \ne-mail: Vadim.poltoratsky@gmail.com; ORCID ID: https://orcid.org/0009-0003-5312-4939. \nНечипоренко Аліна Сергіївна –доктор технічних наук, професорка, професорка кафедри кафедра молекулярної біології \nта функціональної геноміки, Технічний університет прикладних наук Вільдау, Вільдау, Німеччина; \nAlina Nechyporenko – Doctor of Technical Sciences, Professor, Professor of Department of Molecular Biotechnology and \nFunctional Genomics, Technical University of Applied Sciences Wildau, Wildau, Germany; \ne-mail: nechyporenko@th-wildau.de; ORCID ID: https://orcid.org/0000-0002-4501-7426; \nScopus ID: https://www.scopus.com/authid/detail.uri?authorId=57189386760. \n \nМодель виявлення вторгнень \nна основі покращеного трансформера \nС. Ю. Гавриленко, В. О. Полторацький, Нечипоренко А. С. \nАнотація.  Об'єктом дослідження є процес ідентифікації стану комп'ютерної мережі.  Предметом дослідження є \nметоди ідентифікації стану комп’ютерних мереж. Метою статті є підвищення якості  виявлення вторгнень у комп’ютерні \nмережі шляхом  розробки методу на основі моделей -трансформерів. Отримані результати . У роботі проаналізовано \nтрадиційні алгоритми машинного навчання та методи глибокого навчання, розглянуто переваги використання моделей -\nтрансформерів. Запропонов ано метод виявлення вторгнень в комп’ютерні мережі, який відрізняється від відомих \nвикористанням алгоритму глибокого навчання Vision Transformer for Small -size Datasets (ViTSD), містить процедури \nзменшення кореляції вихідних даних та перетворення табличних  вихідних даних у спеціальний формат, необхідний для \nроботи моделей. Досліджені  методи реалізовані програмно з використанням хмарного сервісу GOOGLE COLAB на основі \nJupyter Notebook. Висновки. Проведені експерименти підтвердили працездатність запропонован ого методу. Отримано, \nщо використання розробленого методу на основі алгоритму ViTSD  та процедури попередньої обробки даних надає \nможливість підвищити точність моделі до 98,7%. Це надає можливість рекомендувати його для практичного використання \nз метою підвищення точності ідентифікації стану комп’ютерної системи. \nКлючові слова:  попередня обробка даних ; машинне навчання ; нейронні мережі ; RNN; SVM; KNN; моделі-\nтрансформери; Vision Transformer.  ",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5867601037025452
    },
    {
      "name": "Computer science",
      "score": 0.5356513857841492
    },
    {
      "name": "Intrusion detection system",
      "score": 0.5070624351501465
    },
    {
      "name": "Data mining",
      "score": 0.22991493344306946
    },
    {
      "name": "Engineering",
      "score": 0.22182327508926392
    },
    {
      "name": "Electrical engineering",
      "score": 0.17462721467018127
    },
    {
      "name": "Voltage",
      "score": 0.06432855129241943
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I67256668",
      "name": "National Technical University \"Kharkiv Polytechnic Institute\"",
      "country": "UA"
    },
    {
      "id": "https://openalex.org/I38517508",
      "name": "Technical University of Applied Sciences Wildau",
      "country": "DE"
    }
  ],
  "cited_by": 8
}