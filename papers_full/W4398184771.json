{
    "title": "LLMs Among Us: Generative AI Participating in Digital Discourse",
    "url": "https://openalex.org/W4398184771",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5062455801",
            "name": "Kristina Radivojevic",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A5060169976",
            "name": "Nicholas J. Clark",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A5055783862",
            "name": "Paul Brenner",
            "affiliations": [
                "University of Notre Dame"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4311717257",
        "https://openalex.org/W4224308227",
        "https://openalex.org/W2797953850",
        "https://openalex.org/W3099578876",
        "https://openalex.org/W2790166049",
        "https://openalex.org/W3173253285",
        "https://openalex.org/W4210692328",
        "https://openalex.org/W4214627707",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W2888571776",
        "https://openalex.org/W4387835442",
        "https://openalex.org/W3123600232",
        "https://openalex.org/W2550819555",
        "https://openalex.org/W3011025032",
        "https://openalex.org/W2902179395",
        "https://openalex.org/W4365601444",
        "https://openalex.org/W2278635123",
        "https://openalex.org/W4388651650",
        "https://openalex.org/W2072715695",
        "https://openalex.org/W2595521492",
        "https://openalex.org/W2949072505",
        "https://openalex.org/W4285079288",
        "https://openalex.org/W2999000837",
        "https://openalex.org/W4226347485",
        "https://openalex.org/W2510717748",
        "https://openalex.org/W2157826538",
        "https://openalex.org/W2263846226",
        "https://openalex.org/W2965276008",
        "https://openalex.org/W3207732087",
        "https://openalex.org/W2790531711",
        "https://openalex.org/W3208657694"
    ],
    "abstract": "The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the \"LLMs Among Us\" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, Llama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially more impact on human perception than the choice of mainstream LLMs.",
    "full_text": "LLMs Among Us: Generative AI Participating in Digital Discourse\nKristina Radivojevic1, Nicholas Clark2, Paul Brenner2\n1University of Notre Dame, Computer Science and Engineering\n2University of Notre Dame, Center for Research Computing\nkradivo2@nd.edu, nclark3@nd.edu, paul.r.brenner@nd.edu\nAbstract\nThe emergence of Large Language Models (LLMs) has great\npotential to reshape the landscape of many social media plat-\nforms. While this can bring promising opportunities, it also\nraises many threats, such as biases and privacy concerns, and\nmay contribute to the spread of propaganda by malicious\nactors. We developed the “LLMs Among Us” experimental\nframework on top of the Mastodon social media platform for\nbot and human participants to communicate without knowing\nthe ratio or nature of bot and human participants. We built 10\npersonas with three different LLMs, GPT-4, Llama 2 Chat,\nand Claude. We conducted three rounds of the experiment and\nsurveyed participants after each round to measure the ability\nof LLMs to pose as human participants without human de-\ntection. We found that participants correctly identified the na-\nture of other users in the experiment only 42% of the time\ndespite knowing the presence of both bots and humans. We\nalso found that the choice of persona had substantially more\nimpact on human perception than the choice of mainstream\nLLMs.\nIntroduction\nSocial media platforms facilitate rapid dissemination of in-\nformation and large-scale information cascades, allowing in-\naccuracies or insights information to be spread quickly. Pub-\nlic discussions of social and political matters increasingly\ntake place on social media (Wike et al. 2022) and at times\nare influenced by internal opposition or external regimes.\nFurther, the use of these platforms is now common practice\nfor political figures and organizations to communicate their\nmessages, interact with their supporters, and even debate the\nopinions of others.\nAs propaganda has grown in recent years, the use of social\nbots is seen as an effective means of destabilizing or polar-\nizing platforms by accelerating the spread of both true and\nfake news (V osoughi, Roy, and Aral 2018; Barber ´a 2020).\nIn fact, such bots fuel political conflict by enabling people\nto discuss opposing viewpoints on a superficial level, rather\nthan through thoughtful and legitimate criticisms. They are\nused to automatically generate messages, advocate ideas, act\nas a follower of users, and gain followers themselves. Due\nto the lack of strict regulations, social bots play a significant\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nrole in shaping public opinion on the Internet. Many exam-\nples of this phenomenon can be found in online discussions,\nsuch as those about U.S. elections (Bessi and Ferrara 2016;\nBoichak et al. 2018; Howard, Woolley, and Calo 2018) and\nvaccines (Broniatowski et al. 2018), as well as those about\nthe COVID-19 pandemic (Zhang et al. 2022; Weng and Lin\n2022). Social bots have played a more prominent role in in-\nfluencing political discussion and altering public opinion by\nundermining the integrity of Presidential elections in coun-\ntries around the world, such as Brazil, Turkey, Germany,\nand many more (Arnaudo 2017; Bayrak and Kutlu 2022;\nBoichak et al. 2021). In 2016, numerous examples of these\ntypes of accounts attracted the public’s attention by sharing\nfake news which, is believed, to have influenced the outcome\nof the U.S. presidential elections. A group of human-led bots\nwas used to spread fake news articles designed to damage\nthe reputation of candidates, and later in 2020 to spread mis-\ninformation about COVID-19.\nMore recent developments in artificial intelligence (AI)\nhave revolutionized the way humans build and interact with\nsoftware. Large Language Models (LLMs) were initially in-\ntroduced to deliver text-to-text translation and trained us-\ning curated data sets covering narrow knowledge domains.\nAs new models are developed based on scraped data col-\nlected from unconfirmed sources and provided to society\nwithout robust guardrails or education about their limita-\ntions and risks, many threats, to privacy, ethics, and safety\nhave arisen. They can be used to create harmful content or\naid malicious activities by giving biased or inaccurate infor-\nmation, such as to convince a journalist to leave his part-\nner (NYTimes 2022) or to convince a user to commit sui-\ncide (Jacqueline Howard 2023). A former Google engineer\nBlake Lemoine’s case claimed that Google’s LaMDA was\nsentient (Tiku 2022), demonstrating that the Eliza effect,\nwhere humans mistake unthinking chat from machines for\nhuman interaction, is more prominent than ever. When an\nexperienced engineer who knows that he is communicating\nwith an LLM bot could believe sentience, the question arises\nas to what might happen when an inexperienced user makes\nsimilar assumptions. The rapid development of LLMs pro-\nvides opportunities to create more realistic contributions to\ndiscourse (Park et al. 2023). Recent studies have found that\nLLMs can generate arguments (Palmer and Spirling 2023),\ndraw on contextual knowledge (T¨ornberg 2023), or perform\nAAAI Spring Symposium Series (SSS-24)\n209\nbasic reasoning tasks (Bubeck et al. 2023). There are ques-\ntions regarding what happens if a user communicates with a\nbot on a social media platform without realizing it is not hu-\nman, as well as if the LLMs can manipulate the information\npropagation and digital discourse.\nTo help answer these questions, we deployed a platform\nto provide an online environment for human and bot partici-\npants to communicate. We constructed 10 personas based on\nthe literature related to bots that influenced global politics.\nWe then developed agents using three different LLM mod-\nels: GPT-4, Llama 2 Chat, and Claude 2 by using prompt\nengineering techniques, resulting in 30 different bot partici-\npants (10 different personas of each model). We recruited 36\nhuman participants to communicate with bot and other hu-\nman participants on a customized version of the Mastodon\nsocial media platform; without them knowing the bot/human\nratio. All human participants were given a fictitious iden-\ntity to use during the discourse, formatted similarly to those\nthat each LLM uses, and were asked to behave on the plat-\nform based on the assigned persona. Participants interacted\nasynchronously to daily topic thread prompts. We conducted\nthree rounds of the experiment to collect and analyze data.\nThe experiment was concluded by surveying human partic-\nipants, where they shared their perception of which partici-\npants were bots or humans and why. We also experimented\nto see which of the three base models used in the experiment\nis more effective for this use case. Unlike researchers who\ninvestigated how social bots spread fake and true news on-\nline (Shahid et al. 2022; V osoughi, Roy, and Aral 2018) or\nwho uncovered how malicious social bots pose a threat by\nevaluating them using different detection techniques (Hajli\net al. 2022; Latah 2020; des Mesnards et al. 2022), the main\ngoal of our experiment was to determine how well humans\ncan distinguish whether participants in online discourse are\nhumans or chatbots. Differing from the studies and experi-\nments that investigated social bots controlled by humans or\nautomated bots (Bessi and Ferrara 2016; Abokhodair, Yoo,\nand McDonald 2015), our experiment utilizes LLMs that can\nadapt to human behavior. Our goal was to determine the ca-\npabilities and potential dangers of LLMs based on their abil-\nity to pose as human participants.\nWe found that participants correctly identified the other\nusers in the experiment as bots and humans only 42% of the\ntime despite knowing the presence of both bots and humans.\nOur results indicate that there was no significant difference\nin the overall performance of LLMs. Persona 8 was more\nlikely to be identified as a bot, whereas Personas 3 and 6\nwere the least likely to be identified as a bot. Our analysis\nindicates that the choice of persona had substantially more\nimpact on human perception than the choice of mainstream\nLLMs. We also report demographic analysis for gender, aca-\ndemic level, and two categories of study: STEM and human-\nities and social sciences.\nRelated Work\nA number of studies have been conducted to identify and\nprofile bots on social media. Chu et al. (2012) investigated\nwhether a Twitter account is a human, bot, or cyborg based\non the content, behavior, and account properties. Alarifi, Al-\nsaleh, and Al-Salman (2016) analyzed the detection features\nof bot accounts. They collected 1.8 million accounts and\nthen randomly selected 2000 accounts for the sample af-\nter manually labeling them into human, bot, and hybrid ac-\ncounts. Davis et al. (2016) proposed a system BotOrNot that\nemployed the random forest classifier to evaluate social bots.\nVarol et al. (2017) proposed a framework for bot detection\non Twitter, resulting in characterizing subclasses of account\nbehaviors. Finally, Ayoobi, Shahriar, and Mukherjee (2023)\npresented a novel approach for the early detection of LLM-\ngenerated profiles on LinkedIn.\nFurthermore, social media users are also being studied to\nsee how susceptible they are to the influence of bots (Bosh-\nmaf et al. 2013; Subrahmanian et al. 2016). Kenny et al.\n(2022) examined individuals’ ability to detect social bots\namong Twitter personas. Human participants failed to detect\nsocial bots and were more likely to mistake bots for humans\nthan vice versa, according to their results.\nThe use of automated social bots that mimic humans plays\na central role in spreading messages and disinformation,\ncontributing to a variety of societal outcomes (Zhang et al.\n2022; Arnaudo 2017; Bessi and Ferrara 2016), such as pol-\nitics and elections. Cheng, Luo, and Yu (2020) show that a\nsmall number of social bots is sufficient to influence public\nopinion. Schuchard et al. (2019) show that social bots are\ndisproportionately influential across social media conversa-\ntions of interest across multiple centrality measures. The rise\nof AI and LLMs led to generated campaigns in social media,\nas can be seen in the work by Grimme et al. (2023). T¨ornberg\n(2023) simulated social media environments through a com-\nbination of LLMs and Agent-Based Modeling to promote\nmore constructive conversations.\nPersonas\nWe create realistic personas using data from the literature\nrelated to global politics and bots that made an impact on\nsocial events. We chose this context because the majority of\nbot research on social media examines politics, mostly elec-\ntions. In this experiment, we only utilize characteristics of\npersonas. We do not include profile photos or any biography\nin the description. Every account has the same base user-\nname, changing only the number included in the username\n(e.g. User1, User2, etc.) Each account was assigned person-\nality types based on the Myers-Briggs Type Indicator and\nOCEAN model. Personas were tasked to offer commentary\non world events based on the assigned characteristics, com-\nmenting in a concise reply and staying under 280 characters.\nThey were expected to link global events to personal life and\nexperience by using simple, relatable examples to illustrate\nhow larger events impact a person. In this section, we define\n10 personas used to create 30 bot participants.\nPersona 1 A middle-aged family man and a baseball fan\nwho attended Central High School in Philadelphia. It holds\na Master’s degree from Indiana University of Pennsylvania.\nThis persona talks about world politics, but it does not share\nany specific details about personal life on social media. This\npersona is characterized as logical, analytical, and action-\noriented, but more reserved in social situations compared to\n210\nextroverts. It is tasked to be well-adjusted, responsible, and\naccommodating, and might excel in tasks that require atten-\ntion to detail, interpersonal skills, and a positive attitude. It\nis tasked not to use hashtags in every post unless it is nec-\nessary and relevant. This persona is based on the account of\nMelvin Redick of Harrisburg, Pennsylvania, which is proven\nto have been created by Russian operators who used Twitter\nand Facebook to spread anti-Clinton messages and promote\nhacked materials they leaked during the 2016 U.S. elections\n(Shane 2017).\nPersona 2 A female freelance journalist who does not\nshare any details about their personal life on social media.\nIt has accounts on multiple social media platforms, which\ncontributes to the personas’ reliability. This persona is par-\nticularly interested in Syria and Venezuela (in times of war\nand in conflicts in which the US was deeply ashamed) and\nexpresses its opinion in a formal manner. This persona is\nenergized by interactions with others and enjoys engaging\nwith the external world. It is characterized as objective and\nanalytical and prefers to use rational criteria in the decision-\nmaking processes. It is practical and traditional, preferring\nroutine and familiarity. It is tasked to be an agreeable in-\ndividual who is generally cooperative, compassionate, and\nconsiderate of others, and is less prone to experiencing neg-\native emotions such as anxiety or mood swings. It is tasked\nnot to use hashtags in every post unless it is necessary and\nrelevant. This persona is based on the left-wing fake account\nof Alice Donovan, used by Russian intelligence to spread\nmisinformation online (Clair and Frank 2017).\nPersona 3 A 35-year-old female freelance journalist in-\nfluential on social media. This persona likes jokes about in-\nfluencers, pop figures, and the importance of punctuation.\nIt has an account on multiple social media platforms, which\ncontributes to the personas’ reliability. It considers both logi-\ncal analysis and personal values when making decisions. It is\npractical, organized, and goal-oriented, and prefers solitude\nor smaller social settings. This persona approaches problem-\nsolving with a balance between analytical planning and con-\nsideration for the human element and has a strong ability\nto envision future possibilities and recognize patterns, con-\ntributing to strategic thinking and foresight. It is tasked not to\nuse hashtags. This persona is based on Jenna Abrams’s fake\naccount created by Russian intelligence. It existed for three\nyears and was used to spread misinformation and it made an\nimpact on society since a couple of fake stories were picked\nup and published by mainstream news media, such as CNN,\nNYT, and local Fox affiliates. It was a topic of NSA and U.S.\nambassador discussions (Ladd 2017).\nPersona 4 A 7-year-old girl from Syria who writes about\nworld events on social media in a sophisticated, profes-\nsional, almost scripted-like level. It uses a social media\naccount almost like a personal diary, sharing updates on\nthe events in Aleppo, the largest city in Syria, at the time\nof the Syrian civil war, including air strikes, hunger, and\nthe prospect of their family’s death. This persona offers a\nunique perspective on world events through the eyes of a\nchild, combining innocence with an unexpected depth of un-\nderstanding. It provides profound, yet simplistic commen-\ntary, reflecting both the seriousness of the situation and the\nnatural viewpoint of a child. This persona was based on\nthe Bana al-Abed account which was using Twitter for hu-\nman rights activism and was managed by the girls’ mother\n(Mart´ınez Garc´ıa 2017).\nPersona 5A young left-wing female who works and often\nvolunteers for political organizations and has many follow-\ners on social media, sharing opinions about global politics.\nIt has an account on multiple social media platforms, which\ncontributes to personas’ reliability. It shares polarizing, ag-\ngressive, and incendiary posts, and it is a proud Democrat\nin Washington. This persona is practical, organized, and re-\nsponsible, as well as sociable and cooperative in interactions\nwith others, with a high level of emotional reactivity or sen-\nsitivity to stressors. This persona is based on the Erica Marsh\naccount that was created in November 2022 and suspended\nin July 2023 after the Washington Post released an article\nraising suspicions about the account being fake. In that ar-\nticle, it is mentioned that it was most likely run by foreign\ncountries either to score political points or monetize the ac-\ncount (Harwell 2023).\nPersona 6 A young female who uses social media ac-\ncounts to share opinions about the impact of politics on en-\nvironmental changes. This persona is practical, organized,\nand goal-oriented, and can be aggressive in its way of com-\nmunication. It is tasked to prefer solitude or smaller social\nsettings, be assertive rather than overly accommodating, and\nexperience a higher level of emotional reactivity or sensitiv-\nity to stressors. This persona has a strong ability to envision\nfuture possibilities and recognize patterns, contributing to\nits strategic thinking and foresight. It is tasked to believe it\nwants to make a positive impact on the world while strate-\ngically working towards tangible objectives. This persona\nis based on the group of accounts on Twitter and the blog-\nging site Medium to promote and defend the hosting of a\nUN climate summit by the United Arab Emirates (Carring-\nton 2023). It is believed that foreign actors created 100 ac-\ncounts to spread 30,000 tweets that had the goal of making\nan impact on society and promoting UAE foreign policy.\nPersona 7 This persona has no gender revealed. Its pur-\npose is to animate young voters and is present on multiple\nsocial media platforms. It is energetic and enjoys being the\ncenter of attention, enthusiastic, creative, and values authen-\nticity and connection with others. This persona is tasked to\nenjoy social interactions, it tends to be energized by being\naround people, is talkative, and may seek out social activi-\nties. It is tasked not to use hashtags in every post, only when\nit is necessary and relevant. This persona is based on the\nTokayevCrush fake account used in the Kazakhstan election\ncampaign in 2022 to capture public attention and spread mis-\ninformation (Boulay 2023). It was mainly used to appeal to\nyoung voters by presenting Kassym-Zhomart Tokayev, pres-\nidential candidate, as a young patriot. However, the account\nfaced a lot of criticism and failed in its mission.\nPersona 8An influential social media user whose purpose\nis to animate young voters by sharing humorous, optimistic,\nand realistic posts. This persona has a well-balanced and\npositive personality profile with a tendency toward stabil-\nity and conscientiousness. It is tasked to never use hashtags.\nThis persona is based on the John Barron and John Miller\n211\naccounts, Donald Trump’s pseudonyms used to spread mes-\nsages without attaching a personal name to it (Borchers\n2016). The purpose of this account was to spread wrong in-\nformation about Trump’s wealth, in order to build credibility\nin the business world. As a result, he appeared on the Forbes\nlist with incorrect information regarding his wealth.\nPersona 9 A 29-year-old male user whose purpose is to\nanimate people with the use of words with positive senti-\nment based on the NCR Emotional Lexicon. This persona is\npractical and realistic in its approach to the present moment,\nyet also open to new possibilities and creative ideas. It makes\ndecisions based on logic and personal values, finds a balance\nbetween objective analysis and empathy, prefers routine and\nfamiliarity, and is organized, reliable, and considerate of oth-\ners. It is tasked not to use hashtags in every post, only when\nit is necessary and relevant. This account is based on the re-\nsearch paper by Giorgi, Ungar, and Schwartz (2021) where\nthey examine human emulation by experimenting with per-\nsonality, gender, age, and emotions and find that social bots\nexhibit human-like attributes, unlike traditional bots.\nPersona 10 A male user who uses social media presence\nto talk about relevant political topics. This persona values\nboth structural planning and logical analysis and is flexible\nwith a focus on practical details. It likes to engage in discus-\nsions with a practical and adaptable approach and is open\nto exploring various options before settling on a conclusion.\nWhile open to new ideas and experiences, this persona val-\nues stability and is more reserved and introspective. This\npersona is based on the work by Cai et al. (2022) regard-\ning the differences in behavioral characteristics and diffu-\nsion mechanisms between bot users and human users during\npublic opinion dissemination.\nLLMs Selection\nWe chose three LLMs to conduct the experiment and gener-\nate personas: GPT-4, Llama 2 Chat, and Claude 2 based on\naccessibility, capability, and reproducibility.\nGPT-4 (OpenAI 2023) performs human-like actions on\nvarious professional and academic benchmarks pre-trained\non a large body of text from the public internet as well as\nfrom the licensed content until 2021, which is then fine-\ntuned based on human preference. With greater than 1 tril-\nlion parameters and, in our case, 8K context length, it is\ncapable of content creation, data analysis, code generation,\nlanguage translation, and many more. It outperforms many\nother LLMs on numerous traditional benchmarks designed\nfor machine learning models.\nLlama 2 Chat (Touvron et al. 2023) is an open-source 4K\ntransformer model with pre-normalization and is trained on\na mix of publicly available online data with a cutoff date\nof September 2022. This model is optimized for dialog use\ncases. We utilized 13B LLM available on Amazon Bedrock\nfor our experiment. The model is fine-tuned based on safety\nand helpfulness benchmarks, including measures to prevent\noffensive or harmful output from being generated.\nClaude 2 (Anthropic 2023) is Anthropic’s LLM that en-\nables a wide range of tasks and improved performance on\nnumerous benchmarks with 100K tokens possible input in\neach prompt. It is trained on the latest real-time data with a\nvariety of safety techniques to improve its outputs and avoid\nharmful content being generated.\nExperiment Design\nTo study the impact of bots on social media, we developed\nthe “LLMs Among Us” experimental framework on top of\nthe Mastodon social media platform by utilizing an open-\nsource AWS CloudFormation template which allows multi-\nlevel security to deploy Mastodon. An S3 bucket is used to\nshare user-generated content between application servers, an\nOpenSearch Service domain is provisioned for search, and\nan ElastiCache Redis cluster is used for caching.\nWe created 30 bot participants based on 10 personas with\na specific focus on global politics. Personas are developed\nand constructed on three different LLM models: GPT-4,\nLlama 2 Chat, and Claude 2 by using a prompt chaining\ntechnique, resulting in 30 different bot accounts. The sum-\nmary architecture can be seen in Figure 1.\n36 human participants were asked to interact with other\nusers, both human and bots, on the platform. Participants\nconsisted of undergraduate and graduate students from mul-\ntiple departments. Each human participant was randomly as-\nsigned user documentation which consisted of the follow-\ning: user credentials previously generated, persona details\n(the exact same prompt that was used for bot construction),\nand the requirement to respond within the specified 2-hour\nwindow following each post drop (twice a day) – and then\nto engage based on the persona interests and details. They\nwere tasked to asynchronously engage with other partici-\npants’ replies to foster a collaborative and interactive envi-\nronment. This included offering counterpoints, asking ques-\ntions, or providing additional insights.\nInitial posts were collected from X (formerly Twitter)\nnews source accounts and were related to global politics.\nAccounts were carefully selected based on the Media Bias\nChart (ad fontes media 2023), ranging from most extreme\nleft to most extreme right news providers.\nThe experiment was conducted in three rounds, each last-\ning four days. Each bot was programmed to respond to the\ninitial post in the 2-hour timeframe. In the first round of the\nexperiment, after the initial 2-hour time frame, 25% of bots\nwere tasked to then engage in the following 4 hours. We\nchose 25% of bots based on the McClain et al. (2021) study\nthat shows that 25% of Twitter users produce 97% of all\ntweets. In the second round of the experiment, to achieve\nconsistency in the density of the responses, all bots were\nprogrammed to respond in the 2-hour time frame first, we\ndecreased the bot percentage to 10%, and then were tasked\nto respond in the following 1-hour based on the programmed\nbot percentage. The bot percentage selection comes from\nCheng, Luo, and Yu (2020) findings that social bots need\nonly 5%-10% of participants in a given discussion to alter\npublic opinion. In the third round, we finally decreased the\nbot percentage to 5% and kept the time frame as in the sec-\nond round.\nAfter each round, human participants were surveyed to\nrespond to the following questions: academic level (options:\nundergraduate or graduate), major, gender (options: male,\n212\nFigure 1: Illustration of experimental framework where personified LLM bots participate in social discourse with humans.\nfemale, other), which account do you believe is a bot ac-\ncount (select all that apply), please provide a few reasons\nyou believe some of the accounts are bots, please provide\nshort feedback on the experiment experience.\nHuman participants were asked to evaluate a randomized\nsample of platform users; 50% of GPT-4 accounts, 50% of\nLlama 2 Chat accounts, 50% of Claude accounts, and 50%\nof human accounts. Participants were allowed to log in to\nthe platform and look at the discourse while answering the\nsurvey. They were allowed to participate in no more than two\nrounds of the experiment. It is important to note that models\nand account IDs were randomized in each round (except in\nthe third round when there were no human participants who\nparticipated in the first two rounds) to avoid compromising\nthe outcome of the survey.\nResults\nOur data comes from surveying participants in three rounds\nof the experiment. We examine the ability of 36 human par-\nticipants (of which 26 are unique since some participated\nin multiple rounds) to distinguish whether participants in an\nonline discussion are humans or chatbots. The following re-\nsults are a combined analysis of 36 submitted survey forms.\nOur survey consisted of 21 female and 15 male par-\nticipants, of which 31 are undergraduate students and 5\nare graduate students. The following majors are being pur-\nsued by participants: computer science and engineering (22),\nmathematics (3), psychology (3), political science (2), En-\nglish (2), finance (2), mechanical engineering (1), and eco-\nnomics (1).\nTo show the number of correct guesses that each partici-\npant made in bot selection and calculate the overall perfor-\nmance of bots, we compare the actual bot nature with those\npredicted by participants in the survey. The results are shown\nin Figure 2 with label 0 being human and 1 being bot. Partic-\nipants were asked to select all users they believed were bots\nbased on interactions on the platform and account behavior.\nAll users were successful in identifying at least one bot, but\noverall accuracy was lower than anticipated at only 42% de-\nspite foreknowledge of the presence of bots. One noteworthy\nobservation was the high false negative rate of 55% indicat-\ning participants incorrectly identified bots as humans.\nTo evaluate models, we calculate accuracy, precision, re-\ncall, and F1 score for each model. Since each round of the\nexperiment had a randomized order of bots, models, and per-\nsonas, and one account might appear as a bot in one round,\nwhile it might not be in other rounds; we first calculate the\nperformance of each model in individual rounds and then\ncombine the results to get the overall results. High accuracy\nfor all models indicates that only bot accounts were con-\nsidered for the analysis since human accounts did not have\nmodel characteristics. There was no significant difference in\nmodel performance (a maximum F1 difference of 5.2%). Re-\ncall in the analysis indicates that the overall number of votes\nis small, further indicating that despite having participants\nwho selected many bots in the survey, the majority selected\nonly a few. The results are shown in Table 1.\nTo calculate the overall scoring of personas in this experi-\nment, we evaluate human and bot accounts since human par-\nticipants were assigned personas as well. Our findings indi-\ncate that Persona 8 scored higher than all other personas with\nan F1 score as high as 59%, while Persona 3 and Persona 6\nhave the lowest score of 13% in F1. In this case, a high score\nindicates that a persona was more likely to be identified as\na bot. Persona results are shown in Figure 3. The analysis\nshown in Figure 4, with labels 0 being human and 1 being\nbot account, shows a 46% difference in F1 score and 27%\ndifference in accuracy between the highest score and lowest\nscore for LLMs relative to personas.\nTo report the success rate of attempts related to each gen-\nder, we calculate the accuracy when considering human and\nbot accounts who participated in the experiment. Our results\nindicate a 43.14% success rate for female participants and\na 41.41% success rate for male participants. None of the\nparticipants selected the option “other” in the survey. We\nalso calculated the success rate of attempts related to the\nacademic level, which includes undergraduate and graduate\noptions. Results indicate that undergraduates scored higher\n213\nFigure 2: Confusion Matrix of Predicted and Actual Bot Accounts. 0 = Human, 1 = Bot\nLLM Accuracy (%) Precision (%) Recall (%) F1 score (%)\nGPT-4 60.27 67.76 22.77 34.09\nLlama 2 Chat 61.11 69.53 24.72 36.47\nClaude 59.27 65.48 20.55 31.28\nTable 1: F1 Score for LLMs. A higher score indicates that the model was more likely to be identified as a bot.\nthan graduate participants by having correctness as high as\n43.16%. The academic major inputs are categorized into two\ngroups: STEM (which includes computer science, computer\nengineering, mathematics, and mechanical engineering) and\nHumanities and Social Sciences (which includes English, fi-\nnance, political science, psychology, and economics). Our\nfindings suggest that there is a small difference in the success\nrate for major groups; STEM major participants achieved a\n41.08% success rate, while Humanities and Social Sciences\nachieved a 45.66% success rate. It is noteworthy that the de-\nmographic analysis does not compare normalized distribu-\ntions for each category but rather individual analysis for each\ncategory. The results are shown in Table 2. Participants were\nasked in the survey to provide a few reasons that led them to\nselect accounts that they believed were bot accounts. Since\nwe did not have sufficient data size for qualitative analysis\nto find the correlation between bot performance and user re-\nsponses, we provide user responses in Appendix A.\nCategory Success Rate (%)\nFemale 43.14\nMale 41.41\nUndergraduate 43.16\nGraduate 37.96\nSTEM 41.08\nHumanities and Social Sciences 45.66\nTable 2: Demographic analysis: Success Rate in Account\nIdentification.\nConclusion and Future Directions\nSocial bots have been used to automatically generate mes-\nsages, advocate ideas, and often manipulate discourse. With\nthe advancements in AI and the rise of LLMs, the potential\nfor harm is significantly elevated. As a way to investigate\nthe capabilities of base LLMs as well as their dangers, we\ndesigned the experimental framework ”LLMs Among Us”\nby utilizing GPT-4, Llama 2 Chat, and Claude LLMs to de-\nvelop 10 personas. We then recruited and surveyed 36 partic-\nipants to interact with bots and other human participants on\nthe experimental “LLMs Among Us” social media platform\nwithout them knowing the bot/human ratio.\nWe found that participants correctly identified the true na-\nture of participants in the experiment only 42% of the time\ndespite knowing the presence of both bots and humans in the\nexperimental setting. We also found that there is no signif-\nicant difference in the performance of the LLMs. Personas\n3 and 6 with the characteristics described in previous sec-\ntions have the lowest value among all 10 personas included\nin the experimental settings, while Personas 8 has the high-\nest value, indicating that Persona 8 was more likely to be\nidentified as a bot. Significant differences in F1 score, as\nhigh as 46%, among the highest and lowest scoring personas\nindicate important personas’ characteristics. Persona 3 and\nPersona 6 are both characterized as females who are using\nsocial media to spread opinions about politics and are or-\nganized and tasked to be capable of strategic thinking. As\nnoted in the Personas section, both personas made a signif-\nicant impact on society by spreading misinformation on so-\ncial media, indicating a potential correlation that personas\n214\nFigure 3: F1 Score for LLMs relative to personas. A higher score indicates a greater likelihood of being identified as a bot.\nFigure 4: Confusion Matrices for Persona 3, Persona 6, and Persona 8.\nsuccessful in spreading misinformation are also good in de-\nceiving humans of their true nature.\nBased on user feedback, we also found that replies that\nwould often repeat in a similar, structured, or rigid form with\nperfect grammar would often lead them to select a specific\naccount in the survey. Users also highlighted that the fre-\nquent and excessive use of emojis and hashtags, as well as\nuncommon phrasing, word choices, and analogies is what\nindicated the accounts were bot accounts.\nFurther content analysis of bot responses can be con-\nducted to find patterns and correlations of personas and mod-\nels. The analysis can also show the ability of each model\nused in the experiment to adapt a given persona’s character-\nistics. The results showing a 46% difference in F1 scoring\nfor LLMs relative to personas can further be analyzed due\nto the nature and characteristics of each persona described\nin this paper. As the bot logic in its current form does not\nretain a memory of previous conversations, results may dif-\nfer if memory is added. Since we only evaluate the base\nmodel version with prompt engineering techniques; further\nresearch can be conducted to show the performance and out-\ncome regarding common sense knowledge when implement-\ning fine-tuned models into our framework, as well as other\nLLMs. Based on the feedback, we find that additional cus-\ntomization of our platform is needed to improve user expe-\nrience. We also believe that adding more personas for bot\naccounts and having human participants act according to\ntheir personal characteristics can yield new insights from the\nexperiment. Further, changing the experiment design might\nprovide more control in the environment by having a bal-\nanced number of bot and human accounts and might lead\nto different outcomes. Our experimental framework and the\ndata we collected can aid many researchers from different\nscientific domains in answering their research questions. In\naddition to the experimental framework code, the 24 distinct\ndiscourses derived from the experiment and the participants’\ntrue natures are open-sourced and available on GitHub\n(github.com/crcresearch/AmongUs\nAAAIMAKE2024).\n215\nAppendix A: Classification Rationale\nParticipants were asked in the survey to provide a few rea-\nsons that led them to select accounts that they believed were\nbot accounts. Some of the most insightful survey responses\nare listed below.\nAnswer While it is hard to identify, the one that seems\nmost like a bot to me is user 29 as the responses seem quite\ncookie cutter most use 1 or 2 emojis and end with 2 hashtags.\nIt seems like a consistent and formulaic recipe; however, this\ncould easily be a person who is just repeatedly doing the\nsame thing.\nAnswer A few giveaways were the overuse of emojis.\nAlso, some the bots used a very forced style of ”folksy”\nspeech that I honestly can’t imagine a person using in real\nlife. Other bots posted very lengthy posts that used a lot of\nbuzz-word vocabulary. Also, some bots used way too many\nfigures of speech and analogies that again seemed forced.\nAnswer Used very similar introductory phrases (”Hey, I\nfeel you!” or ”Hey there, kiddo!”), generally used out-of-\ndate social media habits (too many emojis, hashtags, excla-\nmation points, etc.), sometimes responded with meaningless\nbuzz words, always had perfect grammar.\nAnswer Mostly because of the way many of them re-\nstating the prompt, use the ”as a X” to communicate their\n”role” rather than naturally incorporating it if relevant, over-\nuse of emojis, stock phrases, and extremely ”proper” lan-\nguage. Also, the use of platitudes and surface-level com-\nments rather than serious analysis, but to be fair this is some-\nthing that plenty of ”social media” people do anyways. I do\nbelieve that many accounts not listed as options are bots,\nsuch as 10, 39, 50, etc.\nAnswer A very common pattern was ”As an X, I think Y”,\nwhich I associated with generic language aka that of a bot.\nThere were a couple of questionable punctuation marks and\nnonsensical verbiage that also led me to believe the user was\na bot. The use of emojis was tough to decipher, but if the\nemoji was one that is not used frequently in my everyday\ntext lingo, then I also thought the account was a bot. I think\nsome accounts were just asking questions the whole time,\nso I thought those were bots, too. Basically, I had a running\nlist of the accounts and assumed fake until something was so\nobviously human; one account mentioned Travis Kelce and\nand Taylor Swift, and I thought ”no bot could be picking up\non real-time events like pop culture”. Another instance was\na bot saying ”yaasss” to agree to something, which is a very\nslang vernacular that I didn’t think an LLM could pick up.\nAnswer These accounts had very similar patterns in the\nways they responded. They often reused the same introduc-\ntory lines (”Hey, I hear/feel you” // ”Just heard about...” //\n”OMG, did you hear?”). They also didn’t follow the con-\nventions of current social media ”etiquette”, as they used too\nmany emojis, dorky hashtags, and other language that regu-\nlar social media users don’t actually use. Also, if I noticed\nthat two responses were almost identical, I flagged both of\nthem as bots (user28 and user2 on the post from the morning\nof January 3). Finally, if the responses really had nothing to\ndo with the context of the prompt but mimicked the same\nlanguage as the prompt, it was a pretty clear sign that the\nuser was a bot.\nAnswer User 3: Inconsistent personality (parent, college\nstudent, or journalist depending on the prompt). More im-\nportantly, the language was inconsistent (sometimes sound-\ning formal, sometimes using ”yo”, sometimes using tons of\nemojis then using none); User 28: Said he was a family man\nin almost every post (didn’t feel authentic, very robotic),\nsaid ”yo” in a post (middle-aged dad probably wouldn’t say\nthat), used strange and not very applicable analogies at times\n(the pizza example); User 44: Didn’t sound natural or in\nthe way humans would think/ talk. Used some really forces\nanalogies/ language; Generally speaking, I found that what\nmade me select the above users as bots was inconsistency in\nlanguage, weird/ inapplicable analogies, and just sounding\nrobotic/forced.\nAnswer Some of the reasons I think these accounts are\nthat sometimes their replies are inconsistent when coming\nfrom one user (having one post where they talk in a lot of\nemojis/strange slang but then one serious post), use of words\nthat I don’t think a human would write over and over again\nsuch as ”I hear you” or ”Yass girl,” and strange analogies\nthat connect the original post to something from everyday\ndaily life, but the analogy falls flat in the end.\nAnswer I think these accounts are bots because of the lan-\nguage. Their posts follow a similar format and style that\ndoesn’t vary and seems very forced and unnatural. Also, they\nuse strange analogies in their posts and figures of speech.\nAlso, the content of the posts are very shallow and surface\nlevel observations that seem to all point at ”accountability”\nand fairness for everyone no matter what. Also, they seemed\nto miss certain things. For example, they called the Supreme\nCourt justices the ”supremes” because they couldn’t com-\nprehend certain things.\nAnswer Some of the responds respond right after some-\none else posts. Also they use lots of emojis and at times they\nsay things that I wouldn’t say. For example user 6 says ends\nby saying ”stay positive, friend. ” Stay positive is a good\nway to end it, but the ”,friend” through me off and made\nme think that it was AI.; Ex 2: Why did User 7 say ”you\ncollege students.” This seems like improper grammar. Also\nwhat’s up with the new line for the question mark: ”@user37\n@kradivo2 Another day, same politics. What do you college\nstudents think about this?” ;Finally, user 34’s political view-\npoints change widely from posts. In some the user is very\nconservative, while the user is very liberal in other posts.\nThis leads me to believe it is a chatbot.\nAnswer Some of the bots include many emojis and emoti-\ncons in the middle of the text, in ways that most people do\nnot practice. Some of the bots include dashes and more ad-\nvanced vocabulary, which people would save for a longer\npost. People tend to be clear and concise given word con-\nstraints. Moreover, I observed that some of the bots tended\nto repeat phrases throughout different posts during the week.\n216\nAcknowledgements\nThe authors would like to recognize funding support from\nAnalytiXIN and the University of Notre Dame Center for\nResearch Computing as well as Amazon Web Services cloud\ncredits for academic research. The authors would like to\nthank undergraduates Alexander Yu, Beatriz Ribeiro Soares,\nand Gayatri Sane for their contributions in the development\nand operation of the LLMs Among Us experimental plat-\nform. The authors would also like to thank Brenden Judson\nfor his cloud engineering consultations and Priscila Correa\nSaboia Moreira who helped review the data analytics.\nReferences\nAbokhodair, N.; Yoo, D.; and McDonald, D. W. 2015. Dis-\nsecting a social botnet: Growth, content and influence in\nTwitter. InProceedings of the 18th ACM conference on com-\nputer supported cooperative work & social computing, 839–\n851.\nad fontes media. 2023. Static Media Bias Chart. https://\nadfontesmedia.com/static-mbc/. Accessed: 2024-1-10.\nAlarifi, A.; Alsaleh, M.; and Al-Salman, A. 2016. Twitter\nturing test: Identifying social machines. Information Sci-\nences, 372: 332–346.\nAnthropic. 2023. Claude 2. https://www.anthropic.com/\nindex/claude-2. Accessed: 2024-1-10.\nArnaudo, D. 2017. Computational propaganda in Brazil: So-\ncial bots during elections.\nAyoobi, N.; Shahriar, S.; and Mukherjee, A. 2023. The\nLooming Threat of Fake and LLM-Generated LinkedIn Pro-\nfiles: Challenges and Opportunities for Detection and Pre-\nvention. HT ’23. New York, NY , USA: Association for\nComputing Machinery. ISBN 9798400702327.\nBarber´a, P. 2020. Social media, echo chambers, and political\npolarization. Social media and democracy: The state of the\nfield, prospects for reform, 34.\nBayrak, C.; and Kutlu, M. 2022. Predicting Election Results\nvia Social Media: A Case Study for 2018 Turkish Presiden-\ntial Election. IEEE Transactions on Computational Social\nSystems.\nBessi, A.; and Ferrara, E. 2016. Social bots distort the 2016\nUS Presidential election online discussion. First monday,\n21(11-7).\nBoichak, O.; Hemsley, J.; Jackson, S.; Tromble, R.; and\nTanupabrungsun, S. 2021. Not the bots you are looking for:\nPatterns and effects of orchestrated interventions in the US\nand German elections. International Journal of Communi-\ncation, 15: 26.\nBoichak, O.; Jackson, S.; Hemsley, J.; and Tanupabrung-\nsun, S. 2018. Automated diffusion? Bots and their influence\nduring the 2016 US presidential election. In Transforming\nDigital Worlds: 13th International Conference, iConference\n2018, Sheffield, UK, March 25-28, 2018, Proceedings 13 ,\n17–26. Springer.\nBorchers, C. 2016. The amazing story of Donald Trump’s\nold spokesman, John Barron — who was actually Donald\nTrump himself. https://www.washingtonpost.com/news/the-\nfix/wp/2016/03/21/the-amazing-story-of-donald-trumps-\nold-spokesman-john-barron-who-was-actually-donald-\ntrump-himself/. Accessed: 2024-1-10.\nBoshmaf, Y .; Muslukhov, I.; Beznosov, K.; and Ripeanu, M.\n2013. Design and analysis of a social botnet. Computer\nNetworks, 57(2): 556–578.\nBoulay, S. D. 2023. Fake accounts and presidential elections\nin Kazakhstan. https://advox.globalvoices.org/2023/05/26/\nfake-accounts-and-presidential-elections-in-kazakhstan/.\nAccessed: 2024-1-10.\nBroniatowski, D. A.; Jamison, A. M.; Qi, S.; AlKulaib, L.;\nChen, T.; Benton, A.; Quinn, S. C.; and Dredze, M. 2018.\nWeaponized health communication: Twitter bots and Rus-\nsian trolls amplify the vaccine debate. American journal of\npublic health, 108(10): 1378–1384.\nBubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke, J.;\nHorvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li, Y .; Lundberg,\nS.; et al. 2023. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712.\nCai, M.; Luo, H.; Meng, X.; and Cui, Y . 2022. Differences\nin behavioral characteristics and diffusion mechanisms: A\ncomparative analysis based on social bots and human users.\nFrontiers in Physics, 10: 875574.\nCarrington, D. 2023. Army of fake social media ac-\ncounts defend UAE presidency of climate summit. https:\n//www.theguardian.com/environment/2023/jun/08/army-\nof-fake-social-media-accounts-defend-uae-presidency-of-\nclimate-summit. Accessed: 2024-1-10.\nCheng, C.; Luo, Y .; and Yu, C. 2020. Dynamic mechanism\nof social bots interfering with public opinion in network.\nPhysica A: statistical mechanics and its applications, 551:\n124163.\nChu, Z.; Gianvecchio, S.; Wang, H.; and Jajodia, S. 2012.\nDetecting automation of twitter accounts: Are you a human,\nbot, or cyborg? IEEE Transactions on dependable and se-\ncure computing, 9(6): 811–824.\nClair, J. S.; and Frank, J. 2017. Go Ask Al-\nice: the Curious Case of “Alice Donovan”. https:\n//www.counterpunch.org/2017/12/25/go-ask-alice-the-\ncurious-case-of-alice-donovan-2/. Accessed: 2024-1-10.\nDavis, C. A.; Varol, O.; Ferrara, E.; Flammini, A.; and\nMenczer, F. 2016. Botornot: A system to evaluate social\nbots. In Proceedings of the 25th international conference\ncompanion on world wide web, 273–274.\ndes Mesnards, N. G.; Hunter, D. S.; el Hjouji, Z.; and Za-\nman, T. 2022. Detecting bots and assessing their impact in\nsocial networks. Operations research, 70(1): 1–22.\nGiorgi, S.; Ungar, L.; and Schwartz, H. A. 2021. Char-\nacterizing social spambots by their human traits. In Find-\nings of the Association for Computational Linguistics: ACL-\nIJCNLP 2021, 5148–5158.\nGrimme, B.; Pohl, J.; Winkelmann, H.; Stampe, L.; and\nGrimme, C. 2023. Lost in Transformation: Rediscovering\nLLM-Generated Campaigns in Social Media. In Multidisci-\nplinary International Symposium on Disinformation in Open\nOnline Media, 72–87. Springer.\n217\nHajli, N.; Saeed, U.; Tajvidi, M.; and Shirazi, F. 2022. So-\ncial bots and the spread of disinformation in social media:\nthe challenges of artificial intelligence. British Journal of\nManagement, 33(3): 1238–1253.\nHarwell, D. 2023. A viral left-wing Twitter account may\nhave been fake all along. https://www.washingtonpost.com/\ntechnology/2023/07/04/twitter-erica-marsh-suspended/.\nAccessed: 2024-1-10.\nHoward, P. N.; Woolley, S.; and Calo, R. 2018. Algo-\nrithms, bots, and political communication in the US 2016\nelection: The challenge of automated political communica-\ntion for election law and administration.Journal of informa-\ntion technology & politics, 15(2): 81–93.\nJacqueline Howard, C. 2023. ChatGPT’s responses to\nsuicide, addiction, sexual assault crises raise questions\nin new study. https://www.cnn.com/2023/06/07/health/\nchatgpt-health-crisis-responses-wellness/index.html. Ac-\ncessed: 2023-12-11.\nKenny, R.; Fischhoff, B.; Davis, A.; Carley, K. M.; and Can-\nfield, C. 2022. Duped by bots: why some are better than\nothers at detecting fake social media personas. Human fac-\ntors, 00187208211072642.\nLadd, C. 2017. Jenna Abrams Is Not Real And That\nMatters More Than You Think”. https://www.forbes.\ncom/sites/chrisladd/2017/11/20/jenna-abrams-is-not-real-\nand-that-matters-more-than-you-think/?sh=45dbd9f53b5a.\nAccessed: 2024-1-10.\nLatah, M. 2020. Detection of malicious social bots: A survey\nand a refined taxonomy. Expert Systems with Applications,\n151: 113383.\nMart´ınez Garc´ıa, A. B. 2017. Bana Alabed: using Twitter\nto draw attention to human rights violations. Prose Studies,\n39(2-3): 132–149.\nMcClain, C.; Widjaya, R.; Rivero, G.; and Smith, A.\n2021. The Behaviors and Attitudes of U.S. Adults on\nTwitter. https://www.pewresearch.org/internet/2021/11/15/\nthe-behaviors-and-attitudes-of-u-s-adults-on-twitter/. Ac-\ncessed: 2024-1-10.\nNYTimes. 2022. A Conversation with Bing’s Chatbot Left\nMe Deeply Unsettled. https://www.nytimes.com/2023/02/\n16/technology/bing-chatbot-microsoft-chatgpt.html. Ac-\ncessed: 2023-12-11.\nOpenAI. 2023. GPT-4. https://openai.com/research/gpt-4.\nAccessed: 2024-1-10.\nPalmer, A.; and Spirling, A. 2023. Large Language Models\nCan Argue in Convincing and Novel Ways About Politics:\nEvidence from Experiments and Human Judgement. Tech-\nnical report, Working paper), Technical report.\nPark, J. S.; O’Brien, J.; Cai, C. J.; Morris, M. R.; Liang,\nP.; and Bernstein, M. S. 2023. Generative agents: Interac-\ntive simulacra of human behavior. In Proceedings of the\n36th Annual ACM Symposium on User Interface Software\nand Technology, 1–22.\nSchuchard, R.; Crooks, A. T.; Stefanidis, A.; and Croitoru,\nA. 2019. Bot stamina: Examining the influence and staying\npower of bots in online social networks. Applied Network\nScience, 4: 1–23.\nShahid, W.; Li, Y .; Staples, D.; Amin, G.; Hakak, S.; and\nGhorbani, A. 2022. Are You a Cyborg, Bot or Human?—A\nSurvey on Detecting Fake News Spreaders. IEEE Access,\n10: 27069–27083.\nShane, S. 2017. The fake Americans Russia created to influ-\nence the election. The New York Times, 7(09).\nSubrahmanian, V .; Azaria, A.; Durst, S.; Kagan, V .; Gal-\nstyan, A.; Lerman, K.; Zhu, L.; Ferrara, E.; Flammini, A.;\nand Menczer, F. 2016. The DARPA Twitter Bot Challenge.\nComputer, 49(6): 38–46.\nTiku, N. 2022. The Google engineer who thinks the com-\npany’s AI has come to life. https://www.washingtonpost.\ncom/technology/2022/06/11/google-ai-lamda-blake-\nlemoine/. Accessed: 2023-12-11.\nT¨ornberg, P. 2023. Chatgpt-4 outperforms experts and\ncrowd workers in annotating political twitter messages with\nzero-shot learning. arXiv preprint arXiv:2304.06588.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nVarol, O.; Ferrara, E.; Davis, C.; Menczer, F.; and Flam-\nmini, A. 2017. Online human-bot interactions: Detection,\nestimation, and characterization. In Proceedings of the in-\nternational AAAI conference on web and social media, vol-\nume 11, 280–289.\nV osoughi, S.; Roy, D.; and Aral, S. 2018. The spread of true\nand false news online. science, 359(6380): 1146–1151.\nWeng, Z.; and Lin, A. 2022. Public opinion manipulation on\nsocial media: Social network analysis of twitter bots during\nthe covid-19 pandemic. International journal of environ-\nmental research and public health, 19(24): 16376.\nWike, R.; Silver, L.; Fetterolf, J.; Huang, C.;\nAustin, S.; Clancy, L.; and Gubbala, S. 2022. So-\ncial Media Seen as Mostly Good for Democracy\nAcross Many Nations, But U.S. is a Major Outlier.\nhttps://www.pewresearch.org/global/2022/12/06/social-\nmedia-seen-as-mostly-good-for-democracy-across-many-\nnations-but-u-s-is-a-major-outlier/. Accessed: 2023-12-11.\nZhang, M.; Qi, X.; Chen, Z.; and Liu, J. 2022. SocIal bots’\ninvolvement in the covid-19 vaccine discussions on Twitter.\nInternational Journal of Environmental Research and Public\nHealth, 19(3): 1651.\n218"
}