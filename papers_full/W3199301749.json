{
  "title": "ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning",
  "url": "https://openalex.org/W3199301749",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2479473496",
      "name": "Rujun Han",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": [
        "Southern California University for Professional Studies",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2147504447",
      "name": "Nanyun Peng",
      "affiliations": [
        "University of Southern California",
        "Southern California University for Professional Studies",
        "University of California, Los Angeles"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3102401511",
    "https://openalex.org/W3123123873",
    "https://openalex.org/W3101244625",
    "https://openalex.org/W2963993699",
    "https://openalex.org/W2917019145",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2971236147",
    "https://openalex.org/W2137407193",
    "https://openalex.org/W2798602728",
    "https://openalex.org/W2983354073",
    "https://openalex.org/W2251754074",
    "https://openalex.org/W2511178802",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2561222820",
    "https://openalex.org/W2941760716",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3101620491",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4288364319",
    "https://openalex.org/W3152866586",
    "https://openalex.org/W3153241113",
    "https://openalex.org/W2468432491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2185599447",
    "https://openalex.org/W3111992880",
    "https://openalex.org/W2157275230",
    "https://openalex.org/W3106484161",
    "https://openalex.org/W3034602344",
    "https://openalex.org/W2084413241",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2251220668",
    "https://openalex.org/W3099246072",
    "https://openalex.org/W3037965442",
    "https://openalex.org/W2997090102",
    "https://openalex.org/W3088193755",
    "https://openalex.org/W2963797084",
    "https://openalex.org/W3102173443",
    "https://openalex.org/W2970170773"
  ],
  "abstract": "While pre-trained language models (PTLMs) have achieved noticeable success on many NLP tasks, they still struggle for tasks that require event temporal reasoning, which is essential for event-centric applications. We present a continual pre-training approach that equips PTLMs with targeted knowledge about event temporal relations. We design self-supervised learning objectives to recover masked-out event and temporal indicators and to discriminate sentences from their corrupted counterparts (where event or temporal indicators got replaced). By further pre-training a PTLM with these objectives jointly, we reinforce its attention to event and temporal information, yielding enhanced capability on event temporal reasoning. This **E**ffective **CON**tinual pre-training framework for **E**vent **T**emporal reasoning (ECONET) improves the PTLMs’ fine-tuning performances across five relation extraction and question answering tasks and achieves new or on-par state-of-the-art performances in most of our downstream tasks.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5367–5380\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n5367\nECONET: Effective Continual Pretraining of Language Models for Event\nTemporal Reasoning\nRujun Han1 Xiang Ren1 Nanyun Peng1,2\n1University of Southern California 2University of California, Los Angeles\n{rujunhan,xiangren}@usc.edu;violetpeng@cs.ucla.edu\nAbstract\nWhile pre-trained language models (PTLMs)\nhave achieved noticeable success on many\nNLP tasks, they still struggle for tasks that re-\nquire event temporal reasoning, which is essen-\ntial for event-centric applications. We present\na continual pre-training approach that equips\nPTLMs with targeted knowledge about event\ntemporal relations. We design self-supervised\nlearning objectives to recover masked-out\nevent and temporal indicators and to discrim-\ninate sentences from their corrupted counter-\nparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM\nwith these objectives jointly, we reinforce its\nattention to event and temporal information,\nyielding enhanced capability on event tem-\nporal reasoning. This Effective CONtinual\npre-training framework for Event Temporal\nreasoning ( ECONET) improves the PTLMs’\nﬁne-tuning performances across ﬁve relation\nextraction and question answering tasks and\nachieves new or on-par state-of-the-art perfor-\nmances in most of our downstream tasks.1\n1 Introduction\nReasoning event temporal relations is crucial for\nnatural language understanding, and facilitates\nmany real-world applications, such as tracking\nbiomedical histories (Sun et al., 2013; Bethard\net al., 2015, 2016, 2017), generating stories (Yao\net al., 2019; Goldfarb-Tarrant et al., 2020), and\nforecasting social events (Li et al., 2020; Jin et al.,\n2020). In this work, we study two prominent event\ntemporal reasoning tasks as shown in Figure 1:\nevent relation extraction (ERE) (Chambers et al.,\n2014; Ning et al., 2018; O’Gorman et al., 2016;\nMostafazadeh et al., 2016) that predicts temporal\nrelations between a pair of events, and machine\nreading comprehension (MRC) (Ning et al., 2020;\nZhou et al., 2019) where a passage and a question\n1Reproduction code, training data and models are available\nhere: https://github.com/PlusLabNLP/ECONET.\nFigure 1: Top: an example illustrating the difference\nbetween ERE and QA / MRC samples of event tempo-\nral reasoning. Bottom: our targeted masking strategy\nfor ECONET v.s. random masking in PTLMs.\nabout event temporal relations is presented, and\nmodels need to provide correct answers using the\ninformation in a given passage.\nRecent approaches leveraging large pre-trained\nlanguage models (PTLMs) achieved state-of-the-\nart results on a range of event temporal reasoning\ntasks (Ning et al., 2020; Pereira et al., 2020; Wang\net al., 2020; Zhou et al., 2020c; Han et al., 2019b).\nDespite the progress, vanilla PTLMs do not focus\non capturing event temporal knowledge that can\nbe used to infer event relations. For example, in\nFigure 1, an annotator of the QA sample can easily\ninfer from the temporal indicator “following” that\n“transfer” happens BEFORE “preparing the paper-\nwork”, but a ﬁne-tuned RoBERTa model predicts\nthat “transfer” has no such relation with the event\n“preparing the paperwork.” Plenty of such cases ex-\nist in our error analysis on PTLMs for event tempo-\nral relation-related tasks. We hypothesize that such\ndeﬁciency is caused by original PTLMs’ random\nmasks in the pre-training where temporal indicators\nand event triggers are under-weighted and hence\nnot attended well enough for our downstream tasks.\n5368\nCategory Words\nbefore, until, previous to, prior to,[before] preceding, followed by\n[after] after, following, since, now that\nsoon after, once∗∗\nduring, while, when, at the time,[during] at the same time, meanwhile\n[past] earlier, previously, formerly,\nyesterday, in the past, last time\nconsequently, subsequently, in turn,[future] henceforth, later, then\n[beginning] initially, originally, at the beginning\nto begin, starting with, to start with\n[ending] ﬁnally, in the end, at last, lastly\nTable 1: The full list of the temporal lexicon. Cat-\negories are created based on authors’ domain knowl-\nedge and best judgment. ∗∗‘once’ can be also placed\ninto [past] category due to its second meaning of ‘pre-\nviously’, which we exclude to keep words unique.\nTacoLM (Zhou et al., 2020a) explored the idea of\ntargeted masking and predicting textual cues of\nevent frequency, duration and typical time, which\nshowed improvements over vanilla PTLMs on re-\nlated tasks. However, event frequency, duration and\ntime do not directly help machines understand pair-\nwise event temporal relations. Moreover, the mask\nprediction loss of TacoLM leverages a soft cross-\nentropy objective, which is manually calibrated\nwith external knowledge and could inadvertently\nintroduce noise in the continual pre-training.\nWe propose ECONET, a continual pre-training\nframework combining mask prediction and con-\ntrastive loss using our masked samples. Our tar-\ngeted masking strategy focuses only on event trig-\ngers and temporal indicators as shown in Figure 1.\nThis design assists models to concentrate on events\nand temporal cues, and potentially strengthen mod-\nels’ ability to understand event temporal relations\nbetter in the downstream tasks. We further pre-train\nPTLMs with the following objectives jointly: the\nmask prediction objective trains a generator that\nrecovers the masked temporal indicators or events,\nand the contrastive loss trains a discriminator that\nshares the representations with the generator and\ndetermines whether a predicted masked token is\ncorrupted or original (Clark et al., 2020). Our ex-\nperiments demonstrate that ECONET is effective\nat improving the original PTLMs’ performances on\nevent temporal reasoning.\nWe brieﬂy summarize our contributions. 1) We\npropose ECONET, a novel continual pre-training\nframework that integrates targeted masking and\ncontrastive loss for event temporal reasoning. 2)\nOur training objectives effectively learn from the\ntargeted masked samples and inject richer event\ntemporal knowledge in PTLMs, which leads to\nstronger ﬁne-tuning performances over ﬁve widely\nused event temporal commonsense tasks. In most\ntarget tasks, ECONET achieves SOTA results in\ncomparison with existing methods. 3) Compared\nwith full-scale pre-training, ECONET requires a\nmuch smaller amount of training data and can cope\nwith various PTLMs such as BERT and RoBERTa.\n4) In-depth analysis shows that ECONET success-\nfully transfers knowledge in terms of textual cues\nof event triggers and relations into the target tasks,\nparticularly under low-resource settings.\n2 Method\nOur proposed method aims at addressing the issue\nin vanilla PTLMs that event triggers and tempo-\nral indicators are not adequately attended for our\ndownstream event reasoning tasks. To achieve this\ngoal, we propose to replace the random masking in\nPTLMs with a targeted masking strategy designed\nspeciﬁcally for event triggers and temporal indi-\ncators. We also propose a continual pre-training\nmethod with mask prediction and contrastive loss\nthat allows models to effectively learn from the\ntargeted masked samples. The beneﬁts of our\nmethod are manifested by stronger ﬁne-tuning per-\nformances over downstream ERE and MRC tasks.\nOur overall approach ECONET consists of three\ncomponents. 1) Creating targeted self-supervised\ntraining data by masking out temporal indicators\nand event triggers in the input texts; 2) leveraging\nmask prediction and contrastive loss to continually\ntrain PTLMs, which produces an event temporal\nknowledge aware language model; 3) ﬁne-tuning\nthe enhanced language model on downstream ERE\nand MRC datasets. We will discuss each of these\ncomponents in the following subsections.\n2.1 Pre-trained Masked Language Models\nThe current PTLMs such as BERT (Devlin et al.,\n2018) and RoBERTa (Liu et al., 2019) follow a\nrandom masking strategy. Figure 1 shows such\nan example where random tokens / words are\nmasked from the input sentences. More formally,\nlet x = [x1,...,x n] be a sequence of input tokens\nand xm\nt ∈xm represents random masked tokens.\nThe per-sample pre-training objective is to predict\nthe identity (xt) of xm\nt with a cross-entropy loss,\nLMLM = −\n∑\nxm\nt ∈xm\nI[xm\nt = xt] log(p(xm\nt |x)) (1)\n5369\nFigure 2: The proposed generator-discriminator ( ECONET) architecture for event temporal reasoning. The upper\nblock is the mask prediction task for temporal indicators and the bottom block is the mask prediction task for\nevents. Both generators and the discriminator share the same representations.\nNext, we will discuss the design and creation of\ntargeted masks, training objectives and ﬁne-tuning\napproaches for different tasks.\n2.2 Targeted Masks Creation\nTemporal Masks. We ﬁrst compile a lexicon of\n40 common temporal indicators listed in the Ta-\nble 1 based on previous error analysis and expert\nknowledge in the target tasks. Those indicators in\nthe [before], [after] and [during] categories can\nbe used to represent the most common temporal\nrelations between events. The associated words\nin each of these categories are synonyms of each\nother. Temporal indicators in the [past], [future],\n[beginning] and [ending] categories probably do\nnot represent pairwise event relations directly, but\npredicting these masked tokens may still be helpful\nfor models to understand time anchors and hence\nfacilitates temporal reasoning.\nWith the temporal lexicon, we conduct string\nmatches over the 20-year’s New York Times news\narticles 2 and obtain over 10 million 1-2 sentence\npassages that contain at least 1 temporal indicators.\nFinally, we replace each of the matched temporal\nindicators with a ⟨mask⟩token. The upper block\nin Figure 2 shows two examples where “following”\nand “after” are masked from the original texts.\nEvent Masks. We build highly accurate event\ndetection models (Han et al., 2019c; Zhang et al.,\n2021) to automatically label event trigger words\nin the 10 million passages mentioned above. Simi-\nlarly, we replace these events with ⟨mask⟩tokens.\nThe bottom block in Figure 2 shows two examples\nwhere events “transfer” and “resumed” are masked\nfrom the original texts.\n2NYT news articles are public from 1987-2007.\n2.3 Generator for Mask Predictions\nTo learn effectively from the targeted samples, we\ntrain two generators with shared representations to\nrecover temporal and event masks.\nTemporal Generator. The per-sample temporal\nmask prediction objective is computed using cross-\nentropy loss,\nLT = −\n∑\nxT\nt ∈xT\nI[xT\nt = xt] log(p(xT\nt |x)) (2)\nwhere p(xT\nt |x) = Softmax (fT(hG(x)t)) and\nxT\nt ∈xT is a masked temporal indicator. hG(x) is\nx’s encoded representation using a transformer and\nfT is a linear layer module that maps the masked\ntoken representation into label space T consisting\nof the 40 temporal indicators.\nEvent Generator. The per-sample event mask\nprediction objective is also computed using cross-\nentropy loss,\nLE= −\n∑\nxE\nt ∈xE\nI[xE\nt = xt] log(p(xE\nt |x)) (3)\nwhere p(xE\nt |x) = Softmax (fE(hG(x)t)) and\nxE\nt ∈xEare masked events. hG(x) is the shared\ntransformer encoder as in the temporal generator\nand fE is a linear layer module that maps the\nmasked token representation into label space E\nwhich is a set of all event triggers in the data.\n2.4 Discriminator for Contrastive Learning\nWe incorporate a discriminator that provides addi-\ntional feedback on mask predictions, which helps\ncorrect errors made by the generators.\nContrastive Loss. For a masked token xt, we\ndesign a discriminator to predict whether the re-\ncovered token by the mask prediction is original\n5370\nor corrupted. As shown in Figure 2, “ following”\nand “resumed” are predicted correctly, so they are\nlabeled as original whereas “during” and “run” are\nincorrectly predicted and labeled ascorrupted. We\ntrain the discriminator with a contrastive loss,\nLD = −\n∑\nxt∈M\nylog(D(xt|x)) + (1−y) log(1−D(xt|x))\nwhere M = xE ∪ xT and D(xt|x) =\nSigmoid(fD(hD(x)t)) and yis a binary indicator\nof whether a mask prediction is correct or not. hD\nshares the same transformer encoder with hG.\nPerturbed Samples. Our mask predictions fo-\ncus on temporal and event tokens, which are easier\ntasks than the original mask predictions in PTLMs.\nThis could make the contrastive loss not so pow-\nerful as training a good discriminator requires rel-\natively balanced original and corrupted samples.\nTo deal with this issue, for r% of the generator’s\noutput, instead of using the recovered tokens, we\nreplace them with a token randomly sampled from\neither the temporal lexicon or the event vocabu-\nlary. We ﬁx r= 50 to make original and corrupted\nsamples nearly balanced.\n2.5 Joint Training\nTo optimize the combining impact of all compo-\nnents in our model, the ﬁnal training loss calcu-\nlates the weighted sum of each individual loss,\nL= LT + αLE+ βLD, where αand βare hyper-\nparameters that balance different training objec-\ntives. The temporal and event masked samples are\nassigned a unique identiﬁer (1 for temporal, 0 for\nevent) so that the model knows which linear layers\nto feed the output of transformer into. Our over-\nall generator-discriminator architecture resembles\nELECTRA (Clark et al., 2020). However, our pro-\nposed method differs from this work in 1) we use\ntargeted masking strategy as opposed to random\nmasks; 2) both temporal and event generators and\nthe discriminator, i.e. hG and hD share the hidden\nrepresentations, but we allow task-speciﬁc ﬁnal lin-\near layers fT, fEand fD; 3) we do not train from\nscratch and instead continuing to train transformer\nparameters provided by PTLMs.\n2.6 Fine-tuning on Target Tasks\nAfter training with ECONET, we ﬁne-tune the up-\ndated MLM on the downstream tasks. ERE sam-\nples can be denoted as [P,ei,ej,ri,j], where P is\nthe passage and (ei,ej) is a pair of event trigger\ntokens in P. As Figure 3a shows, we feed(P,ei,ej)\n(a) ERE\n(b) QA: TORQUE\n(c) QA: MCTACO\nFigure 3: Target ERE and QA task illustrations.\ninto an MLM (trained with ECONET). Following\nthe setup of Han et al. (2019a) and Zhang et al.\n(2021), we concatenate the ﬁnal event representa-\ntions vi,vj associated with (ei,ej) to predict tem-\nporal relation ri,j. The relation classiﬁer is imple-\nmented by a multi-layer perceptron (MLP).\nMRC/QA samples can be denoted as [P,Q,A ],\nwhere Q represents a question and A denotes an-\nswers. Figure 3b illustrates an extractive QA task\nwhere we feed the concatenated [P,Q] into an\nMLM. Each token xi ∈P has a label with 1 in-\ndicating xi ∈Aand 0 otherwise. The token clas-\nsiﬁer implemented by MLP predicts labels for all\nxi. Figure 3c illustrates another QA task where A\nis a candidate answer for the question. We feed\nthe concatenated [P,Q,A ] into an MLM and the\nbinary classiﬁer predicts a 0/1 label of whether A\nis a true statement for a given question.\n3 Experimental Setup\nIn this section, we describe details of implement-\ning ECONET, datasets and evaluation metrics, and\ndiscuss compared methods reported in Section 4.\n3.1 Implementation Details\nEvent Detection Model. As mentioned brieﬂy\nin Section 2, we train a highly accurate event pre-\ndiction model to mask event (triggers). We experi-\nmented with two models using event annotations\nin TORQUE (Ning et al., 2020) and TB-Dense\n(Chambers et al., 2014). These two event annota-\ntions both follow previous event-centric reasoning\n5371\nresearch by using a trigger word (often a verb or\nan noun that most clearly describes the event’s oc-\ncurrence) to represent an event (UzZaman et al.,\n2013; Glavaš et al., 2014; O’Gorman et al., 2016).\nIn both cases, we ﬁne-tune RoBERTa LARGE on\nthe train set and select models based on the perfor-\nmance on the dev set. The primary results shown\nin Table 2 uses TORQUE ’s annotations, but we\nconduct additional analysis in Section 4 to show\nboth models produce comparable results.\nContinual Pretraining. We randomly selected\nonly 200K out of 10 million samples to speed up\nour experiments and found the results can be as\ngood as using a lot more data. We used half of\nthese 200K samples for temporal masked samples\nand the other half for the event masked samples.\nWe ensure none of these sample passages overlap\nwith the target test data. To keep the mask tokens\nbalanced in the two training samples, we masked\nonly 1 temporal indicator or 1 event (closest to the\ntemporal indicator). We continued to train BERT\nand RoBERTa up to 250K steps with a batch size of\n8. The training process takes 25 hours on a single\nGeForce RTX 2080 GPU with 11G memory. Note\nthat our method requires much fewer samples and\nis more computation efﬁcient than the full-scale\npre-training of language models, which typically\nrequires multiple days of training on multiple large\nGPUs / TPUs.\nFor the generator only models reported in Ta-\nble 2, we excluded the contrastive loss, trained\nmodels with a batch size of 16 to fully utilize GPU\nmemories. We leveraged the dev set of TORQUE\nto ﬁnd the best hyper-parameters.\nFine-tuning. Dev set performances were used\nfor early-stop and average dev performances over\nthree randoms seeds were used to pick the best\nhyper-parameters. Note that test set for the target\ntasks were never observed in any of the training\nprocess and their performances are reported in Ta-\nble 2. All hyper-parameter search ranges can be\nfound in Appendix C.\n3.2 Datasets\nWe evaluate our approach on ﬁve datasets concern-\ning temporal ERE and MRC/QA. We brieﬂy de-\nscribe these data below and list detailed statistics\nin Appendix A.\nERE Datasets. TB-Dense (Chambers et al.,\n2014), MATRES (Ning et al., 2018) and RED\n(O’Gorman et al., 2016) are all ERE datasets. Their\nsamples follow the input format described in Sec-\ntion 2.6 where a pair of event (triggers) together\nwith their context are provided. The task is to\npredict pairwise event temporal relations. The dif-\nferences are how temporal relation labels are de-\nﬁned. Both TB-Dense and MATRES leverage a\nVAGUE label to capture relations that are hard to\ndetermine even by humans, which results in denser\nannotations than RED . RED contains the most\nﬁne-grained temporal relations and thus the lowest\nsample/relation ratio. MATRES only considers start\ntime of events to determine their temporal order,\nwhereas TB-Dense and RED consider start and end\ntime, resulting in lower inter-annotator agreement.\nTORQUE (Ning et al., 2020) is an MRC/QA\ndataset where annotators ﬁrst identify event trig-\ngers in given passages and then ask questions re-\ngarding event temporal relations (ordering). Cor-\nrect answers are event trigger words in passages.\nTORQUE can be considered as reformulating tem-\nporal ERE tasks as an MRC/QA task. Therefore,\nboth ERE datasets and TORQUE are highly cor-\nrelated with our continual pre-training objectives\nwhere targeted masks of both events and temporal\nrelation indicators are incorporated.\nMCTACO (Zhou et al., 2019) is another MR-\nC/QA dataset, but it differs from TORQUE in\n1) events are not explicitly identiﬁed; 2) answers\nare statements with true or false labels; 3) ques-\ntions contain broader temporal commonsense re-\ngarding not only temporal ordering, but also event\nfrequency, during and typical time that may not be\ndirectly helpful for reasoning temporal relations.\nFor example, knowing how often a pair of events\nhappen doesn’t help us ﬁgure out which event hap-\npens earlier. Since our continual pre-training fo-\ncuses on temporal relations, MCTACO could the\nleast compatible dataset in our experiments.\n3.3 Evaluation Metrics\nThree metrics are used to evaluate the ﬁne-tuning\nperformances.\nF1: for TORQUE and MCTACO , we follow\nthe data papers (Ning et al., 2020) and (Zhou et al.,\n2019) to report macro average of each question’s\nF1 score. For TB-Dense, MATRES and RED , we\nreport standard micro-average F1 scores to be con-\nsistent with the baselines.\nExact-match (EM): for both MRC datasets, EM\n= 1 if answer predictions match perfectly with gold\nannotations; otherwise, EM = 0.\n5372\nTORQUE MCTACO TB-DenseMATRES RED\nMethods F1 EM C F1 EM F1 F1 F1\nTacoLM 65.4(±0.8) 37.1(±1.0) 21.0(±0.8)69.3(±0.6) 40.5(±0.5)64.8(±0.7)70.9(±0.3)40.3(±1.7)\nBERTLARGE 70.6(±1.2) 43.7(±1.6) 27.5(±1.2)70.3(±0.9) 43.2(±0.6)62.8(±1.4)70.5(±0.9)39.4(±0.6)\n+ECONET 71.4(±0.7) 44.8(±0.4) 28.5(±0.5)69.2(±0.9) 42.3(±0.5)63.0(±0.6)70.4(±0.9)40.2(±0.8)\nRoBERTaLARGE75.1(±0.4) 49.6(±0.5) 35.3(±0.8)75.5(±1.0) 50.4(±0.9)62.8(±0.3)78.3(±0.5)39.4(±0.4)\n+ Generator 75.8(±0.4) 51.2(±1.1) 35.8(±0.9)75.1(±1.4) 50.2(±1.2)65.2(±0.6)77.0(±0.9)41.0(±0.6)\n+ECONET 76.1(±0.2) 51.6(±0.4) 36.8(±0.2)76.3(±0.3) 52.8(±1.9)64.8(±1.4)78.8(±0.6)42.8(±0.7)\nECONET(best) 76.3 52.0 37.0 76.8 54.7 66.8 79.3 43.8\nCurrent SOTA 75.2∗ 51.1 34.5 79.5† 56.5 66.7†† 80.3‡ 34.0‡‡\nTable 2: Overall experimental results. Refer to Section 3.4 for naming conventions. The SOTA performances for\nTORQUE ∗are provided by Ning et al. (2020) and the numbers are average over 3 random seeds. The SOTA\nperformances for MCTACO †are provided by Pereira et al. (2020); TB-Dense ††and M ATRES ‡by Zhang et al.\n(2021) and RED ‡‡by Han et al. (2019b). †, ††, ‡and ‡‡only report the best single model results, and to make fair\ncomparisons with these baselines, we report both average and best single model performances. TacoLM baseline\nuses the provided and recommended checkpoint for extrinsic evaluations.\nEM-consistency (C): in TORQUE , some ques-\ntions can be clustered into the same group due to\nthe data collection process. This metric reports\nthe average EM score for a group as opposed to a\nquestion in the original EM metrics.\n3.4 Compared Methods\nWe compare several pre-training methods with\nECONET: 1) RoBERTaLARGE is the origi-\nnal PTLM and we ﬁne-tune it directly on tar-\nget tasks; 2) RoBERTaLARGE + ECONET is\nour proposed continual pre-training method; 3)\nRoBERTaLARGE + Generator only uses the gen-\nerator component in continual pre-training; 4)\nRoBERTaLARGE + random mask keeps the orig-\ninal PTLMs’ objectives and replaces the targeted\nmasks in ECONET with randomly masked to-\nkens. The methods’ names for continual pre-\ntraining BERTLARGE can be derived by replacing\nRoBERTaLARGE with BERTLARGE.\nWe also ﬁne-tune pre-trained TacoLM on target\ndatasets. The current SOTA systems we compare\nwith are provided by Ning et al. (2020), Pereira\net al. (2020), Zhang et al. (2021) and Han et al.\n(2019b). More details are presented in Section 4.1.\n4 Results and Analysis\nAs shown in Table 2, we report two baselines.\nThe ﬁrst one, TacoLM is a related work that fo-\ncuses on event duration, frequency and typical time.\nThe second one is the current SOTA results re-\nported to the best of the authors’ knowledge. We\nalso report our own implementations of ﬁne-tuning\nBERTLARGE and RoBERTaLARGE to compare\nfairly with ECONET. Unless pointing out speciﬁ-\ncally, all gains mentioned in the following sections\nare in the unit of absolute percentage.\n4.1 Comparisons with Existing Systems\nTORQUE. The current SOTA system reported\nin Ning et al. (2020) ﬁne-tunes RoBERTaLARGE\nand our own ﬁne-tuned RoBERTaLARGE achieves\non-par F1, EM and C scores. The gains of\nRoBERTaLARGE + ECONET against the current\nSOTA performances are 0.9%, 0.5% and 2.3% per\nF1, EM and C metrics.\nMCTACO. The current SOTA system ALICE\n(Pereira et al., 2020) also uses RoBERTa LARGE\nas the text encoder, but leverages adversarial at-\ntacks on input samples. ALICE achieves 79.5%\nand 56.5% perF1 and EM metrics on the test set for\nthe best single model, and the best performances\nfor RoBERTaLARGE + ECONET are 76.8% and\n54.7% per F1 and EM scores, which do not outper-\nform ALICE. This gap can be caused by the fact\nthat the majority of samples in MCTACO reason\nabout event frequency, duration and time, which\nare not directly related to event temporal relations.\nTB-Dense + M ATRES . The most recent SOTA\nsystem reported in Zhang et al. (2021) uses both\nBERTLARGE and RoBERTaLARGE as text en-\ncoders, but leverages syntactic parsers to build large\ngraphical attention networks on top of PTLMs.\nRoBERTaLARGE + ECONET’s ﬁne-tuning perfor-\nmances are essentially on-par with this work with-\nout additional parameters. For TB-Dense, our best\nmodel outperforms Zhang et al. (2021) by 0.1%\nwhile for MATRES , our best model underperforms\nby 1.0% per F1 scores.\nRED. The current SOTA system reported in Han\net al. (2019b) uses BERTBASE as word represen-\ntations (no ﬁnetuning) and BiLSTM as feature ex-\ntractor. The single best model achieves 34.0% F1\nscore and RoBERTaLARGE + ECONET is 9.8%\nhigher than the baseline.\n5373\n4.2 The Impact of ECONET\nOverall Impact. ECONET in general works bet-\nter than the original RoBERTaLARGE across 5 dif-\nferent datasets, and the improvements are more\nsalient in TORQUE with 1.0%, 2.0% and 1.5%\ngains per F1, EM and C scores, in MCTACO with\n2.4% lift over the EM score, and in TB-Dense and\nRED with 2.0% and 3.4% improvements respec-\ntively over F1 scores. We observe that the improve-\nments of ECONET over BERTLARGE is smaller\nand sometimes hurts the ﬁne-tuning performances.\nWe speculate this could be related to the property\nthat BERT is less capable of handling temporal\nreasoning tasks, but we leave more rigorous inves-\ntigations to future research.\nImpact of Contrastive Loss. Comparing the av-\nerage performances of continual pre-training with\ngenerator only and with ECONET (generator + dis-\ncriminator), we observe that generator alone can\nimprove performances of RoBERTa LARGE in 3\nout of 5 datasets. However, except for TB-Dense,\nECONET is able to improve ﬁne-tuning perfor-\nmances further, which shows the effectiveness of\nusing the contrastive loss.\nSigniﬁcance Tests. As current SOTA models\nare either not publicly available or under-perform\nRoBERTaLARGE, we resort to testing the statisti-\ncal signiﬁcance of the best single model between\nECONET and RoBERTaLARGE. Table 8 in the\nappendix lists all improvements’ p-values per Mc-\nNemar’s test (McNemar, 1947).MATRES appears\nto be the only one that is not statistically signiﬁcant.\n4.3 Impact of Event Models\nEvent trigger deﬁnitions have been consistent in\nprevious event temporal datasets (O’Gorman et al.,\n2016; Chambers et al., 2014; Ning et al., 2020).\nTrigger detection models built on TORQUE and\nTB-Dense both achieve > 92% F1 scores and >\n95% precision scores. For the 100K pre-training\ndata selected for event masks, we found an 84.5%\noverlap of triggers identiﬁed by both models. We\nfurther apply ECONET trained on both event mask\ndata to the target tasks and achieve comparable\nperformances shown in Table 10 of the appendix.\nThese results suggest that the impact of different\nevent annotations is minimal and triggers detected\nin either model can generalize to different tasks.\n4.4 Additional Ablation Studies\nTo better understand our proposed model, we exper-\niment with additional continual training methods\nand compare their ﬁne-tuning performances.\nTORQUETB-DRED\nMethods F1 EM C F1 F1\nRoBERTaLARGE75.1 49.6 35.362.8 39.4\n+ random mask74.9 49.5 35.158.7 38.3\n+ECONET 76.1 51.6 36.864.8 42.8\nTable 3: Fine-tuning performances with different pre-\ntraining methods. All numbers are average over 3 ran-\ndom seeds. Std. Dev. ≥1% is underlined.\nRandom Masks. As most target datasets we use\nare in the news domain, to study the impact of\npotential domain-adaption, we continue to train\nPTLMs with the original objective on the same\ndata using random masks. To compare fairly with\nthe generator and ECONET, we only mask 1 token\nper training sample. The search range of hyper-\nparameters is the same as in Section 3. As Table 3\nand 11 (appendix) show, continual pre-training\nwith random masks, in general, does not improve\nand sometimes hurt ﬁne-tuning performances com-\npared with ﬁne-tuning with original PTLMs. We\nhypothesize that this is caused by masking a smaller\nfraction (1 out of≈50 average) tokens than the orig-\ninal 15%. RoBERTaLARGE + ECONET achieves\nthe best ﬁne-tuning results across the board.\nFull Train Data 10% Train Data\nRoBERTa∆ ∆% RoBERTa∆ ∆%\nTORQUE75.1 +1.0 +1.3% 59.7 +7.2 +12.1%\nMCTACO75.5 +0.8 +1.1% 44.0 +5.6 +12.7%\nTB-Dense62.8 +2.0 +3.2% 48.8 +2.8 +5.7%\nMATRES 78.3 +0.5 +1.3% 71.0 +2.4 +3.4%\nRED 39.4 +2.4+6.0% 27.2 +1.8 +6.6%\nTable 4: RoBERTaLARGE + ECONET’s improvements\nover RoBERTaLARGE using full train data v.s. 10% of\ntrain data. ∆ indicates absolute points improvements\nwhile ∆% indicates relative gains per F1 scores.\n4.5 Fine-tuning under Low-resource Settings\nIn Table 4, we compare the improvements of\nﬁne-tuning RoBERTa LARGE + ECONET over\nRoBERTaLARGE using full and 10% of the training\ndata. Measured by both absolute and relative per-\ncentage gains, the majority of the improvements are\nmuch more signiﬁcant under low-resource settings.\nThis suggests that the transfer of event temporal\nknowledge is more salient when data is scarce. We\nfurther show ﬁne-tuning performance comparisons\nusing different ratios of the training data in Fig-\nure 6a-6b in the appendix. The results demonstrate\nthat ECONET can outperform RoBERTaLARGE\nconsistently when ﬁne-tuning TORQUE and RED.\n4.6 Attention Scores on Temporal Indicators\nIn this section, we attempt to show explicitly how\nECONET enhances MLMs’ attentions on temporal\nindicators for downstream tasks. As mentioned in\n5374\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nLayers\n0\n5\n10\n15\n20\n25\n30Average Attention Scores\nECONET [before]\nRoBERTa [before]\nECONET [after]\nRoBERTa [after]\nECONET [during]\nRoBERTa [during]\nFigure 4: Cumulative attention score comparisons be-\ntween RoBERTaLARGE and ECONET on TB-Dense\ntest data. All numbers are multiplied by 100 and aver-\naged over 3 random seeds for illustration clarity.\nSec. 2.6, for a particular ERE task (e.g. TB-Dense),\nwe need to predict the temporal relations between\na pair of event triggers ei,ej ∈Pi,j with associ-\nated vector representations vl,h\ni ,vl,h\nj ,l ∈L,h ∈H\nin an MLM. L and H are the number of layers\nand attention heads respectively. We further use\nTm ∈T to denote a temporal indicator category\nlisted in Table 1, andtm,n ∈Tm denote a particular\ntemporal indicator. If we let attn(vl,h\ni ,vl,h\nx ) repre-\nsents the attention score between an event vector\nand any other hidden vectors, we can aggregate the\nper-layer attention score between ei and tm,n as,\nal\ni,tm,n = 1\nH\n∑H\nh attn(vl,h\ni ,vl,h\ntm,n). Similarly, we\ncan compute al\nj,tm,n. The ﬁnal per-layer attention\nscore for (ei,ej) is al\ntm,n = 1\n2\n(\nal\ni,tm,n + al\nj,tm,n\n)\n.\nTo compute the attention score for the Tm cate-\ngory, we take the average of {al\ntm,n | ∀tm,n ∈\nTm and ∀tm,n ∈Pi,j}. Note we assume a tempo-\nral indicator is a single token to simplify notations\nabove; for multiple-token indicators, we take the\naverage of attn(vl,h\ni ,vl,h\nx∈tm,n).\nFigure 4 shows the cumulative attention scores\nfor temporal indicator categories, [before], [af-\nter] and [during] in ascending order of model\nlayers. We observe that the attention scores\nfor RoBERTaLARGE and ECONET align well\non the bottom layers, but ECONET outweighs\nRoBERTaLARGE in middle to top layers. Previ-\nous research report that upper layers of pre-trained\nlanguage models focus more on complex semantics\nas opposed to shallow surface forms or syntax on\nthe lower layers (Tenney et al., 2019; Jawahar et al.,\n2019). Thus, our ﬁndings here show another piece\nof evidence that targeted masking is effective at\ncapturing temporal indicators, which could facili-\ntate semantics tasks including temporal reasoning.\n4.7 Temporal Knowledge Injection\nWe hypothesize in the introduction that vanilla\nPTLMs lack special attention to temporal indica-\nBEFORE AFTER INCLUDES IS_INCLUDED VAGUE\n[before]\n[after]\n[during]\n[future]\n-23.09 -27.11 -2.36 -6.06 1.99\n-22.37 -27.11 -2.90 -4.76 2.26\n-21.67 -25.27 -2.67 -3.81 1.36\n-23.25 -21.02 -2.65 -6.35 2.15\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n(a) Random Mask - RoBERTaLARGE\nBEFORE AFTER INCLUDES IS_INCLUDED VAGUE\n[before]\n[after]\n[during]\n[future]\n10.53 3.90 -3.03 2.27 2.06\n8.47 8.54 -2.90 -3.10 1.92\n8.51 6.44 -2.81 1.65 0.70\n10.44 2.99 -3.18 2.66 1.27\n10.0\n 7.5\n 5.0\n 2.5\n 0.0 2.5 5.0 7.5 10.0\n(b) ECONET - RoBERTaLARGE\nFigure 5: Performance (F1 score) differences by tempo-\nral indicator categories and label classes in TB-Dense.\nFine-tuning on 10% TB-Dense training data.\ntors and events, and our proposed method addresses\nthis issue by a particular design of mask prediction\nstrategy and a discriminator that is able to distin-\nguish reasonable events and temporal indicators\nfrom noises. In this section, we show more details\nof how such a mechanism works.\nThe heat maps in Figure 5 calculate the\nﬁne-tuning performance differences between 1)\nRoBERTaLARGE and continual pre-training with\nrandom masks (Figure 5a); and 2) between\nRoBERTaLARGE and ECONET (Figure 5b). Each\ncell shows the difference for each label class in\nTB-Dense conditional on samples’ input passage\ncontaining a temporal indicator in the categories\nspeciﬁed in Table 1. Categories with less than 50\nsample matches are excluded from the analysis.\nIn Figure 5a, the only gains come from VAGUE,\nwhich is an undetermined class in TB-Dense to han-\ndle unclear pairwise event relations. This shows\nthat continual pre-training with random masks\nworks no better than original PTLMs to leverage\nexisting temporal indicators in the input passage\nto distinguish positive temporal relations from un-\nclear ones. On the other hand, in Figure 5b, having\ntemporal indicators in general beneﬁts much more\nfor BEFORE, AFTER, IS_INCLUDED labels. The\nonly exception is INCLUDES, but it is a small class\nwith only 4% of the data.\nMore interestingly, notice the diagonal cells, i.e.\n([before], BEFORE), ([after], AFTER) and ([dur-\ning], INCLUDES) have the largest values in the\nrespective columns. These results are intuitive as\ntemporal indicators should be most beneﬁcial for\ntemporal relations associated with their categories.\n5375\nCombining these two sets of results, we provide\nadditional evidence that ECONET helps PTLMs\nbetter capture temporal indicators and thus results\nin stronger ﬁne-tuning performances.\nOur ﬁnal analysis attempts to show why discrim-\ninator helps. We feed 1K unused masked samples\ninto the generator of the best ECONET in Table 2\nto predict either the masked temporal indicators\nor masked events. We then examine the accuracy\nof the discriminator for correctly and incorrectly\npredicted masked tokens. As shown in Table 12\nof the appendix, the discriminator aligns well with\nthe event generator’s predictions. For the temporal\ngenerator, the discriminator disagrees substantially\n(82.2%) with the “incorrect” predictions, i.e. the\ngenerator predicts a supposedly wrong indicator,\nbut the discriminator thinks it looks original.\nTo understand why, we randomly selected 50 dis-\nagreed samples and found that 12 of these “incor-\nrect” predictions fall into the same temporal indica-\ntor group of the original ones and 8 of them belong\nto the related groups in Table 1. More details and\nexamples can be found in Table 13 in the appendix.\nThis suggests that despite being nearly perfect re-\nplacements of the original masked indicators, these\n40% samples are penalized as wrong predictions\nwhen training the generator. The discriminator, by\ndisagreeing with the generator, provides opposing\nfeedback that trains the overall model to better cap-\nture indicators with similar temporal signals.\n5 Related Work\nLanguage Model Pretraining. Since the break-\nthrough of BERT (Devlin et al., 2018), PTLMs\nhave become SOTA models for a variety of\nNLP applications. There have also been sev-\neral modiﬁcations/improvements built on the orig-\ninal BERT model. RoBERTa (Liu et al., 2019)\nremoves the next sentence prediction in BERT\nand trains with longer text inputs and more\nsteps. ELECTRA (Clark et al., 2020) proposes a\ngenerator-discriminator architecture, and addresses\nthe sample-inefﬁciency issue in previous PTLMs.\nRecent research explored methods to continue\nto train PTLMs so that they can adapt better to\ndownstream tasks. For example, TANDA (Garg\net al., 2019) adopts an intermediate training on\nmodiﬁed Natural Questions dataset (Kwiatkowski\net al., 2019) so that it performs better for the An-\nswer Sentence Selection task. Zhou et al. (2020b)\nproposed continual training objectives that require\na model to distinguish natural sentences from those\nwith concepts randomly shufﬂed or generated by\nmodels, which enables language models to capture\nlarge-scale commonsense knowledge.\nEvent Temporal Reasoning. There has been a\nsurge of attention to event temporal reasoning re-\nsearch recently. Some noticeable datasets include\nERE samples: TB-Dense (Chambers et al., 2014),\nMATRES (Ning et al., 2018) and RED (O’Gorman\net al., 2016). Previous SOTA systems on these data\nleveraged PTLMs and structured learning (Han\net al., 2019c; Wang et al., 2020; Zhou et al., 2020c;\nHan et al., 2020) and have substantially improved\nmodel performances, though none of them tack-\nled the issue of lacking event temporal knowledge\nin PTLMs. TORQUE (Ning et al., 2020) and\nMCTACO (Zhou et al., 2019) are recent MRC\ndatasets that attempt to reason about event tem-\nporal relations using natural language rather than\nERE formalism.\nZhou et al. (2020a) and Zhao et al. (2020) are\ntwo recent works that attempt to incorporate event\ntemporal knowledge in PTLMs. The formal one\nfocuses on injecting temporal commonsense with\ntargeted event time, frequency and duration masks\nwhile the latter one leverages distantly labeled pair-\nwise event temporal relations, masks before/after\nindicators, and focuses on ERE application only.\nOur work differs from them by designing a tar-\ngeted masking strategy for event triggers and com-\nprehensive temporal indicators, proposing a con-\ntinual training method with mask prediction and\ncontrastive loss, and applying our framework on a\nbroader range of event temporal reasoning tasks.\n6 Conclusion and Future Work\nIn summary, we propose a continual training frame-\nwork with targeted mask prediction and contrastive\nloss to enable PTLMs to capture event temporal\nknowledge. Extensive experimental results show\nthat both the generator and discriminator compo-\nnents can be helpful to improve ﬁne-tuning perfor-\nmances over 5 commonly used data in event tempo-\nral reasoning. The improvements of our methods\nare much more pronounced in low-resource set-\ntings, which points out a promising direction for\nfew-shot learning in this research area.\nAcknowledgments\nThis work is supported by the Intelligence Ad-\nvanced Research Projects Activity (IARPA), via\nContract No. 2019-19051600007 and DARPA un-\nder agreement number FA8750-19-2-0500.\n5376\nReferences\nSteven Bethard, Leon Derczynski, Guergana Savova,\nJames Pustejovsky, and Marc Verhagen. 2015.\nSemEval-2015 task 6: Clinical TempEval. In Pro-\nceedings of the 9th International Workshop on Se-\nmantic Evaluation (SemEval 2015), pages 806–814,\nDenver, Colorado. Association for Computational\nLinguistics.\nSteven Bethard, Guergana Savova, Wei-Te Chen, Leon\nDerczynski, James Pustejovsky, and Marc Verhagen.\n2016. SemEval-2016 task 12: Clinical TempEval.\nIn Proceedings of the 10th International Workshop\non Semantic Evaluation (SemEval-2016) , pages\n1052–1062, San Diego, California. Association for\nComputational Linguistics.\nSteven Bethard, Guergana Savova, Martha Palmer,\nand James Pustejovsky. 2017. SemEval-2017 task\n12: Clinical TempEval. In Proceedings of the\n11th International Workshop on Semantic Evalua-\ntion (SemEval-2017) , pages 565–572, Vancouver,\nCanada. Association for Computational Linguistics.\nNathanael Chambers, Taylor Cassidy, Bill McDowell,\nand Steven Bethard. 2014. Dense event ordering\nwith a multi-pass architecture. In ACL.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. Electra: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In International Conference on Learn-\ning Representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nSiddhant Garg, Thuy Vu, and Alessandro Moschitti.\n2019. Tanda: Transfer and adapt pre-trained trans-\nformer models for answer sentence selection.\nGoran Glavaš, Jan Šnajder, Marie-Francine Moens, and\nParisa Kordjamshidi. 2014. HiEve: A corpus for\nextracting event hierarchies from news stories. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14),\npages 3678–3683, Reykjavik, Iceland. European\nLanguage Resources Association (ELRA).\nSeraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph\nWeischedel, and Nanyun Peng. 2020. Content plan-\nning for neural story generation with aristotelian\nrescoring. In the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4319–4338.\nRujun Han, I-Hung Hsu, Mu Yang, Aram Galstyan,\nRalph Weischedel, and Nanyun Peng. 2019a. Deep\nstructured neural network for event temporal rela-\ntion extraction. In Proceedings of the 23rd Confer-\nence on Computational Natural Language Learning\n(CoNLL), pages 666–106, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nRujun Han, Mengyue Liang, Bashar Alhafni, and\nNanyun Peng. 2019b. Contextualized word embed-\ndings enhanced event temporal relation extraction\nfor story understanding. In 2019 Annual Conference\nof the North American Chapter of the Association\nfor Computational Linguistics (NAACL-HLT 2019),\nWorkshop on Narrative Understanding.\nRujun Han, Qiang Ning, and Nanyun Peng. 2019c.\nJoint event and temporal relation extraction with\nshared representations and structured prediction. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 434–\n444, Hong Kong, China. Association for Computa-\ntional Linguistics.\nRujun Han, Yichao Zhou, and Nanyun Peng. 2020. Do-\nmain knowledge empowered structured neural net\nfor end-to-end event temporal relation extraction. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5717–5729, Online. Association for Computa-\ntional Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nWoojeong Jin, Suji Kim, and Xiang Ren. 2020. Fore-\ncastqa: A question answering challenge for event\nforecasting. arXiv preprint arXiv:2005.00792.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nManling Li, Qi Zeng, Ying Lin, Kyunghyun Cho, Heng\nJi, Jonathan May, Nathanael Chambers, and Clare\nV oss. 2020. Connecting the dots: Event graph\nschema induction with path language modeling. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 684–695, Online. Association for Computa-\ntional Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint, arXiv:1907.11692.\nQuinn McNemar. 1947. Note on the sampling error\nof the difference between correlated proportions or\npercentages. Psychometrika, 12(2):153–157.\n5377\nNasrin Mostafazadeh, Alyson Grealish, Nathanael\nChambers, James Allen, and Lucy Vanderwende.\n2016. Caters: Causal and temporal relation scheme\nfor semantic annotation of event structures. In\nNAACL, San Diego, USA.\nQiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt\nGardner, and Dan Roth. 2020. TORQUE: A reading\ncomprehension dataset of temporal ordering ques-\ntions. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (EMNLP) , pages 1158–1172, Online. Associa-\ntion for Computational Linguistics.\nQiang Ning, Hao Wu, and Dan Roth. 2018. A multi-\naxis annotation scheme for event temporal relations.\nIn ACL. Association for Computational Linguistics.\nTim O’Gorman, Kristin Wright-Bettner, and Martha\nPalmer. 2016. Richer event description: Integrating\nevent coreference with temporal, causal and bridg-\ning annotation. In Proceedings of the 2nd Work-\nshop on Computing News Storylines (CNS 2016) ,\npages 47–56, Austin, Texas. Association for Com-\nputational Linguistics.\nLis Pereira, Xiaodong Liu, Fei Cheng, Masayuki Asa-\nhara, and Ichiro Kobayashi. 2020. Adversarial train-\ning for commonsense inference. In Proceedings of\nthe 5th Workshop on Representation Learning for\nNLP, pages 55–60, Online. Association for Compu-\ntational Linguistics.\nWeiyi Sun, Anna Rumshisky, and Ozlem Uzuner. 2013.\nEvaluating temporal relations in clinical text: 2012\ni2b2 challenge.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nNaushad UzZaman, Hector Llorens, Leon Derczyn-\nski, James Allen, Marc Verhagen, and James Puste-\njovsky. 2013. SemEval-2013 task 1: TempEval-3:\nEvaluating time expressions, events, and temporal\nrelations. In Second Joint Conference on Lexical\nand Computational Semantics (*SEM), Volume 2:\nProceedings of the Seventh International Workshop\non Semantic Evaluation (SemEval 2013) , pages 1–\n9, Atlanta, Georgia, USA. Association for Computa-\ntional Linguistics.\nHaoyu Wang, Muhao Chen, Hongming Zhang, and\nDan Roth. 2020. Joint constrained learning for\nevent-event relation extraction. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 696–706,\nOnline. Association for Computational Linguistics.\nLili Yao, Nanyun Peng, Weischedel Ralph, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nThe Thirty-Third AAAI Conference on Artiﬁcial In-\ntelligence (AAAI-19).\nShuaicheng Zhang, Lifu Huang, and Qiang Ning. 2021.\nExtracting temporal event relation with syntactic-\nguided temporal graph transformer. arXiv preprint\narXiv:arXiv:2104.09570.\nXinyu Zhao, Shih-ting Lin, and Greg Durrett. 2020. Ef-\nfective distant supervision for temporal relation ex-\ntraction. arXiv preprint arXiv:2010.12755.\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan\nRoth. 2019. “going on a vacation” takes longer\nthan “going for a walk”: A study of temporal com-\nmonsense understanding. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3363–3369, Hong Kong,\nChina. Association for Computational Linguistics.\nBen Zhou, Qiang Ning, Daniel Khashabi, and Dan\nRoth. 2020a. Temporal Common Sense Acquisition\nwith Minimal Supervision. In Proc. of the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nWangchunshu Zhou, Dong-Ho Lee, Ravi Kiran Sel-\nvam, Seyeon Lee, Bill Yuchen Lin, and Xiang Ren.\n2020b. Pre-training text-to-text transformers for\nconcept-centric common sense.\nYichao Zhou, Yu Yan, Rujun Han, J. Harry Cauﬁeld,\nKai-Wei Chang, Yizhou Sun, Peipei Ping, and Wei\nWang. 2020c. Clinical temporal relation extrac-\ntion with probabilistic soft logic regularization and\nglobal inference. arXiv preprint, arXiv:2012.08790.\n5378\nA Data Summary\nTable 5 describes basic statistics for target datasets\nused in this work. The numbers of train/dev/test\nsamples for TORQUE and MCTACO are ques-\ntion based. There is no training set provided in\nMCTACO . So we train on the dev set and re-\nport the evaluation results on the test set following\nPereira et al. (2020). The numbers of train/dev/test\nsamples for TB-Dense, MATRES and RED refer to\n(event, event, relation) triplets. The standard dev\nset is not provide by MATRES and RED , so we\nfollow the split used in Zhang et al. (2021) and Han\net al. (2019b).\nData #Train #Dev #Test #Label\nTORQUE 24,523 1,483 4,668 2\nMCTACO - 3,783 9,442 2\nTB-Dense 4,032 629 1,427 6\nMATRES 5,412 920 827 4\nRED 3,336 400 473 11\nTable 5: The numbers of samples for TORQUE refers\nto number of questions; the numbers for MCTACO\nare valid question and answer pairs; and the numbers\nof samples for TB-Dense, M ATRES and RED are all\n(event, event, relation) triplets.\nDownloading link for the (processed) continual\npretraining data is provided in the README ﬁle\nof the code package.\nB Event Detection Model\nAs mentioned brieﬂy in Secion 2, we train an\nevent prediction model using event annotations in\nTORQUE . We ﬁnetune RoBERTaLARGE on the\ntraining set and select models based on the perfor-\nmance on the dev set. The best model achieves\n>92% event prediction F1 score with >95% pre-\ncision score after just 1 epoch of training, which\nindicates that this is a highly accurate model.\nC Reproduction Checklist\nNumber of parameters. We continue to train\nBERTLARGE and RoBERTaLARGE and so the\nnumber of parameters are the same as the origi-\nnal PTLMs, which is 336M.\nHyper-parameter Search Due to computation\nconstraints, we had to limit the search range of\nhyper-parameters for ECONET. For learning rates,\nwe tried (1e−6,2e−6); for weights on the con-\ntrastive loss (β), we tried (1.0,2.0).\nBest Hyper-parameters. In Table 6 and Table 7,\nwe provide hyper-parameters for our best per-\nforming language model using RoBERTaLARGE +\nECONET and BERTLARGE + ECONET and best\nhyper-parameters for ﬁne-tuning them on down-\nstream tasks. For ﬁne-tuning on the target datasets.\nWe conducted grid search for learning rates in the\nrange of (5e−6,1e−5) and for batch size in the\nrange of (2,4,6,12). We ﬁne-tuned all models for\n10 epochs with three random seeds (5,7,23).\nMethod learning rate batch size β\nECONET 1e−6 8 1.0\nTORQUE 1e−5 12 -\nMCTACO 5e−6 4 -\nTB-Dense 5e−6 4 -\nMATRES 5e−6 2 -\nRED 5e−6 2 -\nTable 6: Hyper-parameters of our best performing LM\nwith RoBERTa LARGE + ECONET as well as best\nhyper-parameters for ﬁne-tuning on downstream tasks.\nMethod learning rate batch size β\nECONET 2e−6 8 1.0\nTORQUE 1e−5 12 -\nMCTACO 1e−5 2 -\nTB-Dense 1e−5 2 -\nMATRES 5e−6 4 -\nRED 1e−5 6 -\nTable 7: Hyper-parameters of our best performing LM\nwith BERTLARGE + ECONET as well as best hyper-\nparameters for ﬁne-tuning on downstream tasks.\nDev Set Performances We show average dev set\nperformances in Table 9 corresponding to our main\nresults in Table 2.\nD Signiﬁcance Tests.\nWe leverage McNemar’s tests (McNemar, 1947)\nto show ECONET’s improvements against\nRoBERTaLARGE. McNemar’s tests compute\nstatistics by aggregating all samples’ prediction\ncorrectness. For ERE tasks, this value is simply\nclassiﬁcation correctness; for QA tasks (TORQUE\nand MCTACO ), we use EM per question-answer\npairs.\nE Fine-tuning under Low-resource\nSettings\nTable 4 shows improvements of ECONET over\nRoBERTaLARGE are much more salient under low-\n5379\nDatasets p-values\nTORQUE 0.002∗∗\nMCTACO 0.007∗∗\nTB-Dense 0.004∗∗\nMATRES 0.292\nRED 0.059∗\nTable 8: McNemar’s tests for improvement signiﬁ-\ncance between best single models of RoBERTaLARGE\nand ECONET on the test data. Tests with p-values <\n0.05 ( ∗∗) indicate strong statistical signiﬁcance; tests\nwith p-values < 0.1 (∗) indicate weak statistical signiﬁ-\ncance.\nresource setting.\n0.1 0.3 0.5 0.7 1.0\n60.0\n62.5\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n59.7\n72.8 73.6 74.1 75.1\n66.9\n73.4 74.2 74.9 76.1RoBERTa\n+ ECONET\n(a) TORQUE\n0.1 0.3 0.5 0.7 1.0\n25.0\n27.5\n30.0\n32.5\n35.0\n37.5\n40.0\n42.5\n45.0\n27.2\n30.9\n37.2\n38.7 39.4\n29.0\n35.2\n37.6\n39.4\n42.8RoBERTa\n+ ECONET\n(b) RED\nFigure 6: Performances (F 1 scores) compari-\nson between ﬁne-tuning RoBERTa LARGE vs.\nRoBERTaLARGE + ECONET over different ratios of\nthe training data.\nF Variants of ECONET\nWe also experimented with a variant ofECONET\nby ﬁrst pretraining RoBERTaLARGE + Generator\nfor a few thousands steps and then continue to pre-\ntrain with ECONET. However, this method leads\nworse ﬁnetuning results, which seems to contradict\nthe suggestions in Zhou et al. (2020b) and Clark\net al. (2020) that the generator needs to be ﬁrst\ntrained to obtain a good prediction distribution for\nthe discriminator. We speculate that this is due\nto our temporal and event mask predictions being\neasier tasks than those in the previous work, which\nmakes the “warm-up steps” for the generator not\nnecessary.\nG Impact of Event Models\nTable 10 compares results based on two event an-\nnotations.\nH Ablation Studies for BERT\nI Attention Scores\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nLayers\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5Average Attention Scores\nRoBERTa-large (before)\nECONET (before)\n(a) Attentions scores for [before] indicators.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nLayers\n0.0\n0.5\n1.0\n1.5\n2.0Average Attention Scores\nRoBERTa-large (after)\nECONET (after)\n(b) Attentions scores for [after] indicators.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\nLayers\n0.0\n0.5\n1.0\n1.5\n2.0Average Attention Scores\nRoBERTa-large (during)\nECONET (during)\n(c) Attentions scores for [during] indicators.\nFigure 7: Attentions score comparisons between\nRoBERTaLARGE and ECONET for all model layers.\nAll numbers are multiplied by 100 and average over 3\nrandom seeds for illustration purpose\nJ Analysis of the Discriminator\nTable 12 shows the alignment of between the gen-\nerator and the discriminator, and Table 13 shows\nthe examples of “disagreed” samples between the\n5380\nTORQUE TB-Dense MATRES RED\nMethods F1 EM C F1 F1 F1\nTacoLM 65.5(±0.8) 36.2(±1.6) 22.7(±1.4) 56.9(±0.7) 75.1(±0.8) 40.7(±0.3)\nBERTLARGE 70.9(±1.0) 42.8(±1.7) 29.0(±0.9) 56.7(±0.2) 73.9(±1.0) 40.6(±0.0)\n+ ECONET 71.8(±0.3) 44.8(±0.4) 31.1(±1.6) 56.1(±0.8) 74.4(±0.4) 41.7(±1.5)\nRoBERTaLARGE 76.7(±0.2) 50.5(±0.5) 36.2(±1.1) 59.8(±0.3) 79.9(±1.0) 43.7(±1.0)\n+ Generator 76.6(±0.1) 51.0(±1.0) 36.3(±0.8) 61.5(±0.9) 79.8(±0.9) 43.2(±1.2)\n+ ECONET 76.9(±0.4) 52.2(±0.9) 37.7(±0.4) 60.8(±0.6) 79.5(±0.1) 44.1(±1.7)\nTable 9: Average Dev Performances Corresponding to Table 2. Note that for MCTACO, we train on dev set and\nevaluate on the test set as mentioned in Section 3, so we do not report test performance again here.\nTORQUETB-DRED\nEvent AnnotationsF1 EM C F1 F1\nTORQUE 76.1 51.6 36.8 64.8 42.8\nTB-Dense 76.1 51.3 36.465.1 42.6\nTable 10: Fine-tuning performance comparisons using\nevent detection models trained on TORQUE v.s. TB-\nDense event annotations. All numbers are average over\n3 random seeds. Std. Dev. ≥1% is underlined.\nTORQUETB-DRED\nMethods F1 EM C F1 F1\nBERTLARGE 70.643.727.562.8 39.4\n+ random mask70.6 44.1 27.263.4 35.3\n+ECONET 71.4 44.8 28.563.0 40.2\nTable 11: Fine-tuning performances. All numbers are\naverage over 3 random seeds. Std. Dev. ≥1% is un-\nderlined.\ngenerator and the discriminator. Detailed analysis\ncan be found in Section 4 in the main text.\nTemporal Generator Event Generator\nCorr. Incorr. Corr. Incorr.\nTotal # 837 163 26 974\nDiscr. Corr. # 816 29 25 964\nAccuracy 97.5% 17.8% 96.2% 99.0%\nTable 12: Discriminator’s alignment with generator’s\nmask predictions in ECONET. Second column shows\nthat discriminator strongly disagree with the “errors”\nmade by the temporal generator.\nType I. Same Group: 12/50 (24%)\n⟩⟩Ex 1. original: when; predicted: while\nText: A letter also went home a week ago in Pelham, in\nWestchester County, New York,⟨mask⟩a threat made\nby a student in a neighboring town circulated in\nseveral communities within hours...\n⟩⟩Ex 2. original: prior to; predicted: before\nText: ... An investigation revealed that rock gauges were\npicking up swifter rates of salt movement in the ceiling\nof the room, but at Wipp no one had read the computer\nprintouts for at least one month ⟨mask⟩the collapse.\nType II. Related Group: 8/50 (16%)\n⟩⟩Ex 3. original: in the past; predicted: before\ntext: Mr. Douglen confessed that Lautenberg, which\nhad won ⟨mask⟩, was “a seasoned roach and was\nready for this race...\n⟩⟩Ex 4. original: previously; predicted: once\ntext: Under the new legislation enacted by Parliament,\ndivers who ⟨mask⟩had access to only 620 miles of the\n10,000 miles of Greek coast line will be able to explore\nships and “archaeological parks” freely...\nTable 13: Categories and examples of highly related\n“incorrect” temporal indicator predictions by the gener-\nator, but labeled as “correct” by the discriminator.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7621611952781677
    },
    {
      "name": "Event (particle physics)",
      "score": 0.7508770227432251
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6543421149253845
    },
    {
      "name": "Natural language processing",
      "score": 0.5669328570365906
    },
    {
      "name": "Relation (database)",
      "score": 0.5416915416717529
    },
    {
      "name": "Question answering",
      "score": 0.48939767479896545
    },
    {
      "name": "Machine learning",
      "score": 0.41861802339553833
    },
    {
      "name": "Data mining",
      "score": 0.1239938735961914
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I161318765",
      "name": "University of California, Los Angeles",
      "country": "US"
    }
  ],
  "cited_by": 30
}