{
  "title": "Transformer fault diagnosis using continuous sparse autoencoder",
  "url": "https://openalex.org/W2340811421",
  "year": 2016,
  "authors": [
    {
      "id": "https://openalex.org/A2357489240",
      "name": "Wang Lu-kun",
      "affiliations": [
        "Ocean University of China"
      ]
    },
    {
      "id": "https://openalex.org/A1933614324",
      "name": "Zhao Xiao-ying",
      "affiliations": [
        "Taishan Medical University"
      ]
    },
    {
      "id": null,
      "name": "Pei, Jiangnan",
      "affiliations": [
        "Shandong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A1868140084",
      "name": "Tang Gong-you",
      "affiliations": [
        "Ocean University of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2050853198",
    "https://openalex.org/W2547229958",
    "https://openalex.org/W1970964495",
    "https://openalex.org/W2102321334",
    "https://openalex.org/W1650018112",
    "https://openalex.org/W2147768505",
    "https://openalex.org/W2158969303",
    "https://openalex.org/W2084277209",
    "https://openalex.org/W2100495367",
    "https://openalex.org/W2172000360",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W2111838090",
    "https://openalex.org/W2170433518",
    "https://openalex.org/W2080850431",
    "https://openalex.org/W2145889472",
    "https://openalex.org/W2031014509",
    "https://openalex.org/W2095980522",
    "https://openalex.org/W1995094777",
    "https://openalex.org/W2025768430",
    "https://openalex.org/W2058670155",
    "https://openalex.org/W4252684946",
    "https://openalex.org/W2123958476",
    "https://openalex.org/W22861983",
    "https://openalex.org/W166926187",
    "https://openalex.org/W2110798204",
    "https://openalex.org/W2145094598"
  ],
  "abstract": null,
  "full_text": "Transformer fault diagnosis using \ncontinuous sparse autoencoder\nLukun Wang1*, Xiaoying Zhao2, Jiangnan Pei3 and Gongyou Tang1\nBackground\nTransformer is one of the most important equipment in power network. It will bring \nhuge economic loss to the power network if it fails. The periodical monitoring of the \ncondition of the transformer is necessary. There are a lot of methods used for detecting \npower failures such as oil breakdown voltage test, resistivity test and moisture analysis in \ntransformer oil (Saha 2003). Among these methods, dissolved gas analysis (DGA) is the \nmost widely used method (Arakelian 2004). This method diagnoses the transformer fault \nbased on the analysis of dissolved gas concentrations in transformer oil (Duval 2003). \nThe gases in transformer oil mainly include hydrocarbons, such as: methane (CH 4), \nethane (C2H6), ethylene (C2H4), acetylene (C2H2) and other gases, such as: hydrogen (H2) \nand carbon dioxide (CO 2). In recent years, researchers have proposed transformer fault \ndiagnosis methods including particle swarm optimization (Ballal et  al. 2013), support \nvector machine (Chen et  al. 2009), fuzzy learning vector quantization network (Yang \net al. 2001) and back propagation (BP) neural network (Patel and Khubchandani 2004). \nMiranda et al. ( 2012) built a diagnosis system based on a set of auto-associative neural \nnetworks to diagnose the faults of power transformer. The information theoretic mean \nAbstract \nThis paper proposes a novel continuous sparse autoencoder (CSAE) which can be \nused in unsupervised feature learning. The CSAE adds Gaussian stochastic unit into \nactivation function to extract features of nonlinear data. In this paper, CSAE is applied \nto solve the problem of transformer fault recognition. Firstly, based on dissolved gas \nanalysis method, IEC three ratios are calculated by the concentrations of dissolved \ngases. Then IEC three ratios data is normalized to reduce data singularity and improve \ntraining speed. Secondly, deep belief network is established by two layers of CSAE and \none layer of back propagation (BP) network. Thirdly, CSAE is adopted to unsupervised \ntraining and getting features. Then BP network is used for supervised training and \ngetting transformer fault. Finally, the experimental data from IEC TC 10 dataset aims to \nillustrate the effectiveness of the presented approach. Comparative experiments clearly \nshow that CSAE can extract features from the original data, and achieve a superior cor-\nrect differentiation rate on transformer fault diagnosis.\nKeywords: Continuous sparse autoencoder, Dissolved gas analysis, Deep belief \nnetwork, Deep learning, Transformer fault\nOpen Access\n© 2016 Wang et al. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate \nif changes were made.\nRESEARCH\nWang et al. SpringerPlus  (2016) 5:448 \nDOI 10.1186/s40064-016-2107-7\n*Correspondence:  \nwanglukun@gmail.com \n1 College of Information \nScience and Engineering, \nOcean University of China, \nQingdao, China\nFull list of author information \nis available at the end of the \narticle\nPage 2 of 13Wang et al. SpringerPlus  (2016) 5:448 \nshift (ITMS) algorithm was adopted to densify the data clusters. Dhote and Helonde \n(2012) proposed a new five fuzzy ratios method and developed a fuzzy diagnostic expert \nsystem to diagnose the transformer fault. Souahlia et al. (2012) combined the Rogers and \nDoernenburg ratios together to be the gases signature. The multi-layer perceptron neu\n-\nral network was applied for decision making. Bhalla et al. (2012) applied a pedagogical \napproach for rule extraction from function approximating ANN (REFANN). REFANN \nderives linear equations by approximating the hidden unit activation function and \nsplitting the input space into sub-region. Ren et al. (2010) used the rough set theory to \nreduce the degree of complex training samples; the speed of learning and training was \nenhanced. Then the quantum neural network was applied to the classifier of transformer \nfault diagnosis.\nIn 1996, sparse coding was proposed by Olshausen and Field (1996) which showed \nthat the receptive fields of simple cells in mammalian primary visual cortex could learn \nhigher level representations from the outside input signals (Vinje and Gallant 2000). \nAfter then, autoencoder was proposed to learn higher level features. In 2006, a new \nneural network model called deep belief network (DBN) was proposed by Hinton and \nSalakhutdinov (2006) as a new neural network (Cottrell 2006). With the development \nof the deep learning theory, DBN is widely used in many AI areas (Le Roux and Bengio \n2010).\nAccording to Bengio et  al. (2006), DBN was successfully comprised of autoencoder \n(AE). He used AE as a basic model of DBN. With this structure, the training of handwrit\n-\nten digits recognition has achieved more than 99 % accuracy rate. It is proved that AE \ncan completely replace  restricted Boltzmann machine (RBM) as the basic elements of \nDBN. In 2008, Vincent et al. (2008) proposed denoising autoencoder (DAE) which could \nbe adopted in corrupted data. DAE learns to project the corrupted data back onto the \nmanifold, and can make the characteristics of the data more robust. On this basis, Vin\n-\ncent et al. (2010) introduced stacked denoising autoencoder (SDAE) by stacking several \nlayers of DAE with the category constraint. At present, AE has been successfully applied \nto speech recognition (Dahl et al. 2012), handwritten digit recognition, natural language \nprocessing fields (Glorot et al. 2011), etc.\nThe current research on transformer fault diagnosis which applies neural network to \nthe classification algorithm is mainly based on single-layer neural network. Instead of a \nsingle-layer neural network, a deep network composed of multiple layers of continuous \nsparse autoencoder (CSAE) is designed to solve the problem of transformer fault recog\n-\nnition. The second section describes the method of DGA, the relationship between the \ntransformer fault classification and the concentrations of five fault gases has been intro -\nduced. In the third section, the basic autoencoder is briefly reviewed and a new continu -\nous sparse autoencoder is proposed to extract the features of nonlinear data. The fourth \nsection, several experiments are designed to verify the validity of CSAE. The last section \nconcludes our work and points out the future direction.\nDissolved gas analysis\nDGA is an analytic technique by detecting the dissolved gas in transformer oil. The insu-\nlating materials will release small amounts of hydrocarbons if transformer breaks down. \nThe concentrations of these hydrocarbons can be used for electrical fault classification. \nPage 3 of 13\nWang et al. SpringerPlus  (2016) 5:448 \nThe gases generated by transformer have useful information. They can be applied to \nelectrical equipment diagnosis.\nIEC publication 60599 (Duval 2003) provided a list of faults for DGA. The common \ntransformer faults and their symbols are shown in Table 1.\nUnder the influence of thermal faults and electrical faults, hydrocarbon molecules of \nmineral oil can be decomposed from active hydrogen and hydrocarbon fragments. Then \nsmall amounts of gases, such like H\n2, CH4, C2H6, C2H4, and C 2H2 will be released. The \nemergence of these gases often accompanies with transformer faults, therefore these \nfive gases are named as fault gases. The fault gases are released in the following order: \nH\n2  →  CH4  → C 2H6  → C 2H4  → C 2H2. The concentration of hydrogen will increase \nsteadily when the temperature is relatively low, while the acetylene will be released \nat a very high temperature. Therefore, the fault gases keep in touch with transformer \nfault. The relationship between the concentration of fault gases and transformer fault is \nshown in Table 2. In electrical faults, hydrogen is of high importance, while in thermal \nfaults, acetylene tends to be important. In low thermal faults, methane, ethane and eth\n-\nylene is of high importance but in high thermal faults only ethylene tends to be of high \nimportance. The main difference between low thermal faults and high thermal faults \nis the concentration of thane. Ethane will be released when low thermal faults happen. \nAccording to the analysis above, DGA can diagnose the transformer fault by detecting \nthe concentrations of these five fault gases.\nMethods\nDBN model\nDBN is a logic model consisted of multiple layers of RBM. It also can be composed of \nmultiple layers of AE. The structure of DBN based on multiple layers of AE is shown in \nFig. 1.\nTable 1 Fault classification\nSymbol Transformer fault\nPD Partial discharges\nLED Low energy discharge\nHED High energy discharge\nTF1 Thermal faults <700 °C\nTF2 Thermal faults >700 °C\nTable 2 Gas importance by faults\n●: high importance, ○: medium importance\nCause of gas generation H2 CH4 C2H6 C2H4 C2H2\nElectrical fault\n PD ● ○\n LED ● ●\n HED ● ○ ●\nThermal fault\n TF1 ○ ● ● ●\n TF2 ○ ○ ● ○\nPage 4 of 13Wang et al. SpringerPlus  (2016) 5:448 \nThe process of training DBN can be divided into the following steps:\nStep 1 Each layer of AE can be used for unsupervised feature learning. In the process \nof training, each layer of AE can extract different features from the input data. These \nfeatures are stored in the feature vector W. In this step, the optimization is not meant \nfor the entire DBN.\nStep 2 One layer of BP neural network is set at the bottom layer of DBN. The reason of \nsetting one layer of BP is to receive trained AE weight. After AE unsupervised training, \nBP will calculate the error between DBN output and expected output. The error will be \npassed back to previous layers of AE. According to the error, the weight matrix of the \nwhole DBN will be updated. The process of reconstruction will be repeated based on \nthe set epochs until the error converges. It realizes the optimization of feature data.\nDBN overcomes the disadvantages of signal-layer neural network: falling into local \noptimum and long training time.\nBasic autoencoder\nAutoencoder is a famous neural network model in which the target output is as same \nas the input, such as y(i) = x(i). Autoencoder has two processes: encoder process and \ndecoder process. In the encoder process, the input is transformed into the hidden fea -\ntures. In the decoder process, the hidden features are reconstructed to be the target out -\nput. The weight matrix of each layer can be updated through training neural network. \nThe structure is shown in Fig. 2.\nWhere xi,i ∈ 1, ... ,n is the input of autoencoder, h j,j ∈ 1, ... ,k is the value of hid -\nden units, ˆxi,i ∈ 1, ... ,n is the target output, W (i),i∈ 1, 2 denotes the weight matrix. AE \ntries to learn a function like hW ,b(x) = x which can make ˆx approximate to x. hW ,b(x) is \nan activation function. The purpose of training AE is to get \n{\nW (l), b(l)}\n.\nFig. 1 DBN model\nPage 5 of 13\nWang et al. SpringerPlus  (2016) 5:448 \nIn order to acquire the weight matrix, the square error of single sample can be calcu -\nlated as\nwhere x and y denote the real input and output respectively, hW ,b(x) is the output of acti-\nvation function.\nThe error loss function of whole network can be obtained\nwhere m is the number of training examples, /afii9838 controls the relative importance of the \nsecond term, the first term of loss function (2) is an average sum-of-squares error term, \nthe second term is the weight decay term which tends to decrease the magnitude of \nweights and prevent over-fitting.\nContinuous sparse autoencoder\nIn order to extract the features of nonlinear data, the zero-mean Gaussian with variance \nσ2 stochastic unit is added into activation function of each visible unit.\n(1)J\n(\nW ,b; x,y\n)\n= 1\n2\nhW ,b(x) − y\n2\n(2)\nJ(W ,b) =\n[\n1\nm\nm∑\ni=1\nJ\n(\nW ,b;x(i),y(i)\n)]\n+ /afii9838\n2\nnl−1∑\nl=1\nsl∑\ni=1\nsl+1∑\nj=1\n(\nW l\nji\n)2\n=\n[\n1\nm\nm∑\ni=1\n(1\n2\n\n\nhW ,b\n(\nx(i)\n)\n− y(i)\n\n\n2 )]\n+ /afii9838\n2\nnl−1∑\nl=1\nsl∑\ni=1\nsl+1∑\nj=1\n(\nW l\nji\n)2\n(3)sj = ϕj\n(∑\ni\nw ijxi + ai + σ · N j(0, 1)\n)\nFig. 2 Model of AE\nPage 6 of 13Wang et al. SpringerPlus  (2016) 5:448 \nEquation (3) refers to the activation function with Gaussian stochastic unit, ϕj repre-\nsents the activation function, and sj is the output of network with input x i, a i is the bias \nunit, N j(0, 1) means a zero-mean Gaussian, σ and N j(0, 1) composes nj = σ · N j(0, 1), n j \nsubjects to the distribution as\nThe unit activation of hidden layer can be defined as (Andrew 2012)\nwhere a (2)\nj\n(\nx (i))\n means the activation of hidden layer unit with the input x, ρ means the \nsparse parameter. In this paper, we assume that ˆρj =ρ, the difference between ˆρj and ρ \ncan be calculated by Kullback–Leibler (KL) divergence (Kullback and Leibler 1951)\nwhere β is the weight coefficient that controls the sparse penalty factor. According to the \nloss function (1), suppose that L2 is a hidden layer, L1 represents the input layer and L3 is \nthe output layer, the error of output layer can be calculated\nwhere δ(3 )\ni  means the error of output layer, a (3)\ni  is the activation function, \nz(3)\ni = W (2)\ni a(2)\ni + b(2). In the hidden layer L2, the error of each unit can be calculated as\nThe gradient descent optimization parameters can be obtained:\n(4)p\n(\nnj\n)\n= 1\nσ\n√\n2π\nexp\n(\n−n2\nj\n2σ2\n)\n(5)ˆρj = 1\nm\nm∑\ni=1\n[\na(2)\nj\n(\nx(i)\n)]\n(6)KL (ρ|| ˆρj ) = ρ log ρ\nˆρj\n+ (1 − ρ) log 1 − ρ\n1 −ˆρj\n(7)Jsparse(W ,b) = J(W ,b) + β\ns2∑\nj=1\nKL (ρ|| ˆρj)\n(8)δ(3 )\ni = ∂\n∂z(3 )\ni\n1\n2\nhW ,b(x) − y\n2 =−\n(\nyi − a(3 )\ni\n)\nf′\n(\nz(3 )\ni\n)\n(9)δ(2)\ni =\n\n\n\n\ns2�\nj=1\nW (2)\nji δ(3)\ni\n\n + β\n�\n− ρ\nˆρj\n+ 1 − ρ\n1 −ˆρj\n�\nf′\n�\nz(2)\ni\n�\n(10)\n∂\n∂W (l)\nji\nJ\n(\nW ,b; x,y\n)\n= a(l)\nj δ(l+1 )\ni\n(11)\n∂\n∂b(l)\ni\nJ\n(\nW ,b; x,y\n)\n= δ(l+1 )\ni\nPage 7 of 13\nWang et al. SpringerPlus  (2016) 5:448 \n1. Setting ∆W (l) := 0, ∆b(l) := 0\n2. Calculating ∇W (l) J\n(\nW , b; x, y\n)\n and ∇b(l) J\n(\nW , b; x, y\n)\n3. Calculating ∆W (l) := ∆W (l) +∇ W (l) J\n(\nW , b; x, y\n)\n and ∆ b (l) :=∆ b (l)+\n∇b(l) J\n(\nW , b; x, y\n)\n4. Updating the weight: \nIn this paper, manifold learning is drawn to analyze the effect of stochastic unit. \nAccording to the manifold learning theory, the high-dimensional data can be repre -\nsented by low-dimensional manifold. The operator p(x| ˜x) attempts to transform the \nhigh-dimensional x to low-dimensional ˜x. In the process of learning, the distribution of \nstochastic unit is not in high-dimensional manifold, so the gradient of p(x| ˜x) should be \nchanged greatly to approximate x. Essentially, CSAE can be considered as a manifold \nlearning algorithm. The stochastic unit added into activation function can change the \ngradient direction and prevent over-fitting.\nThe contrast experiment of autoencoder and CSAE has been designed. The swiss-roll \nmanifold is adopted as the experiment dataset. The result of experiment is shown in \nFig. 3. Figure 3a is the raw swiss-roll manifold, Fig. 3b, c   are the reconstruction of swiss-\nroll dataset by autoencoder and CSAE. It can be concluded that CSAE is more suitable \nfor reconstructed of continuous data than autoencoder.\nExperiments\nDataset and normalization\nIn this paper we use IEC TC 10 as the experiment dataset (Duval and DePablo 2001) \nprovided by Mirowski and LeCun (2012). There are 134 transformer fault samples in \nthis dataset. Each sample contains the concentrations of CH 4, C2H2, C2H4, C2H6 and H2 \nin parts per million (ppm). Three ratios including CH 4/H2, C2H2/C2H4, C2H4/C2H6 can \nbe calculated as the input of DBN. The five classifications of transformer faults corre -\nsponding to binary codes can be set as the output of DBN, they are 00001 (partial dis -\ncharges), 00010 (low energy discharge), 00100 (high energy discharge), 01000 (thermal \nfaults <700 °C) and 10000 (thermal faults >700 °C).\nThe concentrations of gases dissolved in transformer oil have a direct guiding sig -\nnificance for transformer fault analysis. In order to reduce the singularity of data and \nimprove the training speed, the input data can be normalized to [ymin , ymax ] by normali-\nzation formula\nwhere y represents the normalized data, making ymax = 1, ymin =− 1. xmax is the maxi -\nmum value of input data, while xmin is the minimum value of input data.\nW (l) = W (l) − α\n[( 1\nm ∆W (l)\n)\n+ /afii9838W(l)\n]\nb(l) = b(l) − α\n[( 1\nm ∆b(l)\n)]\n(12)y = (ymax − ymin )(x − xmin )\n(xmax − xmin ) + ymin\nPage 8 of 13Wang et al. SpringerPlus  (2016) 5:448 \nNetwork structure\nThe network structure is shown in Fig. 4. The white circles are neuron units and the blue \ncircles denote bias units. There are six layers: the input layer V, the hidden layer H 0, the \nhidden layer H1, the hidden layer H 2, the hidden layer H 3 and the output layer T. Layer \nV, layer H0 and layer H 1 compose the first CSAE network. Layer H 1, layer H 2 and layer \nH3 compose the second CSAE network. Layer H 2, layer H3 and layer T compose BP net-\nwork. In layer V, there are 3 units (not including the bias unit, the same below) which \ncontain three ratios of transformer fault gases. T layer contains 5 units corresponding \nto the transformer faults binary codes. The hidden layer H 0 contains 10 units which are \nused to store the high-dimensional features. The hidden layer H 1 contains 3 units which \nare used to reconstruct the high-dimensional features to low-dimensional approximate \noutput. The hidden layer H\n2 and H3 contain 11 and 3 units respectively.\nThe flowchart of proposed method is shown in Fig.  5. The phases of transformer fault \ndiagnosis mainly include preprocessing and DBN training. In the preprocessing phase, \nthe three ratios of transformer fault samples can be calculated. Then the data can be \nnormalized by Eq. (12) as the input of DBN. In DBN training phase, two CSAEs are used \nto extract the hidden features of input, BP is used to reduce the dimension of hidden fea\n-\ntures and classify the transformer fault.\nParameters setting\nParameters are very important for neural network. Recent studies (Nguyeny et al. 2013) \nhave shown that if parameters is not set properly, the correct differentiation rate will be \nlow and the speed of convergence will be slow. According to previous experience, the \nauthors set parameters as follows.\nFig. 3 Reconstruction of swiss-roll mainfold a raw swiss-roll mainfold. b reconstruction of swiss-roll mainfold \nby autoencoder, c reconstruction of swiss-roll mainfold by CSAE\nPage 9 of 13\nWang et al. SpringerPlus  (2016) 5:448 \nLearning rate: the learning rate is very important. If it is big, the system will become \nunstable. Otherwise the training epoch will become too long. Generally, a relatively small \nlearning rate will make the error converge asymptotically. At the same time, because the \nnetwork size is different, the learning rate should be adjusted according to the network \nsize. In this experiment, the learning rate is set to be 0.05.\nMomentum: in order to avoid over-fitting and fine-tune the direction of gradient, we \napply the momentum parameter to change the gradient of likelihood function. In this \nexperiment, the momentum is set to be 0.9.\n(13)W ij← k × W ij+ ǫ\n(⟨\nvihj\n⟩\ndata −\n⟨\nvihj\n⟩\nreconstruct\n)\nFig. 4 Network structure\nFig. 5 Flowchart of proposed method\nPage 10 of 13Wang et al. SpringerPlus  (2016) 5:448 \nSparse parameter: sparse parameter is used to determine the unit activation. In this \nexperiment, the sparse parameter is set to be 0.01.\nSimulation\nAbout the simulation environment, the software is Matlab 8.1.0 and the hardware is the \ndesktop computer with Intel i5 processer with 8 GB RAM and 2.5 GHz frequency, and \nthe operating system is Microsoft Windows 8.1 professional. In this experiment, the 125 \nsamples are applied to the training dataset, and the other 9 samples are applied to the \npredicting dataset. The K-fold is adopted to the cross validation method. In this section, \nK is set to be 5, it means that 125 samples will be divided into 5 partitions. One partition \nis used for testing and the other 4 partitions are used for training. The process will repeat \n5 times until each partition can be regarded as training and testing data.\nThrough training of 125 samples, Fig.  6 shows the error curve of CSAE and BP . It can \nbe proved that the convergence speed of CSAE curve is faster than BP curve. And the \nerror of CSAE curve is lower than BP curve.\nIn order to verify the validity of our approach, the classification accuracy of K-nearest \nneighbor (K-NN), support value machine (SVM), BP and CSAE are contrasted. Table  3 \nshows the classification accuracy of K-NN algorithm. When K  =  15, the accuracy is \n90 %. SVM is applied as one of the standard tool for pattern classification and recogni\n-\ntion. SVM converts samples into a feature space using kernel functions which commonly \ninclude radial basis function (RBF), polynomial function (PLOY) and sigmoid function \n(SIG) (Hsu and Lin 2002). The classification accuracy of SVM with different kernel func\n-\ntions is shown in Table 4, the highest correct rate of SVM using RBF as the kernel func -\ntion is 79.9 %. \nFig. 6 CSAE and BP error curve\nTable 3 Classification accuracy of K-NN\nK 10 (%) 15 (%) 20 (%) 60 (%)\nAccuracy (%) 88.9 90 83.9 77.8\nPage 11 of 13\nWang et al. SpringerPlus  (2016) 5:448 \nThrough 1000 epochs training, Table  5 lists the correct differentiation rates of BP and \nCSAE. These two models have the same parameters of network which can ensure the \nfairness of results. The correct rate of BP algorithm is 86.6 % in TF1 and HED. The cor -\nrect rate of CSAE algorithm is 100 % in TF1, 83.3 % in PD.\nWilcoxon rank sum test (Wilcoxon 1945) is a well-known nonparametric statistical \ntest used to evaluate the ranking of features. In this paper, the Wilcoxon rank sum test \nis used to compare the differences of CSAE and BP algorithm. It is assumed that h  = 1 \ndenotes the fact that the correct differentiation rate of CSAE is significantly better than \nBP; h = 0 denotes the fact that the correct differentiation rate of CSAE is as same as BP; \nthe level of significance α  = 0.05. The results are shown in Table  6. The average correct \ndifferentiation rate of CSAE and BP are 93.6 ± 6.22 and 84.1 ± 2.44 % respectively. The p \nvalue is 0.0195 which is smaller than α. So it can be concluded that the correct differen\n-\ntiation rate of CSAE is significantly better than BP .\nBased on the training network, 9 test samples are adopted to check the forecast ability \nof CSAE. In Table 7, it can be seen that the fault of CSAE algorithm forecast is consistent \nwith the actual fault.\nConclusion and future work\nIn this paper, we propose a novel CSAE model which can be used in unsupervised learn -\ning of representations. CSAE added Gaussian stochastic unit in activation function is \nadopted to solve the problem of transformer fault recognition. The IEC three ratios are \ncalculated by the concentrations of dissolved gases. Then the three ratios are normalized \nto reduce data singularity. In the experiments, DBN is established by two layers of CSAE \nand one layer of BP . CSAE is applied to unsupervised training and getting features. BP \nTable 4 Classification accuracy of SVM\nKernel function SVM_RBF (%) SVM_SIG (%) SVM_PLOY (%)\nAccuracy (%) 79.9 59.5 68.8\nTable 5 Classification accuracy of BP and CSAE\nClassification CSAE (%) BP (%)\nTF1 (%) 100 86.6\nTF2 (%) 93.7 81.2\nPD (%) 83.3 83.3\nLED (%) 95.6 82.6\nHED (%) 95.5 86.6\nTable 6 Results of Wilcoxon rank sum test\nState CSAE (%) BP (%)\nStandard deviation (%) 6.22 2.44\nAverage accuracy (%) 93.6 84.1\np-value 0.0195\nPage 12 of 13Wang et al. SpringerPlus  (2016) 5:448 \nis used for supervised training and transformer fault classification. Comparative experi -\nments clearly show the advantages of CSAE on transformer fault diagnosis. This neural \nnetwork diagnosis algorithm is better than the traditional algorithm with its value in the \nactual transformer fault diagnosis.\nThe CSAE model have the advantages of outstanding recognition ability of continu\n-\nous data, unsupervised feature learning ability, high precision and robust ability. The \nmain disadvantages of CSAE model include long time training and high performance \ncomputer requirement. In summary, CSAE has great potential. In the future work, we \nwill continue to research CSAE and try to use some tricks to shorten the training time. \nFurthermore, we plan to investigate some optimization strategies to diagnosis the trans\n-\nformer fault.\nAuthors’ contributions\nA mathematical model for transformer fault diagnosis has been proposed. All authors read and approved the final \nmanuscript.\nAuthor details\n1 College of Information Science and Engineering, Ocean University of China, Qingdao, China. 2 College of Foreign \nLanguages, Taishan Medical University, Taian, China. 3 College of Electrical Engineering and Automation, Shandong \nUniversity of Science and Technology, Qingdao, China. \nAcknowledgements\nThis work was supported by National Natural Science Foundation of China (41276086), National Natural Science Founda-\ntion of Shandong Province (ZR2015FM004).\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 10 December 2015   Accepted: 5 April 2016\nReferences\nAndrew NG (2012) Autoencoders and sparsity. http://ufldl.stanford.edu/wiki/index.php/Au-toencoders_and_Sparsity\nArakelian VG (2004) The long way to the automatic chromatographic analysis of gases dissolved in insulating oil. IEEE \nElectr Insul Mag 20(6):8–25. doi:10.1109/MEI.2004.1367506\nBallal MS, Ballal DM, Suryawanshi HM, Choudhari BN (2013) Computational intelligence algorithm based condition \nmonitoring system for power transformer. In: IEEE 1st international conference on condition assessment techniques \nin electrical systems, IEEE CATCON 2013, pp 154–159. doi:10.1109/CATCON.2013.6737538\nBengio Y, Lamblin P , Popovici D, Larochelle H (2006) Greedy layer-wise training of deep networks. In: 20th annual confer-\nence on neural information processing systems, NIPS 2006, pp 153–160\nBhalla D, Bansal RK, Gupta HO (2012) Function analysis based rule extraction from artificial neural networks for trans-\nformer incipient fault diagnosis. Int J Electr Power 43(1):1196–1203. doi:10.1016/j.ijepes.2012.06.042\nTable 7 A part of training results\nNo CH4/H2 C2H2/C2H4 C2H4/C2H6 Actual fault Forecast fault\n1 0.06 0 1.35 LED LED\n2 1 0.007 2.52 TF1 TF1\n3 0.96 0.025 8.12 TF2 TF2\n4 2.3 0 3.83 TF2 TF2\n5 7.19 0.005 8.63 TF2 TF2\n6 0.235 1.1 7.67 PD PD\n7 1.3 0 1.22 TF1 TF1\n8 1.23 0.05 9.22 TF2 TF2\n9 0.17 1 9.615 PD PD\nPage 13 of 13\nWang et al. SpringerPlus  (2016) 5:448 \nChen W, Pan C, Yun Y, Liu Y (2009) Wavelet networks in power transformers diagnosis using dissolved gas analysis. IEEE \nTrans Power Deliver 24(1):187–194. doi:10.1109/TPWRD.2008.2002974\nCottrell GW (2006) New life for neural networks. Science 313(5786):454–455. doi:10.1126/science.1129813\nDahl GE, Yu D, Deng L, Acero A (2012) Context-dependent pre-trained deep neural networks for large-vocabulary speech \nrecognition. IEEE Trans Audio Speech 20(1):30–42. doi:10.1109/TASL.2011.2134090\nDhote NK, Helonde JB (2012) Diagnosis of power transformer faults based on five fuzzy ratio method. WSEAS Trans Power \nSyst 7(3):114–125\nDuval M (2003) New techniques for dissolved gas-in-oil analysis. IEEE Electr Insul M 19(2):6–15. doi:10.1109/\nMEI.2003.1192031\nDuval M, DePablo A (2001) Interpretation of gas-in-oil analysis using new IEC publication 60599 and IEC TC 10 databases. \nIEEE Electr Insul Mag 17(2):31–41. doi:10.1109/57.917529\nGlorot X, Bordes A, Bengio Y (2011) Domain adaptation for large-scale sentiment classification: a deep learning approach. \nIn: Proceedings of the 28th international conference on machine learning, ICML 2011, pp 513–520\nHinton GE, Salakhutdinov RR (2006) Reducing the dimensionality of data with neural networks. Science 313(5786):504–\n507. doi:10.1126/science.1127647\nHsu C, Lin C (2002) A comparison of methods for multiclass support vector machines. IEEE Trans Neural Network \n13(2):415–425. doi:10.1109/72.991427\nKullback S, Leibler RA (1951) On Information and Sufficiency. Ann Math Stat 22(1):79–86. doi:10.1214/aoms/1177729694\nLe Roux N, Bengio Y (2010) Deep belief networks are compact universal approximators. Neural Comput 22(8):2192–2207. \ndoi:10.1162/neco.2010.08-09-1081\nMiranda V, Castro ARG, Lima S (2012) Diagnosing faults in power transformers with autoassociative neural networks and \nmean shift. IEEE Trans Power Deliver 27(3):1350–1357. doi:10.1109/TPWRD.2012.2188143\nMirowski P , LeCun Y (2012) Statistical machine learning and dissolved gas analysis: a review. IEEE Trans Power Deliv \n27(4):1791–1799. http://www.mirowski.info/pub/dga\nNguyeny TD, Tranyz T, Phungy D, Venkateshy S (2013) Learning parts-based representations with nonnegative restricted \nboltzmann machine. In: 5th Asian conference on machine learning, ACML 2013, pp 133–148\nOlshausen BA, Field DJ (1996) Emergence of simple-cell receptive field properties by learning a sparse code for natural \nimages. Nature 381(6583):607–609. doi:10.1038/381607a0\nPatel NK, Khubchandani RK (2004) ANN based power transformer fault diagnosis. J Inst Eng (India) Electr Eng Div \n85:60–63\nRen X, Zhang F, Zheng L, Men X (2010) Application of quantum neural network based on rough set in transformer fault \ndiagnosis. In: Proceedings of the power and energy engineering conference (APPEEC), 2010 Asia-Pacific, 28–31 \nMarch 2010, pp 1–4. doi:10.1109/APPEEC.2010.5448911\nSaha TK (2003) Review of modern diagnostic techniques for assessing insulation condition in aged transformers. IEEE \nTrans Dielectr El In 10(5):903–917. doi:10.1109/TDEI.2003.1237337\nSouahlia S, Bacha K, Chaari A (2012) MLP neural network-based decision for power transformers fault diagnosis using an \nimproved combination of Rogers and Doernenburg ratios DGA. Int J Electr Power 43(1):1346–1353. doi:10.1016/j.\nijepes.2012.05.067\nVincent P , Larochelle H, Bengio Y, Manzagol P-A (2008) Extracting and composing robust features with denoising autoen-\ncoders. In: Proceedings of the 25th international conference on machine learning, pp 1096–1103\nVincent P , Larochelle H, Lajoie I, Bengio Y, Manzagol P-A (2010) Stacked denoising autoencoders: learning useful repre-\nsentations in a deep network with a local denoising criterion. J Mach Learn Res 11:3371–3408\nVinje WE, Gallant JL (2000) Sparse coding and decorrelation in primary visual cortex during natural vision. Science \n287(5456):1273–1276. doi:10.1126/science.287.5456.1273\nWilcoxon F (1945) Individual comparisons by ranking methods. Biom Bull 1(6):80–83 \nYang HT, Liao CC, Chou JH (2001) Fuzzy learning vector quantization networks for power transformer condition assess-\nment. IEEE Trans Dielect Electr Insul 8(1):143–149. doi:10.1109/94.910437",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7442322969436646
    },
    {
      "name": "Autoencoder",
      "score": 0.6952414512634277
    },
    {
      "name": "Transformer",
      "score": 0.6734291315078735
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5974684357643127
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.577155590057373
    },
    {
      "name": "Nonlinear system",
      "score": 0.5125651359558105
    },
    {
      "name": "Data mining",
      "score": 0.45598816871643066
    },
    {
      "name": "Machine learning",
      "score": 0.4370279312133789
    },
    {
      "name": "Artificial neural network",
      "score": 0.3988915681838989
    },
    {
      "name": "Engineering",
      "score": 0.07756504416465759
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}