{
    "title": "Low-Resource Machine Translation Using Cross-Lingual Language Model Pretraining",
    "url": "https://openalex.org/W3167012965",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2987720521",
            "name": "Francis Zheng",
            "affiliations": [
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A3011077313",
            "name": "Machel Reid",
            "affiliations": [
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A2167815369",
            "name": "Edison Marrese Taylor",
            "affiliations": [
                "The University of Tokyo"
            ]
        },
        {
            "id": "https://openalex.org/A2102950735",
            "name": "Yutaka Matsuo",
            "affiliations": [
                "The University of Tokyo"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W3089196216",
        "https://openalex.org/W3036839309",
        "https://openalex.org/W2803426146",
        "https://openalex.org/W2811426970",
        "https://openalex.org/W630532510",
        "https://openalex.org/W2963359165",
        "https://openalex.org/W3105378761",
        "https://openalex.org/W3031427604",
        "https://openalex.org/W3119525619",
        "https://openalex.org/W625337927",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2944815030",
        "https://openalex.org/W3116178498",
        "https://openalex.org/W3032816972",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W2529039315",
        "https://openalex.org/W2963088995",
        "https://openalex.org/W2775632580",
        "https://openalex.org/W2963532001",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2949303037",
        "https://openalex.org/W2963506925",
        "https://openalex.org/W3154311556",
        "https://openalex.org/W3164777966",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2250342921",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W2574596763",
        "https://openalex.org/W3128651145"
    ],
    "abstract": "This paper describes UTokyo's submission to the AmericasNLP 2021 Shared Task on machine translation systems for indigenous languages of the Americas. We present a low-resource machine translation system that improves translation accuracy using cross-lingual language model pretraining. Our system uses an mBART implementation of fairseq to pretrain on a large set of monolingual data from a diverse set of high-resource languages before finetuning on 10 low-resource indigenous American languages: Aymara, Bribri, Asháninka, Guaraní, Wixarika, Náhuatl, Hñähñu, Quechua, Shipibo-Konibo, and Rarámuri. On average, our system achieved BLEU scores that were 1.64 higher and chrF scores that were 0.0749 higher than the baseline.",
    "full_text": "Low-Resource Machine Translation Using Cross-Lingual Language\nModel Pretraining\nFrancis Zheng, Machel Reid, Edison Marrese-Taylor, Yutaka Matsuo\nGraduate School of Engineering\nThe University of Tokyo\n{francis, machelreid, emarrese, matsuo}@weblab.t.u-tokyo.ac.jp\nAbstract\nThis paper describes UTokyo’s submission to\nthe AmericasNLP 2021 Shared Task on ma-\nchine translation systems for indigenous lan-\nguages of the Americas. We present a low-\nresource machine translation system that im-\nproves translation accuracy using cross-lingual\nlanguage model pretraining. Our system\nuses an mBART implementation of FAIRSEQ\nto pretrain on a large set of monolingual\ndata from a diverse set of high-resource lan-\nguages before ﬁnetuning on 10 low-resource\nindigenous American languages: Aymara,\nBribri, Asháninka, Guaraní, Wixarika, Náhu-\natl, Hñähñu, Quechua, Shipibo-Konibo, and\nRarámuri. On average, our system achieved\nBLEU scores that were 1.64 higher and CHR F\nscores that were 0.0749 higher than the base-\nline.\n1 Introduction\nNeural machine translation (NMT) systems have\nproduced translations of commendable accuracy\nunder large-data training conditions but are data-\nhungry (Zoph et al., 2016) and perform poorly in\nlow-resource languages, where parallel data is lack-\ning (Koehn and Knowles, 2017).\nMany of the indigenous languages of the Ameri-\ncas lack adequate amounts of parallel data, so exist-\ning NMT systems have difﬁculty producing accu-\nrate translations for these languages. Additionally,\nmany of these indigenous languages exhibit linguis-\ntic properties that are uncommon in high-resource\nlanguages, such as English or Chinese, that are\nused to train NMT systems.\nOne striking feature of many indigenous Ameri-\ncan languages is their polysynthesis (Brinton, 1885;\nPayne, 2014). Polysynthetic languages display\nhigh levels of inﬂection and are morphologically\ncomplex. However, NMT systems are weak in\ntranslating “low-frequency words belonging to\nhighly-inﬂected categories (e.g. verbs)\" (Koehn\nand Knowles, 2017). Quechua, a low-resource,\npolysynthetic American language, has on average\ntwice as many morphemes per word compared to\nEnglish (Ortega et al., 2020b), which makes ma-\nchine translation difﬁcult. Mager et al. (2018b)\nshows that information is often lost when translat-\ning polysynthetic languages into Spanish due to a\nmisalignment of morphemes. Thus, existing NMT\nsystems are not appropriate for indigenous Amer-\nican languages, which are low-resource, polysyn-\nthetic languages.\nDespite the scarcity of parallel data for these in-\ndigenous languages, some are spoken widely and\nhave a pressing need for improved machine trans-\nlation. For example, Quechua is spoken by more\nthan 10 million people in South America, but some\nQuechua speakers are not able to access health care\ndue to a lack of Spanish ability (Freire, 2011).\nOther languages lack a large population of speak-\ners and may appear to have relatively low demand\nfor translation, but many of these languages are\nalso crucial in many domains such as health care,\nthe maintenance of cultural history, and interna-\ntional security (Klavans, 2018). Improved trans-\nlation techniques for low-resource, polysynthetic\nlanguages are thus of great value.\nIn light of this, we participated in the Americas-\nNLP 2021 Shared Task to help further the develop-\nment of new approaches to low-resource machine\ntranslation of polysynthetic languages, which are\nnot commonly studied in natural language process-\ning. The task consisted of producing translations\nfrom Spanish to 10 different indigenous American\nlanguages.\nIn this paper, we describe our system designed\nfor the AmericasNLP 2021 Shared Task, which\nachieved BLEU scores that were 1.64 higher and\nCHR F scores that were 0.0749 higher than the base-\nline on average. Our system improves translation\naccuracy by using monolingual data to improve un-\nderstanding of natural language before ﬁnetuning\n234\nProceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas, pages 234–240\nJune 11, 2021. ©2021 Association for Computational Linguistics\nfor each of the 10 indigenous languages.\n2 Methods\n2.1 Data\nOur model employs two types of data:\n1. 13 GB of monolingual data from Bulgarian,\nEnglish, French, Irish, Korean, Latin, Spanish,\nSundanese, Vietnamese, and Yoruba\n2. 140 MB of parallel data between Spanish\nand Aymara, Bribri, Asháninka, Guaraní,\nWixarika, Náhuatl, Hñähñu, Quechua,\nShipibo-Konibo, and Rarámuri\n2.1.1 Monolingual Data\nWe selected a variety of widely-spoken languages\nacross the Americas, Asia, Europe, Africa, and\nOceania for the monolingual data we used during\nour pretraining, allowing our model to learn from\na wide range of language families and linguistic\nfeatures. These monolingual data were acquired\nfrom CC1001 (Wenzek et al., 2020; Conneau et al.,\n2020). We use these monolingual data as part of\nour pretraining, as this has been shown to improve\nresults with smaller parallel datasets (Conneau and\nLample, 2019; Liu et al., 2020; Song et al., 2019).\n2.1.2 Parallel Data\nThe parallel data between Spanish and the indige-\nnous American languages were provided by Amer-\nicasNLP 2021 (Mager et al., 2021).\nWe have summarized some important details\nof the training data and development/test sets\n(Ebrahimi et al., 2021) below. More details about\nthese data can be found in the AmericasNLP 2021\nofﬁcial repository2.\nAymara The Aymara–Spanish data came from\ntranslations by Global V oices and Facebook AI.\nThe training data came primarily from Global\nV oices3 (Prokopidis et al., 2016; Tiedemann, 2012),\nbut because translations were done by volunteers,\nthe texts have potentially different writing styles.\nThe development and test sets came from transla-\ntions from Spanish texts into Aymara La Paz jilata,\na Central Aymara variant.\n1http://data.statmt.org/cc-100/\n2https://github.com/AmericasNLP/\namericasnlp2021/blob/main/data/\ninformation_datasets.pdf\n3https://opus.nlpl.eu/GlobalVoices.php\nBribri The Bribri–Spanish data (Feldman and\nCoto-Solano, 2020) came from six different\nsources (a dictionary, a grammar, two language\nlearning textbooks, one storybook, and transcribed\nsentences from a spoken corpus) and three major\ndialects (Amubri, Coroma, and Salitre). Two differ-\nent orthographies are widely used for Bribri, so an\nintermediate representation was used to facilitate\ntraining.\nAsháninka The Asháninka–Spanish data4 were\nextracted and pre-processed by Richard Castro\n(Cushimariano Romano and Sebastián Q., 2008;\nOrtega et al., 2020a; Mihas, 2011). Though the\ntexts came from different pan-Ashaninka dialects,\nthey were normalized using AshMorph (Ortega\net al., 2020a). The development and test sets came\nfrom translations of Spanish texts done by Feli-\nciano Torres Ríos.\nGuaraní The Guaraní–Spanish data (Chiruzzo\net al., 2020) consisted of training data from web\nsources (blogs and news articles) written in a mix\nof dialects and development and test sets written in\npure Guaraní. Translations were provided by Perla\nAlvarez Britez.\nWixarika The Wixarika–Spanish data came\nfrom Mager et al. (2018a). The training, devel-\nopment, and test sets all used the same dialect\n(Wixarika of Zoquipan) and orthography, though\nword boundaries were not consistent between the\ndevelopment/test and training sets. Translations\nwere provided by Silvino González de la Crúz.\nNáhuatl The Náhuatl–Spanish data came from\nGutierrez-Vasques et al. (2016). Náhuatl has a\nwide dialectal variation and no standard orthogra-\nphy, but most of the training data were close to\na Classical Náhuatl orthographic “standard.” The\ndevelopment and test sets came from translations\nmade from Spanish into modern Náhuatl. An ortho-\ngraphic normalization was applied to these transla-\ntions to make them closer to the Classical Náhuatl\northography found in the training data. This nor-\nmalization was done by employing a rule-based ap-\nproach based on predictable orthographic changes\nbetween modern varieties and Classical Náhuatl.\nTranslations were provided by Giovany Martinez\nSebastián, José Antonio, and Pedro Kapoltitan.\n4https://github.com/hinantin/\nAshaninkaMT\n235\nHñähñu The Hñähñu–Spanish training data\ncame from translations into Spanish from Hñähñu\ntext from a set of different sources5. Most of these\ntexts are in the Valle del Mezquital dialect. The\ndevelopment and test sets are in the Ñûhmû de Ix-\ntenco, Tlaxcala variant. Translations were done by\nJosé Mateo Lino Cajero Velázquez.\nQuechua The training set for Quechua–Spanish\ndata (Agi´c and Vuli´c, 2019) came from Jehova’s\nWitnesses texts (available in OPUS), sentences ex-\ntracted from the ofﬁcial dictionary of the Minis-\nter of Education (MINEDU) in Peru for Quechua\nAyacucho, and dictionary entries and samples col-\nlected and reviewed by Diego Huarcaya. Training\nsets were provided in both the Quechua Cuzco and\nQuechua Ayacucho variants, but our system only\nemployed Quechua Ayacucho data during training.\nThe development and test sets came from transla-\ntions of Spanish text into Quechua Ayacucho, a\nstandard version of Southern Quechua. Transla-\ntions were provided by Facebook AI.\nShipibo-Konibo The training set of the Shipibo-\nKonibo–Spanish data (Galarreta et al., 2017) was\nobtained from translations of ﬂashcards and trans-\nlations of sentences from books for bilingual ed-\nucation done by a bilingual teacher. Additionally,\nparallel sentences from a dictionary were used as\npart of the training data. The development and\ntest sets came from translations from Spanish into\nShipibo-Konibo done by Liz Chávez.\nRarámuri The training set of the Rarámuri–\nSpanish data came from a dictionary (Brambila,\n1976). The development and test sets came from\ntranslations from Spanish into the highlands Rará-\nmuri by María del Cármen Sotelo Holguín. The\ntraining set and development/test sets use different\northographies.\n2.2 Preprocessing\nWe tokenized all of our data together using Sen-\ntencePiece (Kudo and Richardson, 2018) in prepa-\nration for our multilingual model. We used a vo-\ncabulary size of 8000 and a character coverage of\n0.9995, as the wide variety of languages cover a\nrich character set.\nThen, we sharded our data for faster processing.\nWith our SentencePiece model and vocabulary, we\n5https://tsunkua.elotl.mx/about/\nused FAIRSEQ 6 (Ott et al., 2019) to build vocabu-\nlaries and binarize our data.\n2.3 Pretraining\nWe pretrained our model on the 20 languages de-\nscribed in 2.1 with an mBART (Liu et al., 2020)\nimplementation of FAIRSEQ (Ott et al., 2019). We\npretrained on 32 NVIDIA V100 GPUs for three\nhours.\nBalancing data across languages\nDue to the large variability in text data size be-\ntween different languages, we used the exponen-\ntial sampling technique used in Conneau and Lam-\nple (2019); Liu et al. (2020), where the text is re-\nsampled according to smoothing parameter αas\nfollows:\nqi = pα\ni∑N\nj=1 pα\nj\n(1)\nIn equation 1, qi refers to the resample probabil-\nity for language i, given multinomial distribution\n{qi}i=1...N with original sampling probability pi.\nAs we want our model to work well with the\nlow-resource languages, we chose a smoothing pa-\nrameter of α= 0.25 (compared with α= 0.7 used\nin mBART (Liu et al., 2020)) to alleviate model\nbias towards the higher proportion of data from\nhigh-resource languages.\nHyperparameters\nWe used a six-layer Transformer with a hidden di-\nmension of 512 and feed-forward size of 2048. We\nset the maximum sequence length to 512, with a\nbatch size of 1024. We optimized the model using\nAdam (Kingma and Ba, 2015) using hyperparam-\neters β = (0.9,0.98) and ϵ = 10−6. We used a\nlearning rate of 6 ×10−4 over 10,000 iterations.\nFor regularization, we used a dropout rate of 0.5\nand weight decay of 0.01. We also experimented\nwith lower dropout rates but found that a higher\ndropout rate gave us a model that produces better\ntranslations.\n2.4 Finetuning\nUsing our pretrained model, we performed ﬁnetun-\ning on each of the 10 indigenous American lan-\nguages with the same hyperparameters used dur-\ning pretraining. For each language, we conducted\nour ﬁnetuning using four NVIDIA V100 GPUs for\nthree hours.\n6https://github.com/pytorch/fairseq\n236\nLanguage Baseline 1 Dev2 Test13 Test24\nBLEU CHR F BLEU CHR F BLEU CHR F BLEU CHR F\nAymara (aym) 0.01 0.157 2.84 0.2338 1.17 0.214 1.03 0.209\nBribri (bzd) 0.01 0.058 1.22 0.1203 1.7 0.143 1.29 0.131\nAsháninka (cni) 0.01 0.102 0.48 0.2188 0.2 0.216 0.45 0.214\nGuaraní (gn) 0.12 0.193 3.64 0.2492 3.21 0.265 3.16 0.254\nWixarika (hch) 2.2 0.126 4.89 0.2093 7.09 0.238 6.74 0.229\nNáhuatl (nah) 0.01 0.157 0.3 0.253 0.55 0.239 1.2 0.238\nHñähñu (oto) 0 0.054 0.04 0.1035 2.45 0.152 1.28 0.133\nQuechua (quy) 0.05 0.304 1.46 0.3155 2.35 0.332 2.47 0.33\nShipibo-Konibo (shp) 0.01 0.121 0.49 0.176 0.33 0.163 0.71 0.175\nRarámuri (tar) 0 0.039 0.12 0.1163 0.1 0.122 0.06 0.123\n1 Baseline test results provided by AmericasNLP 2021, from a system where the development set was not used for\ntraining\n2 Our own results on the development set\n3 Our ofﬁcial test results for our system where the development set was used for training\n4 Our ofﬁcial test results for our system where the development set was not used for training\nTable 1: Results\n2.5 Evaluation\nUsing the SacreBLEU library 7 (Post, 2018), we\nevaluated our system outputs with detokenized\nBLEU (Papineni et al., 2002; Post, 2018). Due to\nthe polysynthetic nature of the languages involved\nin this task, we also used CHR F (Popovi´c, 2015)\nto measure performance at the character level and\nbetter see how well morphemes or parts of mor-\nphemes were translated, rather than whole words.\nFor these reasons, we focused on optimizing the\nCHR F score.\n3 Results\nWe describe our results in Table 1. Our test re-\nsults (Test1and Test2) show considerable improve-\nments over the baseline provided by AmericasNLP\n2021. We also included our own results on the de-\nvelopment set (Dev) for comparison. The trends we\nsaw in the Dev results parallel our test results; lan-\nguages for which our system achieved high scores\nin Dev (e.g. Wixarika and Guaraní) also demon-\nstrated high scores in Test1 and Test2. Likewise,\nlanguages for which our system performed rela-\ntively poorly in Dev (e.g. Rarámuri, whose poor\nperformance may be attributed to the difference in\northographies between the training set and develop-\nment/test sets) also performed poorly in Test1 and\nTest2. This matches the trend seen in the baseline\nscores.\nThe baseline results and Test2 results were both\n7https://github.com/mjpost/sacrebleu\nproduced using the same test set and by systems\nwhere the development set was not used for train-\ning. Thus, the baseline results and Test2 results\ncan be directly compared. On average, our system\nused to produce the Test2 results achieved BLEU\nscores that were 1.54 higher and CHR F scores that\nwere 0.0725 higher than the baseline. On the same\ntest set, our Test1 system produced higher BLEU\nand CHR F scores for nearly every language. This\nis expected, as the system used to produce Test1\nwas trained on slightly more data; it used the devel-\nopment set of the indigenous American languages\nprovided by AmericasNLP 2021 in addition to the\ntraining set.\nIf we factor in our results from Test1 to our\nTest2 results, we achieved BLEU scores that were\n1.64 higher and CHR F scores that were 0.0749\nhigher than the baseline on average. Overall, we\nattribute this improvement in scores primarily to\nthe cross-lingual language model pretraining (Con-\nneau and Lample, 2019) we performed, allowing\nour model to learn about natural language from the\nmonolingual data before ﬁnetuning on each of the\n10 indigenous languages.\n4 Conclusions and Future Work\nWe described our system to improve low-resource\nmachine translation for the AmericasNLP 2021\nShared Task. We constructed a system using the\nmBART implementation of FAIRSEQ to translate\nfrom Spanish to 10 different low-resource indige-\nnous languages from the Americas. We demon-\n237\nstrated strong improvements over the baseline by\npretraining on a large amount of monolingual data\nbefore ﬁnetuning our model for each of the low-\nresource languages.\nWe are interested in using dictionary augmenta-\ntion techniques and creating pseudo-monolingual\ndata to use during the pretraining process, as\nwe have seen improved results with these two\ntechniques when translating several low-resource\nAfrican languages. We can also incorporate these\ntwo techniques in an iterative pretraining proce-\ndure (Tran et al., 2020) to produce more pseudo-\nmonolingual data and further train our pretrained\nmodel for potentially better results.\nFuture research should also explore using prob-\nabilistic ﬁnite-state morphological segmenters,\nwhich may improve translations by exploiting regu-\nlar agglutinative patterns without the need for much\nlinguistic knowledge (Mager et al., 2018a) and thus\nmay work well with the low-resource, polysyn-\nthetic languages dealt with in this paper.\nReferences\nŽeljko Agi ´c and Ivan Vuli ´c. 2019. JW300: A wide-\ncoverage parallel corpus for low-resource languages.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3204–3210, Florence, Italy. Association for Compu-\ntational Linguistics.\nDavid Brambila. 1976. Diccionario Raramuri–\nCastellano (Tarahumara). Obra Nacional de la\nBuena Prensa, México.\nD.G. Brinton. 1885. On Polysynthesis and Incorpo-\nration: As Characteristics of American Languages.\nMcCalla & Stavely.\nLuis Chiruzzo, Pedro Amarilla, Adolfo Ríos, and Gus-\ntavo Giménez Lugo. 2020. Development of a\nGuarani - Spanish parallel corpus. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 2629–2633, Marseille, France. Euro-\npean Language Resources Association.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems 32: An-\nnual Conference on Neural Information Processing\nSystems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057–7067.\nRubén Cushimariano Romano and Richer C. Se-\nbastián Q. 2008. Ñaantsipeta asháninkaki bi-\nrakochaki. diccionario asháninka-castellano. versión\npreliminar. http://www.lengamer.org/\npublicaciones/diccionarios/.\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay,\nVishrav Chaudhary, Luis Chiruzzo, Angela Fan,\nJohn Ortega, Ricardo Ramos, Annette Rios, Ivan\nVladimir, Gustavo A. Giménez-Lugo, Elisabeth\nMager, Graham Neubig, Alexis Palmer, Rolando\nA. Coto Solano, Ngoc Thang Vu, and Katharina\nKann. 2021. Americasnli: Evaluating zero-shot nat-\nural language understanding of pretrained multilin-\ngual models in truly low-resource languages.\nIsaac Feldman and Rolando Coto-Solano. 2020. Neu-\nral machine translation models with back-translation\nfor the extremely low-resource indigenous language\nBribri. In Proceedings of the 28th International\nConference on Computational Linguistics, pages\n3965–3976, Barcelona, Spain (Online). Interna-\ntional Committee on Computational Linguistics.\nGermán Freire. 2011. Perspectivas en salud indígena:\ncosmovisión, enfermedad y políticas públicas. Edi-\nciones Abya-Yala.\nAna-Paula Galarreta, Andrés Melgar, and Arturo On-\ncevay. 2017. Corpus creation and initial SMT ex-\nperiments between Spanish and Shipibo-konibo. In\nProceedings of the International Conference Recent\nAdvances in Natural Language Processing, RANLP\n2017, pages 238–244, Varna, Bulgaria. INCOMA\nLtd.\nXimena Gutierrez-Vasques, Gerardo Sierra, and\nIsaac Hernandez Pompa. 2016. Axolotl: a web\naccessible parallel corpus for Spanish-Nahuatl. In\nProceedings of the Tenth International Conference\non Language Resources and Evaluation (LREC’16),\npages 4210–4214, Portorož, Slovenia. European\nLanguage Resources Association (ELRA).\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJudith L. Klavans. 2018. Computational challenges\nfor polysynthetic languages. In Proceedings of the\nWorkshop on Computational Modeling of Polysyn-\nthetic Languages, pages 1–11, Santa Fe, New Mex-\nico, USA. Association for Computational Linguis-\ntics.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\n238\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nManuel Mager, Diónico Carrillo, and Ivan Meza.\n2018a. Probabilistic ﬁnite-state morphological seg-\nmenter for Wixarika (Huichol) language. Journal of\nIntelligent & Fuzzy Systems, 34(5):3081–3087.\nManuel Mager, Elisabeth Mager, Alfonso Medina-\nUrrea, Ivan Vladimir Meza Ruiz, and Katharina\nKann. 2018b. Lost in translation: Analysis of in-\nformation loss during machine translation between\npolysynthetic and fusional languages. In Proceed-\nings of the Workshop on Computational Modeling\nof Polysynthetic Languages, pages 73–83, Santa Fe,\nNew Mexico, USA. Association for Computational\nLinguistics.\nManuel Mager, Arturo Oncevay, Abteen Ebrahimi,\nJohn Ortega, Annette Rios, Angela Fan, Xi-\nmena Gutierrez-Vasques, Luis Chiruzzo, Gustavo\nGiménez-Lugo, Ricardo Ramos, Anna Currey,\nVishrav Chaudhary, Ivan Vladimir Meza Ruiz,\nRolando Coto-Solano, Alexis Palmer, Elisabeth\nMager, Ngoc Thang Vu, Graham Neubig, and Katha-\nrina Kann. 2021. Findings of the AmericasNLP\n2021 Shared Task on Open Machine Translation for\nIndigenous Languages of the Americas. In Proceed-\nings of the The First Workshop on NLP for Indige-\nnous Languages of the Americas, Online. Associa-\ntion for Computational Linguistics.\nElena Mihas. 2011. Añaani katonkosatzi parenini, El\nidioma del alto Perené. Milwaukee, WI: Clarks\nGraphics.\nJohn Ortega, Richard Alexander Castro-Mamani, and\nJaime Rafael Montoya Samame. 2020a. Overcom-\ning resistance: The normalization of an Amazonian\ntribal language. In Proceedings of the 3rd Workshop\non Technologies for MT of Low Resource Languages,\npages 1–13, Suzhou, China. Association for Compu-\ntational Linguistics.\nJohn E. Ortega, Richard Castro Mamani, and\nKyunghyun Cho. 2020b. Neural machine trans-\nlation with a polysynthetic low resource language.\nMachine Translation, 34(4):325–346.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nD.L. Payne. 2014. Morphological Characteristics of\nLowland South American Languages. University of\nTexas Press.\nMaja Popovi ´c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392–395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nProkopis Prokopidis, Vassilis Papavassiliou, and Ste-\nlios Piperidis. 2016. Parallel Global Voices: a col-\nlection of multilingual corpora with citizen media\nstories. In Proceedings of the Tenth International\nConference on Language Resources and Evaluation\n(LREC’16), pages 900–905, Portorož, Slovenia. Eu-\nropean Language Resources Association (ELRA).\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to se-\nquence pre-training for language generation. In Pro-\nceedings of the 36th International Conference on\nMachine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA, volume 97 of Pro-\nceedings of Machine Learning Research , pages\n5926–5936. PMLR.\nJörg Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eighth In-\nternational Conference on Language Resources and\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nChau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. 2020.\nCross-lingual retrieval for iterative self-supervised\ntraining. 34th Conference on Neural Informa-\ntion Processing Systems (NeurIPS 2020), Vancouver,\nCanada.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n239\n4003–4012, Marseille, France. European Language\nResources Association.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource\nneural machine translation. In Proceedings of the\n2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1568–1575, Austin,\nTexas. Association for Computational Linguistics.\n240"
}