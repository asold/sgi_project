{
  "title": "Comparison of Prompt Engineering and Fine-Tuning Strategies in Large Language Models in the Classification of Clinical Notes",
  "url": "https://openalex.org/W4391640544",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2112667573",
      "name": "Xiaodan Zhang",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5113080810",
      "name": "Nabasmita Talukdar",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2788034717",
      "name": "Sandeep Vemulapalli",
      "affiliations": [
        "Spectrum Health",
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2807801243",
      "name": "Sumyeong Ahn",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2105343037",
      "name": "Jiankun Wang",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2111340035",
      "name": "Han Meng",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5109681361",
      "name": "Sardar Mehtab Bin Murtaza",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2776745589",
      "name": "Dmitry Leshchiner",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5111126403",
      "name": "Aakash Ajay Dave",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A4213683088",
      "name": "Dimitri F Joseph",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A4286391439",
      "name": "Martin Witteveen‐Lane",
      "affiliations": [
        "Spectrum Health"
      ]
    },
    {
      "id": "https://openalex.org/A1941271915",
      "name": "Dave Chesla",
      "affiliations": [
        "Michigan State University",
        "Spectrum Health"
      ]
    },
    {
      "id": "https://openalex.org/A2097879502",
      "name": "Jiayu Zhou",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2095826315",
      "name": "Bin Chen",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2112667573",
      "name": "Xiaodan Zhang",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5113080810",
      "name": "Nabasmita Talukdar",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2788034717",
      "name": "Sandeep Vemulapalli",
      "affiliations": [
        "Spectrum Health",
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2807801243",
      "name": "Sumyeong Ahn",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2105343037",
      "name": "Jiankun Wang",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2111340035",
      "name": "Han Meng",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5109681361",
      "name": "Sardar Mehtab Bin Murtaza",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2776745589",
      "name": "Dmitry Leshchiner",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A5111126403",
      "name": "Aakash Ajay Dave",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A4213683088",
      "name": "Dimitri F Joseph",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A4286391439",
      "name": "Martin Witteveen‐Lane",
      "affiliations": [
        "Spectrum Health"
      ]
    },
    {
      "id": "https://openalex.org/A1941271915",
      "name": "Dave Chesla",
      "affiliations": [
        "Michigan State University",
        "Spectrum Health"
      ]
    },
    {
      "id": "https://openalex.org/A2097879502",
      "name": "Jiayu Zhou",
      "affiliations": [
        "Michigan State University"
      ]
    },
    {
      "id": "https://openalex.org/A2095826315",
      "name": "Bin Chen",
      "affiliations": [
        "Michigan State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4286376980",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4386867830",
    "https://openalex.org/W4386322180",
    "https://openalex.org/W4367369313",
    "https://openalex.org/W4380373763",
    "https://openalex.org/W4385757404",
    "https://openalex.org/W2789244308",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4221150520"
  ],
  "abstract": "Abstract The emerging large language models (LLMs) are actively evaluated in various fields including healthcare. Most studies have focused on established benchmarks and standard parameters; however, the variation and impact of prompt engineering and fine-tuning strategies have not been fully explored. This study benchmarks GPT-3.5 Turbo, GPT-4, and Llama-7B against BERT models and medical fellows’ annotations in identifying patients with metastatic cancer from discharge summaries. Results revealed that clear, concise prompts incorporating reasoning steps significantly enhanced performance. GPT-4 exhibited superior performance among all models. Notably, one-shot learning and fine-tuning provided no incremental benefit. The model’s accuracy sustained even when keywords for metastatic cancer were removed or when half of the input tokens were randomly discarded. These findings underscore GPT-4’s potential to substitute specialized models, such as PubMedBERT, through strategic prompt engineering, and suggest opportunities to improve open-source models, which are better suited to use in clinical settings.",
  "full_text": " \n \nComparison of Prompt Engineering and Fine-Tuning Strategies in Large \nLanguage Models in the Classification of Clinical Notes \n \nXiaodan Zhang 1, Nabasmita Talukdar 1, Sandeep Vemulapalli 1,2, Sumyeong Ahn 3, Jiankun \nWang3, Han Meng3, Sardar Mehtab Bin Murtaza3, Dmitry Leshchiner1, Aakash Ajay Dave1,5, \nDimitri F. Joseph4, Martin Witteveen-Lane2, Dave Chesla2,6, Jiayu Zhou3, and Bin Chen1,3,4,* \n \n1Department of Pediatrics and Human Development, College of Human Medicine, Michigan \nState University, Grand Rapids, MI, USA \n2Office of Research, Spectrum Health, Grand Rapids, MI, USA \n3Department of Computer Science and Engineering, College of Engineering, Michigan State \nUniversity, East Lansing, MI, USA \n4Department of Pharmacology and Toxicology, College of Human Medicine, Michigan State \nUniversity, Grand Rapids, MI, USA \n5Center for Bioethics and Social Justice, Michigan State University, Grand Rapids, MI, USA \n6Department of Obstetrics, Gynecology and Reproductive Biology, College of Human \nMedicine, Michigan State University, Grand Rapids, MI, USA \n \n*Correspondences: Bin Chen (chenbi12@msu.edu)  \n \nAbstract \nThe emerging large language models (LLMs) are actively evaluated in various fields including healthcare. Most \nstudies have focused on established benchmarks and standard parameters; however, the variation and impact of \nprompt engineering and fine -tuning strategies have not been fully explored. This study bench marks GPT -3.5 \nTurbo, GPT-4, and Llama-7B against BERT models and medical fellows' annotations in identifying patients with \nmetastatic cancer from discharge summaries. Results revealed that clear, concise prompts incorporating \nreasoning steps significantly enhanced performance. GPT -4 exhibited superior performance among all models. \nNotably, one-shot learning and fine-tuning provided no incremental benefit. The model's accuracy sustained even \nwhen keywords for metastatic cancer were removed or when half of the input tokens were random ly discarded. \nThese findings underscore GPT -4's potential to substitute specialized models, such as PubMedBERT, through \nstrategic prompt engineering, and suggest opportunities to improve open-source models, which are better suited \nto use in clinical settings. \n \nIntroduction \nLarge Language Models (LLMs) are transforming many fields 1-2. Built on the Transformer architecture 3, these \nmodels employ billions, sometimes, trillions, of tokens and parameters, enabling them to accomplish a range of \ntasks that were previously considered unattainable. From answering straightforward questions to handling \ncomplex reasoning tasks, their performance is often astonishing. Inspired by the scaling law 4, which revealed a \npositive correlation between model performance and the number of parameters employed, tech companies are \nincreasingly investing in these models using their proprietary training sets and massive computing power. Some \nmodels could be acces sed through the API, such as OpenAI' s GPT-35 (trained with 175 billion parameters and \n300 billion tokens) or the software packages, including Meta's Llama6 (trained with 65 billion parameters and 1.4 \ntrillion tokens), and Google's PaLM 7 (trained with 540 billion parameters and 780 billion tokens). While some \nspecifications, like the number of parameters, are publicly disclosed, many technical intricacies remain \nproprietary. Moreover, the task performance highly relies on skilled user prompting. \n  \nIn the realm of biomedicine, LLM applications are burgeoning. While pretrained models like GatorTron8, trained \non >80 billion words extracted from de -identified clinical text, showcased their capabilities across multiple \nclinical Natural Language Processing (NLP) tasks, most of the efforts remained on the fine-tuning of open-source \nLLM models. For example , Med -PaLM was a fine -tuned model from PaLM for biomedical research and \nperformed considerably well but remained inferior to clinicians 9. Compared to fine -tuned biomedical pretrained \nlanguage models, LLMs like GPT -3.5 and GPT -4 showed promise in biomedical semantic similarity and \nreasoning tasks but are less effective in information extraction and classification 10. Despite these efforts, few \nstudies have systematically evaluated various prompting and fine -tuning strategies in published LLMs and \ncompared with human performance. \n  \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n \n \nBuilding on our prior work 11, where we used early versions of language models like BERT 12 for identifying \nmetastatic cancer patients through clinical notes, we aim to further this research. Metastatic cancer is a leading \ncause of cancer-related deaths, thus identifying patients with metastatic cancer for early intervention is crucial for \nimproving survival. In the Electronic Health Record  (EHR) systems like Epic, metastatic cancer is often \ninadequately defined and labeled11, therefore, identifying those patients from clinical notes is intriguing. Training \nlanguage models from scratch for clinical notes is resource -intensive, making pre -trained models followed by \ndomain-specific fine -tuning a more viable alternative. In our previous study, we investigated several BERT \nvariations such as BioBERT13, clinicalBERT14, and PubMedBERT15, and found that fine-tuning on PubMedBERT \nyielded the best results. However, compared to those LLMs trained with billions of parameters, the 110 million \nparameters in BERT are quite small. Moreover, although we could use tools to identify important featur es in the \nBERT models, compared to LLMs trained on large corpus, their reasoning capabi lities remained limited. \nPrompt engineering is an integral part to enhance model output quality, especially in complex tasks demanding \ncausal reasoning16. In general, there are three different types of methods: zero-shot, one-shot and few-shot. Giving \nclear and specific instructions, describing the overall context, utilizing explicit constraints, and asking to play \nroles are some techniques that can be emp loyed to achieve better results 17-19. It is difficult to recommend a best \npractice, but with iterative testing and refinement, p rompts can be significantly improved. Recent studies \ndemonstrate that utilizing structured prompts can elevate LLM performance in medical diagnostics, where \nprecision and interpretability are essential20-22. \nTherefore, in this study, using metastatic cancer identification as a case study, we explored multiple recommended \nprompting strategies as well as common LLMs. We also evaluated the impact of various key parameters and \ncompared their performance with that of domain experts. By fusing domain knowledge and chains of thought, we \nproposed an optimal prompt for in-context zero-shot learning and observed its favorable outcomes. \n \nMethods \nDataset and Data Preprocessing \nWe used the approach previously presented in ,23 to prepare the dataset from MIMIC -III24. More details can be \nfound in Appendix A. As a result, we created a dataset that includes 1,873 discharge summaries with 178 patients \nwith metastatic cancer. Each record consists of a discharge summary and a label for metastatic cancer. We \nconducted multiple pre -processing steps on the discharge  summaries by following the same strategy in 11. We \nemployed a 7:2:1 ratio to split the data into training, validation, and testing sets.  \n \nManual Annotation \nWe engaged a panel of three medical fellows, each either possessing a medical degree or currently undergoing \nmedical training, to manually annotate the test set. The fellows were provided with concise guidelines on how to \nidentify instances of metastatic c ancer based solely on the patient's discharge summaries, without the aid of any \nsupplementary tools. For this study, 'metastatic cancer' was defined according to criteria specified in the previous \nwork23: \" Cancers with very high or imminent mortality (panc reas, esophagus, stomach, cholangiocarcinoma, \nbrain); mention of distant or multi -organ metastasis, where palliative care would be considered (prognosis < 6 \nmonths)\". This definition guided the annotation process to ensure consistency and accuracy in the identification \nof relevant cases. A designated coordinator shared a screen with the annotators and advanced to the subsequent \npage only upon unanimous agreement amon g the fellows. To ensure anonymity, all annotations were recorded \nwithout identifiers. In to tal, the panel successfully annotated 188 discharge summaries in the test set within six \nhours. On average, each multi -page summary required approximately two minutes for thorough review. The \nanalysis of the annotated data was conducted using Python. \n \nLLMs \nWe utilized various LLMs including OpenAI's GPT models and Meta's Llama, to classify the presence of \nmetastatic cancer within discharge summaries. The setup and deployment of these models, alongside prompt \nengineering, one-shot, and fine-tuning strategies, were designed to ensure a rigorous evaluation protocol. \n \nOpenAI's GPT models: The deployment of OpenAI’s GPT models,  specifically GPT-3.5 Turbo and GPT -4, \nwas executed on Microsoft Azure cloud computing service. An Azure subscription was obtained,  and a dedicated \nResource Group was established to manage essential resources such as compute instances and storage services.  \nThese models were then deployed within this Resource Group, ensuring an organized and efficient management \nof cloud resources. Access to the models was facilitated via the OpenAI Application Programming Interface \n(API), which required authentication using a unique API key. To adhere to OpenAI's API usage policies, a robust \nrate limiting and request batching mechanism was implemented. The interaction with the API was encapsulated \nwithin a Python function designed to handle API requests with a retry mechanis m. This function was tailored to \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \n \n \nefficiently manage both high-volume requests and API-specific errors, such as rate limit exceedances. It ensured \nreliable and efficient predictions retrieval from the GPT models. \n \nIn GPT models, the temperature parameter is used to control the randomness and creativity of the generated text \nin a generative language model. It adjusts the probabilities of predicted words in the model's SoftMax output layer. \nA lower temperature sharpens these probabilities, making the word with the highest probability more likely to be \nchosen. As a result, the output becomes more conservative and stable. We have set the temperature to be 0.2 to \nget a more deterministic response. We have also tested for different token sizes to determine whether the output \ndiffers in any way. We have first tried one prompt with a variety of token sizes ranging from (500, 1000, 1500, \n2000, 3000, 4000) and observed a variation in the F1 scores and other performance metrics.  To illustrate the \nchanges in F1 scores, we ultimately chose 1500 tokens and 3000 tokens to apply to all prompts.  \n  \nMeta Llama: Meta offers Llama as an open -source LLM. Llama comes in two main versions, with version 1 \n(V1) being released in February 2023, and version 2 (V2) in July of the same year. V1 offers model sizes of 7B, \n13B, 33B, and 65B, and V2 provides sizes of 7B, 13B, and 70B. In this paper, fine -tuning was conducted using \nthe 7B model from Llama V1 due to the hardware constraints of using larger models. These experiments were \nconducted with the research purpose consent provided by Meta and  were carried out u sing the Huggingface \nModule. \n \nPrompt Engineering \nInitially, we created a baseline prompt (prompt 0) based on the suggestion from ChatGPT and common \nknowledge. Following the concept of chains of thought, we refined these prompts by adding instructional steps. \nWe also incorporated a universal prompt, as su ggested in a recent study that introduced zero -shot reasoners, by \nsimply adding \"Let's think step by step\" before each answer25. In total, we explored six different prompts. \n  \n#Prompt 0 \n\"Please act as a curator. Based on the input discharge summary, classify if the patient has metastatic cancer or not. \nPlease provide a concise response as either 'Yes' or ‘No'. \" \n \n#Prompt 1 \n\"Please act as a curator. Based on the input discharge summary, classify if the patient has metastatic cancer or not \nusing the following steps. \n Step 1: identify if this patient has cancer or not. \n Step 2: if this patient has cancer, identify its staging, grade, and primary site. \n Step 3: if this patient has metastatic cancer, identify its primary site, and metastatic.  \n Give a final decision 'Yes' or 'No' only.” \n \n#Prompt 2 \n\"This is a discharge summary for a patient who underwent diagnostic tests, please act as a healthcare professional \nand classify if the patients has metastatic cancer or not using following steps: \n1. Identify if a patient has cancer or not. \n2. If this patient has cancer, identify if it's metastatic cancer. \n3. Based on the information please provide a concise response as either 'Yes' or 'No'.\" \n \n#Prompt 3 \n\"This is a discharge summary for a patient who recently underwent diagnostic tests for suspected metastatic \ncancer. Please analyze the following information and provide a concise response as either 'Yes' or 'No' based on \nthe presence of only metastatic cancer in the patient's discharge summary.\" \n \n#Prompt 4  \n\"This is a discharge summary for a patient. Please act as a healthcare professional and provide a response based \non the summary using the following instructions: \n1. Identify if a patient has metastatic cancer or not. \n2. If there is clear evidence of metastatic cancer, respond 'Yes'. \n3. If there is no strong evidence of metastatic cancer or if the evidence is unclear, respond 'No.'  \nPlease provide a concise response as either 'Yes' or ‘No’. \" \n  \n#Prompt 5 \n\"Please act as a curator and analyze the following discharge summary, classify if the patient has metastatic cancer \nor not. Let's think step by step. Choose the final answer from the list 'Yes' or 'No'.\" \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \n \n \nContext in Zero-Shot, One-Shot Learning, and Fine-Tuning \nTo use LLMs for our target classification task, there are three approaches we study in this work. The first approach \nis zero-shot learning, which inputs a combination of our designed prompts and clinical notes on untouched LLMs \nwithout giving any example. We can also conduct one-shot learning, which adds a few examples of clinical note \nand prediction labels, in addition to the inputs used by zero-shot learning, again on untouched LLMs. Finally, we \ncan revise LLMs by fine-tuning clinical notes from the training set and adjusting the distribution of LLMs through \nback-propagation.  \n \nWe tested the proposed six prompts in a zero-shot learning setting. We found that with the proper construction of \nprompts, one shot learning did not offer any additional benefits, so we focused primarily on zero -shot learning. \nFor the fine-tuning process, we utilized the Llama architecture. The Llama model, known for its versatility and \nefficiency, offers various sizes ranging from 7 billion to 65 billion parameters. In our research, we leverage the \nLlama-7B model, which has been adapted for compatibility with the Transformers/HuggingFace framework. To \nmake it better suited for specific tasks, such as c linical note classification and other medical -related challenges, \nwe contemplate further fine -tuning of the Llama -7B model. Technically, we further employ approaches such as \nParameter-Efficient Fine-Tuning (PEFT)26, 4-bit floating-point quantization27, and low-rank adaptation (LoRA)28 \nin our fine-tuning process to make it more efficient. The number of fine-tuning epochs in our experiment is setting \nto be 3. And the batch size is 8. Within the optimization phase, we have chosen the AdamW(32 -bit) optimizer, a \nwidely adopted selection for fine -tuning large language models. Furthermore, for our experiments, we have \nstandardized the token size within the Llama model to 2048 tokens, which is appropriate in our computing \nenvironment. The Llama fine -tuning experiment was executed within a high -performance computing \nenvironment, featuring four NVIDIA A5000 graphics cards, each having 24 gigabytes (GB) of GPU memory. \nThis robust hardware configuration provided us with ample computational power and memory capacity to \nefficiently fine-tune the Llama-7B model on our training dataset. \n  \nEvaluation \nTo assess the classification performance of the GPT models, we executed a rigorous evaluation protocol across \nvarious prompts and input sizes (1500 and 3000 tokens). This analysis included the utilization of both GPT -3.5 \nTurbo and GPT -4 models. For each pr ompt configuration, we performed five separate runs to ensure result \nconsistency and reliability. The evaluations were done using input lengths of 1500 and 3000 tokens to determine \nthe impact of input size on the classification quality. Key metrics such as  F1 scores, recall, and precision were \nrecorded for each run. The averages of these metrics were then computed and summarized in a comprehensive \ntable, facilitating direct comparisons of the effectiveness of each prompt and input size on the model's \nclassification accuracy. For manual annotation, the inter-annotator agreement was measured using Fleiss' Kappa, \na statistical measure that accounts for agreement occurring by chance. The performance of each medical fellow \nannotator was individually assessed in t erms of F1 score, recall, and precision to gauge the reliability of manual \nannotations in identifying metastatic cancer within the discharge summaries. Comparative analysis was conducted \nacross all models and methods employed in the study, including PubMedBERT, and Meta Llama -7B. For each \nmodel and approach, we reported the F1 scores, recall, and precision metrics derived from the test set. This enabled \na thorough comparative evaluation, highlighting the relative strengths and weaknesses of each method in the task \nof metastatic cancer classification from clinical notes. All the analyses were conducted using Python, leveraging \nlibraries such as Pandas for data manipulation, Scikit-learn for model evaluation, and Huggingface's Transformers \nfor LLM interactions. This ensured a consistent and reproducible analysis environment.  \n \nResults \nData Statistics and Baseline Performance \nBefore training the model, the dataset was preprocessed and tokenized. The token counts for each row in the \ntraining, validation, and test sets were calculated to understand the distribution of the data and to ensure that it \naligns with the model's limitations. Within the training set, token counts ranged from a minimum of 5 tokens to a \nmaximum of 5348 tokens. On average, each record contained approximately 1791 tokens, with a standard \ndeviation of 1008 tokens. In the validation set, the token values spanned from a minimum of 27 to a maximum of \n5401 tokens. The average token count for this set were around 1747 tokens, accompanied by a standard deviation \nof approximately 1004 tokens. Similarly, the test dataset displayed token counts that varied from a minimu m of \n15 to a maximum of 5104 tokens. On average, each row consisted of approximately 1824 length of tokens, and \nthe standard deviation was approximately 973 tokens. \n \nThe PubMedBERT model was implemented utilizing the MetBERT code repository from GitHub 11. In the \nidentification of cases with metastasis, the precision was 0.790 and a recall of 0.610, resulting in an F1 -score of \n0.690. These metrics indicate the model’s effectiveness in correctly classifying instances with and without \nmetastasis. \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \n \n \nManual Annotation \nIn evaluating the reliability of manual annotations for metastatic cancer identification within discharge summaries, \nwe examined inter -annotator agreement and individual annotator performance ( Table 1 ). Using the percent \nagreement metric, we found an impressive 94% agreement among the three medical fellows, demonstrating the \nrobustness of the manual annotation process and dataset validity. The high percent agreement suggests a shared \nunderstanding of annotation guidelines. Fleiss' Kappa κ, indicating inter -rater agreement beyond chance, yielded \na substantial value of 0.868, further confirming annotation reliability.  Together, these metrics suggest that the \nannotators were highly competent in identifying metastatic cancer cases from patients’ discharge summaries.  \n \nTable 1: Evaluation Metrics of Annotator Performance Compared to Actual Cases \nAnnotator F1 Score Recall Precision \n1 0.710 0.611 0.846 \n2 0.774 0.667 0.923 \n3 0.727 0.667 0.800 \n  \nZero-shot and One-shot Learning \nWe explored the GPT-3.5 Turbo and GPT -4 models using the test set. Regardless of the input token size and \nprompts, GPT-4 consistently outperformed GPT-3.5 Turbo, often by a large margin (Table 2).  \n \nTable 2: Evaluation of the Six Prompts in GPT Models Using the Test Set \nGPT Model Prompt ID Input size F1 Score Recall Precision \n \n \n \n \n \n \n \nGPT-3.5 \nTurbo \n \nPrompt 0 \n1500 0.341 0.811 0.210 \n3000 0.318 0.877 0.195 \n \nPrompt 1 \n1500 0.332 0.868 0.215 \n3000 0.357 0.855 0.232 \n \nPrompt 2 \n1500 0.331 0.866 0.205 \n3000 0.284 0.867 0.169 \n \nPrompt 3 \n1500 0.330 0.7 0.214 \n3000 0.288 0.833 0.178 \n \nPrompt 4 \n1500 0.331 0.822 0.203 \n3000 0.265 0.889 0.254 \n \nPrompt 5 \n1500 0.313 0.889 0.189 \n3000 0.274 0.922 0.162 \n \n \n \n \n \nGPT-4 \n \nPrompt 0 \n1500 0.823 0.722 0.957 \n3000 0.853 0.867 0.839 \n \nPrompt 1 \n1500 0.457 0.322 0.819 \n3000 0.470 0.322 0.874 \n \nPrompt 2 \n1500 0.803 0.789 0.818 \n3000 0.933 0.933 0.934 \n \nPrompt 3 \n1500       0.816 0.689 1.000 \n3000 0.839 0.722 1.000 \n \nPrompt 4 \n1500 0.830 0.711 1.000 \n3000 0.941 0.889 1.000 \n \nPrompt 5 \n1500 0.774 0.667 0.923 \n3000 0.815 0.733 0.917 \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \n \n \nAmong the six prompts, for both the token size prompt 4 yielded better results with an F1 score of 0.941 followed \nby prompt 2 with an F1 score of 0.933 in GPT -4. It is clear that prompts with structured instructions generally \nperform better, moreover simple prompts written in clear and concise way without any complexity also gives \ncomparable performance. Notably, smaller tokens tend to result in better performance than longer tokens in GPT-\n3.5 Turbo. We observed that GPT-4 handles longer input sizes (3000 tokens) effectively, often outperforming its \nhandling of shorter input sizes (1500 tokens). This is particularly eviden t with prompt 4, where GPT-4 achieved \nan F1 Score of 0.941 and precision of 1.000 at an input size of 3000, suggesting a remarkable ability to maintain \nhigh precision with increased input length. \n \nFigure 1 showed an example of the discharge summaries of a metastatic cancer patient along with the reasoning \nfrom GPT. GPT -4 correctly identified metastatic cancer based on the presence of key factors in the discharge \nsummaries. \n \nFigure 1: Example of Discharge Summaries for a Patient with Metastatic Cancer.  \n \nNext, we used a single label/example for the one -shot approach. The prompt accompanying this label/example \nwas similar to the prompt 4 in the zero-shot approach, as it yielded better results. An input size of 1500 tokens for \neach combination, resulted in an F1 score of 0.228 for GPT-3.5 Turbo and 0.824 for GPT-4. \n \nLlama Fine-Tuning \nNext, we explored the fine -tuning in Llama -7B. The performance metrics for the Llama fine -tuning models, as \noutlined in Table 3, indicate a range of effectiveness across different prompts. Specifically, prompts 1, 3 and 4 \nshow the highest F1 scores at 0.600, accompanied by a recall of 0.500 and the highest precision of 0.750. The \nworst performance among the Llama fine-tuning models is for prompt 0, which has the lowest F1 score of 0.400 \nand recall of 0.286. However, its precision is relatively higher at 0.6 67. This indicates that while prompt 0 is \nprecise when it does identify relevant cases, it misses a significant number of relevant instances. All these results \nsuggest that while some fine -tuning of Llama models yields promising results, particularly in precision, there is \nroom for improvement, especially when compared to the in-context zero-shot learning capabilities of models like \nGPT-4 as well as the fine -tuned PubMedBERT. This points towards a potential avenue for enhancing the fine -\ntuning strategies for LLMs, especially considering the accessibility and cost-effectiveness of open-source models \nsuch as Llama for research purposes. \n \nTable 3: Performance Metrics for Llama-7B Fine-tuning Models \nModel Prompt ID F1 Score Recall Precision \n  \n   \nLlama-7B \n  \n  \nPrompt 0 0.400 0.286 0.667 \nPrompt 1 0.600 0.500 0.750 \nPrompt 2 0.400 0.333 0.500 \nPrompt 3 0.600 0.500 0.750 \nPrompt 4 0.600 0.500 0.750 \nPrompt 5 0.500 0.429 0.600 \n \n \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \n \n \nImpact of Key Parameters on GPT Performance \nIn our analysis of GPT -4's reasoning abilities under various conditions, we utilized prompt 4 across a set of 100 \ntest samples. We explored the impact of temperature settings, ranging from 0.1 to 1, on the performance of the \nmodel. Interestingly, we did not observe any variation in performance as we varied the temperature settings. This \nstability suggests that the reasoning task required for metastatic cancer identification is robust to such parameter \nadjustments. \n  \nThe significance of keywords is well -established in the context of both machine learning models and human \ndecision-making. To test the robustness of LLMs, we selectively removed key terms such as \"metasta\", \"cancer”, \n“advanced\" and \"metastatic\" from the input text. The term \"metasta\" covers variations like metastasize, metastasis, \nand metastatic. Remarkably, GPT -4's reasoning performance when guided by prompt 4 displayed remarkable \nresilience. We found that the removal of individual keywords from the input did not significantly affect the model's \nreasoning capabilities. \n  \nEncouraged by the observation, we further experimented by randomly removing a specific percentage of tokens \nfrom the input text, while maintaining the original sequence of tokens. Our experiments revealed that the model's \nperformance remained stable, even with the removal of up to 40% of the tokens. These findings are in alignment \nwith our earlier findings, where a reduction in token count s showed a better performance in the comparisons \nbetween inputs of differing token lengths ( Table 2 ). Such resilience to  information sparsity has added a \ncompelling dimension to our understanding of LLM robustness in real -world scenarios, where incomplete or \nsparse data is a common challenge.  \n \nFigure 2 illustrates the correlation between the degree of sparsity in the input text and the model's performance. \nFor each sparsity level tested, we replicated the experiment five times to ensure statistical reliability and added \nerror bars to the resulting averag e F1 scores, thereby providing a clearer representation of the variability in our \nresults. \n \n \nFigure 2. Correlation with Sparsity and Model Performance.  \n \nDiscussion \nThrough an in-depth study of one common task, we have demonstrated the enormous potential of applying LLMs \nin real clinical settings, owing to their remarkable performance and effectiveness. At the same time, we recognize \nthe need for human expert intervention in certain challenging cases. After reviewing several false -negative \ninstances identified by medical fellows, we suspec t that evidence of metastatic cancer is absent in the discharge \nsummaries. If such cases were removed from the test set, we expect that the overall performance of both LLMs \nand human annotators would improve. We did not observe hallucinations likely becaus e of the nature of this \nreasoning task. \n  \nWe found that a well -crafted prompt in zero -shot learning can yield performance that is comparable to, or even \nbetter than, that achieved through one-shot learning and fine-tuning strategies in this task. The addition of “ Let’s \nthink step by step” couldn’t compete in comparison to simple and clear prompts. The performance of the models \n(GPT-3.5 Turbo, GPT -4 and Llama -7b) correlates with the number of parameters in the pre -trained models. \nAlthough we did not investigate Llama-2, which has 65 billion parameters, we anticipate that significant effort is \nneeded to optimize the fine -tune process. The Google BARD 29 model was also tested using internal data, but \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \n \n \nprovided no comparable results, so we proceeded without further investigation. Remarkably, the PubMedBERT \nmodel did not maintain its superior performance over the GPT -4 model. This observation suggests that the \npowerful abilities of GPT -4, when effectively harnessed through advanced prompt engineering strategies, can \noutperform specialized models that have undergone extensive domain-specific fine-tuning. This is potentially due \nto their large scale, diverse training data, and advanced arc hitectures. This highlights the potential of leveraging \nthe broad foundational knowledge encoded in LLMs to achieve remarkable accuracy in highly specialized fields \nlike biomedicine, without the necessity for model specialization. \n \nWe observed that using the first 1,500 tokens yielded better results in most cases than using the first 3,000 tokens \nin the GPT-3.5Turbo model. This is likely because the essential information for diagnosis, such as patient medical \nhistory, is generally found at the beginning of the note, while the latter part often contains irrelevant information \nlike medication history. This suggests that segmenting relevant sections of the notes or using the summarized text \nfrom the models like BART could not only reduce  costs but also enhance performance. Additionally, we found \nthat removing a significant number of tokens or keywords related to metastatic cancer did not adversely affect \nperformance. This finding is particularly important given that our input text is ofte n incomplete. It also implies \nthat we could achieve comparable performance using a smaller subset of tokens. GPT -4 demonstrates superior \nperformance in processing longer input sequences, maintaining high precision and recall, as evidenced by its \noptimal handling of 3000-token inputs across various prompts. Lastly, we noted that by instructing the model to \nprovide concise answers either “Yes” or “No” at the end of the prompt can avoid lengthy responses thus \nfacilitating downstream processing. Overall, this s tudy provides practical guidelines for the use of LMM in the \nbiomedical field. \n \nFunding \nThe research is supported by the MSU-Corewell Health Alliance fund and the NIH R01GM145700, \nR01GM134307, and 1RF1AG072449. The content is solely the responsibility of the authors and does not \nnecessarily represent the official views of the funders. \n \nCompeting interests  \nThe authors declare no competing interests.  \n \nAuthor contributions \nB.C. conceived and supervised the study, conducted the robustness test, and wrote the manuscript. X.Z. \ncoordinated the research, and the manual annotations. X.Z. and N.T. implemented the GPT models. S.V. prepared \ndatasets. S.A. and J.W. implemented fine -tuning in Llama, H.M. conducted ex plorations on Google Bard, S.M. \nimplemented PubMedBERT, D.L., A.D., and D.J. participated in manual annotation. M.W. helped prepare \ndatasets. D.C. and J.Z. provided resources. All authors participated in manuscript preparati on.  \n  \nReferences \n \n1. Zhao WX, Zhou K, Li J, Tang T, Wang X, Hou Y, et al. A survey of large language models [Internet]. \narXiv; 2023 [cited 2023 Sep 10]. Available from: http://arxiv.org/abs/2303.18223 \n2. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large language models in \nmedicine. Nat Med. 2023 Aug;29(8):1930–40.  \n3. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need \n[Internet]. arXiv; 2017 [cited 2023 Feb 25]. Available from:http://arxiv.org/abs/1706.03762 \n4. Kaplan J, McCandlish S, Henighan T, Brown TB, Chess B, Child R, et al. Scaling laws for neural language \nmodels [Internet]. arXiv; 2020 [cited 2023 Feb 21].Available from: http://arxiv.org/abs/2001.08361  \n5. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language models are few -shot \nlearners [Internet]. arXiv; 2020 [cited 2023 Sep 10]. Available from: http://arxiv.org/abs/2005.14165  \n6. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al. LLaMA: open and efficient \nfoundation language models [Internet]. arXiv; 2023 [cited 2023 Sep 10]. Available from: \nhttp://arxiv.org/abs/2302.13971 \n7. Anil R, Dai AM, Firat O, Johnson M, Lepikhin D, Passos A, et al. PaLM 2 Technical report [Internet]. \narXiv; 2023 [cited 2023 Sep 10]. Available from: http://arxiv.org/abs/2305.10403 \n8. Yang X, Chen A, PourNejatian N, Shin HC, Smith KE, Parisien C, et al. GatorTron: a large clinical \nlanguage model to unlock patient information from unstructured electronic health records [Internet]. arXiv; \n2022 [cited 2023 Sep 10].Available from: http://arxiv.org/abs/2203.03540 \n9. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical \nknowledge [Internet]. arXiv; 2022 [cited 2023 Sep 10]. Available from: http://arxiv.org/abs/2212.13138  \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint \n \n \n10. Chen Q, Du J, Hu Y, Keloth VK, Peng X, Raja K, et al. Large language models in biomedical natural \nlanguage processing: benchmarks, baselines, and recommendations [Internet]. arXiv; 2023 [cited 2023 Sep \n10]. Available from: http://arxiv.org/abs/2305.16326 \n11. Liu K, Kulkarni O, Witteveen-Lane M, Chen B, Chesla D. MetBERT: a generalizable and pre-trained deep \nlearning model for the prediction of metastatic cancer from clinical notes. AMIA Annu Symp Proc. \n2022;2022:331–8.  \n12. Devlin J, Chang MW, Lee K, Toutanova K. BERT: Pre-training of deep bidirectional transformers for \nlanguage understanding [Internet]. arXiv; 2019 [cited 2023 Sep 10]. Available from: \nhttp://arxiv.org/abs/1810.04805 \n13. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language \nrepresentation model for biomedical text mining. Bioinformatics. 2020 Feb 15;36(4):1234 –40.  \n14. Huang K, Altosaar J, Ranganath R. ClinicalBERT: modeling clinical notes and predicting hospital \nreadmission [Internet]. arXiv; 2020 [cited 2023 Sep 10]. Available from: http://arxiv.org/abs/1904.05342  \n15. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain-specific language model pretraining \nfor biomedical natural language processing. ACM Trans Comput Healthcare. 2022 Jan 31;3(1):1 –23.  \n16. Kojima T, Gu SS, Reid M, et al. Large language models are zero-shot reasoners. Advances in neural \ninformation processing systems. 2022 Dec 6;35:22199-213 \n17. Meskó B, Prompt engineering as an important emerging skill for medical professionals: tutorial, J Med \nInternet Res 2023;25:e50638 URL: https://www.jmir.org/2023/1/e50638 DOI: 10.2196/50638 \n18. Heston, Thomas F., and Charya Khun. 2023. \"Prompt engineering in medical education\" International \nMedical Education 2, no. 3: 198-205. https://doi.org/10.3390/ime2030019 \n19. Sabit Ekin. Prompt engineering for chatGPT: a quick guide to techniques, tips, and best practices. \nTechRxiv. May 04, 2023. \n20. Lu Y, Liu X, Du Z, et al. MedKPL: a heterogeneous knowledge enhanced prompt learning framework for \ntransferable diagnosis. Journal of Biomedical Informatics. 2023 Jun 12:104417.  \n21. Taylor N, Zhang Y, Joyce DW, et al. Clinical prompt learning with frozen language models. IEEE \nTransactions on Neural Networks and Learning Systems. 2023 Aug 11. \n22. Sivarajkumar S, Wang Y. Healthprompt: A zero-shot learning paradigm for clinical natural language \nprocessing. In AMIA Annual Symposium Proceedings 2022 (Vol. 2022, p. 972). American Medical \nInformatics Association \n23. Gehrmann S, Dernoncourt F, Li Y, et al. Comparing deep learning and concept extraction based methods \nfor patient phenotyping from clinical narratives. PLoS One. 2018;13(2):e0192360. Published 2018 Feb 15. \ndoi:10.1371/journal.pone.0192360 \n24. Johnson AEW, Pollard TJ, Shen L, Lehman L wei H, Feng M, Ghassemi M, et al. MIMIC -III, a freely \naccessible critical care database. Sci Data. 2016 May 24;3:160035.  \n25. Kojima T, Gu SS, Reid M, Matsuo Y, Iwasawa Y. Large language models are zero -shot reasoners \n[Internet]. arXiv; 2023 [cited 2023 Sep 13]. Available from: http://arxiv.org/abs/2205.11916  \n26. Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, et al. Parameter-efficient fine-tuning of large-scale pre-\ntrained language models. Nat Mach Intell. 2023 Mar;5(3):220–35.  \n27. Banner R, Nahshan Y, Hoffer E, Soudry D. arXiv.org. 2018 [cited 2023 Sep 14]. Post -training 4-bit \nquantization of convolution networks for rapid-deployment. Available from: \nhttps://arxiv.org/abs/1810.05723v3 \n28. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li Y, Wang S, et al. arXiv.org. 2021 [cited 2023 Sep 14]. LoRA: \nLow-Rank adaptation of large language models. Available from: https://arxiv.org/abs/2106.09685v2 \n29. Park M. Google  bard API [Internet]. 2023 [cited 2023 Sep 14]. Available from: \nhttps://github.com/dsdanielpark/Bard-API \n  \n \n \n \n \n \n \n \n . CC-BY 4.0 International licenseIt is made available under a \nperpetuity. \n is the author/funder, who has granted medRxiv a license to display the preprint in(which was not certified by peer review)preprint \nThe copyright holder for thisthis version posted February 8, 2024. ; https://doi.org/10.1101/2024.02.07.24302444doi: medRxiv preprint ",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.42857202887535095
    },
    {
      "name": "Computer science",
      "score": 0.39917492866516113
    },
    {
      "name": "Natural language processing",
      "score": 0.37415382266044617
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33826708793640137
    },
    {
      "name": "Mathematics education",
      "score": 0.334603875875473
    },
    {
      "name": "Psychology",
      "score": 0.31747710704803467
    },
    {
      "name": "Philosophy",
      "score": 0.14722487330436707
    }
  ]
}