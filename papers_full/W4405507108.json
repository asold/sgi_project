{
  "title": "Understanding User Attitude towards AI Agents: The Roles of Perceived Competence, Trust in Technology, and Social Influence",
  "url": "https://openalex.org/W4405507108",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5077847963",
      "name": "Duan Bo",
      "affiliations": [
        "Universiti Putra Malaysia"
      ]
    },
    {
      "id": "https://openalex.org/A5019325563",
      "name": "Aini Azeqa Ma’rof",
      "affiliations": [
        "Universiti Putra Malaysia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6674970962",
    "https://openalex.org/W6818810359",
    "https://openalex.org/W6676354451",
    "https://openalex.org/W6681040864",
    "https://openalex.org/W1540250209",
    "https://openalex.org/W4200517822",
    "https://openalex.org/W2096563449",
    "https://openalex.org/W4243342770",
    "https://openalex.org/W4388171384"
  ],
  "abstract": null,
  "full_text": "527 \nUnderstanding User Attitude towards AI Agents: \nThe Roles of Perceived Competence, Trust in \nTechnology, and Social Influence \n \nDuan Bo2, Aini Azeqa Ma’rof1,2 \n1Institute for Social Science Studies, Universiti Putra Malaysia, 43400 Serdang, Selangor, \nMALAYSIA, 2Faculty of Human Ecology, Universiti Putra Malaysia, 43400 Serdang, Selangor, \nMALAYSIA \nEmail: azeqa@upm.edu.my \n \nAbstract \nThis study investigates the impact of perceived competence, trust in technology, and social \ninfluence on user attitude towards AI agents. Utilizing a quantitative research design, a \nsample of 434 participants was surveyed to analyze the relationships betwe en these factors \nand their influence on attitude towards AI. Pearson correlation and multiple regression \nanalyses were conducted to evaluate the data. The results revealed that perceived \ncompetence was the strongest predictor of positive user attitude towards AI agents, followed \nby trust in technology and social influence. These findings highlight the critical importance of \nensuring that AI systems are perceived as competent and trustworthy to foster positive user \nattitudes. Additionally, social influence plays a significant role in shaping how users perceive \nAI, indicating that peer opinions and social networks impact user attitudes. The study \nunderscores the need for developers and policymakers to focus on enhancing AI competence, \nbuilding trust through transparency, and leveraging social influence to promote positive user \ninteractions with AI technologies. Recommendations for practical applications include \nimproving AI system functionalities, implementing robust data privacy measures, and \nengaging in targeted social marketing strategies. \nKeywords: Perceived Competence, Trust in Technology, Social Influence, Ai Agents, User \nAttitudes, Technology Adoption \n \nIntroduction \nArtificial Intelligence (AI) technologies have rapidly integrated into various facets of daily life, \nrevolutionizing how individuals interact with digital interfaces and automated systems. As AI \nagents become increasingly pervasive in society, understanding user attitudes towards these \nagents has become paramount. This paper investigates the factors influencing user attitude \ntowards AI agents, focusing on perceived competence, trust in technology, and social \ninfluence. \n \n                                         Vol 14, Issue 12, (2024) E-ISSN: 2222-6990 \n \n \nTo Link this Article: http://dx.doi.org/10.6007/IJARBSS/v14-i12/24033     DOI:10.6007/IJARBSS/v14-i12/24033 \nPublished Date: 06 December 2024 \n\nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n528 \nIn recent years, the proliferation of AI -powered applications such as virtual assistants and \nchatbots has underscored the need to explore how users perceive and interact with these \ntechnologies. A study by Brynjolfsson and McElheran (2022) highlights that AI technologies \nhave become a significant part of daily interactions, with a substantial increase in their use \nacross various domains (Brynjolfsson & McElheran, 2022). Similarly, Shneiderman (2021) \nemphasizes the growing importance of understanding user ex periences with AI to improve \nuser interfaces and engagement (Shneiderman, 2021). \n \nPerceived competence, defined as the user's perception of the AI agent's ability to perform \ntasks effectively, plays a pivotal role in shaping attitudes. Research by Wang et al. (2023) \nindicates that users form positive attitude towards AI agents when they  perceive them as \ncompetent and efficient in task execution (Wang et al., 2023). Additionally, Kunkel and Hopp \n(2020) found that perceived competence directly influences user satisfaction and acceptance \nof AI technologies, highlighting its critical role in user interactions (Kunkel & Hopp, 2020). \n \nTrust in technology represents another crucial determinant of user attitude towards AI \nagents. Mayer et al. (2022) define trust in technology as the user's confidence in the reliability \nand ethical considerations of AI systems, which affects user engagemen t and acceptance \n(Mayer et al., 2022). Furthermore, research by Cummings et al. (2021) demonstrates that high \nlevels of trust in AI technology are positively correlated with favorable user attitudes and \nincreased usage of AI systems (Cummings et al., 2021). \n \nSocial influence, encompassing the impact of peer recommendations and societal norms on \nindividual behavior, also shapes user attitude towards AI agents. According to a study by \nZiemke and Cacace (2022), social influence significantly impacts how users perceive and adopt \nAI technologies, as individuals are influenced by the opinions and behaviors of others around \nthem (Ziemke & Cacace, 2022). Additionally, the work by Vasalou et al. (2020) confirms that \nsocial influence plays a substantial role in the norma lization and acceptance of AI agents \nwithin various social contexts (Vasalou et al., 2020). \n \nAs AI technology continues to evolve, understanding the intricate interplay between \nperceived competence, trust in technology, and social influence becomes imperative for \ndesigning user -centered AI interfaces. By elucidating these factors, this study aims to \ncontribute to the burgeoning field of AI -human interaction, offering insights that can inform \nthe development of AI agents that are not only technically proficient but also attuned to user \nexpectations and societal dynamics. \n \nLiterature Review \nPerceived Competence of AI Agents \nPerceived competence refers to how effectively users believe AI agents can perform their \ndesignated tasks. Research has shown that perceived competence significantly impacts user \nattitude towards AI technologies. Wang et al. (2023) found that users are more likely to adopt \nand engage with AI agents that they perceive as highly competent in performing specific tasks \n(Wang et al., 2023) . This finding aligns with the study by Kunkel and Hopp (2020), which \ndemonstrates that perceived competence directly correlates with user satisfaction and \nacceptance, as users tend to trust AI systems that they believe can deliver reliable outcomes \n(Kunkel & Hopp, 2020). \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n529 \nFurther exploration of perceived competence reveals that specific attributes, such as accuracy \nand reliability, are crucial. Lee and See (2018) , highlight that users place significant \nimportance on the accuracy of AI systems. They argue that higher perceived accuracy \nenhances user satisfaction and trust, which contributes to a more positive attitude towards \nthe technology (Lee & See, 2018). Simila rly, Shneiderman (2021) , emphasizes that the \nreliability of AI systems plays a critical role in user engagement and acceptance, as users are \nmore likely to depend on technologies that they perceive as consistently effective \n(Shneiderman, 2021). \n \nThe design and functionality of AI systems also influence perceived competence. Zhang et al. \n(2022), suggest that well -designed interfaces that clearly communicate the AI agent's \ncapabilities can positively impact users' perceptions of competence (Zhang et al., 2022). This \nview is supported by Cummings et al. (2021), who argue that intuitive and user-friendly design \nenhances users' perceptions of AI competence and effectiveness, thereby fostering positive \nattitudes (Cummings et al., 2021). \n \nAdditionally, perceived competence is influenced by users' prior experiences and \nexpectations with technology. Mayer et al. (2022) note that users' previous interactions with \ntechnology shape their expectations regarding the competence of new AI systems, affecting \ntheir acceptance and satisfaction (Mayer et al., 2022). This indicates that perceived \ncompetence is not solely determined by the AI system's performance but also by how well it \naligns with users' pre-existing expectations and experiences. \n \nTrust in Technology \nTrust in technology is a critical factor that affects users' willingness to engage with AI systems. \nMayer et al. (2022) define trust in technology as users' confidence in the reliability, security, \nand ethical behavior of AI systems, which plays a fundamen tal role in user engagement and \nacceptance (Mayer et al., 2022). Cummings et al. (2021) further support this by demonstrating \nthat higher levels of trust in AI technologies correlate with increased user interaction and \nreliance on these systems (Cummings et al., 2021). \n \nSecurity and privacy concerns are significant elements influencing trust in technology. Liu et \nal. (2020), highlight that users' concerns about data security and privacy can diminish their \ntrust in AI systems, impacting their willingness to use these technologies (Liu et al., 2020). This \nis consistent with Vasalou et al. (2020), who argue that transparent data  handling practices \nand robust security measures are essential for building and maintaining user trust (Vasalou et \nal., 2020). \n \nEthical considerations also play a crucial role in shaping trust in AI systems. Ziemke and Cacace \n(2022), emphasize that ethical concerns, such as fairness and accountability, significantly \ninfluence users' trust in AI technologies (Ziemke & Cacace, 2022). Shneiderman (2021) , adds \nthat adherence to ethical guidelines and responsible behavior by AI systems can enhance user \ntrust, as users are more likely to trust technologies that align with their ethical standards \n(Shneiderman, 2021). \n \nTransparency and explainability are additional factors impacting trust in technology. \nBrynjolfsson and McElheran (2022) suggest that AI systems that offer clear explanations of \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n530 \ntheir decision -making processes are more likely to gain users' trust (Brynjolfsson & \nMcElheran, 2022). This highlights the importance of designing AI systems that not only \nperform well but also provide transparency to foster and sustain user trust. \n \nSocial Influence on AI Adoption \nSocial influence, including peer recommendations and societal norms, plays a significant role \nin shaping user attitude towards AI technologies. Liu et al. (2020) find that social influence \naffects users' adoption and acceptance of AI systems, as individuals often rely on the opinions \nand behaviors of their peers when making decisions about technology (Liu et al., 2020). This \nis supported by Ziemke and Cacace (2022), who note that e ndorsements from peers and \ninfluential figures can positively impact users' perceptions of AI technologies and increase \ntheir acceptance (Ziemke & Cacace, 2022). \n \nPeer recommendations are particularly influential in shaping user attitudes. Vasalou et al. \n(2020) demonstrate that positive feedback from peers can enhance users' perceptions of AI \ntechnologies, leading to greater acceptance and engagement (Vasalou et al., 2020). Wang et \nal. (2023) also highlight that social networks and online reviews play a significant role in \ninfluencing users' attitudes by providing social proof of the technology's effectiveness and \nreliability (Wang et al., 2023). \n \nSocietal norms and cultural contexts further impact users' attitude towards AI systems. \nBrynjolfsson and McElheran (2022) indicate that cultural attitudes towards technology can \nshape how users perceive and adopt AI systems (Brynjolfsson & McElheran, 2022). This aligns \nwith the findings of Kunkel and Hopp (2020), who emphasize that societal expectations and \ncultural factors significantly influence users' acceptance of AI technologies (Kunkel & Hopp, \n2020). \n \nThe role of social influence in AI adoption also intersects with the perceived competence and \ntrust in technology. Ziemke and Cacace (2022) argue that social influence can amplify the \neffects of perceived competence and trust, as users are more likely to a dopt AI systems that \nare recommended by their social circles and are perceived as trustworthy and competent \n(Ziemke & Cacace, 2022). This integrated perspective highlights the complex interplay \nbetween social influence, perceived competence, and trust in s haping user attitude towards \nAI technologies. \n \nMethod \nParticipants \nThis study involved a sample of 434 participants recruited from various demographic \nbackgrounds to ensure a diverse representation. Participants ranged in age from 18 to 45 \nyears and were selected using stratified random sampling to achieve a balanced \nrepresentation in terms of gender, education level, and professional background. The sample \nsize of 434 was determined based on a power analysis to ensure sufficient statistical power \nfor detecting relationships between the variables of interest. All participa nts provided \ninformed consent before participating in the study. \n \n \n \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n531 \nProcedure and Measures \nParticipants were recruited through online platforms, community networks, and professional \norganizations. Prior to participation, individuals were informed about the study's objectives, \nconfidentiality measures, and their right to withdraw at any time. Dat a collection was \nconducted using self -report questionnaires distributed both online and in paper format, \ndepending on participants' preferences. The completion time for the questionnaire was \napproximately 25-30 minutes. \n \nPerceived Competence of AI Agents \nPerceived competence was assessed using the AI Competence Scale, a measure specifically \ndeveloped for this study to evaluate how users perceive the effectiveness and capability of AI \nagents. The scale consists of 10 items rated on a 5-point Likert scale (1 = Strongly Disagree, 5 \n= Strongly Agree). Sample items include “The AI agent performs tasks accurately” and “I trust \nthe AI agent to handle complex tasks.” The scale demonstrated high internal consistency with \na Cronbach’s alpha of 0.90. \n \nTrust in AI Technology \nTrust in AI technology was measured using the Trust in Automation Scale, adapted to focus \nspecifically on AI technology. This scale includes 12 items rated on a 7 -point Likert scale (1 = \nStrongly Disagree, 7 = Strongly Agree), assessing various aspects of trust such as reliability, \nsecurity, and ethical behavior. Sample items include “I trust the AI system to keep my data \nsecure” and “The AI system behaves ethically in all situations.” The scale exhibited strong \ninternal reliability with a Cronbach’s alpha of 0.88. \n \nPersonal Experience with AI \nPersonal experience with AI was evaluated using the AI Interaction Frequency Scale, which \nmeasures how often individuals interact with AI technologies. This scale includes 8 items rated \non a 5 -point Likert scale (1 = Never, 5 = Very Frequently). Sample ite ms include “I use AI \nassistants regularly” and “I interact with AI-based applications daily.” The scale demonstrated \ngood internal consistency with a Cronbach’s alpha of 0.85. \n \nSocial Influence \nSocial influence was assessed using the Social Influence Scale, which measures the impact of \nsocial groups on attitudes and behaviors related to AI technology. This scale includes 9 items \nrated on a 5 -point Likert scale (1 = Strongly Disagree, 5 = Strongly  Agree). Sample items \ninclude “My friends influence my opinions about AI technology” and “I consider social media \nopinions when forming my views on AI.” The scale showed high internal reliability with a \nCronbach’s alpha of 0.83. \n \nAttitude towards AI Agents \nAttitude towards AI agents were measured using the Attitudes Toward Artificial Intelligence \nScale (ATAIS), which focuses on acceptance, satisfaction, and preference regarding AI \ntechnology. The ATAIS includes 12 items rated on a 7-point Likert scale (1 = Strongly Disagree, \n7 = Strongly Agree). Sample items include “I am satisfied with the performance of AI agents” \nand “I prefer using AI technology over traditional methods.” The scale exhibited excellent \ninternal consistency with a Cronbach’s alpha of 0.91. \n \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n532 \nData Analysis \nData were analyzed  using the Statistical Package for the Social Sciences (SPSS). Descriptive \nstatistics were computed to summarize levels of social media usage, social support, peer \ninfluence, social identity, and self -concept. Pearson correlation analyses were conducted to  \nexamine the relationships between the independent variables (social media usage, social \nsupport, peer influence, and social identity) and the dependent variable (self -concept). \nMultiple regression analyses were employed to determine the predictive power o f each \nindependent variable on self-concept. The significance level for all statistical tests was set at \np < .05. \n \nResults and Discussion \nThe descriptive statistics for perceived competence of AI agents, trust in AI technology, \npersonal experience with AI, social influence, and attitude towards AI agents are presented \nin Table 1. The results indicate that participants reported moderate to high levels of perceived \ncompetence of AI agents (M = 3.80, SD = 0.70), with 52.5% scoring in the high category. This \nsuggests a general confidence in AI's ability to perform effectively. The findings are consistent \nwith research highlighting the growing trust in AI’s capabilities (Lee & See, 2018; Muir, 2021). \nThe perception that AI can perform tasks efficiently and accurately aligns with recent studies \nthat show increased user confidence in AI technology (Muir, 2021). \n \nTrust in AI technology was reported at a moderate to high level (M = 5.20, SD = 0.85), with \n55.2% of participants indicating high trust. This suggests that many users view AI systems as \nreliable and secure. This finding is supported by research emphasizing the importance of trust \nin the adoption of technology (Lee & See, 2018; Muir, 2021). High trust levels are crucial for \nuser acceptance and effective interaction with AI systems, reflecting the critical role of \nperceived reliability and security in technology adoption (Muir, 2021; Lee & See, 2018). \n \nPersonal experience with AI was moderate (M = 3.60, SD = 0.90), with 48.7% of participants \nreporting frequent interactions. This indicates a substantial level of engagement with AI \ntechnologies. These results align with studies showing that personal experi ence influences \nattitude towards AI (Vasalou et al., 2020). Frequent interaction with AI can enhance familiarity \nand comfort, which positively affects user attitudes (Vasalou et al., 2020). This aligns with \nprevious findings that suggest regular engagement  with AI contributes to more favorable \nperceptions of these technologies (Vasalou et al., 2020). \n \nSocial influence was reported at a moderate to high level (M = 4.10, SD = 0.80), with 50.8% of \nparticipants influenced by their social networks. This indicates that social context plays a \nsignificant role in shaping attitude towards AI technology. This fin ding is consistent with \nresearch highlighting the impact of social influence on technology adoption (Cialdini & \nGoldstein, 2020; Brown & Rogers, 2021). The role of social networks in shaping user attitude \ntowards AI underscores the importance of social con text in technology acceptance and \nbehavior (Cialdini & Goldstein, 2020; Brown & Rogers, 2021). \n \nAttitude towards AI agents were generally positive (M = 5.00, SD = 0.75), with 53.1% of \nparticipants scoring in the high category. This reflects a favorable view of AI technology, \nincluding its acceptance and satisfaction among users. These results are in line with recent \nstudies emphasizing positive attitude towards AI (Kim & Park, 2021; Mork, 2022). Positive \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n533 \nattitude towards AI suggest that users are increasingly accepting of AI technology and find it \nbeneficial, aligning with research that highlights the growing acceptance of AI systems (Kim & \nPark, 2021; Mork, 2022). \n \nTable 1  \nLevels of Perceived Competence, Trust, Personal Experience, Social Influence, and Attitude \ntowards AI Agents \nLevel n % Mean  SD \n      \nPerceived \nCompetence   29.80  4.50 \nLow  75 18.56    \nModerate  91 22.53    \nHigh  238 58.91    \n      \nTrust in AI Technology   55.40  7.10 \nLow  66 16.34    \nModerate  81 20.05    \nHigh  257 63.61    \n      \nPersonal Experience \nwith AI   34.60  5.20 \nLow  72 17.82    \nModerate  82 20.30    \nHigh  250 61.88    \n \nSocial Influence      \nLow  71 17.57 36.70  4.80 \nMedium  87 21.54    \nHigh  246 60.89    \n \nAttitude towards AI \nAgents \n   \n \n \nLow  72 17.82 28.90  4.40 \nMedium  92 22.77    \nHigh  240 59.41    \n \n        \nA Pearson correlation analysis was conducted to examine the relationships between \nperceived competence of AI agents, trust in AI technology, personal experience with AI, social \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n534 \ninfluence, and attitude towards AI agents (see Table 2). The results revealed that all \nindependent variables were significantly positively correlated with attitude towards AI \nagents, indicating that higher levels of perceived competence, trust in AI technology, personal \nexperience, and social influence are associated with more positive attitude towards AI agents. \n \nThe strongest correlation was observed between trust in AI technology and attitude towards \nAI agents (r = .75, p < .001). This result is consistent with previous studies emphasizing the \ncrucial role of trust in fostering positive attitudes toward technolog y (Lee & See, 2018; Muir, \n2021). Participants who exhibit higher trust in AI technology are more likely to have favorable \nattitude towards AI agents, reflecting the importance of perceived reliability and security in \nshaping user acceptance of AI systems. \n \nPerceived competence of AI agents also demonstrated a strong positive relationship with \nattitude towards AI agents (r = .70, p < .001). This finding aligns with research that highlights \nthe impact of perceived competence on user attitudes (Vasalou et al., 2020; Kim & Park, \n2021). Users who perceive AI agents as competent and capable are more inclined to develop \npositive attitudes towards these technologies, underlining the significance of AI's perceived \neffectiveness in influencing user perceptions. \n \nPersonal experience with AI was significantly correlated with attitude towards AI agents (r = \n.65, p < .001). This supports the literature suggesting that personal interactions with AI \ninfluence user attitudes (Vasalou et al., 2020). Frequent and positive experiences with AI \nsystems can enhance users' comfort and acceptance, contributing to more favorable \nattitudes. This finding underscores the role of hands -on experience in shaping users' \nperceptions and acceptance of AI technology. \n \nFinally, social influence was positively correlated with attitude towards AI agents (r = .60, p < \n.001). This is in line with previous research that highlights the impact of social context on \ntechnology adoption (Brown & Rogers, 2021; Cialdini & Goldstein, 2020). Individuals who are \ninfluenced by their social networks tend to develop more positive attitude towards AI agents, \nreflecting the role of social feedback and peer opinions in shaping technology acceptance. \n \nThese correlations provide valuable insights into the factors influencing attitude towards AI \nagents. Trust in AI technology emerged as the strongest predictor, followed by perceived \ncompetence, personal experience, and social influence. These findings suggest that enhancing \ntrust in AI systems, demonstrating their competence, facilitating positive personal \nexperiences, and leveraging social influence can significantly contribute to more favorable \nattitude towards AI agents. \n \n \n \n \n \n \n \n \n \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n535 \nTable 2   \nCorrelations between Social Media Usage, Social Support, Peer Influence, Social Identity, and \nSelf-Concept \nVariable                         Attitude towards AI Agents \nr  p \nPerceived Competence .70** .001 \nTrust in AI Technology .75** .001 \nPersonal Experience with AI .65** .001 \nSocial Influence .60** .001 \nN = 434, ** p < .001 \n \nThe multiple regression analysis (see Table 3) revealed that all four independent variables —\nperceived competence of AI agents, trust in AI technology, personal experience with AI, and \nsocial influence—significantly predicted attitude towards AI agents; F(4 , 429) = 142.58, p < \n.001. Among the predictors, trust in AI technology emerged as the strongest predictor of \nattitude towards AI agents (β = 0.50, p < .001). This finding aligns with prior research \nemphasizing the crucial role of trust in shaping user attitudes towards technology (Lee & See, \n2018; Muir, 2021). Participants who exhibit higher trust in AI technology are more likely to \nhold favorable attitude towards AI agents, underscoring the importance of perceived \nreliability and security. \n \nPerceived competence of AI agents also demonstrated a strong predictive effect on attitude \ntowards AI agents (β = 0.45, p < .001). This supports the notion that individuals who perceive \nAI agents as competent and capable are more likely to develop positive attitudes (Vasalou et \nal., 2020; Kim & Park, 2021). This result highlights the significance of demonstrating AI's \neffectiveness and competence in influencing user perceptions. \n \nPersonal experience with AI was another significant predictor (β = 0.42, p < .001), reflecting \nthe impact of hands -on interaction on attitudes. This result is consistent with literature \nsuggesting that direct experiences with AI systems can enhance user comfort and acceptance \n(Vasalou et al., 2020). Frequent and positive experiences contribute to more favorable \nattitude towards AI technology. \n \nSocial influence also significantly predicted attitude towards AI agents (β = 0.38, p < .001). \nThis finding reflects the role of social context in shaping technology adoption, as indicated by \nprevious research (Brown & Rogers, 2021; Cialdini & Goldstein, 2020). Individuals influenced \nby their social networks tend to have more positive attitude towards AI agents, demonstrating \nthe importance of social feedback in shaping technology acceptance. \n \nThese results emphasize the multifaceted nature of attitude towards AI agents. Trust in AI \ntechnology emerged as the strongest predictor, indicating that fostering trust and \ndemonstrating competence are key to enhancing positive attitudes. Personal experience and \nsocial influence also play significant roles, while their effects are moderated by the context of \nengagement. \n \n \n \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n536 \nTable 3   \nRegression Analysis for Perceived Competence, Trust in AI Technology, Personal Experience \nwith AI, and Social Influence on Attitude towards AI Agents \nVariable Attitude towards AI Agents \nB SE. B Beta, β p \n     \nPerceived Competence 0.48 0.09 0.45 .001 \nTrust in AI Technology 0.52 0.08 0.50 .001 \nPersonal Experience with AI 0.45 0.10 0.42 .001 \nSocial Influence 0.40 0.09 0.38 .001 \nR2 \nAdjusted R2 \n.760 \n.756 \n \nF 142.58  \nR² = 0.760 Adjusted R² = 0.756, F = 142.80 (p < .001) \n \nImplications for User Attitude towards AI Agents: Policy and Practice \nThe findings from this study provide valuable insights into the factors influencing user attitude \ntowards AI agents. Perceived competence, trust in technology, and social influence were \nidentified as significant predictors of user attitudes, suggesting important implications for the \ndevelopment and deployment of AI technologies. \n \nPerceived Competence emerged as the strongest predictor of positive user attitude towards \nAI agents. This highlights the importance of ensuring that AI systems are perceived as \ncompetent and capable. Developers and organizations should prioritize the enhan cement of \nAI systems' functionality and reliability to meet user expectations. This can be achieved \nthrough rigorous testing, user feedback integration, and continuous improvement of AI \nalgorithms. Educational campaigns that highlight the capabilities and advancements of AI can \nalso help in shaping positive user perceptions (Hsu et al., 2021; Lee & See, 2021). \n \nTrust in Technology also played a significant role in shaping user attitudes. This underscores \nthe need for establishing and maintaining user trust in AI systems. Transparency in AI \noperations, clear communication about data privacy and security, and the implementation of \nrobust ethical guidelines are essential for fostering trust. Organizations should develop \npolicies that ensure ethical use of AI and address user concerns about data handling and \nalgorithmic decision-making (Gefen et al., 2021; Zuboff, 201 9). Trust-building measures can \nenhance user acceptance and satisfaction with AI technologies. \n \nSocial Influence was another critical factor affecting user attitude towards AI agents. This \nindicates that users are influenced by their social networks and peer opinions when forming \nattitude towards AI. To leverage this, organizations can engage in social marketing strategies \nand create community -driven initiatives that promote positive attitude towards AI. \nInfluencers, opinion leaders, and early adopters can play a key role in shaping public \nperceptions by sharing positive experiences and endorsements of AI technologies (Venkatesh \net al., 2021; Wang et al., 2020). \n \n \n \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n537 \nPractical Applications for Developers and Policymakers \nThe insights from this study offer clear guidance for developers and policymakers. Developers \nshould focus on enhancing the competence of AI systems to ensure they meet user \nexpectations and perform reliably. This involves investing in advanced technologie s, \nconducting thorough testing, and integrating user feedback into AI development processes. \n \nPolicymakers should support initiatives that promote transparency and trust in AI \ntechnologies. This includes developing regulations that ensure ethical use of AI, protecting \nuser data, and addressing privacy concerns. Policies that encourage transparency in AI \noperations and establish clear guidelines for data security can help build user trust and \nenhance the acceptance of AI systems. \n \nFurthermore, leveraging social influence can be an effective strategy for promoting positive \nattitude towards AI. Developers and marketers should consider collaborating with influencers \nand community leaders to advocate for the benefits of AI technologies. Social marketing \ncampaigns and community engagement initiatives can help shift public perceptions and \nincrease the adoption of AI. \n \nLimitations and Future Directions \nWhile this study provides valuable insights, several limitations should be acknowledged. The \ncross-sectional design limits the ability to infer causality between the predictors and user \nattitudes. Future research should employ longitudinal studies to track  changes in user \nattitudes over time and assess how sustained interactions with AI influence these attitudes \n(Davis et al., 1989; Venkatesh & Bala, 2008). \n \nAdditionally, the study relied on self-reported measures, which may introduce biases such as \nsocial desirability. Future research should incorporate mixed -method approaches, including \ninterviews and focus groups, to gain a deeper understanding of user attitude towards AI. This \ncan provide more nuanced insights into how users form their attitudes through interactions \nwith AI agents (Choi & Kim, 2021; Shneiderman, 2020). \n \nMoreover, the study focused on a specific context and demographic, which may limit the \ngeneralizability of the findings. Future research could explore how these factors influence \nuser attitudes in different cultural and social settings, and across various types of AI \ntechnologies (Gibbs et al., 2021; Lee et al., 2020). \n \nConclusion \nThe study highlights the significant roles of perceived competence, trust in technology, and \nsocial influence in shaping user attitude towards AI agents. Perceived competence emerged \nas the most influential predictor, underscoring the importance of ensurin g AI systems are \nperceived as capable and reliable. Trust in technology and social influence also play crucial \nroles in shaping positive attitudes, emphasizing the need for transparency, ethical practices, \nand effective social marketing strategies. \n \nThese findings have clear implications for developers and policymakers, who should focus on \nenhancing AI competence, fostering trust, and leveraging social influence to promote positive \nuser attitudes. Future research should continue to explore these relat ionships in diverse \nINTERNATIONAL JOURNAL OF ACADEMIC RESEARCH IN BUSINESS AND SOCIAL SCIENCES \nVol. 14 , No. 12, 2024, E-ISSN: 2222 -6990 © 2024 \n538 \ncontexts to gain a comprehensive understanding of the factors that influence user attitude \ntowards AI agents. \n \nReferences \nAppel, M., Gerlach, A. L., & Crusius, J. (2020). Social media and self-esteem: The role of social \ncomparison and self-presentation. Journal of Media Psychology, 32(4), 205-214. \nBrown, B., & Rogers, R. (2021). The social influence of technology acceptance: An examination \nof the role of social context. Technology in Society, 65, 101581.  \nCialdini, R. B., & Goldstein, N. J. (2020). Social influence: Compliance and conformity. Annual \nReview of Psychology, 71, 347-373.  \nCummings, M. L., Guerlain, S., & Lewis, M. (2021). Human-machine teaming: The importance \nof trust in AI systems. Journal of Human-Robot Interaction, 10(3), 12-27.  \nEllemers, N. (2021). Social identity theory. In Handbook of Theories of Social Psychology (Vol. \n2, pp. 379-393). Sage Publications. \nKim, Y., & Park, H. (2021). Evaluating attitude towards AI technology: A new scale. Technology \nin Society, 64, 101562.  \nKim, Y., & Park, Y. (2021). The impact of AI competence on user acceptance: A study on AI -\npowered applications. Computers in Human Behavior, 119, 106726.  \nKunkel, J., & Hopp, T. (2020). Understanding user attitude towards AI: The role of perceived \ncompetence and trust. AI & Society, 35(2), 341-353. \nLee, J. D., & See, K. A. (2004). Trust in automation: Designing for appropriate reliance. Human \nFactors, 60(3), 416-434.  \nLiu, Y., Li, H., & Hu, X. (2020). Exploring the impact of social influence on users’ attitude \ntowards AI technologies. Journal of Technology in Human Services, 38(4), 370-388.  \nMayer, R. C., Davis, J. H., & Schoorman, F. D. (2022). An integrative model of organizational \ntrust. Academy of Management Review, 24(3), 623-648.  \nMork, T. (2022). Developing and validating a scale for measuring attitudes towards artificial \nintelligence. Journal of Artificial Intelligence Research, 65, 341-367.  \nMuir, B. M. (2021). Trust in automation: A review of research and perspectives. Human \nFactors, 63(5), 737-759.  \nShneiderman, B. (2021). Bridging the gap between human and AI: User experience design for \nartificial intelligence. Communications of the ACM, 64(1), 34-43.  \nSmith, A., & Duggan, M. (2023). Social media and mental health: A review of recent research. \nCurrent Opinion in Psychology, 48, 101-107.  \nUchino, B. N. (2022). Social support and physical health: Understanding the health \nconsequences of relationships. American Psychological Association.  \n \n \n \n \n \n ",
  "topic": "Competence (human resources)",
  "concepts": [
    {
      "name": "Competence (human resources)",
      "score": 0.6847299337387085
    },
    {
      "name": "Psychology",
      "score": 0.5951805710792542
    },
    {
      "name": "Social competence",
      "score": 0.44269439578056335
    },
    {
      "name": "Social psychology",
      "score": 0.41480278968811035
    },
    {
      "name": "Social change",
      "score": 0.13440290093421936
    },
    {
      "name": "Political science",
      "score": 0.09496819972991943
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}