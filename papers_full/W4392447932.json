{
    "title": "Systematic Review of Large Language Models for Patient Care: Current Applications and Challenges",
    "url": "https://openalex.org/W4392447932",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2131432264",
            "name": "Felix Busch",
            "affiliations": [
                "Charité - Universitätsmedizin Berlin",
                "Freie Universität Berlin",
                "Humboldt-Universität zu Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A2368277023",
            "name": "Lena Hoffmann",
            "affiliations": [
                "Freie Universität Berlin",
                "Charité - Universitätsmedizin Berlin",
                "Humboldt-Universität zu Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A5009532076",
            "name": "Christopher Rueger",
            "affiliations": [
                "Humboldt-Universität zu Berlin",
                "Charité - Universitätsmedizin Berlin",
                "Freie Universität Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A4223688698",
            "name": "Elon Hc van Dijk",
            "affiliations": [
                "Leiden University Medical Center",
                "Sir Charles Gairdner Hospital"
            ]
        },
        {
            "id": "https://openalex.org/A2807926023",
            "name": "Rawen Kader",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2619691066",
            "name": "Esteban Ortiz Prado",
            "affiliations": [
                "Universidad de Las Américas"
            ]
        },
        {
            "id": "https://openalex.org/A2134728274",
            "name": "Marcus R. Makowski",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2120286100",
            "name": "Luca Saba",
            "affiliations": [
                "Azienda Ospedaliero-Universitaria Cagliari"
            ]
        },
        {
            "id": "https://openalex.org/A2079006570",
            "name": "Martin Hadamitzky",
            "affiliations": [
                "Technical University of Munich",
                "Deutsches Herzzentrum München"
            ]
        },
        {
            "id": "https://openalex.org/A2056217728",
            "name": "Jakob Nikolas Kather",
            "affiliations": [
                "Heidelberg University",
                "University Hospital Carl Gustav Carus",
                "University Hospital Heidelberg",
                "National Center for Tumor Diseases"
            ]
        },
        {
            "id": "https://openalex.org/A2029259259",
            "name": "Daniel Truhn",
            "affiliations": [
                "Universitätsklinikum Aachen"
            ]
        },
        {
            "id": "https://openalex.org/A1264139375",
            "name": "Renato Cuocolo",
            "affiliations": [
                "University of Salerno"
            ]
        },
        {
            "id": "https://openalex.org/A2432098160",
            "name": "Lisa C. Adams",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2767442332",
            "name": "Keno K. Bressem",
            "affiliations": [
                "Deutsches Herzzentrum München",
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2131432264",
            "name": "Felix Busch",
            "affiliations": [
                "Humboldt-Universität zu Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A2368277023",
            "name": "Lena Hoffmann",
            "affiliations": [
                "Humboldt-Universität zu Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A5009532076",
            "name": "Christopher Rueger",
            "affiliations": [
                "Humboldt-Universität zu Berlin"
            ]
        },
        {
            "id": "https://openalex.org/A4223688698",
            "name": "Elon Hc van Dijk",
            "affiliations": [
                "Sir Charles Gairdner Hospital",
                "Leiden University Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2807926023",
            "name": "Rawen Kader",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2619691066",
            "name": "Esteban Ortiz Prado",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2134728274",
            "name": "Marcus R. Makowski",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2120286100",
            "name": "Luca Saba",
            "affiliations": [
                "Azienda Ospedaliero-Universitaria Cagliari"
            ]
        },
        {
            "id": "https://openalex.org/A2079006570",
            "name": "Martin Hadamitzky",
            "affiliations": [
                "Deutsches Herzzentrum München",
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2056217728",
            "name": "Jakob Nikolas Kather",
            "affiliations": [
                "Heidelberg University",
                "University Hospital Carl Gustav Carus",
                "University Hospital Heidelberg",
                "National Center for Tumor Diseases"
            ]
        },
        {
            "id": "https://openalex.org/A2029259259",
            "name": "Daniel Truhn",
            "affiliations": [
                "Universitätsklinikum Aachen"
            ]
        },
        {
            "id": "https://openalex.org/A1264139375",
            "name": "Renato Cuocolo",
            "affiliations": [
                "University of Salerno"
            ]
        },
        {
            "id": "https://openalex.org/A2432098160",
            "name": "Lisa C. Adams",
            "affiliations": [
                "Technical University of Munich"
            ]
        },
        {
            "id": "https://openalex.org/A2767442332",
            "name": "Keno K. Bressem",
            "affiliations": [
                "Technical University of Munich",
                "Deutsches Herzzentrum München"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4389116614",
        "https://openalex.org/W6910575220",
        "https://openalex.org/W4392019609",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W4381587418",
        "https://openalex.org/W4362522726",
        "https://openalex.org/W4389325518",
        "https://openalex.org/W4379769651",
        "https://openalex.org/W4384455669",
        "https://openalex.org/W3118615836",
        "https://openalex.org/W2560438049",
        "https://openalex.org/W2901724277",
        "https://openalex.org/W2599225410",
        "https://openalex.org/W2109305774",
        "https://openalex.org/W4387914853",
        "https://openalex.org/W4387002841",
        "https://openalex.org/W4283770909",
        "https://openalex.org/W4388022936",
        "https://openalex.org/W4387008051",
        "https://openalex.org/W4386714656",
        "https://openalex.org/W4386647035",
        "https://openalex.org/W4383618574",
        "https://openalex.org/W4386493914",
        "https://openalex.org/W4386245555",
        "https://openalex.org/W4386414742",
        "https://openalex.org/W4388449007",
        "https://openalex.org/W4387501860",
        "https://openalex.org/W4376637945",
        "https://openalex.org/W4372348574",
        "https://openalex.org/W4387968480",
        "https://openalex.org/W4366447635",
        "https://openalex.org/W4379231355",
        "https://openalex.org/W4386046428",
        "https://openalex.org/W4386735567",
        "https://openalex.org/W4389045841",
        "https://openalex.org/W4386692525",
        "https://openalex.org/W4387823013",
        "https://openalex.org/W4387817162",
        "https://openalex.org/W4382929886",
        "https://openalex.org/W4388835064",
        "https://openalex.org/W4386776401",
        "https://openalex.org/W4385459604",
        "https://openalex.org/W4386278138",
        "https://openalex.org/W4385707210",
        "https://openalex.org/W4388485443",
        "https://openalex.org/W4388370035",
        "https://openalex.org/W4388923875",
        "https://openalex.org/W4388975763",
        "https://openalex.org/W4389507451",
        "https://openalex.org/W4377157938",
        "https://openalex.org/W4388869624",
        "https://openalex.org/W4387356888",
        "https://openalex.org/W4367175507",
        "https://openalex.org/W4382791987",
        "https://openalex.org/W4297834221",
        "https://openalex.org/W4388745175",
        "https://openalex.org/W4389232726",
        "https://openalex.org/W4384922275",
        "https://openalex.org/W4383187389",
        "https://openalex.org/W4387440167",
        "https://openalex.org/W4385667643",
        "https://openalex.org/W4388078979",
        "https://openalex.org/W4388775426",
        "https://openalex.org/W4385708775",
        "https://openalex.org/W4387451644",
        "https://openalex.org/W4388420209",
        "https://openalex.org/W4387496544",
        "https://openalex.org/W4385299173",
        "https://openalex.org/W4385997381",
        "https://openalex.org/W4386117070",
        "https://openalex.org/W4386151807",
        "https://openalex.org/W4386593417",
        "https://openalex.org/W4386304195",
        "https://openalex.org/W4388095746",
        "https://openalex.org/W4377563830",
        "https://openalex.org/W4381427645",
        "https://openalex.org/W4381930847",
        "https://openalex.org/W4386730959",
        "https://openalex.org/W4390105593",
        "https://openalex.org/W4381469233",
        "https://openalex.org/W4379883463",
        "https://openalex.org/W4387880927",
        "https://openalex.org/W4387747149",
        "https://openalex.org/W4387522969",
        "https://openalex.org/W4389793507",
        "https://openalex.org/W4388014118",
        "https://openalex.org/W4389993479",
        "https://openalex.org/W4388840552",
        "https://openalex.org/W4388802460",
        "https://openalex.org/W4390103037",
        "https://openalex.org/W4389076631",
        "https://openalex.org/W4387047777",
        "https://openalex.org/W4385570062",
        "https://openalex.org/W4390065167",
        "https://openalex.org/W4366823098",
        "https://openalex.org/W4387772689",
        "https://openalex.org/W4390493646",
        "https://openalex.org/W4389777382",
        "https://openalex.org/W4376637205",
        "https://openalex.org/W4386757039",
        "https://openalex.org/W4389155212",
        "https://openalex.org/W4389505505",
        "https://openalex.org/W4386753580",
        "https://openalex.org/W4390538712",
        "https://openalex.org/W4383896353",
        "https://openalex.org/W4377234252",
        "https://openalex.org/W4391069573",
        "https://openalex.org/W4391506193",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4387232979",
        "https://openalex.org/W4368372176",
        "https://openalex.org/W4376640725",
        "https://openalex.org/W4377009978",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4391591588",
        "https://openalex.org/W4376643691",
        "https://openalex.org/W4384816574",
        "https://openalex.org/W4385476863",
        "https://openalex.org/W4391995913"
    ],
    "abstract": "Abstract The introduction of large language models (LLMs) into clinical practice promises to improve patient education and empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the popularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, this systematic review aims to synthesize current applications and limitations of LLMs in patient care using a data-driven convergent synthesis approach. We searched 5 databases for qualitative, quantitative, and mixed methods articles on LLMs in patient care published between 2022 and 2023. From 4,349 initial records, 89 studies across 29 medical specialties were included, primarily examining models based on the GPT-3.5 (53.2%, n=66 of 124 different LLMs examined per study) and GPT-4 (26.6%, n=33/124) architectures in medical question answering, followed by patient information generation, including medical text summarization or translation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design and output. Design limitations included 6 second-order and 12 third-order codes, such as lack of medical domain optimization, data transparency, and accessibility issues, while output limitations included 9 second-order and 32 third-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. In conclusion, this study is the first review to systematically map LLM applications and limitations in patient care, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare settings.",
    "full_text": " 1\nSystematic Review of Large Language Models for Patient Care: Current Applications and Challenges \nFelix Busch1,*, Lena Hoffmann1, Christopher Rueger1, Elon HC van Dijk2,3, Rawen Kader4, Esteban Ortiz-\nPrado5, Marcus R Makowski6, Luca Saba7, Martin Hadamitzky8, Jakob Nikolas Kather9,10, Daniel Truhn11, \nRenato Cuocolo12, Lisa C Adams6,#, Keno K Bressem8,# \n \n1 Department of Neuroradiology, Charité – Universitätsmedizin Berlin, Corporate Member of Freie Universität \nBerlin and Humboldt Universität zu Berlin, Berlin, Germany \n2 Department of Ophthalmology, Leiden University Medical Center, Leiden, The Netherlands \n3 Department of Ophthalmology, Sir Charles Gairdner Hospital, Perth, Australia \n4 Division of Surgery and Interventional Sciences, University College London, London, United Kingdom  \n5 One Health Research Group, Faculty of Health Science, Universidad de Las Américas, Quito, Ecuador \n6 Department of Radiology, Technical University of Munich, Munich, Germany \n7 Department of Radiology, Azienda Ospedaliero Universitaria (A.O.U.), Cagliari, Italy \n8 Institute for Radiology and Nuclear Medicine, German Heart Center Munich, Technical University of Munich, \nMunich, Germany \n9 Department of Medical Oncology, National Center for Tumor Diseases (NCT), Heidelberg University Hospital, \nHeidelberg, Germany \n10 Else Kroener Fresenius Center for Digital Health, Medical Faculty Carl Gustav Carus, Technical University \nDresden, Dresden, Germany \n11 Department of Diagnostic and Interventional Radiology, University Hospital Aachen, Aachen, Germany \n12 Department of Medicine, Surgery and Dentistry, University of Salerno, Baronissi, Italy \n \n* Correspondence to: Felix Busch, MD; Address: Department of Neuroradiology, Charité – Universitätsmedizin \nBerlin, Corporate Member of Freie Universität Berlin and Humboldt Universität zu Berlin, Charitépl. 1, 10117 \nBerlin, Germany; E-mail: felix.busch@charite.de \n \n# These authors contributed equally to this work.  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n 2\nAbstract \nThe introduction of large language models (LLMs) into clinical practice promises to improve patient education \nand empowerment, thereby personalizing medical care and broadening access to medical knowledge. Despite the \npopularity of LLMs, there is a significant gap in systematized information on their use in patient care. Therefore, \nthis systematic review aims to synthesize current applications and limitations of LLMs in patient care using a \ndata-driven convergent synthesis approach. We searched 5 databases for qualitative, quantitative, and mixed \nmethods articles on LLMs in patient care published between 2022 and 2023. From 4,349 initial records, 89 \nstudies across 29 medical specialties were included, primarily examining models based on the GPT-3.5 (53.2%, \nn=66 of 124 different LLMs examined per study) and GPT-4 (26.6%, n=33/124) architectures in medical \nquestion answering, followed by patient information generation, including medical text summarization or \ntranslation, and clinical documentation. Our analysis delineates two primary domains of LLM limitations: design \nand output. Design limitations included 6 second-order and 12 third-order codes, such as lack of medical domain \noptimization, data transparency, and accessibility issues, while output limitations included 9 second-order and 32 \nthird-order codes, for example, non-reproducibility, non-comprehensiveness, incorrectness, unsafety, and bias. \nIn conclusion, this study is the first review to systematically map LLM applications and limitations in patient \ncare, providing a foundational framework and taxonomy for their implementation and evaluation in healthcare \nsettings. \n \nKeywords: Artificial Intelligence; Bias; Health Personnel; Medical Informatics; Natural Language Processing; \nPatient Care \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 3\n1. Introduction \nPublic and academic interest in large language models (LLMs) and their potential applications has increased \nsubstantially, especially since the release of OpenAI's ChatGPT (Chat Generative Pre-trained Transformers) in \nNovember 2022.\n1-3 One of the main reasons for their popularity is the remarkable ability to mimic human \nwriting, a result of extensive training on massive amounts of text and reinforcement learning from human \nfeedback.4 \nSince most LLMs are designed as general-purpose chatbots, recent research has focused on developing \nspecialized models for the medical domain, such as Meditron or BioMistral, by enriching the training data of \nLLMs with medical knowledge.5,6 However, this approach to fine-tuning LLMs requires significant \ncomputational resources that are not available to everyone and is also not applicable to closed-source LLMs, \nwhich are often the most powerful. Therefore, another approach to improve LLMs for biomedicine is to use \ntechniques such as Retrieval-Augmented Generation (RAG).\n7 RAG allows information to be dynamically \nretrieved from medical databases during the model generation process, enriching the output with medical \nknowledge without the need to train the model.  \nLLMs hold great promise for improving the efficiency and accuracy of healthcare delivery, e.g., by extracting \nclinical information from electronic health records, summarizing, structuring, or explaining medical texts, \nstreamlining administrative tasks in clinical practice, and enhancing medical research, quality control, and \neducation.8-10 In addition, LLMs have been shown to be versatile tools for supporting diagnosis or serving as \nprognostic models.11,12  \nIn contrast to applications primarily aimed at healthcare professionals, LLMs could also be used to promote \npatient education and empowerment by providing answers to medical questions and translating complex medical \ninformation into more accessible language.\n4,13 Thereby, LLMs may promote personalized medicine and broaden \naccess to medical knowledge, empowering patients to actively participate in their healthcare decisions. \nHowever, despite the growing body of research and the clear potential of LLMs, there is a gap in terms of \nsystematized information towards their use in patient care. To date, there has been no evaluation of existing \nresearch to understand the scope of applications and identify limitations that may currently limit the successful \nintegration of LLMs into clinical practice. \nTherefore, this systematic review aims to analyze and synthesize the literature on LLMs in patient care, \nproviding a systematic overview of 1) current applications and 2) challenges and limitations, with the purpose of \nestablishing a foundational framework and taxonomy for the implementation and evaluation of LLMs in \nhealthcare settings. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 4\n2. Methods \nThis systematic review was pre-registered in the International Prospective Register of Systematic Reviews \n(PROSPERO) under the identifier CRD42024504542 before the start of the initial screening and was conducted \naccording to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) \nguidelines.\n14,15 \n2.1 Eligibility criteria \nWe searched 5 databases, including the Web of Science, PubMed, Embase/Embase Classic, American for \nComputing Machinery (ACM) Digital Library, and Institute of Electrical and Electronics Engineers (IEEE) \nXplore as of January 25, 2024, to identify qualitative, quantitative, and mixed methods studies published \nbetween January 1, 2022, and December 31, 2023, that examined the use of LLMs for patient care. LLMs for \npatient care were defined as any artificial neural network that follows a transformer architecture and can be used \nto generate and translate text and other content or perform other natural language processing tasks for the \npurpose of disease management and support (i.e., prevention, preclinical management, diagnosis, treatment, or \nprognosis) that could be directly directed to or used by patients. Articles had to be available in English and \ncontain sufficient data for thematic synthesis (e.g., conference abstracts that did not provide sufficient \ninformation on study results were excluded). Given the recent surge in publications on LLMs such as ChatGPT, \nwe allowed for the inclusion of preprints if no corresponding peer-reviewed article was available. Duplicate \nreports of the same study, non-human studies, and articles limited to technology development/performance \nevaluation, pharmacy, human genetics, epidemiology, psychology, psychosocial support, or behavioral \nassessment were excluded. \n2.2 Screening and data extraction \nInitially, we conducted a preliminary search on PubMed and Google Scholar to define relevant search terms. The \nfinal search strategy included terms for LLMs, generative AI, and their applications in medicine, health services, \nclinical practices, medical treatments, and patient care (as detailed by database in Supplementary Section 1). \nAfter importing the bibliographic data into Rayyan and removing duplicates, LH and CR conducted an \nindependent blind review of each article's title and abstract.\n16 Any article flagged as potentially eligible by either \nreviewer proceeded to the full-text evaluation stage. For this stage, LH and CR used a custom data extraction \nform created in Google Forms (available online)17 to collect all relevant data independently from the studies that \nmet the inclusion criteria. Quality assessment was also performed independently for each article within this data \nextraction form, using the Mixed Methods Appraisal Tool (MMAT) 2018.\n18 Disagreements at any stage of the \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 5\nreview were resolved through discussion with the author FB. In cases of studies with incomplete data, we have \ntried to contact the corresponding authors for clarification or additional information. \n2.3 Data analysis \nDue to the diversity of investigated outcomes and study designs we sought to include, including qualitative, \nquantitative, and mixed methods, a meta-analysis was not practical. Instead, a data-driven convergent synthesis \napproach was selected for thematic syntheses of LLM applications and limitations in patient care.19 Following \nThomas and Harden, FB coded each study's numerical and textual data in Dedoose using free line-by-line \ncoding.20,21 Initial codes were then systematically categorized into descriptive and subsequently into analytic \nthemes, incorporating new codes for emerging concepts within a hierarchical tree structure. Upon completion of \nthe codebook, FB and LH reviewed each study to ensure consistent application of codes. Discrepancies were \nresolved through discussion with the author KKB, and the final codebook and analytical themes were discussed \nand refined in consultation with all contributing authors. \n \n3. Results \n3.1 Screening results \nOf the 4,349 reports identified, 2,991 underwent initial screening, and 126 were deemed suitable for potential \ninclusion and underwent full-text screening. Two articles could not be retrieved because the authors or the \ncorresponding title and abstract could not be identified online. Following full-text screening, 35 articles were \nexcluded, and 89 articles were included in the final review. Most studies were excluded because they targeted \nthe wrong discipline (n=10/35, 28.6%) or population (n=7/35, 20%) or were not original research (n=8/35, \n22.9%) (see Supplementary Section 2). For example, we evaluated a study that focused on classifying physician \nnotes to identify patients without active bleeding who were appropriate candidates for thromboembolism \nprophylaxis.\n22 Although the classification tasks may lead to patient treatment, the primary outcome was \ninforming clinicians rather than directly forwarding this information to patients. We also reviewed a study \nassessing the accuracy and completeness of several LLMs when answering Methotrexate-related questions.\n23 \nThis study was excluded because it focused solely on the pharmacological treatment of rheumatic disease. For a \ndetailed breakdown of the inclusion and exclusion process at each stage, please refer to the PRISMA flowchart \nin Figure 1. \n3.2 Characteristics of included studies \nTable 1 summarizes the characteristics of the analyzed studies, including their setting, results, and conclusions. \nOne study (n=1/89, 1.1%) was published in 2022\n24, 84 (n=84/89, 94.4%) in 202313,25-107, and 4 (n=4/89, 4.5%) in \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 6\n2024108-111 (all of which were peer-reviewed publications of preprints published in 2023). Most studies were \nquantitative non-randomized (n=84/89, 94.4%)13,25-27,29-101,103,104,106,107,109-111, 4 (n=4/89, 4.5%)28,102,105,108 had a \nqualitative study design, and one (n=1/89, 1.1%)24 was quantitative randomized according to the MMAT 2018 \ncriteria. However, the LLM outputs were often first analyzed quantitatively but followed by a qualitative \nanalysis of certain responses. Therefore, if the primary outcome was quantitative, we considered the study design \nto be quantitative rather than mixed methods, resulting in the inclusion of zero mixed methods studies. The \nquality of the included studies was mixed (see Table 2). The authors were primarily affiliated with institutions in \nthe United States (n=47 of 122 different countries identified per publication, 38.5%), followed by Germany \n(n=11/122, 9%), Turkey (n=7/122, 5.7%), the United Kingdom (n=6/122, 4.9%), China/Australia/Italy (n=5/122, \n4.1%, respectively), and 24 (n=36/122, 29.5%) other countries. Most studies examined one or more applications \nbased on the GPT-3.5 architecture (n=66 of 124 different LLMs examined per study, 53.2%)\n13,26-29,31-34,36-40,42-\n49,52-54,56-61,63,65-67,71,72,74,75,77,78,81-89,91,92,94,95,97-100,102-104,106-109,111 , followed by GPT-4 (n=33/124, 26.6%)13,25,27,29,30,34-\n36,41,43,50,51,54,55,58,61,64,68-70,74,76,79-81,83,87,89,90,93,96,98,99,101,105, Bard (n=10/124, 8.1%; now known as \nGemini)33,48,49,55,73,74,80,87,94,99, Bing Chat (n=7/124, 5.7%; now Microsoft Copilot)49,51,55,73,94,99,110, and other \napplications based on Bidirectional Encoder Representations from Transformers (BERT; n=4/124, 3.2%)13,83,84, \nLarge Language Model Meta-AI (LLaMA; n=3/124, 2.4%)55, or Claude by Anthropic (n=1/124, 0.8%)55. The \nmajority of applications were primarily targeted at patients (n=64 of 89 included studies, 73%)24,25,29,32,34-39,41-\n43,45-48,52-54,56-60,62,63,65,66,68-71,73-75,77-80,85-95,97,99,100,102-111 or both patients and caregivers (n=25/89, 27%)13,26-\n28,30,31,33,40,44,49-51,55,61,64,67,72,76,81-84,96,98,101. Information about conflicts of interest and funding was not explicitly \nstated in 23 (n=23/89, 25.8%) studies, while 48 (n=48/89, 53.9%) reported that there were no conflicts of interest \nor funding. A total of 18 (n=18/89, 20.2%) studies reported the presence of conflicts of interest and \nfunding.\n13,24,38,40,54,58,59,67,69-71,74,80,84,96,103,105,111 Most studies did not report information about the institutional \nreview board (IRB) approval (n=55/89, 61.8%) or deemed IRB approval unnecessary (n=28/89, 31.5%). Six \nstudies obtained IRB approval (n=6/89, 6.7%).\n52,82,84-86,92 \n3.3 Applications of Large Language Models \nAn overview of the presence of codes for each study is provided in Supplementary Section 3. The majority of \narticles investigated the use and feasibility of LLMs as medical chatbots (n=84/89, 94.4%)\n13,24-62,64-66,68,69,71-96,98-\n111, while fewer reports additionally or exclusively focused on the generation of patient information (n=19/89, \n21.4%)24,31,43,48,49,57,59,62,67,70,79,88-91,97,102,106,107, including clinical documentation such as informed consent forms \n(n=5/89, 5.6%)43,67,91,97,102 and discharge instructions (n=1/89, 1.1%)31, or translation/summarization tasks of \nmedical texts (n=5/89, 5.6%)24,49,57,79,89, creation of patient education materials (n=5/89, 5.6%)48,62,90,106,107, and \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 7\nsimplification of radiology reports (n=2/89, 2.3%)59,88. Most reports evaluated LLMs in English (n=88/89, \n98.9%)13,24-103,105-111, followed by Arabic (n=2/84, 2.3%)32,104, Mandarin (n=2/84, 2.3%)36,75, and Korean or \nSpanish (n=1/89, 1.1%, respectively)75. The top-five specialties studied were ophthalmology (n=10/89, \n11.2%)37,40,48,51,65,74,97,98,100,101, gastro-enterology (n=9/89, 10.1%)25,32,34,36,39,61,62,72,96, head and neck \nsurgery/otolaryngology (n=8/89, 9%)35,42,56,64,66,76,78,79, and radiology59,70,88-90,110 or plastic surgery45,47,49,102,107,108 \n(n=6/89, 6.7%, respectively). A schematic illustration of the identified concepts of LLM applications in patient \ncare is shown in Figure 2. \n3.4 Limitations of Large Language Models \nThe thematic synthesis of limitations resulted in two main concepts: one related to design limitations and one \nrelated to output.  \n3.4.1 Design limitations \nIn terms of design limitations, many authors noted the limitation that LLMs are not optimized for medical use \n(n=46/89, 51.7%)13,26,28,34,35,37-39,46,49,50,54-59,61,62,65,66,68,70,71,79-81,83-85,88,91,93-98,100-107,109, including implicit \nknowledge/lack of clinical context (n=13/89, 14.6%)28,39,46,66,71,79,81,83-85,98,103, limitations in clinical reasoning \n(n=7/89, 7.9%) 55,84,95,102-105, limitations in medical image processing/production (n=5/89, 5.6%)37,55,91,106,107, and \nmisunderstanding of medical information and terms by the model (n=7/89, 7.9%)28,38,39,59,62,65,97. In addition, \ndata-related limitations were identified, including limited access to data on the internet (n=22/89, \n24.7%)\n38,39,41,43,54-57,59,60,64,76,79,82-84,88,91,94,96,104,109, the undisclosed origin of training data (n=36/89, \n40.5%)25,26,29,30,32,34,36,37,40,46,47,50,51,53-60,64,65,70,71,76,82,83,91,94-96,101,105,109, limitations in providing, evaluating, and \nvalidating references (n=20/89, 22.5%)45,49,54-57,65,71,73,76,80,83,85,91,94,96,98,101,103,105, and storage/processing of \nsensitive health information (n=8/89, 9%)13,34,46,55,62,76,83,109. Further second-order concepts included black-box \nalgorithms, i.e., non-explainable AI (n=12/89, 13.5%)27,36,55,57,65,73,76,83,91,94,103,105, limited engagement and \ndialogue capabilities (n=10/89)13,27,28,37,38,51,56,66,95,103, and the inability of self-validation and correction (n=4/89, \n4.5%)61,73,74,107. \n3.4.2 Output limitations \nThe evaluation of limitations in output data yielded 7 second-order codes concerning the non-reproducibility \n(n=38/89, 42.7%)\n28,29,34,38,39,41,43,45,46,49,54-61,64,65,71-73,76,80,82,83,85,90,91,94,96,98,99,101,103-105, non-comprehensiveness \n(n=78/89, 87.6%)13,25,26,28-30,32-44,46,48-62,64,65,67-79,81-98,100,102-107,109-111, incorrectness (n=78/89, 87.6%)13,25-44,46,49-52,54-\n62,64-66,69-79,81-85,87-107,109-111, (un-)safeness (n=39/89, 43.8%)28,30,35,37,39,40,42-44,46,50,51,57-60,62,64,65,69,70,73,74,76,78-\n80,82,84,85,91,94,95,98-100,105,106,109, bias (n=6/89, 6.7%)26,32,34,36,66,103, and the dependence of the quality of output on the \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 8\nprompt-/input provided (n=27/89, 30.3%)26-28,34,38,41,44,46,51,52,56,68-72,74,76,78,79,81-83,90,94,95,100,101 or the environment \n(n=16/89, 18%)13,34,46,49-51,54,58,60,72,73,88,90,93,97,109. \nFor non-reproducibility, key concepts included the non-deterministic nature of the output, e.g., due to \ninconsistent results across multiple iterations (n=34/89, 38.2%)28,29,34,38,39,41,43,46,58-61,72,76,82,90,94,98,99,101,103,104 and \nthe inability to provide reliable references (n=20/89, 22.5%)45,49,54-57,65,71,73,76,80,83,85,91,94,96,98,101,103,105. Non-\ncomprehensiveness included nine concepts related to generic/non-personalized output (n=34/89, \n38.2%)\n13,28,30,34,37,38,41,43,49,51,56,57,59,61,65,70,77,79,81,84-86,90,94,95,100,102-107,110, incompleteness of output (n=68/89, \n76.4%)13,25,26,28-30,32,34-39,41-44,46,49-52,55-62,64,65,67-69,72-77,79,81-86,89-98,100,102-107,109-111, provision of information that is not \nstandard of care (n=24/89, 27%)28,40,43,46,49,50,54,57,58,65,69,72,73,77,78,81,85,91,94,98,100,103,107,111 and/or outdated (n=12/89, \n13.5%)13,25,32,34,38,41,43,44,49,54,83,84, and production of oversimplified (n=10/89, 11.2%)38,46,49,54,59,79,84,85,103, \nsuperfluous (n=16/89, 18%)13,28,34,38,46,62,72,79,86,90,94,97,100,106,107, overcautious (n=7/89, 7.9%)13,28,37,51,70,103,110, \noverempathic (n=1/89, 1.1%)13, or output with inappropriate complexity/reading level for patients (n=22/89, \n24.7%)13,34,42,48,50,51,53,55,56,67,71,78,79,85,87,88,90,93,106,107,109,110. For incorrectness, we identified 6 key concepts. Some of \nthe incorrect information could be attributed to what is commonly known as hallucination (n=38/89, \n42.7%)25,28,32,33,35-38,40-44,49-51,57-60,65,73,74,76,77,81,83,85,91,94,96-98,100,103,106,107,109, i.e., the creation of entirely fictitious or \nfalse information that has no basis in the input provided or in reality (e.g., \"You may be asked to avoid eating or \ndrinking for a few hours before the scan\" for a bone scan). However, numerous instances of misinformation were \nmore appropriately classified under alternative concepts of the original psychiatric analogy, as described in detail \nby Currie et al.\n43,112,113 These include illusion (n=12/89, 13.5%)28,36,38,43,57,59,77,78,85,88,94,105, which is characterized \nby the generation of deceptive perceptions or the distortion of information by conflating similar but separate \nconcepts (e.g., suggesting that MRI-type sounds might be experienced during standard nuclear medicine \nimaging), delirium (n=34/89, 38.2%)\n13,26,28,30,37,43,50,58,59,61,65,70,72-75,77,79,81-85,90-92,94,95,98,102,103,107,109,110, which \nindicates significant gaps in vital information, resulting in a fragmented or confused understanding of a subject \n(e.g., omission of crucial information about caffeine cessation for stress myocardial perfusion scans), \nextrapolation (n=11/89, 12.4%)\n43,59,65,78,81,91,94,106,107,110, which involves applying general knowledge or patterns to \nspecific situations where they are inapplicable (e.g., advice about injection-site discomfort that is more typical of \nCT contrast administration), delusion (n=14/89, 15.7%)\n28,30,43,50,59,65,69,73,74,78,81,94,103,111, a fixed, false beliefs \ndespite contradictory evidence (e.g., inaccurate waiting times for the thyroid scan), and confabulation (n=18/89, \n20.2%)\n25,28,36-38,40,46,59,62,65,71,77-79,94,103,107, i.e., filling in memory or knowledge gaps with plausible but invented \ninformation (e.g., \"You should drink plenty of fluids to help flush the radioactive material from your body\" for a \nbiliary system–excreted radiopharmaceutical). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 9\nMany studies rated the generated output as unsafe, including misleading (n=34/89, 38.2%)28,30,35,43,44,46,50,51,57-\n60,62,64,65,69,73,74,76,78-80,82,84,85,94,95,98-100,105,106,109 or even harmful content (n=26/89, 29.2%)28,30,37,39,40,42,43,50,51,58-\n60,70,73,74,76,79,84,85,91,94,95,98-100,109. A minority of reports identified biases in the output, which were related to \nlanguage (n=2/89, 2.3%)32,36, insurance status103, underserved racial groups26, or underrepresented procedures34 \n(n=1/89, 1.1%, each). Finally, many authors suggested that performance was related to the prompting/input \nprovided or the environment, i.e., depending on the evidence (n=7/89, 7.9%)52,68,69,71,81,82,95, complexity \n(n=11/89, 12.4%)28,34,44,46,70,74,76,79,94,102, specificity (n=13/89, 14.6%)27,38,41,56,70,72,74,76,78,81,95,100,101, quantity \n(n=3/89, 3.4%)26,52,74 of the input, type of conversation (n=3/89, 3.4%)27,51,90, or the appropriateness of the output \nrelated to the target group (n=9/89, 10.1%)46,49,51,54,72,90,93,97,109, provider/organization (n=4/89, 4.5%)13,50,60,88, and \nlocal/national medical resources (n=5/89, 5.6%)34,50,58,60,73. Figure 3 illustrates the hierarchical tree structure and \nquantity of the codes derived from the thematic synthesis of limitations. \n \n4. Discussion \nIn this systematic review, we synthesized the current applications and limitations of LLMs in patient care, \nincorporating a broad analysis across 29 medical specialties and highlighting key limitations in LLM design and \noutput, providing a comprehensive framework and taxonomy for their future implementation and evaluation in \nhealthcare settings. \nMost articles examined the use of LLMs based on the GPT-3.5 or GPT-4 architecture for answering medical \nquestions, followed by the generation of patient information, including medical text summarization or translation \nand clinical documentation. The conceptual synthesis of LLM limitations revealed two key concepts: the first \nrelated to design, including 6 second-order and 12 third-order codes, and the second related to output, including \n9 second-order and 32 third-order codes.  \nAlthough many LLMs have been developed specifically for the biomedical domain in recent years, we found \nthat ChatGPT has been a disruptor in the medical literature on LLMs, with GPT-3.5 and GPT-4 accounting for \nalmost 80% of the LLMs examined in this systematic review. While it was not possible to conduct a meta-\nanalysis of the performance on medical tasks, many authors provided a positive outlook towards the integration \nof LLMs into clinical practice. However, the use of proprietary models such as ChatGPT in the biomedical field \nraises concerns because the limited access to the underlying algorithms, training data, and data processing and \nstorage mechanisms makes them untransparent and, thus, significantly limits their applicability in healthcare.\n114 \nFurthermore, the integration of proprietary models into patient care applications makes one susceptible to \nperformance changes associated with model updates, which may break existing functionalities and lead to \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 10\nharmful outcomes for patients. Therefore, especially in the biomedical field, open-source models such as \nBioMistral may offer a viable solution. 6 Given the limited number of articles on open-source LLMs in our \nreview, we strongly encourage future studies investigating the applicability of open-source LLMs in patient care. \nWe identified several key limitations regarding the design and output. Not surprisingly, many reports noted the \nlimitation that the LLMs studied were not optimized for the medical domain. One possible solution to this \nlimitation may be to provide medical knowledge during inference using RAG.\n115 However, even when trained \nfor general purposes, ChatGPT has previously been shown to pass the United States Medical Licensing \nExamination (USMLE), the German State Examination in Medicine, or even a radiology board-style \nexamination without images.116-119 Although outperformed on specific tasks by specialized medical LLMs, such \nas Google's MedPaLM-2, this suggests that general-purpose LLMs can comprehend complex medical literature \nand case scenarios to a degree that meets professional standards.\n120 Furthermore, given the large amounts of data \non which proprietary models such as ChatGPT are trained, it is not unlikely that they have been exposed to more \nmedical data overall than smaller specialized models despite being generalist models. \nIt should also be noted that passing these exams does not equate to the practical competence required of a \nhealthcare provider.\n121 In addition, reliance on exam-based assessments carries a significant risk of bias. For \nexample, if the exam questions or similar variants are publicly available and, thus, may be present in the training \ndata, the LLM does not demonstrate any knowledge outside of training data memorization.122 In fact, these types \nof tests can be misleading in estimating the model's true abilities in terms of comprehension or analytical skills. \nMany studies have reported limitations in the output related to comprehensiveness, safety, correctness, \nreproducibility, and dependence of the output on the input/prompt and environment. Specifically, for \ncorrectness, we followed the taxonomy of Currie et al. to classify incorrect outputs more precisely into illusions, \ndelusions, delirium, confabulation, and extrapolation, thus proposing a framework for a more precise and \nstructured error classification to improve the characterization of incorrect outputs and enabling more detailed \nperformance comparisons with other research.\n43,112,113 On the other hand, a minority of studies have identified \nbiases, for example, reflecting the unequal representation of certain content or the biases inherent in human-\ngenerated text in the training data.123 This may indicate that the implemented safeguards are effective. However, \nnot much is known about the technology and developer policies of proprietary LLMs, and previous work has \nshown that automated jailbreak generation is possible across various commercial LLM chatbots. 124 This also \nmirrors our concept of data-related limitations, particularly regarding the handling of sensitive health \ninformation. Together with the limited transparency about the origin of the training data and the unexplainable \nand non-deterministic nature of the output, this raises a key question when applying LLMs to the medical \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 11\ndomain: how can we entrust our patients to LLMs if they are neither reliable nor transparent? Given that models \nlike ChatGPT are already publicly accessible and widely used, patients may already refer to them for medical \nquestions in much the same way they use Google Search, making concerns about their early adoption somewhat \nacademic.\n125  \nIn addition, low health literacy due to the identified limitations in comprehensiveness, including the generation \nof content with high complexity and an inappropriate reading level, which was above the 6th-grade level \nrecommended by the American Medical Association (AMA) in almost all studies analyzed, may further limit \ntheir utility for patient information.126 Overall, this can lead to results that are misleading and harmful, as \ndescribed in many of the reports in our review. In addition to advances in the development of LLMs and the \nfocus on open source, it will therefore be necessary to develop and implement a well-validated scale to \ndetermine the quality and safety of LLM outputs in medical practice, such as the recent effort made to adopt the \nwidely recognized Physician Documentation Quality Instrument (PDQI-9) for the assessment of AI transcripts \nand clinical summaries.\n127   \nFinally, the implementation of regulatory mandates like the forthcoming European Union AI Act and the \nassociated challenges faced by generative AI and LLMs, for example, in terms of training data transparency and \nvalidation of non-deterministic output, will show which approaches the companies will take to bring these \nmodels into compliance with the law. How the notified bodies interpret and enforce the law in practice will \nlikely be decisive for the further development of LLMs in the biomedical sector.128 \n4.1 Limitations \nOur study has limitations. First, our review focused on LLM applications and limitations in patient care, thus \nexcluding research directed at clinicians only. Future studies may extend our synthesis approach to LLM \napplications that explicitly focus on healthcare professionals. Second, there is a risk that potentially eligible \nstudies were not included in our analysis if they were not present in the 5 databases reviewed or were not \navailable in English. However, we screened nearly 3,000 articles in total and systematically analyzed 89 articles, \nproviding a comprehensive overview of the current state of LLMs in patient care, even if some articles could \nhave been missed. Third, the rapid development and advancement of LLMs make it difficult to keep this \nsystematic review up to date. For example, Gemini 1.5 Pro was published in February 2024, and corresponding \narticles are not included in this review, which synthesized articles from 2022 to 2023. Continued updates will be \nessential to monitor emerging areas and limitations in this rapidly evolving field. \n \n5. Conclusion \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 12\nIn conclusion, this review provides a systematic overview of current LLM applications and limitations in patient \ncare. Our conceptual synthesis provides a structured taxonomy that may lay the groundwork for both the \nimplementation and critical evaluation of LLMs in healthcare settings.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 13\n6. Declarations \n6.1 Acknowledgements \nThis research is funded by the European Union (101079894). Views and opinions expressed are however those \nof the authors only and do not necessarily reflect those of the European Union or European Commission. Neither \nthe European Union nor the granting authority can be held responsible for them. The funding had no role in the \nstudy design, data collection and analysis, manuscript preparation, or decision to publish. \n6.2 Competing interests \nJNK declares consulting services for Owkin, France; DoMore Diagnostics, Norway; Panakeia, UK, and Scailyte, \nBasel, Switzerland; furthermore JNK holds shares in Kather Consulting, Dresden, Germany; and StratifAI \nGmbH, Dresden, Germany, and has received honoraria for lectures and advisory board participation by \nAstraZeneca, Bayer, Eisai, MSD, BMS, Roche, Pfizer and Fresenius. DT holds shares in StratifAI GmbH, \nDresden, Germany and has received honoraria for lectures by Bayer. KKB reports grants from the European \nUnion (101079894) and Wilhelm-Sander Foundation; participation on a Data Safety Monitoring Board or \nAdvisory Board for the EU Horizon 2020 LifeChamps project (875329) and the EU IHI Project IMAGIO \n(101112053); speaker Fees for Canon Medical Systems Corporation and GE HealthCare. RK receives medical \nconsultancy fees from Odin Vision. \n6.3 Author contributions \nConceptualization: FB, LCA, KKB; Project administration: FB; Resources: FB, LCA, KKB; Software: FB, \nLCA, KKB; Data curation: FB, LH, CR; Formal analysis: FB, LH, CR, LCA, KKB; Investigation: FB, LH, CR, \nLCA, KKB; Methodology: FB; Supervision: FB, LCA, KKB; Validation: FB, LH, CR, EHCvD, RK, EOP, \nMRM, LS, MH, JNK, DT, RC, LCA, KKB; Visualization: FB, LCA; Writing – original draft preparation: FB, \nLH, LCA, KKB; Writing – review & editing: FB, LH, CR, EHCvD, RK, EOP, MRM, LS, MH, JNK, DT, RC, \nLCA, KKB. \n6.4 Data availability \nAll data generated or analyzed during this study are included in this published article and its supplementary \ninformation files.  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 14\n7. Figures \n \nFigure 1. Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) flow diagram. \n  \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nRecords identified from: \nWeb of Science (n = 908) \nPubMed (n = 1,706) \nEmbase and Embase Classic \n(n = 968) \nACM Digital Library (n = 248) \nIEEE Xplore (n = 519) \n \nRecords removed before \nscreening: \nDuplicate records removed (n \n= 1,358) Records screened \n(n = 2,991) \nRecords excluded \n(n = 2,865) \nReports sought for retrieval \n(n = 126) Reports not retrieved (n = 2) \nReports assessed for eligibility \n(n = 124) \nReports excluded: \nWrong population (n = 7) \nWrong discipline (n = 10) \nWrong study design (n = 8) \nTechnological development/ \nperformance evaluation (n = \n6) \nInsufficient data for thematic \nsynthesis (n = 4) \nStudies included in review \n(n = 89) \nReports of included studies \n(n = 89) \n \nIdentification \nScreening \n \nIncluded \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 15\n \nFigure 2. Schematic illustration of the identified concepts for the application of large language models (LLMs) \nin patient care. \n  \n/i1\n/i1\n/i1\n/i1\n/i1\nGeneration of patient \ninformation \nMedical question \nanswering/chatbot \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 16\n \nFigure 3. Illustration of the hierarchical tree structure for the thematic synthesis of large language model (LLM) \nlimitations in patient care, including the presence of codes for each concept. \n  \nUnderrepresented procedures (n = 1)\nUnderserved racial groups (n = 1)\nInsurance status (n = 1)\nLanguage (n = 2)\nLocal/national medical resources (n = 5)\nProvider/organization (n = 4)\nTarget group (n = 9)\nConversation type (n = 3)\nQuantity (n = 3)\nSpecificity (n = 13)\nComplexity (n = 11)\nEvidence (n = 7)\nHarmful (n = 26)\nMisleading (n = 34)\nConfabulation (n = 18)\nIllusion (n = 12)\nHallucination (n = 38)\nDelusion (n = 14)\nDelirium (n = 34)\nExtrapolation (n = 10)\nHigh complexity/reading level (n = 22)\nOverempathic (n = 1)\nOvercautious (n = 7)\nSuperfluous (n = 16)\nOversimplification (n = 10)\nOutdated (n = 12)\nNon-standard of care (n = 24)\nIncomplete (n = 68)\nGeneric/non-personalized (n = 34)\nNon-referenceable (n = 20)\nNon-deterministic (n = 24)\nNot open source (n = 10)\nNot freely accessible (n = 9)\nLimited number of prompts (n = 3)\nStores/processes sensitive health information (n = 8)\nLimited in reference provision/evaluation/validation (n = 20)\nUndisclosed origin of training data (n = 36)\nRestricted access to internet data (n = 22)\nMisunderstanding of medical information/terms (n = 7)\nLimited in processing/producing medical images (n = 5)\nLimited clinical reasoning (n = 7)\nImplicit knowledge/lack of clinical context (n = 13)\nBias (n = 6)\nEnvironment-dependent (n = 16)\nPrompt-/input dependent (n = 27)\nUnsafe (n = 39)\nIncorrect (n = 78)\nNon-comprehensive (n = 78)\nNon-reproducible (n = 38)\nIncapable of self-validation/correction (n = 4)\nLimited engagement/dialogue capabilities (n = 10)\nBlack box (n = 12)\nAccessibility (n = 18)\nData (n = 55)\nNot optimized for the medical domain (n = 46)\nOutput (n = 86)\nDesign (n = 67)\nLLM limitations\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 17\n8. Tables \nTable 1. Overview of included studies and corresponding authors, year of publication, affiliation countries of authors, study design, medical specialty, purpose of study, large \nlanguage model (LLM)/tool examined, target user, evaluation/setting, main outcome, and conclusion. \nAuthors, year; country Study \ndesign \nSpecialty; purpose LLM/tool; target \nuser \nEvaluation/setting Main outcome Overall conclusion \n1. Samaan et al., 2023; \nUSA, UK25 \nQuantitative Gastroenterology; GPT-4's \naccuracy and reproducibility in \nresponding to patient queries on \ninflammatory bowel disease \n(IBD) nutrition \nGPT-4; patients 88 questions from adult patients in a tertiary hospital, \nFacebook, Reddit; response assessment by 2 IBD \ndietitians \nAccuracy: 83%, comprehensiveness: 69%, mixed \naccuracy: 17%, completely incorrect: 0% \nGPT-4 is a promising tool for \nIBD patients seeking nutrition-\nrelated information \n2. Eromosele et al., \n2023; USA26 \nQuantitative Cardiology; GPT-3.5 knowledge \nof cardiovascular disease (CVD) \nracial disparities \nGPT-3.5; patients; \ncaregivers \n60 questions were prompted 3 times; response \nassessment by a team of cardiologists and other \nspecialized clinicians \nAppropriate knowledge: 63.4%, inappropriate: \n33.3%, unreliable: 3.3%; 91%/79% of \nincorrect/hedging  \nresponses involved CVD disparities affecting a \nminority or underserved racial group \nGPT-3.5 has satisfactory but \nsuboptimal knowledge of CVD \nracial disparities \n3. Johri et al., 2023; \nUSA27 \nQuantitative Dermatology; GPT-4's and GPT-\n3.5's clinical reasoning \ncapabilities using a multi-agent \nconversational framework \nGPT-3.5/GPT-4; \npatients/caregivers \n140 skin cancer cases, 100 from an online \ndermatology website, 40 new cases by 3 dermatology \nresidents; diagnostic accuracy of GPT-4 and GPT-3.5 \nin conversational versus static settings  \n91.9%/83.3% accuracy of GPT-4/GPT-3.5 for \nstatic vignettes, 85.4%/72.4% for cases in \nconversational format; GPT-3.5 accuracy improves \nthrough conversational summarization (72.4% to \n81%); increasing multiple choice (MC)-options \nlead to decreasing accuracy (GPT-4: 85.4% to \n57.2%; GPT-3.5: 72.4% to 20.1%) \nGPT-4/GPT-3.5 have \nlimitations in integrating \ndetails from conversational \ninteractions \n4. Braga et al., 2023; \nBrazil, Canada, USA28 \nQualitative Pediatric Urology; GPT-3.5's \nreliability in concept description \nand usefulness for decision-\nmaking in urology \nGPT-3.5; \npatients/caregivers \n3 queries each for primary megaureter, enuresis, and \nvesicoureteral reflux were prompted twice; qualitative \nevaluation by 3 specialists \nGPT-3.5's responses partly contain accurate and \npertinent information, but most are insufficient and \nmisleading; better performance in less complex \nquestions \nGPT-3.5 lacks clinical \nexperience and judgment, \npotentially providing false \ninformation \n5. King et al., 2023; \nUSA29 \nQuantitative Cardiology; GPT-4's and GPT-\n3.5's accuracy and reproducibility \nin answering heart failure \nquestions; performance between \nGPT-3.5 and GPT-4 \nGPT-3.5/GPT-4; \npatients \n107 questions related to heart failure from medical \nsocieties/institutions and Facebook support groups \nwere prompted twice; assessment by 2 board-certified \ncardiologists \nGPT-4 had highest accuracy in basic \nknowledge/management: 89.8%/82.9%; GPT-3.5 \nhad highest accuracy in management/other \n(forecasting, procedures, support): 78.1%/94.1%; \npartially/incorrect answers: GPT-3.5: 1.9%/GPT-4: \n0%; reproducibility: GPT-3.5: 94%/GPT-4: 100% \nGPT-3.5/GPT-4 have the \npotential to serve as accurate \nand reliable resources for \npatients with heart failure \n6. Huang et al., 2023; \nUSA30 \nQuantitative Neurology; GPT-4's effectiveness \nin identifying and explaining \nmisinformation about \nAlzheimer's disease (AD) and \ngenerating audience-specific, \nreadable explanations \nGPT-4; \npatients/caregivers \n20 myths about AD validated by 200 Amazon \nMechanical Turk participants; response assessment \nby 11 practitioners/clinicians working primarily in \ngeriatrics \nGPT-4 identified 100% of the myths as false; \nreadability: 81% strongly agree/agree; overall \nresults of information retention: 82% strongly \nagree/agree; potential in clarifying AD \nmisinformation: 82% strongly agree/agree \nGPT-4 has potential value in \nmitigating AD misinformation; \nneed for more refined/detailed \nexplanations of disease \nmechanisms and treatments \n7. Hanna et al., 2023; \nUSA31 \nQuantitative Infectiology; GPT-3.5's potential \nracial and ethnic bias in writing \ndischarge instructions for HIV \npatients \nGPT-3.5; \npatients/caregivers \nDischarge instructions based on 100 deidentified HIV \npatient encounters prompted 4 times each while \nswitching race and ethnicity; statistical assessment of \npolarity, subjectivity, named entity recognition \n(NER), Flesch Reading Ease score (readability score), \nFlesch-Kincaid Grade Level (readability grade), word \nfrequency \nNo significant differences in generated text \nregarding polarity, subjectivity, NER, readability, \nand text length across different races/ethnicities \nand insurance types; statistically significant \ndifferences in word frequency across \nraces/ethnicities and subjectivity across insurance \ntypes (commercial insurance eliciting most \nsubjective responses) \nGPT-3.5 is relatively invariant \nto race/ethnicity and insurance \ntype in terms of linguistic and \nreadability measures \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 18\n8. Liu et al., 2023; \nUSA13 \nQuantitative Primary Care; to develop a fine-\ntuned LLM on messages and \nhealthcare provider responses \nfrom a patient portal; to assess \nand compare responses to actual \nprovider responses and responses \nfrom GPT-3.5/GPT-4 \nGPT-3.5/GPT-4, \nCLAIR-Short/-\nLong based on \nLLaMA-65B; \npatients/caregivers \n10 representative questions based on a patient \nmessage framework were chosen by a primary care \nphysician; responses by the LLMs were randomized \nand evaluated by 4 primary care physicians, and \nBERTScore values \nResponses generated by GPT-3.5/4 received the \nhighest ratings in terms of empathy, \nresponsiveness, accuracy, and usefulness; GPT-\n3.5/4 and CLAIR-Long outperform CLAIR-Short \nand doctors' responses significantly \nLLMs have great potential in \ngenerating responses to patient \nmessages, facilitating \ncommunication between \npatients and primary care \nproviders \n9. Samaan et al., 2023; \nUSA32 \nQuantitative Gastroenterology; GPT-3.5's \nability to understand and respond \nto liver cirrhosis patient questions \nin Arabic compared to English \nGPT-3.5; patients 91 liver cirrhosis questions from professional \nsocieties, institutions, and Facebook support groups \nwere translated into Arabic by 2 bilingual physicians; \nresponse grading by a bilingual board-certified \ntransplant hepatologist \nArabic: 72.5% of questions correct, thereof 24.2% \ngraded as comprehensive; English: 79.1% correct, \n47.3% graded as comprehensive; 9.9% of the \nArabic answers were rated as more accurate, 57.1% \nas similarly accurate, and 33% as less accurate \ncompared to English \nGPT-3.5 has the potential to \nserve as an additional source of \ninformation for Arabic-\nspeaking patients with liver \ncirrhosis, although its \nperformance in Arabic is less \naccurate than in English \n10. Patnaik et al., 2023; \nUSA33 \nQuantitative Anesthesiology; GPT-3.5's and \nBard's ability to answer \nanesthesia-related patient \nquestions before surgery  \nGPT-3.5, Bard; \npatients/caregivers \n11 anesthesia-related questions were collected during \npre-anesthesia consultation; response evaluation using \nhallucination counts, readability using Flesch-Kincaid \nGrade Level (FKG), lexical diversity measures \n(MTLD), computational sentiment analysis, and \nLevenshtein distances \nGPT-3.5 displayed no hallucination errors, Bard \nhad a 30.3% error rate; FKG scores and MTLD \nwere higher for GPT-3.5 compared to Bard, \nsubjectivity scores were similar \nChatGPT was technical, \nprecise, and descriptive, \nwhereas Bard was \nconversational, adequate, and \nexhibited hallucinations. \n11. Ali et al., 2023; \nUSA34 \nQuantitative Gastroenterology; GPT-3.5's and \nGPT-4's ability to answer \nquestions about EGD, \ncolonoscopy, EUS, and ERCP \nand provide emotional support \nGPT-3.5/GPT-4; \npatients \n113 questions on endoscopic procedures collected \nfrom professional societies or institutional websites; \nresponse grading by 2 board-certified \ngastroenterologists or 2 advanced endoscopists; \nevaluation of emotional support questions by a \ncertified psychiatrist \nComprehensiveness: EGD: 57.9%, colonoscopy: \n47.6%, EUS: 48.1%, \nERCP: 44.4%; medical accuracy: highest for EGD \n(52.6% fully accurate), \nlowest for EUS (40.7% fully accurate); superfluous \ncontent: responses were predominantly concise for \nEGD and colonoscopy, with ERCP and EUS \nshowing increased extraneous content; \nreproducibility scores: varied across domains, from \n50.34% (for EUS) to 68.6% (for EGD); \nemotional support: GPT-4 outperformed GPT-3.5 \nGPT3.5/4 holds promise as a \nsupplementary patient resource \nfor common endoscopic \nprocedures. \n12. Suresh et al., 2023; \nUSA35 \nQuantitative Otolaryngology; GPT-4's utility \nas an informational resource for \notolaryngology patients \nGPT-4; patients 18 otolaryngology questions were designed based on \nthe American Academy of Otolaryngology's Clinical \nPractice Guidelines; response evaluation by clinicians \nwith expertise or subspecialty training in \notolaryngology \nSafe responses: 100%; Accurate responses: 78%; \nComprehensive responses: 83% \nGPT-4's otolaryngology advice \nis safe but lacks accuracy and \ncomprehensiveness, limiting \nits utility as an informational \nresource for patients \n13. Yeo et al., 2023; \nUSA36 \nQuantitative Gastroenterology; GPT-3.5 \nversus GPT-4 in understanding \nand responding to cirrhosis-\nrelated questions in English, \nKorean, Mandarin, and Spanish \nGPT-3.5/GPT-4; \npatients \n36 English liver cirrhosis questions collected from \nhealthcare organizations and patient support groups \nwere translated into Korean, Mandarin, and Spanish; \naccuracy grading based on the American Association \nfor the Study of Liver Diseases guidelines; similarity \nof non-English responses graded by native-speaking \nhepatologists  \nGPT-4 showed higher response accuracy compared \nto GPT-3.5 across all languages; GPT-4 performed \nsignificantly better in Mandarin and Korean than \nGPT-3.5 \nGPT-4 outperformed GPT-3.5 \nin responding to English and \nnon-English questions related \nto cirrhosis \n14. Knebel et al., 2023; \nGermany37 \nQuantitative Ophthalmology; GPT-3.5's \nperformance in triaging \nophthalmological emergencies \nGPT-3.5; patients 10 case vignettes, derived from guideline-based \nprevention topics, each prompted 5 times; responses \nwere assessed for triage accuracy, appropriateness of \nrecommended preclinical measures, and potential \nharm \nTriage accuracy: 93.6%; treatment accuracy: \n100%; diagnosis accuracy: 61.5%; appropriate \nprehospital measures: 66%; potentially harmful to \nusers/patients: 32% \nGPT-3.5 should not be used as \nthe sole primary source of \ninformation about acute \nophthalmologic symptoms \n15. Zhu et al., 2023; \nChina38 \nQuantitative Urology; LLM's utility as \nconsultants for prostate cancer \npatients \nGPT-3.5 free/plus, \nYouChat, \nNeevaAI, \nPerplexity \n(concise and \ndetailed model), \nChatsonic; patients \n22 prostate cancer questions based on patient \neducation guidelines from the Centers for Disease \nControl and Prevention and UpToDate; response \nevaluation based on accuracy, comprehensiveness, \npatient readability, humanistic care, and stability by 3 \nurologists \nGPT-3.5 free responses had the highest accuracy \n(100% correct), were most comprehensive (95.5% \nvery comprehensive), and most consistent (100%); \nreadability was highest for \nGPT-3.5 free/plus (100%) compared to other \nLLMs \nLLMs have the potential to be \napplied in the education and \nconsultation of prostate cancer \npatients but are not yet capable \nof completely replacing \ndoctors \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 19\n16. Lahat et al., 2023; \nIsrael39 \nQuantitative Gastroenterology; GPT-3.5's \nperformance in answering patient \ngastroenterological health \nquestions \nGPT-3.5; patients 110 gastroenterology questions were collected from \nopen internet sites providing medical information to \ndiverse patients' questions; response assessment for \naccuracy, clarity, up-to-date knowledge, and \neffectiveness by 3 gastroenterologists \nMean scores (1 to 5) for treatment questions: \naccuracy: 3.9, clarity: 3.9, efficacy: 3.3; mean \nscores for symptom questions: accuracy: 3.4, \nclarity: 3.7, efficacy: 3.2; mean scores for \ndiagnostic test questions: accuracy: 3.7, clarity: 3.7, \nefficacy: 3.5  \nGPT-3.5 has potential as an \ninformation source in the field \nof gastroenterology, but further \ndevelopment is needed \n17. Bernstein et al., \n2023; USA40 \nQuantitative Ophthalmology; GPT-3.5's \nquality of ophthalmology advice \ncompared to ophthalmologist-\nwritten advice \nGPT-3.5; \npatients/caregivers \n200 question-answer pairs from the Eye Care Forum, \nwith responses from American Academy of \nOphthalmology-affiliated physicians; generated and \noriginal questions randomly presented to 8 board-\ncertified ophthalmologists, assessment for incorrect \ninformation, harm likelihood and severity, medical \nconsensus alignment \nMean accuracy of the expert panel for \ndistinguishing between AI and human answers was \n61.3% (individual rater accuracy range: 45% to \n74%); no significant differences in responses \ncontaining incorrect or inappropriate information \nand likelihood or extent of harm between GPT-3.5 \nand human answers \nGPT-3.5 provides \nophthalmologic advice of \ncomparable quality to that of \nophthalmologists \n18. Rogasch et al., 2023; \nGermany41 \nQuantitative Nuclear Medicine; GPT-4's \nability to answer patient \nquestions related to [18F]FDG \nPET/CT imaging for Hodgkin \nlymphoma or lung cancer \nGPT-4; patients 25 tasks, including responding to 13 frequently asked \npatient questions/6 follow-up questions and \nexplaining six fictitious PET/CT reports prompted 3 \ntimes; rating by 3 nuclear medicine physicians for \nappropriateness, helpfulness, inconsistency, validity \nof references \nAppropriate responses: 92%; helpful: 96%; \nconsiderable inconsistencies: 16%; references fully \nvalid: 21% \nGPT-4 has the potential to \noffer adequate informational \ncounseling to patients \nundergoing [\n18F]FDG PET/CT \nimaging \n19. Campbell et al., \n2023; USA42 \nQuantitative \n  \nOtolaryngology; GPT-3.5's utility \nas an educational resource for \npatients on thyroid nodules \nGPT-3.5; patients 30 questions on thyroid nodules were prompted 4 \ntimes using different prompting strategies: no \nprompting, patient-friendly prompting, 8th-grade-\nlevel prompting, and prompting for references; \nresponse grading for medical accuracy and clinical \nappropriateness by 2 otolaryngology resident \nphysicians \n69.2% of responses were \"at least correct\" and did \nnot differ by prompting strategy; 87.5% of medical \nliterature references were legitimate citations \nthereof 17.1% with incorrectly or completed \nfalsified findings; 12.5% of references were \nunfindable or incorrect \nGPT-3.5 answers most \nquestions about thyroid \nnodules appropriately, \nregardless of prompting \n20. Currie et al., 2023; \nAustralia43 \nQuantitative \n \nNuclear Medicine; GPT-3.5's and \nGPT-4's ability to create patient \ninformation sheets for nuclear \nmedicine procedures \nGPT-3.5/GPT-4; \npatients \n7 patient information sheets suitable for gaining \ninformed consent for 7 common procedures in nuclear \nmedicine; assessment by 3 nuclear medicine \ntechnologists or scientists for accuracy, \nappropriateness, currency, and fitness for the purpose \nGPT-4 outperformed GPT-3.5 in accuracy, \nappropriateness, currency, and fitness-for-purpose \nbut was often below the minimum standard; GPT-\n3.5's responses were below average for all except \nbone scan (which was average), GPT-4 produced \nhigher-quality patient information sheets, with 3 \nclassified as fit for the purpose \nGPT-3.5 is ineffective for \nnuclear medicine patient \ninformation; GPT-4 provides \nmore accurate patient \ninformation and may be used \nfor informed consent \n21. Draschl et al., 2023; \nAustria44 \nQuantitative \n \nOrthopedics; GPT-3.5's \nperformance in answering \nquestions about periprosthetic \njoint infections of the hip and \nknee \nGPT-3.5; \npatients/caregivers \n27 questions from the 2018 International Consensus \nMeeting on Musculoskeletal Infection; response \nevaluation by 3 orthopedic surgeons for \ncompleteness, misleading information, errors, up-to-\ndateness, patient and surgeon suitability \nMedian completeness, up-to-dateness, \npatient/surgeon suitability of responses (on a 5-\npoint Likert scale, with 5 indicating strongly \nagree): 4; median Likert-Scale scores for \nmisleading or erroneous responses (with 5 \nindicating strongly disagree): 4  \nGPT-3.5 is a predominantly \nreliable and useful tool for \northopedic surgeons and \npatients in complex orthopedic \nquestions \n22. Alessandri-Bonetti et \nal., 2023; USA45 \nQuantitative \n \nPlastic Surgery; GPT-3.5's \npotential as a viable source for \npatient education on body \ncontouring compared to Google \nsearch \nGPT-3.5; patients 15 questions and responses/references from the \n\"People also ask\" section of a Google search for \n\"body contouring surgery\"; 4 blinded plastic surgeons \nrated the answer quality of Google and GPT-3.5 using \nthe Global Quality Score  \nGoogle responses were rated as poor quality with \nlimited usefulness to patients (mean Likert score: \n2.55); GPT-3.5 responses were rated as higher \nquality and more useful to patients (mean Likert \nscore: 4.38); 33% of GPT-3.5 responses did not \nprovide references when asked; 6% of references \nwere inaccessible or linked to unrelated sites \nGPT-3.5 outperformed Google \nsearch and can be a useful tool \nfor patient education on body \ncontouring \n23. Capelleras et al., \n2024; Turkey, Spain108 \nQualitative Plastic Surgery; GPT-3.5's \npotential in providing \npostoperative guidance during \nrhinoplasty recovery \nGPT-3.5; patients 8 standardized questions were formulated based on \nthe Rhinobase 2.0 database; qualitatively response \nassessment for recurring themes, patterns, and trends \nrelated to rhinoplasty recovery \nGPT-3.5's responses guide common concerns after \nrhinoplasty, including swelling, emotional \nadjustment, asymmetry, breathing difficulties, pain, \nskin color changes, bleeding, and numbness; GPT-\n3.5 emphasizes the importance of consulting a \nsurgeon for personalized medical advice \nGPT-3.5 has the potential to \nenhance patient education \nduring rhinoplasty recovery \nbut should not replace \npersonalized advice from \nqualified healthcare \nprofessionals \n24. Coskun et al., 2023; \nTurkey46 \nQuantitative \n  \nUrology; GPT-3.5's utility in \nproviding patient information on \nGPT-3.5; patients 59 questions were derived from the EAUPI website; \nresponse evaluation by 2 urologists for content \nGPT-3.5's responses were suboptimal in accuracy \nand quality, with an average F1 score: 0.426, \nCaution should be exercised \nwhen using GPT-3.5 for \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 20\nprostate cancer compared to the \nEuropean Association of Urology \nPatient Information (EAUPI) \naccuracy/similarity and quality using precision, recall, \nF1 score, cosine similarity, and the General Quality \nScore (GQS) \nprecision: 0.349, recall: 0.549, cosine similarity: \n0.609, and GQS: 3.62/i1 ±/i1 0.49; no answer \nachieved the maximum GQS of 5 \npatient information on prostate \ncancer \n25. Durairaj et al., 2023; \nUSA, Italy47 \nQuantitative \n \nPlastic Surgery; to compare GPT-\n3.5's performance in responding \nto patient questions on \nseptorhinoplasty to the responses \nfrom a rhinoplasty surgeon \nGPT-3.5; patients 6 hypothetical questions on septorhinoplasty were \ndesigned by the author; blinded responses of a board-\ncertified rhinoplasty surgeon and GPT-3.5 were \nevaluated by 7 rhinoplasty surgeons for empathy, \naccuracy, completeness, overall quality, preferred \nresponse \nGPT-3.5 outperformed the surgeon response in \naccuracy, completeness, and overall quality; \nempathy rating did not significantly differ; GPT-\n3.5 responses were preferred in 81% of cases \nGPT-3.5 has the potential to \nassist surgeons in educating \nand counseling patients on \nseptorhinoplasty \n26. Kianian et al., 2023; \nUSA48 \nQuantitative \n \nOphthalmology; GPT-3.5's and \nBard's ability to produce patient-\ntargeted health information on \nuveitis and to improve the \nreadability of online health \ninformation \nGPT-3.5, Bard; \npatients \n2 prompts for generating patient-focused health \ninformation about uveitis; 9 patient-focused uveitis \nweb page texts from the first Google page were asked \nto be rewritten for readability; responses were \nanalyzed for readability using the Flesch-Kincaid \nGrade Level (FKGL); appropriateness rated by 2 \nfellowship-trained uveitis specialists \nAppropriateness GPT-3.5: 100%/Bard: 88.9%; \nGPT-3.5 provided significantly more \ncomprehensible responses (mean FKGL: 6.3) \ncompared to Bard (mean FKGL: 10.5); online \nuveitis health information averaged a FKGL of \n11.0, GPT-3.5 had a mean FKGL of 8.0, Bard had \na mean FKGL of 11.1 \nGPT-3.5 outperforms Bard in \ngenerating/rewriting patient-\nfriendly health information on \nuveitis \n27. Seth et al., 2023; \nAustralia49 \nQuantitative \n \nHand Surgery; GPT-3.5's \nprecision and comprehensiveness \nof answers on the management of \ncarpal tunnel syndrome (CTS); to \nassess the safety of GPT-3.5's \nmedical advice \nGPT-3.5; \npatients/caregivers \n2 plastic surgeons developed 6 CTS questions and \nevaluated responses for accuracy, reliability, \ncomprehensiveness, and reference generation; \nsimulated doctor-patient interactions were employed \nto assess the safety of GPT-3.5's medical advice \nGPT-3.5 provided relevant but superficial \ninformation on CTS; references were considered \ninsufficient; during the simulated doctor-patient \ninteractions, GPT-3.5 suggested a diagnostic \npathway that differed from the widely accepted \nclinical consensus on CTS diagnosis \nGPT-3.5 has the potential to \nprovide general medical \ninformation to patients but \nrequires refinement, \nparticularly regarding accurate \nreferencing and depth of \ninformation \n28. Inojosa et al., 2023; \nGermany50 \nQuantitative \n \nNeurology; GPT-4's performance \nin communicating medical \ninformation relevant to multiple \nsclerosis (MS) to medical \nprofessionals and MS patients \nGPT-4; \npatients/caregivers \n64 clinical scenarios related to MS treatment were \nmanually created and used to generate one \nexplanation each for general practitioners and MS \npatients; response grading by 3 medical doctors \nspecialized in MS treatment for humanness, \naccuracy, reliability, writing quality; readability \nassessment using the Flesch-Kincaid Grade Level \n(FKGL) \nMedian humanness score (on a 5-point Likert \nscale): 5; median correctness score: 4.25; median \nrelevance score: 4; mean FKGL of 15.26 for \ngeneral practitioners' and 63.14 for MS patients' \ninformation \nGPT-4 shows promise for \ncommunicating medical \ninformation related to MS; \nvalidation and correction by \nexpert care providers are \nnecessary to ensure patient \nsafety  \n29. Lyons et al., 2023; \nUSA51 \nQuantitative \n \nOphthalmology; to evaluate the \ntriage performance of AI chatbots \nfor ophthalmic conditions \nGPT-4, Bing Chat; \npatients/caregivers \n44 clinical vignettes were developed based on a \nliterature review of common emergency room \nophthalmologic diagnoses; comparison of \nperformance with WebMD Symptom Checker and 8 \nophthalmology trainee respondents; response \nevaluation by 2 experts for accurate diagnosis listed \nin the top 3 possible diagnoses and correct triage \nurgency, grossly inaccurate statements, mean reading \ngrade level, mean response word count, proportion \nwith attribution, and most common sources cited \nOphthalmology trainees achieved the highest \ncorrect diagnosis rate (95%), followed by GPT-4 \n(93%), Bing Chat (77%), and WebMD Symptom \nChecker (33%); GPT-4 scored highest in triage \naccuracy (98%), followed by ophthalmology \ntrainees (86%) and Bing Chat (84%); gross \ninaccuracies were found in 0% of responses by \nGPT-4 and trainees, 14% by Bing Chat, and 50% \nby WebMD Symptom Checker \nGPT-4 offers high diagnostic \nand triage accuracy for \nophthalmic conditions \ncomparable to that of \nophthalmology trainees, \nsuggesting potential utility as a \ntriage tool in healthcare \nsettings \n30. Babayiğ it et al., \n2023; Turkey52 \nQuantitative \n \nPeriodontology; GPT-3.5's ability \nto answer the most frequently \nasked questions on different \ntopics in periodontology \nGPT-3.5; patients 70 most-frequently asked patient questions generated \nby GPT-3.5 on 7 different periodontology topics \ndetermined by periodontists; 20 periodontists were \ncontacted via email to evaluate the answers for \naccuracy and completeness \nMean accuracy score (7-point Likert scale): 5.5; \nmean completeness score (3-point Likert scale): \n2.34; statistically significant differences in \nperformance between subjects \nGPT-3.5 can be an \ninformational resource for \npatients and periodontists, but \nexpert supervision is needed to \naddress potential inaccuracies \n31. Mondal et al., 2023; \nIndia53 \nQuantitative \n \nDiscipline not specified; GPT-\n3.5's ability to answer patient \nquestions related to lifestyle-\nrelated diseases and disorders \nGPT-3.5; patients 20 fictional cases with 4 lifestyle-related disease \nquestions each were created; content validity checked \nby a public health expert; response evaluation for \naccuracy, guidance, sentiment analysis, readability, \nand content evaluation by two primary care \nphysicians \nAverage accuracy score (3-point assessment scale \nfrom 0 to 2): 1.83; average guidance score: 1.9; \nhigh Flesh-Kincaid Grade Level of 14.37 and \nFlesch Reading Ease Score of 27.8; responses were \nin a natural and positive tone \nGPT-3.5 provides accurate \nresponses and adequate \nguidance for lifestyle-related \nhealth diseases and disorders \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 21\n32. Kim et al., 2023; \nSouth Korea54 \nQuantitative \n \nNeurology; GPT-3.5's versus \nGPT-4's performance in \nproviding educational \ninformation on epilepsy  \nGPT-3.5/GPT-4; \npatients \n57 epilepsy questions were developed based on the \nKorean Epilepsy Society's 'Epilepsy Patient and \nCaregiver Guide' and prompted twice; response \nevaluation by 2 epileptologists for educational \nvalue/correctness \n70% of GPT-4's responses had sufficient \neducational value; 28% were correct but \ninadequate; no response was entirely incorrect; \nGPT-4 outperformed GPT-3.5 and was often on par \nor better than the actual guide \nGPT-4 can be a valuable tool \nin delivering reliable epilepsy-\nrelated information \n33. Song et al., 2023; \nChina55 \nQuantitative \n \nUrology; effectiveness of LLMs \nin providing medical \nconsultations and patient \neducation on urolithiasis \nBard, Claude, \nGPT-4, Bing Chat; \npatients/caregivers \n21 questions from online consultation platforms, \nsurveys conducted among hospitalized urolithiasis \npatients, and researchers' clinical experience; 2 case \nscenarios with different complexity; response \nevaluation by 3 urolithiasis experts for accuracy, ease \nof understanding, comprehensibility, human caring \nClaude consistently scored the highest in all \ndimensions; GPT-4 ranked second in accuracy, \nwith shortcomings in empathy and human caring; \nBard had the lowest accuracy and overall \nperformance \nClaude shows superior \nperformance compared to the \nother 3 LLMs in providing \nconsultations and education on \nurolithiasis \n34. Bitar et al., 2022; \nSaudi Arabia, USA24 \nQuantitative \n \nGynecology; to assess if BERT \ntext summarization increases \nwomen's knowledge about HPV \nBERT; patients 386 women aged ≥  20 years recruited via Amazon \nMechanical Turk were randomly assigned to 2 \ngroups: 1. BERT summarized text, 2. original text on \nHPV based on 3 publications; a 29-item questionnaire \nbased on Waller et al.'s HPV knowledge measure was \nused to assess participants' pre- and post-knowledge \nWomen who read the original texts were more \nlikely to correctly answer 2 questions on the \ngeneral HPV knowledge subscale and 1 question \non the HPV testing knowledge subscale; HPV \nvaccination knowledge did not significantly differ \nBERT text summarization \ncould be a valuable tool in \npublic health education, \nproviding a balance between \ninformation completeness and \nreader time efficiency \n35. Zalzal et al., 2023; \nUSA56 \nQuantitative \n \nOtolaryngology; GPT-3.5's utility \nfor answering otolaryngology-\nrelated questions from the lay \npublic \nGPT-3.5; patients 30 commonly asked questions by patients/families \nwere collected over 3 months by pediatric \notolaryngologists; response ratings by 2 board-\ncertified otolaryngologists and 13 lay public graders \nfor correctness or confidence of accuracy \nExperts: 98.3% of questions correct; non-experts: \n79.8% confidence in GPT-3.5's response accuracy \nGPT-3.5 can serve as a helpful \nmedical information tool; \nwhile physicians rate its \ninformation as accurate and \ncomprehensive, laypersons \nlack confidence in GPT-3.5 \n36. Chervenak et al., \n2023; USA57 \nQuantitative \n \nGynecology; GPT-3.5's \nperformance in responding to \nfertility-related questions \nGPT-3.5; patients 1. 17 infertility questions from the FAQ of the \nCenters for Disease Control (CDC), response \nassessment by 2 physicians for sentiment analysis, \nfactual statements, incorrectness, and references; 2. 2 \nvalidated fertility knowledge surveys, evaluation of \npercentiles compared to published population data; 3. \n7 statements from the American Society for \nReproductive Medicine Committee converted into \nquestions, assessed for identification of missing facts \n1. GPT-3.5 responses matched CDC's in length, \nfactual content, sentiment, and subjectivity; 6.12% \nof GPT-3.5's factual statements were incorrect, \nonly one (0.7%) provided a reference; 2. GPT-3.5 \nscored at the 87th percentile for the Cardiff Fertility \nKnowledge Scale and at the 95th percentile for the \nFertility and Infertility Treatment Knowledge \nScore; 3. all 7 missing facts for the summary \nstatements were reproduced \nGPT-3.5 produces relevant and \nmeaningful responses to \nfertility-related questions \ncomparable to established \nresources \n37. Bushuven et al., \n2023; Germany58 \nQuantitative \n \nEmergency Medicine; GPT-3.5's \nand GPT-4's performance in \nsupporting parents in Basic Life \nSupport (BLS) and Pediatric \nAdvanced Life Support (PALS) \nGPT-3.5/GPT-4; \npatients \n22 case vignettes describing prototypical BLS \n(n=2)/PALS emergencies (n=20), developed and \nvalidated by 5 emergency physicians and prompted 3 \ntimes; response evaluation for diagnostic accuracy, \nemergency call advice, and validity of advice \nGPT-3.5/GPT-4 accurately diagnosed the condition \nin 94% of cases, advised calling emergency \nservices in 54% of cases, provided correct first aid \ninstructions in 45% of cases, and incorrectly \nrecommended advanced life support techniques in \n13.6% of cases \nThe reliability and safety of \nGPT-3.5/GPT-4 as emergency \nsupport tools are questionable, \nbut they show potential for \naiding in diagnosing pediatric \nemergencies \n38. Jeblick et al., 2023; \nGermany59 \nQuantitative Radiology; to assess the quality \nof GPT-3.5 in generating \nsimplified radiology reports \nGPT-3.5; patients Three fictitious radiology reports were created by a \nradiologist and simplified by GPT-3.5 15 times each; \nquality assessment by 15 radiologists for actual \ncorrectness, completeness, and potential harm \nFactual correctness: 75% \"Agree/Strongly agree\"; \nincorrect passages in 51% of reports; missing \nrelevant information in 22% of reports; potentially \nharmful content in 36% of reports \nGPT-3.5 shows potential in \nsimplifying radiology reports \nbut needs refinement to ensure \naccuracy and prevent harm \n39. Samaan et al.; 2023; \nUSA60 \nQuantitative \n \nGeneral Surgery; GPT-3.5's \naccuracy and reproducibility in \nanswering patient questions \nabout bariatric surgery \nGPT-3.5; patients 151 questions from professional societies, health \ninstitutions, and Facebook support groups prompted \ntwice each; response grading for accuracy and \nreproducibility by 2 board-certified bariatric surgeons  \nComprehensive: 86.8% of responses; reproducible: \n90.7% of responses \nGPT-3.5 is a useful \ninformation resource for \npatients about bariatric \nsurgery, but it should \ncomplement, not replace, \nstandard care from healthcare \nprofessionals \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 22\n40. Zhou et al.; 2023; \nGermany, India, Spain61 \nQuantitative \n \nGastroenterology; GPT-3.5's and \nGPT-4's potential in \ndisseminating gastric cancer \nknowledge, providing \nconsultation recommendations, \nand interpreting gastroscopy \nreports \nGPT-3.5/GPT-4; \npatients/caregivers \n23 gastric cancer questions prompted 3 times, \nevaluation of appropriateness and consistency of \nresponses; case materials from the Chinese Medical \nCase Repository, Journal of Medical Case Reports, \nand F1000 Research prompted 3 times to assess \nconsultation recommendations and endoscopy report \nanalysis for abnormalities and consistency \nGPT-4 outperformed GPT-3.5 in all tasks; gastric \ncancer questions: 91.3% appropriate, 95.7% \nconsistent (GPT-3.5: 78.3%/82.6%); consultation \nrecommendations: 80.4% appropriate, 82.6% \nconsistent (GPT-3.5: 69.6%/73.9%); endoscopy \nreport analysis: 69.6% appropriate, 65.2% \nconsistent (GPT-3.5: 56.5%/58.7%) \nGPT-4 shows potential in \ndisseminating medical \nknowledge and assisting in \nmedical consultation but \nshould not be a substitute for \nprofessional medical advice \n41. Oniani et al., 2023; \nUSA62 \nQuantitative \n \nDiscipline not specified; \nEffectiveness of neural machine \ntranslation (NMT) models in \ntranslating health illiterate \nlanguage in patient education \nmaterials \nBERT, BioBERT, \nBioClinicalBERT; \npatients \nA corpus for training NMT models was created using \ndata on patient education materials from \nMedlinePlus.gov, Drugs.com, Mayoclinic.org, and \nReddit.com; conversion into illiterate language using \nthe CDC Plain Language Thesaurus; evaluation of \nNMT models using BLEU score for translation \nquality \nBiLSTM outperformed LLMs with a mean BLEU \nscore of 41.578, followed by BERT: 33.582, \nBioBERT: 33.278, and BioClinicalBERT: 31.191 \nNMT models show \neffectiveness in translating \nhealth illiterate language into \npatient education materials \n42. Hernandez et al., \n2023; Barbados, USA63 \nQuantitative \n \nEndocrinology; GPT-3.5's \ncorrectness and consistency in \nresponding to questions on type 2 \ndiabetes mellitus (T2DM) and \nassociated complications \nGPT-3.5; patients 70 questions about T2DM and its complications were \ndeveloped by physicians and evaluated by research \nstaff; prompted 3 times each; response evaluation by \n2 internal medicine board-certified physicians \n98.5% of responses were appropriate; 1.4% of \nresponses were inappropriate but still met minimal \nstandards of care \nGPT-3.5 demonstrates \npotential as a supplementary \ntool for diabetes education \n43. Kuş cu et al., 2023; \nTurkey, Iran64 \nQuantitative \n \nOtolaryngology; GPT-4's \naccuracy and reliability in \nresponding to head and neck \ncancer (HNC) questions \nGPT-4; \npatients/caregivers \n154 HNC questions from professional societies, \ninstitutions, patient support groups, and social media; \nresponse grading by 2 head and neck surgeons for \naccuracy and reproducibility \nComprehensive/correct: 86.4% of responses; \nreproducible: 94.1% of responses; no responses \ncompletely inaccurate/irrelevant \nGPT-4 has the potential to \nserve as a valuable information \nsource on HNC for patients \nand healthcare professionals  \n44. Biswas et al., 2023; \nUK65 \nQuantitative \n \nOphthalmology; GPT-3.5's \naccuracy and information quality \nin answering myopia questions \nGPT-3.5; patients 11 questions on myopia were constructed based on \nthe \"frequently asked questions on myopia\" webpage \nof the Association of British Dispensing Opticians; \nprompted 5 times each; response evaluation by 5-\nmembers of the optometry teaching and research staff \nfor accuracy and response quality \n24% of questions were rated as very good, 49% as \ngood, 22% as acceptable, 3.6% as poor, and 1.8% \nas very poor; information quality was good for \n90.9% of questions and acceptable for one question \n(9.1%) \nGPT-3.5 shows potential in \nproviding information on \nmyopia, but limitations and \ninaccuracies need to be \naddressed before its \nimplementation in clinical \nsettings \n45. Chiesa-Estomba et \nal., 2023; Spain, Austria, \nBelgium, France, Italy\n66 \nQuantitative Otorhinolaryngology; to assess \nthe level of agreement between \nGPT-3.5 and expert \nsialendoscopists (EESS) in \nclinical decision-making and \npatient information support for \nthe management of salivary gland \ndisorders \nGPT-3.5; patients 6 questions based on the most common clinical \nscenarios in 3 sialendoscopy clinics; 10 EESS \nresponded via e-questionnaire and were compared \nagainst GPT-3-5 by another set of 10 EESS, assessing \nthe level of agreement \nMean agreement score (5-point Likert scale): GPT-\n3.5: 3.4, ES: 4.1; mean therapeutic alternatives \nnumber: GPT-3.5: 3.3, ES: 2.6 \nGPT-3.5 is a promising tool in \nthe clinical decision-making \nprocess within the salivary \ngland clinic \n46. Decker et al., 2023; \nUSA67 \nQuantitative General Surgery; to compare \nGPT-3.5's readability, accuracy, \nand completeness with surgeon-\ngenerated information on the \nrisks, benefits, and alternatives \n(RBAs) of common surgical \nprocedures \nGPT-3.5; \npatients/caregivers \n6 RBAs for common surgical procedures were \ngenerated using GPT-3.5 by a multidisciplinary group \nof surgeons and compared against 5 surgeon-\ngenerated RBAs for each of the 6 surgical procedures \nfor readability, accuracy, and completeness using a \nrubric with recommendations from LeapFrog, the \nJoint Commission, and the American College of \nSurgeons by at least 2 blinded reviewers \nMean composite scores for completeness and \naccuracy: GPT-3.5: 2.2, surgeons: 1.6; mean \nreadability scores: GPT-3.5: 12.9, surgeons: 15.7  \nGPT-3.5 has the potential to \nenhance informed consent \ndocumentation by providing \nmore readable, accurate, and \ncomplete information \ncompared to surgeon-generated \ncontent \n47. Kaarre et al., 2023; \nUSA, Sweden68 \nQuantitative Orthopedics; GPT-4's usefulness \nin answering questions by \npatients and non-orthopedic \nmedical doctors on anterior \ncruciate ligament (ACL) surgery \nGPT-4; patients 20 questions on ACL surgery developed based on a \nliterature search and frequently asked patient \nquestions for patients and non-orthopedic doctors; \nresponse evaluation by 4 orthopedic sports medicine \nsurgeons for correctness, completeness, and \nadaptiveness \nGPT-4 patient responses fully or majority correct: \n65%; mean correctness score (2-point scale from 0 \nto 2): patients: 1.69, surgeons: 1.66; mean \ncompleteness score: patients: 1.51, surgeons: 1.64; \nmean adaptiveness score: patients: 1.75, surgeons: \n1.73 \nGPT-4 has the potential as a \nsupplementary tool for patient \neducation on ACL surgery but \ncannot replace the expertise of \northopedic sports medicine \nsurgeons \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 23\n48. Ferreira et al., 2023; \nUSA69 \nQuantitative Dermatology; GPT-4's \nappropriateness in responding to \ncommon questions by \ndermatology patients \nGPT-4; patients 31 questions on 6 common skin conditions and \nqueries were created by 3 dermatologists based on \ntheir experience and a literature review and prompted \n3 times; response grading as \"appropriate\" or \n\"inappropriate\" by 3 dermatologists \n88% of responses were appropriate; 12% of \nresponses were inappropriate; 16.1% of responses \nhad an inappropriate response average, with a \nminimum of 2 dermatologists rating 2 out of 3 \nresponses as inappropriate \nGPT-4 shows potential as a \npublic dermatology resource, \nbut it should not replace \nprofessional medical advice \nand remain a supplementary \ninformational tool \n49. Truhn et al., 2023; \nGermany70 \nQuantitative Radiology, Orthopedics; GPT-4's \nvalidity of patient treatment \nrecommendations for common \nknee and shoulder orthopedic \nconditions using clinical MRI \nreports \nGPT-4; patients 20 anonymized reports out of 94 knee and 38 \nshoulder MRI studies were selected by a \nmusculoskeletal radiologist and prompted twice for \nGerman-English translation and the provision of \ntreatment recommendations; response evaluation by 2 \northopedic surgeons for overall quality, scientific and \nclinical basis, and clinical usefulness and relevance \nQuality of treatment recommendations was rated as \ngood (10%), very good (60%), or excellent (30%) \nfor the knee and shoulder; recommendations were \nmainly up-to-date and consistent, adhering to \nclinical and scientific evidence; no signs of \nhallucinations or nonsensical responses, but a \ntendency to provide generic and unspecific answers \nGPT-4 shows promise in \noffering accurate and clinically \nrelevant treatment \nrecommendations for \northopedic knee and shoulder \nissues but should not replace \nconsultations with specialists \nfor treatment advice \n50. Hurley et al., 2023; \nUSA71 \nQuantitative Orthopedics; GPT-3.5's quality \nand readability of information \nregarding shoulder stabilization \nsurgery \nGPT-3.5; patients 23 patient questions on shoulder stabilization surgery \nwere developed based on prior studies; response \nevaluation by 3 residents for quality (DISCERN \nscore, JAMA benchmark criteria) and readability \n(Flesch-Kincaid Reading Ease Score (FRES) and \ngrade level (FKGL)) \nJAMA benchmark criteria score: 0 (no reference \ncited); DISCERN score: 60 (considered good); \nFRES: 26.2; FKGL of a college graduate \nGPT-3.5 has the potential to \nprovide high-quality answers \nto questions relating to \nshoulder stabilization surgery, \nbut it is unclear where the \nanswers originated \n51. Cankurtaran et al., \n2023; Turkey72 \nQuantitative Gastroenterology; GPT-3.5's \nperformance in answering \ninflammatory bowel disease \n(IBD) questions for patients and \nhealthcare professionals \nGPT-3.5; \npatients/caregivers \n5 patient questions each on Crohn's disease and \nColitis Ulcerosa based on Google Trends; 5 questions \neach generated by 4 gastroenterologists; response \nevaluation for reliability and usefulness by 2 \ngastroenterologists \nMean patient reliability/usefulness scores (7-point \nLikert scale): professional sources 5/5.2, patient-\nderived responses: 4/4.35 \nGPT-3.5 exhibits partial \nreliability and usefulness in the \ncontext of IBD but has \nlimitations and deficiencies \n52. Birkun et al., 2023; \nRussia, India73 \nQuantitative Emergency Medicine; to evaluate \nthe performance of LLMs in \nproviding guideline-consistent \nadvice on help to a non-breathing \nvictim in emergencies \nBard, Bing Chat; \npatients \nBing Chat and Bard were prompted 20 times each \nwith the query \"What to do if someone is not \nbreathing?\"; original and self-corrected response rated \nby 2 authors for compliance with the Resuscitation \nCouncil UK Guidelines \nLLM's responses lacked guideline-consistent \ninstructions for helping a non-breathing victim; \nBing Chat: 9.5% compliance; Bard: 11.4% \ncompliance; LLMs overestimated the quality of \ntheir response compared to expert ratings (10-point \nLikert scale); Bing Chat: 7 points; Bard: 9.0 points; \nLLMs denied containing guidelines-inconsistent \ninstructions \nBing Chat and Bard provide \nunderstandable but unreliable \nresuscitation information, \nlacking essential details and \noccasionally including harmful \ndirectives \n53. Pushpanathan et al., \n2023; Singapore, China74 \nQuantitative Ophthalmology; proficiency of \nLLMs in addressing queries \nrelated to ocular symptoms \nGPT-3.5/GPT-4, \nBard; patients \n37 questions on ocular symptoms were developed by \na team of 5 ophthalmologists and clinical optometrists \nconsidering common online health information sites; \nrandom presentation to 3 ophthalmologists and \nevaluation for accuracy, comprehensiveness, \nevaluation of self-awareness levels through prompts \nfor self-correction; qualitative analysis of poorly rated \nresponses by two ophthalmologists \nGPT-4 exhibited higher average total accuracy (8.2 \nof 9) compared to GPT-3.5 (7.5) and Google Bard \n(7); comprehensiveness was good without \nsignificant differences; GPT-3.5 issued a general \ndisclaimer when prompted to self-check, \nemphasizing the need for additional personal \nmedical information; GPT-4 and Google Bard \nconsistently asserted the accuracy of their original \nresponses, even when deemed as 'poor' or \n'borderline' \nGPT-4 demonstrates superior \nperformance in addressing \nophthalmologic queries, \nhighlighting its utility in \nproviding accurate and \ncomprehensive responses 54. Shao et al., 2023; \nChina75 \nQuantitative Thoracic Surgery; GPT-3.5's \nappropriateness and \ncomprehensiveness for \nperioperative patient education in \nthoracic surgery in English and \nChinese contexts \nGPT-3.5; patients 37 questions focused on perioperative thoracic \nsurgery patient education based on guideline-based \ntopics and personal experience prompted in English \nand Chinese; response evaluation by 35 reviewers \nwith thoracic surgery experience for appropriateness \nand comprehensiveness \n92% of responses were rated as \"qualified\" both in \nEnglish and Chinese contexts, 8% of responses in \nboth languages were rated as \"unqualified\" \nGPT-3.5 shows promise in \nproviding appropriate and \ncomprehensive information, \nwhich could enhance patient \neducation and clinical service \nquality in thoracic surgery \n55. Vaira et al., 2023; \nItaly, Belgium, Spain, \nFrance76 \nQuantitative Head and Neck Surgery; GPT-4's \naccuracy in answering questions \nand solving clinical scenarios \nrelated to head and neck surgery \nGPT-4; \npatients/caregivers \n144 questions (50% each open-ended/binary) and 15 \nclinical scenarios developed by 18 head and neck \nsurgeons from 14 Italian centers; response evaluation \nfor accuracy and completeness by the same 18 \nsurgeons; reference assessment by 2 reviewers; \ncomparison of performance with a resident surgeon \n84.7% correct response rate for closed-ended \nquestions; median accuracy score of 6 (of 6) for \nopen-ended questions; median completeness score \nof 3 (of 3) for open-ended questions; fully or nearly \nfully correct diagnoses in 81.7% of clinical \nscenarios; complete diagnostic or therapeutic \nGPT-4 demonstrates a good \nlevel of accuracy in responding \nto head and neck surgery but \nshould not be considered a \nreliable support for decision-\nmaking in clinical settings \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 24\nby 3 reviewers procedures proposed in 56.7% of cases; the \nresident significantly outperformed GPT-4 in all \ndomains \n56. Chen et al., 2023; \nUSA77 \nQuantitative Oncology; GPT-3.5's \nperformance in providing cancer \ntreatment recommendations \nconcordant with National \nComprehensive Cancer Network \n(NCCN) guidelines \nGPT-3.5; patients 4 zero-shot prompts for treatment recommendations \nfor each of 26 cancer diagnosis descriptions; 3 board-\ncertified oncologists assessed concordance with the \n2021 NCCN guideline based on 5 scoring criteria \n100% of outputs included at least one NCCN-\nconcordant treatment; 34.3% of outputs also \nrecommended one or more non-concordant \ntreatments; hallucinated treatments in 13 of 104 \n(12.5%) outputs \nOne-third of GPT-3.5's \ntreatment recommendations \nwere at least partially non-\nconcordant with NCCN \nguidelines; therefore, clinicians \nshould advise patients that \nchatbots are not reliable \nsources of treatment \ninformation \n57. Bellinger et al., \n2023; USA78 \nQuantitative Otolaryngology; GPT-3.5's \nperformance in responding to \nquestions on Benign Paroxysmal \nPositional Vertigo (BPPV) \ncompared to Google webpages \nGPT-3.5; patients 5 questions based on the top 30 Google search results \nfor BBPV prompted 3 times; response assessment and \ncomparison to Google Websites addressing these \nquestions by 2 first-year medical student reviewers \nfor readability (Flesch/i1 Kincaid Grade Level (FKGL) \nand Flesch Reading Ease (FRE)), quality (DISCERN \nscore), and understandability and actionability \n(PEMAT-P); accuracy and currency evaluation by 2 \nphysicians \nExperts rated GPT-3.5 information as accurate \n(mean score 4.2 of 5) and current (mean score 4.3); \nGPT-3.5 had higher FKGL scores (mean 13.9) \ncompared to Google search results (mean 10.7); \nGoogle webpages had higher total DISCERN \nscores (mean 56.5) compared to GPT-3.5 \nindividual responses (mean 17.5); Google \nwebpages scored higher in understandability \n(82.3%) compared to GPT-3.5 individual responses \n(72.3%) \nGPT-3.5 provides accessible \ninformation but is generally of \nlower quality, readability, \nunderstandability, and \nactionability compared to \nGoogle webpage resources \n58. Nielsen et al., 2023; \nDenmark79 \nQuantitative Otorhinolaryngology; GPT-4's \naccuracy in providing relevant \nmedical information on \notolaryngology (ORL) questions \nGPT-4; patients 27 questions on 9 ORL conditions; response \nevaluation by 13 physicians from a tertiary ORL \ndepartment for accuracy, relevance, and depth of \nresponses; GPT-4 was asked to evaluate the responses \non the same metrics as an otolaryngologist \nMean score of 3.4 (of 5) for accuracy, relevance, \nand depth; highest rating for relevance (mean score \nof 3.7); lowest rating for depth (mean score of 3); \nself-assessment rating of GPT-4 was 5 for all \ncategories \n \nGPT-4 demonstrates promise \nin providing relevant and \naccurate medical information \nbut requires enhancements in \nresponse depth and mitigating \npotential biases \n59. Sezgin et al., 2023; \nUSA80 \nQuantitative Gynecology; LLMs' quality of \nresponses compared to Google \nSearch results in addressing \nquestions about postpartum \ndepression (PPD)  \nGPT-4, Bard; \npatients \n14 PPD-related patient questions from the American \nCollege of Obstetricians and Gynecologists; response \nevaluation by 2 board-certified physicians using the \nGRADE-informed scale \nMean GRADE score for GPT-4: 3.9; significant \nhigher quality compared to Bard (Z=2.143; \nadjusted P=.048) and Google Search (Z=3.464; \nadjusted P<.001) \nGPT-4 and Bard exhibit \npotential in delivering \nclinically accurate responses \nthat are of higher quality \ncompared to those obtained \nfrom Google Search \n60. Floyd et al., 2023; \nUSA81 \nQuantitative Radiation Oncology; GPT-\n3.5's/GPT-4's accuracy and \ncomprehensiveness in answering \npatient questions related to \nradiation oncology \nGPT-3.5/GPT-4; \npatients/caregivers \n252 patient-centered questions, including 28 \ntemplates applied to 9 cancer types based on the \nPatient Concerns Inventory for Head and Neck \nCancer and the National Cancer Institute's website \n\"Questions to Ask Your Doctor About Treatment\"; \nevaluation by 2 independent radiation oncology \nresidents for accuracy and comprehensiveness \n34.1% of answers contained inaccurate \ninformation, 26.2% contained correct information \nbut missed essential context, 39.7% of responses \nwere correct and comprehensive; among inaccurate \nresponses, 71% were categorized as potentially \nappropriate within a context other than the prompt, \nand 29% were deemed inaccurate in any clinical \ncontext; GPT-4 performed similarly on a subset of \nquestions \nGPT-3.5/4 fail to consistently \ngenerate accurate and \ncomprehensive responses to \nthe majority of radiation \noncology patient-centered \nquestions \n61. Uz et al., 2023; \nTurkey82 \nQuantitative Rheumatology; GPT-3.5's \nreliability and usefulness in \nproviding information about \ncommon rheumatic diseases \nGPT-3.5; \npatients/caregivers \n7 common rheumatic diseases were identified using \nthe American College of Rheumatology and \nEuropean League against Rheumatism guidelines; \nGoogle Trends was used to determine the 4 most \nfrequently searched keywords for each disease; \nkeywords were used by different users in an ongoing \nchat; response evaluation for reliability and \nusefulness by 2 physical medicine and rehabilitation \nspecialists \nHighest reliability score for osteoarthritis (mean: \n5.62 of 7); highest usefulness score for ankylosing \nspondylitis (mean: 5.87 of 7); no significant \ndifference between the subjects in terms of \nreliability and usefulness \nAlthough GPT-3.5 is reliable \nand useful for patients, it may \ncontain false answers \n62. Athavale et al., 2023; \nUSA83 \nQuantitative Vascular Surgery; to assess the \npotential of chatbots in answering \nchronic venous disease patient \nGPT-3.5/GPT-4, \nLLaMA (Clinical \nCamel); \n2 questionnaires (1 for non-complex \nmedical/administrative matters, 1 for complex \nmedical chronic venous disease questions based on \nNon-complex medical questions: GPT-4: 100% \nappropriate and complete, GPT-3.5: 70%; complex \nmedical questions: GPT-4: 75% appropriate and \nGPT-4 demonstrates potential \nin responding to \nadministrative/non-complex \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 25\nquestions and electronic health \nrecord inbox management \npatients/caregivers patient messages) consisting of 20 questions each; \ncomplex medical questions were also posed to \nClinical Camel; response grading by 2 physicians for \nappropriateness and completeness \ncomplete, GPT-3.5: 45%; Clinical Camel: 0% \nappropriate and complete, 25% appropriate but \nincomplete, 25% neither appropriate nor \ninappropriate, 50% wrong \nmedical and complex medical \nquestions related to chronic \nvenous disease \n63. Li et al., 2023; USA, \nChina84 \nQuantitative Discipline not specified; to create \na specialized LLM with enhanced \naccuracy in medical advice \nGPT-3.5, LLaMA \n(ChatDoctor); \npatients/caregivers \nDataset curation using roughly 100,000 interactions \nfrom HealthCareMagic and 10,000 conversations \nfrom iCliniq; MedlinePlus/Wikipedia as an external \nknowledge brain; performance evaluation by testing \nvarious contemporary medical queries; BERTScore \nwas employed to compute precision, recall, and F1 \nscores for ChatDoctor and GPT-3.5 on questions from \nthe iCliniq database, with responses from physicians \nused as benchmark \nChatDoctor significantly outperformed GPT-3.5 in \nprecision, recall, and F1 scores and provided more \nsufficient and reliable answers on novel diseases \nand drugs \n  \n \nChatDoctor has the potential to \nimprove accuracy and \nefficiency in medical \ndiagnosis, reducing the \nworkload for medical \nprofessionals \n \n64. Seth et al., 2023; \nAustralia85 \nQuantitative Plastic Surgery; to assess the \nefficacy of employing LLMs in \nobtaining and synthesizing \ninformation about rhinoplasty \nGPT-3.5, Bard, \nBing Chat; \npatients \n6 questions on rhinoplasty were developed by 3 \nboard-certified plastic surgeons and prompted to \nGPT-3.5, Bard, and Bing Chat; response evaluation \nby comparing them with current healthcare guidelines \nfor rhinoplasty and through evaluation by the panel of \nplastic surgeons; readability assessment using the \nFlesch Reading Ease Score (FRES), Flesch–Kincaid \nGrade Level (FKGL), and Coleman–Liau Index \n(CLI); suitability assessment with modified \nDISCERN score \nBard and GPT-3.5 showed a significantly higher \nmean FRES than Bing Chat: Bard: 47.47, GPT-3.5: \n37.68, Bing Chat: 18.29; Bard and GPT-3.5 had a \nlower mean FKGS: Bard: 9.7, GPT-3.5: 10.15, \nBing Chat: 18.25; Bard and GPT-3.5 exhibited a \nlower CLI: Bard: 10.83, GPT-3.5: 12.17, Bing \nChat: 12.00; Bard had the highest DISCERN score: \nBard: 46.33, GPT-3.5: 42.17, Bing Chat: 35.75 \nThe use of LLMs such as GPT-\n3.5, Bard, and Bing Chat to \nobtain detailed information \nabout specific surgical \nprocedures such as rhinoplasty \ndemonstrate potential, but \nchallenges regarding their \ndepth and specificity remain.  \n65. Kuckelman et al., \n2024; USA110 \nQuantitative Radiology; Bing Chat's accuracy \nin providing patient education for \ncommon radiologic exams \nBing Chat; \npatients \n10 questions each for MRI Spine, CT abdomen, and \nbone biopsy were developed by the authors and Bing \nChat itself (50% each); prompted twice using 3 \ndifferent chatbot settings; response grading for \naccuracy and completeness by 2 independent \nreviewers compared to radiologyinfo.org \n93% of responses \"entirely correct\", 7% \"mostly \ncorrect\"; 65% of responses \"complete\", 35% of \nresponses \"mostly complete\" \nBing Chat offers precise \nresponses regarding radiology \nexams and procedures, \nindicating its potential to \nenhance the patient experience \nin radiology \n66. Lockie et al., 2023; \nAustralia86 \nQuantitative General Surgery; to evaluate a \nGPT-3.5 generated patient \ninformation leaflet (PIL) against \na surgeon-generated version \nGPT-3.5; patients 28 patients undergoing laparoscopic cholecystectomy \nand 16 doctors from 2 hospitals in Melbourne were \nasked to complete a survey based on a validated \nevaluation instrument for PIL; comparison of the PIL \nabout laparoscopic cholecystectomy generated by \nGPT-3.5 versus one developed by surgeons \nPatients scored GPT-3.5 and surgeon-generated \nPILs similarly (both median: 8 (of 8)); doctors \nrated GPT-3.5's PIL slightly higher (median: 7 for \nGPT-3.5 versus 6 for surgeons) \nGPT-3.5's-generated PIL was \ncomparable to or slightly better \nthan the surgeon-generated \nversion, indicating the \nfeasibility of using LLMs for \nPIL creation \n67. Haver et al., 2023; \nUSA87 \nQuantitative Radiation Oncology; to evaluate \nthe effectiveness of LLMs in \nsimplifying responses to patient \nquestions on lung cancer and \nlung cancer screening (LCS) to \nimprove readability and clinical \nappropriateness \nGPT-3.5/GPT-4, \nBard; patients \n19 questions, each prompted 3 times, about lung \ncancer and LCS were posed to GPT-3.5 to generate \nbaseline responses; assessment for readability and \naccuracy by 3 fellowship-trained cardiothoracic \nradiologists; simplified baseline responses evaluated \nby the same radiologists \nBaseline: GPT-3.5 Flesch reading ease score: 49.7, \nFlesch-Kincaid readability grade: 12.6; Simplified: \nGPT-3.5 reading ease: 62, readability grade: 10, \nGPT-4 reading ease: 68, readability grade: 9.6, \nBard reading ease: 71, readability grade: 8.2; \nresponses were deemed clinically accurate in 84% \n(GPT-3.5), 79% (GPT-4), and 95% (Bard) \nGPT-3.5, GPT-4, and Bard \ndemonstrate potential in \ngenerating and simplifying \nresponses to patient questions \non lung cancer and LCS \n68. Li et al., 2023; \nUSA88 \nQuantitative Radiology; GPT-3.5's \npotential in simplifying radiology \nreports to the reading level of the \naverage United States adult \nGPT-3.5; patients 400 deidentified radiology reports (100 each for X-\nray (XR), ultrasound (US), MRI, CT) were randomly \nselected from the institution's database and prompted \nfor simplification; response evaluation for report \nlength, Flesch reading ease score (FRES), and Flesch-\nKincaid reading level (FKRL) \nFollowing simplification, all reports had an FKRL \n<8.5, with a mean increase in FRES of 46 points \nand a mean decrease in FKRL by 5-grade levels to \nan average of <6\nth-grade level \nGPT-3.5 effectively simplifies \nradiology reports to the reading \nlevel of the average United \nStates adult, but further \nevaluation of accuracy and \nimpact on patient \ncomprehension is needed \n69. Scheschenja et al., \n2023; Germany89 \nQuantitative Radiology; GPT-3.5's and GPT-\n4's accuracy in providing patient \neducation regarding specific \ninterventional radiology (IR) \nprocedures \nGPT-3.5/GPT-4; \npatients \n133 questions on port implantation, percutaneous \ntransluminal angioplasty, and transarterial \nchemoembolization procedures developed by 2 \nradiology residents and validated by a third \nradiologist; response evaluation by 2 radiologists for \naccuracy \nGPT-3.5: 30.8% \"completely correct\", 48.1% \"very \ngood\"; GPT-4: 35.3% \"completely correct\", 47.4% \n\"very good\"; GPT-4 was significantly more \naccurate than GPT-3.5; no responses were \nidentified as potentially harmful \nGPT-3.5 and GPT-4 show \npotential for safe and relatively \naccurate in-depth patient \neducation in IR procedures \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 26\n70. Gordon et al., 2023; \nUSA90 \nQuantitative Radiology; GPT-4's  accuracy, \nrelevance, and readability in \nanswering patients' imaging-\nrelated questions and evaluating \nthe influence of a patient-directed \nprompt on these parameters \nGPT-4; patients 22 questions developed based on the expertise of 4 \nradiologists and existing literature prompted 3 times \nby using an unstructured and structured prompt; \nresponse evaluation for accuracy, relevance, \nconsistency, and readability (Flesch–Kincaid Grade \nLevel (FKGL)) by the 4 radiologists and patients \nAccuracy: 83% unprompted, 87% structured \nprompt; relevance: 98.5% unprompted, 98.8% \nstructured prompt; FKGL: 13.6 unprompted, 13.0 \nstructured prompt; consistency: 72% unprompted, \n86% structured prompt; patient utility assessment: \n92%-97% responses deemed as relevant and \nhelpful by patients \nGPT-4 has the potential to \nprovide accurate and relevant \nanswers to patient-centered \nimaging questions but is \ncautioned against immediate \nclinical implementation due to \nimperfections in accuracy, \nconsistency, and readability. \n71. Stroop et al., 2023; \nGermany91  \nQuantitative Neurosurgery; GPT-3.5's \naccuracy in providing medical \ninformation on acute lumbar disc \nherniation (LDH) \nGPT-3.5; patients 52 spinal surgeons completed an online survey, \nimagining themselves as patients with acute LDH, \nand interacted with GPT-3.5 for information; quality \nevaluation of responses based on predefined \ncategories by the spinal surgeons; responses were \ncompared to a standardized informed consent sheet \nfor LDH \n97% of GPT-3.5's responses understandable; 55% \nof responses medically comprehensive; GPT-3.5 \ncovered 48% informed consent form information \nfor LDH \nGPT-3.5 shows potential in \nsupporting medical \ncommunication with patients \nby providing understandable \nresponses related to LDH \n72. Coraci et al., 2023; \nItaly, Bulgaria92 \nQuantitative Orthopedics; effectiveness of a \nGPT-3.5-generated questionnaire \nfor assessing low back pain \n(LBP) compared to routinely \nused and validated questionnaires \nGPT-3.5; patients GPT-3.5 was prompted to generate a questionnaire \nfor the assessment of the LBP (ChatQ) that consisted \nof 10 questions in Italian; ChatQ was administered to \n20 Italian-speaking patients with a history of LBP for \nself-compilation compared to the Numeric Pain \nRating Scale (NRS) and 3 validated questionnaires \nfor back pain: Oswestry Disability Index (ODI), \nQuebec Back Pain Disability Scale (QBPDS), and \nRoland-Morris Disability Questionnaire (RMDQ) \nChatQ: median score: 8/17; ODI: median score: \n12%; QBPDS: median score: 9/100; RMDQ: \nmedian score: 3/24; NRS: median score: 4/10; \nstrong correlation between ODI and ChatQ, \nmoderate correlation between QBPDS and ChatQ, \nno statistical correlation between ChatQ and \nRMDQ or NRS \nChatQ, generated by GPT-3.5, \noffers potential benefits for the \nassessment of LBP but cannot \nreplace established validated \nquestionnaires \n73. Ye et al., 2023; \nCanada, Germany93 \nQuantitative Rheumatology; GPT-4's \nquality of responses compared to \nthose of rheumatologists for real \nrheumatology patient questions \nGPT-4; patients 30 rheumatology questions and physician-generated \nanswers were extracted from the Alberta \nRheumatology website; 17 rheumatology patients and \n4 rheumatologists rated GPT-4-generated responses \nand physician-generated responses for \ncomprehensiveness, readability, and overall \npreference \nPatient ratings: GPT-4: mean comprehensiveness \n(10-point Likert scale): 7.12, readability score: 7.9, \nphysician-generated responses: mean \ncomprehensiveness: 8.76, readability score: 8.75 \n(no significant differences); rheumatologists' \nratings: GPT-4 responses were rated significantly \nlower for comprehensiveness (5.52 versus 8.76), \nreadability (7.85 versus 8.75), and accuracy (6.48 \nversus 9.08) compared to physician responses \nAlthough rheumatology \npatients rated GPT-4-generated \nresponses similarly to \nphysician-generated responses, \nrheumatologists found GPT-4-\ngenerated responses to be \ninferior, especially in terms of \naccuracy \n74. Mohammad-Rahimi \net al., 2023; Germany, \nIran, USA, UK\n94 \nQuantitative Dentistry; validity and reliability \nof responses by LLMs on \nfrequently asked questions in the \nfield of endodontics \nGPT-3.5, Bard, \nBing Chat; \npatients \n20 questions on endodontics were selected, thereof 10 \nquestions developed by 2 endodontists based on their \nclinical experience and 10 questions provided by \nGPT-3.5; each prompted 3 times; response evaluation \nby 2 endodontists using the modified Global Quality \nScore (GQS) for correctness and content in addition \nto validity and reliability \nLow-threshold validity: GPT-3.5 had the highest \nvalidity with 95% valid responses, followed by \nBard (85%) and Bing Chat (75%); high-threshold \nvalidity: GPT-3.5 had the highest validity with \n60% valid responses, Bard and Bing Chat: 15%; \nBing Chat had the highest consistency (Cronbach's \nalpha: 0.955), followed by GPT-3.5 and Bard \n(Cronbach's alpha: 0.746 and 0.703) \nWhile LLMs show potential as \npublic sources for endodontic \ninformation, there are areas for \nimprovement in terms of \nvalidity, reliability, and the \npotential for misinformation \n75. Scquizzato et al., \n2024; Italy, UK109 \nQuantitative Emergency Medicine; GPT-3.5's \naccuracy, relevance, and \ncomprehensiveness in answering \nquestions on cardiac arrest, \ncardiopulmonary resuscitation \n(CPR), and post-resuscitation \nrecovery \nGPT-3.5; patients 40 questions on different aspects of cardiac arrest \nwere obtained from websites of institutions, scientific \nsocieties, and organizations; response evaluation by 8 \ndoctors, 5 nurses, one psychologist, and 16 laypeople \nfor relevance, clarity, comprehensiveness, overall \nvalue, and readibility \nOverall positive evaluation by professionals and \nlaypeople (5-point Likert scale): 4.3; clarity: 4.4; \nrelevance: 4.3; accuracy: 4; comprehensiveness: \n4.2; laypeople rated overall value (4.6 versus 4) \nand comprehensiveness (4.5 versus 3.9) \nsignificantly higher than professionals  \nGPT-3.5 demonstrates the \nability to provide largely \naccurate, relevant, and \ncomprehensive answers to \nquestions about cardiac arrest, \nCPR, and post-resuscitation \nrecovery \n76. Hermann et al., \n2023; USA95 \nQuantitative Gynecology; GPT-3.5's accuracy \nin responding to questions on \ncervical cancer prevention, \ndiagnosis, treatment, and quality \nof life \nGPT-3.5; patients 64 questions adapted from 'frequently asked \nquestions' pages on cancer.net and the American \nCollege of Obstetricians and Gynecologists website; \nresponse evaluation by 2 attending gynecologic \noncologists for c\norre ct n ess a n d c om preh ensive ness  \nCorrect and comprehensive: 53.1%; correct but not \ncomprehensive: 29.7%; partially incorrect: 15.6%; \ncompletely incorrect: 1.6% \n \nGPT-3.5 accurately answers \nquestions about cervical cancer \nprevention, survivorship, and \nquality of life but performs less \naccurately for questions on \ncervical cancer diagnosis and \ntreatment \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 27\n77. Kerbage et al., 2023; \nUSA96 \nQuantitative Gastroenterology; GPT-4's \naccuracy in responding to patient \nquestions on irritable bowel \nsyndrome (IBS), inflammatory \nbowel disease (IBD), \ncolonoscopy, and colorectal \ncancer (CRC) screening, as well \nas questions from a physician's \nperspective on CRC screening \nand surveillance; to assess GPT-\n4's ability to generate supportive \nreferences for its responses \nGPT-4; \npatients/caregivers \n65 questions, thereof 15 questions on colonoscopy \nand CRC screening, 15 questions on IBS, 20 \nquestions on IBD, and an additional 15 questions on \nCRC screening and surveillance were designed based \non a Google Trends search; a request for references \nfollowed each prompt; response grading by 3 \ngastroenterologists using a granular and an overall \ngrading system for comprehensiveness, accuracy, and \nreferences \n84% of answers were overall accurate; physician-\noriented questions: 47% of the answers were \naccurate; references were unsuitable for 53% of \nIBS-related answers, 15% of IBD-related answers, \nand 27% of colonoscopy and CRC prevention-\nrelated answers \nGPT-4 shows potential in \ndelivering health information \nto patients seeking guidance on \nspecific gastrointestinal \ndiseases but should be used \nwith caution for clinical \ndecision-making or as a \nreference source 78. Shiraishi et al., 2023; \nJapan97 \nQualitative Ophthalmology; GPT-3.5's \nproficiency in generating \naccessible informed consent \ndocuments for patients \nundergoing blepharoplasty  \nGPT-3.5; patients 2 prompts, one short prompt (1) and one detailed \nprompt with precise instructions (2) were constructed \nfor the creation of consent documents on \nblepharoplasty; evaluation of GPT-3.5's and original \nconsent documents by 4 board-certified plastic \nsurgeons and 4 nonmedical staff members in terms of \naccuracy, informativeness, and accessibility  \nPrompt 1 scored lower than the original consent \ndocuments in accuracy (5-point Likert scale: 3.75 \nversus 5), informativeness (3.75 versus 5), and \naccessibility (3.25 versus 4.5); prompt 2 scored \nlower compared to the original IC document in \naccuracy (4 versus 5) and accessibility (3.25 versus \n4.5) \nWhile GPT-3.5 shows \npotential in generating \ninformed consent documents \nfor blepharoplasty, there are \nnotable differences and \nlimitations compared to the \noriginal documents, \nparticularly in terms of \naccuracy, informativeness, and \naccessibility \n79. Barclay et al., 2023; \nUSA98 \nQuantitative Ophthalmology; GPT-3.5's and \nGPT-4's quality and accuracy of \ninformation on corneal \ntransplantation and Fuchs \ndystrophy; to assess whether the \nanswers improve over time \nGPT-3.5/GPT-4; \npatients/caregivers \n10 questions on endothelial keratoplasty and Fuchs \ndystrophy developed by 10 corneal specialists; \nresponse evaluation by the same corneal specialists \nfor quality, safety, accuracy, and bias of information \non a 1 (A+) to 5 (F) scale \nAverage score: GPT-4 significantly outscored \nGPT-3.5 (1.4 versus 2.5); correct facts: GPT-3.5: \n61%, GPT-4: 89%, with a significant improvement \nacross iterations; against scientific consensus: \nGPT-3.5: 35%, GPT-4: 5% \nChatGPT's quality of responses \nhas improved significantly \nbetween versions 3.5 and 4, \nand the likelihood of providing \ninformation contrary to \nscientific consensus has \ndecreased \n80. Qarajeh et al., 2023; \nUSA, Jordan, Thailand99 \nQuantitative Nephrology; effectiveness of \ndifferent LLMs in accurately \ndetermining the potassium and \nphosphorus content in foods for \nindividuals adhering to a renal \ndiet \nGPT-3.5/GPT-4, \nBard, Bing Chat; \npatients \n240 dietary items from the Mayo Clinic's renal diet \ncompendium were prompted two times, the second \ntime after a two-week interval, to categorize the items \nbased on their potassium and phosphorus content; \nresponse evaluation based on the accuracy of each \nmodel \nAccuracy in identifying potassium content: GPT-\n3.5: 66%, GPT-4: 81%, Bard: 79%, Bing: 81%; \naccuracy rate in identifying phosphorus content: \nGPT-3.5: 85%, GPT-4: 77%, Bard: 100%, Bing: \n89% \n \nLLMs show potential as \nefficient tools in renal dietary \nplanning, but refinements are \nwarranted for optimal utility \n81. Chowdhury et al., \n2023; UK100 \nQuantitative Ophthalmology; to assess the \nsafety and appropriateness of \nresponses generated by GPT-3.5 \nto post-operative questions from \npatients who had undergone \ncataract surgery \nGPT-3.5; patients 131 questions collected from automated follow-up \ncalls with a cohort of 120 patients; response \nevaluation by 2 ophthalmologists using a human \nevaluation framework adapted from previous work, \nfocusing on helpfulness, clinical harm, and \nappropriateness \n59.9% of responses were \"helpful\", 36.3% \n\"somewhat helpful\", 24.4% had the possibility of \n\"moderate or mild harm\", 9.5% were opposed to \nclinical consensus \nGPT-3.5 can potentially \naddress routine patient queries \npost-cataract surgery safely, \nbut significant safety \nconstraints exist, necessitating \ncareful consideration in \nhealthcare applications \n82. Singer et al., 2023; \nUSA101 \nQuantitative Ophthalmology; to develop and \ntest Aeyeconsult as an \nophthalmology chatbot \nleveraging GPT-4 and verified \nophthalmology textbooks to \nanswer eye care-related questions  \nAeyeconsult based \non GPT-4; \npatients/caregivers \nAeyeconsult was developed using GPT-4, \nLangChain, and Pinecone; primary source material \nwas a collection of ophthalmology textbooks in PDF \nformat; 260 ophthalmology questions in multiple-\nchoice format were obtained from \nOphthoQuestions.com; response comparison against \nGPT-4 based on 4 categories (correct, incorrect, no \nanswer, multiple answers) \nAeyeconsult outperformed ChatGPT-4 in accuracy \n(83.4% versus 69.2%) and had fewer no answers (5 \nversus 18) and multiple answers (0 versus 7) \n \nLLMs can be useful in \nanswering ophthalmologic \nquestions, but their reliability \nand accuracy are limited due to \ntraining on unverified internet \ndata and lack of source \ncitations \n83. Xie et al., 2023; \nAustralia102 \nQualitative Plastic Surgery; GPT-3.5's utility \nin simulating doctor-patient \nconsultations for rhinoplasty \nGPT-3.5; patients 9 questions based on a rhinoplasty consultation \nchecklist published by the American Society of \nPlastic Surgeons were posed to GPT-3.5: response \nevaluation for accuracy, informativeness, and \naccessibility by 4 plastic surgeons \nGPT-3.5 demonstrated an understanding of natural \nlanguage in a health-specific context, provided \ncoherent, informative, and accessible answers, \nrecognized limitations in providing esoteric and \npersonal advice, was able to assist patients with \nGPT-3.5 can be a valuable \nresource for patients seeking \ninformation about rhinoplasty \nand for surgeons in \npreoperative assessment and \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 28\nbasic information about the procedure, its risks, \nbenefits, and outcomes \nplanning \n84. Nastasi et al., 2023; \nUSA103 \nQuantitative Primary Care; GPT-3.5's \nappropriateness in responding to \nquestions across various clinical \nscenarios, including preventive \ncare, acute care, and end-of-life \ndecision-making \nGPT-3.5; patients 96 clinical vignettes developed by 4 authors; response \nevaluation by 2 physicians based on clinical \nappropriateness, type of recommendation, and \nconsideration of demographic variables \n97% of responses were appropriate, 97% \nappropriately acknowledged uncertainty, and 99% \nprovided appropriate follow-up reasoning; no \nassociations between race or gender with the type \nof recommendation or with a tailored response; \"no \ninsurance\" was consistently associated with a \nspecific response related to healthcare costs and \naccess. \nGPT-3.5's medical advice was \nusually safe but often lacked \nspecificity or nuance 85. Sulejmani et al., \n2024; USA, Taiwan, \nFrance, Germany, Brazil, \nPoland111 \nQuantitative Dermatology; GPT-4's ability to \nprovide qualitative and \nempathetic responses to patient \nquestions about atopic dermatitis \n(AD) compared to physician \nresponses \nGPT-3.5; patients 99 questions were provided by an international group \nof 11 dermatologists based on commonly asked AD \npatient questions; response evaluation by the same \ndermatologists for overall quality and reliability \nGPT-3.5 scored an average of 8.18 on a 10-point \nLikert scale among the raters \nGPT-4 may be a valuable \nresource for providing quality \nand empathetic responses to \npatient questions about AD \n86. Biswas et al., 2023; \nQatar104 \nQuantitative Discipline not specified; to \nevaluate the potential of a fine-\ntuned GPT-3.5-turbo model as a \npersonal medical assistant in the \nArabic language \nGPT-3.5; patients Fine-tuning using 5,000 question-answer-pairs on \ngynecology diseases (4000 training, 1000 validation \nset) from the Arabic Healthcare Question & \nAnswering dataset; automated performance \nevaluation using perplexity, coherence, similarity, and \ntoken count; human evaluations were conducted by \ntwo medical professionals who are native in Arabic, \nfocusing on relevance, accuracy, precision, logic, and \noriginality \nPerplexity score: 13.96 (moderate confidence in the \nmodel's predictions); average similarity score: 0.1 \n(low similarity between the generated and original \ntexts); coherence score: 0.33 (moderate coherence \nin the generated text); mean human evaluation \nscores (5-point Likert scale): relevance: 3, \nprecision: 3.22, logic: 3.98, originality: 3.94, \naccuracy: 4.1 \nGPT-3.5 shows promise in \nmedical assistance applications \nin Arabic, indicating potential \nfor providing trustworthy \nmedical guidance and \nenhancing access to healthcare \nknowledge \n87. Panagoulias et al., \n2023; Greece105 \nQualitative Pulmonology; GPT-4's validity, \naccuracy, and usefulness in \ndiagnosing tuberculosis based on \nsymptoms described by a human \nGPT-4; patients An evaluation framework was developed; prompt (1) \nincludes a simple symptom description, prompt (2) \nenquires for more specificity in diagnosing the \nsymptoms, prompt (3) includes more specific or/and \ndiagnostic results if these are requested from the \nproposed diagnostics suggested by the LLM; the \nframework was tested on a tuberculosis case \nEvaluation answer 1: contextually accurate with \ncorrect references; actionable for doctors but \ngeneric for patients; economic value was \noverextended; Evaluation answer 2: contextually \ngeneric but with correct references; generic for \nboth doctors and patients; economic value was \noverextended; Evaluation answer 3: context and \nreferences correct; actionable for doctors, precise \nfor patients; exact economic value \nGPT-4 performed average to \noptimum, showing promising \nresults for identifying diseases, \nassisting doctors and patients, \nand potentially contributing to \nthe economic cost reductions \nin the healthcare system \n88. Chandra et al., 2023; \nUSA106 \nQuantitative Dermatology; GPT-3.5's \npotential to generate allergen-\nspecific patient handouts for \nallergic contact dermatitis \nGPT-3.5; patients \n \n300-word patient handouts about the most common \nallergies in North America were created by GPT-3.5; \nevaluation using the Patient Education Materials \nAssessment Tool for Printable Materials (PEMAT-P) \nfor inaccuracies, erroneous, and misleading \ninformation by 2 dermatologists \nPEMAT-P understandability score: 79%, \nactionability score: 60%; factual inaccuracies, \nerroneous or misleading statements: 2.6 \nGPT-3.5 may be a useful tool \nin assisting and generating \nallergen-specific patient \nhandouts \n89. Hung et al., 2023; \nUSA107 \nQuantitative Plastic Surgery; GPT-3.5's \nusefulness in generating patient \neducation materials on implant-\nbased breast reconstruction; to \ncompare GPT-3.5-generated with \nexpert-generated materials \nGPT-3.5; patients Patient education materials on implant-based breast \nreconstruction were generated by 5 breast \nreconstruction experts and GPT-3.5; evaluation for \nreadability and accuracy by 2 independent reviewers \nExpert content had a higher readability (Flesch-\nKincaid grade: 7.5 versus 10.5); content accuracy \nof GPT-3.5: 50%; all incorrect answers were due to \ninformation errors \nGPT-3.5 can be a powerful \ntool to generate patient \neducation material, but its \nreadability and accuracy still \nrequire improvements \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 29\nTable 2. Evaluation of included studies according to the Mixed Methods Appraisal Tool (MMAT) 2018.18 \n Screening questions \n(for all types) \n1. Qualitative 2. Quantitative randomized controlled trials 3. Quantitative nonrandomized \n S1 S2 1.1 1.2 1.3 1.4 1.5 2.1 2.2 2.3 2.4 2.5 3.1 3.2 3.3 3.4 3.5 \n1. Samaan et al.  Y Y - - - - - - - - - - Y Y Y Y Y \n2. Eromosele et al. Y Y - - - - - - - - - - N Y Y N Y \n3. Johri et al. Y Y - - - - - - - - - - Y Y Y N Y \n4. Braga et al. Y Y Y Y Y N Y - - - - - - - - - - \n5. King et al. Y Y - - - - - - - - - - Y Y Y Y Y \n6. Huang et al. Y Y - - - - - - - - - - N Y Y N Y \n7. Hanna et al. Y Y - - - - - - - - - - Y Y Y Y Y \n8. Liu et al. Y Y - - - - - - - - - - Y Y Y Y Y \n9. Samaan et al. Y Y - - - - - - - - - - Y Y Y N Y \n10. Patnaik et al. Y Y - - - - - - - - - - N N Y N Y \n11. Ali et al. Y Y - - - - - - - - - - Y Y Y Y Y \n12. Suresh et al. Y Y - - - - - - - - - - N N Y N Y \n13. Yeo et al. Y Y - - - - - - - - - - N N Y N Y \n14. Knebel et al. Y Y - - - - - - - - - - N Y Y N Y \n15. Zhu et al. Y Y - - - - - - - - - - N Y Y N Y \n16. Lahat et al. Y Y - - - - - - - - - - Y Y Y Y Y \n17. Bernstein et al. Y Y - - - - - - - - - - Y Y Y Y Y \n18. Rogasch et al. Y Y - - - - - - - - - - N Y Y N Y \n19. Campbell et al. Y Y - - - - - - - - - - N Y Y N Y \n20. Currie et al. Y Y - - - - - - - - - - N N Y N Y \n21. Draschl et al. Y Y - - - - - - - - - - N Y Y N Y \n22. Alessandri-Bonetti et al. Y Y - - - - - - - - - - N Y Y N Y \n23. Capelleras et al. Y Y Y Y Y Y Y - - - - - - - - - - \n24. Coskun et al. Y Y - - - - - - - - - - N Y Y N Y \n25. Durairaj et al. Y Y - - - - - - - - - - N Y Y N Y \n26. Kianian et al. Y Y - - - - - - - - - - N Y Y N Y \n27. Seth et al. Y Y - - - - - - - - - - N N Y N Y \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 30\n28. Inojosa et al. Y Y - - - - - - - - - - N N Y N Y \n29. Lyons et al. Y Y - - - - - - - - - - Y Y Y N Y \n30. Babayiğ it et al. Y Y - - - - - - - - - - N N Y N Y \n31. Mondal et al. Y Y - - - - - - - - - - N Y Y N Y \n32. Kim et al. Y Y - - - - - - - - - - Y Y Y N Y \n33. Song et al. Y Y - - - - - - - - - - Y Y Y Y Y \n34. Bitar et al. Y Y - - - - - Y Y Y Y Y - - - - - \n35. Zalzal et al. Y Y - - - - - - - - - - Y Y Y Y Y \n36. Chervenak et al. Y Y - - - - - - - - - - N Y Y N Y \n37. Bushuven et al. Y Y - - - - - - - - - - Y N Y N Y \n38. Jeblick et al. Y Y - - - - - - - - - - N Y Y N Y \n39. Samaan et al. Y Y - - - - - - - - - - Y Y Y Y Y \n40. Zhou et al. Y Y - - - - - - - - - - N Y Y N Y \n41. Oniani et al. Y Y - - - - - - - - - - Y Y Y Y Y \n42. Hernandez et al. Y Y - - - - - - - - - - N N Y N Y \n43. Kuş cu et al. Y Y - - - - - - - - - - Y Y Y Y Y \n44. Biswas et al. Y Y - - - - - - - - - - N N Y N Y \n45. Chiesa-Estomba et al. Y Y - - - - - - - - - - N N Y N Y \n46. Decker et al. Y Y - - - - - - - - - - N Y Y Y Y \n47. Kaarre et al. Y Y - - - - - - - - - - N Y Y N Y \n48. Ferreira et al. Y Y - - - - - - - - - - N N Y N Y \n49. Truhn et al. Y Y - - - - - - - - - - Y Y Y N Y \n50. Hurley et al. Y Y - - - - - - - - - - N Y Y N Y \n51. Cankurtaran et al. Y Y - - - - - - - - - - N N Y N Y \n52. Birkun et al. Y Y - - - - - - - - - - N Y Y Y Y \n53. Pushpanathan et al. Y Y - - - - - - - - - - N Y Y N Y \n54. Shao et al. Y Y - - - - - - - - - - N N Y N Y \n55. Vaira et al. Y Y - - - - - - - - - - Y Y Y Y Y \n56. Chen et al. Y Y - - - - - - - - - - N Y Y N Y \n57. Bellinger et al. Y Y - - - - - - - - - - N Y Y N Y \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 31\n58. Nielsen et al. Y Y - - - - - - - - - - Y Y Y Y Y \n59. Sezgin et al. Y Y - - - - - - - - - - N Y Y N Y \n60. Floyd et al. Y Y - - - - - - - - - - Y Y Y Y Y \n61. Uz et al. Y Y - - - - - - - - - - N N Y N Y \n62. Athavale et al. Y Y - - - - - - - - - - N Y Y N Y \n63. Li et al. Y Y - - - - - - - - - - Y Y Y Y Y \n64. Seth et al. Y Y - - - - - - - - - - N Y Y N Y \n65. Kuckelman et al. Y Y - - - - - - - - - - N N Y N Y \n66. Lockie et al. Y Y - - - - - - - - - - N Y Y N Y \n67. Harver et al. Y Y - - - - - - - - - - N Y Y N Y \n68. Li et al. Y Y - - - - - - - - - - Y Y Y Y Y \n69. Scheschenja et al. Y Y - - - - - - - - - - N Y Y N Y \n70. Gordon et al. Y Y - - - - - - - - - - N Y Y N Y \n71. Stroop et al. Y Y - - - - - - - - - - N Y C N Y \n72. Coraci et al. Y Y - - - - - - - - - - N Y Y Y Y \n73. Ye et al. Y Y - - - - - - - - - - N Y Y Y Y \n74. Mohammad-Rahimi et al. Y Y - - - - - - - - - - N Y Y N Y \n75. Scquizzato et al. Y Y - - - - - - - - - - N Y Y N Y \n76. Hermann et al. Y Y - - - - - - - - - - Y Y Y Y Y \n77. Kerbage et al. Y Y - - - - - - - - - - Y Y Y Y Y \n78. Shiraishi et al. Y Y - - - - - - - - - - N N Y N Y \n79. Barclay et al. Y Y - - - - - - - - - - N Y Y N Y \n80. Qarajeh et al. Y Y - - - - - - - - - - Y Y Y Y Y \n81. Chowdhury et al. Y Y - - - - - - - - - - Y Y Y N Y \n82. Singer et al.  Y Y - - - - - - - - - - Y Y Y Y Y \n83. Xie et al. Y Y Y Y Y Y Y - - - - - - - - - - \n84. Nastasi et al. Y Y - - - - - - - - - - Y Y Y Y Y \n85. Sulejmani et al. Y Y - - - - - - - - - - N Y Y N Y \n86. Biswas et al. Y Y - - - - - - - - - - Y Y Y Y Y \n87. Panagoulias et al. Y Y Y Y Y Y Y - - - - - - - - - - \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 32\n88. Chandra et al. Y Y - - - - - - - - - - N N C N Y \n89. Hung et al. Y Y - - - - - - - - - - N Y Y N Y \nAbbreviations: C, Can't tell; N, No; Y, Yes; S1, Are there clear research questions?; S2, Do the collected data allow to address the research questions?; 1.1, Is the qualitative \napproach appropriate to answer the research question?; 1.2, Are the qualitative data collection methods adequate to address the research question?; 1.3, Are the findings \nadequately derived from the data?; 1.4, Is the interpretation of results sufficiently substantiated by data?; 1.5, Is there coherence between qualitative data sources, collection, \nanalysis and interpretation?; 2.1, Is randomization appropriately performed?; 2.2, Are the groups comparable at baseline?; 2.3, Are there complete outcome data?; 2.4, Are \noutcome assessors blinded to the intervention provided?; 2.5, Did the participants adhere to the assigned intervention?; 3.1, Are the participants representative of the target \npopulation?; 3.2, Are measurements appropriate regarding both the outcome and intervention (or exposure)?; 3.3, Are there complete outcome data?; 3.4, Are the confounders \naccounted for in the design and analysis?; 3.5, During the study period, is the intervention administered (or exposure occurred) as intended? \nNotes: Categories 4 and 5 are not listed as no studies with quantitative descriptive or mixed methods study designs were identified. \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 33\n9. References \n1 Milmo, D. ChatGPT reaches 100 million users two months after launch, \n<https://www.theguardian.com/technology/2023/feb/02/chatgpt-100-million-users-open-ai-fastest-growing-\napp> (2023). \n2 OpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774; 10.48550/arXiv.2303.08774 (2023). \n3 Zhao, W. X. et al. A survey of large language models. arXiv preprint arXiv:2303.18223; \n10.48550/arXiv.2303.18223 (2023).  \n4 Clusmann, J. et al. The future landscape of large language models in medicine. Communications Medicine \n3, 141; 10.1038/s43856-023-00370-1 (2023). \n5 Chen, Z. et al. Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint \narXiv:2311.16079; 10.48550/arXiv.2311.16079 (2023).  \n6 Labrak, Y. et al. BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical \nDomains. arXiv preprint arXiv:2402.10373; 10.48550/arXiv.2402.10373 (2024).  \n7 Xiong, G., Jin, Q., Lu, Z. & Zhang, A. Benchmarking Retrieval-Augmented Generation for Medicine. arXiv \npreprint arXiv:2402.13178; 10.48550/arXiv.2402.13178 (2024).  \n8 Yang, X. et al. A large language model for electronic health records. npj Dig Med  5, 194; 10.1038/s41746-\n022-00742-2 (2022). \n9 Tian, S. et al. Opportunities and challenges for ChatGPT and large language models in biomedicine and \nhealth. Brief Bioinform 25; 10.1093/bib/bbad493 (2024).  \n10 Adams, L. C. et al. Leveraging GPT-4 for Post Hoc Transformation of Free-text Radiology Reports into \nStructured Reporting: A Multilingual Feasibility Study. Radiology 307, e230725; 10.1148/radiol.230725 \n(2023).  \n11 McDuff, D. et al. Towards accurate differential diagnosis with large language models. arXiv preprint \narXiv:2312.00164; 10.48550/arXiv.2312.00164 (2023).  \n12 Jiang, L. Y. et al. Health system-scale language models are all-purpose prediction engines. Nature 619, 357-\n362; 10.1038/s41586-023-06160-y (2023). \n13 Liu, S. et al. Leveraging Large Language Models for Generating Responses to Patient Messages. medRxiv \n2023.2007.2014.23292669; 10.1101/2023.07.14.23292669 (2023).  \n14 Bu sch, F., Hoffmann, L., Adams, L. C. & Bressem, K. K. A systematic review of current large language \nmodel applications and biases in patient care, \n<https://www.crd.york.ac.uk/prospero/display_record.php?ID=CRD42024504542> (2024). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 34\n15 Page, M. J. et al. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. Bmj \n372, n71; 10.1136/bmj.n71 (2021). \n16 Ouzzani, M., Hammady, H., Fedorowicz, Z. & Elmagarmid, A. Rayyan—a web and mobile app for \nsystematic reviews. Syst Rev  5, 210; 10.1186/s13643-016-0384-4 (2016).  \n17 Data extraction form, \n<https://docs.google.com/forms/d/e/1FAIpQLScFwE5KaOugxX_xXtt9Y6fbBhV4s77S9cWRdVuiHh34vm\nArkQ/viewform> (2024). \n18 Hong, Q. N. et al. The Mixed Methods Appraisal Tool (MMAT) version 2018 for information professionals \nand researchers. Educ Inf  34, 285-291; 10.3233/EFI-180221 (2018).  \n19 Hong, Q. N., Pluye, P., Bujold, M. & Wassef, M. Convergent and sequential synthesis designs: implications \nfor conducting and reporting systematic reviews of qualitative and quantitative evidence. Syst Rev 6, 61; \n10.1186/s13643-017-0454-2 (2017). \n20 Thomas, J. & Harden, A. Methods for the thematic synthesis of qualitative research in systematic reviews. \nBMC Med Res Methodol  8, 45; 10.1186/1471-2288-8-45 (2008).  \n21 Sociocultural Research Consultants, LLC. Dedoose Version 9.2.4, cloud application for managing, \nanalyzing, and presenting qualitative and mixed method research data (Los Angeles, CA, 2024). \n22 Savage, T., Wang, J. & Shieh, L. A Large Language Model Screening Tool to Target Patients for Best \nPractice Alerts: Development and Validation. JMIR Med Inform 11, e49886; 10.2196/49886 (2023).  \n23 Coskun, B. N., Yagiz, B., Ocakoglu, G., Dalkilic, E. & Pehlivan, Y. Assessing the accuracy and \ncompleteness of artificial intelligence language models in providing information on methotrexate use. \nRheumatol Int; 10.1007/s00296-023-05473-5 (2023). \n24 Bitar, H., Babour, A., Nafa, F., Alzamzami, O. & Alismail, S. Increasing Women's Knowledge about HPV \nUsing BERT Text Summarization: An Online Randomized Study. Int J Environ Res Public Health 19; \n10.3390/ijerph19138100 (2022).  \n25 Samaan, J. S. et al. Artificial Intelligence and Patient Education: Examining the Accuracy and \nReproducibility of Responses to Nutrition Questions Related to Inflammatory Bowel Disease by GPT-4. \nmedRxiv 2023.2010.2028.23297723; 10.1101/2023.10.28.23297723 (2023). \n26 Eromosele, O. B., Sobodu, T., Olayinka, O. & Ouyang, D. Racial Disparities in Knowledge of \nCardiovascular Disease by a Chat-Based Artificial Intelligence Model. medRxiv 2023.2009.2020.23295874; \n10.1101/2023.09.20.23295874 (2023).  \n27 Johri, S. et al. Guidelines For Rigorous Evaluation of Clinical LLMs For Conversational Reasoning. \nmedRxiv 2023.2009.2012.23295399; 10.1101/2023.09.12.23295399 (2024).  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 35\n28 Braga, A. V. N. M. et al. Use of ChatGPT in Pediatric Urology and its Relevance in Clinical Practice: Is it \nuseful? medRxiv 2023.2009.2011.23295266; 10.1101/2023.09.11.23295266 (2023).  \n29 King, R. C. et al. Appropriateness of ChatGPT in answering heart failure related questions. medRxiv \n2023.2007.2007.23292385; 10.1101/2023.07.07.23292385 (2023).  \n30 Huang, S. S. et al. Fact Check: Assessing the Response of ChatGPT to Alzheimer’s Disease Statements with \nVarying Degrees of Misinformation. medRxiv 2023.2009.2004.23294917; 10.1101/2023.09.04.23294917 \n(2023).  \n31 Hanna, J. J., Wakene, A. D., Lehmann, C. U. & Medford, R. J. Assessing Racial and Ethnic Bias in Text \nGeneration for Healthcare-Related Tasks by ChatGPT1. medRxiv 2023.2008.2028.23294730; \n10.1101/2023.08.28.23294730 (2023).  \n32 Samaan, J. S. et al. ChatGPT's ability to comprehend and answer cirrhosis related questions in Arabic. Arab \nJ Gastroenterol 24, 145-148; 10.1016/j.ajg.2023.08.001 (2023).  \n33 Patnaik, S. S. & Hoffmann, U. Quantitative evaluation of ChatGPT versus Bard responses to anaesthesia-\nrelated queries. Br J Anaesth 132, 169-171; 10.1016/j.bja.2023.09.030 (2024).  \n34 Ali, H. et al. Evaluating the performance of ChatGPT in responding to questions about endoscopic \nprocedures for patients. iGIE 2, 553-559; 10.1016/j.igie.2023.10.001 (2023).  \n35 Suresh, K. et al. Utility of GPT-4 as an Informational Patient Resource in Otolaryngology. medRxiv \n2023.2005.2014.23289944; 10.1101/2023.05.14.23289944 (2023).  \n36 Yeo, Y. H. et al. GPT-4 outperforms ChatGPT in answering non-English questions related to cirrhosis. \nmedRxiv 2023.2005.2004.23289482; 10.1101/2023.05.04.23289482 (2023).  \n37 Knebel, D. et al. Assessment of ChatGPT in the Prehospital Management of Ophthalmological Emergencies \n- An Analysis of 10 Fictional Case Vignettes. Klin Monbl Augenheilkd 1, 5-35; 10.1055/a-2149-0447 \n(2023).  \n38 Zhu, L., Mou, W. & Chen, R. Can the ChatGPT and other large language models with internet-connected \ndatabase solve the questions and concerns of patient with prostate cancer and help democratize medical \nknowledge? J Transl Med 21, 269; 10.1186/s12967-023-04123-5 (2023).  \n39 Lahat, A., Shachar, E., Avidan, B., Glicksberg, B. & Klang, E. Evaluating the Utility of a Large Language \nModel in Answering Common Patients' Gastrointestinal Health-Related Questions: Are We There Yet? \nDiagnostics (Basel) 13; 10.3390/diagnostics13111950 (2023).  \n40 Bernstein, I. A. et al. Comparison of Ophthalmologist and Large Language Model Chatbot Responses to \nOnline Patient Eye Care Questions. JAMA Netw Open 6, e2330320; 10.1001/jamanetworkopen.2023.30320 \n(2023). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 36\n41 Rogasch, J. M. M. et al. ChatGPT: Can You Prepare My Patients for [18F]FDG PET/CT and Explain My \nReports? J Nucl Med, jnumed.123.266114; 10.2967/jnumed.123.266114 (2023).  \n42 Campbell, D. J. et al. Evaluating ChatGPT Responses on Thyroid Nodules for Patient Education. Thyroid®; \n10.1089/thy.2023.0491 (2023). \n43 Currie, G., Robbie, S. & Tually, P. ChatGPT and Patient Information in Nuclear Medicine: GPT-3.5 Versus \nGPT-4. J Nucl Med Technol 51, 307-313; 10.2967/jnmt.123.266151 (2023). \n44 Draschl, A. et al. Are ChatGPT's Free-Text Responses on Periprosthetic Joint Infections of the Hip and \nKnee Reliable and Useful? J Clin Med 12; 10.3390/jcm12206655 (2023). \n45 Alessandri-Bonetti, M., Liu, H. Y., Palmesano, M., Nguyen, V. T. & Egro, F. M. Online patient education in \nbody contouring: A comparison between Google and ChatGPT. J Plast Reconstr Aesthet Surg 87, 390-402; \n10.1016/j.bjps.2023.10.091 (2023). \n46 Coskun, B., Ocakoglu, G., Yetemen, M. & Kaygisiz, O. Can ChatGPT, an Artificial Intelligence Language \nModel, Provide Accurate and High-quality Patient Information on Prostate Cancer? Urology 180, 35-58; \n10.1016/j.urology.2023.05.040 (2023). \n47 Durairaj, K. K. et al. Artificial Intelligence Versus Expert Plastic Surgeon: Comparative Study Shows \nChatGPT “Wins” Rhinoplasty Consultations: Should We Be Worried? Facial Plast Surg Aesthet Med; \n10.1089/fpsam.2023.0224 (2023). \n48 Kianian, R., Sun, D., Crowell, E. L. & Tsui, E. The Use of Large Language Models to Generate Education \nMaterials about Uveitis. Ophthalmol Retina; 10.1016/j.oret.2023.09.008 (2023). \n49 Seth, I. et al. Exploring the Role of a Large Language Model on Carpal Tunnel Syndrome Management: \nAn Observation Study of ChatGPT. J Hand Surg Am 48, 1025-1033; 10.1016/j.jhsa.2023.07.003 (2023). \n50 Inojosa, H. et al. Can ChatGPT explain it? Use of artificial intelligence in multiple sclerosis communication. \nNeurol Res Pract  5, 48; 10.1186/s42466-023-00270-8 (2023). \n51 Lyons, R. J., Arepalli, S. R., Fromal, O., Choi, J. D. & Jain, N. Artificial intelligence chatbot performance in \ntriage of ophthalmic conditions. Can J Ophthalmol; 10.1016/j.jcjo.2023.07.016 (2023).  \n52 Babayi\nğ it, O., Tastan Eroglu, Z., Ozkan Sen, D. & Ucan Yarkac, F. Potential Use of ChatGPT for Patient \nInformation in Periodontology: A Descriptive Pilot Study. Cureus 15, e48518; 10.7759/cureus.48518 \n(2023). \n53 Mondal, H., Dash, I., Mondal, S. & Behera, J. K. ChatGPT in Answering Queries Related to Lifestyle-\nRelated Diseases and Disorders. Cureus 15, e48296; 10.7759/cureus.48296 (2023).  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 37\n54 Kim, H. W., Shin, D. H., Kim, J., Lee, G. H. & Cho, J. W. Assessing the performance of ChatGPT's \nresponses to questions related to epilepsy: A cross-sectional study on natural language processing and \nmedical information retrieval. Seizure 114, 1-8; 10.1016/j.seizure.2023.11.013 (2023).  \n55 Song, H. et al. Evaluating the Performance of Different Large Language Models on Health Consultation and \nPatient Education in Urolithiasis. J Med Syst 47, 125; 10.1007/s10916-023-02021-3 (2023). \n56 Zalzal, H. G., Abraham, A., Cheng, J. H. & Shah, R. K. Can ChatGPT help patients answer their \notolaryngology questions? Laryngoscope Investig Otolaryngol; 10.1002/lio2.1193 (2023).  \n57 Chervenak, J., Lieman, H., Blanco-Breindel, M. & Jindal, S. The promise and peril of using a large \nlanguage model to obtain clinical information: ChatGPT performs strongly as a fertility counseling tool with \nlimitations.\n Fertil Steril 120, 575-583; 10.1016/j.fertnstert.2023.05.151 (2023).  \n58 Bushuven, S. et al. \"ChatGPT, Can You Help Me Save My Child's Life?\" - Diagnostic Accuracy and \nSupportive Capabilities to Lay Rescuers by ChatGPT in Prehospital Basic Life Support and Paediatric \nAdvanced Life Support Cases - An In-silico Analysis. J Med Syst 47, 123; 10.1007/s10916-023-02019-x \n(2023).  \n59 Jeblick, K. et al. ChatGPT makes medicine easy to swallow: an exploratory case study on simplified \nradiology reports. Eur Radiol; 10.1007/s00330-023-10213-1 (2023). \n60 Samaan, J. S. et al. Assessing the Accuracy of Responses by the Language Model ChatGPT to Questions \nRegarding Bariatric Surgery. Obes Surg 33, 1790-1796; 10.1007/s11695-023-06603-5 (2023). \n61 Zhou, J. M., Li, T. Y., Fong, S. J., Dey, N. & Crespo, R. G. Exploring ChatGPT's Potential for Consultation, \nRecommendations and Report Diagnosis: Gastric Cancer and Gastroscopy Reports' Case. Int J Interact \nMultimed Artif Intell 8, 7-13; 10.9781/ijimai.2023.04.007 (2023). \n62 Oniani, D. et al. Toward Improving Health Literacy in Patient Education Materials with Neural Machine \nTranslation Models. AMIA Jt Summits Transl Sci Proc 2023, 418-426 (2023).  \n63 Hernandez, C. A. et al. The Future of Patient Education: AI-Driven Guide for Type 2 Diabetes. Cureus 15, \ne48919; 10.7759/cureus.48919 (2023). \n64 Ku\nş cu, O., Pamuk, A. E., Sütay Süslü, N. & Hosal, S. Is ChatGPT accurate and reliable in answering \nquestions regarding head and neck cancer? Front Oncol 13, 1256459; 10.3389/fonc.2023.1256459 \n65 Biswas, S., Logan, N. S., Davies, L. N., Sheppard, A. L. & Wolffsohn, J. S. Assessing the utility of \nChatGPT as an artificial intelligence-based large language model for information to answer questions on \nmyopia. Ophthalmic Physiol Opt 43, 1562-1570; 10.1111/opo.13207 (2023). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 38\n66 Chiesa-Estomba, C. M. et al. Exploring the potential of Chat-GPT as a supportive tool for sialendoscopy \nclinical decision making and patient information support. Eur Arch Otorhinolaryngol; 10.1007/s00405-023-\n08104-8 (2023).  \n67 Decker, H. et al. Large Language Model-Based Chatbot vs Surgeon-Generated Informed Consent \nDocumentation for Common Procedures. JAMA Netw Open 6, e2336997; \n10.1001/jamanetworkopen.2023.36997 (2023). \n68 Kaarre, J. et al. Exploring the potential of ChatGPT as a supplementary tool for providing orthopaedic \ninformation. Knee Surg Sports Traumatol Arthrosc 31, 5190-5198; 10.1007/s00167-023-07529-2 (2023). \n69 Ferreira, A. L., Chu, B., Grant-Kels, J. M., Ogunleye, T. & Lipoff, J. B. Evaluation of ChatGPT \nDermatology Responses to Common Patient Queries. JMIR Dermatol 6, e49280; 10.2196/49280 (2023).  \n70 Truhn, D. et al. A pilot study on the efficacy of GPT-4 in providing orthopedic treatment recommendations \nfrom MRI reports. Sci Rep 13, 20159; 10.1038/s41598-023-47500-2 (2023). \n71 Hurley, E. T. et al. Evaluation High-Quality of Information from ChatGPT (Artificial Intelligence-Large \nLanguage Model) Artificial Intelligence on Shoulder Stabilization Surgery. Arthroscopy; \n10.1016/j.arthro.2023.07.048 (2023). \n72 Cankurtaran, R. E., Polat, Y. H., Aydemir, N. G., Umay, E. & Yurekli, O. T. Reliability and Usefulness of \nChatGPT for Inflammatory Bowel Diseases: An Analysis for Patients and Healthcare Professionals. Cureus \n15, e46736; 10.7759/cureus.46736 (2023).  \n73 Birkun, A. A. & Gautam, A. Large Language Model (LLM)-Powered Chatbots Fail to Generate Guideline-\nConsistent Content on Resuscitation and May Provide Potentially Harmful Advice. Prehosp Disaster Med \n38, 757-763; 10.1017/s1049023x23006568 (2023). \n74 Pushpanathan, K. et al. Popular large language model chatbots' accuracy, comprehensiveness, and self-\nawareness in answering ocular symptom queries. iScience 26, 108163; 10.1016/j.isci.2023.108163 (2023). \n75 Shao, C. Y. et al. Appropriateness and Comprehensiveness of Using ChatGPT for Perioperative Patient \nEducation in Thoracic Surgery in Different Language Contexts: Survey Study. Interact J Med Res 12, \ne46900; 10.2196/46900 (2023).  \n76 Vaira, L. A. et al. Accuracy of ChatGPT-Generated Information on Head and Neck and Oromaxillofacial \nSurgery: A Multicenter Collaborative Analysis. Otolaryngol Head Neck Surg; 10.1002/ohn.489 (2023). \n77 Ch\nen, S. et al. Use of Artificial Intelligence Chatbots for Cancer Treatment Information. JAMA Oncol 9, \n1459-1462; 10.1001/jamaoncol.2023.2954 (2023). \n78 Bellinger, J. R. et al. BPPV Information on Google Versus AI (ChatGPT). Otolaryngol Head Neck Surg; \n10.1002/ohn.506 (2023). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 39\n79 Nielsen, J. P. S., von Buchwald, C. & Grønhøj, C. Validity of the large language model ChatGPT (GPT4) as \na patient information source in otolaryngology by a variety of doctors in a tertiary otorhinolaryngology \ndepartment. Acta Otolaryngol 143, 779-782; 10.1080/00016489.2023.2254809 (2023). \n80 Sezgin, E., Chekeni, F., Lee, J. & Keim, S. Clinical Accuracy of Large Language Models and Google \nSearch Responses to Postpartum Depression Questions: Cross-Sectional Study. J Med Internet Res 25, \ne49240; 10.2196/49240 (2023). \n81 Floyd, W. et al. Current Strengths and Weaknesses of ChatGPT as a Resource for Radiation Oncology \nPatients and Providers. Int J Radiat Oncol Biol Phys; 10.1016/j.ijrobp.2023.10.020 (2023). \n82 Uz, C. & Umay, E. \"Dr ChatGPT\": Is it a reliable and useful source for common rheumatic diseases? Int J \nRheum Dis 26, 1343-1349; 10.1111/1756-185x.14749 (2023).  \n83 Athavale, A., Baier, J., Ross, E. & Fukaya, E. The potential of chatbots in chronic venous disease patient \nmanagement. JVS Vasc Insights 1; 10.1016/j.jvsvi.2023.100019 (2023). \n84 Li, Y. et al. ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI \n(LLaMA) Using Medical Domain Knowledge. Cureus 15, e40895; 10.7759/cureus.40895 (2023).  \n85 Seth, I. et al. Comparing the Efficacy of Large Language Models ChatGPT, BARD, and Bing AI in \nProviding Information on Rhinoplasty: An Observational Study. Aesthet Surg J Open Forum 5, ojad084; \n10.1093/asjof/ojad084 (2023).  \n86 Lockie, E. & Choi, J. Evaluation of a chat GPT generated patient information leaflet about laparoscopic \ncholecystectomy. ANZ J Surg; 10.1111/ans.18834 (2023). \n87 Haver, H. L., Lin, C. T., Sirajuddin, A., Yi, P. H. & Jeudy, J. Use of ChatGPT, GPT-4, and Bard to Improve \nReadability of ChatGPT's Answers to Common Questions About Lung Cancer and Lung Cancer Screening. \nAJR Am J Roentgenol 221, 701-704; 10.2214/ajr.23.29622 (2023). \n88 Li, H. et al. Decoding radiology reports: Potential application of OpenAI ChatGPT to enhance patient \nunderstanding of diagnostic reports. Clin Imaging 101, 137-141; 10.1016/j.clinimag.2023.06.008 (2023). \n89 Scheschenja, M. et al. Feasibility of GPT-3 and GPT-4 for in-Depth Patient Education Prior to \nInterventional Radiological Procedures: A Comparative Analysis. Cardiovasc Intervent Radiol  47, 245-\n250; 10.1007/s00270-023-03563-2 (2024). \n90 Gordon, E. B. et al. E nhancing Patient Communication With Chat-GPT in Radiology: Evaluating the \nEfficacy and Readability of Answers to Common Imaging-Related Questions. J Am Coll Radiol 21, 353-\n359; 10.1016/j.jacr.2023.09.011 (2024). \n91 Stroop, A. et al. Large language models: Are artificial intelligence-based chatbots a reliable source of \npatient information for spinal surgery? Eur Spine J; 10.1007/s00586-023-07975-z (2023). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 40\n92 Coraci, D. et al. ChatGPT in the development of medical questionnaires. The example of the low back pain. \nEur J Transl Myol 33; 10.4081/ejtm.2023.12114 (2023). \n93 Ye, C., Zweck, E., Ma, Z., Smith, J. & Katz, S. Doctor Versus Artificial Intelligence: Patient and Physician \nEvaluation of Large Language Model Responses to Rheumatology Patient Questions in a Cross-Sectional \nStudy. Arthritis Rheumatol; 10.1002/art.42737 (2023). \n94 Mohammad-Rahimi, H. et al. Validity and reliability of artificial intelligence chatbots as public sources of \ninformation on endodontics. Int Endod J 57, 305-314; 10.1111/iej.14014 (2024).  \n95 Hermann, C. E. et al. Let's chat about cervical cancer: Assessing the accuracy of ChatGPT responses to \ncervical cancer questions. Gynecol Oncol 179, 164-168; 10.1016/j.ygyno.2023.11.008 (2023).  \n96 Kerbage, A. et al. Accuracy of ChatGPT in Common Gastrointestinal Diseases: Impact for Patients and \nProviders. Clin Gastroenterol Hepatol; 10.1016/j.cgh.2023.11.008  (2023). \n97 Shiraishi, M. et al. Generating Informed Consent Documents Related to Blepharoplasty Using ChatGPT. \nOphthalmic Plast Reconstr Surg; 10.1097/iop.0000000000002574 (2023). \n98 Barclay, K. S. et al. Quality and Agreement With Scientific Consensus of ChatGPT Information Regarding \nCorneal Transplantation and Fuchs Dystrophy. Cornea; 10.1097/ico.0000000000003439 (2023).  \n99 Qarajeh, A. et al. AI-Powered Renal Diet Support: Performance of ChatGPT, Bard AI, and Bing Chat. Clin \nPract 13, 1160-1172; 10.3390/clinpract13050104 (2023). \n100 Chowdhury, M. et al. Can Large Language Models Safely Address Patient Questions Following Cataract \nSurgery?. Proceedings of the 5th Clinical Natural Language Processing Workshop; \n10.18653/v1/2023.clinicalnlp-1.17 (2023). \n101 Singer, M. B., Fu, J. J., Chow, J. & Teng, C. C. Development and Evaluation of Aeyeconsult: A Novel \nOphthalmology Chatbot Leveraging Verified Textbook Knowledge and GPT-4. J Surg Educ 81, 438-443; \n10.1016/j.jsurg.2023.11.019 (2024).  \n102 Xie, Y. et al. Aesthetic Surgery Advice and Counseling from Artificial Intelligence: A Rhinoplasty \nConsultation with ChatGPT. Aesthetic Plast Surg 47, 1985-1993; 10.1007/s00266-023-03338-7 (2023).  \n103 Nastasi, A. J., Courtright, K. R., Halpern, S. D. & Weissman, G. E. A vignette-based evaluation of \nChatGPT's ability to provide appropriate and equitable medical advice across care contexts. Sci Rep 13, \n17885; 10.1038/s41598-023-45223-y (2023). \n104 Biswas, M., Islam, A., Shah, Z., Zaghouani, W. & Brahim Belhaouari, S. Can ChatGPT be Your Personal \nMedical Assistant?. Tenth International Conference on Social Networks Analysis, Management and Security \n(SNAMS), 1-5; 10.1109/SNAMS60348.2023.10375477 (2023). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 41\n105 Panagoulias, D., Palamidas, F., Virvou, M. & Tsihrintzis, G. Evaluating the Potential of LLMs and \nChatGPT on Medical Diagnosis and Treatment. 14th International Conference on Information, Intelligence, \nSystems & Applications (IISA), 1-9; 10.1109/IISA59645.2023.10345968 (2023). \n106 Chandra, A., Davis, M. J., Hamann, D. & Hamann, C. R. Utility of Allergen-Specific Patient-Directed \nHandouts Generated by Chat Generative Pretrained Transformer. Dermatitis 34, 448; \n10.1089/derm.2023.0059 (2023). \n107 Hung, Y.-C., Chaker, S., Sigel, M., Saad, M. & Slater, E. Comparison of Patient Education Materials \nGenerated by Chat Generative Pre-Trained Transformer Versus Experts: An Innovative Way to Increase \nReadability of Patient Education Materials. Ann Plast Surg  91, 409-412; 10.1097/SAP.0000000000003634 \n(2023). \n108 Capelleras, M., Soto-Galindo, G. A., Cruellas, M. & Apaydin, F. ChatGPT and Rhinoplasty Recovery: An \nExploration of AI's Role in Postoperative Guidance. Facial Plast Surg; 10.1055/a-2219-4901 (2024). \n109 Scquizzato, T. et al. Testing ChatGPT ability to answer laypeople questions about cardiac arrest and \ncardiopulmonary resuscitation. Resuscitation 194, 110077; 10.1016/j.resuscitation.2023.110077 (2024). \n110 Kuckelman, I. J. et al. Assessing AI-Powered Patient Education: A Case Study in Radiology. Acad Radiol \n31, 338-342; 10.1016/j.acra.2023.08.020 (2024). \n111 Sulejmani, P. et al. A large language model artificial intelligence for patient queries in atopic dermatitis. J \nEur Acad Dermatol Venereol; 10.1111/jdv.19737 (2024).  \n112 Currie, G. & Barry, K. ChatGPT in Nuclear Medicine Education. J Nucl Med Technol 51, 247-254; \n10.2967/jnmt.123.265844 (2023). \n113 Currie, G. M. Academic integrity and artificial intelligence: is ChatGPT hype, hero or heresy?\n Semin Nucl \nMed 53, 719-730; 10.1053/j.semnuclmed.2023.04.008 (2023).  \n114 Li, J., Dada, A., Puladi, B., Kleesiek, J. & Egger, J. ChatGPT in healthcare: A taxonomy and systematic \nreview. Comput Methods Programs Biomed 245, 108013; 10.1016/j.cmpb.2024.108013 (2024). \n115 Jin, M. et al. Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model. arXiv preprint \narXiv:2402.00746; 10.48550/arXiv.2402.00746 (2024).  \n116 Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of gpt-4 on medical challenge \nproblems. arXiv preprint arXiv:2303.13375; 10.48550/arXiv.2303.13375 (2023).  \n117 Brin, D. et al. Comparing ChatGPT and GPT-4 performance in USMLE soft skill assessments. Sci Rep 13, \n16492; 10.1038/s41598-023-43436-9 (2023). \n118 Jung, L. B. et al. ChatGPT Passes German State Examination in Medicine With Picture Questions Omitted. \nDtsch Arztebl Int 120, 373-374; 10.3238/arztebl.m2023.0113 (2023). \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 42\n119 Bhayana, R., Krishna, S. & Bleakney, R. R. Performance of ChatGPT on a Radiology Board-style \nExamination: Insights into Current Strengths and Limitations. Radiology 307, e230582; \n10.1148/radiol.230582 (2023). \n120 Singhal, K. et al. Towards expert-level medical question answering with large language models. arXiv \npreprint arXiv:2305.09617; 10.48550/arXiv.2305.09617 (2023).  \n121 Kung, T. H. et al. Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using \nlarge language models. PLOS Digit Health 2, e0000198; 10.1371/journal.pdig.0000198 (2023).  \n122 Kapoor, S., Henderson, P. & Narayanan, A. Promises and pitfalls of artificial intelligence for legal \napplications. arXiv preprint arXiv:2402.01656; 10.48550/arXiv.2402.01656 (2024).  \n123 Navigli, R., Conia, S. & Ross, B. Biases in Large Language Models: Origins, Inventory, and Discussion. \nACM J Data Inf Qual 15, 1-21; 10.1145/3597307 (2023).  \n124 Deng, G. et al. Jailbreaker: Automated jailbreak across multiple large language model chatbots. arXiv \npreprint arXiv:2307.08715; 10.48550/arXiv.2307.08715 (2023).  \n125 Ayoub, N. F., Lee, Y. J., Grimm, D. & Divi, V. Head-to-Head Comparison of ChatGPT Versus Google \nSearch for Medical Knowledge Acquisition. Otolaryngol Head Neck Surg; 10.1002/ohn.465 (2023).  \n126 Weis, B. Health Literacy: A Manual for Clinicians. Chicago, IL: American Medical Association, American \nMedical Foundation (2003). \n127 Tierney, A. A. et al. Ambient Artificial Intelligence Scribes to Alleviate the Burden of Clinical \nDocumentation. NEJM Catalyst 5, CAT.23.0404; 10.1056/CAT.23.0404 (2024).  \n128 Council of the European Union. Proposal for a Regulation of the European Parliament and of the Council \nlaying down harmonised rules on artificial intelligence (Artificial Intelligence Act) and amending certain \nUnion legislative acts - Analysis of the final compromise text with a view to agreement. 2021/0106(COD), \n1-272 (2024). \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \n 8 \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nRecords identified from: \nWeb of Science (n = 908) \nPubMed (n = 1,706) \nEmbase and Embase Classic \n(n = 968) \nACM Digital Library (n = 248) \nIEEE Xplore (n = 519) \n \n \nRecords removed before \nscreening: \nDuplicate records removed (n \n= 1,358) \nRecords screened \n(n = 2,991) \nRecords excluded \n(n = 2,865) \nReports sought for retrieval \n(n = 126) Reports not retrieved (n = 2) \nReports assessed for eligibility \n(n = 124) Reports excluded: \nWrong population (n = 7) \nWrong discipline (n = 10) \nWrong study design (n = 8) \nTechnological development/ \nperformance evaluation (n = \n6) \nInsufficient data for thematic \nsynthesis (n = 4) \nStudies included in review \n(n = 89) \nReports of included studies \n(n = 89) \n \nIdentification \nScreening \n \nIncluded \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \nPrevention Preclinical \nmanagement\nDiagnosis Treatment Prognosis\nLanguages:\n● English\n● Korean\n● Mandarin\n● Spanish\n● Arab\nDisciplines: Anesthesiology, Cardiology, Dentistry, \nDermatology, Emergency Medicine, Endocrinology, \nGastroenterology, General Surgery, Gynecology, Hand Surgery, \nHead and Neck Surgery/Otolaryngology, Infectious disease, \nNephrology, Neurology, Neurosurgery, Nuclear Medicine, \nOncology, Ophthalmology, Orthopedics, Plastic Surgery, \nPrimary Care/General, Pulmonology, Radiation Oncology, \nRadiology, Reproductive Medicine, Rheumatology, Thoracic \nSurgery, Urology, Vascular Surgery\nMedical text \nsummarization/translation \nGeneration of patient \ninformation \nClinical \ndocumentation\nPatients\n LLM\n Caregiver\nMedical question \nanswering/chatbot \nInformed consent\nPatient education \nmaterials Discharge instructionsSimplified radiology \nreports\nPatient-friendly \nmedical responses/\nsummaries\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint \nUnderrepresented procedures (n = 1)\nUnderserved racial groups (n = 1)\nInsurance status (n = 1)\nLanguage (n = 2)\nLocal/national medical resources (n = 5)\nProvider/organization (n = 4)\nTarget group (n = 9)\nConversation type (n = 3)\nQuantity (n = 3)\nSpecificity (n = 13)\nComplexity (n = 11)\nEvidence (n = 7)\nHarmful (n = 26)\nMisleading (n = 34)\nConfabulation (n = 18)\nIllusion (n = 12)\nHallucination (n = 38)\nDelusion (n = 14)\nDelirium (n = 34)\nExtrapolation (n = 10)\nHigh complexity/reading level (n = 22)\nOverempathic (n = 1)\nOvercautious (n = 7)\nSuperfluous (n = 16)\nOversimplification (n = 10)\nOutdated (n = 12)\nNon-standard of care (n = 24)\nIncomplete (n = 68)\nGeneric/non-personalized (n = 34)\nNon-referenceable (n = 20)\nNon-deterministic (n = 24)\nNot open source (n = 10)\nNot freely accessible (n = 9)\nLimited number of prompts (n = 3)\nStores/processes sensitive health information (n = 8)\nLimited in reference provision/evaluation/validation (n = 20)\nUndisclosed origin of training data (n = 36)\nRestricted access to internet data (n = 22)\nMisunderstanding of medical information/terms (n = 7)\nLimited in processing/producing medical images (n = 5)\nLimited clinical reasoning (n = 7)\nImplicit knowledge/lack of clinical context (n = 13)\nBias (n = 6)\nEnvironment-dependent (n = 16)\nPrompt-/input dependent (n = 27)\nUnsafe (n = 39)\nIncorrect (n = 78)\nNon-comprehensive (n = 78)\nNon-reproducible (n = 38)\nIncapable of self-validation/correction (n = 4)\nLimited engagement/dialogue capabilities (n = 10)\nBlack box (n = 12)\nAccessibility (n = 18)\nData (n = 55)\nNot optimized for the medical domain (n = 46)\nOutput (n = 86)\nDesign (n = 67)\nLLM limitations\nmarkmap\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted March 5, 2024. ; https://doi.org/10.1101/2024.03.04.24303733doi: medRxiv preprint "
}