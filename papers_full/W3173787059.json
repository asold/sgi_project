{
  "title": "Self-Attention Attribution: Interpreting Information Interactions Inside Transformer",
  "url": "https://openalex.org/W3173787059",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2323301275",
      "name": "Yaru Hao",
      "affiliations": [
        "Ulverscroft (United Kingdom)",
        "Microsoft Research (India)",
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A1974723233",
      "name": "Li Dong",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2171151462",
      "name": "Furu Wei",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2100549106",
      "name": "Ke Xu",
      "affiliations": [
        "Beihang University"
      ]
    },
    {
      "id": "https://openalex.org/A2323301275",
      "name": "Yaru Hao",
      "affiliations": [
        "Microsoft Research (United Kingdom)",
        "Beihang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2133564696",
    "https://openalex.org/W2983219369",
    "https://openalex.org/W2336525064",
    "https://openalex.org/W6768061436",
    "https://openalex.org/W3015038845",
    "https://openalex.org/W2974236805",
    "https://openalex.org/W3042711927",
    "https://openalex.org/W6764724796",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W6763527631",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2885396331",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W6666761814",
    "https://openalex.org/W6764072591",
    "https://openalex.org/W2983693488",
    "https://openalex.org/W6767535143",
    "https://openalex.org/W2948967934",
    "https://openalex.org/W2782630856",
    "https://openalex.org/W2587529872",
    "https://openalex.org/W6765429426",
    "https://openalex.org/W2605409611",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W6766865527",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2950768109",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2970726176",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3034350582",
    "https://openalex.org/W2996507500",
    "https://openalex.org/W4288301883",
    "https://openalex.org/W2913289032",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2977944219",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2998653236",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W3171975879",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W2923014074"
  ],
  "abstract": "The great success of Transformer-based models benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions. In this paper, we propose a self-attention attribution method to interpret the information interactions inside Transformer. We take BERT as an example to conduct extensive studies. Firstly, we apply self-attention attribution to identify the important attention heads, while others can be pruned with marginal performance degradation. Furthermore, we extract the most salient dependencies in each layer to construct an attribution tree, which reveals the hierarchical interactions inside Transformer. Finally, we show that the attribution results can be used as adversarial patterns to implement non-targeted attacks towards BERT.",
  "full_text": "Self-Attention Attribution:\nInterpreting Information Interactions Inside Transformer\nYaru Hao,1,2\u0003 Li Dong,2 Furu Wei,2 Ke Xu1\n1 Beihang University\n2 Microsoft Research\nfhaoyaru@,kexu@nlsde.gbuaa.edu.cn\nflidong1,fuweig@microsoft.com\nAbstract\nThe great success of Transformer-based models beneﬁts from\nthe powerful multi-head self-attention mechanism, which\nlearns token dependencies and encodes contextual informa-\ntion from the input. Prior work strives to attribute model deci-\nsions to individual input features with different saliency mea-\nsures, but they fail to explain how these input features interact\nwith each other to reach predictions. In this paper, we pro-\npose a self-attention attribution method to interpret the infor-\nmation interactions inside Transformer. We take BERT as an\nexample to conduct extensive studies. Firstly, we apply self-\nattention attribution to identify the important attention heads,\nwhile others can be pruned with marginal performance degra-\ndation. Furthermore, we extract the most salient dependencies\nin each layer to construct an attribution tree, which reveals the\nhierarchical interactions inside Transformer. Finally, we show\nthat the attribution results can be used as adversarial patterns\nto implement non-targeted attacks towards BERT.\nIntroduction\nTransformer (Vaswani et al. 2017) is one of state-of-the-art\nNLP architectures. For example, most pre-trained language\nmodels (Devlin et al. 2019; Liu et al. 2019; Dong et al. 2019;\nBao et al. 2020; Clark et al. 2020; Conneau et al. 2020; Chi\net al. 2020a,b) choose stacked Transformer as the backbone\nnetwork. Their great success stimulates broad research on\ninterpreting the internal black-box behaviors. Some prior ef-\nforts aim at analyzing the self-attention weights generated\nby Transformer (Clark et al. 2019; Kovaleva et al. 2019). In\ncontrast, some other work argues that self-attention distribu-\ntions are not directly interpretable (Serrano and Smith 2019;\nJain and Wallace 2019; Brunner et al. 2020). Another line\nof work strives to attribute model decisions back to input to-\nkens (Sundararajan, Taly, and Yan 2017; Shrikumar, Green-\nside, and Kundaje 2017; Binder et al. 2016). However, most\nprevious attribution methods fail on revealing the informa-\ntion interactions between the input words and the composi-\ntional structures learnt by the network.\nTo address the above issues, we propose a self-attention\nattribution method (A TTATTR ) based on integrated gradi-\nent (Sundararajan, Taly, and Yan 2017). We conduct ex-\n\u0003 Contribution during internship at Microsoft Research.\nCopyright c\r2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nperiments for BERT (Devlin et al. 2019) because it is one\nof the most representative Transformer-based models. No-\ntice that our method is general enough, and can be applied\nto other Transformer networks without signiﬁcant modiﬁca-\ntions. Results show that our method well indicates the in-\nformation ﬂow inside Transformer, which makes the self-\nattention mechanism more interpretable.\nFirstly, we identify the most important attention connec-\ntions in each layer using A TTATTR . We ﬁnd that attention\nweights do not always correlate well with their contributions\nto the model prediction. We then introduce a heuristic algo-\nrithm to construct self-attention attribution trees, which dis-\ncovers the information ﬂow inside Transformer. In addition,\na quantitative analysis is applied to justify how much the\nedges of an attribution tree contribute to the ﬁnal prediction.\nNext, we use ATTATTR to identify the most important at-\ntention heads and perform head pruning. The derived algo-\nrithm achieves competitive performance compared with the\nTaylor expansion method (Michel, Levy, and Neubig 2019).\nMoreover, we ﬁnd that the important heads of BERT are\nroughly consistent across different datasets as long as the\ntasks are homogeneous.\nFinally, we extract the interaction patterns that contribute\nmost to the model decision, and use them as adversarial trig-\ngers to attack BERT-based models. We ﬁnd that the ﬁne-\ntuned models tend to over-emphasize some word patterns to\nmake the prediction, which renders the prediction process\nless robust. For example, on the MNLI dataset, adding one\nadversarial pattern into the premise can drop the accuracy\nof entailment from 82:87% to 0:8%. The results show\nthat ATTATTR not only can interpret the model decisions,\nbut also can be used to ﬁnd anomalous patterns from data.\nThe contributions of our work are as follows:\n\u000f We propose to use self-attention attribution to interpret the\ninformation interactions inside Transformer.\n\u000f We conduct extensive studies for BERT. We present how\nto derive interaction trees based on attribution scores,\nwhich visualizes the compositional structures learnt by\nTransformer.\n\u000f We show that the proposed attribution method can be used\nto prune self-attention heads, and construct adversarial\ntriggers.\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n12963\n(a) Attention Score\n (b) Attribution Score\nFigure 1: Attention score (left) and attribution score (right)\nof a single head in BERT. The color is darker for larger val-\nues. The prediction of the sentence from MNLI dataset is\ncontradiction. ATTATTR tends to identify more sparse\nword interactions that contribute to the ﬁnal model decision.\nBackground\nTransformer (Vaswani et al. 2017) Given input tokens\nfxigjxj\ni=1, we pack their word embeddings to a matrix X0 =\n[x1;\u0001\u0001\u0001 ;xjxj]. The stacked L-layer Transformer computes\nthe ﬁnal output via Xl = Transformerl(Xl\u00001);l 2[1;L].\nThe core component of a Transformer block is multi-head\nself-attention. The h-th self-attention head is described as:\nQh = XWQ\nh ; K= XWK\nh ; V= XWV\nh (1)\nAh = softmax(QhK|\nh\npdk\n) (2)\nHh = AttentionHead(X) = AhVh (3)\nwhere Q;K 2Rn\u0002dk , V 2Rn\u0002dv , and the score Ai;j in-\ndicates how much attention token xi puts on xj. There are\nusually multiple attention heads in a Transformer block. Let\njhjdenote the number of attention heads in each layer, the\noutput of multi-head attention is given by MultiH(X) =\n[H1;\u0001\u0001\u0001 ;Hjhj]Wo, where Wo 2Rjhjdv\u0002dx , [\u0001] means con-\ncatenation, and Hi is computed as in Equation (3).\nBERT (Devlin et al. 2019) We conduct all experiments\non BERT, which is one of the most successful applications\nof Transformer. The pretrained language model is based\non bidirectional Transformer, which can be ﬁne-tuned to-\nwards downstream tasks. Notice that our method can also be\napplied to other multi-layer Transformer models with few\nmodiﬁcations. For single input, a special token [CLS] is\nadded to the beginning of the sentence, and another token\n[SEP] is added to the end. For pairwise input, [SEP] is\nalso added as a separator between the two sentences. When\nBERT is ﬁne-tuned on classiﬁcation tasks, a softmax classi-\nﬁer is added on top of the [CLS] token in the last layer to\nmake predictions.\nMethods: Self-Attention Attribution\nFigure 1a shows attention scores of one head in ﬁne-tuned\nBERT. We observe that the attention score matrix is quite\ndense, although only one of twelve heads is plotted. It poses\na huge burden on us to understand how words interact with\neach other within Transformer. Moreover, even if an atten-\ntion score is large, it does not mean the pair of words is im-\nportant to model decisions. In contrast, we aim at attributing\nmodel decisions to self-attention relations, which tends to\nassign higher scores if the interaction contributes more to\nthe ﬁnal prediction.\nGiven input sentence x, let Fx(\u0001) represent the Trans-\nformer model, which takes the attention weight matrix A\n(Equation (2)) as the model input. Inspired by Sundararajan,\nTaly, and Yan (2017), we manipulate the internal attention\nscores \u0016A, and observe the corresponding model dynamics\nFx( \u0016A) to inspect the contribution of word interactions. As\nthe attribution is always targeted for a given inputx, we omit\nit for the simplicity of notations.\nLet us take one Transformer layer as an example to de-\nscribe self-attention attribution. Our goal is to calculate an\nattribution score for each attention connection. For the h-th\nattention head, we compute its attribution score matrix as:\nA= [A1;\u0001\u0001\u0001 ;Ajhj]\nAttrh(A) = Ah \f\nZ 1\n\u000b=0\n@F(\u000bA)\n@Ah\nd\u000b2Rn\u0002n\nwhere \fis element-wise multiplication, Ah 2 Rn\u0002n de-\nnotes the h-th head’s attention weight matrix (Equation (2)),\nand @F(\u000bA)\n@Ah\ncomputes the gradient of model F(\u0001) along Ah.\nThe (i;j)-th element of Attrh(A) is computed for the inter-\naction between input token xi and xj in terms of the h-th\nattention head.\nThe starting point (\u000b = 0 ) of the integration represents\nthat all tokens do not attend to each other in a layer. When\u000b\nchanges from 0 to 1, if the attention connection (i;j) has a\ngreat inﬂuence on the model prediction, its gradient will be\nsalient, so that the integration value will be correspondingly\nlarge. Intuitively, Attrh(A) not only takes attention scores\ninto account, but also considers how sensitive model predic-\ntions are to an attention relation.\nThe attribution score can be efﬁciently computed via\nRiemman approximation of the integration (Sundararajan,\nTaly, and Yan 2017). Speciﬁcally, we sum the gradients\nat points occurring at sufﬁciently small intervals along the\nstraightline path from the zero attention matrix to the origi-\nnal attention weight A:\n~Attrh(A) = Ah\nm \f\nmX\nk=1\n@F( k\nmA)\n@Ah\n(4)\n12964\nFigure 2: Effectiveness analysis of ATTATTR . The blue and\nred lines represent pruning attention heads according to at-\ntribution scores, and attention scores, respectively. The solid\nlines mean the attention heads with the smallest values are\npruned ﬁrst, while the dash lines mean the largest values are\npruned ﬁrst. The results show that ATTATTR better indicates\nthe importance of attention heads.\nwhere m is the number of approximation steps. In our ex-\nperiments, we set mto 20, which performs well in practice.\nFigure 1 is an example about the attention score map\nand the attribution score map of a single head in ﬁne-tuned\nBERT. We demonstrate that larger attention scores do not\nmean more contribution to the ﬁnal prediction. The atten-\ntion scores between the [SEP] token and other tokens are\nrelatively large, but they obtain little attribution scores. The\nprediction of the contradiction class attributes most to\nthe connections between “don’t” in the ﬁrst segment and “I\nknow” in the second segment, which is more explainable.\nExperiments\nWe employ BERT-base-cased (Devlin et al. 2019) in our ex-\nperiments. The number of BERT layers jlj= 12, the num-\nber of attention heads in each layer jhj= 12, and the size\nof hidden embeddings jhjdv = 768. For a sequence of 128\ntokens, the attribution time of the BERT-base model takes\nabout one second on an Nvidia-v100 GPU card. Moreover,\nthe computation can be parallelized by batching multiple in-\nput examples to increase throughput.\nWe perform BERT ﬁne-tuning and conduct experiments\non four classiﬁcation datasets. MNLI (Williams, Nangia,\nand Bowman 2018) Multi-genre Natural Language Infer-\nence is to predict whether a premise entails the hypoth-\nesis (entailment), contradicts the given hypothesis (con-\ntradiction), or neither (neutral). RTE (Dagan, Glickman,\nand Magnini 2006; Bar-Haim et al. 2006; Giampiccolo\net al. 2007; Bentivogli et al. 2009) Recognizing Textual\nEntailment comes from a series of annual textual entail-\nment challenges. SST-2 (Socher et al. 2013) Stanford Sen-\ntiment Treebank is to predict the polarity of a given sen-\ntence. MRPC (Dolan and Brockett 2005) Microsoft Re-\nsearch Paraphrase Corpus is to predict whether the pairwise\nsentences are semantically equivalent. We use the same data\nsplit as in (Wang et al. 2019). The accuracy metric is used for\nevaluation. When ﬁne-tuning BERT, we follow the settings\nand the hyper-parameters suggested in (Devlin et al. 2019).\nEffectiveness Analysis\nWe conduct a quantitative analysis to justify the self-\nattention edges with larger attribution scores contribute more\nto the model decision. We prune the attention heads incre-\nmentally in each layer according to their attribution scores\nwith respect to the golden label and record the performance\nchange. We also establish a baseline that prunes attention\nheads with their average attention scores for comparison.\nExperimental results are presented in Figure 2, we ob-\nserve that pruning heads with attributions scores conduces\nmore salient changes on the performance. Pruning only two\nheads within every layer with the top-2attribution scores can\ncause an extreme decrease in the model accuracy. In con-\ntrast, retaining them helps the model to achieve nearly 97%\naccuracy. Even if only two heads are retained in each layer,\nthe model can still have a strong performance. Compared\nwith attribution scores, pruning heads with average atten-\ntion scores are less remarkable on the performance change,\nwhich proves the effectiveness of our method.\nUse Case 1: Attention Head Pruning\nAccording to the previous section, only a small part of atten-\ntion heads contribute to the ﬁnal prediction, while others are\nless helpful. This leads us to the research about identifying\nand pruning the unimportant attention heads.\nHead Importance The attribution scores indicate how\nmuch a self-attention edge attributes to the ﬁnal model deci-\nsion. We deﬁne the importance of an attention head as:\nIh = Ex[max(Attrh(A))] (5)\nwhere xrepresents the examples sampled from the held-out\nset, and max(Attrh(A)) is the maximum attribution value\nof the h-th attention head. Notice that the attribution value\nof a head is computed with respect to the probability of the\ngolden label on a held-out set.\nWe compare our method with other importance metrics\nbased on the accuracy difference and the Taylor expan-\nsion, which are both proposed in (Michel, Levy, and Neu-\nbig 2019). The accuracy difference of an attention head is\nthe accuracy margin before and after pruning the head. The\nmethod based on the Taylor expansion deﬁnes the impor-\ntance of an attention head as:\nIh = Ex\n\f\f\f\fA|\nh\n@L(x)\n@Ah\n\f\f\f\f (6)\nwhere L(x) is the loss function of example x, and Ah is the\nattention score of the h-th head as in Equation (2).\nFor all three methods, we calculate Ih on 200 examples\nsampled from the held-out dataset. Then we sort all the\nheads according to the importance metrics. The less impor-\ntant heads are ﬁrst pruned.\n12965\nFigure 3: Evaluation accuracy as a function of head prun-\ning proportion. The attention heads are pruned according to\nthe accuracy difference (baseline; dash yellow), the Taylor\nexpansion method (Michel, Levy, and Neubig 2019; solid\nred), and ATTATTR (this work; solid blue).\nEvaluation Results of Head PruningFigure 3 describes\nthe evaluation results of head pruning. The solid red lines\nrepresent pruning heads based on our method ATTATTR . We\nobserve that pruning head with attribution score is much bet-\nter than the baseline of accuracy difference.\nMoreover, the pruning performance of ATTATTR is com-\npetitive with the Taylor expansion method, although A T-\nTATTR is not speciﬁcally designed for attention head prun-\ning. The result show that attention attribution successfully\nindicates the importance of interactions inside Transformer.\nOn the MNLI dataset, when only 10% attention heads are re-\ntained, our method can still achieve approximately 60% ac-\ncuracy, while the accuracy of the Taylor expansion method\nis about 40%.\nUniversality of Important Heads Previous results are\nperformed on speciﬁc datasets respectively. Besides iden-\ntifying the most important heads of Transformer, we in-\nvestigate whether the important heads are consistent across\ndifferent datasets and tasks. The correlation of attribution\nscores of attention heads between two different datasets is\nmeasured by the Pearson coefﬁcient. As described in Fig-\nure 4, as long as the tasks are homogeneous (i.e., solv-\ning similar problems), the important attention heads are\nhighly correlated. The datasets RTE, MPRC, and MNLI\nare about entailment detection, where the important self-\nattention heads (i.e., with large attribution scores) of BERT\nare roughly consistent across the datasets. In contrast, the\ndataset SST-2 is sentiment classiﬁcation. We ﬁnd that the\nimportant heads on SST-2 are different from the ones on\nRTE, and MRPC. In conclusion, the same subset of atten-\ntion heads is ﬁne-tuned for similar tasks.\nFigure 4:\nCorrelation of attribution scores of different atten-\ntion heads between datasets. Each point represents the attri-\nbution scores of a single attention head on two datasets.\nUse Case 2: Visualizing Information Flow Inside\nTransformer\nWe propose a heuristic algorithm to construct attribution\ntrees, the results discover the information ﬂow inside Trans-\nformer, so that we can know the interactions between the\ninput words and how they attribute to the ﬁnal predic-\ntion. Such visualization can provide insights to understand\nwhat dependencies Transformer tends to capture. The post-\ninterpretation helps us to debug models and training data.\nThe problem is a trade-off between maximizing the sum-\nmation of attribution scores and minimizing the number of\nedges in the tree. We present a greedy top-down algorithm to\nefﬁciently construct attribution trees. Moreover, we conduct\na quantitative analysis to verify the effectiveness.\nAttribution Tree Construction After computing self-\nattention attribution scores, we can know the interactions be-\ntween the input words in each layer and how they attribute\nto the ﬁnal prediction. We then propose an attribution tree\nconstruction algorithm to aggregate the interactions. In other\nwords, we build a tree to indicate how information ﬂows\nfrom input tokens to the ﬁnal predictions. We argue that such\nvisualization can provide insights to understand what depen-\ndencies Transformer tends to capture.\nFor each layer l, we ﬁrst calculate self-attention attribu-\ntion scores of different heads. Then we sum them up over\nthe heads, and use the results as the l-th layer’s attribution:\nAttr(Al) =\njhjX\nh=1\nAttrh(Al) = [al\ni;j]n\u0002n\nwhere larger al\ni;j indicates more interaction between xi and\nxj in the l-th layer in terms of the ﬁnal model predictions.\n12966\n(a) Example from MNLI\n (b) Example from SST-2\nFigure 5: Examples of attribution trees. (a) is from MNLI, which is predicted as entailment by BERT. (b) is from SST-2,\nwhich is predicted as positive by BERT. The grey words from the inputs do not appear in the attribution trees.\n(a) MNLI\n (b) SST-2\nFigure 6: Distance distribution of interactions extracted by\nthe attribution tree in each layers.\nThe construction of attribution trees is a trade-off between\nmaximizing the summation of attribution scores and min-\nimizing the number of edges in the tree. The objective is\ndeﬁned as:\nTree = arg max\nfElgjlj\nl=1\njljX\nl=1\nX\n(i;j)2El\nal\ni;j \u0000\u0015\njljX\nl=1\njElj;\nEl \u001af( i;j)j al\ni;j\nmax(Attr(Al)) >\u001c g\nwhere jEljrepresents the number of edges in the l-th layer,\n\u0015is a trade-off weight, and the threshold \u001c is used to ﬁlter\nthe interactions with relatively large attribution scores.\nRather than solving a combinatorial optimization prob-\nlem, we use a heuristic top-down method to add these edges\nto the attribution tree. The process is detailed in Algorithm 1.\nSettings We set \u001c = 0 :4 for layers l < 12. The larger\n\u001c tends to generate more simpliﬁed trees, which contains\nthe more important part of the information ﬂow. Because\nthe special token [CLS] is the terminal of the information\nﬂow for classiﬁcation tasks, we set \u001c to 0 for the last layer.\nWe observe that almost all connections between[CLS] and\nother tokens in the last layer have positive attribution scores\nwith respect to model predictions.\nCase Studies As shown in Figure 5, the two attribution\ntrees are from MNLI and SST-2, respectively. The attribu-\ntion tree Figure 5a is generated from MNLI, whose golden\nlabel is entailment. At the bottom of Figure 5a, we ﬁnd\nthat the interactions are more local, and most information\nﬂows are concentrated within a single sentence. The in-\nformation is hierarchically aggregated to “supplement ” and\n“extra” in each sentence. Then the “supplement ” token ag-\ngregates the information in the ﬁrst sentence and “add some-\nthing extra” in the second sentence, these two parts “supple-\nment” and “add something extra” have strong semantic rele-\nvance. Finally, all the information ﬂows to the terminal token\n[CLS] to make the prediction entailment. The attribu-\ntion tree interprets how the input words interacts with each\nother, and reach the ﬁnal prediction, which makes model de-\ncisions more interpretable.\nFigure 5b is an example from SST-2, whose golden label\nis positive, correctly predicted by the model. From Fig-\nure 5b, we observe that information in the ﬁrst part of the\nsentence “seldom has a movie so closely” is aggregated to\nthe “has” token. Similarly, information in the other part of\nthe sentence “the spirit of a man and his work ” ﬂows to the\n“spirit” token, which has strong positive emotional tenden-\ncies. Finally, with the feature interactions, all information\naggregates to the verb “matched ”, which gives us a better\nunderstanding of why the model makes the speciﬁc decision.\nReceptive Field The self-attention mechanism is sup-\nposed to have the ability to capture long-range dependen-\n12967\nAlgorithm 1Attribution Tree Construction\nInput: [al\ni;j]n\u0002n: Attribution scores\nfElgjlj\nl=1: Retained attribution edges\nOutput: V;E: Node set and edge set of Attr tree\n1: .Initialize the state of all tokens\n2: for i n;\u0001\u0001\u0001 ;1 do\n3: Statei  NotAppear\n4: .Choose the top node of the attribution tree\n5: [AttrAlli]n = Pjlj\nl=1\nPn\nj=1;j6=ial\ni;j\n6: TopNode = arg max([AttrAlli]n)\n7: V f TopNodeg; StateTopNode  Appear\n8: .Build the attribution tree downward\n9: for l jlj\u00001;\u0001\u0001\u0001 ;1 do\n10: for (i;j)l\ni6=j 2El do\n11: if Statei is Appear and\n12: Statej is NotAppear then\n13: E E Sf(i;j)g; V V Sfjg\n14: Statei  Fixed; Statej  Appear\n15: if Statei is Fixed and\n16: Statej is NotAppear then\n17: E E Sf(i;j)g; V V Sfjg\n18: Statej  Appear\n19: .Add the terminal of the information ﬂow\n20: V f [CLS]g\n21: for j  n;\u0001\u0001\u0001 ;1 do\n22: if Statej 2fAppear; Fixedgthen\n23: E E Sf([CLS];j)g\n24: return fV;Eg\ncies. In order to better understand the layer-wise effective\nreceptive ﬁeld in Transformer, we plot the distance distri-\nbution of interactions extracted by the attribution tree. As\nshown in Figure 6, we observe that for the paired input of\nMNLI, the effective receptive ﬁeld is relatively local in the\nﬁrst two layers and the 6-8th layers, while are more broad in\nthe top three layers. For the single input of SST-2, the effec-\ntive receptive ﬁeld is monotonically increasing along with\nthe layer number. Generally, the effective receptive ﬁeld in\nthe second layer is more restricted, while the latter layers\nextract more broad dependencies. Moreover, for pairwise-\ninput tasks (such as MNLI), the results indicate that Trans-\nformer models tend to ﬁrst conduct local encoding and then\nlearn to match between the pair of input sentences, which is\ndifferent with training from scratch (Bao et al. 2019).\nUse Case 3: Adversarial Attack\nThe model decision attributes more to the attention con-\nnections with larger attribution scores. We observe that the\nmodel tends to over-emphasize some individual patterns\nto make the prediction, while omitting most of the input.\nWe then use the over-conﬁdent patterns as adversarial trig-\ngers (Wallace et al. 2019) to attack the BERT model.\nTrigger Construction We extract the attention dependen-\ncies with the largest attribution scores across different layers\nFigure 7: We use ATTATTR to extract the trigger (i.e., high-\nlighted word patterns) from the MNLI instance that is la-\nbeled as contradict. After adding the adversarial trigger\nto the examples in other categories, the model predictions\nﬂip from neutral and entailment to contradict.\n(i.e., maxL\nl=1 fal\ni;jg) from the input, and employ these pat-\nterns as the adversarial triggers. During the attack, the ad-\nversarial triggers are inserted into the test input at the same\nrelative position and segment as in the original sentence.\nThe speciﬁc attack process is shown in Figure 7. The two\npatterns “ﬂoods-ice” and “Iowa-Florida” contribute most\nto the prediction contradict in the source sentence.\nNext we employ them as the trigger to attack other exam-\nples, the model predictions ﬂip from both neutral and\nentailment to contradict. Our attack method relies\non attribution scores, which utilizes the gradient informa-\ntion, therefore it belongs to white-box non-targeted attacks.\nWe extract the dependencies with the largest attribution\nscores as the adversarial triggers from 3,000 input exam-\nples. Each trigger contains less than ﬁve tokens. The score of\na trigger is deﬁned as the maximum attribution value iden-\ntiﬁed within it. When attacking the BERT model on SST-\n2, we use a lexicon 1 to blacklist the words with the obvi-\nous emotional tendencies (such as “disgust” for negative\ntriggers). The adversarial triggers are inserted into the attack\ntext at the same relative position as in the original sentence.\nResults of Attack We conduct the adversarial attacks on\nmultiple datasets. The top-3 adversarial triggers for MNLI\nand SST-2 are listed in Table 1. We report the attack results\nwith these triggers in Table 2. For MNLI, after inserting the\n1www.cs.uic.edu/\u0018liub/FBS/sentiment-analysis.html\n12968\nMNLI SST-2\ncontradict entailment neutral positive negative\nTrigger1 falso, sometimes, Sg f with, mathg fﬂoods, Iowa,\nice, Floridag f[CLS], nowhereg f remove, ##ﬁesg\nTrigger2 fnobody, should, notg f light, morningg f never, butg fbut, has, nothingg fnot, alien, ##ateg\nTrigger3 fdo, well, Usually, butg fﬂoods, Iowa,\nice, Floridag\nfMassachusetts,\nMexicog foffers, littleg f ##reshing, ##lyg\nTable 1: Top-3adversarial triggers for the MNLI and SST-2 datasets. The tokens are inserted into input sentences at the speciﬁc\npositions for non-targeted attacks. We omit the tokens’ positions in the table for brevity.\nMNLI SST-2 MRPC RTE\ncontra- entail- neutral pos- neg- equal not- entail- not-\nBaseline 84.94 82.87 82.00 92.79 91.82 90.32 72.87 72.60 65.65\nTrigger1 34.17 0.80 34.77 54.95 72.20 29.39 51.94 9.59 59.54\nTrigger2 39.81 1.83 47.36 59.68 74.53 32.62 55.04 11.64 62.50\nTrigger3 41.83 2.99 51.49 70.50 77.80 36.56 58.91 13.70 62.60\nAvg. \u0001 -46.34 -80.00 -37.46 -31.08 -16.98 -57.46 -17.57 -60.96 -12.31\nTable 2: Attack results of the top-3 triggers. We abbreviate not equal and not entailment to not- for MRPC and\nRTE, respectively. The baseline represents the original accuracy of model on each category.\nwords (“with”, and “math”) to the second segment of the\ninput sentences, the model accuracy of the entailment\nclass drops from 82.87% to 0.8%. For SST-2, adding the\ntop-1 adversarial trigger to the input causes nearly 50%\npositive examples to be misclassiﬁed.\nAnalysis of Triggers For both MNLI and RTE, the\nentailment class is more vulnerable than others, because\nthe current models and data seem to heavily rely on word\nmatching, which would result in spurious patterns. More-\nover, we also observe that the trigger is sensitive to the in-\nsertion order and the relative position in the sentence, which\nexhibits the anomalous behaviors of the model, i.e., over-\nrelying on these adversarial triggers to make the prediction.\nRelated Work\nPrevious work has explored attributing predictions to the\ninput features with various saliency measures, such as\nDeepLift (Shrikumar, Greenside, and Kundaje 2017), layer-\nwise relevance propagation (Binder et al. 2016), and Inte-\ngrated Gradients (IG; Sundararajan, Taly, and Yan 2017).\nSpeciﬁc to the NLP domain, Murdoch and Szlam (2017)\nintroduce a decomposition method to track the word impor-\ntance in LSTM (Hochreiter and Schmidhuber 1997). Mur-\ndoch, Liu, and Yu (2018) extend the above method to con-\ntextual decomposition in order to capture the contributions\nof word combinations. Another strand of previous work\ngenerates the hierarchical explanations, which aims at re-\nvealing how the features are composed together (Jin et al.\n2020; Chen, Zheng, and Ji 2020). However, they both de-\ntect interaction within contiguous chunk of input tokens. The\nattention mechanism (Bahdanau, Cho, and Bengio 2015)\nrises another line of work. The attention weights generated\nfrom the model indicate the dependency between two words\nintuitively, but Jain and Wallace (2019) and Serrano and\nSmith (2019) draw the same conclusion that they largely do\nnot provide meaningful explanations for model predictions.\nHowever, Wiegreffe and Pinter (2019) propose several alter-\nnative tests and conclude that prior work does not disprove\nthe usefulness of attention mechanisms for interpretability.\nFurthermore, Ghaeini, Fern, and Tadepalli (2018) aim at in-\nterpreting the intermediate layers of NLI models by visual-\nizing the saliency of attention and LSTM gating signals.\nFor Transformer, Clark et al. (2019) propose a attention-\nbased visualization method and a probing classiﬁer to ex-\nplain the behaviors of BERT (Devlin et al. 2019). Brunner\net al. (2020) study the identiﬁability of attention weights of\nBERT, which shows that self-attention distributions are not\ndirectly interpretable. Moreover, some work extracts the la-\ntent syntactic trees from hidden representations (Hewitt and\nManning 2019; Rosa and Marecek 2019; Coenen et al. 2019)\nand attention weights (Marecek and Rosa 2019).\nConclusion\nWe propose self-attention attribution (ATTATTR ), which in-\nterprets the information interactions inside Transformer and\nmakes the self-attention mechanism more explainable. First,\nwe conduct a quantitative analysis to justify the effective-\nness of A TTATTR . Moreover, we use the proposed method\nto identify the most important attention heads, which leads\nto a new head pruning algorithm. We then use the attribution\nscores to derive the interaction trees, which visualizes the\ninformation ﬂow of Transformer. We also understand the re-\nceptive ﬁeld in Transformer. Finally, we show that ATTATTR\ncan also be employed to construct adversarial triggers to im-\nplement non-targeted attacks.\n12969\nAcknowledgements\nThe work was partially supported by National Natural Sci-\nence Foundation of China (NSFC) [Grant No. 61421003].\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural Ma-\nchine Translation by Jointly Learning to Align and Trans-\nlate. In 3rd International Conference on Learning Repre-\nsentations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nBao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Cui, L.;\nPiao, S.; and Zhou, M. 2019. Inspecting Uniﬁcation of En-\ncoding and Matching with Transformer: A Case Study of\nMachine Reading Comprehension. In Proceedings of the\n2nd Workshop on Machine Reading for Question Answer-\ning, 14–18. Association for Computational Linguistics.\nBao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu,\nX.; Wang, Y .; Piao, S.; Gao, J.; Zhou, M.; and Hon, H.-\nW. 2020. UniLMv2: Pseudo-Masked Language Models\nfor Uniﬁed Language Model Pre-Training. arXiv preprint\narXiv:2002.12804 .\nBar-Haim, R.; Dagan, I.; Dolan, B.; Ferro, L.; and Giampic-\ncolo, D. 2006. The second PASCAL recognising textual en-\ntailment challenge. In Proceedings of the Second PASCAL\nChallenges Workshop on Recognising Textual Entailment.\nBentivogli, L.; Dagan, I.; Dang, H. T.; Giampiccolo, D.; and\nMagnini, B. 2009. The Fifth PASCAL Recognizing Textual\nEntailment Challenge. In In Proc Text Analysis Conference.\nBinder, A.; Montavon, G.; Bach, S.; M¨uller, K.; and Samek,\nW. 2016. Layer-wise Relevance Propagation for Neu-\nral Networks with Local Renormalization Layers. CoRR\nabs/1604.00825.\nBrunner, G.; Liu, Y .; Pascual, D.; Richter, O.; Ciaramita, M.;\nand Wattenhofer, R. 2020. On Identiﬁability in Transform-\ners. In International Conference on Learning Representa-\ntions.\nChen, H.; Zheng, G.; and Ji, Y . 2020. Generating Hierarchi-\ncal Explanations on Text Classiﬁcation via Feature Interac-\ntion Detection. In ACL.\nChi, Z.; Dong, L.; Wei, F.; Wang, W.; Mao, X.; and Huang,\nH. 2020a. Cross-Lingual Natural Language Generation via\nPre-Training. In The Thirty-Fourth AAAI Conference on Ar-\ntiﬁcial Intelligence, 7570–7577. AAAI Press.\nChi, Z.; Dong, L.; Wei, F.; Yang, N.; Singhal, S.; Wang,\nW.; Song, X.; Mao, X.; Huang, H.; and Zhou, M.\n2020b. InfoXLM: An Information-Theoretic Framework\nfor Cross-Lingual Language Model Pre-Training. CoRR\nabs/2007.07834.\nClark, K.; Khandelwal, U.; Levy, O.; and Manning, C. D.\n2019. What Does BERT Look At? An Analysis of BERT’s\nAttention. CoRR abs/1906.04341.\nClark, K.; Luong, M.-T.; Le, Q. V .; and Manning, C. D.\n2020. ELECTRA: Pre-training Text Encoders as Discrim-\ninators Rather Than Generators. In ICLR.\nCoenen, A.; Reif, E.; Yuan, A.; Kim, B.; Pearce, A.; Vi´egas,\nF. B.; and Wattenberg, M. 2019. Visualizing and Measuring\nthe Geometry of BERT. CoRR abs/1906.02715.\nConneau, A.; Khandelwal, K.; Goyal, N.; Chaudhary, V .;\nWenzek, G.; Guzm ´an, F.; Grave, E.; Ott, M.; Zettlemoyer,\nL.; and Stoyanov, V . 2020. Unsupervised Cross-lingual Rep-\nresentation Learning at Scale. InProceedings of the 58th An-\nnual Meeting of the Association for Computational Linguis-\ntics, 8440–8451. Association for Computational Linguistics.\nDagan, I.; Glickman, O.; and Magnini, B. 2006. The PAS-\nCAL Recognising Textual Entailment Challenge. In Pro-\nceedings of the First International Conference on Machine\nLearning Challenges: Evaluating Predictive Uncertainty Vi-\nsual Object Classiﬁcation, and Recognizing Textual Entail-\nment, MLCW’05, 177–190. Berlin, Heidelberg: Springer-\nVerlag. ISBN 3-540-33427-0, 978-3-540-33427-9.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1, 4171–4186. Minneapolis, Minnesota: Asso-\nciation for Computational Linguistics.\nDolan, W. B.; and Brockett, C. 2005. Automatically con-\nstructing a corpus of sentential paraphrases. In Proceed-\nings of the Third International Workshop on Paraphrasing\n(IWP2005).\nDong, L.; Yang, N.; Wang, W.; Wei, F.; Liu, X.; Wang, Y .;\nGao, J.; Zhou, M.; and Hon, H.-W. 2019. Uniﬁed Language\nModel Pre-training for Natural Language Understanding\nand Generation. In 33rd Conference on Neural Information\nProcessing Systems (NeurIPS 2019).\nGhaeini, R.; Fern, X. Z.; and Tadepalli, P. 2018. In-\nterpreting Recurrent and Attention-Based Neural Models:\na Case Study on Natural Language Inference. CoRR\nabs/1808.03894.\nGiampiccolo, D.; Magnini, B.; Dagan, I.; and Dolan, B.\n2007. The Third PASCAL Recognizing Textual Entailment\nChallenge. In Proceedings of the ACL-PASCAL Workshop\non Textual Entailment and Paraphrasing, 1–9. Prague: As-\nsociation for Computational Linguistics.\nHewitt, J.; and Manning, C. D. 2019. A Structural Probe\nfor Finding Syntax in Word Representations. In Proceed-\nings of the 2019 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human\nLanguage Technologies, Volume 1, 4129–4138. Minneapo-\nlis, Minnesota: Association for Computational Linguistics.\nHochreiter, S.; and Schmidhuber, J. 1997. Long Short-Term\nMemory. Neural Computation 9: 1735–1780. ISSN 0899-\n7667.\nJain, S.; and Wallace, B. C. 2019. Attention is not Explana-\ntion. CoRR abs/1902.10186.\nJin, X.; Wei, Z.; Du, J.; Xue, X.; and Ren, X. 2020. Towards\nHierarchical Importance Attribution: Explaining Composi-\ntional Semantics for Neural Sequence Models. In ICLR.\n12970\nKovaleva, O.; Romanov, A.; Rogers, A.; and Rumshisky,\nA. 2019. Revealing the Dark Secrets of BERT. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP), 4364–4373. Hong Kong, China: Asso-\nciation for Computational Linguistics.\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. RoBERTa: A Robustly Optimized BERT Pretraining\nApproach. arXiv preprint arXiv:1907.11692 .\nMarecek, D.; and Rosa, R. 2019. From Balustrades to Pierre\nVinken: Looking for Syntax in Transformer Self-Attentions.\nCoRR abs/1906.01958.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are Sixteen\nHeads Really Better than One? CoRR abs/1905.10650.\nMurdoch, W. J.; Liu, P. J.; and Yu, B. 2018. Beyond Word\nImportance: Contextual Decomposition to Extract Interac-\ntions from LSTMs. CoRR abs/1801.05453.\nMurdoch, W. J.; and Szlam, A. 2017. Automatic Rule Ex-\ntraction from Long Short Term Memory Networks. CoRR\nabs/1702.02540.\nRosa, R.; and Marecek, D. 2019. Inducing Syntactic Trees\nfrom BERT Representations. CoRR abs/1906.11511.\nSerrano, S.; and Smith, N. A. 2019. Is Attention Inter-\npretable? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2931–2951. Flo-\nrence, Italy: Association for Computational Linguistics.\nShrikumar, A.; Greenside, P.; and Kundaje, A. 2017. Learn-\ning Important Features Through Propagating Activation Dif-\nferences. CoRR abs/1704.02685.\nSocher, R.; Perelygin, A.; Wu, J.; Chuang, J.; Manning,\nC. D.; Ng, A.; and Potts, C. 2013. Recursive Deep Models\nfor Semantic Compositionality Over a Sentiment Treebank.\nIn Proceedings of the 2013 Conference on Empirical Meth-\nods in Natural Language Processing, 1631–1642. Seattle,\nWashington, USA: Association for Computational Linguis-\ntics.\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic\nAttribution for Deep Networks. CoRR abs/1703.01365.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Advances in Neural Information\nProcessing Systems 30, 5998–6008. Curran Associates, Inc.\nWallace, E.; Feng, S.; Kandpal, N.; Gardner, M.; and Singh,\nS. 2019. Universal Adversarial Triggers for Attacking\nand Analyzing NLP. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP), 2153–2162. Hong\nKong, China: Association for Computational Linguistics.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2019. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding. In\nInternational Conference on Learning Representations.\nWiegreffe, S.; and Pinter, Y . 2019. Attention is not not Ex-\nplanation. In EMNLP-IJCNLP, 11–20. Hong Kong, China:\nAssociation for Computational Linguistics.\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), 1112–1122. New Orleans, Louisiana:\nAssociation for Computational Linguistics.\n12971",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7137619256973267
    },
    {
      "name": "Attribution",
      "score": 0.6910113096237183
    },
    {
      "name": "Transformer",
      "score": 0.6878464221954346
    },
    {
      "name": "Salient",
      "score": 0.6216034889221191
    },
    {
      "name": "Security token",
      "score": 0.5299723148345947
    },
    {
      "name": "Adversarial system",
      "score": 0.5292165875434875
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45306238532066345
    },
    {
      "name": "Machine learning",
      "score": 0.33758455514907837
    },
    {
      "name": "Psychology",
      "score": 0.20831656455993652
    },
    {
      "name": "Social psychology",
      "score": 0.12562158703804016
    },
    {
      "name": "Computer security",
      "score": 0.10802626609802246
    },
    {
      "name": "Engineering",
      "score": 0.09089314937591553
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I82880672",
      "name": "Beihang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210124949",
      "name": "Microsoft Research (India)",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ]
}