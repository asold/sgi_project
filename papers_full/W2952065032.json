{
    "title": "Context-specific Language Modeling for Human Trafficking Detection from Online Advertisements",
    "url": "https://openalex.org/W2952065032",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2523741293",
            "name": "Saeideh Shahrokh Esfahani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4269269884",
            "name": "Michael J. Cafarella",
            "affiliations": [
                "University of Michigan–Ann Arbor"
            ]
        },
        {
            "id": "https://openalex.org/A2245271040",
            "name": "Maziyar Baran Pouyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2517446630",
            "name": "Gregory DeAngelo",
            "affiliations": [
                "Accenture (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2307980829",
            "name": "Elena Eneva",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2949486650",
            "name": "Andy E. Fano",
            "affiliations": [
                "Accenture (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2787349793",
        "https://openalex.org/W1983719983",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2612390875",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W1880262756",
        "https://openalex.org/W2296719434",
        "https://openalex.org/W2222863356",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2112763662",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W1549786592"
    ],
    "abstract": "Saeideh Shahrokh Esfahani, Michael J. Cafarella, Maziyar Baran Pouyan, Gregory DeAngelo, Elena Eneva, Andy E. Fano. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 2019.",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1180–1184\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n1180\nContext-speciﬁc language modeling for human trafﬁcking detection from\nonline advertisements\nSaeideh Shahrokh Esfahani\nAccenture Technology Labs\nSan Francisco, CA\nsaeideh.shahrokh@accenture.com\nMichael J. Cafarella\nDepartment of Computer Science\nUniversity of Michigan\nmichjc@umich.edu\nMaziyar Baran Pouyan\nAccenture Technology Labs\nSan Francisco, CA\nmaziyar.baran.pouyan@accenture\nGregory J. DeAngelo\nDepartment of Economics\nClaremont Graduate University\ngregory.deangelo@cgu.edu\nElena Eneva\nAccenture Technology Labs\nSan Francisco, CA\nelena.eneva@accenture.com\nAndrew E. Fano\nAccenture Technology Labs\nSan Francisco, CA\nandrew.e.fano@accenture.com\nAbstract\nHuman trafﬁcking is a worldwide crisis. Traf-\nﬁckers exploit their victims by anonymously\noffering sexual services through online adver-\ntisements. These ads often contain clues that\nlaw enforcement can use to separate out po-\ntential trafﬁcking cases from volunteer sex ad-\nvertisements. The problem is that the sheer\nvolume of ads is too overwhelming for man-\nual processing. Ideally, a centralized semi-\nautomated tool can be used to assist law en-\nforcement agencies with this task. Here, we\npresent an approach using natural language\nprocessing to identify trafﬁcking ads on these\nwebsites. We propose a classiﬁer by integrat-\ning multiple text feature sets, including the\npublicly available pre-trained textual language\nmodel Bi-directional Encoder Representation\nfrom transformers (BERT). In this paper, we\ndemonstrate that a classiﬁer using this com-\nposite feature set has signiﬁcantly better per-\nformance compared to any single feature set\nalone.\n1 Introduction\nIn 2013, the Global Slavery Index reported that\n30 million individuals were living in involuntary\nservitude. Another estimation found that 600,000\nwomen are trafﬁcked in the sex industry per year\nwith the United States being the second most\npopular destination for these individuals (Kara,\n2009); (Schauer and Wheaton, 2006). In the last\ndecade, it has become more difﬁcult for law en-\nforcement (LE) to trace trafﬁckers as they have\nbegun to take increasing advantage of online ad-\nvertisement platforms for sexual services to solicit\nclients and become less visible. LE is capable of\ntracking the posted ads and mining such data to\ndetect trafﬁcking victims. However, the large vol-\nume of online unstructured data, the high degree\nof similarity of ads (Figure 1), and the lack of an\nautomated approach in detecting suspicious activ-\nities through advertisements present obstacles for\nLE to independently develop methods for survey-\ning these criminal activities. Sex trafﬁcking ad-\nvertisements are unique texts. They have incor-\nrect grammatical structures and misspellings, and\nare enriched with unconventional words, abbre-\nviations, and emojis. Oftentimes the author uses\nemojis and emoticons to convey messages to a po-\ntential customer. In particular these types of ad-\nvertisements may also contain equivocal words,\ne.g., roses as a substitute for dollars. Addition-\nally, dominant keywords from these online ads\ncontinuously evolve as trafﬁckers and consent-\ning sex workers alike seek to evade prosecution.\nWhile previous researchers have tried to develop\nautomated systems to detect trafﬁcking advertise-\nments, this has proved an enormous challenge for\nnatural language processing and machine learn-\ning. In (Whitney et al., 2018), Whitney and col-\nleagues propose to track the use of emojis and\ntheir signiﬁcance in online sex ads as a poten-\ntial indicator of trafﬁcking. This team processed\nemojis to determine the meaning of them used\n1181\n(a)\nClose your eyes and imagine sliding into a warm flowing river of relaxation as I slowly pull and push your worries away. I want you here with me. Satisfy my need to please you now.Call Lisa xxx-xxxx-xxxx\n(A)\n(b)\nHi gentlemen,Meet xxxxbeauty Annie, She is 5\\'8, very slim, honey blonde hair, gorgeous long legs. Very sexy, friendly and engaging.Call xxx-xxxx-xxxxto schedule your visit. Xo Xo,See u soon\n(B)\nFigure 1: Two examples of online sex ads describ-\ning (a) a trafﬁcking victim and (b) a non-trafﬁcked\nprovider, selected from our labeled ads.\nin a sample of online ads, as indicated by inter-\nviews with law enforcement ofﬁcials and individ-\nuals combating human trafﬁcking. Taking a differ-\nent approach, Tong, Zadeh, and colleagues (Tong\net al., 2017) collaborated with LE ofﬁcials and an-\nnotated 10,000 ads. With these annotated texts,\nthey proposed the use of deep multimodal models\nto reach the accuracy of LE ofﬁcials in identifying\nsuspicious ads. Szekely and colleagues (Szekely\net al., 2015) created a large generic knowledge\ngraph from a large database of online sexual ads\nthat allows for visualization and querying data.\nIn this paper, we present part of an ongoing\nproject. Unlike previous studies, we tested our\nmethod on a relatively large number of ads labeled\nbased on the corresponding phone number rather\nthan human interpretation of the text itself. In the\nfollowing sections, we propose a method relying\non extracting feature sets from ads to quantify their\ncontext. We later use these feature sets in several\npredictive models to ﬂag suspicious ads. We also\ninvestigate the performance of a newly released\npre-trained language model called the Bidirec-\ntional Encoder Representation from Transformers\n(BERT) (Devlin et al., 2018) to assess its power in\nanalyzing this type of unstructured data.\n2 Advertisement Annotation\nWe created a dataset of advertisement texts by\ncrawling thousands of ads extracted from various\nadult websites in 2017. We then performed our\nanalysis to a subset, only including the data from\nJanuary, February and March of 2017. In order\nto annotate the ads in our dataset, we further ex-\ntracted phone numbers from these ads leading to\na set of more than 3 million distinct phone num-\nbers. We then used a database consisting of phone\nnumbers associated with trafﬁcking victims, con-\nstructed in conjunction with human trafﬁcking do-\nmain experts without direct reference to the ad-\nvertising texts. Afterwards, we created a labeled\ndata set by ﬁnding phone numbers that appear\nin both sets. The overlapping set contains 6,387\nphone numbers, which we used to label as traf-\nﬁcking ads (i.e., the positive label in our preci-\nsion/recall analysis). We limited our analysis to\ntwo websites, Backpage and Eroticmugshots, with\n4385 ads. We selected non-trafﬁcking’s ad exam-\nples by randomly sub-sampling from the remain-\ning ads (i.e. not labeled as trafﬁcking) and treated\nthem as negative examples to make a balanced\n10K dataset. We assumed a very low prevalence\nof trafﬁcking ads (less than 5%) in our initial set\n(≈ 3 million phones). We discuss this decision\nlater in the paper.\nAfter choosing approximately 10K ads, we in-\nvestigated the basic characteristics of the two la-\nbels. The median lengths of ads, including white\nspaces, are 538 and 401 for positive and negative\nlabels, respectively. After excluding stop-words\nand lemmatizing the words, we found 24,000 dis-\ntinct uni-grams in non-trafﬁcking ads, and 9,662\ndistinct unigrams in the trafﬁcking ads. It should\nbe noted that lemmatizing was only done for cal-\nculating the statistics in this section.\n3 Text Featurization\nIn the feature extraction step, the fundamen-\ntal challenge is to quantify the textual context\nwhile retrieving information from unconventional\nwords, abbreviations and equivocals. Here, we re-\nvisit different developed feature sets that eventu-\nally lead us to our desired contextual model.\n3.1 Topic Modeling Via LDA\nOur hypothesis is that language patterns, includ-\ning topics and word usages, can aid in discerning\nthe ads of trafﬁcking victims from those of non-\nvictims. That being said, independent or voluntary\nsex providers vary in their use of words, context,\nand topics. To test this hypothesis, we use a La-\ntent Dirichlet Allocation (LDA) model (Blei et al.,\n2003). Our vision was that clustering the words,\n1182\nwith the use of LDA to enhance the featuriza-\ntion, would allow us to identify the performance of\nwords in specialized textual contexts. LDA model\nassigns a score based on the importance of repre-\nsentation of the words within each topic. There-\nfore, the value of assigned scores to topics indi-\ncates which ones dominate throughout the text and\ncreate the feature set as si = [si1, . . . , sik], where\nsi is the i-th feature vector for documenti contain-\ning k scores.\n3.2 Average Word Vector\nWe choose to use word embedding as a key part of\nour model. Although Word2Vec (Mikolov et al.,\n2013) and GloVe (Pennington et al., 2014) word\nembeddings have shown promising results in se-\nmantic vector representations of words, when we\nused these models on our texts we found that they\nmissed many of the novel word uses and abbre-\nviations. Instead, we chose to use FastText (Bo-\njanowski et al., 2017) for our semantic word rep-\nresentation, as it is based on character-level word\nembedding and the word representation is the sum\nof vectors. With that said, we hereby deﬁne the\nsecond feature set for each text as νi =\n∑\nj νi,j\nn ,\nwhere n is the number of words in the texti, νi,j is\nthe vector representation of j-th word of language\nmodel with dimension of pν (here set to 100 based\non experiment).\n3.3 Pre-trained BERT\nThus far, we have deﬁned features which need to\nbe trained using the ads we already had. As our\nnext features set, we propose to use a pre-trained\nmodel. Since we believe pre-trained word em-\nbedding on general domain is not able to capture\nall the rare, equivocal, and abbreviated words and\nphrases in our sexual advertisement text (Tong\net al., 2017), we are motivated in ﬁnding the most\ncomprehensive deep learning model and chose to\nassess the newly released Bidirectional Encoder\nRepresentation from Transformers (BERT) (De-\nvlin et al., 2018). A word representation using\nBERT is made by using its bidirectional, i.e., left\nand right, context. BERT is released with two\nmodel sizes: (1) BERTBASE with 12 layers, 768\nhidden layers and 12 self-attention heads, and (2)\nBERTLARGE with 24 layers, 1024 hidden layers,\nand 16 self-attention heads. One should note that\nin this study we do not use ﬁne-tuned BERT model\nto examine the true power of BERT. Here, we\nchoose to use the pre-trained BERTBASE model\nwhich encodes our document to a vector represen-\ntation of size 768 for each document i and denote\nthat by bi = [bi1, . . . , bi768].\n3.4 Integrating LDA, A WV and BERT\nFinally, we propose a new feature set consisting of\nthe three types of features explained above. The\nrationale behind this composite feature set is to al-\nlow for the use of textual context as well as the\nsimpler features. Therefore, we have the ﬁnal fea-\nture vector deﬁned as as xi = [si, νi, bi], with the\ndimension of p = k + pν + 768.\n4 Experiments\nIn our study, we employ the feature models de-\nscribed above and compare the results of the bi-\nnary classiﬁcation corresponding to them. We use\nlogistic regression and compute the precision and\nrecall curve (PRC) to evaluate the performance of\ndifferent models. Moreover, in this application,\nit is important to have a model with good recall\nwhile keeping high precision, i.e., a high positive\npredictive value (PPV) to avoid unnecessary ac-\ntions. To do so, we investigate the sensitivity of\nmodels in different high PPVs.\nPre-processing. We choose to not remove stop\nwords or not use any stemming or lemmatization\ntechniques as we are faced with different writ-\ning structures which could be informative for our\nmodel. We test the impact of emojis and punctua-\ntion by training and testing our model by creating\ntwo text sets. In the ﬁrst text set, we keep the emo-\njis and punctuation and remove them in the sec-\nond set. In the second set, we convert the emojis\nto words. Numbers in the texts are removed, be-\ncause: 1) we have made the labels based on phone\nnumbers and 2), the ads are likely to have the same\nage or same price throughout the texts. We then di-\nvide the data into an 80/20% training/testing set.\nIn the following sections, we describe how each\nset of features is processed while using logistic re-\ngression as our ﬁxed classiﬁcation model.\nLDA Features. We begin with features coming\nfrom LDA topic modeling scores where we assign\nit to 12 topics. Gensim LDA is implemented by\nmaking a bag of words dictionary of our training\nset. We ﬁnd this optimal topic number where we\nexamined the explained LDA feature set via cross-\nvalidation on January 2017 alone.\n1183\nA WV Features.Our FastText model is trained\non a set including a minimum count of 2 words\nand a window size of 3 to give us a vector of di-\nmension 100. After training the FastText model,\nthe average word vector of the training set is com-\nputed. Using this saved language model from the\ntraining set, we compute the feature test vectors.\nBERT Features. For encoding our texts us-\ning BERT, we make a list of all documents and\nuse the BERT service client. We use the weights\nof the words that BERTBASE learned in its pre-\ntraining to encode each document to a vector of\nsize 768 for both the training and testing sets. We\nexamine encoding texts with both Cased BERT\n(C-BASED) and Uncased BERT (U-BERT). In the\nU-BERT, the text has been lower cased, whereas\nin C-BERT, the true case and accent markers are\npreserved.\nFull Features. In this ﬁnal step in featuriza-\ntion towards our composite model, we combine all\nthree types of features to build a uniﬁed feature set,\ni.e. combining LDA, AWV and BERT.\n5 Results and Discussions\nFigure 2 depicts the results of the classiﬁcations of\nthe different feature sets. It can be seen that both\nclassiﬁcation approaches based on LDA and the\naverage word vector features achieve similarly av-\nerage precision scores (APS). Based on our anal-\nysis, keeping the entire text or removing emojis\nand punctuation do not signiﬁcantly impact the re-\nsults. From Figure 2, it can be seen that, despite\nsmall improvements, different featurizations pro-\nvide similar APS values. However, focusing more\non the early parts of the PRC, i.e., high precision,\nwe can see that there is a signiﬁcant improvement\nof recall. For example, as summarized in Figure 3,\nat 85% precision, our proposed full model (with\nU-BERT) achieves 69% and 67% sensitivity on\npure text and text without emojis and punctuation,\nrespectively. However, in the composite model\nwith C-BERT, there is an opposite effect where re-\ncalls become 65% and 69% for the two scenarios,\nrespectively.\nComparing to the results of the classiﬁers with\ndifferent feature sets (under U-BERT), the model\nutilizing the full feature set provides 26% recall\nimprovement over the three individual ones, i.e.\n69% vs 28%−42%, when precision is set to 85%.\nA similar observation holds for90% precision. As\na concluding remark, we should emphasize our\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Precision\nLDA (APS: 0.79)\nAWV (APS: 0.81)\nC-BERT (APS: 0.78)\nU-BERT (APS: 0.80)\nC-BERT+LDA+AWV (APS: 0.87)\nU-BERT+LDA+AWV (APS: 0.87)\n(b)\n0.0 0.2 0.4 0.6 0.8 1.0\nRecall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Precision\nLDA (APS: 0.81)\nAWV (APS: 0.80)\nC-BERT (APS: 0.82)\nU-BERT (APS: 0.80)\nC-BERT+LDA+AWV (APS: 0.88)\nU-BERT+LDA+AWV (APS: 0.86)\nFigure 2: Precision and Recall curves (PRCs) and their\ncorresponding APS values: (a) pure text, (b) text with-\nout emojis and punctuation.\nsigniﬁcant improvement in recall rate over each in-\ndividual model.\n6 Conclusions and Future Work\nIn this paper, we introduced different models\nbased on different text featurizations where the\nmain goal was to engineer features that allowed\nfor understanding the context of sexual ads and re-\nmove the restriction of using keywords. We have\nproposed a composite model and compared its per-\nformance with other simpler models. For more\nevaluation, we examined the recall rate of mod-\nels in 85% and 90% of precision. The full feature\nset, i.e. LDA+AWV+BERT, outperformed others\nas it indicated that having comprehensive features\nmay be conveying more information about the ad-\nvertisements.\nThus, we can signiﬁcantly increase the PPV of\nour model while maintaining a high recall rate. It\nalso should be noted that our non-trafﬁcking ex-\namples may still contain some trafﬁcking ads. We\nthus note with caution that the false positives in\nour model may not be truly false. Given that, in\n1184\n(a)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Recall\n0.21 0.24\n0.11\n0.18\n0.48 0.46\n0.42\n0.30 0.28\n0.42\n0.65\n0.69\nAWV\nLDA\nC-BERT\nU-BERT\nLDA+AWV+C-BERT\nLDA+AWV+U-BERT\n(b)\n90%\n85%\n90%\n85%\n90%\n85%\n90%\n85%\n90%\n85%\n90%\n85%\nPrecision\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8Recall\n0.19\n0.34 0.34\n0.30\n0.52\n0.44\n0.38\n0.44\n0.50\n0.43\n0.69 0.67\nFigure 3: Recall rates corresponding to 90% and 85%\nprecision: (a) pure text, (b) text without emojis and\npunctuation.\nour future work, we will be investigating those\nfalse positive cases with our collaborators to as-\nsess what the correct label for these ads should\nbe. Moreover, since the proposed full feature set\ninvolves hundreds of features we plan to increase\nour sample size to have a better estimation of the\nperformance of our ﬁnal predictor. We also en-\nvision that by including other underlying compo-\nnents from these advertisements, we can assist law\nenforcement ofﬁcers with an automated frame-\nwork to sift millions of sexual advertisements and\nspend time on especially suspicious activities. Fi-\nnally, in this study, we tested our model on a bal-\nanced data set. However, in the real world, the\nnumber of trafﬁcking ads is always far lower than\nthe number of non-trafﬁcking ones. After col-\nlecting more labeled data, and tuning our model\nusing anomaly detection techniques like Isolation\nForests (Liu et al., 2008), we hope to expand this\nstudy to the stage where we are able to use unbal-\nanced data sets.\nAcknowledgments\nThis study was supported by Accenture Labs. We\nwould like to thank Jana Thompson for critical\nfeedback on the manuscript.\nReferences\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of Ma-\nchine Learning Research, 3(Jan):993–1022.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nSiddharth Kara. 2009. Sex trafﬁcking: Inside the busi-\nness of modern slavery. Columbia University Press.\nFei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. 2008.\nIsolation forest. In 2008 Eighth IEEE International\nConference on Data Mining, pages 413–422. IEEE.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1532–1543.\nEdward J Schauer and Elizabeth M Wheaton. 2006.\nSex trafﬁcking into the united states: A literature re-\nview. Criminal Justice Review, 31(2):146–169.\nPedro Szekely, Craig A Knoblock, Jason Slepicka,\nAndrew Philpot, Amandeep Singh, Chengye Yin,\nDipsy Kapoor, Prem Natarajan, Daniel Marcu,\nKevin Knight, et al. 2015. Building and using a\nknowledge graph to combat human trafﬁcking. In\nInternational Semantic Web Conference, pages 205–\n221. Springer.\nEdmund Tong, Amir Zadeh, Cara Jones, and Louis-\nPhilippe Morency. 2017. Combating human traf-\nﬁcking with deep multimodal models. arXiv\npreprint arXiv:1705.02735.\nJessica Whitney, Murray Jennex, Aaron Elkins, and\nEric Frost. 2018. Don’t want to get caught? don’t\nsay it: The use of emojis in online human sex traf-\nﬁcking ads."
}