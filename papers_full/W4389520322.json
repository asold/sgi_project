{
  "title": "UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers",
  "url": "https://openalex.org/W4389520322",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3197355071",
      "name": "Jon Saad-Falcon",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2144133057",
      "name": "Omar Khattab",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2911114451",
      "name": "Keshav Santhanam",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2132510150",
      "name": "Radu Florian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097113910",
      "name": "Martin Franz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2336572873",
      "name": "Salim Roukos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2044978172",
      "name": "Avirup Sil",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4360675274",
      "name": "Md Sultan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114426036",
      "name": "Christopher Potts",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287829148",
    "https://openalex.org/W4385571022",
    "https://openalex.org/W4200635123",
    "https://openalex.org/W4286969177",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3172806051",
    "https://openalex.org/W4287809523",
    "https://openalex.org/W4297162632",
    "https://openalex.org/W3103291112",
    "https://openalex.org/W3005296017",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4287780085",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W4221151914",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4285080136",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W4287645694",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W3103250468",
    "https://openalex.org/W3217305727",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3206786886",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4313680149",
    "https://openalex.org/W4287120901",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W4385570234",
    "https://openalex.org/W4385570706",
    "https://openalex.org/W4385570569",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3021397474",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4281251078",
    "https://openalex.org/W3211686893",
    "https://openalex.org/W2998184481",
    "https://openalex.org/W3020786614",
    "https://openalex.org/W4312046631",
    "https://openalex.org/W2969574947"
  ],
  "abstract": "Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Sultan, Christopher Potts. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11265–11279\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nUDAPDR: Unsupervised Domain Adaptation via LLM Prompting and\nDistillation of Rerankers\nJon Saad-Falcon1 Omar Khattab1 Keshav Santhanam1 Radu Florian2 Martin Franz2\nSalim Roukos2 Avirup Sil2 Md Arafat Sultan2 Christopher Potts1\n1Stanford University 2IBM Research AI\nAbstract\nMany information retrieval tasks require large\nlabeled datasets for fine-tuning. However, such\ndatasets are often unavailable, and their util-\nity for real-world applications can diminish\nquickly due to domain shifts. To address this\nchallenge, we develop and motivate a method\nfor using large language models (LLMs) to\ngenerate large numbers of synthetic queries\ncheaply. The method begins by generating a\nsmall number of synthetic queries using an ex-\npensive LLM. After that, a much less expensive\none is used to create large numbers of synthetic\nqueries, which are used to fine-tune a family of\nreranker models. These rerankers are then dis-\ntilled into a single efficient retriever for use in\nthe target domain. We show that this technique\nboosts zero-shot accuracy in long-tail domains\nand achieves substantially lower latency than\nstandard reranking methods.\n1 Introduction\nThe advent of neural information retrieval (IR) has\nled to notable performance improvements on docu-\nment and passage retrieval tasks (Nogueira and\nCho, 2019; Khattab and Zaharia, 2020; Formal\net al., 2021) as well as downstream knowledge-\nintensive NLP tasks such as open-domain question-\nanswering and fact verification (Guu et al., 2020;\nLewis et al., 2020; Khattab et al., 2021; Izacard\net al., 2022). Neural retrievers for these tasks often\nbenefit from fine-tuning on large labeled datasets\nsuch as SQuAD (Rajpurkar et al., 2018), Natural\nQuestions (NQ) (Kwiatkowski et al., 2019), and\nKILT (Petroni et al., 2021). However, IR models\ncan experience significant drops in accuracy due to\ndistribution shifts from the training to the target do-\nmain (Thakur et al., 2021; Santhanam et al., 2022b).\nFor example, dense retrieval models trained on\nMS MARCO (Nguyen et al., 2016) might not gen-\neralize well to queries about COVID-19 scientific\npublications (V oorhees et al., 2021; Wang et al.,\nFigure 1: Overview of UDAPDR. An expensive LLM\nlike GPT-3 is used to create an initial set of synthetic\nqueries. These are incorporated into a set of prompts for\na less expensive LLM that can generate large numbers\nof synthetic queries cheaply. The queries stemming\nfrom each prompt are used to train separate rerankers,\nand these are distilled into a single ColBERTv2 retriever\nfor use in the target domain.\n2020), considering for instance that MS MARCO\npredates COVID-19 and thus lacks related topics.\nRecent work has sought to adapt IR models\nto new domains by using large language models\n(LLMs) to create synthetic target-domain datasets\nfor fine-tuning retrievers (Bonifacio et al., 2022;\nMeng et al., 2022; Dua et al., 2022). For example,\nusing synthetic queries, Thakur et al. (2021) and\nDai et al. (2022) fine-tune the retriever itself and\ntrain a cross-encoder to serve as a passage reranker\nfor improving retrieval accuracy. This significantly\nimproves retriever performance in novel domains,\nbut it comes at a high computational cost stemming\nfrom extensive use of LLMs. This has limited the\napplicability of these methods for researchers and\npractitioners, particularly in high-demand, user-\n11265\nfacing settings.\nIn this paper, we develop Unsupervised Domain\nAdaptation via LLM Prompting and Distillation of\nRerankers (UDAPDR),1 an efficient strategy for us-\ning LLMs to facilitate unsupervised domain adap-\ntation of neural retriever models. We show that\nUDAPDR leads to large gains in zero-shot settings\non a diverse range of domains.\nThe approach is outlined in Figure 1. We begin\nwith a collection of passages from a target domain\n(no in-domain queries or labels are required) as\nwell as a prompting strategy incorporating these\npassages with the goal of query generation. A pow-\nerful (and perhaps expensive) language model like\nGPT-3 is used to create a modest number of syn-\nthetic queries. These queries form the basis for\ncorpus-adapted prompts that provide examples of\npassages with good and bad queries, with the goal\nof generating good queries for new target domain\npassages. These prompts are fed to a smaller (and\npresumably less expensive) LM that can generate\na very large number of queries for fine-tuning neu-\nral rerankers. We train a separate reranker on the\nqueries from each of these corpus-adapted prompts,\nand these rerankers are distilled into a single stu-\ndent ColBERTv2 retriever (Khattab and Zaharia,\n2020; Santhanam et al., 2022b,a), which is evalu-\nated on the target domain.\nBy distilling from multiple passage rerankers\ninstead of a single one, we improve the utility of\nColBERTv2, preserving more retrieval accuracy\ngains while reducing latency at inference. Our core\ncontributions are as follows:\n• We propose UDAPDR, a novel unsupervised\ndomain adaptation method for neural IR that\nstrategically leverages expensive LLMs like\nGPT-3 (Brown et al., 2020) and less expensive\nones like Flan-T5 XXL (Chung et al., 2022),\nas well as multiple passage rerankers. Our\napproach improves retrieval accuracy in zero-\nshot settings for LoTTE (Santhanam et al.,\n2022b), SQuAD, and NQ.\n• We preserve the accuracy gains of these\nrerankers while maintaining the competitive\nlatency of ColBERTv2. This leads to substan-\ntial reductions in query latency.\n• Unlike a number of previous domain adapta-\ntion approaches that utilize millions of syn-\nthetic queries, our technique only requires\n1pronounced: Yoo-Dap-ter\n1000s of synthetic queries to prove effective\nand is compatible with various LLMs de-\nsigned for handling instruction-based tasks\nlike creating synthetic queries (e.g., GPT-3,\nT5, Flan-T5).\n• We generate synthetic queries using multi-\nple prompting strategies that leverage GPT-\n3 and Flan-T5 XXL. This bolsters the effec-\ntiveness of our unsupervised domain adapta-\ntion approach. The broader set of synthetic\nqueries allows us to fine-tune multiple passage\nrerankers and distill them more effectively.\n2 Related Work\n2.1 Data Augmentation for Neural IR\nLLMs have been used to generate synthetic datasets\n(He et al., 2022; Yang et al., 2020; Anaby-Tavor\net al., 2020; Kumar et al., 2020), which have been\nshown to support effective domain adaptation in\nTransformer-based architectures (Vaswani et al.,\n2017) across various tasks. LLMs have also been\nused to improve IR accuracy in new domains via\nthe creation of synthetic datasets for retriever fine-\ntuning (Bonifacio et al., 2022; Meng et al., 2022).\nDomain shift is the most pressing challenge for\ndomain transfer. Dua et al. (2022) categorize dif-\nferent types of domain shifts, such as changes in\nquery or document distributions, and provide inter-\nvention strategies for addressing each type of shift\nusing synthetic data and indexing strategies.\nQuery generation can help retrieval models\ntrained on general domain tasks adapt to more tar-\ngeted domains through the use of generated query–\npassage pairs (Ma et al., 2020; Nogueira et al.,\n2019). Wang et al. (2022) also use generative\nmodels to pseudo-label synthetic queries, using\nthe generated data to adapt dense retrieval mod-\nels to domain-specific datasets like BEIR (Thakur\net al., 2021). Thakur et al. (2021) and Dai et al.\n(2022) generate millions of synthetic examples for\nfine-tuning dense retrieval models, allowing for\nzero-shot and few-shot domain adaptation.\nSynthetic queries can also be used to train pas-\nsage rerankers that assist neural retrievers. Cross-\nencoders trained with synthetic queries boost re-\ntrieval accuracy substantially while proving more\nrobust to domain shifts (Thakur et al., 2020, 2021;\nHumeau et al., 2019). Dai et al. (2022) explore\ntraining the few-shot reranker Promptagator++,\nleveraging an unsupervised domain-adaptation ap-\nproach that utilizes millions of synthetically gener-\n11266\nated queries to train a passage reranker alongside\na dense retrieval model. Additionally, Wang et al.\n(2022) found using zero-shot cross-encoders for\nreranking could further improve quality.\nHowever, due to the high computational cost\nof rerankers at inference, both Dai et al. (2022)\nand Wang et al. (2022) found it unlikely these ap-\nproaches would be deployed in user-facing settings\nfor information retrieval. Our work seeks to bolster\nthe utility of passage rerankers in information re-\ntrieval systems. Overall, Dai et al. (2022) and Wang\net al. (2022) demonstrated the efficacy of unsuper-\nvised domain adaptation approaches utilizing syn-\nthesized queries for fine-tuning dense retrievers or\npassage rerankers. By using distillation strategies,\nwe can avoid their high computational cost while\npreserving the latent knowledge gained through\nunsupervised domain adaptation approaches.\n2.2 Pretraining Objectives for IR\nPretraining objectives can help neural IR systems\nadapt to new domains without annotations. Masked\nLanguage Modeling (MLM) (Devlin et al., 2019)\nand Inverse Cloze Task (ICT) (Lee et al., 2019)\noffer unsupervised approaches for helping retrieval\nmodels adapt to new domains. Beyond MLM and\nICT, Chang et al. (2020) proposed two unsuper-\nvised pretraining tasks, Body First Selection (BFS)\nand Wiki Link Prediction (WLP), which use sam-\npled in-domain sentences and passages to warm-up\na neural retriever to new domains. Additionally,\nGysel et al. (2018) developed the Neural Vector\nSpace Model (NVSM), an unsupervised pretraining\ntask for news article retrieval that utilizes learned\nlow-dimensional representations of documents and\nwords. Izacard et al. (2021) also explore a con-\ntrastive learning objective for unsupervised training\nof dense retrievers, improving retrieval accuracy in\nnew domains across different languages.\nThese pretraining objectives can be paired with\nadditional domain adaptation strategies. Wang et al.\n(2022) coupled ICT with synthetic query data to\nachieve domain adaptation in dense retrieval mod-\nels without the need for annotations. Dai et al.\n(2022) also paired the contrastive learning objec-\ntive in Izacard et al. (2021) with their unsupervised\nPromptagator strategy. While our zero-shot domain\nadaptation approach can pair with other techniques,\nit does not require any further pretrainingfor bol-\nstered retrieval performance; our approach only\nneeds the language model pretraining of the re-\ntriever’s base model (Devlin et al., 2019), and we\nshow that it combines effectively with multi-vector\nretrievers (Khattab and Zaharia, 2020; Santhanam\net al., 2022b).\n3 Methodology\nFigure 1 outlines each stage of the UDAPDR strat-\negy. For the target domain T, our approach re-\nquires access to in-domain passages (i.e., within\nthe domain of T). However, it does not require any\nin-domain queries or labels. The overall goal is to\nleverage our store of in-domain passages and LLM\nprompting to inexpensively generate large numbers\nof synthetic queries for passages. These synthetic\nqueries are used to train domain-specific reranking\nmodels that serve as teachers for a single retriever.\nThe specific stages of this process are as follows:\nStage 1: We begin with a set of prompts that em-\nbed examples of passages paired with good and bad\nqueries and that seek to have the model generate\na novel good query for a new passage. We sam-\nple X in-domain passages from the target domain\nT, and we use the prompts to generate 5X syn-\nthetic queries. In our experiments, we test values\nof X such as 5, 10, 50, and 100. (In Appendix A,\nwe explore different strategies for selecting the X\nin-domain passages from the target domain.)\nIn this stage, we use GPT-3 (Brown et al., 2020),\nspecifically the text-davinci-002 model. The\nguiding idea is to use a very effective LLM for\nthe first stage, to seed the process with very high\nquality queries. We employ the five prompting\nstrategies in Figure 2. Two of our prompts are from\nBonifacio et al. 2022, where they proved successful\nfor generating synthetic queries in a few-shot set-\nting. The remaining three use a zero-shot strategy\nand were inspired by prompts in Asai et al. 2022.\nStage 2: The queries generated in Stage 1 form\nthe basis for prompts in which passages from the\ntarget domain T are paired with good and bad syn-\nthetic queries. The prompt seeks to lead the model\nto generate a good query for a new passage. Our\nprompt template for this stage is given in Figure 3.\nWe create Y corpus-adapted prompts in this fash-\nion, which vary according to the demonstrations\nthey include. In our experiments, we test out sev-\neral values for Y , specifically, 1, 5, and 10. This\nprogrammatic creation of few-shot demonstrations\nfor language models is inspired by the Demonstrate\nstage of the DSP framework (Khattab et al., 2022).\n11267\nFigure 2: The five prompts used in Stage 1 (Section 3). The few-shot prompts #1 and #2 were inspired by Bonifacio\net al. (2022) while the zero-shot prompts #3, #4, and #5 were inspired by Asai et al. (2022). In our experiments, we\nprompt GPT-3 in this stage to generate an initial set of queries.\nFigure 3: The prompt template used in Stage 2. (Sec-\ntion 3). In our experiments, we create Y variants of\nthis prompt, and each one is used with Flan-T5 XXL to\ngenerate Z queries for each Y .\nStage 3: Each of the corpus-adapted prompts\nfrom Stage 2 is used to generate a unique set of\nZ queries with Flan-T5 XXL (Chung et al., 2022).\nThe gold passage for each synthetic query is the\npassage it was generated from. We have the option\nof letting Z be large because using Flan-T5 XXL\nis considerably less expensive than using GPT-3 as\nwe did in Stage 1. In our experiments, we test 1K,\n10K, 100K, and 1M as values for Z. We primarily\nfocus on Z = 10K and 100K in Section 4.3.\nWe use multiple corpus-adapted prompts to mit-\nigate edge cases in which we create a low-quality\nprompt based on the chosen synthetic queries and\nin-domain passages from the target domainT. (See\nStage 4 below for a description of how low-quality\nprompts can optionally be detected and removed.)\nAs a quality filter for selecting synthetic queries,\nwe test whether a synthetic query can return its\ngold passage within the top 20 retrieved results\nusing a zero-shot ColBERTv2 retriever. We only\nuse synthetic queries that pass this filter, which\nhas been shown to improve domain adaptation in\nneural IR (Dai et al., 2022; Jeronymo et al., 2023).\nStage 4: With each set of synthetic queries gen-\nerated using the Y corpus-adapted prompts in\nStage 3, we train an individual passage reranker\nfrom scratch for the target domain T. These will\nbe used as teachers for a single ColBERTv2 model\nin Stage 5. Our multi-reranker strategy draws in-\nspiration from Hofstätter et al. (2021), who found\na teacher ensemble effective for knowledge distil-\nlation into retrievers. At this stage, we can simply\n11268\nuse all Y of these rerankers for the distillation pro-\ncess. As an alternative, we can select the N best\nrerankers based on their accuracy on the validation\nset of the target domain. For our main experiments,\nwe use all of these rerankers. This is the most\ngeneral case, in which we do not assume that a\nvalidation set exists for the target domain. (In Ap-\npendix A, we evaluate settings where a subset of\nthem is used.)\nStage 5: The domain-specific passage rerankers\nfrom Stage 4 serve as multi-teachers creating a\nsingle ColBERTv2 retriever in a multi-teacher dis-\ntillation process. For distillation, we use annotated\ntriples that are created by using the trained domain-\nspecific reranker to label additional synthetic ques-\ntions. Overall, our distillation process allows us\nto improve the retrieval accuracy of ColBERTv2\nwithout needing to use the rerankers at inference.\nStage 6: We test our domain-adapted Col-\nBERTv2 retriever on the evaluation set for the tar-\nget domain T. This corresponds to deployment of\nthe retriever within the target domain.\n4 Experiments\n4.1 Models\nWe leverage the Demonstrate-Search-Predict (DSP)\n(Khattab et al., 2022) codebase for running our ex-\nperiments. The DSP architecture allows us to build\na modular system with both retrieval models and\nLLMs. For our passage rerankers, we selected\nDeBERTaV3-Large (He et al., 2021) as the cross-\nencoder after performing comparison experiments\nwith RoBERTa-Large (Liu et al., 2019), BERT (De-\nvlin et al., 2019), and MiniLMv2 (Wang et al.,\n2021). For our IR system, we use the ColBERTv2\nretriever since it remains competitive for both ac-\ncuracy and query latency across various domains\nand platforms (Santhanam et al., 2022c).\n4.2 Datasets\nFor our experiments, we use LoTTE (Santhanam\net al., 2022b), BEIR (Thakur et al., 2021), NQ\n(Kwiatkowski et al., 2019), and SQuAD (Rajpurkar\net al., 2016). This allows us to cover both long-tail\nIR (LoTTE, BEIR) and general-knowledge ques-\ntion answering (NQ, SQuAD).\nWe note that NQ and SQuAD were both part of\nFlan-T5’s pretraining datasets (Chung et al., 2022).\nWikipedia passages used in NQ and SQuAD were\nalso part of DeBERTaV3 andGPT-3’spretraining\ndatasets (He et al., 2021; Brown et al., 2020). Sim-\nilarly, the raw StackExchange answers and ques-\ntions (i.e., from which LoTTE-Forum is derived)\nmay overlap in part with the training data ofGPT-3.\nThe overlap between pretraining and evaluation\ndatasets may impact the efficacy of domain adap-\ntation on NQ and SQuAD, leading to increased\naccuracy not caused by our approach.\n4.3 Multi-Reranker Domain Adaptation\nTable 1 provides our main results for UDAPDR\naccuracy. For these experiments, we set a total bud-\nget of 100K synthetic queries and distribute these\nequally across the chosen number of rerankers to be\nused as teachers in the distillation process. When\nexploring UDAPDR system designs, we report dev\nresults, and we report core test results for LoTTE\nand BEIR in Section 4.7.\nWe compare UDAPDR to two baselines. In the\nleftmost column of Table 1, we have a Zero-shot\nColBERTv2 retriever (no distillation). This model\nhas been shown to be extremely effective in our\nbenchmark domains, and it is very low latency, so\nit serves as an ambitious baseline. In the rightmost\ncolumn, we have a Zero-shot ColBERTv2 retriever\npaired with a single non-distilled passage reranker,\ntrained on 100K synthetic queries. We expect this\nmodel to be extremely competitive in terms of ac-\ncuracy, but also very high latency.\nAll versions of UDAPDR are far superior to\nZero-shot ColBERTv2 across all the domains we\nevaluated. In addition, two settings of our model\nare competitive with or superior to Zero-shot Col-\nBERTv2 plus a Reranker: distilling into Col-\nBERTv2 the scores from 5 rerankers, each trained\non 20k synthetic queries, as well as 10 rerankers,\neach trained on 10k synthetic queries.\n4.4 Query Latency\nThe accuracy results in Table 1 show that UDAPDR\nis highly effective. In addition to this, Table 2\nreports a set of latency evaluations using the LoTTe\nLifestyle section. Our latency costs refer to the\ncomplete retrieval process for a single query, from\nquery encoding to the last stage of passage retrieval.\nZero-shot ColBERTv2 is known to have low re-\ntrieval latency (Santhanam et al., 2022a). However,\nits accuracy (repeated from Table 1), which is at\na state-of-the-art level (Santhanam et al., 2022c),\ntrails by large margins the methods we propose in\nthis work. UDAPDR (line 2) has similar latency\nbut also achieves the best accuracy results. The\n11269\nColBERTv2 Distillation with UDAPDR\nZero-shot\nColBERTv2\nY = 1 reranker,\nZ = 100k queries\nY = 5 rerankers,\nZ = 20k queries\nY = 10 rerankers,\nZ = 10k queries\nZero-shot\nColBERTv2\n+ Reranker\nLoTTE Lifestyle 64.5 73.0 74.8 74.4 73.5\nLoTTE Techology 44.5 50.2 51.3 51.1 50.6\nLoTTE Writing 80.0 84.3 85.7 86.2 85.5\nLoTTE Recreation 70.8 76.9 80.4 79.8 79.1\nLoTTE Science 61.5 65.6 67.9 68.0 67.2\nLoTTE Pooled 63.7 70.0 72.1 72.2 71.1\nNaturalQuestions 68.9 72.4 73.7 74.0 73.9\nSQuAD 65.0 71.8 73.8 73.6 72.6\nTable 1: Success@5 for Multi-Reranker Domain Adaptation Strategies with Different Synthetic Query Counts.\nLoTTE results are for the Forum configuration. All results are for dev sets. The reranker used is DeBERTa-v3-Large.\nThe ColBERTv2 distillation strategies trainY rerankers each with Z synthetic queries before distilling each reranker\nwith the same ColBERTv2 model. No selection process for rerankers is needed nor access to annotated in-domain\ndev sets (cf. Table 7 in our Appendices). The non-distilled reranker in the final column is trained on 100K synthetic\nqueries created using Flan-T5 XXL model and the prompting strategy outlined in Section 3.\nRetriever and Reranker Passages\nReranked\nQuery\nLatency Success@5\nZero-shot ColBERTv2 N/A 35 ms 64.5\nColBERTv2 Distillation: Y = 5 rerankers, Z = 20k queries N/A 35 ms 74.8\nZero-shot ColBERTv2 + Reranker 20 412 ms 73.3\nZero-shot ColBERTv2 + Reranker 100 2060 ms 73.5\nZero-shot ColBERTv2 + Reranker 1000 20600 ms 73.5\nTable 2: Average Single Query Latency for Retrieval + Reranker Systems.Latencies and Success@5 are for\nLoTTE Lifestyle. The ColBERTv2 distillation strategies train Y rerankers each with Z synthetic queries before\ndistilling each reranker with the same ColBERTv2 model. These experiments were performed on a single NVIDIA\nV100 GPU with PyTorch, version 1.13 (Paszke et al., 2019). Query latencies rounded to three significant digits.\nZero-shot ColBERTv2 + Reranker models come\nclose, but only with significantly higher latency.\n4.5 Impact of Pretrained Components\nUDAPDR involves three pretrained components:\nGPT-3 to generate our initial set of synthetic\nqueries, Flan-T5 XXL to generate our second,\nlarger set of synthetic queries, and DeBERTaV3-\nLarge for the passage rerankers. What is the impact\nof these specific components on system behavior?\nTo begin to address this question, we explored\na range of variants. These results are summarized\nin Table 4. Our primary setting for UDAPDR per-\nforms the best, but it is noteworthy that very com-\npetitive performance can be obtained with no use\nof GPT-3 at all. Additionally, we tried using Flan-\nT5 XL instead of Flan-T5 XXL for the second stage\nof synthetic query generation, since it is more than\n90% smaller than Flan-T5 XXL in terms of model\nparameters. This still leads to better performance\nthan Zero-shot ColBERTv2.\nWe also explored using a smaller cross-encoder\nfor UDAPDR. We tested using DeBERTaV3-Base\ninstead of DeBERTaV3-Large for our passage\nreranker, decreasing the number of model parame-\nters by over 70%. We found that DeBERTaV3-Base\nwas still effective, though it results in a 4.1 point\ndrop in Success@5 compared to DeBERTaV3-\nLarge for LoTTE Pooled (Table 3). (In our initial\nexplorations, we also tested using BERT-Base or\nRoBERTa-Large as the cross-encoder but found\nthem less effective than DeBERTaV3, leading to\n6–8 point drops in Success@5.)\n4.6 Different Prompting Strategies\nWe tested whether a simpler few-shot prompting\nstrategy might be better than our corpus-adapted\nprompting approach for domain adaptation. In Ta-\nble 4, we compare the InPars (Bonifacio et al.,\n2022) few-shot prompt to our corpus-adapted\nprompt approach for synthetic query generation\nand passage reranker distillation. We evaluate these\nusing query generation with both Flan XXL and\nGPT-3. We find that our multi-reranker, corpus-\n11270\nQuery Generators Passage Reranker Success@5\nGPT-3 + Flan-T5 XXL DeBERTav3 Large 71.1\nGPT-3 + Flan-T5 XL DeBERTav3 Large 66.7\nFlan-T5 XXL DeBERTav3 Large 68.0\nFlan-T5 XL DeBERTav3 Large 65.9\nGPT-3 + Flan-T5 XXL DeBERTav3 Base 67.0\nGPT-3 + Flan-T5 XL DeBERTav3 Base 64.1\nZero-shot ColBERTv2 N/A 63.7\nTable 3: Model Configurations for Synthetic Query Generation and Passage Reranker. We describe the first\nand second query generators for UDAPDR in Section 3. The Success@5 scores are for the LoTTE Pooled dev task.\nA single non-distilled reranker is trained on 100K synthetic queries for each configuration. We do not explore a\nconfiguration with exclusively GPT-3 generated queries due to GPT-3 API costs.\nPrompt Strategy Query Generators Reranker Count Success@5\nInPars Prompt GPT-3 1 65.8\nInPars Prompt Flan-T5 XXL 1 67.6\nInPars Prompt Flan-T5 XXL 5 67.1\nCorpus-Adapted Prompts GPT-3 + Flan-T5 XXL 1 67.4\nCorpus-Adapted Prompts GPT-3 + Flan-T5 XXL 5 71.1\nZero-shot ColBERTv2 N/A N/A 63.7\nTable 4: Model Configurations for Prompting Strategies. We specify the prompting strategy, query generators,\nand reranker counts for each configuration. The Success@5 scores are for the LoTTE Pooled dev task. 100,000\nsynthetic queries total are used for each approach except for the top row, which uses 5,000 synthetic queries due to\nthe costs of the GPT-3 API. The synthetic queries are split evenly amongst the total rerankers used. The rerankers\nare distilled with a ColBERTv2 retriever for configuration.\nadapted prompting strategy is more successful,\nleading to a 3.5 point increase in Success@5 af-\nter ColBERTv2 distillation while using the same\nnumber of synthetic queries for training.\n4.7 LoTTE and BEIR Test Results\nIn Table 5 and Table 6, we include the test set\nresults for LoTTE and BEIR, respectively. For\nLoTTE, UDAPDR increases ColBERTv2 zero-shot\nSuccess@5 for both Forum queries and Search\nqueries, leading to a 7.1 point and a 3.9 point av-\nerage improvement, respectively. For BEIR, we\ncalculated ColBERTv2 accuracy using nDCG@10.\nWe found that UDAPDR increases zero-shot ac-\ncuracy by 5.2 points on average. Promptagator++\nFew-shot offers similar improvements to zero-shot\naccuracy, achieving a 4.2 point increase compared\nto a zero-shot ColBERTv2 baseline. However,\nPromptagator++ Few-shot also uses a reranker dur-\ning retrieval, leading to additional computational\ncosts at inference time. By comparison, UDAPDR\nis a zero-shot method (i.e., that does not assume\naccess to gold labels from the target domain) and\nonly uses the ColBERTv2 retriever and thus has a\nlower query latency at inference time.\n4.8 Additional Results\nTable 1 and Table 2 explore only a limited range\nof potential uses for UDAPDR. In Appendix A,\nwe consider a wider range of uses. First, we ask\nwhether it is productive to filter the set of rerankers\nbased on in-domain dev set performance. We\nmostly find that this does not lead to gains over\nsimply using all of them, and it introduces the re-\nquirement that we have a labeled dev set. Second,\nwe evaluate whether substantially increasing the\nvalue of Z from 100K leads to improvements. We\nfind that it does not, and indeed that substantally\nlarger values of Z can hurt performance.\n5 Discussion & Future Work\nOur experiments with UDAPDR suggest several\ndirections for future work:\n• While we show that our domain adaptation\nstrategy is effective for the multi-vector Col-\nBERTv2 model, whether it is also effective\nfor other retrieval models is an open question\nfor future research.\n• For our base model in ColBERTv2, we use\nBERT-Base. However, ColBERTv2 now al-\nlows for other base models, such as DeBER-\nTaV3, ELECTRA (Clark et al., 2020), and\n11271\nLoTTE Datasets\nForum Search\nLife. Tech. Writing Rec. Science Pooled Life. Tech. Writing Rec. Science Pooled\nBM25 60.6 39.4 64.0 55.4 37.1 47.2 63.8 41.8 60.3 56.5 32.7 48.3\nSPLADEv2 74.0 50.8 73.0 67.1 43.7 60.1 82.3 62.4 77.1 69.0 55.4 68.9\nRocketQAv2 73.7 47.3 71.5 65.7 38.0 57.7 82.1 63.4 78.0 72.1 55.3 69.8\nZero-shot ColBERTv2 76.2 54.0 75.8 69.8 45.6 62.3 82.4 65.9 80.4 73.2 57.5 71.5\nUDAPDR 84.9 59.9 83.2 78.6 48.8 70.8 86.8 67.7 84.3 77.9 61.0 76.6\nTable 5: Success@5 for LoTTE Test Set Results. The ColBERTv2 distillation strategies train Y rerankers each\nwith Z synthetic queries before distilling each reranker with the same ColBERTv2 model. For UDAPDR, we use 5\nrerankers and 20K distinct synthetic queries for training each of them. For BM25, SPLADEv2, and RocketQAv2,\nwe take results directly from Santhanam et al. (2022b).\nBEIR Datasets\nArguAna Touché Covid NFcorpus HotpotQA DBPedia Climate-FEVER FEVER SciFact SCIDOCS FiQA\nBM25 31.5 36.7 65.6 32.5 60.3 31.3 21.3 75.3 66.5 15.8 23.6\nDPR (MS MARCO) 41.4 - 56.1 20.8 37.1 28.1 17.6 58.9 47.8 10.8 27.5\nANCE 41.5 - 65.4 23.7 45.6 28.1 19.8 66.9 50.7 12.2 29.5\nColBERT (v1) 23.3 - 67.7 30.5 59.3 39.2 18.4 77.1 67.1 14.5 31.7\nTAS-B 42.7 - 48.1 31.9 58.4 38.4 22.8 70.0 64.3 14.9 30.0\nRocketQAv2 45.1 24.7 67.5 29.3 53.3 35.6 18.0 67.6 56.8 13.1 30.2\nSPLADEv2 47.9 27.2 71.0 33.4 68.4 43.5 23.5 78.6 69.3 15.8 33.6\nBM25 Reranking w/ Coherelarge 46.7 27.6 80.1 34.7 58.0 37.2 25.9 67.4 72.1 19.4 41.1\nBM25 Reranking w/ OpenAIada2 56.7 28.0 81.3 35.8 65.4 40.2 23.7 77.3 73.6 18.6 41.1\nZero-shot ColBERTv2 46.1 26.3 84.7 33.8 70.3 44.6 27.1 78.0 66.0 15.4 45.8\nGenQ 49.3 18.2 61.9 31.9 53.4 32.8 17.5 66.9 64.4 14.3 30.8\nGPL + TSDAE 51.2 23.5 74.9 33.9 57.2 36.1 22.2 78.6 68.9 16.8 34.4\nUDAPDR 57.5 32.4 88.0 34.1 75.3 47.4 33.7 83.2 72.2 17.8 53.5\nPromptagator Few-shot 63 38.1 76.2 37 73.6 43.4 24.1 86.6 73.1 20.1 49.4\nTable 6: nDCG@10 for BEIR Test Set Results. For each dataset, the highest-accuracy zero-shot result is marked\nin bold while the highest overall is underlined. For UDAPDR, we use 5 rerankers and 20K distinct synthetic queries\nfor training each of them. The Promptagator and GPL results are taken directly from their respective papers. For\nPromptagator, we include both the best retriever-only configuration (Promptagator Few-shot) and the best retriever +\nreranker configuration (Promptagator++ Few-shot). We include the GPL+TSDAE pretraining strategy, which is\nfound to improve retrieval accuracy (Wang et al., 2022). We copy the results for BM25, GenQ, ANCE, TAS-B, and\nColBERT from Thakur et al. (2021), for MoDIR and DPR-M from Xin et al. (2022), for SPLADEv2 from Formal\net al. (2021), and for BM25 Reranking of Coherelarge and OpenAIada2 from Kamalloo et al. (2023).\nRoBERTa (Liu et al., 2019). We would be\ninterested to see the efficacy of our domain\nadaptation strategy with these larger encoders.\n• We explored distillation strategies for combin-\ning passage rerankers with ColBERTv2. How-\never, testing distillation strategies for shrink-\ning the reranker itself could be a viable direc-\ntion for future work.\n• We draw upon several recent publications, in-\ncluding Bonifacio et al. (2022) and Asai et al.\n(2022), to create the prompts used for GPT-3\nand Flan-T5 XXL in our domain adaptation\nstrategy. Creating a more systematic approach\nfor generating the initial prompts would be an\nimportant item for future work.\n6 Conclusion\nWe present UDAPDR, a novel strategy for adapting\nretrieval models to new domains. UDAPDR uses\nsynthetic queries created using generative models,\nsuch as GPT-3 and Flan-T5 XXL, to train multiple\npassage rerankers on queries for target domain pas-\nsages. These passage rerankers are then distilled\ninto ColBERTv2 to boost retrieval accuracy while\nkeeping query latency competitive as compared to\nother retrieval systems. We validate our approach\nacross the LoTTE, BEIR, NQ, and SQuAD datasets.\nAdditionally, we explore various model configura-\ntions that alter the generative models, prompting\nstrategies, retriever, and passage rerankers used in\nour approach. We find that UDAPDR can boost\nzero-shot retrieval accuracy on new domains with-\nout the use of labeled training examples. We also\ndiscuss several directions for future work.\n11272\n7 Limitations\nWhile our domain adaptation technique does not\nrequire questions or labels from the target domain,\nit does require a significant number of passages\nin the target domain. These passages are required\nfor use in synthetic query generation with the help\nof LLMs like GPT-3 and Flan-T5, so future work\nshould evaluate how effective these methods are on\nextremely small passage collections.\nThe synthetic queries created in our approach\nmay also inherit biases from the LLMs and their\ntraining data. Moreover, the exact training data of\nthe LLMs is not precisely known. Our understand-\ning is that subsets of SQuAD and NQ, in particular,\nhave been used in the pretraining of Flan-T5 mod-\nels as we note in the main text. More generally,\nother subtle forms of data contamination are possi-\nble as with all research based on LLMs that have\nbeen trained on billions of tokens from the Web.\nWe have mitigated this concern by evaluating on a\nvery large range of datasets and relying most heav-\nily on open models (i.e., Flan-T5, DeBERTa, and\nColBERT). Notably, our approach achieves consis-\ntently large gains across the vast majority of the\nmany evaluation datasets tested (e.g., the individ-\nual sets within BEIR), reinforcing our trust in the\nvalidity and transferability of our findings.\nAdditionally, the LLMs used in our technique\nbenefit substantially from GPU-based hardware\nwith abundant and rapid storage. These technolo-\ngies are not available to all NLP practitioners and\nresearchers due to their costs. Lastly, all of our\nselected information retrieval tasks are in English,\na high-resource language. Future work can ex-\npand on the applicability of our domain adaptation\ntechniques by using non-English passages in low-\nresource settings, helping us better understand the\napproach’s strengths and limitations.\nReferences\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, Naama\nTepper, and Naama Zwerdling. 2020. Do not have\nenough data? deep learning to the rescue! In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 34, pages 7383–7390.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022. Inpars: Data augmentation\nfor information retrieval using large language models.\narXiv preprint arXiv:2202.05144.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nWei-Cheng Chang, Felix X. Yu, Yin-Wen Chang, Yim-\ning Yang, and Sanjiv Kumar. 2020. Pre-training\nTasks for Embedding-based Large-scale Retrieval.\nIn International Conference on Learning Representa-\ntions.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. arXiv preprint arXiv:2003.10555.\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B\nHall, and Ming-Wei Chang. 2022. Promptagator:\nFew-shot dense retrieval from 8 examples. arXiv\npreprint arXiv:2209.11755.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDheeru Dua, Emma Strubell, Sameer Singh, and\nPat Verga. 2022. To adapt or to annotate: Chal-\nlenges and interventions for domain adaptation in\nopen-domain question answering. arXiv preprint\narXiv:2212.10381.\nThibault Formal, Carlos Lassance, Benjamin Pi-\nwowarski, and Stéphane Clinchant. 2021. Splade\nv2: Sparse lexical and expansion model for informa-\ntion retrieval. arXiv preprint arXiv:2109.10086.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929–3938.\nPMLR.\nChristophe Van Gysel, Maarten de Rijke, and Evangelos\nKanoulas. 2018. Neural Vector Spaces for Unsuper-\nvised Information Retrieval. ACM Trans. Inf. Syst.,\n36(4).\n11273\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDeBERTaV3: Improving deberta using electra-style\npre-training with gradient-disentangled embedding\nsharing. arXiv preprint arXiv:2111.09543.\nXuanli He, Islam Nassar, Jamie Kiros, Gholamreza Haf-\nfari, and Mohammad Norouzi. 2022. Generate, An-\nnotate, and Learn: NLP with Synthetic Text. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:826–842.\nSebastian Hofstätter, Sophia Althammer, Michael\nSchröder, Mete Sertkan, and Allan Hanbury. 2021.\nImproving efficient neural ranking models with cross-\narchitecture knowledge distillation.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2019. Poly-encoders: Trans-\nformer architectures and pre-training strategies for\nfast and accurate multi-sentence scoring. arXiv\npreprint arXiv:1905.01969.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense infor-\nmation retrieval with contrastive learning.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nVitor Jeronymo, Luiz Bonifacio, Hugo Abonizio,\nMarzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and\nRodrigo Nogueira. 2023. Inpars-v2: Large language\nmodels as efficient dataset generators for information\nretrieval. arXiv preprint arXiv:2301.01820.\nEhsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo,\nNandan Thakur, David Alfonso-Hermelo, Mehdi\nRezagholizadeh, and Jimmy Lin. 2023. Evaluating\nembedding apis for information retrieval.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Baleen: Robust Multi-Hop Reasoning at Scale\nvia Condensed Retrieval. In Thirty-Fifth Conference\non Neural Information Processing Systems.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-Search-\nPredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nOmar Khattab and Matei Zaharia. 2020. Colbert: Ef-\nficient and effective passage search via contextual-\nized late interaction over BERT. In Proceedings of\nthe 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval,\nSIGIR 2020, Virtual Event, China, July 25-30, 2020,\npages 39–48. ACM.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data Augmentation using Pre-trained Trans-\nformer Models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18–26, Suzhou, China. Association for Com-\nputational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural Questions: A benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–\n466.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6086–6096, Florence, Italy.\nAssociation for Computational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive NLP tasks. Advances in\nNeural Information Processing Systems , 33:9459–\n9474.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nJi Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan\nMcDonald. 2020. Zero-shot neural passage retrieval\nvia domain-targeted synthetic question generation.\narXiv preprint arXiv:2004.14503.\nRui Meng, Ye Liu, Semih Yavuz, Divyansh Agarwal,\nLifu Tu, Ning Yu, Jianguo Zhang, Meghana Bhat, and\nYingbo Zhou. 2022. Unsupervised dense retrieval\ndeserves better positive pairs: Scalable augmentation\nwith query extraction and generation. arXiv preprint\narXiv:2212.08841.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In CoCo@ NIPs.\n11274\nRodrigo Nogueira and Kyunghyun Cho. 2019. Pas-\nsage re-ranking with BERT. arXiv preprint\narXiv:1901.04085.\nRodrigo Nogueira, Jimmy Lin, and AI Epistemic. 2019.\nFrom doc2query to doctttttquery. Online preprint, 6.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nKeshav Santhanam, Omar Khattab, Christopher Potts,\nand Matei Zaharia. 2022a. PLAID: an efficient en-\ngine for late interaction retrieval. In Proceedings of\nthe 31st ACM International Conference on Informa-\ntion & Knowledge Management, pages 1747–1756.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022b. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKeshav Santhanam, Jon Saad-Falcon, Martin Franz,\nOmar Khattab, Avirup Sil, Radu Florian, Md Arafat\nSultan, Salim Roukos, Matei Zaharia, and Christo-\npher Potts. 2022c. Moving beyond downstream\ntask accuracy for information retrieval benchmarking.\narXiv preprint arXiv:2212.01340.\nNandan Thakur, Nils Reimers, Johannes Daxenberger,\nand Iryna Gurevych. 2020. Augmented SBERT: Data\nAugmentation Method for Improving Bi-Encoders\nfor Pairwise Sentence Scoring Tasks. arXiv preprint\narXiv:2010.08240.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nEllen V oorhees, Tasmeer Alam, Steven Bedrick, Dina\nDemner-Fushman, William R Hersh, Kyle Lo, Kirk\nRoberts, Ian Soboroff, and Lucy Lu Wang. 2021.\nTREC-COVID: constructing a pandemic information\nretrieval test collection. In ACM SIGIR Forum, vol-\nume 54, pages 1–12. ACM New York, NY , USA.\nKexin Wang, Nandan Thakur, Nils Reimers, and Iryna\nGurevych. 2022. GPL: Generative pseudo labeling\nfor unsupervised domain adaptation of dense retrieval.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2345–2360, Seattle, United States. Association\nfor Computational Linguistics.\nLucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar,\nRussell Reas, Jiangjiang Yang, Doug Burdick, Darrin\nEide, Kathryn Funk, Yannis Katsis, Rodney Michael\nKinney, Yunyao Li, Ziyang Liu, William Merrill,\nPaul Mooney, Dewey A. Murdick, Devvret Rishi,\nJerry Sheehan, Zhihong Shen, Brandon Stilson,\nAlex D. Wade, Kuansan Wang, Nancy Xin Ru Wang,\nChristopher Wilhelm, Boya Xie, Douglas M. Ray-\nmond, Daniel S. Weld, Oren Etzioni, and Sebastian\nKohlmeier. 2020. CORD-19: The COVID-19 open\nresearch dataset. In Proceedings of the 1st Work-\nshop on NLP for COVID-19 at ACL 2020 , Online.\nAssociation for Computational Linguistics.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021. MiniLMv2: Multi-head self-\nattention relation distillation for compressing pre-\ntrained transformers. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 2140–2151, Online. Association for Computa-\ntional Linguistics.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 4008–4020, Dublin, Ireland. Association for\nComputational Linguistics.\n11275\nYiben Yang, Chaitanya Malaviya, Jared Fernandez,\nSwabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang,\nChandra Bhagavatula, Yejin Choi, and Doug Downey.\n2020. Generative data augmentation for common-\nsense reasoning. In Findings of the Association for\nComputational Linguistics: EMNLP 2020 , pages\n1008–1025, Online. Association for Computational\nLinguistics.\n11276\nAppendix\nA Reranker Configurations\nWe aim to understand the impact of different model configurations on the efficacy of our domain adaptation\ntechnique. We expand on experiments from Section 4.3 and explore alternate configurations of key factors\nin the UDAPDR approach. Specifically, we want to answer the following questions through their\ncorresponding experiments:\n1. (a) Question: Do corpus-adapted prompts improve domain adaptation and retriever distillation?\n(b) Experiment: Compare zero/few-shot prompts to corpus-adapted prompts for synthetic question\ngeneration (Table 4).\n2. (a) Question: How does the number of rerankers affect downstream retrieval accuracy?\n(b) Experiment: Compare single-reranker to multi-reranker approach (with different reranker\nselection strategies) across various target domains (Table 7).\n3. (a) Question: How does the synthetic query count affect downstream retrieval accuracy?\n(b) Experiment: Explore different query count configurations ranging from several thousand to\nhundreds of thousands of synthetic queries (Table 1).\n4. (a) Question: How do triple counts during distillation affect domain adaptation for ColBERTv2?\n(b) Experiment: Explore different triple counts ranging from several thousand to millions of triples\n(Table 8).\nIn Table 7 and Table 8 (and Table 1 in the main text), we outline the results of different experimental\nconfigurations in which we alter synthetic query generation and passage reranker training. Based on\nour results for the LoTTE pooled dataset, we find that training multiple rerankers and selecting the best\nperforming rerankers can improve our unsupervised domain adaptation approach. We generate multiple\ncorpus-adapted prompts and rerankers to prevent edge cases in which sampled in-domain passages and\nqueries have low quality. Furthermore, distilling the passage rerankers with ColBERTv2 allows us preserve\ntheir accuracy gains while avoiding their high computational costs. However, training many rerankers and\nselecting the best five rerankers can be computationally intensive and ultimately unnecessary to achieve\ndomain adaptation. The simpler approach of training several rerankers and using them for distillation,\nwithout any quality filtering, yields comparable results with only a 0.6 point drop in Success@5 on average\nwhile computing 10x less synthetic queries (Table 7). Additionally, by using our rerankers to generate\nmore triples for distillation with ColBERTv2, we were able to boost performance even further as shown in\nTable 8.\nB Fine-tuning Rerankers and Retriever\nFor all passage reranker models that we fine-tune, we optimize for cross-entropy loss using Adam (Kingma\nand Ba, 2014) and apply a 0.1 dropout to the Transformer outputs. We feed the final hidden state of\nthe [CLS] token into a single linear classification layer. We fine-tune for 1 epoch in all experimental\nconfigurations. Additionally, we using a 5e-6 learning rate combined with a linear warmup and linear decay\nfor training (Howard and Ruder, 2018). We use a batch size of 32 across all experimental configurations.\nFor our ColBERTv2 retriever, we use a 1e-5 learning rate and a batch size of 32 during distillation. The\nColBERTv2 maximum document length is set to 300 tokens. We use a BERT-Base model (Devlin et al.,\n2019) as our encoder.\nInstead of fine-tuning the rerankers, we also tried fine-tuning ColBERTv2 directly with the synthetic\ndatasets. We found that fine-tuning the retriever directly with the synthetic queries offered only limited\nbenefits, only improving zero-shot retrieval by 1-3 points of accuracy at best and decreasing zero-shot\naccuracy at worst (for the LoTTE Forum dev set). Distilling the rerankers offered more substantive gains\nand better adaptation to the target domains more generally.\n11277\nColBERTv2 Distillation with UDAPDR\nZero-shot\nColBERTv2\n1 of 50\nRerankers\n5 of 50\nRerankers\n10 of 50\nRerankers\n5 of 5\nRerankers\nZero-shot\nColBERTv2\n+ Reranker\nLoTTE Lifestyle 64.5 67.5 73.2 73.6 72.8 73.5\nLoTTE Techology 44.5 46.3 50.3 50.7 49.9 50.6\nLoTTE Writing 80.0 81.5 83.4 84.2 83.0 85.5\nLoTTE Recreation 70.8 73.7 77.8 78.3 76.7 79.1\nLoTTE Science 61.5 63.0 66.3 66.8 65.5 67.2\nLoTTE Pooled 63.7 66.3 70.3 70.7 69.6 71.1\nNaturalQuestions 68.9 70.1 73.0 73.5 72.7 73.9\nSQuAD 65.0 67.2 71.0 71.3 70.4 72.6\nTable 7: Success@5 for Multi-Reranker Domain Adaptation Strategies with Different Reranker Counts.\nLoTTE dataset results correspond to the Forum configuration. All results correspond to dev sets of each task. The\nreranker used in the experiments is DeBERTa-v3-Large. The ColBERTv2 distillation strategies trainX number of\nrerankers before selecting the best Y based on their performance on the dev set of the target domain. Through the\nselection process, we aim to find an upper bound for retrieval accuracy, even though access to an annotated dev set\nis not realistic for all domains. In our distillation strategies, each reranker was trained on 2,000 synthetic queries.\nFor our non-distilled reranker used in the final column, we trained it on 100,000 synthetic queries. The synthetic\nqueries were created using Flan-T5 XXL model and the prompting strategy outlined in Section 3.\nColBERTv2 Distillation with UDAPDR\nZero-shot\nColBERTv2\n1000\ntriples\n10000\ntriples\n100000\ntriples\nZero-shot\nColBERTv2\n+ Reranker\nLoTTE Lifestyle 64.5 67.4 70.1 72.4 73.5\nLoTTE Techology 44.5 46.0 47.8 50.2 50.6\nLoTTE Writing 80.0 82.5 83.4 85.0 85.5\nLoTTE Recreation 70.8 72.3 76.5 77.7 79.1\nLoTTE Science 61.5 62.0 64.2 66.5 67.2\nLoTTE Pooled 63.7 66.0 68.4 70.4 71.1\nNaturalQuestions 68.9 70.5 72.7 73.4 73.9\nSQuAD 65.0 67.2 70.6 71.5 72.6\nTable 8: Success@5 for Multi-Reranker Domain Adaptation Strategies with Various Distillation Triples\nCounts. LoTTE dataset results correspond to the Forum configuration. All results correspond to dev sets of each\ntask. The reranker used in the experiments is DeBERTa-v3-Large. The ColBERTv2 distillation strategies use a\nsingle reranker trained on 10,000 synthetic queries; this reranker then generates the specified number of labeled\ntriples. For our non-distilled reranker used in the final column, we trained it on 100,000 synthetic queries. The\nsynthetic queries were created using Flan-T5 XXL model and the prompting strategy outlined in Section 3.\n11278\nWe ran additional experiments testing UDAPDR’s efficacy on LoTTE Search dev, using one reranker\ntrained with a unique set of 2000 synthetic queries. We found that the approach boosted accuracy by 1.6\npoints, increasing accuracy from 71.5 to 73.1. However, since the synthetic queries could be generated so\ncheaply, we decided to scale to tens of thousands of synthetic queries for further experiments.\n11279",
  "topic": "Adaptation (eye)",
  "concepts": [
    {
      "name": "Adaptation (eye)",
      "score": 0.644472062587738
    },
    {
      "name": "Computer science",
      "score": 0.5795363187789917
    },
    {
      "name": "Domain adaptation",
      "score": 0.5299243330955505
    },
    {
      "name": "Distillation",
      "score": 0.47789013385772705
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4699682891368866
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4132692515850067
    },
    {
      "name": "Cognitive science",
      "score": 0.3218645453453064
    },
    {
      "name": "Psychology",
      "score": 0.24105939269065857
    },
    {
      "name": "Chemistry",
      "score": 0.17374154925346375
    },
    {
      "name": "Mathematics",
      "score": 0.1729855239391327
    },
    {
      "name": "Chromatography",
      "score": 0.09815013408660889
    },
    {
      "name": "Neuroscience",
      "score": 0.0917813777923584
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ]
}