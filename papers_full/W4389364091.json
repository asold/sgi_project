{
    "title": "SPROUT: an Interactive Authoring Tool for Generating Programming Tutorials with the Visualization of Large Language Models",
    "url": "https://openalex.org/W4389364091",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2313022044",
            "name": "Liu YiHan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2272795273",
            "name": "Wen Zhen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4365821777",
            "name": "Weng, Luoxuan",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Woodman, Ollie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1979858332",
            "name": "Yang Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1976031155",
            "name": "Chen Wei",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385682544",
        "https://openalex.org/W2789980854",
        "https://openalex.org/W4318931874",
        "https://openalex.org/W4225080353",
        "https://openalex.org/W4389989038",
        "https://openalex.org/W2593635859",
        "https://openalex.org/W3029367893",
        "https://openalex.org/W3202191909",
        "https://openalex.org/W4284880452",
        "https://openalex.org/W2592537823",
        "https://openalex.org/W3206079177",
        "https://openalex.org/W4387801187",
        "https://openalex.org/W4291476001",
        "https://openalex.org/W4411639645",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4309674289",
        "https://openalex.org/W4281763794",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4221055872",
        "https://openalex.org/W2786672974",
        "https://openalex.org/W4382654294",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4385571232",
        "https://openalex.org/W2142848577",
        "https://openalex.org/W2796354015",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4282826180",
        "https://openalex.org/W4286910674",
        "https://openalex.org/W4377371585",
        "https://openalex.org/W4366729084",
        "https://openalex.org/W4311426581",
        "https://openalex.org/W4225012671",
        "https://openalex.org/W4377130677",
        "https://openalex.org/W4402670429",
        "https://openalex.org/W4283705032",
        "https://openalex.org/W4385438855",
        "https://openalex.org/W2899498297",
        "https://openalex.org/W2094662297",
        "https://openalex.org/W4323033814",
        "https://openalex.org/W2099857277",
        "https://openalex.org/W4313563813"
    ],
    "abstract": "The rapid development of large language models (LLMs), such as ChatGPT, has revolutionized the efficiency of creating programming tutorials. LLMs can be instructed with text prompts to generate comprehensive text descriptions of code snippets. However, the lack of transparency in the end-to-end generation process has hindered the understanding of model behavior and limited user control over the generated results. To tackle this challenge, we introduce a novel approach that breaks down the programming tutorial creation task into actionable steps. By employing the tree-of-thought method, LLMs engage in an exploratory process to generate diverse and faithful programming tutorials. We then present SPROUT, an authoring tool equipped with a series of interactive visualizations that empower users to have greater control and understanding of the programming tutorial creation process. A formal user study demonstrated the effectiveness of SPROUT, showing that our tool assists users to actively participate in the programming tutorial creation process, leading to more reliable and customizable results. By providing users with greater control and understanding, SPROUT enhances the user experience and improves the overall quality of programming tutorial. A free copy of this paper and all supplemental materials are available at https://osf.io/uez2t/?view_only=5102e958802341daa414707646428f86.",
    "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nSPROUT: an Interactive Authoring Tool for\nGenerating Programming Tutorials with the\nVisualization of Large Language Models\nYihan Liu, Zhen Wen, Luoxuan Weng, Ollie Woodman, Yi Y ang, Wei Chen.\n✦\nAbstract—The rapid development of large language models (LLMs),\nsuch as ChatGPT, has revolutionized the efficiency of creating program-\nming tutorials. LLMs can be instructed with text prompts to generate\ncomprehensive text descriptions for code snippets provided by users.\nHowever, the lack of transparency in the end-to-end generation process\nhas hindered the understanding of model behavior and limited user\ncontrol over the generated results. To tackle this challenge, we introduce\na novel approach that breaks down the programming tutorial creation\ntask into actionable steps. By employing the tree-of-thought method,\nLLMs engage in an exploratory process to generate diverse and faithful\nprogramming tutorials. We then present SPROUT, an authoring tool\nequipped with a series of interactive visualizations that empower users\nto have greater control and understanding of the programming tutorial\ncreation process. A formal user study demonstrated the effectiveness\nof SPROUT, showing that our tool assists users to actively participate\nin the programming tutorial creation process, leading to more reliable\nand customizable results. By providing users with greater control and\nunderstanding, SPROUT enhances the user experience and improves\nthe overall quality of programming tutorial. A free copy of this paper and\nall supplemental materials are available at https://osf.io/uez2t/?view\nonly=5102e958802341daa414707646428f86.\nIndex Terms—Large language model, programming tutorial, authoring\ntool, interactive visualizations.\n1 I NTRODUCTION\nP\nROGRAMMING tutorials are used to teach learners cod-\ning skills and how to solve programming problems.\nNumerous programmers disseminate their programming\nexpertise through authoring various tutorials [1], such as\nblog articles, online videos, programming games, etc. As\nthe demand to learn programming has expanded, there has\nbeen a rise in the number of tutorial authors (e.g., developers\n[2], computer science educators [3], data scientists [4], etc.),\nand the research on improving authoring experiences and\nthe workflow of programming tutorials.\nThe recent prevalence ofLarge Language Models (LLMs)\nhas revolutionized the conventional process of authoring\narticles [5]–[7]. Similarly, programming tutorial authors can\nprovide LLMs with code snippets and text instructions to\nYihan Liu, Zhen Wen, Luoxuan Weng, Ollie Woodman, Yi Yang, and Wei\nChen are with the State Key Lab of CAD&CG, Zhejiang University. E-mail:\n{lyh1024 | wenzhen | lukeweng | orw | yang-yi | chenvis}@zju.edu.cn.\nWei Chen is the corresponding author.\nManuscript received April 19, 2021; revised August 16, 2021.\ngenerate tutorials. The prominent ability of LLMs dimin-\nishes the effort for authors to author programming tutorials.\nHowever, the text generated by LLMs is not always\nperfect or error-free [8]. Sometimes the exhaustive LLM-\ngenerated content is also not aligned with users’ expec-\ntations. These problems call for further refinement on\nthe auto-generated content. Owing to the inherent non-\ntransparency of the generation process of LLMs, users lose\naccess to the intermediate steps of the creation, leading\nto limited control and customizability when authoring ar-\nticles [6]. Additionally, to achieve ideal modifications of-\nten necessitates users’ continuous interactions with LLMs\nvia text-based conversation interfaces, which can not ade-\nquately support the complex requirements of users seeking\nto fine-tune tutorials. It is even more time-consuming when\ndealing with programming tutorials, as it requires precise\nverification for the correctness and consistency.\nThe aforementioned limitations pose challenges in the\nprocess of authoring programming tutorials using LLMs.\nThese challenges have an impact on the overall quality and\nreliability of LLM-generated programming tutorials. Some\nprevious works delve in training LLMs to generate code-\nrelated content [9], [10] or proposing advanced prompting\nmethods to promote the generation quality of LLMs [11]–\n[13]. However, to tackle with programming tutorial task,\nthese approaches are not out-of-box solutions for generating\nfaithful content accurately under human’s interventions.\nSeveral interactive systems have also been developed to\nenhance the efficiency of interactions with LLMs. While\nproving useful, these systems primarily concentrate on spe-\ncific needs, such as creative writing or data-driven article\nauthoring [6], [14]. Unfortunately, these approaches are not\nsuitable for the context of programming tutorial authoring,\nwhich necessitate accurate information organized in a more\ncoherent way aligning with the source code. Therefore,\nnovel approaches are needed that account for the need to\ngenerate customizable programming tutorial and guarantee\nits faithfulness and consistency with the source code.\nRecent studies propose instructing LLM to “think step\nby step” to enhance the faithfulness and consistency of the\ngenerated results [15], [16]. Inspired by this, we decompose\nthe tutorial authoring tasks into a step-by-step exploratory\nprocess towards final document accomplishment. This ap-\nproach has potential to enables users to attain a lucid com-\n0000–0000/00$00.00 © 2021 IEEE\narXiv:2312.01801v2  [cs.HC]  26 Oct 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nprehension and control over the intermediate steps through-\nout the entire creation process. Thus, this study investigates\nmethods to enhance user comprehension and control over\nthe text generation process with LLMs, with the ultimate\ngoal of improving the quality and experience of authors.\nFor the sake of better understanding the underlying chal-\nlenges in the authoring process, we conducted a formative\nstudy with 6 experienced tutorial authors to have a grasp\nof the writing process and the challenges they encounter\nwhen transitioning from their conventional workflow to\nthe authoring workflow with LLMs. Participants reported\nthat they can not effectively engage in the creation process\nusing typical conversation interfaces due to the difficulty of\ncontrolling the generated content solely with text. The lack\nof visual cues between source code and generated text also\nled to the overhead in comprehending and verifying the\ntutorial.\nConsequent upon the the findings of formative study,\nwe designed SPROUT1 (Step-by-step programming t utorial\nauthoring tool ), an interactive system that breaks down the\ntutorial creation task into actionable steps. These steps are\naccompanied by interactive visualizations that empower\nusers to have a greater control and understanding over the\nexploratory process, ending with more accurate and reliable\nresults. SPROUT adopts tree-like prompting strategies to\nallow LLMs to generate diverse and faithful tutorial con-\ntent more reasonably, which serves as a entry point for\nsubsequent user interventions to make accurate adjustment\non tutorial in different aspects. A series of purpose-specific\ninteractions are also provided on the basis of the real-time\nvisualizations of the text generation process with LLMs,\naiding users in crafting the tutorial from multiple levels.\nTo facilitate an intuitive understanding of the generated\ncontent, SPROUT leverages the inherent connections be-\ntween the source code and tutorial, which are distilled\nby LLMs and demonstrated with visual representations to\nusers, enabling the explicit comprehension of the creation\nprocess and model’s behavior. To evaluate the effective-\nness of SPROUT, we conducted a technical evaluation and\na user study. The technical evaluation demonstrated that\nSPROUT achieved accurate text-code connections. The user\nstudy revealed that SPROUT effectively assisted users in\ngenerating, modifying, and understanding programming\ntutorials with LLMs, ultimately leading to more reliable\nand customized results. In summary, this paper makes the\nfollowing contributions:\n• Understanding of the challenges inherent in the process\nof authoring programming tutorials with LLMs.\n• SPROUT, an interactive system that utilizes novel\nprompting strategies and interactive visualizations to\nfacilitate the step-by-step generation of programming\ntutorials with LLMs.\n• A technical evaluation demonstrating the effectiveness\nof our prompting methods and a user evaluation con-\nfirming the usability of SPROUT and its support for the\nprogramming tutorial authoring workflow.\n1. SPROUT is derived from“Step-by-step programming tutorial author-\ning tool” , with the metaphorical representation of the thoughts gen-\nerating process of Large Language Models, symbolizing the system’s\ngoal to enable users a more controllable experience by utilizing the\ninteractive visualizations of this process.\n2 R ELATED WORK\n2.1 Programming Tutorial Authoring Tools\nTutorials are a prevalent medium through which developers\ndisseminate coding and programming knowledge. Typi-\ncally, a tutorial consists of source code, textual explanations,\ncode examples, and multimedia components such as images\nand videos [17]. Crafting a high-quality tutorial necessitates\nconsideration of various perspectives [1], [18], [19]. Even\nexperienced authors require significant time to structure and\npresent the tutorial content. Therefore, numerous tools have\nbeen designed to improve the tutorial authoring process.\nIn-editor plugins like JTutor [20] and JTourBus [21] facilitate\nthe construction of Java tutorials within code editors. Some\nstudies leverage interaction provenance collected from oper-\nating systems [22] or web-based editors [23] to capture the\nprogramming process and produce tutorials. For example,\nVT-Revolution [24] offers interactive video tutorial creation\nbased on user actions, while Chat.codes [25] promotes col-\nlaborative code annotations and explanations. Other works\nsupport users in drafting code examples by propagating\nchanges to multiple code versions [26] or extracting concise\ncode using a mixed-initiative strategy [27].\nAmong the many forms of tutorials, the step-by-step\nstructure is particularly accessible for novice authors due to\nits straightforward nature. Also, describing code as “a tour\nthrough the source code” [21] proves beneficial for beginners\nin programming. Therefore, our primary emphasis is on the\nauthoring of textual content in step-by-step programming\ntutorials, where LLMs exhibit optimal performance. We\ninvestigate the pain points in the authoring process and\nleverage LLMs to assist users in efficiently generating, mod-\nifying and understanding tutorial content, which furthers\nthe line of research on programming tutorial authoring.\n2.2 LLMs for Document Generation\nThe remarkable generative capabilities of LLMs have\nspurred the proliferation of LLM-based applications across\nvarious domains in writing scenarios. In the domain of\ncode-related content generation, several studies investigate\nthe potential of LLMs in crafting code explanations by\ncomparing LLM-generated content with that sourced from\nstudents [28] or previous approaches [29], and validate\nits accuracy and comprehensibility. Some endeavors have\nintegrated these auto-generated code explanations into ed-\nucational scenarios. For instance, MacNeil et al. incorporate\nLLM-generated code explanations within a web software\ndevelopment e-book [30]. Other efforts utilize LLMs to gen-\nerate a myriad of CS learning materials including program-\nming assignments and multifaceted code explanations [31],\n[32]. Additionally, Sarsa et al. explore LLMs’ abilities in cre-\nating programming exercises and code explanations, with\nevaluations indicating that the generated content is novel,\nsensible and ready to use in certain cases [33]. There are\nalso developer tools for flexible code comment generation,\nfor instance, GitHub Copilot [34] provides comprehensive\ntext content pertaining to the code context. Some researchers\nhave developed LLM-based agents [35], [36] responsible for\ndocumenting in software engineering, showcasing substan-\ntial potential in authoring code-related content.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\nNonetheless, previous works mostly focus on generating\nhigh-quality content, assessing the thoroughness and accu-\nracy of code explanations. For effective presentation to read-\ners, such content often necessitates additional refinements,\neither through manual modifications or carefully-designed\nprompts. NLP researchers are actively examining diverse\nprompt strategies and frameworks to optimize LLMs’ per-\nformance [15], [37] or mitigate hallucinations [38], [39], seek-\ning to harness their full potential. Our work builds on top of\nthese endeavors by employing advanced prompt strategies\nto more effectively incorporate LLMs into the programming\ntutorial authoring workflow, ending with reliable and high-\nquality tutorials that meet users’ expectations.\n2.3 Visual Interfaces for LLMs\nDespite the impressive capabilities of LLMs, traditional text-\nbased conversational interfaces present challenges when it\ncomes to complex tasks that require the integration and ma-\nnipulation of diverse data types [6], [16], [40]–[42], as they\nlack the capacity for direct multimodal data manipulation\nand the visualization necessary for complex, dynamic con-\ntent creation and information synthesis [43], [44]. Therefore,\na variety of visual interfaces have been developed to en-\nhance user interactions with LLMs. For instance, researchers\nhave designed interactive interfaces for visual programming\nprompt chains to lower the barrier for non-AI-experts [45]–\n[47]. Some works aim to assist users generate better prompt\nwith fow-shot examples [48], coordinated visualizations [49]\nor direct manipulation actions [50] in graphical interfaces. In\nterms of diverse output representations, Graphologue [43]\ntranslates LLM responses into graphical diagrams, while\nSensecape [51] constructs a multi-level abstraction for in-\nformation exploration and sensemaking. Both of them fa-\ncilitate the information-seeking process. Other works focus\non designing interfaces for specific writing tasks, such as\nemail writing [52], story creation [14], argumentative writ-\ning [16], scientific writing [5] and journalistic angle ideation\n[53]. These interfaces ensure users achieve ideal results\nthrough iterative interactions. Notably, DataTales [6] intro-\nduces more flexible and intuitive interactions for authoring\ndata-driven narratives, while TaleBrush [54] utilizes line\nsketching interactions with GPT-based models to control\nand understand a protagonist’s fortune in co-created stories.\nMore recently, Kim et al. develop a framework for object-\noriented interaction with LLMs, which can generalize to a\nwider range of writing tasks [55].\nMotivated by these prior studies, our work aims to\nfacilitate the process of authoring programming tutorials\nwith LLMs. Despite the similarities to various writing tasks,\nthe unique aspects such as writing style, the process of\ncreation, the nature of data involved, and the necessity\nfor consistency and faithfulness make their methodology\nunsuitable for the task of authoring programming tutorials.\nThrough a set of interactive visualizations, our approach\nempowers users to actively participate in the generation\nand refinement of tutorials while maintaining flexibility and\nprecision in their control on the process. Additionally, we\nextract and present visual representations of the connec-\ntions between the source code and the corresponding LLM-\ngenerated results, facilitating users’ explicit understanding\nof the tutorial content.\n3 F ORMATIVE STUDY\nWe conducted a formative interview study to analyze the\nchallenges encountered in tutorial creation and workflow\nwith LLMs, from which we distilled four essential design\nrequirements to improve the authoring process.\n3.1 Participants and Procedure\nIn order to comprehend the challenges and difficulties en-\ncountered across various tutorial authoring scenarios, six\nexperienced tutorial authors were interviewed (age from 23\nto 28). Their wide-ranging expertise in authoring diverse tu-\ntorials for various users spans from novice to expert levels.\nThus, the insights derived from their collective experiences\nprovide a more comprehensive and universally applicable\nunderstanding, beneficial to programming tutorial authors\nof various proficiency levels. Four of them are experienced\nprogrammers (E1-4) and two are educators. One of the two\neducators teaches computer science (E5), while the other\ninstructs competitive programming (E6). All of them have\nmore than three years of experience authoring programming\ntutorials. In addition, they all have used ChatGPT in recent\nmonths. Before the interview, we collected twenty text-\nbased programming tutorials from various sources, includ-\ning Stack Overflow, Wikipedia, GeeksforGeeks, etc., as well\nas those auto-generated by LLMs. In our formal interviews,\nwe first inquired about the authors’ writing process and\ncommonly-used tools for creating tutorials, then presented\nthem with all of the collected tutorials and solicited their\nopinions on the advantages and downsides of each. Af-\nterwards, participants were asked to create programming\ntutorials utilizing ChatGPT and document editors. Finally,\nwe collected feedback on their authoring experience, fo-\ncusing on how they utilized LLMs in their workflow and\nwhat made it challenging to create high-quality tutorials in\nthis process. The interviews were conducted through online\nmeetings and lasted 45 to 60 minutes.\n3.2 Findings\nAll participants tried to author programming tutorials with\nLLMs. They provided the source code, retrieved the result-\ning tutorial, and iteratively modified it by seeking Chat-\nGPT’s assistance. We found that they usually managed to\ngenerate different versions with multiple categories of con-\ntent to enrich the tutorial. The five most common categories\namong them are title, background, code explanation, noti-\nfication, and summary. The behavior analysis indicated a\nfrequent tendency among participants to leverage LLMs for\ngenerating content, structuring the tutorial, and polishing\ndetails. However, during the tutorial authoring process with\nLLMs, participants commonly encountered challenges that\nimpacted production efficiency.\nDifficult to generate tutorials that accurately reflect\ndiverse coding scenarios. This challenge stems from a need\nto encompass a wide array of programming paradigms,\nlanguages, and problem-solving techniques within instruc-\ntional content. It potentially creates gaps in understanding\nfor learners who come from different experience levels or\nwho are looking to apply their skills to a variety of con-\ntexts. As such, during the authoring process with ChatGPT,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nparticipants often complained about the difficulty to obtain\ntutorials aligning with their mental model. For example, E5\nhoped that ChatGPT could only explain the crucial parts of\nthe code since the tutorial is for experienced programmers,\nbut only found the output elaborated on some unimportant\ndetails, calling for additional effort in modifying the prompt\nto generate tutorials again. Some participants proposed the\ndesire to obtain tutorials with different frameworks, which\ncan “provide more inspiration when having no ideas about what\nto do next” (E4). Apart from that, during the iterative tutorial\nauthoring process, some participants noted that the content\ngenerated by ChatGPT occasionally deviated from being\nfaithful to the original source code. It is significant to keep\nthe tutorial consistent with the original source code. Such\nexperiences underscore the need for reasonable prompting\nstrategies for generating rich and reliable content in diverse\nwriting scenarios with different requirements.\nLack of flexible interactions for tutorial generation and\nmodification. As auto-generated content seldom fully met\nparticipants’ expectations, they had to iteratively modify\nthe tutorial content during the exploration. Some of them\nsought to simplify this process using ChatGPT, but only to\nfind it “inconvenient and confusing” (E6). E6 reported that\ncombining two paragraphs to shorten the document neces-\nsitated a meticulously crafted prompt, otherwise ChatGPT’s\nresponse was merely “a non-sensible mess” (E6). Additionally,\nparticipants mentioned concerns over the tediousness of\nconstantly “switching back and forth between the editor and\nChatGPT” (E4), crafting prompts, and ensuring that the new\ncontent seamlessly integrated into the existing context.\nSignificant overhead for understanding and verifying\nthe tutorial. The linear conversation interface of ChatGPT,\nwhere user inputs and LLM outputs appear sequentially,\nbecomes problematic due to the lack of visual linkages be-\ntween code and text, particularly in multi-step interactions\nwith iterative refinement. They usually needed to“browse the\nwhole document” (E5) to discern which code segment each\nparagraph is describing, as ChatGPT omitted code refer-\nences. The situation was exacerbated during modifications,\nas responses often excluded the original content and code,\nforcing participants to “painstakingly match code snippets to\ntheir text because of their separate presentations”(E3). Moreover,\nthe independently showcased nested code often lost its\nproper indentation, pushing participants to trace back to its\ncontext. This caused “additional mental overload to recognize\nthe correct execution scope” (E2). Collectively, participants felt\nthat these unpredictabilities “somewhat negated the intended\nefficiency gains from Large Language Models” (E6).\n3.3 Design Requirements\nTo tackle the problems concluded above, we aim to imple-\nment an interactive system to improve the authoring process\nwith LLMs and obtain desirable programming tutorials. The\nDesign Requirements can be summarized as follows:\nDR1. Generating diverse and faithful content. In pro-\ngramming tutorial authoring, users tend to utilize LLMs\nto generate a wide range of content, including variations\nin structure, level of detail, and other aspects, in order to\ninspire their writing ideas. Simultaneously, they require the\ngenerated content aligns precisely with the intended func-\ntionality and logic. Therefore, the system should generate\ndiverse content while maintaining its faithfulness to the\noriginal source code provided by users.\nDR2. Supporting precise and in-context modification.\nUsers often engage in iterative interactions with LLMs\nthrough text-based conversations to refine the generated\ntutorial content according to their preferences, mainly focus-\ning on the aspects of tutorial structure and content details.\nHowever, even slight modifications in the input prompts\ncan result in significant and unforeseen changes in the entire\ngenerated output. To maintain the coherence and accuracy\nof the final output, the system should support users in mak-\ning precise modifications while preserving the contextual\nintegrity of the existing generated tutorial content.\nDR3. Maintaining explicit connection between inputs\nand outputs. Users usually need to carefully read the source\ncode and outputs with the purpose of understanding and\nverifying the LLM-generated content. However, in linear\nconversation interfaces, this task becomes time-consuming\nand challenging. Additionally, the lack of transparency and\ntraceability between user inputs and model outputs can\nresult in inadequate trust and confidence towards the fi-\nnal tutorial. Therefore, in this system, the tutorial content\nshould be augmented with explicit visual representations of\nconnections between the generated content and its related\ncode snippets, which should be extracted and maintained\nby the system. This allows users to easily comprehend and\nverify the tutorial and the reasoning behind it.\nDR4. Providing intuitive and flexible interactions.\nUtilizing natural language as the channel to communicate\nwith LLMs poses challenges for users to clearly convey\ntheir requirements and ideal output. In order to acquire\nqualified and satisfactory tutorials, users have to craft them\ncontinually, by some cumbersome editing operations and\niterations on prompts for generation and modifications on\nthe draft from ChatGPT. Therefore, to circumvent these\nissues, the system should be synthesized with intuitive and\nflexible interactions to eliminate users’ efforts for manual\nadjustment on content or revision on prompts, for instance,\nenable direct manipulation of the source code or generated\ntextual content. This allows authors to focus more intently\non the programming tutorial itself.\n4 P ROMPTING FOR PROGRAMMING TUTORIAL AU-\nTHORING\nA primary goal of our research is to make the LLM gener-\nation process more transparent and controllable. To achieve\nthis, we adopt the tree-of-thought (ToT) methodology [15]\nto prompt models. It enables models to take intermediate\nsteps towards completing the tutorial. By adopting the ToT\nmethodology, we allow models to explore multiple choices\nduring generation process. This promotes a more interactive\nand iterative approach to tutorial authoring, empowering\nusers to have greater control over the generated content. In\nthis section, we present our proposed prompt design that\ncombines the ToT methodology with the specific require-\nments of programming tutorial authoring. Figure 1 illus-\ntrates the framework of our prompt strategies. Our prompt\nstrategies are developed and tested on gpt-3.5-turbo\nversion of ChatGPT [56], one of the most popular LLMs.\nPlease refer to supplemental materials for original prompts.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nObservation: \nThe distance update \nstep of the code snippet \nhas been explained.\nThought 1: \nI should explain the \nfinal step of the code\nThought 2: \nI should write a noti-\nfication for ...\nThought 3...\nAction:  \n{step_number: \"5\", \ncode: \"return distan..\", \nexplanation: \"Last,...\", \nsummary: \"Return Result\"}\nSource Code\ndef dijkstra(g, start, end):\n  distances = {node: 32767 for\n  distances[start] = 0\n  nodes = [node for node in g]\nWrite Title\nAction List\nWrite Background \nWrite Explanation\nWrite Notification\nWrite Summary\nObservation\nTree of Thoughts\nVote\nTitle\nBackground\nExplanation\nExplanation\nMemory\nWrite \nExplanation\nProgramming\nTutorial\nAuthoring Task Action\nTutorialAgent\nHuman\nInstruction\nB\nA\nC\nD E\nF\nFig. 1. The framework of SPROUT’s prompt strategies. We break down the programming tutorial creation task into actionable steps (A) which\nare provided as initial system prompts to the agent. During the generation process, the agent receives the source code (B) and user instructions,\ngenerating multiple potential thoughts (C) based on its memory and the observation derived from the source code and current tutorial content (E).\nThen it takes the proper action (D) and plans for subsequent steps. The iterative process continues until the tutorial is complete. (F) shows an\nexample iteration.\n4.1 Generation of Thoughts\nWe envision the LLM as an agent that takes on the role\nof making decisions on how to plan and execute steps to\naccomplish programming tutorials. The agent is instructed\nto think in a step-by-step manner to generate faithful con-\ntent, and expand thoughts in a tree-like structure to produce\ndiverse tutorial content (DR1).\nDividing task into actions. We break down the pro-\ngramming tutorial creation task into discrete and action-\nable steps. This allows the agent to consider these steps\nas fundamental units of a plan. Through the formative\nstudy (section 3), we have identified 5 key components\nof tutorial content, including the title, background, code\nexplanation, notification, and summary. Consequently, we\ndefine a list of actions that correspond to writing differ-\nent content within the tutorial, e.g., ‘write title’ or\n‘write background’. The action list and the descriptions\nof actions are provided to the agent before solving tasks.\nThinking step by step. To guide the decision-making\nprocess leading to faithful results, we instruct the agent to\nthink step-by-step following the ReAct (Reason and Act)\nparadigm [37], which requires the agent to solve the task in\na specific order: observation, thought, action. At each step, the\nagent first presents the observation derived from the source\ncode and the current content of tutorial. Subsequently, it\ngenerates the thought of the appropriate next action to be\ntaken with a reasonable justification. The agent then takes\nthe action and continues to plan for the next step. This pro-\ncess continues iteratively until the tutorial creation process\nis completed. This systematic approach allows the agent\nto make informed decisions at each stage of the tutorial\ncreation process, resulting in faithful and reliable outcomes.\nExpanding tree of thoughts. We integrate the tree-of-\nthought methodology in the decision-making process to\nenable the agent to generate multiple potential thoughts\nfor consideration, leading to diverse content in the gener-\nated tutorial. For example, after explaining a specific code\nsnippets, the agent may generate thoughts such as writing\na notification highlighting common mistakes related to the\ncode or directly providing a summary to conclude the\ntutorial. Consequently, the agent votes on the generated\nthoughts and make a decision on the next action to take. To\nprovide an exploratory space for authors, we maintain the\ntree of thoughts as a memory space for the agent. Users are\nallowed to navigate through the tree of thoughts to explore\nmultiple potential results of generation.\n4.2 Manipulation of Model Actions\nThe above prompt strategies enables a completely automatic\nprocess to create tutorial without human intervention. We\npropose to perform in-context interventions by inserting\ninstructions in the gap of each step to manipulate the model\nactions. This capability enables authors to make contextual\nadjustments to the tutorial materials, resulting in more\ntailored and customized programming tutorials (DR2).\nIn particular, we design prompt strategies to support\nauthors in the process of generating content (generation),\nadjusting the structure of tutorial (organization), and refin-\ning the detail of content (refinement), which are derived\nfrom the formative study (section 3). These strategies serve\nas underlying methods within features of our system (see\nsection 5):\n• Generation: “The next step should be to write for ⟨ code ⟩”.\nThis prompt is used to enable users to define a specific\nrange of code to explain. It instructs the agent to thought\nclosely associated with the specific code fragments when\ngenerating new content for tutorial.\n• Organization: “Explain ⟨ code ⟩ in the next multiple steps” or\n“Explain ⟨ code ⟩ in one paragraph”. These prompts are used\nto adjust the structure of the tutorial. They assist users in\norganizing the tutorial by separating the explanation of\nimportant code into multiple steps or summarizing less\nimportant code in a concise manner.\n• Refinement: “Explain ⟨ code ⟩ in the style of ⟨ style ⟩”,\n“Refine the last step ⟨ ‘shorter’ — ‘longer’ ⟩”. These prompts\nare used to refine the writing style or level of detail in\na paragraph, which allows users to modify the tutorial\ncontent in a more granular manner.\nThese prompt strategies empower authors to iteratively\nrefine and customize the programming tutorial content ac-\ncording to their individual preferences and needs.\n4.3 Extraction of Text-Code Connections\nThe relationship between the LLM-generated tutorial and\nits underlying code are not always clarified. The extraction\nof text-code connections strategy focuses on identifying and\nestablishing links between relevant textual explanations and\ncorresponding code snippets (DR3).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\nWe instruct the agent to present each code-related text\naccompanied with its connection to the code. The agent is\nrequired to provide extra information when writing textual\ndescriptions for a specific piece of code, e.g., step number ,\ncode, explanation, and summary. We then link the reference\ncode with the source code using string matching. Figure 2\nshowcases an example of our method. This strategy is\nseamlessly integrated into the generation process, and also\nensures that textual explanations are closely associated with\nthe relevant code, enhancing the overall comprehensibil-\nity and effectiveness of the tutorial materials. The tree of\nthoughts annotated with extracted connections helps au-\nthors structure their thoughts and logically sequence the tu-\ntorial materials. It assists authors in creating and organizing\ntutorial content effectively, aiding authors in maintaining\ncoherence and clarity throughout the tutorial.\nAgent\nOutput in JSON from LLM\nSource Code\n1  def dijkstra(g, start, end):\n2    distances = {node: 32767 �))\n3    distances[start] = 0\n4    nodes = [node for node in g]\n5    �))\n{step_number: \"2\", \n code: \"nodes = [node for \n node in g]\", \n explanation: \"Create a \n list of nodes: Next, \n we create a list called \n nodes which contains \n all the nodes in the  \n graph.\", \n summary: \"Create List\"}\nPrompt for Response\n\"The step number, such as \n 1, 1.1, 1.2, 2, 2.1, ...\", \n\"Part of the code snippet to \n explain\", \n\"Text description for the \n code snippet\", \n\"Summary for this step using \n 1 or 2 words\"\nstep_number:\ncode:\nexplanation:\nsummary:\nFig. 2. An example output with text-code connection. The agent is in-\nstructed to provide text-code connection information within its response.\n5 SPROUT\nWe develop SPROUT, a prototype system that supports\nefficient and flexible step-by-step programming tutorial au-\nthoring with LLMs according to interactive visualizations\nof the intermediate generation process. In this section, we\nfirst present an overview of the system, introducing the\ninterface design and visual representations in section 5.1.\nNext, we introduce a series of features that are designed\nfor enhancing user experience in the process of tutorial\nauthoring (section 5.2-section 5.6).\n5.1 Overview\nOur system consists of 6 views as demonstrated in fig. 3.\nThe code view (fig. 3 A) allows users to enter the source\ncode they intend to describe about. The tutorial view (fig. 3\nB) displays the text content generated by LLM with a\nblock-based structure, where a block presents a paragraph.\nBetween the code view and tutorial view lies thechain view\n(fig. 3 C), which serves as a connector, assisting users to\nnavigate between code and tutorial. Within this view, each\nnode\n is a simplified visual representation of the discrete\noutput units generated during the step-by-step creation\nprocess. Each node displays information about a specific\nparagraph and the corresponding code it describes. The\nchain\n is concatenated by these nodes, which collectively\nrepresent the entire tutorial.\nThe outline view (fig. 3 D) visualizes the intermediate\ngeneration steps of LLM in the form of a tree graph\n . It\nconsists of nodes\n and branches\n , illustrating the hier-\narchical structure of the LLM’s thought process. The branch\nview (fig. 3 E) enables users to switch between different\nbranches\n to explore alternative thought paths and make\nadjustments to the structure of generated tutorial. The node\nspace view (fig. 3 F) offers alternative content options for a\nselected node. Users have the flexibility to choose from these\nalternatives and customize them in multiple dimensions.\n5.2 Content Generation from Code Snippets\nSPROUT supports two interactions that enable the genera-\ntion of diverse and user-customized content from the source\ncode provided by users (DR1, DR4).\nAgent generation. Sometimes users have limited knowl-\nedge about the code they want to describe or feel hesitated\nabout the next steps to take. In this situation, they can\nmake the LLM-based agent shoulder the responsibility to\nsketch the framework. In the tutorial view (fig. 3 B), users\ncan click on the Generate option to start the generation.\nSPROUT then automatically generates the tutorial content\nstep by step, following the tree-of-thought methodology.\nTo ensure users’ awareness and control over the tutorial\ngeneration process, SPROUT updates the tutorial content\nand renders the tree graph in real-time. Meanwhile, users\ncan use the Pause option to halt the generation process and\nreview the current tutorial content, ensuring it aligns with\ntheir expectations, especially if the auto-generated content\nsignificantly deviates from what users anticipated.\nUser-defined generation. In some cases, users have a\nclear intention regarding the code snippet they want to write\nabout next. However, they may find it challenging to effec-\ntively communicate their ideas in written form to inform\nthe LLMs. SPROUT saves users’ arduous work to write and\ntest various prompts to get a satisfactory response. Instead,\nthey can simply brush their target code snippet and click\nthe floating button to effortlessly add new content to the\ncurrent tutorial (fig. 3 A1). Supported by prompting strate-\ngies we proposed (section 4),SPROUT generates paragraphs\nrelated to the code with consideration of the coherence to\nthe surrounding contexts. And the node corresponding to\nthe newly generated content is appended to the end of\nthe current chain. This ensures that the generated content\nis seamlessly integrated and follows a logical progression\nwithin the existing tutorial.\n5.3 Tutorial Modification through Tree Graph\nSPROUT enables users to utilize the tree graph as the pri-\nmary controller for flexible and intuitive modification of the\ntutorial content through manipulating the nodes within the\ntree (DR2, DR4). This feature aims to alleviate the need for\nusers to directly prompt the LLMs and reduces their burden.\nFor instance, users can consolidate multiple paragraphs to\nreduce their quantity, emphasize important code snippets by\nelaborating on them, and conveniently remove unnecessary\nparts from the tree graph for easier content management.\nCondensing through node grouping. Sometimes LLMs\nprovide overly exhaustive content for users, resulting in\nunclear key points in the tutorial and overwhelming read-\ners. Users have to manually identify paragraphs that are\nless important or too familiar to the majority of readers\nand condense the content together in the typical workflow.\nSPROUT thus supports node grouping in the outline view\nto simplify this process. For example, when a user finds out\nthat the content of two paragraphs can be integrated into\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nA B\nC\nD E F\nD1\nA1\nB1\nF1 F2\nFig. 3. The system interface of SPROUT. The Code View (A) displays the source code. The Tutorial View (B) presents the LLM-generated tutorial\ncontent. Between them is the Chain View (C), showing the node chain of current paragraphs selected by users. The Outline (D), Branches (E), and\nNode Space (F) Views provide interactive visualizations for multi-level modifications such as elaborating, adjusting structure, and polishing content.\none, they can select them in the tree graph and navigate\nto the Group option to make the LLM achieve a seamless\ncontent combination (fig. 4). The selected nodes will all be\nreplaced by the newly-integrated node in a new branch,\nensuring access to their old version.\nB\nA\nFig. 4. Select two nodes (A) and group them into one (B).\nElaborating through node splitting. In addition to con-\ndensing, elaboration is another essential task for maintain-\ning a balanced quantity throughout the entire tutorial. When\nusers explore the tree graph and discover that a key concept\nis mentioned but lacks sufficient explanation, they can select\nthe corresponding node and navigate to the Split option.\nSPROUT prompts the LLM to generate additional content,\nwhich is then presented in new paragraphs. Simultaneously,\nSPROUT creates a copy of the branch where the selected\nnodes are located, and the selected nodes are split into\nnewly-generated nodes (fig. 3 D1).\nDeleting through node trimming. Since SPROUT show-\ncases complete thinking paths of the agent when authoring\nthe tutorial, the tree graph may accumulate redundant\nnodes as users iterate on the tutorial, making it difficult\nto navigate and manage effectively. Therefore, SPROUT\noffers a node trimming feature that allows users to delete\nspecific nodes in the tree graph that contain the undesired\ncontent. When users pinpoint some redundant or unnec-\nessary paragraphs, they can use the Trim option to delete\nthe corresponding nodes, and their children nodes will be\nremoved from the tree at the same time (fig. 3 D1).\n5.4 Context Switch across Branches\nTo facilitate a more diverse (DR1) and flexible (DR4) orga-\nnization of the tutorial’s content, SPROUT provides two\ninteractions to assemble paragraphs and switch between\nvarious contexts generated by LLMs flexibly.\n5.4.1 Quick Assembling\nSPROUT supports swift switching between different\nbranches. When users are not satisfied with the current\nversion of the tutorial and desire to explore additional\npossibilities, they can select an arbitrary node in the graph\nas the tail of the chain. As a result, the selected node and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nits ancestor nodes will be automatically assembled together\nand replace the original chain (fig. 5). By selecting an arbi-\ntrary node as the starting point for exploration, users can\ndelve into new directions and uncover untapped potential\nin the tutorial’s structure and content.\nA2 B2A1 B1\nFig. 5. Users can achieve quick assembling nodes by selecting any node\nin the tree graph as the tail of the chain. For example, after user clicks\non node (A1) in outline view, the content in tutorial view (A2) updates\ncorrespondingly. (B1) and (B2) is another example.\n5.4.2 Step-by-Step Assembling\nInstead of quickly assembling a chain of nodes, SPROUT\nalso provides a deliberate method to manually decide on\neach step from any selected node in the graph as the entry\npoint. The branch view (fig. 3 E) is designed to demonstrate\nthe details of multiple reasoning paths from the selected\nparent node, which encompasses two aspects:\n• Choices: When the agent generates several candidate\nthoughts for the next step, we recommend the best choice\nthrough a voting process. At each step, the agent casts\nits vote for the most promising thoughts. SPROUT then\npresents the top 3 choices, visually representing the vot-\ning results through the line width that connects the nodes.\nThis visual encoding serves as a reference for evaluating\nthe reliability of the diverse thinking paths.\n• Reasons: SPROUT instructs the agent to provide explicit\nreasons for its choices. Leveraging our prompt strate-\ngies, the agent mostly bases its reasoning on the logical\nconnection between code snippets or the coherence of\ntextual paragraphs related to nodes. SPROUT presents\nthe textual reason of each choice on the links, enabling\nusers to make more informed and discerning decisions.\nWhen a candidate node is chosen, the branch view\ntransitions to a deeper level, allowing users to delve further\ninto exploring the possibilities for the next paragraph. This\nprocess can be repeated iteratively until users finalize the\nnew framework of the tutorial. Moreover, users retain the\ncapability to navigate back to previous nodes, allowing\nthem to backtrack and revise their decisions whenever nec-\nessary. This iterative and flexible approach empowers users\nto progressively explore the content of the tutorial.\n5.5 Detail Refinement in Node Space\nAfter confirming the structure of the tutorial, users often\nneed to further refine the textual content to align it with\ntheir expectations (DR2). They may not satisfied with the\ntext content pertaining to a particular intent, such as an\nexplanation for specific code fragments, and try to refine it.\nTo streamline this process, SPROUT implements two-part\ninteractions, providing users with a systematic approach to\nrefine the content and achieve the desired outcome (DR4).\n5.5.1 Exploring Alternatives\nSPROUT offers users the ability to chose alternative repre-\nsentations for a specific intent. LLM-generated paragraphs\nare visualized as scattered nodes in the node space view\n(fig. 3 F1). We utilize OpenAI’s text embedding model [57]\nto measure the semantic similarity of generated paragraphs\nand projected the embeddings onto a 2D plane using the\nUMAP algorithm [58]. As a result, paragraphs with similar\nmeanings are positioned closer to each other in this 2D\nspace, which facilitates the quick retrieval of semantically\nrelevant alternatives.\nThe node alternatives that share the same intent as the\nuser-selected node are highlighted, with color distinguish-\ning their origin, aligning with the color encoding in the\noutline view (fig. 3 D). Nodes are considered to have the\nsame intent if they are generated through the same action\ntype and are associated with the same code fragments. For\nexample, two nodes may both describe the first line of code\nbut with distinct representations. This view enables users to\nfilter and select content that aligns with their specific intent,\nserving as a foundation for subsequent steps.\n5.5.2 Rewriting Details\nAccording to the feedback from the formative study, we\nconclude several aspects that users primarily focus on when\nthey intend to refine the generated tutorial content.SPROUT\nthen allows users to customize the content from the follow-\ning aspects:\n• Writing styles. Users may have their preferences for the\nstyle of the content. SPROUT provides several options to\nmake LLMs rephrase the content in an designated style.\n• Level of details. Users might be troubled by the unpre-\ndictable initial quantity of LLM-generated content, while\nan excellent tutorial should be reasonably detailed. There-\nfore, SPROUT allows users to adjust the level of details\nwhile preserving the originally conveyed information.\n• Prompt Refinement. In order to meet a wider range of\nuser needs, in addition to the two aspects mentioned\nabove, SPROUT also supports authors to freely modify\nthe prompt in order to improve the content.\nAs shown in fig. 6, when users find the background is too\nbrief and not attracting enough, they can adjust the level\nof details and choose a polishing style in the configuration\npanel, then navigate to the Confirm option to prompt the\nLLM to rephrase the content accordingly.\nA\nC\nB\nFig. 6. In SPROUT, users can polish the content and adjust the levels\nof detail of any paragraph (A) by setting options in configuration panel\n(B), then retrieve polished paragraph (C) from LLM.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\n5.6 Visualization for Tutorial Understanding\nTo strike a balance between clearly separating two kinds of\nmedia resources (i.e., code and text) and offering visual el-\nements of their connections (DR3), SPROUT designs visual\nrepresentations for users to understand what the text block\nis about and where it comes from.\nBrief Block. Although the chain view provides the\nstructure of a tutorial explicitly, it is still arduous work for\nusers to grasp the main idea of each paragraph. Therefore,\nSPROUT prompts LLMs to brief each paragraph in few\nwords to enable users to quickly comprehend the key con-\ncept it intends to convey.\nElements Matching. To reduce the switch of users’ at-\ntention between the code view and the tutorial content,\nSPROUT presents a set of visual cues for element match-\ning between the source code and the tutorial document.\nWe extract the connection between the code snippet and\nparagraph, then present the information in the node. Fur-\nthermore, SPROUT provides visual links of the connection\nbetween code snippet and text blocks when users focus\non certain node or text block; thus, they can locate the\ntext and code quickly. During the tutorial generation stage,\nthe focused node will be automatically updated as the\nlatest generated node, enabling users to follow the LLM’s\ngeneration process more easily and absorb the newly-added\ninformation in a more moderate way.\n6 T ECHNICAL EVALUATION\nTo validate the effectiveness of our prompting technique, we\nconducted a technical evaluation focusing on the accuracy\nof the extracted text-code connections.\n6.1 Experiment Settings\nDataset. We collected 10 code snippets with diverse pro-\ngramming languages, lengths, and structures, covering dif-\nferent categories. These snippets were fed into our system\nto automatically generate programming tutorials. Then, we\nmanually labeled the code segments that each paragraph\nreferences, establishing a ground truth for evaluating text-\ncode connection extraction accuracy.\nProcedure. The labeling work and accuracy calculation\nwere conducted by 3 experienced programmers. They sepa-\nrately manually labeled the connection between each para-\ngraph and code segments. They then compared the three\nlabeled results and performed a secondary verification if\nthere were differences among these results. This procedure\nwas designed to guarantee the accuracy of the result, pre-\nventing potential errors that may occur during an individ-\nual’s labeling process. After that, they examined the text-\ncode connections extracted by SPROUT to calculate their\naccuracy. Subsequent analyses were conducted on failure\ncases and contributing factors to these errors.\n6.2 Results\nThe dataset comprised 10 programming tutorials, encom-\npassing 138 paragraphs and the corresponding text-code\nconnections. For the details of the procedure, generated con-\ntent, and statistical analysis, please refer to the supplemental\nmaterials.\nMetrics. The extracted text-code connections exhibited\nan overall accuracy of 86.2%, comprising 119 correct con-\nnections and 19 erroneous ones.\nError Analysis. Of the 19 erroneous connections iden-\ntified, we categorized them into three types: (1) No Code\n(5/19), (2) Incorrect Code Range (12/19), and (3) Incorrect\nCode Content (2/19). For the first error type, the LLM failed\nto provide any code for the associated paragraph. For the\nsecond error type, the LLM provided an inappropriate\ncode range mismatched with the corresponding paragraph.\nNotably, the majority (10/12) presented the entire code\nrange excessively, whereas only two instances demonstrated\nincomplete or unrelated connections. For the third error\ntype, the LLM provided fictitious code content misaligned\nwith the original code snippet. We discern these errors as\nmanifestations of LLMs’ hallucination, particularly evident\nwhen employing complex prompting strategies like Tree-of-\nThoughts prompting strategy, coupled with a high temper-\nature which might lead to more unexpected results.\nSummary. Despite the few failure cases, the results in-\ndicate the reliability and accuracy of our prompting tech-\nnique in extracting text-code connections. This efficiently\nfacilitates users in obtaining faithful content, laying a robust\nfoundation for further refinements of the content.\n7 U SER STUDY\nTo evaluate the effectiveness of our system in facilitating\nthe programming tutorial authoring process, we conducted\na user evaluation to collect feedback on (1) the features of\nSPROUT, and (2) its support for tutorial authoring com-\npared to a baseline interface. Subsequently, we analyzed\nuser interaction logs to gain an understanding of the emer-\ngent workflow patterns and user behaviors using SPROUT.\n7.1 Methodology\n7.1.1 Participants\nWe recruited 12 participants (P1-P12) for our experiment,\nconsisting of 5 females and 7 males, aged 22-28, from a\nlocal university. P1-6 were experienced tutorial authors who\nhad also participated in the formative study (E1-6). The\nremaining participants were students with experience in\ncode documentation, but none had experience in authoring\nprogramming tutorials. All participants had more than 4\nyears of programming experience and had read program-\nming tutorials in the past. Ten of them were familiar with\nChatGPT and regularly utilized it for writing tasks ( e.g.,\nideation, drafting, polishing), while two had heard of it\nbut seldom incorporated it into daily work. Participants\nattended our experiment through online meetings, used our\nsystem deployed online, and the process was recorded.\n7.1.2 Experiment Condition\nThe experiment was conducted under two conditions:\n• ChatGPT ( baseline). The baseline interface offered an\nenvironment comprising a document editor, a code\nviewer, and ChatGPT. Users could freely arrange these\nelements within the interface.\n• SPROUT (ours). The system integrated a code viewer\nand document editor, enriched with a series of interac-\ntive visualizations of the creation process with LLMs.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nEach participant was tasked with creating tutorials using\nboth of the aforementioned tools. The order of tool usage\nwas counterbalanced among participants: half began with\nChatGPT, while the other half started with SPROUT.\n7.1.3 Procedure\nThe experiment consists of the following phases:\nIntroduction and Training. First, the purpose of the ex-\nperiment was presented. Participants then signed a consent\nform and filled out a pre-study questionnaire regarding\ntheir background and experience with tutorial authoring.\nAfter that, they were introduced to SPROUT’s features,\nfollowed by a demonstration of crafting a complete algo-\nrithm tutorial using these features. We gave the participants\nadequate time to familiarize themselves with the system and\nencouraged them to reproduce the tutorial independently.\nAll participants were free to ask any clarifying questions.\nTargeted Task.For this task, participants were provided\nwith a piece of code for reference. Their objective was to\nauthor a tutorial based on a given framework, which in-\nvolved two sub-tasks: modifying the content of at least one\nparagraph and describing the main idea of each paragraph.\nParticipants underwent two trials in this phase, creating\nprogramming tutorials in each trial using two different tools\n– baseline and ours – respectively.\nOpen-ended Task. For this task, only the problem’s\nbackground and the source code were provided. Partici-\npants were encouraged to create a step-by-step program-\nming tutorial freely using SPROUT. They were expected\nto produce at least three tutorial versions and select one\nas the final result. We ensured participants had enough\ntime to validate the generated content. There were no time\nconstraints on the writing process during trials, allowing\nthem to create tutorials to the best of their abilities.\nPost-study Interview. Upon completion of all trials, we\nconducted a semi-structured interview with each partici-\npant. They were first invited to fill out a questionnaire\nwith 5-point Likert-scale questions, regarding five features\nin SPROUT, the general usability, and the support for\nprogramming tutorial authoring with our system and the\nbaseline. Then, we inquired their thoughts on how SPROUT\ncould help their actual workflow, the advantages and limi-\ntations of the current system design, as well as their sugges-\ntions for potential improvements.\n2 Tutorial Modification  through Tree Graph \nStrongly disagree1 Neutral3 Strongly agree55\n1 Content Generation from Code Snippets 4.67\n4.58\n4.33\n4.33\n4.50\n3 Context Switch across Branches\n5 Visualizations for Tutorial Understanding\n4 Detail Refinement in Node Space\n4 Agree22 Disagree\n(avg)\n5 7\n7\n6\n4\n3\n2\n8\n8\n1\n1 1\n2\n5\nFig. 7. User feedback regarding the features in SPROUT, measured on\na 5-point Likert-scale.\n7.2 Results Analysis\n7.2.1 Features\nWe evaluated five features of SPROUT through 5-point\nLikert-scale questionnaires in the post-study interview. Fig-\nure 7 illustrates the detailed ratings.\nParticipants chose different generation methods based\non their needs . They responded favorably to the feature\nof generating content based on selected code, finding it\nuseful when they “already have an initial framework in mind”\n(P6) for authoring content related to familiar code. The free\ngeneration was also praised since it could “provide diverse\nperspectives as a starting point when at lost” (P3) and allowed\nthem to “pick up a best one among different versions” (P1).\nParticipants utilized visual connections to quickly\ngrasp the tutorial content. The visual representations of\nconnections were favored by all participants, as it alleviated\nthe effort to “maintain the connections in mind” (P5). In\ntraditional chat-based interfaces, even if referred code is\nprovided, participants sometimes still needed to “scroll back\nto check the position of the code snippet within the complete code”\n(P11) and “the indentation of the code is missing” (P2).\nParticipants switched between thoughts smoothly and\nsensibly. According to some participants, utilizing the\nbranch view with thoughts is like “building text blocks under\nthe guidance of LLMs” (P3). Moreover, the quick assembling\nin Outline by selecting leaf nodes enables “swift comparison”\n(P9) between documents based on different thoughts.\nParticipants employed the tree graph to modify the\ntutorial. Most participants praised the graph-level modifi-\ncations such as splitting node, describing them as “novel and\nconvenient interactions within a document” (P7) that would be\n“frequently used” (P10). For example, P6 split the node which\ndescribed the main loop in Dijkstra algorithm, because he\nfound the content was too brief. Similarly, P8 utilized “Trim”\nto delete the content describing the engineering process,\nwhich “can be omitted in a tutorial” .\nParticipants achieved easy detail refinement in node\nspace. When focusing on specific content from LLMs, the\nnode space serves as “a good container which collects all the\ncontent for me” (P4). SPROUT also enables purpose-specific\ncontent refinement. For instance, P9 easily got a concise\nparagraph in SPROUT, whereas with ChatGPT, he had to\ntest prompts iteratively. However, P7 expressed dissatisfac-\ntion about this feature as his demands were not covered\ncurrently, such as introducing metaphors in the tutorial.\n7.2.2 Tutorial Authoring Support Analysis\nAt the end of the study, we asked participants to rate the two\nauthoring tools across seven dimensions (shown in fig. 8).\nTheir feedback demonstrated the system usability and its\neffectiveness in supporting tutorial creation process.\nSPROUT enables flexible tutorial generation for en-\nhanced engagement. Based on participants’ feedback, the\ngeneration methods in SPROUT allow them to conveniently\ntailor LLM-generated content to match their expectations\n(µ = 4.5 > 3.83, p = .021), offering different degrees of\nautonomy, while it is challenging with ChatGPT to main-\ntain such control. For example, P4 started the authoring\nprocess with the free generation as the “initial execution for\ninspirations”, paused the process, and then searched for “a\nreasonably detailed content from all branches” in the branch\nview to serve as a foundation for the tutorial. After spotting\na favorable structure, he proceeded to manually select the\ncode pieces for content generation, thereby ensuring the\nresulting output met his expectations.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nEasy to use\nBaseline\nOur System \nBaseline\nOur System \nBaseline\nOur System \nEasy to learn\nUse again\n0\n1\n5\n3\n2\n2\n4\n5\n3\n4\n4\n4\n6\n2\n51\n1\n1\n2\n6 126\navg=4.17\navg=3.42\navg=3.92\navg=4.08\navg=4.33\navg=3.83\n4\n5\n6\n2\nBaseline\nOur System Satisfactory about the \noverall quality of the \ngenerated tutorial\n2\n2\n7\n7\navg=4.08\navg=3.83 2\n3\n0 6 126\nDisagreeStrongly disagree1 Neutral3 Strongly agree54 Agree2\n1\nBaseline\nOur System \nBaseline\nOur System \nBaseline\nOur System \nBaseline\nOur System \nEffective in helping \ngenerate tutorial \ncontent\nEffective in helping \nunderstand tutorial \ncontent\nEffective in helping \nmodify tutorial\ncontent\nConfident in final \ntutorial content\n1\n1\n3\n3\n3\n6\n2\n4\n4\n4\n7\n3\n6\n8\n1\n1\n3\navg=4.50\navg=3.83\navg=4.17\navg=3.83\navg=4.42\navg=3.00\navg=4.25\navg=3.67\n4\n1\n1\n6\n7\n8\n6\n1\n2\nFig. 8. The results of the questionnaire regarding the authoring support\nof our system and the baseline.\nSPROUT provides easier modifications on tutorials\nhierarchically. Participants reported that the interactions\nprovided in SPROUT enabled easy tutorial modifications\nacross various aspects (µ = 4.42 > 3, p = .007), eliminating\nthe need for “repeatedly writing prompts” (P11). Some partic-\nipants concentrated on the narrative order of the tutorial,\nwhile others tended to manipulate the style or detail levels\nof certain paragraphs. Often, participants employed a com-\nbination of these methods. For example, P3 first decided\nthe tutorial structure according to LLM recommendations,\nthen grouped some nodes to shorten the tutorial, and finally\nrefined the text of an unsatisfactory paragraph. In contrast,\nwhen working with ChatGPT, participants had to interact\nwith it in multiple rounds, as the prompts were “ambiguous\nto convey their demands in mind” (P7).\nSPROUT enhances tutorial comprehension and relia-\nbility during the authoring process. Participants reflected\nthat they could produce tutorials with more comprehension\n(µ = 4.17 > 3.83, p = .157) and confidence ( µ = 4.25 >\n3.67, p = .07) on the outcomes throughout the workflow\nof SPROUT. During the generation process, the connections\nare updated synchronously, which “offers clear visual cues”\n(P1) about the content of each paragraph. P12 also pointed\nout that when utilizing ChatGPT to modify content, the\nrevised content often appeared separately from the original\ntutorial and code, leaving him uncertain about its correct-\nness. Moreover, we found that most participants would\nbrowse the entire tutorial to guarantee the key points were\ncovered. In SPROUT, the visual connections can support\nusers to “easily verify the coverage of the provided code” (P7),\nthus speeding up the examination process.\nSPROUT elevates the level of engagement and ease for\nauthors. Participants appreciated that SPROUT provided\na more user-friendly experience in the process of author-\ning tutorials comparing to ChatGPT ( µ = 4 .17 > 3.42,\np = .038), without significantly increasing the learning\neffort ( µ = 3 .92 < 4.08, p = .564). P2 said the novel\nvisual interface made him “more engaged” with the author-\ning work. Participants also conveyed a positive inclination\ntowards integrating SPROUT into their real work practices\n(µ = 4.33 > 3.83, p = .083).\nSPROUT produces user-satisfying tutorials of superior\nquality. Participants expressed their satisfaction with the\nquality of tutorials generated by SPROUT (µ = 4.08 > 3.83,\np = 0.317), appreciating aspects such as structure, clarity,\nand educational value. P5 suggested that our system has the\npotential to serve as an “interesting learning tool”, in addition\nto its capabilities as an authoring tool. Beyond the quality\nof the textual content, the visual design of the system also\ncontributes substantial educational value. As a teacher, P6\nvalued the ease with which he could customize tutorials to\ncater to learners of varying levels with minimal effort.\n7.2.3 Authoring Workflow Analysis\nWe analyzed the interaction logs from participants to in-\nvestigate the impact of SPROUT on the authoring work-\nflow. Our observations revealed diverse workflow patterns\nadopted by participants in completing their tutorials, along\nwith notable behaviors when interacting with SPROUT.\nWorkflow Patterns. Two predominant workflow pat-\nterns were observed among participants. The most preva-\nlent one was to utilize the automatic generation feature first\nto produce a complete tutorial, with a primary focus on\nthe tutorial view and chain view. Following the completion\nof the generation process, participants proceeded to modify\nthe tutorial using the Outline, Branches, and Node Space to\nsuit their specific requirements. The second pattern emerged\nwhen participants desired to take the initiative. In this\nsituation they used user-defined generation, modified the\ncontent then moved to the next step, where they switched\nattention between different views. Our system offered ro-\nbust support for both of the aforementioned workflows.\nObserved Behaviors. Participants sometimes tended to\nutilize the features of SPROUT with novel target according\nto their original authoring habits, which were not consid-\nered before. For instance, the demonstration of the reasons\nin branch view also assisted participants to verify the rea-\nsoning of tutorials upon completion and during the final\nverification process. In terms of tree graph, some experi-\nenced participants utilized it to compare different branches\nfor optimal structuring apart from understanding the think-\ning steps of LLM. The node space view also served as an\norganizational tool for participants to efficiently access their\ndesired content among multiple versions of paragraphs\nresulting from iterative content refinement.\n8 D ISCUSSION AND FUTURE WORK\nIn this section, we reflect on our research, mainly discussing\nthe implications, limitations, and future work.\n8.1 Implications\nIncorporating LLMs into programming tutorial author-\ning workflow. The development of large language models\nopens vast possibilities to facilitate the programming tuto-\nrial authoring process through reducing manual effort on\nwriting and editing. While our work provides a step-stone\ntowards programming tutorial authoring with the assistance\nof LLMs, future work could investigate on further enriching\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\ntutorials with more multifaceted content, such as execution\nexamples, flow charts, or figures illustrating code execution\nprocesses, which calls for utilizing multi-modal approaches\nand specific interactions to combine versatile media into\ncurrent workflow. Recent state-of-the-art research leverag-\ning in-context learning technologies has been successful\nin producing code-related content of remarkable quality\n[13], [59]. Our work, however, concentrates on enhancing\nthe interaction between humans and AI. Integrating these\ntwo approaches with advanced techniques, such as active\nlearning, has the potential to yield more user-centric and\nefficient tools [60]. We believe that our work can be a solid\nfoundation for future researches on authoring programming\ntutorials. By incorporating a wider array of materials, we\ncan ultimately create content with higher quality, granting\nreaders a richer understanding of the programming con-\ncepts presented.\nStep-by-step exploratory process offers a more con-\ntrolled authoring experience. Numerous studies have\ndelved into advancing strategies that prompt LLMs to think\nstep by step [12], [15], [37] for higher quality content.\nBeyond this, we see potential in using this decomposed\nprocess as an entry point to grant users broader access to\nthe intermediate steps of the authoring workflow. This em-\npowers them to achieve anticipated outcomes, transform-\ning a typically passive experience into an active one. The\nevaluation results indicated that the interactive, segmented\nrepresentations of the creation process and generated tu-\ntorial content in SPROUT offer more precise generation\nand allow for multi-level adjustments to modules. Such\nan approach addresses the often-encountered challenge of\naccurately conveying intentions in chat-based interfaces.\nMaintaining connections between provided inputs and\nLLM-generated outputs ensures transparency and trace-\nability. We identify and visualize latent connections be-\ntween the source code and the generated tutorial, as we\nbelieve it can assist users to comprehend and verify the\nreasoning behind LLMs’ creation process. Informed by our\nevaluation results, users utilized these presented connec-\ntions to examine the outcomes after generating or modifying\nprocedures, easing the burden of spending extra cognitive\nefforts to recognize and memorize them. Moreover, consis-\ntently maintaining these connections significantly improves\nuser confidence in the final results crafted collaboratively\nby users and LLMs. This is essential in materials-backed\nauthoring scenarios, where consistency and correctness are\nvital. Besides, our LLM-based methodology and visual de-\nsign strategies for establishing connection between cross-\nmodal data promise to be beneficial across a wide range of\nmulti-modal creation and data analysis scenarios [61]–[63].\nStriking a balance on the degree of interaction integrity\nin interface design. A key finding is that user preferences\nfor interacting with LLMs vary by individuals. For example,\ndepending on the specific operations they intend to per-\nform, some individuals preferred utilizing integrated inter-\nactions as a means to effortlessly obtain accurate outcomes,\nwhile others expressed reservations regarding the compre-\nhensiveness of these approaches in covering all possible\nscenarios. There exists a conflict between the extensiveness\nof features coverage and the precision of the results after\nexecution when designing interactions. Thus, our research\ncould inspire future studies to more systematically explore\nthe interface design paradigm for LLM-based applications,\nwhich could range from completely-free form to structured\nmodule-based interactions.\nApplying methodologies to diverse LLM-based cre-\nation scenarios. User feedback from our evaluation un-\nderscores the importance of both fidelity and innovative\ncapabilities of LLMs in authoring tasks. Designing and\ndeveloping systems or tools for creative scenarios requires\na harmonious mix of adaptability and innovation to ensure\ntheir use is seamless and user-friendly [64]. Although our\nstudy focuses on utilizing novel prompt strategies and in-\nteractive visualizations for programming tutorial authoring,\nthe methodologies employed herein can potentially be ex-\ntended to other creation scenarios that require consistency\nwith reference materials, such as fact-based analysis for\nlegal documents [65], medical applications [66], [67]. The\nunderlying idea involves designing purpose-specific and\nscenario-oriented prompting methods to enable appropriate\nexploratory workflows. The visualization of the generation\nprocess of LLMs can also be generalized to the majority\nof authoring tasks [6], [14], [16] to make the process more\ntracable. Furthermore, the value of a step-by-step rationale\nextends beyond authoring application, with potential ben-\nefits to other domains such as commonsense Q&A [68] or\ntext learning [69]. By breaking down complex or lengthy\ncontent into manageable segments, the learning process\nbecomes more palatable through this incremental method-\nology. This suggests the possibility of integrating the step-\nby-step methodology into applications aimed at enhancing\nthe learning workflow (e.g., mathematical concepts and\nlanguage learning), which can offer a more structured and\ndigestible approach to knowledge acquisition [70].\n8.2 Limitations and Future Work\nAlthough our work has shown promising results, there are\nseveral limitations and opportunities for future research.\nScalability. Although our system effectively supports\ntutorial authoring process in most cases, it may face lim-\nitations in adequately covering the context when dealing\nwith lengthy source code and tutorials, primarily due to\ntoken limitations. Nevertheless, these limitations can be\naddressed with advancements in LLM capabilities or by\nutilizing memory summarization techniques.\nPotentiality. While our study focuses on modifying con-\ntent within a single chain of thought, the current design\nframework can support more varied tutorial revision oper-\nations. For example, future work could explore facilitating\nusers in merging thoughts from two distinct paths, ensuring\ncontext preservation. This enhancement, rooted in our cur-\nrent prompting strategies, would also necessitate broaden-\ning the scope of graph manipulations to enable more flexible\ninteractions for users to customize the content.\nGeneralizability. Further exploration into multi-modal\napproaches can benefit tasks involving non-textual ele-\nments, like data visualization. Based on the nature of source\nmaterials and expected outcomes, visual representations\nbeyond tree graphs should be further explored to enhance\nthe comprehension and manipulation during the creation\nprocess. For example, future researches can investigate how\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\nto properly reify the creation process with LLMs when\nhandling non-linear structured data.\nLimitations. While the current prompting methods are\nsufficiently reliable and accurate for the authoring work-\nflow, the extraction accuracy can be further enhanced. Fu-\nture work could focus on utilizing correction prompts to\nexamine the extracted connections or integrating add-on\nmodules to identify connections to guarantee a higher accu-\nracy. Additionally, while insightful, our study is limited by\nits laboratory setting as it does not encompass assessments\nin real educational scenarios. Introducing student agents for\ntutorial assessment and iterative modification could poten-\ntially enhance its education value. Conducting a evaluation\nset in real education scenarios is also suggested for a more\nholistic understanding of the realistic learning impact.\n9 C ONCLUSION\nThis work presents SPROUT, a programming tutorial au-\nthoring tool with LLMs which breaks down the program-\nming tutorial authoring task into actionable steps and\nadopts novel prompting strategies to generate high-quality\nand diverse tutorial content. With a series of interactive\nvisualizations of the LLM generation process, users are\nable to effectively engage in an exploratory process to\ngenerate, modify, and understand the generated tutorial. A\nuser study with 12 participants demonstrates that users can\neffectively utilize SPROUT to author programming tutorials\nwith LLMs in a transparent and traceable process, leading\nto more controllable and reliable results.\nACKNOWLEDGMENTS\nWe would like to thank Jiehui Zhou and Minfeng Zhu\nfor their heartwarming support. We thank anonymous re-\nviewers for their insightful reviews. This paper is sup-\nported by the National Natural Science Foundation of China\n(62132017, 62302435), Zhejiang Provincial Natural Science\nFoundation of China (LD24F020011) and “Pioneer” and\n“Leading Goose” R&D Program of Zhejiang (2024C01167).\nREFERENCES\n[1] A. S. Kim and A. J. Ko, “A pedagogical analysis of online coding\ntutorials,” in Proc. ACM SIGCSE, 2017, pp. 321–326.\n[2] H. Jiang, J. Zhang, Z. Ren, and T. Zhang, “An unsupervised\napproach for discovering relevant tutorial fragments for apis,” in\nProc. IEEE/ACM ICSE, 2017, pp. 38–48.\n[3] S. Hamouda, S. H. Edwards, H. G. Elmongui, J. V . Ernst, and\nC. A. Shaffer, “Recurtutor: An interactive tutorial for learning\nrecursion,” ACM Trans. Comput. Educ., vol. 19, no. 1, pp. 1:1–1:25,\n2019.\n[4] A. Y. Wang, D. Wang, J. Drozdal, M. J. Muller, S. Park, J. D. Weisz,\nX. Liu, L. Wu, and C. Dugan, “Documentation matters: Human-\ncentered AI system to assist data science code documentation in\ncomputational notebooks,” ACM Trans. Comput. Hum. Interact. ,\nvol. 29, no. 2, pp. 17:1–17:33, 2022.\n[5] K. I. Gero, V . Liu, and L. Chilton, “Sparks: Inspiration for science\nwriting using language models,” in Proc. ACM DIS , 2022, pp.\n1002–1019.\n[6] N. Sultanum and A. Srinivasan, “Datatales: Investigating the use\nof large language models for authoring data-driven articles,” in\nProc. IEEE VIS, 2023, pp. 231–235.\n[7] S. Biswas, “Chatgpt and the future of medical writing,” p. e223312,\n2023.\n[8] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. Bang,\nA. Madotto, and P . Fung, “Survey of hallucination in natural\nlanguage generation,” ACM Comput. Surv. , vol. 55, no. 12, pp.\n248:1–248:38, 2023.\n[9] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . de Oliveira Pinto,\nJ. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman, A. Ray,\nR. Puri, G. Krueger, M. Petrov, H. Khlaaf, G. Sastry, P . Mishkin,\nB. Chan, S. Gray, N. Ryder, M. Pavlov, A. Power, L. Kaiser,\nM. Bavarian, C. Winter, P . Tillet, F. P . Such, D. Cummings,\nM. Plappert, F. Chantzis, E. Barnes, A. Herbert-Voss, W. H. Guss,\nA. Nichol, A. Paino, N. Tezak, J. Tang, I. Babuschkin, S. Balaji,\nS. Jain, W. Saunders, C. Hesse, A. N. Carr, J. Leike, J. Achiam,\nV . Misra, E. Morikawa, A. Radford, M. Knight, M. Brundage,\nM. Murati, K. Mayer, P . Welinder, B. McGrew, D. Amodei, S. Mc-\nCandlish, I. Sutskever, and W. Zaremba, “Evaluating large lan-\nguage models trained on code,” CoRR, vol. abs/2107.03374, 2021.\n[10] F. F. Xu, U. Alon, G. Neubig, and V . J. Hellendoorn, “A systematic\nevaluation of large language models of code,” in MAPS@PLDI,\n2022, pp. 1–10.\n[11] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P . Dhari-\nwal, A. Neelakantan, P . Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, “Language models are\nfew-shot learners,” in Proc. NeurIPS, 2020.\n[12] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H.\nChi, Q. V . Le, and D. Zhou, “Chain-of-thought prompting elicits\nreasoning in large language models,” in Proc. NeurIPS, 2022.\n[13] T. Ahmed, K. S. Pai, P . Devanbu, and E. T. Barr, “Improving few-\nshot prompts with relevant static analysis products,”arXiv preprint\narXiv:2304.06815, 2023.\n[14] A. Yuan, A. Coenen, E. Reif, and D. Ippolito, “Wordcraft: story\nwriting with large language models,” inProc. ACM IUI, New York,\nNY, USA, 2022, pp. 841–852.\n[15] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and\nK. Narasimhan, “Tree of thoughts: Deliberate problem solving\nwith large language models,” CoRR, vol. abs/2305.10601, 2023.\n[16] Z. Zhang, J. Gao, R. S. Dhaliwal, and T. J.-J. Li, “VISAR: A human-\nai argumentative writing assistant with visual programming and\nrapid draft prototyping,” in Proc. ACM UIST, 2023, pp. 5:1–5:30.\n[17] A. Head, J. Jiang, J. Smith, M. A. Hearst, and B. Hartmann,\n“Composing flexibly-organized step-by-step tutorials from linked\nsource code, snippets, and outputs,” in Proc. ACM CHI, 2020, pp.\n1–12.\n[18] R. Tiarks and W. Maalej, “How does a typical tutorial for mobile\ndevelopment look like?” in Proc. ACM MSR, 2014, pp. 272–281.\n[19] S. M. Nasehi, J. Sillito, F. Maurer, and C. Burns, “What makes a\ngood code example?: A study of programming q&a in stackover-\nflow,” in Proc. IEEE ICSM, 2012, pp. 25–34.\n[20] C. Kojouharov, A. Solodovnik, and G. Naumovich, “Jtutor: an\neclipse plug-in suite for creation and replay of code-based tuto-\nrials,” in Proc. ETX, 2004, pp. 27–31.\n[21] C. Oezbek and L. Prechelt, “Jtourbus: Simplifying program under-\nstanding by documentation that provides tours through the source\ncode,” in Proc. IEEE ICSM, 2007, pp. 64–73.\n[22] A. Mysore and P . J. Guo, “Torta: Generating mixed-media GUI and\ncommand-line app tutorials using operating-system-wide activity\ntracing,” in Proc. ACM UIST, 2017, pp. 703–714.\n[23] E. L. Ouh, B. K. S. Gan, and D. Lo, “ITSS: interactive web-based\nauthoring and playback integrated environment for programming\ntutorials,” in Proc. ICSE (SEET), 2022, pp. 158–164.\n[24] L. Bao, Z. Xing, X. Xia, and D. Lo, “Vt-revolution: Interactive\nprogramming video tutorial authoring and watching system,”\nIEEE Trans. Software Eng., vol. 45, no. 8, pp. 823–838, 2019.\n[25] S. Oney, C. Brooks, and P . Resnick, “Creating guided code expla-\nnations with chat.codes,” Proc. ACM Hum. Comput. Interact., vol. 2,\nno. CSCW, pp. 131:1–131:20, 2018.\n[26] S. Ginosar, L. F. D. Pombo, M. Agrawala, and B. Hartmann, “Au-\nthoring multi-stage code examples with editable code histories,”\nin Proc. ACM UIST, 2013, pp. 485–494.\n[27] A. Head, E. L. Glassman, B. Hartmann, and M. A. Hearst, “Inter-\nactive extraction of examples from existing code,” in Proc. ACM\nCHI, 2018, p. 85.\n[28] J. Leinonen, P . Denny, S. MacNeil, S. Sarsa, S. Bernstein, J. Kim,\nA. Tran, and A. Hellas, “Comparing code explanations created by\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\nstudents and large language models,” in Proc. ACM ITiCSE, New\nYork, NY, USA, 2023, pp. 124–130.\n[29] J. Y. Khan and G. Uddin, “Automatic code documentation gener-\nation using gpt-3,” in Proc. ACM ASE, 2022, pp. 1–6.\n[30] S. MacNeil, A. Tran, A. Hellas, J. Kim, S. Sarsa, P . Denny, S. Bern-\nstein, and J. Leinonen, “Experiences from using code explanations\ngenerated by large language models in a web software develop-\nment e-book,” in Proc. ACM SIGCSE, 2023, pp. 931–937.\n[31] S. MacNeil, A. Tran, J. Leinonen, P . Denny, J. Kim, A. Hellas,\nS. Bernstein, and S. Sarsa, “Automatically generating CS learning\nmaterials with large language models,” CoRR, 2022.\n[32] S. MacNeil, A. Tran, D. Mogil, S. Bernstein, E. Ross, and Z. Huang,\n“Generating diverse code explanations using the gpt-3 large lan-\nguage model,” in Proc. ACM ICER, New York, NY, USA, 2022, pp.\n37–39.\n[33] S. Sarsa, P . Denny, A. Hellas, and J. Leinonen, “Automatic genera-\ntion of programming exercises and code explanations using large\nlanguage models,” in Proc. ACM ICER, New York, NY, USA, 2022,\npp. 27–43.\n[34] Github, “Github copilot,” https://docs.github.com/en/copilot,\n2023.\n[35] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y. Cheng, C. Zhang,\nJ. Wang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran,\nL. Xiao, C. Wu, and J. Schmidhuber, “Metagpt: Meta program-\nming for a multi-agent collaborative framework,” arXiv preprint\narXiv:2308.00352, 2023.\n[36] C. Qian, X. Cong, W. Liu, C. Yang, W. Chen, Y. Su, Y. Dang,\nJ. Li, J. Xu, D. Li, Z. Liu, and M. Sun, “Communicative agents\nfor software development,” arXiv preprint arXiv:2307.07924, 2023.\n[37] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and\nY. Cao, “ReAct: Synergizing reasoning and acting in language\nmodels,” in Proc. ICLR. OpenReview.net, 2023.\n[38] S. Dhuliawala, M. Komeili, J. Xu, R. Raileanu, X. Li, A. Celikyil-\nmaz, and J. Weston, “Chain-of-verification reduces hallucination\nin large language models,” 2023.\n[39] R. Sennrich, J. Vamvas, and A. Mohammadshahi, “Mitigating\nhallucinations and off-target machine translation with source-\ncontrastive and language-contrastive decoding,” 2023.\n[40] R. Yen, J. Zhu, S. Suh, H. Xia, and J. Zhao, “Coladder: Supporting\nprogrammers with hierarchical code generation in multi-level\nabstraction,” arXiv preprint arXiv:2310.08699, 2023.\n[41] Y. Feng, X. Wang, K. K. Wong, S. Wang, Y. Lu, M. Zhu, B. Wang,\nand W. Chen, “Promptmagician: Interactive prompt engineering\nfor text-to-image creation,” IEEE Trans. Vis. Comput. Graph., 2023.\n[42] B. Wang, Y. Li, Z. Lv, H. Xia, Y. Xu, and R. Sodhi, “Lave: Llm-\npowered agent assistance and language augmentation for video\nediting,” in Proc. ACM IUI, 2024, pp. 699–714.\n[43] P . Jiang, J. Rayan, S. P . Dow, and H. Xia, “Graphologue: Exploring\nlarge language model responses with interactive diagrams,” in\nProc. ACM UIST, 2023, pp. 3:1–3:20.\n[44] S. Suh, M. Chen, B. Min, T. J.-J. Li, and H. Xia, “Structured\ngeneration and exploration of design space with large language\nmodels for human-ai co-creation,” arXiv preprint arXiv:2310.12953,\n2023.\n[45] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J.\nCai, “Promptchainer: Chaining large language model prompts\nthrough visual programming,” in Proc. ACM CHI, 2022, pp. 1–10.\n[46] T. Wu, M. Terry, and C. J. Cai, “Ai chains: Transparent and con-\ntrollable human-ai interaction by chaining large language model\nprompts,” in Proc. ACM CHI, 2022, pp. 1–22.\n[47] I. Arawjo, C. Swoopes, P . Vaithilingam, M. Wattenberg, and E. L.\nGlassman, “Chainforge: A visual toolkit for prompt engineering\nand LLM hypothesis testing,” in Proc. ACM CHI, 2024, pp. 304:1–\n304:18.\n[48] E. Jiang, K. Olson, E. Toh, A. Molina, A. Donsbach, M. Terry, and\nC. J. Cai, “Promptmaker: Prompt-based prototyping with large\nlanguage models,” in Proc. ACM CHI, 2022, pp. 35:1–35:8.\n[49] A. Mishra, U. Soni, A. Arunkumar, J. Huang, B. C. Kwon, and\nC. Bryan, “Promptaid: Prompt exploration, perturbation, testing\nand iteration using visual analytics for large language models,”\narXiv preprint arXiv:2304.01964, 2023.\n[50] D. Masson, S. Malacria, G. Casiez, and D. Vogel, “Directgpt:\nA direct manipulation interface to interact with large language\nmodels,” in Proc. ACM CHI, 2024, pp. 1–16.\n[51] S. Suh, B. Min, S. Palani, and H. Xia, “Sensecape: Enabling multi-\nlevel exploration and sensemaking with large language models,”\nin Proc. ACM UIST, 2023, pp. 1:1–1:18.\n[52] S. M. Goodman, E. Buehler, P . Clary, A. Coenen, A. Donsbach, T. N.\nHorne, M. Lahav, R. MacDonald, R. B. Michaels, A. Narayanan\net al. , “Lampost: Design and evaluation of an ai-assisted email\nwriting prototype for adults with dyslexia,” in Proc. ACM SIGAC-\nCESS, 2022, pp. 1–18.\n[53] S. Petridis, N. Diakopoulos, K. Crowston, M. Hansen, K. Hender-\nson, S. Jastrzebski, J. V . Nickerson, and L. B. Chilton, “Anglekin-\ndling: Supporting journalistic angle ideation with large language\nmodels,” in Proc. ACM CHI, 2023, pp. 1–16.\n[54] J. J. Y. Chung, W. Kim, K. M. Yoo, H. Lee, E. Adar, and M. Chang,\n“Talebrush: Sketching stories with generative pretrained language\nmodels,” in Proc. ACM CHI, 2022, pp. 1–19.\n[55] T. S. Kim, Y. Lee, M. Chang, and J. Kim, “Cells, generators,\nand lenses: Design framework for object-oriented interaction with\nlarge language models,” in Proc. ACM UIST, New York, NY, USA,\n2023.\n[56] OpenAI, “Introducing ChatGPT,”\nhttps://openai.com/blog/chatgpt, [Online; accessed 2023-04-30].\n[57] OpenAI, “Introducing text and code embeddings,”\nhttps://openai.com/blog/introducing-text-and-code-\nembeddings, [Online; accessed 2023-10-01].\n[58] L. McInnes, J. Healy, and J. Melville, “Umap: Uniform manifold\napproximation and projection for dimension reduction,” arXiv\npreprint arXiv:1802.03426, 2018.\n[59] M. Geng, S. Wang, D. Dong, H. Wang, G. Li, Z. Jin, X. Mao, and\nX. Liao, “Large language models are few-shot summarizers: Multi-\nintent comment generation via in-context learning,” in Proc. ICSE,\n2024, pp. 39:1–39:13.\n[60] Q. Li, Z. Yu, H. Xu, and B. Guo, “Human-machine interactive\nstreaming anomaly detection by online self-adaptive forest,” Fron-\ntiers Comput. Sci., vol. 17, no. 2, p. 172317, 2023.\n[61] Y. Feng, J. Chen, K. Huang, J. K. Wong, H. Ye, W. Zhang, R. Zhu,\nX. Luo, and W. Chen, “iPoet: interactive painting poetry creation\nwith visual multimodal analysis,” J. Vis., vol. 25, no. 3, pp. 671–\n685, Jun 2022.\n[62] Y. Feng, X. Wang, B. Pan, K. K. Wong, Y. Ren, S. Liu, Z. Yan, Y. Ma,\nH. Qu, and W. Chen, “XNLI: Explaining and diagnosing nli-based\nvisual data analysis,” IEEE Trans. Vis. Comput. Graph. , pp. 1–14,\n2023.\n[63] X. Wang, Z. Wu, W. Huang, Y. Wei, Z. Huang, M. Xu, and W. Chen,\n“VIS+AI: integrating visualization with artificial intelligence for\nefficient data analysis,” Frontiers Comput. Sci. , vol. 17, no. 6, p.\n176709, 2023.\n[64] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta,\nS. Yoo, and J. M. Zhang, “Large language models for software\nengineering: Survey and open problems,” 2023.\n[65] J. Cui, Z. Li, Y. Yan, B. Chen, and L. Yuan, “Chatlaw: Open-source\nlegal large language model with integrated external knowledge\nbases,” arXiv preprint arXiv:2306.16092, 2023.\n[66] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez,\nT. F. Tan, and D. S. W. Ting, “Large language models in medicine,”\nNature medicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[67] Y. Jin, F. Zhu, J. Li, and L. Ma, “Tcmfvis: A visual analytics\nsystem toward bridging together traditional chinese medicine and\nmodern medicine,” Vis. Informatics, vol. 7, no. 1, pp. 41–55, 2023.\n[68] F. Ocker, J. Deigm ¨oller, and J. Eggert, “Exploring large language\nmodels as a source of common-sense knowledge for robots,” arXiv\npreprint arXiv:2311.08412, 2023.\n[69] E. Kasneci, K. Seßler, S. K ¨uchemann, M. Bannert, D. Dementieva,\nF. Fischer, U. Gasser, G. Groh, S. G ¨unnemann, E. H ¨ullermeier\net al., “Chatgpt for good? on opportunities and challenges of large\nlanguage models for education,” Learning and individual differences,\nvol. 103, p. 102274, 2023.\n[70] M. Yu, Y. Wang, X. Yu, G. Shan, and Z. Jin, “Pubexplorer: An\ninteractive analytical system for visualizing publication data,” Vis.\nInformatics, vol. 7, no. 3, pp. 65–74, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\nYihan Liu received the B.E. degree in computer\nscience and technology from the Zhejiang Uni-\nversity, China, in 2022. She is currently work-\ning towards the master’s degree at the State\nKey Lab of CAD&CG, Zhejiang University. Her\nresearch interests include visual analytics, hu-\nman–computer interaction, and software engi-\nneering.\nZhen Wen is a Ph.D. candidate at the State Key\nLab of CAD & CG, College of Computer Sci-\nence and Technology, Zhejiang University. He re-\nceived the B.E. degree in Software Engineering\nfrom Zhejiang University of Technology in 2021.\nHis research interests include visual analytics,\nsoftware engineering, and quantum computing.\nLuoxuan Weng is currently a Ph.D. candidate\nin the College of Computer Science and Tech-\nnology, Zhejiang University, where he also re-\nceived his B.E. degree in Software Engineering\nin 2021. His research interests include visual an-\nalytics, human-centered computing, and natural\nlanguage processing.\nOllie Woodman is a Masters Student at the\nState Key Lab of CAD & CG, College of Com-\nputer Science and Technology, Zhejiang Univer-\nsity. He obtained a Bachelors of IT in Software\nDevelopment and a Bachelors of Arts in Chinese\nStudies at Monash University in 2022. His re-\nsearch interests include large language models,\nsoftware engineering and machine learning.\nYi Yang is currently a Ph.D. candidate in the\nCollege of Computer Science and Technology at\nthe Zhejiang University. She received the B.E.\ndegree in Computer Science and Technology\nfrom the China University of Mining and Tech-\nnology in 2023. Her research interests include\ncomputer vision and deep learning.\nWei Chen is a professor in the State Key Lab\nof CAD&CG at Zhejiang University. His current\nresearch interests include visualization and vi-\nsual analytics. He has published more than 80\nIEEE/ACM Transactions and IEEE VIS papers.\nHe actively served in many leading conferences\nand journals, like IEEE PacificVIS steering com-\nmittee, ChinaVIS steering committee, paper\ncochairs of IEEE VIS, IEEE PacificVIS, IEEE\nLDAV and ACM SIGGRAPH Asia VisSym. He is\nan associate editor of IEEE TVCG, IEEE TBG,\nACM TIST, IEEE T -SMC-S, IEEE TIV, IEEE CG&A, FCS, and JOV. More\ninformation can be found at: http://www.cad.zju.edu.cn/home/chenwei."
}