{
  "title": "Learning Differentially Private Recurrent Language Models",
  "url": "https://openalex.org/W2950527268",
  "year": 2017,
  "authors": [
    {
      "id": "https://openalex.org/A2388919843",
      "name": "H. Brendan McMahan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2087112254",
      "name": "Daniel Ramage",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2125003972",
      "name": "Kunal Talwar",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1973164540",
      "name": "Li Zhang",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2413533759",
    "https://openalex.org/W2138865266",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2119874464",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1985511977",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W1873763122",
    "https://openalex.org/W2148825261",
    "https://openalex.org/W2053637704",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W2027595342",
    "https://openalex.org/W2950602864",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W5855678",
    "https://openalex.org/W2077217970",
    "https://openalex.org/W2963999993",
    "https://openalex.org/W1992926795"
  ],
  "abstract": "We demonstrate that it is possible to train recurrent language models with user-level differential privacy guarantees with only a negligible cost in predictive accuracy. Our work builds on recent advances in the training of deep networks on user-partitioned data and privacy accounting for stochastic gradient descent. In particular, we add user-level privacy protection to the federated averaging algorithm, which makes large step updates from user-level data. Our work demonstrates that given a dataset with a sufficiently number of users (a requirement easily met by even small internet-scale datasets), achieving differential privacy comes at the cost of increased computation, rather than in decreased utility as in most prior work. We find that our private LSTM language models are quantitatively and qualitatively similar to un-noised models when trained on a dataset.",
  "full_text": "Published as a conference paper at ICLR 2018\nLEARNING DIFFERENTIALLY PRIVATE RECURRENT\nLANGUAGE MODELS\nH. Brendan McMahan\nmcmahan@google.com\nDaniel Ramage\ndramage@google.com\nKunal Talwar\nkunal@google.com\nLi Zhang\nliqzhang@google.com\nABSTRACT\nWe demonstrate that it is possible to train large recurrent language models with\nuser-level differential privacy guarantees with only a negligible cost in predictive\naccuracy. Our work builds on recent advances in the training of deep networks\non user-partitioned data and privacy accounting for stochastic gradient descent. In\nparticular, we add user-level privacy protection to the federated averaging algo-\nrithm, which makes “large step” updates from user-level data. Our work demon-\nstrates that given a dataset with a sufﬁciently large number of users (a requirement\neasily met by even small internet-scale datasets), achieving differential privacy\ncomes at the cost of increased computation, rather than in decreased utility as\nin most prior work. We ﬁnd that our private LSTM language models are quan-\ntitatively and qualitatively similar to un-noised models when trained on a large\ndataset.\n1 I NTRODUCTION\nDeep recurrent models like long short-term memory (LSTM) recurrent neural networks (RNNs)\nhave become a standard building block in modern approaches to language modeling, with applica-\ntions in speech recognition, input decoding for mobile keyboards, and language translation. Because\nlanguage usage varies widely by problem domain and dataset, training a language model on data\nfrom the right distribution is critical. For example, a model to aid typing on a mobile keyboard is\nbetter served by training data typed in mobile apps rather than from scanned books or transcribed\nutterances. However, language data can be uniquely privacy sensitive. In the case of text typed\non a mobile phone, this sensitive information might include passwords, text messages, and search\nqueries. In general, language data may identify a speaker—explicitly by name or implicitly, for\nexample via a rare or unique phrase—and link that speaker to secret or sensitive information.\nIdeally, a language model’s parameters would encode patterns of language use common to many\nusers without memorizing any individual user’s unique input sequences. However, we know convo-\nlutional NNs can memorize arbitrary labelings of the training data (Zhang et al., 2017) and recurrent\nlanguage models are also capable of memorizing unique patterns in the training data (Carlini et al.,\n2018). Recent attacks on neural networks such as those of Shokri et al. (2017) underscore the im-\nplicit risk. The main goal of our work is to provide a strong guarantee that the trained model protects\nthe privacy of individuals’ data without undue sacriﬁce in model quality.\nWe are motivated by the problem of training models for next-word prediction in a mobile keyboard,\nand use this as a running example. This problem is well suited to the techniques we introduce, as\ndifferential privacy may allow for training on data from the true distribution (actual mobile usage)\nrather than on proxy data from some other source that would produce inferior models. However,\nto facilitate reproducibility and comparison to non-private models, our experiments are conducted\non a public dataset as is standard in differential privacy research. The remainder of this paper is\nstructured around the following contributions:\n1. We apply differential privacy to model training using the notion ofuser-adjacent datasets, leading\nto formal guarantees of user-level privacy, rather than privacy for single examples.\n1\narXiv:1710.06963v3  [cs.LG]  24 Feb 2018\nPublished as a conference paper at ICLR 2018\n2. We introduce a noised version of the federated averaging algorithm (McMahan et al., 2016)\nin §2, which satisﬁes user-adjacent differential privacy via use of the moments accountant (Abadi\net al., 2016a) ﬁrst developed to analyze differentially private stochastic gradient descent (SGD) for\nexample-level privacy. The federated averaging approach groups multiple SGD updates together,\nenabling large-step model updates.\n3. We demonstrate the ﬁrst high quality LSTM language model trained with strong privacy guar-\nantees in §3, showing no signiﬁcant decrease in model accuracy given a large enough dataset. For\nexample, on a dataset of 763,430 users, baseline (non-private) training achieves an accuracy of\n17.5% in 4120 rounds of training, where we use the data from 100 random users on each round. We\nachieve this same level of accuracy with(4.6,10−9)-differential privacy in 4980 rounds, processing\non average 5000 users per round—maintaining the same level of accuracy at a signiﬁcant compu-\ntational cost of roughly 60×.1 Running the same computation on a larger dataset with 108 users\nwould improve the privacy guarantee to (1.2,10−9). We guarantee privacy and maintain utility de-\nspite the complex internal structure of the LSTM—with per-word embeddings as well as dense state\ntransitions—by using the federated averaging algorithm. We demonstrate that the noised model’s\nmetrics and qualitative behavior (with respect to head words) does not differ signiﬁcantly from the\nnon-private model. To our knowledge, our work represents the most sophisticated machine learning\nmodel, judged by the size and the complexity of the model, ever trained with privacy guarantees,\nand the ﬁrst such model trained with user-level privacy.\n4. In extensive experiments in §3, we offer guidelines for parameter tuning when training complex\nmodels with differential privacy guarantees. We show that a small number of experiments can\nnarrow the parameter space into a regime where we pay for privacy not in terms of a loss in utility\nbut in terms of an increased computational cost.\nWe now introduce a few preliminaries. Differential privacy (DP) (Dwork et al., 2006; Dwork, 2011;\nDwork and Roth, 2014) provides a well-tested formalization for the release of information derived\nfrom private data. Applied to machine learning, a differentially private training mechanism allows\nthe public release of model parameters with a strong guarantee: adversaries are severely limited in\nwhat they can learn about the original training data based on analyzing the parameters, even when\nthey have access to arbitrary side information. Formally, it says:\nDeﬁnition 1. Differential Privacy: A randomized mechanism M: D→R with a domain D(e.g.,\npossible training datasets) and rangeR(e.g., all possible trained models) satisﬁes(ϵ,δ)-differential\nprivacy if for any two adjacent datasets d,d′∈D and for any subset of outputs S ⊆R it holds that\nPr[M(d) ∈S] ≤eϵPr[M(d′) ∈S] + δ.\nThe deﬁnition above leaves open the deﬁnition of adjacent datasets which will depend on the\napplication. Most prior work on differentially private machine learning (e.g. Chaudhuri et al. (2011);\nBassily et al. (2014); Abadi et al. (2016a); Wu et al. (2017); Papernot et al. (2017)) deals with\nexample-level privacy: two datasets dand d′are deﬁned to be adjacent ifd′can be formed by adding\nor removing a single training example from d. We remark that while the recent PATE approach of\n(Papernot et al., 2017) can be adapted to give user-level privacy, it is not suited for a language model\nwhere the number of classes (possible output words) is large.\nFor problems like language modeling, protecting individual examples is insufﬁcient—each typed\nword makes its own contribution to the RNN’s training objective, so one user may contribute many\nthousands of examples to the training data. A sensitive word or phrase may be typed several times\nby an individual user, but it should still be protected.2 In this work, we therefore apply the deﬁnition\nof differential privacy to protect whole user histories in the training set. This user-level privacy is\nensured by using an appropriate adjacency relation:\nDeﬁnition 2. User-adjacent datasets: Let dand d′be two datasets of training examples, where each\nexample is associated with a user. Then, dand d′are adjacent if d′can be formed by adding or\nremoving all of the examples associated with a single user from d.\n1The additional computational cost could be mitigated by initializing by training on a public dataset, rather\nthan starting from random initialization as we do in our experiments.\n2Differential privacy satisﬁes a property known as group privacy that can allow translation from example-\nlevel privacy to user-level privacy at the cost of an increasedϵ. In our setting, such a blackbox approach would\nincur a prohibitive privacy cost. This forces us to directly address user-level privacy.\n2\nPublished as a conference paper at ICLR 2018\nModel training that satisﬁes differential privacy with respect to datasets that are user-adjacent satis-\nﬁes the intuitive notion of privacy we aim to protect for language modeling: the presence or absence\nof any speciﬁc user’s data in the training set has an imperceptible impact on the (distribution over)\nthe parameters of the learned model. It follows that an adversary looking at the trained model can-\nnot infer whether any speciﬁc user’s data was used in the training, irrespective of what auxiliary\ninformation they may have. In particular, differential privacy rules out memorization of sensitive\ninformation in a strong information theoretic sense.\n2 A LGORITHMS FOR USER -LEVEL DIFFERENTIALLY PRIVATE TRAINING\nOur private algorithm relies heavily on two prior works: the FederatedAveraging (or\nFedAvg) algorithm of McMahan et al. (2016), which trains deep networks on user-partitioned data,\nand the moments accountant of Abadi et al. (2016a), which provides tight composition guarantees\nfor the repeated application of the Gaussian mechanism combined with ampliﬁcation-via-sampling.\nWhile we have attempted to make the current work as self-contained as possible, the above refer-\nences provide useful background.\nFedAvg was introduced by McMahan et al. (2016) for federated learning, where the goal is to train\na shared model while leaving the training data on each user’s mobile device. Instead, devices down-\nload the current model and compute an update by performing local computation on their dataset. It\nis worthwhile to perform extra computation on each user’s data to minimize the number of commu-\nnication rounds required to train a model, due to the signiﬁcantly limited bandwidth when training\ndata remains decentralized on mobile devices. We observe, however, that FedAvg is of interest\neven in the datacenter when DP is applied: larger updates are more resistant to noise, and fewer\nrounds of training can imply less privacy cost. Most importantly, the algorithm naturally forms per-\nuser updates based on a single user’s data, and these updates are then averaged to compute the ﬁnal\nupdate applied to the shared model on each round. As we will see, this structure makes it possible\nto extend the algorithm to provide a user-level differential privacy guarantee.\nWe also evaluate the FederatedSGD algorithm, essentially large-batch SGD where each mini-\nbatch is composed of “microbatches” that include data from a single distinct user. In some datacen-\nter applications FedSGD might be preferable toFedAvg, since fast networks make it more practical\nto run more iterations. However, those additional iterations come at a privacy cost. Further, the pri-\nvacy beneﬁts of federated learning are nicely complementary to those of differential privacy, and\nFedAvg can be applied in the datacenter as well, so we focus on this algorithm while showing that\nour results also extend to FedSGD.\nBoth FedAvg and FedSGD are iterative procedures, and in both cases we make the following\nmodiﬁcations to the non-private versions in order to achieve differential privacy:\nA) We use random-sized batches where we select users independently with probability q,\nrather than always selecting a ﬁxed number of users.\nB) We enforce clipping of per-user updates so the total update has bounded L2 norm.\nC) We use different estimators for the average update (introduced next).\nD) We add Gaussian noise to the ﬁnal average update.\nThe pseudocode for DP-FedAvg and DP-FedSGD is given as Algorithm 1. In the remainder of\nthis section, we introduce estimators for C) and then different clipping strategies for B). Adding the\nsampling procedure from A) and noise added in D) allows us to apply the moments accountant to\nbound the total privacy loss of the algorithm, given in Theorem 1. Finally, we consider the properties\nof the moments accountant that make training on large datasets particular attractive.\nBounded-sensitivity estimators for weighted average queries Randomly sampling users (or\ntraining examples) by selecting each independently with probability q is crucial for proving low\nprivacy loss through the use of the moments accountant (Abadi et al., 2016a). However, this proce-\ndure produces variable-sized samples C, and when the quantity to be estimated f(C) is an average\nrather than a sum (as in computing the weighted average update inFedAvg or the average loss on a\nminibatch in SGD with example-level DP), this has ramiﬁcations for the sensitivity of the query f.\nSpeciﬁcally, we consider weighted databases dwhere each row k∈dis associated with a particular\nuser, and has an associated weight wk ∈[0,1]. This weight captures the desired inﬂuence of the\n3\nPublished as a conference paper at ICLR 2018\nMain training loop:\nparameters\nuser selection probability q∈(0,1]\nper-user example cap ˆw∈R+\nnoise scale z∈R+\nestimator ˜ff, or ˜fc with param Wmin\nUserUpdate (for FedAvg or FedSGD)\nClipFn (FlatClip or PerLayerClip)\nInitialize model θ0, moments accountant M\nwk = min\n(nk\nˆw ,1\n)\nfor all users k\nW = ∑\nk∈d wk\nfor each round t= 0,1,2,... do\nCt ←(sample users with probability q)\nfor each user k∈Ct in parallel do\n∆t+1\nk ←UserUpdate(k,θt,ClipFn)\n∆t+1 =\n\n\n\n∑\nk∈Ct wk∆k\nqW for ˜ff\n∑\nk∈Ct wk∆k\nmax(qWmin,∑\nk∈Ct wk) for ˜fc\nS ←(bound on ∥∆k∥for ClipFn)\nσ←\n{\nzS\nqW for ˜ff or 2zS\nqWmin\nfor ˜fc\n}\nθt+1 ←θt + ∆t+1 + N(0,Iσ2)\nM.accum priv spending(z)\nprint M.get privacy spent()\nFlatClip(∆):\nparameter S\nreturn π(∆,S) // See Eq. (1).\nPerLayerClip(∆):\nparameters S1,...S m\nS =\n√∑\nj S2\nj\nfor each layer j ∈{1,...,m }do\n∆′(j) = π(∆(j),Sj)\nreturn ∆′\nUserUpdateFedAvg(k,θ0 , ClipFn):\nparameters B, E, η\nθ←θ0\nfor each local epoch ifrom 1 to Edo\nB← (k’s data split into sizeBbatches)\nfor batch b∈B do\nθ←θ−η▽ℓ(θ; b)\nθ←θ0 + ClipFn(θ−θ0)\nreturn update ∆k = θ−θ0 // Already clipped.\nUserUpdateFedSGD(k,θ0 , ClipFn):\nparameters B, η\nselect a batch bof size Bfrom k’s examples\nreturn update ∆k = ClipFn(−η▽ℓ(θ; b))\nAlgorithm 1: The main loop for DP-FedAvg and DP-FedSGD, the only difference being in the\nuser update function (UserUpdateFedAvg or UserUpdateFedSGD). The calls on the moments ac-\ncountant Mrefer to the API of Abadi et al. (2016b).\nrow on the ﬁnal outcome. For example, we might think of row k containing nk different training\nexamples all generated by user k, with weight wk proportional to nk. We are then interested in a\nbounded-sensitivity estimate off(C) =\n∑\nk∈C wk∆k∑\nk∈C wk\nfor per-user vectors∆k, for example to estimate\nthe weighted-average user update in FedAvg. Let W = ∑\nkwk. We consider two such estimators:\n˜ff(C) =\n∑\nk∈Cwk∆k\nqW , and ˜fc(C) =\n∑\nk∈Cwk∆k\nmax(qWmin,∑\nk∈Cwk).\nNote ˜ff is an unbiased estimator, sinceE[∑\nk∈Cwk] = qW. On the other hand, ˜fc matches fexactly\nas long as we have sufﬁcient weight in the sample. For privacy protection, we need to control the\nsensitivity of our query function ˜f, deﬁned as S( ˜f) = max C,k∥˜f(C∪{ k}) −˜f(C)∥2, where the\nadded user kcan have arbitrary data. The lower-bound qWmin on the denominator of ˜fc is necessary\nto control sensitivity. Assuming each wk∆k has bounded norm, we have:\nLemma 1. If for all users k we have ∥wk∆k∥2 ≤S, then the sensitivity of the two estimators is\nbounded as S( ˜ff) ≤ S\nqW and S( ˜fc) ≤ 2S\nqWmin\n.\nA proof is given in Appendix §A.\nClipping strategies for multi-layer models Unfortunately, when the user vectors ∆k are gradi-\nents (or sums of gradients) from a neural network, we will generally have no a priori bound3 Ssuch\nthat ∥∆k∥≤ S. Thus, we will need to “clip” our updates to enforce such a bound before applying\n˜ff or ˜fc. For a single vector ∆, we can apply a simple L2 projection when necessary:\nπ(∆,S)\ndef\n= ∆ ·min\n(\n1, S\n∥∆∥\n)\n. (1)\n3To control sensitivity, Lemma 1 only requires that ∥wk∆k∥is bounded. For simplicity, we only apply\nclipping to the updates ∆k, using the fact wk ≤1, leaving as future work the investigation of weight-aware\nclipping schemes.\n4\nPublished as a conference paper at ICLR 2018\nTable 1: Privacy for different total numbers of users K (all with equal weight), expected number of\nusers sampled per round ˜C, and the number of rounds of training. For each row, we set δ = 1\nK1.1\nand report the value of ϵfor which (ϵ,δ)-differential privacy holds after 1 to 106 rounds. For large\ndatasets, additional rounds of training incur only a minimal additional privacy loss.\nusers sample noise Upper bound on privacy ϵafter 1,10,... 106 rounds\nK ˜C z 100 101 102 103 104 105 106\n105 102 1.0 0.97 0.98 1.00 1.07 1.18 2.21 7.50\n106 101 1.0 0.68 0.69 0.69 0.69 0.69 0.72 0.73\n106 103 1.0 1.17 1.17 1.20 1.28 1.39 2.44 8.13\n106 104 1.0 1.73 1.92 2.08 3.06 8.49 32.38 187.01\n106 103 3.0 0.47 0.47 0.48 0.48 0.49 0.67 1.95\n109 103 1.0 0.84 0.84 0.84 0.85 0.88 0.88 0.88\nHowever, for deep networks it is more natural to treat the parameters of each layer as a separate\nvector. The updates to each of these layers could have vastly different L2 norms, and so it can be\npreferable to clip each layer separately.\nFormally, suppose each update ∆k contains mvectors ∆k = (∆k(1),..., ∆k(m)). We consider\nthe following clipping strategies, both of which ensure the total update has norm at most S:\n1. Flat clipping Given an overall clipping parameter S, we clip the concatenation of all the\nlayers as ∆′\nk = π(∆k,S).\n2. Per-layer clipping Given a per-layer clipping parameterSj for each layer, we set∆′\nk(j) =\nπ(∆k(j),Sj). Let S =\n√∑m\nj=1 S2\nj. The simplest model-independent choice is to take\nSj = S√m for all j, which we use in experiments.\nWe remark here that clipping itself leads to additional bias, and ideally, we would choose the clipping\nparameter to be large enough that nearly all updates are smaller than the clip value. On the other\nhand, a larger Swill require more noise in order to achieve privacy, potentially slowing training. We\ntreat Sas a hyper-parameter and tune it.\nA privacy guarantee Once the sensitivity of the chosen estimator is bounded, we may add Gaus-\nsian noise scaled to this sensitivity to obtain a privacy guarantee. A simple approach is to use an\n(ϵ,δ)-DP bound for this Gaussian mechanism, and apply the privacy ampliﬁcation lemma and the\nadvanced composition theorem to get a bound on the total privacy cost. We instead use the Mo-\nments Accountant of Abadi et al. (2016a) to achieve much tighter privacy bounds. The moments\naccountant for the sampled Gaussian mechanism upper bounds the total privacy cost of T steps of\nthe Gaussian mechanism with noise N(0,σ2) for σ = z·S, where zis a parameter, S is the sensi-\ntivity of the query, and each row is selected with probability q. Given a δ >0, the accountant gives\nan ϵfor which this mechanism satisﬁes (ϵ,δ)-DP. The following theorem is a slight generalization\nof the results in Abadi et al. (2016a); see §A for a proof sketch.\nTheorem 1. For the estimator (˜ff, ˜fc), the moments accountant of the sampled Gaussian mechanism\ncorrectly computes the privacy loss with the noise scale of z= σ/S and steps T, where S = S/qW\nfor ( ˜ff) and 2S/qWmin for ( ˜fc).\nDifferential privacy for large datasets We use the implementation of the moments accountant\nfrom Abadi et al. (2016b). The moments accountant makes strong use of ampliﬁcation via sampling,\nwhich means increasing dataset size makes achieving high levels of privacy signiﬁcantly easier. Ta-\nble 1 summarizes the privacy guarantees offered as we vary some of the key parameters. The take-\naway from this table is that as long as we can afford the cost in utility of adding noise proportional\nto ztimes the sensitivity of the updates, we can get reasonable privacy guarantees over a large range\nof parameters. The size of the dataset has a modest impact on the privacy cost of a single query\n(1 round column), but a large effect on the number of queries that can be run without signiﬁcantly\nincreasing the privacy cost (compare the 106 round column). For example, on a dataset with 109\n5\nPublished as a conference paper at ICLR 2018\n0 1000 2000 3000 4000 5000\ncommunication rounds\n0.11\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\n0.18AccuracyTop1\nAccuracy of noised models vs baseline\nbaseline\n¾ = 0.012, S=20\n¾ = 0.006, S=10\n¾ = 0.003, S=15\n¾ = 0.006, S=15\n¾ = 0.012, S=15\n¾ = 0.024, S=15\nFigure 1: Noised training versus the non-private\nbaseline. The model with σ = 0 .003 nearly\nmatches the baseline.\nTable 2: Privacy (ϵat δ=10−9) and accuracy af-\nter 5000 rounds of training for models with dif-\nferent σ and S from Figure 1. The ϵ’s are strict\nupper bounds on the true privacy loss given the\ndataset size K and ˜C; AccuracyTop1 (AccT1) is\nestimated from a model trained with the same σ\nas discussed in the text.\nmodel data\nσ S users K ˜C ϵ AccT1\n0.000 ∞ 763430 100 ∞ 17.62%\n0.003 15 763430 5000 4.634 17.49%\n0.006 10 763430 1667 2.314 17.04%\n0.012 15 763430 1250 2.038 16.33%\n0.003 15 108 5000 1.152 17.49%\n0.006 10 108 1667 0.991 17.04%\n0.012 15 108 1250 0.987 16.33%\nusers, the privacy upper bound is nearly constant between 1 and 106 calls to the mechanism (that is,\nrounds of the optimization algorithm).\nThere is only a small cost in privacy for increasing the expected number of (equally weighted) users\n˜C = qW = qK selected on each round as long as ˜C remains a small fraction of the size of the\ntotal dataset. Since the sensitivity of an average query decreases like 1/˜C(and hence the amount of\nnoise we need to add decreases proportionally), we can increase ˜C until we arrive at a noise level\nthat does not adversely effect the optimization process. We show empirically that such a level exists\nin the experiments.\n3 E XPERIMENTAL RESULTS\nIn this section, we evaluateDP-FedAvg while training an LSTM RNN tuned for language modeling\nin a mobile keyboard. We vary noise, clipping, and the number of users per round to develop an\nintuition of how privacy affects model quality in practice.\nWe defer our experimental results on FedSGD as well as on models with larger dictionaries to\nAppendix §D. To summarize, they show that FedAvg gives better privacy-utility trade-offs than\nFedSGD, and that our empirical conclusions extend to larger dictionaries with relatively little need\nfor additional parameter tuning despite the signiﬁcantly larger models. Some less important plots\nare deferred to §C.\nModel structure The goal of a language model is to predict the next word in a sequence st from\nthe preceding words s0...st−1. The neural language model architecture used here is a variant of\nthe LSTM recurrent neural network (Hochreiter and Schmidhuber, 1997) trained to predict the next\nword (from a ﬁxed dictionary) given the current word and a state vector passed from the previous\ntime step. LSTM language models are competitive with traditional n-gram models (Sundermeyer\net al., 2012) and are a standard baseline for a variety of ever more advanced neural language model\narchitectures (Grave et al., 2016; Merity et al., 2016; Gal and Ghahramani, 2016). Our model uses\na few tricks to decrease the size for deployment on mobile devices (total size is 1.35M parameters),\nbut is otherwise standard. We evaluate using AccuracyTop1, the probability that the word to\nwhich the model assigns highest probability is correct . Details on the model and evaluation metrics\nare given in §B. All training began from a common random initialization, though for real-world\napplications pre-training on public data is likely preferable (see §B for additional discussion).\nDataset We use a large public dataset of Reddit posts, as described by Al-Rfou et al. (2016).\nCritically for our purposes, each post in the database is keyed by an author, so we can group the data\nby these keys in order to provide user-level privacy. We preprocessed the dataset to K = 763,430\nusers each with 1600 tokens. Thus, we take wk = 1 for all users, so W = K. We write ˜C =\n6\nPublished as a conference paper at ICLR 2018\n0.31 1.25 5.00 20.00\nS, total bound on L2 norm of updates\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18AccuracyTop1\nEffect of clipping updates\n3000 rounds, flat clip\n3000 rounds, per-layer\n500 rounds, flat clip\n500 rounds, per-layer\n100 rounds, flat clip\n100 rounds, per-layer\nFigure 2: The effect of update clipping on the\nconvergence of FedAvg, after 100, 500, and\n3000 rounds of training.\n0.0015 0.0060 0.0240 0.0960\n¾, noise std dev\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18AccuracyTop1\nEffect of noising updates\n3000 rounds, flat clip\n3000 rounds, per-layer clip\n500 rounds, flat clip\n500 rounds, per-layer clip\n100 rounds, flat clip\n100 rounds, per-layer clip\nFigure 3: The effect of different levels of noise\nσfor ﬂat and per-layer clipping at S = 20. The\nvertical dashed red line is σ= 0.2.\nqK = qW for the expected number of users sampled per round. See §B for details on the dataset\nand preprocessing. To allow for frequent evaluation, we use a relatively small test set of 75122\ntokens formed from random held-out posts. We evaluate accuracy every 20 rounds and plot metrics\nsmoothed over 5 evaluations (100 rounds).\nBuilding towards DP: sampling, estimators, clipping, and noise Recall achieving differential\nprivacy for FedAvg required a number of changes (§2, items A–D). In this section, we examine the\nimpact of each of these changes, both to understand the immediate effects and to enable the selection\nof reasonable parameters for our ﬁnal DP experiments. This sequence of experiments also provides\na general road-map for applying differentially private training to new models and datasets. For these\nexperiments, we use the FedAvg algorithm with a ﬁxed learning rate of 6.0, which we veriﬁed was\na reasonable choice in preliminary experiments.4 In all FedAvg experiments, we used a local batch\nsize of B = 8 , an unroll size of 10 tokens, and made E = 1 passes over the local dataset; thus\nFedAvg processes 80 tokens per batch, processing a user’s 1600 tokens in 20 batches per round.\nFirst, we investigate the impact of changing the estimator used for the average per-round update,\nas well as replacing a ﬁxed sample of C = 100 users per round to a variable-sized sample formed\nby selecting each user with probability q = 100 /763430 for an expectation of ˜C = 100 users.\nNone of these changes signiﬁcantly impacted the convergence rate of the algorithm (see Figure 5\nin §C). In particular, the ﬁxed denominator estimator ˜ff works just as well as the higher-sensitivity\nclipped-denominator estimator ˜fc. Thus, in the remaining experiments we focus on estimator ˜ff.\nNext, we investigate the impact of ﬂat and per-layer clipping on the convergence rate of FedAvg.\nThe model has 11 parameter vectors, and for per-layer clipping we simply chose to distribute the\nclipping budget equally across layers with Sj = S/\n√\n11. Figure 2 shows that choosing S ∈[10,20]\nhas at most a small effect on convergence rate.\nFinally, Figure 3 shows the impact of various levels of per-coordinate Gaussian noise N(0,σ2)\nadded to the average update. Early in training, we see almost no loss in convergence for a noise of\nσ = 0.024; later in training noise has a larger effect, and we see a small decrease in convergence\npast σ= 0.012. These experiments, where we sample only an expected 100 users per round, are not\nsufﬁcient to provide a meaningful privacy guarantee. We have S = 20.0 and ˜C = qW = 100, so\nthe sensitivity of estimator ˜ff is 20/100.0 = 0.2. Thus, to use the moments accountant with z = 1,\nwe would need to add noise σ= 0.2 (dashed red vertical line), which destroys accuracy.\nEstimating the accuracy of private models for large datasets Continuing the above example,\nif instead we choose q so ˜C = 1250, set the L2 norm bound S = 15.0, then we have sensitivity\n4The proper choice of for the clipping parameters may depend on the learning rate, so if the learning rate is\nchanged, clipping parameter choices will also need to be re-evaluated.\n7\nPublished as a conference paper at ICLR 2018\n¾=0.001\nS=1.67\n¾=0.005\nS=8.33\n¾=0.01\nS=16.67\n0.156\n0.158\n0.160\n0.162\n0.164\n0.166\n0.168\n0.170\n0.172AccuracyTop1\nEffect of different noise vs. clipping tradeoffs\n4885 + 6000 rounds\n4885 + 1000 rounds\n2000 rounds\nFigure 4: Different noise/clipping tradeoffs (all\nof equal privacy cost), for initial training (red)\nand adjusted after 4885 rounds (green and blue).\nSolid lines use ﬂat clipping, dashed are per-layer.\nClients Accuracy\nTop1 Noise Corpus top n\nn=10n=50n= 100\n100 0.162 0.0120\n1250 0.156 0.0120\n1250 0.076 0.0120\n1250 0.115 0.0120\n1250 0.156 0.0120\n100 0.151 0.0000\n100 0.151 0.0004\n100 0.150 0.0015\n100 0.151 0.0060\n100 0.142 0.0240\n100 0.086 0.0960\nTable 3: Count histograms recording how many\nof a model’s (row’s) top 10 predictions are found\nin the n = 10 , 50, or 100 most frequent words\nin the corpus. Models that predict corpus top- n\nmore frequently have more mass to the right.\n15/1250 = 0.012, and so we add noise σ = 0.012 and can apply the moments account with noise\nscale z = 1. The computation is now signiﬁcantly more computationally expensive, but will give a\nguarantee of (1.97,10−9)-differential privacy after 3000 rounds of training. Because running such\nexperiments is so computationally expensive, for experimental purposes it is useful to ask: does\nusing an expected 1250 users per round produce a model with different accuracy than a model\ntrained with only 100 expected users per round? If the answer is no, we can train a model with\n˜C = 100 and a particular noise level σ, and use that model to estimate the utility of a model trained\nwith a much larger q (and hence a much better privacy guarantee). We can then run the moments\naccountant (without actually training) to numerically upper bound the privacy loss. To test this, we\ntrained two models, both with S = 15 and σ = 0.012, one with ˜C = 100 and one with ˜C = 1250;\nrecall the ﬁrst model achieves a vacuous privacy guarantee, while the second achieves(1.97,10−9)-\ndifferential privacy after 3000 rounds. Figure 7 in§C shows the two models produce almost identical\naccuracy curves during training. Using this observation, we can use the accuracy of models trained\nwith ˜C = 100 to estimate the utility of private models trained with much larger˜C. See also Figure 6\nin §C, which also shows diminishing returns for larger Cfor the standard FedAvg algorithm.\nFigure 1 compares the true-average ﬁxed-sample baseline model (see Figure 5 in §C) with models\nthat use varying levels of clipping S and noise σ at ˜C = 100. Using the above approach, we can\nuse these experiments to estimate the utility of LSTMs trained with differential privacy for different\nsized datasets and different values of ˜C. Table 2 shows representative values setting ˜C so that\nz = 1. For example, the model with σ = 0.003 and S = 15 is only worse than the baseline by an\nadditive −0.13% in AccuracyTop1 and achieves (4.6,10−9)-differential privacy when trained with\n˜C = 5000 expected users per round. As a point of comparison, we have observed that training on a\ndifferent corpus can cost an additive −2.50% in AccuracyTop1.5\nAdjusting noise and clipping as training progresses Figure 1 shows that as training progresses,\neach level of noise eventually becomes detrimental (the line drops somewhat below the baseline).\nThis suggests using a smaller σ and correspondingly smaller S (thus ﬁxing z so the privacy cost\nof each round is unchanged) as training progresses. Figure 4 (and Figure 8 in §C) shows this can\nbe effective. We indeed observe that early in training (red), S in the 10 – 12.6 range works well\n(σ = 0 .006 – 0.0076). However, if we adjust the clipping/noise tradeoff after 4885 rounds of\ntraining and continue for another 6000, switching to S = 7.9 and σ= 0.0048 performs better.\nComparing DP and non-DP models While noised training with DP-FedAvg has only a small\neffect on predictive accuracy, it could still have a large qualitative effect on predictions. We hy-\n5This experiment was performed on different datasets, comparing training on a dataset of public social\nmedia posts to training on a proprietary dataset which is more representative of mobile keyboard usage, and\nevaluating on a held-out sample of the same representative dataset. Absolute AccuracyTop1 was similar to the\nvalues we report here for the Reddit dataset.\n8\nPublished as a conference paper at ICLR 2018\npothesized that noising updates might bias the model away from rarer words (whose embeddings\nget less frequent actual updates and hence are potentially more inﬂuenced by noise) and toward the\ncommon “head” words. To evaluate this hypothesis, we computed predictions on a sample of the\ntest set using a variety of models. At each st we intersect the top 10 predictions with the most\nfrequent 10,50,100 words in the dictionary. So for example, an intersection of size two in the top\n50 means two of the model’s top 10 predictions are in the 50 most common words in the dictionary.\nTable 3 gives histograms of these counts. We ﬁnd that better models (higher AccuracyTop1) tend to\nuse fewer head words, but see little difference from changing ˜Cor the noise σ(until, that is, enough\nnoise has been added to compromise model quality, at which point the degraded model’s bias toward\nthe head matches models of similar quality with less noise).\n4 C ONCLUSIONS\nIn this work, we introduced an algorithm for user-level differentially private training of large neural\nnetworks, in particular a complex sequence model for next-word prediction. We empirically evalu-\nated the algorithm on a realistic dataset and demonstrated that such training is possible at a negligible\nloss in utility, instead paying a cost in additional computation. Such private training, combined with\nfederated learning (which leaves the sensitive training data on device rather than centralizing it),\nshows the possibility of training models with signiﬁcant privacy guarantees for important real world\napplications. Much future work remains, for example designing private algorithms that automate\nand make adaptive the tuning of the clipping/noise tradeoff, and the application to a wider range\nof model families and architectures, for example GRUs and character-level models. Our work also\nhighlights the open direction of reducing the computational overhead of differentially private train-\ning of non-convex models.\nREFERENCES\nMartin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.\nDeep learning with differential privacy. In 23rd ACM Conference on Computer and Communications Secu-\nrity (ACM CCS), 2016a.\nMartin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang, and Xin\nPan. Source code for “deep learning with differential privacy”. github, October 2016b.http://github.\ncom/tensorflow/models/tree/master/differential_privacy.\nRami Al-Rfou, Marc Pickett, Javier Snaider, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil. Conversational\ncontextual cues: The case of personalization and history for response ranking.CoRR, abs/1606.00372, 2016.\nURL http://arxiv.org/abs/1606.00372.\nR. Bassily, K. Nissim, U. Stemmer, and A. Thakurta. Practical locally private heavy hitters. ArXiv e-prints,\nJuly 2017. URL https://arxiv.org/abs/1707.04982.\nRaef Bassily, Adam Smith, and Abhradeep Thakurta. Private empirical risk minimization: Efﬁcient algorithms\nand tight error bounds. In Proceedings of the 2014 IEEE 55th Annual Symposium on Foundations of Com-\nputer Science, FOCS ’14, pages 464–473, Washington, DC, USA, 2014. IEEE Computer Society. ISBN 978-\n1-4799-6517-5. doi: 10.1109/FOCS.2014.56. URL http://dx.doi.org/10.1109/FOCS.2014.\n56.\nN. Carlini, C. Liu, J. Kos, ´U. Erlingsson, and D. Song. The Secret Sharer: Measuring Unintended Neural\nNetwork Memorization & Extracting Secrets. ArXiv e-prints, February 2018. URL https://arxiv.\norg/abs/1802.08232.\nT-H Hubert Chan, Mingfei Li, Elaine Shi, and Wenchang Xu. Differentially private continual monitoring of\nheavy hitters from distributed streams. In International Symposium on Privacy Enhancing Technologies\nSymposium, pages 140–159. Springer, 2012.\nKamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially private empirical risk mini-\nmization. J. Mach. Learn. Res., 12, July 2011.\nCynthia Dwork. A ﬁrm foundation for private data analysis. Commun. ACM, 54(1):86–95, January 2011. ISSN\n0001-0782. doi: 10.1145/1866739.1866758. URL http://doi.acm.org/10.1145/1866739.\n1866758.\n9\nPublished as a conference paper at ICLR 2018\nCynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Foundations and Trends\nin Theoretical Computer Science. Now Publishers, 2014.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in private\ndata analysis. In TCC, pages 265–284. Springer, 2006.\nYarin Gal and Zoubin Ghahramani. A theoretically grounded application of dropout in recurrent neural net-\nworks. In Advances in Neural Information Processing Systems, pages 1019–1027, 2016.\nEdouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous\ncache. arXiv preprint arXiv:1612.04426, 2016.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–1780,\n1997.\nH. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Federated learning of deep\nnetworks using model averaging, 2016.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. arXiv\npreprint arXiv:1609.07843, 2016.\nNicolas Papernot, Mart´ın Abadi, ´Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-supervised knowl-\nedge transfer for deep learning from private training data. In Proceedings of the International Conference\non Learning Representations, 2017. URL https://arxiv.org/abs/1610.05755.\nOﬁr Press and Lior Wolf. Using the output embedding to improve language models. InProceedings of the 15th\nConference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short\nPapers, pages 157–163. Association for Computational Linguistics, 2017.\nReddit Comments Dataset. Reddit comments dataset. BigQuery, 2016. https://bigquery.cloud.\ngoogle.com/dataset/fh-bigquery.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks against\nmachine learning models. In IEEE Symposium on Security and Privacy, Oakland, 2017. To Appear.\nMartin Sundermeyer, Ralf Schl ¨uter, and Hermann Ney. LSTM neural networks for language modeling. In\nInterspeech, pages 194–197, 2012.\nXi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey F. Naughton. Bolt-on differen-\ntial privacy for scalable stochastic gradient descent-based analytics. In Proceedings of SIGMOD, 2017.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning\nrequires rethinking generalization. 2017. URL https://arxiv.org/abs/1611.03530.\n10\nPublished as a conference paper at ICLR 2018\nA A DDITIONAL PROOFS\nProof of Lemma 1. For the ﬁrst bound, observe the numerator in the estimator ˜ff can change by\nat most S between neighboring databases, by assumption. The denominator is a constant. For\nthe second bound, the estimator ˜fc can be thought of as the sum of the vectors wk∆k divided by\nmax(qWmin,∑\nk∈C∆k). Writing Num(C) for the numerator ∑\nk∈Cwk∆k, and Den(C) for the\ndenominator max(qWmin,∑\nk∈Cwk), the following are immediate for any Cand C′def\n= C∪{ k}:\n∥Num(C′) −Num(C)∥= ∥wk∆k∥≤ S.\n∥Den(C′) −Den(C)∥≤ 1.\n∥Den(C′)∥≥ qWmin.\nIt follows that\n∥˜fc(C′) −˜fc(C)∥=\n\nNum(C′)\nDen(C′) −Num(C)\nDen(C)\n\n=\n\nNum(C′) −Num(C)\nDen(C′) + Num(C)\n( 1\nDen(C′) − 1\nDen(C)\n)\n≤\n\nwk∆k\nDen(C′)\n+\n\nNum(C)\nDen(C)\n(Den(C) −Den(C′)\nDen(C′)\n)\n≤ S\nqWmin\n+ ∥˜fc(C)∥\n( 1\nqWmin\n)\n≤ 2S\nqWmin\n.\nHere in the last step, we used the fact that ∥˜fc(C)∥≤ S. The claim follows.\nProof of Theorem 1. It sufﬁces to verify that 1. the moments (of the privacy loss) at each step are\ncorrectly bounded; and, 2. the composability holds when accumulating the moments of multiple\nsteps.\nAt each step, users are selected randomly with probability q. If in addition the L2-norm of each\nuser’s update is upper-bounded byS, then the moments can be upper-bounded by that of the sampled\nGaussian mechanism with sensitivity 1, noise scale σ/S, and sampling probability q.\nOur algorithm, as described in Figure 1, uses a ﬁxed noise variance and generates the i.i.d. noise\nindependent of the private data. Hence we can apply the composability as in Theorem 2.1 in Abadi\net al. (2016a).\nWe obtain the theorem by combining the above and the sensitivity bounds ˜ff and ˜fc.\nB E XPERIMENT DETAILS\nModel The ﬁrst step in training a word-level recurrent language model is selecting the vocabulary\nof words to model, with remaining words mapped to a special “UNK” (unknown) token. Training\na fully differentially private language model from scratch requires a private mechanism to discover\nwhich words are frequent across the corpus, for example using techniques like distributed heavy-\nhitter estimation (Chan et al., 2012; Bassily et al., 2017). For this work, we simpliﬁed the problem\nby pre-selecting a dictionary of the most frequent 10,000 words (after normalization) in a large\ncorpus of mixed material from the web and message boards (but not our training or test dataset).\nOur recurrent language model works as follows: wordstis mapped to an embedding vectoret ∈R96\nby looking up the word in the model’s vocabulary. The et is composed with the state emitted by\nthe model in the previous time step st−1 ∈ R256 to emit a new state vector st and an “output\nembedding” ot ∈ R96. The details of how the LSTM composes et and st−1 can be found in\nHochreiter and Schmidhuber (1997). The output embedding is scored against the embedding of\neach item in the vocabulary via inner product, before being normalized via softmax to compute a\nprobability distribution over the vocabulary. Like other standard language modeling applications,\n11\nPublished as a conference paper at ICLR 2018\nwe treat every input sequence as beginning with an implicit “BOS” (beginning of sequence) token\nand ending with an implicit “EOS” (end of sequence) token.\nUnlike standard LSTM language models, our model uses the same learned embedding for the input\ntokens and for determining the predicted distribution on output tokens from the softmax. 6 This\nreduces the size of the model by about 40% for a small decrease in model quality, an advantageous\ntradeoff for mobile applications. Another change from many standard LSTM RNN approaches\nis that we train these models to restrict the word embeddings to have a ﬁxed L2 norm of 1.0, a\nmodiﬁcation found in earlier experiments to improve convergence time. In total the model has\n1.35M trainable parameters.\nInitialization and personalization For many applications public proxy data is available, e.g., for\nnext-word prediction one could use public domain books, Wikipedia articles, or other web con-\ntent. In this case, an initial model trained with standard (non-private) algorithms on the public data\n(which is likely drawn from the wrong distribution) can then be further reﬁned by continuing with\ndifferentially-private training on the private data for the precise problem at hand. Such pre-training\nis likely the best approach for practical applications. However, since training models purely on pri-\nvate data (starting from random initialization) is a strictly harder problem, we focus on this scenario\nfor our experiments.\nOur focus is also on training a single model which is shared by all users. However, we note that our\napproach is fully compatible with further on-device personalization of these models to the particular\ndata of each user. It is also possible to give the central model some ability to personalize simply by\nproviding information about the user as a feature vector along with the raw text input. LSTMs are\nwell-suited to incorporating such additional context.\nAccuracy metrics We evaluate usingAccuracyTop1, the probability that the word to which the\nmodel assigns highest probability is correct (after some minimal normalization). We always count it\nas a mistake if the true next word is not in the dictionary, even if the model predicts UNK, in order\nto allow fair comparisons of models using different dictionaries. In our experiments, we found that\nour model architecture is competitive on AccuracyTop1 and related metrics (Top3, Top5, and\nperplexity) across a variety of tasks and corpora.\nDataset The Reddit dataset can be accessed through Google BigQuery (Reddit Comments\nDataset). Since our goal is to limit the contribution of any one author to the ﬁnal model, it is\nnot necessary to include all the data from users with a large number of posts. On the other hand,\nprocessing users with too little data slows experiments (due to constant per-user overhead). Thus,\nwe use a training set where we have removed all users with fewer than 1600 tokens (words), and\ntruncated the remaining K = 763,430 users to have exactly 1600 tokens.\nWe intentionally chose a public dataset for research purposes, but carefully chose one with a struc-\nture and contents similar to private datasets that arise in real-world language modeling task such as\npredicting the next-word in a mobile keyboard. This allows for reproducibility, comparisons to non-\nprivate models, and inspection of the data to understand the impact of differential privacy beyond\ncoarse aggregate statistics (as in Table 3).\n6Press and Wolf (2017) independently introduced this technique and provide an empirical analysis compar-\ning models with and without weight tying.\n12\nPublished as a conference paper at ICLR 2018\nC S UPPLEMENTARY PLOTS\n0 500 1000 1500 2000 2500 3000 3500 4000\ncommunication rounds\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18AccuracyTop1\nComparison of estimators and sampling strategies\ntrue average, fixed sample\nfixed denominator, variable sample\ntrue average, variable sample\nclipped denominator, variable sample\n2500 3000 3500 4000\n0.171\n0.173\n0.175\nFigure 5: Comparison of sampling strategies and estimators. Fixed sample is exactly C = 100 users\nper round, and variable sample selects uniformly with probability qfor ˜C = 100. The true average\ncorresponds to f, ﬁxed denominator is ˜ff, and clipped denominator is ˜fc.\n1 2 4 8 16 32 64 128 256\nC (clients trained per round for FedAvg)\n0.12\n0.13\n0.14\n0.15\n0.16\n0.17\n0.18AccuracyTop1\nVarying the number of clients per round\n12000 rounds\n6000 rounds\n3000 rounds\n1500 rounds\n500 rounds\nFigure 6: The effect of C for FedAvg using the\nexact estimator and without noise or clipping.\n0 500 1000 1500 2000\ncommunication rounds\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16AccuracyTop1\nComparison of private training on 100 and 1250 clients, ¾ = 0: 012\n 100 clients\n1250 clients\nFigure 7: Training with (expected) 100 vs 1250\nusers per round, both with ﬂat-clipping at S =\n15.0 and per-coordinate noise with σ= 0.012.\n0 1000 2000 3000 4000 5000\ncommunication rounds\n0.08\n0.10\n0.12\n0.14\n0.16AccuracyTop1\nTraining with different noise vs. clipping tradeoffs\n3000 3500 4000 4500\n0.160\n0.164\n0.168\n0.172\n2000 4000 6000 8000 10000\ncommunication rounds\n0.158\n0.160\n0.162\n0.164\n0.166\n0.168\n0.170\n0.172AccuracyTop1\nContinuing training with different tradeoffs\nS =  1.2, ¾ = 0.0008\nS =  2.5, ¾ = 0.0015\nS =  5.0, ¾ = 0.0030\nS =  7.9, ¾ = 0.0048\nS = 10.0, ¾ = 0.0060\nS = 12.6, ¾ = 0.0076\nS = 15.9, ¾ = 0.0095\nS = 20.0, ¾ = 0.0120\nS = 30.0, ¾ = 0.0180\nFigure 8: The effect of different noise vs. clipping tradeoffs on convergence. Both plots use the same\nlegend, where we vary S and σ together to maintain the same z = 0.06 with 100 users (actually\nused), or z = 1 with 1667 users. We take S = 20 and σ= 0.012 (black line) as a baseline; the left-\nhand plot shows training from a randomly initialized model, and includes two different runs with\nS = 20, showing only mild variability. For the right-hand plot, we took a snapshot of the S = 20\nmodel after 4885 initial rounds of training, and resumed training with different tradeoffs.\n13\nPublished as a conference paper at ICLR 2018\n100 101\nS, total bound on L2 norm of updates\n0.10\n0.12\n0.14\n0.16\n0.18AccuracyTop1\nEffect of clipping updates (Federated SGD)\n20000 rounds, flat clip\n20000 rounds, per-layer\n5000 rounds, flat clip\n5000 rounds, per-layer\n1000 rounds, flat clip\n1000 rounds, per-layer\nFigure 9: Effect of clipping on FedSGD with\n˜C = 50 users per round and a learning rate of\nη = 6. A much smaller clipping level S can be\nused compared to FedAvg.\n10-4 10-3 10-2 10-1\n¾, noise std dev\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18AccuracyTop1\nEffect of noising updates (Federated SGD)\n20000 rounds, flat clip\n20000 rounds, per-layer\n5000 rounds, flat clip\n5000 rounds, per-layer\n1000 rounds, flat clip\n1000 rounds, per-layer\nFigure 10: Effect of noised updates on FedSGD\nwith S = 20 (based on Figure 9, a smaller\nvalue would actually be better when doing pri-\nvate training). FedSGD is more sensitive to noise\nthan FedAvg, likely because the updates are\nsmaller in magnitude.\n100 101 102\nS, total bound on L2 norm of updates\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18AccuracyTop1\n2500 rounds\n 500 rounds\n 100 rounds\nEffect of clipping updates (20k & 30k dict models)\n30k dict, flat clip\n30k dict, per-layer\n20k dict, flat clip\n20k dict, per-layer\nFigure 11: Effect of clipping on models with\nlarger dictionaries (20000 and 30000 tokens).\n10-4 10-3 10-2 10-1\n¾, noise std dev\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\n0.16\n0.18AccuracyTop1\n3000 rounds\n500 rounds\n100 rounds\nEffect of noising updates (20k & 30k dict models)\n30k dict, flat clip\n30k dict, per-layer clip\n20k dict, flat clip\n20k dict, per-layer clip\nFigure 12: Effect of noised updates on models\nwith larger dictionaries, when clipped atS = 20.\nD A DDITIONAL EXPERIMENTS\nExperiments with SGD We ran experiments usingFedSGD taking B = 1600, that is, computing\nthe gradient on each user’s full local dataset. To allow more iterations, we used ˜C = 50 rather than\n100. Examining Figures 9 and 10, we see S = 2 and σ = 2 ·10−3 are reasonable values, which\nsuggests for private training we would need in expectation qW = S/σ = 1500 users per round,\nwhereas for FedAvg we might choose S = 15 and σ= 10−2 for ˜C = qW = 1000 users per round.\nThat is, the relative effect of the ratio of the clipping level to noise is similar between FedAvg and\nFedSGD. However, FedSGD takes a signiﬁcantly larger number of iterations to reach equivalent\naccuracy. Fixing z= 1, ˜C = 5000 (the value that produced the best accuracy for a private model in\nTable 2) and total of 763,430 users gives (3.81,10−9)-DP after 3000 rounds and (8.92,10−9)-DP\nafter 20000 rounds, so there is indeed a signiﬁcant cost in privacy to these additional iterations.\nModels with larger dictionaries We repeated experiments on the impact of clipping and noise on\nmodels with 20000 and 30000 token dictionaries, again using FedAvg training with η= 6, equally\nweighted users with 1600 tokens, and ˜C = 100 expected users per round. The larger dictionaries\ngive only a modest improvement in accuracy, and do not require changing the clipping and noise\nparameters despite having signiﬁcantly more parameters. Results are given in Figures 11 and 12.\nOther experiments We experimented with adding an explicit L2 penalty on the model updates\n(not the full model) on each user, hoping this would decrease the need for clipping by preferring\nupdates with a smaller L2 norm. However, we saw no positive effect from this.\n14",
  "topic": "Differential privacy",
  "concepts": [
    {
      "name": "Differential privacy",
      "score": 0.9223470687866211
    },
    {
      "name": "Computer science",
      "score": 0.8095712065696716
    },
    {
      "name": "Computation",
      "score": 0.6192588210105896
    },
    {
      "name": "Stochastic gradient descent",
      "score": 0.5586028099060059
    },
    {
      "name": "The Internet",
      "score": 0.5356338620185852
    },
    {
      "name": "Language model",
      "score": 0.5274531841278076
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5162184238433838
    },
    {
      "name": "Work (physics)",
      "score": 0.5154193639755249
    },
    {
      "name": "Gradient descent",
      "score": 0.4531278610229492
    },
    {
      "name": "Machine learning",
      "score": 0.4523022770881653
    },
    {
      "name": "Differential (mechanical device)",
      "score": 0.45140114426612854
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43239596486091614
    },
    {
      "name": "Private information retrieval",
      "score": 0.4207678735256195
    },
    {
      "name": "Data mining",
      "score": 0.3520495295524597
    },
    {
      "name": "Computer security",
      "score": 0.21116524934768677
    },
    {
      "name": "Algorithm",
      "score": 0.1840612292289734
    },
    {
      "name": "World Wide Web",
      "score": 0.1423931121826172
    },
    {
      "name": "Artificial neural network",
      "score": 0.10149195790290833
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 103
}