{
    "title": "IIITT at CASE 2021 Task 1: Leveraging Pretrained Language Models for Multilingual Protest Detection",
    "url": "https://openalex.org/W3186581985",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2269691920",
            "name": "Pawan Kalyan",
            "affiliations": [
                "Indian Institute of Information Technology Allahabad",
                "Indian Institute of Management Tiruchirappalli"
            ]
        },
        {
            "id": "https://openalex.org/A3184102172",
            "name": "Duddukunta Reddy",
            "affiliations": [
                "Indian Institute of Information Technology Allahabad",
                "Indian Institute of Management Tiruchirappalli"
            ]
        },
        {
            "id": "https://openalex.org/A3121011784",
            "name": "Adeep Hande",
            "affiliations": [
                "Indian Institute of Management Tiruchirappalli",
                "Indian Institute of Information Technology Allahabad"
            ]
        },
        {
            "id": "https://openalex.org/A2972869875",
            "name": "Ruba Priyadharshini",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2625593156",
            "name": "Ratnasingam Sakuntharaj",
            "affiliations": [
                "Eastern University, Sri Lanka"
            ]
        },
        {
            "id": "https://openalex.org/A2913265672",
            "name": "Bharathi Raja Chakravarthi",
            "affiliations": [
                "Ollscoil na Gaillimhe – University of Galway"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2405991457",
        "https://openalex.org/W2971081948",
        "https://openalex.org/W2975437251",
        "https://openalex.org/W2064314529",
        "https://openalex.org/W2931732047",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2963612171",
        "https://openalex.org/W2962907576",
        "https://openalex.org/W2288977654",
        "https://openalex.org/W2280364541",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2883845484",
        "https://openalex.org/W3159519707",
        "https://openalex.org/W2186547183",
        "https://openalex.org/W2125086802",
        "https://openalex.org/W2965449209",
        "https://openalex.org/W128570927",
        "https://openalex.org/W2889275779",
        "https://openalex.org/W2320556257",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2490855655",
        "https://openalex.org/W611915850",
        "https://openalex.org/W3087798676",
        "https://openalex.org/W2094474086",
        "https://openalex.org/W3086837826",
        "https://openalex.org/W2620926136",
        "https://openalex.org/W3088577908",
        "https://openalex.org/W3184121905",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3131531807",
        "https://openalex.org/W2767327746",
        "https://openalex.org/W4298136482",
        "https://openalex.org/W3174724031",
        "https://openalex.org/W2095705004"
    ],
    "abstract": "Pawan Kalyan, Duddukunta Reddy, Adeep Hande, Ruba Priyadharshini, Ratnasingam Sakuntharaj, Bharathi Raja Chakravarthi. Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE 2021). 2021.",
    "full_text": "Proceedings of the 4th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE), pages 98–104\nAugust 5–6, 2021, ©2021 Association for Computational Linguistics\n98\nIIITT at CASE 2021 Task 1: Leveraging Pretrained Language\nModels for Multilingual Protest Detection\nPawan Kalyan Jada1, Duddukunta Sashidhar Reddy1, Adeep Hande1,\nRuba Priyadharshini2, Ratnasingam Sakuntharaj3, Bharathi Raja Chakravarthi4\n1 Indian Institute of Information Technology Tiruchirappalli\n2 ULTRA Arts and Science College, India, 3Eastern University, Sri Lanka\n4Insight SFI Research Centre for Data Analytics, National University of Ireland Galway\npawankj19c@iiitt.ac.in\nAbstract\nIn a world abounding in constant protests re-\nsulting from events like a global pandemic,\nclimate change, religious or political con-\nﬂicts, there has always been a need to de-\ntect events/protests before getting ampliﬁed\nby news media or social media. This paper\ndemonstrates our work on the sentence clas-\nsiﬁcation subtask of multilingual protest de-\ntection in CASE@ACL-IJCNLP 2021. We\napproached this task by employing various\nmultilingual pre-trained transformer models to\nclassify if any sentence contains information\nabout an event that has transpired or not. Fur-\nthermore, we performed soft voting over the\nmodels, achieving the best results among the\nmodels, accomplishing a macro F1-Score of\n0.8291, 0.7578, and 0.7951 in English, Span-\nish, and Portuguese, respectively. The source\ncodes for our systems are published1.\n1 Introduction\nThe recent surge in social media users has led many\npeople to express their opinions on various global\nissues. These opinions travel far and wide within\na matter of seconds (Hossny et al., 2018). This\ncan inﬂuence many people and may engage public\nmovements (Won et al., 2017a). Therefore, there is\na deﬁnite need to detect these protests and analyse\nthem to know the signiﬁcant areas of disinterest.\nBeing a free and easy to use platform, social\nmedia has become a part of our day to day life.\nIt incorporates people of different ages, gender,\nlocation, religions, background, and so on. The\nenormous number of rich and diversiﬁed users re-\nsults in an enormous amount of information being\ngenerated, which is helpful in many ways (Kapoor\net al., 2018). Some of this even contains private\ninformation about the users, which others could\nmisuse. Cases were also found where certain users\n1https://github.com/adeepH/\nCASE-2021-Task-1\nwere being targeted and harassed by people using\nthis platform, a common scenario in cyberbullying\n(Abaido, 2020).\nSocial media plays a crucial role in amplifying\nthese protests and movements (Won et al., 2017b).\nIt enables political groups and protesters to organ-\nise protest movements and share information. It\nacts as a platform for the people who are underrep-\nresented by giving a voice to them. It also offers\nnew opportunities for people to engage in activism,\npolitical resistance, and protest outside the political\ngroups and civic institutions. Thus, it has a social\nimpact on everyone (Pulido et al., 2018). It is to\nbe noted that social media, similar to news media,\nplays a vital role in its social and political events\nworldwide (Holt et al., 2013). For the above rea-\nsons, we can state that social media plays a crucial\nrole in most worldwide events.\nThe English language is widely regarded as the\nﬁrst Lingua Franca. Statistically, it is one of the\nmost widely spoken languages globally, having of-\nﬁcial status in over 53 countries (Crystal, 2008).\nOver 400 million people speak English as their pri-\nmary language and widely spoken in the United\nStates and the United Kingdom. BlackLivesMat-\nter (Dave et al., 2020), EarthDay (Rome, 2010)\nare some of the major protests that have occurred\nin these countries. Espa ˜nol commonly referred to\nas Spanish, is spoken by over 360 million peo-\nple worldwide, with most of its speakers resid-\ning in Mexico, Argentina, Spain. 15-M Move-\nment (Casero-Ripoll´es and Feenstra, 2012) and\nYoSoy132 (Garc´ıa and Trer´e, 2014) are some of\nthe recent protests where people have been vocal\nabout in the Spanish language. Portuguese has\nover 220 million native speakers. Brazil, Portugal,\nAngola are some of the major countries where this\nlanguage is spoken. Protests likeRacism Kills, May\n68 (Ross, 2008) are the recent ones that occurred\nin the Portuguese language.\nThe recent upheavals of protests are due to so-\n99\nSentence Language Label\nFabius ran against Royal for the presidential nomination in 2007. English Event\nHe planned to start a race war. English Event\nMetro police intervened and the ﬁre was put out. English Not-event\nPero no es ´ese el mayor problema. Spanish Event\nLa Argentina retroceder´ıa un paso todos los d´ıas. Spanish Event\nCarri´o no objet´o que se trat´o de un secuestro. Spanish Not-event\nOs servidores do Piau´ı est˜ao em greve h´a 17 dias. Portuguese Not-event\n´E uma nova experiˆencia mobilizat´oria. Portuguese Event\n´E decidiram ir `as aulas e passar o dia de saia. Portuguese Not-event\nTable 1: Examples of the dataset indicating events of the past and not-events.\ncial media, youth, exaggeration of certain events.\n(Basile and Caselli, 2020). Any early detection of\nmass protest detection through social media plat-\nforms such as Facebook, Twitter, and Instagram to\nhelp minimizing the aftermath of the protests (Wil-\nson, 2017). This has motivated Natural Language\nProcessing (NLP) researchers to develop NLP sys-\ntems to generalize on data coming from diverse\nsources to leverage the NLP systems to more real-\nistic environments (B¨uy¨uk¨oz et al., 2020). Hence,\nthere is a need to develop NLP systems that could\nbe generalized to any protest/events (Peng et al.,\n2013), which has motivated us to participate in\nthe shared task for multilingual protest detection\n(H¨urriyeto˘glu et al., 2019a, 2021) The objective of\nthe task is to identify if any sentence talks about any\nmentions of protests or events in three languages,\nnamely, English, Spanish, and Portuguese. Hence,\nwe treat this as a sequence classiﬁcation task.\nThe rest of the paper is organized as follows, Sec-\ntion 2 presents previous work on protest detection\nand analysis. Section 3 entails a comprehensive\nanalysis of the dataset used for our cause. Next,\nsection 4 gives a detailed description of the models\nused for the multilingual event detection. Finally,\nsection 5 analyses the results obtained, and Section\n6 concludes our work while discussing the potential\ndirections for future work.\n2 Related Work\nThe need to detect events that could lead to protests\nis of prime interest to sociologists and govern-\nments (Danilova et al., 2016). There are several\nactive ongoing projects for socio-political event\nsystems such as KEDS (Kansas Event Data Sys-\ntem) (Schrodt and Hall, 2006), CAMEO (Conﬂict\nand Mediation Event Observation) (Gerner et al.,\n2002), and several other databases for protest de-\ntection systems (Danilova, 2015). These methods\nhave focused on news data as they have tradition-\nally been the most reliant source of events. Protest\ndetection has been one of the major issues in the\ncontext of social and political (Ettinger et al., 2017).\nPapanikolaou and Papageorgiou (2020) presented a\ncomputational social science methodology to anal-\nyse protests in Greece. H ¨urriyeto˘glu et al. (2021)\nconstructed a corpus of protest events compris-\ning various language sources from various coun-\ntries. Several systems were submitted to the CLEF\nProtestNews Track that consisted of three shared\ntasks, primarily aimed at identifying and extracting\nevent information spanning to multiple countries\n(H¨urriyeto˘glu et al., 2019b, 2020).\n3 Dataset\nThis dataset comprises 26,208 sentences in three\nlanguages, namely English, Spanish, and Por-\ntuguese. The dataset consists of two classes:\n• Event: The sentence indicates an event of the\npast.\n• Not-event: The sentence does not talk about\nany event.\nThe volume of sequences indicating Not-event is\nhigher in contrast to that of the Event label. There-\nfore, the dataset distribution is quite imbalanced.\nWe can also notice that the number of English sam-\nples exceeds that of Spanish and Portuguese ones.\nRefer to Table 1 for examples of sentences talking\nabout events and not talking about events displayed\nin English, Spanish, and Portuguese. The dataset\ndistribution is displayed in Table 2. For our cause,\nwe split the training and validation set in the ratio\nof 80:20.\n100\n[CLS] Tok 1 Tok 2 Tok N..... \nE[CLS] E1 E2 EN..... \nC TNT2T1\n..... \n..... \n..... \nLSTM LSTMLSTM LSTM..... \nGlobal Average Pooling\nDropout\n..... \n..... \nDense\nLayer(128\nNeurons)\nDense\nLayer(1\nNeuron)\nPretrained \nLanguage \nModel \nNot-event Event \nFigure 1: System Architecture based on BERT (Devlin et al., 2019)\nLanguage English Spanish Portuguese\nNot-event 18,602 2,291 901\nEvent 4,223 450 281\nTotal 22,285 2,741 1,182\nTable 2: Classwise distribution of the training set\n4 Methodology\nWe used pretrained transformer-based models for\nidentifying if a sentence talks about an event or not.\nThe models that were used are BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019) and DistilBERT\n(Sanh et al., 2019). Even though there are 3 dif-\nferent languages, we used a single model for all\nthree due to memory constraints and reduced train-\ning time. We ﬁne-tuned these models for sequence\nclassiﬁcation. Soft V oting is done on all these mod-\nels to produce the respective ﬁnal outputs for the\nlanguages. In soft voting, each classiﬁer predicts\nthat a speciﬁc data point belongs to the particular\ntarget class. A weighted sum of the predictions is\ndone based on the importance of the classiﬁer (all\nmodels have equal weights). The overall predic-\ntion is chosen as the target with the greatest sum\nof the weighted probability, thus winning the vote\n(Beyeler, 2017; Hande et al., 2021).\n4.1 BERT\nBidirectional Encoder Representations from\nTransformers (BERT) (Devlin et al., 2019) is a\npretrained language model which was created with\nthe objective that ﬁne-tuning a pretrained model\nyields better performance. BERT’s pretraining\nphase includes two tasks. Firstly, Masked\nLanguage Modeling (MLM) is where certain\nwords are randomly masked in a sequence. About\n15% of the words in a sequence is masked. The\nmodel then attempts to predict the masked words.\nSecondly, Next Sentence Prediction (NSP), where\nthe model has an additional loss function, NSP\nloss, indicates if the second sequence follows the\nﬁrst one. Around 50% of the inputs are a pair, and\nthey randomly chose the other 50. Here, we use a\nbert-base-multilingual-cased (Pires et al., 2019)\ntrained on top of 104 languages in the largest\nWikipedia corpus. This model has 12 layers, 12\nAttention heads with over 179 million parameters.\n101\nEvent Not-event Overall\nModel P R F1 P R F1 Acc M(P) M(R) M(F1)\nmBERT 0.928 0.917 0.922 0.641 0.675 0.658 0.873 0.784 0.796 0.790\nDistilmBERT 0.924 0.947 0.936 0.729 0.646 0.685 0.893 0.827 0.797 0.810\nRoBERTa 0.910 0.938 0.924 0.670 0.578 0.621 0.873 0.790 0.758 0.772\nSoftV oting 0.937 0.939 0.938 0.720 0.713 0.717 0.899 0.829 0.826 0.827\nTable 3: Precision (P), recall (R), and F1-Score of the models on the validation set; M(P), M(R), and M(F1) are\nthe Macro averages of precision, recall, and F1-Score respectively\n4.2 DistilmBERT\nDistilBERT (Sanh et al., 2019) is the distilled ver-\nsion of BERT. DistilBERT employs a triple loss\nlanguage modelling, where it integrates cosine dis-\ntance loss with knowledge distillation. DistilBERT\nhas 40% fewer parameters than BERT but still\npromises 97% of the latter’s performance. It is\nalso 60% faster than BERT. In this system, we used\na cased multilingual DistilBERT model as they are\nthree different languages. For our cause, we ﬁne-\ntune distilbert-base-multilingual-cased, which is\ndistilled from the mBERT checkpoint. The model\nhas 6 layers, 768 dimensions, and 12 Attention\nheads, totalizing about 134 million parameters.\n4.3 RoBERTa\nRobustly Optimized BERT (RoBERTa) (Liu et al.,\n2019) follows the same architecture of BERT while\ndiffering in the pretraining strategy. It is pretrained\nwith MLM as its objective where the model tries\nto predict the masked words. RoBERTa model\nis trained on the vast English Wikipedia and CC-\nNews datasets. The NSP is not employed as a\npretraining strategy, and the tokens are dynami-\ncally masked, making the model slightly different\nto BERT. During tokenization, RoBERTa follows\nbyte-pair encoding (BPE) (Gall´e, 2019) as opposed\nto WordPiece employed in BERT. We useroberta-\nbase, a pretrained language model consisting of\n12 layers, 768 hidden, 12 attention heads, and 125\nmillion parameters.\n4.4 System Description\nFor our system, we ﬁne-tune the pretrained models\ndiscussed in Section 4.1, 4.2, and 4.3. We com-\nbine the three datasets as the number of samples\nfor Spanish and Portuguese are quite low. Af-\nter combining the models, we split the validation\nset accordingly, maintaining the split’s ratio and\ntabulating the results on the concatenated dataset\nin Table3. The embeddings are extracted from\nthese models to be fed as input to the LSTM layer,\n(Hochreiter and Schmidhuber, 1997) as shown in\nFigure1. The resulting output is fed into a global\naverage pooling layer (Lin et al., 2014) and then\npassed into fully connected layers, followed by a\nsigmoid activation function to obtain the resulting\nprobability score for the input sentences. The same\nparameters are used for all three models. A dropout\nlayer (Srivastava et al., 2014) is also added in be-\ntween the fully connected layers for regularization.\nRefer Table 4 for the parameters used in the model.\nParameters Values\nNumber of LSTM units 128\nDropout Rate 0.2\nBatch Size 16\nMax Length 128\nOptimizer Adam\nLearning Rate 3e-5\nActivation Function Sigmoid\nLoss Function cross-entropy\nTable 4: Parameters used for training the Models\n5 Results and Analysis\nAll pretrained language models are ﬁne-tuned in\nGoogle Colab2 for ten epochs. We use the Tensor-\nﬂow implementation of the models 3 on the Hug-\ngingface transformers library 4. We compare the\nmacro F1-Scores of our ﬁne-tuned models on the\nvalidation set, which were created by splitting the\ngiven dataset. The remaining split is the training\ndata. The validation set contains samples from all\nthree languages. It has 4,387 Not-event sequences\nand 963 Event sequences making a combined total\nof 5,350. The results are shown in Table3.\nWe ﬁne-tuned BERT, DistilBERT, and RoBERTa\nmodels on the training set. We have combined the\n2https://colab.research.google.com/\n3https://huggingface.co/transformers/pretrained models.html\n4https://huggingface.co/\n102\nLanguage Macro F1-Score\nEnglish 0.8291\nSpanish 0.7578\nPortuguese 0.7951\nTable 5: Macro F1-Scores on the Test Set\nthree language corpora into a single corpus com-\nprising of all the three languages together. The\nmain intention towards using a multilingual model\nis that the representations learnt during one lan-\nguage’s pretraining would help the other. We can\nobserve that DistilBERT achieved a better F1-Score\namong the models mentioned in the previous sec-\ntions. RoBERTa gave the lowest score among these.\nThe reason could be that the RoBERTa model was\nnot multilingual, unlike the other two; however, it\nstill managed to get a score very close to the BERT\nmodel. It is imperative that performing soft vot-\ning on all three models has managed to increase\nthe score. One of the reasons for the poor per-\nformance of the models is the imbalance in the\ndistribution of the classes. In the dataset, there are\n21,794 Not-event sentences and only 4,954 Event\nones. The models performed very well in the major-\nity class and poorly in the minority class. Having\nmore Event samples could have certainly helped the\nmodel in distinguishing better among the classes.\nBased on the performance of soft voting on the\nvalidation set, we have used the same for the test\nset. The results for the test set are shown in Table5.\nThe reason for relatively low scores of Spanish and\nPortuguese could be due to the inadequate support\nof the training set (2,741 and 1,182) instead of En-\nglish (22,825). We also believe that our approach\nof combining datasets could have inﬂuenced the\nperformance of the low support datasets.\n6 Conclusion\nThe need to develop automated systems to detect\nany event is an active protest has constantly been\nincreasing because of the escalation of social media\nusers and several platforms to support them. In this\npaper, we have explored several multilingual lan-\nguage models to classify if a given sentence talks\nabout an event that has happened ( Event) or not\n(Not-event) in three languages. Our work primarily\nfocuses on ﬁne-tuning language models and feed-\ning them to an architecture we created. We also\nobserve that the problem of class imbalance has\nhad a signiﬁcant impact on the performance of the\nmodels. The soft voting approach has achieved\nmacro F1-Scores of 0.8291, 0.7578, and 0.7951\nfor English, Spanish, and Portuguese, respectively.\nFor future work, we intend to explore class weight-\ning techniques and semi-supervised approaches to\nimprove our performance.\nReferences\nGhada Abaido. 2020. Cyberbullying on social media\nplatforms among university students in the united\narab emirates. International journal of adolescence\nand youth, 25:407–420.\nAngelo Basile and Tommaso Caselli. 2020. Protest\nevent detection: When task-speciﬁc models outper-\nform an event-driven method. In Experimental IR\nMeets Multilinguality, Multimodality, and Interac-\ntion, pages 97–111, Cham. Springer International\nPublishing.\nMichael Beyeler. 2017. Machine Learning for\nOpenCV. Packt Publishing Ltd.\nBerfu B¨uy¨uk¨oz, Ali H ¨urriyeto˘glu, and Arzucan ¨Ozg¨ur.\n2020. Analyzing ELMo and DistilBERT on socio-\npolitical news classiﬁcation. In Proceedings of\nthe Workshop on Automated Extraction of Socio-\npolitical Events from News 2020, pages 9–18, Mar-\nseille, France. European Language Resources Asso-\nciation (ELRA).\nAndreu Casero-Ripoll ´es and Ram ´on Feenstra. 2012.\nThe 15-m movement and the new media: A case\nstudy of how new themes were introduced into span-\nish political discourse. Media International Aus-\ntralia incorporating Culture and Policy, 144:68.\nDavid Crystal. 2008. Two thousand million? English\nToday, 24(1):3–6.\nVera Danilova. 2015. A pipeline for multilingual\nprotest event selection and annotation. In 2015\n26th International Workshop on Database and Ex-\npert Systems Applications (DEXA), pages 309–313.\nVera Danilova, Svetlana Popova, and Mikhail Alexan-\ndrov. 2016. Multilingual protest event data collec-\ntion with gate. In Natural Language Processing\nand Information Systems, pages 115–126, Cham.\nSpringer International Publishing.\nDhaval M Dave, Andrew I Friedson, Kyutaro Mat-\nsuzawa, Joseph J Sabia, and Samuel Safford. 2020.\nBlack lives matter protests and risk avoidance: The\ncase of civil unrest during a pandemic. Working Pa-\nper 27408, National Bureau of Economic Research.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\n103\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAllyson Ettinger, Sudha Rao, Hal Daum ´e III, and\nEmily M. Bender. 2017. Towards linguistically gen-\neralizable NLP systems: A workshop and shared\ntask. In Proceedings of the First Workshop on\nBuilding Linguistically Generalizable NLP Systems,\npages 1–10, Copenhagen, Denmark. Association for\nComputational Linguistics.\nMatthias Gall´e. 2019. Investigating the effectiveness of\nBPE: The power of shorter sequences. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 1375–1381, Hong\nKong, China. Association for Computational Lin-\nguistics.\nRodrigo G ´omez Garc ´ıa and Emiliano Trer ´e. 2014.\nThe# yosoy132 movement and the struggle for\nmedia democratization in mexico. Convergence,\n20(4):496–510.\nDeborah J Gerner, Philip A Schrodt, Omur Yilmaz, and\nRajaa Abu-Jabr. 2002. The creation of cameo (con-\nﬂict and mediation event observations): An event\ndata framework for a post cold war world. In an-\nnual meeting of the American Political Science As-\nsociation, volume 29.\nAdeep Hande, Karthik Puranik, Ruba Priyadharshini,\nand Bharathi Raja Chakravarthi. 2021. Domain\nidentiﬁcation of scientiﬁc articles using transfer\nlearning and ensembles. In Trends and Applications\nin Knowledge Discovery and Data Mining, pages\n88–97, Cham. Springer International Publishing.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural Computation,\n9(8):1735–1780.\nKristoffer Holt, Adam Shehata, Jesper Str¨omb¨ack, and\nElisabet Ljungberg. 2013. Age and the effects of\nnews media attention and social media use on politi-\ncal interest and participation: Do social media func-\ntion as leveller? European Journal of Communica-\ntion, 28(1):19–34.\nAhmad Hany Hossny, Terry Moschuo, Grant Osborne,\nLewis Mitchell, and Nick Lothian. 2018. Enhanc-\ning keyword correlation for event detection in social\nnetworks using svd and k-means: Twitter case study.\nSocial Network Analysis and Mining, 8(1):1–10.\nAli H ¨urriyeto˘glu, Osman Mutlu, Farhana Ferdousi\nLiza, Erdem Y ¨or¨uk, Ritesh Kumar, and Shyam\nRatan. 2021. Multilingual protest news detection -\nshared task 1, case 2021. In Proceedings of the 4th\nWorkshop on Challenges and Applications of Auto-\nmated Extraction of Socio-political Events from Text\n(CASE 2021), online. Association for Computational\nLinguistics (ACL).\nAli H ¨urriyeto˘glu, Erdem Y ¨or¨uk, Deniz Y ¨uret, C ¸ a˘grı\nYoltar, Burak G ¨urel, Fırat Durus ¸an, and Osman\nMutlu. 2019a. A task set proposal for automatic\nprotest information collection across multiple coun-\ntries. In Advances in Information Retrieval, pages\n316–323, Cham. Springer International Publishing.\nAli H ¨urriyeto˘glu, Erdem Y ¨or¨uk, Deniz Y ¨uret, C ¸ a˘grı\nYoltar, Burak G ¨urel, Fırat Durus ¸an, Osman Mutlu,\nand Arda Akdemir. 2019b. Overview of clef 2019\nlab protestnews: Extracting protests from news\nin a cross-context setting. In Experimental IR\nMeets Multilinguality, Multimodality, and Interac-\ntion, pages 425–432, Cham. Springer International\nPublishing.\nAli H ¨urriyeto˘glu, Vanni Zavarella, Hristo Tanev, Er-\ndem Y ¨or¨uk, Ali Safaya, and Osman Mutlu. 2020.\nAutomated extraction of socio-political events from\nnews (AESPEN): Workshop and shared task report.\nIn Proceedings of the Workshop on Automated Ex-\ntraction of Socio-political Events from News 2020,\npages 1–6, Marseille, France. European Language\nResources Association (ELRA).\nAli H ¨urriyeto˘glu, Erdem Y ¨or¨uk, Osman Mutlu, Fırat\nDurus ¸an, C ¸ a˘grı Yoltar, Deniz Y ¨uret, and Burak\nG¨urel. 2021. Cross-Context News Corpus for\nProtest Event-Related Knowledge Base Construc-\ntion. Data Intelligence, pages 1–28.\nKawal Kapoor, Kuttimani Tamilmani, Nripendra Rana,\nPushp Patil, Yogesh Dwivedi, and Sridhar Nerur.\n2018. Advances in social media research: Past,\npresent and future. Information Systems Frontiers,\n20.\nMin Lin, Qiang Chen, and Shuicheng Yan. 2014. Net-\nwork in network.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nKonstantina Papanikolaou and Haris Papageorgiou.\n2020. Protest event analysis: A longitudinal anal-\nysis for Greece. In Proceedings of the Workshop on\nAutomated Extraction of Socio-political Events from\nNews 2020, pages 57–62, Marseille, France. Euro-\npean Language Resources Association (ELRA).\nYifan Peng, Manabu Torii, Cathy H. Wu, and K. Vijay-\nShanker. 2013. A generalizable nlp framework for\nfast development of pattern-based biomedical rela-\ntion extraction systems. BMC Bioinformatics, 15.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\n104\nCristina M. Pulido, Gisela Redondo-Sama, Teresa\nSord´e-Mart´ı, and Ramon Flecha. 2018. Social im-\npact in social media: A new method to evaluate the\nsocial impact of research. PLOS ONE, 13(8):1–20.\nAdam Rome. 2010. The genius of earth day. Environ-\nmental History, 15(2):194–205.\nKristin Ross. 2008. May’68 and its Afterlives. Univer-\nsity of Chicago Press.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. ArXiv,\nabs/1910.01108.\nPhilip A Schrodt and Blake Hall. 2006. Twenty years\nof the kansas event data system project. The politi-\ncal methodologist, 14(1):2–8.\nNitish Srivastava, Geoffrey E. Hinton, Alex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. 2014. Dropout: a simple way to prevent neural\nnetworks from overﬁtting. Journal of Machine\nLearning Research, 15(1):1929–1958.\nS. Wilson. 2017. Detecting mass protest through social\nmedia. Social media and society, 6:5–25.\nDonghyeon Won, Zachary C Steinert-Threlkeld, and\nJungseock Joo. 2017a. Protest activity detection and\nperceived violence estimation from social media im-\nages. In Proceedings of the 25th ACM international\nconference on Multimedia, pages 786–794.\nDonghyeon Won, Zachary C. Steinert-Threlkeld, and\nJungseock Joo. 2017b. Protest activity detection and\nperceived violence estimation from social media im-\nages. CoRR, abs/1709.06204."
}