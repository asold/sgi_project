{
    "title": "FaciesViT: Vision transformer for an improved core lithofacies prediction",
    "url": "https://openalex.org/W4301395778",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1491907285",
            "name": "Ardiansyah Koeshidayatullah",
            "affiliations": [
                "King Fahd University of Petroleum and Minerals"
            ]
        },
        {
            "id": "https://openalex.org/A2943968897",
            "name": "Sadam Al-Azani",
            "affiliations": [
                "King Fahd University of Petroleum and Minerals"
            ]
        },
        {
            "id": "https://openalex.org/A4222738440",
            "name": "Evgeny E. Baraboshkin",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2577353977",
            "name": "Motaz Alfarraj",
            "affiliations": [
                "King Fahd University of Petroleum and Minerals"
            ]
        },
        {
            "id": "https://openalex.org/A1491907285",
            "name": "Ardiansyah Koeshidayatullah",
            "affiliations": [
                "King Fahd University of Petroleum and Minerals"
            ]
        },
        {
            "id": "https://openalex.org/A2943968897",
            "name": "Sadam Al-Azani",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222738440",
            "name": "Evgeny E. Baraboshkin",
            "affiliations": [
                "Skolkovo Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2577353977",
            "name": "Motaz Alfarraj",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2973127997",
        "https://openalex.org/W2963941635",
        "https://openalex.org/W2896434829",
        "https://openalex.org/W3085848884",
        "https://openalex.org/W2322715126",
        "https://openalex.org/W654711263",
        "https://openalex.org/W4210320489",
        "https://openalex.org/W4206956848",
        "https://openalex.org/W2906252920",
        "https://openalex.org/W3193635235",
        "https://openalex.org/W2976736125",
        "https://openalex.org/W6761956422",
        "https://openalex.org/W3128592650",
        "https://openalex.org/W3004916592",
        "https://openalex.org/W6794166682",
        "https://openalex.org/W3180045188",
        "https://openalex.org/W2484830594",
        "https://openalex.org/W2931285398",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W3011758222",
        "https://openalex.org/W4283528320",
        "https://openalex.org/W3011887221",
        "https://openalex.org/W2611953050",
        "https://openalex.org/W4288081119",
        "https://openalex.org/W4283754521",
        "https://openalex.org/W6739622702",
        "https://openalex.org/W123489686",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6768872936",
        "https://openalex.org/W1571804710",
        "https://openalex.org/W3081524168",
        "https://openalex.org/W4281570153",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W2919115771",
        "https://openalex.org/W2597243853",
        "https://openalex.org/W6792155083",
        "https://openalex.org/W2497703895",
        "https://openalex.org/W3173149037",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W6741574158",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W3192598395",
        "https://openalex.org/W1980287119",
        "https://openalex.org/W2125687509",
        "https://openalex.org/W3125653960",
        "https://openalex.org/W2954996726",
        "https://openalex.org/W6634649555",
        "https://openalex.org/W2200402913",
        "https://openalex.org/W3124889144",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2395579298",
        "https://openalex.org/W2807914764",
        "https://openalex.org/W2999581854",
        "https://openalex.org/W2911424749",
        "https://openalex.org/W3211490618",
        "https://openalex.org/W3156418461",
        "https://openalex.org/W6731085071",
        "https://openalex.org/W4242177601",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W2736834450",
        "https://openalex.org/W2499961455",
        "https://openalex.org/W324189739",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W4295312788"
    ],
    "abstract": "Lithofacies classification is a fundamental step to perform depositional and reservoir characterizations in the subsurface. However, such a classification is often hindered by limited data availability and biased and time-consuming analysis. Recent work has demonstrated the potential of image-based supervised deep learning analysis, specifically convolutional neural networks (CNN), to optimize lithofacies classification and interpretation using core images. While most works have used transfer learning to overcome limited datasets and simultaneously yield a high-accuracy prediction. This method raises some serious concerns regarding how the CNN model learns and makes a prediction as the model was originally trained with entirely different datasets. Here, we proposed an alternative approach by adopting a vision transformer model, known as FaciesViT , to mitigate this issue and provide improved lithofacies prediction. We also experimented with various CNN architectures as the baseline models and two different datasets to compare and evaluate the performance of our proposed model. The experimental results show that the proposed models significantly outperform the established CNN architecture models for both datasets and in all cases, achieving an f1 score and weighted average in all tested metrics of 95%. For the first time, this study highlights the application of the Vision Transformer model to a geological dataset. Our findings show that the FaciesViT model has several advantages over conventional CNN models, including (i) no hyperparameter fine-tuning and exhaustive data augmentation required to match the accuracy of CNN models; (ii) it can work with limited datasets; and (iii) it can better generalize the classification to a new, unseen dataset. Our study shows that the application of the Vision transformer could further optimize image recognition and classification in the geosciences and mitigate some of the issues related to the generalizability and the explainability of deep learning models. Furthermore, the implementation of our proposed FaciesViT model has been shown to improve the overall performance and reproducibility of image-based core lithofacies classification which is significant for subsurface reservoir characterization in different basins worldwide.",
    "full_text": "FaciesViT: Vision transformer for\nan improved core lithofacies\nprediction\nArdiansyah Koeshidayatullah1*† , Sadam Al-Azani2† ,\nEvgeny E. Baraboshkin3,4 and Motaz Alfarraj2*\n1Department of Geosciences, College of Petroleum Engineering and Geosciences, King Fahd\nUniversity of Petroleum and Minerals, Dhahran, Saudi Arabia,2SDAIA-KFUPM Joint Research Center for\nArtiﬁcial Intelligence (JRC-AI), King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia,\n3Skolkovo Institute of Science and Technology, Moscow, Russia,4Digital Petroleum LLC, Moscow,\nRussia\nLithofacies classiﬁcation is a fundamental step to perform depositional and reservoir\ncharacterizations in the subsurface. However, such a classiﬁcation is often hindered\nby limited data availability and biased and time-consuming analysis. Recent work has\ndemonstrated the potential of image-based supervised deep learning analysis,\nspeciﬁcally convolutional neural networks (CNN), to optimize lithofacies\nclassiﬁcation and interpretation using core images. While most works have used\ntransfer learning to overcome limited datasets and simultaneously yield a high-\naccuracy prediction. This method raises some serious concerns regarding how the\nCNN model learns and makes a prediction as the model was originally trained with\nentirely different datasets. Here, we proposed an alternative approach by adopting a\nvision transformer model, known asFaciesViT, to mitigate this issue and provide\nimproved lithofacies prediction. We also experimented with various CNN\narchitectures as the baseline models and two different datasets to compare and\nevaluate the performance of our proposed model. The experimental results show\nthat the proposed models signi ﬁcantly outperform the established CNN\narchitecture models for both datasets andi na l lc a s e s ,a c h i e v i n ga nf 1s c o r ea n d\nweighted average in all tested metrics of 95%. For theﬁrst time, this study highlights\nthe application of the Vision Transformer model to a geological dataset. Ourﬁndings\nshow that the FaciesViT model has several advantages over conventional CNN\nmodels, including (i) no hyperparameter ﬁne-tuning and exhaustive data\naugmentation required to match the accuracy of CNN models; (ii) it can work\nwith limited datasets; and (iii) it can better generalize the classiﬁcation to a new,\nunseen dataset. Our study shows that the application of the Vision transformer could\nfurther optimize image recognition and classiﬁcation in the geosciences and\nmitigate some of the issues related to the generalizability and the explainability\nof deep learning models. Furthermore, the implementation of our proposed\nFaciesViT model has been shown to improve the overall performance and\nreproducibility of image-based core lithofacies classiﬁcation which is signiﬁcant\nfor subsurface reservoir characterization in different basins worldwide.\nKEYWORDS\nlithofacies, geosciences, transformers, AI, deep learning\nOPEN ACCESS\nEDITED BY\nAtaollah Shirzadi,\nUniversity of Kurdistan, Iran\nREVIEWED BY\nUmar Ashraf,\nYunnan University, China\nGolale Asghari,\nUniversity of Kurdistan, Iran\n*CORRESPONDENCE\nArdiansyah Koeshidayatullah,\nkoeshidayatullah@kfupm.edu.sa\nMotaz Alfarraj,\nmotaz@kfupm.edu.sa\n†These authors have contributed equally\nto this work and shareﬁrst authorship\nSPECIALTY SECTION\nThis article was submitted to\nEnvironmental Informatics and Remote\nSensing,\na section of the journal\nFrontiers in Earth Science\nRECEIVED 12 July 2022\nACCEPTED 06 September 2022\nPUBLISHED 04 October 2022\nCITATION\nKoeshidayatullah A, Al-Azani S,\nBaraboshkin EE and Alfarraj M (2022),\nFaciesViT: Vision transformer for an\nimproved core lithofacies prediction.\nFront. Earth Sci.10:992442.\ndoi: 10.3389/feart.2022.992442\nCOPYRIGHT\n© 2022 Koeshidayatullah, Al-Azani,\nBaraboshkin and Alfarraj. This is an\nopen-access article distributed under\nthe terms of theCreative Commons\nAttribution License (CC BY). The use,\ndistribution or reproduction in other\nforums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which does\nnot comply with these terms.\nFrontiers in Earth Science frontiersin.org01\nTYPE Original Research\nPUBLISHED 04 October 2022\nDOI 10.3389/feart.2022.992442\n1 Introduction\nSince the seminal work of Krizhevsky et al. (2012) that\nused deep convolutional neu ral networks (CNN), known as\nAlexNet, to win the ImageNet challenge, the application of\nCNNs has been widely popular and has signi ﬁcantly\ntransformed the landscape of visual and pattern\nrecognition. Availability of big data and easy access to\nhigh-performance computing resources allows rapid\ndevelopment of deeper and wider CNN architectures to\nlearn more complex features in an image and produce a\nprediction accuracy that surpasses human accuracy ( LeCun\net al., 2015; He et al., 2016). CNNs are particularly powerful\nfor image analysis because CNNs are translational and scale\ninvariance through weight sharing and pooling, respectively.\nIn geosciences, various CNN algorithms have been explored\nand implemented to perform either supervised or\nunsupervised learning approach on multi-scale image\ndatasets and analyzes, including basin-scale seismic\ninterpretation ( Wrona et al., 2018 ; Alaudah et al., 2019 );\nWu et al., 2019 and micro-scale analysis of petrography or\ncomputed tomography scan datasets (Alqahtani et al., 2018;\nKoeshidayatullah et al., 2020; Ferreira et al., 2022).\nPrevious works have highlighted CNNs as a data-hungry\nmodel, and to achieve a high-performance result, a much deeper\nnetwork is required to increase the receptiveﬁelds and capture\nlong-range dependencies (LeCun et al., 2015; He et al., 2016).\nThis issue can be mitigated by the ability of CNNs to be trained in\na domain where a large volume of datasets is obtained more easily\nand transfer the knowledge to a more speciﬁc domain where data\nis difﬁcult to collect and expensive (Weiss et al., 2016). This\nmethod is referred to as transfer learning, and it provides a way to\noptimize machine learning performance when the dataset is\nlimited, such as in geosciences. Since then, most deep learning\nimplementations in geosciences, particularly for image analysis\ntasks, have relied primarily on transfer learning methods (Li\net al., 2017; de Lima et al., 2019; Baraboshkin et al., 2020; Wu\net al., 2020 ). Although this method allows for some\nbreakthroughs in geological image classi ﬁcation and\nrecognition, the model was originally trained on a domain\nthat inherently has different data features and distributions,\nbut can still produce a high-performance result that could\nraise some concerns in the long run ( Pires de Lima and\nDuarte, 2021; Koeshidayatullah, 2022). Furthermore, this is\ncompounded by the relatively stagnant performance and low\nexplainability of various CNN models, which created the urgency\nto develop a deeper and wider CNN model. In such a case, while\nthe performance of CNN models may improve, it comes with a\nsigniﬁcant trade-off, in which the models become\ncomputationally expensive and even more challenging to\ninterpret.\nMotivated by the success of the self-attention mechanism in\nnatural language processing tasks (Vaswani et al., 2017; Devlin\net al., 2018), the recent development of CNN models for vision\ntasks has also attempted to incorporate such self-attention\nmodules within CNN layers andhas been shown to improve\nthe overall image classiﬁcation performance (Cao et al., 2020).\nThe implementation of attention modules allows the model to\nfocus on speci ﬁc areas of the input image that has more\ninﬂuence on the prediction. However, one major drawback\nof using self-attention for the image data set is the quadratic\nc o m p l e x i t yo ft h es e q u e n c el e n g t h ,b e c a u s et h ei m a g ep i x e l\nneeds to be unrolled into long 1D sequences, and each pixel\nn e e d st oa t t e n dt oa l lo t h e rp i x e l s .T h e s ei s s u e sm a k e\ntransformer models computationally expensive and\ninefﬁcient for image analysis. Furthermore, the transformer\nmodel requires positional embedding to capture the correct\npixel sequence, which requires architectural changes. Recent\nwork proposed a new architecture, Vision Transformer (ViT),\nto mitigate these issues by splitting the image into 16x16-sized\npatches and treating these patches as tokens (Dosovitskiy et al.,\n2020). Furthermore, this model encodes positional embedding\nand class embedding to capture the spatial relationship between\npatches and learn the global relationship of the image,\nrespectively. This model achiev ed state-of-the-art results on\nthe ImageNet dataset, which inspires further works to develop\nViT architectures for various computer vision tasks (Chen C.-F.\net al., 2021 ; Liu et al., 2021 ). Despite these successful\napplications, the ViT model is well-known for requiring a\nlarge number of training datasets. Recent work has focused\ni t se f f o r t so ni m p r o v i n gt h eV i Tm o d e li no r d e rt om a k et h e\nmodel work with a small dataset.\nIn geosciences, most image classiﬁcation and segmentation\ntasks are dominated by CNNs, and the application of ViT is still\nsigniﬁcantly limited. To date, only a few studies have\nexperimented with ViT, primarily for remote sensing images\n(Bazi et al., 2021; Chen H. et al., 2021), and no studies have\nutilized the Vision Transformer model to perform lithofacies\nclassiﬁcation on borehole core images. Therefore, our study aims\nto explore how ViT learns and makes predictions on a geological\ndataset. Furthermore, we evaluate and validate (i) the\nperformance of ViT to classify the lithofacies of subsurface\ncore images; (ii) the ability of ViT to generalize the learning\nprocess and predict new, unseen datasets, and further compare\nthe prediction with conventional CNNs architecture (base and\npretrained models); and (iii) the limitation and potential of ViT\nfor classifying limited geological dataset. The successful\napplication of ViT has the potential to improve the\ngeneralizability and explainability of deep learning models in\ngeosciences.\n2 Core-based lithofacies analysis\nThe characterization of the subsurface reservoir involves\nmulti-scale and -dimensional analyzes, from one-dimensional\nFrontiers in Earth Science frontiersin.org02\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nborehole analysis (well log and core samples) to three-\ndimensional depositional and property modeling (Amel et al.,\n2015; Al-Ramadan et al., 2020; Anees et al., 2022b; Anees et al.,\n2022a). In such cases, the classiﬁcation and interpretation of\nlithofacies play a key role in providing the basic building block to\na more advanced interpretation. Lithofacies (facies)— is deﬁned\nas a body of rock with certain speciﬁed attributes that distinguish\nit from other rock units ( Leeder, 2012 ). In reservoir\ncharacterization, lithofacies classi ﬁcation is particularly\nimportant for identifying depositional processes and\ndelineating the depositional environment and quantifying the\nnet-to-gross ratio of reservoir facies ( Amel et al., 2015 ;\nKoeshidayatull ah et al., 2016 ; Ashraf et al., 2019 ).\nInformation related to lithofacies can be obtained from well\ndata (e.g., Gamma-Ray, Neutron-Density), seismic attribute\nanalysis, and cored intervals in a borehole. The latter is the\nonly dataset that provides the ground truth of lithofacies\nclassiﬁcation and interpretation. However, it is also the most\nexpensive and difﬁcult to obtain and characterize from the\nsubsurface; hence, its availability is relatively limited.\nConventional lithofacies classiﬁcation from core samples\nrelies on visual pattern analysis (e.g., grain size, depositional\ntexture, and structure analysis) ( Rothwell and Rack, 2006 ).\nSecondary physical and chemical analyzes, such as hardness,\nacid tests and other nondestructive geochemical techniques, can\nalso be performed to conﬁrm the mineralogy of the lithofacies\n(Croudace and Rothwell, 2015; McPhee et al., 2015; Amao et al.,\n2016). Although physical analysis cannot be analyzed in machine\nlearning, machine learning is an excellent tool for replicating\nvisual pattern analysis and performing more robust and unbiased\nclassiﬁcation (Thomas et al., 2011; Baraboshkin et al., 2018;\nMartin et al., 2021 ). Visual-based machine learning,\nspeciﬁcally deep neural networks, has been widely applied in\nthe past 5 years to perform lithological classiﬁcation (de Lima\net al., 2019; Baraboshkin et al., 2020; Koeshidayatullah et al.,\n2020). The primary motivation behind employing machine\nlearning for this task is a time-consuming, expensive, and\npotentially biased interpretation when performed by a human.\nTo date, it is clear that machine learning will never completely\nreplace humans or geologists in performing this task, but\nmachine learning has the potential to help optimize and\nstandardize the process. Such advancement will not only\nsigniﬁcantly reduce the cost associated with core description\nbut also improve the reproducibility of core analysis across\ngeologists.\nVarious models and approaches have been proposed to\nconduct core-based lithofacies analysis, from support vector\nmachine (SVM) to deep neural networks. A set of works that\naims to classify rocks by different types of features. A set of works\nextracted different color distributions and intensity (e.g., color\nhistogram, hue, saturation) from rock samples and used different\nalgorithms based on statistics (Singh et al., 2004; Harinie et al.,\n2012), SVM (Lobos et al., 2016; Patel et al., 2016; Patel et al.,\n2017), combination of statistics and machine learning (Prince\net al., 2005; Thomas et al., 2011; Seleznev et al., 2020), to perform\nlithology classiﬁcation. In addition, previous works applied\nLeNet (named CIFAR in the publication) and other\nconvolutional neural networks to classify granite tiles (Ferreira\nand Giraldi, 2017) and rock images (Zhang et al., 2017). Another\nwork shows the application of deep convolutional neural\nnetworks (CNN) to classify different lithologies directly from\ncore images (de Lima et al., 2019). Despite highlighting several\nlimitations of deep learning, this study shows a promising\npotential of CNN in optimizing core-based lithofacies analysis\nonly from digital core images. Several follow-up studies\ndemonstrated the power of CNN to perform core lithofacies\nclassiﬁcation by comparing different architectures and found that\nResNet architecture (He et al., 2016) outperforms other CNN\narchitectures, achieving up to 95% in analytical precision\n(Baraboshkin et al., 2018; Ivchenko et al., 2018; Baraboshkin\net al., 2020). Recent work byMartin et al., 2021shows a successful\ncase of coupling core facies and extracted color-channel\nlog(CCL) to predict centimeter-scale lithofacies. This study\nuses two different CNN, WaveNet ( Oord et al., 2016 ) and\nDeep TEN ( Zhang et al., 2017 ), to perform sequence-to-\nsequence learning in the CCL data and texture classiﬁcation\nin the core image dataset, respectively. The CNN currently\napplied for different rock types and analyses: igneous rocks\n(Fan et al., 2020; Fu et al., 2022), rock quality designation\n(Alzubaidi et al., 2021), and trace fossils detection (Ayranci\net al., 2021; Timmer et al., 2021). Furthermore, a recent work\nproposed the use of elemental data in addition to images to\nimprove the accuracy of classiﬁcation (Xu et al., 2021).\nA previous study explored how networks learn by extracting\ndifferent feature maps at different layers, indicating that\nnetworks learn rather differently than humans and, most of\nthe time, use unrelated features to predict lithofacies\n(Baraboshkin et al., 2020). Furthermore, this study shows that\nthe model could not achieve the same level of performance in a\nnew unseen core dataset with similar lithofacies, so the accuracy\ndropped to close to 50%. This raises a serious concern about the\nexplainability and generalizability of CNN models and how\nmuch we can trust the model. While another study\nhighlighted how probability averaging may improve the\nresults of classi ﬁcation ( Alzubaidi et al., 2021 ), the class\nimbalance problem, which is very typical in the geosciences\ndataset, could have a detrimental impact on the overall\nperformance of the deep learning model ( Koeshidayatullah\net al., 2020 ; Koeshidayatullah, 2022 ). This is further\ncompounded by the fact that most of these studies rely\nheavily on the transfer learning method and intensive data\naugmentation to perform training (Baraboshkin et al., 2020).\nTherefore, there is an urgent need to explore another deep\nlearning method, such as Vision Transformer, to analyze\ngeological image datasets and improve the explainability of\ndeep learning.\nFrontiers in Earth Science frontiersin.org03\nKoeshidayatullah et al. 10.3389/feart.2022.992442\n3 Methodology\nThis section presents the methodology designed and followed\nto evaluate ResNet, ViT, and the hybrid structures of ResNet and\nViT to classify Core Lithofacies.\n3.1 Dataset collection and preparation\nTable 1 describes the datasets used in this study collected by\nBaraboshkin et al. (2020). 10% of the training set are used for\nvalidation. Theﬁrst dataset was collected from different wells placed\nin Russia, it includes various fo rmations: Bazhenov, Abalak\n(Vasuganskaya and Georgievskaya), Vikulovskaya, and Domanik.\nThe second dataset is collected from the Achimov formation. The\ndataset was collected as a photo of core boxes from the RFGF\n(uniﬁed fund of subsurface geological information) website and\nautomatically cropped out from the photo to separate different core\nbox columns. The columns were then sliced into 10 x 10 cm images.\nEach image is resized to 256 × 256 pixels and normalized to a range\nof pixel intensities from 0 to 1.\n3.2 Augmentation\nThe augmentation technique is a powerful tool for increasing\nthe generalization performance of models by minimizing overﬁtting.\nIt can be carried out online or ofﬂine. Online augmentation, also\nknown as real-time augmentation, is performed during the training\nphase, whereas ofﬂine augmentation isﬁrst implemented on the\ndataset before training, then the augmented dataset is used for\ntraining the model. It is noteworthy that both online and ofﬂine\naugmentations have their beneﬁts and drawbacks. For example, the\nmain disadvantages of of ﬂine augmentation include space\ncomplexity and impleme ntation complexity ( Shorten and\nKhoshgoftaar, 2019). On the other hand, a main drawbacks of\nonline augmentation is that the original samples of the dataset might\nnot be kept during the training. Another issue with online\naugmentation compared with ofﬂine augmentation is that the\nsize of the dataset is as the same as the size of the original dataset.\nIn this study, online augmentation is taken into account and\nimplemented on the input dataset using TorchVision\nTransforms in Pytorch. The considered image augmentation\nincludes cropping, rotation,ﬂipping, color jitter, and Gaussian\nblur. They are implemented by random selection. To alleviate the\naforementioned drawbacks of online data augmentation, we\ncreated three data loaders and combined them. Theﬁrst is for\nthe original data (to ensure the original samples are included in\nthe training) and the other ones for the considered augmentation\ntypes with different orders and parameters.\n Random crop: Each input image isﬁrst resized to 256 × 256\npixels. Then random subset with 224 × 224 pixels from the\noriginal image is created.\n Rotation: Images are rotated at 90 and 180\n° for theﬁrst and\nthe second image augmenters.\n Flipping: horizontal and verticalﬂips are considered with a\nprobability of 0.5 for the ﬁrst and second image\naugmenters.\n Color jitter: this type of augmentation is to randomly\nchange the contrast, brightness, saturation, and hue of\nan image. The brightness factor of 0.5 is considered to add a\nrandom brightness jitter to images for one data loader.\nHowever, the brightness for the other data augmenter is\nnot changed. The saturation factor is chosen uniformly\nfrom 0 to one to adjust the amount of jitter in saturation.\nHue factor of 0.3 is also adopted to add random shade to\nthe images.\n Blur: using Gaussianﬁlter to blur an image. For theﬁrst\ndata augmenter the kernel width and height size of 21 is\nadapted, whereas a kernel size ofﬁve is used for the second\nimage augmenter.\nTABLE 1 Datasets description.\nClass Dataset I Dataset II\nTraining Testing All samples Testing\nArgillaceous 517 104 621 37\nGranite 550 111 661 0\nlimestone 263 53 316 0\nSandstone-Laminated 534 108 642 217\nSandstone-Massive 561 114 675 60\nSiltstone 542 110 652 126\nTotal 2967 600 3567 440\nFrontiers in Earth Science frontiersin.org04\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nFigure 1 depicts examples of the developed data loaders.\n3.3 Training andﬁne-tuning\nOptimization is the process of adjusting model parameters to\nreduce model error in each training step. Stochastic gradient\ndescent (SGD) optimizer is used with Momentum (Qian, 1999)\nvalue of 0.9 in order to help accelerate SGD in the relevant\ndirection and dampens oscillations.\nThe learning rate warm-up (Goyal et al., 2017) approach is\nadopted at the beginning of the training steps by which the\nlearning rate increases linearly from zero to the initial learning\nrate during the deﬁned steps. Then, for the remaining steps the\nlearning rate goes down from the initial learning rate to zero\nfollowing cosine function/curve.\nDropout as a regularization technique is a signi ﬁcant\nmethod to prevent over ﬁtting. It deactivates randomly\nchosen neurons during training phase by assigning zero\nvalues for the selected neurons. Dropout cannot only be\napplied for the neuron level but also on the path level\n(Drop-path) in order to prevent co-adapting different\ndepths of sub-networks.\nWeight decay is also implemented to help prevent overﬁtting\nand the exploding gradient problem issues by adding a penalty\nterm to the cost function. In this study, L2 penalty is applied\nwhich leads to shrink the model weights. Cross-entropy loss is\napplied to adjust the weights during the training phase to\nminimize the loss value.\nLoss /equals− ∑\nn\ni/equals 1\ntilog pi() (1)\nwhere n is the number of classes, in this studyn =6 ,ti is the truth\nlabel and pi is the Softmax probability for classi.\nThe transfer learning paradigm is utilized here for both\nCNN-based and ViT-based architectures due to the small size\nof the training dataset. Networks were pre-trained using\nImageNet Dataset ( Deng et al., 2009 ) and the generated\nweights are ﬁne-tuned and the networks were re-trained for\nthe classiﬁcation task of this study.\n3.4 Evaluation measures\nTo evaluate the proposed models, the confusion matrix,\nprecision, precision, recal, andF1 are considered. A confusion\nmatrix is anN × N table whereN is the number of classes in the\ndataset. It is a good method to evaluate the performance of\nclassiﬁcation models especially, for highly imbalanced datasets. It\nis composed of four main evaluation measures namely: True\nPositive (TF), True Negative (TN), False Positive (FP) and False\nNegative (FN). Several different evaluation measures can be\nformulated from those four measures, including Accuracy,\nPrecision, Recall andF\n1.\nFIGURE 1\nExamples of(A) original data,(B) image augmenter I and(C) image augmenter II.\nFrontiers in Earth Science frontiersin.org05\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nAccuracy /equals number of instances classified correctly\ntotal number of instances\n/equals TP + TN\nTP + TN + FP + FN (2)\nPrecision /equals TP\nTP + FP (3)\nRecal /equals TP\nTP + FN (4)\nF1 /equals 2× Precision × Recal\nPrecision + Recal (5)\nWeighted average (WA) is considered due to the\nimbalanced nature of the datasets especially Dataset II ( El-\nAlfy and Al-Azani, 2020).\nWAPrecision /equals ∑iPrecisionipcounti\n∑icounti\n(6)\nWARecal /equals ∑iRecalipcounti\n∑icounti\n(7)\nWAF1 /equals 2× WAPrecision × WARecal\nWAPrecision + WARecal (8)\nTABLE 2 Description and variations of ViT (Dosovitskiy et al., 2020) models.\nLayers Hidden size dim Heads MLP dim Params\nViT-Base (ViT-B) 12 768 12 3027 ~86M\nViT-Large (Vit-L) 24 1024 16 4096 ~307M\nViT-Huge (ViT-H) 32 1280 16 5120 ~632M\nFIGURE 2\nAn overview of Vision Transformer architecture:(A) FaciesViT, (B) the hybrid ResNet and ViT architecture, and(C) the encoder block. The output\nlayer is represented by Classes from C1 to C6 where C1: Argil, C2: Granit, C3: limestone, C4: Standstone lam, C5: Standstone mas, and C6: Siltstone.\nSince the input image belongs to limestone, this is highlighted in the output layer.\nFrontiers in Earth Science frontiersin.org06\nKoeshidayatullah et al. 10.3389/feart.2022.992442\n3.5 CNN architecture\nResidual-based Convolutional Neural Network (ResNet) (He\net al., 2016) is considered as our baseline in this study due to its\nscalability while achieving satisfying results in image\nclassiﬁcation and object detection comparing with other\nCNN-based architectures. ResNet has different structures. In\nthis study we consider ResNet50 and ResNet101 structures.\nBoth ResNet50 and ResNet101 are trained either from scratch\nor by utilizing the pretraining approach. Therefore, four\nnetworks are evaluated as the baseline of this study, namely\nBasic ResNet50 (B-ResNet50), Basic ResNet101 (B-ResNet101),\nPretrained ResNet50 (P-ResNet50) and Pretrained ResNet101\n(P-ResNet101). Those models are implemented using Pytorch\nframework.\n3.6 Vision transformers\n3.6.1 ViT-base (ViT-B)\nThree main models are presented and developed by\nDosovitskiy et al. (2020) described in Table 2. Due to the\nsmall sizes of the datasets used in this study, the ViT-B\nstructure is selected to be evaluated among others.\nTABLE 3 Parameters considered for training models.\nParameter Batch size Learning rate Momentum Weight decay Decay type #Epochs Learning\nrate warmup (K)\nValue 64 1e-02 0.9 1e-04 Cosine 100 1\nTABLE 4 Classiﬁcation reports for CNN-based models for Dataset I and Dataset II.\nDataset I\nB-ResNet50 B-ResNet101 P-ResNet50 P-ResNet101 support\nprc recal f1 prc recall f1 prc recall f1 prc recal f1\nargil 0.817 0.856 0.836 0.795 0.894 0.842 0.942 0.942 0.942 0.917 0.962 0.939 104\ngranite 0.973 0.973 0.973 0.973 0.982 0.978 0.982 0.973 0.977 1.000 0.973 0.986 111\nlimestone 0.839 0.887 0.862 0.906 0.906 0.906 0.906 0.906 0.906 0.925 0.925 0.925 53\nsandstone_lam 0.759 0.759 0.759 0.784 0.806 0.795 0.808 0.898 0.851 0.890 0.824 0.856 108\nsandstone_mas 0.863 0.772 0.815 0.860 0.860 0.860 0.899 0.860 0.879 0.861 0.868 0.865 114\nsiltstone 0.728 0.755 0.741 0.817 0.691 0.749 0.885 0.836 0.860 0.809 0.845 0.827 110\nAccuracy 0.828 0.852 0.902 0.897\nWeighted Avg 0.830 0.828 0.828 0.852 0.852 0.850 0.903 0.902 0.902 0.898 0.897 0.897 600\nDataset II\nargil 0.442 0.622 0.517 0.299 0.541 0.385 0.647 0.297 0.407 0.364 0.216 0.271 37\ngranite 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0\nlimestone 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0\nsandstone_lam 0.939 0.355 0.515 0.927 0.410 0.569 0.841 0.512 0.636 0.845 0.502 0.630 217\nsandstone_mas 0.154 0.167 0.160 0.067 0.067 0.067 0.241 0.333 0.280 0.200 0.267 0.229 60\nsiltstone 0.652 0.683 0.667 0.707 0.651 0.678 0.678 0.651 0.664 0.756 0.492 0.596 126\nAccuracy 0.445 0.443 0.509 0.443\nWeighted Avg 0.708 0.445 0.510 0.694 0.443 0.516 0.696 0.509 0.576 0.691 0.443 0.535 440\nFrontiers in Earth Science frontiersin.org07\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nAs depicted inFigure 2A, an input image with a resolution of\n244 × 224 × 3ﬁrst is divided into 16 patches and input into the\npatch embed block to generate a sequence of 2D patches. The\nEmbedding Block is composed of a convolution layer of 16 × 16\nand aﬂatten layer. The output of theﬂatten layer (196 × 768) is\nthen contacted with a class token with a size of 1 × 768 and added\nto the training parameter, Position Embedding, to retain\npositional information, which is followed by the Dropout\nLayer. The resulting sequences of embedding vectors are fed\ninto the Encoder Block which is composed of alternating layers of\nmulti-headed self-attention depicted inFigure 2C. The outputs of\nthe Encoder Block is fed into the Layer Norm. The output related\nto the class token (1 × 768) is then extracted and fed into MLP\nHead block.\n3.6.2 ResNet50-ViT\nInstead of feeding image patches into the transformer, feature\nmaps generated using CNN can be fed into the transformers\n(Dosovitskiy et al., 2020). A hybrid model of CNN, especially\nResNet50 and vision transformer is also evaluated in this study\n(ResNet50-ViT). The ResNet50 network serves as a feature\nextractor. Therefore, the input to the vision transformer is the\nfeatures maps generated using ResNet instead of feeding the\nimage patches in the previous vision transformer model.Figures\n2B,C depict the structure of the hybrid model.\n3.7 Experimental setup\nOur experiments are implemented on a Lambda Workstation\nof AMD® Ryzen threadripper pro 3975wx 32-cores × 64 and\nthree NVIDIA GeForce RTX 3090 GPUs with a graphic memory\nof 24 GB for each. Anaconda 4.13.0 (2022-05-19) is conﬁgured\non Ubuntu 20.04.4 LTS and Pytorch framework is used (Paszke\net al., 2019). In addition, Scikit learn package (Pedregosa et al.,\n2011) is used to report the results.Table 3presents the parameters\nconsidered for training and evaluating the models.\n4 Results\n4.1 Dataset I\n4.1.1 Convolutional neural networks\nIn this study, we evaluated and compared two different\nresidual-based CNN architectures (ResNet), ResNet50 and\nFIGURE 3\nConfusion matrices for Dataset I(A) B-ResNet50 (B)B-ResNet101 (C)P-ResNet50 (D)P-ResNet101 (E) FaciesViT (F) P-R50-ViT.\nFrontiers in Earth Science frontiersin.org08\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nResNet101 to train and perform image classiﬁcation on theﬁrst\ncore image dataset. In theﬁrst experiment, we train the CNNs\nfrom scratch following the learning parameters established\nin Table 3. Overall, ResNet101, which has more layers, skip\nconnections, and deeper networks, perform better than\nResNet50 achieving up to 86% in all classi ﬁcation metrics,\nparticularly the weighted average of f1 score, precision, and\naccuracy (Table 4). In contrast, RestNet50 only achieved 83%\nin both the f1 score, precision, and weighted average across the\nmetrics (Table 4). In the second experiment, we pre-trained the\nResNet models with the ImageNet dataset and with learning\nparameters similar to those in the previous experiment. All\nclassiﬁcation metrics in both models have improved\nsigniﬁcantly yielding up to 90% in the weighted average of\nall metrics and accuracy, an increase of 8.5% from the\nﬁrst experiment (Table 4). Unlike the ﬁrst experiment, the\nResNet50 performs slightly better than the ResNet101 during\nthe second experiment (Table 4).\nThe result table and the confusion matrix further show\nthat the baseline ResNet50 model particularly struggled to\nclassify laminated sandstone, massive sandstone, and siltstone\n(Figure 3 and Table 4). The model is often confused between\nlaminated sandstone and siltstone. In contrast, the baseline\nResNet101 model faces some difﬁculties to differentiate between\nargillite and siltstone (Figure 3). However, this mistake is rather\nconsistent with a typical human error as argillite and siltstone\ncan have very similar appearances. Although pre-trained\nmodels achieved much improved results, the models still exhibit\nissues similar to baseline models, particularly in distinguishing\nbetween sandstones and siltstone (Figure 3).\n4.1.2 Vision transformer\nFor Dataset I, two Vision Transformer-based architectures,\nmentioned above, were evaluated. This was conducted to have a\nbetter insight into the actual performance of ViT in the\ngeosciences dataset. Both models were pre-trained with the\nTABLE 5 Classiﬁcation reports for ViT-based models for Dataset I and Dataset II.\nDataset I\nP-R50-ViT FaciesViT support\nprc recal f1 prc recal f1\nArgil 0.936 0.990 0.963 0.990 0.990 0.990 104\nGranite 0.991 1.000 0.996 1.000 0.991 0.995 111\nlimestone 0.962 0.962 0.962 1.000 0.981 0.990 53\nsandstone_lam 0.881 0.889 0.885 0.856 0.935 0.894 108\nsandstone_mas 0.929 0.921 0.925 0.944 0.886 0.914 114\nSiltstone 0.932 0.873 0.901 0.945 0.936 0.941 110\nAccuracy 0.937 0.950\nWeighted Avg 0.937 0.937 0.936 0.952 0.950 0.950 600\nDataset II\nArgil 0.700 0.378 0.491 0.429 0.162 0.235 37\ngranite 0.000 0.000 0.000 0.000 0.000 0.000 0\nlimestone 0.000 0.000 0.000 0.000 0.000 0.000 0\nsandstone_lam 0.870 0.585 0.700 0.797 0.705 0.748 217\nsandstone_mas 0.267 0.333 0.296 0.447 0.567 0.500 60\nsiltstone 0.769 0.556 0.645 0.697 0.659 0.678 126\nAccuracy 0.525 0.627\nWeighted Avg 0.745 0.525 0.612 0.690 0.627 0.651 440\nFrontiers in Earth Science frontiersin.org09\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nImageNet dataset and with hyperparameters similar to CNN\nmodels (Table 3). In general, both ViT-based models signiﬁcantly\noutperform all CNN architectures, with an increase of up to 15%\nin all classiﬁcation metrics (Table 5). Our proposed ViT achieved\n95% in both the weighted average of the f1 score and other\nmetrics (Table 5), while the hybrid model shows a slightly lower\nperformance, achieving around 93% in both the weighted average\nof the f1 score and other metrics in the test data set (Table 5).\nFurthermore, the FaciesViT model shows the most stable\nperformance across different classes and classiﬁcation metrics\n(Figure 4 and Table 5).\nThe confusion matrix shows that the hybrid CNN-ViT model\nhas almost correctly predicted all classes, except between laminated\nand massive sandstones (Figure 3). Although this issue may\nnot occur for geologists, it can be understood when looking at\nthe training and test datasets of these two classes as they can\nsometimes appear similar (Figure 1). The FaciesViT model seems\nto ﬁx this problem and can achieve 95% accuracy (Figure 3).\nHowever, the model still struggled slightly to differentiate between\nargillite and siltstone (Figure 3).\n4.2 Dataset II\nIn this study, we examined the generalizability of all models\nby performing a blind test on a new, unseen core dataset (Dataset\nII). This core dataset is collected from a different geological basin\nbut has almost all the rock types as in Dataset I, except granite\nand limestone. Across the different CNN models, baseline and\npre-trained, both the accuracy and the weighted average of the\nf1 score have decreased signi ﬁcantly, only 44 and 57%,\nFIGURE 4\nConfusion matrices for Dataset II(A) B-ResNet50, (B)B-ResNet101 (C)P-ResNet50 (D)P-ResNet101 (E) FaciesViT (F) P-R50-ViT.\nTABLE 6 Summary of results. The best results are presented in bold.\nDataset Model Precision Recall f1 Acc\nDataset I B-ResNet50 0.8297 0.8283 0.8284 0.8283\nB-ResNet101 0.8521 0.8517 0.8504 0.8517\nP-ResNet50 0.9035 0.9017 0.9019 0.9017\nP-ResNet101 0.8977 0.8967 0.8968 0.8967\nFaciesViT 0.9517 0.9500 0.9503 0.9500\nP-R50-ViT 0.9366 0.9367 0.9363 0.9367\nDataset II B-ResNet50 0.7079 0.4455 0.5102 0.4455\nB-ResNet101 0.6938 0.4432 0.5160 0.4432\nP-ResNet50 0.6961 0.5091 0.5763 0.5091\nP-ResNet101 0.6911 0.4432 0.5354 0.4432\nFaciesViT 0.6898 0.6273 0.6510 0.6273\nP-R50+ViT 0.7445 0.525 0.6116 0.5250\nFrontiers in Earth Science frontiersin.org10\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nrespectively (Table 4). While the recall values show similar scores\nacross CNN architectures, precision could produce around 70%\nin Dataset II (Table 4) with the baseline ResNet achieving the\nhighest value among all CNN-based models. The model most\ncorrectly predicted siltstone and laminated sandstone but at the\nsame time struggles to classify the rest of the rock types and even\nmade a classi ﬁcation error between laminated and massive\nsandstones (Figure 4).\nAlthough the ViT models also experienced a similar\nperformance decrease in this dataset. Both ViT-based\nmodels still outperform CNN models, achieving up to 65%\nin both f1-score and accuracy (Table 5). Furthermore, the base\nViT model performs slightly better than the hybrid ViT-CNN\nmodel (Tables 5and 6). Compared to all models, the weighted\naverage precision score of the hybrid model yields the highest\nvalue, reaching 75% (Table 5). However, its performance is\nnot stable and only achieved 52% in the weighted average\nrecall score. The results of our experiment with Dataset II\nfurther show that our proposed FaciesViT model exhibits the\nmost consistent and stable pe rformance across all the\nevaluation metrics. Similarly, both transformer-based models\nshow high accuracy when classifying laminated sandstone and\nsiltstone (Figure 4and Table 5). The confusion matrix shows that\nthe proposed FaciesViT model can generalize better and predict all\ntypes of rock more equally than the other CNN models (Figure 4).\n5 Discussion\n5.1 Vision transformer for core lithofacies\nclassiﬁcation\nApplications of deep learning to automate core lithofacies\nclassi ﬁcation have reached state-of-the-art results, matching\ngeologist-level classi ﬁcation for core interpretation (de Lima\net al., 2019; Baraboshkin et al., 2020 ; Falivene et al., 2022).\nNow more than ever, the need to automate subsurface\ngeological interpretation has peaked due to rapid digital\ntransformation and streamlined data transfer. However,\nt h et r u ep o t e n t i a lo fd e e pl e a r n i n gf o rc o r ei m a g e\nclassi ﬁcation remains under-explored due to limited data\navailability and a high-quality labeled dataset. This is\nfurther compounded by the narrow focus of applying\nConvolutional Neural Network s and supervised learning to\nperform this task. Recent developments of a deep learning\nalgorithm for computer vision have introduced the\nimplementation of a transformer-based algorithm, known\nas Vision Transformer (ViT; Dosovitskiy et al., 2020 ). This\nalgorithm differs from CNN because it focuses on the\nsequence of images and patches rather than individual\npixels. Furthermore, this model bene ﬁts from a multi-head\nself-attention mechanism to learn the importance of features\nfor the classi ﬁcation and optimize the classi ﬁcation with a\nlimited dataset. Recent work indicates that Vision\nTransformer has outperformed CNN in many computer\nvision tasks, including image classi ﬁcation, image\nsuperresolution, and segmentation ( Liu et al., 2021 ; Xie\net al., 2021 ). However, the applications of ViT are very\nlimited in geosciences and further exploration is required\nto fully uncover the power and applicability of the\ntransformer-based algorithm to conduct visual recognition\nin geosciences.\nIn this study, for theﬁrst time, we proposed and developed\na novel transformer-based framework to perform fully\nautomated core lithofacies classi ﬁcation using datasets that\nhave been studied by Baraboshkin et al. (2020).T oe v a l u a t e\nand validate our proposed model, we conducted several\nexperiments using recent CNN architectures (ResNet50 and\nResNet101) and a hybrid CNN-ViT model. Our results show\nthat the proposed FaciesViT model is much superior to the\nCNN and hybrid CNN-ViT models, achieving a weighted\naverage f1 score 15% higher than all CNN architectures\n(Table 6 ). In addition, we further tested the trained ViT\nmodel (train to Dataset I) to perform classi ﬁcation on the\nentirely new and unseen dataset (Dataset II). A similar\nexperiment was previously p erformed and showed that\ntheir best CNN model that achieved > 90% could not\ngeneralize the information to the new dataset and only\nachieved a score of less than 50% f1-score even after\nextensive data augmentation ( Baraboshkin et al., 2020 ). In\nour study, we showed that the proposed ViT model could\ngeneralize better when tested on the unseen data set compared\nt oo t h e rC N Na l g o r i t h m s .T h i sp h e n o m e n o ns u g g e s t st h a tV i T\nlearns better than CNN and could transfer knowledge from\nt h ed a t a s e tw h e r ei ti st r a i n e dt ot h eo u t - o f - d i s t r i b u t i o n\ndataset. This is evident through the visualization of the\nlayers using the attention rollout to show what features are\nimportant for the classiﬁcation (Figure 5) and most are similar\nto the characteristics used by geologists to identify the\ndifferent types of rocks. Among the different methods, the\nmean attention rollout provides the most information on how\nthe FaciesViT model informed its decision (Figure 5). This can\nbe justiﬁed as the FaciesViT works better with the original\nimages than with the feature maps extracted using CNN which\nis a major advantage of using ViT because the ViT model can\noutperform CNN models without requiring preprocessing\nsteps and feature extraction processes. This provides a\nmajor advance towards developing a general deep learning\nmodel for image classi ﬁcation using transformed-based\narchitecture. As reported in previous work, ViT has the\npotential to replace CNN in performing various computer\nvision tasks because it provides (i) a more efﬁcient and robust\nmodel; (ii) a model with higher generalizability, and (iii) a\nmore explainable deep learning model.\nFrontiers in Earth Science frontiersin.org11\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nFIGURE 5\nVisualizations of learned features within the transformer layers using attention rollout in different rock types.\nFrontiers in Earth Science frontiersin.org12\nKoeshidayatullah et al. 10.3389/feart.2022.992442\n5.2 Future recommendation and limitation\nThis study is designed as a proof-of-concept on the\napplication of ViT for core lithofacies classi ﬁcation and\nexplores how it improved the overall performance of a deep\nlearning algorithm for such a task when compared with\ntraditional CNN algorithms. Hence, we also acknowledged\nseveral limitations of this work, including: (i) our study uses a\nfairly limited dataset( < 10k) and an imbalance dataset which\nmay have a negative impact on the learning and prediction\nprocesses. Therefore, an additional dataset is required to\nunravel the actual potential of ViT; and ((ii) we only\nexamined the six most common lithofacies in the subsurface\nreservoir and uses a general category to group them. A typical\nreservoir characterization would require a more detailed analysis\nand grouping. For example, limestone can be further divided into\nat leastﬁve lithofacies, such as mudstone, wackestone, packstone,\ngrainstone, and boundstone which will hold more information\nthan just using limestone as a category. Although we have tested\nour model to predict an unseen dataset from different geological\nsettings to test the transferability of our model, future works\nshould consider using more detailed lithofacies types to better\nrepresent actual subsurface reservoir conditions and\ncomplexities. In addition, ViT-based models require larger\ntraining dataset and more computational recourses compared\nto CNN-based models due to the complexity of the used attention\nmechanism. We recommend using more advanced ViTs\novercoming such limitations. Furthermore, a more advanced\ndata augmentation (e.g., label smoothing, CutMix;\nKoeshidayatullah, 2022) may further improve the performance\nof the model and optimize the training processes.\n6 Conclusion\n This study, for theﬁrst time, utilized a transformer-based\narchitecture FaciesViT, to perform lithofacies classiﬁcation\ndirectly from core images.\n Our proposed model can match the performance of other\nCNN models without heavy data augmentation.\nFurthermore, the model can generalize better for the\nunseen dataset, which provides a signi ﬁcant step\nforward in the application of deep learning to lithofacies\ninterpretation.\n The attention rollout technique shows that the\nalgorithm bases its classi ﬁcation on features used by\ngeologists to differentiate and classify lithofacies. This\nimproves the overall explainability and transferability of\nthe model.\n Although the model can predict an unseen and out-of-\ndistribution dataset with an accuracy of up to 65%, a more\ndiverse and larger volume of dataset would help the\nprediction of our model to other datasets.\nData availability statement\nThe raw data is available upon a reasonable request to the\ncorresponding authors.\nAuthor contributions\nAK: Conceptualization, Writing — original draft, Data\ncuration, Writing — review and editing, equal contribution.\nSA-A: Conceptualization, Methodology, Writing — original\ndraft, Writing — review and editing, equal contribution. EB:\nData Curation, Writing — review and editing. MA:\nConceptualization, Methodology, Writing — review and\nediting.\nAcknowledgments\nThe authors would like to thank King Fahd University of\nPetroleum and Minerals (KFUPM), Dhahran, Saudi Arabia for\nall its support. AK would like to thank the support from the CPG\nstartup fund (SF-21011) for the resources and facilities to\nconduct this study. The authors would also like to thank\nSDAIA-KFUPM Joint Research Center for Arti ﬁcial\nIntelligence (JRC-AI) for the support and computing\nresources. We acknowledged the constructive discussions with\nDmitry Koroteev and Denis Orlov from Skoltech, Russia in the\npreparation of the dataset and manuscript.\nConﬂict of interest\nEB is employed by Digital Petroleum LLC, Moscow, Russia.\nThe remaining authors declare that the research was conducted\nin the absence of any commercial orﬁnancial relationships that\ncould be construed as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their afﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nFrontiers in Earth Science frontiersin.org13\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nReferences\nAl-Ramadan, K., Koeshidayatullah, A., Cantrell, D., and Swart, P. K. (2020).\nImpact of basin architecture on diagenesis and dolomitization in a fault-bounded\ncarbonate platform: Outcrop analogue of a pre-salt carbonate reservoir, red sea rift,\nnw Saudi Arabia.Pet. Geosci. 26, 448–461. doi:10.1144/petgeo2018-125\nAlaudah, Y., Michałowicz, P., Alfarraj, M., and AlRegib, G. (2019). A machine-\nlearning benchmark for facies classiﬁcation. Interpretation 7, SE175–SE187. doi:10.\n1190/int-2018-0249.1\nAlqahtani, N., Armstrong, R. T., and Mostaghimi, P. (2018).“Deep learning\nconvolutional neural networks to predict porous media properties,” in SPE Asia\nPaciﬁc oil and gas conference and exhibition. (OnePetro). Brisbane, Australia: SPE.\nAlzubaidi, F., Mostaghimi, P., Swietojanski, P., Clark, S. R., and Armstrong, R. T.\n(2021). Automated lithology classi ﬁcation from drill core images using\nconvolutional neural networks.J. Petroleum Sci. Eng.197, 107933. doi:10.1016/j.\npetrol.2020.107933\nAmao, A. O., Al-Ramadan, K., and Koeshidayatullah, A. (2016). Automated\nmineralogical methodology to study carbonate grain microstructure: An example\nfrom oncoids. Environ. Earth Sci.75, 666. doi:10.1007/s12665-016-5492-x\nAmel, H., Jafarian, A., Husinec, A., Koeshidayatullah, A., and Swennen, R. (2015).\nMicrofacies, depositional environment and diagenetic evolution controls on the\nreservoir quality of the permian upper dalan formation, kish gasﬁeld, zagros basin.\nMar. Petroleum Geol.67, 57–71. doi:10.1016/j.marpetgeo.2015.04.012\nAnees, A., Zhang, H., Ashraf, U., Wang, R., Liu, K., Mangi, H., et al. (2022b).\nIdentiﬁcation of favorable zones of gas accumulation via fault distribution and\nsedimentary facies: Insights from hangjinqi area, northern ordos basin.Front. Earth\nSci. (Lausanne). 9, 822670. doi:10.3389/feart.2021.822670\nAnees, A., Zhang, H., Ashraf, U., Wang, R., Liu, K., Abbas, A., et al. (2022a).\nSedimentary facies controls for reservoir quality prediction of lower shihezi\nmember-1 of the hangjinqi area, ordos basin. Minerals 12, 126. doi:10.3390/\nmin12020126\nAshraf, U., Zhu, P., Yasin, Q., Anees, A., Imraz, M., Mangi, H. N., et al. (2019).\nClassiﬁcation of reservoir facies using well log and 3d seismic attributes for prospect\nevaluation and ﬁeld development: A case study of sawan gas ﬁeld, Pakistan.\nJ. Petroleum Sci. Eng.175, 338–351. doi:10.1016/j.petrol.2018.12.060\nAyranci, K., Yildirim, I. E., Waheed, U. b., and MacEachern, J. A. (2021). Deep\nlearning applications in geosciences: Insights into ichnological analysis.Appl. Sci.\n11, 7736. doi:10.3390/app11167736\nBaraboshkin, E. E., Ismailova, L. S., Orlov, D. M., Zhukovskaya, E. A., Kalmykov,\nG. A., Khotylev, O. V., et al. (2020). Deep convolutions for in-depth automated rock\ntyping. Comput. Geosciences 135, 104330. doi:10.1016/j.cageo.2019.104330\nBaraboshkin, E., Ivchenko, A., Ismailova, L., Orlov, D., Baraboshkin, E. Y., and\nKoroteev, D. (2018). “Core photos lithological interpretation using neural\nnetworks,” in 20th international sedimentological congress. Quebec, Canada: IAS.\nBazi, Y., Bashmal, L., Rahhal, M. M. A., Dayil, R. A., and Ajlan, N. A. (2021).\nVision transformers for remote sensing image classiﬁcation. Remote Sens.13, 516.\ndoi:10.3390/rs13030516\nCao, R., Fang, L., Lu, T., and He, N. (2020). Self-attention-based deep feature\nfusion for remote sensing scene classiﬁcation. IEEE Geosci. Remote Sens. Lett.18,\n43–47. doi:10.1109/lgrs.2020.2968550\nChen, C.-F. R., Fan, Q., and Panda, R. (2021).“Crossvit: Cross-attention multi-\nscale vision transformer for image classiﬁcation,” in Proceedings of the IEEE/CVF\ninternational conference on computer vision(Montreal, Canada: IEEE), 357–366.\nC h e n ,H . ,Q i ,Z . ,a n dS h i ,Z .( 2 0 2 1 ) .R e m o t es e n s i n gi m a g ec h a n g ed e t e c t i o nw i t h\ntransformers.IEEE Trans. Geosci. Remote Sens.60, 1–14. doi:10.1109/tgrs.2021.3095166\nCroudace, I. W., and Rothwell, R. G. (2015).Micro-XRF studies of sediment cores:\nApplications of a non-destructive tool for the environmental sciences, 17. Berlin,\nGermany: Springer.\nde Lima, R. P., Suriamin, F., Marfurt, K. J., and Pranter, M. J. (2019).\nConvolutional neural networks as aid in core lithofacies classi ﬁcation.\nInterpretation 7, SF27–SF40. doi:10.1190/int-2018-0245.1\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009).“Imagenet: A\nlarge-scale hierarchical image database,” in 2009 IEEE conference on computer\nvision and pattern recognition(FL, United States: IEEE), 248–255.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2018). Bert: Pre-training of deep\nbidirectional transformers for language understanding. New York, USA.arXiv[Preprint].\ndoi:10.48550/arXiv.1810.04805\nDosovitskiy, A., Beyer, L., Kolesnikov, A.,Weissenborn, D., Zhai, X., Unterthiner, T.,\net al. (2020). An image is worth 16x16 words: Transformers for image recognition at scale.\nNew York, USA.arXiv [Preprint]. doi:10.48550/arXiv.2010.11929\nEl-Alfy, E.-S. M., and Al-Azani, S. (2020). Empirical study on imbalanced\nlearning of Arabic sentiment polarity with neural word embedding.J. Intelligent\nFuzzy Syst. 38, 6211–6222. doi:10.3233/jifs-179703\nFalivene, O., Auchter, N. C., Pires de Lima, R., Kleipool, L., Solum, J. G.,\nZarian, P., et al. (2022). Lithofacies identiﬁcation in cores using deep learning\nsegmentation and the role of geoscientists: Turbidite deposits (gulf of Mexico\nand north sea). A m .A s s o c .P e t .G e o l .B u l l .106, 1357 –1372. doi:10.1306/\n03112221015\nFan, G., Chen, F., Chen, D., and Dong, Y. (2020). Recognizing multiple types of\nrocks quickly and accurately based on lightweight cnns model.IEEE Access 8,\n55269–55278. doi:10.1109/access.2020.2982017\nFerreira, A., and Giraldi, G. (2017). Convolutional neural network approaches to\ngranite tiles classiﬁcation. Expert Syst. Appl. 84, 1–11. doi:10.1016/j.eswa.2017.\n04.053\nFerreira, I., Ochoa, L., and Koeshidayatullah, A. (2022). On the generation of\nrealistic synthetic petrographic datasets using a style-based gan.Sci. Rep.12, 12845.\ndoi:10.1038/s41598-022-16034-4\nFu, D., Su, C., Wang, W., and Yuan, R. (2022). Deep learning based lithology\nclassiﬁcation of drill core images.Plos one17, e0270826. doi:10.1371/journal.pone.\n0270826\nGoyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., et al.\n(2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. New York,\nUSA. arXiv [Preprint]. doi:10.48550/arXiv.1706.02677\nHarinie, T., Janani Chellam, I., Sathya Bama, S., Raju, S., and Abhaikumar, V.\n(2012). “Classiﬁcation of rock textures, ” in Proceedings of the international\nconference on information systems design and intelligent applications 2012 (India\n2012) held in visakhapatnam, India, january 2012(Berlin, Germany: Springer),\n887–895.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016).“Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision and pattern\nrecognition (Las Vegas, United States: IEEE), 770–778.\nIvchenko, A. V., Baraboshkin, E. E., Ismailova, L. S., Orlov, D. M., Koroteev, D. A.,\nand Baraboshkin, E. Y. (2018).“Core photo lithological interpretation based on\ncomputer analyses,” in Proceedings of the IEEE northwest Russia conference on\nmathematical methods in engineering and technology (Saint-Petersburg, Russia:\nIEEE), 10–14.\nKoeshidayatullah, A., Al-Ramadan, K., and Hughes, G. W. (2016). Facies mosaic\nand diagenetic patterns of the early devonian (late pragian –early emsian)\nmicrobialite-dominated carbonate sequences, qasr member, jauf formation,\nSaudi Arabia. Geol. J. 51, 704–721. doi:10.1002/gj.2678\nKoeshidayatullah, A., Morsilli, M., Lehrmann, D. J., Al-Ramadan, K., and Payne,\nJ. L. (2020). Fully automated carbonate petrography using deep convolutional\nneural networks.Mar. Petroleum Geol.122, 104687. doi:10.1016/j.marpetgeo.2020.\n104687\nKoeshidayatullah, A. (2022). Optimizing image-based deep learning for energy\ngeoscience via an effortless end-to-end approach.J. Petroleum Sci. Eng.215, 110681.\ndoi:10.1016/j.petrol.2022.110681\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classiﬁcation\nwith deep convolutional neural networks.Adv. neural Inf. Process. Syst.25.\nLeCun, Y., Bengio, Y., and Hinton, G. (2015). Deep learning.nature 521, 436–444.\ndoi:10.1038/nature14539\nLeeder, M. R. (2012). Sedimentology: Process and product. Berlin, Germany:\nSpringer Science & Business Media.\nLi, N., Hao, H., Gu, Q., Wang, D., and Hu, X. (2017). A transfer learning method\nfor automatic identiﬁcation of sandstone microscopic images.Comput. Geosciences\n103, 111–121. doi:10.1016/j.cageo.2017.03.007\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (2021). “Swin\ntransformer: Hierarchical vision transformer using shifted windows, ” in\nProceedings of the IEEE/CVF international conference on computer vision\n(Montreal, Canada: IEEE), 10012–10022.\nLobos, R., Silva, J. F., Ortiz, J. M., Díaz, G., and Egaña, A. (2016). Analysis and\nclassiﬁcation of natural rock textures based on new transform-based features.Math.\nGeosci. 48, 835–870. doi:10.1007/s11004-016-9648-8\nMartin, T., Meyer, R., and Jobe, Z. (2021). Centimeter-scale lithology and facies\nprediction in cored wells using machine learning.Front. Earth Sci. (Lausanne).491.\ndoi:10.3389/feart.2021.659611\nMcPhee, C., Reed, J., and Zubizarreta, I. (2015).Core analysis: A best practice\nguide. Amsterdam, Netherlands: Elsevier.\nFrontiers in Earth Science frontiersin.org14\nKoeshidayatullah et al. 10.3389/feart.2022.992442\nOord, A. V. D., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., et al.\n(2016). Wavenet: A generative model for raw audio. New York, USA. arXiv\n[Preprint]. doi:10.48550/arXiv.1609.03499\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., et al. (2019).\nPytorch: An imperative style, high-performance deep learning library.Adv. neural\nInf. Process. Syst.32.\nPatel, A. K., Chatterjee, S., and Gorai, A. K. (2017).“Development of online\nmachine vision system using support vector regression (svr) algorithm for grade\nprediction of iron ores,” in 2017 ﬁfteenth IAPR international conference on\nmachine vision applications (MVA), Nagoya, Japan, 08-12 May 2017 (Nagoya,\nJapan: IEEE), 149–152.\nPatel, A. K., Gorai, A. K., and Chatterjee, S. (2016).Development of machine\nvision-based system for iron ore grade prediction using Gaussian process regression\n(gpr). Minsk, Belarus.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O.,\net al. (2011). Scikit-learn: Machine learning in Python.J. Mach. Learn. Res. 12,\n2825–2830. doi:10.48550/arXiv.1201.0490\nPires de Lima, R., and Duarte, D. (2021). Pretraining convolutional neural\nnetworks for mudstone petrographic thin-section image classi ﬁcation.\nGeosciences 11, 336. doi:10.3390/geosciences11080336\nPrince, C., Dixon, M., and Haynes, L. (2005).“The use of high-resolution core\nimagery in reservoir characterization: An example from unlithi ﬁed miocene\nturbidites,” in Paper SCA2005-02, society of core analysts annual international\nsymposium. Toronto, Canada: SPWLA.\nQian, N. (1999). On the momentum term in gradient descent learning algorithms.\nNeural Netw. 12, 145–151. doi:10.1016/s0893-6080(98)00116-6\nRothwell, R. G., and Rack, F. R. (2006). New techniques in sediment core analysis:\nAn introduction.Geol. Soc. Lond. Spec. Publ.267, 1–29. doi:10.1144/gsl.sp.2006.267.\n01.01\nSeleznev, I., Abashkin, V., Chertova, A., Makienko, D., Istomin, S.,\nRomanov, D., et al. (2020). “Joint usage of whole core images obtained in\ndifferent frequency ranges for the tasks of automatic lithotype description\nand modeling of rocks ’ petrophysics properties, ” in Geomodel 2020\n(Gelendzhik, Russia: European Association of Geoscientists & Engineers),\n2020, 1–5.\nShorten, C., and Khoshgoftaar, T. M. (2019). A survey on image data\naugmentation for deep learning. J. Big Data 6, 60–48. doi:10.1186/s40537-019-\n0197-0\nSingh, M., Javadi, A., and Singh, S. (2004).“A comparison of texture teatures for\nthe classiﬁcation of rock images,” in International conference on intelligent data\nengineering and automated learning(Berlin, Germany: Springer), 179–184.\nThomas, A., Rider, M., Curtis, A., and MacArthur, A. (2011).Automated lithology\nextraction from core photographs. ﬁrst break 29. Netherlands: EAGE.\nTimmer, E., Knudson, C., and Gingras, M. (2021). Applying deep learning for\nidentifying bioturbation from core photographs.Am. Assoc. Pet. Geol. Bull.105,\n631–638. doi:10.1306/08192019051\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need,” in Advances in neural information processing\nsystems (CA, United States: MIT Press), 30.\nWeiss, K., Khoshgoftaar, T. M., and Wang, D. (2016). A survey of transfer\nlearning. J. Big Data3, 9–40. doi:10.1186/s40537-016-0043-6\nWrona, T., Pan, I., Gawthorpe, R. L., and Fossen, H. (2018). Seismic facies analysis\nusing machine learning.Geophysics 83, O83–O95. doi:10.1190/geo2017-0595.1\nWu, B., Meng, D., Wang, L., Liu, N., and Wang, Y. (2020). Seismic impedance\ninversion using fully convolutional residual network and transfer learning.IEEE\nGeosci. Remote Sens. Lett.17, 2140–2144. doi:10.1109/lgrs.2019.2963106\nWu, X., Liang, L., Shi, Y., and Fomel, S. (2019). Faultseg3d: Using synthetic data\nsets to train an end-to-end convolutional neural network for 3d seismic fault\nsegmentation. Geophysics 84, IM35–IM45. doi:10.1190/geo2018-0646.1\nXie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J. M., and Luo, P. (2021).\nSegformer: Simple and ef ﬁcient design for semantic segmentation with\ntransformers. Adv. Neural Inf. Process. Syst. 34, 12077–12090. doi:10.48550/\narXiv.2105.15203\nXu, Z., Shi, H., Lin, P., and Liu, T. (2021). Integrated lithology identiﬁcation based\non images and elemental data from rocks.J. Petroleum Sci. Eng.205, 108853. doi:10.\n1016/j.petrol.2021.108853\nZhang, H., Xue, J., and Dana, K. (2017).“Deep ten: Texture encoding network,” in\nProceedings of the IEEE conference on computer vision and pattern recognition\n(Hawaii, United States: IEEE), 708–717.\nFrontiers in Earth Science frontiersin.org15\nKoeshidayatullah et al. 10.3389/feart.2022.992442"
}