{
    "title": "Federated Learning of N-gram Language Models",
    "url": "https://openalex.org/W2980154417",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2195061591",
            "name": "Chen, Mingqing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2744150477",
            "name": "Suresh, Ananda Theertha",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221976403",
            "name": "Mathews, Rajiv",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Wong, Adeline",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224010888",
            "name": "Allauzen, Cyril",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2754237597",
            "name": "Beaufays, Françoise",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2404376174",
            "name": "Riley, Michael",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1561171833",
        "https://openalex.org/W2293634267",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2535838896",
        "https://openalex.org/W2057169877",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2980470892",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2963917928",
        "https://openalex.org/W3140224754",
        "https://openalex.org/W2541884796",
        "https://openalex.org/W2784621220",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W2963385194",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2117278770",
        "https://openalex.org/W2900120080",
        "https://openalex.org/W1689711448",
        "https://openalex.org/W2963189173",
        "https://openalex.org/W2962824709",
        "https://openalex.org/W1930690627",
        "https://openalex.org/W2767079719",
        "https://openalex.org/W2530417694",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2605710880",
        "https://openalex.org/W2746762542",
        "https://openalex.org/W2130942839"
    ],
    "abstract": "We propose algorithms to train production-quality n-gram language models using federated learning. Federated learning is a distributed computation platform that can be used to train global models for portable devices such as smart phones. Federated learning is especially relevant for applications handling privacy-sensitive data, such as virtual keyboards, because training is performed without the users' data ever leaving their devices. While the principles of federated learning are fairly generic, its methodology assumes that the underlying models are neural networks. However, virtual keyboards are typically powered by n-gram language models for latency reasons. We propose to train a recurrent neural network language model using the decentralized FederatedAveraging algorithm and to approximate this federated model server-side with an n-gram model that can be deployed to devices for fast inference. Our technical contributions include ways of handling large vocabularies, algorithms to correct capitalization errors in user data, and efficient finite state transducer algorithms to convert word language models to word-piece language models and vice versa. The n-gram language models trained with federated learning are compared to n-grams trained with traditional server-based algorithms using A/B tests on tens of millions of users of virtual keyboard. Results are presented for two languages, American English and Brazilian Portuguese. This work demonstrates that high-quality n-gram language models can be trained directly on client mobile devices without sensitive training data ever leaving the devices.",
    "full_text": "Federated Learning of N-gram Language Models\nMingqing Chen, Ananda Theertha Suresh, Rajiv Mathews, Adeline Wong,\nCyril Allauzen, Franc ¸oise Beaufays, Michael Riley\nGoogle, Inc.\n{mingqing,theertha,mathews,adelinew,allauzen,fsb,riley}\n@google.com\nAbstract\nWe propose algorithms to train production-\nquality n-gram language models using feder-\nated learning. Federated learning is a dis-\ntributed computation platform that can be used\nto train global models for portable devices\nsuch as smart phones. Federated learning is\nespecially relevant for applications handling\nprivacy-sensitive data, such as virtual key-\nboards, because training is performed with-\nout the users’ data ever leaving their devices.\nWhile the principles of federated learning are\nfairly generic, its methodology assumes that\nthe underlying models are neural networks.\nHowever, virtual keyboards are typically pow-\nered by n-gram language models for latency\nreasons.\nWe propose to train a recurrent neural net-\nwork language model using the decentral-\nized FederatedAveraging algorithm and\nto approximate this federated model server-\nside with an n-gram model that can be de-\nployed to devices for fast inference. Our\ntechnical contributions include ways of han-\ndling large vocabularies, algorithms to cor-\nrect capitalization errors in user data, and efﬁ-\ncient ﬁnite state transducer algorithms to con-\nvert word language models to word-piece lan-\nguage models and vice versa. The n-gram lan-\nguage models trained with federated learning\nare compared to n-grams trained with tradi-\ntional server-based algorithms using A/B tests\non tens of millions of users of a virtual key-\nboard. Results are presented for two lan-\nguages, American English and Brazilian Por-\ntuguese. This work demonstrates that high-\nquality n-gram language models can be trained\ndirectly on client mobile devices without sen-\nsitive training data ever leaving the devices.\nFigure 1: Glide trails are shown for two spatially-\nsimilar words: “Vampire” (in red) and “Value” (in or-\nange). Viable decoding candidates are proposed based\non context and language model scores.\n1 Introduction\n1.1 Virtual keyboard applications\nVirtual keyboards for mobile devices provide a\nhost of functionalities from decoding noisy spatial\nsignals from tap and glide typing inputs to provid-\ning auto-corrections, word completions, and next-\nword predictions. These features must ﬁt within\ntight RAM and CPU budgets, and operate under\nstrict latency constraints. A key press should re-\nsult in visible feedback within about 20 millisec-\nonds (Ouyang et al., 2017; Alsharif et al., 2015).\nWeighted ﬁnite-state transducers have been used\nsuccessfully to decode keyboard spatial signals us-\ning a combination of spatial and language mod-\nels (Ouyang et al., 2017; Hellsten et al., 2017).\nFigure 1 shows the glide trails of two spatially-\nsimilar words. Because of the similarity of the\ntwo trails, the decoder must rely on the language\nmodel to discriminate between viable candidates.\nFor memory and latency reasons, especially on\nlow-end devices, the language models are typi-\ncally based on n-grams and do not exceed ten\nmegabytes. A language model (LM) is a prob-\nabilistic model on words. Given previous words\nx1,x2,...,x m−1, an LM assigns a probability to\nthe new words, i.e. p(xm|xm−1,...,x 1). An\nn-gram LM is a Markovian distribution of order\narXiv:1910.03432v1  [cs.CL]  8 Oct 2019\nFigure 2: An illustration of the federated learning pro-\ncess from McMahan and Ramage (2017): (A) client\ndevices compute SGD updates on locally-stored data,\n(B) a server aggregates the client updates to build a new\nglobal model, (C) the new model is sent back to clients,\nand the process is repeated.\nn−1, deﬁned by\np(xm|xm−1,...,x 1) = p(xm|xm−1,...,x m−n+1),\nwhere n is the order of the n-gram. For compu-\ntation and memory efﬁciency, keyboard LMs typ-\nically have higher-order n-grams over a subset of\nthe vocabulary, e.g. the most frequent 64K words,\nand the rest of the vocabulary only has unigrams.\nWe consider n-gram LMs that do not exceed1.5M\nn-grams and include fewer than 200K unigrams.\nN-gram models are traditionally trained by ap-\nplying a smoothing method to n-gram counts from\na training corpus (Chen and Goodman, 1999). The\nhighest quality n-gram models are trained over\ndata that are well-matched to the desired out-\nput (Moore and Lewis, 2010). For virtual key-\nboards, training over users’ typed text would lead\nto the best results. Of course, such data are very\npersonal and need to be handled with care.\n1.2 Federated learning\nWe propose to leverage Federated Learning\n(FL) (Kone ˇcn`y et al., 2016; Konen et al., 2016),\na technique where machine learning models are\ntrained in a decentralized manner on end-users’\ndevices, so that raw data never leaves these de-\nvices. Only targeted and ephemeral parameter up-\ndates are aggregated on a centralized server. Fig-\nure 2 provides an illustration of the process. Fed-\nerated learning for keyboard input was previously\nexplored in Hard et al. (2018), in which a feder-\nated recurrent neural network (RNN) was trained\nfor next-word prediction. However, latency con-\nstraints prevent the direct use of an RNN for de-\ncoding. To overcome this problem, we propose\nto derive an n-gram LM from a federated RNN\nLM model and use that n-gram LM for decod-\ning. Speciﬁcally, the approximation algorithm is\nbased on SampleApprox , which was recently\nproposed in Suresh et al. (2019a,b). The proposed\napproach has several advantages:\nImproved model quality:Since the RNN LM is\ntrained directly on domain-matched user data, its\npredictions are more likely to match actual user\nbehavior. In addition, as shown in Suresh et al.\n(2019a), an n-gram LM approximated from such\nan RNN LM is of higher quality than an n-gram\nLM trained on user data directly.\nMinimum information transmission: In FL,\nonly the minimal information necessary for model\ntraining (the model parameter deltas) is transmit-\nted to centralized servers. The model updates\ncontain much less information than the complete\ntraining data.\nAdditional privacy-preserving techniques: FL\ncan be further combined with privacy-preserving\ntechniques such as secure multi-party computa-\ntion (Bonawitz et al., 2017) and differential pri-\nvacy (McMahan et al., 2018; Agarwal et al., 2018;\nAbadi et al., 2016). By the post-processing theo-\nrem, if we train a single differentially private re-\ncurrent model and use it to approximate n-gram\nmodels, all the distilled models will also be differ-\nentially private with the same parameters (Dwork\net al., 2014).\nFor the above reasons, we have not pro-\nposed to learn n-gram models directly using\nFederatedAveraging of n-gram counts for\nall orders.\n2 Outline\nThe paper is organized along the lines of chal-\nlenges associated with converting RNN LMs to n-\ngram LMs for virtual keyboards: the feasibility of\ntraining neural models with a large vocabulary, in-\nconsistent capitalization in the training data, and\ndata sparsity in morphologically rich languages.\nWe elaborate on each of these challenges below.\nLarge vocabulary:Keyboard n-gram models are\ntypically based on a carefully hand-curated vocab-\nulary to eliminate misspellings, erroneous capital-\nizations, and other artifacts. The vocabulary size\noften numbers in the hundreds of thousands. How-\never, training a neural model directly over the vo-\ncabulary is memory intensive as the embedding\nand softmax layers require space |V|× Ne, where\n|V|is the vocabulary size andNe is the embedding\ndimension. We propose a way to handle large vo-\ncabularies for federated models in Section 3.\nIncorrect capitalization: In virtual keyboards,\nusers often type with incorrect casing (e.g. “She\nlives in new york” instead of “She lives in New\nYork”). It would be desirable to decode with\nthe correct capitalization even though the user-\ntyped data may be incorrect. Before the discussion\nof capitalization, the SampleApprox algorithm\nis reviewed in Section 4. We then modify\nSampleApprox to infer capitalization in Sec-\ntion 5.\nLanguage morphology: Many words are com-\nposed of root words and various morpheme com-\nponents, e.g. “crazy”, “crazily”, and “craziness”.\nThese linguistic features are prominent in mor-\nphologically rich languages such as Russian. The\npresence of a large number of morphological vari-\nants increases the vocabulary size and data sparsity\nultimately making it more difﬁcult to train neural\nmodels. Algorithms to convert between word and\nword-piece models are discussed in Section 6.\nFinally, we compare the performance of word\nand word-piece models and present the results of\nA/B experiments on real users of a virtual key-\nboard in Section 7.\n3 Unigram distributions\nAmong the 200K words in the vocabulary, our vir-\ntual keyboard models only use the top 64K words\nin the higher-order n-grams. We train the neu-\nral models only on these most frequent words and\ntrain a separate unigram model over the entire vo-\ncabulary. We interpolate the two resulting models\nto obtain the ﬁnal model for decoding.\n3.1 Collection\nUnigrams are collected via a modiﬁed version\nof the FederatedAveraging algorithm. No\nmodels are sent to client devices. Instead of re-\nturning gradients to the server, counting statistics\nare compiled on each device and returned. In our\nexperiments, we aggregate over groups of approx-\nimately 500 devices per training round. We count\na unigram distribution U from a whitelist vocabu-\nlary by U = ∑\ni wiCi, where iis the index over\ndevices, Ci are the raw unigram counts collected\nfrom a single device i, and wi is a weight applied\nto device i.\nTo prevent users with large amounts of data\n(a)\n (b)\nFigure 3: Unigram distribution convergence. Note that\nby 3000 rounds, the unigram distribution is stable, but\nthe model is still learning new tail unigrams.\nfrom dominating the unigram distribution, we ap-\nply a form of L1-clipping:\nwi = λ\nmax(λ,∑Ci), (1)\nwhere λis a threshold that caps each device’s con-\ntribution. When λ = 1, L1-clipping is equivalent\nto equal weighting. The limit λ →∞ is equiva-\nlent to collecting the true counts, since wi →1.\n3.2 Convergence\nConvergence of the unigram distribution is mea-\nsured using the unbiased chi-squared statistic (for\nsimplicity, referred to as the Z-statistic) deﬁned\nin Bhattacharya and Valiant (2015), the number of\nunique unigrams seen, and a moving average of\nthe number of rounds needed to observe new uni-\ngrams.\nFigure 3(a) shows the overall distributional con-\nvergence based on theZ-statistic. At round k, uni-\ngram counts after k/2 and krounds are compared.\nFigure 3(b) plots the number of whitelist vocabu-\nlary words seen and a moving average of the num-\nber of rounds containing new unigrams. New un-\nigrams are determined by comparing a round k\nwith all rounds throughk−1 and noting if any new\nwords are seen. The shaded bands range from the\nLM’s unigram capacity to the size of the whitelist\nvocabulary.\n3.3 Experiments\nSince the whitelist vocabulary is uncased, capital-\nization normalization is applied based on an ap-\nproach similar to Section 5. We then replace the\nunigram part of an n-gram model with this distri-\nbution to produce the ﬁnal LM.\nIn A/B experiments, unigram models with\ndifferent L1-clipping thresholds are compared\nagainst a baseline unigram model gathered from\nModel acc@1 [%] OOV rate [%]\nbaseline 8.14 18.08\nλ= 1 +0.19 ±0.21 −1.33 ±0.75\nλ= 1K +0.11 ±0.24 −1.06 ±0.66\nλ= 5K −0.08 ±0.26 −0.78 ±0.93\nTable 1: Relative change with L1-clipped unigrams\non live trafﬁc of en US users on the virtual keyboard.\nQuoted 95% conﬁdence intervals are derived using the\njackknife method with user buckets.\ncentralized log data. Results are presented in Ta-\nble 1. Accuracy is unchanged and OOV rate is\nimproved at λ= 1 and λ= 1K.\nBefore we discuss methods to address in-\nconsistent capitalization and data sparsity in\nmorphologically rich languages, we review\nSampleApprox .\n4 Review of SampleApprox\nSampleApprox , proposed in Suresh et al.\n(2019a,b), can be used to approximate a RNN as\na weighted ﬁnite automaton such as an n-gram\nmodel. A weighted ﬁnite automaton (WFA) A =\n(Σ,Q,E,i,F ) over R+ (probabilities) is given by\na ﬁnite alphabet Σ (vocabulary words), a ﬁnite set\nof states Q(n-gram contexts), an initial statei∈Q\n(sentence start state), a set of ﬁnal states F ∈Q\n(sentence end states), and a set of labeled transi-\ntions E and associated weights that represent the\nconditional probability of labels (from Σ) given\nthe state (list of n-grams and their probabilities).\nWFA models allow a special backoff label ϕ for\nsuccinct representation as follows. Let L[q] be\nthe set of labels on transitions from state q. For\nx∈L[q], let wq[x], be the weight of the transition\nof x at state q and dq[x] be the destination state.\nFor a label xand a state q,\np(x|q) = wq[x] if x∈L[q],\n= wq[ϕ] ·p(x|dq[ϕ]) otherwise.\nIn other words, ϕ is followed if x /∈L[q]. The\ndeﬁnition above is consistent with that of backoff\nn-gram models (Chen and Goodman, 1999). Let\nB(q) denote the set of states from which qcan be\nreached by a path of backoff labels and let q[x]\nbe the ﬁrst state at which label x can be read by\nfollowing a backoff path from q.\nGiven an unweighted ﬁnite automaton Aand a\nneural model, SampleApprox ﬁnds the proba-\nbility model on A that minimizes the Kullback-\nLeibler (KL) divergence between the neural model\nand the WFA. The algorithm has two steps: a\ncounting step and a KL minimization step. For\nthe counting step, let ¯x(1),¯x(2),..., ¯x(k) be kin-\ndependent samples from the neural model. For a\nsequence ¯x, let xi denote the ith label and ¯xi =\nx1,x2,...,x i denote the ﬁrst i labels. For every\nq∈Qand x∈Σ, the algorithm computesC(x,q)\ngiven by\n∑\nq′∈B(q)\nm∑\nj=1\n∑\ni≥0\n1q(¯xi(j))=q′,q=q′[x] ·pnn(x|¯xi(j)).\nWe illustrate this counting with an example.\nSuppose we are interested in the count of\nthe bi-gram New York. Given a bi-gram\nLM, SampleApprox generates msentences and\ncomputes\nC(York,New) =\n∑\nj,i:xi(j)=New\npnn(York|¯xi(j)).\nIn other words, it ﬁnds all sentences that have the\nword New, observes how frequently York appears\nsubsequently, and computes the conditional prob-\nability. After counting, it uses a difference of con-\nvex (DC) programming based algorithm to ﬁnd the\nKL minimum solution. If ℓ is the average num-\nber of words per sentence, the computational com-\nplexity of counting is ˜O(k·ℓ·|Σ|) 1 and the com-\nputational complexity of the KL minimization is\n˜O(|E|+ |Q|) per iteration of DC programming.\n5 Capitalization\nAs mentioned in Section 2, users often type with\nincorrect capitalization. One way of handling in-\ncorrect capitalization is to store an on-device capi-\ntalization normalizer (Beaufays and Strope, 2013)\nto correctly capitalize sentences before using them\nto train the neural model. However, capitalization\nnormalizers have large memory footprints and are\nnot suitable for on-device applications. To over-\ncome this, the neural model is ﬁrst trained on un-\ncased user data. SampleApprox is then modi-\nﬁed to approximate cased n-gram models from un-\ncased neural models.\nAs before, let ¯x(1),¯x(2),..., ¯x(k) be k in-\ndependent (uncased) samples from the neural\nmodel. We capitalize them correctly at the\nserver using Beaufays and Strope (2013). Let\n1an = ˜O(bn), means an ≤bn ·poly log(n), ∀n ≥n0.\n¯y(1),¯y(2),... ¯y(k) represent the corresponding k\ncorrectly capitalized samples. Let pcap be another\nprobability model on non-user data that approxi-\nmates the ratio of uncased to cased probabilities\ngiven a context. Given a labely, let u(y) be the un-\ncased symbol. For example, ifyis York, thenu(y)\nis york. With the above deﬁnitions, we modify the\ncounting step of SampleApprox as follows:\n∑\nq′∈B(q)\nm∑\nj=1\n∑\ni≥0\n1q(¯yi(j))=q′,q=q′[y] ·˜p(y|¯yi(j)),\nwhere ˜p(y|¯yi(j)) is given by\npnn(u(y)|u(¯yi(j)))· pcap(y|¯yi(j))∑\ny′:u(y′)=u(y) pcap(y′|¯yi(j)).\nWe refer to this modiﬁed algorithm as\nCapSampleApprox . We note that word-\npiece to word approximation incurs an additional\ncomputation cost of ˜O((|E|+|Q|+|∆|)ℓ), where\n∆ is the number of words, E and Qare the set of\narcs and set of states in the word n-gram model,\nand ℓis the maximum number of word-pieces per\nword.\n6 Morphologically rich languages\nTo train neural models on morphologically rich\nlanguages, subword segments such as byte-pair\nencodings or word-pieces (Shibata et al., 1999;\nSchuster and Nakajima, 2012; Kudo, 2018) are\ntypically used. This approach assigns conditional\nprobabilities to subword segments, conditioned on\nprior subword segments. It has proved successful\nin the context of speech recognition (Chiu et al.,\n2018) and machine translation (Wu et al., 2016).\nFollowing these successes, we propose to train\nRNN LMs with word-pieces for morphologically\nrich languages.\nWe apply the word-piece approach of Kudo\n(2018), which computes a word-piece unigram\nLM using a word-piece inventory VP. Each word-\npiece xi ∈VPis associated with a unigram prob-\nability p(xi). For a given word y and its possible\nsegmentation candidates, the word is encoded with\nthe segmentation that assigns the highest probabil-\nity.\nThroughout this paper we apply 4K, 16K, and\n30K as the word-piece inventory sizes. These val-\nues lie within a range that provides good trade-off\nbetween the LSTM embedding size and the rich-\nness of the language morphology. We apply100%\n0\n2φ\n1\nab\nab\n3ac\n0 1\na:ε\nb:ab\nc:ac\n(a) (b)\n0,0\n0,2φ:ε\n1,0\na:ε\n1,2\na:ε\nφ:ε\n0,1\nb:ab\nb:ab\n0,3c:ac\n(c)\nFigure 4: The (a) WFA Aand WFSTs (b) T and (c) B\nfor the word vocabulary {ab,ac}and word-piece vo-\ncabulary {a,b,c }. Initial states are represented by bold\ncircles and ﬁnal states by double circles.\ncharacter coverage to include all the symbols that\nappeared in the unigram distribution (Section 3),\nincluding the common English letters, accented\nletters e.g. ´e, ˆo, and digits. Accented letters are\nimportant for languages like Portuguese. For fast\ndecoding, the n-gram models still need to be at\nthe word-level, since word-piece n-gram models\nincrease the depth of the beam-search during de-\ncoding. We convert the word n-gram topology to\nan equivalent word-piece WFA topology and use\nSampleApprox to approximate the neural word-\npiece model on the word-piece WFA topology. We\nthen convert the resulting word-piece WFA LM to\nthe equivalent n-gram LM. The remainder of this\nsection outlines efﬁcient algorithms for converting\nbetween word and word-piece WFA models.\nA natural way to represent the transduction\nfrom word-piece sequences to word sequences is\nwith a ﬁnite-state transducer. Given the properties\nof our word-piece representation, that transducer\ncan be made sequential (i.e., input deterministic).\nA sequential weighted ﬁnite-state transducer\n(WFST) is a deterministic WFA where each tran-\nsition has an output label in addition to its (input)\nlabel and weight. We will denote by oq[x] the\noutput label of the transition at state q with input\nlabel x, oq[x] ∈∆ ∪{ϵ}, where ∆ denotes the\noutput alphabet of the transducer and ϵthe empty\nstring/sequence.\nLet M be the minimal sequential (unweighted)\nﬁnite-state transducer (FST) lexicon from word-\npiece sequences in Σ∗to word sequences in ∆∗,\nwhere Σ denotes our word-piece inventory, ∆ de-\nnotes our vocabulary, and ∗is Kleene closure.\nA word-piece topology B equivalent to the word\ntopology A can be obtained by composing the\nword-piece-to-word transducer M with A:\nB = M ◦A.\nSince Ahas backoff transitions, the generic com-\nposition algorithm of (Allauzen et al., 2011) is\nused with a custom composition ﬁlter that ensures\nthe result, B, is deterministic with a well-formed\nbackoff structure, and hence is suitable for the\ncounting step of SampleApprox . We give an\nexplicit description of the construction of B, from\nwhich readers familiar with Allauzen et al. (2011)\ncan infer the form of the custom composition ﬁlter.\nThe states in Bare pairs (q1,q2), with q1 ∈QM\nand q2 in QA, initial state iB = (iM ,iA), and ﬁnal\nstate fB = (fM ,fA). Given a state (q1,q2) ∈QB,\nthe outgoing transitions and their destination states\nare deﬁned as follows. If x ∈L[q1], then an x-\nlabeled transition is created if one of two condi-\ntions holds:\n1. if oq1 [x] ∈L[q2], then\nd(q1,q2)[x] = (dq1 [x],dq2 [oq1 [x]]) and\no(q1,q2)[x] = oq1 [x];\n2. if oq1 [x] = ϵand R[dq1 [x]] ∩L[q2] ̸= ∅, then\nd(q1,q2)[x] = (dq1 [x],dq2 [oq1 [x]]) and\no(q1,q2)[x] = ϵ\nwhere R[q] denotes the set of output non- ϵlabels\nthat can be emitted after following an output- ϵ\npath from q. Finally if ϕ ∈L[q1], a backoff tran-\nsition is created:\nd(q1,q2)[ϕ] = (q1,dq2 [ϕ]) and oq1,q2 [ϕ] = ϵ.\nThe counting step of SampleApprox is applied\nto B, and transfers the computed counts fromBto\nAby relying on the following key property of M.\nFor every word yin ∆, there exists a unique state\nqy ∈QM and unique word-piece xy in Σ such that\noqy [xy] = y. This allows us to transfer the counts\nfrom Bto Aas follows:\nwq[y] = w(qy,q)[xy]\nThe KL minimization step of SampleApprox to\nAis applied subsequently.\nAs an alternative, the unweighted word automa-\nton Acould be used to perform the counting step\nModel Nl Nh Ne Se Stotal\nW30K 1 670 96 2.91M 3.40M\nP4K-S 1 670 96 0.38M 0.85M\nP4K-L 2 1080 140 0.56M 2.70M\nP4K-G 2 1080 280 1.12M 2.71M\nP16K-S 1 670 96 1.54M 2.00M\nP16K-L 1 670 160 2.56M 3.33M\nP30K 1 670 96 2.91M 3.40M\nTable 2: Parameters for neural language models. W\nand P refer to word and word-piece models, respec-\ntively. Nl, Nh, Ne, Se and Stotal refer to the number\nof LSTM layers, the number of hidden states in LSTM,\nthe embedding dimension size, the number of param-\neters in the embedding layer and in total, respectively.\nThe sufﬁxes “S” and “L” indicate small and large mod-\nels. “G” represents GLSTM. The sufﬁxes 4K, 16K and\n30K represent the vocabulary sizes.\ndirectly. Each sample ¯x(j) could be mapped to\na corresponding word sequence ¯y(j), mapping\nout-of-vocabulary word-piece sequences to an un-\nknown token. However, the counting steps would\nhave become much more computationally expen-\nsive, since pnn(y|¯yi(j)) would have to be evalu-\nated for all i, j and for all words yin the vocabu-\nlary, where pnn is now a word-piece RNN.\n7 Experiments\n7.1 Neural language model\nLSTM models (Hochreiter and Schmidhuber,\n1997) have been successfully used in a variety of\nsequence processing tasks. LSTM models usually\nhave a large number of parameters and are not suit-\nable for on-device learning. In this work, we use\nvarious techniques to reduce the memory footprint\nand to improve model performance.\nWe use a variant of LSTM with a Coupled Input\nand Forget Gate (CIFG) (Greff et al., 2017) for the\nfederated neural language model. CIFG couples\nthe forget and input decisions together, which re-\nduces the number of LSTM parameters by 25%.\nWe also use group-LSTM (GLSTM) (Kuchaiev\nand Ginsburg, 2017) to reduce the number of train-\nable variables of an LSTM matrix by the number\nof feature groups, k. We set k = 5 in experi-\nments. Table 2 lists the parameter settings of the\nword (W) and word-piece (P) models used in this\nstudy. Due to the memory limitations of on-device\ntraining, all models use fewer than 3.5M parame-\nters. For each vocabulary size, we ﬁrst start with a\nbase architecture consisting of one LSTM layer, a\nAlgorithm 1Approximating a Neural Model as an N-Gram with a Supplemental Topology.\nTrain Ru\nW , Ru\nP with FederatedAveraginga\nTrain AW from supplemental corpus C\nAWe ,AWi ,AWm ,AWr ←Gen(Ru\nW , AW , ø, NN2WFAW)\nAPe ,APi ,APm ,APr ←Gen(Ru\nP , AW , AWi , NN2WFAP)\nfunction Gen(Ru, AW , AWi , function NN2WFA )\nAe ←NN2WFA (Ru, AW )\nif NN2WFA ==NN2WFAW then\nAi ←NN2WFA (Ru, AW , self infer=true)\nelse\nAi ←NN2WFA (Ru, AWi )\nend if\nAm ←Interpolate(Ae, Ai)\na T denotes an unweighted topology and A denotes the\nweighted n-gram model. Superscript u represents uncased\nmodels.\nAr ←NN2WFA (Ru, Am)\nreturn Ae, Ai, Am, Ar\nend function\nfunction NN2WFAW(Ru\nW , AW , self infer=false)\nif self infer then\nreturn CapSampleApprox (Ru\nW , ø, AW )\nelse\nreturn CapSampleApprox (Ru\nW , AW , AW )\nend if\nend function\nfunction NN2WFAP(Ru\nP , AW )\nTu\nW ←ConvertToLowercaseTopology(AW )\nTu\nP ←ConvertToWordPieceTopology(Tu\nW )\nAu\nP ←SampleApprox (Ru\nP , Tu\nP )\nAu\nW ←ConvertToWordTopology(Au\nP )\nreturn CapSampleApprox (Au\nW , AW , AW )\nend function\n96-dimensional embedding, and 670 hidden state\nunits. We then attempt to increase the represen-\ntational power of the LSTM cell by increasing\nthe number of hidden units and using multi-layer\nLSTM cells (Sutskever et al., 2014). Residual\nLSTM (Kim et al., 2017) and layer normaliza-\ntion (Lei Ba et al., 2016) are used throughout ex-\nperiments, as these techniques were observed to\nimprove convergence. To avoid the restriction that\nNh = Ne in the output, we apply a projection\nstep at the output gate of the LSTM (Sak et al.,\n2014). This step reduces the dimension of the\nLSTM hidden state from Nh to Ne. We also share\nthe embedding matrix between the input embed-\nding and output softmax layer, which reduces the\nmemory requirement by |V|× Ne. We note that\nother recurrent neural models such as gated recur-\nrent units (Chung et al., 2014) can also be used\ninstead of CIFG LSTMs.\nThe federated RNN LMs are trained on two\nlanguage settings of the virtual keyboard: Amer-\nican English (en US) and Brazilian Portuguese\n(pt BR). Following McMahan et al. (2017), 500\nreporting clients are used to compute the gradi-\nent updates for each round. A server-side learn-\ning rate of 1.0, a client-side learning rate of 0.5,\nand Nesterov momentum of 0.9 are used. Both\nthe word and word-piece models are trained over\nthe same time range and with the same hyperpa-\nrameters. Prior to federated training of the RNN\nLM, the word-piece inventory is constructed from\nthe unigram distribution collected via the feder-\nated approach introduced in Section 3.\nA common evaluation metric for both word and\nword-piece models is desirable during federated\ntraining. Such a metric can be used to monitor the\ntraining status and select models to be used for the\nCapSampleApprox algorithm. Neither cross-\nentropy nor accuracy serves this need due to the\nmismatch in vocabularies used. Word-level accu-\nracy is hard to compute for the word-piece model,\nsince it requires hundreds of inference calls to tra-\nverse all combinations of a word from the word-\npiece vocabulary. In this study, we apply sentence\nlog likelihood (SLL) in the evaluation. Given a\nsentence ¯xm = {x1,x2,...,x m}composed of m\nunits (either words or word-pieces), SLL is eval-\nuated as ∑m\ni=1 log(pnn(xi|¯xi−1)). One issue that\narises is the handling of out-of-vocabulary (OOV)\nwords. The OOV probability of the word model is\nabout 8%. The comparable probability of an OOV\nword (according to V) for word-piece models is\nthe product of the corresponding word-piece con-\nditional probabilities, which is much smaller than\n8%. To mitigate this issue, we deﬁne SLL exclud-\ning OOV as:\nSLLe =\nm∑\ni:xi̸=OOV\nlog(pnn(xi|¯xi−1)),\nwhere the OOV in the equation includes word-\npieces that are components of OOV words. In the\nfollowing, SLLe is used as model selection metric.\n7.2 Approximated n-gram model\nAlgorithm 1 illustrates the workﬂow we use to\ngenerate different n-gram models for evaluation.\nRecall that CapSampleApprox takes a RNN\nLM, an n-gram topology, and a reweighting FST\nfor capitalization normalization. The n-gram\ntopology is empty under self-inference mode.\nSuresh et al. (2019a) showed that inferring topol-\nogy from the RNN LM does not perform as well as\nFigure 5: Sentence log likelihood excluding OOV token for en US (left) and pt BR (right).\nModel en US pt BR\nBaseline 10.03% 8.55%\nAWe 10.52 ±0.03% 9.66 ±0.02%\nAWi 10.47 ±0.02% 9.67 ±0.02%\nAWm 10.27 ±0.03% 9.40 ±0.02%\nAWr 10.49 ±0.03% 9.65 ±0.02%\nTable 3: Result of top-1 prediction accuracy on the\nlive trafﬁc of the virtual keyboard for en US and pt BR\npopulations. Quoted 95% conﬁdence intervals for fed-\nerated models are derived using the jackknife method.\nusing the true n-gram topology obtained from the\ntraining corpus. Hence, we supplement the neural-\ninferred topology with the topology obtained by a\nlarge external large corpus denoted by AW . We\nuse CapSampleApprox on four topologies and\ncompare the resulting models: an n-gram model\nobtained from an external corpus’s topology Ae,\nan n-gram model obtained from a neural inferred\ntopology Ai, an n-gram model obtained by in-\nterpolating (merging) the two models above Am,\nand an n-gram model obtained by approximating\non the interpolated topology Ar. We repeat this\nexperiment for both word and word-piece RNN\nLMs and use subscripts W and P, respectively.\nWe evaluate all eight produced n-gram models di-\nrectly on the trafﬁc of a production virtual key-\nboard, where prediction accuracy is evaluated over\nuser-typed words.\n7.3 Results\nFigure 5 shows the SLLe metric for all the exper-\niments listed in Table 2. In general, larger models\ngenerate better results than smaller baseline mod-\nels. For the baseline architectures with same RNN\nsize, having a larger vocabulary leads to some\ngains. For the larger architectures that have similar\nModel top-1\nBaseline 10.03%\nAPe 10.49 ±0.03%\nAPi 10.46 ±0.03%\nAPm 10.48 ±0.04%\nAPr 10.53 ±0.03%\nTable 4: Result of top-1 prediction accuracy on the live\ntrafﬁc of the virtual keyboard for en US derived using\nword-piece models.\ntotal numbers of parameters, 4K word-piece mod-\nels are shown to be superior to 16K and 30K. For\n4K word-piece models, GLSTM is in general on-\npar with its P4K-L counterpart. The word model is\nbetter than all the word-piece models in both lan-\nguages in SLLe. We were surprised by this result,\nand hypothesize that it is due to the SLL e metric\ndiscounting word-piece models’ ability to model\nthe semantics of OOV words. The solid lines are\nthe best models we pick for A/B experiment eval-\nuation for the virtual keyboard (P4K-L and W30K).\nTable 3 shows the A/B evaluation result on both\nen US and pt BR populations. The baseline model\nis an n-gram model trained directly from central-\nized logs. All of the federated trained models\nperform better than the baseline model. We re-\npeated the A/B evaluation with word-piece mod-\nels on en US and the results are in Table 4. The\nperformance of word-piece models is similar to\nthat of word models. Among the federated mod-\nels for en US, APr has the best result. This meets\nour expectation that the supplemental corpus helps\nimprove the performance of the topology inferred\nfrom the RNN LM.\n8 Conclusion\nWe have proposed methods to train production-\nquality n-gram language models using federated\nlearning, which allows training models without\nuser-typed text ever leaving devices. The proposed\nmethods are shown to perform better than tradi-\ntional server-based algorithms in A/B experiments\non real users of a virtual keyboard.\nAcknowledgments\nThe authors would like to thank colleagues in\nGoogle Research for providing the federated\nlearning framework and for many helpful discus-\nsions.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC\nConference on Computer and Communications Se-\ncurity, pages 308–318. ACM.\nNaman Agarwal, Ananda Theertha Suresh, Felix\nYu, Sanjiv Kumar, and Brendan McMahan. 2018.\ncpsgd: Communication-efﬁcient and differentially-\nprivate distributed sgd. In Neural Information Pro-\ncessing Systems.\nCyril Allauzen, Michael Riley, and Johan Schalk-\nwyk. 2011. A ﬁlter-based algorithm for efﬁcient\ncomposition of ﬁnite-state transducers. Interna-\ntional Journal of Foundations of Computer Science,\n22(8):1781–1795.\nOuais Alsharif, Tom Ouyang, Franoise Beaufays,\nShumin Zhai, Thomas Breuel, and Johan Schalk-\nwyk. 2015. Long short term memory neural network\nfor keyboard gesture decoding. 2015 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 2076–2080.\nFranc ¸oise Beaufays and Brian Strope. 2013. Language\nmodel capitalization. In 2013 IEEE International\nConference on Acoustics, Speech and Signal Pro-\ncessing, pages 6749–6752. IEEE.\nBhaswar Bhattacharya and Gregory Valiant. 2015.\nTesting closeness with unequal sized samples. In\nAdvances in Neural Information Processing Systems\n28. NIPS.\nKeith Bonawitz, Vladimir Ivanov, Ben Kreuter, Anto-\nnio Marcedone, H. Brendan McMahan, Sarvar Patel,\nDaniel Ramage, Aaron Segal, and Karn Seth. 2017.\nPractical secure aggregation for privacy-preserving\nmachine learning. In Proceedings of the 2017 ACM\nSIGSAC Conference on Computer and Communi-\ncations Security, CCS ’17, pages 1175–1191, New\nYork, NY , USA. ACM.\nStanley F Chen and Joshua Goodman. 1999. An\nempirical study of smoothing techniques for lan-\nguage modeling. Computer Speech & Language ,\n13(4):359–394.\nChung-Cheng Chiu, Tara N Sainath, Yonghui Wu, Ro-\nhit Prabhavalkar, Patrick Nguyen, Zhifeng Chen,\nAnjuli Kannan, Ron J Weiss, Kanishka Rao, Eka-\nterina Gonina, et al. 2018. State-of-the-art speech\nrecognition with sequence-to-sequence models. In\n2018 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), pages\n4774–4778. IEEE.\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. arXiv preprint arXiv:1412.3555.\nCynthia Dwork, Aaron Roth, et al. 2014. The algo-\nrithmic foundations of differential privacy. Foun-\ndations and Trends R⃝in Theoretical Computer Sci-\nence, 9(3–4):211–407.\nKlaus Greff, Rupesh K Srivastava, Jan Koutn´ık, Bas R\nSteunebrink, and J ¨urgen Schmidhuber. 2017. Lstm:\nA search space odyssey. IEEE transactions on neu-\nral networks and learning systems , 28(10):2222–\n2232.\nAndrew Hard, Kanishka Rao, Rajiv Mathews,\nFranc ¸oise Beaufays, Sean Augenstein, Hubert Eich-\nner, Chlo ´e Kiddon, and Daniel Ramage. 2018.\nFederated learning for mobile keyboard prediction.\nCoRR, abs/1811.03604.\nLars Hellsten, Brian Roark, Prasoon Goyal, Cyril Al-\nlauzen, Francoise Beaufays, Tom Ouyang, Michael\nRiley, and David Rybach. 2017. Transliterated mo-\nbile keyboard input via weighted ﬁnite-state trans-\nducers. In Proceedings of the 13th International\nConference on Finite State Methods and Natural\nLanguage Processing (FSMNLP).\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJaeyoung Kim, Mostafa El-Khamy, and Jungwon Lee.\n2017. Residual LSTM: design of a deep recurrent\narchitecture for distant speech recognition. In Inter-\nspeech 2017, 18th Annual Conference of the Inter-\nnational Speech Communication Association, Stock-\nholm, Sweden, August 20-24, 2017 , pages 1591–\n1595.\nJakub Kone ˇcn`y, H Brendan McMahan, Daniel Ram-\nage, and Peter Richt´arik. 2016. Federated optimiza-\ntion: Distributed machine learning for on-device in-\ntelligence. arXiv preprint arXiv:1610.02527.\nJakub Konen, H. Brendan McMahan, Felix X. Yu, Pe-\nter Richtarik, Ananda Theertha Suresh, and Dave\nBacon. 2016. Federated learning: Strategies for im-\nproving communication efﬁciency. In NIPS Work-\nshop on Private Multi-Party Machine Learning.\nOleksii Kuchaiev and Boris Ginsburg. 2017. Factor-\nization tricks for LSTM networks. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017,\nWorkshop Track Proceedings.\nTaku Kudo. 2018. Subword regularization: Improv-\ning neural network translation models with multiple\nsubword candidates. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, pages 66–75.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nBrendan McMahan, Eider Moore, Daniel Ramage,\nSeth Hampson, and Blaise Ag ¨uera y Arcas. 2017.\nCommunication-efﬁcient learning of deep networks\nfrom decentralized data. In Proceedings of the 20th\nInternational Conference on Artiﬁcial Intelligence\nand Statistics, AISTATS 2017, 20-22 April 2017,\nFort Lauderdale, FL, USA, pages 1273–1282.\nBrendan McMahan and Daniel Ramage. 2017.\nFederated learning: Collaborative machine\nlearning without centralized training data.\nhttps://ai.googleblog.com/2017/04/\nfederated-learning-collaborative.\nhtml.\nBrendan McMahan, Daniel Ramage, Kunal Talwar,\nand Li Zhang. 2018. Learning differentially private\nrecurrent language models. In International Confer-\nence on Learning Representations (ICLR).\nRobert C. Moore and William Lewis. 2010. Intelli-\ngent selection of language model training data. In\nProceedings of the ACL 2010 Conference Short Pa-\npers, ACLShort ’10, pages 220–224, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nTom Ouyang, David Rybach, Franc ¸oise Beaufays,\nand Michael Riley. 2017. Mobile keyboard in-\nput decoding with ﬁnite-state transducers. CoRR,\nabs/1704.03987.\nHas ¸im Sak, Andrew Senior, and Franc ¸oise Beaufays.\n2014. Long short-term memory recurrent neural\nnetwork architectures for large scale acoustic mod-\neling. In Fifteenth annual conference of the interna-\ntional speech communication association.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152. IEEE.\nYusuxke Shibata, Takuya Kida, Shuichi Fukamachi,\nMasayuki Takeda, Ayumi Shinohara, Takeshi Shi-\nnohara, and Setsuo Arikawa. 1999. Byte pair encod-\ning: A text compression scheme that accelerates pat-\ntern matching. Technical report, Technical Report\nDOI-TR-161, Department of Informatics, Kyushu\nUniversity.\nAnanda Theertha Suresh, Michael Riley, Brian Roark,\nand Vlad Schogol. 2019a. Approximating proba-\nbilistic models as weighted ﬁnite automata. CoRR,\nabs/1905.08701.\nAnanda Theertha Suresh, Brian Roark, Michael Riley,\nand Vlad Schogol. 2019b. Distilling weighted ﬁ-\nnite automata from arbitrary probabilistic models.\nIn Proceedings of the 14th International Conference\non Finite State Methods and Natural Language Pro-\ncessing (FSMNLP 2019).\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144."
}