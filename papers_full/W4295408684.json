{
  "title": "Adaptive Language Model Training for Molecular Design",
  "url": "https://openalex.org/W4295408684",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2595474306",
      "name": "Andrew Blanchard",
      "affiliations": [
        "Amgen (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1988289531",
      "name": "Debsindhu Bhowmik",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2734772578",
      "name": "John Gounley",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2114151169",
      "name": "Jens Gläser",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1844685798",
      "name": "Belinda S Akpa",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1815861180",
      "name": "Stephan Irle",
      "affiliations": [
        "Oak Ridge National Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3008443627",
    "https://openalex.org/W3014476516",
    "https://openalex.org/W2790808809",
    "https://openalex.org/W3113182646",
    "https://openalex.org/W3167905129",
    "https://openalex.org/W4205410305",
    "https://openalex.org/W2959938226",
    "https://openalex.org/W2406943157",
    "https://openalex.org/W2461470610",
    "https://openalex.org/W3158755582",
    "https://openalex.org/W3133325765",
    "https://openalex.org/W2110791536",
    "https://openalex.org/W2914542247",
    "https://openalex.org/W2107160601",
    "https://openalex.org/W2953128081",
    "https://openalex.org/W1998693213",
    "https://openalex.org/W2027478081",
    "https://openalex.org/W2066810295",
    "https://openalex.org/W1592238003",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W4210592951",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W6601211009",
    "https://openalex.org/W2034549041",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2080635178",
    "https://openalex.org/W6637031373",
    "https://openalex.org/W3129831491",
    "https://openalex.org/W2973114758",
    "https://openalex.org/W3114291043",
    "https://openalex.org/W3165630607",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W2807772724",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3006889321",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4297951436",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2415369160"
  ],
  "abstract": "The vast size of chemical space necessitates computational approaches to automate and accelerate the design of molecular sequences to guide experimental efforts for drug discovery. Genetic algorithms provide a useful framework to incrementally generate molecules by applying mutations to known chemical structures. Recently, masked language models have been applied to automate the mutation process by leveraging large compound libraries to learn commonly occurring chemical sequences (i.e., using tokenization) and predict rearrangements (i.e., using mask prediction). Here, we consider how language models can be adapted to improve molecule generation for different optimization tasks. We use two different generation strategies for comparison, fixed and adaptive. The fixed strategy uses a pre-trained model to generate mutations; the adaptive strategy trains the language model on each new generation of molecules selected for target properties during optimization. Our results show that the adaptive strategy allows the language model to more closely fit the distribution of molecules in the population. Therefore, for enhanced fitness optimization, we suggest the use of the fixed strategy during an initial phase followed by the use of the adaptive strategy. We demonstrate the impact of adaptive training by searching for molecules that optimize both heuristic metrics, drug-likeness and synthesizability, as well as predicted protein binding affinity from a surrogate model. Our results show that the adaptive strategy provides a significant improvement in fitness optimization compared to the fixed pre-trained model, empowering the application of language models to molecular design tasks",
  "full_text": "Blanchard et al.\nRESEARCH\nAdaptive Language Model Training\nfor Molecular Design\nAndrew E Blanchard 1†, Debsindhu Bhowmik 1*, John Gounley 1, Jens Glaser 2, Belinda S Akpa 3,4 and\nStephan Irle1\n*Correspondence:\nbhowmikd@ornl.gov\n1Computational Sciences and\nEngineering Division, Oak Ridge\nNational Laboratory, Oak Ridge,\nTN, 37831, USA\nFull list of author information is\navailable at the end of the article\n†This manuscript has been\nauthored by UT-Battelle LLC\nunder Contract No.\nDE-AC05-00OR22725 with the US\nDepartment of Energy (DOE).\nThe US government retains and\nthe publisher, by accepting the\narticle for publication,\nacknowledges that the US\ngovernment retains a nonexclusive,\npaid-up, irrevocable, worldwide\nlicense to publish or reproduce the\npublished form of the manuscript,\nor allow others to do so, for US\ngovernment purposes. DOE will\nprovide public access to these\nresults of federally sponsored\nresearch in accordance with the\nDOE Public Access Plan\n(http://energy.gov/downloads/doe-\npublic-access-plan).\nAbstract\nThe vast size of chemical space necessitates computational approaches to\nautomate and accelerate the design of molecular sequences to guide experimental\neﬀorts for drug discovery. Genetic algorithms provide a useful framework to\nincrementally generate molecules by applying mutations to known chemical\nstructures. Recently, masked language models have been applied to automate the\nmutation process by leveraging large compound libraries to learn commonly\noccurring chemical sequences (i.e., using tokenization) and predict\nrearrangements (i.e., using mask prediction). Here, we consider how language\nmodels can be adapted to improve molecule generation for diﬀerent optimization\ntasks. We use two diﬀerent generation strategies for comparison, ﬁxed and\nadaptive. The ﬁxed strategy uses a pre-trained model to generate mutations; the\nadaptive strategy trains the language model on each new generation of molecules\nselected for target properties during optimization. Our results show that the\nadaptive strategy allows the language model to more closely ﬁt the distribution of\nmolecules in the population. Therefore, for enhanced ﬁtness optimization, we\nsuggest the use of the ﬁxed strategy during an initial phase followed by the use of\nthe adaptive strategy. We demonstrate the impact of adaptive training by\nsearching for molecules that optimize both heuristic metrics, drug-likeness and\nsynthesizability, as well as predicted protein binding aﬃnity from a surrogate\nmodel. Our results show that the adaptive strategy provides a signiﬁcant\nimprovement in ﬁtness optimization compared to the ﬁxed pre-trained model,\nempowering the application of language models to molecular design tasks.\nKeywords: Masked Language Model; Drug Discovery; Genetic Algorithm\nBlanchard et al. Page 2 of 17\nIntroduction\nThe goal of rational drug design is to identify molecules with speciﬁed properties\nassociated with therapeutic value. Emerging infectious diseases (e.g. SARS-CoV-2\nand the associated pandemic) highlight the need for rational design to accelerate\nthe discovery of drugs in response to novel protein targets [1, 2]. Computer aided\ndrug discovery (CADD) provides a set of tools to shorten the time and cost of\nsearching chemical space for new applications [2–7]. In addition to the development\nof biophysical models and simulations traditionally associated with CADD [5–7],\nmuch recent work has focused on using methods from machine learning (ML) and\nartiﬁcial intelligence (AI) for molecular design [4, 5, 7–9].\nThe use of ML models in drug design has been enabled by the availability of large\ncompound libraries [10] and experimental datasets [11, 12] along with computa-\ntional libraries for cheminformatics [13]. Within a design application, models gener-\nally serve one of two possibly overlapping roles, molecule generation and molecule\nscoring. Generative models, such as variational autoencoders [8, 14] and generative\nadversarial networks [15, 16], are capable of sampling new molecules from chemical\nspace based oﬀ a training set. Scoring models, on the other hand, take a molecule as\ninput and generate a prediction for a given property (e.g. protein binding aﬃnity).\nThrough iterations of generation and scoring, searches over chemical space can be\nperformed to optimize a given property. The iterative process for optimization is\ncommonly referred to as a genetic algorithm [17].\nGenetic algorithms provide a useful strategy for the design of molecular sequences\nfor drug discovery applications. To use a genetic algorithm, a representation for a\nchemical sequence must be chosen along with a mutation operator to generate new\nsequences. The mutation operator is then used to explore chemical space and se-\nlection is performed according to a pre-deﬁned ﬁtness objective. Previous studies\nhave used genetic algorithms successfully for a range of drug discovery applica-\ntions [18–22]. Furthermore, benchmark studies have shown that genetic algorithms\ncan achieve state-of-the-art results for molecule generation, comparing favorably to\nrecent machine learning techniques [19, 21].\nDespite the success of genetic algorithms, the need to deﬁne an appropriate rep-\nresentation for a chemical sequence and a mutation operator poses a challenge.\nPrevious studies have often utilized a simple representation by enumerating indi-\nBlanchard et al. Page 3 of 17\nvidual atoms and bonds within a molecule [18, 19, 22]. For mutation, hand-crafted\nrules, such as add an atom, delete an atom, or create a ring, have been proposed and\nused for large scale exploration of chemical space [18]. Additional studies have used\ndata mining techniques to discover commonly occurring multi-atom fragments and\nused custom mutation operators to rearrange the speciﬁed fragments [20, 22–25].\nHowever, specifying ﬁxed rules for rearrangements limits the ability to adapt the\noptimization procedure to a given task. Ideally, the mutation operator can be au-\ntomatically inferred from the data, reducing the need for intuition and generalizing\nthe genetic algorithm approach to new molecular design tasks.\nInspired by advances in natural language processing (NLP) [26], recent studies\nhave shown how to automate both the choice of representation for chemical struc-\nture and the mutation operator [2, 27]. Starting with a text-based representation for\nmolecules, Simpliﬁed Molecular Input Line Entry System (SMILES) [28], the pro-\ncess of tokenization is used to determine commonly occurring subsequences [29, 30].\nThe subsequences are stored as a vocabulary and are used to map a given molecule\nsequence into a list of token IDs. Each token ID may correspond to multiple char-\nacters (i.e., atoms and bonds) in a given molecule. Once a tokenization scheme is\ndeﬁned, the molecule data can be used to train a masked language model. In the\ntraining for such a model, tokens are randomly masked and the loss is determined\nby how well the model reproduces the original sequence when predicting the masked\ntokens [26].\nWithout the need for labels, unsupervised training of masked language models can\nbe performed on large compound libraries (e.g. Enamine REAL database) [10]. For\na given mask, a trained model will rank possible ways to complete the molecular se-\nquence based on the vocabulary. Therefore, sampling from the top mask predictions\nprovides an automated mutation operator for a genetic algorithm [27]. Therefore, in\ncontrast to manually deﬁning rules for mutations, masked language models provide\nan automated solution for discovering both useful molecular representations (i.e.,\nthrough tokenization) and mutations (i.e., through mask prediction) as shown in\nFigure 1.\nAlthough the use of a ﬁxed pre-trained masked language model provides a useful\nimprovement over manually deﬁned rules, the challenge to adapt molecule gener-\nation for diﬀerent optimization tasks remains. For example, the dataset used for\nBlanchard et al. Page 4 of 17\nStarting Population\nMutations + Original\nMutationOperatorFixed| Adaptive\nScoring\nSelected Population\nTokenized\nMasked?\nLanguageModel\nFigure 1 Strategy for molecule optimization using a language model. An initial population of\nmolecules is used as input. The language model then generates mutations using predictions for\nrandomly placed masks. Molecules are ranked according to a speciﬁed score and top performers\nare selected for another round of mutations. Two approaches for the language model are\ninvestigated, ﬁxed and adaptive. For the ﬁxed approach, the language model is pre-trained on a\nlarge molecule dataset and it does not change during the optimization process. For the adaptive\napproach, the language model is trained on the selected population, which itself changes during\nthe optimization process.\nmodel pre-training may have certain biases that limit structural rearrangements\nuseful for a new task. In order to overcome this diﬃculty, we here propose a novel\nway to use language models within genetic algorithm optimization. Speciﬁcally, we\ncontinue to train the masked language model on populations selected for a speciﬁed\nﬁtness objective. By continued training on the selected population, we hypothesized\nthat the language model would adapt to new regions of chemical space useful for\noptimization.\nIn order to test our hypothesis, we implemented two approaches for comparison -\nﬁxed and adaptive. In the ﬁxed approach, a pre-trained language model was used to\ngenerate new molecules. In the adaptive approach, the pre-trained language model\nis used as a starting point and further trained using mask prediction on a speciﬁed\npopulation. Continued training is performed after each iteration of the genetic algo-\nrithm to produce a new population of molecules. Our results show that the adaptive\napproach produces data that more closely mimics the genetic algorithm population.\nFor optimization, the adaptive approach leads to increases in ﬁtness for tasks using\nboth heuristic metrics and a ML surrogate model. Therefore, by introducing the\nadaptive approach for automating mutations we broaden the capabilities of genetic\nalgorithm optimization for molecular design.\nBlanchard et al. Page 5 of 17\nMethods\nGenetic Algorithm\nIn this work, we focused on the molecule generation capabilities of a masked lan-\nguage model for ﬁtness optimization. The source code for this work can be found at\nhttps://code.ornl.gov/candle/mlmol in the adaptive-lm directory. As described in\nprevious work [27], a masked language model can be used as an automated muta-\ntion operator within a genetic algorithm. Figure 1 shows the major components for\noptimization. An initial population of molecules, in the form of SMILES strings are\nused as input to the masked language model. Portions of a given SMILES string\nare then randomly masked and the language model is used to predict mutations\nto the original molecule. The generated molecules are then scored and selection\nis performed based on the speciﬁed ﬁtness to generate an optimized population.\nThe process of mutation and selection can be repeated for a speciﬁed number of\niterations.\nFor the language model acting as the mutation operator, we considered two diﬀer-\nent training strategies, ﬁxed and adaptive. In both cases, we started by pre-training\na masked language model on a dataset with billions of molecules (for further de-\ntails on the dataset, see Methods Section - Molecule Data). For the ﬁxed strategy,\nweights of the pre-trained model were frozen, and the model was used only for in-\nference (i.e., mask prediction) as part of the genetic algorithm. For the adaptive\nstrategy, however, model training based on mask prediction was performed for one\nepoch during each generation, with the current population of molecules used as the\ntraining data. The language model, therefore, adapted to the patterns found in the\ncurrent population of the genetic algorithm before generating mutations.\nTo distinguish between the optimization performance of the ﬁxed and adaptive\nstrategies, we utilized a relatively simple genetic algorithm with a (µ+ 5µ) survivor\nselection scheme. Random uniform sampling with replacement was used to select µ\nparents from the population, and only mutation was used to generate new molecules,\nsimilar to our previous work [27]. A population size ( µ) of 10 5 was used for all\nreported genetic algorithm simulations. Mutations were generated by taking the top\n5 predictions from the masked language model for a given set of masks. Validity and\nuniqueness of the generated molecules were determined using rdkit [13] to convert\nSMILES strings into canonical form. Only unique molecules were retained in the\nBlanchard et al. Page 6 of 17\npopulation. All reported results, except for example histograms, show the mean\nover six repeated runs, with the standard deviation used to calculate error bars.\nExample histograms show the distribution of metric values for a single run.\nFor mask generation, we considered the following diﬀerent values for the mutation\nrate (i.e., probability that a given token will be masked): [0.15,0.30,0.45,0.60,0.75].\nIn addition, three diﬀerent types of mutation (replacement, insertion, and deletion)\nwere used. For each type, the number of mutations was determined using the bi-\nnomial distribution for the appropriate number of tokens and mutation rate. A\nminimum number of 1 mask per molecule was enforced. The locations for each mu-\ntation within the molecule string were then randomly sampled. For replacement,\nthe sampled token locations were replaced with a mask. For insertion, one sampled\nlocation was used to insert a mask before the given token. Similarly, for deletion, one\nsampled location was used to delete the token following the mask. The remaining\nsampled locations for both insertion and deletion were used for replacement.\nFitness in the genetic algorithm simulations was determined using the harmonic\nmean of multiple molecular metrics. For example, for two metrics ( x1 and x2), we\nused a ﬁtness F given by:\nF(x1,x2) = 2x1x1\nx1 + x2\n(1)\nBy default, we used quantitative estimations of drug-likeness and normalized syn-\nthesizability, similar to several previous studies on molecular optimization [15, 16,\n31, 32]. To apply the genetic algorithm strategies on a more realistic drug discovery\nscenario, we also utilized a recently released model for protein binding aﬃnity pre-\ndiction to generate a molecular metric [33]. Speciﬁcally, we used a predicted aﬃnity\nscore for the main protease of SARS-CoV-2. The resulting ﬁtness was, therefore, the\nharmonic mean of drug-likeness, synthesizability, and the predicted aﬃnity score.\nMolecule Data\nSimilar to previous work [2], we generated a molecule dataset starting from the\nEnamine REAL database [10]. Using a data augmentation strategy with a previously\ntrained language model, we increased the number of molecules to approximately\n3.6· 1010. In preparation for model training, the dataset was partitioned into 7.2· 104\nBlanchard et al. Page 7 of 17\nﬁles, each with 5·105 molecules, stored using the WebDataset [34] library for sharded\ndata loading during model training.\nIn addition to the constructed molecule dataset, we used two additional datasets\nas the starting population for genetic algorithm simulations. First, we used a subset\nof 105 molecules from QM9 [35, 36], referred to in the text and ﬁgures as GDB9.\nSecond, we selected the top 10 5 in terms of drug-likeness and synthesizability from\na hold-out set of the training data, referred to in the text and ﬁgures as Top.\nThese two datasets were used to show the diﬀerence in performance for the ﬁxed\nand adaptive strategies when starting from a relatively low and high initial ﬁtness\nrespectively.\nLanguage Model Training\nLanguage model pre-training consists of two diﬀerent stages, tokenization and mask\nprediction. During tokenization, a vocabulary is generated for the model based on\ncommonly occurring subsequences within the SMILES string for molecules. Here,\nwe split the SMILES string during pre-processing based on punctuation, which is\nthe default splitting used for the BERT WordPiece tokenizer in the Hugging Face\ntransformers library [37]. The vocabulary for the WordPiece tokenizer was then\ngenerated using the full 36 billion molecule dataset, with the vocabulary size set to\n32,768.\nFor mask prediction, we used PyTorch and Hugging Face transformers along with\nDeepSpeed for distributed training [38]. As described in [2], we used data parallelism\nwith DeepSpeed’s fused LAMB optimizer to train at scale on a dataset of 3 billion\nmolecules (i.e., the ﬁrst 6000 partitions of the full molecule dataset). Pre-training\nwas performed on the Summit supercomputer using 1000 nodes (6 Nvidia 16 GB\nV100 GPUs per node), with each partition of the dataset assigned to a single GPU.\nWe used a batch size of 80 molecules with 3 gradient accumulation steps per GPU,\nleading to a global batch size of 1.44 million. Pre-training was done for 7 epochs,\ntaking approximately 2.5 hours, and model validation was done using mask predic-\ntion on a hold-out set of molecules. The best validation accuracy occurred for the\nﬁnal epoch, and the resulting model weights were frozen for language model muta-\ntions in the ﬁxed strategy. The model weights were used as the initial conditions\nfor continued training in the adaptive strategy.\nBlanchard et al. Page 8 of 17\nSurrogate Model for Binding Aﬃnity\nIn addition to the heuristic metrics for drug molecules, synthesizability and drug-\nlikeness, we also used an ML model to predict protein binding aﬃnity for a given\ntarget, in this case the main protease of SARS-CoV-2. As described in previous\nwork [2], the binding aﬃnity model was generated by ﬁne-tuning language models\nfor both molecule and protein sequences. The output of the model is the predicted\nnegative log (base 10) of the binding aﬃnity. To convert to an aﬃnity score for\nﬁtness, we divided the prediction by 10 and clipped the resulting values between 0\nand 1. Although the validation and discussion of this model are beyond the scope\nof the current work, we chose it as an example to illustrate that our proposed opti-\nmization strategies can be applied to ﬁnd high-scoring candidates for both heuristic\nand ML surrogate scoring models.\nResults\nFixed and Adaptive Strategies for Molecule Generation\nBefore analyzing the impact of continued language model training on molecule\noptimization, we considered a simpler task: generating mutations for a ﬁxed set of\ninitial molecules. We implemented this task by using the genetic algorithm without\nselection (i.e. the parent population remains unchanged). During each generation,\nmutations are generated and the resulting unique molecules are saved for further\nanalysis. For the ﬁxed strategy, mutations are generated from the ﬁxed pre-trained\nmodel, while for the adaptive strategy, the language model is trained for 1 epoch\non the initial data in each generation before producing mutations.\nAs shown in Figure 2, we used two diﬀerent initial datasets, GDB9 [36] and Top\n(see Methods Section - Molecule Data). The mutation rate determines the fraction\nof tokens that are randomly masked during the generation of new molecules. Each\ngenetic algorithm simulation was run for 5 generations. For each run, the mean\ndrug-likeness and synthesizability scores were calculated for all unique molecules\nproduced in each generation outside of the original data. The histograms show an\nexample of the distributions for novel molecules with a metric value greater than\nzero produced from the ﬁnal generation of a single run with a mutation rate of 0.3.\nDue to the continued training of the language model, the mutations generated by\nthe adaptive strategy are much closer, in terms of synthesizability and drug-likeness,\nBlanchard et al. Page 9 of 17\n0.150.300.450.600.75Mutation Rate0.10.20.30.40.5Synthesizability0.150.300.450.600.75Mutation Rate0.30.40.50.6Drug-likeness\nSynthesizability0.0 0.5 1.0\nProb. Density0.0\n1.5\n3.0\nDrug-likeness0.0 0.5 1.0\nProb. Density0.03.0\n9.06.0\nGDB9 Data\nFixed LM\nAdaptive LM\n0.150.300.450.600.75Mutation Rate0.40.50.60.70.8Synthesizability0.150.300.450.600.75Mutation Rate0.50.60.70.8Drug-likenessProb. Density0.0\n3.0\n6.0\nDrug-likeness0.0 0.5 1.0\nProb. Density0.0\n3.0\n6.0\nSynthesizability0.0 0.5 1.0\nTop Data\nFixed LM\nAdaptive LM\nFigure 2 Distributions of molecules produced by a ﬁxed and adaptive approach. Two datasets\n(GDB9 and a custom dataset with the top scoring molecules for drug-likeness and\nsynthesizability) are used as training data. The ﬁxed approach (blue) generates a broad\ndistribution of molecule scores, while the adaptive approach (orange) more closely mimics the\ntraining dataset. Notice that for initial training data with low scores (i.e., GDB9), the adaptive\napproach produces lower scores on average than the ﬁxed approach, while the situation is reversed\nfor initial training data with high scores (i.e., Top).\nto the initial population of molecules. This leads to a decrease in typical values for\nthe GDB9 dataset. However, for the Top molecules, the adaptive strategy produces\nhigher scores. This result can be intuitively explained, as the ﬁxed model is biased\nby the data used in pre-training (i.e., the pre-trained model will tend to produce\nmutations that were prevalent in its training dataset). Continued training allows\nthe model to adapt to the new data, either GDB9 or Top.\nFixed and Adaptive Strategies for Molecule Optimization\nFor molecular optimization, the ability to adapt to a given initial dataset may or may\nnot be beneﬁcial. In the case of initial data with relatively low scores, we expect the\nadaptive strategy to slow down optimization, as the generated molecules with have\nscores similar to the poor initial data. To test this hypothesis, we applied a genetic\nalgorithm to optimize molecules for drug-likeness and synthesizability starting from\nthe GDB9 dataset. As shown in Figure 3, the adaptive strategy indeed results in\ndecreased ﬁtness relative to the ﬁxed strategy. The ﬁtness plot shown over ﬁve\ngenerations was generated with a mutation rate of 0.3. Furthermore, the adaptive\nstrategy produced less valid molecules and less accepted molecules (i.e., molecules\naccepted into the population during selection) for all mutation rates.\nThe same genetic algorithm applied to the Top dataset produces the opposite re-\nsults in terms of ﬁtness. Here, the adaptive strategy outperforms the ﬁxed strategy\nBlanchard et al. Page 10 of 17\n0.150.300.450.600.75Mutation Rate0.01.02.03.0Valid (105)\n0.150.300.450.600.75Mutation Rate0.0\n3.0\n6.0Accepted (104)\n0.150.300.450.600.75Mutation Rate0.7\n0.8\n0.9Final Fitness01234Generations50.750.80\n0.90Fitness0.85\nMutation Rate0.150.300.450.600.75Mutation Rate0.01.02.03.0Valid (105)4.0\n0.150.300.450.600.750.0\n4.0\n8.0Accepted (104)\n0.150.300.450.600.75Mutation Rate0.6\n0.7\n0.8Final Fitness01234Generations50.2\n0.5Fitness\n0.8\nGDB9 Data\nFixed LM\nAdaptive LM Top Data\nFixed LM\nAdaptive LM\nFigure 3 Optimization of molecules for drug-likeness and synthesizability produced by a ﬁxed and\nadaptive approach. Two datasets (GDB9 and a custom dataset with the top scoring molecules for\ndrug-likeness and synthesizability) are used as initial data. The ﬁxed approach (blue) results in a\nfaster increase in ﬁtness, along with greater valid and accepted molecules for the GDB9 dataset.\nFor the top dataset, however, the adaptive approach leads to a faster increase in ﬁtness along\nwith greater accepted molecules.\nfor all mutation rates considered. Interestingly, although the adaptive strategy pro-\nduces fewer valid molecules for most mutation rates (similar to the GDB9 dataset),\nit produces more accepted molecules in all cases. The decrease in valid molecules\ncan be understood as adaptive training leading to possible issues with over-ﬁtting\nthe current dataset, rather than learning from the large compound library used for\npre-training. However, the increase in accepted molecules suggests that molecular\nrearrangements learned from a high scoring dataset can improve ﬁtness optimiza-\ntion despite the decrease in valid molecules. For the following analysis, we ﬁxed the\nmutation rate to 0.3 and focused on ways to use the ﬁxed and adaptive strategies\ntogether for molecular design.\nCombining Fixed and Adaptive Strategies\nThe trade-oﬀ in performance for the ﬁxed and adaptive strategies, depending on the\ndistribution of values in the initial dataset, suggests that mixing ﬁxed and adaptive\nstrategies may be useful for molecular optimization. For a new optimization task, a\npreviously optimized dataset will likely not exist to serve as an initial population.\nIn many cases, generating a reasonably optimized dataset may be the entire goal\nof applying the optimization procedure. Therefore, we assume that the case with\npoorly optimized initial data, similar to GDB9, is more representative of a typical\nmolecular design problem. In this case, our results have shown that the ﬁxed strategy\nBlanchard et al. Page 11 of 17\noutperforms the adaptive strategy for optimization. However, as the ﬁtness of the\npopulation increases, we expect that the adaptive strategy may provide a better\nalternative to optimize ﬁtness.\nTo test this hypothesis, we implemented various schedules for combining the ﬁxed\nand adaptive strategies. As show in Figure 4, the ﬁxed strategy was used initially\nand then replaced by the adaptive strategy after a speciﬁed number of generations.\nAs expected, the optimal strategy involves a combination of the two strategies,\nwith ﬁve generations of ﬁxed followed by 20 generations of adaptive. Interestingly,\nalthough the purely adaptive strategy (orange) increases much more slowly than\nthe purely ﬁxed strategy (blue), adaptive overtakes ﬁxed in terms of ﬁtness after\napproximately 15 generations. This suggests that the diﬃculties associated with\nﬁtting more closely to a poor initial dataset can be overcome with the ability to\nadapt to the population as ﬁtness increases.\n0510152025Generations with Fixed Model0.85\n0.90\n0.95Final Fitness\nGenerations5 101520250.80\n0.900.95Fitness0.85\n0 5 10 15 20 25\nFixed LM\nAdaptive LMGenerations\nFigure 4 Combining ﬁxed and adaptive approaches during optimization. The ﬁxed approach is\nused during optimization for 25 epochs. For comparison, the adaptive approach is used starting\nfrom the output population of the ﬁxed approach at diﬀerent generations. The highest ﬁtness is\nachieved in the case where the adaptive approach is used after 5 epochs of the ﬁxed approach.\nNotice that the adaptive approach starting from the same initial data as the ﬁxed approach\nachieves a higher ﬁtness after approximately 15 epochs.\nMolecular Optimization Using a Surrogate Model\nAll of the results we have shown so far have used heuristic functions to score\nmolecules (i.e., synthesizability and drug-likeness scores). However, molecular opti-\nmization applications may involve additional ML-based surrogate models for scor-\nBlanchard et al. Page 12 of 17\ning. For example, a ML model may be trained on a limited experimental dataset\nin order to search for molecules with related properties. Here, we use a previously\ntrained surrogate model, which is available for download [33], developed to predict\nbinding aﬃnity for a given protein and molecule. We ﬁx the protein sequence to the\nmain protease of SARS-CoV-2, as described previously [2], and generate a normal-\nized aﬃnity score to use in ﬁtness calculations. Due to the added computational\ncost of evaluating the surrogate model, genetic algorithm simulations were run in\nintervals of ﬁve generations. Upon restarting, the model weights were initialized to\nthe ﬁxed pre-trained model.\nBuilding oﬀ the results for optimization with heuristic metrics, we compare two\noptimization schedules. We ﬁrst apply the ﬁxed strategy for ﬁve generations. This\nis followed by the adaptive strategy for 20 generations, with the continued ﬁxed\nstrategy for comparison. As shown in Figure 5, the adaptive strategy results in\na substantial increase in ﬁtness over the ﬁxed strategy for optimization with the\nsurrogate model. By comparing the histograms for synthesizability, drug-likeness,\nand aﬃnity score, we determined that the increase in ﬁtness values was primarily\nthe result of increases to the aﬃnity score, suggesting that the adaptive strategy\nis particularly useful for optimizing the ML scoring model. We also show examples\nof molecules with diﬀerent values for the three metrics used during ﬁtness opti-\nmization. Beyond generating molecules with high values for all three metrics, the\nexamples show how changes in the chemical structure for a family of molecules\nresult in trade-oﬀs amongst synthesizability, drug-likeness, and aﬃnity score.\nDiscussion\nSequence-Only Models for Drug Design\nThe models presented in this work for both molecule generation and scoring rely only\non the molecular sequence (i.e., the SMILES is the only model input). A sequence-\nonly approach is in contrast to ML models that utilized many local and global\nfeatures (e.g. molecular ﬁngerprints) [3]. Simulation and modeling approaches out-\nside of ML, such as molecular dynamics and docking, use the full three-dimensional\nstructures of both the protein and molecule to predict binding aﬃnity [5]. The pri-\nmary strength and weakness, therefore, of sequence-only models is the simplicity of\nthe model input. By using SMILES, the model has no direct information concern-\nBlanchard et al. Page 13 of 17\nProb. Density0.04.0\n12.0\nDrug-likeness0.6 0.8 1.0\n8.0Mean: 0.86Mean: 0.88\nSynthesizability0.6 0.8 1.0\nProb. Density0.03.0\n9.06.0Mean: 0.83Mean: 0.83\nAffinity Score0.5 0.751.0\nMean: 0.66Mean: 0.71\nProb. Density0.04.0\n12.08.0\nGenerations05101520250.650.70\n0.80Fitness0.75 R\nS: 0.88 | D: 0.91 | A: 0.82S: 0.82 | D: 0.94 | A: 0.89\nS: 0.85 | D: 0.88 | A: 0.99\nFixed LM\nAdaptive LM\nFigure 5 Fixed and adaptive approaches to optimize ﬁtness given by the harmonic mean of\nsynthesizability, drug-likeness, and aﬃnity score. Changing to the adaptive approach after 5\ngenerations results in an increase in ﬁtness as shown by the histograms for drug-likeness and\naﬃnity score. The histograms were generated from the ﬁnal population for the runs with the\nhighest ﬁtness for ﬁxed and adaptive approaches. Sample molecules with similar chemical\nstructures are shown for the adaptive approach. Mutations proposed by the language model show\nhow modiﬁcations result in changes in the metrics used to calculate ﬁtness.\ning geometry or chemical properties. However, SMILES enables the model to be\nused to train and make predictions on data without known three dimensional struc-\ntures or previously calculated chemical properties, enabling searches and screening\nover large portions of chemical space. Furthermore, sequence-only models have been\nshown to compare favorably to more traditional approaches with manually deﬁned\nfeatures [39–41].\nMolecule Generation through Mutations\nIn this work we have considered molecule generation for design using a language\nmodel to generate mutations. This strategy diﬀers from other approaches to develop\ngenerative models, such as variational autoencoders [14, 42] and generative adver-\nsarial networks [15, 16]. The mutation strategy is dependent on an original molecule\nin which certain subsequences are changed rather than generating an entire molecule\nby sampling from a latent space or noise distribution. Although mutation relies upon\nan original molecule, and thus limits the amount of chemical space accessible for a\ngiven round of molecule generation, it has multiple beneﬁts. First, mode collapse\nshould not in principle present a problem for molecule generation through muta-\ntion. Because mutations are sampled from each molecule in the population, the full\ntraining set is represented in each generation of generated molecules. Second, each\nBlanchard et al. Page 14 of 17\nround of mutations can be manually inspected along with the scores for each respec-\ntive molecule, enabling a user to better understand the types of mutations being\ngenerated and their impact on ﬁtness. Furthermore, through multiple iterations of\nmutations and selection, large regions of chemical space can be explored [18], even\nthough a single iteration remains close to the original data.\nConclusions\nMasked language models coupled with genetic algorithms provide a useful frame-\nwork for molecular optimization. During tokenization and pre-training, the model\ndetermines commonly occurring chemical sequences and rearrangements that can\nbe leveraged for molecule generation through mutations. Furthermore, the language\nmodel can be reﬁned using continued training on populations of molecules selected\nfor desired properties. Here, we have shown that the continued training of a lan-\nguage model during genetic algorithm optimization provides a powerful approach\nto search for molecules according to both heuristic and ML model scoring functions.\nModels pre-trained on large compounds libraries serve as a useful starting point for\nboth initial optimization from a poorly optimized dataset and initial weights for\ncontinued training.\nAvailability of data and materials\nThe source code for this work can be found at https://code.ornl.gov/candle/mlmol in the adaptive-lm directory.\nCompeting interests\nThe authors declare that they have no competing interests.\nFunding\nThis research was funded by the AI Initiative, as part of the Laboratory Directed Research and Development\nProgram of Oak Ridge National Laboratory, managed by UT-Battelle, LLC, for the U.S. Department of Energy\n(DOE); the Exascale Computing Project (ECP) (17-SC-20-SC), a collaborative eﬀort of the U.S. Department of\nEnergy Oﬃce of Science and the National Nuclear Security Administration.\nAuthor’s contributions\nAll authors contributed in developing the concept for the study. A.E.B., D.B., J. Gounley, and J. Glaser developed\nthe code and trained the models. A.E.B. performed data analysis and generated the ﬁgures. All authors assisted in\nmanuscript preparation.\nAcknowledgements\nAn award of computer time was provided by the Innovative and Novel Computational Impact on Theory and\nExperiment (INCITE) program. This research used resources of the Oak Ridge Leadership Computing Facility at the\nOak Ridge National Laboratory, which is supported by the Oﬃce of Science of the U.S. Department of Energy under\nContract No. DE-AC05-00OR22725.\nBlanchard et al. Page 15 of 17\nAuthor details\n1Computational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN, 37831, USA.\n2National Center for Computational Sciences, Oak Ridge National Laboratory, Oak Ridge, TN, 37831, USA.\n3Biosciences Division, Oak Ridge National Laboratory, Oak Ridge, TN, 37831, USA. 4Chemical & Biomolecular\nEngineering, University of Tennessee, Knoxville, TN, 37996, USA.\nReferences\n1. Dong, E., Du, H., Gardner, L.: An interactive web-based dashboard to track COVID-19 in real time. The\nLancet Infectious Diseases 20(5), 533–534 (2020). doi:10.1016/S1473-3099(20)30120-1\n2. Blanchard, A.E., Gounley, J., Bhowmik, D., Chandra Shekar, M., Lyngaas, I., Gao, S., Yin, J., Tsaris, A.,\nWang, F., Glaser, J.: Language Models for the Prediction of SARS-CoV-2 Inhibitors\n3. Minnich, A.J., McLoughlin, K., Tse, M., Deng, J., Weber, A., Murad, N., Madej, B.D., Ramsundar, B., Rush,\nT., Calad-Thomson, S., Brase, J., Allen, J.E.: AMPL: A Data-Driven Modeling Pipeline for Drug Discovery.\nJournal of chemical information and modeling 60(4), 1955–1968 (2020). doi:10.1021/acs.jcim.9b01053\n4. Chen, H., Engkvist, O., Wang, Y., Olivecrona, M., Blaschke, T.: The rise of deep learning in drug discovery.\nDrug Discovery Today 23(6), 1241–1250 (2018). doi:10.1016/j.drudis.2018.01.039\n5. Acharya, A., Agarwal, R., Baker, M.B., Baudry, J., Bhowmik, D., Boehm, S., Byler, K.G., Chen, S.Y., Coates,\nL., Cooper, C.J., Demerdash, O., Daidone, I., Eblen, J.D., Ellingson, S., Forli, S., Glaser, J., Gumbart, J.C.,\nGunnels, J., Hernandez, O., Irle, S., Kneller, D.W., Kovalevsky, A., Larkin, J., Lawrence, T.J., LeGrand, S., Liu,\nS.-H., Mitchell, J.C., Park, G., Parks, J.M., Pavlova, A., Petridis, L., Poole, D., Pouchard, L., Ramanathan, A.,\nRogers, D.M., Santos-Martins, D., Scheinberg, A., Sedova, A., Shen, Y., Smith, J.C., Smith, M.D., Soto, C.,\nTsaris, A., Thavappiragasam, M., Tillack, A.F., Vermaas, J.V., Vuong, V.Q., Yin, J., Yoo, S., Zahran, M.,\nZanetti-Polzi, L.: Supercomputer-Based Ensemble Docking Drug Discovery Pipeline with Application to\nCovid-19. Journal of Chemical Information and Modeling 60(12), 5832–5852 (2020).\ndoi:10.1021/acs.jcim.0c01010\n6. Cho, E., Rosa, M., Anjum, R., Mehmood, S., Soban, M., Mujtaba, M., Bux, K., Moin, S.T., Tanweer, M.,\nDantu, S., Pandini, A., Yin, J., Ma, H., Ramanathan, A., Islam, B., Mey, A.S.J.S., Bhowmik, D., Haider, S.:\nDynamic proﬁling of β-coronavirus 3cl mpro protease ligand-binding sites. Journal of Chemical Information and\nModeling 61(6), 3058–3073 (2021). doi:10.1021/acs.jcim.1c00449. PMID: 34124899.\nhttps://doi.org/10.1021/acs.jcim.1c00449\n7. Chen, S.H., Todd Young, M., Gounley, J., Stanley, C., Bhowmik, D.: How distinct structural ﬂexibility within\nsars-cov-2 spike protein reveals potential therapeutic targets, 4333–4341 (2021).\ndoi:10.1109/BigData52589.2021.9671323\n8. Bhowmik, D., Gao, S., Young, M.T., Ramanathan, A.: Deep clustering of protein folding simulations. BMC\nBioinformatics 19(S18), 484 (2018)\n9. Yang, X., Wang, Y., Byrne, R., Schneider, G., Yang, S.: Concepts of Artiﬁcial Intelligence for Computer-Assisted\nDrug Discovery. Chemical Reviews 119(18), 10520–10594 (2019). doi:10.1021/acs.chemrev.8b00728\n10. Enamine REAL Database. https://enamine.net/compound-collections/real-compounds/real-database.\nAccessed: 2020-04-01 through https://virtual-ﬂow.org/\n11. Martins, I.F., Teixeira, A.L., Pinheiro, L., Falcao, A.O.: A Bayesian approach to in Silico blood-brain barrier\npenetration modeling. Journal of Chemical Information and Modeling 52(6), 1686–1697 (2012).\ndoi:10.1021/ci300124c\n12. Subramanian, G., Ramsundar, B., Pande, V., Denny, R.A.: Computational Modeling of β-Secretase 1 (BACE-1)\nInhibitors Using Ligand Based Approaches. Journal of Chemical Information and Modeling 56(10), 1936–1949\n(2016). doi:10.1021/acs.jcim.6b00290\n13. RDKit: Open-source cheminformatics. http://www.rdkit.org\n14. Jacobs, S.A., Moon, T., McLoughlin, K., Jones, D., Hysom, D., Ahn, D.H., Gyllenhaal, J., Watson, P.,\nLightstone, F.C., Allen, J.E., Karlin, I., Van Essen, B.: Enabling rapid COVID-19 small molecule drug design\nthrough scalable deep learning of generative models. International Journal of High Performance Computing\nApplications (2021). doi:10.1177/10943420211010930\n15. Blanchard, A.E., Stanley, C., Bhowmik, D.: Using GANs with adaptive training data to search for new\nmolecules. Journal of Cheminformatics 13(1), 4–11 (2021). doi:10.1186/s13321-021-00494-3\nBlanchard et al. Page 16 of 17\n16. De Cao, N., Kipf, T.: MolGAN: An implicit generative model for small molecular graphs. ICML 2018 workshop\non Theoretical Foundations and Applications of Deep Generative Models (2018)\n17. Eiben, A.E., Smith, J.E.: Introduction to Evolutionary Computing, 2nd edn. Springer, Springer-Verlag GmbH\nGermany (2015)\n18. Virshup, A.M., Contreras-Garc ´ ıa, J., Wipf, P., Yang, W., Beratan, D.N.: Stochastic voyages into uncharted\nchemical space produce a representative library of all possible drug-like compounds. Journal of the American\nChemical Society 135(19), 7296–7303 (2013). doi:10.1021/ja401184g\n19. Jensen, J.H.: A graph-based genetic algorithm and generative model/Monte Carlo tree search for the\nexploration of chemical space. Chemical Science 10(12), 3567–3572 (2019). doi:10.1039/c8sc05372c\n20. Brown, N., McKay, B., Gilardoni, F., Gasteiger, J.: A graph-based genetic algorithm and its application to the\nmultiobjective evolution of median molecules. Journal of Chemical Information and Computer Sciences 44(3),\n1079–1087 (2004). doi:10.1021/ci034290p\n21. Brown, N., Fiscato, M., Segler, M.H.S., Vaucher, A.C.: GuacaMol: Benchmarking Models for de Novo\nMolecular Design. Journal of Chemical Information and Modeling 59(3), 1096–1108 (2019).\ndoi:10.1021/acs.jcim.8b00839. 1811.09621\n22. Lameijer, E.W., Kok, J.N., B¨ ack, T., Ijzerman, A.P.: The molecule evoluator. An interactive evolutionary\nalgorithm for the design of drug-like molecules. Journal of Chemical Information and Modeling 46(2), 545–552\n(2006). doi:10.1021/ci050369d\n23. Nicolaou, C.A., Apostolakis, J., Pattichis, C.S.: De novo drug design using multiobjective evolutionary graphs.\nJournal of Chemical Information and Modeling 49(2), 295–307 (2009). doi:10.1021/ci800308h\n24. Lameijer, E.W., Kok, J.N., Back, T., Ijzerman, A.P.: Mining a chemical database for fragment co-occurrence:\nDiscovery of ”chemical clich´ es”. Journal of Chemical Information and Modeling 46(2), 553–562 (2006).\ndoi:10.1021/ci050370c\n25. Schneider, G., Lee, M.L., Stahl, M., Schneider, P.: De novo design of molecular architectures by evolutionary\nassembly of drug-derived building blocks. Journal of Computer-Aided Molecular Design 14(5), 487–494 (2000).\ndoi:10.1023/A:1008184403558\n26. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional transformers for\nlanguage understanding. NAACL HLT 2019 - 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies - Proceedings of the Conference\n1(Mlm), 4171–4186 (2019). 1810.04805\n27. Blanchard, A.E., Chandra Shekar, M., Gao, S., Gounley, J., Lyngaas, I., Glaser, J., Bhowmik, D.: Automating\nGenetic Algorithm Mutations for Molecules Using a Masked Language Model. IEEE Transactions on\nEvolutionary Computation (2022). doi:10.1109/TEVC.2022.3144045\n28. Weininger, D.: SMILES, a chemical language and information system. 1. Introduction to methodology and\nencoding rules. J. Chem. Inf. Comput. Sci. 28, 31–36 (1998). doi:10.1021/ci00057a005\n29. Schuster, M., Nakajima, K.: Japanese and korean voice search. In: 2012 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 5149–5152 (2012). doi:10.1109/ICASSP.2012.6289079\n30. Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey,\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser,  L., Gouws, S., Kato, Y., Kudo, T., Kazawa, H.,\nStevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O.,\nCorrado, G., Hughes, M., Dean, J.: Google’s Neural Machine Translation System: Bridging the Gap between\nHuman and Machine Translation, 1–23 (2016). 1609.08144\n31. Bickerton, G.R., Paolini, G.V., Besnard, J., Muresan, S., Hopkins, A.L.: Quantifying the chemical beauty of\ndrugs. Nature Chemistry 4(2), 90–98 (2012). doi:10.1038/nchem.1243\n32. Ertl, P., Schuﬀenhauer, A.: Estimation of synthetic accessibility score of drug-like molecules based on molecular\ncomplexity and fragment contributions. Journal of Cheminformatics 1(1), 1–11 (2009).\ndoi:10.1186/1758-2946-1-8\n33. jglaser/protein-ligand-mlp-1. https://huggingface.co/jglaser/protein-ligand-mlp-1\n34. Aizman, A., Maltby, G., Breuel, T.: High performance I/O for large scale deep learning. In: 2019 IEEE\nInternational Conference on Big Data (Big Data), pp. 5965–5967 (2019). IEEE\n35. Ramakrishnan, R., Dral, P.O., Rupp, M., Von Lilienfeld, O.A.: Quantum chemistry structures and properties of\nBlanchard et al. Page 17 of 17\n134 kilo molecules. Scientiﬁc Data 1, 1–7 (2014). doi:10.1038/sdata.2014.22\n36. gdb9 Dataset. http://deepchem.io.s3-website-us-west-1.amazonaws.com/datasets/gdb9.tar.gz. Accessed:\n2021-05-28\n37. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz,\nM., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.L., Gugger, S., Drame,\nM., Lhoest, Q., Rush, A.M.: Transformers: State-of-the-art natural language processing. In: Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 38–45.\nAssociation for Computational Linguistics, Online (2020).\nhttps://www.aclweb.org/anthology/2020.emnlp-demos.6\n38. Rajbhandari, S., Rasley, J., Ruwase, O., He, Y.: Zero: Memory optimizations toward training trillion parameter\nmodels. International Conference for High Performance Computing, Networking, Storage and Analysis, SC\n2020-Novem, 1–24 (2020). doi:10.1109/SC41405.2020.00024. 1910.02054\n39. Wang, S., Guo, Y., Wang, Y., Sun, H., Huang, J.: Smiles-Bert: Large scale unsupervised pre-training for\nmolecular property prediction. ACM-BCB 2019 - Proceedings of the 10th ACM International Conference on\nBioinformatics, Computational Biology and Health Informatics, 429–436 (2019). doi:10.1145/3307339.3342186\n40. Xue, D., Zhang, H., Xiao, D., Gong, Y., Chuai, G., Sun, Y., Tian, H., Wu, H., Li, Y., Liu, Q.: X-MOL:\nlarge-scale pre-training for molecular understanding and diverse molecular analysis. bioRxiv (2020).\ndoi:10.1101/2020.12.23.424259\n41. Kim, H., Lee, J., Ahn, S., Lee, J.R.: A merged molecular representation learning for molecular properties\nprediction with a web-based service. Scientiﬁc Reports 11(1), 1–9 (2021). doi:10.1038/s41598-021-90259-7\n42. G´ omez-Bombarelli, R., Wei, J.N., Duvenaud, D., Hern´ andez-Lobato, J.M., S´ anchez-Lengeling, B., Sheberla, D.,\nAguilera-Iparraguirre, J., Hirzel, T.D., Adams, R.P., Aspuru-Guzik, A.: Automatic Chemical Design Using a\nData-Driven Continuous Representation of Molecules. ACS Central Science 4(2), 268–276 (2018).\ndoi:10.1021/acscentsci.7b00572. 1610.02415",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7438622117042542
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5492309927940369
    },
    {
      "name": "Heuristic",
      "score": 0.5279057621955872
    },
    {
      "name": "Chemical space",
      "score": 0.5233123302459717
    },
    {
      "name": "Fitness landscape",
      "score": 0.5057923793792725
    },
    {
      "name": "Population",
      "score": 0.4961772859096527
    },
    {
      "name": "Language model",
      "score": 0.44819629192352295
    },
    {
      "name": "Machine learning",
      "score": 0.4302270710468292
    },
    {
      "name": "Genetic algorithm",
      "score": 0.42417973279953003
    },
    {
      "name": "Drug discovery",
      "score": 0.23316776752471924
    },
    {
      "name": "Bioinformatics",
      "score": 0.18163999915122986
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Demography",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1320553840",
      "name": "Amgen (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1289243028",
      "name": "Oak Ridge National Laboratory",
      "country": "US"
    }
  ]
}