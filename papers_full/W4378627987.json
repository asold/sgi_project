{
  "title": "A new deep learning architecture with inductive bias balance for transformer oil temperature forecasting",
  "url": "https://openalex.org/W4378627987",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4378629684",
      "name": "Manuel J. Jiménez-Navarro",
      "affiliations": [
        "Universidad de Sevilla"
      ]
    },
    {
      "id": "https://openalex.org/A4304247726",
      "name": "María Martínez-Ballesteros",
      "affiliations": [
        "Universidad de Sevilla"
      ]
    },
    {
      "id": "https://openalex.org/A2893272491",
      "name": "Francisco Martínez-Álvarez",
      "affiliations": [
        "Universidad Pablo de Olavide"
      ]
    },
    {
      "id": "https://openalex.org/A2153064588",
      "name": "Gualberto Asencio Cortés",
      "affiliations": [
        "Universidad Pablo de Olavide"
      ]
    },
    {
      "id": "https://openalex.org/A4378629684",
      "name": "Manuel J. Jiménez-Navarro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4304247726",
      "name": "María Martínez-Ballesteros",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2893272491",
      "name": "Francisco Martínez-Álvarez",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2153064588",
      "name": "Gualberto Asencio Cortés",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3185376699",
    "https://openalex.org/W2178310074",
    "https://openalex.org/W4205532383",
    "https://openalex.org/W1970018269",
    "https://openalex.org/W3090455890",
    "https://openalex.org/W4226034222",
    "https://openalex.org/W3140854437",
    "https://openalex.org/W3158937888",
    "https://openalex.org/W4206965408",
    "https://openalex.org/W3110622722",
    "https://openalex.org/W3081799531",
    "https://openalex.org/W2914393402",
    "https://openalex.org/W3177318507",
    "https://openalex.org/W4206173445",
    "https://openalex.org/W4206429903",
    "https://openalex.org/W4200472480",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W3179002366",
    "https://openalex.org/W3199325741",
    "https://openalex.org/W2064675550"
  ],
  "abstract": "Abstract Ensuring the optimal performance of power transformers is a laborious task in which the insulation system plays a vital role in decreasing their deterioration. The insulation system uses insulating oil to control temperature, as high temperatures can reduce the lifetime of the transformers and lead to expensive maintenance. Deep learning architectures have been demonstrated remarkable results in various fields. However, this improvement often comes at the cost of increased computing resources, which, in turn, increases the carbon footprint and hinders the optimization of architectures. In this study, we introduce a novel deep learning architecture that achieves a comparable efficacy to the best existing architectures in transformer oil temperature forecasting while improving efficiency. Effective forecasting can help prevent high temperatures and monitor the future condition of power transformers, thereby reducing unnecessary waste. To balance the inductive bias in our architecture, we propose the Smooth Residual Block, which divides the original problem into multiple subproblems to obtain different representations of the time series, collaboratively achieving the final forecasting. We applied our architecture to the Electricity Transformer datasets, which obtain transformer insulating oil temperature measures from two transformers in China. The results showed a 13% improvement in MSE and a 57% improvement in performance compared to the best current architectures, to the best of our knowledge. Moreover, we analyzed the architecture behavior to gain an intuitive understanding of the achieved solution.",
  "full_text": "Open Access\n© The Author(s) 2023. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate‑\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/.\nMETHODOLOGY\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80  \nhttps://doi.org/10.1186/s40537‑023‑00745‑0\nJournal of Big Data\nA new deep learning architecture \nwith inductive bias balance for transformer oil \ntemperature forecasting\nManuel J. Jiménez‑Navarro1*, María Martínez‑Ballesteros1, Francisco Martínez‑Álvarez2 and \nGualberto Asencio‑Cortés2 \nAbstract \nEnsuring the optimal performance of power transformers is a laborious task in which \nthe insulation system plays a vital role in decreasing their deterioration. The insulation \nsystem uses insulating oil to control temperature, as high temperatures can reduce \nthe lifetime of the transformers and lead to expensive maintenance. Deep learning \narchitectures have been demonstrated remarkable results in various fields. However, \nthis improvement often comes at the cost of increased computing resources, which, \nin turn, increases the carbon footprint and hinders the optimization of architectures. In \nthis study, we introduce a novel deep learning architecture that achieves a comparable \nefficacy to the best existing architectures in transformer oil temperature forecasting \nwhile improving efficiency. Effective forecasting can help prevent high temperatures \nand monitor the future condition of power transformers, thereby reducing unneces‑\nsary waste. To balance the inductive bias in our architecture, we propose the Smooth \nResidual Block, which divides the original problem into multiple subproblems to obtain \ndifferent representations of the time series, collaboratively achieving the final forecast‑\ning. We applied our architecture to the Electricity Transformer datasets, which obtain \ntransformer insulating oil temperature measures from two transformers in China. The \nresults showed a 13% improvement in MSE and a 57% improvement in performance \ncompared to the best current architectures, to the best of our knowledge. Moreo‑\nver, we analyzed the architecture behavior to gain an intuitive understanding of the \nachieved solution.\nKeywords: Electricity transformer, Insulate oil, Time series, Efficiency, Efficacy, \nForecasting, Deep learning\nIntroduction\nDemand forecasting is a challenging field that has been extensively studied in the lit -\nerature [1 , 2]. Long-term demand forecasting remains an open field where a perfect \nsolution has yet to be found. Poor demand prediction can damage power transform -\ners, which is why several studies focus on analyzing the state of power transformers. \nThe insulation oil is a crucial component in the maintenance of power transformers to \nregulate temperature, as high temperatures can reduce the useful life of transformer \n*Correspondence:   \nmjimenez3@us.es\n1 Department of Computer \nScience, University of Seville, \nSeville, Spain\n2 Data Science and Big Data \nLab, Pablo de Olavide University, \nSeville, Spain\nPage 2 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n[3, 4]. Forecasting the insulation oil helps to analyze the future state of the power \ntransformer. By setting a defined threshold, it becomes possible to alert when the \ntransformer insulation oil temperature is going to be harmful before it happens, or to \nevaluate the transformer’s safety and avoid unnecessary waste.\nHowever, forecasting the insulation oil presents a challenge in terms of both effi -\ncacy and efficiency. Efficacy is necessary to accurately predict future behavior, while \nefficiency is necessary to scale the problem to real-world scenarios. Different power \ntransformers may have different behaviors, which divides the problem into several \nsubproblems. Each subproblem requires a solution that may require large computa -\ntional resources to optimize. For this reason, the solution must be as effective and \nefficient as possible to improve the scalability of the solution.\nTime series data is one of the most common data types used in the industry. It is \ndefined as a set of records ordered by time. Time series forecasting has a significant \nimpact on society due to its importance in a variety of real-world applications such as \nelectricity, sales, air pollution, and more. The primary goal of time series forecasting \nis to predict future records to make plans, mitigate damages, save resources, and so \non. Past records are used to predict future records due to the causal nature of time \nseries. The number of features in a time series determines whether it is defined as \nunivariate, with one single feature, or multivariate, with several features [5 ]. In addi -\ntion, two types of forecasting can be considered: one-step, where the model predicts \none record in the future, and multi-step, where the model predicts several records in \nthe future.\nDeep learning is rapidly advancing, with new architectures being developed almost \nevery year that improve the state-of-the-art in supervised fields such as computer vision \n[6, 7], natural language processing [8, 9], and time series forecasting [10, 11]. How -\never, recent research suggests that achieving state-of-the-art results requires a signifi -\ncant increase in computational resources. This trend can be seen in recent studies that \nintroduce new architectures like the Transformer [12], which suffer from issues such as \nquadratic time complexity and high memory usage [13]. Typically, in public competi -\ntions, the best models are usually large architectures or ensembles of several architec -\ntures [14], which require more computational resources. Moreover, experts have been \nusing the carbon footprint metric for a long time to raise awareness about the environ -\nmental impact of training deep learning architectures. In addition, training, experimen -\ntation, and inference with large and/or multiple architectures can be challenging to scale \nin resource-limited environments, slowing down academic and industry development. \nFor these reasons, we focus on developing an architecture that can either improve or \ncompete with the current best architectures in terms of efficacy while reducing compu -\ntational resources.\nSince there are not many works that consider the problem of transformer oil tempera -\nture prediction in the literature [15] and reproducing/comparing the results of different \napproaches can be challenging. Therefore, we compared our proposed method using one \nof the most widely used dataset in this field, namely the Electricity Transformer Temper-\nature (ETT) dataset [13], which has been used by more than 50 works. In this context, \nthe most effective architectures correspond to the works that obtained the best results \nin terms of efficacy in this specific dataset, some of them are analyzed in the \"Related \nPage 3 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \nworks\" section. The results seem to indicate that our method obtains a remarkable effi -\ncacy and efficiency achieving the best results.\nWe present a novel deep learning architecture for multi-step forecasting, named \nSmooth Residual Connections Network (SRCNet), 1 which prioritizes efficacy and effi -\nciency. SRCNet employs convolution layers as building blocks and introduces a new \nmechanism known as smooth residual stacking. This mechanism builds upon the dou -\nbly stacking approach developed by Oreshkin et al. [16] and is analyzed in the \"Experi -\nmentation\" section, which has been shown to yield remarkable results when combined \nwith residual connections in convolutional layers, as demonstrated by Liu et  al. [17] \nin the SCINet architecture. By introducing an inductive bias, the proposed mecha -\nnism enhances the efficacy of SCINet without imposing any significant restrictions on \nthe architecture. The motivation for leveraging inductive bias will be discussed in the \n\"Methods\" section.\nOur method is compared to the approach proposed by Liu et al. [17], which, to our \nknowledge, reported the best results in the ETT dataset. In contrast to their work, we \nemploy a convolutional neural network with the smooth residual stacking mechanism, \nwhich has demonstrated remarkable improvements in terms of efficacy and efficiency in \nseveral scenarios. Additionally, we include the results of six baseline architectures in the \ncomparison due to their similarity to our approach.\nThe main contributions of our work are as follows:\n• We introduce a new smooth residual stacking mechanism that improves the efficacy \nand efficiency of the architecture in several scenarios.\n• We use a time series decomposition with less inductive bias than previous studies, \nwhich maintains both efficacy and efficiency.\n• Our application of the proposed architecture to the Electricity Transformer Temper-\nature datasets outperforms the compared architectures in terms of both efficiency \nand efficacy.\nThe remaining sections of this work are organized as follows. In the \"Related works \" sec-\ntion, we provide a review of recent literature on time series forecasting methods with a \nfocus on efficiency. The \"Methods\" section describes the proposed architecture in detail. \nThe Experiments section presents the datasets used, experimental settings, and evalua -\ntion metrics. Next, in the \"Results and discussion \" section, we present and analyze the \nresults obtained and the behavior learned by the model. Finally, we discuss the conclu -\nsions and future directions of this research.\nRelated works\nTo address the efficiency issue in making effective forecasting for ETT dataset, several \narchitectures have been developed and applied using deep learning approaches. One of \nthe main challenges in this dataset is the need for a large window size, which results \nin efficiency issues that considerably increase computational costs. Previous works have \n1 All the code employed in the experimentation has been included in the following repository: https://github.com/man -\njimnav/SRCNet.\nPage 4 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \nanalyzed this problem using two approaches: the sparse approach and the inductive bias \napproach.\nSparse approach\nA recently proposed approach is to use the self-attention mechanism [12] for time \nseries forecasting. However, it suffers from quadratic time complexity and high mem -\nory usage that make its application difficult. The sparse approach attempts to improve \nthe efficiency of the self-attention mechanism by adding downsampling of the records \nin the input window using some heuristic. In general, these studies are based on the \nsparse nature of the data, which may not be fully proven. Sparsity leads to an informa -\ntion bottleneck that limits the amount of information available based on some criteria. \nIn our work, we do not use a self-attention approach to avoid increasing computational \nresources or information loss by adding sparsity.\nThe first proposal applied to the ETT dataset was the Informer architecture developed \nby Zhou et al. in 2020, with the proposed ProbSparse Self-Attention [13]. This mecha -\nnism avoids storing and calculating the dot product between all queries and keys based \non the hypothesis that few pair query-keys contribute to most self-attention scores. \nUsing this sparsity hypothesis, the queries are truncated based on the top pairs with a \ngreater contribution from a subset of pairs selected by a probabilistic method.\nKlimek et  al. proposed QuerySelector in 2022 [18] as an improvement to Informer. \nThis architecture uses the same hypothesis as in the previous work, but instead of sub -\nsampling the pairs based on a probabilistic method, they subsample the query and keys \nbased on a defined hyperparameter. This hyperparameter allows one to improve the \ncontrol of the sparsity and the reproducibility of the problem.\nDu et  al. developed the SCformer architecture in 2022 [19] with the SCAttention \nmechanism. The architecture uses an encoder-decoder architecture similar to the origi -\nnal transformer replacing the self-attention mechanism with SCAttention. The key idea \nis to divide the input window into segments of the same size according to the period. \nAttention scores are computed using the correlation between each segment of queries \nand keys, which reduces the data precision, but reduces the computation.\nAnother approach to obtain a sparse representation of the input data would be to \napply some dimensionality reduction technique. Žagar et al. [20] applied the principal \ncomponent analysis to spectral data obtaining a remarkable reduction. Despite being \nwidely used, this process still required storing and processing all the time series data \nto build the principal components. In addition, mechanism like self-attention which \nrequires sequential data may not be used.\nInductive bias approach\nSome architectures introduce an inductive bias based on some hypothesis about the \nnature of the problem. For example, some solutions divide the problem into trend and \nseasonality, resulting in competitive results. However, the inductive bias may restrict \nsolutions not inside the solution subspace, degrading the efficacy and generality. The \neffect of inductive bias is discussed in the \"Methods \" section. In this work, we use \na different inductive bias approach, which does not impose a strong limitation on the \nproblem.\nPage 5 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \nWoo et al. proposed CoST [21], an approach to learning the trend and seasonality \nof time series through contrastive learning. This approach develops two contrastive \nloss functions and two components for trend and seasonality. The Trend Feature Dis -\nentangler (TFD) uses a mixture of autoregressive experts to represent the trend and \na variation of MoCo contrastive loss [22]. The Seasonal Feature Disentangler uses a \nlayer with learnable Fourier transformations and a contrastive loss for the amplitude \nand phase of each representation.\nWu et  al. proposed the Autoformer [23], an architecture that decomposes time \nseries into trend and seasonality using the series decomposition and autocorrela -\ntion block. The series decomposition block uses moving averages to extract the trend \nand seasonality from the input. The autocorrelation block is used on period-based \ndependencies using autocorrelation to group similar series. This allows the series to \nfocus on specific periods instead of specific records.\nLiu et  al. developed the Sample Convolution and Interaction Network (SCINet) \n[17], an architecture that takes advantage of multiple representations obtained at \ndifferent temporal resolutions. The principal block is the SCI-block which splits the \ntime series into even and odd records and extracts features for both using interaction \nlearning. Then, all the learned features are used to produce the final forecasting.\nIn some scenarios, the inductive bias is introduced by including information about \na physical model that supports the estimations. Kosanić et al. [24] developed a sur -\nvey where they illustrate different approaches to geophysics forecasting, in which the \ncombination of physical and statistical models is described, using examples. However, \nin some scenarios, the physical model may not be available, which limits the applica -\ntion of this inductive bias approach.\nMethods\nIn this section, we provide a detailed explanation of the main components and \nhypotheses of the architecture. Specifically, the \"Hypothesis \" subsection discusses \nthe principal hypothesis and motivation behind the architecture. The \"Nomenclature \" \nsubsection defines all symbols used to describe the architecture. The \"Embedding\" \nsubsection provides a detailed description of the embedding layer. Lastly, the \"Smooth \nResidual Block\" section introduces the basic component in the smooth residual stack -\ning mechanism.\nHypothesis\nInductive bias is used by some machine learning algorithms to design a method that \ncan solve a problem based on certain assumptions [25]. These assumptions impose \nrestrictions or regularization that prioritize a solution subspace over others. In the \ncontext of deep learning algorithms, these assumptions can be included in the archi -\ntecture design by the transformations applied by the learned weights [26].\nThe hypothesis of our proposed architecture relates to the effect of inductive bias \non the efficacy of the architecture, which we refer to as the inductive bias-efficacy bal -\nance hypothesis.\nPage 6 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \nHypothesis 1 (H1) Strong restrictions introduced by inductive bias may improve effi -\nciency by reducing the probability of finding a solution outside the prioritized solution \nspace.\nThe proposed mechanism is motivated by our hypothesis of bias-efficacy balance 1 \n(H1). The hypothesis suggests that incorporating an inductive bias based on assump -\ntions into the architecture design improves efficiency by prioritizing a solution subspace \nand penalizing or removing solutions that do not fit the design. This increases the likeli -\nhood of converging into a solution within the prioritized subspace. Conversely, an archi-\ntecture with less bias has to explore a larger solution space, potentially increasing the \ntime needed to converge to a local minimum.\nHowever, limiting or prioritizing the solution space does not guarantee that solutions \noutside the subspace perform worse than solutions inside it. This may lead to suboptimal \nsolutions, but mitigating the effect of inductive bias may help the architecture find an \noptimal solution that it could not previously discover. Our proposal aims to find a bal -\nance by incorporating enough inductive bias to improve efficiency without excessively \nprioritizing the suboptimal solution space, thereby preserving efficacy.\nPrevious studies have designed architectures for time series forecasting which decom -\npose the input sequence into its constituent parts of trend, seasonality, and noise. Such \narchitectures introduce a strong inductive bias that prioritizes a solution subspace which \ncan be modeled by the combination of these components. In our work, we propose a \ndecomposition of the input sequence that must satisfy the following properties: \n1 Each decomposition must remove the complexity of the input window by removing \nits smoothed version.\n2 The decomposition is performed sequentially, such that the next decomposition \ndepends on the previous decomposition.\n3 Each Smooth Residual Block focuses on one component without sharing informa -\ntion with other blocks.\n4 Smooth Residual Blocks collaborate using the different decompositions to produce \nthe final prediction.\nNomenclature\nTo understand the architecture, we need to detail the nomenclature used in this section \nand the following. The elements implied are as follows:\n• W denotes the number of past records used to feed the architecture and generate the \nforecasting, also known as window size.\n• D denotes the number of features in each record. Note that in univariate time series \nD = 1.\n• F denotes the embedding dimension size used in the embedding layer.\n• SW ,D denotes the input window, which is a set of W time-ordered records with D \nfeatures.\n• SW ,F denotes the encoded input window after embedding with F features.\nPage 7 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \n• SW ,F\nSmooth denotes the smoothed version of the encoded input window.\n• ˆSW ,F denotes the result of the substrate of the original encoded input and the smooth \nversion of the encoded input window.\n• H denotes the number of future steps to forecast, also known as the horizon size. \nNote that in one-step forecasting H = 1.\n• P denotes the number of predicted features. Note that it may or may not be equal to \nD.\n• N denotes the number of Smooth Residual Blocks used in the architecture.\n• OH,P denotes the partial forecast of the Smooth Residual Blocks. All partial forecasts \nare added to produce the final forecasts.\nEmbedding\nThe embedding layer is the first transformation applied to the input sequence, using a \ncausal convolution layer that considers only the previous records to apply the convolu -\ntion to a record at time t. To achieve causality, the input sequence is padded by replicat -\ning the first record K − 1 times, where K is the kernel size of the convolution layer. The \nembedding layer encodes the initial sequence with D features into a set of causal features \nF, where F is determined by the number of filters used in the convolution layer. Addi -\ntionally, a positional encoding is added to provide relative information about the input \nsequence.\nSmooth Residual Block\nThe Smooth Residual Block is a crucial component of the architecture. Its main pur -\npose is to decompose an input sequence SW ,F into N components, the sum of which is \nFig. 1 Diagram of the architecture developed and the Smooth Residual Block\nPage 8 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \nSW ,H . To achieve this, causal convolutions are used as the primary layer, as previous \nstudies have shown that Transformers do not outperform convolutions in time series \nforecasting [27].\nFigure  1 shows the complete architecture developed and details the Smooth Resid -\nual Block which constitutes the main component.\nThe initial Smooth Residual Block produces a partial prediction by generating the \nSW ,F\nSmooth component for the embedded input window. It is important to note that the \ndecomposition is applied to the encoded version of the input sequence, denoted as \nSW ,F . This allows the model to handle both univariate and multivariate time series, as \nall types are projected onto a latent space. Moreover, encoding the input may aid in \nidentifying a latent space that captures the time series in a more informative feature \nspace.\nThe component SW ,F\nSmooth is generated by the SmoothLayer which takes the previ -\nous sequence SW ,F as input and outputs the component SW ,F\nSmooth . Decomposition is \nachieved using moving averages, which allows us to obtain a smooth representation \nof the original sequence. By choosing an appropriate window size or repeating the \nmoving average several times, we can obtain the trend. However, choosing a shorter \nwindow size applied once can result in an intermediate function with a combination \nof trend and seasonality where the noise is reduced. We are interested in this repre -\nsentation because we do not want to restrict the decomposition of the input window \ninto trend and seasonality.\nAs we use moving averages, we need to define a hyperparameter for the window size \nin all Smooth Residual Blocks. The hyperparameter is the same for all layers except \nfor the last one, whose window size is one and the sequence remains unchanged. This \nmeans that the last layer focuses only on the residuals of all previous layers, mostly \nobtaining the noise of the original sequence.\nAfter the component SW ,F\nSmooth is generated in the first Smooth Residual Block, it \nis subtracted from the input sequence SW ,F . Then, the next block applies the same \nprocess using the residual ˆSW ,F as the input window. This step is essential because \nit allows the blocks to focus on a different aspect of the sequence. Additionally, it is \nimportant to standardize the input to maintain each component within the distribu -\ntion of the data when we subtract the component from the original sequence.\nThe component SW ,F\nSmooth is fed into two causal convolution layers that output the pre -\ndiction O H ,P\ni  for layer i . Finally, the output of each block is added to obtain the final \nprediction. As the total sum is used for the final prediction, the blocks collaborate \nusing their respective representations of the input. This collaboration, combined with \nthe decomposition explained above, motivates a collaborative division of the problem.\nExperimentation\nThis section provides details on the training and evaluation process of different mod -\nels for each dataset, along with the results obtained. The \"Datasets \" section analyzes \nthe datasets used in the experimentation. The \"Experimental settings \" section lists \nthe different models used for comparison and the hyperparameter space used for grid \nPage 9 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \nsearch. Finally, the \"Evaluation metrics \" section outlines the metrics proposed for \nevaluating the efficacy and efficiency of the models.\nDatasets\nThe data studied in this work comes from the Electricity Transformer Datasets [13], \nwhich includes data from two electricity transformers at two stations in China. The data \nwas collected on an hourly basis from July 2016 to July 2018. The target of both datasets \nis to forecast the transformer oil temperature (OT), which reflects the condition of the \npower transformer.\nThe data contains several features related to the transformer load. However, recent \nstudies have shown that incorporating these features leads to increased errors compared \nto the univariate version. Therefore, we have chosen to focus solely on using the trans -\nformer oil temperature as input without any additional features.\nFigure 2 shows the evolution of the transformer oil temperature in the datasets, while \nFig. 3 shows the autocorrelation and partial autocorrelation plots. ETTh2 shows greater \nvariance and stronger decays in the autocorrelation plot than the ETTh1 dataset.\nFig. 2 Transformer oil temperature variable in ETTh1 and ETTh2 datasets\nFig. 3 ETTh1 and ETTh2 autocorrelation plots (left) and partial autocorrelation plots (right)\nPage 10 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \nThe plots demonstrate a different behavior for both datasets, which encourages us to \nuse different architectures that model the behavior of each dataset independently. This is \none of the reasons for finding a method that is not only effective but also efficient.\nExperimental settings\nThe processing applied to the datasets is the same as proposed in the original work \n[13], using identical divisions for training, validation, and testing to ensure fair com -\nparisons with other architectures. The data is standardized using the training division \nto obtain the mean and standard deviation.\nTo ensure a fair comparison between the architectures explored, a grid search over \nthe hyperparameters has been selected as the optimization method. The hyperparam -\neters optimized during the grid search are defined in Table  1. This approach ensures \nthat all explored architectures use the same hyperparameter space.\nSome hyperparameters are not included in the grid search due to computational \nlimitations or because they do not significantly affect the efficacy of the model. A \nbatch size of 32 was chosen, since it is a standard value that generally yields good \nresults. The number of epochs was set to 100, which is more than enough for the \nselected architectures to converge. The model is considered to have converged if the \nvalidation error does not decrease for 10 consecutive epochs. Once the model con -\nverges, the training stops, and the weights with the lowest error are restored. A pre -\ndiction horizon of 24 future steps is considered sufficient. To reduce the influence of \nrandomness, all experiments for each combination of hyperparameters were repeated \nthree times, minimizing the impact of abnormal results.\nThe models selected to compare with SRCNet are: ARIMA as a classical method, \nthe Informer architecture which originally published the ETT dataset, a convolu -\ntional neural network (CNN) used as the baseline, NBEATS to compare our proposed \nmechanism with theirs, SCINet, known for its excellent efficacy results, and HLNet, \nthe predecessor of SRCNet.\nThe ARIMA and Informer architecture results are obtained from the work of Liu \net al. [17] as the experimentation is reproduced using the same settings.\nA CNN consists of a set of convolutional layers with a final fully-connected layer \nto transform the output into predictions. This simple model serves as a baseline for \ncomparing with the other methods.\nThe NBEATS [16] model is based on blocks called BEAT, which are made up of \na set of fully-connected layers. Each BEAT obtains a decomposition of the input \nsequence called the backcast and partial forecasting. The backcast is subtracted from \nTable 1 Hyperparameter space used during the grid search\nHyperparameter Values\nWindow size 24, 48, 72, 96, 120\nEmbedding size 8, 16\n# Neurons for each layer 8, 16, 32, 64\n# Layers 1, 2, 3\nKernel size for each layer 3, 5, 7\nPage 11 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \nthe window, and the result is fed into the next BEAT. The partial components pro -\nposed in the method decompose the window sizes between trend and seasonality. The \nfinal prediction is the sum of each forecast obtained in each BEAT.\nThe SCINet [17] model primarily uses convolutional layers to generate the forecast. \nThe main mechanism is interaction learning, which subsamples the input window and \ndivides it between odd and even sample positions. Interaction learning is applied to the \nsubsamples, generating the features that are concatenated to obtain the forecasting.\nThe HLNet [28] model employs the LSTM [29] as the primary layer. This model \ndivides the problem into difficulty levels, assigning each level to a specific layer with its \nown output. The final output feeds the representation of each level to the final fully-\nconnected layer.\nEvaluation metrics\nThis document considers two types of metrics: efficacy and efficiency. Efficacy metrics \nmeasure the accuracy of our predictions compared to the actual behavior of the time \nseries. Efficiency metrics, on the other hand, measure the resources required to develop \nand deploy the solution.\nThe notation used for the metrics is as follows:\n• Let t be the prediction moment.\n• Let H be the number of future events to forecast (horizon).\n• Let yt+h be the predicted value for the moment t + h.\n• Let ˆyt+h  be the real value for t + h.\n• Let N be the number of instances used to evaluate the metric.\nThe Mean Absolute Error (MAE) was used as the efficacy metric due to its simplicity. \nThis metric obtains the average over the absolute error for all instances.\nHere, |yt+h −ˆyt+h | denotes the absolute value of the difference between the prediction \nand the actual value. The error is averaged over all horizons.\nAs for efficiency metrics, we used the training time and the number of model param -\neters. Training time is important because it is the stage where the model consumes the \nmost resources, and we need to run several experiments to find the optimal hyperpa -\nrameters. The time is measured in minutes from the start of training to convergence of \nthe model, which is defined as the point where there is no improvement in the valida -\ntion dataset after 10 epochs. The number of model parameters is particularly relevant in \nresource-limited contexts where memory is scarce and large or multiple models are not \nfeasible.\n(1)MAE = 1\nN\nN∑\nn=1\n1\nH\nH∑\nh=1\n|yt+h −ˆyt+h |,\nPage 12 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \nResults and discussion\nThe \"Efficacy and efficiency analysis \" section discusses the results obtained in the evalu -\nation process. \"Output analysis \" section analyzes the behavior learned by our proposal \nshowing the partial predictions. Finally, \"Statistical test\" section applies a Bayesian statis-\ntical test to support the conclusions obtained.\nEfficacy and efficiency analysis\nThis section is divided into two subsections to analyze the performance of our pro -\nposed methodology: efficacy and efficiency. In the first section, the efficacy is studied \nby comparing our proposal with the architectures specified in \"Experimental settings \" \nsection, while in the second section the efficiency is examined focusing on the models \nwith the best efficacy obtained in previous section.\nEfficacy\nThe results are presented in terms of standardized efficacy and efficiency metrics. \nEach table displays the metrics for the models with the best efficacy found during the \ngrid search. As experiments for each combination of hyperparameters are repeated \nthree times, the average and standard deviation for the efficacy metric are reported.\nTable 2 displays the standardized MAE metrics for the best models for each window \nsize. The model with the best MAE for a given window size is indicated in bold, while \nthe second-best result is underlined. The table also highlights the best MAE for all \nwindow sizes in each dataset for the top two models in bold and with an asterisk.\nTable 2 MAE metric for the best models for every window size and dataset tested\na These results have been obtained from the Liu et al. work. Note that the concept of input window does not exists in ARIMA, \nwhich explains why there are repeated values in each window. In addition, randomness does not affect ARIMA which \njustifies there is no standard deviation\nDataset Window ARIMA Informera CNNa HLNet NBEATS SCINet SRCNet\nETTh1 24 0.290 0.247 0.172 \n(±0.030)\n0.197 \n(±0.014)\n0.201 ( \n±0.012)\n0.186(±0.006) 0.138 \n(±0.010)\n48 0.167(±0.015)0.193 \n(±0.019)\n0.210 \n(±0.043)\n0.166 \n(±0.021)\n0.137 \n(±0.006)\n72 0.139(±0.013) 0.202 \n(±0.007)\n0.152 \n(±0.009)\n0.149 \n(±0.013)\n0.131 \n(±0.003)\n96 0.144(±0.007) 0.205 \n(±0.014)\n0.149 \n(±0.013)\n0.146 \n(±0.010)\n*0.129 \n(±0.001)\n120 0.156 \n(±0.012)\n0.192 \n(±0.007)\n0.15 \n(±0.018)\n*0.138 \n(±0.010)\n0.131 \n(±0.004)\nETTh2 24 0.433 0.240 0.205 \n(±0.008)\n0.333 \n(±0.016)\n0.197 \n(±0.003)\n0.198(±0.006) 0.198(±0.006)\n48 0.198(±0.002)0.339 \n(±0.011)\n0.201 \n(±0.003)\n0.201 \n(±0.009)\n0.195 \n(±0.005)\n72 0.195 \n(±0.004)\n0.334 \n(±0.010)\n0.202 \n(±0.002)\n0.196 \n(±0.009)\n0.189 \n(±0.003)\n96 0.200 \n(±0.005)\n0.338 \n(±0.003)\n0.207 \n(±0.003)\n0.197(±0.004) *0.193 \n(±0.002)\n120 0.201 \n(±0.005)\n0.339 \n(±0.004)\n0.206 \n(±0.002)\n0.198(±0.004) 0.194 \n(±0.005)\nPage 13 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \nAs shown, SRCNet consistently delivers the best results across all datasets and win -\ndow sizes, followed by SCINet. On the other hand, HLNet yields a higher error rate \nthan all other models, indicating that the LSTM used inside is not suitable for achiev -\ning accurate results.\nIn the ETTh1 dataset, using window sizes of 24 and 48 appears to be insufficient to \nobtain all the information necessary for accurate forecasts, particularly for SCINet, \nNBEATS, and CNN, which seem to have a great difference in MAE with larger win -\ndow sizes. In contrast, SRCNet and HLNet exhibit less variation between shorter and \nlonger window sizes. Additionally, there is a clear relationship between window size \nand MAE for SRCNet and SCINet. Nevertheless, SCINet requires a window size of \n120 to achieve the same MAE as SRCNet, which consumes five times more resources \nduring inference. Moreover, SRCNet obtains the lowest standard deviation between \nexperiments, which is ten times lower than that of SCINet in the best-case scenario.\nIn the ETTh2 dataset, the MAE is higher than that of ETTh1, indicating that this \ndataset is more challenging to predict. There is no apparent correlation between win -\ndow size and the error obtained. In general, larger window sizes, such as 96 and 120 \ndo not appear to improve the efficacy of the models. The best performance is achieved \nin CNN, SRCNet, and SCINet with a window size of 72, while HLNet and NBEATS \nperform best with a window size of 24. Furthermore, the difference between the best \nand worst MAE is lower in ETTh2 than in ETTh1. Additionally, the standard devia -\ntion between experiments appears to be lower for all models. Specifically, NBEATS \nand SRCNet have the lowest standard deviation, followed by SCINet. When compar -\ning the configuration with the best efficacy, the standard deviation for NBEATS and \nSRCNet is three times better than that of SCINet.\nFigure  4 shows the MAE for all models trained on the ETTh1 and ETTh2 datasets. \nThe box plots show the distribution of the MAE obtained for each hyperparameter \ncombination during the grid search. The figure seems to indicate that the results \nobtained by the CNN in Table  2 are outliers, which are not representative of the com -\nplete distribution shown in this figure.\nFor the ETTh1 dataset, CNN exhibits the highest interquartile range with a decreas -\ning median as the window size increases. HLNet has a smaller interquartile range, \nbut the model does not appear to take advantage of more samples in the window; \nFig. 4 MAE for each model and window size\nPage 14 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \nthe median remains almost constant regardless of the window size. NBEATS seems \nto improve efficacy as the window size increases, but the results are still worse than \nthose of SRCNet, which performs better overall, particularly for larger window sizes. \nSCINet appears to have the better median and interquartile range for short window \nsizes, but SRCNet outperforms SCINet for larger window sizes.\nIn the ETTh2 dataset, errors are greater than in ETTh1, as shown in Table  2. HLNet \nachieves the worst efficacy by a large margin, regardless of window size. The best \nmodels are NBEATS, SCINet, and SRCNet, but the median for NBEATS decreases as \nthe window size increases, suggesting that larger window sizes introduce noise that \nharms efficacy. In general, SRCNet obtains the global minimum, followed by SCINet, \nwithout a strong relationship between window size and error.\nEfficiency\nTable 3 displays the training times required for the best models to converge, analyzed \nfor each window size and dataset. The training time with the lowest value for each \nTable 3 Time required (minutes) to train the two models with the best efficacy\nDataset Window size SCINet SRCNet\nETTh1 24 1.10 0.50\n48 2.14 1.26\n72 2.81 1.90\n96 3.70 1.01\n120 1.75 1.04\nETTh2 24 3.97 3.23\n48 5.23 1.20\n72 5.60 1.57\n96 5.05 1.17\n120 7.24 1.25\nFig. 5 Training time (in minutes) for each model and window size\nPage 15 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \nwindow size is highlighted in bold. SRCNet, in general, shows better training times \ncompared to SCINet.\nIn ETTh1, SCINet exhibits the poorest efficiency for all window sizes, indicating \nthat the mechanism used in the architecture is highly inefficient. However, SRCNet \nachieves better results than SCINet with a time reduction of over 50.\nIn ETTh2, SCINet has the highest training times for each window size, which has \nincreased significantly compared to the times obtained in ETTh1. This suggests that con-\nvergence in ETTh2 is considerably more challenging than in ETTh1. However, SRCNet \ndoes not show any significant increase in training time, except for a window size of 24 h.\nFigure 5 illustrates the training time (in minutes) for all trained models.\nFor the ETTh1 dataset, CNN exhibits the lowest training times for window sizes of 24, \n48, and 72. However, for larger window sizes, the training time increases significantly. \nHLNet displays the same trend as CNN, with increasing training times for larger win -\ndow sizes. In contrast, NBEATS shows a different behavior, with lower training times for \nlarger window sizes. While SRCNet does not achieve better training times than faster \nmodels like CNN or NBEATS, it remains almost constant, regardless of the window size.\nFor the ETTh2 dataset, NBEATS has the lowest median training time for all window \nsizes, followed by CNN. HLNet and SRCNet exhibit slightly greater training times for \nwindow sizes of 24 and 48, which then slightly decrease.\nIn general, the size of the training window does not significantly affect the time \nrequired for modeling until convergence. SCINet has the highest training times com -\npared to all other models, regardless of the window size.\nOutput analysis\nTo gain a deeper understanding of the behavior learned by SRCNet, we analyze the par -\ntial predictions generated by the model. We used the best hyperparameters obtained for \nthe ETTh1 dataset, and partial predictions were obtained from the test set.\nFigure  6 illustrates the real behavior of the time series (red), the final prediction of \nSRCNet (dashed blue), and the partial predictions obtained from the three Smooth \nResidual Blocks (represented as level 0, 1, and 2) in green shades. The first block appears \nto predict a baseline for the prediction, almost matching the mean of the future records. \nFig. 6 Output produced by SRCNet for an instance of ETTh1 dataset. The forecast includes the output \nproduced by the levels of SRCNet, which is summed up as the final forecast\nPage 16 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \nThe second block focuses on capturing the global shape of the prediction, with a particu-\nlar emphasis on the ups and spikes, while the last block seems to concentrate on predict-\ning the downs.\nIn the overall prediction, as observed in Fig.  7, the first block produces an initial \nestimate of the predictions. The second and third blocks appear to build on this base \nprediction, focusing on the spikes of the prediction. Specifically, the second block \nobtains a global representation of the spikes, while the last block appears to capture \nsmaller changes.\nStatistical tests\nIn this section, we apply a statistical test to support our conclusions based on the \nresults obtained during the grid search. We use the Bayesian version of the Wilcoxon \nsigned-rank test proposed by Benavoli et al. [30] due to the well-known issues with \np-values in hypothesis testing [30].\nFor each combination of hyperparameters, the Bayesian test compares each pair of \nmetrics for models A and B, producing three output values. The first value represents \nthe probability that model A is better than model B (p(A>B)), the second value repre -\nsents the inverse scenario (p(B>A)), and the third value represents the probability that \nmodel A and B are similar (p(Rope)). To establish the range of statistical similarity, we \nFig. 7 Output produced by SRCNet for the complete test set of the ETTh1 dataset. The forecast includes the \noutput produced by the SRCNet levels, which summarizes the final forecast\nTable 4 Results of Bayesian analysis comparing the efficacy of all reference models with SRCNet\nDataset Reference p (reference>SRCNet) p (Rope) p \n(SRCNet>reference)\nETTh1 CNN 1 0 0\nNBEATS 1 0 0\nHLNet 1 0 0\nSCINet 1 0 0\nETTh2 CNN 1 0 0\nNBEATS 1 0 0\nHLNet 1 0 0\nSCINet 1 0 0\nPage 17 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \nneed to set a threshold, which will be determined after analyzing the results for each \nmetric.\nAs the selected method is a non-parametric paired test, we need to obtain each pair \nof results for each combination of hyperparameters in both datasets. However, each \nmodel has a different number of hyperparameters, and some are shared while others \nare not. Therefore, we only select the shared hyperparameters to obtain the pairs, and \nthen choose the best combination of non-shared hyperparameters.\nTable 4 presents the results of the Bayesian test applied to WAPE. The probabilities \nthat the reference model has a greater WAPE are represented by p(reference>SRCNet), \nwhile the contrary is represented by p(SRCNet>reference), and the probability of non-\nsignificant changes is p(Rope).\nThe statistical test used a threshold of 0.1% of the relative error to determine the range \nof statistical similarity. This means that results with an error difference of less than 0.1% \nare considered similar. The test indicates a 100% probability in all cases, indicating that \nSRCNet performs better than the reference models and generally achieves lower errors.\nTable 5 displays the Bayesian test applied to the efficiency metric. Once again, the \nprobabilities are shown as p(reference>SRCNet) for the probability that the reference \nmodel has a longer training time, p(SRCNet>reference) for the opposite scenario, and \np(Rope) for the probability of non-significant changes.\nIn this case, a 60-second threshold was selected, which means that training times \nwith differences of less than 60  seconds are considered similar. The results for CNN, \nNBEATS, and HLNet indicate that they are generally similar to SRCNet across all data -\nsets. Despite the fact that SCINet has the closest efficacy results to SRCNet, its efficiency \nis clearly worse in both datasets.\nConclusions and future works\nSRCNet has been demonstrated to outperform one of the best methods reported in \nthe literature for this dataset [17] in terms of efficacy, while also improving efficiency. \nEfficiency was achieved using the approach of decomposing the input window, which \ndiffered from conventional trend and seasonality decomposition. This supports the \nhypothesis that reducing bias in the architecture may improve efficacy, as greater free -\ndom allows for a more optimal data decomposition. The introduction of the Smooth \nResidual Block also provided the necessary amount of bias to enhance efficacy without \nTable 5 Results of Bayesian analysis comparing the efficiency for all reference models with SRCNet\nDataset Reference p  (reference>SRCNet) p (Rope) p \n(SRCNet>reference)\nETTh1 CNN 0 1 0\nNBEATS 0.31 0.69 0\nHLNet 0.25 0.75 0\nSCINet 1 0 0\nETTh2 CNN 0 1 0\nNBEATS 0 1 0\nHLNet 0.02 0.98 0\nSCINet 1 0 0\nPage 18 of 19Jiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \ncompromising efficiency. In future work, an extended version of the architecture could \nbe explored to assign importance to each block and reduce the impact of noise in the last \nblock, if necessary. Additionally, an investigation into the impact of inductive bias on \narchitectures could be conducted to assess their performance in terms of generalization, \ndata scaling, and efficiency.\nAbbreviations\nCNN  Convolutional neural network\nLSTM  Long short‑term memory\nHLNet  Hierarchical learning network\nMAE  Mean absolute error\nSRCNet  Smooth residual connection network\nSCINet  Sample convolution and interaction network\nETTh1  Hourly electricity transformer temperature dataset for trasformer 1\nETTh2  Hourly electricity transformer temperature dataset for trasformer 2\nOT  Transformer oil temperature\nAcknowledgements\nThe authors would like to thank the Spanish Ministry of Science and Innovation for the support under the projects \nPID2020‑117954RB and TED2021‑131311B, the European Regional Development Fund and Junta de Andalucía for pro‑\njects PY20‑00870, PYC20 RE 078 USE and UPO‑138516.\nAuthor contributions\nMJJN: conceptualization, methodology, software development, writing. MMB: writing reviewing, statistical test analysis, \nsupervision. FMA: writing reviewing, validation, supervision. GAC: data curation, writing reviewing, supervision. All \nauthors read and approved the final manuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nThe dataset has no restrictions that all data can be acquired at the related site.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 20 October 2022   Accepted: 2 May 2023\nReferences\n 1. Román‑Portabales A, López‑Nores M, Pazos‑Arias JJ. Systematic review of electricity demand forecast using ANN‑\nbased machine learning algorithms. Sensors. 2021; 21(13):4544.\n 2. Martínez‑Álvarez F, Troncoso A, Asencio‑Cortés G, Riquelme JC. A survey on data mining techniques applied to \nelectricity‑related time series forecasting. Energies. 2015; 8(11):13162–93.\n 3. Thiviyanathan VA, Ker PJ, Leong YS, Abdullah F, Ismail A, Jamaludin ZM. Power transformer insulation system: a \nreview on the reactions, fault detection, challenges and future prospects. Alex Eng J. 2022; 61(10):7697–713.\n 4. Dursun K. Oil and winding temperature control in power transformers. In: Proceedings of the International Confer‑\nence Power engineering, energy and electrical drives, 2013; pp. 1631–1639.\n 5. Beeram S.R, Kuchibhotla S. Time series analysis on univariate and multivariate variables: a comprehensive survey. In: \nProceedings of the International Conference on Communication Software and Networks, 2021; pp. 119–126.\n 6. Tabrizchi H, Razmara J, Mosavi A, Varkonyi‑Koczy A.R. Deep learning applications for COVID‑19: a brief review. In: \nProceedings of the International Conference on Research and education: traditions and innovations, 2022; pp. \n117–130.\n 7. Alzubaidi L, Zhang J, Humaidi AJ, Al‑dujaili A, Duan Y, Al‑Shamma O, Santamaría J, Fadhel MA, Al‑Amidie M, Farhan \nL. Review of deep learning: concepts, CNN architectures, challenges, applications, future directions. J Big Data, 2021; \n8(1):53.\n 8. Al‑Maleh M, Desouki S. Arabic text summarization using deep learning approach. J Big Data. 2021; 7(1):56.\nPage 19 of 19\nJiménez‑Navarro et al. Journal of Big Data           (2023) 10:80 \n \n 9. Zewdu A, Yitagesu B. Part of speech tagging: a systematic review of deep learning and machine learning \napproaches. J Big Data. 2022; 9(1):10.\n 10. Aghdam M, Tabbakh SK, Chabok S, Kheirabadi M. Optimization of air traffic management efficiency based on deep \nlearning enriched by the long short‑term memory (LSTM) and extreme learning machine (ELM). J Big Data. 2021; \n8(1):54.\n 11. Shen J, Shafiq M. Short‑term stock market price trend prediction using a comprehensive deep learning system. J Big \nData. 2020; 7(1):66.\n 12. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A.N, Kaiser L, Polosukhin I. Attention is all you need. In: \nProceedings of the International Conference on Neural Information Processing Systems. 2017; pp. 6000–6010.\n 13. Zhou H, Zhang S, Peng J, Zhang S, Li J, Xiong H, Zhang W. Informer: beyond efficient transformer for long sequence \ntime‑series forecasting. In: Proceedings of the International Conference on Advancement of Artificial Intelligence. \n2021; pp. 11106–11115.\n 14. Makridakis S, Spiliotis E, Assimakopoulos V. M5 accuracy competition: results, findings, and conclusions. Int J Fore‑\ncast. 2022; pp. 1346–1364.\n 15. Sui J, Ling X, Xiang X, Zhang G, Zhang X, Transformer oil temperature prediction based on long and short‑term \nmemory networks. In: Proceedings of the International Conference on Big Data, 2021; pp. 6029–6031.\n 16. Oreshkin B.N, Carpov D, Chapados N, Bengio Y. N‑beats: Neural basis expansion analysis for interpretable time series \nforecasting. arXiv. 2019.\n 17. Liu M, Zeng A, Xu Z, Lai Q, Xu Q. Time Series is a Special Sequence: Forecasting with Sample Convolution and Inter‑\naction. arXiv. 2021.\n 18. Klimek J, Klimek J, Kraśkiewicz W, Topolewski M. Query selector‑efficient transformer with sparse attention. Softw \nImpacts. 2022; 11(1): 100187.\n 19. Du D, Su B, Wei Z. SCformer: segment correlation transformer for long sequence time series forecasting. In: Proceed‑\nings of International Conference on Learning Representations. 2022; pp. 1–12.\n 20. Žagar J, Mihelič J. Creation of attribute vectors from spectra and time‑series data for prediction model develop‑\nment. IPSI Trans Internet Res. 2019; 15(2):1054.\n 21. Woo G, Liu C, Sahoo D, Kumar A, Hoi S. CoST: Contrastive learning of disentangled seasonal‑trend representations \nfor time series forecasting. In: Proceedings of International Conference on Learning Representations. 2022; pp. 1–18.\n 22. He K, Fan H, Wu Y, Xie S, Girshick R. Momentum contrast for unsupervised visual representation learning. In: Proceed‑\nings of International Conference on Computer Vision and Pattern Recognition, 2020; pp. 9726–9735.\n 23. Wu H, Xu J, Wang J, Long M. Autoformer: Decomposition transformers with auto‑correlation for long‑term series \nforecasting. In: Proceedings of International Conference on  Neural Information Processing Systems. 2021; pp. 1–12.\n 24. Kosanić M, Milutinović V. A survey on mathematical aspects of machine learning in geophysics: The cases of \nweather forecast, wind energy, wave energy, oil and gas exploration. In: Proceedings of the International Confer‑\nence on Embedded Computing. 2021; pp. 1–6.\n 25. Goyal A, Bengio Y. Inductive biases for deep learning of higher‑level cognition. arXiv. 2020.\n 26. LI X, Grandvalet Y, Davoine F. Explicit inductive bias for transfer learning with convolutional networks. In: Proceed‑\nings of the International Conference on Machine Learning. Machine Learning Research. 2018; pp. 2825–2834.\n 27. Zeng A, Chen M, Zhang L, Xu Q. Are transformers effective for time series forecasting?. arXiv. 2022.\n 28. Jiménez‑Navarro MJ, Martínez‑Álvarez F, Troncoso A, Cortés GA. HLNet: A novel hierarchical deep neural network \nfor time series forecasting. In: Proceedings of International Conference on Soft Computing Models in Industrial and \nEnvironmental Applications (SOCO 2021). 2022; pp. 717–727.\n 29. Hochreiter S, Schmidhuber J. Long short‑term memory. Neural Comput. 1997; 9(8):1735–1780.\n 30. Benavoli A, Corani G, Demsar J, Zaffalon M. Time for a change: a tutorial for comparing multiple classifiers through \nBayesian analysis. J Mach Learn Res. 2017; 18(77):2653–88.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7641483545303345
    },
    {
      "name": "Computer science",
      "score": 0.673715353012085
    },
    {
      "name": "Architecture",
      "score": 0.5639054775238037
    },
    {
      "name": "Transformer oil",
      "score": 0.5446571111679077
    },
    {
      "name": "Electricity",
      "score": 0.46934688091278076
    },
    {
      "name": "Deep learning",
      "score": 0.4659838080406189
    },
    {
      "name": "Inductive bias",
      "score": 0.42663395404815674
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34910809993743896
    },
    {
      "name": "Machine learning",
      "score": 0.32544225454330444
    },
    {
      "name": "Multi-task learning",
      "score": 0.24063724279403687
    },
    {
      "name": "Electrical engineering",
      "score": 0.2315894365310669
    },
    {
      "name": "Task (project management)",
      "score": 0.14471283555030823
    },
    {
      "name": "Voltage",
      "score": 0.14295744895935059
    },
    {
      "name": "Engineering",
      "score": 0.13244935870170593
    },
    {
      "name": "Systems engineering",
      "score": 0.12350893020629883
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ]
}