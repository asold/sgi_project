{
  "title": "Optimizing Large Language Models: A Deep Dive into Effective Prompt Engineering Techniques",
  "url": "https://openalex.org/W4406984954",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2148565466",
      "name": "Minjun Son",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2748690604",
      "name": "Yun-jae Won",
      "affiliations": [
        "Korea Electronics Technology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2104664459",
      "name": "Sung-jin Lee",
      "affiliations": [
        "Soonchunhyang University"
      ]
    },
    {
      "id": "https://openalex.org/A2148565466",
      "name": "Minjun Son",
      "affiliations": [
        "Sungkyunkwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2748690604",
      "name": "Yun-jae Won",
      "affiliations": [
        "Korea Electronics Technology Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2104664459",
      "name": "Sung-jin Lee",
      "affiliations": [
        "Soonchunhyang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389109142",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4385569785",
    "https://openalex.org/W3210277894",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4385570481",
    "https://openalex.org/W4402670429",
    "https://openalex.org/W2807434717",
    "https://openalex.org/W2971896267",
    "https://openalex.org/W1996041053",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W2963091133",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W3194676777"
  ],
  "abstract": "Recent advancements in Natural Language Processing (NLP) technologies have been driven at an unprecedented pace by the development of Large Language Models (LLMs). However, challenges remain, such as generating responses that are misaligned with the intent of the question or producing incorrect answers. This paper analyzes various Prompt Engineering techniques for large-scale language models and identifies methods that can optimize response performance across different datasets without the need for extensive retraining or fine-tuning. In particular, we examine prominent Prompt Engineering techniques including In-Context Learning (ICL), Chain of Thought (CoT), Retrieval-Augmented Generation (RAG), Step-by-Step Reasoning (SSR), and Tree of Thought (ToT), and we apply these techniques to leading LLMs such as Gemma2, LlaMA3, and Mistral. The performance of these models was evaluated using the AI2 Reasoning Challenge (ARC), HellaSwag, Massive Multitask Language Understanding (MMLU), TruthfulQA, Winogrande, and Grade School Math (GSM8k) datasets across metrics such as BLEU, ROUGE, METEOR, BLEURT, and BERTScore. The experimental results indicate that the most suitable Prompt Engineering technique can vary depending on the characteristics of each dataset. Specifically, for datasets emphasizing mathematical and logical reasoning, Prompt Engineering strategies centered around CoT, SSR, and ToT were found to be advantageous. For datasets focusing on natural language understanding, ICL-centric strategies were more effective, while RAG-based strategies were beneficial for datasets where factual accuracy is crucial. However, it was also observed that the optimal combination of Prompt Engineering techniques could differ depending on the specific LLM, indicating that fine-tuning the Prompt Engineering approach to the model and dataset is essential for achieving the best performance. The findings indicate that as LLMs become more advanced, their reliance on Prompt Engineering (PE) techniques diminishes, yet the magnitude of their performance improvement when PE strategies are applied increases. Furthermore, these advanced models tend to depend less on ICL techniques while exhibiting a greater reliance on RAG strategies. It is also evident that implementing RAG with PE-based preprocessing yields superior performance enhancements compared to the mere application of RAG on raw data.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.4627068042755127
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I848706",
      "name": "Sungkyunkwan University",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I4210131650",
      "name": "Korea Electronics Technology Institute",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I24541011",
      "name": "Soonchunhyang University",
      "country": "KR"
    }
  ]
}