{
  "title": "Transformer and Convolutional Hybrid Neural Network for Seismic Impedance Inversion",
  "url": "https://openalex.org/W4391235357",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2229195243",
      "name": "Chun-yu Ning",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2109540786",
      "name": "Bangyu Wu",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2278029502",
      "name": "Baohai Wu",
      "affiliations": [
        "Xi'an Jiaotong University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2125294793",
    "https://openalex.org/W4256129314",
    "https://openalex.org/W2111248943",
    "https://openalex.org/W2116088582",
    "https://openalex.org/W2143504795",
    "https://openalex.org/W2107627556",
    "https://openalex.org/W6704517872",
    "https://openalex.org/W2042535472",
    "https://openalex.org/W6764582490",
    "https://openalex.org/W2129657353",
    "https://openalex.org/W4366377597",
    "https://openalex.org/W2952182982",
    "https://openalex.org/W6763595875",
    "https://openalex.org/W3048641434",
    "https://openalex.org/W3005407459",
    "https://openalex.org/W2968094316",
    "https://openalex.org/W2999581854",
    "https://openalex.org/W4210272445",
    "https://openalex.org/W3200923794",
    "https://openalex.org/W3090517003",
    "https://openalex.org/W3131047970",
    "https://openalex.org/W4295332190",
    "https://openalex.org/W2969839555",
    "https://openalex.org/W3089883154",
    "https://openalex.org/W2970977200",
    "https://openalex.org/W3186076733",
    "https://openalex.org/W3207208155",
    "https://openalex.org/W2966977996",
    "https://openalex.org/W2752782242",
    "https://openalex.org/W3034552520",
    "https://openalex.org/W4287888658",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3107331169",
    "https://openalex.org/W3106728613",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W4214893857",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W6798415342",
    "https://openalex.org/W3097777922",
    "https://openalex.org/W1971588070",
    "https://openalex.org/W6745995898",
    "https://openalex.org/W3213689740",
    "https://openalex.org/W2341896195",
    "https://openalex.org/W3098108457",
    "https://openalex.org/W2947704004"
  ],
  "abstract": "The inversion of elastic parameters especially P-wave impedance is an essential task in seismic exploration. Over the years, deep learning methods have made significant achievements in seismic impedance inversion, and convolutional neural networks (CNNs) become the dominating framework relying on extracting local features effectively. In fact, the elastic parameters temporal correlation consists of local and global characteristics, with the latter as a general trend in vertical direction due to gravity and diagenesis (vertical mechanical compression). Therefore, considering the excellent performance in capturing global dependencies of Transformer, we design an improved transformer encoder, a transformer and convolutional hybrid neural network (trans-CNN), for seismic impedance inversion. The designed network not only has the ability of transformer capturing global features with the facilitation of parallel computing but also the advantage of extracting local features of CNNs. With sparse well log data as labels, it can infer the absolute impedance from seismic data without an initial model. We also devise a relative time interval prediction self-supervised task to assist the network in better extracting seismic data features without adding any labels. Therefore, a multitask framework composed of self-supervised and supervised learning is used to train the network. We first conduct experiments on the Marmousi2 and overthrust model. The prediction profiles show that the proposed trans-CNN has better inversion and transfer learning ability than several comparable networks. We then test the proposed network on a field data, the experiments further suggest that trans-CNN can obtain stable inversion results with better horizontal continuity and high vertical resolution.",
  "full_text": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n1\nTransformerandConvolutionalHybridNeural\nNetworkforSeismicImpedanceInversion\nChunyuNing,BangyuWu,Member, IEEE andBaohaiWu\nAbstract—The inversion of elastic parameters especially P-wave\nimpedance is an essential task in seismic exploration. Over the\nyears, deep learning methods have made significant achievements\nin seismic impedance inversion, and Convolutional Neural\nNetworks (CNNs) become the dominating framework relying on\nextracting local features effectively. In fact, the elastic\nparameters temporal correlation consists of local and global\ncharacteristics, with the latter as a general trend in vertical\ndirection due to gravity and diagenesis (vertical mechanical\ncompression). Therefore, considering the excellent performance\nin capturing global dependencies of Transformer, we design an\nimproved Transformer Encoder, a Transformer and\nconvolutional hybrid neural network (Trans-CNN), for seismic\nimpedance inversion. The designed network not only has the\nability of Transformer capturing global features with facilitation\nof parallel computing, but also the advantage of extracting local\nfeatures of CNNs. With sparse well log data as labels, it can infer\nthe absolute impedance from seismic data without initial model.\nWe also devise a relative time interval prediction self-supervised\ntask to assist the network better extract seismic data features\nwithout adding any labels. Therefore, a multi-task framework\ncomposed of self-supervised and supervised learning is used to\ntrain the network. We first conduct experiments on the\nMarmousi2 and Overthrust model. The prediction profiles show\nthat the proposed Trans-CNN has better inversion and transfer\nlearning ability than several comparable networks. We then test\nthe proposed network on a field data, the experiments further\nsuggest that Trans-CNN can obtains stable inversion results with\nbetter horizontal continuity and high vertical resolution.\nIndex Terms—Impedance inversion, seismic data, self-supervised\nlearning, Transformer.\nI.INTRODUCTION\nEISMIC inversion is a process to obtain physical\npropertiesand spatial structure ofstratabased on seismic\nimage. It is playing a vital role in stratigraphic\ncharacterization of underground space and the evaluation of\nreservoir physical properties [1], [2]. P-wave impedance is a\nkey elastic parameter for exploration and identification of\nreservoir. It indicates the property of the reservoirs and is\nindispensableinhydrocarbonprediction.\nManuscript received ????; revised ????; accepted ????. This work was\nsupported in part by the Natural Science Basic Research Program of Shaanxi\nunderProgram2023-JC-YB-269,andinpartbytheNationalNaturalScience\nFoundation of China under Grant 41974122 (Corresponding author: Bangyu\nWu).\nChunyu Ning and Bangyu Wu are with the School of Mathematics and\nStatistics, Xi’an Jiaotong University, Xi’an, Shaanxi, 710049, China (e-mail:\nncy0312@stu.xjtu.edu.cn,bangyuwu@xjtu.edu.cn).\nBaohai Wu is with the The CGG GeoSoftware (Beijing), Beijing, 100016,\nChina(e-mail:baohaiwu@163.com).\nNowadays, for the seismic inversion methods,there are two\nmajor categories: model-driven and data-driven inversions.\nModel-driven seismic inversion usually relies on good low-\nfrequency initial models and precise physical priors, which\nincludes deterministic methods [3], [4], [5], [6], [7] and\nstochastic methods [8], [9]. Due to frequency band mismatch,\napproximation of physical principles (forward modeling) and\nunavoidable noise in field seismic data [10], [11], it is often\ndifficult for traditional model-driven methods to stably obtain\nhigh-resolutioninversionresults.Overthepastfewyears,with\nthevigorousgrowthofmachinelearningandbigdataanalysis,\ndata-driven inversion methods especially deep learning has\nachieved tremendous progress in seismic inversion. In 2019,\nDeep Neural Network (DNN) model was used to invert\nmultiple elastic parameters [12]. As the rapid growth of\nConvolutional Neural Networks (CNNs),they areinvestigated\nintensively to solve the seismic inversion problem. It showed\nthat CNNs have great potential in predicting high-accuracy\nimpedance from band-limited seismic signals [13], [14], [15].\nAlmost at the same time, the Full Convolutional Neural\nNetwork (FCN) was proposed, which substituted all the fully\nconnection layers with convolutional layers, and made a\nbreakthrough in seismic inversion [16]. Wuet al. incorporated\nresidual convolutions into FCN (FCRN) and transfer learning\nto improve model generalization ability for different synthetic\nmodels [17]. Zheng et al. extended FCRN to Multi-task\nlearning (MLT) impedance inversion with seismic data\nreconstruction as auxiliary task [18]. In addition, semi-\nsupervised methods such as CycleGAN, WcycleGAN and\ngeophysical-guided cycle-consistent GAN are also explored\nfor seismicinversiontasks[19],[20],[21],[22].\nThe above methods generally use convolution to extract\nfeatures in seismic data. Essentially, seismic traces can be\nregardedastimeseriesandthemagnitudeofsubsurfaceelastic\nparametersareincreasing in ageneral trend dueto gravity and\ndiagenesis (vertical mechanical compression), which shows\nlong range and global characteristics. In this regard, the\nRecurrentNeuralNetworks(RNNs)[23]and itsvariants Long\nShort-Term Memory (LSTM) [24], [25] and Gated Recurrent\nUnit(GRU) [26]weresuccessively utilized tomodelthelong-\nterm dependencein seismic sequences. GRU and bidirectional\nGRU were respectively combined with convolutionallayers to\nextract information of seismic traces, enhancing the\neffectiveness of inversion networks [27], [28]. Nevertheless,\nthe inherent gradient disappearance and inability to compute\nin parallel limit their capabilities for efficient applications.\nBesides, Mustafa et al. applied the Time Convolutional\nNetwork (TCN) to acoustic impedance inversion [29]. TCN\nS\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n2\nencapsulatesthestrengthsofRNNsandCNNsandcancapture\nlow frequency features with fewer network parameters.\nHowever, the above-mentioned sequence-to-sequence models\naregenerallyunabletoaccuratelyprocesslongsequences.The\nattention mechanism can preserve and utilize all the\ninformation of the input during decoding, thus mitigating this\nproblem. It assigns higher weights to the important and\nrelevant features, enhancing the accuracy of the output\nprediction.\nThe attention mechanism imitates human visual attention\nsystem, and acquires the target zonethat should befocused by\nsearching theoverallimage.Byadjusting theweight matrix of\nthe attention mechanism, information within and between\nchannels can be fetched according to the importance, such as\nSqueeze and Excitation Network (SE-Net) [30], Efficient\nChannel Attention Network (ECA-Net) [31]. Wu et al. added\nfeature-mapattentionandthechannel-attentionmechanismsto\nthe impedance inversion network, which greatly improved the\naccuracy of the proposed ResANet [32]. The self-attention\ntakes the correlation between input vectors as the entry point,\nmakingthenetwork extractfullscaleinformationaccordingto\nthe correlation between every two positions. Transformer\ntakes this a step further, based entirely on self-attention\nmodules, drawing multiple global dependencies within a\nsequence. Most importantly, Transformer is no longer focused\nsolely on temporal association within series, but pays more\nattentiontolearningmultiplesemanticinformation.\nIn2017,Transformer architecturewas proposedfor thefirst\ntime and applied to Natural Language Processing (NLP) [33].\nIts excellent performance in sequence modeling and machine\ntranslation has made it rapidly gain attention in various fields.\nIn 2018, Transformer was applied in computer vision [34]. In\n2020, the proposals of object detection model DETR [35] and\nimage classification model ViT [36] ignite the rapid\ndevelopment of Transformer. At present, Transformer has\nbeen widely used in image classification [37], [38], target\ndetection [39], [40], image segmentation [41], [42], [43] and\nother research directions, sweeping the entire field of\ncomputer vision. Moreand more scholars and engineers begin\nto take advantage of transformer as a powerful tool for\ndifferenttasks.\nConsidering the time dependence of seismic series and the\nmetric of Transformer, we propose to use Transformer for\nmodeling low-frequency trends between seismic trace and\nimpedance. We design an improved Convolution-augmented\nTransformer Encoder for seismic inversion by analogizing\ntime sampling points to words in NLP. In addition, we also\ndevise a self-supervised task to help network extract latent\neffective information without adding any labeled samples. On\nthe whole, we propose a Transformer and convolutional\nhybrid neural network, coined as Trans-CNN, for seismic\nimpedance inversion. Trans-CNN takes advantage of long-\nterm feature acquisition and parallel computing in\nTransformer, while also benefit from the local feature\nextraction ability of CNN. To strengthen the modelrobustness\nwhen labeled data is deficient, we introduce a self-supervised\nlearning task for the modeling of time relationship inside a\ntrace. Generally speaking, the proposed model is trained in a\nmulti-task framework for both self-supervised and supervised\nlearning, improving the accuracy of results even in case of\nlimitedlabeleddata.Inthefirsttask,thenetworkaims tolearn\nthe relative time interval inside seismic trace by self-\nsupervised learning.Whiletheother task,thenetwork predicts\nthe impedance by supervised learning. Due to improved\nTransformer Encoder and CNN, Trans-CNN can enhance\ninformation extraction ability and inversion accuracy while\nensuring efficiency. The inversion results on two synthetic\nmodel indicate that Trans-CNN can inverse impedance\nprofiles with detailed strata and also has better transfer\nlearning ability. For the field data, the inversion profile of\nTrans-CNN is more consistent with the well logs than several\ncomparablenetworks.\nThe full text is organized asfollows. Relevant theories used\nin Trans-CNN are introduced in section II. The network\nArchitectureandthelossfunctionareintroducedinsectionⅢ.\nSection Ⅳ demonstrates performance of Trans-CNN on the\nsynthetic and field data tests. Section Ⅴ and Ⅵ show\ndiscussionandconclusionrespectively.\nII.COMPONENTS OF TRANS-CNNNETWORK\nA. Positional Encoding\nInstead ofprocessingsequentialdatainchronologicalorder,\nTransformer computes in parallel. Location information needs\ntobeencodedinthedata,thatis,positionalencoding.Fortime\ndomain seismic inversion, it measures the distance between\nanytwo timepoints,andgeneralizestoinputembeddings with\nanylength.\nFor the vector at the position  in the sequence, the position\nvectorisdefinedas\n\n = (),  %2 = 0.\n(),  %2 = 1. (1)\nWherethefrequencyoftrigonometricfunction is\n =\n1\n10000(−1)/ ,  %2 = 0.\n1\n10000/ ,  %2 = 1.\n(2)\nHere,  is the channel number in convolutional layers, and\nrepresentstheinputdimensionofattentionmechanismaswell.\nB. Multi-head Self-attention\nIn Fig. 1, it shows the self-attention mechanism calculation\nflow. Data points in a trace are now represented by the vector\n ∈ 1×,where isthesame asabove.Each ismultiplied\nby the corresponding matrix to get query  ∈ 1× , key  ∈\n1× and value  ∈ 1× , where  denotes the number of\ncolumns in query and key. The vital step of the attention\nmechanismistocalculatethesimilarity, between  and,\nandcalculationisasfollows\n, = (\n\n\n\n). (3)\nFinally,wecangetthe\nℎ1 correspondingtothe\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n3\nFig. 1. Self-attentioncalculationflow(taking 1\nℎ1 asexample).\nFig. 2. Four-head Self-attention calculation flow (taking 1 as\nexample).\n\nℎ1 = j=1\n ,.\u0000 (4)\nThe self-attention mechanism in matrix form can be\nrepresentedasfollows\n(, , ) = (\n\n\n), (5)\nhere all the  ,  , are packed together into matrix  ∈\n×,  ∈ ×,  ∈ ×.  denotes the length of seismic\ntraces.\nForsingleself-attention,thereisone and corresponding\nto a seismic trace, and the information included in similarity\nmatrix between  and  is inadequate for complex geological\nstructureinseismicdata.Therefore,inordertoobtainstronger\ninformationextraction capability, thenumber of, and  is\nincreased inMulti-head Self-attention.Ingeneral,equationfor\nthe H-head Self-attention can be represented in a similar way\n[33]:\n(, , ) = (ℎ1, ⋯, ℎ), (6)\nℎ=(\n , \n , \n ), (7)\nwhere the Wm\nQ ∈ Rdk×Dk , Wm\nK ∈ Rdk×Dk , Wm\nV ∈ Rd×d and W ∈\nRHT×d arelearnedweightsofthelinearprojection.InFig.2,an\nexampleofFour-headSelf-attentionmoduleisillustrated.\nWe use the Multi-head Self-attention to enhance features\nextraction and especially long-term trends in seismic traces,in\norder to help the network achieve better accuracy and lateral\nFig. 3. Relativetimeintervalpredictionprocess.\ncontinuity results. The heads number in the Multi-head Self-\nattention is set to 4, the dimension of the Feed-forward layers\nto1024,andtheparametersdk,Dk to8.\nC. Relative Time Interval Prediction Self-Supervised Learning\nDeep learning is mainly to summarize the internal rules\nwith various level of representations, which usually requires\nthe support of large-scale training samples. In the field of\nseismic inversion, insufficient label data has always been a\nproblem for supervised deep learning method in practical\napplications due to limited well logs. We apply self-\nsupervision to mitigate this problem. Self-supervised learning\nwas initially used successfully in NLP as a pretext task to\nreplace expensive annotations and obtain supervision from\ntextinstead[44],[45].\nIn Trans-CNN, we use the measure of the relative location\nof temporal data random pairs as a self-supervised learning\ntask. In this process, the network obtains the temporal\ninformation of seismic traces and improves the inversion\nperformance without adding any additional labels [46]. Fig. 3\nshows the relative time interval self-supervised learning\nprocessofaseismictrace.Wecanselecttheℎ timesampling\npair, inatraceoflength ,and their truedistance isdefined\nas\n1\n =  − \n , ,  ∈ [1, ]. (8)\nAfter the trace passes through all encoding layers, each time\nsampling point is represented by a feature vector. Then we\ntakeoutthevector , andsend theminto P asarandom\npair.The P calculates 2\n= , \n\nto predict distance\nbetweentwopoints.\nInthispaper,thenumberofrandompairs isrelatedtothe\nseismic trace length . On synthetic models  is 100 and on\nthe field data  is 10. Finally, we define the relative time\nintervallossbyMeanAbsoluteError(MAE)as\n_ =\n1\n =1\n 1\n − 2\n\u0000 . (9)\nIII.TRANS-CNNNETWORK FOR SEISMIC IMPEDANCE\nINVERSION\nA. Network Architecture\nFig.4ashowsthedesignedarchitectureofTrans-CNN.This\nnetwork consists of five main submodules with name as\nBranch1, Branch2, Combination, Task1 and Task2. And each\nsubmodulefulfillsadifferentroleintheoverallnetwork.\nThe Branch1 submodule extracts local features from\nseismictracestomodelhigh-frequencydetails.Thefirstpart\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n4\n(a)\n(b)\nFig. 4. ArchitectureoftheTrans-CNN. (a)Trans-CNN;(b)Convolution-augmentedTransformerEncoder.\nof Branch1 is a convolutional layer with 32 kernels of size\n299 × 1 , which changes according to the length of source\nwavelet.Itis followed by aResidualBlock mainly consisting\nof two convolutional layers. One is 32 convolution kernels\nwith size 299 × 1 and the other is 3 × 1. In Residual Blocks,\nBatch Normalization (BN) is applied to accelerate the\ntraining speed of network after all convolutional layers, and\nthe Rectifified Linear Unit (ReLu) helps the network depict\ncomplexnonlinearrelationships.\nThe Branch2 submodule captures global dependencies of\nseismic traces and models their low-frequency content. In the\nBranch2, the convolutional layer and the Residual Block are\nto map the original seismic traces to corresponding feature\nmatrices. Then the feature matrices are added positional\ninformation by Positional Encoder and sent into Transformer\nEncoder for global features extraction. While Transformer\nfocuses on capturing long-term dependencies, it may ignore\nimportant relative-offset-based local correlations. Therefore,\nweimprovethestructureofTransformerEncoderbyaddinga\nconvolutional layer after attention to make the Encoder pay\nattention to global and fine-grained correlations at the same\ntime for more accurate inversion results. The details of\nconvolution-augmented Transformer Encoder are shown in\nFig. 4b. Multi-head Self-attention, as Fig. 2, is the first and\nmost important part of the Transformer Encoder. Then, a\nconvolutional layer of T kernels with size 3 × 1 is added in\nthe middle to capture the relative-offset-based local\ncorrelations,whereTisthelengthofseismictrace.TheFeed-\nforward layer follows at the end, which is consisting of two\nfully connected layers [47]. Moreover, the first part Multi-\nhead Self-attention and last part Feed-forward layer adopt\nresidual connection, Layer Normalization (LN) and ReLu,\nwhilethemiddleconvolutionallayerusesBN.\nIn the Combination submodule, there is a Residual Block\nsame as previous to combine global and local information\nfrom two branches. The network starts to perform two tasks\nafter Combination submodule. Task1 is applied to learn the\nrelativetimeintervalusingMLP,whicharemainlycomposed\nof two fully connected layers. Task2 predicts the impedance\nofinputseismictraces,thistask usesa1kernelconvolutional\nlayerofsize3 × 1 todecodethedata.\nB. Loss Function\nThe loss function in this paper is composed of the relative\ntime interval loss _ and the predicted impedance loss\n_. The _ is the output of Task1 in the network.\nAnd the _ uses Mean Square Error (MSE) to compute\nthe loss of the predicted impedance (\n' ) and the true value\n(),writtenas\n_ =\n1\n =1\n (\n' − )2\u0000 . (10)\nWesummarizethelossas\n = _ + _, (11)\nhereλisahyperparameterrelatedtotheinputdata.\nIV.EXPERIMENTS\nThe experiments are mainly divided into three parts\naccording to data sources: The Marmousi2 synthetic model,\nthe Overthrust synthetic model used for transfer learning and\nthe field data are trained and predicted using FCRN, RNN-\nCNN, LSTM-CNN and Trans-CNN networks respectively,\nwhere RNN-CNN and LSTM-CNN refer to replacing the\nTransformer Encoder in Trans-CNN with a RNN/LSTM\nmodule.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n5\nFig. 5. TheMarmousi2model.(a)Impedance;(b)CorrespondingseismicdatasynthesizedbysourceRickerwaveletwith30Hz\n0° phase.\nFig. 6. Loss curves in training and validation process of the FCRN, RNN-CNN, LSTM-CNN, and Trans-CNN on Marmousi2\nmodel(theepochsoffinalparametershavebeenmarkedwithdot).(a)Trainingloss;(b)Validationloss.\nA. Marmousi2 Model\nTheimpedanceofMarmousi2modelisshowninFig.5a.It\nis used to calculate reflection coefficient and then convolved\nwith the source Ricker wavelet with 30 Hz 0° phase to\nsynthesize corresponding seismic data (Fig. 5b). The\nMarmousi2 model is built to simulate the geological\nenvironment of continental drift. It includes several typical\ngeologicalstructures,forinstancegreatchangesofvelocityin\nbothtemporalandspatialdirectionandfaults[48].Themodel\nhas13,601tracesand2800timesamplingpointsineachtrace,\nwithtimeintervalsof1ms.\nFor this model, 136 traces are selected through isometric\nsampling to train the network, of which 90% are taken into\nthe training dataset and 10% into the validation dataset. The\nvalidation dataset evaluates network performance every 50\nepochs, for selecting the parameters with the least validation\nloss as the final parameters of trained networks. The batch\nsize and epochs are designated as 5 and 1000. In the loss\nfunction, the value of hyperparameter  is set to 1. We use\nAdam optimizer to update network parameters, setting its\nweight decay to 1 × 10−7 and learning rate to 0.001\nrespectively. Network training is executed under PyTorch\nframeworkandGPU-acceleratedcalculationisadopted.\nWe can see the training loss curves from four networks in\nFig. 6a and validation loss curves in Fig. 6b. In Fig. 6b, the\nepochs of final network parameters have been marked with\ndot on corresponding curves. For training loss curves, the\nTrans-CNN is higher than FCRN with small margin. This is\nmainly because the loss function of Trans-CNN includes two\nparts: relative time interval loss _ and predicted\nimpedance loss _ , while the network loss of FCRN\nonly includes the latter. But it can be inferred from the\nvalidation loss curves in Fig. 6b that its generalization ability\nisthebestamongfournetworks.\nThe impedance profiles predicted by all networks are\nshown in Fig. 7a-d, and their corresponding residuals are\nshown below each one (Fig. 7e-h). From the 5000-10000th\ntraces of prediction profiles which are more challenging to\ninvert, we can observe that FCRN and RNN-CNN are\nsignificantly deviate from the ground truth, while the LSTM-\nCNN and Trans-CNN can predict the stratigraphic structures\nmore accurate. It can be inferred that the latter two networks\nhave better inversion ability when facing complex geological\nstructures, and further inspection shows that Trans-CNN\nachieves the bestperformance.Inthe blackcircles ofFig.7c-\nd, the Trans-CNN result (Fig. 7d) is with distinct structural\nedgesandlesswhiteverticalstrips. IntheblackcirclesofFig.\n7e-h, there are strong changes of impedance, and only the\nresult of Trans-CNN does not have obvious errors. For the\npurpose of comparing the prediction details of single trace,\ntwo traces (5729, 8542) between 5000-10000th are selected\nforinspection(showinFig.8aand Fig.8b).Asshown inFig.\n8, the proposed network predicts reliable impedance for the\ncomplex stratigraphic changes. Table Ⅰ shows the MSEs and\nPearson Correlation Coefficients (PCCs) between prediction\nresults and the ground truth in the above experiments. It can\nbe seen that the MSE (0.0027) and PCC (0.9986) of Trans-\nCNN are the best. In addition, the training and inferring time\nof networks are shown in Table Ⅱ . It explains that\nTransformer Encoder has great advantages over RNN and\nLSTMincomputingefficiency,especiallyininferencetime.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n6\nFig. 7. Impedance prediction results and their residuals on Marmousi2 model. (a) FCRN; (b) RNN-CNN; (c) LSTM-CNN; (d)\nTrans-CNN; (e-h)arecorrespondingresidualsinfirstrow.\nFig. 8. ImpedancepredictionresultsoftwotracesonMarmousi2model.(a)Trace5729;(b)Trace8542.\nTABLEI\nMSES AND PCCS BETWEEN PREDICTED AND TRUE IMPEDANCE ON THE MARMOUSI2MODEL (BEST VALUES ARE HIGHLIGHTED IN\nRED)\nIndicators FCRN RNN-CNN LSTM-CNN Trans-CNN\nMSE 0.0081 0.0181 0.0065 0.0027\nPCC 0.9960 0.9909 0.9967 0.9986\nTABLEⅡ\nTRAINING AND INFERRING TIME ON THE MARMOUSI2MODEL\nIndicators FCRN RNN-CNN LSTM-CNN Trans-CNN\nTraining 08m16s 53m13s 55m56s 22m32s\nInferring 57s 11m42s 12m46s 1m40s\nB. Overthrust Model\nFig.9showsthedatasetoftheOverthrustsyntheticmodel.\nAscanbeseen,its geologicalstructure isquite differentfrom\nthe Marmousi2 model. Therefore, we choose the Overthrust\nmodel to compare the transfer learning ability of four\nnetworks: they are pre-trained on the Marmousi2 model, and\nthenfine-tunedbytheOverthrustmodel. Thismodelconsists\nof 401 traces with 2800 time points of 1ms interval in each\ntrace. Wechoose 5 traces (1th, 100th, 200th, 300th, 400th) to\nparticipate in transfer learning. Due to insufficient labeled\ntraces,cubicsplineinterpolationmethodisappliedforsample\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n7\nFig. 9. The Overthrust model. (a) Impedance; (b) Corresponding seismic datasynthesized by source Ricker wavelet wiyh 30Hz\n0° phase.\nFig. 10. InterpolatedresultsoftheOverthrustmodel.(a)Interpolatedimpedance;(b)Correspondinginterpolatedseismicdata.\nFig. 11. Fine tuning loss curves in training and validation process of the FCRN, RNN-CNN, LSTM-CNN, and Trans-CNN on\ntheOverthrustmodel(theepochsoffinalparametershavebeenmarkedwithdot).(a)Trainingloss;(b)Validationloss.\nFig. 12. Impedancepredictionresultswithfine-tuningontheOverthrustmodel.(a)FCRN;(b)RNN-CNN;(c)LSTM-CNN;(d)\nTrans-CNN;(e-h)arecorrespondingresidualsinfirstrow.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n8\nTABLEⅢ\nMSES AND PCCS BETWEEN PREDICTED AND TRUE IMPEDANCE ON THE OVERTHRUST MODEL (BEST VALUES ARE HIGHLIGHTED IN\nRED)\nIndicators FCRN RNN-CNN LSTM-CNN Trans-CNN\nMSE 0.0035 0.0030 0.0016 0.0007\nPCC 0.9983 0.9981 0.9994 0.9997\nTABLEⅣ\nTRAINING AND INFERRING TIME ON THE OVERTHRUST MODEL\nIndicators FCRN RNN-CNN LSTM-CNN Trans-CNN\nTraining 02m34s 23m30s 22m46s 8m09s\nInferring 0.7s 22.6s 24.2s 3.6s\naugmentation. Cubic spline interpolation uses the selected 5\noriginal traces to get interpolated data of 50 traces (Fig. 10).\n90% of the them is used for training and 10% for validation.\nThe networks are estimated every 50 epochs for validation.\nAnd the hyperparameters of networks are same as section Ⅳ\nA.\nDuring fine-tuning, Fig. 11a-b shows the training and\nvalidation loss curves. What can be obtained is the FCRN\nlosses are the smallest on the interpolated training and\nvalidationdata.First,itisduetolossofFCRNonlyconsistof\n_ while losses of other three networks include _\nand_.Then,thehighsimilaritybetween thetrainingand\nvalidation data makes the performance of four validation loss\ncurves consistent with the training process. Fig. 12 illustrates\nthat the generalization ability of Trans-CNN exceeds other\nthree networks dramatically. From the prediction profiles of\nfour networks (Fig. 12a-d) and their residuals (Fig. 12e-h),\nTrans-CNN can accurately predict the structure of the thin\nstrata and faults, while other prediction profiles are\ncontaminated by vertical strip errors. To quantify the results,\nTable Ⅲ shows the MSEs and PCCs between the predicted\nand true values. As can be seen, Trans-CNN has the most\noutstanding performance on both MSE (0.0007) and PCC\n(0.9997). The training and inferring time of all networks are\nshowninTableⅣ.\nC. The field data\nAccording to the above experimental results, Trans-CNN\nhas presented advantage due to its global information\nextraction on synthetic models. To further prove its\neffectiveness in application, we conduct experiments on a\nfield data. A total of 12100 (110×110) seismic traces (219\ninline × 219 crossline) are collected on the working area,\nhaving 1501 sampling points with 2ms time interval in each\ntrace. A random seismic data time slice is shown in Fig. 13a.\nThe2244to2494msoftracesisselectedasourtargetlayer.\nThe locations of 6 wells and their cross-well profile have\nbeen indicated in Fig. 13a, and Fig. 13b shows the seismic\ndataof cross well profile. We applytransfer learning strategy\non this dataset to overcome the issue of insufficient labeled\ndata. First, we interpolate the 6 wells impedance and use\nconvolution model to obtain the corresponding seismic data,\nthey collectively form the synthetic model. The interpolated\nimpedance and corresponding seismic profile of synthetic\nmodel are shown in Fig. 14a-b, and the source wavelet used\nfor convolution is shown in Fig. 14c. The networks are pre-\ntrained with the 185 seismic trace of cross well profile (1.5%\nof total data) on synthetic model, and it needs to be\nspecificallystatedthatthedataof6wellsdonotparticipatein\npre-training process. Then we use the same interpolation\nmethod for impedance and seismic datato obtaininterpolated\ndata, and select the 24 traces closest to each well to fine-tune\nthenetwork.Withthisaugmentationoperation,weareableto\nfine-tune the network by 144 seismic traces (1.2% of total\ndata). In addition, network hyper parameters need to be\nadjusted as follows: the size 299 × 1 of convolution kernels\nis changed to 29 × 1, the Adam optimizer weight decay rate\nis changed to 1 × 10−6, and the value of  in loss fuction is\nchanged to 0.5 according to ablation experiments. The\nremainingsettingsinthenetworksaresameassectionⅣA.\nIn the pre-training and fine-tuning process of the\nexperiments, we both make use of 90% data as training\ndataset and 10% for validation. The training and testing\ncurves obtained in the fine-tuning process are shown in Fig.\n15. As we can see that the curves are consistent to those of\nthe Overthrust synthetic model. Fig. 16 shows the cross-well\nprofiles predicted by four networks, and impedance of wells\nis inserted in corresponding positions. It can be observed that\nthere are many vertical stripes with no geological meaning in\ntheresultsobtainedbyFCRN(Fig.16a)andRNN-CNN(Fig.\n16b), such as stratigraphic structure in the black boxes. And\nthere are some incomplete stratigraphic structures that are in\nconflict with the continuous sedimentary model in Fig. 16a,\nindicated by the arrow. For Fig. 16c-d, the profile predicted\nby Trans-CNN (Fig. 16d) is more consistent with impedance\nin wells than LSTM-CNN (Fig. 16c), such as the area\nindicated by the white boxes. Then, the LSTM-CNN profile\nshows more unreasonable stratigraphic and structures than\nTrans-CNN, especially the structure in black circle and\nvertical stripsin blackboxes.Moreover,thelateralcontinuity\nofTrans-CNNisimproved.Thepredictedandrealimpedance\nat 6 wells is plotted in Fig. 17. As we can see, the prediction\nvalues of four networks at w1 are similar. There are large\ndeviations with the ground truth due to rapid change of\nimpedance,suchastimespanbetween2394msand2444ms\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n9\nFig. 13. The seismic data of the field data. (a) A time slice of 2300ms, the yellow diamonds are well w1-w6 locations, and the\nyellow linedepictstheposition ofcrosswell profile;(b)The seismicdataof crosswell profile (dashed verticallines indicatethe\nlocationofwells).\nFig. 14. Synthesize data for pretraining of the field data. (a) The impedance obtained by interpolation from wells; (b) Synthetic\nseismicdata;(c)Thesourcewavelet.\nFig. 15. Finetuning losscurvesin training and validationprocessoftheFCRN,RNN-CNN,LSTM-CNN, and Trans-CNNon\nthefielddata(theepochsoffinalparametershavebeenmarkedwithdot).(a)Trainingloss;(b)Validationloss.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n10\nFig. 16. Impedancepredictionprofilesonthefielddata.(a)FCRN;(b)RNN-CNN;(c)LSTM-CNN;(d)Trans-CNN.\nTABLEⅤ\nMSES AND PCCS BETWEEN PREDICTED AND TRUE IMPEDANCE OF THE 6WELLS ON THE FIELD DATA (BEST VALUES ARE\nHIGHLIGHTED IN RED).\nMSE/PCC FCRN RNN-CNN LSTM-CNN Trans-CNN\nW1 0.0666/0.9539 0.0852/0.9412 0.1016/0.9293 0.0815/0.9437\nW2 0.0098/0.9916 0.0137/0.9878 0.0135/0.9876 0.0132/0.9879\nW3 0.0360/0.9620 0.0334/0.9689 0.0266/0.9747 0.0220/0.9778\nW4 0.0004/0.9997 0.0010/0.9991 0.0014/0.9987 0.0011/0.9991\nW5 0.0004/0.9995 0.0016/0.9982 0.0018/0.9980 0.0010/0.9991\nW6 0.0058/0.9945 0.0082/0.9928 0.0072/0.9931 0.0052/0.9951\n(indicated by yellow box). Trans-CNN achieves the best\nperformance at w3. The difference between predictions at w2\nand w4-w6 is not obvious, and the predicted impedance of\nfour networks are quite close to the ground truth. The MSEs\nandPCCsof6wellsareshown inTableⅤ.Ingeneral,Trans-\nCNN and FCRN perform best than other networks. Taking\nPCC as the evaluation metric, the predicted values of Trans-\nCNN are better at w3 (0.9778) and w6 (0.9951), while the\nPCCofFCRNaregoodatwellw1(0.9539),w2 (0.9916),w4\n(0.9997)andw5(0.9995).\nTo verify the function of self-supervised learning task in\nTrans-CNN, we presented the results of comparative\nexperiment of Single-task Trans-CNN and our Trans-CNN in\nTable Ⅵ and Fig.18. Based on the combination of numerical\nindicatorsandfigure,itcanbeseenthatthesingletaskTrans-\nCNN performs better in terms of MSEs and PCCs on six\nwells, as it only updates network parameters according to the\nindicator MSE. However, its prediction results on the cross-\nwell profile have a large number of vertical stripes. This\nclearly verifies that the self-supervised task can mitigate the\noverfitting and improve generalization ability of the Trans-\nCNN.\nTo further demonstrat the superiority of our network, we\nconductblindwelltestonthefielddata.Wechoosew4asthe\nblind well and do not use any information about it in pre-\ntraining and fine-tuning process. The predicted impedance of\nw4 isshowninFig.19.AndtheMSEsandPCCsofnetworks\nare recorded in Table Ⅶ. It shows that the Trans-CNN\npredictionresult hasthehighestmatching degreewithground\ntruth on MSE (0.0674) and PCC (0.9383). Next, we use four\nnetworks to predict inline profile where w4 is located. We\nshow the predicted impedance profiles in Fig. 20 and show\nthe corresponding seismic data in Fig.21. It is shown that\nTrans-CNN predicted profiles (Fig. 20d)have more complete\nstratigraphicstructureswithbetterlateralcontinuity.\nFinally, we present the predicted results of the three-\ndimensional field data volume in Fig. 22. From the figure, it\ncan be seen that the data volume predicted by Trans-CNN\nhas strong horizontal continuity and the time slice matches\ntheoneinFig.13a.\nⅤ.DISCUSSION\nAt present, CNNs has become one of the fastest growing\nfields in artificial intelligence. In CNNs, a unit in the\nnetworks only depends on a region of the input, which is one\nof the basic concepts, called receptive field. For a network,\nonlytheareawithinreceptivefieldisrelatedtotheunitvalue,\nsotheaccuracyofreceptivefieldsizeiscrucial.Therearea\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n11\nFig.17. Impedancepredictionresultsof6wellsonthefielddata.\nTABLEⅥ\nAVERAGE MSEAND PCCBETWEEN PREDICTED AND TRUE IMPEDANCE OF RHE 6WELLS ON FIELD DATA (BEST VALUES ARE\nHIGHLIGHTED IN RED)\nIndicators Single-taskTrans-CNN Trans-CNN\nMSE 0.0226 0.0674\nPCC 0.9825 0.9383\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n12\nFig.18. Impedancepredictionprofilesonthefielddata.(a)Single-taskTrans-CNN;(b)Trans-CNN.\nFig.19. Impedancepredictionresultsofw4onthefielddataforblindwelltest.\nTABLEⅦ\nMSES AND PCCS BETWEEN PREDICTED AND TRUE IMPEDANCE FOR THE BLIND WELL W4(BEST VALUES ARE HIGHLIGHTED IN RED)\nIndicators FCRN RNN-CNN LSTM-CNN Trans-CNN\nMSE 0.0903 0.1786 0.1774 0.0674\nPCC 0.9136 0.7675 0.7917 0.9383\nFig.20. Inline profile impedance predictionon the field datafor blind well test. (a) FCRN; (b) RNN-CNN;(c) LSTM-CNN; (d)\nTrans-CNN.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n13\nFig.21. Inlineprofileseismicdataonthefielddataforblindwelltest(dashedverticallinesindicatethelocationofw4).\nFig.22. The3DimpedancepredictionvolumnofTrans-CNNonthefielddata.\nnumber of ways to increase the receptive field size, of which\nincreasing convolutional kernels size and stacking more\nconvolutional layers are the most common way. However,\nthese methods have limitations in application. On one hand,\nthe excessive increase of convolution kernels size will affect\nthe feature extraction effectiveness; On the other hand, the\nunreasonable stack of the network depth may cause some\noriginal information loss in the training process. Therefore,\nwe adopt Transformer Encoder architecture and use it to\nextract global features in Trans-CNN, mitigating the\nliminationof receptive field size in CNN. In the experiments\nof synthetic models, it demonstrates that compared to RNN\nand LSTM, the improved Transformer Encoder effectively\nimproves the accuracy and continuity of impedance inversion\nresults and reduces training and inferring time. This is\nextremely crucial in exploring reservoir characteristics. In\naddition, the self-supervised learning task further improves\nthe inversion performance under limited labels. And when\ntransfer learning is carried out for different datasets, Trans-\nCNNshows thebesttransfer learning ability.Inthefielddata\nexperiments, we first use synthetic data to pretrain the\nnetwork, which enables our network to learn the stable\ncorrespondence between impedance and seismic data, which\nisapplicabletotheentiredatavolume.Duringthefine-tuning\nprocess, due to the high accuracy of the interpolation data\naroundthewell,weonlyusethesedatatofine-tunenetworks.\nAlthough Trans-CNN performs the best among all the\ncompared networks, there are still some directions for\nimprovement. First, our network is trained under the multi-\ntask framework, so the mixed loss function composed of\n_ and _ is used. In this paper, the relative weight\nfactor λ in the loss function is selected through ablation\nexperiments, and cannot be guaranteed the optimal. In this\nregard, it can be improved to use some methods such as\nhomescedic uncertainty [18] and gradient normalization [49]\nto treat the λ as a trainable parameter. Second, we conduct\nanti-noisetestsaboutthenetworks.WhentheSignal-to-Noise\nRatio (SNR) is 20,Trans-CNN and FCRN have similar anti-\nnoise performance, but when the SNR is lower, FCRN has\nstronger robustness. So the robustness of Trans-CNN is a\nproblem worth of investigation. Third, the proposed network\nis improved on the basis of 1D CNN. It extracts the\ninformation in each seismic trace independently, but does not\nconsider the spatial correlation among traces. Therefore,\nfuture research is going to combine the global feature\nextraction ability of Transformer with 2D CNN to further\nenhance the horizontal continuity and accuracy of predicted\nprofiles. Last, prestack seismic data inversion can also be\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n14\ninvestigated to estimate multiple elastic parameters directly\n[50].\nⅥ.CONCLUSIONS\nIn this paper, we design an efficient hybrid neural network\nTrans-CNNforseismicdataimpedanceinversion.Intermsof\nnetwork architecture, we incorporate the Transformer\nEncoder according to impedance inversion task and combine\nit with convolutional module, so as to effectively extract\nglobaland localinformationatthesame time.Inaddition,we\nadd a relative time interval self-supervised learning task, and\nuse a multi-task framework to enhance time dimension\nfeaturesextractionwithoutaddinganylabels.\nThe qualitative and quantitative comparisons on synthetic\nmodels indicate that Trans-CNN obtains the most accurate\nprediction results. And it is shown that the Trans-CNN also\nhas the best ability in transfer learning. Finally, we test our\nnetwork on a field data. Faced with insufficient labeled data,\nwe use synthesis and interpolation method for data\naugmentation and introduce transfer learning strategy during\nnetwork training. The proposed network obtains the most\ncomplete stratigraphic structure with a higher horizontal\ncontinuity, and also achieve consistent performance on blind\nwelltest.\nACKNOWLEDGMENT\nThanks for the help of team members Zhanxin Tang and\nXudongLiu.\nREFERENCES\n[1] P. G. Lelièvre and D. W. Oldenburg, \"A comprehensive study of\nincluding structural orientation information in geophysical inversions,\"\nGeophysical Journal International, vol. 178, no. 2, pp. 623-637, 2009.\ndoi:10.1111/j.1365-246X.2009.04188.x.\n[2] G. Mavko, T. Mukerji, and J. Dvorkin, The rock physics handbook,\nCambridgeUniversityPress,2009. doi.:10.1017/CBO9780511626753.\n[3] E. A. Robinson, “Predictive decomposition of seismic traces,”\nGeophysics,vol.22,no.4,pp.767–778,1957.doi:10.1190/1.1438415.\n[4] D. A. Cooke and W. A. Schneider, “Generalized linear inversion of\nreflection seismic data,” Geophysics, vol. 48, no. 6, pp. 665–676, 1983.\ndoi:10.1190/1.1441497.\n[5] M. D. Sacchi, “Reweighting strategies in seismic deconvolution,”\nGeophysical Journal International, vol. 129, no. 3, pp. 651–656, 1997.\ndoi:10.1111/j.1365-246X.1997.tb04500.x.\n[6] R. O. Lindseth, “Synthetic sonic logs—a process for stratigraphic\ninterpretation,” Geophysics, vol. 44, no. 1, pp. 3–26, 1979. doi:\n10.1190/1.1440922.\n[7] R. J. Ferguson and G. F. Margrave, “A simple algorithm for band-\nlimitedimpedanceinversion,”CREWESannual,1996.\n[8] W. P. Gouveia and J. A. Scales, “Bayesian seismic waveform inversion:\nParameter estimation and uncertainty analysis,” Journal of Geophysical\nResearch: Solid Earth, vol. 103, no. B2, pp. 2759–2779, 1998. doi:\n10.1029/97JB02933.\n[9] D. Carron, “High resolution acoustic impedance cross-sections from\nwireline and seismic data,” SPWLA 30th Annual Logging Symposium.\nSocietyofPetrophysicistsandWell-LogAnalysts,1989.\n[10]K. Baddari, J. Ferahtia, T. Aifa, and N. Djarfour, “Seismic noise\nattenuation by means of an anisotropic non-linear diffusion filter,”\nComputers & Geosciences, Volume 37, Issue 4, ISSN 0098-3004, pp.\n456-463,2011.doi:10.1016/j.cageo.2010.09.009.\n[11]Z. T. Xu, Y. S. Luo, B. Y. Wu, and D. Y. Meng, “S2S-WTV: Seismic\nData Noise Attenuation Using Weighted Total Variation Regularized\nSelf-Supervised Learning,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 61, Art no. 5908315, pp. 1-15, 2023. doi:\n10.1109/TGRS.2023.3268554.\n[12]Z. D. Zhang and T. Alkhalifah, “Regularized elastic full waveform\ninversion using deep learning,” 81st EAGE Conference and Exhibition\n2019,Volume2019,pp.1-5,2019.doi:10.3997/2214-4609.201901345.\n[13]R.Biswas,M.K.Sen,V.DasandT.Mukerji,“Pre-stackandPost-stack\ninversion using a Physics-Guided Convolutional Neural Network,”\nSociety of Exploration Geophysicists and American Association of\nPetroleumGeologists,7(3):1-76,2019. doi:10.1190/INT-2018-0236.1.\n[14]J. W. Fang, H. Zhou, Y. E. Li, Q. Zhang and J. Zhang, “Data-driven\nlow-frequency signal recovery using deep learning predictions in full-\nwaveform inversion,” Geophysics, 85(6):1-42, 2020. doi:\n10.1190/geo2020-0159.1.\n[15]Y. Wang, Q. Ge, W. Lu, and X. Yan, “Well-Logging Constrained\nSeismic Inversion Based on Closed-Loop Convolutional Neural\nNetwork,” IEEE Transactions on Geoscience and Remote Sensing,\n58(8):5564-5574,2020.doi:10.1109/TGRS.2020.2967344.\n[16]V. Das, A. Pollack, U. Wollner and T. Mukerji, “Convolutional Neural\nNetwork for Seismic Impedance Inversion,” Geophysics, 84(6): R869-\nR880,2020.doi:10.1190/geo2018-0838.1.\n[17]B. Y. Wu, D. L. Meng, L. L. Wang, N. H. Liu and Y. Wang, “Seismic\nImpedance Inversion Using Fully Convolutional Residual Network and\nTransfer Learning,” IEEE Geoscience and Remote Sensing Letters,\n17(12):2140-2144,2020.doi:10.1109/LGRS.2019.2963106.\n[18]X.Zheng,B.Y.Wu,X.S.ZhuandX.Zhu,“Multi-TaskDeepLearning\nSeismic Impedance Inversion Optimization Based on Homoscedastic\nUncertainty,” Geoscience and Remote Sensing Symposium IGARSS\n2022, 2022 IEEE International, pp. 6149-6152, 2022. doi:\n10.3390/app12031200.\n[19]Y. Q. Wang, Q. Wang, W. K. Lu, Q. Ge and X. F. Yan, “Seismic\nimpedance inversion based on cycle-consistent generative adversarial\nnetwork,” SEG Technical Program Expanded Abstracts 2019, pp. 2498-\n2502,2019.doi:10.1016/j.petsci.2021.09.038.\n[20]A. Cai, H. B. Di, Z. Li, H. Maniar, and A. Abubakar, “Wasserstein\ncycle-consistent generative adversarial network for improved seismic\nimpedance inversion: Example on 3D SEAM model,” SEG Technical\nProgram Expanded Abstracts 2020, pp. 1274-1278, 2020. doi:\n10.1190/segam2020-3425785.1.\n[21]D. L. Meng, B. Y. Wu, N. H. Liu and W. C. Chen, “Semi-Supervised\nDeep Learning Seismic Impedance Inversion Using Generative\nAdversarial Networks,” Geoscience and Remote Sensing Symposium\nIGARSS 2020, pp. 1393-1396, 2020. doi:\n10.1109/IGARSS39084.2020.9323119.\n[22]H. H Zhang, G. Z. Zhang, J. H. Gao, S. J. Li, J. M. Zhang and Z. Y.\nZhu,“Seismic impedance inversion based on geophysical-guided cycle-\nconsistent generative adversarial networks,” Journal of Petroleum\nScience and Engineering, Volume 218, 111003, ISSN 0920-4105, 2022.\ndoi:10.1016/j.petrol.2022.111003.\n[23]M. Alfarraj and G. Alregib, “Semisupervised sequence modeling for\nelastic impedance inversion,” Interpretation, 7(3): SE237-SE249, 2019.\ndoi:10.1190/segam2019-3215902.1.\n[24]Z. Gao, C. Li, B. Zhang, X. Jiang and Z. Xu, “Building large-scale\ndensity model via a deep-learning-based data driven method,”\nGeophysics,86(1):M1-M15,2021. doi:10.1190/geo2019-0332.1.\n[25]R.Guo,J.J.Zhang,D.Liu,Y.B.ZhangandD.W.Zhang,“Application\nof bi-directional long short-term memory recurrent neural network for\nseismic impedance inversion,” 81st EAGE Conference and Exhibition\n2019, European Association of Geoscientists & Engineers, 2019(1):1-5,\n2019. doi:10.3997/2214-4609.201901386.\n[26]J. Wangand J. X. Cao “Data-driven S-wave velocity predictionmethod\nvia a deep-learning-based deep convolutional gated recurrent unit fusion\nnetwork,” Geophysics, 86(6):M185-M196, 2021. doi: 10.1190/geo2020-\n0886.1.\n[27]L.Song, X. Y. Yin, Z. Y. Zong, and M. Jiang, “Semi-supervised\nlearning seismic inversion based on Spatio-temporal sequence residual\nmodeling neural network,” Journal of Petroleum Science and\nEngineering, Volume 208, Part D, 109549, ISSN 0920-4105, 2022. doi:\n10.1016/j.petrol.2021.109549.\n[28]H.HZhang,G.Z.Zhang,J.H.Gao,S.J.Li,J.M.ZhangandZ.Y.Zhu,\n“Seismic impedance inversion based on geophysical-guided cycle-\nconsistent generative adversarial networks,” Journal of Petroleum\nScience and Engineering, Volume 218, 111003, ISSN 0920-4105, 2022.\ndoi:10.1016/j.petrol.2022.111003.\n[29]A. Mustafa, M. Alfarraj, and A. Ghassan. “Estimation of acoustic\nimpedance from seismic data using temporal convolutional network,”\nSEG International Exposition and Annual Meeting, San Antonio, SEG-\n2019-3216840,2019.doi:10.1190/segam2019-3216840.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nIEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, VOL XX, NO. X, XXX XXXX\n15\n[30]J. Hu, L. Shen and G. Sun, “Squeeze-and-Excitation Networks,” 2018\nIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.\n7132-7141,2018.doi:10.1109/CVPR.2018.00745.\n[31]Q.Wang,B.Wu,P.Zhu,P.Li,W.ZuoandQ.Hu,“ECA-Net:Efficient\nChannel Attention for Deep Convolutional Neural Networks,” 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR),pp.11531-11539,2020.doi:10.1109/CVPR42600.2020.01155.\n[32]B. Y. Wu, Q. Xie and B. H. Wu, “Seismic Impedance Inversion Based\non Residual Attention Network,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 60, pp. 1-17, 2022. doi:\n10.1109/TGRS.2022.3193563.\n[33]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A.N.\nGomez, L. Kaiser and I. Polosukhin, “Attention is all you need,” 31st\nInternational Conference on Neural Information Processing Systems,\npp.6000-6010,2017.doi:10.48550/arXiv.1706.03762.\n[34]N. Parmar, A. Vaswani, J. Uszkoreit, Ł. Kaiser, N. Shazeer, A. Ku and\nD. Tran, “lmage transformer,” 35th International Conference on\nMachine Learning (ICML), PMLR 80, pp. 4055-4064, 2018. doi:\n10.48550/arXiv.1802.05751.\n[35]N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov and S.\nZagoruyko, “End-to-end object detection with transformers,” European\nConference on Computer Vision 2020, Part of LNIP, pp.213-229, 2020.\ndoi:10.48550/arXiv.2005.12872.\n[36]A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. H. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J.\nUszkoreit and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” International Conference\non Learning Representations, pp.1-22, 2021. doi:\n10.48550/arXiv.2010.11929.\n[37]Z.Liu,Y.T.Lin,Y.Cao,H.Hu,Y.X.Wei,Z.Zhang,S.Linand B.N.\nGuo, “Swin transformer: hierarchical vision transformer using shifted\nwindows,” 2021 IEEE/CVF International Conference on Computer\nVision (ICCV), pp. 9992-10002, 2021. doi:\n10.1109/ICCV48922.2021.00986.\n[38]K. Han, A. Xiao, E. H. Wu, J. Y. Guo, C. J. Xu and Y. H. Wang,\n“Transformer in transformer,” 35th Conference on Neural Information\nProcessingSystems(NeurIPS2021),Volume34,pp.15908-15919,2021.\ndoi:10.48550/arXiv.2103.00112.\n[39]Z. Q. Sun, S. C. Cao, Y. M. Yang and K. Kitani, “Rethinking\ntransformer-based set prediction for object detection,” 2021 IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pp. 3611-3620,\n2021.doi:10.1109/ICCV48922.2021.00359.\n[40]D.Zhang,H.W.Zhang,J.H.Tang,M.Wang,X.S.HuaandQ.R.Sun,\n“Feature pyramid transformer,” European Conference on Computer\nVision,” pp.323-339,2020.doi:10.48550/arXiv.2007.09451.\n[41]S.X.Zheng,J.C.Lu,H.S.Zhao,X.T.Zhu,Z.K.Luo,Y.B.Wang,Y.\nW. Fu, J. F. Feng, T. Xiang, P. H. S. Torr and L. Zhang, “Rethinking\nsemantic segmentation from a sequence-to-sequence perspective with\ntransformers,” 2021 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 6881-6890, 2021. doi:\n10.1109/CVPR46437.2021.00681.\n[42]E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez and P. Luo,\n“SegFormer:simple and effioient design for semantic segmentation with\ntransformers,” 35th Conference on Neural Information Processing\nSystems (NeurIPS 2021), pp. 12077-12090, 2021. doi:\n10.48550/arXiv.2105.15203.\n[43]R.Strudel,R.Garcia,I.LaptevandC.Schmid,“Segmenter:transformer\nfor semantic segmentation,” 2021 IEEE/CVF International Conference\non Computer Vision, pp. 7262-7272, 2021. doi:\n10.48550/arXiv.2105.05633.\n[44]T. Mikolov, K. Chen, G. Corrado and J. Dean, “Effificient estimation\nof word representations in vector space,” International Conference on\nLearningRepresentations,2013.doi:10.48550/arXiv.1301.3781.\n[45]T.Mikolov,I.Sutskever,K.Chen,G.CorradoandJ.Dean,“Distributed\nrepresentations of words and phrases and their compositionality,” 27th\nConference on Neural Information Processing Systems (NIPS), 2013.\ndoi:10.48550/arXiv.1310.4546.\n[46]Y. H. Liu, E. Sangineto, W. Bi, N. Sebe, B. Lepri and M. D. Nadai,\n“Efficient Training of Visual Transformers with Small-Size Datasets,”\n35th Conference on Neural Information Processing Systems (NIPS), pp.\n23818-23830,2021.doi:10.48550/arXiv.2106.03746.\n[47]A.Gulati,J.Qin,C.C.Chiu,N.Parmar,Y.Zhang,J.H.Yu,W.Han,S.\nB. Wang, Z. D. Zhang, Y. H. Wu and R. M. Pang, “Conformer:\nConvolution-augmented Transformer for Speech Recognition,”\nInterspeech Conference, pp. 5036-5040, 2020. doi:\n10.48550/arXiv.2005.08100.\n[48]A. Brougois, M. Bourget, P. Lailly, M. Poulet, P. Ricarte and R.\nVersteeg, “Marmousi, model and data,” Practical Aspects Seismic Data\nInversion,cp-108-00002,1990.doi:10.3997/2214-4609.201411190\n[49]Z.Chen,V.Badrinarayanan,C.U.LeeandA.Rabinovich,“GradNorm:\nGradient Normalization for Adaptive Loss Balancing in Deep Multitask\nNetworks,”35thInternationalConferenceonMachineLearning(ICML),\nPMLR80,2018.doi:10.48550/arXiv.1711.02257.\n[50]D. P. Cao,Y. Q. Su and R. G. Cui, “Multi-parameter pre-stack seismic\ninversion based on deep learning with sparse reflection coefficient\nconstraints,”JournalofPetroleumScienceandEngineering,Volume209,\n109836,ISSN0920-4105,2022.doi:10.1016/j.petrol.2021.109836.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3358610\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7264267206192017
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7149875164031982
    },
    {
      "name": "Deep learning",
      "score": 0.5309367179870605
    },
    {
      "name": "Transformer",
      "score": 0.5283573269844055
    },
    {
      "name": "Multi-task learning",
      "score": 0.526806116104126
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5068092942237854
    },
    {
      "name": "Inversion (geology)",
      "score": 0.496254026889801
    },
    {
      "name": "Artificial neural network",
      "score": 0.44176167249679565
    },
    {
      "name": "Electrical impedance",
      "score": 0.4401935935020447
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4386966824531555
    },
    {
      "name": "Recurrent neural network",
      "score": 0.4104927182197571
    },
    {
      "name": "Geology",
      "score": 0.23849374055862427
    },
    {
      "name": "Voltage",
      "score": 0.14884960651397705
    },
    {
      "name": "Seismology",
      "score": 0.13013538718223572
    },
    {
      "name": "Task (project management)",
      "score": 0.09593164920806885
    },
    {
      "name": "Engineering",
      "score": 0.0953870415687561
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Tectonics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ]
}