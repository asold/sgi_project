{
    "title": "Application of Large Language Models to Software Engineering Tasks: Opportunities, Risks, and Implications",
    "url": "https://openalex.org/W4367059011",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5079888779",
            "name": "İpek Özkaya",
            "affiliations": [
                "Software Engineering Institute"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6847025045",
        "https://openalex.org/W4367059627",
        "https://openalex.org/W4293039235",
        "https://openalex.org/W4283704460",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4313477803",
        "https://openalex.org/W4388858772",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "Has the day we all have been waiting for really arrived? Have advances in deep learning and machine learning (ML) finally reached a turning point and have started to produce \"accurate enough\" assistants to help us in a variety of tasks, including software development? Are large language models (LLM) going to turn us all into better writers, artists, translators, programmers, health-care workers, not to mention software engineers? Or are we at a risky turning point where we will not be able to separate artificial intelligence (AI)-generated content from user-created ones, drowning in misinformation and perfect sounding yet fake and incorrect information and AI-generated faulty programs?",
    "full_text": "4 IEEE SOFTWARE  |  PUBLISHED BY THE IEEE COMPUTER SOCIETY  0740-7459/23©2023IEEE\nIEEE Software   To be the best source of reliable, useful, peer-reviewed information for leading software practitioners—\nMission Statement  the developers and managers who want to keep up with rapid technology change.\nHAS THE DAY we all have been wait-\ning for really arrived? Have advances \nin deep learning and machine learning \n(ML) finally reached a turning point \nand have started to produce “accurate \nenough” assistants to help us in a vari-\nety of tasks, including software devel-\nopment? Are large language models \n(LLM) going to turn us all into bet -\nter writers, artists, translators, pro -\ngrammers, health-care workers, not to \nmention software engineers? Or are \nwe at a risky turning point where we \nwill not be able to separate artificial \nintelligence (AI)-generated content \nfrom user-created ones, drowning in \nmisinformation and perfect sound -\ning yet fake and incorrect information \nand AI-generated faulty programs?\nRecently released LLMs, such as  \nGenerative Pretrained Transformer \n(GTP) 4 used in ChatGPT by OpenAI \nand BERT used in Bard by Google, dis-\nrupt the search engine model that we \nhave been used to. Use of these models \nshifts the end-user computer interaction \nfrom “here are a list of places to look \nat to potentially find an answer to your \nquestion” to “here is a suggested answer \nto your questions with well-constructed \nsyntax, what is your next question \nbased on this?” \nWithout a doubt, LLMs have use \ncases in assisting software engineering \ntasks as well, including code generation \nmodels trained in programming lan -\nguages, such as CoPilot by GitHub. \nThe reaction of the software engineer-\ning community to the accelerated ad-\nvances that LLMs have been enjoying \nsince 2022 has been varied, ranging \nfrom considering capabilities offered \nby these models as “snake oil”1 to “end \nof programming and computer science \neducation as we know it.”2 In this ar -\nticle, after a brief overview of LLMs, \nI will focus on the opportunities LLMs \nopen up for software development and \nimplications of incorporating LLMs \ninto systems as well as assisting with \nsoftware development tasks.\nWhat Are LMMs?\nAn LLM is a deep neural network model \nwhich has been trained on large amounts \nof data, such as books, code, articles, and \nwebsites, to learn the underlying patterns \nand relationships in the language that it \nwas trained for. By doing so, the model \nis able to generate coherent content such \nas grammatically correct sentences and \nparagraphs that mimic human language \nor syntactically correct code snippets. \nDigital Object Identifier 10.1109/MS.2023.3248401\nDate of current version: 18 April 2023\nFROM THE EDITOR \nEditor in Chief: Ipek Ozkaya  \nCarnegie Mellon Software Engineering Institute  \nipek.ozkaya@computer.org  \nApplication of Large \nLanguage Models to \nSoftware Engineering \nTasks: Opportunities, \nRisks, and Implications\nIpek Ozkaya\nFROM THE EDITOR\n MAY/JUNE 2023  |  IEEE SOFTWARE  5\nLLMs have applications in a variety of \ntasks, including language translation, \nsummarization, and question answer-\ning and have potential in many fields as \nlong as the data that the models have \nbeen trained on provide the appropriate \ninput. While the con  tent generated by \nLLMs are often grammatically correct, \nthey may not always be semantically \ncorrect. The probabilistic and random-\nized selection of the “next token” in con-\nstructing the outputs on one hand gives \nthe end user the impressions of correct-\nness and style, on the other hand may \nresult in mistakes.3\nWhile the recently released ver -\nsions of LLMs, ChatGPT driving the \npack, have made significant improve-\nments, there are several areas of cau -\ntion around their generation and use:\n• Data quality and bias concerns: \nLLMs require enormous amounts \nof training data to learn language \npatterns and their outputs are \nhighly dependent on the data that \nthey are trained on. Any of the \nissues that exist in the training \ndata, such as biases and mistakes, \nwill be amplified by LLMs, po-\ntentially resulting in models that \nexhibit discriminatory behavior, \nsuch as making prejudiced recom-\nmendations. This means that the \nquality and representativeness of \nthe training data can significantly \nimpact the model’s performance \nand generalizability, mistakes can \npropagate. For example, language \nmodels that are used to recom-\nmend code patterns have been \nfound to carry security flaws \nforward.4 This creates risks in not \nonly generating buggy code, but \nalso perpetuating immature imple-\nmentation practices in developers.\n• Privacy and content ownership \nconcerns:  LLMs are gener -\nated using content developed by \nothers which both may contain \nprivate information as well as \ncontent creators’ unique creativity \ncharacteristics. Training on such \ndata using patterns in recom-\nmended output creates plagiarism \nconcerns. Some content is boiler \nplate and the ability to generate \noutput in correct and understand-\nable ways creates opportuni-\nties for improved efficiency. But \ncontent, including code, where in-\ndividual contributions matter be-\ncomes difficult to differentiate. In \nthe long run, increasing popular-\nity of language models will likely \ncreate boundaries around data \nsharing and open source software \nand open science. Techniques \nto indicate ownership or even \npreventing certain data to be used \nto train such models will likely \nemerge. However, such techniques \nand attributes to complement \nLLMS are yet to come. \n• Environmental concerns: The \nvast amounts of computing power \nrequired in training deep learn-\ning models has been increasingly \na concern related to their impact \non carbon footprint. Research in \ndifferent training techniques, al-\ngorithmic efficiencies, and varying \nallocation of computing resources \nduring training will likely increase. \nIn addition, improved data col-\nlection and storage techniques are \nanticipated to eventually reduce  \nthe impact of LLMs on the envi-\nronment, but development of  \nsuch techniques are still in their \nearly phases.5\n• Explainability and unintended con-\nsequence concerns: Explainability \nof deep learning and ML models \nis a general concern in AI, includ-\ning but not limited to LLMs. Users \nseek to understand the reason-\ning behind the recommendations, \nCONTACT \nUS\nAUTHORS\nFor detailed information on submitting \narticles, visit the “Write for Us” section at \nwww.computer.org/software\nLETTERS TO THE EDITOR\nSend letters to software@computer.org\nON THE WEB\nwww.computer.org/software\nSUBSCRIBE\nwww.computer.org/subscribe\nSUBSCRIPTION  \nCHANGE OF ADDRESS\naddress.change@ieee.org  \n(please specify IEEE Software.)\nMEMBERSHIP  \nCHANGE OF ADDRESS\nmember.services@ieee.org\nMISSING  \nOR DAMAGED COPIES\ncontactcenter@ieee.org\nREPRINT PERMISSION\nIEEE utilizes Rightslink for permissions \nrequests. For more information, visit www.\nieee.org/publications/rights/rights-link.html\nFROM THE EDITOR\n6 IEEE SOFTWARE  |  WWW.COMPUTER.ORG/SOFTWARE   |  @IEEESOFTWARE\nespecially if such models are to be \nused in safety or business critical \nsettings. Dependence on the qual-\nity of the data and inability to \ntrace the recommendations to the \nsource increase trust concerns.6 \nIn addition, since the sequences \nare generated using a random-\nized probabilistic approach, \nexplainability of correctness of \nthe recommendations create \nadded challenges. Explainability \nas well as responsible AI practices \nare critical since such models \ncan easily be used to spread \nmisinformation.\nThe application programming in-\nterfaces (API) of GPT and BERT are \nnow also available to other develop-\ners. This contributes to both acceler-\nating the use and improvements on \nLLMs as well as increasing the num-\nber of opportunities of their misuse. \nOpenAI researchers are open about \ntheir lessons learned and have no \nchoice but rely on software engineer-\ning best practices. They recommend \npolicy enforcement as a mechanism \nto enforce avoiding misuses.7 Applica-\ntions which help detect text written by \nsuch models have been quick to come, \nsuch as GPTZero written for educa-\ntors to detect such text, and ironically \nit uses ChatGPT in doing so.8 It is safe \nto say LLMs have attracted a fair \nshare of confusion, criticism, and \nexcitement all at the same time.\nApplications in Software \nEngineering\nResearch agendas developed recently \nhad already shined the light on the fu-\nture of software engineering to be an \nAI-augmented development lifecycle \nwhere both software engineering and \nAI assistants share roles from copilot \nto student, expert, and supervisor. 9 \nIn the National Agenda for Software \nEngineering, my colleagues and I had \nsuggested that developers will need to \nguide and consequently improve the \nAI assistants. AI assistants will also \ntake on a supervisory role by provid-\ning real-time feedback and, in time, \ndemonstrating repeated mistakes to \ndevelopers. On a developer team, \nthere will always be some developers \nwho you trust more than others (per-\nhaps due to experience, skill sets, or \ndemonstrated performance). The AI-\nassisted development workflows will \ntrigger the need to think of AI “part-\nners” in the same way.9\nWhile with caution, software en-\ngineers need to think about LLMs \nas partners and focus on where their \noptimal application can be. There are \nquite a number of software engineer-\ning tasks which can effectively ben -\nefit from using LLMs. Indulge me for \na moment to assume that we solved \nthe trust and unethical use issues as I \nenumerate potential use cases where \nLLMs can create strides of advances \nin improved productivity of software \nengineering tasks, and where the risks \ncan still be manageable.\n• Specification generation: Quite \na number of requirements can \nbe common across applica-\ntions, yet oftentimes require -\nments are also incomplete. \nLLMs can assist in generating \nmore complete specifications \nsignificantly quicker.\n• Just in time developer feed-\nback: Applications of LLMs in \nsoftware development has been \nreceived with much skepticism, \nrightly so at the time being. \nWhile the code generated by \ncurrent AI assistants, such as \nCopilot, have been found to carry \nmore security issues,4 in time this \nwill change. AI-based and other \napproaches which give developers \nEDITORIAL \nSTAFF\nIEEE SOFTWARE  STAFF \nJournals Production Manager:  Peter Stavenick, \np.stavenick@ieee.org\nCover Design: Andrew Baker\nPeer Review Administrator: software@computer.org\nPeriodicals Portfolio Specialist: Cathy Martin\nPeriodicals Operations Project Specialist:  \nChristine Shaughnessy\nContent Quality Assurance Manager: Jennifer Carruth\nPeriodicals Portfolio Senior Manager: Carrie Clark\nDirector of Periodicals and Special Products: \nRobin Baldwin\nIEEE Computer Society Executive Director:   \nMelissa Russell\nSenior Advertising Coordinator: Debbie Sims\nCS PUBLICATIONS BOARD\nGreg Byrd (Interim VP of Publications), Terry Benzel,  \nIrena Bojanova, David Ebert, Dan Katz, Shixia Liu, \nDimitrios Serpanos, Jaideep Vaidya; Ex officio: \nRobin Baldwin, Nita Patel, Melissa Russell\nCS MAGAZINE OPERATIONS \nCOMMITTEE\nIrena Bojanova (Chair), Lorena Barba, Lizy K. John, \nFahim Kawsar, San Murugesan, Ipek Ozkaya, \nGeorge Pallis, Charalampos (Babis) Z. Patrikakis, \nSean Peisert, Balakrishnan (Prabha) Prabhakaran, \nAndré Stork, Ramesh Subramanian, Jeff Voas\nIEEE PUBLICATIONS OPERATIONS\nSenior Director, Publishing Operations: Dawn M. Melley\nDirector, Editorial Services:  Kevin Lisankie\nDirector, Production Services: Peter M. Tuohy\nAssociate Director, Information Conversion and \nEditorial Support:  Neelam Khinvasara\nSenior Manager, Journals Production: Katie Sullivan\nSenior Art Director: Janet Dudar\nEditorial: All submissions are subject to editing for clarity, style, and \nspace. Unless otherwise stated, bylined articles and departments, \nas well as product and service descriptions, reflect the author’s \nor firm’s opinion. Inclusion in IEEE Software does not necessarily \nconstitute endorsement by IEEE or the IEEE Computer Society.\nTo Submit: Access the IEEE Computer Society’s Web-based \nsystem, ScholarOne, at http://mc.manuscriptcentral.com/  \nsw-cs. Be sure to select the right manuscript type when \nsubmitting. For complete submission information, please visit \nthe Author Information menu item under “Write for Us” on our \nwebsite: www.computer.org/software .\nIEEE prohibits discrimination, harassment and bullying:   \nFor more information, visit www.ieee.org  \n/web/aboutus/whatis/policies/p9-26.html .\nDigital Object Identifier 10.1109/MS.2022.3228353\nFROM THE EDITOR\n MAY/JUNE 2023  |  IEEE SOFTWARE  7\nsyntactic corrections and sugges-\ntions have been around a while. \nLLMs carry the promise of going \nthe extra mile and recommending \nnot just corrections, but next steps.\n• Improved testing: Generating \nunit tests is one of the tasks where \ndevelopers shortcut the most. \nAbility to generate test cases at \nease would increase overall test \neffectiveness and coverage, and \nconsequently  system quality.\n• Documentation:  Ranging from \ncontracting language to regu -\nlatory requirements, there are \nmany applications of LLMs \nto software development \ndocumentation.\n• Language translation: Legacy soft-\nware and brownfield development \nis the norm of system develop-\nment today, and many organiza-\ntions need to go through language \ntranslation efforts when they need \nto modernize their systems. This \nprocess is often manual and error \nprone, while some tools do exist \nto support developers. While will \nnot work at scale, portions of code \ncan potentially be translated to \nother programming languages us-\ning LLMs. Rewriting a system in \nan other programming language is \nnot just a language translation ex-\nercise, it is mostly also a re-archi-\ntecting exercise; however, ability \nto rewrite selected portions at ease \nwould be a welcomed capability.\nLLMs will also require software \nengineers to become more savvy in \nhow they incorporate them into sys -\ntems as elements. Example areas in -\nclude the following:\n• LLMs as functional components: \nLLMs will definitely change some \nof the ways capabilities are bun-\ndled and delivered as well, where \npretrained models become parts of \nsystems or parts of external systems. \nAPIs to LLMs will drive different \nsystem composition scenarios and \nwill be available as services.\n• Operations informing develop-\nment: Data is the first-class citizen \nin LLM tools. Operational data \nwill need to be more timely fed \nback to both the development \n process, e.g., areas where users \nmake most mistakes, as well as \nfunctionality development, e.g., \ninform functionality that users do \nnot use to be deprecated.\nThese examples focus on existing \nsoftware engineering tasks that can be \ndone better or faster because such mod-\nels exist. There are also, however, task \nflows that will change, and new activi-\nties will likely emerge while time spent \non others get reduced. An AI-augment \nsoftware development lifecycle will \nlikely have different task flows, efficien-\ncies, and roadblocks than the current \ndevelopment lifecycles of agile and iter-\native development workflows. For ex-\nample, rather than thinking about steps \nof development as requirements, de -\nsign, implementation, test, and deploy, \nLLMs can enable bundling these tasks \ntogether. This would change the num-\nber of hand-offs and where they hap-\npen, shifting task dependencies within \nthe software development lifecycle.\nGoing Forward\nAll the areas of cautions and risks re-\nlated to LLMs are areas where we need \nnew research and innovations. These \nneed to be targeted at improving cor-\nrectness of LLM recommendations, \nimproving their generalizability, as \nwell as improving the ethical implica-\ntions of data use and content creation.\nWe are likely to see most advances \nin generalizability of models, devel -\nopment of integrated development \nenvironments with new paradigms, \nand reliable data collection and use \ntechniques in the near future. Curri -\ncula development and education of the \nnext generation of computer scientists \nand software engineers cannot stay \nblind to the implications of such de -\nvelopments in generative AI either.\nGeneralizability of Models \nCurrently, LLMs work by pretraining \non a large corpus of content followed \nby fine-tuning on a specific task. What \nthis implies is that the architecture of \nthe model is task independent; how -\never, its application for specific tasks \nrequires further fine-tuning with sig -\nnificantly large numbers of examples. \nGeneralizability of these models to ap-\nplications where data are sparse, few-\nshot settings, is already a focus area by \nresearchers.10\nNew Development Environments\nIf we are convinced by the argu -\nment that some tasks can be acceler -\nated and improved in correctness by \nAI assistants including LLMs, that \nalso implies that the current inte -\ngrated development tools will need \nComputer science and software \nengineering programs need to start  \na shift in their curricula today.\nFROM THE EDITOR\n8 IEEE SOFTWARE  |  WWW.COMPUTER.ORG/SOFTWARE   |  @IEEESOFTWARE\nto incorporate these assistants. When \nassistants are integrated in, then de -\nvelopment becomes a more interactive \nprocess with the tool environment. \nSoftware engineering bots are already \npushing the envelope of the develop -\nment environments in the direction of \nincorporating developer assistants.11\nData as a Unit of Computation\nThe most critical input which drives this \nnext generation of AI innovations is not \nonly the algorithms, but also data. Not \nonly will a significant portion of com-\nputer science and software engineer -\ning talent shift to data science and data \nengineer careers, but also, we will need \nmore tool-supported innovations in data \ncollection, data quality assessment, and \ndata ownership rights management. This \nis an area with huge gaps that requires \nskill sets that span computer science, pol-\nicy, engineering, as well as deep knowl-\nedge in security, privacy, and ethics.\nComputer Science and Software \nEngineering Education \nThe biggest implications of LLMs are \nin how we teach programming lan -\nguages and system design. LLMs are \nlikely to take already existing plat -\nforms such as StackOverflow and Red-\ndit, which have become indispensable \nresources for developers, to a new level \nof reduced barrier of entry. Computer \nscience and software engineering pro-\ngrams need to start a shift in their cur-\nricula today. Software engineering and \ncomputer science education has al -\nready missed the boat by continuing to \nfocus on teaching green field develop-\nment while today the reality of system \ndevelopment is brownfield. Students \nare not adequately exposed to theo -\nries and techniques to support system \ndevelopment by composition, legacy \nevolution, and using heterogeneous \nplatforms and programming languages \nin concert. We teach students hello \nworld development, while we should \nbe teaching them how to read millions \nof lines of code, triage and fix bugs that \nthey have not contributed to and un -\nderstand the structure and behavior of \nthe software rather than the single class \nor story card they are responsible for. \nWith LLMs and their sister AI-driven \napps assisting developers, we need to \nbe teaching next-generation software \nengineers when to trust, how to cre -\nate evidence to trust, how to do trust \nassessment rapidly and correctly, and \nhow to improve such assistants. We \nneed to teach them how to evolve sys-\ntems to incorporate such components, \nand we need to teach them to treat \ndata as code. We need to make ethics \ncourses mandatory every year of the \ncurriculum. The list goes on.\nAfter the two winters of AI, gener-\nally attributed to late 1970s and early \n1990s, we have entered not only a \nperiod of AI blossoms, but also ex -\nponential growth in funding, in use, \nand in scare from AI. Advances in \nLLMs without a doubt are huge con-\ntributors to this growth. What will \ndetermine if the next phase includes \ninnovations beyond our imagination \nor another AI winter is largely de -\npendent on not our ability to continue \ntechnical innovations, but on our abil-\nity to practice software engineering \nand computer science through the \nhighest level of ethics and respon -\nsible practices. We need to be bold in \nexperimenting with the potential of \nLLMs in improving software devel -\nopment, and we need to be cautious \nand not forget fundamentals of engi -\nneering ethics and rigor. \nReferences\n1. S. Shankland. “Computing guru  \ncriticizes ChatGPT AI tech for  \nmaking things up.” CNET. Accessed:  \nFeb. 2023. [Online]. Available: https:// \nwww.cnet.com/tech/computing/  \ncomputing-guru-criticizes-chatgpt \n-ai-tech-for-making-things-up/\n2. M. Welsh, “The end of program -\nming,” Commun. ACM , vol. 66, \nno. 1, pp. 34–35, Jan. 2023, doi: \n10.1145/3570220.\n3. S. Wolfram. “What is ChatGPT \ndoing … and why does it work?” Ste -\nphen Wolfram. Accessed: Feb. 2023. \n[Online]. Available: https://writings.  \nstephenwolfram.com/2023/02/what  \n-is-chatgpt-doing-and-why-does-it  \n-work/\n4. N. Perry, M. Srivastava, D. Kumar, \nand D. Boneh, “Do users write more \ninsecure code with AI assistants?” \n2022, arXiv:2211.03622 .\n5. D. A. Patterson et al., “The carbon \nfootprint of machine learning training \nwill plateau, then shrink,” Computer, \nvol. 55, no. 7, pp. 18–28, Jul. 2022, doi: \n10.1109/MC.2022.3148714.\n6. C. Tantithamthavorn, J. Cito, H. He -\nmati, and S. Chandra, “Explainable \nAI for SE: Experts’ interviews, chal -\nlenges, and future directions,” IEEE \nSoftw., vol. 40, no. 4, 2023.\n7. M. Brundage et al., “Lessons \nlearned on language model safety \nand misuse.” OpenAI. Accessed: \nFeb. 2023. [Online]. Avail -\nable: https://openai.com/blog/\nlanguage-model-safety-and-misuse/\n8. GPTZero. Accessed: Feb. 2023. [On -\nline] Available: https://gptzero.me/faq \n9. A. Carleton et al., “Architecting the \nfuture of software engineering: A \nnational agenda for software engi -\nneering research and development,” \nSoftw. Eng. Inst., Pittsburgh, PA, \nUSA, AD1152714, 2021.\n10. T. B. Brown et al., “Language mod -\nels are few-shot learners,” 2020, \narXiv:2005.14165 .\n11. I. Ozkaya, “A paradigm shift in \nautomating software engineering \ntasks: Bots,” IEEE Softw. , vol. 39, \nno. 5, pp. 4–8, Sep./Oct. 2022, doi: \n10.1109/MS.2022.3167801."
}