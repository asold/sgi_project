{
  "title": "InfoBERT: Improving Robustness of Language Models from An Information\\n Theoretic Perspective",
  "url": "https://openalex.org/W3128654100",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2349034822",
      "name": "Wang, Boxin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202139473",
      "name": "Wang, Shuohang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1973031759",
      "name": "Cheng Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2365636915",
      "name": "Gan Zhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3183121254",
      "name": "Jia, Ruoxi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967255241",
      "name": "Li Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1922531866",
      "name": "Liu, Jingjing",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2243397390",
    "https://openalex.org/W3008555961",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2970078867",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W3035736465",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W3106428938",
    "https://openalex.org/W2995040292",
    "https://openalex.org/W3036928441",
    "https://openalex.org/W2122925692",
    "https://openalex.org/W115285041",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2932893307",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2964184826",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2911634294",
    "https://openalex.org/W3035164976",
    "https://openalex.org/W2963969878",
    "https://openalex.org/W2803832867",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2962768284",
    "https://openalex.org/W3035688398",
    "https://openalex.org/W2963126845",
    "https://openalex.org/W3017003177",
    "https://openalex.org/W2917551568",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W2970449623",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2962718684",
    "https://openalex.org/W3006647218",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2798302089",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W2964082701",
    "https://openalex.org/W2963859254"
  ],
  "abstract": "Large-scale language models such as BERT have achieved state-of-the-art\\nperformance across a wide range of NLP tasks. Recent studies, however, show\\nthat such BERT-based models are vulnerable facing the threats of textual\\nadversarial attacks. We aim to address this problem from an\\ninformation-theoretic perspective, and propose InfoBERT, a novel learning\\nframework for robust fine-tuning of pre-trained language models. InfoBERT\\ncontains two mutual-information-based regularizers for model training: (i) an\\nInformation Bottleneck regularizer, which suppresses noisy mutual information\\nbetween the input and the feature representation; and (ii) a Robust Feature\\nregularizer, which increases the mutual information between local robust\\nfeatures and global features. We provide a principled way to theoretically\\nanalyze and improve the robustness of representation learning for language\\nmodels in both standard and adversarial training. Extensive experiments\\ndemonstrate that InfoBERT achieves state-of-the-art robust accuracy over\\nseveral adversarial datasets on Natural Language Inference (NLI) and Question\\nAnswering (QA) tasks. Our code is available at\\nhttps://github.com/AI-secure/InfoBERT.\\n",
  "full_text": "Published as a conference paper at ICLR 2021\nINFO BERT: I MPROVING ROBUSTNESS OF LANGUAGE\nMODELS FROM AN INFORMATION THEORETIC\nPERSPECTIVE\n‚àóBoxin Wang1, Shuohang Wang2, Yu Cheng2, Zhe Gan2, Ruoxi Jia3, Bo Li1, Jingjing Liu2\n1University of Illinois at Urbana-Champaign 2 Microsoft Dynamics 365 AI Research 3 Virginia Tech\n{boxinw2,lbo}@illinois.edu {shuohang.wang,yu.cheng,zhe.gan,jingjl}@microsoft.com\nABSTRACT\nLarge-scale pre-trained language models such as BERT and RoBERTa have\nachieved state-of-the-art performance across a wide range of NLP tasks. Re-\ncent studies, however, show that such BERT-based models are vulnerable fac-\ning the threats of textual adversarial attacks. We aim to address this problem\nfrom an information-theoretic perspective, and propose InfoBERT, a novel learn-\ning framework for robust Ô¨Åne-tuning of pre-trained language models. InfoBERT\ncontains two mutual-information-based regularizers for model training: ( i) an\nInformation Bottleneck regularizer, which suppresses noisy mutual information\nbetween the input and the feature representation; and ( ii) an Anchored Feature\nregularizer, which increases the mutual information between local stable fea-\ntures and global features. We provide a principled way to theoretically analyze\nand improve the robustness of language models in both standard and adversar-\nial training. Extensive experiments demonstrate that InfoBERT achieves state-\nof-the-art robust accuracy over several adversarial datasets on Natural Language\nInference (NLI) and Question Answering (QA) tasks. Our code is available at\nhttps://github.com/AI-secure/InfoBERT.\n1 I NTRODUCTION\nSelf-supervised representation learning pre-trains good feature extractors from massive unlabeled\ndata, which show promising transferability to various downstream tasks. Recent success includes\nlarge-scale pre-trained language models ( e.g., BERT, RoBERTa, and GPT-3 (Devlin et al., 2019;\nLiu et al., 2019; Brown et al., 2020)), which have advanced state of the art over a wide range of\nNLP tasks such as NLI and QA, even surpassing human performance. SpeciÔ¨Åcally, in the computer\nvision domain, many studies have shown that self-supervised representation learning is essentially\nsolving the problem of maximizing the mutual information (MI) I(X; T) between the input X and\nthe representation T (van den Oord et al., 2018; Belghazi et al., 2018; Hjelm et al., 2019; Chen\net al., 2020). Since MI is computationally intractable in high-dimensional feature space, many MI\nestimators (Belghazi et al., 2018) have been proposed to serve as lower bounds (Barber & Agakov,\n2003; van den Oord et al., 2018) or upper bounds (Cheng et al., 2020) of MI. Recently, Kong et al.\npoint out that the MI maximization principle of representation learning can be applied to not only\ncomputer vision but also NLP domain, and propose a uniÔ¨Åed view that recent pre-trained language\nmodels are maximizing a lower bound of MI among different segments of a word sequence.\nOn the other hand, deep neural networks are known to be prone to adversarial examples (Goodfel-\nlow et al., 2015; Papernot et al., 2016; Eykholt et al., 2017; Moosavi-Dezfooli et al., 2016), i.e., the\noutputs of neural networks can be arbitrarily wrong when human-imperceptible adversarial pertur-\nbations are added to the inputs. Textual adversarial attacks typically perform word-level substitution\n(Ebrahimi et al., 2018; Alzantot et al., 2018; Ren et al., 2019) or sentence-level paraphrasing (Iyyer\net al., 2018; Zhang et al., 2019) to achieve semantic/utility preservation that seems innocuous to\nhuman, while fools NLP models. Recent studies (Jin et al., 2020; Zang et al., 2020; Nie et al., 2020;\nWang et al., 2020) further show that even large-scale pre-trained language models (LM) such as\n‚àóWork was done during Boxin Wang‚Äôs Summer internship in Microsoft Dynamics 365 AI Research.\n1\narXiv:2010.02329v4  [cs.CL]  22 Mar 2021\nPublished as a conference paper at ICLR 2021\nBERT are vulnerable to adversarial attacks, which raises the challenge of building robust real-world\nLM applications against unknown adversarial attacks.\nWe investigate the robustness of language models from an information theoretic perspective, and\npropose a novel learning framework InfoBERT, which focuses on improving the robustness of lan-\nguage representations by Ô¨Åne-tuning both local features (word-level representation) and global fea-\ntures (sentence-level representation) for robustness purpose. InfoBERT considers two MI-based\nregularizers: (i) the Information Bottleneck regularizer manages to extract approximate minimal\nsufÔ¨Åcient statistics for downstream tasks, while removing excessive and noisy information that may\nincur adversarial attacks; (ii) the Anchored Feature regularizer carefully selects useful local stable\nfeatures that are invulnerable to adversarial attacks, and maximizes the mutual information between\nlocal stable features and global features to improve the robustness of the global representation. In\nthis paper, we provide a detailed theoretical analysis to explicate the effect of InfoBERT for robust-\nness improvement, along with extensive empirical adversarial evaluation to validate the theory.\nOur contributions are summarized as follows. (i) We propose a novel learning framework InfoBERT\nfrom the information theory perspective, aiming to effectively improve the robustness of language\nmodels. (ii) We provide a principled theoretical analysis on model robustness, and propose two MI-\nbased regularizers to reÔ¨Åne the local and global features, which can be applied to both standard and\nadversarial training for different NLP tasks. ( iii) Comprehensive experimental results demonstrate\nthat InfoBERT can substantially improve robust accuracy by a large margin without sacriÔ¨Åcing the\nbenign accuracy, yielding the state-of-the-art performance across multiple adversarial datasets on\nNLI and QA tasks.\n2 R ELATED WORK\nTextual Adversarial Attacks/Defenses Most existing textual adversarial attacks focus on word-\nlevel adversarial manipulation. Ebrahimi et al. (2018) is the Ô¨Årst to propose a whitebox gradient-\nbased attack to search for adversarial word/character substitution. Following work (Alzantot et al.,\n2018; Ren et al., 2019; Zang et al., 2020; Jin et al., 2020) further constrains the perturbation search\nspace and adopts Part-of-Speech checking to make NLP adversarial examples look natural to human.\nTo defend against textual adversarial attacks, existing work can be classiÔ¨Åed into three categories:\n(i) Adversarial Trainingis a practical method to defend against adversarial examples. Existing work\neither uses PGD-based attacks to generate adversarial examples in the embedding space of NLP as\ndata augmentation (Zhu et al., 2020a), or regularizes the standard objective using virtual adversarial\ntraining (Jiang et al., 2020; Liu et al., 2020; Gan et al., 2020). However, one drawback is that\nthe threat model is often unknown, which renders adversarial training less effective when facing\nunseen attacks. (ii) Interval Bound Propagation(IBP) (Dvijotham et al., 2018) is proposed as a new\ntechnique to consider the worst-case perturbation theoretically. Recent work (Huang et al., 2019; Jia\net al., 2019) has applied IBP in the NLP domain to certify the robustness of models. However, IBP-\nbased methods rely on strong assumptions of model architecture and are difÔ¨Åcult to adapt to recent\ntransformer-based language models. ( iii) Randomized Smoothing (Cohen et al., 2019) provides a\ntight robustness guarantee in ‚Ñì2 norm by smoothing the classiÔ¨Åer with Gaussian noise. Ye et al.\n(2020) adapts the idea to the NLP domain, and replace the Gaussian noise with synonym words to\ncertify the robustness as long as adversarial word substitution falls into predeÔ¨Åned synonym sets.\nHowever, to guarantee the completeness of the synonym set is challenging.\nRepresentation Learning MI maximization principle has been adopted by many studies on self-\nsupervised representation learning (van den Oord et al., 2018; Belghazi et al., 2018; Hjelm et al.,\n2019; Chen et al., 2020). SpeciÔ¨Åcally, InfoNCE (van den Oord et al., 2018) is used as the lower\nbound of MI, forming the problem as contrastive learning (Saunshi et al., 2019; Yu et al., 2020).\nHowever, Tian et al. (2020) suggests that the InfoMax (Linsker, 1988) principle may introduce ex-\ncessive and noisy information, which could be adversarial. To generate robust representation, Zhu\net al. (2020b) formalizes the problem from a mutual-information perspective, which essentially per-\nforms adversarial training for worst-case perturbation, while mainly considers the continuous space\nin computer vision. In contrast, InfoBERT originates from an information-theoretic perspective\nand is compatible with both standard and adversarial training for discrete input space of language\nmodels.\n2\nPublished as a conference paper at ICLR 2021\n3 I NFO BERT\nBefore diving into details, we Ô¨Årst discuss the textual adversarial examples we consider in this paper.\nWe mainly focus on the dominant word-level attack as the main threat model, since it achieves higher\nattack success and is less noticeable to human readers than other attacks. Due to the discrete nature\nof text input space, it is difÔ¨Åcult to measure adversarial distortion on token level. Instead, because\nmost word-level adversarial attacks (Li et al., 2019; Jin et al., 2020) constrain word perturbations via\nthe bounded magnitude in the semantic embedding space, by adapting from Jacobsen et al. (2019),\nwe deÔ¨Åne the adversarial text examples with distortions constrained in the embedding space.\nDeÔ¨Ånition 3.1. (œµ-bounded Textual Adversarial Examples). Given a sentence x = [x1; x2; ...; xn],\nwhere xi is the word at the i-th position, the œµ-bounded adversarial sentence x‚Ä≤ = [x‚Ä≤\n1; x‚Ä≤\n2; ...; x‚Ä≤\nn]\nfor a classiÔ¨Åer FsatisÔ¨Åes: (1) F(x) = o(x) = o(x‚Ä≤) but F(x‚Ä≤) Ã∏= o(x‚Ä≤), where o(¬∑) is the oracle\n(e.g., human decision-maker); (2) ||ti ‚àít‚Ä≤\ni||2 ‚â§œµfor i= 1,2,...,n , where œµ‚â•0 and ti is the word\nembedding of xi.\n3.1 I NFORMATION BOTTLENECK AS A REGULARIZER\nIn this section, we Ô¨Årst discuss the general IB implementation, and then explain how IB formulation\nis adapted to InfoBERT as a regularizer along with theoretical analysis to support why IB regularizer\ncan help improve the robustness of language models. The IB principle formulates the goal of deep\nlearning as an information-theoretic trade-off between representation compression and predictive\npower (Tishby & Zaslavsky, 2015). Given the input source X, a deep neural net learns the internal\nrepresentation T of some intermediate layer and maximizes the MI between T and label Y, so that\nT subject to a constraint on its complexity contains sufÔ¨Åcient information to infer the target labelY.\nFinding an optimal representation T can be formulated as the maximization of the Lagrangian\nLIB = I(Y; T) ‚àíŒ≤I(X; T), (1)\nwhere Œ≤ >0 is a hyper-parameter to control the tradeoff, and I(Y; T) is deÔ¨Åned as:\nI(Y; T) =\n‚à´\np(y,t) log p(y,t)\np(y)p(t) dydt. (2)\nSince Eq. (2) is intractable, we instead use the lower bound from Barber & Agakov (2003):\nI(Y; T) ‚â•\n‚à´\np(y,t) logqœà(y|t) dydt, (3)\nwhere qœà(y|t) is the variational approximation learned by a neural network parameterized by œàfor\nthe true distribution p(y|t). This indicates that maximizing the lower bound of the Ô¨Årst term of IB\nI(Y; T) is equivalent to minimizing the task cross-entropy loss ‚Ñìtask = H(Y |T).\nTo derive a tractable lower bound of IB, we here use an upper bound (Cheng et al., 2020) ofI(X; T)\nI(X; T) ‚â§\n‚à´\np(x,t) log(p(t|x)) dxdt ‚àí\n‚à´\np(x)p(t) log(p(t|x)) dxdt. (4)\nBy combining Eq. (3) and (4), we can maximize the tractable lower bound ÀÜLIB of IB in practice by:\nÀÜLIB = 1\nN\nN‚àë\ni=1\n[\nlog qœà(y(i) |t(i))\n]\n‚àíŒ≤\nN\nN‚àë\ni=1\n[\nlog(p(t(i) |x(i))) ‚àí 1\nN\nN‚àë\nj=1\nlog(p(t(j) |x(i)))\n]\n(5)\nwith data samples {x(i),y(i)}N\ni=1, where qœà can represent any classiÔ¨Åcation model (e.g., BERT), and\np(t|x) can be viewed as the feature extractor fŒ∏ : X‚ÜíT , where Xand T are the support of the\ninput source X and extracted feature T, respectively.\nThe above is a general implementation of IB objective function. In InfoBERT, we consider T as\nthe features consisting of the local word-level features after the BERT embedding layer fŒ∏. The\nfollowing BERT self-attentive layers along with the linear classiÔ¨Åcation head serve as qœà(y|t) that\npredicts the target Y given representation T.\nFormally, given random variablesX = [X1; X2; ...; Xn] representing input sentences withXi(word\ntoken at i-th index), let T = [T1; ...; Tn] = fŒ∏([X1; X2; ...; Xn]) = [ fŒ∏(X1); fŒ∏(X2); ...; fŒ∏(Xn)]\n3\nPublished as a conference paper at ICLR 2021\ndenote the random variables representing the features generated from input X via the BERT em-\nbedding layer fŒ∏, where Ti ‚ààRd is the high-dimensional word-level local feature for word Xi.\nDue to the high dimensionality dof each word feature ( e.g., 1024 for BERT-large), when the sen-\ntence length nincreases, the dimensionality of features T becomes too large to compute I(X; T) in\npractice. Thus, we propose to maximize a localized formulation of IB LLIB deÔ¨Åned as:\nLLIB := I(Y; T) ‚àínŒ≤\nn‚àë\ni=1\nI(Xi; Ti). (6)\nTheorem 3.1. (Lower Bound of LIB) Given a sequence of random variables X = [X1; X2; ...; Xn]\nand a deterministic feature extractor fŒ∏, let T = [T1; ...; Tn] = [fŒ∏(X1); fŒ∏(X2); ...; fŒ∏(Xn)]. Then\nthe localized formulation of IB LLIB is a lower bound of LIB (Eq. (1)), i.e.,\nI(Y; T) ‚àíŒ≤I(X; T) ‚â•I(Y; T) ‚àínŒ≤\nn‚àë\ni=1\nI(Xi; Ti). (7)\nTheorem 3.1 indicates that we can maximize the localized formulation of LLIB as a lower bound of\nIB LIB when I(X; T) is difÔ¨Åcult to compute. In Eq. (6), if we regard the Ô¨Årst term ( I(Y; T)) as a\ntask-related objective, the second term (‚àínŒ≤‚àën\ni=1 I(Xi; Ti)) can be considered as a regularization\nterm to constrain the complexity of representationT, thus named as Information Bottleneck regular-\nizer. Next, we give a theoretical analysis for the adversarial robustness of IB and demonstrate why\nlocalized IB objective function can help improve the robustness to adversarial attacks.\nFollowing DeÔ¨Ånition 3.1, let T = [T1; T2; ...; Tn] and T‚Ä≤ = [T‚Ä≤\n1; T‚Ä≤\n2; ...; T‚Ä≤\nn] denote the features for\nthe benign sentence X and adversarial sentence X‚Ä≤. The distributions of X and X‚Ä≤are denoted\nby probability p(x) and q(x) with the support Xand X‚Ä≤, respectively. We assume that the feature\nrepresentation T has Ô¨Ånite support denoted by T considering the Ô¨Ånite vocabulary size in NLP.\nTheorem 3.2. (Adversarial Robustness Bound) For random variables X = [X1; X2; ...; Xn] and\nX‚Ä≤ = [ X‚Ä≤\n1; X‚Ä≤\n2; ...; X‚Ä≤\nn], let T = [ T1; T2; ...; Tn] = [ fŒ∏(X1); fŒ∏(X2); ...; fŒ∏(Xn)] and T‚Ä≤ =\n[T‚Ä≤\n1; T‚Ä≤\n2; ...; T‚Ä≤\nn] = [ fŒ∏(X‚Ä≤\n1); fŒ∏(X‚Ä≤\n2); ...; fŒ∏(X‚Ä≤\nn)] with Ô¨Ånite support T, where fŒ∏ is a deterministic\nfeature extractor. The performance gap between benign and adversarial data |I(Y; T) ‚àíI(Y; T‚Ä≤)|\nis bounded above by\n|I(Y; T) ‚àíI(Y; T‚Ä≤)|‚â§ B0 + B1\nn‚àë\ni=1\n‚àö\n|T|(I(Xi; Ti))1/2 + B2\nn‚àë\ni=1\n|T|3/4(I(Xi; Ti))1/4\n+ B3\nn‚àë\ni=1\n‚àö\n|T|(I(X‚Ä≤\ni; T‚Ä≤\ni))1/2 + B4\nn‚àë\ni=1\n|T|3/4(I(X‚Ä≤\ni; T‚Ä≤\ni))1/4, (8)\nwhere B0,B1,B2,B3 and B4 are constants depending on the sequence length n, œµand p(x).\nThe sketch of the proof is to express the difference of |I(Y; T) ‚àíI(Y‚Ä≤; T)|in terms of I(Xi; Ti).\nSpeciÔ¨Åcally, Eq. (25) factorizes the difference into two summands. The Ô¨Årst summand, the con-\nditional entropy |H(T | Y) ‚àíH(T‚Ä≤ | Y)|, can be bound by Eq. (42) in terms of MI be-\ntween benign/adversarial input and representation I(Xi; Ti) and I(X‚Ä≤\ni; T‚Ä≤\ni). The second summand\n|H(T) ‚àíH(T‚Ä≤)|has a constant upper bound (Eq. (85)), since language models have bounded vo-\ncabulary size and embedding space, and thus have bounded entropy.\nThe intuition of Theorem 3.2 is to bound the adversarial performance drop |I(Y; T) ‚àíI(Y; T‚Ä≤)|\nby I(Xi; Ti). As explained in Eq. (3), I(Y; T) and I(Y; T‚Ä≤) can be regarded as the model perfor-\nmance on benign and adversarial data. Thus, the LHS of the bound represents such a performance\ngap. The adversarial robustness bound of Theorem 3.2 indicates that the performance gap becomes\ncloser when I(Xi; Ti) and I(X‚Ä≤\ni; T‚Ä≤\ni) decrease. Note that our IB regularizer in the objective function\nEq. (6) achieves the same goal of minimizing I(Xi; Ti) while learning the most efÔ¨Åcient informa-\ntion features, or approximate minimal sufÔ¨Åcient statistics, for downstream tasks. Theorem 3.2 also\nsuggests that combining adversarial training with our IB regularizer can further minimizeI(X‚Ä≤\ni; T‚Ä≤\ni),\nleading to better robustness, which is veriÔ¨Åed in ¬ß4.\n3.2 A NCHORED FEATURE REGULARIZER\nIn addition to the IB regularizer that suppresses noisy information that may incur adversarial at-\ntacks, we propose a novel regularizer termed ‚ÄúAnchored Feature Regularizer‚Äù, which extracts local\n4\nPublished as a conference paper at ICLR 2021\nAlgorithm 1 - Local Anchored Feature Extraction.This algorithm takes in the word local features\nand returns the index of local anchored features.\n1: Input: Word local features t, upper and lower threshold ch and cl\n2: Œ¥‚Üê0 // Initialize the perturbation vector Œ¥\n3: g(Œ¥) = ‚àáŒ¥‚Ñìtask(qœà(t+ Œ¥),y) // Perform adversarial attack on the embedding space\n4: Sort the magnitude of the gradient of the perturbation vector from\n||g(Œ¥)1||2,||g(Œ¥)2||2,..., ||g(Œ¥)n||2 into ||g(Œ¥)k1 ||2,||g(Œ¥)k2 ||2,..., ||g(Œ¥)kn||2 in ascending\norder, where zi corresponds to its original index.\n5: Return: ki,ki+1,...,k j, where cl ‚â§ i\nn ‚â§j\nn ‚â§ch.\nstable features and aligns them with sentence global representations, thus improving the stability\nand robustness of language representations.\nThe goal of the local anchored feature extraction is to Ô¨Ånd features that carry useful and stable in-\nformation for downstream tasks. Instead of directly searching for local anchored features, we start\nwith searching for nonrobust and unuseful features. To identify local nonrobust features, we perform\nadversarial attacks to detect which words are prone to changes under adversarial word substitution.\nWe consider these vulnerable words as features nonrobust to adversarial threats. Therefore, global\nrobust sentence representations should rely less on these vulnerable statistical clues. On the other\nhand, by examining the adversarial perturbation on each local word feature, we can also identify\nwords that are less useful for downstream tasks. For example, stopwords and punctuation usually\ncarry limited information, and tend to have smaller adversarial perturbations than words containing\nmore effective information. Although these unuseful features are barely changed under adversarial\nattacks, they contain insufÔ¨Åcient information and should be discarded. After identifying the non-\nrobust and unuseful features, we treat the remaining local features in the sentences as useful stable\nfeatures and align the global feature representation based on them.\nDuring the local anchored feature extraction, we perform ‚Äúvirtual‚Äù adversarial attacks that generate\nadversarial perturbation in the embedding space, as it abstracts the general idea for existing word-\nlevel adversarial attacks. Formally, given an input sentencex= [x1; x2; ...; xn] with its correspond-\ning local embedding representation t= [t1; ...; tn], where xand tare the realization of random vari-\nables X and T, we generate adversarial perturbation Œ¥in the embedding space so that the task loss\n‚Ñìtask increases. The adversarial perturbation Œ¥is initialized to zero, and the gradient of the loss with\nrespect to Œ¥is calculated by g(Œ¥) = ‚àáŒ¥‚Ñìtask(qœà(t+Œ¥),y) to update Œ¥‚Üê‚àè\n||Œ¥||F ‚â§œµ(Œ∑g(Œ¥)/||g(Œ¥)||F).\nThe above process is similar to one-step PGD with zero-initialized perturbation Œ¥. Since we only\ncare about the ranking of perturbation to decide on robust features, in practice we skip the update of\nŒ¥to save computational cost, and simply examine the ‚Ñì2 norm of the gradient g(Œ¥)i of the perturba-\ntion on each word featureti. A feasible plan is to choose the words whose perturbation is neither too\nlarge (nonrobust features) nor too small (unuseful features),e.g., the words whose perturbation rank-\nings are among 50% ‚àº80% of all the words. The detailed procedures are provided in Algorithm 1.\nAfter local anchored features are extracted, we propose to align sentence global representations Z\nwith our local anchored features Ti. In practice, we can use the Ô¨Ånal-layer [CLS] embedding to\nrepresent global sentence-level feature Z. SpeciÔ¨Åcally, we use the information theoretic tool to\nincrease the mutual information I(Ti; Z) between local anchored features Ti and sentence global\nrepresentations Z, so that the global representations can share more robust and useful information\nwith the local anchored features and focus less on the nonrobust and unuseful ones. By incorporating\nthe term I(Ti; Z) into the previous objective function Eq. (6), our Ô¨Ånal objective function becomes:\nmax I(Y; T) ‚àínŒ≤\nn‚àë\ni=1\nI(Xi; Ti) + Œ±\nM‚àë\nj=1\nI(Tkj ; Z), (9)\nwhere Tkj are the local anchored features selected by Algorithm 1 and M is the number of local\nanchored features. An illustrative Ô¨Ågure can be found in Appendix Figure 2.\n5\nPublished as a conference paper at ICLR 2021\nIn addition, due to the intractability of computing MI, we use InfoNCE (van den Oord et al., 2018)\nas the lower bound of MI to approximate the last term I(Tkj ; Z):\nÀÜI(InfoNCE)(Ti; Z) := EP\n[\ngœâ(ti,z) ‚àíEÀúP\n[\nlog\n‚àë\nt‚Ä≤\ni\negœâ(t‚Ä≤\ni,z)\n]]\n, (10)\nwhere gœâ(¬∑,¬∑) is a score function (or critic function) approximated by a neural network, ti are the\npositive samples drawn from the joint distribution P of local anchored features and global repre-\nsentations, and t‚Ä≤\ni are the negative samples drawn from the distribution of nonrobust and unuseful\nfeatures ÀúP.\n4 E XPERIMENTS\nIn this section, we demonstrate how effective InfoBERT improves the robustness of language models\nover multiple NLP tasks such as NLI and QA. We evaluate InfoBERT against both strong adversarial\ndatasets and state-of-the-art adversarial attacks.\n4.1 E XPERIMENTAL SETUP\nAdversarial Datasets The following adversarial datasets and adversarial attacks are used to eval-\nuate the robustness of InfoBERT and baselines. (I) Adversarial NLI (ANLI) (Nie et al., 2020)\nis a large-scale NLI benchmark, collected via an iterative, adversarial, human-and-model-in-the-\nloop procedure to attack BERT and RoBERTa. ANLI dataset is a strong adversarial dataset which\ncan easily reduce the accuracy of BERT Large to 0%. (II) Adversarial SQuAD (Jia & Liang, 2017)\ndataset is an adversarial QA benchmark dataset generated by a set of handcrafted rules and reÔ¨Åned\nby crowdsourcing. Since adversarial training data is not provided, we Ô¨Åne-tune RoBERTa Large on\nbenign SQuAD training data (Rajpurkar et al., 2016) only, and test the models on both benign and\nadversarial test sets. (III) TextFooler (Jin et al., 2020) is the state-of-the-art word-level adversarial\nattack method to generate adversarial examples. To create an adversarial evaluation dataset, we sam-\npled 1,000 examples from the test sets of SNLI and MNLI respectively, and run TextFooler against\nBERTLarge and RoBERTaLarge to obtain the adversarial text examples.\nBaselines Since IBP-based methods (Huang et al., 2019; Jia et al., 2019) cannot be applied to large-\nscale language models yet, and the randomized-smoothing-based method (Ye et al., 2020) achieves\nlimited certiÔ¨Åed robustness, we compare InfoBERT against three competitive baselines based on\nadversarial training: (I) FreeLB (Zhu et al., 2020a) applies adversarial training to language models\nduring Ô¨Åne-tuning stage to improve generalization. In ¬ß4.2, we observe that FreeLB can boost the\nrobustness of language models by a large margin. (II) SMART (Jiang et al., 2020) uses adversarial\ntraining as smoothness-inducing regularization and Bregman proximal point optimization during\nÔ¨Åne-tuning, to improve the generalization and robustness of language models. (III) ALUM (Liu\net al., 2020) performs adversarial training in both pre-training and Ô¨Åne-tuning stages, which achieves\nsubstantial performance gain on a wide range of NLP tasks. Due to the high computational cost of\nadversarial training, we compare InfoBERT to ALUM and SMART with the best results reported in\nthe original papers.\nEvaluation Metrics We use robust accuracy or robust F1 score to measure how robust the baseline\nmodels and InfoBERT are when facing adversarial data. SpeciÔ¨Åcally, robust accuracy is calculated\nby: Acc = 1\n|Dadv|\n‚àë\nx‚Ä≤‚ààDadv 1 [arg maxqœà(fŒ∏(x‚Ä≤)) ‚â°y], where Dadv is the adversarial dataset, y is\nthe ground-truth label,arg maxselects the class with the highest logits and1 (¬∑) is the indicator func-\ntion. Similarly, robust F1 score is calculated by: F1 = 1\n|Dadv|\n‚àë\nx‚Ä≤‚ààDadv v(arg maxqœà(fŒ∏(x‚Ä≤)),a),\nwhere v(¬∑,¬∑) is the F1 score between the true answeraand the predicted answerarg maxqœà(fŒ∏(x‚Ä≤)),\nand arg max selects the answer with the highest probability (see Rajpurkar et al. (2016) for details).\nImplementation Details To demonstrate InfoBERT is effective for different language models, we\napply InfoBERT to both pretrained RoBERTaLarge and BERTLarge. Since InfoBERT can be applied\nto both standard training and adversarial training, we here use FreeLB as the adversarial training\nimplementation. InfoBERT is Ô¨Åne-tuned for 2 epochs for the QA task, and 3 epochs for the NLI\ntask. More implementation details such as Œ±,Œ≤,c h,cl selection can be found in Appendix A.1.\n6\nPublished as a conference paper at ICLR 2021\nTraining Model Method Dev Test\nA1 A2 A3 ANLI A1 A2 A3 ANLI\nStandard\nTraining\nRoBERTa Vanilla 49.1 26.5 27.2 33.8 49.2 27.6 24.8 33.2\nInfoBERT 47.8 31.2 31.8 36.6 47.3 31.2 31.1 36.2\nBERT Vanilla 20.7 26.9 31.2 26.6 21.8 28.3 28.8 26.5\nInfoBERT 26.0 30.1 31.2 29.2 26.4 29.7 29.8 28.7\nAdversarial\nTraining\nRoBERTa FreeLB 50.4 28.0 28.5 35.2 48.1 30.4 26.3 34.4\nInfoBERT 48.4 29.3 31.3 36.0 50.0 30.6 29.3 36.2\nBERT FreeLB 23.0 29.0 32.2 28.3 22.2 28.5 30.8 27.4\nInfoBERT 28.3 30.2 33.8 30.9 25.9 28.1 30.3 28.2\nTable 1: Robust accuracy on the ANLI dataset. Models are trained on the benign datasets (MNLI +\nSNLI) only. ‚ÄòA1-A3‚Äô refers to the rounds with increasing difÔ¨Åculty. ‚ÄòANLI‚Äô refers to A1+A2+A3.\nTraining Model Method Dev Test\nA1 A2 A3 ANLI A1 A2 A3 ANLI\nStandard\nTraining\nRoBERTa Vanilla 74.1 50.8 43.9 55.5 73.8 48.9 44.4 53.7\nInfoBERT 75.2 49.6 47.8 56.9 73.9 50.8 48.8 57.3\nBERT Vanilla 58.5 46.1 45.5 49.8 57.4 48.3 43.5 49.3\nInfoBERT 59.3 48.9 45.5 50.9 60.0 46.9 44.8 50.2\nAdversarial\nTraining\nRoBERTa\nFreeLB 75.2 47.4 45.3 55.3 73.3 50.5 46.8 56.2\nSMART 74.5 50.9 47.6 57.1 72.4 49.8 50.3 57.1\nALUM 73.3 53.4 48.2 57.7 72.3 52.1 48.4 57.0\nInfoBERT 76.4 51.7 48.6 58.3 75.5 51.4 49.8 58.3\nBERT\nFreeLB 60.3 47.1 46.3 50.9 60.3 46.8 44.8 50.2\nALUM 62.0 48.6 48.1 52.6 61.3 45.9 44.3 50.1\nInfoBERT 60.8 48.7 45.9 51.4 63.3 48.7 43.2 51.2\nTable 2: Robust accuracy on the ANLI dataset. Models are trained on both adversarial and benign\ndatasets (ANLI (training) + FeverNLI + MNLI + SNLI).\n4.2 E XPERIMENTAL RESULTS\nEvaluation on ANLI As ANLI provides an adversarial training dataset, we evaluate models in two\nsettings: 1) training models on benign data (MNLI (Williams et al., 2018) + SNLI (Bowman et al.,\n2015)) only, which is the case when the adversarial threat model is unknown; 2) training models\non both benign and adversarial training data (SNLI+MNLI+ANLI+FeverNLI), which assumes the\nthreat model is known in advance.\nResults of the Ô¨Årst setting are summarized in Table 1. The vanilla RoBERTa and BERT models\nperform poorly on the adversarial dataset. In particular, vanilla BERT Large with standard training\nachieves the lowest robust accuracy of 26.5% among all the models. We also evaluate the robust-\nness improvement by performing adversarial training during Ô¨Åne-tuning, and observe that adversar-\nial training for language models can improve not only generalization but also robustness. In contrast,\nInfoBERT substantially improves robust accuracy in both standard and adversarial training. The ro-\nbust accuracy of InfoBERT through standard training is even higher than the adversarial training\nbaseline FreeLB for both RoBERTa and BERT, while the training time of InfoBERT is 1/3 ‚àº1/2\nless than FreeLB. This is mainly because FreeLB requires multiple steps of PGD attacks to gener-\nate adversarial examples, while InfoBERT essentially needs only 1-step PGD attack for anchored\nfeature selection.\nResults of the second setting are provided in Table 2, which shows InfoBERT can further improve\nrobust accuracy for both standard and adversarial training. SpeciÔ¨Åcally, when combined with adver-\nsarial training, InfoBERT achieves the state-of-the-art robust accuracy of 58.3%, outperforming all\nexisting baselines. Note that although ALUM achieves higher accuracy for BERT on the dev set, it\ntends to overÔ¨Åt on the dev set, therefore performing worse than InfoBERT on the test set.\n7\nPublished as a conference paper at ICLR 2021\nTraining Model Method SNLI MNLI adv-SNLI adv-MNLI adv-SNLI adv-MNLI\n(m/mm) (BERT) (BERT) (RoBERTa) (RoBERTa)\nStandard\nTraining\nRoBERTa Vanilla 92.6 90.8/90.6 56.6 68.1/68.6 19.4 24.9/24.9\nInfoBERT 93.3 90.5/90.4 59.8 69.8/70.6 42.5 50.3/52.1\nBERT Vanilla 91.3 86.7/86.4 0.0 0.0/0.0 44.9 57.0/57.5\nInfoBERT 91.7 86.2/86.0 36.7 43.5 /46.6 45.4 57.2/58.6\nAdversarial\nTraining\nRoBERTa FreeLB 93.4 90.1/90.3 60.4 70.3/72.1 41.2 49.5/50.6\nInfoBERT 93.1 90.7/90.4 62.3 73.2/73.1 43.4 56.9/55.5\nBERT FreeLB 92.4 86.9/86.5 46.6 60.0/60.7 50.5 64.0/62.9\nInfoBERT 92.2 87.2/87.2 50.8 61.3/62.7 52.6 65.6/67.3\nTable 3: Robust accuracy on the adversarial SNLI and MNLI(-m/mm) datasets generated by\nTextFooler based on blackbox BERT/RoBERTa (denoted in brackets of the header). Models are\ntrained on the benign datasets (MNLI+SNLI) only.\nTraining Method benign AddSent AddOneSent\nStandard\nTraining\nVanilla 93.5/86.9 72.9/66.6 80.6/74.3\nInfoBERT 93.5/87.0 78.5/72.9 84.6/78.3\nAdversarial\nTraining\nFreeLB 93.8/87.3 76.3/70.3 82.3/76.2\nALUM - 75.5/69.4 81.4/75.9\nInfoBERT 93.7/87.0 78.0/71.8 83.6/77.1\nTable 4: Robust F1/EM scores based on RoBERTaLarge on\nthe adversarial SQuAD datasets (AddSent and AddOne-\nSent). Models are trained on standard SQuAD 1.0 dataset.\n0\n0.02\n0.04\n0.06\n0.08\nMI Improvement after adding adv \nexamples in the training set\nAdversarial Test Data Benign Test Data\n‚àÜùêº!\n\" ‚àÜùêº!\n\t ‚àÜùêº$\n\" ‚àÜùêº$\n\t\nFigure 1: Local anchored features con-\ntribute more to MI improvement than\nnonrobust/unuseful features, unveiling\ncloser relation with robustness.\nEvaluation against TextFooler InfoBERT can defend against not only human-crafted adversarial\nexamples (e.g., ANLI) but also those generated by adversarial attacks (e.g., TextFooler). Results are\nsummarized in Table 3. We can see that InfoBERT barely affects model performance on the benign\ntest data, and in the case of adversarial training, InfoBERT even boosts the benign test accuracy. Un-\nder the TextFooler attack, the robust accuracy of the vanilla BERT drops to0.0% on both MNLI and\nSNLI datasets, while RoBERTa drops from 90% to around 20%. We observe that both adversarial\ntraining and InfoBERT with standard training can improve robust accuracy by a comparable large\nmargin, while InfoBERT with adversarial training achieves the best performance among all models,\nconÔ¨Årming the hypothesis in Theorem 3.2 that combining adversarial training with IB regularizer\ncan further minimize I(X‚Ä≤\ni; T‚Ä≤\ni), leading to better robustness than the vanilla one.\nEvaluation on Adversarial SQuADPrevious experiments show that InfoBERT can improve model\nrobustness for NLI tasks. Now we demonstrate that InfoBERT can also be adapted to other NLP\ntasks such as QA in Table 4. Similar to our observation on NLI dataset, we Ô¨Ånd that InfoBERT\nbarely hurts the performance on the benign test data, and even improves it in some cases. Moreover,\nInfoBERT substantially improves model robustness when presented with adversarial QA test sets\n(AddSent and AddOneSent). While adversarial training does help improve robustness, InfoBERT\ncan further boost the robust performance by a larger margin. In particular, InfoBERT through stan-\ndard training achieves the state-of-the-art robust F1/EM score as 78.5/72.9 compared to existing\nadversarial training baselines, and in the meantime requires only half the training time of adversarial-\ntraining-based methods.\n4.3 A NALYSIS OF LOCAL ANCHORED FEATURES\nWe conduct an ablation study to further validate that our anchored feature regularizer indeed Ô¨Ål-\nters out nonrobust/unuseful information. As shown in Table 1 and 2, adding adversarial data in\nthe training set can signiÔ¨Åcantly improve model robustness. To Ô¨Ånd out what helps improve the\nrobustness from the MI perspective, we Ô¨Årst calculate the MI between anchored features and global\nfeatures 1\nM\n‚àëM\nj=1 I(Tkj ; Z) on the adversarial test data and benign test data, based on the model\ntrained without adversarial training data (denoted by I‚Ä≤\nR and IR). We then calculate the MI between\nnonrobust/unuseful features and global features 1\nM‚Ä≤\n‚àëM‚Ä≤\ni=1 I(Tki; Z) on the adversarial test data and\n8\nPublished as a conference paper at ICLR 2021\nbenign data as well (denoted by I‚Ä≤\nN and IN). After adding adversarial examples into the training set\nand re-training the model, we Ô¨Ånd that the MI between the local features and the global features\nsubstantially increases on the adversarial test data, which accounts for the robustness improvement.\nWe also observe that those local anchored features extracted by our anchored feature regularizer,\nas expected, contribute more to the MI improvement. As shown in Figure 1, the MI improvement\nof anchored features on adversarial test data ‚àÜI‚Ä≤\nR (red bar on the left) is higher than that of nonro-\nbust/unuseful ‚àÜI‚Ä≤\nN (red bar on the right), thus conÔ¨Årming that local anchored features discovered by\nour anchored feature regularizer have a stronger impact on robustness than nonrobust/unuseful ones.\nWe conduct more ablation studies in Appendix ¬ßA.2, including analyzing the individual impact of\ntwo regularizers, the difference between global and local features for IB regularizer, hyper-parameter\nselection strategy and so on.\n5 C ONCLUSION\nIn this paper, we propose a novel learning framework InfoBERT from an information theoretic per-\nspective to perform robust Ô¨Åne-tuning over pre-trained language models. SpeciÔ¨Åcally, InfoBERT\nconsists of two novel regularizers to improve the robustness of the learned representations: (a) In-\nformation Bottleneck Regularizer, learning to extract the approximated minimal sufÔ¨Åcient statistics\nand denoise the excessive spurious features, and (b) Local Anchored Feature Regularizer, which\nimproves the robustness of global features by aligning them with local anchored features. Supported\nby our theoretical analysis, InfoBERT provides a principled way to improve the robustness of BERT\nand RoBERTa against strong adversarial attacks over a variety of NLP tasks, including NLI and\nQA tasks. Comprehensive experiments demonstrate that InfoBERT outperforms existing baseline\nmethods and achieves new state of the art on different adversarial datasets. We believe this work will\nshed light on future research directions towards improving the robustness of representation learning\nfor language models.\n6 A CKNOWLEDGEMENT\nWe gratefully thank the anonymous reviewers and meta-reviewers for their constructive feedback.\nWe also thank Julia Hockenmaier, Alexander Schwing, Sanmi Koyejo, Fan Wu, Wei Wang, Pengyu\nCheng, and many others for the helpful discussion. This work is partially supported by NSF grant\nNo.1910100, DARPA QED-RML-FP-003, and the Intel RSA 2020.\nREFERENCES\nMoustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani B. Srivastava, and Kai-Wei\nChang. Generating natural language adversarial examples. In Ellen Riloff, David Chiang, Julia\nHockenmaier, and Jun‚Äôichi Tsujii (eds.),EMNLP, pp. 2890‚Äì2896. Association for Computational\nLinguistics, 2018.\nDavid Barber and Felix V . Agakov. The im algorithm: A variational approach to information maxi-\nmization. In NeurIPS, 2003.\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron\nCourville, and Devon Hjelm. Mutual information neural estimation. In Jennifer Dy and An-\ndreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, vol-\nume 80 of Proceedings of Machine Learning Research, pp. 531‚Äì540, Stockholmsm¬®assan, Stock-\nholm Sweden, 10‚Äì15 Jul 2018. PMLR.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large an-\nnotated corpus for learning natural language inference. In Llu ¬¥ƒ±s M`arquez, Chris Callison-Burch,\nJian Su, Daniele Pighin, and Yuval Marton (eds.), EMNLP, pp. 632‚Äì642. The Association for\nComputational Linguistics, 2015.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\n9\nPublished as a conference paper at ICLR 2021\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. 2020.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. CoRR, abs/2002.05709, 2020.\nPengyu Cheng, Weituo Hao, Shuyang Dai, Jiachang Liu, Zhe Gan, and L. Carin. Club: A contrastive\nlog-ratio upper bound of mutual information. ArXiv, abs/2006.12013, 2020.\nJeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. CertiÔ¨Åed adversarial robustness via random-\nized smoothing. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), ICML, volume 97 of\nProceedings of Machine Learning Research, pp. 1310‚Äì1320. PMLR, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and\nThamar Solorio (eds.), NAACL-HLT, pp. 4171‚Äì4186. Association for Computational Linguistics,\n2019.\nKrishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan\nO‚ÄôDonoghue, Jonathan Uesato, and Pushmeet Kohli. Training veriÔ¨Åed learners with learned ver-\niÔ¨Åers. CoRR, abs/1805.10265, 2018.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. HotÔ¨Çip: White-box adversarial examples\nfor text classiÔ¨Åcation. In Iryna Gurevych and Yusuke Miyao (eds.), ACL, pp. 31‚Äì36. Association\nfor Computational Linguistics, 2018.\nKevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul\nPrakash, Tadayoshi Kohno, and Dawn Xiaodong Song. Robust physical-world attacks on deep\nlearning models. 2017.\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\ntraining for vision-and-language representation learning. arXiv preprint arXiv:2006.06195, 2020.\nIan J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. CoRR, abs/1412.6572, 2015.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam\nTrischler, and Yoshua Bengio. Learning deep representations by mutual information estimation\nand maximization. In ICLR, 2019.\nPo-Sen Huang, Robert Stanforth, Johannes Welbl, Chris Dyer, Dani Yogatama, Sven Gowal, Krish-\nnamurthy Dvijotham, and Pushmeet Kohli. Achieving veriÔ¨Åed robustness to symbol substitutions\nvia interval bound propagation. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.),\nEMNLP-IJCNLP, pp. 4081‚Äì4091. Association for Computational Linguistics, 2019.\nMohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. Adversarial example generation\nwith syntactically controlled paraphrase networks. In Marilyn A. Walker, Heng Ji, and Amanda\nStent (eds.), NAACL-HLT, pp. 1875‚Äì1885. Association for Computational Linguistics, 2018.\nJoern-Henrik Jacobsen, Jens Behrmann, Richard Zemel, and Matthias Bethge. Excessive invariance\ncauses adversarial vulnerability. In ICLR, 2019.\nRobin Jia and Percy Liang. Adversarial examples for evaluating reading comprehension systems. In\nMartha Palmer, Rebecca Hwa, and Sebastian Riedel (eds.), EMNLP, pp. 2021‚Äì2031. Association\nfor Computational Linguistics, 2017.\nRobin Jia, Aditi Raghunathan, Kerem G ¬®oksel, and Percy Liang. CertiÔ¨Åed robustness to adversarial\nword substitutions. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), EMNLP-\nIJCNLP, pp. 4127‚Äì4140. Association for Computational Linguistics, 2019.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. SMART:\nrobust and efÔ¨Åcient Ô¨Åne-tuning for pre-trained natural language models through principled regu-\nlarized optimization. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (eds.),\nACL, pp. 2177‚Äì2190. Association for Computational Linguistics, 2020.\n10\nPublished as a conference paper at ICLR 2021\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is BERT really robust? A strong baseline\nfor natural language attack on text classiÔ¨Åcation and entailment. In AAAI, pp. 8018‚Äì8025. AAAI\nPress, 2020.\nLingpeng Kong, Cyprien de Masson d‚ÄôAutume, Lei Yu, Wang Ling, Zihang Dai, and Dani Yo-\ngatama. A mutual information maximization perspective of language representation learning. In\nICLR, 2020.\nJinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. Textbugger: Generating adversarial text\nagainst real-world applications. In NDSS. The Internet Society, 2019.\nRalph Linsker. Self-organization in a perceptual network. Computer, 21(3):105‚Äì117, 1988.\nXiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu Wang, Hoifung Poon, and Jianfeng\nGao. Adversarial training for large neural language models. CoRR, abs/2004.08994, 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692, 2019.\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: A simple and\naccurate method to fool deep neural networks. CVPR, pp. 2574‚Äì2582, 2016.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adver-\nsarial NLI: A new benchmark for natural language understanding. In Dan Jurafsky, Joyce Chai,\nNatalie Schluter, and Joel R. Tetreault (eds.),ACL, pp. 4885‚Äì4901. Association for Computational\nLinguistics, 2020.\nNicolas Papernot, Patrick D. McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation\nas a defense to adversarial perturbations against deep neural networks. 2016 IEEE Symposium on\nSecurity and Privacy (SP), pp. 582‚Äì597, 2016.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions\nfor machine comprehension of text. In Jian Su, Xavier Carreras, and Kevin Duh (eds.), EMNLP,\npp. 2383‚Äì2392. The Association for Computational Linguistics, 2016.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che. Generating natural language adversarial\nexamples through probability weighted word saliency. In Anna Korhonen, David R. Traum, and\nLlu¬¥ƒ±s M`arquez (eds.), ACL, pp. 1085‚Äì1097. Association for Computational Linguistics, 2019.\nNikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar.\nA theoretical analysis of contrastive unsupervised representation learning. In Kamalika Chaud-\nhuri and Ruslan Salakhutdinov (eds.), ICML, volume 97 of Proceedings of Machine Learning\nResearch, pp. 5628‚Äì5637. PMLR, 2019.\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What\nmakes for good views for contrastive learning. CoRR, abs/2005.10243, 2020.\nN. Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE\nInformation Theory Workshop (ITW), pp. 1‚Äì5, 2015.\nA¬®aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. CoRR, abs/1807.03748, 2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von\nLuxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V . N. Vishwanathan, and Roman\nGarnett (eds.), NeurIPS, pp. 5998‚Äì6008, 2017.\nBoxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, and Bo Li. T3: Tree-\nautoencoder constrained adversarial text generation for targeted attack. In EMNLP, 2020.\nAdina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for\nsentence understanding through inference. In Marilyn A. Walker, Heng Ji, and Amanda Stent\n(eds.), NAACL-HLT, pp. 1112‚Äì1122. Association for Computational Linguistics, 2018.\n11\nPublished as a conference paper at ICLR 2021\nMao Ye, Chengyue Gong, and Qiang Liu. SAFER: A structure-free approach for certiÔ¨Åed robustness\nto adversarial word substitutions. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R.\nTetreault (eds.), ACL, pp. 3465‚Äì3475. Association for Computational Linguistics, 2020.\nYue Yu, Simiao Zuo, Haoming Jiang, Wendi Ren, Tuo Zhao, and Chao Zhang. Fine-tuning pre-\ntrained language model with weak supervision: A contrastive-regularized self-training approach.\nCoRR, abs/2010.07835, 2020.\nYuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun.\nWord-level textual adversarial attacking as combinatorial optimization. In Dan Jurafsky, Joyce\nChai, Natalie Schluter, and Joel R. Tetreault (eds.), ACL, pp. 6066‚Äì6080. Association for Com-\nputational Linguistics, 2020.\nYuan Zhang, Jason Baldridge, and Luheng He. PAWS: paraphrase adversaries from word scram-\nbling. In Jill Burstein, Christy Doran, and Thamar Solorio (eds.), NAACL-HLT, pp. 1298‚Äì1308.\nAssociation for Computational Linguistics, 2019.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. Freelb: Enhanced\nadversarial training for natural language understanding. In ICLR. OpenReview.net, 2020a.\nSicheng Zhu, Xiao Zhang, and David Evans. Learning adversarially robust representations via\nworst-case mutual information maximization. CoRR, abs/2002.11798, 2020b.\n12\nPublished as a conference paper at ICLR 2021\nA A PPENDIX\nAdversarial\nRobustness\nimproves\n(b) Information Bottleneck Regularizer\nBoth drop\ndue to aggresive\n compression\ndecreases\nBenign Accuracy \nbarely drops\nAccuracy Benign Acc\nAdversarial Acc\nInput\nEmbedding ...\nT0 T1 T2 T3 T4 T5 T6 T7 T8 T9 ...T10\nLocal\nFeatures\nGlobal\nFeatures\n[CLS] Two woman, both sitting near a pile of poker chips ,\nare playing cards . [SEP] Two woman playing poker . [SEP]\n(a) Task Objective\n(c) Local Anchored Feature Regularizer\n......\nBERT Encoder\n......\nBERT Encoder\n...BERT Encoder\nFigure 2: The complete objective function of InfoBERT, which can be decomposed into (a) standard\ntask objective, (b) Information Bottleneck Regularizer, and (c) Local Anchored Feature Regularizer.\nFor (b), we both theoretically and empirically demonstrate that we can improve the adversarial\nrobustness by decreasing the mutual information of I(Xi; Ti) without affecting the benign accuracy\nmuch. For (c), we propose to align the local anchored features Tkj (highlighted in Yellow) with the\nglobal feature Zby maximizing their mutual information I(Tkj ; Z).\nA.1 I MPLEMENTATION DETAILS\nModel Details1 BERT is a transformer (Vaswani et al., 2017) based model, which is unsupervised\npretrained on large corpora. We use BERT Large-uncased as the baseline model, which has 24 lay-\ners, 1024 hidden units, 16 self-attention heads, and 340M parameters. RoBERTa Large shares the\nsame architecture as BERT, but modiÔ¨Åes key hyperparameters, removes the next-sentence pretrain-\ning objective and trains with much larger mini-batches and learning rates, which results in higher\nperformance than BERT model on GLUE, RACE and SQuAD.\nStandard Training Details For both standard and adversarial training, we Ô¨Åne-tune InfoBERT for\n2 epochs on the QA task, and for 3 epochs on the NLI task. The best model is selected based on the\nperformance on the development set. All Ô¨Åne-tuning experiments are run on Nvidia V100 GPUs.\nFor NLI task, we set the batch size to 256, learning rate to 2 √ó10‚àí5, max sequence length to 128\nand warm-up steps to 1000. For QA task, we set the batch size to 32, learning rate to 3 √ó10‚àí5 and\nmax sequence length to 384 without warm-up steps.\nAdversarial Training Details 2 Adversarial training introduces hyper-parameters including ad-\nversarial learning rates, number of PGD steps, and adversarial norm. When combing adversarial\ntraining with InfoBERT, we use FreeLB as the adversarial training implementation, and set adver-\nsarial learning rate to10‚àí1 or 4‚àó10‚àí2, adversarial steps to3, maximal perturbation norm to3‚àó10‚àí1\nor 2 ‚àó10‚àí1 and initial random perturbation norm to 10‚àí1 or 0.\nInformation Bottleneck Regularizer Details For information bottleneck, there are different ways\nto model p(t|x):\n1. Assume that p(t |x) is unknown. We use a neural net parameterized by qŒ∏(t |x) to learn\nthe conditional distribution p(t |x). We assume the distribution is a Gaussian distribution. The\n1We use the huggingface implementation https://github.com/huggingface/transformers\nfor BERT and RoBERTa.\n2We follow the FreeLB implementations in https://github.com/zhuchen03/FreeLB.\n13\nPublished as a conference paper at ICLR 2021\nneural net qŒ∏ will learn the mean and variance of the Gaussian given inputxand representation t.\nBy reparameterization trick, the neural net can be backpropagated to approximate the distribution\ngiven the training samples.\n2. p(t|x) is known. Since tis the representation encoded by BERT, we actually already know the\ndistribution of p. We also denote it asqŒ∏, where Œ∏is the parameter of the BERT encoderfŒ∏. If we\nassume the conditional distribution is a Gaussian N(ti,œÉ) for input xi whose mean is the BERT\nrepresentation ti and variance is a Ô¨Åxed constant œÉ, the Eq.6 becomes\nLLIB = 1\nN\nN‚àë\ni=1\n(\n[\nlog qœà(y(i) |t(i))\n]\n‚àíŒ≤\nn‚àë\nk=1\n[\n‚àíc(œÉ)||t‚Ä≤(i)\nk ‚àít(i)\nk ||2\n2 + 1\nn\nN‚àë\nj=1\nc(œÉ)||tj ‚àítk||2\n2\n])\n,\n(11)\nwhere c(œÉ) is a positive constant related to œÉ. In practice, the sample t‚Ä≤\ni from the conditional\ndistribution Gaussian N(ti,œÉ) can be ti with some Gaussian noise, an adversarial examples of\nti, or ti itself (assume œÉ= 0).\nWe use the second way to model p(t |x) for InfoBERT Ô¨Ånally, as it gives higher robustness im-\nprovement than the Ô¨Årst way empirically (shown in the following ¬ßA.2). We suspect that the main\nreason is because the Ô¨Årst way needs to approximate the distribution p(t|x) via another neural net\nwhich could present some difÔ¨Åculty in model training.\nInformation Bottleneck Regularizer also introduces another parameterŒ≤to tune the trad-off between\nrepresentation compression I(Xi; Ti) and predictive power I(Y; T). We search for the optimal Œ≤\nvia grid search, and set Œ≤ = 5 √ó10‚àí2 for RoBERTa, Œ≤ = 10‚àí3 for BERT on the NLI task. On the\nQA task, we set Œ≤ = 5 √ó10‚àí5, which is substantially lower than Œ≤ on NLI tasks, thus containing\nmore word-level features. We think it is mainly because the QA task relies more on the word-level\nrepresentation to predict the exact answer spans. More ablation results can be found in the following\n¬ßA.2.\nAnchored Feature Regularizer Details Anchored Feature Regularizer uses Œ±to weigh the bal-\nance between predictive power and importance of anchored feature. We set Œ±= 5 √ó10‚àí3 for both\nNLI and QA tasks. Anchored Feature Regularizer also introduces upper and lower threshold cl and\nch for anchored feature extraction. We setch = 0.9 and cl = 0.5 for the NLI task, and setch = 0.95\nand cl = 0 .75 for the QA task. The neural MI estimator used by infoNCE uses two-layer fully\nconnected layer to estimate the MI with the intermediate layer hidden size set to 300.\nA.2 A DDITIONAL EXPERIMENTAL RESULTS\nA.2.1 A BLATION STUDY ON INFORMATION BOTTLENECK REGULARIZER\nModeling p(t | x) As discussed in ¬ßA.1, we have two ways to model p(t | x): ( i) using an\nauxiliary neural network to approximate the distribution; ( ii) directly using the BERT encoder fŒ∏\nto calculate the p(t |x). Thus we implemented these two methods and compare the robustness\nimprovement in Table 5. To eliminate other factors such as Anchored Feature Regularizer and\nadversarial training, we set Œ± = 0,Œ≤ = 5 √ó10‚àí2 and conduct the following ablation experiments\nvia standard training on standard datasets. We observe that although both modeling methods can\nimprove the model robustness, modeling as BERT encoder gives a larger margin than the Auxiliary\nNet. Moreover, the second way barely sacriÔ¨Åces the performance on benign data, while the Ô¨Årst\nway can hurt the benign accuracy a little bit. Therefore, we use the BERT Encoder fŒ∏ to model the\np(t|x) in our main paper.\nLocal Features v.s. Global Features Information Bottlenck Regularizer improves model robust-\nness by reducing I(X; T). In the main paper, we use T as word-level local features. Here we\nconsider T as sentence-level global features, and compare the robustness improvement with T as\nlocal features. To eliminate other factors such as Anchored Feature Regularizer and adversarial\ntraining, we set Œ±= 0,Œ≤ = 5 √ó10‚àí2 and conduct the following ablation experiments via standard\ntraining.\n14\nPublished as a conference paper at ICLR 2021\nModel Datasets Method Adversarial Accuracy Benign Accuracy\n(ANLI) (MNLI/SNLI)\nBERT Standard\nDatasets\nVanilla 26.5 86.7/91.3\nAuxiliary Net 27.1 83.1/90.7\nBERT Encoder fŒ∏ 27.7 85.9/91.7\nTable 5: Robust accuracy on the ANLI dataset. Here we refer ‚ÄúStandard Datasets‚Äù as training on\nthe benign datasets (MNLI + SNLI) only. ‚ÄúVanilla‚Äù refers to the vanilla BERT trained without\nInformation Bottleneck Regularizer.\nThe experimental results are summarized in Table 6. We can see that while both features can boost\nthe model robustness, using local features yield higher robust accuracy improvement than global\nfeatures, especially when adversarial training dataset is added.\nHyper-parameter Search We perform grid search to Ô¨Ånd out the optimal Œ≤ so that the optimal\ntrade-off between representation compression (‚Äúminimality‚Äù) and predictive power (‚ÄúsufÔ¨Åciency‚Äù)\nis achieved. An example to search for the optimal Œ≤ on QA dataset is shown in Fingure 3, which\nillustrates how Œ≤ affects the F1 score on benign and adversarial datasets. We can see that from a\nvery small Œ≤, both the robust and benign F1 scores increase, demonstrating InfoBERT can improve\nboth robustness and generalization to some extent. When we set Œ≤ = 5 √ó10‚àí5 (log(Œ≤) = ‚àí9.9),\nInfoBERT achieves the best benign and adversarial accuracy. When we set a larger Œ≤ to further\nminimize I(Xi; Ti), we observe that the benign F1 score starts to drop, indicating the increasingly\ncompressed representation could start to hurt its predictive capability.\nA.2.2 A BLATION STUDY ON ANCHORED FEATURE REGULARIZER\nVisualization of Anchored Words To explore which local anchored features are extracted, we\nconduct another ablation study to visualize the local anchored words. We follow the best hyper-\nparameters of Anchored Feature Regularizer introduced in ¬ßA.1, use the best BERT model trained\non benign datasets (MNLI + SNLI) only and test on the ANLI dev set. We visualize the local\nanchored words in Table 7 as follows. In the Ô¨Årst example, we Ô¨Ånd that Anchored Features mainly\nfocus on the important features such as quantity number ‚ÄúTwo‚Äù, the verb ‚Äúplaying‚Äù and objects\n‚Äúcard‚Äù/‚Äúpoker‚Äù to make a robust prediction. In the second example, the matching robust features\nbetween hypothesis and premise, such as ‚Äúpeople‚Äù, ‚Äúroller‚Äù v.s. ‚Äúpark‚Äù, ‚ÄúÔ¨Çipped upside‚Äù v.s. ‚Äúride‚Äù,\nare aligned to infer the relationship of hypothesis and premise. These anchored feature examples\nconÔ¨Årm that Anchored Feature Regularizer is able to Ô¨Ånd out useful and stable features to improve\nthe robustness of global representation.\nModel Datasets Features Adversarial Accuracy Benign Accuracy\n(ANLI) (MNLI/SNLI)\nRoBERTa\nStandard\nDatasets\nVanilla 33.2 90.8/92.6\nGlobal Feature 33.8 90.4/93.5\nLocal Feature 33.9 90.6/93.7\nStandard\nand\nAdversarial\nDatasets\nVanilla 53.7 91.0/92.6\nGlobal Feature 55.1 90.8/93.3\nLocal Feature 56.2 90.5/93.3\nTable 6: Robust accuracy on the ANLI dataset. Here we refer ‚ÄúStandard Datasets‚Äù as training on\nthe benign datasets (MNLI + SNLI) only, and ‚ÄúStandard and Adversarial Datasaets‚Äù as training on\nthe both benign and adversarial datasets (ANLI(trianing) + MNLI + SNLI + FeverNLI). ‚ÄúVanilla‚Äù\nrefers to the vanilla RoBERTa trained without Information Bottleneck Regularizer.\n15\nPublished as a conference paper at ICLR 2021\n12\n 11\n 10\n 9\n 8\n 7\n 6\n 5\nlog \n91.5\n92.0\n92.5\n93.0\n93.5\n94.0\n94.5Benign F1\n73\n74\n75\n76\n77\n78\n79\n80\nAdversarial F1 (AddSent)\nBenign/Robust F1 on Benign/Adversarial SQuAD Dataset\nBenign F1\nAdv F1 (AddSent)\nFigure 3: Benign/robust F1 score on benign/adversarial QA datasets. Models are trained on the\nbenign SQuAD dataset with different Œ≤.\nInput (bold = local stable words for local anchored features.)\nPremise: Two woman, both sitting near a pile of poker chips, are playing cards.\nHypothesis: Two woman playing poker.\nPremise: People are Ô¨Çipped upside - down on a bright yellow roller coaster.\nHypothesis: People on on a ride at an amusement park.\nTable 7: Local anchored features extracted by Anchored Feature Regularizer.\nA.2.3 A BLATION STUDY ON DISENTANGLING TWO REGULARIZERS\nTo understand how two regularizers contribute to the improvement of robustness separetely, we\napply two regularizers individually to both the standard training and adversarial training. We refer\nInfoBERT trained with IB regularizer only as ‚ÄúInfoBERT (IBR only)‚Äù and InfoBERT trained with\nAnchored Feature Regularizer only as ‚ÄúInfoBERT (AFR only)‚Äù. ‚ÄúInfoBERT (Both)‚Äù is the standard\nsetting for InfoBERT, where we incorporate both regularizers during training. For ‚ÄúInfoBERT (IBR\nonly)‚Äù, we set Œ± = 0 and perform grid search to Ô¨Ånd the optimal Œ≤ = 5 √ó10‚àí2. Similarly for\n‚ÄúInfoBERT (AFR only)‚Äù, we setŒ≤ = 0 and Ô¨Ånd the optimal parameters as Œ±= 5 √ó10‚àí3,ch = 0.9\nand cl = 0.5.\nThe results are shown in Table 8. We can see that both regularizers improve the robust accuracy\non top of vanilla and FreeLB to a similar margin. Applying one of the regularizer can achieve\nsimilar performance of FreeLB, but the training time of InfoBERT is only1/3Àú1/2 less than FreeLB.\nMoreover, after combining both regularizers, we observe that InfoBERT achieves the best robust\naccuracy.\nA.2.4 E XAMPLES OF ADVERSARIAL DATASETS GENERATED BY TEXT FOOLER\nWe show some adversarial examples generated by TextFooler in Table 9. We can see most adver-\nsarial examples are of high quality and look valid to human while attacking the NLP models, thus\nconÔ¨Årming our adversarial dastasets created by TextFooler is a strong benchmark dataset to evaluate\nmodel robustness. However, as also noted in Jin et al. (2020), we observe that some adversarial\nexamples look invalid to human For example, in the last example of Table 9, TextFooler replaces\n‚Äústand‚Äù with ‚Äúposition‚Äù, losing the critical information that ‚Äúgirls arestanding instead of kneeling‚Äù\nand fooling both human and NLP models. Therefore, we expect that InfoBERT should achieve\nbetter robustness when we eliminate such invalid adversarial examples during evaluation.\n16\nPublished as a conference paper at ICLR 2021\nModel Training Method Adversarial Accuracy Benign Accuracy\n(ANLI) (MNLI/SNLI)\nBERT\nStandard\nTraining\nVanilla 26.5 86.7/91.3\nInfoBERT (IBR only) 27.7 85.9/91.7\nInfoBERT (AFR only) 28.0 86.6/91.9\nInfoBERT (Both) 29.2 85.9/91.6\nAdversarial\nTraining\nFreeLB 27.7 86.7/92.3\nInfoBERT (IBR only) 29.3 87.0/92.3\nInfoBERT (AFR only) 30.3 86.9/92.3\nInfoBERT (Both) 30.9 87.2/92.2\nTable 8: Robust accuracy on the ANLI dataset. Models are trained on the benign datasets (MNLI +\nSNLI). Here we refer ‚ÄúIBR only‚Äù as training with Information Bottleneck Regularizer only. ‚ÄúAFR\nonly‚Äù refers to InfoBERT trained with Anchored Feature Regularizer only. ‚ÄúBoth‚Äù is the standard\nInfoBERT that applies two regularizers together.\nInput (red = ModiÔ¨Åed words, bold = original words.)\nValid Adversarial Examples\nPremise: A young boy is playing in the sandy water.\nOriginal Hypothesis: There is a boy in the water.\nAdversarial Hypothesis: There is a man in the water.\nModel Prediction: Entailment ‚ÜíContradiction\nPremise: A black and brown dog is playing with a brown and white dog .\nOriginal Hypothesis: Two dogs play.\nAdversarial Hypothesis: Two dogs gaming.\nModel Prediction: Entailment ‚ÜíNeutral\nPremise: Adults and children share in the looking at something, and some young ladies stand to the side.\nOriginal Hypothesis: Some children are sleeping.\nAdversarial Hypothesis: Some children are dreaming.\nModel Prediction: Contradiction ‚ÜíNeutral\nPremise: Families with strollers waiting in front of a carousel.\nOriginal Hypothesis: Families have some dogs in front of a carousel.\nAdversarial Hypothesis: Families have some doggie in front of a carousel.\nModel Prediction: Contradiction ‚ÜíEntailment\nInvalid Adversarial Examples\nPremise: Two girls are kneeling on the ground.\nOriginal Hypothesis: Two girls stand around the vending machines.\nAdversarial Hypothesis: Two girls position around the vending machinery.\nModel Prediction: Contradiction ‚ÜíNeutral\nTable 9: Adversarial Examples Generated by TextFooler for BERTLarge on SNLI dataset.\n17\nPublished as a conference paper at ICLR 2021\nA.3 P ROOFS\nA.3.1 P ROOF OF THEOREM 3.1\nWe Ô¨Årst state two lemmas.\nLemma A.1. Given a sequence of random variables X1,X2,...,X n and a deterministic function f,\nthen ‚àÄi,j = 1,2,...,n, we have\nI(Xi; f(Xi)) ‚â•I(Xj; f(Xi)) (12)\nProof. By the deÔ¨Ånition,\nI(Xi; f(Xi)) = H(f(Xi)) ‚àíH(f(Xi) |Xi) (13)\nI(Xj; f(Xi)) = H(f(Xi)) ‚àíH(f(Xi) |Xj) (14)\nSince f is a deterministic function,\nH(f(Xi) |Xi) = 0 (15)\nH(f(Xi) |Xj) ‚â•0 (16)\nTherefore,\nI(Xi; f(Xi)) ‚â•I(Xj; f(Xi)) (17)\nLemma A.2. Let X = [ X1; X2; ...; Xn] be a sequence of random variables, and T =\n[T1; T2; ...; Tn] = [ f(X1); f(X2); ...; f(Xn)] be a sequence of random variables generated by a\ndeterministic function f. Then we have\nI(X; T) ‚â§n\nn‚àë\ni=1\nI(Xi; Ti) (18)\nProof. Since X = [ X1; X2; ...; Xn] and T = [ T1; T2; ...; Tn] are language tokens with its corre-\nsponding local representations, we have\nI(X; T) = I(X; T1,T2,...,T n) =\nn‚àë\ni=1\n[H(Ti |T1,T2,...,T i‚àí1) ‚àíH(Ti |X,T1,T2,...,T i‚àí1)]\n(19)\n‚â§\nn‚àë\ni=1\n[H(Ti) ‚àíH(Ti |X)] =\nn‚àë\ni=1\nI(X; Ti) (20)\n‚â§\nn‚àë\ni=1\nn‚àë\nj=1\nI(Xj; Ti) ‚â§n\nn‚àë\ni=1\nI(Xi; Ti), (21)\nwhere the Ô¨Årst inequality follows because conditioning reduces entropy, and the last inequality is\nbecause I(Xi; Ti) ‚â•I(Xj; Ti) based on Lemma A.1.\nThen we directly plug Lemma A.2 into Theorem 3.1, we have the lower bound of LIB as\nI(Y; T) ‚àíŒ≤I(X; T) ‚â•I(Y; T) ‚àínŒ≤\nn‚àë\ni=1\nI(Xi; Ti). (22)\nA.3.2 P ROOF OF THEOREM 3.2\nWe Ô¨Årst state an easily proven lemma,\n18\nPublished as a conference paper at ICLR 2021\nLemma A.3. For any a,b ‚àà[0,1],\n|alog(a) ‚àíblog(b)|‚â§ œÜ(|a‚àíb|), (23)\nwhere œÜ(¬∑) : R+ ‚ÜíR+ is deÔ¨Åned as\nœÜ(x) =\nÔ£±\nÔ£≤\nÔ£≥\n0 x= 0\nxlog( 1\nx) 0 <x< 1\ne\n1\ne x> 1\ne\n. (24)\nIt is easy to verify that œÜ(x) is a continuous, monotonically increasing, concave and subadditive\nfunction.\nNow, we can proceed with the proof of Theorem 3.2.\nProof. We use the fact that\n|I(Y; T) ‚àíI(Y; T‚Ä≤)|‚â§| H(T |Y) ‚àíH(T‚Ä≤|Y)|+ |H(T) ‚àíH(T‚Ä≤)| (25)\nand bound each of the summands on the right separately.\nWe can bound the Ô¨Årst summand as follows:\n|H(T |Y) ‚àíH(T‚Ä≤|Y)|‚â§\n‚àë\ny\np(y)|H(T |Y = y) ‚àíH(T‚Ä≤|Y = y)| (26)\n=\n‚àë\ny\np(y)|\n‚àë\nt\np(t|y) log(1/p(t|y)) ‚àí\n‚àë\nt\nq(t|y) log(1/q(t|y))| (27)\n‚â§\n‚àë\ny\np(y)\n‚àë\nt\n|p(t|y) logp(t|y) ‚àíq(t|y) logq(t|y)| (28)\n‚â§\n‚àë\ny\np(y)\n‚àë\nt\nœÜ(|p(t|y) ‚àíq(t|y)|) (29)\n=\n‚àë\ny\np(y)\n‚àë\nt\nœÜ(|\n‚àë\nx\np(t|x)[p(x|y) ‚àíq(x|y)]|), (30)\nwhere\np(x|y) = p(y|x)p(x)‚àë\nxp(y|x)p(x) (31)\nq(x|y) = p(y|x)q(x)‚àë\nxp(y|x)q(x). (32)\nSince ‚àë\nx‚ààX‚à™X‚Ä≤ p(x|y) ‚àíq(x|y) = 0 for any y‚ààY, we have that for any scalar a,\n|\n‚àë\nx\np(t|x)[p(x|y) ‚àíq(x|y)])| (33)\n= |\n‚àë\nx\n(p(t|x) ‚àía)(p(x|y) ‚àíq(x|y))| (34)\n‚â§\n‚àö‚àë\nx\n(p(t|x) ‚àía)2\n‚àö‚àë\nx\n(p(x|y) ‚àíq(x|y))2. (35)\nSetting a= 1\n|X‚àíX‚Ä≤|\n‚àë\nx‚ààX‚à™X‚Ä≤ p(t|x) we get\n|H(T |Y) ‚àíH(T‚Ä≤|Y) ‚â§\n‚àë\ny\np(y)\n‚àë\nt\nœÜ\n(‚àö\nV(p(t|x‚ààX‚à™X ‚Ä≤) ¬∑||p(x|y) ‚àíq(x|y)||2\n)\n,\n(36)\n19\nPublished as a conference paper at ICLR 2021\nwhere for any real-value vector a = (a1,...,a n), V(a) is deÔ¨Åned to be proportional to the variance\nof elements of a:\nV(a) =\nn‚àë\ni=1\n(ai ‚àí1\nn\nn‚àë\nj=1\naj)2, (37)\np(t |x ‚àà X‚à™X‚Ä≤) stands for the vector in which entries are p(t |x) with different values of\nx‚ààX‚à™X ‚Ä≤for a Ô¨Åxed t, and p(x|y) and q(x|y) are the vectors in which entries are p(x|y) and\nq(x|y), respectively, with different values ofx‚ààX‚à™X ‚Ä≤for a Ô¨Åxed y.\nSince\n||p(x|y) ‚àíq(x|y)||2 ‚â§||p(x|y) ‚àíq(x|y)||1 ‚â§2, (38)\nit follows that\n|H(T |Y) ‚àíH(T‚Ä≤|Y)|‚â§\n‚àë\ny\np(y)\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX‚à™X ‚Ä≤))\n)\n(39)\nMoreover, we have\n‚àö\nV(p(t|x‚ààX‚à™X ‚Ä≤) ‚â§\n‚àö\nV(p(t|x‚ààX)) + V(p(t|x‚ààX‚Ä≤)) (40)\n‚â§\n‚àö\nV(p(t|x‚ààX)) +\n‚àö\nV(p(t|x‚ààX‚Ä≤)), (41)\nwhere the Ô¨Årst inequality is because sample mean is the minimizer of the sum of the squared dis-\ntances to each sample and the second inequality is due to the subadditivity of the square root func-\ntion. Using the fact that œÜ(¬∑) is monotonically increasing and subadditive, we get\n|H(T |Y) ‚àíH(T‚Ä≤|Y)|‚â§\n‚àë\ny\np(y)\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX))\n)\n+\n‚àë\ny\np(y)\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX‚Ä≤))\n)\n(42)\nNow we explicate the process for establishing the bound for ‚àë\nyp(y) ‚àë\ntœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX))\n)\nand the one for ‚àë\nyp(y) ‚àë\ntœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX‚Ä≤))\n)\ncan be similarly derived.\nBy deÔ¨Ånition of V(¬∑) and using Bayes‚Äô theoremp(t|x) = p(t)p(x|t)\np(x) for x‚ààX, we have that\n‚àö\nV(p(t|x‚ààX)) = p(t)\n‚àö‚àë\nx‚ààX\n(p(x|t)\np(x) ‚àí 1\n|X|\n‚àë\nx‚Ä≤‚ààX\np(x‚Ä≤|t)\np(x‚Ä≤)\n)2 (43)\n20\nPublished as a conference paper at ICLR 2021\nDenoting 1 = (1,..., 1), we have by the triangle inequality that\n‚àö‚àë\nx‚ààX\n(p(x|t)\np(x) ‚àí 1\n|X|\n‚àë\nx‚Ä≤‚ààX\np(x‚Ä≤|t)\np(x‚Ä≤)\n)2 (44)\n‚â§||p(x|t)\np(x) ‚àí1||2 +\n‚àö‚àë\nx‚ààX\n(\n1 ‚àí 1\n|X|\n‚àë\nx‚Ä≤‚ààX\np(x‚Ä≤|t)\np(x‚Ä≤)\n)2 (45)\n= ||p(x|t)\np(x) ‚àí1||2 +\n‚àö\n|X|\n(\n1 ‚àí 1\n|X|\n‚àë\nx‚Ä≤‚ààX\np(x‚Ä≤|t)\np(x‚Ä≤)\n)2 (46)\n= ||p(x|t)\np(x) ‚àí1||2 +\n‚àö\n1\n|X|\n(\n|X|‚àí\n‚àë\nx‚Ä≤‚ààX\np(x‚Ä≤|t)\np(x‚Ä≤)\n)2 (47)\n= ||p(x|t)\np(x) ‚àí1||2 + 1‚àö\n|X|\n|\n‚àë\nx‚Ä≤‚ààX\n(1 ‚àíp(x‚Ä≤|t)\np(x‚Ä≤) )| (48)\n‚â§||p(x|t)\np(x) ‚àí1||2 + 1‚àö\n|X|\n||p(x|t)\np(x) ‚àí1||1 (49)\n‚â§(1 + 1‚àö\n|X|\n)||p(x|t)\np(x) ‚àí1||1 (50)\n‚â§ 2\nminx‚ààXp(x)||p(x|t) ‚àíp(x)||1 (51)\nFrom an inequality linking KL-divergence and the l1 norm, we have that\n||p(x|t) ‚àíp(x)||1 ‚â§\n‚àö\n2 log(2)DKL[p(x|t)||p(x)] (52)\nPlugging Eq. (52) into Eq. (51) and using Eq. (43), we have the following bound:\n‚àö\nV(p(t|x‚ààX)) ‚â§B\n2 p(t)\n‚àö\ndt, (53)\nwhere B =\n4\n‚àö\n2 log(2)\nminx‚ààX p(x) and dt = DKL[p(x|t)||p(x)].\nWe will Ô¨Årst proceed the proof under the assumption thatBp(t)‚àödt ‚â§1\ne for any t. We will later see\nthat this condition can be discarded. If Bp(t)‚àödt ‚â§1\ne,then\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX))\n)\n(54)\n‚â§\n‚àë\nt\nBp(t)\n‚àö\ndt\n(\nlog( 1\nB) + log( 1\np(t)dt\n)\n)\n(55)\n= Blog( 1\nB)\n‚àë\nt\np(t)\n‚àö\ndt + B\n‚àë\nt\np(t)\n‚àö\ndtlog( 1\np(t)dt\n) (56)\n‚â§Blog( 1\nB)||p(t)\n‚àö\ndt||1 + B||\n‚àö\np(t)\n‚àö\ndt||1, (57)\nwhere the last inequality is due to an easily proven fact that for anyx> 0,x log( 1\nx) ‚â§‚àöx. We p(t)\nand d(t) are vectors comprising p(t) and dt with different values of t, respectively.\nUsing the following two inequalities:\n||p(t)\n‚àö\ndt||1 ‚â§\n‚àö\n|T|||p(t)\n‚àö\ndt||2 ‚â§\n‚àö\n|T|||\n‚àö\np(t)dt||2 (58)\nand\n||\n‚àö\np(t)\n‚àö\ndt||1 ‚â§\n‚àö\n|T|||\n‚àö\np(t)\n‚àö\ndt||2 (59)\n=\n‚àö\nT\n‚àö\n||p(t)\n‚àö\ndt||1 ‚â§|T| 3/4\n‚àö\n||\n‚àö\np(t)dt||2 (60)\n21\nPublished as a conference paper at ICLR 2021\nwe have\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX))\n)\n‚â§Blog( 1\nB)\n‚àö\n|T|||\n‚àö\np(t)dt||2 + B|T|3/4\n‚àö\n||\n‚àö\np(t)dt||2. (61)\nUsing the equality\n||\n‚àö\np(t)dt||2 =\n‚àö\nE[DKL[p(x|t)||p(x)]] =\n‚àö\nI(X; T), (62)\nwe reach the following bound\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX))\n)\n(63)\n‚â§Blog( 1\nB)|T|1/2I(X; T)1/2 + B|T|3/4I(X; T)1/4. (64)\nPlug Lemma A.2 into the equation above, we have\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX))\n)\n(65)\n‚â§Blog( 1\nB)|T|1/2(n\nn‚àë\ni=1\nI(Xi; Ti))1/2 + B|T|3/4(n\nn‚àë\ni=1\nI(Xi; Ti))1/4 (66)\n‚â§‚àönBlog( 1\nB)|T|1/2\nn‚àë\ni=1\nI(Xi; Ti)1/2 + n1/4B|T|3/4\nn‚àë\ni=1\nI(Xi; Ti)1/4 (67)\nWe now show the bound is trivial if the assumption that Bp(t)‚àödt ‚â§ 1\ne does not hold. If the\nassumption does not hold, then there exists a tsuch that Bp(t)‚àödt > 1\ne. Since\n‚àö\nI(X; T) =\n‚àö‚àë\nt\np(t)dt ‚â•\n‚àë\nt\np(t)\n‚àö\ndt ‚â•p(t)\n‚àö\ndt (68)\nfor any t, we get that\n‚àö\nI(X; T) ‚â• 1\neB.Since |T|‚â• 1 and C ‚â•0, we get that our bound in Eq. (63)\nis at least\nBlog( 1\nB)|T|1/2I(X; T)1/2 + B|T|3/4I(X; T)1/4 (69)\n‚â•\n‚àö\n|T|(log(1/B)\ne + B1/2|T|1/4\ne1/2 ) (70)\nLet f(c) = log(1/c)\ne + c1/2|T|1/4\ne1/2 . It can be verifed that f‚Ä≤(c) >0 if c >0. Since B >4\n‚àö\n2 log(2)\nby the deÔ¨Ånition of B, we have f(B) >f (4\n‚àö\n2 log(2)) >0.746.Therefore, we have\nBlog( 1\nB)|T|1/2I(X; T)1/2 + B|T|3/4I(X; T)1/4 (71)\n‚â•0.746\n‚àö\n|T|‚â• log(|T|) (72)\nTherefore, if indeed Bp(t)‚àödt > 1\ne for some t, then the bound in Eq. (63) is trivially\ntrue, since H(T | Y) is within [0,log(|T|)]. Similarly, we can establish a bound for‚àë\ntœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX‚Ä≤))\n)\nas follows:\n‚àë\nt\nœÜ\n(\n2\n‚àö\nV(p(t|x‚ààX‚Ä≤))\n)\n‚â§‚àönB‚Ä≤log( 1\nB‚Ä≤)|T|1/2\nn‚àë\ni=1\nI(X‚Ä≤\ni; T‚Ä≤\ni)1/2 + n1/4B‚Ä≤|T|3/4\nn‚àë\ni=1\nI(X‚Ä≤\ni; T‚Ä≤\ni)1/4,\n(73)\nwhere B‚Ä≤=\n4\n‚àö\n2 log(2)\nminx‚ààX‚Ä≤ q(x) .\n22\nPublished as a conference paper at ICLR 2021\nPlugging Eq. (73) and Eq. (65) into Eq. (42), we get\n|H(T |Y) ‚àíH(T‚Ä≤|Y)|‚â§‚àönBlog( 1\nB)|T|1/2\nn‚àë\ni=1\nI(Xi; Ti)1/2 + n1/4B|T|3/4\nn‚àë\ni=1\nI(Xi; Ti)1/4+\n‚àönB‚Ä≤log( 1\nB‚Ä≤)|T|1/2\nn‚àë\ni=1\nI(X‚Ä≤\ni; T‚Ä≤\ni)1/2 + n1/4B‚Ä≤|T|3/4\nn‚àë\ni=1\nI(X‚Ä≤\ni; T‚Ä≤\ni)1/4\n(74)\nNow we turn to the third summand in Eq. (25), we have to bound |H(T) ‚àíH(T‚Ä≤)|.\nRecall the deÔ¨Ånition of œµ-bounded adversarial example. We denote the set of the benign data repre-\nsentation tthat are within the œµ-ball of t‚Ä≤by Q(t‚Ä≤). Then for any t‚ààQ(t‚Ä≤), we have\n||t‚Ä≤\ni ‚àíti||‚â§ œµ, (75)\nfor i = 1,2,...,n . We also denote the number of the œµ-bounded adversarial examples around the\nbenign representation t by c(t). Then we have the distribution of adversarial representation t‚Ä≤as\nfollows:\nq(t‚Ä≤) =\n‚àë\nt‚ààQ(t‚Ä≤)\np(t)\nc(t) (76)\n|H(T) ‚àíH(T‚Ä≤)| (77)\n= |\n‚àë\nt\np(t) logp(t) ‚àí\n‚àë\nt‚Ä≤\nq(t‚Ä≤) logq(t‚Ä≤)| (78)\n= |\n‚àë\nt\np(t) logp(t) ‚àí\n‚àë\nt‚Ä≤\n[\n(\n‚àë\nt‚ààQ(t‚Ä≤)\np(t)\nc(t) ) log(\n‚àë\nt‚ààQ(t‚Ä≤)\np(t)\nc(t) )\n]\n| (79)\n‚â§|\n‚àë\nt\np(t) logp(t) ‚àí\n‚àë\nt‚Ä≤\n‚àë\nt‚ààQ(t‚Ä≤)\np(t)\nc(t) log(p(t)\nc(t) )| (80)\n= |\n‚àë\nt\np(t) logp(t) ‚àí\n‚àë\nt\nc(t)p(t)\nc(t) log(p(t)\nc(t) )| (81)\n= |\n‚àë\nt\np(t) logc(t)|, (82)\nwhere the inequality is by log sum inequality. If we denote the C = maxtc(t) which is the maxi-\nmum number of œµ-bounded textual adversarial examples given a benign representation tof a word\nsequence x, we have\n|H(T) ‚àíH(T‚Ä≤)| (83)\n‚â§|\n‚àë\nt\np(t) logc(t)| (84)\n‚â§|\n‚àë\nt\np(t) logC|= log C. (85)\nNote that given a word sequence x of n with representation t, the number of œµ-bounded textual\nadversarial examples c(t) is Ô¨Ånite given a Ô¨Ånite vocabulary size. Therefore, if each word has at most\nkcandidate word perturbations, then log C ‚â§nlog kcan be viewed as some constants depending\nonly on nand œµ.\nNow, combining Eq. (25), Eq. (74) and Eq. (85), we prove the bound in Theorem 3.2.\n23",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8238680362701416
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.703372597694397
    },
    {
      "name": "Mutual information",
      "score": 0.676159143447876
    },
    {
      "name": "Information bottleneck method",
      "score": 0.6392703652381897
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6346100568771362
    },
    {
      "name": "Inference",
      "score": 0.6231191158294678
    },
    {
      "name": "Adversarial system",
      "score": 0.609773576259613
    },
    {
      "name": "Language model",
      "score": 0.5941414833068848
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.5323923826217651
    },
    {
      "name": "Bottleneck",
      "score": 0.4886389374732971
    },
    {
      "name": "Representation (politics)",
      "score": 0.4844343960285187
    },
    {
      "name": "Natural language understanding",
      "score": 0.48324593901634216
    },
    {
      "name": "Machine learning",
      "score": 0.47269466519355774
    },
    {
      "name": "Feature learning",
      "score": 0.467191219329834
    },
    {
      "name": "Natural language",
      "score": 0.39813172817230225
    },
    {
      "name": "Natural language processing",
      "score": 0.3261159062385559
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    }
  ],
  "institutions": []
}