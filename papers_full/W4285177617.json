{
  "title": "Revisiting Uncertainty-based Query Strategies for Active Learning with Transformers",
  "url": "https://openalex.org/W4285177617",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2169027054",
      "name": "Christopher Schröder",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2431624491",
      "name": "Andreas Niekler",
      "affiliations": [
        "Leipzig University"
      ]
    },
    {
      "id": "https://openalex.org/A1678657404",
      "name": "Martin Potthast",
      "affiliations": [
        "Leipzig University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2951911250",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W2128678390",
    "https://openalex.org/W3105522431",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2798803565",
    "https://openalex.org/W1484084878",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W4250800088",
    "https://openalex.org/W2117763124",
    "https://openalex.org/W3023176787",
    "https://openalex.org/W2970043232",
    "https://openalex.org/W2970450573",
    "https://openalex.org/W2128073546",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4206648492",
    "https://openalex.org/W2963238274",
    "https://openalex.org/W2963742748",
    "https://openalex.org/W1573810412",
    "https://openalex.org/W1580375566",
    "https://openalex.org/W4294027320",
    "https://openalex.org/W3101345273",
    "https://openalex.org/W2098742124",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W2774918944",
    "https://openalex.org/W2952164680",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2114524997",
    "https://openalex.org/W2160660844",
    "https://openalex.org/W4206734067",
    "https://openalex.org/W1514940655",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963026768"
  ],
  "abstract": "Active learning is the iterative construction of a classification model through targeted labeling, enabling significant labeling cost savings. As most research on active learning has been carried out before transformer-based language models (“transformers”) became popular, despite its practical importance, comparably few papers have investigated how transformers can be combined with active learning to date. This can be attributed to the fact that using state-of-the-art query strategies for transformers induces a prohibitive runtime overhead, which effectively nullifies, or even outweighs the desired cost savings. For this reason, we revisit uncertainty-based query strategies, which had been largely outperformed before, but are particularly suited in the context of fine-tuning transformers. In an extensive evaluation, we connect transformers to experiments from previous research, assessing their performance on five widely used text classification benchmarks. For active learning with transformers, several other uncertainty-based approaches outperform the well-known prediction entropy query strategy, thereby challenging its status as most popular uncertainty baseline in active learning for text classification.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2022, pages 2194 - 2203\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nRevisiting Uncertainty-based Query Strategies for\nActive Learning with Transformers\nChristopher Schröder, Andreas Niekler, and Martin Potthast\nLeipzig University\nAbstract\nActive learning is the iterative construction\nof a classiﬁcation model through targeted la-\nbeling, enabling signiﬁcant labeling cost sav-\nings. As most research on active learning has\nbeen carried out before transformer-based lan-\nguage models (“transformers”) became popu-\nlar, despite its practical importance, compara-\nbly few papers have investigated how trans-\nformers can be combined with active learning\nto date. This can be attributed to the fact that\nusing state-of-the-art query strategies for trans-\nformers induces a prohibitive runtime over-\nhead, which effectively nulliﬁes, or even out-\nweighs the desired cost savings. For this rea-\nson, we revisit uncertainty-based query strate-\ngies, which had been largely outperformed be-\nfore, but are particularly suited in the context\nof ﬁne-tuning transformers. In an extensive\nevaluation, we connect transformers to exper-\niments from previous research, assessing their\nperformance on ﬁve widely used text classiﬁ-\ncation benchmarks. For active learning with\ntransformers, several other uncertainty-based\napproaches outperform the well-known predic-\ntion entropy query strategy, thereby challeng-\ning its status as most popular uncertainty base-\nline in active learning for text classiﬁcation.\n1 Introduction\nCollecting labeled data for machine learning can\nbe costly and time-consuming. A key technique to\nminimize labeling costs has been active learning,\nwhere an oracle (e.g., a human expert) is queried to\nlabel problem instances selected that are deemed\nto be most informative to the learning algorithm’s\nnext iteration according to a query strategy.\nActive learning is characterized by the real-\nworld machine learning scenario in which large\namounts of training data are unavailable, which\nmay explain why comparably little research has\ninvestigated deep learning in this context. The re-\ncent widely successful transformer-based language\nmodels can circumvent the limitations imposed by\nsmall training datasets (Vaswani et al., 2017; De-\nvlin et al., 2019). Pre-trained on large amounts of\nunlabeled text, they can be ﬁne-tuned to a given\ntask using far less training data than when trained\nfrom scratch. However, their high number of model\nparameters renders them computationally highly\nexpensive, for query strategies that are targeted at\nneural networks or text classiﬁcation (Settles et al.,\n2007; Zhang et al., 2017), resulting in prohibitive\nturnaround times between labeling steps.\nIn this paper, we systematically investigate\nuncertainty-based query strategies as a computa-\ntionally inexpensive alternative. Despite their rela-\ntive disadvantages in traditional active learning,\nwhen paired with transformers, they are highly\neffective as well as efﬁcient. Our extensive ex-\nperiments assess a multitude of combinations in-\ncluding state-of-the-art transformer models BERT\n(Devlin et al., 2019) and DistilRoBERTa (Sanh\net al., 2019), ﬁve well-known sentence classiﬁca-\ntion benchmarks, and ﬁve query strategies.1\n2 Related Work\nUncertainty-based query strategies used to be the\nmost common choice in active learning, using\nuncertainty scores obtained from the learning al-\ngorithm (Lewis and Gale, 1994), estimates ob-\ntained via ensembles (Krogh and Vedelsby, 1994;\nRayChaudhuri and Hamey, 1995), or prediction\nentropy (Perona et al., 2008). More recently—\npredating transformers—neural network-based ac-\ntive learning predominantly employed query strate-\ngies that select problem instances according to\n(1) the magnitude of their backpropagation-induced\ngradients (Settles et al., 2007; Zhang et al., 2017),\nwhere instances causing a high-magnitude gradient\ninform the model better, and (2) representativity-\nbased criteria (e.g., coresets (Sener and Savarese,\n2018)), which select instances from a vector space\nto geometrically represent the full dataset.\n1Code: https://github.com/webis-de/ACL-22\n2194\nFor today’s deep neural networks, ensembles are\ntoo computationally expensive, and prediction en-\ntropy has been observed to be overconﬁdent (Guo\net al., 2017; Lakshminarayanan et al., 2017). The\nexception are ﬂat architectures, where, among oth-\ners, Prabhu et al. (2019) showed fastText (Joulin\net al., 2017) to be effective, well-calibrated, and\ncomputationally efﬁcient. Prior to transformers,\nquery strategies relying on expected gradient length\n(Settles et al., 2007) achieved the best results on\nmany active learning benchmarks for text classiﬁ-\ncation (Zhang et al., 2017). Gradients depend on\nthe current model, which means, when used for a\nquery strategy, they scale with the vast number of a\ntransformer’s parameters, and moreover, they need\nto be computed per-instance instead of batch-wise,\nthereby becoming computationally expensive.\nThe cost of ensembles, the adverse scaling of net-\nwork parameters in gradient-based strategies, and a\nhistory of deeming neural networks to be overcon-\nﬁdent effectively rule out the most predominantly\nused query strategies. This might explain why\ntransformers, despite the success of ﬁne-tuning\nthem for text classiﬁcation (Howard and Ruder,\n2018; Yang et al., 2019; Sun et al., 2019), have only\nvery recently been considered at all in combination\nwith active learning (Lu and MacNamee, 2020;\nYuan et al., 2020; Ein-Dor et al., 2020; Margatina\net al., 2021). All of the related works mitigate the\ncomputationally complex query strategies by sub-\nsampling the unlabeled data before querying (Lu\nand MacNamee, 2020; Ein-Dor et al., 2020; Mar-\ngatina et al., 2021), by performing fewer queries\nwith larger sample sizes (Yuan et al., 2020; Mar-\ngatina et al., 2021), or by tailoring to less expen-\nsive settings, namely binary classiﬁcation (Ein-Dor\net al., 2020). Subsampling, however, introduces\nadditional randomness which can aggravate com-\nparability across experiments, and large sample\nsizes increase the amount of labeled data, which is\ncontrary to minimizing the labeling effort.\nDue to this computationally challenging setting,\nthe uncertainty-based prediction entropy query\nstrategy (Roy and McCallum, 2001; Schohn and\nCohn, 2000) is therefore a frequently used baseline\nand a lowest common denominator in recent work\non active learning for text classiﬁcation (Zhang\net al., 2017; Lowell et al., 2019; Prabhu et al., 2019;\nEin-Dor et al., 2020; Lu and MacNamee, 2020;\nYuan et al., 2020; Margatina et al., 2021; Zhang and\nPlank, 2021). Apart from being employed as base-\nlines, uncertainty-based query strategies have not\nbeen systematically analyzed in conjunction with\ntransformers, and moreover, comparisons to the\nprevious benchmarks by Zhang et al. (2017) have\nbeen omitted by the aforementioned related work.\nOur work not only closes this gap, but also reevalu-\nates the relative strength of uncertainty-based ap-\nproaches, including two recently largely neglected\nstrategies, thereby challenging the status of predic-\ntion entropy as the most popular baseline.\n3 Transformer-based Active Learning\nThe goal of active learning is to minimize the label-\ning costs of training data acquisition while maxi-\nmizing a model’s performance (increase) with each\nnewly labeled problem instance. In contrast to reg-\nular supervised text classiﬁcation (“passive learn-\ning”), it operates iteratively, where in each iteration\n(1) a so-called query strategy selects new instances\nfor labeling according to an estimation of their in-\nformativeness, (2) an oracle (e.g., a human expert)\nprovides the respective label, and (3) a learning\nalgorithm either uses the newly labeled instance for\nits next learning step, or a model is retrained from\nscratch using all previously labeled instances. This\nwork considers pool-based active learning (Lewis\nand Gale, 1994), where the query strategies have\naccess to all unlabeled data. Notation-wise, we\ndenote instances by x1,x2,...,x n, the number of\nclasses by c, the respective label for instance xi by\nyi (where ∀i: yi ∈{1,...,c }), and P(yi|xi) is a\nprobability-like predicted class distribution.\nQuery Strategies We consider three well-known\nuncertainty-based query strategies, one recent state-\nof-the-art strategy that coincidentally also includes\nuncertainty, and a random baseline:\n(1) Prediction Entropy (PE; Roy and McCallum,\n2001; Schohn and Cohn, 2000) selects instances\nwith the highest entropy in the predicted label dis-\ntribution with the aim to reduce overall entropy:\narg max\nxi\n\n−\nc∑\nj=1\nP(yi = j|xi) logP(yi = j|xi)\n\n\n(2) Breaking Ties (BT; Scheffer et al., 2001; Luo\net al., 2005) takes instances with the minimum mar-\ngin between the top two most likely probabilities:\narg min\nxi\n[\nP(yi = k∗\n1|xi) −P(yi = k∗\n2|xi)\n]\nwhere k∗\n1 is the most likely label in the posterior\nclass distribution P(yi|xi), and k∗\n2 the second most\n2195\nDataset Name (ID) Type Classes Training Test\nAG’s News(AGN) N 4 120,000 (*) 7,600\nCustomer Reviews (CR) S 2 3,397 378\nMovie Reviews (MR) S 2 9,596 1,066\nSubjectivity (SUBJ) S 2 9,000 1,000\nTREC-6 (TREC-6) Q 6 5,500 (*) 500\nTable 1: Key information about the examined datasets.\nThe dataset type was abbreviated as follows: N: News,\nS: Sentiment, Q: Questions. (*): Predeﬁned test sets\nwere available and adopted.\nlikely label respectively. In the binary case , this\nmargin is small iff the label entropy is high, which\nis why BT and PE then select the same instances.\n(3) Least Conﬁdence (LC; Culotta and McCallum,\n2005) selects instances whose most likely label has\nthe least conﬁdence according to the current model:\narg max\nxi\n[\n1 −P(yi = k∗\n1|xi)\n]\n(4) Contrastive Active Learning (CA; Margatina\net al., 2021) selects instances with the maximum\nmean Kullback-Leibler (KL) divergence between\nthe predicted class distributions (“probabilities”) of\nan instance and each of its mnearest neighbors:\narg max\nxi\n\n 1\nm\nm∑\nj=1\nKL(P(yj|xknn\nj ) ∥P(yi|xi))\n\n\nwhere the instances xknn\nj are the mnearest neigh-\nbors of instance xi.\n(5) Random Sampling (RS), a commonly used base-\nline, draws uniformly from the unlabeled pool.\nOracle The oracle is usually operationalized us-\ning the training datasets of existing benchmarks:\nTo ensure comparability with the literature, we pick\nimportant standard text classiﬁcation tasks.\nClassiﬁcation We ﬁne-tune BERT (Devlin et al.,\n2019) and DistilRoBERTa (Sanh et al., 2019) on\nseveral natural language understanding datasets.\nBERT is well-researched as transformer and has\nrecently also shown strong results in active learning\n(Yuan et al., 2020; Ein-Dor et al., 2020; Margatina\net al., 2021). The model consists of 24 layers, hid-\nden units of size 1024 and 336M parameters in total.\nDistilRoBERTa, by contrast, is a more parameter-\nefﬁcient alternative which has merely six layers,\nhidden units of size 768, and 82M parameters. We\nalso trained a passive model on the full data.\nThe classiﬁcation model consists of the respec-\ntive transformer, on top of which we add a fully\nModel Strategy Mean Rank Mean Result\nAcc. AUC Acc. AUC\nSVM PE 1.80 2.60 0.764 0.663\nBT 1.60 1.60 0.767 0.697\nLC 3.00 2.60 0.751 0.672\nCA 5.00 5.00 0.667 0.593\nRS 3.00 2.60 0.757 0.686\nKimCNN PE 1.60 2.40 0.818 0.742\nBT 1.60 2.00 0.818 0.750\nLC 3.80 2.80 0.810 0.732\nCA 3.80 4.80 0.793 0.711\nRS 3.60 2.40 0.804 0.749\nD.RoBERTa PE 2.60 3.00 0.901 0.856\nBT 2.20 1.80 0.902 0.864\nLC 1.40 2.00 0.904 0.860\nCA 3.00 3.40 0.901 0.852\nRS 5.00 4.20 0.884 0.853\nBERT PE 2.40 2.40 0.909 0.859\nBT 2.00 1.60 0.914 0.873\nLC 2.20 3.80 0.917 0.866\nCA 2.80 2.60 0.916 0.872\nRS 5.00 4.00 0.899 0.861\nTable 2: The “Mean Rank” columns show the mean\nrank when ordered by mean accuracy (Acc.) after the\nﬁnal iteration and by overall AUC. The “Mean Result”\ncolumns show the mean accuracy and AUC.\nconnected projection layer, and a ﬁnal softmax out-\nput layer. We use the “[CLS]” token that is com-\nputed by the transformer as sentence representa-\ntion. Regarding ﬁne-tuning, we adopt the com-\nbination of discriminative ﬁne-tuning and slanted\ntriangular learning rates (Howard and Ruder, 2018).\nThe main active learning routine is then as follows:\n(1) The query strategy, either using the model from\nthe previous iteration, or sampling randomly, se-\nlects 25 instances from the unlabeled pool. (2) The\noracle provides labels for these instances. (3) The\nnext model is trained using all data labeled so far.\nBaselines For comparison, we consider a linear\nSVM, and KimCNN (Kim, 2014), which have been\nused extensively in text classiﬁcation, disregarding\nactive learning. We adopted the KimCNN parame-\nters from Kim (2014) and Zhang et al. (2017).\n4 Evaluation\nWe evaluate ﬁve query strategies in combination\nwith BERT, DistilRoBERTa and two baselines.\nDatasets and Experimental Setup In Table 1,\nwe show the ﬁve datasets employed, which have\npreviously been used to evaluate active learning:\nAG’s News (AGN; Zhang et al., 2015), Customer\nReviews (CR; Hu and Liu, 2004), Movie Reviews\n2196\nAccuracy\nLC\nBT\nPE\nDis�l-\nRoBERTa \nBERT\nRS\npassive\nCA\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAGN\nCR\nMR\nSUBJ\nTREC-6\n0.86\n0.88\n0.90\n0.88\n0.90\n0.92\n25\n275\n525\n25\n275\n525\n0.92\n0.94\n0.96\n25\n275\n525\n0.92\n0.94\n0.96\nNumber of Instances\nFigure 1: Active learning curves of BERT and DistilRoBERTa when combined with ﬁve query strategies: Predic-\ntion Entropy (PE), Breaking Ties (BT), Least Conﬁdence (LC), Contrastive Active Learning (CA), and Random\nSampling (RS). The tubes around the lines represent standard deviation over ﬁve runs. For comparison, the hori-\nzontal line depicts a passive text classiﬁcation for which BERT has been trained using the entire training set.\n(MR; Pang and Lee, 2005), Subjectivity (SUBJ;\nPang and Lee, 2004), and TREC-6 (Li and Roth,\n2002). These datasets encompass binary and multi-\nclass classiﬁcation in different domains, and they\nare class-balanced, except for TREC-6. Where\navailable, we employed the pre-existing test sets,\nor otherwise a random sample of 10%.\nWe follow the experiment setup of Zhang et al.\n(2017): 25 training instances are used to train the\nﬁrst model, followed by 20 active learning itera-\ntions, during each of which 25 instances are queried\nand labeled. Using 10% of the so far labeled data\nas validation set, we stop early (Duong et al., 2018)\nwhen accuracy surpasses 98%, or the validation\nloss does not increase for ﬁve epochs.\nResults For each combination of dataset, model,\nand query strategy, Figure 1 shows the respec-\ntive learning curves. The horizontal line shows\nthe best model’s score when trained on the full\ndataset, which four out of ﬁve datasets approach\nvery closely, or even exceed. As expected, BERT\ngenerally achieves steeper learning curves than Dis-\ntilRoBERTa, but surprisingly, during later itera-\ntions DistilRoBERTa reaches scores only slightly\nworse than BERT for all datasets except MR. Re-\ngarding query strategies, RS is a strong contender\nduring early iterations, e.g., as can be seen for the\nDataset Model Strategy Acc. Data Use\nAGN\nBERT BT 0.904 0.4%\nBERT passive (ours) 0.946 100.00%\nXLNet1 passive 0.955 100.00%\nCR\nBERT LC 0.919 15.45%\nBERT passive (ours) 0.925 100.00%\nHAC2 passive 0.889 100.00%\nMR\nBERT PE, BT 0.857 0.547%\nBERT passive (ours) 0.893 100.00%\nSimCSE3 passive 0.884 100.00%\nSUBJ\nBERT LC 0.958 5.83%\nBERT passive (ours) 0.969 100.00%\nAdaSent4 passive 0.955 100.00%\nTREC-6\nBERT CA 0.968 9.55%\nBERT passive (ours) 0.958 100.00%\nRCNN5 passive 0.962 100.00%\nTable 3: Best ﬁnal accuracy compared to (our) pas-\nsive classiﬁcation and state-of-the-art text classiﬁca-\ntion: 1Yang et al. (2019), 2Zheng et al. (2019), 3Gao\net al. (2021), 4Zhao et al. (2015), 5Tay et al. (2018).\n“Data Use” indicates proportion of training data used.\nﬁrst few iterations of CR. This is partly because all\nbut one of the datasets are balanced, but neverthe-\nless, RS is eventually outperformed by the other\nstrategies in most cases. For imbalanced datasets,\nEin-Dor et al. (2020) have shown RS to be less ef-\nfective, which we can conﬁrm for TREC-6. While\nin terms of area under the learning curve (AUC)\n2197\nthere seems to be no overall best strategy, PE/BT\nand CA often show very steep learning curves.\nIn Table 2, we rank the query strategies by their\naverage accuracy and AUC results, ranging from\n1 (best) to 5 (worst). We also report their average\naccuracy and AUC per model and query strategy.\nSurprisingly, we can see that PE, a commonly used\nand proven to be strong baseline, which has been\na lowest common denominator in recent work on\nactive learning for text classiﬁcation (Zhang et al.,\n2017; Lowell et al., 2019; Prabhu et al., 2019; Ein-\nDor et al., 2020; Lu and MacNamee, 2020; Yuan\net al., 2020; Margatina et al., 2021; Zhang and\nPlank, 2021), is on average outranked by BT when\nusing transformers. BT achieves the best AUC\nranks and scores, and in many cases also the best\naccuracy ranks and scores. It seems to be simi-\nlarly effective on the baselines as well. Moreover,\nLC also outperforms PE for DistilRoBERTa where\nit even competes with BT. Detailed accuracy and\nAUC scores including standard deviations are re-\nported in Appendix Tables 5 & 7.\nTable 3 compares the best model trained via ac-\ntive learning per dataset against passive text clas-\nsiﬁcation, namely (1) our own model trained on\nthe full training set, and (2) state-of-the-art results.\nThe largest discrepancy between active learning\nand passive text classiﬁcation is observed on AGN,\nwhich is also the largest dataset from which the\nactive learning models use less than 1% for train-\ning. Otherwise, all models are close to or even sur-\npass the state of the art, using only between 0.4%\nand 14% of the data. Noteworthy, LC achieves\nthe best accuracy result for two datasets, while the\nstrong baseline PE and the state-of-the-art approach\nCA perform best on only one dataset each.\nIn Table 4, we report the best AUC scores per\ndataset, and compare them to previous work. BT\nranks highest in two out of three cases with CA\nachieving the best result on the remaining two\ndatasets. BERT achieves the best AUC scores on\nall datasets with a considerable increase in AUC\ncompared to Zhang et al. (2017).\nIn summary, we use recent transformer mod-\nels in combination with several query strategies\nto evaluate a previously established but lately ne-\nglected benchmark. We ﬁnd that the PE baseline\nis outperformed by BT, which, as a reminder, se-\nlects the same instances as PE for binary classiﬁ-\ncation, but shows superior results on multi-class\ndatasets. We conclude that BT, which even out-\nDataset Model AUC\nAGN BERT (BT, ours) 0.875\n–\nCR BERT (PE, BT; ours) 0.877\nCNN6 0.743\nMR BERT (PE, BT; ours) 0.833\nCNN6 0.707\nSUBJ BERT (CA, ours) 0.943\nCNN6 0.856\nTREC-6 BERT (CA, ours) 0.868\n–\nTable 4: Best area under curve (AUC) scores (averaged\nover ﬁve runs) compared to Zhang et al. (2017).\nperforms the state-of-the-art strategy CA in many\ncases, is therefore a strong contender to become\nthe new default uncertainty-based baseline. Finally,\nDistilRoBERTa, using less than 25% of BERT’s\nparameters, achieves results that are remarkably\nclose to BERT at only a fraction of the overhead.\nConsidering the computational burdens that moti-\nvated this work, this increase in efﬁciency is often\npreferable from a practitioner’s perspective.\n5 Conclusions\nAn investigation of the effectiveness of uncertainty-\nbased query strategies in combination with BERT\nand DistilRoBERTa for active learning on sev-\neral sentence classiﬁcation datasets shows that\nuncertainty-based strategies still perform well. We\nevaluate ﬁve query strategies on an established\nbenchmark, for which we achieve results close to\nstate-of-the-art text classiﬁcation on four out of ﬁve\ndatasets, using only a small fraction of the training\ndata. Contrary to current literature, prediction en-\ntropy, the supposedly strongest uncertainty-based\nbaseline, is outperformed by several uncertainty-\nbased strategies on this benchmark—in particularly\nby the breaking ties strategy. This invalidates the\ncommon practice of solely relying on prediction en-\ntropy as baseline, and shows that uncertainty-based\nstrategies demand renewed attention especially in\nthe context of transformer-based active learning.\nAcknowledgments\nWe thank the anonymous reviewers for their valu-\nable and constructive feedback. This research was\npartially funded by the Development Bank of Sax-\nony (SAB) under project number 100335729.\n2198\nEthical Considerations\nResearch on active learning improves the labeling\nof data, by efﬁciently supporting the learning al-\ngorithm with targeted information, so that overall\nless data has to be labeled. This could contribute\nto creating machine learning models, which would\notherwise be infeasible, either due to limited bud-\nget, or time. Active learning can be used for good\nor bad, and our contributions would—in both cases–\nshow how to make this process more efﬁcient.\nMoreover, we use pre-trained models, which can\ncontain one or more types of bias. Bias, however,\naffects all approaches based on ﬁne-tuning pre-\ntrained language models, but therefore this has to\nbe kept in mind and mitigated all the more.\nReferences\nAron Culotta and Andrew McCallum. 2005. Reduc-\ning labeling effort for structured prediction tasks. In\nProceedings of the 20th National Conference on Ar-\ntiﬁcial Intelligence, pages 746–751.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics (NAACL) , pages 4171–\n4186.\nLong Duong, Hadi Afshar, Dominique Estival, Glen\nPink, Philip Cohen, and Mark Johnson. 2018. Ac-\ntive learning for deep semantic parsing. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (ACL), pages 43–48.\nLiat Ein-Dor, Alon Halfon, Ariel Gera, Eyal Shnarch,\nLena Dankin, Leshem Choshen, Marina Danilevsky,\nRanit Aharonov, Yoav Katz, and Noam Slonim.\n2020. Active Learning for BERT: An Empirical\nStudy. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7949–7962.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 6894–6910.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In Proceedings of the 34th International\nConference on Machine Learning (ICML) , pages\n1321–1330.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL) , pages\n328–339.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the Tenth\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, pages 168–177.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efﬁcient text\nclassiﬁcation. In Proceedings of the 15th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics (EACL), pages 427–431.\nYoon Kim. 2014. Convolutional neural networks for\nsentence classiﬁcation. In Proceedings of the 2014\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1746–1751.\nAnders Krogh and Jesper Vedelsby. 1994. Neural net-\nwork ensembles, cross validation and active learning.\nIn Proceedings of the 7th International Conference\non Neural Information Processing Systems (NIPS) ,\npages 231–238.\nBalaji Lakshminarayanan, Alexander Pritzel, and\nCharles Blundell. 2017. Simple and scalable pre-\ndictive uncertainty estimation using deep ensembles.\nIn Proceedings of the 31st International Conference\non Neural Information Processing Systems (NIPS) ,\npages 6405–6416.\nDavid D. Lewis and William A. Gale. 1994. A se-\nquential algorithm for training text classiﬁers. In\nProceedings of the 17th Annual International ACM-\nSIGIR Conference on Research and Development in\nInformation Retrieval, pages 3–12.\nXin Li and Dan Roth. 2002. Learning question classi-\nﬁers. In Proceedings of the 19th International Con-\nference on Computational Linguistics (COLING) ,\npages 1–7.\nDavid Lowell, Zachary C. Lipton, and Byron C. Wal-\nlace. 2019. Practical obstacles to deploying active\nlearning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n21–30.\nJinghui Lu and Brian MacNamee. 2020. Investigat-\ning the effectiveness of representations based on pre-\ntrained transformer-based language models in active\nlearning for labelling text datasets. arXiv preprint\narXiv:2004.13138.\nTong Luo, Kurt Kramer, Dmitry B. Goldgof,\nLawrence O. Hall, Scott Samson, Andrew Remsen,\nand Thomas Hopkins. 2005. Active learning to rec-\nognize multiple types of plankton. Journal of Ma-\nchine Learning Research (JMLR), 6:589–613.\n2199\nKaterina Margatina, Giorgos Vernikos, Loïc Barrault,\nand Nikolaos Aletras. 2021. Active learning by ac-\nquiring contrastive examples. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 650–663.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings of\nthe 42nd Annual Meeting of the Association for Com-\nputational Linguistics (ACL), pages 271–278.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In Proceedings of the\n43rd Annual Meeting of the Association for Compu-\ntational Linguistics (ACL), pages 115–124.\nP. Perona, A. Holub, and M. C. Burl. 2008. Entropy-\nbased active learning for object recognition. In Pro-\nceedings of the 2008 IEEE Computer Society Con-\nference on Computer Vision and Pattern Recognition\nWorkshops (CVPR Workshops), pages 1–8.\nAmeya Prabhu, Charles Dognin, and Maneesh Singh.\n2019. Sampling bias in deep active classiﬁcation:\nAn empirical study. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 4058–4068.\nTirthankar RayChaudhuri and Leonard G. C. Hamey.\n1995. Minimisation of data collection by active\nlearning. In Proceedings of International Confer-\nence on Neural Networks (ICNN), pages 1338–1341.\nNicholas Roy and Andrew McCallum. 2001. Toward\noptimal active learning through sampling estimation\nof error reduction. In Proceedings of the Eigh-\nteenth International Conference on Machine Learn-\ning (ICML), pages 441–448.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nTobias Scheffer, Christian Decomain, and Stefan Wro-\nbel. 2001. Active hidden markov models for infor-\nmation extraction. In Proceedings of the 4th Inter-\nnational Conference on Advances in Intelligent Data\nAnalysis (IDA), pages 309–318.\nGreg Schohn and David Cohn. 2000. Less is more:\nActive learning with support vector machines. In\nProceedings of the Seventeenth International Con-\nference on Machine Learning (ICML) , pages 839–\n846.\nOzan Sener and Silvio Savarese. 2018. Active learn-\ning for convolutional neural networks: A core-set\napproach. In Proceedings of the 6th International\nConference on Learning Representations (ICLR).\nBurr Settles, Mark Craven, and Soumya Ray. 2007.\nMultiple-instance active learning. In Proceedings\nof the 20th International Conference on Neural In-\nformation Processing Systems (NIPS) , pages 1289–\n1296.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to ﬁne-tune BERT for text classiﬁcation?\nIn Chinese Computational Linguistics - 18th China\nNational Conference (CCL), pages 194–206.\nYi Tay, Anh Tuan Luu, and Siu Cheung Hui. 2018. Re-\ncurrently controlled recurrent networks. In Proceed-\nings of the Advances in Neural Information Process-\ning Systems 31 (NeurIPS), pages 4731–4743.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Proceedings of the Advances in Neu-\nral Information Processing Systems 30 (NeurIPS) ,\npages 5998–6008.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, and Quoc V . Salakhutdinov, Russ R a.nd Le.\n2019. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d'Alché-Buc,\nE. Fox, and R. Garnett, editors, Proceedings of the\nAdvances in Neural Information Processing Systems\n32 (NeurIPS), pages 5753–5763.\nMichelle Yuan, Hsuan-Tien Lin, and Jordan Boyd-\nGraber. 2020. Cold-start active learning through\nself-supervised language modeling. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n7935–7948.\nMike Zhang and Barbara Plank. 2021. Cartography\nactive learning. In Findings of the Association\nfor Computational Linguistics (EMNLP Findings) ,\npages 395–406.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In C. Cortes, N. D. Lawrence, D. D. Lee,\nM. Sugiyama, and R. Garnett, editors, Proceedings\nof the Advances in Neural Information Processing\nSystems 28 (NIPS), pages 649–657.\nYe Zhang, Matthew Lease, and Byron C. Wallace.\n2017. Active discriminative text representation\nlearning. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence , pages 3386–\n3392.\nHan Zhao, Zhengdong Lu, and Pascal Poupart. 2015.\nSelf-adaptive hierarchical sentence model. In Pro-\nceedings of the 24th International Conference on Ar-\ntiﬁcial Intelligence, pages 4069–4076.\n2200\nWanshan Zheng, Zibin Zheng, Hai Wan, and Chuan\nChen. 2019. Dynamically route hierarchical struc-\nture representation to attentive capsule for text classi-\nﬁcation. In Proceedings of the Twenty-Eighth Inter-\nnational Joint Conference on Artiﬁcial Intelligence\n(IJCAI), pages 5464–5470.\nSupplementary Material\nThe experiments can be reproduced using the code\nthat is referenced on the ﬁrst page2. In the follow-\ning, we summarize important details for reproduc-\ntion, including details on the results.\nA Technical Environment\nAll experiments were conducted within a Python\n3.8 environment. The system had CUDA 11.1 in-\nstalled and was equipped with an NVIDIA GeForce\nRTX 2080 Ti (11GB VRAM). Computations for\nﬁne-tuning transformers and training KimCNN\nwere performed on the GPU .\nB Implementation Details\nOur experiments were built using well-known ma-\nchine learning libraries: PyTorch 3, huggingface\ntransformers4, scikit-learn5, scipy6, and numpy 7.\n2https://github.com/webis-de/ACL-22\n3https://pytorch.org/, 1.8.0\n4https://github.com/huggingface/transformers, 4.11.0\n5https://scikit-learn.org/, 0.24.0\n6https://www.scipy.org/, 1.6.0\n7https://numpy.org/, 1.19.5\nFor active learning and text classiﬁcation, we used\nsmall-text8 (Schröder et al., 2022).\nC Experiments\nEach experiment conﬁguration represents a combi-\nnation of model, dataset and query strategy, and has\nbeen run for ﬁve times. We used a class-balanced\ninitial set to support the warm start of the ﬁrst\nmodel for the imbalanced TREC-6 dataset, whose\nrarest class would otherwise only rarely be encoun-\ntered if sampled randomly.\nC.1 Pre-Trained Models\nWe ﬁne-tuned DistilRoBERTa (distilroberta-base)\nand BERT-large (bert-large-uncased). Both of them\nare available via the huggingface model repository.\nDataset Max. Seq. Length\nAGN 60\nCR 50\nMR 60\nSUBJ 50\nTREC 40\nTable 6: Hyperparameter settings for the maximum se-\nquence length (as number of tokens) per dataset.\n8https://github.com/webis-de/small-text, 1.0.0a8\nDataset Model Query Strategy\nPE BT LC CA RS\nAGN\nSVM 0.804 ± 0.000 0.804 ± 0.000 0.802 ± 0.009 0.539 ± 0.088 0.801 ± 0.006\nKimCNN 0.871 ± 0.004 0.874 ± 0.005 0.856 ± 0.012 0.814 ± 0.015 0.866 ± 0.007\nDistilRoBERTa 0.892 ± 0.002 0.894 ± 0.003 0.894 ± 0.002 0.894 ± 0.008 0.879 ± 0.008\nBERT 0.896 ± 0.003 0.904 ± 0.002 0.894 ± 0.006 0.889 ± 0.014 0.884 ± 0.003\nCR\nSVM 0.757 ± 0.000 0.755 ± 0.014 0.742 ± 0.022 0.763 ± 0.025\nKimCNN 0.765 ± 0.012 0.762 ± 0.012 0.748 ± 0.015 0.745 ± 0.014\nDistilRoBERTa 0.906 ± 0.007 0.911 ± 0.008 0.905 ± 0.011 0.886 ± 0.007\nBERT 0.904 ± 0.010 0.919 ± 0.009 0.913 ± 0.005 0.896 ± 0.008\nMR\nSVM 0.674 ± 0.000 0.650 ± 0.012 0.633 ± 0.014 0.641 ± 0.010\nKimCNN 0.719 ± 0.011 0.719 ± 0.017 0.726 ± 0.008 0.720 ± 0.013\nDistilRoBERTa 0.819 ± 0.012 0.826 ± 0.009 0.826 ± 0.011 0.809 ± 0.011\nBERT 0.857 ± 0.009 0.852 ± 0.009 0.856 ± 0.015 0.846 ± 0.011\nSUBJ\nSVM 0.843 ± 0.000 0.857 ± 0.006 0.827 ± 0.012 0.839 ± 0.012\nKimCNN 0.897 ± 0.004 0.880 ± 0.008 0.877 ± 0.010 0.896 ± 0.009\nDistilRoBERTa 0.944 ± 0.004 0.948 ± 0.008 0.939 ± 0.008 0.926 ± 0.005\nBERT 0.957 ± 0.004 0.958 ± 0.005 0.954 ± 0.005 0.949 ± 0.003\nTREC-6\nSVM 0.740 ± 0.000 0.758 ± 0.000 0.692 ± 0.101 0.596 ± 0.145 0.742 ± 0.031\nKimCNN 0.840 ± 0.016 0.836 ± 0.012 0.834 ± 0.015 0.802 ± 0.017 0.792 ± 0.020\nDistilRoBERTa 0.942 ± 0.008 0.950 ± 0.009 0.942 ± 0.009 0.940 ± 0.011 0.918 ± 0.016\nBERT 0.932 ± 0.010 0.947 ± 0.014 0.960 ± 0.006 0.968 ± 0.004 0.921 ± 0.025\nTable 5: Final accuracy per dataset, model, and query strategy. We report the mean and standard deviation over\nﬁve runs. The best result per dataset is printed in bold.\n2201\nC.2 Datasets\nOur experiments used datasets that are well-known\nbenchmarks in text classiﬁcation and active learn-\ning. All datasets have been made accessible to the\nPython ecosystem by several Python libraries that\nprovide fast access to the raw text of those datasets.\nWe obtain CR and SUBJ using gluonnlp, and AGN,\nMR, and TREC using huggingface datasets.\nC.3 Hyperparameters\nMaximum Sequence Lenght We set the maxi-\nmum sequence length to the minimum multiple\nof ten for which 95% of the respective dataset’s\nsentences contain less than or an equal number of\ntokens for both KimCNN and transformers (shown\nin Table 6).\nTransformers AGN is trained for 50 epochs and\nall other datasets for15 epochs (Howard and Ruder,\n2018). For training, we use AdamW (Loshchilov\nand Hutter, 2019) with a learning rate of η= 2e−5,\nbeta coefﬁcients of β1 = 0.9 and β2 = 0.999, and\nan epsilon of ϵ= 1e−8. Training is done in batches,\nwith a batch size of 12.\nKimCNN We adopt the parameters by Zhang\net al. (2017), i.e., 50 ﬁlters and ﬁlter heights of\n(3,4,5). Training is done in batches with a batch\nsize of 25, a learning rate of η= 1e−3, and word\nembeddings from word2vec (Mikolov et al., 2013).\nD Standard Deviations and Runtimes\nIn Table 5 and Table 7 we report ﬁnal accuracy\nand AUC scores including standard deviations,\nmeasured after the last iteration of active learn-\ning. Moreover, we report the runtimes of the query\nstep per strategy in Table 8.\nD.1 Evaluation Metrics\nActive learning was evaluated using standard active\nlearning metrics, namely accuracy und area under\nthe learning curve. For both metrics, the respective\nscikit-learn implementation was used.\nReferences\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Asso-\nciation for Computational Linguistics (ACL) , pages\n328–339.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In Proceedings of the\n7th International Conference on Learning Represen-\ntations (ICLR).\nTomás Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space. In Proceedings 1st Inter-\nnational Conference on Learning Representations\n(ICLR).\nDataset Model Query Strategy\nPE BT LC CA RS\nAGN\nSVM 0.693 ± 0.000 0.705 ± 0.000 0.690 ± 0.011 0.458 ± 0.057 0.699 ± 0.012\nKimCNN 0.753 ± 0.005 0.791 ± 0.013 0.739 ± 0.019 0.699 ± 0.022 0.810 ± 0.013\nDistilRoBERTa 0.855 ± 0.018 0.875 ± 0.007 0.852 ± 0.018 0.863 ± 0.020 0.855 ± 0.006\nBERT 0.858 ± 0.015 0.872 ± 0.005 0.848 ± 0.018 0.864 ± 0.012 0.849 ± 0.007\nCR\nSVM 0.717 ± 0.000 0.713 ± 0.009 0.695 ± 0.009 0.718 ± 0.007\nKimCNN 0.713 ± 0.015 0.717 ± 0.009 0.707 ± 0.004 0.705 ± 0.014\nDistilRoBERTa 0.874 ± 0.012 0.875 ± 0.008 0.853 ± 0.019 0.870 ± 0.010\nBERT 0.877 ± 0.011 0.857 ± 0.016 0.866 ± 0.017 0.868 ± 0.008\nMR\nSVM 0.612 ± 0.000 0.615 ± 0.012 0.584 ± 0.018 0.597 ± 0.004\nKimCNN 0.674 ± 0.009 0.683 ± 0.015 0.671 ± 0.009 0.677 ± 0.011\nDistilRoBERTa 0.784 ± 0.013 0.786 ± 0.026 0.785 ± 0.010 0.783 ± 0.007\nBERT 0.833 ± 0.013 0.831 ± 0.012 0.817 ± 0.009 0.827 ± 0.006\nSUBJ\nSVM 0.801 ± 0.000 0.802 ± 0.003 0.768 ± 0.008 0.797 ± 0.010\nKimCNN 0.859 ± 0.013 0.841 ± 0.007 0.838 ± 0.011 0.864 ± 0.008\nDistilRoBERTa 0.924 ± 0.006 0.925 ± 0.003 0.915 ± 0.015 0.902 ± 0.008\nBERT 0.939 ± 0.007 0.938 ± 0.016 0.943 ± 0.005 0.933 ± 0.005\nTREC-6\nSVM 0.491 ± 0.000 0.648 ± 0.000 0.538 ± 0.085 0.462 ± 0.112 0.619 ± 0.026\nKimCNN 0.711 ± 0.010 0.714 ± 0.009 0.683 ± 0.029 0.639 ± 0.025 0.688 ± 0.013\nDistilRoBERTa 0.840 ± 0.023 0.864 ± 0.014 0.860 ± 0.013 0.842 ± 0.005 0.856 ± 0.020\nBERT 0.789 ± 0.032 0.844 ± 0.013 0.858 ± 0.030 0.868 ± 0.027 0.828 ± 0.018\nTable 7: Final AUC per dataset, model, and query strategy. We report the mean and standard deviation over ﬁve\nruns. The best result per dataset is printed in bold.\n2202\nDataset Model Query Strategy\nPE BT LC CA RS\nAGN\nSVM 1.852 ± 0.415 0.907 ± 0.203 0.432 ± 0.097 516.554 ± 115.583 0.001 ± 0.000\nKimCNN 7.264 ± 1.626 6.199 ± 1.389 10.256 ± 2.359 481.758 ± 142.013 0.002 ± 0.000\nDistilRoBERTa 97.479 ± 21.800 96.372 ± 21.551 87.398 ± 19.560 852.457 ± 230.157 0.002 ± 0.000\nBERT 528.884 ± 118.347 503.454 ± 112.583 480.401 ± 107.422 1475.960 ± 391.579 0.002 ± 0.000\nCR\nSVM 0.005 ± 0.001 0.005 ± 0.001 0.003 ± 0.001 0.307 ± 0.070 0.000 ± 0.000\nKimCNN 0.184 ± 0.042 0.155 ± 0.035 0.163 ± 0.036 0.705 ± 0.189 0.000 ± 0.000\nDistilRoBERTa 1.942 ± 0.434 1.916 ± 0.428 1.912 ± 0.428 2.627 ± 0.648 0.000 ± 0.000\nBERT 12.112 ± 2.709 12.374 ± 2.767 12.427 ± 2.780 12.750 ± 2.852 0.000 ± 0.000\nMR\nSVM 0.014 ± 0.003 0.014 ± 0.003 0.009 ± 0.002 1.889 ± 0.425 0.000 ± 0.000\nKimCNN 0.521 ± 0.117 0.436 ± 0.098 0.468 ± 0.105 3.672 ± 1.098 0.000 ± 0.000\nDistilRoBERTa 7.558 ± 1.691 7.481 ± 1.673 7.183 ± 1.627 12.303 ± 3.293 0.000 ± 0.000\nBERT 41.428 ± 9.265 42.247 ± 9.447 41.960 ± 9.391 43.480 ± 9.747 0.000 ± 0.000\nSUBJ\nSVM 0.014 ± 0.003 0.013 ± 0.003 0.009 ± 0.002 1.969 ± 0.444 0.000 ± 0.000\nKimCNN 0.472 ± 0.106 0.409 ± 0.091 1.708 ± 1.144 3.161 ± 0.954 0.000 ± 0.000\nDistilRoBERTa 5.219 ± 1.167 5.153 ± 1.153 5.099 ± 1.140 10.508 ± 2.885 0.000 ± 0.000\nBERT 31.332 ± 7.006 32.908 ± 7.358 33.043 ± 7.393 37.832 ± 8.478 0.000 ± 0.000\nTREC-6\nSVM 0.085 ± 0.019 0.042 ± 0.009 0.018 ± 0.004 0.609 ± 0.138 0.000 ± 0.000\nKimCNN 0.289 ± 0.065 0.248 ± 0.055 1.111 ± 0.745 1.504 ± 0.447 0.000 ± 0.000\nDistilRoBERTa 2.934 ± 0.656 2.887 ± 0.646 3.239 ± 1.473 4.691 ± 1.271 0.000 ± 0.000\nBERT 14.577 ± 3.260 14.539 ± 3.251 14.963 ± 3.350 17.901 ± 17.213 0.000 ± 0.000\nTable 8: Query time in seconds. We report the mean and standard deviation over ﬁve runs. The best result (with\nthe lowest query time) per dataset and model is printed in bold.\nChristopher Schröder, Lydia Müller, Andreas Niek-\nler, and Martin Potthast. 2022. Small-Text: Ac-\ntive learning for text classiﬁcation in python. arXiv\npreprint arXiv:2107.10314.\nYe Zhang, Matthew Lease, and Byron C. Wallace.\n2017. Active discriminative text representation\nlearning. In Proceedings of the Thirty-First AAAI\nConference on Artiﬁcial Intelligence , pages 3386–\n3392.\n2203",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7160128951072693
    },
    {
      "name": "Transformer",
      "score": 0.6456819176673889
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5051012635231018
    },
    {
      "name": "Machine learning",
      "score": 0.5013465881347656
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4664340317249298
    },
    {
      "name": "Engineering",
      "score": 0.21212905645370483
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I926574661",
      "name": "Leipzig University",
      "country": "DE"
    }
  ],
  "cited_by": 46
}