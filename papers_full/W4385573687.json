{
    "title": "Empowering Language Models with Knowledge Graph Reasoning for Open-Domain Question Answering",
    "url": "https://openalex.org/W4385573687",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2408226291",
            "name": "Ziniu Hu",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A2155579404",
            "name": "Yichong Xu",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2124365211",
            "name": "Wenhao Yu",
            "affiliations": [
                "University of Notre Dame"
            ]
        },
        {
            "id": "https://openalex.org/A2127805350",
            "name": "Shuohang Wang",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2101945034",
            "name": "Ziyi Yang",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2110288568",
            "name": "Chen-guang Zhu",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2208999240",
            "name": "Kai-Wei Chang",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A2110119782",
            "name": "Yizhou Sun",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3166444100",
        "https://openalex.org/W4226281578",
        "https://openalex.org/W4297899309",
        "https://openalex.org/W3092288641",
        "https://openalex.org/W3176750236",
        "https://openalex.org/W2769099080",
        "https://openalex.org/W2962886429",
        "https://openalex.org/W1756422141",
        "https://openalex.org/W4226418288",
        "https://openalex.org/W2983995706",
        "https://openalex.org/W3176175717",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2127795553",
        "https://openalex.org/W3102844651",
        "https://openalex.org/W2962985038",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W3153094109",
        "https://openalex.org/W4223626817",
        "https://openalex.org/W4309417034",
        "https://openalex.org/W3100283070",
        "https://openalex.org/W3091432621",
        "https://openalex.org/W2989312920",
        "https://openalex.org/W3199212893",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3152740956",
        "https://openalex.org/W3005977214",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2962922117",
        "https://openalex.org/W2889787757",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2251079237",
        "https://openalex.org/W2094728533",
        "https://openalex.org/W3097986428",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W2252136820",
        "https://openalex.org/W3172335055",
        "https://openalex.org/W2990928880",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2983102021",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W3090656107",
        "https://openalex.org/W3207095490",
        "https://openalex.org/W4287865052"
    ],
    "abstract": "Answering open-domain questions requires world knowledge about in-context entities. As pre-trained Language Models (LMs) lack the power to store all required knowledge, external knowledge sources, such as knowledge graphs, are often used to augment LMs. In this work, we propose knOwledge REasOning empowered Language Model(OREO-LM), which consists of a novel Knowledge Interaction Layer that can be flexibly plugged into existing Transformer-based LMs to interact with a differentiable Knowledge Graph Reasoning module collaboratively. In this way, LM guides KG to walk towards the desired answer, while the retrieved knowledge improves LM.By adopting OREO-LM to RoBERTa and T5, we show significant performance gain, achieving state-of-art results in the Closed-Book setting. The performance enhancement is mainly from the KG reasoning’s capacity to infer missing relational facts. In addition, OREO-LM provides reasoning paths as rationales to interpret the model’s decision.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9562–9581\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nEmpowering Language Models with Knowledge Graph Reasoning for\nQuestion Answering\nZiniu Hu1, Yichong Xu2, Wenhao Yu3, Shuohang Wang2\nZiyi Yang2, Chenguang Zhu2, Kai-Wei Chang1, Yizhou Sun1\n1University of California, Los Angeles\n2Microsoft Cognitive Services Research, 3University of Notre Dame\nAbstract\nAnswering open-domain questions requires\nworld knowledge about in-context entities. As\npre-trained Language Models ( LMs) lack the\npower to store all required knowledge, exter-\nnal knowledge sources, such as knowledge\ngraphs, are often used to augment LMs. In this\nwork, we propose knOwledge REasOning em-\npowered Language Model (OREO LM), which\nconsists of a novel Knowledge Interaction\nLayer that can be flexibly plugged into existing\nTransformer-based LMs to interact with a differ-\nentiable Knowledge Graph Reasoning module\ncollaboratively. In this way, LM guides KG to\nwalk towards the desired answer, while the re-\ntrieved knowledge improves LM. By adopting\nOREO LM to RoBERTa and T5, we show signif-\nicant performance gain, achieving state-of-art\nresults in the Closed-Book setting. The per-\nformance enhancement is mainly from the KG\nreasoning’s capacity to infer missing relational\nfacts. In addition, O REO LM provides reason-\ning paths as rationales to interpret the model’s\ndecision.\n1 Introduction\nOpen-Domain Question Answering (ODQA), one\nof the most knowledge-intensive NLP tasks, re-\nquires QA models to infer out-of-context knowl-\nedge to the given single question. Following the\npioneering work by Chen et al. (2017), ODQA\nsystems often assume to access an external text\ncorpus (e.g., Wikipedia) as an external knowledge\nsource. Due to the large scale of such textual knowl-\nedge sources (e.g., 20GB for Wikipedia), it cannot\nbe encoded in the model parameters. Therefore,\nmost works retrieve relevant passages as knowl-\nedge and thus named Open-Book models (Roberts\net al., 2020), with an analogy of referring to text-\nbooks during an exam. Another line of Closed-\nbook models (Roberts et al., 2020) assume knowl-\nedge could be stored implicitly in parameters of\nLanguage Models (LM, e.g. BERT (Devlin et al.,\nFigure 1: An Illustrative figure of OREO LM. Compared\nwith previous KBQA systems that stack reasoner on top\nof LM, OREO LM enables interaction between the two.\n2019) and T5 (Raffel et al., 2020)). These LMs\ndirectly generate answers without retrieving from\nan external corpus and thus benefit from faster in-\nference speed and simpler training. However, cur-\nrent LMs still miss a large portion of factual knowl-\nedge (Pörner et al., 2020; Lewis et al., 2021a), and\nare not competitive with Open-Book models.\nTo improve the knowledge coverage ofLM, one\nnatural choice is to leverage knowledge stored\nin Knowledge Graph ( KG, e.g. FreeBase (Bol-\nlacker et al., 2008) and WikiData (Vrandecic and\nKrötzsch, 2014)), which explicitly encodes world\nknowledge via relational triplets between entities.\nThere are several good properties of KG: 1) a KG\ntriplet is a more abstract and compressed represen-\ntation of knowledge than text, and thus KGcould\nbe stored in memory and directly enhance LM with-\nout using an additional retrieval model; 2) the struc-\ntural nature of KGcould support logical reason-\ning (Ren et al., 2020) and infer missing knowledge\nthrough high-order paths (Lao et al., 2011; Das\net al., 2018). Taking the question “what cheese is\nused to make the desert cannoli?” as an example,\neven if this relational fact is missing in KG, we\ncould still leverage high-order relationships, e.g.,\nboth Ricotta Cheese and Cannoli are specialties in\nItaly, to infer the answer “Ricotta Cheese.”\n9562\nIn light of the good properties of KG, there are\nseveral efforts to build Knowledge Base Question\nAnswering (KBQA) systems. As is illustrated in\nFigure 1(a), most KBQA models useLM as a parser\nto map textual questions into a structured form\n(e.g., SQL query or subgraph), and then based on\nKG, the queries could be executed by symbolic\nreasoning (Berant et al., 2013) or neural reasoning\n(e.g. Graph Neural Networks) (Sun et al., 2019)\nto get the answer. Another recent line of research\n(Verga et al., 2021; Yu et al., 2022b) tries to en-\ncode the knowledge graph as the memory into LM\nparameters. However, for most methods discussed\nabove, LM is not interacting with KGto correctly\nunderstand the question, and the answer is usually\nrestricted to a node or edge in KG.\nIn this paper, we propose kn Owledge\nREasOning empowered Language Model\n(OREO LM), a model architecture that can be\napplied to Transformer-based LMs to improve\nClosed-Book ODQA. As is illustrated in Fig-\nure 1(b), the key component is the Knowledge\nInteraction Layers (KIL) inserted amid LM layers,\nwhich is like cream filling within two waffles,\nleading to our model’s name OREO . KIL interacts\nwith a KG reasoning module, in which we\nmaintain different reasoning paths for each entity\nin the question. We formulate the retrieval and\nreasoning process as a contextualized random\nwalk over the KG, starting from the in-context\nentities. Each KIL is responsible for one reasoning\nstep. It first predicts a relation distribution for\nevery in-context entity, and then the KGreasoning\nmodule traverses the graph following the predicted\nrelation distribution. The reasoning result in\neach step is summarized as a weighted averaged\nembedding over the retrieved entities from the\ntraversal.\nBy stacking T layers of KIL, OREO LM can re-\ntrieve entities that are T-hop away from in-context\nentities and help LM to answer open questions that\nrequire out-of-context knowledge or multi-hop rea-\nsoning. The whole procedure is fully differentiable,\nand thus OREO LM learns and infers in an end-to-\nend manner. We further introduce how to pre-train\nOREO LM over unlabelled Wikipedia corpus. In\naddition to the salient entity span masking objec-\ntive, we introduce two self-supervised objectives to\nguide OREO LM to learn better entity and relation\nrepresentations and how to reason over them.\nWe test OREO LM with RoBERTa and T5 as our\nbase LMs. By evaluating on several single-hop\nODQA datasets in closed-book setting, we show\nthat OREO LM outperforms existing baselines with\nfewer model parameters. Specifically, O REO LM\nhelps more for questions with missing relations\nin KG, and questions that require multi-hop rea-\nsoning. We further show that OREO LM can serve\nas a backbone for open-book setting and achieves\ncomparable performance compared with the state-\nof-the-art QA systems with dedicated design. In\naddition, O REO LM has better interpretability as\nit can generate reasoning paths for the answered\nquestion and summarize general relational rules to\ninfer missing relations.\nThis key contributions are as follows:\n• We propose OREO LM to integrate symbolic\nknowledge graph reasoning with neural LMs.\nDifferent from prior works, OREO LM can be\nseamlessly plugged into existing LMs.\n• We pretrain OREO LM with RoBERTa and T5\nto on the Wikipedia corpus. O REO LM can\nbring significant performance gain on ODQA.\n• OREO LM offers interpretable reasoning paths\nfor answering the question and high-order rea-\nsoning rules as rationales.\n2 Methodology\nPreliminary We denote a Knowledge Graph\nKG=\n(\nE,R,A= {Ar}r∈R\n)\n, where each e ∈E\nand r ∈ Ris entity node and relation label.\nAr ∈ {0,1}|E|×|E| is a sparse adjacency matrix\nindicating whether relation rholds between a pair\nof entities. The task of knowledge graph reason-\ning aims at answering a factoid query (s,r,?), i.e.,\nwhich target entity has relation rwith the source\nentity s. If KGis complete, we could simply get\nanswers by checking the adjacency matrix, i.e.,\n{∀t : Ar[s,t] = 1}. For incomplete KGwhere\nmany relational facts are missing, path-based rea-\nsoning approaches (Lao et al., 2011; Xiong et al.,\n2017; Das et al., 2018) have been proposed to an-\nswer the one-hop query via finding multi-hop paths.\nFor example, to answer the query (s,Mother,?), a\npath s Father−−−→j Wife−−→tcould reach the target an-\nswer t. In this paper we try to integrate symbolic\nKGreasoning into neural LMs and help it deal with\nODQA problems.\nOverview of OREO LM We illustrate the over-\nall architecture of O REO LM in Figure 2. All the\nlight blue blocks are our added components to\n9563\nFigure 2: Model architecture of OREO LM. Three key procedures are highlighted in red dotted box: 1) Relation\nPrediction (Sec. 2.1.1): Knowledge Interaction Layers (KIL) predicts relation action for each entity mention. 2)\nOne-step State Transition(Sec. 2.1.2): Based on the predicted relation, KGre-weights each graph and conduct\ncontextualized random walk to update entity distribution state. 3) Knowledge Integration(Sec. 2.2): An weighted\naggregated entity embedding is added into a placeholder token as retrieved knowledge.\nsupport KGreasoning, while the dark blue Trans-\nformer layers are knowledge-injected LM. The key\ncomponent of O REO LM for conducting KGrea-\nsoning is the Knowledge Interaction Layers (KIL),\nwhich are added amid LM layers to enable deeper\ninteraction with the KG.\nGiven a question q = “The Bauhaus repre-\nsented Germany’s recovery from which event?”,\nQA model needs to extract knowledge about all n\nin-context entity mentions M = {mi}n\ni=1, e.g., the\nhistory of “Germany” at the time when “Bauhaus”\nis founded, to get the answer a = “World War\nI”. Such open-domain Q&A can be abstracted as\nP(a|q,M).\nStarting from each mentioned entity mi, we de-\nsire the model to learn to walk over the graph to\nretrieve relevant knowledge and form a T-length\nreasoning path for answering this question, where\nT is a hyper-parameter denote the longest reason-\ning path required to answer the questions. We de-\nfine each reasoning path starting from the entity\nmention mi as a chain of entities (states) random\nvariables ρi = {et\ni}T\nt=0, where each mentioned en-\ntity is the initial state, i.e., e0\ni = mi. The union of\nall paths for this question is defined as ϱ= {ρi},\nwhich contains the reasoning paths from each men-\ntioned entity to answer the question.\nOREO LM factorizes P\n(\na|q,M\n)\nby incorporat-\ning possible paths ϱas a latent variable, yielding:\nP\n(\na|q,M\n)\n=\n∑\nϱ\nP\n(\nϱ|q,{mi}n\ni=1\n)\n·P\n(\na|q,M, ϱ\n)\n=\n∑\nϱ\n(n∏\ni=1\nP\n(\nρi|q,mi\n))\n·P\n(\na|q,{mi,ρi}n\ni=1\n)\n=\n∑\nϱ\n(n∏\ni=1\nT∏\nt=1\nP\n(\net\ni|q,e<t\ni\n)\n  \nKG Reasoning (2.1)\n)\nP\n(\na|q,{e0:T\ni }n\ni=1\n)\n  \nknowledge-injected LM (2.2)\nWe assume (1) reasoning paths starting from dif-\nferent entities are generated independently; and (2)\nreasoning paths can be generated autoregressively.\nIn this way, the QA problem can be decomposed\ninto two entangled steps: 1) KGReasoning, which\nautoregressively walks through the graph to get a\npath ρi starting from each entity mention mi; and\n2) knowledge-injected LM, which benefits from the\nreasoning paths to obtain the out-context knowl-\nedge for answer prediction.\nThe relational path ρi in KGReasoning requires\nthe selection of next entity et\ni at each step t. We\nfurther decompose it into two steps: 1.a) relation\nprediction, in which LM is involved to predict the\nnext-hop relation based on the current state and\ncontext; and 1.b) the non-parametric state transi-\ntion, which is to predict the next-hop entity based\non the KGand the predicted relation. Formally:\n9564\nP\n(\net\ni|q,e<t\ni\n)\n  \nKG Reasoning (2.1)\n=\n∑\nr\nPrel\n(\nrt\ni|q,e<t\ni\n)\n  \nrelation prediction (2.1.1)\n· Pwalk\n(\net\ni|rt\ni,e<t\ni\n)\n  \ncontextualized random walk (2.1.2)\nWe keep track of the entity distribution at each\nstep tvia the probability vector1 π(t)\ni ∈R|E|, with\nπ(t)\ni [e] being the probability of staying at entity e,\ni.e., P\n(\net\ni = e|q,e<t\ni\n)\n.\nWe highlight the three procedures in red dot-\nted box in Figure 2. We take the first reason-\ning step starting from entity mention “Bauhaus”\nas an example. In the first red box within KIL,\nwe predict which relation action should be taken\nfor entity “Bauhaus”, and send the prediction (e.g.\n“Founded”) to KG. In the second red box, KGre-\nweights the graph and conducts contextualized ran-\ndom walk to update entity distribution, where “Wal-\nter” has the highest probability. Finally, weighted\nby the entity distribution, an aggregated entity em-\nbedding is sent back to KIL and added into a place-\nholder token as the knowledge, so the later LM\nlayer knows to focus on the retrieved “Walter”. We\nintroduce these steps in the following.\nInput Initially, we first identify all N entity men-\ntions {mi}N\ni=1 in the input question q as well as\nthe corresponding KGentities2.. For each men-\ntion mi we add three special tokens as the inter-\nface for Knowledge Interaction Layers ( KIL) to\nsend instruction and receive knowledge: we add\na [S-ENT] token before, and [REL], [T-ENT] to-\nkens after each entity mention mi. KIL can be\nflexibly inserted into arbitrary LM intermediate\nlayer. By default, we just insert each KIL every N\nTransformer-based LM layers, thus the input to the\nt-th KIL are contextualized embeddings of each\ntoken kas LM(t)\nk , including added special tokens.\n2.1 LM involved KGReasoning\nWe first introduce the reasoning process\nP\n(\net\ni|q,e<t\ni\n)\n=∑\nr P\n(\nrt\ni|q,e<t\ni\n)\n·P\n(\net\ni|rt\ni,e<t\ni\n)\n.\n2.1.1 Relation Prediction.\nFor each entity mention mi, we desire to predict\nwhich relation action should take rt\ni as instruction\nto transit state. We define the predicted relation\nprobability vector γ(t)\ni = Prel\n(\nrt\ni|q,e<t\ni\n)\n∈R|R|\n1Throughout the paper, all vectors are row-vectors\n2For Wikipedia pretraining, we use the ground-truth entity\nlabel as one-hot initialization for π0\ni . For downstream tasks\nwe use GENRE (Cao et al., 2021) to get top 5 entity links.\nrepresenting the relation distribution to guide walk-\ning through the graph. Denote the corresponding\n[REL] token as REL[i] (and similarly for other spe-\ncial tokens). The contextual embedding LM(t)\nREL[i]\nencode the relevant information in question qthat\nhints next relation. We maintain a global relation\nkey memory Krel ∈R|R|×d storing each relation’s\nd-dimentional embedding. To calculate similarity,\nwe first get relation query Q(t)\nREL[i] by projecting\nrelation token’s embedding into the same space\nof key memory via a projection head Q-Proj3 fol-\nlowed by a LayerNorm (abbreviated as LN), and\nthen calculate dot-product similarity followed by\nsoftmax:\nQ(t)\nREL[i] = LN(t)(\nQ-Proj(t)(LM(t)\nREL[i])\n)\n, (1)\nγ(t)\ni = Prel\n(\nrt\ni|q,e<t\ni\n)\n= Softmax\n(\nQ(t)\nREL[i] KT\nrel\n)\n.\n(2)\nNote that the relation queries LM(t)\nREL[i] are dif-\nferent for every mention mi and reasoning step t\ndepending on the context, and thus the the relation\ndistributions γ(t)\ni gives contextualized predictions\nbased on the question q. The predicted relations\nare sent to the knowledge graph reasoning module\nas instruction to conduct state transition.\n2.1.2 Contextualized KG Random Walk\nNext, we introduce how we conduct state transi-\ntion Pwalk\n(\net\ni|rt\ni,e<t\ni\n)\n. One classic transition algo-\nrithm is random walk, which is a special case of\nmarkov chain, i.e. the transition probability only\ndepends on previous state. Consider a state at en-\ntity s, the probability walking to target tis 1\ndeg(s) if\nA[s,t] = 1. Based on it, we define the Markov tran-\nsition matrix for random walk as Mrw = D−1\nA A,\nwhere the degree matrix DA ∈ R|E|×|E| is de-\nfined as the diagonal matrix with the degrees\ndeg(1),...,deg (|E|) on the diagonal. With ran-\ndom walk Markov matrix Mrw we can transit the\nstate distribution as: π(t) = π(t−1)M, The limita-\ntion of random walk is that the transition strategy is\nnot dependent on the question q. We thus propose\na Contextualized Random Walk (CRW).\nBased on the predicted relation distribution γ(t)\ni ,\nwe calculate a different weighted adjacency matrix\n3We denote a non-linear MLP projection as X-Proj(h) =\nWX\n2 σ(WX\n1 h+b1)+b2, where X have different instantiations.\n9565\n˜A(t)\ni ∈R|E|×|E| by adjusting the edge weight:\n˜A(t)\ni =\n∑\nr∈R\nwr ·γ(t)\ni,r ·Ar, (3)\nMcrw,i(t) = D−1\n˜A(t)\ni\n˜A(t)\ni , ∀i∈[1,N]. (4)\nwhere wr is a learnable importance weight for re-\nlation rthat helps solving downstream tasks, and\nγ(t)\ni,r is the probability corresponding to relation r\nin γ(t)\ni . With the transition matrix Mcrw,i(t), the\nstate transition is defined as π(t)\ni = π(t−1)\ni M(t)\ncrw,i.\nCRW allows each reasoning path ρi to have its\ntransition matrix. However, as the total number of\nentity nodes |E|could be huge (e.g., 5M for Wiki-\nData), we cannot afford to update the entire adja-\ncency matrix for every in-batch mention. We thus\nadopt a scatter-gather pipeline to implement graph\nwalking as shown in Algorithm 1. We first gather\nthe entity and relation probability to each edge, and\nthen scatter the probability to target nodes. This al-\nlows us to simultaneously conduct message passing\nwith modified adjacency weight ˜At\ni for all entity\nmention mi in parallel.\nAlgorithm 1:Pytorch Pseudocode of CRW\ndef ContextualizedRandomWalk(\ni_init, KG, # initial entity index and Graph\nw_deg, w_rel, # inv(degree) and relation weights\np_ent, p_rel # entity and predicted relation dis-\n# tribution tensor @ t-th step.\n): -> FloatTensor\n# Get <src, rel, tgt> edge list of k-hop subgraph\ni_src, i_rel, i_tgt = k_hop_subgraph(i_init, KG)\n# Gather entity and relation probability to edge\np_src = (p_ent * w_deg)[:, i_src] # N x n_edge\np_rel = (p_rel * w_rel)[:, i_rel] # N x n_edge\np_edge = l1_normalize(p_src * p_rel, dim=1)\n# Scatter edge probability to target node\np_ent = scatter_add(src=p_edge, idx=i_tgt, dim=1)\nreturn p_ent #(t+1)-th step’s entity distribution\nThe complexity is # of in-batch entities times\n# of edges in T-hop subgraph starting from these\nentities, i.e., O(n×#edge), and thus this operation\nis not expensive. Another concern is why not us-\ning Graph Neural Networks (GNNs). We provide\ndiscussion in Sec. C in Appendix.\n2.2 Knowledge-Injected LM\nAfter we get the updated entity distribution π(t)\ni ,\nwe want to inject such information back to the\nLM without harming its overall structure. We\nmaintain a global entity embedding value mem-\nory Vent ∈R|E|×d storing entity embeddings. We\nonly consider the entities within the sampled lo-\ncal subgraph in each batch. We thus get an entity\nindex list Ias the query to sparsely retrieve a set\nof candidate entity embeddings and then aggregate\nthem weighted by entity distribution and embed-\nding table. We then use a Value Projection block\nto map the aggregated entity embedding into the\nspace of LM, and then directly add the transformed\nembedding back to the output of T-ENT.\nV(t)\ni = V-Proj(t)(\nπ(t)\ni ·Vent[I]\n)\n, (5)\nˆLM\n(t)\nT-ENT[i] = LN(t)(\nLM(t)\nT-ENT[i] + V(t)\ni\n)\n. (6)\nThen, we just take all ˆLM\n(t)\nT-ENT as input to next\nTransformer-based LM layer to learn the interaction\nbetween the retrieved knowledge with in-context\nwords via self-attention.\nBy repeating the KIL for T times, the final rep-\nresentation ˆLM\nT\nis conditioned on the reasoning\npaths ρi = e0:T\ni , which reaches entities that are T-\nhop away from initial entity mi in the question. Fi-\nnally, we can predict the answer of open questions\nP\n(\na|q,{e0:T\ni }n\ni=1\n)\nby taking knowledge-injected\nrepresentation ˆLM\nT\nfor span extraction, entity pre-\ndiction or direct answer generation.\n2.3 Pre-Train OREO LM to Reason\nThe design of OREO LM allows end-to-end training\ngiven QA datasets. However, due to the small cov-\nerage of knowledge facts for existing QA datasets,\nwe need to pretrain OREO LM on a large-scale cor-\npus to get good entity embeddings.\nSalient Span Masking One straightforward ap-\nproach is to use Salient Span Masking (SSM) ob-\njective (Guu et al., 2020) masks out entities or noun\ntokens requiring specific out-of-context knowledge.\nWe mainly mask out entities for guiding OREO LM\nto reason. Instead of randomly masking entity men-\ntions, we explicitly sample a set of entity IDs and\nmask every mentions linking to these entities. This\ncould prevent the model copy the entity from the\ncontext to fill in the blank. We also follow (Yang\net al., 2019) to mask out consecutive token spans.\nWe then calculate the cross-entropy loss on each\nsalient span masked (SSM) token as LSSM .\n2.3.1 Weakly Supervised Training ofKIL\nIdeally, OREO LM can learn all the entity knowl-\nedge and how to access the knowledge graph by\nsolely optimizing LSSM . However, without a good\ninitialization of entity and relation embeddings,\nKIL makes a random prediction, and the retrieved\nentities by KGreasoning are likely to be unrelated\n9566\nFigure 3: Pre-training sample w/ golden reasoning path.\nMore real examples are shown in Table 8 in Appendix.\nto the question. In this situation, KIL does not\nreceive meaningful gradients to update the param-\neters, and LM learns to ignore the knowledge. To\navoid this cold-start problem and provide entity\nand relation embedding a good initialization, We\nutilize the following two external signals as self-\nsupervised guidance.\nEntity Linking Loss To initialize the large entity\nembedding tables in Vent, we use other entities that\nare not masked as supervision. Similar to Févry\net al. (2020), we force the output embedding of\n[S-ENT] token before the first KIL followed by a\nprojection head E-Proj to be close to its correspond-\ning entity embedding:\nES-ENT[i] = LN\n(\nE-Proj(LM(1)\nS-ENT[i])\n)\n,\nP(0)\nent\n(\ne|mi,q\n)\n= Softmax\n(\nES-ENT[i] Vent[I]T )\n,\nLent =\n∑\nmi\n−log P(0)\nent\n(\ne|mi,q\n)\n·π0\ni [I].\nSimilar to Section 2.2, we only consider entities\nwithin the batch, denoted by index I. This con-\ntrastive loss guides each entity’s embeddingVent[e]\ncloser to all its previously mentioned contextual-\nized embedding, and thus memorizes those context\nas a good initialization for later knowledge integra-\ntion.\nWeakly Supervised Relation Path LossEntity\nmentions within each Wikipedia passage are natu-\nrally grounded to WikiData KG. Therefore, after\nwe mask out several entities, we can utilize the KG\nto get all reasoning paths from other in-context en-\ntities to the masked entities as weakly supervised\nrelation labels.\nFormally, we define a Grounded Dependency\nGraph DG, which contains all reasoning paths\nwithin T-step from other in-context entities to\nmasked entities, and then define RDG(mi,t) as\nName Number dimension #param (M)\nNumber of Entity 4,947,397 128 633\nNumber of Relation 2,008 768 1.5\nNumber of Edges 45,217,947 - 47\nTable 1: Statistics and parameter of KGMemory.\nthe set of all relations over every edges for entity\nmention mi at t-th hop. Based on it, we define\nthe weakly supervised relation label q(t)\ni ∈R|R|\nas the probabilistic vector which uniformly dis-\ntributed on each relation in set. Note that we call\nuniformly-weighted q(t)\ni as weakly supervised be-\ncause 1) some paths lead to multiple entities rather\nthan only the target masked entity; 2) the correct re-\nlation is dependent on the context. Therefore, q(t)\ni\nonly provides all potential candidates for reacha-\nbility, and more fine-grained signals for reasoning\nshould be learned from unsupervised LSSM . We\nadopt a list-wise ranking loss to guide the model to\nassign a higher score on these relations than others.\nLrel =\n∑\nmi\n∑ T\nt=1\n−log P(t)\nrel\n(\nr|mi,q\n)\n·q(t)\ni .\nOverall, Lent and Lrel provide OREO LM with\ngood initialization of the large KGmemory. After-\nward, via optimizing LSSM , the reasoning paths\nthat provide informative knowledge receive a posi-\ntive gradient, guiding OREO LM to reason.\n3 Experiments\nThe proposed KIL layers can be pugged into most\nTransformer-based Language Models without hurt-\ning its original structure. In this paper, we experi-\nment with both encoder-based LM, i.e. RoBERTa-\nbase ( d = 768,l = 12), and encoder-decoder\nLM, i.e. T5-base ( d = 768,l = 12) and T5-large\n(d= 1024,l = 24). For all LMs, add 1 KIL layer\nor 2 KIL layers to the encoder layers. The statistics\nof KGare shown in Table 1. Altogether, it takes\nabout 0.67B parameter for KGmemory, which is\naffordable to load as model parameter. We pre-\ntrain all LMs using the combination of LSSM , Lent\nand Lrel for 200k steps on 8 V100 GPUs, with a\nbatch size of 128 and default optimizer and learning\nrate in the original paper, taking approximately one\nweek to finish pre-training of T5-large model, and\n1-2 days for base model. Implementation details\nare elaborated in Appendix A.\n3.1 Evaluate for Closed-Book QA\nOREO LM is designed for improving Closed-Book\nQA, so we first evaluate it in this setting.\n9567\nModels #param NQ WQ TQA ComplexWQ HotpotQA\nT5 (Base) 0.22B 25.9 27.9 29.1 11.6 22.8\n+ OREO LM (T=1) 0.23B + 0.68 B 28.3 30.6 32.4 20.8 24.1\n+ OREO LM (T=2) 0.24B + 0.68 B 28.9 31.2 33.7 23.7 26.3\nT5 (Large) 0.74B 28.5 30.6 35.9 16.7 25.3\n+ OREO LM (T=1) 0.75B + 0.68 B 30.6 32.8 39.1 24.5 28.2\n+ OREO LM (T=2) 0.76B + 0.68 B 31.0 34.3 40.0 27.1 31.4\nT5-3B (Roberts et al., 2020) 3B 30.4 33.6 43.4 - 27.8\nT5-11B (Roberts et al., 2020) 11B 32.6 37.2 50.1 - 30.2\nTable 2: Closed-Book Generative QAperformance of Encoder-Decoder LM on Single- and Multi-hop Dataset.\nGenerative QA Task Following the hyperpa-\nrameters and setting in (Roberts et al., 2020),\nwe directly fine-tune the T5-base and T5-large\naugmented by our O REO LM on the three\nsingle-hop ODQA datasets: Natural Question\n(NQ) (Kwiatkowski et al., 2019), WebQues-\ntions (WQ) (Berant et al., 2013) and TriviaQA\n(TQA) (Joshi et al., 2017). To test OREO LM’s abil-\nity to solve complex questions, we also evaluate on\ntwo multi-hop QA datasets, i.e.Complex WQ(Tal-\nmor and Berant, 2018) and HotpotQA (Yang et al.,\n2018). Detailed dataset statistics and experimental\nsetups are in Appendix B.\nExperimental results are shown in Table 7. We\nuse Exact Match accuracy as the metric for all the\ndatasets. On the three single-hop ODQA datasets,\nOREO LM with 2 KIL blocks achieves 3.3 abso-\nlute accuracy improvement to T5-base, and 3.4 im-\nprovement to T5-large. Compared with T5 model\nwith more model parameters (e.g., T5-3B and T5-\n11B), our T5-large augmented by OREO LM could\noutperform T5-3B on NQ and WQ datasets. In ad-\ndition, OREO LM could use the generated reasoning\npath to interpret the model’s prediction. We show\nexamples in Table 10 in Appendix.\nFor the two multi-hop QA datasets, the per-\nformance improvement brought by O REO LM is\nmore significant, i.e., 7.8 to T5-base and 8.2 to T5-\nlarge. Notably, by comparing the T5-3B and T5-\n11B’s performance on HotpotQA (we take results\nfrom (Chen et al., 2022)), T5-large augmented by\nOREO LM achieves 1.2 higher than T5-11B. This\nshows that OREO LM is indeed very effective for im-\nproving Closed-Book QA performance, especially\nfor complex questions.\nEntity Prediction Task Encoder-based LM (i.e.\nRoBERTa) in most cases cannot be directly used\nfor Closed-Book QA, but more serve as reader to\nextract answer span. However, Verga et al. (2021)\npropose a special evaluation setting asClosed-Book\nEntity Prediction. They add a single [MASK] token\nafter the question, and use its output embedding\nto classify WikiData entity ID. This restricts that\nanswers must be entities that are covered by Wiki-\nData, which they call WikiData-Answerable ques-\ntions. We follow Verga et al. (2021) to use such re-\nduced version of WebQuestionsSP (WQ-SP) (Yih\net al., 2015) and TriviaQA (TQA) as evaluation\ndataset, and finetune the RoBERTa (base) model\naugmented by OREO LM to classify entity ID. We\nmainly compare OREO LM with EaE (Févry et al.,\n2020) and FILM (Verga et al., 2021), which are two\nKGmemory augmented LM. We also run experi-\nments on KEPLER (Wang et al., 2019), a RoBERTa\nmodel pre-trained with knowledge augmented task.\nExperimental results are shown in Table 3. Sim-\nilar to the observation reported by Verga et al.\n(2021), adding KGmemory for this entity predic-\ntion task could significantly improve over vanilla\nLM, as most of the factual knowledge required to\npredict entities are stored in KG. By comparing\nwith FILM (Verga et al., 2021), which is the state-\nof-the-art model in this setup, OREO LM with rea-\nsoning step ( T = 2) outperforms FILM by 2.9,\nwith smaller memory consumption.\n3.2 Analyze KGReasoning Module\nIn our previous studies, we find that using a higher\nreasoning step, i.e. T = 2, generally performs\nbetter than T = 1. We hypothesize that the KG\nwe use has many missing one-hop facts, and high-\norder reasoning helps recover them and empow-\ners the model to answer related questions. To test\nwhether OREO LM indeed can infer missing facts,\nwe use EntityQuestions (EQ)(Sciavolino et al.,\n2021), which is a synthetic dataset by mapping\neach WikiData triplet to natural questions. We take\nRoBERTa-base model augmented by O REO LM\ntrained on NQ as entity predictor and directly test\n9568\nits transfer performance on EQ dataset without fur-\nther fine-tuning.\nTo test whether OREO LM could recover missing\nrelation, we mask all the edges corresponding to\neach relation separately and make the prediction\nagain. The average results before and after remov-\ning edges are shown on the left part of Figure 4.\nWhen we remove all the edges to each relation,\nOREO LM with T = 1 drops significantly, while\nT = 2could still have good accuracy. To under-\nstand why OREO LM (T = 2) is less influenced, in\nthe right part of Figure 4, we generate a reasoning\npath for each relation by averaging the predicted\nprobability score at each reasoning step and pick\nthe relation with the top score. For example, to\npredict the “Capital” of a country, the model learns\nto find the living place of the president, or the lo-\ncation of a country’s central bank. Both are very\nreasonable guesses. Many previous works (Xiong\net al., 2017) could also learn such rules in an ad-hoc\nmanner and require costly searching or reinforce-\nment learning. In contrast, O REO LM could learn\nsuch reasoning capacity for all relations end-to-end\nduring pre-training.\nAblation Studies We conduct several ablation\nstudies to evaluate which model design indeed con-\ntributes to the model. As shown in the bottom\nblocks in Table 3, we first remove theKGreason-\ning component and provide RoBERTa base model\nvia concatenated KB triplets and train such a model\nusing LSSM over the same WikiDataset. Such a\nmodel’s results are close to the KEPLER results\nbut much lower than other models with explicit\nknowledge memory. We further investigate the\nrole of pre-training tasks. Without pre-training,\nthe OREO LM only performs slightly better than\nRoBERTa baseline, due to the cold-start problem\nof entity and relation embedding. We further show\nthat removing Lent and Lent could significantly\ninfluence final performance. The current combina-\ntion is the best choice to train OREO LM to reason.\n3.3 Evaluate for Open-Book QA\nThough OREO LM is designed forClosed-Book QA,\nthe learned model can serve as backbone for Open-\nBook QA. We take DPR and FiD models as base-\nline. For DPR retriever, we replace the question\nencoder to RoBERTa + OREO LM, fixing the pas-\nsage embedding and only finetune on each down-\nstream QA dataset. For FiD model, we replace the\nT5 + OREO LM. We also changed the retriever with\nModels #param (B) WQ-SP TQA\nEaE (Févry et al., 2020) 0.11 + 0.26 62.4 24.4\nFILM (Verga et al., 2021) 0.11 + 0.72 78.1 37.3\nKEPLER (Wang et al., 2019) 0.12 48.3 24.1\nRoBERTa (Base) 0.12 43.5 21.3\n+ OREO LM (T=1) 0.12 + 0.68 80.1 39.7\n+ OREO LM (T=2) 0.13 + 0.68 80.9 40.3\nAblation Studies\nRoBERTa + Concat KB +LSSM 0.12 47.1 22.6\n+ OREO LM (T=2) w/o PT 0.13 + 0.68 46.9 22.7\nw. LSSM 0.13 + 0.68 51.9 26.8\nw. LSSM + Lent 0.13 + 0.68 68.4 35.7\nTable 3: Closed-Book Entity Predictionperformance\nof Encoder LM on WikiData-Answerable Dataset.\nModels #param (B) NQ TQA\nGraph-Retriever (Min et al., 2019) 0.11 34.7 55.8\nREALM (Guu et al., 2020) 0.33 + 16 40.4 -\nDPR (Karpukhin et al., 2020) + BERT 0.56 + 16 41.5 56.8\n+ OREO LM (DPR, T=2) 0.57 + 17 43.7 58.5\nFiD (Base) = DPR + T5 (Base) 0.44 + 16 48.2 65.0\n+ OREO LM (T5, T=2) 0.45 + 17 49.3 67.1\n+ OREO LM (DPR & T5, T=2) 0.46 + 17 51.1 68.4\nFiD (Large) = DPR + T5 (Large) 0.99 + 16 51.4 67.6\n+ OREO LM (T5, T=2) 0.99 + 17 52.4 68.9\n+ OREO LM (DPR & T5, T=2) 1.00 + 17 53.2 69.5\nKG-FiD (Base) (Yu et al., 2022a) 0.44 + 16 49.6 66.7\nKG-FiD (Large) (Yu et al., 2022a) 0.99 + 16 53.2 69.8\nEMDR2 (Sachan et al., 2021b) 0.44 + 16 52.5 71.4\nTable 4: Open-Book QAEvaluation.\nour tuned DPR. Results in Table 4 show that by\naugmenting both retriever and generator, OREO LM\nimproves a strong baseline like FiD, for about 3.1%\nfor Base and 1.8% for Large, and it outperforms the\nvery recent KG-FiD model for 1.6% in base setting,\nand achieve comparative performance in a large set-\nting. Note that though our results is still lower than\nsome recent models (e.g., EMDR2), these methods\nare dedicated architecture or training framework for\nOpen-Book QA. We may integrate OREO LM with\nthese models to further improve their performance.\n4 Related Work\nOpen-Domain Question Answering (ODQA)\ngives QA model a single question without any con-\ntext and asks the model to infer out-of-context\nknowledge. Following the pioneering work\nby Chen et al. (2017), most ODQA systems as-\nsume the model can access an external text corpus\n(e.g. Wikipedia). Due to the large scale of web\ncorpus (20GB for Wikipedia), it could not be sim-\nply encoded in the QA model parameters, and thus\nmost works propose a Retrieval-Reader pipeline,\nby firstly index the whole corpus and use a re-\ntriever model to identify which passage is relevant\n9569\nFigure 4: Testing the reasoning capacity of OREO LM to infer missing relations. On the left, the barplot shows\nthe transfer performance on EQ before and after removing relation edges, OREO LM (T = 2) is less influenced. On\nthe right shows reasoning paths (rules) automatically generated by OREO LM for each missing relation.\nto the question; then the retrieved text passage con-\ncatenate with question is re-encoded by a seperate\nreader model (e.g., LM) to predict answer. As the\nknowledge is outside of model parameter, Roberts\net al. (2020) defines these methods as Open-book,\nwith an analogy to referring textbooks during exam.\nClosed-book QA models (mostly a single LM) try\nto answer open questions without accessing exter-\nnal knowledge. This setting is much harder as it\nrequires LM to memorize all pertinent knowledge\nin its parameters, and even recent LMs with much\nlarger model parameters is still not competitive to\nstate-of-the-art Open-book models.\nKnowledge-augmented Language Modelsexplic-\nitly incorporate external knowledge (e.g. knowl-\nedge graph) into LM (Yu et al., 2022d). Over-\nall, these approaches can be grouped into two\ncategories: The first one is to explicitly inject\nknowledge representation into language model\npre-training, where the representations are pre-\ncomputed from external sources (Zhang et al.,\n2019; Liu et al., 2021; Hu et al., 2021). For ex-\nample, ERNIE (Zhang et al., 2019) encodes the\npre-trained TransE (Bordes et al., 2013) embed-\ndings as input. The second one is to implicitly\nmodel knowledge information into language model\nby performing knowledge-related tasks, such as\nentity category prediction (Yu et al., 2022b) and\ngraph-text alignment (Ke et al., 2021). For exam-\nple, JAKET (Yu et al., 2022b) jointly pre-trained\nboth the KG representation and language represen-\ntation by adding entity category and relation type\nprediction self-supervised tasks.\nThere also exists several QA works using KGto\nhelp ODQA. For example, Asai et al. (2020) and\nMin et al. (2019) expand the entity graph following\nwikipedia hyperlinks or triplets in knowledge base.\nDing et al. (2019) extract entities from current con-\ntext via entity-linking and turn them into a cogni-\ntive graph, and a graph neural network is applied\non top of it to extract answer. Dhingra et al. (2020)\nand Lin et al. (2020) construct an entity-mention\nbipartite graph and then model the QA reasoning\nas graph traversal by filtering only the contexts\nthat are relevant to the question. Lin et al. (2019),\nFeng et al. (2020) and Yasunaga et al. (2021) parse\nthe question into a sub-graph of knowledge base,\nand apply graph neural networks as reasoner for\nextracting one of the entities as the answer.\nTo encode knowledge (significantly smaller than\nthe web corpus) as memory into LM parameter, a\nline of works try compressed knowledge including\nQA pairs (Chen et al., 2022; Lewis et al., 2021b;\nYu et al., 2022c), entity embedding (Févry et al.,\n2020) and reasoning cases (Das et al., 2021, 2022).\nThere’s also several works utilizing Knowledge\nGraph (KG) to augment LM. FILM (Verga et al.,\n2021) turns KGtriplets into memory. Given a ques-\ntion, LM retrieves most relevant triplet as answer.\nGreaseLM (Zhang et al., 2022) propose to interact\nLM with KGvia a interaction node.\n5 Conclusion\nWe presented OREO LM, a novel model that in-\ncorporates symbolic KGreasoning with existing\nLMs. We showed that OREO LM can bring signifi-\ncant performance gain to open-domain QA bench-\nmarks, both for closed-book and open-book set-\ntings, as well as encoder-only and encoder-decoder\nmodels. Additionally, OREO LM produces reason-\ning paths that helps interpret the model prediction.\nIn future, we’d like to improve OREO LM by train-\ning to conduct more reasoning steps, supporting\nlocial reasoning, and apply OREO LM to a broader\nrange of knowledge-intensive NLP tasks.\nAcknowledgement We sincerely thank anony-\nmous reviewers for their constructive comments to\nimprove this paper. The project was partially sup-\nported in part by CISCO, NSF III-1705169, NSF\n1937599, NASA, Okawa Foundation Grant, Ama-\nzon Research Awards, Cisco research grant, and\nPicsart gift. Ziniu is supported by the Amazon\nFellowship and Baidu PhD Fellowship.\n9570\n6 Limitations\nLimited Reasoning Steps In our experiments,\nwe show that using reasoning stepT = 2has better\nperformance to T = 1on one-hop and multi-hop\n(mostly two) QA datasets. Thus, it’s a natural ques-\ntion about whether we could extending reasoning\nsteps more? As previous KG reasoning mostly\ncould support very long path (with LSTM design)\nThough we didn’t spend much time exploring\nbefore the paper submission, we indeed try using\nT = 3, but currently it didn’t get better results. We\nhypothesize the following reasons: 1) A large por-\ntion of our current model’s improvement relies on\nthe weakly supervised relation pre-training. To do\nit, we construct a K-hop (K=2 now) subgraph, and\nsample dependency graph based on it. The larger\nKwe choose, the more noise is included into the\ngenerated relation label, in an exponential increas-\ning speed. Thus, it’s harder to get accurate reason-\ning path ground-truth for high-order T. Another\npotential reason is that within Transformer model,\nthe representation space in lower and upper layer\nmight be very different, say, encode more syntax\nand surface knowledge at lower layers, while more\nsemantic knowledge at upper layers. Currently we\nadopt a MLP projection head, wishing to map inte-\ngrated knowledge into the same space, but it might\nhave many flaws and need further improvement.\nLarge Entity Embedding Table requires Pre-\nTraining and GPU resourcesOur current design\nhas a huge entity embedding table, which should be\nlearned through additional supervision and could\nnot directly fine-tune to downstream tasks. This is\nrestricts our approach’s usage.\nRequire Entity Linking Current model design\nrequires an additional step of entity linking for\nincoming questions, and then add special tokens as\ninterface. A truly end-to-end model should identify\nwhich elements to start conducting reasoning by its\nown without relying on external models.\nOnly support relational path-based reasoning\nThough there are lots of potential reasoning tasks,\nsuch as logical reasoning, commonsense reasoning,\nphysical reasoning, temporal reasoning, etc. Our\ncurrent model design mainly focus on path-based\nrelational reasoning, and it should not work for\nother reasoning tasks at current stage.\nUnreasonable Assumption of Path In-\ndependency When we derive equation 1,\nwe have the assumption that reasoning paths\nstarting from different entities should be inde-\npendent. This is not always correct, especially\nfor questions that require logical reasoning, say,\nhave conjunction or disjunction operation over\neach entity state. And thus our current methods\nmight not work for those complex QA with logical\ndependencies.\nReferences\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over wikipedia graph\nfor question answering. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2013, 18-21 October\n2013, Grand Hyatt Seattle, Seattle, Washington, USA,\nA meeting of SIGDAT, a Special Interest Group of the\nACL, pages 1533–1544. ACL.\nKurt D. Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collabo-\nratively created graph database for structuring human\nknowledge. In Proceedings of the ACM SIGMOD\nInternational Conference on Management of Data,\nSIGMOD 2008, Vancouver, BC, Canada, June 10-12,\n2008, pages 1247–1250. ACM.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013, Lake\nTahoe, Nevada, United States, pages 2787–2795.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May\n3-7, 2021. OpenReview.net.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1870–1879.\nAssociation for Computational Linguistics.\nWenhu Chen, Pat Verga, Michiel de Jong, John Wieting,\nand William Cohen. 2022. Augmenting pre-trained\nlanguage models with qa-memory for open-domain\nquestion answering. CoRR, abs/2204.04581.\n9571\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nLuke Vilnis, Ishan Durugkar, Akshay Krishnamurthy,\nAlex Smola, and Andrew McCallum. 2018. Go for a\nwalk and arrive at the answer: Reasoning over paths\nin knowledge bases using reinforcement learning. In\n6th International Conference on Learning Represen-\ntations, ICLR 2018, Vancouver, BC, Canada, April\n30 - May 3, 2018, Conference Track Proceedings .\nOpenReview.net.\nRajarshi Das, Ameya Godbole, Ankita Naik, Elliot\nTower, Robin Jia, Manzil Zaheer, Hannaneh Ha-\njishirzi, and Andrew McCallum. 2022. Knowledge\nbase question answering by case-based reasoning\nover subgraphs. CoRR, abs/2202.10610.\nRajarshi Das, Manzil Zaheer, Dung Thai, Ameya God-\nbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros\nPolymenakos, and Andrew McCallum. 2021. Case-\nbased reasoning for natural language queries over\nknowledge bases. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021 , pages\n9594–9611. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nBhuwan Dhingra, Manzil Zaheer, Vidhisha Balachan-\ndran, Graham Neubig, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2020. Differentiable reasoning\nover a virtual knowledge base. In 8th International\nConference on Learning Representations, ICLR 2020,\nAddis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nMing Ding, Chang Zhou, Qibin Chen, Hongxia Yang,\nand Jie Tang. 2019. Cognitive graph for multi-hop\nreading comprehension at scale. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n2694–2703. Association for Computational Linguis-\ntics.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng\nWang, Jun Yan, and Xiang Ren. 2020. Scalable multi-\nhop relational reasoning for knowledge-aware ques-\ntion answering. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 1295–1309. Association for Computa-\ntional Linguistics.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. CoRR, abs/2004.07202.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR,\nabs/2002.08909.\nZiniu Hu, Yizhou Sun, and Kai-Wei Chang. 2021.\nRelation-guided pre-training for open-domain ques-\ntion answering. In Findings of the Association for\nComputational Linguistics: EMNLP 2021, Virtual\nEvent / Punta Cana, Dominican Republic, 16-20\nNovember, 2021, pages 3431–3448. Association for\nComputational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Volume\n1: Long Papers, pages 1601–1611. Association for\nComputational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Ledell\nWu, Sergey Edunov, Danqi Chen, and Wen-tau Yih.\n2020. Dense passage retrieval for open-domain ques-\ntion answering. CoRR, abs/2004.04906.\nPei Ke, Haozhe Ji, Yu Ran, Xin Cui, Liwei Wang, Lin-\nfeng Song, Xiaoyan Zhu, and Minlie Huang. 2021.\nJointgt: Graph-text joint representation learning for\ntext generation from knowledge graphs. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-6,\n2021, volume ACL/IJCNLP 2021 ofFindings of ACL,\npages 2526–2538. Association for Computational\nLinguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics, 7:452–\n466.\nNi Lao, Tom M. Mitchell, and William W. Cohen. 2011.\nRandom walk inference and learning in A large scale\nknowledge base. In Proceedings of the 2011 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2011, 27-31 July 2011, John\nMcIntyre Conference Centre, Edinburgh, UK, A meet-\ning of SIGDAT, a Special Interest Group of the ACL,\npages 529–539. ACL.\nPatrick S. H. Lewis, Pontus Stenetorp, and Sebastian\nRiedel. 2021a. Question and answer test-train over-\nlap in open-domain question answering datasets. In\nProceedings of the 16th Conference of the European\n9572\nChapter of the Association for Computational Lin-\nguistics: Main Volume, EACL 2021, Online, April 19\n- 23, 2021, pages 1000–1008. Association for Com-\nputational Linguistics.\nPatrick S. H. Lewis, Yuxiang Wu, Linqing Liu, Pasquale\nMinervini, Heinrich Küttler, Aleksandra Piktus, Pon-\ntus Stenetorp, and Sebastian Riedel. 2021b. PAQ: 65\nmillion probably-asked questions and what you can\ndo with them. CoRR, abs/2102.07033.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. Kagnet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2829–2839. Association for\nComputational Linguistics.\nBill Yuchen Lin, Haitian Sun, Bhuwan Dhingra, Manzil\nZaheer, Xiang Ren, and William W. Cohen. 2020.\nDifferentiable open-ended commonsense reasoning.\nCoRR, abs/2010.14439.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S.\nYu. 2021. KG-BART: knowledge graph-augmented\nBART for generative commonsense reasoning. In\nThirty-Fifth AAAI Conference on Artificial Intelli-\ngence, AAAI 2021, Thirty-Third Conference on In-\nnovative Applications of Artificial Intelligence, IAAI\n2021, The Eleventh Symposium on Educational Ad-\nvances in Artificial Intelligence, EAAI 2021, Virtual\nEvent, February 2-9, 2021, pages 6418–6425. AAAI\nPress.\nSewon Min, Danqi Chen, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Knowledge guided text re-\ntrieval and reading for open domain question answer-\ning. CoRR, abs/1911.03868.\nNina Pörner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: efficient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, Online Event,\n16-20 November 2020, volume EMNLP 2020 ofFind-\nings of ACL, pages 803–818. Association for Compu-\ntational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nHongyu Ren, Weihua Hu, and Jure Leskovec. 2020.\nQuery2box: Reasoning over knowledge graphs in\nvector space using box embeddings. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 5418–5426. Association for\nComputational Linguistics.\nDevendra Singh Sachan, Mostofa Patwary, Mohammad\nShoeybi, Neel Kant, Wei Ping, William L. Hamilton,\nand Bryan Catanzaro. 2021a. End-to-end training of\nneural retrievers for open-domain question answer-\ning. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n6648–6662. Association for Computational Linguis-\ntics.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021b. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances\nin Neural Information Processing Systems 34: An-\nnual Conference on Neural Information Processing\nSystems 2021, NeurIPS 2021, December 6-14, 2021,\nvirtual, pages 25968–25981.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 6138–6148. Association for\nComputational Linguistics.\nHaitian Sun, Tania Bedrax-Weiss, and William W. Co-\nhen. 2019. Pullnet: Open domain question answering\nwith iterative retrieval on knowledge bases and text.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 2380–2390.\nAssociation for Computational Linguistics.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA,\nJune 1-6, 2018, Volume 1 (Long Papers), pages 641–\n651. Association for Computational Linguistics.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam W. Cohen. 2021. Adaptable and inter-\npretable neural memory over symbolic knowledge.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nNAACL-HLT 2021, Online, June 6-11, 2021, pages\n3678–3691. Association for Computational Linguis-\ntics.\n9573\nDenny Vrandecic and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juanzi Li, and Jian Tang. 2019. KEPLER: A uni-\nfied model for knowledge embedding and pre-trained\nlanguage representation. CoRR, abs/1911.06136.\nWenhan Xiong, Thien Hoang, and William Yang Wang.\n2017. Deeppath: A reinforcement learning method\nfor knowledge graph reasoning. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2017, Copen-\nhagen, Denmark, September 9-11, 2017, pages 564–\n573. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754–5764.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369–2380. Association for Computational\nLinguistics.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosselut,\nPercy Liang, and Jure Leskovec. 2021. QA-GNN:\nreasoning with language models and knowledge\ngraphs for question answering. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2021,\nOnline, June 6-11, 2021, pages 535–546. Association\nfor Computational Linguistics.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and Jian-\nfeng Gao. 2015. Semantic parsing via staged query\ngraph generation: Question answering with knowl-\nedge base. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics\nand the 7th International Joint Conference on Natu-\nral Language Processing of the Asian Federation of\nNatural Language Processing, ACL 2015, July 26-31,\n2015, Beijing, China, Volume 1: Long Papers, pages\n1321–1331. The Association for Computer Linguis-\ntics.\nDonghan Yu, Chenguang Zhu, Yuwei Fang, Wenhao\nYu, Shuohang Wang, Yichong Xu, Xiang Ren, Yim-\ning Yang, and Michael Zeng. 2022a. Kg-fid: Infus-\ning knowledge graph in fusion-in-decoder for open-\ndomain question answering. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 4961–\n4974. Association for Computational Linguistics.\nDonghan Yu, Chenguang Zhu, Yiming Yang, and\nMichael Zeng. 2022b. JAKET: joint pre-training of\nknowledge graph and language understanding. Con-\nference on Artificial Intelligence, AAAI,.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2022c. Generate\nrather than retrieve: Large language models are\nstrong context generators. CoRR, abs/2209.10063.\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022d.\nA survey of knowledge-enhanced text generation.\nACM Computing Survey.\nXikun Zhang, Antoine Bosselut, Michihiro Yasunaga,\nHongyu Ren, Percy Liang, Christopher D. Manning,\nand Jure Leskovec. 2022. Greaselm: Graph reason-\ning enhanced language models for question answer-\ning. CoRR, abs/2201.08860.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 1441–1451. Association for Computa-\ntional Linguistics.\n9574\nA Implementation Details\nEntity Linking durine pre-training We use\nthe 2021 Jan. English dump of Wikidata and\nWikipedia. For each wikipedia page, we link all\nentity mentions with hyperlinks to WikiData en-\ntity entry, augment all other mentions with same\naliases, tokenize via each LM’s tokenizer and split\ninto chunks with maximum token length allowed.\nWe then construct induced k-hop subgraphs con-\nnecting entities within each chunk for quickly get\ngrounded computational graph.\nFor entities, Wikipedia provides hyperlinks with\nground-truth entity ID, but it doesn’t cover all the\nentity mentions, mostly hyperlinks only appear\nwhen this entity appears for the first time. There-\nfore, we first collect all entities appeared in hy-\nperlinks as well as their aliases stored in WikiData,\nand then search any mentions that have any of these\nalias and link it to the corresponding entity.\nHyperparameters In this work, we don’t have\ntoo much hyperparmaters to be tuned, as most pa-\nrameters as well as optimizing setting of LM is\nfixed. Our random walk part is non-parametric.\nThe only tunable hyperparamter is hidden dimen-\nsion size. We simply choose one setting, which\nis 128 for entity embedding, and 768 for relation\nembedding. The former is because entity is super\nlarge (over 5M), so we use a reletively smaller di-\nmension size. Detailed statistics about wikidata\nmemory is in Table 1.\nB Dataset Details\nBelow shows details for each dataset, and the\ndetailed dataset split is shown in Figure 5\nNatural Questions (Kwiatkowski et al., 2019)\ncontains questions from Google search queries, and\nthe answers are text spans in Wikipedia. We report\nshort answer Exact Match (EM) performance. The\nopen version of this dataset is obtained by discard-\ning answers with more than 5 tokens.\nWebQuestions (WQ) (Berant et al., 2013) con-\ntains questions from Google Suggest API, and the\nanswers are entities in Freebase.\nTriviaQA (Joshi et al., 2017) contains trivia ques-\ntions and answers are text spans from the Web. We\nreport Exact Match (EM) performance. We use its\nunfiltered version for evaluation.\nHotpotQA (Yang et al., 2018) is a multi-hop QA\ndataset. There are two evaluation settings. In the\ndistractor setting, 10 candidate paragraphs are pro-\nvided for each question, of which there are two\ngolden paragraphs. In the full-wiki setting, a model\nis required to extract paragraphs from the entire\nWikipedia. We report Exact Match (EM) on full-\nwiki setting.\nComplex WebQuestions (Talmor and Berant,\n2018) is a dataset that composite simple one-hot\nquestions in WebQuestionsSP by extending enti-\nties or adding constraints, so that each question\neequires complex reasoning to solve.\nWebQuestionsSP (Yih et al., 2015) is annotated\ndataset from WebQuestions, such taht each quet-\nsion is answerable using Freebase via a SQL query.\nC Discussion with Previous Works\nCompare with FILM Though FILM has the ad-\nvantage of end-to-end training and easily modifi-\ncation of knowledge memory, it simply stacks KG\nmodule on top of LM without interaction, and can\nonly handle one-hop relational query that is an-\nswerable by KG. Our approach, OREO LM, follows\nthe same memory idea by encoding KGinto LM\nparameter, and we desire LM and KGreasoning\nmodule could interact and collaboratively improve\neach other.\nNotably, OREO LM with T = 1shares a similar\ndesign with FILM. The major differences are: 1)\nthey store every triplet as a key-value pair, while\nwe explicitly keep the KGadjacency matrix and\nconduct a random walk, which has smaller search\nspace and is more controllable. 2) They add the\nmemory on top of LM, and thus the knowledge\ncould not help language understanding, and FILM\ncould mainly help wikipedia-answerable questions.\nInstead, we insert the KIL layer amid LM layers\nto encourage interaction, and thus the model could\nalso benefit encoder-decoder model (as shown\nabove).\nCompare with Previous Path-Based Reasoning\nand Retrieval Pre-Training Note that as our def-\ninition of entity state πi and relation action γi are\nboth continuous probabilistic vector, the wholeKG\nReasoning is fully differentiable and thus could be\nintegrated into LM seamlessly and trained end-to-\nend. This is different from previous path traversal\nworks such as DeepPath (Xiong et al., 2017) and\nMINERV A (Das et al., 2018), which defines state\n9575\nDataset Train Dev Test\nNatural Questions 58880 8757, 3610\nTrivia QA 60413 8837 11313\nWeb Questions 2474 361 2032\nComplex WebQ 27623 3518 3531\nWebQ-SP (Wiki-answerable) 1388 153 841\nFreebaseQA (Wiki-answerable) 12535 2464 2440\nTable 5: Dataset Train/Valid/Test splits.\nModels #param (B) WQ-SP TQA\nRoBERTa (Base) 0.12 47.5 40.3\n+ OREOLM(T=1) 0.12 + 0.6889.7 61.4\n+ OREOLM(T=2) 0.13 + 0.6892.4 66.8\nTable 6: Closed-Book Entity Prediction valida-\ntion performance of Encoder RoBERTa on WikiData-\nAnswerable Dataset.\nand action as discrete and could only be trained\nvia reinforcement learning rewards. The reasoner\ntraining is also different from passage retrieval pre-\ntraining (Guu et al., 2020; Sachan et al., 2021a),\nas the passage are naturally consisted of discrete\ntokens, and thus the reader is still required to re-\nencode the question with each passage, and differ-\nent objectives are required to train retriever and\nreader separately.\nDiscussion of Graph Walking-based Reasoning\nvs Graph Neural Networks Recently, Graph\nNeural Networks (GNNs) have shown superior\nperformance for structured representation learning.\nThere’s also a lot of works trying to use GNNs for\nQuestion Answering (Yasunaga et al., 2021; Zhang\net al., 2022). The one that has very similar moti-\nvation with us is GreaseLM. Therefore, a natural\nquestion is, whether could we use GNN instead\nof the non-parametric random walk module, for\nODQA?\nTo answer this question, let’s consider a simplest\nsetup of GNN. We could identify initial entities,\nconnected them via a k-hop subgraph, and encode\ngraph with text (Zhang et al., 2022) or indepen-\ndently (Yu et al., 2022b). When we want to retrieve\nknowledge from graph to LM, normally we just\ntake the contextualized node embedding as input\nfor knowledge fusion.\nIn this setup, say the answer is K-hop away\nfrom an initial entity, the ground-truth reasoning\npath is e0,r1,e1,r2,...,e k−1,rk,ek = a. Using\nour method, we first predict r1, transit to e1, and\nstep by step conduct reasoning via walking. How-\never, if we use GNN’s final embedding, it requires\nto pass information from neighbor to itself. There-\nfore, suppose we have aK-layer GNN, the first step\nshould be identify rk, and pass information from\nanswer ek = a to ek−1. This is conter-intuitive\nas we normally cannot assume to know the an-\nswer, nor knowling the last step to reach the an-\nswer. In situations where all candidate answer is\ngiven, like CommonSenseQA, where GreaseLM\nmainly works on, this problem is less harmful as\nit’s guaranteed to contain the answer in a restricted\nsmall graph. However, in open-domain setup, we\nneed to try best to narrow down the search space\nby following the forward reasoning instead of the\nbackward manner. Therefore, in this work we adopt\nwalking-based reasoning.\nD Illustration of Pre-Trained Data and\nReasoning Paths\nThe pre-training samples and reasoning paths\n(generated by T5-large on NQ dataset) is shown\nfrom Table 8-11.\n9576\nModels #param NQ WQ TQA ComplexWQ HotpotQA\nT5 (Large) 0.74B - - - - -\n+ OREO LM (T=2) 0.76B + 0.68 B 33.6 38.9 42.7 29.6 35.5\nTable 7: Closed-Book Generative QAvalidation performance of T5.\n9577\nTitle Masked Text Ground Truth Dependency Graph 2-Hop Graph\nPoolbeg the lighthouse was [mask] [s-\nent] [mask] [rel] [t-ent] com-\npleted in 1795. overview.\nthe [s-ent] poolbeg[rel] [t-ent]\n“peninsula” is home to a num-\nber of landmarks including\nthe [s-ent] [mask][rel] [t-ent]\n, the [s-ent] pool[mask] light-\nhouse[rel] [t-ent] , the [s-ent]\nirishtown nature park[rel] [t-\nent] , the southern part of [s-\nent] [mask][rel] [t-ent] ...\n[ ’ connected to land\nby the’, ’ great south\nwall’, ’ great south\nwall’, ’beg’, ’ dublin\nport’, \"’s main\npower station,\", ’\nstructures in’, ’48’, ’\na process to list the’,\n’ after the station’,\n’, including 3,’, ’\ndublin city council’,\n’ quarter” on the’]\nRylstone it is situated very\nnear to [s-ent]\n[mask][rel] [t-\nent] and about\n6 miles south\nwest[mask] [s-ent]\n[mask]ington[rel] [t-\nent] . the population\nof the [s-ent] civil\nparish[rel] [t-ent] as\nof the 2011 census\nwas 160. [s-ent]\nrylstone railway\nstation[rel] [t-ent]\nopened in 1902,\nclosed to passen-\ngers in 1930, and\nclosed completely in\n1969....\n[’ craven’, ’ cracoe’,\n’ of’, ’ grass’, ’\nthe inspiration for’,\n’ tour de france’,\n’stone’, ’ by will’...]\nKarpinsk ologist [s-ent]\n[mask] [rel] [t-ent]\n. history.[mask]the\nsettlement of bo-\ngoslovsk () was\nfounded in either\n1759 or in 1769. it\nremained one of\nthe largest [s-ent]\ncopper[rel] [t-ent]\nproduction cen-\nters in the [s-ent]\nurals[rel] [t-ent]\n[mask] [s-ent]\n[mask][rel] [t-ent]\ndeposits started to\nbe mined in 1911.....\n[’ alexander karpin-\nsky’, ’ until 1917.’,\n’ coal’, ’erman civil-\nians, who’, ’ and’, ’\nyears of’, ’ forest la-\nborers. moreover’, ’\nin’, ’ the’, ’ frame-\nwork of the’, ’ dis-\ntricts’, ’ karpinsk’,\n’insk’...]\n3 (The X-\nFiles)\n[s-ent]\n[mask][mask][rel]\n[t-ent] \". [s-ent]\ngillian ander-\nson[rel] [t-ent] is\nabsent[mask][mask]\nepisode as she was\non leave to give\nbirth to her daughter\npiper at the time.\nthis episode was\nthe first[mask] not\nappear. reception.\nratings. \"3\" pre-\nmiered on the [s-ent]\nfox network[rel]\n[t-ent] on, and was\nfirst broadcast in the\n[s-ent] united king-\ndom[rel] [t-ent].....\n[ ’ny had’, ’\nepisode’, ’born\nagain’, ’ from the’, ’\nin which scully did’,\n’. it was’, ’egall’,\n’ metacritic’, ’ as\n\"wretched’, ’ fact\nthat’, ’ background\nnoise for a’, ’ heavy-\nhanded attempts\nat’, ’ glen morgan’,\n’ doing an episode\non’]\nTable 8: Example of Pre-training data points (Part 1).\n9578\nTitle Masked Text Ground Truth Dependency Graph K-Hop Graph\nShen\nChun-\nshan\nhis memoirs, he\nsuffered his second\nstroke[mask][mask],\neven after his second\nstroke, he continued\nwriting; his series of\nbiographies of five\ngo masters [s-ent]\n[mask][mask][mask][rel]\n[t-ent] , [s-ent] mi-\nnoru kit[mask][rel]\n[t-ent] .....\n[’. however’, ’\ngo seigen’, ’ani’, ’\n2007, he’, ’ was hos-\npital’, ’ hsinchu’, ’af-\nter surgery’, ’ scale’,\n’ continuing to im-\nprove.’, ’ his coma.\nin’...]\n2007\nFlorida\nGators\nfootball\nteam\n[s-ent]\ntim[mask][mask][rel]\n[t-ent] completed 22\nof 27 passes for 281\nyards passing and\nalso ran for[mask]\nyards on 6 carries.\n[s-ent] [mask] [rel]\n[t-ent] carried the\nball 11 times for 113\nyards[mask] two\ntouchdowns and also\ncaught 9 passes for\n110[mask] receiving,\nbecoming the first\nplayer in school\nhistory .....\n[’ tebow’, ’ 35’, ’\npercy harvin’, ’ and’,\n’ yards’, ’ 30–9’, ’\nrenewed their bud-\nding’, ’ gamecocks’,\n’gator’, ’ quarter-\nback’, ’ set a career-\nhigh’, ’ of these five\nrushing’, ’.’, ’ percy\nharvin’, ’ sinus in-\nfection.’, ’ators’, ’\ntouchdown’]\nJudgment\nDay\n(Awe-\nsome\nComics)\n[s-ent] alan\nmoore[rel] [t-\nent] used \"judgment\nday\" to reject the\nviolent, deconstruc-\ntive clichés of 1990s\ncomics inadvertently\ncaused by his own\nwork on \" [s-ent]\nwatchmen[rel]\n[t-ent] \", \"\" and\n\" [s-ent] saga of\nthe[mask][mask][rel]\n[t-ent] \" and uphold\nthe values of classic\nsuperhero comics.\nthe series deals with\na metacommentary\nof the notion of ret-\ncons to super-hero\nhistories as [s-ent]\nalan moore[rel] [t-\nent] [mask] for the\ncharacters of [s-ent]\n[mask][mask][rel]\n[t-ent] , to replace\nthe shared universe\nthey left when [s-\nent] rob liefeld[rel]\n[t-ent] left image\nseveral years earlier.\nplot. in[mask],\nmick tombs/ [s-ent]\nknightsabre[rel]\n[t-ent].....\n[ ’ swamp thing’,\n’ himself creates a\nnew backstory’, ’\nawesome comics’,\n’ 1997’, ’riptide’,\n’ knightsabre ap-\npears to be’, ’ and\nsw’, ’ badrock’, ’\nsupreme’, ’by’, ’\nanalyzing’, ’ cyber-\nnetic young’, ’ it,\nand it has’, ’ue out’,\n’, administrator for\nyoungblood’]\nTable 9: Example of Pre-training data points (Part 2).\n9579\nQuestion Answer Reasoning Paths as Rationale\nsouthern soul was consid-\nered the sound of what in-\ndependent record label\n[’Motown’] soul music\ngenre-R\n−−−−→? label−−→?\nindependent record label\nbelong\n−−−→? is a-R−−−→?\nwho is the bad guy in lord\nof the rings\n[’Sauron’] the lord of the rings (film series) theme−−−→? characters−−−−−→?\nwhere was the mona lisa\nkept during ww2\n[’the Ingres\nMuseum’,\n\"Château\nd’Amboise\",\n’Château\nde Cham-\nbord’, ’the\nLoc - Dieu\nAbbey’]\nmona lisa creator−−−→? tomb−−→?\nworld war 2\ntake place\n−−−−−→? located-R−−−−−→?\nwho have won the world\ncup the most times\n[’Brazil’] fifa world cup\nparts\n−−→? land−−→?\nwho wrote the song the\nbeat goes on\n[’Sonny\nBono’]\nsong\nalbum type-R\n−−−−−−−→? author−−−→?\nwho plays mrs. potato\nhead in toy story\n[’Estelle Har-\nris’]\ntoy story series−−−→? VO−−→?\nwho plays caroline on the\nbold and beautiful\n[’Linsey\nGodfrey’]\nthe bold and the beautiful in work-R−−−−−→? actor−−→?\nwhere are the fruits of the\nspirit found in the bible\n[’Epistle to\nthe Gala-\ntians’]\nbible\nparts\n−−→?\nparts\n−−→?\nwho is the only kaurava\nwho survived the kuruk-\nshetra war\n[’Yuyutsu’] kaurava in work−−−−→? in work-R−−−−−→?\nKurukshetra War location−−−−→live in-R−−−−→\nwhat is the deepest depth\nin the oceans\n[’Mariana\nTrench’]\nocean in− →?\nlowest point\n−−−−−−→?\nwhere did the french na-\ntional anthem come from\n[’Strasbourg’] national anthem is a-R−−−→? released in−−−−−→?\nTable 10: Example of QA prediction with reasoning path on NQ (part 1).\n9580\nQuestion Answer Generated Reasoning Paths as Rationale\nwho sings the song where\nhave all the flowers gone\n[’Pete\nSeeger’]\nsong\nalbum type-R\n−−−−−−−→? actor−−→?\nwho discovered some is-\nlands in the bahamas in\n1492\n[’Christopher\nColumbus’]\nthe bahamas\nentry\n−−→?\nentry-R\n−−−−→?\nwhich type of wave re-\nquires a medium for trans-\nmission\n[’mechanical\nwaves’, ’heat\nenergy’,\n’Sound’]\nwave\nbelong-R\n−−−−−→?\nbelong-R\n−−−−−→?\nland conversion through\nburning of biomass re-\nleases which gas\n[’traces of\nmethane’,\n’carbon\nmonoxide’,\n’hydrogen’]\ngas\nbelong-R\n−−−−−→? as-R−−→?\nthe sum of the kinetic and\npotential energies of all\nparticles in the system is\ncalled the\n[’internal en-\nergy’]\nkinetic energy\nbelong\n−−−→?\nbelong-R\n−−−−−→?\npotential energy\nbelong\n−−−→?\nbelong-R\n−−−−−→?\nwho did seattle beat in the\nsuper bowl\n[’Denver\nBroncos’]\nsuper bowl\norganizer\n−−−−−→?\nleague-R\n− −−−− →?\nwhat is the name of the\ngirl romeo loved before\njuliet\n[’Rosaline’] romeo in work−−−−→? in work-R−−−−−→?\nwho will get relegated\nfrom the premier league\n2016/17\n[ ’Hull\nCity’, ’Sun-\nderland’,\n’Middles-\nbrough’]\npremier league\nleague-R\n− −−−− →? POB−−→?\nactress in the girl with the\ndragon tattoo swedish\n[’Noomi Ra-\npace’]\nsweden\nspeaking\n−−−−→?\nmother tongue-R\n−−−−−−−−−→?\nTable 11: Example of QA prediction with reasoning path on NQ (part 2).\n9581"
}