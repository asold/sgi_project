{
  "title": "Code Summarization with Structure-induced Transformer",
  "url": "https://openalex.org/W3115080412",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5112570819",
      "name": "Hongqiu Wu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5036050911",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A5108856873",
      "name": "Min Zhang",
      "affiliations": [
        "Soochow University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2023925487",
    "https://openalex.org/W3086449553",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2154415691",
    "https://openalex.org/W3034372013",
    "https://openalex.org/W2963371736",
    "https://openalex.org/W2887364112",
    "https://openalex.org/W2979271470",
    "https://openalex.org/W2888557792",
    "https://openalex.org/W2964645190",
    "https://openalex.org/W3034961030",
    "https://openalex.org/W2784777439",
    "https://openalex.org/W2963392741",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W2962995178",
    "https://openalex.org/W3177441850",
    "https://openalex.org/W3042954354",
    "https://openalex.org/W3099636232",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3014755496",
    "https://openalex.org/W2998665041",
    "https://openalex.org/W2034209539",
    "https://openalex.org/W2963661253",
    "https://openalex.org/W2971008324",
    "https://openalex.org/W3034716028",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2963015915",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W1992114977",
    "https://openalex.org/W3155713635",
    "https://openalex.org/W2516621648",
    "https://openalex.org/W2018844270",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W3034689979",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3008088841",
    "https://openalex.org/W2964268484",
    "https://openalex.org/W2136294701",
    "https://openalex.org/W2133333349",
    "https://openalex.org/W2133348086",
    "https://openalex.org/W3108074259",
    "https://openalex.org/W3162962341",
    "https://openalex.org/W2807964941",
    "https://openalex.org/W3091730360",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963499994",
    "https://openalex.org/W3101693329",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Code summarization (CS) is becoming a promising area in recent language understanding, which aims to generate sensible human language automatically for programming language in the format of source code, serving in the most convenience of programmer developing. It is well known that programming languages are highly structured. Thus previous works attempt to apply structure-based traversal (SBT) or non-sequential models like Tree-LSTM and graph neural network (GNN) to learn structural program semantics. However, it is surprising that incorporating SBT into advanced encoder like Transformer instead of LSTM has been shown no performance gain, which lets GNN become the only rest means modeling such necessary structural clue in source code. To release such inconvenience, we propose structure-induced Transformer, which encodes sequential code inputs with multi-view structural clues in terms of a newly-proposed structure-induced self-attention mechanism. Extensive experiments show that our proposed structure-induced Transformer helps achieve new state-of-the-art results on benchmarks.",
  "full_text": "Code Summarization with Structure-induced Transformer\nHongqiu Wu1,2,3, Hai Zhao1,2,3, ∗\n, Min Zhang4\n1 Department of Computer Science and Engineering, Shanghai Jiao Tong University\n2 Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University, Shanghai, China\n3 MoE Key Lab of Artiﬁcial Intelligence, AI Institute, Shanghai Jiao Tong University, Shanghai, China\n4 Institute of Artiﬁcial Intelligence, School of Computer Science and Technology, Soochow University, Suzhou, China\nwuhongqiu@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn, minzhang@suda.edu.cn\nAbstract\nCode summarization (CS) is becoming a\npromising area in recent language understand-\ning, which aims to generate sensible human\nlanguage automatically for programming lan-\nguage in the format of source code, serving\nin the most convenience of programmer de-\nveloping. It is well known that program-\nming languages are highly structured. Thus\nprevious works attempt to apply structure-\nbased traversal (SBT) or non-sequential mod-\nels like Tree-LSTM and graph neural net-\nwork (GNN) to learn structural program se-\nmantics. However, it is surprising that incorpo-\nrating SBT into advanced encoder like Trans-\nformer instead of LSTM has been shown no\nperformance gain, which lets GNN become\nthe only rest means modeling such necessary\nstructural clue in source code. To release\nsuch inconvenience, we propose structure-\ninduced Transformer, which encodes sequen-\ntial code inputs with multi-view structural\nclues in terms of a newly-proposed structure-\ninduced self-attention mechanism. Extensive\nexperiments show that our proposed structure-\ninduced Transformer helps achieve new state-\nof-the-art results on benchmarks.\n1 Introduction\nBy 2020, software development and maintenance\nbecome an indispensable part of human work and\nlife. Various assistant technical measures have been\ntaken to facilitate more enjoyable software devel-\nopment, among which it is especially welcomed by\nprogrammers when there is a code summarization\ntask generating sensible human language annota-\ntions automatically.\n∗Corresponding author. This paper was partially supported\nby National Key Research and Development Program of China\n(No. 2017YFB0304100), Key Projects of National Natu-\nral Science Foundation of China (U1836222 and 61733011),\nHuawei-SJTU long term AI project, Cutting-edge Machine\nReading Comprehension and Language Model. This work\nwas supported by Huawei Noah’s Ark Lab.\nCode\n(Java)\nprivate void attachPlot (SVGPlot newplot){\nthis.plot = newplot;\nif (newplot == null){\nsuper.setSVGDocument(null);\nreturn;\n}\nnewplot.synchronizeWith(synchronizer);\nsuper.setSVGDocument(\nnewplot.getDocument());\nsuper.setDisableInteractions(\nnewplot.getDisableInteractions());\n}\nSumm. Attach to a new plot and display.\nCode\n(Python)\ndef getchangelinesinﬁlefortag(tag,\nchangedict):\ncleanedlines = []\ndatalist = changedict.get(’data’, [])\nfor datadict in datalist:\nblock = datadict.get(’block’, ”)\nlines = block.split(’\\\\n’)\nfor line in lines:\nindex = line.ﬁnd(tag)\nif (index>(-1)):\nline = line[index:]\ncleanedlines.append(line)\nreturn cleanedlines\nSumm.\nThe received changedict is the jsoniﬁed version of\nthe changes to a ﬁle in a changeset being pushed to\nthe Tool Shed from the command line. This method\ncleans and returns appropriate lines for inspection.\nTable 1: Task samples of code summarization, where\nsumm. refers to the output summary.\nIn early days, code summarization was a deriva-\ntive problem of information retrieval (Haiduc et al.,\n2010; Eddy et al., 2013; Wong et al., 2013, 2015)\nby matching the most similar code snippets which\nare labeled with summaries. Such method lacks\ngeneralization and performs unsatisfactorily. Thus\nin recent years, researchers treated code summa-\nrization as a task of language generation (Iyer et al.,\n2016; Liang and Zhu, 2018), which usually de-\npends on RNN-based Seq2Seq models (Cho et al.,\n2014; Bahdanau et al., 2015).\nIt is already known that RNN-based models\nmay encounter bottleneck when modeling long\narXiv:2012.14710v2  [cs.CL]  1 Jun 2021\nStructure-\nsensitive\nLong-term\ndependency\nFeat-model\nmatch\nLSTM\nTree-LSTM ✓\nTransformer ✓\nLSTM + SBT ✓ ✓\nTransformer + SBT ✓ ✓\nSiT ✓ ✓ ✓\nTable 2: Comparison of the previous models with pro-\nposed SiT model. The last column refers to whether\ninput features match with the corresponding model.\nsequences due to its poor long-term dependency.\nFor instance, a normal snippet of Java as shown\nin Table 1 usually has hundreds of tokens. More\nrecently, Ahmad et al. (2020) used an enhanced\nTransformer-based model to capture long-term and\nnon-sequential information of source code, which\noutperformed previous RNN-based models by a\nlarge margin.\nOn the other hand, in the light of the structural\nnature of programming languages, structure clues\nare supposed to greatly enhance programming lan-\nguage processing task like code summarization\n(Fernandes et al., 2019). Indeed, substantial empir-\nical studies showed that Abstract Syntax Tree may\nhelp models better comprehend code snippets and\nachieve more sensible generation results. Previous\napproaches could be divided into two categories.\nThe ﬁrst is to employ non-sequential encoders (e.g.,\nTBCNN (Mou et al., 2016), Tree-LSTM (Shido\net al., 2019), Tree-Transformer (Harer et al., 2019),\nGraph Neural Network (Allamanis et al., 2018; Liu\net al., 2020; Alex et al., 2020; Wang et al., 2021))\nto directly model structural inputs. The other is\nto pre-process structural inputs to apply sequential\nmodels on them. Uri et al. (2019) used LSTM to\nencode code structure by sampling possible paths\nof AST. Another similar work is structure-based\ntraversal (SBT) (Hu et al., 2018a), which manages\nto ﬂatten ASTs into linear sequences.\nThough existing studies achieve success on the\nconcerned code summarization task more or less,\nthere is still room in improving both of the above\nmodeling approaches. It is well known RNN en-\ncoders like LSTM only have limited capabilities\nin capturing long-range dependencies in sequence,\nand GNN-like models may be too sensitive to local\ninformation, which casts a natural solution, what\nif incorporating SBT into the Transformer? How-\never, it is surprising that SBT only works effec-\ntively with LSTM but not the Transformer accord-\ning to Ahmad et al. (2020). We attribute this to the\nlinear and nonlinear inconsistence between SBT\nand encoder forms. SBT enables sequential en-\ncoders to learn non-sequential relationship (such\nas syntax) still in a certain elaborate linear forms.\nRNN may be effectively enhanced by SBT right\nbecause of its sequential architecture through at-\ntention mechanism. Transformer learns features\nthrough self-attention network (SAN), nevertheless\nwhich acts more like a non-sequential process. Con-\nsequently, such sequential features are unsuitable\nfor a non-sequential architecture to extract implicit\nstructural information. We boldly call it Feature-\nModel Match problem in Table 2. In this paper,\nwe thus design an improved Transformer variants,\nstructure-induced Transformer (SiT) to alleviate\nsuch difﬁculty in terms of a structure-induced self-\nattention mechanism, so that the resulted model\nmay enjoy both merits, capturing long-range de-\npendencies and more global information. The pro-\nposed model design has been applied to benchmark\ndatasets and helps achieve new state-of-the-art per-\nformance.\nA\nB C\nD E\nA\nB C\nD E\nFigure 1: Use of adjacency matrix to transform original\nself-attention, left-hand complete graph, into structure-\ninduced self-attention, right-hand graph which looks\nclear-cut. Note that we omit self-circles for concision.\n2 Structure-based Code Summarization\nThe following sections present our code summa-\nrization method with two parts, in which the ﬁrst is\nabout structure representation of code, and the sec-\nond is our proposed structure-induced Transformer.\n2.1 Structure Representation of Code\nNote that programming language like source code\nis subtle that certain different formats may result in\ndifferent compilations. Thus pre-processing could\nbe an great impact in code summarization.\nWe adopt Abstract Syntax Tree (AST) for rep-\nresenting the language grammar of source code as\nusual. Figure 2 depicts a typical AST, which is com-\nposed of terminal nodes and non-terminal nodes. A\nnon-terminal node represents certain construction\nlike If and BinaryOp, while terminal nodes repre-\nsent its semantic components, such as identiﬁers\nand numbers.\nIn model implementation, we adopt adjacency\nmatrix A to represent the AST instead of struc-\nture based traversal method as in Hu et al. (2018a),\nwhich represents tree structure in a sequential for-\nmat. Such choice is well compatible with Trans-\nformer, which calculates attention weights by per-\nforming a dot-product of key-query pairs and re-\nsults in an attention matrix of l×l. We let lequal\nto number of AST nodes, then code summariza-\ntion with Transformer becomes possible through\napplying a position-wise multiplication of Aand\noriginal attention matrix.\nsample()\nFunctionDef\n[]\nArguments\na = random()\nAssign\na\nName\nrandom()\nCall\nrandom\nName\nif a % 2 == 0\nIf\na % 2 == 0\nCompare\na % 2\nBinOp\na\nName\n%\nMod\n2\nNum\n0\nNum\nb = a + 1\nAssign\nb\nName\na + 1\nBinOp\na\nName\n+\nAdd\n1\nNum\nprint(b)\nExpression\nprint\nName\nb\nName\nprint(b)\nCall\n==\nEq\nGlobal attention\nTerminal\nAST\nFlow\nData dependency\ndef sample():\n    a = random()\n    if a % 2 == 0:\n        b = a + 1\n        print(b)\nSoure code\nFigure 2: A Python code sample of multi-view graph\nused in Si-SAN. The code snippet is referred from Liu\net al. (2020), which is original in Java.\nInspired by Code Property Graph (CPG) (Yam-\naguchi et al., 2014; Liu et al., 2020), we further\nexpand AST into a multi-view network (MVN or\nmulti-view graph) (Sindhwani et al., 2005; Zhou\nand Burges, 2007; Kumar et al., 2011). An MVN is\ncomposed of multiple views, each view correspond-\ning to a type of structural relationships while all\nviews sharing the same set of vertexes (Shi et al.,\n2018). In this paper, we construct a three-view\ngraph based on different code semantics, which are\nabstract syntax, control ﬂow and data dependency.\nWe show an example in Figure 2, where we use\ncolorful strokes to describe different compositions\nin the graph. Note that we only utilize terminal\nnodes which are marked as rounded rectangles.\nSpeciﬁcally, we ﬁrst generate an AST, on the\nbasis of which we add additional edges to further\nrepresent the ﬂow of control and data. For con-\ntrol ﬂow, since Transformer is order-sensitive with\nposition encoding, we only need to focus on each\nstatement node. For instance, nodes b, =, a, +, 1\nmake a complete statement b=a+1. We connect\neach of them since they are in the same execution\norder. For data dependencies, we connect relevant\ndata across the whole program, as the variable b in\nexpression print(b) and assignment b=a+1 respec-\ntively, where the former is deﬁned and loaded from\nthe latter.\nNow we may obtain three adjacency matrices of\nsyntax, ﬂow and dependency respectively, which\nare colored in red, yellow and blue in Figure 2. We\ncombine them together and ﬁnally obtain a multi-\nview graph. Additionally, we add global attention\non the root, which is allowed to attend to all tokens\nin the code, and all tokens in the code can attend to\nit. With aggregated structure, our structure-based\ncode summarization is expected to capture various\nsemantics of programs.\nNote that our multi-view graph is different from\nCPG. which is original for C/C++ only and we do\nnot ﬁnd an appropriate analysis platform for other\nlanguages.\n2.2 Structure-induced Transformer\nFollowed by appropriate structure representation\nand graph construction, we now propose our\nstructure-induced Transformer (SiT) for code sum-\nmarization, which is a structure-sensitive trans-\nformer (Zhang et al., 2020b; Narayan et al., 2020;\nXu et al., 2020) model and is able to compre-\nhend code snippets both semantically and syntac-\nKey\n×n\nSummary\n+\n+\nCode\nTransformer Decoder\nAttention\nAttention\nValue Query\nKeyQueryValue\nFigure 3: Overall architecture of Structure-induced Transformer (SiT).\ntically. Meanwhile, we do not introduce extra pa-\nrameters in SiT so that guarantee the training ef-\nﬁciency. In this section, we ﬁrst review the self-\nattention network (SAN) of Transformer in terms\nof attention graph. Then we correspondingly pro-\npose structure-induced self-attention to build the\nstructure-induced Transformer.\nVanilla Self-Attention Transformer is com-\nposed of stacks of identical layers for both encoder\nand decoder (Vaswani et al., 2017). Each layer\nemphasizes on self-attention mechanism, which is\ndenoted as:\nSAN(X) = Softmax\n(QKT\n√dk\n)\nV (1)\nwhere X = ( x1,...,x l) denotes the input se-\nquence of sub-words, ldenotes the sequence length\nand dk denotes the hidden size per head. Now we\nview each sub-word as a vertex nand inner prod-\nuct of each key-value pair as a directed edge e, the\nSAN can be described as a directed cyclic graph.\nEquation 1 can be rewritten as follow:\nSAN(X) = E·N (2)\nThe attention scores E = {eij}refers to a weight\nmatrix of edges where eij represents how signiﬁ-\ncant node ni attend to node nj, while value matrix\nN = {ni}refers to each node representation. Fig-\nure 1 depicts the process of calculating attention\nscores.\nNote that SAN actually generates a fully con-\nnected cyclic graph without consideration of the\nvery needed structure-aware representation for our\nconcerned task.\nStructure-induced Self-Attention To represent\nthe needed structure information, we propose\nstructure-induced self-attention network (Si-SAN).\nSpeciﬁcally, we introduce multi-view network\ninto Equation 1, that is, multiply the adjacency\nmatrix by key-query pairs:\nSiSAN(X) = Softmax\n(Amv ·QKT\n√dk\n)\nV\n(3)\nwhere Amv refers to the multi-view representation\nof code.\nNote that Si-SAN does not change the input code\nbut appropriately incorporate code structure into\nSAN by changing its attention pattern. As shown\nin Figure 1, when aij = 0 in Amv, the attention\nbetween ni and nj will be dropped out (Wu et al.,\n2021). We consequently obtain a more explicit\nattention graph. Different from calculating global\ninformation onto the whole sentence in original\nSAN, Si-SAN is expected to calculate structural\ninformation more accurately.\nStructure-induced Module To enhance robust-\nness and avoid over-pruning, we introduce\nstructure-induced module, which is a stack of two\nlayers, SAN and Si-SAN. In each module, SAN\nis followed by Si-SAN and the output is the com-\nbination of both layers. Speciﬁcally, given input\nsequence X = ( x1,...,x l), where l denotes se-\nquence length, we ﬁrst pass it through an SAN\nlayer to obtain hidden representation denoted as\nH = (h1,...,h l):\nH = Concat(SAN1(X),...,SAN h(X)) (4)\nwhere hrefers to number of heads of multi-head\nattention while SANi refers to self-attention of\nModel Java Python\nBLEU ROUGE-L METEOR BLEU ROUGE-L METEOR\nCODE-NN (Iyer et al., 2016) 27.60 41.10 12.61 17.36 37.81 09.29\nTree2Seq (Eriguchi et al., 2016)37.88 51.50 22.55 20.07 35.64 08.96\nHybrid2Seq (Wan et al., 2018)38.22 51.91 22.75 19.28 39.34 09.75\nDeepCom (Hu et al., 2018a) 39.75 52.67 23.06 20.78 37.35 09.98\nAPI + Code (Hu et al., 2018b)41.31 52.25 23.73 15.36 33.65 08.57\nDual Model (Wei et al., 2019)42.39 53.61 25.77 21.80 39.45 11.14\nTransformer (Ahmad et al., 2020)44.58 54.76 26.43 32.52 46.73 19.77\nTransformer∗(Ahmad et al., 2020)44.87 54.95 26.58 32.85 46.93 19.86\nSiT 45.76(↑1.18) 55.58(↑0.82) 27.58(↑1.15) 34.11(↑1.59) 48.35(↑1.62) 21.11(↑1.34)\nCodeBERT∗†(Feng et al., 2020)43.33 54.64 26.20 33.47 49.35 21.69\nSiT on CodeBERT† 45.19(↑0.61) 55.87(↑1.11) 27.52(↑1.09) 34.31(↑1.79) 49.71(↑2.98) 22.09(↑2.32)\nTable 3: BLEU, ROUGE-L and METEOR for our approach compared with other baselines. †refers to pre-trained\nmodels while ∗ refers to models we rerun. The results of upper part are directly reported from Ahmad et al.\n(2020). Note that we only rerun Transformer and CodeBERT since they are much stronger than the other baselines.\nHowever, our results are even stronger. We show the ranges compared to the Transformer in Ahmad et al. (2020).\n50000 100000 150000 200000 250000 300000\nSteps\n20\n25\n30\n35\n40\n45BLEU\nTransformer\nSiT\n(a) BLEU score with training steps on Java\n50000 100000 150000 200000 250000\nSteps\n20\n22\n24\n26\n28\n30\n32\n34BLEU\nTransformer\nSiT (b) BLEU score with training steps on Python\nFigure 4: Convergence between Transformer and SiT.\nhead i. Subsequently, we pass H through a Si-\nSAN layer to obtain H′= (h′\n1,...,h ′\nl):\nH′= Concat(SiSAN1(H),...,SiSAN h(H))\n(5)\nFinally, we use an aggregation to fuse Hand H′to\nobtain ﬁnal representation ¯H = ( ¯h1,..., ¯hl):\n¯H = Aggr(H,H′) (6)\nwhere the aggregation we use is simple position-\nwise sum. We explore that the structure-induced\nmodule is more robust and leads to a better per-\nformance. In each stack, model begins to learn\nglobal information with SAN, where all connec-\ntions are available. Subsequently, through Si-SAN,\nmodel is told which of the connections are useful\nand which should be shut down and thus avoid-\ning over-pruning. Note that SiT with 3 stacks of\nstructure-induced modules still consists of 6 en-\ncoder layers and 6 decoder layers, but only changes\nthe architecture between modules of Transformer,\nnot introducing any extra parameters.\nFigure 3 depicts the overall architecture of SiT.\nCompared to original Transformer, our SiT with\nSi-SAN encodes a more accurate relative represen-\ntation of code through pruning redundant connec-\ntions.\n2.3 SiT-based Code Summarization\nBased on our structure-induced Transformer (SiT),\nnow we specify our code summarization process.\nWe ﬁrst transform the input code into adjacency\nmatrices of multiple views and combine them\nthrough a weighted sum:\nAmv = αAast + βAfl + γAdp (7)\nwhere α,β,γ refer to the corresponding weight for\neach view. Then we pass code sequences and cor-\nresponding adjacency matrices into SiT encoder,\nwhich contains 3 Si-SAN layers. For decoder, we\napply original Transformer decoder with cross at-\ntention. Finally, the summarization of the input\ncode is generated through autoregressive decoding.\n3 Experiments\n3.1 Datasets and Pre-processing\nDatasets Our experiments are conducted on two\nbenchmarks of Java (Hu et al., 2018a) and Python\n(Wan et al., 2018), and for both we follow their\ntraining, test and development divisions.\nGraph Construction For Java code, we refer to\nthe method provided in (Hu et al., 2018a). They\nuse javalang module of Python to compile Java\nand fetch AST in a dictionary form. For Python\ncode, we generate trees by ourselves based on ast\nand asttokens modules. Finally, we write a script to\nresolve ASTs into multi-view adjacency matrices1,\nwhere we let α= β = γ = 1 for all experiments2.\nOut-Of-Vocabulary Code corpus in program-\nming language may have a much bigger vocab-\nulary than natural language, including vast oper-\nators and identiﬁers. We have to introduce vast\nout-of-vocabulary (OOV) tokens (usually replaced\nby ⟨UNK⟩) (Hu et al., 2018a) to keep it in a regular\nsize. To avoid OVV problem, we applyCamelCase\nand snake case tokenizers (Ahmad et al., 2020) to\nreduce code vocabulary and remove all extra nodes\nwhich do not correspond to speciﬁc tokens.\n3.2 Baselines\nWe take all three categories of state-of-the-art mod-\nels as our baselines for comparison.\nTransformer We refer to the enhanced Trans-\nformer in (Ahmad et al., 2020) which equipped\nwith copy attention (See et al., 2017) and relative\nposition encoding (RPE) (Shaw et al., 2018). For\nfair enough comparison, we run their model on\nour machine under the same environment with SiT.\nNote that we also utilize RPE in SiT because of its\nbetter capability in capturing long sequences, while\nwe do not utilize copy attention.\nLSTM This group includes all relevant LSTM\nmodels with sequential and non-sequential inputs\n(Iyer et al., 2016; Eriguchi et al., 2016; Wan et al.,\n2018; Hu et al., 2018a,b; Wei et al., 2019).\n1https://github.com/gingasan/astruc\n2We try to adjust the weights of three views, showing\nlittle performance variant, which suggests that self-attention\nnetwork itself may balance the relative signiﬁcance between\nthe three.\n(a) Full\n (b) Window\n(c) Random\n (d) Structure-induced\nFigure 5: Comparison of different types of self-\nattention pattern. (b) Window attention with w = 2 .\n(c) Random attention with r= 2.\nPre-trained Language Model We also compare\nour model with CodeBERT (Feng et al., 2020), a\npre-trained language model on both natural and\nprogramming languages. It is pre-trained over six\nprogramming languages with MLM (Devlin et al.,\n2019) and RTD (Clark et al., 2020).\n3.3 Training Details\nWe train our model on a single nVidia Titan RTX\nwith batch size in {32, 64}. The learning rate is\nin {3e-5, 5e-5}with warm-up rate of 0.06 and L2\nweight decay of 0.01. The maximum number of\nepochs is set to 150 for Transformer and 30 for\nCodeBERT. For validation, we simply use greedy\nsearch, while for evaluation, we use beam search\nwith beam size in {4, 5, 8 }and choose the best\nresult3.\n3.4 Main Results\nScores Table 3 shows the overall results on Java\nand Python benchmarks. The Transformer baseline\nis strong enough as it outperforms all the previ-\nous works by a signiﬁcant margin. However, our\nmodel is more powerful, further boosting Trans-\nformer with more than 1 BLEU points on Java and\nPython respectively and achieves new state-of-the-\nart results. Speciﬁcally, SiT achieves higher scores\n3https://github.com/gingasan/sit3\non Python, increasing by 1.59, 1.62 and 1.34 points\non BLEU, ROUGE-L and METEOR respectively.\nAccording to dataset statistics, Python contains 5\ntimes more unique code tokens than Java, which\nmakes it much more challenging. Thus the superi-\nority of SiT on Python tends to be notable. Even\nso, SiT still boosts Transformer by 1.18, 0.82 and\n1.15 points on BLEU, ROUGE-L and METEOR\nrespectively on Java.\nConvergence Moreover, Figure 4 shows the\ntrend of BLEU scores on development set over\ntraining steps. SiT achieves a much faster con-\nvergence rate than Transformer. For instance on\nPython dataset, SiT arrives the best performance of\nTransformer in about 100 epochs, while the latter\none still needs 50 more to ﬁnally achieve the opti-\nmal. Note that the running time of each epoch for\nboth models is the same. Such high convergence\nrate helps showcase the necessity of Si-SAN.\nPre-training On the other hand, we can see\nthat CodeBERT also achieves competitive results\non both Java and Python. However, SiT is still\nmore powerful on most metrics, which outper-\nforms CoderBERT by 2.15, 0.95 and 1.15 points\non BLEU, ROUGE-L and METEOR respectively\non Java. However, CodeBERT performs much bet-\nter on Python, which outperforms SiT by 1.00 and\n0.58 points on ROUGE-L and METEOR. Note that\nCodeBERT is much bigger in size than Transformer\nand SiT (see Appendix A).\nFor further veriﬁcation, we follow CodeBERT\nand conduct a RoBERTa-based (Liu et al., 2019)\nSiT to further ﬁne-tune on both Java and Python.\nAs shown in Table 3, pre-trained SiT obtains attrac-\ntive results, further improving CodeBERT on all\nthe metrics, which implies that our elaborate en-\ncoder design is still effective even under powerful\npre-training assistance.\n4 Ablation Study and Analysis\nThis section reports our ablation studies to valid our\nmodel on the dataset of Python-V24 (Barone and\nSennrich, 2017), in which we conduct standard and\nuniﬁed pre-processing for strict fair comparison.\n4.1 Si-SAN vs. SAN\nTo valid the effectiveness of Si-SAN, we gradually\nreplace SAN layers in original Transformer with\n4https://github.com/EdinburghNLP/code-docstring-\ncorpus/tree/master/V2\nModel Prop. BLEU ROUGE-L METEOR\nTransformer 0 47.42 57.28 29.62\nTransformer 50% 49.64 59.39 31.16\nTransformer100% 49.80 59.38 31.30\nSiT 50% 50.04 59.56 31.46\nTable 4: BLEU, ROUGE-L and METEOR for variant\nmodels with incremental proportions of Si-SAN.\nModel Attn. BLEU ROUGE-L METEOR\nTransformer Full 47.42 57.28 29.62\nTransformerWindow 49.28 58.80 30.90\nTransformerRandom 38.06 57.28 22.76\nTransformerStruc. 49.80 59.38 31.30\nTable 5: BLEU, ROUGE-L and METEOR for variant\nmodels with different attention patterns.\nSi-SAN. Take Transformer model with Si-SAN\nproportion of 50% as an instance, we replace the\nsecond, fourth and last three encoder layers with Si-\nSAN and do not apply structure-induced module.\nThe results of variant models with incremental\nproportions of Si-SAN layers are shown in Table 4.\nIntuitively, all of the Transformers obtain improve-\nments when equipped with Si-SAN layers. We can\nalso see that SiT outperforms Transformer with\nsimilar proportion of Si-SAN, which proves the ef-\nfectiveness of structure-induced module. However,\nit is surprising that Transformer with all 6 layers\nof Si-SAN still outperforms original Transformer\neven if it may be over-pruned.\n4.2 Si-SAN vs. Sparse SAN\nTo further valid our structure-based approach, we\ncompare the performance of structure-induced at-\ntention with other sparse attention patterns, win-\ndow attention in Longformer, ETC (Beltagy et al.,\n2020; Ainslie et al., 2020) and random attention in\nBigBird (Zaheer et al., 2020). We depict different\nattention patterns in Figure 5. The default sequence\nlength in SiT is 400, and then we set both wand r\nto 64 in window and random attention respectively.\nAs shown in Table 5, Transformer with arbitrary\nsparse attention can not bring improvement as Si-\nModel BLEU RE.-L MTR. SPEED\nTransformer 44.87 54.95 26.58 1.0x\nTransformer + SBT43.34 53.97 25.02 1.5x\nSiT-AST only 45.43 55.30 27.21 1.0x\nSiT 45.76 55.58 27.58 1.0x\nTable 6: Comparison of Si-SAN and SBT methods.\nBoth methods only leverage AST information.\nModel Para. BLEU RE.L MTR.\nTransformer-8 140M 47.42 57.28 29.62\nSiT-8 139M 50.04 59.56 31.46\nTransformer-12 244M 50.11 59.47 31.44\nSiT-12 242M 50.53 60.08 31.96\nTransformer-16 370M 50.43 59.80 31.75\nSiT-16 367M 50.97 60.51 32.35\nTransformer-ALBERT enc.124M 44.83 55.34 27.73\nSiT-ALBERT enc. 124M 49.31 58.46 30.83\nTable 7: BLEU, ROUGE-L and METEOR for variant\nmodels with different sizes, where RE.L and MTR. re-\nfer to ROUGE-L and METEOR respectively. Models\nlike SiT-12 refers to SiT with 12 heads.\nSAN, which refutes that SiT learns better through\ndenoising. Speciﬁcally, random attention seriously\ndeteriorates Transformer. It is surprising that win-\ndow attention achieves a better result than Vanilla\nTransformer. Intuitively, tree structures like AST\nare highly localized. That is why window atten-\ntion may show good performance. Nevertheless,\nTransformer with Si-SAN still outperforms window\nattention by 0.52 BLEU point.\n4.3 Si-SAN vs. SBT\nWe reproduce SBT method on Java (Hu et al.,\n2018a) and apply it on our Transformer. For fair\nenough comparison, we let β = γ = 0 and con-\nduct single-view SiT which only leverages AST\ninformation. As depicted in Figure 6, ﬂattening\nASTs into linear sequences does not result in im-\nprovement, which is consistent with Ahmad et al.\n(2020). However, we achieve substantial improve-\nment while incorporating AST into Transformer us-\ning Si-SAN, which indicates our improved model\ndesign is indeed effective.\nIn addition, the average length of the input code\nwill be much longer with SBT, which may intro-\nduce additional training cost. As shown in Figure 6,\nSiT is 1.5 times faster than Transformer with SBT.\n4.4 Large Model\nIt is known that for nearly all deep models, increas-\ning model size may cover quite much of model\nstructure design improvement. Thus, it is possible\nthat the improvement on base-size model may not\nwork on large-size one. To valid this, we compare\nSiTs with Transformers under larger scale. As we\ncan see pictorially in Table 7, with increasing pa-\nrameter scale, SiTs with 12 heads and 16 heads\nboth outperform the corresponding Transformers\nby 0.42 and 0.54 BLEU point respectively.\n4.5 Parameter Sharing\nRecently, parameter sharing on BERT (Devlin et al.,\n2019) has achieved promising results (Lan et al.,\n2020). Similar as ALBERT, we introduce cross-\nlayer parameter sharing in both Transformer and\nSiT, sharing all parameters in all encoder layers.\nNote that we train our models from scratch and\nkeep the decoder ﬁxed.\nAs shown in Table 7, SiT performs much bet-\nter on parameter sharing than Transformer does.\nWe believe that code summarization task highly\ndepends on structural information, and this is why\nSiT can still achieve good results with simply one\ngroup of encoder parameters while Transformer\nencounters a serious decline. On the other hand, it\nmakes possible for lite model, which may balance\nhigh efﬁciency and performance.\n5 Related Work\nRNN-based Approaches While numbers of\nworks (Haiduc et al., 2010; Eddy et al., 2013; Wong\net al., 2013, 2015; Zhang et al., 2020a) on code\nsummarization usually depended on information re-\ntrieval, most of the recent works tend to treat it as a\nmachine translation problem. Meanwhile attention\nmechanism is broadly used for better performance\non capturing long-range features. Allamanis et al.\n(2016) proposed a Convolution Neural Network\n(CNN) with copy attention, and more commonly,\nIyer et al. (2016); Liang and Zhu (2018) proposed\nto use Recurrent Neural Network (RNN) with atten-\ntion mechanism to summarize code snippets into\nnatural language. Hu et al. (2018b) introduced\nAPI knowledge from related tasks while Cai et al.\n(2020) introduced type information to assist train-\ning, which also gained promising results. Addition-\nally, reinforce learning (Wan et al., 2018) and dual\nlearning (Wei et al., 2019; Ye et al., 2020) are also\nshown effective to boost model performance.\nTransformer-based Approaches It is known\nthat RNN-based models may encounter bottleneck\nwhen modeling long code sequences. Ahmad et al.\n(2020) proposed an enhanced Transformer with\ncopy attention and relative position encoding while\nGupta (2020); Dowdell and Zhang (2020) proposed\nto use Transformer (Vaswani et al., 2017) and\nTransformer-XL (Dai et al., 2019), all of which\noutperformed previous RNN-based models by a\nlarge margin.\nStructure-based Approaches Recent works on\ncode summarization pay more and more attention\non structural information, which usually treats the\nsource code in form of its Abstract Syntax Tree\n(AST). Hu et al. (2018a); LeClair et al. (2019);\nUri et al. (2019) leveraged ﬂattened ASTs as in-\nputs and trained with LSTMs. Mou et al. (2016);\nBui et al. (2021a); Shido et al. (2019); Harer et al.\n(2019) proposed TBCNN, TreeCaps, Tree-LSTM\nand Tree-Transformer to directly encode tree-style\ninputs. Differ from modeling code with sequential\nmodels, Allamanis et al. (2018); Liu et al. (2020);\nAlex et al. (2020) treated AST as graph and applied\ngraph neural network, while Wang et al. (2021) ap-\nplied heterogeneous graph neural network to model\ndifferent types of nodes.\nPre-training Approaches Apart from training\nfrom scratch, CodeBERT (Feng et al., 2020) is\npre-trained on vast bimodal corpora with masked\nlanguage model (Devlin et al., 2019) and replaced\ntoken detection (Clark et al., 2020), and achieves\npowerful performances on downstream tasks. Nie\net al. (2020) intensiﬁed contextualized code rep-\nresentation through masked code fragment predic-\ntions while Bui et al. (2021b) incorporated struc-\ntural information using TBCNN. However, all of\nthem do not include generation-related objectives.\nIt is worth further exploration and practice on pre-\ntraining approaches for out concerned tasks.\n6 Conclusion\nThis paper presents a novel structured-induced\nTransformer model on code summarization task.\nBy well-designed architecture, the proposed model\nmay effectively incorporate multi-view structure\ninto attention mechanism without tricky imple-\nmentation. We further adopt a new module ar-\nchitecture to aggregate both global self-attention\nand structure-induced self-attention representa-\ntions. Experiments on two challenging benchmarks\nincluding Java and Python show that the proposed\nmodel yields new state-of-the-art results.\nReferences\nWasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and\nKai-Wei Chang. 2020. A transformer-based ap-\nproach for source code summarization. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 4998–5007,\nOnline. Association for Computational Linguistics.\nJoshua Ainslie, Santiago Onta˜n´on, Chris Alberti, Philip\nPham, Anirudh Ravula, and Sumit Sanghai. 2020.\nETC: encoding long and structured data in trans-\nformers. CoRR, abs/2004.08483.\nLeClair Alex, Haque Sakib, Wu Lingfei, and McMil-\nlan Collin. 2020. Improved code summarization via\na graph neural network. In 2020 IEEE/ACM Inter-\nnational Conference on Program Comprehension.\nMiltiadis Allamanis, Marc Brockschmidt, and Mah-\nmoud Khademi. 2018. Learning to represent pro-\ngrams with graphs. In International Conference on\nLearning Representations.\nMiltiadis Allamanis, Hao Peng, and Charles Sutton.\n2016. A convolutional attention network for ex-\ntreme summarization of source code. In Proceed-\nings of The 33rd International Conference on Ma-\nchine Learning, volume 48 of Proceedings of Ma-\nchine Learning Research , pages 2091–2100, New\nYork, New York, USA. PMLR.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nAntonio Valerio Miceli Barone and Rico Sennrich.\n2017. A parallel corpus of python functions and\ndocumentation strings for automated code documen-\ntation and code generation. In Proceedings of the\nEighth International Joint Conference on Natural\nLanguage Processing, IJCNLP 2017, Taipei, Taiwan,\nNovember 27 - December 1, 2017, Volume 2: Short\nPapers, pages 314–319. Asian Federation of Natural\nLanguage Processing.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\nCoRR, abs/2004.05150.\nNghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021a.\nTreecaps: Tree-based capsule networks for source\ncode processing. In Proceedings of the Thirty-Fifth\nAAAI Conference on Artiﬁcial Intelligence.\nNghi DQ Bui, Yijun Yu, and Lingxiao Jiang. 2021b.\nInfercode: Self-supervised learning of code repre-\nsentations by predicting subtrees. In Proceedings of\nthe 43rd International Conference on Software Engi-\nneering, ICSE 2021.\nRuichu Cai, Zhihao Liang, Boyan Xu, Zijian Li, Yuex-\ning Hao, and Yao Chen. 2020. TAG : Type auxil-\niary guiding for code comment generation. In Pro-\nceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics, ACL 2020, On-\nline, July 5-10, 2020 , pages 291–301. Association\nfor Computational Linguistics.\nKyunghyun Cho, Bart van Merrienboer, C ¸ aglar\nG¨ulc ¸ehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder-decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2014, October\n25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a\nSpecial Interest Group of the ACL, pages 1724–1734.\nACL.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a ﬁxed-length context. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n2978–2988. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nThomas Dowdell and Hongyu Zhang. 2020. Lan-\nguage modelling for source code with transformer-\nxl. CoRR, abs/2007.15813.\nBrian P Eddy, Jeffrey A Robinson, Nicholas A Kraft,\nand Jeffrey C Carver. 2013. Evaluating source code\nsummarization techniques: Replication and expan-\nsion. In 2013 21st International Conference on Pro-\ngram Comprehension (ICPC), pages 13–22. IEEE.\nAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa\nTsuruoka. 2016. Tree-to-sequence attentional neural\nmachine translation. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2016, August 7-12, 2016, Berlin,\nGermany, Volume 1: Long Papers. The Association\nfor Computer Linguistics.\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xi-\naocheng Feng, Ming Gong, Linjun Shou, Bing Qin,\nTing Liu, Daxin Jiang, et al. 2020. Codebert: A\npre-trained model for programming and natural lan-\nguages. arXiv preprint arXiv:2002.08155.\nPatrick Fernandes, Miltiadis Allamanis, and Marc\nBrockschmidt. 2019. Structured neural summariza-\ntion. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net.\nVivek Gupta. 2020. Deepsumm - deep code sum-\nmaries using neural transformer architecture. CoRR,\nabs/2004.00998.\nSonia Haiduc, Jairo Aponte, Laura Moreno, and An-\ndrian Marcus. 2010. On the use of automated text\nsummarization techniques for summarizing source\ncode. In 2010 17th Working Conference on Reverse\nEngineering, pages 35–44. IEEE.\nJacob Harer, Chris Reale, and Peter Chin. 2019. Tree-\ntransformer: A transformer-based method for cor-\nrection of tree-structured data. arXiv preprint\narXiv:1908.00449.\nXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin.\n2018a. Deep code comment generation. In 2018\nIEEE/ACM 26th International Conference on Pro-\ngram Comprehension (ICPC) , pages 200–20010.\nIEEE.\nXing Hu, Ge Li, Xin Xia, David Lo, Shuai Lu, and Zhi\nJin. 2018b. Summarizing source code with trans-\nferred api knowledge. In Proceedings of the Twenty-\nSeventh International Joint Conference on Artiﬁcial\nIntelligence, IJCAI-18 , pages 2269–2275. Interna-\ntional Joint Conferences on Artiﬁcial Intelligence\nOrganization.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2016. Summarizing source code\nusing a neural attention model. In Proceedings\nof the 54th Annual Meeting of the Association for\nComputational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, Volume 1: Long Papers. The\nAssociation for Computer Linguistics.\nAbhishek Kumar, Piyush Rai, and Hal Daum´e III. 2011.\nCo-regularized multi-view spectral clustering. In\nAdvances in Neural Information Processing Systems\n24: 25th Annual Conference on Neural Information\nProcessing Systems 2011. Proceedings of a meeting\nheld 12-14 December 2011, Granada, Spain , pages\n1413–1421.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAlexander LeClair, Siyuan Jiang, and Collin McMillan.\n2019. A neural model for generating natural lan-\nguage summaries of program subroutines. In Pro-\nceedings of the 41st International Conference on\nSoftware Engineering, ICSE 2019, Montreal, QC,\nCanada, May 25-31, 2019 , pages 795–806. IEEE /\nACM.\nYuding Liang and Kenny Qili Zhu. 2018. Auto-\nmatic generation of text descriptive comments for\ncode blocks. In Proceedings of the Thirty-Second\nAAAI Conference on Artiﬁcial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artiﬁcial In-\ntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artiﬁcial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 5229–5236. AAAI Press.\nShangqing Liu, Yu Chen, Xiaofei Xie, Jing Kai Siow,\nand Yang Liu. 2020. Automatic code summarization\nvia multi-dimensional semantic fusing in gnn. arXiv\npreprint arXiv:2006.05405.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nLili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin.\n2016. Convolutional neural networks over tree struc-\ntures for programming language processing. In Pro-\nceedings of the Thirtieth AAAI Conference on Arti-\nﬁcial Intelligence, February 12-17, 2016, Phoenix,\nArizona, USA, pages 1287–1293. AAAI Press.\nShashi Narayan, Joshua Maynez, Jakub Ad ´amek,\nDaniele Pighin, Blaz Bratanic, and Ryan T. McDon-\nald. 2020. Stepwise extractive summarization and\nplanning with structured transformers. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020 , pages 4143–4159. As-\nsociation for Computational Linguistics.\nLun Yiu Nie, Cuiyun Gao, Zhicong Zhong, Wai Lam,\nYang Liu, and Zenglin Xu. 2020. Contextualized\ncode representation learning for commit message\ngeneration. CoRR, abs/2007.06934.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1073–1083.\nAssociation for Computational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nYu Shi, Fangqiu Han, Xinran He, Carl Yang, Jie Luo,\nand Jiawei Han. 2018. mvn2vec: Preservation\nand collaboration in multi-view network embedding.\nCoRR, abs/1801.06597.\nYusuke Shido, Yasuaki Kobayashi, Akihiro Yamamoto,\nAtsushi Miyamoto, and Tadayuki Matsumura. 2019.\nAutomatic source code summarization with ex-\ntended tree-lstm. In 2019 International Joint Con-\nference on Neural Networks (IJCNN) , pages 1–8.\nIEEE.\nVikas Sindhwani, Partha Niyogi, and Mikhail Belkin.\n2005. A co-regularization approach to semi-\nsupervised learning with multiple views. In Pro-\nceedings of ICML workshop on learning with mul-\ntiple views, volume 2005, pages 74–79. Citeseer.\nAlon Uri, Brody Shaked, Levy Omer, and Yahav Eran.\n2019. code2seq: Generating sequences from struc-\ntured representations of code. In International Con-\nference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nYao Wan, Zhou Zhao, Min Yang, Guandong Xu,\nHaochao Ying, Jian Wu, and Philip S Yu. 2018. Im-\nproving automatic source code summarization via\ndeep reinforcement learning. In Proceedings of the\n33rd ACM/IEEE International Conference on Auto-\nmated Software Engineering, pages 397–407.\nWenhan Wang, Kechi Zhang, Ge Li, and Zhi Jin. 2021.\nLearning to represent programs with heterogeneous\ngraphs.\nBolin Wei, Ge Li, Xin Xia, Zhiyi Fu, and Zhi Jin. 2019.\nCode generation as a dual task of code summariza-\ntion. In Advances in Neural Information Processing\nSystems, pages 6563–6573.\nEdmund Wong, Taiyue Liu, and Lin Tan. 2015. Clo-\ncom: Mining existing source code for automatic\ncomment generation. In 2015 IEEE 22nd Interna-\ntional Conference on Software Analysis, Evolution,\nand Reengineering (SANER), pages 380–389. IEEE.\nEdmund Wong, Jinqiu Yang, and Lin Tan. 2013.\nAutocomment: Mining question and answer sites\nfor automatic comment generation. In 2013 28th\nIEEE/ACM International Conference on Automated\nSoftware Engineering (ASE), pages 562–567. IEEE.\nHongqiu Wu, Hai Zhao, and Min Zhang. 2021. Not all\nattention is all you need. CoRR, abs/2104.04692.\nJiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing Liu.\n2020. Discourse-aware neural extractive text sum-\nmarization. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5021–5031, Online. Association for Computa-\ntional Linguistics.\nFabian Yamaguchi, Nico Golde, Daniel Arp, and Kon-\nrad Rieck. 2014. Modeling and discovering vul-\nnerabilities with code property graphs. In 2014\nIEEE Symposium on Security and Privacy, SP 2014,\nBerkeley, CA, USA, May 18-21, 2014 , pages 590–\n604. IEEE Computer Society.\nWei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xi-\naoyin Wang, and Shikun Zhang. 2020. Leveraging\ncode generation to improve code retrieval and sum-\nmarization via dual learning. In WWW ’20: The Web\nConference 2020, Taipei, Taiwan, April 20-24, 2020,\npages 2309–2319. ACM / IW3C2.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nJian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,\nand Xudong Liu. 2020a. Retrieval-based neural\nsource code summarization. In Proceedings of the\nACM/IEEE 42nd International Conference on Soft-\nware Engineering, ICSE ’20, page 1385–1397, New\nYork, NY , USA. Association for Computing Machin-\nery.\nZhuosheng Zhang, Yuwei Wu, Junru Zhou, Sufeng\nDuan, Hai Zhao, and Rui Wang. 2020b. Sg-net:\nSyntax-guided machine reading comprehension. In\nThe Thirty-Fourth AAAI Conference on Artiﬁcial In-\ntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2020, The Tenth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2020, New York, NY, USA, February 7-12, 2020 ,\npages 9636–9643. AAAI Press.\nDengyong Zhou and Christopher J. C. Burges. 2007.\nSpectral clustering and transductive learning with\nmultiple views. In Machine Learning, Proceed-\nings of the Twenty-Fourth International Conference\n(ICML 2007), Corvallis, Oregon, USA, June 20-24,\n2007, volume 227 of ACM International Conference\nProceeding Series, pages 1159–1166. ACM.\nA Model Parameters\nModel dh dff h l\nTransformer 64 2048 8 12\nSiT 64 2048 8 12\nTransformer-Window 64 2048 8 12\nTransformer-Random 64 2048 8 12\nTransformer-Struc. 64 2048 8 12\nCodeBERT 64 3072 12 12\nSiT on CodeBERT 64 3072 12 12\nTransformer-ALBERT enc.64 2048 8 12\nSiT-ALBERT enc. 64 2048 8 12\nTable 8: Model parameters in our experiments.\nB Qualitative Samples\nFor qualitative analysis, we give some samples of\ncode summarization with different models. We\ncan see that SiT performs most precisely, while\nCodeBERT performs better than Transformer does.\nCode\nprivate ﬂoat computeOverscrollPercent (){if ( mOverScrollOffset>= NUM ){return mOverScrollOffset / mMaxOverScroll;}else{return mOverScrollOffset / mMaxUnderScroll;}}\nSummary\nGold: determine the current amount of overscroll . if the value is 0 , there is no overscroll . if the value is<0 , tabs areoverscrolling towards the top or or left . if the value is>0 , tabs are overscrolling towards the bottom or right .SiT: determine the current amount of overscroll . if the value is 0 , there is no overscroll . if the value is<0 , tabs areoverscrolling towards the top or or left . if the value is>0 , tabs are overscrolling towards the bottom or right .Transformer: determine the current amount of overscroll . if the value is 0 , there is no overscroll . if the value is<0 , tabsare overscrolling towards the top or or left . if the value is>0 , tabs are overscrolling towards the top or right .CodeBERT: determine the current amount of overscroll . if the value is 0 , there is no overscroll . if the value is<0 , tabsare overscrolling towards the top or or left . if the value is>0 , tabs are overscrolling towards the bottom or right .\nCode\npublic String peek (){String result = null;if (isEmpty()){return null;}else{int cachedCurrentIndex = currentIndex;if (isEatingBlocksOfDelimiters){trimStartingDelimiters();}int nearestDelimeter = -NUM ;for (int i =NUM; i<delimiters.length(); i++){int delimiter = source.indexOf(delimiters.charAt(i), currentIndex);if (nearestDelimeter == -NUM||delimiter != -NUM && delimiter<nearestDelimeter){nearestDelimeter = delimiter;}}if (nearestDelimeter == -NUM){result = source.substring(currentIndex);}else{result = source.substring(currentIndex, nearestDelimeter);}currentIndex = cachedCurrentIndex;}return result;}\nSummary\nGold: returns null if there is nothing left .SiT: returns null if there is nothing left .Transformer: ﬁnds the next unique identiﬁer .CodeBERT: returns the index of the ﬁrst delimited string removing from the current position .\nTable 9: Qualitative samples of Java code summarization.\nCode\ndefasFilesystemBytes(path, encoding=None):if (type(path) == bytes): return pathelse:if (encoding is None):encoding = sys.getﬁlesystemencoding()return path.encode(encoding)\nSumm.\nGold: return cpath as a string of lbytes suitable for use on this systems ﬁlesystem .SiT: return cpath as a string of lunicode suitable for use on this systems ﬁlesystem .Transformer: convert a ﬁlesystem path of a byte string .CodeBERT: return a byte string suitable for use in cpath as a byte string .\nCode\ndef absent(name, DomainName, region=None, key=None, keyid=None, proﬁle=None):ret ={’name’: DomainName, ’result’: True, ’comment’: ”, ’changes’:{}}r = salt[’botoelasticsearchdomain.exists’](DomainName, region=region, key=key, keyid=keyid, proﬁle=proﬁle)if (’error’ in r):ret[’result’] = Falseret[’comment’] = ’Failed to delete domain:{0}.’.format(r[’error’][’message’])return retif (r and (not r[’exists’])):ret[’comment’] = ’Domain{0}does not exist.’.format(DomainName)return retif opts[’test’]:ret[’comment’] = ’Domain{0}is set to be removed.’.format(DomainName)ret[’result’] = Nonereturn retr = salt[’botoelasticsearchdomain.delete’](DomainName, region=region, key=key, keyid=keyid, proﬁle=proﬁle)if (not r[’deleted’]):ret[’result’] = Falseret[’comment’] = ’Failed to delete domain:{0}.’.format(r[’error’][’message’])return retret[’changes’][’old’] ={’domain’: DomainName}ret[’changes’][’new’] ={’domain’: None}ret[’comment’] = ’Domain{0}deleted.’.format(DomainName)return ret\nSumm.\nGold: ensure domain with passed properties is absent .SiT: ensure domain with passed properties is absent .Transformer: ensure the iam role exists .CodeBERT: ensure the named domain is absent .\nTable 10: Qualitative samples of Python code summarization.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.836586594581604
    },
    {
      "name": "Automatic summarization",
      "score": 0.7081965804100037
    },
    {
      "name": "Encoder",
      "score": 0.6581027507781982
    },
    {
      "name": "Programming language",
      "score": 0.6118898391723633
    },
    {
      "name": "Transformer",
      "score": 0.5769898891448975
    },
    {
      "name": "Programmer",
      "score": 0.572041392326355
    },
    {
      "name": "Source code",
      "score": 0.5650829076766968
    },
    {
      "name": "Tree traversal",
      "score": 0.47100841999053955
    },
    {
      "name": "Tree structure",
      "score": 0.4325888454914093
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4117882251739502
    },
    {
      "name": "Natural language processing",
      "score": 0.3526275157928467
    },
    {
      "name": "Theoretical computer science",
      "score": 0.34754812717437744
    },
    {
      "name": "Data structure",
      "score": 0.2533578872680664
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}