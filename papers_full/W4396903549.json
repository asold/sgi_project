{
  "title": "RNA secondary structure prediction using transformer-based deep learning models",
  "url": "https://openalex.org/W4396903549",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2494389126",
      "name": "Yanlin Zhou",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2112082607",
      "name": "Tong Zhan",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2103126550",
      "name": "Yichao Wu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2013710843",
      "name": "Bo Song",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2117304989",
      "name": "Chenxi Shi",
      "affiliations": [
        "Universidad del Noreste",
        "Software (Spain)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6606671977",
    "https://openalex.org/W2098571862",
    "https://openalex.org/W4392223676",
    "https://openalex.org/W4392489953",
    "https://openalex.org/W6650040718",
    "https://openalex.org/W1585306636",
    "https://openalex.org/W2554722417",
    "https://openalex.org/W4392271034",
    "https://openalex.org/W4392271008",
    "https://openalex.org/W4392224100",
    "https://openalex.org/W4392012745",
    "https://openalex.org/W4399768997",
    "https://openalex.org/W4401241242",
    "https://openalex.org/W4399159256",
    "https://openalex.org/W1998618859",
    "https://openalex.org/W166206240",
    "https://openalex.org/W4399610836",
    "https://openalex.org/W4399769862",
    "https://openalex.org/W4399769716"
  ],
  "abstract": "The Human Genome Project has led to an exponential increase in data related to the sequence, structure, and function of biomolecules. Bioinformatics is an interdisciplinary research field that primarily uses computational methods to analyze large amounts of biological macromolecule data. Its goal is to discover hidden biological patterns and related information. Furthermore, analysing additional relevant information can enhance the study of biological operating mechanisms. This paper discusses the fundamental concepts of RNA, RNA secondary structure, and its prediction.Subsequently, the application of machine learning technologies in predicting the structure of biological macromolecules is explored. This chapter describes the relevant knowledge of algorithms and computational complexity and presents a RNA tertiary structure prediction algorithm based on ResNet. To address the issue of the current scoring function's unsuitability for long RNA, a scoring model based on ResNet is proposed, and a structure prediction algorithm is designed. The chapter concludes by presenting some open and interesting challenges in the field of RNA tertiary structure prediction.",
  "full_text": " \n \nRNA secondary structure prediction using transformer-based \ndeep learning models \nYanlin Zhou1a,*, Tong Zhan1b,5, Yichao Wu2,6, Bo Song3,7, Chenxi Shi4,8 \n1aComputer Science, Johns Hopkins University, Baltimore, USA \n1bComputer Science, Columbia University, NY, USA \n2Computer Science, Northeastern University, Boston, MA, USA \n3Computer Science, Northeastern University, Boston, MA, USA \n4Softwardevelopment, Telecommunicationon Systems Management, Northeastern \nUniversity, Boston, MA, USA \n*Corresponding author: popojoyzhou@gmail.com \n5tz2483@columbia.edu \n6wu.yicha@northeastern.edu \n7song.bo1@northeastern.edu \n8shi.che@northeastern.edu \nAbstract. The Human Genome Project has led to an exponential increase in data related to the \nsequence, structure, and function of biomolecules. Bioinformatics is an interdisciplinary research \nfield that primarily uses computational methods to analyze large amounts of biological \nmacromolecule data. Its goal is to discover hidden biological patterns  and related information. \nFurthermore, analysing additional relevant information can enhance the study of biological \noperating mechanisms. This paper discusses the fundamental concepts of RNA, RNA secondary \nstructure, and its prediction.Subsequently, the application of machine learning technologies in \npredicting the structure of biological macromolecules is explored. This chapter describes the \nrelevant knowledge of algorithms  and computational complexity and presents a RNA tertiary \nstructure prediction algorithm based on ResNet. To address the issue of the current scoring \nfunction's unsuitability for long RNA, a scoring model based on ResNet is proposed, and a \nstructure prediction algorithm is designed. The chapter concludes by presenting some open and \ninteresting challenges in the field of RNA tertiary structure prediction. \nKeywords: Gene prediction, RNA secondary structure , Bioengineering, Artificial intelligence, \nMachine learning. \n1.  Introduction \nThe computational analysis of RNA sequences is a crucial step in the field of RNA biology. In May \n2023, Briefings in Bioinformatics published a review article highlighting recent trends in predicting \nRNA secondary structure, RNA aptamers, and RNA drug discovery using machine learning, deep \nlearning, and related techniques, and discussing potentia l future pathways in the field of RNA \ninformatics. Currently, popular RNA tertiary structure prediction algorithms include knowledge -based \nRNA tertiary structure pr ediction algorithm and physics -based RNA tertiary structure prediction \nProceedings of the 6th International Conference on Computing and Data Science \nDOI: 10.54254/2755-2721/64/20241362 \nÂ© 2024 The Authors. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0 (https://creativecommons.org/licenses/by/4.0/). \n87 \n \n \nalgorithm. Each of these two types of prediction algorithms has its advantages and disadvantages, but \nneither of them can achieve high -precision and high -integrity RNA tertiary structure modeling. \nTherefore, the research direction of this paper is to further optimize and improve the relevant prediction \nalgorithm. Deep learning has matured and has achieved great success in many fields such as computer \nvision [ 1] and natural language processing [ 2]. However, deep learning methods that have been \nsuccessful in many fields face challenges in predicting RNA secondary structure. At present, there are \nno high precision, low data dependence and high convenience RNA secondary structure prediction \nmodels. Despite the emergence of efficient and low -cost calculation meth ods, the accuracy of this \ncalculation method is not satisfactory. Many machine learning methods have also been applied to this \narea, however, the accuracy rate has not improved significantly. Therefore, more innovative and \neffective methods are still needed to improve the accuracy and efficiency of RNA secondary structure \nprediction. \n2.  Related work \n2.1.  RNA structure prediction \nRNA consists of long chains of molecules, typically four bases connected by phosphodiester bonds. \n[3]Hydrogen bonds can also form between bases, creating a pair of bases. These pairs can be classified \nas either normative or non -standard. \"Canonical pairing\" refers to A base pair in an RNA or DNA \nmolecule where a stable hydrogen bond is formed between ade nine (A) and uracil (U) and between \nguanine (G) and cytosine (C). These are common forms of pairing in biology. The term \"irregular \npairing\" refers to any b ase pairing other than AU, GC, and GU. These ways of pairing may be less \ncommon, but still play a role in RNA or DNA molecules in some cases.  RNA molecules exhibit a \nquaternary structure, comprising a single strand with base pairs forming the primary structure. The \nsecondary structure consists of various elements such as hairpin loops, stems, internal loops, and \npseudoknots, which emerge as a result of the folding process. [4]These secondary structure motifs play \ncrucial roles in RNA function and regulation. Additionally, the tertiary structure of RNA involves further \nspatial bending of the helix based on the secondary structure, contributing to the molecule's overall \nthree-dimensional conformation. Furthermore, the quaternary structure of RNA involves compl ex \ninteractions between RNA molecules and proteins, forming intricate complexes essential for various \ncellular processes. \n2.2.  Deep learning and RNA secondary structure prediction \nRecent advancements in deep learning have revolutionized the prediction of RNA and protein structures. \nThese models, like the optimised differentiable model, bridge local and global protein structures by \noptimizing global geometry while adhering to local c ovalent chemistry principles, enabling accurate \nprediction of protein folding structures without prior co -evolution data. In RNA structure prediction, \nalgorithms like [5]FARFAR2 and scoring systems like ARES, based on geometric deep learning, have \ndemonstrated success in blind tests like RNA-Puzzles experiment. Inspired by AlphaFold2's success in \nprotein structure prediction, new approaches like [ 6]DeepFoldRNA, RoseTTAFoldNA, and RhoFold \nhave emerged. AlphaFold's neural network -based algorithm predicts base pair distances, optimized \nthrough gradient descent, enabling the generation of protein structures without complex sampling \nprocedures. In comparison to fully connected networks, convolutional neural networks are capable of \nperforming operations such as sp atial translation and rotation. This not only preserves the internal \ncorrelation of the data but also reduces the relevant parameters in the network model. The convolutional \nstructure effectively reduces the probability of overfitting the model. \nThe ResNet residual unit can be expressed as: \n ð‘¦ð‘™ = â„Ž(ð‘¥ð‘™) + ð¹(ð‘¥ð‘™, ð‘Šð‘™) ð‘¥ð‘™+1 = ð‘“(ð‘¦1) â„Ž(ð‘¥1) = ð‘¥ð‘™  (10 \nProceedings of the 6th International Conference on Computing and Data Science \nDOI: 10.54254/2755-2721/64/20241362 \n88 \n \n \nWhere l represents the L-th residual unit, and xl and xl+1 represent its input and output, respectively. \nF() represents the residual function, f( ) represents the Relu activation function, and there are many types \nof Relu functions. \nThe main focus of algorithm analysis is to assess its correctness and complexity. [ 7]Correctness is \nthe fundamental criterion for evaluating an algorithm, which is achieved when the algorithm produces \nthe correct output after a series of limited and clear instructions when given an example problem. \nComplexity analysis is another crucial f actor in evaluating algorithm performance. Algorithm \ncomplexity analysis typically involves evaluating the space -time complexity of algorithms. Time \ncomplexity refers to th e total number of times the algorithm's basic operation is executed during its \nexecution, while space complexity refers to the amount of memory required during implementation. \n2.3.  Transformer automatically predicts RNA structure \nThe emergence of the Transformer neural network, driven by an attention mechanism, has \nrevolutionized structural biology. In 2020, DeepMind's AlphaFold2 marked a significant breakthrough \nby accurately predicting the three -dimensional structure of proteins from their amino acid sequences. \nThis framework, powered by a Transformer neural network, excels in capturing long -range \ndependencies within input sequences, going beyond their sequential neighborhood. Subsequent \nframeworks such as [ 8]RoseTTAFold and Omega Fold have further advanced protein structure \nprediction, building upon the success of AlphaFold2.  The Transformer model employs probabilistic \nautoencoders and [9]ELBO optimization to maximize marginal likelihood. Initially, the input sequence \nis encoded to generate vectors representing it. These vectors undergo processing using the Probabilistic \nTransformer model to produce a probability distribution representing the likelihood of each target tag \ngiven the input. During inference, the prediction model can yield  varying results depending on the \nsample taken. In practice, the Transformer operates as an Encoder-Decoder architecture, with the middle \nportion divided into encoding and decoding components.  The Transformer architecture revolutionized \nnatural language processing (NLP) tasks by introducing a self -attention mechanism, enabling it to \ncapture long-range dependencies in sequences efficiently. In practice, the Transformer architecture can \nindeed be viewed as an Encoder-Decoder architecture, consisting of two main components: the encoder \nand the decoder. By dividing the Transformer architecture into these two components, the model can \neffectively handle various sequence-to-sequence tasks, such as machine translation, text summarization, \nand question answering. The encoder learns to encode the input sequenc e into a fixed -length \nrepresentation, capturing its semantic meaning, while the decoder uses this representation to generate \nthe output sequence. This modular design has proven to be highly effective and flexible for a wide range \nof NLP tasks. \n \nFigure 1. Transformer model (Encoder-Decoder architecture pattern) \nThe encoding component of the Transformer architecture consists of a multi -layer Encoder, which \nin this paper employs six layers. Each encoder layer contains two sub-layers(figure 1): a Self-Attention \nlayer and a Position -wise Feed Forward Network (FFN)[1 0]. In the Self -Attention layer, the encoder \ncan leverage information from other words in the input sentence to encode a specific word, allowing it \nto focus not only on the current word but also on contextual information from surrounding words. The \noutput from the Self-Attention layer is then passed to the feedforward network for further processing.  \nProceedings of the 6th International Conference on Computing and Data Science \nDOI: 10.54254/2755-2721/64/20241362 \n89 \n \n \nSimilarly, the decoding component of the Transformer architecture also consists of decoders with six \nlayers, each containing Self -Attention and FFN sub -layers. Additionally, there is an Attention layer \n(encoder-decoder Attention) between these sub-layers, enabling the decoder to focus on relevant parts \nof the input sentence, akin to the attention mechanism in seq2seq models. \nThe Transformer architecture's suitability for addressing challenges in predicting RNA structures \nstems from two key features. Firstly, it can accurately model long -term dependencies in sequence data \nby incorporating positional encoding into the input sequence. This allows the model to capture remote \ndependencies between input features without interference from intervening features. Secondly, the \nTransformer architecture is adept at modeling unordered sets of entities and their interactions, which is \nchallenging for many other deep learning architectures. This is achieved by conducting most operations \nin a positional manner, enabling the model to handle unordered sets of features effectively. These \nadvantages make the Transformer architecture an attractive choice for quantitative modeling of histone \ncodes, as it enables researchers to simultaneously consider multiple remote regulatory regions near the \ntranscription start site (TSS) in the wider genomic window. \n3.  Experiment and Methodology \ntrRosettaRNA, a tool for predicting RNA structure, has two main steps. First, it uses a technique called \ntransformer networks to predict the one - and two-dimensional geometries of RNA. It then minimizes \nthe energy to convert these geometries into the three-dimensional structure of RNA. By benchmarking \nits performance, trRosettaRNA was found to work better than traditional automated methods. In some \nblind tests, trRosettaRNA's predictions matched the top predictions of human experts. As a result, \ntrRosettaRNA also outp erforms other deep learning -based methods in many structural prediction \ncompetitions. \n3.1.  Experimental data set \nThis paper proposes the use of a Transformer network as an automatic sequence prediction model to \npredict RNA nucleotide sequences. To systematically generate different nucleic acid secondary \nstructures, traditional neural network rMSA and SPOT -RNA program are employed. The generated \nsecondary structure is then converted into a model, and the MSA representation and pair representation \nare modified. The resulting initial transformer network, called RNAformer, is used to predict 1D and \n2D geometry. At the core of these steps is the ability to translate the geometry of the generated structural \nmodel into constraints to guide the final step in the folding of the gene structure based on energy \nminimization. \n3.2.  Data processing procedure \nThe complete process of transformer networks in RNA secondary structure prediction involves three \nmain steps. The first is the input data preparation phase. Second, the final MSA is selected by running \nthe Infernal program against the smaller RNAcentral database and based on the quality of the predicted \ndistance graph. At the same time, SPOT-RNA was used to predict the secondary structure of RNA. The \nnext step is to predict one - and two-dimensional geometries. In this step, a transformer network called \nRNAformer is used, similar to the Evoformer network in AlphaFold2. The RNAformer network first \nconverts the input MSA and secondary structure into two representations: the MSA representation and \nthe pair representation. Each RNAformer block then updates these  two representations through four \nsteps: (1) MSA to MSA, (2) MSA pairing, (3) pairs of two, and (4) pairing to MSA. In single -channel \nRNAformer, 48 blocks are looped 4 times in a complete inference, and finally the two -dimensional \ngeometric prediction probability distribution is obtained by linear layer and softmax operation. The final \nstep is to generate the all -atomic structure model. This process may involve parsing, optimizing, and \nvalidating the predicted results to obtain a final high-quality structural model. \nSimilar to trRosetta, trRosettaRNA uses deep learning potentials and physics-based energy terms in \nRosetta to generate a complete model of atomic structure by minimizing the energy defined below: \n E = Ï‰1Edist + Ï‰2Eori + Ï‰3Econt + Ï‰4Eros   (1) \nProceedings of the 6th International Conference on Computing and Data Science \nDOI: 10.54254/2755-2721/64/20241362 \n90 \n \n \n ð¸ð‘œð‘Ÿð‘– = ð¸ð‘œð‘Ÿð‘–,2ð· +\nð¿\n2 ð¸ð‘œð‘Ÿð‘–,1ð·   (2) \nThe folding process in pyRosetta involves generating 20 all-atomic starting structures for each RNA \nusing RNA_HelixAssembler, followed by refinement through quasi -Newtonian optimization L-BFGS \nto minimize total energy. The total energy comprises constraints based on distance (Edist), dir ection \n(Eori), contact (Econt), and Rosetta's internal energy term. Constraints in 2D and 1D directions are \nrepresented by Eori,2D and Eori,1D, respectively. The weights (w1 = 1.03, w2 = 1.0, w3 = 1.05, w4 = \n0.05) are determined to minimize the average RMS D, based on hundreds of randomly selected RNAs \nfrom the training set. After refinement, 20 finely refined all -atomic structure models are obtained for \neach RNA, from which the model with the lowest total energy (Eq.1) is chosen as the final prediction. \n3.3.  Experimental results explain \n1.trRosettaRNA's performance on 30 individual RNAs \ntrRosettaRNA was tested against two other methods, RNAComposer and SimRNA, using 30 RNA \nstructures. The average deviation in structure prediction (RMSD) was 8.5 angstroms for trRosettaRNA, \ncompared to 17.4 angstroms for RNAComposer and 17.1 angstroms for S imRNA. This significant \ndifference highlights trRosettaRNA's superior performance in RNA structure prediction. \n \nFigure 2. Results of RNA2D structure prediction model \nThe results from the RNA2D structure prediction model, as depicted in Figure 2, highlight the \nsuperior performance of trRosettaRNA compared to traditional methods such as RNAComposer and \nSimRNA. In a dataset comprising 30 instances, trRosettaRNA outperformed RNAComposer 86.7% of \nthe time and SimRNA 96.7% of the time. Notably, 20% of the models generated by trRosettaRNA \nexhibited an RMSD (Root Mean Square Deviation) of less than 4 A, a level of accuracy that neither \nRNAComposer nor SimRNA could achieve. [11] These findings underscore the efficacy of \ntrRosettaRNA in RNA structure prediction, surpassing the capabilities of conventional methods. \nThe Das group was identified as the most accurate method, submitting models for 17 targets. It's \nworth noting that while some participating groups may leverage human expertise or literature data for \nguidance, trRosettaRNA's predictions are entirely automated and demonstrate comparable accuracy to \nthe top -performing human prediction group. Overall, these results highlight the effectiveness of \ntrRosettaRNA in addressing the challenges of RNA structure prediction and its potential to rival expert \nhuman predictions. \nProceedings of the 6th International Conference on Computing and Data Science \nDOI: 10.54254/2755-2721/64/20241362 \n91 \n \n \n2. Blind test of CASP15 \nIn the blind testing of CASP15, the researchers participated as part of the Yang-Server team, utilizing \nthe trRosettaRNA model as an automation server. They achieved a significant 9th position among 42 \nRNA structure prediction teams, including both human a nd server teams. Notably, within the server \nteams, Yang -Server ranked second, following UltraFold_Server. Furthermore, Yang -Server's \nperformance was enhanced when considering the cumulative Z -score (> 0.0) of RMSD, placing 5th \namong all groups and 1st amon g server groups. Remarkably, Yang -Server surpassed other deep \nlearning-based groups in terms of Z-scores for RMSD. Particularly accurate predictions were observed \nfor two protein -binding targets, R1189 and R1190, highlighting the method's potential in pred icting \nprotein-binding RNAs, despite the absence of binding partner information, though further accuracy \nimprovements are possible. \nTo address these challenges and improve future RNA structure predictions, one potential approach \nis to integrate the method with traditional techniques and optimize algorithms for underrepresented RNA \nstructures. For instance, utilizing neural networks, such as physics-based neural networks, to learn force \nfields or identify/assemble local patterns instead of directly predicting global[12] 3D structures could \nmitigate biases against known RNA folding and enhance prediction accuracy. \n4.  Conclusion \nMachine learning techniques, particularly deep learning approaches, have shown promising results in \npredicting RNA secondary structures. Compared to thermodynamic model -based methods, deep \nlearning approaches make fewer assumptions, allowing for the consideration of false knots, third-order \ninteractions, non -standard base pairing, and other previously unidentified constraints. Experimental \nfindings indicate that the prediction accuracy of the multi -objective evolutionary strategy algorithm \nsurpasses that of the evolutionary strategy algorithm, especially for long RNA sequences. However, the \nprediction performance for some complex RNA secondary structures remains suboptimal. This may be \nattributed to the inaccuracy of the s elected external interface for computing free energy, which affects \nthe accurate calculation of free energy for complex RNA secondary structures. Additionally, in some \ncases, the real structure of complex RNA secondary structures may be predominantly deter mined by \ncorresponding points in the target space, resulting in predicted structures that are extremely close to \nreality but not accurately predicted. \nFuture improvements in the prediction performance of the algorithm can be achieved by refining the \nmutation operator and fitness evaluation function based on the multi -objective evolutionary strategy \nalgorithm. Furthermore, exploring alternative free energ y computing interfaces and refining the false \nknot free energy computation method could enhance the prediction accuracy of the algorithm. These \nadvancements are crucial for addressing the challenges posed by complex RNA secondary structures \nand improving the overall efficacy of machine learning-based RNA structure prediction methods. \nReferences \n[1] Seetin, Matthew G., and David H. Mathews. \"RNA structure prediction: an overview of methods.\" \nBacterial regulatory RNA: methods and protocols (2012): 99-122. \n[2] Reuter, Jessica S., and David H. Mathews. \"RNAstructure: software for RNA secondary structure \nprediction and analysis.\" BMC bioinformatics 11 (2010): 1-9. \n[3] Wang, Yong, et al. \"Construction and application of artificial intelligence crowdsourcing map \nbased on multi-track GPS data.\" arXiv preprint arXiv:2402.15796 (2024). \n[4] Zhou, Y., Tan, K., Shen, X., & He, Z. (2024). A Protein Structure Prediction Approach \nLeveraging Transformer and CNN Integration. arXiv preprint arXiv:2402.19095. \n[5] Ni, Chunhe, et al. \"Enhancing Cloud-Based Large Language Model Processing with Elasticsearch \nand Transformer Models.\" arXiv preprint arXiv:2403.00807 (2024). \n[6] Shapiro, Bruce A., et al. \"Bridging the gap in RNA structure prediction.\" Current opinion in \nstructural biology 17.2 (2007): 157-165. \nProceedings of the 6th International Conference on Computing and Data Science \nDOI: 10.54254/2755-2721/64/20241362 \n92 \n \n \n[7] Gardner, Paul P., and Robert Giegerich. \"A comprehensive comparison of comparative RNA \nstructure prediction approaches.\" BMC bioinformatics 5 (2004): 1-18. \n[8] Miao, Zhichao, and Eric Westhof. \"RNA structure: advances and assessment of 3D structure \nprediction.\" Annual review of biophysics 46 (2017): 483-503. \n[9] Zheng, Jiajian, et al. \"The Random Forest Model for Analyzing and Forecasting the US Stock \nMarket in the Context of Smart Finance.\" arXiv preprint arXiv:2402.17194 (2024). \n[10] Yang, Le, et al. \"AI-Driven Anonymization: Protecting Personal Data Privacy While Leveraging \nMachine Learning.\" arXiv preprint arXiv:2402.17191 (2024). \n[11] Cheng, Qishuo, et al. \"Optimizing Portfolio Management and Risk Assessment in Digital Assets \nUsing Deep Learning for Predictive Analysis.\" arXiv preprint arXiv:2402.15994 (2024). \n[12] Wu, Jiang, et al. \"Data Pipeline Training: Integrating AutoML to Optimize the Data Flow of \nMachine Learning Models.\" arXiv preprint arXiv:2402.12916 (2024). \nProceedings of the 6th International Conference on Computing and Data Science \nDOI: 10.54254/2755-2721/64/20241362 \n93 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6052839756011963
    },
    {
      "name": "RNA",
      "score": 0.5875477194786072
    },
    {
      "name": "Machine learning",
      "score": 0.5218141078948975
    },
    {
      "name": "Artificial intelligence",
      "score": 0.515161395072937
    },
    {
      "name": "Field (mathematics)",
      "score": 0.49088340997695923
    },
    {
      "name": "Function (biology)",
      "score": 0.45464879274368286
    },
    {
      "name": "Nucleic acid structure",
      "score": 0.41476550698280334
    },
    {
      "name": "Nucleic acid secondary structure",
      "score": 0.41256067156791687
    },
    {
      "name": "Computational biology",
      "score": 0.36671000719070435
    },
    {
      "name": "Data mining",
      "score": 0.34936538338661194
    },
    {
      "name": "Biology",
      "score": 0.16391980648040771
    },
    {
      "name": "Mathematics",
      "score": 0.10915693640708923
    },
    {
      "name": "Gene",
      "score": 0.10704749822616577
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}