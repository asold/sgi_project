{
  "title": "FM-Fusion: Instance-Aware Semantic Mapping Boosted by Vision-Language Foundation Models",
  "url": "https://openalex.org/W4390969311",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5078952009",
      "name": "Chuhao Liu",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5100726478",
      "name": "Ke Wang",
      "affiliations": [
        "Chang'an University"
      ]
    },
    {
      "id": "https://openalex.org/A5061924094",
      "name": "Jieqi Shi",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5089660303",
      "name": "Zhijian Qiao",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A5001947944",
      "name": "Shaojie Shen",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3183768538",
    "https://openalex.org/W4283332944",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W4390874575",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W6850787431",
    "https://openalex.org/W6853520483",
    "https://openalex.org/W2594519801",
    "https://openalex.org/W2563685048",
    "https://openalex.org/W4312956471",
    "https://openalex.org/W6853256829",
    "https://openalex.org/W1999478155",
    "https://openalex.org/W6852789751",
    "https://openalex.org/W2523049145",
    "https://openalex.org/W1745334888",
    "https://openalex.org/W3090386798",
    "https://openalex.org/W2888144883",
    "https://openalex.org/W4312961455",
    "https://openalex.org/W2919379406",
    "https://openalex.org/W2950538710",
    "https://openalex.org/W2607968634",
    "https://openalex.org/W2336416123",
    "https://openalex.org/W2132360065",
    "https://openalex.org/W6747827861",
    "https://openalex.org/W6853219587",
    "https://openalex.org/W2786036844",
    "https://openalex.org/W3102129812",
    "https://openalex.org/W4366781106",
    "https://openalex.org/W4379932565",
    "https://openalex.org/W4393387578",
    "https://openalex.org/W3102330937",
    "https://openalex.org/W4324128075"
  ],
  "abstract": "Semantic mapping based on the supervised object detectors is sensitive to\\nimage distribution. In real-world environments, the object detection and\\nsegmentation performance can lead to a major drop, preventing the use of\\nsemantic mapping in a wider domain. On the other hand, the development of\\nvision-language foundation models demonstrates a strong zero-shot\\ntransferability across data distribution. It provides an opportunity to\\nconstruct generalizable instance-aware semantic maps. Hence, this work explores\\nhow to boost instance-aware semantic mapping from object detection generated\\nfrom foundation models. We propose a probabilistic label fusion method to\\npredict close-set semantic classes from open-set label measurements. An\\ninstance refinement module merges the over-segmented instances caused by\\ninconsistent segmentation. We integrate all the modules into a unified semantic\\nmapping system. Reading a sequence of RGB-D input, our work incrementally\\nreconstructs an instance-aware semantic map. We evaluate the zero-shot\\nperformance of our method in ScanNet and SceneNN datasets. Our method achieves\\n40.3 mean average precision (mAP) on the ScanNet semantic instance segmentation\\ntask. It outperforms the traditional semantic mapping method significantly.\\n",
  "full_text": "IEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 1\nFM-Fusion: Instance-aware Semantic Mapping\nBoosted by Vision-Language Foundation Models\nChuhao Liu1, Ke Wang2,∗, Jieqi Shi 1, Zhijian Qiao 1 and Shaojie Shen 1\nAbstract—Semantic mapping based on the supervised object\ndetectors is sensitive to image distribution. In real-world envi-\nronments, the object detection and segmentation performance\ncan lead to a major drop, preventing the use of semantic\nmapping in a wider domain. On the other hand, the development\nof vision-language foundation models demonstrates a strong\nzero-shot transferability across data distribution. It provides an\nopportunity to construct generalizable instance-aware semantic\nmaps. Hence, this work explores how to boost instance-aware se-\nmantic mapping from object detection generated from foundation\nmodels. We propose a probabilistic label fusion method to predict\nclose-set semantic classes from open-set label measurements. An\ninstance refinement module merges the over-segmented instances\ncaused by inconsistent segmentation. We integrate all the modules\ninto a unified semantic mapping system. Reading a sequence of\nRGB-D input, our work incrementally reconstructs an instance-\naware semantic map. We evaluate the zero-shot performance\nof our method in ScanNet and SceneNN datasets. Our method\nachieves 40.3 mean average precision (mAP) on the ScanNet\nsemantic instance segmentation task. It outperforms the tradi-\ntional semantic mapping method significantly. Code is available\nat https://github.com/HKUST-Aerial-Robotics/FM-Fusion.\nIndex Terms —Semantic Scene Understanding; Mapping;\nRGB-D Perception\nI. I NTRODUCTION\nIstance-aware semantic mapping in indoor environments is\na key module for an autonomous system to achieve a higher\nlevel of intelligence. Based on the semantic map, a mobile\nrobot can detect loop more robust [1] and efficiently [2]. The\ncurrent methods rely on supervised object detectors like Mask\nR-CNN [3] to detect semantic instances and fuse them into an\ninstance-level semantic map. However, the supervised object\ndetectors are trained in specific data distribution and lack\ngeneralization ability. In deploying them in other real-world\nscenarios without fine-tune the networks, their performance is\nseriously degenerated. As a result, the reconstructed semantic\nmap is also of poor quality in the target environment.\nOn the other hand, foundation models have been developing\nrapidly in vision-language modality [4] [5]. Multiple foun-\ndation models are combined to detect and segment objects.\nGroundingDINO [6], the latest State-of-the-Arts (SOTA) open-\nset object detection network, reads a text prompt and performs\nManuscript received: October 24, 2023; Accepted: January, 1, 2024.\nThis paper was recommended for publication by Editor Markus Vincze\nupon evaluation of the Associate Editor and Reviewers’ comments.\n1Authors are with the Department of Electronic and Computer En-\ngineering, the Hong Kong University of Science and Technology,\nHong Kong, China. {cliuci,jshias,zqiaoac}@connect.ust.hk,\neeshaojie@ust.hk\n2Author is with the Department of Information Engineering, Chang’an\nUniversity, China. kwangdd@chd.edu.cn\n∗ Corresponding author.\nFig. 1: Our system reads a sequence of RGB-D frames. The\nvision-language foundation models detect objects in open-set\nlabels and high-quality masks. The SLAM modules generate\na camera pose and a global volumetric map. Our method\nincrementally fuses the object detections from foundation\nmodels into an instance-aware semantic map. A reconstructed\nsemantic map from ScanNet scene0011 01 is shown.\nvision-language modal fusion. It detects objects with bounding\nboxes and open-set labels. The open-set labels are open vo-\ncabulary semantic classes. GroundingDINO has achieved 52.5\nmAP on the zero-shot COCO object detection benchmark. It is\nhigher than most of the supervised object detectors. Moreover,\nthe image tagging model recognizes anything (RAM) [7]\npredicts semantic tags from an image. The tags can be encoded\nas a text prompt and sent to GroundingDINO. Vision foun-\ndation model segment anything (SAM) [4] generates precise\nzero-shot image segmentation results from geometric prompts,\nincluding a bounding box prompt. SAM can generate high-\nquality masks for detection results from GroundingDINO.\nRAM, GroundingDINO, and SAM can be combined to\ndetect objects in open-set labels and high-quality masks. All\nof these foundation models are trained using large-scale data\nand demonstrate strong zero-shot generalization ability in\nvarious image distributions. They provide a new approach\nfor the autonomous system to reconstruct a generalizable\ninstance-aware semantic map. This paper explores how to\nfuse object detection from foundation models into an instance-\naware semantic map.\nTo fuse object detection from foundation models, two\narXiv:2402.04555v2  [cs.CV]  31 Oct 2024\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 2\nchallenges should be addressed. Firstly, the foundation models\ngenerate open-set tags or labels. However, the semantic map-\nping task requires each constructed instance to be classified in\nclose-set semantic classes. A label fusion method is required\nto predict an instance’s semantic class from a sequence of\nobserved open-set labels. Secondly, SAM is operating on a\nsingle image. In dense indoor environments, SAM frequently\ngenerates inconsistent instance masks at changed viewpoints.\nIt results in over-segmented and noisy instance volumes. Refin-\ning instance volumes integrated from inconsistent instance seg-\nmentation results is the challenge. However, these challenges\nhave not been considered in traditional semantic mapping\nworks. If foundation models are directly used in a traditional\nsemantic mapping system, they reconstruct semantic instances\nin a less satisfied quality.\nTo address such challenges, we propose a probabilistic\nlabel fusion method following the Bayes filter algorithm.\nMeanwhile, we refine the instance volume via merging over-\nsegmentation and fuse instance volume with the global volu-\nmetric map. The label fusion and instance refinement modules\nare incrementally run in our system. As shown in Figure 1,\nreading a sequence of RGB-D frames, FM-Fusion fuses the\ndetections from foundation models and runs simultaneously\nwith a traditional SLAM system. Our main contributions are:\n• An approach to fuse the object detections from vision-\nlanguage foundation models into an instance-aware se-\nmantic map. The foundation models are used without\nfine-tune.\n• A probabilistic label fusion method that predicts close-set\nsemantic classes from open-set label measurements.\n• Instances are refined to address inconsistent masks at\nchanged viewpoints.\n• The method is zero-shot evaluated in ScanNet [8]. It\noutperforms the traditional semantic mapping method\nsignificantly. We further evaluate it in SceneNN [9] to\ndemonstrate its robustness in other image distributions.\nII. R ELATED WORKS\nA. Vision-Language Foundation Models\nThe image tagging foundation model RAM [7], recognizes\nthe semantic categories in the image and generates related\ntags. The open-set object detector, such as GLIP [10] and\nGroundingDINO [6], reads a text prompt to detect the objects.\nThe text prompt can be a sentence or a series of semantic\nlabels. It extracts the regional image embeddings and matches\nthe image embedding to the phrase of the text prompt through\na grounding scheme. The network is trained using contrastive\nlearning to align the image embeddings and text embeddings.\nThe detection results contain a bounding box and a set of open-\nset label measurements. SAM [4] can precisely segment any\nobject with a geometric prompt. It is trained with 11M images\nand evaluated in zero-shot benchmarks. SAM demonstrates\nstrong generalization ability across data distribution without\nfine-tune. The combined foundation models read an image and\ndetect objects with open-set labels and masks. We denote them\nas RAM-Grounded-SAM. 1.\n1https://github.com/IDEA-Research/Grounded-Segment-Anything\nThe foundation models have been applied in a series of\ndownstream tasks without fine-tuning. Without semantic pre-\ndiction, SAM3D [11] projects the image-wise segmentation\nfrom SAM to a 3D point cloud map. It further merges the\nsegments with geometric segments generated from graph-\nbased segmentation [12]. SAM is also combined with a neural\nradiance field to generate a novel view of objects [13]. On the\nother hand, combining the SAM or other foundation models\nwith semantic mapping is still an open area.\nB. Semantic Mapping\nSemanticFusion [14] is a pioneer work in semantic mapping.\nIt trains a lightweight CNN-based semantic segmentation\nnetwork [15] on the NYUv2 dataset. SemanticFusion incre-\nmentally fuses the semantic labels, ignoring the instance-\nlevel information, into each surfel of the global volumetric\nmap. In Bayesian fusing the label measurement, the semantic\nprobability is directly provided by the object detector. Relying\non a pre-trained Mask R-CNN on the COCO dataset, Kimera\n[16] uses similar methods to fuse semantic labels into a voxel\nmap. It clusters the nearby voxels with identical semantic\nlabels into instances. Kimera further constructs a scene graph,\nwhich is a hierarchical map representation. Based on Kimera,\nHydra [2] utilizes the scene graph to detect loops more\nefficiently.\nOn the other hand, Fusion++ [17] directly detects semantic\ninstances on images and fuses them into instance-wise volu-\nmetric maps. It further demonstrates that semantic landmarks\ncan be used in loop detection. Later methods use similar\nmethods to construct semantic instance maps but utilize the\nsemantic landmarks in novel methods to detect loops [1] [18].\nRather than a pure dense map such as a surfel map or\nvoxel map, V oxblox++ [19] first generates geometric segments\non each depth frame [20]. If the object detection masks the\ncomplete region of an instance, it can merge those broken\nsegments generated from geometric segmentation. Then, the\nmerged segments with their labels are fused into a global\nsegment map through a data association strategy.\nThe main limitation of the current semantic mapping meth-\nods is the lack of ability to generalize. The supervised ob-\nject detection networks are trained with limited source data.\nConsidering the majority of target SLAM scenarios do not\nprovide annotated semantic data, object detection can not be\nfine-tuned on the target distribution. To avoid the issue of\ngeneralization, Kimera has to experiment in a synthetic dataset\n[16], including some experiments that rely on ground-truth\nsegmentation. Lin etc. [1] sets up an environment with sparsely\ndistributed objects to reconstruct a semantic map. V oxblox++\nevaluates a few of the 9 semantic classes in 10 scans. Although\nthey propose novel semantic SLAM methods, the semantic\nmapping module prevents their methods from being used in\nother real-world scenes.\nTo enhance robustness in the distribution shift, our method\nfuses the object detections from foundation models to recon-\nstruct the instance-aware semantic map. We evaluate its zero-\nshot performance on the ScanNet semantic instance segmenta-\ntion benchmark. It involves 20 classes in the NYUv2 label-set\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 3\nand evaluates their average precision(AP) in 3D space. We also\nshow the qualitative results in several SceneNN scans, which\nhave been used by the previous semantic mapping works.\nIII. F USE MULTI -FRAME DETECTIONS\nFig. 2: System overview of FM-Fusion\nA. Overview\nAs shown in Figure 2, FM-Fusion reads an RGB-D se-\nquence and reconstructs a semantic instance map. Each se-\nmantic instance is represented as f = {Ls, v}, where Ls is\nits predicted semantic class and v is its voxel grid map. Ls is\npredicted as a label cn over the NYUv2 label-set Lc. At each\nRGB-D frame {It, Dt} at frame index t, RAM generates a\nset of possible object tags. The valid tags are encoded into the\ntext prompt qt. GroundingDINO generates object detections\nwith each of the detection zt\nk = {yi, si, qt}i, where yi is\nthe predicted open-set label, si is the corresponding similarity\nscore and qt is the frame-wise text prompt. For each zt\nk, SAM\ngenerates an object mask mk.\nB. Prepare the object detector\nWe first construct open-set labels of our interests Lo. RAM\ngenerates various tags. Many of them are not correlated\nwith the pre-defined labels Lc. The labels of interest can be\nselected by sampling a histogram of measured labels for each\nsemantic class in Lc. In the ScanNet experiment, we select\n38 open-set labels to construct Lo. Only the tags belonging\nto Lo are encoded into the qt and sent to GroundingDINO.\nGroundingDINO matches each detected object with the tags in\nthe text prompt. The tags in qt and label measurements {yi}i\nin each zt\nk are all from the label-set Lo.\nIn a single image frame, RAM can miss some objects in its\ngenerated tags due to occlusion. The missing tags further cause\nGroundingDINO to detect objects incorrectly. It is a natural\nlimitation of running foundation models on a single image. To\naddress it, we encode the detected labels in adjacent frames\ninto the text prompt. The augmented text prompt qt = ¯qt ∪Ut,\nwhere ¯qt is the valid tags from RAM and Ut is a set of\nmeasured labels in previous adjacent frames. All the tags\nin ¯qt and labels in Ut belong to the Lo. The text prompt\naugmentation can reduce the missing tags generated from\na single image. More complete tags improve the detection\nperformance of GroundingDINO.\nC. Data association and integration\nIn our system, each instance maintains an individual voxel\ngrid map v, similar to Fusion++ [17]. Meanwhile, the SLAM\nmodule integrates a global TSDF map [21] separately. The ad-\nvantage of separating semantic mapping and global volumetric\nmapping is that false or missed object detection can not affect\nthe global volumetric map. So, in each RGB-D frame, all the\nobserved sub-volumes are integrated into the global TSDF map\ndespite the detection variances.\nIn each detection frame, data association is conducted be-\ntween detection results and volumes of the existing instances.\nSpecifically, the observed instance voxels are first queried.\nThey can be searched by projecting the depth image into the\nvoxel grid map of all the instances. If an instance is observed,\nits voxels are projected to the current RGB frame. For a\ndetection zt\nk and a projected instance fj, their intersection over\nunion (IoU) can be calculated Ω(zt\nk, fj) = mk∩rj\nmk∪rj\n, where mk\nis a detection mask and rj is the projected mask of an existed\ninstance. If Ω(zt\nk, fj) is larger than a threshold, the detection\nk is associated with instance j.\nAfter data association, we integrate the voxel grid map of\nmatched instances accordingly. Those unmatched detections\ninitiate new instances. An instance voxel grid map v is\nintegrated using the traditional voxel map fusion method [21].\nSpecifically, we raycast the masked depth of a detected object\nand update all of its observed voxels.\nD. Probabilistic label fusion\nFig. 3: GroundingDINO detects a bookshelf and generates\nmultiple open-set label measurements across frames. Our label\nfusion module predicts its semantic class in NYUv2 label-set\nLc from label measurements in Lo.\nAs shown in Figure 3, an object is observed by Ground-\ningDINO across frames. Each generated detection result zt\nk =\n{yi, si, qt}i contains multiple label measurements yi, the cor-\nresponding similarity score si and a text prompt qt, where\nyi = om, om ∈ Lo. Based on the associated detections, we\npredict a probability distribution p(Lt\ns = cn), where cn ∈ Lc\nand t is the index of the image frame.\nWe follow the Bayes filter algorithm [22] to fuse open-\nset label measurements and propagate them along the image\nsequence. The input to the Bayesian label fusion is detection\nresult zt\nk, semantic probability distribution at the last frame\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 4\np(Lt−1\ns ), and a uniform control input ut. And it predicts the\nlatest semantic probability distribution p(Lt\ns).\nAlgorithm 1: Bayes Filter for Label Fusion\nInput: p(Lt−1\ns ) , zt\nk = {yi, si, qt}i∈[0:J), ut = 1\nOutput: p(Lt\ns)\nfor cn ∈ Lc do\nPrediction:\n¯p(Lt\ns = cn) = p(Lt\ns = cn|Lt−1\ns = cn, ut)p(Lt−1\ns = cn)\n(1)\n¯p(Lt\ns = cn) = p(Lt−1\ns = cn) (2)\nMeasurement Update:\np(Lt\ns = cn) = ηp(zt\nk|Lt\ns = cn)¯p(Lt\ns = cn) (3)\np(Lt\ns = cn)\n= ηΠJ−1\ni=0 p(yi, si, qt|Lt\ns = cn)¯p(Lt\ns = cn)\n(4)\np(yi, si, qt|Lt\ns = cn)\n= p(si|yi, qt, Lt\ns = cn)p(yi, qt|Lt\ns = cn) (5)\np(yi, qt|Lt\ns = cn) = p(yi = om, ∃om ∈ qt|Lt\ns = cn)\n(6)\nend\nThe key part in our Bayesian label fusion module is the\nlikelihood function p(yi, si, qt|Lt\ns = cn), as shown in equation\n(5). The score likelihood p(si|yi, qt, Lt\ns = cn) is given by\nGroundingDINO, while label likelihood p(yi, qt|Lt\ns = cn)\nshould be statistic summarized. Since GroundingDINO can\nonly detect a label yi if it is given in the text prompt, the\nlabel likelihood can be transmitted as equation (6). ∃om ∈ qt\ndenotes the detected label om exists in the text prompt qt.\nHere, we further expand the label likelihood in equation (6)\ninto two conditional probabilities,\np(yi = om, ∃om ∈ qt|Lt\ns = cn)\n= p(yi = om|∃om ∈ qt, Lt\ns = cn)p(∃om ∈ qt|Lt\ns = cn) (7)\nThe first term is a detection likelihood while the second term\nis a tagging likelihood. They can be statistically summarized\nusing the detection results from GroundingDINO and tagging\nresults from RAM. We follow the equation (7) to construct\na label likelihood matrix over om ∈ Lo and cn ∈ Lc. In\nthe ScanNet training set, we sample 35, 000 image frames\nwith tagging results, detection results, and ground-truth an-\nnotation to summarize the statistics. In the Bayesian update\nstep in equation (5), the label likelihood between each pair of\n{om, cn} can be queried from the constructed label likelihood\nmatrix.\nAs shown in Figure 4(a), parts of the constructed label\nlikelihood matrix are visualized, while the complete likelihood\nmatrix involves the entire Lo and Lc. For comparison, we\nconstruct a manually assigned label likelihood matrix similar\nto Kimera. As shown in Figure 4, the statistic summarized\nlikelihood matrix is quite different from the manually assigned\nFig. 4: The label likelihood matrix p(yi = om, ∃om ∈ qt|Ls =\ncn) summarized in ScanNet is shown on the left. Each column\nrepresents a specific true semantic class cn, while each row\nrepresents a measured open-set label om. On the right, it is a\nmanually assigned likelihood matrix.\none. In the statistical label likelihood, each semantic class can\nbe detected by its similar open-set labels at various probabili-\nties. Those cells beyond the diagonal can also have likelihood\nvalues, indicating the probability of falsely measured labels.\nThe summarized likelihood matrix following equation (7)\ndescribes the probability distribution of label measurements\nreasonably.\nIn actual implementation, the multiplicative measurement\nupdate in equation (3) frequently generates over-confident\nprobability distribution, which is also reported in Fusion++\n[17]. It causes p(Lt\ns) can be easily dominated by the latest\nmeasurement zt\nk even if previous label measurements are all\ndifferent with zt\nk. As a result, in the measurement update, we\npropagate the probability distribution by weighted addition.\np(Lt\ns) = p(zt\nk|Lt\ns) + (t − 1)¯p(Lt\ns)\nt (8)\nThen, the predicted semantic class for each instance at frame\nt is arg maxcn p(Lt\ns = cn).\nIV. I NSTANCE REFINEMENT\nFig. 5: An example of an inconsistent instance mask generated\nfrom SAM. In each of the three frames, different areas of the\nbed are segmented.\nA. Merge over-segmentation\nAlthough SAM has demonstrated promising segmentation\non a single image, it generates inconsistent instance masks at\nchanged viewpoints, as shown in Figure 5. The inconsistent\nmasks prevent a correct data association between detections\nand observed instances. Those mismatched detections are\ninitialized as new instances and cause over-segmentation, as\nshown in Figure 6(a).\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 5\n(a)\n (b)\nFig. 6: The visualization shows instance voxel grid map (a)\nbefore and (b) after the merge.\nThe inconsistent instance mask is a natural limitation\nfor image-based segmentation networks, including SAM and\nMask R-CNN. To address it, we utilize spatial overlap infor-\nmation to merge the over-segmentation. For a pair of instances\n{fa, fb} at detection frame t, where fa is volumetric larger than\nfb, their semantic similarity σ(fa, fb) and 3D IoU Ω(fa, fb) are\ncalculated,\nσ(fa, fb) = ¯p(Lt\ns(a)) · ¯p(Lt\ns(b)) (9)\nΩ(fa, fb) = ˆva ∪ vb\nvb\n(10)\nwhere ¯pt(Ls(a)) is the normalized semantic distribution, v is\nan instance voxel grid map and ˆv is the inflated voxel grid\nmap. The voxel inflation is designed to enhance 3D IoU for\ninstances with sparse volume. It can be directly generated by\nscaling the length of each voxel in v. If the semantic similarity\nand 3D IoU are both larger than the corresponding thresholds,\nfb is integrated into the voxel grid map of fa and further\ncleared from the instance map. As shown in Figure 6(b), over-\nsegmented instances caused by inconsistent object masks are\nmerged.\nB. Instance-geometry fusion\nFig. 7: Illustration of the instance-geometry fusion. Geometric\npoints are extracted from the global map.\nThe instance-wise voxel grid map can contain voxel outliers\ndue to noisy depth images being integrated. On the other hand,\nthe global TSDF map is a precise 3D geometric representation.\nIt is because the global TSDF map integrates all the observed\nvolumes in each RGB-D frame, while instance volume only in-\ntegrates a masked RGB-D frame if the corresponding instance\nis correctly detected. To filter voxel outliers, we fuse instance-\nwise voxel grid map v with the point cloud P extracted from\nthe global TSDF map. As shown in Figure 7, those voxels in\nv that are not occupied by any point in P are outliers and\nhave been removed. The fused voxel grid map represents the\ninstance volume precisely.\nV. E XPERIMENT\nWe chose the public dataset ScanNet and SceneNN to\nevaluate the semantic mapping quality. In ScanNet, 30 scans\nfrom its validation set are used. We evaluated its semantic\ninstance segmentation by average precision (AP). In another\nexperiment, we selected 5 scans from SceneNN and evalu-\nated the generalization ability of our method. The SceneNN\nscans are also used by the previous method [19]. In all the\nexperiments, camera poses are provided by the dataset.\nWe compared our method with Kimera 2 and a self-\nimplemented Fusion++. To enable Kimera to read open-set\nlabels Lo, we converted each label in Lo to a semantic\nclass in NYUv2 label-set Lc. The hard association between\n{Lo, Lc} are decided by an academic ChatGPT 3. Then,\nKimera can reconstruct a point cloud with semantic labels. To\nfurther generate instance-aware point cloud, we employed the\ngeometric segmentation method known as ”Cluster-All” [23].\nIt clusters the nearby points with identical semantic labels\ninto an instance. Cluster-All is applied as a post-processing\nstep on the reconstructed semantic map from Kimera. Notice\nthat Cluster-All is very similar to the post-processing module\nprovided by Kimera. But we use Cluster-All for convenient\nimplementation. Meanwhile, Fusion++ is implemented based\non our system modules. Compared with the original Fusion++\nmethod, the main difference is that our implemented version\ndoes not maintain a foreground probability for each voxel.\nInstead, we updated the voxel’s weight and filter background\nvoxels using their weights. In experiments with traditional\nobject detection, we used a Mask R-CNN backbone with\nFPN101 image backbone. We evaluated a pre-trained Mask\nR-CNN and a fine-tuned Mask R-CNN. The pre-trained one\nis trained in COCO instance segmentation dataset, while we\nalso fine-tuned it using ScanNet dataset.\nIn implementation, we utilized Open3D [24] toolbox to\nconstruct the global TSDF map and instance-wise voxel grid\nmap. The global TSDF map is integrated for every RGB-\nD frame, while our method and all baselines run in every\n10 frames to integrate the detected instances. In all the\nexperiments, the RGB-D images are in 640 × 480 dimension\nand the voxel length is set to be 1.5 cm. The experiment is\nrun on an Intel-i7 computer with Nvidia RTX-3090 GPU in\nan offline fashion.\nA. ScanNet Evaluation\nIn the instance segmentation benchmark, as shown in Table\nI, semantic mapping based on Mask R-CNN can only recon-\nstruct a few of the semantic categories. It is because the pre-\ntrained Mask R-CNN is trained using COCO label-set and\nthose new semantic classes in NYUv2 label-set are predicted\nwith 0 AP. Even for those predictable semantic classes, the pre-\ntrained Mask R-CNN suffers from the issue of generalization\nand achieve low AP 50 scores. In experiment with fine-tune\nMask R-CNN, although the mean AP is improved, they still\nreconstruct a few of semantic classes with 0 AP. We also\nnotice that Kimera performs significantly better than the imple-\nmented Fusion++. We believe the difference comes from their\n2https://github.com/MIT-SPARK/Kimera-Semantics\n3https://chatgpt.ust.hk\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 6\nMethod cab. bed cha. sof. tab. door win. bkf. pic. cou. desk cur. ref. show. toi. sink bath. oth. mAP50\nM-CNN& Kimera 0.0 6.4 10.0 25.1 17.3 0.0 0.0 0.0 0.0 0.0 0.0 0.0 24.6 0.0 10.4 4.3 0.0 0.0 5.4\nM-CNN& Fusion++ 0.0 27.1 3.7 14.7 4.4 0.0 0.0 0.0 0.0 0.0 0.0 0.0 23.9 0.0 46.6 20.0 0.0 0.0 7.8\nM-CNN∗& Kimera 27.8 55.5 18.7 0.0 0.0 16.5 33.1 31.2 12.9 23.3 3.9 26.0 0.0 75.0 60.0 22.7 60.0 0.0 25.9\nM-CNN∗& Fusion++ 7.1 22.0 31.2 0.0 13.3 12.5 15.0 11.5 28.5 0.0 0.0 18.1 0.0 0.0 0.0 40.1 0.0 0.0 11.1\nG-SAM &Kimera 32.5 21.0 16.8 54.5 21.7 26.0 31.5 45.3 21.9 8.7 3.9 24.8 29.6 0.0 50.0 9.4 46.2 0.0 24.7\nG-SAM & Ours 4.6 46.2 49.2 39.6 37.3 19.5 12.0 50.4 44.0 3.8 8.9 15.5 66.7 82.2 100.0 41.5 75.0 30.7 40.3\nTABLE I: Evaluate Kimera, the implemented Fusion++, and the proposed method on ScanNet using 30 validation scans.\nWe report AP 50 at 50% IoU threshold for each semantic class. M-CNN denotes Mask R-CNN, M-CNN ∗ is fine-tuned Mask\nR-CNN, while G-SAM refers to RAM-Grounded-SAM.\n(a) Kimera Semantic\n (b) Our Semantic\n (c) Kimera Instances\n (d) Our Instances\nFig. 8: The reconstructed instance map using RAM-Grounded-SAM in ScanNet scene0011, scene0435 and scene0633 (from\ntop to bottom). The falsely predicted semantic classes in (a) and (b) are highlighted in red circles, while spatial conflicted\nsemantics are in yellow. All semantic maps are colored following the NYUv2 color map and instances are colored randomly.\ndifferent map management methods. Unlike Kimera ignores\nthe instance-wise segmentation, Fusion++ maintains instance-\nwise volumes and requires data association. But the fine-tuned\nMask R-CNN still generate detections with noisy instance\nmasks, causing a large amount of false data association. As\na result, Fusion++ generates instances with too many over-\nsegmentation and maintains a low AP score.\nThe results demonstrated that semantic mapping based on\nsupervised object detection can be easily affected by image\ndistribution, label-set distribution and annotation quality. On\nthe other hand, boosted by pre-trained foundational models\nRAM-Grounded-SAM, both Kimera and our method recon-\nstructed semantic instances in higher quality than semantic\nmapping methods based on the supervised object detection.\nHowever, simply replacing object detectors with foundation\nmodels could not utilize the maximum potential of the founda-\ntion models. Compared with Kimera using RAM-Grounded-\nSAM, our method achieved +15.6 mAP50. The boosted per-\nformance comes from two aspects. Firstly, our probabilistic\nlabel fusion predicts semantic class in higher accuracy. As\nshown in Figure 8(a), Kimera predicts semantics of some sub-\nvolumes falsely. Since Kimera updates the label measurements\nwith a manually assigned likelihood probability and ignores\nthe similarity score provided by GroundingDINO, it is easier\nto be affected by false label measurements. Secondly, Kimera\nignores the instance-level segmentation and reconstructs many\nover-segmented instances. Some of them are predicted with\ndifferent semantic labels, as shown in Figure 8(a) and 8(c).\nHowever, our method is instance-aware. Each instance volume\nis maintained separately. Our instance refinement module\nmerges over-segmented instances caused by inconsistent in-\nstance masks at changed viewpoints. We further fused instance\nvolume with a global volumetric map. Hence, our instances\nvolumes are spatially consistent and relative precise, as shown\nin Figure 8(d).\nThe rest of the ScanNet experiment focus on evaluating\neach module of our method through an ablation study. As\nshown in Table.II, the text prompt augmentation, probabilistic\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 7\n(a) Ablation-A Semantic\n (b) Our Semantic\n (c) Ablation-A Instances\n (d) Our Instances\nFig. 9: A visualized example at ScanNet scene0025 01. (a) A falsely predicted instance caused by manually assigned likelihood\nis highlighted in a red circle, while the spatial conflicted semantic predictions are highlighted in yellow. (b) Proposed semantic\nresult. (c) Over-segmentation is highlighted in yellow. (d) Our refined instances.\nMethod Prm. Aug. Likelihood Refine mAP 50\nA ✓ Manual Assign ✓ 35.9\nB × statistic ✓ 34.1\nC ✓ statistic × 23.4\nOurs ✓ statistic ✓ 40.3\nTABLE II: Ablation study of FM-Fusion. Prm. Aug. denotes\ntext prompt augmentation.\nlabel fusion with statistic summarized likelihood, and instance\nrefinement all improve the reconstructed semantic instances.\nA visualized example of the Ablation-A is shown in Figure\n9. As shown in Figure 9(a), Ablation-A predicts an instance\nfalsely, similar to Kimera. It also predicts overlapped instances\nwith over-confident semantic probability distributions. They\ncan not be merged during refinement due to their low se-\nmantic similarity. So, the over-segmented instances can not\nbe merged, as shown in Figure 9(c). On the other hand, our\nmethod predicts the corresponding semantic classes correctly.\nThe over-segmented instances are predicted with a similar\nsemantic probability distribution and have been merged suc-\ncessfully, as shown in Figure 9(b) and 9(d).\nFig. 10: An image of object detection from Ablation-B and our\nmethod are shown in (a) and (b). The labels incorporated by\ntext prompt augmentation are highlighted in red. The images\nare from ScanNet scene0329.\nAs shown in Figure 10(a), RAM fails to recognize a table\ndue to the extreme viewpoint, and GroundingDINO cannot\ndetect it either. On the other hand, as illustrated in section\nIII-B, our method maintains a series of labels Ut that has\nbeen detected in previous 5 frames. If a label in Ut is\nnot given in RAM tags, the corresponding label is added\nto the text prompt. As shown in Figure 10(b), our method\ndetects the table correctly. Beyond miss detecting objects, the\nincomplete tags from RAM cause false label measurements in\nother frames. Hence, Ablation-B reconstructs a few instances\nwith a false semantic class. More results can be found in our\nsupplementary video.\nTo sum up, simply replacing traditional object detectors with\nRAM-Grounded-SAM to construct the semantic map improves\nthe semantic mapping performance significantly. However,\nfalse label measurements, inconsistent instance masks, and\nmissed tags in the text prompt still exist in foundation mod-\nels. They limit the performance of semantic mapping. We\nconsider those limitations of foundation models. Compared\nwith Kimera using RAM-Grounded-SAM, our method further\nimproves mAP50 by +15.6.\nB. SceneNN evaluation\nIn the SceneNN experiment, we kept using the label likeli-\nhood matrix P(yi = om, ∃om ∈ qt|Ls = cn) summarized in\nScanNet and compare it with Kimera.\nAs shown in Figure11, Kimera reconstructed some instances\nwith false labels and over-segmentation, similar to its recon-\nstruction in ScanNet. On the other hand, our semantic predic-\ntion is more accurate and significantly less over-segmentation.\nThe quantitative results can be found in Table III. Although\nour statistical label likelihood is summarized using ScanNet\ndata, we have not observed a domain gap in implementing\nit in SceneNN. One of the reasons is that foundation models\npreserve strong generalization ability. RAM-Grounded-SAM\nmaintains a similar label likelihood matrix across the image\ndistribution. For example, a door is frequently detected as a\ncabinet in both ScanNet and SceneNN datasets, which are\nhighlighted in red in Figure9(a) and Figure11(a). Hence, our\nstatistical label likelihood can be used across domains.\n096 206 223 231 255 All\nKimera 52.0 29.9 28.6 34.0 37.5 41.1\nOurs 63.1 69.7 37.5 38.8 25.0 49.7\nTABLE III: SceneNN Quantitative results (mAP 25).\nC. Efficiency\nBase Scaling\nFoundation RAM 28.5 ms -\nModels GroundingDINO 120.7 ms -\nSAM 464.4 ms -\nFM-Fusion\nProjection 307ms 63.4 ms/obj\nData Assoc. 47.1ms 9.7 ms/obj\nIntegration 71.9ms 14.9 ms/obj\nTotal 1039.6 ms -\nTABLE IV: Runtime analysis for each frame in ScanNet.\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 8\n(a) Kimera Semantic\n (b) Our Semantic\n (c) Kimera Instances\n (d) Our Instances\nFig. 11: Reconstructions in SceneNN 096. False semantic and over-segmented instances are highlighted in red circles.\nSo far, the system run offline. As shown in Table. IV, the\ntotal time for each frame is 1039.6 ms. Although it is not a\nreal-time system yet, many modules can be optimized in the\nfuture. SAM-related variants have been published to generate\ninstance masks faster [25]. In FM-Fusion, a few modules\nare implemented with Python, the efficiency can be further\nimproved by deploying it with C++. That would be one of\nour future works.\nVI. C ONCLUSION\nIn this work, we explored how to boost instance-aware se-\nmantic mapping with zero-shot foundation models. With foun-\ndation models, objects are detected in open-set semantic labels\nat various probabilities. The object masks generated at changed\nviewpoints are inconsistent and cause over-segmentation. The\ncurrent semantic mapping methods have not considered such\nchallenges. On the other hand, our method uses a Bayesian\nlabel fusion module with statistic summarized likelihood and\nrefines the instance volumes simultaneously. Compared with\nthe baselines, our method performs significantly better in\nScanNet and SceneNN benchmarks.\nREFERENCES\n[1] S. Lin, J. Wang, M. Xu, H. Zhao, and Z. Chen, “Topology Aware Object-\nLevel Semantic Mapping towards More Robust Loop Closure,” IEEE\nRobotics and Automation Letters (RA-L) , vol. 6, pp. 7041–7048, 2021.\n[2] N. Hughes, Y . Chang, and L. Carlone, “Hydra: A real-time spatial\nperception system for 3D scene graph construction and optimization,”\nin Proc. of Robotics: Science and System (RSS) , 2022.\n[3] K. He, G. Gkioxari, P. Doll ´ar, and R. Girshick, “Mask r-cnn,” in Proc.\nof the IEEE intl. Conf. on Comp. Vis. (ICCV) , 2017, pp. 2961–2969.\n[4] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson,\nT. Xiao, S. Whitehead, A. C. Berg, W.-Y . Lo, P. Doll´ar, and R. Girshick,\n“Segment anything,” arXiv:2304.02643, 2023.\n[5] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable\nvisual models from natural language supervision,” in International\nconference on machine learning(ICML). PMLR, 2021, pp. 8748–8763.\n[6] S. Liu, Z. Zeng, T. Ren, F. Li, H. Zhang, J. Yang, C. Li, J. Yang,\nH. Su, J. Zhu et al. , “Grounding dino: Marrying dino with grounded\npre-training for open-set object detection,” arXiv preprint:2303.05499,\n2023.\n[7] Y . Zhang, X. Huang, J. Ma, Z. Li, Z. Luo, Y . Xie, Y . Qin, T. Luo, Y . Li,\nS. Liu et al. , “Recognize anything: A strong image tagging model,”\narXiv preprint:2306.03514, 2023.\n[8] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\nM. Nießner, “Scannet: Richly-annotated 3d reconstructions of indoor\nscenes,” in Proc. of the IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2017.\n[9] B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F. Yu, and S.-K.\nYeung, “Scenenn: A scene meshes dataset with annotations,” in Proc.\nof the International Conference on 3D Vision (3DV) , 2016, pp. 92–101.\n[10] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y . Zhong, L. Wang,\nL. Yuan, L. Zhang, J.-N. Hwang et al., “Grounded language-image pre-\ntraining,” in Proc. of the IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR), 2022, pp. 10 965–10 975.\n[11] D. Zhang, D. Liang, H. Yang, Z. Zou, X. Ye, Z. Liu, and X. Bai,\n“Sam3d: Zero-shot 3d object detection via segment anything model,”\narXiv preprint:2306.02245, 2023.\n[12] P. F. Felzenszwalb and D. P. Huttenlocher, “Efficient graph-based image\nsegmentation,” International journal of computer vision (IJCV) , vol. 59,\nno. 2, pp. 167–181, 2004.\n[13] Q. Shen, X. Yang, and X. Wang, “Anything-3d: Towards single-view\nanything reconstruction in the wild,” arXiv preprint:2304.10261, 2023.\n[14] J. McCormac, A. Handa, A. Davison, and S. Leutenegger, “Semanticfu-\nsion: Dense 3d semantic mapping with convolutional neural networks,”\nin Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA) . IEEE,\n2017, pp. 4628–4635.\n[15] H. Noh, S. Hong, and B. Han, “Learning deconvolution network for\nsemantic segmentation,” in Proc. of the IEEE intl. Conf. on Comp. Vis.\n(ICCV), 2015, pp. 1520–1528.\n[16] A. Rosinol, M. Abate, Y . Chang, and L. Carlone, “Kimera: an open-\nsource library for real-time metric-semantic localization and mapping,”\nin Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA) , 2020.\n[17] J. McCormac, R. Clark, M. Bloesch, A. J. Davison, and S. Leutenegger,\n“Fusion++: V olumetric object-level SLAM,” Proc. of the International\nConference on 3D Vision (3DV) , pp. 32–41, 2018.\n[18] J. Yu and S. Shen, “Semanticloop: loop closure with 3d semantic graph\nmatching,” IEEE Robotics and Automation Letters (RA-L) , vol. 8, no. 2,\npp. 568–575, 2022.\n[19] M. Grinvald, F. Furrer, T. Novkovic, J. J. Chung, C. Cadena, R. Siegwart,\nand J. Nieto, “V olumetric instance-aware semantic mapping and 3D\nobject discovery,” IEEE Robotics and Automation Letters (RA-L), vol. 4,\nno. 3, pp. 3037–3044, 2019.\n[20] F. Furrer, T. Novkovic, M. Fehr, A. Gawel, M. Grinvald, T. Sattler,\nR. Siegwart, and J. Nieto, “Incremental object database: Building 3d\nmodels from multiple partial observations,” in Proc. of the IEEE/RSJ\nIntl. Conf. on Intell. Robots and Syst.(IROS) . IEEE, 2018, pp. 6835–\n6842.\n[21] H. Oleynikova, Z. Taylor, M. Fehr, R. Siegwart, and J. Nieto, “V oxblox:\nIncremental 3D Euclidean Signed Distance Fields for on-board MA V\nplanning,” Proc. of the IEEE/RSJ Intl. Conf. on Intell. Robots and\nSyst.(IROS), vol. 2017-Sep, pp. 1366–1373, 2017.\n[22] S. Thrun, “Probabilistic robotics,” Communications of the ACM, vol. 45,\nno. 3, pp. 52–57, 2002.\n[23] B. Douillard, J. Underwood, N. Kuntz, V . Vlaskine, A. Quadros,\nP. Morton, and A. Frenkel, “On the segmentation of 3d lidar point\nclouds,” in Proc. of the IEEE Intl. Conf. on Robot. and Autom. (ICRA) .\nIEEE, 2011, pp. 2798–2805.\n[24] Q.-Y . Zhou, J. Park, and V . Koltun, “Open3D: A modern library for 3D\ndata processing,” arXiv:1801.09847, 2018.\n[25] X. Zhao, W. Ding, Y . An, Y . Du, T. Yu, M. Li, M. Tang, and J. Wang,\n“Fast segment anything,” arXiv preprint:2306.12156, 2023.\nIEEE ROBOTICS AND AUTOMATION LETTERS, VOL.9, NO.3, MARCH 2024 9\nAPPENDIX A\nGENERATE HARD -ASSOCIATED LABEL SET\n(a)\n(b)\nFig. 12: (a) We ask ChatGPT to generate a hard association\nbetween Lo and Lc. (b) A sample segmentation image sent\nto Kimera. The detected labels in Lo are converted to corre-\nsponding labels in Lc following the hard-associated label set.\nAs shown in Fig. 12(a), we ask ChatGPT to generate a hard\nassociation between open-set labels Lo and NYUv2 labels Lc.\nIn experiment with Kimera, we follow the hard association to\nconvert each label.\nAPPENDIX B\nSUMMARIZE LABEL LIKELIHOOD\nWe summarized the image tagging likelihood matrix\np(∃om ∈ qt|Lt\ns = cn) and object detection likelihood matrix\np(yi = om|∃om ∈ qt, Lt\ns = cn) that are introduced in equation\n(7). For example, the corresponding likelihood of table can be\nsummarized as follows,\np(∃table ∈ qt|Lt\ns = table) = |ˆItable|\nItable\np(yi = table|∃table ∈ qt, Lt\ns = table) = | ˆOtable|\n|Otable|\n(11)\nItable is a set of image frames that observed a ground-truth\ntable, while ˆItable is a set of image frames with table in their\npredicted tags. Similarly, Otable is a set of observed ground-\ntruth table instances if their image tags contain a table, while\nˆOtable is a set of predicted table instances. We summarized\nthe label likelihood matrix using the ScanNet dataset. The\n(a)\n (b)\nFig. 13: (a) The summarized RAM tagging likelihood. (b) The\nsummarized Grounding-DINO detection likelihood.\n(a)\n (b)\nFig. 14: (a) The summarized label likelihood matrix. (b) The\nmanually assigned label likelihood matrix.\nsummarized image tagging likelihood and object detection\nlikelihood are visualized in Fig. 13.\nWe follow equation 7 to compute the label likelihoodp(yi =\nom, ∃om ∈ qt|Lt\ns = cn). It is visualized in Fig. 14(a). To\ncompare, the manually assigned label likelihood is visualized\nin Fig. 14(b). It is generated based on the hard-associated label\nset given by ChatGPT, and we manually assign a likelihood\nat 0.9. The manually assigned label likelihood is used in\nexperiments with Ablation-A.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8230940103530884
    },
    {
      "name": "Segmentation",
      "score": 0.6544574499130249
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6537400484085083
    },
    {
      "name": "Semantic mapping",
      "score": 0.5880221128463745
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.46283650398254395
    },
    {
      "name": "Semantic similarity",
      "score": 0.4512847065925598
    },
    {
      "name": "Probabilistic logic",
      "score": 0.44730469584465027
    },
    {
      "name": "Object (grammar)",
      "score": 0.44281086325645447
    },
    {
      "name": "Semantic computing",
      "score": 0.44215139746665955
    },
    {
      "name": "Natural language processing",
      "score": 0.42761075496673584
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4254530072212219
    },
    {
      "name": "Object detection",
      "score": 0.42227989435195923
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.35099732875823975
    },
    {
      "name": "Semantic Web",
      "score": 0.07869476079940796
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I25355098",
      "name": "Chang'an University",
      "country": "CN"
    }
  ]
}