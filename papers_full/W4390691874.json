{
  "title": "TCCU-Net: Transformer and CNN Collaborative Unmixing Network for Hyperspectral Image",
  "url": "https://openalex.org/W4390691874",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100330952",
      "name": "Jianfeng Chen",
      "affiliations": [
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5057602729",
      "name": "Chen Yang",
      "affiliations": [
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5100322293",
      "name": "Lan Zhang",
      "affiliations": [
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5044793021",
      "name": "Linzi Yang",
      "affiliations": [
        "Guizhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5109012684",
      "name": "Lifeng Bian",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A5101537056",
      "name": "Zijiang Luo",
      "affiliations": [
        "Shunde Polytechnic"
      ]
    },
    {
      "id": "https://openalex.org/A5108999883",
      "name": "Jihong Wang",
      "affiliations": [
        "Guizhou University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2087263574",
    "https://openalex.org/W2007502829",
    "https://openalex.org/W4313438466",
    "https://openalex.org/W4366150256",
    "https://openalex.org/W4233760599",
    "https://openalex.org/W2782517596",
    "https://openalex.org/W2022470997",
    "https://openalex.org/W2078296814",
    "https://openalex.org/W2067782748",
    "https://openalex.org/W2127062304",
    "https://openalex.org/W2804275394",
    "https://openalex.org/W2125298866",
    "https://openalex.org/W2169924573",
    "https://openalex.org/W2097211423",
    "https://openalex.org/W2081555128",
    "https://openalex.org/W1755724306",
    "https://openalex.org/W2032944446",
    "https://openalex.org/W2910655660",
    "https://openalex.org/W3165729427",
    "https://openalex.org/W4211249244",
    "https://openalex.org/W4214854488",
    "https://openalex.org/W4210555639",
    "https://openalex.org/W3137191419",
    "https://openalex.org/W3154512708",
    "https://openalex.org/W4385151993",
    "https://openalex.org/W4381885522",
    "https://openalex.org/W4387188337",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4294068665",
    "https://openalex.org/W4379984088",
    "https://openalex.org/W4289656123",
    "https://openalex.org/W4386649074",
    "https://openalex.org/W4387385639",
    "https://openalex.org/W4393906060",
    "https://openalex.org/W3097353710",
    "https://openalex.org/W4319069095",
    "https://openalex.org/W4312845279",
    "https://openalex.org/W1965309615",
    "https://openalex.org/W3101195009"
  ],
  "abstract": "In recent years, deep-learning-based hyperspectral unmixing techniques have garnered increasing attention and made significant advancements. However, relying solely on the use of convolutional neural network (CNN) or transformer approaches is insufficient for effectively capturing both global and fine-grained information, thereby compromising the accuracy of unmixing tasks. In order to fully harness the information contained within hyperspectral images, this article explores a dual-stream collaborative network, referred to as TCCU-Net. It end-to-end learns information in four dimensions: spectral, spatial, global, and local, to achieve more effective unmixing. The network comprises two core encoders: one is a transformer encoder, which includes squeeze-launch modules, DSSCR&#x2013;vision transformer modules, and stripe pooling modules, while the other one is a CNN encoder, which is composed of two-dimensional (2-D) pyramid convolutions and 3-D pyramid convolutions. By fusing the outputs of these two encoders, the semantic gap between the encoder and decoder is bridged, resulting in improved feature mapping and unmixing outcomes. This article extensively evaluates TCCU-Net and seven hyperspectral unmixing methods on four datasets (Samson, Apex, Jasper Ridge, and Synthetic dataset). The experimental results firmly demonstrate that the proposed approach surpasses others in terms of accuracy, holding the potential to effectively address hyperspectral unmixing tasks.",
  "full_text": " \n1 \n \nTCCU-Net: Transformer and CNN Collaborative \nUnmixing Network for Hyperspectral image \n \nJianfeng Chena, Chen Yanga,d,*, Lan Zhanga, Linzi Y anga, Lifeng Bianb, Zijiang Luoc, Jihong Wanga,d,* \n \nAbstract-In recent years, deep learning -based hyperspectral \nunmixing techniques have garnered increasing attention and made \nsignificant advancements. However, relying solely on the use of \nCNN or Transformer approaches is insufficient for effectively \ncapturing both global and fine -grained information, thereby \ncompromising the accuracy of unmixing tasks.  In order to fully \nharness the information contained within HSIs, this study explores \na dual-stream collaborative network, referred to as TCCU-Net. It \nend-to-end learns information in four dimensions: spectral, spatial, \nglobal, and local, to achieve more effective unmixing. The network \ncomprises two core encoders: one is a Transformer encoder, which \nincludes squeeze -launch mod ules, DSSCR -VIT modules, and \nstripe pooling modules, while the other one is a CNN encoder, \nwhich is composed of 2D pyramid convolutions and 3D pyramid \nconvolutions. By fusing the outputs of these two encoders, the \nsemantic gap between the encoder and decod er is bridged, \nresulting in improved feature mapping and unmixing outcomes.  \nThis study extensively evaluates TCCU -Net and seven \nhyperspectral unmixing methods on four datasets (Samson, Apex, \nJasper Ridge  and Synthetic dataset). The experimental results \nfirmly demonstrate that the proposed approach surpasses others \nin terms of accuracy, holding the potential to effectively address \nhyperspectral unmixing tasks. \nIndex Termsâ€”Hyperspectral image Unmixing (HSU), C NN, \nTransformer, Global and Local Information, Spectral and \nSpatial information \n* Corresponding authors.  \nE-mail addresses: eliot.c.yang@163.com (C. Yang), \nE-mail addresses: wjihong20@163.com \na Power Systems Engineering Research Center, Ministry of \nEducation, College of Big Data and Inform ation Engineering, \nGuizhou University, Guiyang 550025, China \nb Frontier Institute of Chip and System, Fudan University, \nShanghai 200433, China \nc. Institute of Intelligent Manufacturing, Shunde Polytechnicï¼Œ\nGuangdong Shunde 528300, China \nd China State Key La boratory of Public Big Data, Guizhou \nUniversity, Guiyang 550025, China \nI INTRODUCTION \nyperspectral imaging is a highly regarded remote  \nsensing technology. Hyperspectral images (HSI) [1] \ncombine spectral information reflecting material \nradiation with spatial information of the terrain. Due to the rich \nspectral and spatial information within HSIs, they find extensive \napplications in fields such as food safety [2], environmental \nmonitoring [3], mineral exploration [4], and so on. However, \ndue to the spatial resolution limitations of hyperspectral imaging \ninstruments [5], pixels in HSIs often consist of mixed spectra \n[6], [7], representing a combination of various materials, known \nas mixed pixels. In practical applications, the abundance of \nmixed pixels can significantly impact the accuracy of pixel -\nbased material classification and area measurement methods, \nmaking the development and applicati on of hyperspectral \nimagery more challenging. To tackle this issue, there are \ntypically two approaches to contemplate: The first involves \nenhancing the spatial resolution of the spectrometer, which \ninevitably leads to increased human and financial costs. T he \nsecond approach, hyperspectral unmixing [8], is often chosen to \nreduce costs. The primary objective of spectral unmixing is to \nextract/estimate endmembers and their abundance fractions in \neach pixel solely based on the observed HSI [9]. \nAmong the numerous methods for hyperspectral unmixing, \nthe Linear Spectral Mixture Model (LSMM) [10] stands out for \nits simplicity, efficiency, and its ability to provide a good \ndescription of real spectral mixing processes. Expanding upon \nthe LSMM framework, resear chers have introduced several \neffective unmixing algorithms, including some of the most \nrepresentative ones, such as geometric, statistical, or sparse \nmethods. In the realm of geometric methods, Vertex Component \nAnalysis (VCA) [11] and Fully Constrained Le ast Squares \nUnmixing (FCLSU) [12] are the most commonly employed \ntechniques. In the field of sparse unmixing methods, on one \nhand, collaborative methods such as the Least Absolute \nShrinkage and Selection Operator (LASSO) [13] consider  the \nspectral variabil ity of endmembers by utilizing a dictionary \ngenerated from the data itself in a specific set of sparse \nH \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n2 \n \nunmixing methods. On the other hand, sparse unmixing \ntechniques like Splitting and Augmented Lagrangian -based \nsparse unmixing (SUnSAL) [14] represent ano ther prominent \nexample due to its exceptional performance, garnering \nsignificant attention. The statistical-based approaches also serve \nas effective alternatives for handling highly mixed HSIs, \ndrawing substantial interest. For instance, the Bayesian \nframework formulates unmixing as an inference problem, \nleveraging statistical assumptions and priors to constrain \nunmixing results [15] -[17]. Due to the unique advantages in \nlearning component -based representations, Non -Negative \nMatrix Factorization (NMF) [18] and L1/2-NMF [19] are two \nof the most frequently employed algorithms within the \nstatistical methods category for the simultaneous estimation of \nboth endmembers and abundance.  Jing Yao and his colleagues \nleveraged the insightful properties of natural  HSI, specifically \nthe non-local smoothness, to propose novel blind hyperspectral \nunmixing models, NLTV/NLHTV , and logarithm and \nregularization-based nonnegative matrix factorization (NLTV -\nLSRNMF/NLHTV-LSRNMF) [20], achieving widespread \napplications. \nIn recen t years, with the rise of deep learning, various \nconvolutional neural network (CNN)-based methods in the field \nof hyperspectral unmixing have experienced rapid development. \nAmong these, the EGU -Net proposed by Danfeng Hong et al. \n[21] for endmember -guided unmixing has introduced the \nconcept of using endmembers to guide the unmixing network. \nIt represents the first instance of utilizing such techniques in \nunmixing research, offering new insights for the future \ndevelopment of unmixing studies. Lin Qi et al. i ntroduced the \nSSCU-Net [22] , which employs spectral -spatial cooperative \nnetworks for unmixing tasks. This network is the first to \nincorporate spectral -spatial cooperation into the unmixing \nprocess via dual-branch tasks. Zhu Han et al. presented the MU-\nNet [23], designed for hyperspectral unmixing with multi -\nmodal inputs. This network employs two image inputs to guide \nthe unmixing network, addressing research gaps in unmixing \ntasks involving different inputs. Behnood Rasti et al. proposed \nthe Minimum Simplex  Convolutional Network (MiSiC -Net) \n[24], which combines spatial correlations between neighboring \npixels and the geometric properties of linear simplex. The \nRecurrent Consistency Unmixing Network (CyCU -Net) [25], \nintroduced by the Dongfeng Hong team, utiliz es two \nconvolutional autoencoders that are cascaded and cyclically \nexecuted. The proposed loss function includes two terms for \nspectral reconstruction and one for abundance reconstruction, \neffectively incorporating high-level semantic information. Jing \nYao et al. introduced a novel blind HU model called Sparse -\nEnhanced Convolutional Decomposition (SeCoDe -Net) [26]. \nThis model jointly captures the spatial -spectral information of \nHSI in a tensor -based manner.  In SeCoDe -Net, the use of \nconvolutional operations models the spatial relationships \nbetween target pixels and their neighboring pixels, providing a \nrobust explanation for the effectiveness of spectral bundles in \naddressing spectral variability. Simultaneously, it maintains \nphysically continuous spectral components by decomposing \ninvariance and spectral domains. Built upon sparse -enhanced \nregularization, the network also incorporates an alternative \noptimization strategy based on the Alternating Direction \nMethod of Multipliers (ADMM) to achieve efficient model \ninference. It can be observed that due to the outstanding \ngeneralization capability and accuracy of CNNs, they have \nmade significant strides in unmixing tasks. Other fields besides \nunmixing, such as , the GNet proposed by D. Hong et al. [27] \nhas achieved significant success in classification tasks. In \naddition to representing spectral -spatial features in three \nclassical paradigms (simultaneous, hierarchical, and individual), \nGNet can learn them in two new processes: multi -stage and  \nmulti-path. This approach allows for a comprehensive and \nbalanced exploration of spectral and spatial features. On the \nother hand, Chen Z et al.'s work on hyperspectral image \nclassification using spectral -induced superpixel segmentation \nbased on local agg regation and global attention network [28] \nintroduces a novel superpixel generation strategy termed \nSpectral-Induced Alignment Superpixel Segmentation. This \nstrategy simultaneously leverages segmentation results from \nHSI with both raw and deeply abstracted  spectral features. \nZhonghao Chen et al. propose a Temporal Difference Guided \nNetwork (TDGN) for hyperspectral image change detection \n[29]. Specifically, the network hierarchically extracts rich \nspectral features from dual temporal images, generating \ndifferences between the two images at various levels using \nconvolutional gated recursive units designed in the spatial \ndimension. These networks, as the latest research outcomes in \ndeep learning, significantly propel the development of various \ntasks related to hyperspectral imaging across different domains. \nRecent years, development of the Transformer [ 30], [31] \nhas achieved great success in NLP, while the Vision \nTransformer (VIT) [32] extends this architecture to the field of \ncomputer vision. It has showcased i ts distinctive capability \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n3 \n \nseparate from convolutional operations and has achieved \nexceptional results in image classification tasks. Such as the \nrecent CASST [ 33], which explores the use of cross -attention \nmechanisms for hyperspectral classification tasks.  And for \nexample, the ExViT model proposed by the Danfeng Hong team \n[34] utilizes parallel branches of position -shared VIT extended \nwith separable convolution modules to handle patches in \nmultimodal remote sensing images. This presents an \neconomical solution for leveraging both spatial and modality -\nspecific channel information. The entire VIT model structure \nconsists of several modules: the Embedding layer, Layer \nNormalization, Multi -Head Attention, Dropout, MLP Block, \nand MLP Head. The primary reason for the widespread adoption \nof VIT in the computer vision domain is its utilization of the \nmulti-head attention mechanism, which facilitates long -range \nfeature association calculations and establishes global feature \ndependencies. The expression for the multi -head attention is as \nEq. (1): \n  ğ‘´ğ’–ğ’ğ’•ğ’Šğ’‰ğ’†ğ’‚ğ’…(ğ‘¸, ğ‘², ğ‘½) =  ğ‘ªğ’ğ’ğ’„ğ’‚ğ’•(ğ‘¯ğ’†ğ’‚ğ’…ğŸ, ğ‘¯ğ’†ğ’‚ğ’…ğŸ, . . . , ğ‘¯ğ’†ğ’‚ğ’…ğ’‰) â‹… ğ‘¾ğ‘¶   (1)  \nIn this equation, Q, K, and V respectively denote queries, keys, \nand values, while ğ»ğ‘’ğ‘ğ‘‘ğ‘–   represents the computation result of \nthe i-th attention head. The variable \"h\" signifies the number of \nattention heads, and \"Concatenate\" denotes the process of \ncombining their outputs. ğ‘Šğ‘‚  signifies the final output weight \nmatrix.  \nRecently, Preetam Ghosh and colleagues introduced the \nDeep-Trans [35] network, marking the first attempt to apply the \nTransformer architecture to hyperspectral unmixing tasks, \nachieving remarkable research outcome s. This convincingly \ndemonstrates the viability of Transformers in image unmixing \ntasks. UnDAT [36], led by the team under the leadership of \nZhenwei Shi, aims to achieve unmixing tasks by simultaneously \nharnessing spatial uniformity and spectral correlatio ns within \nHSIs. The most recent Transformer network employed for \nhyperspectral unmixing is the innovative deep neural U -Net-\nbased model known as UST-Net [37], introduced by Zhiru Yang \nand colleagues. This network prioritizes discriminative \ninformation within the spatial scene and operates on the entire \nimage, eliminating inconsistencies.  SpectralGPT [38] is \nspecifically designed for processing hyperspectral remote \nsensing images using a novel 3D generative pre -trained \ntransformer (GPT). The research present ed in this article has \nalso provided us with new avenues for exploration. \nIn recent years, CNNs have been widely adopted by \nexperts and scholars in the field of hyperspectral unmixing, \nmaking significant contributions to the advancement of \nunmixing tasks. However, due to the complexity of HSI s, the \napplication of CNNs presents considerable challenges. This is \nprimarily because convolution operations are limited to \ncapturing local features determined by kernel sizes, resulting in \nthe loss of a substantial am ount of contextual information \npresent in the original HSI. On the hand, Transformers, \nrenowned for their outstanding performance in NLP tasks, have \nfound wide -ranging applications in various domains. \nResearchers have also harnessed Transformers in \ndecomposition tasks, as evidenced by recent developments like \nthe Trans -Net network. Trans -Net effectively applies the \nTransformer to hyperspectral data unmixing, yielding \nsignificant results. On the other hand, this paper aims to achieve \nsatisfactory performance  in the unmixing task by referencing \nseveral Transformer-based methods. For instance, the CUCaNet \nproposed by Danfeng Hong's team [39] introduces a \nTransformer-DSSCR-VIT with a cross -attention mechanism, \nanticipating satisfactory performance in unmixing ta sks. \nAdditionally, the Spectral â€“Spatial Morphological Attention \nTransformer proposed by Swalpa Kumar Roy et al. [40] has \nbeen a great source of inspiration. It implements a learnable \nspectral and spatial morphological network, utilizing spectral \nand spatia l morphological convolution operations (combined \nwith attention mechanisms) to enhance the interaction between \nstructural and shape information of HSI tokens and CLS tokens. \nHowever, standalone transformers may focus solely on \ncontextual information, poten tially overlooking finer details, \nand relying solely on neural networks may struggle to capture \nglobal information comprehensively. In addressing this issue, \nwe consulted various literature and noted that Danfeng Hong's \nteam proposed a new method called \"G lobal to Local: A \nHierarchical Detection Algorithm for Hyperspectral Image \nObject Detection\" [41]. This paper introduces a Global-to-Local \nHierarchical Target Detection (G2LHTD) algorithm for HSI, \nproviding us with a fresh perspective for our upcoming work. \nBuilding on the insights provided by the above-mentioned \nliterature, we have designed a novel dual -stream unmixing \nnetwork that combines Transformer and CNN architectures to \naddress the limitations associated with both. On one hand, this \napproach employs separate Transformer and CNN autoencoders \nto attend to spectral -spatial information and global -local \ninformation, respectively. On the other hand, by merging the \noutputs of the dual-stream network, it leads to improved quality \nabundance mapping and overal l unmixing results, aiding the \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n4 \n \ndecoder in better reconstructing the HSI. The proposed method \ncontributes to this purpose in the following ways: \n1.In this paper, we introduce a new transformer network, \nDSSCR_VIT, for the encoder part, which consists of three main \ncomponents. Firstly, it combines the Dual -stream Spectral -\nSpatial Cross Fusion (DSSCF) module with three fusion stages \nfor comprehensive learning of spectral spatial information. \nSecondly, channel squeeze-stretch modules are applied at both \nthe inpu t and output ends to facilitate the learning of detailed \nspectral information, deepening the network to achieve better \nunmixing effects for the Transformer, alleviating interference \nfrom excessive irrelevant details. Finally, each layer of DSSCR-\nVIT incorporates a stripe pooling module, enabling the model \nto effectively capture spatial information. On the one hand, this \nlogically structured setup provides a more stable configuration \nfor the Transformer encoder module for unmixing tasks; on the \nother hand, i t addresses the inherent limitations of the \nTransformer in focusing on image details. \n2.Unlike traditional neural networks with limited attention \nto global information and small receptive fields, we propose a \nnew Pyramid Convolution Network , that includes receptive \nfields of different scales, avoiding the problem of traditional \nconvolution being limited by receptive fields and affecting \nnetwork performance . Adopting a dual -stream structure, one \nbranch utilizes 3D pyramid convolution to fully explor e the \noptical frequency spectrum neighborhood information, while \nthe other branch uses 2D pyramid convolution to learn spatial \ninformation. Therefore, compared to traditional CNNs, our \nnetwork achieves comprehensive learning in both spectral and \nspatial domains, improving its unmixing performance. \n3.Based on the contributions mentioned above, the dual -\nstream network designed in this paper exhibits competitive \nunmixing performance on four datasets. Compared to three \ntraditional unmixing networks and four rec ently proposed deep \nlearning-based unmixing networks, our approach achieves \npromising unmixing results. \nII PROPOSED METHOD \nThis paper proposes a dual -branch codec network for \nhyperspectral unmixing based on a combination of CNN and \nTransformer. The encoder  section of the proposed network is \ndivided into two branches: a Transformer encoder branch and a \nCNN encoder branch, as shown in Fig.1. The Transformer \nencoder branch employs a channel extrusion structure, allowing \nsubsequent Transformers to focus on more relevant information \nwhile capitalizing on the Transformer's ability to handle long -\nterm tasks with deep networks. Additionally, three interlaced \ncross-VITs are added to the Transformer encoder, which are \nthen pooled by two sets of stripes. The cross -VITs utilize the \ncross-attention mechanism to learn the input pictures of \ndifferent patches separately. The stripe pooling deploys a long \nstrip of pooled core shape along a spatial dimension, capturing \nlong-distance relationships in isolated regions, thereby h elping \nthe cross-VIT learn more about image information. On the other \nhand, The CNN-based encoder adopts a dual -branch structure \nwith two branches: the 2D pyramid CNN branch and the 3D \npyramid CNN branch. The 2D pyramid branch consists of four \nconvolutional layers, each containing different levels of cores \nwith varying sizes and depths to capture varying levels of detail. \nThe 3D pyramid CNN branch uses 3 -layer 3D convolution to \nlearn spectral neighborhood information. Finally, the output of \nCNN-encoder and Transformer-encoder branches are fused and \nfed into a decoder comprising four convolutional layers for \nprocessing. The endmembers are then reconstructed using the \noutput abundance values. In the following sections of II-B to II-\nD, we would provide a detail ed discussion of the model \ncomponents. For section II -B, we will present the Transformer \nencoder from three perspectives: the extrusion module, the \nDSSCR module, and the stripe pooling module. Additionally, in \nII-C, we will discuss the CNN encoder from two  aspects: the \n3D-CNN module and the pyramid convolution module. Finally, \nin II-D, we will primarily focus on the decoder part of this article. \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n5 \n \nFHSI\nDSSCR1 Strip-Pool1 DSSCR2 Strip-Pool2 DSSCR3\nLayer3 Layer3\nTransformer-\nEncoder\nLayer1\nLayer4\nLayer2\nF\nCNN-Encoder\nLayer1\nLayer2\nLayer3\nLayer4\nDecoder\nAbu\nEnd\nLayer1\nLayer1\nDSSCR-VIT \nBlock\nStripe-pooling \nBlock\nChannel \nSqueeze Block\nChannel \nRecovery Block\n3D-Py Conv\nConv 1Ã—1\nConv \n3Ã—3+Softmax\nAbundance\nConv \n1Ã—1+ReLU\nEndmember\nFusion\n2D-Conv\nHyperspectral \nimage\n2D-Py Conv\nLayer n(n=1,2,3,4,...):Indicates the \nnumber of convolutional layers\nFig.1 Proposed network framework  \nA. Related Issues \n1. Formulas and related symbolic representations: \nThe symbols involved in this paper are represented by the \nfollowing: Setting up HSIs are indicated by ğ¼ âˆˆ â„ğµÃ—ğ»Ã—ğ‘Š,where \nthe spatial dimension is represented by  ğ» Ã— ğ‘Š, and the spectral \nchannels are denoted by B. A HSI can be reshaped to produce a \nmatrix ğ‘Œ = [ğ‘¦1, ğ‘¦2, ğ‘¦3. . . ğ‘¦ğ‘›] âˆˆ â„ğµÃ—ğ‘›, ğ‘› = ğ» âˆ™ ğ‘Š  is registered \nby the number of hyperspectral pixels,  ğ‘¦ğ‘–   represents the ith \nobserved spectrum. The endmember matrix will be displayed \nas ğ¸ = [ğ‘’1, ğ‘’2, ğ‘’3. . . ğ‘’ğ‘…] âˆˆ â„ğµÃ—ğ‘… , ğ‘’ğ‘–  is the  ith  endmember \nvector, and the number of endmembers present in HSI \nare represented by R. The corresponding abundance cube is \nrepresented by  ğ‘€ âˆˆ â„ğ‘…Ã—ğ»Ã—ğ‘Š . The abundance cube can be \nreshaped to produce a matrix of  ğ´ = [ğ‘1, ğ‘2, ğ‘3. . . ğ‘ğ‘›] âˆˆ â„ğ‘…Ã—ğ‘›, \nğ‘ğ‘– represents the fractional abundance corresponding to the ith \nobserved pixel. \nThe LMM model has been widely used for unmixing, and \nthe reflectance observed in LMM is Eq. (2): \nğ’€ = ğ‘¬ğ‘¨ + ğ‘µ                   (2) \nThe ğ‘ âˆˆ â„ğµÃ—ğ‘›  is the additive noise present in Y. In addition, \nthree physical constraints should usually be met in the task of \nunmixing: Firstly, the endmember matrix should be non -\nnegative \n0ï‚³ E  . Secondly, it is necessary to satisfy the non -\nnegative constraint of abundance ğ´ğ‘ğ¶(ğ´ â‰¥ 0) . Finally, \nabundance and one constraint ğ´ğ‘†ğ¶(1ğ‘…\nğ‘‡ ğ´ = 1ğ‘›\nğ‘‡ ) , the formula \n1ğ‘› is indicated N-dimensional column vector of 1. \n2. Encoder: \nIn the field of hyperspectral unmixing, autoencoder models \nhave emerged as a representative deep learning technique due \nto their strong representation and reconstruction capabilities, \nwhich allow them to extract information from given inputs. In \nthis article, autoencoder is composed of two parts: a CNN -\nencoder and a Transformer -encoder. The encoder takes input \npixels ğ‘¦ğ‘– âˆˆ â„ğ‘› and transforms them into a hidden low -\ndimensional representation ğ‘£ğ‘– âˆˆ â„ğ‘…using the following formula: \nğ’—ğ’Š = ğ‘­ğ‘«(ğ’šğ’Š) = ğ‘­(ğ‘¾(ğ’…)ğ‘»ğ’šğ’Š + ğ’ƒ(ğ’…))        (3) \nHere, ğ¹(. )  is a non -linear activation function such as sigmoid \nor ReLU, while ğ‘Š(ğ‘‘) ğ‘ğ‘›ğ‘‘ ğ‘(ğ‘‘)  represent the weight and bias in \nthe dth encoder part. \n3.Decoder: \nThe main task of the decoder is to convert the extracted \nhidden features into their corresponding original input pixels \nusing linear mixing models (LMM). The reconstructed pixels, \ndenoted by  ğ‘¦Ì‚ğ‘– âˆˆ â„ğ‘› are computed as: \n ğ’šÌ‚ğ’Š = ğ’‡ğ‘¬(ğ’—ğ’Š) = ğ‘¾(ğ’†)ğ‘»ğ’—ğ’Š             (4) \nHere, ğ‘Š(ğ‘’) is the weight matrix of the decoder part. The \nextracted endmembers matrix  ğ‘’Ì‚ğ‘– and the estimated abundance \nvector  ğ‘Ì‚ğ‘– correspond to ğ‘Š(ğ‘’)and ğ‘£ğ‘–, respectively. \n4. Loss Function: \nThe objective function of the autoencoder u nmixing \nnetwork is to measure reconstruction error (RE) which  ğ‘¦ğ‘–  \nand ğ‘¦Ì‚ğ‘–  are used by different measurement forms. In this paper, \ntwo loss function are used that consist of mean square error \n(MSE) and spectral angular distance (SAD): \n        ğ‘³ğ‘¹ğ‘¬(ğ’šğ’Š, ğ’šÌ‚ğ’Š) =\nğŸ\nğ‘¯â‹…ğ‘¾ âˆ‘ âˆ‘ (ğ’šÌ‚ğ’Š âˆ’ ğ’šğ’Š)ğ‘¾\nğ’‹=ğŸ\nğ‘¯\nğ’Š=ğŸ\nğŸ\n    (5)  \n ğ‘³ğ‘ºğ‘¨ğ‘«(ğ’šğ’Š, ğ’šÌ‚ğ’Š) =\nğŸ\nğ‘¹ âˆ‘ ğœğ¨ğ¬âˆ’ğŸ(\nğ’šÌ‚ğ’Š\nğ‘»ğ’šğ’Š\nâ€–ğ’šÌ‚ğ’Šâ€–ğŸâ€–ğ’šğ’Šâ€–ğŸ\nğ‘¹\nğ’Š=ğŸ )    (6)  \nThe MSE objective function is used to calculate the RE loss, \nwhich enables the encoder to learn only the fundamental \nfeatures of the  input HSI while discarding irrelevant details . \nHowever, MSE is sensitive to differences in absolute \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n6 \n \nmagnitude, which can lead to problems in distinguishing \nendmembers in HSI unmixing. To address this, the sum of \nabsolute differences (SAD) loss, which is a scale -invariant \nobjective function, is added to the model. The SAD loss helps \nto mitigate the shortcomings of the MSE objective function \nand accelerate model convergence. The total loss is  obtained \nby adding the RE loss and the SAD loss: \n ğ‘³ = ğğ‘³ğ‘¹ğ‘¬ + â„‡ğ‘³ğ‘ºğ‘¨ğ‘«            (7)  \nThe regularization parameter is ğœ• and â„‡. \nB. Transformer Encoder  \nThis article proposes a novel spectral squeezed \nTransformer structure design for the encoding component of the \nunmixing task, which aims to capitalize on the exceptional  \ncapabilities of Transformers in processing long -range \ndependencies to improve learning in unmixing tasks. Then, we \nwill introduce its structure from three components. \n1.Squeeze -Recovery Block:  \nThe structure of the block is demonstrated in Fig.2, which \nhas two main functions: the first is to compress the spectral \nchannel so that the next layer Transformer Module (DSSCR -\nVIT) can better focus on the global information of the input HSI. \nThe second is to restore the spectral channel, which elevates the \noriginal HSI from B channel to P channelï¼ˆğ¼ âˆˆ â„ğµÃ—ğ»Ã—ğ‘Šâˆ’>\nğ¼ âˆˆ â„ğ‘ƒÃ—ğ»Ã—ğ‘Š)  for adjusting the output tensor dimension the \nsame as the CNN encoder branch. To achieve the above \nfunctions, the proposed structure consists of two parts: a spectral \nchannel compression and restoration module.  \nThe spectral channel compression module consists of three \nlayers of 2D convolution, which it gradually compresses the \nfeatures of the input image ğ¼ âˆˆ ğ‘…ğµÃ—ğ»Ã—ğ‘Š to ğ¼â€² âˆˆ â„ğ»Ã—ğ‘ŠÃ—ğ¿, where \nL can be represented as  ğ¿ =\n(ğ‘‘ğ‘–ğ‘šâˆ—ğ‘…)\nğ‘ğ‘ğ‘¡ğ‘â„2  . In this expression, dim \nrepresents the dimension of the spectral band mapping, R \nrepresents the number of endmembers and p is the patch size. \nAs shown in Fig.2, it can be seen that after the compression \nmodule, the spectral channel is compressed while the spatial \nscale remains unchanged. The above process can be represented \nby the following formula: \n ğ‘°ğŸ = ğ‘ªğ’ğ’ğ’—ğŸ(ğ‘¾ğŸğ‘° + ğ‘½ğŸ), ğ¢ğ§ ğ°ğ¡ğ¢ğœğ¡ ğ‘°ğŸ âˆˆ â„ğ‘©/ğŸÃ—ğ‘¯Ã—ğ‘¾   (8)  \n ğ‘°ğŸ = ğ‘ªğ’ğ’ğ’—ğŸ(ğ‘¾ğŸğ‘°ğŸ + ğ‘½ğŸ) , ğ¢ğ§ ğ°ğ¡ğ¢ğœğ¡ ğ‘°ğŸ âˆˆ â„ğ‘©/ğŸ’Ã—ğ‘¯Ã—ğ‘¾  (9)  \nğ‘°ğŸ‘ = ğ‘ªğ’ğ’ğ’—ğŸ‘(ğ‘¾ğŸ‘ğ‘°ğŸ + ğ‘½ğŸ‘), ğ¢ğ§ ğ°ğ¡ğ¢ğœğ¡ ğ‘°ğŸ‘ âˆˆ â„ğ‘³Ã—ğ‘¯Ã—ğ‘¾   (10)  \n ğ‘°â€² = ğ‘°ğŸ‘\nğ‘», ğ¢ğ§ ğ°ğ¡ğ¢ğœğ¡ ğ‘°â€² âˆˆ â„ğ‘¯Ã—ğ‘¾Ã—ğ‘³      (11)  \nAmong them, ğ¶ğ‘œğ‘›ğ‘£1(âˆ™) , ğ¶ğ‘œğ‘›ğ‘£2(âˆ™)  and ğ¶ğ‘œğ‘›ğ‘£3(âˆ™)  represents a \nthree-layer convolutional layer, ğ‘Š1, ğ‘Š2, ğ‘Š3  and  ğ‘‰1, ğ‘‰2, ğ‘‰3 \nrepresents the weight and bias of each layer, and the superscript \nT represents the transpose operation of the matrix. \nThe spectral channel recovery module is also composed of \nthree layers of 2D convolutional layers, which perform the \nopposite operation of the compression module except the output \nchannel is P. The P is an hyperparameter can be set, which used \nto adjust and optimize network performance \n \nSqueeze-Recovery Block\nSqueeze-Channel\nBÃ— HÃ— W\nB/2Ã— HÃ— W\nB/4Ã— HÃ— W\nLÃ— HÃ— W\nï¼šCONV-2D\nRecovery-Channel\nBÃ— HÃ— W\nPÃ— HÃ— WB/2Ã— HÃ— W\nLÃ— HÃ— W\nCONV-2D  ï¼š\nDSSCR-VITÃ—3\nStripe-poolÃ—2\nFig2 Squeeze-Recovery Block \n2.DSSCR-VIT Block: \nIn order to effectively analyze three-dimensional HSI data \nstructures with spatial spectral joint information, a new dual \nchannel Transformer structure with cross attention mechanism \nis designed that called DSSCR -VIT. DSSCR-VIT consists of \ntwo modules: a s pectral spatial dual branch module for \nextracting relevant dimensional features and preparing \nTransformer token sequences, and a DSSCF module for spectral \nand spatial feature interaction. The structure is shown in Fig.3, \nand the two modules in detail will be introduced below.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n7 \n \nE*dim\nDSSCR-VIT \nView+Upsample\nSpectral-patch\nSpatial-patch\nPosition+Feature-Embedding\nExternal Class Embedding\n0\n*\n1\nn\nm\n...\n1\n*\n0\n...\nSpectral\n-MLP\nSpartial-\nMLP\nCat\nCross  \nAttention\nSpectral \ntokenization\nSpatial \ntokenization\n...\n...\nCONV+Dropout+\nBN+ReLU\n Spectral-Spatial \ntokenization DSSCF Module\n \nFig.3 DSSCR-VIT Structure \n2.1. Spectral spatial dual branch module: \nThis module tokenizes the spatial and spectral feature \nsequences for input into the SSDCA module. The schematic \ndiagram of its structure is shown in Fig.4. Next, we will describe \nthe specific details of the two branches. \nFor spectral sequence branches, first of all, a sample pixel \nğ¼ğ‘  âˆˆ â„1Ã—1Ã—ğ¿ is extracted from the input HSI (ğ¼â€² âˆˆ â„ğ»Ã—ğ‘ŠÃ—ğ¿ ) and \nexpands into a sample cube ğ¼ğ‘ \nâ€² âˆˆ â„ğ‘˜Ã—ğ‘˜Ã—ğ¿  in its neighborhood, \nwhere k represents the spatial size of the sample cube. Then, \nshallow features of the input HSI are extracted through two \nconvolutional layers with kernels of 1Ã—1, the input sample cube \nis gradually Transformed into a spectral vector group ğ‘ â€² =\n{ğ‘ 1\nâ€² , ğ‘ 2\nâ€² , . . . , ğ‘ ğ‘›\nâ€² } âˆˆ â„ğ‘˜Ã—ğ‘˜Ã—ğ¿â€²\n , where ğ¿â€²  represents the size of the \nspectral channel after convolutional transformation and n \nrepresents the number of spectral bands. Finally, spectral \nsequence characteristics ğ‘ â€²â€² = {ğ‘ 1\nâ€²â€², ğ‘ 2\nâ€²â€², . . . , ğ‘ ğ‘›\nâ€²â€²} âˆˆ â„1Ã—ğ·   are \nfurther generated through a pooling layer and a fully connected \nlayer, where D represents the dimension of each spectral \nsequence feature after tokenization. \nFor the spatial sequence branch, similarly, the cube ğ¼ğ‘ \nâ€² âˆˆ\nâ„ğ‘˜Ã—ğ‘˜Ã—ğ¿  is firstly transfor med into a space vector group  ğ‘– =\n{ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘š} âˆˆ â„ğ‘˜â€²Ã—ğ‘˜â€²Ã—ğ¿ through a convolutional layer with a \nkernel of 3Ã—3, where ğ‘˜â€²  represents the size of the space after \nconvolutional transformation and m represents the number of \nsmall space blocks. Then, each small block is flattened to \ngenerate spatial sequence features ğ‘–â€² = {ğ‘–1\nâ€² , ğ‘–2\nâ€² , . . . , ğ‘–ğ‘š\nâ€² } âˆˆ â„1Ã—ğ‘‘ , \nwhere d represents the dimensionality of each spatial sequence \nfeature after tokenization. \nSpatial-branchSpectral-branch\n...\nCONV+BN+ReLU\nGAP+FC\n...\nCONV+Dropout+BN\n+ReLU\n... Flatten\n...\n...\nCONV+Dropout+BN+\nReLU\nCONV+BN+ReLU\n \nFig.4 Process of Spectral-Spatial tokenization \n \n \n \n \n \n \n \nğ¼ğ‘  âˆˆ â„1Ã—1Ã—ğ¿\n \nğ‘ â€² = {ğ‘ 1\nâ€² , ğ‘ 2\nâ€² , . . . , ğ‘ ğ‘›\nâ€² } âˆˆ â„ğ‘˜Ã—ğ‘˜Ã—ğ¿â€²\n \nğ‘– = {ğ‘–1, ğ‘–2, . . . , ğ‘–ğ‘š} âˆˆ â„ğ‘˜â€²Ã—ğ‘˜â€²Ã—ğ¿\n \nğ‘ â€²â€² = {ğ‘ 1\nâ€²â€², ğ‘ 2\nâ€²â€², . . . , ğ‘ ğ‘›\nâ€²â€²} âˆˆ â„1Ã—ğ·\n \nğ‘–â€² = {ğ‘–1\nâ€² , ğ‘–2\nâ€² , . . . , ğ‘–ğ‘š\nâ€² } âˆˆ â„1Ã—ğ‘‘ \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n8 \n \n2.2. Dual branch spectral spatial cross fusion module: \nThis article designs a DSSCF module to ensure the \nadequacy of spectral and spatial feature fusion, as shown in \nFig.5. By constructing a transformer based cross attention \nmechanism, this module performs a three -stage fusion of \nfeatures from dual channel inputs that interacting in the early, \nmiddle and late stages. \n The early fusion: For spectral space double branch, after \nembedding the tokenized spatial branch feature sequence ğ‘¥ğ‘ ğ‘ğ‘ \nand spectral branch feature sequence ğ‘¥ğ‘ ğ‘ğ‘’, the class tokens and \nfeature embeddings of each branch are obtained. The class \ntokens between the two branches are exchanged to concatenate \nthem with the feature embeddings of the other branch and sent \nto the multi head self -attention module that could achieve the \nfirst fusion between spatial and spectral features. \nThe intermediate fusion: Taking the spatial branch as an \nexample, the output ğ‘¥ğ‘ ğ‘ğ‘â€² of multi head self -attention is added \nto the output ğ‘¥ğ‘ ğ‘ğ‘’â€² of spectral branch multi head self -attention, \nas well as the spatial feature sequence ğ‘¥ğ‘ ğ‘ğ‘  to complete the \nsecond fusion that further interacts with spectral spatial \ninformation. \nThe late f usion: The output xspeâ€²â€²  and xspaâ€²â€²  after the \nsecond fusion are respectively spliced through the MLP layer to \ncomplete the third fusion, and thus the spectral and spatial \ninformation have been fully learned. \nReshaping Block: the output of the third fusion ğ¼ğ¸ âˆˆ\nâ„ğ¸Ã—ğ‘‘ğ‘–ğ‘š  is reshaped as a three dimensional ğ¼ğ¸\nâ€² âˆˆ â„ğ¸Ã—ğ»Ã—ğ‘Š , the \nğ¼ğ¸\nâ€²   spectral channel is restored to L (which can be represented \nas ğ¼â€² âˆˆ â„ğ¿Ã—ğ»Ã—ğ‘Š) to match the input of the next stage after the \nupsampling operation. The above process can be represented by \nthe following formula: \nğ’™ğ’”ğ’‘ğ’‚â€² = ğ‘¨ğ’•ğ’•ğ’†ğ’ğ’•ğ’Šğ’ğ’[ğ‘ªğ’ğ’ğ’„ğ’‚ğ’•(ğ’™ğ’„ğ’ğ’‚ğ’”ğ’”\nğ’”ğ’‘ğ’‚ , ğ’™ğ’‡ğ’†ğ’‚ğ’•ğ’–ğ’“ğ’†\nğ’”ğ’‘ğ’† )]    (12) \n ğ’™ğ’”ğ’‘ğ’‚â€²â€² = ğ‘¨ğ’…ğ’…(ğ’™ğ’”ğ’‘ğ’‚â€², ğ’™ğ’”ğ’‘ğ’‚, ğ’™ğ’”ğ’‘ğ’†â€²)          (13) \n  ğ‘°ğ‘¬ = ğ‘ªğ’ğ’ğ’„ğ’‚ğ’•{[ğ’‡ğŸ(ğ’™ğ’”ğ’‘ğ’†â€²â€²)], [ ğ’‡ğŸ(ğ’™ğ’”ğ’‘ğ’‚â€²â€²)]}       (14) \nğ‘°â€² = ğ‘¼ğ’‘ğ’”ğ’‚ğ’ğ’‘ğ’ğ’†[ğ‘­(ğ‘°ğ‘¬)]              (15) \nThe ğ‘¥ğ‘ğ‘™ğ‘ğ‘ ğ‘ \nğ‘ ğ‘ğ‘   is a class token for spatial branches,  ğ‘¥ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’\nğ‘ ğ‘ğ‘’  \nrepresents the feature embedding of spectral branches, \nğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(âˆ™) represents the multi head self -attention module, \nğ‘“(âˆ™)  represents the MLP processing process, ğ¹(âˆ™)represents the \nreshaping operation, and ğ‘ˆğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’(âˆ™) represents up-sampling. \nConcat Concat\nMulti-Head Attention Multi-Head Attention\n+ +\nDSSCF-Module\nFeature-\nEmbedding\nClass-\nEmbedding\nFeature-\nEmbedding\nClass-\nEmbedding\nSpatial-MLP Spectral-MLP\nLiner Liner Liner\nMatMul\nScale\nMask\nSoftmax\nMatMul\nConcat\nLiner\nQ K V\nConcat\nSpatial-\nBranch\nSpectral-\nBranch\nThe Early Fusion\nThe Intermediate \nFusion\nThe Late Fusion\nReshaping-Block\nView+Scale\nUpsample\nNext Module\n \nFig.5 Process of DSSCF Module\nğ‘ â€²â€² = {ğ‘ 1\nâ€²â€², ğ‘ 2\nâ€²â€², . . . , ğ‘ ğ‘›â€²â€²} âˆˆ â„1Ã—ğ· ğ‘–â€² = {ğ‘–1\nâ€² , ğ‘–2\nâ€² , . . . , ğ‘–ğ‘šâ€² } âˆˆ â„1Ã—ğ‘‘  \nğ‘¥ğ‘ ğ‘ğ‘’ ğ‘¥ğ‘ ğ‘ğ‘ \nğ‘¥ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’\nğ‘ ğ‘ğ‘’  ğ‘¥ğ‘“ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’\nğ‘ ğ‘ğ‘  \nğ‘¥ğ‘ğ‘™ğ‘ğ‘ ğ‘ \nğ‘ ğ‘ğ‘’  ğ‘¥ğ‘ğ‘™ğ‘ğ‘ ğ‘ \nğ‘ ğ‘ğ‘  \nğ‘¥ğ‘ ğ‘ğ‘’â€² ğ‘¥ğ‘ ğ‘ğ‘â€² \nğ‘¥ğ‘ ğ‘ğ‘’â€²â€² ğ‘¥ğ‘ ğ‘ğ‘â€²â€² \nğ¼ğ¸ âˆˆ â„ğ¸Ã—ğ‘‘ğ‘–ğ‘š \nğ¼ğ¸\nâ€² âˆˆ â„ğ¸Ã—ğ»Ã—ğ‘Š \nğ¼â€² âˆˆ â„ğ¿Ã—ğ»Ã—ğ‘Š \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n9 \n \n3.Stripe-pool Block: \nThe stripe-pool block is designed for the output of DSSCR-\nVIT to expect further fine -grained learning of image features, as \nshown in Fig.6. The stri pe pooling module is divided into two \nparallel branches horizontal pooling and vertical pooling that \nperform pooling operations along corresponding direction. \nMathematically, the 3D feature image denoted as ğ¼â€² âˆˆ\nâ„ğ¿Ã—ğ»Ã—ğ‘Šundergoes a series of polling and convolution operations \nto derive the values ğ‘¦ğ‘–\nâ„ and ğ‘¦ğ‘—\nğ‘¤ . Notably, the horizontal pooling \noutput ğ‘¦ğ‘–\nâ„ can be formally defined as eq (16): \n       ğ’šğ’Š\nğ’‰ =\nğŸ\nğ‘¾ âˆ‘ ğ‘°ğ’Š,ğ’‹\nâ€²\nğŸâ‰¤ğ’‹â‰¤ğ‘¾                (16) \nVertical pooling output ğ‘¦ğ‘—\nğ‘¤ can be represented as: \nğ’šğ’‹\nğ’˜ =\nğŸ\nğ‘¯ âˆ‘ ğ‘°ğ’Š,ğ’‹\nâ€²\nğŸâ‰¤ğ’‹â‰¤ğ‘¯                 (17) \nIn the formula, the i and j represent the positions of feature image \npixels, and h and w represent the spatial range of pooling. And \nthen  ğ‘¦ğ‘–\nâ„ and ğ‘¦ğ‘—\nğ‘¤ is fused as follows: \nğ’€ = ğ’šğ’Š\nğ’‰ + ğ’šğ’‹\nğ’˜                  (18) \nFinally, the output z is obtained by a convolution and \nsigmoid function that can be calculated as: \n  ğ’› = ğˆğ’‡(ğ’€)              (19)  \nAmong them, Ïƒ represents the sigmoid function, and ğ‘“() \nrepresents the 1Ã—1 convolution. \nConv\n3Ã—1\nConv\n1Ã—3\n1*W\nH*1\nexpand\nexpand\nStripe-Pool Block\nVertical \npooling\nHorizontal \npooling\nSigm\nodF\nConv\n1Ã—1\n \nFig.6 Process of stripe pooling \nC. Multiscale Pyramid CNN Encoder \nWithin the context of the Multiscale Pyramid CNN encoder \nbranch, this article employs a dual -branch encoding framework \nthat integrates 2D pyramid convolution and 3D pyramid \nconvolution. This configuration facilitates the unde rstanding of \nthe input feature image, as depicted in Fig.7. Notably, the 2D \npyramid convolution branch is dedicated to acquiring spatial \nfeatures with varying receptive fields, while the 3D pyramid \nconvolutional branch is strategically designed to delve in to the \nrealm of richer spectral contextual information. \n1) 2D-Pyramid Conv: \nThe 2D Pyramid Convolutions mentioned in this article \nserve the purpose of comprehensively capturing the spatial \nattributes of images. This is achieved through the processing of \ninputs at various kernel scales, all while avoiding the escalation \nof computational expenses or model intricacy. \nIn this branch, the number of convolutional integral groups \nare set to 1, 4, 4, 16, and the stride is set to 1. The convolutional \nkernel size is shown in the Fig.7, the processing of  convolution \nand the ğ¼1\nâ€²â€² can be represented as: \n {ğ’™ğŸ = ğ‘ªğ’ğ’ğ’—ğŸ[ğ’‡ğŸ(ğ‘°)]\nğ’™ğŸ = ğ‘ªğ’ğ’ğ’—ğŸ[ğ’‡ğŸ(ğ‘°)]           {ğ’™ğŸ‘ = ğ‘ªğ’ğ’ğ’—ğŸ‘[ğ’‡ğŸ(ğ‘°)]\nğ’™ğŸ’ = ğ‘ªğ’ğ’ğ’—ğŸ’[ğ’‡ğŸ(ğ‘°)] (20)  \nğ‘°ğŸ\nâ€²â€² = ğ’™ğŸ + ğ’™ğŸ + ğ’™ğŸ‘ + ğ’™ğŸ’          (21)  \nAmong them, ğ‘“1() represents a convolution with a kernel size of \n1 Ã— 1  used to modulate the size of input channel, while \nConv 1,2,3,4  correspond to convolutions with kernel sizes of 3, \n5, 7, 9, respectively. \n2) 3D-Pyramid Conv: \nIn this branch, firstly a non-extrusion operation is performed \nto channel, which can be represented as: \nğ’™â€² = ğ’–ğ’ğ’”ğ’†ğ’’ğ’–ğ’†ğ’†ğ’›ğ’†(ğ‘°, ğŸ)             (22) \nSecondly, ğ‘¥â€² âˆˆ â„1Ã—ğ»Ã—ğ‘Š  is fed into two sub -branches with \ndifferent receptive fields, and then further fused to obtain ğ‘¥3\nâ€² . The \nprocess can be expressed as: \nğ’™ğŸ\nâ€² = ğ‘ªğ’ğ’ğ’—ğŸ‘ğ’…ğŸ(ğ‘ªğ’ğ’ğ’—ğŸ‘ğ’…ğŸ)             (23) \n   ğ’™ğŸ\nâ€² = ğ‘ªğ’ğ’ğ’—ğŸ‘ğ’…ğŸ’(ğ‘ªğ’ğ’ğ’—ğŸ‘ğ’…ğŸ‘)             (24) \nğ’™ğŸ‘\nâ€² = ğ’™ğŸ\nâ€² + ğ’™ğŸ\nâ€²                   (25) \nAmong them ğ¶ğ‘œğ‘›ğ‘£3ğ‘‘1ï¼Œğ¶ğ‘œğ‘›ğ‘£3ğ‘‘2ï¼Œğ¶ğ‘œğ‘›ğ‘£3ğ‘‘3ï¼Œğ¶ğ‘œğ‘›ğ‘£3ğ‘‘4  are \n3D convolutions of kernel sizes are 1Ã—1Ã—1 ï¼Œ9Ã—5Ã—5ï¼Œ1Ã—1Ã—1ï¼Œ\n9Ã—5Ã—5, respectively. \nFinally, after sequentially passing through a 1Ã—1Ã—1 \nconvolution and a sequence opera tion, the ğ¼2\nâ€²â€² âˆˆ â„ğ‘ƒÃ—ğ»Ã—ğ‘Š  is \nrestored to the same channel size of the 2D pyramid convolution \nbranch. It can be represented as: \nğ‘°ğŸ\nâ€²â€² = ğ’”ğ’’ğ’–ğ’†ğ’†ğ’›ğ’†(ğ‘ªğ’ğ’ğ’—ğŸ‘ğ’…ğŸ“(ğ’™ğŸ‘\nâ€² ), ğ‘·)         (26) \nAmong them ğ¶ğ‘œğ‘›ğ‘£3ğ‘‘5 is a 3D convolution with  kernel size of \n1Ã—1. \n \nğ‘¦ğ‘–\nâ„ \nğ‘¦ğ‘—\nğ‘¤ \nz ğ¼â€² âˆˆ â„ğ¿Ã—ğ»Ã—ğ‘Š \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n10 \n \n2D-Py-Conv\n3Ã—3 Conv\n5Ã—5 Conv\n7Ã—7 Conv\n9Ã—9 Conv +\n+\n+\n+\nConv3d\n1Ã—1Ã—1\nConv3d\n9Ã—5Ã—5\nConv3d\n9Ã—3Ã—3\n+ Conv3d\n1Ã—1Ã—1\n3D-Py-Conv\n1Ã—1 Conv\nUnsqueeze squeeze\nConv3d\n1Ã—1Ã—1\nCNN-Encoder\n \nFig.7 CNN-Encoder structure \nD. Decoder \nAfter fusing the output of the dual stream network, we will \nsend the output of the automatic encoder to the decoder module \nfor decoding. The specific operating steps are as follows: Firstly, \nthe decoder in this article compresses the channel to the \ncorresponding number of end elements in the dataset through a 4-\nlayer convolutional layer with a kernel of 1Ã—1. Secondly, after \nconvolutional layer processing, the accuracy of endme mber \nreconstruction and abundance estimation is improved by sharing \nweights. The estimated abundance is obtained through a \nconvolution and softmax function with a kernel of 3Ã—3. Finally, \nthe estimated abundance obtained from the output is passed \nthrough a convolutional layer with a kernel of 1Ã—1 to obtain the \nreconstructed endmember.  \nIII EXPERIMENTAL RESULTS  \nThis paper compares the results of the p roposed network \nwith seven different unmixing techniques, specifically FCLSU \n[12], ğ¿1 2â„  -NMF [19], Coolab [13], EGU -Net [21], MiSiC -Net \n[24], Trans-Net [35], and UST -Net [37], applied individually to \nthree real datasets (Samson, Apex, and Jasper Ridge) and one  \nsynthetic dataset.  \n \nA. Hyperspectral Dataset Description \nSamson dataset: The Samson hyperspectral dataset was \nemployed in this work, which included 156 different spectral \nchannels in the wavelength range of [401 -889] nm, as well as 95 \nÃ— 95 pixels. The true color image of the Samson dataset, as shown \nin Fig.8 (a), contained three endmembers: soil, trees, and water. \n Apex dataset: The cropped images of the Apex dataset used \nin this work were shown in Fig.8 (b). This image contained 110 \nÃ— 110 pixels and covered 285 different bands spanning spectral \nchannels of [413-2420] nm. There were four endmembers in this \nHSI, namely water, trees, roads, and roofs. \nSynthetic dataset: We created the simulated data based on the \nmethod followed by Fang et al. [42], and its RGB images were \nshown in Fig.8 (c). Its size was 104 Ã— 104 pixels, distributed over \n200 spectral bands, with four endmembers. We generated these \nmixed pixels by multiplying four endmembers and four \nabundance maps based on LMM. \nJasper Ridge Dataset: The original data comprises 512Ã—614 \npixels, with each pixel recorde d across 224 channels spanning \nfrom 380 to 2500nm. Due to the complexity of the original image, \nwhich is less conducive to the specific research at hand, we \nconsidered and adopted 100Ã—100  pixel sub-images in this paper, \nretaining a total of 198 channels, as illustrated in Figure 8(d). \n    \n(a)                      (b)                    (c)                   (d) \nFig.8 (a) Samson True -color image;(b) Apex True -color image; \n(c) Synthetic RGB image; (d) Jasper Ridge RGB image \nB. Description of experimental equipment and parameters \nThe proposed model has been researched on a PC equipped \nwith an i7 8750H core processor and NVIDA GTX 1050Ti GPU, \nusing a Python 3.8.0 interpreter. This study explores several \nhyperparameters across different datasets, which are summarized \nin Table I. As shown in the table, the regularization parameters Î´ \nand Îµ, which is used to adjust contribution of RE and SAD loss. \nFurthermore, detailed of training parameters of epoch, learning \nrate, and weight decay are also illustrated in the table. \n \n \n \n \n \n \nğ‘¥1 \nğ‘¥2 \nğ‘¥3 \nğ‘¥4 \nğ‘¥â€² âˆˆ â„1Ã—ğ»Ã—ğ‘Š \nğ¼ âˆˆ â„ğµÃ—ğ»Ã—ğ‘Š \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n11 \n \nTABLE I: INFORMATION ON HYPERPARAMETER SETTINGS \nHyperparameters  Samson  Apex  Synthetic \nğ›¿ 5ğ‘’3 1ğ‘’6 1ğ‘’5 \nğœ€ 3ğ‘’âˆ’3 9ğ‘’âˆ’1 1ğ‘’âˆ’4 \nEpoch  100  100  50  \nLearning rate  1ğ‘’âˆ’3 1ğ‘’âˆ’3 4ğ‘’âˆ’5 \nWeight decay  2ğ‘’âˆ’5 4ğ‘’âˆ’5 5ğ‘’âˆ’5 \nC. Experimental results and comparison  \nThis paper evaluates the performance of the proposed model \nand compares it with six different approaches on above three \ndatasets. These include three traditional unmixing methods: the \nFCLSU utilizing VCA for endmember extraction, the ğ¿1 2â„ -NMF \nemploying abu ndance sparsity for unmixing, and the collab \nconsidering spatial -spectral information for joint unmixing. \nAdditionally, three state -of-the-art deep neural network -based \nmethods are considered: the EGU -Net, an endmember -guided \nunmixing network; the MiSiC -Net, a spectral -spatial \ncollaborative network; and the Trans-Net, the first application of \na Transformer network for unmixing. \n \nTABLE II:  MEAN RMSE RESULTS OF THE SAMSON DATASET. THE BEST ONE IS SHO WN IN BOLD  \n \nTABLE III: MEAN SAD RESULTS OF THE SAMSON DATASET. THE BEST ONE IS SHOWN IN BOLD  \n \nTABLE IV:  MEAN RMSE RESULTS OF THE APEX DATASET. THE BEST ONE IS SHOWN IN BOLD  \n \n \n \n FCLSU ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-Net Trans-Net UST-Net proposed \nSoil 0.16712 0.15694 0.17232 0.19219 0.14861 0.07561 0.08095 0.06115 \nTree 0.12563 0.16788 0.07326 0.13112 0.11232 0.06135 0.13245 0.06277 \nWater 0.09634 0.16988 0.10261 0.23225 0.07167 0.09313 0.07641 0.03740 \nMean 0.12969 0.16490 0.11209 0.18518 0.11086 0.07962 0.10099 0.05501 \n VCA ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-Net Trans-Net UST-Net proposed \nSoil 0.13615 0.16763 0.09556 0.13115 0.09191 0.06891 0.03443 0.02059 \nTree 0.16552 0.16611 0.12553 0.12189 0.10295 0.08113 0.08919 0.06584 \nWater 0.09164 1.27861 0.26954 0.27234 0.09632 0.07326 0.06728 0.07644 \nMean 0.13110 0.53745 0.16354 0.17513 0.09706 0.07443 0.07963 0.05762 \n FCLSU ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-Net Trans-Net UST-Net proposed \nRoad 0.16164 0.68812 0.45631 0.25112 0.13166 0.12718 0.10771 0.12591 \nRoof 0.27631 0.28631 0.21664 0.15019 0.13568 0.12957 0.06418 0.07249 \nTree 0.15996 0.17112 0.10450 0.12223 0.12125 0.07132 0.09134 0.08345 \nWater 0.52369 1.82363 0.52011 0.22614 0.12317 0.05399 0.06830 0.04841 \nMean 0.28040 0.74229 0.32439 0.18742 0.12794 0.093115 0.08539 0.08256 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n12 \n \nTABLE V:  MEAN SAD RESULTS OF THE APEX DATASET. THE BEST ONE IS SHOWN IN BOLD  \n \nTABLE VI:  MEAN RMSE RESULTS OF THE SYNTHETIC DATASET. THE BEST ONE IS SHOWN IN BOLD  \n \nTABLE VII: MEAN SAD RESULTS OF THE SYNTHETIC DATASET. THE BEST ONE IS SHOWN IN BOLD  \n \nTABLE VIII:  MEAN RMSE RESULTS OF THE JASPER DATASET. THE BEST ONE IS SHOWN IN BOLD  \n \nTABLE IX:  MEAN SAD RESULTS OF THE JASPER DATASET. THE BEST ONE IS SHOWN IN BOLD  \n \nSamson dataset: The quantitative results on the Samson \ndataset can be found in Table II and III. The results indicate that \nthe proposed model outperforms other techniques in terms of \nabundance and endmember estimation in most cases. Among \nthem, the average  RMSE of our network is 0.05501, which is \n30.90% higher than the suboptimal method; and its average SAD \nis 0.05762, which is 22.58% higher than the suboptimal method. \nThe experimental results reveal the competitiveness of the \nproposed network on Samson dataset, and it proves the feasibility \nand superiority of the cross -fusion network between CNN and \nTransformer in the task of unmixing. \n \n VCA ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-\nNet \nTrans-Net UST-Net proposed \nRoad 0.24193 0.19234 0.31652 0.20913 0.17631 0.17891 0.15601 0.02418 \nRoof 0.13625 0.24991 0.20916 0.15136 0.12615 0.10256 0.11544 0.01848 \nTree 0.13022 0.23909 0.16538 0.24316 0.26139 0.12694 0.13540 0.01540 \nWater 0.14067 0.38965 0.17526 0.29936 0.42123 0.09135 0.23636 0.04411 \nMean 0.16226 0.26774 0.19158 0.22575 0.27127 0.12494 0.13581 0.03562 \n FCLSU ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-\nNet \nTrans-Net UST-Net proposed \n20dB 0.24631 0.21686 0.22329 0.15791 0.10125 0.15314 0.20566 0.03258 \n30dB 0.19657 0.22691 0.18691 0.14963 0.09631 0.12311 0.13517 0.02861 \n40dB 0.15639 0.18614 0.17631 0.14031 0.08912 0.10167 0.10542 0.02239 \n50dB 0.14998 0.15691 0.17665 0.13667 0.08011 0.09963 0.09208 0.01542 \nMean 0.18731 0.19670 0.1760 0.14613 0.09169 0.11938 0.14158 0.02475 \n VCA ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-\nNet \nTrans-Net UST-Net proposed \n20dB 0.28646 0.31191 0.29167 0.14125 0.05361 0.07326 0.03837 0.02791 \n30dB 0.24926 0.20165 0.21855 0.13517 0.04310 0.08112 0.02388 0.00455 \n40dB 0.23169 0.18715 0.14161 0.09364 0.02102 0.06132 0.02057 0.00306 \n50dB 0.16139 0.15189 0.08846 0.08710 0.01131 0.03962 0.00495 0.00274 \nMean 0.22322 0.21315 0.18072 0.11429 0.03226 0.06383 0.02195 0.00956 \n FCLSU ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-\nNet \nTrans-Net UST-Net proposed \nTree 0.76169 0.68812 0.16234 0.17939 0.13655 0.10718 0.11771 0.11037 \nWater 0.88731 0.91324 0.11369 0.26974 0.11794 0.10957 0.12037 0.08761 \nSoil 0.23791 0.30136 0.19367 0.31796 0.13125 0.12132 0.11961 0.10122 \nRoad 0.16645 028252 0.17922 0.23039 0.14074 0.09617 0.11279 0.08324 \nMean 0.51334 0.54631 0.16223 0.24937 0.12963 0.10856 0.11762 0.09561 \n VCA ğ¿1 2â„ -NMF Collab EGU-Net MiSiC-\nNet \nTrans-Net UST-Net proposed \nTree 0.94316 0.86923 0.23615 0.28933 0.15671 0.12916 0.15636 0.08105 \nWater 0.67133 0.67922 0.12975 0.20397 0.11652 0.09364 0.11544 0.10385 \nSoil 0.76712 0.61334 0.26377 0.48637 0.12937 0.10763 0.08671 0.07991 \nRoad 0.71435 072369 0.14651 0.49111 0.11648 0.09133 0.09063 0.08867 \nMean 0.77399 0.72137 0.19404 0.36769 0.12976 0.10544 0.11228 0.08837 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n13 \n \n Apex dataset: The quantitative results of the Apex dataset \nare shown in Tables IV and V . From the table, it can be seen that \nthe endmembers \"Road\" and \"Water\" in the Apex dataset pose a \nconsiderable challenge compared to most other methods. \nHowever, the proposed method greatly improves  the estimation \nof these two endmembers, mainly due to the proposed network's \ncollaborative learning strategy for spectral spatial information, \nwhich takes into account the geometric information of the \nendmembers and fully utilizes effective spectral bands. The \naverage RMSE value of this method is 34.26% higher than that \nof the suboptimal method, and the average SAD is 71.10% higher \nthan that of the suboptimal method. Furthermore, based on the \nSAD value, the proposed method provides the best endmember \nestimation for all endmembers. \nSynthetic Dataset: To evaluate the robustness of the \nproposed method to noise, we added Gaussian white noise with \ndifferent noise powers to the simulated dataset, and obtained data \nwith signal-to-noise ratios of 20, 30, 40, and 50 dB, respectively. \nThe endmember and abundance estimation results under different \nnoise conditions are shown in Tables VI and VII, including the \nresults of all alternative unmixing methods. Overall, on the one \nhand, as the signal-to-noise ratio increases, the abundance of each \nnetwork and the estimation results of endmembers have improved. \nOn the other hand, under different signal -to-noise ratios, most \ndeep learning based unmixing networks have better effects than \ntraditional methods that also reflects the superiority of deep \nlearning in the unmixing tasks. In addition, among several deep \nlearning-based methods, there are notable distinctions. One point \nto consider, the first Transformer-based unmixing task, Trans-Net, \noutperforms the classic CNN network, EGU -Net. However, it \nstill falls slightly short of MiSiC -Net, which leverages spectral -\nspatial information for unmixing. This underscores the vast \npotential for further exploration of Transformers in unmixing \ntasks. On the flip side, in comparison to alternative networks, the \nnetwork proposed in this paper yields superior unmixing results, \nexhibiting significant advantages in terms of RMSE and SAD \nacross all signal-to-noise ratios. This is primarily attributed to the \nproposed method's amalgamation of CNN and Transformer \nstrengths, cou pled with its comprehensive and meticulous \nlearning of both spatial and spectral information. \nJasper Ridge Dataset: Quantitative results in the Jasper \ndataset are presented in Tables VIII and IX. As evident from the \ntables, our approach outperforms most ot her methods in \nestimating the abundances of the four endmembers in the Jasper \ndataset, delivering competitive results. The method exhibits an \naverage RMSE value that is 11.93% higher than the second -best \nmethod, and an average SAD value that is 16.19% higher than the \nsecond-best method. Moreover, the proposed method provides \nthe best estimates for the abundances of all endmembers. Across \nthe four datasets mentioned above, our method consistently \nachieves the best average SAD and RMSE values, reaffirming the \nsuperiority of the approach introduced in this paper. \nD. Visual analysis \nAfter experimental research on different datasets and \ncomparative networks, we have decided to visually analyze the \nabundance and endmember maps generated from each dataset. \nFrom the abundance maps and endpoint maps, it can be seen that \nthe abundance maps and endpoint maps obtained by the proposed \nmethod in this paper are visually closest to ground truths (GTs) \ncompared to other comparison networks. Next, th e results on \neach dataset will be discussed one by one.\n \n \nFigure 9. Abundance map of the Samson dataset \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n14 \n \n \nFigure 10. Abundance map of the Apex dataset \n \nFigure 11. Abundance map of the Synthetic dataset \n \nFigure 12. Abundance map of the Jasper dataset \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n15 \n \n \nFigure 13. Endmember graph of the Samson dataset \n \nFigure 14. Endmember graph of the Apex dataset \n \nFigure 15. Endmember graph of the Synthetic dataset \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n16 \n \n \nFigure 16. Endmember graph of the Jasper dataset \n \nFor the Samson dataset as shown in Figures 9 and 13: The \ntraditional unmixing methods such as FCLSU, NMF, and Coolab \nhave shown inadequate results on the abundance and endmember \nplots. This is because their performance is affected by the \ndifferences between end elements, which limits their accuracy in \nunmixing tasks and leads to overall performance losses for these \nmodels. The overall effect of the EGU -Net method is relatively \ngood, but its performance on the end element \"Water\" is less \nsatisfactory. It pro bably due to the network mainly targets pure \nendmembers for unmixing, and the performance of this type of \ntask in mixed pixel unmixing is not outstanding. The MiSiC-Net, \nas one of the representative neural networks used in hyperspectral \nunmixing tasks in r ecent years, has also shown competitive \nunmixing results on Samson. Nevertheless, owing to the inherent \nconstraints of CNN, it fails to effectively capture global \ninformation, thereby leaving room for enhancement in the final \nreconstructed abundance and en dmember maps. As the first \nnetwork based on Transformer to handle unmixing tasks, Trans -\nNet has improved the accuracy of unmixing tasks. However, due \nto the fact that Transformers mainly focus on global information, \ntheir unmixing performance is usually no t as good as the \nproposed method. \nFor the Apex dataset as shown in Figures 10 and 14: we \nobserved that the proposed method on the \"Road\" endmember is \nclosest to the original abundance map, and relatively speaking,  \nother methods have lower accuracy in estim ating endmembers \nand abundance compared to the proposed method On the Roof \nand Tree endmembers, the Trans -Net and the proposed method \nare significantly superior to the previous methods; and on the \nWater endmember, the proposed method is also more competitive. \nThe significant enhancement in overall network performance \nprimarily stems from the collaborative training of CNN and \nTransformer architectures in this article. This collaborative \napproach empowers the network to acquire both global and \nintricate details, leading to a substantial improvement in network \nperformance. \nFor the Synthetic dataset as shown in Figures 11 and 15: it \ncan be seen that the deep learning methods proposed in recent \nyears have achieved better results compared to the classical \nunmixing methods. The MiSiC -Net, Trans-Net, as well as the \nabundance maps and endmember maps generated by the \nproposed methods in this article exhibit a visual resemblance that \nis closer to the GTs. This is mainly due to the stronger learning \nability of deep learning methods, as well as their wider coverage \nand better adaptability. \nFor the Jasper dataset, as depicted in Figures 12 and 16, we \nobserve that traditional methods like VCA and NMF fail to \ncorrectly estimate \"Tree\" and \"Water.\" This is due to the \nlimitations of traditional methods in capturing fine details in \nimages. In contrast, deep learning -based approaches perform \nsignificantly better, underscoring the superiority of deep learning \nover traditional methods.  Our proposed method also excels in \nestimating the \"Soil\" endmember, further demonstrating that our \nneural network is better at learning image details and contextual \ninformation compared to other deep learning methods. This \nhighlights the effectiveness of our proposed approach. \n \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n17 \n \nE. Ablation Study \nThis study employs a dual-branch encoder-decoder structure \nto tackle unmixing tasks across three different datasets. To \nvalidate the roles of various encoder components within the \nproposed network, we conducted ablation experiments on the \ndual-branch encoder. \n \nTABLE X: PERFORMANCE OF DIFFERENT ENCODER MODULES IN THE PROPOSED  \nNETWORK ON THREE DATASETS \n \n \n \n \n \n \nTABLE XI: THE PERFORMANCE OF THE PROPOSED NETWORK USING ONLY A SINGLE MODULE OF CNN ENCODER \nON THREE DATASETS \n \n \n \n \n \n \n \nTABLE XII: THE PERFORMANCE OF THE PROPOSED NETWORK USING ONLY TRANSFORMER ENCODER SINGLE OR \nTWO COMBINED MODULES ON THREE DATASETS \n \n \n \n \n \n \n \nFrom Table X, it is evident that using only a single -branch \nencoder in the proposed network yields inferior unmixing results \ncompared to the fusion achieved by the dual -branch approach. \nThis highlights a substantial enhancement in source separation \nwhen utilizing the dual -branch fusion. Notably, across all three \ndatasets, the CNN branch demonstrates higher effective \nseparation compared to the Transformer branch. We attribute this \nto the nature of the Transformer as a network that excels in tasks \nwith long-range dependencies, typically performing optimally in \ndeeper networks. \nTable XI presents experimental results on three datasets \nusing a single module from the CNN encoder. The results indicate \nthat the individual use of either the 2D pyramid module or the 3D \npyramid module is less effective than their combination. T his \nsubstantiates the rationality of the designed CNN encoder \nmodules. Additionally, it is observed that the 2D pyramid \nconvolution outperforms the 3D pyramid convolution slightly on \nall three datasets, likely due to the 3D pyramid convolution \ncompressing spectral channels, leading to the loss of some \nspectral information. \nTable XII illustrates the combinations of various \ncomponents within the Transformer encoder across the three \ndatasets that the performance remains subpar when compared to \nthe collaborativ e effect of all three modules. Furthermore, the \nperformance of the DSSCR -VIT module alone is the poorest \namong the three combinations, indicating that the inclusion of the \nSqueeze-launch module and stripe pooling module enhances the \nunmixing performance of the Transformer encoder. \n \n \n Encoder-CNN Encoder-Transformer Proposed \n SAD RMSE SAD RMSE SAD RMSE \nSamson 0.09632 0.10339 0.11325 0.12931 0.05762 0.05501 \nApex 0.10329 0.11361 0.09997 0.08622 0.06935 0.07302 \nSynthetic 0.05336 0.06562 0.06913 0.07931 0.00865 0.02434 \n 2D-pyramid-CNN+Transformer-\nEncoder \n3D-pyramid-CNN+Transformer-Encoder \n SAD RMSE SAD RMSE \nSamson 0.06133 0.05991 0.08726 0.07411 \nApex 0.09364 0.08972 0.09765 0.09963 \nSynthetic 0.02366 0.04115 0.02061 0.03697 \n DSSCR-VIT+CNN-Encoder DSSCR-VIT + Squeeze \n+CNN-Encoder \nDSSCR-VIT +Stripe-pool+ \nCNN-Encoder \n SAD RMSE SAD RMSE SAD RMSE \nSamson  0.07131 0.06933 0.065141 0.06639 0.06983 0.07016 \nApex 0.07539 0.08562 0.07128 0.07633 0.07496 0.08235 \nSynthetic 0.03697 0.04014 0.01326 0.03658 0.01063 0.03366 \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n18 \n \nIV . CONCLUSION \nIn this study, we introduced a hyperspectral unmixing \nnetwork based on VIT and Pyramid CNN, which exhibits \ncompetitive performance in unmixing accuracy compared to the \nbenchmark networks. The design of the squeeze -stretch module \nin the Transformer encoder of the proposed TCCU -Net helps \ndeepen the network while mitigating the interference of overly \ndetailed information. The inclusion of the DSSCF module \nenables the Transformer encoder to simultaneously focus on \nspectral and spatial information. Additionally, the use of the stripe \npooling module ensures that the Transformer encoder pays \nattention to local fine -grained details, further enhancing the \nnetwork's performance. In the CNN encoder, the 2D pyramid \nconvolution focuses on spatial information by employing \nconvolution kernels of varying scales, while the 3D pyramid \nconvolution module prioritizes spectral neighborhood \ninformation by configuring the spectral channels as one, thus \nallowing the CNN encoder to attend to  both spatial and spectral \ninformation simultaneously. To validate the effectiveness of the \nproposed TCCU-Net, extensive ablation studies were conducted \non the Samson, Apex, Jasper Ridge, and synthetic datasets. The \nexperimental results demonstrate that th e TCCU -Net is an \neffective unmixing method, consistently delivering robust \nunmixing performance across all four datasets. We look forward \nto the application of the TCCU -Net in relevant domains, \ncontributing to the advancement of hyperspectral unmixing. \nACKNOWLEDGMENT \nThe work was supported in part by the National Natural \nScience Foundation of China with Grant 62065003; Supported by \nGuizhou Provincial Science and Technology Projects â€“ZK [2022] \nKey-020; in part by Renjihe of Guizhou University (2012) . The \nauthors give thanks for the computing support of the State Key \nLaboratory of Public Big Data, Guizhou University. \nREFERENCE \n[1] Bioucas-Dias, J.; Plaza, A.; Camps-Valls, G.; Scheunders, P.; \nNasrabadi, N.M.; Chanussot, J. Hyperspectral remote sensing \ndata analysis and future challenges. IEEE Geosci. Remote Sens. \nMag. 2013, 1, 6â€“36. [CrossRef] \n[2] M. O. Smith, P. E. Johnson, and J. B. Adams, â€œQuantitative \ndetermi-nation of mineral types and abundances from reflectance \nspectra using principal component analysis ,â€ in Proc. Lunar and \nPlanetary Science Conf., 1985, vol. 90, pp. 797â€“904. \n[3] Z. Lv, H. Huang, X. Li, M. Zhao, J. A. Benediktsson, W. Sun, \nand N. Falco, â€œLand cover change detection with heterogeneous \nremote sensing images: Review, progress, and perspecti ve,â€ \nProceedings of the IEEE, vol. 110, no. 12, pp. 1976â€“1991, 2022. \n[4] Z. Lv, P. Zhong, W. Wang, Z. Y ou, and N. Falco, â€œMultiscale \nattention network guided with change gradient image for land \ncover change detection using remote sensing images,â€ IEEE \nGeoscience and Remote Sensing Letters, vol. 20, pp. 1â€“5, 2023. \n[5] N . K e s h a v a a n d J . F . M u s t a r d , â€œ S p e c t r a l u n \nm i x i n g , â€ IEEE Signal Process. Mag., vol. 19, no. 1, pp. 44 â€“\n57, 2002. \n[6] J. M. Bioucas-Dias, A. Plaza, G. Camps-Valls, P. Scheunders, \nN. M. Nasrabadi, and J. Chanussot, â€œHyperspectral remote \nsensing data analysis and future challenges,â€ IEEE Geosci. \nRemote Sens. Mag., v o l . 1 ,no. 2, pp. 6â€“36, Jun. 2013. \n[7] P .Ghamisi et al., â€œAdvances in hyperspectral image and \nsignal processing: A comprehensive overview of the state of the \nart,â€ IEEE Geosci. Remote Sens. Mag., vol. 5, no. 4, pp. 37 â€“78, \nDec. 2017. \n[8] R. O. Green, M. L. Eastwood, C. M. Sarture, T. G. Chrien, M. \nAron-s s o n, B . J . C h i p p e n d a l e, J A .F a u s t , B . E . P a \nv r i , C . J . C h o v i t , a n d M .Solis et al., â€œImaging spectroscopy \nand the airborne visible/infrared imaging spectrometer (A \nVIRIS),â€ Remote Sens. Environ., vol. 65, no.3, pp. 227 â€“248, \n1998. \n[9] G. Shaw and D. Manolakis, â€œSignal processing for \nhyperspectral image exploitation,â€ IEEE Signal Process. Mag., \nvol. 19, no. 1, pp. 12â€“16, 2002. \n[10] N. Keshava and J. F. Mustard, â€œSpectral unmixing,â€ IEEE \nsignal processing magazine, vol. 19, no. 1, pp. 44â€“57, 2002. \n[11] D. Manolakis and G. Shaw, â€œDetection algorithms for \nhyperspectral imaging applications,â€ IEEE Signal Process. Mag., \nvol. 19, no. 1, pp. 29â€“43, 2002. \n[12] D. C. Heinz and C. -I. Chang, â€œFully constrained least \nsquares linear spectral mixture analysis method for  material \nquantification in hyper -spectral imagery,â€ IEEE Trans. Geosci. \nRemote Sens., vol. 39, no. 3, pp. 529â€“545, Mar. 2001. \n[13] L. Drumetz, T. R. Meyer, J. Chanussot, A. L. Bertozzi, and \nC. Jutten, â€œHyperspectral image unmixing with endmember \nbundles and group sparsity inducing mixed norms,â€ IEEE Trans. \nImage Process., vol. 28, no. 7, pp. 3435 â€“3450, Jul. 2019, doi: \n10.1109/TIP .2019.2897254. \n[14] M.-D. Iordache, J. M. Bioucas -Dias, and A. Plaza, â€œSparse \nunmixing of hyperspectral data,â€ IEEE Trans. Geosc i. Remote \nSens., vol. 49, no. 6, pp. 2014â€“2039, Jun. 2011. \n[15] N. Dobigeon, S. Moussaoui, M. Coulon, J.-Y . Tourneret, and \nA. O.Hero, â€œJoint bayesian endmember extraction and linear \nunmixing for hyperspectral imagery,â€ IEEE Trans. Signal \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n19 \n \nProcess., vol. 57, no. 11,pp. 4355â€“4368, Nov. 2009. \n[16] O. Eches, N. Dobigeon, C. Mailhes, and J. -Y . Tourneret, \nâ€œBayesian estimation of linear mixtures using the normal \ncompositional model. application to hyperspectral imagery,â€ \nIEEE Trans. Image Process.,vol. 19, no. 6, p p. 1403â€“1413, Jun. \n2010. \n[17] J. M. Nascimento and J. M. Bioucas -Dias, â€œHyperspectral \nunmixing based on mixtures of dirichlet components,â€ IEEE \nTrans. Geosci. Remote Sens., vol. 50, no. 3, pp. 863 â€“878, Mar. \n2012. \n[18] J. Li, J. M. Bioucas -Dias, A. Plaza, a nd L. Liu, â€œRobust \ncollaborative nonnegative matrix factorization for hyperspectral \nunmixing,â€ IEEE Transactions on Geoscience and Remote \nSensing, vol. 54, no. 10, pp. 6076â€“6090, 2016. \n[19] Y Qian, S. Jia, J. Zhou, and A. Robles-Kelly, â€œHyperspectral \nunmixing via l {1/2} sparsity -constrained nonnegative matrix \nfactorization.â€ IEEE Transactions on Geoscience and Remote \nSensing, vol. 49, no. 11 pp. 4282â€“4297, 2011. \n[20] Yao J, Meng D, Zhao Q, Cao W, Xu Z. Nonconvex sparsity \nand Nonlocal-smoothness Based Blind Hyperspectral \nUnmixing. IEEE Trans Image Process. 2019 Jan 17. doi: \n10.1109/TIP.2019.2893068. Epub ahead of print. PMID: \n30668470 \n[21] D. Hong et al., \"Endmember -Guided Unmixing Network \n(EGU-Net): A General Deep Learning Framework for Self -\nSupervised Hype rspectral Unmixing,\" in IEEE Transactions on \nNeural Networks and Learning Systems, vol. 33, no. 11, pp. \n6518-6531, Nov. 2022, doi: 10.1109/TNNLS.2021.3082289. \n[22] L. Qi, F. Gao, J. Dong, X. Gao and Q. Du, \"SSCU -Net: \nSpatialâ€“Spectral Collaborative Unmixing  Network for \nHyperspectral Images,\" in IEEE Transactions on Geoscience and \nRemote Sensing, vol. 60, pp. 1 -15, 2022, Art no. 5407515, doi: \n10.1109/TGRS.2022.3150970. \n[23] Z. Han, D. Hong, L. Gao, J. Yao, B. Zhang and J. Chanussot, \n\"Multimodal Hyperspectral Unmixing: Insights From Attention \nNetworks,\" in IEEE Transactions on Geoscience and Remote \nSensing, vol. 60, pp. 1 -13, 2022, Art no. 5524913, doi: \n10.1109/TGRS.2022.3155794 \n[24] B. Rasti, B. Koirala, P. Scheunders and J. Chanussot, \n\"MiSiCNet: Minimum Simplex Convolutional Network for Deep \nHyperspectral Unmixing,\" in IEEE Transactions on Geoscience \nand Remote Sensing, vol. 60, pp. 1 -15, 2022, Art no. 5522815, \ndoi: 10.1109/TGRS.2022.3146904. \n[25] L. Gao, Z. Han, D. Hong, B. Zhang, and J. Chanussot, \nâ€œCycu-net: Cycle-consistency unmixing network by learning \ncascaded autoencoders,â€IEEE Transactions on Geoscience and \nRemote Sensing, pp. 1â€“14, 2021. \n[26] J. Yao, D. Hong, L. Xu, D. Meng, J. Chanussot and Z. Xu, \n\"Sparsity-Enhanced Convolutional Decomposition: A Novel  \nTensor-Based Paradigm for Blind Hyperspectral Unmixing,\" \nin IEEE Transactions on Geoscience and Remote Sensing , vol. \n60,pp.1-14,2022,Artno.5505014,doi: \n10.1109/TGRS.2021.3069845. \n[27] Z. Chen, D. Hong and H. Gao, \"Grid Network: Feature \nExtraction in Aniso tropic Perspective for Hyperspectral Image \nClassification,\" in IEEE Geoscience and Remote Sensing Letters, \nvol. 20, pp. 1 -5, 2023, Art no. 5507105, doi: \n10.1109/LGRS.2023.3297612. \n[28] Chen, Z., Wu, G., Gao, H., Ding, Y ., Hong, D., & Zhang, B. \n(2023). Loca l aggregation and global attention network for \nhyperspectral image classification with spectral -induced aligned \nsuperpixel segmentation. Expert Syst. Appl., 232, 120828. \n[29]Zhonghao Chen, Yuyang Wang, Hongmin Gao, Yao Ding, \nQiqiang Zhong, Danfeng Hong & Bing Zhang (2023) Temporal \ndifference guidednet work for hyperspectralimage \nchangedetection, InternationalJournalofRemoteSensing, 44:19, \n60336059, DOI: 10.1080/01431161.2023.2258563. \n[30] Vaswani, Ashish, Noam M. Shazeer, Niki Parmar, Jakob \nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia \nPolosukhin. â€œAttention is All you Need.â€  Neural Information \nProcessing Systems (2017).  \n[31] Jacob Devlin, Ming -Wei Chang, Kenton Lee, and Kristina \nToutanova. 2019.  BERT: Pre -training of Deep Bidirectional \nTransformers for Language Understanding. In Proceedings of the \n2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers) , pages 4171 â€“\n4186, Minneapolis, Minnesota. Association for Computational \nLinguistics. \n[32] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., \nZhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, \nG., Gelly, S., Uszkoreit, J., & Houlsby, N. (2020). An Image is \nWorth 16x16 Words: Transformers for Image Recognition at \nScale. ArXiv, abs/2010.11929.  \n[33] Y . Peng, Y . Zhang, B. Tu, Q. Li and W. Li, \"Spatialâ€“Spectral \nTransformer With Cross -Attention for Hyperspectral Image \nClassification,\" in IEEE Transactions on Geoscience and Remote \nSensing, vol. 60, pp. 1 -15, 2022, Art no. 5537415, doi: \n10.1109/TGRS.2022.3203476. \n[34] J. Yao, B. Zhang, C. Li, D. Hong and J. Chanussot, \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n \n20 \n \n\"Extended Vision Transformer (ExViT) for Land Use and Land \nCover Classification: A Multimodal Deep Learning Framework,\" \nin IEEE Transactions on Geoscience and Remote Sensing , vol. \n61, pp.  1-15, 2023, Art no. 5514415, doi: \n10.1109/TGRS.2023.3284671. \n[35] P. Ghosh, S. K. Roy, B. Koirala, B. Rasti and P. Scheunders, \n\"Hyperspectral Unmixing Using Transformer Network,\" in IEEE \nTransactions on Geoscience and Remote Sensing, vol. 60, \npp.116,2022 Artno.5535116,doi:10.1109/TGRS.2022.3196057. \n[36] Y . Duan, X. Xu, T. Li, B. Pan and Z. Shi, \"UnDAT: Double-\nAware Transformer for Hyperspectral Unmixing,\" in IEEE \nTransactions on Geoscience and Remote Sensing, vol. 61, pp. 1 -\n12,2023,Art no. 5522012, doi: 10.1109/TGRS.2023.3310155. \n[37] Z. Yang, M. Xu, S. Liu, H. Sheng and J. Wan, \"UST-Net: A \nU-Shaped Transformer Network Using Shifted Windows for \nHyperspectral Unmixing,\" in  IEEE Transactionson Geoscience \nand Remote Sensing,doi:10.1109/TGRS.2023.3321839. \n[38] Hong, D., Zhang, B., Li, X., Li, Y ., Li, C., Yao, J., Yokoya, \nN., Li, H., Ghamisi, P., Jia, X., Plaza, A., Gamba, P., \nBenediktsson, J.A., & Chanussot, J. (2023). SpectralGPT: \nSpectral Foundation Model. ArXiv, abs/2311.07113.  \n[39] Yao, J., Hong, D., Chanussot, J., Meng, D., Zhu, X., Xu, Z. \n(2020). Cross-Attention in Coupled Unmixing Nets for \nUnsupervised Hyperspectral Super-Resolution. In: Vedaldi, A., \nBischof, H., Brox, T., Frahm, JM. (eds) Computer Vision â€“ \nECCV 2020. ECCV 2020. Lecture Notes in Computer Science \n(), vol 12374. Springer, Cham https://doi.org/10.1007/978-3-\n030-58526-6_13 \n[40] S. K. Roy, A. Deria, C. Shah, J. M. Haut, Q. Du and A. Plaza, \n\"Spectralâ€“Spatial Morphological Attention Transformer for \nHyperspectral Image Classification,\" in  IEEE Transactions on \nGeoscience and Remote Sensing, vol. 61, pp. 1-15, 2023, Art no. \n5503615, doi: 10.1109/TGRS.2023.3242346 \n[41] Z. Chen et al., \"Global to L ocal: A Hierarchical Detection \nAlgorithm for Hyperspectral Image Target Detection,\" in IEEE \nTransactions on Geoscience and Remote Sensing, vol. 60, pp. 1 -\n15,2022,Art no. 5544915, doi: 10.1109/TGRS.2022.3225902. \n[42] Debes, C.; Merentitis, A.; Heremans, R.;  Hahn, J.; \nFrangiadakis, N.; van Kasteren, T.; Liao, W.; Bellens, R.; PiÅ¾urica, \nA.; Gautama, S. Hyperspectral and LiDAR data fusion: Outcome \nof the 2013 GRSS data fusion contest. IEEE J. Sel. T op. Appl. \nEarth Obs. Remote Sens. 2014, 7, 2405â€“2418. [CrossRef]. \n \nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2024.3352073\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Hyperspectral imaging",
  "concepts": [
    {
      "name": "Hyperspectral imaging",
      "score": 0.8207933902740479
    },
    {
      "name": "Computer science",
      "score": 0.7002864480018616
    },
    {
      "name": "Transformer",
      "score": 0.548704206943512
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48015540838241577
    },
    {
      "name": "Computer vision",
      "score": 0.39264997839927673
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3827083706855774
    },
    {
      "name": "Engineering",
      "score": 0.10235050320625305
    },
    {
      "name": "Voltage",
      "score": 0.10031971335411072
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}