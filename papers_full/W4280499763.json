{
  "title": "PTNet3D: A 3D High-Resolution Longitudinal Infant Brain MRI Synthesizer Based on Transformers",
  "url": "https://openalex.org/W4280499763",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287339076",
      "name": "Zhang, Xuzhe",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2742828811",
      "name": "He, Xinzi",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2086050698",
      "name": "Guo Jia",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4287339079",
      "name": "Ettehadi, Nabil",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4287339080",
      "name": "Aw, Natalie",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4287339081",
      "name": "Semanek, David",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4287339082",
      "name": "Posner, Jonathan",
      "affiliations": [
        "Duke University Hospital",
        "Duke Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4282282791",
      "name": "Laine, Andrew",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A1927958059",
      "name": "Wang Yun",
      "affiliations": [
        "Duke University Hospital",
        "Duke Medical Center"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2025655593",
    "https://openalex.org/W1896627062",
    "https://openalex.org/W1996817665",
    "https://openalex.org/W1582430325",
    "https://openalex.org/W2116895317",
    "https://openalex.org/W2788755901",
    "https://openalex.org/W2953130519",
    "https://openalex.org/W3028368183",
    "https://openalex.org/W2892324021",
    "https://openalex.org/W2984254165",
    "https://openalex.org/W3038918411",
    "https://openalex.org/W2963768110",
    "https://openalex.org/W2004129819",
    "https://openalex.org/W1605218991",
    "https://openalex.org/W1794421049",
    "https://openalex.org/W2015897296",
    "https://openalex.org/W2102849263",
    "https://openalex.org/W1983060851",
    "https://openalex.org/W2526871",
    "https://openalex.org/W2517395172",
    "https://openalex.org/W1979775925",
    "https://openalex.org/W2767044624",
    "https://openalex.org/W6745560452",
    "https://openalex.org/W2963800363",
    "https://openalex.org/W2963073614",
    "https://openalex.org/W3009961050",
    "https://openalex.org/W3194713134",
    "https://openalex.org/W3117981784",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6783944145",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2793631099",
    "https://openalex.org/W2963767194",
    "https://openalex.org/W2962793481",
    "https://openalex.org/W2982041717",
    "https://openalex.org/W6791447439",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3180355996",
    "https://openalex.org/W6795892075",
    "https://openalex.org/W2566832195",
    "https://openalex.org/W2103504761",
    "https://openalex.org/W4243684731",
    "https://openalex.org/W6621378261",
    "https://openalex.org/W6600324250",
    "https://openalex.org/W3036170900",
    "https://openalex.org/W6772427365",
    "https://openalex.org/W2331128040",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W2064076387",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W6765779288",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W6795999542",
    "https://openalex.org/W6766254915",
    "https://openalex.org/W3012412627",
    "https://openalex.org/W3093635910",
    "https://openalex.org/W2964478178",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4287320642",
    "https://openalex.org/W4294643831",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W3122698081",
    "https://openalex.org/W4225724954",
    "https://openalex.org/W3164513001",
    "https://openalex.org/W8437397",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2998605847"
  ],
  "abstract": "An increased interest in longitudinal neurodevelopment during the first few years after birth has emerged in recent years. Noninvasive magnetic resonance imaging (MRI) can provide crucial information about the development of brain structures in the early months of life. Despite the success of MRI collections and analysis for adults, it remains a challenge for researchers to collect high-quality multimodal MRIs from developing infant brains because of their irregular sleep pattern, limited attention, inability to follow instructions to stay still during scanning. In addition, there are limited analytic approaches available. These challenges often lead to a significant reduction of usable MRI scans and pose a problem for modeling neurodevelopmental trajectories. Researchers have explored solving this problem by synthesizing realistic MRIs to replace corrupted ones. Among synthesis methods, the convolutional neural network-based (CNN-based) generative adversarial networks (GANs) have demonstrated promising performance. In this study, we introduced a novel 3D MRI synthesis framework- pyramid transformer network (PTNet3D)- which relies on attention mechanisms through transformer and performer layers. We conducted extensive experiments on high-resolution Developing Human Connectome Project (dHCP) and longitudinal Baby Connectome Project (BCP) datasets. Compared with CNN-based GANs, PTNet3D consistently shows superior synthesis accuracy and superior generalization on two independent, large-scale infant brain MRI datasets. Notably, we demonstrate that PTNet3D synthesized more realistic scans than CNN-based models when the input is from multi-age subjects. Potential applications of PTNet3D include synthesizing corrupted or missing images. By replacing corrupted scans with synthesized ones, we observed significant improvement in infant whole brain segmentation.",
  "full_text": "IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022 2925\nPTNet3D: A 3D High-Resolution Longitudinal\nInfant Brain MRI Synthesizer Based on\nTransformers\nXuzhe Zhang , Graduate Student Member, IEEE, Xinzi He, Jia Guo, Nabil Ettehadi, Natalie Aw,\nDavid Semanek, Jonathan Posner, Andrew Laine,Life Fellow, IEEE, and Yun Wang\nAbstract — An increased interest in longitudinal neurode-\nvelopment during the ﬁrst few years after birth has emerged\nin recent years. Noninvasive magnetic resonance imag-\ning (MRI) can provide crucial information about the devel-\nopment of brain structures in the early months of life.\nDespite the success of MRI collections and analysis for\nadults, it remains a challenge for researchers to collect\nhigh-quality multimodal MRIs from developing infant brains\nbecause of their irregular sleep pattern, limited attention,\ninability to follow instructions to stay still during scanning.\nIn addition, there are limited analytic approaches avail-\nable. These challenges often lead to a signiﬁcant reduc-\ntion of usable MRI scans and pose a problem for mod-\neling neurodevelopmental trajectories. Researchers have\nexplored solving this problem by synthesizing realistic\nMRIs to replace corrupted ones. Among synthesis meth-\nods, the convolutional neural network-based (CNN-based)\ngenerative adversarial networks (GANs) have demonstrated\npromising performance. In this study, we introduced a novel\n3D MRI synthesis framework – pyramid transformer network\n(PTNet3D) – which relies on attention mechanisms through\ntransformer and performer layers. We conducted extensive\nexperiments on high-resolutionDevelopingHuman Connec-\ntome Project (dHCP) and longitudinal Baby Connectome\nProject (BCP) datasets. Compared with CNN-based GANs,\nPTNet3D consistently shows superior synthesis accuracy\nand superior generalization on two independent, large-scale\ninfant brain MRI datasets. Notably, we demonstrate that\nPTNet3D synthesized more realistic scans than CNN-based\nmodels when the input is from multi-age subjects. Potential\napplications of PTNet3D include synthesizing corrupted or\nManuscript received 24 March 2022; revised 7 May 2022; accepted\n10 May 2022. Date of publication 13 May 2022; date of current version\n30 September 2022. This work was supported in part by the Envi-\nronmental inﬂuences on Child Health Outcomes (ECHO) Program—\nOpportunities and Infrastructure Fund under Grant EC0360 (Yun Wang)\nand in part by the National Institutes of Health under Grant K99HD103912\n(Yun Wang), Grant UH3OD023328, Grant R01MH119510, and Grant\nR01MH121070 (Jonathan Posner).\n(Xuzhe Zhang and Xinzi He con-\ntributed equally to this work.) (Corresponding author: Yun Wang.)\nXuzhe Zhang, Xinzi He, Nabil Ettehadi, and Andrew Laine are with the\nDepartment of Biomedical Engineering, Columbia University, New Y ork,\nNY 10027 USA (e-mail: xz2778@columbia.edu; xh2435@columbia.edu;\nne2289@columbia.edu; al418@columbia.edu).\nJia Guo, Natalie Aw, and David Semanek are with the Department\nof Psychiatry, Columbia University, New Y ork, NY 10032 USA\n(e-mail: jg3400@columbia.edu; Natalie.Aw@nyspi.columbia.edu;\nDavid.Semanek@nyspi.columbia.edu).\nJonathan Posner and Yun Wang are with the Department of Psy-\nchiatry and Behavioral Sciences, Duke University Medical Center,\nDurham, NC 27701 USA (e-mail: jonathan.posner@nyspi.columbia.edu;\nyun.wang974@duke.edu).\nDigital Object Identiﬁer 10.1109/TMI.2022.3174827\nmissing images. By replacing corrupted scans with synthe-\nsized ones, we observed signiﬁcant improvement in infant\nwhole brain segmentation.\nIndex Terms — Infant brain MRI, MRI synthesis, neural\nnetwork, performer, transformer.\nI. I NTRODUCTION\nT\nHE ﬁrst two years of life after birth mark rapid periods\nof postnatal growth and development for the human\nbrain. The brain structures, functions, and neural pathways\nthat develop during this time lay the foundation for the\nindividuals that we will become. An important goal for many\nstudies of early childhood is to identify early biomarkers\nof later cognitive functions, behaviors, or risks. Structural\nmagnetic resonance imaging (MRI) has become an important\nnon-invasive approach to investigate brain structural changes\nwith high spatial resolution. Over the last decade, researchers\nhave found a modest relationship between brain structure,\ncognition, and behavior [1]–[4], suggesting that with improved\nmethodologies, early imaging biomarkers may be useful in\npredicting later risk.\nCompared with adults, infant brains have 1) lower contrast-\nto-noise ratios due to the relative lack of myelination and\nshorter scan times [5]; 2) lower spatial resolution due to the\nsmaller overall volume of the brain; and most importantly 3)\ntissue intensities that change dramatically over the ﬁrst two\nyears of life. In addition, given infants characteristics such\nas long preparation time (feeding and swaddling to induce\nsleep), irregular sleep patterns, and the inability to follow\ninstructions to keep still, it is often difﬁcult to collect high-\nquality multimodal MRI scans for infants [6]. Depending on\nthe research goal of studies and the choice of MRI processing\npipelines, researchers often prioritize only one modality /\nprotocol. For example, studies with a focus on newborns\nwill most likely prioritize the acquisitions of T2 weighted\n(T2w) over T1 weighted (T1w) scans if they have chosen\nto use the developing human connectome project (dHCP)\nstructural pipeline [7] or T1w over T2w scans if using the\nInfant FreeSurfer pipeline [8]. Obtaining high-quality struc-\ntural MRI scans for both modalities (T1w and T2w) may be\nimpractical. Of note, structuralMRI processing (tissue/region\nsegmentation, surface reconstruction) is the ﬁrst procedure\nfor analyzing other MRI modalities, e.g., functional MRI and\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\n2926 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\ndiffusion MRI. Poor quality structural MRI scans can limit the\nability of research to study other MRI modalities. Moreover,\nincluding both T1w and T2w scans into structural analysis\nmay enhance surface-based morphological measurements by\nproviding more accurate whole brain segmentation [9]–[11].\nTherefore, novel and robust methodologies, which can syn-\nthesize missing or corrupted infant MRI scans, can be\nvery helpful for developmental neuroscience and clinical\nresearch [12].\nPrevious studies have demonstrated that synthesized single\nor multimodal MRIs based on existing high-quality scans,\nto some extent, improve biomedical imaging processing pro-\ncedures, e.g., segmentation and registration. For example,\nreplacing corrupted ﬂuid-attenuated inversion recovery MRI\nscans with its synthesized version based on corresponding\nT1w, T2w, and proton density (PD) scans can yield better\nsegmentation [13]. Similarly, a previous study has shown that\nsynthesized T1w scans can replace real T1w scans in inter-\nmodality and cross-subject brain MRI registration, and this\napproach improves registration as compared with only using\nreal PD scans [14].\nPrior to the rise of deep learning (DL), registration-based\nand intensity-based transformation methods were prevalent in\nthis domain. Registration-based methods rely on a group atlas\nas well as deformable registration to synthesize images with\ndifferent contrast [15]. Although registration-based image syn-\nthesis provides promising performance in synthesizing com-\nputed tomography and positron emission tomography from\nMRI [16], [17], it may not be applicable to infant MRI\nsynthesis because of 1) lack ofan accurate and longitudinal\ninfant brain MRI atlas; 2) more profound variations in the\ninfant brain at different ages which may introduce more error\nwhen registering to an atlas. Intensity-based transformation\nmethods often utilize image analogies, sparse reconstruction,\nnon-linear regression, as well as neural network to achieve\nimage synthesis [13], [14], [18]–[22]. However, an earlier\nstudy has concluded that these methods, whether dictionary\nreconstruction, random forest regression, or neural network-\nbased, tend to lose ﬁne details and yield suboptimal results in\nsynthesis [12].\nGiven the success of generative adversarial network (GAN)\nin image synthesis, translation, and manipulation [23]–[26],\nrecent studies have attempted t o introduce the convolution\nneural network (CNN)-based GAN framework into medical\nimage synthesis and have shown improved performance com-\npared with aforementioned methods [12], [20], [22], [27]–[29].\nRecently, the transformer lay er, which is a self-attention\nand convolution-free architecture, has been introduced to\nthe computer vision domain and demonstrates outstanding\nperformance in classiﬁcatio n and segmentation in terms of\naccuracy and efﬁciency [30]–[33]. The performer layer is\nalso introduced and applied to vision tasks [34], [35]; it is\na similar attention-based architecture to the transformer but\nwith a simpliﬁed self-attention and requires less computation\nthan the transformer.\nIn this study, we focus on synthesizing infant brain structural\nMRIs (T1w and T2w scans) using both transformer and\nperformer layers. We design a novel 3D framework, inheriting\nthe U-Net-like as well as multi-resolution pyramid structures\n[25], [36], and utilizing performer encoder (PFE), performer\ndecoder (PFD), and transformer bottleneck to synthesize\nhigh-quality infant MRI. We conduct extensive experiments\nbased on a large-scale high-resolution infant MRI dataset – the\nDeveloping Human Connectome Project (dHCP) dataset [7] as\nwell as another longitudinal infant MRI dataset – the Baby\nConnectome Project (BCP) dataset [37], and compare our\nmodel’s performance with other methods including pix2pix,\npix2pixHD, and StarGAN [25], [26], [38]. We demonstrate\nthat our proposed model can synthesize realistic T1w scans\nbased on T2w scans and vice versa. Compared with CNN-\nbased models, our framework is superior in various metrics\nwhen validated on the unseen test dataset. More importantly,\nour PTNet3D can provide good synthetic results across differ-\nent ages while CNN-based models fail on scans from subjects\n<= 6 months old. We also have experimentally shown that\nusing PTNet3D to synthesize corrupted modality (T1w) based\non good-quality T2w improves dual-channel segmentation\nusing 3D U-Net.\nII. R\nELA TEDWORKS\nA. GAN-Based MRI Synthesis\nCNN-based GAN is the most prevalent framework in the\nimage translation and synthesis domain. It utilizes adversarial\ntraining, which uses the discriminator network’s feedback to\ngenerate images similar to the training data. During the train-\ning, two subnetworks: generator and discriminator, are trained\nsimultaneously. The generator employs a decoder (original\nGAN) or an encoder-decoder (conditional GAN) architecture.\nThe original GAN was proposed to unconditionally generate\nimages from latent space noise vector [23]. The discriminator\nis a classiﬁer trained by the real and synthesized image. The\ndiscriminator has access to the true label during the training.\nThe generator is trained usingthe feedback from the discrimi-\nnator and aims to “fool” the discriminator and generate images\nthat cannot be distinguished from real images. The conditional\nGAN has been used in various downstream applications, such\nas super-resolution, style transfer, sketch-to-image generation,\nand image inpainting [26], [38], [39]. However, the training\nof a GAN model can be unstable. Stability is improved in a\nconditional GAN model as the input is not random noise but\ninformative images. Other advancements in conditional GAN\ninclude using a uniﬁed generator for multi-domain synthesis\nand reducing the data required for training a GAN [38], [40].\nInspired by the previous success of conditional GANs in\nnatural image translation, early studies have explored their\napplication in medical image synthesis [12], [27]–[29]. Specif-\nically, studies [27], [12] have used a similar framework\nin [26] and [25] such as pix2pix and pix2pixHD, respec-\ntively, for MRI cross-modality synthesis. Daret al. [12] has\nexplicitly shown that GAN-based methods outperform the\nprevious intensity-based transformation and neural network-\nbased methods (i.e., Replica and Multimodal) in MRI synthesis\n[20], [22]. Reference [29] has utilized a uniﬁed generator\nextended from the StarGAN [38]. Both [29], [28] have further\nintroduced supervision on latent features to improve synthetic\nresults.\nZHANG et al.: PTNet3D: 3D HIGH-RESOLUTION LONGITUDINAL INFANT BRAIN MRI SYNTHESIZER 2927\nFig. 1. Self-attention mechanism used in Transformer and a basic\ntransformer block. Head count (H) is the number of scaled dot-product\nattention used in the multi-head attention. N is the number of successively\nused transformer blocks.\nB. Transformer in Computer Vision Tasks\nThe transformer is an architecture that solely relies\non self-attention mechanisms ( Fig. 1 ) and is completely\nconvolution-free [30]. A transformer layer consists of a multi-\nhead self-attention layer and a fully connected feed-forward\nnetwork (multilayer perceptron). A residual connection and\na layer normalization are applied on both components. The\ntransformer model was originally designed for sequence\nprocessing and is becoming a popular and fundamental archi-\ntecture for NLP tasks. Recently, it has been extended to\ncomputer vision tasks, such as image classiﬁcation, image\nsegmentation, image generati on, and object detection [31],\n[35], [41]–[45]. In those applications, the transformer has\ndemonstrated a great potential to achieve or outperform state-\nof-the-art CNN-based networks, largely because of its self-\nattention mechanism.\nThe self-attention mechanism is based on multiplicative\nattention through the dot-product of weights and values (of\ndimension d\nv), where the weight matrix is calculated by a\ncompatibility function of the query with the corresponding key\n(of dimension dk = dv). In practice, queries, keys, and values\nare packed together into a matrix Q, K, V , respectively. The\nscaled dot-product attention is calculated using (1). Instead\nof performing the scaled dot-product attention one time, the\noriginal paper proposed a multi-head attention (MHA) module\n[46], which is more beneﬁcial for capturing global dependen-\ncies. As shown inFig. 1, Q, K, and V are linearly projectedH\ntimes, by linear projectionsW\nQ , W K ,a n dW V . For each head,\nthe single head attention is calculated in parallel based on Eq.\n(2). The ﬁnal output of MHA is given by the linear projection\nW O of the concatenation of head attentions as shown in Eq.\n(3) below.\nAttention (Q, K, V ) = sof tmax ( QK T\n√\ndk\n)V (1)\nhead i (Q, K, V ) = Attention (QW Q\ni , KW K\ni , VW V\ni ) (2)\nMHA (Q, K, V ) = Concat (head 1,..., head h)W O (3)\nC. Performer Block for Simpliﬁed Attention Mechanism\nThe original transformer model employed a full-rank soft-\nmax attention. Despite the superior performance of the trans-\nFig. 2. Difference between transformer and performer models. Upper\npanel: Transformer block as explained in Eq (4). Lower panel: Performer\nblock as explained in Eq (5, 6). The red dashed block is ﬁrst computed to\nreduce complexity. The entire green solid block is proposed to approxi-\nmate the full-rank self-attention in the upper panel.\nformer block and its self-attention mechanism, the space and\ntime complexity of computing the full-rank attention matrix\nquadratically grows with the number of tokens L (which is\nproportional to image size in vision tasks). To prove this,\nwe rewrote Eq. (1) by decomposing the softmax into expo-\nnential and normalization components, yielding:\nAttention (Q, K, V ) = D−1 AV , A = exp( QK T\n√\ndk\n),\nD = diag(A1L). (4)\nI nE q .( 4 ) ,e x p(x) is element-wise exponential function,D−1\nperforms normalization where diag(x) is a diagonal matrix\nwith the input vector x as the diagonal and1L is the all-ones\nvector of lengthL. By deﬁnition, we haveQ, K, V ∈ RL×dk .\nTherefore, Eq. (1) and (4) require a time complexity of\nO(L2dk) and a space complexity of O(L2 + dk) because A\nhas to be calculated and stored ﬁrstly (Fig.2). The quadratic\ncomplexity limits the application of the original transformer\nto large input sequence.\nThe performer block was proposed to approximate the\nregular full-rank softmax attention by Fast Attention Via\npositive Orthogonal Random features (FA VOR+) mechanism\n[34]. The FA VOR+ replace the regular attention matrixD−1 A\nby approximating exp ( QK T\n√\ndk\n) through Q\u0004(K \u0004)T . Therefore,\nwe have:\n\u0002Attention (Q, K, V ) = ˆD−1(Q\u0004((K \u0004)T V )),\nˆD = diag(Q\u0004(K \u0004)T 1L ). (5)\nIn Eq. (5), we replace the non-linear exp ( QK T\n√\ndk\n) with a\nlinear operation Q\u0004(K \u0004)T so that we can switch the order\nof multiplication. As indicated by the brackets in Eq. (5)\nand depicted in Fig. 2, we ﬁrst calculate the (K \u0004)T V .S u c h\nan approximation reduces the time and space complexity to\nO(Ld \u0004\nkdk) and O(Ld \u0004\nk + Ldk + d\u0004\nkdk), respectively (Fig.2).\nThe mapping from Q, K ∈ RL×dk to Q\u0004, K \u0004 ∈ RL×d\u0004\nk is\nachieved by kernelφ : Rdk → R\nd\u0004\nk\n+ so that each row inQ\u0004 and\n2928 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\nK \u0004 is given by φ(qT\ni )T and φ(kT\ni )T , respectively (qi and ki\ndenote rows in Q and K ).T h eφ is deﬁned by\nφ(x)= exp(−\u0006x\u00062\n2 )\n√\nm (exp(χT\n1 x),··· exp(χT\nm x)), m =d\u0004\nk. (6)\nIn Eq. (6), χ1,··· χm are ﬁxed, non-learnable, and random\northogonal vectors drawing from an isotropic distribution.\nIn the original paper, several different conﬁgurations for\nφ were proposed and compared. We selected the one with\npositive and orthogonal feature maps, which provided the\nmost negligible variance and provable accuracy while approx-\nimating the softmax kernel. Interested readers may ﬁnd more\ndetailed mathematical proofs and experiments in [34].\nIII. M ETHODS\nA. 3D Pyramid Transformer Net (PTNet3D) for MRI\nSynthesis\nIn this work, we introduce a novel 3D MRI synthe-\nsis framework: 3D Pyramid Transformer Net (PTNet3D).\nPTNet3D takes high-resolution 64× 64 × 64 block as input\nand its architecture consists of transformer/performer layers,\nskip-connections, and a multi-scale pyramid representation.\nAn overview of our proposed PTNet3D model is depicted in\nFig. 3. Speciﬁcally, we exploit the transformer block in the\nbottleneck layer to take advantage of its self-attention mech-\nanism on latent features (Fig. 4c). Considering its quadratic\ntime and space complexity, it cannot be directly applied to\nthe encoding/decoding path because of the higher spatial res-\nolution of feature maps. Therefore, we design the performer-\nbased encoder/decoder, which allows us to approximate full-\nrank softmax attention based on FA VOR+ in a linear time\nand space complexity (Fig. 4a&b). We adopt the successful\nU-shaped structure of U-Net and reshape the output tokens\nfrom each layer for skip connection, aiming to preserve ﬁne\nstructures of the brain.\nDespite the ﬂexibility of running on a high-resolution 3D\nimage block (64\n3) brought by the performer, it may not be\nable to capture global dependencies as well as the transformer\nbecause the performer is not equipped with full-rank attention\n[34]. Inspired by some previous studies in traditional com-\nputer vision and deep learning tasks [25], [47]–[52], we re-\nintroduce the pyramid representation to our framework to: 1)\nleverage the global information extracted by transformers to\ncompensate for the potential information loss caused by the\nperformer; 2) and alleviate the intensive computation need for\nhigh-resolution features. The entire framework operates in two\nlevels: the original resolution and downsampled resolution (1/4\nin x, y, and z axes). The original resolution image goes through\nthe performer encoder, transformer bottleneck, and performer\ndecoder. The downsampeld image is unfolded directly and is\nfed into the transformer bottleneck to take advantage of its\nfull-rank attention.\nIn the following sections, we introduce each component\nof our proposed PTNet3D model in detail. We introduced\nthe performer-based encoder and decoder in Section A.1,\ntransformer-based bottleneck in Section A.2, pyramid layer\nin Section A.3, and model details inSection B.\n1) Performer Encoder and Decoder: The most signiﬁcant\nchallenges for applying the original transformer model in\nvision tasks are computational time and GPU memories when\nthe input has high spatial resolution, such as in the case of\nbrain MRIs. To solve this issue, instead of the transformer,\nwe adapt the performer in our encoding and decoding blocks\nand name them as the PerFormer Encoder (PFE) and Per-\nFormer Decoder (PFD), which are illustrated inFig. 4. In PFE,\nfor an input 3D tensor with a size ofN × C\nin × X × Y × Z,\nwe unfold the 3D matrix into a series of tokens using a window\nof n by n and a stride S. The resultant tokens are with size\nN × 1\nS3 XYZ × Cin n3 and are fed into the performer block\n(Fig. 2). The output from the performer is then transposed and\nreshaped to a size ofN×Cout ×X ×Y ×Z (Fig. 4a). During the\nencoding, theS is often set as 2,n is set as 3. In its counterpart\nPFD, the input tensors are ﬁrst upsampled by a factor ofS\nthrough trilinear interpolation and are then concatenated with\nthe feature maps from the encoding path along the channel\ndimension (Fig. 2 and Fig. 4b). The concatenated feature\nmaps are then fed into the performer block following the same\nunfolding process. In the end, a similar transpose and reshape\nare performed to form the output. During the decoding, theS\nis often set as 2,n is set as 1 (Fig. 4 panel b).\n2) T ransformer Bottleneck: In the bottleneck, we employ the\noriginal transformer blocks (Fig. 1) as the input feature maps\nare already of low spatial size. Such a transformer bottleneck\n(Fig. 4c) allows to better capture any global dependencies\nacross the bottleneck features. The input of transformer bot-\ntleneck is a series of tokens formed by an unfolding process\nsimilar to the PFE and PFD. The output from the last PFE\nis unfolded with S = 2a n dn = 3. After unfolding, a fully\nconnected layer is applied to linearly project the token from\nN ×\n1\nS3 XYZ × Cin to N × 1\nS3 XYZ × Cembd ,w h e r eCin\nequals the channel number from the last PFE multiplied by\nn3,a n dCembd represents the embedding dimension throughout\nthe transformer blocks. The embeddings are then fed intoM\ntransformer blocks, in whichM is set as 9. Before feeding the\nembedded tokens into the transformer blocks, following the\nprevious studies [30], we add the positional encoding (PE).\nPE is proposed simultaneously with the transformer and aims\nto provide some information about the relative or absolute\nposition of the tokens in the sequence. In this work, we utilize\nthe same sine and cosine functions as the previous study\nsuggested [30] to generate the positional encoding according\nto Eq. (7) and (8), where pos is the position and i is the\ndimension. After going through M transformer blocks, the\noutput is projected, transposed, and reshaped as usual and is\nfed into the ﬁrst PFD as shows inFig. 2.\nPE\npos,2i = sin (pos/100002i/Cembd ) (7)\nPE pos,2i+1 = cos(pos/100002i/Cembd ) (8)\n3) Pyramid Layer: The PFE and PFD reduce the computa-\ntion complexity and allow the PTNet3D to operate on high-\nresolution 3D blocks. However, performer is theoritecially\nless powerful than transformer in terms of capturing global\ndependencies as it is approxima ting the full rank attention\nof transformer [34]. Therefore, to avoid the potential infor-\nZHANG et al.: PTNet3D: 3D HIGH-RESOLUTION LONGITUDINAL INFANT BRAIN MRI SYNTHESIZER 2929\nFig. 3. Overview of proposed 3D Pyramid Transformer Net (PTNet3D) model. We follow the classic U-shape structure and inherit the skip connection.\nWe parallelize the conversion at two distinct resolutions and concatenate them before feeding into the transformer bottleneck. The detailed structures\nof each component are illustrated inFig. 4below. The spatial projection is a fully-connected layer that reduces the channel to output channel number.\nmation loss due to performer, we equip the PTNet3D with\na layer that solely relies on transformer blocks with full-rank\nattention. Considering the quadratic complexity of transformer\nblock, we re-introduce the pyramid representation which was\nproven to beneﬁt vision tasks in both traditional and deep\nlearning-based vision tasks [25], [47], [48], [50], [52], [53].\nThe idea of pyramid representation came from [51], and\nauthors hypothesized that it mi rrors the multiple scales of\nprocessing in the human visual system in a computational-\nfriendly way. We downsample the image by a ratio of 4 so that\nwe can apply transformer at this resolution. We demonstrate\nthat the pyramid layer can be stacked several layers from the\n1/4 of the original resolution in the following sections.\nB. Model Details\nWe provided detailed conﬁgurations of the propsoed\nPTNet3D in this section. As illustrated in Fig. 3,P T N e t 3 D\nhas 3 PFEs and 4 PFDs. All PFEs have a window with\nn = 3 and except for the ﬁrst PFE which utilizes ans = 1,\nothers set s as 2 to reduce feature maps’ spatial dimension.\nAll PFDs set n as 1 and except for the last PFD, others\nﬁrstly upsample the input by a ratio of 2 through trilinear\ninterpolation. Prior to the transformer bottleneck, the output\nfrom the previous layers is unfolded withs = 2a n dn = 3.\nThe input and output channels (Cin and Cout ) for PFEs are\n1, 16, 32 and 16, 32, 64 respectively. The pyramid layer ﬁrst\nunfolds the downsampled image withn = 3a n ds = 1, and\nthen linearly projects the unfolded tokens to an embedding\ndimension of 64. The projected tokens are then fed into\n9 transformer blocks. The output is then linearly projected to\na dimension of 32 and it is thereby reshaped and concatenated\nback to the original branch prior to the transformer bottleneck.\nThe concatenated feature maps are unfolded withn = 3a n d\ns = 2 and are linearly projected to an embedding dimension\nof 256. After going through 9 transformer blocks, resultant\ntokens are projected to a dimension of 96 and are feed into the\ndecoding path. The input and output channels (C\nin and Cout )\nfor PFDs are 192, 64, 32, 17 and 32, 16, 16, 16 respectibely.\nThe ﬁnal output of PFDs is linearly projected to 1-channel\nand reshaped as an output image.\nC. Datasets\n1) Developmental Human Connectome Project—dHCP: We\nused 459 paired T1w and T2w high-resolution infant brain\nMRI scans from dHCP v1.0.2 data release (0.5 × 0.5 × 0.5\nmm3). The structural T1w and T2w scans from dHCP were\ncollected within one month after birth. The average postmen-\nstrual age of infants at scan is 40.65 ± 2.19 weeks. More\ndetails about the image acquisition can be found in the original\nwork [7]. Necessary data exclusion based on quality and data\npreprocessing were performed and details as well as example\nwere provided in theAppendix A . Afterwards, we split the\ndata with a ratio of 7:1:2.\n2) Baby Connectome Project—BCP: To further evaluate our\nmodel’s performance at different developing ages and differ-\nent datasets, we used the Baby Connectome Project (BCP)\ndataset [37]. Image synthesis tasks on longitudinal datasets\nare more challenging owing to the varying contrast of the\ndeveloping brain. Therefore, we believe that re-evaluating our\nPTNet3D on the BCP dataset can further prove its value. BCP\nadopted a mixed study design containing both longitudinal and\ncross-sectional time points, ranging from birth to 72 months.\nThe BCP scans have an isotropic resolution of 0.8 × 0.8 ×\n0.8 mm\n3. We employed similar preprocessing and exclusion\nto those used in dHCP dataset. We also designed a fair and\nrigorous data split to ensure each available age is included in\nthe training/validation/testing sets. The details can be found in\nthe Appendix A .\nD. Experiments\n1) Model Implementation: We ﬁrst compared the proposed\nPTNet3D with other previously state-of-the-art CNN-based\nmodels, including pix2pix, pix2pixHD, and StarGAN. The\npix2pix model is a U-Net-like model and its generator is an\nencoder-decoder that progressively downsampled the feature\nmaps by a factor of 2 while increasing the dimension of the\nfeature maps. It does not use any bottleneck layers but uses\na skip connection between the encoder and decoder path. The\npix2pixHD is an advanced version of pix2pix, which utilizes\na residual block as the bottleneck layer’s backbone. It has\ntwo variants: the pix2pixHD-global only employs a single\ngenerator proposed by [54]; the pix2pixHD-local is addi-\ntionally equipped with a local enhancer network that works\non high-resolution feature maps [25]. The pix2pixHD-global\nemploys 9 residual blocks in its bottleneck, and its -local\nversion utilized 3 and 6 residual blocks in high- and low-\nresolution branches, respectively. The StarGAN is an uniﬁed\n2930 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\nFig. 4. Proposed performer encoder(a), performer decode(b), and transformer bottleneck(c). (a): The performer encoder will ﬁrst unfold the feature\nmaps into tokens. The channel after unfolding is decided by the input channelCin and unfold kernel sizen. Unfolded tokens are then fed into a\nperformer layer. The resultant token is lastly transposed andreshaped to a feature map which has been downsampled by a scale ofs (stride). In the\nencoding path, the unfold kernel sizen is usually set as 3, and unfold strides is usually set as 2.(b): The performer decoder will ﬁrst upsample\nthe input feature maps by a factor ofs. The upsampled feature maps are then processed as mentioned in the performer encoder, but there is no\nstride so the upsampled feature size remainsunchanged. In the decoding path, the upsample factors is usually set as 2. The unfold kernel sizen\nis usually set as 1 and unfold strides is usually set as 1.Cin and Cout are changed at different levels of the network.(c): Transformer bottleneck:\nThe transformer bottleneck utilize the same unfold as the performer encoder. And additional position encoding and linear projection are used prior\nto feeding in transformer blocks. The output of M transformer blocks is then transposed and reshaped and fed into the decoding path.\nGAN model which enables a multi-domain image translation\nwithin a signle network [38]. We borrowed the implementation\nfor abovementioned models from their public GitHub repos-\nitories. Comparisons of computation cost of different models\nwere provided in Appendix B .\n2) T raining Strategies: The pix2pix series need both adver-\nsarial loss (Lad v, Eq. (9)) and other regularization terms to sta-\nbilize the training. Therefore, a generatorG and discriminator\nD are used during the training. We termX as the input source\nimage, and Y as the target image. For the pix2pix, it utilizes\nthe Lad v and L1 reconstruction loss (mean absolute error, Eq.\n(10), with a weight of 100) as the loss function described\nin Eq. (11); for the pix2pixHD, instead of incorporating an\nL1 reconstruction loss, it incorporates the L1l o s si nt h e\nfeature-level (L fe a t, Eq. (12), with a weight of 10) with the\nLad v Eq. (13). It should be noted that, for Eq. (12), Di\nindicates the output from thei-th layer of the discriminatorD.\nSpeciﬁcally, i-th layer is the layer prior to the ﬁnal patch-level\nclass prediction andi is set from 1 to 3.\nLad v = EX,Y [logD(X,Y ) + log(1 − D(X, G(X)))] (9)\nLMAE = \u0006G(x) − Y \u00061 (10)\nL p = Lad v + 100 ∗ LMAE (11)\nL fe a t=\n∑ 3\ni=1 \u0006Di (X,Y ) − Di (X, G(X))\u00061 (12)\nL pH = Lad v + 10 ∗ L fe a t (13)\nUnlike the pix2pix series, the StarGAN applies different\nadversarial loss and regularization term. StarGAN utilizes two\ndifferent discriminators: Dsrc to distinguish real and fake\nimages, and Dcls for domain class classiﬁcation. Denoting the\ninput source image X, source domain class cx , target image\nY , source domain classcy, and generated imageG(X,cy),t h e\nStarGAN is trained using Eq. (17):\nLad vStar = EX [log(Dsrc (X)]\n+ EX,cy [log(1 − Dsrc (X, G(X,cy)))] (14)\nLcls = EX,cx [− log (Dcls (cx | X)]\n+ EX,cy [−log(Dcls (cy|G(X,cy))] (15)\nLcycle =\nG(G(X,cy),cx ) − X\n\n1 (16)\nLStar = Lad vStar + Lcls + 10 ∗ Lcycle (17)\nPTNet3D also utilizes a hybrid loss function. It uses the\nsame adversarial loss shown in Eq. (9) with a 3D patch-level\ndiscriminator similar to [25]. It uses aL2 norm on pixel-wise\nreconstruction loss and feature-level perceptual loss. In addi-\ntion to Eq. (9-11), we also use a 3D ResNet-18 model pre-\ntrained on Kinetics-400 dataset as the externel discriminator\nZHANG et al.: PTNet3D: 3D HIGH-RESOLUTION LONGITUDINAL INFANT BRAIN MRI SYNTHESIZER 2931\n[55]. The pretrained ResNet-18 is frozen during training and\nonly provides supervision in feature-level (Eq. (18)). We term\nall the feauture-level regularizations as perceptual lossL Perc ,\nwhich is deﬁned as Eq. (19). And the loss function for\nPTNet3D is deﬁned as Eq. (21)\nL\nResFeat =\n∑ 4\ni=1 \u0006Resi (Y ) − Resi (G(X))\u00061 (18)\nL Perc = L fe a t+ L ResFeat (19)\nLMSE = \u0006G(x) − Y \u00062 (20)\nL PT Net = Lad v + 10 ∗ L Perc + 100 ∗ LMSE (21)\nFive models were separately trained for T1w-to-T2w and\nT2w-to-T1w conversions. For T1w-to-T2w conversion,X was\nT1w scan and Y was the corresponding T2w scan, and vice\nversa. We used the default training strategies for pix2pix seriex\nand StarGAN as explained and demonstrated in the previous\nstudies [25], [26], [38]. Detail training conﬁgurations can be\nfound in the Appendix B .\n3) Evaluation: To compare models’ performance on\nT1w-to-T2w and T2w-to-T1w synthesis tasks, four different\nmetrics: the structural similarity index (SSIM), peak signal-\nto-noise ratio (pSNR), mean absolute error (MAE), and\nFréchet Inception Distance (FID) were calculated on the test\ndataset [56]–[58]. We employed these four metrics to evaluate\nsynthetic results from different perspectives. More detailed\nintroductions to the metrics can be found in theAppendix D .\nAfter directly comparing the models’ performance from the\nquantitative metrics, we evaluate each model based on the\nvalidity of its synthetic results indirectly. Speciﬁcally, using\nthe same distribution introduced in Section C.1, we trained a\n3D UNet which takes concatenated T1w and T2w blocks as\ninputs [59] to segment the entire brain into 87 brain regions\nbased on the labels provided by dHCP studies. We compared\nthe segmentation maps of real T1w + real T2w with those\nof real T1w+ synthetic T2w. We hypothesized that the more\nvalid synthetic scans are, the closer segmentation results are\ncompared to those of real scans. Three different metrics were\nutilized in comparison: Dice score (DSC), average surface\ndistance, and 95% Hausdorff distance (HD).\n4) Ablation Experiments: To further study the PTNet3D\nwhere transformer and performer layers are ﬁrst introduced\nto replace convolutional layers completely in MRI synthesis,\nwe conducted ablation studies on loss functions, pyramid lay-\ners, and the dimensionality of the input image. We compared\nthe PTNet3D performance when it was trained by MES loss\n(Eq. (20)) solely, by adversarial loss and MSE (with a weight\nof 100), and by a combination of adversarial loss, perceptual\nloss, and MSE loss together (Eq. (21)). Furthermore, we\ncompared model performance withdifferent pyramid layers: 0,\n1, and 2. For the case of two layers, the second pyramid layer\nruns on a resolution of 8\n3, and the output is concatenated back\nto the main branch prior to the linear projection in the trans-\nformer bottleneck. We also evaluated PTNet3D on different\ninput dimensions. As both transformer and performer layers\ntake tokens as input, in this ablation experiment, we formed\ntokens through unfolding the 3D block (64\n3) and 2D image\n(224 × 256). The detailed results can be found inSection IV.\nFig. 5. Visualizations and absolute error maps among existing synthesis\nmodels and our (PTNet3D) model. From left to right columes: real\nscan, synthetic results from pix2pix,pix2pidHD-Global, pix2pixHD-Local,\nStarGAN, and the proposed PTNet3D. From top to bottom rows: sagittal,\ncoronal, and axial orientations. We noticed that other models yield a more\nextensive error map than the proposed PTNet3D. Red arrows indicated\nregions in which our PTNet3D generated more accurate results.\nIV . RESULTS\nA. Synthesis Results on dHCP Dataset\n1) Visual Comparisons: We ﬁrst visualized synthesized\nscans and calculated their absolute error maps between the\noriginal scan and synthesized one from each model (Fig. 5)f o r\nT1w-to-T2w synthesis. Absolute error is calculated between\nnormalized ground truth and converted scans, ranging from\n[0,1] – lower values (white color) indicate minor differ-\nence. From Fig 5., we found that our model produces less\nextensive absolute error than the CNN-based models in gen-\neral. Especially, the arrows indicated the region where our\nmodel produced a more realistic synthesis compared to other\nmethods.\n2) Quantitative Results: We quantitatively compared the\nresults of our proposed PTNet3D model to results of other\nCNN-based models using the same high-resolution infant brain\nMRI dataset. The mean and standard deviation of SSIM,\npSNR, MAE, and FID were calculated from the test dataset\nand were reported in Table 1. PTNet3D outperforms other\nmodels in almost all metrics, except for the FID in T2w-to-\nT1w synthesis. In the T1w-to-T2w synthesis scenario, com-\npared to the pix2pix model, our PTNet3D achieves a 2.67%\nincrease in SSIM, 4.45 dB rise in pSNR, and a 32% reduction\nin MAE. Similarly, for T2w-to-T1w synthesis, PTNet3D deliv-\ners a 1.5% increase in SSIM, 1.88 dB rise in pSNR, and an\n18% decrease in MAE, when compared to the pix2pix model.\nThe superior performance of PTNet3D was consis-\ntently observed for both T1w-to-T2w and T2w-to-T1w\ntasks. We noticed that pix2pixHD-local, Star-GAN, and our\n2932 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\nTABLE I\nRESULTS ON DHCP DATASET\nPTNet3D have very close FID in T2w-to-T1w synthesis.\nHowever, our PTNet3D has the best performance in SSIM,\npSNR, and MAE, representing higher structural similarities.\nB. Synthesis Results on Longitudinal BCP Dataset\nThe detailed quantitative results from all 5 models were\nlisted inTable 2. We reported the average SSIM, pSNR, MAE,\nand FID from 46 testing scans, which were acquired at differ-\nent ages. We noticed that PTNet3D continues to outperform\nall other CNN-based counterparts on the longitudinal BCP\ndataset. In addition, we noticed that StarGAN’s performance\nsigniﬁcantly drops compared to the performance on dHCP\ndataset. Despite its relatively high SSIM, pSNR, and MAE\non T2w-to-T1w synthesis task, StarGAN performed poorly in\nFID and failed on T1w-toT2w synthesis tasks.\nAs introduced before, the longitudinal infant brain MRI\nposes a challenge in image synthesis because of the varying\ncontrasts in rapidly developing brains. To investigate the\npossible impact of longitudinal data on synthetic quality and\naccuracy, we divided the test subjects into four different age\ngroups: 0-6 months, 7-12 months, 13-24 months, and >24\nmonths. A comparison of the model’s performance at each\nage group was conducted (Fig. 6).\nFig. 6 shows the distributions of SSIM, MAE, and FID\nfor each model and age group separately. The pSNR is based\non intensity difference as MAE does, so we provided it in\nAppendix C . In the left panel are the longitudinal results\nof T1w-to-T2w synthesis, while the right panel illustrates the\nT2w-to-T1w synthesis results. Our PTNet3D model consis-\ntently yielded higher SSIM, lower MAE and lower FID than\nCNN-based models.\nNotably, the CNN-based models could not tackle the chal-\nlenging longitudinal synthesis, especially when the input scans\nwere from the 0-6 months group. An example was provided\nin Fig. 7. As a comparison, we provided the results from\npix2pixHD-Local – the best CNN-based model ( Table 2\nand Fig. 6). The synthesized scan from PTNet3D, which is\nshown in the top middle, is less noisy, has fewer artifacts,\nTABLE II\nRESULTS ON BCP DATASET\nFig. 6. Boxplots for T1w-to-T2w synthesis(a, c, ande) and T2w-to-T1w\nsynthesis (b, d, andf) on multi-age BCP dataset.\nand remarkably retains ﬁne structural details. The superior\nsynthetic quality of PTNet3D is clearer in the zoomed region,\nwhere pix2pixHD-Local loses detail of the cerebral cortex, that\nis preserved in the PTNet3D model.\nZHANG et al.: PTNet3D: 3D HIGH-RESOLUTION LONGITUDINAL INFANT BRAIN MRI SYNTHESIZER 2933\nFig. 7. An example from a 3-month-old subject. The middle and right\ncolumns are synthesized outputs from PTNet3D and pix2pixHD-Local.\nThe bottom row is the zoomed view of the region highlighted by the red\nbox.\nC. Segmentation of 87 Brain Regions Using Synthesized\nScans\nFurthermore, we evaluated the validity of synthesized scans\non real-world application – infant whole brain segmentation.\nWe used the same dHCP dataset and followed the same data\npartition as Section A. To perform the segmentation task,\nwe built a dual-channel 3D UNet model (detailed conﬁgu-\nrations in the Appendix E ). After training on 291 pairs of\nmultimodal MRI scans (T1w and T2w) and labels, we seg-\nmented six different sets of test scans, which contained real\nT1w and real T2w or real T1w + synthesized T2w. Each\nabove synthesizer generated a unique pair of real T1w and\nsynthesized T2w.\nSegmentation accuracy using synthesized scans was eval-\nuated by three metrics, including the Dice, Average Sur-\nface Distance (ASD), and 95% Hausdorff Distance (HD95),\nlisted in Table 3. We noticed that PTNet3D outperforms\nother methods in all metrics, providing the closest perfor-\nmance to using real scans. We can conclude that PTNet3D\nsynthesized more realistic scans based on the segmentation\nresults.\nCompared to pix2pixHD-Local, 87 out of 87 segmented\nregions from the derivative scans of PTNet3D are closer to\nthose from real scans (signiﬁcantly higher Dice at an FDR\nadjusted p value of 0.05, seeAppendix F ).\nWe further provided visualization of segmentation results in\nFig. 8. For simpliﬁcation, only segmentation maps from real\nscans, PTNet3D, and pix2pixHD-Local were provided. Com-\npared to those using the real scans, PTNet3D and pix2pixHD-\nLocal both performed well on segmenting white matter and\ngray matter. However, using the synthesized scans from\nPTNet3D generated more reliable segmentation, especially for\nsmall-to-medium structure regions. As highlighted inFig. 8,\nusing pix2pixHD-Local as the synthesizer led to an inaccurate\nsegmentation of the brain stem and the ventral lateral nucleus\nwithin thalamus (the top row), a false negative segmentation\nof thalamus, and ambiguous boundary between parietal and\noccipital frontal lobe (middle row and bottom row).\nTABLE III\nSEGMENTATION RESULTS USING DIFFERENT SYNTHESIZER\nFig. 8. Segmentation maps from distinct inputs. Left to right: real/true\nscans, synthesized scans by PTNet3D, synthesized scans by pix2pixHD-\nLocal. From top to bottom: axial view, sagittal view, coronal view. Red\ncircles indicate the region where synthesized scans by PTNet3D yield\nsegmentation results that are closer to those from real scans.\nD. Ablation Studies\nTo evaluate the contribution of each component in the\nproposed PTNet3D, we conducted ablation studies by com-\nparing the synthetic results ofPTNet3D under different con-\nﬁgurations. Speciﬁcally, we investigated the inﬂuence of each\ncomponent of the loss function; we justiﬁed the effectiveness\nof the proposed pyramid structure; we compared the proposed\nPTNet3D to its 2D variant. The detailed experiments and\nresults can be found below.\n2934 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\nTABLE IV\nDHCP T1W-TO-T2 W SYNTHESIS RESULTS USING DIFFERENT LOSS\nFUNCTIONS\nTABLE V\nDHCP T1W-TO-T2 W SYNTHESIS RESULTS USING DIFFERENT\nPYRAMID LAYERS\n1) Contribution of Adversarial and Perceptual Losses: We\nﬁrst conducted an ablation study on the choice of loss\nfunctions. Three different conﬁgurations were compared, and\nresults were provided inTable 4. A previous study suggested\nthat adversarial training (adversarial and perceptual losses) can\nnot only improve image quality in terms of high-frequency\ncomponents but also will improve intensity-based metrics such\nas SSIM [28]. However, we noticed that our PTNet3D could\nbe directly and efﬁciently trained with MSE and achieve\nvery high performance in intensity-based metrics. Nonetheless,\nonly using MSE will lead to resultant scans, which are over-\nsmooth and lack high-frequency components. Incorporating\nadversarial loss and perceptu al loss will yield the optimal\nFID score and best visual quality without scarifying intensity\nrestoration accuracy too much.\n2) Contribution of Pyramid Layer: As introduced before,\nwe assume utilizing transformers directly at downsampled\nimage will take advantage of the full-rank attention to avoid\npotential information loss caused by the performer. We con-\nducted an ablation experiment on removing the pyramid layer,\nadding one or two pyramid layers to justify the effectiveness of\nthe proposed pyramid layer. We only stacked pyramid layers\nup to two because of resolution limitation. Taking a 64\n3 block\nas the input, the lowest spatial resolution of main branch is\n83, while the third pyramid layer will run at a resolution\nof 43. The results were concluded inTable 5. Compared to\nremoving pyramid layers, adding one pyramid layer running at\na1 63 resolution improved all metrics. This improvement was\nconsistently observed when two pyramid layers were utilized.\nWe believe these results suggest the pyramid layer beneﬁts the\nPTNet3D by directly applying transformers at different scales,\nespecially with respect to capturing global dependencies. After\ndownsampling the image from 64\n3 to 83, the local structural\ndetails are barely preserved in the input of the second pyramid\nlayer, but it still improves the synthesis performance. The\npyramid layer might provide such an improvement as long\nas it runs at a reasonable resolution.\nTABLE VI\nDHCP T1W-TO-T2 W SYNTHESIS RESULTS USING 2D AND 3D\nPTN ETS\nFig. 9. Feature maps from the decoding path of different models.\na), PTNet_2D, b), PTNet3D, c), pix2pixHD-Global. ∗The checkboard\nartifact in panel b. is caused by stitching and doesn’t indicate failures.\n3) 2D and 3D Comparison: Lastly, we conducted an ablation\nstudy between the proposed PTNet3D and our previous 2D\nvariant [60]. The results were listed below. The PTNet3D\ntaking 3D blocks as input outperformed the 2D variant. This\nﬁnding is in alignment with several previous studies using\nCNN-based model [28], [29]. Similar to CNN-based model,\nPTNet3D also beneﬁts from the 3D information within the\ninput.\nE. Visual Comparisons of Models’ Feature Maps\nWe further visualized and compared internal feature maps\nfrom pix2pixHD-global and PTNets. Fig. 9 a)panel showed\nfeature maps from the decoder path of PTNet-2D with a matrix\nsize of 56× 64; b) panel showed stitched feature maps from\nthe PTNet3D decoder with a matrix size of 56 × 64 (the\noriginal size is 16× 16, and we stitched feature maps from\n16 neighboring blocks to form this one); andc) panel displayed\nfeature maps from pix2pixHD-global with a matrix size of\n56 × 64. It is remarkable that the transformer-based network\ngenerated more structured activations given the same input.\nThe transformer-based PTNet3D model is able to detect very\nﬁne structural details (e.g., edges, textures) and signiﬁcantly\nenhances the feature richnesscompared with the CNN-based\npix2pixHD. We also noticed that the PTNet3D almost pro-\nvided rich and meaningful activation at each channel while the\npix2pixHD generated zero-value activation at some channels.\nWe speculate that such a remarkable difference in feature maps\nmay account for the improved quality of synthesized MRI\nscans.\nF . Application—Prevent Data Exclusion in Downstream\nTasks by Synthesizing Corrupted Scans\nTo showcase the potential utility of our PTNet3D, we ﬁrst\nquantitatively demonstrated how corrupted scans could affect\ndownstream processing and analysis. We take the segmentation\nZHANG et al.: PTNet3D: 3D HIGH-RESOLUTION LONGITUDINAL INFANT BRAIN MRI SYNTHESIZER 2935\nTABLE VII\nSEGMENTATION RESULTS UNDER DIFFERENT RATI OS OFCORRUPTED\nOR SYNTHESIZED SCANS\nas an example, which is an important step towards both\nvolume-based and surface-based analysis of brain regions.\nUsing the test dataset introduced inSection IV.C,w eﬁ r s t\nrandomly sampled 5%, 10%, 15%, 20%, and 25% of good\nquality T2 scans in the original test dataset. Next, we added\nrandom motion artifacts to these scans. Then we investigated\nhow these corrupted scans might affect segmentation results\nand how synthesized ones using PTNet3D could attenuate the\nproblem.\nRandom motion artifacts were introduced using approaches\ndeveloped in previous studies [60], [61]. Details and examples\nof motion artifact injection were provided in Appendix G .\nThese datasets which contain d ifferent ratios of corrupted\nscans were fed into the pre-trained segmentation model\ndescribed in Section IV.C. The average dice score, mean\nsurface distance, and mean 95% HD across 87 regions were\nreported in Table 7.\nTable 7 shows that the segmentation performance will\ncontinue to deteriorate as more corrupted scans are included.\nAverage Dice for 87 brain regions fell 5% when the ratio of\ncorrupted scans rose to 25%. In the same scenario, we also\nnoticed a 215% and 194% increase in the ASD and HD95,\nrespectively. Contrary to this, when replacing corrupted scans\nwith synthesized scans using our PTNet3D, the average Dice\nonly dropped by 1%. Meanwhile, mean ASD and HD95 only\nincreased by 15% and 6%, respectively.\nFig. 10 shows an example of how PTNet3D can improve\nsegmentation. From the error map. it is evident that the\ncorrupted T2w scan impairs the segmentation, especially at\nthe boundary between white matter and gray matter. These\nerrors could be further propagated if surface-based analysis\nis performed on such masks. These corrupted scans are typi-\ncally excluded to prevent inaccurate surface analysis, which\nwill reduce the data availability. From Table 7, results of\nsegmenting synthesized scans were signiﬁcantly better than\nthose of segmenting corrupted scans. It was also very close to\nsegmenting good-quality scans. Therefore, we conclude that\nour proposed PTNet3D can be used to synthesize MRI scans\nto surrogate the corrupted ones. This will avoid data exclusion\nin downstream tasks and allow for larger sample size in infant\nMRI brain studies.\nV. D\nISCUSSIONS\nIn this work, we introduced a novel 3D MRI synthesis\nframework – PTNet3D – speciﬁcally for infants and tod-\ndlers. The convolution-free PTNet3D ﬁrst introduces per-\nformer and transformer together into brain MRI synthesis\ntask. We compared its performance with other state-of-the-\nart CNN-based models on two independent and large-scale\ninfant MRI datasets. The results of extensive experiments show\nthat PTNet3D consistently outperforms other models. More\nimportantly, PTNet3D is able to tackle the challenging task\nof synthesizing longitudinal infant scans, which have different\ntissue contrast and appearances at each age. It performs consis-\ntently well on the longitudinal BCP dataset. In contrast, other\nCNN-based models fail to synthesize good quality scans for\ninfants under 6 months. We also found that our PTNet3D could\nextract more structured and richer features than CNN-based\nmodels. This may partially explain its superior performance.\nOur PTNet3D solves the intensive computation requirement\nfor the attention-based transformer block by incorporating a\nnovel performer block that approximates the full-rank attention\nmechanism by FA VOR+. This allows the processing of 64\n3\nhigh-resolution image blocks. PTNet3D is also equipped with\na pyramid layer. This allows it to avoid information loss caused\nby performer blocks by taking advantage of the pyramid\nrepresentation of images, which has been proven to beneﬁt\nvision tasks in the past. To provide insights into the early\nstages of application of transformer in MRI synthesis tasks,\nwe conducted extensive ablation studies on the loss functions,\nnumber of pyramid layers, as well as 2D/3D comparisons.\nWe found that unlike CNN-based models introduced in previ-\nous studies, PTNet3D could achieve exceptional performance\nin intensity-based metrics by using only L2 loss. After incor-\nporating adversarial loss and perceptual loss – two important\ncomponents in CNN-based GAN model training, PTNet3D\navoids over-smooth results and yields more realistic scans with\nrich high-frequency components. In addition, in agreement\nwith previous studies, we found that pyramid layer and 3D\ninput effectively improve the performance of PTNet3D.\nIn addition to the direct comparison using quantitative\nmetrics, we conducted indirect comparison among PTNet3D\nand other methods by comparing segmentation results on\nsynthesized scans. The resultsindicate that synthesized scans\nfrom PTNet3D are more reliable than CNN-based models.\nMoreover, we demonstrated an important use for PTNet3D –\navoiding data exclusion by replacing corrupted scans with syn-\nthesized ones. PTNet3D will offer developmental researchers a\nvaluable tool to investigate the developing brain. Longitudinal\nMRI studies that investigate brain development from infancy\nto toddlerhood may suffer from substantial data loss, with\n2936 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\nFig. 10. Using PTNet3D in real world application. Two concatenated\ninputs (good-quality T1w + corrupted T2w, good-quality T1w + syn-\nthesized T2w) are fed into a dual-channel 3D UNet. The bottom panel\nvisualizes the segmentation maps from different inputs. From left to right:\nsegmentation from corrupted scans, ground truth released by dHCP\nstudy, and segmentation from synthesized scans.\nmany infant MRI datasets having either a T1w or T2w scan,\nbut not both. This may, for example, present a signiﬁcant\nchallenge for the recently launched Healthy Brain and Child\nDevelopment Study [63], which aims to enroll>10,000 sub-\njects. Considerable data loss could hinder within- and between-\nsubject analysis, which is critical for modeling neurodevel-\nopmental trajectories. We demonstrated that PTNet3D offers\nan approach to address these incomplete datasets maximizing\ntheir utility with minimal loss of quality.\nThough we have demonstrated that PTNet3D performed\nbetter than CNN-based counterparts on the large-scale dHCP\ndataset and the longitudinal BCP dataset, we cannot ignore\nseveral limitations in this work. First, we noticed that the\nquality of MRI synthesis from the BCP dataset was not as\nhigh as that from the dHCP dataset. This drop in quality\nmight be attributed to, 1) pulse sequence differences between\ndHCP and BCP during MRI acquisitions, 2) dynamic and large\nbrain tissue contrast shifts as the brain is developing, and 3)\na relatively small sample size at each age in BCP dataset.\nTo increase the stability and accuracy of multimodal infant\nMRI synthesis, future work should focus on a few important\naspects: 1) increasing the sample size of high-quality infant\nMRI scans at each time point especially the-ﬁrst-year scans; 2)\nincorporating the age as a domain classiﬁcation label into the\nadversarial training framework; 3) exploring other potential\nvariants of PTNet3D to further improve synthesis accuracy.\nFurther studies can also extend PTNet3D to other modalities,\nFig. 11. Examples of data inclusion and exclusion. Scans(a-d) were\nincluded during model development, and(e-h) were excluded. Scan(a)\nhas the best quality while(b) and (c) are slightly worse than a. And scan\n(d) has minor artifacts (circled region) and was not excluded since it is\nacceptable. Scans (e-h) were excluded because of their unacceptable\nqualities.\nages, and species of medical images. We provide public access\nto our code via GitHub: https://github.com/XuzheZ/PTNet3D.\nAPPENDIX\nA. Data Preprocessing and Exclusion\nTo remove outliers with extremely high intensities, each\nvolume was normalized to [0,1] by its minimum intensity\nvalue and 99.95 percentile maximum intensity value. The\ndHCP scans have a matrix size of 290× 290 × 203 and was\ncropped to 224 × 256 × 202 by removing background. The\noriginal matrix size of BCP is 208 × 300 × 213. The BCP\nscans underwent the same preprocessing as dHCP scans did,\nincluding cropping, padding, and normalization. The resultant\nmatrix size is 224× 256 × 210.\nThe quality of skull-stripping and co-registration was\nassessed by a senior MRI technician. We excluded paired\nscans with motion/scanner artifacts, poor skull stripping, or co-\nregistration problem. In total, we included 416 paired T1w and\nT2w scans from dHCP and 231 paired T1w and T2w scans\nfrom age of 2 months to 34 months from BCP . We provide\nsome example scans inFig. 11.\nFor the BCP dataset, 156 scans were used for training;\n29 scans were used for validation, and the remaining 46 scans\nwere used for testing by partially random sampling. The data\nsplit was randomly generated based on the following rules: 1)\nThe ratio of training, validation, and testing datasets is 7:1:2;\n2) All scans were randomly sampled to three partitions based\non the ratio; 3): At least one scan per age was guaranteed to\nbe distributed to each partition; 4) No signiﬁcant distribution\ndifference across three groups. The exact age distribution can\nbe found inFig. 12.\nIn addition, to make sure the age distribution is balanced in\nthe training set, we applied data augmentation for the training\nscans while the distribution of validation and testing scans\nremain unchanged. For the dataaugmentation, a 3D rotation\nalong the x-y axis was applied. Given the maximum frequency\nof scans at 24 months of age (n= 25), for other ages with\nM scans, we randomly sampled one scan from that age and\napplied rotation by\n25◦\nn−M × X,X = 1,2,... n − M. After\nZHANG et al.: PTNet3D: 3D HIGH-RESOLUTION LONGITUDINAL INFANT BRAIN MRI SYNTHESIZER 2937\nTABLE VIII\nALL MODELS ’DETAILS\ndata augmentation, each age had 25 scans in the training set,\nresulting in 625 scans.\nB. Training Conﬁgurations\nWe provided comparisons of computational cost inTable 8.\nFor a fair comparison, all models were ﬁrst trained at a ﬁxed\nlearning rate 2e−4 for 5 epochs then another 5 epochs at a\nlinearly decreasing learning rate (to 0) on the dHCP dataset.\nSimilarly, on the BCP dataset, all models were trained for\n3 epochs at a ﬁxed learning rate and another 3 epochs at\na linearly decreasing learning rate because there were more\nscans. After training, the model with the highest Structural\nSimilarity Index Measure (SSIM) on the validation dataset was\nselected for comparison on the testing dataset. All experiments\nwere trained on an GeForce RTX 2080 TI with 11GB memory\nand an NVIDIA Titan RTX GPU with 24 GB memory. The\nentire framework was implemented in PyTorch, and publically\navailable on GitHub ( https://github.com/XuzheZ/PTNet3D).\nC. Synthetic Results at Different Time Points on BCP\nDataset\nWe provided the distributions of pSNR for each model and\nage group separately in Fig. 13. Our PTNet3D consistently\nshows the best performance.\nD. Introduction to the Evaluation Metrics\nWe employed four metrics to evaluate synthetic results\nfrom different perspectives. The MAE and pSNR assess the\nimage quality from the accuracy of tissue intensity restoration\nbetween the synthetic and real scans. SSIM is correlated to the\nquality perception of the human visual system in the perspec-\ntive of distortion and degradation of structural information.\nAlthough MAE, pSNR, and SSIM are widely used in previous\nimage synthesis tasks, they might ignore the high-frequency\ncomponents that also play a critical role in visual perception.\nFID evaluates the performance of GANs in terms of visual\nsimilarity and is more consistent with human judgment than\nthe Inception Score. The detailed mathematical justiﬁcation\ncan be found in the original studies [56]–[58].\nFig. 12. Age distribution of the BCP dataset that is used for training,\nvalidation, and testing.\nFig. 13. Boxplots for T1w-to-T2w synthesis (a) and T2w-to-T1w\nsynthesis (b) on multi-age BCP dataset.\nIn our study, we normalized the ground truth and synthe-\nsized volumes to the same intensity range [0,1]. SSIM and\nPSNR were calculated using Eq. (22) and Eq. (23), to be\n2938 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\nFig. 14. Boxplots for regional Dice score on synthesizeddHCP scans by PTNet3D and pix2pixHD-Local.\nFig. 15. Visualization of original T2w scans (left) and generated motion-\ncorrupted T2w scans (right). From top to bottom: sagittal, coronal, and\naxial. The left bottom number indicates different subjects.\nnoted that, in Eq. (22) and Eq. (23), X and Y represented\nvolume instead of slice; μ indicated mean intensity; σ was\nstandard deviation; σXY was the covariance between X and\nY ; positive constant C was used to prevent division by zero.\nThe MAE was also calculated on a volume basis. FID can\nonly be calculated on 2D slices, so we calculated subject-wise\nFID by taking the average FID scores from three orientations\n(i.e., sagittal, coronal, and axial).\nPSNR (X,Y ) = 10log\n10\n( 1\nMSE (X,Y )\n)\n(22)\nSSIM (X,Y ) = 2μX μY + C1\nμ2\nX + μ2\nY + C1\n2σX σY + C2\nσ2\nX + σ2\nY + C2\nσXY + C3\nσX σY + C3\n(23)\nE. Detail Conﬁgurations of 3D Dual-Channel UNet for\nWhole Brain Segmentation\nWe used vanilla 3D UNet with the following parameters:\n1. Encoding Conv block: {GroupNormalization\n(num_groups = 8), Conv3D layer (input_channel,\noutput_channel), ReLU} + {GroupNormalization\n(num_groups = 8), Conv3D layer (output _channel,\noutput_channel), ReLU}\n2. Layers of Encoding Conv block: 4\n3. Pooling layer used in the encoding path: 3D max pooling\nwith stride = 2, kernel size=2\n4. Input channels for each encoding block: 2, 32, 64, 128\n5. Output channels for eachencoding block: 32, 64, 128,\n256\n6. Bottleneck: N/A\n7. Decoding Conv block: {GroupNormalization\n(num_groups = 8), Conv3D layer (input_channel,\noutput_channel), ReLU} + {GroupNormalization\n(num_groups = 8), Conv3D layer (output _channel,\noutput_channel), ReLU}\n8. Layers of Encoding Conv block: 3\n9. Upsampling layer: 3D nearest interpolation upsampling\n10. Input channels for each decoding block: 384, 192, 96\n11. Output channels for eachdecoding block: 128, 64, 32\n12. Final Conv block: Conv3D (32,88)\n13. Input 3D patch size: 128× 128 × 128\nZHANG et al.: PTNet3D: 3D HIGH-RESOLUTION LONGITUDINAL INFANT BRAIN MRI SYNTHESIZER 2939\nF . Dice of 87 Segmented Regions on Synthesized Scans\nWe reported regions with signiﬁcant higher Dice score while\nusing PTNet3D to synthesize the T2w MRI rather than using\npix2pixHD-Local in Fig. 14. Reader can ﬁnd the correspond-\ning brain region of each tag number in the publication of dHCP\nstudy [7].\nG. Generation of Motion-Artifacted Scans\nFollowing the previous studies [60], [61], we generated\nrandom motion-corrupted scans with these parameters: random\nrotation (−7.5, 7.5) degrees, random translation (−7.5, 7.5)\nmm, and the number of transformations: 6. The randomization\nfollows a uniform distribution. We randomly chose 4 subjects\nand provided visualizations of injecting motion artifacts in\nthree orientations (i.e., sagittal, coronal, and axial) (Fig. 15).\nR\nEFERENCES\n[1] S. J. Short et al., “Associations between white matter microstructure\nand infants’ working memory,” NeuroImage, vol. 64, pp. 156–166,\nJan. 2013, doi:10.1016/j.neuroimage.2012.09.021.\n[2] M. N. Spann, R. Bansal, T. S. Rosen, and B. S. Peterson, “Morpholog-\nical features of the neonatal brain support development of subsequent\ncognitive, language, and motor abilities,”Hum. Brain Mapping, vol. 35,\nno. 9, pp. 4459–4474, Sep. 2014, doi:10.1002/hbm.22487.\n[3] A. M. Fjellet al., “Multimodal imaging of the self-regulating developing\nbrain,” Proc. Nat. Acad. Sci. USA, vol. 109, no. 48, pp. 19620–19625,\nNov. 2012, doi:10.1073/pnas.1208243109.\n[4] J. O’Muircheartaigh et al., “White matter development and early cog-\nnition in babies and toddlers,” Hum. Brain Mapping, vol. 35, no. 9,\npp. 4475–4487, Sep. 2014, doi:10.1002/hbm.22488.\n[5] M. Prastawa, J. H. Gilmore, W. Lin, and G. Gerig, “Automatic segmenta-\ntion of MR images of the developing newborn brain,”Med. Image Anal.,\nvol. 9, no. 5, pp. 457–466, Oct. 2005, doi:10.1016/j.media.2005.05.007.\n[6] J. H. Gilmore, R. C. Knickmeyer, and W. Gao, “Imaging structural and\nfunctional brain development in early childhood,”Nature Rev. Neurosci.,\nvol. 19, no. 3, pp. 123–137, 2018, doi:10.1038/NRN.2018.1.\n[7] A. Makropoulos et al., “The developing human connectome project: A\nminimal processing pipeline for neonatal cortical surface reconstruc-\ntion,” NeuroImage, vol. 173, pp. 88–112, Jun. 2018, doi: 10.1016/j.\nneuroimage.2018.01.054.\n[8] L. Zöllei, J. E. Iglesias, Y . Ou, P. E. Grant, and B. Fischl,\n“Infant FreeSurfer: An automated segmentation and surface extraction\npipeline for T 1-weighted neuroimaging data of infants 0–2 years,”\nNeuroImage, vol. 218, Sep. 2020, Art. no. 116946, doi: 10.1016/j.\nneuroimage.2020.116946.\n[9] L. Wang et al., “V olume-based analysis of 6-month-old infant brain\nMRI for autism biomarker identiﬁcation and early diagnosis,” inProc.\nInt. Conf. Med. Image Comput. Comput.-Assisted Intervent. (MICCAI),\nvol. 11072, Sep. 2018, pp. 411–419, doi: 10.1007/978-3-030-00931-\n1_47.\n[10] J. Dolz, C. Desrosiers, L. Wang, J. Yuan, D. Shen, and I. B. Ayed,\n“Deep CNN ensembles and suggestive annotations for infant brain MRI\nsegmentation,” Computerized Med. Imag. Graph., vol. 79, Jan. 2020,\nArt. no. 101660, doi:10.1016/j.compmedimag.2019.101660.\n[11] Y. S u net al., “Multi-site infant brain segmentation algorithms: The iSeg-\n2019 challenge,”I E E ET r a n s .M e d .I m a g ., vol. 40, no. 5, pp. 1363–1376,\nMay 2021, doi:10.1109/TMI.2021.3055428.\n[12] S. U. H. Dar, M. Yurt, L. Karacan, A. Erdem, E. Erdem, and T. Çukur,\n“Image synthesis in multi-contrast MRI with conditional generative\nadversarial networks,” I E E ET r a n s .M e d .I m a g ., vol. 38, no. 10,\npp. 2375–2388, Oct. 2019, doi:10.1109/TMI.2019.2901750.\n[13] A. Jog, A. Carass, D. L. Pham, and J. L. Prince, “Random forest\nﬂair reconstruction from T\n1,T 2, and PD-weighted MRI,” in Proc.\nIEEE Int. Symp. Biomed. Imag. , Apr. 2014, pp. 1079–1082, doi:\n10.1109/ISBI.2014.6868061.\n[14] J. E. Iglesias, E. Konukoglu, D. Zikic, B. Glocker, K. Van Leemput,\nand B. Fischl, “Is synthesizing MRI contrast useful for inter-modality\nanalysis,” in Proc. Int. Conf. Med. Image Comput. Comput.-Assist.\nIntervent, vol. 16, no. 1, 2013, pp. 631–638, doi:10.1007/978-3-642-\n40811-3_79.\n[15] A. Jog, A. Carass, S. Roy, D. L. Pham, and J. L. Prince, “MR image\nsynthesis by contrast learni ng on neighborhood ensembles,” Med.\nImage Anal., vol. 24, no. 1, pp. 63–76, Aug. 2015, doi: 10.1016/\nj.media.2015.05.002.\n[16] N. Burgos et al., “Attenuation correction synthesis for hybrid PET-MR\nscanners: Application to brain studies,”I E E ET r a n s .M e d .I m a g ., vol. 33,\nno. 12, pp. 2332–2341, Dec. 2014, doi:10.1109/TMI.2014.2340135.\n[17] M. I. Miller, G. E. Christensen, Y . Amit, and U. Grenander,\n“Mathematical textbook of deformable neuroanatomies,” Proc. Nat.\nAcad. Sci. USA , vol. 90, no. 24, pp. 11944–11948, 1993, doi:\n10.1073/PNAS.90.24.11944.\n[18] S. Roy, A. Carass, and J. L. Prince, “Magnetic resonance image example-\nbased contrast synthesis,” I E E ET r a n s .M e d .I m a g ., vol. 32, no. 12,\npp. 2348–2363, Dec. 2013, doi:10.1109/TMI.2013.2282126.\n[19] S. Roy, A. Carass, and J. Prince, “A compressed sensing approach for\nMR tissue contrast synthesis,” in Information Processing in Medical\nImaging, G. Székely H. K. Hahn, Eds. Berlin, Germany: Springer, 2011,\npp. 371–383.\n[20] A. Jog, A. Carass, S. Roy, D. L. Pham, and J. L. Prince, “Random forest\nregression for magnetic resonance image synthesis,”Med. Image Anal.,\nvol. 35, pp. 475–488, Jan. 2017, doi:10.1016/j.media.2016.08.009.\n[21] A. Jog, S. Roy, A. Carass, and J. L. Prince, “Magnetic resonance image\nsynthesis through patch regression,” inProc. IEEE 10th Int. Symp. Bio-\nmed. Imag., Apr. 2013, pp. 350–353, doi:10.1109/ISBI.2013.6556484.\n[22] A. Chartsias, T. Joyce, M. V . Giuffrida, and S. A. Tsaftaris, “Multi-\nmodal MR synthesis via modality-invariant latent representation,”IEEE\nTrans. Med. Imag. , vol. 37, no. 3, pp. 803–814, Mar. 2018, doi:\n10.1109/TMI.2017.2764326.\n[23] I. J. Goodfellow et al., “Generative adversarial nets,” presented at the\n27th Int. Conf. Neural Inf. Process. Syst., vol. 2. Montreal, QC, Canada,\n2014.\n[24] T. Karras, T. Aila, S. Laine, and J. Lehtinen, “Progressive grow-\ning of gans for improved quality, stability, and variation,” 2017,\narXiv:1710.10196.\n[25] T.-C. Wang, M.-Y . Liu, J.-Y . Zhu, A. Tao, J. Kautz, and B. Catanzaro,\n“High-resolution image synthesis and semantic manipulation with condi-\ntional GANs,” inProc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\nJun. 2018, pp. 8798–8807, doi:10.1109/CVPR.2018.00917.\n[26] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, “Image-to-image trans-\nlation with conditional adversarial networks,” in Proc. IEEE Conf.\nComput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 5967–5976, doi:\n10.1109/CVPR.2017.632.\n[27] Q. Yang, N. Li, Z. Zhao, X. Fan, E. I.-C. Chang, and Y . Xu, “MRI cross-\nmodality image-to-image translation,”Sci. Rep., vol. 10, no. 1, p. 3753,\nDec. 2020, doi:10.1038/s41598-020-60520-6.\n[28] S .H u ,B .L e i ,S .W a n g ,Y .W a n g ,Z .F eng, and Y . Shen, “Bidirectional\nmapping generative adversarial networks for brain MR to PET synthe-\nsis,” IEEE Trans. Med. Imag., vol. 41, no. 1, pp. 145–157, Jan. 2022.\n[29] L. Shen et al., “Multi-domain image completion for random missing\ninput data,” IEEE Trans. Med. Imag., vol. 40, no. 4, pp. 1113–1122,\nApr. 2021.\n[30] A. Vaswaniet al., “Attention is all you need,” presented at the 31st Int.\nConf. Neural Inf. Process. Syst., Long Beach, CA, USA, 2017.\n[31] A. Dosovitskiy et al., “An image is worth 16×16 words: Transformers\nfor image recognition at scale,” 2020,arXiv:2010.11929.\n[32] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y . Wang, “Transformer\nin transformer,” inProc. Adv. Neural Inf. Process. Syst., vol. 34, 2021,\npp. 1–10.\n[33] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted Windows,” inProc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV),\nOct. 2021, pp. 10012–10022.\n[34] K. Choromanski et al., “Rethinking attention with performers,” 2020,\narXiv:2009.14794.\n[35] L. Yuanet al., “Tokens-to-Token ViT: Training vision transformers from\nscratch on ImageNet,” 2021,arXiv:2101.11986.\n[36] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional net-\nworks for biomedical image segmentation,” inProc. Med. Image Com-\nput. Comput.-Assist. Intervent, 2015, pp. 234–241.\n[37] B. R. Howellet al., “The UNC/UMN baby connectome project (BCP):\nAn overview of the study design and protocol development,” Neu-\nroImage, vol. 185, pp. 891–905, Jan. 2018, doi:10.1016/j.neuroimage.\n2018.03.049.\n[38] Y . Choi, M. Choi, M. Kim, J.-W. Ha, S. Kim, and J. Choo, “StarGAN:\nUniﬁed generative adversarial networks for multi-domain image-to-\nimage translation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., Jun. 2018, pp. 8789–8797.\n2940 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. 41, NO. 10, OCTOBER 2022\n[39] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros, “Unpaired image-to-\nimage translation using cycle-consistent adversarial networks,” inProc.\nIEEE Int. Conf. Comput. Vis. (ICCV), Oct. 2017, pp. 2242–2251, doi:\n10.1109/ICCV .2017.244.\n[40] T. R. Shaham, T. Dekel, and T. Michaeli, “SinGAN: Learning a\ngenerative model from a single natural image,” inProc. IEEE/CVF Int.\nConf. Comput. Vis. (ICCV), Oct. 2019, pp. 4570–4580.\n[41] Z. Liu et al., “Swin transformer: Hierarchical vision transformer using\nshifted Windows,” 2021,arXiv:2103.14030.\n[42] D. A. Hudson and C. L. Zitnick, “Generative adversarial transformers,”\n2021, arXiv:2103.01209.\n[43] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nProc. Eur. Conf. Comput. Vis., Cham, Switzerland: Springer, 2020,\npp. 213–229.\n[44] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-\nresolution image synthesis,” in Proc. IEEE/CVF Conf. Comput. Vis.\nPattern Recognit. (CVPR), Jun. 2021, pp. 12873–12883.\n[45] Y . Jiang, S. Chang, and Z. Wang, “TransGAN: Two pure transformers\ncan make one strong GAN, and that can scale up,” inProc. Adv. Neural\nInf. Process. Syst., vol. 34, 2021, pp. 1–13.\n[46] A. Vaswaniet al., “Attention is all you need,” 2017,arXiv:1706.03762.\n[47] X. Huang, Y . Li, O. Poursaeed, J. Hopcroft, and S. Belongie,\n“Stacked generative adversarial networks,” inProc. IEEE Conf. Com-\nput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 1866–1875, doi:\n10.1109/CVPR.2017.202.\n[48] P. J. Burt and E. H. Adelson, “The Laplacian pyramid as a compact\nimage code,”IEEE Trans. Commun., vol. COM-31, no. 4, pp. 532–540,\nApr. 1983, doi:10.1109/TCOM.1983.1095851.\n[49] M. Brown and D. G. Lowe, “Recognising panoramas,” in Proc. 9th\nIEEE Int. Conf. Comput. Vis., vol. 2, Oct. 2003, pp. 1218–1225, doi:\n10.1109/ICCV .2003.1238630.\n[50] E. Denton, S. Chintala, A. Szlam, andR. Fergus, “Deep generative image\nmodels using a Laplacian pyramid of adversarial networks,” presented\nat the 28th Int. Conf. Neural Inf. Process. Syst., vol. 1. Montreal, QC,\nCanada, 2015.\n[51] E. H. Adelson, C. H. Anderson, J. R. Bergen, P. J. Burt, and\nJ. M. Ogden, “Pyramid methods in image processing,” RCA Eng.,\nvol. 29, no. 6, pp. 33–41, 1984.\n[52] Z. Wang, Z. Cui, and Y . Zhu, “Multi-modal medical image fusion by\nLaplacian pyramid and adaptive sparse representation,”Comput. Biol.\nMed., vol. 123, Aug. 2020, Art. no. 103823.\n[53] Z. Lei, L. Qi, Y . Wei, and Y . Zhou, “Infant brain MRI segmentation with\ndilated convolution pyramid downsampling and self-attention,” 2019,\narXiv:1912.12570.\n[54] J. Johnson, A. Alahi, and L. Fei-Fei, “Perceptual losses for real-time\nstyle transfer and super-resolution,” in Proc. Eur. Conf. Comput. Vis.,\n2016, pp. 694–711.\n[55] D. Tran, H. Wang, L. Torresani, J. Ray, Y . LeCun, and M. Paluri,\n“A closer look at spatiotemporal convolutions for action recognition,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.\n, Jun. 2018,\npp. 6450–6459.\n[56] A. Hore and D. Ziou, “Image quality metrics: PSNR vs. SSIM,” in\nProc. 20th Int. Conf. Pattern Recognit., Aug. 2010, pp. 2366–2369, doi:\n10.1109/ICPR.2010.579.\n[57] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image\nquality assessment: From error visibility to structural similarity,”IEEE\nTrans. Image Process., vol. 13, no. 4, pp. 600–612, Apr. 2004, doi:\n10.1109/TIP.2003.819861.\n[58] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter,\n“GANs trained by a two time-scale update rule converge to a local nash\nequilibrium,” in Proc. Adv. Neural Inf. Process. Syst., vol. 30, 2017,\npp. 1–60.\n[59] Ö. Çiçek, A. Abdulkadir, S. S. Lienka mp, T. Brox, and O. Ronneberger,\n“3D U-Net: Learning dense, volumetric segmentation from sparse\nannotation,” in Proc. Int. Conf. Med. Image Comput. Comput.-Assist.\nIntervent., Cham, Switzerland: Springer, 2016, pp. 424–432.\n[60] X. Zhanget al., “PTNet: A high-resolution infant MRI synthesizer based\non transformer,” 2021,arXiv:2105.13993.\n[61] R. Shaw, C. Sudre, S. Ourselin, and M. J. Cardoso, “MRI k-space motion\nartefact augmentation: Model robustness and task-speciﬁc uncertainty,”\npresented at the 2nd Int. Conf. Med. Imag. With Deep Learn., 2019.\n[Online]. Available: https://proceedings.mlr.press/v102/shaw19a.html\n[62] F. Pérez-García, R. Sparks, an d S. Ourselin, “TorchIO: A Python\nlibrary for efﬁcient loading, prep rocessing, augmentation and patch-\nbased sampling of medical images in deep learning,” Comput. Meth-\nods Programs Biomed., vol. 208, Sep. 2021, Art. no. 106236, doi:\n10.1016/j.cmpb.2021.106236.\n[63] C. J. Jordan, S. R. B. Weiss, K. D. Howlett, and M. P. Freund,\n“Introduction to the special issue on ‘informing longitudinal studies on\nthe effects of maternal stress and substance use on child development:\nPlanning for the HEALthy brain and child development (HBCD) study,”’\nAdversity Resilience Sci., vol. 1, no. 4, pp. 217–221, Dec. 2020, doi:\n10.1007/s42844-020-00022-6.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.4997408390045166
    },
    {
      "name": "High resolution",
      "score": 0.4646211564540863
    },
    {
      "name": "Computer science",
      "score": 0.4509533941745758
    },
    {
      "name": "Magnetic resonance imaging",
      "score": 0.42207515239715576
    },
    {
      "name": "Medical physics",
      "score": 0.4005444049835205
    },
    {
      "name": "Medicine",
      "score": 0.2262745201587677
    },
    {
      "name": "Engineering",
      "score": 0.18919888138771057
    },
    {
      "name": "Radiology",
      "score": 0.16279879212379456
    },
    {
      "name": "Electrical engineering",
      "score": 0.14337840676307678
    },
    {
      "name": "Voltage",
      "score": 0.1312463879585266
    },
    {
      "name": "Remote sensing",
      "score": 0.09094324707984924
    },
    {
      "name": "Geology",
      "score": 0.0801374614238739
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210126298",
      "name": "Duke Medical Center",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210153043",
      "name": "Duke University Hospital",
      "country": "US"
    }
  ]
}