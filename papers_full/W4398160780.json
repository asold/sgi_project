{
  "title": "Concept-Guided LLM Agents for Human-AI Safety Codesign",
  "url": "https://openalex.org/W4398160780",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2207358218",
      "name": "Florian Geissler",
      "affiliations": [
        "Fraunhofer Institute for Cognitive Systems"
      ]
    },
    {
      "id": "https://openalex.org/A1981631086",
      "name": "Karsten Roscher",
      "affiliations": [
        "Fraunhofer Institute for Cognitive Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2163232796",
      "name": "Mario Trapp",
      "affiliations": [
        "Fraunhofer Institute for Cognitive Systems"
      ]
    },
    {
      "id": "https://openalex.org/A2207358218",
      "name": "Florian Geissler",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1981631086",
      "name": "Karsten Roscher",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2163232796",
      "name": "Mario Trapp",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4389421988",
    "https://openalex.org/W4377111679",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4285092328",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1539949938",
    "https://openalex.org/W2145071552",
    "https://openalex.org/W4386712343",
    "https://openalex.org/W4380993239",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2616372743",
    "https://openalex.org/W4387891144"
  ],
  "abstract": "Generative AI is increasingly important in software engineering, including safety engineering, where its use ensures that software does not cause harm to people. This also leads to high quality requirements for generative AI. Therefore, the simplistic use of Large Language Models (LLMs) alone will not meet these quality demands. It is crucial to develop more advanced and sophisticated approaches that can effectively address the complexities and safety concerns of software systems. Ultimately, humans must understand and take responsibility for the suggestions provided by generative AI to ensure system safety. To this end, we present an efficient, hybrid strategy to leverage LLMs for safety analysis and Human-AI codesign. In particular, we develop a customized LLM agent that uses elements of prompt engineering, heuristic reasoning, and retrieval-augmented generation to solve tasks associated with predefined safety concepts, in interaction with a system model graph. The reasoning is guided by a cascade of micro-decisions that help preserve structured information. We further suggest a graph verbalization which acts as an intermediate representation of the system model to facilitate LLM-graph interactions. Selected pairs of prompts and responses relevant for safety analytics illustrate our method for the use case of a simplified automated driving system.",
  "full_text": "Concept-Guided LLM Agents for Human-AI Safety Codesign\nFlorian Geissler1, Karsten Roscher1, and Mario Trapp1, 2\n1 Fraunhofer IKS, Fraunhofer Institute for Cognitive Systems IKS, Munich, Germany\n2 School of Computation, Information and Technology, Technical University of Munich, Germany\n{florian.geissler, karsten.roscher, mario.trapp}@iks.fraunhofer.de, mario.trapp@tum.de\nAbstract\nGenerative AI is increasingly important in software engineer-\ning, including safety engineering, where its use ensures that\nsoftware does not cause harm to people. This also leads to\nhigh quality requirements for generative AI. Therefore, the\nsimplistic use of Large Language Models (LLMs) alone will\nnot meet these quality demands. It is crucial to develop more\nadvanced and sophisticated approaches that can effectively\naddress the complexities and safety concerns of software sys-\ntems. Ultimately, humans must understand and take respon-\nsibility for the suggestions provided by generative AI to en-\nsure system safety. To this end, we present an efficient, hybrid\nstrategy to leverage LLMs for safety analysis and Human-AI\ncodesign. In particular, we develop a customized LLM agent\nthat uses elements of prompt engineering, heuristic reason-\ning, and retrieval-augmented generation to solve tasks asso-\nciated with predefined safety concepts, in interaction with a\nsystem model graph. The reasoning is guided by a cascade\nof micro-decisions that help preserve structured information.\nWe further suggest a graph verbalization which acts as an in-\ntermediate representation of the system model to facilitate\nLLM-graph interactions. Selected pairs of prompts and re-\nsponses relevant for safety analytics illustrate our method for\nthe use case of a simplified automated driving system.\nIntroduction\nThe advent of transformer-based (Vaswani et al. 2017) large\nlanguage models (LLMs) has sparked enormous popularity\nof generative artificial intelligence (AI) for creative, text-\nbased tasks. Representative of this trend is the reported\nrecord of OpenAI’s ChatPGT for the fastest growing user\nbase of all times in February 2023 (Reuters 2023). The\nhigh quality of auto-generated text has inspired the explo-\nration of LLMs for tasks that involve structured data, such as\nknowledge graphs. In particular, a widely desired use case\nis the application of LLMs to safety-analytical tasks (Jin\net al. 2023; Pan et al. 2023; Wang et al. 2023), for exam-\nple, reasoning about fault propagation in model graphs and\nassociated risks. Attempts to make use of LLMs for a for-\nmal hazard analysis have been made with moderate success\n(Diemert and Weber 2023).\nA particular challenge lies in the transformation of ver-\nbalized content to a structured graph information, and vice\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nIntermediate \nrepresentation Database/toolset\nPrompt\nRespond\nRetrieve/Update \nInformation\nDisplay\nGive input\nCall\nAgent\nSystem model\nUser interface\nLLM\nFigure 1: Layout of the Human-AI safety codesign frame-\nwork: A user interacts with the LLM agent through a chat\nprompt and receives a text response. The LLM agent inter-\nfaces to a database containing a graph description of the sys-\ntem model in IR, as well as safety concepts, and analysis\ntools. System manipulations can update the database and al-\nter the system model. The system model and its changes are\ndisplayed to the user.\nversa. Since the text generation process is of statistical na-\nture, thus not bound by logical constraints, LLM responses\nwill not necessarily preserve information structures of a\ngiven input. Existing strategies to tackle this problem can be\nbroadly categorized as follows (Jin et al. 2023): 1) Prompt-\nengineering to encourage specific output formats or struc-\ntural rules; 2) Heuristic or algorithmic reasoning: Encour-\naging the LLM to perform chain-of-thought (CoT) reason-\ning, for example, to solve the problem step by step follow-\ning self-generated or predetermined instructions; 3) Making\nuse of external knowledge using retrieval-augmented gener-\nation (RAG), for example via non-AI tools; 4) Fine-tuning or\nretraining models to empirically minimize the loss of struc-\ntured information, see e.g., GraphGPT (Tang et al. 2023).\nThis article presents a concept-guided approach aimed at\nenhancing the capabilities of LLMs for graph analysis and\nmanipulation, particularly in the context of safety-relevant\ndevelopments. To achieve this goal, we have integrated the\nstrengths of LLMs with the rigorous standards of safety en-\ngineering models and analysis. Starting from a structured\nsystem model graph, we first establish a verbalized interme-\nAAAI Spring Symposium Series (SSS-24)\n100\nDatabase of system\ndescription, relevant \ndocuments and \nconcepts\nConcept decision:\n1) Safety question answering\n2) System safety analysis\n3) Suggestions for fault-tolerance\n4) Other\nPrompt\nAnswering concept:\n1a) Knowledge retrieval\n1\nAnalysis concept:\n2a) Fault evaluation\n2b) Critical path\n2c) Single point of failure\nAugment with relevant text\nexcerpts\nFault propagation• Identify faulty start nodes\n• Decide on and possibly specify\nexisting faults\nFault-tolerance concept:\n3a) Node Replication\n3b) Edge replication\n3\nGenerate replyResponse\nCritical path\ncalculation\nFind single points of\nfailure\n• Identify best components/ \nedges for replication\n• Retrieve instructions for\nrelevant concept and map it to\ngraph manipulation sequence\nCascading Decision Layer for Task Identification Task Formulation Layer Information retrieval\n4\n2\nLLM agent Database/Toolset\nTooling/ \nLookup\n…\nGraph node/edge\nreplication\nFigure 2: Outline of the workflow of LLM agent decisions and database interactions. The custom agent runs a cascading\ndecision layer to identify the task type, and a subsequent layer to formulate the task for information retrieval. The latter looks\nup knowledge from a vectorstore database, or uses functional tools to calculate for example the critical path.\ndiate representation (IR) of the system to facilitate the com-\nprehension of its components and relationships by the LLM.\nA custom LLM agent is designed who deploys a hybrid\nstrategy featuring the techniques 1) to 3) above: The agent\nperforms a series of LLM calls to categorize and formal-\nize the task at hand according to predefined concepts. Sub-\nsequently, RAG techniques are leveraged to offload struc-\ntural computations to external functions. We test our method\nfor the example of a simplified automated driving architec-\nture, and present selected experimental results for the tasks\nof fault propagation, critical path finding, single-points-of-\nfailure detection, and node replication. Our work provides\nthe basis for an interactive, LLM-based Human-AI safety\ncodesign framework.\nModel\nThe overall system layout is shown in Fig. 1. Key compo-\nnents of our architecture are explained in the following.\nSystem model and intermediate representation:The\nsystem model contains the system architecture, including\nthe system components and their interactions, as well as ad-\nditional safety-related information required to model fault\npropagation within the system. To ensure interoperability\nwith established industrial meta model formats, we cre-\nate the system model with the Eclipse-based OSATE tool\n(Carnegie Mellon University 2023) and export the resulting\nECore file as xml. The elements of the xml model (EClass,\nEReference, EAttribute) can be directly mapped onto the el-\nements of a generic graph (nodes, edges, attributes). Even\nthough LLMs are capable of reading and interpreting xml\nstructures directly, we find that inaccuracies can be reduced\nwhen operating with a system description that has a closer\nresemblance with natural language. Therefore, we further\nverbalize the xml system model to an IR which takes an in-\ntuitive list structure:\nNodes:\n- Node 1\n- Node 2\n- ...\nEdges:\n- Node 1 --> Node 2\n- ...\nAttributes:\n- Node 1: Attribute 1\n- ...\nThis IR represents the basis of system-relevant informa-\ntion for the LLM agent. For safety analysis, we assume that\neach node is subject to possible failure. In order to model\nthe propagation of system faults, we populate graph node\nattributes with verbalized logic of fault gates (e.g., AND,\nOR, N-out-of-M) to represent fault trees (Avi ˇzienis et al.\n2004; Trapp 2016). Further, start and end nodes of the sys-\ntem graph are specified with corresponding attributes. For\nthe current proof-of-concept, we used an integrated system-\nsafety model. However, in model-based safety engineering,\nlong-lasting work has been conducted on integrating system\nand safety models (Domis and Trapp 2008), which can be\nused as a scalable basis for further development.\nLLM: At the core, we use OpenAI’s GPT 3.5 − turbo\n(OpenAI 2023) model for LLM inferences. As our concept-\nguided approach requires the LLM to solve a series of rather\nsimple micro-decisions, we expect to see in further work,\nthat also much smaller and simpler models such as LLama2\n(Touvron et al. 2023) or Mistral-7B (Jiang et al. 2023) suf-\nfice for this purpose.\n101\nFigure 3: Example use case of a simplified automated driving system. The node labels denote the component name (top row)\nand the fault gate attribute below (if none is given, an AND fault gate of all inputs is assumed). 2OO3 means that two out of\nthree of the listed inputs are required. Start and end nodes are further labeled explicitly. The figure is a pydot visualization of\nthe ECore file.\nLLM agent: LLMs are conveniently orchestrated by\nfunctional wrappers called agents, which can be configured\nto self-induce an iterative chain of thoughts, or to interface\nautomatically to external tools and sources of information.\nWe here use theLangChain library (Harrison Chase 2022) to\ndesign a customized agent. Importantly, we find that equip-\nping an agent with multiple tools can quickly lead to inac-\ncurate tool usage, unless the verbalized trigger conditions\nare well-separated in linguistic space. At the same time, for\nself-induced CoT flows, it is challenging to incorporate and\nassert procedural safety guardrails.\nWe therefore implement a different workflow, as illus-\ntrated in Fig. 2: The agent cascades the input prompt through\na network of micro-decisions, where a single LLM call as-\nsociates the input with the best match among only 2-4 pre-\ndefined concepts in each decision. For example, as shown in\nFig. 2, in the first decision node the agent associates the input\nwith one of four possible task concepts of Safety question\nanswering, System safety analysis, Suggestions for fault-\ntolerance, and Other. To improve the quality of the micro-\ndecision, we use few-shot learning (Brown et al. 2020) by\naugmenting the prompts with around ten hand-crafted exam-\nple triggers for each concept. The last layer of our decision\nnetwork is a Task formulation layer, where the fully identi-\nfied task concept is used together with the given prompt to\nrefine a specific task prompt. Finally, the external tools or\ninformation lookup mechanisms that are provided for this\nspecific task are called and the result is rephrased as a user\nresponse. We find that this method overcomes some of the\nchallenges mentioned above: The CoT flow is tightly guided\nby predefined concepts, yet allows sufficient flexibility in the\nprompt formulation, and tool triggers cannot overlap since\nthey are strictly associated with specific task concepts.\nDatabase/toolset: We provide a collection of information\nthat is retrieved from a knowledge base, or generated on de-\nmand with deterministic functions. This includes, but is not\nlimited to the following: The IR of the system, documenta-\ntion about safety practices, a function to calculate fault prop-\nagation using the fault gates retrieved from the system IR, a\nfunction to calculate the critical path using Dijkstra’s algo-\nrithm (Dijkstra 1959) for the shortest path and start and end\nnodes from the IR, a function to find the single points of fail-\nure, a function to replicate a given node or edge in the model\nIR. The usage of these tools is triggered in a predefined way\ndepending on the identified task category.\nExperiments\nWe test our model with the use case of a simplified auto-\nmated driving system modeled in OSATE, see Fig. 3, here\nvisualized with pydot (Carrera 2021). Fault gates are imple-\nmented as respective node attributes and given in the fig-\nure below the node label, for example, the ImageProcessor\nnode will turn faulty if at least two out of three (2OO 3)\ncamera input nodes are faulty. To verify the effectiveness of\nour concept-guided agent, we test our approach with sample\nquestions in the domain of system safety design. Representa-\ntive examples and results are given in Tab. 1. In all examples,\nwe find that the tasks have been accurately identified and for-\nmulated, and the correct information was retrieved with the\nappropriate tools. For example, the agent can make sugges-\ntions about modifying the graph to improve fault tolerance.\nIn our implementation, this is based on the predefined con-\ncept of redundancy and the usage of the tool which finds\nsingle points of failures to select the best replication candi-\ndate. To modify the system graph, the agent uses the graph\nreplication tool and updates the xml model accordingly. We\nfurther note that, in the response, the agent leverages general\nknowledge to relate the functionality of the relevant compo-\nnents with its impact on safety.\n102\nPrompt Response\nWhat happens if\nRadar1, Radar2\nand IMU have a\nfault?\nIn this scenario, the faulty components are IMU, Radar1, Radar2, and SignalProcessor. When these\ncomponents have a fault, it can have several consequences for the system. The IMU (Inertial Mea-\nsurement Unit) is responsible for measuring the orientation, velocity, and acceleration of the vehicle.\nIf the IMU has a fault, it can lead to inaccurate measurements, which can affect the overall navigation\nand control of the vehicle. The radars (Radar1 and Radar2) are used for detecting and tracking objects\nin the environment. If these radars have a fault, it can result in missed or incorrect detections, which\ncan impact the perception and decision-making capabilities of the system. The SignalProcessor is\nresponsible for processing the sensor data and extracting relevant information. If the SignalProces-\nsor has a fault, it can lead to incorrect or incomplete data processing, which can affect the overall\nperformance and reliability of the system. Overall, when these components have a fault, it can lead\nto degraded performance, reduced safety, and potentially compromised functionality of the system.\nShow me the crit-\nical path.\nThe critical path in the given scenario includes the following components: Camera1, Camera2, Col-\nlisionAvoidance, GPS, IMU, ImageProcessor, Lidar1, Map, PathPlanner, PointCloudProcessor, Sen-\nsorFusion, and VehicleController. The critical path represents the sequence of components that are\nessential for the system’s operation and have the longest duration in terms of time. These compo-\nnents are crucial for tasks such as perception, sensor fusion, planning, and control, and any faults\nor failures in these components can significantly impact the overall performance and safety of the\nsystem.\nExplain the criti-\ncal path, given the\nlast fault.\nThe critical path in this scenario, considering the last reported faulty system components, consists of\nthe following components: Camera1, Camera3, CollisionAvoidance, GPS, ImageProcessor, Lidar1,\nMap, PathPlanner, PointCloudProcessor, SensorFusion, VehicleController, and VelocitySensor. The\ncritical path represents the sequence of components and processes that are essential for the system’s\noperation and performance. Any delay or failure in these components can significantly impact the\noverall functionality and reliability of the system. It is important to note that the previously reported\nsystem faults, including IMU, Radar1, Radar2, and SignalProcessor, can also affect the critical path\nand further exacerbate the potential consequences to the system.\nWhat are the sin-\ngle points of fail-\nure?\nThe single point of failure components in the given scenario are PathPlanner, VehicleController,\nMap, SensorFusion, CollisionAvoidance, and GPS. These components are considered single points\nof failure because if any of them were to fail, it could result in a complete system failure or significant\ndegradation in the system’s performance. For example, if the PathPlanner fails, the system may not\nbe able to generate safe and efficient trajectories for the vehicle. Similarly, if the VehicleController\nfails, the system may lose control over the vehicle’s movements. The Map, SensorFusion, Collision-\nAvoidance, and GPS components are also critical for accurate perception, situational awareness, and\nnavigation, and their failure can have serious consequences for the overall system functionality.\nHow would you\nmake my system\nsafer?\nTo make your system safer, I would suggest performing replication on the component SensorFusion.\nSensorFusion should be replicated because it is a critical component that relies on multiple inputs\nfrom ImageProcessor, SignalProcessor, and PointCloudProcessor. If SensorFusion fails, it can lead\nto incorrect data being sent to PathPlanner and CollisionAvoidance, compromising the overall safety\nof the system. By replicating SensorFusion, the system can continue to function even if one instance\nof SensorFusion fails, ensuring that accurate and reliable data is provided to the other components.\nTable 1: Selected example prompts and responses. Refer to Fig. 3 for the system description. The example in row three refer-\nences the agent’s memory, as last fault refers to the last mentioned system fault in the chat history.\nConclusion and Outlook\nOur approach of a concept-guided LLM agent overcomes\ntwo challenges typically encountered when solving genera-\ntive text tasks on structured data: 1) Chain-of-thought pro-\ncesses derail and no longer comply with a set of required\nrules, or miss their goals, 2) Trigger conditions to retrieve\nexternal information via tools become inaccurate when the\nnumber of possible options increases. Instead, we here guide\nthe thought process with a series of micro-decisions to en-\nsure that the right tool triggers are met. We envision to ex-\ntend our setup to more concepts and larger decision cas-\ncades, such that more complex tasks can be tackled. Our\nwork provides the basis for an interactive framework where\nLLMs assist in Human-AI safety codesign.\nAcknowledgments\nThis work was funded by the Bavarian Ministry for Eco-\nnomic Affairs, Regional Development and Energy as part of\na project to support the thematic development of the Institute\nfor Cognitive Systems.\nReferences\nAviˇzienis, A.; Laprie, J. C.; Randell, B.; and Landwehr, C.\n2004. Basic concepts and taxonomy of dependable and se-\n103\ncure computing. IEEE Transactions on Dependable and Se-\ncure Computing, 1(1): 11–33.\nBrown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; Agarwal, S.; Herbert-V oss, A.; Krueger, G.; Henighan,\nT.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.; Winter,\nC.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.;\nChess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford,\nA.; Sutskever, I.; and Amodei, D. 2020. Language models\nare few-shot learners. Advances in Neural Information Pro-\ncessing Systems, 2020-Decem.\nCarnegie Mellon University. 2023. OSATE 2.13.\nhttps://osate.org/. Accessed: 2023-12-01.\nCarrera, E. 2021. pydot. https://pypi.org/project/pydot/. Ac-\ncessed: 2023-12-01.\nDiemert, S.; and Weber, J. H. 2023. Can Large Language\nModels Assist in Hazard Analysis? InLecture Notes in Com-\nputer Science (including subseries Lecture Notes in Artifi-\ncial Intelligence and Lecture Notes in Bioinformatics), vol-\nume 14182 LNCS, 410–422. ISBN 9783031409523.\nDijkstra, E. W. 1959. A note on two problems in connexion\nwith graphs. Numer. Math., 271: 269–271.\nDomis, D.; and Trapp, M. 2008. Integrating Safety Analy-\nses and Component-Based Design. In Harrison, M. D.; and\nSujan, M.-A., eds., Computer Safety, Reliability, and Secu-\nrity, 58–71. Berlin, Heidelberg: Springer Berlin Heidelberg.\nISBN 978-3-540-87698-4.\nHarrison Chase. 2022. LangChain.\nhttps://github.com/langchain-ai/langchain. Accessed:\n2023-12-01.\nJiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.;\nChaplot, D. S.; de las Casas, D.; Bressand, F.; Lengyel, G.;\nLample, G.; Saulnier, L.; Lavaud, L. R.; Lachaux, M.-A.;\nStock, P.; Scao, T. L.; Lavril, T.; Wang, T.; Lacroix, T.; and\nSayed, W. E. 2023. Mistral 7B. arXiv:2310.06825.\nJin, B.; Liu, G.; Han, C.; Jiang, M.; Ji, H.; and Han, J. 2023.\nLarge Language Models on Graphs: A Comprehensive Sur-\nvey. arXiv:2312.02783.\nOpenAI. 2023. ChatGPT 3.5-turbo. https://openai.com/.\nAccessed: 2023-12-01.\nPan, S.; Luo, L.; Wang, Y .; Chen, C.; Wang, J.; and Wu,\nX. 2023. Unifying Large Language Models and Knowledge\nGraphs: A Roadmap. arXiv:2306.08302.\nReuters. 2023. ChatGPT sets record for\nfastest-growing user base - analyst note.\nhttps://www.reuters.com/technology/chatgpt-sets-record-\nfastest-growing-user-base-analyst-note-2023-02-01/.\nAccessed: 2023-12-01.\nTang, J.; Yang, Y .; Wei, W.; Shi, L.; Su, L.; Cheng, S.; Yin,\nD.; and Huang, C. 2023. GraphGPT: Graph Instruction Tun-\ning for Large Language Models. arXiv:2310.13023.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\nTrapp, M. 2016. Assuring Functional Safety\nin Open Systems of Systems. https://nbn-\nresolving.de/urn:nbn:de:hbz:386-kluedo-44221.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need.Advances in Neural Information Pro-\ncessing Systems, 2017-Decem: 5999–6009.\nWang, H.; Feng, S.; He, T.; Tan, Z.; Han, X.; and Tsvetkov,\nY . 2023. Can Language Models Solve Graph Problems in\nNatural Language? arXiv:2305.10037.\n104",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7229600548744202
    },
    {
      "name": "Generative grammar",
      "score": 0.5241494178771973
    },
    {
      "name": "Software engineering",
      "score": 0.4941411316394806
    },
    {
      "name": "Graph",
      "score": 0.46280401945114136
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4566594064235687
    },
    {
      "name": "Heuristic",
      "score": 0.4154573678970337
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37483182549476624
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.35902827978134155
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3316740393638611
    },
    {
      "name": "Theoretical computer science",
      "score": 0.13713034987449646
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ]
}