{
  "title": "Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers",
  "url": "https://openalex.org/W3199241049",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222427123",
      "name": "Tay, Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2273124145",
      "name": "Dehghani, Mostafa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227174931",
      "name": "Rao, Jinfeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4225466497",
      "name": "Fedus, William",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286755007",
      "name": "Abnar, Samira",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747279576",
      "name": "Chung, Hyung Won",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224379199",
      "name": "Narang, Sharan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221452034",
      "name": "Yogatama, Dani",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4286939780",
      "name": "Vaswani, Ashish",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3177605537",
      "name": "Metzler, Donald",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3034340181",
    "https://openalex.org/W3162090017",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W3190965961",
    "https://openalex.org/W2950455323",
    "https://openalex.org/W2983040767",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3035435378",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2963351145",
    "https://openalex.org/W3016697633",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3133450183",
    "https://openalex.org/W3133264589",
    "https://openalex.org/W2784823820",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W3128590981",
    "https://openalex.org/W3156891177",
    "https://openalex.org/W3136149525",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963204221",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3095645723",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3162276117",
    "https://openalex.org/W3134582802",
    "https://openalex.org/W3170111948",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2540646130",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W3138994021",
    "https://openalex.org/W3174418826",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2920807444",
    "https://openalex.org/W3120074043",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W2963748441"
  ],
  "abstract": "There remain many open questions pertaining to the scaling behaviour of Transformer architectures. These scaling decisions and findings can be critical, as training runs often come with an associated computational cost which have both financial and/or environmental impact. The goal of this paper is to present scaling insights from pretraining and finetuning Transformers. While Kaplan et al. presents a comprehensive study of the scaling behaviour of Transformer language models, the scope is only on the upstream (pretraining) loss. Therefore, it is still unclear if these set of findings transfer to downstream task within the context of the pretrain-finetune paradigm. The key findings of this paper are as follows: (1) we show that aside from only the model size, model shape matters for downstream fine-tuning, (2) scaling protocols operate differently at different compute regions, (3) widely adopted T5-base and T5-large sizes are Pareto-inefficient. To this end, we present improved scaling protocols whereby our redesigned models achieve similar downstream fine-tuning quality while having 50\\% fewer parameters and training 40\\% faster compared to the widely adopted T5-base model. We publicly release over 100 pretrained checkpoints of different T5 configurations to facilitate future research and analysis.",
  "full_text": "Preprint\nSCALE EFFICIENTLY : I NSIGHTS FROM PRE-TRAINING\nAND FINE -TUNING TRANSFORMERS\nYi Tay∗, Mostafa Dehghani∗, Jinfeng Rao, William Fedus, Samira Abnar, Hyung Won Chung,\nSharan Narang, Dani Yogatama†, Ashish Vaswani, Donald Metzler\nGoogle Research & DeepMind†\n{yitay,dehghani}@google.com\nABSTRACT\nThere remain many open questions pertaining to the scaling behaviour of Trans-\nformer architectures. These scaling decisions and ﬁndings can be critical, as\ntraining runs often come with an associated computational cost which have both\nﬁnancial and/or environmental impact. The goal of this paper is to present scaling\ninsights from pretraining and ﬁnetuning Transformers. While Kaplan et al. (2020)\npresents a comprehensive study of the scaling behaviour of Transformer language\nmodels, the scope is only on the upstream (pretraining) loss. Therefore, it is still\nunclear if these set of ﬁndings transfer to downstream task within the context of\nthe pretrain-ﬁnetune paradigm. The key ﬁndings of this paper are as follows: (1)\nwe show that aside from only the model size, model shape matters for downstream\nﬁne-tuning, (2) scaling protocols operate differently at different compute regions,\n(3) widely adopted T5-base and T5-large sizes are Pareto-inefﬁcient. To this end,\nwe present improved scaling protocols whereby our redesigned models achieve\nsimilar downstream ﬁne-tuning quality while having 50% fewer parameters and\ntraining 40% faster compared to the widely adopted T5-base model. We publicly\nrelease over 100 pretrained checkpoints of different T5 conﬁgurations to facilitate\nfuture research and analysis.\n1 I NTRODUCTION\nTraining Transformers incurs both ﬁnancial and environmental costs (Schwartz et al., 2019; Patterson\net al., 2021). To this end, researchers and practitioners often have to work around ﬁxed compute\nbudgets and ﬁgure out the best ways to train their models. In lieu of the rising computation demand\nfor training state-of-the-art Transformer (Vaswani et al., 2017; Devlin et al., 2018; Raffel et al., 2019;\nBrown et al., 2020; Fedus et al., 2021) models, the goal of this paper is to present insights and\nlessons from scaling Transformers and making them efﬁcient and effective for transfer learning on\ndownstream tasks.\nDespite the insights offered in scaling laws research (Kaplan et al., 2020; Hernandez et al., 2021)\nthere remain unresolved questions: Should one follow ﬁxed scaling ratios? If not, should one scale by\ndepth? Or by width? Will scaling experiments on upstream pre-training generalize for downstream\ntransfer? Do scaling protocols for small models generalize to larger models? Are scaling behaviours\nsimilar in all compute regions? We hope the insights presented in this paper can be useful to both\npractitioners and researchers in informing their scaling decisions.\nNeural scaling laws (Kaplan et al., 2020) is a common resource that many look to for advice on\nscaling Transformer architectures. However, this paper limited its scope to an exhaustive study of\nupstream cross entropy on language modeling tasks. It is furthermore unclear if ﬁndings from (Kaplan\net al., 2020) will transfer to downstream applications. Speciﬁcally, Kaplan et al. (2020) proposed that\nthe performance of a Transformer language model strongly depends on model size and only weakly\non its shape. They also argue that many model conﬁgurations with the same number of parameters\nperform similarly regardless of architectural details. Our work empirically conﬁrms this on upstream\ntraining but ﬁnds a distinct discrepancy when considering practical downstream performance – a key\ninsight that we believe is highly important.\n∗Equal contribution\n1\narXiv:2109.10686v2  [cs.CL]  30 Jan 2022\nPreprint\nTo this end, we conduct extensive experiments involving pre-training and ﬁne-tuning over 200\ntransformer conﬁgurations ranging from 5M to 30B parameters. To the best of our knowledge, this is\nthe largest empirical study of practical scaling of transformer to date that considers both upstream\nand practical downstream transfer. While there have been many proposed scaling protocols for\nConvNets (Tan and Le, 2019; Bello et al., 2021), there is still limited advice on scaling of transformer\narchitectures, apart from (Kaplan et al., 2020; Li et al., 2020). Hence, the key goal of this paper is to\ndistill our experiences and insights with scaling Transformer architectures and share them with the\nbroader community.\nContributions The overall ﬁndings and insights of the paper can be summarized as follows:\n• We ﬁnd that scaling laws may differ in upstream and downstream setups. Speciﬁcally,\ncontrary to Kaplan et al. (2020), we ﬁnd that downstream performance strongly depends\non shape and not only on model size. Hence, pretraining performance may not necessarily\ntransfer to downstream applications. (Figure 1).\n• Our ﬁndings show that pre-training perplexity can often be a deceiving indicator of down-\nstream quality and therefore model building based on upstream perplexity can be challenging.\nScaling laws can differ substantially when considering metrics on actual downstream ﬁne-\ntuning. (Figure 1)\n• Given that empirical scaling laws differ when considering quality on the downstream, our\nwork investigates the pareto-frontier of transformer conﬁgurations in this setup. We ﬁnd\nthat the canonical model conﬁgurations such as T5-Base and T5-Large sizes (Raffel et al.,\n2019) are relatively inefﬁcient (Figure 2). Note that these sizes are based off the canonical\nBERT (Devlin et al., 2018) base and large sizes.\n• We ﬁnd that scaling strategies differ at different compute regions, i.e., applying same\nstrategies at different compute regions (small vs large) has a different effect on model quality.\nThis has practical implications since ﬁnding strategies at small scale might not necessarily\ntransfer or generalize to higher compute regions (section 4.2).\n• After extensive empirical exploration of the pareto-frontier of transformer models, we\npropose a simple but effective scaling strategy which we call the DeepNarrow strategy.\nWe show that we are able to obtain model quality on par or better than canonical model\nsizes (e.g., base) with 50% less parameters and being 40% faster. While we highlight the\nlimitations of this strategy, we also show that this DeepNarrow strategy is applicable to all\nmodel sizes. (Table 4).\n• To consider how generalized these scaling strategies are, we conduct additional experiments\non Vision Transformers (ViT; Dosovitskiy et al., 2020) to verify them in the vision domain.\nMoreover, on top of the 17 GLUE (Wang et al., 2018) / SuperGLUE (Wang et al., 2019) and\nSQuAD (Rajpurkar et al., 2016) tasks we employed in our extensive study, we verify our\nﬁndings via additional downstream experiments across 12 diverse language tasks (section\n4.6).\n• We release (1) the pre-trained checkpoints for our T5 models with improved scaling\nprotocols and (2) all 100+ model checkpoints, including intermediate training check-\npoints to the research community. We believe that this is a treasure trove of data to\nstudy the behaviour of large LM pretraining and ﬁnetuning especially pertaining to scal-\ning laws. The checkpoints and code will be released at https://github.com/\ngoogle-research/google-research/scaling-transformers . The check-\npoints are now publicly available at our Google Cloud Bucket gs://scenic-bucket/\nscaling_explorer/scaling_explorer.\n2 R ELATED WORK\nTransformers (Vaswani et al., 2017) have become ubiquitous in the modern deep learning stack and\nhave seen widespread impact across not only language (Devlin et al., 2018; Raffel et al., 2019; Brown\net al., 2020) but also computer vision (Dosovitskiy et al., 2020; Arnab et al., 2021), reinforcement\nlearning (Parisotto et al., 2020) and computational biology (Senior et al., 2020). To this end,\n2\nPreprint\nTable 1: Table of model conﬁgurations. NL is the number of\nlayers, dff is the size of the MLP, dmodel is the hidden size\nof the model. dkv is the size of each key-value vector. NH is\nthe number of heads. P is the default model parallelism.\nModel NL dff dmodel dkv NH #Params\nTiny 4/4 1024 256 32 4 16M\nMini 4/4 1536 384 32 8 31M\nSmall 6/6 2048 512 32 8 60M\nBase 12/12 3072 768 64 12 220M\nLarge 24/24 4096 1024 64 16 738M\nXL 24/24 16384 1024 128 32 3B\nXXL 24/24 65536 1024 128 128 11B\nXXXL 28/28 131072 1280 128 256 30B\nTable 2: Description of different\nknobs used in the paper to deﬁne\nscaling operations.\nScaling Op Description\nNL Num. layers\nEL Num enc. layers\nDL Num. dec. layers\nDM dmodel\nKV dKV\nNH Num. of heads\nFF dff\nSH Shared heads\nSKV Tied key-values\ndiscovering empirical scaling laws of these models is a research area that has garnered considerable\ninterest (Kaplan et al., 2020; Henighan et al., 2020; Hernandez et al., 2021; Bahri et al., 2021).\nDiscovering empirical scaling laws that govern neural language model scaling has been a recent\nsubject of keen interest (Kaplan et al., 2020; Hernandez et al., 2021; Bahri et al., 2021). Many of\nthese works present scaling laws across a variety of axis such as model size, compute and/or dataset\nsize. It is worth to note that many of these works evaluate on autoregressive language modeling and\nuse cross entropy loss to measure performance (Kaplan et al., 2020; Hernandez et al., 2021). There\nare a multitude of interesting ﬁndings presented (Kaplan et al., 2020) whereby the authors show that\nperformance (loss) scales as a power-law with model size and dataset size. However, one notable\nclaim is that architectural details (e.g., network depth and width) have minimal effects. Subsequently,\nHernandez et al. (2021) builds upon the work of Kaplan et al. (2020), evaluating scaling laws for\n‘transfer’. To this end, the authors study the effect of dataset scaling on unsupervised transfer learning\nand ﬁnetuning. That said, the experiments of Hernandez et al. (2021) are mainly targeted at dataset\ntransfer between two different distributions (language and code) and make the same assumptions as\nKaplan et al. (2020) about model scaling. In a similar vein, Henighan et al. (2020) studied empirical\nscaling laws for different domains for generative modeling in vision, video and multimodal setups.\nThere have been increasing demand for training and scaling Transformers (Shoeybi et al., 2019; Raffel\net al., 2019; Fedus et al., 2021; Conneau et al., 2019; Lin et al., 2021). Despite the beneﬁts from\nimproved performance, there are ﬁnancial considerations and environmental costs (Schwartz et al.,\n2019; Patterson et al., 2021) to training these models. Given that every moment spent on hardware\naccelerators is a cost incurring activity, we believe that research in distilling practical scaling insights\nand recommendations to be highly crucial (Li et al., 2020; Kaplan et al., 2020; Bello et al., 2021).\nNotably, the research problem of making transformers efﬁcient have also been tackled from an exten-\nsive number of angles such as (but not limited to) distillation (Hinton et al., 2015), compression (Zafrir\net al., 2019), parameter sharing (Lan et al., 2019; Tay et al., 2019; Zhang et al., 2021), efﬁcient atten-\ntion (Tay et al., 2020c; Kitaev et al., 2020; Choromanski et al., 2020; Tay et al., 2020b; Ainslie et al.,\n2020; Jaegle et al., 2021), architecture search (So et al., 2019), alternative non Transformer-based\narchitectures (Tolstikhin et al., 2021; Tay et al., 2021a; 2020a; Lee-Thorp et al., 2021). With so much\nextensive research into novel techniques to improving the efﬁciency of transformers, it is surprising\nthat the standard conﬁgurations (e.g., base, large) of transformers in BERT (Devlin et al., 2018) or\nT5 (Raffel et al., 2019) have not been rethought.\n3 M ETHODS\nThis section describes our main experimental setup.\nArchitecture We study a Transformer encoder-decoder architecture that uses relative attention\nbased of the T5 architecture (Raffel et al., 2019). The choice of adopting Seq2Seq architec-\ntures (Sutskever et al., 2014) is mainly due to their universality and ability to both subsume encoder\n3\nPreprint\n(Bert-like) and decoder (language) models within an identical framework. Moreover, the universality\nof Seq2Seq architectures also allow us to ﬁne-tune across a broad range of tasks. Our implementation\nand experiments are performed in Mesh Tensorﬂow 1 (Shazeer et al., 2018) using the default T5\nlibrary2.\nModel Conﬁgurations We ﬁrst deﬁne eight Transformer sizes, i.e., tiny, mini, small, base, large,\nXL, XXL and XXXL. The small, base, large, XL and XXL corresponds to the canonical T5 sizes that\nare released in Raffel et al. (2019). We use three other sizes as starting points, e.g., tiny and mini\nsince there is a lack of representation of transformers at lower compute regions.\nPretraining We pretrain on the Colossal Cleaned Common Crawl Corpus (C4; Raffel et al., 2019).\nWe pre-train encoder-decoder models using the span-based masked language modeling (MLM)\nobjective (Fedus et al., 2018; Devlin et al., 2018). We pretrain all our models for 219 steps using\n16 TPU-v3 chips. For larger models, we run our models with 64 TPU-v3 chips. We use 219 steps\nsince majority of the experiments in (Raffel et al., 2019) were conducted in the same fashion. We\nwould also like to emphasize that the ofﬁcial released T5 checkpoints were pretrained on1T tokens (1\nmillion steps with a batch size of 2048). Given that this extended long pretraining setup is infeasible\ngiven the number of experiments we would have to run, we opt to follow the standard ablation setup\nin (Raffel et al., 2019) which pretrains on more manageable number of tokens.\nDownstream Tasks We consider a myriad of downstream tasks. In total, we consider 17\ntasks. We ﬁnetune on a mixture of GLUE (Wang et al., 2018), SuperGLUE (Wang et al., 2019),\nSQuAD (Rajpurkar et al., 2016) for the key downstream experiment results and report aggregate\nGLUE/SuperGLUE scores. We believe that an aggregate of 17 tasks in natural language understand-\ning that conmprises of both high-resource and low-resource tasks gives us a good overview of a\nmodel’s downstream performance. Finetuning is typically performed with16 TPU-v3 chips.\nNotation for Scaling Operators For the remainder of the paper, we use a shortform code for each\nscaling operator applied on a standard transformer size. For example, NL32-SM refers to scaling\nsmall (SM) transformers to32 layers (NL32). We use EL,DL to represent scaling encoder and decoder\nlayers independently, KV to represent scaling each key-value size, DM to represent scalingdmodel.\nNH to represent modifying the number of heads and FF to represent scalingdFF . The initial/standard\nmodel sizes are tiny, mini, small, base, large, XL, XXL and XXXL. This is described in Table 2.\nConvention With the exception of Figure 1, all charts are plotted with FLOPS as the main compute\nmetric. We use number of params for Figure 1 to align with Kaplan et al. (2020). All of the\ndownstream results are plot with SuperGLUE accuracy (Wang et al., 2019) as the Y-axis. Due to the\nlack of space, we report charts/plots of other metrics (params of speed) and other tasks (GLUE or\nSQuAD) in the supplementary material. All parameter counts also include the embedding parameters.\nWe re-emphasize that it is critical to take into account multiple facets of efﬁciency and therefore\nreport all three key metrics (FLOPs, throughput/speed and parameter count) in the supplementary\nmaterial.\nModel and Data Parallelism By default, our models are trained across multiple workers via data\nparallelism. As per convention in the T5 paper, our larger models use the default model parallelism.\nSpeciﬁcally, this is set to 2 for large models, 8 for XL models and 32 for XXL models.\n4 A NALYSIS AND RESULTS\nThis section presents our overall ﬁndings and key results.\n4.1 M ODEL SHAPE MATTERS\nWe extend the results of Kaplan et al. (2020) to ﬁne-tuning and present model shape dependence\nnot highlighted in Hernandez et al. (2021). Kaplan et al. (2020) studies pre-training (upstream)\n1https://github.com/tensorflow/mesh\n2https://github.com/google-research/text-to-text-transfer-transformer\n4\nPreprint\n2.7e+8 5.4e+8 1.1e+9 2.1e+9 4.3e+9 8.6e+9 1.7e+10\nParams\n-1.70\n-1.65\n-1.60\n-1.55\n-1.50\n-1.45\n-1.40\n-1.35Negative Log-Perplexity\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n(a) Pre-training scaling\n2.7e+8 5.4e+8 1.1e+9 2.1e+9 4.3e+9 8.6e+9 1.7e+10\nParams\n72\n73\n74\n75\n76\n77\n78\n79\n80SuperGlue Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL (b) Fine-tuning scaling\nFigure 1: The predictability and unpredictability of pre-training versus ﬁne-tuning. While the\nupstream pre-training performance measured by negative log-perplexity scales with model size quite\nindependently from the model shape, the downstream performance (SuperGlue (avg) score) does not.\nThis indicates that the shape of models plays an important role on how it performs on the target task\nand the performance is not merely a function of parameter size.\nTable 3: Upstream performance does not guarantee downstream performance. Example points\nfrom Figure 1. A model with improved upstream quality (as measured by validation perplexity) can\ndo signiﬁcantly worse on transfer if the shape setting is not right. Hence, pre-training perplexity can\nbe misleading.\nName NL dff dmodel dkv NH #Params PPL Downstream\nNL12-XXL 12 65536 1024 128 128 3.6B -1.46 85.1/76.5/88.1\nNL32-XL 32 16384 1024 128 32 3.8B -1.49 86.9/79.9/89.5\nand concludes that performance depends only weakly on model shape, but strongly on model size.\nHernandez et al. (2021) extends this work to measure an effective data transfer measure when pre-\ntraining and then ﬁne-tuning on a Python dataset. However, this work does not consider details of\nmodel shape, and instead focused on the relative predictability with model scale alone. Our work\nstands in contrasts since we ﬁnd that model shape matters considerably for downstream ﬁne-tuned\nperformance.\nFigure 1 shows compute-performance scatter plots for pre-training (left) and ﬁne-tuning (right) over a\ndozen Transformers. The models considered are sampled diversely within a two-order of magnitude\nband of model scales. We adjust the model shape primarily through depth variations, starting with\nconﬁgurations such as XXXL (33B), XXL (11B), XL (3B) and LG (750M) parameters but have their\ndepths/lengths modiﬁed. From Figure 1 reveals a strong correlation of the upstream performance with\nmodel size, corroborating the neural scaling laws of Kaplan et al. (2020). But the strong pre-training\ncorrelation largely vanishes when ﬁne-tuning these models on SuperGLUE (Wang et al., 2019). While\nwe conﬁrm the ﬁndings of Kaplan et al. (2020) that performance scales strongly dependent on model\nsize but weakly on model shape, we ﬁnd that model shape (such as depth-scaling) is highly important\nfor downstream transfer – a dimension that is not considered in Kaplan et al. (2020).\nAs a substantiating point and additional context to Figure 1, we also show via a counter-example that\npretraining perplexity is not indicative of transfer performance, i.e., we explicitly show that a case (in\nTable 3) where a model can have outstanding pre-training perplexity but substantially undeliver when\nit comes to downstream performance. To the best of our knowledge, while this has been mentioned\nimplicitly in several existing works (Narang et al., 2021), this is the ﬁrst work explicitly shows this\npoint.\n5\nPreprint\n1.1e+12 2.2e+12 4.4e+12 8.8e+12 1.8e+13\nFLOPS\n56\n58\n60\n62\n64\n66\n68\n70\n72SuperGlue Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n(a) T5-Small Model\n2.2e+12 4.4e+12 8.8e+12 1.8e+13 3.5e+13\nFLOPS\n54\n56\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76SuperGlue Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6 (b) T5-Base Model\n4.4e+12 8.8e+12 1.8e+13 3.5e+13 7.0e+13\nFLOPS\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80SuperGlue Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG (c) T5-Large Model\nFigure 2: Downstream scaling properties is scale-dependent. The downstream performance on\nSuperGLUE has qualitatively different scaling properties across models sizes. From left to right, we\nﬁne-tune model conﬁgurations closely matched to T5-Small, T5-Base and T5-Large.\nZooming in Versus Zooming Out Here, one may argue that a general trend (even on downstream)\nmay still exist if we zoom out and cover a very wide range of model sizes (e.g., very tiny to very\nlarge). A tiny model is not likely to outperform a very large model no matter how well-conﬁgured it\nmight be. Our purpose here is not to contradict this general trend but to distinguish between both\narguments. We argue that, in practical setups, comparisons between models and scaling decisions\nare often made when zooming-in and our pairwise comparisons above are not on largely different\nmodels, rather those that are on the same neighborhood in the size (close in the x-axis). Thus, what\nwe claim is that when you zoom in, which is what happen in practice, it is not uncommon to see\ncases similar to the models in Table 3 where taking the upstream perplexity into account may lead to\na sub-optimal choice. It is also worth to mention that zoom-ing in on upstream returns very different\ntrends compared to zoom-ing in on downstream results.\n4.2 S CALING BEHAVIOUR AT DIFFERENT COMPUTE REGIONS IS DIFFERENT\nIn this section, we evaluate how each scaling hyperparameter and model shape inﬂuences a model’s\nposition on the compute-performance chart. Figure 2 shows three plots which varying different\nscaling knobs. Given three starting points, small, base and large, we scale the starting points across\ndifferent knobs. It is clear from Figure 2 that the effect of applying different scaling operators is\nvery different across different compute regions. We also observe that the Pareto-boundary is also\nvery different at different compute regions. The implications of this ﬁnding is nontrivial, since\nthis effectively means that ﬁnding improvements at a (plausibly) smaller scale in hopes that it will\ngeneralize at large scale might not be an effective strategy. This corroborates recent ﬁndings of Bello\net al. (2021) which studied scaling properties of ImageNet models. Their paper demonstrates that the\ncompound scaling rules (Tan and Le, 2019) derived in a small-scale regime, lead to Pareto-inefﬁcient\nmodels when then extrapolated to larger scales.\n4.3 N OT ALL SCALING STRATEGIES AND MODEL SHAPES ARE CREATED EQUAL\nFrom Figure 2, we can also observe that different scaling strategies results in very different outcome.\nA common pattern amongst all three model sizes is that the NL operator has strong inﬂuence on the\nPareto-frontier. On the other hand, settings such as KV (varyingdkv) seem to result in models that are\nless Pareto-efﬁcient. We notice mixed signals from varying dmodel. In this case, scaling down results\nin a model on the pareto-frontier, but scaling up, i.e., DM2K results in a highly pareto-inefﬁcient\nmodel. When compared to scaling up, NL is a substantially superior option to dmodel. We describe\nscaling recommendations in subsequent sections. Overall, it does seem like dkv and NH does not\ninﬂuence the Pareto frontier as much as other knobs.\nEffect of scaling different knobs Figure 3 illustrates the effect of scaling different knobs on the\ncompute-performance boundary. It becomes clear that not all strategies inﬂuence the boundary\nwith the same impact. For example, NL has the biggest impact while NH does not inﬂuence the\n6\nPreprint\n-2.00\n-1.95\n-1.90\n-1.85\n-1.80\n-1.75\n-1.70\n-1.65\n-1.60\n-1.55Negative Log-Perplexity\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nBase Base Base Base\nDM\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nBase Base Base Base Base\nFF\nNH\n8\nNH\n1\n6\nNH\n2\n4\nNH\n3\n2\nBase Base Base Base\nNH\nNL\n4\nNL\n8\nNL\n1\n2\nNL\n1\n6\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n4\n0\nNL\n4\n8\nBase Base Base Base BaseBase Base Base Base\nNL\n0.0e+0\n2.0e+8\n4.0e+8\n6.0e+8\n8.0e+8\nParams\n(a) Upstream\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78SuperGlue Accuracy\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\n0.0e+0 1.0e+13 2.0e+13 3.0e+13 4.0e+13\nFLOPS\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nBase Base Base Base\nDM\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nBase Base Base Base Base\nFF\nNH\n8\nNH\n1\n6\nNH\n2\n4\nNH\n3\n2\nBase Base Base Base\nNH\nNL\n4\nNL\n8\nNL\n1\n2\nNL\n1\n6\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n4\n0\nNL\n4\n8\nBase Base Base Base BaseBase Base Base Base\nNL\n0.0e+0\n2.0e+8\n4.0e+8\n6.0e+8\n8.0e+8\nParams\n(b) Downstream\nFigure 3: Different scaling with respects to different knobs, in upstream and downstream. On the\nplots, DM refers to scaling model dimension, FF refers to scaling FFN hidden size, NH is number of\nheads, and NL is number of layers. Size of each circle indicates the model size in terms of number of\ntrainable parameter parameters.\nmodel’s position on the graph much. Finally, we also note that the effect of scaling on upstream and\ndownstream is quite different. For example, FF2K is clearly better option than the canonical base\nmodel in downstream but not upstream.\nScaling Depth vs Width In Figure 3, we also note that the NL scaling operator (depth) has\ngenerally impact on the Pareto-boundary as compared to width (FF). For instance, we can see that\nFF12K (in Figure 3 is clearly not Pareto-optimal, being outclassed by conﬁgurations such as NL16-\nSM, EL12-SM. Likewise in the base setup, FF9K and F12K are less Pareto-efﬁcient as compared to\nNL16 and NL24.\n4.4 S CALING RECOMMENDATIONS\nWe generally recommend aDeepNarrow strategy where the model’s depth is preferentially increased3\nbefore considering any other forms of uniform scaling across other dimensions. This is largely due to\nhow much depth inﬂuences the Pareto-frontier as shown in earlier sections of the paper. Speciﬁcally, a\ntall small (deep and narrow) model is generally more efﬁcient compared to the base model. Likewise,\na tall base model might also generally more efﬁcient compared to a large model. We generally ﬁnd\nthat, regardless of size, even if absolute performance might increase as we continue to stack layers,\nthe relative gain of Pareto-efﬁciency diminishes as we increase the layers, converging at 32 to 36\nlayers. Finally, we note that our notion of efﬁciency here relates to any one compute dimension, i.e.,\nparams, FLOPs or throughput (speed). We report all three key4 efﬁciency metrics (number of params,\nFLOPS and speed) and leave this decision to the practitioner to decide which compute dimension to\nconsider.\n3Our concurrent work Charformer (Tay et al., 2021b) makes use of a DeepNarrow inspired strategy which is\nreferred to as Tall in the paper.\n4It is often assumed that number of parameters, speed and FLOPs tend to correlate. We ﬁnd that this is not\nalways the case especially when dealing with modeling choices that inﬂuences parallelism (depth vs width).\nTherefore, we emphasize the importance of reporting all key efﬁciency metrics.\n7\nPreprint\nTable 4: Efﬁcient DeepNarrow alternatives to the canonical T5 model sizes using the DeepNarrow\nstrategy. Models are all Pareto efﬁcient at least to one or more aspect of compute and one or more\ndownstream task. XXL and XL32L models are trained on 64 TPU-V3 chips and so they are faster.\nModel #Params #TFlops Steps/s Ppl (C4) GLUE SGLUE SQuAD A VG\nSmall 61M 3.7 23 -2.021 77.45 62.88 80.39 73.57\nMini-8L 50M 3.2 24 -2.056 77.11 63.35 80.12 73.52\nBase 223M 11 9 -1.752 82.53 69.80 85.14 79.16\nSmall 16L 134M 7.2 13 -1.825 82.57 69.51 84.12 78.73\nSmall 20L 164M 8.6 11 -1.798 83.22 69.44 85.23 79.30\nSmall 22L 179M 9.3 10 -1.798 82.52 70.68 85.39 79.54\nSmall 24L 193M 10 9 -1.783 83.11 71.11 85.45 79.92\nSmall 32EL 143M 10 10 -1.897 82.77 70.66 86.01 79.81\nLarge 738M 34 4 -1.605 85.08 75.97 87.55 82.87\nBase 36L 621M 29 3 -1.626 85.26 75.57 87.84 82.89\nXL 2.9B 64 1 -1.487 86.49 77.99 88.70 84.38\nLarge 36L 1.1B 50 2 -1.564 87.22 79.34 89.21 85.27\nXXL 11.3B 367 1 -1.430 86.91 79.20 89.50 85.20\nXL 32L 3.8B 169 3 -1.500 86.94 79.87 89.46 85.42\nEfﬁcient Alternatives to T5-Base/Large/XL/XXL Table 4 describes this phenomena in which we\nlist efﬁcient alternatives to the canonical model sizes using the DeepNarrow strategy. Note that this\nlist is not exhaustive. Firstly, we ﬁnd that signiﬁcantly increasing the depth of the small model does\nsubstantially better in terms of the compute-performance trade-off and may result in pareto-efﬁcient\nmodels. The Small 16L model achieves comparable performance to Base while being 40% faster,\ncost 50% less parameters and has only about 63.1% of total FLOPs. Alternatively, the Small 24L\nmodel has 87% of FLOPs of the base model, similar speed (steps/s), and only 16% parameter savings\nand yet outperforms Base on all three downstream tasks. Meanwhile the canonical large model can\nbe outperformed by a base model of 36 layers with 16% parameter saving and lower ﬂops cost. The\nLarge 36L model is only 37% of the parameter cost of the XL model and yet outperforms the XL\nmodel on all three downstream tasks. Finally, the XL 32L model is only a third the size of the XXL\nmodel, approximately consume 44% the number of FLOPs of the XXL model and is about 3 times\nfaster on the same hardware.\nThe Limits of Depth vs Width We note an obvious limitation with our advice. Scaling depth has\nan obvious limiter, i.e., they are non-parallelizable across different machines or devices and every\ncomputation has to always wait for the previous layer. This is unlike width, which can be easily\nparallelizable over thousands or hundreds of thousands of devices. Within the limitation of scaling\nto 64 workers with a model parallelism of 32, we still ﬁnd that scaling depth can still improve the\nPareto-efﬁciency of models. From our experiments, from Table 4, we see that the efﬁcient small\nDeepNarrow models (e.g., Small 16L etc) are still much faster than the base models. Things get\ntricky as we approach larger models where model parallelism makes it difﬁcult to compare the utility\nbetween wide and deep models. To this end, we believe the proposed scaling protocol holds within a\ncertain hardware limit. Scaling to extreme number of machines via model parallelism (of width) is\nout of scope of this paper. Another potential drawback to depth-scaling is that this may inﬂuence the\nstability of training these models (due to vanishing gradients). However, we did not observe this in\nour experiments with T5 models.\nRelationship of Model Depth with Pareto-frontier Figure 2 shows the performance of scaling\nsmall, base and large models by depth. It is clear that the small model (green) dominates the Pareto-\nfrontier initially but slowly tapers off at a certain point. Here, we note that the depth-scaled small\nmodel is more Pareto-efﬁcient compared to the base model. After the Pareto-efﬁciency of the small\nmodel tapers off, the base model (red line) becomes Pareto-efﬁcient. Similarly, this tapers off and the\nlarge model becomes Pareto-efﬁcient.\n8\nPreprint\n2.2e+12 4.4e+12 8.8e+12 1.8e+13 3.5e+13 7.0e+13\nFLOPS\n-2.4\n-2.3\n-2.2\n-2.1\n-2.0\n-1.9\n-1.8\n-1.7\n-1.6\n-1.5Negative Log-Perplexity\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n2\nNL\n1\n6\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n4\n0\nNL\n4\n8\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n6\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n0\n-\nS\nM\nNL\n2\n2\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nNL\n3\n2\n-\nS\nM\nNL\n3\n6\n-\nS\nM\nNL\n4\n0\n-\nS\nM\nNL\n4\n8\n-\nS\nM\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n0\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\n(a) Negative Log-Perplexity\n2.2e+12 4.4e+12 8.8e+12 1.8e+13 3.5e+13 7.0e+13\nFLOPS\n54\n56\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80SuperGlue Accuracy\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n2\nNL\n1\n6\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n4\n0\nNL\n4\n8\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n6\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n0\n-\nS\nM\nNL\n2\n2\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nNL\n3\n2\n-\nS\nM\nNL\n3\n6\n-\nS\nM\nNL\n4\n0\n-\nS\nM\nNL\n4\n8\n-\nS\nM\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nBa\ns\ne\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nSm\na\nl\nl\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne\nLa\nr\ng\ne (b) SuperGlue Score (Avg)\nFigure 4: Compute-Performance trade-off when scaling model depth of different starting points\n(Small, Base, and Large).\n4.5 T RANSFERABILITY OF RESULTS TO VISION TRANSFORMERS (VIT)\nFollowing our language experiments, and as per the advice of (Narang et al., 2021), in order to\nexamine our DeepNarrow scaling strategyin another domain and check if the observations extends\nto the cases where Transformers are applied on other modalities, we pre-retrained several different\nconﬁgurations of Vision Transformer (ViT; Dosovitskiy et al., 2020) and evaluated on downstream\nfew-shot image recognition task. We focused on investigating the pareto-efﬁciency of DeepNarrow\nsmall models compared to base models.\nWe follow the exact same setup as Dosovitskiy et al. (2020) and pre-train ViT on the JFT dataset (Sun\net al., 2017) with 18k classes and 303M images, for 7 epochs. We evaluate our model on ImageNet\n10-shot classiﬁcation. In our experiments, we use the patch size of 32 × 32.\nTable 5: Results on image recognition task. All models are trained with the same batch size using 64\nTPU-V3 chips.\nModel #Params GFLops Steps/s ImageNet-10Shot\nViT-S 62M 1.37 8.83 45.3\nViT-B 102M 4.44 6.74 58.9\nViT-SL=24 87M 3.94 6.11 59.7\nViT-SL=28 99M 4.58 5.36 61.6\nResults Table 5 report results on ViT experiments. When considering the number of trainable\nparameters or FLOPs, we observe that DeepNarrow scaling of the ViT-S model achieves better Pareto\nefﬁciency compared to the ViT-B model. Notably, whenL = 24, the model achieves better few-shot\naccuracy with 15% less parameters, 11% less FLOPs and achieves +1.4% percentage improvement\nin accuracy. With respect to the step per seconds (speed), given ViT-SL=24 or ViT-SL=28 add to the\nsequential operations in depth, they become a bit slower than ViT-B. However, we consider6.11s\nand 6.74s to be reasonably within the same ballpark. In short, the ViT-SL=24 is still a compelling\nalternative.\n4.6 H OW TRANSFERABLE ARE THESE RESULTS ACROSS MULTIPLE DIVERSE NLP TASKS ?\nTo verify that our scaling protocols transfer to tasks outside of the 17 tasks explored in\n(GLUE/SuperGLUE and SQuaD), we run additional experiments on a myriad of diverse NLP\n9\nPreprint\nTable 6: Rainbow dataset.\nTask Base DeepNarrow\nANLI 65.7 65.7\nCosmoQA 69.9 70.0\nHellaSwag 49.7 48.9\nPQA 73.7 74.4\nSQA 65.1 66.0\nWino 65.3 65.9\nTable 7: Generation tasks (Rouge-L).\nTask Base DeepNarrow\nXSum 32.3 33.1\nCNN/Dailymail 38.9 38.9\nMultiNews 20.2 20.5\ntasks. Verifying whether the ﬁndings generalize outside GLUE and SuperGLUE is important since\nwe do not want to fall prey to the benchmark lottery problem(Dehghani et al., 2021). As such, the\npurpose of this additional experiment is to verify if our results are universal enough for a general\nrecommendation.\nSetup In this experiment, we compare the base T5 transformer with the efﬁcient 24 layer small\nmodel using theDeepNarrow strategy, which has14% less parameters and10% less FLOPS compared\nto the T5 base model. This ﬁnetuning protocol uses a constant learning rate of 10−3 and a batch size\nof 128 for all tasks. Note that we used the same pretrained models as earlier sections that produced\nﬁnetuned results on SuperGLUE and GLUE.\nDiverse NLP tasks We conduct experiments on a total of 12 extra tasks, i.e., 6 tasks of Rain-\nbow (Lourie et al., 2021) which contains commonsense reasoning tasks, 3 generation/summarization\ntasks (XSum (Narayan et al., 2018), CNN/Dailymail (See et al., 2017) and MultiNews (Fabbri et al.,\n2019)), along with 3 text classiﬁcation tasks (civil comments (Borkan et al., 2019), wiki toxicity (Wul-\nczyn et al., 2017) and IMDb reviews (Maas et al., 2011)). For the Rainbow suite, we co-train all tasks\nin (Lourie et al., 2021) and report peak validation results.\nTable 8: Classiﬁcation tasks (Acc).\nTask Base DeepNarrow\nCivilComments 88.2 88.4\nWikiComments 95.4 95.4\nIMDb 94.4 94.4\nResults on other tasks Table 6, Table 7 and Ta-\nble 8 reports results on these 12 tasks. On all 12\nadditional diverse NLP tasks considered , we show\nthat the Pareto-efﬁcient alternative outperforms or\nties with the base T5 model on 11 out of 12 tasks\nwhere 4 of them are ties. It is good to bear in mind\nthat this is a model which is effectively smaller in\nparameters and has 10% less FLOPs.\n5 C ONCLUSION\nIn this paper, we present several important ﬁndings pertaining to training and practical usage of\nefﬁcient transformer architectures. Speciﬁcally, we discovered that scaling laws differ in upstream\nand downstream. Contrary to prior work (Kaplan et al., 2020), we emphasize the importance of model\nshape for ensuring strong downstream performance. Next, we also discovered that scaling happens\nrather differently at different compute regions and scaling a small model would behave differently\nfrom scaling a large model. We highlight that this has implications since model development in a\ncertain compute region could potentially not transfer to another compute region. We go on to show\nthat not all model knobs are created equal, i.e., some knobs (such as depth) has a larger impact on the\nPareto-frontier. Finally, we propose a simple but highly effective improved scaling protocol. With\nthis strategy, we obtain models with similar downstream ﬁnetuning quality while having 50% fewer\nparameters and/or training 40% faster.\nREFERENCES\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham,\nAnirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. Etc: Encoding long and structured\ninputs in transformers. arXiv preprint arXiv:2004.08483, 2020.\n10\nPreprint\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid.\nVivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021.\nYasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural\nscaling laws. arXiv preprint arXiv:2102.06701, 2021.\nIrwan Bello, William Fedus, Xianzhi Du, Ekin D Cubuk, Aravind Srinivas, Tsung-Yi Lin, Jonathon\nShlens, and Barret Zoph. Revisiting resnets: Improved training and scaling strategies. arXiv\npreprint arXiv:2103.07579, 2021.\nDaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. Nuanced\nmetrics for measuring unintended bias with real data for text classiﬁcation. CoRR, abs/1903.04561,\n2019. URL http://arxiv.org/abs/1903.04561.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nKrzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention\nwith performers. arXiv preprint arXiv:2009.14794, 2020.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-\ncisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised\ncross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019.\nMostafa Dehghani, Yi Tay, Alexey A Gritsenko, Zhe Zhao, Neil Houlsby, Fernando Diaz, Donald\nMetzler, and Oriol Vinyals. The benchmark lottery. 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\nAlexander R. Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir R. Radev. Multi-news: a\nlarge-scale multi-document summarization dataset and abstractive hierarchical model, 2019.\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai. Maskgan: better text generation via ﬁlling in\nthe_. arXiv preprint arXiv:1801.07736, 2018.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter\nmodels with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\nTom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo\nJun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative\nmodeling. arXiv preprint arXiv:2010.14701, 2020.\nDanny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer.\narXiv preprint arXiv:2102.01293, 2021.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David\nDing, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A\ngeneral architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795, 2021.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\n11\nPreprint\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In\nInternational Conference on Learning Representations, 2020. URL https://openreview.\nnet/forum?id=rkgNKkHtvB.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu\nSoricut. Albert: A lite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942, 2019.\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and Santiago Ontanon. Fnet: Mixing tokens with\nfourier transforms. arXiv preprint arXiv:2105.03824, 2021.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez.\nTrain big, then compress: Rethinking model size for efﬁcient training and inference of transformers.\nIn International Conference on Machine Learning, pages 5958–5968. PMLR, 2020.\nJunyang Lin, Rui Men, An Yang, Chang Zhou, Ming Ding, Yichang Zhang, Peng Wang, Ang\nWang, Le Jiang, Xianyan Jia, et al. M6: A chinese multimodal pretrainer. arXiv preprint\narXiv:2103.00823, 2021.\nNicholas Lourie, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Unicorn on rainbow:\nA universal commonsense reasoning model on a new multitask benchmark. arXiv preprint\narXiv:2103.13009, 2021.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y . Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Human Language Technologies, pages 142–150,\nPortland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:\n//www.aclweb.org/anthology/P11-1015.\nSharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Kar-\nishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modiﬁcations\ntransfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. ArXiv, abs/1808.08745,\n2018.\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayakumar,\nMax Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers\nfor reinforcement learning. In International Conference on Machine Learning, pages 7487–7498.\nPMLR, 2020.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. arXiv\npreprint arXiv:2104.10350, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text\ntransformer. arXiv preprint arXiv:1910.10683, 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions for\nmachine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren Etzioni. Green ai, 2019.\nAbigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with\npointer-generator networks. CoRR, abs/1704.04368, 2017. URL http://arxiv.org/abs/\n1704.04368.\nAnderw Senior, John Jumper, Demis Hassabis, and Pushmeet Kohli. Alphafold: Using ai for scientiﬁc\ndiscovery. DeepMind. Recuperado de: https://deepmind. com/blog/alphafold, 2020.\n12\nPreprint\nNoam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,\nPeter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-tensorﬂow: Deep\nlearning for supercomputers. In Advances in Neural Information Processing Systems, pages\n10414–10423, 2018.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.\narXiv preprint arXiv:1909.08053, 2019.\nDavid R So, Chen Liang, and Quoc V Le. The evolved transformer.arXiv preprint arXiv:1901.11117,\n2019.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable\neffectiveness of data in deep learning era. In Proceedings of the IEEE international conference on\ncomputer vision, 2017.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks.\narXiv preprint arXiv:1409.3215, 2014.\nMingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural networks.\nIn International Conference on Machine Learning, pages 6105–6114. PMLR, 2019.\nYi Tay, Aston Zhang, Luu Anh Tuan, Jinfeng Rao, Shuai Zhang, Shuohang Wang, Jie Fu, and\nSiu Cheung Hui. Lightweight and efﬁcient neural natural language processing with quaternion\nnetworks. arXiv preprint arXiv:1906.04393, 2019.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:\nRethinking self-attention in transformer models. arXiv preprint arXiv:2005.00743, 2020a.\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. arXiv\npreprint arXiv:2002.11296, 2020b.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey. arXiv\npreprint arXiv:2009.06732, 2020c.\nYi Tay, Mostafa Dehghani, Jai Gupta, Dara Bahri, Vamsi Aribandi, Zhen Qin, and Donald Metzler. Are\npre-trained convolutions better than pre-trained transformers? arXiv preprint arXiv:2105.03322,\n2021a.\nYi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin,\nSimon Baumgartner, Cong Yu, and Donald Metzler. Charformer: Fast character transformers via\ngradient-based subword tokenization. arXiv preprint arXiv:2106.12672, 2021b.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An\nall-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pages 5998–6008, 2017.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. In Proceed-\nings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP, pages 353–355, Brussels, Belgium, November 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/W18-5446. URL https://www.aclweb.org/anthology/\nW18-5446.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. arXiv preprint arXiv:1905.00537, 2019.\n13\nPreprint\nEllery Wulczyn, Nithum Thain, and Lucas Dixon. Ex machina: Personal attacks seen at scale. In\nProceedings of the 26th International Conference on World Wide Web, WWW ’17, pages 1391–\n1399, Republic and Canton of Geneva, CHE, 2017. International World Wide Web Conferences\nSteering Committee. ISBN 9781450349130. doi: 10.1145/3038912.3052591. URL https:\n//doi.org/10.1145/3038912.3052591.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8bert: Quantized 8bit bert. arXiv\npreprint arXiv:1910.06188, 2019.\nAston Zhang, Yi Tay, Shuai Zhang, Alvin Chan, Anh Tuan Luu, Siu Cheung Hui, and Jie Fu. Beyond\nfully-connected layers with quaternions: Parameterization of hypercomplex multiplications with\n1/n parameters. arXiv preprint arXiv:2102.08597, 2021.\n14\nPreprint\nAPPENDIX\n2.7e+8 5.4e+8 1.1e+9 2.1e+9 4.3e+9 8.6e+9 1.7e+10\nParams\n-1.70\n-1.65\n-1.60\n-1.55\n-1.50\n-1.45\n-1.40\n-1.35Negative Log-Perplexity\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n1.8e+13 3.5e+13 7.0e+13 1.4e+14 2.8e+14\nFLOPS\n-1.70\n-1.65\n-1.60\n-1.55\n-1.50\n-1.45\n-1.40\n-1.35Negative Log-Perplexity\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n2.0e-1 3.0e-1 4.0e-1 5.0e-1 6.0e-1 7.0e-1 9.0e-1\n1/Throughput\n-1.70\n-1.65\n-1.60\n-1.55\n-1.50\n-1.45\n-1.40\n-1.35Negative Log-Perplexity\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n(a) Upstream: Negative Log Perplexity.\n2.7e+8 5.4e+8 1.1e+9 2.1e+9 4.3e+9 8.6e+9 1.7e+10\nParams\n72\n73\n74\n75\n76\n77\n78\n79\n80SuperGlue Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n1.8e+13 3.5e+13 7.0e+13 1.4e+14 2.8e+14\nFLOPS\n72\n73\n74\n75\n76\n77\n78\n79\n80SuperGlue Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n2.0e-1 3.0e-1 4.0e-1 5.0e-1 6.0e-1 7.0e-1 9.0e-1\n1/Throughput\n72\n73\n74\n75\n76\n77\n78\n79\n80SuperGlue Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n(b) Downstream: SuperGLUE Accuracy\n2.7e+8 5.4e+8 1.1e+9 2.1e+9 4.3e+9 8.6e+9 1.7e+10\nParams\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\n87.5Glue Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n1.8e+13 3.5e+13 7.0e+13 1.4e+14 2.8e+14\nFLOPS\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\n87.5Glue Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n2.0e-1 3.0e-1 4.0e-1 5.0e-1 6.0e-1 7.0e-1 9.0e-1\n1/Throughput\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\n87.5Glue Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n(c) Downstream: GLUE Accuracy\n2.7e+8 5.4e+8 1.1e+9 2.1e+9 4.3e+9 8.6e+9 1.7e+10\nParams\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5\n89.0\n89.5\n90.0Squad Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n1.8e+13 3.5e+13 7.0e+13 1.4e+14 2.8e+14\nFLOPS\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5\n89.0\n89.5\n90.0Squad Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n2.0e-1 3.0e-1 4.0e-1 5.0e-1 6.0e-1 7.0e-1 9.0e-1\n1/Throughput\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5\n89.0\n89.5\n90.0Squad Accuracy\nLa\nr\ng\ne\nXL\nNL\n2\n4\nNL\n3\n2\nNL\n3\n6\nNL\n3\n2\n-\nL\nG\nNL\n8\n-\nX\nL\nNL\n3\n2\n-\nX\nL\nNL\n6\n-\nX\nX\nL\nNL\n8\n-\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nL\nNL\n6\n-\nX\nX\nX\nL\nNL\n1\n2\n-\nX\nX\nX\nL\n(d) Downstream: Squad Accuracy\nFigure 5: Performance on upstream and different downstream tasks with respect to number of\nparameters, FLOPs, and throughput for models presented in Figure 1b.\n15\nPreprint\n8.4e+6 1.7e+7 3.4e+7 6.7e+7 1.3e+8 2.7e+8\nParams\n-2.5\n-2.4\n-2.3\n-2.2\n-2.1\n-2.0\n-1.9\n-1.8\n-1.7Negative Log-Perplexity\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n1\n2\n8\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n1.1e+12 2.2e+12 4.4e+12 8.8e+12 1.8e+13\nFLOPS\n-2.5\n-2.4\n-2.3\n-2.2\n-2.1\n-2.0\n-1.9\n-1.8\n-1.7Negative Log-Perplexity\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n1\n2\n8\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n4.0e-2 5.0e-2 6.0e-2 7.0e-2 8.0e-29.0e-2 1.0e-1 1.2e-1\n1/Throughput\n-2.5\n-2.4\n-2.3\n-2.2\n-2.1\n-2.0\n-1.9\n-1.8\n-1.7Negative Log-Perplexity\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n1\n2\n8\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n(a) Upstream: Negative Log Perplexity.\n1.7e+7 3.4e+7 6.7e+7 1.3e+8 2.7e+8\nParams\n56\n58\n60\n62\n64\n66\n68\n70\n72SuperGlue Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n1.1e+12 2.2e+12 4.4e+12 8.8e+12 1.8e+13\nFLOPS\n56\n58\n60\n62\n64\n66\n68\n70\n72SuperGlue Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n4.0e-2 5.0e-2 6.0e-2 7.0e-2 8.0e-29.0e-2 1.0e-1 1.2e-1\n1/Throughput\n56\n58\n60\n62\n64\n66\n68\n70\n72SuperGlue Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n(b) Downstream: SuperGLUE Accuracy\n1.7e+7 3.4e+7 6.7e+7 1.3e+8 2.7e+8\nParams\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84Glue Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n1.1e+12 2.2e+12 4.4e+12 8.8e+12 1.8e+13\nFLOPS\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84Glue Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n4.0e-2 5.0e-2 6.0e-2 7.0e-2 8.0e-29.0e-2 1.0e-1 1.2e-1\n1/Throughput\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84Glue Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n(c) Downstream: GLUE Accuracy\n1.7e+7 3.4e+7 6.7e+7 1.3e+8 2.7e+8\nParams\n64\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86Squad Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n1.1e+12 2.2e+12 4.4e+12 8.8e+12 1.8e+13\nFLOPS\n64\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86Squad Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n4.0e-2 5.0e-2 6.0e-2 7.0e-2 8.0e-29.0e-2 1.0e-1 1.2e-1\n1/Throughput\n64\n66\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86Squad Accuracy\nSm\na\nl\nl\nFF\n1\nK\n-\nS\nM\nFF\n3\nK\n-\nS\nM\nFF\n6\nK\n-\nS\nM\nFF\n9\nK\n-\nS\nM\nFF\n1\n2\nK\n-\nS\nM\nEL\n2\n-\nS\nM\nEL\n4\n-\nS\nM\nEL\n8\n-\nS\nM\nEL\n1\n2\n-\nS\nM\nDL\n2\n-\nS\nM\nDL\n4\n-\nS\nM\nDL\n8\n-\nS\nM\nDL\n1\n2\n-\nS\nM\nNL\n2\n-\nS\nM\nNL\n4\n-\nS\nM\nNL\n8\n-\nS\nM\nNL\n1\n6\n-\nS\nM\nNL\n2\n4\n-\nS\nM\nKV\n1\n6\n-\nS\nM\nKV\n3\n2\n-\nS\nM\nKV\n1\n2\n8\n-\nS\nM\nKV\n2\n5\n6\n-\nS\nM\nDM\n2\n5\n6\n-\nS\nM\nDM\n7\n6\n8\n-\nS\nM\nDM\n1\nK\n-\nS\nM\nDM\n2\nK\n-\nS\nM\n(d) Downstream: Squad Accuracy\nFigure 6: Performance on upstream and different downstream tasks with respect to number of\nparameters, FLOPs, and throughput for small models presented in Figure 2a.\n16\nPreprint\n3.4e+7 6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9\nParams\n-2.20\n-2.15\n-2.10\n-2.05\n-2.00\n-1.95\n-1.90\n-1.85\n-1.80\n-1.75\n-1.70\n-1.65\n-1.60Negative Log-Perplexity\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n2.2e+12 4.4e+12 8.8e+12 1.8e+13 3.5e+13\nFLOPS\n-2.20\n-2.15\n-2.10\n-2.05\n-2.00\n-1.95\n-1.90\n-1.85\n-1.80\n-1.75\n-1.70\n-1.65\n-1.60Negative Log-Perplexity\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n4.0e-2 6.0e-2 8.0e-2 1.0e-1 1.2e-1 1.4e-1 1.8e-1 2.2e-1\n1/Throughput\n-2.20\n-2.15\n-2.10\n-2.05\n-2.00\n-1.95\n-1.90\n-1.85\n-1.80\n-1.75\n-1.70\n-1.65\n-1.60Negative Log-Perplexity\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n(a) Upstream: Negative Log Perplexity.\n3.4e+7 6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9\nParams\n60\n62\n64\n66\n68\n70\n72\n74\n76SuperGlue Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n2.2e+12 4.4e+12 8.8e+12 1.8e+13 3.5e+13\nFLOPS\n60\n62\n64\n66\n68\n70\n72\n74\n76SuperGlue Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n4.0e-2 6.0e-2 8.0e-2 1.0e-1 1.2e-1 1.4e-1 1.8e-1 2.2e-1\n1/Throughput\n60\n62\n64\n66\n68\n70\n72\n74\n76SuperGlue Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n(b) Downstream: SuperGLUE Accuracy\n3.4e+7 6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9\nParams\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85Glue Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n2.2e+12 4.4e+12 8.8e+12 1.8e+13 3.5e+13\nFLOPS\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85Glue Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n4.0e-2 6.0e-2 8.0e-2 1.0e-1 1.2e-1 1.4e-1 1.8e-1 2.2e-1\n1/Throughput\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85Glue Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n(c) Downstream: GLUE Accuracy\n3.4e+7 6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9\nParams\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88Squad Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n2.2e+12 4.4e+12 8.8e+12 1.8e+13 3.5e+13\nFLOPS\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88Squad Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n4.0e-2 6.0e-2 8.0e-2 1.0e-1 1.2e-1 1.4e-1 1.8e-1 2.2e-1\n1/Throughput\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88Squad Accuracy\nBa\ns\ne\nFF\n1\nK\nFF\n2\nK\nFF\n6\nK\nFF\n9\nK\nFF\n1\n2\nK\nDM\n2\n5\n6\nDM\n5\n1\n2\nDM\n1\nK\nDM\n2\nK\nNL\n2\nNL\n4\nNL\n8\nNL\n1\n6\nNL\n2\n4\nEL\n2\nEL\n4\nEL\n8\nDL\n2\nDL\n4\nDL\n8\nKV\n1\n6\nKV\n3\n2\nKV\n1\n2\n8\nKV\n2\n5\n6\n(d) Downstream: Squad Accuracy\nFigure 7: Performance on upstream and different downstream tasks with respect to number of\nparameters, FLOPs, and throughput for base models presented in Figure 2b.\n17\nPreprint\n6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9 2.1e+9\nParams\n-2.10\n-2.05\n-2.00\n-1.95\n-1.90\n-1.85\n-1.80\n-1.75\n-1.70\n-1.65\n-1.60\n-1.55\n-1.50Negative Log-Perplexity\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n0\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nDM\n2\nK\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nKV\n2\n5\n6\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n4.4e+12 8.8e+12 1.8e+13 3.5e+13 7.0e+13 1.4e+14\nFLOPS\n-2.10\n-2.05\n-2.00\n-1.95\n-1.90\n-1.85\n-1.80\n-1.75\n-1.70\n-1.65\n-1.60\n-1.55\n-1.50Negative Log-Perplexity\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n0\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nDM\n2\nK\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nKV\n2\n5\n6\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n3.1e-2 6.3e-2 1.3e-1 2.5e-1 5.0e-1 1.0e+0\n1/Throughput\n-2.10\n-2.05\n-2.00\n-1.95\n-1.90\n-1.85\n-1.80\n-1.75\n-1.70\n-1.65\n-1.60\n-1.55\n-1.50Negative Log-Perplexity\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n0\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nDM\n2\nK\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nKV\n2\n5\n6\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n(a) Upstream: Negative Log Perplexity.\n6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9 2.1e+9\nParams\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80SuperGlue Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n4.4e+12 8.8e+12 1.8e+13 3.5e+13 7.0e+13\nFLOPS\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80SuperGlue Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n3.1e-2 6.3e-2 1.3e-1 2.5e-1 5.0e-1\n1/Throughput\n58\n60\n62\n64\n66\n68\n70\n72\n74\n76\n78\n80SuperGlue Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n(b) Downstream: SuperGLUE Accuracy\n6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9 2.1e+9\nParams\n72\n74\n76\n78\n80\n82\n84\n86\n88Glue Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n4.4e+12 8.8e+12 1.8e+13 3.5e+13 7.0e+13\nFLOPS\n72\n74\n76\n78\n80\n82\n84\n86\n88Glue Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n3.1e-2 6.3e-2 1.3e-1 2.5e-1 5.0e-1\n1/Throughput\n72\n74\n76\n78\n80\n82\n84\n86\n88Glue Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n(c) Downstream: GLUE Accuracy\n6.7e+7 1.3e+8 2.7e+8 5.4e+8 1.1e+9 2.1e+9\nParams\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88\n90Squad Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n4.4e+12 8.8e+12 1.8e+13 3.5e+13 7.0e+13\nFLOPS\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88\n90Squad Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n3.1e-2 6.3e-2 1.3e-1 2.5e-1 5.0e-1\n1/Throughput\n68\n70\n72\n74\n76\n78\n80\n82\n84\n86\n88\n90Squad Accuracy\nNL\n2\n-\nL\nG\nNL\n4\n-\nL\nG\nNL\n8\n-\nL\nG\nNL\n1\n2\n-\nL\nG\nNL\n1\n6\n-\nL\nG\nNL\n2\n0\n-\nL\nG\nNL\n2\n4\n-\nL\nG\nNL\n3\n2\n-\nL\nG\nNL\n3\n6\n-\nL\nG\nDM\n1\n2\n8\n-\nL\nG\nDM\n2\n5\n6\n-\nL\nG\nDM\n5\n1\n2\n-\nL\nG\nDM\n7\n6\n8\n-\nL\nG\nKV\n1\n6\n-\nL\nG\nKV\n3\n2\n-\nL\nG\nKV\n1\n2\n8\n-\nL\nG\nNH\n2\n-\nL\nG\nNH\n4\n-\nL\nG\nNH\n8\n-\nL\nG\nNH\n1\n2\n-\nL\nG\nNH\n2\n4\n-\nL\nG\nNH\n3\n2\n-\nL\nG\nEL\n2\n-\nL\nG\nEL\n4\n-\nL\nG\nEL\n6\n-\nL\nG\nEL\n8\n-\nL\nG\nEL\n1\n2\n-\nL\nG\nDL\n2\n-\nL\nG\nDL\n4\n-\nL\nG\nDL\n6\n-\nL\nG\nDL\n8\n-\nL\nG\nDL\n1\n2\n-\nL\nG\nDL\n1\n6\n-\nL\nG\nDL\n3\n2\n-\nL\nG\n(d) Downstream: Squad Accuracy\nFigure 8: Performance on upstream and different downstream tasks with respect to number of\nparameters, FLOPs, and throughput for large models presented in Figure 2c.\n18",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.818834125995636
    },
    {
      "name": "Computer science",
      "score": 0.7576059699058533
    },
    {
      "name": "Scaling",
      "score": 0.7546381950378418
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.6057997345924377
    },
    {
      "name": "Scalability",
      "score": 0.47945988178253174
    },
    {
      "name": "Language model",
      "score": 0.4319024085998535
    },
    {
      "name": "Machine learning",
      "score": 0.33953607082366943
    },
    {
      "name": "Artificial intelligence",
      "score": 0.338827908039093
    },
    {
      "name": "Voltage",
      "score": 0.12388339638710022
    },
    {
      "name": "Engineering",
      "score": 0.11606502532958984
    },
    {
      "name": "Operations management",
      "score": 0.10784351825714111
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 56
}