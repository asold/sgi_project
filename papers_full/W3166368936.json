{
    "title": "Uformer: A General U-Shaped Transformer for Image Restoration",
    "url": "https://openalex.org/W3166368936",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1928996633",
            "name": "Wang Zhen-dong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221392679",
            "name": "Cun, Xiaodong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2143844023",
            "name": "Bao, Jianmin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2231152892",
            "name": "Zhou, Wengang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2231725535",
            "name": "Liu, Jianzhuang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2643175103",
            "name": "Li, Houqiang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2967584026",
        "https://openalex.org/W2888338418",
        "https://openalex.org/W3179869055",
        "https://openalex.org/W2964101377",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W2133665775",
        "https://openalex.org/W2984466630",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2560533888",
        "https://openalex.org/W2970318705",
        "https://openalex.org/W2866634454",
        "https://openalex.org/W2897392422",
        "https://openalex.org/W2964030969",
        "https://openalex.org/W2796940687",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3178192988",
        "https://openalex.org/W3201809115",
        "https://openalex.org/W2963163009",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W3109494165",
        "https://openalex.org/W2167307343",
        "https://openalex.org/W2963085671",
        "https://openalex.org/W2962737939",
        "https://openalex.org/W3168101492",
        "https://openalex.org/W3170697543",
        "https://openalex.org/W2799192307",
        "https://openalex.org/W3034504121",
        "https://openalex.org/W2962767526",
        "https://openalex.org/W2126926806",
        "https://openalex.org/W2056370875",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3085824656",
        "https://openalex.org/W3035484352",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W3035250394",
        "https://openalex.org/W3202715235",
        "https://openalex.org/W3136416617",
        "https://openalex.org/W1916731006",
        "https://openalex.org/W2983315964",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W2097073572",
        "https://openalex.org/W3176153963",
        "https://openalex.org/W3106758205",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W3182000414",
        "https://openalex.org/W3166513219",
        "https://openalex.org/W3103174683",
        "https://openalex.org/W2508457857",
        "https://openalex.org/W3210134292",
        "https://openalex.org/W2962770929",
        "https://openalex.org/W2982795046",
        "https://openalex.org/W3174738881",
        "https://openalex.org/W2963372104",
        "https://openalex.org/W2969717429",
        "https://openalex.org/W3099686304",
        "https://openalex.org/W2887181327",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2466666260",
        "https://openalex.org/W3207918547",
        "https://openalex.org/W3202362072",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3164540605",
        "https://openalex.org/W3171125843",
        "https://openalex.org/W2985577924",
        "https://openalex.org/W2884068670",
        "https://openalex.org/W3203606893",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W2912268344",
        "https://openalex.org/W3013529009",
        "https://openalex.org/W2798735168",
        "https://openalex.org/W2965217508",
        "https://openalex.org/W2988641380"
    ],
    "abstract": "In this paper, we present Uformer, an effective and efficient Transformer-based architecture for image restoration, in which we build a hierarchical encoder-decoder network using the Transformer block. In Uformer, there are two core designs. First, we introduce a novel locally-enhanced window (LeWin) Transformer block, which performs nonoverlapping window-based self-attention instead of global self-attention. It significantly reduces the computational complexity on high resolution feature map while capturing local context. Second, we propose a learnable multi-scale restoration modulator in the form of a multi-scale spatial bias to adjust features in multiple layers of the Uformer decoder. Our modulator demonstrates superior capability for restoring details for various image restoration tasks while introducing marginal extra parameters and computational cost. Powered by these two designs, Uformer enjoys a high capability for capturing both local and global dependencies for image restoration. To evaluate our approach, extensive experiments are conducted on several image restoration tasks, including image denoising, motion deblurring, defocus deblurring and deraining. Without bells and whistles, our Uformer achieves superior or comparable performance compared with the state-of-the-art algorithms. The code and models are available at https://github.com/ZhendongWang6/Uformer.",
    "full_text": "Uformer: A General U-Shaped Transformer for Image Restoration\nZhendong Wang1, Xiaodong Cun 2*, Jianmin Bao, Wengang Zhou 1, Jianzhuang Liu3, Houqiang Li1\n1 University of Science and Technology of China, 2 University of Macau,\n3 University of Chinese Academy of Sciences\nAbstract\nIn this paper, we present Uformer, an effective and efÔ¨Å-\ncient Transformer-based architecture for image restoration,\nin which we build a hierarchical encoder-decoder network\nusing the Transformer block. In Uformer, there are two\ncore designs. First, we introduce a novel locally-enhanced\nwindow (LeWin) Transformer block, which performs non-\noverlapping window-based self-attention instead of global\nself-attention. It signiÔ¨Åcantly reduces the computational\ncomplexity on high resolution feature map while capturing\nlocal context. Second, we propose a learnable multi-scale\nrestoration modulator in the form of a multi-scale spatial\nbias to adjust features in multiple layers of the Uformer\ndecoder. Our modulator demonstrates superior capabil-\nity for restoring details for various image restoration tasks\nwhile introducing marginal extra parameters and compu-\ntational cost. Powered by these two designs, Uformer en-\njoys a high capability for capturing both local and global\ndependencies for image restoration. To evaluate our ap-\nproach, extensive experiments are conducted on several im-\nage restoration tasks, including image denoising, motion\ndeblurring, defocus deblurring and deraining. Without bells\nand whistles, our Uformer achieves superior or compara-\nble performance compared with the state-of-the-art algo-\nrithms. The code and models are available at https:\n//github.com/ZhendongWang6/Uformer.\n1. Introduction\nWith the rapid development of consumer and industry\ncameras and smartphones, the requirements of removing\nundesired degradation (e.g., noise, blur, rain, and so on) in\nimages are constantly growing. Recovering genuine images\nfrom their degraded versions, i.e., image restoration, is a\nclassic task in computer vision. Recent state-of-the-art meth-\nods [9, 44, 71, 73, 74] are mostly ConvNets-based, which\nachieve impressive results but show a limitation in capturing\nlong-range dependencies. To address this problem, several\n*Corresponding author\n101 102 103\nComputational cost in log scale (GMACs)\n39.4\n39.6\n39.8\nPSNR (dB)\nUNet-T UNet-S\nUNet-B\nUformer-T\nUformer-S\nUformer-B\nDANet\nVDN\nNBNet\nMIRNet\nMPRNet\nCycleISP\nFigure 1. PSNR vs. computational cost on the SIDD dataset [1].\nrecent works [30, 38, 80] start to employ single or few self-\nattention layers in low resolution feature maps due to the\nself-attention computational complexity being quadratic to\nthe feature map size.\nIn this paper, we aim to leverage the capability of self-\nattention in feature maps at multi-scale resolutions to recover\nmore image details. To this end, we present Uformer, an\neffective and efÔ¨Åcient Transformer-based structure for image\nrestoration. Uformer is built upon an elegant architecture\nUNet [46], wher we modify the convolution layers to Trans-\nformer blocks while keeping the same overall hierarchical\nencoder-decoder structure and the skip-connections.\nWe propose two core designs to make Uformer suitable\nfor image restoration tasks. First, we propose the Locally-\nenhanced Window (LeWin) Transformer block, which is an\nefÔ¨Åcient and effective basic component. The LeWin Trans-\nformer block performs non-overlapping window-based self-\nattention instead of global self-attention, which signiÔ¨Åcantly\nreduces the computational complexity on high resolution fea-\nture maps. Since we build hierarchical feature maps and keep\nthe window size unchanged, the window-based self-attention\nat low resolution is able to capture much more global de-\npendencies. On the other hand, local context is essential for\nimage restoration, we further introduce a depth-wise con-\nvolutional layer between two fully-connected layers of the\nfeed-forward network in the Transformer block to better cap-\nture local context. We also notice that recent works [34, 69]\nuse the similar design for different tasks.\n1\narXiv:2106.03106v2  [cs.CV]  25 Nov 2021\nSecond, we propose a learnable multi-scale restoration\nmodulator to handle various image degradations. The mod-\nulator is formulated as a multi-scale spatial bias to adjust\nfeatures in multiple layers of the Uformer decoder. SpeciÔ¨Å-\ncally, a learnable window-based tensor is added to features\nin each LeWin Transformer block to adapt the features for\nrestoring more details. BeneÔ¨Åting from the simple operator\nand window-based mechanism, it can be Ô¨Çexibly applied for\nvarious image restoration tasks in different frameworks.\nBased on the above two designs, without bells and whis-\ntles, e.g., the multi-stage or multi-scale framework [74, 75]\nand the advanced loss function [27, 28], our simple U-\nshaped Transformer structure achieves state-of-the-art per-\nformance on multiple image restoration tasks. For de-\nnoising, Uformer outperforms the previous state-of-the-\nart method (NBNet [9]) by 0.14 dB and 0.09 dB on the\nSIDD [1] and DND [43] benchmarks, respectively. For the\nmotion blur removal task, Uformer achieves the best (Go-\nPro [42], RealBlur-R [45], and RealBlur-J [45]) or compet-\nitive (HIDE [49]) performance, displaying its strong capa-\nbility of deblurring. Uformer also shows the potential on\nthe defocus deblurring task [3] and outperforms the previous\nbest model [51] by 1.04 dB. Also, on the SPAD dataset [58]\nfor deraining, it obtains 47.84 dB on PSNR, an improvement\nof 3.74 dB over the previous state-of-the-art method [44]. We\nexpect our work will encourage further research to explore\nTransformer-based architectures for image restoration.\nOverall, we summarize the contributions of this paper as\nfollows:\n‚Ä¢ We present Uformer, a general and superior U-\nshaped Transformer for various image restoration tasks.\nUformer is built on the basic LeWin Transformer block\nthat is both efÔ¨Åcient and effective.\n‚Ä¢ We present an extra light-weight learnable multi-scale\nrestoration modulator to adjust on multi-scale features.\nThis simple design signiÔ¨Åcantly improves the restora-\ntion quality.\n‚Ä¢ Extensive experiments show that Uformer establishes\nnew state-of-the-arts on various datasets for image\nrestoration tasks.\n2. Related Work\nImage Restoration Architectures Image restoration\naims to restore the clean image from its degraded version.\nA popular solution is to learn effective models using the\nU-shaped structures with skip-connection to capture multi-\nscale information hierarchically for various image restora-\ntion tasks, including image denoising [9, 71, 74], deblur-\nring [3,27,28], and demoireing [37,52]. Some image restora-\ntion methods are inspired by the key insight from the rapid\ndevelopment of image classiÔ¨Åcation [17, 26]. For example,\nResNet-based structure has been widely used for general\nimage restoration [39, 80] as well as for speciÔ¨Åc tasks in\nimage restoration such as super-resolution [36, 81] and im-\nage denoising [15, 78]. More CNN-based image restoration\narchitectures can be found in the recent surveys [31, 54, 62]\nand the NTIRE Challenges [2].\nUntil recently, some works start to explore the atten-\ntion mechanism to boost the performance. For example,\nsqueeze-and-excitation networks [20] and non-local neu-\nral networks [60] inspire a branch of methods for different\nimage restoration tasks, such as super-resolution [35, 79],\nderaining [32, 74], and denoising [73, 74]. Our Uformer also\napplies the hierarchical structure to build multi-scale features\nwhile using the newly introduced LeWin Transformer block\nas the basic building block.\nVision TransformersTransformer [56] shows a signif-\nicant performance in natural language processing (NLP).\nDifferent from the design of CNNs, Transformer-based net-\nwork structures are naturally good at capturing long-range\ndependencies in the data by the global self-attention. The\nsuccess of Transformer in the NLP domain also inspires\nthe computer vision researchers. The pioneering work of\nViT [14] directly trains a pure Transformer-based architec-\nture on the medium-size (16 √ó16) Ô¨Çattened patches. With\nlarge-scale data pre-training (i.e., JFT-300M), ViT gets ex-\ncellent results compared to state-of-the-art CNNs on image\nclassiÔ¨Åcation.\nSince the introduction of ViT, many efforts have been\nmade to reduce the quadratic computational cost of global\nself-attention for making Transformer more suitable for\nvision tasks. Some works [19, 59] focus on establishing\na pyramid Transformer architecture simlilar to ConvNet-\nbased structure. To overcome the quadratic complexity of\noriginal self-attention, self-attention is performed on local\nwindows with the halo operation or window shift [40, 55]\nto help cross-window interaction, and get promising re-\nsults. Rather than focusing on image classiÔ¨Åcation, recent\nworks [10, 13, 22, 67, 82] propose a brunch of Transformer-\nbased backbones for more general high-level vision tasks.\nBesides high-level discriminative tasks, there are also\nsome Transformer-based works [24, 65, 83] for generative\ntasks. While there are a lot of explorations in the vision area,\nintroducing Transformer to low-level vision still lacks explo-\nration. Early work [66] makes use of self-attention mech-\nanism to learn texture for super-resolution. As for image\nrestoration tasks, IPT [8] Ô¨Årst applies standard Transformer\nblocks within a multi-task learning framework. However,\nIPT relies on pretraining on a large-scale synthesized dataset\nand multi-task learning for good performance. In contrast,\nwe design a general U-shaped Transformer-based structure,\nwhich proves to be efÔ¨Åcient and effective for image restora-\ntion.\n2\n3√óùêª√óùëä C√óùêª√óùëä2ùê∂√óùêª2√óùëä2 16ùê∂√óùêª16√óùëä16\n+\nDegraded Image\nRestored Image\nInputProjection\nOutputProjection\nLeWinBlocks√óùëÅ! LeWinBlocks√óùëÅ#\nLeWinBlocks√óùëÅ\"\nLeWinBlocks√óùëÅ! LeWinBlocks√óùëÅ#\n‚Ä¶\nModulators‚Ä¶‚Ä¶\n‚Ä¶\nW/MW-MSA\nLNLeFF\n(b)\nLN\nMSAMSAMSAMSA\nModulator\n(a) (c)FeaturemapWindow\nDown-SamplingUp-Sampling\nFigure 2. (a) Overview of the Uformer structure. (b) LeWin Transformer block. (c) Illustration of how the modulators modulate the W-MSAs\nin each LeWin Transformer block which is named MW-MSA in (b).\n3. Method\nIn this section, we Ô¨Årst describe the overall pipeline and\nthe hierarchical structure of Uformer for image restoration.\nThen, we provide the details of the LeWin Transformer block\nwhich is the basic component of Uformer. After that, we\npresent the multi-scale restoration modulator.\n3.1. Overall Pipeline\nAs shown in Figure 2(a), the overall structure of the pro-\nposed Uformer is a U-shaped hierarchical network with skip-\nconnections between the encoder and the decoder. To be\nspeciÔ¨Åc, given a degraded image I ‚ààR3√óH√óW , Uformer\nÔ¨Årstly applies a 3 √ó3 convolutional layer with LeakyReLU\nto extract low-level features X0 ‚ààRC√óH√óW . Next, fol-\nlowing the design of the U-shaped structures [23, 46], the\nfeature maps X0 are passed through Kencoder stages. Each\nstage contains a stack of the proposed LeWin Transformer\nblocks and one down-sampling layer. The LeWin Trans-\nformer block takes advantage of the self-attention mecha-\nnism for capturing long-range dependencies, and also cuts\nthe computational cost due to the usage of self-attention\nthrough non-overlapping windows on the feature maps. In\nthe down-sampling layer, we Ô¨Årst reshape the Ô¨Çattened fea-\ntures into 2D spatial feature maps, and then down-sample\nthe maps, double the channels using 4 √ó4 convolution\nwith stride 2. For example, given the input feature maps\nX0 ‚ààRC√óH√óW , the l-th stage of the encoder produces the\nfeature maps Xl ‚ààR2lC√óH\n2l √óW\n2l .\nThen, a bottleneck stage with a stack of LeWin Trans-\nformer blocks is added at the end of the encoder. In this\nstage, thanks to the hierarchical structure, the Transformer\nblocks capture longer (even global when the window size\nequals the feature map size) dependencies.\nFor feature reconstruction, the proposed decoder also con-\ntains Kstages. Each consists of an up-sampling layer and a\nstack of LeWin Transformer blocks similar to the encoder.\nWe use 2 √ó2 transposed convolution with stride 2 for the up-\nsampling. This layer reduces half of the feature channels and\ndoubles the size of the feature maps. After that, the features\ninput to the LeWin Transformer blocks are concatenation\nof the up-sampled features and the corresponding features\nfrom the encoder through skip-connection. Next, the LeWin\nTransformer blocks are utilized to learn to restore the image.\nAfter the Kdecoder stages, we reshape the Ô¨Çattened features\nto 2D feature maps and apply a 3 √ó3 convolution layer to\nobtain a residual image R ‚ààR3√óH√óW . Finally, the restored\nimage is obtained by I‚Ä≤= I + R. We train Uformer using\nthe Charbonnier loss [7, 73]:\n‚Ñì(I‚Ä≤,ÀÜI) =\n‚àö\n||I‚Ä≤‚àíÀÜI||2 + œµ2, (1)\nwhere ÀÜI is the ground-truth image, and œµ = 10‚àí3 is a con-\nstant in all the experiments.\n3.2. LeWin Transformer Block\nThere are two main challenges to apply Transformer for\nimage restoration. First, the standard Transformer architec-\nture [14, 56] computes self-attention globally between all\ntokens, which contributes to the quadratic computation cost\nwith respect to the number of tokens. It is unsuitable to\napply global self-attention on high-resolution feature maps.\nSecond, the local context information is essential for image\nrestoration tasks since the neighborhood of a degraded pixel\ncan be leveraged to restore its clean version, but previous\nworks [34, 63] suggest that Transformer shows a limitation\nin capturing local dependencies.\nTo address the above mentioned two issues, we propose a\nLocally-enhanced Window (LeWin) Transformer block, as\nshown in Figure 2(b), which beneÔ¨Åts from the self-attention\nin Transformer to capture long-range dependencies, and\nalso involves the convolution operator into Transformer to\ncapture useful local context. SpeciÔ¨Åcally, given the features\n3\nat the ( l-1)-th block Xl‚àí1, we build the block with two\ncore designs: (1) non-overlapping Window-based Multi-\nhead Self-Attention (W-MSA) and (2) Locally-enhanced\nFeed-Forward Network (LeFF). The computation of a LeWin\nTransformer block is represented as:\nX‚Ä≤\nl = W-MSA(LN(Xl‚àí1)) +Xl‚àí1,\nXl = LeFF(LN(X‚Ä≤\nl)) +X‚Ä≤\nl, (2)\nwhere X‚Ä≤\nl and Xl are the outputs of the W-MSA module\nand LeFF module, respectively. LN represents the layer\nnormalization [5]. In the following, we elaborate W-MSA\nand LeFF separately.\nWindow-based Multi-head Self-Attention (W-MSA).In-\nstead of using global self-attention like the vanilla Trans-\nformer, we perform the self-attention within non-overlapping\nlocal windows, which reduces the computational cost sig-\nniÔ¨Åcantly. Given the 2D feature maps X ‚ààRC√óH√óW with\nH and W being the height and width of the maps, we split\nX into non-overlapping windows with the window size of\nM √óM, and then get the Ô¨Çattened and transposed features\nXi ‚ààRM2√óC from each window i. Next, we perform self-\nattention on the Ô¨Çattened features in each window.\nSuppose the head number is kand the head dimension is\ndk = C/k. Then computing the k-th head self-attention in\nthe non-overlapping windows can be formulated as follows,\nX = {X1,X2,¬∑¬∑¬∑ ,XN}, N = HW/M2,\nYi\nk = Attention(XiWQ\nk ,XiWK\nk ,XiWV\nk ), i= 1,¬∑¬∑¬∑ ,N,\nÀÜXk = {Y1\nk,Y2\nk,¬∑¬∑¬∑ ,YM\nk },\n(3)\nwhere WQ\nk , WK\nk , WV\nk ‚ààRC√ódk represent the projection\nmatrices of the queries, keys, and values for the k-th head,\nrespectively. ÀÜXk is the output of the k-th head. Then the\noutputs for all heads{1,2,¬∑¬∑¬∑ ,k}are concatenated and then\nlinearly projected to get the Ô¨Ånal result. Inspired by previous\nworks [40, 48], we also apply the relative position encoding\ninto the attention module, so the attention calculation can be\nformulated as:\nAttention(Q,K,V) =SoftMax(QKT\n‚àödk\n+ B)V, (4)\nwhere B is the relative position bias, whose values are taken\nfrom ÀÜB ‚ààR(2M‚àí1)√ó(2M‚àí1) with learnable parameters [40,\n48].\nWindow-based self-attention can signiÔ¨Åcantly reduce\nthe computational cost compared with global self-attention.\nGiven the feature maps X ‚ààRC√óH√óW , the computational\ncomplexity drops from O(H2W2C) to O(HW\nM2 M4C) =\nO(M2HWC). Since we design Uformer as a hierarchical\narchitecture, our window-based self-attention at low reso-\nlution feature maps works on larger receptive Ô¨Åelds and is\nsufÔ¨Åcient to learn long-range dependencies. We also try the\nshifted-window strategy [40] in the even LeWin Transformer\nblock of each stage in our framework, which gives only\nslightly better results.\nConv.(1x1)Img2Tokens\nTokens2ImgDepthwiseConv.(3x3)\nConv.(1x1)\nFigure 3. Locally-enhanced feed-forward network.\nLocally-enhanced Feed-Forward Network (LeFF). As\npointed out by previous works [63, 69], the Feed-Forward\nNetwork (FFN) in the standard Transformer suffers limited\ncapability to leverage local context. Actually, neighboring\npixels are crucial references for image restoration [6, 21].\nTo overcome this issue, we add a depth-wise convolutional\nblock to the FFN in our Transformer-based structure follow-\ning the recent works [34, 47, 69]. As shown in Figure 3, we\nÔ¨Årst apply a linear projection layer to each token to increase\nits feature dimension. Next, we reshape the tokens to 2D fea-\nture maps, and use a 3 √ó3 depth-wise convolution to capture\nlocal information. Then we Ô¨Çatten back the features to tokens\nand shrink the channels via another linear layer to match the\ndimension of the input channels. We use GELU [18] as the\nactivation function after each linear/convolution layer.\n3.3. Multi-Scale Restoration Modulator\nDifferent types of image degradation ( e.g. blur, noise,\nrain, etc.) have their own distinctive perturbed patterns to\nbe handled or restored. To further boost the capability of\nUformer for approaching various perturbations, we propose\na light-weight multi-scale restoration modulator to calibrate\nthe features and encourage more details recovered.\nAs shown in Figure 2(a) and 2(c), the multi-scale restora-\ntion modulator applies multiple modulators in the Uformer\ndecoder. Specially in each LeWin Transformer block, a mod-\nulator is formulated as a learnable tensor with a shape of\nM√óM√óC, where M is the window size andCis the chan-\nnel dimension of current feature map. Each modulator is\nsimply served as a shared bias term that is added into all non-\noverlapping windows before self-attention module. Due to\nthis light-weight addition operation and window-sized shape,\nthe multi-scale restoration modulator introduces marginal\nextra parameters and computational cost.\nWe prove the effectiveness of the multi-scale restoration\nmodulator on two typical image restoration tasks: image\ndeblurring and image denoising. The visualization compar-\nisons are presented in Figure 4. We observe that adding\nthe multi-scale restoration modulator makes more motion\nblur/noising patterns removed and yields a much cleaner\n4\n(a) w/o Modulator\n (b) w/ Modulator\n (c) Target\n(d) w/o Modulator\n (e) w/ Modulator\n (f) Target\nFigure 4. Effect of the multi-scale restoration modulator on image\ndeblurring (top samples from GoPro [42]) and denoising (bottom\nsamples from SIDD [1]). Compared with (a), Uformer w/ Mod-\nulator (b) can remove much more blur and recover the numbers\naccurately. Compared with (d), the image restored by Uformer w/\nModulator (e) is closer to the target with more details.\nimage. These results show that our multi-scale restoration\nmodulator truly helps to recover restoration details with little\ncomputation cost. One possible explanation is that adding\nmodulators at each stage of the decoder enables a Ô¨Çexible ad-\njustment of the feature maps that boosts the performance for\nrestoring details. This is consistent with the previous work\nStyleGAN [25] using a multi-scale noise term adding to the\nconvolution features, which realizes stochastic variation for\ngenerating photo-realistic images.\n4. Experiments\nIn this section, we Ô¨Årst discuss the experimental setup. Af-\nter that, we verify the effectiveness and efÔ¨Åciency of Uformer\non various image restoration tasks on eight datasets. Finally,\nwe perform comprehensive ablation studies to evaluate each\ncomponent of our proposed Uformer.\n4.1. Experimental Setup\nBasic settings.Following the common training strategy of\nTransformer [56], we train our framework using the AdamW\noptimizer [41] with the momentum terms of (0.9,0.999)\nand the weight decay of 0.02. We randomly augment the\ntraining samples using the horizontal Ô¨Çipping and rotate the\nimages by 90‚ó¶, 180‚ó¶, or 270‚ó¶. We use the cosine decay\nstrategy to decrease the learning rate to 1e-6 with the ini-\ntial learning rate 2e-4. We set the window size to 8 √ó8 in\nall LeWin Transformer blocks. The number of Uformer\nencoder/decoder stages K equals 4 by default. And the di-\nmension of each head in Transformer block dk equals C.\nMore dataset-speciÔ¨Åc experimental settings can be found in\nthe supplementary materials.\nArchitecture variants.For a concise description, we intro-\nduce three Uformer variants in our experiments, Uformer-\nT (Tiny), Uformer-S (Small), and Uformer-B (Base) by set-\nting different Transformer feature channels Cand the num-\nbers of the Transformer blocks in each encoder and decoder\nstages. The details are listed as follows:\n‚Ä¢ Uformer-T: C = 16, depths of Encoder = {2, 2, 2, 2},\n‚Ä¢ Uformer-S: C = 32, depths of Encoder = {2, 2, 2, 2},\n‚Ä¢ Uformer-B: C = 32, depths of Encoder = {1, 2, 8, 8},\nand the depths of Decoder are mirrored depths of Encoder.\nEvaluation metrics.We adopt the commonly-used PSNR\nand SSIM [61] metrics to evaluate the restoration perfor-\nmance. These metrics are calculated in the RGB color space\nexcept for deraining where we evaluate the PSNR and SSIM\non the Y channel in the YCbCr color space, following the\nprevious work [57].\nSIDD DND\nMethod PSNR‚Üë SSIM‚Üë PSNR‚Üë SSIM‚Üë\nBM3D [12] 25.65 0.685 34.51 0.851\nRIDNet [4] 38.71 0.914 39.26 0.953\nVDN [70] 39.28 0.909 39.38 0.952\nDANet [71] 39.47 0.918 39.59 0.955\nCycleISP [72] 39.52 0.957 39.56 0.956\nMIRNet [73] 39.72 0.959 39.88 0.956\nMPRNet [74] 39.71 0.958 39.80 0.954\nNBNet [9] 39.75 0.959 39.89 0.955\nUformer-B 39.89 0.960 39.98 0.955\nTable 1. Denoising results on the SIDD [1] and DND [43] datasets.\n4.2. Real Noise Removal\nTable 1 reports the results of real noise removal on the\nSIDD [1] and DND [43] datasets. We compare Uformer with\n8 state-of-the-art denoising methods, including the feature-\nbased BM3D [12] and seven learning-based methods: RID-\nNet [4], VDN [70], CycleISP [72], NBNet [9], DANet [71],\nMIRNet [73], and MPRNet [74]. Our Uformer-B achieves\n39.89 dB on PSNR, surpassing all the other methods by\nat least 0.14 dB. As for the DND dataset, we follow the\ncommon evaluation strategy and test our model trained on\nSIDD via the online server testing. Uformer outperforms\nthe previous state-of-the-art method NBNet [9] by 0.09 dB.\nTo verify whether the gains beneÔ¨Åt from more computation\ncost, we present the results of PSNR vs. computational\ncost in Figure 1. We notice that our Uformer-T can achieve\na better performance than most models but with the least\ncomputation cost, which demonstrates the efÔ¨Åciency and\neffectiveness of Uformer. We also show the qualitative re-\nsults on the SIDD and DND datasets in Figure 5, in which\nUformer can not only successfully remove the noise but also\nkeep the texture details.\n4.3. Motion Blur Removal\nFor motion blur removal, Uformer also shows state-of-\nthe-art performance. We follow the previous method [74] to\n5\n0010-0004-full\nDANet/34.73dBVDNet/34.44dB CycleISP/34.70dB\nMIRNet/34.84dB TargetInput\n Uformer-B/35.05dBMPRNet/34.71dB\nNBNet/34.84dB\nInput/18.01dB\nRIDNet/33.77dB\n0010-0004-full\nDANet/40.56dBVDNet/40.41dB CycleISP/40.39dB\nMIRNet/41.05dBInput Uformer-B/41.29dBMPRNet/41.04dB\nNBNet/41.12dBInput/31.46dB\nRIDNet/40.43dB\n BM3D/36.26dB\nFigure 5. Visual comparisons with state-of-the-art methods on real noise removal. The top sample comes from SIDD while the bottom one is\nfrom DND.\nGoPro HIDE RealBlur-R RealBlur-J\nMethod PSNR ‚ÜëSSIM ‚ÜëPSNR ‚ÜëSSIM ‚ÜëPSNR ‚ÜëSSIM ‚ÜëPSNR ‚ÜëSSIM ‚Üë\nNah et al. [42] 29.08 0.914 25.73 0.874 32.51 0.841 27.87 0.827\nDeblurGAN [27] 28.70 0.858 24.51 0.871 33.79 0.903 27.97 0.834\nXu et al. [64] 21.00 0.741 - - 34.46 0.937 27.14 0.830\nDeblurGAN-v2 [28] 29.55 0.934 26.61 0.875 35.26 0.944 28.70 0.866\nDBGAN [77] 31.10 0.942 28.94 0.915 - - - -\nSPAIR [44] 32.06 0.953 30.29 0.931 - - 28.81 0.875\n‚Ä†Zhang et al. [76] 29.19 0.931 - - 35.48 0.947 27.80 0.847\n‚Ä†SRN [53] 30.26 0.934 28.36 0.915 35.66 0.947 28.56 0.867\n‚Ä†DMPHN [75] 31.20 0.940 29.09 0.924 35.70 0.948 28.42 0.860\n‚Ä†MPRNet [74] 32.66 0.959 30.96 0.939 35.99 0.952 28.70 0.873\nUformer-B 32.97 0.967 30.83 0.952 36.22 0.957 29.06 0.884\nTable 2. Results on motion deblurring. Following privous works [27,28,74], our Uformer is only trained on the GoPro dataset [42]. Then we\napply our GoPro trained model directly on the HIDE dataset [49] and the RealBlur dataset [45] to evaluate the generalization on real scenes.\n‚Ä† denotes recurrent/multi-stage designs for better performance.\ntrain Uformer on the GoPro dataset and test it on the four\ndatasets: two synthesized datasets ( HIDE [49] and the test\nset of GoPro [42]), and two real-world datasets (RealBlur-\nR/-J from the RealBlur dataset [45]). We compare Uformer\nwith ten state-of-the-art methods: Nah et al . [42], De-\nblurGAN [27], Xu et al. [64], DeblurGAN-v2 [28], DB-\nGAN [77], SPAIR [44], Zhang et al. [76], SRN [53], DM-\nPHN [75], and MPRNet [74]. The results are reported in\nTable 2. For synthetic deblurring, Uformer gets signiÔ¨Åcant\nbetter performance on GoPro than previous state-of-the-art\nmethods and shows a comparable result on the HIDE dataset.\nAs for real-world deblurring, the causes of blur are compli-\ncated so the task is usually more challenging. Our Uformer\noutperforms other methods by at least 0.23 dB and 0.36\ndB on RealBlur-R and RealBlur-J, respectively, showing a\nstrong generalization ability. Besides, we show some visual\nresults in Figure 6. Compared with other methods, the im-\nages restored by Uformer are more clear and closer to their\nground truth.\n4.4. Defocus Blur Removal\nWe perform defocus blur removal on the DPD dataset [3].\nTable 3 and Figure 7 report the quantitative and qualita-\ntive results, respectively. Uformer achieves a better per-\n6\nformance (1.04 dB, 1.15 dB, 1.44 dB, and 1.87 dB) over\nprevious state-of-the-art methods KPAC [51], DPDNet [3],\nJNB [50], and DMENet [29], respectively. From the visu-\nalization results, we observe that the images recovered by\nUformer are sharper and closer to the ground-truth images.\nDMENet JNB DPDNet KPAC Uformer-B[29] [50] [3] [51]\nPSNR‚Üë 23.41 23.84 25.13 25.24 26.28\nSSIM‚Üë 0.714 0.715 0.786 0.842 0.891\nTable 3. Results on the DPD dataset [3] for defocus blur removal .\n4.5. Real Rain Removal\nWe conduct the deraining experiments on SPAD [58]\nand compare with 6 deraining methods: GMM [33], RES-\nCAN [32], SPANet [58], JORDER-E [68], RCDNet [57],\nand SPAIR [44]. As shown in Table 4, Uformer presents\na signiÔ¨Åcantly better performance, achieving 3.74 dB im-\nprovement over the previous best work [44]. This indicates\nthe strong capability of Uformer for deraining on this real\nderain dataset. We also provide the visual results in Figure 7\nwhere Uformer can remove the rain more successfully while\nintroducing fewer artifacts.\nGMM RESCAN SPANet JORDER-E RCDNet SPAIRUformer-B[33] [32] [58] [68] [57] [44]PSNR‚Üë 34.30 38.11 40.24 40.78 41.47 44.10 47.84SSIM‚Üë 0.9428 0.9707 0.9811 0.9811 0.9834 0.9872 0.9925\nTable 4. Results on the SPAD dataset [58] for real rain removal.\n4.6. Ablation Study\nIn this section, we analyze the effect of each compo-\nnent of Uformer in detail. The evaluations are conducted\non image denoising (SIDD [1]), deblurring (GoPro [42],\nRealBlur [45]), and deraining (SPAD [58]) using different\nvariants. The ablation results are reported in Tables 5, 6,\nand 7.\nTransformer vs. convolution.We replace all the LeWin\nTransformer blocks in Uformer with the convolution-based\nResBlocks [9], resulting in the so-called \"UNet\", while keep-\ning all others unchanged. Similar to the Uformer variants,\nwe design UNet-T/-S/-B:\n‚Ä¢ UNet-T: C = 32, depths of Encoder = {2, 2, 2, 2},\n‚Ä¢ UNet-S: C = 48, depths of Encoder = {2, 2, 2, 2},\n‚Ä¢ UNet-B: C = 76, depths of Encoder = {2, 2, 2, 2},\nand the depths of Decoder are mirrored depths of Encoder.\nTable 5 reports the comparison results. We observe\nthat Uformer-T achieves 39.66 dB and outperforms UNet-\nT by 0.04 dB with fewer parameters and less computation.\nGMACs # Param PSNR‚Üë\nUNet-T 15.49G 9.50M 39.62\nUNet-S 34.76G 21.38M 39.65\nUNet-B 86.97G 53.58M 39.71\nViT 8.83G 14.86M 38.51\nUformer-T 12.00G 5.23M 39.66\nUformer-S 43.86G 20.63M 39.77\nUformer-B 89.46G 50.88M 39.89\nTable 5. Comparison of different network architectures for denois-\ning on the SIDD dataset [1].\nW-MSA FFN GMACs # Param PSNR‚Üë\nUformer-S(SIDD [1])\n- - 43.00G 20.47M 39.74\n‚úì - 43.64G 20.59M 39.72\n- ‚úì 43.86G 20.63M 39.77\nUformer-B(RealBlur-R/-J [45])\n- - 88.31G 50.45M 36.15/28.99\n‚úì ‚úì 90.31G 51.20M 36.19/28.85\n- ‚úì 89.46G 50.88M 36.22/29.06\nTable 6. Effect of enhancing locality in different modules.\nUformer-S achieves 39.77 dB and outperforms UNet-S by\n0.12 dB with fewer parameters and a slightly higher com-\nputation cost. And Uformer-B achieves 39.89 dB which\noutperforms UNet-B by 0.18 dB. This study indicates the\neffectiveness of the proposed LeWin Transformer block,\ncompared with the original convolutional block.\nHierarchical structure vs. single scale.We further build\na ViT-based architecture which only contains a single scale\nof the feature maps for image denoising. This architecture\nemploys a head of two convolution layers for extracting fea-\ntures from the input image and also a tail of two convolution\nlayers for the output. 12 standard Transformer blocks are\nused between the head and the tail. We train the ViT with\nthe hidden dimension of 256 on patch size 16 √ó16. The\nresults are presented in Table 5. We observe that the vanilla\nViT structure gets an unsatisfactory result compared with\nUNet, while our Uformer signiÔ¨Åcantly outperforms both the\nViT-based and UNet architectures, which demonstrates the\neffectiveness of hierarchical structure for image restoration.\nWhere to enhance locality? Table 6 compares the re-\nsults of no locality enhancement and enhancing locality in\nthe self-attention calculation [63] or the feed-forward net-\nwork based on Uformer-S and Uformer-B. We observe that\nintroducing locality into the feed-forward network yields\n0.03 dB (SIDD), 0.07 dB (RealBlur-R)/0.07 dB (RealBlur-J)\nover the baseline (no locality enhancement), while introduc-\ning locality into the self-attention yields -0.02 dB (SIDD).\nFurther, we combine introducing locality into the feed-\nforward network and introducing into the self-attention. The\nresults on RealBlur-R/-J also drop from 36.22 dB/29.06 dB\nto 36.19 dB/28.85 dB, indicating that compared to involv-\ning locality into self-attention, introducing locality into the\nfeed-forward network is more suitable for image restoration\n7\nDBGAN/27.26dBInput/23.21dB DMPHN/24.56dB\nSRN/26.50dBInput Target/InfMPRNet/28.90dB\nDBGAN-v2/25.73dB\nUformer-B/30.95dB\nDBGAN27.26dBInput23.21dB DMPHN24.56dBInput MPRNet28.90dBDeblurGAN-v225.73dB Uformer-B30.95dB\n Target\nDBGAN/28.13dBInput/24.46dB DMPHN/27.30dB\nSRN/26.72dBInput TargetMPRNet/27.58dB\nDeblurGAN-v2/26.38dB\nUformer-B/28.31dB\nDBGAN28.13dBInput24.46dB DMPHN27.30dBInput TargetMPRNet27.58dBDeblurGAN-v226.38dB Uformer-B28.31dB\nFigure 6. Visual comparisons with state-of-the-art methods on the GoPro dataset [42] for motion blur removal.\nInput\n27.49 dB\nDPDNet\n28.64 dB\nKPAC\n28.46 dB\nUformer-B\n30.32 dB\nInput\n21.02 dB\nDPDNet\n22.37 dB\nKPAC\n22.23 dB\nUformer-B\n23.02 dB\nInput\n30.80 dB\nSPANet\n37.59 dB\nRCDNet\n39.00 dB\nUformer-B\n46.51 dB\nInput\n31.86 dB\nSPANet\n41.99 dB\nRCDNet\n43.00 dB\nUformer-B\n49.32 dB\nFigure 7. Top row: Visual comparisons with state-of-the-art methods on the DPD dataset [3] for defocus blur removal. Bottom row: Visual\ncomparisons with state-of-the-art methods on the SPAD dataset [58] for real rain removal.\ntasks.\nGoPro [42] SIDD [1] SPAD [58]\nUformer-T Uformer-B Uformer-B\nModulator - ‚úì - ‚úì - ‚úì\nPSNR‚Üë 29.11 29.57 39.86 39.89 47.43 47.84\nTable 7. Effect of the multi-scale restoration modulator.\nEffect of the multi-scale restoration modulator.In Ta-\nble 7, to verify the effect of the modulator, we conduct\nexperiments on GoPro for image deblurring, SIDD for im-\nage denoising, and SPAD for deraining. For deblurring,\nwe observe that w/ modulator can bring a performance im-\nprovement of 0.46 dB, which reveals the effectiveness of the\nmodulator for deblurring. We also compare the results of\nUformer-B with/without the modulator on SIDD and SPAD,\nand the comparisons indicate that the proposed modulator\nintroduces 0.03 dB improvement (SIDD)/0.41 dB improve-\nment (SPAD). In Figure 4, we have provided visual com-\nparisons of Uformer w/ and wo/ the modulator. This study\nvalidates the proposed modulator can bring extra ability of\nrestoring more details.\n5. Discussion and Conclusion\nIn this paper, we have presented an alternative architec-\nture Uformer for image restoration tasks by introducing the\nTransformer block. In contrast to existing ConvNet-based\nstructures, our Uformer builds upon the main component\nLeWin Transformer block, which can not only handle local\ncontext but also capture long-range dependencies efÔ¨Åciently.\nTo handle various image restoration degradation and en-\nhance restoration quality, we propose a learnable multi-scale\nrestoration modulator inserted into the Uformer decoder. Ex-\ntensive experiments demonstrate that Uformer achieves state-\nof-the art performance on several tasks, including denois-\ning, motion deblurring, defocus deblurring, and deraining.\nUformer also surpasses the UNet family by a large margin\nwith less computation cost and fewer model parameters.\nLimitation and broader impacts.Thanks to the proposed\narchitecture, Uformer achieves the state-of-the-art perfor-\nmance on a variety of image restoration tasks (image denois-\ning, deblurring, and deraining). But we have not evaluated\nUformer for more vision tasks such as image-to-image trans-\nlation, image super-resolution, and so on. We look forward to\ninvestigating Uformer for more applications. Meanwhile, we\nnotice that there are several negative impacts caused by abus-\ning image restoration techniques. For example, it may cause\nhuman privacy issue with the restored images in surveillance.\nThe techniques may destroy the original patterns for camera\nidentiÔ¨Åcation and multi-media copyright [11], which hurts\nthe authenticity for image forensics.\n8\nReferences\n[1] Abdelrahman Abdelhamed, Stephen Lin, and Michael S.\nBrown. A High-Quality Denoising Dataset for Smartphone\nCameras. In CVPR, 2018. 1, 2, 5, 7, 8, 12, 14\n[2] Abdelrahman Abdelhamed, Radu Timofte, and Michael S.\nBrown. NTIRE 2019 Challenge on Real Image Denoising:\nMethods and Results. In CVPR Workshop, 2019. 2\n[3] Abdullah Abuolaim and Michael S Brown. Defocus Deblur-\nring Using Dual-Pixel Data. In ECCV. Springer, 2020. 2, 6,\n7, 8, 13, 16\n[4] Saeed Anwar and Nick Barnes. Real Image Denoising with\nFeature Attention. In ICCV, 2019. 5\n[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer Normalization. arXiv preprint arXiv:1607.06450, 2016.\n4\n[6] A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm\nfor image denoising. In CVPR, 2005. 4\n[7] P. Charbonnier, L. Blanc-Feraud, G. Aubert, and M. Barlaud.\nTwo Deterministic Half-Quadratic Regularization Algorithms\nfor Computed Imaging. In ICIP, 1994. 3\n[8] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-Trained Image Processing Transformer. In\nCVPR, 2021. 2\n[9] Shen Cheng, Yuzhi Wang, Haibin Huang, Donghao Liu, Hao-\nqiang Fan, and Shuaicheng Liu. NBNet: Noise Basis Learn-\ning for Image Denoising with Subspace Projection. In CVPR,\n2021. 1, 2, 5, 7, 12, 13\n[10] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing\nRen, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins:\nRevisiting the Design of Spatial Attention in Vision Trans-\nformers. In NeurIPS, 2021. 2\n[11] Davide Cozzolino and Luisa Verdoliva. Noiseprint: a\nCNN-based camera model Ô¨Ångerprint. arXiv preprint\narXiv:1808.08396, 2018. 8\n[12] Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and\nKaren Egiazarian. Image Denoising by Sparse 3-D Transform-\nDomain Collaborative Filtering. TIP, 16(8):2080‚Äì2095, 2007.\n5\n[13] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang,\nNenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. CSWin\nTransformer: A General Vision Transformer Backbone with\nCross-Shaped Windows. arXiv preprint arXiv:2107.00652,\n2021. 2\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An Image is Worth 16x16 Words: Trans-\nformers for Image Recognition at Scale. In ICLR, 2020. 2,\n3\n[15] Shuhang Gu, Yawei Li, Luc Van Gool, and Radu Timofte.\nSelf-Guided Network for Fast Image Denoising. In ICCV,\n2019. 2\n[16] Bin He, Ce Wang, Boxin Shi, and Ling-Yu Duan. Mop Moire\nPatterns Using MopNet. In ICCV, 2019. 12, 13\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In CVPR,\n2016. 2\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian Error Linear\nUnits (GELUs). arXiv preprint arXiv:1606.08415, 2016. 4\n[19] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk\nChun, Junsuk Choe, and Seong Joon Oh. Rethinking Spatial\nDimensions of Vision Transformers. In ICCV, 2021. 2\n[20] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-Excitation\nNetworks. In CVPR, 2018. 2\n[21] Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and\nJianzhuang Liu. Neighbor2Neighbor: Self-Supervised De-\nnoising from Single Noisy Images. In CVPR, 2021. 4\n[22] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng,\nGang Yu, and Bin Fu. ShufÔ¨Çe Transformer: Rethinking\nSpatial ShufÔ¨Çe for Vision Transformer. arXiv preprint\narXiv:2106.03650, 2021. 2\n[23] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros.\nImage-to-Image Translation with Conditional Adversarial\nNetworks. In CVPR, 2017. 3\n[24] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. TransGAN:\nTwo Pure Transformers Can Make One Strong GAN, and\nThat Can Scale Up. arXiv preprint arXiv:2102.07074, 2021.\n2\n[25] Tero Karras, Samuli Laine, and Timo Aila. A Style-Based\nGenerator Architecture for Generative Adversarial Networks.\nIn CVPR, 2019. 5\n[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nageNet ClassiÔ¨Åcation with Deep Convolutional Neural Net-\nworks. In NeurIPS, 2012. 2\n[27] Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych,\nDmytro Mishkin, and Jiri Matas. DeblurGAN: Blind Motion\nDeblurring Using Conditional Adversarial Networks. ArXiv\ne-prints, 2017. 2, 6\n[28] Orest Kupyn, Tetiana Martyniuk, Junru Wu, and Zhangyang\nWang. DeblurGAN-v2: Deblurring (Orders-of-Magnitude)\nFaster and Better. In ICCV, 2019. 2, 6\n[29] Junyong Lee, Sungkil Lee, Sunghyun Cho, and Seungyong\nLee. Deep Defocus Map Estimation using Domain Adapta-\ntion. In CVPR, 2019. 7\n[30] Guanbin Li, Xiang He, Wei Zhang, Huiyou Chang, Le Dong,\nand Liang Lin. Non-locally Enhanced Encoder-Decoder Net-\nwork for Single Image De-raining. In ACMMM, 2018. 1\n[31] Siyuan Li, Iago Breno Araujo, Wenqi Ren, Zhangyang Wang,\nEric K Tokuda, Roberto Hirata Junior, Roberto Cesar-Junior,\nJiawan Zhang, Xiaojie Guo, and Xiaochun Cao. Single image\nderaining: A comprehensive benchmark analysis. In CVPR,\n2019. 2\n[32] Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, and Hongbin\nZha. Recurrent Squeeze-and-Excitation Context Aggregation\nNet for Single Image Deraining. In ECCV, 2018. 2, 7\n[33] Yu Li, Robby T. Tan, Xiaojie Guo, Jiangbo Lu, and Michael S.\nBrown. Rain Streak Removal Using Layer Priors. In CVPR,\n2016. 7\n[34] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. LocalViT: Bringing Locality to Vision Transform-\ners. arXiv preprint arXiv:2104.05707, 2021. 1, 3, 4\n9\n[35] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. SwinIR: Image Restoration\nUsing Swin Transformer. In ICCV Workshops, 2021. 2\n[36] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced Deep Residual Networks for\nSingle Image Super-Resolution. In CVPR Workshop, 2017. 2\n[37] Bolin Liu, Xiao Shu, and Xiaolin Wu. Demoir√©ing of Camera-\nCaptured Screen Images Using Deep Convolutional Neural\nNetwork. arXiv preprint arXiv:1804.03809, 2018. 2, 12\n[38] Ding Liu, Bihan Wen, Yuchen Fan, Chen Change Loy, and\nThomas S Huang. Non-Local Recurrent Network for Image\nRestoration. In NeurIPS, 2018. 1\n[39] Xing Liu, Masanori Suganuma, Zhun Sun, and Takayuki\nOkatani. Dual Residual Networks Leveraging the Potential of\nPaired Operations for Image Restoration. In CVPR, 2019. 2\n[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin Transformer: Hi-\nerarchical Vision Transformer using Shifted Windows. arXiv\npreprint arXiv:2103.14030, 2021. 2, 4, 12\n[41] Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay\nRegularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[42] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nMulti-Scale Convolutional Neural Network for Dynamic\nScene Deblurring. In CVPR, 2017. 2, 5, 6, 7, 8, 13\n[43] Tobias Plotz and Stefan Roth. Benchmarking Denoising\nAlgorithms with Real Photographs. In CVPR, 2017. 2, 5, 13\n[44] Kuldeep Purohit, Maitreya Suin, AN Rajagopalan, and\nVishnu Naresh Boddeti. Spatially-Adaptive Image Restora-\ntion using Distortion-Guided Networks. In ICCV, 2021. 1, 2,\n6, 7\n[45] Jaesung Rim, Haeyun Lee, Jucheol Won, and Sunghyun Cho.\nReal-World Blur Dataset for Learning and Benchmarking\nDeblurring Algorithms. In ECCV, 2020. 2, 6, 7, 13\n[46] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net:\nConvolutional Networks for Biomedical Image Segmentation.\nIn MICCAI. Springer, 2015. 1, 3, 12\n[47] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. MobileNetV2: Inverted\nResiduals and Linear Bottlenecks. In CVPR, 2018. 4\n[48] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nAttention with Relative Position Representations. arXiv\npreprint arXiv:1803.02155, 2018. 4\n[49] Ziyi Shen, Wenguan Wang, Jianbing Shen, Haibin Ling,\nTingfa Xu, and Ling Shao. Human-Aware Motion Deblurring.\nIn ICCV, 2019. 2, 6, 13\n[50] Jianping Shi, Li Xu, and Jiaya Jia. Just Noticeable Defocus\nBlur Detection and Estimation. In CVPR, 2015. 7\n[51] Hyeongseok Son, Junyong Lee, Sunghyun Cho, and Seungy-\nong Lee. Single Image Defocus Deblurring Using Kernel-\nSharing Parallel Atrous Convolutions. In ICCV, 2021. 2,\n7\n[52] Yujing Sun, Yizhou Yu, and Wenping Wang. Moir√© photo\nrestoration using multiresolution convolutional neural net-\nworks. TIP, 27(8):4160‚Äì4172, 2018. 2, 12, 13\n[53] Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Jiaya\nJia. Scale-recurrent Network for Deep Image Deblurring. In\nCVPR, 2018. 6\n[54] Chunwei Tian, Yong Xu, Lunke Fei, and Ke Yan. Deep\nLearning for Image Denoising: A Survey. In International\nConference on Genetic and Evolutionary Computing, 2018. 2\n[55] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki\nParmar, Blake Hechtman, and Jonathon Shlens. Scaling Local\nSelf-Attention for Parameter EfÔ¨Åcient Visual Backbones. In\nCVPR, 2021. 2\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention Is All You Need. In NeurIPS, 2017. 2,\n3, 5, 12\n[57] Hong Wang, Qi Xie, Qian Zhao, and Deyu Meng. A Model-\nDriven Deep Neural Network for Single Image Rain Removal.\nIn CVPR, 2020. 5, 7\n[58] Tianyu Wang, Xin Yang, Ke Xu, Shaozhe Chen, Qiang Zhang,\nand Rynson WH Lau. Spatial Attentive Single-Image Derain-\ning with a High Quality Real Rain Dataset. In CVPR, 2019.\n2, 7, 8, 13, 15, 16, 17\n[59] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid Vision Transformer: A Versatile Backbone for Dense\nPrediction Without Convolutions. In ICCV, 2021. 2\n[60] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local Neural Networks. In CVPR, 2018. 2\n[61] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image Quality Assessment:From Error Visibility\nto Structural Similarity. TIP, 13(4):600‚Äì612, 2004. 5\n[62] Zhihao Wang, Jian Chen, and Steven CH Hoi. Deep Learning\nfor Image Super-resolution: A Survey. TPAMI, 2020. 2\n[63] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang\nDai, Lu Yuan, and Lei Zhang. CvT: Introducing convolutions\nto vision transformers. arXiv preprint arXiv:2103.15808 ,\n2021. 3, 4, 7\n[64] Li Xu, Shicheng Zheng, and Jiaya Jia. Unnatural L0 Sparse\nRepresentation for Natural Image Deblurring. In CVPR, 2013.\n6\n[65] Rui Xu, Xiangyu Xu, Kai Chen, Bolei Zhou, and\nChen Change Loy. STransGAN: An Empirical Study on\nTransformer in GANs. arXiv preprint arXiv:2110.13107 ,\n2021. 2\n[66] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning Texture Transformer Network for Image\nSuper-Resolution. In CVPR, 2020. 2\n[67] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal Self-attention\nfor Local-Global Interactions in Vision Transformers. arXiv\npreprint arXiv:2107.00641, 2021. 2\n[68] Wenhan Yang, Robby T. Tan, Jiashi Feng, Zongming Guo,\nShuicheng Yan, and Jiaying Liu. Joint Rain Detection and\nRemoval from a Single Image with Contextualized Deep\nNetworks. TPAMI, 42(6):1377‚Äì1393, 2020. 7\n[69] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei\nYu, and Wei Wu. Incorporating Convolution Designs into\nVisual Transformers. arXiv preprint arXiv:2103.11816, 2021.\n1, 4\n[70] Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, and\nLei Zhang. Variational Denoising Network: Toward Blind\nNoise Modeling and Removal. In NeurIPS, 2019. 5\n10\n[71] Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng. Dual\nAdversarial Network: Toward Real-world Noise Removal and\nNoise Generation. In ECCV, 2020. 1, 2, 5, 12, 13\n[72] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. CycleISP: Real image restoration via improved data\nsynthesis. In CVPR, 2020. 5\n[73] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. Learning Enriched Features for Real Image Restoration\nand Enhancement. In ECCV, 2020. 1, 2, 3, 5, 13\n[74] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar\nHayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling\nShao. Multi-Stage Progressive Image Restoration. In CVPR,\n2021. 1, 2, 5, 6, 13\n[75] Hongguang Zhang, Yuchao Dai, Hongdong Li, and Piotr\nKoniusz. Deep Stacked Hierarchical Multi-patch Network for\nImage Deblurring. In CVPR, 2019. 2, 6, 13\n[76] Jiawei Zhang, Jinshan Pan, Jimmy Ren, Yibing Song, Linchao\nBao, Rynson WH Lau, and Ming-Hsuan Yang. Dynamic\nScene Deblurring Using Spatially Variant Recurrent Neural\nNetworks. In CVPR, 2018. 6\n[77] Kaihao Zhang, Wenhan Luo, Yiran Zhong, Lin Ma, Bjorn\nStenger, Wei Liu, and Hongdong Li. Deblurring by Realistic\nBlurring. In CVPR, 2020. 6\n[78] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and\nLei Zhang. Beyond a Gaussian Denoiser: Residual Learning\nof Deep CNN for Image Denoising. TIP, 26(7):3142‚Äì3155,\n2017. 2\n[79] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image Super-Resolution Using Very\nDeep Residual Channel Attention Networks. In ECCV, 2018.\n2\n[80] Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu.\nResidual Non-local Attention Networks for Image Restora-\ntion. In ICLR, 2019. 1, 2\n[81] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun\nFu. Residual Dense Network for Image Super-Resolution. In\nCVPR, 2018. 2\n[82] Zizhao Zhang, Han Zhang, Long Zhao, Ting Chen, and Tomas\nPÔ¨Åster. Aggregating Nested Transformers. In arXiv preprint\narXiv:2105.12723, 2021. 2\n[83] Long Zhao, Zizhao Zhang, Ting Chen, Dimitris N Metaxas,\nand Han Zhang. Improved Transformer for High-Resolution\nGANs. arXiv preprint arXiv:2106.07631, 2021. 2\n11\nA. Additional Ablation Study\nA.1. Is Window Shift Important\nTable 8 reports the results of whether to use the shifted\nwindow design [40] in Uformer. We observe that window\nshift brings an improvement of 0.01 dB for image denois-\ning. We use the window shift as the default setting in our\nexperiments.\nUformer-S PSNR ‚Üë\nw/o window shift 39.76\nw window shift 39.77\nTable 8. Effect of window shift.\nA.2. Variants of Skip-Connections\nTo investigate how to deliver the learned low-level fea-\ntures from the encoder to the decoder, considering the self-\nattention computing in Transformer, we present three dif-\nferent skip-connection schemes, including concatenation-\nbased skip-connection, cross-attention as skip-connection,\nand concatenation-based cross-attention as skip-connection.\nConcatenation-based Skip-connection (Concat-Skip).\nConcat-Skip is based on the widely-used skip-connection in\nUNet [9, 46, 71]. To build our network, Ô¨Årstly, we concate-\nnate the l-th stage Ô¨Çattened features El and each encoder\nstage with the features DK‚àíl+1 from the (K-l+1)-th de-\ncoder stage channel-wisely. Here, K is the number of the\nencoder/decoder stages. Then, we feed the concatenated\nfeatures to the W-MSA component of the Ô¨Årst LeWin Trans-\nformer block in the decoder stage, as shown in Figure 8(a).\nCross-attention as Skip-connection (Cross-Skip).Instead\nof directly concatenating features from the encoder and the\ndecoder, we design Cross-Skip inspired by the decoder struc-\nture in the language Transformer [56]. As shown in Fig-\nure 8(b), we Ô¨Årst add an additional attention module into the\nÔ¨Årst LeWin Transformer block in each decoder stage. The\nÔ¨Årst self-attention module in this block (the shaded one) is\nused to seek the self-similarity pixel-wisely from the decoder\nfeatures DK‚àíl+1, and the second attention module in this\nblock takes the features El from the encoder as the keys and\nvalues, and uses the features from the Ô¨Årst module as the\nqueries.\nConcatenation-based Cross-attention as Skip-\nconnection (ConcatCross-Skip). Combining above\ntwo variants, we also design another skip-connection. As\nillustrated in Figure 8(c), we concatenate the features El\nfrom the encoder and DK‚àíl+1 from the decoder as the keys\nand values, while the queries are only from the decoder.\nTable 9 compares the results of using different skip-\nconnections in our Uformer: concatenating features ( Con-\ncat), cross-attention ( Cross), and concatenating keys and\n(a) (b) (c)\nW-MSAùëÑ\nE!\nùê∑\"#!$%\nW-MSA\nùê∑\"#!$%\nùëÑ W-MSA\nùê∑\"#!$%\nùëÑùê∏! ùê∏!W-MSAùëÑùêæùëâ\nùêæùëâùêæ‚Ä≤ùëâ‚Ä≤ùêæùëâ\nùë™ ùë™\nFigure 8. Three skip-connection schemes: (a) Concat-Skip,\n(b) Cross-Skip, and (c) ConcatCross-Skip.\nGMACs # Param PSNR ‚Üë\nUformer-S-Concat 43.86G 20.63M 39.77\nUformer-S-Cross 44.78G 27.95M 39.75\nUformer-S-ConcatCross 42.75G 27.28M 39.73\nTable 9. Different skip-connections.\nvalues for cross-attention ( ConcatCross). For a fair com-\nparison, we increase the channels in Uformer-S from 32 to\n44 in variants Cross and ConcatCross. These three skip-\nconnections achieve similar results, and concatenating fea-\ntures gets slightly better performance. We adopt the feature\nconcatenation as the default setting in Uformer.\nB. Additional Experiment for Demoireing\nWe also conduct an experiment of moire pattern removal\non the TIP18 dataset [52]. As shown in Table 10, Uformer\noutperforms previous methods MopNet [16], MSNet [52],\nCFNet [37], UNet [46] by 1.53 dB, 2.29 dB, 3.19 dB, and\n2.79 dB, respectively. And in Figure 13, we show examples\nof visual comparisons with other methods. This experiment\nfurther demonstrates the superiority of Uformer.\nUNet CFNet MSNet MopNet Uformer-B[46] [37] [52] [16]\nPSNR‚Üë 26.49 26.09 26.99 27.75 29.28\nSSIM‚Üë 0.864 0.863 0.871 0.895 0.917\nTable 10. Results on the TIP18 dataset [52] for demoireing.\nC. Additional Experimental Settings for Differ-\nent Tasks\nDenoising. The training samples are randomly cropped from\nthe original images in SIDD [1] with size 128 √ó128, which\nis also the common training strategy for image denoising in\n12\nrecent works [9, 71, 73]. And the training process lasts for\n250 epochs with batch size 32. Then, the trained model is\nevaluated on the 256 √ó256 patches of SIDD and 512 √ó512\npatches of the DND test images [43], following [9, 73]. The\nresults on DND are online evaluated.\nMotion deblurring.Following previous methods [74, 75],\nwe train Uformer only on the GoPro dataset [42], and evalu-\nate it on the test set of GoPro, HIDE [49], and RealBlur-R/-\nJ [45]. The training patches are randomly cropped from the\ntraining set with size 256 √ó256. The batch size is set to 32.\nFor validation, we use the central crop with size 256 √ó256.\nThe number of training epochs is 3k. For evaluation, the\ntrained model is tested on the full-size test images.\nDefocus deblurring.Following the ofÔ¨Åcial patch segmen-\ntation algorithm [3] of DPD, we crop the training and val-\nidation samples to 60% overlapping 512 √ó512 patches to\ntrain the model. We also discard 30% of the patches that\nhave the lowest sharpness energy (by applying Sobel Ô¨Ålter to\nthe patches) as [3]. The whole training process lasts for 160\nepochs with batch size 4. For evaluation, the trained model\nis tested on the full-size test images.\nDeraining. We conduct deraining experiments on the SPAD\ndataset [58]. This dataset contains over 64k256√ó256 images\nfor training and 1k 512 √ó512 images for evaluation. We\ntrain Uformer on two GPUs, with mini-batches of size 16 on\nthe 256√ó256 samples. Since this dataset is large enough and\nthe training process converges fast, we just train Uformer\nfor 10 epochs in the experiment. Finally, we evaluate the\nperformance on the test images following the default setting\nin [58].\nDemoireing. We further validate the effectiveness of\nUformer on the TIP18 dataset [52] for demoireing. Since\nthe images in this dataset contain additional borders, fol-\nlowing [16], we crop the central regions with the ratio of\n[0.15,0.85] in all training/validation/testing splits and resize\nthem to 256 √ó256 for training and evaluation. Since this\ntask is sensitive to the down-sampling operation, we choose\nthe bilinear interpolation same as the previous work [16]1.\nThe training epochs are 250.\nD. More Visual Comparisons\nAs shown in Figures 9-13 in this supplementary materials,\nwe give more visual results of our Uformer and others on\nthe Ô¨Åve tasks (denoising, motion deblurring, defocus deblur-\nring, deraining, and demoireing) as the supplement of the\nvisualization in the main paper.\n1The dataset we used is also downloaded from the Github Page of [16].\n13\n0005-0019\nDANet/35.12dBVDNet/34.80dB CycleISP/34.74dB\nMIRNet/35.01dB TargetInput Uformer-B/35.32dBMPRNet/35.00dB\nNBNet/35.25dBInput/24.21dB\nRIDNet/33.78dB\n0008-0003\nDANet/37.60dBVDNet/37.32dB CycleISP/37.08dB\nMIRNet/37.35dB TargetInput Uformer-B/39.23dBMPRNet/37.39dB\nNBNet/37.83dBInput/24.21dB\nRIDNet/36.26dB\n0008-0022\nDANet/41.37dBVDNet/40.54dB CycleISP/40.40dB\nMIRNet/38.34dB TargetInput Uformer-B/41.42dBMPRNet/41.16dB\nNBNet/40.87dBInput/24.53dB\nRIDNet/38.34dB\n0008-0022\nDANet/35.76dBVDNet/35.46dB CycleISP/35.37dB\nMIRNet/35.73dB TargetInput Uformer-B/36.05dBMPRNet/35.45dB\nNBNet/35.73dBInput/18.01dB\nRIDNet/35.13dB\n0008-0022\nDANet/39.91dBVDNet/39.76dB CycleISP/39.91dB\nMIRNet/40.01dB TargetInput Uformer-B/40.38dBMPRNet/40.06dB\nNBNet/40.09dBInput/23.92dB\nRIDNet/39.30dB\nFigure 9. More visual results on the SIDD dataset [1] for image denoising. The PSNR value under each patch is computed on the\ncorresponding whole image.\n14\nInput / 19.45 dB\n SRN / 23.62 dB\n DBGAN / 23.56 dB\n DMPHN / 22.74 dB\nDeblurGAN-v2 / 21.54 dB\n MPRNet / 25.67 dB\n Uformer-B / 27.00 dB\n Target\nInput / 25.84 dB\n SRN / 31.21 dB\n DBGAN / 32.40 dB\n DMPHN / 31.74 dB\nDeblurGAN-v2 / 29.55 dB\n MPRNet / 32.55 dB\n Uformer-B / 33.77 dB\n Target\nInput / 21.13 dB\n SRN / 27.42 dB\n DBGAN / 29.21 dB\n DMPHN / 28.43 dB\nDeblurGAN-v2 / 25.42 dB\n MPRNet / 32.36 dB\n Uformer-B / 32.66 dB\n Target\nInput / 23.04 dB\n SRN / 27.14 dB\n DBGAN / 28.20 dB\n DMPHN / 28.05 dB\nDeblurGAN-v2 / 25.00 dB\n MPRNet / 29.13 dB\n Uformer-B / 30.65 dB\n Target\nFigure 10. More results on GoPro [58] for image motion deblurring. The PSNR value under each patch is computed on the corresponding\nwhole image.\n15\nInput / 28.44 dB\n DPDNet / 28.48 dB\n KPAC / 28.57 dB\n Uformer-B / 29.26 dB\n Target\nInput / 22.98 dB\n DPDNet / 26.94 dB\n KPAC / 26.88 dB\n Uformer-B / 27.81 dB\n Target\nInput / 24.36 dB\n DPDNet / 27.56 dB\n KPAC / 27.49 dB\n Uformer-B / 28.60 dB\n Target\nInput / 25.58 dB\n DPDNet / 29.13 dB\n KPAC / 28.99 dB\n Uformer-B / 29.30 dB\n Target\nFigure 11. More results on DPD [3] for image defocus deblurring. We report the performance of PSNR on the whole test image and show\nthe zoomed region only for visual comparison.\nInput / 29.29 dB\n RCDNet / 38.67 dB\n SPANet / 37.15 dB\n Uformer-B / 47.37 dB\n Target\nInput / 22.13 dB\n RCDNet / 30.00 dB\n SPANet / 27.18 dB\n Uformer-B / 37.12 dB\n Target\nInput / 26.50 dB\n RCDNet / 31.47 dB\n SPANet / 29.72 dB\n Uformer-B / 37.44 dB\n Target\nFigure 12. More results on SPAD [58] for image deraining.\n16\nInput / 19.47 dB\n UNet / 29.62 dB\n MopNet / 30.45 dB\n Uformer-B / 32.74 dB\n Target\nInput / 14.99 dB\n UNet / 26.78 dB\n MopNet /25.44 dB\n Uformer-B / 31.76 dB\n Target\nInput / 19.98 dB\n UNet / 26.27 dB\n MopNet / 29.41 dB\n Uformer-B / 30.79 dB\n Target\nInput / 17.15 dB\n UNet / 19.35 dB\n MopNet / 18.84 dB\n Uformer-B / 26.86 dB\n Target\nInput / 15.20 dB\n UNet / 28.17 dB\n MopNet / 29.09 dB\n Uformer-B / 30.63 dB\n Target\nFigure 13. Results on the TIP18 dataset [58] for image demoireing.\n17"
}