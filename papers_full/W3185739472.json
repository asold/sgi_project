{
  "title": "Audio Captioning Transformer",
  "url": "https://openalex.org/W3185739472",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221426056",
      "name": "Mei, Xinhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1790448053",
      "name": "Liu Xubo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2287249697",
      "name": "Huang, Qiushi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221426058",
      "name": "Plumbley, Mark D.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1970133991",
      "name": "Wang Wen-wu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3124149278",
    "https://openalex.org/W3094550259",
    "https://openalex.org/W3160577380",
    "https://openalex.org/W3112467147",
    "https://openalex.org/W3164279099",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W3015591594",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2982669287",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W3126565544",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2945761034",
    "https://openalex.org/W2964891022",
    "https://openalex.org/W2936774411",
    "https://openalex.org/W3088092535",
    "https://openalex.org/W2916103538",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3093937952",
    "https://openalex.org/W2964213897",
    "https://openalex.org/W3097791920"
  ],
  "abstract": "Audio captioning aims to automatically generate a natural language description of an audio clip. Most captioning models follow an encoder-decoder architecture, where the decoder predicts words based on the audio features extracted by the encoder. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are often used as the audio encoder. However, CNNs can be limited in modelling temporal relationships among the time frames in an audio signal, while RNNs can be limited in modelling the long-range dependencies among the time frames. In this paper, we propose an Audio Captioning Transformer (ACT), which is a full Transformer network based on an encoder-decoder architecture and is totally convolution-free. The proposed method has a better ability to model the global information within an audio signal as well as capture temporal relationships between audio events. We evaluate our model on AudioCaps, which is the largest audio captioning dataset publicly available. Our model shows competitive performance compared to other state-of-the-art approaches.",
  "full_text": "Detection and ClassiÔ¨Åcation of Acoustic Scenes and Events 2021 15‚Äì19 November 2021, Online\nAUDIO CAPTIONING TRANSFORMER\nXinhao Mei1, Xubo Liu1, Qiushi Huang2, Mark D. Plumbley1, Wenwu Wang1\n1 Centre for Vision, Speech and Signal Processing (CVSSP),\n2 Department of Computer Science,\nUniversity of Surrey, UK\nABSTRACT\nAudio captioning aims to automatically generate a natural language\ndescription of an audio clip. Most captioning models follow an\nencoder-decoder architecture, where the decoder predicts words\nbased on the audio features extracted by the encoder. Convolutional\nneural networks (CNNs) and recurrent neural networks (RNNs) are\noften used as the audio encoder. However, CNNs can be limited\nin modelling temporal relationships among the time frames in an\naudio signal, while RNNs can be limited in modelling the long-range\ndependencies among the time frames. In this paper, we propose an\nAudio Captioning Transformer (ACT), which is a full Transformer\nnetwork based on an encoder-decoder architecture and is totally\nconvolution-free. The proposed method has a better ability to model\nthe global information within an audio signal as well as capture\ntemporal relationships between audio events. We evaluate our model\non AudioCaps, which is the largest audio captioning dataset publicly\navailable. Our model shows competitive performance compared to\nother state-of-the-art approaches.\nIndex Terms‚Äî Audio captioning, Transformer, sequence-to-\nsequence model, cross-modal task\n1. INTRODUCTION\nAutomated audio captioning (AAC) is concerned with describing an\naudio clip using natural language and is a cross-modal translation\ntask at the intersection of audio processing and natural language\nprocessing. Generating a meaningful description for an audio clip\nnot only needs to determine what audio events are presented, but also\nneeds to capture and express their spatial-temporal relationships. Au-\ndio captioning is practically useful in applications such as assisting\nthe hearing-impaired to understand environmental sounds, retrieving\nmultimedia content, and analyzing sounds for security surveillance.\nUnlike image and video captioning, which have been studied in\ncomputer vision (CV) for a longer time, audio captioning is a task\ninvestigated only recently [1]. With the announcement of the AAC\ntask in DCASE 2020 and 2021, this topic has attracted increasing\nattention, and several methods have been proposed [ 2, 3, 4]. The\nAAC task is usually treated as a sequence-to-sequence problem,\nand existing methods are typically based on an encoder-decoder\narchitecture, where the decoder generates words according to the\naudio features extracted by the encoder. Early works often adopted\nan ‚ÄúRNN-RNN‚Äù architecture with an attention mechanism [ 1, 3].\nHowever, RNNs can be limited in modeling long-term temporal\ndependencies in an audio signal. Recently, CNNs have become a\ndominant approach in audio-related tasks (audio tagging and sound\nevent detection) [5], with many researchers using pre-trained CNNs\nas the audio encoder, which signiÔ¨Åcantly improved the performance\nin these systems [6]. More recently, inspired by the great success of\nthe Transformer model in natural language processing [7], the RNN\ndecoder has been replaced by a Transformer decoder in captioning\nmodels, and the ‚ÄúCNN+Transformer‚Äù architecture has been shown\nto achieve state-of-the-art performance in this area [8, 9].\nDescription of an audio signal needs to capture temporal-spatial\nrelationships between audio objects that may be far apart in time.\nHowever, convolution is a local operator and has limitations in mod-\nelling temporal information, especially with a long audio signal.\nThis can be alleviated by enlarging receptive Ô¨Åelds with deeper con-\nvolutional layers. However, such deep CNNs can be hard to train\nand can lead to over-Ô¨Åtting. To address this problem, we propose\nan Audio Captioning Transformer (ACT), a convolution-free Trans-\nformer network based on the self-attention mechanism. We use\nlog mel-spectrograms as input and split the mel-spectrograms into\nsmaller non-overlapping patches along the time axis. By adopting\nthe self-attention mechanism, each patch can attend to all the other\npatches at each layer of the encoder, which can model global long-\nrange dependencies among the small mel-spectrogram patches from\nthe beginning. Without the need for down-sampling, the features ex-\ntracted by Transformer are Ô¨Åne-grained, which can contain detailed\nlocal audio topics.\nThe Transformer usually requires more training data than CNNs\n[10]. However, the amount of data currently available for audio\ncaptioning is relatively small. To address this issue, the ACT encoder\nis Ô¨Årstly pre-trained on AudioSet dataset [11] as an audio tagging task\nin order to improve its generalization ability. A class token designed\nto model the global information of an audio clip is appended at the\nbeginning of each patch sequence and is used to output audio tagging\nresults. As a result, when generating words, the decoder can attend\nto local and global information of an audio clip simultaneously. The\nproposed ACT model is evaluated on the AudioCaps dataset [3] and\nshows competitive performance as compared to other state-of-the-art\nmethods.\nThe remaining sections of this paper are organised as follows.\nIn Section 2, we introduce the related work. The proposed model is\ndescribed in detail in Section 3. Experimental settings are shown in\nSection 4. Results are discussed in Section 5. Finally, we conclude\nour work in Section 6.\n2. RELATED WORK\nPrevious work proposed in audio captioning has been based on deep\nlearning methods with an encoder-decoder architecture. Drossos\net al. [1] proposed the Ô¨Årst approach to AAC using an RNN-based\nencoder-decoder architecture with an alignment model in between.\nTo control the information contained in the output text, Ikawa and\nKashino [4] introduced a conditional parameter called ‚ÄúspeciÔ¨Åcity‚Äù\nto guide the caption generation. With the release of two freely\narXiv:2107.09817v1  [eess.AS]  21 Jul 2021\nDetection and ClassiÔ¨Åcation of Acoustic Scenes and Events 2021 15‚Äì19 November 2021, Online\navailable datasets AudioCaps [3] and Clotho [12], AAC has attracted\nincreasing attention and more approaches have been proposed. Kim\net al. [3] proposed a model with a top-down multi-scale encoder and\naligned semantic attention, which enabled the joint use of multi-level\nfeatures and semantic attributes. As CNNs have achieved state-of-\nthe-art performance in audio tagging and sound event detection tasks\n[5], some researchers replaced the RNN encoder with CNNs, which\nbrings signiÔ¨Åcant performance gains [8, 6]. Recently, Transformer\nhas been introduced as the language decoder with a powerful ability\nin natural language generation tasks [8, 13, 14]. Takeuchi et al. [15]\nformulated audio captioning as a multi-task learning problem, where\nthey proposed keywords estimation and sentence length estimation\nto avoid the indeterminacy of word selection. Koizumi et al. [16]\nutilized a pre-trained large-scale language model GPT-2 [17] with\naudio-based similar caption retrieval to guide the caption generation.\nReinforcement learning was used to optimize the audio captioning\nmodels with non-differentiable evaluation metrics [18].\nThe Transformer was originally proposed for machine transla-\ntion and has now become the dominant approach in natural language\nprocessing tasks [7]. Recently, many researchers adopted the Trans-\nformer for computer vision tasks which was shown to approach or\noutperform the state-of-the-art CNNs-based systems in image recog-\nnition. Dosovitskiy et al. [10] proposed a Vision Transformer (ViT)\nwhich was based purely on the attention mechanism, i.e. without us-\ning convolution kernels, and applied directly to sequences of image\npatches for the image classiÔ¨Åcation task. However, a large amount\nof data are required for pre-training the Transformer models, which\nlimits their adoption. To address this problem, Touvron et al. [19]\nintroduced Data-efÔ¨Åcient image Transformers (DeiT) using a data\nefÔ¨Åciency training and distillation strategy. Based on ViT and DeiT,\nLiu et al. [20] proposed a CaPtion TransformeR (CPTR) for image\ncaptioning. As the Transformer is designed to deal with sequential\ndata, we argue that the Transformer can be adapted for audio sig-\nnals, and the self-attention mechanism makes it more suitable to\ncapture temporal relationships between audio features and to model\nthe global information. Inspired by these ViT-related works, we pro-\npose the Audio Captioning Transformer (ACT) for audio captioning,\nwhich, to our knowledge, has not been done in the literature.\n3. PROPOSED METHOD\nFig. 1 shows the proposed Audio Captioning Transformer model,\nwhich is based on the traditional sequence-to-sequence architecture\nand is convolution-free. The model takes the log mel-spectrogram\nof an audio clip as input and outputs the posterior probabilities of\nthe predicted words.\n3.1. Encoder\nLet X ‚ààRT√óF denote the log mel-spectrogram of an audio clip,\nwhere T is the number of time frames and F is the number of mel\nbins. The log mel-spectrogram is Ô¨Årst split into N non-overlapping\nsmall patches XN = {x1,...,x n}along the time axis with size\nof t√óF where N = T/t and t is the number of time frames\nof each patch. Then each mel-spectrogam patch is Ô¨Çattened to a\n1D embedding and projected to a latent space through a learnable\nmatrix We ‚ààR(t√óF)√ód, where d is the dimension of the latent\nembedding. In line with ViT and DeiT, a global learnable class token\nXcls ‚ààR1√ód is appended to the beginning of the patch sequences,\nwhich contains the global information for the audio clip. As the\nself-attention mechanism cannot capture position information [7], a\n‚Ä¶\n<sos> a woman talks nearby as water pours\nLinear projection of flatten patches\n*\nMasked Multi-Head Attention\nLayer Norm\nMulti-Head Attention\nLayer Norm\nFeed ForwardLayer Norm\nLinear & Softmax\nMulti-Head AttentionLayer Norm\nFeed ForwardLayer NormùëÅ!√ó\nPositional embedding\nPositional embedding\nLog Mel-Spectrogram\nùëÅ\"√ó\nWord Embedding Layer\n*: CLS Token\na woman talks nearby as water pours <eos>\nInput Audio Waveform\nLog Mel-Spectrogram\nAudio Features\nNon-overlapping Mel-Spectrogram patches\nLinear projection of flatten patches\nFigure 1: System overview of Audio Captioning Transformer, the\nencoder is on the left side while the decoder on the right side.\ntrainable positional embedding Xpos ‚ààR(T+1)√ód is added to each\npatch embedding. Mathematically, the Ô¨Ånal input representation is\ngiven by\nXe = [Xcls + WeX] +Xpos (1)\nThe ACT encoder consists of Ne stacked identical layers. Each\nlayer contains two sub-layers, a multi-head self-attention layer and\na position-wise fully-connected feed-forward layer. In the self-\nattention sub-layer, the input is Ô¨Årst transformed into queryQ, key K\nand value V through matrix multiplication with three learnable ma-\ntrices WQ,WK,WV ‚ààRd√ódk , where dk is the dimension of each\nattention head. Then the scaled dot-product attention is computed as\nAttn(Q,K,V ) = Softmax(QKT\n‚àödk\n)V (2)\nEach self-attention layer contains hattention heads which extends\nthe model‚Äôs ability to attend to different positions and creates multi-\nple representation subspaces [7]. The outputs of heads are then ag-\ngregated through a linear transformation matrix Wo ‚ààR(h√ódk)√ódk ,\nwhich can be formulated as\nMultiHead(Q,K,V ) = Concat(head1,..., headh)Wo (3)\nThe feed-forward network contains two linear layers with GLEU\nactivation function and dropout applied between them. Layer normal-\nization is applied before each sub-layer and a residual connection is\nemployed around each of them, such that the output of each sub-layer\nis given by\nXout = Xin + Sublayer(LayerNorm(Xin)) (4)\nIn order to make use of pre-trained models, the encoder architecture\nis the same as ViT and DeiT containing 12 encoder blocks and 12\nheads with an embedding dimension of 768.\n3.2. Decoder\nThe ACT decoder contains three parts: a word embedding layer, a\nTransformer decoder block, and a linear layer. Each input word is\nDetection and ClassiÔ¨Åcation of Acoustic Scenes and Events 2021 15‚Äì19 November 2021, Online\nModel embedding dim # layers ( Nd) # heads\nACT s 512 2 4\nACT m 512 4 8\nACT l 512 6 8\nTable 1: Variants of the proposed ACT decoder.\nembedded through the word embedding layer into a Ô¨Åxed dimension\nword vector and then fed into the Transformer decoder block. The\nword vectors are pre-trained by a Word2Vec model on all caption\ncorpus [21].\nThe Transformer decoder consists ofNd identical stacked layers.\nThere are two main differences compared to the ACT encoder block.\nFirst, the Ô¨Årst self-attention sub-layer in the decoder is a masked\nself-attention because the caption generating process is causal and\nauto-regressive. Second, there is a new cross multi-head attention\nsub-layer between self-attention sub-layer and feed-forward sub-\nlayer, which allows every position in the decoder to attend over all\npositions in the audio features extracted by the encoder [ 7]. The\noutput of the decoder module is fed through a Ô¨Ånal linear layer with\nsoftmax activation function to output a probability distribution over\nthe vocabulary.\nThe training objective of the model is to minimize the cross-\nentropy (CE) loss\nLCE(Œ∏) =‚àí1\nT\nT‚àë\nt=1\nlog p(yt|y1:t‚àí1,Œ∏) (5)\nwhere yt is the ground-truth word at time step tand Œ∏are the model\nparameters. The ‚ÄúTeacher forcing‚Äù strategy is used during training,\ni.e. each word to be predicted is conditioned on previous ground-\ntruth words. We experiment with three models, which share the\nsame encoder architecture described in Section 3.2 but have different\nnumber of layers and heads in the decoder. Table 1 summarizes the\nparameters in the decoder of these models.\n4. EXPERIMENTS\n4.1. Dataset\n4.1.1. AudioSet\nAudioSet is a large-scale audio dataset with an ontology of 527 sound\nclasses [11]. AudioSet contains more than 2 million 10-second audio\nclips extracted from YouTube videos. As some audio clips are no\nlonger downloadable, there are 1 934 187and 18 887audio clips in\nour training and evaluation set, respectively. Each audio clip can\nhave one or more labels for their presented audio events.\n4.1.2. AudioCaps\nAudioCaps is the largest audio captioning dataset currently available\nwith around 50k audio clips sourced from AudioSet [3]. AudioCaps\nis divided into three splits. Each audio clip in the training set contains\none human-annotated caption, while each contains Ô¨Åve captions in\nthe validation and test set.\n4.2. Data pre-processing\nAll audio clips in these two datasets are converted to 32k Hz and\npadded to 10-second long. Log mel-spectrograms extracted using\na 1024-points Hanning window with 50% overlap and 64 mel bins\nare used as the input features. Each log mel-spectrogram is split into\n125 non-overlap small patches with the size of 64 √ó4 along the time\naxis. SpecAugment [ 22] is applied to augment the input features\nduring training.\nCaptions are tokenized and transformed to lower case with punc-\ntuation removed. To indicate the start and end of each caption, two\nspecial tokens ‚Äú<sos>‚Äù and ‚Äú<eos>‚Äù are padded. The vocabu-\nlary of AudioCaps contains 5277 distinct words.\n4.3. Audio tagging pre-training\nAs proved in previous works, Transformer requires more training\ndata to achieve competitive performance with CNNs [10]. However,\nthe amount of training data in audio processing area is much less\nthan that in computer vision. Cross-modal transfer learning from\nImageNet pre-trained models to audio-related tasks proves to be\neffective [23]. Thus we make use of pre-trained DeiT models for\nimage classiÔ¨Åcation to initialize the parameters in ACT encoder\n[10, 19]. As images have three channels and spectrograms just have\none channel, we take the average of the weights from the patch\nembedding layer in DeiT in order to adapt it for spectrogram.\nAs pre-trained audio neural networks (PANNs) proved to per-\nform well in audio captioning [ 9], we pre-train ACT encoder on\nAudioSet as an audio tagging task in order to solve the data scarcity\nproblem and learn more generalized audio patterns. Audio tagging\nis a multi-classiÔ¨Åcation task of predicting the presence or absence\nof sound classes within an audio clip [24]. The class token output\nfrom the encoder is fed through a linear layer with sigmoid activa-\ntion function to output the audio events probabilities. The model is\ntrained to minimize the binary cross-entropy loss between the output\nof the model and the true label\nLBCE(Œ∏) =‚àí\nN‚àë\nn=1\n(yn ¬∑ln f(xn) + (1‚àíyn) ¬∑ln(1 ‚àíf(xn)) (6)\nwhere xn is the n-th audio clip in AudioSet and N is the number\nof training samples. f(xn) ‚àà[0,1]K is the output of the model\nand yn ‚àà{0,1}K is the true label where Kis the number of sound\nclasses. The ACT encoder is pre-trained for 20 epochs with batch\nsize of 128 and learning rate of 1 √ó10‚àí4, which achieves a mean\naverage precision (mAP) of 0.43 on the evaluation set of AudioSet\ndataset.\n4.4. Experimental setups\nWe train the proposed model for 30 epochs using Adam optimizer\n[25] and a batch size of 32. The learning rate is linearly increased\nto 1 √ó10‚àí4 in the Ô¨Årst Ô¨Åve epochs using warm-up, which is then\nmultiplied by 0.1 every 10 epochs. To mitigate over-Ô¨Åtting prob-\nlem, dropout with rate of 0.2 is applied in the whole model. Label\nsmoothing [26] with a smoothing factor of 0.1 is used to avoid over-\nconÔ¨Ådent prediction. We use beam search with a beam size up to 5\nto improve the decoding performance during inference stage.\n4.5. Evaluation metrics\nIn line with previous works, we evaluate our methods using ma-\nchine translation and captioning metrics [ 13]. BLEU n, ROUGEl\nand METEOR are machine translation metrics. BLEUn is a modi-\nÔ¨Åed precision metric with a sentence-brevity penalty, calculated as a\nweighted geometric mean over different length n-grams. ROUGEl\nDetection and ClassiÔ¨Åcation of Acoustic Scenes and Events 2021 15‚Äì19 November 2021, Online\nModel BLEU 1 BLEU2 BLEU3 BLEU4 ROUGEL METERO CIDEr SPICE SPIDEr\nACT s DeiT AudioSet 0.643 0.483 0.352 0.249 0.469 0.218 0.669 0.160 0.415\nACT m DeiT AudioSet 0.653 0.495 0.363 0.259 0.471 0.222 0.663 0.163 0.413\nACT l DeiT AudioSet 0.647 0.488 0.356 0.252 0.468 0.222 0.679 0.160 0.420\nACT m scratch 0.567 0.411 0.285 0.191 0.417 0.187 0.501 0.127 0.314\nACT m DeiT 0.606 0.445 0.319 0.224 0.445 0.207 0.586 0.147 0.367\nRNN+RNN [3] 0.614 0.446 0.317 0.219 0.450 0.203 0.593 0.144 0.369\nCNN+RNN [6] 0.655 0.476 0.335 0.231 0.467 0.229 0.660 0.168 0.414\nCNN+Transformer [9] 0.641 0.479 0.344 0.236 0.469 0.221 0.693 0.159 0.426\nCNN+Transformer scratch [9] 0.610 0.461 0.334 0.234 0.455 0.206 0.629 0.144 0.386\nTable 2: Scores of the ACT model on the AudioCaps test set. DeiT: the ACT encoder is initialized with the parameters in DeiT, AudioSet: the\nACT encoder is pre-trained on AudioSet.\ncalculates F-measures by counting the longest common subsequence.\nMETEOR evaluates a caption by computing a harmonic mean of\nprecision and recall based on explicit word-to-word matches be-\ntween the caption and given references. Captioning metrics contain\nCIDEr, SPICE and SPIDEr. CIDEr calculates the cosine similar-\nity between term frequency inverse document frequency (TF-IDF)\nweighted n-grams. SPICE creates scene graphs for captions and\ncalculates F-score based on tuples in the scene graphs. SPIDE ris\nthe average of SPICE and CIDEr and is selected as the ofÔ¨Åcial rank-\ning metric in DCASE challenge, the SPICE score ensures captions\nare semantically faithful to the audio content, while CIDE r score\nensures captions are syntactically Ô¨Çuent.\n5. RESULTS\n5.1. Performance comparison\nTable 2 presents the results on AudioCaps test set. We compare the\nproposed ACT model with three representative audio captioning mod-\nels, ‚ÄúRNN+RNN‚Äù [3], ‚ÄúCNN+RNN‚Äù [6] and ‚ÄúCNN+Transformer‚Äù\n[9]. In these models, CNNs are all pre-trained on upstream audio-\nrelated tasks. As can be seen in Table 2 that the ACT model outper-\nforms ‚ÄúRNN+RNN‚Äù model substantially in all evaluation metrics\nand achieves slightly higher scores than ‚ÄúCNN+RNN‚Äù model in most\nmetrics. Compared with the state-of-the-art ‚ÄúCNN+Transformer‚Äù ap-\nproach, ACT model outperforms it in machine translation metrics\nbut gives slightly lower scores in CIDE r. As machine translation\nmetrics are based mostly on n-grams, these results show that the\nACT model has better ability in generating words accurately. In\naddition, training an ACT model is faster than ‚ÄúCNN+Transformer‚Äù\narchitecture, where the former takes less than Ô¨Åve minutes for one\nepoch and ‚ÄúCNN+Transformer‚Äù needs seven minutes in our experi-\nments. In summary, the ACT model shows competitive performance\nas compared to other state-of-the-art approaches, and it is simple as\nit based only on the self-attention mechanism.\n5.2. Ablation studies\nThe ablation studies are carried out to investigate the effectiveness\nof the pre-trained encoder and the inÔ¨Çuence of the hyper-parameters\nin the decoder. From the experimental results, we can see that pre-\ntraining the ACT encoder can boost the performance signiÔ¨Åcantly.\nEven only using the pre-trained DeiT model, which is originally\ntrained for image classiÔ¨Åcation task, can bring signiÔ¨Åcant perfor-\nmance gains in all the evaluation metrics. Pre-training on AudioSet\nas an audio tagging task further improves the system to approach the\nstate-of-the-art performance. We also compare the ACT model with\nthe ‚ÄúCNN+Transformer‚Äù model both trained from scratch, the results\nshow that the ACT model performs worse than ‚ÄúCNN+Transformer‚Äù\nwithout encoder pre-training. These results suggest that pre-training\nthe ACT encoder with a large dataset is important, and prove that\nTransformer network needs more training data than CNNs to achieve\ncompetitive performance.\nWe perform experiments on the three models with different\nnumbers of layers and heads in the decoder. From the observations,\nthe ACT model is slightly sensitive to the choice of hyper-parameters\nin the decoder. These three models achieve similar performance,\namong which ACT m with four decoder layers performs better in\nmachine translation metrics, while ACT l achieves higher CIDEr\nand SPIDEr scores. The ACT model only needs shallow Transformer\ndecoder layers compared to machine translation models in natural\nlanguage tasks which typically contain 12 Transformer decoder\nlayers [7]. There might be two reasons. First, the amount of training\ndata in audio captioning is far less than data in natural language\nprocessing tasks. Second, the length of the audio captions are usually\nshorter than sentences in the natural language tasks.\n6. CONCLUSION\nWe have presented a novel audio captioning model, Audio Caption-\ning Transformer (ACT), which is a full Transformer model based\non the self-attention mechanism. The encoder of the proposed ACT\nmodel can model the global and Ô¨Åne-grained information within an\naudio signal simultaneously, and has better ability to capture tem-\nporal relationships between audio events than CNNs. Experimental\nresults show that the ACT model can outperform other state-of-\nthe-art audio captioning systems in most metrics. Further research\nshould be carried out to adapt the ACT model for audio clips of\nvaried lengths.\n7. ACKNOWLEDGMENT\nThis work is partly supported by grant EP/T019751/1 from the Engi-\nneering and Physical Sciences Research Council (EPSRC), a Newton\nInstitutional Links Award from the British Council, titled ‚ÄúAuto-\nmated Captioning of Image and Audio for Visually and Hearing\nImpaired‚Äù (Grant number 623805725) and a Research Scholarship\nfrom the China Scholarship Council (CSC) No. 202006470010.\nDetection and ClassiÔ¨Åcation of Acoustic Scenes and Events 2021 15‚Äì19 November 2021, Online\nReferences\n[1] K. Drossos, S. Adavanne, and T. Virtanen, ‚ÄúAutomated audio\ncaptioning with recurrent neural networks,‚Äù in IEEE Workshop\non Applications of Signal Processing to Audio and Acoustics\n(WASPAA). IEEE, 2017, pp. 374‚Äì378.\n[2] M. Wu, H. Dinkel, and K. Yu, ‚ÄúAudio caption: Listen and tell,‚Äù\nin IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2019, pp. 830‚Äì834.\n[3] C. D. Kim, B. Kim, H. Lee, and G. Kim, ‚ÄúAudiocaps: Gener-\nating captions for audios in the wild,‚Äù in Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, 2019, pp. 119‚Äì132.\n[4] S. Ikawa and K. Kashino, ‚ÄúNeural audio captioning based on\nconditional sequence-to-sequence model,‚Äù Proceedings of the\nDetection and ClassiÔ¨Åcation of Acoustic Scenes and Events\nWorkshop, 2019.\n[5] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D.\nPlumbley, ‚ÄúPanns: Large-scale pretrained audio neural net-\nworks for audio pattern recognition,‚ÄùIEEE/ACM Transactions\non Audio, Speech, and Language Processing, vol. 28, pp. 2880‚Äì\n2894, 2020.\n[6] X. Xu, H. Dinkel, M. Wu, Z. Xie, and K. Yu, ‚ÄúInvestigating\nlocal and global information for automated audio captioning\nwith transfer learning,‚Äù inIEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE,\n2021, pp. 905‚Äì909.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, ≈Å. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù\nin Advances in neural information processing systems, 2017,\npp. 5998‚Äì6008.\n[8] K. Chen, Y . Wu, Z. Wang, X. Zhang, F. Nian, S. Li, and\nX. Shao, ‚ÄúAudio captioning based on transformer and pre-\ntrained cnn,‚Äù inProceedings of the Detection and ClassiÔ¨Åcation\nof Acoustic Scenes and Events Workshop, 2020, pp. 21‚Äì25.\n[9] X. Mei, Q. Huang, X. Liu, G. Chen, J. Wu, Y . Wu, J. Zhao,\nS. Li, T. Ko, H. L. Tang, X. Shao, M. D. Plumbley, and\nW. Wang, ‚ÄúAn encoder-decoder based audio captioning system\nwith transfer and reinforcement learning for DCASE challenge\n2021 task 6,‚Äù DCASE2021 Challenge, Tech. Rep., July 2021.\n[10] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold,\nS. Gelly et al. , ‚ÄúAn image is worth 16x16 words: Trans-\nformers for image recognition at scale,‚Äù arXiv preprint\narXiv:2010.11929, 2020.\n[11] J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen,\nW. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, ‚ÄúAudio\nset: An ontology and human-labeled dataset for audio events,‚Äù\nin IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), New Orleans, LA, 2017.\n[12] K. Drossos, S. Lipping, and T. Virtanen, ‚ÄúClotho: An au-\ndio captioning dataset,‚Äù inIEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE,\n2020, pp. 736‚Äì740.\n[13] A. Tran, K. Drossos, and T. Virtanen, ‚ÄúWavetransformer: A\nnovel architecture for audio captioning based on learning\ntemporal and time-frequency information,‚Äù arXiv preprint\narXiv:2010.11098, 2020.\n[14] Y . Koizumi, R. Masumura, K. Nishida, M. Yasuda, and S. Saito,\n‚ÄúA transformer-based audio captioning model with keyword\nestimation,‚Äù arXiv preprint arXiv:2007.00222, 2020.\n[15] D. Takeuchi, Y . Koizumi, Y . Ohishi, N. Harada, and K. Kashino,\n‚ÄúEffects of word-frequency based pre-and post-processings for\naudio captioning,‚Äù arXiv preprint arXiv:2009.11436, 2020.\n[16] Y . Koizumi, Y . Ohishi, D. Niizumi, D. Takeuchi, and M. Ya-\nsuda, ‚ÄúAudio captioning using pre-trained large-scale language\nmodel guided by audio-based similar caption retrieval,‚ÄùarXiv\npreprint arXiv:2012.07331, 2020.\n[17] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever\net al., ‚ÄúLanguage models are unsupervised multitask learners,‚Äù\nOpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[18] X. Xu, H. Dinkel, M. Wu, and K. Yu, ‚ÄúA crnn-gru based\nreinforcement learning approach to audio captioning,‚Äù inPro-\nceedings of the Detection and ClassiÔ¨Åcation of Acoustic Scenes\nand Events Workshop, 2020, pp. 225‚Äì229.\n[19] H. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles,\nand H. J¬¥egou, ‚ÄúTraining data-efÔ¨Åcient image transformers &\ndistillation through attention,‚Äù in International Conference on\nMachine Learning. PMLR, 2021, pp. 10 347‚Äì10 357.\n[20] W. Liu, S. Chen, L. Guo, X. Zhu, and J. Liu, ‚ÄúCptr: Full\ntransformer network for image captioning,‚Äù arXiv preprint\narXiv:2101.10804, 2021.\n[21] T. Mikolov, K. Chen, G. Corrado, and J. Dean, ‚ÄúEfÔ¨Åcient\nestimation of word representations in vector space,‚Äù arXiv\npreprint arXiv:1301.3781, 2013.\n[22] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V . Le, ‚ÄúSpecaugment: A simple data augmenta-\ntion method for automatic speech recognition,‚Äù arXiv preprint\narXiv:1904.08779, 2019.\n[23] Y . Gong, Y .-A. Chung, and J. Glass, ‚ÄúPsla: Improving audio\nevent classiÔ¨Åcation with pretraining, sampling, labeling, and\naggregation,‚ÄùarXiv preprint arXiv:2102.01243, 2021.\n[24] Q. Kong, C. Yu, Y . Xu, T. Iqbal, W. Wang, and M. D. Plumb-\nley, ‚ÄúWeakly labelled audioset tagging with attention neural\nnetworks,‚Äù IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 27, no. 11, pp. 1791‚Äì1802, 2019.\n[25] D. P. Kingma and J. Ba, ‚ÄúAdam: A method for stochastic\noptimization,‚Äù arXiv preprint arXiv:1412.6980, 2014.\n[26] C. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna,\n‚ÄúRethinking the inception architecture for computer vision,‚Äù in\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2016, pp. 2818‚Äì2826.",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9229846000671387
    },
    {
      "name": "Computer science",
      "score": 0.8357791900634766
    },
    {
      "name": "Transformer",
      "score": 0.7385580539703369
    },
    {
      "name": "Encoder",
      "score": 0.6884575486183167
    },
    {
      "name": "Speech recognition",
      "score": 0.5911129713058472
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5069263577461243
    },
    {
      "name": "Recurrent neural network",
      "score": 0.5009498596191406
    },
    {
      "name": "Architecture",
      "score": 0.49707749485969543
    },
    {
      "name": "Audio signal",
      "score": 0.4878580570220947
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4343804717063904
    },
    {
      "name": "Language model",
      "score": 0.42982012033462524
    },
    {
      "name": "Speech coding",
      "score": 0.28902512788772583
    },
    {
      "name": "Artificial neural network",
      "score": 0.2860516309738159
    },
    {
      "name": "Image (mathematics)",
      "score": 0.07467448711395264
    },
    {
      "name": "Engineering",
      "score": 0.07210338115692139
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 32
}