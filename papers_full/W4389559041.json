{
    "title": "Learning to Make Rare and Complex Diagnoses With Generative AI Assistance: Qualitative Study of Popular Large Language Models",
    "url": "https://openalex.org/W4389559041",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A3181242783",
            "name": "Tassallah Abdullahi",
            "affiliations": [
                "Brown University"
            ]
        },
        {
            "id": "https://openalex.org/A2186720768",
            "name": "Ritambhara Singh",
            "affiliations": [
                "Brown University"
            ]
        },
        {
            "id": "https://openalex.org/A1992666041",
            "name": "Carsten Eickhoff",
            "affiliations": [
                "University of Tübingen"
            ]
        },
        {
            "id": "https://openalex.org/A3181242783",
            "name": "Tassallah Abdullahi",
            "affiliations": [
                "Brown University"
            ]
        },
        {
            "id": "https://openalex.org/A2186720768",
            "name": "Ritambhara Singh",
            "affiliations": [
                "Brown University"
            ]
        },
        {
            "id": "https://openalex.org/A1992666041",
            "name": "Carsten Eickhoff",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4384664803",
        "https://openalex.org/W4319662928",
        "https://openalex.org/W4367051110",
        "https://openalex.org/W4368367885",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4319301505",
        "https://openalex.org/W3012641931",
        "https://openalex.org/W2969358130",
        "https://openalex.org/W3202960634",
        "https://openalex.org/W111989437",
        "https://openalex.org/W3193458379",
        "https://openalex.org/W3016389613",
        "https://openalex.org/W4319663047",
        "https://openalex.org/W4366769280",
        "https://openalex.org/W4323050332",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W3153672153",
        "https://openalex.org/W4382245789",
        "https://openalex.org/W4385569968",
        "https://openalex.org/W3204906989",
        "https://openalex.org/W3004612364",
        "https://openalex.org/W2888625704",
        "https://openalex.org/W4394886691"
    ],
    "abstract": "Background Patients with rare and complex diseases often experience delayed diagnoses and misdiagnoses because comprehensive knowledge about these diseases is limited to only a few medical experts. In this context, large language models (LLMs) have emerged as powerful knowledge aggregation tools with applications in clinical decision support and education domains. Objective This study aims to explore the potential of 3 popular LLMs, namely Bard (Google LLC), ChatGPT-3.5 (OpenAI), and GPT-4 (OpenAI), in medical education to enhance the diagnosis of rare and complex diseases while investigating the impact of prompt engineering on their performance. Methods We conducted experiments on publicly available complex and rare cases to achieve these objectives. We implemented various prompt strategies to evaluate the performance of these models using both open-ended and multiple-choice prompts. In addition, we used a majority voting strategy to leverage diverse reasoning paths within language models, aiming to enhance their reliability. Furthermore, we compared their performance with the performance of human respondents and MedAlpaca, a generative LLM specifically designed for medical tasks. Results Notably, all LLMs outperformed the average human consensus and MedAlpaca, with a minimum margin of 5% and 13%, respectively, across all 30 cases from the diagnostic case challenge collection. On the frequently misdiagnosed cases category, Bard tied with MedAlpaca but surpassed the human average consensus by 14%, whereas GPT-4 and ChatGPT-3.5 outperformed MedAlpaca and the human respondents on the moderately often misdiagnosed cases category with minimum accuracy scores of 28% and 11%, respectively. The majority voting strategy, particularly with GPT-4, demonstrated the highest overall score across all cases from the diagnostic complex case collection, surpassing that of other LLMs. On the Medical Information Mart for Intensive Care-III data sets, Bard and GPT-4 achieved the highest diagnostic accuracy scores, with multiple-choice prompts scoring 93%, whereas ChatGPT-3.5 and MedAlpaca scored 73% and 47%, respectively. Furthermore, our results demonstrate that there is no one-size-fits-all prompting approach for improving the performance of LLMs and that a single strategy does not universally apply to all LLMs. Conclusions Our findings shed light on the diagnostic capabilities of LLMs and the challenges associated with identifying an optimal prompting strategy that aligns with each language model’s characteristics and specific task requirements. The significance of prompt engineering is highlighted, providing valuable insights for researchers and practitioners who use these language models for medical training. Furthermore, this study represents a crucial step toward understanding how LLMs can enhance diagnostic reasoning in rare and complex medical cases, paving the way for developing effective educational tools and accurate diagnostic aids to improve patient care and outcomes.",
    "full_text": "Original Paper\nLearning to Make Rare and Complex Diagnoses With Generative\nAI Assistance: Qualitative Study of Popular Large Language\nModels\nTassallah Abdullahi1, MSc; Ritambhara Singh1,2, PhD; Carsten Eickhoff3, PhD\n1Department of Computer Science, Brown University, Providence, RI, United States\n2Center for Computational Molecular Biology, Brown University, Providence, RI, United States\n3School of Medicine, University of Tübingen, Tübingen, Germany\nCorresponding Author:\nCarsten Eickhoff, PhD\nSchool of Medicine\nUniversity of Tübingen\nSchaffhausenstr, 77\nTübingen, 72072\nGermany\nPhone: 49 7071 29 843\nEmail: carsten.eickhoff@uni-tuebingen.de\nAbstract\nBackground: Patients with rare and complex diseases often experience delayed diagnoses and misdiagnoses because\ncomprehensive knowledge about these diseases is limited to only a few medical experts. In this context, large language models\n(LLMs) have emerged as powerful knowledge aggregation tools with applications in clinical decision support and education\ndomains.\nObjective: This study aims to explore the potential of 3 popular LLMs, namely Bard (Google LLC), ChatGPT-3.5 (OpenAI),\nand GPT-4 (OpenAI), in medical education to enhance the diagnosis of rare and complex diseases while investigating the impact\nof prompt engineering on their performance.\nMethods: We conducted experiments on publicly available complex and rare cases to achieve these objectives. We implemented\nvarious prompt strategies to evaluate the performance of these models using both open-ended and multiple-choice prompts. In\naddition, we used a majority voting strategy to leverage diverse reasoning paths within language models, aiming to enhance their\nreliability. Furthermore, we compared their performance with the performance of human respondents and MedAlpaca, a generative\nLLM specifically designed for medical tasks.\nResults: Notably, all LLMs outperformed the average human consensus and MedAlpaca, with a minimum margin of 5% and\n13%, respectively, across all 30 cases from the diagnostic case challenge collection. On the frequently misdiagnosed cases\ncategory, Bard tied with MedAlpaca but surpassed the human average consensus by 14%, whereas GPT-4 and ChatGPT-3.5\noutperformed MedAlpaca and the human respondents on the moderately often misdiagnosed cases category with minimum\naccuracy scores of 28% and 11%, respectively. The majority voting strategy, particularly with GPT-4, demonstrated the highest\noverall score across all cases from the diagnostic complex case collection, surpassing that of other LLMs. On the Medical\nInformation Mart for Intensive Care-III data sets, Bard and GPT-4 achieved the highest diagnostic accuracy scores, with\nmultiple-choice prompts scoring 93%, whereas ChatGPT-3.5 and MedAlpaca scored 73% and 47%, respectively. Furthermore,\nour results demonstrate that there is no one-size-fits-all prompting approach for improving the performance of LLMs and that a\nsingle strategy does not universally apply to all LLMs.\nConclusions: Our findings shed light on the diagnostic capabilities of LLMs and the challenges associated with identifying an\noptimal prompting strategy that aligns with each language model’s characteristics and specific task requirements. The significance\nof prompt engineering is highlighted, providing valuable insights for researchers and practitioners who use these language models\nfor medical training. Furthermore, this study represents a crucial step toward understanding how LLMs can enhance diagnostic\nreasoning in rare and complex medical cases, paving the way for developing effective educational tools and accurate diagnostic\naids to improve patient care and outcomes.\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 1https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\n(JMIR Med Educ 2024;10:e51391) doi: 10.2196/51391\nKEYWORDS\nclinical decision support; rare diseases; complex diseases; prompt engineering; reliability; consistency; natural language processing;\nlanguage model; Bard; ChatGPT 3.5; GPT-4; MedAlpaca; medical education; complex diagnosis; artificial intelligence; AI\nassistance; medical training; prediction model\nIntroduction\nBackground\nNatural language processing has witnessed remarkable advances\nwith the introduction of generative large language models\n(LLMs). In November 2022, OpenAI released ChatGPT-3.5\n(OpenAI), a large natural language processing chatbot trained\non a large corpus collected from the internet to generate\nhumanlike text in response to user queries. ChatGPT-3.5 has\nseen massive popularity, and users have praised its creativity\nand language comprehension for several tasks, such as text\nsummarization and writing computer programs [1]. In March\n2023, OpenAI responded to the success of ChatGPT-3.5 by\nintroducing an enhanced iteration called GPT-4, specifically\ndesigned to address intricate queries and nuanced directives\nmore effectively. Shortly thereafter, Google released their\ncomparable model, Bard (Google LLC), which joined the league\nof impressive LLMs. What sets Bard apart is its real-time access\nto and use of internet information, enriching its response\ngeneration with up-to-date information [2]. In contrast, GPT-4\npossesses multimodal capabilities, including image inputs, albeit\nnot publicly available during the study [3].\nThese LLMs were not originally designed for medical\napplications. However, several studies [4,5] have shown their\nextraordinary capabilities in excelling in various medical\nexaminations, such as the Self-Assessment in Neurological\nSurgery examination and the USMLE (United States Medical\nLicensing Examination). Their results demonstrated the ability\nof these models to handle clinical information and complex\ncounterfactuals. Furthermore, numerous investigations [6-8]\nhave revealed the remarkable advantages of harnessing the\npower of LLMs in diverse medical scenarios. Notably, Lee et\nal [8] demonstrated using LLMs as a reliable conversational\nagent to collect patient information to assist in medical\nnotetaking, whereas Patel and Lam [9] delved into using LLMs\nas a valuable tool for generating comprehensive patient\ndischarge summaries. The ability of LLMs to process and\ngenerate medical text has unlocked new opportunities to enhance\ndiagnostic reasoning, particularly in tackling rare and complex\nmedical cases.\nRare diseases are characterized by their low prevalence in the\ngeneral population, whereas complex diseases are conditions\nwith overlapping factors and multiple comorbidities that are\noften difficult to diagnose [10,11]. Sometimes, a condition can\nbe rare and complex if it is infrequent and challenging to\ndiagnose accurately [11]. Rare and complex diagnoses present\nsignificant challenges across various medical levels and often\nrequire extensive medical knowledge or expertise for accurate\ndiagnosis and management [10,11]. This may be because, during\ntheir education, physicians are trained to prioritize ruling out\ncommon diagnoses before considering rare ones during patient\nevaluation [12]. In addition, most medical education programs\nrarely cover some complex conditions, and guidance for\npracticing clinicians is often outdated and inappropriate [13,14].\nAs a result, most physicians perceive their knowledge of rare\ndiseases as insufficient or very poor, and only a few feel\nadequately prepared to care for patients with these conditions\n[12,15]. This knowledge gap increases the risk of misdiagnosis\namong individuals with rare and complex conditions.\nFurthermore, the scarcity of available data and the relatively\nsmall number of affected individuals create a complicated\ndiagnostic landscape, even for experienced and specialized\nclinicians [10]. Consequently, patients often endure a prolonged\nand arduous diagnostic process. Therefore, there is a pressing\nneed for comprehensive educational tools and accurate\ndiagnostic aids to fill the knowledge gap and address these\nchallenges effectively.\nThis study aims to explore the potential of 3 LLMs, namely\nBard, GPT-4, and ChatGPT-3.5, as continuing medical education\n(CME) systems to enhance the diagnoses of rare and complex\nconditions. Although these models have demonstrated\nimpressive success in standardized medical examinations [4,5],\nit is important to acknowledge that most examinations reflect\ngeneral clinical situations, which may not fully capture the\nintricacies encountered in real-world diagnostic scenarios.\nFurthermore, these standardized tests often feature questions\nthat can be answered through memorization [16]. In contrast,\nreal-world complex diagnostic scenarios that physicians face\ninvolve dynamic, multifaceted patient cases with numerous\nvariables and uncertainties. Although previous studies by Liu\net al [17] and Cascella et al [18] have highlighted the ability of\nLLMs to support health care professionals in real-world\nscenarios, their effectiveness in diagnosing rare and complex\nconditions remains an area of exploration. Despite the promising\nuse of LLMs in medical applications, studies have reported that\ntheir responses to user queries are often nondeterministic (ie,\ndepending on the query format) and exhibit significant variance\n[17,19]. This attribute may pose challenges in clinical decision\nsupport scenarios because the dependability of a system is\nuncertain when its behavior cannot be accurately predicted.\nHowever, no investigation has been conducted to show how\ndifferent input formats (prompts) affect LLM responses in the\nmedical context.\nPrompt engineering is a technique for carefully designing\nqueries (inputs) to improve the performance of generative\nlanguage models [20,21]. We can guide LLMs to generate more\naccurate and reliable responses by carefully crafting effective\nprompts. Our study investigated effective prompting strategies\nto improve the accuracy and reliability of LLMs in diagnosing\nrare and complex conditions within an educational context. We\nevaluated the performance of LLMs by comparing their\nresponses to those of human respondents and the responses of\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 2https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nMedAlpaca [22], an open-source generative LLM designed for\nmedical tasks. Given the documented advantages of using LLMs\nas a complementary tool rather than a substitute for clinicians\n[17,18], our study incorporated LLMs with the understanding\nthat clinicians may use them beyond real-time diagnostic\nscenarios. Although our premise is based on a clinician having\nestablished an initial diagnostic hypothesis and seeking further\nassistance to refine the precise diagnosis, we acknowledge the\nbroader utility of LLMs. They can be valuable in real-time\ndecision support and retrospective use during leisure or\ndocumentation, allowing physicians to experiment with and\nenhance their understanding of rare and complex diseases. This\napproach recognizes the inherent uncertainty in diagnosis and\nharnesses the capabilities of LLMs to assist clinicians in various\naspects of their diagnostic processes. In the context of CME,\nour study highlights the possibility of integrating LLMs as a\nvaluable addition. By providing further assistance in refining\ncomplex and rare diagnoses, these LLMs could support\nevidence-based decision-making among health care\nprofessionals for improved patient outcomes.\nObjectives\nOur study has 2 main objectives: first, to examine the potential\nof LLMs as a CME tool for diagnosing rare and complex\nconditions, and second, to highlight the impact of prompt\nformatting on the performance of LLMs. Understanding these\naspects could significantly contribute to advancing diagnostic\npractices and effectively using LLMs to improve patient care.\nMethods\nData Sets\nWe used 2 data sets to examine the capacity of LLMs to\ndiagnose rare and complex conditions as follows:\n1. Diagnostic case challenge collection (DC3) [11] comprises\n30 complex diagnostic cases curated by medical experts in\nthe New England Journal of Medicine web-based case\nchallenges. The original cases contained text and image\ndescriptions of patients’ medical history, diagnostic\nimaging, and laboratory results; however, we used only\ntextual information to form prompts (queries). The\nweb-based polls recorded an average of 5850 (SD 2522.84)\nrespondents per case, many of whom were health care\nprofessionals. The participants were required to identify\nthe correct diagnosis from a list of differential diagnoses.\nCase difficulty was categorized based on the percentage of\ncorrect responses received from the respondents on the\nweb-based survey. The case categories were: “rarely\nmisdiagnosed cases” (with ≥21/30, 70% correct responses),\n“moderately misdiagnosed cases” (with >9/30, 30% and\n<21/30, 70% correct responses), and “frequently\nmisdiagnosed cases” (with ≤9/30, 30% correct responses).\nFurthermore, the final diagnoses determined by the treating\nphysicians of the cases were provided alongside the poll\nresults, enabling the comparison of the performance of\nhuman respondents with that of the targeted LLMs.\n2. Medical Information Mart for Intensive Care-III\n(MIMIC-III) [23] comprises deidentified electronic health\nrecord data from approximately 50,000 Boston Beth Israel\nDeaconess Medical Center intensive care unit patients. We\nfocused on discharge summaries containing the accumulated\npatient information from admission to discharge. Similar\nto previous work on clinical outcome prediction by van\nAken et al [24] and Abdullahi et al [25], we filtered\ndocument sections unrelated to admissions, such as\ndischarge information or hospital course and retained\nsections related to admissions, such as chief complaint,\nhistory of illness or present illness, medical history,\nadmission medications, allergies, physical examination,\nfamily history, and social history. Each discharge summary\nhad a discharge diagnosis section that indicated the patient’s\nfinal diagnosis for that admission. We reviewed the\ndischarge summaries to identify rare diseases and referred\nto the Orphanet website [26]. In this study, we randomly\nselected 15 unique, rare conditions as our target. These\ncases were selected as pilot studies for a focused and\nin-depth analysis.\nModels\nIn this study, we conducted experiments using LLMs designed\nfor conversational context. Specifically, we used the July 6,\n2023, version of Bard; the July 4, 2023, versions of GPT-4 and\nChatGPT-3.5; and the publicly available version of MedAlpaca\n7b [22]. We entered prompts individually through the chat\ninterface to evaluate Bard, GPT-4, and ChatGPT-3.5, treating\neach prompt as a distinct conversation. MedAlpaca differs from\nBard, ChatGPT-3.5, and GPT-4 in that it requires users to submit\nqueries or prompts through a Python (Python Software\nFoundation) script. Consequently, we used a single Python script\nfor each prompt strategy to submit queries for each data set. It\nis worth noting that Bard has certain limitations compared with\nChatGPT-3.5 and GPT-4. Bard has a restricted capacity to\nhandle lengthy queries. Moreover, Bard is more sensitive to\nnoisy input and specific characters. For example, the MIMIC-III\ndata set contained deidentified patients’notes filled with special\ncharacters such as “[**Hospital 18654**]” and laboratory results\nwritten in shorthand, for example, * Hgb-9.6* Hct-29.7*\nMCV-77* MCH-24.9*. Consequently, to work effectively with\nBard, we preprocessed the text by removing special characters\nand retaining only alphanumeric characters.\nPrompting Strategies\nDirect (standard prompting) and iterative prompting (chain of\nthought prompting) [27] are the 2 major prompting methods.\nIterative prompting is a promising method for improving LLM\nperformance on specialized tasks; however, it requires a\npredefined set of manually annotated reasoning steps, which\ncan be time consuming and difficult to create, especially for\nspecialized domains. Most users opt for a direct prompt method\nto save time and obtain an immediate response. Therefore, to\nanalyze the effect of prompt formats on LLM performance, we\nassessed each model’s performance for every case using the 3\ndistinct direct prompt strategies outlined in Table 1. These\nstrategies varied from open-ended to multiple-choice formats.\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 3https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nTable 1. Prompt strategies.\nPrompt samplePrompt strategy descriptionApproach\n“What is the diagnosis? The case is: A 32-year-old man was\nevaluated in the emergency department of this hospital for the\nabrupt onset of postprandial chest pain...”\nIn this approach, prompts were formatted in an open-ended\nfashion. Formatting a prompt using this method allows the\nmodel to formulate a hypothesis for the case and explain\nwhy and what it thinks is the diagnosis. Here, we scored a\nmodel based on its ability to provide the correct diagnosis\nwithout additional assistance.\nApproach 1 (open-\nended prompt)\n“Choose the most likely diagnosis from the following: Option I:\nCholecystitis, Option II: Acute coronary syndrome, Option III:\nPericarditis, Option IV: Budd-Chiari syndrome. The case is: A\n32-year-old man was evaluated in the emergency department of\nthis hospital for the abrupt onset of postprandial chest pain...”\nWe formatted prompts as multiple-choice questions, and the\nLLMsa were expected to select a single diagnosis from a list\nof options. The models were assigned a positive score in this\ntask if they selected the correct diagnosis from the options.\nApproach 2 (multi-\nple-choice prompt)\n“Rank the following diagnoses according to the most likely.\nOption I: Cholecystitis, Option II: Acute coronary syndrome,\nOption III: Pericarditis, Option IV: Budd-Chiari syndrome. The\ncase is: A 32-year-old man was evaluated in the emergency de-\npartment of this hospital for the abrupt onset of postprandial chest\npain...”\nThe prompts were presented as a case and a list of diagnoses\nto be ranked by the LLMs. Models were assigned a positive\nscore if the correct diagnosis was ranked first in this format.\nApproach 3 (ranking\nprompt)\naLLM: large language model.\nBuilding upon prior research by Wang et al [28] and Li et al\n[29], we hypothesized that using a diverse range of prompts can\nreveal distinct reasoning paths while maintaining consistency\nin the correct responses regardless of the variations. When using\nmultiple-choice prompts for the DC3 cases, we presented the\nsame options available in the original web-based polls to the\nmodels, but on the MIMIC-III data set, we generated random\nwrong answers that were closely related to the correct diagnosis.\nWe evaluated each LLM by assigning a positive or negative\nscore (binary score) based on their responses. A positive score\nwas assigned only if the models correctly selected the diagnosis\nfor either data set. Conversely, we omitted the options for\nopen-ended prompts, expecting the models to generate the\ncorrect diagnosis independently. Positive scores were awarded\nonly if the models accurately provided the correct diagnosis.\nPrompt Ensemble: Majority Voting\nTo safely use imperfect language models, users must determine\nwhen to trust their predictions, particularly in critical situations,\nsuch as clinical decision support. Therefore, we used a majority\nvoting (prompt ensembling) strategy to enhance the reliability\nof LLMs’ responses. The majority voting approach involves\naggregating multiple responses and selecting the most common\nanswer. By applying this approach to responses generated by\ndifferent LLMs, we can observe the level of agreement and infer\nthe consistency in their outputs for a given prompt. Specifically,\nwe hypothesized that using a majority voting approach from\nthe ensemble of prompt responses would boost the reliability\nof language models, minimizing potential errors, variations,\nand biases associated with individual prompting approaches.\nTo achieve this, in independent chats, we prompted the LLM\nwith 3 distinct prompt formats per case, as presented in Table\n1. Subsequently, we collected the responses of each model and\napplied majority voting to aggregate its predictions, as presented\nin Figure 1. In majority voting, each prompt produced a response\nfrom the language model, and the majority response was chosen\nas the final response. In a scenario where all prompt strategies\nresulted in different responses, we assumed that the model was\nunsure of that question and scored the final response as a failure\ncase. We limited the number of prompts in the ensemble to 3\nbecause studies by Wang et al [28] and Li et al [29] have shown\nthat we obtain diminishing returns as we increase the overall\nnumber of prompts in an ensemble.\nFigure 1. Our proposed method contains the following steps: (1) prompt a language model using a distinct set of prompts, (2) obtain diverse responses,\nand (3) choose the most consistent response as the final answer (majority voting).\nEthical Considerations\nNo ethics approval was pursued for this research, given that the\ndata was publicly accessible and deidentified. This aligns with\nthe guidelines outlined in the National Institutes of Health\ninvestigator manual for human subjects research [30].\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 4https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nResults\nPerformance Across Prompt Strategies\nFigure 2 reveals the performance of LLMs across different\nprompts on the DC3 data set. Overall, approach 2\n(multiple-choice prompt) yielded the highest score for all 30\ncases, with GPT-4 and Bard achieving an accuracy score of\n47% (14/30) and ChatGPT-3.5 obtaining a score of 43% (13/30).\nHowever, when considering case difficulty, the results varied.\nOn the frequently misdiagnosed cases category, GPT-4 and\nChatGPT-3.5 performed better with open-ended prompts\n(approach 1), scoring 30% (3/10) and 20% (2/10), respectively.\nIn contrast, Bard demonstrated superior performance with\nmultiple-choice prompts for selection and ranking (approaches\n2 and 3), achieving a score of 30% (3/10). ChatGPT-3.5 and\nBard performed equally well on the rarely misdiagnosed cases\ncategory using approaches 2 and 3, achieving a perfect score\nof 100% (2/2). Furthermore, GPT-4 attained a score of 100%\n(2/2) but only with approach 2. For the moderately misdiagnosed\ncases category, all LLMs achieved their best performance with\napproach 2, scoring 67% (12/18), 56% (10/18), and 50% (9/18)\nfor GPT-4, ChatGPT-3.5, and Bard, respectively. Table S1 in\nthe Multimedia Appendix 1 presents the inconsistencies in the\ncorrect responses across the approaches for different cases. For\nexample, Bard could only diagnose milk alkali syndrome using\napproach 1 but failed to use other prompt approaches.\nChatGPT-3.5 correctly diagnosed primary adrenal insufficiency\n(Addison disease) with only approach 2, whereas GPT-4 was\nable to diagnose acute hepatitis E virus infection with only\napproach 1. These results indicate that no universal prompt\napproach is optimal for all LLMs when dealing with complex\ncases.\nResults on the MIMIC-III data set in Figure 3 showed that the\nLLMs also performed best using approach 2 (multiple-choice\nprompt), with Bard and GPT-4 obtaining scores of 93% (14/15)\neach and ChatGPT-3.5 obtaining 73% (11/15). Using approach\n3 (ranking prompt) resulted in a slight drop in performance for\nGPT-4 and Bard, with a 6% decrease, whereas the performance\nof ChatGPT-3.5 dropped by 26%. Approach 1 (open-ended\nprompt) proved challenging for the LLMs, with scores of 47%\n(7/15), 60% (9/15), and 27% (4/15) for Bard, GPT-4, and\nChatGPT-3.5, respectively. Table S2 in the Multimedia\nAppendix 1 illustrates that approach 1 was only beneficial to\nGPT-4 in diagnosing amyloidosis, whereas it was consistently\nnever the sole correct approach for Bard and ChatGPT-3.5.\nThese results aligned with the findings from the DC3 data set\nand emphasized the varying performances of different models\nand prompt approaches across tasks.\nFigure 2. Results of the diagnostic case challenge collection data set comparing prompt strategies. OpenAI GPT-4 outperformed all other models,\nachieving the highest score in all 30 cases using the majority voting approach. Furthermore, all large language models except MedAlpaca outperformed\nthe human consensus (denoted by a black dashed line) across all cases, regardless of the difficulty, using at least 1 prompt approach. GPT-4: generative\npretrained transformer-4.\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 5https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nFigure 3. Results of the Medical Information Mart for Intensive Care-III data set across prompt strategies. Approach 1 (open-ended prompt) proved\nchallenging for all the large language models compared with approach 2 (multiple-choice prompt) and approach 3 (ranking prompt).\nPerformance With Majority Voting\nPrevious experiments have demonstrated that there is no perfect\nprompting strategy because LLM users may not know\nbeforehand which prompt will produce a correct response. We\nused the majority voting approach to estimate consistency,\nmaximize the benefits of different prompt strategies, and\nenhance the reliability of the LLMs’ responses. Figure 2\nillustrates the results for all DC3 cases. Majority voting\nimproved the overall performance of GPT-4 from 47% to 50%,\nwhereas the performance of ChatGPT-3.5 remained at 43%\nbecause majority voting did not decrease its performance\ncompared with that of approach 2. In contrast, the performance\nof Bard decreased from 47% to 43% compared with that of\napproach 2. Summarizing the overall performance based on\nquery difficulty, majority voting resulted in a perfect score of\n100% for the rarely misdiagnosed cases category across all the\nLLMs. For the frequently misdiagnosed cases category in DC3,\nBard achieved the highest score with majority voting and\nmultiple-choice prompts, whereas GPT-4 performed best for\nthe moderately misdiagnosed cases category with majority\nvoting and approach 2. In addition, GPT-4 outperformed all\nother LLMs across all DC3 cases using the majority voting\napproach, regardless of the case difficulty. This score surpassed\nthe performance of the individual prompt approaches in all\ncases.\nResults on the MIMIC-III data set in Figure 3 showed that, the\nscores with majority voting were 87% (13/15) for GPT-4 and\nBard each and 53% (8/15) for ChatGPT-3.5. These results\nindicate that the ensemble method did not substantially improve\ntheir performance compared with their best individual approach.\nIt is worth noting that although the majority voting approach\ndid not consistently outperform individual approaches in terms\nof the highest number of correct responses, it did provide a\nmeans to consolidate predictions and mitigate potential errors\nand biases from single approaches.\nComparison With Human Respondents\nIn the DC3 cases, although the human respondents had the\nadvantage of accessing supporting patient information such as\nimage scans and magnetic resonance imaging, the LLMs\nconsistently outperformed the average human consensus. As\nshown in Figure 2, using the majority voting approach, all LLMs\nachieved a higher performance than the human consensus\n(denoted by a black dashed line), with a minimum margin of\n5% across all 30 cases. Specifically, when considering query\ndifficulty, the LLMs demonstrated even greater superiority. In\nthe rarely misdiagnosed cases category, all LLMs surpassed the\naverage human consensus by a substantial margin of 26%. For\nthe moderately misdiagnosed cases category, GPT-4 and\nChatGPT-3.5 maintained their advantage over human\nrespondents, achieving a minimum margin of 11% with the\nmajority voting approach. In contrast, only Bard outperformed\nthe human average consensus on the frequently misdiagnosed\ncases category, with a margin of 14%.\nWe conducted a Spearman rank correlation test to analyze the\npattern in the responses between each LLM and the human\nrespondents. This involved correlating the average percentage\nof correct responses for each LLM across the prompt strategies\nwith that of correct human responses. The results of the\nSpearman correlation test revealed that Bard had a relatively\nweak correlation coefficient of 0.30, whereas GPT-4 and\nChatGPT-3.5 exhibited moderate positive correlations of 0.51\nand 0.50, respectively. This suggested that the diagnostic\nperformance patterns of GPT-4 and ChatGPT-3.5 aligned\nmoderately with those of the human respondents. The observed\ncorrelation in answering patterns between human respondents\nand LLMs may stem from the inherent data bias present in the\ntraining data sets. The LLMs learn from vast amounts of data,\nand if the training data are biased toward certain diagnostic or\ndecision-making patterns commonly expressed by human\nphysicians, the model is likely to replicate those patterns.\nAlthough the correlation suggested that the LLMs have the\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 6https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\npotential to be valuable tools in medical education, it is\nimportant to note their correlation with human physicians and\nthat the performance of LLMs does not necessarily mean that\nthey are as good as human physicians in diagnosing and treating\ndiseases.\nWe could not directly compare the performance of human\nrespondents on the MIMIC-III data sets because of the\nunavailability of data. Overall, the results indicated that the\nLLMs consistently outperformed the average human consensus\nin diagnosing medical cases, showcasing their potential as a\ntool to complement and enhance care quality and education for\ncomplex diagnostic cases.\nComparison With MedAlpaca\nOn the DC3 data sets, Bard, GPT-4, and ChatGPT-3.5\noutperformed MedAlpaca across all cases using the majority\nvoting approach by a minimum margin of 13%. MedAlpaca\nalso displayed the worst performance in the open-ended prompts,\nirrespective of query difficulty. However, when multiple-choice\noptions were provided, MedAlpaca outperformed the other\nLLMs in the frequently misdiagnosed cases category. Similar\nto the DC3 data set, MedAlpaca consistently demonstrated its\nbest performance using the ranking prompt on the MIMIC-III\ndata sets. However, its overall performance was significantly\npoorer than the other LLMs, with each LLM outperforming the\nmodel by at least 26% using the majority voting approach. In\ncontrast to the general-purpose LLMs (eg, Bard, GPT-4, and\nChatGPT-3.5), investigating the MedAlpaca model was\nfinetuned using diverse medical tasks and assessed using\nmultiple-choice medical examinations. This tailored training\napproach likely contributed to its notable performance,\nparticularly excelling in DC3 cases (frequently misdiagnosed\ninstances) and demonstrating optimal results in multiple-choice\nqueries.\nQualitative Analysis\nIn our experiments, we manually observed the responses of each\nLLM to all our prompts and noted that each LLM consistently\njustified its diagnosis choice except for MedAlpaca. Specifically,\neach LLM offered a logical explanation for its chosen response\nregardless of the prompting strategy. For further investigation,\nwe analyzed each LLM’s responses in 3 scenarios: (1) when\npresented with multiple-choice options containing the true\ndiagnosis and they responded accurately, (2) when their response\nwas incorrect, and (3) when given only incorrect multiple-choice\noptions to pick from. In the first scenario, as presented in\nMultimedia Appendix 1, all LLMs (eg, Bard, GPT-4, and\nChatGPT-3.5) mentioned that their rationale for diagnosing\nmiliary tuberculosis was owing to relevant symptoms presented\nin the case, such as a history of respiratory illness and the\npresence of mesenteric lymph nodes and numerous tiny nodules\nthroughout both lungs distributed in a miliary pattern. This\npattern of offering insightful reasons for the likelihood of a\ndiagnosis and explaining why other diagnostic options are less\nprobable is valuable for educational purposes. In the second\nscenario, we observed that there was a notable disparity in the\naccuracy of human respondents. Only 6% (217/3624) of the\nhuman participants provided the correct response, with most\nvotes (1232/3624, 34%) favoring ulcerative colitis, whereas\n23% (833/3624) of the human responses opted for salmonellosis.\nNotably, Bard and GPT-4 displayed similar behavior by\nselecting salmonellosis, whereas ChatGPT-3.5 and MedAlpaca\nchose ulcerative colitis.\nAnother notable finding occurred in the responses of GPT-4\nand ChatGPT-3.5. Regardless of the correctness of their chosen\ndiagnoses, these models consistently recommended further tests\nto confirm their responses. This behavior suggested a general\ntendency toward advocating additional examinations to validate\ntheir diagnoses, potentially reflecting a cautious approach. In\ncontrast, Bard adopted a different approach. Instead of\nrecommending further tests, Bard highlighted that the provided\nquery information supported the diagnosis without suggesting\nadditional confirmatory measures. In the scenario where only\nincorrect options were given, Bard, ChatGPT-3.5, and\nMedAlpaca made choices and justified their responses. In\ncontrast, GPT-4 explicitly mentioned that none of the provided\noptions matched the case presentation. Furthermore, GPT-4\nsuggested a more probable diagnosis and recommended\nadditional testing to explore its feasibility.\nDiscussion\nPrincipal Findings\nPrevious studies [4,5] have presented the impressive success of\nLLMs in standardized medical examinations. We conducted\nexperiments to assess the potential of LLMs as a CME system\nfor rare and complex diagnoses, and our findings demonstrated\nthat LLMs have the potential to be a valuable tool for rare\ndisease education and differential diagnosis. Although LLMs\ndemonstrated superior performance compared with the average\nhuman consensus in diagnosing complex diseases, it is essential\nto note that this does not imply their superiority over physicians.\nNumerous unknown factors, including the level of respondents’\nexpertise, may influence the outcome of web-based polls.\nFurthermore, we examined the knowledge capacity of LLMs\nthrough open-ended and multiple-choice prompts and found\nthat LLMs, including MedAlpaca, performed better with\nmultiple-choice prompts. This improvement can be attributed\nto the options provided, which narrowed the search space for\npotential diagnoses from thousands to a few likely possibilities.\nConsequently, we surmise that LLMs are not yet ready to be\nused as stand-alone tools, which aligns with the findings of\nprevious studies [5,17,18]. Our observations revealed the\nconsistent outperformance of general-purpose LLMs over\nMedAlpaca in various experiments. Their superior ability to\nprovide valuable justifications for making diagnoses was\nparticularly noteworthy, a strength not matched by MedAlpaca.\nThis difference may stem from MedAlpaca’s exclusive\nfinetuning and assessment for multiple-choice medical\nexaminations, which slightly differ in format from the clinical\ncases in our experiments.\nA notable finding in the response of LLMs to queries was their\nconsistent provision of coherent and reasoned explanations,\nregardless of the query format. For instance, when diagnosing\nmiliary tuberculosis, all 3 LLMs emphasized that the patient’s\nsystemic symptoms, exposure risks, chest radiograph, computed\ntomography scan findings, and the suspected compromised\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 7https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nimmune state collectively support the diagnosis of miliary\ntuberculosis. Furthermore, Bard and GPT-4 ruled out other\ndiagnoses presented in the multiple-choice prompt by\nhighlighting their less typical presentations and lack of certain\nassociated symptoms or risk factors. In addition, the\nconversational nature of LLMs allows users to ask follow-up\nquestions for further context. These attributes hold great\npotential for educating users and offering them insights.\nHowever, we observed that LLMs provided logical explanations,\neven when their diagnoses were incorrect. ChatGPT-3.5 and\nGPT-4 may suggest additional testing to validate their selected\ndiagnosis or use cautious terms like “potential diagnosis.”\nHowever, it remains unclear whether these recommendations\nstem from the models’ internal confidence or whether there are\nfeatures intentionally designed by the developers for cautious\nuse. The absence of explicit information regarding the level of\nuncertainty of LLMs for a specific case is concerning as it could\npotentially mislead clinicians. The ability to quantify uncertainty\nis crucial in medical decision-making, in which accurate\ndiagnoses and treatment recommendations are paramount.\nClinicians heavily rely on confidence levels and probability\nassessments to make informed judgments [29]. Without an\nindication of uncertainty, there is a risk that clinicians may trust\nthe logical explanations provided by the LLMs even when they\nare incorrect, leading to misdiagnoses or inappropriate treatment\nplans.\nConsidering the delicate role of clinical decision support, it is\nessential to address validity and reliability as crucial aspects of\nuncertainty. Moreover, a reliable system is of paramount\nimportance for medical education. However, the stochastic\nnature of LLMs introduces doubts among clinicians regarding\ntheir reliability. Although a specific metric to quantitatively\nassess the reliability of the LLMs used in this study is currently\nlacking, we acknowledge the significance of consistency in\nachieving reliability. To address this, we used different\nprompting strategies and implemented a majority voting\napproach to select the most consistent response from each LLM.\nAfter examining the individual prompt strategies, we anticipated\nconsistent responses across strategies for a specific case.\nHowever, our findings revealed that the responses of LLMs\nwere sensitive to concrete prompt formats, particularly in\ncomplex diagnoses. For instance, ChatGPT-3.5 and GPT-4\nperformed better with the open-ended prompt (approach 1) in\nthe frequently misdiagnosed cases category of DC3 cases but\nstruggled with similar cases using multiple-choice and ranking\nprompts (approaches 2 and 3). In contrast, Bard performed better\nwith multiple-choice prompts. These results highlighted that\nthere is no one-size-fits-all prompting approach nor does a single\nstrategy apply universally to all LLMs. Although the majority\nvoting strategy did not yield optimal results for all models across\ndata sets, it served as a means to consolidate responses from\nmultiple prompts and provided a starting point for incorporating\nreliability.\nSeveral studies [10-12,14,15] have emphasized the significance\nof enhancing the education of clinicians at all levels to provide\nbetter support for rare and complex diagnoses. In this pursuit,\nthe studies by Lee et al [8] and Decherchi et al [31] have\nhighlighted the potential advantages of artificial intelligence\n(AI) systems, whereas the studies by Abdullahi et al [25] and\nSutton et al [32] have reported a lack of acceptance of AI tools\namong clinicians. For instance, younger medical students and\nresidents appeared more receptive to integrating technology\n[33]. One notable reason for this lack of acceptance is that\nconventional AI systems typically require training before\nclinicians can effectively use them, which can be burdensome\nand time consuming [32]. In contrast, conversational LLMs,\nsuch as ChatGPT-3.5, Bard, and GPT-4, offer a distinct\nadvantage with their simple interface and dialogue-based nature.\nThese conversational LLMs eliminate the need for extensive\ntraining, increasing their potential for high acceptance across\nall levels of medical practice. Although the exciting ease of use,\nconversational nature, impressive display of knowledge, and\nlogical explanations of LLMs have the potential for user\neducation and insights, their current limitations in reliability\nand expressing uncertainty must be addressed to ensure their\neffective and responsible use in critical domains, such as health\ncare.\nLimitations\nFirst, the limitations of the knowledge of ChatGPT-3.5 and\nGPT-4 to the latest trends and updates in health care (or medical)\ndata till 2021 pose the risk of potentially incomplete information\nand hamper the effectiveness of the models as a CME tool,\nespecially when addressing emerging diseases. In contrast,\nalthough continuous updates to Bard are advantageous for\nkeeping the model up-to-date, this attribute may impact the\nreproducibility of our study. Second, it is notable that our\nexperiments had a limited scope owing to a small sample size\nconsisting of only 30 diseases from the DC3 data set and 15\ncases from the MIMIC-III data set. In addition, although we\ntook precautions to preprocess the MIMIC-III notes to prevent\nleakage of the final diagnosis, the discharge summaries may\nstill contain nuanced information that could make the diagnosis\nobvious. Furthermore, the closed nature of the LLMs used in\nthis study restricted our technique for measuring reliability to\na majority voting approach, which consolidated responses from\ndiverse prompts. Although majority voting can help to mitigate\nthe variability of LLM output, it is notable that LLMs may still\ngenerate different responses for the same prompt. This\nvariability should be considered when interpreting the results\nof this study. However, when these LLMs are released with an\nenhanced iteration that allows for finetuning and calibration,\nfuture work should incorporate more effective mechanisms to\nestimate and communicate uncertainty. An example of such an\napproach could involve assigning a confidence score to the\nprobability score of their responses. This methodology could\nallow clinicians to make informed decisions regarding whether\nto accept or reject responses that fall within a desired threshold.\nConclusions\nIn this study, we conducted experiments to assess the potential\nof LLMs, including ChatGPT-3.5, GPT-4, and Bard, as a CME\nsystem for rare and complex diagnoses. First, we evaluated their\ndiagnostic capability specifically for rare and complex cases.\nSubsequently, we explored the impact of prompt formatting on\ntheir performance. Our results revealed that these LLMs\npossessed potential diagnostic capacities for rare and complex\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 8https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nmedical cases, surpassing the average crowd consensus on the\nDC3 cases. For selected rare cases from the MIMIC-III data\nset, Bard and GPT-4 achieved a diagnostic accuracy of 93%,\nwhereas ChatGPT-3.5 achieved an accuracy of 73%. Our\nfindings highlighted that users might discover an approach that\nyields favorable results for various queries by exploring different\nprompt formats. In contrast, using majority voting of responses\nfrom multiple prompt strategies offers the benefit of a robust\nand reliable model, instilling confidence in the generated\nresponses. However, determining the best prompt strategy versus\nrelying on the majority voting approach involves a tradeoff\nbetween exploration and exploitation. Although prompt\nengineering research is continuing, we hope that future studies\nwill yield better solutions to enhance the reliability and\nconsistency of the responses of LLMs. Overall, our study’s\nresults and conclusions provide a benchmark for the\nperformance of LLMs and shed light on their strengths and\nlimitations in generating responses, expressing uncertainty, and\nproviding diagnostic recommendations. The insights gained\nfrom this study can serve as a foundation for further exploration\nand research on using LLMs as medical education tools to\nenhance their performance and capabilities as conversational\nlanguage models.\nAcknowledgments\nWe acknowledge support from the Open Access Publication Fund of the University of Tübingen.\nData Availability\nThe URLs for the diagnostic case challenge collection data set can be obtained via A Diagnostic Case Challenge Collection [34].\nThe Medical Information Mart for Intensive Care data sets can be accessed via the database, Medical Information Mart for\nIntensive Care-III Clinical Database v1.4 [35], after obtaining permission from Physionet.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nComprehensive tables detailing the performance of each model across data sets, with included examples of prompts and responses\nfor each model.\n[DOCX File , 46 KB-Multimedia Appendix 1]\nReferences\n1. Introducing ChatGPT. OpenAI. URL: https://openai.com/blog/chatgpt/ [accessed 2023-03-23]\n2. Manyika J, Hsiao S. An overview of Bard: an early experiment with generative AI. Google. URL: https://ai.google/static/\ndocuments/google-about-bard.pdf [accessed 2024-01-26]\n3. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. GPT-4 technical report. arXiv. Preprint posted online\nMarch 15, 2023. [FREE Full text]\n4. Resnick DK. Commentary: performance of ChatGPT, GPT-4, and Google Bard on a neurosurgery oral boards preparation\nquestion bank. Neurosurgery. Jul 19, 2023 (forthcoming). [doi: 10.1227/neu.0000000000002618] [Medline: 37466324]\n5. Kung TH, Cheatham M, Medenilla A, Sillos C, De Leon L, Elepaño C, et al. Performance of ChatGPT on USMLE: potential\nfor AI-assisted medical education using large language models. PLOS Digit Health. Feb 9, 2023;2(2):e0000198. [FREE\nFull text] [doi: 10.1371/journal.pdig.0000198] [Medline: 36812645]\n6. Gao CA, Howard FM, Markov NS, Dyer EC, Ramesh S, Luo Y, et al. Comparing scientific abstracts generated by ChatGPT\nto real abstracts with detectors and blinded human reviewers. NPJ Digit Med. Apr 26, 2023;6(1):75. [FREE Full text] [doi:\n10.1038/s41746-023-00819-6] [Medline: 37100871]\n7. Dave T, Athaluri SA, Singh S. ChatGPT in medicine: an overview of its applications, advantages, limitations, future\nprospects, and ethical considerations. Front Artif Intell. May 4, 2023;6:1169595. [FREE Full text] [doi:\n10.3389/frai.2023.1169595] [Medline: 37215063]\n8. Lee P, Bubeck S, Petro J. Benefits, limits, and risks of GPT-4 as an AI chatbot for medicine. N Engl J Med. Mar 30,\n2023;388(13):1233-1239. [doi: 10.1056/nejmsr2214184]\n9. Patel SB, Lam K. ChatGPT: the future of discharge summaries? Lancet Digit Health. Mar 2023;5(3):e107-e108. [doi:\n10.1016/s2589-7500(23)00021-3]\n10. Mitani AA, Haneuse S. Small data challenges of studying rare diseases. JAMA Netw Open. Mar 02, 2020;3(3):e201965.\n[FREE Full text] [doi: 10.1001/jamanetworkopen.2020.1965] [Medline: 32202640]\n11. Eickhoff C, Gmehlin F, Patel AV, Boullier J, Fraser H. DC3 -- a diagnostic case challenge collection for clinical decision\nsupport. In: Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval. Presented\nat: ICTIR '19; October 2-5, 2019, 2019; Santa Clara, CA. [doi: 10.1145/3341981.3344239]\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 9https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\n12. Walkowiak D, Domaradzki J. Are rare diseases overlooked by medical education? Awareness of rare diseases among\nphysicians in Poland: an explanatory study. Orphanet J Rare Dis. Sep 28, 2021;16(1):400. [FREE Full text] [doi:\n10.1186/s13023-021-02023-9] [Medline: 34583737]\n13. Sartorious N. Comorbidity of mental and physical diseases: a main challenge for medicine of the 21st century. Shanghai\nArch Psychiatry. Apr 2013;25(2):68-69. [FREE Full text] [doi: 10.3969/j.issn.1002-0829.2013.02.002] [Medline: 24991137]\n14. Bateman L, Bested AC, Bonilla HF, Chheda BV, Chu L, Curtin JM, et al. Myalgic encephalomyelitis/chronic fatigue\nsyndrome: essentials of diagnosis and management. Mayo Clin Proc. Nov 2021;96(11):2861-2878. [FREE Full text] [doi:\n10.1016/j.mayocp.2021.07.004] [Medline: 34454716]\n15. Faviez C, Chen X, Garcelon N, Neuraz A, Knebelmann B, Salomon R, et al. Diagnosis support systems for rare diseases:\na scoping review. Orphanet J Rare Dis. Apr 16, 2020;15(1):94. [FREE Full text] [doi: 10.1186/s13023-020-01374-z]\n[Medline: 32299466]\n16. Mbakwe AB, Lourentzou I, Celi LA, Mechanic OJ, Dagan A. ChatGPT passing USMLE shines a spotlight on the flaws\nof medical education. PLOS Digit Health. Feb 9, 2023;2(2):e0000205. [FREE Full text] [doi: 10.1371/journal.pdig.0000205]\n[Medline: 36812618]\n17. Liu S, Wright AP, Patterson BL, Wanderer JP, Turer RW, Nelson SD, et al. Using AI-generated suggestions from ChatGPT\nto optimize clinical decision support. J Am Med Inform Assoc. Jun 20, 2023;30(7):1237-1245. [FREE Full text] [doi:\n10.1093/jamia/ocad072] [Medline: 37087108]\n18. Cascella M, Montomoli J, Bellini V, Bignami E. Evaluating the feasibility of ChatGPT in healthcare: an analysis of multiple\nclinical and research scenarios. J Med Syst. Mar 04, 2023;47(1):33. [FREE Full text] [doi: 10.1007/s10916-023-01925-4]\n[Medline: 36869927]\n19. Qin G, Eisner J. Learning how to ask: querying LMs with mixtures of soft prompts. arXiv. Preprint posted online April 14,\n2021. [FREE Full text] [doi: 10.18653/v1/2021.naacl-main.410]\n20. Si C, Gan Z, Yang Z, Wang S, Wang J, Boyd-Graber J, et al. Prompting GPT-3 to be reliable. arXiv. Preprint posted online\nOctober 17, 2022. [FREE Full text]\n21. Zhou Y, Muresanu AI, Han Z, Paster K, Pitis S, Chan H, et al. Large language models are human-level prompt engineers.\narXiv. Preprint posted online November 3, 2022. [FREE Full text]\n22. Han T, Adams LC, Papaioannou JM, Grundmann P, Oberhauser, T, Löser A, et al. MedAlpaca -- an open-source collection\nof medical conversational AI models and training data. arXiv. Preprint posted online April 14, 2023. [FREE Full text]\n23. Johnson AE, Pollard TJ, Shen LW, Lehman LH, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible critical care\ndatabase. Sci Data. May 24, 2016;3(1):160035. [FREE Full text] [doi: 10.1038/sdata.2016.35] [Medline: 27219127]\n24. van Aken B, Papaioannou JM, Mayrdorfer M, Budde K, Gers F, Loeser A. Clinical outcome prediction from admission\nnotes using self-supervised knowledge integration. In: Proceedings of the 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main Volume. Presented at: 16th Conference of the European Chapter of the\nAssociation for Computational Linguistics: Main Volume; April 21-23, 2021, 2021; Online. [doi:\n10.18653/v1/2021.eacl-main.75]\n25. Abdullahi TA, Mercurio L, Singh R, Eickhoff C. Retrieval-based diagnostic decision support. JMIR Preprints. Preprint\nposted online June 25, 2023. [FREE Full text] [doi: 10.2196/preprints.50209]\n26. Orphanet: about rare diseases. Orphanet. URL: https://www.orpha.net/consor/cgi-bin/Education_AboutRareDiseases.\nphp?lng=EN [accessed 2023-07-03]\n27. Wei J, Wang X, Schuurmans D, Bosma M, Ichter B, Xia F, et al. Chain-of-thought prompting elicits reasoning in large\nlanguage models. arXiv. Preprint posted online January 28, 2022. [FREE Full text]\n28. Wang X, Wei J, Schuurmans D, Le Q, Chi E, Narang S, et al. Self-consistency improves chain of thought reasoning in\nlanguage models. arXiv. Preprint posted online March 21, 2022. [FREE Full text]\n29. Li Y, Lin Z, Zhang S, Fu Q, Chen B, Lou JG, et al. Making language models better reasoners with step-aware verifier. In:\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).\nPresented at: 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers); July 9-14,\n2023, 2023; Toronto, ON. [doi: 10.18653/v1/2023.acl-long.291]\n30. NIH investigator manual for human subjects research. Office of Intramural Research. Office of Human Subjects Research\nProtections. URL: https://ohsrp.nih.gov/confluence/display/ohsrp/\nChapter+1+-+Types+of+Research_+Human+Subjects+Research+Vs.+Not+Human+Subjects+Research[accessed 2024-01-31]\n31. Decherchi S, Pedrini E, Mordenti M, Cavalli A, Sangiorgi L. Opportunities and challenges for machine learning in rare\ndiseases. Front Med (Lausanne). Oct 5, 2021;8:747612. [FREE Full text] [doi: 10.3389/fmed.2021.747612] [Medline:\n34676229]\n32. Sutton RT, Pincock D, Baumgart DC, Sadowski DC, Fedorak RN, Kroeker KI. An overview of clinical decision support\nsystems: benefits, risks, and strategies for success. NPJ Digit Med. Feb 06, 2020;3(1):17. [FREE Full text] [doi:\n10.1038/s41746-020-0221-y] [Medline: 32047862]\n33. Eckleberry-Hunt J, Lick D, Hunt R. Is medical education ready for generation Z? J Grad Med Educ. Aug 2018;10(4):378-381.\n[FREE Full text] [doi: 10.4300/JGME-D-18-00466.1] [Medline: 30154963]\n34. codiag-public / dc3. GitHub. URL: https://github.com/codiag-public/dc3/blob/master/cases.url [accessed 2024-01-31]\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 10https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\n35. Johnson A, Pollard T, Mark R. MIMIC-III clinical database (version 1.4). PhysioNet. 2016. URL: https://physionet.org/\ncontent/mimiciii/1.4/ [accessed 2024-01-31]\nAbbreviations\nAI: artificial intelligence\nCME: continuing medical education\nDC3: diagnostic case challenge collection\nLLM: large language model\nMIMIC-III: Medical Information Mart for Intensive Care-III\nUSMLE: United States Medical Licensing Examination\nEdited by G Eysenbach, K Venkatesh, MN Kamel Boulos; submitted 30.07.23; peer-reviewed by L Modersohn, S Ghanvatkar; comments\nto author 20.10.23; revised version received 07.11.23; accepted 11.12.23; published 13.02.24\nPlease cite as:\nAbdullahi T, Singh R, Eickhoff C\nLearning to Make Rare and Complex Diagnoses With Generative AI Assistance: Qualitative Study of Popular Large Language Models\nJMIR Med Educ 2024;10:e51391\nURL: https://mededu.jmir.org/2024/1/e51391\ndoi: 10.2196/51391\nPMID: 38349725\n©Tassallah Abdullahi, Ritambhara Singh, Carsten Eickhoff. Originally published in JMIR Medical Education\n(https://mededu.jmir.org), 13.02.2024. This is an open-access article distributed under the terms of the Creative Commons\nAttribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work, first published in JMIR Medical Education, is properly cited. The complete bibliographic\ninformation, a link to the original publication on https://mededu.jmir.org/, as well as this copyright and license information must\nbe included.\nJMIR Med Educ 2024 | vol. 10 | e51391 | p. 11https://mededu.jmir.org/2024/1/e51391\n(page number not for citation purposes)\nAbdullahi et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX"
}