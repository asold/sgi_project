{
  "title": "Efficient Lightweight Image Denoising with Triple Attention Transformer",
  "url": "https://openalex.org/W4393147995",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2113191491",
      "name": "Yubo Zhou",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2098127262",
      "name": "Jin Lin",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2703263914",
      "name": "Fangchen Ye",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2125810221",
      "name": "Yanyun Qu",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2095844698",
      "name": "Yuan Xie",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2113191491",
      "name": "Yubo Zhou",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2098127262",
      "name": "Jin Lin",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2703263914",
      "name": "Fangchen Ye",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2125810221",
      "name": "Yanyun Qu",
      "affiliations": [
        "Xiamen University"
      ]
    },
    {
      "id": "https://openalex.org/A2095844698",
      "name": "Yuan Xie",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2799192307",
    "https://openalex.org/W6761664391",
    "https://openalex.org/W6761784800",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6786585107",
    "https://openalex.org/W6810161528",
    "https://openalex.org/W1906770428",
    "https://openalex.org/W3113887880",
    "https://openalex.org/W2105119246",
    "https://openalex.org/W3119866685",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2048695508",
    "https://openalex.org/W2832157980",
    "https://openalex.org/W6647720530",
    "https://openalex.org/W6790749177",
    "https://openalex.org/W6800689796",
    "https://openalex.org/W3153623182",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W2132984323",
    "https://openalex.org/W2520164769",
    "https://openalex.org/W2727642811",
    "https://openalex.org/W3174300208",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4309872131",
    "https://openalex.org/W2743529218",
    "https://openalex.org/W6746466744",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W2618839904",
    "https://openalex.org/W2498645844",
    "https://openalex.org/W2798427787",
    "https://openalex.org/W2899544788",
    "https://openalex.org/W4362467143",
    "https://openalex.org/W2970318705",
    "https://openalex.org/W3041692653",
    "https://openalex.org/W3212228063",
    "https://openalex.org/W3011614284",
    "https://openalex.org/W6790137093",
    "https://openalex.org/W3081639259",
    "https://openalex.org/W2508457857",
    "https://openalex.org/W3104725225",
    "https://openalex.org/W6757937817",
    "https://openalex.org/W6760516191",
    "https://openalex.org/W4313344036",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W4287682995",
    "https://openalex.org/W4214588794",
    "https://openalex.org/W3099686304",
    "https://openalex.org/W3106758205",
    "https://openalex.org/W4312938066",
    "https://openalex.org/W2963315679",
    "https://openalex.org/W1990592195",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W3182000414",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3000775737",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W4287330714",
    "https://openalex.org/W2963263347",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W2955105324",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2983315964",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4281570575",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2938386730",
    "https://openalex.org/W2997508111",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2962767526",
    "https://openalex.org/W4225672218",
    "https://openalex.org/W2964125708",
    "https://openalex.org/W4382450087",
    "https://openalex.org/W2963725279",
    "https://openalex.org/W4310278871"
  ],
  "abstract": "Transformer has shown outstanding performance on image denoising, but the existing Transformer methods for image denoising are with large model sizes and high computational complexity, which is unfriendly to resource-constrained devices. In this paper, we propose a Lightweight Image Denoising Transformer method (LIDFormer) based on Triple Multi-Dconv Head Transposed Attention (TMDTA) to boost computational efficiency. LIDFormer first implements Discrete Wavelet Transform (DWT), which transforms the input image into a low-frequency space, greatly reducing the computational complexity of image denoising. However, the low-frequency image lacks fine-feature information, which degrades the denoising performance. To handle this problem, we introduce the Complementary Periodic Feature Reusing (CPFR) scheme for aggregating the shallow-layer features and the deep-layer features. Furthermore, TMDTA is proposed to integrate global context along three dimensions, thereby enhancing the ability of global feature representation. Note that our method can be applied as a pipeline for both convolutional neural networks and Transformers. Extensive experiments on several benchmarks demonstrate that the proposed LIDFormer achieves a better trade-off between high performance and low computational complexity on real-world image denoising tasks.",
  "full_text": "Efficient Lightweight Image Denoising with Triple Attention Transformer\nYubo Zhou1*, Jin Lin1*, Fangchen Ye1, Yanyun Qu1† , Yuan Xie2†\n1School of Informatics, Xiamen University, Fujian, China\n2School of Computer Science and Technology, East China Normal University, Shanghai, China\nybzhou@stu.xmu.edu.cn, yxie@cs.ecnu.edu.cn, yyqu@xmu.edu.cn\nAbstract\nTransformer has shown outstanding performance on image\ndenoising, but the existing Transformer methods for image\ndenoising are with large model sizes and high computational\ncomplexity, which is unfriendly to resource-constrained de-\nvices. In this paper, we propose a Lightweight Image De-\nnoising Transformer method (LIDFormer) based on Triple\nMulti-Dconv Head Transposed Attention (TMDTA) to boost\ncomputational efficiency. LIDFormer first implements Dis-\ncrete Wavelet Transform (DWT), which transforms the in-\nput image into a low-frequency space, greatly reducing the\ncomputational complexity of image denoising. However, the\nlow-frequency image lacks fine-feature information, which\ndegrades the denoising performance. To handle this problem,\nwe introduce the Complementary Periodic Feature Reusing\n(CPFR) scheme for aggregating the shallow-layer features\nand the deep-layer features. Furthermore, TMDTA is pro-\nposed to integrate global context along three dimensions,\nthereby enhancing the ability of global feature representa-\ntion. Note that our method can be applied as a pipeline for\nboth convolutional neural networks and Transformers. Ex-\ntensive experiments on several benchmarks demonstrate that\nthe proposed LIDFormer achieves a better trade-off between\nhigh performance and low computational complexity on real-\nworld image denoising tasks.\nIntroduction\nImage denoising is an important task in image restoration\nand is widely applied to many scenarios (Anwar, Khan, and\nBarnes 2020). With the rise of deep learning, image de-\nnoising methods have made great progress (Tai et al. 2017;\nChen and Pock 2016; Zhou et al. 2020; Mao, Shen, and\nYang 2016; Ulyanov, Vedaldi, and Lempitsky 2018; Cheng\net al. 2021). However, the existing models mostly require\nhigh computational complexity in order to obtain good per-\nformance, which may hinder the widespread application of\nmethods on resource-limited devices such as mobile phones,\nrobotics, and some edge devices. Efficient and lightweight\ndenoising methods attract more and more attention.\nWith the rising up of deep learning, convolutional neu-\nral networks are used for image denoising. The method (Xu,\n*These authors contributed equally.\n†Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n0 20 40 60 80 100 120 140 160\nGFLOPs(G)\n39.2\n39.3\n39.4\n39.5\n39.6\n39.7Average…PSNR(dB)\nLIDFormer\n(ours)\nLiNAFNet\n(ours)\nInvDN\n(CVPR2021)\nDeamNet\n(CVPR2021)\nVDN\n(CVPR2019)DANet\n(ECCV2020)\n(AAAI2023)\nADFNet\nFigure 1: Performance and FLOPs cost of LIDFormer com-\npared to other popular efficient and lightweight denoising\nmethods on SIDD. The LiNAFNet is formed by applying\nthe module from LIDFormer to NAFNet (Chen et al. 2022a).\nOur method achieves a better trade-off between performance\nand FLOPs cost on image denoising tasks.\nYang, and Jiang 2017; Yuan, Liu, and Liang 2023) reduces\ncomputations and storage costs by utilizing the sparse nature\nof images. It has achieved remarkable results in denoising\neffect and computational speed. Moreover, the approach (Yu\net al. 2018) based on the joint loss function is a new idea pro-\nposed in recent years, which improves the denoising effect\nby simultaneously considering the local and global informa-\ntion of the image. Meanwhile, the method (Jin et al. 2019)\nbased on depthwise separable convolution is widely used in\nimage denoising tasks. It improves the denoising efficiency\nand accuracy by separating spatial and channel dimensions\nwhile reducing model parameters and computational costs.\nThis method has been shown to be effective in many im-\nage denoising tasks, especially for practical applications and\nlarge-scale data.\nAlthough the above methods have accelerated the denois-\ning process in different aspects, the computational efficiency\nof current lightweight image denoising models still has re-\nsource barriers compared with advanced semantic tasks such\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7704\nas image classification. Therefore, in order to narrow the\ngap with advanced semantic tasks and realize the compu-\ntational efficiency of image denoising algorithms adapted to\npractical devices, image denoising methods with less than\n5 GFLOPs of computational cost are worth exploring and\ndesigning.\nIn response to the above problems, we propose a\nLightweight Image Denoising Transformer method (LID-\nFormer) based on Discrete Wavelet Transform (DWT) (Mal-\nlat 1989) and Triple Multi-Dconv Head Transposed Atten-\ntion (TMDTA), which aims to produce excellent perfor-\nmance while being computationally efficient. To be spe-\ncific, our proposed lightweight feature module utilizes DWT\n(Mallat 1989) to losslessly transform the input image into a\nlow-resolution space composed of high-frequency and low-\nfrequency information sets. Notably, DWT (Mallat 1989)\nis an established lossless frequency-domain transformation\nfunction that is not involved in model training, so it can be\nconsidered a non-computationally consuming module.\nMoreover, Complementary Periodic Feature Reusing\n(CPFR) is introduced to mitigate the loss of information\ndue to low resolution. Through continuous complementary\nresidual connection, CPFR combines the historical feature\nwith the current feature in a weighted and complementary\nway. It also avoids the discarding of valid features due to the\nrefinement of the feature information as the network level\ngoes deeper. In particular, the complementary residual con-\nnections are learnable channel attention functions.\nFrom another point of view, the multi-head self-attention\n(MHSA) proposed by Transformer (Vaswani et al. 2017) can\neffectively refine characteristic information and overcome\nthe “short-range” effect of local convolution. However, since\nthe global pixel-based computation of self-attention is too\nlarge (proportional to the resolution of the features), it is usu-\nally not directly applicable to image restoration tasks. The\nfeature lightweighting strategy propounded by LIDFormer\nallows all the computations of features to be performed in\nlow-scale space, thus making global self-attention possible\nfor resource-constrained devices. Based on the above dis-\ncussion, LIDFormer introduces TMDTA, namely horizontal\nself-attention, vertical self-attention and channel-wise self-\nattention, for collaborative computing. Finally, LIDFormer\nachieves a computational cost close to image classification\nwith 2.8 GFLOPs. More intuitively, as shown in Fig. 1, LID-\nFormer significantly outperforms the majority of popular ef-\nficient image denoising methods and has much lower com-\nputational complexity than these approaches.\nWe summarize the main contributions of this work as fol-\nlows:\n• We propose an efficient and lightweight image denois-\ning method based on DWT and TMDTA (namely LID-\nFormer). Our LIDFormer provides a novel pipeline to re-\nduce computational complexity, and it is a universal and\ngeneralizable efficient method.\n• We design the Complementary Periodic Feature Reusing\nmodule (CPFR), which can effectively overcome the\nproblem of compact and insufficient feature information\ncaused by feature lightweighting. The reused effect can\nsolve the issue of catastrophic forgetting to a certain ex-\ntent and effectively retain low-frequency information.\n• We introduce a Triple Multi-Dconv Head Transposed At-\ntention module (TMDTA) to improve the performance of\nconventional multi-head self-attention based on feature\npixels in a multi-dimensional and lightweight manner.\n• Extensive experiments demonstrate that our LIDFormer\nachieves a better trade-off between performance and\ncomputational complexity. The pipeline can also be gen-\neralized to different image denoising methods.\nRelated Work\nDeep Learning-based Image Denoising\nImage denoising tasks aim to restore a high-quality image\nfrom the noisy observation (Chen et al. 2022a). In recent\nyears, with the rise of deep learning technology, CNN-based\nnetwork architectures (Tai et al. 2017; Chen and Pock 2016;\nZamir et al. 2020, 2021; Zhang et al. 2020, 2017; Cheng\net al. 2021) have achieved significant success in the field\nof image denoising, and their performance is far superior to\nthat of traditional restoration methods (Dabov et al. 2008;\nGu et al. 2014; Xu et al. 2017; Yair and Michaeli 2018;\nHe, Sun, and Tang 2010). These deep networks have dif-\nferent characteristics in their designs, and most of them\n(Wang et al. 2022; Yue et al. 2020; Zamir et al. 2021; Zhang\net al. 2021) are based on the UNet (Ronneberger, Fischer,\nand Brox 2015) architecture, which uses skip-connections\nto fuse the pixel-level features of the image with semantic-\nlevel features for better restoration results.\nAs Transformer-based models (Vaswani et al. 2017; Fe-\ndus, Zoph, and Shazeer 2022; Radford et al. 2018) have\nachieved excellent performance in the NLP domain, more\nand more vision applications, both high-level tasks (Gra-\nham et al. 2021; Liu et al. 2021b; Carion et al. 2020; Xie\net al. 2021) and low-level tasks (Liang et al. 2021a; Kumar,\nWeissenborn, and Kalchbrenner 2020; Zamir et al. 2022;\nWang et al. 2022), have tried to introduce it recently due\nto its strong capability of modeling long-range relations.\nMost of them have achieved better results compared to con-\nvolutional networks. The Vision Transformer (ViT) (Doso-\nvitskiy et al. 2020) divides an image down into a series of\npatches (local windows) and discovers how they relate to\none another. Benefiting from the powerful multi-head self-\nattention mechanism, its ability to calculate long-distance\ninformation interaction is particularly outstanding. Some ex-\nisting works (Zamir et al. 2022; Chen et al. 2021, 2022b)\nhave achieved promising performance by applying the ViT\narchitecture to image denoising while alleviating the pro-\nhibitively expensive training complexity. Vision Transform-\ners have shown their strong potential as an alternative to\nthe previously dominant CNNs (Liang et al. 2021b). Re-\ncently, Restormer (Zamir et al. 2022) is proposed as a high-\nperformance Transformer model for image denoising. It in-\ntroduces a gating mechanism based on depth-wise convolu-\ntions to perform controlled feature transformation. Although\nthis method achieves state-of-the-art denoising performance,\nit also sacrifices a large amount of computational cost. In this\npaper, we propose a computationally friendly method named\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7705\nDWT PFR\nBlocks\nPFR\nBlocks\nPFR\nBlocks\nHW C44\nHW 2C88\nHW 8C16 16\nPFR\nBlocks\nPFR\nBlocks\nHW 16C32 32\nC\nC\nSkip Connections\nTMDTA\nPFR\nBlocks\nGDFN\nC\nUPUP\nTMDTA Triple Multi-Dconv Head Transposed Attention\nGated DconvFeed-Forward NetworkGDFN\nDWTDWT  Discrete Wavelet Transform\nUPUP Pixel-Shuffle Upsample\nDepth-Downsample\nDepth-Upsample\nC\nPDconv Point-wise+Depth-wise\nConvolution\nElement-wise Addition\nConcatenation\nS\nS\nS Channel Split\nS\nHW C44\nHW 2C44\nHW 4C88\nHW 2C88\nHW 4C16 16\nHW 16C32 32\nHW 4C16 16\nHW 4C16 16\nHW 8C16 16\nHW 2C88\nHW 4C88\nHW C44\nHW 2C44\n0F\nNorm Layer Normalization\nR Reshape\nMatrix Multiplication\n\n\nQs\nKs\nAs\nVs\n\n\n\n\n\n\n\n\n\nDWT PFR\nBlocks\nPFR\nBlocks\nPFR\nBlocks\nHW C44\nHW 2C88\nHW 8C16 16\nPFR\nBlocks\nPFR\nBlocks\nHW 16C32 32\nC\nC\nSkip Connections\nTMDTA\nPFR\nBlocks\nGDFN\nC\nUP\nTMDTA Triple Multi-Dconv Head Transposed Attention\nGated DconvFeed-Forward NetworkGDFN\nDWT  Discrete Wavelet Transform\nUP Pixel-Shuffle Upsample\nDepth-Downsample\nDepth-Upsample\nC\nPDconv Point-wise+Depth-wise\nConvolution\nElement-wise Addition\nConcatenation\nS\nS\nS Channel Split\nS\nHW C44\nHW 2C44\nHW 4C88\nHW 2C88\nHW 4C16 16\nHW 16C32 32\nHW 4C16 16\nHW 4C16 16\nHW 8C16 16\nHW 2C88\nHW 4C88\nHW C44\nHW 2C44\n0F\nNorm Layer Normalization\nR Reshape\nMatrix Multiplication\n\n\nQs\nKs\nAs\nVs\n\n\n\n\n\n\n\nHW C44\nTMDTANorm\nQ\nK\nV\nQs\nKs\nVs\n\nAs\n\nR\nR\nR R\n1x1\n\nPDconv PDconv PDconv \nh\nw × c h × c h × w \nw c\nh × c \nw × c \nh × w \nh\nh\nw\nw c\nc\nh\nw × c h × c h × w \nFigure 2: Illustration of our proposed LIDFormer. First, the input image is transformed into a low-resolution frequency-domain\nspace using DWT. Then, the CPFR module is used to combine features from historical and current periods, effectively mul-\ntiplexing the features and avoiding the issue of shallow features being forgotten due to the filtering of depth information.\nAdditionally, LIDFormer incorporates TMDTA to capture global feature information in three dimensions, which approximates\ntraditional high-computation full-pixel self-attention.\nLIDFormer for image denoising. Our method reduces the\ncomputational workload of existing models without com-\npromising the capability of denoising.\nEfficient and Lightweight Image Denoising\nAlthough the performance of the image denoising meth-\nods mentioned above improves significantly, they mostly\nsuffer from high computational costs, which do not favor\nresource-constrained devices such as smart phones. To re-\nlieve the computation burden and improve efficiency, there\nare emerging efforts to design efficient and lightweight im-\nage denoising approaches. Zhang et al.(Zhang, Zuo, and\nZhang 2018) propose a new CNN model based on DnCNN\n(Zhang et al. 2017) , namely FFDNet, for rapid, effec-\ntive, and adjustable discriminative denoising. FFDNet uses\ndownsampled sub-images, which significantly speeds up\ntraining and testing while also expanding the receptive area.\nYue et al.(Yue et al. 2019) utilize a new variational infer-\nence method (VDN) to fast infer both the underlying clean\nimage and the noise distribution from an observed noisy\nimage in a unique Bayesian framework. DANet (Yue et al.\n2020) approximates the joint distribution from two different\nfactorized forms in a dual adversarial manner. The joint dis-\ntribution theoretically contains more complete information\nunderlying the data set, which significantly reduces the time\nrequired to collect clean-noisy image pairs. Zou et al.(Zou\net al. 2023) make contributions for efficient image denoising\nby a lightweight network and a novel distillation algorithm\nwith retargeting supervision. Another related work is Thun-\nder (Zhou et al. 2022), which leverages the RGB thumbnail\ninstead of the feature subspace to accelerate the denoising\nprocess. More specifically, it adopts the subspace projection\nmethod to guarantee the denoising effect while refining the\nthumbnail. Unlike Thunder (Zhou et al. 2022), our method\nyields better denoising effects with faster calculation effi-\nciency by incorporating the DWT module and the TMDTA\nmodule.\nMethod\nOverview of LIDFormer\nAs shown in Fig. 2, LIDFormer consists of three main\ncomponents: (1) A feature lightweighting module based on\nDWT, which maps a given noisy image x from RGB space\nto low-resolution frequency-domain space through a double\ndiscrete wavelet transform (DWT); (2) A Complementary\nPeriodic Feature Reusing (CPFR) module, which performs\nnon-linear operations on low-resolution frequency-domain\nfeatures; (3) A Triple Multi-Dconv Head Transposed Atten-\ntion (TMDTA) module, which introduces three-dimensional\nco-computation of horizontal, vertical and channel self-\nattention. First of all, the input image is transformed into\na low-resolution frequency-domain space using DWT to al-\nleviate the computational bottleneck. Then, the function of\nfeature multiplexing is realized through the CPFR module.\nCPFR can effectively combine the features of different pe-\nriods and avoid the problem of shallow features being for-\ngotten due to the filtering of depth information. Besides,\nTMDTA is utilized to obtain the global information of fea-\ntures in three dimensions and approximately replaces the tra-\nditional high-computation full-pixel self-attention. Note that\nthe upsampling module is implemented directly by using the\nconventional non-computationally intensive “pixel-shuffle”\noperation, which compresses the feature channel, and then\nthe compressed part is filled in the channel to achieve loss-\nless amplification feature resolution. The specific process is\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7706\nDWTDWT Transformer\nDWTDWTDWT IWTIWT\n IWTIWT\n（H，W，3）\n(H，W)×3 (H/2，H/2)×3×4 (H/4，H/4)×3×4×4 (H/4，H/4)×3×4×4 (H/2，H/2)×3×4 (H，W)×3\n（H，W，3）\nFigure 3: Illustration of the double DWT feature lightweighting network. Here, we choose Transformer-based models as the\nbackbone network. A color image with three RGB channels is used as the initial input. The channel of middle input is increased\nby 16 times compared to the original image by a double DWT, while the resolution is decreased by 16 times.\nexpressed as: (B, C × γ2 , H, W) −→(B, C, H × γ , W ×\nγ). The above process can be expressed as:\nF0 = fDWT (x),\nFn = fUnetCPFR (F0),\nIRestored = x + fUP (Fn).\n(1)\nAmong them, fDWT (•) means that the double discrete\nwavelet transform performs frequency-domain compression\non the input noise image x, and fUnetCPFR (•) means the\nnoise extraction function for low-resolution features.\nDWT-based Feature Lightweighting\nAt present, the denoising models based on the deep network\nmainly rely on the noise extraction method for image de-\nnoising. The details are as follows:\ny = x + η, (2)\nwhere y denotes the noisy image, x denotes the denoised\nclear image, both represented as vectors, and η is the noise\ndistribution of the noisy image. Moreover, in most deep\nlearning-based models, η is usually learned from the noisy\nimage in an end-to-end image denoising task, as follows:\nη = F(y), (3)\nwhere F(•) denotes the noise extractor in the denoising pro-\ncess. As shown in Fig. 2, before noise extraction, the origi-\nnal input image will be converted to the frequency-domain\nspace through the DWT-based resolution compression mod-\nule, and the input image resolution will be compressed to 1\n16\nof the original size by the double DWT, which can greatly re-\nduce the computational complexity while keeping the num-\nber of channels and model parameters unchanged. The spe-\ncific low-resolution frequency-domain compression module\ncan be described as:\nη = F(DW T(y)), (4)\nwhere DW T(•) denotes the double DWT transform func-\ntion, which is an established lossless frequency-domain\ntransform function. What needs to be emphasized here is\nthat our method uses the classic Haar (Mallat 1989) as the\ndiscrete wavelet and aims to decompose the input image\nX ∈ RH×W×3 into 48 low-resolution frequency-domain\nsub-features fi ∈ R\nH\n4 ×W\n4 , i∈ [1, . . . ,48].\nThe image denoising process based on the DWT makes\nfeatures lightweight, as shown in Fig. 3. The input image is a\ncolor one with three RGB channels. Through two first-order\nDWT transformations (i.e., double DWT), the feature chan-\nnels of middle input are expanded by 16 times compared to\nthe original image, while the feature resolution is reduced by\n16 times, achieving a lossless feature lightweighting effect.\nComplementary Periodic Feature Reusing\nAlthough feature lightweighting based on the double DWT\ngreatly reduces the computational complexity of the model,\nthe intermediate feature is compressed 16 times com-\npared to the original model without lightweighting, result-\ning in a serious shortage of feature information. Therefore,\nLIDFormer proposes the Complementary Periodic Feature\nReusing (CPFR) module, which aims to reuse historical fea-\ntures and fill in the shallow historical feature information\nlost during the learning process of compact features. First,\nthe compact features are expanded by a factor of two in the\nchannel dimension, and a simple linear feature embedding\nis done by using a generalized 3 × 3 convolution (CONV3\nbelow) to double the information space of compact features:\nF0 = CONV 3(DW T(x)). (5)\nThen, as shown in Fig. 4, in order to make full use of the\nextended feature information and perform effective histor-\nical feature reusing, our method calculates the features of\nthe next stage while retaining the feature information of the\nprevious stage. The simple CPFR is expressed as follows:\nfn =\n\u001a\nT · Fn(fn−1) + (1− T) · fn−1, n = 2× k\nfn−1, else\n(6)\nwhere fn denotes the upper or lower half of the extended\nfeature, Fn(•) represents the processing unit (i.e., TMDTA),\nand T is the complementary coefficient. In the experiment of\nthis method, the value of T is set to 0.5.\nComplementary Adaptive Channel Attention\nSince the information of each feature is constantly chang-\ning and the information of deep and shallow features is not\nuniform at each pixel, it is not friendly to set the value of\nthe complementary coefficient rigidly. To address the above\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7707\nPFR Block\nCA\nTMDTA\n CA\n\nCAf\nf\nf\n2f\n1f\n2f\nCA\nFigure 4: Illustration of the Complementary Periodic Feature\nReusing (CPFR) module. CA denotes the channel attention\nfunction. f1 and f2 denote the historical feature and the cur-\nrent feature, respectively.\nproblem, CPFR uses Complementary Adaptive Channel At-\ntention (CACA) to compensate for the deficiency of hard\ncomplementary coefficients, which is represented as fol-\nlows:\ngn = Fn(fn−1)\nfn =\n\u001a\nFCA\nn1 (gn) · gn + FCA\nn2 (fn−1) · fn−1, n = 2× k\nfn−1, else\n(7)\nwhere FCA\nn represents the channel attention function, as\nshown in the CA module in Fig. 4, which consists of sim-\nple convolution, activation, pooling, and other basic con-\nstructions. More importantly, the convolution calculation is\nprocessed on the pooled single-point multi-channel features;\nthat is, the overall calculation is done with the feature reso-\nlution of only one, which is almost negligible compared to\nthe overall feature calculation.\nIn addition, CPFR imposes a complementary constraint\non adaptive channel attention and construes this part with a\nsimple MSE loss. The effectiveness of the complementary\nconstraint has been demonstrated through experiments. The\ncomplementary constraint is shown below:\nLCA =\nnX\ni=1\n\r\rFCA\ni1 (Fi(fi−1)) +FCA\ni2 (fi−d) − ONEs\n\r\n\r\n2 ,\n(8)\nwhere n denotes the number of computing units and ONEs\ndenotes a matrix that elements with values of one in the same\ndimension as the outputs of FCA(fi−d), achieving pixel-\nlevel complementarity constraints.\nTriple Multi-Dconv Head Transposed Attention\nIn addition to the above approaches, LIDFormer considers\na very significant issue: the limitation of Transformer in\nimage restoration lies in the huge computational complex-\nity caused by the demand to complete high-resolution cor-\nrelation calculations between various pixels. As shown in\nFig. 3, the pixel magnification of intermediate features has\nbeen scaled by 16 times, and the computation can be re-\nduced by 256 times if the traditional self-attention mech-\n* ** ** ** ** ** ** * * *\nLayer Norm \nPDConv\nHeight Attention\nWidth Attention\nChannel Attention\nC Projection GDFN\nC\nH\nHeight Attention\nMHA-H\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\nC\nH\nWidth Attention\nMHA-W\nMHA-W\nMHA-W\nMHA-W\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\n*\nC\nH\nChannel Attention\nMHA-C\nMHA-C\nMHA-C\nMHA-C\n*\n*\n*\nMHA-H\nMHA-H\nMHA-H\nFigure 5: Illustration of the Triple Multi-Dconv Head Trans-\nposed Attention (TMDTA) moudle. The attention of char-\nacteristic pixels is decomposed into three directions of\nself-attention for cooperative computation: horizontal self-\nattention, vertical self-attention, and channel self-attention.\nanism of full pixels is adopted. Even so, when it comes\nto higher resolution images, there is still the problem of\n“high computational complexity”. To this end, considering\nthe information redundancy of full-pixel self-attention, LID-\nFormer proposes Triple Multi-Dconv Head Transposed At-\ntention (TMDTA). It decomposes the attention of character-\nistic pixels into three directions of self-attention for coop-\nerative computation: horizontal self-attention, vertical self-\nattention, and channel self-attention.\nAs shown in Fig. 5, the input features first pass through\nthe “Layer Norm + PDConv” layer to generate the lo-\ncally enriched query(Q), key(K) and value(V ). The\nLayer Norm (LN) denotes the regular layer normaliza-\ntion, and the PDConv denotes the combination of Point-\nwise Convolution (PWConv) and Depthwise Convolution\n(DWConv). Then, the query(Q) and key(K) are reshaped\nin three-dimensional directions, resulting in the horizontal\nqueryH(QH) and keyH(KH), the vertical queryW (QW )\nand keyW (KW ), and the queryC(QC) and keyC(KC), re-\nspectively. Then, matrix multiplication is performed on them\nrespectively to generate three transposed attention matrices\nwith sizes of RH×H, RW×W and RC×C, instead of the\nregular attention matrix RHW ×HW of characteristic pix-\nels (Vaswani et al. 2017; Dosovitskiy et al. 2020). It is\nworth noting that all three processes are transformed from\nquery(Q), key(K) and are synergistically related to each\nother. In general, the process definition of TMDTA is as fol-\nlows:\nX′ = Wp Atention(Qs,Ks, Ys) +X,\nAtention(Qs,Ks, Vs) = Concat(AH, AW, AC),\nAH = VH × Softmax(KH × QH/αH),\nAW = VW × Softmax(KW × QW/αW ),\nAC = VC × Softmax(KC × QC/αC),\n(9)\nwhere X and X′ denote the input and output\nfeatures; Qi ∈ (RWC ×H, RHC×W , RHW ×C),\nKi ∈ (RH×WC , RW×HC , RC×HW ), Vi ∈\n(RWC ×H, RHC×W , RHW ×C) denotes the horizontal,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7708\nBaseline DWT CPFR CACA TMDTA GFLOPs PSNR\n✓ × × × × 140 40.02\n✓ ✓ × × × 8.75 39.55\n✓ ✓ ✓ × × 2.82 39.55\n✓ ✓ ✓ ✓ × 2.83 39.58\n✓ ✓ ✓ ✓ ✓ 2.83 39.62\nTable 1: Ablation experiments are conducted with different\nmodules of the LIDFormer.\nvertical, and channel reshaping by the generated query(Q),\nkey(K) and value(V ), respectively; αi denotes a learnable\nscaling parameter to control the size of the dot product of\nQi and Ki before applying the activation function. In the\nabove expression, i ∈ [H, W, C].\nExperiments\nImplementation Details\nTo ensure the fairness of the comparison between meth-\nods, our method and conventional denoising methods adopt\nthe same classic denoising dataset SIDD (Abdelhamed, Lin,\nand Brown 2018) for model training. Moreover, the trained\nmodel is evaluated on two publicly available datasets, SIDD\n(Abdelhamed, Lin, and Brown 2018) and DND (Plotz and\nRoth 2017). In our work, the Adam optimizer withβ1 = 0.9,\nβ2 = 0.999 and L1 loss are utilized to train the model. The\ntraining process takes 300K iterations with the learning rate\nbeing initially set to 3e-4. And the learning rate will grad-\nually decrease to 1e-6 by using cosine annealing technique\n(Loshchilov and Hutter 2016). For iterative learning, 128 ×\n128 image patches with RGB channels are used to train a\nlightweight denoising model. The mini-batch size is set to\n16. Besides, the resolution of image patches and the batch\nsize are updated at iteration numbers of 92k, 156K, 204K,\n240K, and 276K to (1602, 8), (1922, 6), (2562, 4), (3202, 2),\nand (3842, 1), respectively. Horizontal and vertical flipping\nare implemented for data augmentation.\nEvaluation Metrics\nObjective criteria, i.e., peak signal-to-noise ratio (PSNR)\nand structural similarity index (SSIM), are adopted to eval-\nuate the performance of denoising models. The two met-\nrics are both calculated on the Y channel of the YCbCr\nspace. Besides, Giga Floating-point Operations Per second\n(GFLOPs) is used as the efficient evaluation criterion for the\ndenoising model in our work.\nAblation Study\nWe conduct ablation studies to validate the effect of each\ncomponent in our proposed method. All the experiments use\nRestormer (Zamir et al. 2022) as the baseline model. The\nquantity results are shown in Table 1.\nEffectiveness of Double DWT. As shown in Table 1, us-\ning the double DWT is able to compress the original model\nby 16 times with high performance. It can be seen from the\ntable that as the computational complexity of the model re-\nduces, the performance also decreases slightly. Therefore, in\norder to verify whether the method of feature lightweighting\nis feasible and universal, we have carried out corresponding\nexperiments on different methods, as shown in Table 2. It\ncan be observed from the table that the feature lightweight-\ning method will cause a very serious decline in the perfor-\nmance of the model, especially in the experiment on CBD-\nNet (Guo et al. 2019) (dwtCBDNet in the table), where the\nperformance of the model encountered a catastrophic muta-\ntion. From this, the corresponding experimental conclusion\ncan be drawn: feature lightweighting by using DWT can sig-\nnificantly reduce the computational complexity of the model\nand alleviate the computational pressure, but it cannot guar-\nantee the performance of the lightweight model.\nEffectiveness of CPFR. As shown in Table 1, we aim to\nexplore efficient image denoising methods whose compu-\ntational complexity approximates image classification tasks\n(i.e., below 5 GFLOPs). By reducing the number of mod-\nules in the original model and reusing historical features,\nthis deliberate architectural refinement achieves an efficient\ndenoising model of 2.82 GFLOPs in the table. It is worth\nnoting that the utilization of CPFR has greatly saved the\nperformance of dwtCBDNet in Table 2. The results show\nthat our proposed CPFR module further reduces the compu-\ntational complexity of the efficient denoising model, which\nhas the same performance advantages as the model of 8.75\nGFLOPs, verifying the effectiveness of this module.\nEffectiveness of CACA. Since the complementary coeffi-\ncient set in the simple CPFR module is a constant value\n(i.e., 0.5), the flexibility of feature learning is limited. There-\nfore, Complementary Adaptive Channel Attention (CACA)\nis proposed to release the pressure of the given value, mak-\ning CPFR adaptively complementary. To combine historical\nfeatures and current deep features, adaptive channel atten-\ntion considers freely choosing summation coefficients. As\nshown in Table 1, compared with the simple CPFR module,\nthe introduction of the CACA module has a certain improve-\nment effect. In addition, the adaptive learning method can\nenhance the generalization of our method and avoid the dis-\ncomfort of the given value in other methods.\nEffectiveness of TMDTA. The channel-wise multi-head\nself-attention designed in the original Restormer (Zamir\net al. 2022) effectively overcomes the inadequacy of the\nTransformer’s (Vaswani et al. 2017) full-pixel self-attention\nin dense prediction tasks. However, channel-wise multi-head\nself-attention cannot completely replace the role of full-pixel\nself-attention because channel global information is unable\nto represent spatial global information. Therefore, as shown\nin Table 1, TMDTA is more effective than the original local-\nglobal representation learning by aggregating spatial global\ninformation and channel global information.\nApplication to Other Image Denoising Models\nTo demonstrate the versatility of the proposed lightweight\nframework (LIDFormer), a generalization analysis of our\nmethod is performed on three representative image denois-\ning approaches: Restormer (Zamir et al. 2022), CBDNet\n(Guo et al. 2019) and NAFNet (Chen et al. 2022a). All these\ndenoising models are retrained under the conditions of the\noriginal model. The results are presented in Table 2. It is\nshown that the proposed pipeline is generally applicable to\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7709\nMethods GFLOPs SIDD DND\nPSNR / SSIM PSNR / SSIM\nRestormer 140.00 40.02 / 0.9600 40.03 / 0.9560\ndwtRestormer 8.75 39.55 / 0.9326 39.65 / 0.9417\nLIDFormer 2.83 39.62 / 0.9557 39.76 / 0.9558\nCBDNet 34.00 39.30 / 0.9214 39.35 / 0.9351\ndwtCBDNet 2.20 27.68 / 0.7214 27.54 / 0.7134\nLiCBDNet 2.00 39.01 / 0.9014 39.06 / 0.9117\nNAFNet 65.00 39.77 / 0.9524 39.81 / 0.9561\ndwtNAFNet 4.00 39.43 / 0.9317 39.48 / 0.9342\nLiNAFNet 4.20 39.51 / 0.9437 39.62 / 0.9525\nTable 2: Generalization results of the efficient framework in\nLIDFormer for different image denoising methods. Among\nthem, Restormer is based on Transformer while CBDNet\nand NAFNet are based on convolutional neural networks.\nMethods GFLOPs / Params SIDD DND\nPSNR / SSIM PSNR / SSIM\nDnCNN - / 0.56M 23.66 / 0.5830 32.43 / 0.7900\nFFDNet - / 0.48M - / - 34.40 / 0.8474\nCBDNet 34 / 4.34M 33.28 / 0.8680 38.06 / 0.9421\nRIDNet 196.52 / 1.49M - / - 39.26 / 0.9528\nVDN 99.00 / 7.81M 39.26 / 0.9550 39.38 / 0.9518\nDANet 14.85 / 9.15M 39.25 / 0.9160 39.47 / 0.9548\nDeamNet 146.36 / 2.23M 39.35 / 0.9550 39.63 / 0.9555\nInvDN 47.80 / 2.64M 39.28 / 0.9550 39.57 / 0.9522\nThunder 18.81 / 2.68M 39.47 / 0.9570 39.57 / 0.9526\nADFNet 117.32 / 7.65M 39.63 / 0.9580 39.87 / 0.9555\nLIDFormer 2.83 / 2.72M 39.62 / 0.9575 39.76 / 0.9558\nTable 3: Quantitative comparison of LIDFormer with other\nefficient and lightweight denoising methods. The best per-\nformance is bolded and the second is underlined. “GFLOPs”\npresents the computational cost for peer 256 × 256 images.\n“Params” means the number of model parameters.\nexisting denoising methods, both convolutional neural net-\nworks and Transformers. Compared with the original model,\nthe performance of the model after computational complex-\nity reduction has a slight decrease, but the computational\ncomplexity has been optimized by more than 16 times, in-\ndicating that our pipeline is an effective and universally effi-\ncient method.\nComparison with State-of-the-Art Methods\nWe compare the proposed LIDFormer with popular state-of-\nthe-art efficient and lightweight methods for real-world im-\nage denoising, including DnCNN (Zhang et al. 2017), FFD-\nNet (Zhang, Zuo, and Zhang 2018), CBDNet (Guo et al.\n2019), RIDNet (Anwar and Barnes 2019), VDN (Yue et al.\n2019), DANet (Yue et al. 2020), DeamNet (Ren et al. 2021),\nInvDN (Liu et al. 2021a), Thunder (Zhou et al. 2022) and\nADFNet (Shen, Zhao, and Zhang 2023). The compared re-\nsults are shown in Table 3.\nFrom the table, it can be concluded that our LIDFormer\nachieves the best results in terms of computational complex-\nity and performance compromise. In particular, the perfor-\nmance of ADFNet (Shen, Zhao, and Zhang 2023) is slightly\nbetter than our method, but the FLOPs cost is more than ×\nDANetGT\n InvDN\nVDN ADFNet OursNoisy Image\nFigure 6: Visual comparison of LIDFormer with other effi-\ncient and lightweight denoising methods on SIDD.\nFigure 7: Visual comparison of LIDFormer with other effi-\ncient and lightweight denoising methods on DND.\n40 ours. Therefore, LIDFormer achieves a better trade-off\nbetween high performance and low computational complex-\nity. Moreover, the visual comparisons of our proposed LID-\nFormer with other methods are given in Fig. 6 and Fig. 7.\nOur proposed method is not inferior to other efficient and\nlightweight denoising methods in terms of visual effect.\nConclusion\nIn this paper, we propose an efficient and lightweight im-\nage denoising method named LIDFormer. LIDFormer in-\ncludes three parts: feature lightweighting based on double\nDiscrete Wavelet Transform (DWT), Complementary Peri-\nodic Feature Reusing (CPFR) and Triple Multi-Dconv Head\nTransposed Attention (TMDTA). Among them, the feature\nlightweighting based on the double DWT is used to trans-\nform the input image into a low-resolution space for low-\ncomputing operation; the CPFR module effectively ampli-\nfies feature information in low-resolution space and alle-\nviates catastrophic forgetting; the TMDTA mechanism en-\nhances the interaction of feature information and relieves\nthe computational complexity of full-pixel self-attention.\nThe qualitative and quantitative experimental results indi-\ncate that LIDFormer can achieve a “low computational com-\nplexity” level close to advanced semantic tasks while main-\ntaining high performance. Moreover, the efficient frame-\nwork in LIDFormer can be generalized to other image de-\nnoising methods for effective optimization.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7710\nAcknowledgments\nThis work was supported by the National Key Research\nand Development Program of China No.2020AAA0108301,\nthe National Natural Science Foundation of China un-\nder Grant No.62176224, No.62222602, No.62176092,\nthe Natural Science Foundation of Chongqing under\nNo.CSTB2023NSCOJOX0007, and the CCF-Lenovo Blue\nOcean Research Fund.\nReferences\nAbdelhamed, A.; Lin, S.; and Brown, M. S. 2018. A high-\nquality denoising dataset for smartphone cameras. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 1692–1700.\nAnwar, S.; and Barnes, N. 2019. Real image denoising with\nfeature attention. In Proceedings of the IEEE/CVF interna-\ntional conference on computer vision, 3155–3164.\nAnwar, S.; Khan, S.; and Barnes, N. 2020. A deep journey\ninto super-resolution: A survey. ACM Computing Surveys\n(CSUR), 53(3): 1–34.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part I 16, 213–229. Springer.\nChen, H.; Wang, Y .; Guo, T.; Xu, C.; Deng, Y .; Liu, Z.; Ma,\nS.; Xu, C.; Xu, C.; and Gao, W. 2021. Pre-trained image\nprocessing transformer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n12299–12310.\nChen, L.; Chu, X.; Zhang, X.; and Sun, J. 2022a. Simple\nbaselines for image restoration. In Computer Vision–ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part VII, 17–33. Springer.\nChen, Y .; and Pock, T. 2016. Trainable nonlinear reaction\ndiffusion: A flexible framework for fast and effective image\nrestoration. IEEE transactions on pattern analysis and ma-\nchine intelligence, 39(6): 1256–1272.\nChen, Z.; Zhang, Y .; Gu, J.; Kong, L.; Yuan, X.; et al.\n2022b. Cross Aggregation Transformer for Image Restora-\ntion. Advances in Neural Information Processing Systems,\n35: 25478–25490.\nCheng, S.; Wang, Y .; Huang, H.; Liu, D.; Fan, H.; and Liu, S.\n2021. Nbnet: Noise basis learning for image denoising with\nsubspace projection. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 4896–\n4906.\nDabov, K.; Foi, A.; Katkovnik, V .; and Egiazarian, K. 2008.\nImage restoration by sparse 3D transform-domain collabora-\ntive filtering. In Image Processing: Algorithms and Systems\nVI, volume 6812, 62–73. SPIE.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An Image is Worth 16x16\nWords: Transformers for Image Recognition at Scale. In In-\nternational Conference on Learning Representations.\nFedus, W.; Zoph, B.; and Shazeer, N. 2022. Switch trans-\nformers: Scaling to trillion parameter models with simple\nand efficient sparsity. The Journal of Machine Learning Re-\nsearch, 23(1): 5232–5270.\nGraham, B.; El-Nouby, A.; Touvron, H.; Stock, P.; Joulin,\nA.; J ´egou, H.; and Douze, M. 2021. Levit: a vision trans-\nformer in convnet’s clothing for faster inference. InProceed-\nings of the IEEE/CVF international conference on computer\nvision, 12259–12269.\nGu, S.; Zhang, L.; Zuo, W.; and Feng, X. 2014. Weighted nu-\nclear norm minimization with application to image denois-\ning. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2862–2869.\nGuo, S.; Yan, Z.; Zhang, K.; Zuo, W.; and Zhang, L. 2019.\nToward convolutional blind denoising of real photographs.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 1712–1722.\nHe, K.; Sun, J.; and Tang, X. 2010. Single image haze re-\nmoval using dark channel prior. IEEE transactions on pat-\ntern analysis and machine intelligence, 33(12): 2341–2353.\nJin, Y .; Jiang, X.-B.; Wei, Z.-k.; and Li, Y . 2019. Chest X-ray\nimage denoising method based on deep convolution neural\nnetwork. IET Image Processing, 13(11): 1970–1978.\nKumar, M.; Weissenborn, D.; and Kalchbrenner, N. 2020.\nColorization Transformer. In International Conference on\nLearning Representations.\nLiang, J.; Cao, J.; Sun, G.; Zhang, K.; Van Gool, L.; and\nTimofte, R. 2021a. Swinir: Image restoration using swin\ntransformer. In Proceedings of the IEEE/CVF international\nconference on computer vision, 1833–1844.\nLiang, Y .; Chongjian, G.; Tong, Z.; Song, Y .; Wang, J.;\nand Xie, P. 2021b. EViT: Expediting Vision Transformers\nvia Token Reorganizations. In International Conference on\nLearning Representations.\nLiu, Y .; Qin, Z.; Anwar, S.; Ji, P.; Kim, D.; Caldwell, S.;\nand Gedeon, T. 2021a. Invertible denoising network: A\nlight solution for real noise removal. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 13365–13374.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021b. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF international conference on computer vision,\n10012–10022.\nLoshchilov, I.; and Hutter, F. 2016. SGDR: Stochastic Gra-\ndient Descent with Warm Restarts. In International Confer-\nence on Learning Representations.\nMallat, S. G. 1989. A theory for multiresolution signal de-\ncomposition: the wavelet representation. IEEE transactions\non pattern analysis and machine intelligence, 11(7): 674–\n693.\nMao, X.; Shen, C.; and Yang, Y .-B. 2016. Image restoration\nusing very deep convolutional encoder-decoder networks\nwith symmetric skip connections. Advances in neural in-\nformation processing systems, 29.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7711\nPlotz, T.; and Roth, S. 2017. Benchmarking denoising al-\ngorithms with real photographs. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 1586–1595.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by gener-\native pre-training.\nRen, C.; He, X.; Wang, C.; and Zhao, Z. 2021. Adaptive\nconsistency prior based deep network for image denoising.\nIn Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 8596–8606.\nRonneberger, O.; Fischer, P.; and Brox, T. 2015. U-net:\nConvolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, 234–241. Springer.\nShen, H.; Zhao, Z.-Q.; and Zhang, W. 2023. Adaptive dy-\nnamic filtering network for image denoising. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol-\nume 37, 2227–2235.\nTai, Y .; Yang, J.; Liu, X.; and Xu, C. 2017. Memnet: A\npersistent memory network for image restoration. In Pro-\nceedings of the IEEE international conference on computer\nvision, 4539–4547.\nUlyanov, D.; Vedaldi, A.; and Lempitsky, V . 2018. Deep\nimage prior. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 9446–9454.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, Z.; Cun, X.; Bao, J.; Zhou, W.; Liu, J.; and Li, H.\n2022. Uformer: A general u-shaped transformer for image\nrestoration. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 17683–17693.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient design\nfor semantic segmentation with transformers. Advances in\nNeural Information Processing Systems, 34: 12077–12090.\nXu, J.; Zhang, L.; Zhang, D.; and Feng, X. 2017. Multi-\nchannel weighted nuclear norm minimization for real color\nimage denoising. In Proceedings of the IEEE international\nconference on computer vision, 1096–1104.\nXu, S.; Yang, X.; and Jiang, S. 2017. A fast nonlocally cen-\ntralized sparse representation algorithm for image denois-\ning. Signal Processing, 131: 99–112.\nYair, N.; and Michaeli, T. 2018. Multi-scale weighted nu-\nclear norm image restoration. In Proceedings of the IEEE\nconference on computer vision and pattern recognition ,\n3165–3174.\nYu, Y .; Chang, M.; Feng, H.; Xu, Z.; Li, Q.; and Chen, Y .\n2018. Image denoising algorithm based on adversarial learn-\ning using joint loss function. In Fifth Conference on Fron-\ntiers in Optical Imaging Technology and Applications, vol-\nume 10832, 204–210. SPIE.\nYuan, W.; Liu, H.; and Liang, L. 2023. Joint group\ndictionary-based structural sparse representation for image\nrestoration. Digital Signal Processing, 104029.\nYue, Z.; Yong, H.; Zhao, Q.; Meng, D.; and Zhang, L. 2019.\nVariational denoising network: Toward blind noise modeling\nand removal. Advances in neural information processing\nsystems, 32.\nYue, Z.; Zhao, Q.; Zhang, L.; and Meng, D. 2020. Dual\nadversarial network: Toward real-world noise removal and\nnoise generation. In Computer Vision–ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part X 16, 41–58. Springer.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nand Yang, M.-H. 2022. Restormer: Efficient transformer\nfor high-resolution image restoration. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 5728–5739.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nYang, M.-H.; and Shao, L. 2020. Learning enriched features\nfor real image restoration and enhancement. In Computer\nVision–ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23–28, 2020, Proceedings, Part XXV 16, 492–\n511. Springer.\nZamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;\nYang, M.-H.; and Shao, L. 2021. Multi-stage progressive\nimage restoration. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, 14821–\n14831.\nZhang, K.; Li, Y .; Zuo, W.; Zhang, L.; Van Gool, L.; and\nTimofte, R. 2021. Plug-and-play image restoration with\ndeep denoiser prior. IEEE Transactions on Pattern Analy-\nsis and Machine Intelligence, 44(10): 6360–6376.\nZhang, K.; Zuo, W.; Chen, Y .; Meng, D.; and Zhang, L.\n2017. Beyond a gaussian denoiser: Residual learning of\ndeep cnn for image denoising. IEEE transactions on image\nprocessing, 26(7): 3142–3155.\nZhang, K.; Zuo, W.; and Zhang, L. 2018. FFDNet: Toward\na fast and flexible solution for CNN-based image denoising.\nIEEE Transactions on Image Processing, 27(9): 4608–4622.\nZhang, Y .; Tian, Y .; Kong, Y .; Zhong, B.; and Fu, Y . 2020.\nResidual dense network for image restoration. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 43(7):\n2480–2495.\nZhou, Y .; Jiao, J.; Huang, H.; Wang, Y .; Wang, J.; Shi, H.;\nand Huang, T. 2020. When awgn-based denoiser meets real\nnoises. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, 13074–13081.\nZhou, Y .; Xu, X.; Liu, S.; Wang, G.; Lu, H.; and Shen, H. T.\n2022. Thunder: Thumbnail based Fast Lightweight Image\nDenoising Network. arXiv preprint arXiv:2205.11823.\nZou, B.; Zhang, Y .; Wang, M.; and Liu, S. 2023. Toward Ef-\nficient Image Denoising: A Lightweight Network with Re-\ntargeting Supervision Driven Knowledge Distillation. InAd-\nvances in Computer Graphics: 39th Computer Graphics In-\nternational Conference, CGI 2022, Virtual Event, September\n12–16, 2022, Proceedings, 15–27. Springer.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7712",
  "topic": "Noise reduction",
  "concepts": [
    {
      "name": "Noise reduction",
      "score": 0.5718081593513489
    },
    {
      "name": "Image denoising",
      "score": 0.5532101988792419
    },
    {
      "name": "Transformer",
      "score": 0.48218029737472534
    },
    {
      "name": "Computer science",
      "score": 0.4008041024208069
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3779129981994629
    },
    {
      "name": "Computer vision",
      "score": 0.34293806552886963
    },
    {
      "name": "Engineering",
      "score": 0.19861826300621033
    },
    {
      "name": "Electrical engineering",
      "score": 0.14795726537704468
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I191208505",
      "name": "Xiamen University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I66867065",
      "name": "East China Normal University",
      "country": "CN"
    }
  ]
}