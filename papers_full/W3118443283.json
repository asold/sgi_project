{
  "title": "Does injecting linguistic structure into language models lead to better alignment with brain recordings?",
  "url": "https://openalex.org/W3118443283",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4223214302",
      "name": "Abdou, Mostafa",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287257639",
      "name": "Gonzalez, Ana Valeria",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226968168",
      "name": "Toneva, Mariya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224856868",
      "name": "Hershcovich, Daniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221452188",
      "name": "S√∏gaard, Anders",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2168217710",
    "https://openalex.org/W2909386406",
    "https://openalex.org/W2922565841",
    "https://openalex.org/W2095589793",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2160654481",
    "https://openalex.org/W2520305281",
    "https://openalex.org/W2983845924",
    "https://openalex.org/W2902562135",
    "https://openalex.org/W2970481354",
    "https://openalex.org/W3031914912",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W2579343286",
    "https://openalex.org/W3039556919",
    "https://openalex.org/W2970451234",
    "https://openalex.org/W1857789879",
    "https://openalex.org/W3042795397",
    "https://openalex.org/W2604593109",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W3037035104",
    "https://openalex.org/W2626769189",
    "https://openalex.org/W2170167574",
    "https://openalex.org/W2970648593",
    "https://openalex.org/W2782213998",
    "https://openalex.org/W2948337921",
    "https://openalex.org/W2949558627",
    "https://openalex.org/W2616551000",
    "https://openalex.org/W2955844292",
    "https://openalex.org/W2511593322",
    "https://openalex.org/W2951891110",
    "https://openalex.org/W2969245029",
    "https://openalex.org/W3014518624",
    "https://openalex.org/W2806560616",
    "https://openalex.org/W1517853909",
    "https://openalex.org/W2773956126",
    "https://openalex.org/W2625800120",
    "https://openalex.org/W2085574295",
    "https://openalex.org/W2407291067",
    "https://openalex.org/W2970455584",
    "https://openalex.org/W2963061446",
    "https://openalex.org/W3037273551",
    "https://openalex.org/W123237996",
    "https://openalex.org/W2119728020",
    "https://openalex.org/W2252123671",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2250537251",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2805003518",
    "https://openalex.org/W2154776925"
  ],
  "abstract": "Neuroscientists evaluate deep neural networks for natural language processing as possible candidate models for how language is processed in the brain. These models are often trained without explicit linguistic supervision, but have been shown to learn some linguistic structure in the absence of such supervision (Manning et al., 2020), potentially questioning the relevance of symbolic linguistic theories in modeling such cognitive processes (Warstadt and Bowman, 2020). We evaluate across two fMRI datasets whether language models align better with brain recordings, if their attention is biased by annotations from syntactic or semantic formalisms. Using structure from dependency or minimal recursion semantic annotations, we find alignments improve significantly for one of the datasets. For another dataset, we see more mixed results. We present an extensive analysis of these results. Our proposed approach enables the evaluation of more targeted hypotheses about the composition of meaning in the brain, expanding the range of possible scientific inferences a neuroscientist could make, and opens up new opportunities for cross-pollination between computational neuroscience and linguistics.",
  "full_text": "Does injecting linguistic structure into language models lead to better\nalignment with brain recordings?\nMostafa Abdou1, Ana Valeria Gonz√°lez1, Mariya Toneva2,\nDaniel Hershcovich1, and Anders S√∏gaard1\n1University of Copenhagen, 2Carnegie Mellon University\n{mabdou, ana, dh, soegaard}@di.ku.dk\nmktoneva@cs.cmu.edu\nAbstract\nNeuroscientists evaluate deep neural net-\nworks for natural language processing as\npossible candidate models for how language\nis processed in the brain. These models\nare often trained without explicit linguistic\nsupervision, but have been shown to learn\nsome linguistic structure in the absence of\nsuch supervision (Manning et al., 2020), po-\ntentially questioning the relevance of sym-\nbolic linguistic theories in modeling such\ncognitive processes (Warstadt and Bowman,\n2020). We evaluate across two fMRI datasets\nwhether language models align better with\nbrain recordings, if their attention is biased\nby annotations from syntactic or semantic\nformalisms. Using structure from depen-\ndency or minimal recursion semantic anno-\ntations, we Ô¨Ånd alignments improve signif-\nicantly for one of the datasets. For another\ndataset, we see more mixed results. We\npresent an extensive analysis of these results.\nOur proposed approach enables the evalua-\ntion of more targeted hypotheses about the\ncomposition of meaning in the brain, expand-\ning the range of possible scientiÔ¨Åc inferences\na neuroscientist could make, and opens up\nnew opportunities for cross-pollination be-\ntween computational neuroscience and lin-\nguistics.\n1 Introduction\nRecent advances in deep neural networks for nat-\nural language processing (NLP) have generated\nexcitement among computational neuroscientists,\nwho aim to model how the brain processes lan-\nguage. These models are argued to better capture\nthe complexity of natural language semantics than\nprevious computational models, and are thought to\nrepresent meaning in a more similar way to how\nit is hypothesized to be represented in the human\nbrain. For neuroscientists, these models provide\npossible hypotheses for how word meanings com-\npose in the brain. Previous work has evaluated\nthe plausibility of such candidate models by test-\ning how well representations of text extracted from\nthese models align with brain recordings of humans\nduring language comprehension tasks (Wehbe et al.,\n2014; Jain and Huth, 2018; Gauthier and Ivanova,\n2018; Gauthier and Levy, 2019; Abnar et al., 2019;\nToneva and Wehbe, 2019; Schrimpf et al., 2020;\nCaucheteux and King, 2020), and found some cor-\nrespondences.\nHowever, modern NLP models are often trained\nwithout explicit linguistic supervision (Devlin et al.,\n2018; Radford et al., 2019), and the observation\nthat they nevertheless learn some linguistic struc-\nture has been used to question the relevance of sym-\nbolic linguistic theories. Whether injecting such\nsymbolic structures into language models would\nlead to even better alignment with cognitive mea-\nsurements, however, has not been studied. In this\nwork, we address this gap by training BERT (¬ß3.1)\nwith structural bias, and evaluate its alignment with\nbrain recordings (¬ß3.2). Structure is derived from\nthree formalisms‚ÄîUD, DM and UCCA (¬ß3.3)‚Äî\nwhich come from different linguistic traditions, and\ncapture different aspects of syntax and semantics.\nOur approach, illustrated in Figure 1, allows for\nquantifying the brain alignment of the structurally-\nbiased NLP models in comparison to the base mod-\nels, as related to new information about linguistic\nstructure learned by the models that is also poten-\ntially relevant to language comprehension in the\nbrain. More speciÔ¨Åcally, in this paper, we:\n(a) Employ a Ô¨Åne-tuning method utilising struc-\nturally guided attention for injecting structural\nbias into language model (LM) representa-\ntions.\n(b) Assess the representational alignment to brain\nactivity measurements of the Ô¨Åne-tuned and\nnon-Ô¨Åne-tuned LMs.\narXiv:2101.12608v1  [cs.CL]  29 Jan 2021\nùë• alignment prior \nto intervention\n ùë•Ôºãùõø alignment \n post intervention\nBERT model\naltered BERT\nSentence or word\nRepresentations\n‚ÄúHarry never \nthought \nhe would ...‚Äù\nfMRI\n Decoder\n If   ùõø >> 0,       was successful at \nencoding brain-relevant bias\nDecoder\nFigure 1: Overview of our approach. We use BERT as a baseline and inject structural bias in two ways.\nThrough a brain decoding task, we then compare the alignment of the (sentence and word) representations\nof our baseline and our altered models with brain activations.\n(c) Further evaluate the LMs on a range of tar-\ngeted syntactic probing tasks and a seman-\ntic tagging task, which allow us to uncover\nÔ¨Åne-grained information about their structure-\nsensitive linguistic capabilities.\n(d) Present an analysis of various linguistic fac-\ntors that may lead to improved or deteriorated\nbrain alignment.\n2 Background: Brain activity and NLP\nMitchell et al. (2008) Ô¨Årst showed that there is a\nrelationship between the co-occurrence patterns of\nwords in text and brain activation for processing the\nsemantics of words. SpeciÔ¨Åcally, they showed that\na computational model trained on co-occurrence\npatterns for a few verbs was able to predict fMRI\nactivations for novel nouns. Since this paper was\nintroduced, many works have attempted to isolate\nother features that enable prediction and interpreta-\ntion of brain activity (Frank et al., 2015; Brennan\net al., 2016; Lopopolo et al., 2017; Anderson et al.,\n2017; Pereira et al., 2018; Wang et al., 2020). Gau-\nthier and Ivanova (2018) however, emphasize that\ndirectly optimizing for the decoding of neural rep-\nresentation is limiting, as it does not allow for the\nuncovering of the mechanisms that underlie these\nrepresentations. The authors suggest that in order\nfor us to better understand linguistic processing in\nthe brain, we should also aim to train models that\noptimize for a speciÔ¨Åc linguistic task and explicitly\ntest these against brain activity.\nFollowing this line of work, Toneva and Wehbe\n(2019) present experiments both predicting brain\nactivity and evaluating representations on a set of\nlinguistic tasks. They Ô¨Årst show that using uniform\nattention in early layers of BERT (Devlin et al.,\n2018) instead of pretrained attention leads to better\nprediction of brain activity. They then use the repre-\nsentations of this altered model to make predictions\non a range of syntactic probe tasks, which isolate\ndifferent syntactic phenomena (Marvin and Linzen,\n2019), Ô¨Ånding improvements against the pretrained\nBERT attention. Gauthier and Levy (2019) present\na series of experiments in which they Ô¨Åne-tune\nBERT on a variety of tasks including language\nmodeling as well as some custom tasks such as\nscrambled language modeling and part-of-speech-\nlanguage modeling. They then perform brain de-\ncoding, where a linear mapping is learnt from fMRI\nrecordings to the Ô¨Åne-tuned BERT model activa-\ntions. They Ô¨Ånd that the best mapping is obtained\nwith the scrambled language modelling Ô¨Åne-tuning.\nFurther analysis using a structural probe method\nconÔ¨Årmed that the token representations from the\nscrambled language model performed poorly when\nused for reconstructing Universal Dependencies\n(UD; Nivre et al., 2016) parse trees.\nWhen dealing with brain activity, many con-\nfounds may lead to seemingly divergent Ô¨Åndings,\nsuch as the size of fMRI data, the temporal resolu-\ntion of fMRI, the low signal-to-noise ratio, as well\nas how the tasks were presented to the subjects,\namong many other factors. For this reason, it is es-\nsential to take sound measures for reporting results,\nsuch as cross-validating models, evaluating on un-\nseen test sets, and conducting a thorough statistical\nanalysis.\n3 Approach\nFigure 1 shows a high-level outline of our experi-\nmental design, which aims to establish whether in-\njecting structure derived from a variety of syntacto-\nsemantic formalisms into neural language model\nrepresentations can lead to better correspondence\nwith human brain activation data. We utilize fMRI\nrecordings of human subjects reading a set of texts.\nRepresentations of these texts are then derived from\nthe activations of the language models. Following\nGauthier and Levy (2019), we obtain LM represen-\ntations from BERT1 for all our experiments. We\napply masked language model Ô¨Åne-tuning with at-\ntention guided by the formalisms to incorporate\nstructural bias into BERT‚Äôs hidden-state represen-\ntations. Finally, to compute alignment between\nthe BERT-derived representations‚Äîwith and with-\nout structural bias‚Äîand the fMRI recordings, we\nemploy the brain decoding framework, where a lin-\near decoder is trained to predict the LM derived\nrepresentation of a word or a sentence from the\ncorresponding fMRI recordings.\n3.1 LM-derived Representations\nBERT uses wordpiece tokenization, dividing the\ntext to sub-word units. For a sentence S made\nup of P wordpieces , we perform mean-pooling\nover BERT‚Äôs Ô¨Ånal layer hidden-states[h1,...,h P ],\nobtaining a vector representation of the sentence\nSmean = 1\nP\n‚àë\np hp (Wu et al., 2016). In initial\nexperiments, we found that this leads to a closer\nmatch with brain activity measurements compared\nto both max-pooling and the special [CLS] token,\nwhich is used by Gauthier and Levy (2019). Simi-\nlarly, for a wordW made up ofP wordpieces, to de-\nrive word representations, we apply mean-pooling\nover hidden-states [h1,...,h P ], which correspond\nto the wordpieces that make up W: Wmean =\n1\nP\n‚àë\np hp. For each dataset, DLM ‚ààRn√ódH de-\n1SpeciÔ¨Åcally: bert-large-uncased trained with\nwhole-word masking.\nnotes a matrix of nLM-derived word or sentence\nrepresentations where dH is BERT‚Äôs hidden layer\ndimensionality (dH = 1024in our experiments).\n3.2 Neuroimaging Datasets\nWe utilize two fMRI datasets, which differ in the\ngranularity of linguistic cues to which human re-\nsponses were recorded. The Ô¨Årst, collected in\nPereira et al. (2018)‚Äôs experiment 2, comprises a\nsingle brain image per entire sentence. In the sec-\nond, more Ô¨Åne-grained dataset, recorded by Wehbe\net al. (2014), each brain image corresponds to 4\nwords. We conduct a sentence-level analysis for\nthe former and a word-level one for the latter.2\nPereira2018 consists of fMRI recordings from 8\nsubjects. The subjects were presented with stimuli\nconsisting of 96 Wikipedia-style passages written\nby the authors, consisting of 4 sentences each. The\nsubjects read the sentences one by one and were\ninstructed to think about their meaning. The result-\ning data for each subject consists of 384 vectors of\ndimension 200,000; a vector per sentence. These\nwere reduced to 256 dimensions using PCA by\nGauthier and Levy (2019). These PCA projections\nexplain more than 95% of the variance among sen-\ntence responses within each subject. We use this\nreduced version in our experiments.\nWehbe2014 consists of fMRI recordings from 8\nsubjects as they read a chapter from Harry Potter\nand the Sorcerer‚Äôs Stone. For the 5000 word chap-\nter, subjects were presented with words one by one\nfor 0.5 seconds each. An fMRI image was taken ev-\nery 2 seconds, as a result, each image corresponds\nto 4 words. The data was further preprocessed\n(i.e. detrended, smoothed, trimmed) and released\nby Toneva and Wehbe (2019). We use this prepro-\ncessed version to conduct word-level analysis, for\nwhich we use PCA to reduce the dimensions of the\nfMRI images from 25,000 to 750, explaining at\nleast 95% variance for each participant.\n3.3 Formalisms and Data\nTo inject linguistic structure into language models,\nwe experiment with three distinct formalisms for\nrepresentation of syntactic/semantic structure, com-\ning from different linguistic traditions and captur-\ning different aspects of linguistic signal: UD, DM\nand UCCA. An example graph for each formalism\n2Even though the images are recorded at the 4-gram level\nof granularity, a word-level analysis is applied, as in (Schwartz\net al., 2019).\nHehadbeenlookingforwardtolearningtoÔ¨Çymorethananythingelse.\nnsubjauxaux\nroot\nadvmodmark\nadvcl\nmarkxcomp\nadvmod\ncaseobl amod\npunct\nARG1\nTOP\nARG2 ARG2\nARG1\nmweARG2ARG1\n(a) UD (above, orange), DM (below, blue)\nHe\nA\nhad\nD\nbeen\nD\nlooking forward\nP\nto\nF\nlearning\nP\nto\nF\nÔ¨Çy\nP\nA\nA\nmore\nD\nH\nthan\nL\nanything\nC\nelse\nE\nA\nH\nA\nA\nA\nP\n(b) UCCA\nFigure 2: Manually annotated example graphs for a sentence from the Wehbe2014 dataset. While UCCA\nand UD attach all words, DM only connects content words. However, all formalisms capture basic\npredicate-argument structure, for example, denoting that ‚Äúmore than anything else‚Äù modiÔ¨Åes ‚Äúlooking\nforward‚Äù rather than ‚ÄúÔ¨Çy‚Äù.\nis shown in Figure 2. Although there are other im-\nportant linguistic structured formalisms, including\nmeaning representations such as AMR (Banarescu\net al., 2013), DRS (Kamp and Reyle, 1993; Bos\net al., 2017) and FGD (Sgall et al., 1986; Hajic\net al., 2012), we select three relatively different\nformalisms as a somewhat representative sample.\nAll three have manually annotated datasets, which\nwe use for our experiments.\nUD (Universal Dependencies; Nivre et al., 2020)\nis a syntactic bi-lexical dependency framework (de-\npendencies are denoted as arcs between words,\nwith one word being the head and another the de-\npendent), which represents grammatical relations\naccording to a coarse cross-lingual scheme. For\nUD data, we use the English Web Treebank cor-\npus (EWT; Silveira et al., 2014), which contains\n254,830 words and 16,622 sentences, taken from\nÔ¨Åve genres of web media: weblogs, newsgroups,\nemails, reviews, and Yahoo! answers.\nDM (DELPH-IN MRS Bi-Lexical Dependen-\ncies; Ivanova et al., 2012) is derived from the un-\nderspeciÔ¨Åed logical forms computed by the En-\nglish Resource Grammar (Flickinger et al., 2017;\nCopestake et al., 2005), and is one of the frame-\nworks targeted by the Semantic Dependency Pars-\ning SemEval Shared Tasks (SDP; Oepen et al.,\n2014, 2015). We use the English SDP data for\nDM (Oepen et al., 2016), annotated on newspaper\ntext from the Wall Street Journal (WSJ), containing\n802,717 words and 35,656 sentences.\nUCCA (Universal Cognitive Conceptual Annota-\ntion; Abend and Rappoport, 2013) is based on cog-\nnitive linguistic and typological theories, primar-\nily Basic Linguistic Theory (Dixon, 2010/2012).\nWe use UCCA annotations over web reviews text\nfrom the English Web Treebank, and from English\nWikipedia articles on celebrities. In total, they con-\ntain 138,268 words and 6,572 sentences. For unifor-\nmity with the other formalisms, we use bi-lexical\napproximation to convert UCCA graphs, which\nhave a hierarchical constituency-like structure, to\nbi-lexical graphs with edges between words. This\nconversion keeps about 91% of the information\n(Hershcovich et al., 2017).\n3.4 Injecting Structural Bias into LMs\nRecent work has explored ways of modifying at-\ntention in order to incorporate structure into neural\nmodels (Chen et al., 2016; Strubell et al., 2018;\nStrubell and McCallum, 2018; Zhang et al., 2019;\nBugliarello and Okazaki, 2019). For instance,\nStrubell et al. (2018) incorporate syntactic infor-\nmation by training one attention head to attend to\nsyntactic heads, and Ô¨Ånd that this leads to improve-\nments in Semantic Role Labeling (SRL). Drawing\non these approaches, we modify the BERT Masked\nLanguage Model (MLM) objective with an addi-\ntional structural attention constraint. BERTLARGE\nconsists of 24 layers and 16 attention heads. Each\nattention head headi takes in as input a sequence\nof representations h= [h1,...,h P ] corresponding\nto the P wordpieces in the input sequence. Each\nrepresentation in hp is transformed into query, key,\nand value vectors. The scaled dot product is com-\nputed between the query and all keys and a softmax\nfunction is applied to obtain the attention weights.\nThe output of headi is a matrix Oi, corresponding\nto the weighted sum of the value vectors.\nFor each formalism and its corresponding cor-\npus, we extract an adjacency matrix from each sen-\ntence‚Äôs parse. For the sequence S, the adjacency\nmatrix AS is a matrix of size P √óP, where the\ncolumns correspond to the heads in the parse tree\nand the rows correspond to the dependents. The\nmatrix elements denote which tokens are connected\nin the parse tree, taking into account BERT‚Äôs word-\npiece tokenization. Edge directionality is not con-\nsidered. We modify BERT to accept as input a\nmatrix AS as well as S; maintaining the original\nMLM objective. For each attention head headi, we\ncompute the binary cross-entropy loss between Oi\nand AS and add that to our total loss, potentially\ndown-weighted by a factor ofŒ±(a hyperparameter).\nBERT‚Äôs default MLM Ô¨Åne-tuning hyperparameters\nare employed andŒ±is set to0.1 based on validation\nset perplexity scores in initial experiments.\nStructural information can be injected into BERT\nin many ways, in many heads, across many lay-\ners. Because the appropriate level and extent of\nsupervision is unknown a priori, we run various\nÔ¨Åne-tuninig settings with respect to combinations\nof number of layers (1,..., 24) and attention heads\n(1,3,5,7,9,11,12) supervised via attention guid-\nance. Layers are excluded from the bottom up (e.g.:\nwhen 10 layers are supervised, it is the topmost\n10); heads are chosen according to their indices\n(which are arbitrary). This results in a total of 168\nÔ¨Åne-tuning settings per formalism. For each Ô¨Åne-\ntuning setting, we perform two Ô¨Åne-tuning runs.3\nFor each run r of each Ô¨Åne-tuning setting f, we\nderive a set of sentence or word representations\nDfr ‚ààRn√ódH from each Ô¨Åne-tuned model using\nthe approach described in ¬ß3.1 for obtaining DLM ,\nthe baseline set of representations from BERT be-\nfore Ô¨Åne-tuning. We then use development set 4\nembedding space hubness‚Äîan indicator of the de-\ngree of difÔ¨Åculty of indexing and analysing data\n(Houle, 2015) which has been used to evaluate em-\nbedding space quality (Dinu et al., 2014)‚Äîas an\nunsupervised selection criterion for the Ô¨Åne-tuned\nmodels, selecting the model with the lowest degree\nof hubness (per formalism) according to the Robin\nHood Index (Feldbauer et al., 2018). This yields\nthree models for each of the two datasets‚Äîone per\nformalism‚Äîfor which we present results below.\nIn addition to the approach described above, we\n3We Ô¨Ånd that the mean difference in brain decoding score\n(Pearson‚Äôsr) between two runs of the same setting (across all\nsettings) is low (0.003), indicating that random initialization\ndoes not play a major part in our results. We, therefore, do not\ncarry out more runs.\n4For Wehbe2014: second chapter of Harry Potter. For\nPereira2018: Ô¨Årst 500 sentences of English Wikipedia.\nalso experiment with directly optimizing for the\nprediction of the formalism graphs (i.e., parsing) as\na way of encoding structural information in LM rep-\nresentations. We Ô¨Ånd that this leads to a consistent\ndecline in alignment of the LMs‚Äô representations\nto brain recordings. Further details can be found in\nAppendix A.\n3.5 Brain Decoding\nTo measure the alignment of the different LM-\nderived representations to the brain activity mea-\nsurements, brain decoding is performed, following\nthe setup described in Gauthier and Levy (2019).5\nFor each subject i‚Äôs fMRI images corresponding\nto a set of n sentences or words, a ridge regres-\nsion model is trained to linearly map from brain\nactivity Bi ‚ààRn√ódB (n = 384; dB = 256 for\nPereira2018 and n = 4369; dB = 750 for We-\nhbe2014) to a LM-derived representation (Dfr or\nDLM ), minimizing the following loss:\nLifr = ‚à•BiGi‚Üífr ‚àíDfr ‚à•2\n2 + Œª ‚à•Gi‚Üífr ‚à•2\n2\nwhere Gi‚Üífr : RdH√ódB is a linear map, and Œªis\na hyperparameter for ridge regularization. Nested\n12-fold cross-validation (Cawley and Talbot, 2010)\nis used for selection of Œª, training and evaluation.\nEvaluation To evaluate the regression models,\nPearson‚Äôs correlation coefÔ¨Åcient between the pre-\ndicted and the corresponding heldout true sentence\nor word representations is computed. We Ô¨Ånd that\nthis metric6 is consistent across subjects and across\nthe two datasets. We run 5000 bootstrap resam-\npling iterations and a) report the mean 7 corre-\nlation coefÔ¨Åcient (referred to as brain decoding\nscore/performance), b) use a paired bootstrap test\nto establish whether two models‚Äô mean (across\nstimuli) scores were drawn from populations hav-\n5Other methods for evaluating representational corre-\nspondence such as Representational Similarity Analysis\n(Kriegeskorte et al., 2008) and the Centered Kernel Alignment\nsimilarity index (Kornblith et al., 2019) were also explored\nbut were found to be either less powerful or less consistent\nacross subjects and datasets.\n6Appendix B shows results for the rank-based metric re-\nported in (Gauthier and Levy, 2019), which we Ô¨Ånd to strongly\ncorrespond to Pearson‚Äôs correlation. This metric evaluates\nrepresentations based on their support for contrasts between\nsentences/words which are relevant to the brain recordings.\nOther metrics for the evaluation of goodness of Ô¨Åt were found\nto be less consistent.\n7Across Ô¨Åne-tuning runs, cross-validation splits, and boot-\nstrap iterations.\ndm ud ucca\nformalism\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40pearson r\nPereira et al. (2018) \ndm ud ucca\nformalism\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40pearson r\nWehbe et al. (2014)\npretrained\ndomain-finetuned\nFigure 3: Brain decoding score (mean Pearson‚Äôs r; with 95% conÔ¨Ådence intervals shown for subject\nscores) for models Ô¨Åne-tuned by MLM with guided attention on each of the formalisms, as well the\nbaseline models: pretrained BERT (dotted line), and BERT Ô¨Åne-tuned by MLM on each formalism‚Äôs\ntraining text without guided attention (domain-Ô¨Ånetuned BERT, solid lines).\ning the same distribution 8, c) apply the Wilcoxon\nsigned rank test (Wilcoxon, 1992) to the by-subject\nscores to test for evidence of strength of general-\nization over subjects. Bonferroni correction (for 3\nmultiple comparisons) is used to adjust for multiple\nhypothesis testing. See Appendix C for details.\n4 Results\nTo evaluate the effect of the structurally-guided\nattention, we compute the brain decoding scores\nfor the guided attention models corresponding to\neach formalism and fMRI dataset and compare\nthese scores against the brain decoding scores from\ntwo baseline models: 1) a domain-Ô¨Ånetuned BERT\n(DF), which Ô¨Ånetunes BERT using the regular\nMLM objective on the text of each formalism‚Äôs\ntraining data, and a pretrained BERT. We introduce\nthe domain-Ô¨Ånetuned baseline in order to control\nfor any effect that Ô¨Ånetuning using a speciÔ¨Åc text\ndomain may have on the model representations.\nComparing against this baseline allows us to bet-\nter isolate the effect of injecting the structural bias\nfrom the possible effect of simply Ô¨Åne-tuning on\nthe text domain. We further compare to a pretrained\nbaseline in order to evaluate how the structurally-\nguided attention approach performs against an off-\nthe-shelf model that is commonly used in brain-\nalignment experiments.\n8This is applied per subject to test for strength of evidence\nof generalization over sentence stimuli.\n4.1 Pereira2018\nFigure 3 shows the sentence-level decoding perfor-\nmance on the Pereira2018 dataset, for the guided\nattention Ô¨Åne-tuned models (GA) and both baseline\nmodels (domain-Ô¨Ånetuned and pretrained). We Ô¨Ånd\nthat the DF baseline (shown in Figure 3 as solid\nlines) leads to brain decoding scores that are ei-\nther lower than or not signiÔ¨Åcantly different from\nthe pretrained baseline. SpeciÔ¨Åcally, for DM and\nUCCA, it performs below the pretrained baseline,\nwhich suggests that simply Ô¨Åne-tuning on these cor-\npora results in BERT‚Äôs representations becoming\nless aligned with the brain activation measurements\nfrom Pereira2018. We Ô¨Ånd that all GA models\noutperform their respective DF baselines (for all\nsubjects, p< 0.05). We further Ô¨Ånd that compared\nto the pretrained baselines, with p <0.05: a) the\nUD GA model shows signiÔ¨Åcantly better brain de-\ncoding scores for 7 out of 8 subjects, b) the DM GA\nmodel for 4 out of 8 subjects, c) UCCA GA shows\nscores not signiÔ¨Åcantly different from or lower, for\nall subjects. For details see Appendix C.\n4.2 Wehbe2014\nFor Wehbe2014, where analysis is conducted on\nthe word level, we again Ô¨Ånd that domain-Ô¨Ånetuned\nmodels‚Äîespecially the one Ô¨Ånetuned on the UCCA\ndomain text‚Äîachieve considerably lower brain de-\ncoding scores than the pretrained model, as shown\nin Figure 3. Furthermore, the guided attention mod-\nels for all three formalisms outperform both base-\nlines by a large, signiÔ¨Åcant margin (after Bonfer-\nroni correction, p< 0.0001).\n5 Discussion and Analysis\nOverall, our results show that structural bias from\nsyntacto-semantic formalisms can improve the abil-\nity of a linear decoder to map the BERT representa-\ntions of stimuli sentences to their brain recordings.\nThis improvement is especially clear for Wehbe\n2014, where token representations and not aggre-\ngated sentence representations (as inPereira 2018)\nare decoded, indicating that Ô¨Åner-grain recordings\nand analyses might be necessary for modelling the\ncorrelates of linguistic structure in brain imaging\ndata. To arrive at a better understanding of the\neffect of the structural bias and its relationship to\nbrain alignment, in what follows, we present an\nanalysis of the various factors which affect and\ninteract with this relationship.\nThe effect of domain Our results suggest that\nthe domain of Ô¨Åne-tuning data and of stimuli might\nplay a signiÔ¨Åcant role, despite having been previ-\nously overlooked: simply Ô¨Åne-tuning on data from\ndifferent domains leads to varying degrees of align-\nment to brain data. To quantify this effect, we\ncompute the average word perplexity of the stimuli\nfrom both fMRI datasets for the pretrained and DF\nbaselines on each of the three domain datasets.9 If\nthe domain of the corpora used for Ô¨Åne-tuning in-\nÔ¨Çuences our results as hypothesized, we expect this\nscore to be higher for the DF baselines. We Ô¨Ånd\nthat this is indeed the case and that for those base-\nlines (DF), increase in perplexity roughly corre-\nsponds to lower brain decoding scores‚Äîsee details\nin Appendix D. This Ô¨Ånding calls to attention the\nnecessity of accounting for domain match in work\nutilizing cognitive measurements and emphasizes\nthe importance of the DF baseline in this study.\nTargeted syntactic evaluation We evaluate all\nmodels on a range of syntactic probing tasks pro-\nposed by Marvin and Linzen (2019).10 This dataset\ntests the ability of models to distinguish mini-\nmal pairs of grammatical and ungrammatical sen-\ntences across a range of syntactic phenomena. Fig-\nure 4 shows the results for the three Wehbe2014\n9Note that this is not equivalent to the commonly utilised\nsequence perplexity (which can not be calculated for non-auto-\nregressive models) but sufÔ¨Åces for quantifying the effect of\ndomain shift.\n10Using the evaluation script from Goldberg (2019).\nmodels across all subject-verb agreement (SV A)\ntasks.11 We observe that after GA Ô¨Åne-tuning: a)\nthe DM guided-attention model, and to a lesser ex-\ntent the UD guided-attention model have a higher\nscore than the pretrained baseline and the domain-\nÔ¨Ånetuned baselines for most SV A tasks and b) the\nranking of the models corresponds to their rank-\ning on the brain decoding task ( DM > UD >\nUCCA).12 Although all three formalisms anno-\ntate the subject-verb-object or predicate-argument\nstructure necessary for solving SV A tasks, it ap-\npears that some of them do so more effectively, at\nleast when encoded into a LM by GA.\nEffect on semantics To evaluate the impact of\nstructural bias on encoding of semantic informa-\ntion, we consider Semantic Tagging (Abzianidze\nand Bos, 2017), commonly used to analyse the se-\nmantics encoded in LM representations (Belinkov\net al., 2018; Liu et al., 2019): tokens are labeled to\nreÔ¨Çect their semantic role in context. For each of\nthe three guided attention Wehbe2014 models and\nthe pretrained model, a linear probe is trained to\npredict a word‚Äôs semantic tag, given the contextual\nrepresentation induced by the model (see Appendix\nE for details). For each of the three GA models,\nFigure 5 shows the change in test set classiÔ¨Åcation\nF1-score,13 relative to the pretrained baseline, per\ncoarse-grained grouping of tags. 14 We Ô¨Ånd that\nthe structural bias improves the ability to correctly\nrecognize almost all of the semantic phenomena\nconsidered, indicating that our method for inject-\ning linguistic structure leads to better encoding of\na broad range of semantic distinctions. Further-\n11See Appendix F for the full set of results for both We-\nhbe2014 and for Pereira2018 with similar patterns.\n12For reÔ¨Çexive anaphora tasks, these trends are reversed:\nthe models underperform the pretrained baseline and their\nranking is the converse of their brain decoding scores. Re-\nÔ¨Çexive Anaphora, are not explicitly annotated for in any of\nthe three formalisms. We Ô¨Ånd, however, that they occur in\na larger proportion of the sentences comprising the UCCA\ncorpus (1.4%) than those the UD ( 0.67%) or DM ( 0.64%)\nones, indicating that domain might play a role here too.\n13Note that the test set consists of 263,516 instances, there-\nfore, the margin of change in number of instances here is\nconsiderable, e.g. 5652 ‚àó 0.6 ‚âà 40 instances for the DM and\nUCCA models on the temporal category, which is the least\nfrequent in the test set. See test set category frequencies in the\nappendix.\n14The eight most frequent coarse-grained categories from\nan original set of ten are included‚Äîordered by frequency from\nleft to right; we exclude the UNKNOWN category because it\nis uninformative and the ANAPHORIC category because it\nshows no change from the baseline for all three models.\nFigure 4: Accuracy per subject-verb agreement category of (Marvin and Linzen, 2019) for the three\nWehbe2014 models and each of the four baselines.\nmore, the improvements are largest for phenomena\nthat have a special treatment in the linguistic for-\nmalisms, namely discourse markers and temporal\nentities. Identifying named entities is negatively\nimpacted by GA with DM, where they are indis-\ncriminately labeled as compounds.\nContent words and function words are treated\ndifferently by each of the formalisms: UD and\nUCCA encode all words, where function words\nhave special labels, and DM only attaches con-\ntent words. Our guided attention ignores edge la-\nbels (dependency relations), and so it considers\nUD and UCCA‚Äôs attachment of function words just\nas meaningful as that of content words. Figure 8\nin Appendix G shows a breakdown of decoding\nperformance on content and function words for\nWehbe2014. We Ô¨Ånd that: a) all GA models and\nthe pretrained model show a higher function than\ncontent word decoding score, b) a large part of\nthe decrease in score of two of the three domain-\nÔ¨Ånetuned baselines (UD and DM) compared to the\npretrained model is due to content words.\nDiscrepancy between datasets While the mod-\nels Ô¨Åne-tuned with GA show considerable improve-\nment in brain decoding forWehbe2014 (word level\nanalysis), the improvements are much more modest\nfor Pereira2018 (sentence level analysis). A possi-\nble reason for this is the loss of structural informa-\ntion that occurs when aggregating over token rep-\nresentations to construct sentence-level ones. For\na more direct comparison, we conduct a sentence-\nlevel analysis for the Wehbe2014 dataset, mean\npooling over token hidden-states and their corre-\nsponding fMRI time slices15. If the advantage of\nthe guided attention models over the baseline drops,\nthis would indicate that mean pooling is at least\npartially responsible for the lower improvements\nobserved for Pereira2018. We Ô¨Ånd that this is in-\ndeed the case: in this setting, decoding scores for\nthe the GA models are not signiÔ¨Åcantly different\nfrom or lower than the pretrained baseline.\nCaveats The fMRI data used for both the sen-\ntence and word level analyses was recorded while\nparticipants read text without performing a speciÔ¨Åc\n15Note that since the Pereira2018 fMRI recordings are\ntaken and the sentence level and Wehbe2014 at the 4-gram\nlevel, the comparison is still approximate. A possible con-\nfounds is that averaging over fMRI time slices could also lead\nto loss of information.\nFigure 5: Change in F1-score per coarse-grained semantic class compared to the pretrained baseline for\nthe three guided attention Wehbe2014 models.\ntask. Although we observe some correlates of lin-\nguistic structure, it is possible that uncovering more\nÔ¨Åne-grained patterns would necessitate brain data\nrecorded while participants perform a targeted task.\nFor future work it would be interesting to inves-\ntigate if an analysis based on a continuous, natu-\nralistic listening fMRI dataset (Brennan and Hale,\n2019) matches up to the results we have obtained.\nRegarding the different linguistic formalisms, there\nare potential confounds such as domain, corpus\nsize16, and dependency length, (i.e. the distance be-\ntween words attached by a relation), which depend\nboth on the formalism and on the underlying train-\ning set text. To properly control for them, a corpus\nannotated for all formalisms is necessary, but such\na corpus of sufÔ¨Åcient size is not yet available.\nConclusions We propose a framework to investi-\ngate the effect of incorporating speciÔ¨Åc structural\nbiases in language models for brain decoding. We\npresent evidence that inducing linguistic structure\nbias through Ô¨Åne-tuning using attention guided ac-\ncording to syntacto-semantic formalisms, can im-\nprove brain decoding performance across two fMRI\n16It is interesting to note that decoding score rank for We-\nhbe2014 corresponds to Ô¨Åne-tuning corpus size for the GA\nmodels (DM > UD > UCCA), but not the domain-Ô¨Ånetuned\nmodels. A reasonable conclusion to draw from this is that\ndataset size might play a role in the effective learning of a\nstructural bias.\ndatasets. For each of the 3 investigated formalisms,\nwe observed that the models that aligned most with\nthe brain performed best at a range of subject-verb\nagreement syntactic tasks, suggesting that language\ncomprehension in the brain, as captured by fMRI\nrecordings, and the tested syntactic tasks may rely\non common linguistic structure, that was partly in-\nduced by the added attention constraints. Across\nformalisms, we found that models with attention\nguided by DM and UD consistently exhibited bet-\nter alignment with the brain than UCCA for both\nfMRI datasets. Rather than concluding that DM\nand UD are more cognitively plausible, controlled\nexperiments, with Ô¨Åne-tuning on each annotated\ncorpus as plain text, suggest that the text domain\nis an important, previously overlooked confound.\nFurther investigation is needed using a common\nannotated corpus for all formalisms to make con-\nclusions about their relative aptness.\nOverall, our proposed approach enables the eval-\nuation of more targeted hypotheses about the com-\nposition of meaning in the brain, and opens up new\nopportunities for cross-pollination between compu-\ntational neuroscience and linguistics. To facilitate\nthis, we make all code and data for our experiments\navailable at: https://github.com/mhany90/\nStructural_bias_brain\nReferences\nOmri Abend and Ari Rappoport. 2013. Univer-\nsal conceptual cognitive annotation (UCCA). In\nProceedings of the 51st Annual Meeting of the\nAssociation for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 228‚Äì238, SoÔ¨Åa,\nBulgaria. Association for Computational Lin-\nguistics.\nSamira Abnar, Lisa Beinborn, Rochelle Choenni,\nand Willem Zuidema. 2019. Blackbox meets\nblackbox: Representational similarity and sta-\nbility analysis of neural language models and\nbrains. arXiv preprint arXiv:1906.01539.\nLasha Abzianidze and Johan Bos. 2017. To-\nwards universal semantic tagging. arXiv preprint\narXiv:1709.10381.\nAndrew James Anderson, Jeffrey R Binder,\nLeonardo Fernandino, Colin J Humphries,\nLisa L Conant, Mario Aguilar, Xixi Wang, Do-\nnias Doko, and Rajeev DS Raizada. 2017. Pre-\ndicting neural activity patterns associated with\nsentences using a neurobiologically motivated\nmodel of semantic representation. Cerebral Cor-\ntex, 27(9):4379‚Äì4395.\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina\nGeorgescu, Kira GrifÔ¨Ått, Ulf Hermjakob, Kevin\nKnight, Philipp Koehn, Martha Palmer, and\nNathan Schneider. 2013. Abstract meaning rep-\nresentation for sembanking. In Proceedings of\nthe 7th Linguistic Annotation Workshop and In-\nteroperability with Discourse, pages 178‚Äì186,\nSoÔ¨Åa, Bulgaria. Association for Computational\nLinguistics.\nYonatan Belinkov, Llu√≠s M√†rquez, Hassan Sajjad,\nNadir Durrani, Fahim Dalvi, and James Glass.\n2018. Evaluating layers of representation in\nneural machine translation on part-of-speech\nand semantic tagging tasks. arXiv preprint\narXiv:1801.07772.\nJohan Bos, Valerio Basile, Kilian Evang, Noortje J\nVenhuizen, and Johannes Bjerva. 2017. The\ngroningen meaning bank. In Handbook of lin-\nguistic annotation, pages 463‚Äì496. Springer.\nJonathan R Brennan and John T Hale. 2019. Hi-\nerarchical structure guides rapid linguistic pre-\ndictions during naturalistic listening. PloS one,\n14(1):e0207741.\nJonathan R Brennan, Edward P Stabler, Sarah E\nVan Wagenen, Wen-Ming Luh, and John T Hale.\n2016. Abstract linguistic structure correlates\nwith temporal activity during naturalistic com-\nprehension. Brain and language, 157:81‚Äì94.\nEmanuele Bugliarello and Naoaki Okazaki.\n2019. Enhancing machine translation with\ndependency-aware self-attention. arXiv preprint\narXiv:1909.03149.\nCharlotte Caucheteux and Jean-R√©mi King. 2020.\nLanguage processing in brains and deep neural\nnetworks: computational convergence and its\nlimits. BioRxiv.\nGavin C Cawley and Nicola LC Talbot. 2010. On\nover-Ô¨Åtting in model selection and subsequent\nselection bias in performance evaluation. The\nJournal of Machine Learning Research, 11:2079‚Äì\n2107.\nWanxiang Che, Longxu Dou, Yang Xu, Yuxuan\nWang, Yijia Liu, and Ting Liu. 2019. Hit-scir\nat mrp 2019: A uniÔ¨Åed pipeline for meaning\nrepresentation parsing via efÔ¨Åcient training and\neffective encoding. In Proceedings of the Shared\nTask on Cross-Framework Meaning Representa-\ntion Parsing at the 2019 Conference on Natural\nLanguage Learning, pages 76‚Äì85.\nYun-Nung Chen, Dilek Hakkani-Tur, Gokhan Tur,\nAsli Celikyilmaz, Jianfeng Gao, and Li Deng.\n2016. Knowledge as a teacher: Knowledge-\nguided structural attention networks. arXiv\npreprint arXiv:1609.03286.\nAnn Copestake, Dan Flickinger, Carl Pollard, and\nIvan A Sag. 2005. Minimal recursion seman-\ntics: An introduction. Research on language\nand computation, 3(2-3):281‚Äì332.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of\ndeep bidirectional transformers for language un-\nderstanding. arXiv preprint arXiv:1810.04805.\nGeorgiana Dinu, Angeliki Lazaridou, and Marco\nBaroni. 2014. Improving zero-shot learning by\nmitigating the hubness problem. arXiv preprint\narXiv:1412.6568.\nRobert M. W. Dixon. 2010/2012. Basic Linguistic\nTheory. Oxford University Press.\nRoman Feldbauer, Maximilian Leodolter, Clau-\ndia Plant, and Arthur Flexer. 2018. Fast ap-\nproximate hubness reduction for large high-\ndimensional data. In 2018 IEEE International\nConference on Big Knowledge (ICBK) , pages\n358‚Äì367. IEEE.\nDan Flickinger, Stephan Oepen, and Emily M Ben-\nder. 2017. Sustainable development and reÔ¨Åne-\nment of complex linguistic annotations at scale.\nIn Handbook of Linguistic Annotation , pages\n353‚Äì377. Springer.\nStefan L Frank, Leun J Otten, Giulia Galli, and\nGabriella Vigliocco. 2015. The erp response to\nthe amount of information conveyed by words\nin sentences. Brain and language, 140:1‚Äì11.\nJon Gauthier and Anna Ivanova. 2018. Does the\nbrain represent words? an evaluation of brain de-\ncoding studies of language understanding. arXiv\npreprint arXiv:1806.00591.\nJon Gauthier and Roger Levy. 2019. Linking arti-\nÔ¨Åcial and human neural representations of lan-\nguage. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 529‚Äì539.\nYoav Goldberg. 2019. Assessing bert‚Äôs syntactic\nabilities. arXiv preprint arXiv:1901.05287.\nJan Hajic, Eva Hajicov√°, Jarmila Panevov√°, Petr\nSgall, Ondrej Bojar, Silvie Cinkov√°, Eva\nFuc√≠kov√°, Marie Mikulov√°, Petr Pajas, Jan\nPopelka, et al. 2012. Announcing prague czech-\nenglish dependency treebank 2.0. In LREC,\npages 3153‚Äì3160.\nDaniel Hershcovich, Omri Abend, and Ari Rap-\npoport. 2017. A transition-based directed\nacyclic graph parser for UCCA. In Proceedings\nof the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long\nPapers), pages 1127‚Äì1138, Vancouver, Canada.\nAssociation for Computational Linguistics.\nDaniel Hershcovich, Miryam de Lhoneux, Artur\nKulmizev, Elham Pejhan, and Joakim Nivre.\n2020. K√∏psala: Transition-based graph pars-\ning via efÔ¨Åcient training and effective encoding.\nIn Proceedings of the 16th International Con-\nference on Parsing Technologies and the IWPT\n2020 Shared Task on Parsing into Enhanced Uni-\nversal Dependencies, pages 236‚Äì244, Online.\nAssociation for Computational Linguistics.\nMichael E Houle. 2015. Inlierness, outlierness,\nhubness and discriminability: an extreme-value-\ntheoretic foundation. National Institute of Infor-\nmatics Technical Report NII-2015-002E, Tokyo,\nJapan.\nAngelina Ivanova, Stephan Oepen, Lilja √òvrelid,\nand Dan Flickinger. 2012. Who did what to\nwhom? a contrastive study of syntacto-semantic\ndependencies. In Proceedings of the Sixth Lin-\nguistic Annotation Workshop, pages 2‚Äì11, Jeju,\nRepublic of Korea. Association for Computa-\ntional Linguistics.\nShailee Jain and Alexander Huth. 2018. Incorpo-\nrating context into language encoding models\nfor fmri. bioRxiv, page 327601.\nHans Kamp and Uwe Reyle. 1993. From discourse\nto logic: introduction to modeltheoretic seman-\ntics of natural language, formal logic and dis-\ncourse representation theory. Studies in linguis-\ntics and philosophy.\nSimon Kornblith, Mohammad Norouzi, Honglak\nLee, and Geoffrey Hinton. 2019. Similarity of\nneural network representations revisited. arXiv\npreprint arXiv:1905.00414.\nNikolaus Kriegeskorte, Marieke Mur, and Peter A\nBandettini. 2008. Representational similarity\nanalysis-connecting the branches of systems neu-\nroscience. Frontiers in systems neuroscience ,\n2:4.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019.\nLinguistic knowledge and transferability of\ncontextual representations. arXiv preprint\narXiv:1903.08855.\nAlessandro Lopopolo, Stefan L Frank, Antal\nVan den Bosch, and Roel M Willems. 2017. Us-\ning stochastic language models (slm) to map lexi-\ncal, syntactic, and phonological information pro-\ncessing in the brain. PloS one, 12(5):e0177794.\nChristopher D Manning, Kevin Clark, John He-\nwitt, Urvashi Khandelwal, and Omer Levy. 2020.\nEmergent linguistic structure in artiÔ¨Åcial neural\nnetworks trained by self-supervision. Proceed-\nings of the National Academy of Sciences.\nRebecca Marvin and Tal Linzen. 2019. Targeted\nsyntactic evaluation of language models. Pro-\nceedings of the Society for Computation in Lin-\nguistics, 2(1):373‚Äì374.\nTom M Mitchell, Svetlana V Shinkareva, Andrew\nCarlson, Kai-Min Chang, Vicente L Malave,\nRobert A Mason, and Marcel Adam Just.\n2008. Predicting human brain activity asso-\nciated with the meanings of nouns. science,\n320(5880):1191‚Äì1195.\nJoakim Nivre, Marie-Catherine De Marneffe, Filip\nGinter, Yoav Goldberg, Jan Hajic, Christopher D\nManning, Ryan McDonald, Slav Petrov, Sampo\nPyysalo, Natalia Silveira, et al. 2016. Univer-\nsal dependencies v1: A multilingual treebank\ncollection. In Proceedings of the Tenth Interna-\ntional Conference on Language Resources and\nEvaluation (LREC‚Äô16), pages 1659‚Äì1666.\nJoakim Nivre, Marie-Catherine de Marneffe, Filip\nGinter, Jan Haji Àác, Christopher D. Manning,\nSampo Pyysalo, Sebastian Schuster, Francis Ty-\ners, and Daniel Zeman. 2020. Universal Depen-\ndencies v2: An evergrowing multilingual tree-\nbank collection. In Proceedings of The 12th\nLanguage Resources and Evaluation Conference,\npages 4034‚Äì4043, Marseille, France. European\nLanguage Resources Association.\nStephan Oepen, Marco Kuhlmann, Yusuke Miyao,\nDaniel Zeman, Silvie Cinkov√°, Dan Flickinger,\nJan Hajic, Angelina Ivanova, and Zdenka Ure-\n≈°ov√°. 2016. Semantic dependency parsing (sdp)\ngraph banks release 1.0 ldc2016t10. Web Down-\nload.\nStephan Oepen, Marco Kuhlmann, Yusuke Miyao,\nDaniel Zeman, Silvie Cinkov√°, Dan Flickinger,\nJan HajiÀác, and ZdeÀánka Ure≈°ov√°. 2015. SemEval\n2015 task 18: Broad-coverage semantic depen-\ndency parsing. In Proceedings of the 9th Inter-\nnational Workshop on Semantic Evaluation (Se-\nmEval 2015), pages 915‚Äì926, Denver, Colorado.\nAssociation for Computational Linguistics.\nStephan Oepen, Marco Kuhlmann, Yusuke Miyao,\nDaniel Zeman, Dan Flickinger, Jan Haji Àác, An-\ngelina Ivanova, and Yi Zhang. 2014. SemEval\n2014 task 8: Broad-coverage semantic depen-\ndency parsing. In Proceedings of the 8th In-\nternational Workshop on Semantic Evaluation\n(SemEval 2014), pages 63‚Äì72, Dublin, Ireland.\nAssociation for Computational Linguistics.\nFrancisco Pereira, Bin Lou, Brianna Pritchett,\nSamuel Ritter, Samuel J Gershman, Nancy Kan-\nwisher, Matthew Botvinick, and Evelina Fe-\ndorenko. 2018. Toward a universal decoder of\nlinguistic meaning from brain activation. Nature\ncommunications, 9(1):1‚Äì13.\nAlec Radford, Jeffrey Wu, Rewon Child, David\nLuan, Dario Amodei, and Ilya Sutskever. 2019.\nLanguage models are unsupervised multitask\nlearners. OpenAI Blog, 1(8):9.\nMartin Schrimpf, Idan Blank, Greta Tuckute, Ca-\nrina Kauf, Eghbal A. Hosseini, Nancy Kan-\nwisher, Joshua Tenenbaum, and Evelina Fe-\ndorenko. 2020. ArtiÔ¨Åcial neural networks ac-\ncurately predict language processing in the brain.\nbioRxiv.\nDan Schwartz, Mariya Toneva, and Leila Wehbe.\n2019. Inducing brain-relevant bias in natural\nlanguage processing models. In Advances in\nNeural Information Processing Systems, pages\n14123‚Äì14133.\nPetr Sgall, Eva Hajicov√°, and Jarmila Panevov√°.\n1986. The meaning of the sentence and its se-\nmantic and pragmatic aspects. academia.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nDe Marneffe, Samuel R Bowman, Miriam Con-\nnor, John Bauer, and Christopher D Manning.\n2014. A gold standard dependency corpus for\nenglish. In LREC, pages 2897‚Äì2904. Citeseer.\nEmma Strubell and Andrew McCallum. 2018. Syn-\ntax helps elmo understand semantics: Is syntax\nstill relevant in a deep neural architecture for srl?\narXiv preprint arXiv:1811.04773.\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for seman-\ntic role labeling. In Proceedings of the 2018\nConference on Empirical Methods in Natural\nLanguage Processing, pages 5027‚Äì5038.\nMariya Toneva and Leila Wehbe. 2019. Interpret-\ning and improving natural-language processing\n(in machines) with natural language-processing\n(in the brain). In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGlue: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353‚Äì355.\nShaonan Wang, Jiajun Zhang, Haiyan Wang, Nan\nLin, and Chengqing Zong. 2020. Fine-grained\nneural decoding with distributed word represen-\ntations. Information Sciences, 507:256‚Äì272.\nAlex Warstadt and Samuel R Bowman. 2020.\nCan neural networks acquire a structural bias\nfrom raw linguistic data? arXiv preprint\narXiv:2007.06761.\nLeila Wehbe, Ashish Vaswani, Kevin Knight, and\nTom M. Mitchell. 2014. Aligning context-based\nstatistical models of language with brain activ-\nity during reading. In Proceedings of the 2014\nConference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 233‚Äì243,\nDoha, Qatar. Association for Computational Lin-\nguistics.\nFrank Wilcoxon. 1992. Individual comparisons by\nranking methods. In Breakthroughs in statistics,\npages 196‚Äì202. Springer.\nYonghui Wu, Mike Schuster, Zhifeng Chen,\nQuoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao,\nKlaus Macherey, et al. 2016. Google‚Äôs neural\nmachine translation system: Bridging the gap\nbetween human and machine translation. arXiv\npreprint arXiv:1609.08144.\nYue Zhang, Rui Wang, and Luo Si. 2019. Syntax-\nenhanced self-attention-based semantic role la-\nbeling. arXiv preprint arXiv:1910.11204.\nA Injecting Structure by Predicting\nParse\nOne way to encode structural information from\neach of these formalisms into language model rep-\nresentations is to directly optimize for the predic-\ntion of the formalism graphs, i.e., parsing. For\nDM and UCCA, we use the HIT-SCIR parser (Che\net al., 2019), the best performing parser from the\nMRP 2019 Shared Task. For UD, we use the K√∏p-\nsala parser (Hershcovich et al., 2020) from the\nEUD Shared Task, which is largely based on the\nHIT-SCIR one. Both are transition-based parsers,\nwhich Ô¨Åne-tune BERT during training: BERT takes\nin a sequence S of P wordpieces and outputs a\nsequence of contextualized token representations\n[h1,...,h P ], which the parsers use as embeddings,\nÔ¨Åne-tuning the BERT model. Our assumption\nis that these representations are Ô¨Åne-tuned dur-\ning parser training to better capture the linguistic\ndistinctions made by each formalism. After Ô¨Åne-\ntuning on each formalism‚Äôs respective corpus, we\nextract sentence and word representations for all\nÔ¨Åne-tuned models as described above. Each of the\nparsers‚Äô default hyperparameters are employed.\nModel Epoch 0 Epoch 1 Epoch 2\nPereira et al. (2018)\nDM 0.278 0.201 0.167\nUD 0.277 0.186 0.159\nUCCA 0.277 0.189 0.161\nPRE 0.277\nWehbe et al. (2014)\nDM 0.225 0.126 0.083\nUD 0.225 0.092 0.065\nUCCA 0.225 0.110 0.077\nPRE 0.226\nTable 1: Brain decoding scores (Pearson‚Äôs r) for\neach of the BERT models Ô¨Åne-tuned via parsing,\nand for the pretrained baseline (PRE). Note that the\nlatter is not Ô¨Åne-tuned.\nResults for the models Ô¨Åne-tuned via pars-\ning show divergence in brain decoding perfor-\nmance. Indeed, we Ô¨Ånd that as parsing performance\n(as measured by unlabeled undirected attachment\nscores (UUAS)) improves on the held-out devel-\nopment set, brain decoding performance declines.\nThis Ô¨Ånding is congruent with the results of Gau-\nthier and Levy (2019), which show that Ô¨Åne-tuning\non GLUE tasks (Wang et al., 2018) leads to a de-\ncline in brain decoding performance, until a ceiling\npoint where it eventually stabilizes. In our exper-\niments, after one epoch of Ô¨Åne-tuning, decoding\nperformance is equivalent to the one achieved by\nthe pretrained model. However, with more Ô¨Åne-\ntuning, the models consistently diverge, as shown\nin Table 1. These results are averaged over two Ô¨Åne-\ntuning runs. Understanding the learning dynamics\nthat lead to such divergence is an interesting avenue\nfor future work.\nB Mean/median rank results\nTable 2 shows results for the Pearson‚Äôs r metric\nreported in the main paper, alongside the mean\nand median rank metrics reported in (Gauthier and\nLevy, 2019), which give the rank of a ground-\ntruth sentence representation in the list of nearest\nneighbors of a predicted sentence representation,\nordered by increasing cosine distance. This met-\nric evaluates representations based on their sup-\nport for contrasts between sentences/words which\nare relevant to the brain recordings. The table\nshows that the models which have higher Pear-\nson r scores, also have a lower average ground\ntruth word/sentence nearest neighbour rank i.e. in-\nduce representations that better support contrasts\nbetween sentences/words which are relevant to the\nbrain recordings.\nModel Pearson‚Äôs r Mean rank Median rank\nPereira et al. (2018)\nDF-B DM 0.269 33.58 12.95\nDF-B UD 0.277 32.91 13.03\nDF-B UCCA 0.259 37.05 15.09\nGA DM 0.280 32.66 12.39\nGA UD 0.286 30.79 11.44\nGA UCCA 0.268 34.54 13.77\nPRE 0.276 32.18 12.13\nWehbe et al. (2014)\nDF-B DM 0.204 493.11 89.32\nDF-B UD 0.206 497.24 81.69\nDF-B UCCA 0.164 689.89 227.30\nGA DM 0.343 172.45 10.96\nGA UD 0.280 255.127 18.28\nGA UCCA 0.261 315.73 25.78\nPRE 0.225 436.70 53.13\nTable 2: Brain decoding scores as measured via\nthree metrics ‚Äî Pearson‚Äôs r, Mean rank, and Me-\ndian Rank ‚Äî for each of the domain-Ô¨Ånetuned\nbaseline (DF-B) models, the guided attention mod-\nels (GA), and the pretrained (PRE) model.\nC SigniÔ¨Åcance testing\nBootstrapping The bootstrapping procedure is\ndescribed below. For each subject of msubjects:\n1. There are nstimuli sentences, corresponding\nto n fMRI recordings. A linear decoder is\ntrained to map each recording to its corre-\nsponding LM-extracted (PRE, DF-B,GA) sen-\ntence representation. This is done using 12-\nfold cross-validation. This yields predicted a\n‚Äòsentence representation‚Äô per stimuli sentence.\n2. To compensate for the small size of the dataset\nwhich might lead to a noise estimate of the\nlinear decoder‚Äôs performance, we now ran-\ndomly resample n datapoints (with replace-\nment) from the full ndatapoints.\n3. For each resampling, our evaluation metrics\n(pearson‚Äôsr, mean rank, etc.) are computed\nbetween the sampled predictions and their cor-\nresponding ‚Äògold representations‚Äô, for all sets\nof LM reps. We store the mean metric value\n(e.g. pearson r score) across the n‚Äòsampled‚Äô\ndatapoints. We run 5000 such iterations.\n4. This gives us 5000 such paired mean (across\nthe nsamples, that is) scores for all models.\n5. When comparing two models, e.g. GA DM\nvs.PRE, to test our results for strength of evi-\ndence of generalization over stimuli, we com-\npute the proportion of these 5000 paired sam-\nples where e.g. GA DM‚Äôs mean sample score\nis greater than PRE. After Bonferroni correc-\ntion for multiple hypothesis testing, is the p-\nvalue we report. See 3 for these per subject\np-values for Pereira 2018. For Wehbe 2014,\ncomparisons between each of the GA models\nand the pretrained baseline lead to p= 0.000\n(i.e. The GA model mean score is greater than\nthe pretrained baseline‚Äôs mean score for all\n5000 sets of paired samples), for all subjects.\nWe, therefore, do not include a similar table.\n6. We average over these 5000 samples per sub-\nject, and use these msubject means for the\nacross-subject signiÔ¨Åcance testing, which is\ndescribed below.\nStrength of generalization across subjects To\ntest our results for strength of generalization across\nsubjects, we apply the Wilcoxon signed rank test\n(Wilcoxon, 1992) to the mby-subject mean scores\n(see above), comparing the GA models to the pre-\ntrained baselines. Since m= 8for both datasets,\nthe lowest p-value is 0.0078 (if every subject‚Äôs dif-\nference score consistently favors the GA model\nover the baseline or vice versa).\nIn the case of Pereira 2018: for PRE vs. GA\nUD we get a p-value of 0.0078 (0.0234 after Bon-\nferroni correction); for PRE vs. GA DM we get\nan p-value of 0.015 (0.045 after Bonferroni correc-\ntion); for PRE vs. GA UCCA we get a p-value of\n0.0078 (0.0234 after Bonferroni correction, here\nPRE >GA UCCA for all subjects).\nIn the case of Wehbe 2014: all comparisons\nyield a p-value of 0.0078 (0.045 after Bonferroni\ncorrection), where the GAmodel >the pretrained\nbaseline.\nPereira et al. (2018)\nModel/Subject M02 M04 M07 M08 M09 M14 M15 P01\nGA UD 0.000 0.110 0.011 0.021 0.000 0.009 0.000 0.039\nGA DM 0.132 0.216 0.031 0.014 0.000 0.417 0.186 0.085\nGA UCCA0.014 0.015 0.052 0.041 0.452 0.002 0.000 0.003\nTable 3: p-values resulting from paired bootstrap\ntest described above, for each of the three GA mod-\nels when compared to the pretrained baseline.\nD The Domain effect\nTable 4 shows average word perplexity scores for\nthe pretrained model and the domain-Ô¨Ånetuned\nmodels for each of the three text domains on\nthe stimuli from Pereira2018 and Wehbe2014.\nScores are averaged over the words in a sentence\nand the sentences (stimuli) in the datasets.\nE Semantic Tagging\nProbing details Representations for the probing\ntask are derived as described in 3.1 for each sen-\ntence in the development and testing sets from\n(Abzianidze and Bos, 2017). The development set\nis employed as a training set, because it is mostly\nmanually annotated/corrected (as opposed to the\nmuch noisier training set) and because it is already\npossible to train rather accurate semantic taggers\nwhich sufÔ¨Åce for our analysis with a training set of\nthat size (131337 instances). We report results for\nthe ofÔ¨Åcial test set. Table 5 shows the frequency\nof each semantic category we report scores for in\nthe test set. An L2 regularised logistic regression\nmodel is utilised.\nFurther discussion We observe the largest im-\nprovements for the DISCOURSE and TEMPORAL\ncategories. The former involves identifying sub-\nordinate, coordinate, appositional, and contrast\nPereira et al. (2018)\nPRE 14.09\nDF-B DM 19.11\nDF-B UD 19.08\nDF-B UCCA 20.67\nGA DM 20.82\nGA UD 17.15\nGA UCCA 17.47\nWehbe et al. (2014)\nPRE 34.79\nDF-B DM 36.11\nDF-B UD 38.41\nDF-B UCCA 40.45\nGA DM 33.24\nGA UD 37.16\nGA UCCA 33.60\nTable 4: Average word perplexity scores for each of\nthe domain-Ô¨Ånetuned baseline (DF-B) models, the\nguided attention models (GA), and the pretrained\n(PRE) model.\nrelations. These relations are highly inÔ¨Çuenced\nby context, and correctly classifying them can of-\nten be contingent on longer dependencies, which\nthe structural bias increases ‚Äôawareness‚Äô of. The\nTEMPORAL category, on the other hand, consists\nof tags such as clocktime or time of day\nwhich are applied to multi-word expressions, e.g\n27th December. Highlighting these dependencies\nby assigning more weight to the attention between\ntheir sub-parts is likely helpful for their accurate\nidentiÔ¨Åcation.\nCategory / Frequency\nAttribute 63763\nUnamed Entity 48654\nLogical 32973\nNamed Entity 29271\nEvent 25338\nTense and Aspect 15208\nDiscourse 9948\nTemporal 5652\nTable 5: Semantic category frequency in the test\nset.\nF Targeted Syntactic Evaluation Scores\nFigures 6 and 7 show the performance of the\nPereira2018 and Wehbe2014 models and the four\nbaselines for each of the syntactic categories from\nMarvin and Linzen (2019).\nG Content words and function words\nanalysis\nFigure 8 shows the breakdown of brain decoding\naccuracy by content and function words for We-\nhbe2014. We consider content words as words\nwhose universal part-of-speech according to spaCy\nis one of the following: {ADJ, ADV , NOUN,\nPROPN, VERB, X, NUM}. Out of a total of 4369,\n2804 are considered content words and 1835 as\nfunction words.\nFigure 6: Targeted syntactic evaluation accuracy scores per category for Pereira2018 models.\nFigure 7: Targeted syntactic evaluation accuracy scores per category for Wehbe2014 models.\nDM UD UCCA\nFormalism\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Pearson r\nContent Words\nDM UD UCCA\nFormalism\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Pearson r\nFunction Words\npretrained\ndomain-finetuned\nFigure 8: Content word and function word brain decoding score (mean Pearson‚Äôs r) for all models\nÔ¨Åne-tuned by MLM with guided attention on each of the formalisms (points), as well the four baselines:\npretrained BERT, dotted line), and the domain-Ô¨Ånetuned BERT by MLM on each formalism‚Äôs training\ntext without guided attention (solid lines).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6637962460517883
    },
    {
      "name": "Principle of compositionality",
      "score": 0.5898526906967163
    },
    {
      "name": "Natural language processing",
      "score": 0.5277729034423828
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5019381046295166
    },
    {
      "name": "Cognitive science",
      "score": 0.4473447799682617
    },
    {
      "name": "Meaning (existential)",
      "score": 0.4316618740558624
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.4283478260040283
    },
    {
      "name": "Dependency (UML)",
      "score": 0.4210747182369232
    },
    {
      "name": "Deep linguistic processing",
      "score": 0.4174043536186218
    },
    {
      "name": "Linguistics",
      "score": 0.3617711365222931
    },
    {
      "name": "Psychology",
      "score": 0.3395538330078125
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ]
}