{
    "title": "Hierarchical Bayesian Language Models for Conversational Speech Recognition",
    "url": "https://openalex.org/W2116361377",
    "year": 2010,
    "authors": [
        {
            "id": "https://openalex.org/A2159562265",
            "name": "Songfang Huang",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": null,
            "name": "S Renals",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A2159562265",
            "name": "Songfang Huang",
            "affiliations": []
        },
        {
            "id": null,
            "name": "S Renals",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2122560783",
        "https://openalex.org/W160901726",
        "https://openalex.org/W2159782014",
        "https://openalex.org/W2075201173",
        "https://openalex.org/W2156833271",
        "https://openalex.org/W6675752074",
        "https://openalex.org/W2132957691",
        "https://openalex.org/W2083751884",
        "https://openalex.org/W6683650587",
        "https://openalex.org/W2154099718",
        "https://openalex.org/W6664420536",
        "https://openalex.org/W2015093644",
        "https://openalex.org/W1526361935",
        "https://openalex.org/W6628438334",
        "https://openalex.org/W2007553631",
        "https://openalex.org/W1967687583",
        "https://openalex.org/W4233559841",
        "https://openalex.org/W1516111018",
        "https://openalex.org/W2134963415",
        "https://openalex.org/W2111305191",
        "https://openalex.org/W6635726886",
        "https://openalex.org/W2134237567",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2158195707",
        "https://openalex.org/W6600402650",
        "https://openalex.org/W2069429561",
        "https://openalex.org/W6682569104",
        "https://openalex.org/W2053218206",
        "https://openalex.org/W2087309226",
        "https://openalex.org/W2027499299",
        "https://openalex.org/W2135194391",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2153773386",
        "https://openalex.org/W4232383088",
        "https://openalex.org/W1934041838",
        "https://openalex.org/W2045656233",
        "https://openalex.org/W2056250865",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W2103731025",
        "https://openalex.org/W1631260214",
        "https://openalex.org/W3141933106",
        "https://openalex.org/W2467379829",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W1424447973",
        "https://openalex.org/W2159399018",
        "https://openalex.org/W1579838312",
        "https://openalex.org/W1542491098",
        "https://openalex.org/W1508165687",
        "https://openalex.org/W10097614",
        "https://openalex.org/W2151967501",
        "https://openalex.org/W263845233",
        "https://openalex.org/W2143707047",
        "https://openalex.org/W2005902041",
        "https://openalex.org/W2082092506"
    ],
    "abstract": "Traditional n-gram language models are widely used in state-of-the-art large vocabulary speech recognition systems. This simple model suffers from some limitations, such as overfitting of maximum-likelihood estimation and the lack of rich contextual knowledge sources. In this paper, we exploit a hierarchical Bayesian interpretation for language modeling, based on a nonparametric prior called the Pitman--Yor process. This offers a principled approach to language model smoothing, embedding the power-law distribution for natural language. Experiments on the recognition of conversational speech in multiparty meetings demonstrate that by using hierarchical Bayesian language models, we are able to achieve significant reductions in perplexity and word error rate.",
    "full_text": "IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010 1941\nHierarchical Bayesian Language Models for\nConversational Speech Recognition\nSongfang Huang, Student Member, IEEE, and Steve Renals, Member, IEEE\nAbstract— Traditional\n -gram language models are widely\nused in state-of-the-art large vocabulary speech recognition sys-\ntems. This simple model suffers from some limitations, such as\noverﬁtting of maximum-likelihood estimation and the lack of\nrich contextual knowledge sources. In this paper, we exploit a\nhierarchical Bayesian interpretation for language modeling, based\non a nonparametric prior called Pitman–Y or process. This offers\na principled approach to language model smoothing, embedding\nthe power-law distribution for natural language. Experiments on\nthe recognition of conversational speech in multiparty meetings\ndemonstrate that by using hierarchical Bayesian language models,\nwe are able to achieve signiﬁcant reductions in perplexity and\nword error rate.\nIndex Terms— AMI corpus, conversational speech recognition,\nhierarchical Bayesian model, language model (LM), meetings,\nsmoothing.\nI. INTRODUCTION\nA\nLANGUAGE model (LM), which provides a predictive\nprobability distribution for the next word based on a his-\ntory of previously observed words, is an essential component\nof automatic speech recognition (ASR) systems. The dominant\nLM for most state-of-the-art large vocabulary ASR systems\nis the conventional\n-gram model, which approximates the\nhistory as the immediately preceding\n words. Due to\ndata sparsity,\n -gram models based on maximum-likelihood\nestimation (MLE) severely overﬁt the training data, resulting in\nunseen events being assigned zero probabilities [1]. Assigning\nzero probabilities to events observed in a test set which were\nnot observed in the training set is problematic for applications\nsuch as speech recognition and machine translation. The zero\nprobability problem is addressed by smoothing, for which a\nlarge number of methods have been proposed in the literature\n[2], [3], including Good–Turing [4], Katz back-off [5], deleted\nManuscript received May 08, 2009; revised November 17, 2009. Date of pub-\nlication January 19, 2010; date of current version September 01, 2010. This work\nwas supported in part by the Wolfson Microelectronics Scholarship and in part\nby the European IST Program Projects FP6-506811 (AMI) and FP6-033812\n(AMIDA). This work has made use of the resources provided by the Edinburgh\nCompute and Data Facility (ECDF) (http://www.ecdf.ed.ac.uk/), which is sup-\nported in part by the eDIKT initiative (http://www.edikt.org.uk/). This paper\nonly reﬂects the authors’ views and funding agencies are not liable for any use\nthat may be made of the information contained herein. The associate editor co-\nordinating the review of this manuscript and approving it for publication was\nProf. Haizhou Li.\nThe authors are with the Centre for Speech Technology Research, Univer-\nsity of Edinburgh, Edinburgh EH8 9AB, U.K. (e-mail: s.f.huang@ed.ac.uk; s.re-\nnals@ed.ac.uk).\nColor versions of one or more of the ﬁgures in this paper are available online\nat http://ieeexplore.ieee.org.\nDigital Object Identiﬁer 10.1109/TASL.2010.2040782\ninterpolation [6], interpolated Kneser–Ney [7], and modiﬁed\nKneser–Ney [2].\nAlthough the\n -gram LM has been demonstrated to be a\nsimple but effective model, the struggle to improve over it\ncontinues. Broadly speaking, such attempts focus on the im-\nproved modeling of word sequences, or on the incorporation of\nricher knowledge. Approaches which aim to improve on max-\nimum-likelihood\n-gram models of word sequences include\nneural network-based models [8], latent variable models [9],\nand a Bayesian framework [10]–[12]. The exploitation of richer\nknowledge has included the use of morphological information\nin factored LMs [13], syntactic knowledge using structured\nLMs [14], and semantic knowledge such as topic information\nusing nonparametric hierarchical Bayesian models [15].\nIn this paper, we propose the use of hierarchical Bayesian ap-\nproaches [16] to better model of word sequences in language\nmodels, in the context of a practical large-vocabulary conver-\nsational speech recognition system. Bayesian models have ex-\nplicitly declared prior assumptions, and an internally coherent\nframework for inference. They also have the advantages of in-\ncorporating additional knowledge sources and including them-\nselves in larger models in a principled manner. More specif-\nically, we present the application of hierarchical Pitman–Yor\nprocess language models (HPYLM) [12] on a large vocabulary\nmeeting transcription system [17], using large training corpora.\nThe HPYLM provides an alternative interpretation to language\nmodels in theory [11], [12], and a better smoothing algorithm\nfor language modeling in practice [18].\nThe main goal of this paper is to carry out a comprehensive\nstudy on the application of the HPYLM to large vocabulary\nASR. The HPYLM is a theoretically elegant language model\nﬁrst proposed in the machine learning ﬁeld [12]. In the speech\ncommunity, however, two questions remain interesting and have\nnot been studied before. First, will the HPYLM work for ASR\ntasks, which are normally evaluated in terms of word error rate\n(WER)? Second, is it possible to scale up the HPYLMs to work\non large-vocabulary ASR using large training corpora? In the\nrest of this paper, we provide our answers to these two questions,\nby extending our previous work in [18] including the presenta-\ntion of a parallel training algorithm, more detailed descriptions,\nmore experimental results, and a thorough discussion.\nFor the ﬁrst question, we verify the HPYLM in terms of both\nperplexity and WER using an efﬁcient computational imple-\nmentation. In recent years there has been a growing research\ninterest in the automatic transcription of multiparty meetings,\nwhich is typically one of the ﬁrst several essential steps for fur-\nther processing of meetings, such as information retrieval and\nsummarization. European projects AMI and AMIDA [17] are\n1558-7916/$26.00 © 2010 IEEE\n1942 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010\nexamples of such efforts. This has provided us a well-deﬁned\nbenchmark on which to evaluate a state-of-the-art large vocab-\nulary ASR system. We present comprehensive experimental\nresults on multiparty conversational meeting corpora, and\nobserve consistent and signiﬁcant reductions in perplexity and\nWER in comparison to the interpolated Kneser–Ney language\nmodel (IKNLM) [7] and the modiﬁed Kneser–Ney language\nmodel (MKNLM) [2], which are the state-of-the-art smoothing\nmethods for language modeling.\nIt is often expensive to do Bayesian inference on large\ntraining data. In order to obtain a sufﬁciently large language\nmodel for state-of-the-art large-vocabulary ASR systems, we\nhave developed a parallel algorithm for the estimation of an\nHPYLM, enabling the use of a large training corpus. We also\ndemonstrate that inference of an HPYLM converges quickly,\ntaking only a few tens of iterations to converge to a language\nmodel of comparable (or better) accuracy than the IKNLM or\nthe MKNLM.\nII. H\nIERARCHICAL BAYESIAN MODELS\nBayesian analysis explicitly uses probability to quantify de-\ngrees of belief. Bayes’ theorem (1) explains the relationship be-\ntween theprior, thelikelihood, and theposterior, where\n de-\nnotes the unknown parameters from the sample space\n, and\ndenotes the data from some sample space\n(continuous or discrete)\n(1)\nThe prior\n represents the belief in the parameters\nbefore\nobserving the data\n , and the posterior\n represents the\nupdated belief in the parameters after having observed the data\n. The likelihood is typically described using an exponential\nfamily distribution, which is mathematically convenient since\nconjugate priors exist for exponential family likelihood func-\ntions.\n1 Bayesian modeling differs from MAP (maximuma pos-\nteriori) by computing the posterior distribution of\nrather than\nits maximum: this can be computationally challenging, due to\nthe complexity of integrating over a large sample space\n.\nHierarchical Bayesian models have been well studied and en-\nable the construction of richer statistical models in which the\nprior may depend on parameters that are not involved in the\nlikelihood [16]. The prior distribution\n is itself a member\nof a family of densities withhyperparameters\n ,\n . The\nhierarchical Bayesian framework provides a better modeling of\nmulti-parameter problems for hierarchical data, where non-hi-\nerarchical models usually tend to inappropriately overﬁt such\ndata. Hierarchical Bayesian models can have enough parameters\nto ﬁt the data well, while using a population distribution to struc-\nture some dependence into the parameters, thereby avoiding\nproblems of overﬁtting [16].\nA. Bayesian Priors\nBayesian analysis begins with a prior distribution capturing\nany available prior knowledge about the process generating\n1A prior distribution is conjugate to the likelihood functions if Bayes’ theorem\nresults in a posterior distribution from the same family as the prior.\nthe data. We will introduce several popular prior distributions\nin Bayesian data analysis, including Dirichlet distributions,\nDirichlet processes, and Pitman–Yor processes.\n1) Dirichlet Distribution: The Dirichlet distribution [19], a\nmultivariate generalization of the beta distribution, is a density\nover a\n-simplex, i.e.,\n -dimensional vectors\n whose\ncomponents\n are all non-negative and sum to 1. The Dirichlet\ndistribution is parameterized by a\n -dimensional measure\nwhere\n is a normalized measure over\n components\nand\n is a positive scalar. The Dirichlet distri-\nbution with parameters\n has a probability density function\ngiven by\n(2)\nwith normalization constant\nwhere\n is the gamma function. The vector\n represents the\nmean of the Dirichlet distribution, and\nis a concentration pa-\nrameter with larger values of\ndrawing samples away from the\ncorners of the simplex to the centre, peaking around the mean\n. One reason why the Dirichlet distribution is selected as a\nprior is because it is conjugate to the multinomial distribution:\ni.e., the posterior is also a Dirichlet distribution when the prior is\na Dirichlet distribution and the likelihood is a multinomial. We\nuse\nto denote a Dirichlet density with hyperparame-\nters\n . When\n , the Dirichlet distribution is equivalent\nto the beta distribution.\n2) Dirichlet Process: The Dirichlet process (DP) is a sto-\nchastic process, ﬁrst formalized in [20] for general Bayesian\nmodeling, which has become an important prior distribution\nfor nonparametric models. Nonparametric models are charac-\nterized by allowing the number of model parameters to grow\nwith the amount of training data. This helps to alleviate over- or\nunder-ﬁtting problems, and provides an alternative approach to\nparametric model selection or averaging.\nA random distribution\nover a space\n is called a Dirichlet\nprocess distributed with base distribution\nand strength or con-\ncentration parameter\n ,i f\n(3)\nfor every ﬁnite measurable partition\n of\n [20]. We\nwrite this as\n , and it may be interpreted as a dis-\ntribution over distributions. The parameter\n, a measure over\n,\nis intuitively the mean of the DP. The parameter\n, on the other\nhand, can be regarded as an inverse variance of its mass around\nthe mean\n, with larger values of\nfor smaller variances. More\nimportantly in inﬁnite mixture models,\n controls the expected\nnumber of mixture components in a direct manner, with a larger\nimplying a larger number of mixture componentsa priori.\nDraws from an DP are composed as a weighted sum of point\nmasses located at the previous draws\n . This leads to a\nconstructive deﬁnition of the DP called the stick-breaking con-\nstruction [21]. If we construct\n as follows:\n(4)\nHUANG AND RENALS: HIERARCHICAL BAYESIAN LMs FOR CONVERSATIONAL SPEECH RECOGNITION 1943\nThen\n .\n is a unique value among\n ,\nand\n denotes a point mass at\n . The construction of\n can\nbe understood as follows [22]. Starting with a stick of length\n1, ﬁrst break it at\n , assign\n to be the length of stick just\nbroken off. Then recursively break the other portion to obtain\n,\n and so forth. The stick-breaking distribution over\nsat-\nisﬁes\n with probability one. This deﬁnition is im-\nportant for inference of a DP.\nGiven observed values of\n , the posterior distribu-\ntion of\n is again distributed according to another DP with up-\ndated hyperparameters, with the following form [22]:\n(5)\nwhere the posterior base distribution is a weighted average be-\ntween the prior base distribution\nand the empirical distribu-\ntion\n .\n3) Pitman–Yor Process: The Pitman–Yor process or the two-\nparameter Poisson–Dirichlet process [23], [24]\nis a three-parameter distribution over distributions, where\nis\na discount parameter,\n a strength parameter, and\n a base\ndistribution that can be understood as a mean of draws from\n. When\n , the Pitman–Yor process reverts to\nthe Dirichlet process\n . In this sense, the Pitman–Yor\nprocess is a generalization of the Dirichlet process.\nThe procedure for generating draws\nfrom a Pitman–Yor process can be described using the “Chi-\nnese Restaurant” metaphor [24], [25]. Imagine a Chinese restau-\nrant containing an inﬁnite number of tables, each with inﬁnite\nseating capacity. Customers enter the restaurant and seat them-\nselves. The ﬁrst customer sits at the ﬁrst available table, while\neach of the subsequent customers sits at an occupied table with\nprobability proportional to the number of customers already sit-\nting there\n, or at a new unoccupied table with probability\nproportional to\n , where\n is the current number of oc-\ncupied tables. That is, if\n is the index of the table chosen by\nthe\n customer, then the\n customer sits at table\n given\nthe seating arrangement of the previous\n customers\nwith probability\n(6)\nwhere\n is the number of customers sitting at table\n and\nis the total number of customers. The Pitman–Yor\nprocess with parameters\n produces a power-law distri-\nbution with index\n over the number of customers seated at\neach table [11]. The power-law distribution—a few outcomes\nhave very high probability and most outcomes occur with low\nprobability—has been found to be one of the most striking\nstatistical properties of word frequencies in natural language.\nB. Bayesian Inference\nIn general, there are two types of approximate inference al-\ngorithms in Bayesian analysis for the estimation of posterior\ndistributions of interest: Monte Carlo methods and variational\nmethods. Monte Carlo methods [16], [26] use random samples\nto simulate probabilistic models. They are guaranteed to give ar-\nbitrarily precise estimates with sufﬁcient computation.Markov\nchain Monte Carlo (MCMC) methods are a family of itera-\ntive Monte Carlo algorithms that draw samples from an other-\nwise intractable target density via a ﬁrst-order Markov process.\nGibbs sampling, a special case of the Metropolis–Hastings al-\ngorithm [26], is one of the most widely used MCMC methods\nfor Bayesian inference. It assumes that it is tractable to sample\nfrom the conditional distribution of one of these variables given\nthe other\nones. We will use Gibbs sampling methods\nfor inference in this paper.\nVariational methods [27], [28], on the other hand, are a class\nof deterministic approximations to the problems of learning and\ninference for Bayesian inference. A variational method begins\nby expressing a statistical inference task as the solution to a\nmathematical optimization problem [29]. By approximating or\nrelaxing the objective function, one can derive computationally\ntractable algorithms which bound or approximate the statistics\nof interest. Sudderth [29] gives a good review and comparison\nof various Bayesian inference algorithms.\nIII. S\nMOOTHING N-GRAM LANGUAGE MODELS\nThe goal of an\n -gram model is to estimate the conditional\nprobability distribution over next words given the context\n(ap-\nproximated by the immediately preceding\n words) from a\ntraining corpus:\n(7)\nThe simplest way to estimate the required\n-gram probabilities\nis maximum-likelihood estimation, which uses the frequencies\nof co-occurrences of word\nfollowing the context\n , and\nmaximizes the likelihood over the training data. This has\na poor generalization ability because\n -gram training data\nis sparse, leading to zero probability estimates for\n -grams\nnot observed in the training corpus. In order to alleviate\nthis problem, a number of smoothing techniques have been\nproposed for\n-gram models estimated using maximum like-\nlihood. Good–Turing smoothing [4], [5], which is based on\nleave-one-out estimation, re-estimates the probability mass\nassigned to\n-grams with zero counts by making use of the\nfrequency of\n -grams occurring only once. Absolute dis-\ncounting [30], the root of Kneser–Ney smoothing [7], subtracts\na ﬁxed absolute discount\nfrom each nonzero count, and\nredistributes the unallocated probability mass to those unseen\nevents. In general, it is also useful to take advantage of the\n-gram hierarchy for smoothing methods through back-off [5]\nor interpolation [6]. Back-off relies on only the lower order\n-grams when smoothing unseen events, while interpolation\nlinearly interpolates higher order\n -gram models with lower\norder\n -gram models. To obtain a smoother LM, interpolated\nKneser–Ney smoothing [7] utilizes absolute discounting, mod-\niﬁed counts for lower order\n-gram probabilities\n ,\nand interpolation with low order\n -gram probabilities\n(8)\n1944 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010\nwhere\n is the total number of word tokens fol-\nlowing the context\n ,\n is the context one word shorter than\n,\n is the number of distinct words\noccurring after\n , and the discount\n is dependent on the\nlength of the context. Modiﬁed Kneser–Ney smoothing extends\ninterpolated Kneser–Ney smoothing by allowing three different\ndiscount parameters,\n,\n , and\n for\n -grams with\none, two, and three or more counts, respectively [2]. Counts\nof counts statistics are used to estimate the optimal values for\naverage discounts in the MKNLM. As an alternative to these\nsmoothing schemes, class-based\n-gram language models [31]\nhave been used, in which word classes or clusters (either hand-\ndesigned or automatically induced) help to address the data spar-\nsity problem.\nIn the Bayesian framework for language modeling, a prior\ndistribution is placed over the predictive distribution of interest\nfor LMs in (7), and the posterior distribution is inferred from\nthe observed training data. The ﬁnal predictive probability can\nthen be estimated from the posterior by marginalizing out the\nlatent variables and hyperparameters. The Bayesian interpreta-\ntion essentially avoids the zero-probability problem to estimate\nsmoother LMs by taking advantage of knowledge expressed by\nthe priors.\nThere has been considerable prior work in which prior\ndistributions are placed over LM parameters. Nádas [32] used\na beta-binomial model (the beta distribution is the conjugate\nprior to the binomial) in which the so-called “empirical Bayes”\nmethod was used to obtain point estimates of the hyperparam-\neters of the prior distribution, by maximizing the likelihood\non the training data rather than by full Bayesian inference.\nMore recently Yamanet al. [33] proposed a structural Bayesian\nlanguage modeling and adaptation framework, employing a\nDirichlet prior density on\n-grams assembled in a tree structure.\nIn this case MAP estimation was employed to estimate the\nlanguage model parameters.\nGoodman [34] analyzed back-off Kneser–Ney smoothing in\na Bayesian manner, in which an exponential prior distribution\nis used in conjunction with a maximum entropy model. This\nanalysis provides a justiﬁcation for Kneser–Ney smoothing, in\nterms of the discounting procedure and the satisfaction of mar-\nginal constraints when estimating lower order\n-grams.\nA full Bayesian approach to language modeling was in-\ntroduced by MacKay and Peto [10], which extended Nádas’\nempirical Bayes framework to a hierarchical Dirichlet LM, by\nusing Dirichlet distributions as the priors. The predictions of\nhierarchical Dirichlet LMs are similar to those of a traditionally\nsmoothed LM. MacKay and Peto demonstrated in this way, on\na small corpus, that a hierarchical Dirichlet language model\nhad comparable performance to a bigram model smoothed\nby deleted interpolation with speciﬁc values of interpolation\nweight.\nIt was argued by Goldwateret al. [11] that a Pitman–Yor\nprocess is more suitable as a prior distribution than a Dirichlet\ndistribution to applications in natural language processing, as\nthe power-law distributions of word frequencies produced by\nPitman–Yor processes more closely resemble the heavy-tailed\ndistributions observed in natural language. The hierarchical ex-\ntension of the Pitman–Yor process—HPYLM—was indepen-\ndently proposed for language modelling by Goldwateret al.\n[11] and by Teh [12]. The HPYLM can be considered as a nat-\nural generalization of the hierarchical Dirichlet language model\n[10], by using a Pitman–Yor process rather than the Dirichlet\ndistribution. Experiments on the AP News corpus showed that\nthe novel hierarchical Pitman–Yor process language model pro-\nduces results superior to hierarchical Dirichlet language models\nand\n-gram LMs smoothed by interpolated Kneser–Ney (IKN),\nand comparable to those smoothed by modiﬁed Kneser–Ney\n(MKN) [12]. Wood and Teh [35] additionally extended the hi-\nerarchical Hierarchical Bayesian Language Models for Conver-\nsational Speech Recognition language model for domain adap-\ntation of language models.\nIV . H\nIERARCHICAL BAYESIAN LANGUAGE MODELS\nBASED ON PITMAN–YOR PROCESSES\nWe introduce a Bayesian language model based on\nPitman–Yor processes using a hierarchical framework. This\nsection brieﬂy summarizes the original work on the HPYLM\n[11], [12], and refers to our previous work [18].\nA. Hierarchical Pitman–Yor Process Language Models\nThe Pitman–Yor process can be used to create a two-stage\nlanguage modeling framework [11]. Following the Chinese\nrestaurant metaphor discussed in Section II-A3, a language\nmodel can be viewed as a restaurant in which each table has a\nlabel of a word\ngenerated by\n . Each customer repre-\nsents a word token, so that the number of customers at a table\ncorresponds to the frequency of the lexical word labeling that\ntable. A customer may only be assigned to a table whose label\nmatches that word token.\nConsider a vocabulary\nwith\n word types. Let\nbe the unigram probability of\n , and\nrepresents the\nvector of word probability estimates for unigrams. A\nPitman–Yor process prior is placed over\nwith an uninformative base distribution\n for all\n. According to the Chinese restaurant metaphor, cus-\ntomers (word tokens) enter the restaurant and seat themselves\nat either an occupied table or a new one, with probabilities\nexpressed in (6). Each table has a label\ninitialized by the\nﬁrst customer seated on it, and the next customer can only sit\non those tables with the same label. Those\n customers that\ncorrespond to the same word label\n, can sit at different tables,\nwith\n denoting the number of tables with labels\n. Given the\nseating arrangement\n of customers, and the hyperparameters\nand\n , the predictive probability of a new word\nis given\nin (9), by collecting probabilities in (6) corresponding to each\nlabel\nfor tables\n(9)\nwhere\n equals to 1 if table\n has the label of\n , and 0 oth-\nerwise,\n is the total number of customers, and\nis the total number of tables, in the restaurant for\nHUANG AND RENALS: HIERARCHICAL BAYESIAN LMs FOR CONVERSATIONAL SPEECH RECOGNITION 1945\nFig. 1. Hierarchy of Pitman–Yor process priors for/110 -gram LMs. Pitman–Yor\nprocesses are placed recursively as priors over the/110 -order predictive distribu-\ntions until we reach the unigram model/71\n . /25 /40 /117 /41 denotes the back-off context\nof /117 .\nunigrams. Averaging over seating arrangements and hyperpa-\nrameters, we can obtain the probability\n for a unigram LM.\nWhen\n , the Pitman–Yor process reduces to a Dirichlet\ndistribution\n , and (9) becomes the predictive distri-\nbution in a Bayesian language model using the Dirichlet distri-\nbution as the prior [10]\n(10)\nThis can be regarded as an additive smoothing of the empir-\nical probability\n , by balancing the empirical counts\nwith the additive pseudo-counts\n of the prior\nDirichlet distribution.\nWe can generalize the above unigram example to the\n-gram\ncase. An\n -gram LM deﬁnes a probability distribution over the\ncurrent word given a context\n consisting of\n words. Let\nbe the probability of the current word\n and\nbe the target probability distribution given the con-\ntext\n . A Pitman–Yor process is served as the prior over\n ,\nwith discounting parameter\n and strength parameter\n spe-\nciﬁc to the length of the context\n . The base distribution is\n, the lower order model of probabilities of the current\nword given all but the earliest word in the context. That is,\n(11)\nSince\n is still an unknown probability distribution, a\nPitman–Yor process is recursively placed over it with parame-\nters speciﬁc to\n ,\n .\nThis is repeated until we reach\n for a unigram model dis-\ncussed above. This results in a hierarchical prior (Fig. 1),\nenabling us to generalize from the unigram to the\n -gram\ncase. There are multiple restaurants (Pitman–Yor processes)\nin the prior hierarchy, with each corresponding to one context.\nBy using the hierarchical framework of Pitman–Yor priors,\ndifferent orders of\n-gram can thus share information with each\nother, similar to the traditional interpolation of higher order\n-grams with lower order\n -grams.\nBased on this overall framework for an HPYLM, a central\ntask is the inference of seating arrangements in each restaurant\nand the estimation of the context-speciﬁc parameters from the\ntraining data. Given training data\n , we know the number of\nco-occurrences of a word\n after a context\n of length\n ,\n. This is the only information we need to train an HPYLM.\nAn MCMC algorithm can be used to infer the posterior distri-\nbution of seating arrangements. We use Gibbs sampling to keep\ntrack of which table each customer sits at, by iterating over all\ncustomers present in each restaurant—ﬁrst removing a customer\nfrom the restaurant\n , and then adding the customer\n back\nto the restaurant\n by resampling the table at which that cus-\ntomer sits. After a sufﬁcient number of iterations, the states of\nvariables of interest in the seating arrangements will converge\nto the required samples from the posterior distribution. In the\nHPYLM the more frequent a word token, the more likely it is\nthere are more tables corresponding to that word token.\nFor an\n-gram LM, there are\n parameters\nto be estimated in total. We use a sampling\nmethod based on auxiliary variables [36].\nUnder a particular setting of seating arrangements\nand hy-\nperparameters\n , the predictive probability\n can\nbe obtained similarly to the case for unigram in (9) for each\ncontext\n(12)\nin which if we set the discounting parameters\n for all\n ,\nwe resort to a hierarchical Dirichlet language model (HDLM)\n[10], similar to (10). The HDLM and the HPYLM share the\nsame idea of interpolation with the lower order\n-grams. The\ndifference is that the HPYLM explores discounts from empirical\ncounts, while the HDLM does not.\nThe overall predictive probability can be approximately\nobtained by collecting\nsamples from the posterior over\nand\n , and then averaging (12) to approximate the integral\nwith samples\n(13)\nIf we assume that the strength parameters\n for all\n, and restrict\n to be at most 1 (i.e., all customers repre-\nsenting the same word token should only sit on the same table\ntogether), then the predictive probability in (12) directly reduces\nto the predictive probability given by the IKNLM in (8). We can\nthus interpret IKN as an approximate inference scheme for the\nhierarchical Pitman–Yor process language model [12].\nB. Inference\nIn the HPYLM, we are interested in the posterior distri-\nbution over the latent predictive probabilities\nand the hyperparameters\n, given the training data\n . The hierarchical Chi-\nnese restaurant process represents it as the seating arrangement,\ndenoted by\nall contexts\n , in the corresponding\nrestaurant, by marginalizing out each\n . The central task for\nthe inference is thus to infer the posterior distribution over\nthe seating arrangements\n and the\n1946 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010\nFig. 2. Partition of an HPYLM into sub-HPYLMs, denoted by dotted rectangles, for the parallel inference. The dotted circles represent pseudo/71\n /115 to complete a\nPitman–Yor process hierarchy, and collect additional insertion/deletion information. Each circle corresponds to a context, or a restaurant in theChinese restaurant\nmetaphor.\nhyperparameters\n given\nthe training data\n . With\nthe posterior, we can calculate the predictive probabilities\naccording to (12) and (13) by further integrating out\nand\n .\nWe follow inference schemes based on MCMC for the HPYLM\n[12], [36], depicted in algorithm 1.\nAlgorithm 1 InferHpylm\n : An Algorithm for the Inference\nof an HPYLM, Where\n is the Order of LMs,\n is the Context\n(Restaurant),\n is the Word Token (Customer),\n is the\nNumber of Occurrences of\n After Context\n .\n , and\n are\nthe Discount and Strength Parameters of Pitman–Yor Processes\nfor\n-Grams of Order\n .\nprocedure INFERHPYLM (order\n )\ninput: the order of\n -gram with\n1 for each order\n to\n do\n2 for each context\n of order\n do\n3 for each word\n appearing after context\n do\n4 for\n to\n do iterations for\n times\n5 if REMOVECUSTOMER\n then\n6A DDCUSTOMER\n ;\n7 endif\n8 endfor\n9 endfor\n10 endfor\n11\n SAMPLEPARAMS\n ; /* sampling\nhyperparameters\n for order\n */\n12 endfor\nFor A DDCUSTOMER\n , we use (6) to sample a\nnew seating table in restaurant\n for customer\n .F o r\nREMOVECUSTOMER\n , we simply remove a customer\nfrom\n table according to the probability proportional to\nthe number of customers already seated there\n .F o r\nSAMPLEPARAMS\n , a sampling method based on auxiliary\nvariables is used for sampling the hyperparameters\n and\n ,\nwhich assumes that each discount parameter\n has a prior beta\ndistribution\n while each strength parameter\nhas a prior gamma distribution\n[12], [36].\nC. Parallelization\nIt is computationally expensive in terms of computing time\nand memory requirements to infer an HPYLM from a large\ncorpus. This motivated us to design a divide-and-conquer\nalgorithm to efﬁciently estimate an HPYLM. The algorithm\nhas two steps:data partition and model combination. Generally\nspeaking, we divide the inference task, which is normally\ninfeasible or expensive using a single machine, into sub-tasks\nthat ﬁt well to the computational capacity of a single com-\nputing node—alleviating the memory requirement and allowing\nsub-tasks to be run in parallel.\nIn the data partition step, we ﬁrst divide word types in the\nvocabulary\ninto subsets\n . For each subset\n , we then\ncompose those bigrams beginning with words\n , and their\ncorresponding child\n -grams with\n , as a sub-HPYLM\n(dotted rectangles), and put a pseudo\n (dotted circles) as the\nPitman–Yor process for unigrams of the sub-HPYLM, as shown\nin Fig. 2. Each sub-HPYLM can be inferred separately using\nthe same routines as those for a normal HPYLM, except that the\npseudo\nnow additionally collects the number of insertion and\ndeletion for customer\n . The inference of sub-HPYLMs\ncan be executed in parallel.\nIn the model combination step, we combine all the\nsub-HPYLM models level-by-level in the HPY hierarchy.\nFor each level, we accumulate auxiliary parameters, and\nsample the hyperparameters\n and\n . For the global\n for\nunigrams, we infer the seating arrangements by using the in-\nsertion and deletion statistics accumulated by each pseudo\n,\nHUANG AND RENALS: HIERARCHICAL BAYESIAN LMs FOR CONVERSATIONAL SPEECH RECOGNITION 1947\nTABLE I\nSTATISTICS OF THE TRAINING AND TESTING DATA SETS\nFOR LANGUAGE MODELS IN THERT06S TASK\nto make sure the HPYLM is consistent regarding themodiﬁed\ncounts for lower order\n -grams [7], [12].\nOne approximation we made during this parallelism is the\ninference for unigram\n , which is inferred based on pseudo\nof sub-HPYLMs and not globally optimized. However, in\nSection VI-C we show experimentally that this approximation\ndoes not signiﬁcantly affect the performance in terms of per-\nplexity or WER.\nThe parallel training algorithm enables us to estimate an\nHPYLM on a large corpus using a large vocabulary. We de-\nscribe the parallel training algorithm for the HPYLM in more\ndetail in [37].\nD. Implementation\nWe implemented the hierarchical Pitman–Yor process lan-\nguage model by extending the SRILM toolkit [38]. We highlight\nfour characteristics of this implementation. First, it is consistent\nand coherent with the existing SRILM software. We inher-\nited the HPYLM classes from the base SRILM classes, and\nprovided the same interfaces for language modeling. Second,\nit has efﬁcient memory management and computational per-\nformance by directly using the data structures available in\nSRILM. Third, it is a ﬂexible framework for Bayesian language\nmodeling. We can, for example, train a language model with\nKneser–Ney smoothing for unigrams, modiﬁed Kneser–Ney\nsmoothing for bigrams, and Pitman–Yor process smoothing for\ntrigrams. Finally, this implementation is extensible for future\ndevelopments: e.g., taking into accounts the combination with\nmultimodal cues for language models via probabilistic topic\nmodels.\nThis implementation of an HPYLM outputs a standard ARPA\nformat LM, with an identical format to a conventional\n-gram\nLM. This makes it easy to evaluate the HPYLM in a conven-\ntional ASR system.\nV. EXPERIMENTS AND RESULTS\nA. Experimental Setup\nThe experiments reported in this paper were performed using\nthe U.S. National Institute of Standard and Technology (NIST)\nTABLE II\nSTATISTICS OF THE FIVEFOLD CROSS VALIDATION\nSETUP OF THEAMI SCENARIO MEETINGS\nRich Transcription (RT) 2006 spring meeting recognition\nevaluation data (RT06s).2 We tested only on those audio data\nrecorded from individual headset microphones (IHM), con-\nsisting of meeting data collected by the AMI project, Carnegie\nMellon University (CMU), NIST, and Virginia Tech (VT).\nThe training data sets for language models used in this paper,\nand the test transcription, are listed in Table I. The web-data\nfor meetings and conversational speech were collected from the\nworld wide web using strategies described in [39]. We exploited\ndifferent combinations of these training data sets for the fol-\nlowing experiments.\nA second corpus we consider in this paper is a domain-spe-\nciﬁc meeting corpus—the AMI Meeting Corpus,\n3 [40] which\nconsists of 100 hours of multimodal meeting recordings with\ncomprehensive annotations at a number of different levels.\nAbout 70% of the corpus was elicited using a design scenario,\nin which the participants play the roles of employees—project\nmanager, marketing expert, user interface designer, and in-\ndustrial designer—in an electronics company that decides to\ndevelop a new type of television remote control. We used the\nscenario part of the AMI Meeting Corpus for our experiments,\nin a ﬁvefold cross validation setup. There are 137 scenario\nmeetings in total, as shown in Table II.\nMost of the following experiments used a common vocabu-\nlary with 50 000 word types, unless explicitly indicated other-\nwise. For the ﬁvefold cross-validation experiments on the sce-\nnario AMI meetings, the vocabulary was slightly tuned for each\nfold, while keeping the vocabulary size ﬁxed to 50 000 word\ntypes. These vocabularies were those used in the AMI-ASR\nsystem [41].\nThe lower discounting cutoffs of the\n-gram counts (i.e.,\n,\n , and\n for ngram-count in the\nSRILM toolkit [38]) were set to 1 in all the LMs used in the\nfollowing experiments.\nB. Perplexity Experiments\nWe took the LM data sets from No.1 to No.5 in Table I as a\ncore training set, named MTRAIN, which consists of 205 814\nsentences and 1 847 201 words. We trained trigram IKN, MKN,\nHD, and HPY LMs using this training data. For the HDLM and\nthe HPYLM, we ran 200 iterations for inference, and collected\n100 samples from the posterior over seating arrangements and\nhyperparameters.\n2http://www.nist.gov/speech/tests/rt/\n3http://corpus.amiproject.org\n1948 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010\nTABLE III\nPERPLEXITY RESULTS ONRT06SEV ALTESTING DATA\nTABLE IV\nPERPLEXITY RESULTS OF THE FIVEFOLD CROSS VALIDATION\nON THE AMI SCENARIO MEETINGS\nThe test data for perplexity estimation was extracted from the\nreference transcriptions forrt06seval. The ﬁnal test data con-\nsisted of 3597 sentences and 31 810 words. Four different exper-\nimental conditions were considered and are shown in Table III:\nthe combination of whether or not a closed vocabulary (CV/OV)\nwas used and/or mapping unknown words to a special symbol\n“UNK”\nduring training and testing. If the “UNK” symbol\nis not included in the vocabulary, all unknown words will be\nskipped from the training data for the estimation and from the\ntesting data for the evaluation.\nTable III shows the perplexity results. We can see that in all\nfour experiment conditions, the HPYLM has a lower perplexity\nthan both the IKNLM and the MKNLM, and the HDLM has\nthe highest perplexity, which is as expected and consistent with\nthe previous results in [12]. We used the CV condition, i.e.,\nwith a closed vocabulary but without the “UNK” symbol, when\ntraining an LM in all the rest of experiments.\nTable IV shows the perplexity results on the ﬁvefold cross val-\nidation of the scenario AMI Meeting Corpus, using the CV con-\ndition. The HPYLM again has a consistently lower perplexity\nthan both the IKNLM and the MKNLM, and the HDLM again\nis the worst.\nC. Word Error Rate Experiments\nWe used the AMI-ASR system [41] as the baseline plat-\nform for our ASR experiments. The feature stream comprised\nof 12 MF-PLP features and raw log energy and ﬁrst– and\nsecond-order derivatives are added. Cepstral mean and variance\nnormalisation was performed on a per channel basis. The\nacoustic models were taken from the second pass of AMI-ASR\nsystem, which were trained on 108 hours speech data from\nICSI, ISL, NIST, and AMI, using vocal tract length normal-\nization, heteroscedastic linear discrimant analysis, speaker\nadaptive training, and minimum phone error discriminative\ntraining. They are adapted using the transcripts of the ﬁrst pass\nand a single constrained maximum-likelihood linear regression\ntransform. We only tested LMs trained using training data\nMTRAIN (see Table I) under condition CV in Table III, that\nTABLE V\nWER (%) RESULTS ONRT06SEV ALTESTING DATA\nis, we used a 50 k vocabulary but without mapping unknown\nwords to “UNK” during training. For the HDLM and the\nHPYLM, we output an ARPA format LM. Different LMs were\nthen used in the ﬁrst pass decoding using\n .4\nTable V shows the WER results for the RT06s task. Unsur-\nprisingly, the HDLM produces the highest WER. The HPYLM,\nhowever, results in a lower WER than both the IKNLM and the\nMKNLM. These reductions by the HPYLM from the IKNLM\nand the MKNLM are both signiﬁcant using a matched-pair sig-\nniﬁcance test [42], with\nand\n , respectively.\nThis is an encouraging result, since it is the ﬁrst time that the\nHPYLM has been tested using a state-of-the-art large-vocabu-\nlary ASR system on standard evaluation data.\nTable VI shows the WER results for the ﬁvefold cross-vali-\ndation experiments on the scenario AMI meetings. We observed\nstatistically signiﬁcant\nreductions in WER by the\nHPYLM, which are consistent among all the ﬁve folds and the\nscenario AMI meetings as a whole. Our experiments also show\nthat the HDLM consistently gives highest WERs. We therefore\nstop presenting results for the HDLM in the rest of experiments.\nWe used only the transcriptions of the scenario AMI meetings\nto train LMs.\nD. Scalability\nTo investigate the scalability of the HPYLM, we gradually\nincreased the size of training data for the HPYLM, as shown\nin Table VII. MTRAIN includes the training data sets No.1–5.\nconsists of MTRAIN and the No.6 data set\nFisher-p1. Further adding the data set No.7 to\n we\nobtained ALL-WC. Finally, we put together all the data No.1–8\nas shown in Table I, named ALL.\nFor MTRAIN,\n , and ALL-WC, experiments\nwere carried out on a machine with dual quad-core Intel Xeon\n2.8-GHz processors and 12 GB of memory. Table VII shows\nthe computational time per iteration and memory requirements\nwhen we change the size of training data, or vary the size of the\nvocabulary. From the results in Table VII, we can see that the\ntraining time for each iteration scales linearly with the size of\ntraining data when vocabulary size is constant. The smaller the\nsize of the vocabulary, the quicker each iteration and the lower\nthe memory requirement. The observations conﬁrm the neces-\nsity of proposing a parallel training algorithm for the HPYLM.\nFor IKNLM and MKNLM trained on ALL-WC, the memory\nrequirement is around 1 GB.\nFor ALL, it would be extremely demanding to train an\nHPYLM on this data set using a single machine, due to the\ncomputational time and memory limitations. We instead used\nthe parallel training algorithm described in Section IV-D. We\n4http://htk.eng.cam.ac.uk/\nHUANG AND RENALS: HIERARCHICAL BAYESIAN LMs FOR CONVERSATIONAL SPEECH RECOGNITION 1949\nTABLE VI\nWER (%) RESULTS OF FIVEFOLD CROSS VALIDATION ON THESCENARIO\nAMI MEETINGS.A LL THE REDUCTIONS BY THE HPYLM WITH\nRESPECT TO THE IKNLM AND THE MKNLM ARE STATISTICALLY\nSIGNIFICANT,W ITH /112/60 /48 /58 /48/48 /49\nTABLE VII\nCOMPARISON OF COMPUTATIONAL TIME AND MEMORY REQUIREMENT\nOF THE HPYLM ON DIFFERENT TRAINING DATA SETS.\nDATA SET NUMBERS REFER TO TABLE I\ndivided the inference into 50 sub-tasks. It turns out that it is\nfeasible to train an HPYLM on such a large corpus of more\nthan 200 million word tokens, using a vocabulary of 50 k\nwork types in this way. The inference took around one day to\nﬁnish 100 iterations, although this was highly dependent on\nsubmission and queueing times of the compute cluster. For\nALL, we evaluated two HPYLM models—one after 32 Gibbs\nsampling iterations (HPYLM-iter32), and the other after 100\niterations (HPYLM-iter100).\nWe again evaluated perplexity performance over these four\ndata sets to investigate the scalability of perplexity experiments.\nThe perplexity results in Table VIII indicate that the HPYLM\nscales well to larger training data. We obtained consistent re-\nductions in perplexity over both the IKNLM and the MKNLM.\nThis further strengthens the perplexity results of Section V-B.\nFor two HPYLM models trained on ALL, we did not observe a\nsigniﬁcant difference in perplexity between HPYLM-iter32 and\nHPYLM-iter100.\nFinally we trained three types of ARPA format trigram\nLMs—IKNLM, MKNLM, and HPYLM—on both ALL-WC\nTABLE VIII\nPERPLEXITY RESULTS ON RT06SEV ALUSING DIFFERENT\nSCALE SIZES OF TRAINING DATA\nTABLE IX\nWER (%) RESULTS ON RT06SEV ALUSING DIFFERENT\nSCALE SIZES OF TRAINING DATA\n(a corpus of around 50 million word tokens) and ALL (a corpus\nof around 210 million word tokens) training data sets. Table IX\nshows the WER results of these three different LMs in the ﬁrst\ndecoding using\n. On ALL-WC, we see the HPYLM\nperforms slightly better than the IKNLM and the MKNLM.\nSigniﬁcance testing shows the reductions by the HPYLM are\nnot signiﬁcant. On ALL, however, we observed signiﬁcant\nreductions in WER by using the HPYLM, with\nand\n for reductions over the IKNLM and MKNLM,\nrespectively. Once again, there is no signiﬁcant difference in\nWER between HPYLM-iter32 and HPYLM-iter100.\nE. Data Combination Versus Model Interpolation\nGiven several text corpora, i.e., those seven shown in\nTable XI, there are two different ways to estimate a language\nmodel. Data combination simply concatenates those seven\ncorpora and trains a single language model on the combined\ncorpus, without considering the differences between the cor-\npora. This is the way in which we trained most LMs for\nthe above experiments. Model interpolation , on the contrary,\nestimates seven separate language models on the corpora\nrespectively, and linearly interpolates these seven LMs using\nsome development data to optimize the interpolation weights.\nWe have demonstrated the effectiveness of the HPYLM in the\ndata combination style. Since model interpolation is commonly\nused in most state-of-the-art ASR systems, it is worthwhile for\nus to investigate the case of model interpolation.\nWe ﬁrst investigated the experiments on rt06seval.W e\ntrained four sets of language models separately on MTRAIN,\nFISHER, WEBMEET, and WEBCONV in Table I, with each\nset using interpolated Kneser–Ney, modiﬁed Kneser–Ney, and\nhierarchical Pitman–Yor process smoothing respectively. For\neach type of smoothing method, we interpolated the four sepa-\nrate language models using a development set of the evaluation\ndata from NIST RT05srt05seval (with 2216 sentences, 16 282\nword tokens). The ﬁnal language models were obtained by\n1950 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010\nTABLE X\nPERPLEXITY (PPL) AND WER (%) RESULTS OF COMPARING DATA\nCOMBINATION ANDMODEL INTERPOLATION ONRT06SEV ALTESTING DATA\nTABLE XI\nSTATISTICS OF CORPORA, AND THE PERPLEXITY RESULTS\nON THE FOLD 1 OF THE SCENARIO AMI MEETINGS\ninterpolating with the optimal weights. Table X shows the per-\nplexity and WER results for model interpolation onrt06seval.\nIn comparison to data combination results for ALL (perplexity\nin Table VIII and WER in Table IX), we ﬁnd model interpo-\nlation has both lower perplexity and lower WER. Considering\nthe results for model interpolation, the HPYLM has slightly\nlower perplexity, but statistically insigniﬁcant WER, than the\nIKNLM and the MKNLM.\nWe additionally carried out the experiments on the ﬁvefold\nscenario AMI meetings in Table II to compare the two cases,\nusing fold 2–5 as the training data and fold 1 as the testing data.\nWe furthermore included six extra corpora in Table XI as the\ntraining data for LMs. For data combination, we reached a com-\nbined corpus of 157 million word tokens in total. The parallel\ntraining algorithm was used to infer an HPYLM on this com-\nbined corpus, with 100 Gibbs sampling iterations. For model\ninterpolation, we trained IKNLMs, MKNLMs, and HPYLMs\n(with 100 iterations) separately, using the seven training cor-\npora, respectively. Table XI shows the perplexity results, in-\ndicating that the HPYLM consistently has a lower perplexity\nthan the IKNLM and the MKNLM on each LM component. We\nused a fourfold cross validation on folds 2–5 of the scenario\nAMI meetings to tune the optimal interpolated weights. Each\ntime we took one fold from 2–5 as the development set, and\nthe remaining three as the training data on which we trained the\nIKNLM, MKNLM, and HPYLM, respectively. The weights for\nthe seven LM components were then optimized using the devel-\nopment set, for the IKNLM, MKNLM, and HPYLM, respec-\ntively. We considered the average of the accumulated weights\nas the ﬁnal optimal weights, which were used to estimate an in-\nterpolated LM.\nTable XII shows the perplexity and WER results on fold 1.\nIt is not surprising to ﬁnd that model interpolation is superior\nto data combination, because model interpolation weights the\nLM components of different domains to better match the testing\ndata. Model interpolation provides signiﬁcantly better results\nthan data combination in perplexity and WER. We observed\nTABLE XII\nPERPLEXITY AND WER (%) RESULTS OFCOMPARING DATA COMBINATION\nAND MODEL INTERPOLATION USING THE FOLD 1 OF THE SCENARIO\nAMI MEETINGS IN TABLE II AS THE TESTING DATA\nmuch higher perplexity results from data combination compared\nto model interpolation, due to the fact that a large portion of\nout-of-domain data (Hub4-lm96) was weighted identically to\nthe in-domain meeting data in data combination. In either data\ncombination or model interpolation, however, the HPYLM con-\nsistently has a lower perplexity result, and signiﬁcantly\nlower WERs than the IKNLM and the MKNLM (although\nthe absolute reductions are small for model interpolation). This\nsuggests that we can train separate HPYLMs on several dif-\nferent corpora, and then use the standard method to interpolate\nthese separate HPYLMs. This further consolidates our claim\nthat the HPYLM is a better smoothing method than the IKNLM\nand the MKNLM for practical ASR tasks. It is more desirable,\nhowever, for a method to automatically weight and interpolate\nseveral HPYLMs directly within the hierarchical Pitman–Yor\nprocess framework. Wood and Teh [35] have proposed a model\nwithin the hierarchical Pitman–Yor process framework for do-\nmain adaptation. This approach, however, can only deal with\ntwo components, the in-domain LM and the out-of-domain LM,\nrespectively. Additionally the computational complexity should\nbe considered when doing interpolation for large corpora.\nVI. A\nNALYSIS ANDDISCUSSIONS\nA. Convergence\nIt is often expensive to train an HPYLM, especially when\nworking with large training corpora as demonstrated in\nTable VII. Therefore, the convergence of HPYLM is an impor-\ntant factor. We trained an HPYLM using the data set MTRAIN\nin Table I. During each iteration, we collected the log likelihood\nover the training data, and the predictive log likelihood over the\ntesting data rt06seval. Fig. 3 shows the convergence of likeli-\nhoods over training and testing data for the ﬁrst 150 iterations.\nFrom this we can see that after about 20 iterations, the HPYLM\nhas quickly converged to a lower predictive log likelihood\nvalue on the testing data, which roughly remains the same for\nfurther iterations. On the other hand, although it is slow to train\nan HPYLM on large corpora, we only need to train the model\nonce and output an ARPA format LM, then apply it in an ASR\nsystem as a standard\n-gram LM. We also observed that the\nlikelihood over the training data decreases after more and more\niterations, while the likelihood over the testing data increases,\nwhich means the generalization of the HPYLM improves.\nThe ﬁnding is further conﬁrmed by from the experimental\nperplexity and WER results in Tables VIII and IX, respectively,\nfor two HPYLM models, one from the 32nd iteration and the\nHUANG AND RENALS: HIERARCHICAL BAYESIAN LMs FOR CONVERSATIONAL SPEECH RECOGNITION 1951\nFig. 3. Convergence of the HPYLM. The log likelihood over the training data\nMTRAIN (top), and the log likelihood over the testing datart06seval (bottom),\nto investigate the convergence of the HPYLM with iterations.\nother from the 100th iteration. There is no signiﬁcant difference\nbetween these two models, which indicates normally we need\nonly tens of iterations to infer an HPYLM model, since the pos-\nterior of the HPYLM is well-behaved.\nB. Average Discounts\nFor a speciﬁc order of an\n-gram LM, there is only one dis-\ncount\n for the IKNLM. The MKNLM, instead, has three dif-\nferent discount parameters,\n ,\n , and\n , according to\nthe counts of\n -grams. This additional ﬂexibility of discounting\nin the MKNLM has been proven to be superior in practice to\nthe IKNLM [2]. We ﬁnd from (12) that the HYPLM has a more\nrelaxed discounting scheme, by allowing each context to use a\ndifferent discounting value:\n. It was noted by Teh [36]\nthat the expected number of tables\n in a Pitman–Yor process\nscales as\n where\n is the number of customers and\nis the discount parameter of the Pitman–Yor process. There-\nfore, the actual amount of discount,\n , will grow slowly\nas the count\n grows.\nTo demonstrate the rich-get-richer property of discounting in\nthe HPYLM, we investigate an HPYLM trained for 300 itera-\ntions using MTRAIN (meeting data) in Table VII, and plot av-\nerage discounts as a function of trigram counts in Fig. 4. We\nadditionally plot on the bottom of Fig. 4 thecounts of counts\nstatistics used to do the averaging over discounts. The counts of\ncounts histogram exhibits a long-tailed distribution: most words\noccur with lower counts, while only a few words occur fre-\nquently. We ﬁnd that the growth of average discounts in the\nHPYLM is sublinear, with respect to counts of trigrams. For a\ncomparison, the IKNLM uses a single disccounting parameter:\n, and the MKNLM uses three different discounts:\n. It is interesting\nto note that discounts in the HPYLM for trigrams with large\nvalues of counts are also surprisingly large, which reminds us\nto question whether or not it is enough to use only one discount\nparameter\nin the MKNLM for all\n -grams with three\nor more counts. Finally we note that similar counts may have\nsubstantial differences in average discounts, especially in the\ncase of larger counts. This may arise from local effects during\nthe sample-based inference of the seating arrangements—more\nlikely in a restaurant with more customers (larger counts)—-and\nalso because discounts for larger counts are averaged over fewer\ntrigrams (count of counts changes inversely with counts), which\nmakes it look less smoothed for large counts.\nC. Validation of Parallelization Approximation\nTo measure experimentally the errors introduced by the par-\nallel training algorithm, we trained two HPYLMs of order 3\non FISHER shown in Table I for 100 iterations, one without\nparallelism, while the other with the parallel training algorithm\nintroduced in Section IV-C. We evaluated the two HPYLMs\nfor the transcription ofrt06seval, recording the perplexity re-\nsults for each iteration. Fig. 5 shows that there is no statisti-\ncally signiﬁcant difference between perplexity results of these\ntwo HPYLMs, which indicates that the error caused by the ap-\nproximation in parallel training algorithm can be ignored. We\nalso ﬁnd no signiﬁcant difference in the WER [37].\nD. Learning Curve\nTo further verify that the HPYLM is a suitable choice for\nlanguage modeling when there is a large amount of data, we\ninvestigated the learning curve of the HPYLM with respect\nto the amount of training data. We considered ALL, a corpus\nof 211 million word tokens consisting of data sets No.1–8\nin Table I. We ﬁrst randomly reordered the corpus, and then\nvaried the training set size between 1 million and 211 million\nwords by gradual increments to obtained training sets of 1,\n10, 25, 50, 100, 200, and 211 million words. We trained the\nIKNLM, MKNLM, and HPYLM using the CV condition on\nthese training sets, respectively, and evaluated the language\nmodels on the RT06s test data (rt06seval), as in Section V-B.\nFig. 6 shows the learning curve of perplexity results as the\namount of training data increases. We see that, although there is\na runtime overhead, the HPYLM consistently outperforms the\nIKNLM and the MKNLM as the size of training data increases.\nMoreover, the reductions by the HPYLM become larger with\nlarger amount of training data available.\nE. Comparison to Other Models\nIn our experiments, the HPYLM produces consistently better\nperplexity results than the MKNLM. This observation is in con-\ntrast to the ﬁnding in [12], where the HPYLM performs slightly\nworse than the MKNLM in terms of perplexity. We argue that\nthere are two potential reasons for this. First, [12] used conju-\ngate gradient descent in the cross-entropy on the validation set\nto determine the discounting parameters for the IKNLM and the\nMKNLM, which is a different approach to that used in most\nstandard language model toolkits such as SRILM [38]. We used\n1952 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010\nFig. 4. Average discounts as a function of trigram counts (top), and counts of trigram counts (bottom) in the HPYLM trained on the data set MTRAIN, with\ndifferent scales for horizontal axis: ﬁrst 50 counts (left), ﬁrst 100 counts (middle), and ﬁrst 1000 counts(right).\nFig. 5. Perplexity results by iterations, trained on FISHER and tested on RT06s\nevaluation datart06seval for the HPYLM, to validate parallelization approxima-\ntion.\nFig. 6. Learning curve of perplexity results on RT06s evaluation datart06seval\nfor the HPYLM with respect to the amount of training data.\nin this paper the SRILM toolkit to build reasonable baseline re-\nsults and evaluate our various LMs. Second, the difference in\nthe data and the implementation may be another reason. Based\non the discussion in Section VI-B, however, we believe that it\nmakes sense for the HPYLM to outperform the MKNLM, be-\ncause of its more ﬂexible discounting scheme for each different\ncontext.\nThere are different interpretations for the unusual modiﬁed\ncounts for lower order\n-grams in the IKNLM. Kneser and Ney\n[7], and Chen and Goodman [2], derived the modiﬁed counts\nfor lower order\n-grams in terms of preserving marginal word\ndistribution constraints. Goodman [34] justiﬁed this from a\nBayesian view of a maximum entropy model with an exponen-\ntial prior, and clearly explained why the Kneser–Ney smoothing\nhas that particular form including the modiﬁed counts for lower\norder\n-grams. The maximum entropy model, to which the\nback-off version of Kneser–Ney smoothing forms an approxi-\nmation, also preserves the marginal constraints. The HPYLM\nis a full Bayesian interpretation of interpolated Kneser–Ney as\napproximate inference in a hierarchical Bayesian model con-\nsisting of Pitman–Yor processes [12]. The modiﬁed counts for\nlower order\n-grams,\n , are derived directly from Chinese\nrestaurant processes and different from those in the IKNLM,\nsubject to the following relationships among\n and\n[12]:\nif\nif\nwhere\n is the number of tables seated by\n customers in\nthe child restaurant of\n . The value of\n , and consequently\nthe modiﬁed count\n , are naturally determined by the seating\narrangements induced from Chinese restaurant processes. If we\nconstrain\n , the modiﬁed count\n is the\nsame as that in the IKNLM. However, the HPYLM also satisﬁes\nmarginal constraints when the strength parameter\nfor all\n, as proved in [36].\nThe structural Bayesian language model by Yamanet al.\n[33] shares similar ideas to the hierarchical Dirichlet language\nmodel [10] (although it is estimated by a MAP approach),\nin that both models assemble\n-grams in a tree structure and\nHUANG AND RENALS: HIERARCHICAL BAYESIAN LMs FOR CONVERSATIONAL SPEECH RECOGNITION 1953\nuse the Dirichlet distribution as the prior to smooth the em-\npirical counts. In comparison to the HPYLM, however, both\nmodels suffer from performance improvements compared to\nstate-of-the-art smoothing methods such as IKN and MKN, for\nlack of one important issue for language model smoothing—ab-\nsolute discounting.\nVII. C\nONCLUSION\nIn this paper, we present the application of hierarchical\nPitman–Yor process language models on a large-vocabulary\nASR system for conversational speech, using reasonably large\ncorpora. We show comprehensive experimental results on\nmultiparty conversational meeting corpora, with the observa-\ntion that the HPYLM outperforms both the IKNLM and the\nMKNLM.\nOverall, we hope to convey our judgment that it is feasible,\nand worthwhile, to use the HPYLM for applications in large-vo-\ncabulary ASR systems. In detail, the conclusions we make in\nthis paper are as follows: 1) the HPYLM provides an alterna-\ntive interpretation, Bayesian inference, to language modeling,\nwhich can be reduced to interpolated Kneser–Ney smoothing\n[12]; 2) the HPYLM provides a better smoothing algorithm for\nlanguage modeling in practice, which has better perplexity and\nWER results than both the IKNLM and the MKNLM, consis-\ntent and signiﬁcant; 3) HPYLM training converges in relatively\nquickly; 4) a parallel training scheme makes it possible to es-\ntimate models in the case of large training corpora and large\nvocabularies.\nThe main contributions of the HPYLM work in this paper\ninclude the introduction of a novel Bayesian language mod-\neling technique to the ASR community, and the experimental\nveriﬁcation on the task of large-vocabulary ASR for conver-\nsational speech in meetings. We have demonstrated that it is\nfeasible to infer a hierarchical non-parametric Bayesian lan-\nguage model from a large corpus, thus making it practical to use\nfor large vocabulary speech recognition or machine translation.\nOur experimental results have shown that any approximations\nresulting from the parallel algorithm have a negligible effect\non performance. Overall, the HPYLM results in signiﬁcantly\nimproved accuracy compared with the current state-of-the-art\n(IKNLM/MKNLM). The resulting language model may be in-\nterpreted as a smoothed\n-gram model, can be implemented in a\nstandard way (e.g., using an ARPA format language model ﬁle),\nand may be used in place of other smoothed\n-gram language\nmodels.\nAPPENDIX A\nPROGRAM AND SCRIPTS FOR THEHPYLM\nThe executable program (hpylm), the corresponding Python\nscripts, and part of the text data used in this paper to train and test\nan HYPLM, are available from http://homepages.inf.ed.ac.uk/\ns0562315/.\nTABLE XIII\nMEETING IDS FOR THE5F OLDS OF THEAMI SCENARIO MEETINGS.T HE\nINITIAL LETTER “S” REPRESENTS THE SCENARIO MEETINGS\nAPPENDIX B\nTHE FIVEFOLD CROSS-VALIDATION SPLIT-UP\nWe include below in the Table XIII the meeting identities\nfrom the scenario AMI Meeting Corpus for the ﬁvefold cross\nvalidation in Section V.\nAPPENDIX C\nMATCHED-PAIR SIGNIFICANCE TEST\nWe use in this paper the matched-pair signiﬁcance test\nmethod [42] for signiﬁcance testing of WER results in the ASR\nexperiments.\nSuppose\n and\n are the ﬁltered hypothese\nﬁles, generated by the NIST scoring tool\n . We use the\nscript\n from the SRILM toolkit [38] to com-\npute the matched pairs:\nLet\n be the number of times the two systems’ hypotheses\ndiffer, and\n be the number of times new system improves upon\nbaseline. We compute the probability that a fair coin would have\ncome out heads at least\ntimes in totally\n trials:\nas the signiﬁcance p-value, with\n to be signiﬁ-\ncant, and\n to be signiﬁcant but weak.\nACKNOWLEDGMENT\nThe authors would like to thank the anonymous reviewers for\ntheir excellent comments, the AMI-ASR team for providing the\nbaseline ASR system for experiments, and Y. W. Teh for helpful\ndiscussions and sharing his Matlab codes to double-check our\nimplementation.\nR\nEFERENCES\n[1] F. Jelinek, Statistical Methods for Speech Recognition . Cambridge,\nU.K.: MIT Press, 1997.\n1954 IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 18, NO. 8, NOVEMBER 2010\n[2] S. F. Chen and J. Goodman, “An empirical study of smoothing tech-\nniques for language modeling,”Comput. Speech Lang., vol. 13, no. 4,\npp. 359–393, 1999.\n[3] J. T. Goodman, “A bit of progress in language modeling,”Comput.\nSpeech Lang., pp. 403–434, 2001.\n[4] I. J. Good, “The population frequencies of species and the estimation\nof population parameters,”Biometrika, vol. 40, no. 3/4, pp. 237–264,\n1953.\n[5] S. M. Katz, “Estimation of probabilities from sparse data for the lan-\nguage model component of a speech recogniser,”IEEE Trans. Acoust.,\nSpeech, Signal Process., vol. ASSP-35, no. 3, pp. 400–401, Mar. 1987.\n[6] F. Jelinek and R. L. Mercer, E. S. Gelsema and L. N. Kanal, Eds., “In-\nterpolated estimation of Markov source parameters from sparse data,”\nin Proc. Workshop Pattern Recognition in Practice, 1980, pp. 381–397.\n[7] R. Kneser and H. Ney, “Improved backing-off for/109 -gram language\nmodeling,” Proc. ICASSP’95, pp. 181–184, 1995.\n[8] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural proba-\nbilistic language model,”J. Mach. Learn. Res., vol. 3, pp. 1137–1155,\n2003.\n[9] J. Blitzer, A. Globerson, and F. Pereira, “Distributed latent variable\nmodels of lexical co-occurrences,” inProc. 10th Int. Workshop Artif.\nIntell. Statist., 2005.\n[10] D. J. C. MacKay and L. C. B. Peto, “A hierarchical Dirichlet language\nmodel,” Natural Lang. Eng., vol. 1, no. 3, pp. 1–19, 1994.\n[11] S. J. Goldwater, T. L. Grifﬁths, and M. Johnson, “Interpolating between\ntypes and tokens by estimating power-law generators,”Adv. Neural Inf.\nProcess. Syst. 18, 2006.\n[12] Y. W. Teh, “A hierarchical Bayesian language model based on\nPitman–Yor processes,” inProc. Annu. Meeting ACL, 2006, vol. 44.\n[13] J. A. Bilmes and K. Kirchhoff, “Factored language models and gener-\nalized parallel backoff,” inProc. HLT/NACCL, 2003, pp. 4–6.\n[14] P. Xu, A. Emami, and F. Jelinek, “Training connectionist models for\nthe structured language model,” inProc. Empirical Methods Natural\nLang. Process. (EMNLP’03), 2003.\n[15] S. Huang and S. Renals, “Unsupervised language model adaptation\nbased on topic and role information in multiparty meetings,” inProc.\nInterspeech’08, Sep. 2008, pp. 833–836.\n[16] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data\nAnalysis, 2nd ed. London, U.K.: Chapman & Hall/CRC, 2004.\n[17] S. Renals, T. Hain, and H. Bourlard, “Recognition and interpretation\nof meetings: The AMI and AMIDA projects,” inProc. IEEE Work-\nshop Autom. Speech Recognition Understanding (ASRU’07), 2007, pp.\n238–247.\n[18] S. Huang and S. Renals, “Hierarchical Pitman–Y or language models\nfor ASR in meetings,” in Proc. IEEE ASRU’07 , Dec. 2007, pp.\n124–129.\n[19] C. E. Antoniak, “Mixtures of Dirichlet processes with application to\nnon-parametric problems,”Ann. Statist., vol. 2, no. 6, pp. 1152–1174,\n1974.\n[20] T. S. Ferguson, “A Bayesian analysis of some nonparametric prob-\nlems,” Ann. Statist., vol. 1, no. 2, pp. 209–230, 1973.\n[21] J. Sethuraman, “A constructive deﬁnition of Dirichlet priors,”Statist.\nSinica, vol. 4, pp. 639–650, 1994.\n[22] Y. W. Teh, “Dirichlet processes,” inEncyclopedia Mach. Learn. .: ,\n2007.\n[23] J. Pitman and M. Yor, “The two-parameter Poisson-Dirichlet distribu-\ntion derived from a stable subordinator,”Ann. Probability, vol. 25, no.\n2, pp. 855–900, 1997.\n[24] J. Pitman, “Exchangeable and partially exchangeable random parti-\ntions,” Probability Theory and Related Fields, vol. 102, pp. 145–158,\n1995.\n[25] D. Aldous, “Exchangeability and related topics,” in École d’Été\nde Probabilités de Saint-Flour, ser. XIII–1983 . Berlin, Germany:\nSpringer, 1985, pp. 1–198.\n[26] C. Andrieu, N. de Freitas, A. Doucet, and M. I. Jordan, “An introduc-\ntion to mcmc for machine learning,”Mach. Learn., vol. 50, no. 1, pp.\n5–43, Jan. 2003.\n[27] B. J. Frey and N. Jojic, “A comparison of algorithms for inference and\nlearning in probabilistic graphical models,”IEEE Trans. Pattern Anal.\nMach. Intell., vol. 27, no. 9, pp. 1392–1416, Sep. 2005.\n[28] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul, “An in-\ntroduction to variational methods for graphical models,”Mach. Learn.,\nvol. 37, no. 2, pp. 183–233, 1999.\n[29] E. B. Sudderth, “Graphical models for visual object recognition and\ntracking,” Ph.D. dissertation, Mass. Inst. of Technol., Cambridge, MA,\n2006.\n[30] H. Ney, U. Essen, and R. Kneser, “On structuring probabilistic depen-\ndences in stochastic language modelling,”Comput. Speech Lang., vol.\n8, no. 1, pp. 1–38, 1994.\n[31] P. F. Brown, V. J. D. Pietra, P. V. deSouza, J. C. Lai, and R. L. Mercer,\n“Class-based /110 -gram models of natural language,”Comput. Linguist.,\nvol. 18, no. 4, pp. 467–479, 1992.\n[32] A. Nadas, “Estimation of probabilities in the language model of the\nIBM speech recognition system,”IEEE Trans. Acoust.. Speech, Lang.\nProcess., vol. ASSP-32, no. 4, pp. 859–861, Aug. 1984.\n[33] S. Yaman, J.-T. Chien, and C.-H. Lee, “Structural Bayesian language\nmodeling and adaptation,” inProc. Interspeech’07, Antwerp, Belgium,\nAug. 2007, pp. 2365–2368.\n[34] J. Goodman, “Exponential priors for maximum entropy models,” in\nProc. HLT-NAACL, 2004, pp. 305–311.\n[35] F. Wood and Y. W. Teh, “A hierarchical nonparametric Bayesian ap-\nproach to statistical language model domain adaptation,” inProc. Int.\nConf. Artif. Intell. Statist., 2009, vol. 12, pp. 607–614.\n[36] Y. W. Teh, “A Bayesian interpretation of interpolated Kneser–Ney,”\nSchool of Computing, National Univ. of Singapore, 2006, Tech. Rep.\nTRA2/06.\n[37] S. Huang and S. Renals, “A parallel training algorithm for hierarchical\nPitman–Y or process language models,” inProc. Interspeech’09, Sep.\n2009, pp. 2695–2698.\n[38] A. Stolcke, “SRILM—An extensible language modeling toolkit,” in\nProc. ICSLP’02, Denver, CO, Sep. 2002, pp. 901–904.\n[39] V. Wan and T. Hain, “Strategies for language model web-data\ncollection,” in Proc. ICASSP’06, Toulouse, France, May 2006, pp.\n1069–1072.\n[40] J. Carletta, “Unleashing the killer corpus: Experiences in creating the\nmulti-everything AMI meeting corpus,”Lang. Res. Eval. J. , vol. 41,\nno. 2, pp. 181–190, 2007.\n[41] T. Hain, L. Burget, J. Dines, G. Garau, M. Karaﬁat, M. Lincoln, J.\nVepa, and V. Wan, “The AMI system for the transcription of speech in\nmeetings,” inProc. ICASSP’07, Apr. 2007, pp. 357–360.\n[42] D. Jurafsky and J. H. Martin, Speech and Language Processing: An\nIntroduction to Natural Language Processing, Computational Linguis-\ntics, and Speech Recognition, 2nd ed. Upper Saddle River, NJ: Pren-\ntice-Hall, 2009.\nSongfang Huang (S’08) received the B.Sc. degree\nfrom North China Electric Power University, Beijing,\nChina, in 2002, and the M.Sc. degree from Peking\nUniversity, Beijing, China, in 2005. He is currently\npursuing the Ph.D. degree at the Centre for Speech\nTechnology Research, School of Informatics, Univer-\nsity of Edinburgh, Edinburgh, U.K.\nHe was a Co-op Researcher at the IBM T. J.\nWatson Research Center, Yorktown Heights, NY ,\nfrom July to November 2008, and a Research\nAssociate for the AMIDA project at the Centre for\nSpeech Technology Research, School of Informatics, University of Edinburgh,\nfrom December 2008 to September 2009. Since November 2009, He has been a\nPostdoctoral Researcher at the IBM T. J. Watson Research Center. His research\ninterests mainly concern the application of machine learning algorithms to\nautomatic speech recognition, natural language processing, and statistical\nmachine translation.\nSteve Renals(M’91) received the B.Sc. degree from\nthe University of Shefﬁeld, Shefﬁeld, U.K., in 1986,\nand the M.Sc. and Ph.D. degrees from the University\nof Edinburgh, Edinburgh, U.K., in 1987 and 1991, re-\nspectively.\nHe is a Professor of speech technology in the\nSchool of Informatics and Director of the Centre\nfor Speech Technology Research, University of\nEdinburgh. He held postdoctoral fellowships at the\nInternational Computer Science Institute, Berkeley,\nCA, (1991–1992) and at the University of Cam-\nbridge, Cambridge, U.K. (1992–1994). He was a member of academic staff\nat the University of Shefﬁeld for nine years as a Lecturer (1994–1999), then\nReader (1999–2003). He is an Associate Editor of the ACM Transactions on\nSpeech and Language Processing. His main research areas are in spoken lan-\nguage processing and multimodal interaction, and he has over 150 publications\nin these areas."
}