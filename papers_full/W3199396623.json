{
  "title": "Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach",
  "url": "https://openalex.org/W3199396623",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3198416478",
      "name": "Koren Lazar",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A3199129694",
      "name": "Benny Saret",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A3199964025",
      "name": "Asaf Yehudai",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A2052883242",
      "name": "Wayne Horowitz",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A2118388568",
      "name": "Nathan Wasserman",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    },
    {
      "id": "https://openalex.org/A2251783221",
      "name": "Gabriel Stanovsky",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3097696621",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3097252586",
    "https://openalex.org/W2329512520",
    "https://openalex.org/W2159263831",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2970619458",
    "https://openalex.org/W3031791107",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W3082145431",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2963323103",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "We present models which complete missing text given transliterations of ancient Mesopotamian documents, originally written on cuneiform clay tablets (2500 BCE - 100 CE). Due to the tablets’ deterioration, scholars often rely on contextual cues to manually fill in missing parts in the text in a subjective and time-consuming process. We identify that this challenge can be formulated as a masked language modelling task, used mostly as a pretraining objective for contextualized language models. Following, we develop several architectures focusing on the Akkadian language, the lingua franca of the time. We find that despite data scarcity (1M tokens) we can achieve state of the art performance on missing tokens prediction (89% hit@5) using a greedy decoding scheme and pretraining on data from other languages and different time periods. Finally, we conduct human evaluations showing the applicability of our models in assisting experts to transcribe texts in extinct languages.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4682–4691\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n4682\nFilling the Gaps in Ancient Akkadian Texts:\nA Masked Language Modelling Approach\nKoren Lazar♢ Benny Saret♠ Asaf Yehudai♢\nWayne Horowitz♠ Nathan Wasserman♠ Gabriel Stanovsky♢\n♢School of Computer Science and Engineering ♠The Institute of Archaeology\nThe Hebrew University of Jerusalem, Jerusalem, Israel\n{koren.lazar, gabriel.stanovsky}@mail.huji.ac.il\nAbstract\nWe present models which complete miss-\ning text given transliterations of ancient\nMesopotamian documents, originally written\non cuneiform clay tablets (2500 BCE - 100\nCE). Due to the tablets’ deterioration, schol-\nars often rely on contextual cues to manually\nﬁll in missing parts in the text in a subjective\nand time-consuming process. We identify that\nthis challenge can be formulated as a masked\nlanguage modelling task, used mostly as a pre-\ntraining objective for contextualized language\nmodels. Following, we develop several archi-\ntectures focusing on the Akkadian language,\nthe lingua franca of the time. We ﬁnd that de-\nspite data scarcity (1M tokens) we can achieve\nstate of the art performance on missing tokens\nprediction (89% hit@5) using a greedy decod-\ning scheme and pretraining on data from other\nlanguages and different time periods. Finally,\nwe conduct human evaluations showing the ap-\nplicability of our models in assisting experts to\ntranscribe texts in extinct languages.\n1 Introduction\nThe Akkadian language was the lingua franca of\nthe Middle East and Egypt in the Late Bronze and\nEarly Iron Ages, spoken or in use from 2500 BCE\nuntil its gradual extinction around 100 CE ( Oppen-\nheim, 2013). It was written in cuneiform signs —\nwedge-shaped imprints on clay tablets, as depicted\nin Figure 1 (Walker, 1987). These tablets are the\nmain record from the Mesopotamian cultures, in-\ncluding religious texts, bureaucratic records, royal\ndecrees, and more. Therefore they are a target of\nextensive transcription and transliteration efforts.\nOne such transcription is exempliﬁed by the La-\ntinized text to the right of the tablet in Figure 1.\nThe Open Richly Annotated Cuneiform Corpus\n(Oracc)1 is one of the major Akkadian transcrip-\ntion collections, culminating in approximately\n1http://oracc.org\nFigure 1: A clay tablet from Oracc (left) with its corre-\nsponding Latin transliteration (right). Words are delim-\nited by spaces, while signs are delimited by hyphens or\ndots. A sign which is missing due to deterioration is de-\nnoted by ‘x’ and highlighted in red in the ﬁgure. We de-\nvelop models which automatically complete these miss-\ning signs based on the surrounding context.\n2.3M transcribed signs from 10K tablets. As fur-\nther evidenced in Figure 1, many of the signs in\nthe tablets were eroded over time and some parts\nwere broken or lost, forcing editors to “ﬁll in the\ngaps” where possible, based on the context of the\nsurrounding words.\nIn this paper, we identify that the task of masked\nlanguage modeling, used ubiquitously in recent\nyears for pretraining other downstream tasks ( Pe-\nters et al. , 2018; Howard and Ruder , 2018; Liu\net al. , 2019) lends itself directly to missing sign\nprediction in the transliterated texts. We exper-\niment with various adaptations of BERT-based\nmodels ( Devlin et al. , 2019) trained and tested on\nOracc, combined with a greedy decoding scheme\nto extend the prediction from single tokens to mul-\ntiple words. We speciﬁcally focus on the effect\nmultilingual pretraining has on downstream per-\nformance, which was recently shown beneﬁcial\nfor low-resource settings ( Chau et al. , 2020).\n4683\nIn an automatic evaluation, we ﬁnd that a combi-\nnation of large-scale multilingual pretraining with\nAkkadian ﬁnetuning achieves state-of-the-art re-\nsults, with a top 5 accuracy of 89.5%, vastly im-\nproving over other models and baselines. Inter-\nestingly, we ﬁnd that the multilingual pretraining\nsignal seems to be more important than the sig-\nnal of the target small-scale Akkadian data, as the\nzero-shot performance of a multilingual language\nmodel surpasses that of a monolingual Akkadian\nmodel by about 10%.\nFinally, we show the model’s potential applica-\nbility in assisting transcription by ﬁlling in miss-\ning parts. To account for the challenges in human\nassessment of an extinct language, we created a\ncontrolled setup where domain experts are asked\nto identify plausible predictions out of a combi-\nnation of model predictions, the original masked\nsequences, and noise. We ﬁnd that in a major-\nity of cases, the annotators found at least one of\nthe model’s top 3 predictions useful, while the per-\nformance degrades on longer sequences. Future\nwork can improve the model by designing more\nelaborate decoding schemes and exploring the spe-\nciﬁc effect of related languages (e.g., Arabic and\nHebrew) on downstream performance. Our code\nand trained models are made publicly available at\nwww.github.com/SLAB-NLP/Akk.\nOur main contributions are:\n• We identify that the longstanding challenge\nof ﬁlling in gaps in Akkadian texts directly\ncorresponds to advances in masked language\nmodeling.\n• We train the ﬁrst Akkadian language model,\nwhich can serve as a pretrained starting point\nfor other downstream tasks such as Akkadian\nmorphological analysis.\n• We develop state-of-the-art models for com-\npleting missing signs by combining large-\nscale multilingual pretraining with Akkadian\nlanguage ﬁnetuning.\n• We devise a controlled user study, showing\nthe potential applicability of our model in\nassisting scholars ﬁll in gaps in real-world\nAkkadian texts.\n2 Background\nIn this section, we will introduce the Akkadian lan-\nguage and the Open Richly Annotated Cuneiform\nCorpus (Oracc). While it is one of the largest\nsources of the Akkadian language, it is of or-\nders of magnitude smaller compared to resources\nfor other languages, such as English or German.\nThen, we will introduce masked language model-\ning, which will serve as the basis for our sign pre-\ndiction model.\n2.1 The Akkadian Language and the Oracc\nDataset\nAkkadian is a Semitic language, related to sev-\neral languages spoken today, such as Hebrew, Ara-\nmaic, Amharic, Maltese, and Arabic. It has been\ndocumented from the 3 rd millennium B.C.E. un-\ntil the ﬁrst century of the common era, in mod-\nern Iraq, between the Euphrates and the Tigris\nrivers, as well as in modern Syria, east Turkey,\nand the Northern Levant ( Huehnergard, 2011). In\nthis work, we will use the Open Richly Annotated\nCuneiform Corpus (Oracc), one of the largest inter-\nnational cooperative projects gathering cuneiform\ntexts from many archaeological sites.\nMost relevant to this work, Oracc contains La-\ntinized transliterations of the cuneiform texts, as\ncan be seen in Figure 1, depicting a clay tablet\nand its transliteration in Oracc. It also contains\nEnglish translations for parts of the texts. In to-\ntal, as can be seen in Table 1, Oracc consists of\nabout 10K texts (each a transliteration of a sin-\ngle tablet), containing 1M words and 2.3M signs,\nas well as 9K translated texts in English contain-\ning 1.2M English words. Importantly, the editors\ncan often visually estimate the number of missing\nsigns in a deteriorated or missing part and denote\neach with ‘x’ in the transliteration (marked in red\nin Figure 1). Therefore, in the following sections,\nwe will assume that the number of missing signs\nis given as input to our models.\n# Texts # Words # Signs\nAkkadian Train 8K 950K 1.8M\nAkkadian Test 2K 250K 500K\nEnglish Train 7K 950K –\nEnglish Test 2K 250K –\nTable 1: Number of texts, words, and signs in our\npreprocessed version of Oracc, English texts are cor-\nresponding translations of the Akkadian texts.\n4684\n2.2 Multilingual Masked Language Modeling\nIn masked language modeling (MLM), a model\nis asked to predict masked parts in a text given\ntheir surrounding context. Recent years have seen\nlarge gains for almost all NLP tasks by using\nthe token representations learned during MLM\nas a starting point for downstream applications.\nIn particular, recent work has noticed that joint\ntraining on various languages greatly helps down-\nstream applications, especially where labeled data\nis sparse ( Pires et al. , 2019; Chau et al. , 2020; Con-\nneau et al. , 2020).\nIn this work we identify that the MLM objective\ndirectly corresponds to the task of ﬁlling in gaps in\nAkkadian texts and train several MLM variants on\nit. In the following sections, we will especially\nexamine the effect of multilingual pretraining on\nour task.\n3 Task Deﬁnition\nIntuitively, our task, as demonstrated in Figure 2,\nis to predict missing tokens or signs given their\ncontext in transliterated Akkadian documents. Hu-\nman experts achieve this when compiling Oracc\nby considering not only the surrounding context\nin the tablet, but also its wider, external con-\ntext, such as its corpus, or the time and location\nwhere the text was originally written or found. In\nmany cases, researchers can estimate the number\nof missing signs even after their physical deterio-\nration, and mark them as sequences of ‘x’s. E.g.,\nnote the sequence of 2 ‘x’s marked in red in Fig-\nure 2. We will use this signal as input to our\nmodel, which speciﬁes the number of signs to be\npredicted.2\nFormally, let T = ( s1, ..., sn) ∈ Σn be a\ntransliterated Akkadian document comprised of\na concatenation of n signs, where Σ is the set\nof all Akkadian signs. Let I ⊆ [n] such that\n∀i ∈ I : si = x, where x denotes a missing\nsign. The number of missing signs is assumed to\nbe known a priori, based on the editor’s examina-\ntion of the tablets. Therefore, the model should\noutput (p1, ..., p|I|) ∈ Σ|I| predictions for the miss-\ning signs in T.\n4 Model\nIn this section, we will introduce BERT-based\nmodels aiming to solve the task of predicting miss-\n2We ﬁlter cases where the editors can not estimate the\nnumber of missing signs.\ning signs in Akkadian texts. We chose these mod-\nels since their pretraining task is also our down-\nstream task. The high-level diagram of the model\nis presented in Figure 2 and is elaborated below.\nFirst, in Section 4.1, we outline the preprocessing\nof Oracc, aiming to remove annotations that are ex-\nternal to the original text. Then in Section 4.2, we\npropose two models for predicting missing signs.\nLastly, in Section 4.3, we present an algorithm to\nextend BERT sub-word level prediction to multi-\nple signs and words. In the following two sections\nwe will test these models in both automatic and\nhuman evaluation setups.\n4.1 Preprocessing\nOracc is a collaborative effort to transliterate\nMesopotamian tablets, mainly in Akkadian. Fig-\nure 1 exempliﬁes different characteristics of the\ncorpus. We removed signs added by editors in\nthe transliteration process as they were not part of\nthe original text. For example, we removed signs\nwhich indicate how certain the editors are in their\nreading of the tablet. As an example, note that\nin Figure 2 the ﬁrst sign in the transliterated text\nis marked as uncertain with the ⌜⌝ characters be-\nfore preprocessing. In addition, we also remove\nsuperscripts and subscripts, which indicate differ-\nent readings of the Akkadian cuneiform text, e.g.,\nan ‘m’ superscript is preceding the last word in the\ntransliterated text.\nDuring training, similarly to Devlin et al.\n(2019), we train the model to predict known tokens\nby masking them at random. During inference, we\nmask each missing sign, indicated by ‘x’ in Oracc,\nFigure 2: High-level diagram of our model, producing\na sequence of signs (marked in blue) given input from\nOracc with missing signs (red ‘x’s). We experiment\nwith different language models and pretraining data.\n4685\nand iteratively predict each of the tokens compos-\ning it.\n4.2 Masked Language Models\nWe experimented with monolingual and multilin-\ngual versions of BERT.\nFirst, we pretrained from scratch a monolin-\ngual BERT model with a reduced number of pa-\nrameters ( 750K) following conclusions from Ka-\nplan et al. (2020). Second, following recent re-\nsearch suggesting that pretraining on similar lan-\nguages is beneﬁcial for many NLP tasks, includ-\ning in low-resource settings ( Pires et al. , 2019;\nWu and Dredze , 2019; Chau et al. , 2020; Con-\nneau et al. , 2020), we ﬁnetuned a pretrained mul-\ntilingual BERT (M-BERT) model ( Devlin et al. ,\n2019).3 M-BERT was trained on the 104 most\ncommon languages of Wikipedia, including He-\nbrew and Arabic - Semitic languages that are ty-\npologically similar to Akkadian.\nTo adapt M-BERT to Akkadian, we assign its\n99 available free tokens, optimizing for maximum\nlikelihood by the WordPiece tokenization algo-\nrithm ( Schuster and Nakajima , 2012; Wu et al. ,\n2016).\n4.3 Decoding: From Tokens to Signs\nWhile the MLM task is designed to predict single\ntokens, in our setting, multiple signs and words\nmay be omitted due to deterioration. To bridge this\ngap, we greedily extend the token level prediction\nby adapting the k-beams algorithm such that it out-\nputs possible predictions given an Akkadian text\nwith a sequence of missing signs. See the exam-\nple at the top of Figure 2, where the two ‘x’ signs\nin the input are predicted as a-na. To achieve this,\nwe count the number of sign delimiters (space, dot,\nhyphens) predicted at each time step, and choose\nthe best k candidates according to the following\nconditional probability:\np(X1, ..., Xn, C) =\nn∏\ni=1\np(Xi|X1, ..., Xi−1, C)\n(1)\nWhere Xi denotes the ith masked token, and C\ndenotes the observed context. For example, in Fig-\nure 2, a-na is composed of three sub-sign tokens:\n’a’, ’-’, ’na’ , while C = (‘a-bat LUGAL’, ‘a ˘s-˘sur’),\nand the sequence probability is p(na|−, a, C) ·\np(−|a, C) · p(a|C) .\n3https://huggingface.co/bert-base-multilingual-cased\n5 Automatic Evaluation\nWe present an automatic evaluation of our mod-\nels’ predictions for missing signs in ancient Akka-\ndian texts, testing several masked language mod-\neling variants for single token prediction, as well\nas our greedy extension to multiple tokens and\nsigns. In all evaluations, we mask known tokens\nand evaluate the model’s ability to predict the orig-\ninal masked tokens. This setup allows us to test\nagainst large amounts of texts in Oracc from dif-\nferent periods of time, locations or genres.\n5.1 Models and Datasets\nWe use two strong baselines: (1) the LSTM model\nthat was proposed by Fetaya et al. (2020), and was\nretrained on our dataset using their default conﬁg-\nuration;4,5 and (2) the cased BERT-base multilin-\ngual model, without ﬁnetuning over Oracc. 6\nWe compare these two baselines against our\nmodels, as presented in 4.2, trained in three con-\nﬁgurations: (1) BERT+AKK(mono) refers to the\nreduced size BERT model, trained from scratch on\nthe Akkadian texts from Oracc; (2) MBERT+Akk\nis a ﬁnetuned version of M-BERT on the Akka-\ndian texts, using the model’s additional free to-\nkens to encode sub-word tokens from Oracc; and\n(3) MBERT+Akk+Eng further ﬁnetunes on the En-\nglish translations available in Oracc to introduce\nadditional domain-speciﬁc signal. We test all mod-\nels against 5 different genres of Akkadian texts\ntagged in Oracc, masking 15% of the tokens. The\ngenres can be largely divided into two groups.\nFirst, the Royal Inscription, Monumental, and As-\ntrological Reports are the most common genres in\nthe dataset and consist of longer coherent texts,\nmostly of essays and correspondence. Second, we\ntest on two other genres: Lexical which consists\nmostly of tabular information (lists of synonyms\nand translations), and Decree that contains con-\ncatenated non-contextualized short sentences.\n5.2 Experimental Setup\nFor all our experiments, we used a random 80%\n- 20% split for train and test (see Table 1). For\nthe monolingual model, we trained our reduced-\nparameters BERT model from scratch for 300\nepochs with 4 NVIDIA Tesla M60 GPUs for 2\nhours. For the multilingual experiments, we ﬁne-\n4https://github.com/DigitalPasts/Atrahasis\n5https://github.com/DigitalPasts/Akkademia\n6https://huggingface.co/bert-base-multilingual-cased\n4686\nGenre Metric LSTM MBERT-base BERT+AKK(mono) MBERT+Akk MBERT+Akk+Eng\nRoyal\nInscription\nMRR .52 .57 .57 .83 .83\nHit@5 .60 .65 .56 .90 .90\nRoyal or\nMonumuental\nMRR .51 .61 .61 .84 .83\nHit@5 .61 .69 .69 .90 .90\nAstrological\nReport\nMRR .53 .55 .55 .81 .80\nHit@5 .60 .64 .64 .88 .88\nLexical MRR .10 .61 .69 .69 .66\nHit@5 .10 .76 .76 .85 .85\nDecree MRR .49 .67 .39 .71 .74\nHit@5 .60 .73 .51 .76 .76\nOverall MRR .52 .60 .50 .83 .83\nHit@5 .59 .67 .60 .89 .89\nTable 2: MRR and Hit@5 precision by genre. The ﬁrst two models from the left are our baselines: LSTM refers\nto the model from ( Fetaya et al. , 2020) retrained on our data, MBERT-base refers to the zero-shot M-BERT model\nwithout training on Oracc. The following three models are introduced in Section 4.2: BERT+AKK(mono) is\ntrained mono-lingually from scratch on Oracc Akkadian texts; MBERT+Akk ﬁnetunes on Oracc Akkadian texts;\nand MBERT+Akk+Eng is also ﬁnetuned on their English translations. The three genres at the top of the Table\n(Royal Inscription, Monumental, Astrological) are the most common in our test dataset and contain longer, more\ncoherent texts. The two genres at the bottom (Lexical and Decree) contain tabular texts and non-contextualized,\nshort sentences.\ntuned M-BERT for 20 epochs similarly to ( Chau\net al. , 2020), with 8 NVIDIA Tesla M60 GPUs\nfor 2-3 hours. We used the original architecture\nof M-BERT, adding a masked language modeling\nhead for prediction. For the LSTM model of Fe-\ntaya et al. (2020), we train for 200 epochs, with 1\nNVIDIA Tesla M60 GPU for 68 hours.\n5.3 Metrics\nWe report performance according to the Hit@k\nand mean reciprocal rank (MRR) metrics, as de-\nﬁned below:\nMRR = 1\nN\nN∑\ni=1\n1\nranki\n(2)\nHit@k = 1\nN\nN∑\ni=1\n/x31 [ranki≤k] (3)\nWhere N is the number of masked instances,\nranki is the rank of the original masked token\nin the model’s predictions, and /x31 is the indicator\nfunction.\nThe Hit@k metric directly measures applicabil-\nity in our target application, i.e., how likely is the\ncorrect prediction to appear if we present the user\nwith our model’s top k predictions. MRR comple-\nments Hit@k by providing a ﬁner-grained evalua-\ntion, as the model receives partial credit in correla-\ntion with every ranking.\n5.4 Results\nTable 2 compares token level evaluation across\nour different models and genres, while Figure 3\npresents an evaluation of the prediction of multi-\nple signs and words. We note several interesting\nobservations based on these results.\nMultilingual pretraining + Akkadian ﬁnetun-\ning achieves state-of-the-art performance. On\naverage, the two M-BERT models, which were\nﬁnetuned over Oracc texts, outperform all other\nmodels by at least 20% on both metrics. This is\nparticularly pronounced in the more natural ﬁrst\nset of genres, where the multilingual models often\nsurpass 85% in both MRR and Hit@5.\nZero-shot multilingual pretraining outper-\nforms monolingual training. Surprisingly, in\nmost tested settings, the zero-shot version of\nM-BERT outperforms both BERT+AKK(mono)\nand the LSTM models, despite never training\non Akkadian. This suggests that the signal from\npretraining is stronger than that of the Akkadian\ntexts, likely due to the relatively small amounts\nof data. Moreover, as M-BERT was trained\nover the MLM task in other languages during\nits pretraining, this evaluation can be seen as a\n4687\nFigure 3: Hit@k precision for sequences of varying lengths in Akkadian (A) and English (B). We ﬁnd that both\nlanguages do well on 1 token and 1 sign, where the correct answer is expected to be in the models’ top 5 predictions\nfor half of the instances. Performance drops sharply for longer sequences, possibly due to the large search space.\nWe directly measure the model’s applicability in user studies in Section 6.\nzero-shot cross-lingual transfer learning, on which\nM-BERT was found to be competitive in many\nNLP tasks ( Pires et al. , 2019; Wu and Dredze ,\n2019; Conneau et al. , 2020).\nPerformance degrades on the Lexical genre.\nThe gains of the multilingual models are reduced\nin the Lexical genre. Speciﬁcally, they are on par\nwith BERT+AKK(mono) in this genre. This may\nindicate that this genre’s idiosyncratic syntax does\nnot beneﬁt much from multilingual pretraining.\nContext matters after ﬁnetuning M-BERT.\nThe performance of the ﬁnetuned M-BERT is the\nlowest in the Decree genre and is very close to that\nof the MBERT-base. This is perhaps not surprising\nas the Decree texts are concatenations of unrelated\nshort sentences, while one of BERT’s main advan-\ntages is its learned contextualized representations\nof different domains.\nFinetuning on English Oracc translations\ndoes not improve performance. Finetuning M-\nBERT only on Akkadian (MBERT+Akk) leads\nto results on par with additional ﬁnetuning on\nEnglish (MBERT+Akk+Eng), possibly indicating\nthat the amount of Akkadian texts and English\ntranslations is not enough to make M-BERT align\nbetween the two languages in Oracc’s unique do-\nmains.\nPerformance degrades on longer masked se-\nquences for both English and Akkadian. Fig-\nure 3 compares our best-performing model in\npredicting a varying number of signs against M-\nBERT on English texts, where both use our greedy\ndecoding strategy to extend their predictions to\nmultiple signs and words. We note similar patterns\nfor both languages. The performance for a single\nsign and word is high, and it deteriorates when\nmore elements are predicted. In the following sec-\ntion, we extend this evaluation by conducting a hu-\nman evaluation that aims to test the model’s appli-\ncability in a real-world setting.\n6 Human Evaluation and User Studies\nWe note that the automatic evaluation presented in\nthe previous section offers only an upper bound\nof the model’s ability to suggest reasonable com-\npletions, since the original text is often only one\nout of many other equiprobable completions of the\nmasked text. Consider, for example, the masked\nEnglish text at the top of Figure 4. While the\noriginal text was “of the former” , the model’s top\npredictions (“of the previous” , “of the ﬁrst” ) may\nalso be acceptable to scholars. This may also ex-\nplain the degradation in performance in Figure 3,\nas the number of plausible completions rises in cor-\nrelation with the length of the predicted span.\nTo address this, we conduct a direct manual\nevaluation of the top performing model’s predic-\ntions (M-BERT ﬁnetuned over Oracc) in a con-\ntrolled environment, on both the original Akka-\ndian, as well as its corresponding English trans-\nlation. We begin by describing the experiment\nsetup, which aims to cope with the inherent noise\nof human analysis in the MLM task, especially in\nan extinct language. Then, we discuss our ﬁnd-\nings, which show that the model provides sensible\n4688\nFigure 4: Human evaluation interface for English (top)\nand transliterated Akkadian (bottom). Given the tex-\ntual context from the tablet and a missing span of text\n(marked by red X’s), the annotator decides whether\neach presented option is plausible. The options consist\nof the top three model predictions (marked in blue) and\ntwo controls: the original masked span (marked in yel-\nlow) and a randomly sampled span of text functioning\nas a distractor (marked in red).\nFigure 5: Human evaluation results. The X-axis repre-\nsents the number of signs (in Akkadian) or words (in\nEnglish) in a predicted sequence, and the Y-axis rep-\nresents the average number of model predictions that\nour human experts approved for the given predicted se-\nquence. The upper error bars represent false negatives,\nwhere the gold sequence was labeled not plausible. The\nlower error bars represent false positives, where the dis-\ntractor was labeled as plausible. We ﬁnd that annotators\ntend to introduce false negatives, while they are less\nprone to falsely label distractors as plausible.\nsuggestions in most instances, while the compari-\nson with English reveals that there is room for im-\nprovement, especially on longer sequences.\n6.1 Experiment Setup: Coping with Noisy\nHuman Evaluation\nOur human evaluation of missing sign prediction\nin Akkadian was done by two of the authors, who\nare professional Assyriologists. They can read\nAkkadian at an academic level, and represent the\nusers who work on cuneiform transliteration and\nmay beneﬁt from our model’s predictions. Despite\ntheir unique expertise, they do not speak the lan-\nguage ﬂuently like native speakers did, and the lan-\nguage’s natural variations over thousands of years\nmakes the reading even more difﬁcult.\nTo address this, we created an annotation\nscheme7 which evaluates the model’s predictions\nand estimates the noise introduced in the annota-\ntion process. As exempliﬁed in Figure 4, for each\nannotation instance, we generated 5 suggestions: 3\nmodel predictions, the original masked term, and\na distractor sequence that was randomly sampled\nfrom the Akkadian texts. 8 The annotators observe\nthe 5 suggestions in a randomized order, oblivi-\nous to which ones are model predictions. They\nare then required to mark each suggestion as either\nplausible or implausible, given the document’s sur-\nrounding context.\nInserting the original masked sequence and the\ndistractor enabled us to quantitatively estimate two\nsources of noise. First, the percentage of gold\nsamples which were marked as incorrect reﬂects\nan underestimation of the model’s ability as these\nare samples which in fact occurred in the original\nancient texts, yet were ruled out by our experts.\nSimilarly, the percentage of distractors marked as\nplausible reﬂects an overestimation of the model’s\nperformance.\nBy combining the estimated model accu-\nracy (the percentage of the predictions marked as\nplausible) with both sources of noise, we can esti-\nmate a range in which the actual performance of\nthe model may lie. Finally, for comparison with\na high-resource language, we asked two ﬂuent En-\nglish speakers to annotate instances from the En-\nglish translations of Oracc when predictions were\ngenerated by English BERT-base uncased model\nin the same experimental setup, as demonstrated\nat the top of Figure 4.\nWe conclude this part with an example human\nannotation and its corresponding analysis.\nAnnotation example. Consider the English an-\nnotation instance presented in Figure 4, and as-\nsume the annotator marked as plausible the fol-\nlowing four items: the artiﬁcially introduced noise\n(“of Enlil’s” ); two of the model predictions: “of\nthe ﬁrst”, “of the previous” ; and the gold instance\n(“of the former” ), while the remaining model pre-\ndiction ( “, your father” ) is considered wrong by\nthe human annotator. In which case, we compute\n7Created with docanno ( Nakayama et al. , 2018).\n8In case the model predicted the gold sequence, we added\nan additional model prediction, to ensure we always present\n5 options.\n4689\nthe annotator’s quality assessment for this instance\nas 2\n3 , while we record that they tend to overesti-\nmate the model performance, as they marked the\nartiﬁcial noise as plausible. Both of these met-\nrics (accuracy and error estimation) are aggregated\nand averaged over the entire annotation.\n6.2 Results\nEach of our two annotators marked the top 5\nmodel predictions for 70 different missing se-\nquences, resulting in 700 binary annotations over-\nall. 150 of these annotations were doubly anno-\ntated to compute agreement, overall ﬁnding good\nlevels of agreement (.81 κ for English and .79 κ\nfor Akkadian). These were drawn from royal in-\nscriptions, as tagged in Oracc. This genre con-\ntains straight-forward, yet elaborate syntax and\nis well known by our annotators. We can make\nseveral observations based on Figure 5 which de-\npicts the results of the human evaluation, based\non the number of missing signs and the tested lan-\nguage (Akkadian versus English).\nOur model’s Akkadian predictions are applica-\nbly useful... Per sequence of one or two signs,\nthe annotators tended to accept on average at least\none suggestion as plausible, while for three signs,\nthey accepted on average about one suggestion\nper two sequences. From an applicative point of\nview, this functionality readily lends itself to aid\ntransliteration of missing signs for sequences of\nsuch lengths, which constitute the majority (57%)\nof missing spans in Oracc. 9\n... yet performance degrades with the number\nof missing tokens. In Figure 5, we observe that\nthe performance of the Akkadian model (in or-\nange) degrades faster than the English model (in\nblue) the longer the predicted sequence gets. This\nindicates that the greedy decoding from a single\nspan to multiple spans works better for English\nthan for Akkadian. Designing a better decoding\nscheme is left as an interesting avenue for future\nwork.\nHumans tend to underestimate the model per-\nformance. By examining the assessments for\nthe artiﬁcially introduced gold and distractor se-\nquences we can estimate that the actual model\nperformance may be higher than our experts es-\ntimated. We see that for both languages and in\n9E.g., imagine a virtual keyboard auto-complete feature\nthat suggests plausible completions in half of the cases.\nmost tested scenarios, our annotators were able to\nrule out the distractor, while they tended to also\nwrongly discarded the gold sequence (shown by\nthe upper error bar), indicating that they may have\nalso ruled out other plausible predictions made by\nthe model.\n7 Related Work\nMost related to our work, Fetaya et al. (2020)\ndesigned an LSTM model which similarly aims\nto complete fragmentary sequences in Babylonian\ntexts. They differ from us in two major aspects.\nFirst, they focus on small-scale highly-structured\ntexts, for example, lists (parataxis), such as re-\nceipts or census documents ( Jursa, 2004). Sec-\nond, their LSTM model does not use multilingual\npretraining, instead, it is trained on monolingual\nAkkadian data and its parameters are randomly ini-\ntialized. In Section 5, we retrain their model on\nour data, showing that it underperforms on all gen-\nres compared to models which were pretrained us-\ning multilingual data, even in a zero-shot setting,\nfurther attesting to the valuable signal of multilin-\ngual pretraining in low-resource settings.\nOther works have used Oracc and other Akka-\ndian resources and may beneﬁt from our language\nmodel for Akkadian. Jauhiainen et al. (2019) used\nOracc for a shared task around language and di-\nalect identiﬁcation. Luukko et al. (2020) recently\nintroduced a syntactic treebank for Akkadian over\ntexts from Oracc, while Sahala et al. (2020) built\na morphological analyzer using annotations from\nOracc. Finally, Gordin et al. (2020) automatically\ntransliterated Unicode cuneiform glyphs into the\nLatinized transliterated form.\nSeveral recent works also noticed the cross-\nlingual transfer capabilities of M-BERT. Wu and\nDredze (2019) and Conneau et al. (2020) found\nthat M-BERT can successfully learn various NLP\ntasks in a zero-shot setting using cross-lingual\ntransfer, pointing at the shared parameters across\nlanguages as the most important factor. Pires\net al. (2019) showed that M-BERT is capable\nof zero-shot transfer learning even between lan-\nguages with different writing systems.\n8 Conclusions and Future Work\nWe presented a state-of-the-art model for missing\nsign completion in Akkadian texts, using multilin-\ngual pretraining and ﬁnetuning on Akkadian texts.\nInterestingly, we discovered that in such a low-\n4690\nresource setting, the signal from pretraining may\nbe more important than the ﬁnetuning objective.\nEvidently, a zero-shot model outperforms mono-\nlingual Akkadian models. Finally, we conducted a\ncontrolled user study showing the model’s poten-\ntial applicability in aiding human editors.\nOur work sets the ground for various avenues\nof future work. First, A more elaborate decoding\nscheme can be designed to mitigate the degrada-\ntion of performance for longer masked sequences,\nfor example by employing SpanBERT ( Joshi et al. ,\n2020) to represent the missing sequences during\ntraining and inference. Second, our ﬁndings sug-\ngest that an exploration of the speciﬁc utility of\nsimilar languages, e.g., Arabic or Hebrew, may\nyield improvements in missing sign prediction.\nAcknowledgements\nWe thank Ethan Fetaya and Shai Gordin for\ninsightful discussions and suggestions and the\nanonymous reviewers for their helpful comments\nand feedback. This work was supported in part by\na research gift from the Allen Institute for AI.\nReferences\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith. 2020.\nParsing with multilingual BERT, a small corpus, and\na small treebank . In Findings of the Association\nfor Computational Linguistics: EMNLP 2020 , pages\n1324–1334, Online. Association for Computational\nLinguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models . In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6022–\n6034, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nEthan Fetaya, Yonatan Lifshitz, Elad Aaron, and Shai\nGordin. 2020. Restoration of fragmentary babylo-\nnian texts using recurrent neural networks . Pro-\nceedings of the National Academy of Sciences ,\n117(37):22743–22751.\nShai Gordin, Gai Gutherz, Ariel Elazary, Avital\nRomach, Enrique Jiménez, Jonathan Berant, and\nYoram Cohen. 2020. Reading akkadian cuneiform\nusing natural language processing. PloS one ,\n15(10):e0240511.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation . In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJohn Huehnergard. 2011. Introduction. A Grammar of\nAkkadian, pages xxiii–xlii.\nTommi Jauhiainen, Heidi Jauhiainen, Tero Alstola, and\nKrister Lindén. 2019. Language and dialect iden-\ntiﬁcation of cuneiform texts . In Proceedings of the\nSixth Workshop on NLP for Similar Languages, Vari-\neties and Dialects , pages 89–98, Ann Arbor, Michi-\ngan. Association for Computational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S.\nWeld, Luke Zettlemoyer, and Omer Levy. 2020.\nSpanBERT: Improving pre-training by representing\nand predicting spans . Transactions of the Associa-\ntion for Computational Linguistics , 8:64–77.\nMichael Jursa. 2004. Accounting in neo-babylonian\ninstitutional archives: structure, usage, implications.\nCreating Economic Order: Record-keeping, Stan-\ndardization, and the Development of Accounting in\nthe Ancient Near East, Bethesda , pages 145–198.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling laws for neural language\nmodels.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692 .\nMikko Luukko, Aleksi Sahala, Sam Hardwick, and\nKrister Lindén. 2020. Akkadian treebank for early\nneo-assyrian royal inscriptions . In Proceedings\nof the 19th International Workshop on Treebanks\nand Linguistic Theories , pages 124–134, Düsseldorf,\nGermany. Association for Computational Linguis-\ntics.\nHiroki Nakayama, Takahiro Kubo, Junya Kamura, Ya-\nsufumi Taniguchi, and Xu Liang. 2018. doccano:\nText annotation tool for human . Software available\nfrom https://github.com/doccano/doccano.\nA Leo Oppenheim. 2013. Ancient Mesopotamia: por-\ntrait of a dead civilization . University of Chicago\nPress.\n4691\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers) , pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\nAleksi Sahala, Miikka Silfverberg, Antti Arppe, and\nKrister Lindén. 2020. BabyFST - towards a ﬁnite-\nstate based computational model of ancient baby-\nlonian. In Proceedings of the 12th Language Re-\nsources and Evaluation Conference , pages 3886–\n3894, Marseille, France. European Language Re-\nsources Association.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search . In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152.\nChristopher Bromhead Fleming Walker. 1987.\nCuneiform, volume 3. Univ of California Press.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP) , pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, ukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation . CoRR, abs/1609.08144.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7678240537643433
    },
    {
      "name": "Akkadian",
      "score": 0.6395804286003113
    },
    {
      "name": "Natural language processing",
      "score": 0.6274862885475159
    },
    {
      "name": "Language model",
      "score": 0.6235514879226685
    },
    {
      "name": "Task (project management)",
      "score": 0.5884466767311096
    },
    {
      "name": "Artificial intelligence",
      "score": 0.505896270275116
    },
    {
      "name": "Process (computing)",
      "score": 0.49592360854148865
    },
    {
      "name": "Decoding methods",
      "score": 0.4775746166706085
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.41625523567199707
    },
    {
      "name": "Linguistics",
      "score": 0.41379475593566895
    },
    {
      "name": "Programming language",
      "score": 0.11920753121376038
    },
    {
      "name": "Engineering",
      "score": 0.09050890803337097
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    }
  ]
}