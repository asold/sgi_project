{
    "title": "Mitigating Societal Harms in Large Language Models",
    "url": "https://openalex.org/W4389520048",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2181361850",
            "name": "Sachin Kumar",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2914372190",
            "name": "Vidhisha Balachandran",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2806826429",
            "name": "Lucille Njoo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2400815267",
            "name": "Antonios Anastasopoulos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2234266251",
            "name": "Yulia Tsvetkov",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2952328691",
        "https://openalex.org/W3093233911",
        "https://openalex.org/W3032502206",
        "https://openalex.org/W3165654682",
        "https://openalex.org/W3092288641",
        "https://openalex.org/W3033129824",
        "https://openalex.org/W4386576626",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3037546592",
        "https://openalex.org/W4385573912",
        "https://openalex.org/W3105388824",
        "https://openalex.org/W3102825016",
        "https://openalex.org/W3169017236",
        "https://openalex.org/W4221141417",
        "https://openalex.org/W4299567010",
        "https://openalex.org/W3093000437",
        "https://openalex.org/W4287028455",
        "https://openalex.org/W2595653137",
        "https://openalex.org/W3130178918",
        "https://openalex.org/W3102867977",
        "https://openalex.org/W2950888501",
        "https://openalex.org/W2951211142",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W3177242492",
        "https://openalex.org/W3034656957",
        "https://openalex.org/W4385572722",
        "https://openalex.org/W3204561666",
        "https://openalex.org/W3172205429",
        "https://openalex.org/W3106234277",
        "https://openalex.org/W3101286153",
        "https://openalex.org/W3154654049",
        "https://openalex.org/W2962990575",
        "https://openalex.org/W3173168772",
        "https://openalex.org/W2768957049",
        "https://openalex.org/W3127062506",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W3104260136",
        "https://openalex.org/W4307802703",
        "https://openalex.org/W3102645206",
        "https://openalex.org/W3105882417",
        "https://openalex.org/W3155742828",
        "https://openalex.org/W4385574044",
        "https://openalex.org/W4287028715",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3174685870",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3199926081",
        "https://openalex.org/W2969958763",
        "https://openalex.org/W3034383590",
        "https://openalex.org/W2893425640",
        "https://openalex.org/W4283170666",
        "https://openalex.org/W3101551503",
        "https://openalex.org/W3103639864",
        "https://openalex.org/W4385468994",
        "https://openalex.org/W3101767999",
        "https://openalex.org/W3100355250"
    ],
    "abstract": "Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios Anastasopoulos, Yulia Tsvetkov. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts. 2023.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, pages 26–33\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMitigating Societal Harms in Large Language Models\nSachin Kumar∗,♣ Vidhisha Balachandran∗,♣ Lucille Njoo♡\nAntonios Anastasopoulos♢ Yulia Tsvetkov♡\n♣Language Technologies Institute, Carnegie Mellon University\n♢Department of Computer Science, George Mason University\n♡Paul G. Allen School of Computer Science & Engineering, University of Washington\n{sachink,vbalacha}@cs.cmu.edu, lnjoo@cs.washington.edu\nantonis@gmu.edu, yuliats@cs.washington.edu\nAbstract\nNumerous recent studies have highlighted so-\ncietal harms that can be caused by language\ntechnologies deployed in the wild. While sev-\neral surveys, tutorials, and workshops have dis-\ncussed the risks of harms in specific contexts—\ne.g., detecting and mitigating gender bias in\nNLP models—no prior work has developed a\nunified typology of technical approaches for\nmitigating harms of language generation mod-\nels. Our tutorial is based on a survey we re-\ncently wrote that proposes such a typology. We\nwill provide an overview of potential social is-\nsues in language generation, including toxicity,\nsocial biases, misinformation, factual incon-\nsistency, and privacy violations. Our primary\nfocus will be on how to systematically identify\nrisks, and how eliminate them at various stages\nof model development, from data collection, to\nmodel development, to inference/language gen-\neration. Through this tutorial, we aim to equip\nNLP researchers and engineers with a suite of\npractical tools for mitigating safety risks from\npretrained language generation models.\n1 Motivation\nWith the widespread success and increasing adop-\ntion on natural language processing (NLP) tech-\nnologies in user-facing products including machine\ntranslation (Vaswani et al., 2017; Lewis et al.,\n2020), dialogue systems (Andreas et al., 2020; Gan-\ngadharaiah and Narayanaswamy, 2020) and recom-\nmendation systems (Jannach et al., 2020) the NLP\ncommunity is becoming increasingly aware that\nwe have a responsibility to evaluate the effects of\nour research and mitigate harmful outcomes (Ben-\nder et al., 2021). Indeed, models have been shown\nto introduce vulnerabilities and threats, both inad-\nvertent and malicious, to individual users, social\ngroups, and content integrity. Without social con-\ntext and content control, deployed language gen-\nerators have quickly derailed to racist, homopho-\nbic, hateful comments (Hunt, 2016; Jang, 2021;\nWolf et al., 2017; Vincent, 2022), compromised\nuser privacy (Carlini et al., 2021), spread disinfor-\nmation (Shao et al., 2018), and even encouraged\nsuicide (Daws, 2020). Prior works have outlined\nthese risks (Maynez et al., 2020; Sheng et al., 2021;\nWeidinger et al., 2021), proposed taxonomies (Wei-\ndinger et al., 2022), discussed their points of origin,\nand advocated for research on ethical development\nof LMs (Bender et al., 2021; Solaiman et al., 2019).\nHowever, there is little work that summarizes\nactionable approaches and technical solutions\nto preventing or mitigating these harms. This is\nthe purpose of our tutorial, which is based on a\nsurvey we have recently conducted (Kumar et al.,\n2022). In this tutorial, we aim to provide acompre-\nhensive, unified taxonomy of relevant mitigation\nstrategies proposed in prior literature, specifically\nfocusing on language generation models.\n2 Tutorial Content and Relevance\nWhat are language models? A brief back-\nground: To build a common ground for dis-\ncussing the risk mitigation strategies, this tutorial\nwill begin with a brief overview of recent trends in\nlanguage modeling and pretraining. We will cover\nboth causal (Radford et al., 2019; Brown et al.,\n2020) and non-causal language models (Devlin\net al., 2019) highlighting their differences and their\nimpact on NLP research. We will briefly discuss\nhow pretrained models can be adapted to different\ntasks covering model finetuning (both complete\nand adapter based) as well as prompt-based formu-\nlation to solve NLP tasks. We will also focus on\ntheir scale both in terms of model parameters as\nwell as training data size.\nHow can language models cause societal harm?\nAfter presenting the background on language mod-\nels, we will then give a formal definition of harms\nbased on taxonomy defined in prior work (Barocas\net al., 2017) and focus on representational harms\n26\nFigure 1: Overview of Intervention Strategies. Our\nsurvey presents a taxonomy of intervention strategies\norganized around the different phases where they can\nbe applied.\nin this tutorial. Highlighting the impact of heed-\nlessly using web data which is usually population-\nimbalanced (Bender et al., 2021) and contains bi-\nased language against towards specific populations,\nwe will discuss how language models tend to re-\ninforce and amplify bias against sub-populations\nbased on different personal and social attributes\nsuch as gender (Stanovsky et al., 2019; de Vassi-\nmon Manela et al., 2021), race (Liang et al., 2021;\nField et al., 2021), region (Huang et al., 2020), de-\nmographics (Huang et al., 2020), age (Nangia et al.,\n2020) among others. We will also discuss, that\nby not being grounded in real world knowledge,\nthey pickup on spurious statistical correlations in\ndata and generate (in other words, hallucinate) fac-\ntually incorrect content which can potentially be\nused to spread misinformation (Zellers et al., 2020;\nKryscinski et al., 2020). Major content of this\nsection is borrowed from the course on Ethics in\nNLP developed at Carnegie Mellon University and\nthe University the Washington by organizer Yulia\nTsvetkov.\nCan we reduce or mitigate such harms? Fi-\nnally, in this part, we will focus on work on mitigat-\ning harmful effects of language generation systems.\nWhile still a nascent field of research, several so-\nlutions in this space have been proposed which\nwe categorize into four categories, visualized in\nFig. 1. We organize and discuss in detail interven-\ntion strategies based on where they fit in different\nstages of LM development: in data collection,\nmodeling, post-factum decoding, and applica-\ntion. Within each of these categories, our taxon-\nomy brings together prior works that have been\ntreated as disjoint areas targeting different types of\nharms (toxic/biased language and misinformation).\nSince LMs learn and amplify biases present in\nthe training data, we will first discuss data level\ninterventions which focus on either (1) filtering\nthe pretraining corpora to create more balanced\ndatasets (Jia et al., 2020), or (2) finetuning trained\nLMs on sanitized data (Gehman et al., 2020a).\nSecond, we will review model level interventions\nwhere we consider approaches which modify either\nthe architecture or training objectives to induce or\nremove desired biases (Nan et al., 2021; Cao and\nWang, 2021). Third, we will present methods to\nmodify model outputs post generation using de-\ncoding and editing methods to demote or remove\nharmful content (Yang and Klein, 2021; Kumar\net al., 2021; Cao et al., 2020). These techniques are\nespecially useful for cases where it is impossible\nto modify data or models or even decoding strate-\ngies such as in case of GPT3 (Brown et al., 2020)\nwhich are only available through an API. Finally,\nwe will end with application level interventions\nwhere we show how methods to flag and redact\nharmful content allow applications to shield such\ncontent from reaching users (Vaidya et al., 2020;\nSun et al., 2019).\nThroughout the tutorial, we will highlight both\ndetection and mitigation approaches, as well as\ntheir specific limitations and shortcomings. By\nthe end of the tutorial, participants will be better\ninformed where to focus future research efforts.\nDue to the vast range of societal harms and their\nmitigation strategies, we do not plan an exhaustive\ntreatment of this material. One central goal is to\nraise awareness for participants of the relevant is-\nsues, so that when they return to their research they\nwill be more able to notice ways in which their\nresearch based on large language models might im-\npact different variety of users. To achieve this goal,\nwe will aim for a “T-shape” in terms of breadth and\ndepth: to briefly mention a number of core ques-\ntions and then to drill down into a few particular\ncase studies to see how these issues play out in real\nresearch settings.\n27\n3 Tutorial Structure\nWe propose a cutting-edge tutorial on an emerg-\ning area that has not been previously covered in\nACL/EMNLP/NAACL/COLING tutorials. This\nwould be a discussion-style tutorial where the or-\nganizers will present material with structured time\nthroughout for questions, and discussion amongst\nattendees. The duration of the tutorial will be 3\nhours with 5 min breaks at the end of each hour.\nThe following would be the outline of the talk:\n1. Brief Introduction to Language models (10 mins)\n- We will provide a quick background on current\nstate of NLP research with introduction to language\nmodels and their capabilities.\n2. Possible Harms of Language Technologies (15\nmins) - We will briefly cover examples of ethical\nconcerns, societal harms and biases present in cur-\nrent NLP tools.\n• Fairness/Bias - Research on human-like biases\nin NLP (Field et al., 2021; Caliskan et al.,\n2017; Field and Tsvetkov, 2020)\n• Toxicity - Research on toxic text generated\nby NLP models (Gehman et al., 2020a) and\nbiases propagated in efforts to correct them\n(Davidson et al., 2017).\n• Misinformation, Factual Inconsistencies - fac-\ntual errors in generated text (Cao et al., 2018;\nBuchanan et al., 2021; Zellers et al., 2020)\n• Privacy - Models generating sensitive, iden-\ntifying information like addresses, SSN, etc.\n(Carlini et al., 2020; Inan et al., 2021)\n3. Application Level Interventions (30 mins) -\nTechniques to filter harmful content before present-\ning model outputs to users.\n• Harm Detection - Research on Toxic text de-\ntection (Vaidya et al., 2020; Han and Tsvetkov,\n2020), fact-checking (Zhou et al., 2021), hal-\nlucination detection (Kryscinski et al., 2020;\nGoyal and Durrett, 2020), bias-detection (Sun\net al., 2019; Park et al., 2018).\n• Redacting or Flagging Harmful Text - Re-\nsearch on application level warnings or redac-\ntion for harmful or inappropriate generated\ntext (Xu et al., 2020).\n4. Output Level Interventions (30 mins) - Tech-\nniques to modify outputs to remove harmful con-\ntent.\n• Decoding Techniques - Research on search\nand sampling algorithms for controllable gen-\neration by promoting or demoting specific\nproperties in output text (Zhang et al., 2022;\nKrishna et al., 2022; King et al., 2022).\n• Post-Factum Editing - Research to edit or re-\nvise generated text to remove harmful content\n(Pryzant et al., 2020; He et al., 2021; Bal-\nachandran et al., 2022).\n5. Model Level Interventions (30 mins) - Tech-\nniques to modify or optimize model parameters to\nprevent risky generations.\n• Architecture and Training - Research on objec-\ntives and model architectures to enforce safe\nand reliable text generation (Yu et al., 2022;\nNan et al., 2021; Falke et al., 2019).\n• Finetuning and Model Editing - Research\non editing or finetuning model parameters\nto incorporate safety constraints, through\nwith new objectives (Gururangan et al., 2020;\nChan et al., 2021; Gehman et al., 2020b;\nChronopoulou et al., 2020).\n6. Data Level Interventions (30 mins) - Techniques\nto curate clean training data to prevent models from\nusing harmful text.\n• Data Filtration - Research on filter-\ning/removing training data instances\ncontaining toxic or harmful content (Ngo\net al., 2021; Brown et al., 2020).\n• Data Augmentation - Research on adding\nsafer examples to datasets to offset the effect\nof problematic data (Mathew et al., 2018; Di-\nnan et al., 2020; Stafanoviˇcs et al., 2020).\n7. Open Problems and Future Research (20 mins)\nThe tutorial will be a series of presentations with\na set of references to related research papers and\nexternal demos. The presentation will cover a wide\narray of research on the topics from across the\nfield. We will share the slides with the participants\nin advance. We will additionally share an online\nrepository of relevant research material and online\nlinks to available code and demos to help partici-\npants navigate and use relevant research for their\nwork. No copyright issues are expected as we will\nuse open-source material.\n4 General Information\n4.1 Organizers\nSachin Kumar is a sixth year PhD candidate at the\nLanguage Technologies Institute, School of Com-\nputer Science at CMU. Sachin’s research tackles\ncritical technical problems in core language gener-\nation with deep learning, such as open-vocabulary\ngeneration, detection and demotion of spurious con-\nfounders, and controllable generation.\n28\nVidhisha Balachandran (she/her) is a fourth-year\nPh.D. student at the Language Technologies Insti-\ntute, School of Computer Science at CMU. Her\ncurrent research focuses on building interpretable\nand reliable NLP models with a focus on summa-\nrization, factuality, and KB-based reasoning.\nLucille Njoo (she/her) is a second-year PhD stu-\ndent at the Paul G. Allen School of Computer Sci-\nence and Engineering at the University of Washing-\nton. She works in the intersection of NLP, ethics,\nand computational social science, working on iden-\ntifying societal harms in NLP models.\nAntonios Anastasopoulos (he/him) is an Assistant\nProfessor at the Department of Computer Science\nat George Mason University, USA. His research fo-\ncuses on NLP for local and low-resource languages\nand varieties, cross-lingual learning and multilin-\nguality, and cross-lingual fairness.\nYulia Tsvetkov (she/her) is an Assistant Profes-\nsor at the Paul G. Allen School of Computer Sci-\nence and Engineering at the University of Washing-\nton, USA. Her research focuses on computational\nethics, multilingual NLP, and machine learning for\nNLP. She developed a course on Computational\nEthics in NLP and is teaching it at both undergrad-\nuate and graduate levels since 2017, and she is a\nco-chair of the ACL Ethics Committee.\n4.2 Audience and Pre-Requisites\nWe expect participants from a wide array of back-\ngrounds, including researchers, engineers, and end\nusers of NLP technologies. Based on prior itera-\ntions of the tutorial, we expect an audience size\nof 50-100. No prior experience with NLP/ML is\nrequired, but we believe that our tutorial will most\nbenefit those who are currently using NLP or are in-\ntending to use NLP tools in the near future in their\nresearch/products. An optional list of papers is\npresented in our survey paper (Kumar et al., 2022).\n4.3 Diversity\nThe content of this tutorial highlights the impact\nof LMs on diverse users and therefore we aim to\nreach wide and diverse audiences. We will adver-\ntise this tutorial to diverse groups of researchers\n(e.g., Masakane, LatinX, North Africans, disabled\nin AI, indigenous in AI, Khipu) to bring in partici-\npants from various backgrounds. A previous ver-\nsion of this tutorial attracted audience from diverse\ngender, race as well as professional backgrounds\nlike researchers, beginners and industry practition-\ners. Accordingly, our content will be made accessi-\nble to such audiences. Our own team is also diverse\nacross multiple demographic attributes as well as\nprofessional expertise.\n5 Logistics\nPrevious Editions This is the second iteration\nof the tutorial. The first edition of the tutorial was\npresented at The Web Conference 2022. While the\nprevious iteration was focused to a general CS audi-\nence with less NLP background, this iteration will\nbe modified to be aligned more for NLP-focused\naudience. This would entail including deeper tech-\nnical specification of the interventions, including\ndata, models and objectives.\nOur tutorial is related and complementary to\nprior ACL tutorials related to bias and fairness in\nNLP (Socially Responsible NLP at NAACL 2018,\nBias and Fairness in NLP at EMNLP 2019, Inte-\ngrating Ethics into the NLP Curriculum at ACL\n2020). Complementary to the content of the above\ntutorials which highlight social harms in NLP and\ndiscuss their detection, primarily focusing on rep-\nresentation learning and text classification, our tu-\ntorial will focus on practical methods to identify\nand mitigate harms in large language models and\nlanguage generation.\nVenue We prefer EMNLP or ACL, but any venue\nwould work for us.\nTechnical Requirements We will not require ad-\nditional equipment other than presentation material:\nan LCD projector, a computer with PowerPoint and\nAcrobat Reader, and internet connection.\nPublic Release We will publicly release all tu-\ntorial materials, including prerecorded lectures as\nbackup for the tutorial which will be uploaded prior\nto the tutorial. These will be hosted on an open-\naccess platform and linked from our University\nwebsites.\n6 Ethics Statement\nAlthough the aim of this tutorial is to improve the\nsafety and inclusivity of NLP technologies and\nequip practitioners with tools to do so, we are well\naware that as a not perfectly-diverse group of re-\nsearchers we might incorporate our own biases into\ntutorial stricture and its technical focus. We will\nacknowledge this limitation in our tutorial, as well\nas the fact that the field of computational ethics\nis developing rapidly, and thus the content of our\ntutorial is inherently incomplete.\n29\nReferences\nJacob Andreas, John Bufe, David Burkett, Charles Chen,\nJosh Clausman, Jean Crawford, Kate Crim, Jordan\nDeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan\nGuo, David Hall, Kristin Hayes, Kellie Hill, Diana\nHo, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant\nKrishnamurthy, Theo Lanman, Percy Liang, Christo-\npher H. Lin, Ilya Lintsbakh, Andy McGovern, Alek-\nsandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent\nRead, Dan Roth, Subhro Roy, Jesse Rusak, Beth\nShort, Div Slomin, Ben Snyder, Stephon Striplin,\nYu Su, Zachary Tellman, Sam Thomson, Andrei\nV orobev, Izabela Witoszko, Jason Wolfe, Abby Wray,\nYuchen Zhang, and Alexander Zotov. 2020. Task-\nOriented Dialogue as Dataflow Synthesis. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:556–571.\nVidhisha Balachandran, Hannaneh Hajishirzi, William\nCohen, and Yulia Tsvetkov. 2022. Correcting diverse\nfactual errors in abstractive summarization via post-\nediting and language model infilling. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nSolon Barocas, Kate Crawford, Aaron Shapiro, and\nHanna Wallach. 2017. The problem with bias: From\nallocative to representational harms in machine learn-\ning. In Proc. SIGCIS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big? In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nBen Buchanan, Andrew Lohn, Micah Musser, and Kate-\nrina Sedova. 2021. Truth, lies, and automation.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nMeng Cao, Yue Dong, Jiapeng Wu, and Jackie Chi Kit\nCheung. 2020. Factual error correction for abstrac-\ntive summarization models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6251–6258,\nOnline. Association for Computational Linguistics.\nShuyang Cao and Lu Wang. 2021. Cliff: Contrastive\nlearning for improving faithfulness and factuality in\nabstractive summarization.\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.\nFaithful to the original: Fact aware neural abstractive\nsummarization. In Proceedings of the Thirty-Second\nAAAI Conference on Artificial Intelligence, (AAAI-\n18), the 30th innovative Applications of Artificial\nIntelligence (IAAI-18), and the 8th AAAI Symposium\non Educational Advances in Artificial Intelligence\n(EAAI-18), New Orleans, Louisiana, USA, February\n2-7, 2018, pages 4784–4791. AAAI Press.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2020. Extracting training\ndata from large language models. arXiv preprint\narXiv:2012.07805.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nAlvin Chan, Yew-Soon Ong, Bill Pung, Aston Zhang,\nand Jie Fu. 2021. Cocon: A self-supervised approach\nfor controlled text generation. In Proc. ICLR.\nAlexandra Chronopoulou, Dario Stojanovski, and\nAlexander Fraser. 2020. Reusing a Pretrained Lan-\nguage Model on Languages with Limited Corpora for\nUnsupervised NMT. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2703–2711, Online. As-\nsociation for Computational Linguistics.\nThomas Davidson, Dana Warmsley, Michael Macy, and\nIngmar Weber. 2017. Automated hate speech de-\ntection and the problem of offensive language. In\nProceedings of the International AAAI Conference\non Web and Social Media, volume 11.\nRyan Daws. 2020. Medical chatbot using OpenAI’s\nGPT-3 told a fake patient to kill themselves.\nDaniel de Vassimon Manela, David Errington, Thomas\nFisher, Boris van Breugel, and Pasquale Minervini.\n2021. Stereotype and skew: Quantifying gender bias\nin pre-trained and fine-tuned language models. In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume , pages 2232–2242, Online.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\n30\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nEmily Dinan, Angela Fan, Adina Williams, Jack Ur-\nbanek, Douwe Kiela, and Jason Weston. 2020.\nQueens are powerful too: Mitigating gender bias in\ndialogue generation. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8173–8188, Online. As-\nsociation for Computational Linguistics.\nTobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019. Rank-\ning generated summaries by correctness: An interest-\ning but challenging application for natural language\ninference. In ACL (1), pages 2214–2220.\nAnjalie Field, Su Lin Blodgett, Zeerak Waseem, and\nYulia Tsvetkov. 2021. A survey of race, racism, and\nanti-racism in nlp. arXiv preprint arXiv:2106.11410.\nAnjalie Field and Yulia Tsvetkov. 2020. Unsupervised\ndiscovery of implicit gender bias. arXiv preprint\narXiv:2004.08361.\nRashmi Gangadharaiah and Balakrishnan\nNarayanaswamy. 2020. Recursive template-\nbased frame generation for task oriented dialog.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n2059–2064, Online. Association for Computational\nLinguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A Smith. 2020a. Realtoxi-\ncityprompts: Evaluating neural toxic degeneration in\nlanguage models. EMNLP.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020b. RealTox-\nicityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nTanya Goyal and Greg Durrett. 2020. Evaluating factu-\nality in generation with dependency-level entailment.\nIn EMNLP (Findings), pages 3592–3603.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nXiaochuang Han and Yulia Tsvetkov. 2020. Fortifying\ntoxic speech detectors against veiled toxicity. arXiv\npreprint arXiv:2010.03154.\nZexue He, Bodhisattwa Prasad Majumder, and Julian\nMcAuley. 2021. Detect and perturb: Neutral rewrit-\ning of biased and sensitive text via gradient-based\ndecoding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2021 , pages 4173–\n4181, Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nPo-Sen Huang, Huan Zhang, Ray Jiang, Robert Stan-\nforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani\nYogatama, and Pushmeet Kohli. 2020. Reducing sen-\ntiment bias in language models via counterfactual\nevaluation. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020, pages 65–83,\nOnline. Association for Computational Linguistics.\nElle Hunt. 2016. Tay, Microsoft’s AI chatbot, gets a\ncrash course in racism from Twitter.\nHuseyin A Inan, Osman Ramadan, Lukas Wutschitz,\nDaniel Jones, Victor Rühle, James Withers, and\nRobert Sim. 2021. Training data leakage analysis in\nlanguage models. arXiv preprint arXiv:2101.05405.\nHeesoo Jang. 2021. A South Korean chatbot shows just\nhow sloppy tech companies can be with user data.\nDietmar Jannach, Ahtsham Manzoor, Wanling Cai, and\nLi Chen. 2020. A survey on conversational recom-\nmender systems. arXiv preprint arXiv:2004.00646.\nShengyu Jia, Tao Meng, Jieyu Zhao, and Kai-Wei\nChang. 2020. Mitigating gender bias amplification\nin distribution by posterior regularization. In ACL\n(short).\nDaniel King, Zejiang Shen, Nishant Subramani,\nDaniel S Weld, Iz Beltagy, and Doug Downey. 2022.\nDon’t say what you don’t know: Improving the con-\nsistency of abstractive summarization by constraining\nbeam search. arXiv preprint arXiv:2203.08436.\nKalpesh Krishna, Ya yin Chang, John Wieting, and Mo-\nhit Iyyer. 2022. Rankgen: Improving text generation\nwith large ranking models. ArXiv, abs/2205.09726.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nSachin Kumar, Vidhisha Balachandran, Lucille Njoo,\nAntonios Anastasopoulos, and Yulia Tsvetkov. 2022.\nLanguage generation models can cause harm: So\nwhat can we do about it? an actionable survey. arXiv\npreprint arXiv:2210.07700.\nSachin Kumar, Eric Malmi, Aliaksei Severyn, and Yulia\nTsvetkov. 2021. Controlled text generation as contin-\nuous optimization with multiple constraints. In Proc.\nNeurIPS.\n31\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nPaul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and\nRuslan Salakhutdinov. 2021. Towards understand-\ning and mitigating social biases in language models.\nIn International Conference on Machine Learning,\npages 6565–6576. PMLR.\nBinny Mathew, Punyajoy Saha, Hardik Tharad, Subham\nRajgaria, Prajwal Singhania, Suman Kalyan Maity,\nPawan Goyal, and Animesh Mukherje. 2018. Thou\nshalt not hate: Countering online hate speech.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nFeng Nan, Cicero Nogueira dos Santos, Henghui Zhu,\nPatrick Ng, Kathleen McKeown, Ramesh Nallapati,\nDejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and\nBing Xiang. 2021. Improving factual consistency\nof abstractive summarization via question answering.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n6881–6894, Online. Association for Computational\nLinguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nHelen Ngo, Cooper Raterink, João GM Araújo, Ivan\nZhang, Carol Chen, Adrien Morisot, and Nicholas\nFrosst. 2021. Mitigating harm in language models\nwith conditional-likelihood filtration. arXiv preprint\narXiv:2108.07790.\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018. Reduc-\ning gender bias in abusive language detection. arXiv\npreprint arXiv:1808.07231.\nReid Pryzant, Richard Diehl Martinez, Nathan Dass,\nSadao Kurohashi, Dan Jurafsky, and Diyi Yang. 2020.\nAutomatically neutralizing subjective bias in text.\nProceedings of the AAAI Conference on Artificial\nIntelligence, 34(01):480–489.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nChengcheng Shao, Giovanni Luca Ciampaglia, Onur\nVarol, Kai-Cheng Yang, Alessandro Flammini, and\nFilippo Menczer. 2018. The spread of low-credibility\ncontent by social bots. Nature communications ,\n9(1):1–9.\nEmily Sheng, Kai-Wei Chang, Prem Natarajan, and\nNanyun Peng. 2021. Societal biases in language\ngeneration: Progress and challenges. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 4275–4293, Online.\nAssociation for Computational Linguistics.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda\nAskell, Ariel Herbert-V oss, Jeff Wu, Alec Rad-\nford, Gretchen Krueger, Jong Wook Kim, Sarah\nKreps, et al. 2019. Release strategies and the so-\ncial impacts of language models. arXiv preprint\narXiv:1908.09203.\nArt¯urs Stafanoviˇcs, Toms Bergmanis, and M¯arcis Pinnis.\n2020. Mitigating gender bias in machine translation\nwith target gender annotations. In Proceedings of\nthe Fifth Conference on Machine Translation, pages\n629–638, Online. Association for Computational Lin-\nguistics.\nGabriel Stanovsky, Noah A. Smith, and Luke Zettle-\nmoyer. 2019. Evaluating gender bias in machine\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 1679–1684, Florence, Italy. Association for\nComputational Linguistics.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\nBelding, Kai-Wei Chang, and William Yang Wang.\n2019. Mitigating gender bias in natural language\nprocessing: Literature review. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 1630–1640, Florence, Italy.\nAssociation for Computational Linguistics.\nAmeya Vaidya, Feng Mai, and Yue Ning. 2020. Em-\npirical analysis of multi-task learning for reducing\nidentity bias in toxic comment detection. In Proceed-\nings of the International AAAI Conference on Web\nand Social Media, volume 14, pages 683–693.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJames Vincent. 2022. YouTuber trains AI bot on 4chan’s\npile o’ bile with entirely predictable results.\n32\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\nZac Kenton, Sasha Brown, Will Hawkins, Tom\nStepleton, Courtney Biles, Abeba Birhane, Julia\nHaas, Laura Rimell, Lisa Anne Hendricks, William S.\nIsaac, Sean Legassick, Geoffrey Irving, and Iason\nGabriel. 2021. Ethical and social risks of harm from\nlanguage models.\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh,\nConor Griffin, Po-Sen Huang, John Mellor, Amelia\nGlaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh,\net al. 2022. Taxonomy of risks posed by language\nmodels.\nMarty J Wolf, Keith W Miller, and Frances S Grodzin-\nsky. 2017. Why we should have seen that com-\ning: comments on Microsoft’s Tay “experiment,” and\nwider implications. The ORBIT Journal, 1(2):1–12.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Ja-\nson Weston, and Emily Dinan. 2020. Recipes for\nsafety in open-domain chatbots. arXiv preprint\narXiv:2010.07079.\nKevin Yang and Dan Klein. 2021. FUDGE: Controlled\ntext generation with future discriminators. In Proc.\nNAACL.\nWenhao Yu, Chenguang Zhu, Zaitang Li, Zhiting Hu,\nQingyun Wang, Heng Ji, and Meng Jiang. 2022. A\nsurvey of knowledge-enhanced text generation. ACM\nComput. Surv. Just Accepted.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2020. Defending against neural fake\nnews. Neurips.\nHanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,\nand Dawei Song. 2022. A survey of controllable\ntext generation using transformer-based pre-trained\nlanguage models.\nXuhui Zhou, Maarten Sap, Swabha Swayamdipta,\nNoah A Smith, and Yejin Choi. 2021. Challenges\nin automated debiasing for toxic language detection.\narXiv preprint arXiv:2102.00086.\n33"
}