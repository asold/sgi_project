{
    "title": "Fine Tuning Large Language Model for Secure Code Generation",
    "url": "https://openalex.org/W4399567378",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2104629437",
            "name": "Junjie Li",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A3212238403",
            "name": "Aseem Sangalay",
            "affiliations": [
                "Delhi Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A1987889063",
            "name": "Cheng Cheng",
            "affiliations": [
                "Concordia University"
            ]
        },
        {
            "id": "https://openalex.org/A1038466737",
            "name": "Yuan Tian",
            "affiliations": [
                "Queen's University"
            ]
        },
        {
            "id": "https://openalex.org/A2140811187",
            "name": "Jinqiu Yang",
            "affiliations": [
                "Concordia University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3170092793",
        "https://openalex.org/W2770568736",
        "https://openalex.org/W3183469243",
        "https://openalex.org/W4285225959",
        "https://openalex.org/W3091588759",
        "https://openalex.org/W4281567711",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W4224060952",
        "https://openalex.org/W4281836191",
        "https://openalex.org/W4320560161",
        "https://openalex.org/W3162364548",
        "https://openalex.org/W3195703954",
        "https://openalex.org/W4226326075",
        "https://openalex.org/W4288057765",
        "https://openalex.org/W4385187279",
        "https://openalex.org/W4226205863",
        "https://openalex.org/W4282813445",
        "https://openalex.org/W4281763794",
        "https://openalex.org/W4221141536",
        "https://openalex.org/W3163206498",
        "https://openalex.org/W3193639695",
        "https://openalex.org/W4388867283"
    ],
    "abstract": "AI pair programmers, such as GitHub's Copilot, have shown great success in automatic code generation. However, such large language model-based code generation techniques face the risk of introducing security vulnerabilities to codebases. In this work, we explore the direction of fine-tuning large language models for generating more secure code. We use real-world vulnerability fixes as our fine-tuning dataset. We craft a code-generation scenario dataset (C/C++) for evaluating and comparing the pre-trained and fine-tuned models. Our experiments on GPT-J show that the fine-tuned GPT-J achieved 70.4% and 64.5% ratios of non-vulnerable code generation for C and C++, respectively, which has a 10% increase for C and a slight increase for C++ compared with the pre-trained large language model.",
    "full_text": null
}