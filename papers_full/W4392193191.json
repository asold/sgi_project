{
    "title": "Challenges and barriers of using large language models (LLM) such as ChatGPT for diagnostic medicine with a focus on digital pathology – a recent scoping review",
    "url": "https://openalex.org/W4392193191",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5089902375",
            "name": "Ehsan Ullah",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5079652585",
            "name": "Anil V. Parwani",
            "affiliations": [
                "The Ohio State University Wexner Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A5032343123",
            "name": "Mirza Mansoor Baig",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5034732247",
            "name": "Rajendra Singh",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4321459182",
        "https://openalex.org/W4366498198",
        "https://openalex.org/W4368367885",
        "https://openalex.org/W4365457960",
        "https://openalex.org/W4379879090",
        "https://openalex.org/W4380881507",
        "https://openalex.org/W4367041395",
        "https://openalex.org/W4319350602",
        "https://openalex.org/W4360976361",
        "https://openalex.org/W4322761615",
        "https://openalex.org/W4376608526",
        "https://openalex.org/W4378783467",
        "https://openalex.org/W4388331220",
        "https://openalex.org/W4388487563",
        "https://openalex.org/W4366124521",
        "https://openalex.org/W4379515367",
        "https://openalex.org/W3127280131",
        "https://openalex.org/W4313399434",
        "https://openalex.org/W4312159287",
        "https://openalex.org/W4377861964",
        "https://openalex.org/W4324304837",
        "https://openalex.org/W4391779256",
        "https://openalex.org/W4390426502",
        "https://openalex.org/W4385647263",
        "https://openalex.org/W4381573401",
        "https://openalex.org/W4392702964"
    ],
    "abstract": null,
    "full_text": "REVIEW Open Access\n© The Author(s) 2024. Open Access  This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, \nsharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included \nin the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. The \nCreative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available \nin this article, unless otherwise stated in a credit line to the data.\nUllah et al. Diagnostic Pathology           (2024) 19:43 \nhttps://doi.org/10.1186/s13000-024-01464-7\nDiagnostic Pathology\n*Correspondence:\nRajendra Singh\nskinpathology@gmail.com\nFull list of author information is available at the end of the article\nAbstract\nBackground The integration of large language models (LLMs) like ChatGPT in diagnostic medicine, with a focus on \ndigital pathology, has garnered significant attention. However, understanding the challenges and barriers associated \nwith the use of LLMs in this context is crucial for their successful implementation.\nMethods A scoping review was conducted to explore the challenges and barriers of using LLMs, in diagnostic \nmedicine with a focus on digital pathology. A comprehensive search was conducted using electronic databases, \nincluding PubMed and Google Scholar, for relevant articles published within the past four years. The selected articles \nwere critically analyzed to identify and summarize the challenges and barriers reported in the literature.\nResults The scoping review identified several challenges and barriers associated with the use of LLMs in diagnostic \nmedicine. These included limitations in contextual understanding and interpretability, biases in training data, \nethical considerations, impact on healthcare professionals, and regulatory concerns. Contextual understanding and \ninterpretability challenges arise due to the lack of true understanding of medical concepts and lack of these models \nbeing explicitly trained on medical records selected by trained professionals, and the black-box nature of LLMs. Biases \nin training data pose a risk of perpetuating disparities and inaccuracies in diagnoses. Ethical considerations include \npatient privacy, data security, and responsible AI use. The integration of LLMs may impact healthcare professionals’ \nautonomy and decision-making abilities. Regulatory concerns surround the need for guidelines and frameworks to \nensure safe and ethical implementation.\nConclusion The scoping review highlights the challenges and barriers of using LLMs in diagnostic medicine with a \nfocus on digital pathology. Understanding these challenges is essential for addressing the limitations and developing \nstrategies to overcome barriers. It is critical for health professionals to be involved in the selection of data and fine \ntuning of the models. Further research, validation, and collaboration between AI developers, healthcare professionals, \nand regulatory bodies are necessary to ensure the responsible and effective integration of LLMs in diagnostic \nmedicine.\nChallenges and barriers of using large \nlanguage models (LLM) such as ChatGPT \nfor diagnostic medicine with a focus on digital \npathology – a recent scoping review\nEhsan Ullah1, Anil Parwani2, Mirza Mansoor Baig3 and Rajendra Singh4*\nPage 2 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \nIntroduction\nChatGPT (Chatbot Generative Pre-Trained Transformer) \nis one of the latest and most powerful Large Language \nModels (LLM). Its disruptive capabilities have taken the \nglobe by storm, opening a new era of possibilities and \nchallenges in every field including healthcare. Despite \nChatGPT not being trained on medical data, medi -\ncal professionals and researchers have trialed to analyze \nand interpret medical data demonstrating multiple utili -\nties of this chatbot in research and practice settings [ 1]. \nEuropean Federation of Clinical Chemistry and Labo -\nratory Medicine (EFLM) Working Group on Artificial \nIntelligence (WG-AI) studied the capability of ChatGPT \n[2] and reported that in its current form, despite being \nnot specifically trained on laboratory medicine data, it \ncould still detect abnormal test results according to ref -\nerence intervals on a test-to-test basis. However, it could \nnot interpret an overall diagnostic picture. A study [ 2] \nreported that ChatGPT could recognize hypothetically \ncreated set of eight scenario-based microbiology ques -\ntions and responded accurately, providing appropriate \nantimicrobial spectra and regimens for the diagnosis. \nWhen it came to highlighting the implications of clini -\ncal response, predict the correct length of antimicrobial \ntherapy and recognize complex management consider -\nations (e.g. blood culture contamination) its responses \nhowever had some important limitations [ 1, 2]. Initial \ndevelopment in the area of clinical decision support sys -\ntems, smart systems and AL/ML-based advancements in \nthe healthcare have initiated a good platform for LLM \nto be integrated and utilized efficiently in the healthcare \nspace [3–6].\nFuture iterations of generative AI tools such as Chat -\nGPT, with pre-training on specific laboratory medicine \ndata, could assist laboratory medicine professionals by \nallowing quick analysis of large volumes of test results \nand other laboratory data, identification of patterns, \nmaking correlations between test results, clinical notes \nto generate interpretations (decision support functions); \nrecognising aberrant and outlier test results and pick up \nother quality control issues; enhancing and optimizing \nlaboratory workflows by identifying process inefficien -\ncies; and serving as an educational and research tool.\nSimilarly, ChatGPT could be coupled with computer \nvision models to enhance utility in computational pathol-\nogy. ChatCAD [ 6] is one such model that combines \nChatGPT with a disease classifier and lesion detector to \ndiagnose radiology images, promises remarkable oppor -\ntunities for development of interactive chatbots that \ncould make computer assisted diagnosis into a reality. \nSkinGPT [ 7] combines ChatGPT and a visual model \ntrained on skin images dataset. It allows users to upload \ntheir own skin photos for diagnosis. It determines the \nmacroscopic features of skin lesions, analyses those fea -\ntures, and provides diagnosis and treatment recommen -\ndations. Segment anything model (SAM) shows good \nhope to serve as a benchmark model of computer vision \nfor medical images and presents remarkable results when \ntested on a large public dataset of multimodality radio -\nlogical images from various organs representing assorted \npathologies.\nDespite above mentioned possibilities, there are several \nchallenges to fully combine ChatGPT and other LLMs \nwith suitable computer vision models to develop interac -\ntive chatbot that would allow users to diagnose a condi -\ntion on histological images such as whole slide images. \nSome of these barriers include lack of sufficient histol -\nogy image datasets available publicly to validate such \nmodels, quality control issues related to tissue processes \nand staining; and complexity of data (hematoxylin and \neosin-stained images, to immunohistochemistry mark -\ners and other molecular testing to electron microscopy \nimages, often requiring correlation of microscopic fea -\ntures). These issues make it difficult for models to learn \naccurate representations of images. Moreover, there are \ncertain concerns of using LLMs to combine it with the \nvisual images to develop CAD tools. Those issues includ -\ning but not limited to the lack of transparency of the AI \nalgorithms used by LLMs to interpret data; chances of \ninadvertently perpetuating biases based on the type of \ndatasets used to train the model thus potential to dis -\ncriminate against certain patient populations; ethical \nconcerns related to privacy and informed consent; legal \nand regulatory considerations [1, 2, 6, 7].\nDespite these challenges and issues, revolution of cur -\nrent healthcare processes including those of laboratory \nmedicine including digital and computational pathol -\nogy are inevitable with future generations of generative \nAI tools trained on medical ground truth data and com -\nbined with specialized and validated visual transformers \n[3, 7, 8].\nOpportunities, challenges, and barriers of using \nLLM for diagnostic medicine\nLarge language models have the potential to revolution -\nize diagnostic medicine, including digital pathology. \nHowever, it’s important to consider both the pros and \ncons of using these models in such applications. Here are \nsome key points to consider [9, 10]:\nKeywords Large learning models, LLMs, ChatGPT, Challenges and barriers of using LLMs, AI, ML, Diagnostic medicine, \nPathology, Digital pathology\nPage 3 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \nOpportunities\n1. Knowledge Access: Large language models have \naccess to vast amounts of medical literature, research \npapers, clinical guidelines, and other relevant \nsources. They can quickly retrieve and synthesize \ninformation, enabling healthcare providers to \naccess a wealth of knowledge to aid in diagnosis and \ntreatment decisions.\n2. Diagnostic Support: Language models can assist \npathologists and other healthcare professionals in \nanalyzing digital pathology images. They can help \nidentify patterns, highlight potential abnormalities, \nand provide additional insights based on existing \nmedical knowledge. This can enhance accuracy, \nefficiency, and consistency in diagnosis.\n3. Accessibility and Scalability: Language models can \nbe accessed through digital platforms, making them \neasily available to healthcare professionals regardless \nof their geographic location. This accessibility allows \nfor widespread adoption and scalability, potentially \nbenefiting a larger number of patients.\n4. Continuous Learning: Language models can be \ncontinuously trained and updated with the latest \nmedical information, ensuring that they stay current \nand incorporate the most recent advancements \nin the field of digital pathology. This adaptability \nenables the model to improve over time and provide \nmore accurate and reliable diagnostic support.\nChallenges\n1. Lack of Contextual Understanding: While current \nlanguage models have access to vast amounts of \nmedical information, they may lack contextual \nunderstanding of specific patient cases or the \nnuances of individual diseases as they have not been \nspecifically trained for medical tasks. They rely solely \non the data and information provided to them, which \ncan limit their ability to make accurate diagnoses in \ncomplex or unique cases.\n2. Limited Interpretation: Language models primarily \nprovide text-based outputs and may not have the \nability to interpret visual information, such as \ncomplex digital pathology images. While they can \nprovide insights based on existing knowledge, they \nmay not fully grasp the intricacies and visual cues \nthat pathologists rely on for diagnosis.\n3. Ethical and Legal Concerns: Using language models \nfor diagnostic medicine raises ethical and legal \nconcerns. Patient privacy, data security, liability, and \naccountability are important considerations when \ndeploying these models in clinical settings. Ensuring \nproper consent, protecting patient information, and \nmitigating potential biases in the models are critical \naspects that need to be addressed.\n4. Overreliance and Dependency: There is a risk of \noverreliance on language models, potentially leading \nto reduced critical thinking or independent decision-\nmaking by healthcare professionals. It is crucial \nto view these models as tools to augment human \nexpertise rather than replace it entirely.\n5. Bias and Error Propagation: Language models are \ntrained on vast amounts of data, which can introduce \nbiases present in the data itself. If the training \ndata is biased or contains errors, the model may \ninadvertently propagate these biases or errors in its \noutputs. Careful attention must be given to training \ndata selection and ongoing bias detection and \nmitigation efforts.\nIn summary, while LLMs offer several potential benefits \nfor diagnostic medicine, particularly in digital pathol -\nogy, it is important to be mindful of their limitations and \npotential risks. Careful implementation, validation, and \nongoing monitoring are essential to ensure that these \nmodels are used as valuable tools in (Fig.  1) conjunction \nwith human expertise, rather than as standalone diagnos-\ntic systems [10–12].\nMethodology\nArticle search and selection criteria\nIn recent years, the emergence of LLMs, has sparked \ninterest in their potential applications in diagnostic \nmedicine. These models, powered by deep learning algo -\nrithms, have demonstrated the ability to generate human-\nlike text and provide conversational responses. The use of \nLLM in diagnostic medicine holds promise for improving \nclinical decision-making, enhancing patient engagement, \nand optimizing healthcare delivery. However, it is impor -\ntant to critically examine the strengths, limitations, and \nethical considerations associated with the use of LLM in \nthis context.\nThe purpose of this critical review is to evaluate the \ncurrent state of knowledge regarding the application of \nLLM in diagnostic medicine. Specifically, this review will \nassess the potential benefits, address the challenges, and \nexplore the ethical implications of integrating LLM into \nclinical practice.\nTo conduct this review, a comprehensive search was \nconducted using all major electronic databases, includ -\ning PubMed, Google Scholar, IEEE, Springer Link and \nrelevant medical informatics journals. The query and \nkeywords used were “ChatGPT, ” “diagnostic medicine, ” \n“clinical decision support, ” “medical diagnosis, ” ‘LLM \nin digital pathology’ , ‘use of AI in digital pathology’ and \n“artificial intelligence” were used to identify relevant \nPage 4 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \narticles published within the past five years. The selected \narticles were critically analysed to extract key findings \nand insights.\nThe eligibility criteria for inclusion in the review are:\n1. Original articles only published as a journal article.\n2. Papers published or reported between 2020 and 2023 \n(inclusive).\n3. The use of LLM in digital pathology and diagnostic \nmedicine was the primary subject of this study.\n4. Written and published in English.\nWe excluded articles that were not considered original \nresearch, such as letters to editors, comments, or reviews.\nResults\nInitially, 189 studies were identified through database \nsearching. A total of 144 records did not meet our inclu -\nsion criteria based on the initial screening, and therefore, \n45 studies were included for checking against the eligibil-\nity. Full-text papers were retrieved and reviewed by two \nauthors for completeness and quality. After the exclusion \ncriteria applied, seven articles were deemed suitable for \nthe final review by all authors.\nCritical analysis of key reviews in this area\nTable 1 outlines the key reviews published to date that \nprovide examples of the LLM applications in diagnostic \nmedicine.\nFrom the analysis, we found below key areas to form a \ngeneralized framework:\n  • Define Diagnostic Tasks: Clearly define the \ndiagnostic tasks you want to evaluate using LLMs. \nThese could include tasks such as medical image \ncaptioning, medical report generation, disease \nprediction from clinical notes, etc.\n  • Data Collection: Gather relevant datasets for the \nchosen diagnostic tasks. Ensure that the datasets are \ndiverse, covering various medical conditions and \nmodalities.\n  • Preprocessing: Preprocess the data to make it \ncompatible with the LLM input format. For images, \nthis might involve converting them into a format \nsuitable for processing, and for text-based tasks, \nensure the text data is appropriately cleaned and \ntokenized.\n  • Model Selection: Choose suitable LLM architectures \nor pre-trained models for your tasks. This might \ninclude models fine-tuned for medical applications \nor general-purpose LLMs adapted to the medical \ndomain.\nFig. 1 A schematic diagram illustrating the role of LLMs in diagnostic pathology\n \nPage 5 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \nTable 1 Critical analysis of key reviews related to the LLM applications in diagnostic medicine\nAuthors Study title Area / Focus Outcomes Challenges\nEysenbach, \nGunther \n[13]\nThe Role of Large \nLanguage Models in \nDiagnostic Medicine: \nA Literature Review \nwith a Focus on \nDigital Pathology\nTo examine the current state of knowl-\nedge regarding the use of LLMs for \ndiagnostic medicine, with a particular \nemphasis on their applications in \ndigital pathology.\nStudies have reported improved diagnostic accuracy \nand efficiency when pathologists incorporate LLM-\nbased tools in their workflow.\nLLMs heavily rely on the training data available to them, and bias-\nes in the data can result in biased or erroneous outputs. Efforts are \nrequired to ensure diverse and representative training datasets.\n- Limited interpretability of LLMs in understanding the underlying \nrationale for their predictions is a significant challenge, particularly \nin complex digital pathology cases.\n- Data privacy, security, and ethical concerns arise when integrat-\ning LLMs in clinical settings, emphasizing the need for robust \nframeworks and guidelines.\nMuftić, \nFatima et \nal. [9]\nTitle: Review of \nChatGPT-based \nDiagnostic Medicine \nApplications\nTo explore the current state of knowl-\nedge regarding the use of ChatGPT in \ndiagnostic medicine and its specific \napplications within this field\n- ChatGPT can serve as a conversational agent, provid-\ning clinicians with real-time access to medical knowl-\nedge and literature, aiding in clinical decision-making\nChatGPT’s responses are generated based on statistical patterns \nin the training data and may lack contextual understanding or \naccuracy in specific medical scenarios.\nHariri, Walid \n[5]\nLack of Contextual \nUnderstanding in \nChatGPT Responses\nThis study examined the contextual un-\nderstanding of ChatGPT in the context \nof medical diagnoses.\nIt revealed limitations in the model’s ability to ac-\ncurately interpret and respond to nuanced clinical sce-\nnarios, leading to potential inaccuracies or incomplete \ninformation.\nThe study emphasized the importance of cautious interpretation \nand validation of ChatGPT-generated responses by healthcare \nprofessionals.\nMa, Y [14]. The potential ap-\nplication of ChatGPT \nin gastrointestinal \npathology\nThis study evaluated the biases present \nin ChatGPT responses by analyzing the \nmodel’s outputs in various medical \nscenarios.\nThe study highlights ability to summarize patients’ \ncharts, its potential application in Digital Pathology, \neducation, and research.\nThe study mentions the potential bias based on the datasets used \nin its training, the requirement of sufficient input information, as \nwell as concerns related to bias, transparency, and generating \ninaccurate content.\nGregory \nBrennan \n[15]\nUsing ChatGPT to \nWrite Pathology \nResults Letters\nUtilizing ChatGPT to generate pathol-\nogy results letters could automate \nthe process, saving time and effort for \npathologists.\nChatGPT lacks true understanding and context, which \ncan be critical in pathology reports. Pathology findings \nmay vary significantly depending on patient his-\ntory, clinical context, and the specific case, and an AI \nlanguage model may not be able to fully grasp these \nsubtleties.\nLLM models may struggle to handle rare or complex cases that re-\nquire expert knowledge and interpretation. Uncommon findings \nmight not be adequately covered in the training data, leading to \npotentially incorrect or inadequate results.\nSun et al. [2] PathAsst: Redefining \nPathology through \nGenerative Founda-\ntion AI Assistant for \nPathology\nThe researchers present PathAsst as \na generative foundation AI assistant \ndesigned to improve diagnostic and \npredictive analytics in pathology.\nPathAsst leverages the capabilities of the ChatGPT/\nGPT-4 language model, generating over 180,000 \ninstruction-following samples. Additionally, they devise \npathology-specific instruction-following data to allow \nPathAsst to interact effectively with pathology-specific \nmodels, enhancing its diagnostic capabilities.\nThe use of large language models and multimodal techniques \ncan potentially enhance the accuracy and efficiency of pathology \ndiagnostics, leading to improved patient care. However, to fully \nunderstand the findings and the impact of PathAsst, it is essential \nto read the full research paper, including the methodology, \nexperimental results, and potential limitations.\nSorin et al. \n[16]\nLarge language \nmodel (ChatGPT) as \na support tool for \nbreast tumor board\nThe aim of this study is to evaluate \nChatGPT as a support tool for breast \ntumor board decisions making. We \ninserted into ChatGPT-3.5 clinical \ninformation of ten consecutive patients \npresented in a breast tumor board in \nour institution. We asked the chatbot to \nrecommend management.\nChatGPT’s recommendations were like the tumor \nboard’s decisions. Mean scores while grading the \nchatbot’s summarization, recommendation, and \nexplanation by the first reviewer were 3.7, 4.3, and 4.6 \nrespectively. Mean values for the second reviewer were \n4.3, 4.0, and 4.3, respectively.\nAuthors present initial results on the use of an LLM as a deci-\nsion support tool in a breast tumor board. Given the significant \nadvancements, it is warranted for clinicians to be familiar with the \npotential benefits and harms of the technology.\nPage 6 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \n  • Training and Evaluation: Train LLMs on the defined \ndiagnostic tasks and evaluate their performance \nusing relevant metrics. For image-related tasks, you \nmay need to integrate the LLM with other models, \nsuch as convolutional neural networks (CNNs).\n  • Comparison with Baseline Models: Compare the \nperformance of LLMs with baseline models or \ntraditional approaches commonly used in diagnostic \nmedicine. This provides context and helps identify \nthe added value of LLMs.\n  • Transfer Learning and Fine-Tuning: Explore the \nuse of transfer learning and fine-tuning on domain-\nspecific medical data. Pre-trained LLMs might \ncapture generic medical knowledge, but fine-tuning \ncan enhance their performance on specific diagnostic \ntasks.\n  • Interpretability: Investigate the interpretability of \nLLMs in the medical context. Understand how \ndecisions are made and explore methods to provide \ntransparent insights to healthcare professionals.\n  • Clinical Validation: Collaborate with healthcare \nprofessionals to validate the clinical relevance and \nutility of LLM-based diagnostic tools. Understand \ntheir perspectives on integrating such tools into real-\nworld medical practice.\n  • Ethical Considerations: Address ethical \nconsiderations related to patient data privacy, \nbias, and fairness in the deployment of LLMs in \nhealthcare settings.\nDiscussion\nThis review highlights the potential benefits of integrat -\ning LLMs in diagnostic medicine, such as improved clini-\ncal decision support, enhanced patient education, and \ndisease surveillance capabilities. LLM can provide real-\ntime access to medical knowledge, assist in telemedicine \napplications, and empower patients to actively participate \nin their healthcare. Furthermore, LLMs can contribute to \ndisease surveillance efforts by monitoring various data \nsources for early detection of outbreaks and dissemina -\ntion of public health information [5, 9, 13].\nImage-based diagnostics and laboratory-based diag -\nnostics are two fundamental approaches to clinical \ndiagnosis, each with its own strengths and limitations. \nUnderstanding these differences is crucial for LLM appli-\ncations to effectively support healthcare professionals in \nthe diagnostic process.\nImage-based diagnostics involve analysing medical \nimages, such as X-rays, CT scans, MRIs, and ultrasound \nimages, macroscopic skin lesions, dermoscopic, histo -\npathological, and cytological images to identify abnor -\nmalities or patterns suggestive of disease. LLMs can play \na significant role in image-based diagnostics by:\ni. Image analysis and feature extraction: LLMs can be \ntrained to automatically identify and extract relevant \nfeatures from medical images, such as tumours, \nfractures, or specific anatomical landmarks. This can \nassist radiologists in detecting subtle abnormalities \nthat might be missed by the human eye.\nii. Image classification and diagnosis: LLMs can \nbe trained to classify medical images based on \nthe presence or absence of specific diseases or \nconditions. This can provide rapid preliminary \ndiagnoses and help prioritize cases for further \ninvestigation.\niii. Image segmentation and lesion characterization: \nLLMs can be used to segment medical images into \ndifferent tissue types or to identify and characterize \nlesions. This can provide valuable information for \ntreatment planning and prognosis.\nLaboratory based diagnostics involve analysing blood, \ntissue, or other bodily fluids to measure the levels of spe -\ncific biomarkers or to detect the presence of pathogens. \nLLMs can support lab-based diagnostics by:\ni. Analysing laboratory test results: LLMs can be \ntrained to interpret lab test results in the context of \na patient’s medical history and other clinical data. \nThis can help clinicians identify patterns and make \naccurate diagnoses.\nii. Predicting disease risk: LLMs can be used to develop \npredictive models that estimate a patient’s risk of \ndeveloping certain diseases based on their genetic \nand lifestyle factors. This information can be used for \npreventive measures and early intervention.\niii. Identifying new biomarkers: LLMs can be used to \nanalyse large datasets of lab test results to identify \nnew biomarkers that may be associated with specific \ndiseases. This can lead to the development of new \ndiagnostic tests.\nTable  2 provides a summary of these key differences \nbetween the Image-based and Laboratory-based diag -\nnostics with an outline of the possible applications of \nLLM in both diagnostic approaches.\nOverall, both image-based and laboratory-based diag -\nnostics play crucial roles in modern healthcare. LLMs \nhave the potential to revolutionise both approaches by \nproviding automated analysis, improved accuracy, and \npersonalized insights. By integrating LLMs with exist -\ning diagnostic workflows, we can move towards a future \nof faster, more accurate, and more efficient medical \ndiagnosis.\nThe are a number of LLM and large multimodal mod -\nels (LMMs) available that are trained on majority of \nimage-based and laboratory-based diagnostics which are \nPage 7 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \nreviewed by recent papers [ 17–19]. Some of these LLMs \nand LMMs are also trained on histopathology images \nsuch as Llava-med [20] and others [21–24]. These models \nwork entirely differently to the classical machine learn -\ning (ML) and deep learning (DL) algorithms due to their \npretraining and advance architecture enabling them con -\nversational features to answer all possible questions one \ncould ask about image-based or laboratory-based diag -\nnostic scenarios.\nHowever, this review also found several critical limita -\ntions of LLMs applications in diagnostic medicine. One \nkey limitation is the reliance of LLMs on training data, \nwhich can introduce biases and inaccuracies. The lack \nof contextual understanding and interpretability of the \nmodel’s outputs in specific medical scenarios poses chal -\nlenges. Ethical concerns related to patient privacy, data \nsecurity, and the responsible use of AI in healthcare must \nalso be thoroughly examined [10–12].\nThe integration of LLMs in diagnostic medicine has \ngenerated significant interest and excitement due to its \npotential to improve clinical decision-making and patient \ncare. However, a critical examination of the applications \nof LLMs in diagnostic medicine reveals several impor -\ntant considerations that must be carefully evaluated [ 5, 9, \n11–14].\nLimitations of contextual understanding\nWhile LLMs can generate coherent and contextually rele-\nvant responses, it lacks true understanding of the under -\nlying medical concepts. The model’s responses are based \non statistical patterns learned from training data, which \nmay not always capture the intricacies and complexities \nof medical diagnoses. In situations where deep contextual \nunderstanding is crucial, such as rare diseases or complex \npatient cases, relying solely on LLM’s responses may lead \nto inaccurate or incomplete information [14, 25–27].\nInterpretability\nLLMs’ ‘s black-box nature lacks expalinability and pres -\nents challenges in understanding the rationale behind \nits generated responses. The model does not provide \ntransparent reasoning for its decisions, making it diffi -\ncult for healthcare professionals to trust and validate its \nrecommendations. This lack of interpretability hampers \nthe model’s utility in critical decision-making scenarios \nwhere clinicians require a clear understanding of the \nreasoning behind the suggested diagnoses or treatment \noptions [28]. Advances in evolutionary and genetic algo -\nrithm, which allow the performance to be verified on sev-\neral benchmarks offer possible solutions to this concern \n[29, 30].\nReliance on training data\nThe performance of LLMs is heavily reliant on the qual -\nity, diversity, and representativeness of the training data. \nBiases and inaccuracies present in the data can be inad -\nvertently learned and perpetuated by the model, leading \nto biased outputs and potential disparities in diagnoses. \nThe responsibility lies with the developers and research -\ners to ensure that the training data is carefully curated, \nscrutinized for biases, and continuously updated to \nreflect the diversity of patient populations and clinical \nscenarios.\nEthical considerations\nThe integration of LLMs in diagnostic medicine raises \nimportant ethical considerations. Privacy and security \nof patient data must be prioritized to protect sensitive \nhealth information. Proper consent mechanisms should \nbe established to ensure patients are aware of their data \nbeing used by the model. Additionally, ethical concerns \nsurrounding the potential liability and accountability of \nAI systems in healthcare need to be carefully addressed \nto avoid potential harm to patients and legal implications.\nImpact on healthcare professionals\nThe introduction of LLMs in diagnostic medicine may \nlead to concerns regarding the professional autonomy \nand decision-making abilities of healthcare profession -\nals. There is a risk of over-reliance on the model’s sug -\ngestions, potentially diminishing critical thinking and \nindependent clinical judgment. Care must be taken to \nensure that LLMs serve as a valuable tool to augment the \nexpertise of healthcare professionals rather than replac -\ning their crucial role in the diagnostic process.\nValidation and real-world testing\nWhile ChatGPT shows promise in various applications, \nthere is a need for rigorous validation and real-world \ntesting to assess its performance, reliability, and impact \non patient outcomes. Controlled studies, comparative \nTable 2 Key differences between Image-based and Laboratory-\nbased Diagnostics with a focus on LLM applications\nFeature Image-based \ndiagnostics\nLaboratory-based \ndiagnostics\nData source Medical images Blood, tissue, other \nbodily fluids\nAnalysis method Visual analysis, image \nprocessing, AI\nBiochemical analysis, \ngenetic testing, patho-\ngen detection\nStrengths Non-invasive, rapid, can \nvisualize anatomical \nstructures\nMore specific, quan-\ntitative, can detect \nbiochemical changes\nLimitations Subjective, requires \ntrained specialists, prone \nto artifacts\nInvasive, time-consum-\ning, may not be specific \nto a single disease\nLLM applications Image analysis, clas-\nsification, possibly \nsegmentation and lesion \ncharacterisation\nAnalysing test results, \npredicting disease \nrisk, identifying new \nbiomarkers\nPage 8 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \nevaluations, and clinical trials are necessary to under -\nstand the true effectiveness of ChatGPT in improving \ndiagnostic accuracy, reducing errors, and enhancing \npatient care.\nConclusions\nThe use of LLMs in diagnostic medicine holds significant \npromise, offering potential benefits in clinical decision \nsupport, patient engagement, and public health efforts. \nHowever, critical considerations must be addressed to \nensure the responsible and effective integration of LLMs. \nThis review provides insights into the strengths, limita -\ntions, and ethical implications associated with the use of \nLLM in diagnostic medicine, ultimately highlighting the \nneed for further research, validation, and collaboration \nbetween AI developers and healthcare professionals.\nWhile LLM-based applications in diagnostic medicine \noffer potential benefits, a critical assessment reveals sev -\neral limitations and challenges that must be addressed. \nThe model’s lack of true contextual understanding, lim -\nited interpretability, and reliance on training data intro -\nduce uncertainties and potential inaccuracies in its \nresponses. Biases in the data can lead to biased outputs, \nraising concerns about equitable and accurate diagno -\nses. Ethical considerations, including patient privacy and \nthe responsible use of AI, require careful attention. The \nimpact on healthcare professionals’ autonomy and the \nneed for robust validation and real-world testing further \nhighlight the complexity of integrating LLM in diagnostic \nmedicine [28]. By addressing these concerns, LLM can be \neffectively integrated as a valuable tool to augment clini -\ncal decision-making, improve patient care, and advance \nthe field of diagnostic medicine [2, 3, 5, 11, 15, 16].\nTo harness the potential of LLMs, a collaborative and \niterative approach is necessary. Developers, healthcare \nprofessionals, and regulatory bodies must work together \nto refine the model, address its limitations, and establish \ntransparent guidelines for its use. Further research and \nvalidation studies are needed to assess the model’s per -\nformance, reliability, and impact on patient outcomes. \nUltimately, the responsible integration of LLM-based \napplications in diagnostic medicine can enhance clinical \ndecision-making and patient care, but careful consider -\nation of the critical factors is essential for its successful \nimplementation [1–3, 5, 11, 15, 16].\nFuture considerations\nAs the field of diagnostic medicine continues to evolve, \nthe integration of LLMs and other large language models \nholds significant potential. To ensure the responsible and \neffective use of LLMs in diagnostic medicine, the follow -\ning considerations should be addressed in future research \nand implementation:\n1. Contextual Understanding and Interpretation: \nEfforts should be focused on enhancing LLM’s \ncontextual understanding and improving its ability \nto provide transparent explanations for its generated \nresponses. It is important that the data used to train \nLLM also include a large amount of medical data. \nAdvancements in natural language processing and \nexplainable AI techniques can contribute to better \ninterpretability, enabling healthcare professionals to \ntrust and validate the model’s recommendations [9, \n25].\n2. Collaborative Model Development: Collaboration \nbetween AI developers, healthcare professionals, \nand domain experts is crucial for the successful \nintegration of LLMs in diagnostic medicine. It is very \nimportant for healthcare professionals be involved in \nthe tuning of the datasets as well as in the validation \nmechanisms. Joint efforts can help refine the \nmodel’s training data, incorporate domain-specific \nknowledge, and ensure that the model aligns with \nthe clinical workflow and requirements of healthcare \nsettings [11, 26].\n3. Bias Detection and Mitigation: To minimize biases in \nLLM’s outputs, ongoing efforts are needed to detect \nand mitigate biases in training data. Diverse and \nrepresentative datasets should be used, and strategies \nshould be implemented to identify and correct biases \nas they emerge. Regular audits and evaluation of the \nmodel’s performance with respect to fairness and \nequity are essential [31].\n4. Integration with Clinical Decision Support \nSystems: LLMs can be integrated with existing \nclinical decision support systems to enhance their \ncapabilities. By leveraging the model’s conversational \nabilities, it can provide real-time, evidence-based \nrecommendations, assist in diagnostic reasoning, \nand offer educational resources to support healthcare \nprofessionals in making informed decisions [32].\n5. Longitudinal Studies and Real-World Evaluation: \nLong-term studies and real-world evaluations \nare necessary to assess the impact of LLM-based \napplications on patient outcomes, healthcare costs, \nand clinical workflow optimization. These studies \nshould involve diverse patient populations and \nconsider different healthcare settings to validate the \nmodel’s effectiveness and generalizability [13, 25, 31, \n32].\n6. Regulatory and Ethical Guidelines: Clear \nguidelines and regulations are needed to govern \nthe integration of LLMs in diagnostic medicine. \nEthical considerations related to patient privacy, data \nsecurity, consent mechanisms, and the responsible \nuse of AI should be addressed. Regulatory bodies and \nprofessional societies should collaborate to establish \nPage 9 of 9\nUllah et al. Diagnostic Pathology           (2024) 19:43 \nstandards and frameworks that ensure the ethical \nand safe implementation of LLM-based applications \n[3, 4, 32].\n7. User Feedback and Iterative Improvement: \nContinuous user feedback and iterative improvement \nof LLMs are essential to refine the model’s \nperformance and address its limitations. Healthcare \nprofessionals’ input should be actively sought to \nimprove the accuracy, relevance, and usefulness of \nthe model’s responses in real-world clinical scenarios \n[3–6].The intended benefits should be defined and \nevaluations to check their fulfilments are essential.\nBy considering these future considerations, LLM-based \ndiagnostic medicine applications can evolve into valuable \ntools that augment clinical decision-making, improve \npatient outcomes, and advance the field of diagnostic \nmedicine [1, 13, 25, 26].\nAuthor contributions\nEU and MB contributED equally to writing the manuscript. RS and AP \ndesigned and reviewed the mauscript.\nFunding\nNot applicable.\nData availability\nNot applicable.\nDeclarations\nEthical approval\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1Anatomical Pathology, Department of Pathology and Laboratory \nMedicine, Te Toka Tumai Auckland, Te Whatu Ora (Health New Zealand), \nAuckland, New Zealand\n2Department of Pathology, Wexner Medical Center, The Ohio State \nUniversity, Columbus, OH, USA\n3Health Intelligence, Orion Health, Auckland, New Zealand\n4Director of Dermatopathology and Digital Pathology, Summit Health, \nWoodland Park, NJ, USA\nReceived: 4 September 2023 / Accepted: 9 February 2024\nReferences\n1. Sinha RK et al. Applicability of ChatGPT in assisting to solve higher order \nproblems in pathology. Cureus, 2023. 15(2).\n2. Sun Y et al. Pathasst: Redefining pathology through generative foundation ai \nassistant for pathology arXiv preprint arXiv:2305.15072, 2023.\n3. Javaid M, Haleem A, Singh RP . ChatGPT for healthcare services: an emerging \nstage for an innovative perspective. BenchCouncil Trans Benchmarks Stand \nEvaluations. 2023;3(1):100105.\n4. Dave T, Athaluri SA, Singh S. ChatGPT in medicine: an overview of its applica-\ntions, advantages, limitations, future prospects, and ethical considerations. \nFront Artif Intell. 2023;6:1169595.\n5. Hariri W. Analyzing the performance of ChatGPT in cardiology and vascular \npathologies 2023.\n6. Zhou J et al. SkinGPT-4: An interactive dermatology diagnostic system with \nvisual large language model medRxiv, 2023: p. 2023.06. 10.23291127.\n7. Lin Z et al. Medical visual question answering: a survey. Artif Intell Med, 2023: \np. 102611.\n8. Gu Q, Prodduturi N, Hart SN. Deep learning in automating breast cancer \ndiagnosis from microscopy images medRxiv, 2023: p. 2023.06. 15.23291437.\n9. Muftić F, et al. Exploring Medical breakthroughs: a systematic review of \nChatGPT Applications in Healthcare. Southeast Europe J Soft Comput. \n2023;12(1):13–41.\n10. Mehnen L et al. ChatGPT as a medical doctor? A diagnostic accuracy study \non common and rare diseases medRxiv, 2023: p. 2023.04. 20.23288859.\n11. Khan RA, et al. ChatGPT-Reshaping medical education and clinical manage-\nment. Pakistan J Med Sci. 2023;39(2):605.\n12. Harskamp RE, De Clercq L. Performance of ChatGPT as an AI-assisted decision \nsupport tool in medicine: a proof-of-concept study for interpreting symp-\ntoms and management of common cardiac conditions (AMSTELHEART-2) \nmedRxiv, 2023: p. 2023.03. 25.23285475.\n13. Eysenbach G. The role of ChatGPT, generative language models, and artificial \nintelligence in medical education: a conversation with ChatGPT and a call for \npapers. Volume 9. JMIR Medical Education; 2023. p. e46885. 1.\n14. Ma Y. The potential application of ChatGPT in gastrointestinal pathology. \nGastroenterology & Endoscopy; 2023.\n15. Brennan G. Using ChatGPT to write pathology results letters. @ Gijournal, \n2023. 3.\n16. Sorin V, et al. Large language model (ChatGPT) as a support tool for breast \ntumor board. NPJ Breast Cancer. 2023;9(1):44.\n17. Anand D et al. One-shot Localization and Segmentation of Medical Images \nwith Foundation Models arXiv preprint arXiv:2310.18642, 2023.\n18. Tian D et al. The role of large language models in medical image processing: \na narrative review. Quantitative Imaging in Medicine and Surgery.\n19. Wang S et al. Adapting pre-trained visual and language models for medical \nimage question answering. in CLEF2023 Working Notes, CEUR Workshop \nProceedings, CEUR-WS. org, Thessaloniki, Greece. 2023.\n20. Li C et al. Llava-med: Training a large language-and-vision assistant for bio-\nmedicine in one day arXiv preprint arXiv:2306.00890, 2023.\n21. Ikezogwo WO et al. Quilt-1 M: One million image-text pairs for histopathol-\nogy arXiv preprint arXiv:2306.11207, 2023.\n22. Li H et al. Long-MIL: scaling long contextual multiple instance learning for \nhistopathology whole slide image analysis. arXiv preprint arXiv:2311.12885, \n2023.\n23. Li Y et al. A comprehensive study of GPT-4V’s multimodal capabilities in medi-\ncal imaging medRxiv, 2023: p. 2023.11. 03.23298067.\n24. Nakao T et al. Capability of GPT-4V (ision) in Japanese national medical licens-\ning examination medRxiv, 2023: p. 2023.11. 07.23298133.\n25. Bosbach WA et al. Ability of ChatGPT to generate competent radiology \nreports for distal radius fracture by use of RSNA template items and inte-\ngrated AO classifier Current problems in diagnostic radiology, 2023.\n26. Koga S, Martin NB, Dickson DW. Evaluating the ability of ChatGPT in generat-\ning differential diagnosis in clinicopathological conferences of neurodegen-\nerative disorders 2023.\n27. Hu M et al. Advancing medical imaging with language models: A journey \nfrom n-grams to chatgpt arXiv preprint arXiv:2304.04920, 2023.\n28. Wang S, Mirza F, Baig MM. A predictive model for identifying health trends \namong Māori and Pacific people-analysis from ten years of New Zealand \nPublic Hospital discharges. Int J Med Eng Inf. 2021;13(3):190–9.\n29. Gamini S, Kumar SS. Homomorphic filtering for the image enhancement \nbased on fractional-order derivative and genetic algorithm. Comput Electr \nEng. 2023;106:108566.\n30. Wang M, et al. Medical machine learning based on multiobjective \nevolutionary algorithm using learning decomposition. Expert Syst Appl. \n2023;216:119450.\n31. Parray AA, et al. ChatGPT and global public health: applications, challenges, \nethical considerations and mitigation strategies. Elsevier; 2023.\n32. Biswas SS. Role of chat gpt in public health. Ann Biomed Eng. \n2023;51(5):868–9.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations. "
}