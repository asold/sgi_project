{
  "title": "Knowledgeable or Educated Guess? Revisiting Language Models as Knowledge Bases",
  "url": "https://openalex.org/W3173673636",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3169498763",
      "name": "Boxi Cao",
      "affiliations": [
        "Mylan (Switzerland)",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098375810",
      "name": "Hongyu Lin",
      "affiliations": [
        "Mylan (Switzerland)"
      ]
    },
    {
      "id": "https://openalex.org/A2171485312",
      "name": "Xianpei Han",
      "affiliations": [
        "Institute of Software",
        "Mylan (Switzerland)",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097220636",
      "name": "Le Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2971111791",
      "name": "Lingyong Yan",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Mylan (Switzerland)"
      ]
    },
    {
      "id": "https://openalex.org/A2145615484",
      "name": "Meng Liao",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1987282521",
      "name": "Tong Xue",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2008268640",
      "name": "Jin Xu",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W3020987135",
    "https://openalex.org/W3116459227",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W4288631803",
    "https://openalex.org/W3093871960",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W4288265479",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W3035290244",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W3119438769",
    "https://openalex.org/W3102839769",
    "https://openalex.org/W2785611959",
    "https://openalex.org/W3105066976",
    "https://openalex.org/W2017205387",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2130237711",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3118741274",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3033176962",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W3049346316",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2991265431",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3101204082",
    "https://openalex.org/W3100283070"
  ],
  "abstract": "Boxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingyong Yan, Meng Liao, Tong Xue, Jin Xu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1860–1874\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1860\nKnowledgeable or Educated Guess? Revisiting Language Models as\nKnowledge Bases\nBoxi Cao1,3, Hongyu Lin1∗, Xianpei Han1,2∗, Le Sun1,2\nLingyong Yan1,3, Meng Liao4, Tong Xue4, Jin Xu4\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n4Data Quality Team, WeChat, Tencent Inc., China\n{boxi2020,hongyu,xianpei,sunle,lingyong2014}@iscas.ac.cn\n{maricoliao,xavierxue,jinxxu}@tencent.com\nAbstract\nPrevious literatures show that pre-trained\nmasked language models (MLMs) such as\nBERT can achieve competitive factual knowl-\nedge extraction performance on some datasets,\nindicating that MLMs can potentially be a reli-\nable knowledge source. In this paper, we con-\nduct a rigorous study to explore the underly-\ning predicting mechanisms of MLMs over dif-\nferent extraction paradigms. By investigating\nthe behaviors of MLMs, we ﬁnd that previous\ndecent performance mainly owes to the biased\nprompts which overﬁt dataset artifacts. Fur-\nthermore, incorporating illustrative cases and\nexternal contexts improve knowledge predic-\ntion mainly due to entity type guidance and\ngolden answer leakage. Our ﬁndings shed\nlight on the underlying predicting mechanisms\nof MLMs, and strongly question the previous\nconclusion that current MLMs can potentially\nserve as reliable factual knowledge bases1.\n1 Introduction\nRecently, pre-trained language models (Peters et al.,\n2018; Devlin et al., 2019; Brown et al., 2020) have\nachieved promising performance on many NLP\ntasks. Apart from utilizing the universal representa-\ntions from pre-trained models in downstream tasks,\nsome literatures have shown the potential of pre-\ntrained masked language models (e.g., BERT (De-\nvlin et al., 2019) and RoBERTa (Liu et al., 2019b))\nto be factual knowledge bases (Petroni et al., 2019;\nBouraoui et al., 2020; Jiang et al., 2020b; Shin et al.,\n2020; Jiang et al., 2020a; Wang et al., 2020; Kass-\nner and Sch¨utze, 2020a; Kassner et al., 2020). For\nexample, to extract the birthplace of Steve Jobs, we\ncan query MLMs like BERT with “Steve Jobs was\nborn in [MASK]”, where Steve Jobs is the subject\n∗Corresponding Authors\n1We openly release the source code and data at https:\n//github.com/c-box/LANKA\nPromptBias“wasbornin”withoutXpredicts <?>\nTypeGuidance<?>willhavethesametypeasB\nAnswerLeakageContexthelpsifitleaks<?>\nMechanism Prompt-basedXwasbornin<?>.Case-basedAwasborninB.Xwasbornin<?>.Context-basedXlivesinY.Xwasbornin<?>.\nParadigm \nFigure 1: This paper explores three different kinds of\nfactual knowledge extraction paradigms from MLMs,\nand reveal the underlying predicting mechanisms be-\nhind them.\nof the fact, “was born in” is a prompt string for the\nrelation “place-of-birth” and [MASK] is a\nplaceholder for the object to predict. Then MLMs\nare expected to predict the correct answer “Califor-\nnia” at the [MASK] position based on its internal\nknowledge. To help MLMs better extract knowl-\nedge, the query may also be enriched with external\ninformation like illustrative cases ( e.g., (Obama,\nHawaii)) (Brown et al., 2020) or external context\n(e.g., Jobs lives in California) (Petroni et al., 2020).\nSome literatures have shown that such paradigms\ncan achieve decent performance on some bench-\nmarks like LAMA (Petroni et al., 2019).\nDespite some reported success, currently there\nis no rigorous study looking deeply into the un-\nderlying mechanisms behind these achievements.\nBesides, it is also unclear whether such achieve-\nments depend on certain conditions (e.g., datasets,\ndomains, relations). The absence of such kind of\nstudies undermines our trust in the predictions of\nMLMs. We could neither determine whether the\npredictions are reliable nor explain why MLMs\nmake a speciﬁc prediction, and therefore signiﬁ-\ncantly limits MLMs’ further applications and im-\nprovements.\n1861\nTo this end, this paper conducts a thorough study\non whether MLMs could be reliable factual knowl-\nedge bases. Throughout our investigations, we\nanalyze the behaviors of MLMs, ﬁgure out the\ncritical factors for MLMs to achieve decent per-\nformance, and demonstrate how different kinds of\nexternal information inﬂuence MLMs’ predictions.\nSpeciﬁcally, we investigate factual knowledge ex-\ntraction from MLMs2 over three representative fac-\ntual knowledge extraction paradigms, as shown in\nFigure 1:\n•Prompt-based retrieval(Petroni et al., 2019;\nJiang et al., 2020b; Shin et al., 2020), which\nqueries MLM for object answer only given the\nsubject and the corresponding relation prompt\nas input, e.g., “Jobs was born in [MASK]. ”\n•Case-based analogy (Brown et al., 2020;\nMadotto et al., 2020; Gao et al., 2020), which\nenhances the prompt-based retrieval with sev-\neral illustrative cases, e.g., “Obama was born\nin Hawaii. [SEP] Jobs was born in [MASK]. ”\n•Context-based inference (Petroni et al.,\n2020; Bian et al., 2021), which augments\nthe prompt-based retrieval with external rele-\nvant contexts, e.g., “Jobs lives in California.\n[SEP] Jobs was born in [MASK]. ”\nSurprisingly, the main conclusions of this pa-\nper somewhat diverge from previous ﬁndings in\npublished literatures, which are summarized in Fig-\nure 1. For prompt-based paradigm ( §3), we ﬁnd\nthat the prediction distribution of MLMs is signiﬁ-\ncantly prompt-biased. Speciﬁcally, we ﬁnd that\nprompt-based retrieval generates similar predic-\ntions on totally different datasets. And predictions\nare spuriously correlated with the applied prompts,\nrather than the facts we want to extract. Therefore,\nprevious decent performance mainly stems from\nthe prompt over-ﬁtting the dataset answer distri-\nbution, rather than MLMs’ knowledge extraction\nability. Our ﬁndings strongly question the conclu-\nsions of previous literatures, and demonstrate that\ncurrent MLMs can not serve as reliable knowledge\nbases when using prompt-based retrieval paradigm.\n2This paper shows the experimental results on BERT-large\nbecause previous work has shown that it can achieve the\nbest performance on factual knowledge extraction among all\nMLMs. In the Appendix, we also report the experimental\nresults on RoBERTa-large, which also reach the main conclu-\nsions reported in the paper.\nFor case-based paradigm (§4), we ﬁnd that the\nillustrative cases mainly provide a “type guidance”\nfor MLMs. To show this, we propose a novel al-\ngorithm to induce the object type of each relation\nbased on Wikidata3 taxonomy. According to the\ninduced types, we ﬁnd that the performance gain\nbrought by illustrative cases mainly owes to the\nimprovement on recognizing object type. By con-\ntrast, it cannot help MLMs select the correct answer\nfrom the entities with the same type: the rank of\nanswer within its entity type is changed randomly\nafter introducing illustrative cases. That is to say,\nunder the case-based paradigm, although MLMs\ncan effectively analogize between entities with the\nsame type, they still cannot well identify the exact\ntarget object based on their internal knowledge and\nthe provided illustrative cases.\nFor context-based paradigm (§5), we ﬁnd that\ncontext can help the factual knowledge extraction\nmainly because it explicitly or implicitly leaks the\ncorrect answer. Speciﬁcally, the knowledge ex-\ntraction performance improvement mainly happens\nwhen the introduced context contains the answer.\nFurthermore, when we mask the answer in the con-\ntext, the performance still signiﬁcantly improves as\nlong as MLMs can correctly reconstruct the masked\nanswer in the remaining context. In other words, in\nthese instances, the context itself servers as a dele-\ngator of the masked answer, and therefore MLMs\ncan still obtain sufﬁcient implicit answer evidence\neven the answer doesn’t explicitly appear.\nAll the above ﬁndings demonstrate that current\nMLMs are not reliable in factual knowledge extrac-\ntion. Furthermore, this paper sheds some light on\nthe underlying predicting mechanisms of MLMs,\nwhich can potentially beneﬁt many future studies.\n2 Related Work\nThe great success of Pre-trained Language Models\n(PLMs) raises the question of whether PLMs can be\ndirectly used as reliable knowledge bases. Petroni\net al. (2019) propose the LAMA benchmark, which\nprobes knowledge in PLMs using prompt-based\nretrieval. Jiang et al. (2020a) build a multilingual\nknowledge probing benchmark based on LAMA.\nThere are many studies focus on probing speciﬁc\nknowledge in PLMs, such as linguistic knowl-\nedge (Lin et al., 2019; Tenney et al., 2019; Liu\net al., 2019a; Htut et al., 2019; Hewitt and Man-\nning, 2019; Goldberg, 2019; Warstadt et al., 2019),\n3www.wikidata.org\n1862\nsemantic knowledge (Tenney et al., 2019; Wal-\nlace et al., 2019; Ettinger, 2020) and world knowl-\nedge (Davison et al., 2019; Bouraoui et al., 2020;\nForbes et al., 2019; Zhou et al., 2019; Roberts et al.,\n2020; Lin et al., 2020; Tamborrino et al., 2020). Re-\ncently, some studies doubt the reliability of PLMs\nas knowledge base by discovering the the spurious\ncorrelation to surface forms (McCoy et al., 2019;\nPoerner et al., 2020; Shwartz et al., 2020), and their\nsensitivity to “negation” and “mispriming” (Kass-\nner and Sch¨utze, 2020b).\nCurrently, there are three main paradigms for\nknowledge extraction from PLMs: prompt-based\nretrieval (Schick and Sch¨utze, 2021; Li and Liang,\n2021), case-based analogy (Schick and Sch ¨utze,\n2020a,b), and context-based inference. For prompt-\nbased retrieval, current studies focus on seeking\nbetter prompts by either mining from corpus (Jiang\net al., 2020b) or learning using labeled data (Shin\net al., 2020). For case-based analogy, current stud-\nies mostly focus on whether good cases will lead\nto good few-shot abilities, and many tasks are\ntried (Brown et al., 2020; Madotto et al., 2020;\nGao et al., 2020). For context-based inference, cur-\nrent studies focus on enhancing the prediction by\nseeking more informative contexts, e.g., for knowl-\nedge extraction (Petroni et al., 2020) and Com-\nmonsenseQA (Bian et al., 2021). However, there\nis no previous work which focuses on systemati-\ncally study the underlying predicting mechanisms\nof MLMs on these paradigms.\n3 Prompt-based Retrieval\nThe prompt-based retrieval extracts factual\nknowledge by querying MLMs with (subject,\nprompt, [MASK]). For example, to extract the\n“place-of-birth” of Steve Jobs , we could\nquery BERT with “ Steve Jobs was born in\n[MASK].” and the predicted “ California” would\nbe regarded as the answer. We consider three kinds\nof prompts: the manually prompts Tman created\nby Petroni et al. (2019), the mining-based prompts\nTmine by Jiang et al. (2020b) and the automatically\nsearched prompts Tauto from Shin et al. (2020).\n3.1 Overall Conclusion\nConclusion 1. Prompt-based retrieval is prompt-\nbiased. As a result, previous decent performance\nactually measures how well the applied prompts\nﬁt the dataset answer distribution, rather than the\nfactual knowledge extraction ability from MLMs.\nLondon Paris Rome Tokyo Boston ChicagoMontrealBerlin Milan Moscow\nLAMA\nWIKI-UNI 0.1\n0.2\n(a) The true answer distributions are very different between\nLAMA and WIKI-UNI.\nLondon Paris Rome Tokyo Boston ChicagoMontrealBerlin Milan Moscow\nLAMA\nWIKI-UNI\n0.2\n0.4\n(b) However, the prediction distribution made by MLMs on\nthem are still very similar.\nFigure 2: An illustration example of the vastly different\nanswer distributions but similar prediction distributions\non LAMA and WIKI-UNI on “ place-of-birth”\nrelation.\nSpeciﬁcally, we conduct studies and ﬁnd that\n1) Prompt-based retrieval will generate similar re-\nsponses given quite different datasets. To show this,\nwe construct a new dataset from Wikidata – WIKI-\nUNI, which have a totally different answer distribu-\ntion from the widely-used LAMA4 dataset (Petroni\net al., 2019). However, we ﬁnd that the predic-\ntion distributions on WIKI-UNI and LAMA are\nhighly correlated, and this spurious correlation\nholds across different prompts. Such results re-\nveal that there is just a weak correlation between\nthe predictions of MLMs and the factual answer\ndistribution of the dataset. 2) The prediction dis-\ntribution is dominated by the prompt, i.e., the pre-\ndiction distribution using only (prompt, [MASK])\nis highly correlated to the prediction distribution\nusing (subject, prompt, [MASK]). This indicates\nthat it is the applied prompts, rather than the ac-\ntual facts, determine the predictions of MLMs. 3)\nThe performance of the prompt can be predicted\nby the divergence between the prompt-only distri-\nbution and the answer distribution of the dataset.\nAll these ﬁndings reveal that previous decent per-\nformance in this ﬁeld actually measures the degree\nof prompt-dataset ﬁtness, rather than the universal\nfactual knowledge extraction ability.\n3.2 Different Answers, Similar Predictions\nFinding 1. Prompt-based retrieval will generate\nsimilar responses to quite different datasets.\nA reliable knowledge extractor should generate\n4Since we focus on factual knowledge, we use the T-\nREx (Elsahar et al., 2018) subset of the LAMA benchmark.\n1863\n0.2 0.4 0.6 0.8 1.0\nTman\nTmine\nTauto\nFigure 3: Correlations of the prediction distributions on\nLAMA and WIKI-UNI. Even these two datasets have\ntotally different answer distributions, MLMs still make\nhighly correlated predictions.\nDistributionDatasets Top1 Top3 Top5Precision\nAnswer LAMA 22.04 39.37 48.03 -\nWIKI-UNI1.68 5.03 7.78 -\nPrediction LAMA 31.09 49.21 57.93 30.36\nWIKI-UNI27.12 44.19 52.18 16.47\nTable 1: Average percentage of instances being cov-\nered by top-k answers or predictions. For answer dis-\ntribution, top-5 objects in LAMA cover 6.2 times of\ninstances than that in WIKI-UNI, however, for predic-\ntion distribution, they are almost the same. As a result,\nthe precision is signiﬁcantly dropped in WIKI-UNI.\ndifferent responses to different knowledge queries.\nTo verify whether MLMs meet this standard, we\nmanually construct a new dataset – WIKI-UNI,\nwhich has a comparable size but totally different\nanswer distribution to LAMA, and then compare\nthe prediction distributions on them. For a fair\ncomparison, we follow the construction criteria of\nLAMA: we use the same 41 relations, ﬁlter out\nthe queries whose objects are not in the MLMs’\nvocabulary. Compared with LAMA, the major dif-\nference is that WIKI-UNI has a uniform answer\ndistribution, i.e., for each relation, we keep the\nsame number of instances for each object. Please\nrefer to Appendix for more construction details.\nFigure 2a shows the answer distributions of LAMA\nand WIKI-UNI on relation “place-of-birth”.\nWe can see that the answers in LAMA are highly\nconcentrated on the head object entities, while the\nanswers in WIKI-UNI follow a uniform distribu-\ntion.\nGiven LAMA and WIKI-UNI, we investigate\nthe predicting behaviors of MLMs. Surprisingly,\nthe prediction distributions on these two totally\ndifferent datasets are highly correlated. Figure 2b\nshows an example. We can see that the prediction\ndistribution on WIKI-UNI is very similar to that on\nLAMA. And these two distributions are both close\nto the answer distribution of LAMA but far away\nfrom the answer distribution of WIKI-UNI.\nTo investigate whether this spurious correlation\n0.0 0.2 0.4 0.6 0.8 1.0\nTman\nTmine\nTauto\nFigure 4: Correlations between the prompt-only dis-\ntribution and prediction distribution on WIKI-UNI.\nMLMs make correlated predictions w. or w/o. subjects.\nis a common phenomenon, we analyze the Pearson\ncorrelation coefﬁcient between prediction distribu-\ntions on LAMA and WIKI-UNI across different\nrelations and three kinds of prompts. The boxplot\nin Figure 3 shows the very signiﬁcant correlation\nbetween the prediction distributions on LAMA and\nWIKI-UNI: on all three kinds of prompts, the cor-\nrelation coefﬁcients exceed 0.8 in more than half\nof relations. These results demonstrate that prompt-\nbased retrieval will lead to very similar prediction\ndistributions even when test sets have vastly differ-\nent answer distributions.\nFurthermore, we ﬁnd that the prediction distri-\nbution obviously doesn’t correspond to the answer\ndistribution of WIKI-UNI. From Table 1, we can\nsee that on average, the top-5 answers of each rela-\ntion in WIKI-UNI cover only 7.78% instances. By\ncontrast, the top-5 predictions of each relation in\nWIKI-UNI cover more than 52% instances, which\nis close to the answer distribution and prediction\ndistribution on LAMA. As a result, the perfor-\nmance on WIKI-UNI (mean P@1: 16.47) is sig-\nniﬁcantly worse than that on LAMA (mean P@1:\n30.36). In conclusion, the facts of a dataset cannot\nexplain the predictions of MLMs, and therefore pre-\nvious evaluations of the MLMs’ ability on factual\nknowledge extraction are unreliable.\n3.3 Prompts Dominates Predictions\nFinding 2. The prediction distribution is severely\nprompt-biased.\nTo investigate the underlying factors of the pre-\ndicting behavior of MLMs, we compare the prompt-\nonly prediction distribution using only (prompt,\n[MASK]) and the full prediction distribution using\n(subject, prompt, [MASK]). To obtain the prompt-\nonly distribution, we mask the subject and then use\n([MASK], prompt, [MASK]) to query MLMs (e.g.,\n[MASK] was born in [MASK]). Because there is no\nsubject information in the input, MLMs can only\ndepend on applied prompt’s information to make\n1864\nthe prediction at the second [MASK]. Therefore,\nwe regard the probability distribution at the second\n[MASK] symbol as the prompt-only distribution.\nAfter that, we analyze the correlations between\nthe prompt-only distribution and the prediction dis-\ntribution on WIKI-UNI dataset. Figure 4 shows\nthe boxplot. On all three kinds of prompts, correla-\ntion coefﬁcients between the prompt-only distribu-\ntion and the prediction distribution on WIKI-UNI\nexceed 0.6 in more than half of relations. This\ndemonstrates that in these relations, the prompt-\nonly distribution dominates the prediction distribu-\ntion. Combining with the ﬁndings in Section 3.2,\nwe can summarize that the prompt-based retrieval\nis mainly based on guided guessing, i.e., the predic-\ntions are generated by sampling from the prompt-\nbiased distribution guided by the moderate impact\nof subjects.\nNote that among a minor part of relations, the\ncorrelations between the prompt-only distribution\nand the prediction distribution are relatively low.\nWe ﬁnd that the main reason is the type selectional\npreference provided by the subject entities, and\nSection 4 will further discuss the impact of this\ntype-guidance mechanism for MLMs.\n3.4 Better Prompts are Over-Fitting\nFinding 3. “Better” prompts are the prompts\nﬁtting the answer distribution better, rather than\nthe prompts with better retrieval ability.\nSome previous literatures attempt to ﬁnd bet-\nter prompts for factual knowledge extraction from\nMLMs. However, as we mentioned above, the\nprompt itself will lead to a biased prediction dis-\ntribution. This raises our concern that whether the\nfound better prompts are really with better knowl-\nedge extraction ability, or the better performance\njust come from the over-ﬁtting between the prompt-\nonly distribution and the answer distribution of the\ntest set.\nTo answer this question, we evaluate the KL\ndivergence between the prompt-only distribution\nand the answer distribution of LAMA on different\nkinds of prompts. The results are shown in Ta-\nble 2. We ﬁnd that the KL divergence is a strong\nindicator of the performance of a prompt, i.e., the\nsmaller the KL divergence between the prompt-\nonly distribution and the answer distribution of\nthe test set is, the better performance the prompt\nachieve. Furthermore, Table 3 shows several com-\nparisons between different kinds of prompts and\nPrompt Precision KL divergence\nTman 30.36 12.27\nTmine 39.49 10.40\nTauto 40.36 10.27\nTable 2: The smaller KL divergence between the\nprompt-only distribution and golden answer distribu-\ntion of LAMA, the better performance of the prompt.\nRelation Prompt Source Prec. KL.\ncitizenshipxisycitizen Tman 0.00 24.67\nxreturned toy T mine 43.58 6.32\nwork locationxused to work iny Tman 11.01 19.07\nxwas born iny T mine 40.25 2.21\ninstance ofxis ay T man 30.15 22.98\nxis a smally T mine 52.60 13.98\nTable 3: Examples of prompts that can achieve signiﬁ-\ncant improvements on LAMA. We can see that the bet-\nter performance actually stems from over-ﬁtting: the\nbetter prompts are not prompts with a stronger seman-\ntic association to the relation.\ntheir performance on LAMA. We can easily ob-\nserve that the better-performed prompts are actually\nover-ﬁtting the dataset, rather than better capturing\nthe underlying semantic of the relation. As a re-\nsult, previous prompt searching studies are actually\noptimized on the spurious prompt-dataset compati-\nbility, rather than the universal factual knowledge\nextraction ability.\n4 Case-based Analogy\nThe case-based analogy enhances the prompt-based\nparadigm with several illustrative cases. For exam-\nple, if we want to know the “place-of-birth”\nof Steve Jobs, we would ﬁrst sample cases such as\n(Obama, place-of-birth, Hawaii), and com-\nbine them with the original query. In this way, we\nwill use “Obama was born in Hawaii. [SEP] Steve\nJobs was born in [MASK].” to query MLMs.\n4.1 Overall Conclusion\nConclusion 2. Illustrative cases guide MLMs to\nbetter recognizing object type, rather than better\npredicting facts.\nTo show this, we ﬁrst design an effective algo-\nrithm to induce the type of an entity set based on\nWikidata taxonomy, which can identify the object\ntype of a relation. According to the induced types,\nwe ﬁnd that the beneﬁts of illustrative cases mainly\nstem from the promotion of object type recognition.\nIn other words, case-based analogy guides MLMs\nwith better type prediction ability but contributes\n1865\nLondonChicagoCapital1\nMilanBigCity2 City3 Area3\nArea1.0City1.0\nEntitySet\nEntityTypeSequence\nEntityTypeGraph\nBigCity0.6Capital0.3\nFigure 5: Illustration of our type induction algorithm.\nThe numbers on the right of each type indicate how\nmany entities does the type cover. The type of an en-\ntity set is the ﬁnest grained type in the type graph that\ncan cover a sufﬁcient number of the instances in the\nentity set, which is City in the example.\nlittle to the entity prediction ability. In the follow-\ning, we ﬁrst illustrate our type inducing algorithm,\nand then explain how we reach the conclusion.\n4.2 Entity Set Type Induction\nTo induce the object type of a relation, we ﬁrst\ncollect all its objects in LAMA and form an entity\nset. Then we induce the type of an entity set by\ndesigning a simple but effective algorithm. The\nmain intuition behind our algorithm is that a rep-\nresentative type should be the ﬁnest grained type\nthat can cover a sufﬁcient number of the instances\nin the entity set. Figure 5 shows an example of our\nalgorithm. Given a set of entities in Wikidata, we\nﬁrst construct an entity type graph (ETG) by recur-\nsively introducing all ancestor entity types accord-\ning to the instance-of and subclass-of\nrelations. For the example in Figure 5, Chicago\nis in the entity set and is an instance-of Big\nCity. Big City is a subclass-of City. As a\nresult, Chicago, Big City and City will all be intro-\nduced into ETG. Then we apply topological sorting\n(Cook, 1985) to ETG to obtain aFine-to-Coarse en-\ntity type sequence. Finally, based on the sequence,\nwe select the ﬁrst type which covers more than 80%\nof entities in the entity set (e.g., City in Figure 5).\nTable 4 illustrates several induced types, and please\nrefer to the Appendix for details.\n4.3 Cases Help Type Recognition\nFinding 4. Illustrative cases help MLMs to better\nrecognize the type of objects, and therefore improve\nfactual knowledge extraction.\nFor case-based analogy, the ﬁrst thing we want\nto know is whether illustrative cases can improve\nthe knowledge extraction performance. To this end,\nfor each (subject, relation) query in LAMA, we\n25%30%35%40%45%\nIn-typeRank\nOverallRank\nRaisedUnchangedDropped\nFigure 6: Percentages on the change of overall rank\n(among all candidates) and the in-type rank (among\ncandidates with the same type) of golden answer. We\ncan see that the illustrative cases mainly raise the over-\nall rank but cannot raise the in-type rank, which means\nthe performance improvements mainly come from bet-\nter type recognition.\nrandomly sample 10 illustrative cases. To avoid\nanswer leakage, we ensure the objects of these\ncases don’t contain the golden answer of the query.\nThen we use (cases, subject, prompt, [MASK]) as\nthe analogous query to MLMs.\nResults show that case-based analogy can signif-\nicantly improve performance. After introducing il-\nlustrative cases, the mean precision increases from\n30.36% to 36.23%. Besides, we ﬁnd that 11.81%\ninstances can beneﬁt from the introduced cases and\nonly 5.94% instances are undermined. This shows\nthat case-based analogy really helps the MLMs to\nmake better predictions.\nBy analyzing the predicting behaviors, we ob-\nserve that the main beneﬁt of introducing illus-\ntrative cases comes from the better type recogni-\ntion. To verify this observation, we investigate\nhow the types of predictions changed after intro-\nducing the illustrative cases. Table 4 shows the re-\nsults on relations whose precision improvement is\nmore than 10% after introducing illustrative cases.\nFrom the table, it is very obvious that illustrative\ncases enhance the factual knowledge extraction by\nimproving type prediction: 1) For queries whose\npredictions are correctly reversed (from wrong to\nright), the vast majority of them stems from the\nrevised type prediction; 2) Even for queries whose\npredictions are mistakenly reversed (from right to\nwrong), the type of the majority of predictions still\nremains correct. In conclusion, introducing illustra-\ntive cases can signiﬁcantly improve the knowledge\nextraction ability by recognizing the object type\nmore accurately. That is, adding illustrative cases\nwill provide more guidance for object type.\n1866\nRelation Induced Object TypePrecision\n∆\nType\nPrec.∆\nWrong→Right\nw/ Type Change\nRight→Wrong\nw/o Type Change\ncountry of citizenshipsovereign state 43.37 84.16 100.00 -\nposition held religious servant 36.88 80.26 91.15 90.00\nreligion religion 33.20 34.88 100.00 -\nwork location city 26.10 70.55 85.04 100.00\ninstrument musical instrument 17.07 55.75 89.08 75.00\ncountry sovereign state 14.30 29.04 88.48 87.93\nemployer business 12.01 99.22 100.00 -\ncontinent continent 10.87 51.18 96.86 88.24\nTable 4: Detailed analysis on relations where the mean precision increased more than 10%. Precision ∆ and Type\nPrec. ∆ represents the precision changes on the answer and the type of the answer respectively. “w/ Type Change”\nand “w/o Type Change” represents the type of prediction changed/unchanged before/after introducing illustrative\ncases. “-” indicate there is no queries whose predictions are mistakenly reversed.\n4.4 Cases do not Help Entity Prediction\nFinding 5. Illustrative cases are of limited help\nfor selecting the answer from entities of the same\ntype.\nTo show this, we introduce a new metric referred\nas in-type rank, which is the rank of the correct an-\nswer within the entities of the same type for a query.\nBy comparing the in-type rank in prompt-based\nand case-based paradigm, we can evaluate whether\nthe illustrative cases can actually help better entity\nprediction apart from better type recognition.\nFigure 6 shows the percentages on the change\nof overall rank (among all candidates) and the\nin-type rank (among candidates with the same\ntype) of golden answer. Unfortunately, we ﬁnd\nthat illustrative cases are of limited help for en-\ntity prediction: the change of in-type rank is\nnearly random. The percentages of queries with\nRaised/Unchanged/Dropped in-type rank are nearly\nthe same: 33.05% VS 35.47% VS 31.47%. Fur-\nthermore, we ﬁnd that the MRR with the type only\nchanged from 0.491 to 0.494, which shows little\nimprovement after introducing illustrative cases.\nThese results show that the raises of overall rank of\ngolden answer are not because of the better predic-\ntion inside the same type. In conclusion, illustrative\ncases cannot well guide the entity prediction, and\nthey mainly beneﬁt the factual knowledge extrac-\ntion by providing guidance for object type recogni-\ntion.\n5 Context-based Inference\nThe context-based inference augments the prompt-\nbased paradigm with external contexts. For exam-\nple, if we want to know the “place-of-birth”\nof Steve Jobs, we can use the external context “Jobs\nwas from California.”, and form a context-enriched\nAnswer\nin contextPrompt-based Context-based∆\nPresent\n(45.30%) 34.83 64.13 +29.30\nAbsent\n(54.70 %) 25.37 23.26 -2.11\nTable 5: Comparison between prompt-based and\ncontext-based paradigms grouped by whether the an-\nswer presents or absents in the context. We can see that\nonly contexts containing the answer can signiﬁcantly\nimprove the performance.\nquery “Jobs was from California. [SEP] Steve Jobs\nwas born in [MASK]. ” to query MLMs. Specif-\nically, we use the same context retrieval method\nas Petroni et al. (2020): for each instance, given\nthe subject and relation as query, we use the ﬁrst\nparagraph of DRQA’s (Chen et al., 2017) retrieved\ndocument as external contexts.\n5.1 Overall Conclusion\nConclusion 3.Additional context helps MLMs to\npredict the answer because they contain the answer,\nexplicitly or implicitly.\nSeveral studies (Petroni et al., 2020; Bian et al.,\n2021) show that external context can help knowl-\nedge extraction from MLMs. To investigate the\nunderlying mechanism, we evaluate which kinds\nof information in contexts contribute to the fact\nprediction, and ﬁnd that the improvement mainly\ncomes from the answer leakage in context. Further-\nmore, we ﬁnd the answers can not only be leaked\nexplicitly, but can also be leaked implicitly if the\ncontext provides sufﬁcient information.\n5.2 Explicit Answer Leakage Helps\nFinding 6. Explicit answer leakage signiﬁcantly\nimproves the prediction performance.\nTo show this, we split LAMA into two parts ac-\n1867\nPrompt-based Context-based Masked Context-based\n30.36 41.44 35.66\nTable 6: Overall performance when introducing differ-\nent kinds of contexts. “Masked Context-based” indi-\ncates that we mask the golden answer in contexts, and\nthere is still a signiﬁcant performance improvement.\nAnswer\nReconstructablePrompt-based Context-based∆\nReconstructable\n(60.23%) 39.58 60.82 +21.24\nNot-reconstructable\n(39.77 %) 28.84 35.83 +6.99\nTable 7: Comparison between prompt-based and\ncontext-based paradigms grouped by whether the\nmasked answer in the context can be reconstructed\nfrom the remaining context. We can see that contexts\ncan reconstruct the masked answer is more likely to im-\nprove the performance.\ncording to whether the additional context contains\nthe answer. Table 5 shows the results on these two\nparts respectively. We can see that the improve-\nments on these two parts diverge signiﬁcantly. For\ncontext containing the answer, context-based infer-\nence signiﬁcantly improves the factual knowledge\nextraction performance. However, there is even a\nlittle performance drop for those instances whose\ncontext does not contain the answer. This indicates\nthat the improvement of factual knowledge extrac-\ntion is mainly due to the explicit existence of the\nanswer in the context.\n5.3 Implicit Answer Leakage Helps\nFinding 7. Implicit answer leakage can also\nsigniﬁcantly improve the prediction performance.\nAs we mentioned above, explicit answer leak-\nage signiﬁcantly helps the answer prediction. The\nanswer-leaked context may explicitly provide the\nanswer or implicitly guide the prediction by provid-\ning answer-speciﬁc information. To understanding\nthe underlying mechanism, we mask the answer in\nthe context and verify whether it can still achieve\nthe performance gain.\nTable 6 shows the results. We ﬁnd that the per-\nformance gain is still very signiﬁcant after mask-\ning the answer. This indicates that the contexts\npreviously containing the answer are still very ef-\nfective even the answer doesn’t explicitly present.\nTo further investigate the reason behind, we split\nthe masked version of answer-leaked instances into\ntwo groups by whether MLMs can or cannot cor-\nrectly reconstruct the masked answer from the re-\nmaining context. The results are shown in Table 7.\nWe can see that the performance gain signiﬁcantly\ndiverges in these two groups: the improvements\nmainly come from the instances whose answer in\ncontext can be reconstructed – we refer to this as\nimplicit answer leakage. That is to say, for these\ninstances, the context serves as a sufﬁcient delega-\ntor of its answer, and therefore MLMs can obtain\nsufﬁcient answer evidence even the answer does\nnot explicitly appear. However, for contexts that\ncannot reconstruct the masked answer, the improve-\nments are relatively minor. In conclusion, the real\nefﬁcacy of context-based inference comes from the\nsufﬁcient answer evidence provided by the context,\neither explicitly or implicitly.\n6 Conclusions and Discussions\nIn this paper, we thoroughly study the underly-\ning mechanisms of MLMs on three representative\nfactual knowledge extraction paradigms. We ﬁnd\nthat the prompt-based retrieval is severely prompt-\nbiased, illustrative cases enhance MLMs mainly via\ntype guidance, and external contexts help knowl-\nedge prediction mostly because they contain the\ncorrect answer, explicitly or implicitly. These\nﬁndings strongly question previous conclusions\nthat current MLMs could serve as reliable factual\nknowledge bases.\nThe ﬁndings of this paper can beneﬁt the commu-\nnity in many directions. By explaining the underly-\ning predicting mechanisms of MLMs, we provide\nreliable explanations for many previous knowledge-\nintensive techniques. For example, our method can\nexplain why and how incorporating external con-\ntexts will help knowledge extraction and Common-\nsenseQA (Talmor et al., 2019). Our ﬁndings also\nreveal why PLM probing datasets may not be re-\nliable and how the evaluation can be promoted by\ndesigning de-biased evaluation datasets.\nThis paper also sheds light on future research\ndirections. For instance, knowing the main bene-\nﬁt of illustrative cases comes from type-guidance,\nwe can enhance many type-centric prediction tasks\nsuch as NER (Lample et al., 2016) and factoid\nQA (Iyyer et al., 2014). Moreover, based on the\nmechanism of incorporating external contexts, we\ncan better evaluate, seek, and denoise external con-\ntexts for different tasks using MLMs. For exam-\nple, we can assess and select appropriate facts for\nCommonsenseQA based on whether they can re-\nconstruct the candidate answers.\n1868\nThis paper focuses on masked language mod-\nels, which have been shown very effective and are\nwidely used. We also want to investigate another\nrepresentative category of language models – the\ngenerative pre-trained models (e.g., GPT2/3 (Rad-\nford et al., 2019; Brown et al., 2020)), which have\nbeen shown to have quite different mechanisms and\nwe leave it for future work due to page limitation.\nAcknowledgments\nWe sincerely thank all anonymous reviewers for\ntheir insightful comments and valuable suggestions.\nThis work is supported by the National Key Re-\nsearch and Development Program of China (No.\n2020AAA0106400), the National Natural Science\nFoundation of China under Grants no. U1936207,\nand in part by the Youth Innovation Promotion As-\nsociation CAS(2018141).\nReferences\nNing Bian, Xianpei Han, Bo Chen, and Le Sun.\n2021. Benchmarking Knowledge-Enhanced Com-\nmonsense Question Answering via Knowledge-to-\nText Transformation. arXiv:2101.00760 [cs].\nZied Bouraoui, Jos ´e Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom BERT. In The Thirty-Fourth AAAI Conference\non Artiﬁcial Intelligence, AAAI 2020, The Thirty-\nSecond Innovative Applications of Artiﬁcial Intelli-\ngence Conference, IAAI 2020, The Tenth AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7456–7463. AAAI Press.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading Wikipedia to answer open-\ndomain questions. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1870–\n1879, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nStephen A. Cook. 1985. A taxonomy of problems with\nfast parallel algorithms. Information and Control ,\n64(1):2–22.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173–1178, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nHady Elsahar, Pavlos V ougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique\nLaforest, and Elena Simperl. 2018. T-REx: A large\nscale alignment of natural language with knowledge\nbase triples. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC 2018) , Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association\nfor Computational Linguistics, 8:34–48.\nMaxwell Forbes, Ari Holtzman, and Yejin Choi. 2019.\nDo Neural Language Representations Learn Physi-\ncal Commonsense? arXiv:1908.02899 [cs].\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020. Mak-\ning Pre-trained Language Models Better Few-shot\nLearners. arXiv:2012.15723 [cs].\nYoav Goldberg. 2019. Assessing BERT’s Syntactic\nAbilities. arXiv:1901.05287 [cs].\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do Attention\nHeads in BERT Track Syntactic Dependencies?\narXiv:1911.12246 [cs].\nMohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino,\nRichard Socher, and Hal Daum´e III. 2014. A neural\nnetwork for factoid question answering over para-\ngraphs. In Proceedings of the 2014 conference on\n1869\nempirical methods in natural language processing\n(EMNLP), pages 633–644.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-\nFACTR: Multilingual factual knowledge retrieval\nfrom pretrained language models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 5943–\n5959, Online. Association for Computational Lin-\nguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNora Kassner, Benno Krojer, and Hinrich Sch ¨utze.\n2020. Are pretrained language models symbolic\nreasoners over knowledge? In Proceedings of\nthe 24th Conference on Computational Natural Lan-\nguage Learning , pages 552–564, Online. Associa-\ntion for Computational Linguistics.\nNora Kassner and Hinrich Sch ¨utze. 2020a. BERT-\nkNN: Adding a kNN search component to pretrained\nlanguage models for better QA. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 3424–3430, Online. Association for\nComputational Linguistics.\nNora Kassner and Hinrich Sch ¨utze. 2020b. Negated\nand misprimed probes for pretrained language mod-\nels: Birds can talk, but cannot ﬂy. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics , pages 7811–7818, On-\nline. Association for Computational Linguistics.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn NAACL HLT 2016, The 2016 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, San Diego California, USA, June 12-17, 2016 ,\npages 260–270. The Association for Computational\nLinguistics.\nXiang Lisa Li and Percy Liang. 2021. Preﬁx-Tuning:\nOptimizing Continuous Prompts for Generation.\narXiv:2101.00190 [cs].\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020. Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of\nPre-Trained Language Models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 6862–\n6868, Online. Association for Computational Lin-\nguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP , pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs].\nAndrea Madotto, Zihan Liu, Zhaojiang Lin, and\nPascale Fung. 2020. Language Models as Few-\nShot Learner for Task-Oriented Dialogue Systems.\narXiv:2008.06239 [cs].\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019.\nRight for the wrong reasons: Diagnosing syntactic\nheuristics in natural language inference. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 3428–3448,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus,\nTim Rockt¨aschel, Yuxiang Wu, Alexander H. Miller,\nand Sebastian Riedel. 2020. How context affects lan-\nguage models’ factual predictions. In Conference\non Automated Knowledge Base Construction, AKBC\n2020, Virtual, June 22-24, 2020.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Sch ¨utze.\n2020. E-BERT: Efﬁcient-yet-effective entity em-\nbeddings for BERT. In Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 803–818, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\n1870\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nTimo Schick and Hinrich Sch ¨utze. 2020a. Few-Shot\nText Generation with Pattern-Exploiting Training.\narXiv:2012.11926 [cs].\nTimo Schick and Hinrich Sch ¨utze. 2020b. It’s Not\nJust Size That Matters: Small Language Models Are\nAlso Few-Shot Learners. arXiv:2009.07118 [cs].\nTimo Schick and Hinrich Sch ¨utze. 2021. Exploiting\ncloze-questions for few-shot text classiﬁcation and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, EACL 2021, Online, April 19 - 23, 2021, pages\n255–269. Association for Computational Linguis-\ntics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting Knowledge from Language Models with\nAutomatically Generated Prompts. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4222–4235, Online. Association for Computational\nLinguistics.\nVered Shwartz, Rachel Rudinger, and Oyvind Tafjord.\n2020. “you are grounded!”: Latent name artifacts in\npre-trained language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6850–6861,\nOnline. Association for Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. Commonsenseqa: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4149–4158. Association for Computa-\ntional Linguistics.\nAlexandre Tamborrino, Nicola Pellican`o, Baptiste Pan-\nnier, Pascal V oitot, and Louise Naudin. 2020. Pre-\ntraining is (almost) all you need: An application\nto commonsense reasoning. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3878–3887, Online. As-\nsociation for Computational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In 7th Inter-\nnational Conference on Learning Representations,\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019 .\nOpenReview.net.\nDenny Vrande ˇci´c and Markus Kr ¨otzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know\nnumbers? probing numeracy in embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5307–\n5315, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChenguang Wang, Xiao Liu, and Dawn Song. 2020.\nLanguage Models are Open Knowledge Graphs.\narXiv:2010.11967 [cs].\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019. Investi-\ngating BERT’s knowledge of language: Five anal-\nysis methods with NPIs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2877–2887, Hong Kong,\nChina. Association for Computational Linguistics.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2019. Evaluating Commonsense in Pre-\ntrained Language Models. arXiv:1911.11931 [cs].\n1871\nA WIKI-UNI Construction Details\nTo construct WIKI-UNI, we ﬁrst collect all the\ntriples which belong to the same 41 relations with\nLAMA from Wikidata (Vrande ˇci´c and Kr ¨otzsch,\n2014), then we randomly sample 50K triples with\na single-token object for each relation. Similar to\nLAMA, we ﬁlter out the instances whose object\nis not in MLMs’ vocabulary. For each relation,\nwe group the instances based on different objects,\nand indicate fo as the frequency of each object.\nWe denote the median of fo with fm. For groups\nwhere fo > fm, we randomly sample fm instances,\nand delete the groups where fo < fm. Therefore,\nwe acquire a dataset named WIKI-UNI with a uni-\nform answer distribution. There are 70K facts in\nWIKI-UNI and 34K facts in LAMA. Since BERT\nand RoBERTa have a different vocabulary, so the\ndatasets for their evaluation are slightly different.\nB Results on RoBERTa-large\nOur conclusions are similar on BERT-large and\nRoBERTa-large, therefore, we report the results of\nBERT-large in the article and results of RoBERTa-\nlarge here.\nB.1 Promp-based Retrieval\nFigure 7 shows the very signiﬁcant correlation be-\ntween the prediction distributions on LAMA and\nWIKI-UNI for RoBERTa-large: on all three kinds\nof prompts, the Pearson correlation coefﬁcient be-\ntween these two prediction distributions exceeds\n0.9 in most relations. Table 8 shows the percentage\nof instances that the topk object entities cover for\nRoBERTa-large.\n0.2 0.4 0.6 0.8 1.0\nTman\nTmine\nTauto\nFigure 7: The correlations of the prediction distribution\non LAMA and WIKI-UNI for RoBERTa-large.\nB.2 Case-based Analogy\nTable 9 shows the performance improvement after\nintroducing illustrative cases for RoBERTa-large\nmodel, we can see that the illustrative cases could\nalso signiﬁcantly increase the knowledge extraction\nDistribution DatasetsTop1 Top3 Top5Prec.\nAnswer LAMA 23.93 42.02 50.08 -\nWIKI-UNI1.84 5.53 8.61 -\nPrediction LAMA 37.48 56.85 65.45 23.65\nWIKI-UNI36.53 55.51 63.58 13.59\nTable 8: The percentage of instances that the topk ob-\nject entities cover for RoBERTa-large. The statistics\nis different from Table 1 because we ﬁlter LAMA with\nRoBERTa’s vocabulary when evaluate RoBERTa-large.\nperformance for RoBERTa-large. Table 14 shows\nhow the entity types of predictions changed after in-\ntroducing the illustrative cases for RoBERTa-large\nmodel, the conclusion is similar with BERT-large.\nFigure 8 shows the percentage on the change of\noverall rank and in-type rank for RoBERTa-large\nmodel.\nAnd another ﬁnding is that BERT-large has a\nbetter type prediction ability than RoBERTa-large,\neven without illustrative cases. We calculate the\noverall type precision over prompt-based paradigm\n(the percentage of predictions that the type is cor-\nrect). And the type precision for BERT-large is 68%\nand for RoBERTa-large is only 51%, which partly\nexplains why performance of RoBERTa-large is\nsigniﬁcantly worse than BERT-large on LAMA\ndataset.\nEnhanced with\nCases Prec. Better Worse\nNo 23.65 - -\nYes 29.78 14.09 7.96\nTable 9: Performance of the case-based analogy\nparadigm for RoBERTa-large\n10%25%40%55%\nIn-typeRank\nOverallRank\nRaisedUnchangedDropped\nFigure 8: Percentages on the change of overall rank\n(among all candidates) and the in-type rank (among\ncandidates with the same type) of golden answer of\nRoBERTa-large model.\n1872\nB.3 Context-based Inference\nTable 10 shows the comparison of contexts group\nby whether the contexts contain the answer for\nRoBERTa-large. We can see that for contexts con-\ntaining the answer, context-based inference sig-\nniﬁcantly improves the factual extraction perfor-\nmance. Meanwhile, there is a performance drop for\nthose instances whose context does not contain the\nanswer. Table 11 shows the overall performance\nimprovements when introducing different exter-\nnal contexts for RoBERTa-large. Table 12 shows\nthe comparison of the masked contexts based on\nwhether they can/cannot reconstruct the masked\nanswer for RoBERTa-large. The improvements\nmainly comes from the instances whose answer in\ncontexts can be reconstructed.\nAnswer\nin contextPrompt-based Context-based∆\nPresent\n(46.04%) 27.95 52.05 +24.10\nAbsent\n(53.96 %) 18.95 14.72 -4.23\nTable 10: Comparison of contexts grouped by whether\nthe answer presents or absents for RoBERTa-large.\nWithout Contexts Full Contexts Masked Contexts\n23.65 31.44 24.44\nTable 11: The overall performance when introducing\ndifferent contexts for RoBERTa-large.\nAnswer\nReconstructablePrompt-based Context-based∆\nReconstructable\n(61.23%) 30.50 42.37 +11.87\nNot-reconstructable\n(38.77 %) 22.19 22.15 -0.04\nTable 12: Comparison of the masked contexts based on\nwhether they can/cannot reconstruct the masked answer\nfor RoBERTa-large.\nC Full Version of the Type Prediction\nResults\nTable 13 shows the detailed analysis of all rela-\ntions using case-based analogy paradigm for BERT-\nlarge and Table 14 is the results on RoBERTa-\nlarge. Because of the page limit, another ﬁnd-\ning we didn’t mention in the article is that,\napart from “type guidance”, the illustrative cases\ncould also provide a “surface form guidance” in\na few relations (e.g., part of, applies to\njurisdiction, subclass of). Speciﬁcally,\nthe “surface form” indicate that the object entity\nname (e.g., Apple) is a substring of the subject en-\ntity name (e.g., Apple Watch). Such phenomenon\nis also mentioned in Poerner et al. (2020).\n1873\nRelation Induced Object TypePrecision\n∆\nType\nPrec.∆\nWrong→Right\nw/ Type Change\nRight→Wrong\nw/o Type Change\nnamed after physical object 68.06 98.91 99.77 -\ncountry of citizenship sovereign state 43.37 84.16 100.00 -\nposition held religious servant 36.88 80.26 91.15 90.00\nreligion religion 33.20 34.88 100.00 -\nwork location city 26.10 70.55 85.04 100.00\ninstrument musical instrument 17.07 55.75 89.08 75.00\ncountry sovereign state 14.30 29.04 88.48 87.93\nemployer business 12.01 99.22 100.00 -\ncontinent continent 10.87 51.18 96.86 88.24\nlanguages spoken, written or signedIndo-European languages9.91 -0.93 10.56 81.54\napplies to jurisdiction state 8.71 -6.13 7.23 63.64\ncountry of origin sovereign state 8.36 33.22 71.64 98.28\nsubclass of object 7.68 27.28 66.18 87.10\npart of object 7.51 37.66 54.27 97.87\nlanguage of work or name Indo-European languages6.05 10.95 77.23 77.08\nlocation of formation city 5.02 66.34 80.77 100.00\nhas part abstract object 5.02 27.26 25.33 100.00\ngenre series 4.62 17.61 95.45 -\nowned by organization 2.62 11.50 9.57 100.00\ninstance of concrete object 2.06 4.34 35.80 96.77\noccupation profession 1.35 -0.53 0.00 100.00\nplace of death city 1.26 16.37 68.63 100.00\ntwinned administrative body city 0.91 0.80 15.38 75.00\ndiplomatic relation sovereign state 0.80 1.11 10.00 100.00\nnative language Indo-European languages0.20 0.62 38.64 92.86\nmanufacturer business -1.02 0.31 33.33 61.29\nﬁeld of work knowledge -1.15 0.00 26.09 90.32\ndeveloper enterprise -1.52 1.52 4.17 96.97\nlocation community -1.57 4.59 3.03 100.00\ncapital city -2.00 0.14 4.55 97.22\nposition played on team / specialityposition -4.10 11.03 - 100.00\nheadquarters location city -4.24 0.62 0.00 100.00\nofﬁcial language Nostratic languages -5.28 -1.14 5.45 90.57\noriginal language of ﬁlm or TV showNostratic languages -5.84 -16.71 19.15 43.30\nplace of birth city -6.25 4.34 14.29 100.00\ncapital of political territorial entity-6.84 0.42 - 100.00\nshares border with community -7.37 2.72 2.22 97.35\nrecord label record label -7.93 -22.38 - 0.00\noriginal network television station -10.56 0.45 11.36 86.13\nlocated in the administrative territorial entitycommunity -12.94 11.69 10.53 99.25\nmember of organization -14.67 16.45 94.74 98.08\nTable 13: A detailed analysis of all relations using case-based analogy paradigm for BERT-large, which is corre-\nsponding to Table 4 in the article. “-” indicates the number of queries whose predictions are reversed correctly or\nmistakenly is less than 3.\n1874\nRelation Induced Object TypePrecision\n∆\nType\nPrec.∆\nWrong→Right\nw/ Type Change\nRight→Wrong\nw/o Type Change\nreligion religion 56.92 66.36 100.00 -\nposition held religious servant 41.86 47.42 99.03 -\ncountry of citizenship sovereign state 37.16 74.11 100.00 -\nmember of organization 31.03 77.83 100.00 -\ncontinent continent 29.51 87.80 100.00 100.00\ninstrument musical instrument 28.26 6.04 94.04 0.00\ncountry of origin sovereign state 28.18 94.92 99.61 100.00\ncountry sovereign state 26.64 69.84 95.22 96.55\npart of object 24.57 90.22 96.98 100.00\nplace of death city 22.88 95.35 98.95 100.00\ninstance of concrete object 14.97 20.53 34.30 97.50\nlocation of formation city 14.12 99.88 100.00 -\nsubclass of object 12.07 26.25 63.31 90.00\ncapital city 10.62 36.31 92.19 85.71\nnamed after physical object 10.25 85.05 100.00 100.00\nlanguage of work or name Indo-European languages9.10 26.72 89.12 72.17\nhas part abstract object 8.79 67.99 77.65 -\nwork location city 8.09 12.43 96.95 6.45\nlanguages spoken, written or signedIndo-European languages5.09 17.75 54.20 86.90\nemployer business 3.97 10.31 19.05 100.00\nposition played on team / specialityposition 3.26 56.51 71.43 75.00\nnative language Indo-European languages1.09 1.63 28.21 93.10\ngenre series 1.05 0.23 75.00 66.67\nrecord label record label 0.00 -7.55 - -\nplace of birth city -0.13 41.02 66.67 100.00\ntwinned administrative body city -0.45 1.04 0.00 100.00\nheadquarters location city -1.00 0.00 0.00 100.00\ndiplomatic relation sovereign state -1.16 1.05 25.00 100.00\nowned by organization -1.45 43.78 64.62 94.59\nﬁeld of work knowledge -2.10 0.69 10.53 96.77\noccupation profession -2.43 0.00 0.00 100.00\nofﬁcial language Nostratic languages -3.11 3.88 18.37 97.40\nlocated in the administrative territorial entitycommunity -3.35 45.81 75.93 97.50\noriginal language of ﬁlm or TV showNostratic languages -5.29 -21.30 15.38 34.29\nshares border with community -9.82 0.16 0.00 98.86\nlocation community -11.49 27.15 41.43 100.00\ndeveloper enterprise -12.25 6.80 37.50 79.41\noriginal network television station -16.46 -15.84 14.29 72.49\napplies to jurisdiction state -18.38 2.11 35.71 98.00\ncapital of political territorial entity-39.44 7.22 - 100.00\nmanufacturer business -49.63 6.79 44.44 93.82\nTable 14: A detailed analysis of all relations using case-based analogy paradigm for RoBERTa-large, which is\ncorresponding to Table 4 in the article. “-” indicates the number of queries whose predictions are reversed correctly\nor mistakenly is less than 3.",
  "topic": "Computational linguistics",
  "concepts": [
    {
      "name": "Computational linguistics",
      "score": 0.6343192458152771
    },
    {
      "name": "Natural language processing",
      "score": 0.5938068628311157
    },
    {
      "name": "Computer science",
      "score": 0.5913510918617249
    },
    {
      "name": "Joint (building)",
      "score": 0.5633324384689331
    },
    {
      "name": "Association (psychology)",
      "score": 0.4952949583530426
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4918804168701172
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4501780867576599
    },
    {
      "name": "Linguistics",
      "score": 0.40539494156837463
    },
    {
      "name": "Library science",
      "score": 0.39135393500328064
    },
    {
      "name": "Engineering",
      "score": 0.16196197271347046
    },
    {
      "name": "Epistemology",
      "score": 0.15536946058273315
    },
    {
      "name": "Philosophy",
      "score": 0.13725262880325317
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210142437",
      "name": "Mylan (Switzerland)",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210128818",
      "name": "Institute of Software",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    }
  ],
  "cited_by": 72
}