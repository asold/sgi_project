{
    "title": "Hierarchical Transformers for Long Document Classification",
    "url": "https://openalex.org/W2981613521",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2906790199",
            "name": "Raghavendra Pappagari",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2228061263",
            "name": "Piotr Zelasko",
            "affiliations": [
                "Avaya (Bermuda)"
            ]
        },
        {
            "id": "https://openalex.org/A2108722367",
            "name": "Jesús Villalba",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2586090654",
            "name": "Yishay Carmiel",
            "affiliations": [
                "Avaya (Bermuda)"
            ]
        },
        {
            "id": "https://openalex.org/A142824529",
            "name": "Najim Dehak",
            "affiliations": [
                "Johns Hopkins University"
            ]
        },
        {
            "id": "https://openalex.org/A2906790199",
            "name": "Raghavendra Pappagari",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2228061263",
            "name": "Piotr Zelasko",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108722367",
            "name": "Jesús Villalba",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2586090654",
            "name": "Yishay Carmiel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A142824529",
            "name": "Najim Dehak",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2563000706",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6674813771",
        "https://openalex.org/W1964155876",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W6685053522",
        "https://openalex.org/W2884922078",
        "https://openalex.org/W2514741789",
        "https://openalex.org/W2168363555",
        "https://openalex.org/W2802103240",
        "https://openalex.org/W2518058337",
        "https://openalex.org/W2750485702",
        "https://openalex.org/W2514841977",
        "https://openalex.org/W2078295034",
        "https://openalex.org/W2616922365",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2100002341",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2939507640",
        "https://openalex.org/W2613053618",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963012544"
    ],
    "abstract": "BERT, which stands for Bidirectional Encoder Representations from Transformers, is a recently introduced language representation model based upon the transfer learning paradigm. We extend its fine-tuning procedure to address one of its major limitations - applicability to inputs longer than a few hundred words, such as transcripts of human call conversations. Our method is conceptually simple. We segment the input into smaller chunks and feed each of them into the base model. Then, we propagate each output through a single recurrent layer, or another transformer, followed by a softmax activation. We obtain the final classification decision after the last segment has been consumed. We show that both BERT extensions are quick to fine-tune and converge after as little as 1 epoch of training on a small, domain-specific data set. We successfully apply them in three different tasks involving customer call satisfaction prediction and topic classification, and obtain a significant improvement over the baseline models in two of them.",
    "full_text": "arXiv:1910.10781v1  [cs.CL]  23 Oct 2019\nHIERARCHICAL TRANSFORMERS FOR LONG DOCUMENT CLASSIFICA TION\nRaghavendra P appagari1, Piotr ˙Zelasko2, Jes ´us V illalba 1, Y ishay Carmiel 2, and Najim Dehak 1\n1Center for Language and Speech Processing, Johns Hopkins Un iversity, Baltimore, MD\n2A vaya Conversational Intelligence\n{rpappag1,jvillal7,ndehak3}@jhu.edu\npetezor@gmail.com, ycarmiel@avaya.com\nABSTRA CT\nBER T , which stands for Bidirectional Encoder Represen-\ntations from Transformers, is a recently introduced lan-\nguage representation model based upon the transfer learnin g\nparadigm. W e extend its ﬁne-tuning procedure to address one\nof its major limitations - applicability to inputs longer th an\na few hundred words, such as transcripts of human call con-\nversations. Our method is conceptually simple. W e segment\nthe input into smaller chunks and feed each of them into the\nbase model. Then, we propagate each output through a single\nrecurrent layer, or another transformer, followed by a soft max\nactivation. W e obtain the ﬁnal classiﬁcation decision afte r the\nlast segment has been consumed. W e show that both BER T\nextensions are quick to ﬁne-tune and converge after as littl e as\n1 epoch of training on a small, domain-speciﬁc data set. W e\nsuccessfully apply them in three different tasks involving cus-\ntomer call satisfaction prediction and topic classiﬁcatio n, and\nobtain a signiﬁcant improvement over the baseline models in\ntwo of them.\nIndex T erms— Transformer, BER T , Recurrent Neural\nNetworks,T opic Identiﬁcation, Customer Satisfaction Pre dic-\ntion\n1. INTRODUCTION\nBidirectional Encoder Representations from Transformers\n(BER T) is a novel Transformer [1] model, which recently\nachieved state-of-the-art performance in several languag e\nunderstanding tasks, such as question answering, natural l an-\nguage inference, semantic similarity, sentiment analysis , and\nothers [2]. While well-suited to dealing with relatively sh ort\nsequences, Transformers suffer from a major issue that hin-\nders their applicability in classiﬁcation of long sequence s, i.e.\nthey are able to consume only a limited context of symbols as\ntheir input [3].\nThere are several natural language (NLP) processing tasks\nthat involve such long sequences. Of particular interest ar e\ntopic identiﬁcation of spoken conversations [4, 5, 6] and ca ll\ncenter customer satisfaction prediction [7, 8, 9, 10]. Call cen-\nter conversations, while usually quite short and to the poin t,\noften involve agents trying to solve very complex issues tha t\nthe customers experience, resulting in some calls taking ev en\nan hour or more. For speech analytics purposes, these calls\nare typically transcribed using an automatic speech recogn i-\ntion (ASR) system, and processed in textual representation s\nfurther down the NLP pipeline. These transcripts sometimes\nexceed the length of 5000 words. Furthermore, temporal in-\nformation might play an important role in tasks like CSA T .\nFor example, a customer may be angry at the beginning of the\ncall, but after her issue is resolved, she would be very satis -\nﬁed with the way it was handled. Therefore, simple bag of\nwords models, or any model that does not include temporal\ndependencies between the inputs, may not be well-suited to\nhandle this category of tasks. This motivates us to employ\nmodel such as BER T in this task.\nIn this paper, we propose a method that builds upon\nBER T’s architecture. W e split the input text sequence\ninto shorter segments in order to obtain a representation\nfor each of them using BER T . Then, we use either a re-\ncurrent LSTM [11] network, or another Transformer, to\nperform the actual classiﬁcation. W e call these techniques\nRecurrence over BERT (RoBERT) and Transformer over\nBERT (T oBERT). Given that these models introduce a hier-\narchy of representations (segment-wise and document-wise ),\nwe refer to them as Hierarchical Transformers. T o the best\nof our knowledge, no attempt has been done before to use\nthe Transformer architecture for classiﬁcation of such lon g\nsequences.\nOur novel contributions are:\n• T wo extensions - RoBER T and T oBER T - to the BER T\nmodel, which enable its application in classiﬁcation of\nlong texts by performing segmentation and using an-\nother layer on top of the segment representations.\n• State-of-the-art results on the Fisher topic classiﬁcatio n\ntask.\n• Signiﬁcant improvement on the CSA T prediction task\nover the MS-CNN model.\n2. RELA TED WORK\nSeveral dimensionality reduction algorithms such as RBM,\nautoencoders, subspace multinomial models (SMM) are used\nto obtain a low dimensional representation of documents fro m\na simple BOW representation and then classify it using a sim-\nple linear classiﬁers [12, 13, 14, 5]. In [15] hierarchical a t-\ntention networks are used for document classiﬁcation. They\nevaluate their model on several datasets with average num-\nber of words around 150. Character-level CNN are explored\nin [16] but it is prohibitive for very long documents. In [17] ,\ndataset collected from arXiv papers is used for classiﬁcati on.\nFor classiﬁcation, they sample random blocks of words and\nuse them together for classiﬁcation instead of using full do c-\nument which may work well as arXiv papers are usually co-\nherent and well written on a well deﬁned topic. Their method\nmay not work well on spoken conversations as random block\nof words usually do not represent topic of full conversation .\nSeveral researchers addressed the problem of predicting\ncustomer satisfaction [7, 8, 9, 10]. In most of these works,\nlogistic regression, SVM, CNN are applied on different kind s\nof representations.\nIn [18], authors use BER T for document classiﬁcation but\nthe average document length is less than BER T maximum\nlength 512. TransformerXL [3] is an extension to the Trans-\nformer architecture that allows it to better deal with long i n-\nputs for the language modelling task. It relies on the auto-\nregressive property of the model, which is not the case in our\ntasks.\n3. METHOD\n3.1. BERT\nBecause our work builds heavily upon BER T , we provide\na brief summary of its features. BER T is built upon the\nTransformer architecture [1], which uses self-attention, feed-\nforward layers, residual connections and layer normalizat ion\nas the main building blocks. It has two pre-training objec-\ntives:\n• Masked language modelling - some of the words in a\nsentence are being masked and the model has to predict\nthem based on the context (note the difference from the\ntypical autoregressive language model training objec-\ntive);\n• Next sentence prediction - given two input sequences,\ndecide whether the second one is the next sentence or\nnot.\nBER T has been shown to beat the state-of-the-art perfor-\nmance on 11 tasks with no modiﬁcations to the model archi-\ntecture, besides adding a task-speciﬁc output layer [2]. W e\nfollow same procedure suggested in [2] for our tasks. Fig. 1\nshows the BER T model for classiﬁcation. W e obtain two\nFig. 1. BER T model for classiﬁcation. H denotes BER T seg-\nment representations from last transformer block, P denote s\nsegment posterior probabilities. Figure inspired from [2]\nkinds of representation from BER T: pooled output from last\ntransformer block, denoted by H, and posterior probabiliti es,\ndenoted by P . There are two variants of BER T - BER T -Base\nand BER T -Large. In this work we are using BER T -Base for\nfaster training and experimentation, however, our methods are\napplicable to BER T -Large as well. BER T -Base and BER T -\nLarge are different in model parameters such as number of\ntransformer blocks, number of self-attention heads. T otal\nnumber of parameters in BER T -Base are 110M and 340M in\nBER T -Large.\nBER T suffers from major limitations in terms of han-\ndling long sequences. Firstly, the self-attention layer ha s a\nquadratic complexity O(n2) in terms of the sequence length\nn [1]. Secondly, BER T uses a learned positional embed-\ndings scheme [2], which means that it won’t likely be able to\ngeneralize to positions beyond those seen in the training da ta.\nT o investigate the effect of ﬁne-tuning BER T on task per-\nformance, we use either the pre-trained BER T weights 1, or the\nweights from a BER T ﬁne-tuned on the task-speciﬁc dataset\non a segment-level (i.e. we preserve the original label but ﬁ ne-\ntune on each segment separately instead of on the whole text\nsequence). W e compare these results to using the ﬁne-tuned\nsegment-level BER T predictions directly as inputs to the ne xt\nlayer.\n1 A vailable at https://github.com/huggingface/pytorch-p retrained-BER T\n3.2. Recurrence over BERT\nGiven that BER T is limited to a particular input length, we\nsplit the input sequence into segments of a ﬁxed size with\noverlap. For each of these segments, we obtain H or P from\nBER T model. W e then stack these segment-level representa-\ntions into a sequence, which serves as input to a small (100-\ndimensional) LSTM layer. Its output serves as a document\nembedding. Finally, we use two fully connected layers with\nReLU (30-dimensional) and softmax (the same dimensional-\nity as the number of classes) activations to obtain the ﬁnal\npredictions.\nWith this approach, we overcome BER T’s computational\ncomplexity, reducing it to O(n/k∗k2) =O(nk) for RoBER T ,\nwith k denoting the segment size (the LSTM component has\nnegligible linear complexity O(k)). The positional embed-\ndings are also no longer an issue.\n3.3. T ransformer over BERT\nGiven that Transformers’ edge over recurrent networks is\ntheir ability to effectively capture long distance relatio nships\nbetween words in a sequence [1], we experiment with replac-\ning the LSTM recurrent layer in favor of a small Transformer\nmodel (2 layers of transformer building block containing se lf-\nattention, fully connected, etc.). T o investigate if prese rving\nthe information about the input sequence order is important ,\nwe also build a variant of T oBER T which learns positional\nembeddings at the segment-level representations (but is li m-\nited to sequences of length seen during the training).\nT oBER T’s computational complexity O(n2\nk2 ) is asymptot-\nically inferior to RoBER T , as the top-level Transformer mod el\nagain suffers from quadratic complexity in the number of seg -\nments. However, in practice this number is much smaller than\nthe input sequence length ( n\nk << n), so we haven’t observed\nperformance or memory issues with our datasets.\n4. EXPERIMENTS\nW e evaluated our models on 3 different datasets:\n• CSA T dataset for CSA T prediction, consisting of spo-\nken transcripts (automatic via ASR).\n• 20 newsgroups for topic identiﬁcation task, consisting\nof written text;\n• Fisher Phase 1 corpus for topic identiﬁcation task, con-\nsisting of spoken transcripts (manual);\n4.1. CSA T\nCSA T dataset consists of US English telephone speech from\ncall centers. For each call in this dataset, customers parti ci-\npated in that call gave a rating on his experience with agent.\nOriginally, this dataset has labels rated on a scale 1-9 with 9\n1401\n182 180 147\n26 4 30\n127\n2234\n1 2 3 4 5 6 7 8 9\nCustomer Rating\n0\n500\n1000\n1500\n2000\n2500Number of calls\nFig. 2. Histogram of customer ratings. Rating 9 corresponds\nto extremely satisﬁed and 1 to extremely dissatisﬁed\nbeing extremely satisﬁed and 1 being extremely dissatisﬁed .\nFig. 2 shows the histogram of ratings for our dataset. As the\ndistribution is skewed towards extremes, we choose to do bi-\nnary classiﬁcation with ratings above 4.5 as satisﬁed and be -\nlow 4.5 as dissatisﬁed. Quantization of ratings also helped us\nto create a balanced dataset. This dataset contains 4331 cal ls\nand we split them into 3 sets for our experiments: 2866 calls\nfor training, 362 calls for validation and, ﬁnally, 1103 cal ls\nfor testing.\nW e obtained the transcripts by employing an ASR sys-\ntem. The ASR system uses TDNN-LSTM acoustic model\ntrained on Fisher and Switchboard datasets with lattice-fr ee\nmaximum mutual information criterion [19]. The word er-\nror rates using four-gram language models were 9.2% and\n17.3% respectively on Switchboard and CallHome portions\nof Eval2000 dataset 2.\n4.2. 20 newsgroups\n20 newsgroups 3 data set is one of the frequently used datasets\nin the text processing community for text classiﬁcation and\ntext clustering. This data set contains approximately 20,0 00\nEnglish documents from 20 topics to be identiﬁed, with\n11314 documents for training and 7532 for testing. In this\nwork, we used only 90% of documents for training and the\nremaining 10% for validation. For fair comparison with othe r\npublications, we used 53160 words vocabulary set available\nin the datasets website.\n4.3. Fisher\nFisher Phase 1 US English corpus is often used for automatic\nspeech recognition in speech community. In this work, we\n2 https://catalog.ldc.upenn.edu/LDC2002T43\n3 http://qwone.com/ jason/20Newsgroups/\n0 1000 2000 3000 4000 5000\nWord count in document\n0\n0.2\n0.4\n0.6\n0.8\n1\nFraction of Documents\nCSAT\n20News\nFisher\nFig. 3. Cumulative distribution of document lengths.\nDataset C N A W L\nCSA T 2 4331 787 10503\n20 newsgroups 20 18846 266 10334\nFisher 40 2746 1788 2713\nT able 1. Dataset statistics. C indicates number of Classes, N\nthe Number of documents, A W the A verage number of W ords\nper document and L the Longest document length.\nused it for topic identiﬁcation as in [4]. The documents are\n10-minute long telephone conversations between two people\ndiscussing a given topic. W e used same training and test spli ts\nas [4] in which 1374 and 1372 documents are used for training\nand testing respectively. For validation of our model, we us ed\n10% of training dataset and the remaining 90% was used for\nactual model training. The number of topics in this data set i s\n40.\n4.4. Dataset Statistics\nT able 1 shows statistics of our datasets. It can be observed t hat\naverage length of Fisher is much higher than 20 newsgroups\nand CSA T . Cumulative distribution of document lengths for\neach dataset is shown in Fig. 3. It can be observed that almost\nall of the documents in Fisher dataset have length more than\n1000 words. For CSA T , more than 50% of the documents\nhave length greater than 500 and for 20newsgroups only 10%\nof the documents have length greater than 500. Note that,\nfor CSA T and 20newsgroups, there are few documents with\nlength more than 5000.\n4.5. Architecture and T raining Details\nIn this work, we split document into segments of 200 to-\nkens with a shift of 50 tokens to extract features from BER T\nmodel. For RoBER T , LSTM model is trained to minimize\nRoBER T T oBER T\nCSA T 71.16 74.77\n20newsgroups 60.75 65.04\nFisher 38.04 80.68\nT able 2. Results using segment representations (H) from a\npre-trained BER T (without ﬁne-tuning).\nRoBER T T oBER T\nCSA T 83.65 83.48\n20newsgroups 84.71 85.52\nFisher 82.28 95.48\nT able 3. Results using segment representations (H) from a\nﬁne-tuned BER T .\ncross-entropy loss with Adam optimizer [20]. The initial\nlearning rate is set to 0.001 and is reduced by a factor of\n0.95 if validation loss does not decrease for 3-epochs. For\nT oBER T , the Transformer is trained with the default BER T\nversion of Adam optimizer [2] with an initial learning rate\nof 5e-5. W e report accuracy in all of our experiments. W e\nchose a model with the best validation accuracy to calculate\naccuracy on the test set. T o accomodate for non-determinism\nof some T ensorFlow 4 GPU operations, we report accuracy\naveraged over 5 runs.\n5. RESUL TS\nT able 2 presents results using pre-trained BER T features. W e\nextracted features from the pooled output of ﬁnal transform er\nblock as these were shown to be working well for most of\nthe tasks [2]. The features extracted from a pre-trained BER T\nmodel without any ﬁne-tuning lead to a sub-par performance.\nHowever, W e also notice that T oBER T model exploited the\npre-trained BER T features better than RoBER T . It also con-\nverged faster than RoBER T . T able 3 shows results using\nfeatures extracted after ﬁne-tuning BER T model with our\ndatasets. Signiﬁcant improvements can be observed com-\npared to using pre-trained BER T features. Also, it can be\nnoticed that T oBER T outperforms RoBER T on Fisher and\n20newsgroups dataset by 13.63% and 0.81% respectively. On\nCSA T , T oBER T performs slightly worse than RoBER T but it\nis not statistically signiﬁcant as this dataset is small.\nT able 4 presents results using ﬁne-tuned BER T predic-\ntions instead of the pooled output from ﬁnal transformer\nblock. For each document, having obtained segment-wise\npredictions we can obtain ﬁnal prediction for the whole doc-\nument in three ways:\n• Compute the average of all segment-wise predictions\n4 W e used T ensorFlow version 1.14.0.\nMost frequent A verage RoBER T T oBER T\nCSA T 81.03 82.84 83.54 81.48\n20newsgroups 84.78 84.51 84.07 85.47\nFisher 88.70 88.48 91.18 94.16\nT able 4. Comparison of models using ﬁne-tuned BER T\nsegment-level predictions (P) instead of segment represen ta-\ntions (H).\n0-800 800-1100 1100-1400 1400-2000 2000-3000\nDocument Length range\n70\n75\n80\n85\n90\n95\n100\n105Accuracy\nToBERT\nAverage Voting\nFig. 4. Comparison of average voting and T oBER T for vari-\nous document length ranges for Fisher dataset.\nand ﬁnd the most probable class;\n• Find the most frequently predicted class;\n• Train a classiﬁcation model.\nIt can be observed from T able 4 that a simple averaging\noperation or taking most frequent predicted class works com -\npetitively for CSA T and 20newsgroups but not for the Fisher\ndataset. W e believe the improvements from using RoBER T\nor T oBER T , compared to simple averaging or most frequent\noperations, are proportional to the fraction of long docume nts\nin the dataset. CSA T and 20newsgroups have (on average)\nsigniﬁcantly shorter documents than Fisher, as seen in Fig. 3.\nAlso, signiﬁcant improvements for Fisher could be because o f\nless conﬁdent predictions from BER T model as this dataset\nhas 40 classes. Fig. 4 presents the comparison of average\nvoting and T oBER T for various document length ranges for\nFisher dataset. W e used ﬁne-tuned BER T segment-level pre-\ndictions (P) for this analysis. It can be observed that T oBER T\noutperforms average voting in every interval. T o the best of\nour knowledge, this is a state-of-the-art result reported o n the\nFisher dataset.\nT able 5 presents the effect of position embeddings on the\nmodel performance. It can be observed that position embed-\ndings did not signiﬁcantly affect the model performance for\nFisher and 20newsgroups, but they helped slightly in CSA T\nPosition embeddings\nNo Y es\nCSA T 82.84 83.48\n20newsgroups 85.51 85.52\nFisher 95.84 95.48\nT able 5. The effect of including positional embeddings in\nT oBER T model. Fine-tuned BER T segment representations\nwere used for these results.\ndataset Model Accuracy\nCSA T MS-CNN 79.53\nT oBER T 83.48\nRoBER T 83.65\n20 newsgroups\nSCDV [21] 84.6\nMS-CNN [6] 86.12\nT oBER T 85.52\nRoBER T 84.71\nFisher\nSVM MCE [4] 91.9\nMS-CNN [6] 92.93\nT oBER T 95.48\nRoBER T 91.18\nT able 6. Comparison of our results with previous works.\nprediction (an absolute improvement of 0.64% F1-score). W e\nthink that this is explained by the fact that Fisher and 20new s-\ngroups are topic identiﬁcation tasks, and the topic does not\nchange much throughout these documents. However, CSA T\nmay vary during the call, and in some cases a naive assump-\ntion that the sequential nature of the transcripts is irrele vant\nmay lead to wrong conclusions.\nT able 6 compares our results with previous works. It can\nbe seen that our model T oBER T outperforms CNN based ex-\nperiments by signiﬁcant margin on CSA T and Fisher datasets.\nFor CSA T dataset, we used multi-scale CNN (MS-CNN) as\nthe baseline, given its strong results on Fisher and 20news-\ngroups. The setup was replicated from [6] for comparison.\nW e also see that our result on 20 newsgroups is 0.6% worse\nthan the state-of-the-art.\n6. CONCLUSIONS\nIn this paper, we presented two methods for long documents\nusing BER T model: RoBER T and T oBER T . W e evaluated our\nexperiments on two classiﬁcation tasks - customer satisfac tion\nprediction and topic identiﬁcation - using 3 datasets: CSA T ,\n20newsgroups and Fisher. W e observed that T oBER T outper-\nforms RoBER T on pre-trained BER T features and ﬁne-tuned\nBER T features for all our tasks. Also, we noticed that ﬁne-\ntuned BER T performs better than pre-trained BER T . W e have\nshown that both RoBER T and T oBER T improved the sim-\nple baselines of taking an average (or the most frequent) of\nsegment-wise predictions for long documents to obtain ﬁnal\nprediction. Position embeddings did not signiﬁcantly affe ct\nour models performance, but slightly improved the accuracy\non the CSA T task. W e obtained the best results on Fisher\ndataset and good improvements for CSA T task compared to\nthe CNN baseline. It is interesting to note that the longer th e\naverage input in a given task, the bigger improvement we ob-\nserve w .r.t. the baseline for that task. Our results conﬁrm t hat\nboth RoBER T and T oBER T can be used for long sequences\nwith competitive performance and quick ﬁne-tuning proce-\ndure. For future work, we shall focus on training models on\nlong documents directly (i.e. in an end-to-end manner).\n7. REFERENCES\n[1] Ashish V aswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin, “ Attention is all you need, ” in Ad-\nvances in neural information processing systems , 2017,\npp. 5998–6008.\n[2] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and\nKristina T outanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding, ” arXiv\npreprint arXiv:1810.04805 , 2018.\n[3] Zihang Dai, Zhilin Y ang, Y iming Y ang, William W Co-\nhen, Jaime Carbonell, Quoc V Le, and Ruslan Salakhut-\ndinov, “Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context, ” arXiv preprint\narXiv:1901.02860 , 2019.\n[4] Timothy J Hazen, “Mce training techniques for topic\nidentiﬁcation of spoken audio documents, ” IEEE T rans-\nactions on Audio, Speech, and Language Processing ,\nvol. 19, no. 8, pp. 2451–2460, 2011.\n[5] Santosh Kesiraju, Luk ´ as Burget, Igor Sz ¨ oke, and Jan\nCernock ` y, “Learning document representations us-\ning subspace multinomial model., ” in INTERSPEECH,\n2016, pp. 700–704.\n[6] Raghavendra Pappagari, Jes ´ us V illalba, and Najim De-\nhak, “Joint veriﬁcation-identiﬁcation in end-to-end\nmulti-scale cnn framework for topic identiﬁcation, ”\nin 2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP) . IEEE, 2018,\npp. 6199–6203.\n[7] Shammur Absar Chowdhury, Evgeny A Stepanov,\nGiuseppe Riccardi, et al., “Predicting user satisfaction\nfrom turn-taking in spoken conversations., ” in INTER-\nSPEECH, 2016, pp. 2910–2914.\n[8] Jordi Luque, Carlos Segura, Ariadna S´ anchez, Martı\nUmbert, and Luis Angel Galindo, “The role of linguis-\ntic and prosodic cues on the prediction of self-reported\nsatisfaction in contact centre phone calls, ” Proc. Inter-\nspeech 2017 , pp. 2346–2350, 2017.\n[9] Y oungja Park and Stephen C Gates, “T owards real-time\nmeasurement of customer satisfaction using automati-\ncally generated call transcripts, ” in Proceedings of the\n18th ACM conference on Information and knowledge\nmanagement. ACM, 2009, pp. 1387–1396.\n[10] Stefan Meinzer, Ulf Jensen, Alexander Thamm,\nJoachim Hornegger, and Bj ¨ orn M Eskoﬁer, “Can ma-\nchine learning techniques predict customer dissatisfac-\ntion? a feasibility study for the automotive industry, ”\nArtiﬁcial Intelligence Research , vol. 6, no. 1, pp. 80,\n2016.\n[11] Sepp Hochreiter and J ¨ urgen Schmidhuber, “Long short-\nterm memory, ” Neural computation , vol. 9, no. 8, pp.\n1735–1780, 1997.\n[12] Geoffrey E Hinton and Ruslan R Salakhutdinov, “Repli-\ncated softmax: an undirected topic model, ” in Ad-\nvances in neural information processing systems , 2009,\npp. 1607–1614.\n[13] Hugo Larochelle and Y oshua Bengio, “Classiﬁcation\nusing discriminative restricted boltzmann machines, ” in\nProceedings of the 25th international conference on Ma-\nchine learning . ACM, 2008, pp. 536–543.\n[14] Y u Chen and Mohammed J Zaki, “Kate: K-competitive\nautoencoder for text, ” arXiv preprint arXiv:1705.02033 ,\n2017.\n[15] Zichao Y ang, Diyi Y ang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy, “Hierarchical attention\nnetworks for document classiﬁcation, ” in Proceedings\nof the 2016 conference of the North American chapter\nof the association for computational linguistics: human\nlanguage technologies , 2016, pp. 1480–1489.\n[16] Xiang Zhang, Junbo Zhao, and Y ann LeCun,\n“Character-level convolutional networks for text\nclassiﬁcation, ” in Advances in neural information\nprocessing systems , 2015, pp. 649–657.\n[17] Liu Liu, Kaile Liu, Zhenghai Cong, Jiali Zhao, Y efei\nJi, and Jun He, “Long length document classiﬁcation\nby local convolutional feature aggregation, ” Algorithms,\nvol. 11, no. 8, pp. 109, 2018.\n[18] Ashutosh Adhikari, Achyudh Ram, Raphael T ang, and\nJimmy Lin, “Docbert: Bert for document classiﬁcation, ”\narXiv preprint arXiv:1904.08398 , 2019.\n[19] Daniel Povey, V ijayaditya Peddinti, Daniel Galvez, Pe -\ngah Ghahremani, V imal Manohar, Xingyu Na, Y im-\ning W ang, and Sanjeev Khudanpur, “Purely sequence-\ntrained neural networks for asr based on lattice-free\nmmi., ” in Interspeech, 2016, pp. 2751–2755.\n[20] Diederik P Kingma and Jimmy Ba, “ Adam: A\nmethod for stochastic optimization, ” arXiv preprint\narXiv:1412.6980, 2014.\n[21] Dheeraj Mekala, V ivek Gupta, Bhargavi Paranjape, and\nHarish Karnick, “Scdv: Sparse composite document\nvectors using soft clustering over distributional repre-\nsentations, ” in Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing ,\n2017, pp. 670–680."
}