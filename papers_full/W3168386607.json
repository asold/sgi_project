{
  "title": "Too Much in Common: Shifting of Embeddings in Transformer Language Models and its Implications",
  "url": "https://openalex.org/W3168386607",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3189813609",
      "name": "Daniel Biś",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1471623111",
      "name": "Maksim Podkorytov",
      "affiliations": [
        "Florida State University"
      ]
    },
    {
      "id": "https://openalex.org/A2155972192",
      "name": "Xiu-wen Liu",
      "affiliations": [
        "Florida State University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2048176942",
    "https://openalex.org/W2158139315",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2247359815",
    "https://openalex.org/W2963769536",
    "https://openalex.org/W3034782826",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W2998554035",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963340990",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4288333985",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2321470647",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2946232455",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2251012068",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2955463893",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2964073004",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W4298201654",
    "https://openalex.org/W2996657533",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3091049047",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2601529995",
    "https://openalex.org/W2557283755",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2518186251",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2112184938",
    "https://openalex.org/W2045812729",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2964230347",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2105842272",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2907252220",
    "https://openalex.org/W3035488502",
    "https://openalex.org/W2169610",
    "https://openalex.org/W3105513113",
    "https://openalex.org/W2170682101",
    "https://openalex.org/W3141898517",
    "https://openalex.org/W2963421945",
    "https://openalex.org/W2962964385"
  ],
  "abstract": "The success of language models based on the Transformer architecture appears to be inconsistent with observed anisotropic properties of representations learned by such models. We resolve this by showing, contrary to previous studies, that the representations do not occupy a narrow cone, but rather drift in common directions. At any training step, all of the embeddings except for the ground-truth target embedding are updated with gradient in the same direction. Compounded over the training set, the embeddings drift and share common components, manifested in their shape in all the models we have empirically tested. Our experiments show that isotropy can be restored using a simple transformation.",
  "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 5117–5130\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n5117\nToo Much in Common: Shifting of Embeddings in Transformer Language\nModels and its Implications\nDaniel Bi´s\nFlorida State University\nTallahassee, USA\nbis@cs.fsu.edu\nMaksim Podkorytov\nFlorida State University\nTallahassee, USA\nmaksim@cs.fsu.edu\nXiuwen Liu\nFlorida State University\nTallahassee, USA\nliux@cs.fsu.edu\nAbstract\nThe success of language models based on the\nTransformer architecture appears to be incon-\nsistent with observed anisotropic properties of\nrepresentations learned by such models. We\nresolve this by showing, contrary to previous\nstudies, that the representations do not occupy\na narrow cone, but rather drift in common di-\nrections. At any training step, all of the em-\nbeddings except for the ground-truth target em-\nbedding are updated with gradient in the same\ndirection. Compounded over the training set,\nthe embeddings drift and share common com-\nponents, manifested in their shape in all the\nmodels we have empirically tested. Our ex-\nperiments show that isotropy can be restored\nusing a simple transformation.1\n1 Introduction\nWord embeddings, both static (Mikolov et al.,\n2013a; Pennington et al., 2014) and contextual-\nized (Peters et al., 2018), have been instrumental\nto the progress made in Natural Language Process-\ning over the past decade (Turian et al., 2010; Wu\net al., 2016; Liu et al., 2018; Peters et al., 2018; De-\nvlin et al., 2019). In recent years, language models\nbased on Transformer architecture (Vaswani et al.,\n2017) have led to state-of-the-art performance on\nproblems such as machine translation (Vaswani\net al., 2017), question answering (Devlin et al.,\n2019; Liu et al., 2019b), and Word Sense Disam-\nbiguation (Bevilacqua and Navigli, 2020), among\nothers. However, it has been observed that repre-\nsentations from Transformers exhibit undesirable\nproperties, such as anisotropy, that is tend to occupy\nonly a small subspace of the embedding space. The\nobservation has been documented by a number of\nstudies (Gao et al., 2019; Ethayarajh, 2019; Wang\net al., 2020). A similar property has been iden-\ntiﬁed in the past in static word embeddings (Mu\n1The code and datasets used in this paper are available at\nhttps://github.com/danielbis/tooMuchInCommon.\nand Viswanath, 2018). To address the issues, post-\nprocessing methods (Mu and Viswanath, 2018),\nand regularization terms have been proposed (Gao\net al., 2019; Wang et al., 2019c, 2020). However,\nthe mechanism that leads to undesirable proper-\nties remains unclear. Without understanding the\nmechanism, it is going to be difﬁcult to address the\nfundamental issue properly.\nThe deﬁciencies are most pronounced in the rep-\nresentations of rare words, as we will show in\nSection 4. Performance of pretrained language\nmodels is inconsistent and tends to decrease when\ninput contains rare words (Schick and Schütze,\n2020b,a). Schick and Schütze (2020a) observe\nthat replacing a portion of words in the MNLI\n(Williams et al., 2018) entailment data set with\nless frequent synonyms leads to decrease in per-\nformance of BERT-base and RoBERTa-large by\n30% and 21.8% respectively.2 After enriching rare\nwords with surface-form features and additional\ncontext, Schick and Schütze (2020a) decrease the\nperformance gap to 20.7% for BERT and 17% for\nRoBERTa, but the gap remains large nonetheless.\nWhy do even the large-scale, pretrained language\nmodels struggle to learn good representations of\nrare words? Consider a language model with an\nembedding matrix shared between the input and\noutput layers, a standard setup known as weight\ntying trick (Inan et al., 2017). Intuitively, at any\ntraining step\nt, optimization of the cross-entropy\nloss can be characterized as “pulling\" the target em-\nbedding, wT, closer to the model’s output vector\nht, while “pushing\" all other embeddings,W\\wT,\nin the same direction, away from the output vector\nht. This leads to what we call common enemies\neffect – the effect of the target words producing\ngradients of the same direction for all of the non-\ntarget words. Compounded over the training set,\nthe embeddings drift and share common compo-\nnents, manifested in their shape in all the models\n2Based on the results reported by authors.\n5118\nwe have empirically tested; see Figure 1.\nAlthough Gao et al. (2019) report a closely re-\nlated phenomenon and call it representation degen-\neration, their analysis is based on an assumption\nthat the embedding matrix is learned after all other\nparameters of the model are well-optimized and\nﬁxed, which is not the case in practice. We conduct\nour analysis in a more realistic setting, and arrive\nat different conclusions. We show that embeddings\ndo not occupy a narrow cone, but are shifted in one\ncommon direction and only appear as a cone when\nprojected to a lower dimensional space (Section\n4.1). In fact simply removing the mean vector of\nall embeddings, thus centering them, shifts the em-\nbeddings back onto a more spherical shape. We\nevaluate embeddings, before and after centering,\non four standard benchmarks and observe signiﬁ-\ncant performance improvement across all of them.\nWhy is removing the mean so effective? We ﬁnd\nthat the common enemies effect applies to most, if\nnot all, words in the vocabulary but in non-uniform\nmanner. As language is known to follow an approx-\nimately Zipﬁan distribution (Zipf, 1949; Manning\nand Schütze, 2001; Piantadosi, 2014) even com-\nmon words will not occur frequently in a text cor-\npus, and in result will be often “pushed\" by other\ntarget words in the same direction as rare words.\nConsequently, all embeddings share a signiﬁcant\ncommon direction. We will focus on the analy-\nsis of auto-regressive GPT-2 (Radford et al.,2019)\nand two masked language models, BERT (Devlin\net al., 2019) and RoBERTa (Liu et al., 2019b). Our\ncontributions can be summarized as follows:\n• We show that as word embeddings repeat-\nedly share same direction gradients, they are\nshifted in one dominant direction in the vector\nspace. The effects are the most evident in rep-\nresentations of rare words, but are also present\nin representations of frequent words.\n• The shift causes the distribution of projected\nembeddings to appear as a narrow cone; we\nshow that simply removing the mean vector is\nenough to restore the spherical distribution.\n• We provide empirical evidence of our analy-\nses using state-of-the-art pretrained language\nmodels and demonstrate that removing the\nmean dramatically improves isotropy of the\nrepresentations.\n2 Background\n2.1 Distributed Word Representations\nDistributed representations induce a rich similarity\nspace, in which semantically similar concepts are\nclose in distance (Goodfellow et al., 2016; Bengio\net al., 2003; Mikolov et al., 2013c). In a language\nmodel, the regularities of embeddings space facili-\ntate generalization, assigning a high probability to a\nsequence of words that has never been seen before\nbut consists of words that are similar to words form-\ning an already seen sentence (Bengio et al., 2003;\nMikolov et al., 2013c). Although models such as\nBERT or GPT-2 produce representations from a\nfunction of the entire input sequence, the represen-\ntations are a result of a series of transformations\napplied to the input vectors. Consider an example\nsentence: “The building was dilapidated.\", and the\nsentences resulting from replacing “dilapidated\"\nwith either “ruined\" or “reconditioned\". If the dis-\ntance in the embeddings space between the two\nrather infrequent, but antonymous, words “dilapi-\ndated\" and “reconditioned\" is not larger than the\ndistance between “dilapidated\" and its relatively\nfrequent synonym “ruined\", then by the aforemen-\ntioned generalization principle there is little to no\nreason to believe that the distance will become\nlarger in the output layer.3\n2.2 Tokenization\nDo the subword tokenization methods (Schuster\nand Nakajima, 2012; Wu et al., 2016; Sennrich\net al., 2016; Radford et al., 2019) preserve the word\nfrequency imbalance? Examination of the common\ntokenization methods, such as Byte-Pair Encoding\n(Sennrich et al., 2016) and WordPiece (Schuster\nand Nakajima, 2012; Wu et al., 2016), suggests\nthat subword units induced by tokenization algo-\nrithms exhibit similar frequency imbalance to that\nof full vocabulary. This can be explained by the\ngreedy nature of the vocabulary induction process.\nAlthough different methods use different base vo-\ncabulary symbols to begin with (i.e., Unicode code\npoints, or bytes), all of the methods construct the\nvocabulary through iterative merging of the most\nfrequent symbols. As a result, the most frequent\nunits are preserved as words, while the rare words\nare segmented into subword units. Moreover, the\nwords which are segmented into subword units are\n3In fact, all three sentences are assigned a negative senti-\nment, with scores between 97% to 100% by RoBERTa ﬁne-\ntuned on SST.\n5119\n0 2\nu1\n1\n0\n1\n2\nu2\nRoBERTa-lg\n0.0K\n10.0K\n20.0K\n30.0K\n40.0K\n50.0K\nIndex in Vocabulary\n(a)\n0 2\nu1\n1\n0\n1\nu2\nRoBERTa-lg\n0.0K\n10.0K\n20.0K\n30.0K\n40.0K\n50.0K\nIndex in Vocabulary\n (b)\n0 200 400 600 800 1000\nSingular value index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Singular value\nRoBERTa-lg\noriginal\ncentered (c)\n1 2 3\nu1\n1\n0\n1\nu2\nGPT-2-12L\n0.0K\n10.0K\n20.0K\n30.0K\n40.0K\n50.0K\nIndex in Vocabulary\n(d)\n0 2\nu1\n0.5\n0.0\n0.5\n1.0\nu2\nGPT-2-12L\n0.0K\n10.0K\n20.0K\n30.0K\n40.0K\n50.0K\nIndex in Vocabulary\n (e)\n0 200 400 600 800\nSingular value index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Singular value\nGPT-2-12L\noriginal\ncentered (f)\nFigure 1: Top: RoBERTa-large. Bottom: GPT-2 (12 layers). (1a, 1d): Word embeddings projected onto ﬁrst two\nsingular vectors. (1b, 1e) Centered word embeddings projected onto ﬁrst two singular vectors. (1c, 1f) Singular\nvalues of embedding matrix before and after centering. Centering the embedding matrix increases isotropy of\nembeddings.\ninfrequent to such a degree that even their com-\nbined frequency is orders of magnitude lower than\nfrequency of the most common words.\nWe conﬁrm this empirically by tokenizing the\nCNN News corpus (See et al.,2017; Hermann et al.,\n2015) with WordPiece (used in BERT), revealing\nthat over 30% of the corpus can be accounted for\nusing 13 most frequent tokens, and 50% of the cor-\npus can be accounted for using just 85 tokens. On\nthe other hand, to cover at least 98% of the corpus,\nnearly 15000 tokens are needed. Therefore, we\nconclude that the tokens follow approximately Zip-\nﬁan distribution (Zipf, 1949; Manning and Schütze,\n2001) similar to that of full vocabulary. We pro-\nvide a comparison of frequency distributions of\ntokens and words based on CNN-News corpus in\nAppendix B.4\n3 Learning Language Model\n3.1 Autoregressive Language Models\nGiven a sequence of tokens w = [w1,..., wN] as\ninput, autoregressive (AR) language models assign\na probability p(w) to the sequence using factor-\nization p(w) = ∏N\nt=1 p(wt|w<t). Consequently,\nAR language model is trained by maximizing the\n4The preserved imbalance does not imply that subword\ntokenization is not beneﬁcial to performance of language sys-\ntems on rare words. It may mitigate some of the issues as\nshown in (Sennrich et al., 2016), however recent work demon-\nstrates that it does not solve the problem (Schick and Schütze,\n2020b,a).\nlikelihood under the forward autoregressive factor-\nization:5\nmax\nθ\nlog pθ(w) =\nN∑\nt=1\nlog pθ(wt|w<t) (1)\n=\nN∑\nt=1\nlog\nexp\n(⟨\nhθ(w1:t−1)⊤, e(wt)\n⟩)\n∑V\nw′exp\n(⟨\nhθ(w1:t−1)⊤, e(w′)\n⟩)\n=\nN∑\nt=1\nlog softmax\n(\nhθ(w1:t−1)W⊤)\nlabelt\n,\nwhere hθ(w1:t−1) ∈Rd is the output vector of a\nmodel at position t, θare the model’s parameters,\nW ∈ R|V|×d is the learned embedding matrix,\ne(w) is a function mapping a token to its represen-\ntation from the embedding matrix, and labelt is the\nindex of the t-th target token in the vocabulary. To\nestimate the probability, W maps hθ(w1:t−1) to\nunnormalized scores for every word in the vocab-\nulary V; the scores are subsequently normalized\nby the softmax to a probability distribution over\nthe vocabulary. In this paper, we focus on neu-\nral language models which compute hθ using the\nTransformer architecture, however the mechanisms\nis generally applicable to other common variants\nof language models (Mikolov et al., 2010; Sunder-\nmeyer et al., 2012; Peters et al., 2018).\n5We omit the bias term in softmax for clarity.\n5120\n3.2 Masked Language Modeling\nMasked Language Modeling (MLM) pretraining\nobjective is to maximize the likelihood of masked\ntokens conditioned on the (noisy) input sequence.\nGiven a sequence of tokens w = [w1,..., wN], a\ncorrupted version ˆw is constructed by randomly set-\nting a portion of tokens in w to a special [MASK]\nsymbol. Although MLM estimates the token proba-\nbilities of all masked positions, ¯w, simultaneously\nand renders the factorization from Subsection 3.1\nno longer applicable, the mechanism used to “un-\nmask\" a token differs only slightly from that in AR,\nspeciﬁcally:\nmax\nθ\nlog pθ( ¯w|ˆw) ≈\nN∑\nt=1\nmtlog pθ(wt|ˆw) (2)\n=\nN∑\nt=1\nmtlog\nexp\n(⟨\nhθ( ˆw)⊤\nt , e( ˆwt\n⟩)\n∑V\nw′exp\n(⟨\nhθ( ˆw)⊤\nt , e(w′)\n⟩)\n=\nN∑\nt=1\nmtlog softmax\n(\nhθ( ˆw)W⊤)\nlabelt\n,\nwhere mt = 1 indicates wt is masked, and hθ( ˆw)t\nis the output representations computed as function\nof the full, noisy, input sequence. Note, that the\nmain difference between the equations 1 and 2 is\nthe context used to condition the estimation. Mod-\nels trained with MLM objective, like BERT and\nRoBERTa, compute the output vector utilizing bidi-\nrectional context through the self-attention mecha-\nnism, while the unidirectional models use only the\ncontext to the left of the target token. Moreover,\nonly the probabilities of masked words,\nwi such\nthat wi ∈¯w, are estimated.\n3.3 Learning Rules\nAlthough the two objectives described above differ\nin terms of the distribution modeled (Yang et al.,\n2019), both AR and MLM models rely on the soft-\nmax function and cross-entropy loss. Using the\nnotation established above, the cross-entropy loss\nfunction for an AR model is optimized by minimiz-\ning:\nJ(θ) = −Ew∼data [log pθ(w)] , (3)\nand for a MLM model it takes a form of:\nJ(θ) = −Ew∼data [log pθ( ¯w|ˆw)] . (4)\nThe gradient of the cross-entropy loss with respect\nto the embedding matrix W is a sum of the gradi-\nent ﬂowing through two paths: ﬁrst one is through\nthe output layer where the embeddings are used\nto create the targets for the softmax, the second\npath ﬂows through the encoder stack to the input\nlayer. The gradient ﬂowing through the embedding\nstack to the input layer is complex, and depends on\nminute details of a model. Although its contribu-\ntion is not irrelevant, it is not necessary to illustrate\nthe main point of this section. Thus, we focus on\nthe update rule resulting from the gradient with\nrespect to embeddings in the top layer of a model.\nFor prediction of a token wt, let hθ be the output\nvector of either AR model (at index t−1) or MLM\nmodel (at index t), let y = softmax(ft), where\nft = hθW⊤, and let ˆybe the true probability dis-\ntribution, then:\n∂Jt\n∂W = hθ(ˆ x)⊤\nt ·(y−ˆy). (5)\nThe resulting update rule for the embedding matrix\nis:\nW′= W −η·(h⊤\nθ ·(y−ˆy))\n= W −η·h⊤\nθ y+ η·h⊤\nθ ˆy, (6)\nwhere ηbe the learning rate. Since ˆyis equal to 0\nfor all the indices except for the index of the target\nword wt, all the embeddings will become less sim-\nilar to the representation produced by a model with\nthe exception of the target word embedding. This\nleads to what we deﬁne as the common enemies\neffect – target words producing gradients of the\nsame direction for all of the non-target words. As\nthe parameters θare updated during the optimiza-\ntion process, the hθ changes even when the model\nis provided with the same input. Therefore, the\ndirection of the gradient for the non-target words\nchanges accordingly, but at a particular step the\ndirection of the update is the same for all the non-\ntarget words. This is fundamentally different from\nthe conclusion of Gao et al. (2019), who states that\nthere exists a uniformly negative direction such\nthat its minimization yields a nearly optimal solu-\ntion for rare words’ embeddings. We ﬁnd that the\ncommon enemies effect is the most pronounced in\nthe representations of rare words, which are less\nlikely to appear as targets, but it is evident in all\nembeddings nonetheless.\n4 Methods\n4.1 Geometry of Embeddings\nPrevious studies (Gao et al., 2019; Wang et al.,\n2020) suggest that word embeddings learned by\n5121\n1\n 1 3\n0.5\n1.0\n(a)\nat origin shifted\nsphere location\n0\n25\n50\n75\n100singular value (b)\nFigure 2: A toy illustration of the effect that updates in one direction have on geometry of the representations and\ntheir singular values. The singular values in 2b correspond to the spheres of the same color in 2a. As the sphere\nmoves away from the origin, the gap between the singular values of the points sampled from the sphere increases.\nTransformer-based language models degenerate\nand occupy a narrow cone in the embedding space,\nbut instead we ﬁnd that embeddings simply drift in\na common, dominant direction. The conclusions of\nGao et al. (2019) are strongly inﬂuenced by a rapid\ndecay of singular values of an embedding matrix,\nhowever, a rapid decay of singular values is not a\nsufﬁcient condition to reach such conclusions.\nIn fact, points sampled from a 3D sphere satisfy\nthe condition given above. As at ﬁrst glance this is\nnot entirely obvious, we provide a toy example in\nFigure 2 that illustrates why embeddings appear as\na cone when projected to a low dimensional space.\nWe sample points at random from two spheres, one\ncentered at the origin and one shifted away from\nthe origin (Figure 2a), and perform Singular Value\nDecomposition on the two sets of samples. When\nthe sphere moves away from the origin, the dif-\nference between the two singular values increases\n(Figure 2b).\nSimilarly, the projection of uncentered embed-\ndings (see Figures 1a and 1d) appears as a cone, but\nwhen embeddings are centered around origin (Fig-\nures 1b, 1e), the shape of their projection changes\nto resemble a sphere more than a cone; that is sim-\nply removing the mean vector µof an embedding\nmatrix W, where µ = ∑\nw∈W e(w) / |V|, in-\ncreases the isotropy of embeddings. Optimization\nof a neural language model is certainly more com-\nplex than our toy example. Most of all, the common\nenemy effect is not uniform; the amount by which\neach vector moves in the most dominant direction\ndepends on many factors, among others the size\nof the training corpus, the diversity of the train-\ning corpus, or whether static (BERT) or dynamic\n(RoBERTa) masking is used. In a more general\nsense, the magnitude of the gradient with respect\nto a word vector depends on the value in the logit\ncorresponding to that word, hence the shift will not\nbe uniform.\n4.2 Unused Tokens and Rare Words\nWe hypothesize that as rare words drift in common\ndirection, their embeddings become less discrimi-\nnative than embeddings of frequent words. BERT’s\nvocabulary provides a unique opportunity to inves-\ntigate the contribution of the same direction gradi-\nents to embeddings of particular words. There are\n994 special unusedtokens in BERT’s vocabulary\nthat were not used as inputs or targets during pre-\ntraining, thus all the updates to their representations\nwere in the directions opposite to output vectors.\nAs shown in Figure 3, we observe that cosine simi-\nlarity between the unused tokens and other tokens\nincreases as the frequency decreases. The aver-\nage cosine similarity between unused words and\ntokens in indices [28500-29500]6 is 0.63. In com-\nparison the unused tokens have cosine similarity\nof\n0.27 with tokens in indices [2000-3000] (most\nfrequent tokens, i.e., “to\") but the similarity goes\nup rapidly for tokens other than the most frequent\nones.7 Schick and Schütze (2019), evaluate BERT\nand RoBERTa on a dataset explicitly measuring\nthe ability of MLM models to “unmask\" words of\ndifferent frequencies, and report that both models\nstruggle to “unmask\" rare words. Results presented\nin this section provide an explanation of this behav-\nior and conﬁrm that embeddings of the rare tokens\n6Although frequency depends on a corpus, in general\nhigher index implies lower frequency due to the way BERT’s\nvocabulary is constructed.\n7We observe a similar pattern in RoBERTa using the last\n1000 words in its vocabulary in place of the unused tokens.\n5122\n0 5K 10K 15K 20K 25K 30K\nIndex in the Vocabulary\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nCosine Similarity\nBERT-base-unc.\nCentered BERT-base-unc.\nFigure 3: Cosine similarity between the [unused]\ntokens and words in vocabulary of BERT-base-case\ngrouped into bins of 1000 (i.e., [1000:1999]).\nare most affected by the common enemies effect.\n5 Experiments\nWe validate our theoretical analysis through a se-\nries of experiments on geometric properties of non-\ncontextualized embeddings.\n5.1 Isotropy\nAlthough centering an embedding matrix results\nin a more desirable spectral distribution, tokens of\ncomparable frequency tend to remain clustered in\nthe embedding space, as shown in Fig 1. There-\nfore, we empirically test how much actual gain in\nterms of isotropy is obtained in embeddings of the\ntested models by removing the shared direction.\nMoreover, Mu and Viswanath (2018) show that the\ntop principal components in skip-gram embeddings\n(Mikolov et al., 2013a) correspond to frequency of\nwords and demonstrate that such frequency bias\ncan be mitigated by removing the top principal\ncomponents of an embedding matrix. We evaluate\nthe effectiveness of this approach on embeddings\nfrom Transformer-based models. We use BERT,\nRoBERTa, and GPT-2 in different sizes in our ex-\nperiments.\nSetup: We measure the initial isotropy of embed-\ndings in each of the models, and the isotropy after\nremoving the mean vector µ = ∑\nw∈W w/|V|\nfrom each row of an embedding matrix W, yield-\ning ˜W = W−µ. Next, we use a slightly modiﬁed\napproach of Mu and Viswanath(2018), and remove\nD top principal components from each model’s em-\nbedding matrix to obtain highly isotropic represen-\ntations. Finally, we evaluate whether increasing\nisotropy of embeddings from Transformer-based\nmodels can improve performance on standard em-\nbedding benchmarks.\nDeﬁnitions: To measure isotropy, we use the par-\ntition function deﬁned in (Arora et al., 2016),\nZ(c) =\n∑\nw∈V\nexp(c⊤e(w)), (7)\nwhere e(w) maps a word w to its embedding and\ncis a unit vector. For vectors to be isotropic, the\nvalue of Z(c) should be approximately constant,\naccording to Lemma 2.1 in (Arora et al., 2016).\nBased on this property, we empirically measure the\nisotropy of an embedding matrix W using:\nI(W) = minc∈X Z(c)\nmaxc∈X Z(c), (8)\nwhere I(W) ∈[0,1]. We follow the standard ap-\nproach and deﬁne Xto be the set of eigenvectors\nof W⊤W (Mu and Viswanath, 2018; Wang et al.,\n2020). We remove the top principal components\nusing a modiﬁed version of the post-processing\nmethod proposed by Mu and Viswanath (2018):\n˜Wi = Wi − 1\n|V|\nV∑\nj=1\nWj (9)\nU = PCA( ˜W) (10)\nˆWi = ˜Wi −\nD∑\nj=1\n(U⊤\nj ˜Wi)Uj, (11)\nwhere W is the embedding matrix, ˆW is the post-\nprocessed embedding matrix, and D is the number\nof principal components removed from the original\nmatrix. Mu and Viswanath (2018) use W instead\nof ˜W in the term (U⊤\nj ˜Wi)Uj in eq. 11, but we\nﬁnd the centered version ofWto be more effective.\nFollowing Mu and Viswanath (2018), we set D =\n⌈d/100⌉, where dis the dimensionality of a model.\n5.2 Embedding Benchmarks\nSetup: We evaluate each model’s embedding’s\nperformance on common benchmarks for word\nsimilarity and relatedness before and after post-\nprocessing. We use the following data sets:\n• SimLex-999 (Hill et al., 2015) - measures\nsimilarity, rather than relatedness or associ-\nation.\n• MEN Test Collection (Bruni et al., 2014) -\nmeasures the relatedness of words.\n• WordSim353 (Agirre et al., 2009) - consists\nof two parts, one measures similarity, and the\nother measures relatedness of words.\n5123\nModel I(W) I(W c) I(W r) avg( ||e(w)||2) ||µ||2 ||µ||2/avg(||e(w)||2)\nBERT-base-uncased 0.39 0.98 0.998 1.40 0.94 0.67\nBERT-base-cased 0.59 0.98 0.996 1.29 0.50 0.39\nBERT-large-uncased 0.44 0.97 0.997 1.45 0.80 0.55\nBERT-large-cased 0.52 0.96 0.995 1.53 0.65 0.42\nRoBERTa-base 0.50 0.87 0.959 3.65 0.57 0.16\nRoBERTa-large 0.07 0.64 0.956 4.36 2.53 0.58\nGPT-2 (12 layers) 0.12 0.91 0.969 3.96 2.05 0.52\nGPT-2 (24 layers) 0.52 0.95 0.981 3.68 2.04 0.55\nTable 1: Isotropy, I(W) ∈[0,1], of embeddings from various language models. Centering an embedding matrix\nyields nearly perfectly isotropic embeddings in most of the tested models. Wc stands for a centered matrix, Wr\nstands for an embedding matrix with ⌈d/100⌉top principal components removed. ||µ||2/avg(||e(w)||2 is the ratio\nof the L2 norm of the mean vector, µ, to the average of the L2 norms of word embeddings.\n• Stanford Rare Words (RW) (Luong et al.,\n2013) - measures similarity of words. In this\ndataset at least one word in each pair is a rare\nword.\nThe data sets are designed to measure embeddings’\nability to reﬂect semantic relations. The perfor-\nmance on the data sets is measured by the correla-\ntion between the similarities of the representations\nand the human scores. We ﬁlter out samples con-\nsisting of subword units. Although this results in\ndifferent test sets for different models, our goal\nis not to compare different models’ performance\nbut to validate the beneﬁts of increased isotropy of\nembeddings. We score relations with both cosine\nsimilarity and inner product.\nSchakel and Wilson (2015) show that vectors of\nmore frequent words tend to have smaller norms,\nwhich was conﬁrmed for BERT byPodkorytov et al.\n(2020). As the longer vectors of rare words are\nmost affected by common enemies effect (see Sec-\ntion 4.2), we evaluate a “scaled-centering\" method\nto account for that.\nSpeciﬁcally, we ﬁrst compute the mean vector\nof embeddings normalized to unit length ˆµ =∑\nw∈W\ne(w)\n||e(w)||2\n/ |V|. Then we scale the mean\nvector by the norm of each word embedding before\nsubtracting it, e(w)′= e(w) −||e(w)||2 ˆµ.\n5.3 Results\nIsotropy: We ﬁnd that merely removing the\nmean vector is enough for most models to reach\nnearly perfect isotropy. The results are in Table1.\nThe only exception is RoBERTa-large, which had\nthe lowest initial isotropy. Interestingly,Schick and\nSchütze (2020a) show that RoBERTa-large outper-\nforms BERT models on tasks designed explicitly\nfor rare words. Moreover, according to common\nleaderboards (Wang et al., 2019b,a), RoBERTa per-\nforms best on downstream tasks among the models\nwe analyzed.\nWe stress that the I(W) is an approximation of\nthe degree of isotropy, and should be treated as such\nwhen interpreting its relation to downstream per-\nformance. The idea of the partition function Z(c)\nstates that it’s value should be constant for any vec-\ntor\nc(Arora et al., 2016; Mu and Viswanath, 2018).\nAs there is no closed-form solution for minc∈X\nand maxc∈X, a set of eigenvectors of W⊤W has\nbeen used as Xin previous studies to approximate\nthe isotropy (e.g., Mu and Viswanath, 2018; Wang\net al., 2020). The vectors in X, however, cannot be\nconsidered principal components of W, unless the\nmatrix Whas been centered. Pearson (1901) states\nthat unless the mean of the data has been subtracted,\nthe best ﬁtting hyperplane would pass through the\norigin and not through the centroid. Indeed, for\nRoBERTa-large, the cosine similarity between the\ntop eigenvector of W⊤W and the mean vector is\n0.99.\nAdditionally, as the volume of a cube in Rn\ngrows exponentially with n, it may be sufﬁcient\nfor the embeddings to be isotropic around a point\nlying on a lower dimensional subspace to retain\nthe desired separation. In fact, embeddings from\nRoBERTa-large have an average pairwise cosine\nsimilarity of 0.33 (angle of 70.7°).\nWe speculate that a longer pretraining of\nRoBERTa compared to BERT results in a more\nsigniﬁcant shift of the embeddings in the dominat-\ning directions. Simultaneously, a larger pretraining\ncorpus and a dynamic masking scheme used in\nRoBERTa may result in a more diverse set of shift\ndirections. We leave this line of research for future\nstudies.\nMoreover, Mu and Viswanath (2018) demon-\n5124\nModel CosSim ⟨·,·⟩\nBERT-base-cased 62.29 (+0.00) 60.91 (+0.00)\n+ Cen\ntered 60.44 (−1.85) 60.08 (−0.83)\n+ Cen\ntered-Scaled 62.32 (+0.03) 62.41 (+1.50)\n+ P\nost-Process 65.57 (+3.28) 66.10 (+5.19)\nRoBERTa-base 66.81 (+0.00) 66.01 (+0.00)\n+ Cen\ntered 66.85 (+0.04) 67.03 (+1.02)\n+ Cen\ntered-Scaled 67.02 (+0.21) 66.95 (+0.94)\n+ P\nost-Process 66.87 (+0.06) 67.15 (+1.14)\nGPT-2-small 64.71 (+0.00) 59.45 (+0.00)\n+ Cen\ntered 64.95 (+0.24) 66.04 (+6.59)\n+ Cen\ntered-Scaled 66.57 (+1.86) 67.32 (+7.87)\n+ P\nost-Process 67.85 (+)3.14 67.67 (+8.22)\nModel CosSim ⟨·,·⟩\nBERT-large-cased 61.90 (+0.00) 59.42 (+0.00)\n+ Cen\ntered 58.66 (−3.24) 57.99 (−1.43)\n+ Cen\ntered-Scaled 61.18 (−0.72) 61.05 (+1.63)\n+ P\nost-Process 65.72 (+3.82) 65.89 (+6.47)\nRoBERTa-large 61.29 (+0.00) 44.79 (+0.00)\n+ Cen\ntered 64.09 (+2.80) 63.78 (+18.99)\n+ Cen\ntered-Scaled 65.49 (+4.20) 64.16 (+19.37)\n+ P\nost-Process 64.38 (+3.09) 65.17 (+20.38)\nGPT-2-medium 65.53 (+0.00) 59.20 (+0.00)\n+ Cen\ntered 66.83 (+1.30) 67.90 (+8.70)\n+ Cen\ntered-Scaled 67.95 (+2.42) 68.22 (+9.02)\n+ P\nost-Process 68.05 (+2.52) 67.81 (+8.61)\nTable 2: Average performance (Pearson’s r ×100) of the models on the non-contextual benchmarks (SimLex-\n999, MEN, WordSim353, Stanford Rare Words). Centered stands for embedding matrix centered at origin;\nCentered-Scaled stands for embedding matrix with mean direction, scaled by the norm of each word embed-\nding, subtracted; Post-Process corresponds to the method deﬁned in eq. 11. Results from different models are not\ndirectly comparable due to different tokenization. Best results for each model are underlined. In general, increased\nisotropy results in increased performance. The improvement of RoBERTa-large is more signiﬁcant as its initial\nisotropy is lower. Speciﬁc results for each benchmark can be found in Appendix C.\nstrate that neural language models are capable of\nlearning to remove the mean vector. We leave\nthe question whether Transformer-based language\nmodels perform an implicit representation center-\ning operation to future research.\nEmbedding Benchmarks: We present our re-\nsults on common benchmarks for word similar-\nity and relatedness in Table 2. We report average\nscores from all tasks. The results on individual\ndata sets are available in Appendix C. We observe\nthat removing the mean vector, and consequently\nincreasing the isotropy of embeddings, consistently\nimproves the performance across all models, ex-\ncept for the most isotropic BERT-cased models.\nFurthermore, results in Table 2 demonstrate that\n“scaled-centering\" is more effective than simple\nmean subtraction, and nearly as effective as the\nmore expensive post-processing method. The only\ncase in which “scaled-centering\" does not improve\nperformance is BERT-large-cased with cosine sim-\nilarity as a scoring function.\nPerformance gains are more pronounced when\ninner-product is used as a scoring function, regard-\nless of the model or processing method used. Al-\nthough, initially cosine-similarity yields better re-\nsults, especially for embeddings with greater L2\nnorms, mean subtraction is sufﬁcient to close the\ngap in all but two models (BERT-cased models).8\n8Visualization of distributions of L2 norms of embeddings\nfrom the analyzed models is available in Appendix C.\n6 Discussion\nThere has been a body of literature demonstrating\nsubstantial beneﬁts of improved quality of word\nembeddings on downstream performance (e.g., Mu\nand Viswanath, 2018; Wang et al., 2019c; Gao\net al., 2019; Wang et al., 2020; Schick and Schütze,\n2020a). In particular, Gao et al. (2019) propose to\nadd a cosine similarity regularization to the cross-\nentropy loss to increase the aperture of the cone\nin which embeddings are distributed, and report\nimproved performance on machine translation and\nlanguage modeling. It is straightforward to demon-\nstrate that the cosine regularization proposed by\nGao et al. (2019) is equivalent to minimizing the\nsquared norm of the mean direction of embeddings,\nhence constraining the most signiﬁcant drift direc-\ntion. We provide the derivation of the equivalence\nin Appendix A.\nLarge-margin classiﬁcation has been studied ex-\ntensively, both in NLP (Wang et al., 2019c) and\nmachine learning in general (Weston and Watkins,\n1999; Tsochantaridis et al., 2005). As substantial\nshared components of embeddings will lead to a de-\ncreased classiﬁcation margin in the output softmax\nlayer, our work offers explanation for the fragility\nof pretrained language models reported in the liter-\nature (e.g., Schick and Schütze 2019, 2020a).\nOur analyses show clearly that shifting of the\nembeddings in the embedding space is due to the\ndynamic interactions between the representations\nand the embedding vectors. As the embeddings\n5125\nbecome more similar, the resulting representations\nbecome closer, creating a positive feedback mech-\nanism for the representations to drift collectively.\nIn addition, while isotropy of representations is\ndesirable and has an overall positive impact on per-\nformance, the relationships between isotropy and\nperformance in Table 1 and Table 2 suggest that\nthe role of isotropy in model performance needs to\nbe further analyzed. The dynamics of the interac-\ntions are being further investigated to pinpoint the\nroot cause and their relationship with the model’s\nperformance.\n7 Related Work\nGao et al. (2019) present an insightful derivation\nof uniformly negative gradients for nonapparent\nwords and formulate the optimization of rare words\nas an α-strongly convex problem but make strong\nassumptions that the embedding matrix is learned\nafter all other parameters of the model are well-\noptimized and ﬁxed, which is not the case in prac-\ntice. We do not make such assumptions, providing\na more realistic explanation for the learning pro-\ncess. Wang et al. (2020) propose to reparametrize\nthe embedding matrix using SVD and propose di-\nrectly controlling the decay rate of singular values.\nOur paper’s purpose is inherently different from\nthat of Wang et al. (2020); we recognize that the\nfundamental understanding of the problem is miss-\ning and provide an explanation for the observations\nmade in previous studies. Another line of work\nfocuses on limitations of the softmax. Yang et al.\n(2018) suggest that softmax does not have sufﬁ-\ncient capacity to model the complexity of language.\nZhang et al. (2019) analyze the skip-gram model\nto show that optimization based on cross-entropy\nloss and softmax resembles competitive learning\nin which words compete among each other for the\ncontext vector. This idea is closely related to the\ncommon enemies effect reported in this paper, how-\never, skip-gram seems to mitigate this through neg-\native sampling (Mikolov et al., 2013b) but similar\napproaches do not seem to help Transformer pre-\ntraining (Clark et al., 2020).\nA considerable effort has been made to improve\nperformance of language systems on rare words,\nbut the focus has been on either injecting subword\ninformation in non-contextual representations (Lu-\nong et al., 2013; Lazaridou et al., 2017; Pinter\net al., 2017; Bojanowski et al., 2017), replacing\nrare words’ representations through exploiting their\ncontext (Khodak et al., 2018; Liu et al., 2019a), or\nboth (Schick and Schütze, 2019, 2020a). In com-\nparison, we strive to provide an explanation of the\nunderlying problem, which is necessary to render\nsuch post-hoc ﬁxes no longer necessary.\n8 Conclusion\nWe ﬁnd that the embeddings learned by GPT-2,\nBERT, and RoBERTa do not degenerate into a nar-\nrow cone, as has been suggested in the past, but\ninstead drift in one shared direction. We recognize\nthat target words produce gradients in the same di-\nrection for all the non-target words at each training\nstep. Combined with the unbalanced distribution\nof word frequencies, any two words’ embeddings\nwill be repeatedly updated with gradients of the\nsame direction. As such updates accumulate, the\nembeddings drift and share common components.\nOur experiments show that simply centering the\nembeddings restores a nearly perfectly isotropic\ndistribution of tested models’ embeddings and si-\nmultaneously improves embeddings’ ability to re-\nﬂect semantic relations. This understanding of the\nlearning process dynamics opens exciting avenues\nfor future work, such as improving the most af-\nfected embeddings of rare words and formulation\nof more computationally efﬁcient training objec-\ntives.\nReferences\nEneko Agirre, Enrique Alfonseca, Keith B. Hall, Jana\nKravalova, Marius Pasca, and Aitor Soroa. 2009.\nA study on similarity and relatedness using distri-\nbutional and wordnet-based approaches. In HLT-\nNAACL.\nSanjeev Arora, Yuanzhi Li, Yingyu Liang, Tengyu Ma,\nand Andrej Risteski. 2016. A latent variable model\napproach to PMI-based word embeddings. Transac-\ntions of the Association for Computational Linguis-\ntics, 4:385–399.\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. In J. Mach. Learn. Res.\nMichele Bevilacqua and Roberto Navigli. 2020. Break-\ning through the 80% glass ceiling: Raising the state\nof the art in word sense disambiguation by incor-\nporating knowledge graph information. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 2854–2864,\nOnline. Association for Computational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\n5126\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nElia Bruni, Nam-Khanh Tran, and Marco Baroni. 2014.\nMultimodal distributional semantics. J. Artif. Intell.\nRes., 49:1–47.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the ge-\nometry of bert, elmo, and gpt-2 embeddings. In\nEMNLP/IJCNLP.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tieyan Liu. 2019. Representation degenera-\ntion problem in training natural language generation\nmodels. In International Conference on Learning\nRepresentations.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. Deep Learning. MIT Press. http://www.\ndeeplearningbook.org.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In Advances in neural information\nprocessing systems, pages 1693–1701.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimlex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41:665–695.\nHakan Inan, Khashayar Khosravi, and R. Socher.\n2017. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. ArXiv,\nabs/1611.01462.\nMikhail Khodak, Nikunj Saunshi, Yingyu Liang,\nTengyu Ma, Brandon Stewart, and Sanjeev Arora.\n2018. A la carte embedding: Cheap but effective\ninduction of semantic feature vectors. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 12–22, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nA. Lazaridou, Marco Marelli, and M. Baroni. 2017.\nMultimodal word meaning induction from minimal\nexposure to natural text. Cognitive science, 41 Suppl\n4:677–705.\nQianchu Liu, Diana McCarthy, and Anna Korhonen.\n2019a. Second-order contexts from lexical substi-\ntutes for few-shot learning of word representations.\nIn Proceedings of the Eighth Joint Conference on\nLexical and Computational Semantics (*SEM 2019),\npages 61–67, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nXiaodong Liu, Yelong Shen, Kevin Duh, and Jianfeng\nGao. 2018. Stochastic answer networks for ma-\nchine reading comprehension. In Proceedings of the\n56th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1694–1704, Melbourne, Australia. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. In arXiv:1907.11692.\nThang Luong, Richard Socher, and Christopher D.\nManning. 2013. Better word representations with re-\ncursive neural networks for morphology. In CoNLL.\nChristopher D. Manning and Hinrich Schütze. 2001.\nFoundations of statistical natural language process-\ning. In SGMD.\nTomas Mikolov, Kai Chen, Gregory S. Corrado, and\nJeffrey Dean. 2013a. Efﬁcient estimation of word\nrepresentations in vector space. In ICLR.\nTomas Mikolov, Martin Karaﬁát, Lukás Burget, Jan\nˇCernocký, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In INTER-\nSPEECH.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeffrey Dean. 2013b. Distributed represen-\ntations of words and phrases and their composition-\nality. In NIPS.\nTomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.\n2013c. Linguistic regularities in continuous space\nword representations. In Proceedings of the 2013\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 746–751, Atlanta,\nGeorgia. Association for Computational Linguistics.\nJiaqi Mu and Pramod Viswanath. 2018. All-but-the-\ntop: Simple and effective postprocessing for word\nrepresentations. In International Conference on\nLearning Representations.\nKarl Pearson. 1901. LIII. On lines and planes of closest\nﬁt to systems of points in space. In Philosophical\nMagazine.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL-HLT.\n5127\nS. Piantadosi. 2014. Zipf’s word frequency law in natu-\nral language: A critical review and future directions.\nPsychonomic Bulletin & Review, 21:1112–1130.\nYuval Pinter, Robert Guthrie, and Jacob Eisenstein.\n2017. Mimicking word embeddings using subword\nRNNs. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 102–112, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nMaksim Podkorytov, Daniel Bis, Jinglun Cai, Kobra\nAmirizirtol, and X. Liu. 2020. Effects of architec-\nture and training on embedding geometry and fea-\nture discriminability in bert. 2020 International\nJoint Conference on Neural Networks (IJCNN),\npages 1–8.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario\nAmodei, and Ilya Sutskever. 2019. Language mod-\nels are unsupervised multitask learners.\nAdriaan MJ Schakel and Benjamin J Wilson. 2015.\nMeasuring word signiﬁcance using distributed rep-\nresentations of words. In arXiv:1508.02297.\nTimo Schick and Hinrich Schütze. 2019. Attentive\nmimicking: Better word embeddings by attending\nto informative contexts. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 489–494, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2020a. BERTRAM:\nImproved word embeddings have big impact on con-\ntextualized model performance. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3996–4007, Online.\nAssociation for Computational Linguistics.\nTimo Schick and Hinrich Schütze. 2020b. Rare words:\nA major problem for contextualized embeddings and\nhow to ﬁx it by attentive mimicking. In AAAI.\nM. Schuster and K. Nakajima. 2012. Japanese and ko-\nrean voice search. In 2012 IEEE International Con-\nference on Acoustics, Speech and Signal Processing\n(ICASSP), pages 5149–5152.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. CoRR, abs/1704.04368.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nMartin Sundermeyer, Ralf Schlüter, and Hermann Ney.\n2012. Lstm neural networks for language modeling.\nIn INTERSPEECH.\nIoannis Tsochantaridis, T. Joachims, Thomas Hof-\nmann, and Y . Altun. 2005. Large margin methods\nfor structured and interdependent output variables.J.\nMach. Learn. Res., 6:1453–1484.\nJoseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.\n2010. Word representations: A simple and general\nmethod for semi-supervised learning. In Proceed-\nings of the 48th Annual Meeting of the Association\nfor Computational Linguistics, pages 384–394, Up-\npsala, Sweden. Association for Computational Lin-\nguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019a. Superglue:\nA stickier benchmark for general-purpose language\nunderstanding systems. In NeurIPS.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019b.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nDilin Wang, Chengyue Gong, and Qiang Liu. 2019c.\nImproving neural language modeling via adversarial\ntraining. In Proceedings of the 36th International\nConference on Machine Learning, volume 97 of\nProceedings of Machine Learning Research, pages\n6555–6565, Long Beach, California, USA. PMLR.\nLingxiao Wang, Jing Huang, Kevin Huang, Ziniu Hu,\nGuangtao Wang, and Quanquan Gu. 2020. Improv-\ning neural language generation with spectrum con-\ntrol. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nJ. Weston and C. Watkins. 1999. Support vector\nmachines for multi-class pattern recognition. In\nESANN.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Gregory S. Corrado, Mac-\nduff Hughes, and Jeffrey Dean. 2016. Google’s\n5128\nneural machine translation system: Bridging the\ngap between human and machine translation. In\narXiv:1609.08144.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank RNN language model. In Inter-\nnational Conference on Learning Representations.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems, volume 32. Curran\nAssociates, Inc.\nCanlin Zhang, Xiuwen Liu, and Daniel Bis. 2019.\nAn analysis on the learning rules of the skip-gram\nmodel. 2019 International Joint Conference on Neu-\nral Networks (IJCNN), pages 1–8.\nGeorge Kingsley Zipf. 1949. Human behaviour and the\nprinciple of least effort: an introduction to human\necology.\nA Cosine Regularization as Mean\nDirection Minimization\nIn this section we show the equivalence of Cosine\nRegularization and mean direction minimization.\nCosReg = 1\nN2\nN∑\ni\nN∑\nj̸=i\nˆw⊤\ni ˆwj (12)\n= 1\nN2\n( N∑\ni\nˆwi\n)⊤( N∑\ni\nˆwi\n)\n−N (13)\n= 1\nN2 ||\nN∑\ni\nˆwi||2\n2 −N, (14)\nas N is a constant, minimizing 1\nN||∑N\ni ˆwi||2 is\nequivalent to minimizing the CosReg term.\nB Token Frequency Validation\nFigure 4 validates that the word frequency imbal-\nance is preserved in a corpus tokenized with Word-\nPiece (Schuster and Nakajima, 2012; Wu et al.,\n2016).\nC Additional Experimental Results\nIn Table 3, we provide expanded results on the em-\nbedding benchmarks (see Section 5.2 for details).\nOur experiments reveal a negative 0.61 correla-\ntion between the average norm of the embedding\nvectors and their isotropy. Additionally, the ratio of\nthe L2 norm of the mean vector to the average of\nthe L2 norms of embeddings tends to be larger for\nless isotropic embeddings. Figure 5 shows distribu-\ntions of L2 norms of embeddings from the models\nstudied in this paper. Moreover, Figure 6 compares\nthe effect of centering on the L2 norms of embed-\ndings from the BERT-large-cased and RoBERTa-\nlarge. We leave the relationship between the norms\nof the vectors, their isotropy, and the pretraining\ndetails (e.g., corpus size, number of training steps,\nweight decay) for future studies.\n5129\n(a) Words\n (b) Tokens\nFigure 4: Comparison of word and token frequencies on CNN-DailyMail corpus.\nModel MEN RW Simlex-999 WordSim-Sim WordSim-Rel\n⟨·, ·⟩/cos ⟨·, ·⟩/cos ⟨·, ·⟩/cos ⟨·, ·⟩/cos ⟨·, ·⟩/cos\nBert-base-cased 65.53 / 67.07 61.22 / 63.72 52.52 / 52.37 75.03 / 75.57 50.27 / 52.70\n+ Centered-Scaled 69.32 / 68.36 63.57 / 63.94 47.34 / 47.96 78.58 / 77.36 53.23 / 53.96\n+ Post-Process 72.20 / 70.91 67.71 / 68.20 53.60 / 52.53 79.38 / 78.15 57.63 / 58.04\nBert-large-cased 64.64 / 67.20 58.94 / 61.89 52.68 / 52.91 73.44 / 76.32 47.40 / 51.16\n+ Centered-Scaled 68.63 / 67.69 61.67 / 62.19 48.66 / 48.53 77.47 / 77.45 48.81 / 50.02\n+ Post-Process 72.12 / 71.07 67.22 / 68.07 55.41 / 53.77 78.80 / 78.88 55.90 / 56.79\nGPT-2-small 65.55 / 71.16 58.08 / 63.99 50.43 / 51.45 72.98 / 78.32 50.19 / 58.63\n+ Centered-Scaled 75.55 / 74.10 64.05 / 64.94 51.60 / 50.36 81.41 / 80.61 63.98 / 62.86\n+ Post-Process 76.00 / 75.34 65.30 / 66.56 53.74 / 53.15 80.70 / 80.79 62.62 / 63.40\nGPT-2-medium 65.01 / 71.55 58.55 / 65.22 51.13 / 52.51 72.67 / 78.48 48.65 / 59.90\n+ Centered-Scaled 76.10 / 75.02 64.14 / 65.34 54.22 / 53.30 80.85 / 80.56 65.79 / 65.51\n+ Post-Process 75.92 / 75.25 64.80 / 66.11 54.65 / 54.19 79.99 / 80.25 63.71 / 64.43\nRoBERTa-base 72.83 / 72.70 64.72 / 66.61 55.17 / 54.84 78.00 / 78.62 59.31 / 61.27\n+ Centered-Scaled 74.18 / 73.33 65.38 / 66.35 54.16 / 53.77 79.35 / 79.10 61.66 / 62.53\n+ Post-Process 74.22 / 73.13 65.45 / 66.11 53.87 / 53.35 79.64 / 79.08 62.58 / 62.70\nRoBERTa-large 42.92 / 63.44 49.72 / 64.31 45.75 / 54.54 58.09 / 73.46 27.47 / 50.69\n+ Centered-Scaled 70.55 / 70.95 63.09 / 65.63 55.24 / 54.18 75.72 / 77.34 56.19 / 59.35\n+ Post-Process 72.01 / 70.55 63.11 / 63.64 52.48 / 50.93 77.87 / 77.09 60.36 / 59.71\nTable 3: Performance (Pearson’s r ×100) of the models on the non-contextual benchmarks. Centered-Scaled\nstands for embedding matrix with mean direction, scaled by the norm of each word embedding, subtracted;\nPost-Process corresponds to the method deﬁned in eq. 11. Results from different models are not directly compa-\nrable due to different tokenization.\n5130\n1 2 3 4 5 6\nL2 Norm\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Density\nBERT-base-cased\nBERT-large-cased\nRoBERTa-base\nRoBERTa-large\nGPT-2-12L\nGPT-2-24L\nFigure 5: Kernel density estimation of the L2 norms of embeddings from different models. Models trained on\nlarger corpora and with an increased number of pretraining steps exhibit larger embedding norms.\n1.0 1.5 2.0 2.5 3.0 3.5\nL2 Norm\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0Density\nBERT-large-cased\nBERT-large-cased-centered\n(a) BERT-large-cased\n1 2 3 4 5\nL2 Norm\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6Density\nRoBERTa-large\nRoBERTa-large-centered (b) RoBERTa-large\nFigure 6: The effect of mean subtraction on the distribution of L2 norms of embeddings in BERT-large-cased and\nRoBERTa-large.",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.7932286262512207
    },
    {
      "name": "Transformer",
      "score": 0.7814386487007141
    },
    {
      "name": "Isotropy",
      "score": 0.6497253179550171
    },
    {
      "name": "Computer science",
      "score": 0.6048519015312195
    },
    {
      "name": "Language model",
      "score": 0.5255704522132874
    },
    {
      "name": "Common ground",
      "score": 0.5221307873725891
    },
    {
      "name": "Anisotropy",
      "score": 0.48136526346206665
    },
    {
      "name": "Architecture",
      "score": 0.4363856017589569
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4326948821544647
    },
    {
      "name": "Simple (philosophy)",
      "score": 0.4280017912387848
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40590906143188477
    },
    {
      "name": "Algorithm",
      "score": 0.34929147362709045
    },
    {
      "name": "Natural language processing",
      "score": 0.3347485065460205
    },
    {
      "name": "Physics",
      "score": 0.2184009850025177
    },
    {
      "name": "Programming language",
      "score": 0.11048659682273865
    },
    {
      "name": "Optics",
      "score": 0.08939161896705627
    },
    {
      "name": "Communication",
      "score": 0.07035928964614868
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I103163165",
      "name": "Florida State University",
      "country": "US"
    }
  ]
}