{
    "title": "Towards Sentence Level Inference Attack Against Pre-trained Language Models",
    "url": "https://openalex.org/W4376626934",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5054023418",
            "name": "Kang Gu",
            "affiliations": [
                "Dartmouth College"
            ]
        },
        {
            "id": "https://openalex.org/A5003342694",
            "name": "Ehsanul Kabir",
            "affiliations": [
                "Pennsylvania State University"
            ]
        },
        {
            "id": "https://openalex.org/A5091951921",
            "name": "Neha Ramsurrun",
            "affiliations": [
                "Dartmouth College"
            ]
        },
        {
            "id": "https://openalex.org/A5035399743",
            "name": "Soroush Vosoughi",
            "affiliations": [
                "Dartmouth College"
            ]
        },
        {
            "id": "https://openalex.org/A5032020253",
            "name": "Shagufta Mehnaz",
            "affiliations": [
                "Pennsylvania State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2051267297",
        "https://openalex.org/W2022885785",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W3133280145",
        "https://openalex.org/W2205981794",
        "https://openalex.org/W2606092111",
        "https://openalex.org/W3046764764",
        "https://openalex.org/W3209235691",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2912023992",
        "https://openalex.org/W2089940449",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4318959511",
        "https://openalex.org/W1945076080",
        "https://openalex.org/W2664267452",
        "https://openalex.org/W1649611255",
        "https://openalex.org/W2884280357",
        "https://openalex.org/W4287545908",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4241781926",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3096738375",
        "https://openalex.org/W4229820657",
        "https://openalex.org/W3102516861",
        "https://openalex.org/W2122210511",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W4298201312",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4224920097",
        "https://openalex.org/W3132573559",
        "https://openalex.org/W1473189865",
        "https://openalex.org/W3107001518",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W3027379683",
        "https://openalex.org/W3113998794",
        "https://openalex.org/W4214669216",
        "https://openalex.org/W3183960553",
        "https://openalex.org/W3173769540",
        "https://openalex.org/W3035616549",
        "https://openalex.org/W4287200326",
        "https://openalex.org/W2047828095",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W4221141038",
        "https://openalex.org/W3162938759",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W2108325777",
        "https://openalex.org/W3166890286",
        "https://openalex.org/W4221166069",
        "https://openalex.org/W2890220768",
        "https://openalex.org/W2936473415"
    ],
    "abstract": "In recent years, pre-trained language models (e.g., BERT and GPT) have shown the superior capability of textual representation learning, benefiting from their large architectures and massive training corpora. The industry has also quickly embraced language models to develop various downstream NLP applications. For example, Google has already used BERT to improve its search system. The utility of the language embeddings also brings about potential privacy risks. Prior works have revealed that an adversary can either identify whether a keyword exists or gather a set of possible candidates for each word in a sentence embedding. However, these attacks cannot recover coherent sentences which leak high-level semantic information from the original text. To demonstrate that the adversary can go beyond the word-level attack, we present a novel decoder-based attack, which can reconstruct meaningful text from private embeddings after being pre-trained on a public dataset of the same domain. This attack is more challenging than a word-level attack due to the complexity of sentence structures. We comprehensively evaluate our attack in two domains and with different settings to show its superiority over the baseline attacks. Quantitative experimental results show that our attack can identify up to 3.5X of the number of keywords identified by the baseline attacks. Although our method reconstructs high-quality sentences in many cases, it often produces lower-quality sentences as well. We discuss these cases and the limitations of our method in detail",
    "full_text": "Towards Sentence Level Inference Attack Against Pre-trained\nLanguage Models\nKang Gu\nDartmouth College\nHanover, New Hampshire, USA\nKang.Gu.GR@dartmouth.edu\nEhsanul Kabir\nPenn State University\nUniversity Park, Pennsylvania, USA\nejk5818@psu.edu\nNeha Ramsurrun\nDartmouth College\nHanover, New Hampshire, USA\nNeha.Ramsurrun.23@dartmouth.edu\nSoroush Vosoughi\nDartmouth College\nHanover, New Hampshire, USA\nSoroush.Vosougi@dartmouth.edu\nShagufta Mehnaz\nPenn State University\nUniversity Park, Pennsylvania, USA\nsmehnaz@psu.edu\nABSTRACT\nIn recent years, pre-trained language models (e.g., BERT and GPT)\nhave shown the superior capability of textual representation learn-\ning, benefiting from their large architectures and massive training\ncorpora. The industry has also quickly embraced language mod-\nels to develop various downstream NLP applications. For example,\nGoogle has already used BERT to improve its search system. The\nutility of the language embeddings also brings about potential pri-\nvacy risks. Prior works have revealed that an adversary can either\nidentify whether a keyword exists or gather a set of possible can-\ndidates for each word in a sentence embedding. However, these\nattacks cannot recover coherent sentences which leak high-level\nsemantic information from the original text. To demonstrate that\nthe adversary can go beyond the word-level attack, we present\na novel decoder-based attack, which can reconstruct meaningful\ntext from private embeddings after being pre-trained on a public\ndataset of the same domain. This attack is more challenging than\na word-level attack due to the complexity of sentence structures.\nWe comprehensively evaluate our attack in two domains and with\ndifferent settings to show its superiority over the baseline attacks.\nQuantitative experimental results show that our attack can identify\nup to 3.5X of the number of keywords identified by the baseline\nattacks. Although our method reconstructs high-quality sentences\nin many cases, it often produces lower-quality sentences as well.\nWe discuss these cases and the limitations of our method in detail.\nKEYWORDS\nPre-trained Language Models, Inference Attack, Text Reconstruc-\ntion\n1 INTRODUCTION\nRecent years have witnessed many breakthroughs in Natural Lan-\nguage Processing (NLP) domain. After the release of Transformer\n[58], which is the backbone of pre-trained language models, thou-\nsands of language models have been released. So far, the Hugging\nThis work is licensed under the Creative Commons Attribu-\ntion 4.0 International License. To view a copy of this license\nvisit https://creativecommons.org/licenses/by/4.0/ or send a\nletter to Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\nProceedings on Privacy Enhancing Technologies 2023(3), 62â€“78\nÂ© 2023 Copyright held by the owner/author(s).\nhttps://doi.org/10.56553/popets-2023-0070\nFace API1 supports more than 40000 language models, the most\nsignificant models of which are BERT [12], GPT [5, 42, 43], XLNet\n[61], T5 [44], etc. More recently, transformers have also achieved\nsuccess in computer vision tasks [ 8, 31]. The modern language\nmodels usually rely on very large architectures with millions of\nparameters and pre-training on massive datasets. Unlike traditional\nshallow/small neural networks, pre-trained language models are\nhighly generalized and can be employed as a feature extractor for\nvarious downstream tasks. For example, BERT obtained state-of-\nthe-art results on eleven NLP tasks at the time of its release [12].\nPre-trained language models have attracted wide attention in\nmany industries [38, 54, 59]. For example, since neural machine\ntranslation models are extremely data-hungry, an e-commerce com-\npany can rely on the pre-trained mBART [54] to build the multi-\nlingual translation system for its customers. Furthermore, medi-\ncal records are usually stored in relational databases and require\nspecific queries to retrieve information of interest. However, com-\npleting such queries quickly can be challenging even for medical\nexperts due to the barriers among subdomains of medicine. BERT\ncan efficiently extract a syntax tree from an electric medical record,\nwhich can then be converted to SQL query directly [38]. Although\nthe pre-trained language models have the potentials to be adapted\nto various downstream NLP applications, their privacy leakage\nrisks are concerning, e.g., Carlini et al. [6] showed that generative\nlanguage models (e.g., GPT-2) can memorize samples in the private\ntraining set.\nFurthermore, the sentence embeddings of pre-trained language\nmodels can capture personal information, which may be inferred\nby an adversary. Pan et al. [37] first proposed a keyword inference\nattack against private embeddings. Specifically, they trained a bi-\nnary classifier for each keyword to detect the keywordâ€™s existence\nin the embeddings. However, for this attack to be successful, the\nadversary must know the distribution of keywords in the dataset\nso that they can train the classifiers. In addition, the cost of training\nbinary classifiers increases linearly with the number of keywords.\nMore recently, Song et. al [52] proposed gradient-based embedding\ninversion attack, which requires no knowledge of the target dataset.\nTheir attack consists of two steps: 1) first, the observed deep em-\nbeddings (e.g., BERT) are mapped to a lower space (input space)\nby a learned mapping function, and 2) the adversary solves the\nword distribution at each position of the sentence using gradient\n1https://huggingface.co/models\n62\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\ndescent. However, this method can only recover a set of words\nwithout word order, which plays a crucial role in the formation of\nthe sentence [20]. This attack remains at the word level and the\nproblem of reconstructing at the sentence level is still left open.\nOur work. To further investigate if the adversary can go beyond\nthe previous attacks, we design a novel attack method with an ad-\nversarial decoder, which takes in embeddings as input and attempts\nto generate the original sentences. In practice, an organization, e.g.,\na hospital or an e-commerce company, first needs to convert its\nprivate text datasets into embeddings via a pre-trained language\nmodel to perform various downstream NLP tasks. A third-party ser-\nvice provider with expertise in the downstream NLP tasks will then\nhave access to these embeddings. We aim to investigate whether\nthis third-party access to these embeddings can pose privacy threats\nto the original private text dataset.\nIn our attack setting, the adversary (the third-party service\nprovider itself or another entity working with this third-party ser-\nvice provider) leverages an adversarial decoder to reconstruct the\ntexts of the private embeddings and infer the sensitive information.\nThis adversarial decoder is trained by the adversary on a publicly\navailable dataset from the same domain as the private dataset and\nthe embeddings are produced by the same language model. Note\nthat, in our setting, the adversary does not need to know anything\nabout the target private dataset. The decoder learns the rules of\ngenerating text on public datasets and transfers the rules to the\nprivate dataset. Although previous work [37] reported that a reg-\nular RNN-based decoder cannot recover useful information from\ntext embeddings, we demonstrate through our experiments that\na transformer decoder can reconstruct high-quality sentences in\nsome cases.\nContributions. We first perform a keyword inference attack to\ncompare our decoder-based attack with previous attacks [37, 52].\nIn the original classifier-based attack [37], the adversary aims to\nexploit sensitive keywords (e.g., disease) in the target dataset. How-\never, unlike this work that constrains the number of keywords to\n1010, we take into consideration hundreds of keywords existing in\nthe dataset of interest to simulate real-world scenarios. Next, we\npropose a novel attack, namely, sentence inference attack . In this\nattack, instead of focusing only on keywords, the adversary aims\nto reconstruct the entire sentence. In the cases where the adversary\ndoes not know the exact keywords in the dataset (this happens\nwhen they do not have domain expertise), they can still infer the\nmeaning of the original text by the semantics of the reconstructed\ntext. The contributions of this paper are briefly summarized as\nfollows.\n(1) We design a novel inference attack against sentence embed-\ndings that reconstructs the original input text with minimal\nadversarial knowledge.\n(2) We show that a transformer decoder can accurately recon-\nstruct the text from its sentence embeddings in some cases.\n(3) We extensively evaluate our decoder-based attack and also\ncompare it with existing inference attacks [37, 52]. The code-\nbase and datasets of this work will be released to enable\nreproducibility2.\n2https://github.com/KangGu96/Adv_decoder\n(4) We also discuss the limitations of our method, including the\nlower quality of reconstructions in many cases. Please refer\nto Section 11.2 for details.\n2 RELATED WORK\n2.1 Privacy Attacks against ML\nVarious privacy attacks are conducted against machine learning\napplications [30, 46]. Among various forms of attacks, member-\nship inference attack [ 50, 56, 57] discloses the least information.\nShokri et al. [50] introduced the first membership inference attack\nagainst machine learning models: given a trained model and a data\nrecord, the adversary can determine if the data record was in the\nmodelâ€™s training set. More recently, Shejwalkar et al. [49] studied\nthe susceptibility of text classifiers to membership inference, which\nintroduced user-level membership inference that outperformed the\nexisting attacks on both transformer-based and RNN-based mod-\nels. Besides, attacks on generative models [21, 32] have also been\nexplored.\nThe model inversion attack was first proposed by Fredrikson et\nal. in statistical models [16] and then generalized to deep neural\nnetworks [15]. Unlike membership inference, model inversion at-\ntacks aim to reconstruct partially or fully the private training data\nthat the target model is trained on. Fredrikson et al. [15] proposed\ntwo formulations of model inversion attacks. In the first one, the\nadversary aims to learn a sensitive attribute of an individual whose\ndata are used to train the target model, and whose other attributes\nare known to the adversary [33].\nIn the second formulation, the adversary is given access to a clas-\nsification model and a particular class, and aims to come up with\na typical instance for that class [ 63]. For example, the adversary,\nwhen given access to a model that recognizes different individualsâ€™\nfaces, tries to reconstruct an image that is similar to a target indi-\nvidualâ€™s actual facial image. Besides, the additional knowledge has\nbeen proven to increase the risk of inversion attacks. Zhao et al.\ndeveloped inversion models that can take in model explanations,\noutperforming the attack methods that use model prediction only\n[64]. Chen et al. presented a novel GAN model that can better\ndistill knowledge, which is useful for performing attacks on private\nmodels, from public data [7].\nFurthermore, recent studies demonstrated that model inversion\nattacks could recover texts from the training dataset [6, 37]. In our\nadversarial setting, the reconstruction of texts relies only on the\nsentence embeddings generated by the pre-trained language models.\nWe also explore the impact of additional information (pre-training)\non inversion attacks.\n2.2 Privacy Attacks against Pre-trained\nLanguage Models\nPre-trained language models have become a popular component\nof the current NLP pipeline [41]. However, there are several con-\ncerns about their privacy issues. For example, Bguelin et al. studied\na practical scenario in which users need to continuously update\nthe weights of the language model with modified data [ 4]. Their\nresults implicated that an adversary can infer specific sentences or\nfragments of discourse from the difference between the data used\nto train the model. Furthermore, Nakamura et al. showed that an\n63\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\nadversary with some prior knowledge of the patient could employ\na pre-trained masked BERT model to predict the masked personal\ninformation in the input clinical data [36].\nCarlini et al. extended model inversion attacks to training data\nextraction attacks which aim to reconstruct not just trivial unin-\nformative examples but the verbatim training examples [6]. They\ndemonstrated that GPT-2 (trained on Internet text) could memorize\nand generate hundreds of verbatim text sequences in training data\ngiven several starting words. The most obviously-sensitive sample\ncontained the full name, physical address, email address, phone\nnumber, and fax number of an individual. However, this attack\nonly targeted generative language models but excluded masked\nlanguage models such as BERT.\nMeanwhile, Pan et al. first showed keyword inference attack\non the embeddings of pre-trained language models [ 37]. Specifi-\ncally, an adversary with prior knowledge of the confidential dataset\n(e.g., clinical note) could infer the sensitive keywords within the\nsentence embeddings. However, the attack relied on training a bi-\nnary classifier for each keyword and was tested only in the setting\nof 10 keywords. Song et al. designed a gradient-based inversion\nattack to predict a set of candidates for each token from the sen-\ntence embeddings, without recovering word order [52]. Therefore,\nthe method cannot recover the sentence structure or reveal the\nsemantic information about the sentence.\nAlthough our work also focuses on inference attacks against\nsentence embeddings, it is different from [52] in two major aspects:\n(1) Our method can generate a coherent text sequence that is\nclose to the original sentence, thus revealing more semantic\ninformation than a set of unordered words.\n(2) Our method is naturally more efficient when there exist a\nlarge number of private embeddings. Specifically, the decoder\ngenerates text by querying without any gradients involved.\nIn contrast, [52] relies on gradient descent to compute the\nword distribution for each token in the embeddings.\n3 BACKGROUND\n3.1 Transformer\nTransformer [58] was originally proposed for machine translation\ntask, which later became the backbone of recent language mod-\nels. Unlike traditional sequential models, transformer adopted self-\nattention and multi-head attention mechanisms to capture complex\nsequential dependencies.\nEncoder and decoder are the two components of transformer,\nwhere encoder consists of six encoding layers and decoder consists\nof the same number of decoding layers. In the original machine\ntranslation task, encoder maps the input in language â€˜Aâ€™ to a hid-\nden feature space. Then the decoder projects the hidden states to\nlanguage â€˜Bâ€™. Our reconstruction attack is similar to the decoding\nprocess. In this paper, we employ the capacity of the transformer\ndecoder to reconstruct the original sentence from the sentence\nembedding, including sensitive keywords.\n3.2 Pre-trained Language Models\nLanguage models, which are usually built on transformer architec-\nture, are pre-trained on massive corpus to model the complex text\nstructure. For example, BERT, one of the most popular language\nmodels, was trained using BooksCorpus [3] and English Wikipedia\n[51] with the objective of predicting the masked words and/or the\nnext sentence in a text. Additionally, another significant language\nmodel, GPT-2, was trained on 40GB Internet text with the objective\nof predicting the next word in the text.\nOur paper focuses on reconstructing text from sentence em-\nbeddings generated by language models. Therefore, our method is\nagnostic to the model architecture and training objectives.\n3.3 Sentence Embedding\nGiven a vocabulary Vwhich consists of |V|tokens, a sentence ğ‘ is\ndefined as ğ‘  = [ğ‘¤1,ğ‘¤2,...,ğ‘¤ ğ‘›], where each word (or token) belongs\nto the vocabulary V. We define a mapping Ffrom the sentence to\nthe vector space Rğ‘›Ã—ğ‘‘ğ‘¤ as a sentence embedding function, where\nğ‘›is the number of tokens in the sentence and ğ‘‘ğ‘¤ is the dimension\nof each token vector.\nAlthough there exist various methods (word2vec [34], doc2vec\n[26], etc.) to embed the sentences in NLP domain, Frefers to lan-\nguage models in this paper. Finally, the sentence embedding ğ‘§of\nsentence ğ‘  is obtained by ğ‘§ = F(ğ‘ ),ğ‘§ âˆˆRğ‘›Ã—ğ‘‘ğ‘¤.\nIn some applications, pooling operation is applied to the sen-\ntence embedding to produce a 1-dimensional embedding ğ‘§ âˆˆğ‘…ğ‘‘ğ‘¤.\nHowever, the pooled embeddings are only capable of basic NLP ap-\nplications (e.g., classification), while inadequate for more advanced\napplications such as text understanding, entity extraction, and ques-\ntion answering. We consider unpooled embeddings as the main\ntarget in our experiments, which are also studied in [52].\n3.4 Adversarial Decoder\n3.4.1 Architecture. As mentioned earlier, our adversarial decoder\ninherits the architecture of the transformer decoder. Given a set\nof sentence embeddings ğ‘§ = [ğ‘§1,ğ‘§2,...ğ‘§ğ‘›], the decoder M, and a\nprojection function ğ‘”, our objective is to reconstruct each token of\nthe original sentence ğ‘  = [ğ‘¤1,ğ‘¤2,...,ğ‘¤ ğ‘›]in an autoregressive way:\nâ„1 = M(ğ‘§1) (1)\nğ‘¤1 = ğ‘”(â„1) (2)\nğ‘¤ğ‘˜ = ğ‘”(M(ğ‘§ğ‘˜|ğ‘¤ğ‘˜âˆ’1,ğ‘¤ğ‘˜âˆ’2,...ğ‘¤1)) (3)\nwhere â„1 âˆˆRğ‘‘â„ is the first hidden state output of the decoder ğ‘€,\nğ‘¤ğ‘˜ represents the token at ğ‘˜ğ‘¡â„ position, ğ‘‘â„ is the dimension of the\nhidden state, and ğ‘”is a project function to map the hidden states to\nthe vocabulary. The first token ğ‘¤1 is only conditioned on ğ‘§1, while\nthe following tokens are conditioned on both sentence embeddings\nand the previously predicted tokens.\n3.4.2 Projection. After the output hidden states are obtained from\nthe adversarial decoder, projection will be performed to map the\nhidden states to a probability distribution of tokens.\nGiven a dataset D, its vocabulary is defined as VD. Thus the\nsentence generation will be constrained byVD. Since a pre-trained\nlanguage model usually has a large vocabulary (e.g.,âˆ¼30k for BERT),\nthe unconstrained vocabulary may cause noisy and inaccurate pre-\ndiction. The hidden state â„ğ‘˜ âˆˆ Rğ‘‘â„ is projected to probability\ndistribution by:\nğ‘ƒğ‘˜ = ğ‘Šğ· âˆ—â„ğ‘˜ (4)\n64\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\nwhereğ‘Šğ· âˆˆğ‘…|VD|Ã—ğ‘‘â„ stands for the projection matrix.|VD|is the\ncardinality of the vocabulary VDand ğ‘‘â„ is the size of the hidden\nstate. Therefore, the resulting probability distribution ğ‘ƒğ‘˜ is over\nVD.\n3.4.3 Training Objective. The objective of training is simply to\noptimize the cross-entropy loss between the predicted tokens and\nground-truth tokens as below:\nğ¿(ğœƒ)= âˆ’\nâˆ‘ï¸\nğ‘˜\nğ‘ƒğ‘˜ âˆ—ğ‘Œğ‘˜ (5)\nwhere ğ‘ƒğ‘˜ is the probability distribution in the ğ‘˜ğ‘¡â„ word and ğ‘Œğ‘˜ is\nthe hot encoding of the ground truth word ğ‘˜ğ‘¡â„.\n3.5 Sentence Reconstruction\nFinally, sampling strategy will be adopted in order to generate\nactual words.\n3.5.1 Sampling. Since the projection only yields a spectrum of\npossible tokens, we still need a way to sample the distribution.\nThere exist different sampling methods, the most prominent of\nwhich are greedy search [48], beam search [17] and top-k sampling\n[53]. Although the tokens of interest can be scattered throughout\nthe search space, they have a high likelihood to fall into the list of\nmost possible tokens. In fact, by removing the tail of the distribution,\nthe generation is less likely to go off the topic. Therefore, we employ\ntop-k sampling to avoid repetitive generation and to increase the\ndiversity of generation:\nC= ğ‘ğ‘Ÿğ‘”ğ‘ ğ‘œğ‘Ÿğ‘¡(P)[: ğ‘˜] (6)\nğ‘ğ‘– = ğ‘’ğ‘ƒğ‘ğ‘–/ğ‘¡\nÃ\nğ‘—ğ‘’ğ‘ƒğ‘ğ‘—/ğ‘¡,âˆ€ğ‘ğ‘– âˆˆC (7)\nPâ€²= [ğ‘1,ğ‘2,...ğ‘ğ‘›] (8)\nThe top k indices Cin distribution Pare first retrieved and then\nregularized by the softmax function with temperature ğ‘¡. When ğ‘¡\nequals to 1, it is the same as the normal softmax. When ğ‘¡ is larger\nthan 1, it tends to smooth the distribution. We use high temperature\nto make the model less confident about the prediction. Therefore,\nthe generated sentence will be more diverse and potentially extract\nsensitive tokens.\n3.5.2 Decaying temperature. As discussed above, we prefer to raise\nthe temperature to smooth the distribution. Since the tokens with\nthe highest probabilities may be non-informative due to their high\nfrequencies, such as â€œ[PAD]â€, smoothing process will make other in-\nformative tokens more likely to be sampled. However, maintaining\na high temperature throughout the whole generation process would\ndeviate the generation even when the first few tokens are correct.\nThus, we apply a decaying temperature as in [6], which starts at\nğ‘¡ = 3, gradually decaying to ğ‘¡ = 1 over the first 10 tokens. This\nmakes the model explore more possible \"paths\" at the beginning\nwhile still enabling it to follow a high-confidence path once found.\n3.5.3 Maximum Length. We limit the length of all generated text\nto 15 tokens. As the decoding goes further, the decoderâ€™s capacity to\naccurately predict the words gradually reduces. We have compared\n10, 15, and 20, and observed that the first 10 or 15 words were\nusually relevant and coherent. When extended to 20, the last few\nwords might deviate from the topic and be noisy. The length of 15\nis the balance point for preserving coherence and reconstructing\nmore information.\n4 GENERAL ATTACK WORKFLOW\nThe sentence embeddings are used for a wide range of downstream\ntasks [41]. However, as mentioned earlier, their utility is accompa-\nnied by privacy risks.\nFrom classifier-based attack [37] to gradient-based attack [52],\nit has been shown that the adversary can recover a set of possible\nwords for each token from sentence embeddings. However, each\nset of words is solved independently, which ignores the strong\ndependencies between words that belong to the same sentence.\nThere is still a gap between a large group of unordered words and\na coherent and well-structured sentence.\nTo overcome the limitations mentioned above, we propose a\ngenerative decoder model to attack the embeddings produced by\nlanguage models. Due to the nature of the auto-regressive models,\neach token is conditioned on previous tokens, which makes sure the\nreconstructed sentences are coherent and meaningful. Furthermore,\nthe training cost of the decoder does not multiply by the number\nof keywords.\n4.1 Attack Definition\nWe first compare our decoder-based attack with the methods pro-\nposed by [37] and [52] onkeyword inference attack. Then we propose\na novel attack class, namely sentence inference attack . Compared\nwith keyword inference attack , which only focuses on pre-defined\nkeywords, the new attack can still work when the adversary does\nnot know the secrets inside the dataset.\nFor both attacks, the adversary relies on sentence embeddings to\ninfer sensitive information. Formally, we define a target sentence as\nğ‘  and a publicly available language model as F. Then the sentence\nembeddings of ğ‘  are denoted as ğ‘§ = F(ğ‘ ), ğ‘§ âˆˆRğ‘›Ã—ğ‘‘ğ‘¤, where ğ‘›\nis the number of tokens in ğ‘  and ğ‘‘ğ‘¤ is the dimension of vector.\nThen sentence embeddings ğ‘§is mapped back to tokens by attack\nmodel: ğ‘ â€²= A(ğ‘§). For example, here is a real pair of original and\nreconstructed clinical notes â€œabdominal ultrasound of a single preg-\nnant uterus or first fetusâ€ and â€œabdominal ultrasound of pregnant\npregnancy firstâ€. Although the generated text is not a verbatim\ncopy of the original text, it still maintains the semantics and reveals\nsensitive information such as â€œabdominalâ€ and â€œpregnantâ€.\n4.2 Threat Model\nOur threat model is the same as the previous work [37]. The threat\nmodel is defined as below:\n(1) The adversary has access to the language model as a black-\nbox, which takes a sentence as input and outputs a sequence\nof embeddings.\n65\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\nFigure 1: The general workflow of our attack. EHR stands\nfor Electronic Health Record. The adversary first queries the\npre-trained language model to obtain the public sentence\nembeddings. Then the adversarial decoder is trained using\npublic text/embedding pairs. Finally, the trained decoder is\nemployed to infer the sensitive information from private\nembeddings.\n(2) The adversary has access to a set of embeddings of a private\ndataset, but they do not know the exact sensitive information\nto infer.\n(3) The adversary has access to a public dataset that belongs\nto the same domain as the targeted private dataset, but the\npublic dataset is not guaranteed to share the same distribu-\ntion with the private dataset. This is a valid setting since the\nprivate dataset is unknown.\nWe make different assumptions about the distribution of the\nprivate/public datasets. The detailed description can be found in\nSection 8.\n4.3 Attack Pipeline\nOur general attack workflow is shown in Fig. 1. In our setting, there\nexists a publicly accessible electronic health record (EHR) dataset,\nas well as a privately owned EHR dataset. The pre-trained language\nmodel is accessible as an oracle.\nThe adversary can infer the sensitive information in a private\ndataset by following the four steps given below:\n(1) The adversary queries the pre-trained language model to\nobtain the sentence embeddings of the public dataset.\n(2) The adversary then trains the adversarial decoder using\nthe pairs of public dataset text and sentence embeddings\nobtained in the previous step.\n(3) The adversary has access to the embeddings of private dataset\nprovided by a third-party organization.\n(4) Finally, the adversary employs the trained decoder to recon-\nstruct the private dataset text from the sentence embeddings.\n5 KEYWORD INFERENCE ATTACK\nIn this section, we assume that the sentence can be in an arbitrary\nformat and the adversary knows the keywords or the rule of defin-\ning keywords in the target dataset. Then we compare our methods\nwith baselines on the capacity of identifying keywords.\n5.1 Attack Definition\nThe adversary in the keyword inference attack attempts to infer\nall keywords in an unknown text. Keywords are defined by a well-\nknown rule or expertise in the domain. Therefore, keywords can be\nhighly sensitive and an attack can be a serious threat to real-world\nsystems (e.g., medical & airline domains).\nFormally speaking, we define the rule of keywords asK. Given a\nsentence ğ‘ , its sentence embedding is represented byğ‘§. Arepresents\nour general attack model, which includes the pre-defined decoder\nM. The adversary wants to find out the relationship of A(ğ‘ )â†\nK(ğ‘ ). Our attack modelAdoes not require knowingKfor training,\nthus we only utilize the rule Kat the test stage.\nThe workflow of keyword inference attack is displayed in Fig. 2.\nNote that the process of our decoder generating the text is stochastic.\nWe repeat the generation10 times3 to extract diverse outputs. Then\nwe convert the output text into a list of words Lafter filtering\nstopwords and sorting by frequency. We only keep topğ‘˜ words in\nthe list to reduce the number of irrelevant words. The impact of ğ‘˜,\nwhich is the number of words kept, is further studied in Section\nA.4. We slightly constrain the attack definition here to measure the\nrelationship of Lâ†K( ğ‘ ).\n5.2 Attack Settings\nWhite-box Attack. In this attack context, we focus on the situation\nwhere the public dataset and the private dataset share the same\ndistribution. Therefore, the adversary can safely guess the keywords\nin the private dataset by just examining the public dataset. We show\nthat in the white-box setting, both our decoder-based attack and\nthe baseline attacks can threat the privacy of pre-trained language\nmodels.\nBlack-box Attack. In contrast to the white-box setting, the adver-\nsary in the black-box setting has minimal knowledge of the private\ndataset, which means that the public dataset and private dataset\nmay have different distributions. We use two different datasets in\nthe same domain to mimic this setting. Note that the black-box\nsetting challenges the transferability of the attacks. It is more re-\nalistic that the adversary does not know much information about\nthe private dataset due to privacy protocols. An alternative for the\nadversary is thus to study a public dataset within the same domain\nand transfer the knowledge to the private dataset.\n6 SENTENCE INFERENCE ATTACK\nIn keyword inference attack , we have assumed that the adversary\nhas access to a public dataset, and can learn and target a set of\nkeywords. In the scenario where they do not have targets but still\ntry to infer from the embeddings, we propose a novel attack, namely,\nsentence inference attack , which aims to reconstruct the original\ntext verbatim.\n310 is selected to balance between the performance and efficiency, other choices are\nalso acceptable.\n66\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\nFigure 2: The workflow of keyword inference attack. The adversary first repeats the decoding of the same sentence embedding\n10 times to gather all potential outputs. Then the outputs are passed through a stopwords filter to eliminate stopwords. The\nadversary further converts the sentences into a list of words sorted by frequency. Finally, only top- ğ‘˜ words are kept and\nexamined. Note that ğ‘˜ is a hyperparameter in our experiments.\n6.1 Attack Definition\nThe adversary does not know the sensitive information in the pri-\nvate dataset. Therefore, they can only employ our decoder-based\nattack to infer from the private sentence embeddings in a generative\nway.\nSimilarly, to recover a sentence ğ‘  from private dataset using\nits sentence embeddings ğ‘§ generated by a language model and\nan attack model A, the adversary can solve the below equation\nto maximize the similarity between ğ‘  and reconstructed sentence\nğ‘ â€²= A(ğ‘§):\nğ‘ âˆ—= ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥(\nÃ˜\n1â‰¤ğ‘–â‰¤ğ‘Ÿ\nÎ¦(ğ‘ ,ğ‘ â€²\nğ‘–)) (9)\nwhere [ğ‘ â€²\n1,ğ‘ â€²\n2,..ğ‘ â€²ğ‘Ÿ]are a set of reconstructed sentences, Î¦()is\na similarity function, and ğ‘ âˆ—is the most similar candidate. Note\nthat the decoder generation process is stochastic, therefore, the\ngeneration is repeated ğ‘Ÿ times to capture different cases.\n6.2 Attack Settings\nWe generally follow the settings in keyword inference attack to\nconduct experiments. Both the white-box attack and black-box\nattack reuse the previous settings. Nevertheless, instead of counting\nthe words in reconstructed sentences, we directly measure the\nsimilarity between the original text and the reconstructed text.\nNote that we repeat generation 10 times for each sample, therefore,\nwe select the one that maximizes the similarity function Î¦()as the\nbest candidate.\n7 DATASETS\nIn this section, we introduce real-world datasets from two domains:\nairline and medical. We show that the NLP systems of these two\ndomains are threatened by privacy attacks.\n7.1 Airline\nWith the growing competition in the airline industry, airline compa-\nnies need to constantly improve their service quality to survive the\ncompetition [18]. Online reviews are a popular way for customers\nto share their experiences with flights. With the aid of pre-trained\nlanguage models, airline companies can build automatic tools to\nanalyze customersâ€™ opinions (e.g., topic modeling and sentiment\nanalysis [25]). However, as discussed in [37], an adversary can infer\nvarious sensitive information from text embeddings, including, but\nnot limited to location, flight code, and departure/arrival time.\nSkytrax: This dataset4 contains airline reviews from 2006 to 2019\nfor popular airlines around the world. We extract a subset of about\n30k reviews from this dataset for evaluation by filtering out empty\nor non-English reviews. Without performing downsampling, we\nsimply extract the first sentence of each review and form a new\ndataset. This is because we observe that the first sentences are more\nrelevant to our target of interest (location, time, etc.).\nTwitter US Airline: This dataset5 is originally collected for sen-\ntiment analysis, which contains 14614 tweets to the accounts of\nUS airline companies. We clean and extract a subset of around 5k\ntweets from the original dataset. The preserved subset contains\n20 US city names. Note that the tweets may include a mixture\nof full names of these cities and their acronyms (e.g., Athens vs\nATH), which makes it a challenging dataset for performing privacy\nattacks.\n7.2 Medical\nIn recent years, AI-powered applications have been increasingly\napplied to clinical tasks [ 22]. Specifically, various NLP methods\nhave been proposed for the extraction of clinical pathways [ 60],\nrecognition of biomedical entities [27], patient questions answered\n[11], etc. Although the power of language models can benefit pa-\ntients, an adversary can also capitalize on the text embeddings of\nmedical transcriptions to infer personal health information (e.g.,\nprecise disease sites).\nCMS: This dataset6 is from the Center for Medicare and Medicaid\nServices website, which records information on services and proce-\ndures documented by physicians and other healthcare professionals.\nIn total, there are 5569 unique samples. We use all of these samples\nfor our experiments.\nMT: This dataset7 contains sample medical transcriptions for vari-\nous medical specialties, including surgery, consult, and more. There\n4https://github.com/quankiquanki/skytrax-reviews-dataset\n5https://github.com/benhamner/crowdflower-airline-twitter-sentiment\n6https://data.cms.gov/provider-summary-by-type-of-service/medicare-physician-\nother-practitioners/medicare-physician-other-practitioners-by-provider-and-service\n7https://www.kaggle.com/datasets/tboyle10/medicaltranscriptions\n67\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\nare 5k medical transcriptions in total. We first split each transcrip-\ntion into sentences and then count the medical keywords provided\nby the dataset. We then only keep 12018 sentences with the 100\nmost frequent keywords as a subset so that the size of MT dataset\nis close to that of CMS dataset.\n7.3 Data Pre-processing\nTo performkeyword inference attack , we need to identify all the key-\nwords within each dataset. We introduce our method for labelling\nin this section.\nAirline: For Skytrax dataset, we refer to the World Cities dataset8,\nwhich only lists cities above 15,000 inhabitants. For each airline\nreview, we check which cities exist in the text and keep a list of\nexisting cities. As for Twitter US Airline dataset, we simply label\nthe tweets with the 20 US cities in the same way. After manual\nexamination, there are 962 cities on Skytrax and 20 US cities on\nTwitter.\nMedical: For both medical datasets, we rely on named entity recog-\nnition (NER) model pre-trained on Spacy â€˜en_ner_bionlp13cg_mdâ€™\ncorpus9. We simply apply the NER system to identify biological\nterms within each sample as our medical keywords. After manual\nreview, there exist 1195 keywords and 2377 keywords in CMS and\nMT datasets, respectively.\n8 EVALUATION METRICS\n8.1 Keyword Inference Attack Metrics\nTo compare our decoder-based attack with the baseline attacks\n[37, 52], we first introduce the notion of Reconstruction and slightly\nextend it for our method.\nDefinition 1. If a string ğ‘¡ exists both in the original sentence ğ‘ and\nin the sentence ğ‘ â€²generated by the adversarial decoder M, then\nthe string ğ‘¡ is successfully reconstructed by the decoder M.\nIntuitively, only string ğ‘¡ existing in both ğ‘ â€²and ğ‘  is considered\nvalid. Even if ğ‘¡ is sensitive, it is still false positive if ğ‘¡ âˆ‰ ğ‘ . Recon-\nstruction of the decoder is conditioned on the sentence embedding\nğ‘§, denoted as M(ğ‘§|ğ‘ â€²). Since there are hundreds of target keywords\nin our datasets, simply measuring the attack results with overall\naccuracy or the F-1 score does not accurately reflect the perfor-\nmance of an individual keyword. In addition, showing the detailed\nattack results on each individual text sample is informative, but not\nefficient.\nAs a result, the strength of the attack is measured by how many\nkeywords the adversary can extract in total and how many unique\nkeywords it can extract. Combining these two metrics, we can\nbetter measure the effectiveness and generalizability of the attack.\nDefinition 2. Given a datasetDmade up of sentences[ğ‘ 1,ğ‘ 2,...,ğ‘  ğ‘›],\nlet the adversarial decoder Mreconstruct a set of strings. ğ‘¡ğ‘– = âˆªğ‘¡ğ‘–ğ‘—\nfrom each sentenceğ‘ ğ‘–. Thus, at the level of the datasetD, all strings\nreconstructed by the decoder Mcan be denoted as:\nT=\nÃ˜\n1â‰¤ğ‘–â‰¤ğ‘›\nğ‘¡ğ‘–,ğ‘¡ğ‘– âˆˆğ‘ ğ‘– (10)\n8https://github.com/datasets/world-cities\n9https://allenai.github.io/scispacy/\nOur two metrics are defined on the basis of T. If we slightly\nconstrain the type of strings in Definition 1 to be pre-defined key-\nwords, Twill become the union of all the reconstructed keywords.\nThe first metric, the count of reconstructed keywords , can be\ndenoted as |T|, where |Â·| represents cardinality. Also, the second\nmetric, the number of unique keywords , is then formulated as\n|{T}|, where {}is the set notation.\nMoreover, the proposed metrics also generalize well to classifier-\nbased attack [37]. The union of reconstructed keywords, T, can\nbe calculated by examining the true positive predictions made by\nthe classifier. As for the gradient-based attack [52], we apply the\nsampling strategy elaborated in Section 3.5.1 to the final word\ndistributions to obtain actual sentences, followed by sorting and\nsnapping by ğ‘˜. Therefore, the proposed metrics can be used to\nevaluate it as we evaluate our decoder-based attack.\n8.2 Sentence Inference Attack Metrics\nThe sentence inference attack is evaluated by the similarity function\nÎ¦(). There are various similarity functions, e.g., Manhattan distance,\nEuclidean distance, cosine similarity, etc. We select cosine similarity\nas our metric because of its ability to measure the degree to which\ntwo sentences overlap.\nDefinition 3. Given two vectorized sentences ğ‘and ğ‘, the cosine\nsimilarity ğ‘ğ‘œğ‘ (ğ‘,ğ‘)is defined as:\nğ‘ğ‘œğ‘ (ğ‘,ğ‘)=\n# Â»ğ‘ Â·# Â»ğ‘\nâˆ¥ğ‘âˆ¥Â·âˆ¥ ğ‘âˆ¥ (11)\nFormally, given two sentencesğ‘ and ğ‘§, their joint set of tokens are\n{ğ‘ }âˆª{ğ‘§}. Then the vectorized version of ğ‘  can be obtained by one\nhot encoding: 1) initialize an all zero vectorğ‘with length of{ğ‘ }âˆª{ğ‘§}.\n2) check if ğ‘–ğ‘¡â„ element in the joint set exists in ğ‘ . 3) assign 1 to the\nğ‘–ğ‘¡â„ element in ğ‘ if the condition of last step is met. For example,\ngiven two toy sentences â€œI love roseâ€ and â€œI love lilyâ€, the joint set\nof tokens will be {â€œIâ€, â€œloveâ€, â€œroseâ€, â€œlilyâ€}. The vectorized sentences\nshould be represented as [1,1,1,0]and [1,1,0,1], respectively.\nBesides measuring the overlap between two sets of words, we\nalso consider BLEU and ROUGE [39] as additional metrics since they\nfurther measure the overlap between n-grams. Although BLEU uses\nhigh order n-gram (n>1) matches, it does not consider sentence level\nstructure [29]. E.g., given a pair of original/reconstructed sentences:\nâ€œParis (cdg) to Detroit (dtw)â€ vs â€œParis [PAD] to [PAD] dtwâ€, the\nBLEU score is close to 0. However, the adversary can figure out\nfrom the sentence structure that the subject flew from Paris to dtw.\nHence, we also use word order similarity (WOS) metric [28] as it\nbetter captures the evaluation of sentence structure. The WOS value\nfor the above example is 0.52, suggesting that the reconstructed\nsentence preserves the original sentenceâ€™s structure.\nDefinition 4. Let ğ‘‡ = [ğ‘¤1,ğ‘¤2,ğ‘¤3,...]be the ground truth sentence\nand ğ‘† be the reconstructed sentence with the same length. ğ‘‡ is\nvectorized by mapping function ğ‘“ : ğ‘¤ğ‘– â‡’ğ‘–, where ğ‘¤ğ‘– is the word\nat index ğ‘– in ğ‘‡. As a result, the vectorğ‘‡â€²is simply [1,2,3,...]. ğ‘† is\nvectorized by searching for ğ‘¤ğ‘– in ğ‘†. Suppose ğ‘¤ğ‘– appears at index\nğ‘— in ğ‘†, ğ‘– will be assigned to index ğ‘— in ğ‘†â€². If ğ‘¤ğ‘– is not found in ğ‘ ,\nthe most similar word will be matched with ğ‘¤ğ‘–. The word order\nsimilarity is computed by:\n68\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\nğ‘Šğ‘‚ğ‘†(ğ‘‡,ğ‘† )= 1 âˆ’||ğ‘‡â€²âˆ’ğ‘†â€²||\n||ğ‘‡â€²+ğ‘†â€²|| (12)\nA simple example is ğ‘‡ = â€˜A dog jumps over the foxâ€˜â€ and ğ‘† =\nâ€œA fox jumps over the dog\". The vectorized version will be ğ‘‡â€² =\n[1,2,3,4,5,6]and ğ‘†â€²= [1,6,3,4,5,2]. Finally, the order similarity\nis computed as 0.9.\n9 EXPERIMENTAL EVALUATION\n9.1 White-box Setup\nSince the training set and the test set from the same dataset share\nthe same distribution, We split the datasets into training/test sets\nto mimic the white box setting. The two benchmark systems that\nwe aim to attack are described below:\nâ€¢Airline-Skytrax: Suppose an airline company employs the\npre-trained language models to analyze their reviews in order\nto improve the service quality. We use Skytrax dataset to\nmimic the dataset employed by the company. Our goal is to\ninfer the keywords from the sentence embeddings. We split\nthe dataset into 80%-20% to obtain training (public) and test\n(private) datasets, respectively. All attack models are trained\non the same training set and tested on the same test set.\nâ€¢Medical-CMS: Likewise, a hospital builds a prediagnosis sys-\ntem to guide the patients to the right departments according\nto the textual descriptions of their medical conditions. Sup-\npose the CMS dataset is used in their system. We again split\nthe dataset into 80%-20% to get training and test datasets,\nrespectively. All the attack models are developed on the same\ntrain/test sets for fair comparison.\n9.2 Black-box Setup\nFollowing the setup in Section 4, we already have pre-trained attack\nmodels in airline and medical domain. Our goal in this black-box\nsetting is to evaluate their performance on unknown datasets. Dif-\nferent from white-box setting, training set and test set now are\nfrom two different datasets in the same domain.\nTherefore, the weights of all the pre-trained models are frozen\nat this point. As for the gradient-based attack, the mapping module\n(from deep embedding to lower space) is frozen.\n(1) Airline-Twitter: In the airline domain, we let Skytrax be the\npublic dataset and Twitter be the private dataset. The adver-\nsary attempts to attack this new unknown private airline\nsystem with the pre-trained model that is trained on a known\nairline system.\n(2) Medical-MT: In the medical domain, the CMS dataset is\ntreated as a public dataset and the MT dataset as a private one.\nThe goal of the adversary is to infer the unknown private\ndataset with a pre-trained attack model.\n9.3 Implementation\nBaselines: Note that we discussed that training a binary classifier\nfor each keyword is not practical in our setting, where there exist\nhundreds of keywords. According to [37], the adversary needs to\nbuild a balanced dataset for each classifier, which brings tremendous\ncost due to the keywords we have. As a result, we extend the original\nbinary classifier to a multi-class classifier without modifying their\nmethodology.\nâ€¢Decision Tree (DT): the feature selection criterion is set as\ngini. The two most widely used criterions are gini and infor-\nmation gain and [45] shows that their performance is quite\nsimilar and the criterions differ in only 2% of the cases. We\npick gini as it is computationally less intensive. The rest of\nthe parameters follow the default setting in scikit-learn10.\nâ€¢K-Nearest Neighbor (KNN): the â€œn_neighborsâ€ is set to 5,\nweight function is set at default uniform and the optimizing\nalgorithm is set as auto. Both DT and KNN are implemented\nusing scikit-learn.\nâ€¢Deep Neural Network (DNN): The DNN has two fully con-\nnected layers with 250 and 100 hidden units, respectively.\nThe objective is to minimize the cross-entropy loss. Besides,\nAdam optimizer with batch size of 100 and learning rate of\n1ğ‘’âˆ’4 is employed. Finally, the maximum train epochs are\nset as 250. DNN is implemented using pytorch 11.\nâ€¢Gradient-based Embedding Inversion (GEI): At first, a two-\nlayer MLP is trained to map the deep embeddings to lower\nspace. Then we use gradient descent to solve the problem\nmin(ğ‘Šğ‘‡Â·ğ‘âˆ’ğ‘€(ğ‘§)). Whereğ‘Š is the word embedding matrix,\nğ‘€ is the mapping model, ğ‘§ is the embedding and ğ‘ is the\nsolution. More details are provided in [52].\nAdversarial Decoder : Our adversarial decoder is trained to re-\nconstruct the text from sentence embeddings. The training process\nconsists of two steps:\n(1) Let the public dataset be Dğ‘ğ‘¢ğ‘ğ‘™ğ‘–ğ‘, and the split train/test\nset be Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and Dğ‘¡ğ‘’ğ‘ ğ‘¡, respectively. We first pre-train the\ndecoder on Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› to obtain a generalized model. However,\nthe model Mat this stage is not accurate enough to predict\nthe text. The goal is to let the decoder learn the distribution\nof Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›.\n(2) After the pre-training, we further fine-tune the decoder M\non a subset Dğ‘ ğ‘¢ğ‘ of Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. Specifically, Dğ‘ ğ‘¢ğ‘ is extracted\nonly by keeping the samples with keywords in Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›. This\nfine-tuning step is essential for the decoder to focus on key-\nwords.\nMoreover, the adversarial decoder inherits from the transformer\ndecoder architecture, which consists of 6 decoding layers and 8\nheads. The training objective is minimizing cross-entropy loss and\nthe optimizer is AdamW. The decoder Mis first pre-trained on\npublic dataset for 100 epochs with a learning rate of1ğ‘’âˆ’4 and then\nfine-tuned on the subset of the public dataset with a learning rate\nof 1ğ‘’âˆ’5.\nWe propose two types of decoder: 1) vanilla decoder and 2)\npre-trained decoder. The former is the model without pre-training\nprocess (only trained on Dğ‘ ğ‘¢ğ‘). The latter is the model pre-trained\nin Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› and fine-tuned in Dğ‘ ğ‘¢ğ‘.\nEmbedding Dimension The input dimension of DT, KNN and\nDNN is 768, which is resulted by pooling operation on the original\nsentence embeddingğ‘§ âˆˆğ‘…128Ã—768. While the input dimension of GEI\nand decoder is ğ‘…15Ã—768, since only the first 15 tokens are targeted.\n10https://scikit-learn.org/stable/\n11https://pytorch.org/\n69\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\n9.4 Keyword Inference Results & Discussion\nThe results of white-box and black-box attacks are listed in Table\n1. Vanilla Decoder and Pre-trained Decoder refer to our proposed\nattack and the rest models are baseline attacks. Note that â€˜Countâ€™\nstands for the count of all reconstructed keywords and â€˜Uniqueâ€™\nstands for the number of unique keywords. Furthermore, ğ‘˜, the\nnumber of kept words in our attack, is setğ‘˜ = 20. The ablation study\nof ğ‘˜ is shown in Section A.4. ğ‘˜ = 20 is a relatively low threshold (a\nlower ğ‘˜ makes the attack more efficient) since the vocabulary of\nevery dataset in our experiments contains thousands of tokens.\nNote that 20 is still smaller than 1% of the size of the vocabulary.\nEffectiveness of Attacks : Even when ğ‘˜ is set as 20, the exper-\niment results still highlight the effectiveness of our attack over\nthe classifier-based attacks and GEI in both white-box and black-\nbox settings. For example, in Table 1, our Pre-trained Decoder in\nwhite-box setting outperforms all the baselines on two metrics.\nThe classifier-based baseline attacks achieve comparable results\nto our attacks on Skytrax dataset, which suggests that classifier-\nbased attacks are effective in white-box setting where the private\ndataset and public dataset share the same distribution. Besides,\nDNN outperforms DT and KNN consistently.\nOur Pre-trained Decoder exceeds baselines distinctively on CMS\ndataset. Specifically, our decoder can identify 1093 keywords in\ntotal and 203 unique keywords given the BERTâ€™s embeddings of\nthe private medical transcriptions while DNN can only correctly\npredict 531 keywords in total and only 94 of them are unique. The\nperformance of DNN is merely about 50% of our decoderâ€™s. The\nexperimental results imply that our decoder demonstrates better\ngeneralizability over different domains and embeddings.\nThe visualization of attack results in Table 1 are displayed in\nFigure 3. For each dataset, we select the top 10 most frequent key-\nwords. Specifically, the 10 cities in Skytrax dataset are London,\nParis, Bangkok, Toronto, Sydney, Hong Kong, Manchester, Dubai,\nMelbourne, and Singapore. As for the CMS dataset, the 10 medical\nterms are tissue, spinal, muscle, skin, bladder, heart, bone, blood,\nbrain, and eye. Our Pre-trained Decoder outperforms the baselines\nwith a distinctive margin on many keywords, which also supports\nthat our decoder generalizes better.\nComparison between Attack Settings: It is noticeable that the\nnumbers of keywords inferred in the black-box setting are much\nlower than the counterpart of white-box setting across all the at-\ntacks. As we have discussed, the black-box setting is more realistic\nwhere the adversary does not have information about the private\ndataset. Even if the adversary knows the domain of the dataset, it\ncan still be challenging to define the keywords within the dataset.\nFor instance, given a medical dataset, there may exist thousands\nof keywords (e.g., 1195 in CMS and 2377 in MT), which makes it\nextremely difficult to include all of them.\nTherefore, the black-box setting is a more challenging setting,\nwhich understandably leads to poorer experimental results for all\nthe attacks under experiment. Nevertheless, our Pre-trained De-\ncoder remains the most robust attack method, especially on the\nTwitter dataset and GPT-2 embedding. The results suggest that our\nattack handles this setting better due to its flexibility and generaliz-\nability.\nTransferability of Attacks: According to Table 1, the Pre-trained\nDecoder displays much better transferability in all the cases. To be\nspecific, the gap between Pre-trained Decoder and DNN is further\nenlarged in both domains compared with results of Table 1.\nFor example, all baseline attacks completely fail on the Twit-\nter dataset with GPT-2 embeddings. However, our decoder still\nmemorizes 30 city names in total, and there exist 5 unique city\nnames. When it comes to the MT dataset, our decoder memorizes\nmore than double the total medical terms/unique medical terms\nthat captured by DNN. Based on the above observations, we can\nsafely conclude that our decoder continues to behave more robustly\nin the black-box setting. Its better transferability makes it a more\npowerful threat to the real-world systems.\nThe Impact of Pre-training Pre-training has boosted the per-\nformance of the decoder in all cases significantly. Specifically, pre-\ntrained decoder has identified at least at least30% more keywords in\ntotal than the vanilla decoder. The gap of the number of unique key-\nwords is distinctive too. The results imply that the pre-training is a\nrobust way to strengthen both the generalizabilty and transferbility\nof the decoder.\n9.5 Sentence Inference Results & Discussion\nNote that Table 2 displays the results for sentence inference attack.\nâ€œcosine\" stands for cosine similarity and â€œorder\" stands for word\norder similarity. Due to the limitation of the classifier-based attacks,\nthey are removed from this attack setting.\nQuantitative Results\nAs shown in Table 2, Our Pre-trained Decoder achieves signif-\nicant improvements of not only cosine similarity but also word\norder similarity over GEI on both white-box and black-box settings.\nOne of the key observations is that GEI is biased on the empty\ntokens such as â€œ[PAD]\", which yields final sentences with large\nproportion of noises. This phenomenon is caused by the contex-\ntual learning in lanugage models. The embeddings of pre-trained\nlanguage models are highly convoluted due to self attention mech-\nanisms [58]. The same words in different contexts will be trans-\nformed into different deep embeddings. Therefore, the shallow\nmapping function, which maps deep embeddings to lower space,\nmight be biased with embedding variance of each word, which\nleads to biased recovered sentences.\nCompared with the mapping function, our decoder utilizes de-\npendencies in sentences to invert deep embeddings rather than\nsolve each word independently. As a result, our method is capable\nof recovering much more coherent and informative sentences from\nthe embeddings, therefore capture semantic information.\nComparison between Attack Settings We can observe that the\ncosine similarity scores drop from white-box setting to black-box\nsetting. For example, the mean cosine similarity drops from 0.30 to\n0.17 on CMS and GPT-2 embeddings.\nSimilar to the observation in keyword inference attack, there\nmay exist unseen sentence structures and patterns in the black-box\nsetting , which challenges the flexibility of the attacks. Our Pre-\ntrained Decoder still remains relatively robust in black-box setting,\nindicating it is a more practical privacy threat.\nComparison between Metrics In addition, the mean and std of\ncosine similarity in various configurations do not necessarily agree\n70\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\nTable 1: Keyword Inference Results ( ğ‘˜ = 20)\nTarget Dataset Attack White-box Black-Box\nBERT GPT-2 BERT GPT-2\nCount Unique Count Unique Count Unique Count Unique\nSkytrax\nDT 112 52 218 75 68 11 101 14\nKNN 266 92 219 35 80 11 73 6\nDNN 627 124 1120 173 174 12 430 16\nGEI 128 49 159 51 74 11 132 12\nVanilla Decoder 421 63 679 84 192 32 391 28\nPre-trained Decoder* 835 158 1190 179 322 45 513 48\nTwitter\nDT 51 13 76 15 13 4 0 0\nKNN 92 15 67 12 27 6 0 0\nDNN 187 16 331 17 58 5 0 0\nGEI 87 13 91 14 12 4 0 0\nVanilla Decoder 203 15 215 15 23 3 9 2\nPre-trained Decoder 326 20 387 19 87 9 30 5\nCMS\nDT 206 49 208 51 127 20 73 13\nKNN 446 104 195 54 361 23 117 12\nDNN 531 94 488 93 380 31 511 32\nGEI 221 50 201 47 117 32 109 30\nVanilla Decoder 602 98 374 59 431 67 389 60\nPre-trained Decoder 1093 203 814 163 647 135 592 114\nMT\nDT 465 107 325 92 38 13 44 10\nKNN 519 113 546 127 148 23 19 12\nDNN 967 109 1280 119 293 36 136 27\nGEI 527 95 603 99 51 16 59 15\nVanilla Decoder 842 136 791 128 351 70 213 34\nPre-trained Decoder 1250 157 1463 201 694 104 484 88\nTable 2: Sentence Inference Results. Results on GPT are shown in ().\nTarget Dataset Attack White-box Black-Box\nBERT(GPT) BERT(GPT)\nCosine Order BLEU ROUGE Cosine Order BLEU ROUGE\nSkytrax\nGEI .07(.06) .10(.10) .01(.01) .01(.01) .01(.02) .02(.04) .00(.00) .00(.00)\nVanilla Decoder .10(.12) .33(.35) .05(.05) .06(.07) .07(.05) .15(.13) .02(.02) .03(.02)\nPre-trained Decoder* .25(.21) .52(.51) .13(.11) .12(.12) .18(.17) .40(.37) .10(.10) .11(.10)\nTwitter\nGEI .05(.06) .11(.10) .01(.01) .01(.01) .03(.02) .05(.04) .00(.00) .00(.00)\nVanilla Decoder .09(.08) .21(.18) .04(.03) .05(.05) .05(.03) .09(.06) .02(.01) .02(.01)\nPre-trained Decoder .22(.20) .50(.45) .11(.10) .13(.12) .15(.14) .39(.35) .08(.07) .10(.10)\nCMS\nGEI .12(.10) .20(.18) .02(.01) .03(.03) .05(.03) .06(.05) .00(.00) .01(.00)\nVanilla Decoder .20(.17) .35(.31) .10(.09) .12(.11) .11(.10) .30(.26) .04(.03) .05(.05)\nPre-trained Decoder .36(30) .59(.53) .16(.14) .19(.15) .22(.19) .41(.38) .11(.09) .11(.09)\nMT\nGEI .12(.11) .23(.20) .01(.01) .01(.01) .05(.05) .08(.10) .01(.00) .01(.00)\nVanilla Decoder .23(.25) .46(.49) .14(.16) .15(.16) .10(.11) .19(.24) .06(.07) .05(.05)\nPre-trained Decoder .38(.42) .59(.61) .20(.22) .21(.22) .19(.17) .45(.44) .12(13) .11(.12)\nwith the metrics ofkeyword inference attack according to Table 1. For\nexample, the mean cosine similarity and the count of memorized\nkeywords are 0.25 and 835, respectively, on Skytrax and BERT\nembeddings. Although the mean cosine similarity is0.21 in Skytrax\nand GPT embeddings, the actual count of keywords is1190, which is\nhigher than 835. The reason behind this situation is that the cosine\nsimilarity measures the degree to which two sentences overlap,\ntherefore, it does not focus on any keywords. Higher similarity\nmeans a higher number of words in the reconstructed sentence also\nexist in the original sentence.\nQualitative Results\nTo demonstrate the capacity of sentence inference attack , 10 re-\nconstructed sentences are displayed in Table 3. For instance, the\nreconstructed sentence \"Vaccine pneumonia influenza virus nasal\"\n71\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\n(a) Results of top 10 most frequent cities in Skytrax dataset with\nBERT embeddings\n(b) Results of top 10 most frequent cities in Skytrax dataset with\nGPT-2 embeddings\n(c) Results of top 10 most frequent terms in CMS dataset with\nBERT embeddings\n(d) Results of top 10 most frequent terms in CMS dataset with\nGPT-2 embeddings\nFigure 3: The barplots of the results in Table 1. For each dataset, we show the top 10 most frequent keywords.\nTable 3: 10 Examples of Sentence Inference Attack\nDomain Original Reconstructed (cosine>0.5)\nAirline\nBucharest to amsterdam via Prague Bucharestexpressvia Prague\nSatisfactory Flight from Singapore to Hong Kong Satisfactory Flight to Hong Kong\n30 January Dusseldorf to Leeds Bradford 30 JanuaryManchesterto Leeds Bradford\nLuxembourg to London city return Luxembourg toBrusselscity return\nRome to Toronto July 2013 Rome to Toronto 2013\nMedical\nClosed treatment of broken heel bone Closed treatment boneankle\nInjection of bladder and urinary duct (ureter) for X-ray imaging Injection of bladder kidneyrenalurinary duct\nClosed treatment of fracture and/or dislocation of pelvis and/or sacrumClosed treatment ofsuspensionfracture and dislocation\nTransplantation of donor kidney Transplant donorâ€™ kidney\nVaccine for influenza for nasal administration Vaccinepneumoniainfluenzavirusnasal\ncan allow the adversary to accurately infer that this sample is \"vac-\ncine for pneumonia/influenza for nasal\". Another pair of examples\nis \"Rome to Toronto July 2013\" vs. \"Rome to Toronto 2013\". The\ndecoder has captured most of the information accurately except\nthe month. Noticeably, the pair of \"Transplantation of donor kid-\nney\" versus \"Transplant donorâ€™ kidney\" shows that the decoder\nhas learned to use contraction during training, which implies it\ncaptures the underlying language patterns within the pretraining\ndataset. Therefore, sentence inference attack can threaten the NLP\n72\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\nsystems without any knowledge about the private dataset. The\nadversary can infer the semantics of the original sentence given\na similar reconstructed sentence. In addition to the reconstructed\nsentences in Table 3, we report a few randomly picked examples as\nshown in Table 4. We observe lower coherence and fluency in these\nexamples. This is a limitation of our attack method and we further\ndiscuss this in Section 11. However, note that, such reconstructions\nmay still leak information through their structures that are similar\nto the original sentences. E.g., the reconstructed â€œmanual test of\nhand arm behind legâ€ of the original â€œmanual muscle test of arm,\nleg or trunkâ€ achieves a BLEU score (2-gram) of 0.30 and a WOS of\n0.55. Although the BLEU score is relatively low, the adversary can\nstill infer the tested body regions of the subject.\nThe Impacts of Pre-training In sentence inference attack , pre-\ntraining still makes the decoder generate more similar text than the\nvanilla decoder. We can conclude that pre-training does not only\nmake the decoder more sensitive to keywords but also increase the\naccuracy of reconstruction.\n10 POTENTIAL DEFENSES\n10.1 Differential Privacy\nDifferential Privacy (DP) is a popular technique for protecting in-\nformation about individuals in the dataset [14]. In the domain of\nmachine learning, a differentially private stochastic gradient de-\nscent algorithm [1] has been proposed to reduce the risk of privacy.\nGoogle has already applied DP to large-scale image classification\nsystems while maintaining high accuracy and minimizing compu-\ntational cost [24]. The trade-off between utility and information\nleakage has been further investigated [2]. The main disadvantage\nof ensuring differential privacy is that it typically requires more\nnoise infusion than traditional techniques.\nAs for the language modeling, it is demonstrated that DP can be\nused to train privacy-preserving models in various NLP applications\n[13, 19]. To satisfy the DP algorithm, each training sample in the\ndataset requires a user label. This requirement can be challenging\nfor pre-trained language models since their training data is usually\nscraped from the public Web.\n10.2 Privacy Preserving Mapping\nThe inference attacks against sentence embeddings are based on\nthe key idea that public embeddings and private embeddings be-\nlong to the same embedding space. Privacy Preserving Mapping\n(PPM) provides a way to distort the embeddings before they are\naccessible to the third party [47]. On the one hand, PPM is trained\nto minimize the effectiveness of an inference attack by quantifying\nprivacy leakage. On the other hand, to preserve the utility of the\nembeddings, the distortion of the PPM is constrained by a bound.\nAs a result, PPM can be applied to private embeddings so that at-\ntack models trained on public embeddings will suffer from distorted\nembedding space.\n10.3 Avoid Providing Complete Sentence\nEmbeddings\nIf an organization needs to share the embeddings of its confidential\ndata with a third-party service provider, it can only provide the\nFigure 4: The evaluation of two defenses: privacy-preserving map-\nping and incomplete sentence embeddings.\npooled version of the sequential embeddings or a masked version of\nthe sequential embeddings. The incomplete sentence embeddings\ncan reduce effectiveness of our decoder-based inference attack in\naccurately reconstructing the text. However, the performance of\nsome downstream tasks such as machine translation and named\nentity recognition will also degrade.\n10.4 Evaluation of Potential Defenses\nWe evaluate privacy preserving mapping (PPM) and incomplete\nembedding defenses against the keyword inference attack on the\nSkytrax dataset. Besides, we consider entity recognition as the\ndownstream task to demonstrate the effects of defenses. Formally,\ngiven a sequence of embeddings ğ‘§ = [ğ‘§1,ğ‘§2,...,ğ‘§ ğ‘›]and a sequence\nof labels ğ‘¦ = [ğ‘¦1,ğ‘¦2,...,ğ‘¦ ğ‘›],ğ‘¦ âˆˆ(0,1), where 1 stands for targeted\nkeywords and 0 stands for other words, the goal is to identify all\nthe entities labeled with 1. We train a single-layer RNN model\nwith a hidden state size of 300 to perform entity recognition. The\nperformance is measured by the F-1 score, which represents the\nutility of the task. We report the count of reconstructed keywords\n(normalized) as information leakage.\nFor PPM, we follow the setup in [ 37]. Given a mapping ğ·ğœƒ :\nRğ‘‘ â‡’Rğ‘‘ which is trained to minimize the effectiveness of an\nimaginary adversary Ağœ™, formally, the learning objective is a mini-\nmax game by solvingğ‘šğ‘–ğ‘›ğœƒğ‘šğ‘ğ‘¥ğœ™\nÃAğœ™(ğ·ğœƒ(ğ‘§))+ğœ†||ğ·ğœƒ(ğ‘§)âˆ’ğ‘§||[47].\nThe PPM is implemented as a regularization term in the minimax\ngame so that the distortion of the embeddings is only allowed in a\nlimited radius. Note that a high value of ğœ†leads to a lower privacy\nbudget. For incomplete embeddings, we apply a randomly gener-\nated mask to the embeddings [62], with the masking rate ranging\nfrom 0.1 to 0.9. Higher masking rate leads to sentence embeddings\nwith more unknown tokens.\nAs shown in Figure 4, although both defenses can mitigate our\ndecoder-based attack, they inevitably compromise the utility of\nthe downstream task. We can observe the trade-off between utility\nand privacy for both of the defenses, which suggests that more\nsophisticated defense mechanisms that do not compromise utility\nto this extent need to be explored.\n11 DISCUSSION\n11.1 Practicality of Decoder-based Attack\nWe show that a transformer decoder can reconstruct coherent and\ninformative texts, therefore revealing sensitive information. Com-\npared to a prior classifier-based attack, it is a more practical threat,\nsince the adversary does not need to know the secrets within the\n73\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\ndataset of interest. However, the classifier-based attack requires\nthe adversary to know the keywords in the dataset or have the\nexperience to create a set of keywords. Note that it is not practi-\ncal to make the above assumption in many domains (e.g., medical,\nfinancial, industrial, and more).\nAs for the gradient-based attack, it cannot decode the contextual\nlanguage embeddings and therefore produce noise outputs. Our\nmethod inverts the deep embeddings more accurately to generate\nwell-structured sentences.\nFinally, our method can still achieve robust performances in a\nblack-box setting, while the baselinesâ€™ performances degrade sig-\nnificantly. Since the black-box setting is closer to the real world,\nthe decoder-based attack represents a practical threat to NLP appli-\ncations.\n11.2 Limitations\n11.2.1 Reconstruction from Embeddings is Hard . We have demon-\nstrated that the reconstructed texts can reveal high-level semantic\ninformation. Although our method can reconstruct high-quality\nsentences in many cases, it often produces lower-quality sentences\nas well. Both the proposed decoder-based attack and the previous\ngradient-based attack [52] rely on the accurate prediction of the\nprobability distribution of each word. Hence, there are two chal-\nlenges associated with reconstruction: (1) the word probability is\nusually distributed over a large vocabulary (thousands of tokens),\nwhich makes it hard to guarantee the right word is going to be\nselected, (2) it is difficult to reconstruct long texts. The second chal-\nlenge is due to the fact that as the reconstruction goes further, the\ndependencies between current words and previous words decrease,\nwhich leads to less accurate results.\n11.2.2 Implementation limitations. Although we have shown a\nsuccessful decoder-based attack, there are several limitations of this\nwork that could be explored in the future: (1) We have only tested\nthe transformer decoder in our experiments. The performance of\nother architectures such as RNN [9] may provide more insights. (2)\nThe hyperparameters (e.g., depth, learning rate, number of heads)\nof the decoder follow the default setting, which could have been\nimproved by grid search.\n11.3 Future Work\nTo improve the overall fluency and coherence of the reconstructions\nproduced by our attack, we discuss the following future directions.\n11.3.1 Pre-training the Decoder on Large Corpus. We have demon-\nstrated the impacts of pre-training in previous results. The pre-\ntrained decoder outperforms vanilla decoder on both keyword in-\nference attack and sentence inference attack . However, the size of\nthe dataset Dğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘› is relatively small compared to the size of the\ntraining data from pre-trained language models. The generalizabil-\nity of the decoder can be further improved by pre-training on a\nlarge corpus. This has the potential to boost the quality of recon-\nstructions as well. For example, there exists a gold standard dataset\nin the medical domain, namely, MIMIC-III [23]. MIMIC-III includes\nmore than 1 million caregiver notes of thousands of patients, which\ncan be utilized by the adversary to pre-train the decoder. Such a\npre-trained decoder can threaten many applications in the medical\ndomain.\n11.3.2 Upgrading Decoder Architecture. Currently, our decoder is\ninherited from the transformerâ€™s decoder, which exhibits the ca-\npacity of generating high-quality text. However, with the develop-\nment of language models, more advanced decoder architectures\nare emerging. For example, Transformer-XL [10] was proposed to\nlearn longer-term dependencies, while the vanilla transformer is\nlimited by the fixed-length context required by the input. Therefore,\nTransformer XL can generate more coherent text. If the adversary\nadopts such an advanced decoder, the quality of the reconstructions\nwill likely be enhanced without any other modifications.\n11.3.3 Improving Quality of Decoding. To further improve the qual-\nity of the reconstructed text, we have employed various approaches,\nincluding top-k sampling, decaying temperature, and repetitive\ngeneration. However, there are more factors to consider, such as\nfluency and coherence of the language. Pascualet al. [40] presented\na plug-and-play encoding method: Given a keyword or a topic, it\nadded a shift to the probability distribution over the vocabulary\ntowards semantically similar words. Despite the simplicity of this\napproach, it still enabled GPT-2 to generate more diverse and fluent\nsentences while guaranteeing the appearance of given guide words.\nThe adversary can employ the plug-and-play method to improve\nthe coherence of the decoding.\n11.3.4 Handling Acronyms and Numbers. Acronyms and numbers\nmay carry sensitive information (e.g., airline code and medical\nterms). However, it is challenging to reconstruct those accurately.\nBesides the aid of pre-training, a more sophisticated way to rep-\nresent numbers [55] or acronyms [35] may benefit the decoding\nquality.\n12 CONCLUSION\nIn this paper, we demonstrate that a decoder-based inference attack\ncan recover coherent and informative text from sentence embed-\ndings in some cases. It is a more practical attack since it can not\nonly extract the sensitive keywords but also recover higher-level\nsemantic information.\nOur experiments reveal the superiority of our method against\nthe baselines, especially in the black-box setting, which is closer to\nthe real-world scenario. Even when the adversary does not know\nthe targets in the dataset, they can still infer the semantics of the\noriginal text from the reconstructed text.\nThere are several ways to make the attack stronger, e.g., using a\nmore advanced decoder architecture and pre-training on a larger\ncorpus. In addition to that, we have also discussed some potential\ntechniques to defend against such attacks. For instance, we believe\ndifferentially private training can prevent information leakage from\nembeddings to some extend.\nACKNOWLEDGMENTS\nWe thank the anonymous shepherd and the reviewers for their\nvaluable suggestions. The work reported in this paper has been\nsupported by the startup fund provided by The Pennsylvania State\nUniversity.\n74\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\nREFERENCES\n[1] Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov,\nKunal Talwar, and Li Zhang. 2016. Deep Learning with Differential Privacy. In\nProceedings of the 2016 ACM SIGSAC Conference on Computer and Communications\nSecurity (Vienna, Austria) (CCS â€™16) . Association for Computing Machinery, New\nYork, NY, USA, 308â€“318. https://doi.org/10.1145/2976749.2978318\n[2] MÃ¡rio S. Alvim, Miguel E. AndrÃ©s, Konstantinos Chatzikokolakis, Pierpaolo\nDegano, and Catuscia Palamidessi. 2012. Differential Privacy: On the Trade-Off\nbetween Utility and Information Leakage. In Formal Aspects of Security and Trust ,\nGilles Barthe, Anupam Datta, and Sandro Etalle (Eds.). Springer Berlin Heidelberg,\nBerlin, Heidelberg, 39â€“54.\n[3] Jack Bandy and Nicholas Vincent. 2021. Addressing \"Documentation Debt\" in\nMachine Learning Research: A Retrospective Datasheet for BookCorpus. ArXiv\nabs/2105.05241 (2021).\n[4] Santiago Zanella BÃ©guelin, Lukas Wutschitz, Shruti Tople, Victor RÃ¼hle, Andrew J.\nPaverd, Olga Ohrimenko, Boris KÃ¶pf, and Marc Brockschmidt. 2020. Analyzing\nInformation Leakage of Updates to Natural Language Models. Proceedings of the\n2020 ACM SIGSAC Conference on Computer and Communications Security (2020).\n[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. 2020. Language Models are Few-Shot Learners. ArXiv\nabs/2005.14165 (2020).\n[6] Nicholas Carlini, Florian TramÃ¨r, Eric Wallace, Matthew Jagielski, Ariel Herbert-\nVoss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Xiaodong Song, Ãšlfar\nErlingsson, Alina Oprea, and Colin Raffel. 2021. Extracting Training Data from\nLarge Language Models. In USENIX Security Symposium .\n[7] Si Chen, Mostafa Kahla, Ruoxi Jia, and Guo-Jun Qi. 2021. Knowledge-enriched dis-\ntributional model inversion attacks. In Proceedings of the IEEE/CVF international\nconference on computer vision . 16178â€“16187.\n[8] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi Tian.\n2021. Visformer: The Vision-Friendly Transformer. InProceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV) . 589â€“598.\n[9] Kyunghyun Cho, Bart Van MerriÃ«nboer, Caglar Gulcehre, Dzmitry Bahdanau,\nFethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning phrase\nrepresentations using RNN encoder-decoder for statistical machine translation.\narXiv preprint arXiv:1406.1078 (2014).\n[10] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan\nSalakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond a\nFixed-Length Context. ArXiv abs/1901.02860 (2019).\n[11] Dina Demner-Fushman and Jimmy J. Lin. 2005. Knowledge Extraction for Clinical\nQuestion Answering: Preliminary Results.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\nArXiv abs/1810.04805 (2019).\n[13] Christophe Dupuy, Radhika Arava, Rahul Gupta, and Anna Rumshisky. 2022. An\nEfficient DP-SGD Mechanism for Large Scale NLU Models. In ICASSP 2022-2022\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 4118â€“4122.\n[14] Cynthia Dwork. 2006. Differential Privacy. In Automata, Languages and Pro-\ngramming, Michele Bugliesi, Bart Preneel, Vladimiro Sassone, and Ingo Wegener\n(Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 1â€“12.\n[15] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. 2015. Model Inversion\nAttacks that Exploit Confidence Information and Basic Countermeasures. InCCS.\n1322â€“1333. https://doi.org/10.1145/2810103.2813677\n[16] Matt Fredrikson, Eric Lantz, Somesh Jha, Simon M Lin, David Page, and Thomas\nRistenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case Study of\nPersonalized Warfarin Dosing. Proceedings of the USENIX Security Symposium.\nUNIX Security Symposium 2014 (2014), 17â€“32.\n[17] Markus Freitag and Yaser Al-Onaizan. 2017. Beam Search Strategies for Neural\nMachine Translation. In NMT@ACL.\n[18] Daniel Greenfield. 2014. Competition and service quality: New evidence from\nthe airline industry. Economics of Transportation 3, 1 (2014), 80â€“89. https:\n//doi.org/10.1016/j.ecotra.2013.12.005 Special Issue on Airlines and Airports.\n[19] Ivan Habernal. 2021. When differential privacy meets NLP: The devil is in the\ndetail. In EMNLP.\n[20] John A Hawkins. 2014. Word order universals . Vol. 3. Elsevier.\n[21] Sorami Hisamoto, Matt Post, and Kevin Duh. 2019. Membership Inference Attacks\non Sequence-to-Sequence Models. CoRR abs/1904.05506 (2019). arXiv:1904.05506\nhttp://arxiv.org/abs/1904.05506\n[22] Fei Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang, Qiang\nDong, Haipeng Shen, and Yongjun Wang. 2017. Artificial intelligence in health-\ncare: past, present and future. Stroke and Vascular Neurology 2 (2017), 230 â€“\n243.\n[23] Alistair Johnson, Tom Pollard, Lu Shen, Li-wei Lehman, Mengling Feng, Moham-\nmad Ghassemi, Benjamin Moody, Peter Szolovits, Leo Celi, and Roger Mark. 2016.\nMIMIC-III, a freely accessible critical care database. Scientific Data 3 (05 2016),\n160035. https://doi.org/10.1038/sdata.2016.35\n[24] Alexey Kurakin, Steve Chien, Shuang Song, Roxana Geambasu, Andreas Terzis,\nand Abhradeep Thakurta. 2022. Toward training at imagenet scale with differen-\ntial privacy. arXiv preprint arXiv:2201.12328 (2022).\n[25] Hye-Jin Kwon, Hyun-Jeong Ban, Jae-Kyoon Jun, and Hak-Seon Kim. 2021. Topic\nModeling and Sentiment Analysis of Online Review for Airlines. Information 12,\n2 (2021). https://doi.org/10.3390/info12020078\n[26] Quoc V. Le and Tomas Mikolov. 2014. Distributed Representations of Sentences\nand Documents. In ICML.\n[27] Lishuang Li, Liuke Jin, Zhenchao Jiang, Dingxin Song, and Degen Huang. 2015.\nBiomedical named entity recognition based on extended Recurrent Neural Net-\nworks. In 2015 IEEE International Conference on Bioinformatics and Biomedicine\n(BIBM). 649â€“652. https://doi.org/10.1109/BIBM.2015.7359761\n[28] Yuhua Li, Zuhair Bandar, David McLean, and James Oâ€™Shea. 2004. A Method for\nMeasuring Sentence Similarity and its Application to Conversational Agents.\n[29] Chin-Yew Lin and Franz Josef Och. 2004. Automatic Evaluation of Machine Trans-\nlation Quality Using Longest Common Subsequence and Skip-Bigram Statistics.\nIn Proceedings of the 42nd Annual Meeting of the Association for Computational\nLinguistics (ACL-04). Barcelona, Spain, 605â€“612. https://doi.org/10.3115/1218955.\n1219032\n[30] B. Liu, Ming Ding, Sina Shaham, Wenny Rahayu, Farhad Farokhi, and Zihuai Lin.\n2020. When Machine Learning Meets Privacy: A Survey and Outlook. ArXiv\nabs/2011.11819 (2020).\n[31] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer\nUsing Shifted Windows. In Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) . 10012â€“10022.\n[32] Saeed Mahloujifar, Huseyin A. Inan, Melissa Chase, Esha Ghosh, and Marcello\nHasegawa. 2021. Membership Inference on Word Embedding and Beyond. CoRR\nabs/2106.11384 (2021). arXiv:2106.11384 https://arxiv.org/abs/2106.11384\n[33] Shagufta Mehnaz, Sayanton V. Dibbo, Ehsanul Kabir, Ninghui Li, and Elisa Bertino.\n2022. Are Your Sensitive Attributes Private? Novel Model Inversion Attribute\nInference Attacks on Classification Models. In 31st USENIX Security Symposium\n(USENIX Security 22) . USENIX Association, Boston, MA, 4579â€“4596. https:\n//www.usenix.org/conference/usenixsecurity22/presentation/mehnaz\n[34] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nestimation of word representations in vector space.arXiv preprint arXiv:1301.3781\n(2013).\n[35] Dana Movshovitz-Attias and William W. Cohen. 2012. Alignment-HMM-Based\nExtraction of Abbreviations from Biomedical Text. In Proceedings of the 2012\nWorkshop on Biomedical Natural Language Processing (Montreal, Canada) (BioNLP\nâ€™12). Association for Computational Linguistics, USA, 47â€“55.\n[36] Yuta Nakamura, Shouhei Hanaoka, Yukihiro Nomura, Naoto Hayashi, Osamu\nAbe, Shuntaro Yada, Shoko Wakamiya, Eiji Aramaki The University of Tokyo,\nNara Institute of Science, Technology, The Department of Radiology, The Univer-\nsity of Tokyo Hospital, The Department of Radiology, and Preventive Medicine.\n2021. KART: Privacy Leakage Framework of Language Models Pre-trained with\nClinical Records. ArXiv abs/2101.00036 (2021).\n[37] Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. 2020. Privacy Risks of General-\nPurpose Language Models. In 2020 IEEE Symposium on Security and Privacy (SP) .\n1314â€“1331. https://doi.org/10.1109/SP40000.2020.00095\n[38] Youcheng Pan, Chenghao Wang, Baotian Hu, Yang Xiang, Xiaolong Wang, Qing-\ncai Chen, Junjie Chen, and Jingcheng Du. 2021. A BERT-Based Generation Model\nto Transform Medical Texts to SQL Queries for Electronic Medical Records:\nModel Development and Validation. JMIR Med Inform 9, 12 (8 Dec 2021), e32698.\nhttps://doi.org/10.2196/32698\n[39] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU:\nA Method for Automatic Evaluation of Machine Translation. In Proceedings of\nthe 40th Annual Meeting on Association for Computational Linguistics (Philadel-\nphia, Pennsylvania) (ACL â€™02). Association for Computational Linguistics, USA,\n311â€“318.\n[40] Damian Pascual, Beni Egressy, Clara Meister, Ryan Cotterell, and Roger Wat-\ntenhofer. 2021. Keyword2Text: A Plug-and-Play Method for Controlled Text\nGeneration. In Findings of the Association for Computational Linguistics: EMNLP\n2021. Association for Computational Linguistics.\n[41] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing\nHuang. 2020. Pre-trained Models for Natural Language Processing: A Survey.\nArXiv abs/2003.08271 (2020).\n[42] Alec Radford and Karthik Narasimhan. 2018. Improving Language Understanding\nby Generative Pre-Training. OpenAI.\n[43] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya\nSutskever. 2019. Language Models are Unsupervised Multitask Learners.OpenAI.\n[44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. Exploring the\nLimits of Transfer Learning with a Unified Text-to-Text Transformer. CoRR\n75\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\nabs/1910.10683 (2019). arXiv:1910.10683 http://arxiv.org/abs/1910.10683\n[45] Laura Elena Raileanu and Kilian Stoffel. 2004. Theoretical comparison between\nthe gini index and information gain criteria. Annals of Mathematics and Artificial\nIntelligence 41, 1 (2004), 77â€“93.\n[46] Maria Rigaki and SebastiÃ¡n GarcÃ­a. 2020. A Survey of Privacy Attacks in Machine\nLearning. ArXiv abs/2007.07646 (2020).\n[47] Salman Salamatian, Amy Zhang, FlÃ¡vio du Pin Calmon, Sandilya Bhamidipati,\nNadia Fawaz, Branislav Kveton, Pedro Oliveira, and Nina Taft. 2015. Managing\nYour Private and Public Data: Bringing Down Inference Attacks Against Your\nPrivacy. IEEE Journal of Selected Topics in Signal Processing 9 (2015), 1240â€“1255.\n[48] Chenze Shao, Yang Feng, and Xilin Chen. 2018. Greedy Search with Probabilistic\nN-gram Matching for Neural Machine Translation. In EMNLP.\n[49] Virat Shejwalkar, Huseyin A. Inan, Amir Houmansadr, and Robert Sim. 2021.\nMembership Inference Attacks Against NLP Classification Models. In NIPS.\n[50] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Mem-\nbership Inference Attacks Against Machine Learning Models. In 2017 IEEE Sym-\nposium on Security and Privacy (SP) . 3â€“18. https://doi.org/10.1109/SP.2017.41\n[51] Harshdeep Singh, Robert West, and Giovanni Colavizza. 2020. Wikipedia citations:\nA comprehensive data set of citations with identifiers extracted from English\nWikipedia. Quantitative Science Studies (2020), 1â€“19.\n[52] Congzheng Song and Ananth Raghunathan. 2020. Information leakage in em-\nbedding models. In Proceedings of the 2020 ACM SIGSAC Conference on Computer\nand Communications Security . 377â€“390.\n[53] Max Spero. 2019. Improved Beam Search Diversity for Neural Machine Transla-\ntion with k-DPP Sampling.\n[54] Raymond Hendy Susanto, Dongzhe Wang, Sunil Yadav, Mausam Jain, and Ohnmar\nHtun. 2021. Rakutenâ€™s Participation in WAT 2021: Examining the Effectiveness\nof Pre-trained Models for Multilingual and Multimodal Machine Translation. In\nProceedings of the 8th Workshop on Asian Translation (WAT2021) . Association for\nComputational Linguistics, Online, 96â€“105. https://doi.org/10.18653/v1/2021.wat-\n1.9\n[55] Avijit Thawani, Jay Pujara, Pedro A Szekely, and Filip Ilievski. 2021. Representing\nnumbers in NLP: a survey and a vision. arXiv preprint arXiv:2103.13136 (2021).\n[56] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. 2018.\nTowards Demystifying Membership Inference Attacks. ArXiv abs/1807.09173\n(2018).\n[57] Stacey Truex, Ling Liu, Mehmet Emre Gursoy, Lei Yu, and Wenqi Wei. 2021.\nDemystifying Membership Inference Attacks in Machine Learning as a Service.\nIEEE Transactions on Services Computing 14, 6 (2021), 2073â€“2089. https://doi.org/\n10.1109/TSC.2019.2897554\n[58] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\nNeed. ArXiv abs/1706.03762 (2017).\n[59] Yungao Xie, Hongying Wen, and Q. Yang. 2021. Ternary Sentiment Classification\nof Airline Passengersâ€™ Twitter Text Based on BERT.Journal of Physics: Conference\nSeries 1813 (2021).\n[60] Wei Yang and Qiang Su. 2014. Process mining for clinical pathway: Literature\nreview and future directions. In 2014 11th International Conference on Service\nSystems and Service Management (ICSSSM) . 1â€“5. https://doi.org/10.1109/ICSSSM.\n2014.6943412\n[61] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,\nand Quoc V. Le. 2019. XLNet: Generalized Autoregressive Pretraining for Language\nUnderstanding. Curran Associates Inc., Red Hook, NY, USA.\n[62] Jiehang Zeng, Xiaoqing Zheng, Jianhan Xu, Linyang Li, Liping Yuan, and Xuan-\njing Huang. 2021. Certified robustness to text adversarial attacks by randomized\n[mask]. arXiv preprint arXiv:2105.03743 (2021).\n[63] Yuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao Wang, Bo Li, and Dawn Song.\n2020. The Secret Revealer: Generative Model-Inversion Attacks Against Deep\nNeural Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) .\n[64] Xuejun Zhao, Wencan Zhang, Xiaokui Xiao, and Brian Lim. 2021. Exploiting\nexplanations for model inversion attacks. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision . 682â€“692.\nA APPENDIX\nA.1 The Effect of the Size of Private Dataset\nTo verify the generalizabilty of our method, we conduct control\nstudy on the size of MT dataset and show the results in Figure\n5. It is noticeable that the attack performance is roughly linearly\ncorrelated to the size of the dataset, which suggests that our attack\ngeneralizes well and the extent of information leakage scales up\nwith the size of the dataset.\nFigure 5: The attack performance on MT dataset of varying\nsize\nA.2 Comparison with Binary Classifiers\nWe have shown that our decoder-based attack is superior to multi-\nclass-classifier-based attack in various settings. However, extending\nbinary classifier to multi-class classifier may jeopardize its perfor-\nmance for scalability. To compare our attack directly with binary\nclassifier attack, we randomly sample 10 keywords from the each\ntraining dataset then train 10 binary classifiers to perform infer-\nence attack. Specifically, we show the results of both white-box\nand black-box settings on Twitter dataset (airline) and MT dataset\n(medical).\nAs displayed in Figure 6, the binary classifiers perform better\nthan the decoder on most keywords (e.g. Los Angeles and Orlando)\nin the white-box setting. It is reasonable since binary classification is\na relatively simple task given public/private datasets share the same\ndistribution. However, in the black-box setting, the performances\nof binary classifiers degrade significantly due to the different distri-\nbution of the private dataset, while our decoder remains relatively\nmore stable. Similar to our observations in previous experiments,\nour decoder tends to be more robust in black-box setting.\nA.3 Randomly Selected Examples\nAlthough we show that our decoder can reconstruct high quality\nsentences in some cases, there still exist challenges in the reconstruc-\ntion process. According to Table 4, the decoder handles acronyms\nand numbers less accurately. We leave this topic open for future\nresearch.\nA.4 Ablation Study on k\nWe show the impact ofğ‘˜(top-k words are finally kept and examined\nin keyword inference attack) in Figure 7. A largerğ‘˜certainly makes\nour attack more accurate. However, it may also lower the efficiency\nas the adversary needs to check more words. We picked20 to reach\na balance between effectiveness and efficiency. As demonstrated\nin Figure 7, the curves of all keywords and the curves of unique\nkeywords often tend to converge in the early stage (ğ‘˜ = 25), which\nsuggests that we do not need a very large ğ‘˜ to achieve the best\n76\nTowards Sentence Level Inference Attack Against Pre-trained Language Models Proceedings on Privacy Enhancing Technologies 2023(3)\n(a) Results of white-box attack on Twitter dataset\n (b) Results of black-box attack on Twitter dataset\n(c) Results of white-box attack on MT dataset\n (d) Results of black-box attack on MT dataset\nFigure 6: The results of binary classifiers.\nTable 4: 10 Random Examples of Sentence Inference Attack\nDomain Original Reconstructed\nAirline\nParis (cdg) to Detroit (dtw) Paris [PAD] to [PAD] dtw\nLion air 8pm flight Bengkulu to Jakarta March 2 Lion air flight [PAD]ngbulu\ncx841 from New York jfk to hkg c0 New York john\nVenice to Toronto on August 23 2013 Venice 23 made 19 143 2013\nZurich to Ljubljana return Zurich to Ljubljana 3 short\nMedical\nmanual muscle test of arm, leg or trunk manual test of hand arm behind leg\ninjection of agent to destroy rib nerve agent de from to approach rib\nmra scan of neck blood vessels throat chest mra to than neck\nX-ray of abdomen, minimum of 3 views X before and each 4 typically views\nX-ray of upper spine, 4 or 5 views X ray 5 from at 4 6 views\nresults. Hence, our attack is not very sensitive to the choice of ğ‘˜\nwhen ğ‘˜ â‰¥20.\n77\nProceedings on Privacy Enhancing Technologies 2023(3) Trovato et al.\n(a)\n (b)\n (c)\n (d)\nFigure 7: The ablation study of ğ‘˜. (a) The count of reconstructed keywords on Skytrax dataset (b) The count of reconstructed keywords on\nCMS dataset; (c) The number of unique keywords on Skytrax dataset; (d) The number of unique keywords on CMS dataset.\n78"
}