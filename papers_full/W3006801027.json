{
  "title": "Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation",
  "url": "https://openalex.org/W3006801027",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4288524692",
      "name": "Raganato, Alessandro",
      "affiliations": [
        "University of Helsinki"
      ]
    },
    {
      "id": "https://openalex.org/A4289046226",
      "name": "Scherrer, Yves",
      "affiliations": [
        "University of Helsinki"
      ]
    },
    {
      "id": null,
      "name": "Tiedemann, J\\\"org",
      "affiliations": [
        "University of Helsinki"
      ]
    },
    {
      "id": "https://openalex.org/A4223111321",
      "name": "Tiedemann, Jörg",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2922349260",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2786396726",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W2759173152",
    "https://openalex.org/W2970045405",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3088581407",
    "https://openalex.org/W2888539709",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970247882",
    "https://openalex.org/W2905927205",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2894627709",
    "https://openalex.org/W3041866211",
    "https://openalex.org/W2946462349",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2986267869",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2970820321",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2952406142",
    "https://openalex.org/W2962697716",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2899423466",
    "https://openalex.org/W3103729510",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2995446988",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2970810442",
    "https://openalex.org/W2972342261",
    "https://openalex.org/W3006881356",
    "https://openalex.org/W2962911926",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3034955736",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2951563833",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W2988309730",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2964302946",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3202218022",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W2740107491",
    "https://openalex.org/W2971141904",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2971016963",
    "https://openalex.org/W2962788148",
    "https://openalex.org/W2887751429"
  ],
  "abstract": "Transformer-based models have brought a radical change to neural machine translation. A key feature of the Transformer architecture is the so-called multi-head attention mechanism, which allows the model to focus simultaneously on different parts of the input. However, recent works have shown that most attention heads learn simple, and often redundant, positional patterns. In this paper, we propose to replace all but one attention head of each encoder layer with simple fixed -- non-learnable -- attentive patterns that are solely based on position and do not require any external knowledge. Our experiments with different data sizes and multiple language pairs show that fixing the attention heads on the encoder side of the Transformer at training time does not impact the translation quality and even increases BLEU scores by up to 3 points in low-resource scenarios.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 556–568\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n556\nFixed Encoder Self-Attention Patterns in\nTransformer-Based Machine Translation\nAlessandro Raganato, Yves Scherrer and J¨org Tiedemann\nUniversity of Helsinki\n{name.surname}@helsinki.fi\nAbstract\nTransformer-based models have brought a rad-\nical change to neural machine translation. A\nkey feature of the Transformer architecture is\nthe so-called multi-head attention mechanism,\nwhich allows the model to focus simultane-\nously on different parts of the input. However,\nrecent works have shown that most attention\nheads learn simple, and often redundant, posi-\ntional patterns. In this paper, we propose to\nreplace all but one attention head of each en-\ncoder layer with simple ﬁxed – non-learnable\n– attentive patterns that are solely based on po-\nsition and do not require any external knowl-\nedge. Our experiments with different data\nsizes and multiple language pairs show that ﬁx-\ning the attention heads on the encoder side of\nthe Transformer at training time does not im-\npact the translation quality and even increases\nBLEU scores by up to 3 points in low-resource\nscenarios.\n1 Introduction\nModels based on the Transformer architecture\n(Vaswani et al., 2017) have led to tremendous per-\nformance increases in a wide range of downstream\ntasks (Devlin et al., 2019; Radford et al., 2019), in-\ncluding Machine Translation (MT) (Vaswani et al.,\n2017; Ott et al., 2018). One main component\nof the architecture is the multi-headed attention\nmechanism that allows the model to capture long-\nrange contextual information. Despite these suc-\ncesses, the impact of the suggested parametrization\nchoices, in particular the self-attention mechanism\nwith its large number of attention heads distributed\nover several layers, has been the subject of many\nstudies, following roughly four lines of research.\nThe ﬁrst line of research focuses on the interpre-\ntation of the network, in particular on the analysis\nof attention mechanisms and the interpretability of\nthe weights and connections (Raganato and Tiede-\nmann, 2018; Tang et al., 2018; Mareˇcek and Rosa,\n2019; V oita et al., 2019a; Brunner et al., 2020).\nA closely related research area attempts to guide\nthe attention mechanism, e.g. by incorporating\nalignment objectives (Garg et al., 2019), or im-\nproving the representation through external infor-\nmation such as syntactic supervision (Pham et al.,\n2019; Currey and Heaﬁeld, 2019; Deguchi et al.,\n2019). The third line of research argues that Trans-\nformer networks are over-parametrized and learn\nredundant information that can be pruned in vari-\nous ways (Sanh et al., 2019). For example, V oita\net al. (2019b) show that a few attention heads do the\n“heavy lifting” whereas others contribute very little\nor nothing at all. Similarly, Michel et al. (2019)\nraise the question whether 16 attention heads are\nreally necessary to obtain competitive performance.\nFinally, several recent works address the computa-\ntional challenge of modeling very long sequences\nand modify the Transformer architecture with atten-\ntion operations that reduce time complexity (Shen\net al., 2018; Wu et al., 2019; Child et al., 2019;\nSukhbaatar et al., 2019; Dai et al., 2019; Indurthi\net al., 2019; Kitaev et al., 2020).\nThis study falls into the third category and is mo-\ntivated by the observation that most self-attention\npatterns learned by the Transformer architecture\nmerely reﬂect positional encoding of contextual\ninformation (Raganato and Tiedemann, 2018; Ko-\nvaleva et al., 2019; V oita et al., 2019a; Correia\net al., 2019). From this standpoint, we argue that\nmost attentive connections in the encoder do not\nneed to be learned at all, but can be replaced by\nsimple predeﬁned patterns. To this end, we de-\nsign, analyze and experimentally compare intuitive\nand simple ﬁxed attention patterns. The proposed\npatterns are solely based on positional informa-\ntion and do not require any learnable parameters\nnor external knowledge. The ﬁxed patterns reﬂect\nthe importance of locality and pose the question\nwhether encoder self-attention needs to be learned\n557\nat all to achieve state-of-the-art results in machine\ntranslation.\nThis paper provides the following contributions:\n•We propose ﬁxed – non-learnable – attentive\npatterns that replace the learnable attention\nmatrices in the encoder of the Transformer\nmodel.\n•We evaluate the proposed ﬁxed patterns on a\nseries of experiments with different language\npairs and varying amounts of training data.\nThe results show that ﬁxed self-attention pat-\nterns yield consistently competitive results,\nespecially in low-resource scenarios.\n•We provide an ablation study to analyze the\nrelative impact of the different ﬁxed atten-\ntion heads and the effect of keeping one of\nthe eight heads learnable. Moreover, we also\nstudy the effect of the number of encoder and\ndecoder layers on translation quality.\n•We assess the translation performance of the\nﬁxed attention models through various con-\ntrastive test suites, focusing on two linguistic\nphenomena: subject-verb agreement and word\nsense disambiguation.\nOur results show that the encoder self-attention\nin Transformer-based machine translation can be\nsimpliﬁed substantially, reducing the parameter\nfootprint without loss of translation quality, and\neven improving quality in low-resource scenarios.\nAlong with our contributions, we highlight our\nkey ﬁndings that give insights for the further devel-\nopment of more lightweight neural networks while\nretaining state-of-the-art performance for MT:\n•The encoder can be substantially simpliﬁed\nwith trivial attentive patterns at training time:\nonly preserving adjacent and previous tokens\nis necessary.\n•Encoder attention heads solely based on lo-\ncality principles may hamper the extraction\nof global semantic features beneﬁcial for the\nword sense disambiguation capability of the\nMT model. Keeping one learnable head in\nthe encoder compensates for degradations, but\nthis trade-off needs to be carefully assessed.\n•Position-wise attentive patterns play a key role\nin low-resource scenarios, both for related\n(German ↔English) and unrelated (Viet-\nnamese ↔English) languages.\n2 Related work\nAttention mechanisms in Neural Machine Trans-\nlation (NMT) were ﬁrst introduced in combina-\ntion with Recurrent Neural Networks (RNNs) (Cho\net al., 2014; Bahdanau et al., 2015; Luong et al.,\n2015), between the encoder and decoder. The\nTransformer architecture extended the mechanism\nby introducing the so-called self-attention to re-\nplace the RNNs in the encoder and decoder, and\nby using multiple attention heads (Vaswani et al.,\n2017). This architecture rapidly became the de\nfacto state-of-the-art architecture for NMT, and\nmore recently for language modeling (Radford\net al., 2018) and other downstream tasks (Strubell\net al., 2018; Devlin et al., 2019; Bosselut et al.,\n2019). The Transformer allows the attention for\na token to be spread over the entire input se-\nquence, multiple times, intuitively capturing dif-\nferent properties. This characteristic has led to\na line of research focusing on the interpretation\nof Transformer-based networks and their atten-\ntion mechanisms (Raganato and Tiedemann, 2018;\nTang et al., 2018; Mareˇcek and Rosa, 2019; V oita\net al., 2019a; Vig and Belinkov, 2019; Clark et al.,\n2019; Kovaleva et al., 2019; Tenney et al., 2019;\nLin et al., 2019; Jawahar et al., 2019; van Schijndel\net al., 2019; Hao et al., 2019b; Rogers et al., 2020).\nAs regards MT, recent work (V oita et al., 2019b)\nsuggests that only a few attention heads are spe-\ncialized towards a speciﬁc role, e.g., focusing on\na syntactic dependency relation or on rare words,\nand signiﬁcantly contribute to the translation per-\nformance, while all the others are dispensable.\nAt the same time, recent research has attempted\nto bring the mathematical formulation of self-\nattention more in line with the linguistic expec-\ntation that attention would be most useful within\na narrow local scope, e.g. for the translation of\nphrases (Hao et al., 2019a). For instance, Shaw\net al. (2018) replace the sinusoidal position en-\ncoding in the Transformer with relative position\nencoding to improve the focus on local positional\npatterns. Several studies modify the attention for-\nmula to bias the attention weights towards local\nareas (Yang et al., 2018; Xu et al., 2019; Fonol-\nlosa et al., 2019). Wu et al. (2019) and Yang et al.\n(2019) use convolutional modules to replace parts\nof self-attention, making the overall networks com-\nputationally more efﬁcient. Cui et al. (2019) mask\nout certain tokens when computing attention, which\nfavors local attention patterns and prevents redun-\n558\ndancy in the different attention heads. All these\ncontributions have shown the importance of local-\nness, and the possibility to use lightweight con-\nvolutional networks to reduce the number of pa-\nrameters while yielding competitive results (Wu\net al., 2019). In this respect, our work is orthogonal\nto previous research: we focus only on the orig-\ninal Transformer architecture and investigate the\nreplacement of learnable encoder self-attention by\nﬁxed, non-learnable attentive patterns.\nRecent analysis papers have identiﬁed a cer-\ntain number of functions to which different self-\nattention heads tend to specialize. V oita et al.\n(2019b) identiﬁes three types of heads: positional\nheads point to an adjacent token, syntactic heads\npoint to tokens in a speciﬁc syntactic relation, and\nrare word heads point to the least frequent tokens\nin a sentence. Correia et al. (2019) identify two ad-\nditional types of heads: BPE-merging heads spread\nweight over adjacent tokens that are part of the\nsame BPE cluster or hyphenated words, and inter-\nrogation heads point to question marks at the end\nof the sentence. In line with these ﬁndings, we\ndesign our ﬁxed attention patterns and train NMT\nmodels without the need of learning them.\nIn concurrent work, You et al. (2020) propose to\nreplace learnable attention weights in Transformer-\nbased NMT with hard-coded Gaussian distribu-\ntions. This paper is complementary and differs in\nseveral respects: while You et al. (2020) consider\nthree ﬁxed patterns across the encoder-decoder\narchitecture, we focus only on the encoder self-\nattention but present seven ﬁxed patterns that cover\nadditional known properties of self-attention. We\nstudy the relative impact of each of them and an-\nalyze their performance with respect to different\nnumbers of encoder-decoder layers, and as seman-\ntic feature extractor for lexical ambiguity phenom-\nena. Furthermore, in contrast to You et al. (2020),\nwe show that our ﬁxed patterns have a clear beneﬁ-\ncial effect in low-resource scenarios.\n3 Methodology\nIn this section, we brieﬂy describe the Transformer\narchitecture and its self-attention mechanism, and\nintroduce the ﬁxed attention patterns.\n3.1 Self-attention in Transformers\nThe Transformer architecture follows the so-called\nencoder-decoder paradigm where the source sen-\ntence is encoded in a number of stacked encoder\nblocks, and the target sentence is generated through\na number of stacked decoder blocks. Each encoder\nblock consists of a multi-head self-attention layer\nand a feed-forward layer.\nFor a sequence of token representations H ∈\nRn×d (with sequence length nand dimensionality\nd), the self-attention model ﬁrst projects them into\nqueries Q ∈Rn×d, keys K ∈Rn×d and values\nV ∈Rn×d, using three different linear projections.\nThen, the attention energy ξi for position iin the\nsequence is computed by taking the scaled dot prod-\nuct between the query vectorQi and the key matrix\nK:\nξi = softmax\n(QiK⊤\n√\nd\n)\n∈Rn (1)\nThe attention energy is then used to compute a\nweighted average of the values V:\nAtt(ξi,V) =ξiV ∈Rd (2)\nFor multi-head attention with hheads, the query,\nkey and value are linearly projected htimes to al-\nlow the model to jointly attend to information from\ndifferent representations. The attention vectors of\nthe hheads are then concatenated. Finally, the re-\nsulting multi-head attention is fed to a feed-forward\nnetwork that consists of two linear layers with a\nReLU activation in between. This multi-head at-\ntention is often called encoder self-attention, as it\nbuilds a representation of the input sentence that is\nattentive to itself.\nThe decoder follows the same architecture as the\nencoder with multi-head attention mechanisms and\nfeed-forward networks, with two main differences:\ni) an additional multi-head attention mechanism,\ncalled encoder-decoder attention, connects the last\nencoder layer to the decoder layers, and ii) future\npositions are prevented from being attended to, by\nmasking, in order to preserve the auto-regressive\nproperty of a left-to-right decoder.\nThe base version of the Transformer, the stan-\ndard setting for MT, uses 6 layers for both encoder\nand decoder and 8 attention heads in each layer. In\nthis work, we focus on the encoder self-attention\nand replace the learned attention energy ξby ﬁxed,\npredeﬁned distributions for all but one head.\n3.2 Fixed self-attention patterns\nThe inspection of encoder self-attention in standard\nMT models yields the somewhat surprising result\nthat positional patterns, such as “previous token”,\n559\nFigure 1: Token-based (upper row) and word-based (lower row) ﬁxed attention patterns for the example sentence\n“a master of science ﬁc## tion . ”. The word-based patterns treat the subwords “ﬁc##” and “tion” as a single token.\n“next token”, or “last token of the sentence”, are\nkey features across all layers and remain even after\npruning most of the attention heads (V oita et al.,\n2019a,b; Correia et al., 2019). Instead of costly\nlearning these trivial positional patterns using mil-\nlions of sentences, we choose seven predeﬁned pat-\nterns, each of which takes the place of an attention\nhead (see Figure 1, upper row).\nGiven the i-thword within a sentence of length\nn, we deﬁne the following patterns:\n1. the current token: a ﬁxed attention weight of\n1.0 at position i,\n2. the previous token: a ﬁxed attention weight of\n1.0 at position i−1,\n3. the next token: a ﬁxed attention weight of 1.0\nat position i+ 1,\n4. the larger left-hand context: a function f over\nthe positions 0 to i−2,\n5. the larger right-hand context: a function f\nover the positions i+ 2to n,\n6. the end of the sentence: a function f over the\npositions 0 to n,\n7. the start of the sentence: a function f over the\npositions nto 0.\nFor illustration, the attention energies for pat-\nterns 2 and 4 are deﬁned formally as follows:\nξ(2)\ni,j =\n{\n1 if j = i−1\n0 otherwise\nξ(4)\ni,j =\n{\nf(4)(j) if j ≤i−2\n0 otherwise\nwhere\nf(4)(j) = (j+ 1)3\n∑i−2\nj=0(j+ 1)3\nThe same function is used for all patterns, changing\nonly the respective start and end points.1These pre-\ndeﬁned attention heads are repeated over all layers\nof the encoder. The eighth attention head always\nremains learnable.2\nIt is customary in NMT to split words into sub-\nword units, and there is evidence that self-attention\ntreats split words differently than non-split ones\n(Correia et al., 2019). Therefore, we propose a sec-\nond variant of the predeﬁned patterns that assigns\nthe same attention values to all parts of the same\nword (see lower row of Figure 1).\n4 Experiments\nWe perform a series of experiments to evaluate\nthe ﬁxed attentive encoder patterns, starting with\na standard German ↔English translation setup\n(Section 4.1) and then extending the scope to low-\nresource and high-resource scenarios (Section 4.2).\nWe use the OpenNMT-py (Klein et al., 2017) li-\nbrary for training, the base version of Transformer\nas hyper-parameters (Vaswani et al., 2017), and\ncompare against the reference using sacreBLEU\n(Papineni et al., 2002; Post, 2018) .3\n4.1 Results: Standard scenario\nTo assess the general viability of the proposed ap-\nproach and to quantify the effects of different num-\nbers of encoder and decoder layers, we train models\non a mid-sized dataset of 2.9M training sentences\nfrom the German ↔English WMT19 news transla-\ntion task (Barrault et al., 2019), using newstest2013\nand newstest2014 as development and test data, re-\nspectively. We learn truecasers and Byte-Pair En-\n1Pattern 5 and 7 are ﬂipped versions of pattern 4 and 6,\nrespectively.\n2In Section 4.4, we present a contrastive system in which\nthe eighth head is ﬁxed as well.\n3Signature: BLEU+case.lc+#.1+s.exp+tok.13a+v.1.2.11.\n560\nEncoder+Decoder layers\nEncoder heads 1+1 2+1 3+1 4+1 5+1 6+1 6+6\nEN–DE 8L 20.61 21.68 22.63 23.02 23.18 23.36 25.02\n7Ftoken+1L 20.61 21.58 22.38 23.15 23.10 23.07 24.63\n7Fword+1L 19.72* 21.43 21.81* 22.83 22.74 22.88* 24.85\n1L 18.14* 19.88* 21.42* 21.71* 22.63* 22.29* 23.87*\nDE–EN 8L 25.66 27.28 27.88 28.62 28.71 29.31 30.99\n7Ftoken+1L 24.90* 27.01 26.84* 28.09* 28.43 28.61* 30.61\n7Fword+1L 25.03* 26.72* 27.38* 27.78* 27.82* 28.40* 30.69\n1L 23.76* 25.75* 26.96* 27.34* 27.44* 27.56* 30.17*\nTable 1: BLEU scores for the German ↔English (DE ↔EN) standard scenario, for different conﬁgurations of\nlearnable (L) and ﬁxed (F) attention heads. Scores marked in gray with * are signiﬁcantly lower than the respective\n8L model scores, at p< 0.05. Statistical signiﬁcance is computed using the compare-mt tool (Neubig et al., 2019)\nwith paired bootstrap resampling with 1000 resamples (Koehn, 2004).\ncoding (BPE) segmentation (Sennrich et al., 2016)\non the training corpus, using 35 000 merge opera-\ntions.\nWe train four Transformer models:\n•8L: all 8 attention heads in each layer are\nlearnable,\n•7Ftoken+1L: 7 ﬁxed token-based attention\nheads and 1 learnable head per encoder layer,\n•7Fword+1L: 7 ﬁxed word-based attention pat-\nterns and 1 learnable head per encoder layer,\n•1L: a single learnable attention head per en-\ncoder layer.\nEach model is trained in 7 conﬁgurations: 6 en-\ncoder layers with 6 decoder layers, and 1 to 6 en-\ncoder layers coupled to 1 decoder layer. BLEU\nscores are shown in Table 1.\nResults for the most powerful model (6+6) show\nthat the two ﬁxed-attention models are almost in-\ndistinguishable from the standard model, whereas\nthe single-head model yields consistently slightly\nlower results. It could be argued that the 6-layer\ndecoder is powerful enough to compensate for deﬁ-\nciencies due to ﬁxed attention on the encoder side.\nThe 6+1 conﬁguration, which uses a single layer\ndecoder, shows indeed a slight performance drop\nfor German →English, but no signiﬁcant differ-\nence in the opposite direction. Overall translation\nquality drops signiﬁcantly with three and less en-\ncoder layers, but the difference between ﬁxed and\nlearnable attention models is statistically insignif-\nicant in most cases. The ﬁxed attention models\nalways outperform the model with a single learn-\nable head, which shows that the predeﬁned patterns\nare indeed helpful. The (simpler) token-based ap-\nproach seems to outperform the word-based one,\nbut with higher numbers of decoder layers the two\nvariants are statistically equivalent.\n4.2 Results: Low-resource and high-resource\nscenarios\nWe hypothesize that ﬁxed attentive patterns are\nespecially useful in low-resource scenarios since\nintuitive properties of self-attention are directly en-\ncoded within the model, which may be hard to learn\nfrom small training datasets. We empirically test\nthis assumption on four translation tasks:\n•German →English (DE→EN), using the data\nfrom the IWSLT 2014 shared task (Cettolo\net al., 2014). As prior work (Ranzato et al.,\n2016; Sennrich and Zhang, 2019), we report\nBLEU score on the concatenated dev sets:\ntst2010, tst2011, tst2012, dev2010, dev2012\n(159 000 training sentences, 7 282 for devel-\nopment, and 6 750 for testing).\n•Korean → English (KO →EN), using the\ndataset described in Park et al. (2016) (90 000\ntraining sentences, 1 000 for development, and\n2 000 for testing).4\n•Vietnamese ↔English (VI↔EN), using the\ndata from the IWSLT 2015 shared task (Cet-\ntolo et al., 2015), using tst2012 and tst2013\nfor development and testing, respectively\n(133 000 training sentences, 1 553 for devel-\nopment and 1 268 per testing).\n4https://github.com/jungyeul/\nkorean-parallel-corpora\n561\nEnc. heads DE–EN KO–EN EN–VI VI–EN\n8L 30.86 6.67 29.85 26.15\n7Ftoken+1L 32.95 8.43 31.05 29.16\n7Fword+1L 32.56 8.70 31.15 28.90\n1L 30.22 6.14 28.67 25.03\nPrior work †33.60 †10.37 ⊎27.71 ⊎26.15\nTable 2: BLEU scores obtained for the low-resource\nscenarios with 6+6 layer conﬁguration. Results marked\nwith †are taken from Sennrich and Zhang (2019), those\nmarked with ⊎from Kudo (2018).\nEncoder heads EN–DE DE–EN\n8L 26.75 34.10\n7Ftoken+1L 26.52 33.50\n7Fword+1L 26.92 33.17\n1L 26.26 32.91\nTable 3: BLEU scores obtained for the high-resource\nscenario with 6+6 layer conﬁgurations.\nLow-resource scenarios can be sensible to the\nchoice of hyperparameters (Sennrich and Zhang,\n2019). Hence, we apply three of the most success-\nful adaptations to all our conﬁgurations: reduced\nbatch size (4k →1k tokens), increased dropout (0.1\n→0.3), and tied embeddings. Sentences are BPE-\nencoded with 30 000 merge operations, shared be-\ntween source and target language, but independent\nfor Korean →English.\nResults of the 6+6 layer conﬁgurations are\nshown in Table 2. 5 The models using ﬁxed at-\ntention consistently outperform the models using\nlearned attention, by up to 3 BLEU. No clear win-\nner between token-based and word-based ﬁxed at-\ntention can be distinguished though.\nOur English ↔Vietnamese models outperform\nprior work based on an RNN architecture by a\nlarge margin, but the German →English and Ko-\nrean →English models remain below the heavily\noptimized models of Sennrich and Zhang (2019).\nHowever, we note that our goal is not to beat the\nstate-of-the-art in a given MT setting but rather to\nshow the performance of simple non-learnable at-\ntentive patterns across different language pairs and\ndata sizes. Moreover, it is worth to mention that\nthe Korean →English dataset, being automatically\ncreated, includes some noise in the test data that\n5The 6+1 models show globally lower scores, but similar\nrelative rankings between models.\nDisabled head 6+1 layers 6+6 layers\nEN–DE DE–EN EN–DE DE–EN\n1 Current word -0.15 0.11 0.12 -0.04\n2 Previous word -5.72 -5.21 -3.05 -3.26\n3 Next word -1.80 -1.98 -2.08 -1.36\n4 Prev. context -4.73 -5.20 -1.42 -2.85\n5 Next context -0.72 -0.34 -0.47 -0.66\n6 Start context -0.17 -0.12 0.14 0.13\n7 End context -0.02 0.12 -0.30 0.10\n8 Learned head -2.22 -4.05 -0.58 -0.78\nEN–VI VI–EN EN–VI VI–EN\n1 Current word 0.12 -0.14 0.16 -0.05\n2 Previous word -2.32 -2.67 -2.71 -3.04\n3 Next word -1.12 -1.61 -1.35 -2.15\n4 Prev. context -4.11 -4.32 -2.82 -3.09\n5 Next context -0.27 -0.50 -0.83 -0.77\n6 Start context -0.29 -0.08 -0.04 0\n7 End context 0.28 -0.29 -0.23 -0.19\n8 Learned head -0.57 -0.88 -0.18 0.36\nTable 4: BLEU score differences with respect to the\nfull model with 8 enabled heads (values <-1 in bold).\nmay impact the comparison.6\nFinally, we also evaluate a high-resource sce-\nnario for German ↔English with 11.5M training\nsentences.7 Table 3 shows that the results of the\nﬁxed attention models do not degrade even when\nabundant training data allow all attention heads to\nbe learned accurately.\n4.3 Ablation study\nWe perform an ablation study to assess the contri-\nbution of each attention head separately. To this\nend, we mask out one attention pattern across all\nencoder layers at test time. Table 4 shows the dif-\nferences compared to the full model, on the mid-\nsized German ↔English and on the Vietnamese\n↔English models, both in the 6+1 and 6+6 layer\nconﬁgurations.\nWe ﬁnd that heads 2, 3 and 4 (previous word,\nnext word, previous context) are particularly impor-\ntant, whereas the impact of the remaining context\nheads is small. Head 1 (current word) is not useful\nin the token-based model, but shows slightly larger\nnumbers in the word-based setting.\n6https://github.com/jungyeul/\nkorean-parallel-corpora/issues/1\n7This scenario uses the same benchmark from Sec-\ntion 4.1, increasing the training data with a ﬁltered version of\nParaCrawl (bicleaner ﬁltered v3).\n562\nEnc. heads #Param. EN–DE DE–EN EN–VI VI–EN\n8L 91.7M 25.02 30.99 29.85 26.15\n7Ftoken+1L 88.9M 24.63 30.61 31.05 29.16\n8Ftoken 88.5M 24.64 30.56 31.45 28.97\nTable 5: BLEU scores for the experiments with eight\nﬁxed attention heads and 6+6 layers. “#Param.” de-\nnotes the number of parameters for the EN–DE model.\nThe most interesting results concern the eighth,\nlearned head. Its impact is signiﬁcant, but in most\ncases lower than the three main heads listed above.\nInterestingly, disabling it causes much lower degra-\ndation in the 6+6 conﬁgurations, which suggests\nthat a more powerful decoder can compensate for\nthe absence of learned encoder representations.\n4.4 Eight ﬁxed heads\nThe ablation study suggests that it is not crucial\nto keep one learnable head in the encoder layers,\nespecially if the decoder is deep enough. Here,\nwe assess the extreme scenario where the eighth\nattention head is ﬁxed as well. The eighth ﬁxed\nattentive pattern focuses on the last token, with a\nﬁxed weight of 1.0 at position n. Table 5 shows\nthe results for the standard English ↔German sce-\nnario and the low-resource English ↔Vietnamese\nscenario.\nOverall, the learnable attention head is com-\npletely dispensable across both language pairs. As\nshown in Section 4.3, the impact of having learn-\nable attention heads on the encoder side is negli-\ngible. Moreover, we also note that as we replace\nattention heads with non-learnable ones, our conﬁg-\nurations reduce the number of parameters without\ndegrading translation quality.\n5 Analysis\nTo further analyze the ﬁxed attentive encoder pat-\nterns, we perform three targeted evaluations: i) on\nthe sentence length, ii) on the subject-verb agree-\nment task, and iii) on the Word Sense Disambigua-\ntion (WSD) task. The length analysis inspects the\ntranslation quality by sentence length. The subject-\nverb agreement task is commonly used to evaluate\nlong-range dependencies (Linzen et al., 2016; Tran\net al., 2018; Tang et al., 2018), while the WSD\ntask addresses lexical ambiguity phenomena, i.e.,\nwords of the source language that have multiple\ntranslations in the target language representing dif-\nferent meanings (Marvin and Koehn, 2018; Liu\n<10 [10,20)[20,30)[30,40)[40,50)[50,60) ≥60\n20\n25\n30\n6+6 EN–DE\n6+6 DE–EN\n8L\n7Ftoken+1L\n7Ftoken (H8 disabled)\nFigure 2: BLEU scores for different ranges of sentence\nlengths.\net al., 2018; Pu et al., 2018; Tang et al., 2019).\nFor both tasks, we use contrastive test suites\n(Sennrich, 2017; Popovi´c and Castilho, 2019) that\nrely on the ability of NMT systems to score given\ntranslations. Broadly speaking, a sentence contain-\ning the linguistic phenomenon of interest is paired\nwith the correct reference translation and with a\nmodiﬁed translation with a speciﬁc type of error. A\ncontrast is considered successfully detected if the\nreference translation obtains a higher score than\nthe artiﬁcially modiﬁed translation. The evalua-\ntion metric corresponds to the accuracy over all\ndecisions.\nWe conduct the analyses using the DE–EN mod-\nels from Section 4.1, i.e., 8L, 7F token+1L, and\n7Ftoken (H8 disabled).\n5.1 Sentence length analysis\nTo assess whether our ﬁxed attentive patterns may\nhamper modeling of global dependencies support-\ning long sentences, we compute BLEU score by\nreference sentence length.8 Despite the small per-\nformance gap between models, as we can see from\nFigure 2, long sentences beneﬁt from having learn-\nable attentive patterns. This is clearly shown by the\n7Ftoken (H8 disabled) model, which is consistently\ndegraded in almost every length bin.\n5.2 Subject-verb agreement\nThe predeﬁned attention patterns focus on rela-\ntively small local contexts. It could therefore be\nargued that the ﬁxed attention models would per-\nform worse on long-distance agreement, and that\n8We use the compare-mt toolkit by Neubig et al. (2019).\n563\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15>15\n0.8\n0.85\n0.9\n0.95\n6+6 layers\n6+1 layers\n8L\n7Ftoken+1L\n7Ftoken (H8 disabled)\nFigure 3: Subject-verb agreement accuracies of the\nEN–DE models. The x-axis shows distances between\nthe subject and the verb.\ndisabling the learned head in particular would be\ncatastrophic. We test this hypothesis by evaluating\nthe models from Section 4.1 on the subject-verb\nagreement task of the English–German Lingeval\ntest suite (Sennrich, 2017). Figure 3 plots the ac-\ncuracies by distance between subject and verb. In\nthe 6+6 layer conﬁguration, no difference can be\ndetected between the three examined scenarios.\nIn the 6+1 layer conﬁguration, the ﬁxed-attention\nmodel does not seem to suffer from degraded re-\nsults, whereas disabling the learned head leads to\nclearly lower results. This drop is due to the ex-\npected degradation of general translation quality\n(cf. Table 4, ablation study) and is not worse than\nthe degradation observed by disabling one of the\nﬁxed local context heads.\n5.3 Word sense disambiguation\nIt has been shown that the encoder of Transformer-\nbased MT models includes semantic information\nbeneﬁcial for WSD (Tang et al., 2018, 2019). In\nthis respect, a model with predeﬁned ﬁxed pat-\nterns may struggle to encode global semantic fea-\ntures. To this end, we evaluate our models on\ntwo German–English WSD test suites, ContraWSD\n(Rios Gonzales et al., 2017) and MuCoW (Ra-\nganato et al., 2019).9\nTable 6 shows the performance of our models\non the WSD benchmarks. Overall, the model\nwith 6 decoder layers and ﬁxed attentive patterns\n9As MuCoW is automatically built using various parallel\ncorpora, we discarded those ones included in our training. We\nonly report the average result from the TED (Cettolo et al.,\n2013) and Tatoeba (Tiedemann, 2012) sources.\nContraWSD MuCoW\nEncoder heads 6+1 6+6 6+1 6+6\n8L 0.804 0.831 0.741 0.761\n7Ftoken+1L 0.793 0.834 0.734 0.772\n7Ftoken (H8 disabled) 0.761 0.816 0.721 0.757\nTable 6: Accuracy scores of the German–English mod-\nels on the ContraWSD and MuCoW test suites.\n(7Ftoken+1L) achieves higher accuracy than the\nmodel with all learnable attention heads (8L), while\nthe 1-layer decoder models show the opposite\neffect. It appears that having 6 decoder layers\ncan effectively cope with WSD despite having\nonly one learnable attention head. Interestingly\nenough, when we disable the learnable attention\nhead (7Ftoken H8 disabled), performance drops con-\nsistently in both test suites, showing that the learn-\nable head plays a key role for WSD, specializing\nin semantic feature extraction.\n6 Conclusion\nIn this work, we propose to simplify encoder self-\nattention of Transformer-based NMT models by\nreplacing all but one attention heads with ﬁxed posi-\ntional attentive patterns that require neither training\nnor external knowledge.\nWe train NMT models on different data sizes and\nlanguage directions with the proposed ﬁxed pat-\nterns, showing that the encoder self-attention can\nbe simpliﬁed drastically, reducing parameter foot-\nprint at training time without degradation in transla-\ntion quality. In low-resource scenarios, translation\nquality is even improved. Our extensive analyses\nshow that i) only adjacent and previous token at-\ntentive patterns contribute signiﬁcantly to the trans-\nlation performance, ii) the trainable encoder head\ncan also be disabled without hampering translation\nquality if the number of decoder layers is sufﬁcient,\niii) encoder attention heads based on locality pat-\nterns are beneﬁcial in low-resource scenarios, but\nmay affect the semantic feature extraction neces-\nsary for addressing lexical ambiguity phenomena.\nApart from the consistent results given by our\nsimple ﬁxed encoder patterns, this work opens up\npotential further research for simpler and more ef-\nﬁcient neural networks for MT, such as synthetic\nself-attention patterns (Tay et al., 2020).\n564\nAcknowledgments\nThis work is part of the FoTran project,\nfunded by the European Research Council\n(ERC) under the European Union’s Horizon\n2020 research and innovation programme\n(grant agreement No 771113).\nThe authors gratefully acknowledge the support\nof the CSC – IT Center for Science, Finland, for\ncomputational resources. Finally, We would also\nlike to acknowledge NVIDIA and their GPU grant.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nLo¨ıc Barrault, Ond ˇrej Bojar, Marta R. Costa-juss `a,\nChristian Federmann, Mark Fishel, Yvette Gra-\nham, Barry Haddow, Matthias Huck, Philipp Koehn,\nShervin Malmasi, Christof Monz, Mathias M ¨uller,\nSantanu Pal, Matt Post, and Marcos Zampieri. 2019.\nFindings of the 2019 conference on machine transla-\ntion (WMT19). In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 1–61, Florence, Italy. As-\nsociation for Computational Linguistics.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 4762–4779,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nGino Brunner, Yang Liu, Dami ´an Pascual, Oliver\nRichter, Massimiliano Ciaramita, and Roger Watten-\nhofer. 2020. On identiﬁability in transformers. Pro-\nceedings of the International Conference on Learn-\ning Representations (ICLR).\nMauro Cettolo, Jan Niehues, Sebastian St ¨uker, Luisa\nBentivogli, Roldano Cattoni, and Marcello Federico.\n2015. The IWSLT 2015 evaluation campaign. In\nProceedings of the International Workshop on Spo-\nken Language Translation.\nMauro Cettolo, Jan Niehues, Sebastian St ¨uker, Luisa\nBentivogli, and Marcello Federico. 2013. Report on\nthe 10th IWSLT evaluation campaign. In Proceed-\nings of the International Workshop on Spoken Lan-\nguage Translation, Heidelberg, Germany.\nMauro Cettolo, Jan Niehues, Sebastian St ¨uker, Luisa\nBentivogli, and Marcello Federico. 2014. Report\non the 11th IWSLT evaluation campaign, IWSLT\n2014. In Proceedings of the International Workshop\non Spoken Language Translation , page 57, Hanoi,\nVietnam.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKyunghyun Cho, Bart van Merri ¨enboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014. Learning\nphrase representations using RNN encoder–decoder\nfor statistical machine translation. In Proceedings of\nthe 2014 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 1724–\n1734, Doha, Qatar. Association for Computational\nLinguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nGonc ¸alo M. Correia, Vlad Niculae, and Andr ´e F. T.\nMartins. 2019. Adaptively sparse transformers. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2174–\n2184, Hong Kong, China. Association for Computa-\ntional Linguistics.\nHongyi Cui, Shohei Iida, Po-Hsuan Hung, Takehito Ut-\nsuro, and Masaaki Nagata. 2019. Mixed multi-head\nself-attention for neural machine translation. In Pro-\nceedings of the 3rd Workshop on Neural Generation\nand Translation, pages 206–214, Hong Kong. Asso-\nciation for Computational Linguistics.\nAnna Currey and Kenneth Heaﬁeld. 2019. Incorpo-\nrating source syntax into transformer-based neural\nmachine translation. In Proceedings of the Fourth\nConference on Machine Translation (Volume 1: Re-\nsearch Papers), pages 24–33, Florence, Italy. Asso-\nciation for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nHiroyuki Deguchi, Akihiro Tamura, and Takashi Ni-\nnomiya. 2019. Dependency-based self-attention for\ntransformer NMT. In Proceedings of the Interna-\ntional Conference on Recent Advances in Natural\nLanguage Processing (RANLP 2019) , pages 239–\n246, Varna, Bulgaria. INCOMA Ltd.\n565\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJos´e A. R. Fonollosa, Noe Casas, and Marta R.\nCosta-juss`a. 2019. Joint source-target self at-\ntention with locality constraints. arXiv preprint\narXiv:1905.06596.\nSarthak Garg, Stephan Peitz, Udhyakumar Nallasamy,\nand Matthias Paulik. 2019. Jointly learning to align\nand translate with transformer models. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 4453–4462, Hong\nKong, China. Association for Computational Lin-\nguistics.\nJie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang,\nand Zhaopeng Tu. 2019a. Multi-granularity self-\nattention for neural machine translation. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , pages 887–897, Hong\nKong, China. Association for Computational Lin-\nguistics.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019b. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 4143–\n4152, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSathish Reddy Indurthi, Insoo Chung, and Sangha Kim.\n2019. Look harder: A neural machine translation\nmodel with hard attention. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3037–3043, Florence,\nItaly. Association for Computational Linguistics.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. Pro-\nceedings of the International Conference on Learn-\ning Representations (ICLR).\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nsource toolkit for neural machine translation. In\nProceedings of ACL 2017, System Demonstrations ,\npages 67–72, Vancouver, Canada. Association for\nComputational Linguistics.\nPhilipp Koehn. 2004. Statistical signiﬁcance tests\nfor machine translation evaluation. In Proceed-\nings of the 2004 Conference on Empirical Meth-\nods in Natural Language Processing , pages 388–\n395, Barcelona, Spain. Association for Computa-\ntional Linguistics.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4364–4373, Hong Kong, China. Association for\nComputational Linguistics.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP , pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn\nsyntax-sensitive dependencies. Transactions of the\nAssociation for Computational Linguistics , 4:521–\n535.\nFrederick Liu, Han Lu, and Graham Neubig. 2018.\nHandling homographs in neural machine translation.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers) , pages 1336–1345, New\nOrleans, Louisiana. Association for Computational\nLinguistics.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing , pages 1412–1421, Lis-\nbon, Portugal. Association for Computational Lin-\nguistics.\nDavid Mare ˇcek and Rudolf Rosa. 2019. From\nbalustrades to pierre vinken: Looking for syntax in\ntransformer self-attentions. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP , pages 263–\n275, Florence, Italy. Association for Computational\nLinguistics.\n566\nRebecca Marvin and Philipp Koehn. 2018. Exploring\nword sense disambiguation abilities of neural ma-\nchine translation systems (non-archival extended ab-\nstract). In Proceedings of the 13th Conference of the\nAssociation for Machine Translation in the Ameri-\ncas (Volume 1: Research Papers) , pages 125–131,\nBoston, MA. Association for Machine Translation\nin the Americas.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin Neural Information Processing Systems.\nGraham Neubig, Zi-Yi Dou, Junjie Hu, Paul Michel,\nDanish Pruthi, and Xinyi Wang. 2019. compare-mt:\nA tool for holistic comparison of language genera-\ntion systems. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Asso-\nciation for Computational Linguistics (Demonstra-\ntions), pages 35–41, Minneapolis, Minnesota. Asso-\nciation for Computational Linguistics.\nMyle Ott, Sergey Edunov, David Grangier, and\nMichael Auli. 2018. Scaling neural machine trans-\nlation. In Proceedings of the Third Conference on\nMachine Translation: Research Papers , pages 1–9,\nBrussels, Belgium. Association for Computational\nLinguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation. In Proceedings of\nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318. Association\nfor Computational Linguistics.\nJungyeul Park, Jeen-Pyo Hong, and Jeong-Won Cha.\n2016. Korean Language Resources for Everyone. In\nProceedings of the 30th Paciﬁc Asia Conference on\nLanguage, Information and Computation (PACLIC\n30), pages 49–58, Seoul, Korea.\nThuong Hai Pham, Dominik Mach´aˇcek, and Ondˇrej Bo-\njar. 2019. Promoting the knowledge of source syn-\ntax in transformer nmt is not needed. Computaci´on\ny Sistemas, 23(3).\nMaja Popovi ´c and Sheila Castilho. 2019. Challenge\ntest sets for MT evaluation. In Proceedings of Ma-\nchine Translation Summit XVII Volume 3: Tutorial\nAbstracts, Dublin, Ireland. European Association for\nMachine Translation.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nXiao Pu, Nikolaos Pappas, James Henderson, and An-\ndrei Popescu-Belis. 2018. Integrating weakly su-\npervised word sense disambiguation into neural ma-\nchine translation. Transactions of the Association\nfor Computational Linguistics, 6:635–649.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI Blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nAlessandro Raganato, Yves Scherrer, and J ¨org Tiede-\nmann. 2019. The MuCoW test suite at WMT\n2019: Automatically harvested multilingual con-\ntrastive word sense disambiguation test sets for ma-\nchine translation. In Proceedings of the Fourth Con-\nference on Machine Translation (Volume 2: Shared\nTask Papers, Day 1), pages 470–480, Florence, Italy.\nAssociation for Computational Linguistics.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016. Sequence level train-\ning with recurrent neural networks. Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nAnnette Rios Gonzales, Laura Mascarell, and Rico Sen-\nnrich. 2017. Improving word sense disambigua-\ntion in neural machine translation with sense em-\nbeddings. In Proceedings of the Second Conference\non Machine Translation, pages 11–19, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in bertology: What we know about\nhow bert works. arXiv preprint arXiv:2002.12327.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. 5th Work-\nshop on Energy Efﬁcient Machine Learning and\nCognitive Computing - NeurIPS.\nMarten van Schijndel, Aaron Mueller, and Tal Linzen.\n2019. Quantity doesn’t buy quality syntax with\nneural language models. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5831–5837, Hong Kong,\nChina. Association for Computational Linguistics.\nRico Sennrich. 2017. How grammatical is character-\nlevel neural machine translation? assessing MT qual-\nity with contrastive translation pairs. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nVolume 2, Short Papers , pages 376–382, Valencia,\nSpain. Association for Computational Linguistics.\n567\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nRico Sennrich and Biao Zhang. 2019. Revisiting low-\nresource neural machine translation: A case study.\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 211–\n221, Florence, Italy. Association for Computational\nLinguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nand Chengqi Zhang. 2018. Bi-directional block self-\nattention for fast and memory-efﬁcient sequence\nmodeling. Proceedings of the International Confer-\nence on Learning Representations (ICLR).\nEmma Strubell, Patrick Verga, Daniel Andor,\nDavid Weiss, and Andrew McCallum. 2018.\nLinguistically-informed self-attention for semantic\nrole labeling. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 5027–5038, Brussels, Belgium.\nAssociation for Computational Linguistics.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 331–335, Florence, Italy.\nAssociation for Computational Linguistics.\nGongbo Tang, Mathias M¨uller, Annette Rios, and Rico\nSennrich. 2018. Why self-attention? a targeted\nevaluation of neural machine translation architec-\ntures. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4263–4272, Brussels, Belgium. Association\nfor Computational Linguistics.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019.\nEncoders help you disambiguate word senses in\nneural machine translation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1429–1435, Hong Kong,\nChina. Association for Computational Linguistics.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020. Synthesizer: Re-\nthinking self-attention in transformer models. arXiv\npreprint arXiv:2005.00743.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nJ¨org Tiedemann. 2012. Parallel data, tools and inter-\nfaces in OPUS. In Proceedings of the Eight In-\nternational Conference on Language Resources and\nEvaluation (LREC’12), pages 2214–2218, Istanbul,\nTurkey. European Language Resources Association\n(ELRA).\nKe Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe importance of being recurrent for modeling hi-\nerarchical structure. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 4731–4736, Brussels, Bel-\ngium. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nJesse Vig and Yonatan Belinkov. 2019. Analyzing\nthe structure of attention in a transformer language\nmodel. In Proceedings of the 2019 ACL Workshop\nBlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 63–76, Florence, Italy. As-\nsociation for Computational Linguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4395–4405, Hong Kong,\nChina. Association for Computational Linguistics.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019b. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nFelix Wu, Angela Fan, Alexei Baevski, Yann N\nDauphin, and Michael Auli. 2019. Pay less attention\nwith lightweight and dynamic convolutions. Pro-\nceedings of the International Conference on Learn-\ning Representations (ICLR).\nMingzhou Xu, Derek F. Wong, Baosong Yang, Yue\nZhang, and Lidia S. Chao. 2019. Leveraging lo-\ncal and global patterns for self-attention networks.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n3069–3075, Florence, Italy. Association for Compu-\ntational Linguistics.\n568\nBaosong Yang, Zhaopeng Tu, Derek F. Wong, Fandong\nMeng, Lidia S. Chao, and Tong Zhang. 2018. Mod-\neling localness for self-attention networks. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4449–\n4458, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nBaosong Yang, Longyue Wang, Derek F. Wong,\nLidia S. Chao, and Zhaopeng Tu. 2019. Convolu-\ntional self-attention networks. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 4040–4045, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nWeiqiu You, Simeng Sun, and Mohit Iyyer. 2020.\nHard-coded gaussian attention for neural machine\ntranslation. In Proceedings of ACL 2020.",
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.8521683216094971
    },
    {
      "name": "Transformer",
      "score": 0.8314255475997925
    },
    {
      "name": "Computer science",
      "score": 0.7935805320739746
    },
    {
      "name": "Encoder",
      "score": 0.6913262009620667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5531585812568665
    },
    {
      "name": "Architecture",
      "score": 0.4125165343284607
    },
    {
      "name": "Natural language processing",
      "score": 0.3788245916366577
    },
    {
      "name": "Engineering",
      "score": 0.08611586689949036
    },
    {
      "name": "Electrical engineering",
      "score": 0.07537713646888733
    },
    {
      "name": "Voltage",
      "score": 0.06542572379112244
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 24
}