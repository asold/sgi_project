{
    "title": "Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers",
    "url": "https://openalex.org/W3171088343",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5049783368",
            "name": "Hongfei Xu",
            "affiliations": [
                "German Research Centre for Artificial Intelligence",
                "Saarland University"
            ]
        },
        {
            "id": "https://openalex.org/A5049194403",
            "name": "Josef van Genabith",
            "affiliations": [
                "German Research Centre for Artificial Intelligence",
                "Saarland University"
            ]
        },
        {
            "id": "https://openalex.org/A5056590711",
            "name": "Qiuhui Liu",
            "affiliations": [
                "China Mobile (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5055232825",
            "name": "Deyi Xiong",
            "affiliations": [
                "Tianjin University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962969034",
        "https://openalex.org/W2952638691",
        "https://openalex.org/W2964204621",
        "https://openalex.org/W2515741950",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2964147026",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2983902802",
        "https://openalex.org/W3104235057",
        "https://openalex.org/W3105990194",
        "https://openalex.org/W2529194139",
        "https://openalex.org/W2962776659",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W2963536265",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W2970862333",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W3035747971",
        "https://openalex.org/W2914924671",
        "https://openalex.org/W2964213727",
        "https://openalex.org/W2997244573",
        "https://openalex.org/W3135335819",
        "https://openalex.org/W4301785137",
        "https://openalex.org/W2892213699",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2921311659",
        "https://openalex.org/W2148708890",
        "https://openalex.org/W2888196092",
        "https://openalex.org/W2962915948",
        "https://openalex.org/W3125507956",
        "https://openalex.org/W2964265128",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W2971154170",
        "https://openalex.org/W2951977278",
        "https://openalex.org/W2952682849",
        "https://openalex.org/W2613904329",
        "https://openalex.org/W2963430224",
        "https://openalex.org/W2970820321",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2952356761",
        "https://openalex.org/W2976965654",
        "https://openalex.org/W2970810442",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2892205701",
        "https://openalex.org/W2133564696"
    ],
    "abstract": "Hongfei Xu, Josef van Genabith, Qiuhui Liu, Deyi Xiong. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2021.",
    "full_text": "Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 74–85\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n74\nProbing Word Translations in the Transformer and\nTrading Decoder for Encoder Layers\nHongfei Xu1,2 Josef van Genabith1,2 Qiuhui Liu3∗ Deyi Xiong4\n1Saarland University / Saarland, Germany\n2German Research Center for Artiﬁcial Intelligence / Saarland, Germany\n3China Mobile Online Services / Henan, China\n4Tianjin University / Tianjin, China\nhfxunlp@foxmail.com, Josef.Van_Genabith@dfki.de,\nliuqhano@foxmail.com, dyxiong@tju.edu.cn\nAbstract\nDue to its effectiveness and performance, the\nTransformer translation model has attracted\nwide attention, most recently in terms of\nprobing-based approaches. Previous work fo-\ncuses on using or probing source linguistic\nfeatures in the encoder. To date, the way\nword translation evolves in Transformer lay-\ners has not yet been investigated. Naively,\none might assume that encoder layers capture\nsource information while decoder layers trans-\nlate. In this work, we show that this is not quite\nthe case: translation already happens progres-\nsively in encoder layers and even in the input\nembeddings. More surprisingly, we ﬁnd that\nsome of the lower decoder layers do not ac-\ntually do that much decoding. We show all\nof this in terms of a probing approach where\nwe project representations of the layer ana-\nlyzed to the ﬁnal trained and frozen classiﬁer\nlevel of the Transformer decoder to measure\nword translation accuracy. Our ﬁndings moti-\nvate and explain a Transformer conﬁguration\nchange: if translation already happens in the\nencoder layers, perhaps we can increase the\nnumber of encoder layers, while decreasing\nthe number of decoder layers, boosting decod-\ning speed, without loss in translation quality?\nOur experiments show that this is indeed the\ncase: we can increase speed by up to a fac-\ntor 2.3 with small gains in translation qual-\nity, while an 18-4 deep encoder conﬁguration\nboosts translation quality by+1.42 BLEU (En-\nDe) at a speed-up of 1.4.\n1 Introduction\nNeural Machine Translation (NMT) has achieved\ngreat success in the last few years. The popular\nTransformer (Vaswani et al., 2017) model, which\noutperforms previous RNN/CNN based transla-\ntion models (Bahdanau et al., 2014; Gehring et al.,\n2017), is based on multi-layer self-attention net-\nworks and can be parallelized effectively.\n∗ Corresponding author.\nRecently, a wide range of studies related to the\nTransformer have been conducted. For example,\nBisazza and Tump (2018) perform a ﬁne-grained\nanalysis of how various source-side morphological\nfeatures are captured at different levels of an NMT\nencoder. Surprisingly, they do not ﬁnd any corre-\nlation between the accuracy of source morphology\nencoding and translation quality. Morphological\nfeatures are only captured in context and only to the\nextent that they are directly transferable to target\nwords. V oita et al. (2019a) study how information\nﬂows across Transformer layers and ﬁnd that rep-\nresentations differ signiﬁcantly depending on the\nobjectives (machine translation, standard left-to-\nright language models and masked language mod-\neling). Tang et al. (2019) ﬁnd that encoder hidden\nstates outperform word embeddings signiﬁcantly in\nword sense disambiguation. However, to the best\nof our knowledge, to date there is no study about\nhow the Transformer translation model transforms\nindividual source tokens into corresponding target\ntokens (i.e., word translations), and speciﬁcally,\nwhich role each Transformer layer plays in word\ntranslation, and at which layer a word is translated.\nTo investigate the roles of Transformer layers\nin translation, in this paper, we adopt probing ap-\nproaches (Adi et al., 2017; Hupkes et al., 2018;\nConneau et al., 2018) and propose to measure the\nword translation accuracy of output representations\nof individual Transformer layers by probing how\ncapable they are at translating words. Probing uses\nlinear classiﬁers, referred to as “probes”, where a\nprobe can only use the hidden units of a given inter-\nmediate layer as discriminating features. Moreover,\nthese probes cannot affect the training phase of a\nmodel, and they are generally added after training\n(Alain and Bengio, 2017). In addition to analyz-\ning the role of each encoder/decoder layer, we also\nanalyze the contribution of the source context and\nthe decoding history in translation by testing the\neffects of the masked self-attention sub-layer and\n75\n…\n… …\n…\n…\n… …\n…\n…\n… …\n…\n…\n… …\n…\n…\n… …\n…\n…\n… …\n…\n…\n… …\n…\n…\n… …\n…\n…\n… …\n…\nSource \nEmbedding\nLayer 1\nLayer i\nLayer d\n…\nSource words\n…\nTarget \nEmbedding\nLayer 1\nLayer d\n…\nTarget words\n…\nSelf-Attn\nCross-Attn\nFFN\nLayer i\nClassifier\nTarget words (Shifted)\nLinear \nProjection Layer\nClassifier\nTarget words (Shifted)\n…\n… Ai …\n…\nTarget length\nSource length\n…\n… Ai …\n…\n…\n… Ai …\n…\n…\n…\n…\n… A …\n…\n…\np1\np2\npd*k\n…\n… E …\n…\nSource length\nInput dimension\nMatrix \nMultiplication\nLinear \nProjection Layer\nClassifier\nTarget words (Shifted)\nTE : (Size: Target length * Input dimension)\n(Layer 0) (Layer 0)\nFigure 1: Analyzing word translations of Transformer layers. Green indicates layers of the trained Transformer\nmodel frozen for analysis. Orange indicates parameters of the linear projection layer and weights of alignment\nmatrices Ai trained on the training set. Dashed arrows indicate shared modules. When analyzing the separate\neffects of source contexts or decoding history in a decoder layer, one of the cross-attention (in yellow) or self-\nattention sub-layers (in blue) of the analyzed decoder layer are bypassed by a residual connection (Section 2.2).\nLayers are independently analyzed. Target words (Shifted): the reference translation is one-position right-shifted\ncompared to decoder input, i.e., predicting the next word with the current word as input.\nthe cross-attention sub-layer in decoder layers.\nWe present empirical results for how word trans-\nlation is performed in each encoder/decoder layer,\nand how the alignment modeling (cross-attention\nsub-layers) and language modeling (masked self-\nattention sub-layers) contribute to the performance\nin each decoder layer. Our analysis demon-\nstrates how word translation evolves across en-\ncoder/decoder layers and provides insights into the\nimpact of the source “encoding” and the decoding\nhistory on the translation of target tokens. It re-\nveals the existence of target translations in encoder\nstates (and even source word embeddings) and the\ntranslation performed by encoder layers.\nBased on our ﬁndings, we show that the proper\nuse of more encoder layers with fewer decoder lay-\ners can signiﬁcantly boost decoding speed without\nharming quality. Recently, Kasai et al. (2021) inde-\npendently and similar to our encoder-decoder layer\ntrading approach, compare the performance and\nspeed of a 12-layer encoder 1-layer decoder with\nNon-Autoregressive Translation (NAT) approaches,\nand show that a one-layer autoregressive decoder\ncan yield state-of-the-art accuracy with comparable\nlatency to strong non-autoregressive models. Our\nanalysis explains why using a deep encoder with a\nshallow decoder is feasible, and we show that some\nencoder-decoder depth conﬁgurations deliver both\nincreased speed and increased translation quality.\n2 Probing Layer-wise Word Translation\nTo analyze word translation accuracy of the Trans-\nformer, we ﬁrst freeze a trained Transformer model\nso its behavior is consistent in how it performs in\ntranslation during our analysis. We then extract out-\nput representations of the particular layer analyzed,\napply a linear projection layer to extract features\nrelated to translation and feed the projected repre-\nsentations to the frozen decoder classiﬁer of the\ntrained Transformer. Our approach is minimally\ninvasive in that only the linear projection layer and\nthe weights of the alignment matrix A responsi-\nble for combining frozen cross-attention alignment\nmatrices from the decoder are trained and updated\non the training set, with the original Transformer\nbeing frozen. Thus the projection layer will only\n76\ntransform between vector spaces without generat-\ning new features for the word translation, and the\nalignment matrix A will only combine frozen cross-\nattention alignment matrices. A high-level illustra-\ntion of our analysis approach for encoder/decoder\nlayers is shown in Figure 1.\n2.1 Analysis of Encoder Layers\nAnalyzing word translation accuracy of encoder\nlayers requires us to align source tokens with cor-\nresponding target tokens. We use the frozen align-\nment matrices computed by cross-attention sub-\nlayers in decoder layers to align source tokens with\ntarget tokens (Figure 1). As there are multiple ma-\ntrices produced by each sub-layer (due to the multi-\nhead attention mechanism) and multiple decoder\nlayers, we have to ensemble them into one matrix\nof high alignment accuracy using weights. Assume\nthere are d decoder layers with k attention heads in\neach multi-head attention sub-layer, which results\nin d ∗k alignment matrices A1, ..., Ad∗k. We use\na d ∗k dimension weight vector w to combine all\nattention matrices. The weight vector is normalized\nby softmax to a probability distribution p:\npi = ewi\nd∗k∑\nj=1\newj\n(1)\nwhere i indicates the ith element in w.\nThen we use p as the weights of the correspond-\ning attention matrices and merge them into one\nalignment matrix A.\nA =\nd∗k∑\ni=1\nAi ∗pi (2)\nw is trained with the linear projection layer\nthrough backpropagation on the frozen Trans-\nformer.\nAfter we obtain the alignment matrix A, instead\nof selecting the target token with the highest align-\nment weight as the translation of a source token,\nwe perform matrix multiplication between the en-\ncoded source representations E (size: source sen-\ntence length ∗input dimension) and the alignment\nmatrix A (size: source sentence length ∗target\nsentence length) to transform/re-order source rep-\nresentations to the target side TE:\nTE = AT ×E (3)\nwhere AT and ×indicate the transpose of A and\nmatrix multiplication.\nThus TE has the same length as the gold transla-\ntion sequence, and the ground-truth target sequence\ncan be used directly as the translation represented\nby TE.\nThough source representations are transformed\nto the target side, we suggest this does not in-\nvolve any target side information as the pre-trained\nTransformer is frozen and the transformation does\nnot introduce any representation from the decoder\nside. We do not retrieve target tokens with the\nhighest alignment score as word translations of\ncorresponding source tokens because translation\nmay involve zero/one/multiple source token(s) to\nzero/one/multiple target token(s) alignments, and\nwe suggest that using a soft alignment (attention\nweights) may lead to more reliable gradients than a\nhard alignment.\n2.2 Analysis of Decoder Layers\nThe analysis of the prediction accuracy of the de-\ncoder is simpler than the encoder, as we can di-\nrectly use the shifted target sequence (teacher forc-\ning) without the requirement to bridge different\nsequence lengths between the source sentence and\nthe target while analyzing the encoder. We use the\noutput representations of the analyzed layer, and\nevaluate its prediction accuracy after projection.\nHowever, as studied by Li et al. (2019a), the de-\ncoder involves two kinds of “translation”. One (per-\nformed by the self-attention sub-layer) translates\nthe history token sequence to the next token, an-\nother (performed by the cross-attention sub-layer)\ntranslates by attending source tokens. We addi-\ntionally analyze the effects of these two kinds of\ntranslation on predicting accuracy by dropping the\ncorresponding sub-layer (either cross- or masked\nself-attention) of the analyzed decoder layer (i.e.,\nwe only compute the other sub-layer and the feed-\nforward layer where only the residual connection is\nkept as the computation of the skipped sub-layer).\n3 Analysis Experiments\n3.1 Settings\nWe ﬁrst trained a Transformer base model for our\nanalysis on the popular WMT 14 English to Ger-\nman news translation task to compare with Vaswani\net al. (2017). We employed a 512 ∗512 parameter\nmatrix as the linear projection layer. The source\nembedding matrix, the target embedding matrix\nand the weight matrix of the classiﬁer were tied.\nParameters were initialized under the Lipschitz con-\n77\nLayer\nEncoder Decoder\nAcc ∆ Acc ∆\n-Self attention -Cross attention\nAcc ∆ Acc ∆\n0 40.73 13.72\n1 41.85 1.12 20.52 6.80 17.46 -3.06 16.47 -4.05\n2 43.75 1.90 26.06 5.54 21.03 -5.03 22.91 -3.15\n3 45.49 1.74 34.13 8.07 26.68 -7.45 27.79 -6.34\n4 47.14 1.65 55.00 20.87 39.43 -15.57 35.32 -19.68\n5 48.35 1.21 66.14 11.14 62.60 -3.54 55.84 -10.30\n6 49.22 0.87 70.80 4.66 70.13 -0.67 69.03 -1.77\nTable 1: Word translation accuracy of Transformer layers on the WMT 14 En-De task.\nLayer\nEncoder Decoder\nAcc ∆ Acc ∆\n-Self attention -Cross attention\nAcc ∆ Acc ∆\n0 41.87 16.26\n1 43.61 1.74 25.73 9.47 23.31 -2.42 18.89 -6.84\n2 45.26 1.65 32.55 6.82 27.10 -5.45 26.82 -5.73\n3 46.68 1.42 40.80 8.25 34.05 -6.75 32.84 -7.96\n4 47.88 1.20 55.60 14.80 47.29 -8.31 40.48 -15.12\n5 48.73 0.85 64.39 8.79 62.41 -1.98 55.69 -8.70\n6 49.39 0.66 67.10 2.71 66.93 -0.17 66.31 -0.79\nTable 2: Word translation accuracy of Transformer layers on the WMT 15 Cs-En task.\nstraint (Xu et al., 2020) to ensure the convergence\nof deep encoders. We implemented our approaches\nbased on the Neutron implementation (Xu and Liu,\n2019) of the Transformer translation model.\nWe applied joint Byte-Pair Encoding (BPE)\n(Sennrich et al., 2016b) with 32k merge operations.\nWe only kept sentences with a maximum of 256\nsub-word tokens for training. The concatenation\nof newstest 2012 and newstest 2013 was used for\nvalidation and newstest 2014 as the test set.\nThe number of warm-up steps was set to 8k.1\nThe model was trained for 100k training steps with\naround 25k target tokens in each batch. We fol-\nlowed all the other settings of Vaswani et al. (2017).\nWe averaged the last5 checkpoints saved with an\ninterval of 1, 500 training steps. For decoding, we\nused a beam size of4, and evaluated tokenized case-\nsensitive BLEU.2 The averaged model achieved a\n1https://github.com/tensorflow/\ntensor2tensor/blob/v1.15.4/\ntensor2tensor/models/transformer.py#\nL1818.\n2https://github.com/moses-smt/\nmosesdecoder/blob/master/scripts/\ngeneric/multi-bleu.perl.\nBLEU score of 27.96 on the test set.\nThe projection matrix and the weight vector w\nof 48 elements for alignment were trained on the\ntraining set with the frozen Transformer. We mon-\nitored the accuracy on the development set, and\nreport results on the test set.\n3.2 Analysis\nThe analysis results of the trained Transformer are\nshown in Table 1. Layer 0 stands for the embed-\nding layer. “Acc” indicates the prediction accuracy.\n“-Self attention” and “-Cross attention” in the de-\ncoder layer analysis mean bypassing the compu-\ntation of the masked self-attention sub-layer and\nthe cross-attention sub-layer respectively of the an-\nalyzed decoder layer using a residual connection.\nIn our layer analysis of the encoder and decoder,\n“∆” indicates improvements in word translation\naccuracy of the analyzed layer over the previous\nlayer. While analyzing the self-attention and cross-\nattention sub-layers, “∆” is the accuracy loss when\nwe remove the computation of the corresponding\nsub-layer.\nThe results of the encoder layers in Table 1 show\n78\nthat: 1) encoder layers already perform word trans-\nlation, and the translation even starts at the em-\nbedding layer with unexpectedly high accuracy.\n2) With the stacking of encoder layers, the word\ntranslation accuracy improves, and improvements\nbrought about by different layers are relatively sim-\nilar, indicating that all encoder layers are useful.\nSurprisingly, analyzing decoder layers, Table 1\nshows that: 1) shallow decoder layers (0, 1, 2 and\n3) perform signiﬁcantly worse compared to the\ncorresponding encoder layers (all the way up until\nthe 4th decoder layer, where a word translation\naccuracy which surpasses the embedding layer of\nthe encoder is achieved); 2) The improvements\nbrought about by different decoder layers are quite\ndifferent. Speciﬁcally, the relative performance\nincreases between the low-performance decoder\nlayers (0, 1, 2 and 3) are low as well, while layers\n4 and 5 bring more improvements than the others.\nWhile analyzing the effects of the source context\n(“-Cross attention” prevents informing translation\nby the source “encoding”) and the decoding history\n(the self-attention sub-layer is responsible for the\ntarget language re-ordering, and “-Self attention”\nprevents using the decoding history in the analyzed\ndecoder layer), Table 1 shows that in shallow de-\ncoder layers (layer 1-3), the decoding history is as\nimportant as the source “encoding”, while in deep\ndecoder layers, the source “encoding” plays a more\nvital role than the decoding history. Overall, our\nresults provide new insights on the importance of\ntranslation already performed by the encoder.\nSince the English-German translation shares\nmany sub-words naturally (∼13.89% source sub-\nwords including punctuations exist in the subword\nset of the corresponding target translation in the\ntraining set), we additionally provide results on the\nWMT 15 Cs-En task in Table 2. Table 2 conﬁrms\nour observations reported in Table 1.\nZhang and Bowman (2018); Hewitt and Liang\n(2019); V oita and Titov (2020) articulate concerns\nabout analyses with probing accuracies, as differ-\nences in accuracies fail to reﬂect differences in\nrepresentations in several “sanity checks”. Speciﬁ-\ncally, Zhang and Bowman (2018) compare probing\nscores for trained models and randomly initialized\nones, and observe reasonable differences in the\nscores only when reducing the amount of classiﬁer\ntraining data. However, we argue that in our work,\nwe use the frozen classiﬁer of the pre-trained Trans-\nformer decoder as our probing classiﬁer, and the\nLayer BLEU 1 ∆ BLEU ∆\n0 33.1 7.92\n1 35.7 2.6 8.99 1.07\n2 41.0 5.3 11.05 2.06\n3 43.3 2.3 11.89 0.84\n4 46.8 3.5 13.13 1.24\n5 48.1 1.3 13.34 0.21\n6 48.6 0.5 13.45 0.11\nFULL 62.0 13.4 33.26 19.81\nTable 3: Translation performance of encoder layers on\nthe WMT 14 En-De task.\nintroduced linear projection, as well as the align-\nment matrix A, are much smaller and weaker than\nthe frozen classiﬁer and the rest of the frozen Trans-\nformer components. Thus we suggest that our ap-\nproach is minimally invasive and that our analysis\nis less likely to be seriously affected by this issue\neven though we use a large training set. To empiri-\ncally verify this, we apply our analysis approach on\na randomly initialized encoder and evaluate word\ntranslation accuracies obtained by the source em-\nbedding layer and last encoder layer, while the\nalignment between the source and the target is still\nfrom the pre-trained model. Both the source em-\nbedding layer and the last encoder layer resulted\nin the same accuracy of 23.66. Compared to the\ncorresponding values (40.73 and 49.22) in Table\n1, the gap between the randomly initialized layers\nand the pre-trained layers in accuracy is signiﬁcant,\nand the gap between accuracy improvements from\nthe representation extracted from the source embed-\nding layer and propagated through all intermediate\nlayers to the last encoder layer of pre-trained layers\n(8.49) and randomly initialized layers (0.00) is also\nsigniﬁcant. Thus, we suggest our analysis is robust.\n3.3 Translation from Encoder Layers\nwithout Using Decoder Layers\nSince our approach extracts features for transla-\ntion from encoder states while analyzing them, is\nit possible to perform word translation with only\nthese features from encoder layers without using\nthe decoder except the frozen classiﬁer?\nTo test this question, we feed output representa-\ntions from an encoder layer to the corresponding\nlinear projection layer, and feed the output of the\nlinear projection layer directly to the frozen de-\ncoder classiﬁer, and retrieve tokens with the high-\nest probabilities as “translations”. Even though\n79\nsuch “translations” from encoder layers have the\nsame length and the same word order as source\nsentences, individual source tokens are translated\nto the target language to some extent. We evalu-\nated BPEized 3 case-insensitive BLEU and BLEU\n1 (1-gram BLEU, indicates the word translation\nquality), and results are shown in Table 3. “FULL”\nis the performance of the whole Transformer model\n(decoding with a beam size of 4). “∆” means the\nimprovements obtained by the introduced layer (or\nthe decoder for “FULL”) over the previous layer.\nTable 3 shows that while there is a signiﬁcant gap\nin BLEU scores between encoder layers and the\nfull Transformer, the gap in BLEU 1 is relatively\nsmaller than in BLEU. It is reasonable that encoder\nlayers achieve a comparably high BLEU 1 score but\na low BLEU score overall, as they perform word\ntranslation in the same order as the source sentence\nwithout any word re-ordering of the target language.\nWe suggest that the BLEU 1 score achieved by only\nthe source embedding layer (i.e., translating with\nonly embeddings) is surprising and worth noting.\n3.4 Discussion\nOur probing approach involves crucial information\nfrom the decoder (encoder-decoder attention from\nall decoder layers). However, we argue that probe\ntraining requires supervision. For the decoder, we\ncan directly use gold references. On the encoder\nside, parallel data does not provide word transla-\ntions for source tokens, and we have to generate\nthis data by aligning target tokens to source tokens.\nOne choice is extracting alignments by taking an\nargmax of alignment matrices or using toolkits like\nfastalign (Dyer et al., 2013). In this case, probe\ntraining does not involve attention matrices, but\nthis has drawbacks: multiple/no target tokens may\nalign to one source token. We use soft aggregation\nto preserve more information (other attention possi-\nbilities besides the highest are kept) and to alleviate\nerror propagation. We argue that the use of atten-\ntion matrices is only to bring supervision (word\ntranslations) from the target side to the source side,\nwhich is inevitable. Decoder representations can-\nnot ﬂow back to the frozen encoder.\nOur paper also empirically reveals the impact of\nattention matrices: 1) In Section 3.3, where after\nthe training of source probes, we decode target to-\nkens with only encoder layers, the trained probe\n3Since there is no re-ordering of the target language per-\nformed, which makes the merging of translated sub-word units\nin the source sentence order pointless.\n(without involving cross-attention networks) and\nthe pre-trained classiﬁer. 2) In the last paragraph of\nSection 3.2, we train probes with alignment matri-\nces from the pre-trained model but a frozen random\nencoder, showing the effects of cross-attention ma-\ntrices on the probe.\n4 Trading Decoder for Encoder Layers\n4.1 Motivation\nFrom our analysis of the 6-layer Transformer base\nmodel (Table 1), we ﬁnd that in contrast to the im-\nprovements of the word translation accuracy with\nincreasing depth on the encoder side, some decoder\nlayers contribute signiﬁcantly fewer improvements\nthan others (i.e., layers 4 and 5 bring more word\ntranslation accuracy improvements than those from\nlayers 1, 2, 3 and 6 in Table 1). This suggests that\nthere might be more “lazy” layers in the decoder\nthan in the encoder, which means that it might be\neasier to compress the decoder than the encoder,\nand further we conjecture that simply removing\nsome decoder layers while adding the same number\nof encoder layers may even improve the translation\nquality of the transformer. Motivations targeting\nefﬁciency include:\n• Each decoder layer has one more cross-\nattention sub-layer than an encoder layer, and\nincreasing encoder layers while decreasing the\nsame number of decoder layers will reduce the\nnumber of parameters and computational cost;\n• During inference, the decoder has to autore-\ngressively compute the forward pass for every\ndecoding step (the decoding of each target to-\nken), which prevents efﬁcient parallelization,\nwhile encoder layers are non-autoregressively\npropagated and highly parallelized, and the\nacceleration caused by using fewer decoder\nlayers with more encoder layers will be more\nsigniﬁcant in decoding, which is of practical\nvalue.\n4.2 Results and Analysis\nWe examine the effects of reducing the number of\ndecoder layers while adding corresponding num-\nbers of encoder layers, and results are shown in\nTable 4. “Speed up” stands for the decoding accel-\neration compared to the 6-layer Transformer.\nTable 4 shows that while the acceleration of trad-\ning decoder layers for encoder layers in training is\nsmall, in decoding it is signiﬁcant. Speciﬁcally, the\n80\nModel Depth\nBLEU Para. (M)\nTime\nEncoder Decoder Train Decode (/s) Speed up\nZhang et al. (2018a) 6 6 28.13 74.97 40h09m 29 1.52\nTransformer\n6 6 27.96 62.37 33h33m 44 1.00\n7 5 28.07 61.32 32h17m 38 1.16\n8 4 28.61 60.27 31h26m 31 1.42\n9 3 28.53 59.22 30h29m 25 1.76\n10 2 28.47 58.17 30h11m 19 2.32\n11 1 27.02 57.12 29h27m 13 3.38\n18 4 29.38 91.77 52h56m 32 1.38\nTable 4: Effects of encoder/decoder depth on the WMT 14 En-De task. The decoding time is for the test set of\n3, 003 sentences with a beam size of 4.\nLayer\nEncoder Decoder\nAcc ∆ Acc ∆\n-Self attention -Cross attention\nAcc ∆ Acc ∆\n0 40.48 14.04\n1 41.29 0.81 37.42 23.38 25.56 -11.86 20.40 -17.02\n2 43.00 1.71 68.77 31.35 62.01 -6.76 40.67 -28.10\n3 44.07 1.07\n4 45.86 1.79\n5 46.54 0.68\n6 47.46 0.92\n7 48.92 1.46\n8 49.58 0.66\n9 50.24 0.66\n10 50.35 0.11\nTable 5: Word accuracy analysis on Transformer with 10 encoder and 2 decoder layers on the WMT 14 En-De\ntask.\nTransformer with 10 encoder layers and 2 decoder\nlayers is 2.32 times as fast as the 6-layer Trans-\nformer while achieving a slightly higher BLEU.\nCan we use more than 12 encoder layers with a\nshallow decoder to beneﬁt both translation quality\nand inference speed? Table 4 shows that the 18-4\nmodel 4 brings about +1.42 BLEU improvements\nover the strong baseline, while being 1.38 times as\nfast in decoding. Comparing the 18-4 model to the\n8-4 model, the time cost for using 10 more encoder\nlayers only increases 1 second for translating the\ntest set, suggesting that autoregressive decoding\n4A full grid search over conﬁgurations is tedious and ex-\npensive. We take inspiration from Table 4 where going from\n5 to 4 decoder layers brings about the biggest relative jump\nin translation quality. We explored a few conﬁgurations and\nﬁnd that using more than 18 encoder layers can still bring\nimprovements, but the gains are relatively small.\nspeed is quite insensitive to the encoder depth.\nOur results show that using more encoder layers\nwith fewer but sufﬁcient decoder layers can signiﬁ-\ncantly boost the decoding speed with small gains\nin translation quality, and that a good choice in the\ndistribution of encoder and decoder layers (18-4)\ncan result in slightly faster decoding and a substan-\ntial increase in translation quality, which is simple\nbut effective and valuable for back-translation (Sen-\nnrich et al., 2016a) and production applications.\nWe present the word accuracy analysis results of\nthe 10 encoder layer - 2 decoder layer Transformer\non the En-De task in Table 5. Comparing Table 5\nwith Table 1, we ﬁnd that: 1) The differences in\nimprovements (1.71 vs. 0.11) brought by individ-\nual layers of the 10-layer encoder are larger than\nthose of the 6-layer encoder (1.90 vs. 0.87), indi-\n81\nDepth En-De En-Fr Cs-EnEncoder Decoder\n6 27.96 40.13 28.69\n10 2 28.47 40.49 28.87\n18 4 29.38† 40.90† 29.75†\nTable 6: Veriﬁcation of deep encoder and shallow de-\ncoder on WMT En-De, En-Fr and Cs-En tasks. †indi-\ncates signiﬁcance at p <0.01.\ncating that there might now be some “lazy” layers\nin the 10-layer encoder; 2) Decreasing the depth\nof the decoder removes “lazy” decoder layers in\nthe 6-layer decoder and makes decoder layers rely\nmore on the source “encoding” (by comparing the\neffects of skipping the self-attention sub-layer and\ncross-attention sub-layer on performance).\n4.3 Veriﬁcation of Deep Encoder and Shallow\nDecoder on other Language Pairs\nTo investigate how a deep encoder with a shallow\ndecoder will perform in other tasks, we conducted\nexperiments on the WMT 14 English-French and\nWMT 15 Czech-English news translation tasks in\naddition to the WMT 14 English-German task. Re-\nsults on newstest 2014 (En-De/Fr) and 2015 (Cs-\nEn) respectively are shown in Table 6.\nTable 6 shows that the10-2 model consistently\nachieves higher BLEU scores than the 6-layer\nmodel, and the 18-4 model consistently leads to\nsigniﬁcant improvements in all 3 tasks.\n5 Related Work\nAnalysis of NMT Models. Belinkov et al.\n(2020) analyze the representations learned by NMT\nmodels at various levels of granularity and evaluate\ntheir quality through relevant extrinsic properties.\nLi et al. (2019a) analyze the word alignment qual-\nity in NMT and the effect of alignment errors on\ntranslation errors. They demonstrate that NMT cap-\ntures word alignment much better for those words\nmostly contributed from the source than those from\nthe target. V oita et al. (2019b) evaluate the contri-\nbution of individual attention heads to the overall\nperformance of the model and analyze the roles\nplayed by them in the encoder. Yang et al. (2019)\npropose a word reordering detection task to quan-\ntify how well the word order information is learned\nby Self-Attention Networks and RNN, and reveal\nthat although recurrence structure makes the model\nmore universally effective on learning word order,\nlearning objectives matter more in the downstream\ntasks such as machine translation. Tsai et al. (2019)\nregard attention as applying a kernel smoother over\nthe inputs with the kernel scores being the similar-\nities between inputs, and analyze individual com-\nponents of the Transformer’s attention with the\nnew formulation via the lens of the kernel. Tang\net al. (2019) ﬁnd that encoder hidden states out-\nperform word embeddings signiﬁcantly in word\nsense disambiguation. He et al. (2019) measure\nthe word importance by attributing the NMT out-\nput to every input word and reveal that words of\ncertain syntactic categories have higher importance\nwhile the categories vary across language pairs.\nV oita et al. (2019a) use canonical correlation anal-\nysis and mutual information estimators to study\nhow information ﬂows across Transformer layers.\nEarly work by Bisazza and Tump (2018) performs\na ﬁne-grained analysis of how various source-side\nmorphological features are captured at different lev-\nels of the NMT encoder. While they are unable to\nﬁnd any correlation between the accuracy of source\nmorphology encoding and translation quality, they\ndiscover that morphological features are only cap-\ntured in context and only to the extent that they\nare directly transferable to the target words, and\nsuggest encoder layers are “lazy”. Our analysis\noffers an explanation for their results as the trans-\nlation already starts at the source embedding layer,\nand possibly source embeddings already represent\nlinguistic features of their translations.\nAnalysis of BERT. BERT (Devlin et al., 2019)\nuses the Transformer encoder, and analysis of\nBERT may provide valuable references for analyz-\ning the Transformer. Jawahar et al. (2019) provide\nsupport that BERT networks capture structural in-\nformation, and perform a series of experiments to\nunpack the elements of English language structure\nlearned by BERT. Tenney et al. (2019) employ the\nedge probing task suite, and ﬁnd that BERT rep-\nresents the steps of the traditional NLP pipeline\nin an interpretable and localizable way, and that\nthe regions responsible for each step appear in the\nexpected sequence: POS tagging, parsing, NER,\nsemantic roles, then coreference. Pires et al. (2019)\npresent a large number of probing experiments,\nand show that Multilingual-BERT’s robust ability\nto generalize cross-lingually is underpinned by a\nmultilingual representation.\n82\nAccelerating Decoding. Zhang et al. (2018a)\npropose average attention as an alternative to the\nself-attention network in the Transformer decoder\nto accelerate decoding. Wu et al. (2019) introduce\nlightweight convolution and dynamic convolutions.\nThe number of operations required by their ap-\nproach scales linearly in the input length, whereas\nself-attention is quadratic. Zhang et al. (2018b)\napply cube pruning to neural machine translation\nto speed up translation. Zhang et al. (2018c) pro-\npose to adopt an n-gram sufﬁx-based equivalence\nfunction into beam search decoding, which ob-\ntains similar translation quality with a smaller beam\nsize, making NMT decoding more efﬁcient. Non-\nAutoregressive Translation (NAT) (Gu et al., 2018;\nLibovický and Helcl, 2018; Wei et al., 2019; Shao\net al., 2019; Li et al., 2019b; Wang et al., 2019; Guo\net al., 2019) enables parallelized decoding, while\nthere is still a signiﬁcant quality drop compared to\ntraditional autoregressive beam search, our ﬁndings\non using more encoder layers might also be adapted\nto NAT. Recently, and independently of our work,\nKasai et al. (2021) compare the performance and\nspeed between a 12-layer encoder 1-layer decoder\ncase with NAT approaches, and show that a one-\nlayer autoregressive decoder yields state-of-the-art\naccuracy with comparable latency to strong non-\nautoregressive models. Our work explains why\nusing a deep encoder with a shallow decoder is\nfeasible, and we show that substantial increases\nin decoding speed are possible with small gains\nin translation quality, and that for some conﬁgu-\nrations (e.g., 18-4) signiﬁcant translation quality\nincreases with modest increases in decoding speed\nare possible.\n6 Conclusion\nWe propose approaches for the analysis of word\ntranslation accuracy of Transformer layers to inves-\ntigate how translation is performed. To measure\nword translation accuracy, our approach trains a\nlinear projection layer that bridges representations\nfrom the frozen pre-trained analyzed layer and the\nfrozen pre-trained classiﬁer. While analyzing en-\ncoder layers, our approach additionally learns a\nweight vector to merge multiple attention matrices\ninto one, and transforms the source “encoding” to\nthe target shape by multiplying the merged align-\nment matrix. Both the linear projection layer and\nthe weight vector are trained on the frozen Trans-\nformer. This is minimally invasive, and training the\nnew parameters does not account for the ﬁndings\nreported. For the analysis of decoder layers, we\nadditionally analyze the effects of the source con-\ntext and the decoding history in word prediction\nthrough bypassing the corresponding cross- and\nself-attention sub-layers. Our ﬁndings motivate\nand explain the beneﬁts of trading decoder for en-\ncoder layers in our approach and that of Kasai et al.\n(2021).\nOur analysis is the ﬁrst to reveal the existence of\ntarget translations performed by encoder layers (in-\ncluding the source embedding layer). We show that\nincreasing encoder depth while removing decoder\nlayers can lead to signiﬁcant BLEU improvements\nwhile boosting the decoding speed.\nAcknowledgements\nWe thank anonymous reviewers for their insight-\nful comments. Hongfei Xu acknowledges the sup-\nport of China Scholarship Council ([2018]3101,\n201807040056). Josef van Genabith is supported\nby the German Federal Ministry of Education and\nResearch (BMBF) under funding code 01IW20010\n(CORA4NLP). Deyi Xiong is supported by the Na-\ntional Natural Science Foundation of China (Grant\nNo. 61861130364), the Natural Science Founda-\ntion of Tianjin (Grant No. 19JCZDJC31400) and\nthe Royal Society (London) (NAF\\R1\\180122).\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2017. Fine-grained anal-\nysis of sentence embeddings using auxiliary pre-\ndiction tasks. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net.\nGuillaume Alain and Yoshua Bengio. 2017. Under-\nstanding intermediate layers using linear classiﬁer\nprobes. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Workshop Track Proceedings .\nOpenReview.net.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2020. On the linguistic\nrepresentational power of neural machine translation\nmodels. Computational Linguistics, 46(1):1–52.\n83\nArianna Bisazza and Clara Tump. 2018. The lazy en-\ncoder: A ﬁne-grained analysis of the role of mor-\nphology in neural machine translation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2871–2876,\nBrussels, Belgium. Association for Computational\nLinguistics.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Loïc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nChris Dyer, Victor Chahuneau, and Noah A. Smith.\n2013. A simple, fast, and effective reparameter-\nization of IBM model 2. In Proceedings of the\n2013 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies , pages 644–648, At-\nlanta, Georgia. Association for Computational Lin-\nguistics.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nsequence to sequence learning. In Proceedings\nof the 34th International Conference on Machine\nLearning, volume 70 of Proceedings of Machine\nLearning Research, pages 1243–1252, International\nConvention Centre, Sydney, Australia. PMLR.\nJiatao Gu, James Bradbury, Caiming Xiong, Vic-\ntor O.K. Li, and Richard Socher. 2018. Non-\nautoregressive neural machine translation. In Inter-\nnational Conference on Learning Representations.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu,\nand Tie-Yan Liu. 2019. Non-autoregressive neu-\nral machine translation with enhanced decoder in-\nput. In The Thirty-Third AAAI Conference on Artiﬁ-\ncial Intelligence, AAAI 2019, The Thirty-First Inno-\nvative Applications of Artiﬁcial Intelligence Confer-\nence, IAAI 2019, The Ninth AAAI Symposium on Ed-\nucational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 3723–3730. AAAI Press.\nShilin He, Zhaopeng Tu, Xing Wang, Longyue Wang,\nMichael Lyu, and Shuming Shi. 2019. Towards un-\nderstanding neural machine translation with word\nimportance. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 953–962, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJohn Hewitt and Percy Liang. 2019. Designing and\ninterpreting probes with control tasks. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2733–2743, Hong\nKong, China. Association for Computational Lin-\nguistics.\nDieuwke Hupkes, Sara Veldhoen, and Willem Zuidema.\n2018. Visualisation and ‘diagnostic classiﬁers’ re-\nveal how recurrent and recursive neural networks\nprocess hierarchical structure. J. Artif. Int. Res. ,\n61(1):907–926.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nJungo Kasai, Nikolaos Pappas, Hao Peng, James Cross,\nand Noah Smith. 2021. Deep encoder, shallow\ndecoder: Reevaluating non-autoregressive machine\ntranslation. In International Conference on Learn-\ning Representations.\nXintong Li, Guanlin Li, Lemao Liu, Max Meng, and\nShuming Shi. 2019a. On the word alignment from\nneural machine translation. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1293–1303, Florence,\nItaly. Association for Computational Linguistics.\nZhuohan Li, Zi Lin, Di He, Fei Tian, Tao Qin, Liwei\nWang, and Tie-Yan Liu. 2019b. Hint-based train-\ning for non-autoregressive machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5708–\n5713, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJindˇrich Libovický and Jind ˇrich Helcl. 2018. End-to-\nend non-autoregressive neural machine translation\nwith connectionist temporal classiﬁcation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3016–\n3021, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4996–\n5001, Florence, Italy. Association for Computa-\ntional Linguistics.\n84\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016a. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n86–96, Berlin, Germany. Association for Computa-\ntional Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016b. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nChenze Shao, Yang Feng, Jinchao Zhang, Fandong\nMeng, Xilin Chen, and Jie Zhou. 2019. Retrieving\nsequential information for non-autoregressive neural\nmachine translation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3013–3024, Florence, Italy. Asso-\nciation for Computational Linguistics.\nGongbo Tang, Rico Sennrich, and Joakim Nivre. 2019.\nEncoders help you disambiguate word senses in\nneural machine translation. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1429–1435, Hong Kong,\nChina. Association for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nYao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,\nLouis-Philippe Morency, and Ruslan Salakhutdinov.\n2019. Transformer dissection: An uniﬁed under-\nstanding for transformer’s attention via the lens of\nkernel. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4344–4353, Hong Kong, China. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396–4406, Hong Kong,\nChina. Association for Computational Linguistics.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019b. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\narXiv preprint arXiv:2003.12298.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019. Non-autoregressive\nmachine translation with auxiliary regularization. In\nThe Thirty-Third AAAI Conference on Artiﬁcial In-\ntelligence, AAAI 2019, The Thirty-First Innovative\nApplications of Artiﬁcial Intelligence Conference,\nIAAI 2019, The Ninth AAAI Symposium on Edu-\ncational Advances in Artiﬁcial Intelligence, EAAI\n2019, Honolulu, Hawaii, USA, January 27 - Febru-\nary 1, 2019, pages 5377–5384. AAAI Press.\nBingzhen Wei, Mingxuan Wang, Hao Zhou, Junyang\nLin, and Xu Sun. 2019. Imitation learning for non-\nautoregressive neural machine translation. In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 1304–\n1312, Florence, Italy. Association for Computational\nLinguistics.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019. Pay less attention with\nlightweight and dynamic convolutions. In Interna-\ntional Conference on Learning Representations.\nHongfei Xu and Qiuhui Liu. 2019. Neutron: An Im-\nplementation of the Transformer Translation Model\nand its Variants. arXiv preprint arXiv:1903.07402.\nHongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi\nXiong, and Jingyi Zhang. 2020. Lipschitz con-\nstrained parameter initialization for deep transform-\ners. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 397–402, Online. Association for Computa-\ntional Linguistics.\nBaosong Yang, Longyue Wang, Derek F. Wong,\nLidia S. Chao, and Zhaopeng Tu. 2019. Assessing\nthe ability of self-attention networks to learn word\norder. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 3635–3644, Florence, Italy. Association\nfor Computational Linguistics.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2018a. Ac-\ncelerating neural transformer via an average atten-\ntion network. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 1789–\n1798, Melbourne, Australia. Association for Compu-\ntational Linguistics.\n85\nKelly Zhang and Samuel Bowman. 2018. Language\nmodeling teaches you more than translation does:\nLessons learned through auxiliary syntactic task\nanalysis. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 359–361, Brussels, Bel-\ngium. Association for Computational Linguistics.\nWen Zhang, Liang Huang, Yang Feng, Lei Shen, and\nQun Liu. 2018b. Speeding up neural machine trans-\nlation decoding by cube pruning. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 4284–4294, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nZhisong Zhang, Rui Wang, Masao Utiyama, Eiichiro\nSumita, and Hai Zhao. 2018c. Exploring recombina-\ntion for efﬁcient decoding of neural machine trans-\nlation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 4785–4790, Brussels, Belgium. Association\nfor Computational Linguistics."
}