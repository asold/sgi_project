{
  "title": "ViDT: An Efficient and Effective Fully Transformer-based Object Detector",
  "url": "https://openalex.org/W3206263120",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4202080712",
      "name": "Song, Hwanjun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2224568608",
      "name": "Sun, Deqing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222306723",
      "name": "Chun, Sanghyuk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3204959590",
      "name": "Jampani, Varun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3085681968",
      "name": "Han, Dongyoon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202169409",
      "name": "Heo, Byeongho",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2370940768",
      "name": "Kim, Wonjae",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200861175",
      "name": "Yang, Ming-Hsuan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3203606893",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W1608462934",
    "https://openalex.org/W3173270634",
    "https://openalex.org/W2988916019",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2750784772",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3170778815",
    "https://openalex.org/W3202053489",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3202715235",
    "https://openalex.org/W3202406646"
  ],
  "abstract": "Transformers are transforming the landscape of computer vision, especially for recognition tasks. Detection transformers are the first fully end-to-end learning systems for object detection, while vision transformers are the first fully transformer-based architecture for image classification. In this paper, we integrate Vision and Detection Transformers (ViDT) to build an effective and efficient object detector. ViDT introduces a reconfigured attention module to extend the recent Swin Transformer to be a standalone object detector, followed by a computationally efficient transformer decoder that exploits multi-scale features and auxiliary techniques essential to boost the detection performance without much increase in computational load. Extensive evaluation results on the Microsoft COCO benchmark dataset demonstrate that ViDT obtains the best AP and latency trade-off among existing fully transformer-based object detectors, and achieves 49.2AP owing to its high scalability for large models. We will release the code and trained models at https://github.com/naver-ai/vidt",
  "full_text": "VIDT: A N EFFICIENT AND EFFECTIVE\nFULLY TRANSFORMER -BASED OBJECT DETECTOR\nHwanjun Song1, Deqing Sun2, Sanghyuk Chun1, Varun Jampani2, Dongyoon Han1,\nByeongho Heo1, Wonjae Kim1, Ming-Hsuan Yang2,3\n1NA VER AI Lab2Google Research 3University of California at Merced\n{hwanjun.song, sanghyuk.c, dongyoon.han, bh.heo, wonjae.kim}@navercorp.com\n{deqingsun, varunjampani}@google.com, mhyang@ucmerced.edu\nABSTRACT\nTransformers are transforming the landscape of computer vision, especially for\nrecognition tasks. Detection transformers are the ﬁrst fully end-to-end learn-\ning systems for object detection, while vision transformers are the ﬁrst fully\ntransformer-based architecture for image classiﬁcation. In this paper, we integrate\nVision and Detection Transformers (ViDT) to build an effective and efﬁcient ob-\nject detector. ViDT introduces a reconﬁgured attention module to extend the recent\nSwin Transformer to be a standalone object detector, followed by a computation-\nally efﬁcient transformer decoder that exploits multi-scale features and auxiliary\ntechniques essential to boost the detection performance without much increase in\ncomputational load. Extensive evaluation results on the Microsoft COCO bench-\nmark dataset demonstrate that ViDT obtains the best AP and latency trade-off\namong existing fully transformer-based object detectors, and achieves49.2AP ow-\ning to its high scalability for large models. We will release the code and trained\nmodels at https://github.com/naver-ai/vidt.\n1 I NTRODUCTION\nObject detection is the task of predicting both bounding boxes and object classes for each object of\ninterest in an image. Modern deep object detectors heavily rely on meticulously designed compo-\nnents, such as anchor generation and non-maximum suppression (Papageorgiou & Poggio, 2000; Liu\net al., 2020). As a result, the performance of these object detectors depend on speciﬁc postprocessing\nsteps, which involve complex pipelines and make fully end-to-end training difﬁcult.\nMotivated by the recent success of Transformers (Vaswani et al., 2017) in NLP, numerous stud-\nies introduce Transformers into computer vision tasks. Carion et al. (2020) proposed Detection\nTransformers (DETR)to eliminate the meticulously designed components by employing a simple\ntransformer encoder and decoder architecture, which serves as a neck component to bridge a CNN\nbody for feature extraction and a detector head for prediction. Thus, DETR enables end-to-end train-\ning of deep object detectors. By contrast, Dosovitskiy et al. (2021) showed that a fully-transformer\n28\n34\n40\n46\n52\n20 80 140 200 260\nAP\nLatency (MS per Image)\nnano\ntiny\nsmall\nbase\nsmall\ntiny\nnano\ntiny\nsmall\nbase\ntiny small\nnano\ntiny\nsmall\ntiny\nsmall\nbase\nbase\nbase\nbase\n2532394653\nDETR (DeiT) DETR (Swin)\nDeformable DETR (DeiT) Deformable DETR (Swin)\nYOLOS ViDT\nFigure 1. AP and latency (milliseconds) summa-\nrized in Table 2. The text in the plot indicates the\nbackbone model size.\nbackbone without any convolutional layers, Vi-\nsion Transformer (ViT), achieves the state-of-the-\nart results in image classiﬁcation benchmarks. Ap-\nproaches like ViT have been shown to learn effective\nrepresentation models without strong human induc-\ntive biases, e.g., meticulously designed components\nin object detection (DETR), locality-aware designs\nsuch as convolutional layers and pooling mecha-\nnisms. However, there is a lack of effort to synergize\nDETR and ViT for a better object detection archi-\ntecture. In this paper, we integrate both approaches\nto build a fully transformer-based, end-to-end object\ndetector that achieves state-of-the-art performance\nwithout increasing computational load.\nA straightforward integration of DETR and ViT\ncan be achieved by replacing the ResNet backbone\n1\narXiv:2110.03921v2  [cs.CV]  29 Nov 2021\nBody Neck\nDet. \nViT\nBackbone Box \nReg.\nClass.\nHead\nBody\nHead\nNeck\nTrans.\nDec.\nDet.Patch Tokens Det.Patch Tokens\nBody\nDet.\nTrans.\nDec.\nTrans.\nEnc. Box \nReg.\nClass.\nHead\nDet.Patch Tokens\nDet.Patch Tokens\nDet.Patch Tokens\nPatch Tokens\nPatch\nDet.\nDet.\nDet.Patch\nBox \nReg.\nClass.\nPatch Tokens\n(a) DETR (ViT). (b) YOLOS. (c) ViDT (Ours).\nFigure 2. Pipelines of fully transformer-based object detectors. DETR (ViT) means Detection Transformer that\nuses ViT as its body. The proposed ViDT synergizes DETR (ViT) and YOLOS and achieves best AP and latency\ntrade-off among fully transformer-based object detectors.\n(body) of DETR with ViT – Figure 2(a). This naive integration, DETR (ViT)1, has two limitations.\nFirst, the canonical ViT suffers from the quadratic increase in complexity w.r.t. image size, resulting\nin the lack of scalability. Furthermore, the attention operation at the transformer encoder and decoder\n(i.e., the “neck” component) adds signiﬁcant computational overhead to the detector. Therefore, the\nnaive integration of DETR and ViT show very high latency – the blue lines of Figure 1.\nRecently, Fang et al. (2021) propose an extension of ViT to object detection, namedYOLOS, by ap-\npending the detection tokens [DET] to the patch tokens [PATCH] (Figure 2(b)), where [DET] tokens are\nlearnable embeddings to specify different objects to detect. YOLOS is a neck-free architecture and\nremoves the additional computational costs from the neck encoder. However, YOLOS shows limited\nperformance because it cannot use additional optimization techniques on the neck architecture, e.g.,\nmulti-scale features and auxiliary loss. In addition, YOLOS can only accommodate the canonical\ntransformer due to its architectural limitation, resulting in a quadratic complexity w.r.t. the input size.\nIn this paper, we propose a novel integration of Vision and Detection Transformers (ViDT) (Figure\n2(c)). Our contributions are three-folds. First, ViDT introduces a modiﬁed attention mechanism,\nnamed Reconﬁgured Attention Module (RAM), that facilitates any ViT variant to handle the ap-\npended [DET] and [PATCH] tokens for object detection. Thus, we can modify the latest Swin Trans-\nformer (Liu et al., 2021) backbone with RAM to be an object detector and obtain high scalabil-\nity using its local attention mechanism with linear complexity. Second, ViDT adopts a lightweight\nencoder-free neck architecture to reduce the computational overhead while still enabling the ad-\nditional optimization techniques on the neck module. Note that the neck encoder is unnecessary\nbecause RAM directly extracts ﬁne-grained representation for object detection, i.e., [DET] tokens.\nAs a result, ViDT obtains better performance than neck-free counterparts. Finally, we introduce a\nnew concept of token matching for knowledge distillation, which brings additional performance\ngains from a large model to a small model without compromising detection efﬁciency.\nViDT has two architectural advantages over existing approaches. First, similar to YOLOS, ViDT\ntakes [DET] tokens as the additional input, maintaining a ﬁxed scale for object detection, but con-\nstructs hierarchical representations starting with small-sized image patches for [PATCH] tokens. Sec-\nond, ViDT can use the hierarchical (multi-scale) features and additional techniques without a signiﬁ-\ncant computation overhead. Therefore, as a fully transformer-based object detector, ViDT facilitates\nbetter integration of vision and detection transformers. Extensive experiments on Microsoft COCO\nbenchmark (Lin et al., 2014) show that ViDT is highly scalable even for large ViT models, such as\nSwin-base with 0.1 billion parameters, and achieves the best AP and latency trade-off.\n2 P RELIMINARIES\nVision transformers process an image as a sequence of small-sized image patches, thereby allow-\ning all the positions in the image to interact in attention operations (i.e., global attention). However,\nthe canonical ViT (Dosovitskiy et al., 2021) is not compatible with a broad range of vision tasks\ndue to its high computational complexity, which increases quadratically with respect to image size.\nThe Swin Transformer (Liu et al., 2021) resolves the complexity issue by introducing the notion\nof shifted windows that support local attention and patch reduction operations, thereby improving\ncompatibility for dense prediction task such as object detection. A few approaches use vision trans-\nformers as detector backbones but achieve limited success (Heo et al., 2021; Fang et al., 2021).\n1We refer to each model based on the combinations of its body and neck. For example, DETR (DeiT)\nindicates that DeiT (vision transformers) is integrated with DETR (detection transformers).\n2\nDetection transformers eliminate the meticulously designed components (e.g., anchor generation\nand non-maximum suppression) by combining convolutional network backbones and Transformer\nencoder-decoders. While the canonical DETR (Carion et al., 2020) achieves high detection perfor-\nmance, it suffers from very slow convergence compared to previous detectors. For example, DETR\nrequires 500 epochs while the conventional Faster R-CNN (Ren et al., 2015) training needs only 37\nepochs (Wu et al., 2019). To mitigate the issue, Zhu et al. (2021) propose Deformable DETR which\nintroduces deformable attention for utilizing multi-scale features as well as expediting the slow\ntraining convergence of DETR. In this paper, we use the Deformable DETR as our base detection\ntransformer framework and integrate it with the recent vision transformers.\nDETR (ViT) is a straightforward integration of DETR and ViT, which uses ViT as a feature ex-\ntractor, followed by the transformer encoder-decoder in DETR. As illustrated in Figure 2(a), it is\na body–neck–head structure; the representation of input [PATCH] tokens are extracted by the ViT\nbackbone and then directly fed to the transformer-based encoding and decoding pipeline. To predict\nmultiple objects, a ﬁxed number of learnable [DET] tokens are provided as additional input to the\ndecoder. Subsequently, output embeddings by the decoder produce ﬁnal predictions through the de-\ntection heads for classiﬁcation and box regression. Since DETR (ViT) does not modify the backbone\nat all, it can be ﬂexibly changed to any latest ViT model, e.g., Swin Transformer. Additionally, its\nneck decoder facilitates the aggregation of multi-scale features and the use of additional techniques,\nwhich help detect objects of different sizes and speed up training (Zhu et al., 2021). However, the\nattention operation at the neck encoder adds signiﬁcant computational overhead to the detector.\nIn contrast, ViDT resolves this issue by directly extracting ﬁne-grained [DET] features from Swin\nTransformer with RAM without maintaining the transformer encoder in the neck architecture.\nYOLOS (Fang et al., 2021) is a canonical ViT architecture for object detection with minimal\nmodiﬁcations. As illustrated in Figure 2(b), YOLOS achieves a neck-free structure by appending\nrandomly initialized learnable [DET] tokens to the sequence of input [PATCH] tokens. Since all the\nembeddings for [PATCH] and [DET] tokens interact via global attention, the ﬁnal[DET] tokens are gen-\nerated by the ﬁne-tuned ViT backbone and then directly generate predictions through the detection\nheads without requiring any neck layer. While the naive DETR (ViT) suffers from the computational\noverhead from the neck layer, YOLOS enjoys efﬁcient computations by treating the [DET] tokens as\nadditional input for ViT. YOLOS shows that 2D object detection can be accomplished in a pure\nsequence-to-sequence manner, but this solution entails two inherent limitations:\n1) YOLOS inherits the drawback of the canonical ViT; the high computational complexity attributed\nto the global attention operation. As illustrated in Figure 1, YOLOS shows very poor latency\ncompared with other fully transformer-based detectors, especially when its model size becomes\nlarger, i.e., small →base. Thus, YOLOS is not scalablefor the large model.\n2) YOLOS cannot beneﬁt from using additional techniques essential for better performance, e.g.,\nmulti-scale features, due to the absence of the neck layer. Although YOLOS used the same DeiT\nbackbone with Deformable DETR (DeiT), its AP was lower than the straightforward integration.\nIn contrast, the encoder-free neck architecture of ViDT enjoys the additional optimization techniques\nfrom Zhu et al. (2021), resulting in the faster convergence and the better performance. Further, our\nRAM enables to combine Swin Transformer and the sequence-to-sequence paradigm for detection.\n3 V IDT: V ISION AND DETECTION TRANSFORMERS\nViDT ﬁrst reconﬁgures the attention model of Swin Transformer to support standalone object detec-\ntion while fully reusing the parameters of Swin Transformer. Next, it incorporates an encoder-free\nneck layer to exploit multi-scale features and two essential techniques: auxiliary decoding loss and\niterative box reﬁnement. We further introduce knowledge distillation with token matching to beneﬁt\nfrom large ViDT models.\n3.1 R ECONFIGURED ATTENTION MODULE\nApplying patch reduction and local attention scheme of Swin Transformer to the sequence-to-\nsequence paradigm is challenging because (1) the number of [DET] tokens must be maintained at a\nﬁxed-scale and (2) the lack of locality between[DET] tokens. To address this challenge, we introduce\na reconﬁgured attention module (RAM)2 that decomposes a single global attention associated with\n2This reconﬁguration scheme can be easily applied to other ViT variants with simple modiﬁcation.\n3\nV\n[PATCH] Tokens Feature\nMap\nFeature\nMap\nReplicate\nSpatial Positional\nEncoding\nDetection Token \nEncoding\nQ\nV [PATCH]×[PATCH] with Window Partitions\nQ V\nK\n+\n+\n× [PATCH] Tokens\n[DET]×[DET & PATCH]\nK\nQKV\nQ K\nK\n×\nV V\n× [DET] Tokens \n×Q K\nRelative Pos.\nReassemble\n[DET] Tokens \nFigure 3. Reconﬁgured Attention Module (Q: query, K: key, V: value). The skip connection and feedforward\nnetworks following the attention operation is omitted just for ease of exposition.\n[PATCH] and [DET] tokens into the three different attention, namely [PATCH]×[PATCH], [DET]×[DET],\nand [DET] ×[PATCH] attention. Based on the decomposition, the efﬁcient schemes of Swin Trans-\nformer are applied only to [PATCH] ×[PATCH] attention, which is the heaviest part in computational\ncomplexity, without breaking the two constraints on [DET] tokens. As illustrated in Figure 3, these\nmodiﬁcations fully reuse all the parameters of Swin Transformer by sharing projection layers for\n[DET] and [PATCH] tokens, and perform the three different attention operations:\n• [PATCH] ×[PATCH] Attention: The initial [PATCH] tokens are progressively calibrated across the\nattention layers such that they aggregate the key contents in the global feature map (i.e., spatial\nform of [PATCH] tokens) according to the attention weights, which are computed by ⟨query, key⟩\npairs. For [PATCH]×[PATCH] attention, Swin Transformer performs local attention on each window\npartition, but its shifted window partitioning in successive blocks bridges the windows of the\npreceding layer, providing connections among partitions to capture global information. Without\nmodifying this concept, we use the same policy to generate hierarchical[PATCH] tokens. Thus, the\nnumber of [PATCH] tokens is reduced by a factor of 4 at each stage; the resolution of feature maps\ndecreases from H/4 ×W/4 to H/32 ×W/32 over a total of four stages, where H and W denote\nthe width and height of the input image, respectively.\n• [DET] ×[DET] Attention: Like YOLOS, we append one hundred learnable [DET] tokens as the ad-\nditional input to the[PATCH] tokens. As the number of[DET] tokens speciﬁes the number of objects\nto detect, their number must be maintained with a ﬁxed-scale over the transformer layers. In ad-\ndition, [DET] tokens do not have any locality unlike the [PATCH] tokens. Hence, for [DET] ×[DET]\nattention, we perform global self-attention while maintaining the number of them; this attention\nhelps each [DET] token to localize a different object by capturing the relationship between them.\n• [DET] ×[PATCH] Attention: This is cross-attention between [DET] and [PATCH] tokens, which pro-\nduces an object embedding per [DET] token. For each [DET] token, the key contents in [PATCH]\ntokens are aggregated to represent the target object. Since the [DET] tokens specify different ob-\njects, it produces different object embeddings for diverse objects in the image. Without the cross-\nattention, it is infeasible to realize the standalone object detector. As shown in Figure 3, ViDT\nbinds [DET] ×[DET] and [DET] ×[PATCH] attention to process them at once to increase efﬁciency.\nWe replace all the attention modules in Swin Transformer with the proposed RAM, which receives\n[PATCH] and [DET] tokens (as shown in “Body” of Figure 2(c)) and then outputs their calibrated new\ntokens by performing the three different attention operations in parallel.\nPositional Encoding.ViDT adopts different positional encodings for different types of attention. For\n[PATCH]×[PATCH] attention, we use the relative position bias (Hu et al., 2019) originally used in Swin\nTransformer. In contrast, the learnable positional encoding is added for[DET] tokens for [DET]×[DET]\nattention because there is no particular order between [DET] tokens. However, for [DET] ×[PATCH]\nattention, it is crucial to inject spatial biasto the [PATCH] tokens due to the permutation-equivariant\nin transformers, ignoring spatial information of the feature map. Thus, ViDT adds the sinusoidal-\nbased spatial positional encoding to the feature map, which is reconstructed from the[PATCH] tokens\nfor [DET] ×[PATCH] attention, as can be seen from the left side of Figure 3. We present a thorough\nanalysis of various spatial positional encodings in Section 4.2.1.\nUse of [DET] × [PATCH] Attention. Applying cross-attention between [DET] and [PATCH] tokens\nadds additional computational overhead to Swin Transformer, especially when it is activated at the\nbottom layer due to the large number of [PATCH] tokens. To minimize such computational overhead,\nViDT only activates the cross-attention at the last stage (the top level of the pyramid) of Swin Trans-\n4\nformer, which consists of two transformer layers that receives[PATCH] tokens of size H/32 ×W/32.\nThus, only self-attention for [DET] and [PATCH] tokens are performed for the remaining stages ex-\ncept the last one. In Section 4.2.2 we show that this design choice helps achieve the highest FPS,\nwhile achieving similar detection performance as when cross-attention is enabled at every stage. We\nprovide details on RAM including its complexity analysis and algorithmic design in Appendix A.\n3.2 E NCODER -FREE NECK STRUCTURE\nTo exploit multi-scale feature maps, ViDT incorporates a decoder of multi-layer deformable trans-\nformers (Zhu et al., 2021). In the DETR family (Figure 2(a)), a transformer encoder is required at\nthe neck to transform features extracted from the backbone for image classiﬁcation into the ones\nsuitable for object detection; the encoder is generally computationally expensive since it involves\n[PATCH] ×[PATCH] attention. However, ViDT maintains only a transformer decoder as its neck, in\nthat Swin Transformer with RAM directly extracts ﬁne-grained features suitable for object detection\nas a standalone object detector. Thus, the neck structure of ViDT is computationally efﬁcient.\nThe decoder receives two inputs from Swin Transformer with RAM: (1) [PATCH] tokens generated\nfrom each stage (i.e., four multi-scale feature maps,{xl}L\nl=1 where L= 4) and (2) [DET] tokens gen-\nerated from the last stage. The overview is illustrated in “Neck” of Figure 2(c). In each deformable\ntransformer layer, [DET] ×[DET] attention is performed ﬁrst. For each [DET] token, multi-scale de-\nformable attention is applied to produce a new [DET] token, aggregating a small set of key contents\nsampled from the multi-scale feature maps {xl}L\nl=1,\nMSDeformAttn([DET],{xl}L\nl=1) =\nM∑\nm=1\nWm\n[ L∑\nl=1\nK∑\nk=1\nAmlk ·W′\nmxl(\nφl(p) + ∆pmlk\n)]\n, (1)\nwhere mindices the attention head and K is the total number of sampled keys for content aggrega-\ntion. In addition, φl(p) is the reference point of the [DET] token re-scaled for the l-th level feature\nmap, while ∆pmlk is the sampling offset for deformable attention; andAmlk is the attention weights\nof the Ksampled contents. Wm and W′\nm are the projection matrices for multi-head attention.\nAuxiliary Techniques for Additional Improvements.The decoder of ViDT follows the standard\nstructure of multi-layer transformers, generating reﬁned [DET] tokens at each layer. Hence, ViDT\nleverages the two auxiliary techniques used in (Deformable) DETR for additional improvements:\n• Auxiliary Decoding Loss: Detection heads consisting of two feedforward networks (FNNs) for\nbox regression and classiﬁcation are attached to every decoding layer. All the training losses from\ndetection heads at different scales are added to train the model. This helps the model output the\ncorrect number of objects without non-maximum suppression (Carion et al., 2020).\n• Iterative Box Reﬁnement: Each decoding layer reﬁnes the bounding boxes based on predictions\nfrom the detection head in the previous layer. Therefore, the box regression process progressively\nimproves through the decoding layers (Zhu et al., 2021).\nThese two techniques are essential for transformer-based object detectors because they signiﬁcantly\nenhance detection performance without compromising detection efﬁciency. We provide an ablation\nstudy of their effectiveness for object detection in Section 4.3.1.\n3.3 K NOWLEDGE DISTILLATION WITH TOKEN MATCHING FOR OBJECT DETECTION\nWhile a large model has a high capacity to achieve high performance, it can be computationally ex-\npensive for practical use. As such, we additionally present a simple knowledge distillation approach\nthat can transfer knowledge from the large ViDT model by token matching. Based on the fact that\nall ViDT models has exactly the same number of[PATCH] and [DET] tokens regardless of their scale,\na small ViDT model (a student model) can easily beneﬁt from a pre-trained large ViDT (a teacher\nmodel) by matching its tokens with those of the large one, thereby bringing out higher detection\nperformance at a lower computational cost.\nMatching all the tokens at every layer is very inefﬁcient in training. Thus, we only match the to-\nkens contributing the most to prediction. The two sets of tokens are directly related: (1) P: the set\nof [PATCH] tokens used as multi-scale feature maps, which are generated from each stage in the\nbody, and (2)D: the set of [DET] tokens, which are generated from each decoding layer in the neck.\nAccordingly, the distillation loss based on token matching is formulated by\nℓdis(Ps,Ds,Pt,Dt) = λdis\n( 1\n|Ps|\n|Ps|∑\ni=1\nPs[i] −Pt[i]\n\n2\n+ 1\n|Ds|\n|Ds|∑\ni=1\nDs[i] −Dt[i]\n\n2\n)\n, (2)\n5\nBackbone Type (Size) Train Data Epochs Resolution Params ImageNet Acc.\nDeiT\nDeiT-tiny (C) ImageNet-1K 300 224 6M 74.5\nDeiT-small (C) ImageNet-1K 300 224 22M 81.2\nDeiT-base (C) ImageNet-1K 300 384 87M 85.2\nSwin\nTransformer\nSwin-nano ImageNet-1K 300 224 6M 74.9\nSwin-tiny ImageNet-1K 300 224 28M 81.2\nSwin-small ImageNet-1K 300 224 50M 83.2\nSwin-base ImageNet-22K 90 224 88M 86.3\nTable 1. Summary on the ViT backbone. “C” is the distillation strategy for classiﬁcation (Touvron et al., 2021).\nwhere the subscripts s and t refer to the student and teacher model. P[i] and D[i] return the i-th\n[PATCH] and [DET] tokens, n-dimensional vectors, belonging to Pand D, respectively. λdis is the\ncoefﬁcient to determine the strength of ℓdis, which is added to the detection loss if activated.\n4 E VALUATION\nIn this section, we show that ViDT achieves the best trade-off between accuracy and speed (Section\n4.1). Then, we conduct detailed ablation study of the reconﬁgured attention module (Section 4.2) and\nadditional techniques to boost detection performance (Section 4.3). Finally, we provide a complete\nanalysis of all components available for ViDT (Section 4.4).\nDataset. We carry out object detection experiments on the Microsoft COCO 2017 benchmark\ndataset (Lin et al., 2014). All the fully transformer-based object detectors are trained on 118K train-\ning images and tested on 5K validation images following the literature (Carion et al., 2020).\nAlgorithms. We compare ViDT with two existing fully transformer-based object detection pipelines,\nnamely DETR (ViT) and YOLOS. Since DETR (ViT) follows the general pipeline of (Deformable)\nDETR by replacing its ResNet backbone with other ViT variants; hence, we use one canonical ViT\nand one latest ViT variant, DeiT and Swin Transformer, as its backbone without any modiﬁcation. In\ncontrast, YOLOS is the canonical ViT architecture, thus only DeiT is available. Table 1 summarizes\nall the ViT models pre-trained on ImageNet used for evaluation. Note that publicly available pre-\ntrained models are used except for Swin-nano. We newly conﬁgure Swin-nano3 comparable to DeiT-\ntiny, which is trained on ImageNet with the identical setting. Overall, with respect to the number of\nparameters, Deit-tiny, -small, and -base are comparable to Swin-nano, -tiny, and -base, respectively.\nPlease see Appendix B.2 for the detailed pipeline of compared detectors.\nImplementation Details. All the algorithms are implemented using PyTorch and executed using\neight NVIDIA Tesla V100 GPUs. We train ViDT using AdamW (Loshchilov & Hutter, 2019) with\nthe same initial learning rate of 10−4 for its body, neck and head. In contrast, following the (De-\nformable) DETR setting, DETR (ViT) is trained with the initial learning rate of 10−5 for its pre-\ntrained body (ViT backbone) and 10−4 for its neck and head. YOLOS and ViDT (w.o. Neck) are\ntrained with the same initial learning rate of5×10−5, which is the original setting of YOLOS for the\nneck-free detector. We do not change any hyperparameters used in transformer encoder and decoder\nfor (Deformable) DETR; thus, the neck decoder of ViDT also consists of six deformable transformer\nlayers using exactly the same hyperparameters. The only new hyperparameter introduced, the dis-\ntillation coefﬁcient λdis in Eq. (2), is set to be 4. For fair comparison, knowledge distillation is not\napplied for ViDT in the main experiment in Section 4.1. The efﬁcacy of knowledge distillation with\ntoken matching is veriﬁed independently in Section 4.3.2. Auxiliary decoding loss and iterative box\nreﬁnement are applied to the compared methods if applicable.\nRegarding the resolution of input images, we use scale augmentation that resizes them such that the\nshortest side is at least 480 and at most 800 pixels while the longest at most 1333 (Wu et al., 2019).\nMore details of the experiment conﬁguration can be found in Appendix B.3–B.5. All the source code\nand trained models will be made available to the public.\n4.1 M AIN EXPERIMENTS WITH MICROSOFT COCO B ENCHMARK\nTable 2 compares ViDT with DETR (ViT) and YOLOS w.r.t their AP, FPS,# parameters, where the\ntwo variants of DETR (ViT) are simply named DETR and Deformable DETR. We report the result\nof ViDT without using knowledge distillation for fair comparison. A summary plot is provided in\nFigure 1. The experimental comparisons with CNN backbones are provided in Appendix C.1.\n3Swin-nano is designed such that its channel dimension is half that of Swin-tiny. Please see Appendix B.1.\n6\nMethod Backbone Epochs AP AP50 AP75 APS APM APL Param. FPS\nDETR\nDeiT-tiny 50 30.0 49.2 30.5 9.9 30.8 50.6 24M 10.9 (13.1)\nDeiT-small 50 32.4 52.5 33.2 11.3 33.5 53.7 39M 7.8 (8.8)\nDeiT-base 50 37.1 59.2 38.4 14.7 39.4 52.9 0.1B 4.3 (4.9)\nSwin-nano 50 27.8 47.5 27.4 9.0 29.2 44.9 24M 24.7 (46.1)\nSwin-tiny 50 34.1 55.1 35.3 12.7 35.9 54.2 45M 19.3 (28.1)\nSwin-small 50 37.6 59.0 39.0 15.9 40.1 58.9 66M 13.5 (17.7)\nSwin-base 50 40.7 62.9 42.7 18.3 44.1 62.4 0.1B 9.7 (12.6)\nDeformable\nDETR\nDeiT-tiny 50 40.8 60.1 43.6 21.4 43.4 58.2 18M 12.4 (16.3)\nDeiT-small 50 43.6 63.7 46.5 23.3 47.1 62.1 35M 8.5 (10.2)\nDeiT-base 50 46.4 67.3 49.4 26.7 50.1 65.4 0.1B 4.4 (5.3)\nSwin-nano 50 43.1 61.4 46.3 25.9 45.2 59.4 18M 7.0 (7.8)\nSwin-tiny 50 47.0 66.8 50.8 28.1 49.8 63.9 39M 6.3 (7.0)\nSwin-small 50 49.0 68.9 52.9 30.3 52.8 66.6 60M 5.5 (6.1)\nSwin-base 50 51.4 71.7 56.2 34.5 55.1 67.5 0.1B 4.8 (5.4)\nYOLOS\nDeiT-tiny 150 30.4 48.6 31.1 12.4 31.8 48.2 6M 28.1 (31.3)\nDeiT-small 150 36.1 55.7 37.6 15.6 38.4 55.3 30M 9.3 (11.8)\nDeiT-base 150 42.0 62.2 44.5 19.5 45.3 62.1 0.1B 3.9 (5.4)\nViDT\n(w.o. Neck)\nSwin-nano 150 28.7 48.6 28.5 12.3 30.7 44.1 7M 36.5 (64.4)\nSwin-tiny 150 36.3 56.3 37.8 16.4 39.0 54.3 29M 28.6 (32.1)\nSwin-small 150 41.6 62.7 43.9 20.1 45.4 59.8 52M 16.8 (18.8)\nSwin-base 150 43.2 64.2 45.9 21.9 46.9 63.2 91M 11.5 (12.5)\nViDT\nSwin-nano 50 40.4 59.6 43.3 23.2 42.5 55.8 16M 20.0 (45.8)\nSwin-tiny 50 44.8 64.5 48.7 25.9 47.6 62.1 38M 17.2 (26.5)\nSwin-small 50 47.5 67.7 51.4 29.2 50.7 64.8 61M 12.1 (16.5)\nSwin-base 50 49.2 69.4 53.1 30.6 52.6 66.9 0.1B 9.0 (11.6)\nTable 2. Comparison of ViDT with other compared detectors on COCO2017 val set. Two neck-free detectors,\nYOLOS and ViDT (w.o. Neck) are trained for 150 epochs due to the slow convergence. FPS is measured with\nbatch size 1 of 800 ×1333 resolution on a single Tesla V100 GPU, where the value inside the parentheses is\nmeasured with batch size 4 of the same resolution to maximize GPU utilization.\nHighlights. ViDT achieves the best trade-off between AP and FPS. With its high scalability, it per-\nforms well even for Swin-base of 0.1 billion parameters, which is 2x faster than Deformable DETR\nwith similar AP. Besides, ViDT shows 40.4AP only with 16M parameters; it is 6.3–12.6AP higher\nthan those of DETR (swin-nano) and DETR (swin-tiny), which exhibit similar FPS of19.3–24.7.\nViDT vs. Deformable DETR.Thanks to the use of multi-scale features, Deformable DETR exhibits\nhigh detection performance in general. Nevertheless, its encoder and decoder structure in the neck\nbecomes a critical bottleneck in computation. In particular, the encoder with multi-layer deformable\ntransformers adds considerable overhead to transform multi-scale features by attention. Thus, it\nshows very low FPS although it achieves higher AP with a relatively small number of parameters. In\ncontrast, ViDT removes the need for a transformer encoder in the neck by using Swin Transformer\nwith RAM as its body, directly extracting multi-scale features suitable for object detection.\nViDT (w.o. Neck) vs. YOLOS.For the comparison with YOLOS, we train ViDT without using\nits neck component. These two neck-free detectors show relatively low AP compared with other\ndetectors in general. In terms of speed, YOLOS exhibits much lower FPS than ViDT (w.o. Neck)\nbecause of its quadratic computational complexity for attention. However, ViDT (w.o. Neck) extends\nSwin Transformers with RAM, thus requiring linear complexity for attention. Hence, it shows AP\ncomparable to YOLOS for various backbone size, but its FPS is much higher.\nOne might argue that better integration could be also achieved by (1) Deformable DETR without\nits neck encoder because its neck decoder also has [DET] ×[PATCH] cross-attention, or (2) YOLOS\nwith VIDT’s neck decoder because of the use of multiple auxiliary techniques. Such integration is\nactually not effective; the former signiﬁcantly drops AP, while the latter has a much greater drop in\nFPS than an increase in AP. The detailed analysis can be found in Appendix C.2.\n4.2 A BLATION STUDY ON RECONFIGURED ATTENTION MODULE (RAM)\nWe extend Swin Transformer with RAM to extract ﬁne-grained features for object detection without\nmaintaining an additional transformer encoder in the neck. We provide an ablation study on the two\nmain considerations for RAM, which leads to high accuracy and speed. To reduce the inﬂuence of\nsecondary factors, we mainly use our neck-free version, ViDT (w.o. Neck), for the ablation study.\n7\n4.2.1 S PATIAL POSITIONAL ENCODING\nMethod None Pre-addition Post-addition\nType None Sin. Learn. Sin. Learn.\nAP 23.7 28.7 27.4 28.0 24.1\nTable 3. Results for different spatial encodings for\n[DET] ×[PATCH] cross-attention.\nSpatial positional encoding is essential for [DET] ×\n[PATCH] attention in RAM. Typically, the spatial\nencoding can be added to the [PATCH] tokens be-\nfore or after the projection layer in Figure 3. We\ncall the former “pre-addition” and the latter “post-\naddition”. For each one, we can design the encoding\nin a sinusoidal or learnable manner (Carion et al.,\n2020). Table 3 contrasts the results with different spatial positional encodings with ViDT (w.o.\nNeck). Overall, pre-addition results in performance improvement higher than post-addition, and\nspeciﬁcally, the sinusoidal encoding is better than the learnable one; thus, the 2D inductive bias of\nthe sinusoidal spatial encoding is more helpful in object detection. In particular, pre-addition with\nthe sinusoidal encoding increases AP by 5.0 compared to not using any encoding.\n4.2.2 S ELECTIVE [DET] ×[PATCH] CROSS -ATTENTION\nThe addition of cross-attention to Swin Transformer inevitably entails computational overhead, par-\nticularly when the number of [PATCH] is large. To alleviate such overhead, we selectively enable\ncross-attention in RAM at the last stage of Swin Transformer; this is shown to greatly improve FPS,\nbut barely drop AP. Table 4 summarizes AP and FPS when used different selective strategies for the\ncross-attention, where Swin Transformer consists of four stages in total. It is interesting that all the\nstrategies exhibit similar AP as long as cross-attention is activated at the last stage. Since features are\nextracted in a bottom-up manner as they go through the stages, it seems difﬁcult to directly obtain\nuseful information about the target object at the low level of stages. Thus, only using the last stage\nis the best design choice in terms of high AP and FPS due to the smallest number of[PATCH] tokens.\nMeanwhile, the detection fails completely or the performance signiﬁcantly drops if all the stages are\nnot involved due to the lack of interaction between [DET] and [PATCH] tokens that spatial positional\nencoding is associated with. A more detailed analysis of the [DET] ×[PATCH] cross-attention and\n[DET] ×[DET] self-attention is provided in appendices C.3 and C.4.\nStage Ids {1, 2, 3, 4} {2, 3, 4} {3, 4} {4} {}\nMetric AP FPS AP FPS AP FPS AP FPS AP FPS\nw.o. Neck 29.0 21.8 28.8 29.1 28.5 34.3 28.7 36.5 FAIL 37.7\nw. Neck 40.3 14.6 40.1 18.0 40.3 19.5 40.4 20.0 37.1 20.5\nTable 4. AP and FPS comparison with different selective cross-attention strategies.\n4.3 A BLATION STUDY ON ADDITIONAL TECHNIQUES\nWe analyze the performance improvement of two additional techniques, namely auxiliary decoding\nloss and iterative box reﬁnement, and the proposed distillation approach in Section 3.3. Further-\nmore, we introduce a simple technique that can expedite the inference speed of ViDT by dropping\nunnecessary decoding layers at inference time.\n4.3.1 A UXILIARY DECODING LOSS AND ITERATIVE BOX REFINEMENT\nAux. ℓ Box Ref. Neck AP ∆\nYOLOS\n30.4\n✓ 29.2 −1.2\n✓ ✓ 20.1 −10.3\nViDT\n28.7\n✓ 27.2 −1.6\n✓ ✓ 22.9 −5.9\n✓ ✓ 36.2 +7.4\n✓ ✓ ✓ 40.4 +11.6\nTable 5. Effect of extra techniques with\nYOLOS (DeiT-tiny) and ViDT (Swin-nano).\nTo thoroughly verify the efﬁcacy of auxiliary decod-\ning loss and iterative box reﬁnement, we extend them\neven for the neck-free detector like YOLOS; the\nprinciple of them is applied to the encoding layers in\nthe body, as opposed to the conventional way of using\nthe decoding layers in the neck. Table 5 shows the\nperformance of the two neck-free detectors, YOLOS\nand ViDT (w.o. Neck), decreases considerably with\nthe two techniques. The use of them in the encoding\nlayers is likely to negatively affect feature extraction\nof the transformer encoder. In contrast, an opposite\ntrend is observed with the neck component. Since\nthe neck decoder is decoupled with the feature extraction in the body, the two techniques make a\nsynergistic effect and thus show signiﬁcant improvement in AP. These results justify the use of the\nneck decoder in ViDT to boost object detection performance.\n8\n4.3.2 K NOWLEDGE DISTILLATION WITH TOKEN MATCHING\nStudent ViDT (Swin-nano) ViDT (Swin-tiny)\nTeacher ViDT\n(small)\nViDT\n(base)\nViDT\n(small)\nViDT\n(base)\nλdis = 0 40.4 44.8\nλdis = 2 41.4 41.4 45.6 46.1\nλdis = 4 41.5 41.9 45.8 46.5\nTable 6. AP comparison of student models associated\nwith different teacher models.\nWe show that a small ViDT model can beneﬁt\nfrom a large ViDT model via knowledge distil-\nlation. The proposed token matching is a new\nconcept of knowledge distillation for object de-\ntection, especially for a fully transformer-based\nobject detector. Compared to very complex dis-\ntillation methods that rely on heuristic rules with\nmultiple hyperparameters (Chen et al., 2017; Dai\net al., 2021), it simply matches some tokens with\na single hyperparameter, the distillation coefﬁcient λdis. Table 6 summarizes the AP improvement\nvia knowledge distillation with token matching with varying distillation coefﬁcients. Overall, the\nlarger the size of the teacher model, the greater gain to the student model. Regarding coefﬁcients,\nin general, larger values achieve better performance. Distillation increases AP by 1.0–1.7 without\naffecting the inference speed of the student model.\n4.3.3 D ECODING LAYER DROP\nModel ViDT (Swin-nano) ViDT (Swin-tiny)\nMetric AP Param. FPS AP Param. FPS\n0 Drop 40.4 16M 20.0 44.8 38M 17.2\n1 Drop 40.2 14M 20.9 44.8 37M 18.5\n2 Drop 40.0 13M 22.3 44.5 35M 19.6\n3 Drop 38.6 12M 24.7 43.6 34M 21.0\n4 Drop 36.8 11M 26.0 41.9 33M 22.4\n5 Drop 32.5 10M 28.7 38.0 32M 24.4\nTable 7. Performance trade-off by decoding layer\ndrop regarding AP, Param, and FPS.\nViDT has six layers of transformers as its neck\ndecoder. We emphasize that not all layers of the\ndecoder are required at inference time for high\nperformance. Table 7 show the performance of\nViDT when dropping its decoding layer one by\none from the top in the inference step. Although\nthere is a trade-off relationship between accuracy\nand speed as the layers are detached from the\nmodel, there is no signiﬁcant AP drop even when\nthe two layers are removed. This technique is not\ndesigned for performance evaluation in Table 2\nwith other methods, but we can accelerate the inference speed of a trained ViDT model to over10%\nby dropping its two decoding layers without a much decrease in AP.\n4.4 C OMPLETE COMPONENT ANALYSIS\nIn this section, we combine all the proposed components (even with distillation and decoding layer\ndrop) to achieve high accuracy and speed for object detection. As summarized in Table 8, there are\nfour components: (1) RAM to extend Swin Transformer as a standalone object detector, (2) the neck\ndecoder to exploit multi-scale features with two auxiliary techniques, (3) knowledge distillation to\nbeneﬁt from a large model, and (4) decoding layer drop to further accelerate inference speed. The\nperformance of the ﬁnal version is very outstanding; it achieves 41.7AP with reasonable FPS by only\nusing 13M parameters when used Swin-nano as its backbone. Further, it only loses 2.7 FPS while\nexhibiting 46.4AP when used Swin-tiny. This indicates that a fully transformer-based object detector\nhas the potential to be used as a generic object detector when further developed in the future.\nComponent Swin-nano Swin-tiny\n# RAM Neck Distil Drop AP AP50 AP75 Param. FPS AP AP50 AP75 Param. FPS\n(1) ✓ 28.7 48.6 28.5 7M 36.5 36.3 56.3 37.8 29M 28.6\n(2) ✓ ✓ 40.4 59.6 43.3 16M 20.0 44.8 64.5 48.7 38M 17.2\n(3) ✓ ✓ ✓ 41.9 61.1 45.0 16M 20.0 46.5 66.3 50.2 38M 17.2\n(4) ✓ ✓ ✓ ✓ 41.7 61.0 44.8 13M 22.3 46.4 66.3 50.2 35M 19.6\nTable 8. Detailed component analysis with Swin-nano and Swin-tiny.\n5 C ONCLUSION\nWe have explored the integration of vision and detection transformers to build an effective and\nefﬁcient object detector. The proposed ViDT signiﬁcantly improves the scalability and ﬂexibility of\ntransformer models to achieve high accuracy and inference speed. The computational complexity\nof its attention modules is linear w.r.t. image size, and ViDT synergizes several essential techniques\nto boost the detection performance. On the Microsoft COCO benchmark, ViDT achieves 49.2AP\nwith a large Swin-base backbone, and41.7AP with the smallest Swin-nano backbone and only 13M\nparameters, suggesting the beneﬁts of using transformers for complex computer vision tasks.\n9\nREFERENCES\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pp. 213–229, 2020.\n1, 3, 5, 6, 8, 14, 15\nChun-Fu Chen, Quanfu Fan, and Rameswar Panda. CrossViT: Cross-attention multi-scale vision\ntransformer for image classiﬁcation. In ICCV, 2021. 13\nGuobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning efﬁcient\nobject detection models with knowledge distillation. In NeurIPS, 2017. 9\nXing Dai, Zeren Jiang, Zhao Wu, Yiping Bao, Zhicheng Wang, Si Liu, and Erjin Zhou. General\ninstance distillation for object detection. In CVPR, pp. 7842–7851, 2021. 9\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image\nis worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1, 2\nYuxin Fang, Bencheng Liao, Xinggang Wang, Jiemin Fang, Jiyang Qi, Rui Wu, Jianwei Niu, and\nWenyu Liu. You only look at one sequence: Rethinking transformer in vision through object\ndetection. arXiv preprint arXiv:2106.00666, 2021. 2, 3, 14, 15\nByeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and Seong Joon Oh.\nRethinking spatial dimensions of vision transformers. In ICCV, 2021. 2, 13\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nIn CVPR, pp. 3464–3473, 2019. 4\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll´ar, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, pp.\n740–755, 2014. 2, 6\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll´ar. Focal loss for dense object\ndetection. In CVPR, pp. 2980–2988, 2017. 14\nLi Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietik¨ainen.\nDeep learning for generic object detection: A survey. International Journal of Computer Vision,\n128(2):261–318, 2020. 1\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin Transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 2, 12\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6, 15\nConstantine Papageorgiou and Tomaso Poggio. A trainable system for object detection. Interna-\ntional Journal of Computer Vision, 38(1):15–33, 2000. 1\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object\ndetection with region proposal networks. NeurIPS, 28:91–99, 2015. 3\nHamid Rezatoﬁghi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese.\nGeneralized intersection over union: A metric and a loss for bounding box regression. In CVPR,\npp. 658–666, 2019. 14\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerv´e J ´egou. Training data-efﬁcient image transformers & distillation through attention. In\nICML, pp. 10347–10357, 2021. 6\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998–6008,\n2017. 1, 12, 17\n10\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. In ICCV, 2021. 13\nYuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2.\nhttps://github.com/facebookresearch/detectron2, 2019. 3, 6\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR:\nDeformable transformers for end-to-end object detection. In ICLR, 2021. 3, 5, 14, 15\n11\nAn Efﬁcient and Effective Fully Transformer-based Ob-\nject Detector (Supplementary Material)\nA R ECONFIGURED ATTENTION MODULE\nThe proposed RAM in Figure 3 performs three attention operations, namely [PATCH] ×[PATCH],\n[DET] ×[PATCH], and [DET] ×[DET] attention. This section provides (1) computational complexity\nanalysis and (2) further algorithmic design for [DET] tokens.\nA.1 C OMPUTATIONAL COMPLEXITY ANALYSIS\nWe analyze the computational complexity of the proposed RAM compared with the attention used\nin YOLOS. The analysis is based on the computational complexity of basic building blocks for\nCanonical and Swin Transformer, which is summarized in Table 94, where T1 and T2 is the number\nof tokens for self- and cross-attention, and dis the embedding dimension.\nTransformer Canonical Transformer Swin Transformer\nAttention Global Self-attention Global Cross-attention Local Self-attention\nComplexity O(d2T1 + dT2\n1) O(d2(T1 + T2) +dT1T2) O(d2T1 + dk2T1)\nTable 9. Computational complexity of attention modules: In the canonical transformer, the complexity of global\nself-attention is O(d2T1 + dT2\n1), where O(d2T1) is the cost of computing the query, key, and value embed-\ndings and O(dT2\n1) is the cost of computing the attention weights. The complexity of global cross-attention is\nO(d2(T1 + T2) + dT1T2), which is the interaction between the two different tokens T1 and T2. In contrast,\nSwin Transformer achieves much lower attention complexity of O(d2T1 + dk2T1) with window partitioning,\nwhere kis the width and height of the window (k<< T1,T2).\nLet P and D be the number of [PATCH] and [DET] tokens (D <<P in practice, e.g., P = 66,650 and\nD = 100 at the ﬁrst stage of ViDT). Then, the computational complexity of the attention module for\nYOLOS and ViDT (RAM) is derived as below, also summarized in Table 10:\n• YOLOS Attention: [DET] tokens are simply appended to [PATCH] tokens to perform global self-\nattention on [PATCH,DET] tokens (i.e., T1 = P + D). Thus, the computational complexity is\nO(d2(P + D) + d(P + D)2), which is quadratic to the number of [PATCH] tokens. If breaking\ndown the total complexity, we obtain O\n(\n(d2P + dP2) + (d2D + dD2) + dPD\n)\n, where the ﬁrst\nand second terms are for the global self-attention for [PATCH] and [DET] tokens, respectively, and\nthe last term is for the global cross-attention between them.\n• ViDT (RAM) Attention: RAM performs the three different attention operations: (1) [PATCH] ×\n[PATCH] local self-attention with window partition,O(d2P+dk2P); (2) [DET]×[DET] global self-\nattention, O(d2D + dD2); (3) [DET] ×[PATCH] global cross-attention, O(d2(D + P) + dDP). In\ntotal, the computational complexity of RAM is O(d2(D + P) + dk2P + dD2 + dDP), which is\nlinear to the number of [PATCH] tokens.\nConsequently, the complexity of RAM is much lower than the attention module used in YOLOS\nsince D <<P. Note that only RAM achieves the linear complexity to the patch tokens. In addition,\none might argue that YOLOS can be efﬁcient if the cross-attention is selectively removed similar to\nRAM. Even if we remove the complexity O(dPD) for the global cross-attention, the computational\ncomplexity is O(d2(P+ D) +dP2 + dD2), which is still quadratic to the number of [PATCH] tokens.\nAttention Type YOLOS ViDT\n[PATCH] ×[PATCH] O(d2P + dP2) O(d2P + dk2P)\n[DET] ×[DET] O(d2D + dD2) O(d2D + dD2)\n[DET] ×[PATCH] O(dPD) O(d2(D + P) + dDP)\nTotal Complexity O(d2(P + D)+ d(P + D)2) O(d2(D+ P) +dk2P+ dD2 + dDP)\nTable 10. Summary of computational complexity for different attention operations used in YOLOS and ViDT\n(RAM), where P and D are the number of [PATCH] and [DET] tokens, respectively (D <<P).\n4We used the computational complexity reported in the original paper (Vaswani et al., 2017; Liu et al., 2021)\n12\nA.2 A LGORITHMIC DESIGN FOR [DET] TOKENS\nA.2.1 B INDING [DET] ×[DET] AND [DET] ×[PATCH] ATTENTION\nBinding the two attention modules is very simple in implementation. [DET] ×[DET] and [DET] ×\n[PATCH] attention is generating a new [DET] token, which aggregates relevant contents in [DET] and\n[PATCH] tokens, respectively. Since the two attention share exactly the same [DET] query embed-\nding obtained after the projection as shown in Figure 3, they can be processed at once by perform-\ning the scaled-dot product between [DET]Q and\n[\n[DET]K,[PATCH]K\n]\nembeddings, where Q, K are\nthe key and query, and [·] is the concatenation. Then, the obtained attention map is applied to the[\n[DET]V,[PATCH]V\n]\nembeddings, where V is the value and dis the embedding dimension,\n[DET]new = Softmax\n( [DET]Q\n[\n[DET]K,[PATCH]K\n]⊤\n√\nd\n)\n[\n[DET]V,[PATCH]V\n]\n. (3)\nThis approach is commonly used in the recent Transformer-based architectures, such as YOLOS.\nA.2.2 E MBEDDING DIMENSION OF [DET] TOKENS\n[DET] ×[DET] attention is performed across all the stages, and the embedding dimension of [DET]\ntokens increases gradually like [PATCH] tokens. For the [PATCH] token, its embedding dimension\nis increased by concatenating nearby [PATCH] tokens in a grid. However, this mechanism is not\napplicable for [DET] tokens since we maintain the same number of [DET] tokens for detecting a\nﬁxed number of objects in a scene. Hence, we simply repeat a [DET] token multiple times along the\nembedding dimension to increase its size. This allows [DET] tokens to reuse all the projection and\nnormalization layers in Swin Transformer without any modiﬁcation.\nB E XPERIMENTAL DETAILS\nB.1 S WIN -NANO ARCHITECTURE\nModel Channel Layer Numbers\nName Dim. S1 S2 S3 S4\nSwin-nano 48 2 2 6 2\nSwin-tiny 96 2 2 6 2\nSwin-small 128 2 2 18 2\nSwin-base 192 2 2 18 2\nTable 11. Swin Transformer Architecture.\nDue to the absence of Swin models comparable to\nDeit-tiny, we conﬁgure Swin-nano, which is a 0.25×\nmodel of Swin-tiny such that it has 6M training pa-\nrameters comparable to Deit-tiny. Table 11 summa-\nrizes the conﬁguration of Swin Transformer mod-\nels available, including the newly introduced Swin-\nnano; S1–S4 indicates the four stages in Swin Trans-\nformer. The performance of all the pre-trained Swin\nTransformer models are summarized in Table 1 in the manuscript.\nB.2 D ETECTION PIPELINES OF ALL COMPARED DETECTORS\nAll the compared fully transformer-based detectors are composed of either (1) body–neck–head or\n(2) body–head structure, as summarized in Table 12. The main difference of ViDT is the use of\nreconﬁgured attention modules (RAM) for Swin Transformer, allowing the extraction of ﬁne-grained\ndetection features directly from the input image. Thus, Swin Transformer is extended to a standalone\nobject detector called ViDT (w.o. Neck). Further, its extension to ViDT allows to use multi-scale\nfeatures and multiple essential techniques for better detection, such as auxiliary decoding loss and\niterative box reﬁnement, by only maintaining a transformer decoder at the neck. Except for the two\nneck-free detector, YOLOS and ViDT (w.o. Neck), all the pipelines maintain multiple FFNs; that is,\na single FFNs for each decoding layer at the neck for box regression and classiﬁcation.\nWe believe that our proposed RAM can be combined with even other latest efﬁcient vision trans-\nformer architectures, such as PiT (Heo et al., 2021), PVT (Wang et al., 2021) and Cross-ViT (Chen\net al., 2021). We leave this as future work.\nB.3 H YPERPARAMETERS OF NECK TRANSFORMERS\nThe transformer decoder at the neck in ViDT introduces multiple hyperparameters. We follow ex-\nactly the same setting used in Deformable DETR. Speciﬁcally, we use six layers of deformable\n13\nPipeline Body Neck Head\nMethod Name Feature Extractor Tran. Encoder Tran. Decoder Prediction\nDETR (DeiT) DeiT Transformer ⃝ ⃝ Multiple FFNs\nDETR (Swin) Swin Transformer ⃝ ⃝ Multiple FFNs\nDeformable DETR (DeiT) DeiT Transformer ⃝† ⃝† Multiple FFNs\nDeformable DETR (Swin) Swin Transformer ⃝† ⃝† Multiple FFNs\nYOLOS DeiT Transformer \u0015 \u0015 Single FFNs\nViDT (w.o. Neck) Swin Transformer+RAM \u0015 \u0015 Single FFNs\nViDT Swin Transformer+RAM \u0015 ⃝† Multiple FFNs\nTable 12. Comparison of detection pipelines for all available fully transformer-based object detectors, where †\nindicates that multi-scale deformable attention is used for neck transformers.\ntransformers with width 256; thus, the channel dimension of the [PATCH] and [DET] tokens extracted\nfrom Swin Transformer are reduced to 256 to be utilized as compact inputs to the decoder trans-\nformer. For each transformer layer, multi-head attention with eight heads is applied, followed by\nthe point-wise FFNs of 1024 hidden units. Furthermore, an additive dropout of 0.1 is applied before\nthe layer normalization. All the weights in the decoder are initialized with Xavier initialization. For\n(Deformable) DETR, the tranformer decoder receives a ﬁxed number of learnable detection tokens.\nWe set the number of detection tokens to100, which is the same number used for YOLOS and ViDT.\nB.4 I MPLEMENTATION\nB.4.1 D ETECTION HEAD FOR PREDICTION\nThe last [DET] tokens produced by the body or neck are fed to a 3-layer FFNs for bounding box\nregression and linear projection for classiﬁcation,\nˆB = FFN3-layer\n(\n[DET]\n)\nand ˆP = Linear\n(\n[DET]\n)\n. (4)\nFor box regression, the FFNs produce the bounding box coordinates fordobjects, ˆB ∈[0,1]d×4, that\nencodes the normalized box center coordinates along with its width and height. For classiﬁcation, the\nlinear projection uses a softmax function to produce the classiﬁcation probabilities for all possible\nclasses including the background class, ˆP ∈[0,1]d×(c+1), where cis the number of object classes.\nWhen deformable attention is used on the neck in Table 12, only cclasses are considered without\nthe background class for classiﬁcation. This is the original setting used in DETR, YOLOS (Carion\net al., 2020; Fang et al., 2021) and Deformable DETR (Zhu et al., 2021).\nB.4.2 L OSS FUNCTION FOR TRAINING\nAll the methods adopts the loss function of (Deformable) DETR. Since the detection head return a\nﬁxed-size set of dbounding boxes, where dis usually larger than the number of actual objects in an\nimage, Hungarian matching is used to ﬁnd a bipartite matching between the predicted box ˆB and\nthe ground-truth box B. In total, there are three types of training loss: a classiﬁcation lossℓcl5, a box\ndistance ℓl1 , and a GIoU loss ℓiou(Rezatoﬁghi et al., 2019),\nℓcl(i) = −log ˆPσ(i),ci , ℓℓ1 (i) = ||Bi −ˆBσ(i)||1, and\nℓiou(i) = 1−\n( |Bi ∩ˆBσ(i)|\n|Bi ∪ˆBσ(i)|\n−|B(Bi, ˆBσ(i))\\Bi ∪ˆBσ(i)|\n|B(Bi, ˆBσ(i))|\n)\n,\n(5)\nwhere ci and σ(i) are the target class label and bipartite assignment of the i-th ground-truth box,\nand B returns the largest box containing two given boxes. Thus, the ﬁnal loss of object detection is\na linear combination of the three types of training loss,\nℓ= λclℓcl + λℓ1 ℓl1 + λiouℓiou. (6)\n5Cross-entropy loss is used with standard transformer architectures, while focal loss (Lin et al., 2017) is\nused with deformable transformer architecture.\n14\nMethod Backbone Epochs AP AP50 AP75 APS APM APL Param. FPS\nDETR ResNet-50 500 42.0 62.4 44.2 20.5 45.8 61.1 41M 22.8 (38.6)\nDETR-DC5 ResNet-50 500 43.3 63.1 45.9 22.5 47.3 61.1 41M 12.8 (14.2)\nDETR-DC5 ResNet-50 50 35.3 55.7 36.8 15.2 37.5 53.6 41M 14.2 (14.2)\nDeform. DETR ResNet-50 50 45.4 64.7 49.0 26.8 48.3 61.7 40M 13.7 (19.4)\nViDT Swin-tiny 50 44.8 64.5 48.7 25.9 47.6 62.1 38M 17.2 (26.5)\nViDT Swin-tiny 150 47.2 66.7 51.4 28.4 50.2 64.7 38M 17.2 (26.5)\nTable 13. Evaluations of ViDT with other detectors using CNN backbones on COCO2017 val set. FPS is\nmeasured with batch size 1 of 800 ×1333 resolution on a single Tesla V100 GPU, where the value inside the\nparentheses is measured with batch size 4 of the same resolution to maximize GPU utilization.\nMethod Backbone AP AP50 AP75 APS APM APL Param. FPS\nDeformable DETR Swin-nano 43.1 61.4 46.3 25.9 45.2 59.4 17M 7.0\n−neck encoder 34.0 52.8 35.6 18.0 36.3 48.4 14M 22.4\nYOLOS DeiT-tiny 30.4 48.6 31.1 12.4 31.8 48.2 6M 28.1\n+ neck decoder 38.1 57.1 40.2 20.1 40.2 56.0 14M 17.1\nViDT Swin-nano 40.4 59.6 43.3 23.2 42.5 55.8 16M 20.0\n+ neck encoder 46.1 64.1 49.7 28.5 48.7 61.7 19M 6.3\nTable 14. Variations of Deformable DETR, YOLOS, and ViDT with respect to their neck structure. They are\ntrained for 50 epochs with the same conﬁguration used in our main experimental results.\nThe coefﬁcient for each training loss is set to be λcl = 1, λℓ1 = 5, and λiou = 2. If we leverage\nauxiliary decoding loss, the ﬁnal loss is computed for every detection head separately and merged\nwith equal importance. Additionally, ViDT adds the distillation loss in Eq. (2) to the ﬁnal loss if the\ndistillation approach in Section 3.3 is enabled for training.\nB.5 T RAINING CONFIGURATION\nWe train ViDT for 50 epochs using AdamW (Loshchilov & Hutter, 2019) with the same initial\nlearning rate of 10−4 for its body, neck and head. The learning rate is decayed by cosine annealing\nwith batch size of16, weight decay of1×10−4, and gradient clipping of0.1. In contrast, ViDT (w.o.\nNeck) is trained for 150 epochs using AdamW with the initial learning rate of 5 ×10−5 by cosine\nannealing. The remaining conﬁguration is the same as for ViDT.\nRegarding DETR (ViT), we follow the setting of Deformable DETR. Thus, all the variants of this\npipeline are trained for 50 epochs with the initial learning rate of 10−5 for its pre-trained body (ViT\nbackbone) and 10−4 for its neck and head. Their learning rates are decayed at the 40-th epoch by a\nfactor of 0.1. Meanwhile, the results of YOLOS are borrowed from the original paper (Fang et al.,\n2021) except YOLOS (DeiT-tiny); since the result of YOLOS (DeiT-tiny) for 800 ×1333 is not\nreported in the paper, we train it by following the training conﬁguration suggested by authors.\nC S UPPLEMENTARY EVALUATION\nC.1 C OMPARISON WITH OBJECT DETECTOR USING CNN B ACKBONE\nWe compare ViDT with (Deformable) DETR using the ResNet-50 backbone, as summarized in\nTable 13, where all the results except ViDT are borrowed from (Carion et al., 2020; Zhu et al.,\n2021), and DETR-DC5 is a modiﬁcation of DETR to use a dilated convolution at the last stage\nin ResNet. For a fair comparison, we compare ViDT (Swin-tiny) with similar parameter numbers.\nIn general, ViDT shows a better trade-off between AP and FPS even compared with (Deformable)\nDETR with the ResNet-50. Speciﬁcally, ViDT achieves FPS much higher than DETR-DC5 and\nDeformable DETR with competitive AP. Particularly when training ViDT for 150 epochs, ViDT\noutperforms other compared methods using the ResNet-50 backbone in terms of both AP and FPS.\nC.2 V ARIATIONS OF EXISTING PIPELINES\nWe study more variations of existing detection methods by modifying their original pipelines in\nTable 12. Thus, we remove the neck encoder of Deformable DETR to increase its efﬁciency, while\nadding a neck decoder to YOLOS to leverage multi-scale features along with auxiliary decoding\n15\nQuery Id: 82 (Cat) Query Id: 94 (Cat) Query Id: 7 (Remote) Query Id: 53 (Remote)Input Image\n(a) Cross-attention at all stages {1,2,3,4}.\nQuery Id: 44 (Cat) Query Id: 49 (Cat) Query Id: 25 (Remote) Query Id: 3 (Remote)Input Image\n(b) Cross-attention at the last stage {4}.\nFigure 4. Visualization of the attention map for cross-attention with ViDT (Swin-nano).\nloss and iterative box reﬁnement. Note that these modiﬁed versions follow exactly the same de-\ntection pipeline with ViDT, maintaining a encoder-free neck between their body and head. Table 14\nsummarizes the performance of all the variations in terms of AP, FPS, and the number of parameters.\nDeformable DETRshows signiﬁcant improvement in FPS (+14.4) but its AP drops sharply (−9.1)\nwhen its neck encoder is removed.\nThus, it is difﬁcult to obtain ﬁne-grained object detection representation directly from the raw ViT\nbackbone without using an additional neck encoder. However, ViDT compensates for the effect of\nthe neck encoder by adding [DET] tokens into the body (backbone), thus successfully removing the\ncomputational bottleneck without compromising AP; it maintains 6.4 higher AP compared with the\nneck encoder-free Deformable DETR (the second row) while achieving similar FPS. This can be\nattributed to that RAM has a great contribution to the performance w.r.t AP and FPS, especially for\nthe trade-off between them.\nYOLOS shows a signiﬁcant gain in AP ( +7.7) while losing FPS (−11.0) when the neck decoder\nis added. Unlike Deformable DETR, its AP signiﬁcantly increases even without the neck encoder\ndue to the use of a standalone object detector as its backbone (i.e., the modiﬁed DeiT in Figure\n2(b)). However, its AP is lower than ViDT by 2.3AP. Even worse, it is not scalable for large models\nbecause of its quadratic computational cost for attention. Therefore, in the aspects of accuracy and\nspeed, ViDT maintains its dominance compared with the two carefully tuned baselines.\nFor a complete analysis, we additionally add a neck encoder to ViDT. The inference speed of ViDT\ndegrades drastically by13.7 because of the self-attention for multi-scale features at the neck encoder.\nHowever, it is interesting to see the improvement of AP by5.7 while adding only 3M parameters; it is\n3.0 higher even than Deformable DETR. This indicates that lowering the computational complexity\nof the encoder and thus increasing its utilization could be another possible direction for a fully\ntransformer-based object detector.\n16\nC.3 [DET] ×[PATCH] ATTENTION IN RAM\nIn Section 4.2.2, it turns out that the cross-attention in RAM is only necessary at the last stage of\nSwin Transformer; all the different selective strategies show similar AP as long as cross-attention is\nactivated at the last stage. Hence, we analyze the attention map obtained by the cross-attention in\nRAM. Figure 4 shows attention maps for the stages of Swin Transformer where cross-attention is\nutilized; it contrasts (a) ViDT with cross-attention at all stages and (b) ViDT with cross-attention at\nthe last stage. Regardless of the use of cross-attention at the lower stage, it is noteworthy that the\nﬁnally obtained attention map at the last stage is almost the same. In particular, the attention map at\nStage 1–3 does not properly focus the features on the target object, which is framed by the bounding\nbox. In addition, the attention weights (color intensity) at Stage 1–3 are much lower than those at\nStage 4. Since features are extracted from a low level to a high level in a bottom-up manner as they\ngo through the stages, it seems difﬁcult to directly get information about the target object with such\nlow-level features at the lower level of stages. Therefore, this analysis provides strong empirical\nevidence for the use of selective [DET] ×[PATH] cross-attention.\nC.4 [DET] ×[DET] ATTENTION IN RAM\nStage Id Swin-nano\n# 1 2 3 4 AP FPS\n(1) ✓ ✓ ✓ ✓ 40.4 20.0\n(2) ✓ ✓ ✓ 40.3 20.1\n(3) ✓ ✓ 40.4 20.2\n(4) ✓ 40.1 20.3\n(5) 39.7 20.4\nTable 15. AP and FPS comparison with different\n[DET] ×[DET] self-attention strategies with ViDT.\nAnother possible consideration for ViDT is the use\nof [DET] ×[DET] self-attention in RAM. We con-\nduct an ablation study by removing the[DET]×[DET]\nattention one by one from the bottom stage, and\nsummarize the results in Table 15. When all the\n[DET] ×[DET] self-attention are removed, (5) the AP\ndrops by 0.7, which is a meaningful performance\ndegradation. On the other hand, as long as the self-\nattention is activated at the last two stages, (1) – (3)\nall the strategies exhibit similar AP. Therefore, only\nkeeping [DET] ×[DET] self-attention at the last two stages can further increase FPS ( +0.2) without\ndegradation in AP. This observation could be used as another design choice for the AP and FPS\ntrade-off. Therefore, we believe that [DET] ×[DET] self-attention is meaningful to use in RAM.\nD P RELIMINARIES : TRANSFORMERS\nA transformer is a deep model that entirely relies on the self-attention mechanism for machine trans-\nlation (Vaswani et al., 2017). In this section, we brieﬂy revisit the standard form of the transformer.\nSingle-head Attention.The basic building block of the transformer is a self-attention module, which\ngenerates a weighted sum of the values (contents), where the weight assigned to each value is the\nattention score computed by the scaled dot-product between its query and key. Let WQ, WK, and\nWV be the learned projection matrices of the attention module, and then the output is generated by\nAttention(Z) = softmax\n( (ZWQ)(ZWK)⊤\n√\nd\n)\n(ZWV) ∈Rhw×d,\nwhere WQ,WK,WV ∈Rd×d.\n(7)\nMulti-head Attention.It is beneﬁcial to maintain multiple heads such that they repeat the linear\nprojection process k times with different learned projection matrices. Let WQi , WKi , and WVi\nbe the learned projection matrices of the i-th attention head. Then, the output is generated by the\nconcatenation of the results from all heads,\nMulti-Head(Z) = [Attention1(Z),Attention2(Z),..., Attentionk(Z)] ∈Rhw×d,\nwhere ∀i WQi ,WKi ,WVi ∈Rd×(d/k).\n(8)\nTypically, the dimension of each head is divided by the total number of heads.\nFeed-Forward Networks (FFNs).The output of the multi-head attention is fed to the point-wise\nFFNs, which performs the linear transformation for each position separately and identically to allow\nthe model focusing on the contents of different representation subspaces. Here, the residual connec-\n17\ntion and layer normalization are applied before and after the FFNs. The ﬁnal output is generated by\nH = LayerNorm(Dropout(H′) + H′′),\nwhere H′ = FFN(H′′) and H′′= LayerNorm(Dropout(Multi-Head(Z)) + Z). (9)\nMulti-Layer Transformers.The output of a previous layer is fed directly to the input of the next\nlayer. Regarding the positional encoding, the same value is added to the input of each attention\nmodule for all layers.\n18",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7248972654342651
    },
    {
      "name": "Computer science",
      "score": 0.6842856407165527
    },
    {
      "name": "Detector",
      "score": 0.6529959440231323
    },
    {
      "name": "Scalability",
      "score": 0.6331775188446045
    },
    {
      "name": "Object detection",
      "score": 0.5442682504653931
    },
    {
      "name": "Artificial intelligence",
      "score": 0.461746484041214
    },
    {
      "name": "Exploit",
      "score": 0.45584219694137573
    },
    {
      "name": "Architecture",
      "score": 0.4163973033428192
    },
    {
      "name": "Computer engineering",
      "score": 0.3417341709136963
    },
    {
      "name": "Computer vision",
      "score": 0.33787721395492554
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.28134775161743164
    },
    {
      "name": "Engineering",
      "score": 0.1895902454853058
    },
    {
      "name": "Electrical engineering",
      "score": 0.14208349585533142
    },
    {
      "name": "Database",
      "score": 0.09220647811889648
    },
    {
      "name": "Telecommunications",
      "score": 0.07498016953468323
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 46
}