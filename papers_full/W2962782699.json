{
    "title": "Improving Neural Language Models by Segmenting, Attending, and Predicting the Future",
    "url": "https://openalex.org/W2962782699",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2140066969",
            "name": "Hongyin Luo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096616648",
            "name": "Jiang Lan",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2279044859",
            "name": "Yonatan Belinkov",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2154846939",
            "name": "James Glass",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2798569372",
        "https://openalex.org/W2952436057",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2267186426",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2549416390",
        "https://openalex.org/W2792376130",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2963631907",
        "https://openalex.org/W2810075754",
        "https://openalex.org/W2086161653",
        "https://openalex.org/W2804845563",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963537482",
        "https://openalex.org/W182831726",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2963077125",
        "https://openalex.org/W4300427683",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2795285343",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W4289373464",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963754491",
        "https://openalex.org/W4297788867",
        "https://openalex.org/W2962746461",
        "https://openalex.org/W2963411763",
        "https://openalex.org/W4300687381",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2906625520",
        "https://openalex.org/W2962964385",
        "https://openalex.org/W2963266340",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2212703438",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W4294555862",
        "https://openalex.org/W2963983719",
        "https://openalex.org/W2951104886",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2792764867",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2963748792"
    ],
    "abstract": "Common language models typically predict the next word given the context. In this work, we propose a method that improves language modeling by learning to align the given context and the following phrase. The model does not require any linguistic annotation of phrase segmentation. Instead, we define syntactic heights and phrase segmentation rules, enabling the model to automatically induce phrases, recognize their task-specific heads, and generate phrase embeddings in an unsupervised learning manner. Our method can easily be applied to language models with different network architectures since an independent module is used for phrase induction and context-phrase alignment, and no change is required in the underlying language modeling network. Experiments have shown that our model outperformed several strong baseline models on different data sets. We achieved a new state-of-the-art performance of 17.4 perplexity on the Wikitext-103 dataset. Additionally, visualizing the outputs of the phrase induction module showed that our model is able to learn approximate phrase-level structural knowledge without any annotation.",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1483–1493\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n1483\nImproving Neural Language Models by\nSegmenting, Attending, and Predicting the Future\nHongyin Luo1 Lan Jiang2 Yonatan Belinkov1 James Glass1\n1MIT Computer Science and Artiﬁcial Intelligence Laboratory, Cambridge, MA 02139, USA\n{hyluo, belinkov, glass}@mit.edu\n2School of Information Sciences, University of Illinois at Urbana–Champaign\nChampaign, IL 61820, USA\nlanj3@illinois.edu\nAbstract\nCommon language models typically predict\nthe next word given the context. In this work,\nwe propose a method that improves language\nmodeling by learning to align the given con-\ntext and the following phrase. The model\ndoes not require any linguistic annotation of\nphrase segmentation. Instead, we deﬁne syn-\ntactic heights and phrase segmentation rules,\nenabling the model to automatically induce\nphrases, recognize their task-speciﬁc heads,\nand generate phrase embeddings in an unsu-\npervised learning manner. Our method can\neasily be applied to language models with dif-\nferent network architectures since an indepen-\ndent module is used for phrase induction and\ncontext-phrase alignment, and no change is re-\nquired in the underlying language modeling\nnetwork. Experiments have shown that our\nmodel outperformed several strong baseline\nmodels on different data sets. We achieved a\nnew state-of-the-art performance of 17.4 per-\nplexity on the Wikitext-103 dataset. Addi-\ntionally, visualizing the outputs of the phrase\ninduction module showed that our model is\nable to learn approximate phrase-level struc-\ntural knowledge without any annotation.\n1 Introduction\nNeural language models are typically trained by\npredicting the next word given a past context (Ben-\ngio et al., 2003). However, natural sentences are\nnot constructed as simple linear word sequences,\nas they usually contain complex syntactic infor-\nmation. For example, a subsequence of words\ncan constitute a phrase, and two non-neighboring\nwords can depend on each other. These properties\nmake natural sentences more complex than simple\nlinear sequences.\nMost recent work on neural language modeling\nlearns a model by encoding contexts and match-\ning the context embeddings to the embedding of\nthe next word (Bengio et al., 2003; Merity et al.,\n2017; Melis et al., 2017). In this line of work, a\ngiven context is encoded with a neural network,\nfor example a long short-term memory (LSTM;\nHochreiter and Schmidhuber, 1997) network, and\nis represented with a distributed vector. The log-\nlikelihood of predicting a word is computed by\ncalculating the inner product between the word\nembedding and the context embedding. Although\nmost models do not explicitly consider syntax,\nthey still achieve state-of-the-art performance on\ndifferent corpora. Efforts have also been made\nto utilize structural information to learn better\nlanguage models. For instance, parsing-reading-\npredict networks (PRPN; Shen et al., 2017) ex-\nplicitly learn a constituent parsing structure of a\nsentence and predict the next word considering the\ninternal structure of the given context with an at-\ntention mechanism. Experiments have shown that\nthe model is able to capture some syntactic infor-\nmation.\nSimilar to word representation learning models\nthat learns to match word-to-word relation matri-\nces (Mikolov et al., 2013; Pennington et al., 2014),\nstandard language models are trained to factor-\nize context-to-word relation matrices (Yang et al.,\n2017). In such work, the context comprises all pre-\nvious words observed by a model for predicting\nthe next word. However, we believe that context-\nto-word relation matrices are not sufﬁcient for de-\nscribing how natural sentences are constructed.\nWe argue that natural sentences are generated at\na higher level before being decoded to words.\nHence a language model should be able to pre-\ndict the following sequence of words given a con-\ntext. In this work, we propose a model that factor-\nizes a context-to-phrase mutual information ma-\ntrix to learn better language models. The context-\nto-phrase mutual information matrix describes the\nrelation among contexts and the probabilities of\n1484\nphrases following given contexts. We make the\nfollowing contributions in this paper:\n•We propose a phrase prediction model that\nimproves the performance of state-of-the-art\nword-level language models.\n•Our model learns to predict approximate\nphrases and headwords without any annota-\ntion.\n2 Related Work\nNeural networks have been widely applied in\nnatural language modeling and generation (Ben-\ngio et al., 2003; Bahdanau et al., 2014) for\nboth encoding and decoding. Among differ-\nent neural architectures, the most popular mod-\nels are recurrent neural networks (RNNs; Mikolov\net al., 2010), long short-term memory networks\n(LSTMs; Hochreiter and Schmidhuber, 1997), and\nconvolutional neural networks (CNNs; Bai et al.,\n2018; Dauphin et al., 2017).\nMany modiﬁcations of network structures have\nbeen made based on these architectures. LSTMs\nwith self-attention can improve the performance of\nlanguage modeling (Tran et al., 2016; Cheng et al.,\n2016). As an extension of simple self-attention,\ntransformers (Vaswani et al., 2017) apply multi-\nhead self-attention and have achieved competitive\nperformance compared with recurrent neural lan-\nguage models. A current state-of-the-art model,\nTransformer-XL (Dai et al., 2018), applied both\na recurrent architecture and a multi-head atten-\ntion mechanism. To improve the quality of input\nword embeddings, character-level information is\nalso considered (Kim et al., 2016). It has also been\nshown that context encoders can learn syntactic in-\nformation (Shen et al., 2017).\nHowever, instead of introducing architectural\nchanges, for example a self-attention mechanism\nor character-level information, previous studies\nhave shown that careful hyper-parameter tuning\nand regularization techniques on standard LSTM\nlanguage models can obtain signiﬁcant improve-\nments (Melis et al., 2017; Merity et al., 2017).\nSimilarly, applying more careful dropout strate-\ngies can also improve the language models (Gal\nand Ghahramani, 2016; Melis et al., 2018). LSTM\nlanguage models can be improved with these\napproaches because LSTMs suffer from serious\nover-ﬁtting problems.\nRecently, researchers have also attempted to\nimprove language models at the decoding phase.\nInan et al. (2016) showed that reusing the in-\nput word embeddings in the decoder can reduce\nthe perplexity of language models. Yang et al.\n(2017) showed the low-rank issue in factoriz-\ning the context-to-word mutual information ma-\ntrix and proposed a multi-head softmax decoder to\nsolve the problem. Instead of predicting the next\nword by using only similarities between contexts\nand words, the neural cache model (Grave et al.,\n2016) can signiﬁcantly improve language model-\ning by considering the global word distributions\nconditioned on the same contexts in other parts of\nthe corpus.\nTo learn the grammar and syntax in natural lan-\nguages, Dyer et al. (2016) proposed the recurrent\nneural network grammar (RNNG) that models lan-\nguage incorporating a transition parsing model.\nSyntax annotations are required in this model.\nTo utilize the constituent structures in language\nmodeling without syntax annotation, parse-read-\npredict networks (PRPNs; Shen et al., 2017) cal-\nculate syntactic distances among words and com-\nputes self-attentions. Syntactic distances have\nbeen proved effective in constituent parsing tasks\n(Shen et al., 2018a). In this work, we learn phrase\nsegmentation with a model based on this method\nand our model does not require syntax annotation.\n3 Syntactic Height and Phrase Induction\nIn this work, we propose a language model that not\nonly predicts the next word of a given context, but\nalso attempts to match the embedding of the next\nphrase. The ﬁrst step of this approach is conduct-\ning phrase induction based on syntactic heights.\nIn this section, we explain the deﬁnition of syntac-\ntic height in our approach and describe the basics\nideas about whether a word can be included in an\ninduced phrase.\nIntuitively, the syntactic height of a word aims\nto capture its distance to the root node in a depen-\ndency tree. In Figure 1, the syntactic heights are\nrepresented by the red bars. A word has high syn-\ntactic height if it has low distance to the root node.\nA similar idea, named syntactic distance, is pro-\nposed by Shen et al. (2017) for constructing con-\nstituent parsing trees. We apply the method for\ncalculating syntactic distance to calculate syntac-\ntic height. Given a sequence of embeddings of in-\nput words [x1,x2,···,xn], we calculate their syn-\ntactic heights with a temporal convolutional net-\n1485\nwork (TCN) (Bai et al., 2018).\ndi = Wd ·[xi−n,xi−n+1,···,xi]T + bd (1)\nhi = Wh ·ReLU(di) +bh (2)\nwhere hi stands for the syntactic height of wordxi.\nThe syntactic height hi for each word is a scalar,\nand Wh is a 1 ×D matrix, where D is the di-\nmensionality of di. These heights are learned and\nnot imposed by external syntactic supervision. In\nShen et al. (2017), the syntactic heights are used to\ngenerate context embeddings. In our work, we use\nthe syntactic heights to predict induced phrases\nand calculate their embeddings.\nWe deﬁne the phrase induced by a word based\non the syntactic heights. Consider two words xi\nand xk. xk belongs to the phrase induced by xi if\nand only if for any j ∈(i,k), hj < max(hi,hk).\nFor example, in Figure 1, the phrase induced by\nthe red marked word the is “the morning ﬂights”,\nsince the syntactic height of the word morning,\nhmorning <hflights . However, the word “to” does\nnot belong to the phrase because hflights is higher\nthan both hthe and hto. The induced phrase and\nthe inducing dependency connection are labeled in\nblue in the ﬁgure.\nNote that this deﬁnition of an induced phrase\ndoes not necessarily correspond to a phrase in\nthe syntactic constituency sense. For instance,\nthe words “to Houston” would be included in the\nphrase “the morning ﬂights to Houston” in a tra-\nditional syntactic tree. Given the deﬁnition of\ninduced phrases, we propose phrase segmenting\nconditions (PSCs) to ﬁnd the last word of an in-\nduced phrase. Considering the induced phrase of\nthe i-th word, si = [xi,xi+1,···,xj]. If xj is not\nthe last word of a given sentence, there are two\nconditions that xj should satisfy:\n1. (PSC-1) The syntactic height of xj must be\nhigher than the height of xi, that is\nhj −hi >0 (3)\n2. (PSC-2) The syntactic height of xj+1 should\nbe lower that xj.\nhj −hj+1 >0 (4)\nGiven the PSCs, we can decide the induced\nphrases for the sentence shown in Figure 1. The\nlast word of the phrase induced by “United” is\nUnited canceled the morning ﬂights to Houston\nroot\nFigure 1: Groundtruth dependency tree and syntactic\nheights of each word.\n“canceled”, and the last word of the phrase in-\nduced by “ﬂights” is “Houston”. For the word\nassigned the highest syntactic height, its induced\nphrase is all remaining words in the sentence.\n4 Model\nIn this work, we formulate multi-layer neural lan-\nguage models as a two-part framework. For exam-\nple, in a two-layer LSTM language model (Merity\net al., 2017), we use the ﬁrst layer as phrase gen-\nerator and the last layer as a word generator:\n[c1,c2,···,cT ] =RNN1([x1,x2,···,xT ]) (5)\n[y1,y2,···,yT ] =RNN2([c1,c2,···,cT ]) (6)\nFor a L-layer network, we can regard the ﬁrst\nL1 layers as the phrase generator and the next\nL2 = L−L1 layers as the word generator. Note\nthat we use yi to represent the hidden state out-\nput by the second layer instead of hi, since hi\nin our work is deﬁned as the syntactic height of\nxi. In the traditional setting, the ﬁrst layer does\nnot explicitly learn the semantics of the following\nphrase because there is no extra objective function\nfor phrase learning.\nIn this work, we force the ﬁrst layer to out-\nput context embeddings ci for phrase prediction\nwith three steps. Firstly, we predict the induced\nphrase for each word according to the PSCs pro-\nposed in Section 3. Secondly, we calculate the em-\nbedding of each phrase with a head-ﬁnding atten-\ntion. Lastly, we align the context embedding and\nphrase embedding with negative sampling. The\nword generation is trained in the same way as stan-\ndard language models. The diagram of the model\nis shown in Figure 2. The three steps are described\nnext.\n1486\nUnited\ncanceled\nthe\nmorning\nﬂights\nto\nHouston\nPhrase Generator\nStep 1. Syntactic height\nand phrase induction\nmorning ﬂights\nStep2. Phrase embedding\nwith headword attention\nWord Generator\nContext-phrase\nalignment\nnext-word\nembedding:\nmorning\nContext-word\nalignment\nObjective\nFunction\nStep 3. Phrase and word prediction\nPhrase\nEmbedding:\nmorning ﬂights\nFigure 2: The 3-step diagram of our approach. The current target word is “the”, the induced phrase is “morning\nﬂights”, and the next word is “morning”. The context-phrase and context-word alignments are jointly trained.\n4.1 Phrase Segmentation\nWe calculate the syntactic height and predict the\ninduced phrase for each word:\nhi = TCN([xi−n,xi−n+1,···,xi]) (7)\nwhere TCN(·) stands for the TCN model de-\nscribed in Equations (1) and (2), andnis the width\nof the convolution window.\nBased on the proposed phrase segmenting con-\nditions (PSCs) described in the previous section,\nwe predict the probability of a word being the ﬁrst\nword outside a induced phrase. Firstly, we decide\nif each word, xj−1,j ∈(i+ 1,n], satisﬁes the two\nphrase segmenting conditions, PSC-1 and PSC-2.\nThe probability that xj satisﬁes PSC-1 is\np1\npsc(xj) =1\n2 ·(fHT (hj −hi) + 1) (8)\nSimilarly, the probability thatxj satisﬁes PSC-2 is\np2\npsc(xj) =1\n2 ·(fHT (hj −hj+1) + 1) (9)\nwhere fHT stands for the HardTanh function with\na temperature a:\nfHT (x) =\n\n\n\n−1 x≤−1\na\na·x −1\na <x ≤1\na\n1 x> 1\na\nThis approach is inspired by the context attention\nmethod proposed in the PRPN model (Shen et al.,\n2017).\nThen we can infer the probability of whether a\nword belongs to the induced phrase of xi with\npind(xj) =\nj∏\nk=1\nˆp(xk) (10)\nwhere pind(xi) stands for the probability that xi\nbelongs to the induced phrase, and\nˆp(xk)=\n{\n1 k ≤ i + 1\n1 − p1\npsc(xk−1) · p2\npsc(xk−1) k > i+ 1\nNote that the factorization in Equation 10 assumes\nthat words are independently likely to be included\nin the induced phrase of xi.\n4.2 Phrase Embedding with Attention\nGiven induced phrases, we can calculate their em-\nbeddings based on syntactic heights. To calculate\nthe embedding of phrase s= [x1,x2,···,xn], we\ncalculate an attention distribution over the phrase:\nαi = hi ·pind(xi) +c∑\nj hj ·pind(xj) +c (11)\nwhere hi stands for the syntactic height for word\nxi and cis a constant real number for smoothing\nthe attention distribution. Then we generate the\nphrase embedding with a linear transformation:\ns= W ·\n∑\ni\nαi ·ei (12)\nwhere ei is the word embedding of xi. In training,\nwe apply a dropout layer on s.\n4.3 Phrase and Word Prediction\nA traditional language model learns the probabil-\nity of a sequence of words:\np(x1,x2,···,xn) =p(x1) ·\n∏\ni\np(xi+1|xi\n1) (13)\nwhere xi\n1 stands for x1,x2,···,xi, which is the\ncontext used for predicting the next word, xi+1.\nIn most related studies, the probability p(xi+1|xi\n1)\n1487\nis calculated with the output of the top layer of\na neural network yi and the word representations\nei+1 learned by the decoding layer:\np(xi+1) =Softmax(eT\ni+1 ·yi) (14)\nThe state-of-the-art neural language models\ncontain multiple layers. The outputs of different\nhidden layers capture different level of semantics\nof the context. In this work, we force one of the\nhidden layers to align its output with the embed-\ndings of induced phrases si. We apply an em-\nbedding model similar to Mikolov et al. (2013)\nto train the hidden output and phrase embedding\nalignment. We deﬁne the context-phrase align-\nment model as follows.\nWe ﬁrst deﬁne the probability that a phrase phi\ncan be induced by context [x1,...,x i].\np(phi|xi\n1) =σ(cT\ni ·si) (15)\nwhere σ(x) = 1\n1+e−x , and ci stands for the con-\ntext embedding of x1,x2,···,xi output by a hid-\nden layer, deﬁned in Equation 5. si is the gener-\nated embedding of an induced phrase. The proba-\nbility that a phrase phi cannot be induced by con-\ntext [x1,...,x i] is 1 −p(phi|xi\n1). This approach\nfollows the method for learning word embeddings\nproposed in Mikolov et al. (2013).\nWe use an extra objective function and the neg-\native sampling strategy to align context represen-\ntations and the embeddings of induced phrases.\nGiven the context embedding ci, the induced\nphrase embedding si, and random sampled neg-\native phrase embeddings sneg\ni , we train the neu-\nral network to maximize the likelihood of true in-\nduced phrases and minimize the likelihood of neg-\native samples. we deﬁne the following objective\nfunction for context i:\nlCPA\ni = 1−σ(cT\ni ·si) +1\nn\nn∑\nj=1\nσ(cT\ni ·sneg\nj ) (16)\nwhere n stands for the number of negative sam-\nples. With this loss function, the model learns\nto maximize the similarity between the context\nand true induced phrase embeddings, and mini-\nmize the similarity between the context and neg-\native samples randomly selected from the induced\nphrases of other words. In practice, this loss func-\ntion is used as a regularization term with a coefﬁ-\ncient γ:\nl= lLM + γ·lCPA (17)\nIt worth noting that our approach is model-\nagnostic and and can be applied to various ar-\nchitectures. The TCN network for calculating\nthe syntactic heights and phrase inducing is an\nindependent module. In context-phrase align-\nment training with negative sampling, the objec-\ntive function provides phrase-aware gradients and\ndoes not change the word-by-word generation pro-\ncess of the language model.\n5 Experiments\nWe evaluate our model with word-level language\nmodeling tasks on Penn Treebank (PTB; Mikolov\net al., 2010), Wikitext-2 (WT2; Bradbury et al.,\n2016), and Wikitext-103 (WT103; Merity et al.,\n2016) corpora.\nThe PTB dataset has a vocabulary size of 10,000\nunique words. The entire corpus includes roughly\n40,000 sentences in the training set, and more than\n3,000 sentences in both valid and test set.\nThe WT2 data is about two times larger the the\nPTB dataset. The dataset consists of Wikipedia\narticles. The corpus includes 30,000 unique words\nin its vocabulary and is not cleaned as heavily as\nthe PTB corpus.\nThe WT103 corpus contains a larger vocabulary\nand more articles than WT2. It consists of 28k\narticles and more than 100M words in the training\nset. WT2 and WT103 corpora can evaluate the\nability of capturing long-term dependencies (Dai\net al., 2018).\nIn each corpus, we apply our approach to\npublicly-available, state-of-the-art models. This\ndemonstrates that our approach can improve dif-\nferent existing architectures. Our trained models\nwill be published for downloading. The imple-\nmentation of our models is publicly available.1\n5.1 Penn Treebank\nWe train a 3-layer AWD-LSTM language model\n(Merity et al., 2017) on PTB data set. We use\n1,150 as the number of hidden neurons and 400\nas the size of word embeddings. We also apply the\nword embedding tying strategy (Inan et al., 2016).\nWe apply variational dropout for hidden states\n(Gal and Ghahramani, 2016) and the dropout rate\nis 0.25. We also apply weight dropout (Merity\net al., 2017) and set weight dropout rate as 0.5. We\napply stochastic gradient descent (SGD) and av-\neraged SGD (ASGD; Polyak and Juditsky, 1992)\n1https://github.com/luohongyin/PILM\n1488\nModel #Params Dev PPL Test PPL\nInan et al. (2016) – Tied Variational LSTM 24M 75.7 73.2\nZilly et al. (2017) – Recurrent Highway Networks 23M 67.9 65.7\nShen et al. (2017) – PRPN - - 62.0\nPham et al. (2018) – Efﬁcient NAS 24M 60.8 58.6\nMelis et al. (2017) – 4-layer skip LSTM (tied) 24M 60.9 58.3\nShen et al. (2018b) – ON-LSTM 25M 58.3 56.2\nLiu et al. (2018) – Differentiable NAS 23M 58.3 56.1\nMerity et al. (2017) – AWD-LSTM 24M 60.7 58.8\nMerity et al. (2017) – AWD-LSTM + ﬁnetuning 24M 60.0 57.3\nOurs – AWD-LSTM + Phrase Induction - NS 24M 61.0 58.6\nOurs – AWD-LSTM + Phrase Induction - Attention 24M 60.2 58.0\nOurs – AWD-LSTM + Phrase Induction 24M 59.6 57.5\nOurs – AWD-LSTM + Phrase Induction + ﬁnetuning 24M 57.8 55.7\nDai et al. (2018) – Transformer-XL 24M 56.7 54.5\nYang et al. (2017) – AWD-LSTM-MoS + ﬁnetuning 22M 56.5 54.4\nTable 1: Experimental results on Penn Treebank dataset. Compared with the AWD-LSTM baseline models, our\nmethod reduced the perplexity on test set by 1.6.\nModel #Params Dev PPL Test PPL\nInan et al. (2016) – Variational LSTM (tied) 28M 92.3 87.7\nInan et al. (2016) – VLSTM + augmented loss 28M 91.5 87.0\nGrave et al. (2016) – LSTM - - 99.3\nGrave et al. (2016) – LSTM + Neural cache - - 68.9\nMelis et al. (2017) – 1-Layer LSTM 24M 69.3 69.9\nMelis et al. (2017) – 2-Layer Skip Conn. LSTM 24M 69.1 65.9\nMerity et al. (2017) – AWD-LSTM + ﬁnetuning 33M 68.6 65.8\nOurs – AWD-LSTM + Phrase Induction 33M 68.4 65.2\nOurs – AWD-LSTM + Phrase Induction + ﬁnetuning 33M 66.9 64.1\nTable 2: Experimental results on Wikitext-2 dataset.\nfor training. The learning rate is 30 and we clip\nthe gradients with a norm of 0.25. For the phrase\ninduction model, we randomly sample 1 negative\nsample for each context, and the context-phrase\nalignment loss is given a coefﬁcient of 0.5. The\noutput of the second layer of the neural network\nis used for learning context-phrase alignment, and\nthe ﬁnal layer is used for word generation.\nWe compare the word-level perplexity of our\nmodel with other state-of-the-art models and our\nbaseline is AWD-LSTM (Merity et al., 2017). The\nexperimental results are shown in Table 1. Al-\nthough not as good as the Transformer-XL model\n(Dai et al., 2018) and the mixture of softmax\nmodel (Yang et al., 2017), our model signiﬁcantly\nimproved the AWD-LSTM, reducing 2.2 points of\nperplexity on the validation set and 1.6 points of\nperplexity on the test set. Note that the “ﬁnetun-\ning” process stands for further training the lan-\nguage models with ASGD algorithm (Merity et al.,\n2017).\nWe also did an ablation study without either\nheadword attention or negative sampling (NS).\nThe results are listed in Table 1. By simply av-\neraging word vectors in the induced phrase With-\nout the attention mechanism, the model performs\nworse than the full model by 0.5 perplexity, but\nis still better than our baseline, the AWD-LSTM\nmodel. In the experiment without negative sam-\npling, we only use the embedding of true induced\n1489\nModel #Params Test PPL\nGrave et al. (2016) – LSTM - 48.7\nBai et al. (2018) – TCN - 45.2\nDauphin et al. (2017) – GCNN-8 - 44.9\nGrave et al. (2016) – LSTM + Neural cache - 40.8\nDauphin et al. (2017) – GCNN-14 - 37.2\nMerity et al. (2018) – 4-layer QRNN 151M 33.0\nRae et al. (2018) – LSTM + Hebbian + Cache - 29.9\nDai et al. (2018) – Transformer-XL Standard 151M 24.0\nBaevski and Auli (2018) – Adaptive input 247M 20.5\nDai et al. (2018) – Transformer-XL Large 257M 18.3\nOurs – Transformer-XL Large + Phrase Induction 257M 17.4\nTable 3: Experimental results on Wikitext-103 dataset.\nphrases to align with the context embedding. It\nis also indicated that the negative sampling strat-\negy can improve the performance by 1.1 perplex-\nity. Hence we just test the full model in the fol-\nlowing experiments.\n5.2 Wikitext-2\nWe also trained a 3-layer AWD-LSTM language\nmodel on the WT2 dataset. The network has the\nsame input size, output size, and hidden size as the\nmodel we applied on PTB dataset, following the\nexperiments done by Merity et al. (2017). Some\nhyper-parameters are different from the PTB lan-\nguage model. We use a batch size of 60. The\nembedding dropout rate is 0.65 and the dropout\nrate of hidden outputs is set to 0.2. Other hyper-\nparameters are the same as we set in training on\nthe PTB dataset.\nThe experimental results are shown in Table 2.\nOur model improves the AWD-LSTM model by\nreducing 1.7 points of perplexity on both the val-\nidation and test sets, while we did not make any\nchange to the architecture of the AWD-LSTM lan-\nguage model.\n5.3 Wikitext-103\nThe current state-of-the-art language model\ntrained on Wikitext-103 dataset is the\nTransformer-XL (Dai et al., 2018). We apply our\nmethod on the state-of-the-art Transformer-XL\nLarge model, which has 18 layers and 257M\nparameters. The input size and hidden size are\n1024. 16 attention heads are used. We regard the\nﬁrst 14 layers as the phrase generator and the last\n4 layers as the word generator. In other words,\nthe context-phrase alignment is trained with the\noutputs of the 14th layer.\nThe model is trained on 4 Titan X Pascal GPUs,\neach of which has 12G memory. Because of the\nlimitation of computational resources, we use our\napproach to ﬁne-tune the ofﬁcially released pre-\ntrained Transformer-XL Large model for 1 epoch.\nThe experimental results are shown in Table 3.\nOur approach got 17.4 perplexity with the ofﬁ-\ncially released evaluation scripts, signiﬁcantly out-\nperforming all baselines and achieving new state-\nof-the-art performance2.\n6 Discussion\nIn this section, we show what is learned by training\nlanguage models with the context-phrase align-\nment objective function by visualizing the syn-\ntactic heights output by the TCN model and the\nphrases induced by each target word in a sentence.\nWe also visualize the headword attentions over the\ninduced phrase.\nThe ﬁrst example is the sentence showed in Fig-\nure 1. The sentence came from Jurafsky and Mar-\ntin (2014) and did not appear in our training set.\nFigure 1 shows the syntactic heights and the in-\nduced phrase of “the” according to the ground-\ntruth dependency information. Our model is not\ngiven such high-quality inputs in either training or\nevaluation.\nFigure 3 visualizes the structure learned by our\nphrase induction model. The inferred syntactic\nheights are shown in Figure 3a. Heights assigned\n2We did not show Dev PPLs in Table 3 since only the\ncorrect approach to reproduce the test PPL was provided with\nthe pretrained Transformer-XL model.\n1490\nUnitedcanceled\nthe\nmorningﬂights\nto\nHouston\nPredictions Groundtruth\n(a) Syntactic heights of each word.\nunited\ncanceled\nthe\nmorningflights\nto\nhouston\nunited\ncanceled\nthe\nmorning\nflights\nto\n0.0 1.0 0.0 0.0 0.0 0.0 0.0\n0.0 0.0 0.12 0.32 0.34 0.03 0.19\n0.0 0.0 0.0 0.48 0.52 0.0 0.0\n0.0 0.0 0.0 0.0 1.0 0.0 0.0\n0.0 0.0 0.0 0.0 0.0 0.12 0.88\n0.0 0.0 0.0 0.0 0.0 0.0 1.0 (b) Induced phrases and headword attentions.\nFigure 3: Examples of induced phrases and corresponding headword attention for generating the phrase embed-\nding. The word of each row stands for the target word as the current input of the language model, and the values\nin each row in the matrices stands for the words consisting the induced phrase and their weights.\nwedidn't\nevengeta\nchance\ntodothe\nprograms\nwe\nwanted\ntodo\nwe\ndid\nn't\neven\nget\na\nchance\nto\ndo\nthe\nprograms\nwe\nwanted\nto\n(a)\nseveral\nfund\nmanagers\nexpect\na\nroughmarket\nthis\nmorningbeforepricesstabilize\nseveral\nfund\nmanagers\nexpect\na\nrough\nmarket\nthis\nmorning\nbefore\nprices (b)\n(c)\n (d)\nbut a\nmajority\nofthe\n<unk>council\ndidn'tbuythose\narguments\nbut\na\nmajority\nof\nthe\n<unk>\ncouncil\ndid\nn't\nbuy\nthose\n(e)\nat\nleasttheybothspeakwithstrong<unk>\nasdo\n<unk>\nand\n<unk>\nat\nleast\nthey\nboth\nspeak\nwith\nstrong\n<unk>\nas\ndo\n<unk>\nand (f)\nFigure 4: Examples of phrase inducing and headword attentions.\n1491\nto words “the” and “to” are signiﬁcantly lower\nthan others, while the verb “canceled” is assigned\nthe highest in the sentence. Induced phrases are\nshown in Figure 3b. The words at the beginning\nof each row stand for the target word of each step.\nValues in the matrix stand for attention weights\nfor calculating phrase embedding. The weights\nare calculated with the phrase segmenting condi-\ntions (PSC) and the syntactic heights described in\nEquations 8 to 11. For the target word “united”,\nhunited < hcanceled and hcanceled > hthe, hence\nthe induced phrase of “united” is a single word\n“canceled”, and the headword attention of “can-\nceled” is 1, which is indicated in the ﬁrst row of\nFigure 3b. The phrase induced by “canceled” is\nthe entire following sequence, “the morning ﬂights\nto houston”, since no following word has a higher\nsyntactic height than the target word. It is also\nshown that the headword of the induced phrase of\n“canceled” is “ﬂights”, which agrees with the de-\npendency structure indicated in Figure 1.\nMore examples are shown in Figure 4. Figures\n4a to 4d show random examples without any un-\nknown word, while the examples shown in Figures\n4e and 4f are randomly selected from sentences\nwith unknown words, which are marked with the\nUNK symbol. The examples show that the phrase\ninduction model does not always predict the exact\nstructure represented by the dependency tree. For\nexample, in Figure 4b, the TCN model assigned\nthe highest syntactic height to the word “market”\nand induced the phrase “expect a rough market”\nfor the context “the fund managers”. However, in\na ground-truth dependency tree, the verb “expect”\nis the word directly connected to the root node and\ntherefore has the highest syntactic height.\nAlthough not exactly matching linguistic de-\npendency structures, the phrase-level structure\npredictions are reasonable. The segmentation is\ninterpretable and the predicted headwords are ap-\npropriate. In Figure 4c, the headwords are “try-\ning”, “quality”, and “involvement”. The model is\nalso robust with unknown words. In Figure 4e,\n“the <unk>council” is segmented as the induced\nphrase of “but a majority of”. In this case, the\nmodel recognized that the unknown word is de-\npendent on “council”.\nThe sentence in Figure 4f includes even more\nunknown words. However, the model still cor-\nrectly predicted the root word, the verb “speak”.\nFor the target word “with”, the induced phrase is\n“strong <unk>”. Two unknown words are lo-\ncated in the last few words of the sentence. The\nmodel failed to induce the phrase “ <unk> and\n<unk>” for the word “do”, but still successfully\nsplit “<unk>” and “and”. Meanwhile, the atten-\ntions over the phrases induced by “speak”, “do”,\nand the ﬁrst “ <unk>” are not quite informative,\nsuggesting that unknown words made some difﬁ-\nculties for headword prediction in this example.\nHowever, the unknown words are assigned sig-\nniﬁcantly higher syntactic heights than the word\n“and”.\n7 Conclusion\nIn this work, we improved state-of-the-art lan-\nguage models by aligning context and induced\nphrases. We deﬁned syntactic heights and phrase\nsegmentation rules. The model generates phrase\nembeddings with headword attentions. We im-\nproved the AWD-LSTM and Transformer-XL lan-\nguage models on different data sets and achieved\nstate-of-the-art performance on the Wikitext-103\ncorpus. Experiments showed that our model suc-\ncessfully learned approximate phrase-level knowl-\nedge, including segmentation and headwords,\nwithout any annotation. In future work, we aim to\ncapture better structural information and possible\nconnections to unsupervised grammar induction.\nReferences\nAlexei Baevski and Michael Auli. 2018. Adaptive in-\nput representations for neural language modeling.\narXiv preprint arXiv:1809.10853.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nShaojie Bai, J Zico Kolter, and Vladlen Koltun.\n2018. An empirical evaluation of generic convolu-\ntional and recurrent networks for sequence model-\ning. arXiv preprint arXiv:1803.01271.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nJames Bradbury, Stephen Merity, Caiming Xiong, and\nRichard Socher. 2016. Quasi-recurrent neural net-\nworks. arXiv preprint arXiv:1611.01576.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\nLong short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733.\n1492\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2018. Transformer-xl: Language\nmodeling with longer-term dependency.\nYann N Dauphin, Angela Fan, Michael Auli, and David\nGrangier. 2017. Language modeling with gated con-\nvolutional networks. In Proceedings of the 34th\nInternational Conference on Machine Learning-\nVolume 70, pages 933–941. JMLR. org.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. arXiv preprint arXiv:1602.07776.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 1019–1027.\nEdouard Grave, Armand Joulin, and Nicolas\nUsunier. 2016. Improving neural language\nmodels with a continuous cache. arXiv preprint\narXiv:1612.04426.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. arXiv\npreprint arXiv:1611.01462.\nDan Jurafsky and James H Martin. 2014. Speech and\nlanguage processing, volume 3. Pearson London.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In Thirtieth AAAI Conference on Artiﬁcial\nIntelligence.\nHanxiao Liu, Karen Simonyan, and Yiming Yang.\n2018. Darts: Differentiable architecture search.\narXiv preprint arXiv:1806.09055.\nG´abor Melis, Charles Blundell, Tom ´aˇs Ko ˇcisk`y,\nKarl Moritz Hermann, Chris Dyer, and Phil Blun-\nsom. 2018. Pushing the bounds of dropout. arXiv\npreprint arXiv:1805.09208.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. An analysis of neural language\nmodeling at multiple scales. arXiv preprint\narXiv:1803.08240.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nannual conference of the international speech com-\nmunication association.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 confer-\nence on empirical methods in natural language pro-\ncessing (EMNLP), pages 1532–1543.\nHieu Pham, Melody Y Guan, Barret Zoph, Quoc V\nLe, and Jeff Dean. 2018. Efﬁcient neural architec-\nture search via parameter sharing. arXiv preprint\narXiv:1802.03268.\nBoris T Polyak and Anatoli B Juditsky. 1992. Ac-\nceleration of stochastic approximation by averag-\ning. SIAM Journal on Control and Optimization ,\n30(4):838–855.\nJack W Rae, Chris Dyer, Peter Dayan, and Tim-\nothy P Lillicrap. 2018. Fast parametric learn-\ning with activation memorization. arXiv preprint\narXiv:1803.10049.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron Courville. 2017. Neural language model-\ning by jointly learning syntax and lexicon. arXiv\npreprint arXiv:1711.02013.\nYikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessan-\ndro Sordoni, Aaron Courville, and Yoshua Bengio.\n2018a. Straight to the tree: Constituency pars-\ning with neural syntactic distance. arXiv preprint\narXiv:1806.04168.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2018b. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks.\narXiv preprint arXiv:1810.09536.\nKe Tran, Arianna Bisazza, and Christof Monz. 2016.\nRecurrent memory networks for language modeling.\narXiv preprint arXiv:1601.01272.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2017. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. arXiv\npreprint arXiv:1711.03953.\n1493\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent\nhighway networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning-Volume\n70, pages 4189–4198. JMLR. org."
}