{
  "title": "Parameter-Efficient Model Adaptation for Vision Transformers",
  "url": "https://openalex.org/W4382465573",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2228361739",
      "name": "Xuehai He",
      "affiliations": [
        "University of California, Santa Cruz"
      ]
    },
    {
      "id": "https://openalex.org/A2110028274",
      "name": "Chunyuan Li",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2105155771",
      "name": "Pengchuan Zhang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2096084794",
      "name": "Jianwei Yang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A3092156362",
      "name": "Xin Eric Wang",
      "affiliations": [
        "University of California, Santa Cruz"
      ]
    },
    {
      "id": "https://openalex.org/A2228361739",
      "name": "Xuehai He",
      "affiliations": [
        "University of California, Santa Cruz"
      ]
    },
    {
      "id": "https://openalex.org/A2110028274",
      "name": "Chunyuan Li",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2105155771",
      "name": "Pengchuan Zhang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2096084794",
      "name": "Jianwei Yang",
      "affiliations": [
        "Microsoft Research (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A3092156362",
      "name": "Xin Eric Wang",
      "affiliations": [
        "University of California, Santa Cruz"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2894175714",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2531409750",
    "https://openalex.org/W6662259680",
    "https://openalex.org/W2118858186",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W2058641082",
    "https://openalex.org/W4200629441",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287204036",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3005861412",
    "https://openalex.org/W3207612418",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3199960018",
    "https://openalex.org/W4224246420",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W1846799578",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W4293865401",
    "https://openalex.org/W3120074043",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W4312884055",
    "https://openalex.org/W2963477238",
    "https://openalex.org/W2041616772",
    "https://openalex.org/W2523246573",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W2963748441"
  ],
  "abstract": "In computer vision, it has achieved great transfer learning performance via adapting large-scale pretrained vision models (e.g., vision transformers) to downstream tasks. Common approaches for model adaptation either update all model parameters or leverage linear probes. In this paper, we aim to study parameter-efficient model adaptation strategies for vision transformers on the image classification task. We formulate efficient model adaptation as a subspace training problem and perform a comprehensive benchmarking over different efficient adaptation methods. We conduct an empirical study on each efficient model adaptation method focusing on its performance alongside parameter cost. Furthermore, we propose a parameter-efficient model adaptation framework, which first selects submodules by measuring local intrinsic dimensions and then projects them into subspace for further decomposition via a novel Kronecker Adaptation method. We analyze and compare our method with a diverse set of baseline model adaptation methods (including state-of-the-art methods for pretrained language models). Our method performs the best in terms of the tradeoff between accuracy and parameter efficiency across 20 datasets under the few-shot setting and 7 image classification datasets under the full-shot setting.",
  "full_text": "Parameter-Efficient Model Adaptation for Vision Transformers\nXuehai He1, Chuanyuan Li2, Pengchuan Zhang2, Jianwei Yang2, Xin Eric Wang1\n1 UC Santa Cruz,\n2Microsoft Research at Redmond\n{xhe89,xwang366}@ucsc.edu, {chunyl,penzhan,jianwyan}@microsoft.com\nAbstract\nIn computer vision, it has achieved great transfer learning\nperformance via adapting large-scale pretrained vision mod-\nels (e.g., vision transformers) to downstream tasks. Common\napproaches for model adaptation either update all model pa-\nrameters or leverage linear probes. In this paper, we aim to\nstudy parameter-efficient model adaptation strategies for vi-\nsion transformers on the image classification task. We formu-\nlate efficient model adaptation as a subspace training prob-\nlem and perform a comprehensive benchmarking over dif-\nferent efficient adaptation methods. We conduct an empirical\nstudy on each efficient model adaptation method focusing on\nits performance alongside parameter cost. Furthermore, we\npropose a parameter-efficient model adaptation framework,\nwhich first selects submodules by measuring local intrinsic\ndimensions and then projects them into subspace for further\ndecomposition via a novel Kronecker Adaptation (KAdapta-\ntion) method. We analyze and compare our method with a\ndiverse set of baseline model adaptation methods (including\nstate-of-the-art methods for pretrained language models). Our\nmethod performs the best in terms of the tradeoff between\naccuracy and parameter efficiency across 20 image classifica-\ntion datasets under the few-shot setting and 7 image classifi-\ncation datasets under the full-shot setting.\nIntroduction\nIn the last few years, large-scale vision models and lan-\nguage models pretrained on web-scale data have seen a great\nsurge of interest with promising performance (Radford et al.\n2019; Devlin et al. 2018; Yang et al. 2019; Liu et al. 2019).\nMeanwhile, aided by the rapid gains in hardware, their sizes\nkeep growing rapidly. Currently, vision transformers (Doso-\nvitskiy et al. 2020) (ViTs) with billions of parameters such\nas ViT-Large (Dosovitskiy et al. 2020) have been released.\nIt is expected that pretrained vision models with even larger\norders of magnitude will emerge in the foreseeable future.\nThese large-scale pretrained models are powerful when\ntransferred to downstream vision tasks. However, deploying\nmany independent instances of fine-tuned models can also\ncause substantial storage and deployment costs and hinder\nthe applicability of large-scale ViTs to real-world problems.\nMotivated by this and the importance of parameter-efficient\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nOurs\nCompacter\nLinear-probing\nFull-model\nFine-tuning\nAdapter-tuning\nLora\n(10!.#+, 68.92%)\n( 10!.!' ,\t66.32%)\n(10&.+#,\t65.08%)\n( 10!.*# ,\t62.77%) ( 10$.($ ,\t61.48%)\n(10'.#!, 65.49%)\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nNumber of trainable parameters\nAccuracy (%)\nFigure 1: The tradeoff between accuracy and parameter\nnumbers of various model adaptation methods. The results\nare measured using the vision transformer (ViT-B-224/32)\nvia CLIP pretraining across the average of 20 image clas-\nsification datasets. Our method places in the topleft corner\nand achieves the best tradeoff between accuracy and param-\neter efficiency. The color of points and numbers denote the\nperformance-efficiency (PE) metric (higher is better).\nlearning (Houlsby et al. 2019; Hu et al. 2021; Zaken, Ravfo-\ngel, and Goldberg 2021; Mahabadi, Henderson, and Ruder\n2021; He et al. 2021), we aim to study the parameter-\nefficient model adaptation strategy for vision transformers.\nConventional wisdom for transfer learning in our computer\nvision community is fine-tuning all model parameters or\nleveraging linear probes. However, performing full-model\nfine-tuning of pretrained ViTs may incur both financial and\nenvironmental costs (Patterson et al. 2021), requires a high\ncomputational budget, and becomes increasingly infeasible\nas the model size continuously grows. Another go-to strat-\negy is performing linear-probing by stacking an additional\ntrainable multi-layer perceptron (MLP) layer in the end. It is\nparameter-efficient yet suboptimal in terms of performance.\nIdeally, we hope to design model adaptation strategies that\ncan achieve the best tradeoff between efficiency and effec-\ntiveness (see Figure 1) ‚Äî optimizing adaptation parameter-\nefficiency while allowing for the model to maintain the ef-\nfectiveness of transfer learning on downstream vision tasks,\nespecially the image classification task.\nTo this end, we ask an essential question: what are the\ngeneral guidelines one should adopt while adapting large-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n817\nscale pretrained vision models on the downstream image\nclassification datasets? This work aims to answer the ques-\ntion by building a benchmark for adapting ViTs and propos-\ning a more parameter-efficient model adaptation method. We\nchoose ViTs as the pretrained vision models, which are rep-\nresentative mainstream state-of-the-art (SOTA) models on a\nwide range of downstream vision tasks. Specifically, we ex-\nperiment with two off-the-shelf pretrained ViTs in the re-\nmainder of this paper: the one via Contrastive Language-\nImage Pretraining (also known as CLIP) (Radford et al.\n2021), and the one via supervised pretraining (we refer to as\nSupervised ViT) (Vaswani et al. 2017). In addition to Full-\nmodel Fine-tuning and linear-probing, we re-implement sev-\neral SOTA efficient adaptation methods (Houlsby et al.\n2019; R ¬®uckl¬¥e et al. 2021; Hu et al. 2021; Zaken, Ravfogel,\nand Goldberg 2021; Li and Liang 2021) (originally proposed\nfor pretrained language models) on vision tasks, and design\nvarious new baseline methods for comparison.\nAghajanyan et al. (2020) show that pretrained language\nmodels have a low intrinsic dimension and can still learn ef-\nficiently despite a low-dimensional reparameterization. Mo-\ntivated by this observation, we reformulate the task of ef-\nficient model adaptation as a subspace training problem.\nWithin this framework, we measure the local intrinsic di-\nmension of each module in ViTs, which reveals that the\nattention module dominates the training progress. More-\nover, we introduce a novel parameter-efficient model adap-\ntation framework named Kronecker Adaptation (KAdap-\ntation), where during adaptation, pretrained weights are\nfrozen, and only the updates to the weights receive gradi-\nents. And the weight updates are decomposed to a set of\nKronecker products, with the slow weights (Wen, Tran, and\nBa 2020) shared across layers and fast weights (Wen, Tran,\nand Ba 2020) further decomposed into low-rank matrices\nproduct to improve parameter efficiency. We apply KAdap-\ntation to attention weights, and it achieves the best average\naccuracy among efficient model adaptation methods while\ncontaining much less trainable parameters, e.g., around45%\nparameters of LoRA (Hu et al. 2021) and 0.09% of all the\nmodel parameters in CLIP under the few-shot setting.\nThe contributions of this paper are summarized below:\n‚Ä¢ We build a benchmark 1 for parameter-efficient model\nadaptation of ViTs on the image classification task by in-\ntroducing our new baseline methods and several state-of-\nthe-art efficient model adaptation strategies inspired from\nthe NLP community. To our best knowledge, this is the\nfirst empirical study of the efficient model adaptation of\nTransformers to date that considers pure vision tasks.\n‚Ä¢ We formulate efficient model adaptation as a subspace\ntraining problem. To address it, we define the local in-\ntrinsic dimension, based on which we choose submod-\nules ‚Äî attention modules and we employ the proposed\nKAdaptation method to decompose the weight updates\nof attention modules for trainable parameter deduction.\n1To facilitate future research, implementations of all the meth-\nods studied in this work are released at https://github.com/eric-ai-\nlab/PEViT.\n‚Ä¢ We experiment on 20 datasets under the few-shot set-\nting and 7 image classification datasets under the full-\nshot setting. The results demonstrate the effectiveness of\nour method, achieving the best tradeoff between accuracy\nand parameter efficiency, as shown in Figure 1.\nRelated Work\nVision Transformer Fine-tuning large-scale pretrained\nViTs has shown prominent performance for computer vi-\nsion tasks, such as image classification (Dosovitskiy et al.\n2020), object detection (Carion et al. 2020), and etc. Re-\ncently, there are also other variants, including hierarchical\nViTs with varying resolutions and spatial embeddings (Liu\net al. 2021; Dong et al. 2021) been proposed. Undoubtedly,\nthe recent progress of large ViTs posts great demands for\ndeveloping efficient model adaptation strategies.\nEfficient Model Adaptation in NLP In the natural lan-\nguage processing domain, efficient model adaptation tech-\nniques typically involve adding to or modifying a limited\nnumber of parameters of the model ‚Äî limiting the dimen-\nsion of the optimization problem can prevent catastrophic\nforgetting (McCloskey and Cohen 1989). Exiting meth-\nods are mainly divided into two categories depending on\nwhether new trainable parameters are introduced. Specifi-\ncally, one is to train a subset of the model parameters, where\nthe common approach is to use a linear probe on top of\npretrained features (Radford et al. 2021). The other alter-\nnatives include new parameters in between the network (Li\nand Liang 2021; R ¬®uckl¬¥e et al. 2021; Houlsby et al. 2019;\nHu et al. 2021; Pfeiffer et al. 2021; Sung, Cho, and Bansal\n2022). Nevertheless, these methodologies normally have not\nbeen investigated in the computer vision scenario and it is\nfurthermore uncertain if findings from NLP tasks (e.g., ques-\ntion answering (Rajpurkar et al. 2016), natural language un-\nderstanding (Wang et al. 2018), etc.) can transfer to down-\nstream vision applications. Spurred by those facts, we estab-\nlish a benchmark to compare these methods and we further\nadvocate our method which can gain a better tradeoff under\nboth the full-shot and few-shot settings.\nEfficient Model Adaptation with Subspace\nTraining\nGiven a large pretrained vision transformer M with size\n|M|. Our goal is to develop a parameter-efficient model\nadaptation technique with trainable parameters Œ∏ of size\nd ‚â™ |M|, that can attain comparable performance with fine-\ntuning the whole model. Our ultimate goal is that one could\nachieve satisfactory results in both efficacy and efficiency\nwithout the hassle of fine-tuning the full model.\nSubspace Training\nA typical neural network contains numerous dense layers\nthat perform matrix multiplication. The weight matrices in\nthese layers can be full-rank. When adapting to a specific\ntask, Aghajanyan et al. (2020) show that the pretrained lan-\nguage models have a low intrinsic dimension and can learn\nefficiently despite a low-dimensional reparameterization.\n818\nDrawing inspiration from their observation and study, we\nhypothesize that the updates to weights of ViTs during each\nstep in model adaptation also have a low intrinsic rank and\ndevelop our method accordingly. The intuition behind our\nmethod is to perform subspace training on weight updates.\nIn the de-facto training paradigm of neural network models,\nthe gradient is computed first, followed by gradient steps\ntaken by the optimizer in the entire parameter space D.\nWhile in subspace training, we instead build a random d-\ndimensional parameter subspace from M, where generally\nd ‚â™ |M|, and optimize directly in this subspace.\nIn fact, most current parameter-efficient NLP model adap-\ntation strategies perform subspace training. Given a large\npretrained language model M with size |M|, existing meth-\nods either select a submodule fromM or inject an additional\nmodule to M. For the parameter vector Œò ‚àà RD from this\nmodule, they learn a projection P mapping Œò into a ran-\ndom d-dimensional subspace and perform training in that\nsubspace to minimize computational cost. With this observa-\ntion, we motivate our study on the efficient model adaptation\nproblem in the principle of subspace training. We approach\nthe problem by addressing two scientific questions: how to\nchoose these submodulesand how to make the subspace pro-\njection.\nThe Proposed Kronecker Adaptation\nTo answer the two fundamental questions of efficient model\nadaptation, how to choose these submodules and how to\nmake the subspace projection , we propose a novel frame-\nwork with two strategies. First, we define the local intrin-\nsic dimension and we choose submodules based on their\nmeasured results. Second, we propose a Kronecker Adap-\ntation method to perform the subspace projection on the se-\nlected submodules by exploiting parameterized hypercom-\nplex multiplication layers (PHM) (Zhang et al. 2021).\nLocal Intrinsic Dimension Measuring the intrinsic di-\nmension of an objective function was first proposed in Li et\nal. (2018). Aghajanyan et al. (2020) extended it to analyze\nthe quality of pretrained language models. They point out\nthat analyzing model adaptation through the lens of intrinsic\ndimension offers empirical and theoretical intuitions. Both\nof them study the intrinsic dimension of the entire model.\nUnlike them, we propose to measure the intrinsic dimen-\nsion of each individual submodule in ViT. We define the\nintrinsic dimension of the submodule as local intrinsic di-\nmension, to distinguish it from the intrinsic dimension of the\nwhole model. The local intrinsic dimension is indicative of\nthe contribution of each submodule during model adaptation\nand measuring it will tell us how many free parameters are\nrequired to approximate the optimization problem closely.\nThe conventional standard method of measuring the intrinsic\ndimensionality of an objective (Li et al. 2018) asks for per-\nforming grid search over different subspace dimensions d,\ntraining using standard SGD (Ruder 2016) over the subspace\nreparameterization, and selecting the smallest d which can\nproduce a satisfactory solution (e.g., 90% of the full training\nmetric). Likewise, we measure the local intrinsic dimension\nvia finding the smallest d for the measured submodule that\nPretrainedweightsùëæùüé\nUpdateweights‚àÜùëæ\nh\nx\nùíñùüè\nùíóùüè\nùíñùíè\nùíóùíè ‚Ä¶SharedweightsAùíä Bùíä\n‚Ä¶\nSelected submodules based on local intrinsic dimension\nDecomposition\nFigure 2: An illustration of KAdaptation. Ai denotes the\nshared weight matrix, with i ‚àà {1, . . . , n}. Bi is decom-\nposed into two low-rank matrices ui and vi. h is the output\nof the selected ViT submodule.x is the input to the submod-\nule. During model adaptation process, only matricesAi, ui,\nand vi receive gradients to improve parameter efficiency.\ncan reach 90% of the full accuracy.\nTo this end, we first follow the similar definition in Li et\nal. (2018) and define Œò in a subspace in the following way:\nŒò = Œò0 + PŒ∏, (1)\nwhere Œò0 ‚àà RD is the initial parameter vector of Œò when\nthe training begins, P ‚àà RD√ód is the projection matrix\ngenerated by the Fastfood transform (Le, Sarl¬¥os, and Smola\n2014), and Œ∏ ‚àà Rd is the parameter vector in the subspace.\nSubspace training proceeds by computing gradients with re-\nspect to Œ∏ and taking steps in that subspace. By performing\nexperiments with gradually larger values of d, we can find\nthe subspace dimension dt at which the performance of the\nmodel M reaches 90% of the full accuracy. We refer to dt\nthe local intrinsic dimension of the measured submodule.\nThe module with the lowest local intrinsic dimension ‚Äî\nattention module is selected. We project them into subspace\nvia our proposed Kronecker Adaptation method for the sake\nof efficient model adaptation. Kronecker Adaptation fine-\ntunes attention weight matrices indirectly by optimizing de-\ncomposition matrices of the updates to attention weight ma-\ntrices. To lower the parameter cost, the decomposition is\ncomputed as the sum of Kronecker products while the orig-\ninal matrices remain frozen.\nKronecker Product The Kronecker product between ma-\ntrix A ‚àà Rm√ón and B ‚àà Rp√óq, denoted by A ‚äó B ‚àà\nRmp√ónq, is mathematically written in the following form:\nA ‚äó B =\nÔ£´\nÔ£¨Ô£≠\na11B ¬∑¬∑¬∑ a1nB\n... ... ...\nam1B ¬∑¬∑¬∑ amnB,\nÔ£∂\nÔ£∑Ô£∏ (2)\nwhere aij shows the element in thei-th row and j-th column\nof A.\nKronecker Adaptation Leveraging the Kronecker prod-\nuct to perform language model compression has been shown\nto be beneficial in prior works (Tahaei et al. 2021; Edalati\net al. 2021). Recently, Zhang et al. (2021) introduces PHM\nlayers, theoretically demonstrating that Kronecker products\ncan help to reduce learnable parameters in language models\nand maintain performance. Built upon the success of PHM,\nfor an update matrix ‚àÜW ‚àà Rk√ód in the ViT, we propose\n819\nthe Kronecker Adaptation to adapt it into subspace. The il-\nlustration is shown in Fig. 2. Mathematically, we compute\n‚àÜW as follows:\n‚àÜW =\nnX\ni=1\nAi ‚äó Bi, (3)\nwhere n is the user-defined hyperparameter representing the\nnumber of Kronecker products, Ai ‚àà Rn√ón, and Bi ‚àà\nR\nk\nn √ó d\nn . The new representation of the update weights in\nEq. 3 is composed of a sum of n Kronecker products be-\ntween shared slow weights Ai and independent fast weights\nBi, with i ‚àà {1, . . . , n}.\nMeanwhile, low-rank methods (Aghajanyan, Zettle-\nmoyer, and Gupta 2020; Li et al. 2018; Sainath et al. 2013)\nhave demonstrated that strong performance can be achieved\nby optimizing models in a low-rank subspace. Similarly, we\nhypothesize that ‚àÜW can be effectively adapted by learning\ntransformations in a low-rank subspace to reduce parameter\ncost further. Therefore, we parameterize Bi ‚àà R\nk\nn √ó d\nn as\nlow rank and further decompose it into the product of two\nlow-rank matrices ui ‚àà R\nk\nn √ór and vi ‚àà Rr√ó d\nn , where r is\nthe rank of the matrix. The expression of ‚àÜW is then:\n‚àÜW =\nnX\ni=1\nAi ‚äó Bi =\nnX\ni=1\nAi ‚äó\n\u0000\nuiv‚ä§\ni\n\u0001\n. (4)\nThe number of trainable parameters is now substantially\nsaved. Note that similar to Bi ‚àà R\nk\nn √ó d\nn , the shared slow\nweights Ai can also be further decomposed into the product\nof low-rank matrices. Additional bias terms can also be ap-\nplied to the update matrix. We give the analysis of parameter\nefficiency in the next section.\nAnalysis of Efficient Adaptation Methods\nDiscussion of State-of-the-art Methods\nIn what follows, we discuss connections between our\nmethod and state-of-the-art parameter-efficient tuning meth-\nods on NLP tasks and provide additional insight into the\ncharacteristics of our method.\nAdapter-tuning (Houlsby et al. 2019) is the first efficient\nmodel adaptation work in the NLP community. It brings in\nan additional trainable set of modules by adding a train-\nable bottleneck layer after the feedforward network in each\nTransformer layer of the pretrained language models. A bot-\ntleneck layer consists of a down and up projection pair that\nshrinks and recovers the size of token hidden states.\nSimilar to the Adapter-tuning method where they use the\nbottleneck structure in the additional layer, our method im-\nplements low-rank decomposition on the fast rank-one ma-\ntrices (Wen, Tran, and Ba 2020). The critical functional dif-\nference is that our learned weights can be merged with the\nmain weights during inference, thus introducing no latency.\nLoRA (Hu et al. 2021) is another line of work for\nparameter-efficient language model tuning: it treats the\nmodel parameters after fine-tuning as an addition of the pre-\ntrained parameters Œòpretrained and task-specific differences\nMethod #Params Complexity\nAdapter-tuning 4Lkd O (kd)\nLoRA 2Lrdmodel O (rdmodel)\nCompacter 4L( k\nn + d\nn) + n3 O\n\u0000k+d\nn\n\u0001\nKAdaptation 2L(dmodel\nn + r\nn) + n3 O\n\u0000r+dmodel\nn\n\u0001\nTable 1: Parameter count in Adapter-tuning, LoRA, Com-\npacter, and Kronecker Adaptation. L is the number of layers\nin the Transformer.k is the size of the input dimension to the\nAdapter layer. d is the bottleneck dimension in the Adapter\nlayer. dmodel is the Transformer hidden size. r denotes the\nrank in the low-rank decomposition step. n is the number of\nKronecker products usually very small.\nŒ∏task , where Œòpretrained is fixed and a new subset of model\nparameters are added on top. Given a pretrained weight ma-\ntrix W0 ‚àà Rd√ók, they constrain its update by performing\nlow-rank decomposition: W0 + ‚àÜW = W0 + BA, where\nA ‚àà Rr√ók, B ‚àà Rd√ór, and the rank r ‚â™ min(d, k). By do-\ning this, the weight matrices are split into two parts, where\nduring training, W0 is frozen and receives no gradient up-\ndates, while only A and B contain trainable parameters.\nOur work differs from LoRA mainly in that we decom-\npose weight updates to a set of Kronecker product decom-\nposition. The decomposedslow weight are shared across lay-\ners, further reducing the parameter cost.\nCompacter (Mahabadi, Henderson, and Ruder 2021)\ninserts task-specific weight matrices into weights of pre-\ntrained models. Each Compacter weight matrix is computed\nas the sum of Kronecker products between shared slow\nweights and fast matrices defined per Compacter layer.\nIn a similar vein to Compacter, we also leverage the Kro-\nnecker product in our method to reduce parameter cost fur-\nther. Yet, apart from application domains, our method fun-\ndamentally differs from Adapter/Compacter based methods\nin that: first, our method brings in no additional layer and\nintroduces no latency; Second, our method first selects sub-\nmodules by measuring the local intrinsic dimension and then\nperforms the Kronecker Adaptation over the update weights\nto selected submodules; Third, during adaptation, only up-\ndates to the weights of selected submodules receive gradi-\nents and tuned, while pretrained weights are always fixed.\nAnalysis of Parameter Efficiency\nWe analyze the parameter-efficiency of our Kronecker\nAdaptation and other model adaptation methods as below:\nAdapter-tuning In the standard setting, two Adapters are\nadded per layer of a Transformer model (Baevski and Auli\n2019). Each Adapter layer consists of 2 √ó k √ó d parameters\nfor the down and up-projection matrices, where k is the size\nof the input dimension and d is the Adapter‚Äôs bottleneck di-\nmension. The total number of parameters for Adapters for a\nL‚àílayer Transformer is, |Œò| = 2 √ó L √ó 2 √ó k √ó ds.\nLoRA LoRA adds trainable pairs of rank decomposition\nmatrices to existing weight matrices. The number of train-\n820\nable parameters is determined by the rank r: |Œò| = 2 √ó L √ó\ndmodel √ó r, where dmodel is Transformer hidden size.\nCompacter Compacter shares the trained weight matri-\nces {Ai}n\ni=1 consisting of n3 parameters across all layers,\nwhere n is the number of Kronecker products. Compacter\nalso has two rank-one weights for each Adapter layer con-\nsisting of k\nn + d\nn parameters, where the Adapter layers are of\nsize k √ód, resulting in a total of2√ó\n\u0000k\nn + d\nn\n\u0001\nparameters for\ndown and up-projection weights. Therefore, the total num-\nber of parameters of Compacter is 4 √ó L √ó\n\u0000k\nn + d\nn\n\u0001\n+ n3\nfor a Transformer with L layers in the encoder and decoder.\nOur Approach we analyze the parameter efficiency of our\napproach under the scenario where we decompose the up-\ndates to weights into a sum of Kronecker products first and\nthen further perform low-rank decomposition for the fast\nweights. The total number of parameters in this scenario will\nbe: 2 √ó L √ó\n\u0000r+dmodel\nn\n\u0001\n+ n3.\nThe overall comparison of parameter counts is shown in\nTable 1. Our method has a complexity ofO\n\u0000r+dmodel\nn\n\u0001\nwith\nr being a small integer. Our approach greatly reduces the\nnumber of parameters. The exact numbers of trainable pa-\nrameters are present in Table 3.\nExperiments\nDatasets\nFor few-shot benchmark experiments, we conduct exper-\niments on 20 image classification datasets from the EL-\nEV ATER benchmark (Li et al. 2022b) on four Quadro\nRTX A6000 GPUs. Detailed dataset statistics are given in\nthe supplementary material. For full-shot experiments, we\nsummarize the results by computing the average perfor-\nmance on CIFAR10 (Krizhevsky and Hinton 2009), CI-\nFAR100 (Krizhevsky and Hinton 2009), SUN397 (Xiao\net al. 2010), DTD (Cimpoi et al. 2014), STL10 (Coates,\nNg, and Lee 2011), FGVCAircraft (Maji et al. 2013), and\nFER2013 (Goodfellow et al. 2013). We use the official split\nfor each of these datasets.\nImplementation Details\nFor benchmark experiments, we use the SGD (Ruder 2016)\noptimizer with the learning rate and weight decay being au-\ntomatically searched for all methods so that these two hy-\nperparameters have the optimum combination. We borrow\nthe automatic hyper-parameter tuning toolkit from Li et al.\n(2022b). Training epochs are set via grid search. We test two\npretrained 12-layer ViTs: the one using ViT-B-224/32 via\nunsupervised pretraining (CLIP) and the one using ViT-B-\n224/16 via supervised pretraining (Supervised ViT).\nFor intrinsic dimension experiments, we use the\nAdamW (Kingma and Ba 2014) as the optimizer, with the\nweight decay of 10‚àí8, learning rate of 10‚àí5, and batch size\nof 32 following the setting in Li et al. (2018). The Fastfood\ntransform (Le, Sarl¬¥os, and Smola 2014) is applied to the at-\ntention and multi-layer perceptron (MLP) module in the first\nlayer of Supervised ViT, respectively. The dimension d is\nmeasured from 0 ‚àí 2000 in both scenarios. Each model is\nfine-tuned for 300 epochs.\nBaselines\nWe test the baselines below. Unless otherwise specified, the\ntask-specific classification layer and added parameters are\ntuned while the pretrained ViTs are frozen.\nFirst are commonly-used model adaptation methods for\nvision models.\n‚Ä¢ Full-model Fine-tuning: fine-tunes all model parameters.\n‚Ä¢ Linear-probing: only tune the classification layer.\nThe second types are SOTA methods borrowed from NLP.\n‚Ä¢ BitFit (Zaken, Ravfogel, and Goldberg 2021): freezes all\nViT parameters except for the bias terms and the task-\nspecific classification layer.\n‚Ä¢ Adapter-tuning (Houlsby et al. 2019): two Adapters are\nadded and tuned in each Transformer layer.\n‚Ä¢ AdapterDrop (R¬®uckl¬¥e et al. 2021): only keep Adapters\nfrom the last Transformer layer.\n‚Ä¢ LoRA (Hu et al. 2021): apply LoRA to Wq and Wv ma-\ntrices in the attention module and tune the low-rank de-\ncomposition matrices.\n‚Ä¢ Compacter (Mahabadi, Henderson, and Ruder 2021): we\nexperiment with n = 4.\nThe third types are new baseline methods we developed.\n‚Ä¢ Transformer-probing: an additional trainable Trans-\nformer block is stacked before the task-specific classi-\nfication layer and tuned.\n‚Ä¢ LoRA-Fix: the matrixA in LoRA (Hu et al. 2021) is fixed\nand only the matrix B is tuned.\n‚Ä¢ LayerNorm Tuning: the layer norm layers are tuned.\n‚Ä¢ Attention Tuning: the attention layers are tuned.\n‚Ä¢ LePE Tuning (Dong et al. 2021): locally-enhanced posi-\ntional encoding (LePE) is added to the ViT and tuned.\nWe implement it by the depthwise convolution opera-\ntor (Chollet 2017) on the matrix V in the attention layer:\nAtten\ntion(Q, K, V ) = SoftMax\n\u0010\nQKT /\n‚àö\nd\n\u0011\nV + D\nWConv(V ).\n‚Ä¢ Relative P\nosition Bias (RPB) Tuning (Liu et al. 2021):\nan additional relative position bias term B is in-\ncluded in computing self-attention in the ViT and tuned:\nAtten\ntion(Q, K, V ) = SoftMax\n\u0010\nQKT /\n‚àö\nd + B\n\u0011\nV .\nLayerNorm T\nuning, Attention Tuning, and BitFit shed light\non which parameters in ViT matter more during model adap-\ntation. Among all modules in ViT, multi-layer perceptron\n(MLP) tuning is not considered a baseline because it is pro-\nhibitively costly compared to others. Given that the special\nstructure of ViT and its variants, e.g., depthwise convolu-\ntion operator and relative position bias, are different from the\ngeneral transformers in natural language processing, we ac-\ntually made the first step towards parameter-efficient model\nadaptation for the ViT via LePE Tuning and Relative Posi-\ntion Bias Tuning.\n821\nMethod\nCaltech101\nCIFAR10\nCIFAR100\nCountry211\nDTD\nEuroSat\nFER2013\nFGVCAircraft\nFood101\nGTSRB\nHatefulMemes\nKittiDistance\nMNIST\nFlowers102\nOxfordPets\nPatchCamelyon\nSST2\nRESISC45\nStanfordCars\nVOC2007\nAv\ne Acc (‚Üë)\n#Params\n(‚Üì)\nPE (\n‚Üë)\nFinetune 87.6\n91.1 71.5 15.8 54.4 85.2 52.7 26.2 83.3 74.1 55.6 39.2 65.6 80.6 87.3 64.9 59.1 75.6 57.2 83.0 65.5 87.9 0.50\nProbing 91.0 90.4 67.3 17.4 62.0 73.0 51.9 29.5 83.8 56.5 55.8 40.4 77.5 92.3 88.0 59.0 59.4 78.1 68.3 85.0 66.3 0.03 0.66\nAdapter 90.2 90.1 73.6 16.8 57.1 68.0 41.8 30.5 83.6 58.5 48.9 37.2 80.3 90.8 86.5 59.9 58.7 79.2 67.7 82.2 65.1 1.24 0.65\nLoRA 87.6 90.5 69.7 17.1 50.2 74.0 51.0 20.0 83.8 43.0 55.9 48.1 61.4 74.3 85.5 63.2 57.0 62.1 54.9 80.3 61.5 0.18 0.61\nCompact 89.0 80.0 44.3 28.2 52.9 50.5 35.5 41.1 78.3 66.9 47.6 57.7 85.8 88.3 79.2 61.8 64.2 63.8 64.8 75.8 62.8 0.08 0.63\nOurs 89.0\n90.0 73.9 17.5 64.0 76.3 47.5 30.0 84.4 80.7 55.9 42.3 85.2 93.2 89.1 63.4 59.2 80.0 70.2 84.5 68.9 0.08 0.69\nTable 2: The averaged 5-shot experimental result on Full-model Fine-tuning, Linear-probing, Adapter-tuning, LoRA, Com-\npacter, and KAdaptation (ours) across 20 datasets from ELEV ATER benchmark (Li et al. 2022b) in terms of accuracy (%) and\nnumber of trainable parameters (in millions) (#Params) across random seeds of{0, 1, 2}. The ViT-B-224/32 via CLIP pretrain-\ning is evaluated. Our method achieves the best tradeoff between accuracy and parameter efficiency: it obtains the best average\naccuracy among all efficient model adaptation methods, while updating only 0.09% of the model parameters in CLIP.\nResults and Analysis\nMetric with performance-efficiency trade-off To better\ncompare different methods with a single number that con-\nsiders both prediction accuracy and parameter-efficiency,\nwe resort to the performance-efficiency (PE) metric defined\nin Li et al. (2022a):\nPE = score‚àóexp (‚àílog10(# trainable-parameters /M0 + 1))\nwhere score is the prediction accuracy, while # trainable-\nparameters is the number of updated parameters in the model\nadaptation stage, and M0 is the normalization constant. M0\nis set to 108 because most existing vision backbone model\nsize are in this magnitude, e.g. ViT-Base (80M parameters).\nThe experimental results of measured average accuracy\nacross the 20 datasets in the low-data regime and under the\n5-shot setting using random seeds of 0, 1, and 2 are shown\nin Table 2. As observed, the parameter cost of linear-probing\nis the lowest while that of full-model fine-tuning is the high-\nest. Our method has the highest average accuracy and re-\nmains the ideal approach with the optimum tradeoff: our\nmethod has much less trainable parameters than other adap-\ntation methods ‚Äî the second lowest and is only higher than\nLinear-probing. From the performance-efficiency trade-off\nmetric, it can also be seen that ours has the highest PE.\nTo further compare our method with SOTA methods for\nNLP models and more baselines, we investigate the perfor-\nmance of adaptation approaches in the full-data regime and\ntest under the full-shot setting. The results across the seven\ndatasets are shown in Table 3. In our analytical experiments,\nwe first observe that Full-model Fine-tuning has the highest\naccuracy in both scenarios, serving as a performance upper\nbound. Second, different efficient model adaptation meth-\nods exhibit diverse characteristics and perform differently on\nthe same task. Third, the results from CLIP are mostly con-\nsistent with the results from Supervised ViT. This suggests\nthat the pretraining strategy may not affect the selection of\ndownstream model adaptation strategy much. Fourth, previ-\nous methods such as Adapter-tuning (Houlsby et al. 2019)\nand LoRA (Hu et al. 2021) are still effective, and their ac-\ncuracy is substantially higher than naive baselines, includ-\ning BitFit and Attention-tuning regardless of the pretrained\ncheckpoint. Fifth, among naive baselines where only sub-\nmodules or task-specific classification heads are tuned, tun-\ning the parameters of the attention layer turns out to be a sur-\nprisingly effective approach even compared to some SOTA\nmethods, though its parameter cost is significantly higher.\nThis further validates the effectiveness of our method by ap-\nplying Kronecker Adaptation to attention weights. Finally,\nour method outperforms all the SOTA methods borrowed\nfrom the NLP community as well as their variants in both\nscenarios.\nFurthermore, the average number of trainable parameters\nacross seven datasets is also shown in Table 3. As can be\nseen, our Kronecker Adaptation method contains the lowest\nparameter cost compared with other SOTA methods. This\nphenomenon is obviously noticeable when compared with\nFull-model Fine-tuning, where our method takes less than\n0.14% of trainable parameters of end-to-end Full-model\nFine-tuning but it can achieve comparable performance.\nLocal Intrinsic Dimension\nLocal intrinsic dimension (Li et al. 2018) informs us of\nthe importance of each module in the ViT and we select\nsubmodules to perform Kronecker Adaptation based on the\nmeasurement results of local instricsic dimension. We mea-\nsure the local intrinsic dimension of the two fundamental ar-\nchitectural components in the ViT ‚Äî the MLP module and\nthe attention module. We use the remarkable Fastfood trans-\nform (Le, Sarl¬¥os, and Smola 2014) to do the projection. The\naccuracy results averaged across{1, 6, 12}-th ViT layers are\nshown in Fig. 3. As a substantiating point to performing Kro-\nnecker Adapting on attention layers, it can be observed that\nthe attention module has a lower intrinsic dimension than the\nMLP module (300 vs. 575) in the ViT.\nAblation Studies\nWe ablate our method and Adapter-tuning using the set-\ntings in Table 3. In Table 4, several intriguing properties\nare observed. First, applying KAdaptation to MLP mod-\n822\nMethod CLIP Supervised\nViT\nCF10 CF100\nSUN DTD FER FGVC STL Ave #Par CF10 CF100 SUN DTD FER FGVC STL Ave #Par\nCommonly-used model\nadaptation methods for vision models\nFinetune 97.7 85.4 73.8 79.0\n69.8 59.0 99.7 80.6 88 99.0 92.4 75.0 72.4 68.2 52.6 99.6 79.9 86\nProbing 94.8 80.1 72.4 75.4 67.3 49.7 98.4 76.9 0.1 96.3 87.7 70.1 72.7 60.1 45.0 98.7 75.8 0.1\nSOT\nA methods for NLP models\nBitFit 92.1\n76.0 70.8 75.9 68.0 54.5 98.8 76.6 0.2 92.3 81.0 71.8 72.6 60.4 45.9 99.0 74.7 0.4\nAdapter 94.7 81.4 77.1 78.0 68.4 55.3 99.0 79.1 1.2 98.4 90.6 74.2 71.0 63.4 52.4 99.3 78.5 1.5\nAdapterDrop 93.3 78.3 71.4 77.1 67.1 51.3 98.0 76.6 0.1 96.8 88.4 72.3 70.2 46.9 35.6 99.6 72.8 0.2\nLoRA 95.1 78.1 80.8 78.1 67.7 55.8 99.2 79.3 0.2 98.7 90.6 73.6 70.4 62.7 54.9 99.4 78.6 0.2\nBaseline methods\ndeveloped in this work\nTF-Probing 95.6\n80.1 74.3 75.9 67.6 50.9 98.5 77.6 3.2 96.5 86.9 76.7 72.0 60.7 45.5 99.0 76.8 3.2\nLoRA-Fix 92.5 77.1 60.0 77.7 65.5 44.4 88.6 72.3 0.1 96.2 88.3 72.0 65.5 53.4 51.7 99.0 75.2 0.2\nLN-tuning 82.5 76.6 66.7 72.4 61.0 37.6 99.1 70.8 0.1 92.2 71.7 72.0 69.0 52.7 51.0 98.8 72.5 0.1\nAtt-tuning 96.8 81.8 73.1 75.0 62.2 54.2 97.6 77.2 41 93.9 85.7 73.8 69.2 55.2 51.9 99.2 75.6 28\nLePE-tuning 95.1 78.9 68.0 75.4 65.2 54.0 98.0 76.4 0.1 93.7 90.8 73.2 69.8 60.0 49.3 99.1 76.6 0.2\nRPB-tuning 94.7 77.1 68.4 75.2 65.1 54.1 97.9 76.1 0.1 96.7 87.0 72.4 70.4 50.9 51.4 98.9 75.4 0.2\nKAdaptation 95.9\n84.8 74.0 78.1 69.0 56.0 99.2 79.6 0.1 97.9 91.2 75.1 71.4 63.8 55.5 99.4 79.2 0.1\nTable 3: The results comparison on Full-model Fine-tuning (Finetune), Linear-probing (Probing), BitFit, Adapter-tuning\n(Adapter), AdapterDrop, LoRA, Transformer-probing (TF-Probing), LoRA-Fix, LayerNorm Tuning (LN-tuning), Attention\nTuning (Att-tuning), LePE Tuning (LePE-Tuning), Relative Position Bias Tuning (RPB-Tuning), and KAdaptation (Ours)\nacross CIFAR10 (CF10) (Krizhevsky and Hinton 2009), CIFAR100 (CF100) (Krizhevsky and Hinton 2009), SUN397\n(SUN) (Xiao et al. 2010), DTD (Cimpoi et al. 2014), STL10 (STL) (Coates, Ng, and Lee 2011), FGVCAircraft (FGVC) (Maji\net al. 2013), and FER2013 (FER) (Goodfellow et al. 2013) datasets in terms of average accuracy (%) and number of trainable\nparameters (in millions) (#Par).\nùëëùëë! = 575ùëëùëë! = 300\nDimension\nAccuracy (%)\n90% of best\nperformance\nbest\nperformance\nFigure 3: Validation Accuracy vs. Subspace Dimension d of\nMLP and the attention module for Supervised ViT on CI-\nFAR100. The local intrinsic dimension dt of the attention\nmodule is lower than that of the MLP.\nules performs worse than the original method where we ap-\nply KAdaptation to attention modules. This phenomenon\nis consistent with our findings from naive baseline experi-\nments and intrinsic dimension experiments. Second, we test\nanother variant of Adapter-tuning. Instead of inserting two\nAdapters after the attention and feedforward modules re-\nspectively following Houlsbyet al. (2019), we add Adapters\nin the attention layers. The standard Adapter-tuning out-\nperforms this variance, indicating the effectiveness of the\nvanilla Adapter-tuning when it is adapted to vision tasks us-\ning vision transformers.\nMethod Average Accuracy\nAdapters on attention layer 54.1\nStandard Adapter-tuning 87.7\nKronecker Adaptation to MLP 86.6\nKronecker Adaptation 88.1\nTable 4: Kronecker Adaptation and Adapter-tuning ablation\nexperiments with Supervised ViT on CIFAR10 (Krizhevsky\nand Hinton 2009), CIFAR100 (Krizhevsky and Hinton\n2009), and SUN397 (Xiao et al. 2010). We report the av-\nerage accuracy (%) across the three datasets.\nConclusion\nIn this paper, we conduct the first comprehensive compar-\nison of efficient model adaptation on the image classifica-\ntion tasks using vision transformers. We also propose a bet-\nter parameter-efficient model adaptation strategy in the prin-\nciple of subspace training and parameterized hypercomplex\nmultiplication, which achieves the best tradeoff between ac-\ncuracy and parameter efficiency. We release a benchmark by\nproviding the implementation of all the methods studied in\nthis paper, which could be directly used in developing fu-\nture efficient model adaptation strategies and will hopefully\nfacilitate research in this area. Looking into the future, we\nplan to explore the generalization of our method to other\ntasks, especially in the vision-and-language domain.\n823\nReferences\nAghajanyan, A.; Zettlemoyer, L.; and Gupta, S. 2020. Intrin-\nsic Dimensionality Explains the Effectiveness of Language\nModel Fine-Tuning. arXiv:2012.13255 [cs].\nBaevski, A.; and Auli, M. 2019. Adaptive Input Represen-\ntations for Neural Language Modeling. In ICLR.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213‚Äì229. Springer.\nChollet, F. 2017. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, 1251‚Äì\n1258.\nCimpoi, M.; Maji, S.; Kokkinos, I.; Mohamed, S.; ; and\nVedaldi, A. 2014. Describing Textures in the Wild. In Pro-\nceedings of the IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR).\nCoates, A.; Ng, A.; and Lee, H. 2011. An analysis of single-\nlayer networks in unsupervised feature learning. InProceed-\nings of the fourteenth international conference on artificial\nintelligence and statistics, 215‚Äì223. JMLR Workshop and\nConference Proceedings.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv preprint arXiv:1810.04805.\nDong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.;\nChen, D.; and Guo, B. 2021. Cswin transformer: A general\nvision transformer backbone with cross-shaped windows.\narXiv preprint arXiv:2107.00652.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nEdalati, A.; Tahaei, M.; Rashid, A.; Nia, V . P.; Clark, J. J.;\nand Rezagholizadeh, M. 2021. Kronecker decomposition\nfor gpt compression. arXiv preprint arXiv:2110.08152.\nGoodfellow, I. J.; Erhan, D.; Carrier, P. L.; Courville, A.;\nMirza, M.; Hamner, B.; Cukierski, W.; Tang, Y .; Thaler, D.;\nLee, D.-H.; et al. 2013. Challenges in representation learn-\ning: A report on three machine learning contests. In Inter-\nnational conference on neural information processing, 117‚Äì\n124. Springer.\nHe, J.; Zhou, C.; Ma, X.; Berg-Kirkpatrick, T.; and Neu-\nbig, G. 2021. Towards a unified view of parameter-efficient\ntransfer learning. arXiv preprint arXiv:2110.04366.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.; de\nLaroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly,\nS. 2019. Parameter-Efficient Transfer Learning for NLP.\narXiv:1902.00751 [cs, stat].\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang,\nS.; Wang, L.; and Chen, W. 2021. LoRA: Low-Rank Adap-\ntation of Large Language Models. arXiv:2106.09685 [cs].\nKingma, D.; and Ba, J. 2014. Adam: A Method for Stochas-\ntic Optimization. International Conference on Learning\nRepresentations.\nKrizhevsky, A.; and Hinton, G. 2009. Learning multiple lay-\ners of features from tiny images. Master‚Äôs thesis, Depart-\nment of Computer Science, University of Toronto.\nLe, Q. V .; Sarl ¬¥os, T.; and Smola, A. J. 2014. Fastfood:\nApproximate kernel expansions in loglinear time. arXiv\npreprint arXiv:1408.3060.\nLi, C.; Farkhoor, H.; Liu, R.; and Yosinski, J. 2018. Mea-\nsuring the Intrinsic Dimension of Objective Landscapes.\narXiv:1804.08838 [cs, stat].\nLi, C.; Liu, H.; Li, L. H.; Zhang, P.; Aneja, J.; Yang, J.;\nJin, P.; Lee, Y . J.; Hu, H.; Liu, Z.; and Gao, J. 2022a.\nELEV ATER: A Benchmark and Toolkit for Evaluating\nLanguage-Augmented Visual Models. arXiv preprint.\nLi, C.; Liu, H.; Li, L. H.; Zhang, P.; Aneja, J.; Yang,\nJ.; Jin, P.; Lee, Y . J.; Hu, H.; Liu, Z.; et al. 2022b.\nELEV ATER: A Benchmark and Toolkit for Evaluating\nLanguage-Augmented Visual Models. arXiv preprint\narXiv:2204.08790.\nLi, X. L.; and Liang, P. 2021. Prefix-Tuning: Optimiz-\ning Continuous Prompts for Generation. arXiv:2101.00190\n[cs].\nLiu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;\nLevy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V .\n2019. Roberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin Transformer: Hierarchical Vision\nTransformer Using Shifted Windows. arXiv:2103.14030\n[cs].\nMahabadi, R. K.; Henderson, J.; and Ruder, S. 2021. Com-\npacter: Efficient Low-Rank Hypercomplex Adapter Layers.\narXiv:2106.04647 [cs].\nMaji, S.; Rahtu, E.; Kannala, J.; Blaschko, M.; and Vedaldi,\nA. 2013. Fine-grained visual classification of aircraft. arXiv\npreprint arXiv:1306.5151.\nMcCloskey, M.; and Cohen, N. J. 1989. Catastrophic inter-\nference in connectionist networks: The sequential learning\nproblem. In Psychology of learning and motivation, vol-\nume 24, 109‚Äì165. Elsevier.\nPatterson, D.; Gonzalez, J.; Le, Q.; Liang, C.; Munguia, L.-\nM.; Rothchild, D.; So, D.; Texier, M.; and Dean, J. 2021.\nCarbon emissions and large neural network training. arXiv\npreprint arXiv:2104.10350.\nPfeiffer, J.; Kamath, A.; R¬®uckl¬¥e, A.; Cho, K.; and Gurevych,\nI. 2021. AdapterFusion: Non-Destructive Task Composition\nfor Transfer Learning. arXiv:2005.00247 [cs].\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. arXiv preprint arXiv:2103.00020.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language models are unsupervised mul-\ntitask learners. OpenAI Blog, 1(8): 9.\n824\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSquad: 100,000+ questions for machine comprehension of\ntext. arXiv preprint arXiv:1606.05250.\nR¬®uckl¬¥e, A.; Geigle, G.; Glockner, M.; Beck, T.; Pfeiffer, J.;\nReimers, N.; and Gurevych, I. 2021. AdapterDrop: On the\nEfficiency of Adapters in Transformers. arXiv:2010.11918\n[cs].\nRuder, S. 2016. An overview of gradient descent optimiza-\ntion algorithms. arXiv preprint arXiv:1609.04747.\nSainath, T. N.; Kingsbury, B.; Sindhwani, V .; Arisoy, E.; and\nRamabhadran, B. 2013. Low-rank matrix factorization for\ndeep neural network training with high-dimensional output\ntargets. In 2013 IEEE international conference on acoustics,\nspeech and signal processing, 6655‚Äì6659. IEEE.\nSung, Y .-L.; Cho, J.; and Bansal, M. 2022. Vl-\nadapter: Parameter-efficient transfer learning for vision-and-\nlanguage tasks. InProceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 5227‚Äì5237.\nTahaei, M. S.; Charlaix, E.; Nia, V . P.; Ghodsi, A.; and Reza-\ngholizadeh, M. 2021. Kroneckerbert: Learning kronecker\ndecomposition for pre-trained language models via knowl-\nedge distillation. arXiv preprint arXiv:2109.06243.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, ≈Å.; and Polosukhin, I. 2017. At-\ntention is all you need. In Advances in neural information\nprocessing systems, 5998‚Äì6008.\nWang, A.; Singh, A.; Michael, J.; Hill, F.; Levy, O.; and\nBowman, S. R. 2018. Glue: A multi-task benchmark and\nanalysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461.\nWen, Y .; Tran, D.; and Ba, J. 2020. Batchensemble: an alter-\nnative approach to efficient ensemble and lifelong learning.\narXiv preprint arXiv:2002.06715.\nXiao, J.; Hays, J.; Ehinger, K. A.; Oliva, A.; and Torralba,\nA. 2010. Sun database: Large-scale scene recognition from\nabbey to zoo. In 2010 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition, 3485‚Äì3492.\nIEEE.\nYang, Z.; Dai, Z.; Yang, Y .; Carbonell, J.; Salakhutdinov,\nR. R.; and Le, Q. V . 2019. Xlnet: Generalized autoregres-\nsive pretraining for language understanding. In Advances in\nneural information processing systems, 5754‚Äì5764.\nZaken, E. B.; Ravfogel, S.; and Goldberg, Y . 2021. BitFit:\nSimple Parameter-Efficient Fine-Tuning for Transformer-\nBased Masked Language-Models. arXiv:2106.10199 [cs].\nZhang, A.; Tay, Y .; Zhang, S.; Chan, A.; Luu, A. T.;\nHui, S. C.; and Fu, J. 2021. Beyond fully-connected\nlayers with quaternions: Parameterization of hypercom-\nplex multiplications with 1/n parameters. arXiv preprint\narXiv:2102.08597.\n825",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7664717435836792
    },
    {
      "name": "Benchmarking",
      "score": 0.6439946889877319
    },
    {
      "name": "Subspace topology",
      "score": 0.6074544191360474
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5880817174911499
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.5858198404312134
    },
    {
      "name": "Machine learning",
      "score": 0.5813401937484741
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5685316324234009
    },
    {
      "name": "Transformer",
      "score": 0.5213814377784729
    },
    {
      "name": "Engineering",
      "score": 0.09480366110801697
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I185103710",
      "name": "University of California, Santa Cruz",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210164937",
      "name": "Microsoft Research (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 53
}