{
  "title": "Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding",
  "url": "https://openalex.org/W4389519587",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1984332501",
      "name": "Hang Zhang",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100379612",
      "name": "Xin Li",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4367628410",
    "https://openalex.org/W4361866031",
    "https://openalex.org/W4378711593",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4323717348",
    "https://openalex.org/W3204588463",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W4319049530",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4376167553",
    "https://openalex.org/W4383987498",
    "https://openalex.org/W4375869762",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4386076522",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4287113019",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4382132560",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4322718246",
    "https://openalex.org/W4402671548",
    "https://openalex.org/W4380559123",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4367061106",
    "https://openalex.org/W4389524500",
    "https://openalex.org/W4389524372",
    "https://openalex.org/W4384112212"
  ],
  "abstract": "We present Video-LLaMA, a multi-modal framework that empowers Large Language Models (LLMs) with the capability of understanding both visual and auditory content in the video. Video-LLaMA bootstraps cross-modal training from the frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous works that complement LLMs to process the visual or audio signals only, Video-LLaMA enables video comprehension by tackling two challenges: (1) capturing the temporal changes in visual scenes, (2) integrating audio-visual signals. To counter the first challenge, we propose a Video Q-former to assemble a pre-trained image encoder into our video encoder and introduce a video-to-text generation task to learn video-language correspondence. For the second challenge, we leverage ImageBind, a universal embedding model aligning multiple modalities, as the pre-trained audio encoder and introduce an Audio Q-former on top of ImageBind to learn reasonable auditory query embeddings for the LLM module. To align the output of both visual & audio encoders with LLM’s embedding space, we first train Video-LLaMA on massive video/image-caption pairs and then tune our model with visual-instruction datasets of moderate amount but higher quality. We found Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 543–553\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nVideo-LLaMA\nAn Instruction-tuned Audio-Visual Language Model for Video\nUnderstanding\nHang Zhang1 2 Xin Li1 2∗ Lidong Bing1 2\n1 DAMO Academy, Alibaba Group\n2 Hupan Lab, 310023, Hangzhou, China\n{zh401075, xinting.lx, l.bing}@alibaba-inc.com\nAbstract\nWe present Video-LLaMA 1 a multi-modal\nframework that empowers Large Language\nModels (LLMs) with the capability of under-\nstanding both visual and auditory content in the\nvideo. Video-LLaMA bootstraps cross-modal\ntraining from the frozen pre-trained visual &\naudio encoders and the frozen LLMs. Unlike\nprevious works that complement LLMs to pro-\ncess the visual or audio signals only (Zhu et al.,\n2023; Liu et al., 2023; Huang et al., 2023a),\nVideo-LLaMA enables video comprehension\nby tackling two challenges: (1) capturing the\ntemporal changes in visual scenes, (2) integrat-\ning audio-visual signals. To counter the first\nchallenge, we propose a Video Q-former to as-\nsemble a pre-trained image encoder into our\nvideo encoder and introduce a video-to-text\ngeneration task to learn video-language cor-\nrespondence. For the second challenge, we\nleverage ImageBind (Girdhar et al., 2023), a\nuniversal embedding model aligning multiple\nmodalities, as the pre-trained audio encoder\nand introduce an Audio Q-former on top of\nImageBind to learn reasonable auditory query\nembeddings for the LLM module. To align the\noutput of both visual & audio encoders with\nLLM’s embedding space, we first train Video-\nLLaMA on massive video/image-caption pairs\nand then tune our model with visual-instruction\ndatasets of moderate amount but higher qual-\nity. We found Video-LLaMA shows the ability\nto perceive and comprehend video content and\ngenerate meaningful responses grounded in the\nvisual and auditory information presented in\nthe videos.\n1 Introduction\nLarge Language Models (LLMs) (Chowdhery et al.,\n2022; Bai et al., 2022; OpenAI, 2023) have demon-\nstrated remarkable capability of understanding and\n∗Xin Li is the corresponding author.\n1The video demonstration is available at https://youtu.\nbe/RDNYs3Rswhc\nfollowing user intentions and instructions234 . Typ-\nically, the user requests and the corresponding re-\nsponses from LLMs are all in texts, however, text-\nonly human-computer interaction is not sufficient\nfor many application scenarios because real-world\ninformation is usually multi-modal. In order to\nfurther explore the potential of LLMs, many re-\nsearchers attempt to endow LLMs with the capabil-\nity of understanding multi-modal content (Huang\net al., 2023a; Zhang et al., 2023b; Yin et al., 2023).\nAmong these efforts, Alayrac et al. (2022b);\nWang et al. (2022); Huang et al. (2023b); Xu et al.\n(2023b); Zhang et al. (2023b); Sun et al. (2023) pre-\ntrain multi-modal LLMs with massive interleaved\nimage-text data or speech-text data to accommo-\ndate multi-modal input. Meanwhile, another group\nof works adopts a more parameter-efficient way by\ncomplementing LLMs with off-the-shelf vision or\nspeech foundation models to achieve multi-modal\nunderstanding (Li et al., 2023b; Zhu et al., 2023;\nLiu et al., 2023; Ye et al., 2023; Zhang et al., 2023a;\nHuang et al., 2023a; Wu et al., 2023b; Su et al.,\n2023; Li et al., 2023a).\nDespite their effectiveness, these approaches are\ndedicated to aligning the input from exactly one\nadditional modality with text (i.e., image or au-\ndio), which is unsatisfactory for video understand-\ning. Concretely, empowering LLMs to understand\nvideo requires comprehensive processing for dif-\nferent modalities including visual input, auditory\ninput, and textual output, which is more challeng-\ning than image-only understanding and audio-only\nunderstanding tasks. Although there are several\nrecent works attempt to unleash the video under-\nstanding capability of LLMs (Li et al., 2023c; Maaz\net al., 2023; Luo et al., 2023), their primary objec-\ntive is to comprehend only the visual content of the\nvideo, with the auditory content remaining unused.\n2https://chat.openai.com/chat\n3https://www.anthropic.com/product\n4https://bard.google.com/\n543\nModel Name AbilityStatic Image Silent Video Audio\nBLIP2 (Li et al., 2023b) /enc-33\nMiniGPT4 (Zhu et al., 2023)/enc-33\nLLaV A (Liu et al., 2023) /enc-33\nmPLUG-Owl (Ye et al., 2023)/enc-33 /enc-33\nVideoChat (Li et al., 2023c)/enc-33 /enc-33\nAudioGPT (Huang et al., 2023a) /enc-33\nVideo-ChatGPT (Maaz et al., 2023)/enc-33 /enc-33\nVideo-LLaMA /enc-33 /enc-33 /enc-33\nTable 1: Comparison with popular multi-modal large\nlanguage models. Video-LLaMA has the unique ability\nto comprehend auditory and visual information simulta-\nneously.\nIn this work, to fill in the blank of audio-visual\nLLMs, we investigate the possibility of building\nmulti-modal LLMs that support the input of video\nand allow users to chat with computers around\nthe user-uploaded video, which is usually com-\nposed of multiple video frames and audio. Instead\nof employing external perception models to con-\nvert visual/auditory signals to textual signals (Shen\net al., 2023; Li et al., 2023c), we choose to build\nan end-to-end model that can handle the data from\nmultiple modalities within one single framework.\nSpecifically, we adopt the idea of BLIP-2 (Li et al.,\n2023b) to guarantee the efficiency of cross-modal\npre-training. To explicitly capture the change of\nvisual scenes in the video, we use a pre-trained\nvisual encoder to separately compute frame repre-\nsentations. Then, we introduce a frame embedding\nlayer to inject temporal information and a video\nQ-Former to generate visual query tokens. As for\nthe audio signals from the video, we additionally\nleverage a pre-trained audio encoder as well as an\naudio Q-former to learn reasonable auditory query\nembeddings (see the right part of Figure 1).\nTo align textual output with video, we devise\nmulti-branch cross-modal pre-training to learn the\nvision-language correspondence and the audio-\nlanguage correspondence. For vision-language cor-\nrespondence, we first pre-train the vision-related\ncomponents on a large-scale video caption dataset\nwith a video-clips-to-text generation task. To\nenhance the understanding of static visual con-\ncepts, we also add image-caption data into this\npre-training stage. Then, we further fine-tune these\ncomponents on a video-based conversation dataset\nto execute visual instruction tuning. For the align-\nment between the audio encoder and language de-\ncoder, we further pre-train the audio-related com-\nponents on an audio caption dataset with an audio-\nto-text generation task. For the audio-language\ncorrespondence, we leverage Imagebind (Girdhar\net al., 2023) as an encoder, which performs excep-\ntionally well in aligning different modalities to a\ncommon embedding space. Given the limited avail-\nability of audio-text data, we also utilize vision-text\ndata to train the audio-related components. These\ncomponents learn to align the common embedding\nspace provided by Imagebind with the embedding\nspace of LLMs. Despite not being explicitly trained\nwith audio-text data, Video-LLaMA exhibits a re-\nmarkable zero-shot audio understanding capability\nduring inference.\nAs shown in Table 1, our Video-LLaMA stands\nout from other existing multi-modal LLMs in terms\nof its distinctively comprehensive comprehension\nof audiovisual modal information in videos. In\nsummary, our contributions are as follows:\n•We propose Video-LLaMA, a multi-modal\nframework that enables LLM to simultaneously\nprocess both the visual and auditory content of a\ngiven video and engage in conversation with hu-\nmans.\n•To empower LLMs with video understanding\ncapability, we propose a multi-branch cross-modal\npre-training framework to achieve both vision-\nlanguage alignment and audio-language alignment.\n•We open-source the entire codebase for pre-\ntraining and fine-tuning as well as the model\nweights of all the variants of Video-LLaMA5. We\nalso prepared the demos for video-grounded con-\nversation67.\n2 Method\nVideo-LLaMA aims to empower frozen LLMs with\nthe capability of understanding both visual and au-\nditory content in videos. As shown in Figure 1,\nwe design two branches, namely Vision-Language\nBranch and Audio-Language Branch, to respec-\ntively transform the video frames and audio signals\ninto query representations that are compatible with\nthe textual inputs of LLMs. In this section, we first\nintroduce the overall architecture and the building\nblocks of each branch. Then, we delineate the pro-\ncedures of the proposed multi-branch cross-modal\npre-training and audio-visual instruction tuning.\n5https://github.com/DAMO-NLP-SG/Video-LLaMA\n6https://huggingface.co/spaces/DAMO-NLP-SG/\nVideo-LLaMA\n7https://modelscope.cn/studios/damo/\nvideo-llama/summary\n544\nVisual Encoder \n(ViT & Q-Former)\nVideo Q-Former\nAudio Encoder \nAudio Q-Former\nLinear Linear\nLLM (Vicuna/LLaMA)\nDescribe this video:# Human:\nThis video is an animation of a rocket\nlaunching from a launch pad at night...\nVideo frames\n Audio signals\nVision-Language Branch\n Audio-Language Branch\nFigure 1: Overall architecture of Video-LLaMA.\n2.1 Architecture\n2.1.1 Vision-Language Branch\nThe Vision-Language Branch is designed for en-\nabling the LLMs to understand visual inputs. As\nshown in the left part of Figure 1, it is composed\nof a frozen pre-trained image encoder to extract\nfeatures from video frames, a position embedding\nlayer to inject temporal information into video\nframes, a video Q-former to aggregate frame-level\nrepresentations and a linear layer to project the\noutput video representations into the same dimen-\nsion as the text embeddings of LLMs. Given one\nvideo consists of N frames, the image encoder will\nfirst map each frame/image into Kf image embed-\nding vectors, yielding video frame representations\nV = [v1, v2, ...,vN ] where vi ∈RKf ×df is the\nset of df -dimensional image embeddings corre-\nsponding to the i-th frame.\nSince the frame representations vi from the\nfrozen image encoder are computed without consid-\nering any temporal information, we further apply\nposition embeddings as the indicator of temporal\ninformation to the representations from different\nframes. Then, we feed the position-encoded frame\nrepresentations to Video Q-former, which shares\nthe same architecture with Query Transformer (Q-\nFormer) in BLIP-2 (Li et al., 2023b), to obtain kV\nvideo embedding vectors of dimension dv as the\nrepresentation ˆv ∈RkV ×dv of the video.\nTo adapt the video representations to the input of\nLLMs, we add a linear layer to transform the video\nembedding vectors into the video query vectors.\nThe video query vectors are of the same dimension\nas the text embeddings of LLMs. In the forward\npass, they will be concatenated to text embeddings\nas a video soft promptand guide the frozen LLMs\n545\nto generate text conditioned on video content.\nAs for the implementation of the Vision-\nLanguage Branch, we utilize the pre-trained vi-\nsion component of BLIP-2 (Li et al., 2023b) as\nthe frozen visual encoder, which includes a ViT-\nG/14 from EV A-CLIP (Fang et al., 2022) and a\npre-trained Q-former. The remaining components,\nincluding the position embedding layer, Video Q-\nformer, and Linear layer are randomly initialized\nand optimized to well connect the output of the\nfrozen visual encoder to frozen LLMs.\n2.1.2 Audio-Language Branch\nTo deal with the auditory content of the given video,\nwe introduce the Audio-Language Branch. Con-\ncretely, it consists of a pre-trained audio encoder\nto compute features given a short segment of ori-\ngin audio, a position embedding layer to inject\ntemporal information to audio segments, an audio\nQ-former to fuse the features of different audio\nsegments, and a linear layer to map the audio rep-\nresentation into the embedding space of LLMs.\nIn practice, we utilize the pre-trained Image-\nbind (Girdhar et al., 2023) as the audio encoder.\nWe first uniformly sampleM segments of 2-second\nshort audio clips from the video, then convert each\n2-second audio clip into spectrograms using 128\nmel-spectrogram bins. After obtaining the spec-\ntrogram list of input audio, the audio encoder will\nmap each spectrogram into a dense vector. So the\ngenerated audio representation of the given video\ncan be denoted as A = [a1, a2, ..., aM ].\nSimilar to Video Q-Former, the Audio Q-former\ninjects temporal information by adding learnable\npositional embeddings to audio segments. It then\ngenerates fixed-length audio features by computing\nthe interaction across the position-encoded audio\nsegments. Audio Q-Former adopts the same archi-\ntecture as Q-Former. It projects the variable-length\naudio representation list A into a fixed-length se-\nquence ˆA ∈RKa×da, where the Ka is the number\nof audio embedding vectors and da is the dimen-\nsion of each vector. Finally, we employ a linear\nlayer to map audio features to the embedding space\nof the LLM.\n2.2 Multi-branch Cross-Modal Training\nWe train the vision-language and audio-language\nbranches separately. In the first stage, large-\nscale vision-caption datasets are used for training,\nand in the second stage, high-quality instruction-\nfollowing datasets were used for fine-tuning. The\nimage is treated as a one-frame video.\n2.2.1 Training of Vision-Language Branch\nFor pre-training vision-language branch, we uti-\nlized Webvid-2M (Bain et al., 2021), a large-scale\ndataset of short videos with textual descriptions\nsourced from stock footage sites. Moreover, we em-\nployed the image caption dataset CC595k, which\nis sourced from CC3M (Sharma et al., 2018) and\nfiltered by Liu et al. (2023). We adopt a video-to-\ntext generation task during the pre-training stage,\ni.e., given the representation of a video, prompting\nthe frozen LLM to generate the corresponding text\ndescription. We find that a significant portion of\ntextual descriptions are insufficient to reflect the en-\ntire content of the videos. Therefore, the visual se-\nmantics in the videos are not fully aligned with the\ntextual semantics in the video descriptions. Never-\ntheless, this stage aimed to utilize a vast amount of\ndata and enable video features to contain as much\nvisual knowledge as possible. We left the abilities\nof vision-text alignment and instruction-following\nfor the next stage.\nAfter the pre-training stage, the model can gen-\nerate content about information in the video, but its\nability to follow instructions has decreased. There-\nfore, in the second stage, we fine-tuned the model\nusing high-quality instruction data. We integrated\nthe image-detail-description dataset from MiniGPT-\n4 (Zhu et al., 2023), the image-instruction dataset\nfrom LLaV A (Liu et al., 2023), and the video-\ninstruction dataset from Video-Chat (Li et al.,\n2023c). After fine-tuning, Video-LLaMA exhibited\nremarkable abilities in following instructions and\ncomprehending images and videos.\n2.2.2 Training of Audio-Language Branch\nTraining the audio-language branch directly using\naudio-text data is highly challenging due to the\nrarity of such data. The objective of the learn-\nable parameters in the audio-language branch is\nto align the output embedding of the frozen au-\ndio encoder with the embedding space of LLM.\nGiven the scarcity of audio-text data, we employ a\nworkaround strategy to achieve this objective. Im-\nageBind, which is used as our audio encoder, has a\nremarkable ability to align different modalities’ em-\nbeddings to one common space, demonstrating im-\npressive performance on cross-modal retrieval and\ngeneration tasks. In light of the scarcity of audio-\ntext data and the abundance of visual-text data, we\ntrain the audio-language branch using visual-text\n546\ndata, following the same data and process as the vi-\nsion branch. Thanks to the shared embedding space\nprovided by ImageBind, Video-LLaMA exhibits\nthe ability to comprehend audio during inference,\neven though the audio interface has never been\ntrained on audio data.\n3 Related Works\nLarge Language Models: Large language mod-\nels (LLMs) (Black et al., 2022; Scao et al., 2022;\nOpenAI, 2023; Tsimpoukelli et al., 2021) have\ndemonstrated remarkable language understanding\nand reasoning abilities, enabling the generation of\nhigh-quality natural language text across various\ndomains, including articles, conversations, stories,\nand poetry. LLMs have already sparked a techno-\nlogical revolution and have been widely applied\nin different applications. Moreover, a series of\nopen source large models, such as LLaMA (Tou-\nvron et al., 2023), BLOOM (Scao et al., 2022) and\nOPT (Zhang et al., 2022), have greatly promoted\ntechnological advancement and made outstanding\ncontributions to the NLP community. Building\nupon these LLMs, researchers have further ex-\ntended their capabilities and developed excellent\nmodels for various NLP tasks. Examples include\nVicuna (Chiang et al., 2023) and Baize (Xu et al.,\n2023a). Our work is based on these LLMs and\nprovides plug-and-play plugins that empower them\nwith the capability of comprehending both visual\nand auditory content in videos.\nMulti-modal Large Language Models: Re-\nsearchers have been actively exploring the use\nof LLMs for processing multi-modal inputs (Gao\net al., 2023; Li et al., 2023c). Existing approaches\ncan be categorized into two main groups. The\nfirst category involves employing LLMs as con-\ntrollers and utilizing existing multi-modal models\nas tools. In this approach, when receiving the user’s\ntext instruction, the LLM recognizes the user’s in-\ntention and makes decisions about which tools to\ncall. It then generates comprehensive responses by\nincorporating the results obtained from these off-\nthe-shelf multi-modal models. Examples include\nChatGPT (Wu et al., 2023a), HuggingGPT (Shen\net al., 2023), and AudioGPT (Huang et al., 2023a).\nThe second category focuses on training funda-\nmental large-scale multi-modal models. The key\nidea of this line of work is to align the pre-trained\nfoundation models for other modalities to textual\nLLMs. For instance, Flamingo (Alayrac et al.,\n2022a) utilizes a perceiver resampler and a gated\ncross-attention layer to connect a frozen image en-\ncoder and LLM. BLIP2 (Li et al., 2023b) intro-\nduces a Q-Former to map learned image queries\nto the textual embedding space of LLMs. (Liu\net al., 2023), mPLUG-owl (Ye et al., 2023) and\nMiniGPT4 (Zhu et al., 2023) develop instruction-\nfollowing image-LLMs using image-instruction-\nfollowing dataset. Video-Chat (Li et al., 2023c)\nand Video-ChatGPT (Maaz et al., 2023) extend im-\nage encoders to video encoders and connect them\nwith LLMs to understand visual content in videos.\nPandaGPT (Su et al., 2023) utilizes multi-modal\nencoders from ImageBind, trained exclusively on\nimage-instruction pairs, to enable large models to\nunderstand six modalities. Our work falls into the\nsecond category, where we train fundamental mod-\nels to comprehend both the visual and auditory\ncontent in videos.\n4 Examples\nIn this section, we show some cases to demonstrate\nVideo-LLaMA’s multi-modal instruction-following\ncapability in video/audio/image-grounded conver-\nsations. Figure 2 shows some of these examples,\nwhile additional cases can be found in Appendix A.\n(1) Audio-visual integration perception ability.\nFigure 2(a) and Figure 3 show Video-LLaMA’s\nunique ability to comprehend auditory and visual\ninformation simultaneously. The videos in both\ncases contain audio. In each conversation, we pose\ntwo questions related to visual and auditory content\nrespectively. If the model could only receive one\nmodal, it would be unable to answer both of these\nquestions. However, we can observe that Video-\nLLaMA accurately responds to both visual and\nauditory questions in both cases.\n(2) The ability to capture temporal dynamics\nin videos. Figure 2(b) and Figure 4 illustrate the\ncapability of Video-LLaMA to identify actions over\ntime. It successfully describes the actions of the\ngirl and the moving direction of the boat.\n(3) The ability to perceive and understand static\nimages. Figure 2(c) and Figure 5 show Video-\nLLaMA’s ability to perceive and understand pic-\ntures. Figure 2(c) demonstrates Video-LLaMA’s\nability to understand the concept of \"unusual\" and\nspecifically describe the unusual scene. In Figure 5,\nnot only does Video-LLaMA accurately describe\n547\nDescribe what you hear?\nI hear sounds of footsteps on the floor in the background, \nand there is also a dog barking in the house.\n… …\nDoes the man wear glasses?\nYes, he wears a pair of glasses in the video.\n(a) A case that Video-LLaMA answers the questions based\non the background sound and visual content of the video\nSummarize this video in one sentence.\nThe video shows a beautiful scenery of a cherry blossom-\nlined river flowing by a boat on the water, and a cityscape \nwith tall buildings in the background.\n… …\nWhat direction is the ship going\nThe ship is going towards the right side of the video.\n(b) A case showing Video-LLaMA’s ability to identify ac-\ntions over time.\nWhat is unusual about this image?\nUnusual about this image is that a man in a yellow shirt is \nstanding on top of a parked car while holding an ironing board \nand pressing clothes\n(c) A case demonstrating Video-LLaMA’s ability to com-\nprehend static images.\nDo you know the building in this picture?\nThe building in the image appears to be the United States \nCapitol. It is a neoclassical style building located in Washington, \nD.C., the capital of the United States. It serves as the home of \nthe U.S. Congress and is one of the most iconic landmarks in \nthe country\n(d) A case demonstrating Video-LLaMA’s ability to recog-\nnize famous landmarks.\nFigure 2: Some examples generated by Video-LLaMA.\nthe main content, but it also associates it with the\nfriendly interaction between a dog and a human.\n(4) The ability of common-knowledge concept\nrecognition. Figure 2(d) and Figure 6 demon-\nstrate Video-LLaMA’s remarkable capacity for rec-\nognizing common-knowledge concepts in visual\nsignals. Video-LLaMA successfully recognizes fa-\nmous landmarks and characters and can engage in\ncommon-sense question-answering.\n5 Conclusion\nIn this paper, we present Video-LLaMA, a cutting-\nedge multi-modal framework that empowers large\nlanguage models with both audio & video under-\nstanding capabilities. Our experiments demon-\nstrated the impressive abilities of Video-LLaMA\nin audio and video-grounded conversations, high-\nlighting its potential as a promising prototype for\naudio-visual AI assistants. We have open-sourced\nthe entire training code and various model weights,\nalong with detailed instructions to assist developers\nin utilizing our code for further development. In ad-\ndition, we have provided online demo websites and\noffline demo deployment guides for users to experi-\nence Video-LLaMA’s capabilities firsthand. We are\ncommitted to constantly maintaining and improv-\ning Video-LLaMA, and will continue to contribute\nto the open-source community.\n6 Limitations\nAlthough Video-LLaMA has demonstrated impres-\nsive abilities in understanding both visual and au-\nditory content in videos, it is still an early-stage\nprototype and has some limitations, including: (1)\nLimited perception capacities: Video-LLaMA’s\nperformance is hindered by the quality and scale\nof the current training dataset. We are actively con-\nstructing a high-quality audio-video-text alignment\ndataset to enhance the model’s perception capa-\nbilities. (2) Limited ability to handle long videos.\nLong videos(such as movies, and TV shows) con-\ntain a large volume of information and impose\nhigher demands on computational resources. This\nchallenge remains a crucial issue that the research\ncommunity is actively working to address. (3) Hal-\nlucination. Video-LLaMA inherits the hallucina-\ntion problem from the frozen LLMs. We will con-\ntinue to address these challenges and develop more\npowerful versions for video understanding.\n548\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022a. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716–23736.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\nRoman Ring, Eliza Rutherford, Serkan Cabi, Tengda\nHan, Zhitao Gong, Sina Samangooei, Marianne\nMonteiro, Jacob Menick, Sebastian Borgeaud, Andy\nBrock, Aida Nematzadeh, Sahand Sharifzadeh, Miko-\nlaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Karen Simonyan. 2022b.\nFlamingo: a visual language model for few-shot\nlearning. arXiv preprint arXiv:2204.14198.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini,\nCameron McKinnon, et al. 2022. Constitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nMax Bain, Arsha Nagrani, Gül Varol, and Andrew Zis-\nserman. 2021. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In IEEE In-\nternational Conference on Computer Vision.\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell\nWu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. 2022. Eva: Exploring the limits of\nmasked visual representation learning at scale. arXiv\npreprint arXiv:2211.07636.\nPeng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, W. Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, Hongsheng Li, and Yu Jiao Qiao.\n2023. Llama-adapter v2: Parameter-efficient visual\ninstruction model. arXiv preprint arXiv:2304.15010.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-\nnat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. 2023. Imagebind: One embed-\nding space to bind them all. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 15180–15190.\nRongjie Huang, Mingze Li, Dongchao Yang, Jia-\ntong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, et al.\n2023a. Audiogpt: Understanding and generating\nspeech, music, sound, and talking head. arXiv\npreprint arXiv:2304.12995.\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei\nCui, Owais Khan Mohammed, Qiang Liu, Kriti Ag-\ngarwal, Zewen Chi, Johan Bjorck, Vishrav Chaud-\nhary, Subhojit Som, Xia Song, and Furu Wei.\n2023b. Language is not all you need: Aligning\nperception with language models. arXiv preprint\narXiv:2302.14045.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. 2023a. Otter: A\nmulti-modal model with in-context instruction tuning.\narXiv preprint arXiv:2305.03726.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023b. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. arXiv preprint arXiv:2301.12597.\nKunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wen\nWang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. 2023c. Videochat: Chat-centric video un-\nderstanding. arXiv preprint arXiv:2305.06355.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nRuipu Luo, Ziwang Zhao, Min Yang, Junwei Dong,\nMing-Hui Qiu, Pengcheng Lu, Tao Wang, and\nZhongyu Wei. 2023. Valley: Video assistant with\nlarge language model enhanced ability. arXiv\npreprint arXiv:2306.07207.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan,\nand Fahad Shahbaz Khan. 2023. Video-chatgpt:\nTowards detailed video understanding via large\nvision and language models. arXiv preprint\narXiv:2306.05424.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of the 56th Annual\n549\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2556–2565.\nAssociation for Computational Linguistics.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nYixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan\nWang, and Deng Cai. 2023. Pandagpt: One\nmodel to instruction-follow them all. arXiv preprint\narXiv:2305.16355.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,\nXiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang.\n2023. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-\ntimodal few-shot learning with frozen language mod-\nels. Advances in Neural Information Processing Sys-\ntems, 34:200–212.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai,\nZhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou,\nand Hongxia Yang. 2022. Unifying architectures,\ntasks, and modalities through a simple sequence-to-\nsequence learning framework. In International Con-\nference on Machine Learning.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023a.\nVisual chatgpt: Talking, drawing and editing\nwith visual foundation models. arXiv preprint\narXiv:2303.04671.\nJian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yilun\nZhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren,\nLinquan Liu, and Yu Wu. 2023b. On decoder-only\narchitecture for speech-to-text and large language\nmodel integration. arXiv preprint arXiv:2307.03917.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley.\n2023a. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv\npreprint arXiv:2304.01196.\nHaiyang Xu, Qinghao Ye, Mingshi Yan, Yaya Shi, Ji-\nabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi-\nuchen Qian, Wei Wang, Guohai Xu, Ji Zhang, Song-\nfang Huang, Feiran Huang, and Jingren Zhou. 2023b.\nmplug-2: A modularized multi-modal foundation\nmodel across text, image and video. arXiv preprint\narXiv:2302.00402.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming\nYan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng\nShi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong\nChen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and\nFeiyan Huang. 2023. mplug-owl: Modularization\nempowers large language models with multimodality.\narXiv preprint arXiv:2304.14178.\nShukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing\nSun, Tong Xu, and Enhong Chen. 2023. A survey on\nmultimodal large language models. arXiv preprint\narXiv:2306.13549.\nAo Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan\nLiu, and Tat-Seng Chua. 2023a. Transfer vi-\nsual prompt generator across llms. arXiv preprint\narXiv:23045.01278.\nDong Zhang, Shimin Li, Xin Zhang, Jun Zhan,\nPeng Peng Wang, Yaqian Zhou, and Xipeng Qiu.\n2023b. Speechgpt: Empowering large language mod-\nels with intrinsic cross-modal conversational abilities.\narXiv preprint arXiv:2305.11000.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\nA Appendix\n550\nHow was the audience's response?\nThe audience was really enthusiastic and clapped loudly for \nthe performer.\n… …\nWhat is this man doing?\nHe is playing a saxophone on stage at the concert.\nFigure 3: A case showing Video-LLaMA’s ability to identify the sound of applause in a video and infer the positive\nresponse from the audience. Additionally, it infers that a man is playing the saxophone on stage based on the visual\ncontent.\nDescribe this video.\nThe video shows a young red-haired woman, who is \nwearing a striped shirt, posing for the camera in front of a \ngray background. She is holding her fingers up to her mouth, \nas if she is trying to be quiet, and the caption mentions her \nlong hair and red lips. The woman is young, with fair skin, \nand looks calm and confident. She is holding her fingers to \nher mouth, which suggests she might be trying to signal \nsomething to the camera or trying to convey a message.\n… …\nFigure 4: A case where Video-LLaMA provides a detailed description of the visual content in a dynamic video.\n551\nDescribe the image in details.\nThe image features a beautiful and playful golden retriever \nrunning on a bright green lawn. The dog is running with a \ntennis ball in its mouth, as it is an active and lively animal \nthat enjoys playing and having fun. The green grass in the \nbackground creates a vibrant contrast with the golden color \nof the dog's coat, adding a touch of color and texture to the \nscene. The dog's fur is well-groomed, shiny, and nicely \ngroomed, highlighting its health and appearance. The \nbrightness of the green lawn and the playful behavior of the \ndog suggest a joyful atmosphere, inviting viewers to engage \nin this lovely scenery and the dog's energetic play.\nIt's worth mentioning that the dog's focus and dedication to \nplaying with the ball also showcase its natural instincts and \nits hunting abilities. Additionally, this image captures a fun \nand heartwarming moment of bonding between the dog and \nits human companion, as the dog's positive attitude towards \nthe ball reflects its enjoyment of interacting with its owner \nand its environment. Overall, the image conveys an inviting \nand joyful atmosphere.\nFigure 5: A case where Video-LLaMA provides a detailed description of the static image content.\n552\nDo you know who the man on the right is?\nThe man on the right is Jon Snow, played by Kit Harington in the \ntelevision series Game of Thrones. \nWho is the lady on the left?\nThe lady on the left is likely Daenerys Targaryen, played by Emilia \nClarke in the television series Game of Thrones. \nWhat is the relationship between them? \nJon Snow and Daenerys Targaryen are both characters in the \ntelevision series Game of Thrones. In the show, they have a \nromantic relationship. They first meet in the seventh season, and \ntheir attraction to each other grows over the course of the eighth \nseason. Despite their initial attraction, they have a complicated \nrelationship due to their respective backgrounds and the \nchallenges they face in the series\nFigure 6: A case showing Video-LLaMA’s ability to recognize renowned characters and participate in video-\ngrounded question answering.\n553",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7999204397201538
    },
    {
      "name": "Encoder",
      "score": 0.663297176361084
    },
    {
      "name": "Audio visual",
      "score": 0.5776747465133667
    },
    {
      "name": "Embedding",
      "score": 0.48124977946281433
    },
    {
      "name": "Multimedia",
      "score": 0.4802404046058655
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.47243839502334595
    },
    {
      "name": "Video processing",
      "score": 0.4454020857810974
    },
    {
      "name": "Visualization",
      "score": 0.44524332880973816
    },
    {
      "name": "Speech recognition",
      "score": 0.4408799111843109
    },
    {
      "name": "Video quality",
      "score": 0.4333970844745636
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4155486226081848
    },
    {
      "name": "Computer vision",
      "score": 0.4053802490234375
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Metric (unit)",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    }
  ],
  "cited_by": 318
}