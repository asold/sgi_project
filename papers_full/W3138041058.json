{
  "title": "DanceNet3D: Music Based Dance Generation with Parametric Motion Transformer.",
  "url": "https://openalex.org/W3138041058",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2902235306",
      "name": "Buyu Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143124792",
      "name": "Yongchi Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101699347",
      "name": "Lu Sheng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2983796203",
    "https://openalex.org/W3068510429",
    "https://openalex.org/W2989607414",
    "https://openalex.org/W3010876998",
    "https://openalex.org/W2753432109",
    "https://openalex.org/W2899129842",
    "https://openalex.org/W1527575096",
    "https://openalex.org/W2136625416",
    "https://openalex.org/W3123650687",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3005171070",
    "https://openalex.org/W2971856312",
    "https://openalex.org/W2897492344",
    "https://openalex.org/W2982573856",
    "https://openalex.org/W2315431645",
    "https://openalex.org/W2998680134",
    "https://openalex.org/W2101032778",
    "https://openalex.org/W15066456",
    "https://openalex.org/W2052872069",
    "https://openalex.org/W2121378366"
  ],
  "abstract": "In this work, we propose a novel deep learning framework that can generate a vivid dance from a whole piece of music. In contrast to previous works that define the problem as generation of frames of motion state parameters, we formulate the task as a prediction of motion curves between key poses, which is inspired by the animation industry practice. The proposed framework, named DanceNet3D, first generates key poses on beats of the given music and then predicts the in-between motion curves. DanceNet3D adopts the encoder-decoder architecture and the adversarial schemes for training. The decoders in DanceNet3D are constructed on MoTrans, a transformer tailored for motion generation. In MoTrans we introduce the kinematic correlation by the Kinematic Chain Networks, and we also propose the Learned Local Attention module to take the temporal local correlation of human motion into consideration. Furthermore, we propose PhantomDance, the first large-scale dance dataset produced by professional animatiors, with accurate synchronization with music. Extensive experiments demonstrate that the proposed approach can generate fluent, elegant, performative and beat-synchronized 3D dances, which significantly surpasses previous works quantitatively and qualitatively.",
  "full_text": "DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion\nTransformer\nBuyu Li,1 Yongchi Zhao,1 Zhelun Shi,2 Lu Sheng2*\n1 Huiye Technology, 2 College of Software, Beihang University\n{libuyu, zhaoyongchi}@huiye.tech, {18373044, lsheng}@buaa.edu.cn\nAbstract\nGenerating 3D dances from music is an emerged research\ntask that benefits a lot of applications in vision and graphics.\nPrevious works treat this task as sequence generation, how-\never, it is challenging to render a music-aligned long-term se-\nquence with high kinematic complexity and coherent move-\nments. In this paper, we reformulate it by a two-stage process,\ni.e., a key pose generation and then an in-between parametric\nmotion curve prediction, where the key poses are easier to be\nsynchronized with the music beats and the parametric curves\ncan be efficiently regressed to render fluent rhythm-aligned\nmovements. We named the proposed method as Dance-\nFormer, which includes two cascading kinematics-enhanced\ntransformer-guided networks (called DanTrans) that tackle\neach stage, respectively. Furthermore, we propose a large-\nscale music conditioned 3D dance dataset, called Phantom-\nDance, that is accurately labeled by experienced animators\nrather than reconstruction or motion capture. This dataset\nalso encodes dances as key poses and parametric motion\ncurves apart from pose sequences, thus benefiting the train-\ning of our DanceFormer. Extensive experiments demonstrate\nthat the proposed method, even trained by existing datasets,\ncan generate fluent, performative, and music-matched 3D\ndances that surpass previous works quantitatively and qual-\nitatively. Moreover, the proposed DanceFormer, together\nwith the PhantomDance dataset (https://github.com/libuyu/\nPhantomDanceDataset), are seamlessly compatible with in-\ndustrial animation software, thus facilitating the adaptation\nfor various downstream applications.\n1 Introduction\nAutomatically generating music conditioned 3D dances is\nan appealing but challenging task emerged in the research\ncommunity of vision and graphics, which can significantly\nbenefit various downstream applications in AR/VR, games,\nfilms and even the social networks. Beyond human actions\nlike walking, jumping and sitting with atomic functionali-\nties, dancing is a type of artistic performances that focuses\nmore on its choreography, i.e., sequential steps and move-\nments with high kinematic complexity that are synchronized\nwith the beats and rhythms of the music. It is challenging\nfor humans with professional training to generate expressive\n*Lu Sheng is the corresponding author.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nchoreographies, no matter how hard a machine is to generate\nvisually plausible dances accompanied with the music.\nMost of the prior works (Lee et al. 2019; Li et al. 2021;\nLee, Kim, and Lee 2018; Tang, Jia, and Mao 2018; Li et al.\n2020; Zhuang et al. 2020; Alemi, Franc ¸oise, and Pasquier\n2017) formulated the music conditioned 3D dance genera-\ntion as a sequence generation problem, where each frame\nin the sequence describes the human pose by the joint-level\nrotations and translations. However, rendering a long-term\npose sequence with high kinematic complexity and coherent\nmovements is still an open question.\nIn this paper, we would like to exploit a popular anima-\ntion strategy (Lasseter 1987; Thomas, Johnston, and Thomas\n1995) in the field of computer graphics, where the motion\nof characters is efficiently rendered by interpolating poses\nin keyframes through parametric curves. This coarse-to-fine\nstrategy is especially beneficial in our task since it both en-\nsures kinematic complexity and motion conherency in the\ngenerated dances, where the poses in the keyframes can be\ngenerated and synchronized with the music beats due to their\nco-occurrence (Lee et al. 2019; Li et al. 2021), and the pa-\nrameteric curves can be efficiently regressed so as to produce\ndiverse motion patterns that are consistent with the rhythms\nof the music. It also allows user-controlled temporal reso-\nlutions, and generates smooth sequences with fewer tempo-\nral flickers (Williams 2012). Moreover, formulating dances\nas a sequence of key poses and parametric motion curves\nwould be seamlessly compatible with industrial animation\npipelines. Therefore, we would like to tackle the music con-\nditioned 3D dance generation from this new perspective, and\ndecompose this task into two easier sub-tasks, namely, the\nkey pose generation and the subsequent parametric motion\ncurve regression.\nTo this end, we propose DanceFormer, a two-stage music\nconditioned 3D dance generation framework that consists of\ntwo cascaded Transformer-like (Vaswani et al. 2017) net-\nworks (called DanTrans), where the first one is to generate\nkey poses and the second one is to regress the parameters\nof the motion curves between adjacent key poses. At first,\nwe employ an off-the-shelf beat track algorithm (Ellis 2007)\nto detect beats from the input music ( e.g., around 30 sec-\nonds). The DanTrans for the key pose generation takes a\nsequence of music spectrum features centered at each beat\nas the input, and then generate a sequence of key poses that\narXiv:2103.10206v5  [cs.AI]  27 Jul 2023\nsynchronizes with the sequence of the music beats. And the\nDanTrans for the parametric motion curve regression em-\nploys a different sequence of music spectrum features that\nare extracted between adjacent beats, and feeds the gener-\nated key poses as well to regress the sequence of the param-\neters for each motion curve. In our implementation, we em-\nploy the Kochanek-Bartels splines (Kochanek and Bartels\n1984) to model motion curves, which benfits for rendering\nvarious movements from a small set of control parameters.\nEach DanTrans contains a Wave Encoder as a stack of\ntransformer encoders to extract audio features, and a Mo-\ntion Decoderconstructed by a stack of transformer decoders\nto auto-regressively predict key poses or parameters of the\nmotion curves. To be specific, the Motion Decoder contains\na new Kinematic Propagation Module (KPM) and a Struc-\ntured Multi-head Attention Module (SMAM) to enhance the\nspatial correlation for the dance generation process based on\nthe Kinematic topological relations of the human body. This\nmotion decoder increases the physical significance and thus\nalleviates unreasonable predictions seldom appeared in real\ndances. These DanTrans networks are adversarially trained\nin accompany with ℓ2 reconstruction losses.\nIn order to train the proposed DanceFormer, we also\npropose a new PhantomDance dataset that is produced by\nprofessional animators rather than reconstruction from 2D\nvideos (Lee et al. 2019; Li et al. 2021; Lee, Kim, and Lee\n2018) or by motion capture (Zhuang et al. 2020; Tang, Jia,\nand Mao 2018). The reconstructed data can not ensure the\naccuracy due to the limitation of recent 3D reconstruction\nalgorithms. The motion capture methods are more accurate\nbut still suffer from noise, such as limb flickers and joint\nmismatches. In contrast, our PhantomDance dataset is pro-\nduced by a team of experienced animators instructed by\nprofessional dancers. The animated dances are encoded by\nposes from keyframes and motion curve parameters, which\nthus can produce more fluent and expressive human pose se-\nquences that matches the input musics.\nExtensive experiments are conducted on PhantomDance\nand AIST++ (Li et al. 2021), demonstrating that Dance-\nFormer attains state-of-the-art results and significantly sur-\npasses other works both qualitatively and quantitatively.\nAbove all, the contributions are summarized as follows:\n(1) A new perspective to model the music conditioned 3D\ndance generation as key pose generation and parametric mo-\ntion curve regression, which follows the animation princi-\nples and simultaneously ensures kinematic complexity and\nmotion coherency.\n(2) A novel framework named DanceFormer that is con-\nsists of two Transformer-like networks called DanTrans,\nwhere the first network generates key poses and the sec-\nond one regresses the parameters of the motion curves. More\nimportantly, the DanTrans networks calculates attentions in\nadaptive temporal ranges and explicitly enhances the kine-\nmatic correlation among the outputs.\n(3) The PhantomDance dataset (https://github.com/\nlibuyu/PhantomDanceDataset), as the first dance dataset\ncrafted by professional animators instead of 3D reconstruc-\ntion or motion capture, provides more smooth and expres-\nsive dances that are synchronized with music, and directly\ncompatible with industrial animation pipelines.\n2 Related Works\n3D Dance Synthesis 3D dance synthesis has been ad-\ndressed in a number of ways. The traditional methods were\nbased on retrieval from the dance dataset (Takano, Yamane,\nand Nakamura 2010; Chao et al. 2004). Recently, deep\nlearning models have been widely employed in this task, at-\ntaining visually pleasing results, such as those using convo-\nlutional neural networks (Lee, Kim, and Lee 2018; Zhuang\net al. 2020), recurrent neural networks (Tang, Jia, and Mao\n2018), or using variational auto-encoders (Lee et al. 2019) to\nenhance the diversity of the generated dances. CSGN (Yan\net al. 2019) generates the dance sequences by enhancing\nthe skeleton-level relations with a graph neural network\n(GNN), but the proposed Kinematic Propagation Module\n(KPM) used in our DanceFormer adopts forward and in-\nverse kinematic message passing to better involve the kine-\nmatic correlations. ChoreoMaster (Chen et al. 2021) intro-\nduces a choreography-oriented choreomusical embedding\nframework, and use the embedding to retrieve dance snip-\npets in the motion graph for the input music. Because it\ndoesn’t generate new dances, we don’t compare our method\nwith it. As the Transformer (Vaswani et al. 2017) achieves\na great success in sequence-to-sequence generation tasks in\nNLPs, some of the most recent works on the dance genera-\ntion also bring it into use. Li et al. (2020) presented a two-\nstream motion transformer generative model. AI Choreog-\nrapher (Li et al. 2021) presents a cross-model transformer\nwith future- N supervision for auto-regressive motion pre-\ndiction. These works directly generate the sequence of hu-\nman poses, while the proposed DanceFormer generates a se-\nries of key poses and accompanied parameters of the mo-\ntion curves, which ensures necessary kinematic complexity,\nmotion coherency, adaptive temporal resolutions in the gen-\nerated dances, and moreover, compatible with the industrial\nanimation software, such as Maya, Unity and etc.\n3D Dance Dataset The widely used motion datasets, such\nas Human3.6M (Ionescu et al. 2013) and AMASS (Mah-\nmood et al. 2019), collected common actions like walking,\nrunning, jumping and sitting. However, these datasets are\nhard to adapt for the dance generation task due to the huge\ndistribution gap between daily motions and the dance move-\nments, as well as the absence of carefully aligned music-\ndance pairs. In fact, high-quality 3D dance movements with\nsynchronized musics are extremely hard to collect. Many\nexisting datasets are limited by quantity or quality of data.\nSome of them just use sequences of 2D keypoints to rep-\nresent dances (Lee et al. 2019; Lee, Kim, and Lee 2018).\nAIST++ (Li et al. 2021), as the largest dataset up to date, pre-\nsented a 5-hours 3D dance set. But it gathered dances by re-\nconstructing 3D poses from 2D multi-view videos, thus the\naccuracy of pose parameters may not guaranteed. The other\nworks use motion capture to build dataset (Alemi, Franc ¸oise,\nand Pasquier 2017; Tang, Jia, and Mao 2018; Zhuang et al.\n2020), in which the pose reliability is better but misalign-\nment between the dance-music pair is inevitable. On the con-\ntrary, the proposed PhantomDance is the largest public 3D\ndance dataset up to date and its data quality is much higher\nthan existing datasets thanks to careful labels from experi-\nenced animators with the help of professional dancers. The\nstored key poses and parametric motion curves also benefit\nfor animations by industrial softwares.\n3 The PhantomDance Dataset\n3.1 Data Collection\nWe collect260 popular dance videos from more than13 gen-\nres from over 100 different subjects (dancers) on YouTube,\nNicoNico and Bilibili, which have 9.6 hours in total. Then\na team of experienced animators produced the 3D dance an-\nimations, as sequences of key poses and parametric motion\ncurves. An expert dancer provided professional instructions\nto teach the animators, thus improve the holistic expressive-\nness and detail richness of the animated dances, as well as\nthe synchronization with beats and rhythms of the accompa-\nnied musics. This dataset is named PhantomDance, and we\nwill make it publicly available to facilitate future research.\n3.2 Data Formatting\nIn our PhantomDance dataset, the dances can be ani-\nmated in industrial animation software, such as Maya (De-\nrakhshani 2012), on a standard SMPL (Loper et al. 2015)\ncharacter model with 24 body joints. We followed Hu-\nman3.6M (Ionescu et al. 2013) and provided a subject-\ninvariant 3D human skeleton representation for all the 260\nmotion files, with unified limb lengths. The motion is pa-\nrameterized as the parametric curves of root (the hip joint)\nposition and the rotations of the 24 skeleton joints. For each\nsequence we take the root position on the first frame as the\nthe origin of the 3D coordinate. And the rotations are ex-\npressed with quaternions. That is, there are 3 + 4× 24 = 99\ncurves in each motion file. Each parametric motion curve\nis represented as the Kochanek-Bartels Spline interpolated\nfrom labeled key poses synchronized with extracted music\nbeats, where curve segment between adjacent two key poses\ncan be controlled by a fixed set of parameters.\nWe use the parametric motion curves to represent dance\nmovements because (1) its formulation is analytical and\ncan be densely sampled with various temporal resolutions,\nwhich is very useful for real-time rendering; (2) its shape is\nin nature continuous and thus consistent with the aesthetic\nprinciples when evaluating dances.\n3.3 In Comparison to Previous Datasets\nTable 1 shows the comparison between our PhantomDance\ndataset with the other public music-conditioned 3D Dance\nDatasets. The collected dances in PhantomDance mainly\ncover 13 genres, ranging from Urban Dance to Chinese Clas-\nsical Dance, with extra genres from animes or idol groups\nwhich are hard to classify. The beats per minute (BPM) of\nthese dances range from 75 to 178. Apart from labeling the\ndetailed 3D positions and rotations of each pose, our dataset\nalso provides the ground-truth music beats and parameters\nof each motion curve, thus directly facilitates animation or\nmodel learning that is compatible with industrial software.\nDataset Pos/Rot Beat Curve Genres Music Seconds\nGrooveNet ✓/× × × 1 3 1380\nEA-MUD ✓/× × × 6 23 1849\nDance w/ Melody✓/× × × 4 61 5640\nAIST++ ✓/✓ × × 10 60 18694\nPhantomDance ✓/✓ ✓ ✓ 13+ 260 34667\nTable 1: Our PhantomDance dataset v.s. the other datasets,\nnamely GrooveNet (Alemi, Franc ¸oise, and Pasquier 2017),\nEA-MUD (Sun et al. 2020), Dance with Melody (Tang, Jia,\nand Mao 2018) and AIST++ (Li et al. 2021).\nBesides the aspect of dataset scale, the PhantomDance\ndataset also has high quality. Motion data in the other\ndatasets are collected from optical motion capture sys-\ntem (Alemi, Franc ¸oise, and Pasquier 2017; Tang, Jia, and\nMao 2018; Sun et al. 2020) or 3D reconstruction from multi-\nview videos (Li et al. 2021). Motion capture systems can not\ntotally avoid data noise like limb flicker and joint mismatch.\nAnd video based 3D reconstruction suffers from more seri-\nous noise like foot slippery and limb twist due to the limita-\ntion of reconstruction algorithms. Thus professional anima-\ntors are needed for data correction. In contrast, motion data\nin PhantomDance are produced by expert animators from\nscratch with the guidance of a professional dancer. And we\nhave a strict quality checking process so that it costs about\n15 months to finish the dance labeling. More details of our\nPhantomDance dataset and the qualitative comparison with\nthe other datasets can be seen in our project page1.\n4 DanceFormer\nAs illustrated in Figure 1, we propose the DanceFormer, a\ntwo-stage music conditioned 3D dance generation frame-\nwork that consists of two cascaded transformer-like net-\nworks. Each network is named as DanTrans, where the first\none generates key poses, and the second one predicts the pa-\nrameters of the motion curves between adjacent key poses.\nEach DanTrans network contains a wave encoder that is\nsimilar as the standard transformer encoder (Vaswani et al.\n2017), and a motion decoder with a Kinematic Propagation\nModule (KPM) and a Structured Multi-head Attention Mod-\nule (SMAM), which enhances the kinematic correlation for\nthe dance generation process based on the Kinematic topo-\nlogical relations of the human body. Adversarial learning is\napplied during the training in both stages.\n4.1 Stage 1: Key Pose Generation\nFollowing the rhythm (beats) is a basic principle in dance\ntheory (Goodridge 1999), and previous studies also show\nthat music beats and kinematic inflexions have strong con-\nsistency in time (Lee et al. 2019; Li et al. 2021). Thus we de-\nfine the key poses directly as the poses on the beats, which is\nrepresented as position and rotation (in quaternion) of each\nskeleton joint, with the dimension of(3+4) ×24 = 168. We\nfirst leverage the a dynamic programming algorithm (Ellis\n2007) to track the time of each beat in the music. Our target\nin stage 1 is to generate a sequence of key poses to match\nthese extracted beats.\n1https://huiye-tech.github.io/post/danceformer/\nx N\noutputs\nKPM\nAtt.\nKPM\nAtt.\noutputs\nencoder\nfeature\n+ pos encode\nWave\nEncoder\nMotion\nDecoder\nDanTrans\ninputs outputs\n(shifted right)\noutputsDiscriminator\nAdv. Loss Pred. Loss\nmotion\ncurves\ndance animation\nDanTrans\n2\nDanTrans\n1\nmusic\nbeats\nkey \nposes\nembedding\nFigure 1: Overview of the workflow of DanceFormer. The networks in stage 1 take as input the music wave features in the\nneighborhood of each beat and output a sequence of key poses. Then the networks in stage 2 utilize the generated key poses and\nthe music wave features between each two key poses to predict the motion curve in between. Each network adopts the proposed\ntransformer-like network, called DanTrans. The DanTrans has a new transformer decoder, which uses the proposed Kinematic\nPropagation Module (KPM). Note that the attention module (Att. in the figure) is a structured multi-head attention.\nmusic\nmotion\nparameter\ncurves\nbeat\nknots\nFigure 2: Motion Curves. The figure shows an example of\n4-knot TCB spline parameterization for motion curves.\nThe input sequential music features are music spectrum\nfeatures between a window centered at each beat. We use a\nwindow of size 0.8 seconds in our implementation since the\nlowest beat per-minus (BPM) is75 in common dance musics\n(which is validated in our PhantomDance dataset, and also\nvalid in other datasets). We choose the Hamming window\nto tackle the window overlap in music with higher BPMs.\nIn each window, we calculate the 40-dimensional form of\nMFCC (Logan et al. 2000) that has 13 coefficients of Mel\nfilters cepstrum coefficients with the first and second order\ndifferences (13 × 2 dimensions) and the energy sum (1 di-\nmension). We further append a 12-dimension chroma fea-\nture to the feature of every beat and finally attain the inputs\nto the wave encoder. And the motion decoder combines the\nencoded features with the right shifted outputs and sequen-\ntially predict the key poses, as indicated by the sequence-\nto-sequence prediction strategy based on the standard trans-\nformer (Vaswani et al. 2017).\n4.2 Stage 2: Parametric Motion Curve Prediction\nBased on the generated key poses, the DanTrans in stage 2\nis targeted to generate the motion curves in between.\nThe motion curves are defined as the values of trans-\nformation parameters with regard to time. The transfor-\nmation parameters are translation (x, y, z) and rotation\n(rx, ry, rz, rw) in quaternion of each joint. We use a\nmulti-knots Kochanek-Bartels splines (Kochanek and Bar-\ntels 1984) to model each curve. It is a sort of cubic Her-\nmite spline that is used in animation editing softwares. The\nKochanek-Bartels spline is also named TCB spline because\nit has 3 parameters: t for tension, b for bias and c for con-\ntinuity. While for a multi-knot TCB spline, we should also\ndetermine the intermediate knots and their tangents. Figure 2\nshows a example of motion curve parameterization.\nIn our implementation, we use one 4-knot TCB spline to\nfit one motion curve between two adjacent key poses, as an\noptimal tradeoff between fitting accuracy and representation\ncompactness. Since the endpoints in the curve are just on the\nkey poses, we have 7 control parameters to predict, namely\nthe t, c, b and the position of the two intermediate knots.\nSimilar to the DanTrans in stage 1, we extract the se-\nquence of the MFCC features from the music between every\ntwo adjacent beats as the input for the wave encoder, and\nthe motion decoder auto-regressively outputs the 7 control\nparameters that are aligned with each MFCC feature. Note\nthat the motion decoder requires the right shifted output se-\nquences as its input, which should be additionally concate-\nnated with the two key poses at the beginning and the end of\nthe curve, which are generated in stage 1.\n4.3 Network Structure of DanTrans\nDanTrans has a similar Network structure to the standard\ntransformer (Vaswani et al. 2017) as illustrated on the right\nof Figure 1, except that the motion decoder employs the pro-\nposed Kinematic Propagation Module (KPM) and the Struc-\ntured Multi-head Attention Module (SMAM) in each trans-\nformer decoder layer. We employ a stack of N = 6 trans-\nformer decoders in the motion decoder of each DanTrans.\nNote that an additional KPM is applied at the beginning of\nthe motion decoder for output embedding.\nKinematic Propagation Module The Kinematic Propa-\ngation Module (KPM) are constructed on the basis of human\nbody structure, i.e., the 24 skeleton joint nodes with a tree\ntopology. The networks consist of a stack of so-called FK-\nFC\nFC\nFC\nFC\nFC\nFC\nForward Kinematic \nFeature Passing\nInverse Kinematic \nFeature Passing\nJoint \nNode \nFeatures\nFC FC\nFC\nQ Q\nQ\nQ Q\nAtt\nAttAtt\nK K\nK\nK K\nV V\nV\nV V\nAttAtt\nFC\nFC\nFC\nFC\nFCFC\nFCFC\nFC\nFC\nFCFC\nFigure 3: The Kninematic Propagation Module (KPM)\nand the Structured Multi-head Attention. Features are\nembedded and passed along the kinematic chains forward\nand inversely with fusion operations. Note that the endpoints\nof feature passing are root and leaf nodes, and we just show\na part of them for clearer view.\nIK block as illustrated in Figure 3. Note that the fully con-\nnected layer (FC) represents the complete BN-Linear-ReLU\nmodule, while we use FC to represent it for simplicity.\nThe FK-IK block has two steps, i.e., the forward kine-\nmatic (FK) feature passing and then the inverse kinematic\n(IK) feature passing, respectively. The forward kinematic\nfeature passing step firstly performs a feature embedding\non the root node and pass it to the neighbor child nodes\nalong the kinematic chain. After receiving the features from\nthe parent node, the child node performs feature fusion and\nthen conducts feature embedding by linear projection. The\nFC layers are utilized for feature embedding and the fusion\nadopts the addition operation. The backward kinematic fea-\nture passing step has a similar feature embedding and the\npassing process is performed from the leaf nodes to the root.\nThese procedures are similar to the operations of forward\nkinematics (FK) and inverse kinematics (IK) in computer\ngraphics and robotics. The FK-IK block attempts to encode\nfeatures into the motion controlling parameter space and to\nbring physics constraints into the model.\nFor the output embedding process at the beginning of the\nmotion decoder, we use a KPM with one FK-IK block. And\nfor feed-forward process in the intermediate layers, we adopt\na stack of 2 FK-IK blocks for each KPM. All the KPMs have\nthe same output feature size of 64.\nStructured Multi-Head Attention The features out-\nputted by KPM are arranged as the joint nodes, they nat-\nurally act as heads in the multi-head attention mechanism\nused in common transformers (Vaswani et al. 2017). In the\nmulti-head cross-attention, the encoded features from the in-\nput music are processed by two sets of 24 paralleled FC lay-\ners with the output size of 64, which then act as the values\nand keys. Note that no concatenation is needed after atten-\ntion calculation since the features should maintain the body\nstructure, and directly fed into the succeeding KPM module.\nThus this process saves computation compared to the typi-\ncal multi-head attention that concatenates features from each\nhead and uses a big linear layer for feature embedding. The\nstructured multi-head attention module, also shown in the\nright of Figure 3, defines the head as the node in the body\nstructure, which has explicit physical significance. To some\ndegree this design is more reasonable than the multi-heads\nin the original transformer.\n4.4 Training Objective\nWe adopt an adversarial training strategy that regards each\nDanTrans network as an individual generator, and learns two\ndiscriminators to judge whether distributions of the gener-\nated sequences and the groundtruth ones are aligned or not.\nEach discriminator consists of two modules to process the\nmusic feature and the output/groundtruth key pose sequence\n(in stage 1) or the parameters of the motion curves (in stage\n2), and the processed features are combined and fed into\nthe binary classifier to predict the authenticity of the out-\nputs conditioned on the input music. All modules in the dis-\ncriminator are 3-layer MLPs. Moreover, when updating the\ngenerator, we also include an ℓ2 reconstruction loss to avoid\nunwanted solutions that are far away with real data.\n5 Experiments\n5.1 Implementation Details\nSince most musics in PhantomDance have the verse-chorus\nform, we divide the 260 dance animations into 1000 se-\nquences. Among them 900 pieces of music-dance pairs are\nused for model training, and the other 100 are split into the\ntest set. We carefully pick the test set to ensure it cover\n13 genres and the BPM range from 80 to 180. We fol-\nlow the official training/testing splits in AIST++ (Li et al.\n2021). To gather the ground-truth labels for the training of\nthe DanceFormer, we use the beat track algorithm (Ellis\n2007) to extract beats and fit the parameter motion curves\nfrom the provided pose sequences. The DanceFormer is end-\nto-end trained using 4 TITAN Xp GPUs with a batch size\nof 8 on each GPU. We use the Adam optimizer with be-\ntas {0.5, 0.999} and a learning rate of 0.0002. The learning\nrate drops to 2e−5, 2e−6 after 100k, 200k steps. The model\nis trained with 300k steps for AIST++ and 400k steps for\nPhantomDance. The dimension of features in DanceFormer\nis 256 unless otherwise specified.\n5.2 Evaluation Metrics\nNormalized Power Spectrum Simularity (NPSS) It is an\nevaluation metric for long-term motion synthesis compared\nto Mean Square Error (MSE). We just follow its official\nimplementation (Gopalakrishnan et al. 2019) and compute\nNPSS in the joint motion space RT×N×7 (4 for joint rota-\ntion represented as quaternion and 3 for joint position).\nFrechet Distance (FD) It is proposed by AIST++ (Li et al.\n2021) that has two metrics, namely PFD for position and\nVFD for velocity. We also employ it to calculate the distri-\nbution distance of joints.\nPosition Variance (PVar) It evaluates the diversity of the\ngenerated dance. In AIST++, a piece of music corresponds\nto more than one dance, but only one in PhantomDance. So\nwe make a modification to the metric PVar in the experi-\nments on PhantomDance where we compute it along differ-\nent music pieces which have identical length.\nBeat Consistency Score (BC) It is a metric for motion-\nmusic correlation. We follow (Li et al. 2021) to define kine-\nmatic beats as the local minima of the kinetic velocity. BC\nComponent NPSS↓ PFD↓ VFD↑ PVar↑ BC↑\nDanceFormer 8.03 114.03 0.55 0.912 0.785\nCurve to Frames 14.55 1132.4 1.06 0.233 0.317\nKPM to Linear 10.54 162.44 0.79 0.592 0.637\nKPM to GNN 10.34 146.43 0.69 0.647 0.692\nTable 2: Ablation study about our DanceFormer on the\nPhantomDance dataset. ↓ means that lower results indicate\nbetter methods, and ↑ vice versa.\nAtt Algorithm NPSS↓ PFD↓ VFD↑ PVar↑ BC↑\nGlobal 9.73 151.53 0.72 0.649 0.737\nLocal 9.01 125.59 0.68 0.744 0.763\nGaussian Local 8.93 117.14 0.64 0.753 0.764\nLearned Local 8.03 114.03 0.55 0.912 0.785\nTable 3: The comparison of different Attention mecha-\nnisms. ↓ means that lower results indicate better methods,\nand ↑ vice versa.\ncomputes the average distance between every music beat and\nits nearest kinematic beat with the following equation:\nBC = 1\n|Bx|\n|Bx|X\ni=1\nexp\n \n−\nmin∀tx\nj ∈Bx ||tx\nj − ty\ni ||2\n2σ2\n!\n(1)\nwhere Bx = {tx\nj }, By = {ty\ni } are kinematic beats and mu-\nsic beats respectively and σ = 3. Note that BC has a similar\nform to Beat Alignment Score (BA) proposed in (Li et al.\n2021), but they are different in essence. BA forces every\nkinematic beat to match a music beat, but a dance usually\nhas many small kinematic beats that occur between music\nbeats. Moreover, a music synchronized dance just need to\nensure the most salient music beats are accompanied with\nthe action emphasis (kinematic beats). So our proposed BC,\nwhich finds matched kinematic beat for each music beat, is\nmore appropriate in this case.\n5.3 Ablation Study\nTo validate the effectiveness of our DanceFormer, we con-\nduct the ablation study on our PhantomDance dataset.\nEffectiveness of Curve Prediction To evaluate the effec-\ntiveness of our curve prediction, we sample 60-FPS poses\nfrom the data and use a standard transformer to directly pre-\ndict the sequence. The input sequences are extracted from a\n0.5s sliding window with a step of1/60 second on the music\nwaves. Its result is the second line of Table 2 as “Curve to\nFrame”. The experimental result demonstrates the advantage\nof our pose-to-curve two stage generation framework.\nEffectiveness of KPM To study the impact of the KPM\ncomponent, we first replace it with the standard feed-\nforward (linear layers) modules in vanilla transformer\n(Vaswani et al. 2017). And the multi-head attention also\ncomes back to the common form. The third line in Table 2\nshows that the KPM has significant improvement in mo-\ntion generation quality. To further study the FK-IK process-\ning, we introduce a comparison architecture which substi-\ntutes the FK-IK block with a 2-layer graph neural network\n(GNN) (Scarselli et al. 2008). This GNN fuses the joint fea-\ntures according to the adjacency matrix of undirected graph\ndefined by the joint structure. The experimental result shows\nthat GNN has better performances compared to the base-\nline (“KPM to Linear”), which proves that introducing spa-\ntial correlations benefits the representation of the learning\nmodel. While the proposed KPM surpasses the trivial GNN\nby a large margin, validating that involving spatial correla-\ntion and physics constraints into the network brings in sig-\nnificant improvements in the generation quality that is eval-\nuated by NPSS, PFD and VFD.\nVariants of Attention In our implementation, we find that\nlocal attention (Luong, Pham, and Manning 2015) is more\nproper for the motion sequence generation task than the\nglobal attention in standard transformers. It can be explained\nby the temporal locality of motions. That is, the pose sev-\neral seconds ago has no direct influence on the current pose.\nMoreover relationship between motion state is stronger with\nthe time going closer. So we further compare several atten-\ntion algorithms and the results are shown in Table 3. We use\na sequence length of 17 in the simple local attention exper-\niment shown on the second line. And we add a Gaussian\nmask on the attention results before softmax with a stan-\ndard variance of 4. The result is on the third line and it has\na slight improvement. Finally we use a learned mask to be\noptimized with the networks and obtain the best result. The\ntrained mask has a triangle shape on the whole with some\nhumps. Our DanceFormer employs this learnable local at-\ntention in our implementation.\n5.4 Comparison with Other Methods\nWe mainly compare our method with AI Choreographer (Li\net al. 2021), which to our knowledge obtains the state-of-\nthe-art results for music-conditioned dance generation. The\nother two most related works, namely Li et al. (Li et al.\n2020) and Music2Dance (Zhuang et al. 2020) are also com-\npared. The experimental comparisons are performed on both\nthe AIST++ and our PhantomDance datasets.\nQuantitative Comparisons The results are viewed in Ta-\nble 4. Our method outperforms Li et al. (Li et al. 2020) and\nMusic2Dance (Zhuang et al. 2020) by a considerably large\nmargin. And it also surpasses AI Choreographer (Li et al.\n2021) significantly in the quality related metrics (a30% gain\nof NPSS, a 28% gain of PFD and a 27% gain of VFD), di-\nversity metrics (a 46% gain of PVar) and beat consistency\nmetrics (a 93% gain of BC). The promising results on the\ndance quality metrics are mainly owing to the network struc-\nture of KPM that enhances the kinematic correlations. The\ngeneration diversity is ensured by the adversarial learning\nscheme. Note that the upper bound of the metric BC is 1,\nwhich means there exactly exists a kinematic beat at the time\nof each music beat. So our results have relatively high beat\nconsistency score, which is due to the proposed two-stage\nframework of our DanceFormer. Since no metric emphasizes\nthe fluency of the dance performance, the advantages of mo-\ntion curve formulation can only be well revealed in qualita-\ntive results, as shown in Figure 4.\nQualitative Results and User Study Figure 4 provides\na sequence of frames about the generated dances by our\nFigure 4: Visualization of generated results of DanceFormer.\nMethod AIST++ PhantomDance\nNPSS↓ PFD↓ VFD↑ PVar↑ BC↑ NPSS↓ PFD↓ VFD↑ PVar↑ BC↑\nLi et al. (Li et al. 2020) 16.31 5595.91 3.40 0.019 0.359 18.34 7944.78 5.84 0.014 0.175\nMusic2Dance (Zhuang et al. 2020) 14.74 2367.26 1.13 0.215 0.378 15.94 3147.89 3.74 0.267 0.223\nAI Choreographer (Li et al. 2021) 8.29 113.56 0.45 0.509 0.452 10.62 164.33 0.73 0.624 0.388\nDanceFormer 6.01 84.32 0.34 0.734 0.782 8.03 114.03 0.55 0.912 0.785\nTable 4: Dance generation evaluation on the AIST++ and the PhantomDance datasets. Our method outperforms other\nbaselines in terms of quality, diversity and beat consistency. Especially, due to our two-stage prediction schema, our model has\nsuperior performance in term of BC indicating that our model can generate dances which better match the given music.↓ means\nthat lower results indicate better methods, and ↑ vice versa.\n0.03\n0.22\n0.39\n0.86\n0.97\n0.78\n0.61\n0.14\nPerformance Quality\n0.02 0.12\n0.33\n0.82\n0.98 0.88\n0.67\n0.18\nMatching the Music\nMusic2Dance AI Choreographer OursGround-truthLi et al.\nFigure 5: Results of the user study.We conduct a user study\nto ask participants to choose the better dances from pairwise\ncomparisons. The criteria includes the performance quality\nand matching the music. The number denotes the percentage\nof preference on the comparison pairs.\nDanceFormer. These results show that the proposed method\ncan provide diverse movements with high kinematic com-\nplexity. Video results accompanied with musics, including\nthe comparisons with the other works can be found in the\nproject page2.\nWe also conducted a user study to evaluate the quality\nof the generated music-conditioned dances. All the 100 se-\nquences of the validation set of PhantomDance were used\nfor the study. And then we collect the generated dances\nby our method and the compared baselines (Li et al. 2021,\n2020; Zhuang et al. 2020). In addition, the ground-truth\ndances are also included. The user study was conducted us-\ning a pairwise comparison scheme. For each of the 100 mu-\nsics, we provide 4 pairs in which our results occur with the\n2https://huiye-tech.github.io/post/danceformer/\nresults from the baseline methods or the ground truth. Thus\n400 pairs were provided to the participants, and they were\nasked to make two choices for each pair: “Which dance is\na better performance (more fluent, graceful and pleasing)?”\nand “Which dance matches the music better?”. There are\n100 participants in the user study. Figure 5 shows the user\nstudy results, where our DanceFormer outperforms the other\nmethods on both criteria. Most of the participants rated that\nour method generates better dances in performance quality\ncompared with other works, and even more participants held\nthe opinion that the dances generated by our model better\nmatch the musics.\n6 Conclusion\nIn this work, we propose a new perspective to model the\nmusic-conditioned 3D dance generation task. Different from\nprevious works that define the outputs as sequences of poses,\nwe formulate them as key poses and in-between motion\ncurves. The curve representation makes the generated re-\nsults more fluent and graceful. Based on this formulation, we\npropose the transformer-based DanceFormer with the novel\nDanTrans architecture consisting of the KPM module for\nbetter modeling kinematic correlations. DanceFormer thus\nyields high-quality results in the experimental comparisons.\nMoreover, we propose the PhantomDance Dataset, the first\nmusic-conditioned 3D dance dataset that uses curves to rep-\nresent body motion, and it is the largest 3D dance dataset\nwith the best visual quality up to date.\nAcknowledgement This work was partially supported by\nthe National Natural Science Foundation of China (No.\n61906012, No. 62132001).\nReferences\nAlemi, O.; Franc ¸oise, J.; and Pasquier, P. 2017. Groovenet:\nReal-time music-driven dance movement generation using\nartificial neural networks. networks, 8(17): 26.\nChao, S.-P.; Chiu, C.-Y .; Chao, J.-H.; Yang, S.-N.; and Lin,\nT.-K. 2004. Motion retrieval and its application to motion\nsynthesis. In 24th International Conference on Distributed\nComputing Systems Workshops, 2004. Proceedings., 254–\n259. IEEE.\nChen, K.; Tan, Z.; Lei, J.; Zhang, S.-H.; Guo, Y .-C.; Zhang,\nW.; and Hu, S.-M. 2021. ChoreoMaster: choreography-\noriented music-driven dance synthesis. ACM Transactions\non Graphics (TOG), 40(4): 1–13.\nDerakhshani, D. 2012. Introducing Autodesk Maya 2013.\nJohn Wiley & Sons.\nEllis, D. P. 2007. Beat tracking by dynamic programming.\nJournal of New Music Research, 36(1): 51–60.\nGoodridge, J. 1999. Rhythm and timing of movement in per-\nformance: Drama, dance and ceremony. Jessica Kingsley\nPublishers.\nGopalakrishnan, A.; Mali, A.; Kifer, D.; Giles, L.; and Oror-\nbia, A. G. 2019. A neural temporal model for human motion\nprediction. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 12116–12125.\nIonescu, C.; Papava, D.; Olaru, V .; and Sminchisescu, C.\n2013. Human3. 6m: Large scale datasets and predictive\nmethods for 3d human sensing in natural environments.\nIEEE transactions on pattern analysis and machine intel-\nligence, 36(7): 1325–1339.\nKochanek, D. H.; and Bartels, R. H. 1984. Interpolating\nsplines with local tension, continuity, and bias control. In\nProceedings of the 11th annual conference on Computer\ngraphics and interactive techniques, 33–41.\nLasseter, J. 1987. Principles of traditional animation applied\nto 3D computer animation. In Proceedings of the 14th an-\nnual conference on Computer graphics and interactive tech-\nniques, 35–44.\nLee, H. Y .; Yang, X.; Liu, M. Y .; Wang, T. C.; Lu, Y . D.;\nYang, M. H.; and Kautz, J. 2019. Dancing to music. Ad-\nvances in Neural Information Processing Systems, 32.\nLee, J.; Kim, S.; and Lee, K. 2018. Listen to\ndance: Music-driven choreography generation using au-\ntoregressive encoder-decoder network. arXiv preprint\narXiv:1811.00818.\nLi, J.; Yin, Y .; Chu, H.; Zhou, Y .; Wang, T.; Fidler, S.; and\nLi, H. 2020. Learning to Generate Diverse Dance Motions\nwith Transformer. arXiv preprint arXiv:2008.08171.\nLi, R.; Yang, S.; Ross, D. A.; and Kanazawa, A. 2021. AI\nChoreographer: Music Conditioned 3D Dance Generation\nwith AIST++. In The IEEE International Conference on\nComputer Vision (ICCV).\nLogan, B.; et al. 2000. Mel frequency cepstral coefficients\nfor music modeling. In Ismir, volume 270, 1–11. Citeseer.\nLoper, M.; Mahmood, N.; Romero, J.; Pons-Moll, G.; and\nBlack, M. J. 2015. SMPL: A skinned multi-person linear\nmodel. ACM transactions on graphics (TOG), 34(6): 1–16.\nLuong, T.; Pham, H.; and Manning, C. D. 2015. Effective\nApproaches to Attention-based Neural Machine Translation.\nIn EMNLP.\nMahmood, N.; Ghorbani, N.; Troje, N. F.; Pons-Moll, G.;\nand Black, M. J. 2019. AMASS: Archive of motion capture\nas surface shapes. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, 5442–5451.\nScarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; and\nMonfardini, G. 2008. The graph neural network model.\nIEEE transactions on neural networks, 20(1): 61–80.\nSun, G.; Wong, Y .; Cheng, Z.; Kankanhalli, M. S.; Geng,\nW.; and Li, X. 2020. DeepDance: music-to-dance motion\nchoreography with adversarial learning. IEEE Transactions\non Multimedia, 23: 497–509.\nTakano, W.; Yamane, K.; and Nakamura, Y . 2010. Retrieval\nand Generation of Human Motions Based on Associative\nModel between Motion Symbols and Motion Labels. Jour-\nnal of the Robotics Society of Japan, 28(6): 723–734.\nTang, T.; Jia, J.; and Mao, H. 2018. Dance with melody: An\nlstm-autoencoder approach to music-oriented dance synthe-\nsis. In Proceedings of the 26th ACM international confer-\nence on Multimedia, 1598–1606.\nThomas, F.; Johnston, O.; and Thomas, F. 1995.The illusion\nof life: Disney animation. Hyperion New York.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is all you need. arXiv preprint arXiv:1706.03762.\nWilliams, R. 2012. The animator’s survival kit: a manual\nof methods, principles and formulas for classical, computer,\ngames, stop motion and internet animators. Macmillan.\nYan, S.; Li, Z.; Xiong, Y .; Yan, H.; and Lin, D. 2019. Convo-\nlutional sequence generation for skeleton-based action syn-\nthesis. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 4394–4402.\nZhuang, W.; Wang, C.; Xia, S.; Chai, J.; and Wang, Y .\n2020. Music2dance: Music-driven dance generation using\nwavenet. arXiv preprint arXiv:2002.03761.",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7296392321586609
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5519647598266602
    },
    {
      "name": "Encoder",
      "score": 0.49837350845336914
    },
    {
      "name": "Kinematics",
      "score": 0.4965859055519104
    },
    {
      "name": "Dance",
      "score": 0.4926612973213196
    },
    {
      "name": "Parametric statistics",
      "score": 0.47152766585350037
    },
    {
      "name": "Animation",
      "score": 0.4508039355278015
    },
    {
      "name": "Computer vision",
      "score": 0.45032161474227905
    },
    {
      "name": "Transformer",
      "score": 0.4401092231273651
    },
    {
      "name": "Motion capture",
      "score": 0.43192440271377563
    },
    {
      "name": "Motion (physics)",
      "score": 0.39618200063705444
    },
    {
      "name": "Speech recognition",
      "score": 0.339341938495636
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.18716591596603394
    },
    {
      "name": "Mathematics",
      "score": 0.13287478685379028
    },
    {
      "name": "Engineering",
      "score": 0.12114980816841125
    },
    {
      "name": "Literature",
      "score": 0.0
    },
    {
      "name": "Classical mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [],
  "cited_by": 12
}