{
  "title": "Context-Aware Language Modeling for Goal-Oriented Dialogue Systems",
  "url": "https://openalex.org/W4287889972",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5006038162",
      "name": "Charlie Snell",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5103006156",
      "name": "Sherry X. Yang",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5037720811",
      "name": "Justin Fu",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5004835193",
      "name": "Yi Su",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A5026322200",
      "name": "Sergey Levine",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962886331",
    "https://openalex.org/W2163068732",
    "https://openalex.org/W2991355586",
    "https://openalex.org/W3031840745",
    "https://openalex.org/W2417401578",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W2980207396",
    "https://openalex.org/W2997108628",
    "https://openalex.org/W1598178035",
    "https://openalex.org/W2551884415",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W62710299",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2168359464",
    "https://openalex.org/W2169218343",
    "https://openalex.org/W2167224731",
    "https://openalex.org/W4300799055",
    "https://openalex.org/W2904453761",
    "https://openalex.org/W2962682659",
    "https://openalex.org/W4294225490",
    "https://openalex.org/W2891704263",
    "https://openalex.org/W2970799419",
    "https://openalex.org/W2915295540",
    "https://openalex.org/W2963167310",
    "https://openalex.org/W2962852262",
    "https://openalex.org/W3034441005",
    "https://openalex.org/W2953981431",
    "https://openalex.org/W2112804987",
    "https://openalex.org/W2231198303",
    "https://openalex.org/W1681299129",
    "https://openalex.org/W2964044380",
    "https://openalex.org/W1975244201",
    "https://openalex.org/W2964036701",
    "https://openalex.org/W2035934535",
    "https://openalex.org/W2963567240",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2106547558",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W3008970887",
    "https://openalex.org/W4287795696",
    "https://openalex.org/W2565274151",
    "https://openalex.org/W2132997613",
    "https://openalex.org/W2115714256",
    "https://openalex.org/W4287325749",
    "https://openalex.org/W3037440645",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3205794883",
    "https://openalex.org/W2953071719",
    "https://openalex.org/W3184222203",
    "https://openalex.org/W1594201624",
    "https://openalex.org/W3100963818",
    "https://openalex.org/W2963170138",
    "https://openalex.org/W3024509506",
    "https://openalex.org/W2040123554",
    "https://openalex.org/W1590511541",
    "https://openalex.org/W2062175565",
    "https://openalex.org/W3022566517",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2996125406",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Goal-oriented dialogue systems face a trade-off between fluent language generation and task-specific control. While supervised learning with large language models is capable of producing realistic text, how to steer such responses towards completing a specific task without sacrificing language quality remains an open question. In this work, we formulate goal-oriented dialogue as a partially observed Markov decision process, interpreting the language model as a representation of both the dynamics and the policy. This view allows us to extend techniques from learning-based control, such as task relabeling, to derive a simple and effective method to finetune language models in a goal-aware way, leading to significantly improved task performance. We additionally introduce a number of training strategies that serve to better focus the model on the task at hand. We evaluate our method, Context-Aware Language Models (CALM), on a practical flight-booking task using AirDialogue. Empirically, CALM outperforms the state-of-the-art method by 7% in terms of task success, matching human-level task performance.",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 2351 - 2366\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nContext-Aware Language Modeling for Goal-Oriented Dialogue Systems\nCharlie Snell Mengjiao Yang Justin Fu\nUC Berkeley\n{csnell22,sherryy,justinfu,suyi,svlevine}@berkeley.edu\nYi Su Sergey Levine\nAbstract\nGoal-oriented dialogue systems face a trade-\noff between fluent language generation and\ntask-specific control. While supervised learn-\ning with large language models is capable of\nproducing realistic text, how to steer such re-\nsponses towards completing a specific task\nwithout sacrificing language quality remains\nan open question. In this work, we formulate\ngoal-oriented dialogue as a partially observed\nMarkov decision process, interpreting the lan-\nguage model as a representation of both the\ndynamics and the policy. This view allows\nus to extend techniques from learning-based\ncontrol, such as task relabeling, to derive a sim-\nple and effective method to finetune language\nmodels in a goal-aware way, leading to signif-\nicantly improved task performance. We addi-\ntionally introduce a number of training strate-\ngies that serve to better focus the model on the\ntask at hand. We evaluate our method, Context-\nAware Language Models (CALM), on a practi-\ncal flight-booking task using AirDialogue. Em-\npirically, CALM outperforms the state-of-the-\nart method by 7% in terms of task success,\nmatching human-level task performance.\n1 Introduction\nDialogue systems have typically approached the\nproblem of generating realistic dialogue from the\nperspective of supervised learning (Dušek and Ju-\nrcicek, 2016; Eric and Manning, 2017; Mei et al.,\n2017; Chen et al., 2019; Wu et al., 2019a; Hosseini-\nAsl et al., 2020; Peng et al., 2020; Adiwardana\net al., 2020). However, dialogue can also be viewed\nas a sequential decision making process, which is\nwell-suited to planning and reinforcement learning\n(RL) algorithms. A challenge with the classical RL\napproach to dialogue is the requirement for active\ninteraction with humans (Gaši´c et al., 2011). Train-\ning such a system with active human-in-the-loop\nCode at https://sea-snell.github.io/CALM_LM_site/\nSamples from CALM\nSure, flight 1000 meets \nyour needs, shall I book \nit?\nFlight 1002 is perfect for \nyou, can I book it?\nCan I book flight 1001 \nfor you?\nSure, flight 1001 meets \nyour needs, shall I book \nit?\nFlight 1001 is perfect for \nyou, can I book it?\nCan I book flight 1001 \nfor you?\nSamples from standard LM\nStandard LM training produces coherent \ngenerations, but fails to learn the task.\nCALM’s training forces it to pay attention \nto the dialogue context, producing \ncoherent and successful outputs.\nHello.\nHow can I help you?\nCould you help me in \nbooking a flight ticket from \nAUS to EWR?\nFlight 1000\nGPT-2\n</s>\nFlight 1001\nFlight 1002\n…\n…\nSure\nSure\n,\n,\nmeetsflight\nflight\n1001\n1001\n…\n…\nFigure 1: CALM is an end-to-end language model\nfor goal oriented dialogue. CALM’s training objective\nteaches the model to better pay attention to the dialogue\ntask context, yielding a ∼50% improvement in task\nsuccess over standard LM training on a flight booking\ntask.\ninteraction quickly becomes expensive and cum-\nbersome, making it desirable to develop techniques\nfor goal-directed training of dialogue systems that\ncan effectively leverage offline data.\nWhile many dialogue generation techniques\nbased on RL and learned control have been pro-\nposed (Eckert et al., 1997; Levin et al., 2000;\nChung, 2004; Georgila et al., 2006; Schatzmann\net al., 2007; Heeman, 2009; Georgila and Traum,\n2011), most such systems take a pipelined ap-\nproach, where an abstract representation of states\nand actions is designed by hand and then combined\nwith RL to train a “dialogue management” system,\nrather than generating dialogue end-to-end. These\npipelined approaches rely on a manually designed\ndecomposition of the dialogue task, which may be\ndomain-specific and, more importantly, may not\nenjoy all of the benefits of tightly integrating low-\nlevel text generation with the overall goals of the\n2351\ntask. In this work, we instead ask: how can we\nscalably and effectively introduce the mechanisms\nof goal-directed decision making into end-to-end\nlanguage models, to directly steer language gen-\neration toward completing specific dialogue tasks\nrather than simply generating probable responses?\nTo this end, rather than utilizing a pipelined ap-\nproach, we aim to directly finetune language mod-\nels in a task-aware manner such that they can maxi-\nmize a given utility function. We observe that large\nlanguage models can already be formulated within\na Markov decision processes (MDP) as capturing\nboth the dynamics and policy for a decision-making\ntask, where dialogue history serves as state, and\nthe agent’s utterances serve as actions. We could\nutilize this observation by finetuning the models\ndirectly with online RL, but the need for human-in-\nthe-loop training makes this difficult. Offline RL\nmethods (Levine et al., 2020; Fujimoto et al., 2019;\nWu et al., 2019b; Wang et al., 2020b) provide an\nalternative approach, but typically require value\nfunction estimation, which is not straightforward\nto perform with a language model. Instead, we\npropose a conditional imitation learning strategy\ncoupled with a novel task relabeling approach that\ncan finetune language models from offline data,\nsuch that the model still represents the joint dis-\ntribution over dialogues, but tilts this distribution\ntoward dialogues with a high reward. This amounts\nto a task-aware finetuning strategy that integrates\ntask information into the model. The main con-\ntribution of our work is CALM ( Context-Aware\nLanguage Modeling), a framework for end-to-end\ngoal-directed dialogue generation. CALM unifies\nthe traditional language modeling objective with\ntask-specific supervision, where a language model\nis interpreted as a joint representation of dynamics\nand policies in an MDP, and the finetuning process\nutilizes a conditional imitation learning objective\nwith a novel task relabeling strategy that teaches the\nmodel how to generate high-utility dialogues (see\nFigures 1 and 2). Because CALM interprets the\nlanguage model as both a dynamics model and a\npolicy, it can be used as either a model-free method,\nwhere the dynamics are discarded and the policy\ncomponent is used to greedily generate responses,\nor as a model-based method, where the dynamics\ncomponent can be used to plan at test-time. We\nempirically evaluate CALM on AirDialogue (Wei\net al., 2018), the largest dataset for goal-oriented\ndialogue based-on a flight-booking task. CALM\nimproves the task success by 10% over the pre-\nvious state-of-the-art method (Chen et al., 2020)\nfollowing the evaluation protocol proposed by Wei\net al. (2018), achieving the first-ever human-level\nperformance on this dataset.\n2 Related Work\nOur goal is to enable end-to-end training of goal-\ndirected dialogue agents. In these settings, an\nagent aims to complete a particular task with its ut-\nterances (Smith and Hipp, 1994). Goal-directed\nagents have been explored in contexts such as\npersonal assistants (McTear, 2002; Budzianowski\net al., 2018; Williams et al., 2014), recommenda-\ntion systems (Liu et al., 2010; Kang et al., 2019),\neducation (Yuan et al., 2008), and negotiation (He\net al., 2018; Lewis et al., 2017). While there\nare multiple approaches to constructing dialogue\nagents, in this work we frame the problem of gen-\nerating dialogue as a sequential decision making\nproblem within a (partially observed) Markov De-\ncision Process (MDP) (Singh et al., 1999; Young\net al., 2013). Prior works that utilize such an MDP\nformulation typically aim to train a dialogue man-\nagement system (Singh et al., 2002), in which the\nagent reasons about higher-level abstractions of the\nstate of the conversation, and language generation\nis performed using a downstream procedure. Dia-\nlogue management systems have been trained using\ntechniques such as online reinforcement learning\nvia policy gradients (Gaši´c et al., 2011; He et al.,\n2018), off-policy reinforcement learning (Pietquin\net al., 2011; Yu et al., 2016) or actor-critic meth-\nods (Su et al., 2017). Our method differs from\ndialogue management systems in that CALM is an\nend-to-end system optimized for successful task\ncompletion, and performs both high-level decision\nmaking and language generation.\nRecent advancements in language models, such\nas recurrent neural networks (Sundermeyer et al.,\n2012; Asri et al., 2016; Su et al., 2016; Zhao et al.,\n2019; Wang et al., 2020a; Zhang et al., 2020) and\nattention-based architectures (Vaswani et al., 2017;\nLiu et al., 2019; Devlin et al., 2018; Brown et al.,\n2020), have spurred increasing interest in such end-\nto-end dialogue systems (Hosseini-Asl et al., 2020;\nPeng et al., 2020; Adiwardana et al., 2020). Model-\nbased approaches, in which a learned agent is sub-\nstituted for a human, allow learning to be done\nentirely within simulation without human interven-\ntion (Li et al., 2016; He et al., 2018; Kang et al.,\n2019; Lewis et al., 2017; Liu et al., 2018). In con-\n2352\nBad task context\nGood task context\nReward 0\nHello.\nHow can I help you?\nCould you help me in \nbooking a flight ticket from \nAUS to EWR?\nSure, flight 1001 meets \nyour needs, shall I book it?\nReward 1\nre-label context to \nmaximize reward\n2) Context Aware Fine-tuning1) Dialogue Task Relabeling\nimperfect \ndialogue \ndataset\n~\nHello.\nHow can I help you?\nCould you help me \nin booking a flight \nticket from AUS to \nEWR?\nLanguage Modeling \nLoss\nTask Specific Auxiliary Loss \nre-labeled \ndialogue \ndataset\n~\nSure, flight 1001 meets \nyour needs, shall I book it?\n20%\n30%50%\nFigure 2: A visual outline of CALM. We apply Task Relabeling to our static offline dataset, by swapping out the\ntask context — in this case a flight table — such that the attached dialogue becomes an optimal example of task\ncompletion. When fine-tuning on this relabeled dataset, we then apply a Task Specific Auxiliary Loss on top of the\nstandard language modeling objective; this helps the model learn to use the task context. Once trained, CALM can\nconsistently solve goal-directed dialogue tasks.\ntrast to these approaches, CALM augments the\ntraditional language modeling objective with task-\nspecific rewards in order to finetune a model that is\nmore aware of task goals, which significantly im-\nproves performance over a naïve language model\nwithout the need for simulating human responses\nin an interactive training loop. Jaques et al. (2019)\nrecently proposed a model-free, offline approach to\nundirected dialogue, or dialogue without a specific\ntask goal. Our method differs in that we aim to\nsolve goal-oriented dialogue which allows us to\noptimize task-specific objectives, and that we take\na model-based RL approach which enables us to\nleverage fine-tuned language models.\n3 Preliminaries\nIn this section, we review our notation and prob-\nlem formulation for casting dialogue within a se-\nquential decision making framework.\nPOMDP formulation. We formulate dialogue\ngeneration as a partially observable Markov de-\ncision process (POMDP) (Kaelbling et al., 1998),\nwith a state that consists of known and unknown\ncontext information about the task. Let ch ∈C(h)\ndenote the hidden context for the task, and let\nco ∈ C(o) denote the observed context. For in-\nstance, in a flight booking task, a table of available\nflights might correspond to co, while the particular\nflight that the human wants to book, which is un-\nknown to the agent, corresponds to ch. Note that\nthe reward, which requires booking the right flight,\ndepends on both hidden and observed contexts. We\ncan define such an environment as a POMDPM=\n(S,A,O,T,Z,µ0,R,γ). We denote a conversa-\ntion τ as τ := {a0,e0,...,a T}, where T denotes\nthe number of turns in a conversation and at and et\nrepresent utterances (strings of tokens) from the dia-\nlogue agent (at) and the human (et) at the t-th turn,\nrespectively. We additionally use τ<t to denote\nconversation history up to thet-th turn. We can rep-\nresent the underlying POMDP state st ∈S as the\nconcatenation of both of the contexts and the pre-\nvious conversation history st := {ch,co,τ<t}=\n{ch,co,a0,e0,...,a t−1,et−1}. However, we only\nobserve the last two elements of the state tuple,\nsuch that our observation ot ∈O at the t-th conver-\nsation turn is ot = {co,τ<t}. An action at ∈A is\nthe agent’s response to the current state st. Given\nour definition of the state, the full conversation in\na dialogue can be conveniently represented by the\nlast observation and action, {oT,aT}. An agent\nπ: O→P (A) maps observations to sets of proba-\nbility measures over the action space P(·). A tran-\nsition function T(·|st,at), represents a distribution\nover the human’s utterances, returning st+1 as the\nstate at turn t+ 1. We only consider the sparse\nreward setting with rT = R(sT,aT) ∈{0,1}de-\nnoting task completion, and rt = 0, ∀t<T . Our\nfinal reward is therefore dependent on both the con-\ntext and the dialogue: R(sT,aT) = R(τ,ch,co),\nwhere the context {co,ch}is randomly sampled for\neach dialogue from some initial distribution µ0.\nGoal-oriented dialogue. Goal-oriented dialogue\nsystems aim to maximize the expected reward of\nthe above POMDP\nE{co,ch}∼µ0,π,T[∑T\nt=0 γtR(st,at)], (1)\nwhere {ch,co}is sampled from distributionµ0. On-\npolicy RL algorithms optimize this objective via\nenvironment interaction, which is represented by a\nreal human. However, because human-in-the-loop\ntraining is expensive, we pursue an offline learning\napproach where we are given a fixed dataset and\nthere is no further interaction with the human in\nthe learning process. This dataset is composed of\nntrajectories with Doff = {c(i)\nh ,c(i)\no ,τ(i),r(i)}n\ni=1\nwith each τ(i) = {a(i)\n0 ,e(i)\n0 ,,...,a (i)\nT }and its corre-\nsponding final reward for task completion r(i). Our\n2353\ngoal is to learn the policy π(a|o) which improves\nthe dialog agent’s ability in achieving the highest\ntask reward defined in Equation 1.\nLanguage models. While conventionally a lan-\nguage model is seen simply as a sequence model\nover tokens of the form ∏T\nt=1 p(xt+1|x1:t), when\nthe sequence x1:T corresponds to a dialogue tra-\njectory τ, we can also interpret a language model\nas learning the distribution over τ. This distribu-\ntion can be factored into the product of the policy\nπ(at|τ<t) and the dynamics T(τ<t+1|τ<t,at), and\nso we can say that a language model also repre-\nsents the policy and the dynamics. Therefore, the\nmaximum likelihood objective for training or fine-\ntuning a language model on a dialogue datasetDoff\nconsisting of dialogue trajectories τ can be written\nas\nLLM(θ) = max\nθ E\nτ∼Doff\nT∑\nt=1\n(\nlog πθ(at|τ<t)\n+ logTθ(τ<t+1|τ<t,at)\n)\n, (2)\nwhere πθ(at|ot) represents a policy that generates\nnew dialogue based on the observed context and\ndialogue history, and Tθ(τ<t+1|τ<t,at) represents\nthe observed dynamics characterizing human re-\nsponses, and θ denotes parameters in π and T.\nNote that τ<t consists only of the conversation his-\ntory, and does not contain any task-specific context.\nA naïve approach to train dialogue systems is to\njointly parameterize both πand T as one language\nmodel, and optimize Equation 2 on pre-collected\nconversations Doff. This method corresponds to\nbehavioral cloning (BC) (Pomerleau, 1989).\nContext conditioning. While an agent trained us-\ning Equation 2 can learn policies and dynamics\nthat imitate human conversations, this objective\ndoes not incorporate the task goal, and may not\nproduce a policy that is more performant than the\ndataset Doff. While it is possible to input co into\nthe language model to maximize the conditional\nprobability of P(τ|co) using a conditional version\nof the language modeling objective, LCTX (θ),\nLCTX (θ) = max\nθ E\n(τ,co)∼Doff\nT∑\nt=1\n(\nlog πθ(at|τ<t,co)\n+ logTθ(ot+1|τ<t,at,co)\n)\n, (3)\ncontexts with particular task structures (e.g., a set\nof entries in a table) may not be simply processed\nas a sequence similarly to τ. Additionally, the lan-\nguage model is not pretrained to read structured\ncontext, and oftentimes the recent dialogue history\nis much more predictive of the next utterance than\nthe task context is. As a result, language models\ncan ignore the task context and only learn P(τ) de-\nspite being conditioned on co. Our approach builds\non this conditional modeling approach, but makes\na number of improvements that allow it to be more\naware of the context information, which attains\nsignificantly better results in our experiments.\n4 Context-Aware Language Modeling\nIn this section, we present our method for goal-\noriented dialogue systems, Context-Aware Lan-\nguage Modeling (CALM). CALM interprets a lan-\nguage model as a combination of a policy and a\ndynamics model in the POMDP formulation of a\ndialogue task, as described in Section 3. Under\nthis interpretation, naïve supervised finetuning on\nthe dialogue dataset can be viewed as behavioral\ncloning (BC) (Pomerleau, 1989). However, BC\nonly imitates data and does not necessarily produce\na good policy in terms of completing tasks. We\npropose to improve the policy by utilizing a task re-\nlabeling strategy (described in Section 4.1), analo-\ngous to prior task relabeling approaches (Kaelbling,\n1993; Andrychowicz et al., 2017; Pong et al., 2018;\nSavinov et al., 2018; Ghosh et al., 2019; Lynch\net al., 2020; Eysenbach et al., 2020). This relabel-\ning procedure augments the data with examples\nof near-optimal utterances, making the language\nmodel more task-aware. However, we find several\nshortcomings with this approach alone and propose\nthe following improvements. First, an expressive\nlanguage model is liable to ignore the task context,\nwhich we address by proposing an auxiliary loss\n(Section 4.2) that forces the model to utilize this\ninformation. Second, learning from structured task\ninformation is difficult and can result in models\nthat fail to capture complex task structure, so we\npropose a task pre-training procedure to improve\nthe learnability (Section 4.3). Finally, to further\nimprove performance we use a model-based plan-\nning procedure (Section 4.4) on top of the proposed\nmethod that samples multiple dialogues in parallel\nand selects the most promising candidates.\n4.1 Dialogue Task Relabeling\nLCTX (θ) defines a context-conditional maxi-\nmum likelihood objective for training an expert\nimitation policy in conjunction with a dynamics\nmodel. However, simply imitating all the dialogue\ndata does not necessarily produce the best possible\npolicy. We would like to learn a policy that pro-\n2354\nduces dialogue that is more optimal, in the sense of\nbetter maximizing the task utility, than the average\ndialogue in the dataset. Task relabeling enables\nus to learn from optimal trajectories without sim-\nply filtering the dataset for high-reward trajectories,\nwhich would unnecessarily discard potentially in-\nformative data. In the case of dialogue, we can\nperform task relabeling by considering the con-\ntext {co,ch}as defining the task. While a given\ndialogue may be unsuccessful for the context for\nwhich it was collected, it could be considered suc-\ncessful under a different context. In this case, we\ncan simply swap out {co,ch}to create optimal task\nexamples from the many sub-optimal examples pro-\nvided by Doff. Since our reward R(ch,co,τ) is a\nfunction of the dialogue and context, we can mod-\nify the reward for a given dialogue just by changing\nthe given observed context co. Using this observa-\ntion, we can relabel unsuccessful dialogues with\nsuccessful ones, and even for already successful\ndialogues there may be multiple co corresponding\nto task success, allowing us to augment the number\nof successful (ch,co,τ) tuples.\nFormally, since our POMDP includes a prior\ndistribution over contexts {ch,co}∼ µ0, there ex-\nists a posterior q(co|τ,ch) over observed contexts\nthat correspond to optimal task completion under a\ngiven τ. We can then re-label τ to be optimal under\nits context by sampling a new co from q(co|τ,ch).\nIn practice, this sampling is performed by rejection\nsampling from either µ0 or some P(co|ch); the lat-\nter, lower entropy distribution, can be preferred if\nthere is a low probability of sampling valid, high-\nreward contexts under µ0. Now, given any τ from\nan offline dataset of dialogues, we can learn from\nthe full distribution of contexts corresponding to\noptimal task completion under this dialogue.\nIn order for this relabeling procedure not to\nbias our policy towards behavior that is overly-\noptimistic about the user’s responses, it is neces-\nsary that the distribution of these responses in our\ndataset does not depend on the portion of the con-\ntext that is relabeled. For example, relabeling the\ntable of available flights for a flight booking task\nshould generally be reasonable, because the user\nis usually unaware of the flight table. On the other\nhand, relabeling the desired flight would not make\nsense, since the user’s utterances are strongly de-\npend on this. To provide another example, in a bar-\ngaining task (Lewis et al., 2017), the agent might\nfail to obtain the desired item and instead get an\nitem of lesser value. But relabeling with a con-\ntext that assigns a higher value to the item received\nwould not lead to a reasonable example, since the\nagent mainly received this item as a result of the\nuser’s responses rather than as a result of their own\nbargaining skill.\nMethods based on similar principles have pre-\nviously been proposed in the deep RL community\nfor simple parametric tasks, such as goal-reaching\nor linearly-parameterized reward functions (Kael-\nbling, 1993; Andrychowicz et al., 2017; Eysenbach\net al., 2020). However, the dialogue task relabel-\ning that we employ is particularly effective in our\nsetting, since there may be exponentially many\ncontexts that are optimal for a given dialogue (e.g.,\nmany different flight tables for a flight booking\ntask), in contrast to the simpler task parameteriza-\ntions used in prior work, where for example only\none goal might be optimal for a given trajectory\n(the one that is reached). As a result, this technique\nnot only allows us to turn sub-optimal task data\ninto optimal data, but it also allows us to greatly\nincrease the number of optimal task examples from\nwhich we can learn, which we will show leads to a\nlarge performance improvement.\n4.2 Task-Specific Auxiliary Loss\nGoal-oriented dialogue generation can be viewed\nas learning the conditional distribution P(τ|co),\nwhere τ represents the generated dialogue given a\nspecific context co. However when trained naïvely,\nlanguage models are liable to ignore this condition-\ning context, instead focusing purely on the previous\nutterances in the dialogue. In this case, the model\nis effectively only learning P(τ) despite having\nboth the capacity and the context to learn the lower-\nentropy conditional distribution P(τ|co).\nWhile dialogue tasks are by definition carried\nout through natural language, there is often an ab-\nstract high-level action αh ∈A that essentially\ndetermines the success of the task. In the case of\nthe information retrieval task that we consider in\nthis paper, these high-level actions correspond to\ndeciding which database entity to retrieve for the\nuser (e.g., suggesting a flight to the customer that\nmeets all of their needs). While these high-level ac-\ntions are theoretically learnable from correlations\nbetween the dialogue and the given context, in gen-\neral, we find that learning these correlations corre-\nsponds to a relatively small decrease in dialogue\nentropy under the model. As a result, the model\nis less incentivized to learn these correlations rele-\n2355\nvant to the task than the form of the dialogue. To\naddress this issue, we incorporate an auxiliary ob-\njective into our training, which trains the model\ndirectly to predict the abstract high-level actions\ntaken in the present dialogue. This objective effec-\ntively up-weights gradients relevant for learning the\nhigh-level actions, which further helps the model\nto utilize the context to solve the high-level task\nthrough dialogue.\nFor a given dialogue-context pair (τ,{ch,co})\nand high-level action, αh, our auxiliary objective\nis then simply to maximize the likelihood of the\nhigh-level actions taken in the dialogue:\nC(ϕ) = max\nϕ E\n(ch,co,τ,αh)∼Doff\nlog Pϕ(αh|τ,co).\n(4)\nJust like the language modeling objective, this\nclassification objective is averaged over each to-\nken in the dialogue sequence. Our full training\nobjective then becomes:\nmax\nθ,ϕ\nLCTX (θ) +β∗C(ϕ), (5)\nwhere βis a hyper-parameter and LCTX (θ) is the\nstandard context-conditional language modeling\nobjective as defined in Section 3.\n4.3 Task Pretraining\nAs observed by Liu et al. (2021), for some struc-\ntured tasks, such as table question answering, pre-\ntraining on a simplified version of the given task\nwith a synthetic context can help the model to focus\nlearning on the “skills” that are most relevant to\nutilize the task context, which leads to improved\ndownstream task performance. We instantiate this\nidea in our method by pre-training our model on a\nsimplified (dialogue-free) version of the task. In-\nstead of simultaneously modeling all the details of\nthe raw dialogue, as is required to learn P(τ|co),\nthe key observation here is that in our case the\ntask reward only depends on the tuple {ch,co,aT}.\nThis enables us to effectively learn to execute the\ntask by only modeling P(ch,aT|co), without any\ndialogue at all. By pre-training our model to first\nlearn this simplified distribution, we effectively fo-\ncus on learning the necessary skills for completing\nthe task. It is expected that the skills learned dur-\ning this pre-training phase should also generalize\nand transfer when we later perform training on the\nreal dialogue. The particular instantiation of this\nprinciple in the case of AirDialogue is described in\nSection D.\n4.4 Model-Based Dialogue Rollouts\nWhile the methodology discussed so far can pro-\nduce effective policies, language models also rep-\nresent task dynamics, as discussed in Section 3.\nWe can leverage this fact to further improve the\nperformance of our fine-tuned models by perform-\ning model-based planning at test-time, using both\nthe policy and dynamics components in concert\nto further maximize task reward. A full dialogue\ntrajectory can then be formed by concatenating this\nsampled future trajectory τ≥t with the current state\nof the dialogue τ<t i.e., τ = {τ<t,τ≥t}. We per-\nform the model-based planning by samplingksuch\nfuture trajectories from the final fine-tuned model,\nand ranking them according to an estimated reward\nfunction ˆR(τ,co) (see Appendix E.1). Then, we\nimprove upon the policy π from which we took\nthe samples by taking the action (i.e., the next ut-\nterance) at which receives the highest estimated\nreward among the sampled trajectories. This roll-\nout sampling procedure is identical to the one used\nby Lewis et al. (2017).\n5 CALM for AirDialogue\nIn this section, we instantiate our proposed\nmethod, CALM, for the AirDialogue flight booking\ntask (Wei et al., 2018). We first give an overview\nof the task, and then describe how to do relabeling\nand context conditioning on this specific task.\n5.1 AirDialogue Dataset\nDataset overview. The AirDialogue dataset (Wei\net al., 2018) is a recently published large-scale\nairline reservation dataset based on the aforemen-\ntioned task. The dataset includes 402,038 conver-\nsations. The dataset involves three distinct tasks:\nbooking, canceling, and changing flights. We de-\nscribe the booking task in detail below.\nFlight booking task. The (human) customer is\ngiven a set of 12 trip requirements, and the flight\nagent (bot) is provided with a table of 30 flights.\nThe goal of the flight agent is to book a flight from\nthe table for the customer which meets all their\nrequirements, or to correctly inform them that no\nsuch flight is available. To determine task success,\nthe flight agent must predict an explicit action at the\nend of the dialogue indicating the flight that was\nbooked or inform no flight available. See Figure 7\nfor an example conversation from the dataset.\n5.2 Processing Tables\nThe AirDialogue booking tasks require effi-\nciently querying a flight table containing flight in-\nformation (e.g., departing location, ticket price)\n2356\ngiven to the agent prior to the conversation. In\norder to successfully complete the booking task,\nthe agent needs to be able to filter, select, and inte-\ngrate information from the flight table based on the\ncustomer’s preferences inferred from the dialogue.\nInstead of treating the tables as unstructured se-\nquences (Wei et al., 2018; Jiang et al., 2021) or as\nSQL databases (Chen et al., 2020), CALM models\ntables as an observable context consisting of a set\nco = {f1,f2,f3,...,f N}of table rows. These rows\nare then input to our model as a set of embeddings\n(see appendix C and G for more details).\n5.3 Relabeling AirDialogue with CALM\nWhile the AirDialogue dataset only includes one\nflight table for each dialogue, there are potentially\nmany flight tables compatible with each dialogue\nas each flight can appear in many tables. We hence\nimplement our relabeling procedure as described in\nSection 4.1 as follows. We perform rejection sam-\npling on the observable context (i.e., the table of\nflights) co ∼q(co|τ,ch), sampling until we obtain\na new context (ch,co,τ), which gives maximum\nreward possible R(τ,ch,co) =maxco R(τ,ch,co).\nThe prior distributions p(co) and p(co|ch), from\nwhich the tables in the AirDialogue dataset were\nsampled, are provided with the dataset. By rejec-\ntion sampling from p(co|ch), we can effectively\nsample from the posterior q(co|τ,ch) within a cer-\ntain computational budget. In this setting, co de-\nnotes tables and there are exponentially many ta-\nbles which correspond to a task success under a\ngiven dialogue. Therefore, with our relabeling ap-\nproach, we increase the number of near-optimal\ntask examples exponentially, which makes it much\neasier for the language model to learn to query the\nflight table.\nOur relabeling is approximately valid according\nto the condition specified in Section 4.1. While\nthe customer does not have access to the flight ta-\nble and therefore is not directly affected by our\nrelabeling, there are still some minor edge-cases\nin which over-optimism about the dynamics could\nbe learned by our policy. If for example, in the\ndataset the customer were to occasionally reject the\nfirst flight that we suggest, our policy may learn to\nassign a small probability to the action of initially\noffering the wrong flight, relying on them subse-\nquently rejecting it such that we can later recover\nand offer the correct one. However, in practice we\nobserve that these cases are rare in AirDialogue.\nsuccess rate\nCALM (greedy) 0.88 ± 4e-3\nLM(GPT2-small) (greedy) 0.38 ± 1e-3\nAirConcierge (greedy) 0.81 ± 7e-3\nCALM (planning) 0.90 ± 2e-3\nLM(GPT2-small) (planning) 0.74 ± 7e-3\nHuman 0.88\nTable 1: Comparison of our method and baselines\nacross all tasks. Using greedy decoding, our method\nmatches human performance, greatly improving over\nbaselines. Adding roll-outs (32 samples) further im-\nproves task completion.\n5.4 Table Selection as Auxiliary Loss\nThe primary high-level action involved in Air-\nDialogue is the decision of which flight table entry,\nif any, to recommend to the user. We therefore im-\nplement our auxiliary objective as a classification\nhead on top of the language model, trained to pre-\ndict the flight table entry that meets the customer’s\nrequests. Specifically, our set of high-level actions\nA is the set of flight table rows {f1,f2,f3,...,f N}\nplus an additional item f0, corresponding to the\ncase in which no flights meet the customer’s re-\nquirements. If f∗is the flight recommended in the\ndialogue, then our auxiliary objective is:\nC(ϕ) = max\nϕ E\n(co,τ)∼Doff\nlog Pϕ(f∗|τ,co). (6)\n6 Experiments\nIn this section, we empirically evaluate the per-\nformance of CALM on AirDialogue (Wei et al.,\n2018). We first show that CALM outperforms the\nSOTA on the AirDialogue dataset by around 7%\nin the standard simulated evaluation protocol pro-\nposed by Chen et al. (2020), which prior work\ndenotes as “self-play\" (see Appendix E), and this\nmatches human-level performance as reported by\nWei et al. (2018). Beyond this, we also perform a\ncomprehensive set of ablation studies to validate\nthe necessity of each component of CALM.\nExperiment Setup and Baselines. We compare\nCALM on AirDialogue with two baselines. The\nfirst is AirConcierge, the previous SOTA on Air-\nDialogue, which explicitly parses and executes\nSQL queries from the dialogue (Chen et al., 2020).\nThe other is a standard language model (denoted\nas LM(GPT2-small)) trained on a dataset filtered\nfor successful task examples, without any of our\ncontext-aware language modeling techniques (see\nAppendix Section A for more details on dataset\nfiltering). CALM uses the fine-tuned GPT2-small\nmodel (Radford et al., 2018) as the backbone of\nthe policy and dynamics model. After learning the\n2357\nAgent: Hello, how may I assist you?\nCustomer: I would like to fly on 08/30 at 08/01, \ncan you help me with that?\nAgent: Sure, I will help you.\nCustomer: Thank you.\nAgent: May I know your airport codes?\nCustomer: My airport codes are from MCO to \nBOS.\nAgent: Do you have any specifications?\nCustomer: Yes, I need a connection in between.\nAgent: There is a connecting flight with price of \n100 and 1010 flight number. Shall I proceed?\nCustomer: Ok, proceed with the booking.\nAgent: Your seat has been reserved with \nHawaiian airline.\nCustomer: Thanks for your help.    \nAgent: Thank you for choosing us.\nSubmit: book , Melissa Clark , 1010\nAgent: Hello, how can I help you?                                                                                                                                                                           \nCustomer: My name is Betty Lee. I want to book \na flight ticket to attend my cousin's wedding.                                                                                                               \nAgent: Sure, please provide me your travelling \ndetails?                                                                                                                                                     \nCustomer: I will start my journey from Las Vegas, \nNV - LAS to Oakland, CA - OAK.                                                                                                                            \nAgent: OK, please provide me your travelling \ndates?                                                                                                                                                         \nCustomer: The departure is 04/19 and arrival is \n04/21.                                                                                                                                                      \nAgent: OK, please wait for few minutes.                                                                                                                                                                     \nCustomer: Sure.                                                                                                                                                                                             \nAgent: Thank you for waiting, we found a flight \nwith your requirement. Flight details are as \nfollows, flight number: 1008 , airline: Spirit, price: \n200 with one connection. Shall I proceed with \nbooking?  \nCustomer: OK,  you can proceed with the \nbooking.                                                                                                                                                            \nAgent: OK, your ticket has been confirmed.                                                                                                                                                                  \nCustomer: OK, Thank you.                                                                                                                                                                                    \nAgent: Thank you, for choosing us.                                                                                                                                                                          \nSubmit: book , Betty Lee , 1008\nAgent: Hello. How can I assist you?                                                                                                                                                                         \nCustomer: I am Ryan Wilson. I need to meet my \ncousin at Charlotte, can you please help me to \nbook a flight ticket from LAX to CLT?                                                                          \nAgent: Sure, can I know the travel dates please?                                                                                                                                                            \nCustomer: Sure, my journey dates are May 29 \nand May 31.                                                                                                                                                     \nAgent: Do you have any requirements?                                                                                                                                                                        \nCustomer: My departing time is evening.                                                                                                                                                                     \nAgent: Sorry, there are no flights available as per \nyour request.                                                                                                                                           \nCustomer: No problem, thank you for your \ninformation.                                                                                                                                                       \nAgent: Thank you for approaching us.                                                                                                                                                                        \nSubmit: no_flight , Ryan Wilson , 0  \nBook Book No Flight Found\nAgent: Hello. How can I assist you?                                                                                                                                                                         \nCustomer: I am planning to visit Disney world at \nWashington, IAD, can you help me with booking?                                                                                                             \nAgent: Sure, I will help you. Can I have your \nname please?                                                                                                                                                  \nCustomer: My name is Angela Perez.                                                                                                                                                                          \nAgent: May I know your source airport code?                                                                                                                                                                 \nCustomer: My source airport code is JFK.                                                                                                                                                                    \nAgent: Please share your planned journey dates.                                                                                                                                                             \nCustomer: My journey dates are June 08 and \nJune 10.                                                                                                                                                         \nAgent: Let me know your connection limit.                                                                                                                                                                   \nCustomer: I need a break in between my journey.                                                                                                                                                             \nAgent: Do you have any other specifications?                                                                                                                                                                \nCustomer: I prefer business class flight ticket.                                                                                                                                                            \nAgent: Thank you for sharing the details, there \nare no flights available with your given details.                                                                                                           \nCustomer: Ok, I will find another alternate for this \ntrip. Thank you.                                                                                                                                       \nAgent: Thank you for approaching us.                                                                                                                                                                        \nSubmit: no_flight , Angela Perez , 0\nAgent: Hello. How can I assist you?                                                                                                                                                                         \nCustomer: I want to cancel my recent reservation, \ndue to some health issue. Can you help me?                                                                                                                \nAgent: Sure, I will help you. May I know your \nname?                                                                                                                                                         \nCustomer: Thank you. I am Linda Smith.                                                                                                                                                                      \nAgent: Please wait a minute.                                                                                                                                                                                \nCustomer: Sure.                                                                                                                                                                                             \nAgent: Thanks for waiting, We found a \nreservation on your name and it has been \ncancelled successfully.                                                                                                      \nCustomer: Thank you for your help.                                                                                                                                                                          \nAgent: Thank you for reaching us.                                                                                                                                                                           \nSubmit: cancel , Linda Smith , 0\nNo Flight Found Cancel No Reservation Found\nAgent: Hello. How can I assist you?                                                                                                                                                                         \nCustomer: Due to a sudden illness of my mother, \nI would like to change my existing reservation. \nCan you help me with this?                                                                                  \nAgent: Sure, I will help you. May I know your \nname?                                                                                                                                                         \nCustomer: My name is Mary Perez.                                                                                                                                                                            \nAgent: Sorry, there is no reservation found on \nyour name.                                                                                                                                                   \nCustomer: Ok, not an issue. May be something \nwent wrong while making the reservation. Thank \nyou for providing information.                                                                                  \nAgent: Thank you for choosing us. \nSubmit: no_reservation , Mary Perez , 0 \nFigure 3: Example dialogues generated by CALM (in green) in the simulated evaluation. Despite being\nend-to-end, CALM produces highly coherent and sensible outputs.\nFigure 4: Task success as a function of the number of\nrollout samples. Note that successful task completion\nimproves with more rollout samples.\ndynamics model, both CALM and the LM(GPT2-\nsmall) can employ two different planning strate-\ngies: (1) a simple greedy decoding of the next utter-\nance (equivalent to beam search with beam-width\none) and (2) the rollout planning as described in\nSection 4.4. For AirConcierge, we only evaluate\ngreedy decoding, as this method cannot be easily\nadapted for producing full rollouts. Rollout plan-\nning requires a method for predicting the reward\nof a given dialogue, and we describe our specific\nreward predictor for AirDialogue in Appendix Sec-\ntion E.1.\nResults for Task Success. In terms of task success,\nCALM outperforms the prior SOTA (AirConcierge)\nby approximately 7%, achieving 88% task success\nwhen using greedy decoding from the language\nmodel (see Table 1). Compared with AirConcierge,\nwhere all reasoning about the task context is done\noutside of the language model, CALM does all of\nthe filtering, selecting, and responding with rele-\nvant flight table entries within the language model,\nin a fully end-to-end manner. Meanwhile, CALM\nalso improves over LM(GPT2-small) by 50% in\nterms of task success, indicating the necessity of\nour context-aware approach for goal-oriented tasks.\nCALM LM(GPT2-small) AirConcierge\nPerplexity 1.63 1.59 -\nBLEU 32.88 35.75 27.75\nTable 2: BLEU score and perplexity results. CALM\nimproves on task success without sacrificing generation\nquality.\nWe further evaluate the the performance of var-\nious methods, when utilizing the rollout planning\ntechnique. As shown in Figure 4, as the number\nof rollout samples increases, the performance im-\nproves for all methods. Remarkably, applying the\nrollout planning to CALM further increases total\ntask success by 2%, raising it to 90% and match-\ning human performance on the AirDialogue task.\nThe baseline LM(GPT2-small) benefits much more\nfrom rollout planning than CALM, and we suspect\nthat at around 90% task completion, the perfor-\nmance becomes bottlenecked by the customer bot’s\nmistakes, therefore we only observe less gain from\nrollout planning with CALM.\nResults for Language Quality. To quantitatively\nmeasure the generated language quality, we present\nperplexity and BLEU for all methods in Table 2.\nCALM performs similarly to LM(GPT2-small) and\noutperforms AirConcierge significantly.\nAblation Study. To examine the effectiveness of\neach single component in our method, we train and\nevaluate four ablations of CALM. Each of these\nablations remove one of the components in our ap-\nproach: task relabeling (Section 4.1), auxiliary loss\n(Section 4.2), and table pre-training (Section 4.3).\nBeyond this, we also examine CALM without both\ntask relabeling and pre-training. As shown in Ta-\nble 3, removing any one of these components drops\ntask success by at least 10%, and in most cases\nmuch more than that. This shows that each piece of\nour method plays a critical role in helping CALM\nto effectively learn the goal-oriented task.\n2358\nSuccess Rate\nCALM 0.88 ± 4e-3\nLM(GPT2-small) 0.38 ± 1e-3\nCALM w/o relabel, pre-train 0.42 ± 4e-3\nCALM w/o relabel 0.66 ± 1e-2\nCALM w/o pre-train 0.39 ± 3e-3\nCALM w/o auxiliary loss 0.78 ± 4e-3\nTable 3: Task success rate for various ablations of\nCALM on AirDialogue (all using greedy decoding).\nRemoving any single component from CALM drops\nperformance by at least 10%.\n7 Conclusion\nWe proposed an end-to-end framework, CALM,\nfor goal-oriented dialogue systems. Formulating\nend-to-end dialogue generation as a Markov de-\ncision process, CALM employs task relabeling\nand context-aware finetuning to steer supervised\nlearning of language models towards specific goals,\nimproving task performance drastically while pre-\nserving language quality. We show that this im-\nproves performance on AirDialogue over the previ-\nous state of the art, and matches previously reported\nhuman performance under the standard simulated\nevaluation protocol.\nCALM optimizes for task-specific measures of\nsuccess, and while such measures might be com-\nparatively simple for domains such as AirDialogue,\nin general specifying the right success measure or\nreward function may present challenges. Further-\nmore, as with all methods based on end-to-end\nlanguage models, CALM is susceptible to internal\nbiases and inconsistencies in the language model it-\nself. There is for example no constraint that ensures\nthat CALM produces truthful answers, or that it\navoids harmful or socially unacceptable outputs. A\npractical deployable dialogue system would likely\nrequire additional measures to account for such is-\nsues, analogously to how learning-based methods\nfor self-driving vehicles might require some addi-\ntional safety mechanisms to ensure constraints, and\nindeed further research on reward specification, en-\nsuring truthful outputs, and other constraint strate-\ngies for dialogue systems that combine language\nmodels and reward maximization is a promising\nand important direction.\nThe context-conditioned supervised learning\nstrategy used by CALM provides for reward maxi-\nmization, but is in general not optimal for arbitrary\nreinforcement learning problems: in general RL\nsettings, learning a value function with dynamic\nprogramming in general can attain significantly\nbetter returns than imitating high-performing tra-\njectories, by recombining good parts of multiple\ndifferent trajectories (which might individually be\nsuboptimal) (Kostrikov et al., 2021; Kumar et al.,\n2022). The simple supervised learning strategy\nworks well in the domain we tested, but extending\nCALM to use value-based reinforcement learning\nmethods is a promising direction for future work.\nIndeed, the improvement obtained from planning\non top of the CALM model likely indicates that the\nsupervised learning approach we employ has room\nfor improvement. Additionally, the auxiliary objec-\ntives and relabeling strategies we employ require\nsome amount of domain-specific design, and more\ngeneral strategies could be developed in future.\nAddressing these limitations in future work and\ndeveloping more advanced methods that combine\nend-to-end language generation via large language\nmodels with concepts from reinforcement learning\nand planning is a promising research direction for\nmaking dialogue systems more capable, while also\nmaking language models more task aware. We\nhope that CALM will serve as an indication for the\npotential of such methods.\nAcknowledgements\nThis research was supported by an Amazon-\nBAIR Commons project, as well as the Office\nof Naval Research. We thank Dilek Hakkani-tur,\nAlexandros Papangelis, Mandi Zhao, Ruiqi Zhong,\nand Yang Liu for advice and feedback.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain chat-\nbot. arXiv preprint arXiv:2001.09977.\nMarcin Andrychowicz, Filip Wolski, Alex Ray, Jonas\nSchneider, Rachel Fong, Peter Welinder, Bob Mc-\nGrew, Josh Tobin, Pieter Abbeel, and Wojciech\nZaremba. 2017. Hindsight experience replay. In\nAdvances in Neural Information Processing Systems,\npages 5055–5065.\nLayla El Asri, Jing He, and Kaheer Suleman. 2016.\nA sequence-to-sequence model for user simula-\ntion in spoken dialogue systems. arXiv preprint\narXiv:1607.00070.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018. Multiwoz-a large-\n2359\nscale multi-domain wizard-of-oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016–5026.\nChieh-Yang Chen, Pei-Hsin Wang, Shih-Chieh Chang,\nDa-Cheng Juan, Wei Wei, and Jia-Yu Pan. 2020. Air-\nconcierge: Generating task-oriented dialogue via ef-\nficient large-scale knowledge retrieval. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing: Findings , pages\n884–897.\nWenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan,\nand William Yang Wang. 2019. Semantically con-\nditioned dialog response generation via hierarchical\ndisentangled self-attention. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3696–3709.\nGrace Chung. 2004. Developing a flexible spoken dia-\nlog system using simulation. In Proceedings of the\n42nd Annual Meeting of the Association for Compu-\ntational Linguistics (ACL-04), pages 63–70.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nOndˇrej Dušek and Filip Jurcicek. 2016. Sequence-to-\nsequence generation for spoken dialogue via deep\nsyntax trees and strings. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 45–51.\nWieland Eckert, Esther Levin, and Roberto Pieraccini.\n1997. User modeling for spoken dialogue system\nevaluation. In 1997 IEEE Workshop on Automatic\nSpeech Recognition and Understanding Proceedings,\npages 80–87. IEEE.\nMihail Eric and Christopher D Manning. 2017. A copy-\naugmented sequence-to-sequence architecture gives\ngood performance on task-oriented dialogue. In Pro-\nceedings of the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Volume 2, Short Papers, pages 468–473.\nBenjamin Eysenbach, Xinyang Geng, Sergey Levine,\nand Ruslan Salakhutdinov. 2020. Rewriting history\nwith inverse rl: Hindsight inference for policy im-\nprovement. arXiv preprint arXiv:2002.11089.\nScott Fujimoto, David Meger, and Doina Precup. 2019.\nOff-policy deep reinforcement learning without ex-\nploration. In International Conference on Machine\nLearning, pages 2052–2062.\nMilica Gaši´c, Filip Jurˇcíˇcek, Blaise Thomson, Kai Yu,\nand Steve Young. 2011. On-line policy optimisation\nof spoken dialogue systems via live interaction with\nhuman subjects. In 2011 IEEE Workshop on Auto-\nmatic Speech Recognition & Understanding, pages\n312–317. IEEE.\nKallirroi Georgila, James Henderson, and Oliver Lemon.\n2006. User simulation for spoken dialogue systems:\nLearning and evaluation. In Ninth International Con-\nference on Spoken Language Processing.\nKallirroi Georgila and David Traum. 2011. Reinforce-\nment learning of argumentation dialogue policies in\nnegotiation. In Twelfth Annual Conference of the\nInternational Speech Communication Association.\nDibya Ghosh, Abhishek Gupta, Justin Fu, Ashwin\nReddy, Coline Devin, Benjamin Eysenbach, and\nSergey Levine. 2019. Learning to reach goals with-\nout reinforcement learning.\nHe He, Derek Chen, Anusha Balakrishnan, and Percy\nLiang. 2018. Decoupling strategy and generation\nin negotiation dialogues. In Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2333–2343. Association for Computational Linguis-\ntics.\nPeter A Heeman. 2009. Representing the reinforcement\nlearning state in a negotiation dialogue. In 2009\nIEEE Workshop on Automatic Speech Recognition &\nUnderstanding, pages 450–455. IEEE.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,\nSemih Yavuz, and Richard Socher. 2020. A sim-\nple language model for task-oriented dialogue. vol-\nume 33.\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen\nShen, Craig Ferguson, Agata Lapedriza, Noah Jones,\nShixiang Gu, and Rosalind Picard. 2019. Way\noff-policy batch deep reinforcement learning of im-\nplicit human preferences in dialog. arXiv preprint\narXiv:1907.00456.\nHaoming Jiang, Bo Dai, Mengjiao Yang, Tuo Zhao, and\nWei Wei. 2021. Towards automatic evaluation of\ndialog systems: A model-free off-policy evaluation\napproach. arXiv preprint arXiv:2102.10242.\nLeslie Pack Kaelbling. 1993. Learning to achieve goals.\nIn IJCAI, pages 1094–1099. Citeseer.\nLeslie Pack Kaelbling, Michael L Littman, and An-\nthony R Cassandra. 1998. Planning and acting in\npartially observable stochastic domains. Artificial\nintelligence, 101(1-2):99–134.\nDongyeop Kang, Anusha Balakrishnan, Pararth Shah,\nPaul A Crook, Y-Lan Boureau, and Jason Weston.\n2019. Recommendation as a communication game:\nSelf-supervised bot-play for goal-oriented dialogue.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1951–1961.\nIlya Kostrikov, Ashvin Nair, and Sergey Levine.\n2021. Offline reinforcement learning with implicit\nq-learning. arXiv preprint arXiv:2110.06169.\n2360\nAviral Kumar, Joey Hong, Anikait Singh, and Sergey\nLevine. 2022. When should we prefer offline rein-\nforcement learning over behavioral cloning? Inter-\nnational Conference on Learning Representations.\nEsther Levin, Roberto Pieraccini, and Wieland Eckert.\n2000. A stochastic model of human-machine interac-\ntion for learning dialog strategies. IEEE Transactions\non speech and audio processing, 8(1):11–23.\nSergey Levine, Aviral Kumar, George Tucker, and Justin\nFu. 2020. Offline reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv\npreprint arXiv:2005.01643.\nMike Lewis, Denis Yarats, Yann Dauphin, Devi Parikh,\nand Dhruv Batra. 2017. Deal or no deal? end-to-end\nlearning of negotiation dialogues. In Conference on\nEmpirical Methods in Natural Language Processing,\npages 2443–2453.\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,\nMichel Galley, and Jianfeng Gao. 2016. Deep re-\ninforcement learning for dialogue generation. In\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nBing Liu, Gokhan Tür, Dilek Hakkani-Tür, Pararth\nShah, and Larry Heck. 2018. Dialogue learning with\nhuman teaching and feedback in end-to-end trainable\ntask-oriented dialogue systems. In Proceedings of\nNAACL-HLT, pages 2060–2069.\nJingjing Liu, Stephanie Seneff, and Victor Zue. 2010.\nDialogue-oriented review summary generation for\nspoken dialogue recommendation systems. In Hu-\nman Language Technologies: The 2010 Annual Con-\nference of the North American Chapter of the Associ-\nation for Computational Linguistics, pages 64–72.\nQian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jian-\nguang Lou. 2021. Tapex: Table pre-training via\nlearning a neural sql executor. arXiv preprint\narXiv:2107.07653.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nCorey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar,\nJonathan Tompson, Sergey Levine, and Pierre Ser-\nmanet. 2020. Learning latent plans from play. In\nConference on Robot Learning , pages 1113–1132.\nPMLR.\nMichael F McTear. 2002. Spoken dialogue technol-\nogy: enabling the conversational user interface. ACM\nComputing Surveys (CSUR), 34(1):90–169.\nHongyuan Mei, Mohit Bansal, and Matthew R Walter.\n2017. Coherent dialogue with attention-based lan-\nguage models. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence , pages\n3252–3258. AAAI Press.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2020. Soloist:\nFew-shot task-oriented dialog with a single pre-\ntrained auto-regressive model. arXiv preprint\narXiv:2005.05298.\nOlivier Pietquin, Matthieu Geist, Senthilkumar Chan-\ndramohan, and Hervé Frezza-Buet. 2011. Sample-\nefficient batch reinforcement learning for dialogue\nmanagement optimization. ACM Transactions on\nSpeech and Language Processing (TSLP), 7(3):1–21.\nDean A Pomerleau. 1989. Alvinn: An autonomous\nland vehicle in a neural network. Technical report,\nCarnegie Mellon University, Artificial Intelligence\nand Psychology.\nVitchyr Pong, Shixiang Gu, Murtaza Dalal, and Sergey\nLevine. 2018. Temporal difference models: Model-\nfree deep rl for model-based control. arXiv preprint\narXiv:1802.09081.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2018. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog.\nNikolay Savinov, Alexey Dosovitskiy, and Vladlen\nKoltun. 2018. Semi-parametric topological memory\nfor navigation. arXiv preprint arXiv:1803.00653.\nJost Schatzmann, Blaise Thomson, Karl Weilhammer,\nHui Ye, and Steve Young. 2007. Agenda-based user\nsimulation for bootstrapping a pomdp dialogue sys-\ntem. In Human Language Technologies 2007: The\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics; Companion\nVolume, Short Papers, pages 149–152.\nSatinder Singh, Michael Kearns, Diane Litman, and\nMarilyn Walker. 1999. Reinforcement learning for\nspoken dialogue systems. Advances in neural infor-\nmation processing systems, 12:956–962.\nSatinder Singh, Diane Litman, Michael Kearns, and\nMarilyn Walker. 2002. Optimizing dialogue manage-\nment with reinforcement learning: Experiments with\nthe njfun system. Journal of Artificial Intelligence\nResearch, 16:105–133.\nRonnie W Smith and D Richard Hipp. 1994.Spoken nat-\nural language dialog systems: A practical approach.\nOxford University Press on Demand.\nPei-Hao Su, Paweł Budzianowski, Stefan Ultes, Mil-\nica Gasic, and Steve Young. 2017. Sample-efficient\nactor-critic reinforcement learning with supervised\ndata for dialogue management. In Proceedings of\nthe 18th Annual SIGdial Meeting on Discourse and\nDialogue, pages 147–157.\nPei-Hao Su, Milica Gasic, Nikola Mrksic, Lina Rojas-\nBarahona, Stefan Ultes, David Vandyke, Tsung-\nHsien Wen, and Steve Young. 2016. Continuously\nlearning neural dialogue management. arXiv preprint\narXiv:1606.02689.\n2361\nMartin Sundermeyer, Ralf Schlüter, and Hermann Ney.\n2012. Lstm neural networks for language modeling.\nIn Thirteenth annual conference of the international\nspeech communication association.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yun-\njie Gu. 2020a. Modelling hierarchical structure be-\ntween dialogue policy and natural language generator\nwith option framework for task-oriented dialogue sys-\ntem. arXiv preprint arXiv:2006.06814.\nZiyu Wang, Alexander Novikov, Konrad Zolna, Josh S\nMerel, Jost Tobias Springenberg, Scott E Reed,\nBobak Shahriari, Noah Siegel, Caglar Gulcehre,\nNicolas Heess, et al. 2020b. Critic regularized re-\ngression. volume 33.\nWei Wei, Quoc Le, Andrew Dai, and Jia Li. 2018. Air-\ndialogue: An environment for goal-oriented dialogue\nresearch. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3844–3854.\nJason D Williams, Matthew Henderson, Antoine Raux,\nBlaise Thomson, Alan Black, and Deepak Ramachan-\ndran. 2014. The dialog state tracking challenge series.\nAI Magazine, 35(4):121–124.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020. Hug-\ngingface’s transformers: State-of-the-art natural lan-\nguage processing.\nQingyang Wu, Yichi Zhang, Yu Li, and Zhou Yu. 2019a.\nAlternating recurrent dialog model with large-scale\npre-trained language models.\nYifan Wu, George Tucker, and Ofir Nachum. 2019b.\nBehavior regularized offline reinforcement learning.\narXiv preprint arXiv:1911.11361.\nSteve Young, Milica Gaši ´c, Blaise Thomson, and Ja-\nson D Williams. 2013. Pomdp-based statistical spo-\nken dialog systems: A review. Proceedings of the\nIEEE, 101(5):1160–1179.\nZhou Yu, Ziyu Xu, Alan W Black, and Alexander Rud-\nnicky. 2016. Strategy and policy learning for non-\ntask-oriented conversational systems. In Proceedings\nof the 17th annual meeting of the special interest\ngroup on discourse and dialogue, pages 404–412.\nTangming Yuan, David Moore, and Alec Grierson. 2008.\nA human-computer dialogue system for educational\ndebate: A computational dialectics approach. Inter-\nnational Journal of Artificial Intelligence in Educa-\ntion, 18(1):3–26.\nYichi Zhang, Zhijian Ou, and Zhou Yu. 2020. Task-\noriented dialog systems that consider multiple appro-\npriate responses under the same context. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 34, pages 9604–9611.\nTiancheng Zhao, Kaige Xie, and Maxine Eskenazi.\n2019. Rethinking action spaces for reinforcement\nlearning in end-to-end dialog agents with latent vari-\nable models. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1, pages 1208–1218.\n2362\nA AirDialogue Dataset Filtering\nWhen training the LM(GPT2-small) and Cus-\ntomer Bot, we filter the dataset by only keeping the\nsuccessful task examples. This is be achieved by\nsimultaneously checking for successful task com-\npletion and whether a set of simple string matching\nheuristics are satisfied in the dialogue. Our heuris-\ntics aim to ensure that strings corresponding to\neach of the customer’s flight requirements and the\ncustomer’s goal are explicitly present in the dia-\nlogue. This combination of filtering steps reduces\nthe size of the training set by 26%. Despite this,\nwe find that this is still more than enough data for\nthe model to successfully learn the task.\nA.1 Rollout Planning\nIn Figure 5, we show the rollout planning proce-\ndure, which described in Section 4.4.\nFigure 5: Our dialogue rollout planning procedure.\nTo generate our response, we sample entire dialogues\nfrom the language model and then re-rank the predicted\ndialogues with a reward function.\nB Training Our Customer Bot\nOur customer bot is fine-tuned from GPT2-small\n(124M parameters), using the standard language\nmodeling objective. We used the Huggingface\nTransformers library’s implementation of GPT2\n(Wolf et al., 2020). The customer’s flight require-\nments are provided to the model as a prefix to the\ndialogue, which formatted as a comma separated\nlist consisting of the customer’s goal and flight re-\nquirements. We trained the customer bot for maxi-\nmum 10 epochs with early stopping on the filtered\ndataset. For training, it takes around 1 day on 4\nGPUs. Specifically, we trained using Adam with\nlearning rate 1e-4 and batch size 8. Our customer\nbot achieves a perplexity of 1.47 on the develop-\nment set and a BLEU score of 38.5.\nC Fight Agent Bot Details\nAll our flight agent bots are fine-tuned from\nGPT2-small (124M parameters) using the stan-\ndard language modeling objective. We used the\nHuggingface Transformers library’s implementa-\ntion of GPT2 (Wolf et al., 2020). Similar as the\ncustomer bot, we trained for maximum 10 epochs\nwith early stopping on the filtered dataset, which\ntakes roughly 1 day on 4 GPUs. Specifically, we\ntrained using Adam with learning rate 1e-4 and\nbatch size 8. We implement the final action predic-\ntion as a sequence of tokens generated at the end\nof each dialogue. The flight table is passed to the\nmodel as a prefix of flight embeddings, where each\nembedding is produced by summing embeddings\ncorresponding to each attribute of a given flight\n(e.g., flight arrival/departure day/location, flight\nprice, etc.).\nD AirDialogue Task Pretraining\nInitialized using GPT2-small (124M parame-\nters), we further pre-train our flight-agent bots by\ntraining on simplified task sequences. Specifically,\nthese sequences consist of our flight table followed\nby a comma separated list of the customer’s flight\nrequirements and a string representing the final ac-\ntion taken. We also apply our auxiliary loss and\ntask-relabeling techniques during this pre-training.\nWe pre-train on 4 million unique samples, using\nbatch size 64 and Adam with learning rate 1e-4,\nwhich takes around 2 days on 4 GPUs. During\npre-training, we found that it took around 2 mil-\nlion unique samples before the model suddenly\nstarted to learn the task of querying the flight ta-\nble, and it took roughly 2 million more samples\nbefore it became proficient at querying the table.\nBoth the unusual progression of learning during\nthis pre-training phase and the high sample com-\nplexity needed to learn the task, indicates the dif-\nficulty in learning to query the flight table. This\ncalls for future work about further investigate the\nchallenges in learning complex logical functions\nusing neural networks.\nE Self-Play Evaluation\nPrior works primarily evaluate bots for the flight\nagent through “self-play\" (Chen et al., 2020; Wei\net al., 2018). We follow the same evaluation proto-\ncol in our work. Basically, we train a bot to play the\nrole of the customer during evaluation and compute\ntask success by simulating conversations against\nthis bot. We run all self-play evaluations on the\nsame subset of 1,000 dialogue scenarios, randomly\nselected from the validation set.\nAll models are evaluated against the same cus-\ntomer bot. including models for the baselines. We\nfind that when running against our self-play bot,\n2363\nFigure 6: Our reward prediction method. We train\na model to parse the customer’s flight requirements\nfrom the dialogue. We execute these flight requirements\nagainst the table and compare the output to the flight\nthat was actually booked; this determines the reward\n(i.e. if the correct flight was booked or not).\ntask completion success for prior methods is in-\ncreased, sometimes by more than 8% (from what\nwas reported by such prior works under the same\nevaluation setting). The only difference is the spe-\ncific model used for customer’s side of the con-\nversation, and we conjecture that this difference\nis likely due to the architecture difference and\nthe details of our dataset filtering. This signifi-\ncant change in evaluation performance compared\nwith prior works, not only indicates the quality\nof our customer bot, but also suggests the impor-\ntance of accounting for these factors in evaluating\nand comparing dialogue systems. We release the\ncode and model weights for our customer bot at\nhttps://sea-snell.github.io/CALM_LM_site/.\nE.1 AirDialogue Reward Predictor for\nRollout Planning\nTo execute rollout planning, we need a reward\npredictor which can estimate whether a given dia-\nlogue is a successful example of task completion or\nnot. In the case of AirDialogue, we found that\nthe most robust way to estimate this reward is\nthe following: we first fine-tune a RoBERTa-base\nmodel (123M parameters) to predict the customer’s\nground-truth goal and flight requirements from the\nset of dialogues in the training set. We used the\nHuggingface Transformers library’s implementa-\ntion of RoBERTa (Wolf et al., 2020). We do not fil-\nter the training-set when training this model. Once\nthis model is trained, our procedure for predicting\ndialogue success is the following:\n1. Given a dialogue, use our RoBERTa model to\npredict the customer’s goal and flight require-\nments.\n2. We then execute this predicted information\nagainst the agent’s flight table and reservation\ndep. city ret. city dep. month ret. month\n0.76 0.76 0.77 0.77\ndep. day ret. day dep. time ret. time\n0.76 0.76 0.94 0.94\nclass price connections airline\n0.92 0.37 0.95 0.97\nTable 4: Our RoBERTa parser’s accuracy in pre-\ndicting each of the customer’s flight requirements.\nThe parser predicts 5 out of 12 flight requirements with\n>90% accuracy and 11 out of 12 with >70% accuracy.\nThe price requirement has the lowest accuracy because\nit is often not explicitly mentioned in the dialogue; the\nmodel has to rely on priors for prediction in these cases.\nflag, to produce a set of valid final actions.\n3. If the final action taken in the dialogue is\nwithin the set of predicted final actions, then\npredict that the current dialogue is successful,\notherwise predict that it is unsuccessful.\nSee Figure 6 for a visual illustration of this proce-\ndure. Our model obtains 94% accuracy in predict-\ning the reward of the dialogues in the validation set\n(see Table 4 for a more extensive breakdown of the\nmodel’s accuracy).\nF Example Conversation in AirDialogue\nIn Figure 7, we showcase a specific example for\nthe conversation in AirDialogue.\nFigure 7: An example conversation in AirDialogue.\nConversations generally begin with a greeting followed\nby some questioning / information gathering, and then\nfinally the agent suggests a flight before ending the\nconversation.\nG Previous Approaches to Flight Table\nProcessing\nPrior works (Wei et al., 2018; Jiang et al., 2021)\ntypically input the table directly into a language\nmodel, expecting that the skill of querying the table\nwill be naturally learned via the standard language\nmodeling objective. We found this approach to\n2364\nunder-perform in our experiments. These findings\nare also consistent with recent works which show\nthat pre-training transformers for querying tables\ncan significantly improve the transformer’s perfor-\nmance on downstream tasks which use tables (Liu\net al., 2021). AirConcierge (Chen et al., 2020)\ntakes a different approach, and explicitly predicts\nand executes SQL queries based on the dialogue.\nThis approach obtains the SOTA task success on\nAirDialogue, but it involves several complex com-\nponents, requires the ability to preform semantic\nparsing on the dialogue, and of course requires ad-\nditional domain knowledge about the format and\nstructure of the flight table, which reprsents the\ntask context. In our work, we show that applying\nCALM for AirDialogue can close this gap by in-\nducing task learning from language models and\nachieve end-to-end learning from the flight table,\nwithout sacrificing the generated language quality.\nH Error Analysis\nIn Table 5 we present a detailed breakdown of\nmodel errors. As expected, determining the flight to\nbook, if any, is consistently shown to be the most\nchallenging sub-task, as evidenced by the lower\n“flight success rate\" and the lower F1 scores for\n“no flight\", “book\", and “change\" on LM (GPT2-\nsmall). In particular, “change\" has a low recall, pre-\ncision, and F1 score for all models because it makes\nup a very small 0.4% of the training data. Lastly,\nthe “constraint success\" row shows that even when\nCALM books the wrong flight, the flight it does\nbooks meets >80% of the customer’s flight require-\nments on average.\n2365\nCALM LM (GPT2-small) AirConcierge\nfull success rate 0.88±4e-3 0.38±1e-3 0.81±7e-3\nstatus success rate 0.92±3e-3 0.84±2e-3 0.90±1e-3\nflight success rate 0.88±4e-3 0.39±1e-3 0.82±5e-3\nname accuracy rate 0.99±2e-3 1.0±8e-4 0.99±1e-3\nbook R/P/F1\n0.85±8e-3 0.06±3e-3 0.81±1e-2\n0.86±8e-3 0.05±2e-3 0.70±9e-3\n0.85±6e-3 0.05±3e-3 0.75±1e-2\nno flight R/P/F1\n0.82±1e-2 0.36±9e-3 0.59±5e-3\n0.80±1e-2 0.74±5e-3 0.93±6e-3\n0.81±1e-3 0.49±9e-3 0.72±5e-3\ncancel R/P/F1\n0.98±2e-2 1.0±0.0 1.0±0.0\n0.95±3e-2 1.0±0.0 0.75±1e-2\n0.97±2e-2 1.0±0.0 0.86±7e-3\nchange R/P/F1\n0.25±8e-2 0.0±0.0 0.0±0.0\n0.33±1e-1 0.0±0.0 0.0±0.0\n0.28±1e-1 0.0±0.0 0.0±0.0\nno reservation R/P/F1\n0.99±5e-3 0.99±2e-3 0.99±3e-3\n0.99±2e-3 0.99±2e-3 0.99±3e-3\n0.99±3e-3 0.99±2e-3 0.99±3e-3\nconstraint success 0.81±9e-3 0.71±3e-3 0.89±1e-3\nTable 5: Detailed statistics for model errors. All models are evaluated with greedy decoding. In addition to the\nfull task success rate, we report success rate for each sub-component of the full task (status / flight / name). We also\nreport recall (R), precision (P), and F1 score for task success under each type of high-level action (book / no flight /\ncancel / change / no reservation). Lastly, we report the average fraction of the customer’s flight requirements that\nare met when the agent books the wrong flight (constraint success).\n2366",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8710103034973145
    },
    {
      "name": "Task (project management)",
      "score": 0.6846378445625305
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5657720565795898
    },
    {
      "name": "Task analysis",
      "score": 0.5606677532196045
    },
    {
      "name": "Language model",
      "score": 0.5480601191520691
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5062119364738464
    },
    {
      "name": "Control (management)",
      "score": 0.44711607694625854
    },
    {
      "name": "Process (computing)",
      "score": 0.4360717833042145
    },
    {
      "name": "Representation (politics)",
      "score": 0.43606099486351013
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4324477016925812
    },
    {
      "name": "Natural language processing",
      "score": 0.42699486017227173
    },
    {
      "name": "Context model",
      "score": 0.41463565826416016
    },
    {
      "name": "Human–computer interaction",
      "score": 0.4139145612716675
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}