{
  "title": "When are Lemons Purple? The Concept Association Bias of Vision-Language Models",
  "url": "https://openalex.org/W4389519568",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3028387646",
      "name": "Yingtian Tang",
      "affiliations": [
        "Electric Propulsion Laboratory (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2094817200",
      "name": "Yutaro Yamada",
      "affiliations": [
        "Electric Propulsion Laboratory (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5111090185",
      "name": "Yoyo Zhang",
      "affiliations": [
        "Electric Propulsion Laboratory (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1970869511",
      "name": "Ilker Yildirim",
      "affiliations": [
        "Electric Propulsion Laboratory (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1933349210",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4312091861",
    "https://openalex.org/W4308165658",
    "https://openalex.org/W4310280970",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W2962749469",
    "https://openalex.org/W4283388932",
    "https://openalex.org/W4287554891",
    "https://openalex.org/W4288404646",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W2970692043",
    "https://openalex.org/W4285191490",
    "https://openalex.org/W4281633595",
    "https://openalex.org/W4226464635",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W4285876068",
    "https://openalex.org/W3199101198",
    "https://openalex.org/W4312261477",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W3152698349",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W3153469116",
    "https://openalex.org/W4384811560",
    "https://openalex.org/W4312910992",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3181158454",
    "https://openalex.org/W2963518342",
    "https://openalex.org/W4307011257",
    "https://openalex.org/W3037492894",
    "https://openalex.org/W3126792443"
  ],
  "abstract": "Large-scale vision-language models such as CLIP have shown impressive performance on zero-shot image classification and image-to-text retrieval. However, such performance does not realize in tasks that require a finer-grained correspondence between vision and language, such as Visual Question Answering (VQA). We investigate why this is the case, and report an interesting phenomenon of vision-language models, which we call the Concept Association Bias (CAB), as a potential cause of the difficulty of applying these models to VQA and similar tasks. We find that models with CAB tend to treat input as a bag of concepts and attempt to fill in the other missing concept crossmodally, leading to an unexpected zero-shot prediction. We demonstrate CAB by showing that CLIP’s zero-shot classification performance greatly suffers when there is a strong concept association between an object (e.g. eggplant) and an attribute (e.g. color purple). We also show that the strength of CAB predicts the performance on VQA. We observe that CAB is prevalent in vision-language models trained with contrastive losses, even when autoregressive losses are jointly employed. However, a model that solely relies on autoregressive loss seems to exhibit minimal or no signs of CAB.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14333–14348\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nWhen are Lemons Purple? The Concept Association Bias of\nVision-Language Models\nYingtian Tang∗ ∗,♣ Yutaro Yamada∗,♢ Yoyo Zhang Ilker Yildirim ♢\n♣EPFL ♢Yale University\nyingtian.tang@epfl.ch, yutaro.yamada@yale.edu\nAbstract\nLarge-scale vision-language models such as\nCLIP have shown impressive performance on\nzero-shot image classification and image-to-\ntext retrieval. However, such performance\ndoes not realize in tasks that require a finer-\ngrained correspondence between vision and\nlanguage, such as Visual Question Answer-\ning (VQA). As a potential cause of the diffi-\nculty of applying these models to VQA and\nsimilar tasks, we report an interesting phe-\nnomenon of vision-language models, which we\ncall the Concept Association Bias (CAB). We\nfind that models with CAB tend to treat in-\nput as a bag of concepts and attempt to fill in\nthe other missing concept crossmodally, lead-\ning to an unexpected zero-shot prediction. We\ndemonstrate CAB by showing that CLIP’s zero-\nshot classification performance greatly suffers\nwhen there is a strong concept association be-\ntween an object (e.g. eggplant) and an at-\ntribute (e.g. color purple). We also show that\nthe strength of CAB predicts the performance\non VQA. We observe that CAB is prevalent\nin vision-language models trained with con-\ntrastive losses, even when autoregressive losses\nare jointly employed. However, a model that\nsolely relies on autoregressive loss seems to\nexhibit minimal or no signs of CAB.\n1 Introduction\nRecent large-scale vision-language models such\nas CLIP (Radford et al., 2021) and ALIGN (Jia\net al., 2021) have shown remarkable performance\non zero-shot classification and text-image retrieval\ntasks. These models are trained via cross-modal\ncontrastive learning on web-scale image-text pairs\nand obtain powerful multimodal representations.\nEncouraged by these strong zero-shot capabilities,\nseveral recent papers explored CLIP for more com-\nplicated vision-language tasks. The initial attempt\nmade by (Shen et al., 2022) reports near chance ac-\n∗ Equal contribution.\nCLIP: \"In this picture, the color of the lemon is purple.\"\nFigure 1: When we ask CLIP the color of the lemon\nin the above image, CLIP answers “purple”. The text\nprompt we use is “In this picture, the color of the lemon\nis [mask]”, where CLIP picks one from [red, green,\nyellow, orange, purple].\ncuracy for zero-shot performance of CLIP on VQA-\nv2 (Goyal et al., 2017), a common visual question\nanswering benchmark. However, they simply use\n“question: [question text] answer: [answer text]”\nas text input for the text encoder of CLIP, which\nmakes the prediction harder than it should be. A\nsubsequent work (Song et al., 2022) proposes a\nbetter prompt generation method. They convert\nquestions into masked prompts (e.g. “What’s in the\nbowl behind the cake” becomes “The [mask] is in\nthe bowl behind the cake”), and filter impossible\nanswers using a language model, which improves\nCLIP’s zero-shot performance on VQA-v2.\nHowever, the zero-shot performance of CLIP\non VQA-v2 is still not state-of-the-art (Shen et al.,\n2022). In this paper, we report a phenomenon,\nwhich we call Concept Association Bias (CAB), as\none of the reasons why CLIP struggles with VQA.\nTo describe this phenomenon, we present a sim-\nple image containing a “lemon” and an “eggplant”\nto CLIP, and ask what color the lemon is, as shown\nin Figure 1. Surprisingly, CLIP predicts “purple”\nwith high confidence. When we instead ask for the\ncolor of the eggplant, CLIP answers “yellow”. To\ncross-check this phenomenon, we formulate a bi-\nnary zero-shot image classification task on the same\n14333\nimage where the two labels are “yellow lemon” and\n“purple lemon”, and find that CLIP predicts “purple\nlemon” with high confidence.\nWe hypothesize that this phenomenon comes\nfrom the discrepancy between what is described\nin the image and text input, where CLIP attempts\nto fill in the missing concept. The association be-\ntween “purple” and “eggplant” is strong, so when\nasked to fill in the mask in “[mask] lemon”, predict-\ning “purple” instead of “yellow” makes more sense\nfor CLIP, because the text description of “purple\nlemon” is aligned with the image that contains both\na lemon and an eggplant more faithfully than “yel-\nlow lemon”, which only describes the lemon in the\nimage. In fact, when we randomize the color of the\nlemon and eggplant (e.g. “red” for the lemon and\n“green” for the eggplant), this bias disappears, and\nCLIP picks the color almost randomly between the\ntwo. We also find that CAB exists for more general\nobject-attribute relationship such as the part-whole\nrelationship (e.g. “humans” tend to have “clothes”\non, and “trees” tend to have “leaves”.)\nDoes CAB exist in other vision and language\nmodels as well? To answer this question, we also\ntest BLIP (Li et al., 2022), BLIP-2 (Li et al., 2023),\nand OFA (Wang et al., 2022). We find that CAB\nexists in both BLIP and BLIP-2, but not in OFA,\nwhich is trained solely with autoregressive loss.\nFinally, we demonstrate that enabling deeper\ninteraction between modalities in CLIP can miti-\ngate CAB. In particular, we show that extending\nCLIP with an additional Transformer layer on top\nand fine-tuning it on VQA is particularly helpful.\nAcross such variants of CLIP, we report that the\nlower the degree of CAB, the higher a model per-\nforms on visual question answering. However,\nwe also find that this fine-tuning method may not\nbe a comprehensive solution for the more general\nbinding problem (Greff et al., 2020), such as accu-\nrately connecting attribute and object representa-\ntions, which leaves room for further research.\n2 Related Work\nVulnerability of vision and language models\nThere are a number of papers that study the ro-\nbustness of vision and language models. Some\nprior work (Sinha et al., 2021) shows that Trans-\nformer trained via Masked Language Modeling\n(Devlin et al., 2019) is insensitive to word orders,\nsuggesting that the success of BERT largely de-\npends on learning higher-order word co-occurrence\nrather than learning syntactic and semantic ab-\nstractions. Many benchmarks are proposed to\nevaluate robustness of ImageNet models towards\nvarious perturbations including common corrup-\ntion (Hendrycks and Dietterich, 2019), image style\nchange (Hendrycks et al., 2021), and different view-\npoints (Barbu et al., 2019). Our work differs from\nthese studies that are purely based on language or\nvision, because CAB is a cross-modal phenomenon,\nwhich occurs when both image and language data\nare used.\nCompositionality in vision and language models\nThe issue of vision and language models strug-\ngling with complex compositional questions has\nbeen studied before, where researchers have pro-\nposed enhanced training methods and modified ar-\nchitectures to tackle this problem (Basu et al., 2023;\nNayak et al., 2022; Jiang et al., 2023). Bogin et al.\n(2021) tests compositional generalization of vision\nand language models. Thrush et al. (2022) intro-\nduced a probing dataset called Winoground, which\nevaluates visuo-linguistic compositionality of vi-\nsion and language models. They evaluate a diverse\nrange of state-of-the-art vision and language mod-\nels, including CLIP, but all of them perform close to\nor below random chance. A subsequent work (Di-\nwan et al., 2022) shows that Winoground requires\nnot only compositional language understanding but\nalso other abilities such as sophisticated common-\nsense reasoning and locating small objects in low\nresolution images, which most vision and language\nmodels currently lack. The work (Lewis et al.,\n2023) is the most relevant to our research, although\nit primarily deals with toy datasets. Our work\nalso reveals brittleness of vision-language models\nthrough the lens of CAB, which has been over-\nlooked in the past.\n3 The Concept Association Bias\nThe zero-shot image classification of CLIP is re-\nmarkable for images that contain a single concept.\nHowever, when there are multiple concepts in the\nimage but the text input does not cover all of them,\nthe zero-shot classification of CLIP can be signifi-\ncantly biased towards the missing concept(s). We\ncall this bias the Concept Association Bias (CAB).\nWe first showcase this bias using color recognition\ntasks.1 For this analysis, we use the Natural-Color\n1For all experiments in the main text, we use the ResNet50-\nx4 backbone for CLIP. The results are consistent with the ViT\nbackbone, which are included in the appendix.\n14334\nFigure 2: Example images from Natural-Color Dataset\n(NCD) (Anwar et al., 2022), modified for our color\nrecognition tasks so that each image contains two differ-\nent objects.\nDataset (NCD) (Anwar et al., 2022), which is a\ndataset of vegetables and fruits with a white back-\nground. We take the following objects: banana,\nbrinjal, broccoli, carrot, cherry, corn, cucumber,\nlemon, orange, plum, pomegranate, strawberry,\ntomato. We then randomly sample two images\nwith different vegetable types and place the two ob-\njects side-by-side, resulting in 494 images in total.\nExamples are shown in Figure 2.\nFor zero-shot transfer from CLIP to our color\nrecognition task, we ask for the color of one of the\nobjects in the image. The labels we use are “red”,\n“yellow”, “purple”, “green”, and “orange”, so it is\na 5-way color recognition task. When there is a\nsingle object in the image, we use the following text\nprompt: “In this picture, the color of the object is\n[mask].” When there are two objects in the image,\nwe specify one of these objects in the prompt. For\nexample, if there is a lemon and another object in\nthe image, the prompt takes the following format:\n“In this picture, the color of the lemon is [mask].”\nThe results are shown in Figure 3. We first\nnote that the zero-shot performance of CLIP on\nour color recognition task is almost perfect when\nthere is a single object per image (“Single object”\nin Figure 3). However, the classification perfor-\nmance degrades to below chance when there are\ntwo objects per image (“Two objects” in Figure 3).\nHow does this happen? We suggest that CLIP\ndoes not have a mechanism that stores object-\ncentric representation that correctly binds the ob-\nject’s name and its attribute. In another words,\nCLIP processes its input as a “bag of concepts”.\nTo inspect this possibility, we look at what kind\nof mistakes CLIP makes when there are two objects\nA and B. We find that many mistakes are derived\nfrom a common source. That is, when asked for\nthe color of object A, CLIP often predicts the color\nof object B in the image. In fact, when we measure\nthe accuracy of predicting the color of the object B\nCAB gap\nFigure 3: Zero-shot performance of CLIP on color\nrecognition tasks using NCD (Anwar et al., 2022). CLIP\nachieves almost perfect accuracy when there is a single\nobject in the image, but the accuracy significantly drops\nwith two objects. “Two object*” refer to the case in\nwhich we instead measure the accuracy of predicting\nthe color of the object B when it is asked for the color of\nthe object A, where we see 80% zero-shot accuracy. We\nclaim this gap between Two objects and Two objects*\nis a result of the Concept Association Bias (CAB).\nwhen in reality it is asked to predict the color of the\nobject A, we see that the zero-shot transfer perfor-\nmance of CLIP is much higher (“Two objects*” in\nFigure 3), approaching the single object accuracy.\nTo understand this phenomenon, we find it help-\nful to consider two variables per object, where each\nvariable represents the object’s name in the image\nand the color attribute of the object, as shown in\nFigure 4. When the colors are natural (Figure 4\n(a)), both the object “lemon” and its attribute “yel-\nlow” in the image are fully explained by the word\n“lemon” in the text prompt, resulting in the concept\nof the eggplant remaining. When CLIP performs\nzero-shot color recognition, we see that placing the\ncolor “purple” in the prompt can most faithfully\nexplain the remaining concept of the eggplant in\nthe image (Figure 4 (b)).\nThe above explanation suggests that there is a\nstrong association between the color “purple” and\nthe object “eggplant” in CLIP to the point where\n“purple” can partially explain the concept of the\neggplant. What if we break this strong associa-\ntion? Does the gap between Two objects and Two\nobjects* disappear?\nTo test this, we generate images of fruit and veg-\netable in unnatural color using Stable Diffusion\n2.0 (Rombach et al., 2022) with a prompt format\n‘[color name] [fruit/vegetable name]’, and filter\nbad images by ourselves. Examples are shown\nin Figure 5. We call this dataset UNnatural-Color\n14335\n- The color of the lemon \nis [mask].\nText\n=\nThe color of the lemon \nis [mask].- =\nLemon\nYellow\nEggplant\nPurple\n(a) Natural color\n(c) Unnatural color\nRed Eggplant\nGreen\nImage\nRed\nEggplant\nGreen\nPurple\nEggplant\nTextImage\nRed\nLemon\nEggplant\nYellow\nLemon Eggplant\nPurple\nThe color of the lemon \nis purple.- =\nLemon\nYellow\nEggplant\nPurple\n(b) Natural color ([mask] = purple)\nTextImage\nYellow\nLemon Eggplant\nPurple\nFigure 4: The concept binding diagram. Two variables\nper object represent the object name and its attribute (e.g.\ncolor), respectively. We suggest that the text prompt and\nthe image are represented as two separate “bags of con-\ncepts” in CLIP. When a pair of object-attribute concepts\nare naturally associated with each other, both concepts\ncan be accounted for by including in the prompt either\nof the object or the attribute. When only some of the\nconcepts in the image are included in the text, this leaves\nother concepts in the image unaccounted for.\nFigure 5: Examples from UNCD. Single object (Top)\nand Two objects per image (Bottom).\nDataset (UNCD). We repeat the same experiment\non UNCD. The results are shown in Figure 6. We\nsee that the zero-shot performance for a single ob-\nject is still high, suggesting that CLIP can pick up\nthe color attribute even if the color is not strongly\nassociated with the object itself. However, for the\ntwo object cases, we see that there is almost no\ndifference between Two objects and Two objects*\ntasks. In other words, CLIP predicts the two non-\nassociated colors in the image with almost equal\nchance. We also create a version of NCD, which\nwe call UNCD-v2, where we artificially change\nthe color of each fruit and vegetable of NCD to\nnon-associated color. As shown in Appendix, we\nsee a similar pattern of CAB as UNCD.\nWhy does the CAB gap disappear when objects\nAccuracy\n0.00\n0.25\n0.50\n0.75\n1.00\nSingle object Two objects Two objects*\nZero-shot transfer from CLIP to unnatural color recognition\nFigure 6: Zero-transfer performance of CLIP to color\nrecognition on UNCD, where we assign non-associated\ncolor to each vegetable. CLIP achieves 80% accuracy\nwhen there is a single object in the image. While the\naccuracy drops for Two objects, the drop is not as sig-\nnificant as the NCD case. Furthermore, the gap between\nTwo objects and Two objects* vanishes, compared to\nthe NCD case.\nare paired with random attributes in images? This\nresult arises from a common mechanism that im-\npacts both the Two objects and Two objects* tasks.\nTo see this, we go back to our diagram in Figure 4\n(c). When the colors are unnatural ( e.g., a lemon\nin red color and an eggplant in green color), then\nthe remaining bag of concepts that are yet to be\nexplained by the text include “red”, “green”, and\n“eggplant”. This is because the color “red” is not as-\nsociated with the concept of “lemon”, and therefore\nthe word “lemon” in the text prompt cannot explain\nthe color “red”, unlike the case that uses natural\ncolor. As a result, CLIP can choose either “red”\nor “green” for color recognition. And indeed, sur-\nprisingly, CLIP randomly chooses between the two\n– it does not associate the concept of “red“ with\nthe lemon, even though in the image the lemon\nunambiguously appears in red. Likewise, for the\nTwo objects* task (in which the correct prediction\nis defined as the color of object B when asked for\nobject A), CLIP essentially randomly picks one of\nthe two colors present in the image, despite the fact\nthat each object has their own very distinct color.\n3.1 CAB exists on real-world dataset\nSo far, we use NCD to verify the existence of CAB.\nHere, we test CAB on a common visual question an-\nswering benchmark: VQA-v2 (Goyal et al., 2017).\nWe perform the zero-shot transfer of CLIP to the\ncolor-related questions in VQA-v2, where our la-\nbels are beige, black, blue, brown, green, gray, pur-\nple, red, orange, pink, white, yellow , and silver.\n14336\nTree: beige > gray > green\nBanana: black > yellow > gray\n Tree: blue > green > silver\nBroccoli: brown > yellow > green\nKnife: orange > yellow > beige\nBroccoli: silver > red > gray\nFigure 7: The Concept Association Bias (CAB) in VQA-\nv2. The text prompt is in the following format: “The\ncolor of the [object] is [color].” The first word on top of\neach image indicates the word used in place of [object]\nand the remaining color names are listed in the order\nCLIP chooses them for [color].\nWe use the prompt format “The color of the [ob-\nject] is [color].” We show example images and\nthe CLIP’s zero-shot color predictions (top three,\nin decreasing order) in Figure 7. We can see that\nthese mistakes are a result of CAB. For example,\nwhen we use the prompt format “The color of the\nbanana is [color]” and the image that contains both\nbanana and black-and-white portraits of people,\nCLIP answers “black” instead of “yellow”. We\nrandomly sample 100 mistakes CLIP makes out of\nall color-related questions, and manually inspect\nthese images to identify if the mistakes are based\non CAB. We find that roughly 50 mistakes are due\nto CAB. In Section 6, we illustrate how the degree\nof the CAB affects the performance on VQA-v2 in\nmore detail.\n3.2 CAB exists for attributes other than color\nIn this section, we test whether or not CAB exists\nfor attributes beyond color. While there are various\nattributes we can evaluate, here we focus on part-\nwhole attributes. Part-whole attributes are suitable\nfor our CAB experiment because just like color, we\ncan construct a syntactically reasonable prompt by\nfinding two objects with a part-whole relationship.\nFor example, “A tree has leaves” would be a good\nexample prompt for our test, where the verb “has”\nindicates the part-whole relationship between the\ntwo objects in the sentence. To evaluate the perfor-\nmance of zero-shot transfer of CLIP on part-whole\nrecognition, we use the Rel3D dataset (Goyal et al.,\n2020), which was originally proposed to test spa-\ntial relationship understanding of vision models.\nRel3D consists of images of 3D scenes, where two\nobjects with a particular spatial relationship are sit-\nuated in each scene. Example images from Rel3D\nFigure 8: Example images from Rel3D (Goyal et al.,\n2020).\nTwo objects\nTwo objects*\n0.0 0.2 0.4\nPrediction accuracy on Rel3D\nFigure 9: Zero-shot transfer performance of CLIP to\npart-whole recognition on Rel3D (Goyal et al., 2020).\nSimilar to the color recognition task, CAB exists for\npart-whole recognition.\nare shown in Figure 8.\nWe select 8 objects and their corresponding part\nattributes as follows: (person, clothes), (camera,\nlens), (plant, leaves), (car, wheel), (cat, tail), (com-\nputer, screen), (bed, sheet), (gun, trigger). In total,\nwe collect 356 images from Rel3D that contain one\nof these pairs. Prompt examples include “In this\npicture, the human has [mask]”, “In this picture,\nthe plant has [mask]” etc., and we let CLIP pick\none from the 8 part attributes as above (e.g., clothes,\nlens, leaves, etc.).\nThe results are shown in Figure 9. We find that\nindeed, similar to the zero-shot color recognition\ntask, the part-whole recognition task also shows\nCAB. This result suggests that CAB more generally\napplies to CLIP across types of object-attribute\nrelations.\n4 How does the strength of concept\nbinding affect CAB?\nIn Section 3, we verify CAB for color recognition\nand part-whole recognition tasks. In this section,\nwe investigate if varying the strength of the binding\nbetween two words affects the degree of CAB. We\nuse ConceptNet (Speer et al., 2017) to measure the\nassociation strength between two words. Concept-\nNet is a knowledge graph that connects words with\nlabelled, weighted edges. When selecting words,\nwe focus on the edges that are labelled as “Relat-\nedTo”. For each Rel3D object name we used in\nSection 3.2, we pick 5 related words in the decreas-\ning order of association strength, as shown in Table\n1. For this concept recognition task, we use the\nfollowing prompt format: “[object] [word]” and\n14337\nobject 1 2 3 4 5\nperson human doll character statue servant\ncamera picture flash subject photographer tripod\nplant seed tree flower green cotton\ncar drive vehicle motor automobile wheels\ncat feline animal pet kitten dog\ncomputer apple desk print dell data\nbed sleeping furniture mattress place pillows\ngun bullet weapon rifle shooting pistol\nTable 1: Object names from Rel3D (the first column)\nand the top five related words from ConceptNet (Speer\net al., 2017). The smaller the column number is, the\nstronger the association to the corresponding object\nname is.\nBinding strength\nAccuracy\n0.0\n0.2\n0.4\n0.6\nRank 1 Rank 2 Rank 3 Rank 4 Rank 5\nTwo objects Two objects*\nPrediction accuracy for different binding strengths\nFigure 10: We vary the strength of concept binding, and\ncompute the accuracy for Two objects and Two objects*.\nWe see that as the association strength gets weaker, CAB\nbecomes smaller, although it is somewhat noisy.\nthe label sets are restricted to the words in the same\ncolumn. The results are shown in Figure 10. While\nit is noisy, we see that as the concept association\nbecomes weaker, CAB becomes smaller.\n5 CAB widely exists in vision-language\nmodels\nIn Section 3, we demonstrate CAB using CLIP. In\nthis section, we extend our experiments to other\nvision-language models, including BLIP (Li et al.,\n2022), BLIP-2 (Li et al., 2023), and OFA (Wang\net al., 2022).\nBLIP and BLIP-2 are both multi-modal, multi-\ntask model that has three heads and pre-training\nobjectives: the CLIP-like image-text contrastive\nloss, the binary image-text matching loss that clas-\nsifies whether the text corresponds to the image,\nand the language modeling loss, which autoregres-\nsively generates the caption for an image. For\nBLIP-2, there is the second stage of pre-training\nfor visual conditioned language generation, boot-\nstrapped from pre-trained LLM. In our experiment,\nwe treat these models with different heads sepa-\nrately and abbreviate them as “contrast”, “match”,\nand “caption”. OFA unifies various vision and\nlanguage tasks via a simple sequence-to-sequence\nframework and employs autoregressive loss for its\nobjective.\nFor comparison across models, we define the\nCAB score as:\nAcctwo object∗ − Acctwo object+ 1\n2\nwhere Acc stands for accuracy. The CAB score\nranges from 0 to 1, and a higher score indicates a\nmore severe CAB.\nIn Table 2, we report results on NCD for the\naforementioned models and order them according\nto the CAB scores. While these networks all have\nhigh single-object recognition performance, they\ndemonstrate a spectrum of levels of CAB.\nThe networks trained with contrastive or match-\ning losses (i.e. CLIP, BLIP-contrast/match, BLIP-\n2-contrast/match) have stronger CAB than those\ntrained with autoregressive loss (i.e. BLIP-caption,\nBLIP-2-caption, and OFA). Moreover, in compari-\nson with BLIP, BLIP-2 uses a large language model\nas the final decoder on top of its sub-networks, mak-\ning them more associated with the autoregressive\nloss and having less CAB scores than their counter-\nparts in BLIP.\nFurthermore, we observe that matching losses\nhave lower CAB than contrastive losses for both\nBLIP and BLIP-2. Although the two losses are\nsimilar, the matching task uses cross-attention that\njointly processes texts and images, whereas the\ncontrastive task only uses unimodal self-attention.\nBased on this observation, we hypothesize that the\ndeeper cross-modal interaction helps mitigate the\nCAB. We further investigate this in Section 6.\nIt is worth noting that the amount of CAB in\nautoregressive models is substantial. For example,\nTwo objects*’s accuracy for BLIP-caption, BLIP-2-\ncaption, and BLIP-2-FlanT5 are 0.471, 0.483, and\n0.377 respectively. This means that when joitnly\ntrained with contrastive losses, even autoregressive\nmodels are biased by concept association, resulting\nin Two objects*’s accuracy being much higher than\nrandom chance. The only model with minimal or\nalmost no CAB is OFA, which sorely relies on\nautoregressive loss.\nIn Table 3, we compare CAB scores for CLIP\nwith vision encoders in different sizes. Although\n14338\nModels Two objects Two objects* Single CAB\nCLIP 0.011 0.932 0.929 0.961\nBLIP-contrast 0.086 0.879 0.846 0.896\nBLIP-match 0.123 0.841 0.925 0.859\nBLIP-2-contrast 0.138 0.840 0.844 0.851\nBLIP-2-match 0.330 0.627 0.925 0.648\nBLIP-2-caption 0.359 0.558 0.775 0.599\nBLIP-caption 0.438 0.471 0.862 0.516\nBLIP-2-FlanT5 0.604 0.377 0.984 0.386\nOFA 0.855 0.078 0.879 0.111\nTable 2: CAB experiments using Natural Color-\nful Dataset (NCD) on multiple architectural variants.\nFlanT5 refers to BLIP-2-ViTg-FlanT5XL .\nCLIP Two objects Two objects* CAB\nViT-B/32 0.023 0.944 0.961\nViT-B/16 0.059 0.886 0.913\nViT-L/14 0.057 0.918 0.931\nViT-L/14@336 0.058 0.926 0.934\nRN50 0.011 0.932 0.961\nRN50x4 0.045 0.916 0.936\nRN50x16 0.121 0.842 0.860\nRN50x64 0.074 0.872 0.899\nRN101 0.034 0.944 0.955\nTable 3: CAB experiments for CLIP with vision en-\ncoders in different sizes.\nthe size varies greatly, the CAB score stays around\nthe same level for these different CLIP models.\nIn Table 4 and Table 5, we compare CAB with\nWinoground (Thrush et al., 2022), whose task is to\ncorrectly match two images with two captions, but\nthe key aspect is that both captions consist of the\nexact same words, albeit arranged differently. We\ncompare CLIP, BLIP-contrast/match, and BLIP-\n2-contrast/match because they are more suitable\nfor the Winoground matching task. We roughly\nsee that as CAB decreases, Winoground perfor-\nmance goes up, which is aligned with what CAB\nattempts to measure. However, we also observe\nthat using matching loss benefits more for both\nCAB and Winoground, presumably because the\nmatching loss uses cross-attention between text\nand image encoders.\n6 How can we mitigate CAB?\nFine-tuning helps reduce CAB In the last sec-\ntion, we show that CAB can be seen in vision-\nlanguage models, and it is especially prominent for\npurely contrastively trained models. In this sec-\ntion, we test our hypothesis from Section 5 that a\ndeeper modality interaction helps mitigate CAB in\nContrastive-loss network CAB Winoground-group\nCLIP 0.961 0.0724\nBLIP-contrast 0.896 0.0800\nBLIP-2-contrast 0.851 0.0850\nTable 4: CAB vs. Winoground with contrastive models.\nMatching-loss network CAB Winoground-group\nBLIP-match 0.859 0.206\nBLIP-2-match 0.648 0.235\nTable 5: CAB vs. Winoground with matching models.\na controlled experiment using CLIP.\nThe idea of using deep modality interaction on\ntop of image and text embeddings has been ex-\nplored before in (Kim et al., 2021). However, in\n(Kim et al., 2021), the image and text encoders\nare shallow unlike CLIP. In (Shen et al., 2022),\ninstead of using CLIP as is, they employ the archi-\ntecture that uses the image encoder of CLIP, the\nBERT text embeddings, and a Transformer as an\nadditional modality interaction module. They ap-\nply this model for vision and language tasks such\nas Visual Question Answering, Visual Entailment,\nand V&L Navigation tasks. The goal of (Shen\net al., 2022) was to demonstrate that the image\nencoder of CLIP is more helpful than ImageNet-\npretrained image encoders. For our architecture,\nwe use both image and text encoders from CLIP,\nand also a Transformer for modality interaction on\ntop of CLIP, as shown in Figure 11.\nWe conducted experiments in two settings: 1.\nFreezing CLIP and only fine-tuning the Trans-\nformer head, and 2. Fine-tuning both CLIP and the\nTransformer head. Following (Shen et al., 2022),\nwe use VQA-v2 to fine-tune these model variants.\nWe follow the standard pre-processing of VQA-v2,\nwhere we filter less common answers and select\nVision \nEncoder\nText \nEncoder\nModality Interaction\nImage Text Image Text\nTransformer\nText \nEncoder\nVision \nEncoder\nCLIP CLIP+Transformer head\nFigure 11: Original architecture of CLIP (Left) and the\narchitecture we use for fine-tuning on VQA-v2 (Right).\n14339\nNCD Two objects Two objects* CAB Score VQA-v2\nCLIP(Frozen) 0.352 0.094 0.371 0.542\nCLIP(Finetuned) 0.328 0.168 0.420 0.390\nTable 6: Given two objects A and B per image, when\nwe ask the color of object A, “Two objects” refer to the\ncase where we use the color of object A as true labels,\nand “Two objects*” refer to the case where we use the\ncolor of object B as true labels. For both cases, the\nTransformer head on top of CLIP are fine-tuned. See\ntext for details of the two models.\n3,129 answer vocabularies. We then formulate the\nVQA task as a classification problem over 3,129\nanswer vocabularies given images and questions as\ninput. After fine-tuning on VQA-v2, we perform\nzero-shot color recognition using NCD to evaluate\nCAB in a similar manner to Section 3. That is,\ngiven two fruits in an image, we ask the color of\none of the fruits. The results are shown in Table\n6. We can see that adding deeper modality inter-\naction reduces CAB (See Fig. 3 for comparison).\nMoreover, we also see that between these model\nvariants, the lower the CAB score is, the higher\nthe accuracy on VQA-v2 is. Does this relationship\nhold more generally? To see this, we prepare three\nother baselines: A. CLIP image encoder + BERT\ntext embeddings + Transformer head, fine-tuned\naltogether; B. The same as A. but with the CLIP\nimage encoder frozen; C. The same as A. but uses\nV&L pre-trained weights before fine-tuning. These\narchitectures are based on (Shen et al., 2022).\nV&L pre-training used the aggregated data from\nMS COCO Captions (Chen et al., 2015), Visual\nGenome Captions (Krishna et al., 2017), VQA\n(Antol et al., 2015), GQA (Hudson and Manning,\n2019), and Visual7W (Zhu et al., 2016), which re-\nsults in 9.18M image-text pairs. For C. we used\nthe publicly available pre-trained model released\nby (Shen et al., 2022). For A. and B., we train the\nmodels on our own. We detail the hyperparameters\nin the appendix. The results are shown in Figure\n12. We see that as the CAB Score becomes lower\n(i.e., as models become less susceptible to the con-\ncept association bias), the accuracy on VQA-v2\nincreases. This encourages us to reduce the bias de-\nrived from Concept Association to further improve\nvision and language models in general.\nFine-tuning alone may not necessarily solve the\nbinding problem The last section demonstrates\nthat by introducing deeper modality interaction and\nfine-tuning, we can mitigate CAB on NCD. Can\nC\nA\nB\nE\nD\nCAB Score\nPerformance on VQA-v2\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.1 0.2 0.3 0.4\nFigure 12: The lower the CAB Score (less susceptible\nto the Concept Association Bias), the higher the models\nperform on VQA-v2. A-E refers to different model\nconfigurations. A-C are detailed in the main text. D and\nE are the same as CLIP(Frozen) and CLIP(Finetuned)\nin Table 6, respectively.\nsuch a procedure solve the binding problem (Greff\net al., 2020) more generally? The binding prob-\nlem for neural networks refers to the inability of\nmodels to dynamically bind information that is dis-\ntributed across the network (Greff et al., 2020). In\nfact, we can view CAB as an instance of the bind-\ning problem because one of the causes of CAB is\nthe model’s inability to correctly bind object and\nattribute representations across modalities. If allow-\ning for deeper modality interaction and fine-tuning\nhelps to more faithfully bind attributes and objects\nacross vision and language inputs, then we should\nexpect to see accuracy improvement for the Two\nobjects setting, even on UNCD. This is because a\nmodel that can successfully localize an object and\nits attributes, and separate the representation of dif-\nferent objects, should be able to identify the color\nof the queried object even if the object is randomly\ncolored. Contrary to this prediction, we find that\nthe accuracy on Two objects for these fine-tuned\nmodels is lower than the original CLIP, except for\nthe model C, which uses large pre-training datasets.\nIn fact, we find that these fine-tuned models also\nhave lower accuracy for Two objects* (Table 7),\nindicating that the fine-tuned models most often\nchoose a color that is not present in the image. This\nsuggests that fine-tuning on VQA-v2 simply allows\nthe model to pick up real-world co-occurrence of\ncolors and objects more easily and consistently.\nTherefore, fine-tuning the Transformer head may\nnot fundamentally solve the problem of correctly\nbinding objects and their attributes.\n14340\nUNCD CLIP(Original) CLIP(Frozen) CLIP(Finetuned) A B C\nTwo objects 0.436 0.216 0.216 0.308 0.313 0.574Two objects* 0.517 0.077 0.132 0.165 0.153 0.105\nTable 7: The performance of CLIP with fine-tuned\ndeeper interaction module. We see that fine-tuning\nrather mostly harms the accuracy for Two objects, in-\nstead of improving it, which suggests that fine-tuning\nmay not solve the more general binding problem (Greff\net al., 2020). The values for CLIP(Original) are the\nsame as Figure 6. A-C refer to specific fine-tuned model\nconfigurations, detailed in the main text.\n7 Conclusion\nEvery object has a set of concepts that are roughly\nassociated with it. For instance, the object “lemon”\ncan be associated with “yellow”, “fruit”, and so on.\nSuch concept association is automatically learned\nin vision-language models, to the point where the\nword “yellow” can partially explain the object\n“lemon” in certain cases. We establish that the\nConcept Association Bias (CAB) exists for vision-\nlanguage models through a series of experiments,\nand find that the models trained with contrastive\nloss are especially affected. Furthermore, we verify\nthat the lower the degree of CAB is, the higher the\nperformance of VQA-v2 is. Contrastive models\nlike CLIP is increasingly popular in both computer\nvision and natural language processing. We hope\nour work raises awareness of the brittleness of con-\ntrastive objectives as we develop new vision and\nlanguage models.\nLimitations\nWhile we verify CAB in zero-shot transfer of con-\ntrastive based vision and language models for color\nrecognition and part-whole recognition, there are\nother object-attribute relationships that are not ex-\nplored in this paper such as object material, shape,\nand texture. Additionally, we focus our study on\nVQA, as the task format of VQA is directly ap-\nplicable to our CAB experiments. An interesting\nfuture study to complement our work is to explore\nthe effect of CAB on other vision-language tasks\n(e.g., visual entailment (Song et al., 2022)), and\nexplore other methods to mitigate CAB. Finally,\nwe focus on CLIP, BLIP, BLIP-2, and OFA in this\nstudy. Future work should also investigate other\nvision-language models that incorporate more ex-\ntensive modality interactions (e.g., FLA V A (Singh\net al., 2022) and ALBEF (Li et al., 2021). However,\ngiven that these models are widely adopted in both\ncomputer vision and natural language processing\nfor a wide variety of downstream tasks, we believe\nour results are important to the community.\nEthics statement\nAlthough our findings may not have immediate im-\nplications for the misuse of AI systems, it is essen-\ntial to acknowledge that biases and unintended be-\nhaviors exhibited by models like CAB can pose po-\ntential risks, including social bias and other forms\nof harm. Addressing these biases and limitations\nbecomes imperative to ensure the ethical and fair\nutilization of vision-language models in real-world\nscenarios. We strongly advocate for continued\nresearch and development that emphasizes trans-\nparency, fairness, and accountability in the design\nand implementation of vision-language models.\nAcknowledgements\nYY’s research is partially supported by Masason\nFoundation.\nReferences\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual Question An-\nswering. In 2015 IEEE International Conference on\nComputer Vision (ICCV), pages 2425–2433.\nSaeed Anwar, Muhammad Tahir, Chongyi Li, Ajmal\nMian, Fahad Shahbaz Khan, and Abdul Wahab\nMuzaffar. 2022. Image Colorization: A Survey and\nDataset.\nAndrei Barbu, David Mayo, Julian Alverio, William\nLuo, Christopher Wang, Dan Gutfreund, Josh Tenen-\nbaum, and Boris Katz. 2019. ObjectNet: A large-\nscale bias-controlled dataset for pushing the limits\nof object recognition models. In Advances in Neural\nInformation Processing Systems, volume 32, pages\n9453–9463.\nSamyadeep Basu, Maziar Sanjabi, Daniela Massiceti,\nShell Xu Hu, and Soheil Feizi. 2023. Augmenting\nCLIP with Improved Visio-Linguistic Reasoning.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.Nat-\nural Language Processing with Python: Analyzing\nText with the Natural Language Toolkit . O’Reilly,\nBeijing.\nBen Bogin, Shivanshu Gupta, Matt Gardner, and\nJonathan Berant. 2021. COVR: A Test-Bed for Vi-\nsually Grounded Compositional Generalization with\nReal Images. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 9824–9846, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\n14341\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollar, and\nC. Lawrence Zitnick. 2015. Microsoft COCO Cap-\ntions: Data Collection and Evaluation Server.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAnuj Diwan, Layne Berry, Eunsol Choi, David Harwath,\nand Kyle Mahowald. 2022. Why is Winoground\nHard? Investigating Failures in Visuolinguistic Com-\npositionality.\nAnkit Goyal, Kaiyu Yang, Dawei Yang, and Jia Deng.\n2020. Rel3D: A Minimally Contrastive Benchmark\nfor Grounding Spatial Relations in 3D. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 10514–10525. Curran Associates,\nInc.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in VQA\nMatter: Elevating the Role of Image Understanding\nin Visual Question Answering. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern\nRecognition, pages 6904–6913.\nKlaus Greff, Sjoerd van Steenkiste, and Jürgen Schmid-\nhuber. 2020. On the Binding Problem in Artificial\nNeural Networks.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav\nKadavath, Frank Wang, Evan Dorundo, Rahul De-\nsai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn\nSong, Jacob Steinhardt, and Justin Gilmer. 2021. The\nMany Faces of Robustness: A Critical Analysis of\nOut-of-Distribution Generalization. International\nConference on Computer Vision (ICCV).\nDan Hendrycks and Thomas Dietterich. 2019. Bench-\nmarking Neural Network Robustness to Common\nCorruptions and Perturbations. In International Con-\nference on Learning Representations (ICLR).\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan\nLe Bras, and Yejin Choi. 2021. CLIPScore: A\nReference-free Evaluation Metric for Image Caption-\ning. In Proceedings of the 2021 Conference on Em-\npirical Methods in Natural Language Processing ,\npages 7514–7528, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGQA: A New Dataset for Real-World Visual Reason-\ning and Compositional Question Answering. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6700–6709.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling Up Vi-\nsual and Vision-Language Representation Learning\nWith Noisy Text Supervision. In Proceedings of the\n38th International Conference on Machine Learning,\npages 4904–4916. PMLR.\nKenan Jiang, Xuehai He, Ruize Xu, and Xin Eric Wang.\n2023. ComCLIP: Training-Free Compositional Im-\nage and Text Matching.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. ViLT:\nVision-and-Language Transformer Without Convo-\nlution or Region Supervision. In Proceedings of the\n38th International Conference on Machine Learning,\npages 5583–5594. PMLR.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma,\nMichael S. Bernstein, and Li Fei-Fei. 2017. Visual\nGenome: Connecting Language and Vision Using\nCrowdsourced Dense Image Annotations. Interna-\ntional Journal of Computer Vision, 123(1):32–73.\nMartha Lewis, Nihal V . Nayak, Peilin Yu, Qinan Yu,\nJack Merullo, Stephen H. Bach, and Ellie Pavlick.\n2023. Does CLIP Bind Concepts? Probing Composi-\ntionality in Large Image Models.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. BLIP-2: Bootstrapping Language-Image Pre-\ntraining with Frozen Image Encoders and Large Lan-\nguage Models.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\n2022. BLIP: Bootstrapping Language-Image Pre-\ntraining for Unified Vision-Language Understanding\nand Generation. In Proceedings of the 39th Inter-\nnational Conference on Machine Learning , pages\n12888–12900. PMLR.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021. Align before Fuse: Vision and Language\nRepresentation Learning with Momentum Distilla-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 9694–9705. Curran Asso-\nciates, Inc.\nNihal V . Nayak, Peilin Yu, and Stephen Bach. 2022.\nLearning to Compose Soft Prompts for Composi-\ntional Zero-Shot Learning. In The Eleventh Interna-\ntional Conference on Learning Representations.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning Transferable Visual Models From Natural Lan-\nguage Supervision. In Proceedings of the 38th In-\nternational Conference on Machine Learning, pages\n8748–8763. PMLR.\n14342\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022. Hierarchical Text-\nConditional Image Generation with CLIP Latents.\nRoyi Rassin, Shauli Ravfogel, and Yoav Goldberg.\n2022. DALLE-2 is Seeing Double: Flaws in Word-\nto-Concept Mapping in Text2Image Models.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Björn Ommer. 2022. High-\nResolution Image Synthesis With Latent Diffusion\nModels. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 10684–10695.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mah-\ndavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan\nHo, David J. Fleet, and Mohammad Norouzi. 2022.\nPhotorealistic Text-to-Image Diffusion Models with\nDeep Language Understanding.\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit Bansal,\nAnna Rohrbach, Kai-Wei Chang, Zhewei Yao, and\nKurt Keutzer. 2022. How Much Can CLIP Benefit\nVision-and-Language Tasks? In International Con-\nference on Learning Representations.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2022. FLA V A: A Foun-\ndational Language and Vision Alignment Model. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 15638–\n15650.\nKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle\nPineau, Adina Williams, and Douwe Kiela. 2021.\nMasked Language Modeling and the Distributional\nHypothesis: Order Word Matters Pre-training for Lit-\ntle. In Proceedings of the 2021 Conference on Empir-\nical Methods in Natural Language Processing, pages\n2888–2913, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nHaoyu Song, Li Dong, Weinan Zhang, Ting Liu, and\nFuru Wei. 2022. CLIP Models are Few-Shot Learn-\ners: Empirical Studies on VQA and Visual Entail-\nment. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 6088–6100, Dublin,\nIreland. Association for Computational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptNet 5.5: An open multilingual graph of gen-\neral knowledge. In Proceedings of the Thirty-First\nAAAI Conference on Artificial Intelligence, AAAI’17,\npages 4444–4451, San Francisco, California, USA.\nAAAI Press.\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet\nSingh, Adina Williams, Douwe Kiela, and Candace\nRoss. 2022. Winoground: Probing Vision and Lan-\nguage Models for Visio-Linguistic Compositional-\nity. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n5238–5248.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022. OFA: Unifying\nArchitectures, Tasks, and Modalities Through a Sim-\nple Sequence-to-Sequence Learning Framework. In\nProceedings of the 39th International Conference on\nMachine Learning, pages 23318–23340. PMLR.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Lu-\nong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan,\nBen Hutchinson, Wei Han, Zarana Parekh, Xin Li,\nHan Zhang, Jason Baldridge, and Yonghui Wu. 2022.\nScaling Autoregressive Models for Content-Rich\nText-to-Image Generation.\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-\nFei. 2016. Visual7W: Grounded Question Answering\nin Images. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages\n4995–5004.\n14343\nAccuracy\nleft & right\nup & down\nupper-left & \ndown-right\nlarge & small\n0.00 0.25 0.50 0.75 1.00\nTwo objects Two objects*\nPrediction accuracy of different spatial arrangements\nFigure 13: The Concept Association Bias (CAB) remains regardless of the spatial configurations such as “left &\nright”, “up & down”, “upper-left & down-right”, and “large & small”. We use the same subset of NCD as in Figure\n3.\nA Details of UNCD\nWe used ‘512-base-ema.ckpt’ from Stable Diffusion’s official github repository2. We used the deafult\nhyperparameters for sampling, except we set the size of images to be 512 by 512, and we sample 4 images\nfor each prompt. We used the following negative prompt: ‘low-res, oversaturated, ugly, cartoon, grain, out\nof focus, ambiguous, blurred, split frame, out of frame, cropped, multiple frame, split panel, multi panel,\npeople, human, logo’\nB CAB is robust across different conditions\nB.1 The spatial arrangement has almost no effect on CAB\nIn our earlier experiments on NCD and UNCD, two objects are positioned side-by-side. To see if CAB\nis robust to the positioning of objects, we vary the spatial arrangement of the two objects in the image.\nConcretely, we test the following spatial configurations: left & right, up & down, and upper-left &\ndown-right. We also vary the size of the two objects for left & right, which is denoted as “large & small”.\nAs Figure 13 shows, CAB is not affected by either spatial arrangements or the object size.\nB.2 CAB persists for prompt variations\nThe original CLIP paper (Radford et al., 2021) reports that varying text prompts changes zero-shot transfer\nperformance of CLIP. Here, we test if CAB remains effective when we vary the prompt. As Figure 14\nshows, CAB is relatively stable across prompt variations. It is interesting that as long as the names of\nthe object and color (denoted as [object] and [color]) are included in the prompt, CAB exists even if text\nprompts are semantically meaningless such as: “[object] [color]”, “The color of [color] is [object]” and\n“This prompt is random [object] [color]”. Moreover, even if we negate the sentence (e.g. “The color of\n[object] is not [color]”) CAB still exists, which suggests that the text encoder of CLIP seems to ignore the\nnegation.\n2https://github.com/CompVis/stable-diffusion\n14344\nAccuracy\n[object] [color]\nThe color of [object] is [color].\nThe color of [object] is not [color].\nThe color of [color] is [object].\nThis prompt is random [object] [color].\n0.00 0.25 0.50 0.75 1.00\nTwo objects Two objects*\nPrediction accuracy of different prompts\nFigure 14: The Concept Association Bias (CAB) is relatively stable across prompt variations, including semantically\nmeaningless prompts. We use the same subset of NCD as in Figure 3\nFigure 15: Examples from UNCD-v2. Single object (Top) and Two objects per image (Bottom).\nC CAB experiments using UNCD-v2\nWe utilize the same instances of NCD, but we assign non-associated colors to each vegetable. Figure\n15 displays some example images. In Figure 16, we observe that Two objects and Two objects* exhibit\nsimilar performance, mirroring the pattern observed with UNCD discussed in the main text.\nD Inspecting object representations of CLIP\nThe section 6 demonstrates that by introducing more modality interaction, we can mitigate CAB. However,\nthis requires an additional procedure to fine-tune the newly introduced module. Can we alleviate CAB by\nspatially pooling features of the original CLIP? Such an approach would work if CLIP develops localized\nobject-centric representations. To investigate this possibility, we use NCD and take the image tokens that\ncorrespond to the left half of images (“Left Pool”), and conduct zero-shot classification to predict the\ncolor of the left object of an image. We compare the performance of this procedure with the case where\nwe use the average of all tokens in the image (“Global Pool”). If CLIP develops localized object-level\nrepresentations, we should see an increase in accuracy compared to Global Pool. However, as shown\nin Table 8, we see that the accuracy of the left pooling is lower than global pooling. (We also show the\naccuracy values based on the original CLIP CLS embeddings.) This suggests that the features of an\nobject that is positioned in the left half of the input image are propagated to the right half as well, so\ntaking the features strictly from the left side reduces the information that is necessary to accurately predict\nthe color. Therefore, when there are multiple objects, we can see that CLIP struggles with the binding\nbetween object representations and attribute representations. Future work should explore incorporating\nobject-centric learning into CLIP to guide and structure the binding between objects and their attributes.\n14345\nAccuracy\n0.00\n0.25\n0.50\n0.75\n1.00\nSingle object Two objects Two objects*\nZero-shot transfer from CLIP to unnatural color recognition\nFigure 16: Zero-transfer performance of CLIP to color recognition on UNCD-v2, where we assign non-associated\ncolor to each vegetable. CLIP achieves 80% accuracy when there is a single object in the image. While the accuracy\ndrops for Two objects, the drop is not as significant as the NCD case. Furthermore, the gap between Two objects\nand Two objects* vanishes, compared to the NCD case.\nCLS Global Pool Left Pool\nNCD 0.617 0.417 0.209\nUNCD 0.474 0.429 0.134\nTable 8: 5-way color classification accuracy using spatial pooling. The prompt format we use is “The color is\n[mask].” The original CLIP uses the CLS embedding to compute the similarity between image and text embedding.\nGlobal Pool takes the average of image tokens as their image embedding. Left Pool take the average of the tokens\ncorresponding to the left side of the image. The reason why CLS for NCD is higher than 0.5 is that sometimes the\ncolor of two vegetables in the image are the same. In that case, the zero-shot classifier’s prediction is almost always\ncorrect, which biases the overall accuracy towards 1.0.\n14346\n                     Percentage (%)0\n25\n50\n75\n100\nBLEU-2BLEU-3BLEU-4METEORROUGE-L\nCIDEr SPICE\nCLIPScore\nRefCLIPScore\nOriginal Noun swapping Word Shuffling\nFigure 17: Ratio of caption score before and after text manipulation. We can see that CLIPScore operates as a bag\nof concepts, which is not affected by either noun swapping or word shuffling, compared to other caption metrics.\nE Hyperparameter details for fine-tuning\nIn our experiment of fine-tuning the modality interaction architecture (Section 6), we used the exact same\nhyperparameters as those in (Shen et al., 2022). The specific repository we used ishttps://github.com/\nclip-vil/CLIP-ViL/tree/master/CLIP-ViL-Pretrain . They originally used a BERT text encoder\nand CLIP image encoder. When replacing the BERT text encoder with CLIP text encoder (i.e., the cases\nD and E in Figure 12), we add a linear layer on top of the CLIP text encoder to map the embedding\ndimension to be the same as the Transformer head. For all fine-tuning experiments, we trained for 5\nepochs, which is the default number of epochs in (Shen et al., 2022).\nF CLIPScore experiment: CAB suggests precaution in downstream applications of CLIP\nAs we see in Section 3, CLIP tends to treat input as a bag of concepts, which can have undesired\nconsequences when we use CLIP for downstream tasks. Here, we examine a recent application of CLIP,\nand find that it suffers from this phenomenon. CLIPScore (Hessel et al., 2021) was recently proposed\nas a way to assess the quality of image captioning models. In contrast to reference-based scores, which\ncompare the similarity between generated captions and reference captions, CLIPScore simply compares\nthe similarity between the embedding from input images and the embedding from corresponding captions\ngenerated by an image captioning model. The original paper reports high similarity of CLIPScore to\nhuman judgement, which is one of the favorable attributes of CLIPScore compared to existing reference-\nbased captioning scores. Here, we show that CLIPScore can be insensitive to swapping and shuffling of\nwords in a sentence, although humans can easily tell such differences.\nWe first use NLTK (Bird et al., 2009) to find part-of-speech tagging, and randomly shuffle the nouns\nwithin each sentence (“noun swapping”). We also prepare a baseline where we shuffle all words in each\nsentence (“word shuffling”). We then evaluate the ground truth captions of the validation set of VQA-v2\n(Goyal et al., 2017) using CLIPScore, and compare how our shuffling procedures affect CLIPScore. In\nFigure 17, we see that there is almost no effect of our text permutations on CLIPScore and its reference\naugmented version RefCLIPScore, while other reference-based score methods are affected. For example,\ngiven a sentence that reads “A man is walking into the room”, CLIPScore returns almost the same score\nfor “A room is walking into the man.” This further illustrates that CLIP treats the input sentence as a bag\nof concepts, and calls for caution when we use CLIPScore to evaluate image captioning models.\n14347\nColor as GT\nObject as GT\n0.00 0.25 0.50 0.75\nPrediction accuracy: object names as GT vs. colors as GT\nFigure 18: When both color and object names are available for ground truth labels, CLIP tends to pick object names\nover colors, suggesting that they are not completely interchangeable.\nG Are the object name and attribute interchangeable?\nWe see evidence that the word “purple” serves as a replacement for the word “eggplant” in our CAB\nexperiments so far, which leads to a caption such as “purple lemon” to represent an image of a lemon and\nan eggplant. However, it would be especially surprising if the color attribute is completely interchange-\nable with the object name. To test this, we expand the labels we use for evaluating zero-shot transfer\nperformance of CLIP. Previously, for color recognition task, we use the colors as our labels. We now\ninclude the object names as our labels in addition to the color labels. In particular, we expand the label\nset from yellow, red, ... , purple to yellow, red, ... , purple, banana, tomato, ... , eggplant. Therefore, if\nthe object names and color attributes are not completely interchangeable and the object names are more\nsuitable to explain the image than attributes, then we should see a decrease in accuracy when we use the\ncolors as our ground truth (GT) label. The results are shown in Figure 18. We see that the accuracy for\n“Color as GT” is much lower than “Object as GT”, which suggests that CLIP only uses colors when object\nnames are not available.\nH Additional related work\nPeculiarities of CLIP In the image generation community, it has been reported that state-of-the-art\nmodels such as DALL·E 2 (Ramesh et al., 2022) struggle with compositionality (Rassin et al., 2022).\nOne of the potential causes of such failure has been attributed to the use of CLIP-based image encoder\n(Ramesh et al., 2022). In fact, image generation models that do not use CLIP such as Imagen and Parti\nare known to be better at generating images that require compositional reasoning (Saharia et al., 2022;\nYu et al., 2022). However, few works go into depth to analyze the behavior of CLIP in zero-shot image\nclassification and visual question answering. Our analysis based on CAB offers a new perspective on the\nweakness of CLIP-based models for compositional reasoning.\n14348",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7526666522026062
    },
    {
      "name": "Autoregressive model",
      "score": 0.688521146774292
    },
    {
      "name": "Association (psychology)",
      "score": 0.605939507484436
    },
    {
      "name": "Artificial intelligence",
      "score": 0.597659707069397
    },
    {
      "name": "Language model",
      "score": 0.5886979103088379
    },
    {
      "name": "Object (grammar)",
      "score": 0.580605149269104
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5469553470611572
    },
    {
      "name": "Question answering",
      "score": 0.5185540318489075
    },
    {
      "name": "Image (mathematics)",
      "score": 0.5154522061347961
    },
    {
      "name": "Natural language processing",
      "score": 0.4718456268310547
    },
    {
      "name": "Shot (pellet)",
      "score": 0.44473832845687866
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4139626920223236
    },
    {
      "name": "Machine learning",
      "score": 0.3924623429775238
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.33483004570007324
    },
    {
      "name": "Linguistics",
      "score": 0.15660405158996582
    },
    {
      "name": "Mathematics",
      "score": 0.14132758975028992
    },
    {
      "name": "Statistics",
      "score": 0.11199596524238586
    },
    {
      "name": "Psychology",
      "score": 0.08281224966049194
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    }
  ],
  "cited_by": 5
}