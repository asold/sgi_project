{
    "title": "Gaze-Swin: Enhancing Gaze Estimation with a Hybrid CNN-Transformer Network and Dropkey Mechanism",
    "url": "https://openalex.org/W4390788770",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5103148043",
            "name": "Ruijie Zhao",
            "affiliations": [
                "Ningbo University"
            ]
        },
        {
            "id": "https://openalex.org/A5101513627",
            "name": "Yuhuan Wang",
            "affiliations": [
                "Ningbo University"
            ]
        },
        {
            "id": "https://openalex.org/A5072269410",
            "name": "Sihui Luo",
            "affiliations": [
                "Ningbo University"
            ]
        },
        {
            "id": "https://openalex.org/A5093709642",
            "name": "Suyao Shou",
            "affiliations": [
                "Ningbo University"
            ]
        },
        {
            "id": "https://openalex.org/A5051652746",
            "name": "Pinyan Tang",
            "affiliations": [
                "Ningbo University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2013112874",
        "https://openalex.org/W2143427281",
        "https://openalex.org/W2077819131",
        "https://openalex.org/W2479344183",
        "https://openalex.org/W1968729467",
        "https://openalex.org/W2024274943",
        "https://openalex.org/W2166964954",
        "https://openalex.org/W2130313210",
        "https://openalex.org/W6676750203",
        "https://openalex.org/W2010854031",
        "https://openalex.org/W1589443630",
        "https://openalex.org/W2166053089",
        "https://openalex.org/W2059916030",
        "https://openalex.org/W2148322785",
        "https://openalex.org/W2610695274",
        "https://openalex.org/W3204273983",
        "https://openalex.org/W4318485428",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W4312349930",
        "https://openalex.org/W4386065847",
        "https://openalex.org/W4312996499",
        "https://openalex.org/W2027879843",
        "https://openalex.org/W2895535699",
        "https://openalex.org/W2897890508",
        "https://openalex.org/W2468114283",
        "https://openalex.org/W2557669140",
        "https://openalex.org/W4390872741",
        "https://openalex.org/W4319300355",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3096147990",
        "https://openalex.org/W2981785948",
        "https://openalex.org/W2042906110",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2921627265",
        "https://openalex.org/W2998355285",
        "https://openalex.org/W4292787489",
        "https://openalex.org/W2112547031"
    ],
    "abstract": "Gaze estimation, which seeks to reveal where a person is looking, provides a crucial clue for understanding human intentions and behaviors. Recently, Visual Transformer has achieved promising results in gaze estimation. However, dividing facial images into patches compromises the integrity of the image structure, which limits the inference performance. To tackle this challenge, we present Gaze-Swin, an end-to-end gaze estimation model formed with a dual-branch CNN-Transformer architecture. In Gaze-Swin, we adopt the Swin Transformer as the backbone network due to its effectiveness in handling long-range dependencies and extracting global features. Additionally, we incorporate a convolutional neural network as an auxiliary branch to capture local facial features and intricate texture details. To further enhance robustness and address overfitting issues in gaze estimation, we replace the original self-attention in the Transformer branch with Dropkey Assisted Attention (DA-Attention). In particular, this DA-Attention treats keys in the Transformer block as Dropout units and employs a decay Dropout rate schedule to preserve crucial gaze representations in deeper layers. Comprehensive experiments on three benchmark datasets demonstrate the superior performance of our method in comparison to the state of the art.",
    "full_text": null
}