{
  "title": "Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models",
  "url": "https://openalex.org/W4377088971",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4361166467",
      "name": "King, Evan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2211417626",
      "name": "Yu, Haoxiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2604478460",
      "name": "Lee Sang-Su",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2745612355",
      "name": "Julien, Christine",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1994559819",
    "https://openalex.org/W2986271513",
    "https://openalex.org/W4385569771",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4319049323",
    "https://openalex.org/W3047975279",
    "https://openalex.org/W2021361613",
    "https://openalex.org/W3168715003",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2066021456",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W4286910674",
    "https://openalex.org/W2809485129",
    "https://openalex.org/W4235743066",
    "https://openalex.org/W4361021201",
    "https://openalex.org/W2039371422",
    "https://openalex.org/W3034618098",
    "https://openalex.org/W2037450062",
    "https://openalex.org/W3159583069",
    "https://openalex.org/W4388720459",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W1967661515",
    "https://openalex.org/W3091582525",
    "https://openalex.org/W4394049255",
    "https://openalex.org/W3099938878",
    "https://openalex.org/W2752957596",
    "https://openalex.org/W4220792580",
    "https://openalex.org/W4361021272",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4377088971",
    "https://openalex.org/W4285239927",
    "https://openalex.org/W2890735031",
    "https://openalex.org/W2560701419",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1989244791",
    "https://openalex.org/W2471380203",
    "https://openalex.org/W2258384749",
    "https://openalex.org/W4281632197"
  ],
  "abstract": "Smart home assistants function best when user commands are direct and well-specified (e.g., \"turn on the kitchen light\"), or when a hard-coded routine specifies the response. In more natural communication, however, human speech is unconstrained, often describing goals (e.g., \"make it cozy in here\" or \"help me save energy\") rather than indicating specific target devices and actions to take on those devices. Current systems fail to understand these under-specified commands since they cannot reason about devices and settings as they relate to human situations. We introduce large language models (LLMs) to this problem space, exploring their use for controlling devices and creating automation routines in response to under-specified user commands in smart homes. We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness. We address these gaps with Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained commands like \"make it cozy\" or \"help me sleep better\" by executing plans to achieve user goals, e.g., setting a mood with available devices, or devising automation routines. We implement and evaluate Sasha in a hands-on user study, showing the capabilities and limitations of LLM-driven smart homes when faced with unconstrained user-generated scenarios.",
  "full_text": "12\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large\nLanguage Models\nEVAN KING,University of Texas at Austin, USA\nHAOXIANG YU, University of Texas at Austin, USA\nSANGSU LEE, University of Texas at Austin, USA\nCHRISTINE JULIEN, University of Texas at Austin, USA\nSmart home assistants function best when user commands are direct and well-specifiedâ€”e.g., â€œturn on the kitchen lightâ€â€”or\nwhen a hard-coded routine specifies the response. In more natural communication, however, human speech is unconstrained,\noften describing goals (e.g., â€œmake it cozy in hereâ€ or â€œhelp me save energyâ€) rather than indicating specific target devices\nand actions to take on those devices. Current systems fail to understand these under-specified commands since they cannot\nreason about devices and settings as they relate to human situations. We introduce large language models (LLMs) to this\nproblem space, exploring their use for controlling devices and creating automation routines in response to under-specified\nuser commands in smart homes. We empirically study the baseline quality and failure modes of LLM-created action plans\nwith a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience\npatterns of failure that diminish their usefulness. We address these gaps with Sasha, a smarter smart home assistant. Sasha\nresponds to loosely-constrained commands like â€œmake it cozyâ€ or â€œhelp me sleep betterâ€ by executing plans to achieve user\ngoalsâ€”e.g., setting a mood with available devices, or devising automation routines. We implement and evaluate Sasha in a\nhands-on user study, showing the capabilities and limitations of LLM-driven smart homes when faced with unconstrained\nuser-generated scenarios.\nCCS Concepts: â€¢ Human-centered computing â†’Ubiquitous and mobile computing ; Ubiquitous and mobile computing\nsystems and tools ; Empirical studies in ubiquitous and mobile computing.\nAdditional Key Words and Phrases: smart environments, pervasive computing, ambient intelligence, large language models\nACM Reference Format:\nEvan King, Haoxiang Yu, Sangsu Lee, and Christine Julien. 2024. Sasha: Creative Goal-Oriented Reasoning in Smart Homes\nwith Large Language Models. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 8, 1, Article 12 (March 2024), 38 pages.\nhttps://doi.org/10.1145/3643505\n1 INTRODUCTION\nA long-standing challenge in ubiquitous computing is to develop smart spaces that cancreatively infer and respond\nto the goals of users . A person who speaks conversationally to a smart home interface like Alexaâ€”e.g., asking it to\nâ€œmake the living room cozyâ€â€”quickly discovers that these systems struggle to relate everyday human situations\nto appropriate smart home actions [17, 27, 32, 42]. When people interact with one another, unconstrained natural\nlanguage is less of a problem. If a person asks us to â€œmake the living room cozyâ€, we relate the concept of â€œcozinessâ€\nto qualities of the environment: temperature, lighting, music, etc. The right response varies based on the space,\nthe parameters the space provides, and individual preferences [25, 47]. Generally speaking, humans are good\nAuthorsâ€™ addresses: Evan King, University of Texas at Austin, Austin, TX, USA, e.king@utexas.edu; Haoxiang Yu, University of Texas at\nAustin, Austin, TX, USA, hxyu@utexas.edu; Sangsu Lee, University of Texas at Austin, Austin, TX, USA, sethlee@utexas.edu; Christine Julien,\nUniversity of Texas at Austin, Austin, TX, USA, c.julien@utexas.edu.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that\ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page.\nCopyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s).\nÂ© 2024 Copyright held by the owner/author(s).\n2474-9567/2024/3-ART12\nhttps://doi.org/10.1145/3643505\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\narXiv:2305.09802v3  [cs.HC]  25 Jan 2024\n12:2 â€¢ E. King, et al.\nat making quick semantic leaps from abstract concepts like â€œcozyâ€ to specific actions to take on the available\ndevices. The systems that drive modern-day smart homes unfortunately lack this ability.\nWhen users discover the limits of home assistantsâ€™ reasoning, they must manually install smart home routines.\nThese tools (e.g., â€œIf This Then Thatâ€ (IFTTT) [ 4]) provide a way to imbue the modern smart home with the\nillusion of intelligence, though the true source of intelligence in this case is still the user who conjures it. If\nsomeone wants to tell Alexa â€œitâ€™s party timeâ€ in order to trigger a lighting scheme and stereo configuration, they\nmust use IFTTT to hard-code the action. By a recent count, around 3,000 smart home routines from IFTTT have\nbeen installed a total of nearly 2 million times [63], which implies that users have many goals that are not achieved\nby current systems without manual intervention. Users must adapt their mental model to the systemâ€™s limitations,\nrather than the system intuitively supporting the user [14, 54].\nWe approach the challenge of flexibly supporting usersâ€™ goals in smart homes using large language models\n(LLMs). LLMs are general-purpose language models trained on diverse corpora that span much of the written\ntext and code available on the internet [ 22, 45]. These models have shown remarkably high performance on\nmany downstream tasks without a need for major model updates tailored to specific use-cases [ 11, 57]. The\nsame GPT-3 model, for instance, has been used to control robotics with natural language [30, 61] and to simulate\nhuman social dynamics in small communities [41]. Some of this generalizability is attributed to the diversity and\nscale of LLMsâ€™ training data: since this data spans disciplines and contexts, it embodies cross-cutting semantic\nrelationships in a way that data input to task-specific models does not [22, 52]. As Dey wrote in a foundational\npaper on context-aware systems, it is â€œthe richness of [human] language... [a] common understanding of how the\nworld works, and an implicit understanding of everyday situationsâ€ that enables humans to communicate in\nterms of high-level ideas rather than specific actions [19]. Given LLMsâ€™ performance at many tasks that involve\nunconstrained language and human situational contexts, we introduce their use in smart environments. We\ninvestigate their potential to bridge the conceptual gap between implicit smart home user goals and specific actions\nneeded to accomplish them, structuring our inquiry around these research questions:\nâ€¢RQ1: What unique capabilities are unlocked when LLMs are used for smart home control? We seek to uncover\nif and how LLMs can provide support for more loosely-constrained interactions in smart homes.\nâ€¢RQ2: What practical challenges will LLM-based systems present? LLMs can produce incorrect or â€œhallucinatedâ€\nresponses and are computationally expensive to run. We seek to establish a baseline understanding of these\npractical issues as they exist in smart homes.\nâ€¢RQ3: What system design choices can address these practical challenges? We seek to apply LLM system\ndesigns from other domains and gauge their usefulness and suitability for tackling the application-specific\nchallenges of smart homes.\nâ€¢RQ4: How well can this new form of smart home support user goals in unconstrained scenarios? In a real-world\nimplementation, we seek to understand how users interact with a less-constrained system, what new\ncapabilities it provides, and what limitations still exist.\nWe investigate RQ1-4 throughout this paper, which consists of the following four efforts (summarized in Fig. 1).\nIntroducing LLMs to smart spaces. There is little prior work at the intersection of LLMs and smart\nenvironments. We introduce LLMs to the problem space with a prototype system that responds to user commands\nin smart homes with concrete, executable action plans that make use of available devices and sensors.\nEmpirical motivation. We study the outputs of our prototype system to establish an empirical understanding\nof the challenges that arise when LLMs are used in smart environments. We qualitatively analyze 600 labels and\nfree-form rationales from ğ‘ = 20 human annotators regarding LLM-generated action plans, supplemented by a\nquantitative analysis. We find that LLMs can flexibly generate creative action plans in response to under-specified\nuser commands. LLMs are also prone to failures at the task, however; we reveal 7 common types of failure\nresponsible for low user satisfaction and propose methods for measuring them. We show that LLMs can realize\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:3\nÂ§3 Prototype SystemIntroducing LLMs to smart spaces\n Â§4 Empirical MotivationUnderstanding capabilities & limitations\nÂ§5,6 Designing SashaImproving system performance\nÂ§7 Implementing Sasha Observing real-world user interactions\nN = 7 \nuser study LLMs\naction \nplans\ncommands\nhomes\nfailure\nmodes\nperf.\nmetrics\nN = 20 \nsurvey\nplan \nquality\nrationale \ncoding\ncorrelation \ntesting\nN = 7\nuser study\nbuild a test home\nkey \ninsights\nempirically- \nmotivated \ndesign\nimproved\nperf.\nclarifying\nfiltering\nplanning\nfeedback\nSasha\nRQ1\nRQ2\nRQ3 RQ4\nFig. 1. Overview of the paperâ€™s four key components, their internal structure, and motivating research questions.\nimmediate user goals like â€œmake it cozy in hereâ€, as well as more complex persistent goals like â€œuse natural light\nwhen the sun is outâ€ that require automation routines.\nDesigning Sasha. Toward addressing the challenges that we identify, we propose Sasha: a smarter smart\nhome assistant. Sashaâ€™s novelty lies in an iterative reasoning process that enables it to work creatively to achieve\nuser goals while modulating LLMsâ€™ tendency to produce non-sensical or hallucinated plans. Our design addresses\nthe challenges of targeting relevant devices and incorporating user preferences that we empirically motivate. Our\nevaluations show that Sasha reduces false positives and erroneous targeting of unrelated devices.\nImplementing Sasha. We implement Sasha in a test home. We inviteğ‘ = 7 participants to give unconstrained\ncommands during user-generated scenarios, allowing us to gather user perspectives and measurements of the\nsystemâ€™s performance in a realistic setting.\nIn summary, our contributions are as follows:\nâ€¢A system that introduces LLMs to the problem of reasoning in response to user commands in smart spaces.\nâ€¢An empirical study that demonstrates the potential for LLMs to support goal-oriented interactions, while\nalso revealing specific patterns of failure that diminish LLMsâ€™ usefulness.\nâ€¢Sasha, a smarter smart home assistant that leverages a novel iterative reasoning process to produce\nhigh-quality action plans in response to under-specified user commands in different smart homes.\nâ€¢A set of evaluations of Sasha that qualitatively and quantitatively validate each step of the iterative reasoning\nprocess that the system uses to produce action plans.\nâ€¢A real-world implementation and user study of Sasha in a test home that demonstrates the systemâ€™s practical\ncapabilities when faced with unconstrained, user-generated scenarios.\nWe provide background in Section 2. We introduce LLMs to smart environments with a prototype system in\nSection 3. We conduct an empirical study in Section 4 that reveals specific avenues for further research. Section 5\nintroduces Sasha, a system that addresses some key identified challenges. We evaluate Sasha in Section 6 and\ndescribe the results of a real-world user study in Section 7. Section 8 discusses implications and future work,\nwhile Section 9 concludes.\n2 BACKGROUND & RELATED WORK\nIn the following, we situate the paper with related research in smart environments and large language models.\n2.1 Smart environments & user goals\nâ€œSmart environmentsâ€ align with Weiserâ€™s vision for ubiquitous computing, where computing capabilities em-\nbedded in everyday devices are used to unobtrusively meet peoplesâ€™ needs [59]. A smart environment relies on\nnetworked devices and sensors embedded in light bulbs, appliances, wearables, and the built environment to\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:4 â€¢ E. King, et al.\nturn on the lights at 7pm\nkeep it well-lit after sundown\ntarget actiontrigger\nhelp me see better\nModerately Under-Specified\ntarget (?)action (?)trigger (?)\nCompletely Under-Specified\nno evident target, action, or trigger. \nWell-Specified\nFig. 2. User commands have varying degrees of specificity. Left: Well-specified commands define specific actions, specific\ntarget devices, and specific triggers. Middle: Moderately under-specified commands may allude to targets, actions, and\ntriggers, but do not specify them. Right: Completely under-specified commands have no obvious targets, desired actions, or\ntriggers. We focus on moderately and completely under-specified commands.\nsense and respond to these needs [20]. Using some form of â€œintelligenceâ€ in the built environment to coordinate\ndevices and sensors has broad applications in building management [26], infrastructure [10], healthcare [6], and\nhomes [7]. We focus on smart homes, which Lutolf described as â€œthe integration of different services within\na homeâ€ that â€œincludes a high degree of . . . flexibilityâ€ [33]. We consider smart homes from a socio-technical\nperspective (i.e., how do they support people), rather than from a strictly technical perspective (i.e., what can\nthey do), since this ultimately influences whether people choose to incorporate them in their daily lives [60]. In a\nrecent socio-technical analysis of smart homes, Reisinger et al. identify customizability, automation, accessibility,\nreliability, and low latency as primary concerns [47]. Each of these influences our approach.\nOur work centers on the challenge of reasoning about available devices and sensors in a home in relation\nto the implicit goals expressed by a user in their own manner of speaking [14, 37, 40]. We frame the problem\nin terms of creating executable â€œaction plansâ€ that use the spaceâ€™s capabilities to meet a userâ€™s immediate (i.e.,\ndoing something right now) and persistent (i.e., automating something) goals. Our separation of interactions\ninto these two categories mirrors that of Clark et al., who classify interaction patterns as either immediate or\nâ€œconditionalâ€ [14]. That study found that users â€œexpect agents to understand basicgoals. If the system were able to\nsupport this, it would provide the agent with a great deal of flexibility in achieving the specified goals. â€ In these\nenvisioned goal-oriented interactions, systems should be made to infer and devise courses of action in response\nto usersâ€™ descriptions of desired end states or changes to the environment (e.g., â€œI want to watch a movieâ€) rather\nthan descriptions of specific actions [24].\nWe focus on user commands with goals that are under-specified in some aspect, i.e., they do not define specific\ntarget devices , specific actions to take on devices, and, in the case of persistent goals, specific triggers. The\nâ€œspecificityâ€ of a command falls on a spectrum, from well-specified (e.g., â€œturn on the kitchen light at 8amâ€) to\nmoderately under-specified (e.g., â€œlighten up the kitchen in the morningâ€) to completely under-specified (e.g.,\nâ€œhelp me wake upâ€) (Fig. 2). In the case of moderately under-specified commands, the goal is often clear to a human\nbut less obvious to a system, which is a common source of frustration: existing systems struggle outside of a rigid,\nhighly-specific command structure [17, 27, 32] and are generally unable to deal with complex commands [42, 54].\nFor this reason, our scope includes moderately under-specified commands (e.g., â€œturn the AC off when itâ€™s cold\noutsideâ€) along with completely under-specified commands (e.g., â€œmake it cozy in hereâ€).\nPrior work has proposed task-specific systems for inferring user goals. Noura et al. proposed the VISH grammar\nand generative dataset for modeling goal-oriented smart home utterances, using it to train task-specific natural\nlanguage models and improve understanding [36, 37]. This approach broadens the supported command structure\nof assistants by introducing a goal-oriented grammar, e.g., a user can say â€œthe blinds are too lowâ€ to raise the blinds.\nThe system does not provide support for completely under-specified commands like â€œmake it cozy in hereâ€ that\nmight entail different responses depending on the environment, nor does it provide support for persistent goals\nlike â€œhelp me save energyâ€ that require reasoning about a homeâ€™s devices and sensors. Palanca et al. approach the\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:5\nproblem of achieving user goals with a task-specific system, proposing a multi-agent approach [40]. Agents (e.g., a\nlighting agent, a TV agent, etc.) coordinate to form a suitable plan to achieve a user goal based on an ontology that\ndescribes the agentsâ€™ capabilities and relationships. Our focus on output â€œaction plansâ€ bears similarityâ€”however,\ndependence on a rigid ontology ultimately imposes assumptions about how people use devices in their homes,\nand what goals they want to accomplish. We aim for greater flexibility than existing systems.\n2.2 Applications of large language models\nLarge language models (LLMs) come from research in natural language processing (NLP) that began in the\n1950s and â€˜60s [12]. Research in NLP aims to enable computers to understand, interpret, and generate human\nlanguage in a useful way. A recent increase in NLP research activity is attributed to the introduction of the\ntransformer model by Vaswani et al. [55]. Transformer-based models like Bidirectional Encoder Representations\nfrom Transformers (BERT) [18], Generative Pre-training Transformer (GPT) [11, 44, 45], Text-to-text Transfer\nTransformer (T5) [46], and Large Language Model Meta AI (Llama) [52, 53] are trained on massive and diverse\ndatasets [22], enabling them to achieve high performance on many tasks involving natural language without a\nneed for task-specific training [11, 57]. We focus on several popular transformer models in our empirical study:\nthe commercial GPT-3.5 and GPT-4 models, as well as the Llama 2 model. These models have recently been\napplied to diverse problems in robotics [50, 61] and social science [41].\nIn smart homes, LLMs may unlock more flexible interactions between users and spaces than task-specific\nsystems. This stems from a deeper exposure to everyday situations that are embodied in their diverse training data.\nFast et al., showed, for instance, that a language model trained only on written works of fiction can be used to\nrecognize activities of daily living by leveraging semantic relationships between objects and the activities they are\nfrequently used in [21]. This embodiment of human behavior by language models is effectively a by-product of\ntheir training on massive amounts of text that contains unstructured depictions of it. However, with the exception\nof our own prior small-scale study [28], we are not aware of work at the intersection of LLMs and smart spaces.\nThat study showed that GPT-3.5 could devise creative plans in response to under-specified user commands in\na smart home. The study did not empirically establish open challenges, nor did it investigate the possibility of\nachieving persistent user goals, propose system improvements, or include a user study. We consider all four.\nIntegrating LLMs into systems remains highly subjective [43, 48]. Two overlapping practices exist to adapt\ngeneral-purpose models to specific tasks: fine tuning [64] and prompt engineering [31]. The former involves\npartially retraining the model using (input, output) pairs that reflect the desired structure of output, while the\nlatter relies on carefully-constructed natural language prompts that elicit the desired output. In some cases,\nengineered prompts may contain â€œfew-shotâ€ training examples that hint at the desired output structure, while\nprompts that contain no training examples are referred to as â€œzero-shotâ€. Fine tuning and prompt engineering\nare not mutually exclusive: generally, prompt engineering performs better on models that have already been\nfine-tuned for instruction following [57], which allows them to adhere to instructions in the prompt. In our work,\nwe utilize prompt engineering. Work with chain of thought prompting shows that deconstructing a problem\ninto several steps of reasoning (rather than a single prompt) can improve performance [ 58, 62]. We design\na prototype system based on zero-shot prompting for our initial empirical study, allowing us to characterize\nLLMsâ€™ performance at this new task in the base case. We then propose an improved system, Sasha, that adapts\nchain-of-thought prompting to smart space applications.\n3 INTRODUCING LARGE LANGUAGE MODELS TO SMART SPACES\nWe build a prototype that utilizes LLMs to produce action plans in response to user commands in smart homes.\nOur prototype works using zero-shot prompts to an LLM containing a userâ€™s natural language command alongside\na â€œhome templateâ€ in JSON that describes the rooms, devices, and sensors in a smart home. Instructions in the\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:6 â€¢ E. King, et al.\nPrompt with command \nand home template\nLLM assigns settings to \ndevices or creates \nautomation\nParse JSON content of \nresponse\nFig. 3. We begin with an initial study of LLM behavior in smart home applications using this experimental setup.\nYou are an AI that controls a smart home. You receive user commands and \ncreate automation routines in response.\nUser command: [ command ]\nDevices: { JSON }\nSensors: { JSON }\nIf there are devices relevant to the user command, respond with JSON that \ndescribes a sensor trigger and how you would change the devices (the action) \nbased on that trigger. Respond with JSON in this form: { \"status\": \n\"success\", \"trigger\": { }, \"action\": { }, \"explanation\": \n\"\" }\nIf there are no devices relevant to the user command, respond with JSON in \nthe form: { \"status\": \"failure\" }\nYou are an AI that controls a smart home. You receive user commands and \nassign settings to devices in response.\nUser command: [ command ]\nDevices: { JSON }\nIf there are devices relevant to the user command, respond with the device \nJSON with settings assigned in the form: { \"status\": \"success\", \n\"devices\": { }, \"explanation\": \"\" }\nIf there are no devices relevant to the user command, respond with JSON in the \nform: { \"status\": \"failure\" }\nFig. 4. The text of immediate and persistent goal prompts (left and right, respectively). Immediate prompts produce action\nplans that can be executed immediately, while persistent prompts produce automation routines.\nprompt guide the LLM to output a machine-parseable action planâ€”i.e., changes to device settings for immediate\ncommands and (trigger, action) pairs for automation routines, both also expressed in JSONâ€”or to reject the\nuserâ€™s request if the goal appears unattainable with the devices available. The JSON action plans generated by\nthe model are then parsed and executed. Since LLMs have not been rigorously studied in the context of smart\nenvironments, our prototype design avoids venturing into state-of-the-art methods from other domains (e.g.,\nfine-tuning or chain-of-thought prompting). This provides an understanding ofperformance in the base case along\nwith a baseline for future improvements. We propose such improvements later in Section 5. We illustrate our\nsetup in Fig. 3 and provide detail in the following.\nPrompting. Our prompts contain (1) instructions about the task, (2) a JSON â€œhome templateâ€ containing\ninformation about the homeâ€™s rooms, devices, sensors, and settings, and (3) the userâ€™s command. Prompts for\nimmediate commands instruct the model to output an action plan to execute immediately, while persistent\ncommand prompts instruct the model to output an automation routine in the form of a (trigger, action) pair. The\ntrigger is a sensor or set of sensors and the corresponding values that trigger the routine, while the action is a set\nof devices and the settings to assign to them. In both prompt types, we instruct the model to reject the userâ€™s\ncommand if the goal appears unachievable with the devices available. We illustrate the prompts in Fig. 4.\nPlanning. Given the prompt, the LLM constructs a JSON action plan that suggests a mapping of the goal\ninferred from the userâ€™s command to specific device settings that accomplish the goal. Since this â€œreasoningâ€\nultimately stems from the semantic relationships between different words, the action plan is constructed based\non the naming of rooms, devices, and settings in the JSON of the home template. A device in the template named\nlamp expresses both the functional capabilities of a lamp (e.g., luminance) as well as the semantic content of lamp\n(e.g., lamps are used when reading, lamps provide mood lighting). This motivates the descriptive naming and\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:7\nI have set the lights in all rooms to a low brightness and a warm white color to \nhelp you wind down.\nâœ…\nI have increased the brightness of the overhead lights in all rooms to make it \nless chilly in here.\nâŒ\nmake it less chilly in here\nhelp me wind down\nWhen it rains, the thermostat in the living room will be turned on and set to a \ncomfortable temperature of 72 degrees, and the overhead lights in the living \nroom and bedroom will be turned on and dimmed, with a warm orange color.\nâœ…\nmake it cozy when it rains\nturn oï¬€ the thermostat when I donâ€™t need it\nWhen the user is in the living room, the thermostat will be turned oï¬€.\nâŒ\nFig. 5. GPT-3.5 responses to smart home commands. We prompt the model with a user command along with information\nabout the devices and sensors available in the home. Responses are often remarkably creative, and often remarkably wrong.\nstructuring of the home template, since it is the primary means by which the specific (e.g., setting) and abstract\n(e.g., â€œmoodâ€) capabilities of the home are made available to the model.\nParsing. After the LLM has produced an action plan, we post-process it to extract the JSON. Our prompts\ncontain instructions to include an â€œexplanationâ€ of the changes made in the JSON for easier inspection. With this\nsystem in place, an LLM can respond to smart home commandsâ€”some examples are illustrated in Fig. 5.\n4 EMPIRICAL MOTIVATION\nToward answering RQ1-2, we conduct an empirical study using the prototype system described previously. We\n(1) construct a representative dataset of user commands and smart home configurations to use as input to our\nsystem, (2) design a survey to elicit user perspectives on the quality and modes of failure of the systemâ€™s action\nplans, (3) establish quantitative metrics on the action plans themselves, (4) and report our findings for both the\nsurvey and the quantitative analysis.\n4.1 Home templates & user commands\nWe construct a representative dataset to use as input to our system, allowing us to produce a large set of action\nplans for analysis. We describe our method for constructing â€œhome templatesâ€ to use as input to the system, then\ndescribe our procedure for creating user commands that are representative of our task.\nHome templates . We constructed three home templates by adapting data from two sources: a dataset of\nIFTTT smart home routines [63] and the layout and sensor suite from a home in the CASAs dataset [15]. The\nIFTTT data consists of the most popular â€œrecipesâ€ users have created and installed to control devices and automate\nroutines. It indicates which devices people are using and how they are using them, as well as types of goals that\nare not met by current systems without manual configuration.\nWe modeled three homes,â„1,â„2, and â„3, with different number and diversity of devices. We chose which device\ntypes to include in each home based on their popularity in the IFTTT dataset. Smart lights are the most popular,\nso â„1 models the most basic smart home with only lights. â„2 adds the next most popular types (climate control\nand entertainment), whileâ„3 again adds the next most popular types (security, a robot vacuum, and miscellaneous\nappliances). Each home has the same suite of sensors, built by taking a subset of the sensors in home HH101 in\nthe CASAs dataset. Each home has the same layout as HH101â€”we illustrate the homes and devices in Fig. 6 and\nthe sensor suite in Fig. 7.\nWe encode each home in a JSON â€œhome templateâ€ using two data structures: one for the homeâ€™s controllable\ndevices and another for its sensors. Each top-level element of the home is a room, named descriptively (e.g.,\nlivingroom) as a semantic cue about the activities that take place there. Rooms contain devices, again named\ndescriptively (e.g., overhead_light). Each device has several settings, expressed as key-value pairs, where the\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:8 â€¢ E. King, et al.\nFig. 6. We model three homes ( â„1, â„2, and â„3 from left to right) with an increasingly diverse set of devices. All three homes\nhave the same suite of sensors. The modelâ€™s reasoning is based on a given command and the devices and sensors available.\nâ—Area motion\nâ—Fridge motion\nâ—Freezer motion\nâ—Area motion\nâ—Bed motion\nâ—Luminosity\nâ—Area motion\nâ—Couch motion\nâ—Temperature\nâ—Luminosity\nâ—Area motion\nâ—Local time\nâ—Local weather\nâ—User location\nâ—Area motion\nâ—Temperature\nâ—Area motion\nâ—Temperature\nâ—Area motion\nFig. 7. Each home template in our set of homes â„1, â„2, and â„3 has the same suite of sensors, as shown.\nkey is the name of the setting (e.g., a lightâ€™s brightness) and the value is its data type (e.g., float). A second\nstructure contains the sensors in the homeâ€”e.g., motion, luminosityâ€”also organized by room. We include a\nglobal field in the sensor structure for values likelocal_time and weather. We choose JSON since it is common\nin LLM training corpora [22, 52]. It is furthermore the de-facto format for smart device APIs [1â€“3, 5], which eases\nthe transition from research to implementation.\nUser commands. We created 40 natural language commands that correspond to common user goals for smart\nhomes. Our commands (1) reflect user goals not met by current systems and (2) have challenging, under-specified\nphrasing. We again mined the IFTTT dataset, examining how people use their devices. Routines in the dataset\nare grouped into 12 goal categories that describe the types of actions and devices they cover. The â€œAmbient\nLuminanceâ€ category, for instance, includes routines that express a userâ€™s goal to change the lighting. We created\ncommands for each category, with the intuition that the existence of a corresponding IFTTT routine implies\nthat manual intervention was required to meet the goal in existing systems. We omit several goal categories\nthat are not relevant to the devices in our homesâ€”Gardening, Device Hubs, Outlets, and Alertsâ€”resulting in 7\ncategories for our commands: Ambient Temperature, Ambient Luminance, Security, Energy Saving, Ambient\nAtmosphere, Robot Control, and Other Appliances. We created 6 commands per category, with the exception of\nthe Energy Saving category. For this category, we created only 4 commands since many of the energy saving\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:9\nroutines in the dataset accomplished similar goals, but simply targeted different manufacturers of similar device\ntypes. We authored declarative commands, i.e., commands that express a desired end state without specifying the\nrequired actions [24]. Furthermore, each of our commands is either moderately or completely under-specified\nwith respect to at least one of the following: intended target device(s), desired action, and, in the case of persistent\ngoals, suitable trigger (Fig. 2). The command â€œkeep it well-lit after sundownâ€ is an example of a moderately\nunder-specified command since it specifies neither a target (lights, blinds, etc.), nor trigger (a specific time and/or\nsensor with which to detect it). â€œHelp me lower my power billâ€ is an example of a completely under-specified\ncommand since it alludes to no target, trigger, or action that will achieve the goal.\nHalf of the commands in each category reflect immediate goals, and half are persistent. Immediate goals are\nachieved through an immediate action or set of actions, e.g., â€œmake it cozy in hereâ€ or â€œmake it less stuffyâ€.\nPersistent goals require automation routines. Examples include â€œhelp me save energyâ€ or â€œlet me know if the\nweather is badâ€. We list all commands in Appendix A with examples of the IFTTT routines that they are based on.\n4.2 User Survey Procedures & Metrics\nWe use an online survey to solicit user perspectives on the quality of action plans. We generate action plans\nusing each command and home template combination as input to our experimental setup described previously.\nWe describe survey design, participant recruitment, and analysis procedures in the following.\n4.2.1 Survey design. For a given action plan, we display a picture of the home (as depicted in Fig. 6) and the user\ncommand given to the home (e.g., â€œmake it cozy in hereâ€). We ask participants to label whether each of thetarget,\naction, and (in the case of persistent goal commands) trigger of the user command were well-specified. We include\na definition of well-specified in the survey instructions (Appendix B), similar to Fig. 2. These labels allow us to\ncheck our assumptions about how under-specified users perceive the commands to be. Next, we provide a textual\ndescription of the LLMâ€™s action plan as extracted from the JSON. We ask participants to label â€œhow satisfiedâ€\nthey would be with the actions taken by the smart home on a 5-point scale from â€œVery unsatisfiedâ€ to â€œVery\nsatisifiedâ€. Toward answeringRQ1-2, the satisfaction label allows us to analyze possible factors that result in high\nor low user-perceived quality of the action plans that the LLM generates. Finally, we ask the participant to briefly\ndescribe in their own words which aspects of the response were satisfactory or unsatisfactory. The free-form\nrationale given by the labeller provides important information about both the capabilities and potential failure\nmodes of the LLM. Each survey contains 30 such questions and takes approximately 45 minutes to complete.\nEach action plan receives a label from 5 participants, enabling us to compare multiple usersâ€™ perceptions of the\nsame action plan. The full survey is included in Appendix B.\n4.2.2 Participants. We conducted our survey with IRB oversight. We recruitedğ‘ = 20 participants via email,\nsnowball, and word-of-mouth. Our aim is for a relatively uniform distribution of participants across age groups\nsince age is a key factor in smart home perceptions and satisfaction [ 49]. Our resulting participant pool is\nsemi-uniformly distributed with 20% Â±5% in each of 6 age groups, with only the 55-64 group underrepresented at\n5%. We compensated participants with $20 gift cards.\n4.2.3 Analysis: User-perceived quality. We score the user-perceived quality of an action plan based on the\nsatisfaction rating assigned from our user survey. We map each categorical rating given on the five-point scale\nfrom â€œVery unsatisfiedâ€ to â€œVery satisifiedâ€ to a numerical rating from [0.2,1.0]. For further qualitative analysis,\nwe study free-form rationales on action plans that have above- or below-average ratings, or have high variability\namong their five labelers.\n4.2.4 Analysis: Failure modes. We derive failure modes from the free-form rationales in our user survey using a\nsystematic two-phase coding process:\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:10 â€¢ E. King, et al.\nID Gender Age Education Smart Home Familiarity English Level\nSP1 F 45-54 Bachelorâ€™s Moderately Native/bilingual\nSP2 M 65+ Graduate Very Native/bilingual\nSP3 F 55-64 Bachelorâ€™s Slightly Native/bilingual\nSP4 F 25-34 Bachelorâ€™s Slightly Native/bilingual\nSP5 M 25-34 Graduate Moderately Native/bilingual\nSP6 F 18-24 High school or equivalent Moderately Native/bilingual\nSP7 M 18-24 High school or equivalent Moderately Native/bilingual\nSP8 M 25-34 Graduate Slightly Native/bilingual\nSP9 F 65+ Some college Moderately Intermediate\nSP10 M 65+ Associates or technical degree Moderately Native/bilingual\nSP11 F 25-34 Graduate Moderately Native/bilingual\nSP12 M 65+ Some college Moderately Native/bilingual\nSP13 F 18-24 Associates or technical degree Very Native/bilingual\nSP14 F 25-34 Graduate Moderately Native/bilingual\nSP15 F 35-44 Graduate Moderately Native/bilingual\nSP16 F 18-24 High school or equivalent Very Native/bilingual\nSP17 F 45-54 Graduate Slightly Native/bilingual\nSP18 M 35-44 Some college Slightly Basic\nSP19 M 45-54 Bachelorâ€™s Moderately Native/bilingual\nSP20 F 35-44 Bachelorâ€™s Very Native/bilingual\nTable 1. Demographic characteristics of survey participants (SP) in our empirical study of LLM-based smart home control.\n(1) One researcher reviewed all responses to identify several high-level categories of failure. Three researchers\nthen independently assigned labels from these high-level categories to the 325 responses with less than\nâ€œVery satisfiedâ€ quality.\n(2) The same three researchers discussed more specific modes of failure they observed during the high-level\ncategorization task. After defining a set of finer-grained failure modes, each researcher independently\nassigned these labels to each of their original annotations.\nThis process yielded a total of 975 annotations with a Krippendorffâ€™s alpha of 0.75 and pairwise Cohenâ€™s kappa\nof 0.75,0.76,and 0.74, suggesting acceptable inter-annotator agreement.\n4.3 Action Plan Analysis Procedures & Metrics\n4.3.1 Analysis: Relevance metrics. We analyze action plans by examining how relevant the target devices are\nto the goal category of the command. We loosely base these metrics on document relevance in information\nretrieval [16]. For each of the 7 goal categories, we identify, as a sort of ground truth, the types of device that\nare relevant (based on the types of device targeted by IFTTT routines in that goal category [63]). Given our set\nof user commands ğ¶, the goal category ğ‘”ğ‘ of command ğ‘ âˆˆğ¶, a set of the goal categories of devices ğºğ‘ that an\nLLMâ€™s action plan targets in response to command ğ‘, and the set of goal categories ğºâ„ supported by devices in\nhome â„, we measure the following:\nFalse positives ( ğ¹ğ‘ƒ): a false positive occurs when the system creates a plan that targets one or more devices,\nbut the home in fact does not have any devices relevant to the command. We count these as:\nğ¹ğ‘ƒ = 1\n|ğ¶|\nâˆ‘ï¸\nğ‘âˆˆğ¶\nfp(ğºğ‘,ğºâ„,ğ‘”ğ‘) fp(ğºğ‘,ğºâ„,ğ‘”ğ‘)=\n(\n1 if |ğºğ‘|> 0 and ğ‘”ğ‘ âˆ‰ ğºâ„\n0 otherwise\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:11\nFalse negatives ( ğ¹ğ‘): in contrast, a false negative occurs when the system does not create a plan, but the\nhome has relevant devices:\nğ¹ğ‘ = 1\n|ğ¶|\nâˆ‘ï¸\nğ‘âˆˆğ¶\nfn(ğºğ‘,ğºâ„,ğ‘”ğ‘) fn(ğºğ‘,ğºâ„,ğ‘”ğ‘)=\n(\n1 if |ğºğ‘|= 0 and ğ‘”ğ‘ âˆˆğºâ„\n0 otherwise\nAccuracy (ğ´ğ‘ğ‘): the accuracy captures the portion of plans that target a relevant device only when one exists:\nğ´ğ‘ğ‘ = 1 âˆ’(ğ¹ğ‘ƒ +ğ¹ğ‘)\nRelevance score ( ğ‘…ğ‘’ğ‘™): finally, we measure the relevance of system outputs by computing, for each command\nin ğ¶, a score ğ‘Ÿ âˆˆ[âˆ’1,1], assigned based on the relative number of relevant and irrelevant devices included in each\ncommandâ€™s generated action plan. Lower numbers indicate more of the target devices are irrelevant, while higher\nnumbers indicate more are relevant. We define ğ‘…ğ‘’ğ‘™ as the average across all ğ‘Ÿ for a systemâ€™s outputs, as follows:\nğ‘…ğ‘’ğ‘™ = 1\n|ğ¶|\nâˆ‘ï¸\nğ‘âˆˆğ¶\nğ‘Ÿ(ğºğ‘,ğºâ„) r (ğºğ‘,ğºâ„)= |ğºğ‘ âˆ©ğºâ„|âˆ’| ğºğ‘ âˆ’ğºâ„|\n|ğºğ‘|\nWhen comparing performance between LLMs, we scale ğ‘…ğ‘’ğ‘™ based on the portion of action plans that do not\nreject the command, since LLMs that reject higher numbers of commands would otherwise have inflated ğ‘…ğ‘’ğ‘™.\n4.3.2 Analysis: Cost and latency. The number of tokens that are input to or output from the LLM are an indicator\nof cost [56]. We measure tokens based on the encoding scheme used by a given model [29, 39]. We also measure\nresponse latency (in seconds) by timing the function in our code that calls the relevant remote API. For GPT models,\nwe use the OpenAI API; for Llama, we use a dedicated deployment on a Hugging Face â€œInference Endpointâ€. In\nboth cases, we access the API from a 1Gbps fiber internet connection. Latency includes network delays, varying\nloads on distributed computing resources, etc., and provides an estimate of user-facing performance.\n4.3.3 Analysis: JSON validity. We note if forced extraction of JSON was necessary and possible. First, we attempt\nto decode the response verbatimâ€”in this case, no extraction is necessary. If this fails, we attempt to extract and\ndecode the portion of the response enclosed in top-level curly brackets. This enables us to identify the valid\nJSON portion of responses that include additional exposition (e.g., â€œSure, here is the JSON action plan: { ... }â€).\nFinally, if this also fails, we attempt to interpret the extracted string as an abstract syntax tree, enabling us to\ndecode the response even if it contains, e.g., portions that are expressed in Python syntax rather than JSON. If all\nsteps fail, we consider the response invalid.\n4.3.4 Correlation testing. We use Kendallâ€™s tau-b (ğœğ‘) to test for correlations between quality labels in our user\nsurvey and characteristics of action plans.ğœğ‘ tests for correlation between two variables that may not obey normal\ndistributions, with âˆ’1 and 1 implying strong negative and positive correlations, respectively, and 0 implying\nno correlation. Given the degree of subjectivity in our task (particularly with respect to user-perceived quality\nratings), we set the threshold for ğœğ‘ â‰¥0.2 or ğœğ‘ â‰¤âˆ’0.2 with ğ‘ â‰¤0.05. Our threshold for ğœğ‘ obeys a common\nguideline for establishing at least â€œmoderateâ€ correlation [9].\n4.4 Findings\n4.4.1 Finding 1: Action plans vary from remarkably helpful to completely non-sensical. Toward answering RQ1-2,\nwe highlight several action plans based on indicators in their user-perceived quality ratings. Table 2 depicts\naction plans with (1) consistently high, (2) consistently low, and (3) highly-variable quality ratings from our\nsurvey. Consistently high- or low-quality plans are rated higher or lower quality on average than the population,\nwith below-average variance. Highly-variable plans have above-average variance in their quality ratings.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:12 â€¢ E. King, et al.\nHigh Satisfaction (47% of plans)\nâ„1 help me lower my power bill\nTo lower your power bill, I will turn off the lights in any room where there has been no motion detected\nfor 15 minutes. This will ensure that lights are not left on unnecessarily.\nVery satisfied SP1: Good choice.\nVery satisfied SP7: Very nice answer, as well as understanding and figuring out the required action to\nachieve the task. Since there are only lights, home fully understood how to save energy.\nVery satisfied SP16: Yeah thatâ€™s great\nâ„2 help me wind down\nI have dimmed the lights in the living room and bedroom to a warm color, turned off the lights in other\nrooms, set the thermostat to a comfortable 68 degrees, and lowered the volume of the smart speaker. The\ntelevision is turned off to help you wind down.\nVery satisfied SP10: Good strategy with devices available.\nVery satisfied SP16: This is solid. The AI has a better idea of how to wind down than I do.\nSatisfied SP17: This sounds lovely for winding down.\nLow Satisfaction (13% of plans)\nâ„2 stop cleaning after sunset\nWhen the local time sensor detects that it is sunset, the smart speaker in the living room will be commanded\nto stop the cleaning process.\nVery unsatisfied SP1: The speaker doesnâ€™t clean.\nUnsatisfied SP8: Smart speaker seems irrelevant.\nVery unsatisfied SP18: Speakers do not clean.\nâ„1 donâ€™t clean when people are here\nWhen motion is detected in the entry, the cleaning robot will be turned off.\nVery unsatisfied SP7: There is no cleaning robot in this house. Plus, checking for the motion in the entry\nwill not tell you if there are people in the house or not.\nNeutral SP12: There is no robot.\nVery unsatisfied SP18: No cleaning robot in house.\nVariable Satisfaction (40% of plans)\nâ„1 let the guest in\nThe entry overhead light has been turned on with 60% brightness to let the guest in.\nSatisfied SP6: The most useful device available was used, so seems like a good response.\nVery unsatisfied SP7: There is no device that controls the entrance door, thus the home should just say\nthat it canâ€™t do this task since there is no device it could use.\nNeutral SP19: Turning on a light helps the guest enter, but thatâ€™s different from letting them in.\nâ„2 turn off the thermostat when I donâ€™t need it\nThe thermostat will be turned off when the temperature sensor in the living room indicates that the\ntemperature is within a comfortable range, suggesting that the thermostat is not needed.\nUnsatisfied SP1: I might not need it based on the fact that I am not home.\nNeutral SP4: I would also want the home to stop heating or cooling when Iâ€™m absent.\nVery satisfied SP18: The home grasped concept of â€œneed itâ€\nTable 2. GPT-4 generated action plans with consistently high, consistently low, and variable user satisfaction across labelers.\nConsistent high or low satisfaction implies the plan is either helpful or exhibits obvious failures, respectively, with low\nsubjectivity. Variable satisfaction implies that some aspects of the plan are subject to individual preferences.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:13\nHigh quality plans provide complex solutions to completely under-specified commands. In the case of\nâ€œhelp me lower my power billâ€, GPT-4 relates the only device available in homeâ„1 (lights) to the under-specified\ngoal of lowering a power bill. It also proposes a sensor trigger and value (motion for 15 minutes) that will\naccomplish it. In the case of â€œhelp me wind downâ€, GPT-4 relates the activity of â€œwinding downâ€ to an extensive\nset of devices and settings to accomplish the goal. Users rate these plans high quality.\nLow quality plans exhibit failures that diminish user satisfaction and limit practical utility. In the\ncase of â€œstop cleaning after sunsetâ€ in home â„2 (which has no cleaning capability), GPT-4 targets the speaker\nwith a non-sensical plan; in the case of â€œdonâ€™t clean when people are hereâ€ inâ„2, GPT-4 chooses a suboptimal\ntrigger and plans to target a cleaning robot that does not exist in the home. Users rate these plans low quality.\nPlans with variable quality involve â€œcreative,â€ subjective choices. This is exemplified by the plan to â€œlet\nthe guest inâ€ in home â„1. One user finds GPT-4â€™s decision to turn on the entryway light as a reasonable response\ngiven the homeâ€™s limitations, while another finds it non-sensical and prefers that the system reject the request. In\nthe case of â€œturn off the thermostat when I donâ€™t need itâ€, usersâ€™ definitions of â€œdonâ€™t need itâ€ vary, resulting in\ndiffering levels of satisfaction. Whether these plans are seen as resourceful (and thus high quality) or non-sensical\n(and thus low quality) varies based on user preference.\n4.4.2 Finding 2: Action plans exhibit specific modes of failure that diminish user satisfaction. We identify 7 common\nfailure modes in LLM-generated action plans, as follows:\n(1) Device â†’No option exists: The action plan targets an irrelevant device and no better option (other than\nrefusing the request) appears to exist, e.g., turning off the smart speaker in response to â€œstop cleaningâ€\nwhen there is no cleaning robot in the house.\n(2) Device â†’Option exists: The action plan does not target a relevant device even when an option exists,\neither because the request is refused or the plan fails to include additional relevant targets, e.g., adjusting\nthe lights but not the thermostat in order to â€œsave energyâ€.\n(3) Device â†’Extra: The action plan targets an extra, irrelevant or relevant device or devices in addition to\nrelevant devices, e.g., turning on both the TV and the smart speaker.\n(4) Device â†’Hallucinated: The action plan targets a device that does not exist in the home, i.e., the LLM\nâ€œhallucinatesâ€ a device and adds it to the action plan, e.g., turning on the cleaning robot when there is no\ncleaning robot in the home.\n(5) Device â†’Setting: The action plan assigns a setting that may be incorrect or dependent on additional\ncontext, e.g., setting the lights to full brightness regardless of the time of day.\n(6) Sensor â†’Suboptimal choice: For persistent goals, the action plan chooses a suboptimal sensor for the\nautomation, e.g., using motion in the entryway to determine occupancy.\n(7) Sensor â†’Trigger value: For persistent goals, the action plan chooses an illogical or under-specified\ntrigger value for the automation, e.g., using the time value of â€œsunsetâ€ as a trigger to turn off the lights.\nWe illustrate an action plan and corresponding rationale that exemplifies each failure mode in Appendix C.\nFalse positives are judged most harshly by users. Fig. 8 depicts the average user-perceived quality of action\nplans that exhibit each failure mode. â€œNo option existsâ€ has the lowest average quality, followed by â€œHallucinatedâ€.\nThis implies that false positives â€”either due to targeting of an extant but irrelevant device, or due to an attempt\nto target a device that does not existâ€”are judged harshly by users. We test for correlation between the average\nquality of an action plan and its false positive rating fp, with ğœğ‘ = âˆ’0.42 and ğ‘ = 0.00000004, suggesting a strong\nnegative correlation with high significance.\nFailure modes are modulated by the capabilities of the home . Fig. 9 depicts the frequency of each failure\nmode, grouped by home. Accounting for all commands, when moving from home â„1 to â„3 the frequency of â€œNo\noptionâ€ and â€œHallucinatedâ€ failures decreases since increasingly more devices are available to meet the goal.\nInversely, the frequency of â€œOption existsâ€ failures increases: when more devices are available, users expect them\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:14 â€¢ E. King, et al.\n0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\naverage quality rating\nNo option exists (D)\nOption exists (D)\nExtra (D)\nHallucinated (D)\nSetting (D)\nSuboptimal choice (S)\nTrigger value (S)\nn = 99\nn = 101\nn = 135\nn = 32\nn = 130\nn = 135\nn = 112\nFig. 8. User-rated quality of GPT-4 generated action plans by failure mode across all homes. (D) denotes device targeting\nfailures; (S) denotes sensor choice failures. Error bars depict a 95% confidence interval. ğ‘›indicates the number of samples\n(i.e., labels assigned to action plans) for each failure mode. Users rate false positives more harshly than other failure modes:\nâ€œNo option existsâ€ and â€œHallucinatedâ€ have the lowest average quality.\nh1 h2 h3\nhome\nNo option exists (D)\nOption exists (D)\nExtra (D)\nHallucinated (D)\nSetting (D)\nSuboptimal choice (S)\nTrigger value (S)\nfailure mode\n0.21 0.20 0.01\n0.02 0.07 0.16\n0.19 0.27 0.21\n0.23 0.01 0.00\n0.09 0.23 0.16\n0.12 0.14 0.23\n0.14 0.08 0.24\nall goals\n0.0\n0.1\n0.2\n0.3\nh1 h2 h3\nhome\nNo option exists (D)\nOption exists (D)\nExtra (D)\nHallucinated (D)\nSetting (D)\nSuboptimal choice (S)\nTrigger value (S)\nfailure mode\n0.07 0.00 0.01\n0.02 0.04 0.16\n0.37 0.34 0.21\n0.00 0.00 0.00\n0.15 0.36 0.16\n0.20 0.15 0.23\n0.20 0.12 0.24\nsupported goals\n0.0\n0.1\n0.2\n0.3\nFig. 9. Frequency of failure modes in GPT-4 generated action plans, organized by home (â„1,â„2, andâ„3, each with an increasing\nnumber and diversity of devices). Left: action plans for all commands. Right: action plans for commands with goals that are\nsupported by the home (e.g., the plan for â€œhelp me cool offâ€ in home â„1 is omitted since â„1 does not have a thermostat). (D)\ndenotes device targeting failures; (S) denotes sensor choice failures.\nto be targeted. Accounting only for commands with goals that are supported by the home, the dominant failure\nmode is â€œExtraâ€â€”action plans tend to target extra devices in cases where the goal is achievable.\n4.4.3 Finding 3: Reliably targeting relevant devices is a challenge, even for the highest-performing models. We\nreport accuracy, false positives, false negatives, and relevance scores for 4 different models, each with 2 different\ntemperature ğ‘¡ parameters in Table 3. ğ‘¡ sets the probability that the LLM predicts less-likely tokens in the output\nsequence (i.e., the â€œcreativityâ€ of output).\nGPT-4 outperforms other models on all metrics. All modelsâ€™ relevance scores are far from 1.0. GPT-4ğ‘¡ = 0.7\nachieves the highest at 0.63, indicating that GPT-4 is the most likely to target relevant devices and not target\nirrelevant ones. 0.63 nonetheless leaves much room for improvement. All models tend to target extra, irrelevant\ndevices in addition to the minimal relevant set as the home provides more capabilities. We depict this in Fig. 10\nby grouping action plan relevance scores by homes and goal categories of commands. In homes with fewer\ncapabilities (e.g., â„1), action plans tend to target the minimal relevant set of devices (achieving 1.0 in several\ngoal categories), but also target completely irrelevant sets of devices with non-sensical plans when the goal is\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:15\nConfiguration Relevance Latency (s) JSON\nModel ğ‘¡ ğ´ğ‘ğ‘ ğ¹ğ‘ƒ ğ¹ğ‘ ğ‘…ğ‘’ğ‘™ Min Max Mean Forced Invalid\nGPT-4 0.00 0.84 0.16 0.00 0.63 0.82 35.83 9.54 0.01 0.00\nGPT-4 0.70 0.82 0.16 0.02 0.67 0.71 46.69 12.59 0.01 0.00\nGPT-3.5 text-davinci-003 0.00 0.68 0.32 0.00 0.30 0.47 20.58 6.91 0.43 0.00\nGPT-3.5 text-davinci-003 0.70 0.68 0.33 0.00 0.26 0.51 23.75 7.46 0.37 0.00\nLlama 2 7B-chat-hf 0.10 0.65 0.28 0.07 0.09 1.23 26.12 12.79 0.61 0.38\nLlama 2 7B-chat-hf 0.70 0.62 0.28 0.10 0.11 1.17 16.61 9.94 0.59 0.38\nLlama 2 13B-chat-hf 0.10 0.62 0.28 0.09 0.18 3.17 49.87 30.05 0.79 0.19\nLlama 2 13B-chat-hf 0.70 0.62 0.29 0.09 0.21 2.97 58.05 29.93 0.77 0.22\nTable 3. Summary of quantitative results from our empirical study. We report relevance metrics, which reflect an LLMâ€™s\ntendency to include devices that are irrelevant to the goal in an action plan (accuracy ğ´ğ‘ğ‘, false positives ğ¹ğ‘ƒ, false negatives\nğ¹ğ‘, and relevance score ğ‘…ğ‘’ğ‘™). We also report response latency and JSON validity. Temperatureğ‘¡ sets the probability that the\nLLM predicts less-likely tokens in the output sequence (i.e., the â€œcreativityâ€ of output). The shaded row denotes the set of\naction plans that participants rated in our user survey.\nambient temperature\nambient luminance\nsecurity\nambient atmosphere\nrobot control\nother appliances\nenergy saving\ngoal category\nh1h2h3\nhome\n-1.00 1.00 1.00 1.00 0.00 -1.00 1.00\n0.70 0.91 0.11 0.74 -1.00 -1.00 1.00\n0.73 0.81 0.67 0.49 1.00 0.73 0.72\nRelevance by Goal Category (GPT-4, t = 0.0)\n1.0\n0.5\n0.0\n0.5\n1.0\nambient temperature\nambient luminance\nsecurity\nambient atmosphere\nrobot control\nother appliances\nenergy saving\ngoal category\nh1h2h3\nhome\n-1.00 1.00 1.00 1.00 -1.00 -1.00 1.00\n0.56 0.86 0.67 0.44 -1.00 -1.00 1.00\n0.10 0.26 1.00 0.24 -0.25 0.00 0.82\nRelevance by Goal Category (Llama 2 13B, t = 0.1)\n1.0\n0.5\n0.0\n0.5\n1.0\nFig. 10. Relevance scores of LLM-generated action plans, grouped by home ( â„1,â„2,and â„3, which have an increasing number\nand diversity of devices) and the goal category of commands (which determines the types of devices that are relevant). -1.0\nimplies all devices in all action plans for commands in that category are irrelevant; 0.0 implies either no target, or an equal\nbalance of relevant to irrelevant; 1.0 implies all are relevant. Action plans in homes with fewer devices (â„1) are more likely to\ntarget a minimal relevant set (1.0), but also target completely irrelevant devices when the goal is not supported (-1.0). In\nhomes with more capabilities (â„3), action plans include both relevant and irrelevant devices, diluting the relevance score.\nunsupported (achieving -1.0 in these categories). As the home gains capabilities, the risk of creating non-sensical\nplans decreases (shown by fewer -1.0 scores in â„2 and â„3). This improvement, however, is moderated by the fact\nthat action plans begin to include extra, irrelevant devices in addition to the minimal relevant set.\n4.4.4 Finding 4: Cost and latency pose practical challenges. Complex homes and goals incur higher costs. For\nimmediate goals (which do not require sensors to be included in the template), input tokens are 469, 529, and 607\nfor â„1,â„2,and â„3, respectively; for persistent goals, these values increase to 670, 730, and 808. Overall, output\ntokens vary from 8 to 566 depending on the goal. In the minimal case, the output is simply to reject the goal. In\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:16 â€¢ E. King, et al.\nthe maximal case, the output is a lengthy action plan. Based on these token measurements in conjunction with\nAPI pricing, a single smart home command costs around $0.02 for GPT-3.5 and $0.03 for GPT-4.1 Accounting for\nlongitudinal data about how often users interact with home assistants [8], this would range from $0.10 to $0.60\nper day for GPT-3.5, or $0.30 to $1.00 per day for GPT-4.\nHigh performance models and complex goals incur longer response times. Latency statistics are shown\nin Table 3. Response latency varies widely, dependent on external factors (e.g., demand on cloud infrastructure)\nand the complexity of the response needed. In the best case, latency for all models is around one second; this\noccurs when the request is rejected. Latency for generating more complex action plans, however, is highly\nunstable, often approaching 30 to 45 seconds for the most powerful models.\n4.4.5 Finding 5: JSON response validity poses a practical challenge. Table 3 depicts the portion of action plans\nthat required either forced extraction or were completely invalid. Nearly all modelsâ€™ outputs occasionally need\nforced extraction. This is frequent with the Llama 2 models, which have been fine-tuned for chat applications.\nResponses often included a valid action plan preceded by exposition: e.g., â€œSure, here is the action plan... â€ Across\nGPT models, there was no invalid JSON; Llama 2, however, occasionally responded without JSON, but rather a\ndescription of the plan, or the JSON separated into several segments. This is an engineering challenge: fine-tuning\nincreases confidence that a modelâ€™s outputs will have a desired structure [57].\n4.4.6 Summary. We draw the following conclusions from our empirical study:\nâ€¢LLMs can generate creative and complex action plans in response to under-specified user commands.(RQ1)\nâ€¢LLMs are prone to failures surrounding the relevance of target devices, their assigned settings, and in the\ncase of persistent goals, sensor triggers and trigger values. False positives (i.e., targeting an irrelevant or\nnonexistent device when none are available) are negatively correlated with user satisfaction. (RQ2)\nâ€¢Performance is compared in terms of accuracy, false positives, false negatives, and relevance scoring of\nmodelsâ€™ action plans. Even the highest-performing models struggle to reliably target relevant devices: they\neither (1) target irrelevant devices when the home has no devices to support the goal or (2) target extra,\nirrelevant devices in addition to the minimal relevant set when the home has more devices available.(RQ2)\nâ€¢The cost and latency of generating action plans, in addition to instability in the validity of the output are\nopen engineering challenges. (RQ2)\n5 DESIGNING SASHA\nThe prototype LLM-based smart home presented in Fig. 3 and used in our empirical study above makes some\nremarkable strides towards addressing RQ3. However, as our empirical study shows, there are shortcomings of\nthe naÃ¯ve approach that must be addressed. In this section, we propose Sasha, asmarter smart home assistant that\nimproves the performance of LLMs in smart environments. We are motivated by the need to improve response\nquality by (1) reducing false positives, (2) increasing the relevance of action plans, and (3) providing input for user\nfeedback to reduce subjectivity. Our empirical study revealed each of these as key factors in user satisfaction.\nSasha deconstructs the process of responding to user commands into iterative reasoning components (Fig. 11).\nRather than creating an action plan in a single inference, Sasha reasons about the response in multiple steps. We\nassume the use of an instruction-tuned model that has been fine-tuned to adhere to written instructions provided\nin prompts [57] but is not fine-tuned for smart home tasks. Furthermore, we assume that the model exhibits high\nperformance with chain of thought prompting [58]. Both are true of numerous models [11, 13, 38, 51]. The two\ninputs to Sasha remain the same as in Section 3: a user command and a template that describes the sensors and\ndevices available. We describe each component of Sashaâ€™s iterative reasoning in the following.\n1OpenAI API pricing in November 2023: $0.02 per 1K tokens for GPT-3.5 and $0.03 per 1K input tokens / $0.06 per 1K output tokens for GPT-4\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:17\nRelevance Immediate\nFollow-up Persistent\nClarifying\nIs the goal achievable \nwith these devices?\nFiltering\nWhat are the relevant \ndevices?\nPlanning\nHow are these used to \nachieve the goal?\nFeedback \nDoes the action plan \nachieve the goal?\nExecution \nMap plan to  \nappropriate API calls\nFig. 11. Sasha uses iterative reasoning to generate consistently high-quality action plans that leverage whichever relevant\ndevices are available in a given smart home.\n5.1 Clarifying: Is the goal achievable with the devices available?\nFalse positives are negatively correlated with user satisfaction, as shown in our empirical study. Our first objective\nis to develop a component that directly reduces the rate of false positives. Theclarifying step therefore prompts the\nmodel to consider the goal of the userâ€™s command in relation to the devices available in the home template. If the\nhome has devices that are semantically relevant to the command, Sasha will determine that the goal is achievable\nand move beyond the clarifying step without interjection from the user. If Sasha does not identify a sufficient set\nof relevant devices, it notifies the user that it is unable to proceed. We allow the user to follow-up with clarifying\ninformation, at which point the prompt is repeated with added context. The output of this component is a status\ncode: success if there are relevant devices, and failure if not.\nWe illustrate with an example. When a user tells Sasha â€œIâ€™m tiredâ€, we prompt the model with the command,\nthe devices in the home, and instructions to consider whether any of the devices are relevant to the command. If\nthe command is issued in, e.g., home â„3 from our empirical study (with a coffee pot), Sasha will relate â€œtiredâ€ to\nthe coffee pot and determine that there do exist relevant devices in the home. If, instead, the same command is\nissued to the same system in a home with only lights (â„1), Sasha is unlikely to find devices that seem relevant.\nThe output is â€œfailureâ€, along with a verbal response to the user: â€œIâ€™m sorry, I didnâ€™t find anything in the home\nthat seems relevant to your command. Can you provide more information?â€ For the sake of example, we assume\nthe user is tired and the lights will help them wake up. The user can provide clarifying information (again in\ntheir own phrasing): â€œI need help waking upâ€. We repeat the prompt with the added context, asking the system\nto again consider whether any devices are relevant. In â„1 with only lights, the model is likely to find semantic\nrelationships between â€œtiredâ€, â€œwaking upâ€, and the color and brightness of lights, so it will now return â€œsuccessâ€.\nIt is also important to consider cases where the home simply cannot support the goal, which we revealed in\nour empirical study can produce non-sensical output. Let us assume that the user follows up â€œIâ€™m tiredâ€ with\nclarification: â€œI could really use some coffeeâ€. At this point, there is sufficient information to infer a goal, but in\nthe case of â„1 and â„2 there are still no relevant devices to support it. Sasha in this case will return â€œfailureâ€ for\nrelevance and avoid producing a non-sensical action plan. Without Sashaâ€™s relevance check, the model would\nturn on a light inâ„1 and justify its reasoning: â€œTurning on the entry overhead light to provide a bright, energizing\natmosphere to help you get ready for your coffee. â€\nPerforming the clarifying step of reasoning with an LLM provides several advantages over a task-specific\ncomponent, as task-specific approaches have two key limitations: (1) their design encodes a constrained set of\nassumptions about which devices are pertinent to which goals, which precludes the invention of creative action\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:18 â€¢ E. King, et al.\nplans, and (2) there exist generic device types (e.g., a smart plug) for which it is nearly impossible to predict\ntheir relationship to a userâ€™s goal. Using an LLM provides an advantage in both cases: (1) it can leverage the\noften-unpredictable situational knowledge encoded in its training corpora to invent creative action plans, and\n(2) it reasons semantically about the devices. This means that simply naming a smart plug in the home template,\ne.g., humidifier_plug or guitar_amp_plug, enables a model to leverage the same device type for a broader\nset of actions than those assumed in task-specific systems. As an implementation note, we have the option to\nperform this reasoning on either the full device template (i.e., a semantically complete and correct JSON snippet)\nor, instead, on a shortened list of available devices (i.e., simply a list of the home-provided names of the devices).\nThe latter option benefits performance, which we highlight in our evaluation.\n5.2 Filtering: Which are the relevant devices?\nRelevance of target devices in action plans tends to be unstable across homes, as shown in our empirical study. Our\nnext objective is to develop a component that provides consistent relevance (ğ‘…ğ‘’ğ‘™) across different environments.\nTo do so, we prompt the model to select the minimal set of relevant devices from the JSON home template. This\nstep outputs a JSON subset of the devices in the home template.\nFor the â€œIâ€™m tired, I need help waking upâ€ command, we prompt the model with the device filtering prompt. In\nresponse, the model in â„3 will return JSON of the coffee maker in the kitchen. In â„1 or â„2, it will return JSON of\nlights in their respective rooms. As with the clarifying step, we may either filter the full set of devices, or filter\na list of devices (with their settings excluded). Performing this step with an LLM as opposed to a task-specific\nsystem has a similar justification as above. Filtering based on the semantic relationship between a command and\nthe named devices in a home unlocks greater flexibility than a system that relies on a rigidly-defined ontology.\n5.3 Planning: How are these devices and sensors used to meet the goal?\nAfter the model has selected the relevant devices, we prompt it to produce an action plan. The planning prompt\nincludes the user command, the filtered devices (and, for persistent goals, the sensors). In the case of immediate\ngoals, we guide the model to produce an action plan that is simply a modified version of the filtered subset of\nthe home template, with settings assigned to devices. For persistent goals, we guide the model to produce an\naction plan in the form of an automation routine, defined as a (trigger, action) pair. The trigger is a sensor or set\nof sensors and corresponding values that will trigger the routine, while the action is the filtered subset of devices\nin the home template with settings assigned. This step outputs an action plan, in JSON.\n5.4 Execution: Map the plan to API calls\nWe execute action plans by mapping changes to device state in the JSON to the appropriate smart home API calls.\nThis is a matter of implementation and requires no help from the model.\n5.5 Feedback: Does the action plan achieve the userâ€™s goal?\nThe user-perceived quality of an action plan is sometimes subjective, as shown in our empirical study. Our last\nobjective is to provide input for users to occasionally refine aspects of generated action plans. After execution,\nthe modelâ€™s action plan is described to the user for them to critique. If the user is satisfied with the choices made\nin the action plan, no further input is needed. If some aspects of the plan need adjustment, however, the user\nhas an opportunity to provide feedback. They can provide this feedback in unconstrained language (as with the\ninitial command). For example, if the user tells Sasha in â„3 â€œIâ€™m boredâ€ and the system responds by turning on\nthe TV and smart speaker, the user may offer some feedback: â€œI donâ€™t need both the TV and the speaker to be onâ€.\nOnce the user provides this feedback, we prompt the model to modify the plan, or apologize and take no action if\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:19\nConfiguration Relevance Latency (s) JSON\nModel List Î”ğ´ğ‘ğ‘ Î”ğ¹ğ‘ƒ Î”ğ¹ğ‘ Î”ğ‘…ğ‘’ğ‘™ Min Max Mean Forced Invalid\nClarifying + Filtering\nGPT-4 gpt-4 +0.10 -0.12 +0.03 0.72 44.54 11.90 0.00 0.00\nGPT-4 gpt-4 âœ“ +0.07 -0.08 +0.02 0.71 14.43 4.84 0.00 0.00\nGPT-3.5 text-davinci-003 +0.01 -0.01 0.0 0.40 19.45 6.26 0.00 0.00\nGPT-3.5 text-davinci-003 âœ“ +0.02 -0.02 0.0 0.40 6.06 2.80 0.01 0.00\nClarifying\nGPT-4 gpt-4 +0.08 -0.15 +0.07 0.59 2.26 1.32 0.00 0.00\nGPT-4 gpt-4 âœ“ +0.11 -0.15 +0.04 0.61 2.45 1.26 0.00 0.00\nGPT-3.5 text-davinci-003 +0.06 -0.12 +0.06 0.43 2.77 1.04 0.00 0.00\nGPT-3.5 text-davinci-003 âœ“ +0.13 -0.16 +0.03 0.36 1.52 0.76 0.00 0.00\nFiltering + Planning\nGPT-4 gpt-4 -0.03 2.97 50.01 18.19 0.00 0.00\nGPT-3.5 text-davinci-003 -0.16 1.02 15.05 7.42 0.00 0.00\nFiltering\nGPT-4 gpt-4 +0.01 2.23 36.66 11.87 0.00 0.00\nGPT-4 gpt-4 âœ“ +0.00 2.14 38.39 12.71 0.01 0.00\nGPT-3.5 text-davinci-003 +0.02 1.59 19.74 7.61 0.01 0.00\nGPT-3.5 text-davinci-003 âœ“ +0.00 1.72 15.84 6.49 0.00 0.00\nFull Reasoning Chain (Clarifying, Filtering, Planning)\nGPT-4 gpt-4 âœ“, - +0.11 -0.15 +0.04 +0.01 0.61 66.98 14.84 0.00 0.00\nGPT-3.5 text-davinci-003 âœ“, - +0.13 -0.16 +0.03 +0.02 0.40 33.61 11.73 0.01 0.00\nTable 4. Summary of quantitative results for Sasha. Results are shown as the difference between the zero-shot approach in\nour empirical study relative to the measurement achieved by iterative components of Sashaâ€™s reasoning chain. In our ablation\nstudy, we test the effect of reasoning on full home templates (containing all devices and their settings) versus reasoning on\nsimplified lists of devices (denoted by a âœ“in the â€œListâ€ column).\nSasha finds no way to improve it. This step outputs verbal responses to the user describing the action plan as it is\nprogressively modified, along with the final action plan JSON.\n6 EVALUATION\nWe evaluate Sashaâ€™s performance against our home and command dataset from Section 3 with 2 different LLMs in\nan ablation study. For repeatability, we set temperatureğ‘¡ = 0.0. Toward answeringRQ3, our goal is to understand\nhow different components of the reasoning chain influence the performance of the system, measured in terms of\naccuracy, false positives, false negatives, and relevance (defined in Section 4.3.1), all of which can be computed by\nsimply examining the responses returned by Sasha.\nResult 1: Clarifying the goal separately from filtering devices reduces the rate of false positives. We\ncompare the performance when combining Clarifying + Filtering into a single step versus Clarifying in a separate\nstep. In the first case, we use a single prompt to ask the LLM to select any relevant devices, or to decline if no\ndevices appear relevant (Clarifying + Filtering). In the latter case, we prompt the model to consider if there are\nany relevant devices (i.e., Sashaâ€™s Clarifying component). In both cases, we measure the impact of reasoning on\nthe full home template versus a simplified list of available devices. We report the results in Table 4.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:20 â€¢ E. King, et al.\nFig. 12. Test home for Sasha user study. We inviteğ‘ = 7 participants to give unconstrained commands to Sasha as they enact\nactivities of daily living in the test home. The user study enabled us to discover the LLM-based smart homeâ€™s capabilities and\nlimitations when faced with user-generated scenarios.\nFor both models, combined Clarifying + Filtering improves accuracy by reducing false positives. For GPT-3.5,\nreasoning on the list of devices provides a slight advantage; this is not the case for GPT-4. For GPT-4, the false\nnegative rate slightly increases. The separate Clarifying step provides a larger performance benefit than combined\nClarifying + Filtering in both cases. This benefit is further increased when Clarifying is done on a list of devices.\nReduction in false positives is slightly offset by a small increase in false negativesâ€”the Clarifying step, in essence,\nmakes the output more conservative.\nResult 2: Filtering devices separately from planning the actions improves relevance. We evaluate the\neffect of combining Filtering + Planning (i.e., the model creates an action plan on the full set of devices) relative\nto separate Filtering and Planning steps (i.e., the model creates an action plan from the filtered set of devices)\n(Table 4). For both models, combined Filtering + Planning reduces relevance over the zero-shot approach. When\nseparating the Filtering step and reasoning on the full set of devices, we achieve a slight improvement in relevance\nfor both models; when Filtering on a list of devices, performance is the same as the zero-shot method.\nSummary. We draw the following conclusions from our quantitative evaluations of Sasha:\nâ€¢Clarifying reduces false positives for multiple models, with probable benefits to user satisfaction. (RQ3)\nâ€¢Filtering provides modest improvement to target device relevance for multiple models. (RQ3)\n7 USER STUDY\nToward answeringRQ4, we evaluate our implementation of Sasha in the presence of unpredictable user-generated\nscenarios in a hands-on study. We constructed a test home in a large lab (Fig. 12) containing smart lights, smart\ncurtains, a coffee pot, an electric kettle, a fan, a smart speaker with 5 genres of music to choose from, a smart TV,\nand a â€œcleaning robotâ€2. We extended our Python implementation of Sasha to control these devices via several\nsmart device APIs. We then invited ğ‘ = 7 participants to interact with Sasha in several open-ended scenarios of\ndaily living. Since our sessions were time-constrained, we studied Sashaâ€™s responses to immediate goals only.\n7.1 User Study Procedure & Analysis\n7.1.1 Participants and session structure. We conducted our user study with IRB oversight. We recruitedğ‘ = 7\nparticipants (Table 5) using email, snowball, and word-of-mouth. Sessions were audio-recorded and lasted 30-45\nminutes. During each session, a single participant (1) received a 5 minute overview and demonstration of Sashaâ€™s\ncapabilities; (2) enacted 3 preset scenarios of daily living while giving unconstrained, user-generated commands\nto the system; (3) enacted custom scenarios related to the participantâ€™s personal routines; and (4) participated in a\nclosing interview. We define 3 preset scenarios (â€œComing Home, â€ â€œMorning Routine, â€ and â€œHaving Partyâ€), each\n2Our cleaning robot lacks the ability to clean, but is illuminated by an indicator light when the system turns it on.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:21\nID Gender Age Education Smart Home Familiarity English Level\nRWP1 F 25-34 Bachelorâ€™s Moderately Native/bilingual\nRWP2 F 55-64 Bachelorâ€™s Slightly Native/bilingual\nRWP3 F 18-24 High school or equivalent Very Native/bilingual\nRWP4 M 18-24 Bachelorâ€™s Moderately Native/bilingual\nRWP5 M 18-24 Bachelorâ€™s Slightly Native/bilingual\nRWP6 M 18-24 Graduate Slightly Native/bilingual\nRWP7 M 25-34 Bachelorâ€™s Very Intermediate\nTable 5. Demographic characteristics of real-world participants (RWP) in our user study of Sasha. RWP gave unconstrained\ncommands during scenarios of daily living in our test home and provided perspectives on the systemâ€™s performance.\nof which has several high-level goals (â€œComing Homeâ€: Have dinner, wind down, bedtime; â€œMorning Routineâ€:\nWake up, leave home; â€œHaving Partyâ€: Prepare home, end party). After completing 3 preset scenarios, participants\ninvented custom scenarios relating to their personal routines and gave commands accordingly. We instructed\nthe participants to continue giving commands and feedback to Sasha until they felt that the devices in the\nroom sufficiently supported each of the goals for that scenario. We instructed participants to use unconstrained\nlanguage in commands and asked them to vocally share their perspectives on the systemâ€™s actions. While Sasha is\ncapable of interpreting spoken commands, for this controlled study, a researcher manually entered participantsâ€™\ncommands. Since response latency is out of scope for our work, we instructed participants to share perspectives\non the systemâ€™s actions rather than system latency. The script for sessions is in Appendix D.1.\n7.1.2 Qualitative analysis. We gathered approximately 5 hours and 30 minutes of audio from our sessions, which\nwe analyze to identify common themes of user interactions with Sasha. We elicit instances where Sasha was able\nto support unconstrained user goals, as well as cases where the systemâ€™s limitations were apparent. We label\ninteractions based on the high-level scenario they occurred in, as well as the goal of the interaction.\n7.1.3 Quantitative analysis. During each session, we logged the state of Sashaâ€™s reasoning chain as users interacted\nwith the system. We measure how frequently Sasha rejected commands, the extent to which participants gave\nfeedback, and how many commands were used to meet each scenarioâ€™s numerous goals.\n7.2 Observations\nWe present common themes from user interactions in the following. Headers indicate the high-level scenario\nthat the interaction occurred in, followed by the specific goal of the interaction.\nObservation 1: A loosened command structure can support complex, multi-part goals. Participants took\nadvantage of Sashaâ€™s loosened command structure to give commands that incorporated multiple steps:\nMorning Routine â†’Wake Up\nRWP4: Iâ€™m too hot after my shower and I would like to get some cleaning done.\nSasha: I turned on the fan to help cool you down and started the cleaning robot to assist with cleaning.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:22 â€¢ E. King, et al.\nUser-Generated â†’Miscellaneous\nRWP7: I want to have coffee at the dining room table and look outside.\nSasha: I have turned on the coffee machine in the kitchen to prepare your coffee. I have also opened the\ndining room curtains for you to look outside and adjusted the overhead light to a comfortable brightness.\nUser-Generated â†’Miscellaneous\nRWP5: Iâ€™m done watching the match and Iâ€™m gonna have some coffee and my dinner.\nSasha: The TV in the living room has been turned off as you finished watching the match. The coffee\nmachine in the kitchen has been turned on for your coffee. The overhead lights in the kitchen and dining\nroom have been turned on with a brightness of 70 and a color of white for your dinner.\nRWP5: Yeah, that makes sense.\nObservation 2: Users leverage feedback to iteratively refine action plans. Participants often opened an\ninteraction with an under-specified command, then refined action plans through iterative feedback. Their\nfeedback was often equally under-specified, and Sasha was nonetheless able to incorporate it:\nHaving Party â†’End Party\nRWP4: People are leaving, letâ€™s wind the party down.\nSasha: I have dimmed the lights in the living room, kitchen, and dining room to create a more relaxed\natmosphere. The entry light is slightly brighter to guide people out. The music player in the living room\nis now playing ambient music to help wind down the party. The TV has been turned off.\nRWP4: (Feedback) The music change was a bit jarring.\nSasha: Changed the music to a less jarring genre (folk) based on your feedback.\nRWP4: Okay, this is much better. I like it... I wouldnâ€™t have thought of that.\nIn some cases, participants began an interaction with a completely under-specified command, then added more\nspecificity through feedback:\nComing Home â†’Bedtime\nRWP1: Iâ€™m going to bed.\nSasha: All lights and TV in the living room and entry have been turned off for your bedtime.\nRWP1: (Feedback) You also have to turn off the light in the kitchen.\nSasha: I have updated the action plan to also turn off the light in the kitchen as per your feedback.\nComing Home â†’Wind Down\nRWP3: Iâ€™m going to relax now.\nSasha: I have turned off the reading lamp and TV in the living room to create a relaxing environment. I\nhave also turned on the music player and set it to play ambient music.\nRWP3: (Feedback) I wanted to turn on the TV.\nSasha: I have updated the action plan to turn on the TV as per your feedback.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:23\nIn another case, Sashaâ€™s responses to feedback were less graceful. When a participant tried to use feedback to\nchoose their preferred color of reading light, they got stuck in a back-and-forth with Sasha:\nComing Home â†’Wind Down\nRWP2: Can you change the reading lamp to a cooler tone?\nSasha: The reading lamp in the living room has been changed to a cooler tone (deep blue)\nRWP2: [Laughs] (Feedback) Thatâ€™s too cool.\nSasha: Changed the color of the reading lamp to a warmer color (deep red) to address the userâ€™s feedback\nthat the original color was too cool.\nRWP2: Ah, wow. (Feedback) Thatâ€™s a little too red.\nSasha: I have reduced the red intensity and added some green and blue to the color to make it less red.\nRWP2: Okay, uh. Hmm. A little pink.\nObservation 3: Greater sensitivity to individual preferences and context is necessary. Some interactions\nhighlighted a need for Sasha to learn individual preferences over time:\nMorning Routine â†’Leave Home\nRWP2: Iâ€™m heading out Sasha, please make sure everything is turned off.\nSasha: All devices have been turned off as you are heading out. (cleaning robot remains on)\nRWP2: Will something go on the fritz while Iâ€™m gone? Iâ€™d want that turned off.\nAnother participant expressed the opposite preference in the same scenario:\nMorning Routine â†’Leave Home\nRWP5: Iâ€™m leaving for work.\nSasha: All lights, TV, and music player have been turned off. The cleaning robot in the kitchen has been\nturned on to clean while youâ€™re away.\nRWP5: Yeah, in an ideal scenario, thatâ€™s what I expect. To do the daily chores once Iâ€™m out.\nOne participant raised an interesting point about the risks posed by Sashaâ€™s lack of sensitivity to context:\nHaving Party â†’End Party\nRWP1: My friend threw up in the kitchen.\nSasha: I have turned on the overhead light in the kitchen to its maximum brightness for better visibility\nand activated the cleaning robot to clean up the mess.\nRWP1: My immediate thought is, [thatâ€™s] a good idea...Like, it made that connection...But, the limit on\nthat is, like, thatâ€™s not what the device can do...the device is not capable of cleaning vomit. So, in actual\npractice, that would just make a mess. I feel like it needs to know what the limits of the devices are.\nObservation 4: Plans can introduce creative actions that the user may not think of themselves. In several\ncases, participants relied on Sasha to make decisions for them, or found that the system was able to craft plans\nthat the participant would not have otherwise considered:\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:24 â€¢ E. King, et al.\nMorning Routine â†’Wake Up\nRWP6: Iâ€™m not sure if I want coffee or tea. Pick one for me.\nSasha: I have chosen coffee for you. The coffee machine is now on.\nUser-Generated â†’Improve Mood\nRWP6: Help me make my girlfriend happy.\nSasha: I have set the music player in the living room to play an ambient playlist, started the coffee\nmachine in the kitchen, and opened the curtains in the dining room to create a pleasant atmosphere.\nRWP6: I think that would actually make her really happy. She likes coffee.\nUser-Generated â†’Having Visitors\nRWP4: My parents are coming to visit.\nSasha: I have set the lights in the living room, entry, kitchen, and dining room to a warm brightness. The\nTV is off to encourage conversation. The music player in the living room is set to play a jazz playlist for a\nrelaxing atmosphere. The coffee machine in the kitchen is on for your parents to enjoy a cup of coffee.\nThe cleaning robot is activated to ensure the house is clean. The curtains are open to let in natural light.\nRWP4: That one surprises me the most. The music choice and the coffee machine and the lights. I feel\nlike it thought of everything in one command...But without me having to say all the steps.\nObservation 5: Complex goals can be accomplished with only a few unconstrained interactions. We\nsummarize quantitative results from our sessions in Appendix D.1. Participants reported goals as accomplished\nin about 3 commands on average; this number varies, however, based on the complexity of the goal. Participants\ninvoked feedback to refine action plans at a rate of 0.27 across 198 commands over all sessions. Feedback was\nused to incorporate user preferences and to correct oversights caused by the systemâ€™s lack of knowledge of state\n(e.g., failing to target devices that had state set by prior interactions). We considered any rejected command to be\na false negative. This was rare, at a rate of 0.06â€”typically when commands were completely under-specified,\nsuch as â€œGoodbye, Sashaâ€ or â€œIs it raining outside?â€\nSummary. We make the following observations from our user study:\nâ€¢Feedback enables users to refine the aspects of action plans that are inherently subjective, but is only\nrequired in the minority of cases. (RQ3)\nâ€¢A loosened command structure enables the system to accomplish surprisingly complex, multi-part goals\nthat were overlooked by our initial evaluation. (RQ4)\nâ€¢The systemâ€™s effectiveness is limited in certain cases by insensitivity to user preferences and context.(RQ4)\nâ€¢The systemâ€™s creative choices can help users accomplish their goals in few steps and with added flexibility,\nwithout a need for hardcoded routines. (RQ4)\n8 DISCUSSION\nOur results raise interesting questions for future research. We discuss these in the following.\nDefining a scope for future natural language interfaces in smart homes. A design challenge in natural\nlanguage interfaces is to define their scope [35], and make their limits apparent to users so that they can form a\nmental model of their capabilities [34]. While the system we have introduced significantly expands this scope, the\nsame design challenge remains. Indeed, while current practical systems tend toward false negatives by rejecting\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:25\ncommands that do not obey the command structure, an LLM-based system introduces a new risk of false positives\nby forming non-sensical plans in response to commands that are otherwise out of scope. In cases where security\nis a concern, e.g., in a home with smart locks, false positives introduce risks. We observed in our evaluations that\nstrategies to reduce false positives tend to increase false negatives. A system that tends toward the former has\ngreater creative capabilities with added risk, while a system with a tendency for the latter is close to the current\nstate of the practice. The risk of false positives is lower when commands have evident goals but increases as\nusers explore the capabilities of the system with different commands. This is common when users first acquire an\nassistant, or when children interact with smart assistants [23]. The question remains: how do we scope and design\nan LLM-based system to provide sufficient creative capabilities at minimal risk?\nLong-term adaptation to user preferences and contexts. Sasha incorporates user feedback to improve action\nplans but does not provide long-term adaptation to user preferences or contexts. This is a desirable feature for\nsmart assistants [47], and our user study reiterates the need for it. In its simplest form, this adaptation entails\na system learning that a user prefers, e.g., one genre of music over another when they give a command like\nâ€œhelp me relaxâ€. At its most nuanced, however, an adaptive system would better understand the capabilities\nand contexts of individual devicesâ€”a cleaning robot, for instance, can clean, but different robots have different\ncapabilities, and this context necessarily informs how the system reasons about them in relation to user goals.\nFurther, the right action to meet a userâ€™s goal can vary depending on situational context: â€œget the house ready for\nmy friendsâ€ at 12:00PM demands one set of actions, while the same command at 9:00PM demands a different set\nof actions. Our relevance score provides a coarse-grained notion of how well Sashaâ€™s design does at targeting\nrelevant devices, but without the level of nuance that this particular problem will demand. User-provided feedback\nincludes valuable information that could aid in this long-term adaptation. In general, determining how to adapt\nLLMs to individual smart home preferences and constraints is an open challenge.\nSensitivity to current state. We reason about user goals in relation to devices and assignable settings without\nsensitivity to their current state. Many goals, however, inherently depend on this state. A goal like â€œmake it less\nchilly in hereâ€ expresses a desire to move from the current state to a new state; likewise a persistent goal like\nâ€œhelp me lower my power billâ€ entails a more complex change in behavior from the current state (or set of states).\nWe show in our evaluations that the system is able to generate surprisingly helpful and complex action plans, but\nsuspect that in a longitudinal setting this sensitivity to state becomes necessary. If, for instance, a user asks the\nsystem to â€œlight it up a bitâ€ and the temperature parameter of the underlying model is set to 0.0 (i.e., the output\ntokens of the action plan will always follow the most probable sequence), the system will always assign the same\nsetting to the lights. This challenge of identifying relevant state is beyond the scope of our work.\n9 CONCLUSION\nWe introduce the use of large language models (LLMs) to flexibly achieve user goals using devices and sensors\nin smart homes. We framed the problem in terms of creating high-quality, executable â€œaction plansâ€ in JSON\nthat either assign settings to available devices or propose triggers and actions for automation routines. Using a\nnovel prototype system and empirical study, we performed qualitative and quantitative analysis of smart home\naction plans created by 4 different LLMs in response to 40 under-specified user commands in 3 homes. Based\non our analysis of 600 responses from ğ‘ = 20 survey participants, we identified 7 failure modes that diminish\nuser satisfaction, along with a set of corresponding metrics. The quality of action plans was hampered by LLMsâ€™\ntendency to target irrelevant devices, assign improper settings, target extra devices, and choose suboptimal sensor\ntriggers and values for automation routines. Toward addressing the issue offalse positives when targeting devices,\nalong with a need to reduce subjectivity by incorporating user feedback, we proposed Sasha, a smarter smart\nhome assistant. Sashaâ€™s novelty lies in an iterative reasoning process that decomposes the LLMâ€™s reasoning about\nthe action plan into 4 components: clarifying the goal, filtering the devices, planning the action, and iteratively\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:26 â€¢ E. King, et al.\nimproving plans with user feedback. We justify each component of the process by ablation study, showing that\nSashaâ€™s action plans have a significantly lower rate of false positives than the naÃ¯ve approach. Finally, we conduct\na user study of Sasha with ğ‘ = 7 participants who interacted with the system during unconstrained scenarios,\ncontributing a set of observations about how users goals may be supported by this new form of smart home.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their constructive feedback on the paper. This work was funded in\npart by the National Science Foundation under grant CNS-1909221. Any opinions, findings, conclusions, or\nrecommendations expressed are those of the authors and do not necessarily reflect the views of the NSF.\nREFERENCES\n[1] 2022. Google Nest API Documentation. https://developers.google.com/nest/device-access\n[2] 2022. Insteon REST API Documentation. https://www.insteon.com/developer\n[3] 2022. Philips Hue API Documentation. https://developers.meethue.com/\n[4] 2023. IFTTT. Retrieved May 1, 2023 from https://ifttt.com/\n[5] 2023. TP-Link Kasa Smart Plugs. https://www.kasasmart.com/us/products/smart-plugs\n[6] Giovanni Acampora, Diane J Cook, Parisa Rashidi, and Athanasios V Vasilakos. 2013. A survey on ambient intelligence in healthcare.\nProc. IEEE 101, 12 (2013), 2470â€“2494.\n[7] Mussab Alaa, Aws Alaa Zaidan, Bilal Bahaa Zaidan, Mohammed Talal, and Miss Laiha Mat Kiah. 2017. A review of smart home\napplications based on Internet of Things. Journal of Network and Computer Applications 97 (2017), 48â€“65.\n[8] Frank Bentley, Chris Luvogt, Max Silverman, Rushani Wirasinghe, Brooke White, and Danielle Lottridge. 2018. Understanding the\nlong-term use of smart speaker assistants. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 2, 3\n(2018), 1â€“24.\n[9] Robert Botsch. 2011. Chapter 12: Significance and measures of association. Scopes and Methods of Political Science (2011).\n[10] Artur Branny, Maja Steen MÃ¸ller, Silviya Korpilo, Timon McPhearson, Natalie Gulsrud, Anton Stahl Olafsson, Christopher M Raymond,\nand Erik Andersson. 2022. Smarter greener cities through a social-ecological-technological systems approach. Current Opinion in\nEnvironmental Sustainability 55 (2022), 101168.\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems 33\n(2020), 1877â€“1901.\n[12] Noam Chomsky. 2002. Syntactic Structures . De Gruyter Mouton, Berlin, New York. https://doi.org/doi:10.1515/9783110218329\n[13] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung,\nCharles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,\nMichael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander\nSpiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai,\nMarie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan\nSaeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel.\n2022. PaLM: Scaling Language Modeling with Pathways. arXiv:2204.02311 [cs.CL]\n[14] Meghan Clark, Mark W Newman, and Prabal Dutta. 2017. Devices and data and agents, oh my: How smart home abstractions prime\nend-user mental models. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 1, 3 (2017), 1â€“26.\n[15] Diane J Cook. 2012. Learning setting-generalized activity models for smart spaces. IEEE intelligent systems 27, 1 (2012), 32.\n[16] William S Cooper. 1971. A definition of relevance for information retrieval. Information storage and retrieval 7, 1 (1971), 19â€“37.\n[17] Benjamin R Cowan, Nadia Pantidi, David Coyle, Kellie Morrissey, Peter Clarke, Sara Al-Shehri, David Earley, and Natasha Bandeira.\n2017. \"What can I help you with?\" infrequent usersâ€™ experiences of intelligent personal assistants. In Proceedings of the 19th International\nConference on Human-Computer Interaction with Mobile Devices and Services . 1â€“12.\n[18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for\nlanguage understanding. arXiv preprint arXiv:1810.04805 (2018).\n[19] Anind K Dey. 2001. Understanding and using context. Personal and ubiquitous computing 5 (2001), 4â€“7.\n[20] Rob Dunne, Tim Morris, and Simon Harper. 2021. A survey of ambient intelligence. ACM Computing Surveys (CSUR) 54, 4 (2021), 1â€“27.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:27\n[21] Ethan Fast, William McGrath, Pranav Rajpurkar, and Michael S Bernstein. 2016. Augur: Mining human behaviors from fiction to power\ninteractive systems. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems . 237â€“247.\n[22] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa\nNabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).\n[23] Radhika Garg and Subhasree Sengupta. 2020. He is just like me: a study of the long-term use of smart speakers by parents and children.\nProceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 4, 1 (2020), 1â€“24.\n[24] Ilche Georgievski and Marco Aiello. 2016. Automated planning for ubiquitous computing. ACM Computing Surveys (CSUR) 49, 4 (2016),\n1â€“46.\n[25] Kyeong-Ah Jeong, Gavriel Salvendy, and Robert W Proctor. 2010. Smart home design and operation preferences of Americans and\nKoreans. Ergonomics 53, 5 (2010), 636â€“660.\n[26] Dongsu Kim, Yeobeom Yoon, Jongman Lee, Pedro J Mago, Kwangho Lee, and Heejin Cho. 2022. Design and implementation of smart\nbuildings: a review of current research trend. Energies 15, 12 (2022), 4278.\n[27] Sunyoung Kim and Abhishek Choudhury. 2021. Exploring older adultsâ€™ perception and use of smart speaker-based voice assistants: A\nlongitudinal study. Computers in Human Behavior 124 (2021), 106914.\n[28] Evan King, Haoxiang Yu, Sangsu Lee, and Christine Julien. 2023. \"Get ready for a party\": Exploring smarter smart spaces with help from\nlarge language models. arXiv preprint arXiv:2303.14143 (2023).\n[29] Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for\nneural text processing. arXiv preprint arXiv:1808.06226 (2018).\n[30] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2022. Code as policies:\nLanguage model programs for embodied control. arXiv preprint arXiv:2209.07753 (2022).\n[31] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A\nsystematic survey of prompting methods in natural language processing. Comput. Surveys 55, 9 (2023), 1â€“35.\n[32] Ewa Luger and Abigail Sellen. 2016. \"Like Having a Really Bad PA\" The Gulf between User Expectation and Experience of Conversational\nAgents. In Proceedings of the 2016 CHI conference on human factors in computing systems . 5286â€“5297.\n[33] R. Lutolf. 1992. Smart Home concept and the integration of energy meters into a home based system. In Seventh International Conference\non Metering Apparatus and Tariffs for Electricity Supply 1992 . 277â€“278.\n[34] Isabela Motta and Manuela Quaresma. 2022. Exploring the opinions of experts in conversational design: A Study on usersâ€™ mental\nmodels of voice assistants. In International Conference on Human-Computer Interaction . Springer, 494â€“514.\n[35] Jesse Mu and Advait Sarkar. 2019. Do we need natural language? Exploring restricted language interfaces for complex domains. In\nExtended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems . 1â€“6.\n[36] Mahda Noura, Sebastian Heil, and Martin Gaedke. 2020. Natural language goal understanding for smart home environments. In\nProceedings of the 10th International Conference on the Internet of Things . 1â€“8.\n[37] Mahda Noura, Sebastian Heil, and Martin Gaedke. 2020. VISH: Does Your Smart Home Dialogue System Also Need Training Data?. In\nWeb Engineering: 20th International Conference, ICWE 2020, Helsinki, Finland, June 9â€“12, 2020, Proceedings 20 . Springer, 171â€“187.\n[38] OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]\n[39] OpenAI. 2023. tiktoken: a fast BPE tokeniser for use with OpenAIâ€™s models. https://github.com/openai/tiktoken.\n[40] Javier Palanca, Elena del Val, Ana Garcia-Fornes, Holger Billhardt, Juan Manuel Corchado, and Vicente JuliÃ¡n. 2018. Designing a\ngoal-oriented smart-home environment. Information Systems Frontiers 20 (2018), 125â€“142.\n[41] Joon Sung Park, Joseph C Oâ€™Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents:\nInteractive simulacra of human behavior. arXiv preprint arXiv:2304.03442 (2023).\n[42] Alisha Pradhan, Amanda Lazar, and Leah Findlater. 2020. Use of intelligent voice assistants by older adults with low technology use.\nACM Transactions on Computer-Human Interaction (TOCHI) 27, 4 (2020), 1â€“27.\n[43] Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang, and Huajun Chen. 2022.\nReasoning with Language Model Prompting: A Survey. arXiv preprint arXiv:2212.09597 (2022).\n[44] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al . 2018. Improving language understanding by generative\npre-training. (2018).\n[45] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog 1, 8 (2019), 9.\n[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020.\nExploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research 21, 1 (2020),\n5485â€“5551.\n[47] Michaela R Reisinger, Sebastian Prost, Johann Schrammel, and Peter FrÃ¶hlich. 2022. User requirements for the design of smart homes:\ndimensions and goals. Journal of Ambient Intelligence and Humanized Computing (2022), 1â€“20.\n[48] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael SchÃ¤rli, and Denny Zhou. 2023. Large\nlanguage models can be easily distracted by irrelevant context. arXiv preprint arXiv:2302.00093 (2023).\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:28 â€¢ E. King, et al.\n[49] Jungwoo Shin, Yuri Park, and Daeho Lee. 2018. Who will be smart home users? An analysis of adoption and diffusion of smart homes.\nTechnological Forecasting and Social Change 134 (2018), 246â€“253.\n[50] Marta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse BjÃ¸rn Kristensen, Kourosh Darvish, AlÃ¡n Aspuru-Guzik,\nFlorian Shkurti, and Animesh Garg. 2023. Errors are Useful Prompts: Instruction Guided Task Programming with Verifier-Assisted\nIterative Prompting. arXiv preprint arXiv:2303.14100 (2023).\n[51] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie\nBaker, Yu Du, et al. 2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239 (2022).\n[52] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman\nGoyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971\n(2023).\n[53] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal\nBhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\n[54] Pooja Upadhyay, Sharon Heung, Shiri Azenkot, and Robin N Brewer. 2023. Studying Exploration & Long-Term Use of Voice Assistants\nby Older Adults. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems . 1â€“11.\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. 2017.\nAttention is all you need. Advances in neural information processing systems 30 (2017).\n[56] Jonathan J Webster and Chunyu Kit. 1992. Tokenization as the initial phase in NLP. In COLING 1992 volume 4: The 14th international\nconference on computational linguistics .\n[57] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021.\nFinetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652 (2021).\n[58] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain of thought prompting\nelicits reasoning in large language models. arXiv preprint arXiv:2201.11903 (2022).\n[59] Mark Weiser. 1999. The computer for the 21st century. ACM SIGMOBILE mobile computing and communications review 3, 3 (1999), 3â€“11.\n[60] Charlie Wilson, Tom Hargreaves, and Richard Hauxwell-Baldwin. 2015. Smart homes and their users: a systematic analysis and key\nchallenges. Personal and Ubiquitous Computing 19 (2015), 463â€“476.\n[61] Jimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg, Szymon Rusinkiewicz, and Thomas\nFunkhouser. 2023. TidyBot: Personalized Robot Assistance with Large Language Models. arXiv:2305.05658 [cs.RO]\n[62] Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022. Ai chains: Transparent and controllable human-ai interaction by chaining\nlarge language model prompts. In Proceedings of the 2022 CHI conference on human factors in computing systems . 1â€“22.\n[63] Haoxiang Yu, Jie Hua, and Christine Julien. 2021. Dataset: Analysis of IFTTT Recipes to Study How Humans Use Internet-of-Things\n(IoT) Devices. In Proceedings of the 19th ACM Conference on Embedded Networked Sensor Systems . 537â€“541.\n[64] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. 2019.\nFine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593 (2019).\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:29\nAppendix A USER COMMANDS\nCommand Example IFTTT Routine [63] Goal\nAmbient Temperature\nmake it less chilly in here â€œTurn A/C offâ€ immediate\nhelp me cool off â€œTurn A/C onâ€ immediate\nmake it less stuffy in here â€˜Circulate air when CO2 risesâ€ immediate\nturn the AC off when itâ€™s cold outside â€œAutomatically turn off AC if itâ€™s cool outsideâ€ persistent\nturn the heat off when itâ€™s hot outside â€œWarm weather...time to turn off the heatingâ€ persistent\nmake it comfortable at night â€œAdjust the temperature... when I get into bedâ€ persistent\nAmbient Luminance\nmake it less bright â€œChange brightness of your lightâ€ immediate\nget ready for bed â€œTurn lights out for bed with Alexa and Hueâ€ immediate\nhelp me see better â€œTurn on lights if it becomes darkâ€ immediate\nkeep it well-lit after sundown â€œTurn your lights on after sunsetâ€ persistent\nuse natural light once the sun is up â€œTurn off lights at sunriseâ€ persistent\nkeep the lights off when Iâ€™m gone â€œTurn off lights when leaving a geofenceâ€ persistent\nSecurity\ncheck the front door â€œClose the garage doorâ€ immediate\nlock up â€œTell Alexa to lock your doorâ€ immediate\nlet the guest in â€œAsk Alexa to unlock door when guests arriveâ€ immediate\nlock the door when I leave â€œWhen I leave my house, close garage doorâ€ persistent\nlet me know when thereâ€™s a visitor â€œTurn on lights when there is motion at doorâ€ persistent\nkeep the home safe during the day â€œ\"Wake the camera up when I leave homeâ€ persistent\nEnergy Saving\nturn off the thermostat when I donâ€™t need it â€œIf you exit an area, turn your heating offâ€ persistent\nsave energy when Iâ€™m gone â€œTurn off TP-Link Plug when you leave areaâ€ persistent\nhelp me save energy â€œTurn your lights off as you leave homeâ€ persistent\nhelp me lower my power bill â€œTurn off TP-Link Plug when you leave areaâ€ persistent\nAmbient Atmosphere\nset up for a party â€œStart a party! Put your lights in disco modeâ€ immediate\nmake it cozy in here â€œCreate a warm atmosphere inside if itâ€™s coldâ€ immediate\nhelp me wind down â€œChange your Hue lights... to help you sleepâ€ immediate\nmake it cozy when it rains â€œWarm the lights to a cozy... when rain startsâ€ persistent\nlet me know when the weather is bad â€œAlert me via Hue if the wind gets dangerousâ€ persistent\nhelp me sleep better â€œMake bedroom light orange at 10pmâ€ persistent\nRobot Control\nclean up the bedroom â€œTell Alexa to start your Roombaâ€ immediate\nstop cleaning â€œTell Google to turn off your botâ€ immediate\ntidy up the house â€œTell Alexa to start your Roombaâ€ immediate\nstop cleaning after sunset â€œSchedule cleaning at a certain time every dayâ€ persistent\ndonâ€™t clean when people are here â€œWhen I arrive home, dock Roombaâ€ persistent\nclean up while Iâ€™m gone â€œWhen I leave home, start a cleaning jobâ€ persistent\nOther Appliances\ngive me some privacy â€œTurn on PowerView Schedules with Alexaâ€ immediate\nlet some sun in â€œTurn on PowerView Schedules with Alexaâ€ immediate\nfinish the coffee â€œSay â€˜Alexa trigger coffeeâ€™ to start the coffeeâ€ immediate\nkeep it humid at night â€œSet your air purifier to sleep mode at nightâ€ persistent\nI need coffee in the morning â€œSay â€˜Alexa trigger coffeeâ€™ to start the coffeeâ€ persistent\nlet some sun in if the weather is nice â€œTurn on PowerView Schedules with Alexaâ€ persistent\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:30 â€¢ E. King, et al.\nAppendix B USER SURVEY DESIGN\nB.1 Demographic Questions\nHowoldareyou?18-24yearsold25-34yearsold35-44yearsold45-54yearsold55-64yearsold65+yearsold\nHowdoyoudescribeyourself?MaleFemaleNon-binaryPrefertoself-describe:______________________Prefernottosay\nWhatisthehighestlevelofeducationyouhavecompleted?SomehighschoolorlessHighschooldiplomaorGEDSomecollege,butnodegreeAssociatesortechnicaldegreeBachelorâ€™sdegreeGraduateorprofessionaldegree(MA,MS,MBA,PhD,JD,MD,DDSetc.)Prefernottosay\nHowfamiliarareyouwithsmarthomeassistants(ex:Alexa)andsmartdevices(ex:smartlightbulbs)?NotfamiliaratallSlightlyfamiliarModeratelyfamiliarVeryfamiliarExtremelyfamiliar\nWhatisyourlevelofproficiencywithEnglish?BasicproficiencyIntermediateproficiencyNative/bilingualproficiency\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:31\nB.2 Instructions\nInstructions\nThere are 32 questions on this survey, and it should take about 30 minutes of your time. The\nfollowing describes how to complete the survey.\nIntroduction\nImagine a person lives in a smart home that they can control by talking to it, like talking to\nAlexa or Siri. There are light bulbs and other electronics that can be controlled using voice\ncommands. If you already know about smart assistants like Google Home, Siri, or Alexa,\nimagine that this smart home is smarter . The person can ask the home things like \"make it\ncozier in here\" and the home will try to Ggure out the best way to do it with the smart devices\navailable.\nHow to answer the survey questions\nEach question will show (1) a layout of the devices in a smart home, (2) a verbal command\nthat a person gave to the home, and (3) a description of how the smart home responded.\nThe Smart Home Layout\nStart by looking at the picture of the home and noticing which smart devices it has in it. Here\nis an example:\n Â \nNotice that there are overhead lights in each room (represented by the light bulb icons). The\nliving room also has a thermostat, a smart speaker, a TV, and a lamp in the corner. Once\nyou've taken note of the devices in the house, move on to the next step.\nThe User Command\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:32 â€¢ E. King, et al.\nWe will show you a command the person gave the home and ask you whether or not\ndifferent parts of the command were speciGc. By speciGc, we mean precise or exact: if\nsomeone gave you the same instructions, you would know exactly what they wanted you to\ndo.\nIs it speciGc what action  the person expects the smart home to do in response?Â \nExample: â€œturn on the lightâ€, the action is speciGcally to â€œturn onâ€ something\nIs it speciGc which target  device(s) the person expects the smart home to control?Â \nExample: â€œturn on the lightâ€, the target is speciGcally â€œthe lightâ€ Â \nIn some cases, the person might want the home to do something in the future based on a\ntrigger . Is it speciGc what the trigger should be?\nExample: â€œturn on the light at 10pmâ€, the trigger is speciGcally â€œat 10pmâ€\nThe Smart Home Response\nWe will show you a written description of what the smart home did in response to the\ncommand and ask you how satisGed you would be with that response. Imagine you  are the\nperson who gave the command. Did the smart home do a good job of responding to it?Â \nTips for Answering\nTry to focus on how satisGed you'd be with the  actions the smart home performed and try to\ndisregard, for example, how \"friendly\" the words of the response were.Â \nThe smart home response may mention using certain sensors in the home. For example, it\nmay mention using a sensor to detect a person's motion, or changes in the weather. You can\nassume that these sensors exist in the house, even if they aren't shown in the picture of the\nhome.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:33\nYour Rationale\nWe will then ask you to describe what was satisfactory and/or unsatisfactory about the\nhome's response. If the home responded in an odd or interesting way, point it out here.\nProceed to the next page to begin the survey.\nâ†’\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:34 â€¢ E. King, et al.\nB.3 Example Question\nThis is the layout of the home and the smart devices in it.\nA person gives this command to the smart home:\n\"don't clean when people are here\"\nCheck all that apply:\nThe smart home responds by doing this:\nThe action  the user wants the home to do is speci=c.\nThe target  smart device(s) of the command is/are speci=c.\nThe event that should trigger  the action is speci=c.\nNone of the above.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:35\n\"When motion is detected at the entry, the Roomba in the living room will be turned off\nto stop cleaning.\"\nHow satis=ed would you be with this response from a smart home? Focus on what the\nsmart home says it did , and not on the tone or friendliness of the response.\nBrieGy describe what was satisfactory and/or unsatisfactory about the home's response:\nVery unsatis=ed\nUnsatis=ed\nNeutral\nSatis=ed\nVery satis=ed\nâ†’\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:36 â€¢ E. King, et al.\nAppendix C FAILURE MODE EXAMPLES\nCommand Action Plan Home Category Failure Mode Rationale\nstop cleaning after sun-\nset\nWhen the local time sensor de-\ntects that it is sunset, the smart\nspeaker in the living room will\nbe commanded to stop the clean-\ning process.\nâ„2 Device No option exists SP18: Speakers do not clean\nmake it less stuffy in here The thermostat in the living\nroom has been set to 70 degrees\nto make the room less stuffy.\nâ„3 Device Option exists SP19: Turning on the diffuser\nwould have helped too.\ngive me some privacy All lights and audio devices have\nbeen turned off for privacy.\nâ„2 Device Extra SP13: I would like the lights to\nstay on\ndonâ€™t clean when people\nare here\nWhen motion is detected in the\nentry, the cleaning robot will be\nturned off.\nâ„1 Device Hallucinated SP12: There is no robot\nmake it cozy in here Adjusted the lights to a warm\ncolor and dimmed them to cre-\nate a cozy atmosphere. The ther-\nmostat in the living room has\nbeen set to a comfortable tem-\nperature. The smart speaker vol-\nume has been lowered.\nâ„2 Device Setting SP7: I would instead of lowering\nspeaker volume, set it to specific\nvalue, thus preventing it from\nbecoming super quiet if initially\nspeakers were already very low.\nhelp me sleep better When motion is detected in the\nbedroom, the system will turn\noff the lights in the bedroom\nand living room. The thermo-\nstat in the living room will be\nset to a comfortable 68 degrees.\nThe smart speaker in the living\nroom will be turned on at a low\nvolume to provide white noise,\nwhich can promote better sleep.\nâ„2 Sensor Suboptimal choice SP6: I think the problem is that\nit bases its actions on the motion\ndetector.\nturn the heat off when\nitâ€™s hot outside\nWhen the weather is hot, heat-\ning system will be turned off.\nâ„1 Sensor Trigger value SP12: Hot would need to be de-\nfined\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\nSasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models â€¢ 12:37\nAppendix D USER STUDY MATERIALS\nD.1 Session Script\nIntroduction. Today weâ€™re testing a system called Sasha. Sasha stands for â€œsmarter smart home assistantâ€. Sasha\ntries to figure out how it can use different devices in the home to help you with your goals. It does this in response\nto open-ended or conversational verbal commands. If I tell Sasha to â€œmake it cozy in hereâ€ or I tell Sasha â€œhey, I\nwant to read a book in the living roomâ€, it tries to figure out what it can do with the devices in the room to help\nyou, without you telling it specifically what you want it to do. Sasha has a few limitations:\nâ€¢Sasha canâ€™t access information on the internet.\nâ€¢Sasha doesnâ€™t know where you are in the room.\nâ€¢Sasha doesnâ€™t have memory. Unless you give feedback on something Sasha just did, it starts from scratch.\nâ€¢Sasha is slow to respond. As we work with Sasha today, try to be forgiving of the delay and focus on the\nthings Sasha does in response to your commands.\nParticipantâ€™s Role. We are interested in your responses to what Sasha does. We are also interested in how you\nphrase your commands to Sasha. Weâ€™ll act out a few different scenarios, you can give Sasha different commands\nto get the house ready, and give us your perspectives. If it did something weird, tell us, like â€œI donâ€™t really know\nwhy it turned on that light over thereâ€. If it did something good or surprising, let us know. Any perspective you\nshare, good, bad, or specific, is helpful.\nTour.Let me show you each of the devices that Sasha can control.\nâ€¢Entryway. Sasha can control the overhead light.\nâ€¢Dining room. Sasha can control the overhead light, and also open the curtains.\nâ€¢Kitchen. Sasha can control the overhead light, and turn the coffee pot and tea kettle on and off. When Sasha\nturns on either of them, the light above them will turn on [demonstrate]. Sasha can control this fan. There\nis also a â€œcleaning robotâ€ down here. It canâ€™t actually clean; but Sasha thinks it can. When Sasha turns on\nthe robot, a light will turn on and illuminate the robot.\nâ€¢Living room. Sasha can turn the TV on and off, control the overhead light, the two lamps, and choose from\na few different genres of music to play on the stereo.\nExamples. Here are some example commands [input these commands].\nâ€¢â€œIâ€™m going to read a book in the living roomâ€\nâ€¢â€œI think I wanna watch a movieâ€ (Feedback) â€œThe lights are too bright for a movie. â€\nâ€¢â€œCan you let some sun in?â€\nScenarios. We will work through three scenarios, each of which has a few goals. An example of a scenario is\nsomething like â€œcoming home from workâ€, and a goal would be â€œhave dinnerâ€. At the start of each scenario, we\nwill turn off all the devices in the room. Your task is to work with Sasha to get the home set up in a way that\nsupports your personal definition of the goals in that scenario. When you feel like the home is ready, let me know\nand we will move on to the next scenario. Once we are done with the pre-defined scenarios, we will try some\nscenarios that are a part of your personal routine.\nClosing Questions.\nâ€¢Was there anything you were surprised that Sasha was able to do? What was most surprising?\nâ€¢Was there anything you felt like Sasha should have been able to do but couldnâ€™t? Did you notice anything\nthat Sasha consistently got wrong?\nâ€¢Are there any other devices you could imagine Sasha connected to? How would or could you use it if we\nhad those devices?\nâ€¢Would you use Sasha at home or at work? What would you use it for? Why wouldnâ€™t you use it?\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.\n12:38 â€¢ E. King, et al.\nD.2 Session Summaries\nID Scenario G oals C mds C /G Example Command & Goal Fdbk ğ¹ğ‘ Err\nRWP1\nComing Home 3 8 2.67 Iâ€™m going to bed. (Bedtime) 0.12 0.00 0.12\nMorning Routine 2 7 3.50 Make sure the lights are on while while Iâ€™m gone. (Leave Home) 0.14 0.43 0.00\nHaving Party 2 8 4.00 My friends need to leave. (End Party) 0.25 0.00 0.00\nUser-Generated 1 8 8.00 I feel sad. Cheer me up. (Improve Mood) 0.25 0.00 0.12\nRWP2\nComing Home 3 12 4.00 Clean up all the dishes. (Have Dinner) 0.50 0.00 0.00\nMorning Routine 2 6 3.00 Good morning Sasha, please wake up the house. (Wake Up) 0.17 0.00 0.00\nHaving Party 2 5 2.50 Please put on the party music. (Prepare Home) 0.00 0.00 0.00\nUser-Generated 2 3 1.50 I need to take a call and itâ€™s noisy. (Phone Call) 0.00 0.33 0.00\nRWP3\nComing Home 3 8 2.67 Iâ€™m going to relax now. (Wind Down) 0.25 0.00 0.00\nMorning Routine 1 2 2.00 Iâ€™m ready to start the day. (Wake Up) 0.50 0.00 0.00\nHaving Party 2 3 1.50 Wind down the party. (End Party) 0.33 0.00 0.00\nUser-Generated 1 2 2.00 Letâ€™s have a movie night. (TV Time) 0.50 0.00 0.00\nRWP4\nComing Home 3 7 2.33 Iâ€™m too scared about the movie Iâ€™m watching. (Wind Down) 0.29 0.00 0.00\nMorning Routine 2 5 2.50 Iâ€™m leaving, see you later in the afternoon. (Leave Home) 0.20 0.00 0.00\nHaving Party 2 5 2.50 People are leaving, letâ€™s wind the party down. (End Party) 0.60 0.00 0.00\nUser-Generated 5 8 1.60 My favorite team is about to play. (TV Time) 0.12 0.25 0.00\nRWP5\nComing Home 3 8 2.67 Make me some coffee. (Have Dinner) 0.12 0.00 0.00\nMorning Routine 2 7 3.50 Iâ€™m leaving for work, make sure you clean the house. (Leave Home) 0.43 0.00 0.00\nHaving Party 2 3 1.50 Set the home for a party. (Prepare Home) 0.00 0.00 0.00\nUser-Generated 1 6 6.00 Iâ€™m done for the day, Iâ€™m going out. (Miscellaneous) 0.00 0.17 0.00\nRWP6\nComing Home 3 11 3.67 Iâ€™m going to need a relaxing drink. (Have Dinner) 0.36 0.00 0.00\nMorning Routine 2 6 3.00 Goodbye Sasha. (Leave Home) 0.17 0.17 0.00\nHaving Party 2 12 6.00 Set the mood for a party. (Prepare Home) 0.25 0.08 0.00\nUser-Generated 2 11 5.50 Help me make my girlfriend happy. (Improve Mood) 0.73 0.00 0.00\nRWP7\nComing Home 3 10 3.33 I need some hot water as well. (Wind Down) 0.20 0.00 0.00\nMorning Routine 2 11 5.50 Iâ€™m still feeling tired. (Wake Up) 0.18 0.09 0.09\nHaving Party 2 6 3.00 Set the lights in party mode. (Prepare Home) 0.17 0.00 0.00\nUser-Generated 2 10 5.00 I want to have soup. (Mealtime) 0.30 0.10 0.00\n198 3.39 0.27 0.06 0.02\nTable 6. Quantitative summary of user study sessions. Each real-world participant (RWP) gave unconstrained commands during 3 preset scenarios with\nseveral goals, followed by user-generated scenarios. All commands were unconstrained; we provide examples. Goals: number of goals in the scenario,\nCmds: total number of commands for all goals in the scenario, Fdbk: portion of commands providing Feedback,ğ¹ğ‘: false negatives, Err: JSON error rate.\nProc. ACM Interact. Mob. Wearable Ubiquitous Technol., Vol. 8, No. 1, Article 12. Publication date: March 2024.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6314398050308228
    },
    {
      "name": "Function (biology)",
      "score": 0.5781316161155701
    },
    {
      "name": "Humanâ€“computer interaction",
      "score": 0.571376383304596
    },
    {
      "name": "Action (physics)",
      "score": 0.5350953340530396
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.45224249362945557
    },
    {
      "name": "Mood",
      "score": 0.4346621334552765
    },
    {
      "name": "Home automation",
      "score": 0.43328794836997986
    },
    {
      "name": "Automation",
      "score": 0.4313186705112457
    },
    {
      "name": "User interface",
      "score": 0.43056654930114746
    },
    {
      "name": "Psychology",
      "score": 0.18446806073188782
    },
    {
      "name": "Engineering",
      "score": 0.17898502945899963
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}