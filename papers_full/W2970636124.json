{
  "title": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models",
  "url": "https://openalex.org/W2970636124",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4300573098",
      "name": "Araci, Dogu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2592751531",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W1965235124",
    "https://openalex.org/W2584429674",
    "https://openalex.org/W2743945814",
    "https://openalex.org/W2625464253",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2785939461",
    "https://openalex.org/W2762466482",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2306706380",
    "https://openalex.org/W3123756285",
    "https://openalex.org/W2900167092",
    "https://openalex.org/W2085582472",
    "https://openalex.org/W2138293190",
    "https://openalex.org/W3125952890",
    "https://openalex.org/W2945824677",
    "https://openalex.org/W2888160375",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2049434052",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W2781474777",
    "https://openalex.org/W2135731857",
    "https://openalex.org/W2798064797",
    "https://openalex.org/W2964236337",
    "https://openalex.org/W1996235486",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2907041544",
    "https://openalex.org/W2470242235",
    "https://openalex.org/W2903285529",
    "https://openalex.org/W2949433733",
    "https://openalex.org/W2108646579"
  ],
  "abstract": "Financial sentiment analysis is a challenging task due to the specialized language and lack of labeled data in that domain. General-purpose models are not effective enough because of the specialized language used in a financial context. We hypothesize that pre-trained language models can help with this problem because they require fewer labeled examples and they can be further trained on domain-specific corpora. We introduce FinBERT, a language model based on BERT, to tackle NLP tasks in the financial domain. Our results show improvement in every measured metric on current state-of-the-art results for two financial sentiment analysis datasets. We find that even with a smaller training set and fine-tuning only a part of the model, FinBERT outperforms state-of-the-art machine learning methods.",
  "full_text": "FinBERT: Financial Sentiment Analysis with Pre-trained Language Models\nsubmitted in partial fulfillment for the degree of master of science\nDogu Araci\n12255068\nmaster information studies\ndata science\nfaculty of science\nuniversity of amsterdam\n2019-06-25\nInternal Supervisor External Supervisor\nTitle, Name Dr Pengjie Ren Dr Zulkuf Genc\nAffiliation UvA, ILPS Naspers Group\nEmail p.ren@uva.nl zulkuf.genc@naspers.com\narXiv:1908.10063v1  [cs.CL]  27 Aug 2019\nFinBERT: Financial Sentiment Analysis with Pre-trained\nLanguage Models\nDogu Tan Araci\ndogu.araci@student.uva.nl\nUniversity of Amsterdam\nAmsterdam, The Netherlands\nABSTRACT\nFinancial sentiment analysis is a challenging task due to the spe-\ncialized language and lack of labeled data in that domain. General-\npurpose models are not effective enough because of specialized\nlanguage used in financial context. We hypothesize that pre-trained\nlanguage models can help with this problem because they require\nfewer labeled examples and they can be further trained on domain-\nspecific corpora. We introduce FinBERT, a language model based\non BERT, to tackle NLP tasks in financial domain. Our results show\nimprovement in every measured metric on current state-of-the-\nart results for two financial sentiment analysis datasets. We find\nthat even with a smaller training set and fine-tuning only a part of\nthe model, FinBERT outperforms state-of-the-art machine learning\nmethods.\n1 INTRODUCTION\nPrices in an open market reflects all of the available information\nregarding assets exchanged in an economy [16]. When new infor-\nmation becomes available, all actors in the economy update their\npositions and prices adjust accordingly, which makes beating the\nmarkets consistently impossible. However, the definition of \"new in-\nformation\" might change as new information retrieval technologies\nbecome available and early-adoption of such technologies might\nprovide an advantage in the short-term.\nAnalysis of financial texts, be it news, analyst reports or official\ncompany announcements is a possible source of new information.\nWith unprecedented amount of such text being created every day,\nmanually analyzing these and deriving actionable insights from\nthem is too big of a task for any single entity. Hence, automated\nsentiment or polarity analysis of texts produced by financial ac-\ntors using natural language processing (NLP) methods has gained\npopularity during the last decade [4].\nThe principal research interest for this thesis is the polarity\nanalysis, which is classifying text as positive, negative or neutral,\nin a specific domain. It requires to address two challenges: 1) The\nmost sophisticated classification methods that make use of neural\nnets require vast amounts of labeled data and labeling financial\ntext snippets requires costly expertise. 2) The sentiment analysis\nmodels trained on general corpora are not suited to the task, because\nfinancial texts have a specialized language with unique vocabulary\nand have a tendency to use vague expressions instead of easily-\nidentified negative/positive words.\nUsing carefully crafted financial sentiment lexicons such as\nLoughran and McDonald (2011) [11] may seem a solution because\nthey incorporate existing financial knowledge into textual analysis.\nHowever, they are based on \"word counting\" methods, which come\nshort in analyzing deeper semantic meaning of a given text.\nNLP transfer learning methods look like a promising solution\nto both of the challenges mentioned above, and are the focus of\nthis thesis. The core idea behind these models is that by train-\ning language models on very large corpora and then initializing\ndown-stream models with the weights learned from the language\nmodeling task, a much better performance can be achieved. The\ninitialized layers can range from the single word embedding layer\n[23] to the whole model [5]. This approach should, in theory, be an\nanswer to the scarcity of labeled data problem. Language models\ndon’t require any labels, since the task is predicting the next word.\nThey can learn how to represent the semantic information. That\nleaves the fine-tuning on labeled data only the task of learning how\nto use this semantic information to predict the labels.\nOne particular component of the transfer learning methods is the\nability to further pre-train the language models on domain specific\nunlabeled corpus. Thus, the model can learn the semantic relations\nin the text of the target domain, which is likely to have a differ-\nent distribution than a general corpus. This approach is especially\npromising for a niche domain like finance, since the language and\nvocabulary used is dramatically different than a general one.\nThe goal of this thesis is to test these hypothesized advantages\nof using and fine-tuning pre-trained language models for financial\ndomain. For that, sentiment of a sentence from a financial news\narticle towards the financial actor depicted in the sentence will be\ntried to be predicted, using the Financial PhraseBank created by\nMalo et al. (2014) [17] and FiQA Task 1 sentiment scoring dataset\n[15].\nThe main contributions of this thesis are the following:\n•We introduce FinBERT, which is a language model based on\nBERT for financial NLP tasks. We evaluate FinBERT on two\nfinancial sentiment analysis datasets.\n•We achieve the state-of-the-art on FiQA sentiment scoring\nand Financial PhraseBank.\n•We implement two other pre-trained language models, ULM-\nFit and ELMo for financial sentiment analysis and compare\nthese with FinBERT.\n•We conduct experiments to investigate several aspects of\nthe model, including: effects of further pre-training on fi-\nnancial corpus, training strategies to prevent catastrophic\nforgetting and fine-tuning only a small subset of model lay-\ners for decreasing training time without a significant drop\nin performance.\nThe rest of the thesis is structured as follows: First, relevant lit-\nerature in both financial polarity analysis and pre-trained language\nmodels are discussed (Section 2). Then, the evaluated models are\ndescribed (Section 3). This is followed by the description of the\nexperimental setup being used (Section 4). In Section 5, we present\n1\nthe experimental results on the financial sentiment datasets. Then\nwe further analyze FinBERT from different perspectives in Section\n6. Finally, we conclude with Section 7.\n2 RELATED LITERATURE\nThis section describes previous research conducted on sentiment\nanalysis in finance (2.1) and text classification using pre-trained\nlanguage models (2.2).\n2.1 Sentiment analysis in finance\nSentiment analysis is the task of extracting sentiments or opinions\nof people from written language [ 10]. We can divide the recent\nefforts into two groups: 1) Machine learning methods with features\nextracted from text with \"word counting\" [1, 19, 28, 30], 2) Deep\nlearning methods, where text is represented by a sequence of em-\nbeddings [2, 25, 32]. The former suffers from inability to represent\nthe semantic information that results from a particular sequence of\nwords, while the latter is often deemed as too \"data-hungry\" as it\nlearns a much higher number of parameters [18].\nFinancial sentiment analysis differs from general sentiment anal-\nysis not only in domain, but also the purpose. The purpose behind\nfinancial sentiment analysis is usually guessing how the markets\nwill react with the information presented in the text [9]. Loughran\nand McDonald (2016) presents a thorough survey of recent works\non financial text analysis utilizing machine learning with \"bag-of-\nwords\" approach or lexicon-based methods [12]. For example, in\nLoughran and McDonald (2011), they create a dictionary of finan-\ncial terms with assigned values such as \"positive\" or \"uncertain\"\nand measure the tone of a documents by counting words with a spe-\ncific dictionary value [11]. Another example is Pagolu et al. (2016),\nwhere n-grams from tweets with financial information are fed into\nsupervised machine learning algorithms to detect the sentiment\nregarding the financial entity mentioned.\nOn of the first papers that used deep learning methods for tex-\ntual financial polarity analysis was Kraus and Feuerriegel (2017) [7].\nThey apply an LSTM neural network to ad-hoc company announce-\nments to predict stock-market movements and show that method\nto be more accurate than traditional machine learning approaches.\nThey find pre-training their model on a larger corpus to improve\nthe result, however their pre-training is done on a labeled dataset,\nwhich is a more limiting approach then ours, as we pre-train a\nlanguage model as an unsupervised task.\nThere are several other works that employ various types of\nneural architectures for financial sentiment analysis. Sohangir et al.\n(2018) [26] apply several generic neural network architectures to\na StockTwits dataset, finding CNN as the best performing neural\nnetwork architecture. Lutz et al. 2018 [13] take the approach of using\ndoc2vec to generate sentence embeddings in a particular company\nad-hoc announcement and utilize multi-instance learning to predict\nstock market outcomes. Maia et al. (2018) [14] use a combination of\ntext simplification and LSTM network to classify a set of sentences\nfrom financial news according to their sentiment and achieve state-\nof-the-art results for the Financial PhraseBank, which is used in\nthesis as well.\nDue to lack of large labeled financial datasets, it is difficult to\nutilize neural networks to their full potential for sentiment analysis.\nEven when their first (word embedding) layers are initialized with\npre-trained values, the rest of the model still needs to learn complex\nrelations with relatively small amount of labeled data. A more\npromising solution could be initializing almost the entire model\nwith pre-trained values and fine-tuning those values with respect\nto the classification task.\n2.2 Text classification using pre-trained\nlanguage models\nLanguage modeling is the task of predicting the next word in a given\npiece of text. One of the most important recent developments in\nnatural language processing is the realization that a model trained\nfor language modeling can be successfully fine-tuned for most\ndown-stream NLP tasks with small modifications. These models\nare usually trained on very large corpora, and then with addition\nof suitable task-specific layers fine-tuned on the target dataset [6].\nText classification, which is the focus of this thesis, is one of the\nobvious use-cases for this approach.\nELMo (Embeddings from Language Models) [23] was one of the\nfirst successful applications of this approach. With ELMo, a deep\nbidirectional language model is pre-trained on a large corpus. For\neach word, hidden states of this model is used to compute a con-\ntextualized representation. Using the pre-trained weights of ELMo,\ncontextualized word embeddings can be calculated for any piece\nof text. Initializing embeddings for down-stream tasks with those\nwere shown to improve performance on most tasks compared to\nstatic word embeddings such as word2vec or GloVe. For text classi-\nfication tasks like SST-5, it achieved state-of-the-art performance\nwhen used together with a bi-attentive classification network [20].\nAlthough ELMo makes use of pre-trained language models for\ncontextualizing representations, still the information extracted us-\ning a language model is present only in the first layer of any model\nusing it. ULMFit (Universal Language Model Fine-tuning) [5] was\nthe first paper to achieve true transfer learning for NLP, as using\nnovel techniques such as discriminative fine-tuning, slanted tri-\nangular learning rates and gradual unfreezing. They were able to\nefficiently fine-tune a whole pre-trained language model for text\nclassification. They also introduced further pre-training of the lan-\nguage model on a domain-specific corpus, assuming target task\ndata comes from a different distribution than the general corpus\nthe initial model was trained on.\nULMFit’s main idea of efficiently fine-tuning a pre-trained a\nlanguage model for down-stream tasks was brought to another level\nwith Bidirectional Encoder Representations from Transformers\n(BERT) [3], which is also the main focus of this paper. BERT has\ntwo important differences from what came before: 1) It defines the\ntask of language modeling as predicting randomly masked tokens\nin a sequence rather than the next token, in addition to a task of\nclassifying two sentences as following each other or not. 2) It is\na very big network trained on an unprecedentedly large corpus.\nThese two factors enabled in to achieve state-of-the-art results in\nmultiple NLP tasks such as, natural language inference or question\nanswering.\nThe specifics of fine-tuning BERT for text classification has not\nbeen researched thoroughly. One such recent work is Sun et al.\n2\n(2019) [27]. They conduct a series of experiments regarding differ-\nent configurations of BERT for text classification. Some of their\nresults will be referenced throughout the rest of the thesis, for the\nconfiguration of our model.\n3 METHOD\nIn this section, we will present our BERT implementation for finan-\ncial domain named as FinBERT, after giving a brief background on\nrelevant neural architectures.\n3.1 Preliminaries\n3.1.1 LSTM. Long short-term memory (LSTM) is a type of re-\ncurrent neural network that allows long-term dependencies in a\nsequence to persist in the network by using \"forget\" and \"update\"\ngates. It is one of the primary architectures for modeling any sequen-\ntial data generation process, from stock prices to natural language.\nSince a text is a sequence of tokens, the first choice for any LSTM\nnatural language processing model is determining how to initially\nrepresent a single token. Using pre-trained weights for initial to-\nken representation is the common practice. One such pre-training\nalgorithm is GLoVe (Global Vectors for Word Representation) [22].\nGLoVr is a model for calculating word representations with the\nunsupervised task of training a log-bilinear regression model on\na word-word co-occurance matrix from a large corpus. It is an ef-\nfective model for representing words in a vector space, however\nit doesn’t contextualize these representations with respect to the\nsequence they are actually used in1.\n3.1.2 ELMo. ELMo embeddings [23] are contextualized word rep-\nresentations in the sense that the surrounding words influence\nthe representation of the word. In the center of ELMo, there is\na bidirectional language model with multiple LSTM layers. The\ngoal of a language model is to learn the probability distribution\nover sequences of tokens in a given vocabulary. ELMo models the\nprobability of a token given the previous (and separately following)\ntokens in the sequence. Then the model also learns how to weight\ndifferent representations from different LSTM layers in order to\ncalculate one contextualized vector per token. Once the contextual-\nized representations are extracted, these can be used to initialize\nany down-stream NLP task2.\n3.1.3 ULMFit. ULMFit is a transfer learning model for down-stream\nNLP tasks, that make use of language model pre-training [5]. Un-\nlike ELMo, with ULMFit, the whole language model is fine-tuned\ntogether with the task-specific layers. The underlying language\nmodel used in ULMFit is AWD-LSTM, which uses sophisticated\ndropout tuning strategies to better regularize its LSTM model [21].\nFor classification using ULMFit two linear layers are added to the\npre-trained AWD-LSTM, first of which takes the pooled last hidden\nstates as input.\nULMFit comes with novel training strategies for further pre-\ntraining the language model on domain-specific corpus and fine-\ntuning on the down-stream task. We implement these strategies\nwith FinBERT as explained in section 3.2.\n1The pre-trained weights for GLoVE can be found here:\nhttps://nlp.stanford.edu/projects/glove/\n2The pre-trained ELMo models can be found here: https://allennlp.org/elmo\n3.1.4 Transformer. The Transformer is an attention-based archi-\ntecture for modeling sequential information, that is an alternative\nto recurrent neural networks [29]. It was proposed as a sequence-to-\nsequence model, therefore including encoder and decoder mecha-\nnisms. Here, we will focus only on the encoder part (though decoder\nis quite similar). The encoder consists of multiple identical Trans-\nformer layers. Each layer has a multi-headed self-attention layer\nand a fully connected feed-forward network. For one self-attention\nlayer, three mappings from embeddings (key, query and value) are\nlearned. Using each token’s key and all tokens’ query vectors, a\nsimilarity score is calculated with dot product. These scores are\nused to weight the value vectors to arrive at the new representation\nof the token. With the multi-headed self-attention, these layers are\nconcatenated together, so that the sequence can be evaluated from\nvarying \"perspectives\". Then the resulted vectors go through fully\nconnected networks with shared parameters.\nAs it was argued by Vaswani 2017 [29], Transformer architecture\nhas several advantages over the RNN-based approaches. Because\nof RNNs’ sequential nature, they are much harder to parallelize on\nGPUs and too many steps between far away elements in a sequence\nmake it hard for information to persist.\n3.1.5 BERT. BERT [3] is in essence a language model that consists\nof a set of Transformer encoders stacked on top of each other.\nHowever it defines the language modeling task differently from\nELMo and AWD-LSTM. Instead of predicting the next word given\nprevious ones, BERT \"masks\" a randomly selected 15% of all tokens.\nWith a softmax layer over vocabulary on top of the last encoder\nlayer the masked tokens are predicted. A second task BERT is\ntrained on is \"next sentence prediction\". Given two sentences, the\nmodel predicts whether or not these two actually follow each other.\nThe input sequence is represented with token and position em-\nbeddings. Two tokens denoted by [CLS] and [SEP] are added to the\nbeginning and end of the sequence respectively. For all classifica-\ntion tasks, including the next sentence prediction, [CLS] token is\nused.\nBERT has two versions: BERT-base, with 12 encoder layers, hid-\nden size of 768, 12 multi-head attention heads and 110M parameters\nin total and BERT-large, with 24 encoder layers, hidden size of\n1024, 16 multi-head attention heads and 340M parameters. Both of\nthese models have been trained on BookCorpus [33] and English\nWikipedia, which have in total more than 3,500M words 3.\n3.2 BERT for financial domain: FinBERT\nIn this subsection we will describe our implementation of BERT: 1)\nhow further pre-training on domain corpus is done, 2-3) how we\nimplemented BERT for classification and regression tasks, 4) train-\ning strategies we used during fine-tuning to prevent catastrophic\nforgetting.\n3.2.1 Further pre-training. Howard and Ruder (2018) [ 5] shows\nthat futher pre-training a language model on a target domain corpus\nimproves the eventual classification performance. For BERT, there\nis not decisive research showing that would be the case as well.\n3The pre-trained weights are made public by creators of BERT. The code and weights\ncan be found here: https://github.com/google-research/bert\n3\nRegardless, we implement further pre-training in order to observe\nif such adaptation is going to be beneficial for financial domain.\nFor further pre-training, we experiment with two approaches.\nThe first is pre-training the model on a relatively large corpus from\nthe target domain. For that, we further pre-train a BERT language\nmodel on a financial corpus (details of the corpus can be found on\nsection 4.2.1). The second approach is pre-training the model only\non the sentences from the training classification dataset. Although\nthe second corpus is much smaller, using data from the direct target\nmight provide better target domain adaptation.\n3.2.2 FinBERT for text classification. Sentiment classification is\nconducted by adding a dense layer after the last hidden state of the\n[CLS] token. This is the recommended practice for using BERT for\nany classification task [3]. Then, the classifier network is trained on\nthe labeled sentiment dataset. An overview of all the steps involved\nin the procedure is presented on figure 1.\n3.2.3 FinBERT for regression. While the focus of this paper is clas-\nsification, we also implement regression with almost the same\narchitecture on a different dataset with continuous targets. The\nonly difference is that the loss function being used is mean squared\nerror instead of the cross entropy loss.\n3.2.4 Training strategies to prevent catastrophic forgetting. As it\nwas pointed out by Howard and Ruder (2018) [ 5], catastrophic\nforgetting is a significant danger with this fine-tuning approach.\nBecause the fine-tuning procedure can quickly cause model to\n\"forget\" the information from language modeling task as it tries to\nadapt to the new task. In order to deal with this phenomenon, we\napply three techniques as it was proposed by Howard and Ruder\n(2018): slanted triangular learning rates, discriminative fine-tuning\nand gradual unfreezing.\nSlanted triangular learning rate applies a learning rate schedule\nin the shape of a slanted triangular, that is, learning rate first linearly\nincreases up to some point and after that point linearly decreases.\nDiscriminative fine-tuning is using lower learning rates for lower\nlayers on the network. Assume our learning rate at layerl is α. Then\nfor discrimination rate of θ we calculate the learning rate for layer\nl −1 as αl−1 = θαl . The assumption behind this method is that the\nlower layers represent the deep-level language information, while\nthe upper ones include information for actual classification task.\nTherefore we fine-tune them differently.\nWith gradual freezing, we start training with all layers but the\nclassifier layer as frozen. During training we gradually unfreeze all\nof the layers starting from the highest one, so that the lower level\nfeatures become the least fine-tuned ones. Hence, during the initial\nstages of training it is prevented for model to \"forget\" low-level\nlanguage information that it learned from pre-training.\n4 EXPERIMENTAL SETUP\n4.1 Research Questions\nWe aim to answer the following research questions:\n(RQ1) What is the performance of FinBERT in short sentence classi-\nfication compared with the other transfer learning methods\nlike ELMo and ULMFit?\nTable 1: Distribtution of sentiment labels and agreement lev-\nels in Financial PhraseBank\nAgreement level Positive Negative Neutral Count\n100% %25.2 %13.4 %61.4 2262\n75% - 99% %26.6 %9.8 %63.6 1191\n66% - 74% %36.7 %12.3 %50.9 765\n50% - 65% %31.1 %14.4 %54.5 627\nAll %28.1 %12.4 %59.4 4845\n(RQ2) How does FinBERT compare to the state-of-the-art in finan-\ncial sentiment analysis with targets discrete or continuous?\n(RQ3) How does futher pre-training BERT on financial domain, or\ntarget corpus, affect the classification performance?\n(RQ4) What are the effects of training strategies like slanted trian-\ngular learning rates, discriminative fine-tuning and gradual\nunfreezing on classification performance? Do they prevent\ncatastrophic forgetting?\n(RQ5) Which encoder layer performs best (or worse) for sentence\nclassification?\n(RQ6) How much fine-tuning is enough? That is, after pre-training,\nhow many layers should be fine-tuned to achieve comparable\nperformance to fine-tuning the whole model?\n4.2 Datasets\n4.2.1 TRC2-financial. In order to further pre-train BERT, we use\na financial corpus we call TRC2-financial. It is a subset of Reuters’\nTRC24, which consists of 1.8M news articles that were published\nby Reuters between 2008 and 2010. We filter for some financial\nkeywords in order to make corpus more relevant and in limits with\nthe compute power available. The resulting corpus, TRC2-financial,\nincludes 46,143 documents with more than 29M words and nearly\n400K sentences.\n4.2.2 Financial PhraseBank. The main sentiment analysis dataset\nused in this paper is Financial PhraseBank5 from Malo et al. 2014\n[17]. Financial Phrasebank consists of 4845 english sentences se-\nlected randomly from financial news found on LexisNexis database.\nThese sentences then were annotated by 16 people with background\nin finance and business. The annotators were asked to give labels\naccording to how they think the information in the sentence might\naffect the mentioned company stock price. The dataset also includes\ninformation regarding the agreement levels on sentences among\nannotators. The distribution of agreement levels and sentiment\nlabels can be seen on table 1. We set aside 20% of all sentences as\ntest and 20% of the remaining as validation set. In the end, our train\nset includes 3101 examples. For some of the experiments, we also\nmake use of 10-fold cross validation.\n4The corpus can be obtained for research purposes by applying here:\nhttps://trec.nist.gov/data/reuters/reuters.html\n5The dataset can be found here: https://www.researchgate.net/publication/251231364\n_FinancialPhraseBank-v10\n4\nReuters TRC2-\nﬁnancial\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]\nDense\nMasked LM prediction\nDense\n[is next sentence] prediction\nBookCorpus +\nWikipedia\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]Embeddings\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]\n[CLS] Token 1 Token 2\n [MASK]\n [SEP]\nEncoder 1\nEncoder 2\nEncoder 12\nDense\nMasked LM prediction\nDense\n[is next sentence] prediction\nLanguage model on general corpus\n[CLS] Token 1 Token 2\n Token k\n [SEP]\n[CLS] Token 1 Token 2\n Token k\n [SEP]\n[CLS] Token 1 Token 2\n Token k\n [SEP]\n[CLS] Token 1 Token 2\n Token k\n [SEP]\nDense\nSentiment  prediction\nClassiﬁcation model on ﬁnancial sentiment dataset\nFinancial\nPhrasebank\nLanguage model on ﬁnancial corpus\nFigure 1: Overview of pre-training, further pre-training and classification fine-tuning\n4.2.3 FiQA Sentiment. FiQA [15] is a dataset that was created for\nWWW ’18 conference financial opinion mining and question an-\nswering challenge6. We use the data for Task 1, which includes\n1,174 financial news headlines and tweets with their corresponding\nsentiment score. Unlike Financial Phrasebank, the targets for this\ndatasets are continuous ranging between [−1, 1]with 1 being the\nmost positive. Each example also has information regarding which\nfinancial entity is targeted in the sentence. We do 10-fold cross\nvalidation for evaluation of the model for this dataset.\n4.3 Baseline Methods\nFor contrastive experiments, we consider baselines with three dif-\nferent methods: LSTM classifier with GLoVe embeddings, LSTM\nclassifier with ELMo embeddings and ULMFit classifier. It should\nbe noted that these baseline methods are not experimented with as\nthoroughly as we did with BERT. Therefore the results should not\nbe interpreted as definitive conclusions of one method being better.\n4.3.1 LSTM classifiers. We implement two classifiers using bidirec-\ntional LSTM models. In both of them, a hidden size of 128 is used,\nwith the last hidden state size being 256 due to bidirectionality.\nA fully connected feed-forward layer maps the last hidden state\nto a vector of three, representing likelihood of three labels. The\ndifference between two models is that one uses GLoVe embeddings,\nwhile the other uses ELMo embeddings. A dropout probability of\n0.3 and a learning rate of 3e-5 is used in both models. We train them\nuntil there is no improvement in validation loss for 10 epochs.\n4.3.2 ULMFit. As it was explained in section 3.1.3, classification\nwith ULMFit consists of three steps. The first step of pre-training\na language model is already done and the pre-trained weights are\nreleased by Howard and Ruder (2018). We first further pre-train\nAWD-LSTM language model on TRC2-financial corpus for 3 epochs.\nAfter that, we fine-tune the model for classification on Financial\n6Data can be found here: https://sites.google.com/view/fiqa/home\nPhraseBank dataset, by adding a fully-connected layer to the output\nof pre-trained language model.\n4.4 Evaluation Metrics\nFor evaluation of classification models, we use three metrics: Ac-\ncuracy, cross entropy loss and macro F1 average. We weight cross\nentropy loss with square root of inverse frequency rate. For exam-\nple if a label constitutes 25% of the all examples, we weight the loss\nattributed to that label by 2. Macro F1 average calculates F1 scores\nfor each of the classes and then takes the average of them. Since our\ndata, Financial PhraseBank suffers from label imbalance (almost\n60% of all sentences are neutral), this gives another good measure of\nthe classification performance. For evaluation of regression model,\nwe report mean squared error and R2, as these are both standard\nand also reported by the state-of-the-art papers for FiQA dataset.\n4.5 Implementation Details\nFor our implementation BERT, we use a dropout probability of\np = 0.1, warm-up proportion of 0.2, maximum sequence length\nof 64 tokens, a learning rate of 2e −5 and a mini-batch size of\n64. We train the model for 6 epochs, evaluate on the validation\nset and choose the best one. For discriminative fine-tuning we set\nthe discrimination rate as 0.85. We start training with only the\nclassification layer unfrozen, after each third of a training epoch we\nunfreeze the next layer. An Amazon p2.xlarge EC2 instance with\none NVIDIA K80 GPU, 4 vCPUs and 64 GiB of host memory is used\nto train the models.\n5 EXPERIMENTAL RESULTS (RQ1 & RQ2)\nThe results of FinBERT, the baseline methods and state-of-the-art\non Financial PhraseBank dataset classification task can be seen on\ntable 2. We present the result on both the whole dataset and subset\nwith 100% annotator agreement.\n5\nTable 2: Experimental Results on the Financial PhraseBank dataset\nAll data Data with 100% agreement\nModel Loss Accuracy F1 Score Loss Accuracy F1 Score\nLSTM 0.81 0.71 0.64 0.57 0.81 0.74\nLSTM with ELMo 0.72 0.75 0.7 0.50 0.84 0.77\nULMFit 0.41 0.83 0.79 0.20 0.93 0.91\nLPS - 0.71 0.71 - 0.79 0.80\nHSC - 0.71 0.76 - 0.83 0.86\nFinSSLX - - - - 0.91 0.88\nFinBERT 0.37 0.86 0.84 0.13 0.97 0.95\nBold face indicates best result in the corresponding metric. LPS [17], HSC [8] and FinSSLX\n[15] results are taken from their respective papers. For LPS and HSC, overall accuracy is not\nreported on the papers. We calculated them using recall scores reported for different classes.\nFor the models implemented by us, we report 10-fold cross validation results.\nFor all of the measured metrics, FinBERT performs clearly the\nbest among both the methods we implemented ourselves (LSTM and\nULMFit) and the models reported by other papers (LPS [17], HSC [8],\nFinSSLX [14]). LSTM classifier with no language model information\nperforms the worst. In terms of accuracy, it is close to LPS and HSC,\n(even better than LPS for examples with full agreement), however\nit produces a low F1-score. That is due to it performing much better\nin neutral class. LSTM classifier with ELMo embeddings improves\nupon LSTM with static embeddings in all of the measured metrics.\nIt still suffers from low average F1-score due to poor performance\nin less represented labels. But it’s performance is comparable with\nLPS and HSC, besting them in accuracy. So contextualized word\nembeddings produce close performance to machine learning based\nmethods for dataset of this size.\nULMFit significantly improves on all of the metrics and it doesn’t\nsuffer from model performing much better in some classes than\nthe others. It also handily beats the machine learning based models\nLPS and HSC. This shows the effectiveness of language model pre-\ntraining. AWD-LSTM is a very large model and it would be expected\nto suffer from over-fitting with this small of a dataset. But due to\nlanguage model pre-training and effective training strategies, it\nis able to overcome small data problem. ULMFit also outperforms\nFinSSLX, which has a text simplification step as well as pre-training\nof word embeddings on a large financial corpus with sentiment\nlabels.\nFinBERT outperforms ULMFit, and consequently all of the other\nmethods in all metrics. In order to measure the performance of the\nmodels on different sizes of labeled training datasets, we ran LSTM\nclassifiers, ULMFit and FinBERT on 5 different configurations. The\nresult can be seen on figure 2, where the cross entropy losses on\ntest set for each model are drawn. 100 training examples is too low\nfor all of the models. However, once the training size becomes 250,\nULMFit and FinBERT starts to successfully differentiate between\nlabels, with an accuracy as high as 80% for FinBERT. All of the\nmethods consistently get better with more data, but ULMFit and\nFinBERT does better with 250 examples than LSTM classifiers do\nwith the whole dataset. This shows the effectiveness of language\nmodel pre-training.\nFigure 2: Test loss different training set sizes\nThe results for FiQA sentiment dataset, are presented on table 3.\nOur model outperforms state-of-the-art models for both MSE and\nR2. It should be noted that the test set these two papers [31] [24]\nuse is the official FiQA Task 1 test set. Since we don’t have access\nto that we report the results on 10-Fold cross validation. There is\nno indication on [15] that the train and test sets they publish come\nfrom different distributions and our model can be interpreted to\nbe at disadvantage since we need to set aside a subset of training\nset as test set, while state-of-the-art papers can use the complete\ntraining set.\n6 EXPERIMENTAL ANALYSIS\n6.1 Effects of further pre-training (RQ3)\nWe first measure the effect of further pre-training on the perfor-\nmance of the classifier. We compare three models: 1) No further\npre-training (denoted by Vanilla BERT), 2) Further pre-training\non classification training set (denoted by FinBERT-task), 3) Fur-\nther pre-training on domain corpus, TRC2-financial (denoted by\nFinBERT-domain). Models are evaluated with loss, accuracy and\n6\nTable 3: Experimental Results on FiQA Senti-\nment Dataset\nModel MSE R2\nYang et. al. (2018) 0.08 0.40\nPiao and Breslin (2018) 0.09 0.41\nFinBERT 0.07 0.55\nBold face indicated best result in corresponding metric.\nYang et. al. (2018) [ 31] and Piao and Breslin (2018) [ 24]\nreport results on the official test set. Since we don’t have\naccess to that set our MSE, and R2 are calculated with 10-\nFold cross validation.\nTable 4: Performance with different pre-\ntraining strategies\nModel Loss Accuracy F1 Score\nVanilla BERT 0.38 0.85 0.84\nFinBERT-task 0.39 0.86 0.85\nFinBERT-domain 0.37 0.86 0.84\nBold face indicates best result in the corresponding met-\nric. Results are reported on 10-fold cross validation.\nmacro average F1 scores on the test dataset. The results can be seen\non table 4.\nThe classifier that were further pre-trained on financial domain\ncorpus performs best among the three, though the difference is not\nvery high. There might be four reasons behind this result: 1) The\ncorpus might have a different distribution than the task set, 2) BERT\nclassifiers might not improve significantly with further pre-training,\n3) Short sentence classification might not benefit significantly from\nfurther pre-training, 4) Performance is already so good, that there is\nnot much room for improvement. We think that the last explanation\nis the likeliest, because for the subset of Financial Phrasebank that\nall of the annotators agree on the result, accuracy of Vanilla BERT\nis already 0.96. The performance on the other agreement levels\nshould be lower, as even the humans can’t agree fully on them. More\nexperiments with another financial labeled dataset is necessary to\nconclude that effect of further pre-training on domain corpus is not\nsignificant.\n6.2 Catastrophic forgetting (RQ4)\nFor measuring the performance of the techniques against cata-\nstrophic forgetting, we try four different settings: No adjustment\n(NA), only with slanted triangular learning rate (STL), slanted tri-\nangular learning rate and gradual unfreezing (STL+GU) and the\ntechniques in the previous one, together with discriminative fine-\ntuning. We report the performance of these four settings with loss\non test function and trajectory of validation loss over training\nepochs. The results can be seen on table 5 and figure 3.\nApplying all three of the strategies produce the best perfor-\nmance in terms of test loss and accuracy. Gradual unfreezing and\ndiscriminative fine-tuning have the same reasoning behind them:\nhigher level features should be fine-tuned more than the lower level\nFigure 3: Validation loss trajectories with different training\nstrategies\nTable 5: Performance with different fine-\ntuning strategies\nStrategy Loss Accuracy F1 Score\nNone 0.48 0.83 0.83\nSTL 0.40 0.81 0.82\nSTL + GU 0.40 0.86 0.86\nSTL + DFT 0.42 0.79 0.79\nAll three 0.37 0.86 0.84\nBold face indicates best result in the correspond-\ning metric. Results are reported on 10-fold cross\nvalidation. STL: slanted triangular learning rates,\nGU: gradual unfreezing, DFT: discriminative fine-\ntuning.\nones, since information learned from language modeling are mostly\npresent in the lower levels. We see from table 5 that using only\ndiscriminative fine-tuning with slanted triangular learning rates\nperforms worse than using the slanted triangular learning rates\nalone. This shows that gradual unfreezing is the most important\ntechnique for our case.\nOne way that catastrophic forgetting can show itself is the sud-\nden increase in validation loss after several epochs. As model is\ntrained, it quickly starts to overfit when no measure is taken accord-\ningly. As it can be seen on the figure 3, that is the case when none of\nthe aforementioned techniques are applied. The model achieves the\nbest performance on validation set after the first epoch and then\nstarts to overfit. While with all three techniques applied, model is\nmuch more stable. The other combinations lie between these two\ncases.\n6.3 Choosing the best layer for classification\n(RQ5)\nBERT has 12 Transformer encoder layers. It is not necessarily a\ngiven that the last layer captures the most relevant information\nregarding classification task during language model training. For\n7\nTable 6: Performance on different encoder layers used for\nclassification\nLayer for classification Loss Accuracy F1 Score\nLayer-1 0.65 0.76 0.77\nLayer-2 0.54 0.78 0.78\nLayer-3 0.52 0.76 0.77\nLayer-4 0.48 0.80 0.77\nLayer-5 0.52 0.80 0.80\nLayer-6 0.45 0.82 0.82\nLayer-7 0.43 0.82 0.83\nLayer-8 0.44 0.83 0.81\nLayer-9 0.41 0.84 0.82\nLayer-10 0.42 0.83 0.82\nLayer-11 0.38 0.84 0.83\nLayer-12 0.37 0.86 0.84\nAll layers - mean 0.41 0.84 0.84\nthis experiment, we investigate which layer out of 12 Transformer\nencoder layers give the best result for classification. We put the clas-\nsification layer after the CLS] tokens of respective representations.\nWe also try taking the average of all layers.\nAs shown in table 6the last layer contributes the most to the\nmodel performance in terms of all the metrics measured. This might\nbe indicative of two factors: 1) When the higher layers are used the\nmodel that is being trained is larger, hence possibly more powerful,\n2) The lower layers capture deeper semantic information, hence\nthey struggle to fine-tune that information for classification.\n6.4 Training only a subset of the layers (RQ6)\nBERT is a very large model. Even on small datasets, fine-tuning\nthe whole model requires significant time and computing power.\nTherefore if a slightly lower performance can be achieved with\nfine-tuning only a subset of all parameters, it might be preferable in\nsome contexts. Especially if training set is very large, this change\nmight make BERT more convenient to use. Here we experiment\nwith fine-tuning only the last k many encoder layers.\nThe results are presented on table 7. Fine-tuning only the clas-\nsification layer does not achieve close performance to fine-tuning\nother layers. However fine-tuning only the last layer handily out-\nperforms the state-of-the-art machine learning methods like HSC.\nAfter Layer-9, the performance becomes virtually the same, only to\nbe outperformed by fine-tuning the whole model. This result shows\nthat in order to utilize BERT, an expensive training of the whole\nmodel is not mandatory. A fair trade-off can be made for much less\ntraining time with a small decrease in model performance.\n6.5 Where does the model fail?\nWith 97% accuracy on the subset of Financial PhraseBank with\n100% annotator agreement, we think it might be an interesting\nexercise to examine cases where the model failed to predict the\ntrue label. Therefore in this section we will present several exam-\nples where model makes the wrong prediction. Also in Malo et\nTable 7: Performance on starting training from different lay-\ners\nFirst layer unfreezed Loss Accuracy Training time\nEmbeddings layer 0.37 0.86 332s\nLayer-1 0.39 0.83 302s\nLayer-2 0.39 0.83 291s\nLayer-3 0.38 0.83 272s\nLayer-4 0.38 0.82 250s\nLayer-5 0.40 0.83 240s\nLayer-6 0.40 0.81 220s\nLayer-7 0.39 0.82 205s\nLayer-8 0.39 0.84 188s\nLayer-9 0.39 0.84 172s\nLayer-10 0.41 0.84 158s\nLayer-11 0.45 0.82 144s\nLayer-12 0.47 0.81 133s\nClassification layer 1.04 0.52 119s\nal. (2014 )[17], it is indicated that most of the inter-annotator dis-\nagreements are between positive and neutral labels (agreement for\nseparating positive-negative, negative-neutral and positive-neutral\nare 98.7%, 94.2% and 75.2% respectively). Authors attribute that\nthe difficulty of distinguishing \"commonly used company glitter\nand actual positive statements\". We will present the confusion ma-\ntrix in order to observe whether this is the case for FinBERT as well.\nExample 1: Pre-tax loss totaled euro 0.3 million ,\ncompared to a loss of euro 2.2 million in the first\nquarter of 2005 .\nTrue value: Positive Predicted: Negative\nExample 2: This implementation is very important to\nthe operator , since it is about to launch its Fixed\nto Mobile convergence service in Brazil\nTrue value: Neutral Predicted: Positive\nExample 3: The situation of coated magazine printing\npaper will continue to be weak .\nTrue value: Negative Predicted: Neutral\nThe first example is actually the most common type of failure.\nThe model fails to do the math in which figure is higher, and in\nthe absence of words indicative of direction like \"increased\", might\nmake the prediction of neutral. However, there are many similar\ncases where it does make the true prediction too. Examples 2 and 3\nare different versions of the same type of failure. The model fails\nto distinguish a neutral statement about a given situation from a\nstatement that indicated polarity about the company. In the third\nexample, information about the company’s business would probably\nhelp.\nThe confusion matrix is presented on figure 4. 73% of the failures\nhappen between labels positive and negative, while same number\nis 5% for negative and positive. That is consistent with both the\ninter-annotator agreement numbers and common sense. It is easier\n8\nFigure 4: Confusion matrix\nto differentiate between positive and negative. But it might be more\nchallenging to decide whether a statement indicates a positive\noutlook or merely an objective observation.\n7 CONCLUSION AND FUTURE WORK\nIn this paper, we implemented BERT for the financial domain by\nfurther pre-training it on a financial corpus and fine-tuning it for\nsentiment analysis (FinBERT). This work is the first application of\nBERT for finance to the best of our knowledge and one of the few\nthat experimented with further pre-training on a domain-specific\ncorpus. On both of the datasets we used, we achieved state-of-the-\nart results by a significant margin. For the classification task, we\nincreased the state-of-the art by 15% in accuracy.\nIn addition to BERT, we also implemented other pre-training\nlanguage models like ELMo and ULMFit for comparison purposes.\nULMFit, further pre-trained on a financial corpus, beat the previous\nstate-of-the art for the classification task, only to a smaller degree\nthan BERT. These results show the effectiveness of pre-trained lan-\nguage models for a down-stream task such as sentiment analysis\nespecially with a small labeled dataset. The complete dataset in-\ncluded more than 3000 examples, but FinBERT was able to surpass\nthe previous state-of-the art even with a training set as small as\n500 examples. This is an important result, since deep learning tech-\nniques for NLP have been traditionally labeled as too \"data-hungry\",\nwhich is apparently no longer the case.\nWe conducted extensive experiments with BERT, investigating\nthe effects of further pre-training and several training strategies.\nWe couldn’t conclude that further pre-training on a domain-specific\ncorpus was significantly better than not doing so for our case. Our\ntheory is that BERT already performs good enough with our dataset\nthat there is not much room for improvement that further pre-\ntraining can provide. We also found that learning rate regimes that\nfine-tune the higher layers more aggressively than the lower ones\nperform better and are more effective in preventing catastrophic\nforgetting. Another conclusion from our experiments was that,\ncomparable performance can be achieved with much less training\ntime by fine-tuning only the last 2 layers of BERT.\nFinancial sentiment analysis is not a goal on its own, it is as\nuseful as it can support financial decisions. One way that our work\nmight be extended, could be using FinBERT directly with stock\nmarket return data (both in terms of directionality and volatility)\non financial news. FinBERT is good enough for extracting explicit\nsentiments, but modeling implicit information that is not neces-\nsarily apparent even to those who are writing the text should be a\nchallenging task. Another possible extension can be using FinBERT\nfor other natural language processing tasks such as named entity\nrecognition or question answering in financial domain.\n8 ACKNOWLEDGEMENTS\nI would like to show my gratitude to Pengjie Ren and Zulkuf Genc\nfor their excellent supervision. They provided me with both inde-\npendence in setting my own course for the research and valuable\nsuggestions when I need them. I would also like to thank Naspers AI\nteam, for entrusting me with this project and always encouraging\nme to share my work. I am grateful to NIST, for sharing Reuters\nTRC-2 corpus with me and to Malo et al. for making the excellent\nFinancial PhraseBank publicly available.\nREFERENCES\n[1] Basant Agarwal and Namita Mittal. 2016. Machine Learning Approach for\nSentiment Analysis. Springer International Publishing, Cham, 21–45. https:\n//doi.org/10.1007/978-3-319-25343-5_3\n[2] Oscar Araque, Ignacio Corcuera-Platas, J. Fernando Sánchez-Rada, and Carlos A.\nIglesias. 2017. Enhancing deep learning sentiment analysis with ensemble tech-\nniques in social applications. Expert Systems with Applications77 (jul 2017),\n236–246. https://doi.org/10.1016/j.eswa.2017.02.002\n[3] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding.\n(2018). https://doi.org/arXiv:1811.03600v2 arXiv:1810.04805\n[4] Li Guo, Feng Shi, and Jun Tu. 2016. Textual analysis and machine leaning: Crack\nunstructured data in finance and accounting. The Journal of Finance and Data\nScience 2, 3 (sep 2016), 153–170. https://doi.org/10.1016/J.JFDS.2017.02.001\n[5] Jeremy Howard and Sebastian Ruder. 2018. Universal Language Model Fine-\ntuning for Text Classification. (jan 2018). arXiv:1801.06146 http://arxiv.org/abs/\n1801.06146\n[6] Neel Kant, Raul Puri, Nikolai Yakovenko, and Bryan Catanzaro. 2018. Prac-\ntical Text Classification With Large Pre-Trained Language Models. (2018).\narXiv:1812.01207 http://arxiv.org/abs/1812.01207\n[7] Mathias Kraus and Stefan Feuerriegel. 2017. Decision support from financial\ndisclosures with deep neural networks and transfer learning.Decision Support Sys-\ntems 104 (2017), 38–48. https://doi.org/10.1016/j.dss.2017.10.001 arXiv:1710.03954\n[8] Srikumar Krishnamoorthy. 2018. Sentiment analysis of financial news articles\nusing performance indicators. Knowledge and Information Systems56, 2 (aug\n2018), 373–394. https://doi.org/10.1007/s10115-017-1134-1\n[9] Xiaodong Li, Haoran Xie, Li Chen, Jianping Wang, and Xiaotie Deng. 2014. News\nimpact on stock price return via sentiment analysis. Knowledge-Based Systems\n69 (oct 2014), 14–23. https://doi.org/10.1016/j.knosys.2014.04.022\n[10] Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Synthesis Lectures on\nHuman Language Technologies5, 1 (may 2012), 1–167. https://doi.org/10.2200/\ns00416ed1v01y201204hlt016\n[11] Tim Loughran and Bill Mcdonald. 2011. When Is a Liability Not a Liability?\nTextual Analysis, Dictionaries, and 10-Ks. Journal of Finance66, 1 (feb 2011),\n35–65. https://doi.org/10.1111/j.1540-6261.2010.01625.x\n[12] Tim Loughran and Bill Mcdonald. 2016. Textual Analysis in Accounting and\nFinance: A Survey. Journal of Accounting Research54, 4 (2016), 1187–1230.\nhttps://doi.org/10.1111/1475-679X.12123\n[13] Bernhard Lutz, Nicolas Pröllochs, and Dirk Neumann. 2018. Sentence-Level\nSentiment Analysis of Financial News Using Distributed Text Representations and\nMulti-Instance Learning. Technical Report. arXiv:1901.00400 http://arxiv.org/\nabs/1901.00400\n[14] Macedo Maia, Andrï£¡ Freitas, and Siegfried Handschuh. 2018. FinSSLx: A Senti-\nment Analysis Model for the Financial Domain Using Text Simplification. In2018\nIEEE 12th International Conference on Semantic Computing (ICSC). IEEE, 318–319.\nhttps://doi.org/10.1109/ICSC.2018.00065\n[15] Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross Mcdermott,\nManel Zarrouk, Alexandra Balahur, and Ross Mc-Dermott. 2018. Companion of\nthe The Web Conference 2018 on The Web Conference 2018, {WWW} 2018, Lyon\n, France, April 23-27, 2018. ACM. https://doi.org/10.1145/3184558\n[16] Burton G Malkiel. 2003. The Efficient Market Hypothesis and Its Critics. Jour-\nnal of Economic Perspectives17, 1 (feb 2003), 59–82. https://doi.org/10.1257/\n9\n089533003321164958\n[17] Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala.\n2014. Good debt or bad debt: Detecting semantic orientations in economic texts.\nJournal of the Association for Information Science and Technology65, 4 (2014),\n782–796. https://doi.org/10.1002/asi.23062 arXiv:arXiv:1307.5336v2\n[18] G. Marcus. 2018. Deep Learning: A Critical Appraisal. arXiv e-prints(Jan. 2018).\narXiv:cs.AI/1801.00631\n[19] Justin Martineau and Tim Finin. 2009. Delta TFIDF: An Improved Feature Space\nfor Sentiment Analysis.. In ICWSM, Eytan Adar, Matthew Hurst, Tim Finin,\nNatalie S. Glance, Nicolas Nicolov, and Belle L. Tseng (Eds.). The AAAI Press.\nhttp://dblp.uni-trier.de/db/conf/icwsm/icwsm2009.html#MartineauF09\n[20] Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017.\nLearned in Translation: Contextualized Word Vectors. Nips (2017), 1–12.\narXiv:1708.00107 http://arxiv.org/abs/1708.00107\n[21] Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regular-\nizing and Optimizing LSTM Language Models. CoRR abs/1708.02182 (2017).\narXiv:1708.02182 http://arxiv.org/abs/1708.02182\n[22] Jeffrey Pennington, Richard Socher, and Christopher Manning. 2014. Glove:\nGlobal Vectors for Word Representation. InProceedings of the 2014 Conference\non Empirical Methods in Natural Language Processing (EMNLP). Association for\nComputational Linguistics, Doha, Qatar, 1532–1543. https://doi.org/10.3115/v1/\nD14-1162\n[23] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word\nrepresentations. (2018). https://doi.org/10.18653/v1/N18-1202 arXiv:1802.05365\n[24] Guangyuan Piao and John G Breslin. 2018. Financial Aspect and Sentiment\nPredictions with Deep Neural Networks. 1973–1977. https://doi.org/10.1145/\n3184558.3191829\n[25] Aliaksei Severyn and Alessandro Moschitti. 2015. Twitter Sentiment Analysis\nwith Deep Convolutional Neural Networks. InProceedings of the 38th International\nACM SIGIR Conference on Research and Development in Information Retrieval -\nSIGIR '15. ACM Press. https://doi.org/10.1145/2766462.2767830\n[26] Sahar Sohangir, Dingding Wang, Anna Pomeranets, and Taghi M Khoshgoftaar.\n2018. Big Data: Deep Learning for financial sentiment analysis. Journal of Big\nData 5, 1 (2018). https://doi.org/10.1186/s40537-017-0111-6\n[27] Chi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang. 2019. How to Fine-Tune\nBERT for Text Classification? (2019). arXiv:1905.05583 https://arxiv.org/pdf/\n1905.05583v1.pdfhttp://arxiv.org/abs/1905.05583\n[28] Abinash Tripathy, Ankit Agrawal, and Santanu Kumar Rath. 2016. Classification\nof sentiment reviews using n-gram machine learning approach. Expert Systems\nwith Applications57 (sep 2016), 117–126. https://doi.org/10.1016/j.eswa.2016.03.\n028\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. Nips (2017). arXiv:1706.03762 http://arxiv.org/abs/1706.03762\n[30] Casey Whitelaw, Navendu Garg, and Shlomo Argamon. 2005. Using appraisal\ngroups for sentiment analysis. In Proceedings of the 14th ACM international\nconference on Information and knowledge management - CIKM'05. ACM Press.\nhttps://doi.org/10.1145/1099554.1099714\n[31] Steve Yang, Jason Rosenfeld, and Jacques Makutonin. 2018. Financial Aspect-\nBased Sentiment Analysis using Deep Representations. (2018). arXiv:1808.07931\nhttps://arxiv.org/pdf/1808.07931v1.pdfhttp://arxiv.org/abs/1808.07931\n[32] Lei Zhang, Shuai Wang, and Bing Liu. 2018. Deep learning for sentiment analysis:\nA survey. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery\n8, 4 (mar 2018), e1253. https://doi.org/10.1002/widm.1253\n[33] Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun,\nAntonio Torralba, and Sanja Fidler. 2015. Aligning Books and Movies: Towards\nStory-like Visual Explanations by Watching Movies and Reading Books. (jun\n2015). arXiv:1506.06724 http://arxiv.org/abs/1506.06724\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7916754484176636
    },
    {
      "name": "Language model",
      "score": 0.6297796964645386
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6151232719421387
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6126466989517212
    },
    {
      "name": "Metric (unit)",
      "score": 0.6115779280662537
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5780588388442993
    },
    {
      "name": "Task (project management)",
      "score": 0.5661824941635132
    },
    {
      "name": "Natural language processing",
      "score": 0.5584650039672852
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5481656789779663
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5471800565719604
    },
    {
      "name": "Training set",
      "score": 0.5170746445655823
    },
    {
      "name": "Machine learning",
      "score": 0.47935372591018677
    },
    {
      "name": "State (computer science)",
      "score": 0.4324477016925812
    },
    {
      "name": "Finance",
      "score": 0.42741912603378296
    },
    {
      "name": "Algorithm",
      "score": 0.06073686480522156
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I887064364",
      "name": "University of Amsterdam",
      "country": "NL"
    }
  ]
}