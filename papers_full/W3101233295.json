{
    "title": "Text Graph Transformer for Document Classification",
    "url": "https://openalex.org/W3101233295",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2122442786",
            "name": "Haopeng Zhang",
            "affiliations": [
                "Florida State University"
            ]
        },
        {
            "id": "https://openalex.org/A2103988011",
            "name": "Jiawei Zhang",
            "affiliations": [
                "Florida State University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2897007327",
        "https://openalex.org/W2042954874",
        "https://openalex.org/W2964321699",
        "https://openalex.org/W2562607067",
        "https://openalex.org/W2963012544",
        "https://openalex.org/W2972693922",
        "https://openalex.org/W2788667846",
        "https://openalex.org/W2154359981",
        "https://openalex.org/W2407776548",
        "https://openalex.org/W2170240176",
        "https://openalex.org/W1832693441",
        "https://openalex.org/W3105625590",
        "https://openalex.org/W3000577518",
        "https://openalex.org/W2964015378",
        "https://openalex.org/W3035568641",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2998436975",
        "https://openalex.org/W2997162759",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4297733535",
        "https://openalex.org/W2964145825",
        "https://openalex.org/W2962767366",
        "https://openalex.org/W4294558607",
        "https://openalex.org/W2970183009",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963355447",
        "https://openalex.org/W2470673105",
        "https://openalex.org/W2962946486",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3012871709",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2963858333",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1980867644"
    ],
    "abstract": "Text classification is a fundamental problem in natural language processing. Recent studies applied graph neural network (GNN) techniques to capture global word co-occurrence in a corpus. However, previous works are not scalable to large-sized corpus and ignore the heterogeneity of the text graph. To address these problems, we introduce a novel Transformer based heterogeneous graph neural network, namely Text Graph Transformer (TG-Transformer). Our model learns effective node representations by capturing structure and heterogeneity from the text graph. We propose a mini-batch text graph sampling method that significantly reduces computing and memory costs to handle large-sized corpus. Extensive experiments have been conducted on several benchmark datasets, and the results demonstrate that TG-Transformer outperforms state-of-the-art approaches on text classification task.",
    "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8322–8327,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n8322\nText Graph Transformer for Document Classiﬁcation\nHaopeng Zhang Jiawei Zhang\nIFM Lab, Department of Computer Science,\nFlorida State University, FL, USA\nhaopeng,jiawei@ifmlab.org\nAbstract\nText classiﬁcation is a fundamental problem\nin natural language processing. Recent stud-\nies applied graph neural network (GNN) tech-\nniques to capture global word co-occurrence\nin a corpus. However, previous works are\nnot scalable to large-sized corpus and ignore\nthe heterogeneity of the text graph. To ad-\ndress these problems, we introduce a novel\nTransformer based heterogeneous graph neu-\nral network, namely Text Graph Transformer\n(TG-Transformer). Our model learns effec-\ntive node representations by capturing struc-\nture and heterogeneity from the text graph.\nWe propose a mini-batch text graph sampling\nmethod that signiﬁcantly reduces computing\nand memory costs to handle large-sized corpus.\nExtensive experiments have been conducted\non several benchmark datasets, and the results\ndemonstrate that TG-Transformer outperforms\nstate-of-the-art approaches on text classiﬁca-\ntion task.\n1 Introduction\nText classiﬁcation is a widely studied problem\nin natural language processing and has been ad-\ndressed in many real-world applications such as\nnews ﬁltering, spam detection, and health record\nsystems (Kowsari et al., 2019; Che et al., 2015;\nZhang et al., 2018). The objective is to assign\ncorresponding labels to textual units based on text\nrepresentations.\nDeep learning models like Convolutional Neural\nNetworks (CNN) (Kim, 2014) and Recurrent Neu-\nral Networks (RNN) (Hochreiter and Schmidhu-\nber, 1997) have been applied for text representation\nlearning instead of traditional hand-crafted features,\nsuch as n-gram and bag-of-words (BoW) (Joulin\net al., 2016). Researchers have recently turned to\nGraph Neural Network (GNN) to exploit global fea-\ntures in text representation learning, which learns\nnode embedding by aggregating information from\nneighbors through edges. Defferrard et al. (2016)\nﬁrst generalized CNN to graph for text classiﬁca-\ntion task. Then Yao et al. (2019) applied Graph\nConvolution Network (GCN) (Kipf and Welling,\n2016) on a corpus level heterogeneous text graph\nand achieved state-of-the-art performance. Liu et al.\n(2020) further improved classiﬁcation accuracy by\nexpanding the text graph with semantic and syntac-\ntic contextual information.\nHowever, these GCN-based models on heteroge-\nneous text graphs suffer from two practical issues.\nFirstly, none of these models are scalable to large-\nsized corpus due to high computation and memory\ncosts. Calculation of all the nodes in the graph is\nrequired at each layer during training. Secondly, all\nthese models ignore the heterogeneity of the text\ngraph, which consists of both document and word\nnodes. Distinguishing nodes of different types will\nbeneﬁt node representation learning.\nTo address the above problems, we pro-\npose a novel Transformer-based heterogeneous\nGNN model, namely Text Graph Transformer\n(TG-Transformer). Instead of learning based on the\nfull text graph, we propose a text graph sampling\nmethod that enables subgraph mini-batch training.\nThe signiﬁcantly reduced computing and memory\ncosts make the model scalable to large-sized cor-\npus. Moreover, we utilizeTransformer to aggregate\ninformation in subgraph batch with two proposed\ngraph structural encodings. We also distinguish\nthe learning process of different type nodes to fully\nutilize the heterogeneity of text graph. The main\ncontributions of this work are as follows:\n1. We propose Text Graph Transformer, a het-\nerogeneous graph neural network for text clas-\nsiﬁcation. It is the ﬁrst scalable graph-based\nmethod for the task to the best of our knowl-\nedge.\n8323\nFigure 1: Overall Structure of TG-Transformer\n2. We propose a novel heterogeneous text graph\nsampling method that signiﬁcantly reduces\ncomputing and memory costs.\n3. We perform experiments on several bench-\nmark datasets, and the results demonstrate the\neffectiveness and efﬁciency of our model.\n2 Methodology\nIn this section, we introduce TG-Transformer in\ngreat detail. First, we present how to construct a\nheterogeneous text graph for a given corpus. Then,\nwe introduce our text graph sampling method,\nwhich can generate subgraph mini-batch from the\ntext graph. These subgraph batches will be fed into\nTG-Transformer to learn efﬁcient node representa-\ntions for classiﬁcation. The overall structure of our\nmodel is shown in Fig. 1.\n2.1 Text Graph Building\nTo capture global word co-occurrence within cor-\npus, we build a heterogeneous text graph G =\n(U,V,E,F). The text graph contains two types\nof nodes: word nodes ( U) representing all docu-\nments in the corpus and document nodes (V) rep-\nresenting all the words in the corpus vocabulary.\nThe text graph also contains two types of edges:\nword-document edges ( E) and word-word edges\n(F). Word-document edges are built based on word\noccurrence within documents with edge weights\nmeasured by the term frequency-inverse document\nfrequency (TF-IDF) method. Word-word edges\nare built based on local word co-occurrence within\nsliding windows in the corpus, with edge weights\nmeasured by point-wise mutual information (PMI):\nPMI(wi,wj) = logpi,j\npipj\n= logNi,jN\nNiNj\n, (1)\nwhere Ni,Nj,Ni,j are the number of sliding win-\ndows in a corpus that contain word wi, word wj\nand both wi,wj. N is the total number of sliding\nwindows in the corpus.\n2.2 Text Graph Sampling\nTo reduce computing and memory cost, we propose\na text graph sampling method. Instead of learning\nbased on the entire text graph, TG-Transformer\nis trained on sampled subgraph mini-batch, mak-\ning it scalable to large-sized corpus. We separate\nsub-graph sampling as a pre-process step in an un-\nsupervised manner for controlling the time costs in\nmodel learning.\nWe ﬁrst calculate the intimacy matrix S of the\ntext graph based on pagerank algorithm:\nS = α·(I −(1 −α) ·A)−1, (2)\nwhere factor α ∈ [0,1] is usually set as 0.15.\nA = D−1\n2 AD−1\n2 is the normalized symmetric\nadjacency matrix, A is the adjacency matrix of the\ntext graph, and D is its corresponding diagonal ma-\ntrix. Each entry Si,j measures the intimacy score\nbetween node iand node j.\nFor any document target nodevi ∈V, we sample\nits context subgraph C(vi) of size kby selecting its\ntop kintimate neighbour word nodes uj ∈U.\nMeanwhile, for any word target node ui ∈U ,\nwe ﬁrst calculate the ratios of two type incident\n8324\nTable 1: Statistics of the experiment datasets. V denotes the vocabulary size, C the number of classes, and W the\naverage number of words per document.\nDataset Train Test V C W\nR8 5,485 2,189 7,688 8 65.72\nR52 6,532 2,568 8,892 52 69.82\nOhsumed 3,357 4,043 14,157 23 135.82\nIMDB 278,732 69,683 115,831 10 325.6\nYelp 2014 900,309 225,077 476,191 5 148.8\nedge:\nrw(ui) = |F(ui)|\n|F(ui)|+ |E(ui)|, (3)\nrd(ui) = |E(ui)|\n|F(ui)|+ |E(ui)|, (4)\nwhere F(ui),E(ui) are the sets of word-word\nedges, word-document edges incident to ui with\nintimacy score larger than threshold θ. We sample\nits context subgraph C(ui) of size k by selecting\nits top k·rw(ui) intimate neighbour word nodes\nand its top k·rd(ui) intimate neighbour document\nnodes, respectively.\n2.3 Text Graph Transformer\nBased on the sampled subgraph mini-batch,\nTG-Transformer will update the text graph nodes’\nrepresentations iteratively for classiﬁcation. We\nbuild one model for each target node type (docu-\nment/word) to model heterogeneity. The input of\nour model will be raw feature embeddings of nodes\nin subgraph batch injected by the following two\nextra structural encodings:\nHeterogeneity Encoding The heterogeneity en-\ncoding can capture the document and word types\nin the text graph. Similar to the segment encoding\nin (Devlin et al., 2018), we use 0 and 1 to encode\ndocument nodes and word nodes, respectively.\nWeisfeiler-Lehman Structural Encoding We\nadopt the WL Role Embedding by (Zhang et al.,\n2020a) to capture the structure of text graph. The\nWeisfeiler-Lehman (WL) algorithm (Niepert et al.,\n2016) can label nodes according to their structural\nroles in the graph. For node vj (document or word\nnode) in the sampled subgraph, we can denote its\nWL code as WL(vj) ∈N, and the encoding is\ndeﬁned as:\n[\nsin\n(\nWL (vj)\n10000\n2l\ndh\n)\n,cos\n(\nWL (vj)\n10000\n2l+1\ndh\n)]⌊ dh\n2\n⌋\nl=0\n.\n(5)\nThese two encodings have the same dimension\n(i.e., dh) as the original raw feature embeddings, so\nwe add them together as the initial node representa-\ntion for the input subgraph, which can be denoted\nas H(0).\nGraph Transformer Layer The D layer graph\ntransformer will aggregate information from sub-\ngraph batch to learn the target node representa-\ntion. Each graph transformer layer contains three\ntrainable matrices: WQ,WK,Wv ∈Rdh×dh and\nqueries Q, keys K and values V are generated by\nmultiplying the input correspondingly:\n{Q,K,V}= H(l−1)\n{\nW(l)\nQ ,W(l)\nK ,W(l)\nV\n}\n. (6)\nThen a TG-Transformer layer can be donated as:\nH(l) = G-Transformer\n(\nH(l−1)\n)\n= softmax\n(QK⊤\n√dh\n)\nV + G-Res,\n(7)\nwhere G-res refers to the graph residual\nterm in (Zhang and Meng, 2019) to solve the\nover-smoothing issue of GNNs. The output of\nthe last layer H(D) will be averaged as the ﬁnal\nrepresentations z of the target node and fed into\na softmax classiﬁer:\nz = softmax(average(H(D)) ∈Rdy×1. (8)\nBased on the sampled subgraphs for all the nodes\nin the training set, e.g., T, we can deﬁne the cross-\nentropy based loss function as:\nℓ= −\n∑\nn∈T\ndy∑\nf=1\nyn(f) logzn(f), (9)\n8325\nTable 2: Text classiﬁcation accuracy results. Models with ”*” utilize pre-trained Glove word embeddings. For the\nscores not reported in the existing works, we mark them with ‘-’ in the table.\nModel R8 R52 Ohsumed IMDB Yelp 2014\nCNN* 95.7 ±0.5 87.6 ±0.4 58.4 ±1.0 42.7 ±0.4 66.1 ±0.6\nLSTM* 96.1 ±0.2 90.5 ±0.8 51.1 ±1.5 52.1 ±0.3 68.4 ±0.1\nfastText* 96.1 ±0.2 92.8 ±0.1 57.7 ±0.5 45.2 ±0.4 66.2 ±0.6\nText GCN 97.0 ±0.1 93.7 ±0.1 67.7 ±0.3 - -\nText GNN* 97.8 ±0.2 94.6 ±0.3 69.4 ±0.6 - -\nTensor GCN* 98.0 ±0.1 95.0 ±0.1 70.1 ±0.2 - -\nTG-Transformer* 98.1 ±0.1 95.2 ±0.2 70.4 ±0.4 53.4 ±1.2 69.8 ±0.6\nwhere n ∈T denotes the target word/document\nnodes in the training set, dy is the label vector di-\nmension, and yn represents the ground-truth label\nvector of node n.\n3 Experiment\n3.1 Experimental Setup\nDatasets We evaluate the effectiveness of our\nmodel on ﬁve benchmarked datasets: R52 and R8\nReuters dataset1 for news documents classiﬁcation,\nOhsumed dataset2 for medical bibliographic classi-\nﬁcation, and two large-scale review rating datasets:\nIMDB and Yelp 2014. Detailed statistics of the\ndatasets are summarized in Table 1.\nBaselines We compare our method with three\nclassical baseline models: CNN in (Kim, 2014),\nLSTM in (Liu et al., 2016) and fastText in (Joulin\net al., 2016) using the average of word/n-grams\nembeddings. In addition, we compare with three\nstate-of-the-art GNN-based models: TextGCN in\n(Yao et al., 2019) using GCN, Text GNN 3 in\n(Huang et al., 2019) using text level graphs and\nTensorGCN in (Liu et al., 2020) using semantic\nand syntactic contextual information.\nImplementation We set the node representation\ndimension as 300 and initialize with Glove word\nembeddings (Pennington et al., 2014). We train\na 2-layer graph transformer with a hidden size 32\nand 4 attention heads. We use mini-batch SGD\nwith Adam optimizer (Kingma and Ba, 2014), and\nthe dropout rate is set as 0.5. The initial learning\nrate as 0.001, and we decay it with weight decay\n5e−4. 10 percent of the training set is randomly\nselected as validation set, and we stop training if\n1https://www.cs.umb.edu/ smimarog/textmining/datasets/\n2http://disi.unitn.it/moschitti/corpora.htm\n3We give this name for simplicity.\nTable 3: Training time per epoch of GNN-based mod-\nels.\nModel R52 Ohsumed\nText GCN 2.64 3.48\nTensor GCN 4.32 5.13\nTG-Transformer 0.83 1.17\nthe validation set loss does not decrease for 10\nconsecutive epochs.\n3.2 Experiment Results\nTable 2 presents the classiﬁcation accuracy of our\nmodel compared with baseline methods. GNN-\nbased models generally perform better than se-\nquential and bag-of-word models due to its ability\nto model global word co-occurrence in the cor-\npus, and TG-Transformer outperforms other graph\nmodels with much less memory and computing\ncost. This is likely due to the utilization of the text\ngraph’s heterogeneity and effective representing\nlearning by Graph Transformer Layers. Moreover,\nTG-Transformer performs well on large-sized cor-\npus such as IMDB and Yelp 14. We also evaluate\nmodel efﬁciency with training time per epoch, as\nshown in Table 3. It can be observed that our text\ngraph sampling method reduces the computing cost\nsigniﬁcantly and makes our model scalable to large\ncorpus, where previous GNN-based models such\nas Text GCN are not applicable due to computing\npower limit.\nHyperparameter Here we analyze the effects of\nsubgraph sizes kin sampling. We notice parameter\nkhas a large inﬂuence on the model performance\nsince it deﬁnes the number of neighbor nodes used\nto update the target node representation. During pa-\nrameter tuning, we notice the learning performance\nimproves steadily as kincreases from 1 to an opti-\nmal value (i.e.,23 for R8) and starts decreasing as\n8326\nTable 4: Ablation study results.\nSettings R52 Ohsumed\nOriginal 95.2 70.4\n(1) Without structural encodings 94.8 69.6\n(2) Without pre-trained Emb. 93.5 66.9\n(3) Simultaneous updating 94.7 69.3\nk further increases. The same trend is noticed for\nall datasets. The computing cost to train the model\nalso increases as k goes larger, but is still less than\nother GNN-based models.\nAblation Study We perform ablation studies to\nanalyze our model further, as shown in Table 4. In\n(1), we remove the two structural encodings and\nonly use raw feature embeddings as input. The\ndecreased performance demonstrates that the struc-\ntural encodings capture some useful heterogeneous\ngraph structure information. In (2), we remove\npre-trained word embeddings and initialize all the\nnodes with random vectors. Model performance\nhas a larger decrease, demonstrating the signiﬁ-\ncance of pre-trained word embeddings and initial\nnode representations on our model. In (3), we train\none model to update and learn both subgraph batch\ntarget node types. The slightly decreasing classiﬁ-\ncation accuracy reﬂects the importance of modeling\nheterogeneity information of the text graph.\n4 Related Work\n4.1 Text Classiﬁcation\nTraditional text classiﬁcation studies rely on hand-\ncrafted features like BoW (Zhang et al., 2010) and\nn-gram (Wang and Manning, 2012). With the de-\nvelopment of deep learning, researchers applied\nCNN (Kim, 2014; Zhang et al., 2015), LSTM (Tai\net al., 2015; Liu et al., 2016), word embedding tech-\nniques (Joulin et al., 2016; Pennington et al., 2014),\nattention mechanism (Yang et al., 2016; Wang et al.,\n2016) in text classiﬁcation models and kept improv-\ning accuracy. Recently, graph based text classiﬁca-\ntion models received growing attention due to its\nability to model global information in corpus (Yao\net al., 2019; Peng et al., 2018; Zhang et al., 2020b;\nNikolentzos et al., 2019). Our paper follows this\nline of works on developing novel GNN for text\nclassiﬁcation.\n4.2 Graph Neural Network\nRepresentative examples of GNN models pro-\nposed by present include GCN (Kipf and Welling,\n2016), Graph Attention Network (GAT)(Veliˇckovi´c\net al., 2017) and Graph SAGE (Hamilton et al.,\n2017). GCN models are based on approximated\ngraph convolutional operator while GAT relies on\nself-attention mechanism. Recently, Transformer\n(Vaswani et al., 2017) models have been applied in\nnovel GNN designs (Hu et al., 2020; Zhang et al.,\n2020a).\n5 Conclusion\nIn this paper, we proposed a scalable heterogeneous\ngraph model, TG-Transformer, for text classiﬁca-\ntion. Experimental results prove its effectiveness\nand efﬁciency compared to state-of-the-art meth-\nods. It also enables parallelization and pre-training\nin GNN models for further research.\nAcknowledgement\nThis work is partially supported by NSF through\ngrant IIS-1763365 and by FSU.\nReferences\nZhengping Che, David Kale, Wenzhe Li, Moham-\nmad Taha Bahadori, and Yan Liu. 2015. Deep com-\nputational phenotyping. In Proceedings of the 21th\nACM SIGKDD International Conference on Knowl-\nedge Discovery and Data Mining, pages 507–516.\nMicha¨el Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. 2016. Convolutional neural networks\non graphs with fast localized spectral ﬁltering. In\nAdvances in neural information processing systems ,\npages 3844–3852.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017.\nInductive representation learning on large graphs. In\nAdvances in neural information processing systems ,\npages 1024–1034.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nZiniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou\nSun. 2020. Heterogeneous graph transformer. In\nProceedings of The Web Conference 2020 , pages\n2704–2710.\nLianzhe Huang, Dehong Ma, Sujian Li, Xiaodong\nZhang, and Houfeng W ANG. 2019. Text level graph\nneural network for text classiﬁcation. arXiv preprint\narXiv:1910.02356.\n8327\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2016. Bag of tricks for efﬁcient text\nclassiﬁcation. arXiv preprint arXiv:1607.01759.\nYoon Kim. 2014. Convolutional neural net-\nworks for sentence classiﬁcation. arXiv preprint\narXiv:1408.5882.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nThomas N Kipf and Max Welling. 2016. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. arXiv preprint arXiv:1609.02907.\nKamran Kowsari, Kiana Jafari Meimandi, Mojtaba Hei-\ndarysafa, Sanjana Mendu, Laura Barnes, and Donald\nBrown. 2019. Text classiﬁcation algorithms: A sur-\nvey. Information, 10(4):150.\nPengfei Liu, Xipeng Qiu, and Xuanjing Huang.\n2016. Recurrent neural network for text classi-\nﬁcation with multi-task learning. arXiv preprint\narXiv:1605.05101.\nXien Liu, Xinxin You, Xiao Zhang, Ji Wu, and Ping Lv.\n2020. Tensor graph convolutional networks for text\nclassiﬁcation. arXiv preprint arXiv:2001.05313.\nMathias Niepert, Mohamed Ahmed, and Konstantin\nKutzkov. 2016. Learning convolutional neural net-\nworks for graphs. In International conference on\nmachine learning, pages 2014–2023.\nGiannis Nikolentzos, Antoine J-P Tixier, and Michalis\nVazirgiannis. 2019. Message passing attention net-\nworks for document understanding. arXiv preprint\narXiv:1908.06267.\nHao Peng, Jianxin Li, Yu He, Yaopeng Liu, Mengjiao\nBao, Lihong Wang, Yangqiu Song, and Qiang Yang.\n2018. Large-scale hierarchical text classiﬁcation\nwith recursively regularized deep graph-cnn. In Pro-\nceedings of the 2018 World Wide Web Conference ,\npages 1063–1072.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of the 2014 conference\non empirical methods in natural language process-\ning (EMNLP), pages 1532–1543.\nKai Sheng Tai, Richard Socher, and Christopher D\nManning. 2015. Improved semantic representations\nfrom tree-structured long short-term memory net-\nworks. arXiv preprint arXiv:1503.00075.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio.\n2017. Graph attention networks. arXiv preprint\narXiv:1710.10903.\nSida Wang and Christopher D Manning. 2012. Base-\nlines and bigrams: Simple, good sentiment and topic\nclassiﬁcation. In Proceedings of the 50th annual\nmeeting of the association for computational linguis-\ntics: Short papers-volume 2 , pages 90–94. Associa-\ntion for Computational Linguistics.\nYequan Wang, Minlie Huang, Xiaoyan Zhu, and\nLi Zhao. 2016. Attention-based lstm for aspect-\nlevel sentiment classiﬁcation. In Proceedings of the\n2016 conference on empirical methods in natural\nlanguage processing, pages 606–615.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\ncal attention networks for document classiﬁcation.\nIn Proceedings of the 2016 conference of the North\nAmerican chapter of the association for computa-\ntional linguistics: human language technologies ,\npages 1480–1489.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nGraph convolutional networks for text classiﬁcation.\nIn Proceedings of the AAAI Conference on Artiﬁcial\nIntelligence, volume 33, pages 7370–7377.\nJiawei Zhang and Lin Meng. 2019. Gresnet: Graph\nresidual network for reviving deep gnns from sus-\npended animation. ArXiv, abs/1909.05729.\nJiawei Zhang, Haopeng Zhang, Li Sun, and Congying\nXia. 2020a. Graph-bert: Only attention is needed\nfor learning graph representations. arXiv preprint\narXiv:2001.05140.\nJinghe Zhang, Kamran Kowsari, James H Harrison,\nJennifer M Lobo, and Laura E Barnes. 2018. Pa-\ntient2vec: A personalized interpretable deep repre-\nsentation of the longitudinal electronic health record.\nIEEE Access, 6:65333–65346.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsiﬁcation. In Advances in neural information pro-\ncessing systems, pages 649–657.\nYin Zhang, Rong Jin, and Zhi-Hua Zhou. 2010. Un-\nderstanding bag-of-words model: a statistical frame-\nwork. International Journal of Machine Learning\nand Cybernetics, 1(1-4):43–52.\nYufeng Zhang, Xueli Yu, Zeyu Cui, Shu Wu,\nZhongzhen Wen, and Liang Wang. 2020b. Every\ndocument owns its structure: Inductive text classi-\nﬁcation via graph neural networks. arXiv preprint\narXiv:2004.13826."
}