{
  "title": "A Short Study on Compressing Decoder-Based Language Models",
  "url": "https://openalex.org/W3206151786",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5032073184",
      "name": "Tianda Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5074422391",
      "name": "Yassir El Mesbahi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5008923192",
      "name": "Ivan Kobyzev",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103935867",
      "name": "Ahmad Rashid",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5037421244",
      "name": "Atif Mahmud",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5077970876",
      "name": "Nithin Anchuri",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5037868135",
      "name": "Habib Hajimolahoseini",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100355692",
      "name": "Yang Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5028862918",
      "name": "Mehdi Rezagholizadeh",
      "affiliations": [
        "Huawei Technologies (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3048823912",
    "https://openalex.org/W2964222566",
    "https://openalex.org/W2294370754",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W3213820586",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3114420105",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3156643189",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3120832022",
    "https://openalex.org/W3175002116",
    "https://openalex.org/W2977720775",
    "https://openalex.org/W3200786561",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3152607317",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W3034695001",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2990704537",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3174731106",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W2970454332"
  ],
  "abstract": "Pre-trained Language Models (PLMs) have been successful for a wide range of natural language processing (NLP) tasks. The state-of-the-art of PLMs, however, are extremely large to be used on edge devices. As a result, the topic of model compression has attracted increasing attention in the NLP community. Most of the existing works focus on compressing encoder-based models (tiny-BERT, distilBERT, distilRoBERTa, etc), however, to the best of our knowledge, the compression of decoder-based models (such as GPT-2) has not been investigated much. Our paper aims to fill this gap. Specifically, we explore two directions: 1) we employ current state-of-the-art knowledge distillation techniques to improve fine-tuning of DistilGPT-2. 2) we pre-train a compressed GPT-2 model using layer truncation and compare it against the distillation-based method (DistilGPT2). The training time of our compressed model is significantly less than DistilGPT-2, but it can achieve better performance when fine-tuned on downstream tasks. We also demonstrate the impact of data cleaning on model performance.",
  "full_text": "A Short Study on Compressing Decoder-Based\nLanguage Models\nTianda Li∗, Yassir El Mesbahi∗, Ivan Kobyzev, Ahmad Rashid,\nAtif Mahmud, Nithin Anchuri, Habib Hajimolahoseini, Yang Liu,\nMehdi Rezagholizadeh\nHuawei Noah’s Ark Lab\n{tianda.li, yassir.el.mesbahi1,ivan.kobyzev, ahmad.rashid, mehdi.rezagholizadeh}@huawei.com\nAbstract\nPre-trained Language Models (PLMs) have been successful for a wide range of\nnatural language processing (NLP) tasks. The state-of-the-art of PLMs, however,\nare extremely large to be used on edge devices. As a result, the topic of model\ncompression has attracted increasing attention in the NLP community. Most\nof the existing works focus on compressing encoder-based models (tiny-BERT,\ndistilBERT, distilRoBERTa, etc), however, to the best of our knowledge, the\ncompression of decoder-based models (such as GPT-2) has not been investigated\nmuch. Our paper aims to ﬁll this gap. Speciﬁcally, we explore two directions: 1)\nwe employ current state-of-the-art knowledge distillation techniques to improve\nﬁne-tuning of DistilGPT-2. 2) we pre-train a compressed GPT-2 model using layer\ntruncation and compare it against the distillation-based method (DistilGPT2). The\ntraining time of our compressed model is signiﬁcantly less than DistilGPT-2, but\nit can achieve better performance when ﬁne-tuned on downstream tasks. We also\ndemonstrate the impact of data cleaning on model performance.\n1 Introduction\nPre-trained Language Models (PLMs) have recently achieved great success on a wide variety of\nNLP problems [Peters et al., 2018, Devlin et al., 2019, Liu et al., 2019, Yang et al., 2020, Radford\nand Narasimhan, 2018, Radford et al., 2019]. With the rapidly increasing parameter count and\ntraining time, the state-of-the-art (SOTA) PLMs are becoming more challenging to be deployed\non edge devices. In particular, RoBERTa-large has 355 million parameters, GPT-2-xl has 1.5\nbillion parameters, and the most recent GPT-3 [Brown et al., 2020] has 175 billion parameters. The\nimportance of model compression methods is emergent in NLP [Gupta and Agrawal, 2021].\nGenerally speaking, the compression of a PLMs can be divided into two stages: initialization and\nﬁne-tuning. In the initialization stage, the compressed model’s parameters can be either transferred\nfrom a larger pre-trained model [Sun et al., 2019, Passban et al., 2020] or pre-trained from scratch as a\nlanguage model. Pre-training the smaller language models is cumbersome since typically knowledge\nis distilled from a larger teacher [Jiao et al., 2020, Sanh et al., 2020]. In the ﬁne-tuning stage, the\ninitialized compressed model is trained on a downstream task. In our work, we will investigate both\nstages of the compression.\nA predominant solution for ﬁne-tuning compressed PLMs is knowledge distillation (KD) [Rogers\net al., 2020]. Most of the reported KD results in the literature [Hinton et al., 2015, Bucilu ˇa et al.,\n2006, Gao et al., 2018, Kamalloo et al., 2021, Rashid et al., 2020, Li et al., 2021, Haidar et al., 2021]\nare for encoder-based models such as BERT, RoBERTa. KD on decoder-based models [Radford et al.,\n∗Equal contribution\nPreprint. Under review.\narXiv:2110.08460v1  [cs.CL]  16 Oct 2021\nFigure 1: The overview of knowledge distillation techniques we apply in the paper: Annealing-KD,\nMATE-KD and RAIL-KD’s.\n2019] has not been investigated much. In this work we explore both teacher-based and teacher-free\ntechniques aiming to improve compressed GPT-2 ﬁne-tuning.\nPre-training of compressed encoder-based models has been extensively explored [Sanh et al., 2020, Xu\net al., 2020, Sajjad et al., 2020]. However, DistilGPT2 [HuggingFace, 2019] is the only compressed\nGPT-2 model we found in the literature. The authors pre-trained DistilGPT-2 with KD using the\noriginal GPT-2 as a teacher, which results in a long training time. In our paper, we investigate\npre-training without KD to signiﬁcantly improve time efﬁciency.\nOur contribution is three-fold:\n1. We benchmark different SOTA teacher-based and teacher-free techniques for ﬁne-tuning\nDistilGPT2 on downstream tasks.\n2. We compare several truncation methods for pre-training initialization of the compressed\nGPT-2 model.\n3. We conduct data cleaning of the OpenWebText dataset and pre-train our compressed model\ninitialized with the best truncation technique. This pre-training scheme is time-efﬁcient. At\nthe same time, ﬁne-tuning on downstream tasks reveal that our pre-trained model achieves\nbetter performance compared to DistilGPT-2.\n2 Methodology\nIn this section, we introduce the techniques we applied for ﬁne-tuning and pre-training compressed\nGPT-2 models. We start with knowledge distillation and teacher-free methods for ﬁne-tuning, then\nwe introduce layer truncation methods for initializing the student from the teacher, and ﬁnally, we\ndiscuss data-cleaning for efﬁcient pre-training.\n2.1 Fine-tuning with and without a teacher\nHere, we discuss the techniques we applied to improve ﬁne-tuning of DistilGPT-2 model on down-\nstream tasks.\n2.1.1 KD Methods\nHinton et al. [2015] proposed KD as a way to improve the training of a small neural network (student).\nGiven a bigger model (teacher), KD adds a speciﬁc loss term to the loss function of the student aiming\n2\nto push the student’s predictions close to the teacher’s. In this paper, we consider four different KD\nmethods: 1) Vanilla KD, 2) Annealing-KD, 3) MATE-KD and 4) RAIL-KD. The overview of these\nmodels is given in Figure 1,\nFor Annealing-KD [Jafari et al., 2021], the student is trained in two phases. During phase 1, the\nstudent model learns only from the teacher. Here the temperature controls the smoothness of the\nteacher’s output, annealing it from easy-to-learn to the actual sharp distribution. During phase 2, the\nstudent model is trained only on the ground-truth label.\nFor MATE-KD [Rashid et al., 2021], the training process has two steps: maximization and mini-\nmization. At the maximization step, a generator is trained to produce perturbed input for both student\nand teacher models. The target of this stage is to produce the input that can maximize the divergence\nbetween the teacher and student output. At the minimization step, the student model is trained to\napproximate the teacher’s output.\nFor RAIL-KD [Haidar et al., 2021], during the training we transfer the knowledge from teacher’s\nintermediate layers to student’s intermediate layers. In our case, the 6-layers student model is distilled\nfrom 12 layers GPT2 model.\n2.1.2 Teacher-free Methods\nHere, we describe the most commonly used teacher-free techniques.\nLabel Smoothing (LS) Szegedy et al. [2015] proposed this method to improve the training of a\nclassiﬁer. For this, a cross-entropy loss should be calculated with smoothed labels rather than one-hot\nlabels. The smoothed labels are given by:\ny′= (1−α)y+ αu, (1)\nwhere u(K) = 1/Kis the uniform distribution on Kclasses, yis the one-hot golden label, and αis\na parameter between 0 and 1 controlling the sharpness of the resulting soft label.\nTF-reg The TF-reg [Yun et al., 2020] technique is very similar to label smoothing, the only\ndifference is that TF-reg switches the uniform distribution uin Equation 1 to the label-dependent\ndistribution p(k), deﬁned by:\npc(k) =\n{a, if k= c(is the correct label)\n1−a\nK−1 , otherwise. (2)\nWhere ais a parameter between 0 and 1. TF-reg has two parameters (aand α) instead of just one (α)\nwhich allows for better tuning. The smoothed label for xin TF-reg is given by:\ny′= (1−α)y+ αpc(x), (3)\nwhere c(x) is the correct label of the sample x.\nSelf-distillation (Self-KD) Self-KD [Furlanello et al., 2018] is a variation of the KD method, in\nwhich we ﬁrst ﬁne-tune a copy of the student on the dataset and then freeze it. This copy serves as a\nteacher during the training.\n2.2 Student Layers Initialization\nIn this section, we introduce the student’s layers initialization from the teacher. Sajjad et al. [2020]\nshows that an easy way of compressing pre-trained large models is to simply \"truncate\" them by\ndropping some layers. Inspired by that, we propose our pruning techniques and list the top 2 pruning\nstrategies below (The overall six pruning techniques and results are introduced in Appendix A.1)\nUniform Selection (Uni) We select layers to copy uniformly, starting from the ﬁrst layer. For\nexample, if the teacher has 12 layers, we would initialize the student (6-layers model) by teacher\nlayers 0, 2, 4, 6, 8 and 10.\n3\nAlgorithm 1 Details of Pseudo-uniform selection for layer initialization from a larger model with n\nlayers to a smaller model with klayers.\nRequire: n>k ; n mod k= 0; n mod 2 = 0\nEnsure: Pseudo-uniform selection of length k\nstep ←⌊n\nk ⌋\nstart ←0\nend ←n−1\nselection ←[]\nwhile start ≤end do\nselection ←selection +[start]\nselection ←selection +[end]\nstart ←start + step\nend ←end −step\nend while\nPseudo-uniform Selection (Psudo) This strategy is inspired from DistilBert’s paper [Sanh et al.,\n2020], where they initilaize their model (DistilBert) with teacher’s (Bert-base) layers 0, 2, 4, 7, 9\nand 11. In contrast with uniform selection, we make sure ﬁrst and last layers are always selected.\nA generalization of this strategy can be described by the Algorithm 1, where nstands for the total\nnumber of teacher’s layers and kis number of layers we want to select (also number of student’s\nlayers).\n3 Experiments\n3.1 Data\nOpenWebText is an open-source recreation of the WebText corpus (on which the GPT-2 model was\ntrained). We use this data for pre-training our compressed model. Original WebText contains over 8\nmillion documents for a total of 40 GB of text. In our experiment, we only used a fraction of these\ndata.\nWe assess compressed models on several downstream tasks. First, we employ the Wikitest103\ndataset [Merity et al., 2016] to ﬁne-tune a compressed model as a language model and measure the\nperformance with perplexity score (the lower - the better). Then, we ﬁne-tune a compressed model\nas a classiﬁer on 6 out of 8 tasks in the SuperGLUE [Wang et al., 2019a] benchmark. Moreover,\nwe evaluate the ﬁne-tuning of a compressed model as a classiﬁer on 7 out of 9 tasks of the General\nLanguage Understanding Evaluation (GLUE) [Wang et al., 2019b] benchmark.\n3.2 Fine-tuning on GLUE\nWe apply KD and teacher-free techniques described in Section 2.1 to ﬁne-tune DistilGPT-2 model on\nGLUE tasks. The results are in Table 1 and Table 2. We can see that Annealing-KD, MATE-KD, and\nRAIL-KD all outperform VanillaKD. Interestingly, regular ﬁne-tuning itself is a strong baseline that\nperforms comparatively well to vanilla KD, and it even outperforms the LS and TF-reg techniques.\nSelf-KD performance is comparable with other teacher-free techniques. RAIL-KD performs worse\nthan MATE-KD and Annealing-KD, which indicates that distilling intermediate layers doesn’t have\nan advantage over data augmentation or annealing scheduling. MATE-KD performs the best among\nfour KD techniques. One should notice that this pattern is slightly different from the ﬁne-tuning of\nBert-based models [Li et al., 2021]. One possible explanation might be that decoder-based models are\nmore sensitive to hyper-parameters. Data augmentation is a more robust way to improve the student\nmodel’s performance.\n3.3 Experiments on Layer Truncation\nFirst, we initialize a 6-layer GPT-2 model with the initialization techniques described in section 2.2.\nThen, we pre-train the models on fraction of the OpenWebText dataset. For these experiments, we use\neither 4 or 8 GPUs and make use of the DeepSpeed framework [Rajbhandari et al., 2019, Rasley et al.,\n2020, Ren et al., 2021, Rajbhandari et al., 2021] to accelerate the training process. Then, we report\n4\nTable 1: Dev set results of teacher-free methods on GLUE. We benchmark pure ﬁnetuning of\nDistilGPT2 (ﬁrst line) with teacher-free regularisation training and Self-KD. The last line is the\nperformance of 12 layers GPT-2 model.\nEvaluated Model CoLA RTE MRPC(f1) SST-2 MNLI QNLI QQP Average\nDistilGPT2 39.0 65.2 87.9 91.5 86.5 79.9 89.7 77.1\nLS 38.9 64.8 87.3 91.6 86.6 80.1 89.6 77.0\nTF-reg 38.7 65.1 87.4 91.4 86.9 80.2 89.6 77.0\nSelf-KD 39.7 64.7 87.3 90.9 87.0 80.5 89.8 77.2\nGPT-2 43.2 66.8 87.6 92.2 82.3 88.6 89.5 78.6\nTable 2: Dev set results of KD methods on GLUE. Here the student is DistilGPT2 and the teacher is\n12 layers GPT-2. See Table 1 for the student’s and teacher’s performance.\nTeacher Evaluated Model CoLA RTE MRPC(f1) SST-2 MNLI QNLI QQP Average\nGPT-2 VanillaKDDistilGPT2 39.3 65.7 88.0 90.7 79.6 86.8 89.4 77.1\nGPT-2 RailKDDistilGPT2 39.4 66.4 88.1 91.2 80.6 87.3 89.9 77.6\nGPT-2 AnnealingKDDistilGPT2 41.6 67.1 86.8 92.0 80.8 87.8 89.4 77.9\nGPT-2 MateKDDistilGPT2 42.1 67.5 88.8 92.0 81.6 87.7 90.0 78.5\nthe zero-shot performance of the pre-trained models in Table 3. Our compressed model outperforms\nDistilGPT-2 even when it’s trained on50% of the dataset. Also, our pre-training is tremendously\ntime-efﬁcient.\nTable 3: Truncated models’ zero-shot perplexity scores on Wikitext103 after pretraining. Models are\ntruncated with techniques from Section 2.2, pre-trained on fraction of the OpenWebtext dataset, and\nthen evaluated on Wikitext103 test set. All the truncated models in this table have 6 layers.\nModels Pretraining on fraction of OpenWebtext\nModel index Teacher Strategy PPL % of the dataset Epochs # GPUs Time (h)\n0 DistilGPT2 Pre-train 45.26 100 4 8 768\n1 GPT2 Psudo 56.38 10\n3\n4 8\n2 GPT2 Psudo 46.91 50 8 38\n3 GPT2 Uni 45.19 50 8 35\n4 GPT2-xl Psudo 54.70 10 4 24\n5 GPT2-large Psudo 59.32 10 4 17\nNext, we ﬁne-tuned the pre-trained models on the Wikitext103 and put the perplexities in Table 4.\nWe can see that the perplexity achieved by GPT2-psudo is still worse than DistilGPT2’s. The 6-\nlayer model truncated from GPT2-xl teacher and initialized with Pseudo-uniform truncation method\n(GPT2-xl-psudo) reaches a perplexity close to DistilGPT2’s despite being pre-trained on a fraction\n(10%) of the OpenWebtext dataset (but it is not comparable to DistilGPT2 since it has three times\nmore parameters).\n3.4 Effect of Data Cleaning on Pre-training\nWe found that the OpenWebText dataset contains a signiﬁcant amount of noisy samples. Some of\nthese are HTML code, others are pure noise (concatenation of special characters). To alleviate the\nproblem of noisy samples, we implemented a program that automatically inspects the samples, clears\nout HTML code and short sentences, eliminates sentences with a high ratio of non-alphanumerical\ncharacters (more than 10%) and duplicates. Using the above algorithm, we managed to dramatically\n5\nTable 4: Truncated models’ perplexity scores on Wikitext103 after ﬁne-tuning. Once models are\npre-trained on fractions of OpenWebText (table 3), they are ﬁne-tuned on Wikitext103 train set and\nthen evaluated on Wikitext103 test set. All the models compared in this table have 6 layers and the\nmodel index indicates corresponds the one in table 3.\nModels Fine-tuning pretrained models\nModel index Teacher Strategy PPL Epochs # GPUs Time (h)\n0 DistilGPT2 Pre-train 21.13 6 4 2\n1 GPT2 Psudo 23.44 12 4 5\n2 GPT2 Psudo 22.67 6 8 2\n3 GPT2 Uni 22.61 6 4 5\n4 GPT2-xl Psudo 21.30 6 4 5\n5 GPT2-large Psudo 23.44 6 4 5\nreduce the size of the OpenWebText dataset (from 332,011,430 to 114,366,559 samples, or by\n65.5%).\nWe pre-train a 6-layer GPT2 model (initialized with the pseudo-uniform strategy) on the cleaned\ndataset, then we ﬁne-tune it on the Wikitext103 and several datasets from the SuperGLUE and\ncompare the results with the model that has been pre-trained on the full dataset. For Wikitext103, we\nmeasure zero-shot (ZS) and post-ﬁne-tuning (FT) perplexities (PPL). Results are shown in Table 5.\nTable 5: Results after pre-training on regular/cleaned OpenWebText dataset. Truncated models are\npretrained on both datasets (original and cleaned) and their performance measured on several tasks.\nModels PretrainPretrain Scores after ﬁne-tuning\ndataset time (h) Wikitext103 BoolQ Copa CB Rte Wic Wsc\nPPL (ZS)PPL (FT) Acc. Acc. Acc. F1 Acc. Acc. F1 Acc.\nDistilGPT2Regular 768 45.26 21.13 71.16 56 73.21 61.45 62.45 63.6 63.46 77.64\nGPT2-PsudoCleaned 42 45.93 22.42 70.67 58 78.57 65.15 63.53 60.3 63.46 77.64\nRegular 49 44.51 22.29 68.07 56 71.42 49.77 58.48 59.24 59.61 73.74\nWe can see that cleaning the dataset helps reducing training time while allowing for achieving\ncomparable or better performance.\n4 Conclusion\nIn this work, we aim to compress GPT-2. First, we benchmark current SOTA KD and teacher-free\nmethods on DistilGPT2 and pick the best performing one. Then, we explore truncation methods for\nthe initialization of the student model from the teacher model’s parameters. Speciﬁcally, we propose\na pseudo-uniform strategy that outperforms alternative initializations in the language modeling\nexperiments on Wikitext-103. Finally, we conduct data cleaning on the OpenWebText dataset and\npre-trained our compressed model. To test the effectiveness of our strategy we carried out the\nexperiments on Wikitext-103 and 6 out of 8 SuperGLUE Benchmark datasets. Our pre-trained model\noutperforms DistilGPT-2 on 5 out of 7 downstream tasks, yet it is signiﬁcantly more time-efﬁcient.\nFor the future direction, we will evaluate our initialization strategy along with KD methods and\ninvestigate if the pre-training of our compressed GPT-2 can be improved even more.\nReferences\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-V oss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\n6\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models\nare few-shot learners, 2020.\nC. Buciluˇa, R. Caruana, and A. Niculescu-Mizil. Model compression. In Proceedings of the 12th\nACM SIGKDD international conference on Knowledge discovery and data mining, pages 535–541,\n2006.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding, 2019.\nT. Furlanello, Z. C. Lipton, M. Tschannen, L. Itti, and A. Anandkumar. Born again neural networks,\n2018.\nJ. Gao, J. Lanchantin, M. L. Soffa, and Y . Qi. Black-box generation of adversarial text sequences\nto evade deep learning classiﬁers. In 2018 IEEE Security and Privacy Workshops (SPW), pages\n50–56, 2018. doi: 10.1109/SPW.2018.00016.\nM. Gupta and P. Agrawal. Compression of deep learning models for text: A survey, 2021.\nM. A. Haidar, N. Anchuri, M. Rezagholizadeh, A. Ghaddar, P. Langlais, and P. Poupart. Rail-kd:\nRandom intermediate layer mapping for knowledge distillation, 2021.\nG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network, 2015.\nHuggingFace. Distilgpt2, 2019. URL https://huggingface.co/distilgpt2.\nA. Jafari, M. Rezagholizadeh, P. Sharma, and A. Ghodsi. Annealing knowledge distillation, 2021.\nX. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, and Q. Liu. TinyBERT: Distilling\nBERT for natural language understanding. In Findings of the Association for Computational\nLinguistics: EMNLP 2020, pages 4163–4174, Online, Nov. 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.ﬁndings-emnlp.372. URL https://www.aclweb.org/\nanthology/2020.findings-emnlp.372.\nE. Kamalloo, M. Rezagholizadeh, P. Passban, and A. Ghodsi. Not far away, not so close: Sam-\nple efﬁcient nearest neighbour data augmentation via MiniMax. In Findings of the Associa-\ntion for Computational Linguistics: ACL-IJCNLP 2021, pages 3522–3533, Online, Aug. 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.ﬁndings-acl.309. URL\nhttps://aclanthology.org/2021.findings-acl.309.\nT. Li, A. Rashid, A. Jafari, P. Sharma, A. Ghodsi, and M. Rezagholizadeh. How to select one among\nall? an extensive empirical study towards the robustness of knowledge distillation in natural\nlanguage understanding, 2021.\nY . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV . Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019.\nS. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer sentinel mixture models. CoRR,\nabs/1609.07843, 2016. URL http://arxiv.org/abs/1609.07843.\nP. Passban, Y . Wu, M. Rezagholizadeh, and Q. Liu. ALP-KD: Attention-Based Layer Projection for\nKnowledge Distillation, 2020.\nM. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep\ncontextualized word representations. In Proc. of NAACL, 2018.\nA. Radford and K. Narasimhan. Improving language understanding by generative pre-training. 2018.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are unsupervised\nmultitask learners. 2019.\nS. Rajbhandari, J. Rasley, O. Ruwase, and Y . He. Zero: Memory optimization towards training A\ntrillion parameter models. CoRR, abs/1910.02054, 2019.\n7\nS. Rajbhandari, O. Ruwase, J. Rasley, S. Smith, and Y . He. Zero-inﬁnity: Breaking the GPU\nmemory wall for extreme scale deep learning. CoRR, abs/2104.07857, 2021. URL https:\n//arxiv.org/abs/2104.07857.\nA. Rashid, V . Lioutas, A. Ghaddar, and M. Rezagholizadeh. Towards zero-shot knowledge distillation\nfor natural language processing. arXiv preprint arXiv:2012.15495, 2020.\nA. Rashid, V . Lioutas, and M. Rezagholizadeh. MATE-KD: Masked adversarial TExt, a companion\nto knowledge distillation. In Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1062–1071, Online, Aug. 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.acl-long.86. URL https://aclanthology.\norg/2021.acl-long.86.\nJ. Rasley, S. Rajbhandari, O. Ruwase, and Y . He. DeepSpeed: System Optimizations Enable\nTraining Deep Learning Models with Over 100 Billion Parameters, page 3505–3506. Association\nfor Computing Machinery, New York, NY , USA, 2020. ISBN 9781450379984. URL https:\n//doi.org/10.1145/3394486.3406703.\nJ. Ren, S. Rajbhandari, R. Y . Aminabadi, O. Ruwase, S. Yang, M. Zhang, D. Li, and Y . He. Zero-\nofﬂoad: Democratizing billion-scale model training. CoRR, abs/2101.06840, 2021. URL https:\n//arxiv.org/abs/2101.06840.\nA. Rogers, O. Kovaleva, and A. Rumshisky. A primer in BERTology: What we know about how\nBERT works. Transactions of the Association for Computational Linguistics, 8:842–866, 2020.\ndoi: 10.1162/tacl_a_00349. URL https://www.aclweb.org/anthology/2020.tacl-1.54.\nH. Sajjad, F. Dalvi, N. Durrani, and P. Nakov. Poor man’s BERT: smaller and faster transformer\nmodels. CoRR, abs/2004.03844, 2020.\nV . Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller, faster,\ncheaper and lighter, 2020.\nS. Sun, Y . Cheng, Z. Gan, and J. Liu. Patient knowledge distillation for bert model compression,\n2019.\nC. Szegedy, V . Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for\ncomputer vision. CoRR, abs/1512.00567, 2015. URL http://arxiv.org/abs/1512.00567.\nA. Wang, Y . Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman.\nSuperglue: A stickier benchmark for general-purpose language understanding systems. CoRR,\nabs/1905.00537, 2019a. URL http://arxiv.org/abs/1905.00537.\nA. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. Glue: A multi-task benchmark\nand analysis platform for natural language understanding, 2019b.\nC. Xu, W. Zhou, T. Ge, F. Wei, and M. Zhou. Bert-of-theseus: Compressing bert by progressive\nmodule replacing. In EMNLP, 2020.\nZ. Yang, Z. Dai, Y . Yang, J. Carbonell, R. Salakhutdinov, and Q. V . Le. Xlnet: Generalized\nautoregressive pretraining for language understanding, 2020.\nS. Yun, J. Park, K. Lee, and J. Shin. Regularizing class-wise predictions via self-knowledge distillation.\nIn 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13873–\n13882, 2020.\n8\nA Appendix\nA.1 Student Layer Initialization\nOverall, we tried 6 pruning strategies as listed below:\nUniform selection We select layers to copy uniformly, starting from the ﬁrst layer. For example, if\nthe teacher has 12 layers, we would initialize the student (6-layers model) by teacher layers\n0, 2, 4, 6, 8 and 10.\nVariant of uniform selection We select layers to copy uniformly, but the last layer we select should\nalways be the one before the last of the teacher’s layers.\nPseudo-uniform selection This strategy is inspired from DistilBert’s paper, where they initilaize\ntheir model(DistilBert) with teacher’s (Bert-Base) layers0, 2, 4, 7, 9 and 11. In contrast with\nuniform selection, we make sure ﬁrst and last layer are always selected. where nrepresents\nthe total number of teacher’s layers and k the number of layers we want to select (also\nnumber of student’s layers).\nBottom half selection This is a generalization of one of the strategies describe in their paper. We\nuniformly select from the bottom-half section of teacher’s layers. As an example, for a\n36-layer teacher, we select uniformly from its ﬁrst 13 layers. In the particular case of a\n12-layer teacher, we pick the ﬁrst six layers.\nTop half selection Similar to the bottom half selection, this strategy consists in selecting layers\nuniformly from the top layers of the teacher. For example, for a 48-layer teacher, we would\nselect uniformly from its top 24 layers. In the particular case of a 12-layer teacher, we pick\nthe last six layers.\nRandom selection We implement this method to have a baseline to compare with. We randomly\npick layers from the teacher, sort them by index and use them to initialize the student.\nWe apply the above pruning techniques on the GPT-2 models and measure their perplexities on\nWikitext103 after ﬁne-tuning. To easily identify the models we are training, we add to the names of\noriginal GPT-2 a sufﬁx indicating which layer selection strategy was used to initialize it. Table 6 shows\nthe correspondence between sufﬁxes and pruning strategies. Table 7 displays some characteristics\nof the resulting models, as well as their performance on Wikitext103 test set. We list several\nTable 6: Pruning strategies sufﬁxes\nStrategy Sufﬁx\nUniform uniform\nUniform (variant) uniform-2\nPseudo-uniform psudo\nBottom-half 6bh\nTop-half 6th\nRandom random\nobservations from ﬁne-tuning results:\nThe best validation curves come from the \"uniform/bottom-half/pseudo-uniform\" strategies\nWe observe a better convergence in these settings, which is similar to previous paper\nreported results.\nThe \"pseudo-uniform\" strategy achieves the best test results after ﬁne-tuning Perplexities are\nthe lowest in this setting, as shown in table 7.\nThe \"bottom-half\" strategy outperforms the \"top-half\" According to previous report, this is due\nto the fact that bottom layers tend to learn embeddings and general representations while\ntop layers are more task-specialized.\nOverall, as a conclusion of this experiment the pseudo-uniform initialization scheme clearly allows\nfor better generalization (table 7). We can also conclude that pre-training plays a signiﬁcant role in\n9\nTable 7: Truncated models’ perplexity scores on Wikitext103 test set\nModels # layers Teacher’s layers # heads Hidden size # parameters Epochs PPL\nDistilGPT2 6 12 768 81 M 6 21.13\nGPT2-xl-psudo\n6\n{0, 8, 16, 31, 39, 47}\n25 1,600 266 M 6\n22.8\nGPT2-xl-uniform {0, 9, 18, 27, 36, 45} 25.37\nGPT2-xl-uniform-2 {0, 10, 20, 30, 40, 46} 26.07\nGPT2-xl-6bh {0, 4, 9, 13, 18, 23} 25.54\nGPT2-xl-6th {24, 28, 33, 37, 42, 47} 47.51\nGPT2-xl-random {2, 11, 17, 25, 34, 40} 42.48\nGPT2-large-psudo\n6\n{0, 6, 12, 23, 29, 35}\n20 1,280 183 M 6\n23.79\nGPT2-large-uniform {0, 7, 14, 21, 28, 35} 27.68\nGPT2-large-uniform-2 {0, 8, 16, 24, 32, 34} 27.78\nGPT2-large-6bh {0, 2, 4, 7, 9, 12} 35.88\nGPT2-large-6th {13, 17, 21, 26, 30, 35} 74.15\nGPT2-large-random {5, 25, 26, 29, 32, 34} 79.68\nGPT2-medium-psudo\n6\n{0, 4, 8, 15, 19, 23}\n16 1,024 128 M 6\n28.09\nGPT2-medium-uniform {0, 4, 8, 12, 16, 20} 35.83\nGPT2-medium-uniform-2 {0, 5, 10, 15, 20, 23} 35.4\nGPT2-medium-6bh {0, 2, 4, 6, 8, 10} 36.2\nGPT2-medium-6th {12, 14, 16, 18, 20, 23} 88.71\nGPT2-medium-random {1, 3, 7, 11, 15, 21} 75.13\nGPT2-psudo\n6\n{0, 2, 4, 7, 9, 11}\n12 768 81 M 6\n26.14\nGPT2-uniform {0, 2, 4, 6, 8, 10} 26.6\nGPT2-6bh {0, 1, 2, 3, 4, 5} 29.86\nGPT2-6th {6, 7, 8, 9, 10, 11} 49.61\naligning the weights and making convergence faster: the performance of DistilGPT2 supports this\nclaim.\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6144301295280457
    },
    {
      "name": "Programming language",
      "score": 0.44508904218673706
    },
    {
      "name": "Arithmetic",
      "score": 0.3829967975616455
    },
    {
      "name": "Parallel computing",
      "score": 0.38047367334365845
    },
    {
      "name": "Natural language processing",
      "score": 0.3651409447193146
    },
    {
      "name": "Speech recognition",
      "score": 0.32666653394699097
    },
    {
      "name": "Algorithm",
      "score": 0.32656627893447876
    },
    {
      "name": "Mathematics",
      "score": 0.18730536103248596
    }
  ]
}