{
  "title": "Project CLAI: Instrumenting the Command Line as a New Environment for AI Agents",
  "url": "https://openalex.org/W3036646712",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2743284296",
      "name": "Agarwal, Mayank",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Barroso, Jorge J.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160361466",
      "name": "Chakraborti, Tathagata",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Dow, Eli M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302006718",
      "name": "Fadnis, Kshitij",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Godoy, Borja",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Pallan, Madhavan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221408871",
      "name": "Talamadupula, Kartik",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2161256997",
    "https://openalex.org/W1572384134",
    "https://openalex.org/W2297325673",
    "https://openalex.org/W1538821400",
    "https://openalex.org/W1515845301",
    "https://openalex.org/W2011301426",
    "https://openalex.org/W2919666802",
    "https://openalex.org/W2886885214",
    "https://openalex.org/W2063244929",
    "https://openalex.org/W1515851193",
    "https://openalex.org/W2964301388",
    "https://openalex.org/W2948223045",
    "https://openalex.org/W3000310589",
    "https://openalex.org/W3093131052",
    "https://openalex.org/W2963545046",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2950956891",
    "https://openalex.org/W2122223050",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W2963386163",
    "https://openalex.org/W78694127",
    "https://openalex.org/W2296335794",
    "https://openalex.org/W3021909058",
    "https://openalex.org/W3101355526"
  ],
  "abstract": "This whitepaper reports on Project CLAI (Command Line AI), which aims to bring the power of AI to the command line interface (CLI). The CLAI platform sets up the CLI as a new environment for AI researchers to conquer by surfacing the command line as a generic environment that researchers can interface to using a simple sense-act API, much like the traditional AI agent architecture. In this paper, we discuss the design and implementation of the platform in detail, through illustrative use cases of new end user interaction patterns enabled by this design, and through quantitative evaluation of the system footprint of a CLAI-enabled terminal. We also report on some early user feedback on CLAI's features from an internal survey.",
  "full_text": "Project CLAI: Instrumenting the Command Line as a\nNew Environment for AI Agents\nMayank Agarwal† · Jorge J. Barroso† · Tathagata Chakraborti† · Eli M. Dow†\nKshitij Fadnis† · Borja Godoy† · Madhavan Pallan‡ · Kartik Talamadupula†\n†IBM Research\n‡ Individual Contributor\nAbstract\nThis paper reports on Project CLAI(Command Line AI) which aims to bring the\npower of AI to the command line interface (CLI). The CLAIplatform sets up the\nCLI as a new environment for AI researchers to conquer by surfacing the command\nline as a generic environment that researchers can interface to using a simple\nsense-act API, much like the traditional AI agent architecture. In this paper, we\ndiscuss the design and implementation of the platform in detail, through illustrative\nuse cases of new end user interaction patterns enabled by this design, and through\nquantitative evaluation of the system footprint of aCLAI-enabled terminal. We also\nreport on some early user feedback on CLAI’s features from an internal survey.\n1 The command line is back!\nFor decades, the AI community has pursued the vision of autonomous assistants that operate with\nend-users inside computing systems. A key factor behind the stagnation of progress on this vision\nhas been that AI developers and researchers – who would together be tasked with bringing cutting-\nedge AI technology to such bots – do not want to engage with the deep intricacies of the typical\ncomputing (operating) system. However, with the arrival of cloud-based ecosystems and cloud-native\napplications, as well as the scalable real-world deployment of AI techniques, we are at an inﬂection\npoint akin to the initial emergence of large-scale networked terminals. This is an opportune moment\nto transform the typical user’s experience of computing systems, and imbue it with the power of AI.\nOne of the most powerful tools in software development is the command line, due to its speed and\nexpressiveness. However, another reason for the popularity of CLIs is that oftentimes, users have to\nuse them. This is proved by recent trends in software development: GUIs can rarely keep up with\nthe rate of change of features (e.g. consider the time it took to move from Docker to Kubernetes to\nOpenShift on cloud platforms). This means that CLIs become the default interfacing medium not\njust for new adopters of a software, but also for experts in one domain (e.g. programming) who are\nno longer experts in others (e.g. devops). The merging of developer and devops roles is certainly an\nemerging trend with the proliferation of cloud applications. This is highlighted by newly emergent\nCLIs with smart features, such as devspace [11], odo [32], and so on for cloud native applications;\nand the well documented second coming of CLIs in popular media: [2, 14, 35, 42].\nWhy command line AI? However, even with the CLI’s re-emergence, the issue of support on the\ncommand line remains a huge problem. [ 10] shows the increasing complexity of CLI commands,\nwhile [3] shows how community-sourced support has failed to keep up with the needs of users,\nthereby motivating the need for on-premise support (such as CLI plugins) with easy accessibility\n(natural language). Indeed, a bit of introspection in the support community [3] reveals that the reasons\nthat community-sourced support is failing – e.g. overwhelming numbers of duplicate questions (that\nGitHub: https://github.com/IBM/clai\narXiv:2002.00762v2  [cs.HC]  17 Jun 2020\ncan be used for training), or straightforward questions (that can be answered from documentation),\nand so on – are the same reasons that the state-of-the-art in AI can succeed.\nIt is this very lacuna in the space of intelligent assistance and automation that Project CLAI seeks\nto ﬁll. Given the CLI as the ﬁxed interface that users must utilize; and current advances in AI,\nML, and NLP technology; a toolkit that combines them both is the need of the hour. The software\ntoolkit provides the developer with a heavily instrumented version of Bash, the most commonly used\ncommand line interface – this acts as a whole new environment for the AI researcher to conquer.\n1.1 Project CLAI: A Challenge to the AI community\nThe AI community has always had a soft spot for AI assistants on the command line. In the early\nto mid 90s, researchers at the University of Washington conducted extensive academic work in this\nspace under the umbrella of “Internet Softbots” [12, 13]. These were AI agents that used a UNIX\nshell and the web to “interact with a wide range of internet sources”. The softbots provided a number\nof novel features, including one of the ﬁrst alternative interfaces to the UNIX command line: a\ndeliberative agent that could reason about multi-step sequences of commands; and the ability to\ngather information about an open world. In the late 90s, Microsoft introduced a slew of assistive\nagents along with their Ofﬁce productivity software. Unfortunately, the (in)famous Clippy and other\ncommercial softbots fell short of user expectations. Notably, that generation of embodied assistants\ntaught future designers valuable lessons [9] for the deployment of similar agents going forward.\nMore recently, a number of rule-based command line assistants such as bash-it, oh-my-zsh,\nthefuck, tldr, etc. have emerged. These CLI assistants generally deal with correcting misspelled\ncommands and other common errors on the command line, as well as automating commonly per-\nformed tasks using predeﬁned scripts. While these assistants certainly make the job of working with\nthe command line easier, they have a high maintenance burden due to the constant up-keep of the\nrules that form their back-end. In general, such domain speciﬁc solutions do not scale or generalize;\nand recent advances in machine learning [23, 24, 25] can make big contributions in this area. The\nLinux Plan Corpus [27] – a collection of Linux sessions – has become a great source of data for\nresearch in this direction [5]. Our own prior work on the UbuntuWorld system [8] used a combination\nof automated planning, reinforcement learning (RL), and information retrieval to drive data-driven\nexploration and decision making on the CLI, by bootstrapping data from the AskUbuntu forum.\nSimilarly, researchers have also attempted to use RL for interpreting actionable commands from\nnatural language instructions in Windows [6]. With recent advances in AI, especially in machine\nlearning and natural language processing, learning agents on the command line are poised for a\ncomeback [25, 21, 18, 24, 23].\n1.1.1 Cross-disciplinary challenges\nInterestingly, the command line environment provides a suite of challenges to the broader AI\ncommunity as well: this includes multi-agent systems (in orchestration of multiple plugins on the\nplatform); and automated planning and reinforcement learning (in being able to observe user behavior\nand learn over continuous interactions). Most importantly, it calls for cross-collaboration with the\nbroader human-computer interaction community so that the mistakes of the past [9] are not repeated.\n1.1.2 Challenge to the State of the Art\nEnergy Footprint. As we highlight later in our user study: .. the responses from bots residing on\nthe CLI must be instantaneous, and not allow for any noticeable lag. More than 75% indicated that\nthey expected a comeback within 3 seconds. Furthermore, these skills cannot consume too much\npower (computational or otherwise), since they must ultimately run on (end) terminals. As has been\ndocumented frequently, the state-of-the-art in machine learning largely ignores the real-world cost of\nrunning AI models [4, 37]. The CLAIplatform surfaces this as a core challenge to the community.\nNeurosymbolic AI. Another emerging theme in AI is the uniﬁcation of end-to-end data-only\napproaches with those that operate on knowledge. For command line support, this is going to be\nnecessary since it is impossible to generate unseen commands from the data in (for example) Stack\nOverﬂow alone. However, the domain does come with a lot of structured knowledge, such as man\n(manual) pages; and forces the skill developer to ﬁgure out the best synergy of both worlds. CLAI\nthus stands to give a big boost to this emerging research theme.\n2\nFigure 1: Overview of CLAI: the developer, while working on the command line interface, has access\nto an assistant in the form of a orchestrated set of skills. The AI researcher has access to an API that\ngives them access to an instrumented command line for building AI capabilities.\n2 CLAI: System Overview\nAt the core of CLAIare AI plugins or “skills” that monitor every user action on the terminal.1 This is\nequivalent to the notion of skills in IBM’s Watson Assistant (ibm.co/2LblJ70) or Amazon’s Alexa\n(amzn.to/2ZH9Olp) – a skill is a function that performs microtasks. Every command that the user\ntypes, or any execution process on the terminal, is piped through the skills active in CLAI for that\nsession. A skill can thus autonomously respond to any event on the terminal.\n2.1 CLAIfor the researcher: The CLAIAPI\nAn important user-persona of CLAIis the developer/researcher who creates the skills. CLAImakes\nBash available to a skill developer via a generic “environment” API, so that the developer does not\nhave to deal with interfacing issues and can instead focus on building their AI plugins. In order to\nmake this very familiar to the AI community, this interface allows execution of actions and sensing\nof the result of those actions in a manner very similar to the classic AI agent architecture [34, 39]. –\nimagine that same classic AI agent architecture, but replace the environment with Bash. This API\nthus makes Bash available as another new, exciting playground for AI agents, much like OpenAIGym:\ngym.openai.com/. The CLAIAPI – built in Python3– has two major components.\nThe CLAISkill API This lets a developer intercept\nas well as execute a callback on every user input\non the command line after the user hits return, and\nlets them respond appropriately. Developers can: (a)\nDo nothing and let normal life on the command line\nfollow. This includes doing nothing but registering\nan event to learn from that event and/or track user\nstate; (b) Add something to the user input – e.g. a\nﬂag that would make a command work; (c) Replace\nthe user input by something else – e.g. respond to a natural language request for automation; (d)\nRespond to the outcome (e.g. error) of a user command for in-situ support and troubleshooting; (e)\nAdd something to the outcome – e.g. for proactive support or pedagogical purposes.\nState The State object contains information about the system – including the state of the shell,\nsystem memory, connectivity, ﬁle system, etc. – as the state information or percept received\nfrom the terminal session that a skill is plugged into.\nAction The Action object is the directive from the skill to the terminal. This includes the suggested\ncommand, a description and an explanation for the suggestion, and other control parameters\nthat control the user action (such as permissions to execute). The skill can also return a\nsequence of Action objects in response to a user command or terminal event to complete a\nprocess based on the current user intent.\n1Although the architecture is generalizable to a large extent, we currently only support Bash since it is the\nmost commonly used CLI. As a proof of this concept, we have recently ported all the discussed functionalities\ninto the USS terminal in the z/OS operating system on mainframes. Those results are not public yet. In the rest\nof this paper, unless otherwise mentioned, we will thus be referring to Bash whenever we mention command\nline, terminal, or shell in the context of the CLAI platform or infrastructure.\n3\nThe CLAIOrchestration API The CLAIassistant is realized in the form of an orchestrated set of\nskills – all active skills communicate with the “orchestration layer” that decides whether to pass\non their individual responses to the terminal. The orchestrator enforces the following contract\nbetween the terminal and the skills: every event is reported to all active skills in the form of the State\nObject, and every skill responds with an Action object (or a list of Action objects) with an associated\nconﬁdence (self-determined by the skill) of its relevance and/or accuracy. This act-sense cycle\nallows an AI agent plugged into Bash to act and learn over continued periods, either by itself [8] or in\nthe course of prolonged interaction with the user.\n2.2 CLAIfor the end user: Interaction Patterns\n>> <command>\nCLAI: augment and / or replace <command>\nuser: y/n/e\n<stdout>\nCLAI: augment to stdout\n<stderr>\nCLAI: respond to stderr\nCLAI in the background. The other user-persona of\nCLAI– the actual user of the command line – has three\nways of using CLAIskills. In the ﬁrst mode, the user’s\ninput most closely resembles normal life on Bash. For\nmost commands, the user experience is entirely un-\nchanged.2 When a skill does get invoked, the user will\nexperience the following interfacing pattern:\n- CLAI may replace the user input command (or augment it) in order to make execution\nwork as the user likely intended. Users see the augmented or altered input command and\nmay approve the input variant for execution, or ask for an explanation on the command\nsubstitution rationale. This may be used to for example ﬁx a mistake in the command on the\nﬂy or translate a natural language input on the command line to their Bash syntax.\n- CLAImay add additional information to the stdout. This may be used, for example, for\npedagogical purposes (e.g. a better way to perform that task) or for alerting the user to\ncertain system information, or just enhancing functionality of existing Bash utilities.\n- CLAImay respond to the stderrby providing additional information for troubleshooting,\nor by suggesting a ﬁx that the user can follow through on in their next command.\nCLAI explicitly invoked. In order to force assistance from CLAI, a user may opt to demand a\nresponse from CLAI using the syntax below. Doing so will cause CLAI to respond with the skill\nthat it believes to be most relevant to the context bypassing the determination of relevance from the\norchestration layer (e.g. ignore low conﬁdence of a response).\n>> CLAI <command>\nCLAIforced skill invocation. Finally, the user can force a particular skill to respond, regardless of\nthe orchestration pattern (e.g. ignoring its conﬁdence and those of other skills currently active):\n>> CLAI <skill_name> <command>\n3 CLAI in Action\nIn this section, we provide details of skills and orchestration patterns illustrating the various capabili-\nties of CLAIand typical user interactions with an assistant on the command line. Please refer to the\nappendix for screenshots of all the examples in the paper.\n3.1 CLAISkills\nCLAI comes with a few default skills aimed to demonstrate a large set of features both to the end\nusers as well as potential skill developers. They fall into one or more of the following categories.\n3.1.1 Skill Categories\nBased on the interfacing options described in Section 2.2, a few key interaction patterns emerge.\nWhile these are not intended to be exhaustive, they do capture some of the most interesting interaction\ntypes that we have explored so far with CLAI.\n2This is a conscious design decision in light of lessons learned from historical deployments of assistants in\noperating systems, often deemed to be unnecessarily obtrusive [9].\n4\nBash CLAI-enabled Bash\nuser leaves to ﬁgure out stuff » do xyz\n» command p CLAI: command p? y/n/e\nuser: y\nNatural language support. This pattern allows\nthe user to interact with the command line in nat-\nural language: e.g. the user can ask » how do\nI extract file.bz2, or tell the terminal to »\nextract all images from this archive.\nBash CLAI-enabled Bash\n» command p » do task xyz\n» command q CLAI: command p\n» command r CLAI: command q\nCLAI: command r\nAutomation. This pattern allows the user to hand off com-\nplex tasks (achieved by one or more commands) to CLAI– e.g.\ndeployment of an application to the cloud. Developers already\nwrite scripts to achieve some of this functionality. This feature\nrescues them from writing tedious automation scripts and in-\nstead provide task-dependent automation. The most obvious technology match is to established AI\ntechniques such as automated planning and reinforcement learning.\nBash CLAI-enabled Bash\n» command q » command q\nError: xyz Error: xyz\nuser leaves to ﬁgure out stuff CLAI: command p\n» command p » command p\n» command q » command q\nOn-premise support. Currently, when the com-\nmand line user encounters an error, the usual re-\nsponse is to indulge in the following loop: copy the\nerror from the terminal, go over to a web browser,\nsearch on the internet, copy the top answer, and\ncome back to the terminal to try it out. This is a\nfrustrating and repetitive pattern of interaction on\nCLIs. The in-situ support and troubleshooting pattern of CLAI brings help from online forums,\nsupport communities, and support documentation, directly to the terminal, so that users do not have to\nremove themselves from their immediate work context. This ensures that the support can be (1) local\nor personalized to the user’s system; (2)immediate; and (3) in-situ without the user losing context.\nBash CLAI-enabled Bash\n» command q » command q\nError: xyz CLAI: command p\nuser leaves to ﬁgure out xyz CLAI: command q\n» command p\n» command q\nProactive support. In certain situations, CLAI\ncan anticipate errors and let the user know about\nthose errors (or even go ahead and ﬁx them in the\nbackground) in advance. For example, it could be\nthe case that a user might need to free up space on\na cloud instance before proceeding to deploy an\napplication. In such cases, CLAI skills can catch\nand prevent future errors that the user would otherwise encounter on the standard command line.\nBash CLAI-enabled Bash\n» command p » command p\nstdout+stderr stdout+stderr\nCLAI: q may be a better option\nPedagogy and Augmentation. The CLAI sys-\ntem can also chime in from time to time and help\nthe user with their proﬁciency on the terminal. This\ncould involve something as simple as letting them\nknow about new features (e.g. letting the user know\nthat the new way of running Flask applications is » flask <file> when they type in » python\n<file>); or in the long run even retaining and guiding (for example) a new adopter of cloud platforms\ninto becoming an expert on the cloud. This may also involve augmenting the standard functionali-\nties of existing command line utilities with new capabilities by adding to the standard output new\ninformation that may enhance the user experience.\n3.1.2 CLAIAvailable Skill Catalog\nnlc2cmd This is the canonical example of a natural language interface to the terminal. It connects\nto a Watson Assistant (WA) instance in the backend to interpret user intents, and translate those\nto popular bash commands like tar, grep, etc. Each Bash utility here corresponds to a separate\nWA workspace, and each ﬂag of that utility maps to a speciﬁc intent with the parameters being the\ncorresponding entities: thus each command is uniquely mapped to a corresponding natural language\nunderstanding layer given its man (manual) page.\ntellina Manually transforming man pages into NLU workspaces does not scale. This skill integrates\nthe state of the art of the nlc2cmd use case: Tellina [ 25]. Tellina builds on recent advances in the\napplication of deep learning techniques to natural language processing (NLP) by adopting Seq2Seq\n[38] models from translation from English to a set of command templates. This model is augmented\nwith an argument ﬁlling module that performs template generation followed by program slot ﬁlling\nin order to ﬁll out the various slots in the command template correctly.\n5\nCLAIﬁxit This skill provides help in response to the last command executed, by echoing back the\nresponse from the massively popular thefuckplugin for the command line. This skill is meant to\nillustrate how to integrate existing Bash plugins into the CLAIplatform.\nman page explorer This skill interprets questions in natural language, and responds with the most\nrelevant command it can ﬁnd from the man pages installed in the system. It also augments its response\nwith a concise description of the man page using the popular tldr plugin. This is an illustration\nof both natural language support as well as plugin integration. This agent trains a scikit-learn\n[28] TF-IDF vectorizer [30] over the retrieved man pages, and uses the cosine similarity between the\nnatural language question and the man page contents to suggest a command.\nCLAIhowdoi This is very similar to the man page explorer, but instead of using manuals, it responds\nwith the most relevant answer from Unix Stack Exchange [ 36]. The posts and their highest rated\nanswers are indexed into Elasticsearch [17]. The query is compared against this index and the most\nrelevant post and its accepted answer is returned.\nCLAIhelpme This is identical to howdoiexcept that it ﬁres when there is a standard error.\nAll the above skills have the same purpose: to ensure that users of the command line do not have to\nleave context every time they face an inane error or do not have the syntax memorized, and go looking\nfor answers on the internet. The following skills, on the other hand, explore much more sophisticated\nuse cases demonstrating how CLAIexpands to way beyond episodic support and troubleshooting.\ngitbot This bot helps a developer navigate their local git setup and their GitHub repository from\nthe command line. It highlights two use cases: 1) the use of a local natural language layer built on\nRasa NLU [31] as opposed to calls to external servers in nlc2cmdand tellina3 – this replicates the\nnl2cmd use case but on git commands; and 2) Illustration of the use of the GitHub Actions API [16]\nto control the online GitHub repository (issues, pull requests, etc.) without leaving the command line.\nCLAIdataXplore Data science has become one of the most popular real-world applications of ML.\nThe dataXploreskill is targeted speciﬁcally toward making the CLI easier to adopt and navigate for\nsuch data scientists. The current version of the skill provides two functionalities: summarizeand\nplot. summarizeutilizes the describe function [44] of the popular Pandas library [45] to generate a\nhuman-readable summary of a speciﬁed CSVﬁle; this functionality is intended to allow data scientists\nto quickly examine any data ﬁle right from the command line. plot builds on the plot function\nprovided by MatPlotLib [20], and the pillow [1, 15] library to generate a plot of a given CSV ﬁle.\nSuch functionalities illustrate how CLAIcan be used as a CLI assistant for data science.\ncloudbot This is a stateful agent that can automate tasks involving Docker [26] and Kubernetes\n[7] requiring the execution of a sequence of actions by harnessing automated planning techniques\nas an instance of the automation use case. The role of the planner here is to generate scripts that\nwould otherwise have to be speciﬁed manually – for example, while deploying an application to\nthe cloud. In addition to automating the lengthy deployment pipeline, the YAML ﬁle that currently\nneeds to be written manually is generated automatically by the skill by: 1) monitoring user activities\non the terminal; 2) pinging the cloud account for the types of services available; and 3) parsing\nthe Dockerﬁle. The planner used is [ 19]. This skills also demonstrates integration of continuous\nmonitoring of user state and plan recognition to continuously predict a possible intent and re-plan as\nthe user executes commands on their terminal. The plan recognizer used is [29].\n3.2 Orchestration Patterns\nThe orchestration layer comes with a unique set of challenges [33]. There may be two approaches\nto orchestrate skills: 1) apriori, where the orchestrator acts as a ﬁlter and decides which plugin to\ninvoke; and 2) posterior, where all plugins listen and respond, and let the orchestrator pick the best\nresponse (this is the current setup). The apriori option is likely to have a smaller system footprint, but\ninvolves a single bottleneck based on the accuracy of the classiﬁer which determines which plugin to\ninvoke. Furthermore, this requires that the orchestrator be cognizant of the full list of plugins and their\n3This design decision has serious consequences – on the one hand local skills can adapt to individual users\nbetter and are preferred in terms of security and privacy and do not need an internet connection to function on a\nmachine. They are also faster. However, it does come with much higher memory and compute footprint on the\nlocal system as well as signiﬁcantly more installation overhead.\n6\nFigure 2: CLAIsystem footprint:\n(a) Latency; (b) Memory.\nFigure 3: Latency proﬁle of activating a CLAI skill. This\nactivates the CLAIcore along with the activation procedure\nof the speciﬁed skill, thus increasing the latency but still\nkeeping it well below the preferred sub-second mark.\nFigure 4: Latency proﬁle of executing a bash command with\ninstalled clai skills. This activates the CLAIcore along\nwith CLAIskills. While the latency increases signiﬁcantly to\n2s, around 80% of it is contributed by the skill itself.\ncapabilities – this is unrealistic. The posterior option – despite increased latency and computational\nload – keeps the skill design process independent from the orchestration layer. Skill conﬁdences can\nbe calibrated over time by learning from user responses to CLAIactions: either directly from their\ny/n/eresponses; or indirectly by observing what command they executed after a suggestion, and\nmatching that to how similar it is to the suggested course of action.\nRule-based orchestration. Among the rule-based orchestration patterns packaged with the soft-\nware are the following: 1) Max-orchestrator: this pools responses from the active skills and passes the\nresponse with the highest conﬁdence, above a threshold speciﬁed by either the user or the developer,\non to the command line; 2) Threshold-orchestrator: This is the max-orchestrator where the threshold\nis bumped up or down based on user feedback; and 3) Preference-based orchestrator: The user can\nprovide partially ordered preferences above the thresholding-and-max mechanism.\nLearning-based orchestration. With continued feedback from the end user, much more sophisti-\ncated orchestration patterns can be learned [43]. This is especially useful in adapting the assistant\nto speciﬁc users and user types. CLAIcomes packaged with a contextual bandit based orchestrator,\nwhich uses the conﬁdences returned by each skill as the context vector and decides which skill should\nrespond to the user command. The user feedback on the orchestration choices is used to reward the\ncontextual bandit model, which in turn helps it to adapt to the user’s requirements. Additionally, to\navoid the initial exploration phase which can adversely affect the user experience, the bandit model\ncan be warm-started with a speciﬁc user proﬁle. We include four warm-start proﬁles: 1) ignore-clai:\nthis warm-starts the orchestrator to ignore all CLAI responses; 2) max-orchestrator: this warm-\nstarts the bandit to select the skill that responds with the maximum conﬁdence; 3) ignore-skill: this\nwarm-starts the bandit to ignore a particular skill and behave as a max-orchestrator otherwise; and 4)\nprefer-skill: this warm-starts the bandit to prefer a particular skill over another and is useful in cases\nwhere the user has preferences over skills with overlapping domains – e.g. a user on a Mac terminal\nmight not have use for a skill that retrieves data from the Unix Stack Exchange.\n4 Internal Evaluations\nOne of the primary challenges in deploying a framework like CLAI is ensuring that the resources\nconsumed do not hinder the user experience on the command line. This is evident from our survey of\nend users – a whopping 80% of the respondents (and 93% of developers/devops) require a latency\nof less than 3 seconds, with more than half that number requiring an even more stringent sub-\nsecond latency. This, in addition to the energy and compute footprint, makes this domain especially\nchallenging to the state of the art in AI. More details are provided in the appendix.\nSystem Footprint We proﬁle CLAIto understand the latency of the system usingyappi[40]. Since\nCLAIconsists of the CLAIcore and pluggable skills, we focus on two scenarios: 1) when only the\nCLAI core is invoked; and 2) when the CLAI core is invoked along with CLAI skills. We perform\nour analysis on a Quad-Core Intel i7 processor with 16 GB RAM, and observe that the core CLAI\nfunctionality of listing CLAIskills takes only 0.05 seconds; while more computationally expensive\n7\n(a)SURVEY RESPONDENT PROFILE\n(c)RESPONDENT PROFILE BY AI EXPERIENCE\n(d)COMFORTABLE ACTING ON BB SUGGESTIONS\n(e)USE CLOUD PLATFORMS FOR WORK\n (h)WHY DO YOU USE CLIs?\n(f)CRUCIAL THAT LOOK & FEEL OF CLI NOT CHANGE\n(g)LATENCY TOLERANCE\n(b)BUILT CLI PLUGINS BEFORE\nFigure 5: Aggregate results from our user study on CLAI.\ncore CLAIfunctionality such as activating a skill takes about 0.2 seconds. On the other hand, using\nCLAIwith installed skills increases the latency to about 2 seconds, with the skills contributing about\n80% to the latency. More details are presented in the appendix.\nIt is clear from the proﬁling results that the onus of making CLAImore responsive rests on the skills.\nFigure 2 shows how the latency varies with increasing numbers of skills. Since each skill is executed\nin parallel threads, there is little change in the user (client) time trend. This is encouraging, since it\nsuggests that the client side, with limited ability to scale its compute, shows near-constant latency.\nIn most cases where the computation happens on the client side, skill developers cannot rely on the\navailability of specialized hardware to accelerate their skills, and instead would need to ensure faster\nskill inference procedures on standard user machines. Speciﬁcally for deep neural network based\nskills, faster inference methods on CPUs have been proposed [22, 41, 46, 47], and CLAIprovides a\npractical test bed for further avenues of research into these areas.\nUser Evaluations We report on user feedback on CLAI from an internal survey based on 235\nresponses. Figure 5(a) offers an aggregate proﬁle of the survey respondents. More than three-\nquarters of the responses came from respondents who identiﬁed as either developers or devops, while\nonly around 14% of the respondents identiﬁed as AI practitioners. This indicates the potential for\nCLAIto positively impact communities that have hitherto not had too much interaction with state-of-\nthe-art AI techniques and technologies. Figure 5(c) zooms in on the respondents’AI background.\nRoughly over half of the respondents had some past AI experience. Of these, Machine Learning\nwas the top area, with Natural Language (Processing) a close second. Figure 5(e) shows that nearly\n4 out of 5 respondents report usage of cloud platforms – this validates one of the unique value\npropositions of CLAI, viz. offering instantaneous and on-premise support for new adopters of cloud\nplatforms. We also report on adoption tolerance by measuring and reporting interest in using CLAI.\nThis was done via a variety of questions, most speciﬁcally represented in Figure 5(f) and Figure 5(g).\nThe former shows that users do not want overt changes to the CLI that they know and love. The\nlatter – Figure 5(g) – talks to users’ patience with processing time and latency in general. Users are\nnot willing to tolerate latencies of more than 3 seconds; however, there is a sizeable contingent of\nrespondents who are happy to trade-off some latency for AI-enabled assistance.\n5 Looking Forward\nProject CLAIwas open sourced a few months ago at AAAI 2020, to widespread interest in the AI\ncommunity and in popular media (for full details, please see the CLAI wiki: github.com/ibm/clai/wiki).\nIt already has 180+ stars and the ﬁrst few open-source contributors. One of the most proliﬁc open-\nsource contributors is a co-author of this whitepaper.\nThe NLC2CMDCompetition at NeurIPS 2020. One of the immediate impacts of ProjectCLAIis the\norganization of competitions around the key user interaction patterns discussed in this paper. Primary\namong them is NLC2CMD @ NeurIPS 2020 (ibm.biz/nlc2cmd) which revolves around translating\nnatural language descriptions of command line tasks to their correct Bash syntax. Other competitions,\nsuch as around the automation use case of observing and learning from user activity on the command\nline – centered around AI planning and reinforcement learning techniques – are planned.\n8\nCode / Data Share Notice\nIn support of democratizing access to the command line, we provide links to the CLAIsystem, CLAI\ncodebase, and data used to report the internal evaluation. These materials are provided for the\ncommunity to reproduce and build on top of our work without any hindrances or hassles.\nHome Project CLAIhome: clai-home.mybluemix.net/\nCode The code is open-source and is available at: github.com/ibm/clai.\nData Aggregate anonymized data from the user study is available at: ibm.biz/bb-survey-results.\nThe link is interactive: you can use this service to explore in further detail the differentiated results\nacross different participant subgroups.\n9\nAppendix\nProject CLAI: Instrumenting the Command Line as a New Environment for AI Agents\nThe following supplementary material provides screenshots of all the user interaction patterns and\nCLAI skills described in Section 3 in the paper. The material also includes more details on the\nproﬁling results, and responses from participants in the user study presented in Section 4.\n1 CLAI Skills in Action\nFigure 6: Screenshot of the CLAI-enabled command line showing available active skills.\nFigure 7: Screencast of a CLAI-enabled command line illustrating how normal life on Bash is\npreserved unless the user invokes a skill explicitly, or there is an event initiated by either the user\nor the terminal itself where a CLAIskill is conﬁdent about interjecting in. Preserving the look and\nfeel of Bash – a tool near and dear to the developer community – is a crucial design choice we made\nbased on the responses from the user study in Section 4; as well as lessons in the HCI community\nlearned from previous deployments [9] of assistants for operating systems.\n10\n1.1 nlc2cmd\nFigure 8: nlc2cmdskill in action.\nInteraction Pattern On-premise support, natural language support\nSupporting Technology Natural Language Processing\n1.2 tellina\nFigure 9: tellinaskill in action.\nInteraction Pattern On-premise support, natural language support\nSupporting Technology Natural Language Processing\nWhile nlc2cmd(Fig. 8) demonstrates more accurate (higher inter-utility coverage) translation from\nEnglish for speciﬁc commands (e.g. common bash utilities like tarand grepthat trouble users the\nmost [25]; or platform speciﬁc commands like cloud services and mainframes), tellina(Fig. 9) is\nthe state of the art in general purpose English to command translation, adopted from [25].\n11\n1.3 ﬁxit\nFigure 10: fixit(demonstrating integration to Bash plugins like github.com/nvbn/thefuck)\nInteraction Pattern On-premise troubleshooting, proactive support\nSupporting Technology Natural Language Processing, Automation\n1.4 cloudbot\nFigure 11: cloudbotautomation of the complex and lengthy deployment pipeline for a containerized\napplication on to a Kubernetes platform. Note that the user answers appear as “no” so as not to\ncomplete the deployment in the course of taking screenshots.\nInteraction Pattern Automation, On-premise support\nSupporting Technology Planning\n12\n1.5 man page explorer\nFigure 12: man page explorer(demonstrating natural language support for Q&A functionality on\nthe command line along with integration to Bash plugins like https://tldr.sh/).\nInteraction Pattern On-premise support, natural language support, augmentation\nSupporting Technology Natural Language Processing, Q&A and Summarization\nThis skill, along with howdoiand helpmedescribed next, are also examples where the user uses the\nexplain functionality described in the user controls in Section 2.2 to gather more information on the\ncommand suggested by CLAI.\n13\n1.6 CLAIhowdoi\nFigure 13: CLAI howdoiskill in action.\nInteraction Pattern On-premise support, natural language support, augmentation\nSupporting Technology Natural Language Processing, Information Retrieval\n1.7 CLAIhelpme\nFigure 14: CLAI helpmeskill in action.\nInteraction Pattern On-premise troubleshooting, proactive support, augmentation\nSupporting Technology Information Retrieval\n14\n1.8 dataXplore\nFigure 15: dataXploreexample of summarize after head function.\nFigure 16: dataXploreexample of plot after head function.\nInteraction Pattern Augmentation\nSupporting Technology Data Analysis\n15\n1.9 gitbot\nFigure 17: gitbotexample of local nlc2cmduse case using Rasa [31] and remote GitHub operations\nusing GitHub Actions Developer API v3 [16].\nInteraction Pattern Natural language support, automation\nSupporting Technology Natural Language Processing\n16\n2 CLAI Orchestration in action\nAll the above examples use the max-orchestrationpattern by default, as described in Section 3.2\n– notice how the user needed to invoke CLAIby force due to lower conﬁdence of tellinain Figure\n9, as per user controls introduced Section 2.2. The following highlight the use of contextual bandits\nthat can model orchestration patterns speciﬁc to users and user types and can adapt over time with\ncontinuous interaction, as described in Section 3.2.\n2.1 Adaptive orchestration using contextual bandits\nThe contextual bandit based orchestrator continues to learn and adapt its behavior depending on the\nfeedback it receives from the user for each skill choice. However, besides this online learning, these\nbandit orchestrators can also be warm-started with a particular behavior proﬁle to bypass the initial\nexploration phase of reinforcement learning algorithms. We demonstrate four different warm-start\nbehavior patterns in ﬁgures 18, 19, 20, and 21.\nFigure 18: “Ignore CLAI” orchestration behavior: the orchestrator here ignores any CLAI skill\nresponse and treats each user command as a normal bash command. Notice how commands that\nusually elicit response from different skills (from previous snapshots) have been ignored and treated\nas native bash commands in the screenshot. This proﬁle can be used to ensure that normal bash\ncommands are not hindered by CLAIif the user has shown preference against it over time.\n17\nFigure 19: “Ignore nlc2cmd” skill orchestration behavior: the orchestrator here ignores response\nfrom the nlc2cmd skill and behaves as a max-orchestrator otherwise. Invocation 3 regarding\ncompressing a directory would have elicited a response from the nlc2cmdskill, but is ignored in this\nwarm-start proﬁle. This proﬁle thus models individual preferences towards speciﬁc skills.\nFigure 20: “Maximum conﬁdence” orchestration behavior: the orchestrator here warm-starts with the\nbehavior of a maximum conﬁdence orchestrator, i.e., it selects the skill with the maximum conﬁdence\nvalue. Note how the ﬁrst invocaation is directed for the man page explorerskill but is responded\nto by the nlc2cmdskill because it has a higher conﬁdence. This is a good default start behavior and\nthe orchestrator can learn to adapt its behavior with the user feedback.\n18\nFigure 21: “Prefer man page explorerskill over nlc2cmd” orchestration behavior. The orchestra-\ntor here prefers the man page explorerskill over nlc2cmdand behaves as a maximum conﬁdence\norchestrator otherwise. Note that for the ﬁrst invocation, a maximum conﬁdence orchestrator would’ve\nchosen the nlc2cmdskill but since we prefer the man page explorerskill over the nlc2cmd, re-\nsponse from the man page exploreris chosen. This is especially useful to model user types: e.g.\na user on Mac may not ﬁnd responses from Unix Stack Exchange useful and thus the bandit can learn\nto prefer answers from man pages instead.\n3 CLAI System Footprint / Detailed Proﬁling Results\nFigure 22: Latency proﬁle of the command clai skills. This invokes only the CLAIcore and thus\nprovides a latency proﬁle of CLAIwithout the overhead of the individual skills.\n19\nFigure 23: Latency proﬁle of activating a CLAI skill. This activates the CLAI core along with the\nactivation procedure of the speciﬁed skill, thus increasing the latency but still keeping it well below\nthe preferred sub-second mark.\nFigure 24: Latency proﬁle of executing a bash command with installed clai skills. This activates\nthe CLAIcore along with CLAIskills. While the latency increases signiﬁcantly to 2 seconds, around\n80% of the latency is contributed by the skill itself.\n4 CLAI User Study\nIn this section, we present an elaborated report of user feedback on CLAI. These results are based\non an internal survey of 235 respondents4. Figure 25 provides an overall snapshot of the user study\nresults, and is part of our main submission. In this section, we elucidate the details of the various\nsub-graphs within this ﬁgure, and analyze the aggregate responses. This section thus supplements the\nresults presented in our “Internal Evaluations” section. We ﬁrst detail the mechanism via which the\nsurvey was constructed and conducted; and then examine the results in detail.\n4An important note here that applies to our entire study is that although these results are based on 235\nsubmitted responses, many of the questions allowed for the selection of multiple options – hence some of the\naggregate numbers are greater than the overall number of respondents.\n20\n(a)SURVEY RESPONDENT PROFILE\n(c)RESPONDENT PROFILE BY AI EXPERIENCE\n(d)COMFORTABLE ACTING ON BB SUGGESTIONS\n(e)USE CLOUD PLATFORMS FOR WORK\n (h)WHY DO YOU USE CLIs?\n(f)CRUCIAL THAT LOOK & FEEL OF CLI NOT CHANGE\n(g)LATENCY TOLERANCE\n(b)BUILT CLI PLUGINS BEFORE\nFigure 25: Combined CLAIuser study results, based on 235 responses.\n4.1 Construction of User Survey\nWe ﬁrst detail the construction, administration, and deployment of the user survey. The survey was\nﬁrst storyboarded and assembled for coverage of all aspects of CLAIthat we wanted to measure and\nreport on. Subsequent to this, the survey was deployed via the forms tool of a major multinational5\nsoftware, services, and information technology company. The survey was sent out via email and other\ncommunication channels to a wide cross-section of employees, in the hopes of attracting responses\nfrom a diverse cohort of respondents who use or would have reason to use the command line interface.\nThe survey itself consisted of questions spread out over three main sections: user demographics,\ninterface patterns, and contributor questions. The user demographics section collected information\npertaining to a respondent’s job role; the usage of cloud platforms and command line interfaces\n(CLIs) for their work; and speciﬁc information on the CLIs that they used. The interface patterns\nsection surveyed respondents on the various ways that they could interface with CLAI, and whether\nthey thought that a speciﬁc pattern or feature would be useful to them. It additionally also surveyed\nusers on extenuating factors like the latency introduced by an assistant like CLAI, and how likely they\nwere to use a CLI integrated with CLAI. Finally, the contributor questions section surveyed whether\nthe respondent would be interested in contributing to Project CLAI; which branch of AI respondents\nhad familiarity with (if any); and what new and upcoming features they would like to see in CLAI. In\nthe below, we collate and analyze the responses to some of these questions.\n4.2 Respondent Background & Demographics\nFigure 26: Demographics for user study respondents.\nFigure 26 proﬁles the survey respondents in aggregate. More than three-quarters of the responses\ncame from respondents who identiﬁed as either developers or devops; while only around 14% of\nthe respondents identiﬁed as AI practitioners. This is an important insight, and shows the potential\nfor CLAI to positively impact communities that have hitherto not had too much interaction with\nstate-of-the-art AI techniques and technologies. This is a point that we elaborate on in our statement\non the democratization of compute (included as part of the impact statement with the main paper).\nOne of the main roles that we envision forCLAIis as a vehicle for cutting-edge AI and ML techniques\nand tools to reach CLI users; and to break AI disciplines out of the cycle of validation and ﬁne-tuning\non toy domains and synthetic data (and datasets) alone.\n5Name omitted for double-blind review purposes.\n21\n4.3 AI Background\nFigure 27: User study respondents’ AI background information.\nSince CLAIis intended ﬁrst and foremost as a means to harness the state of the art in AI tools and\ntechniques towards assistance on the command line, we naturally logged the AI background of the\nsurvey respondents. Figure 27 zooms in on the respondents’ AI interests. Roughly over half of the\nresponses indicated some experience with AI areas. Of these, Machine Learning was the top area;\nwith Natural Language (Processing) a close second. These two results are expected, and follow the\ngeneral trend in the AI world today, particularly when it comes to the non-research population. These\nalso informed our choice of deploying the tellinaskill (c.f. Section 1), which combines the latest\ntechnologies in deep learning and natural language processing. However, it is also interesting to note\nthat there is still a reasonable population of respondents who are familiar withclassical AI disciplines\nsuch as Decision Making, Logic & Reasoning, and Reinforcement Learning. This wide distribution\nacross AI topics is heartening, as it indicates the potential for new skills that can address unique CLI\nuse-cases and become part of CLAIin the future.\n4.4 Journey to Cloud\nFigure 28: User study respondents’ reasons for using cloud platforms.\nIn the introduction to this paper, we presented an argument for why the command line interface was\nmaking a resurgence; and why we are now at an inﬂection point that is akin to the initial emergence\nof large-scale networked terminals. Figure 29(b) attests to this: an overwhelming majority of our\nrespondents – 4 out of every 5 – report having to use cloud platforms for work. The reasons that\nrespondents use cloud-based platforms are outlined in Figure 28: the major reasons mentioned by\nthe respondents are all things that the CLI is known for. CLAIis thus able to fulﬁl its unique value\nproposition of offering instantaneous and on-premise support for (new) adopters of cloud platforms.\nOne of the ultimate goals of CLAI– reﬂected in our discussion on the various skills (c.f. Section 1)\n– is to ease users’journey to the cloud by making available various AI skills that are tuned toward\nspeciﬁc use-cases; while always preserving the speed, power, and ease-of-use of the CLI.\n4.5 Adoption Tolerance\nAs with the introduction of any new technology, we measure and report the tolerance and appetite of\nthe end-user for the tool being offered. This is particularly important in the context of smart assistants\n22\n(a)COMFORTABLE ACTING ON CLAI SUGGESTIONS\n(b)USE CLOUD PLATFORMS FOR WORK\n(c)CRUCIAL THAT LOOK & FEEL OF CLI NOT CHANGE\n(d)LATENCY TOLERANCE\nFigure 29: Results of users’ responses on: (a) Willingness to act on CLAI’s suggestions; (b) Usage of\ncloud platforms for work; (c) Importance of look and feel of the CLI not changing; and (d) Latency\ntolerance.\nand assistive agents, which have had an unfortunate and infamous history of falling well short of user\nexpectations [9]. We measured the adoption tolerance of CLAIvia three questions, whose results are\noutlined in Figure 29. The ﬁrst – Figure 29(a) – reports on users’ willingness to act on suggestions\nfrom CLAI. A signiﬁcant majority of users feel comfortable acting on CLAI’s suggestions, which\nbodes well for adoption. Figure 29(c) talks to the tendency of developers and other power-users\nto not want overt changes to the CLI that they know and love: indeed, a majority of respondents\nare disinclined to such changes. This validates our decision to provide the AI skills in CLAI via\nthe command line, as shown in the screen captures in Section 1. Finally, we also measure users’\ntolerance to latency and processing times. Figure 29(d) shows that users are not willing to tolerate\nlatencies of more than 3 seconds; however, a sizeable contingent are happy to trade-off some increase\nin lantecy for the AI features provided by CLAI. This validates our decisions and analyses while\nproﬁling CLAI’s system footprint (c.f. Section 3).\n4.6 Feature Usefulness\nTHIS FEATURE WOULD BE USEFUL FOR YOUR COMMAND LINE\n(a)AUTOMATION(b)IN-SITU TROUBLESHOOTING & SUPPORT(c)PROACTIVE SUPPORT(d)NATURAL LANGUAGE SUPPORT(e)PEDAGOGY\nFigure 30: User study results for CLAIfeatures that users ﬁnd useful on the CLI.\nWe also quizzed survey respondents on the various user interaction patterns and skills described in\nSection 2.2 of the paper – these results are collated in Figure 30. There are some interesting trends.\nFirst, for the Automation pattern – Figure 30(a) – many users expressed a neutralresponse. This\nmay be attributed to the fact that automation of entire processes is the most complex assistance\npossible; and thus users are likely to be most wary of this. Users also expressed very little negative\nsentiment towards the in-situ troubleshooting & support (Figure 30(b)) and pedagogy (Figure 30(e))\npatterns. The former can be explained by the fact that this is currently the killer app that most\nusers of the command line are waiting for: users do not want to interrupt their task to go look for\n23\nsolutions. The latter can be construed as an aspirational goal, where users are looking forward to a\ntrue command line assistant that can make them better and more efﬁcient power-users with sustained\nusage. Finally, both the proactive support (Figure 30(c)) and natural language support (Figure 30(d))\nuse-cases had high positive sentiment, coupled with a bit of user anxiety about how these patterns\nwould be implemented as skills. We take these results to indicate that user adoption of these patterns\ncomes down to their speciﬁc implementation as skills.\n4.7 Using & Extending CLAI\n(a)LIKELY TO USE CLI INTEGRATED WITH CLAI\n(b)BUILT PLUGINS FOR CLI BEFORE(c)WILL BUILD PLUGINS FOR CLAI\nFigure 31: User study results on usability of CLAI.\nFinally, we surveyed users on whether they would use a version of the CLI integrated withCLAI–\nthese results are aggregated in Figure 31(a). The vast majority of users report that they are likely\nto use CLAI. Furthermore, we present Figure 31(b) and Figure 31(c), which respectively show the\nbreakdown by role of respondents who have built plugins for the CLI previously, and those who\nwould build new AI-based plugins for CLAI. It is particularly informative to note the difference in\nnumbers between the developer and researcher job roles.\n24\nReferences\n[1] Alex Clark and Contributors. Pillow Library, 2010-Present. https://pillow.readthedocs.io/en/\nstable/index.html, 2020.\n[2] API Evangelist. What Is Behind The CLI Making A Comeback? https://apievangelist.com/\n2019/11/05/what-is-behind-the-cli-making-a-comeback/, 2019.\n[3] Ask Ubuntu. Unanswered questions – what to do? https://meta.askubuntu.com/a/8575, 2014.\n[4] Ben Dickson. Artiﬁcial intelligence: Does another huge language model prove anything? https:\n//bdtechtalks.com/2020/02/03/google-meena-chatbot-ai-language-model/, 2020. TechTalks.\n[5] Nate Blaylock and James F Allen. Statistical Goal Parameter Recognition. In ICAPS, 2004.\n[6] Satchuthananthavale RK Branavan, Harr Chen, Luke S Zettlemoyer, and Regina Barzilay.\nReinforcement Learning for Mapping Instructions to Actions. In ACL/AFNLP, 2009.\n[7] Eric A Brewer. Kubernetes and the Path to Cloud Native. In ACM Symposium on Cloud\nComputing, 2015.\n[8] Tathagata Chakraborti, Kartik Talamadupula, Kshitij P Fadnis, Murray Campbell, and Sub-\nbarao Kambhampati. UbuntuWorld 1.0 LTS – A Platform for Automated Problem Solving &\nTroubleshooting in the Ubuntu OS. In IAAI/AAAI, 2017.\n[9] Alan Cooper. The Inmates are Running the Asylum: Why High-tech Products Drive Us Crazy\nand How to Restore the Sanity. Sams Indianapolis, 2004.\n[10] Dan Luu. The Growth of Command Line Options, 1979-Present. https://danluu.com/\ncli-complexity/, 2020.\n[11] DevSpace. DevSpace Cloud – The Dev Platform for Cloud-Native Teams. https://devspace.\ncloud/, 2018-Present.\n[12] Oren Etzioni and Neal Lesh. Planning with Incomplete Information in the UNIX Domain.\nIn Working Notes of the AAAI Spring Symposium: Foundations of Automatic Planning: The\nClassical Approach and Beyond, 1993.\n[13] Oren Etzioni and Daniel Weld. A Softbot-Based Interface to the Internet. Communications of\nthe ACM, 1994.\n[14] Fred Wilson. The Return of the Command Line Interface. https://avc.com/2015/09/\nthe-return-of-the-command-line-interface/, 2015. A VC Blog.\n[15] Fredrik Lundh and Contributors. PIL Library, 2008-Present. https://www.pythonware.com/\nproducts/pil/, 2020.\n[16] GitHub. GitHub Developer REST API v3. https://developer.github.com/v3/, Present.\n[17] Clinton Gormley and Zachary Tong. Elasticsearch: The Deﬁnitive Guide: A Distributed\nReal-Time Search and Analytics Engine. O’Reilly Media, Inc., 2015.\n[18] David Gros. AInix: An Open Platform for Natural Language Interfaces to Shell Commands.\nUndergraduate Honors Thesis, 2019. University of Texas, Austin.\n[19] Malte Helmert. The Fast Downward Planning System. Journal of Artiﬁcial Intelligence\nResearch, 2006.\n[20] J. D. Hunter and Contributors. Matplotlib: A 2D Graphics Environment. In Computing in\nScience & Engineering, 2007.\n[21] Jeff Pickhardt. Betty. https://github.com/pickhardt/betty, 2014-Present. GitHub.\n[22] Nicholas D Lane, Sourav Bhattacharya, Petko Georgiev, Claudio Forlivesi, Lei Jiao, Lorena\nQendro, and Fahim Kawsar. Deepx: A Software Accelerator for Low-Power Deep Learning\nInference on Mobile Devices. In International Conference on Information Processing in Sensor\nNetworks (IPSN), 2016.\n[23] Hao Li, Yu-Ping Wang, Jie Yin, and Gang Tan. SmartShell: Automated Shell Scripts Synthe-\nsis from Natural Language. International Journal of Software Engineering and Knowledge\nEngineering, 2019.\n[24] Xi Victoria Lin, Chenglong Wang, Deric Pang, Kevin Vu, and Michael D Ernst. Program\nSynthesis from Natural Language using Recurrent Neural Networks. University of Washington,\nTechnical Report, 2017.\n25\n[25] Xi Victoria Lin, Chenglong Wang, Luke Zettlemoyer, and Michael D Ernst. Nl2bash: A Corpus\nand Semantic Parser for Natural Language Interface to the Linux Operating System. Language\nResources and Evaluation Conference (LREC), 2018.\n[26] Dirk Merkel. Docker: Lightweight Linux Containers for Consistent Development and Deploy-\nment. Linux Journal, 2014.\n[27] Nate Blaylock. The Linux Plan Corpus. Technical Report, 2010.\n[28] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel,\nP. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, and E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine\nLearning Research, 2011.\n[29] Miguel Ramírez and Hector Geffner. Probabilistic Plan Recognition Using Off-the-shelf\nClassical Planners. In AAAI, 2010.\n[30] Juan Ramos et al. Using TF-IDF to Determine Word Relevance in Document Queries. In\nProceedings of the First Instructional Conference on Machine Learning, 2003.\n[31] Rasa NLU. Language Understanding for Chatbots and AI assistants. https://rasa.com/docs/rasa/\nnlu/about/, Present.\n[32] Red Hat. odo: OpenShift CLI for Developers. https://developers.redhat.com/products/odo/\noverview, Present. Red Hat Developer.\n[33] Yara Rizk, Abhishek Bhandwalder, Scott Boag, Tathagata Chakraborti, Vatche Isahagian,\nYasaman Khazaeni, Falk Pollock, and Merve Unuvar. A Uniﬁed Conversational Assistant\nFramework for Business Process Automation. In AAAI Workshop on Intelligent Process\nAutomation (IPA), 2020.\n[34] Stuart J Russell and Peter Norvig. Artiﬁcial Intelligence: A Modern Approach . Pearson\nEducation Limited, 1995.\n[35] Simon Bisson. Good news for developers: The CLI is back. https://www.zdnet.com/article/\ngood-news-for-developers-the-cli-is-back/, 2019. ZDNet.\n[36] Stack Exchange. Unix & Linux. https://unix.stackexchange.com/, Present.\n[37] Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and Policy Considerations for\nDeep Learning in NLP. ACL, 2019.\n[38] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence Learning with Neural\nNetworks. In Advances in Neural Information Processing Systems, 2014.\n[39] Richard S Sutton and Andrew G Barto. Introduction to Reinforcement Learning. MIT Press\nCambridge, 1998.\n[40] Sümer Cip. Yet Another Python Proﬁler. https://github.com/sumerc/yappi, 2011-Present.\nGitHub.\n[41] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. Branchynet: Fast inference\nvia early exiting from deep neural networks. In ICPR, 2016.\n[42] Tom Warren. Apple replaces bash with zsh as the default shell in macOS Catalina. https://www.\ntheverge.com/2019/6/4/18651872/apple-macos-catalina-zsh-bash-shell-replacement-features,\n2019. The Verge.\n[43] Sohini Upadhyay, Mayank Agarwal, Djallel Bounneffouf, and Yasaman Khazaeni. A Bandit\nApproach to Posterior Dialog Orchestration Under a Budget. NeurIPS Conversational AI\nWorkshop, 2019.\n[44] Wes McKinney. Pandas Describe Function, 2008-Present. https://pandas.pydata.org/\npandas-docs/stable/reference/api/pandas.DataFrame.describe.html, 2020.\n[45] Wes McKinney. Pandas Library, 2008-Present. https://pandas.pydata.org/pandas-docs/stable/\nindex.html, 2020.\n[46] Heiga Zen, Yannis Agiomyrgiannakis, Niels Egberts, Fergus Henderson, and Przemysław\nSzczepaniak. Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric\nSpeech Synthesizers for Mobile Devices. arXiv:1606.06061, 2016.\n[47] Minjia Zhang, Samyam Rajbhandari, Wenhan Wang, and Yuxiong He. DeepCPU: Serving\nRNN-Based Deep Learning Models 10x Faster. In USENIX Annual Technical Conference\n(USENIX ATC), 2018.\n26",
  "topic": "Line (geometry)",
  "concepts": [
    {
      "name": "Line (geometry)",
      "score": 0.4501304030418396
    },
    {
      "name": "Computer science",
      "score": 0.4259386658668518
    },
    {
      "name": "Mathematics",
      "score": 0.06948456168174744
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": []
}