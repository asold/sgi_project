{
  "title": "TMbed: transmembrane proteins predicted through language model embeddings",
  "url": "https://openalex.org/W4290546426",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1132376170",
      "name": "Michael Bernhofer",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2001094594",
      "name": "Burkhard Rost",
      "affiliations": [
        "Technical University of Munich",
        "Institute for Advanced Study"
      ]
    },
    {
      "id": "https://openalex.org/A1132376170",
      "name": "Michael Bernhofer",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A2001094594",
      "name": "Burkhard Rost",
      "affiliations": [
        "Technical University of Munich",
        "Institute for Advanced Study"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2057013716",
    "https://openalex.org/W2159136327",
    "https://openalex.org/W2169317607",
    "https://openalex.org/W1968682237",
    "https://openalex.org/W2137827030",
    "https://openalex.org/W1995808589",
    "https://openalex.org/W2412885026",
    "https://openalex.org/W2534377269",
    "https://openalex.org/W2901361432",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4205620281",
    "https://openalex.org/W4281903464",
    "https://openalex.org/W4205172550",
    "https://openalex.org/W3108574850",
    "https://openalex.org/W2292314415",
    "https://openalex.org/W1786434256",
    "https://openalex.org/W2150853746",
    "https://openalex.org/W1939505221",
    "https://openalex.org/W3212761619",
    "https://openalex.org/W2886471703",
    "https://openalex.org/W2151156268",
    "https://openalex.org/W2164025376",
    "https://openalex.org/W2158623906",
    "https://openalex.org/W2510407459",
    "https://openalex.org/W2190900687",
    "https://openalex.org/W2149407349",
    "https://openalex.org/W2517405041",
    "https://openalex.org/W1938173378",
    "https://openalex.org/W1501531009",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W2995514860",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W3040739508",
    "https://openalex.org/W3144701084",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3156428263",
    "https://openalex.org/W3170504813",
    "https://openalex.org/W4225438928",
    "https://openalex.org/W3118936575",
    "https://openalex.org/W3199468887",
    "https://openalex.org/W4200552574",
    "https://openalex.org/W3191896067",
    "https://openalex.org/W4223551334",
    "https://openalex.org/W2140831051",
    "https://openalex.org/W3112376646",
    "https://openalex.org/W2900674118",
    "https://openalex.org/W2170463736",
    "https://openalex.org/W2105537627",
    "https://openalex.org/W2129838159",
    "https://openalex.org/W2056402029",
    "https://openalex.org/W4205192056",
    "https://openalex.org/W2170747616",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W2951149542",
    "https://openalex.org/W1967513793",
    "https://openalex.org/W2137670821",
    "https://openalex.org/W2110425545",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W1515959604",
    "https://openalex.org/W2029653438",
    "https://openalex.org/W2145020089",
    "https://openalex.org/W3210500140",
    "https://openalex.org/W2095467724",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W3211795435",
    "https://openalex.org/W3183475563",
    "https://openalex.org/W3163595068",
    "https://openalex.org/W3159719254",
    "https://openalex.org/W1974789047",
    "https://openalex.org/W6888788878",
    "https://openalex.org/W4282562958",
    "https://openalex.org/W4280577818",
    "https://openalex.org/W2022428986",
    "https://openalex.org/W2035784781",
    "https://openalex.org/W3163970098",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W4225868104",
    "https://openalex.org/W4206950245",
    "https://openalex.org/W4306879641",
    "https://openalex.org/W4282984452",
    "https://openalex.org/W2898210859",
    "https://openalex.org/W4285088579",
    "https://openalex.org/W4319593844",
    "https://openalex.org/W3177500196"
  ],
  "abstract": null,
  "full_text": "TMbed: transmembrane proteins predicted \nthrough language model embeddings\nMichael Bernhofer1,2* and Burkhard Rost1,3,4 \nBackground\nStructural knowledge of TMPs 4–5 fold underrepresented\nTransmembrane proteins (TMP) account for 20–30% of all proteins within any organ -\nism [1, 2]; most TMPs cross the membrane with transmembrane helices (TMH). TMPs \ncrossing with transmembrane beta strands (TMB), forming beta barrels, have been esti -\nmated to account for 1–2% of all proteins in Gram-negative bacteria; this variety is also \nAbstract \nBackground: Despite the immense importance of transmembrane proteins (TMP) for \nmolecular biology and medicine, experimental 3D structures for TMPs remain about \n4–5 times underrepresented compared to non-TMPs. Today’s top methods such as \nAlphaFold2 accurately predict 3D structures for many TMPs, but annotating transmem-\nbrane regions remains a limiting step for proteome-wide predictions.\nResults: Here, we present TMbed, a novel method inputting embeddings from \nprotein Language Models (pLMs, here ProtT5), to predict for each residue one of four \nclasses: transmembrane helix (TMH), transmembrane strand (TMB), signal peptide, or \nother. TMbed completes predictions for entire proteomes within hours on a single con-\nsumer-grade desktop machine at performance levels similar or better than methods, \nwhich are using evolutionary information from multiple sequence alignments (MSAs) \nof protein families. On the per-protein level, TMbed correctly identified 94 ± 8% of \nthe beta barrel TMPs (53 of 57) and 98 ± 1% of the alpha helical TMPs (557 of 571) in a \nnon-redundant data set, at false positive rates well below 1% (erred on 30 of 5654 non-\nmembrane proteins). On the per-segment level, TMbed correctly placed, on average, 9 \nof 10 transmembrane segments within five residues of the experimental observation. \nOur method can handle sequences of up to 4200 residues on standard graphics cards \nused in desktop PCs (e.g., NVIDIA GeForce RTX 3060).\nConclusions: Based on embeddings from pLMs and two novel filters (Gaussian and \nViterbi), TMbed predicts alpha helical and beta barrel TMPs at least as accurately as any \nother method but at lower false positive rates. Given the few false positives and its out-\nstanding speed, TMbed might be ideal to sieve through millions of 3D structures soon \nto be predicted, e.g., by AlphaFold2.\nKeywords: Protein language models, Protein structure prediction, Transmembrane \nprotein prediction\nOpen Access\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326  \nhttps://doi.org/10.1186/s12859-022-04873-x\nBMC Bioinformatics\n*Correspondence:   \nbernhoferm@rostlab.org\n1 Department of Informatics, \nBioinformatics \nand Computational Biology - i12, \nTechnical University of Munich \n(TUM), Boltzmannstr. 3, \n85748 Garching, Germany\n2 TUM Graduate School, Center \nof Doctoral Studies in Informatics \nand its Applications \n(CeDoSIA), Boltzmannstr. 11, \n85748 Garching, Germany\n3 Institute for Advanced Study \n(TUM-IAS), Lichtenbergstr. 2a, \n85748 Garching, Germany\n4 TUM School of Life Sciences \nWeihenstephan (TUM-WZW), \nAlte Akademie 8, Freising, \nGermany\nPage 2 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \npresent in mitochondria and chloroplasts [3]. Membrane proteins facilitate many essen -\ntial processes, including regulation, signaling, and transportation, rendering them tar -\ngets for most known drugs [4, 5]. Despite this immense relevance for molecular biology \nand medicine, only about 5% of all three-dimensional (3D) structures in the PDB [6, 7] \nconstitute TMPs [8–10].\nAccurate 3D predictions available for proteomes need classification\nThe prediction of protein structure from sequence leaped in quality through AlphaFold2 \n[11], Nature’s method of the year 2021 [12]. Although AlphaFold2 appears to provide \naccurate predictions for only very few novel “folds” , it importantly increases the width \nof structural coverage [13]. AlphaFold2 seems to work well on TMPs [14], but for pro -\nteome-wide high-throughput studies, we still need to filter out membrane proteins from \nthe structure predictions. Most state-of-the-art (SOTA) TMP prediction methods rely \non evolutionary information in the form of multiple sequence alignments (MSA) to \nachieve their top performance. In our tests we included 13 such methods, namely BetA -\nware-Deep [15], BOCTOPUS2 [16], CCTOP [17, 18], HMM-TM [19–21], OCTOPUS \n[22], Philius [23], PolyPhobius [24], PRED-TMBB2 [20, 21, 25], PROFtmb [3], SCAMPI2 \n[26], SPOCTOPUS [27], TMSEG [28], and TOPCONS2 [29].\npLMs capture crucial information without MSAs\nMimicking recent advances of Language Models (LM) in natural language processing \n(NLP), protein Language Models (pLMs) learn to reconstruct masked parts of protein \nsequences based on the unmasked local and global information [30–37]. Such pLMs, \ntrained on billions of protein sequences, implicitly extract important information about \nprotein structure and function, essentially capturing aspects of the “language of life” \n[32]. These aspects can be extracted from the last layers of the deep learning networks \ninto vectors, referred to as embeddings, and used as exclusive input to subsequent meth-\nods trained in supervised fashion to successfully predict aspects of protein structure and \nfunction [30–34, 36, 38–43]. Often pLM-based methods outperform SOTA methods, \nwhich are using evolutionary information on top, and they usually require substantially \nfewer compute resources. Just before submitting this work, we became aware of another \npLM-based TM-prediction method, namely DeepTMHMM [44] using ESM-1b [36] \nembeddings, and included it in our comparisons.\nHere, we combined embeddings generated by the ProtT5 [34] pLM with a simple con -\nvolutional neural network (CNN) to create a fast and highly accurate prediction method \nfor alpha helical and beta barrel transmembrane proteins and their overall inside/outside \ntopology. Our new method, TMbed, predicted the presence and location of any TMBs, \nTMHs, and signal peptides for all proteins of the human proteome within 46 min on our \nserver machine (Additional file 1: Table S1) at the same or better level of performance as \nother methods, which require substantially more time.\nMaterials and methods\nData set: membrane proteins (TMPs)\nWe collected all primary structure files for alpha helical and beta barrel transmembrane \nproteins (TMP) from OPM [45] and mapped their PDB [6, 7] chain identifiers (PDB-id) \nPage 3 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nto UniProtKB [46] through SIFTS [47, 48]. Toward this end, we discarded all chimeric \nchains, all models, and all chains for which OPM failed to map any transmembrane start \nor end position. This resulted in 2,053 and 206 sequence-unique PDB chains for alpha \nhelical and beta barrel TMPs, respectively.\nWe used the ATOM coordinates inside the OPM files to assign the inside/outside ori -\nentation of sequence segments not within the membrane. We manually inspected incon-\nsistent annotations (e.g., if both ends of a transmembrane segment had the same inside/\noutside orientation) and cross-referenced them with PDBTM [49–51], PDB, and Uni -\nProtKB. We then either corrected such inconsistent annotations or discarded the whole \nsequence. As OPM does not include signal peptide annotations, we compared our TMP \ndata sets to the set used by SignalP 6.0 [52] and all sequences in UniProtKB/Swiss-Prot \nwith experimentally annotated signal peptides using CD-HIT [53, 54]. For any matches \nwith at least 95% global sequence identity (PIDE), we transferred the signal peptide \nannotation onto our TMPs. We removed all sequences with fewer than 50 residues to \navoid noise from incorrect sequencing fragments, and all sequences with over 15,000 \nresidues to save energy (lower computational costs).\nFinally, we removed redundant sequences from the two TMP data sets by clustering \nthem with MMseqs2 [55] to at most 20% local pairwise sequence identity (PIDE) with \n40% minimum alignment coverage, i.e., no pair had more than 20% PIDE for any local \nalignment covering at least 40% of the shorter sequence. The final non-redundant TMP \ndata sets contained 593 alpha helical TMPs and 65 beta barrel TMPs, respectively.\nData set: globular non‑membrane proteins\nWe used the SignalP 6.0 (SP6) dataset for our globular proteins. As the SP6 dataset \ncontained only the first 70 residues of each protein, we took the full sequences from \nUniProtKB/Swiss-Prot and transferred the signal peptide annotations. To remove any \npotential membrane proteins from this non-TMP data set, we compared it with CD-\nHIT [53, 54] against three other data sets: (1) our TMP data sets before redundancy \nreduction, (2) all protein sequences from UniProtKB/Swiss-Prot with any annotations \nof transmembrane segments, and (3) all proteins from UniProtKB/Swiss-Prot with any \nsubcellular location annotations for membrane. We removed all proteins from our non-\nTMP data set with more than 60% global PIDE to any protein in sets 1–3. Again, we \ndropped all sequences with less than 50 or more than 15,000 residues and applied the \nsame redundancy reduction as before (20% PIDE at 40% alignment coverage). The final \nnon-redundant data set contained 5,859 globular, water-soluble non-TMP proteins; 698 \nof these have a signal peptide.\nAdditional redundancy reduction\nOne anonymous reviewer spotted homologs in our data set after the application of the \nabove protocol. To address this problem, we performed another iteration of redundancy \nreduction for each of the three data sets using CD-HIT at 20% PIDE. In order to save \nenergy (i.e., avoid retraining our model), we decided to remove clashes for the evalua -\ntion, i.e., if two proteins shared more than 20% PIDE, we removed both from the data \nset (as TMbed was trained on both in the cross-validation protocol). Thereby, this sec -\nond iteration removed 235 proteins: 8 beta barrel TMPs, 22 alpha helical TMPs, and 205 \nPage 4 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \nglobular, non-membrane proteins. Our final test data sets included 57 beta barrel TMPs, \n571 alpha helical TMPs, and 5654 globular, non-membrane proteins.\nMembrane re‑entrant regions\nBesides transmembrane segments that cross the entire membrane, there are also oth -\ners, namely membrane segments that briefly enter and exit the membrane on the same \nside. These are referred to as re-entrant regions [56, 57]. Although rare, some methods \nexplicitly predict them [17, 18, 22, 27, 58]. However, as OPM does not explicitly annotate \nsuch regions and since our data set already had a substantial class imbalance between \nbeta barrel TMPs, alpha helical TMPs and, globular proteins, we decided not to predict \nre-entrant regions.\nEmbeddings\nWe generated embeddings with protein Language Models (pLMs) for our data sets \nusing a transformer-based pLM ProtT5-XL-U50 (short: ProtT5) [34]. We discarded the \ndecoder part of ProtT5, keeping only the encoder for increased efficiency (note: encoder \nembeddings are more informative [34]). The encoder model converts a protein sequence \ninto an embedding matrix that represents each residue in the protein, i.e., each position \nin the sequence, by a 1024-dimensional vector containing global and local contextual -\nized information. We converted the ProtT5 encoder from 32-bit to 16-bit floating-point \nformat to reduce the memory footprint on the GPU. We took the pre-trained ProtT5 \nmodel as is without any further task-specific fine-tuning.\nWe chose ProtT5 over other embedding models, such as ESM-1b [36], based on our \nexperience with the model and comparisons during previous projects [34, 38]. Further-\nmore, ProtT5 does not require splitting long sequences, which might remove valuable \nglobal context information, while ESM-1b can only handle sequences of up to 1022 \nresidues.\nModel architecture\nOur TMbed model architecture contained three modules (Additional file  1: Fig. S1): a \nconvolutional neural network (CNN) to generate per-residue predictions, a Gaussian \nsmoothing filter, and a Viterbi decoder to find the best class label for each residue. We \nimplemented the model in PyTorch [59].\nModule 1: CNN\nThe first component of TMbed is a CNN with four layers (Additional file  1: Fig. S1). \nThe first layer is a pointwise convolution, i.e., a convolution with kernel size of 1, which \nreduces the ProtT5 embeddings for each residue (position in the sequence) from 1024 \nto 64 dimensions. Next, the model applies layer normalization [60] along the sequence \nand feature dimensions, followed by a ReLU (Rectified Linear Unit) activation function \nto introduce non-linearity. The second and third layers consist of two parallel depthwise \nconvolutions; both process the output of the first layer. As depthwise convolutions pro -\ncess each input dimension (feature) independently while considering consecutive res -\nidues, those two layers effectively generate sliding weighted sums for each dimension. \nThe kernel sizes of the second and third layer are 9 and 21, respectively, corresponding \nPage 5 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nto the average length of transmembrane beta strands and helices. As before, the model \nnormalizes the output of both layers and applies the ReLU function. It then concatenates \nthe output of all three layers, constructing a 192-dimensional feature vector for each \nresidue (position in the sequence). The fourth layer is a pointwise convolution combin -\ning the outputs from the previous three layers and generates scores for each of the five \nclasses: transmembrane beta strand (B), transmembrane helix (H), signal peptide (S), \nnon-membrane inside (i), and non-membrane outside (o).\nModule 2: Gaussian filter\nThis module smooths the output from the CNN for adjacent residues (sequence posi -\ntions) to reduce noisy predictions. The filter allows flattening isolated single-residue \npeaks. For instance, peaks extending of only one to three residues for the classes B and \nH are often non-informative; similarly short peaks for class S are unlikely correct. The \nfilter uses a Gaussian distribution with standard deviation of 1 and a kernel size of 7, i.e., \nits seven weights correspond to three standard deviation intervals to the left and right, \nas well as the central peak. A softmax function then converts the filtered class scores to a \nclass probability distribution.\nModule 3: Viterbi decoder\nThe Viterbi algorithm decodes the class probabilities and assigns a class label to each \nresidue (position in the sequence; Additional file 1: Note S3, Fig. S2). The algorithm uses \nno trainable parameter; it scores transitions according to the predicted class probabili -\nties. Its purpose is to enforce a simple grammar such that (1) signal peptides can only \nstart at the N-terminus (first residue in protein), (2) signal peptides and transmembrane \nsegments must be at least five residues long (a reasonable trade-off between filtering out \nfalse positives and still capturing weak signals), and (3) the prediction for the inside/out -\nside orientation has to change after each transmembrane segment (to simulate crossing \nthrough the membrane). Unlike the Gaussian filter, we did not apply the Viterbi decoder \nduring training. This simplified backpropagation and sped up training.\nTraining details\nWe performed a stratified five-fold nested cross-validation for model development \n(Additional file  1: Fig. S3). First, we separated our protein sequences into four groups: \nbeta barrel TMPs, alpha helical TMPs with only a single helix, those with multiple heli -\nces, and non-membrane proteins. We further subdivided each group into proteins with \nand without signal peptides. Next, we randomly and evenly distributed all eight groups \ninto five data sets. As all of our data sets were redundancy reduced, no two splits con -\ntained similar protein sequences for any of the classes. However, similarities between \nproteins of two different classes were allowed, not the least to provide more conservative \nperformance estimates.\nDuring development, we used four of the five splits to create the model and the fifth \nfor testing (Additional file  1: Fig. S3). Of the first four splits, we used three to train the \nmodel and the fourth for validation (optimize hyperparameters). We repeated this 3–1 \nsplit three more times, each time using a different split for the validation set, and calcu -\nlated the average performance for every hyperparameter configuration. Next, we trained \nPage 6 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \na model with the best configuration on all four development splits and estimated its final \nperformance on the independent test split. We performed this whole process a total of \nfive times, each time using a different of the five splits as test data and the remaining four \nfor the development data. This resulted in five final models; each trained, optimized, and \ntested on independent data sets.\nWe applied weight decay to all trained weights of the model and added a dropout layer \nright before the fourth convolutional layer, i.e., the output layer of the CNN. For every \ntraining sample (protein sequence), the dropout layer randomly sets 50% of the features \nto zero across the entire sequence, preventing the model from relying on only a specific \nsubset of features for the prediction.\nWe trained all models for 15 epochs using the AdamW [61] optimizer and cross-\nentropy loss. We set the beta parameters to 0.9 and 0.999, used a batch size of 16 \nsequences, and applied exponential learning rate decay by multiplying the learning \nrate with a factor of 0.8 every epoch. The initial learning rate and weight decay values \nwere part of the hyperparameters optimized during cross-validation (Additional file  1: \nTable S2).\nThe final TMbed model constitutes an ensemble over the five models obtained from \nthe five outer cross-validation iterations (Additional file  1: Fig. S3), i.e., one for each \ntraining/test set combination. During runtime, each model generates its own class prob -\nabilities (CNN, plus Gaussian filter), which are then averaged and processed by the \nViterbi decoder to generate the class labels.\nEvaluation and other methods\nWe evaluated the test performance of TMbed on a per-protein level and on a per-seg -\nment level (Additional file  1: Note S1). For protein level statistics, we calculated recall \nand false positive rate (FPR). We computed those statistics for three protein classes: \nalpha helical TMPs, beta barrel TMPs, and globular proteins.\nWe distinguished correct and incorrect segment predictions using two constraints: (1) \nthe observed and predicted segment must overlap such that the intersection of the two is \nat least half of their union, and (2) neither the start nor the end positions may deviate by \nmore than five residues between the observed and predicted segment (Additional file  1: \nFig. S4). All segments predicted meeting both these criteria were considered as “cor -\nrectly predicted segments” , all others as “incorrectly predicted segments” . This allowed \nfor a reasonable margin of error regarding the position of a predicted segment, while \npunishing any gaps introduced into a segment. For per-segment statistics, we calculated \nrecall and precision. We also computed the percentage of proteins with the correct num-\nber of predicted segments  (Qnum), the percentage of proteins for which all segments are \ncorrectly predicted  (Qok), and the percentage of correctly predicted segments that also \nhave the correct orientation within the membrane  (Qtop). We considered only proteins \nthat actually contain the corresponding type of segment when calculating per-segment \nstatistics, e.g., only beta barrel TMPs for transmembrane beta strand segments.\nWe compared TMbed to other prediction methods for alpha helical and beta barrel \nTMPs (details in Additional file  1: Note S2): BetAware-Deep [15], BOCTOPUS2 [16], \nCCTOP [17, 18], DeepTMHMM [44], HMM-TM [19– 21], OCTOPUS [22], Philius \n[23], PolyPhobius [24], PRED-TMBB2 [20, 21, 25], PROFtmb [3 ], SCAMPI2 [26], \nPage 7 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nSPOCTOPUS [ 27], TMSEG [ 28], and TOPCONS2 [ 29]. We chose those methods \nbased on their good prediction accuracy and public popularity. For methods predict -\ning only either alpha helical or beta barrel TMPs, we considered the corresponding \nother type of TMPs as globular proteins for the per-protein statistics. In addition, we \ngenerated signal peptide predictions with SignalP 6.0 [ 52]. The performance of older \nTMH prediction methods could be triangulated based on previous comprehensive \nestimate of such methods [ 28, 62].\nUnless stated otherwise, all reported performance values constitute the average \nperformance over the five independent test sets during cross-validation (c.f. Train-\ning details ) and their error margins reflect the 95% confidence interval (CI), i.e., 1.96 \ntimes the sample standard error over those five splits (Additional file  1: Tables S5, S6). \nWe considered two values A and B statistically significantly different if they differ by \nmore than their composite 95% confidence interval:\nAdditional out‑of‑distribution benchmark\nIn the most general sense, machine learning models learn and predict distributions. \nMost membrane data sets are small and created using the same resources, including \nOPM [45], PDBTM [49–51], and UniProtKB/Swiss-Prot [46] that often mix experimen -\ntal annotations with sophisticated algorithms [ 50, 63–65] to determine the boundaries \nof transmembrane segments, e.g., by using the 3D structure. Given these constraints, \nwe might expect data sets from different groups to render similar results. Analyzing the \nvalidity of this assumption, we included the data set assembled for the development of \nDeepTMHMM [44]. Three reasons made us chose this set as an alternative perspective: \n(1) it is recent, (2) it contains helical and beta barrel TMPs, and (3) the authors made \ntheir cross-validation predictions available, simplifying comparisons.\nWe created two distinct data sets from the DeepTMHMM data. First, we collected all \nproteins common to both data sets (TMbed and DeepTMHMM). We used those pro -\nteins to estimate how much the annotations within both data sets agree with each other. \nIn total, there were 1788 proteins common to both data sets: 43 beta barrel TMPs, 184 \nalpha helical TMPs, 1,560 globular proteins, and one protein (MSPA_MYCS2; Porin \nMspA) which sits in the outer-membrane of Mycobacterium smegmatis [66]. We clas -\nsified this as beta barrel TMP while DeepTMHMM listed it, most likely incorrectly, as \na globular protein. The second data set that we created contained all proteins from the \nDeepTMHMM data set that were non-redundant to the training data of TMbed. We \nused PSI-BLAST [ 67] to find all significant (e-value  <  10–4) local alignments with a 20% \nPIDE threshold and 40% alignment coverage to remove the redundant sequences. This \nsecond data set contained 667 proteins: 14 beta barrel TMPs, 86 alpha helical TMPs, \nand 567 globular proteins. We generated predictions with TMbed for those proteins and \ncompared them to the cross-validation predictions for DeepTMHMM, as well as the \nbest performing methods from our own benchmark (CCTOP [17, 18], TOPCONS2 [29], \nBOCTOPUS2 [16]); we used the DeepTMHMM data set annotations as ground truth.\n(1)|A − B|> CIc = CI2\nA + CI2\nB\nPage 8 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \nData set of new membrane proteins\nIn order to perform a CASP-like performance evaluation, we gathered all PDB struc -\ntures published since Feb 05, 2022, which is just after the data for our set and that \nof DeepTMHMM [44] have been collected. This comprised 1,511 PDB structures \n(more than 250 of which related to the SARS-CoV-2 protein P0DTD1) that we could \nmap to 1,078 different UniProtKB sequences. We then used PSI-BLAST to remove all \nsequences similar to our data set or that of DeepTMHMM (e-value <  10–4, 20% PIDE \nat 40% coverage), which resulted in 333 proteins. Next, we predicted transmembrane \nsegments within those proteins using TMbed and DeepTMHMM. For 38 proteins, \neither TMbed or DeepTMHMM predicted transmembrane segments. After removing \nany sequences shorter than 100 residues (i.e., fragments) and those in which the pre -\ndicted segments were not within the resolved regions of the PDB structure, we were \nleft with a set of 5 proteins: one beta barrel TMP and four alpha helical TMPs. Finally, \nwe used the PPM [63– 65] algorithm from OPM [45] to estimate the actual membrane \nboundaries.\nResults and discussion\nWe have developed a new machine learning model, dubbed TMbed; it exclusively \nuses embeddings from the ProtT5 [34] pLM as input to predict for each residue in \na protein sequence to which of the following four “classes” it belongs: transmem -\nbrane beta strand (TMB), transmembrane helix (TMH), signal peptide (SP), or non-\ntransmembrane segment. It also predicts the inside/outside orientation of TMBs and \nTMHs within the membrane, indicating which parts of a protein are inside or outside \na cell or compartment. Although the prediction of signal peptides was primarily inte -\ngrated to improve TMH predictions by preventing the confusion of TMHs with SPs \nand vice versa, we also evaluated and compared the performance for SP prediction of \nTMbed to that of other methods.\nReaching SOTA in protein sorting\nTMbed detected TMPs with TMHs and TMBs at levels similar or numerically above the \nbest state-of-the-art (SOTA) methods that use evolutionary information from multiple \nsequence alignments (MSA; Table 1: Recall). Compared to MSA-based methods, TMbed \nachieved this parity or improvement at a significantly lower false positive rate (FPR), tied \nonly with DeepTMHMM [44], another embedding-based method (Table  1: FPR). Given \nthose numbers, we expect TMbed to misclassify only about 215 proteins for a proteome \nwith 20,000 proteins (Additional file  1: Table S10), e.g., the human proteome, while the \nother methods would make hundreds more mistakes (DeepTMHMM: 331, TOPCONS2: \n683, BOCTOPUS2: 880). Such low FPRs suggest our method as an automated high-\nthroughput filter for TMP detection, e.g., for the creation and annotation of databases, \nor the decision which AlphaFold2 [11, 68] predictions to parse through advanced soft -\nware annotating transmembrane regions in 3D structures or predictions [45, 49, 69]. In \nthe binary prediction of whether or not a protein has a signal peptide, TMbed achieved \nsimilar levels as the specialist SignalP 6.0 [52] and as DeepTMHMM [44], reaching 99% \nrecall at 0.1% FPR (Additional file 1: Table S3).\nPage 9 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nMany of the beta barrel TMPs that prediction methods missed had only two or \nfour transmembrane beta strands (TMB). Such proteins cannot form a pore on their \nown, instead they have to form complexes with other proteins to function as TMPs, \neither by binding to other proteins or by forming multimers with additional copies of \nthe same proteins by, e.g., trimerization. In fact, all four beta barrel TMPs missed by \nTMbed fell into this category. Thus, as all other methods, TMbed performed, on aver -\nage, worse for beta barrel TMPs that cannot form pores alone. This appeared unsur -\nprising, as the input to all methods were single proteins. For TMPs with TMHs, we \nalso observed lower performance in the distinction between TMP/other for TMPs \nwith a single TMH (recall: 93 ± 3%) compared to those with multiple TMHs (recall: \n99 ± 1%). However, TMPs with single helices can function alone.\nThe embedding-based methods TMbed (introduced here using ProtT5 [34]) and \nDeepTMHMM [44] (based on ESM-1b [36]) performed at least on par with the SOTA \nusing evolutionary information from MSA (Table  1). While this was already impres -\nsive, the real advantage was in the speed. For instance, our method, TMbed, predicted \nall 6,517 proteins in our data set in about 13 min (i.e., about eight sequences per sec -\nond) on our server machine (Additional file  1: Table S1); this runtime included gener -\nating the ProtT5 embeddings. The other embedding-based method, DeepTMHMM, \nneeded about twice as long (23 min). Meanwhile, methods that search databases and \nTable 1 Per-protein performance. *\n*Evaluation of the ability to distinguish between 57 beta barrel TMPs (β-TMP), 571 alpha helical TMPs (α-TMP) and 5654 \nglobular, water-soluble non-TMP proteins in our data set. Recall and false positive rate (FPR) were averaged over the five \nindependent cross-validation test sets; error margins given for the 95% confidence interval (1.96*standard error); bold: best \nvalues for each column; italics: differences statistically significant with over 95% confidence (only computed between best \nand 2nd best, or all methods ranked 1 and those ranked lower)\n1 Evaluation missing for one of 5,654 globular proteins\n2 Evaluation missing for one of 571 α-TMPs and six of 5,654 globular proteins\n3 Evaluation includes only 51 β-TMPs, 552 α-TMPs, and 5,524 globular proteins due to runtime errors\n4 The local PRED-TMBB2 version did not include the pre-filtering step of the web server. This caused a FPR for β-TMP of \nalmost 78%. Thus, we listed the statistics for the web server predictions, which did not include MSA input\nβ‑TMP (57) α‑TMP (571) Globular (5654)\nRecall (%) FPR (%) Recall (%) FPR (%) Recall (%) FPR (%)\nTMbed 93.8 ± 7.5 0.1 ± 0.1 97.5 ± 0.7 0.5 ± 0.2 99.5 ± 0.2 2.8 ± 1.2\nDeepTMHMM 77.9 ± 12.7 0.1 ± 0.1 95.8 ± 1.3 0.5 ± 0.2 99.5 ± 0.2 5.9 ± 2.2\nTMSEG – – 96.5 ± 1.0 2.3 ± 0.3 97.7 ± 0.3 3.5 ± 1.0\nTOPCONS21 – – 94.2 ± 1.3 2.6 ± 0.3 97.4 ± 0.3 5.8 ± 1.3\nOCTOPUS1 – – 94.2 ± 1.9 9.1 ± 0.7 90.9 ± 0.7 5.8 ± 1.9\nPhilius1 – – 92.5 ± 1.4 2.6 ± 0.2 97.4 ± 0.2 7.5 ± 1.4\nPolyPhobius1 – – 97.2 ± 1.1 5.3 ± 0.4 94.7 ± 0.4 2.8 ± 1.1\nSPOCTOPUS1 – – 97.5 ± 1.6 17.2 ± 0.8 82.8 ± 0.8 2.5 ± 1.6\nSCAMPI2 (MSA) – – 94.2 ± 1.6 5.6 ± 0.3 94.4 ± 0.3 5.8 ± 1.6\nCCTOP2 96.1 ± 2.1 3.7 ± 0.6 96.3 ± 0.6 3.9 ± 2.1\nHMM-TM (MSA)3 – – 97.3 ± 1.6 21.4 ± 0.5 78.6 ± 0.5 2.7 ± 1.6\nBOCTOPUS2 84.0 ± 13.3 4.2 ± 0.5 – – 95.8 ± 0.5 16.0 ± 13.3\nBetAware-Deep 85.1 ± 9.3 4.7 ± 0.3 – – 95.3 ± 0.3 14.9 ± 9.3\nPRED-TMBB24 88.8 ± 12.1 7.1 ± 0.4 – – 92.9 ± 0.4 11.2 ± 12.1\nPROFtmb 91.9 ± 9.0 6.1 ± 0.5 – – 93.9 ± 0.5 8.1 ± 9.0\nPage 10 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \ngenerate MSAs usually take several seconds or minutes for a single protein sequence \n[70], or require significant amounts of computing resources (e.g., often more than \n100 GB of memory) to achieve comparable runtimes [55].\nExcellent transmembrane segment prediction performance\nTMbed reached the highest performance for transmembrane segments amongst all \nmethods evaluated (Tables 2, 3). With recall and precision values of 89 ± 1% for TMHs, \nit significantly outperformed the second best and only other embedding-based method, \nDeepTMHMM, (80 ± 2%, Table 2). TMbed essentially predicted 62% of all transmem -\nbrane helical (TMH) TMPs completely correctly  (Qok, i.e., all TMHs within ± 5 resi-\ndues of true annotation). DeepTMHMM reached second place with  Qok of 46 ± 4%. \nThis difference between TMbed and DeepTMHMM was over twice that between \nTable 2 Per-segment performance for TMH (transmembrane helices). *\n*Segment performance for transmembrane helix (TMH) prediction based on 571 alpha helical TMPs (α-TMP) with a total of \n2936 TMHs. Recall, Precision,  Qok,  Qnum, and  Qtop were averaged over the five independent cross-validation test sets; error \nmargins given for the 95% confidence interval (1.96*standard error); bold: best values for each column; italics: differences \nstatistically significant with over 95% confidence (only computed between best and 2nd best).\n1 Evaluation missing for one of 571 α-TMPs.\n2 Evaluation includes only 552 of the 571 α-TMPs due to runtime errors of the method.\nTMH (571/2936)\nRecall (%) Precision (%) Qok (%) Qnum (%) Qtop (%)\nTMbed 88.7 ± 0.6 88.7 ± 0.7 62.4 ± 3.7 86.0 ± 2.3 96.4 ± 2.7\nDeepTMHMM 80.0 ± 2.4 80.5 ± 2.4 46.2 ± 4.8 85.7 ± 3.5 96.3 ± 2.2\nTMSEG 74.5 ± 2.4 77.1 ± 1.7 35.6 ± 2.4 69.9 ± 2.7 83.8 ± 4.7\nTOPCONS2 76.4 ± 1.5 78.4 ± 0.8 41.0 ± 3.1 74.4 ± 3.3 91.7 ± 3.1\nOCTOPUS 71.6 ± 1.5 75.7 ± 1.4 36.0 ± 2.8 67.6 ± 3.4 87.5 ± 3.1\nPhilius 70.8 ± 2.2 73.7 ± 0.8 34.2 ± 3.7 66.9 ± 3.4 87.5 ± 2.9\nPolyPhobius 76.0 ± 2.1 76.4 ± 1.1 40.3 ± 3.5 74.5 ± 2.8 86.8 ± 2.7\nSPOCTOPUS 71.5 ± 1.2 75.8 ± 1.2 35.7 ± 3.3 67.4 ± 5.5 87.2 ± 3.4\nSCAMPI2 (MSA) 72.3 ± 2.7 74.1 ± 1.5 33.5 ± 3.0 72.2 ± 4.5 90.6 ± 3.5\nCCTOP1 77.0 ± 1.7 79.4 ± 1.0 41.9 ± 3.6 82.6 ± 2.7 92.6 ± 2.6\nHMM-TM (MSA)2 73.3 ± 1.7 72.5 ± 1.2 33.5 ± 1.4 72.1 ± 3.0 88.3 ± 4.2\nTable 3 Per-segment performance for TMB (transmembrane beta strands). *\n*Segment performance for transmembrane beta strand (TMB) prediction based on 57 beta barrel TMPs (β-TMP) with a total \nof 768 TMBs. Recall, Precision,  Qok,  Qnum, and  Qtop were averaged over the five independent cross-validation test sets; error \nmargins given for the 95% confidence interval (1.96*standard error); bold: best values for each column; italics: differences \nstatistically significant with over 95% confidence (only computed between best and 2nd best)\nTMB (57/768)\nRecall (%) Precision (%) Qok (%) Qnum (%) Qtop (%)\nTMbed 95.0 ± 4.3 99.2 ± 0.7 80.5 ± 11.4 88.1 ± 6.9 98.1 ± 3.8\nDeepTMHMM 85.9 ± 6.6 92.5 ± 4.7 46.1 ± 7.6 74.3 ± 13.0 97.2 ± 4.4\nBOCTOPUS2 85.3 ± 9.2 96.6 ± 2.0 56.6 ± 18.9 71.2 ± 11.8 98.0 ± 2.0\nBetAware-Deep 67.1 ± 6.5 62.2 ± 11.4 8.7 ± 5.3 60.9 ± 14.1 95.7 ± 5.4\nPRED-TMBB2 (MSA) 85.4 ± 1.9 75.6 ± 4.8 18.4 ± 15.0 44.5 ± 26.7 95.9 ± 3.4\nPROFtmb 78.2 ± 10.1 78.0 ± 6.9 20.2 ± 12.8 46.6 ± 11.7 97.2 ± 1.0\nPage 11 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nDeepTMHMM and the two methods performing third  best by this measure, CCTOP \n[17, 18] and TOPCONS2 [29], which are based on evolutionary information.\nThe results were largely similar for beta barrel TMPs (TMBs) with TMbed achieving \nthe top performance by all measures: reaching 95% recall and an almost perfect 99% pre-\ncision. The most pronounced difference was a 23 percentage points lead in  Qok with 80%, \ncompared to BOCTOPUS2 [16] with 57% in second place. Overall, TMbed predicted the \ncorrect number of transmembrane segments in 86–88% of TMPs and correctly oriented \n98% of TMBs and 96% of TMHs. For signal peptides, TMbed performed on par with \nSignalP 6.0, reaching 93% recall and 95% precision (Additional file  1: Table S3). For this \ntask, both methods appeared to be slightly outperformed by DeepTMHMM. However, \nnone of those differences exceeded the 95% confidence interval, i.e., the numerically \nconsistent differences were not statistically significant. On top, the signal peptide expert \nmethod SignalP 6.0 is the only of the three that distinguishes between different types of \nsignal peptides.\nAs for the overall per-protein distinction between TMP and non-TMP , the per-seg -\nment recall and precision also slightly correlated with the number of transmembrane \nsegments, i.e., the more TMHs or TMBs in a protein the higher the performance (Addi -\ntional file  1: Table S4). Again, as for the TMP/non-TMP distinction, beta barrel TMPs \nwith only two or four TMBs differed most to those with eight or more.\nGaussian filter and Viterbi decoder improve segment performance\nTMbed introduced a Gaussian filter smoothing over some local peaks in the predic -\ntion and a Viterbi decoder implicitly enforcing some “grammar-like” rules (Materials & \nMethods). We investigated the effect of these concepts by comparing the final TMbed \narchitecture to three simpler alternatives: one variant used only the CNN, the other two \nvariants combined the simple CNN with either the Gaussian filter or the Viterbi decoder, \nnot both as TMbed. For the variants without the Gaussian filter, we retrained the CNN \nusing the same hyperparameters but without the filter. Individually, both modules (fil -\nter and decoder) significantly improved precision and  Qok for both TMH and TMB, \nwhile recall remained largely unaffected (Additional file  1: Table S9). Clearly, either step \nalready improved over just the CNN. However, which of the two was most important \ndepended on the type of TMP: for TMH proteins Viterbi decoder mattered more, for \nTMB proteins the Gaussian filter. Both steps together performed best throughout with -\nout adding any significant overhead to the overall computational costs compared to the \nother components.\nSelf‑predictions reveal potential membrane proteins\nWe checked for potential overfitting of our model by predicting the complete data set \nwith the final TMbed ensemble. This meant that four of the five models had seen each of \nthose proteins during training. While the number of misclassified proteins went down, \nwe found that there were still some false predictions, indicating that our models did not \nsimply learn the training data by heart (Additional file  1: Tables S7, S8). In fact, upon \ncloser inspection of the 11 false positive predictions (8 alpha helical and 3 beta barrel \nTMPs), those appear to be transmembrane proteins incorrectly classified as globular \nproteins in our data set due to missing annotations in UniProtKB/Swiss-Prot, rather \nPage 12 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \nthan incorrect predictions. Two of them, P09489 and P40601, have automatic annota -\ntions for an autotransporter domain, which facilitates transport through the membrane. \nFurther, we processed the predicted AlphaFold2 [11, 68] structures of all 11 proteins \nusing the PPM [45] algorithm, which tries to embed 3D structures into a membrane \nbilayer. For eight of those, the predicted transmembrane segments correlated well with \nthe predicted 3D structures and membrane boundaries (Fig.  1; Additional file 1: Fig. S5). \nFor the other three, the 3D structures and membrane boundaries still indicate trans -\nmembrane domains within those proteins, but the predicted transmembrane segments \nonly cover parts of those domains (Additional file  1: Fig. S5, last row). Together, these \npredictions provided convincing evidence for considering all eleven proteins as TMPs.\nPredicting the human proteome in less than an hour\nGiven that our new method already outperformed the SOTA using evolutionary infor -\nmation from MSAs, the even more important advantage was speed. To estimate pre -\ndiction throughput, we applied TMbed to all human proteins in 20,375 UniProtKB/\nSwiss-Prot (version: April 2022; excluding TITIN_HUMAN due to its extreme length \nof 34,350 residues). Overall, it took our server machine (Additional file  1: Table S1) only \n46 min to generate all embeddings and predictions (estimate for consumer-grade PC in \nthe next section). TMbed identified 14 beta barrel TMPs and 4,953 alpha helical TMPs, \nmatching previous estimates for alpha helical TMPs [1, 28]. Two of the 14 TMBs appear \nto be false positives as TMbed predicted only a single TMB in each protein. The other 12 \nproteins are either part of the Gasdermin family (A to E), or associated with the mito -\nchondrion, including three proteins for a voltage-dependent anion-selective channel and \nthe TOM40 import receptor.\nFurther, we generated predictions for all proteins from UniProtKB/Swiss-Prot (ver -\nsion: May 2022), excluding sequences above 10,000 residues (20 proteins). Processing \nthose 566,976 proteins took about 8.5 h on our server machine. TMbed predicted 1,702 \nbeta barrel TMPs and 77,296 alpha helical TMPs (predictions available via our GitHub \nrepository).\nFig. 1 Potential transmembrane proteins in the globular data set. AlphaFold2 [11, 68] structure of \nextracellular serine protease (P09489) and Lipase 1 (P40601). Transmembrane segments (dark purple) \npredicted by TMbed correlate well with membrane boundaries (dotted lines: red = outside, blue = inside) \npredicted by the PPM [45] web server. Images created using Mol* Viewer [71]. Though our data set lists them \nas globular proteins, the predicted structures indicate transmembrane domains, which align with segments \npredicted by our method. The predicted domains overlap with autotransporter domains detected by the \nUniProtKB [46] automatic annotation system. Transmembrane segment predictions were made with the final \nTMbed ensemble model\nPage 13 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nHardware requirements\nOur model needs about 2.5 GB of memory on the GPU when in 16-bit format. The addi-\ntional memory needed during inference grows with the square of sequence length due \nto the attention mechanism of the transformer architecture. On our consumer-grade \ndesktop PC (Additional file  1: Table S1), this translated to a maximum sequence length \nof about 4,200 residues without maxing out the 12 GB of GPU memory. This barred 76 \n(0.4%) of the 20,376 human proteins from analysis on a personal consumer-hardware \nsolution (NVIDIA GeForce RTX 3060). The prediction (including embedding genera -\ntion) for 99.6% of the human proteome (20,376 proteins) took about 57 min on our desk-\ntop PC. While it is possible to run the model on a CPU, instead of on a GPU, we do not \nrecommend this due to over tenfold larger runtimes. More importantly, the current lack \nof support of 16-bit floating-point format on CPUs would imply doubling the memory \nfootprint of the model and computations.\nOut‑of‑distribution performance\nThe two pLM-based methods DeepTMHMM [44] and TMbed appeared to reach simi -\nlar performance according to the additional out-of-distribution data set (Additional \nfile 1: Tables S11, S12). While DeepTMHMM reached higher scores for beta barrel pro -\nteins  (Qok of 79 ± 22% vs. 64 ± 26%), these were not quite statistically significant. On \nthe other hand, TMbed managed to outperform DeepTMHMM for alpha helical TMPs \n (Qok of 53 ± 11% vs. 47 ± 10%), though again without statistical significance. Further -\nmore, TMbed performed on par with the OPM baseline (Additional file  1: Table S12), \ni.e., using the OPM annotations as predictions for the DeepTMHMM data set, imply -\ning that TMbed reached its theoretical performance limit on that data set. Surprisingly, \nTOPCONS2 and CCTOP both outperformed TMbed and DeepTMHMM with  Qok of \n65 ± 10% and 64 ± 10% (both not statistically significant), respectively.\nTaking a closer look at the length distribution for the transmembrane segments in \nthe TMbed and DeepTMHMM data set annotations and predictions (Additional file  1: \nFig. S6) revealed differences. First, while the TMB segments in both data sets averaged \n9 residues in length, the DeepTMHMM distribution was slightly shifted toward shorter \nsegments (left in Additional file 1: Fig. S6A) but with a wider spread towards longer seg -\nments (right in Additional file  1: Fig. S6A). Both of these features were mirrored in the \ndistribution of predicted TMBs. In contrast, the TMH distributions for DeepTMHMM \nshowed an unexpected peak for TMH with 21 residues (both in the annotations used to \ntrain DeepTMHMM and in the predictions). In fact, the peak for annotated TMHs at \n21 was more than double the value of the two closest length-bins (TMH = 20|22) com-\nbined. As the lipid bilayer remains largely invisible in X-ray structures, the exact begin \nand ends of TMHs may have some errors [28, 45, 49–51, 62]. Thus, when plotting the \ndistribution of TMH length, we expected some kind of normal distribution with a peak \naround 20-odd residues with more points for longer than for shorter TMHs [72]. In stark \ncontrast to this expectation, the distribution observed for the TMHs used to develop \nDeepTMHMM appeared to have been obtained through some very different protocol \n(Additional file 1: Fig. S6B).\nIn contrast, the distributions for the annotations from OMP and the predictions from \nTMbed appeared to be more normally distributed with TMH lengths exhibiting a slight \nPage 14 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \npeak at 22 residues. The larger the AI model, the more it succeeds in reproducing fea -\ntures of the development set even when those might be based on less experimentally \nsupported aspects. The DeepTMHMM model reproduced the dubious experimental \ndistribution of TMHs exceedingly (Additional file  1: Fig. S6B, e.g., orange line and bars \naround peak at 16). Although we do not know the origin of this bias in the DeepTM -\nHMM data set, we have seen similar bias in some prediction methods and automated \nannotations in UniProtKB/Swiss-Prot. In fact, a quick investigation showed that for 80 \nof the 184 common alpha helical TMPs the DeepTMHMM annotations matched those \nfound in UniProtKB but not the OPM annotation in our TMbed data set. Of those \nannotations, 66% (303 of 459) were 21-residues long TMHs, accounting for 73% of all \nsuch segments; the other 104 TMPs contained only 19% (114 of 593) TMHs of length \n21. This led us to believe that the DeepTMHMM data set contained, in part, length-\nbiased annotations found in UniProtKB. Other examples of methods with length biases \ninclude SCAMPI2 and TOPCONS2 that both predicted exclusively TMHs with 21 resi -\ndues; OCTOPUS and SPOCTOPUS predicted only TMHs of length 15, 21, and 31 (with \nmore than 90% of those being 21 residues). BOCTOPUS2 predicted only beta strands of \nlength 8, 9, and 10, with about 80% of them being nine residues long.\nSince TMHs are around 21 residues long, such bias is not necessarily relevant. How -\never, it might point to why performance appears better against some data sets supported \nless by high-resolution experiments than by others.\nPerformance on new membrane proteins\nAlthough, the small data set size did not allow for statistically significant results (Addi -\ntional file 1: Table S13), TMbed performed numerically better than the other methods; in \nparticular, BOCTOPUS2 failed to predict the only beta barrel TMP . While TMbed and \nDeepTMHMM both missed two of the 30 transmembrane beta strands, TMbed placed \nthe remaining ones, on average, more accurately (recall: 93% vs 87%; precision: 100% vs. \n93%). All methods performed worse for the alpha helical TMPs than on the other two \nbenchmark data set, though with a sample size of only four proteins (25 TMHs total), \nwe cannot be sure if this is an effect of testing on novel membrane proteins or simply by \nchance. Nevertheless, the transmembrane segments predicted by TMbed fit quite well to \nthe membrane boundaries estimated by the PPM [63–65] algorithm (Fig. 2).\nNo data leakage through pLM\npLMs such as ProtT5 [34] used by TMbed or ESM-1b [36] used by DeepTMHMM \nare pre-trained on billions of protein sequences. Typically, these include all pro -\ntein sequences known today. In particular, they include all membrane and non-\nmembrane proteins used in this study. In fact, assuming that the TMPs of known \nstructure account for about 2–5% [78, 79] of all TMPs and that TMPs account for \nabout 20–25% of all proteins, we assume pLMs have been trained on over 490 mil -\nlion TMPs that remain to be experimentally characterized. For the development of \nAI/ML solutions, it is crucial to establish that methods do not over-fit to existing \ndata but that they will also work for new, unseen data. This implies that in the stand -\nard cross-validation process, it is important to not leak any data from development \n(training and validation used for hyperparameter optimization and model choice) \nPage 15 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nto test set (used to assess performance). This implies the necessity for redundancy \nreduction. This also implies that the conditions for the test set are exactly the same \nas those that will be encountered in future predictions. For instance, if today’s exper -\nimental annotations were biased toward bacterial proteins, we might expect perfor -\nmance to be worse for eukaryotic proteins and vice versa.\nBoth TMbed introduced here and DeepTMHMM are based on the embeddings of \npre-trained pLMs; both accomplish the TM-prediction through a subsequent step \ndubbed transfer learning, in which they use the pLM embeddings as input to train \na new AI/ML model in supervised manner on some annotations about membrane \nsegments. Could any data leak from the training of pLMs into the subsequent step of \ntraining the TM-prediction methods? Strictly speaking, if no experimental annota -\ntions are used, no annotations can leak: the pLMs used here never saw any annota -\ntion other than protein sequences.\nEven when no annotations could have leaked because none were used for the pLM, \nshould we still ascertain that the conditions for the test set and for the protein for \nwhich the method will be applied in the future are identical? We claim that we do \nnot have to ascertain this. However, we cannot support any data for (nor against) \nthis claim. To play devil’s advocate, let us assume we had to. The reality is that the \nvast majority of all predictions likely to be made over the next five years will be for \nproteins included in these pLMs. In other words, the conditions for future use-cases \nare exactly the same as those used in our assessment.\nFig. 2 New membrane proteins. PDB structures for probable flagellin 1 (Q9YAN8; 7TXI [73]), protein-serine \nO-palmitoleoyltransferase porcupine (Q9H237; 7URD [74]), choline transporter-like protein 1 (Q8WWI5; 7WWB \n[75]), S-layer protein SlpA (Q9RRB6; 7ZGY [76]), and membrane protein (P0DTC5; 8CTK [77]). Transmembrane \nsegments (dark purple) predicted by TMbed; membrane boundaries (dotted lines: red = outside, \nblue = inside) predicted by the PPM [45] web server. Images created using Mol* Viewer [71]\nPage 16 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \nConclusions\nTMbed predicts alpha helical (TMH) and beta barrel (TMB) transmembrane proteins \n(TMPs) with high accuracy (Table  1), performing at least on par or even better than \nstate-of-the-art (SOTA) methods, which depend on evolutionary information from mul-\ntiple sequence alignments (MSA; Tables  1, 2, 3). In contrast, TMbed exclusively inputs \nsequence embeddings from the protein language model (pLM) ProtT5. Our novel \nmethod shines, in particular, through its low false positive rate (FPR; Table 1), incorrectly \npredicting fewer than 1% of globular proteins to be TMPs. TMbed also numerically out -\nperformed all other tested methods in terms of correctly predicting transmembrane \nsegments (on average, 9 out of 10 segments were correct; Tables  2, 3). Despite its top \nperformance, the even more significant advantage of TMbed is speed: the high through -\nput rate of the ProtT5 [34] encoder enables predictions for entire proteomes within an \nhour, given a suitable GPU (Additional file  1: Table  S1). On top, the method runs on \nconsumer-grade GPUs as found in more recent gaming and desktop PCs. Thus, TMbed \ncan be used as a proteome-scale filtering step to scan for transmembrane proteins. Vali -\ndating the predicted segments with AlphaFold2 [11, 68] structures and the PPM [45] \nmethod could be combined into a fast pipeline to discover new membrane proteins, as \nwe have demonstrated with a few proteins. Finally, we provide predictions for 566,976 \nproteins from UniProtKB/Swiss-Prot (version: May 2022) via our GitHub repository.\nAbbreviations\nCI  Confidence interval\nCNN  Convolutional neural network\nMSA  Multiple sequence alignment\nOPM  Orientations of proteins in membranes database\nPDB  Protein data bank\nPDBTM  Protein data bank of transmembrane proteins\npLM  Protein language model\nSIFTS  Structure integration with function, taxonomy and sequence\nSOTA  State-of-the-art\nSP  Signal peptide\nTMB  Transmembrane beta strand\nTMH  Transmembrane helix\nTMP  Transmembrane protein\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 022- 04873-x.\nAdditional file 1. Supporting Online Material (SOM) containing additional figures, tables and notes.\nAcknowledgements\nThanks to Tim Karl and Inga Weise for their help with technical and administrative issues; to Tobias Olenyi, Michael Hein-\nzinger, and Christian Dallago for thoughtful discussions, help with ProtT5, and help with the manuscript; to Konstantinos \nTsirigos and Ioannis Tamposis for their support with setting up HMM-TM and PRED-TMBB2; to Pier Luigi Martelli for \nproviding us with BetAware-Deep predictions. Thanks to all who deposit their experimental data in public databases, \nand to those who maintain them. Last but not least, we thank the reviewers for their constructive criticism, which helped \nto improve our manuscript.\nAuthor contributions\nMB collected the data sets, developed and evaluated the TMbed model, and took the lead in writing the manuscript. BR \nsupervised and guided the work, and co-wrote the manuscript. All authors read and approved the final manuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL. The server machine to run the ProtT5 model was funded \nby Software Campus Funding (BMBF 01IS17049).\nAvailability of data and materials\nOur code, method, and data sets are freely available in the GitHub repository, https:// github. com/ Bernh oferM/ TMbed.\nPage 17 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \nDeclarations\nEthical approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 12 June 2022   Accepted: 3 August 2022\nReferences\n 1. Fagerberg L, Jonasson K, von Heijne G, Uhlen M, Berglund L. Prediction of the human membrane proteome. Prot-\neomics. 2010;10(6):1141–9.\n 2. Liu J, Rost B. Comparing function and structure between entire proteomes. Protein Sci. 2001;10(10):1970–9.\n 3. Bigelow HR, Petrey DS, Liu J, Przybylski D, Rost B. Predicting transmembrane beta-barrels in proteomes. Nucleic \nAcids Res. 2004;32(8):2566–77.\n 4. Overington JP , Al-Lazikani B, Hopkins AL. How many drug targets are there? Nat Rev Drug Discov. 2006;5(12):993–6.\n 5. von Heijne G. The membrane protein universe: what’s out there and why bother? J Intern Med. 2007;261(6):543–57.\n 6. ww PDBc. Protein Data Bank: the single global archive for 3D macromolecular structure data. Nucleic Acids Res. \n2019;47(D1):D520–D8.\n 7. Berman H, Henrick K, Nakamura H. Announcing the worldwide Protein Data Bank. Nat Struct Biol. 2003;10(12):980.\n 8. Hendrickson WA. Atomic-level analysis of membrane-protein structure. Nat Struct Mol Biol. 2016;23(6):464–7.\n 9. Varga J, Dobson L, Remenyi I, Tusnady GE. TSTMP: target selection for structural genomics of human transmem-\nbrane proteins. Nucleic Acids Res. 2017;45(D1):D325–30.\n 10. Newport TD, Sansom MSP , Stansfeld PJ. The MemProtMD database: a resource for membrane-embedded protein \nstructures and their lipid interactions. Nucleic Acids Res. 2019;47(D1):D390–7.\n 11. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneberger O, et al. Highly accurate protein structure prediction \nwith AlphaFold. Nature. 2021;596(7873):583–9.\n 12. Marx V. Method of the Year: protein structure prediction. Nat Methods. 2022;19(1):5–10.\n 13. Bordin N, Sillitoe I, Nallapareddy V, Rauer C, Lam SD, Waman VP , et al. AlphaFold2 reveals commonalities and novel-\nties in protein structure space for 21 model organisms. bioRxiv. 2022:2022.06.02.494367.\n 14. Hegedus T, Geisler M, Lukacs GL, Farkas B. Ins and outs of AlphaFold2 transmembrane protein structure predictions. \nCell Mol Life Sci. 2022;79(1):73.\n 15. Madeo G, Savojardo C, Martelli PL, Casadio R. BetAware-deep: an accurate web server for discrimination and topol-\nogy prediction of prokaryotic transmembrane beta-barrel proteins. J Mol Biol. 2021;433(11): 166729.\n 16. Hayat S, Peters C, Shu N, Tsirigos KD, Elofsson A. Inclusion of dyad-repeat pattern improves topology prediction of \ntransmembrane beta-barrel proteins. Bioinformatics. 2016;32(10):1571–3.\n 17. Dobson L, Remenyi I, Tusnady GE. The human transmembrane proteome. Biol Direct. 2015;10:31.\n 18. Dobson L, Remenyi I, Tusnady GE. CCTOP: a Consensus Constrained TOPology prediction web server. Nucleic Acids \nRes. 2015;43(W1):W408–12.\n 19. Bagos PG, Liakopoulos TD, Hamodrakas SJ. Algorithms for incorporating prior topological information in HMMs: \napplication to transmembrane proteins. BMC Bioinform. 2006;7:189.\n 20. Tamposis IA, Sarantopoulou D, Theodoropoulou MC, Stasi EA, Kontou PI, Tsirigos KD, et al. Hidden neural networks \nfor transmembrane protein topology prediction. Comput Struct Biotechnol J. 2021;19:6090–7.\n 21. Tamposis IA, Theodoropoulou MC, Tsirigos KD, Bagos PG. Extending hidden Markov models to allow conditioning \non previous observations. J Bioinform Comput Biol. 2018;16(5):1850019.\n 22. Viklund H, Elofsson A. OCTOPUS: improving topology prediction by two-track ANN-based preference scores and an \nextended topological grammar. Bioinformatics. 2008;24(15):1662–8.\n 23. Reynolds SM, Kall L, Riffle ME, Bilmes JA, Noble WS. Transmembrane topology and signal peptide prediction using \ndynamic bayesian networks. PLoS Comput Biol. 2008;4(11):e1000213.\n 24. Kall L, Krogh A, Sonnhammer EL. An HMM posterior decoder for sequence feature prediction that includes homol-\nogy information. Bioinformatics. 2005;21(Suppl 1):i251–7.\n 25. Tsirigos KD, Elofsson A, Bagos PG. PRED-TMBB2: improved topology prediction and detection of beta-barrel outer \nmembrane proteins. Bioinformatics. 2016;32(17):i665–71.\n 26. Peters C, Tsirigos KD, Shu N, Elofsson A. Improved topology prediction using the terminal hydrophobic helices rule. \nBioinformatics. 2016;32(8):1158–62.\n 27. Viklund H, Bernsel A, Skwark M, Elofsson A. SPOCTOPUS: a combined predictor of signal peptides and membrane \nprotein topology. Bioinformatics. 2008;24(24):2928–9.\n 28. Bernhofer M, Kloppmann E, Reeb J, Rost B. TMSEG: Novel prediction of transmembrane helices. Proteins. \n2016;84(11):1706–16.\n 29. Tsirigos KD, Peters C, Shu N, Kall L, Elofsson A. The TOPCONS web server for consensus prediction of membrane \nprotein topology and signal peptides. Nucleic Acids Res. 2015;43(W1):W401–7.\n 30. Asgari E, Mofrad MR. Continuous distributed representation of biological sequences for deep proteomics and \ngenomics. PLoS ONE. 2015;10(11):e0141287.\nPage 18 of 19Bernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n 31. Alley EC, Khimulya G, Biswas S, AlQuraishi M, Church GM. Unified rational protein engineering with sequence-based \ndeep representation learning. Nat Methods. 2019;16(12):1315–22.\n 32. Heinzinger M, Elnaggar A, Wang Y, Dallago C, Nechaev D, Matthes F, et al. Modeling aspects of the language of life \nthrough transfer-learning protein sequences. BMC Bioinform. 2019;20(1):723.\n 33. Bepler T, Berger B. Learning the protein language: evolution, structure, and function. Cell Syst. 2021;12(6):654-69 e3.\n 34. Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, et al. ProtTrans: towards cracking the language of \nLifes code through self-supervised deep learning and high performance computing. IEEE Trans Pattern Anal Mach \nIntell. 2021.\n 35. Ofer D, Brandes N, Linial M. The language of proteins: NLP , machine learning & protein sequences. Comput Struct \nBiotechnol J. 2021;19:1750–8.\n 36. Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, et al. Biological structure and function emerge from scaling unsuper-\nvised learning to 250 million protein sequences. Proc Natl Acad Sci USA. 2021;118(15):e2016239118.\n 37. Wu Z, Johnston KE, Arnold FH, Yang KK. Protein sequence design with deep generative models. Curr Opin Chem \nBiol. 2021;65:18–27.\n 38. Marquet C, Heinzinger M, Olenyi T, Dallago C, Erckert K, Bernhofer M, et al. Embeddings from protein language mod-\nels predict conservation and variant effects. Hum Genet. 2021.\n 39. Littmann M, Heinzinger M, Dallago C, Weissenow K, Rost B. Protein embeddings and deep learning predict binding \nresidues for various ligand classes. Sci Rep. 2021;11(1):23916.\n 40. Littmann M, Heinzinger M, Dallago C, Olenyi T, Rost B. Embeddings from deep learning transfer GO annotations \nbeyond homology. Sci Rep. 2021;11(1):1160.\n 41. Sledzieski S, Singh R, Cowen L, Berger B. D-SCRIPT translates genome to phenome with sequence-based, structure-\naware, genome-scale predictions of protein-protein interactions. Cell Syst. 2021;12(10):969-82 e6.\n 42. Heinzinger M, Littmann M, Sillitoe I, Bordin N, Orengo C, Rost B. Contrastive learning on protein embeddings \nenlightens midnight zone. bioRxiv. 2022:2021.11.14.468528.\n 43. Weißenow K, Heinzinger M, Rost B. Protein language model embeddings for fast, accurate, alignment-free protein \nstructure prediction. bioRxiv. 2021:2021.07.31.454572.\n 44. Hallgren J, Tsirigos KD, Pedersen MD, Almagro Armenteros JJ, Marcatili P , Nielsen H, et al. DeepTMHMM predicts \nalpha and beta transmembrane proteins using deep neural networks. bioRxiv. 2022:2022.04.08.487609.\n 45. Lomize MA, Pogozheva ID, Joo H, Mosberg HI, Lomize AL. OPM database and PPM web server: resources for posi-\ntioning of proteins in membranes. Nucleic Acids Res. 2012;40(Database issue):D370–6.\n 46. UniProt C. UniProt: the universal protein knowledgebase in 2021. Nucleic Acids Res. 2021;49(D1):D480–9.\n 47. Dana JM, Gutmanas A, Tyagi N, Qi G, O’Donovan C, Martin M, et al. SIFTS: updated Structure Integration with Func-\ntion, Taxonomy and Sequences resource allows 40-fold increase in coverage of structure-based annotations for \nproteins. Nucleic Acids Res. 2019;47(D1):D482–9.\n 48. Velankar S, Dana JM, Jacobsen J, van Ginkel G, Gane PJ, Luo J, et al. SIFTS: structure integration with function, tax-\nonomy and sequences resource. Nucleic Acids Res. 2013;41(Database issue):D483–9.\n 49. Kozma D, Simon I, Tusnady GE. PDBTM: Protein Data Bank of transmembrane proteins after 8 years. Nucleic Acids \nRes. 2013;41(Database issue):D524–9.\n 50. Tusnady GE, Dosztanyi Z, Simon I. Transmembrane proteins in the Protein Data Bank: identification and classifica-\ntion. Bioinformatics. 2004;20(17):2964–72.\n 51. Tusnady GE, Dosztanyi Z, Simon I. PDB_TM: selection and membrane localization of transmembrane proteins in the \nprotein data bank. Nucleic Acids Res. 2005;33(Database issue):D275–8.\n 52. Teufel F, Almagro Armenteros JJ, Johansen AR, Gislason MH, Pihl SI, Tsirigos KD, et al. SignalP 6.0 predicts all five \ntypes of signal peptides using protein language models. Nat Biotechnol. 2022.\n 53. Fu L, Niu B, Zhu Z, Wu S, Li W. CD-HIT: accelerated for clustering the next-generation sequencing data. Bioinformat-\nics. 2012;28(23):3150–2.\n 54. Li W, Godzik A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences. \nBioinformatics. 2006;22(13):1658–9.\n 55. Mirdita M, Steinegger M, Soding J. MMseqs2 desktop and local web server app for fast, interactive sequence \nsearches. Bioinformatics. 2019;35(16):2856–8.\n 56. Papaloukas C, Granseth E, Viklund H, Elofsson A. Estimating the length of transmembrane helices using Z-coordinate \npredictions. Protein Sci. 2008;17(2):271–8.\n 57. Granseth E, Viklund H, Elofsson A. ZPRED: predicting the distance to the membrane center for residues in alpha-\nhelical membrane proteins. Bioinformatics. 2006;22(14):e191–6.\n 58. Nugent T, Jones DT. Transmembrane protein topology prediction using support vector machines. BMC Bioinform. \n2009;10:159.\n 59. Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, et al. PyTorch: an imperative style, high-performance deep \nlearning library. 2019.\n 60. Lei Ba J, Kiros JR, Hinton GE. Layer normalization, 2016 July 01, 2016: arXiv: 1607. 06450. https:// ui. adsabs. harva rd. edu/ \nabs/ 2016a rXiv1 60706 450L.\n 61. Loshchilov I, Hutter F. Decoupled weight decay regularization 2017 November 01, 2017. arXiv: 1711. 05101. https:// ui. \nadsabs. harva rd. edu/ abs/ 2017a rXiv1 71105 101L.\n 62. Reeb J, Kloppmann E, Bernhofer M, Rost B. Evaluation of transmembrane helix predictions in 2014. Proteins. \n2015;83(3):473–84.\n 63. Lomize AL, Pogozheva ID, Mosberg HI. Anisotropic solvent model of the lipid bilayer. 2. Energetics of insertion of \nsmall molecules, peptides, and proteins in membranes. J Chem Inf Model. 2011;51(4):930–46.\n 64. Lomize AL, Pogozheva ID, Lomize MA, Mosberg HI. Positioning of proteins in membranes: a computational \napproach. Protein Sci. 2006;15(6):1318–33.\n 65. Lomize AL, Todd SC, Pogozheva ID. Spatial arrangement of proteins in planar and curved membranes by PPM 3.0. \nProtein Sci. 2022;31(1):209–20.\nPage 19 of 19\nBernhofer and Rost  BMC Bioinformatics          (2022) 23:326 \n \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 66. Mahfoud M, Sukumaran S, Hulsmann P , Grieger K, Niederweis M. Topology of the porin MspA in the outer mem-\nbrane of Mycobacterium smegmatis. J Biol Chem. 2006;281(9):5908–15.\n 67. Altschul SF, Madden TL, Schaffer AA, Zhang J, Zhang Z, Miller W, et al. Gapped BLAST and PSI-BLAST: a new genera-\ntion of protein database search programs. Nucleic Acids Res. 1997;25(17):3389–402.\n 68. Varadi M, Anyango S, Deshpande M, Nair S, Natassia C, Yordanova G, et al. AlphaFold Protein Structure Database: \nmassively expanding the structural coverage of protein-sequence space with high-accuracy models. Nucleic Acids \nRes. 2022;50(D1):D439–44.\n 69. Tunyasuvunakool K, Adler J, Wu Z, Green T, Zielinski M, Zidek A, et al. Highly accurate protein structure prediction for \nthe human proteome. Nature. 2021;596(7873):590–6.\n 70. Bernhofer M, Dallago C, Karl T, Satagopam V, Heinzinger M, Littmann M, et al. PredictProtein—predicting protein \nstructure and function for 29 years. Nucleic Acids Res. 2021;49(W1):W535–40.\n 71. Sehnal D, Bittrich S, Deshpande M, Svobodova R, Berka K, Bazgier V, et al. Mol* Viewer: modern web app for 3D \nvisualization and analysis of large biomolecular structures. Nucleic Acids Res. 2021;49(W1):W431–7.\n 72. Kauko A, Hedin LE, Thebaud E, Cristobal S, Elofsson A, von Heijne G. Repositioning of transmembrane alpha-helices \nduring membrane protein folding. J Mol Biol. 2010;397(1):190–201.\n 73. Wang F, Cvirkaite-Krupovic V, Baquero DP , Krupovic M, Egelman EH. Cryo-EM of A. pernix flagellum.\n 74. Liu Y, Qi X, Li X. Catalytic and inhibitory mechanisms of porcupine-mediated Wnt acylation.\n 75. Xie T, Chi X, Huang B, Ye F, Zhou Q, Huang J. Rational exploration of fold atlas for human solute carrier proteins. \nStructure. 2022.\n 76. Farci D, Haniewicz P , de Sanctis D, Iesu L, Kereiche S, Winterhalter M, et al. The cryo-EM structure of the S-layer \ndeinoxanthin-binding complex of Deinococcus radiodurans informs properties of its environmental interactions. J \nBiol Chem. 2022;298(6):102031.\n 77. Dolan KA, Kern DM, Kotecha A, Brohawn SG. Cryo-EM structure of SARS-CoV-2 M protein in lipid nanodiscs.\n 78. Pieper U, Schlessinger A, Kloppmann E, Chang GA, Chou JJ, Dumont ME, et al. Coordinating the impact of structural \ngenomics on the human alpha-helical transmembrane proteome. Nat Struct Mol Biol. 2013;20(2):135–8.\n 79. Kloppmann E, Punta M, Rost B. Structural genomics plucks high-hanging membrane proteins. Curr Opin Struct Biol. \n2012;22(3):326–32.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computational biology",
  "concepts": [
    {
      "name": "Computational biology",
      "score": 0.6489036083221436
    },
    {
      "name": "Transmembrane protein",
      "score": 0.5988809466362
    },
    {
      "name": "DNA microarray",
      "score": 0.5304768681526184
    },
    {
      "name": "Computer science",
      "score": 0.5233908295631409
    },
    {
      "name": "Biology",
      "score": 0.3995409607887268
    },
    {
      "name": "Natural language processing",
      "score": 0.3753381371498108
    },
    {
      "name": "Bioinformatics",
      "score": 0.33459922671318054
    },
    {
      "name": "Genetics",
      "score": 0.309501051902771
    },
    {
      "name": "Gene",
      "score": 0.15001070499420166
    },
    {
      "name": "Gene expression",
      "score": 0.07818928360939026
    },
    {
      "name": "Receptor",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ],
  "cited_by": 89
}