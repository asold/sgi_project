{
  "title": "One Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks",
  "url": "https://openalex.org/W4403564662",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5074151977",
      "name": "Anastasiia Birillo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5015564606",
      "name": "Elizaveta Artser",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5093800119",
      "name": "Anna Potriasaeva",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5077202915",
      "name": "Ilya Vlasov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5106342298",
      "name": "Katsiaryna Dzialets",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5013316832",
      "name": "Yaroslav Golubev",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102373030",
      "name": "Igor Gerasimov",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5052659996",
      "name": "Hieke Keuning",
      "affiliations": [
        "Utrecht University"
      ]
    },
    {
      "id": "https://openalex.org/A5065316896",
      "name": "Timofey Bryksin",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4401391312",
    "https://openalex.org/W4313894684",
    "https://openalex.org/W2962829286",
    "https://openalex.org/W3113108648",
    "https://openalex.org/W2344752727",
    "https://openalex.org/W4321120957",
    "https://openalex.org/W4396833177",
    "https://openalex.org/W2728220450",
    "https://openalex.org/W2894540915",
    "https://openalex.org/W4390621504",
    "https://openalex.org/W4394929444",
    "https://openalex.org/W4391584331",
    "https://openalex.org/W4392563703",
    "https://openalex.org/W4395029328",
    "https://openalex.org/W4288076218",
    "https://openalex.org/W2964148749",
    "https://openalex.org/W3089554664",
    "https://openalex.org/W4388691793",
    "https://openalex.org/W4390315357",
    "https://openalex.org/W2604398661",
    "https://openalex.org/W4390490752",
    "https://openalex.org/W1976220211",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W4393967925",
    "https://openalex.org/W4400833413",
    "https://openalex.org/W2128715468"
  ],
  "abstract": "Students often struggle with solving programming problems when learning to code, especially when they have to do it online, with one of the most common disadvantages of working online being the lack of personalized help. This help can be provided as next-step hint generation, i.e., showing a student what specific small step they need to do next to get to the correct solution. There are many ways to generate such hints, with large language models (LLMs) being among the most actively studied right now. While LLMs constitute a promising technology for providing personalized help, combining them with other techniques, such as static analysis, can significantly improve the output quality. In this work, we utilize this idea and propose a novel system to provide both textual and code hints for programming tasks. The pipeline of the proposed approach uses a chain-of-thought prompting technique and consists of three distinct steps: (1) generating subgoals - a list of actions to proceed with the task from the current student's solution, (2) generating the code to achieve the next subgoal, and (3) generating the text to describe this needed action. During the second step, we apply static analysis to the generated code to control its size and quality. The tool is implemented as a modification to the open-source JetBrains Academy plugin, supporting students in their in-IDE courses. To evaluate our approach, we propose a list of criteria for all steps in our pipeline and conduct two rounds of expert validation. Finally, we evaluate the next-step hints in a classroom with 14 students from two universities. Our results show that both forms of the hints - textual and code - were helpful for the students, and the proposed system helped them to proceed with the coding tasks.",
  "full_text": "One Step at a Time: Combining LLMs and Static Analysis to\nGenerate Next-Step Hints for Programming Tasks\nAnastasiia Birillo\nJetBrains Research\nBelgrade, Serbia\nanastasia.birillo@jetbrains.com\nElizaveta Artser\nJetBrains Research\nMunich, Germany\nelizaveta.artser@jetbrains.com\nAnna Potriasaeva\nJetBrains Research\nBelgrade, Serbia\nanna.potriasaeva@jetbrains.com\nIlya Vlasov\nJetBrains Research\nBelgrade, Serbia\nilya.vlasov@jetbrains.com\nKatsiaryna Dzialets\nJetBrains\nMunich, Germany\nkatsiaryna.dzialets@jetbrains.com\nYaroslav Golubev\nJetBrains Research\nBelgrade, Serbia\nyaroslav.golubev@jetbrains.com\nIgor Gerasimov\nJetBrains\nBerlin, Germany\nigor.gerasimov@jetbrains.com\nHieke Keuning\nUtrecht University\nUtrecht, Netherlands\nh.w.keuning@uu.nl\nTimofey Bryksin\nJetBrains Research\nLimassol, Cyprus\ntimofey.bryksin@jetbrains.com\nAbstract\nStudents often struggle with solving programming problems when\nlearning to code, especially when they have to do it online, with\none of the most common disadvantages of working online being\nthe lack of personalized help. This help can be provided asnext-step\nhint generation, i.e., showing a student what specific small step they\nneed to do next to get to the correct solution. There are many ways\nto generate such hints, with large language models (LLMs) being\namong the most actively studied right now.\nWhile LLMs constitute a promising technology for providing\npersonalized help, combining them with other techniques, such as\nstatic analysis, can significantly improve the output quality. In this\nwork, we utilize this idea and propose a novel system to provide\nboth textual and code hints for programming tasks. The pipeline\nof the proposed approach uses a chain-of-thought prompting tech-\nnique and consists of three distinct steps: (1) generating subgoals —\na list of actions to proceed with the task from the current student’s\nsolution, (2) generating the code to achieve the next subgoal, and\n(3) generating the text to describe this needed action. During the\nsecond step, we apply static analysis to the generated code to con-\ntrol its size and quality. The tool is implemented as a modification\nto the open-source JetBrains Academy plugin, supporting students\nin their in-IDE courses.\nTo evaluate our approach, we propose a list of criteria for all\nsteps in our pipeline and conduct two rounds of expert validation.\nFinally, we evaluate the next-step hints in a classroom with 14\nstudents from two universities. Our results show that both forms of\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference’17, July 2017, Washington, DC, USA\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nthe hints — textual and code — were helpful for the students, and\nthe proposed system helped them to proceed with the coding tasks.\nCCS Concepts\n• Computing methodologies →Artificial intelligence; • Social\nand professional topics →Software engineering education ; •\nHuman-centered computing →Interactive systems and tools .\nKeywords\nProgramming Education, in-IDE learning, LLMs, Generative AI,\nNext-Step Hints\nACM Reference Format:\nAnastasiia Birillo, Elizaveta Artser, Anna Potriasaeva, Ilya Vlasov, Kat-\nsiaryna Dzialets, Yaroslav Golubev, Igor Gerasimov, Hieke Keuning, and Tim-\nofey Bryksin. 2024. One Step at a Time: Combining LLMs and Static Analysis\nto Generate Next-Step Hints for Programming Tasks. In . ACM, New York,\nNY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1 Introduction\nA popular format for learning programming is Massive Open On-\nline Courses (MOOCs) [ 16], which became particularly popular\nduring the pandemic [12]. However, while many studies show that\npersonalized feedback helps students learn the material and solve\nproblems faster [14, 23, 34, 40], it is hard to provide personalized\nhelp in MOOCs because of the large number of students. To provide\nthis help, the process needs to be automated, with many approaches\nto this being researched [19]. One way to provide help when work-\ning on a problem is next-step hint generation [19], the purpose of\nwhich is to suggest the next step for the student’s solution, leading\nthem to the final solution that passes all tests.\nIn the era of Large Language Models (LLMs), many works have\nexplored the possibility of applying LLMs in the context of feedback\ngeneration [9, 20, 23, 25, 35]. The authors of such works mainly\nuse the open API of popular LLMs such asgpt-3.5 [17, 23, 35] and\ngpt-4 [25], and utilize various prompt engineering techniques [30].\nSome works focus on tasks for beginners in Python [35] or C [17],\narXiv:2410.09268v1  [cs.SE]  11 Oct 2024\nConference’17, July 2017, Washington, DC, USA Birillo, Artser, Potriasaeva, Vlasov, Dzialets, et al.\nbut the majority offer universal approaches that are language-\nindependent [23, 25]. A next-step hint can be generated in different\nforms, from a simple text message [35] or the solution in (pseudo)\ncode [17] to a combination of both forms [40]. What all these stud-\nies have in common is that they use LLMs directly, without any\nadditional processing to check the LLM output. This could affect\nthe accuracy of the results, as LLMs often give incorrect output due\nto hallucinations, limited context, and probabilistic nature [24, 31].\nRecently, in-IDE learning has been described as a new possible\nlearning format for MOOCs [11]. The main purpose of this approach\nis to integrate the learning process with professional Integrated\nDevelopment Environments (IDEs), helping students learn not only\nhow to program but also how to use IDE features when coding.\nThe format was implemented as an open-source JetBrains Academy\nplugin [5] — an extension of JetBrains IDEs, such as IntelliJ IDEA [3],\nPyCharm [7], or CLion [1]. However, this learning format does not\ncurrently offer personalized help to students.\nIn this work, we present a new approach to next-step hint gen-\neration that combines the power of static analysis and state-of-the-\nart LLMs to achieve better hint quality. We applied the chain-of-\nthought prompting approach [ 39], which splits the prompt into\nmultiple smaller prompts chained together. The final pipeline con-\nsists of three steps: (1) generating subgoals — a list of actions to\nproceed with the task from the current student’s solution, (2) gen-\nerating the code to achieve the next subgoal, and (3) generating\nthe text to describe this needed action. Since recent research has\nshown that providing just one level of help is not enough [40], we\nprovide help in two forms — textual hints and code hints — with\nthe student choosing the desired type. During code generation, we\nemploy static analysis to control the size of the hint using several\nheuristics, as well as its code quality — using inspections [18]. In\nthe proposed approach, we use in-IDE static analysis, but it can be\nreplaced with any other static analysis tools outside of the IDE.\nTo evaluate the proposed hint system, we implemented it as\nan extension of the JetBrains Academy plugin, since it is open-\nsource and publicly available. We conducted two rounds of expert\nvalidation to ensure the system’s quality based on the proposed\nlists of criteria for the different steps in our pipeline. Finally, we\nevaluated the next-step hints in a classroom with 14 students from\ntwo universities, demonstrating their usefulness.\nThe rest of the paper is organized as follows. Section 2 describes\nthe related work and the in-IDE learning format. Section 3 describes\nhow the proposed hint system works from the perspective of the\nstudents, while Section 4 describes how it works under the hood.\nSection 5 explains how we designed the system and presents the in-\nternal validation that we used to achieve high quality, and Section 6\ndescribes the evaluation on students. Finally, Section 7 identifies\npossible limitations of this study, and Section 8 concludes the work.\n2 Background\n2.1 Personalized Feedback\nFeedback generation is a common way for providing personalized\nhelp to students during the learning process [14, 23, 34, 40]. One of\nthe possible ways to provide such help is next-step hint generation ,\ni.e., showing a student what specific small step they need to do next.\nThere are many ways to generate such hints [19], from predefined\nrules or templates [15] to using systems based on previous student\nsubmissions [33, 34] or large language models (LLMs) [ 9, 23, 25,\n35] that have recently become popular. LLM-based systems allow\nproviding immediate feedback to a lot of students without involving\nthe teacher and, compared to data-driven approaches, do not require\ngathering a large amount of data on the student submissions. This\nsection provides an overview of recent approaches to generating\nnext-step hints with LLMs.\nStAP-tutor [35] is a web-based application that gives students a\ntextual next-step hint in case they get stuck when solving program-\nming problems. The tool uses the gpt-3.5 model under the hood\nand provides hints for Python programs at the click of a hint button.\nThe approach focuses on a small number of simple exercises and has\nnot been tested with more complex tasks. The other disadvantage\nof this tool is that it only provides a textual hint, whereas according\nto a recent study [40], students might need several levels of hints\nfor better understanding.\nRecently, Liffiton et al. developed CodeHelp [23]. Like StAP-tutor,\nthis tool only generates textual hints, but in this case, students\nneed to write their own questions to prompt the LLM and provide\nthe context information in a special form to get help. The tool\nseparates the student input into a code part, an error message,\nand the student question to build the final prompt for the LLM\nin a more structured way under the hood. The response consists\nof a detailed text explanation of the next step that the student\nshould proceed with. The authors use a combination of thegpt-3.5\nand davinci models for output processing to remove any code\nfrom the resulting hints. They also added a sufficiency check to\nnotify students in case their prompt is too vague. One of the main\ndrawbacks is that students in this system have to formulate their\nown specific questions, which can be challenging, especially for\nnovices [21].\nCodeAid [17] is a web-based application that allows students to\nnot only find mistakes in the code, but also answer general questions\nand explain the code. Similar to the previous tools, the hint system\nutilizes the gpt-3.5 model and provides hints for programs in\nC. The hints themselves are generated as text and pseudocode to\nsupport students in transitioning from understanding concepts to\nindependently writing their code. In the paper, the authors noted\nthat first generating the correct code and only then the textual hint\ngreatly improves quality and accuracy. The main disadvantages of\nthis tool are that it can be used only with simple tasks and that\nstudents must write their own questions.\nUnlike the above-mentioned tools, CS50.ai [25], introduced by\nLiu et al., is not only a web-based application but also has a plugin\nintegrated with Visual Studio Code [8]. Getting help inside the IDE\nallows students to learn in a production environment that they will\nlater face in their work. To improve the quality of hints, the authors\nuse a more powerful gpt-4 model along with retrieval-augmented\ngeneration (RAG) [22], which reduces LLM hallucinations by in-\ntroducing new information to the LLM from external sources. As\na hint, the authors provide the students with a list of step-by-step\nactions that should be taken to solve the task. Like the previous\ntool, CS50.ai offers some additional features such as explaining\ncode and checking code style. The authors implemented a security\nmechanism to prevent the system’s abuse, but the tool still requires\nstudents to write their own questions as prompts.\nOne Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks Conference’17, July 2017, Washington, DC, USA\n1\n2\n3\nFigure 1: The UI of the JetBrains Academy plugin together with an example of the developed next-step hint: (1) the “Get hint”\nbutton, (2) textual hint, and (3) code hint.\nIn summary, the described works have tried different AI-based\ntechniques to provide students with the next-step hints in different\nforms — textual or textual together with code. As a recent study\nhas shown, the combination of different forms can be crucial for\nthe students’ understanding [ 40] and should thus be used. One\nof the biggest challenges in these works is related to the use of\nLLMs, as they often produce hallucinations and may ignore prompt\ninstructions [24, 31], which can directly affect the quality of the\nhints. This work addresses this challenge and uses static analysis as\na post-processing step to control hint size and code quality. Finally,\nthe chain-of-thought prompting technique [39] could help increase\nthe accuracy of the generated hints, but this technique has not yet\nbeen used in this type of systems.\n2.2 In-IDE Learning\nRecently, the new in-IDE learning format was presented [11] in the\nform of the open-source JetBrains Academy plugin [5], an exten-\nsion for JetBrains IDEs that allows studying programming using a\nprofessional environment. The plugin aims to bring a more realistic\nexperience to those practicing their programming skills [11]. There\nare already over 40 available courses in this format, with all of\nthem being free and any third-party educator being able to create\ntheir own course with this format. A course consists of theoretical\nand practical tasks. A theoretical task explains a concept in a text\nor a video form, while a practical task can be either a quiz or a\nprogramming assignment to be evaluated by a testing system. The\ntasks can be isolated from each other or form a single larger project\nwhere students have to write the code step-by-step. You can see an\nexample of such project in Figure 1, with the numbers in the top-\nright corner representing nine step-by-step tasks that the students\nsolve to complete the project. All of this makes the in-IDE learning\nformat a good environment for implementing a novel next-step hint\nsystem: the programming assignments can be complex, and the IDE\nconveniently provides several crucial features, such as automated\ntests and an API for static analysis.\n3 The Next-Step Hint System: Student’s\nPerspective\nIn this section, we describe how the proposed next-step hint system\nworks from the perspective of a student. In this work, we focused\non console applications as target tasks, which require not only cod-\ning the main logic of the program by implementing several related\nfunctions, but also implementing some helper functions like read-\ning and handling the user’s input. Such tasks are inherently more\ncomplex, they represent a more realistic studying environment but\nmay also require more help for the student. We target the tasks in\nthe Kotlin language, because Kotlin courses are among the most\npopular in the JetBrains Academy plugin [4].\nYou can see all the main elements of the next-step hint system\nin Figure 1. The entry point for the interaction with the system\nis a “Get Hint” button, which was added to the task panel of the\nexisting interface (see (1) in Figure 1). Pressing the button initiates\nthe generation of the next-step hint, which is provided in two\nforms — textual and code. The textual hint covers two levels of help\nconsidered by Xiao et al. [40]: (a) instrumental help, which informs\nstudents on what to do next in concise, descriptive sentences by\nshowing it in a purple window near the “Get Hint” button (see(2) in\nFigure 1); and (b) orientational help, which informs students where\nthey should focus their attention by highlighting the position in\nthe code editor. The textual hint also includes a “Show in code”\nbutton (see (2) in Figure 1), which provides access to the code hint.\nPressing this button opens a code-diff window — a window that\ndisplays the changes between two versions of source code (the\ncurrent student code and the code after applying changes from the\nConference’17, July 2017, Washington, DC, USA Birillo, Artser, Potriasaeva, Vlasov, Dzialets, et al.\nCreator's solution\nNo\nPrompt #3:  \nText hint \ngeneration\nError information\nYes\nIs a \nfunction \nshort?\nStudent’s code\nPrompt #2:  \nCode hint \ngeneration\nApply code \nquality fixes\nControl the hint size \nby static analysis\nCode processing\n1\nPrompt #1:  \nSubgoals  \ngeneration\nInitial state\n2 3\nRemoving \nextra changes\nFigure 2: The overall pipeline of the next-step hint system.\ncode hint), highlighting additions, deletions, and modifications for\neasier comparison (see (3) in Figure 1). This help represents the\nbottom-out help [40], which shows students the exact code they\nneed to write for the next step. The code-diff window also has\ntwo action buttons: an “Accept” button to accept the code hint and\nautomatically apply the suggested changes to the student’s code\nand a “Cancel” button to cancel the code hint.\nStudents may request a next-step hint at any time during the\ntask, except for when it is already solved. Since JetBrains Acad-\nemy is a MOOC platform where students engage and take courses\nvoluntarily [11], we did not consider limiting the number of hint\nrequests in the current version of the next-step hint system.\n4 The Next-Step Hint System: Internal Design\nIn this section, we describe how the proposed system works under\nthe hood. The system usesgpt-4 for all LLM interactions described\nfurther in this section. The details of why we designed the system\nthis way and what students think about the hints are discussed in\nSections 5 and 6, respectively.\n4.1 General Pipeline\nThe general pipeline of the proposed next-step hint system is pre-\nsented in Figure 2 and consists of three main stages: (1) generating\nsubgoals, (2) generating the code hint, and (3) generating the textual\nhint. The stage of generating subgoals aims to analyze the task and\nthe current student’s code, determine the path to the final solution,\nand break it down into a list of coding-related actions with sufficient\ngranularity — subgoals. The purpose of generating code and textual\nhints is to create the corresponding hints, taking into account the\ntask’s subgoals and the current state of the student’s solution. Let\nus consider each stage in more detail.\n4.2 Generating Subgoals\nLarge language models are probabilistic in nature and may ignore\nsome parts of prompts in their output [ 38], which may lead to\ngenerating hints that contain multiple next steps at once even when\nasked not to. To mitigate this effect, we introduced the generation\nof subgoals, which strives to break down the task into smaller,\nmanageable steps [29]. The key idea behind generating subgoals is\nto analyze the task in general first before generating a next-step hint,\nwhich depends on the specific state of the student’s solution. This\napproach ensures that the provided hints are finely granulated, thus\nmaintaining an optimal level of challenge and support. A similar\napproach, showing a list of next-step actions as a hint, was recently\npresented by Liu et al. [25] and showed good performance.\nThe final prompt for generating subgoals can be found in the\nsupplementary materials [10]. We prompt the LLM to generate an\nordered list of steps to solve the given task, providing six parameters\nwith specific details about the task:\n•Task description.\n•Set of signatures of functions that might be imple-\nmented, which is extracted from a model solution (i.e., solu-\ntion provided by the task creator) that is not visible to the\nstudent. We do not provide the entire model solution to the\nLLM to avoid bias towards the approach suggested by the\ntask creator [19].\n•Set of existing functions within the student’s code that\nhave already been implemented in the previous tasks of the\nlarger project and can thus be used in the solution. These\nfunctions are mentioned separately from the functions of the\nmodel solution so that the LLM can directly use the already\nimplemented ones.\n•Static predefined hints , optionally provided by the task’s\ncreator. These are intended to help students with common\ndifficulties, but they are not dynamic and do not depend on\nthe student’s solution.\n•List of theory topics introduced in the current project ,\nsuch as \"variables\", \"loops\", or \"functions\", to ensure that\nthe generated subgoals are aligned with the covered topics.\nThis information is extracted from the theoretical tasks in\nthe project, the names of which correspond to the coding\nconcepts that they teach.\n•Set of string literals extracted from the model solution ,\nwhich the students might use in their solutions. We added\nthis because we observed that GPT-like models often gen-\nerated strings close to the strings from the task description\nbut slightly rephrased. Since we focused on console appli-\ncations, using the exact string constants is often crucial for\ncompleting the task.\nOne Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks Conference’17, July 2017, Washington, DC, USA\nTo force the LLM to use built-in functions from the standard li-\nbraries of the required programming language, we explicitly specify\nthe language that should be used to solve the coding problem.\nDuring our experiments with prompting, we also observed that\nthe LLM often tended to make broad subgoals, not achieving the\ndesired granularity. We try to make each subgoal responsible for\nan independent small coding step, equivalent to a next-step hint.\nTo mitigate this broadness, we also specify the minimum number\nof subgoals to generate — at least 6. Introducing a lower bound\nfor the number of subgoals led to some of the generated subgoals\nbeing non-code-related, especially on the most straightforward\ntasks that require only a few steps to complete. For example, the\nLLM suggested to \"read the task carefully\", \"save the file, \" or \"run\nthe solution\" as subgoals. To avoid such subgoals being proposed as\na next-step hint, we ask the LLM to mark each subgoal as a \"code\"\nor \"no-code\" step, and then additionally remove any \"no-code\" steps\nfrom the final list as a post-processing of the LLM response. As a\nresult of this stage, we obtain a list of granular subgoals.\n4.3 Generating Code and Textual Hints\nThe next stage in our approach is generating the code hint and the\ntextual hint (see (2) and (3) in Figure 2). The code hint is generated\nfirst, followed by the textual hint (even though they are shown\nto the students in the reverse order). According to Kazemitabaar\net al. [17], generating the correct code for the next step initially,\nfollowed by the pseudo-code, proved to deliver significantly better\nresults. Upon comparing the validation results for different orders\nof generating hints, we reached the same conclusion (see Section 5\nfor more details).\n4.3.1 Code hint prompt. The final prompt for the code hint can be\nfound in the supplementary materials [10]. We prompt to generate a\nmodified version of the student’s code by submitting the generated\nlist of subgoals of the corresponding task and the student’s code.\nIf the student already tried to run the tests for the current solution,\nwe also provided the LLM with the reported errors, which was\ndemonstrated to enhance the quality of the LLM output [23].\n4.3.2 Code hint quality improvement. One of the main features of\nour approach is that we use static analysis to control the generated\ncode provided by the LLM and improve the resulting hint.\nRemoving extra changes . To generate a focused and relevant\nhint, we omit irrelevant suggestions, such as when the LLM tries\nto improve the quality of the code in functions that the student\nalready implemented in the previous tasks of the larger project.\nTo achieve this, we compare the current student solution with\nthe model solution and extract functions that should be added or\nchanged. We ignore suggested changes in other functions, and if\nthe LLM proposes to change several relevant functions, we only\nkeep the changes for the first one.\nHandling short functions . Now that we selected the specific\nfunction to which the hint should be applied, we check whether\nthis function is short or long. We believe that very short functions\nalready correspond to an appropriate size for one next-step hint\nand do not require further control. Moreover, for short functions\nwe propose using the model solution provided by the task creator\ninstead of the LLM output, as it is already suitable. We treat short\nfunctions differently because of the inherent unreliability of LLMs\ndue to their non-deterministic output and the potential for hallu-\ncination [24, 31]. The threshold for defining a function as short\nhas been empirically determined to be not more than three lines\nin its body. If the changed function is longer than this, it might\nrepresent a complex step and thus follow a different approach from\nthe one in the model solution, which is why in this case we use\nLLM-generated code that takes into account all the aspects of the\ncurrent student solution. However, for the hint to be concise and to\nprovide code with good quality, the LLM-generated code requires\nfurther control by static analysis, which we describe below.\nEnsuring code quality . One of the crucial aspects of the code\nbeing shown to students is its quality [18], however, the code gen-\nerated by LLMs may lack in this regard [ 26, 36]. To improve the\nquality of the generated code, we utilize the IDE’s code quality\ninspections that can be used for optimizing code and correcting\ncommon security and style issues. We selected 30 Kotlin inspec-\ntions, for which automatic fixes are available. For instance, one\ninspection transforms a comparison into a Kotlin range if possible,\ne.g., month >= 1 && month <= 12 into 1..12.\nControlling hint size . Finally, LLMs do not always compre-\nhend that only one step needs to be generated, even within one\nfunction. We believe that a good hint provides one logical action,\ne.g., create a function, use a for loop, etc., whereas the generated\ncode often contains several such steps,e.g., generating a function or\na for loop with the full body. Consequently, we decided to control\nthis process ourselves with static analysis from the IDE, however,\nit can be replaced with any other static analysis tool. We devel-\noped three general heuristics, each applied to six control structures.\nYou can find the full list of heuristics with detailed descriptions\nin the supplementary materials [ 10], and here we will consider\nspecific motivating examples presented in Figure 3. The first row\nillustrates the case when the LLM returned a new function to be im-\nplemented. Subsequently, we apply the heuristicAdditive Statement\nIsolation, resulting in the function definition being retained, while\nthe body is replaced with a TODO expression. The same approach\ncan be applied to other newly added constructs. The second row\nillustrates that several lines within the if construct were simultane-\nously changed in the generated code, involving both the condition\nand the body. After applying the Intrinsic Structure Modification\nFocus heuristic, we only keep the difference related to the condition\nand remove the remaining ones. In the last row, it can be again\nobserved that several lines of code have been changed in the if\nconstruct, but this time all of them in the body. Subsequently, the\nheuristic Internal Body Change Detection was applied, resulting in\nonly keeping the first modification (the print expression).\nApplying these heuristics, coupled with the initial generation of\nsubgoals, allows us to make sure that the proposed hint really does\nrepresent a single granular step towards the correct solution.\n4.3.3 Textual hint prompt. The final step of our pipeline is gener-\nating the textual hint (see (3) in Figure 2). Even though the textual\nhint is shown to the students first, it is generated after the code\nhint, as this generation order improves the overall quality of the\nhints [17]. The final prompt for the textual hint can be found in\nthe supplementary materials [10]. We prompt to generate a textual\nhint based on the given current student’s code and an improved\nConference’17, July 2017, Washington, DC, USA Birillo, Artser, Potriasaeva, Vlasov, Dzialets, et al.\nStudent’s code Generated code Code hint\nNot yet implemented\nfun isCorrectInput(userInput: String): Boolean {\n    if (userInput.length != 1) {\n        println(\"Incorrect length!\")\n        return false\n    }\n    return true\n}\nfun isCorrectInput(userInput: String): Boolean {\n    TODO(\"Not implemented yet”)\n}\nAdditive Statment \nIsolation\nfun isCorrectInput(userInput: String): Boolean {\n    if (userInput[0] !in ‘a’..’z’) {\n        println(\"Incorrect input!”)\n    }\n    return true\n}\nfun isCorrectInput(userInput: String): Boolean {\n    if (!userInput[0].isLetter()) {\n        println(\"Incorrect input!\")\n        return false\n    }\n    return true\n}\nfun isCorrectInput(userInput: String): Boolean {\n    if (!userInput[0].isLetter()) {\n        println(\"Incorrect input!”)\n    }\n    return true\n}\nIntrinsic Structure \nModiﬁcation Focus\nfun isCorrectInput(userInput: String): Boolean {\n    if (!userInput[0].isLetter()) {\n    }\n    return true\n}\nfun isCorrectInput(userInput: String): Boolean {\n    if (!userInput[0].isLetter()) {\n        println(\"Incorrect input!\")\n        return false\n    }\n    return true\n}\nfun isCorrectInput(userInput: String): Boolean {\n    if (!userInput[0].isLetter()) {\n        println(\"Incorrect input!”)\n    }\n    return true\n}\nInternal Body \nChange Detection\nFigure 3: Illustration of the impact of applying heuristics to reduce changes in student code. The green highlighting indicates\nthe changes that were kept for the code hint. The red highlighting indicates changes that were removed by the heuristic.\nversion of the code generated at the previous step . We instruct\nthe LLM to provide a brief textual instruction in an imperative form\nin the response, without including explanations and code into the\nresulting textual hint.\n5 Internal Validation\n5.1 Overview\nThis section provides an overview of the research process we car-\nried out to develop our solution. Section 5.2 describes a pilot UX\nstudy with students, the main goal of which was to find a conve-\nnient and efficient way to show the next-step hints. Section 5.3\ndescribes the process of designing and validating the prompt for\ngenerating subgoals. Section 5.4 describes the process of designing\nand validating the prompts for generating textual and code hints.\nAll validation data in this section is based on the “Kotlin On-\nboarding: Introduction” course [6], a Kotlin course for beginners in\nthe in-IDE learning format. The course covers basic programming\nconcepts, such as variables, conditional operators, loops, and func-\ntions. The course consists of six console projects, the description of\nwhich can be found in the supplementary materials [10]. In total,\nthe six projects contain 50 individual coding tasks.\nFor financial reasons, we used gpt-3.5 for all experiments in\nthis section, even though gpt-4 was used in the final version. Run-\nning all the intermediate queries for dozens of tasks and multiple\nrounds of validation resulted in a lot of requests. More importantly,\nthese experiments were aimed at finding high-level problems with\nprompts and fixing them. A recent study [32] demonstrated how\nmuch better gpt-4 performs in solving programming assignments,\nwhich is why we used it in the final version of our system. Section 6\ndescribes the evaluation on students that used the final version\nwith gpt-4, and we leave more detailed comparison of different\nmodels for future work.\n5.2 UX Design of The Next-Step Hint System\nIn the proposed next-step hint system, students are first shown\na textual hint, after which they may press a button to view the\ncode-diff window and apply the hint automatically. Since the IDE\nsetting is inherently complex and the in-IDE learning format is not\nyet well studied [11], it was not obvious to us how we should show\nhints to students. Therefore, we conducted a user experience (UX)\nstudy to identify a better solution that would work for different\ntasks and for students with different levels of experience. We did\nnot focus on the code hint, since the IDE already provides a good\nmechanism for code comparison that could be reused for our system,\nand thus focused our attention on how to display the textual hint.\nTo compare different options, we conducted 15-minute comparative\nusability interviews with nine students. We selected this method\nbecause it was simple yet highly informative [37]. The rest of the\nsection describes in detail the options proposed to students, the\nconducted study, and its results.\n5.2.1 Approaches to displaying a textual hint. We considered three\noptions to display a textual hint (see Figure 4):\n•Prototype A: Hint in the same context with the task .\nElement (1) shows the hint in the same context window as\nthe task description. The assumption for this prototype was\nthat it is convenient to keep all the information in the same\nplace: the task, the button, and the response.\n•Prototype B: Hint in the same context with the task +\nhighlighting the position . Element (1) together with (2)\nshows the second option, not only keeping the student in\ncontext with the task but also helping them find the place\nwhere the hint should be applied. The main assumption is\nthat students may have problems with navigating the code,\nespecially if there are several functions in the solution.\nOne Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks Conference’17, July 2017, Washington, DC, USA\n2\n3\n2\n3\n1\nFigure 4: Proposed UX options. Prototype A: a textual hint is shown in the Task descriptionpanel (1). Prototype B: a textual\nhint is shown in the Task descriptionpanel, and the line is highlighted where changes have to be made (1 + 2). Prototype C: a\ntextual hint is shown in the code editor and points to the location where changes have to be made (3).\n•Prototype C: Hint directly at the position . Element (3)\nshows the third option, which separates the task and the\nhint system and shows them independently. The idea is to\ndraw the student’s attention to the code, thus helping them\nsolve the problem.\n5.2.2 Interview design. We developed interactive Figma [2] pro-\ntotypes for all three approaches on three different tasks: a simple\n\"Hello, world\" example, adding a new function to the solution, and\nresolving an error in the solution with several functions. The par-\nticipants interacted with the prototypes and provided feedback in\nreal-time. Having tried all three options, each participant indicated\nthe one they found the most convenient and the reasons for this.\n5.2.3 Interview participants. A total of nine students from two uni-\nversities participated in the study. The age range of the students was\n18 to 23 years old. Seven students are getting a Bachelor’s degree,\nwhile two students are getting a Master’s degree. Six students self-\nreported as experienced in programming, while three self-reported\nas novices. The programming languages that the students were\nfamiliar with included Kotlin, C++, Java, and Python. Six students\nhad previous experience with the JetBrains Academy Plugin and\nthe in-IDE learning format, while three students had little to no\nsuch experience.\n5.2.4 Interview results. The results of the usability session with\nnine students indicated that Prototype B (Figure 4, (1) together\nwith (2)) was the most preferred, with six students selecting it.\nPrototype C (Figure 4, (3)) received two votes, and Prototype\nA (Figure 4, (1)) was the least preferred, with only one student\nselecting it. Among the key insights, we found that highlighting\nthe position in the code where changes need to be applied is crucial.\nRegarding Prototype C, the main problem for the students was\nthat the code overlapped with the hint panel, and the students\ncould not read some of the functions. Also, this design does not\nallow keeping the textual hint together with the code hint, which\nwas confusing for the students. Based on these results, we selected\nPrototype B for the final version.\nHaving selected the preferred UX, we moved on to the expert\nvalidation of different stages of our pipeline, which involved de-\nsigning the validation criteria based on the existing research and\nour experience, as well as conducting two rounds of manual expert\nlabeling. The following two sections describe this process in detail.\n5.3 Validating the Generation of Subgoals\nAs the first part of the validation, we carried out the expert valida-\ntion for the generation of subgoals.\n5.3.1 Validation criteria. Based on prior work [ 13, 28] and our\nexperience, we propose 8 criteria to validate the subgoal generation.\n•Amount criterion evaluates the total number of subgoals\ngenerated for a given task.\n•Specifics criterion assesses whether the subgoals are related\nto specific actions, such as creating variables, calling func-\ntions, or using coding constructs like loops or if statements,\nand not something broad like repeating the task statement.\n•Independence criterion checks that subgoals do not refer\nto each other and are independent, since we do not show\nthe list of subgoals in the final hint and the student will not\nunderstand the reference.\n•Coding-specific criterion checks if all the subgoals are re-\nlated directly to coding tasks, with a poor example being\n\"Think about the problem\". Even though we asked the LLM\nto mark non-code-specific actions and then removed them,\nthe LLM sometimes makes mistakes.\n•Direction criterion determines whether the subgoals collec-\ntively guide the student towards the correct solution.\n•Misleading information criterion identifies subgoals con-\ntaining incorrect guidance, such as non-existent functions\nor wrong constants.\nConference’17, July 2017, Washington, DC, USA Birillo, Artser, Potriasaeva, Vlasov, Dzialets, et al.\n•Granularity criterion examines if the subgoals are limited\nto a single action.\n•Idiomatic criterion evaluates whether the subgoals adhere\nto the idiomatic practices of the target programming lan-\nguage, in our case — Kotlin.\n5.3.2 Methodology. We conducted two rounds of validation by\nfour experts with up to 5 years of programming and teaching ex-\nperience. For each task in the studied Kotlin course, we took the\ninitial state of the solution at the start of solving and ran the first\nstep of our pipeline with the subgoal generation. Then, the experts\nindependently labeled the LLM output for each task, with each\nbeing labeled by at least two experts. The labeling of each list of\nsubgoals consisted of deciding whether it satisfies each validation\ncriterion. After labeling, all experts gathered and finalized their\ndecisions during an online meeting, reaching a consensus for each\ntask. Based on the results of the first round, consisting of a success\nrate of each criterion (i.e., the ratio of tasks, for which the generated\nsubgoals satisfied it), we implemented certain changes (described\nbelow) and conducted a second, final, round of validation.\n5.3.3 First round of validation. The first round of validating the\ngeneration of subgoals was conducted for all coding tasks from\nall six projects of the course (50 tasks in total). The results were\ngenerally positive. They demonstrated that the LLM is capable of\nhandling the task of subgoal generation, achieving a success rate of\nover 70% for most criteria. However, issues were identified for the\nfollowing criteria: Specifics (38% success rate), Coding-specific\n(52% success rate), andMisleading information (58% success rate).\nLet us consider some specific problems in more detail, and describe\nwhat changes we implemented to overcome them. An example of\nthe generated subgoals with these problems can be found in the\nsupplementary materials [10].\n•Limited recognition of built-in Kotlin functions . Sometimes the\nLLM provided an algorithm that does not suggest to use built-\nin functions. One of the popular ways to fix this problem is\nto use the retrieval-augmented generation (RAG) [22] tech-\nnique, which we leave for future work. The same approach\ncan be used to improve the performance on the Misleading\ninformation criterion.\n•Insufficient or excessive granularity . During our first iteration,\nwe discovered that the LLM could not accurately determine\nthe size of a subgoal, e.g., merging several logical subgoals\ninto a single big one or providing many subgoals not related\nto code. We applied several prompt adjustments to resolve\nthis issue:\n– Provided the LLM with a description of what a subgoal\nmeans [29];\n– Added an instruction to label all subgoals as code and\nno-code to then filter all no-code ones programmatically.\n•Suggesting redundant actions . The last problem was related\nto adding extra subgoals that were not asked for in the task,\ne.g., test or run your solution . It is related to the previous one\nand was solved by adding labels to each subgoal.\n5.3.4 Second round of validation. After making improvements, we\ncarried out the second round of validation. We randomly chose 16\nout of 50 tasks from the course (32%), representing all six projects.\nThe results of the second round showed significant improvements in\nthe criteria that were problematic in the first round. The success rate\nfor Specifics increased from 38% to 75%, and for Coding-specific\n— from 52% to 81%. Additionally, we observed slight improvements\nin the Misleading information, Independence, and Idiomatic\ncriteria. The overall success rate for the remaining the criteria was\nover 70% as well.\n5.4 Validating the Generation of Hints\nThe second and third steps of our approach are generating code\nand textual hints (see Section 4.3). This section describes how these\nsteps were validated with experts. We carried out this validation\nfor both types of hints together, because they constitute different\nlevels of representation of the same hint.\n5.4.1 Validation criteria. In order to accurately validate the quality\nof the generated textual and code hints, we propose to use twelve\ncriteria, eight of which were taken and adapted from the litera-\nture [19, 35] and four taken from our experience and preliminary\nexperiments. The eight criteria from the work of Roest et al. [35]\nare as follows.\n•Feedback type criterion characterizes what kind of feed-\nback is generated, such as if the hint takes into account the\nknowledge about task constraints or explains to the student\nthe reasons why the hint was proposed. This criterion has\nseveral sub-criteria, their detailed descriptions can be found\nin the original work [35].\n•Information criterion identifies any additional information\nin the hints, which could potentially help the student to\nunderstand the hint better.\n•Level-of-detail criterion defines if the generated hint is a\nhigh-level description or a specific bottom-out hint.\n•Personalized criterion indicates if the hint refers to the\nstudent code and can be a logical extension of the current\nsolution.\n•Appropriate criterion generally describes if the hint is a\nsuitable next step, given the current state of the student\nprogram and the desired outcome.\n•Specific criterion checks the size of the hint and whether it\nis limited to a single step.\n•Misleading information criterion indicates if the hint con-\ntains misleading information, e.g., asks to use incorrect func-\ntions or undefined variables.\n•Length criterion for the textual hint is the measurement\nof the number of words and sentences. We included into\nthe criteria the Length of the code hint as well, which is\ncalculated in terms of the number of added, changed, and\ndeleted lines of code.\nWe introduce four new criteria:\n•Intersection criterion indicates whether the suggestion in\nthe hint is fully or partially implemented in the student’s\ncode. The aim of this criterion is to mitigate cases where\nhints repeat student’s code.\n•Code quality criterion indicates whether the generated\ncode is compilable, and does not have common code quality\nviolations such as incorrect parentheses or brackets.\nOne Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks Conference’17, July 2017, Washington, DC, USA\n•Idiomatic criterion is designed to check that the generated\ncode uses language-specific constructs,e.g., using therandom\nfunction from Kotlin to choose a random element from a list\nrather than using generic Java-like code with a loop.\n•Subgoals relevance criterion checks if the generated hint\nmatches with the list of subgoals proposed at the previous\nstep of the algorithm.\nThe full description of all criteria can be found in the supplemen-\ntary materials [10].\n5.4.2 Methodology. We conducted two rounds of validation by two\nexperts with up to 5 years of programming and teaching experience.\nTo check more varied hints, we collected real student submissions\nfor the studied Kotlin course and used them for hint generation.\nDuring each round of validation, we ran the entire hint genera-\ntion pipeline on each student submission. The labeling procedure\nwas the same as in Section 5.3.2, with two experts labeling each\ngenerated hint based on the proposed criteria and then reaching\nan agreement. Similarly, based on the success rate of various crite-\nria, certain changes were implemented, and the second round of\nvalidation was conducted.\n5.4.3 First round of validation. The first round of validating hints\nwas conducted on 24 submissions from 18 students across all six\nprojects from the course. We collected the submissions that did not\npass the tests, since this was one of the most appropriate places to\nshow the hints. In this round, we only considered 24 submissions,\nbecause after labeling them we found that the initial quality of the\napproach was too low, and we did not need to label any more data.\nWhile most criteria achieved a success rate of at least 60%, some\nhad notable issues. The Misleading information had the lowest\nrate at 27%, followed by Information at 32%, and Appropriate at\n41%. We highlighted several problems (see examples in Figure 5):\n•Unintended code modification . In the generated code, the LLM\noften modified code that should not have been modified (see\n(1)). The diff window shows that the proposed hint tries\nto modify the getHiddenSecret function implemented in the\nprevious task. These changes should not be pointed out to\nthe student since that part is already correct. As mentioned\nin Section 4.3, we applied static analysis to mitigate these\nproblems:\n– Using static analysis, we defined the list of functions that\nneed to be implemented in the current task and then ana-\nlyzed the generated code to include only these functions\nin the hint.\n•Premature disclosure of the full solution . We discovered that\nthe LLM often ignores the size of the step provided by a\nsubgoal and suggests a large step (see(2)). To fix this problem,\nwe also applied static analysis:\n– We developed three general heuristics, each applied to six\ncontrol structures (see Section 4.3). This helped us manu-\nally control the code hint size and avoid recommending\nlarge constructions in one hint.\n•Unwanted procedural comments in code . Sometimes, the LLM\nadded comments into the hint that referred to the previous\nsubgoals from the list of subgoals (see (3)). Since we do not\nshow the entire list of subgoals to the students, this can\nconfuse them. We applied static analysis to fix this problem\nas well:\n– After the code hint is generated, we analyze the generated\ncode and remove all comments. Since the code hint is\naccompanied by a textual hint, and we manually control\nthe size of the hint, we assume that we do not need to\ninclude additional comments in the code.\n•Excessive detail in textual hint . This problem was connected\nto the size of the textual hint (see (4)). To control the size of\nthe hint, we changed our approach:\n– We changed the order in which the hint is produced, gen-\nerating the code first, and the text second, as was done in\nthe recent work [17].\n– We changed the prompt for generating the textual hint\nand removed the word explain.\n•Limited recognition of built-in Kotlin functions . Similar to\ngenerating subgoals, we noticed that the LLM often ignores\nbuilt-in Kotlin functions (see (5)). To mitigate this issue, we\napplied the following change:\n– We added a heuristic for short functions (see Section 4.3),\nrecommending the solution pre-written by the course cre-\nator instead of the LLM-generated one for functions not\nlarger than three lines of code. We assume that the course\ncreator uses optimal language constructions in their so-\nlution. We chose the threshold of three lines empirically,\nbased on the experience of the experts, and left the detailed\nexperiments for future work.\n5.4.4 Second round of validation. After making the improvements,\nwe carried out the second, final, round of validation. Since our\ngoal is to provide hints not only for errors, we extended the data\nwith different stages of students’ solutions. For that, we adopted\nthe TaskTracker tool [ 27] for our experimental setup. The tool\ncollects all intermediate changes during the course of solving the\ngiven task, allowing us to see different stages of solutions, not only\nafter failing the tests. In total, we collected the data from eight\nfirst-year Bachelor’s students solving all 50 tasks in the “Kotlin\nOnboarding: Introduction” course. Among the intermediate stages\nof their solutions, we randomly extracted 48 compilable versions,\nwith some at the start of the solving process and some very close\nto the end.\nThe results of the second round of validation showed a significant\nimprovement in the hint quality among various criteria. First, the\nAppropriate criterion increased from 41% to 54%. The final success\nrate of the Subgoals relevance criterion also stands significantly\nhigh at 79%. Additionally, we observed a slight improvement in\nMisleading information. Overall, all problems described in the\nprevious sub-section and shown in Figure 5 were largely solved\nand did not manifest themselves in the final version of the tool.\n6 Evaluation with Students\nAfter the proposed approach was developed and validated with\nexperts, we conducted an evaluation with students using the hint\nsystem. The primary objective of this evaluation was to obtain\ntheir feedback regarding the usefulness and the ease of use of the\nprovided hints in a classroom. For this evaluation, we used the final\nnext-step hint system described in Section 4, equipped with gpt-4.\nConference’17, July 2017, Washington, DC, USA Birillo, Artser, Potriasaeva, Vlasov, Dzialets, et al.\n1\n2\n3\n5\n4\nFigure 5: Example problems in the hint: (1) Unintended code modification, (2) Premature disclosure of the full solution, (3)\nUnwanted procedural comments in code, (4) Excessive detail in texual hint, (5) Limited recognition of built-in Kotlin functions.\n6.1 Methodology\nThe evaluation consisted of two components. Firstly, we conducted\nthe analysis of log data obtained by the adapted TaskTracker tool [27].\nThe tool logs all IDE actions performed when solving the tasks,\nas well as all internal LLM-related data (prompts, inputs, outputs\nof all intermediate stages). For this, 14 students from two univer-\nsities installed the TaskTracker tool and the improved version of\nthe JetBrains Academy plugin with the hint system. Then, they\ncompleted at least five out of six projects in the “Kotlin Onboarding:\nIntroduction” course [6]. There were no restrictions placed on using\nthe “Get hint” button. After the students completed the projects,\nwe conducted the second part of the evaluation — a qualitative\nassessment through a survey that included open-ended questions\nabout their interactions with the hints.\nAll students agreed to submit their data via the TaskTracker tool.\nWe only collected data from the files that were created for solving\nthe tasks, and anonymized all the personal data.\n6.2 Results\n6.2.1 How often do students use the hint generation system? To\nanswer this question, we conducted a quantitative analysis of the\nTaskTracker data that we collected.\nAmong all students and all tasks, 191 hints were requested, of\nwhich 101 times students also asked to show the code hint (52.9%).\nThe distribution of requests among projects was not uniform, for\nexample, only 13 hints were requested in the first two basic projects\nabout reading variables and printing them to the console. This be-\nhavior was expected, since all of the students were already familiar\nwith the basic programming concepts, which was enough to solve\nthe first two projects without help. Students only requested the code\nhint in about half the cases, and in the other half, it was sufficient\nfor them to see only a textual description of a possible next step.\nThese observations match well with the work of Xiao et al. [ 40],\nwhere it is shown that there are differences between students, with\nsome requiring help beyond a short explanation on what to do next\nwith a textual hint.\nOut of 101 code hints shown, 65 were accepted by clicking the\n“Accept hint” button (64.4%). It should be noted that we did not\nanalyze the code snippets before and after the code hint was shown\nto find cases where students did not accept the hint but applied the\nsame changes manually. This will be done as part of future work,\nbecause it might highlight different student behaviour patterns.\nOut of 191 next-step hints (both textual and code), 47 were regen-\nerated, i.e., the student requested the hint again without accepting\nthe previous one or changing the code. This highlights the value of\nusing LLMs as opposed to the solution provided by the task creator,\nsince LLMs can generate different suggestions from the same state.\n6.2.2 How do students perceive the proposed hint system? To an-\nswer this question, we conducted a survey.\nReasons to ask for hints. We asked students about the reasons\nthey had for requesting a hint. The most popular responses were I\nwas curious (9 responses) and My solution did not pass the tests (7\nresponses). The options Didn’t know what to do next and Compila-\ntion errors were also indicated by five students. This confirms that\nthe proposed hint system has been used for its purpose, although\nthere is room for improvement in generating hints for compilation\nerrors. One of the students shared an unusual scenario where they\ncompleted the entire project using only the suggested hints. This\nindicates the importance of future considerations on whether the\nnumber or the frequency of the allowed hints should be limited.\nOne Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks Conference’17, July 2017, Washington, DC, USA\nUnclear hints. To better understand the students’ behavior, we\nasked them about their actions when the generated hint was unclear.\nSurprisingly, most of the responses (11 out of 14) indicated that the\nstudents tried to solve the task themselves, without any extra help\nsuch as Google or resources such as ChatGPT. The second popular\naction (7 responses) was regenerating hints. This behavior of stu-\ndents shows the importance of high accuracy in hint generation,\notherwise many students will not trust such a system and will try\nto solve problems without additional help.\nHints representation. Half of the students (7 out of 14) prefer\nto see the combination of textual and code hints, while 6 out of 14\nnoted they would prefer to have code hints even if the textual hints\nare omitted. This indicates the importance of having several levels\nof hints, but again raises the questions about limiting the access to\nthem. We will consider these limitations in future work.\nGeneral feedback. In general, the proposed hint system was\nwell received by students — 8 out of 14 indicated they would con-\ntinue using the proposed system, with most of them being novices\nwith little programming experience. The proposed system is useful\nfor them, because they do not know exactly what to ask ChatGPT\nor Google. At the same time, more experienced students reported\nthat they lacked conversations in this system as they could not\nask specific questions. This indicates the importance of combining\napproaches such as ours and approaches like CS50.ai [25] with a\nchat-based assistant to support students of all experience levels.\n7 Limitations & Discussion\nUser experience study. In order to design the user interface of the\nproposed system, we conducted a short interview study with nine\nstudents. It is possible that the sample of students is not represen-\ntative enough or that not all possible UI options were considered.\nIt is also possible that the option most preferred by the students is\nnot the most useful for their learning progress. In the future, it is\nnecessary to conduct more thorough analysis of different potential\nways to demonstrate the hints to the students.\nEvaluation with students. We conducted the evaluation with\nonly 14 students in a classroom, collecting their usage and feedback\nabout the system. However, we did not conduct a detailed com-\nparison of how students perform with and without the proposed\nnext-step hints and whether the students actually learned from the\nhints. This constitutes the most important part of our future work.\nGeneralizability. We implemented the hint system as part of\nthe JetBrains Academy plugin for Kotlin courses. While this lim-\nits the range of possible applications, our approach is generally\nlanguage-independent and can be extended to other courses for\nother programming languages, as well as other environments such\nas Visual Studio Code. To support another language, the proposed\nstatic analysis techniques should be re-implemented for it, e.g., the\nsize heuristics and the code quality inspections. However, a lot of\nexisting tools and IDEs can provide such functionality.\n8 Conclusion\nIn this work, we presented a next-step hint system that provides\ntextual and code hints to students. The system combines state-\nof-the-art LLMs and static analysis to create high-quality next-\nstep hints. The system implements a chain-of-thought approach\nand consists of three distinct stages: (1) generating subgoals, i.e.,\na list of steps that need to be taken to solve the current task, (2)\ngenerating the code hint that would implement the next subgoal,\nand (3) generating the textual hint to explain the necessary change\nin a concise manner. During the generation of code hints, we used\nstatic analysis to control their size and code quality.\nWe implemented the proposed hint system as part of the open-\nsource JetBrains Academy plugin. We extended the existing inter-\nface with a new “Get Hint” button that aims to display the generated\nhint in two stages. First, we show the textual hint and point to the\nlocation in the student’s code where the changes should be applied.\nThen, as an optional step, we provide the student with a code hint\nthat can be accepted or rejected. We carried out several rounds of\ninternal validation for each step of the proposed pipeline, which\nallowed us to improve the quality of the final solution.\nFinally, we carried out an initial evaluation with 14 students in a\nclassroom. The students actively used the proposed system to get\nhelp, and the results indicated the importance of having different\nforms of hints for different students. Overall, the proposed next-\nstep hint generation system demonstrates potential, and we plan\nto incorporate it more fully into the learning process and conduct\ndetailed studies on the value it brings to the students.\nReferences\n[1] 2024. CLion. Retrieved September 26, 2024 from https://www.jetbrains.com/clion/\n[2] 2024. Figma. Retrieved September 26, 2024 from https://www.figma.com/\n[3] 2024. IntelliJ IDEA . Retrieved September 26, 2024 from https://www.jetbrains.\ncom/idea/\n[4] 2024. JetBrains Academy Marketplace . Retrieved September 26, 2024 from\nhttps://plugins.jetbrains.com/education\n[5] 2024. JetBrains Academy Plugin . Retrieved September 26, 2024 from https:\n//plugins.jetbrains.com/plugin/10081-jetbrains-academy\n[6] 2024. Kotlin Onboarding: Introduction . Retrieved September 26, 2024 from\nhttps://plugins.jetbrains.com/plugin/21067-kotlin-onboarding-introduction\n[7] 2024. PyCharm. Retrieved September 26, 2024 from https://www.jetbrains.com/\npycharm/\n[8] 2024. Visual Studio Code . Retrieved September 26, 2024 from https://code.\nvisualstudio.com/\n[9] Kehinde Aruleba, Ismaila Temitayo Sanusi, George Obaido, and Blessing Og-\nbuokiri. 2023. Integrating ChatGPT in a Computer Science Course: Students\nPerceptions and Suggestions. arXiv preprint arXiv:2402.01640 (2023).\n[10] Anastasiia Birillo, Elizaveta Artser, Anna Potriasaeva, Ilya Vlasov, Katsiaryna\nDzialets, Yaroslav Golubev, Igor Gerasimov, Hieke Keuning, and Timofey Bryksin.\n2024. Supplementary materials. Retrieved September 26, 2024 from https://zenodo.\norg/records/12584502\n[11] Anastasiia Birillo, Mariia Tigina, Zarina Kurbatova, Anna Potriasaeva, Ilya Vlasov,\nValerii Ovchinnikov, and Igor Gerasimov. 2024. Bridging Education and Develop-\nment: IDEs as Interactive Learning Platforms. In Proceedings of the 1st ACM/IEEE\nWorkshop on Integrated Development Environments . 53–58.\n[12] Romana Emilia Cramarenco, Monica Ioana Burcă-Voicu, and Dan-Cristian Dabija.\n2023. Student Perceptions of Online Education and Digital Technologies During\nthe COVID-19 Pandemic: A Systematic Review. Electronics 12, 2 (2023), 319.\n[13] Adrienne Decker, Lauren E Margulieux, and Briana B Morrison. 2019. Using the\nSOLO Taxonomy to Understand Subgoal Labels Effect in CS1. InProceedings of the\n2019 ACM Conference on International Computing Education Research . 209–217.\n[14] Galina Deeva, Daria Bogdanova, Estefanía Serral, Monique Snoeck, and Jochen\nDe Weerdt. 2021. A Review of Automated Feedback Systems for Learners: Classi-\nfication Framework, Challenges and Opportunities. Computers & Education 162\n(2021), 104094.\n[15] Alex Gerdes, Bastiaan Heeren, Johan Jeuring, and L Thomas Van Binsbergen.\n2017. Ask-Elle: An Adaptable Programming Tutor for Haskell Giving Automated\nFeedback. International Journal of Artificial Intelligence in Education 27 (2017),\n65–100.\n[16] Irwanto Irwanto, Dwi Wahyudiati, Anip Dwi Saputro, and Isna Rezkia Lukman.\n2023. Massive Open Online Courses (MOOCs) in Higher Education: A Bibliomet-\nric Analysis (2012-2022). IJIET: International Journal of Information and Education\nTechnology 13, 2 (2023), 223–231.\n[17] Majeed Kazemitabaar, Runlong Ye, Xiaoning Wang, Austin Zachary Henley,\nPaul Denny, Michelle Craig, and Tovi Grossman. 2024. CodeAid: Evaluating a\nConference’17, July 2017, Washington, DC, USA Birillo, Artser, Potriasaeva, Vlasov, Dzialets, et al.\nClassroom Deployment of an LLM-based Programming Assistant That Balances\nStudent and Educator Needs. In Proceedings of the CHI Conference on Human\nFactors in Computing Systems . 1–20.\n[18] Hieke Keuning, Bastiaan Heeren, and Johan Jeuring. 2017. Code Quality Issues\nin Student Programs. In Proceedings of the 2017 ACM Conference on Innovation\nand Technology in Computer Science Education . 110–115.\n[19] Hieke Keuning, Johan Jeuring, and Bastiaan Heeren. 2018. A Systematic Literature\nReview of Automated Feedback Generation for Programming Exercises. ACM\nTransactions on Computing Education (TOCE) 19, 1 (2018), 1–43.\n[20] Natalie Kiesler, Dominic Lohr, and Hieke Keuning. 2023. Exploring the Potential\nof Large Language Models to Generate Formative Programming Feedback. In\n2023 IEEE Frontiers in Education Conference (FIE) . 1–5.\n[21] Nils Knoth, Antonia Tolzin, Andreas Janson, and Jan Marco Leimeister. 2024. AI\nLiteracy and Its Implications for Prompt Engineering Strategies. Computers and\nEducation: Artificial Intelligence 6 (2024), 100225.\n[22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin,\nNaman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al.\n2020. Retrieval-Augmented Generation for Knowledge-Intensive NLP tasks.\nAdvances in Neural Information Processing Systems 33 (2020), 9459–9474.\n[23] Mark Liffiton, Brad E Sheese, Jaromir Savelka, and Paul Denny. 2023. CodeHelp:\nUsing Large Language Models with Guardrails for Scalable Support in Program-\nming Classes. In Proceedings of the 23rd Koli Calling International Conference on\nComputing Education Research . 1–11.\n[24] Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li\nZhang. 2024. Exploring and Evaluating Hallucinations in LLM-powered Code\nGeneration. arXiv preprint arXiv:2404.00971 (2024).\n[25] Rongxin Liu, Carter Zenke, Charlie Liu, Andrew Holmes, Patrick Thornton, and\nDavid J Malan. 2024. Teaching CS50 with AI: Leveraging Generative Artificial\nIntelligence in Computer Science Education. In Proceedings of the 55th ACM\nTechnical Symposium on Computer Science Education V. 1 . 750–756.\n[26] Zhijie Liu, Yutian Tang, Xiapu Luo, Yuming Zhou, and Liang Feng Zhang. 2024.\nNo Need to Lift a Finger Anymore? Assessing the Quality of Code Generation\nby ChatGPT. IEEE Transactions on Software Engineering 50, 6 (2024), 1548–1584.\n[27] Elena Lyulina, Anastasiia Birillo, Vladimir Kovalenko, and Timofey Bryksin. 2021.\nTaskTracker-tool: A Toolkit for Tracking of Code Snapshots and Activity Data\nDuring Solution of Programming Tasks. InProceedings of the 52nd ACM Technical\nSymposium on Computer Science Education . 495–501.\n[28] Lauren E Margulieux, Briana B Morrison, and Adrienne Decker. 2019. Design\nand Pilot Testing of Subgoal Labeled Worked Examples for Five Core Concepts\nin CS1. In Proceedings of the 2019 ACM Conference on Innovation and Technology\nin Computer Science Education . 548–554.\n[29] Lauren E Margulieux, Briana B Morrison, Baker Franke, and Harivololona Ramil-\nison. 2020. Effect of Implementing Subgoals in Code.org’s Intro to Programming\nUnit in Computer Science Principles. ACM Transactions on Computing Education\n(TOCE) 20, 4 (2020), 1–24.\n[30] Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-\nNabende. 2023. Prompt Engineering in Large Language Models. In International\nConference on Data Intelligence and Cognitive Informatics . 387–402.\n[31] Timothy R. McIntosh, Tong Liu, Teo Susnjak, Paul Watters, Alex Ng, and Malka N.\nHalgamuge. 2024. A Culturally Sensitive Test to Evaluate Nuanced GPT Halluci-\nnation. IEEE Transactions on Artificial Intelligence 5, 6 (2024), 2739–2751.\n[32] James Prather, Paul Denny, Juho Leinonen, Brett A Becker, Ibrahim Albluwi,\nMichelle Craig, Hieke Keuning, Natalie Kiesler, Tobias Kohn, Andrew Luxton-\nReilly, et al. 2023. The Robots Are Here: Navigating the Generative AI Revolution\nin Computing Education. In Proceedings of the 2023 Working Group Reports on\nInnovation and Technology in Computer Science Education . 108–159.\n[33] Thomas W. Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: Towards\nIntelligent Tutoring in Novice Programming Environments. In Proceedings of the\n2017 ACM SIGCSE Technical Symposium on Computer Science Education . 483–488.\n[34] Kelly Rivers and Kenneth R Koedinger. 2013. Automatic Generation of Program-\nming Feedback: A Data-Driven Approach. In The First Workshop on AI-supported\nEducation for Computer Science (AIEDCS 2013) , Vol. 50. 50–59.\n[35] Lianne Roest, Hieke Keuning, and Johan Jeuring. 2024. Next-Step Hint Generation\nfor Introductory Programming Using Large Language Models. In Proceedings of\nthe 26th Australasian Computing Education Conference . 144–153.\n[36] Florian Tambon, Arghavan Moradi Dakhel, Amin Nikanjam, Foutse Khomh,\nMichel C Desmarais, and Giuliano Antoniol. 2024. Bugs in Large Language\nModels Generated Code. arXiv preprint arXiv:2403.08937 (2024).\n[37] Lex van Velsen, Thea van der Geest, and Rob Klaassen. 2007. Testing the Usability\nof a Personalized System: Comparing the Use of Interviews, Questionnaires and\nThinking-Aloud. In 2007 IEEE International Professional Communication Confer-\nence. 1–8.\n[38] Albert Webson and Ellie Pavlick. 2021. Do Prompt-based Models Really Under-\nstand the Meaning of Their Prompts? arXiv preprint arXiv:2109.01247 (2021).\n[39] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-Thought Prompting Elicits Rea-\nsoning in Large Language Models. Advances in neural information processing\nsystems 35 (2022), 24824–24837.\n[40] Ruiwei Xiao, Xinying Hou, and John Stamper. 2024. Exploring How Multiple\nLevels of GPT-Generated Programming Hints Support or Disappoint Novices. In\nExtended Abstracts of the CHI Conference on Human Factors in Computing Systems .\n1–10.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8319627046585083
    },
    {
      "name": "Pipeline (software)",
      "score": 0.6887775659561157
    },
    {
      "name": "Plug-in",
      "score": 0.6758285164833069
    },
    {
      "name": "Code (set theory)",
      "score": 0.5949302911758423
    },
    {
      "name": "Task (project management)",
      "score": 0.5779511332511902
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5478271842002869
    },
    {
      "name": "Software engineering",
      "score": 0.45927056670188904
    },
    {
      "name": "Programming language",
      "score": 0.4318188726902008
    },
    {
      "name": "Artificial intelligence",
      "score": 0.333936870098114
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.10530436038970947
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ]
}