{
  "title": "Compact Transformer Tracker with Correlative Masked Modeling",
  "url": "https://openalex.org/W4382461147",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2135272017",
      "name": "Zi-Kai Song",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2134869736",
      "name": "Run Luo",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110723150",
      "name": "Junqing Yu",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4202225942",
      "name": "Yi-Ping Phoebe. Chen",
      "affiliations": [
        "La Trobe University"
      ]
    },
    {
      "id": "https://openalex.org/A1987581894",
      "name": "Wei Yang",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135272017",
      "name": "Zi-Kai Song",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2134869736",
      "name": "Run Luo",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2110723150",
      "name": "Junqing Yu",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4202225942",
      "name": "Yi-Ping Phoebe. Chen",
      "affiliations": [
        "La Trobe University"
      ]
    },
    {
      "id": "https://openalex.org/A1987581894",
      "name": "Wei Yang",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2470394683",
    "https://openalex.org/W2936520980",
    "https://openalex.org/W3014030918",
    "https://openalex.org/W1964846093",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W6792881045",
    "https://openalex.org/W4221154951",
    "https://openalex.org/W6683411478",
    "https://openalex.org/W2901751835",
    "https://openalex.org/W2557641257",
    "https://openalex.org/W2518013266",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2891033863",
    "https://openalex.org/W3151370571",
    "https://openalex.org/W6786369753",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2154889144",
    "https://openalex.org/W2898200825",
    "https://openalex.org/W2886904239",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2908174331",
    "https://openalex.org/W2799058067",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6726654379",
    "https://openalex.org/W2794744029",
    "https://openalex.org/W1857884451",
    "https://openalex.org/W6754805553",
    "https://openalex.org/W2950628590",
    "https://openalex.org/W6810884174",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4280497038",
    "https://openalex.org/W6770825554",
    "https://openalex.org/W3009750455",
    "https://openalex.org/W3138486308",
    "https://openalex.org/W4226324017",
    "https://openalex.org/W6791943378",
    "https://openalex.org/W6803924480",
    "https://openalex.org/W2983890170",
    "https://openalex.org/W3144314306",
    "https://openalex.org/W3017087661",
    "https://openalex.org/W3190827869",
    "https://openalex.org/W2962766617",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3167536469",
    "https://openalex.org/W3108235634",
    "https://openalex.org/W3035211844",
    "https://openalex.org/W2998434318",
    "https://openalex.org/W3035672751",
    "https://openalex.org/W2964038557",
    "https://openalex.org/W3127756064",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4287248588",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W4312519354",
    "https://openalex.org/W3034617042",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3170863103",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W4214759957",
    "https://openalex.org/W2966759264",
    "https://openalex.org/W4312446817",
    "https://openalex.org/W2518876086",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3204554907",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4312472480",
    "https://openalex.org/W4312805142",
    "https://openalex.org/W3168663926",
    "https://openalex.org/W4312312750",
    "https://openalex.org/W4313156423"
  ],
  "abstract": "Transformer framework has been showing superior performances in visual object tracking for its great strength in information aggregation across the template and search image with the well-known attention mechanism. Most recent advances focus on exploring attention mechanism variants for better information aggregation. We find these schemes are equivalent to or even just a subset of the basic self-attention mechanism. In this paper, we prove that the vanilla self-attention structure is sufficient for information aggregation, and structural adaption is unnecessary. The key is not the attention structure, but how to extract the discriminative feature for tracking and enhance the communication between the target and search image. Based on this finding, we adopt the basic vision transformer (ViT) architecture as our main tracker and concatenate the template and search image for feature embedding. To guide the encoder to capture the invariant feature for tracking, we attach a lightweight correlative masked decoder which reconstructs the original template and search image from the corresponding masked tokens. The correlative masked decoder serves as a plugin for the compact transformer tracker and is skipped in inference. Our compact tracker uses the most simple structure which only consists of a ViT backbone and a box head, and can run at 40 fps. Extensive experiments show the proposed compact transform tracker outperforms existing approaches, including advanced attention variants, and demonstrates the sufficiency of self-attention in tracking tasks. Our method achieves state-of-the-art performance on five challenging datasets, along with the VOT2020, UAV123, LaSOT, TrackingNet, and GOT-10k benchmarks. Our project is available at https://github.com/HUSTDML/CTTrack.",
  "full_text": "Compact Transformer Tracker with Correlative Masked Modeling\nZikai Song1, Run Luo1, Junqing Yu1\u0003, Yi-Ping Phoebe Chen2, Wei Yang1\u0003\n1Huazhong University of Science and Technology, China\n2La Trobe University, Australia\nfskyesong, lr 8823, yjqing, weiyangcsg@hust.edu.cn, phoebe.chen@latrobe.edu.au\nAbstract\nTransformer framework has been showing superior perfor-\nmances in visual object tracking for its great strength in in-\nformation aggregation across the template and search image\nwith the well-known attention mechanism. Most recent ad-\nvances focus on exploring attention mechanism variants for\nbetter information aggregation. We ﬁnd these schemes are\nequivalent to or even just a subset of the basic self-attention\nmechanism. In this paper, we prove that the vanilla self-\nattention structure is sufﬁcient for information aggregation,\nand structural adaption is unnecessary. The key is not the at-\ntention structure, but how to extract the discriminative fea-\nture for tracking and enhance the communication between\nthe target and search image. Based on this ﬁnding, we adopt\nthe basic vision transformer (ViT) architecture as our main\ntracker and concatenate the template and search image for\nfeature embedding. To guide the encoder to capture the in-\nvariant feature for tracking, we attach a lightweight correl-\native masked decoder which reconstructs the original tem-\nplate and search image from the corresponding masked to-\nkens. The correlative masked decoder serves as a plugin\nfor the compact transformer tracker and is skipped in in-\nference. Our compact tracker uses the most simple struc-\nture which only consists of a ViT backbone and a box head,\nand can run at 40 fps. Extensive experiments show the pro-\nposed compact transform tracker outperforms existing ap-\nproaches, including advanced attention variants, and demon-\nstrates the sufﬁciency of self-attention in tracking tasks. Our\nmethod achieves state-of-the-art performance on ﬁve chal-\nlenging datasets, along with the VOT2020, UA V123, LaSOT,\nTrackingNet, and GOT-10k benchmarks. Our project is avail-\nable at https://github.com/HUSTDML/CTTrack.\n1 Introduction\nVisual Object Tracking is one of the fundamental tasks\nin computer vision with applications ranging from human-\ncomputer interaction, surveillance, trafﬁc ﬂow monitoring\nand etc. It aims to estimate the location, denoted as a bound-\ning box, of an arbitrary target object throughout the subse-\nquent video sequence. Deep Learning based trackers have\nachieved great success due to their strong representation\nability. Trackers (Bertinetto et al. 2016; Nam and Han 2016;\n\u0003indicates co-corresponding author.\nCopyright c\r 2023, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n………\nencoder\ndecoder\n…\n………\nhead …\nTemplate\nSearch Image\ncat\nFigure 1: Our compact transformer tracker adopts the simple\nViT structure (encoder) with the concatenation of the tem-\nplate and search image as input, which essentially exploits\nthe standard self-attention mechanism for information ag-\ngregation. The encoded tokens pass through a box head to\nestimate the result bounding box. And we develop a correl-\native masked decoder reconstructing the original template\nand search pixels to enhance the information aggregation,\nwhich is skipped during inference.\nLi et al. 2018, 2019) derived from Convolutional Neural\nNetworks (CNN) (Krizhevsky, Sutskever, and Hinton 2012;\nSimonyan and Zisserman 2015; He et al. 2016) produce\ntracking accuracy that beyond the comparison of traditional\napproaches, especially the trackers built on Siamese net-\nwork (Bertinetto et al. 2016; Xu et al. 2020; Li et al. 2018,\n2019; V oigtlaender et al. 2020; Yu et al. 2020; Guo et al.\n2021). The key of Siamese network trackers is to produce\nthe cross-correlation and measure the similarity between the\ntarget template and search image. Nowadays, transformer-\nbased trackers (Chen et al. 2021; Wang et al. 2021; Yan\net al. 2021; Shen et al. 2022; Song et al. 2022; Cui et al.\n2022) have shown great strength by introducing the atten-\ntion mechanism (Vaswani et al. 2017) to enhance and fuse\nthe features of querying sample and tracked objects. Preva-\nlent transformer trackers (Chen et al. 2021; Yan et al. 2021;\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n2321\nCui et al. 2022) more or less adapt the attention for aggre-\ngating information across the template and search image.\nWe ﬁnd that the advanced variants of attention mecha-\nnism in recent research, including mix-attention (Cui et al.\n2022) and cross-attention (Yu et al. 2020; Chen et al. 2021),\nare equivalent or even just a subset of the packed self-\nattention (i.e., standard self-attention with the concatena-\ntion of the template and search image as input). Then the\nquestion is which parts of the self-attention mechanism play\nan important role in visual object tracking? We revisited\nthe transformer tracking framework and ﬁnd that the track-\ning results are generated from tokens corresponding to the\nsearch image (search tokens), while the tokens correspond-\ning to the template (template tokens) are always discarded\nin the last. The representational ability of search tokens\ncomes from two parts: the cross-information enhancement\nfrom the template tokens and the self-information enhance-\nment from the search tokens themselves. In this paper, we\nprove that self-information enhancement in multi-image at-\ntention plays a greater role than cross-information aggrega-\ntion, though cross-information aggregation is indispensable\nin visual object tracking but not greatly beneﬁcial.\nDriven by this analysis, we propose a compact trans-\nformer tracker combined with correlative masked modeling\nfor the cross-information aggregation and self-information\nreinforcement. As shown in Figure 1, our tracker adopts the\nbasic vision transformer as the main branch and applies a\nlightweight masked decoder to enhance the implicit repre-\nsentation capability of the packed self-attention. The cor-\nrelative masked decoder, which is inspired by Masked Im-\nage Modeling (He et al. 2022; Xie et al. 2022), reconstructs\nthe both original template and search pixels from the cor-\nresponding masked tokens, to guide the encoder to capture\nthe invariant feature for tracking. In addition, our decoder\ncan be plugged into other transformer trackers, which can\neffectively improve the tracking performance without com-\npromising speed. Applying our correlative masked mod-\neling strategy to the compact transformer tracker can im-\nprove the AUC from 64.0% to 65.8% on the LaSOT (Fan\net al. 2019) dataset. Extensive comparison experiments on\n5 challenging datasets including VOT2020 (Kristan et al.\n2020), UA V123 (Mueller, Smith, and Ghanem 2016), La-\nSOT, GOT-10k (Huang, Zhao, and Huang 2019), and Track-\ningNet (Muller et al. 2018) exhibits the state-of-the-art per-\nformance, which further evidence the correctness of our\nanalysis regarding the self-attention in visual tracking.\nTo summarize, our main contributions include:\n1. We present a uniﬁed analyzing method for the attention\nmechanism and ﬁnd that the advanced variants of the at-\ntention mechanism are equivalent or even just a subset of\nthe self-attention. We also prove that self-information en-\nhancement in multi-image attention plays a greater role\nthan cross-information aggregation.\n2. We develop a compact transformer tracker with a correl-\native masked decoder, which has a very simple structure\nand achieves state-of-the-art accuracy at a high Frames-\nPer-Seconds (fps) tracking speed. The decoder recon-\nstructs the original template and search image from the\ncorresponding masked tokens and serves as a training\nplugin for the tracker. The experiment demonstrates that\nour analysis regarding self-attention is correct.\n2 Related Work\nTraditional trackers.Traditional single object tracking al-\ngorithms can be roughly summarized as Correlation Filter\nbased trackers (CF), Deep Network based trackers (DLN).\nCF-based trackers(Bolme et al. 2010; Henriques et al. 2015;\nDanelljan et al. 2016, 2017, 2019; Bhat et al. 2019) exploit\nthe convolution theorem and learn a ﬁlter in the Fourier\ndomain that maps known target images to the desired out-\nput. DLN-based trackers refer to algorithms employing\ndeep neural networks for the tracking process. Earlier ap-\nproaches (Nam and Han 2016; Pu et al. 2018) treat the track-\ning task as a classiﬁcation problem and exploit deep fea-\ntures for locating the target. Shortly afterwards more track-\ners adopt the Siamese network (Bertinetto et al. 2016; Li\net al. 2018, 2019) for its effectiveness in measuring simi-\nlarity. The Siamese network consists of two branches, one\noperates on the template and the other for the search area.\nAbove all, these methods mainly consist of a backbone\nwhich extracts the features of search image and template\nseparately, a similarity measuring module, and heads to pre-\ndict the location and bounding box. Compared to our frame-\nwork, traditional trackers have too many modules and a very\ncomplex design, we simply adapt a ViT backbone with a box\nhead to get better tracking results.\nTransformer trackers.The ViT (Dosovitskiy et al. 2021)\nﬁrst introduces the transformer to image recognition tasks\nand presents an impressive performance. Ever since, trans-\nformer has been widely applied in image classiﬁca-\ntion(Dosovitskiy et al. 2021; Wu et al. 2021; Liu et al.\n2021), object detection(Carion et al. 2020; Li et al. 2022),\nvisual object tracking(Yan et al. 2021; Chen et al. 2021;\nWang et al. 2021; Song et al. 2022; Shen et al. 2022; Cui\net al. 2022) and etc. Transformer-based tracking methods\nhave become the mainstream tracking algorithms nowadays.\nTransT (Chen et al. 2021) proposes a feature fusion network\nand employs an attention mechanism to combine the fea-\ntures of the template and search region. STARK (Yan et al.\n2021) develops a spatial-temporal architecture based on the\nencoder-decoder transformer. CSWinTT (Song et al. 2022)\nproposes a transformer architecture with multi-scale cyclic\nshifting window attention for visual tracking, elevating the\nattention from pixel level to window level. MixFormer (Cui\net al. 2022) constructs a compact tracking framework and\ndesigns a mixed attention module that uniﬁes the process of\nfeature extraction and information matching module.\nInstead of designing a complex attention mechanism as in\nthe previous tracking approaches, we compare the essential\ndifferences of attention variants(such as mix-attention and\ncross-attention) and ﬁnd these attention variants are equiv-\nalent or even just a subset of the packed self-attention. To\nverify the capability of self-attention in information aggre-\ngation, we design a compact transformer tracker using the\nmost simple pipeline which only consists of a ViT backbone\nand a box head, without any extra design including separate\n2322\nmodules of feature extraction and aggregation, and multi-\nlayer feature aggregation.\nMasked image modeling (MIM).MIM masks an area of\nthe original images and predicts the missing pixels, which\naims to enhance the representation of models. Recently,\nMIM approaches((Chen et al. 2020; He et al. 2022; Xie et al.\n2022; Wei et al. 2022; Bao, Dong, and Wei 2021)) are ex-\ntended to the modern vision transformers (Dosovitskiy et al.\n2021; Liu et al. 2021). iGPT (Chen et al. 2020) ﬁrst proposes\na transformer to predict unknown pixels from a sequence of\nlow-resolution pixels. BEiT (Bao, Dong, and Wei 2021) to-\nkenizes the images via an additional dV AE (Ramesh et al.\n2021) network with a block-wise masking strategy. Sim-\nMIM (Xie et al. 2022) ﬁnd that a moderately large masked\npatch size of the input image for pixel predictions makes\na strong pre-text task. MAE (He et al. 2022) develops an\nasymmetric encoder-decoder architecture, the encoder op-\nerates on a small proportion of the visible patches, and\nthe decoder reconstructs the original pixels. MaskFeat (Wei\net al. 2022) reconstructs the feature descriptors such as\nHoG (Dalal and Triggs 2005) instead of pixels.\nOur approach is inspired by the previous MIM\nmethod (Xie et al. 2022; He et al. 2022), but we have to\ndeal with two fundamental problems in the tracking frame-\nwork: (1) Visual tracking is a downstream vision task that\ngenerally does not have the pre-train process to apply the\nMIM strategy. We develop a masked decoder to leverage\nthe search and the template tokens to predict the original\nimages, which is embedded as an attachment plugin in the\ntraining phase to implement an end-to-end model. (2) MIM\nmethods reconstructing the single image do not ﬁt the track-\ning framework which involves cross-aggregation of multiple\nimages. According to the properties of packed self-attention,\nwe design a self-decoder and a cross-decoder to reconstruct\nthe original template and search image from the correspond-\ning masked tokens. As far as we know, we are the ﬁrst to\nartfully introduce the MIM into the visual tracking ﬁeld to\nimprove the information aggregation capabilities.\n3 Approach\nIn this section, we introduce our compact transformer\ntracker with correlative masked modeling in detail. Before\nproceed, we ﬁrst present a analysis on the key component of\ntransformer tracker, and demonstrate that existing attention\nvariants are equivalent to the packed self-attention.\n3.1 Revisiting Transformer Tracker\nTransformer tracking framework. As described in\nViT(Vaswani et al. 2017), the query-key-value attention\nmechanism is applied with query Q, key K, and value V.\nThe linear weights of Q, K, V are WQ, WK, WV respec-\ntively. The attention (Attn) is computed as:\nAttn(X) =softmax(XWQ \u0001WT\nKXT\npdk\n) \u0001XWV (1)\nwhere the X is the input token and the dk is the dimension\nof the key. For a clearer description of the post-order steps,\n………\n………\nC C C C\nAttentionAttentionC\n12 34\nTemplateSearchimageQt Kt Vt Vs Ks Qs\n1 32 4\n1 23 4AttentionMap\nCConcatenate\nNo.InfoStream\nTemplateSearchTemplate\nSearch\nFigure 2: Information streams in the attention mechanism.\nThe four information streams of Q-K-V are corresponding\nto the four parts in the attention map. Variants of attention\ncan be uniformly explained under this analytical approach.\nwe apply an attention calculation with the inputs of two dif-\nferent tokens, the token XQ computed with query and the\ntoken XKV computed with key and value. We modify the\nattention formula and deﬁne the attention map (AMap) as:\nAttn(XQ;XKV ) =AMap(XQ;XKV ) \u0001XKV WV\nAMap(XQ;XKV ) =softmax(XQWQ \u0001WT\nKXT\nKVp\nd\n) (2)\nOur compact transformer tracker consists of two parts: a\ntransformer backbone for information aggregation and a box\nhead for the bounding box estimation. Give the templatezin\nthe initial frame and a search image s. We obtain the tokens\nXt 2RLz\u0002d and Xs 2RLs\u0002d respectively through patch\nembedding, where drepresents the number of channels. The\npacked self-attention (PSelf-Attn)in the tracking ﬁeld is\ndeﬁned as the self-attention with the input of the concatena-\ntion (Cat) of the template and the search image:\nPSelf-Attn = Attn\n\u0010\nCat(Xz;Xs);Cat(Xz;Xs)\n\u0011\n(3)\nAnalysis on Attention.As shown in Figure 2, we divide the\ncomputation of attention mechanism, which involves both\ntemplate and search image, into four information streams:\n2323\n1 2\n3 4\n(a) PSelf-Attn\n1 2\n3 4 (b) AMix-Attn\n1 2\n3 4 (c) Cross-Attn\nFigure 3: Conﬁgurations of information stream in atten-\ntion map of packed self-attention (PSelf-Attn), asymmet-\nric mix-attention(AMix-Attn) and cross-attention (Cross-\nAttn).\n(1) self-information enhancement on template;\n(2) cross-information aggregation on template;\n(3) cross-information aggregation on search image;\n(4) self-information enhancement on search image.\nThese four information streams are also reﬂected in the\nfour parts of the attention map (In Figure 2, the index of\neach part in the attention map corresponds to the informa-\ntion stream). Based on this dissection, we can conveniently\ncompare the differences between existing attention, includ-\ning packed self-attention, mix-attention, and cross-attention.\nThe PSelf-Attn and the mix-attention(Cui et al. 2022)\nare essentially equivalent, the mix-attention is calculated as:\nPSelf-Attn == Mix-Attn =\nCat\n\u0010\nAMap\n\u0000\nXz;Cat(Xz;Xs)\n\u0001\n;AMap\n\u0000\nXs;Cat(Xz;Xs)\n\u0001\u0011\n(4)\nwhich is the same as Eqn. 3, and they include all four infor-\nmation streams (the attention map is shown as Figure 3a).\nBy the same analysis, the asymmetric mix-attention\n(AMix-Attn) contains three information streams (#1, #3,\n#4 info stream), which is shown in the Figure 3b and is cal-\nculated as follows:\nAMix-Attn =\nCat\n\u0010\nAMap\n\u0000\nXz;Xz\n\u0001\n;AMap\n\u0000\nXs;Cat(Xz;Xs)\n\u0001\u0011 (5)\nThe cross-attention contains two information streams\n(#2,#3 info stream) for cross information aggregation, which\nis shown in the Figure 3c and is calculated as follows:\nCross-Attn = Cat\n\u0010\nAMap\n\u0000\nXz;Xs\n\u0001\n;AMap\n\u0000\nXs;Xz\n\u0001\u0011\n(6)\nIn order to fully verify the importance of each part of\npacked attention, it is necessary to evaluate the impact of\neach information stream individually. The key of visual ob-\nject tracking is to ﬁnd the target in the search image, there\nmust be a cross-information aggregation of the search im-\nage (#3 info stream). The other information streams can be\nblocked out to verify their performance.\nBased on the above idea, we conduct detailed experi-\nments and the result is shown in Table 1. Removing cross-\ninformation aggregation of the template (#2 info stream) of\n# AMap No. Info Stream AUC Prec\n1 2 3 4\n1\n X X X X 61.7 64.2\n2\n X X X 64.0 67.7\n3\n X X X 60.6 63.7\n4\n X X X 58.8 60.1\n5\n X X 57.9 58.5\nTable 1: The effectiveness of information streams in the at-\ntention mechanism on the LaSOT dataset. The visualized\nfour parts in the attention map (AMap) correspond to the\nfour information streams at the matched location.\nself-attention can greatly improve tracking performance (the\nAUC and Prec of Table 1 #2 are better than that of Table 1\n#1), and the cross-information aggregation of the template\nwill introduce a lot of noise in template features, which is not\nrecommended in visual tracking. However, removing self-\ninformation enhancement (#3 and #4 info stream) of self-\nattention severely degrades the tracking performance (the\nAUC and Prec of Table 1 #3 and #4 are worse than that\nof Table 1 #1). From the results we can conclude that self-\ninformation enhancement in multi-image attention plays a\ngreater role than cross-information aggregation, the cross-\ninformation aggregation is indispensable in tracking but not\ngreatly beneﬁcial.\n3.2 Correlative Masked Modeling\nAccording to the above analysis, the best tracking per-\nformance can be achieved by adopting three information\nstreams: self-information on the template(#1 info stream),\ncross-information on the search image (#3 info stream), and\nself-information on the search image (#4 info stream). These\nthree information streams can be grouped into two cate-\ngories: two self-information enhancements and one cross-\ninformation aggregation. We designed a correlative masked\nmodeling method to enhance the information aggregation\nof our tracking framework, as shown in Figure 1. The ViT\nbackbone is an encoder, and the correlative masked decoder\nreconstructs the original image (the template and search im-\nage respectively) from randomly masked tokens to enhance\nthe self-information and reconstructs the template image\nfrom search tokens to improve cross-information aggrega-\ntion. In parallel with the masked decoder, the search image\ntokens go through a box estimation head as in (Yan et al.\n2021) to generate the result bounding box.\nDecoder. The decoders in our framework consist of a self-\ndecoder and a cross-decoder, these two decoders have the\nsame structure but do not share weights, each one is com-\nposed of a series of transformer blocks similar to the MAE,\nand the last layer of the decoder is a linear projection with\noutput channels equal to the number of pixels in a patch. As\nshown in Figure 4, the decoder takes masked tokens as in-\nput and predicts the original image pixels corresponding to\n2324\nself\ndecoder\n…\n …\ntemplate\ntokens\ncropped\nsearch\ntokens cross\ndecoder\ntemplate\nsearch\nimage\ntemplate\nFigure 4: The correlative masked decoders consists of a self-\ndecoder and a cross-decoder. The self-decoder reconstructs\nthe two original images, template and search image, from\nits corresponding masked tokens. The cross-decoder recon-\nstructs the template image from search tokens.\nthe template token and the search image token, where the\ntemplate tokens are only self-reconstructed to the template\nimage for enhancing the #1 information stream, search to-\nkens are used to crossly reconstruct the template image (for\n#3 info stream) and self-reconstruct the search image (for #4\ninfo stream).\nMasking and Reconstruction. The encoder embeds the\nconcatenation set of template tokens and search tokens.\nThen we split the encoded tokens into template tokens and\nsearch tokens, crop the search tokens using Precise RoI\nPooling(Jiang et al. 2018) to the same size as the template to-\nkens, and sample a subset of them. We randomly sample to-\nkens at a high masking ratio (75%). Our decoder predicts the\npixel values for each masked token, and the output of the de-\ncoder is reshaped to form a reconstructed image. We use the\nmean squared error (MSE) between the reconstructed and\noriginal images on masked tokens as our loss function.\n3.3 Training and Inference\nOur decoder is only used in the training phase, while does\nnot participate in the inference phase, hence it doesn’t affect\nthe tracking speed. During the training phase, our tracker\ntakes a triplet input consisting of one search region and two\ntemplates similar to STARK(Yan et al. 2021). We randomly\nsample multiple frames from sequences in the training set,\nselect the ﬁrst frame and the second frame as templates, and\nthe last frame as the search region. In the target localization\ntraining, we train the whole network except the scoring head\nin an end-to-end manner with the combination of L1 Loss,\ngeneralized IoU loss (Rezatoﬁghi et al. 2019), and decoder\nloss Ldec. The full loss function is deﬁned as follows:\nLoss= \u0015L1L1(Bi; ^Bi) +\u0015gLg(Bi; ^Bi) +\u0015decLdec (7)\nwhere \u0015L1 = 5:0, \u0015g = 2:0 and \u0015dec = 0:3 are the weight-\ning factors of three losses, ^Bi is the estimated box of the tar-\nget and Bi is the ground-truth bounding box. The decoder\nloss Ldec is deﬁned as:\nLdec = L2(z;zp) +L2(s;sp) +L2(z;sp) (8)\nwhere the L2 is the MSE loss, zand srepresent the original\ntemplate image and search image, zp and sp represent the\npredicting template image and search image respectively.\nIn the inference phase, we use two templates of the same\nsize as the input. One of which is the initial template and\nﬁxed, the other is online updated and always set to the latest\ntracking result with high conﬁdence. We use a score head to\ncontrol the updating of the online template. Our score head\nconsists of the multilayer perceptron (MLP) that receives a\nclass-token(Dosovitskiy et al. 2021) as input and evaluates\nthe accuracy of current tracking results.\n4 Experiments\n4.1 Implementation Details\nIn order to effectively verify the correctness of our analy-\nsis, we design the compact transformer tracker without any\nother extra attention mechanisms. The only structures re-\nmaining are feature extraction and aggregation, and multi-\nlayer feature aggregation. The main tracker only consists of\na ViT backbone and a box estimation head, we test both ViT-\nBase and ViT-Large, and the ViT parameters are initialized\nwith MAE (He et al. 2022) pre-trained model. We refer our\nCompact Transformer tracker as CTTrack-B (the backbone\nof ViT-Base) and CTTrack-L (the backbone of ViT-Large)\nin this section.\nWe adopt CoCo(Lin et al. 2014), LaSOT(Fan et al. 2019),\nGOT-10k(Huang, Zhao, and Huang 2019), and Track-\ningNet(Muller et al. 2018) as our training dataset except the\nGOT-10k benchmark. The training samples are directly sam-\npled from the same sequence and we apply common data\naugmentation operations including brightness jitter and hor-\nizontal ﬂip. The size of the input template is 128\u0002128, the\nsearch region is 52 times of the target box area and further\nresized to 320\u0002320. The decoder parameters are initialized\nwith Xavier Uniform. The AdamW optimizer (Loshchilov\nand Hutter 2018) is employed with initial learning rate (lr)\nof 1e-4 with the layer-wise decay 0.75, and the lr decreases\naccording to the cosine function with the ﬁnal decrease fac-\ntor of 0.1. We adopt a warm-up lr with the 0.2 warm-up fac-\ntor on the ﬁrst 5 epochs. We train our model on 4 Nvidia\nTesla V100 GPUs for a total of 500 epochs, each epoch uses\n6\u0002104 images. The mini-batch size is set to 128 images with\neach GPU hosting 32 images. Our approach is implemented\nin Python 3.7 with PyTorch 1.7.\n4.2 Ablation Study\nWe ablate our compact transformer tracker on several in-\ntriguing properties using the challenging LaSOT dataset and\nreport the Area Under the Curve (AUC) and Precision (Prec)\nas the validation accuracy.\nBackbone Comparison.Table 2 shows the comparison of\nthe transformer backbones between the ViT-Base and ViT-\nLarge backbone. The CTTrack-B reaches a higher tracking\nspeed while the CTTrack-L exhibits a better performance.\n2325\nMethods Params(M) FLOPs(G) Speed(fps)\nCTTrack-B 93.8 48.1 40\nCTTrack-L 313.9 163.7 22\nTable 2: Model size and speed using different backbones.\nReconstruction Streams.Our decoder enforces three types\nof reconstruction streams as shown in Figure 4. Table 3\nexhibits different conﬁgurations of reconstruction streams,\nthrough varied combinations of search tokens reconstruct\nsearch image (s2s), template tokens reconstruct template im-\nage (t2t) and search tokens reconstruct template image(s2t).\nThe result is consistent with the conclusion of our previ-\nous analysis that self-information enhancement (#5) plays\nthe most important role in transformer tracking, compared\nto cross-information aggregation(#4). Besides, search image\ninformation has more inﬂuence than the template informa-\ntion, the s2s (#2) improves performance the most among all\nstreams (#2, #3, #4), from 64.0 to 64.7 in AUC score. After\nadopting all three reconstruction streams, tracking accuracy\nimproved by an impressive AUC score of 1.8%, which vali-\ndates the effectiveness of our masked modeling decoders.\n# Recons Type AUC Precs2s t2t s2t\n1 - - - 64.0 67.7\n2 X - - 64.7 69.1\n3 - X - 64.4 68.4\n4 - - X 64.4 68.6\n5 X X - 65.1 69.9\n6 X X X 65.8 70.9\nTable 3: Ablation Study for the reconstruction streams. The\ns2s represents search tokens reconstruct search image, t2t\ndenotes template tokens reconstruct template image and s2t\nmeans search tokens reconstruct template image.\nMasking ratio. When we conduct reconstruction streams,\nwe randomly mask the input tokens according to a pre-\ndeﬁned ratio. Table 4 shows the inﬂuence of different mask-\ning ratios. We mask the encoded template token and search\ntokens with a random sampling strategy at different masking\nrates. Similar to the conclusion obtained by the MAE(He\net al. 2022), the optimal ratios are relatively high, and the\naccuracy increases steadily with the masking ratio growing\nuntil reaching 75%, which produces the best tracking results.\nMask Ratio 25% 50% 75% 90%\nAUC 64.6 65.7 65.8 64.9\nPrec 69.0 70.7 70.9 69.5\nTable 4: Comparison on masking ratio.\nOnline Template Updating.We evaluate the effect of the\nonline update strategy in our method. The ablation study\nTarget S-to-S T-to-T S-to-T\nw/o w w/o w w /o w\nFigure 5: Visualization of attention map which compares\nthe difference between training with correlative decoder\n(w) and training without correlative decoder(w/o). S-to-S\nis self-information enhancement on search image, T-to-T is\nself-information enhancement on template, S-to-T is cross-\ninformation aggregation on search image.\nresult is shown in Table 5, #1 represents the performance\nwithout template updating. We can see that applying a ﬁxed\ninterval to update the online template (#2) is ineffective as it\ngreatly reduces the quality of template and causes tracking\ndrift. It can be seen in #3, there is a 0.2% improvement in\nthe AUC score after applying the scoring head to evaluate\nthe accuracy of current tracking results.\nOnline Score AUC Prec\nCTTrack-B\n- - 65.8 70.9\nX - 64.9 69.9\nX X 66.0 71.1\nTable 5: Ablation for the online template updating compo-\nnent. Online denotes updating the template at a ﬁxed update\ninterval. Score represents the online template is only updated\nwith high conﬁdent samples.\nVisualization of attention maps. We visualize attention\nmaps in Figure5, our tracker adopting the correlative de-\ncoder has a stronger discriminative ability. The baseline\ntransformer without a reconstruction decoder tends to lose\nthe target position, and the distractors in the background get\nsuppressed with the training by the correlative decoder.\n4.3 Comparison with the SOTA\nWe compare our compact tracker with the state-of-the-\nart trackers on UA V123(Mueller, Smith, and Ghanem\n2016), LaSOT(Fan et al. 2019), TrackingNet(Muller et al.\n2018), GOT-10k(Huang, Zhao, and Huang 2019), and\nVOT2020(Kristan et al. 2020). For a fairer comparison, here\nwe adopt relative position biases in our ViT backbones, this\naddition improves AUC by around 1 point.\nUA V123gathers an application-speciﬁc collection of 123 se-\nquences. It adopts the AUC and Precision (P) as the eval-\nuation metrics. As shown in Table 1, Our CTTrack-L out-\nperforms previous trackers and exhibits very competitive\nperformance (71.3% AUC) when compared to the previous\nbest-performing tracker CSWinTT (70.5% AUC).\n2326\nMethods UA V123 LaSOT TrackingNet GOT-10k\nAUC P AUC P Norm P AUC P Norm P AO SR 0:5 SR0:75\nCTTrack-L 71.3 93.3 69.8 79.7 76.2 84.9 89.1 83.5 75.3 84.5 74.0\nCTTrack-B 68.8 89.5 67.8 77.8 74.0 82.5 87.1 80.3 73.5 83.5 70.6\nCTTrack-L -GOT - - - - - - - - 72.8 81.3 71.5\nCTTrack-B -GOT - - - - - - - - 71.3 80.7 70.3\nMixFormer(Cui et al. 2022) 69.5 91.0 70.1 79.9 76.3 83.9 88.9 83.1 70.7 80.0 67.8\nCSWinTT(Song et al. 2022) 70.5 90.3 66.2 75.2 70.9 81.9 86.7 79.5 69.4 78.9 65.4\nUTT(Shen et al. 2022) - - 64.6 - 67.2 79.7 - 77.0 67.2 76.3 60.5\nSTARK(Yan et al. 2021) - - 67.1 77.0 - 82.0 86.9 - 68.8 78.1 64.1\nTransT(Chen et al. 2021) 68.1 87.6 64.9 73.8 69.0 81.4 86.7 80.3 67.1 76.8 60.9\nTrDiMP(Wang et al. 2021) 67.0 87.6 64.0 73.2 66.6 78.4 83.3 73.1 68.8 80.5 59.7\nSTMTrack(Fu et al. 2021) 64.7 - 60.6 69.3 63.3 80.3 85.1 76.7 64.2 73.7 57.5\nAutoMatch(Zhang et al. 2021) 64.4 83.8 58.2 67.5 59.9 76.0 82.4 72.5 65.2 76.6 54.3\nSiamGAT(Guo et al. 2021) 64.6 84.3 53.9 63.3 53.0 - - - 62.7 74.3 48.8\nKYS(Bhat et al. 2020) - - 55.4 63.3 55.8 74.0 80.0 68.8 63.6 75.1 51.5\nMAML(Wang et al. 2020) - - 52.3 - 53.1 75.7 82.2 72.5 - - -\nSiamAttn(Yu et al. 2020) 65.0 84.5 56.0 64.8 - 75.2 81.7 - - - -\nSiamFC++(Xu et al. 2020) 61.8 80.4 54.4 62.3 54.7 75.4 80.0 70.5 59.5 69.5 47.9\nSiamRPN++(Li et al. 2019) 64.2 84.0 49.6 56.9 49.1 73.3 80.0 69.4 51.7 61.6 32.5\nDiMP(Bhat et al. 2019) 64.2 84.9 57.7 66.4 57.9 74.0 80.1 68.7 61.1 71.7 49.2\nATOM(Danelljan et al. 2019) 61.7 82.7 51.5 57.6 50.5 70.3 77.1 64.8 55.6 63.4 40.2\nTable 6: Comparisons with previous state-of-the-art trackers on four challenge benchmarks. The bold and italic performances\nranked at ﬁrst and second places. The GOT denotes only trained on the GOT-10k train split.\nMethods EAO\" Accuracy\" Robustness\"\nSiamFC 0.179 0.418 0.502\nATOM 0.271 0.462 0.734\nDiMP 0.274 0.457 0.740\nUPDT 0.278 0.465 0.755\nTransT 0.293 0.477 0.754\nCSWinTT 0.304 0.480 0.787\nCTTrack-L 0.287 0.453 0.787\nTable 7: Comparisons on VOT2020, where trackers only\npredict bounding boxes rather than masks.\nLaSOT is a long-term dataset including 1400 sequences\nand distributed over 14 attributes, the testing subset of La-\nSOT contains 280 sequences. Methods are ranked by the\nAUC, P, and Normalized Precision (PNorm ). Our CTTrack-\nL achieves the AUC (69.8%) and Prec (76.2%), which is\nan excellent result that outperforms other methods only ex-\ncept the MixFormer. Our tracker has lower performance\nthan MixFormer on LaSOT because it contains long-term\nsequences and large variations in content. ViT backbone is\na plain and non-hierarchical architecture that maintains fea-\nture maps at a certain scale, which may not be able to well\nhandle long-term tracking sequences with scale variations.\nTrackingNet is a large-scale tracking dataset consisting of\n511 sequences for testing. The evaluation is performed on\nthe online server. Table 1 shows that CTTrack-L performs\nbetter quality and ranks ﬁrst in AUC score at 84.9%. The\ngain is 1.0% improvement when compared with the previous\nbest results.\nGOT-10kcontains over 10k videos for training and 180 for\ntesting. It forbids the trackers to use external datasets for\ntraining. We follow this protocol by retraining our trackers to\nonly use the GOT10k train split. As in Table 1, MixFormer\nand CSWinTT provide the best performance, with an AO\nscore of 70.7% and 69.4%. Our CTTrack-L has obtained an\nAO score of 72.8%, signiﬁcantly outperforming the best ex-\nisting tracker by 2.1%.\nVOT2020 benchmark contains 60 challenging videos. The\nperformance is evaluated using the expected average overlap\n(EAO), which takes both accuracy (A) and robustness (R).\nSince our algorithm does not output a segmentation mask,\ntrackers that only predict bounding boxes are selected for\ncomparisons to ensure fairness. It can be seen from Table 7\nthat our CTTrack-L obtains an EAO of 0.287.\n5 Conclusion\nIn this work, we analyze the information stream in the at-\ntention mechanism in depth. We prove that the vanilla self-\nattention structure is sufﬁcient for information aggregation,\nand employ the three information streams of the packed\nself-attention in the transformer tracking framework. To en-\nhance the information representation, we design the cor-\nrelative masked decoder consisting of a self-decoder and a\ncross-decoder to reconstruct the original pixels of both tem-\nplate and search image. Extensive experiments demonstrate\nthe effectiveness of our correlative masked modeling strat-\negy and our compact transformer tracker exhibits impressive\nperformance over previous trackers. In addition, our correl-\native masked decoder can be plugged into other transformer\ntrackers, which can effectively improve the tracking perfor-\nmance without compromising speed. In the future, we plan\nto combine the feature pyramid or convolution module for\nbetter performance on long-term tracking sequences.\n2327\nAcknowledgments\nThis work is supported by the national key research\nand development program of China under Grant\nNo.2020YFB1805601, National Natural Science Foun-\ndation of China (NSFC No. 62272184), and CCF-Tencent\nOpen Research Fund (CCF-Tencent RAGR20220120). The\ncomputation is completed in the HPC Platform of Huazhong\nUniversity of Science and Technology.\nReferences\nBao, H.; Dong, L.; and Wei, F. 2021. Beit: Bert pre-training\nof image transformers. arXiv preprint arXiv:2106.08254.\nBertinetto, L.; Valmadre, J.; Henriques, J. F.; Vedaldi, A.;\nand Torr, P. H. S. 2016. Fully-Convolutional Siamese Net-\nworks for Object Tracking. In Proceedings of the ECCV,\n850–865. Springer.\nBhat, G.; Danelljan, M.; Gool, L. V .; and Timofte, R. 2019.\nLearning Discriminative Model Prediction for Tracking. In\nProceedings of the ICCV, 6182–6191. IEEE.\nBhat, G.; Danelljan, M.; Van Gool, L.; and Timofte, R. 2020.\nKnow Your Surroundings: Exploiting Scene Information for\nObject Tracking. In Proceedings of the ECCV. Springer.\nBolme, D. S.; Beveridge, J. R.; Draper, B. A.; and Lui, Y . M.\n2010. Visual object tracking using adaptive correlation ﬁl-\nters. In Proceedings of the CVPR, 2544–2550. IEEE.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In ECCV, 213–229. Springer.\nChen, M.; Radford, A.; Child, R.; Wu, J.; Jun, H.; Luan,\nD.; and Sutskever, I. 2020. Generative pretraining from pix-\nels. In International conference on machine learning, 1691–\n1703. PMLR.\nChen, X.; Yan, B.; Zhu, J.; Wang, D.; Yang, X.; and Lu, H.\n2021. Transformer tracking. In Proceedings of the CVPR,\n8126–8135.\nCui, Y .; Jiang, C.; Wang, L.; and Wu, G. 2022. MixFormer:\nEnd-to-End Tracking With Iterative Mixed Attention. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 13608–13618.\nDalal, N.; and Triggs, B. 2005. Histograms of oriented gra-\ndients for human detection. In 2005 IEEE computer soci-\nety conference on computer vision and pattern recognition\n(CVPR’05), volume 1, 886–893. Ieee.\nDanelljan, M.; Bhat, G.; Khan, F. S.; and Felsberg, M. 2019.\nATOM: Accurate Tracking by Overlap Maximization. In\nProceedings of the CVPR, 4660–4669. IEEE.\nDanelljan, M.; Bhat, G.; Shahbaz Khan, F.; and Felsberg, M.\n2017. ECO: Efﬁcient Convolution Operators for Tracking.\nIn Proceedings of the CVPR, 6638–6646. IEEE.\nDanelljan, M.; Robinson, A.; Khan, F. S.; and Felsberg,\nM. 2016. Beyond Correlation Filters: Learning Continuous\nConvolution Operators for Visual Tracking. In Proceedings\nof the ECCV, 472–488. Springer.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. ICLR.\nFan, H.; Lin, L.; Yang, F.; Chu, P.; Deng, G.; Yu, S.; Bai,\nH.; Xu, Y .; Liao, C.; and Ling, H. 2019. LaSOT: A High-\nQuality Benchmark for Large-Scale Single Object Tracking.\nIn Proceedings of the CVPR. IEEE.\nFu, Z.; Liu, Q.; Fu, Z.; and Wang, Y . 2021. Stmtrack:\nTemplate-free visual tracking with space-time memory net-\nworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 13774–13783.\nGuo, D.; Shao, Y .; Cui, Y .; Wang, Z.; Zhang, L.; and Shen,\nC. 2021. Graph attention tracking. In Proceedings of the\nCVPR, 9543–9552.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll´ar, P.; and Girshick, R.\n2022. Masked Autoencoders Are Scalable Vision Learners.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 16000–16009.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Resid-\nual Learning for Image Recognition. In Proceedings of the\nCVPR, 770–778. IEEE.\nHenriques, J. F.; Caseiro, R.; Martins, P.; and Batista, J.\n2015. High-Speed Tracking with Kernelized Correlation Fil-\nters. IEEE TPAMI, 37(3): 583–596.\nHuang, L.; Zhao, X.; and Huang, K. 2019. GOT-10k: A\nLarge High-Diversity Benchmark for Generic Object Track-\ning in the Wild. IEEE TPAMI.\nJiang, B.; Luo, R.; Mao, J.; Xiao, T.; and Jiang, Y . 2018.\nAcquisition of localization conﬁdence for accurate object\ndetection. In Proceedings of the European conference on\ncomputer vision (ECCV), 784–799.\nKristan, M.; Leonardis, A.; Matas, J.; Felsberg, M.;\nPﬂugfelder, R.; K ¨am¨ar¨ainen, J.-K.; Danelljan, M.; Zajc,\nL. ˇC.; Lukeˇziˇc, A.; Drbohlav, O.; et al. 2020. The eighth vi-\nsual object tracking VOT2020 challenge results. In ECCV,\n547–601. Springer.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classiﬁcation with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25: 1097–1105.\nLi, B.; Wu, W.; Wang, Q.; Zhang, F.; Xing, J.; and Yan, J.\n2019. SiamRPN++: Evolution of Siamese Visual Tracking\nWith Very Deep Networks. In Proceedings of the CVPR,\n4282–4291. IEEE.\nLi, B.; Yan, J.; Wu, W.; Zhu, Z.; and Hu, X. 2018. High Per-\nformance Visual Tracking With Siamese Region Proposal\nNetwork. In Proceedings of the CVPR, 8971–8980. IEEE.\nLi, Y .; Mao, H.; Girshick, R.; and He, K. 2022. Explor-\ning plain vision transformer backbones for object detection.\narXiv preprint arXiv:2203.16527.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll ´ar, P.; and Zitnick, C. L. 2014. Microsoft\ncoco: Common objects in context. In ECCV, 740–755.\nSpringer.\n2328\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vision\ntransformer using shifted windows. In Proceedings of the\nICCV.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled weight decay\nregularization. In Proceedings of the ICLR.\nMueller, M.; Smith, N.; and Ghanem, B. 2016. A benchmark\nand simulator for uav tracking. InProceedings of the ECCV,\n445–461. Springer.\nMuller, M.; Bibi, A.; Giancola, S.; Alsubaihi, S.; and\nGhanem, B. 2018. TrackingNet: A Large-Scale Dataset and\nBenchmark for Object Tracking in the Wild. InProceedings\nof the ECCV.\nNam, H.; and Han, B. 2016. Learning Multi-Domain Convo-\nlutional Neural Networks for Visual Tracking. In Proceed-\nings of the CVPR, 4293–4302. IEEE.\nPu, S.; Song, Y .; Ma, C.; Zhang, H.; and Yang, M.-H. 2018.\nDeep Attentive Tracking via Reciprocative Learning. In\nNeurIPS, 1931–1941.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Rad-\nford, A.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-\nimage generation. In International Conference on Machine\nLearning, 8821–8831. PMLR.\nRezatoﬁghi, H.; Tsoi, N.; Gwak, J.; Sadeghian, A.; Reid, I.;\nand Savarese, S. 2019. Generalized intersection over union:\nA metric and a loss for bounding box regression. In Pro-\nceedings of the CVPR, 658–666.\nShen, Q.; Qiao, L.; Guo, J.; Li, P.; Li, X.; Li, B.; Feng, W.;\nGan, W.; Wu, W.; and Ouyang, W. 2022. Unsupervised\nLearning of Accurate Siamese Tracking. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 8101–8110.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition. In\nInternational Conference on Learning Representations.\nSong, Z.; Yu, J.; Chen, Y .-P. P.; and Yang, W. 2022. Trans-\nformer Tracking With Cyclic Shifting Window Attention. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), 8791–8800.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NIPS, 5998–6008.\nV oigtlaender, P.; Luiten, J.; Torr, P. H.; and Leibe, B. 2020.\nSiam r-cnn: Visual tracking by re-detection. In Proceedings\nof the CVPR, 6578–6588.\nWang, G.; Luo, C.; Sun, X.; Xiong, Z.; and Zeng, W. 2020.\nTracking by instance detection: A meta-learning approach.\nIn Proceedings of the CVPR, 6288–6297.\nWang, N.; Zhou, W.; Wang, J.; and Li, H. 2021. Transformer\nMeets Tracker: Exploiting Temporal Context for Robust Vi-\nsual Tracking. In Proceedings of the CVPR, 1571–1580.\nWei, C.; Fan, H.; Xie, S.; Wu, C.-Y .; Yuille, A.; and\nFeichtenhofer, C. 2022. Masked feature prediction for\nself-supervised visual pre-training. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 14668–14678.\nWu, H.; Xiao, B.; Codella, N.; Liu, M.; Dai, X.; Yuan, L.;\nand Zhang, L. 2021. Cvt: Introducing convolutions to vision\ntransformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 22–31.\nXie, Z.; Zhang, Z.; Cao, Y .; Lin, Y .; Bao, J.; Yao, Z.; Dai,\nQ.; and Hu, H. 2022. SimMIM: A Simple Framework for\nMasked Image Modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 9653–9663.\nXu, Y .; Wang, Z.; Li, Z.; Yuan, Y .; and Yu, G. 2020.\nSiamFC++: Towards robust and accurate visual tracking\nwith target estimation guidelines. In Proceedings of the\nAAAI, volume 34, 12549–12556.\nYan, B.; Peng, H.; Fu, J.; Wang, D.; and Lu, H. 2021. Learn-\ning spatio-temporal transformer for visual tracking. In Pro-\nceedings of the ICCV.\nYu, Y .; Xiong, Y .; Huang, W.; and Scott, M. R. 2020. De-\nformable siamese attention networks for visual object track-\ning. In Proceedings of the CVPR, 6728–6737.\nZhang, Z.; Liu, Y .; Wang, X.; Li, B.; and Hu, W. 2021. Learn\nto match: Automatic matching network design for visual\ntracking. In Proceedings of the ICCV, 13339–13348.\n2329",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7590227127075195
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6322349309921265
    },
    {
      "name": "Discriminative model",
      "score": 0.563453733921051
    },
    {
      "name": "Computer vision",
      "score": 0.5030571818351746
    },
    {
      "name": "Encoder",
      "score": 0.4577450156211853
    },
    {
      "name": "Embedding",
      "score": 0.44681668281555176
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.44549560546875
    },
    {
      "name": "Transformer",
      "score": 0.41658246517181396
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.4160793423652649
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I47720641",
      "name": "Huazhong University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I196829312",
      "name": "La Trobe University",
      "country": "AU"
    }
  ],
  "cited_by": 70
}