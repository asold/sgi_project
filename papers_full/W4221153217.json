{
  "title": "Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer",
  "url": "https://openalex.org/W4221153217",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2130164631",
      "name": "Guangwei Gao",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications",
        "National Institute of Informatics"
      ]
    },
    {
      "id": "https://openalex.org/A2971635880",
      "name": "Zhengxue Wang",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2099928315",
      "name": "Jun-Cheng Li",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2096170677",
      "name": "Wenjie Li",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A1964260924",
      "name": "Yi Yu",
      "affiliations": [
        "National Institute of Informatics"
      ]
    },
    {
      "id": "https://openalex.org/A2117506288",
      "name": "Tieyong Zeng",
      "affiliations": [
        "Chinese University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2214802144",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W1930824406",
    "https://openalex.org/W2747898905",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3204491849",
    "https://openalex.org/W2795024892",
    "https://openalex.org/W2963645458",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3105328221",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W2935564801",
    "https://openalex.org/W2192954843",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W4287020683",
    "https://openalex.org/W3170026688",
    "https://openalex.org/W2121927366",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W4377561911",
    "https://openalex.org/W3010250471",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3164705069",
    "https://openalex.org/W54257720",
    "https://openalex.org/W3038124041",
    "https://openalex.org/W1791560514",
    "https://openalex.org/W4226226289"
  ],
  "abstract": "Single-image super-resolution (SISR) has achieved significant breakthroughs with the development of deep learning. However, these methods are difficult to be applied in real-world scenarios since they are inevitably accompanied by the problems of computational and memory costs caused by the complex operations. To solve this issue, we propose a Lightweight Bimodal Network (LBNet) for SISR. Specifically, an effective Symmetric CNN is designed for local feature extraction and coarse image reconstruction. Meanwhile, we propose a Recursive Transformer to fully learn the long-term dependence of images thus the global information can be fully used to further refine texture details. Studies show that the hybrid of CNN and Transformer can build a more efficient model. Extensive experiments have proved that our LBNet achieves more prominent performance than other state-of-the-art methods with a relatively low computational cost and memory consumption. The code is available at https://github.com/IVIPLab/LBNet.",
  "full_text": "Lightweight Bimodal Network for Single-Image Super-Resolution via\nSymmetric CNN and Recursive Transformer\nGuangwei Gao1,3∗ , Zhengxue Wang1∗ , Juncheng Li2† , Wenjie Li1 , Yi Yu3 , Tieyong Zeng2\n1Nanjing University of Posts and Telecommunications, Nanjing, China\n2The Chinese University of Hong Kong, Hong Kong, China\n3National Institute of Informatics, Tokyo, Japan\n{csggao, cvjunchengli}@gmail.com, wzx 0826@163.com\nAbstract\nSingle-image super-resolution (SISR) has achieved\nsignificant breakthroughs with the development of\ndeep learning. However, these methods are difficult\nto be applied in real-world scenarios since they are\ninevitably accompanied by the problems of com-\nputational and memory costs caused by the com-\nplex operations. To solve this issue, we propose a\nLightweight Bimodal Network (LBNet) for SISR.\nSpecifically, an effective Symmetric CNN is de-\nsigned for local feature extraction and coarse im-\nage reconstruction. Meanwhile, we propose a Re-\ncursive Transformer to fully learn the long-term de-\npendence of images thus the global information can\nbe fully used to further refine texture details. Stud-\nies show that the hybrid of CNN and Transformer\ncan build a more efficient model. Extensive exper-\niments have proved that our LBNet achieves more\nprominent performance than other state-of-the-art\nmethods with a relatively low computational cost\nand memory consumption. The code is available at\nhttps://github.com/IVIPLab/LBNet.\n1 Introduction\nSingle image super-resolution (SISR) aims to recover the cor-\nresponding high-resolution (HR) image with rich details and\nbetter visual quality from its degraded low-resolution (LR)\none. Recently, convolutional neural networks (CNN) based\nSISR methods have achieved remarkable performance than\ntraditional methods due to their powerful feature extraction\nability. For example, Dong et al. [Dong et al., 2014] pio-\nneered the Super-Resolution Convolutional Neural Network\n(SRCNN). Later, with the emergence of ResNet [He et al.,\n2016] and DenseNet [Huang et al., 2017], plenty of CNN-\nbased SISR models have been proposed, like VDSR [Kim et\nal., 2016a], EDSR [Lim et al., 2017], and RCAN [Zhang et\nal., 2018]. All these methods show that the deeper the net-\nwork, the better the performance. However, these methods\nare difficult to be used in real-life scenarios with limited stor-\nage and computing capabilities. Therefore, a model that can\n∗Co-first authors\n†Corresponding author\nachieve better performance while keeping the lightweight of\nthe network has become attractive research. One of the most\nwidely used strategies is to introduce the recursive mecha-\nnism, such as DRCN [Kim et al., 2016b] and DRRN [Tai et\nal., 2017]. The other one is to explore the lightweight struc-\nture, including CARN [Ahn et al., 2018], FDIWN [Gao et al.,\n2022], and PFFN [Zhang et al., 2021a]. Although these mod-\nels reduce the number of model parameters to a certain extent\nthrough various strategies and structures, they also lead to a\ndegradation in performance, thus it is difficult to reconstruct\nhigh-quality images with rich details.\nRecently, with the continuous progress of Transformer in\nNatural Language Processing (NLP), how to apply it to com-\nputer vision tasks has become a hot topic. Transformer can\nmodel the long-term dependence in the image, and this pow-\nerful representation ability can help to restore the texture de-\ntails of the image. However, most methods blindly use Trans-\nformer to replace all original CNN structures, this is unrea-\nsonable since the ability of CNN to extract local features is\nirreplaceable. These features can maintain their own stability\nunder different viewing angles, also called local invariance,\nwhich is helpful for image understanding and reconstruction.\nTherefore, we recommend fusing CNN and Transformer to\ntake full use of the advantages of both to achieve efficient SR\nimage reconstruction.\nTo achieve this, we propose a Lightweight Bimodal Net-\nwork (LBNet) for SISR. In LBNet, we use both CNN and\nTransformer to achieve dual-mode coordination reconstruc-\ntion. As for the CNN part, we focus on local features ex-\ntraction. Specifically, we propose a novel Local Feature Fu-\nsion Module (LFFM), which consists of a series of Feature\nRefinement Dual-Attention Block (FRDAB). FRDAB uses\nthe channel reduction strategy to reduce the parameters of\nthe model and introduces channel attention and spatial at-\ntention mechanisms to reweight the feature information ex-\ntracted from different branches. Meanwhile, to balance the\nperformance and size of the model, we introduce the parame-\nter sharing strategy to construct the symmetric-like network,\nthe output of the corresponding shared module of the previous\nstage will be integrated through the channel attention module\nas the input of the current module. This method can maximize\nthe use of feed-forward features to restore texture details. As\nfor the Transformer part, we propose a Recursive Transformer\nto learn the long-term dependence of images, thus the texture\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n913\nFigure 1: The complete architecture of the proposed Lightweight Bimodal Network (LBNet).\ndetails can be further refined with global information. In sum-\nmary, the main contributions are as follows\n• We propose an effective Symmetric CNN for local fea-\nture extraction and coarse image reconstruction. Among\nthem, Local Feature Fusion Module (LFFM) and Fea-\nture Refinement Dual-Attention Block (FRDAB) are\nspecially designed for feature extraction and utilization.\n• We propose a Recursive Transformer to learn the long-\nterm dependence of images. This is the first attempt of\nthe recursive mechanism in Transformer, which can re-\nfine the texture details by global information with few\nparameters and GPU memory consumption.\n• We propose a novel Lightweight Bimodal Network (LB-\nNet) for SISR. LBNet elegantly integrates CNN and\nTransformer, enabling it to achieve a better balance be-\ntween the performance, size, execution time, and GPU\nmemory consumption of the model.\n2 Related Works\n2.1 CNN-based SISR\nThanks to the powerful feature representation and learning\ncapabilities of CNN, CNN-based SISR methods have made\ngreat progress in recent years [Li et al., 2021a ]. For ex-\nample, SRCNN [Dong et al., 2014] applied CNN to SISR\nfor the first time and achieved competitive performance at\nthe time. EDSR [Lim et al., 2017 ] greatly improved the\nmodel performance by using the residual blocks [He et al.,\n2016]. RCAN [Zhang et al., 2018] introduced the channel\nattention mechanism and built an 800-layer network. Apart\nfrom these deep networks, many lightweight SISR models\nalso have been proposed in recent years. For instance, Ahn\net al. [Ahn et al., 2018] proposed a lightweight Cascaded\nResidual Network (CARN) by using the cascade mechanism.\nHui et al. [Hui et al., 2019] proposed an Information Multi-\nDistillation Network (IMDN) by using the distillation and se-\nlective fusion strategy. MADNet [Lan et al., 2020] used a\ndense lightweight network to enhance multi-scale feature rep-\nresentation and learning. Xiao et al. [Xiao et al., 2021] pro-\nposed a simple but effective deep lightweight model for SISR,\nwhich can adaptively generate convolutional kernels based on\nthe local information of each position. However, the perfor-\nmance of these lightweight models is not ideal since they dis-\nable obtaining larger receptive fields and global information.\n2.2 Transformer-based SISR\nIn order to model the long-term dependence of images,\nmore and more researchers pay attention to Transformer,\nwhich was first used in the field of NLP. Recently, many\nTransformer-based methods have been proposed for com-\nputer vision tasks, which also promote the development of\nSISR. For example, Chen et al.[Chen et al., 2021] proposed a\npre-trained Image Processing Transformer for image restora-\ntion. Liang et al. [Liang et al., 2021] proposed a SwinIR by\ndirectly migrating the Swin Transformer to the image restora-\ntion task and achieved excellent results. Lu et al. [Lu et al.,\n2022] proposed an Effective Super-resolution Transformer\n(ESRT) for SISR, which reduces GPU memory consump-\ntion through a lightweight Transformer and feature separation\nstrategy. However, all these models do not fully consider the\nfusion of CNN and Transformer, thus difficult to achieve the\nbest balance between model size and performance.\n3 Lightweight Bimodal Network (LBNet)\n3.1 Network Architecture\nAs shown in Figure 1, Lightweight Bimodal Network (LB-\nNet) is mainly composed of Symmetric CNN, Recursive\nTransformer, and reconstruction module. Specifically, Sym-\nmetric CNN is proposed for local feature extraction and Re-\ncursive Transformer is designed to learn the long-term depen-\ndence of images. We define ILR, ISR, and IHR as the input\nLR image, the reconstructed SR image, and the correspond-\ning HR image, respectively. At the head of the model, a3 ×3\nconvolutional layer is applied for shallow feature extraction\nFsf = fsf (ILR), (1)\nwhere fsf (·) represents the convolutional layer,Fsf is the ex-\ntracted shallow features. Then, the extracted shallow features\nwill be sent to Symmetric CNN for local feature extraction\nFCNN = fCNN (Fsf ), (2)\nwhere fCNN (·) represents the Symmetric CNN and FCNN\nrepresents the extracted local features. Symmetric CNN is\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n914\nFigure 2: The architecture of the proposed Local Feature Fusion Module (LFFM) and Feature Refinement Dual-Attention Block (FRDAB).\none of the most important components in LBNet, which con-\nsists of several pairs of parameter-sharing Local Feature Fu-\nsion Modules (LFFMs) and channel attention modules. All\nof these modules will be introduced in the next section.\nAfter that, all these features will be sent to the Recursive\nTransformer for long-term dependence learning\nFRT = fRT (FCNN ), (3)\nwhere fRT (·) is the Recursive Transformer and FRT is the\nfeature enhanced by global information. Finally, the refined\nfeatures FRT and the shallow featuresFsf are added and sent\nto the reconstruction module for SR image reconstruction\nISR = fbuild(Fsf + FRT ), (4)\nwhere fbuild(·) is the reconstruction module, which com-\nposed of a 3 ×3 convolutional layer and a pixel-shuffle layer.\nDuring training, LBNet is optimized withL1 loss function.\nGiven a training dataset\n\b\nIi\nLR, Ii\nHR\n\tN\ni=1, we solve\nˆθ = arg min\nθ\n1\nN\nNX\ni=1\n\r\r\n\rFθ(Ii\nLR) −Ii\nHR\n\r\n\r\n\r\n1\n, (5)\nwhere θ denotes the parameter set of our proposed LBNet,\nF(ILR) = ISR is the reconstruct SR image, and N is the\nnumber of the training images.\n3.2 Symmetric CNN\nSymmetric CNN is specially designed for local feature ex-\ntraction, which mainly consists of some paired parameter-\nsharing Local Feature Fusion Modules (LFFMs) and Chan-\nnel Attention (CA) modules. The parameter-sharing of every\ntwo symmetrical modules can better balance the parameters\nand performance. In addition, each pair of parameter-sharing\nmodules will be fused through the channel attention module,\nso that the extracted features can be fully utilized.\nAs shown in Figure 1, Symmetric CNN is a dual-branch\nnetwork. The shallow feature Fsf will first be sent to the top\nbranch and the outputs of each LFFM in the top branch will\nserve as part of the input of the corresponding LFFM in the\ndown branch. The complete operation can be defined as\nFT,1\nLFFM = fT,1\nLFFM (Fsf ), i= 1, (6)\nFT,i\nLFFM = fT,i\nLFFM (FT,i−1\nLFFM ), i= 2, ..., n, (7)\nFD,i\nLFFM = fD,i\nLFFM (FD,i−1\nLFFM + fi\nCA(FT,i\nLFFM )), i= 2, ..., n,(8)\nwhere fT,i\nLFFM (·) and fD,i\nLFFM (·) represent the i-th LFFM in\nthe top and down-branch, respectively. fi\nCA(·) denotes the i-\nth channel attention module. It is worth noting that when i =\n1, FD,1\nLFFM = fD,1\nLFFM (FT,n\nLFFM + f1\nCA(FT,1\nLFFM )). Moreover,\nthe weight sharing strategy is applied on the paired modules,\nthus fD,i\nLFFM (·) =fT,i\nLFFM (·). Finally, the outputs of all these\nLFFMs are concatenated and a 1 × 1 convolutional layer is\nused for feature fusion and compression. Therefore, the most\neffective features extracted at different levels will be sent to\nthe next part to learn the long-term dependence of images.\nLocal Feature Fusion Module (LFFM). LFFM is the\ncore component of Symmetric CNN. As shown in Figure 2\n(a), LFFM is essentially an improved version of Dense-\nBlock [Huang et al., 2017]. Different from DenseBlock, (1)\nwe use FRDAB to replace the original convolutional layer to\nmake it have a stronger feature extraction ability; (2) we intro-\nduce a 1 × 1 group convolutional layer before each FRDAB\nfor dimensionality reduction; (3) local residual learning is in-\ntroduced to further promote the transmission of information.\nThe complete operation of LFFM can be defined as\nF1\nFRB = f1\nFRB (Fm−1\nLFFM ), (9)\nF2\nFRB = f2\nFRB (f1\ngc([Fm−1\nLFFM , F1\nFRB ])), (10)\nF3\nFRB = f3\nFRB (f2\ngc([Fm−1\nLFFM , F1\nFRB , F2\nFRB ])), (11)\nFm\nLFFM = fm−1\nLFFM + f1×1([Fm−1\nLFFM , F1\nFRB , F2\nFRB , F3\nFRB ]),\n(12)\nwhere Fi\nFRB represents the output of the i-th (i = 1, 2, 3)\nFRDAB module in LFFM. fj\ngc(·) means the j-th (j = 1, 2)\ngroup convolutional layer followed by FRDAB. Fm−1\nLFFM and\nFm\nLFFM represent the input and output of the m-th LFFM\nmodule, respectively.\nFeature Refinement Dual-Attention Block (FRDAB).\nAs shown in Figure 2 (b), FRDAB is a dual-attention block,\nwhich specially designed for feature refinement. Specifically,\nthe multi-branch structure is designed for feature extraction\nand utilization. In this part, the feature will be sent to two\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n915\nbranches and each branch uses a different number of convolu-\ntional layers to change the size of the receptive field to obtain\ndifferent scales features. c/2 indicates the operations of halv-\ning the outputs. After that, channel attention is used to extract\nchannel statistics for re-weighting in the channel dimension\nand spatial attention is used to re-weighted the pixel accord-\ning to the spatial context relationship of the feature map. Fi-\nnally, the output of these two attention operations is fused by\nthe addition operation. With the help of this method, the fi-\nnally obtained features will show a stronger suppression of\nthe smooth areas of the input image.\n3.3 Recursive Transformer\nAs we mentioned before, Symmetric CNN is designed for\nlocal feature extraction. However, this is far from enough\nto reconstruct high-quality images since the depth of the\nlightweight network makes it difficult to have a large enough\nreceptive field to obtain global information. To solve this\nproblem, we introduce Transformer to learn the long-term\ndependence of images and propose a Recursive Transformer\n(RT). Different from previous methods, we introduced the\nrecursive mechanism to allow the Transformer to be fully\ntrained without greatly increasing GPU memory consump-\ntion and model parameters. As shown in Figure 1, RT is\nlocated before the reconstruction module, which consists of\ntwo Transformer Modules (TM) and two convolutional lay-\ners. The completed operation of RT can be defined as\nFRT = f3×3(f⟳\nTM 2(f3×3(f⟳\nTM 1(FCNN )))), (13)\nwhere f3×3(·) and fTM (·) represent the convolutional layer\nand the TM, respectively.⟳ denotes the recurrent connection,\nwhich means that the output of TM will be served as its new\ninput and looped S times. As for the TM, we only use the en-\ncoding part of the standard Transformer structure inspired by\nESRT [Lu et al., 2022]. As shown in Figure 3, TM is mainly\ncomposed of two layer normalization layers, one Multi-Head\nAttention (MHA), and one Multi-Layer Perception (MLP).\nDefine the input embeddings as Fin, the output embeddings\nFout can be obtained by\nFmid = Fin + fMHA (fnorm(Fin)), (14)\nFout = F1 + fMLP (fnorm(Fmid)), (15)\nwhere fnorm(·) represents the layer normalization operation.\nfMHA (·) and fMLP (·) represent the MHA and MLP mod-\nules, respectively. Like ESRT, we project the input feature\nmap of MHA into Q, K, and V through a linear layer to\nreduce GPU memory consumption. Meanwhile, feature re-\nduction strategy is also used to further reduce the memory\nconsumption of the Transformer. Following [Vaswani et al.,\n2017], each head of the MHA must perform a scaled dot prod-\nuct attention, and then concatenate all the outputs and per-\nform a linear transformation to obtain the output. Where the\nscaled dot product attention can be expressed as\nAttention(Q, K, V) =softmax(QKT\n√dk\n)V. (16)\nThis is the first attempt of the recursive mechanism in\nTransformer. With the help of this strategy, we can fully\nFigure 3: The architecture of the Transformer Module (TM).\ntrain and utilize Transformer without increasing the param-\neters and GPU memory consumption of the model. We will\nfurther discuss its effectiveness in the next section.\n4 Experiments\n4.1 Datasets and Evaluation Metrics\nFollowing previous works, we use DIV2K as training set. For\nevaluation, we use five benchmark test datasets to validate\nthe effectiveness of LBNet, including Set5[Bevilacqua et al.,\n2012], Set14 [Zeyde et al., 2010], BSDS100 [Martin et al.,\n2001], Urban100 [Huang et al., 2015], and Manga109 [Mat-\nsui et al., 2017]. In addition, PSNR and SSIM are used as\nevaluation indicators to evaluate the performance of SR im-\nages on the Y channel of the YCbCr color space.\n4.2 Implementation Details\nDuring training, we randomly crop 48 × 48 patches from the\ntraining set as the inputs, and enhance the images with ran-\ndom rotation and horizontal flipping. The initial learning rate\nis 2 ×10−4 and finally dropped to 6.25 ×10−6 by cosine an-\nnealing. Meanwhile, the model is trained by Adam optimizer\nunder the PyTorch framework with a NVIDIA RTX 2080Ti\nGPU. In the final model, the input and output channels of\neach part are set to 32, three (n = 3) LFFMs are used, and\nTransformer module recurses 2 times (S = 2). Moreover, we\nalso built a tiny version, named LBNet-T, which only consists\nof two (n = 2) LFFMs. Meanwhile, the number of channels\nin LBNet-T has also been reduced to 18.\n4.3 Comparison with Lightweight SISR Models\nIn Table 1, we compare our LBNet with 11 advanced\nlightweight SISR models. Most of them achieve the best re-\nsults at the time in the lightweight SISR task. According to\nthe table, we can clearly observe that our LBNet achieves the\nbest results, and our tiny version LBNet-T also achieves the\nbest results under the same parameter level. In addition, the\nnumber of parameters and Mult-Adds of LBNet and LBNet-T\nare also very low, which proves the efficiency of our models.\nMeanwhile, we also provide the visual comparison between\nLBNet and other lightweight SISR models in Figure 5. Ob-\nviously, SR images reconstructed by our LBNet have richer\ndetailed textures with better visual effects. This further vali-\ndates the effectiveness of our proposed LBNet.\n4.4 Model Complexity Studies\nAs can be seen from Table 1, our model achieves an excellent\nbalance between model size, performance, and Mult-Adds. In\naddition, the execution time of the model is also an important\nindicator to measure the complexity of the model. To make\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n916\nMethod Scale Params Mult-Adds Set5 Set14 BSD100 Urban100 Manga109\nPSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM\nVDSR [Kim et al.\n, 2016a]\n×3\n665K 612.6G 33.66/0.9213 29.77/0.8314 28.82/0.7976 27.14/0.8279 32.01/0.9310\nDRRN [Tai et\nal., 2017] 297K 6796.9G 34.03/0.9244 29.96/0.8349 28.95/0.8004 27.53/0.8378 32.74/0.9390\nIDN [Hui et al.\n, 2018] 553K 56.3G 34.11/0.9253 29.99/0.8354 28.95/0.8013 27.42/0.8359 32.71/0.9381\nCARN [Ahn et al.\n, 2018] 1592K 118.8G 34.29/0.9255 30.29/0.8407 29.06/0.8034 28.06/0.8493 33.43/0.9427\nIMDN [Hui et al.\n, 2019] 703K 71.5G 34.36/0.9270 30.32/0.8417 29.09/0.8046 28.17/0.8519 33.61/0.9445\nAWSRN-M [W\nang et al., 2019] 1143K 116.6G 34.42/\n0.9275 30.32/0.8419 29.13/\n0.8059 28.26/\n0.8545 33.64/0.9450\nMADNet [Lan et al.\n, 2020] 930K 88.4G 34.16/0.9253 30.21/0.8398 28.98/0.8023 27.77/0.8439 -\nGLADSR [Zhang et al.\n, 2021b] 821K 88.2G 34.41/0.9272 30.37/0.8418 29.08/0.8050 28.24/0.8537 -\nSMSR [Wang et\nal., 2021] 993K 156.8G 34.40/0.9270 30.33/0.8412 29.10/0.8050 28.25/0.8536 33.68/0.9445\nLAPAR-A [Li et\nal., 2021b] 594K 114.0G 34.36/0.9267 30.34/0.8421 29.11/0.8054 28.15/0.8523 33.51/0.9441\nLBNet-T (Ours) 407K 22.0G 34.33/0.9264 30.25/0.8402 29.05/0.8042 28.06/0.8485 33.48/0.9433\nLBNet (Ours) 736K 68.4G 34.47/\n0.9277 30.38/0.8417 29.13/\n0.8061 28.42/\n0.8559 33.82/\n0.9460\nVDSR [Kim et al.\n, 2016a]\n×4\n665K 612.6G 31.35/0.8838 28.01/0.7674 27.29/0.7251 25.18/0.7524 28.83/0.8809\nDRRN [Tai et\nal., 2017] 297K 6796.9G 31.68/0.8888 28.21/0.7720 27.38/0.7284 25.44/0.7638 29.46/0.8960\nIDN [Hui et al.\n, 2018] 553K 32.3G 31.82/0.8903 28.25/0.7730 27.41/0.7297 25.41/0.7632 29.41/0.8942\nCARN [Ahn et al.\n, 2018] 1592K 90.9G 32.13/0.8937 28.60/0.7806 27.58/0.7349 26.07/0.7837 30.42/0.9070\nIMDN [Hui et al.\n, 2019] 715K 40.9G 32.21/0.8948 28.58/0.7811 27.56/0.7353 26.04/0.7838 30.45/0.9075\nAWSRN-M [W\nang et al., 2019] 1254K 72.0G 32.21/\n0.8954 28.65/\n0.7832 27.60/0.7368 26.15/\n0.7884 30.56/\n0.9093\nMADNet [Lan et al.\n, 2020] 1002K 54.1G 31.95/0.8917 28.44/0.7780 27.47/0.7327 25.76/0.7746 -\nGLADSR [Zhang et al.\n, 2021b] 826K 52.6G 32.14/0.8940 28.62/0.7813 27.59/0.7361 26.12/0.7851 -\nSMSR [Wang et\nal., 2021] 1006K 89.1G 32.12/0.8932 28.55/0.7808 27.55/0.7351 26.11/0.7868 30.54/0.9085\nLAPAR-A [Li et\nal., 2021b] 659K 94.0G 32.15/0.8944 28.61/0.7818 27.61/0.7366 26.14/0.7871 30.42/0.9074\nLBNet-T (Ours) 410K 12.6G 32.08/0.8933 28.54/0.7802 27.54/0.7358 26.00/0.7819 30.37/0.9059\nLBNet (Ours) 742K 38.9G 32.29/\n0.8960 28.68/\n0.7832 27.62/\n0.7382 26.27/\n0.7906 30.76/\n0.9111\nTable 1: Average PSNR/SSIM comparison. The best and second best results are highlighted with red and blue, respectively.\nFigure 4: Model complexity study on Set5 (×4).\na more intuitive comparison with other models, we provide\nthe trade-offs between model performance, parameter quan-\ntity, and execution time in Figure 4. Obviously, our LBNet\nachieved the best PSNR results under the premise of compa-\nrable execution time and parameters. This further illustrates\nthat LBNet is an efficient and lightweight SISR model.\n4.5 Ablation Study\nSymmetric CNN Investigations. In Symmetric CNN, each\npair of LFFMs will be fused through a channel attention mod-\nule. In order to verify the effectiveness of this fusion method,\nwe conduct a series of experiments to study the different fea-\nture interaction methods in Symmetric CNN. It is worth not-\ning that we removed the Transformer part of the model to\nspeed up the training time in this ablation study. Table 2\nshows the results of these different feature interaction meth-\nScale FF\nSA CA Params Mult-Adds PSNR/SSIM\n×4 ! #\n# 96.1K 10.01G 25.28/0.7601\n×4 # ! # 96.3K 10.03G 25.31/0.7614\n×4 # # ! 96.5K 10.01G 25.36/0.7622\nTable 2: Study of the different feature interaction schemes in Sym-\nmetric CNN on Urban100 (×4). Best results are highlighted.\nMethod Params\nMult-Adds PSNR/SSIM\nLBNet+RCAB 228K 23.7G\n29.94/0.9002\nLBNet+IMDB 295K 31.3G\n30.21/0.9043\nLBNet+FRDAB (Ours) 365K 38.9G 30.33\n/0.9059\nTable 3: Performance comparisons of FRDAB and other basic units\non Manga109 for ×4 SR. The best results are highlighted.\nods. Among them, FF means the ordinary concatenate oper-\nation without attention module, SA and CA indicate the use\nof spatial attention and channel attention for fusion, respec-\ntively. Obviously, our method achieves the best results with\nthe same parameters and Multi-Adds level. This fully demon-\nstrates the effectiveness of our method.\nTo verify the effectiveness of our FRDAB, we replace\nFRDAB with some commonly used feature extraction mod-\nules in lightweight SISR models, like IMDB[Hui et al., 2019]\nand RCAB [Zhang et al., 2018]. It can be seen from Ta-\nble 3 that although FRDAB will bring a little increase of pa-\nrameters and Mult-Adds, its performance has been signifi-\ncantly improved. This gain is considerable, which benefit\nfor lightweight models construction. This fully proves that\nFRDAB is an effective feature extraction module.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n917\nUrban100 (4×):\nimg 005\nHR SRCNN DRCN IDN CARN-M\nPSNR/SSIM 25.12/0.8860 26.79/0.9328 27.64/0.9466 26.96/0.9409\nCARN IMDN MADNet LBNet-T (Ours) LBNet (Ours)\n27.70/0.9500 27.35/0.9472 27.09/0.9421 28.57/0.9539 28.70/0.9581\nFigure 5: Visual comparison with other SISR models. Obviously, our LBNet can reconstruct realistic SR images with accurate edges.\nMethod Params\nMulti-Adds Set5 Set14 BSD100 Urban100 Manga109 Average\nSwinIR 897K 49.6G\n32.44/0.8976 28.77/0.7858 27.69/0.7406 26.47/0.7980 30.92/0.9151 29.26/0.8274\nESRT 751K 67.7G\n32.19/0.8947 28.69/0.7833 27.69/0.7379 26.39/0.7962 30.75/0.9100 29.14/0.8244\nLBNet (Ours) 742K 38.9G 32.29/0.8960\n28.68/0.7832 27.62/0.7382 26.27/0.7906 30.76/0.9111 29.12/0.8238\nTable 4: Comparison with other Transformer-based methods. LBNet can achieve competitive results with fewer parameters and Multi-Adds.\nMethod Params\nMult-Adds Running time PSNR/SSIM\nw/o R\nT 365K 38.9028G\n0.0168s 32.07/0.8929\nwith RT 742K 38.9032G\n0.0274s 32.23/0.8949\nTable 5: Study of Recursive Transformer (RT) on Set5 dataset (×4).\nMethod Params\nMult-Adds Running time PSNR/SSIM\nTM-0 741.7K 38.9032G\n0.0274s 32.23/0.8949\nTM-1 741.7K 38.9036G\n0.0356s 32.27/0.8958\nTM-2 741.7K 38.9039G\n0.0401s 32.29/0.8960\nTM-3 741.7K 38.9043G\n0.0516s 32.30/0.8960\nTable 6: Study on the recursion times of Transformer Module (TM)\non Set5 (×4). The final version is highlighted.\nRecursive Transformer Investigations. In order to learn\nthe long-term dependence of images, we introduced the\nTransformer and proposed a Recursive Transformer (RT). To\nverify the effectiveness of the proposed RT, we remove RT\nand provide the results in Table 5. According to the table, we\ncan clearly observe that the introduced RT will increase the\nnumber of parameters. However, the increase in Multi-add\nand execution time is insignificant. In addition, the experi-\nment also shows that the PSNR/SSIM results of the model\nwith RT have been significantly improved. It shows that the\nintroduced Transformers can improve the learning capabili-\nties of the model thus improving the model performance.\nDifferent from previous Transformers, we introduced the\nrecursive mechanism in the Transformer Module (TM). This\ndesign can make the Transformer more fully utilized with-\nout increasing the number of model parameters. To verify\nthe effectiveness of the recursive mechanism in Transformer,\nwe conduct a series of experiments with different recursion\ntimes. In Table 6, TM-N denotes TM recursed N times and\nTM-0 represents the flat model that no recursive mechanism\nis used. Obviously, as the number of recursion times in-\ncreases, the performance of the model will further improve.\nMeanwhile, we also notice that when the number of recursion\ntimes is 3 (TM-3), the improvement of model performance\nis not obvious. Therefore, we set the recursion times as 2\n(S = 2) in the final model to achieve a better balance be-\ntween mode performance, Multi-Adds, and execution time.\nComparison with Other Transformer-based Methods.\nRecently, some Transformer-based methods have been pro-\nposed for SISR. In Table 4, we provide a detailed comparison\nwith SwinIR [Liang et al., 2021] and ESRT [Lu et al., 2022].\nAccording to the results, we can clearly observe that our LB-\nNet achieved competitive results with fewer parameters and\nMulti-Adds. Although the PSNR result of LBNet is 0.14dB\nworse than SwinIR, it is worth noting that SwinIR uses an\nadditional dataset (Flickr2K) for training. This is one of the\nkey factors to further improve model performance. All these\nresults further verify the effectiveness of LBNet.\n5 Conclusions\nIn this paper, we proposed a Lightweight Bimodal Network\n(LBNet) for SISR via Symmetric CNN and Recursive Trans-\nformer. Specifically, we proposed an effective Symmetric\nCNN for local feature extraction and proposed a Recursive\nTransformer to learn the long-term dependence of images. In\nSymmetric CNN, Local Feature Fusion Module (LFFM) and\nFeature Refinement Dual-Attention Block (FRDAB) are de-\nsigned to ensure sufficient feature extraction and utilization.\nIn Recursive Transformer, the recursive mechanism is intro-\nduced to fully train the Transformer, so the global information\nlearned by the Transformer can further refine the features. In\nsummary, LBNet elegantly integrates CNN and Transformer,\nachieving a better balance between the performance, size, ex-\necution time, and GPU memory consumption of the model.\nAcknowledgments\nThis work was supported in part by the National Key R&D\nProgram of China (No.2021YFE0203700), the National Nat-\nural Science Foundation of China (No.61972212), the Natural\nScience Foundation of Jiangsu Province (No.BK20190089),\nthe Six Talent Peaks Project in Jiangsu Province (No.RJFW-\n011), and the CRF (No.8730063).\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n918\nReferences\n[Ahn et al., 2018] Namhyuk Ahn, Byungkon Kang, and\nKyung. Fast, accurate, and lightweight super-resolution\nwith cascading residual network. In ECCV, 2018.\n[Bevilacqua et al., 2012] Marco Bevilacqua, Aline Roumy,\nChristine Guillemot, and Marie Line Alberi-Morel. Low-\ncomplexity single-image super-resolution based on non-\nnegative neighbor embedding. In BMVC, 2012.\n[Chen et al., 2021] Hanting Chen, Yunhe Wang, Tianyu\nGuo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image\nprocessing transformer. In CVPR, 2021.\n[Dong et al., 2014] Chao Dong, Chen Change Loy, Kaiming\nHe, and Xiaoou Tang. Learning a deep convolutional net-\nwork for image super-resolution. In ECCV, 2014.\n[Gao et al., 2022] Guangwei Gao, Wenjie Li, Juncheng Li,\nFei Wu, Huimin Lu, and Yi Yu. Feature distillation in-\nteraction weighting network for lightweight image super-\nresolution. In AAAI, 2022.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\n[Huang et al., 2015] Jia-Bin Huang, Abhishek Singh, and\nNarendra Ahuja. Single image super-resolution from\ntransformed self-exemplars. In CVPR, 2015.\n[Huang et al., 2017] Gao Huang, Zhuang Liu, Laurens Van\nDer Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In CVPR, 2017.\n[Hui et al., 2018] Zheng Hui, Xiumei Wang, and Xinbo Gao.\nFast and accurate single image super-resolution via infor-\nmation distillation network. In CVPR, 2018.\n[Hui et al., 2019] Zheng Hui, Xinbo Gao, Yunchu Yang, and\nXiumei Wang. Lightweight image super-resolution with\ninformation multi-distillation network. InACM MM, 2019.\n[Kim et al., 2016a] Jiwon Kim, Jung Kwon Lee, and Ky-\noung Mu Lee. Accurate image super-resolution using very\ndeep convolutional networks. In CVPR, 2016.\n[Kim et al., 2016b] Jiwon Kim, Jung Kwon Lee, and Ky-\noung Mu Lee. Deeply-recursive convolutional network for\nimage super-resolution. In CVPR, 2016.\n[Lan et al., 2020] Rushi Lan, Long Sun, Zhenbing Liu,\nHuimin Lu, Cheng Pang, and Xiaonan Luo. Madnet: A\nfast and lightweight network for single-image super reso-\nlution. IEEE TCYB, 51(3):1443–1453, 2020.\n[Li et al., 2021a] Juncheng Li, Zehua Pei, and Tieyong Zeng.\nFrom beginner to master: A survey for deep learning-\nbased single-image super-resolution. arXiv preprint\narXiv:2109.14335, 2021.\n[Li et al., 2021b] Wenbo Li, Kun Zhou, Lu Qi, Nianjuan\nJiang, Jiangbo Lu, and Jiaya Jia. Lapar: Linearly-\nassembled pixel-adaptive regression network for single\nimage super-resolution and beyond. In NeurIPS, 2021.\n[Liang et al., 2021] Jingyun Liang, Jiezhang Cao, Guolei\nSun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir:\nImage restoration using swin transformer. In ICCV, 2021.\n[Lim et al., 2017] Bee Lim, Sanghyun Son, Heewon Kim,\nSeungjun Nah, and Kyoung Mu Lee. Enhanced deep\nresidual networks for single image super-resolution. In\nCVPRW, 2017.\n[Lu et al., 2022] Zhisheng Lu, Juncheng Li, Hong Liu,\nChaoyan Huang, Linlin Zhang, and Tieyong Zeng. Trans-\nformer for single image super-resolution. In CVPRW,\n2022.\n[Martin et al., 2001] David Martin, Charless Fowlkes,\nDoron Tal, and Jitendra Malik. A database of human\nsegmented natural images and its application to evaluat-\ning segmentation algorithms and measuring ecological\nstatistics. In ICCV, 2001.\n[Matsui et al., 2017] Yusuke Matsui, Kota Ito, Yuji Aramaki,\nAzuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and\nKiyoharu Aizawa. Sketch-based manga retrieval using\nmanga109 dataset. MTAP, 76(20):21811–21838, 2017.\n[Tai et al., 2017] Ying Tai, Jian Yang, and Xiaoming Liu.\nImage super-resolution via deep recursive residual net-\nwork. In CVPR, 2017.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, 2017.\n[Wang et al., 2019] Chaofeng Wang, Zheng Li, and Jun\nShi. Lightweight image super-resolution with adap-\ntive weighted learning network. arXiv preprint\narXiv:1904.02358, 2019.\n[Wang et al., 2021] Longguang Wang, Xiaoyu Dong,\nYingqian Wang, Xinyi Ying, Zaiping Lin, Wei An, and\nYulan Guo. Exploring sparsity in image super-resolution\nfor efficient inference. In CVPR, 2021.\n[Xiao et al., 2021] Jun Xiao, Qian Ye, Rui Zhao, Kin-Man\nLam, and Kao Wan. Self-feature learning: An efficient\ndeep lightweight network for image super-resolution. In\nACM MM, 2021.\n[Zeyde et al., 2010] Roman Zeyde, Michael Elad, and Matan\nProtter. On single image scale-up using sparse-\nrepresentations. In ICCS, 2010.\n[Zhang et al., 2018] Yulun Zhang, Kunpeng Li, Kai Li,\nLichen Wang, Bineng Zhong, and Yun Fu. Image super-\nresolution using very deep residual channel attention net-\nworks. In ECCV, 2018.\n[Zhang et al., 2021a] Dongyang Zhang, Changyu Li, Ning\nXie, Guoqing Wang, and Jie Shao. Pffn: Progres-\nsive feature fusion network for lightweight image super-\nresolution. In ACM MM, 2021.\n[Zhang et al., 2021b] Xinyan Zhang, Peng Gao, Sunxiangyu\nLiu, Kongya Zhao, Guitao Li, Liuguo Yin, and Chang Wen\nChen. Accurate and efficient image super-resolution\nvia global-local adjusting dense network. IEEE TMM,\n23:1924–1937, 2021.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n919",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8433548808097839
    },
    {
      "name": "Transformer",
      "score": 0.6173251867294312
    },
    {
      "name": "Feature extraction",
      "score": 0.5620220303535461
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5110598802566528
    },
    {
      "name": "Code (set theory)",
      "score": 0.5031306147575378
    },
    {
      "name": "Deep learning",
      "score": 0.46108123660087585
    },
    {
      "name": "Image (mathematics)",
      "score": 0.41500234603881836
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4072608947753906
    },
    {
      "name": "Computer engineering",
      "score": 0.3375943601131439
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I41198531",
      "name": "Nanjing University of Posts and Telecommunications",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I184597095",
      "name": "National Institute of Informatics",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I177725633",
      "name": "Chinese University of Hong Kong",
      "country": "HK"
    }
  ]
}