{
  "title": "DeSTNet: Densely Fused Spatial Transformer Networks",
  "url": "https://openalex.org/W2866844117",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4302266589",
      "name": "Annunziata, Roberto",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302266590",
      "name": "Sagonas, Christos",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4302266591",
      "name": "Cali, Jacques",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963073614",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W2557728737",
    "https://openalex.org/W2951548327",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2035379092",
    "https://openalex.org/W2576915720",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3138516063",
    "https://openalex.org/W2118877769",
    "https://openalex.org/W2952054889",
    "https://openalex.org/W2562066862",
    "https://openalex.org/W2294957811",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2975535607",
    "https://openalex.org/W2396282411",
    "https://openalex.org/W2963671154",
    "https://openalex.org/W2082308025",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W603908379",
    "https://openalex.org/W2162931300",
    "https://openalex.org/W2340427832",
    "https://openalex.org/W252252322",
    "https://openalex.org/W2781155895",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2161969291",
    "https://openalex.org/W2151103935"
  ],
  "abstract": "Modern Convolutional Neural Networks (CNN) are extremely powerful on a range of computer vision tasks. However, their performance may degrade when the data is characterised by large intra-class variability caused by spatial transformations. The Spatial Transformer Network (STN) is currently the method of choice for providing CNNs the ability to remove those transformations and improve performance in an end-to-end learning framework. In this paper, we propose Densely Fused Spatial Transformer Network (DeSTNet), which, to our best knowledge, is the first dense fusion pattern for combining multiple STNs. Specifically, we show how changing the connectivity pattern of multiple STNs from sequential to dense leads to more powerful alignment modules. Extensive experiments on three benchmarks namely, MNIST, GTSRB, and IDocDB show that the proposed technique outperforms related state-of-the-art methods (i.e., STNs and CSTNs) both in terms of accuracy and robustness.",
  "full_text": "ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS1\nDeSTNet: Densely Fused Spatial\nTransformer Networks1\nRoberto Annunziata\nroberto.annunziata@onﬁdo.com\nChristos Sagonas\nchristos.sagonas@onﬁdo.com\nJacques Calì\njacques.cali@onﬁdo.com\nOnﬁdo Research\n3 Finsbury Avenue\nLondon, UK\nAbstract\nModern Convolutional Neural Networks (CNN) are extremely powerful on a range of\ncomputer vision tasks. However, their performance may degrade when the data is char-\nacterised by large intra-class variability caused by spatial transformations. The Spatial\nTransformer Network (STN) is currently the method of choice for providing CNNs the\nability to remove those transformations and improve performance in an end-to-end learn-\ning framework. In this paper, we propose Densely Fused Spatial Transformer Network\n(DeSTNet), which, to our best knowledge, is the ﬁrst dense fusion pattern for combining\nmultiple STNs. Speciﬁcally, we show how changing the connectivity pattern of multiple\nSTNs from sequential to dense leads to more powerful alignment modules. Extensive\nexperiments on three benchmarks namely, MNIST, GTSRB, and IDocDB show that the\nproposed technique outperforms related state-of-the-art methods (i.e., STNs and CSTNs)\nboth in terms of accuracy and robustness.\n1 Introduction\nRecently, signiﬁcant progress has been made in several real-world computer vision appli-\ncations, including image classiﬁcation [13, 22], face recognition [32], object detection and\nsemantic segmentation [12, 14, 31]. These breakthroughs are attributed to advances of CNNs\n[13, 16, 33], as well as the availability of huge amounts of data [21, 22] and computational\npower. In general, performance is adversely affected by intra-class variability caused by\nspatial transformations, such as afﬁne or perspective; therefore, achieving invariance to the\naforementioned transformations is highly desirable. CNNs achieve translation equivariance\nthrough the use of convolutional layers. However, the ﬁlter response is not in itself transfor-\nmation invariant. To compensate for this max-pooling strategies are often applied [4, 22].\nPooling is usually performed on very small regions (e.g., 2 ×2), giving it an effective rate\nof only a few pixels, increasing as we go deeper. Another technique used to achieve in-\nvariance is data augmentation [22]. Speciﬁcally, a set of known transformations are applied\nto training samples. However, this approach has the following disadvantages: (i) the set of\n1Accepted for publication at the 29th British Machine Vision Conference (BMVC 2018)\nc⃝2018. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.\narXiv:1807.04050v2  [cs.CV]  16 Jul 2018\n2ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\n...pinit\nImage\np1' p2' p3' p4'p2 p3 p4 p5' p5\np1\np-STN p-STNp-STNp-STNp-STN\np2 p3 p4 p5\npT\np1\nFigure 1: DeSTNet - A stack of Densely fused Spatial Transformer Networks.\ntransformations must be deﬁned a-priori; and (ii) a large number of samples are required,\nthus reducing training efﬁciency.\nArguably, one of best known methods used to efﬁciently increase invariance to geometric\ntransformations in CNNs is the Spatial Transformer Network (STN) [19]. STN provides an\nend-to-end learning mechanism that can be seamlessly incorporated into a CNN to explicitly\nlearn how to transform the input data to achieve spatial invariance. One might look at an\nSTN as an attention mechanism that manipulates a feature map in a way that the input is\nsimpliﬁed for some process downstream, e.g. image classiﬁcation. For example, in [5] an\nSTN was used in a supervised manner in order to improve the performance of a face detec-\ntor. Similarly, a method based on STN for performing simultaneously face alignment and\nrecognition was introduced in [38]. Although the incorporation of the STN within CNNs\nled to state-of-the-art performance, its effectiveness could reduce drastically in cases where\nthe face is heavily deformed (e.g. due to facial expressions). To overcome this issue, Wu et\nal. [37] proposed multiple STNs linked in a recurrent manner. One of the main drawbacks\nwhen combining multiple STNs can be seen in the boundary pixels. Each STN samples the\noutput image produced by the previous, thus as the image passes through multiple transforms\nthe quality of the transformed image deteriorates. In cases where initial bounding boxes are\nnot of sufﬁcient accuracy, transformed images are heavily affected by the boundary effect,\nshown in [25]. To overcome this and inspired by the Lucas-Kanade algorithm [27], Lin and\nLucey [25] proposed Compositional STNs (CSTNs) and their recurrent version ICSTNs.\nCSTNs are made up of an STN variant (henceforth, p-STN), which propagates transforma-\ntion parameters instead of the transformed images.\nIn this work, building on the success of p-STNs, we present DeSTNet (Fig. 1), an end-\nto-end framework designed to increase spatial invariance in CNNs. Firstly, motivated by\ninformation theory principles, we propose a dense fusion connectivity pattern for p-STNs.\nSecondly, we introduce a novel expansion-contraction fusion block for combining the pre-\ndictions of multiple p-STNs in a dense manner. Finally, extensive experimental results on\ntwo public benchmarks and a non-public real-world dataset suggest that the proposed DeST-\nNet outperforms the state-of-the-art CSTN[25] and the original STN [19].\n2 Related Work\nGeometric transformations can be mitigated through the use of either (i) invariant or equiv-\nariant features; (ii) encoding some form of attention mechanism. More traditional computer\nvision systems achieved this through the use of hand-crafted features such as HOG [9],\nSIFT [26] and SCIRD [1, 2] that were designed to be invariant to various transformations. In\nANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS3\nCNNs translation equivariance is achieved through convolutions and limited spatial invari-\nance from pooling.\nIn [20], a method for creating scale-invariant CNNs was proposed. Locally scale-\ninvariant representations are obtained by applying ﬁlters at multiple scales and locations\nfollowed by max-pooling. Rotational invariance can be achieved by discretely rotating the\nﬁlters [6, 7, 28] or input images and feature maps [10, 23, 30]. Recently, a method for\nproviding continuous rotation robustness was proposed in [36]. To facilitate the translation\ninvariance property of CNNs, Henriques and Vedaldi [15] proposed to transform the image\nvia a constant warp and then employ a simple convolution. Although, the aforementioned is\nvery simple and powerful, it requires prior knowledge of the type of transformation as well\nas the location inside the image where it is applied.\nMore related to our work are methods that encode an attention or detection mechanism.\nSzegedy et al. [35] introduced a detection system as a form of regression within the network\nto predict object bounding boxes and classiﬁcation results simultaneously. Erhan et al. [11]\nproposed a saliency-inspired neural network that predicts a set of class-agnostic bounding\nboxes along with a likelihood of each box containing the object of interest. A few years later,\nHe et al. [14] designed a network that performs a number of complementary tasks: classiﬁca-\ntion, bounding box prediction and object segmentation. The region proposal network within\ntheir model provided a form of learnt attention mechanism. For a more thorough review of\nobject detection systems we point the reader to Huanget al. [17] who look at speed/accuracy\ntrade-offs for modern detection systems.\n3 Methodology\nLet D= {I1,I2,..., IM}be a set of M images and {pi}M\ni=1 ∈Rn (n = 8 for perspective) 1\nthe initial estimation of the distortion parameters for each image. Our goal is to reduce the\nintra-class variability due to the perspective transformations inherently applied to the images\nduring capture. Achieving this goal has the potential to signiﬁcantly simplify subsequent\ntasks, such as classiﬁcation. To this end, we need to ﬁnd the optimal parameters{p∗\ni }M\ni=1 that\nwarp all the images into a transformation-free space.\nArguably, the most notable method for ﬁnding the optimal parameters is the STN [19].\nAn STN is made up of three components, namely the localization network, the grid gen-\nerator and the sampler. The localization network L is used to predict transformation pa-\nrameters for a given input image I and initial parameters pinit, i.e. p = L(I,pinit), the grid\ngenerator and sampler are used for warping the image based on the computed parameters,\ni.e. I(W(p)) (Fig. 2(a)). By allowing the network to learn how to warp the input, it is\nable to gain geometric invariance, thus boosting task performance. When recovering larger\ntransformations a number of STNs can be stacked or used in combination with a recurrent\nframework (Fig. 2(b)). However, this tends to introduce boundary artifacts and image quality\ndegradation in the ﬁnal transformed image, as each STN re-samples from an image that is\nthe result of multiple warpings.\nTo address the aforementioned and inspired by the success of the LK algorithm for image\nalignment, Lin and Lucey [25] proposed compositional STNs (CSTNs). The LK algorithm\nis commonly used for alignment problems [3, 29] as it approximates the linear relationship\nbetween appearance and geometric displacement. Speciﬁcally, given two images I1, I2 that\n1This initial estimation may simply be an identity.\n4ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\nInput \nimage I\np\nLocalizer L\nRecti ed\nimage \nI(W(p))\nSTN\nWarp W\n...Input \nimage I Irec\n1 Irec2 Irec\nT\n(a) (b)\nFigure 2: (a) Spatial Transformer Network (STN) [19] and (b) stack of STNs.\nWarp W\nInput \nimage I\npinit\nLocalizer L\nI(W(pinit))\np\nCompose pout\np-STN\npinit ... pT\np1 p2 p3\nInput \nimage I\np-STNp-STNp-STN\np1 p2 p3\n(a) (b)\nFigure 3: (a) Compositional STN (CSTN) [25] and (b) stack of CSTNs.\nare related by a parametric transformation W, the goal of LK is to ﬁnd the optimal param-\neters that minimize the ℓ2 norm of the error between the deformed version of I1, and I2:\nmin\np\n∥I1(W(p)) −I2∥2\n2. Applying ﬁrst-order Taylor expansion to I1, it has been shown that\nthe previous problem can be optimised by an iterative algorithm with the following additive-\nbased update rule:\npt+1 = pt + ∆pt , (1)\nat each iteration t. In [25], Lin and Lucey introduced the CSTN that predicts the parameters’\nupdates by employing a modiﬁed STN, which we refer to asp-STN, and then compose them\nas in Eq. (1). By incorporating the LK formulation, the resulting CSTN is able to inherit the\ngeometry preserving property of LK. Unlike a stack of STNs that propagateswarped images\nto recover large displacements (Fig. 2(b)), a stack of CSTNs (Fig. 3(b)) propagate the warp\nparameters in a similar fashion to the iterative process used in the LK algorithm.\nHere, we extend the CSTN framework to improve the information ﬂow in terms of pa-\nrameters’ updates. In particular, we modify Eq. (1) and propose the additive-based dense\nfusion update rule:\npt+1 = pt + f (∆p′\nt ,∆p′\nt−1,..., ∆p′\n1), (2)\nwhere the parameters’ update at iteration t, ∆pt , is now a function f : Rn×t →Rn of the\nupdates predicted by the p-STN at iteration t, ∆p′\nt , and all the previous ones, {∆p′\ni}t−1\ni=1\n(Fig. 1). Learning the fusion function f (·) at each iteration t means learning the posterior\ndistribution p(∆pt |∆p′\nt ,∆p′\nt−1,..., ∆p′\n1) for the parameters’ update ∆pt . From an infor-\nmation theory perspective, this amounts to predicting ∆pt with an uncertainty measured by\nthe conditional entropy, H(∆pt |∆p′\nt ,∆p′\nt−1,..., ∆p′\n1). We notice that the CSTN update in\nEq. (1) is a special case of Eq. (2):\npt+1 = pt + f (∆p′\nt ), (3)\nwhere the parameters’ update at iterationt, ∆pt , is only a function of the update predicted by\nthe tth regressor (∆p′\nt ). In fact, no fusion has to be applied (i.e., f (·) is an identity mapping)\nand ∆pt = ∆p′\nt . In other words, the CSTN learns the distribution p(∆pt ) for the parameters’\nupdate ∆pt at each iteration t. This amounts to predicting ∆pt with an uncertainty measured\nANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS5\nc\n1 1\n8\n1 1\n24\n1 1\n8\n1 1\n8\n1 1\n8h2\np1'\np2'\np3'\np3\nconv1-8\nc\n1 1\n8\n1 1\n24\n1 1\n3kF\n1 1\n8\n1 1\n8\n1 1\n8h1 h2\np1'\np2'\np3'\np3\nconv1-8conv1-3 kF\n(a) (b)\nFigure 4: Fusion blocks. (a) The bottleneck-based fusion block proposed in [16]. (b) The\nproposed expansion-contraction fusion block used in Figure 1.\nby the related entropy, H(∆pt ). Invoking the well-known ‘conditioning reduces entropy’\nprinciple from information theory [8], it can be shown that H(∆pt |∆p′\nt ,∆p′\nt−1,..., ∆p′\n1) ≤\nH(∆pt ). In other words, the update predictions in the proposed formulation are upper-\nbounded by those made with CSTN in terms of uncertainty. We advocate that this theoretical\nadvantage can translate into better performance.\nInspired by the recent success of densely connected CNNs [16] and justiﬁed by the ex-\ntension outlined above, we propose an alignment module which we call DeSTNet (Densely\nfused Spatial Transformer Network). DeSTNet consists of a cascade ofp-STNs with a dense\nfusion connectivity pattern, as shown in Fig. 1. The fusion function, implemented by the\nfusion block Fin Fig. 1, is adopted to combine the update predictions of all the previ-\nous p-STNs and estimate the best parameters’ update at each level t. Unlike the fusion\nblocks adopted in [16] consisting of a single bottleneck layer (Fig. 4(a)), we advocate the\nuse of an expansion-contraction fusion block (Fig. 4(b)). This solves the fusion task in a\nhigh-dimensional space and then maps the result back to the original. Speciﬁcally, we con-\ncatenate all the previous parameters’ updates and project them using a 1 ×1 convolution\nlayer with depth n ×t ×kF (expansion), where n is the dimension of the warp parameters p,\nt = 1,..., T is the level within DeSTNet, and kF is the expansion rate. This is then followed\nby a 1 ×1 ×n convolution layer (contraction), as shown in Fig. 4(b). We adopt tanh activa-\ntions (non-linearities) after each convolutional layer of the fusion block to be able to predict\nboth positive and negative parameter values. It is worth noting that the use of expansion\nlayers is made possible by the relatively low dimension of each individual prediction (i.e.,\nn = 8 for perspective warps).\n4 Experiments\nIn this section, we assess the effectiveness of the proposed DeSTNet in (i) adding spatial\ntransformation invariance (up to perspective warps) to CNN-based classiﬁcation models\nand (ii) planar image alignment. To this end, artiﬁcially distorted versions of two widely\nused datasets, namely the German Trafﬁc Sign Recognition Benchmark (GTSRB) [34] and\nMNIST [24] are utilised. Furthermore, we evaluate the performance of DeSTNet on a non-\npublic dataset of ofﬁcial identity documents (IDocDB), which includes substantially larger\nimages (e.g. up to 6,016×3,910 pixels) and, more importantly, real perspective transforma-\ntions. Additional results can be found in supplementary material.\n6ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\nModel Test Error Architecture\nAlignment Classiﬁer\nGTSRB\nCNN 8.29% conv7-6 | conv7-12 | P | conv7-24 | FC(200) | FC(43)\nSTN 6.49% conv7-6 | conv7-24 | FC(8) conv7-6 | conv7-12 | P | FC(43)\nCSTN-1 5.01% [ conv7-6 | conv7-24 | FC(8) ]×1 conv7-6 | conv7-12 | P | FC(43)\nICSTN-4 3.18% [ conv7-6 | conv7-24 | FC(8) ]×4 conv7-6 | conv7-12 | P | FC(43)\nCSTN-4 3.15% [ conv7-6 | conv7-24 | FC(8) ]×4 conv7-6 | conv7-12 | P | FC(43)\nDeSTNet-4 1.99% F{[ conv7-6 | conv7-24 | FC(8) ]×4} conv7-6 | conv7-12 | P | FC(43)\nMNIST\nCNN 6.60% conv3-3 | conv3-6 | P | conv3-9 | conv3-12 | FC(48) | FC(10)\nSTN 4.94% conv7-4 | conv7-8 | P | FC(48) | FC(8) conv9-3 | FC(10)\nCSTN-1 3.69% [ conv7-4 | conv7-8 | P | FC(48) | FC(8) ]×1 conv9-3 | FC(10)\nICSTN-4 1.23% [ conv7-4 | conv7-8 | P | FC(48) | FC(8) ]×4 conv9-3 | FC(10)\nCSTN-4 1.04% [ conv7-4 | conv7-8 | P | FC(48) | FC(8) ]×4 conv9-3 | FC(10)\nDeSTNet-4 0.71% F{[ conv7-4 | conv7-8 | P | FC(48) | FC(8) ]×4} conv9-3 | FC(10)\nTable 1: Test classiﬁcation errors of the compared models on GTSRB and MNIST datasets.\n4.1 Image Classiﬁcation\nTrafﬁc Signs: We report experimental results on the GTSRB dataset [34], consisting of\n39,209 training and 12,630 test colour images from 43 trafﬁc signs taken under various real-\nworld conditions including motion blur, illumination changes and extremely low resolution.\nWe adopt the image classiﬁcation error as a proxy measure for alignment quality. Speciﬁ-\ncally, we build classiﬁcation pipelines made up of two components: an alignment network\nfollowed by a classiﬁcation one (detailed architectures reported in Table 1). Both networks\nare jointly trained with the classiﬁcation-based loss using standard back-propagation. At\nparity of a classiﬁcation network, a lower classiﬁcation error suggests better alignment (i.e.,\nspatial transformation invariance). Following the experimental protocol in [25], we resize\nimages to s ×s, s = 36 pixels and artiﬁcially distort them using a perspective warp. Speciﬁ-\ncally, the four corners of each image are independently and randomly scaled with Gaussian\nnoise N(0,(σs)2), then randomly translated with the same noise model.\nIn the ﬁrst experiment, we follow the same setting adopted in [25] and train all the net-\nworks for 200,000 iterations with a batch of 100 perturbed samples generated on the ﬂy.\nFor DeSTNet, we use αclf = 10−2 as the learning rate for the classiﬁcation network and\nαaln = 10−4 for the alignment network which is reduced by 10 after 100,000 iterations. For\nthe proposed expansion-contraction fusion block we set the expansion rate kF = 256, as a\ngood trade-off between speed and performance, we use dropout with keep probability equal\nto S = 0.9. Finally, images of both train and test sets are perturbed using σ = 10%, corre-\nsponding to a maximum perturbation of 3.6 pixels.\nWe compare the performance of DeSTNet to the most related methods, STN [19], a sin-\ngle CSTN (CSTN-1) [25], and stack of four CSTNs (CSTN-4) [25]. For completeness, we\nreport classiﬁcation results of a CNN with roughly the same number of learnable parameters\nand the recurrent version of CSTN (i.e., ICSTN) [25]. To isolate the contribution of the align-\nment module, we adopt the same CNN classiﬁer for all. By examining Table 1 2 we observe\nthat alignment improves classiﬁcation performance, irrespective of the speciﬁc alignment\nmodule, supporting the need for removing perspective transformations with which a stan-\ndard CNN classiﬁer would not be able to cope. 3 Importantly, CSTN-1 achieves lower clas-\nsiﬁcation error as compared to the STN (5.01% vs 6.49%), thus supporting our architectural\n2 convD1-D2: convolution layer with D1 ×D1 receptive ﬁeld and D2 channels, P: max-pooling layer, FC: fully\nconnected layer, F: fusion operation used in DeSTNet for combining the parameters’ updates, ˜F: standard fusion\noperation [16].\n3Convolution and max-pooling help with small transformations, but are not enough to cope with full perspective\nwarpings.\nANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS7\nModel\nTest error Architecture\nPerturbation σ Alignment Classiﬁer10% 20% 30%\nGTSRB\nCSTN-4 6.86% 8 .92% 13 .72% [ conv7-6 | conv7-24 | FC(8) ]×4 FC(43)\nDeSTNet-4 ( ˜F) 3.60% 4 .65% 5 .25% ˜F{[ conv7-6 | conv7-24 | FC(8) ]×4} FC(43)\nDeSTNet-4 3.04% 3.80% 3.85% F{[ conv7-6 | conv7-24 | FC(8) ]×4} FC(43)\nMNIST\nC-STN-4 1.50% 2 .39% 3 .40% [ conv7-4 | conv7-8 | P | FC(48) | FC(8) ]×4 FC(10)\nDeSTNet-4 ( ˜F) 0.86% 0 .89% 1 .09% ˜F{[ conv7-4 | conv7-8 | P | FC(48) | FC(8) ]×4} FC(10)\nDeSTNet-4 0.66% 0.72% 0.74% F{[ conv7-4 | conv7-8 | P | FC(48) | FC(8) ]×4} FC(10)\nTable 2: Test classiﬁcation errors of the compared models by using a single fully connected\nlayer as classiﬁer under three perturbation levels on GTSRB and MNIST datasets.\nchoice of building DeSTNet using p-STNs. Moreover, using a cascade of four CSTNs fur-\nther improves results. Finally, the DeSTNet-4 outperforms CSTN-4 with an error of 1 .99%\ndown from 3.15% which amounts to a relative improvement of 37%.\nIt is worth noting, (i) the perturbations in this experiment are relatively small (σ = 10%)\nand (ii) the CNN network followed by a fully connected layer as classiﬁer does not fully off-\nload the alignment task to the alignment network. This is due to the translation invariance\nand robustness to small transformations brought about by the convolutions and pooling lay-\ners. Therefore, to further investigate the alignment quality of the state-of-the-art CSTN and\nDeSTNet, we use a single fully connected layer as a classiﬁcation network and report perfor-\nmance under three perturbation levels σ = {10%,20%,30%}corresponding to a minimum\nof 3.6 and a maximum of 10.8 pixels. Results in Table 22 show that, (i) DeSTNet yields an\nalignment quality that signiﬁcantly simpliﬁes the classiﬁcation task compared to CSTN (i.e.,\nup to 9.87% better classiﬁcation performance for DeSTNet); ( ii) DeSTNet exhibits robust-\nness against stronger perturbation levels, with performance degrading by only 0 .81% from\n10% to 30% perturbation, while CSTN performance degrades by 6 .86% in the same range;\nand ( iii) the proposed expansion-contraction fusion block Fleads to better performance\nw.r.t. the standard bottleneck layer ˜Fproposed in [16]. Qualitative experimental results\nfor CSTN and DeSTNet under different perturbation levels are reported in Fig. 5. More\nInitial\nCSTN-4\nDeSTNet-4\n(a) σ = 10% (b) σ = 20% (c) σ = 30%\nFigure 5: Qualitative comparison of CSTN-4 and DeSTNet-4 methods on GTSRB dataset.\nAverages of the test trafﬁc signs under different perturbation levels.\n8ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\n(a) GTSRB (b) MNIST\nFigure 6: Sample alignment results produced by the DeSTNet-4 model on three examples\n(rows) from the GTSRB (a) and the MNIST (b) datasets. Column 1: input image; columns\n2-5: results obtained by applying the intermediate perspective transformations predicted at\nlevels 1-4, respectively.\nspeciﬁcally, the averages of the 43 trafﬁc signs before and after convergence for CSTN-4\nand DeSTNet-4 are shown. We observe that the average images produced by DeSTNet-4\nare much sharper and have more details (even for the 30% perturbation level, Fig. 5(c)) than\nthe averages produced by CSTN-4, this is indicative of the better alignment performance\nfor the proposed model. Fig. 6(a) illustrates aligned examples generated by DeSTNet-4.\nHandwritten Digits: For this experiment, we adopt MNIST dataset [24], consisting of\nhandwritten digits between 0 and 9, with a training set of 60 ,000 and 10,000 test grayscale\nimages (28×28 pixels). We adopt the same settings as for the GTSRB experiments by using\nthe image classiﬁcation error as aproxy measure for alignment quality. Training and test sets\nare distorted using the same perspective warp noise model ( σ = 12.5%, corresponding to a\nmaximum perturbation of 3.5 pixels).\nExperimental results are reported in Table 1 2. In line with the GTSRB experiments, (i)\npre-alignment considerably improves classiﬁcation performance, regardless of the speciﬁc\nalignment module used; (ii) lower classiﬁcation error is achieved when using CSTN-1 as\ncompared to STN, again supporting our choice of using p-STNs as base STNs in DeST-\nNet; (iii) although performance almost saturates with four CSTNs, DeSTNet is still able\nto squeeze extra performance, outperforming CSTN-4 with an error of 0 .71% down from\n1.04% which is a relative improvement of 32%.\nWe further investigate the alignment quality of the state-of-the-art CSTN and DeSTNet,\nwhen a single fully connected layer is used for classiﬁcation and report performance under\nthree perturbation levels corresponding to a minimum of 2 .8 pixels and a maximum of 8 .4\npixels. By inspecting the results reported in Table 22, we can see that, (i) DeSTNet achieves\nan alignment quality that signiﬁcantly simpliﬁes the classiﬁcation task compared to CSTN\n(i.e., up to 2 .66% better classiﬁcation performance for DeSTNet); (ii) DeSTNet exhibits\nrobustness against stronger perturbation levels, with the classiﬁcation performance degrading\nInitial\nCSTN-4\nDeSTNet-4\nInitial\nCSTN-4\nDeSTNet-4\n(a) σ = 10% (b) σ = 20% (c) σ = 30%\nFigure 7: Qualitative comparison of CSTN-4 and DeSTNet-4 on the MNIST dataset. Mean\n(top rows) and variance (bottom rows) of the 10 digits under different perturbation levels.\nANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS9\nAUC@0.04\n0.72\n0.77\n(a) (b)\nFigure 8: (a) Cumulative Error Distribution curves and (b) qualitative results obtained by the\nCSTN-5 and DeSTNet-5 on IDocDB.\nby only 0.08% from 10% to 30% perturbation, while CSTN performance degrades by 1.90%\nin the same range; and (iii) the proposed expansion-contraction fusion block further helps\nreducing the classiﬁcation test error.\nQualitative experimental results are reported in Fig. 7. In particular, the average and\ncorresponding variance of all test samples grouped by digit are computed and shown for\nCSTN-4 and DeSTNet-4. Inspecting the images we can see that the mean images generated\nby DeSTNet-4 are sharper than those of CSTN-4 while the variance ones are thinner. This\nsuggests that DeSTNet is more accurate and robust to different perturbation levels compared\nto CSTN. Finally, aligned images generated by the DeSTNet-4 are displayed in Fig. 6(b).\n4.2 Document Alignment\nHere, we show how DeSTNet can be successfully utilised for aligning planar images. To this\nend, we make use of our non-public ofﬁcial identity documents dataset (IDocDB) consist-\ning of 1,000 training and 500 testing colour images collected under in-the-wild conditions.\nSpeciﬁcally, each image contains a single identity document (UK Driving Licence V2015)\nand their size ranges from 422 ×215 to 6 ,016 ×3,910 pixels. In addition to typical chal-\nlenges such as non-uniform illumination, shadows, and compression noise, several other\naspects make this dataset challenging, including: the considerable variations in resolution;\nhighly variable background which may include clutter and non-target objects; occlusion, e.g.\nthe presence of ﬁngers covering part of the document when held for capture. The ground\ntruth consists of the location of the four corners of each document. From these points, we\ncan compute a homography matrix that maps each document to a reference frame. The\nalignment task can be solved by predicting the location of the corner points on each input\nimage. We train the networks using the smoothℓ1 loss [31] between the ground truth and the\npredicted corner coordinates.\nAdopting the following experimental setting: we resize each image to 256 ×256 pixels\nfor computational efﬁciency, as done for instance in [18, 33]. We set the learning rate for\nthe localisation network to αaln = 10−4, which we reduce by 10 after 20,000 iterations. We\nuse batches with 8 images each for all the models. For the fusion blocks of DeSTNet, we\nset kF = 256 and use S = 0.9. We assess the performance of DeSTNet and compare it with\n10ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\nthe state-of-the-art CSTNs (strongest baseline based on the presented experiments). Given\nthe increased complexity of the task compared to MNIST and GTSRB, we built networks\nwith ﬁve STNs for both CSTN and DeSTNet (architectures are reported in Table 1 of supple-\nmentary material). For comparison, we use the average point-to-point Euclidean distance,\nnormalised by each document’s diagonal, between the ground truth and predicted location\nof the four corners. In addition, the Cumulative Error Distribution (CED) curve for each\nmethod is computed using the fraction of test images for which the average error is smaller\nthan a threshold. The CED curves in Fig. 8(a) show that DeSTNet-5 outperforms CSTN-5\nboth in terms of accuracy and robustness. In fact, DeSTNet achieves a higher AUC@0 .04\n(0.77 vs 0.72). Qualitative results for CSTN and DeSTNet are displayed in Fig. 8(b).\n5 Conclusions\nIt is well-known that image recognition is adversely affected by spatial transformations. In-\ncreasing geometric invariance helps to improve performance. Although CNNs achieve some\nlevel of translation equivariance, they are still susceptible to large spatial transformations.\nIn this paper, we address this problem by introducing DeSTNet, a stack of densely fused\nSTNs that improve information ﬂow in terms of warp parameters’ updates. Furthermore,\nwe provide a novel fusion technique demonstrating its improved performance in our prob-\nlem setting. We show the superiority of DeSTNet over the current state-of-the-art STN\nand its variant CSTN, by conducting extensive experiments on two widely-used benchmarks\n(MNIST, GTSRB) and a new non-public real-world dataset of ofﬁcial identity documents.\nAcknowledgements. We would like to thank all the members of the Onﬁdo research\nteam for their support and candid discussions.\nReferences\n[1] Roberto Annunziata and Emanuele Trucco. Accelerating convolutional sparse cod-\ning for curvilinear structures segmentation by reﬁning SCIRD-TS ﬁlter banks. IEEE\nTransactions on Medical Imaging (IEEE–TMI), 35(11):2381–2392, 2016.\n[2] Roberto Annunziata, Ahmad Kheirkhah, Pedram Hamrah, and Emanuele Trucco. Scale\nand curvature invariant ridge detector for tortuous and fragmented structures. In Inter-\nnational Conference on Medical Image Computing and Computer-Assisted Interven-\ntion (MICCAI), pages 588–595. Springer, 2015.\n[3] Simon Baker and Iain Matthews. Lucas-kanade 20 years on: A unifying framework.\nInternational Journal of Computer Vision (IJCV), 56(3):221–255, 2004.\n[4] Y-Lan Boureau, Jean Ponce, and Yann LeCun. A theoretical analysis of feature pooling\nin visual recognition. InProceedings of International Conference on Machine Learning\n(ICML), pages 111–118, 2010.\n[5] Dong Chen, Gang Hua, Fang Wen, and Jian Sun. Supervised transformer network for\nefﬁcient face detection. In Proceedings of European Conference on Computer Vision\n(ECCV), pages 122–138. Springer, 2016.\nANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS11\n[6] Taco Cohen and Max Welling. Group equivariant convolutional networks. In Pro-\nceedings of International Conference on Machine Learning (ICML), pages 2990–2999,\n2016.\n[7] Taco S Cohen and Max Welling. Steerable cnns. Proceedings of International Confer-\nence on Learning Representations (ICLR), 2017.\n[8] Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley &\nSons, 2012.\n[9] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human detec-\ntion. In Proceedings of IEEE International Conference on Computer Vision & Pattern\nRecognition (CVPR), volume 1, pages 886–893, 2005.\n[10] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic sym-\nmetry in convolutional neural networks. In Proceedings of International Conference\non Machine Learning (ICML), 2016.\n[11] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. Scal-\nable object detection using deep neural networks. InProceedings of IEEE International\nConference on Computer Vision & Pattern Recognition (CVPR) , pages 2147–2154,\n2014.\n[12] Ross Girshick. Fast r-cnn. In Proceedings of IEEE International Conference on Com-\nputer Vision & Pattern Recognition (CVPR), pages 1440–1448, 2015.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of IEEE International Conference on Computer\nVision & Pattern Recognition (CVPR), pages 770–778, 2016.\n[14] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn. In Pro-\nceedings of IEEE International Conference on Computer Vision (ICCV) , pages 2980–\n2988, 2017.\n[15] Joao F Henriques and Andrea Vedaldi. Warped convolutions: Efﬁcient invariance to\nspatial transformations. In Proceedings of International Conference on Machine Learn-\ning (ICML), 2017.\n[16] Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely\nconnected convolutional networks. In Proceedings of IEEE International Conference\non Computer Vision & Pattern Recognition (CVPR), volume 1, page 3, 2017.\n[17] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop Korattikara,\nAlireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio Guadarrama, et al.\nSpeed/accuracy trade-offs for modern convolutional object detectors. In Proceedings\nof IEEE International Conference on Computer Vision & Pattern Recognition (CVPR),\n2017.\n[18] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image trans-\nlation with conditional adversarial networks. In Proceedings of IEEE International\nConference on Computer Vision & Pattern Recognition (CVPR) , pages 1125–1134,\n2017.\n12ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\n[19] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer net-\nworks. In Proceedings of Advances in Neural Information Processing Systems (NIPS),\npages 2017–2025, 2015.\n[20] Angjoo Kanazawa, Abhishek Sharma, and David Jacobs. Locally scale-invariant con-\nvolutional neural networks. arXiv preprint arXiv:1412.5104, 2014.\n[21] Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The\nmegaface benchmark: 1 million faces for recognition at scale. In Proceedings of IEEE\nInternational Conference on Computer Vision & Pattern Recognition (CVPR) , pages\n4873–4882, 2016.\n[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with\ndeep convolutional neural networks. InProceedings of Advances in Neural Information\nProcessing Systems (NIPS), pages 1097–1105, 2012.\n[23] Dmitry Laptev, Nikolay Savinov, Joachim M Buhmann, and Marc Pollefeys. Ti-\npooling: transformation-invariant pooling for feature learning in convolutional neural\nnetworks. In Proceedings of IEEE International Conference on Computer Vision &\nPattern Recognition (CVPR), pages 289–297, 2016.\n[24] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.\ncom/exdb/mnist/, 1998.\n[25] Chen-Hsuan Lin and Simon Lucey. Inverse compositional spatial transformer net-\nworks. In Proceedings of IEEE International Conference on Computer Vision & Pat-\ntern Recognition (CVPR), pages 2568–2576, 2017.\n[26] David G Lowe. Distinctive image features from scale-invariant keypoints.International\nJournal of Computer Vision (IJCV), 60(2):91–110, 2004.\n[27] Bruce D Lucas and Takeo Kanade. An iterative image registration technique with\nan application to stereo vision. In Proceedings of International Joint Conference on\nArtiﬁcial Intelligence (IJCAI), pages 674–679, 1981.\n[28] Diego Marcos, Michele V olpi, and Devis Tuia. Learning rotation invariant convolu-\ntional ﬁlters for texture classiﬁcation. In Proceedings of International Conference on\nPattern Recognition (ICPR), pages 2012–2017, 2016.\n[29] Iain Matthews and Simon Baker. Active appearance models revisited. International\nJournal of Computer Vision (IJCV), 60(2):135–164, 2004.\n[30] Edouard Oyallon and Stéphane Mallat. Deep roto-translation scattering for object clas-\nsiﬁcation. In Proceedings of IEEE International Conference on Computer Vision &\nPattern Recognition (CVPR), volume 3, page 6, 2015.\n[31] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-\ntime object detection with region proposal networks. In Proceedings of Advances in\nNeural Information Processing Systems (NIPS), pages 91–99, 2015.\n[32] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed em-\nbedding for face recognition and clustering. In Proceedings of IEEE International\nConference on Computer Vision & Pattern Recognition (CVPR), pages 815–823, 2015.\nANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS13\n[33] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\nscale image recognition. In Proceedings of International Conference on Learning Rep-\nresentations (ICLR), 2014.\n[34] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german\ntrafﬁc sign recognition benchmark: a multi-class classiﬁcation competition. In Pro-\nceedings of International Joint Conference on Neural Networks (IJCNN), pages 1453–\n1460, 2011.\n[35] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for\nobject detection. In Proceedings of Advances in Neural Information Processing Sys-\ntems (NIPS), pages 2553–2561, 2013.\n[36] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov, and Gabriel J Brostow.\nHarmonic networks: Deep translation and rotation equivariance. In Proceedings of\nIEEE International Conference on Computer Vision & Pattern Recognition (CVPR) ,\nvolume 2, 2017.\n[37] Wanglong Wu, Meina Kan, Xin Liu, Yi Yang, Shiguang Shan, and Xilin Chen. Recur-\nsive spatial transformer (rest) for alignment-free face recognition. In Proceedings of\nIEEE International Conference on Computer Vision (ICCV), pages 3772–3780, 2017.\n[38] Yuanyi Zhong, Jiansheng Chen, and Bo Huang. Toward end-to-end face recognition\nthrough alignment learning. IEEE Signal Processing Letters, 24(8):1213–1217, 2017.\n14ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\n6 Supplementary Material\n6.1 Additional Results for Section 4.1\nFigures 9 and 10 show additional alignment results obtained by the proposed DeSTNet model\non GTSRB [34] and MNIST [24] datasets, respectively.\nFigure 9: Sample alignment results produced by the DeSTNet-4 model on the GTSRB\ndataset. Row 1: input image. Rows 2-4: results produced after each one of the four lev-\nels.\nFigure 10: Sample alignment results produced by the DeSTNet-4 model on the MNIST\ndataset. Row 1: input image. Rows 2-4: results produced after each one of the four levels.\n6.2 Architectures and Additional Results for Section 4.2\nTable 3 reports the architectures of the compared CSTN-5 [25] and DeSTNet-5 models for\nthe task of planar image alignment.\nAdditional qualitative results obtained by the CSTN-5 and DeSTNet-5 on the IDocDB\ndatabase are provided in Figs. 11, 12. These results conﬁrm that the proposed DeSTNet is\nmore accurate than the CSTN and show better robustness against partial-occlusions, clutter\nand low-light conditions.\nANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS15\nModel Architecture\nCSTN-5 [ conv3-64(2) | conv3-128(2) | conv3-256(2) | FC8 ]×5\nDeSTNet-5 F{[ conv3-64(2) | conv3-128(2) | conv3-256(2) | FC8 ]×5}\nTable 3: Architectures utilized by CSTN-5 and DeSTNet-5. convD 1-D2(D3): convolution\nlayer with D1 ×D1 receptive ﬁeld, D2 channels and D3 stride, FC: fully connected layer, F:\nfusion operation used in DeSTNet for fusing the parameters updates.\nFigure 11: Qualitative results obtained with CSTN-5 and DeSTNet-5 on IDocDB. (Results\nare best viewed on a digital screen)\n16ANNUNZIATA, SAGONAS, CALÌ: DENSELY FUSED SPATIAL TRANSFORMER NETWORKS\nFigure 12: Qualitative results obtained with CSTN-5 and DeSTNet-5 on IDocDB. (Results\nare best viewed on a digital screen)",
  "topic": "MNIST database",
  "concepts": [
    {
      "name": "MNIST database",
      "score": 0.8228031396865845
    },
    {
      "name": "Computer science",
      "score": 0.7814944982528687
    },
    {
      "name": "Transformer",
      "score": 0.6772714257240295
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6398925185203552
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.6361924409866333
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6231374740600586
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.48585936427116394
    },
    {
      "name": "Artificial neural network",
      "score": 0.36850202083587646
    },
    {
      "name": "Engineering",
      "score": 0.08484989404678345
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ]
}