{
  "title": "LanguageRefer: Spatial-Language Model for 3D Visual Grounding",
  "url": "https://openalex.org/W3178489527",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Roh, Junha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2751452054",
      "name": "Desingh, Karthik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2273117395",
      "name": "Farhadi, Ali",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749828282",
      "name": "Fox, Dieter",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963726321",
    "https://openalex.org/W2949107813",
    "https://openalex.org/W2884565639",
    "https://openalex.org/W3164900052",
    "https://openalex.org/W3108144224",
    "https://openalex.org/W2963109634",
    "https://openalex.org/W2974759213",
    "https://openalex.org/W3107521863",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2986755220",
    "https://openalex.org/W2909303996",
    "https://openalex.org/W2776202271",
    "https://openalex.org/W3159619744",
    "https://openalex.org/W2964345792",
    "https://openalex.org/W2568262903",
    "https://openalex.org/W2963800628",
    "https://openalex.org/W2251512949",
    "https://openalex.org/W3034758614",
    "https://openalex.org/W2946119234",
    "https://openalex.org/W3009928773",
    "https://openalex.org/W3034578524",
    "https://openalex.org/W2980421174",
    "https://openalex.org/W2995993311",
    "https://openalex.org/W3111739346",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W3205676116",
    "https://openalex.org/W2995849700",
    "https://openalex.org/W3037533539",
    "https://openalex.org/W3111566807",
    "https://openalex.org/W2774005037",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W3133833192",
    "https://openalex.org/W3100923070",
    "https://openalex.org/W3099261920",
    "https://openalex.org/W2929928372",
    "https://openalex.org/W2980139307",
    "https://openalex.org/W3164677036",
    "https://openalex.org/W3101009265",
    "https://openalex.org/W2561715562",
    "https://openalex.org/W3095974555",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2936707910"
  ],
  "abstract": "For robots to understand human instructions and perform meaningful tasks in the near future, it is important to develop learned models that comprehend referential language to identify common objects in real-world 3D scenes. In this paper, we introduce a spatial-language model for a 3D visual grounding problem. Specifically, given a reconstructed 3D scene in the form of point clouds with 3D bounding boxes of potential object candidates, and a language utterance referring to a target object in the scene, our model successfully identifies the target object from a set of potential candidates. Specifically, LanguageRefer uses a transformer-based architecture that combines spatial embedding from bounding boxes with fine-tuned language embeddings from DistilBert to predict the target object. We show that it performs competitively on visio-linguistic datasets proposed by ReferIt3D. Further, we analyze its spatial reasoning task performance decoupled from perception noise, the accuracy of view-dependent utterances, and viewpoint annotations for potential robotics applications.",
  "full_text": "LanguageRefer: Spatial-Language Model for 3D\nVisual Grounding\nJunha Roh, Karthik Desingh, Ali Farhadi, Dieter Fox\nPaul G. Allen School, University of Washington, United States\n{rohjunha, kdesingh, ali, fox}@cs.washington.edu\nAbstract: For robots to understand human instructions and perform meaningful\ntasks in the near future, it is important to develop learned models that compre-\nhend referential language to identify common objects in real-world 3D scenes.\nIn this paper, we introduce a spatial-language model for a 3D visual grounding\nproblem. Speciﬁcally, given a reconstructed 3D scene in the form of point clouds\nwith 3D bounding boxes of potential object candidates, and a language utterance\nreferring to a target object in the scene, our model successfully identiﬁes the tar-\nget object from a set of potential candidates. Speciﬁcally, LanguageRefer uses\na transformer-based architecture that combines spatial embedding from bounding\nboxes with ﬁne-tuned language embeddings from DistilBert [1] to predict the tar-\nget object. We show that it performs competitively on visio-linguistic datasets\nproposed by ReferIt3D [2]. Further, we analyze its spatial reasoning task per-\nformance decoupled from perception noise, the accuracy of view-dependent ut-\nterances, and viewpoint annotations for potential robotics applications. Project\nwebsite: https://sites.google.com/view/language-refer.\nKeywords: Referring task, Language model, 3D visual grounding, 3D Navigation\nFigure 1: Simpliﬁed overview of LanguageRefer. The LanguageRefer model takes as input a grounding\nlanguage description of a single object in the scene, a 3D point cloud of a scene, and bounding boxes of objects\nin the scene and predicts the target object. Its four modules include: a classiﬁer, a spatial embedder, a language\nembedder, and a spatial-language model.\n1 Introduction\nFor robots to communicate seamlessly with humans to perform meaningful tasks in indoor environ-\nments, they must understand natural language utterances and ground them to real-world elements.\nSeveral recent advances have combined language and visual elements, producing methods for tasks\nsuch as visual question and answering (VQA) involving spatio-temporal reasoning tasks [3, 4, 5],\nembodied QA [6], and pre-training for visual recognition tasks with language descriptions [7]. Fur-\nther, embodied agents can follow visually grounded language instructions to perform embodied\ntasks [8, 9]. However, for real robots to intelligently perform these tasks, we need 3D representa-\ntions from raw sensor data; ReferIt3D [2] proposes a benchmarking dataset of language utterances\nreferring to objects in 3D scenes from the ScanNet [10] dataset.\narXiv:2107.03438v3  [cs.RO]  4 Nov 2021\nOur long-term goal is to enable robots to visually navigate indoor environments based on referential\ninstructions. In this paper we take a step towards this goal by leveraging the ReferIt3D dataset to\nbuild a model that can identify the 3D object referred to in a language utterance.\nReferential language to identify an object in real-world 3D scenes poses a challenging problem.\nConsider the sample utterance “Facing the foot of the bed, the bed on the right”along with a 3D\nscene as shown in Figure 1. Humans can easily follow the language clues, infer the point of view\nof the speaker, locate all the referenced elements, and spatially reason to locate the bed in the scene\ndespite two instances of beds. However, viewpoint prediction, object identiﬁcation, and spatial\nreasoning remain open-ended research problems in robotics and vision. The 3D reference task\nproposed by ReferIt3D is difﬁcult because: (1) reconstructed 3D scenes of the real world in the\nScanNet dataset are noisy and lack ﬁne details compared to 2D images or rendered 3D scenes, (2)\nﬁne-grained class labels and expressions in natural language utterances are diverse and not exactly\nmatched, (3) view-dependent utterances often require guessing the original viewpoints, which deters\nthe model from properly learning spatial concepts, and (4) the combined complexity of multiple\nchallenges complicates efforts to analyze what the model learns or understands.\nInspired by the success of the methods in [11, 12] on CLEVR and CLEVRER domains for the spatial\nreasoning task, we hypothesized that for the 3D reference task, decoupling the spatial-reasoning\nfrom the perceptual task of identifying the objects in the 3D scene would improve performance and\nclearly track the role of perception noise in performance. More speciﬁcally, instead of developing an\nintegrated multi-modal perception system, we assumed a pre-trained instance classiﬁcation model\nor ground-truth classes for the objects that informed the spatial reasoning task. We focused on how\nthe language model with spatial information could handle the reference task.\nThe ReferIt3D dataset includes two sub-datasets containing natural and synthetic language utter-\nances, namely Nr3D and Sr3D, respectively. In our experiments, our model achieved comparable\nscores on both with predicted instance class labels. We observed high accuracy with ground-truth la-\nbels in Sr3D, which indicates that our model better understood template-based language data. Since\nour pipeline is modular and features multiple models (perceptual, spatial embedding, pre-trained\nlanguage embedding, and spatial-language), our approach is ﬂexible and adaptable to different en-\nvironments and object entities.\nAnother aspect of the 3D reference task is viewpoint prediction. The ReferIt3D dataset contains\nutterances that can be grouped into view-independent (VI) and view-dependent (VD) categories.\nAn example of a VI utterance is “The lamp closer to the white armchair”and a VD utterance is\n“The lamp on the right in-between the beds”. The VD utterance requires viewpoint prediction. This\ndistinction is crucial for robotics applications where the agent must infer the viewpoint to which the\nspeaker is referring. In the ReferIt3D dataset, some VD utterances lack information to guess the\nvalid orientation of the agent (who utters the language description), which prevents a model from\nunderstanding the signiﬁcance of spatial relationships such as ‘left of.’ This is due to the annotation\nprocess of ReferIt3D datasets, where both a speaker and a listener can freely rotate the scene to\ninfer an ill-deﬁned orientation in the utterance. Human annotators who are aware of spatial concepts\ntend to validate otherwise arbitrary orientations. However, a data-driven model will suffer from\ndegraded learning when it cannot verify orientations as well as human annotators. Viewpoint-free\nannotations may suit most robotics applications; nonetheless, predicting or verifying whether a given\nstatement and the viewpoint match remains important. To better train and investigate the agent in\nview dependencies, we provide an extra collection of orientation annotations for VD utterances and\ncompare the models with and without viewpoint correction from them.\nTo summarize, the paper contributes: (1) a novel transformer-based spatial-language model in a\nmodular pipeline that better understands spatial relationships in a 3D visual grounding task. We\nshow that our model achieves comparable performance with state-of-the-art methods on the Nr3D\nand Sr3D datasets. (2) analysis of the ReferIt3D dataset with viewpoint orientation annotations to\nremove potential artifacts from implicit orientations. (3) ablation and additional experiments with\nground-truth classes that decouple the impact of perception noise in the spatial reasoning task.\n2\n2 Related Work\n2.1 Vision-and-Language Navigation and Robot Navigation\nVision-and-language navigation (VLN) has been extensively studied and made remarkable progress\nover the last few years ([13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]). Given a language instruction\nin the simulation environment, the goal is for an agent to reach the desired node on the pre-deﬁned\ntraversal graph using images as input. The literature offers several extensions. ALFRED [8] pro-\nposes an extended VLN task that grounds a sequence of sub-tasks to achieve a higher level task in\nthe AI2Thor environment [9]. ALFRED navigation sequences have (implicit) goals that are often\nclose to objects of interest in the subsequent sub-tasks, e.g., when an agent is asked to move in front\nof the sink because it is going to clean a cup there. With high-level semantic tasks, object-centric\nspatial understanding is of even greater importance.\nIn another direction, recent approaches have relaxed the constraint of discrete traversal in VLN into\ncontinuous space ([24, 25, 26, 27, 28]). Here, an agent encounters more complex tasks involving\ntime and space. Thus, expanding the space representation to 3D can be an effective solution. In the\ncontext of VLN, we consider the 3D visual grounding task as a proxy for 3D indoor navigation that\nincludes the full observability assumption and goal-oriented language descriptions. In particular,\nour approach focuses on understanding spatial relationships among objects, which plays a key role\nin VLN and robot navigation.\n2.2 2D and 3D Visual Grounding\nThe 2D visual grounding task localizes an object or region in an image given a language description\nabout the object or region ([29, 30, 31]). Most methods use two-stage approaches: they ﬁrst gen-\nerate proposals and then compare the proposals to the language description to choose the grounded\nproposal ([32, 33, 34, 35, 36]).\nThe 3D visual grounding task localizes a 3D bounding box from the point cloud of a scene given a\nlanguage description. Recently, ReferIt3D [2] and ScanRefer [37] were proposed as datasets for 3D\nvisual grounding tasks, with language annotation on the ScanNet [10] dataset. Most 3D grounding\napproaches ([2, 37, 38, 39, 40]) follow a two-stage schemes similar to many 2D visual grounding\ntasks. First, multiple bounding boxes are proposed or the ground-truth bounding boxes are used,\nand then features from the proposals are combined or compared with features from the language\ndescription. InstanceRefer [38] extracts attribute features both from point clouds and utterances\nand compares them to select the object that best matches. FFL-3DOG [39] matches features from\nlanguage and point clouds with guidance from language and a visual scene graph. These methods\nrely on speciﬁc designs, e.g., bird-eye-view mappings with different types of operations or intensive\nlanguage pre-processing for graph generation. In contrast, our approach leverages the language\nembedding space from the pre-trained language model. Following the pipeline and general ar-\nchitecture of the language model therefore requires minimal manual design compared to previous\nworks. SAT [40], like our approach, relies on transformer [41] models; it learns a fused embed-\nding of multi-modal inputs and uses auxiliary 2D images. In contrast, our approach uses a semantic\nclassiﬁer to predict object class labels and takes these labels as input. It has a marginal cost of\nlearning fused embeddings compared to training multiple BERT models [42] from scratch in SAT\n[40]. Even given only semantic information from point clouds, our model still achieved comparable\nperformance with state-of-the-art methods on Nr3D and Sr3D datasets. In addition, the decoupled\nperception module makes our approach modular and thus transferable to different data.\n3 Problem Statement and Methodology\nGiven a 3D scene S with a list of objects (O1, ··· , OM ) and a language utterance U, the problem is\nto predict the target object OT, T ∈IM = {1, ··· , M}referred to in the language. A single object\nOi consists of a bounding box Bi ∈B = R6 and corresponding point cloud Pi ∈P = RNi×6 (xyz\npositions and RGB values) in the bounding box with Ni number of points.\nWe propose an approach based on language models, called LanguageRefer, to solve a 3D visual\ngrounding task. Our model focuses on understanding spatial relationships between objects from\nlanguage descriptions and 3D bounding box information. We chose this approach due to (1) the high\n3\nFigure 2: Detailed overview of LanguageRefer. A semantic classiﬁer predicts class labels from a 3D point\ncloud in each bounding box (using color and xyz positions). The language description or utterance (e.g.,\n“Facing the foot of the bed, the bed on the right”) is transformed into a sequence of tokens. The input token\nembedding in DistilBert [1] converts the tokens into embedded feature vectors (green squares). Bounding\nbox position and size information are positional-encoded to form encoded vectors using techniques from [41]\n(orange squares); they are added to the corresponding embedded feature vectors (green squares). After the\naddition, our reference model processes the modiﬁed features and feeds them to multiple tasks. The main task\nis a reference task, i.e., it chooses the referred object from the object features. The instance classiﬁcation task\nis a binary classiﬁcation, i.e., it determines whether the given object feature belongs to the target class. Finally,\nthe masking task, commonly used in language modeling, recovers the original token from a randomly replaced\ntoken in the utterance.\ndependency on spatial relationship descriptions in language, and (2) the holistic nature of spatial\nrelationship information, which differs from unary attribute information such as color and shape.\nAs shown in Figure. 2, we use a two-stage approach. First, we determine the class labels of objects\nin the scene. Second, we use spatial-language embedding to identify the referenced object. The\nfollowing subsections describe these steps in detail.\nSemantic Classiﬁcation Model and Tokenization. For semantic classiﬁcation of the point cloud\nin a bounding box, we employed PointNet++ [43], which achieved 69% accuracy on average in the\ntest dataset. In training and inference, we use a sampled point cloud P′\ni ∈R1024×6 and PointNet++\npredicts the semantic label ˆl ∈L, where Lis a set of class labels in text. Each scene has pairs of\npredicted class labels and bounding box values ((ˆli, Bi) :ˆli ∈L, Bi ∈B) from objects. Predicted\nlabels are concatenated to the utterance U with a separator [SEP] and then split by a tokenizer into\na list of indices of tokens: U becomes (u1, ··· , ut), and each predicted class label ˆli becomes\n(oi\n1, ··· , oi\nn1 ), where each token index is in ID and D is the size of the dictionary.\nLanguage Model and Token Embedding Generation. Our model uses a pre-trained language\nmodel, DistilBert, for the reference task. Transformers consider relationships among all pairs of\nelements through attention, and they can be effectively leveraged to explain spatial relationships\nbetween objects as discrete entities. In our formulation, predicted class labels are considered to be\nsentences, so they are concatenated to the utterance with separation by [SEP]. Therefore, the ﬁnal\nsequence of token indices would be V = ([CLS],u1, ··· , ut, [SEP], o1\n1, ··· , o1\nn1 , [SEP], ···, [SEP],\noM\n1 , ··· , oM\nnM , [SEP]). Though the token index sequence complies with the speciﬁcation of Distil-\nBert, it violates the number of sentences. Then, we transform V into the token embedding sequence\nW = (w1, ··· , wT ), wi ∈R768 using DistilBert’s word embeddings. For concise notation, we de-\nﬁne the indices of utterance tokens inV or W as a mask MU = (2, ··· , t+1) and the indices of ﬁrst\ntokens from objects (o1\n1, ··· , oM\n1 ) as a mask MO. We also deﬁne a mask operator [·] to manipulate\nspeciﬁc elements in the sequence; for instance, W[MU ] = 0empties all utterance embeddings in\nW.\nSpatial-Language Model and Spatial Embeddings. To combine spatial information from raw\nbounding box values into the token embedding W, we employ sinusoidal positional encoding PE(·)\nfrom [41] to transform the bounding box vector (center position and size) Bi ∈B⊂ R6 to bi =\n4\nPE(Bi) ∈R768, which is then added to W[MO].1 The token embedding W is then combined with\nspatial information and ﬁnally transformed into the output embedding X = (x1, ··· , xT ), xi ∈\nR768 by the reference model, which is ﬁne-tuned from the pre-trained DistilBert. The ﬁnal reference\ntask is performed by the reference classiﬁer from X[MO], as explained in the following subsection.\nLoss Functions. We use three tasks for training and corresponding loss values. First, we use the\nreference loss, Lref , following the original proposal in [2]. We ask the model to choose one object\nas the target instance from M candidates. We collect scalar values from X[MO] by a linear layer\nand take the argmax on those values to choose the target instance.\nSecond, we add a binary target classiﬁcation loss Lclf on X[MO] to determine whether a given\nobject belongs to the target class.\nLast, we employ mask loss from language model pre-training, Lmask. We randomly replace the\ntokens from nouns in the utterance with a probability of 15 %. The noun token is replaced by\n[MASK] with an 80 % chance, by a random token with a 10 % chance, or it remains the same with\na probability of 10 %. Then, the model is asked to recover the original token index. We expect\nthe model to ﬁll in the replaced tokens in the utterance by understanding the relationship between\nobjects. We use cross entropy loss for all tasks and compute the ﬁnal loss as\nL= Lref + 0.5Lclf + 0.5Lmask. (1)\nAt inference, we followed the approach of InstanceRefer [38] to ﬁlter out objects that do not belong\nto the predicted target class. We used an extra DistilBert-based target classiﬁcation model of 94\n% accuracy that takes the language utterance as input to predict the target class. To reduce the\nchance of removing the true target instance in the ﬁltering process, the top-k class predictions (from\nthe semantic classiﬁer) for each object are compared to the predicted target class. We use k = 4\nthroughout the experiments. For masking loss computation, we extracted nouns in the utterance. We\nused ﬂair [44] for part-of-speech (POS) tagging.\n4 Experiments\n4.1 Datasets\nWe evaluated our model on the reference task from ReferIt3D [2] with two datasets, Nr3D (Natural\nreference in 3D) and Sr3D (Spatial reference in 3D, which contains spatial descriptions only). Both\ndatasets augment ScanNet [10], a reconstructed 3D indoor scene dataset with language descriptions.\nNr3D has 41,503 natural language utterances, and Sr3D contains 83,572 template-based utterances,\non 707 scenes following the ofﬁcial splits of ScanNet [10]. The datasets have 76 target classes and\nare designed to have multiple same-class distractors in the scene.\n4.2 Experiment Settings\nReferIt3D [2] provides the ground-truth bounding boxes of objects in the scene, point clouds of\nthe scene, utterances and corresponding target objects. We measured the accuracy of the model\nby comparing the object selected from M candidates to the ground-truth target object. When the\nnumber of same-class distractors exceeded two, we classiﬁed the instance as “hard” according to\n[2]. The other cases were classiﬁed as “easy.”\nWe trained models with a learning rate of 0.0001 using AdamW optimization, warm-up and linear\nscheduling. Our model was initialized with a pre-trained model of the cased Distilbert base [1] from\nthe Hugging Face implementation [45].\nWe compared the performance of our model with state-of-the-art methods on ReferIt3D ([2, 40, 37,\n38, 39]) based on the reported numbers on the challenge website [46] and corresponding papers.\nSince SAT [40] uses an extra 2D image dataset in their training, we separated it from non-SAT, their\nbaseline model that is not trained with the extra dataset.\n1Each value in bi is extended to 128-dimensions by PE and then concatenated to form a 768-dimensional\nvector.\n5\nDataset Method Overall Easy Hard View-dep. View-indep.\nNr3D\nReferIt3D [2] 35.6 % 43.6 % 27.9 % 32.5 % 37.1 %\nScanRefer [37] 34.2 % 41.0 % 23.5 % 29.9 % 35.4 %\nInstanceRefer [38] 38.8 % 46.0 % 31.8 % 34.5 % 41.9 %\nFFL-3DOG [39] 41.7 % 48.2 % 35.0 % 37.1 % 44.7 %\nnon-SAT [40] 37.7 % 44.5 % 31.2 % 34.1 % 39.5 %\nOurs 43.9 % 51.0 % 36.6 % 41.7 % 45.0 %\nNr3D w/ 2D images SAT [40] 49.2 % 56.3 % 42.4 % 46.9 % 50.4 %\nSr3D\nReferIt3D [2] 40.8 % 44.7 % 31.5 % 39.2 % 40.8 %\nInstanceRefer [38] 48.0 % 51.1 % 40.5 % 45.4 % 48.1 %\nnon-SAT [40] 47.4 % N/A N/A N/A N/A\nOurs 56.0 % 58.9 % 49.3 % 49.2 % 56.3 %\nSr3D w/ 2D images SAT [40] 57.9 % 61.2 % 50.0 % 49.2 % 58.3 %\nTable 1: Accuracy on ReferIt3D [2]. Our model outperformed state-of-the-art models on both Nr3D and\nSr3D except for models with additional training data (SAT with 2D images). The average performance gap\nbetween ours and other models on Sr3D (10.6%) is larger than that on Nr3D (6.3%) since our model uses only\nspatial reasoning for the reference task.\n4.3 Evaluation on ReferIt3D [2]\nTable 1 shows the accuracy on Nr3D and Sr3D. Our model outperformed other models (without extra\ntraining datasets) on both Nr3D and Sr3D. It achieved43.9% on Nr3D with an +8.3% improvement\nover the baseline method of ReferIt3D [2] and a 2.2% increase over the best accuracy from other\nmodels in Nr3D. For Sr3D, our model achieved 56.0%, which is a 15.2% and 8.0% increase over\nthe baseline and the best accuracy in the Sr3D section, respectively. It is also comparable to the\naccuracy of SAT (with a 1.9% difference), which was trained with additional 2D image training\ndata. These results prove that our model can accurately reason about spatial relationships from\nspatial-language embeddings and outperforms other models on both datasets despite the loss of\ninformation in appearance. In addition, less diverse language expressions and utterances only about\nspatial reasoning can explain our model’s strong performance on Sr3D.\n4.4 Evaluation with Ground-Truth Class Labels\nWe further investigated the model’s spatial reasoning ability by removing noise in class labels. We\nreplaced predicted class labels with ground-truth class labels, which was possible due to the explicit\nusage of class labels in our model.\nTable 2 shows the result with and without ground-truth class labels. The ﬁrst and second columns\ndemonstrate the type of dataset in training and evaluation, respectively. For instance, the second\nrow shows the evaluation result of the model trained with the Nr3D dataset with ground-truth class\nlabels and evaluated on the Nr3D dataset with predicted class labels. When we trained and evaluated\nour model with ground-truth labels on Nr3D and Sr3D (row 4 and 8), we achieved 54.3% and\n91.1% accuracy, respectively. Compared to the accuracy of models trained and evaluated with noisy\nlabels, we realized an improvement is 10.4% and 35.1%, respectively, for each dataset. When we\nevaluated with ground-truth labels the models trained with noisy labels, their performance increase\nwas 9.7% and 24.2%, respectively. Multiple reasons account for the difference in performance\non Nr3D and Sr3D: diversity in the natural language dataset, a higher portion of view-dependent\nutterances or view-dependent utterances with no mention of orientation, and descriptions other than\nspatial relationships, such as color or appearance.\nIn addition, we examined the accuracy given switched datasets, namely, when we train a model on\nNr3D and evaluate it using Sr3D, and vice versa. The last two rows in Table 2 show two switched\nevaluation results: 40.0% and 37.6% on Sr3D and Nr3D, respectively. The37.6% accuracy on Nr3D\nfrom the model trained on Sr3D is comparable to the accuracy of non-SAT [40] and InstanceRefer\n[38]. It shows our model’s generalizability of spatial reasoning and potential to transfer to different\nperception modules and datasets.\n6\nDataset Overall Easy Hard View-dep. View-indep.Training Evaluation\nNr3D-pred Nr3D-pred 43.9 % 51.0 % 36.6 % 41.7 % 45.0 %\nNr3D-gt Nr3D-pred 41.4 % 48.0 % 34.6 % 38.1 % 43.0 %\nNr3D-pred Nr3D-gt 53.6 % 64.4 % 42.5 % 50.7 % 55.0 %\nNr3D-gt Nr3D-gt 54.3 % 65.5 % 42.8 % 49.1 % 56.8 %\nSr3D-pred Sr3D-pred 56.0 % 58.9 % 49.3 % 49.2 % 56.3 %\nSr3D-gt Sr3D-pred 52.1 % 53.8 % 48.2 % 43.9 % 52.5 %\nSr3D-pred Sr3D-gt 80.2 % 83.2 % 73.1 % 62.5 % 81.0 %\nSr3D-gt Sr3D-gt 91.1 % 93.1 % 86.2 % 67.0 % 92.1 %\nNr3D-pred Sr3D-pred 40.0 % 43.6 % 31.5 % 41.2 % 39.9 %\nSr3D-pred Nr3D-pred 37.6 % 45.3 % 29.6 % 34.5 % 39.0 %\nTable 2: Ablation with ground-truth class labels. First and second columns show types of data used in\ntraining and evaluation. The high overall accuracy ( 91.1% at row 8) of the model both trained and evaluated\nwith ground-truth class labels on Sr3D shows its spatial reasoning ability besides the perception noise. Ac-\ncuracy gaps between ground-truth and predicted class labels on Nr3D and Sr3D ( 9.7%, 24.2%, respectively)\nindirectly tell us about language complexity and information loss due to classiﬁcation. Transferring the model\ntrained with Sr3D to Nr3D evaluation shows an overall number ( 37.6% at row 10) comparable to those from\nother methods. Our model can easily accommodate different classiﬁcation models or datasets.\n4.5 Ablation on Loss Terms\nWe trained models with different combinations of loss terms, and Table 3 shows the results. In\naddition to three tasks, we examined the effect of a text classiﬁcation task that predicts the target\nclass label l ∈L from the tokens from utterance X[MU ].\nWe found that only the binary classiﬁcation loss shows its effect clearly (+3.5% from Ref. to Ref.-\nClf., +3.9% from Ref.-Mask to Ref.-Mask-Clf.,+1.0% from Ref.-Text to Ref.-Text-Clf.). The mask\nloss was not effective, and the text classiﬁcation loss degraded accuracy. One hypothesis here is that\nthe text classiﬁcation loss does not help the model to transform the initial language embedding to a\nspatial-language embedding. The model may not require strong language constraints because (1) it\nalready has a good initial language embedding, and (2) the text classiﬁcation loss is independent of\nthe scene’s spatial conﬁguration. Due to the performance drop, we chose our model without the text\nclassiﬁcation loss.\nRef. Clf. Mask Text Overall Easy Hard View-dep. View-indep.\n✓ - - - 40.6 % 48.2 % 32.8 % 38.5 % 41.6 %\n✓ ✓ - - 44.1 % 51.3 % 36.7 % 41.0 % 45.6 %\n✓ - ✓ - 40.0 % 47.2 % 32.5 % 36.8 % 41.5 %\n✓ - - ✓ 40.0 % 48.7 % 30.8 % 37.8 % 40.9 %\n✓ ✓ ✓ - 43.9 % 51.0 % 36.6 % 41.7 % 45.0 %\n✓ ✓ - ✓ 41.0 % 49.7 % 31.9 % 39.3 % 41.8 %\n✓ ✓ ✓ ✓ 40.0 % 48.2 % 31.5 % 38.9 % 40.6 %\nTable 3: Ablation of loss terms on Nr3D. The classiﬁcation loss was effective, the mask loss did\nnot signiﬁcantly affect accuracy, and the text loss degraded accuracy. We chose the model without\ntext losses (ﬁfth row, in blue).\n4.6 Viewpoint Annotation\nView-dependent utterances without information about original viewpoint make the reference task in\nReferIt3D [2] more challenging. For instance, utterances such as “The door is wood with the handle\non the left side.” assume speciﬁc orientations of the agent, and it is impossible to recover the true\norientation without knowing the referred object; this differs from view-dependent utterances with\nexplicit viewpoint information, such as “Facing the foot of the bed.” However, the original dataset\n7\nCorrection Overall Easy Hard View-dep. View-indep.Training Evaluation\n✓ - 43.5 % 50.7 % 36.0 % 37.0 % 46.6 %\n✓ ✓ 49.0 % 56.0 % 41.8 % 54.4 % 46.4 %\n- ✓ 43.1 % 50.3 % 35.6 % 40.4 % 44.4 %\n- - 43.9 % 51.0 % 36.6 % 41.7 % 45.0 %\nTable 4: Comparison of accuracy with and without corrected orientations on Nr3D.\n(a) An example of stan-\ndard orientation 1.\n(b) An example of stan-\ndard orientation 2.\n(c) An example of stan-\ndard orientation 3.\n(d) An example of stan-\ndard orientation 4.\nFigure 3: Examples of standard orientations for viewpoint annotation on Nr3D (a-d). We assume that the\nrobot is always inside the room except for cases speciﬁed by utterances.\nof ReferIt3D [2] does not distinguish the utterances without orientation information from those with\nit. Therefore, we split the view-dependent (VD) utterance category into two subcategories, VD-\nexplicit and VD-implicit, where VD-explicit has explicit viewpoint information in the utterance.\nWe then collected orientations from human annotators that validated the utterances. We set four\nstandard orientations assuming the agent is in the room (around the center of the scene) and asked\nannotators to select all orientations that could be considered valid from the utterance. Figure 3\nshows examples of the four orientations. We found that four orientations were sufﬁcient to recover\nthe original viewpoints of the speakers. In total, 12,680 view-dependent utterances of the Nr3D\ndataset were annotated; from these, 5,942 utterances were classiﬁed as VD-explicit. For train and\ntest split, 10,206 and 2,474 utterances were annotated, respectively.\nFrom the orientation for view-dependent utterances, we revised the dataset with the corrected ori-\nentation; we rotated the scene with respect to the annotation so all scenes remained valid at the\ncanonical orientation. For view-independent utterances, we randomly rotated the scenes since they\nwere valid in any direction. Table 4 shows the accuracy values for models trained with and without\ncorrected orientations. At inference, we evaluated each model with and without corrected orienta-\ntions, as well. The second row shows the accuracy of the model that was trained and evaluated with\ncorrected orientations. Its overall accuracy was improved by +5.1% from the ﬁnal model without\nthe correction that was used in Table 1 (the last row in Table 4). Note that the improvement on\nview-independent utterances was marginal (+1.4%), but the improvement on view-dependent ones\nwas signiﬁcant (+12.7%). The ﬁrst row shows the accuracy of a model trained with corrected orien-\ntations and evaluated on the test data without correction. This model achieved accuracy comparable\nto the ﬁnal model (−0.4%). This implies the correction helped the model to accurately interpret the\nview-dependent scene when the orientation was consistently aligned, introducing no unwanted bias.\n5 Conclusion\nWe proposed LanguageRefer, a spatial-language model for 3D visual grounding in a reference task.\nLanguageRefer combines language embeddings from utterances and class labels with positional-\nencoded spatial information for efﬁcient learning of the spatial-language embedding space without\ndifferent modules for individual modality. Experimental results show that LanguageRefer outper-\nformed state-of-the-art models on ReferIt3D with no additional training data. Analysis and ab-\nlations we performed demonstrate the effects of 1) noisy class labels, 2) arbitrary viewpoints in\nview-dependent utterances, 3) and the transfer of our model to different datasets for future robotics\napplications.\n8\nAcknowledgments\nWe would like to thank to Xiangyun Meng and Mohit Shridhar for discussion and feedback. This\nwork is in part supported by NSF IIS 1652052, IIS 17303166, DARPA N66001-19-2-4031, DARPA\nW911NF-15-1-0543, Honda Research Institute as part of the Curious Minded Machine initiative,\nand gifts from Allen Institute for Artiﬁcial Intelligence.\nReferences\n[1] V . Sanh, L. Debut, J. Chaumond, and T. Wolf. Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\n[2] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas. Referit3d: Neural listen-\ners for ﬁne-grained 3d object identiﬁcation in real-world scenes. 16th European Conference\non Computer Vision (ECCV), 2020.\n[3] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. Girshick. Clevr:\nA diagnostic dataset for compositional language and elementary visual reasoning. In CVPR,\n2017.\n[4] K. Yi, C. Gan, Y . Li, P. Kohli, J. Wu, A. Torralba, and J. B. Tenenbaum. CLEVRER: collision\nevents for video representation and reasoning. In ICLR, 2020.\n[5] R. Girdhar and D. Ramanan. CATER: A diagnostic dataset for Compositional Actions and\nTEmporal Reasoning. In ICLR, 2020.\n[6] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra. Embodied question answering.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n1–10, 2018.\n[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-\nsion. arXiv preprint arXiv:2103.00020, 2021.\n[8] M. Shridhar, J. Thomason, D. Gordon, Y . Bisk, W. Han, R. Mottaghi, L. Zettlemoyer, and\nD. Fox. Alfred: A benchmark for interpreting grounded instructions for everyday tasks. 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10737–\n10746, 2020.\n[9] E. Kolve, R. Mottaghi, W. Han, E. VanderBilt, L. Weihs, A. Herrasti, D. Gordon, Y . Zhu,\nA. Gupta, and A. Farhadi. AI2-THOR: An Interactive 3D Environment for Visual AI. arXiv,\n2017.\n[10] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner. Scannet: Richly-\nannotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 5828–5839, 2017.\n[11] D. Ding, F. Hill, A. Santoro, and M. Botvinick. Object-based attention for spatio-temporal rea-\nsoning: Outperforming neuro-symbolic models with ﬂexible distributed architectures. arXiv\npreprint arXiv:2012.08508, 2020.\n[12] A. Kamath, M. Singh, Y . LeCun, I. Misra, G. Synnaeve, and N. Carion. Mdetr–modulated\ndetection for end-to-end multi-modal understanding. arXiv preprint arXiv:2104.12763, 2021.\n[13] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. S ¨underhauf, I. Reid, S. Gould, and\nA. V . Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation in-\nstructions in real environments. 2018 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3674–3683, 2018.\n[14] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge. Room-across-room: Multilingual vision-\nand-language navigation with dense spatiotemporal grounding. In EMNLP, 2020.\n9\n[15] Y . Qi, Q. Wu, P. Anderson, X. Wang, W. Wang, C. Shen, and A. V . Hengel. Reverie: Remote\nembodied visual referring expression in real indoor environments.2020 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages 9979–9988, 2020.\n[16] K. Chen, J. K. Chen, J. Chuang, M. V’azquez, and S. Savarese. Topological planning with\ntransformers for vision-and-language navigation. ArXiv, abs/2012.05292, 2020.\n[17] M. Savva, A. Kadian, O. Maksymets, Y . Zhao, E. Wijmans, B. Jain, J. Straub, J. Liu, V . Koltun,\nJ. Malik, D. Parikh, and D. Batra. Habitat: A platform for embodied ai research. 2019\nIEEE/CVF International Conference on Computer Vision (ICCV), pages 9338–9346, 2019.\n[18] C.-Y . Ma, J. Lu, Z. Wu, G. Al-Regib, Z. Kira, R. Socher, and C. Xiong. Self-monitoring\nnavigation agent via auxiliary progress estimation. ArXiv, abs/1901.03035, 2019.\n[19] L. Ke, X. Li, Y . Bisk, A. Holtzman, Z. Gan, J. Liu, J. Gao, Y . Choi, and S. Srinivasa. Tactical\nrewind: Self-correction via backtracking in vision-and-language navigation. 2019 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pages 6734–6742, 2019.\n[20] P. Anderson, A. X. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta, V . Koltun, J. Kosecka,\nJ. Malik, R. Mottaghi, M. Savva, and A. Zamir. On evaluation of embodied navigation agents.\nArXiv, abs/1807.06757, 2018.\n[21] D. Fried, R. Hu, V . Cirik, A. Rohrbach, J. Andreas, L.-P. Morency, T. Berg-Kirkpatrick,\nK. Saenko, D. Klein, and T. Darrell. Speaker-follower models for vision-and-language navi-\ngation. In NeurIPS, 2018.\n[22] Y . Hong, Q. Wu, Y . Qi, C. Rodriguez-Opazo, and S. Gould. A recurrent vision-and-language\nbert for navigation. ArXiv, abs/2011.13922, 2020.\n[23] A. Shrivastava, K. Gopalakrishnan, Y . Liu, R. Piramuthu, G. Tur, D. Parikh, and D. Hakkani-\nTur. Visitron: Visual semantics-aligned interactively trained object-navigator. ArXiv,\nabs/2105.11589, 2021.\n[24] P. Anderson, A. Shrivastava, J. Truong, A. Majumdar, D. Parikh, D. Batra, and S. Lee. Sim-\nto-real transfer for vision-and-language navigation. arXiv preprint arXiv:2011.03807, 2020.\n[25] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee. Beyond the nav-graph: Vision-\nand-language navigation in continuous environments. In European Conference on Computer\nVision, pages 104–120. Springer, 2020.\n[26] V . Blukis, R. A. Knepper, and Y . Artzi. Few-shot object grounding and mapping for natural\nlanguage robot instruction following. ArXiv, abs/2011.07384, 2020.\n[27] J. Roh, C. Paxton, A. Pronobis, A. Farhadi, and D. Fox. Conditional driving from natural\nlanguage instructions. ArXiv, abs/1910.07615, 2019.\n[28] M. Z. Irshad, C.-Y . Ma, and Z. Kira. Hierarchical cross-modal agent for robotics vision-and-\nlanguage navigation. ArXiv, abs/2104.10674, 2021.\n[29] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier, and S. Lazebnik.\nFlickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence\nmodels. International Journal of Computer Vision, 123:74–93, 2015.\n[30] S. Kazemzadeh, V . Ordonez, M. Matten, and T. L. Berg. Referitgame: Referring to objects in\nphotographs of natural scenes. In EMNLP, 2014.\n[31] J. Mao, J. Huang, A. Toshev, O.-M. Camburu, A. Yuille, and K. Murphy. Generation and\ncomprehension of unambiguous object descriptions. 2016 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 11–20, 2016.\n[32] L. Yu, Z. L. Lin, X. Shen, J. Yang, X. Lu, M. Bansal, and T. L. Berg. Mattnet: Modular\nattention network for referring expression comprehension. 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 1307–1315, 2018.\n10\n[33] S. Yang, G. Li, and Y . Yu. Dynamic graph attention for referring expression comprehen-\nsion. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4643–\n4652, 2019.\n[34] L. Yu, P. Poirson, S. Yang, A. Berg, and T. L. Berg. Modeling context in referring expressions.\nArXiv, abs/1608.00272, 2016.\n[35] Y . Liu, B. Wan, X.-D. Zhu, and X. He. Learning cross-modal context graph for visual ground-\ning. ArXiv, abs/1911.09042, 2020.\n[36] L. Ye, M. Rochan, Z. Liu, and Y . Wang. Cross-modal self-attention network for referring\nimage segmentation, 2019.\n[37] D. Z. Chen, A. X. Chang, and M. Nießner. Scanrefer: 3d object localization in rgb-d scans\nusing natural language. 16th European Conference on Computer Vision (ECCV), 2020.\n[38] Z. Yuan, X. Yan, Y . Liao, R. Zhang, Z. Li, and S. Cui. Instancerefer: Cooperative holistic\nunderstanding for visual grounding on point clouds through instance multi-level contextual\nreferring, 2021.\n[39] M. Feng, Z. Li, Q. Li, L. Zhang, X. Zhang, G. Zhu, H. Zhang, Y . Wang, and A. Mian. Free-\nform description guided 3d visual graph network for object grounding in point cloud, 2021.\n[40] Z. Yang, S. Zhang, L. Wang, and J. Luo. SAT: 2d semantics assisted training for 3d visual\ngrounding. CoRR, abs/2105.11450, 2021. URL https://arxiv.org/abs/2105.11450.\n[41] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\n[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[43] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In NIPS, 2017.\n[44] A. Akbik, T. Bergmann, D. Blythe, K. Rasul, S. Schweter, and R. V ollgraf. Flair: An easy-to-\nuse framework for state-of-the-art nlp. In NAACL 2019, 2019 Annual Conference of the North\nAmerican Chapter of the Association for Computational Linguistics (Demonstrations), pages\n54–59, 2019.\n[45] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T. L.\nScao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush. Huggingface’s transformers: State-\nof-the-art natural language processing, 2020.\n[46] P. Achlioptas. Referit3d benchmarks. URL https://referit3d.github.io/\nbenchmarks.html.\n11",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7896444201469421
    },
    {
      "name": "Artificial intelligence",
      "score": 0.671887993812561
    },
    {
      "name": "Visual reasoning",
      "score": 0.6473371982574463
    },
    {
      "name": "Object (grammar)",
      "score": 0.5293208956718445
    },
    {
      "name": "Robot",
      "score": 0.4723033905029297
    },
    {
      "name": "Bounding overwatch",
      "score": 0.46986350417137146
    },
    {
      "name": "Language model",
      "score": 0.46863460540771484
    },
    {
      "name": "Transformer",
      "score": 0.4606049656867981
    },
    {
      "name": "Embedding",
      "score": 0.45800232887268066
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.4474598169326782
    },
    {
      "name": "Utterance",
      "score": 0.4231528043746948
    },
    {
      "name": "Point (geometry)",
      "score": 0.41255998611450195
    },
    {
      "name": "Natural language processing",
      "score": 0.365800678730011
    },
    {
      "name": "Computer vision",
      "score": 0.35053712129592896
    },
    {
      "name": "Programming language",
      "score": 0.09578800201416016
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}