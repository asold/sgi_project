{
  "title": "Marathi To English Neural Machine Translation With Near Perfect Corpus And Transformers",
  "url": "https://openalex.org/W3008231283",
  "year": 2022,
  "authors": [
    {
      "id": null,
      "name": "Jadhav, Swapnil Ashok",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963247703",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W2794365787",
    "https://openalex.org/W2949454572",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W2537667581",
    "https://openalex.org/W2760656271",
    "https://openalex.org/W2963991316",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2778814079",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W2555745756"
  ],
  "abstract": "There have been very few attempts to benchmark performances of state-of-the-art algorithms for Neural Machine Translation task on Indian Languages. Google, Bing, Facebook and Yandex are some of the very few companies which have built translation systems for few of the Indian Languages. Among them, translation results from Google are supposed to be better, based on general inspection. Bing-Translator do not even support Marathi language which has around 95 million speakers and ranks 15th in the world in terms of combined primary and secondary speakers. In this exercise, we trained and compared variety of Neural Machine Marathi to English Translators trained with BERT-tokenizer by huggingface and various Transformer based architectures using Facebook's Fairseq platform with limited but almost correct parallel corpus to achieve better BLEU scores than Google on Tatoeba and Wikimedia open datasets.",
  "full_text": "Marathi To English Neural Machine Translation With Near Perfect Corpus\nAnd Transformers\nSwapnil Ashok Jadhav, Dailyhunt\nAbstract\nThere have been very few attempts to benchmark\nperformances of state-of-the-art algorithms for\nNeural Machine Translation task on Indian Lan-\nguages. Google 1, Bing2, Facebook and Yandex 3\nare some of the very few companies which have\nbuilt translation systems for few of the Indian Lan-\nguages. Among them, translation results from\nGoogle are supposed to be better, based on general\ninspection. Bing-Translator do not even support\nMarathi language4 which has around 95 million\nspeakers and ranks 15th in the world 5 in terms\nof combined primary and secondary speakers. In\nthis exercise, we trained and compared variety\nof Neural Machine Marathi to English Transla-\ntors trained with BERT-tokenizer by huggingface\nand various Transformer based architectures us-\ning Facebook’s Fairseq platform with limited but\nalmost correct parallel corpus to achieve better\nBLEU scores than Google on Tatoeba and Wiki-\nmedia open datasets.\n1. Introduction & Related Work\nIn the last few years, neural machine translation (NMT) has\nachieved tremendous success in advancing the quality of\nmachine translation (Cheng et al., 2016; Hieber et al., 2017).\nAs an end-to-end sequence learning framework, NMT con-\nsists of two important components, the encoder and decoder,\nwhich are usually built on similar neural networks of dif-\nferent types, such as recurrent neural networks ( Sutskever\net al., 2014; Bahdanau et al. , 2015; Chen et al. , 2018), con-\nvolutional neural networks ( Gehring et al. , 2017), and more\nrecently on transformer networks ( Vaswani et al. , 2017).\nTo overcome the bottleneck of encoding the entire input\nsentence into a single vector, an attention mechanism was\nintroduced, which further enhanced translation performance\n1https://translate.google.com/\n2https://www.bing.com/translator\n3https://translate.yandex.com/\n4https://en.wikipedia.org/wiki/Marathi_language\n5https://en.wikipedia.org/wiki/List_of_\nlanguages_by_total_number_of_speakers\n(Bahdanau et al. , 2015). Deeper neural networks with in-\ncreased model capacities in NMT have also been explored\nand shown promising results ( Bapna et al. , 2018).\nSequence-to-sequence neural models (seq2seq) ( Kalchbren-\nner & Blunsom , 2013; Sutskever et al. , 2014; Cho et al. ,\n2014; Bahdanau et al., 2015) have been widely adopted as\nthe state of-the-art approach for machine translation, both in\nthe research community (Bojar et al., 2016; 2017; 2018) and\nfor large-scale production systems (Cheng et al., 2016; Zhou\net al., 2016; Crego et al. , 2016; Hassan et al. , 2018). As a\nhighly expressive and abstract framework, seq2seq mod-\nels can be trained to perform several tasks simultaneously\n(Luong et al., 2015), as exemplified by multilingual NMT\n(Dong et al., 2015; Firat et al., 2016; Ha et al., 2016; John-\nson et al. , 2017) - using a single model to translate between\nmultiple languages.\nMachine translation is highly sensitive to the quality and\nsize of data. Mining sentence pairs using nearest neigh-\nbour with a hard threshold over cosine similarity method\nimproves translation performance significantly as explained\nin (Artetxe & Schwenk , 2019). In our approach we did use\ncleaning of sentence pairs but we chose to go with simple\ndictionary based approach as explained in next section.\nThere are very few efforts where NMT with Indian lan-\nguages are experimented and benchmarked properly. In\nrecent approaches researchers have tried to build Multilin-\ngual NMT systems (Arivazhagan et al., 2019). Though, only\nfew BLEU scores are presented for Indian languages and\nin most cases average BLEU score over multiple language-\npairs has been considered as benchmark. In our approach\nexplained in this paper, we have built a machine translation\nsystem with a widely spoken Indian language ”Marathi” to\nEnglish translation, with minimal engineering efforts for\ndata collection & preparation and existing transformer ar-\nchitectures with easy-to-use Fairseq platform, which shows\nsignificant improvements compared to industry best Google\nTranslator.\n2. Data Collection & Preparation\nWe ideally need huge dataset of marathi-to-english parallel\ncorpus to train deep architectures and to get state-of-the-art\nMarathi to English Neural Machine Translation\nresults with Fairseq platform. Opus 6 provides good amount\nof parallel corpus. For marathi-english pair, we can see that\naround 1 million sentences are available. Among which only\nTatoeba, Wikimedia and bible datasets are useful, as other\ndata is just instructions. Among the valid sets as well, when\nsanity check was done, it was found that not all sentences\nare correctly aligned and there some fetal mismatches. We\ntried to rectify but later decided not to and then ignored the\nbible dataset completely. We decided to keep Wikimedia\nand Tatoeba datasets for validation purpose as we were left\nwith just around 53k sentence pairs and did not use the same\nin training.\nThrough scrapping, we had more than 6 million sentence\npairs, but we determined and used only those sentence pairs\nwhich were almost correct. We put a hard rule of dictionary-\nbased words matching and considered only those sentence\npairs which had at-least 30% of the translated words matched\nwith dictionary words. We were left with around 3 million\nsentence pairs.\nWe used wordpiece tokenizer by huggingface 7 to tokenize\nMarathi and English text. Also, we used lowercased English\ntext throughout the experiments to reduce the learning of\ncases for English language. There is no concept like ”cases”\nfor Marathi language. Sample parallel corpus examples can\nbe found at project gihub location 8.\n3. Experiments\n3.1. Setup\nWe used Facebook’s sequence-to-sequence library Fairseq 9\nto train and inference the translation model. This neatly\nwritten and easy to use library provides multiple state-of-\nthe-art architectures to build translation models. We in-\nstalled the library on a 4x V100 32gb Nvidia GPU linux\nsetup. Even though, there are multiple algorithms available,\nwe focused majorly on following Transformer based archi-\ntectures : transformer-wmt-en-de,transformer-iwslt-de-en,\ntransformer-wmt-en-de-big-t2tandtransformer-vaswani-\nwmt-en-de-big\nFairseq also provides option to tokenize the input text with\nsentencepiece tokenizer10 and gpt tokenizer. But we opted\nto tokenize the text with wordpiece tokenizer instead, even\nbefore passing the text for the training. In future, we would\ntry to build sentencepiece model from Marathi and English\nNews corpus and use and evaluate against the existing word-\n6http://opus.nlpl.eu/\n7https://pypi.org/project/\npytorch-pretrained-bert/\n8https://github.com/swapniljadhav1921/\nmarathi-2-english-NMT\n9https://fairseq.readthedocs.io/\n10https://github.com/google/sentencepiece\npiece tokenizer.\n3.2. Training\nWe trained multiple models with above mentioned trans-\nformer architectures with various hyper-parameters sug-\ngested in respective papers and in Fairseq github discussions.\nFollowing is the one of the training commands we used.\nTraining Command : CUDA_VISIBLE_DE-\nVICES=0,1,2,3 fairseq-train mr2en_token_data –arch\ntransformer_vaswani_wmt_en_de_big –share-decoder-input-\noutput-embed –optimizer adam –adam-betas ’(0.9,0.98)’\n–clip-norm 0.0 –lr 5e-4 –lr-scheduler inverse_sqrt –warmup-\nupdates 10000 –dropout 0.3 –weight-decay 0.0001 –criterion\nlabel_smoothed_cross_entropy–label-smoothing0.1–max-tokens\n4096 –update-freq 2 –max-source-positions 512 –max-target-\npositions512–skip-invalid-size-inputs-valid-test\nNote that, as we were using 4 GPU’s instead of 8 GPUs, men-\ntioned in many state-of-the-art papers, we set–update-freqto\n2. This is done to mimic the training with 8 GPUS. We used\ndifferent optimizers but finally settled on adamoptimizer\nbecause of its stable loss reduction capability. We noticed\nthat increasing warmup-updatesfrom 4k to 10k improved\nthe convergence and reduced overall iterations. Also, note\nthat we didn’t use –FP16option in above command which\ncould have improved the training speed but we observed\nthat it reduces BLEU score marginally.\nWe stopped the training once perplexity(ppl) went be-\nlow 3. Smaller models like transformer-wmt-en-de and\ntransformer-iwslt-de-entook around 30 hours whereas, other\ntwo big models took 50+ hours. For all the transformer-\nbased models, loss went below 1 for train and test sets.\n3.3. Results\nTo make the comparison fair we used 16th iteration of\nthe models throughout. Marathi text was fired against the\nGoogle-cloud-api-v2 to collects the results for the compari-\nson. Inference on GPU was preferred over CPUs as we could\nutilize all 4 GPUs effectively with 4 models. Following is\nthe one of the command for inference we used.\nInference Command : CUDA_VISIBLE_DEVICES=0 python\ninteractive.py –path ../translation_task/checkpoints_trans-\nformer_iwslt_de_en/checkpoint16.pt../translation_task/mr2en_to-\nken_data –beam 5 –source-lang mr –target-lang en –input\n../translation_task/set3_tokens.mr–sacrebleu–skip-invalid-size-\ninputs-valid-test–batch-size32–buffer-size32\nWe used beam search of 5 which worked better in BLEU\nscore than any other option. After the inference, we got\ntokenized English text which we de-tokenized and used fur-\nther for model metrics comparisons. Note that, calculating\nBLEU with tokenized text yields high scores which is unfare\nMarathi to English Neural Machine Translation\nTable1. BLEU score comparison on small sentences having word-\ncount less than 15.\nModels bleu raw-bleu\nGoogle 55.10 46.59\nwmt-en-de 65.23 65.26\niwslt-de-en 63.11 63.13\nwmt-en-de-big-t2t 71.97 71.99\nvaswani-wmt-en-de-big 72.37 72.40\nTable2. BLEU score comparison on medium to large sentences\nhaving word-count more than 15.\nModels bleu raw-bleu\nGoogle 28.60 17.47\nwmt-en-de 26.87 26.10\niwslt-de-en 26.06 25.28\nwmt-en-de-big-t2t 29.50 28.73\nvaswani-wmt-en-de-big 27.18 26.50\nto Google Translator and hence avoided throughout.\nWe used sacreBLEU11 library to calculate corpus-bleu score\n(with smoothing function enabled) and raw-corpus-bleu\nscore (with smoothing function disabled). As mentioned\nbefore we used, Tatoeba and Wikimedia parallel corpus of\naround 53k sentence pairs as validation set. Tatoeba contains\nsmaller everyday sentences and greetings, while Wikimedia\nhas long scientific sentences.\nFrom Table 1, we can see that for smaller sentences hav-\ning wordcount less than 15, all transformer models crushed\nGoogle in BLEU and Raw-BLEU scores. Boundary of 15\nwords was chosen based on study 12 which states that in cur-\nrent generation average words used in a sentence is around\n10-20. Also note that, there is a less gap between BLEU\nand Raw-BLEU scores for all transformer models compared\nto Google. This table shows that for smaller everyday sen-\ntences and greetings, our model outperformed Google easily.\nvaswani-big architecture and wmt-t2t architecture performed\nthe best.\nFrom Table 2, we can see that all the models including\nGoogle struggled to go beyond 30 BLEU score. Also, only\nwmt-t2t model was able to outscore Google in BLEU but\nat the same time Google struggled in Raw-BLEU score\ncompared all other models. This shows that wmt-t2t model\nwas able to translate longer, complex sentences with good\nscore and did better than Google.\nTable 3 shows the comparison between actual English text\nand predicted English text word-counts. Even though, this\n11https://github.com/mjpost/sacreBLEU\n12https://techcomm.nz/Story?Action=View&Story_\nid=106\nTable3. Error between actual translation word-count and predicted\ntranslation word-count.\nModels MAE RMSE\nGoogle 1.257 7.239\nwmt-en-de 0.783 6.965\niwslt-de-en 0.816 6.964\nwmt-en-de-big-t2t 0.691 6.879\nvaswani-wmt-en-de-big 0.723 7.149\nTable4. Comparison between existing Translators\nMarathi Text\nआयुष्य पतंगासारखं आहे . मांजा धरला तर वे-\nगात उंच झेपावत नाही आिण सोडला तर कुठे\nजाईल त्याचा नेम नाही.\nActual Translation Life is like a kite. If you keep holding\nthe thread, it will not rise faster and if\nyou loose it then not sure where it will\nland .\nOur Model life is like a kite . holding a cat does\nnot accelerate high speeds and does not\nspecify where it will go if left unat-\ntended .\nGoogle Life is like a kite. If you catch a cat,\nit does not jump high and it does not\nspecify where you will go.\nFacebook Life is like a fall. If you hold a cat, you\ndon’t run high in speed and if you leave\nit, there is no name where it goes .\nYandex The life is like The Moth . I held her\nnaked buttocks in my hands as she rode\nme until she climaxed .\nscore doesn’t signify quality of the translation but often\nused to check the sanity of the model. Here we can see\nthat, all transformer-based models outperformed Google in\nboth Mean-Absolute-Error(MAE) and Root-Mean-Squred-\nError(RMSE). Also, wmt-t2t model performed the best again\noverall.\n3.4. Discussion\nAny language base model require huge amount of data to\ntrain deep architectures. We saw that one of the best word-\nembedding model BERT 13 was trained on more than 100gb\nof textual data. Similarly, to train translation models and to\nmake them learn how to generate sentence structures and\neven transliterate proper nouns instead of translation, we\nneed large parallel corpus. And Google supposedly have\na very large corpus. As per our knowledge, Google relies\non scrapping and community help 14. As Google has not\n13https://github.com/google-research/bert\n14https://translate.google.com/community#mr/en\nMarathi to English Neural Machine Translation\nTable5. Comparison between Google and Our Best Transformer\nModel\nMarathi Text 1\n\"ज्यात िपल्ले तयार होतात असा पक्षी वा काही\nप्रकारच्या माद्यांपासून उत्पन्न होणारा गोलक,\nकोिकळा आपले अंडे उबवण्यासाठी कावळ्या-\nच्या घरट्यात ठेवते.\"\nGoogle Output 1\n(Few Months Ago)\nSpherul\nGoogle Output 1\n(Current)\nThe spider, the spider, lays its eggs in\nthe nest to hatch.\nOur Model Output 1 a bird that produces chicks or a sphere\nof females keeps the cuckoo in a raven\n’ s nest to hatch its eggs\nMarathi Text 2\n\"आज १८ जून आहे व आज म्यूिरएलचा वाढ-\nिदवस आहे !\"\nGoogle Output 2 Today is June 5th, and today is\nMuriel’s birthday!\nOur Model Output 2 today is june 18th and today is muriel ’\ns birthday !\nreleased any data, we cannot further comment on the data\nquality but note that we just used 3 million sentence pairs to\ncross Google’s BLEU score.\nLet’s take one of the examples to see how various Translators\nperformed and why we chose to compare our results with\nGoogle. Check the Table 4 for the results.\nYandex translation gives us an extreme example here. Some-\nhow it converted philosophical saying into a adult-story con-\ntent. Probably, the training dataset is the culprit here. But we\nalso did check up with many other examples and noticed that\nMarathi-to-English translation provided by Yandex is pretty\nbad. Also, it looks like, it doesn’t have enough vocabulary\ncoverage as well.\nNote that, none of the Translators including ours was able\nto capture the word \"मांजा\" which means ”thread”. Even\nif it looks wrong, it should be noted that our-model uses\ntokenization and possibly Google and Facebook as well.\n”cat” in Marathi translates to \"मांजर\" and due to tokenization\nit is very much possible that the word ”thread” was translated\nto ”cat” by above models. As we did not have any previous\nbenchmarks, we tested all available translators with 500+\nexamples and selected Google translator to be compared\nwith our transformer based models.\nWe will check few more example and compare our-model\nand Google output. The same can be checked in Table-5.\nIn first example, Google few months ago produced only\none word, and when we checked recently, it produced more\nthan one word but completely missed the translation for a\nword ”cuckoo” and ignored first half of the sentence. Our\nmodel overall produced correct sense out of the Marathi\nsentence. We suspect that, Google’s new translation is from\ntranslation-api-v3 which is in beta phase or possibly due to\nregular model updates.\nIn another simple example, Google wrongly translated\nMarathi number ”18” to ”5”. Also note that along with\nGoogle, our model did not try ro translate but transliterated\nthe word ”Muriel” which is proper noun here in the exam-\nple. Our model also doesn’t drop punctuations required\nfor readability. More examples can be found in our github\nrepository15.\n4. Conclusion & Future Work\nFrom the results and examples we can see that our\ntransformer-based model was able to outperform Google\nTranslation with limited but almost correct parallel corpus.\nGoogle will keep improving its models and datasets, but\nwith easy-to-use architectures like Fairseq, it is possible to\ncompete with current state-of-the-art in the future.\nThis work suggests that limited datasets are often enough\nin improving translation accuracy if they are nearly clean.\nAlso, there is a need of unified validation set on which\nresearchers can benchmark their models. We propose that\nopenly available ”Tatoeba + Wikimedia” datasets should\nbe considered to baseline the benchmarks for Marathi-to-\nEnglish translation task.\nWe did use wordpiece tokenizer in this experiment, but we\nare planning to use our own sentencepiece tokenizers trained\non massive news corpus each for Marathi and English and\nthen repeat the exercise again and compare the results. We\nhope to see gain in BLEU score with this new tokenization\nmethod.\nIn future, we are planning to support multiple Indian lan-\nguages for English-Translation task and also multilingual\ntranslation support and hope that, it will help us further in\nother NLP tasks like, Named-Entity-Recognition from news\ntext and detecting similar news across languages.\nReferences\nArivazhagan, N., Bapna, A., Firat, O., Lepikhin, D., Johnson,\nM., Krikun, M., Chen, M. X., Cao, Y., Foster, G., Cherry,\nC., Macherey, W., Chen, Z., and Wu, Y. Massively Multi-\nlingual Neural Machine Translation in the Wild: Findings\nand Challenges. arXive-prints, art. arXiv:1907.05019,\nJul 2019.\nArtetxe, M. and Schwenk, H. Margin-based parallel corpus\n15https://github.com/swapniljadhav1921/\nmarathi-2-english-NMT\nMarathi to English Neural Machine Translation\nmining with multilingual sentence embeddings. In Pro-\nceedingsofthe57thAnnualMeetingoftheAssociation\nforComputationalLinguistics , 2019.\nBahdanau, D., Cho, K., and Bengio, Y. Neural machine\ntranslation by jointly learning to align and translate. In\nInternationalConferenceonLearningRepresentations. ,\n2015.\nBapna, A., Chen, M. X., Firat, O., Cao, Y., and Wu, Y. Train-\ning deeper neural machine translation models with trans-\nparent attention. In arXivpreprint,arXiv:1808.07561. ,\n2018.\nBojar, O., Chatterjee, R., and Federmann, C. Findings of the\n2016 conference on machine translation. In ACL2016\nFIRSTCONFERENCEONMACHINETRANSLATION\n(WMT16),pages131–198.TheAssociationforComputa-\ntionalLinguistics., 2016.\nBojar, O., Chatterjee, R., and Federmann, C. Findings of\nthe 2017 conference on machine translation (wmt17). In\nProceedingsoftheSecondConferenceonMachineTrans-\nlation,pages169–214. , 2017.\nBojar, O., Federmann, C., Fishel, M., Graham, Y., Had-\ndow, B., Koehn, P., and Monz, C. Findings of the 2018\nconference on machine translation (wmt18). In Proceed-\ningsoftheThirdConferenceonMachineTranslation:\nSharedTaskPapers,pages272–303,Belgium,Brussels.\nAssociationforComputationalLinguistics. , 2018.\nChen, M. X., Firat, O., Bapna, A., MelvJohnson, Macherey,\nW., Foster, G., Jones, L., Parmar, N., Schuster, M., and\nChen, Z. The best of both worlds: Combining recent\nadvances neural machine translation. In Associationfor\nComputationalLinguistics., 2018.\nCheng, Y., Xu, W., He, Z., He, W., Wu, H., Sun, M., and Liu,\nY. Semisupervised learning for neural machine transla-\ntion. In AssociationforComputationalLinguistics , 2016.\nCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D.,\nBougares, F., Schwenk, H., and Bengio, Y. Learning\nphrase representations using rnn encoder–decoder for sta-\ntistical machine translation. In Proceedingsofthe2014\nConferenceonEmpiricalMethodsinNaturalLanguage\nProcessing(EMNLP),pages1724–1734,Doha,Qatar.\nAssociationforComputationalLinguistics. , 2014.\nCrego, J. M., Kim, J., and Klein, G. Systran’s pure neural\nmachine translation systems. In CoRR,abs/1610.05540.,\n2016.\nDong, D., Wu, H., He, W., Yu, D., and Wang, H. Multi-task\nlearning for multiple language translation. In Proceed-\ningsofthe53rdAnnualMeetingoftheAssociationfor\nComputationalLinguisticsandthe7thInternationalJoint\nConferenceonNaturalLanguageProcessing(Volume1:\nLongPapers),volume1,pages1723–1732. , 2015.\nFirat, O., Cho, K., and Bengio, Y. Multi-way, multilin-\ngual neural machine translation with a shared attention\nmechanism. In arXivpreprintarXiv:1601.01073. , 2016.\nGehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin,\nY. N. Convolutional sequence to sequence learning. In\nInternationalConferenceonMachineLearning. , 2017.\nHa, T.-L., Niehues, J., and Waibel, A. H. Toward multilin-\ngual neural machine translation with universal encoder\nand decoder. In CoRR,abs/1611.04798., 2016.\nHassan, H., Aue, A., and Chen, C. Achieving human parity\non automatic chinese to english news translation. In arXiv\npreprintarXiv:1803.05567., 2018.\nHieber, F., Domhan, T., Denkowski, M., Vilar, D., Sokolov,\nA., Clifton, A., and Post, M. Sockeye: A toolkit for neural\nmachine translation. In arXivpreprint,arXiv:1712.05690 ,\n2017.\nJohnson, M., Schuster, M., and Le, Q. V. Google’s mul-\ntilingual neural machine translation system: Enabling\nzero-shot translation. In TransactionsoftheAssociation\nofComputationalLinguistics,5(1):339–351. , 2017.\nKalchbrenner, N. and Blunsom, P. Recurrent continuous\ntranslation models. In Proceedingsofthe2013Confer-\nenceonEmpiricalMethodsinNaturalLanguagePro-\ncessing, pages 1700–1709, Seattle, Washington, USA.\nAssociationforComputationalLinguistics. , 2013.\nLuong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., and\nKaiser, L. Multitask sequence to sequence learning. In\narXivpreprintarXiv:1511.06114. , 2015.\nSutskever, I., Vinyals, O., and Le, Q. V. Sequence to se-\nquence learning with neural networks. In AdvancesNeu-\nralInformationProcessingSystems , 2014.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Łukasz Kaiser, and Polosukhin, I. At-\ntention is all you need. In AdvancesNeuralInformation\nProcessingSystems., 2017.\nZhou, J., Cao, Y., Wang, X., Li, P., and Xu, W. Deep\nrecurrent models with fast-forward connections for neural\nmachine translation. In TransactionsoftheAssociation\nforComputationalLinguistics,4:371–383. , 2016.",
  "topic": "Marathi",
  "concepts": [
    {
      "name": "Marathi",
      "score": 0.8148329257965088
    },
    {
      "name": "Transformer",
      "score": 0.7226696014404297
    },
    {
      "name": "Machine translation",
      "score": 0.6145598292350769
    },
    {
      "name": "Translation (biology)",
      "score": 0.5525915622711182
    },
    {
      "name": "Computer science",
      "score": 0.5235204696655273
    },
    {
      "name": "Natural language processing",
      "score": 0.5224593877792358
    },
    {
      "name": "Artificial intelligence",
      "score": 0.435398131608963
    },
    {
      "name": "Speech recognition",
      "score": 0.37379369139671326
    },
    {
      "name": "Linguistics",
      "score": 0.325732946395874
    },
    {
      "name": "Engineering",
      "score": 0.20246121287345886
    },
    {
      "name": "Electrical engineering",
      "score": 0.1322241723537445
    },
    {
      "name": "Philosophy",
      "score": 0.1256912350654602
    },
    {
      "name": "Chemistry",
      "score": 0.052929431200027466
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": []
}