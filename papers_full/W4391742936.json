{
  "title": "Assessing LLMs Responses in the Field of Domestic Sustainability: An Exploratory Study",
  "url": "https://openalex.org/W4391742936",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5003765523",
      "name": "Mathyas Giudici",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5013548937",
      "name": "Giulio Antonio Abbo",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A5093910785",
      "name": "Ottavia Belotti",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5073890782",
      "name": "Alessio Maria Braccini",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5081614661",
      "name": "F Dubini",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5102856818",
      "name": "R. Izzo",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5060049448",
      "name": "Pietro Crovari",
      "affiliations": [
        "Politecnico di Milano"
      ]
    },
    {
      "id": "https://openalex.org/A5047667036",
      "name": "Franca Garzotto",
      "affiliations": [
        "Politecnico di Milano"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2968170890",
    "https://openalex.org/W2006420071",
    "https://openalex.org/W4242513519",
    "https://openalex.org/W2177404962",
    "https://openalex.org/W2967126321",
    "https://openalex.org/W2104113460",
    "https://openalex.org/W2922256442",
    "https://openalex.org/W2403583711",
    "https://openalex.org/W4231399809",
    "https://openalex.org/W4206437029",
    "https://openalex.org/W6766621827",
    "https://openalex.org/W2981081593",
    "https://openalex.org/W2761896474",
    "https://openalex.org/W3004304303",
    "https://openalex.org/W4385572383",
    "https://openalex.org/W4381663758",
    "https://openalex.org/W6763450642",
    "https://openalex.org/W2988647680",
    "https://openalex.org/W6851564937",
    "https://openalex.org/W4385988359",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4322719791",
    "https://openalex.org/W3016125500",
    "https://openalex.org/W4294833327",
    "https://openalex.org/W4295955277",
    "https://openalex.org/W3161241203",
    "https://openalex.org/W4236521339",
    "https://openalex.org/W2964309167",
    "https://openalex.org/W3118866989",
    "https://openalex.org/W2100506586",
    "https://openalex.org/W6680532216",
    "https://openalex.org/W2803612996",
    "https://openalex.org/W4385847513",
    "https://openalex.org/W2999038296",
    "https://openalex.org/W3031637296",
    "https://openalex.org/W4389520333",
    "https://openalex.org/W2983537304",
    "https://openalex.org/W4319779057",
    "https://openalex.org/W1982897610",
    "https://openalex.org/W2990647756",
    "https://openalex.org/W4366989878",
    "https://openalex.org/W2947686949",
    "https://openalex.org/W4379539933",
    "https://openalex.org/W2560566339",
    "https://openalex.org/W2084390204",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226352076",
    "https://openalex.org/W4229914214",
    "https://openalex.org/W4318239126",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2140679639"
  ],
  "abstract": "In the next years, we must challenge climate change, and the urgency of adopting a more sustainable lifestyle has increased. Conversational Agents, such as Smart home Personal Assistants, have shown promise in fostering sustainable behaviors in domestic environments. However, traditional conversations with rule-based approaches in such agents face challenges in addressing users' questions in complex domains like sustainability. Large Language Models (LLMs) are a promising tool to overcome these limitations of their capability to answer open-domain questions. The final objective of this work is to compare the generative capabilities of four large language models in ecological sustainability to determine the most suitable LLM to be embedded into home assistants and create a hybrid model of conversational agent for environmental sustainability. We performed two evaluations. In the former, we constructed a set of trustable sources on the topic and analyzed the extent to which the themes covered in the text generated by the models appeared in it. The results do not show a statistical difference between the outputs of the candidate models, while qualitative analysis determined that ChatGPT, at the moment, is the optimal solution. In the second evaluation, we tested the responses generated by ChatGPT on a corpus of 167 questions from a sample of 75 people. Responses evaluation was performed by a team of experts (N=5) on fluency, coherency, consistency, accuracy, and reasoning. The results suggest that ChatGPT for generic questions on sustainability is quite reliable.",
  "full_text": "Assessing LLMs Responses in the Field of\nDomestic Sustainability: an Exploratory Study\nMathyas Giudici\nPolitecnico di Milano\nMilan, Italy\nmathyas.giudici@polimi.it\nGiulio Antonio Abbo\nIDLab-AIRO\nGhent University – imec\nGhent, Belgium\ngiulioantonio.abbo@ugent.be\nOttavia Belotti\nPolitecnico di Milano\nMilan, Italy\nottavia.belotti@mail.polimi.it\nAlessio Braccini\nPolitecnico di Milano\nMilan, Italy\nalessio.braccini@mail.polimi.it\nFrancesco Dubini\nPolitecnico di Milano\nMilan, Italy\nfranceso.dubini@mail.polimi.it\nRiccardo Andrea Izzo\nPolitecnico di Milano\nMilan, Italy\nriccardo.izzo@mail.polimi.it\nPietro Crovari\nPolitecnico di Milano\nMilan, Italy\npietro.crovari@polimi.it\nFranca Garzotto\nPolitecnico di Milano\nMilan, Italy\nfranca.garzotto@polimi.it\nAbstract—In the next years, we must challenge climate change,\nand the urgency of adopting a more sustainable lifestyle has\nincreased. Conversational Agents, such as Smart home Personal\nAssistants, have shown promise in fostering sustainable behaviors\nin domestic environments. However, traditional conversations\nwith rule-based approaches in such agents face challenges in\naddressing users’ questions in complex domains like sustainabil-\nity. Large Language Models (LLMs) are a promising tool to\novercome these limitations of their capability to answer open-\ndomain questions. The final objective of this work is to compare\nthe generative capabilities of four large language models in\necological sustainability to determine the most suitable LLM to\nbe embedded into home assistants and create a hybrid model\nof conversational agent for environmental sustainability. We\nperformed two evaluations. In the former, we constructed a set of\ntrustable sources on the topic and analyzed the extent to which\nthe themes covered in the text generated by the models appeared\nin it. The results do not show a statistical difference between\nthe outputs of the candidate models, while qualitative analysis\ndetermined that ChatGPT, at the moment, is the optimal solution.\nIn the second evaluation, we tested the responses generated by\nChatGPT on a corpus of 167 questions from a sample of 75\npeople. Responses evaluation was performed by a team of experts\n(N=5) on fluency, coherency, consistency, accuracy, and reasoning.\nThe results suggest that ChatGPT for generic questions on\nsustainability is quite reliable.\nIndex Terms—Conversational Agent, Sustainability, LLM,\nRule-based CA\nI. I NTRODUCTION\nIt has been several years since the climate crisis first\ncaptured global attention [1]. The urgency to adopt a more\nsustainable lifestyle has risen, with the objective of containing\nboth present and future damages to the environment [2]. How-\never, understanding which behaviors contribute to achieving\nthis objective – or even discerning the detrimental impact of\ncertain habits – is often difficult [3]. The two main reasons are\nthe complexity of the topic [4], [5] and the confusion caused\nby widespread misinformation [6], [7]. In 2010, [8] explored\nthe role that human-computer interaction can have in shifting\npeople towards more environmentally sustainable behaviors.\nAmong the different digital technologies nowadays avail-\nable, Conversational Agents (CA) – i.e., technologies that\ninteract with users through natural language [9] – are a promis-\ning tool. Their ability to interact in plain language allows\nusers to focus entirely on the interaction goal, i.e., addressing\nsustainability issues, rather than on the interaction itself. Con-\nversational Agents are deployed as stand-alone applications\nor embedded in larger systems. Among those, there are Smart\nhome Personal Assistants (SPAs), IoT devices that provide a\nconversational interface to a domotic IoT environment [10].\nThese systems have been revealed to be valid solutions to\nfoster sustainability behaviors in domestic environments [11],\n[12] for two main reasons: they are widely adopted and they\ncan provide actionable feedback, since they are already con-\nnected with smart environments, and can measure and monitor\nusers habits, for example in terms of energy consumption.\nNowadays, most Conversational Agents embedded in SPAs\noperate based on a predefined set of rules [13], [14], which\nmeans that developers – or conversation designers – need to\nanticipate and account for every potential scenario and user\nrequest. This approach is designed to ensure that interactions\nwith the CA are predictable and follow a predetermined flow\n[15]. However, when it comes to addressing users’ questions,\nparticularly in a multifaceted and diverse domain like sustain-\nability, this rule-based approach presents challenges, as the CA\nmay struggle to provide satisfactory responses to unanticipated\nquestions.\nTo overcome these limitations, newer approaches, such as\ngenerative conversational agents, are being developed [16].\nThese models leverage large datasets and advanced natural\nlanguage processing techniques to understand and respond to\nuser queries more dynamically. In particular, Large Language\nModels (LLMs) [17], neural networks that process text input\nand produce a textual output, can be used to expand the virtual\nassistants’ capabilities thanks to their ability to answer open-\ndomain questions [18]. However, generative models might not\nmatch the high standards in terms of content and scientific\naccuracy required when the objective is informative and factual\n[19]. This weakness derives from biases in training data,\nlimited access to real-time information, and the inability to\nverify facts. These challenges can lead to generating responses\nthat are inaccurate, outdated, or misleading.\nTo address these issues, additional measures such as fact-\nchecking, human oversight, and expert validation are neces-\nsary. However, implementing these steps can often be cost-\nprohibitive and may not be feasible or practical in many\nsituations, presenting a challenge in achieving consistently\naccurate and reliable responses. For these reasons, we want\nto understand whether LLMs are able to deliver precise and\nscientifically accurate answers on the sustainability topic, with\na particular focus on the household domain. We develop our\nresearch in two phases. First, we present a comparison between\ndifferent LLMs, testing which of the most prominent LLMs\nare best suited to deliver factual information on topic-related\nquestions. Second, we gathered questions on sustainability\nwith a crowdsourced questionnaire that involved 73 people.\nThe corpus created is then analyzed and used to evaluate the\nLLM obtained from the first part of the study, asking energy\nand sustainability five experts to evaluate the generated replies\non fluency, coherence, consistency, accuracy, and argument\nlogic. The results suggest that ChatGPT for generic questions\non sustainability is quite reliable. However. experts reported\nlow accuracy in the content delivered in the responses.\nThis work contributes to the application research for LLMs\nin the field of environmental sustainability while ensuring\nscientifically accurate information and human-like responses.\nWe believe that our results have proven that the way of hy-\nbridization of rule-based and generative-based smart assistants\ncan lead to upgraded tools able to provide effective support\nto householders in learning and adopting more sustainable\nlifestyles.\nII. S TATE OF THE ART\nA. Large Language Models (LLMs)\nA Large Language Model (LLM) is a neural network\nmodel specifically trained, usually on a billion parameters, to\nunderstand, summarize and generate text-based content [20].\nThe applications for LLMs are endless, some notable examples\nbeing summarization, as presented by [21], and Next Sequence\nPrediction (NSP) [22]. However, the most pertinent usage for\nour purposes is employing LLMs as Conversational Agents.\nOpenAI’s GPT-2, released in 2019, can be considered the\npioneer in LLM, and it was trained on 1.5 billion param-\neters [23]. With LLMs in the spotlight due to their recent\nimpressive performance, big corporations decided to make\na move and publish their own versions of LLMs. Accord-\ning to [24], some of the most notable ones nowadays are\nGoogle’s Bard [25], Microsoft’s Bing AI [26] and OpenAI’s\nChatGPT/GPT-3.5 [27].\nAs explained by [28], the large size of modern language\nmodels has rendered traditional weight updates impractical.\nThe unavailability and resource constraints make full model\ntuning unfeasible for many applications. As a result, the field\nof prompting emerged, exploring methods to leverage LLMs\ninputs for influencing the output. This method leverages zero-\nshot learning, a technique by which you can reach state-of-\nthe-art performances by presenting examples of a given task\nto a pre-trained model without fine-tuning it [29]. In fact, in\nthe case of prompting, the examples are fed via some text (a\nprompt) that steers the LLM toward giving the right answer.\nIt is also worth noting how avoiding fine-tuning the models\nis an environmentally sustainable choice since the heavy GPU\ntraining oftentimes results in high carbon dioxide emissions,\nas emerged from the work of [30].\nTo the best of our knowledge, within the sustainability field,\nAI has been widely implemented during the past years [31],\nbut no significant effort was made to implement LLMs as\na channel to spread information about the environment and\nsustainable behavior.\nB. Conversational Agents for Question Answering\nConversational Agents (CA) take advantage of natural lan-\nguage processing techniques to engage users in text-based\ninformation-seeking and task-oriented dialogues for a mul-\ntitude of applications [32]. For example, they are integrated\ninto physical devices (such as Alexa and Google Home)\nand available in many contexts of everyday life, used in\nphones (like Siri, the Apple virtual assistant), cars, and on-\nline consumer assistance [33]. They also find extensive use\nin applications such as question answering, leading to the\ndevelopment of Conversational Question Answering (CQA)\nsystems [34]. CQA systems aim to comprehend given con-\ntext and manage single-turn or multi-turn question answering\nto satisfy a user’s information needs. Finally, according to\nprevious studies [35], [36], conversational technologies are a\npromising interaction paradigm to persuade people towards\nmore sustainable behaviors, previously successfully used in\ndifferent settings.\nELIZA [37] represents a significant historical reference in\nthe field of conversational agents. As a rule-based model,\nELIZA used predefined rules and lexicons for language gen-\neration and comprehension, exploiting the concept of pattern\nmatching [38] that set up the stage for advanced models.\nAlthough rule-based CAs have been a milestone in this\nmode of interaction in the past, they had significant limitations,\nsuch as a lack of adaptability and scalability of conversa-\ntions [39].\nIn the 1980s, there was a shift from rule-based systems\nto data-driven approaches [40] powered by probability distri-\nbutions over sequences of words [41]. Finally, in the 2000s,\nneural networks and the Transformer model were introduced\nfor the first time by [42]. These technologies, combined with\nthe advancements in LLMs (previously described in Section\nII-A), led to the emergence of modern Conversational Agents\nsuch as ChatGPT.\nIn the environmental sustainability field, rule-based ap-\nproaches were used for delivering energy feedback [43],\n[44], suggesting sustainable mobility [45], or reducing food\nwaste [46]. Instead, [47] is an example using data-driven\nsystems to suggest recipes with leftover foods.\nIII. E XTENDING HOME ASSISTANT WITH LLM\nCAPABILITIES\nAs explained in the previous sections, prompting is one of\nthe primary tools in tuning large language models. However,\nsuch a technique has some limitations [48].\nOne of the most current challenging limitations is making\nthe prompt change depending on some real-time variable,\nwhich also implies adjusting the logic of such models to\nincorporate the variables. An approach to overcome that issue\nis proposed in the Socratic models by [49]. All the data\nsensed by the external environment are fed inside the prompt\nand the LLM replies accordingly. However, such a solution\nrelies on the input length of the generative model, becoming\nimpracticable if the contextual data are too long.\nAnother approach is the use of hybrid models; they handle\nrequests related to real-time data retrieval through a rule-\nbased strategy while managing all the other inquiries with a\ngenerative approach. Although considerable effort has been\nmade in the past literature to present different approaches of\nhybrid model mixing generative and retrieval techniques [50],\nthe research panorama for a hybrid model for environmental\nsustainability is still scant.\nIV. E XPLORATORY EVALUATION : LLM S ELECTION\nWe carried out an explorative evaluation, represented in\nFigure 1, aiming to find out which generative models have\nthe potential to be integrated into conversational agents and\ntalk about environmental sustainability in a domestic context.\nTo achieve this objective, we set up a study to investigate\ntwo aspects. First, how domestic sustainability information is\nincluded in a text generated by an LLM and which topics\nare touched, in comparison to those covered by a set of\nreliable sources, as shown in Figure 2. Second, the value of\nthe variability of topics delivered by the same LLM across\nmultiple runs.\nL L M s  \nG a t h e r i n g\nT r u s t a b l e  \nS o u r c e  \nC o l l e c t i o n\nT o p i c  \nE x t r a c t i o n  f r o m  \nT r u s t a b l e  \nS o u r c e\nT o p i c  \nE x t r a c t i o n  f r o m  \nG e n e r a t e d  \nA n s w e r s\nT a b l e s  \nC r e a t i o n\nFig. 1. Steps of the Exploratory Evaluation.\nFig. 2. Graphical Representation of Domestic Sustainability Information\nDomain.\nWe considered for this evaluation the most prominent\nLLMs [51] at the time of writing (i.e., end of June 2023),\nthose considered state-of-the-art and possibly accessible via\nAPI. The LLMs evaluated in this stage wereChatGPT, BingAI,\nBard, and OpenAssistant (LLAMA version).\nWe built a set of ten questions on the topic of sustainability\n(reported in Appendix A 1). To mitigate the introduction of\nhuman bias during this process, we relied on news articles\n(from newspapers, scientific periodicals, reliable scientific\nblogs, etc.) considered a credible source of information by\nnumerous people [52]. We considered the first 10 articles on\nthe topic, published before 2021 (otherwise, the topics covered\nmight not be part of the corpora on which most of the models\nare built), and that had the title or the main line formulated\nas questions. The body of each article represents the trustable\nanswer to each question.\nFrom these 10 article bodies, we extracted the topics cov-\nered. We asked three subjects who did not have previous\nexperience in the field to list the topics covered by each\narticle, i.e., an argument that is discussed in sufficient detail\nto comprehend its meaning. Based on this data, content topics\nare assigned, applying a majority voting technique.\nWe sampled the four candidate LLMs, asking the ques-\ntions extracted from the trustable source, using a zero-shot\npromoting technique. We asked the same question 8 times\nto each model to allow for variance in the answers and\ninvestigate whether different topics appeared. To ensure that\nthere were no correlations between the different generations, a\nnew instance of each model was created after each response.\nWe then extracted the topics from the answers, using the same\nmethodology adopted for the trustable source.\nFinally, we created a table to match the topics identified\nin the answers generated by the LLMs (and their occurrence)\nand the ones extracted from the trustable source. An example\nreporting the topic extraction of the first question is reported\nin Table I.\n1https://doi.org/10.5281/zenodo.10012799\nTABLE I\nEXAMPLE OF TOPIC EXTRACTION AND OCCURRENCES FOR QUESTION 1: Where does all the carbon we release go?\nTopic Trustable source ChatGPT BingAI Bard LLama\nOriginal source ✓ 7 0 7 5\nAtmosphere cycle ✓ 6 5 6 6\nHuman carbon production ✓ 7 0 2 7\nSolutions to reduce carbon ✓ 3 6 2 3\nGlobal warming ✓ 5 0 0 5\nWhy carbon is necessary 1 2 4 0\nA. Results and Discussion on the Exploratory Evaluation\nSince we mapped the presence of topics in the 8 responses\ngenerated by the LLMs to each individual question. The result\nreports that, on average, a trustable source topic is covered\n0.850 by ChatGPT (M=0.850, SD=0.285). For BingAI, the\naverage is 0.655 (SD=0.273), while Bard has a mean of 0.728\n(SD=0.309). Finally, LLama stands with a mean of 0.622\n(SD=0.240).\nTABLE II\nDESCRIPTIVE RESULTS OF ALIGNMENT OF TOPICS BETWEEN TRUSTABLE\nSOURCE AND LLM\nPercentiles\nMean SD 25th 50th 75th\nChatGPT 0.850 0.285 0.800 0.857 1.000\nBingAI 0.655 0.273 0.425 0.686 0.843\nBard 0.728 0.309 0.600 0.657 0.964\nLlama 0.622 0.240 0.450 0.675 0.779\nIn addition, we run a comparative analysis to evaluate the\nalignment between the topics presented in the trustable sources\nand those reflected in the LLMs’ answers, examining the\nvariance between multiple completions from the same model.\nAs shown in Table III, every topic appears on average in\n4.41 out of 8 questions generated by ChatGPT (M=4.41,\nSD=2.39) to the same question. For BingAI, every topic\nappears on average 4.01 times (SD=1.96), while on Bard, they\nare inserted in 4.55 answers (SD=1.99). Finally,LLama repeats\nthe same topics around 4.40 (SD=1.97) on the 8 answers.\nTABLE III\nDESCRIPTIVE RESULTS OF TOPICS EXTRACTED BY GENERATED\nRESPONSES\nMean SD Min Max\nChatGPT 4.41 2.39 1 8\nBingAI 4.01 1.96 1 7\nBard 4.55 1.99 1 7\nLLama 4.40 1.97 1 7\nWe run Repeated Measures ANOV A to assess the variability\nin the answers produced in terms of the number of topics\ncovered (among the repeated runs) in answers to the same\nquestion by the four LLMs (all the results are reported\nin Table IV). There is no statistically significant difference\nin the number of topic generation (F(9,3)=1.386, p=0.250),\nalso running the Friedman non-parametric test (X 2(3)=5.75,\np=0.124).\nGiven the results of this preliminary evaluation, there is no\nLLM model that statistically outperforms the others in terms\nof the number of topics covered in the generated answers. In\nthe same way, we do not have statistical evidence of more\nrecurrent topics in the generated answers.\nFor this reason, for the scope of our study described in\nthe following section, we select an LLM model on qualitative\nobservations. During the topic extraction, we can report that\nqualitatively the impression is that ChatGPT and Bard are\nvery verbose in their responses. On the other hand, BingAI is\nvery brief in its responses but equally perceived as accurate\nin its alignment with the trustable source.\nSince the underlying motivation that led to this study was\nto integrate such a question-answering system into a SPA, we\ndecided to select ChatGPT, since – at the time of the study\n– was the only system offering an API interface, therefore\nnatively supporting integration.\nV. E MPIRICAL STUDY: LLM A NSWERS EVALUATION\nIn this second study, represented in Figure 3, the objective\nis to analyze and evaluate the responses that the LLM chosen\nwith the previous exploratory evaluation could provide to a\nuser. To achieve this, we want to answer the following research\nquestion: What is the performance of responses generated by\nan LLM from a crowdsourced dataset of questions on the topic\nof environmental sustainability?\nM e t r i c s  \nD e f i n i t i o n\nQ u e s t i o n  \nC o l l e c t i o n  f r o m  \nU s e r s  a n d  \nC l u s t e r i n g\nH u m a n  \nE v a l u a t i o n  o n  \nA n s w e r  \nG e n e r a t e d  f r o m  \nC h a t G P T\nFig. 3. Steps of the Empirical Study.\nA. Methodology\nWe will assess the responses generated by the LLM using\na set of metrics grounded on previous works on a similar\ntopic [53], [54]. For each dimension, we gathered five scores\n(on a 5-item Likert scale).\n• Fluency measures how well the response is written in\nnatural language, without syntactic errors or awkward\nphrasing.\n• Coherency measures how a response is free of logical\ncontradictions.\nTABLE IV\nANOVA RESULTS OF TOPICS EXTRACTED BY GENERATED RESPONSES\nWithin Subjects Effects\nSum of Squares df Mean Square F p\nLLM 11.2 3 3.72 1.386 0.250\nLLM * Question 71.1 27 2.63 0.981 0.499\nResidual 362.5 135 2.69\nBetween Subjects Effects\nSum of Squares df Mean Square F p\nQuestion 38.4 9 4.27 0.461 0.893\nResidual 416.9 45 9.27\nNote. Type 3 Sums of Squares\n• Consistency measures how much the topic of the response\naligns with the topic in the question.\n• Accuracy measures how much the response is factual and\naccurate with respect to the topic.\n• Argumentation measures how much the response is well\nexplained, without redundancy or lacking in common\nsense.\nHaving defined the evaluation metrics, we proceeded to\nacquire a set of questions on the topic of sustainability to\nassess the performance of the chosen LLM (i.e., ChatGPT).\nTo this extent, we created a questionnaire asking participants to\nwrite 3 questions they would ask an expert in the sustainability\nfield. Participants belonged to close contacts from the personal\ncommunity, colleagues, or university students (the latter with\na predominantly scientific background), and they were all\nsensitive to environmental sustainability issues (without being\nable to consider them experts).\nWe collected the results into a CSV file, analyzed them\nwith the Sklearn K-means Clustering algorithm and using\nan embedding created with BERT, and extracted the main\ncategories into which they could be divided. Then, the clusters\nwere labeled.\nWe selected a random question from each category identi-\nfied in the previous step, and we generated a response with\nthe LLM. The prompt contained the sentence ”Explain in 50\nwords:” followed by the text of the question.\nFinally, to assess the performance of the generated re-\nsponses, we asked a group of 5 experts specialized in the field\nof sustainability (currently working in major energy companies\nin Italy) to evaluate the generated answers through a specific\nquestionnaire (see Appendix B and C 2), following the metrics\npresented above.\nB. Results and Discussion\nIn total, 75 respondents completed the questionnaire and\nprovided in total 225 partially overlapping questions. After\nhaving discarded low-quality contributions (i.e., questions not\nrelated to the sustainability field), and removed the duplicates,\nwe were left with 167 unique questions.\nWe classified the questions collected into 7 distinct cate-\ngories, as shown in Figure 4, where some points are mixed\n2https://doi.org/10.5281/zenodo.10012799\nwith others due to the t-SNE dimensionality reduction used to\nvisualize the graph in two dimensions.\nThe clusters were labeled based on the topics and the labels\nare here reported together with the number of unique answers\nassociated: Reducing plastic use (6), Concerns about the envi-\nronment and related actions (5), Environmental policies (44),\nGreenwashing and ”green” solutions (9), Eco-sustainability\nand sustainable behaviors (27), Energy and environmental\nimpact (6), and Environmental sustainability (70).\nFig. 4. Sklearn K-mean Clustering Analysis (points of different colors overlap\neach other due to the dimensional reduction caused by t-SNE algorithm)\nThe questions randomly selected for the evaluation and the\ncorresponding generated answers are reported in the support-\ning material. In Table V, we report the results of the evaluation\nof these responses by a group of (N=5) experts in the field.\nExperts reported fluency in the generated answers with an\naverage of 4.49 (SD=0.612). The coherency mean is 4.46\n(SD=0.611), while consistency has M=4.11 and SD=1.022,\nand accuracy has M=3.71 and SD=1.045. Finally, argumenta-\ntion has M=3.80 and SD=0.994.\nTABLE V\nDESCRIPTIVE RESULTS OF SCORES ASSIGNED TO CHATGPT\nMean SD Min Max\nFluency 4.49 0.612 3.00 5.00\nCoherency 4.46 0.611 3.00 5.00\nConsistency 4.11 1.022 2.00 5.00\nAccuracy 3.71 1.045 2.00 5.00\nArguments Logic 3.80 0.994 2.00 5.00\nThe experts agreed that ChatGPT can answer questions\nabout domestic sustainability in a fluent and coherent way.\nHowever, they also reported a low accuracy in the content\nof the generated responses. Our result is contrarily to the\nperformance reported in other domains like [55].\nWe believe that a fine-tuning phase might mitigate such a\nproblem in the sustainability field before the deployment of\nthe system or by extending the prompt with additional context\ninformation.\nIn addition, qualitative insights provided by experts high-\nlighted that the lower accuracy in the answers seems to\ncorrespond to questions provided by users that are difficult\nto interpret, even to human respondents. In the future, we will\nto further understand the relation between the quality of the\nquestions and the obtained output, trying to elicit strategies to\nminimize its influence in the generation of the responses.\nVI. C ONCLUSION\nIn an exploratory evaluation, we compared the generative\ncapabilities of four large language models in the field of\necological sustainability, with the objective of determining the\nmost suitable to be embedded in a conversational agent in\nthe home environment. The models considered are ChatGPT,\nBingAI, Bard, and LLAMA.\nWe constructed a set of trustable sources on the topic and\nanalyzed the extent to which the themes covered in the text\ngenerated by the models appeared in it. We sampled each\nmodel multiple times and performed an ANOV A comparison.\nThe data gathered was not sufficient to highlight a statisti-\ncal difference between the outputs of the candidate models.\nHowever, the results differed in terms of verbosity and we\ndetermined that ChatGPT, at the moment, is the optimal\nsolution.\nTo test the responses generated by ChatGPT, we built a\ncorpus of 167 questions on the topic of sustainability from a\ngroup of 75 participants. We then used ChatGPT to produce\ncandidate answers to these questions and evaluated them\nin terms of fluency, coherency, consistency, accuracy, and\nargumentation. The results confirm that ChatGPT as-is can\nanswer very general questions and is quite reliable. However,\nthe low accuracy reported by experts in the questionnaire\npoints out, that for some specific questions, there is the need\nto add prompting or use fine-tuning techniques. Future work\nwill allow us to test all the questions in each cluster and\nthen correlate them with the various scores. In addition, we\nwill expand the number of experts in evaluating the responses\ngenerated to user questions.\nACKNOWLEDGMENT\nThis work is partially supported by the Italian Ministry\nof University and Research under the PONRI program (Pro-\ngramma Operativo Nazionale ”Ricerca e Innovazione”) 2014-\n2020 and by the European Commission under Horizon Eu-\nrope program - V ALAW AI project (grant agreement number\n101070930).\nREFERENCES\n[1] D. Archer and S. Rahmstorf, The climate crisis: An introductory guide\nto climate change . Cambridge University Press, 2010.\n[2] R. Pierrehumbert, “There is no plan b for dealing with the climate crisis,”\nBulletin of the Atomic Scientists , vol. 75, no. 5, pp. 215–221, 2019.\n[3] L. Carrete, R. Casta ˜no, R. Felix, E. Centeno, and E. Gonz ´alez,\n“Green consumer behavior in an emerging economy: confusion,\ncredibility, and compatibility,” Journal of Consumer Marketing ,\nvol. 29, no. 7, pp. 470–481, Jan 2012. [Online]. Available:\nhttps://doi.org/10.1108/07363761211274983\n[4] M. T. Boykoff, Who speaks for the climate?: Making sense of media\nreporting on climate change . Cambridge University Press, 2011.\n[5] J. D. Sterman, “Sustaining sustainability: creating a systems science in\na fragmented academy and polarized world,” Sustainability science: The\nemerging paradigm and the urban environment , pp. 21–58, 2012.\n[6] P. Parks, “Is climate change a crisis–and who says so? an analysis\nof climate characterization in major us news media,” Environmental\nCommunication, vol. 14, no. 1, pp. 82–96, 2020.\n[7] F. Biondo, G. La Rocca, and V . Viviana Trapani, “Information disorder:\nLearning to recognize fake news,” 2022.\n[8] C. DiSalvo, P. Sengers, and H. Brynjarsd ´ottir, “Mapping the landscape\nof sustainable hci,” in Proceedings of the SIGCHI conference on human\nfactors in computing systems , 2010.\n[9] S. Hussain, O. Ameri Sianaki, and N. Ababneh, “A survey on con-\nversational agents/chatbots classification and design techniques,” in\nWorkshops of the International Conference on Advanced Information\nNetworking and Applications . Springer, 2019, pp. 946–956.\n[10] J. Santos, J. J. Rodrigues, J. Casal, K. Saleem, and V . Denisov,\n“Intelligent personal assistants based on internet of things approaches,”\nIEEE Systems Journal , vol. 12, no. 2, pp. 1793–1802, 2016.\n[11] N. Shevchuk and H. Oinas-Kukkonen, “Exploring green information\nsystems and technologies as persuasive systems: A systematic review of\napplications in published research,” 12 2016.\n[12] B. J. Fogg, “Persuasive technology: Using computers to change what\nwe think and do,” Ubiquity, vol. 2002, no. December, dec 2002.\n[Online]. Available: https://doi.org/10.1145/764008.763957\n[13] A. J. Obaid, “Assessment of smart home assistants as an iot,” In-\nternational Journal of Computations, Information and Manufacturing\n(IJCIM), vol. 1, no. 1, 2021.\n[14] N. Abdi, K. M. Ramokapane, and J. M. Such, “More than smart\nspeakers: Security and privacy perceptions of smart home personal\nassistants.” in SOUPS@ USENIX Security Symposium , 2019.\n[15] K. P. Jadhav and S. A. Thorat, “Towards designing conversational agent\nsystems,” in Computing in Engineering and Technology: Proceedings of\nICCET 2019. Springer, 2020, pp. 533–542.\n[16] K. Ramesh, S. Ravishankaran, A. Joshi, and K. Chandrasekaran, “A\nsurvey of design techniques for conversational agents,” in Informa-\ntion, Communication and Computing Technology: Second International\nConference, ICICCT 2017, New Delhi, India, May 13, 2017, Revised\nSelected Papers. Springer, 2017, pp. 336–350.\n[17] M. Zaib, Q. Z. Sheng, and W. Emma Zhang, “A short survey of pre-\ntrained language models for conversational ai-a new age in nlp,” in\nProceedings of the Australasian computer science week multiconference,\n2020, pp. 1–4.\n[18] G. Lee, V . Hartmann, J. Park, D. Papailiopoulos, and K. Lee, “Prompted\nllms as chatbot modules for long open-domain conversation,” arXiv\npreprint arXiv:2305.04533, 2023.\n[19] P. Pichappan, M. Krishnamurthy, and P. Vijayakumar, “Analysis of\nchatgpt as a question-answering tool,” Journal of Digital Information\nManagement, vol. 21, no. 2, p. 51, 2023.\n[20] A. Lee, “What Are Large Language Models and Why Are They\nImportant? — NVIDIA Blog — blogs.nvidia.com,” https://blogs.nv\nidia.com/blog/2023/01/26/what-are-large-language-models-used-for/,\n2023, [Accessed 07-Jun-2023].\n[21] A. Hoang, A. Bosselut, A. Celikyilmaz, and Y . Choi, “Efficient\nadaptation of pretrained transformers for abstractive summarization,”\nCoRR, vol. abs/1906.00138, 2019. [Online]. Available: http://arxiv.org/\nabs/1906.00138\n[22] Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized bert\npretraining approach,” 2019.\n[23] P. Budzianowski and I. Vuli ´c, “Hello, it’s gpt-2–how can i help you?\ntowards the use of pretrained language models for task-oriented dialogue\nsystems,” arXiv preprint arXiv:1907.05774 , 2019.\n[24] J. Rudolph, S. Tan, and S. Tan, “War of the chatbots: Bard, bing chat,\nchatgpt, ernie and beyond. the new ai gold rush and its impact on higher\neducation,” Journal of Applied Learning and Teaching , vol. 6, no. 1,\n2023.\n[25] S. Pichai, “An important next step on our ai journey,” Feb 2023.\n[Online]. Available: https://blog.google/technology/ai/bard-google-ai-\nsearch-updates/\n[26] Y . Mehdi, “Reinventing search with a new ai-powered microsoft bing\nand edge, your copilot for the web,” May 2023. [Online]. Available:\nhttps://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-\nnew-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/\n[27] Y . Liu, T. Han, S. Ma, J. Zhang, Y . Yang, J. Tian, H. He, A. Li, M. He,\nZ. Liu, Z. Wu, D. Zhu, X. Li, N. Qiang, D. Shen, T. Liu, and B. Ge,\n“Summary of chatgpt/gpt-4 research and perspective towards the future\nof large language models,” 2023.\n[28] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train,\nprompt, and predict: A systematic survey of prompting methods in\nnatural language processing,” ACM Comput. Surv. , vol. 55, no. 9, jan\n2023. [Online]. Available: https://doi.org/10.1145/3560815\n[29] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language mod-\nels are few-shot learners,” Advances in neural information processing\nsystems, vol. 33, pp. 1877–1901, 2020.\n[30] E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy consid-\nerations for deep learning in nlp,” 2019.\n[31] S. S. Biswas, “Potential use of chat gpt in global warming,” Annals\nof Biomedical Engineering , vol. 51, no. 6, pp. 1126–1127, Jun 2023.\n[Online]. Available: https://doi.org/10.1007/s10439-023-03171-8\n[32] J. Lester, K. Branting, and B. Mott, “Conversational agents,” The\npractical handbook of internet computing , pp. 220–240, 2004.\n[33] R. Bavaresco, D. Silveira, E. Reis, J. Barbosa, R. Righi, C. Costa,\nR. Antunes, M. Gomes, C. Gatti, M. Vanzin et al. , “Conversational\nagents in business: A systematic literature review and future research\ndirections,” Computer Science Review , vol. 36, p. 100239, 2020.\n[34] M. Zaib, W. E. Zhang, Q. Z. Sheng, A. Mahmood, and Y . Zhang,\n“Conversational question answering: a survey,” Knowledge and\nInformation Systems , vol. 64, no. 12, pp. 3151–3195, Dec 2022.\n[Online]. Available: https://doi.org/10.1007/s10115-022-01744-y\n[35] M. Giudici, P. Crovari, and F. Garzotto, “Candy: A framework to design\nconversational agents for domestic sustainability,” in 4th Conference\non Conversational User Interfaces , ser. CUI 2022. New York, NY ,\nUSA: Association for Computing Machinery, 2022. [Online]. Available:\nhttps://doi.org/10.1145/3543829.3544515\n[36] L. ˚A. E. J. Hansson, T. Cerratto Pargman, and D. S. Pargman, “A decade\nof sustainable hci: Connecting shci to the sustainable development\ngoals,” in Proceedings of the 2021 CHI Conference on Human Factors\nin Computing Systems , 2021, pp. 1–19.\n[37] J. Weizenbaum, “Eliza—a computer program for the study of natural\nlanguage communication between man and machine,” Communications\nof the ACM , vol. 9, no. 1, pp. 36–45, 1966.\n[38] H.-y. Shum, X.-d. He, and D. Li, “From eliza to xiaoice: challenges and\nopportunities with social chatbots,” Frontiers of Information Technology\n& Electronic Engineering, vol. 19, no. 1, pp. 10–26, Jan 2018. [Online].\nAvailable: https://doi.org/10.1631/FITEE.1700826\n[39] S. A. Thorat and V . Jadhav, “A review on implementation issues of rule-\nbased chatbot systems,” in Proceedings of the international conference\non innovative computing & communications (ICICC) , 2020.\n[40] R. Rosenfeld, “Two decades of statistical language modeling: Where\ndo we go from here?” Proceedings of the IEEE , vol. 88, no. 8, pp.\n1270–1278, 2000.\n[41] Y . Bengio, R. Ducharme, and P. Vincent, “A neural probabilistic\nlanguage model,” Advances in neural information processing systems ,\nvol. 13, 2000.\n[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017.\n[43] U. Gnewuch, S. Morana, C. Heckmann, and A. Maedche, “Designing\nconversational agents for energy feedback,” in International Conference\non Design Science Research in Information Systems and Technology .\nSpringer, 2018.\n[44] M. Giudici, P. Crovari, and F. Garzotto, “Leafy: Enhancing home\nenergy efficiency through gamified experience with a conversational\nsmart mirror,” in Proceedings of the 2023 ACM Conference on\nInformation Technology for Social Good , ser. GoodIT ’23. New York,\nNY , USA: Association for Computing Machinery, 2023, p. 128–134.\n[Online]. Available: https://doi.org/10.1145/3582515.3609526\n[45] S. Diederich, S. Lichtenberg, A. B. Brendel, and S. Trang, “Promot-\ning sustainable mobility beliefs with persuasive and anthropomorphic\ndesign: Insights from an experiment with a conversational agent,” 2019.\n[46] N. M. Cacanindin, “Greening Food Consumption Using Chatbots as\nBehavioral Change Agent,” Journal of Advanced Research in Dynamical\nand Control Systems , vol. 12, no. 01-Special Issue, pp. 204–211, Feb.\n2020.\n[47] M. Gunawardane, H. Pushpakumara, E. Navarathne, S. Lokuliyana,\nK. Kelaniyage, and N. Gamage, “Zero food waste: Food wastage\nsustaining mobile application,” in 2019 International Conference on\nAdvancements in Computing (ICAC) . IEEE, 2019, pp. 129–132.\n[48] R. Cohen, M. Hamri, M. Geva, and A. Globerson, “Lm vs lm: Detecting\nfactual errors via cross examination,” arXiv preprint arXiv:2305.13281 ,\n2023.\n[49] A. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker,\nF. Tombari, A. Purohit, M. Ryoo, V . Sindhwani, J. Lee, V . Vanhoucke,\nand P. Florence, “Socratic models: Composing zero-shot multimodal\nreasoning with language,” 2022.\n[50] L. Yang, J. Hu, M. Qiu, C. Qu, J. Gao, W. B. Croft, X. Liu,\nY . Shen, and J. Liu, “A hybrid retrieval-generation neural conversation\nmodel,” in Proceedings of the 28th ACM International Conference on\nInformation and Knowledge Management , ser. CIKM ’19. New York,\nNY , USA: Association for Computing Machinery, 2019, p. 1341–1350.\n[Online]. Available: https://doi.org/10.1145/3357384.3357881\n[51] M. S. Rahaman, M. Ahsan, N. Anjum, M. M. Rahman, and M. N.\nRahman, “The ai race is on! google’s bard and openai’s chatgpt head\nto head: an opinion article,” Mizanur and Rahman, Md Nafizur, The AI\nRace is on , 2023.\n[52] R. A. Abdulla, B. Garrison, M. Salwen, P. Driscoll, and D. Casey,\n“The credibility of newspapers, television news, and online news,” in\nEducation in Journalism Annual Convention, Florida USA , 2002.\n[53] E. Reiter and A. Belz, “An Investigation into the Validity of Some\nMetrics for Automatically Evaluating Natural Language Generation\nSystems,” Computational Linguistics , vol. 35, no. 4, pp. 529–558, 12\n2009. [Online]. Available: https://doi.org/10.1162/coli.2009.35.4.35405\n[54] Y . Ma, J. Liu, F. Yi, Q. Cheng, Y . Huang, W. Lu, and X. Liu, “Ai vs.\nhuman – differentiation analysis of scientific content generation,” 2023.\n[55] S. Mukherjee, A. Mitra, G. Jawahar, S. Agarwal, H. Palangi, and\nA. Awadallah, “Orca: Progressive learning from complex explanation\ntraces of gpt-4,” 2023.",
  "topic": "Sustainability",
  "concepts": [
    {
      "name": "Sustainability",
      "score": 0.817488431930542
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6738312840461731
    },
    {
      "name": "Computer science",
      "score": 0.598818838596344
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.57021564245224
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5529574751853943
    },
    {
      "name": "Fluency",
      "score": 0.5008804798126221
    },
    {
      "name": "Generative grammar",
      "score": 0.430077463388443
    },
    {
      "name": "Work (physics)",
      "score": 0.424490749835968
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38593775033950806
    },
    {
      "name": "Management science",
      "score": 0.3424701690673828
    },
    {
      "name": "Knowledge management",
      "score": 0.32675108313560486
    },
    {
      "name": "Psychology",
      "score": 0.31744179129600525
    },
    {
      "name": "Engineering",
      "score": 0.19154009222984314
    },
    {
      "name": "Mathematics education",
      "score": 0.15588968992233276
    },
    {
      "name": "Ecology",
      "score": 0.08979213237762451
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}