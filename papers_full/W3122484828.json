{
    "title": "Bottleneck Transformers for Visual Recognition",
    "url": "https://openalex.org/W3122484828",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2795602926",
            "name": "Aravind Srinivas",
            "affiliations": [
                "Berkeley College",
                "University of California, Berkeley"
            ]
        },
        {
            "id": "https://openalex.org/A2522254369",
            "name": "Tsung Yi Lin",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2625834147",
            "name": "Niki Parmar",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2343055381",
            "name": "Jonathon Shlens",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A243981275",
            "name": "Pieter Abbeel",
            "affiliations": [
                "University of California, Berkeley",
                "Berkeley College"
            ]
        },
        {
            "id": "https://openalex.org/A2171687631",
            "name": "Ashish Vaswani",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2795602926",
            "name": "Aravind Srinivas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2522254369",
            "name": "Tsung Yi Lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2625834147",
            "name": "Niki Parmar",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2343055381",
            "name": "Jonathon Shlens",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A243981275",
            "name": "Pieter Abbeel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2171687631",
            "name": "Ashish Vaswani",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2565639579",
        "https://openalex.org/W6772541613",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W6772853553",
        "https://openalex.org/W6638523607",
        "https://openalex.org/W6684191040",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W6638667902",
        "https://openalex.org/W6756709046",
        "https://openalex.org/W6775170262",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2963150697",
        "https://openalex.org/W6749954789",
        "https://openalex.org/W2962872506",
        "https://openalex.org/W2944828972",
        "https://openalex.org/W2549139847",
        "https://openalex.org/W6788467338",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W6631782140",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W6779326418",
        "https://openalex.org/W2990578762",
        "https://openalex.org/W6630336748",
        "https://openalex.org/W6620707391",
        "https://openalex.org/W6775760072",
        "https://openalex.org/W6763442200",
        "https://openalex.org/W6783267081",
        "https://openalex.org/W6770600958",
        "https://openalex.org/W6762718338",
        "https://openalex.org/W2981851019",
        "https://openalex.org/W6637373629",
        "https://openalex.org/W6747753452",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W6639102338",
        "https://openalex.org/W6774314701",
        "https://openalex.org/W6786614245",
        "https://openalex.org/W6688789216",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W6771926570",
        "https://openalex.org/W2031489346",
        "https://openalex.org/W2964444661",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W2097073572",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2982220924",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W6763367864",
        "https://openalex.org/W6730277886",
        "https://openalex.org/W2963016543",
        "https://openalex.org/W6776188000",
        "https://openalex.org/W2981563141",
        "https://openalex.org/W6766673545",
        "https://openalex.org/W2963857746",
        "https://openalex.org/W6766904570",
        "https://openalex.org/W6772899810",
        "https://openalex.org/W3005680577",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2216125271",
        "https://openalex.org/W1536680647",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2981689412",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W1507506748",
        "https://openalex.org/W3138994021",
        "https://openalex.org/W3120129399",
        "https://openalex.org/W2996539502",
        "https://openalex.org/W2998108143",
        "https://openalex.org/W2559085405",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W3085139254",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3035060554",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W2949517790",
        "https://openalex.org/W3034885317",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W3120885796",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W3107668149",
        "https://openalex.org/W3111156583",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2970608575",
        "https://openalex.org/W3034971973",
        "https://openalex.org/W2999512283",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2998228095"
    ],
    "abstract": "We present BoTNet, a conceptually simple yet powerful backbone architecture that incorporates self-attention for multiple computer vision tasks including image classification, object detection and instance segmentation. By just replacing the spatial convolutions with global self-attention in the final three bottleneck blocks of a ResNet and no other changes, our approach improves upon the baselines significantly on instance segmentation and object detection while also reducing the parameters, with minimal overhead in latency. Through the design of BoTNet, we also point out how ResNet bottleneck blocks with self-attention can be viewed as Transformer blocks. Without any bells and whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance Segmentation benchmark using the Mask R-CNN framework; surpassing the previous best published single model and single scale results of ResNeSt evaluated on the COCO validation set. Finally, we present a simple adaptation of the BoTNet design for image classification, resulting in models that achieve a strong performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to 1.64x faster in compute time than the popular EfficientNet models on TPU-v3 hardware. We hope our simple and effective approach will serve as a strong baseline for future research in self-attention models for vision",
    "full_text": "Bottleneck Transformers for Visual Recognition\nAravind Srinivas1 Tsung-Yi Lin2 Niki Parmar2 Jonathon Shlens2 Pieter Abbeel1 Ashish Vaswani2\n1UC Berkeley 2Google Research\n{aravind}@cs.berkeley.edu\nAbstract\nWe present BoTNet, a conceptually simple yet powerful\nbackbone architecture that incorporates self-attention for\nmultiple computer vision tasks including image classiﬁca-\ntion, object detection and instance segmentation. By just\nreplacing the spatial convolutions with global self-attention\nin the ﬁnal three bottleneck blocks of a ResNet and no other\nchanges, our approach improves upon the baselines signiﬁ-\ncantly on instance segmentation and object detection while\nalso reducing the parameters, with minimal overhead in la-\ntency. Through the design of BoTNet, we also point out how\nResNet bottleneck blocks with self-attention can be viewed as\nTransformer blocks. Without any bells and whistles, BoTNet\nachieves 44.4% Mask AP and 49.7% Box AP on the COCO\nInstance Segmentation benchmark using the Mask R-CNN\nframework; surpassing the previous best published single\nmodel and single scale results of ResNeSt [ 67] evaluated\non the COCO validation set. Finally, we present a simple\nadaptation of the BoTNet design for image classiﬁcation,\nresulting in models that achieve a strong performance of\n84.7% top-1 accuracy on the ImageNet benchmark while\nbeing up to 1.64x faster in “compute”1 time than the popu-\nlar EfﬁcientNet models on TPU-v3 hardware. We hope our\nsimple and effective approach will serve as a strong baseline\nfor future research in self-attention models for vision.\n1. Introduction\nDeep convolutional backbone architectures [37, 54, 28,\n66, 56] have enabled signiﬁcant progress in image classiﬁ-\ncation [52], object detection [ 17, 40, 21, 20, 50], instance\nsegmentation [25, 13, 27]. Most landmark backbone archi-\ntectures [37, 54, 28] use multiple layers of3×3 convolutions.\nWhile the convolution operation can effectively capture\nlocal information, vision tasks such as object detection, in-\nstance segmentation, keypoint detection require modeling\nlong range dependencies. For example, in instance segmen-\ntation, being able to collect and associate scene information\nfrom a large neighborhood can be useful in learning relation-\n1Forward and backward propagation for batch size 32\nFigure 1: Left: A ResNet Bottleneck Block, Right: A Bot-\ntleneck Transformer (BoT) block. The only difference is\nthe replacement of the spatial 3 × 3 convolution layer with\nMulti-Head Self-Attention (MHSA). The structure of the\nself-attention layer is described in Figure 4.\nships across objects [32]. In order to globally aggregate the\nlocally captured ﬁlter responses, convolution based archi-\ntectures require stacking multiple layers [54, 28]. Although\nstacking more layers indeed improves the performance of\nthese backbones [67], an explicit mechanism to model global\n(non-local) dependencies could be a more powerful and scal-\nable solution without requiring as many layers.\nModeling long-range dependencies is critical to natural\nlanguage processing (NLP) tasks as well. Self-attention\nis a computational primitive [61] that implements pairwise\nentity interactions with a content-based addressing mecha-\nnism, thereby learning a rich hierarchy of associative features\nacross long sequences. This has now become a standard tool\nin the form of Transformer [61] blocks in NLP with promi-\nnent examples being GPT [46, 5] and BERT [14, 42] models.\nA simple approach to using self-attention in vision is to\nreplace spatial convolutional layers with the multi-head self-\nattention (MHSA) layer proposed in the Transformer [ 61]\n(Figure 1). This approach has seen progress on two seem-\ningly different approaches in the recent past. On the one\nhand, we have models such as SASA [ 49], AACN [ 4],\nSANet [68], Axial-SASA [62], etc that propose to replace\nspatial convolutions in ResNet botleneck blocks [28] with\narXiv:2101.11605v2  [cs.CV]  2 Aug 2021\nFigure 2: A taxonomy of deep learning architectures using self-attention for visual recognition. Our proposed architecture\nBoTNet is a hybrid model that uses both convolutions and self-attention. The speciﬁc implementation of self-attention could\neither resemble a Transformer block [61] or a Non-Local block [63] (difference highlighted in Figure 4). BoTNet is different\nfrom architectures such as DETR [10], VideoBERT [55], VILBERT [44], CCNet [34], etc by employing self-attention within\nthe backbone architecture, in contrast to using them outside the backbone architecture. Being a hybrid model, BoTNet differs\nfrom pure attention models such as SASA [49], LRNet [33], SANet [68], Axial-SASA [31, 62] and ViT [15]. AA-ResNet [4]\nalso attempted to replace a fraction of spatial convolution channels with self-attention.\ndifferent forms of self-attention (local, global, vector, axial,\netc). On the other hand, we have the Vision Transformer\n(ViT) [15], that proposes to stack Transformer blocks [61]\noperating on linear projections of non-overlapping patches.\nIt may appear that these approaches present two different\nclasses of architectures. We point out that it is not the case.\nRather, ResNet botteneck blocks with the MHSA layer can\nbe viewed as Transformer blocks with a bottleneck struc-\nture, modulo minor differences such as the residual connec-\ntions, choice of normalization layers, etc. (Figure 3). Given\nthis equivalence, we call ResNet bottleneck blocks with the\nMHSA layer as Bottleneck Transformer (BoT) blocks.\nHere are a few challenges when using self-attention in\nvision: (1) Image sizes are much larger (1024 ×1024) in ob-\nject detection and instance segmentation compared to image\nclassiﬁcation (224 × 224). (2) The memory and computa-\ntion for self-attention scale quadratically with spatial dimen-\nsions [58], causing overheads for training and inference.\nTo overcome these challenges, we consider the following\ndesign: (1) Use convolutions to efﬁciently learn abstract and\nlow resolutionfeaturemaps from large images; (2) Use global\n(all2all) self-attention to process and aggregate the informa-\ntion contained in the featuremaps captured by convolutions.\nSuch a hybrid design [4] (1) uses existing and well optimized\nprimitives for both convolutions and all2all self-attention; (2)\ncan deal with large images efﬁciently by having convolutions\ndo the spatial downsampling and letting attention work on\nsmaller resolutions. Here is a simple practical instantiation\nof this hybrid design: Replace only the ﬁnal three bottle-\nneck blocks of a ResNet with BoT blocks without any other\nchanges. Or in other words, take a ResNet and only replace\nthe ﬁnal three 3 × 3 convolutions with MHSA layers (Fig\n1, Table 1). This simple change improves the mask AP by\n1.2% on the COCO instance segmentation benchmark [40]\nover our canonical baseline that uses ResNet-50 in the Mask\nR-CNN framework [27] with no hyperparameter differences\nand minimal overheads for training and inference. Moving\nforward, we call this simple instantiation as BoTNet given\nits connections to the Transformer through the BoT blocks.\nWhile we note that there is no novelty in its construction,\nwe believe the simplicity and performance make it a useful\nreference backbone architecture that is worth studying.\nUsing BoTNet, we demonstrate signiﬁcantly improved re-\nsults on instance segmentationwithout any bells and whistles\nsuch as Cascade R-CNN [7], FPN changes [41, 19, 43, 57],\nhyperparameter changes [56], etc. A few key results from\nBoTNet are: (1) Performance gains across various training\nconﬁgurations (Section 4.1), data augmentations (Section\n4.2) and ResNet family backbones (Section 4.4); (2) Signif-\nicant boost from BoTNet on small objects (+2.4 Mask AP\nand +2.6 Box AP) (Appendix); (3) Performance gains over\nNon-Local layers (Section 4.6); (4) Gains that scale well\nwith larger images resulting in 44.4% mask AP, competitive\nwith state-of-the-art performance among entries that only\nstudy backbone architectures with modest training schedules\n(up to 72 epochs) and no extra data or augmentations.2.\n2SoTA is based on https://paperswithcode.com/sota/\ninstance-segmentation-on-coco-minival .\nLastly, we scale BoTNets, taking inspiration from the\ntraining and scaling strategies in [56, 49, 38, 51, 48, 67, 3],\nafter noting that BoTNets do not provide substantial gains in\na smaller scale training regime. We design a family of BoT-\nNet models that achieve up to 84.7% top-1 accuracy on the\nImageNet validation set, while being upto 1.64x faster than\nthe popular EfﬁcientNet models in terms of compute time\non TPU-v3 hardware. By providing strong results through\nBoTNet, we hope that self-attention becomes a widely used\nprimitive in future vision architectures.\n2. Related Work\nA taxonomy of deep learning architectures that employ\nself-attention for vision is presented in Figure 2. In this\nsection, we focus on: (1) Transformer vs BoTNet; (2) DETR\nvs BoTNet; (3) Non-Local vs BoTNet.\nFigure 3: Left: Canonical view of the Transformer with the\nboundaries depicting the deﬁnition of a Transformer block as\ndescribed in Vaswani et. al [61]. Middle: Bottleneck view\nof the Transformer with boundaries depicting what we deﬁne\nas the Bottleneck Transformer (BoT) block in this work. The\narchitectural structure that already exists in the Transformer\ncan be interpreted a ResNet bottleneck block [28] with Multi-\nHead Self-Attention (MHSA) [61] with a different notion of\nblock boundary as illustrated. Right: An instantiation of the\nBottleneck Transformer as a ResNet bottleneck block [28]\nwith the difference from a canonical ResNet block being the\nreplacement of 3 × 3 convolution with MHSA.\nConnection to the Transformer: As the title of the pa-\nper suggests, one key message in this paper is that ResNet\nbottleneck blocks with Multi-Head Self-Attention (MHSA)\nlayers can be viewed as Transformer blocks with a bottle-\nneck structure. This is visually explained in Figure 3 and\nwe name this block as Bottleneck Transformer (BoT). We\nnote that the architectural design of the BoT block is not\nour contribution. Rather, we point out the relationship be-\ntween MHSA ResNet bottleneck blocks and the Transformer\nwith the hope that it improves our understanding of archi-\ntecture design spaces [47, 48] for self-attention in computer\nvision. There are still a few differences aside from the ones\nalready visible in the ﬁgure (residual connections and block\nboundaries): (1) Normalization: Transformers use Layer\nNormalization [1] while BoT blocks use Batch Normaliza-\ntion [35] as is typical in ResNet bottleneck blocks [28]; (2)\nNon-Linearities: Transformers use one non-linearity in the\nFFN block, while the ResNet structure allows BoT block to\nuse three non-linearities; (3) Output projections: The MHSA\nblock in a Transformer contains an output projection while\nthe MHSA layer (Fig 4) in a BoT block (Fig 1) does not;\n(4) We use the SGD with momentum optimizer typically\nused in computer vision [28, 27, 22] while Transformers are\ngenerally trained with the Adam optimizer [36, 61, 10, 15].\nConnection to DETR: Detection Transformer (DETR)\nis a detection framework that uses a Transformer to implicitly\nperform region proposals and localization of objects instead\nof using an R-CNN [21, 20, 50, 27]. Both DETR and BoT-\nNet attempt to use self-attention to improve the performance\non object detection and instance (or panoptic) segmentation.\nThe difference lies in the fact that DETR uses Transformer\nblocks outside the backbone architecture with the motivation\nto get rid of region proposals and non-maximal suppression\nfor simplicity. On the other hand, the goal in BoTNet is to\nprovide a backbone architecture that uses Transformer-like\nblocks for detection and instance segmentation. We are ag-\nnostic to the detection framework (be it DETR or R-CNN).\nWe perform our experiments with the Mask [27] and Faster\nR-CNN [50] systems and leave it for future work to integrate\nBoTNet as the backbone in the DETR framework. With\nvisibly good gains on small objects in BoTNet, we believe\nthere maybe an opportunity to address the lack of gain on\nsmall objects found in DETR, in future (refer to Appendix).\nConnection to Non-Local Neural Nets: 3 Non-Local\n(NL) Nets [63] make a connection between the Transformer\nand the Non-Local-Means algorithm [ 6]. They insert NL\nblocks into the ﬁnal one (or) two blockgroups (c4,c5) in a\nResNet and improve the performance on video recognition\nand instance segmentation. Like NL-Nets [63, 8], BoTNet is\na hybrid design using convolutions and global self-attention.\n3The replacement vs insertion contrast has previously been pointed out\nin AA-ResNet (Bello et. al) [4]. The difference in our work is the complete\nreplacement as opposed to fractional replacement in Bello et al.\n(1) Three differences between a NL layer and a MHSA layer\n(illustrated in Figure 4): use of multiple heads, value pro-\njection and position encodings in MHSA; (2) NL blocks\nuse a bottleneck with channel factor reduction of 2 (instead\nof 4 in BoT blocks which adopt the ResNet structure); (3)\nNL blocks are inserted as additional blocks into a ResNet\nbackbone as opposed to replacing existing convolutional\nblocks as done by BoTNet. Section 4.6 offers a comparison\nbetween BoTNet, NLNet as well as a NL-like version of\nBoTNet where we insert BoT blocks in the same manner as\nNL blocks instead of replacing.\n3. Method\nstage output ResNet-50 BoTNet-50\nc1 512 × 512 7×7, 64, stride 2 7×7, 64, stride 2\nc2 256 × 256\n3×3 max pool, stride 2 3×3 max pool, stride 2\n\n1×1, 64\n3×3, 64\n1×1, 256\n\n×3\n\n\n1×1, 64\n3×3, 64\n1×1, 256\n\n×3\nc3 128 × 128\n\n\n1×1, 128\n3×3, 128\n1×1, 512\n\n×4\n\n\n1×1, 128\n3×3, 128\n1×1, 512\n\n×4\nc4 64 × 64\n\n\n1×1, 256\n3×3, 256\n1×1, 1024\n\n×6\n\n\n1×1, 256\n3×3, 256\n1×1, 1024\n\n×6\nc5 32 × 32\n\n\n1×1, 512\n3×3, 512\n1×1, 2048\n\n×3\n\n\n1×1, 512\nMHSA, 512\n1×1, 2048\n\n×3\n# params. 25.5×106 20.8×106\nM.Adds 85.4×109 102.98×109\nTPU steptime 786.5 ms 1032.66 ms\nTable 1: Architecture of BoTNet-50 (BoT50): The only\ndifference in BoT50 from ResNet-50 (R50) is the use of\nMHSA layer (Figure 4) in c5. For an input resolution of\n1024 × 1024, the MHSA layer in the ﬁrst block of c5 oper-\nates on 64 ×64 while the remaining two operate on 32 ×32.\nWe also report the parameters, multiply-adds (m. adds)\nand training time throughput (TPU-v3 steptime on a v3-8\nCloud-TPU). BoT50 has only 1.2x more m.adds. than R50.\nThe overhead in training throughout is 1.3x. BoT50 also has\n1.2x fewer parameters than R50. While it may appear that it\nis simply the aspect of performing slightly more computa-\ntions that might help BoT50 over the baseline, we show that\nit is not the case in Section 4.4.\nBoTNet by design is simple: replace the ﬁnal three spa-\ntial (3 × 3) convolutions in a ResNet with Multi-Head Self-\nAttention (MHSA) layers that implement global ( all2all)\nself-attention over a 2D featuremap (Fig 4). A ResNet typ-\nically has 4 stages (or blockgroups) commonly referred to\nas [c2,c3,c4,c5] with strides [4,8,16,32] relative\nto the input image, respectively. Stacks [c2,c3,c4,c5]\nconsist of multiple bottleneck blocks with residual connec-\ntions (e.g, R50 has [3,4,6,3] bottleneck blocks).\nH x W x d H x W x dH x W x d\nH*W x d\nH x 1 x d 1 x W x d\nH*W x H*W H*W x H*W\nz\nH*W x H*W\nH x W x d\nsoftmax\nRh\nH x W x d\nRw\nSelf-Attention Layer\nWV : 1 x 1\nH x W x d\nWK : 1 x 1WQ : 1 x 1\nx\nqr k v\nqkTqrT\ncontent-contentcontent-position\nFigure 4: Multi-Head Self-Attention (MHSA) layer used\nin the BoT block. While we use 4 heads, we do not show\nthem on the ﬁgure for simplicity. all2all attention is\nperformed on a 2D featuremap with split relative position\nencodings Rh and Rw for height and width respectively. The\nattention logits are qkT + qrT where q, k, rrepresent query,\nkey and position encodings respectively (we use relative dis-\ntance encodings [53, 4, 49]). ⨁and ⨂represent element\nwise sum and matrix multiplication respectively, while 1 × 1\nrepresents a pointwise convolution. Along with the use of\nmultiple heads, the highlighted blue boxes (position encod-\nings and the value projection are the only three elements that\nare not present in the Non-Local Layer [63, 65].\nApproaches that use self-attention throughout the back-\nbone [ 49, 4, 68, 15] are feasible for input resolutions\n(224 × 224 (for classiﬁcation) and 640 × 640 (for detec-\ntion experiments in SASA [49])) considered in these papers.\nOur goal is to use attention in more realistic settings of high\nperformance instance segmentation models, where typically\nimages of larger resolution ( 1024 × 1024) are used. Con-\nsidering that self-attention when performed globally across\nn entities requires O(n2d) memory and computation [61],\nwe believe that the simplest setting that adheres to the above\nfactors would be to incorporate self-attention at the low-\nest resolution featuremaps in the backbone, ie, the residual\nblocks in the c5 stack. The c5 stack in a ResNet backbone\ntypically uses 3 blocks with one spatial 3 × 3 convolution\nin each. Replacing them with MHSA layers forms the basis\nof the BoTNet architecture. The ﬁrst block in c5 uses a\n3 × 3 convolution of stride 2 while the other two use a stride\nof 1. Since all2all attention is not a strided operation,\nwe use a 2 × 2 average-pooling with a stride 2 for the ﬁrst\nBoT block. The BoTNet architecture is described in Table 1\nand the MHSA layer is presented in Figure 4. The strided\nversion of the BoT block is presented in the Appendix.\nRelative Position Encodings: In order to make the atten-\ntion operation position aware, Transformer based architec-\ntures typically make use of a position encoding [61]. It has\nbeen observed lately that relative-distance-aware position\nencodings [53] are better suited for vision tasks [4, 49, 68].\nThis can be attributed to attention not only taking into ac-\ncount the content information but also relative distances\nbetween features at different locations, thereby, being able\nto effectively associate information across objects with po-\nsitional awareness. In BoTNet, we adopt the 2D relative\nposition self-attention implementation from [49, 4].\nte\n4. Experiments\nWe study the beneﬁts of BoTNet for instance segmen-\ntation and object detection. We perform a thorough abla-\ntion study of various design choices through experiments\non the COCO dataset [40]. We report the standard COCO\nmetrics including the APbb (averaged over IoU thresholds),\nAPbb\n50, APbb\n75, APmk; APmk\n50 , APmk\n75 for box and mask respec-\ntively. As is common practice these days, we train using\nthe COCO train set and report results on the COCO val\n(or minival) set as followed in Detectron [22]4. Our ex-\nperiments are based on the Google Cloud TPU detection\ncodebase5. We run all the baselines and ablations with\nthe same codebase. Unless explicitly speciﬁed, our train-\ning infrastructure uses v3-8 Cloud-TPU which contains\n8 cores with 16 GB memory per core. We train with the\nbfloat16 precision and cross-replica batch normaliza-\ntion [35, 64, 27, 22, 45] using a batch size of 64.\n4.1. BoTNet improves over ResNet on COCO In-\nstance Segmentation with Mask R-CNN\nWe consider the simplest and most widely used setting:\nResNet-506 backbone with FPN7. We use images of resolu-\ntion 1024 ×1024 with a multi-scale jitter of [0.8, 1.25] (scal-\ning the image dimension between 820 and 1280, in order to\nbe consistent with the Detectron setting of using800×1300).\nIn this setting, we benchmark both the ResNet-50 (R50) and\nBoT ResNet-50 (BoT50) as the backbone architectures for\nmultiple training schedules: 1x: 12 epochs, 2x: 24 epochs,\n4train - 118K images, val - 5K images\n5https://github.com/tensorflow/tpu/tree/master/\nmodels/official/detection\n6We use the ResNet backbones pre-trained on ImageNet classiﬁcation as\nis common practice. For BoTNet, the replacement layers arenot pre-trained\nbut randomly initialized for simplicity; the remaining layers are initialized\nfrom a pre-trained ResNet.\n7FPN refers to Feature Pyramid Network [ 39]. We use it in every\nexperiment we report results on, and our FPN levels from 2 to 6 (p2 to p6)\nsimilar to Detectron [22].\nBackbone epochs APbb APmk\nR50 12 39.0 35.0\nBoT50 12 39.4 (+ 0.4) 35.3 (+ 0.3)\nR50 24 41.2 36.9\nBoT50 24 42.8 (+ 1.6) 38.0 (+ 1.1)\nR50 36 42.1 37.7\nBoT50 36 43.6 (+ 1.5) 38.9 (+ 1.2)\nR50 72 42.8 37.9\nBoT50 72 43.7 (+ 0.9) 38.7 (+ 0.8)\nTable 2: Comparing R50 and BoT50 under the 1x (12\nepochs), 3x (36 epochs) and 6x (72 epochs) settings, trained\nwith image resolution 1024 × 1024 and multi-scale jitter of\n[0.8, 1.25].\n3x: 36 epochs, 6x: 72 epochs 8, all using the same hyper-\nparameters for both the backbones across all the training\nschedules (Table 2). We clearly see that BoT50 is a signiﬁ-\ncant improvement on top of R50 barring the 1x schedule (12\nepochs). This suggests that BoT50 warrants longer training\nin order to show signiﬁcant improvement over R50. We also\nsee that the improvement from BoT50 in the 6x schedule (72\nepochs) is worse than its improvement in the 3x schedule\n(32 epochs). This suggests that training much longer with\nthe default scale jitter hurts. We address this by using a more\naggressive scale jitter (Section 4.2).\n4.2. Scale Jitter helps BoTNet more than ResNet\nBackbone jitter APbb APmk\nR50 [0.8, 1.25] 42.8 37.9\nBoT50 [0.8, 1.25] 43.7 (+ 0.9) 38.7 (+ 0.8)\nR50 [0.5, 2.0] 43.7 39.1\nBoT50 [0.5, 2.0] 45.3 (+ 1.8) 40.5 (+ 1.4)\nR50 [0.1, 2.0] 43.8 39.2\nBoT50 [0.1, 2.0] 45.9 (+ 2.1) 40.7 (+ 1.5)\nTable 3: Comparing R50 and BoT50 under three settings of\nmulti-scale jitter, all trained with image resolution 1024 ×\n1024 for 72 epochs (6x training schedule).\nIn Section 4.1, we saw that training much longer (72\nepochs) reduced the gains for BoT50. One way to address\nthis is to increase the amount of multi-scale jitter which has\nbeen known to improve the performance of detection and\nsegmentation systems [16, 18]. Table 3 shows that BoT50\nis signiﬁcantly better than R50 ( + 2.1% on AP bb and +\n1.7% on APmk) for multi-scale jitter of [0.5, 2.0], while also\nshowing signiﬁcant gains ( + 2.2% on APbb and + 1.6% on\n81x, 2x, 3x and 6x convention is adopted from MoCo [26].\nAPmk) for scale jitter of [0.1, 2.0], suggesting that BoTNet\n(self-attention) beneﬁts more from extra augmentations such\nas multi-scale jitter compared to ResNet (pure convolutions).\n4.3. Relative Position Encodings Boost Performance\nBoTNet uses relative position encodings [53]. We present\nan ablation for the use of relative position encodings by\nbenchmarking the individual gains from content-content in-\nteraction (qkT ) and content-position interaction (qrT ) where\nq, k, rrepresent the query, key and relative position encod-\nings respectively. The ablations (Table 4) are performed\nwith the canonical setting9. We see that the gains from qrT\nand qkT are complementary with qrT more important, ie,\nqkT standalone contributes to 0.6% AP bb and 0.6% APmk\nimprovement over the R50 baseline, while qrT standalone\ncontributes to 1.0% AP bb and 0.7 % AP mk improvement.\nWhen combined together ( qkT + qrT ), the gains on both\nAPbb and APmk are additive ( 1.5% and 1.2% respectively).\nWe also see that using absolute position encodings ( qrT\nabs)\ndoes not provide as much gain as relative. This suggests that\nintroducing relative position encodings into architectures\nlike DETR [10] is an interesting direction for future work.\nBackbone Att. Type APbb APmk\nR50 - 42.1 37.7\nBoT50 qkT 42.7 (+ 0.6) 38.3 (+ 0.6)\nBoT50 qrT\nrelative 43.1 (+ 1.0) 38.4 (+ 0.7)\nBoT50 qkT + qrT\nrelative 43.6 (+ 1.5) 38.9 (+ 1.2)\nBoT50 qkT + qrT\nabs 42.5 (+ 0.4) 38.1 (+ 0.4)\nTable 4: Ablation for Relative Position Encoding: Gains\nfrom the two types of interactions in the MHSA layers,\ncontent-content (qkT ) and content-position (qrT ).\n4.4. BoTNet improves backbones in ResNet Family\nHow well does the replacement setup of BoTNet work\nfor other backbones in the ResNet family? Table 5 presents\nthe results for BoTNet with R50, R101, and R152. All\nthese experiments use the canonical training setting (refer\nto footnote in 4.3). These results demonstrate that BoTNet\nis applicable as a drop-in replacement for any ResNet back-\nbone. Note that BoT50 is better than R101 (+ 0.3% AP bb,\n+ 0.5% APmk) while it is competitive with R152 on AP mk.\nReplacing 3 spatial convolutions with all2all attention\ngives more improvement in the metrics compared to stacking\n50 more layers of convolutions (R101), and is competitive\nwith stacking 100 more layers (R152), supporting our initial\nhypothesis that long-range dependencies are better captured\n9res:1024x1024, 36 epochs (3x schedule),\nmulti-scale jitter:[0.8, 1.25]\nthrough attention than stacking convolution layers.10\nBackbone APbb APmk\nR50 42.1 37.7\nBoT50 43.6 (+ 1.5) 38.9 (+ 1.2)\nR101 43.3 38.4\nBoT101 45.5 (+ 2.2) 40.4 (+ 2.0)\nR152 44.2 39.1\nBoT152 46.0 (+ 1.8) 40.6 (+ 1.5)\nTable 5: Comparing R50, R101, R152, BoT50, BoT101 and\nBoT152; all 6 setups using the canonical training schedule of\n36 epochs, 1024×1024 images, multi-scale jitter [0.8, 1.25].\n4.5. BoTNet scales well with larger images\nWe benchmark BoTNet as well as baseline ResNet when\ntrained on 1280 × 1280 images in comparison to 1024 ×\n1024 using the best conﬁg: multi-scale jitter of [0.1, 2.0] and\ntraining for 72 epochs. Results are presented in Tables 6\nand 8. Results in Table 6 suggest that BoTNet beneﬁts from\ntraining on larger images for all of R50, R101 and R152.\nBoTNet trained on 1024 × 1024 (leave alone 1280 × 1280)\nis signiﬁcantly better than baseline ResNet trained on1280×\n1280. Further, BoT200 trained with 1280 × 1280 achieves a\nAPbb of 49.7% and APmk of 44.4%. We believe this result\nhighlights the power of self-attention, in particular, because\nit has been achieved without any bells and whistles such as\nmodiﬁed FPN [41, 19, 16, 57], cascade RCNN [7], etc. This\nresult surpasses the previous best published single model\nsingle scale instance segmentation result from ResNeSt [67]\nevaluated on the COCO minival (44.2% APmk).\nBackbone res APbb APmk\nR50 1280 44.0 39.5\nBoT50 1024 45.9 (+ 1.9) 40.7 (+ 1.2)\nBoT50 1280 46.1 (+ 2.1) 41.2 (+ 1.8)\nR101 1280 46.4 41.2\nBoT101 1024 47.4 (+ 1.0) 42.0 (+ 0.8)\nBoT101 1280 47.9 (+ 1.5) 42.4 (+ 1.2)\nTable 6: All the models are trained for 72 epochs with a\nmulti-scale jitter of [0.1, 2.0].\n10Note that while one may argue that the improvements of BoT50 over\nR50 could be attributed to having 1.2x more M. Adds, BoT50 ( 121 ×\n109 M.Adds) is also better than R101 ( 162.99 × 109 B M. Adds and\nis competitive with R152 ( 240.56 × 109 M. Adds) despite performing\nsigniﬁcantly less computation.\nBackbone Change in backbone APbb APmk\nR50 - 42.1 37.7\nR50 + NL [63] + 1 NL block in c4 43.1 38.4\nR50 + BoT (c4) + 1 BoT block in c4 43.7 38.9\nR50 + BoT (c4, c5) + 2 BoT blocks in c4,c5 44.9 39.7\nBoT50 Replacement in c5 43.6 38.9\nTable 7: Comparison between BoTNet and Non-Local (NL)\nNets: All models trained for 36 epochs with image size\n1024 × 1024, jitter [0.8, 1.25].\nBackbone APbb APbb\n50 APbb\n75 APmk APmk\n50 APmk\n75\nBoT152 49.5 71.0 54.2 43.7 68.2 47.4\nBoT200 49.7 71.3 54.6 44.4 68.9 48.2\nTable 8: BoT152 and BoT200 trained for 72 epochs with a\nmulti-scale jitter of [0.1, 2.0].\n4.6. Comparison with Non-Local Neural Networks\nHow does BoTNet compare to Non-Local Neural Net-\nworks? NL ops are inserted into the c4 stack of a ResNet\nbackbone between the pre-ﬁnal and ﬁnal bottleneck blocks.\nThis adds more parameters to the model, whereas BoTNet\nends up reducing the model parameters (Table 5). In the\nNL mould, we add ablations where we introduce BoT block\nin the exact same manner as the NL block. We also run an\nablation with the insertion of two BoT blocks, one each in\nthe c4,c5 stacks. Results are presented in Table 7. Adding\na NL improves APbb by 1.0 and APbb by 0.7, while adding a\nBoT block gives +1.6 APbb and +1.2 APmk showing that BoT\nblock design is better than NL. Further, BoT-R50 (which\nreplaces instead of adding new blocks) provides +1.5 APbb\nand + 1.2 APmk, as good as adding another BoT block and\nbetter than adding one additional NL block.\n4.7. Image Classiﬁcation on ImageNet\n4.7.1 BoTNet-S1 architecture\nWhile we motivated the design of BoTNet for detection and\nsegmentation, it is a natural question to ask whether the\nBoTNet architecture design also helps improve the image\nclassiﬁcation performance on the ImageNet [52] benchmark.\nPrior work [65] has shown that adding Non-Local blocks\nto ResNets and training them using canonical settings does\nnot provide substantial gains. We observe a similar ﬁnd-\ning for BoTNet-50 when contrasted with ResNet-50, with\nboth models trained with the canonical hyperparameters for\nImageNet [48]: 100 epochs, batch size 1024, weight decay\n1e-4, standard ResNet data augmentation, cosine learning\nrate schedule (Table 9). BoT50 does not provide signiﬁcant\ngains over R50 on ImageNet though it does provide the bene-\nﬁt of reducing the parameters while maintaining comparable\ncomputation (M.Adds).\nA simple method to ﬁx this lack of gain is to take advan-\ntage of the image sizes typically used for image classiﬁcation.\nIn image classiﬁcation, we often deal with much smaller im-\nage sizes ( 224 × 224) compared to those used in object\ndetection and segmentation (1024×1024). The featuremaps\non which the BoT blocks operate are hence much smaller\n(e.g 14 × 14, 7 × 7) compared to those in instance segmen-\ntation and detection (e.g 64 × 64, 32 × 32). With the same\nnumber of parameters, and, without a signiﬁcant increase\nin computation, the BoTNet design in the c5 blockgroup\ncan be changed to uniformly use a stride of 1 in all the ﬁnal\nMHSA layers. We call this design as BoTNet-S1 (S1 to\ndepict stride 1 in the ﬁnal blockgroup). We note that this ar-\nchitecture is similar in design to the hybrid models explored\nin Vision Transformer (ViT) [15] that use a ResNet up to\nstage c4 prior to stacking Transformer blocks. The main\ndifference between BoTNet-S1 and the hybrid ViT models\nlies in the use of BoT blocks as opposed to regular Trans-\nformer blocks (other differences being normalization layer,\noptimizer, etc as mentioned in the contrast to Transformer in\nRelated Work (Sec. 2). The architectural distinction amongst\nResNet, BoTNet and BoTNet-S1, in the ﬁnal blockgroup, is\nvisually explained in the Appendix). The strided BoT block\nis visually explained in the Appendix.\n4.7.2 Evaluation in the standard training setting\nWe ﬁrst evaluate this design for the 100 epoch setting along\nwith R50 and BoT50. We see that BoT-S1-50 improves on\ntop of R50 by 0.9% in the regular setting (Table 9). This\nimprovement does however come at the cost of more compu-\ntation (m.adds). Nevertheless, the improvement is a promis-\ning signal for us to design models that scale well with larger\nimages and improved training conditions that have become\nmore commonly used since EfﬁcientNets [56].\nBackbone M.Adds Params top-1 acc.\nR50 3.86G 25.5M 76.8\nBoT50 3.79G 20.8M 77.0 (+0.2)\nBoT-S1-50 4.27G 20.8M 77.7 (+ 0.9)\nTable 9: ImageNet results in regular training setting: 100\nepochs, batch size 1024, weight decay 1e-4, standard ResNet\naugmentation, for all three models.\n4.7.3 Effect of data augmentation and longer training\nWe saw from our instance segmentation experiments that\nBoTNet and self-attention beneﬁt more from regularization\nsuch as data augmentation (in the case of segmentation, in-\ncreased multi-scale jitter) and longer training. It is natural\nto expect that the gains from BoT and BoT-S1 could im-\nprove when training under an improved setting: 200 epochs,\nbatch size 4096, weight decay 8e-5, RandAugment (2 layers,\nmagnitude 10), and label smoothing of 0.1. In line with our\nintuition, the gains are much more signiﬁcant in this setting\nfor both BoT50 (+ 0.6%) and BoT-S1-50 (+ 1.4%) compared\nto the baseline R50 (Table 10).\nBackbone top-1 acc. top-5 acc.\nR50 77.7 93.9\nBoT50 78.3 (+ 0.6) 94.2 (+ 0.3)\nBoT-S1-50 79.1 (+ 1.4) 94.4 (+ 0.5)\nTable 10: ImageNet results in an improved training setting:\n200 epochs, batch size 4096, weight decay 8e-5, RandAug-\nment (2 layers, magnitude 10), and label smoothing of 0.1\n4.7.4 Scaling BoTNets\n0 200 400 600 800 1000 1200 1400\nTPU-v3 Compute Steptime for Batch Size 32 (milliseconds)\n81.0\n81.5\n82.0\n82.5\n83.0\n83.5\n84.0\n84.5\n85.0Top-1 Accuracy (%)\nT3\nT4\nT5\nT6\nT7\nB3\nB4\nB5\nB6\nB7\nB7-RA\nS5\nS3\nS2\nS4\nViT Regularized (DeiT-384)\nT7-320\nBoTNets (T)\nEfficientNets (B)\nSENets (S)\nFigure 5: All backbones along with ViT and DeiT summa-\nrized in the form of scatter-plot and Pareto curves. SENets\nand BoTNets were trained while the accuracy of other mod-\nels have been reported from corresponding papers.\nThe previous ablations show the BoNets performance\nwith a ResNet-50 backbone and 224 ×224 image resolution.\nHere we study BoTNets when scaling up the model capacity\nand image resolution. There have been several works improv-\ning the performance of ConvNets on ImageNet [67, 56, 3].\nBello et al. [3] recently propose scaling strategies that mainly\nincrease model depths and increase the image resolutions\nmuch slower compared to the compound scaling rule pro-\nposed in EfﬁcientNets [ 56]. We use similar scaling rules\nand design a family of BoTNets. The details of model\ndepth and image resolutions are in the Appendix. We com-\npare to the SENets baseline to understand the impact of the\nBoT blocks. The BoTNets and SENets experiments are per-\nformed under the same training settings (e.g., regularization\nand data augmentation). We additionally show EfﬁcientNet\nand DeiT [60] (regularized version of ViT [15])11 to under-\nstand the performance of BoTNets compared with popular\nConvNets and Transformer models. EfﬁcientNets and DeiT\nare trained under strong data augmentation, model regular-\nization, and long training schedules, similar to the training\nsettings of BoTNets in the experiments.\nResNets and SENets are strong baselines until 83%\ntop-1 accuracy. ResNets and SENets achieve strong perfor-\nmance in the improved EfﬁcientNet training setting. BoT-\nNets T3 and T4 do not outperform SENets, while T5 does\nperform on par with S4. This suggests that pure convo-\nlutional models such as ResNets and SENets are still the\nbest performing models until an accuracy regime of 83%.\nBoTNets scale better beyond 83% top-1 accuracy.While\nSENets are a powerful model class outperforms BoTNets\n(up to T4), we found gains to diminish beyond SE-350 (350\nlayer SENet described in Appendix) trained with image size\n384. This model is referred to as S5 and achieves 83.8%\ntop-1 accuracy. On the other hand, BoTNets scale well to\nlarger image sizes (corroborating with our results in instance\nsegmentation when the gains from self-attention were much\nmore visible for larger images). In particular, T7 achieves\n84.7% top-1 acc., matching the accuracy of B7-RA, with a\n1.64x speedup in efﬁciency. BoTNets perform better than\nViT-regularized (DeiT-384), showing the power of hybrid\nmodels that make use of both convolutions and self-attention\ncompared to pure attention models on ImageNet-1K.\n5. Conclusion\nThe design of vision backbone architectures that use\nself-attention is an exciting topic. We hope that our\nwork helps in improving the understanding of architec-\nture design in this space. Incorporating self-attention for\nother computer vision tasks such as keypoint detection [9]\nand 3D shape prediction [ 23]; studying self-attention ar-\nchitectures for self-supervised learning in computer vi-\nsion [29, 26, 59, 11, 24, 12]; and scaling to much larger\ndatasets such as JFT, YFCC and Instagram, are ripe avenues\nfor future research. Comparing to, and incorporating al-\nternatives to self-attention such as lambda-layers [ 2] is an\nimportant future direction as well.\n6. Acknowledgements\nWe thank Ilija Radosavovic for several useful discus-\nsions; Pengchong Jin and Xianzhi Du for help with the\nTF Detection codebase; Irwan Bello, Barret Zoph, Neil\n11ViT refers to Vision Transformer [ 15], while DeiT refers to Data-\nEfﬁcient Image Transformer [ 60]. DeiT can be viewed as a regularized\nversion of ViT with augmentations, better training hyperparameters tuned\nfor ImageNet, and knowledge distillation [ 30]. We do not compare to\nthe distilled version of DeiT since it’s an orthogonal axis of improvement\napplicable to all models.\nHoulsby, Alexey Dosovitskiy for feedback. We thank Zak\nStone for extensive compute support throughout this project\nthe through TFRC program providing Google Cloud TPUs\n(https://www.tensorflow.org/tfrc).\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[2] Irwan Bello. Lambdanetworks: Modeling long-range inter-\nactions without attention. In International Conference on\nLearning Representations, 2021.\n[3] Irwan Bello, William Fedus, Xianzhi Du, Ekin D. Cubuk,\nAravind Srinivas, Tsung-Yi Lin, Jonathon Shlens, and Barret\nZoph. Revisiting ResNets: Improved Training and Scaling\nStrategies, 2021.\n[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional networks.\nIn Proceedings of the IEEE International Conference on Com-\nputer Vision, pages 3286–3295, 2019.\n[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n[6] Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local\nalgorithm for image denoising. In 2005 IEEE Computer Soci-\nety Conference on Computer Vision and Pattern Recognition\n(CVPR’05), volume 2, pages 60–65. IEEE, 2005.\n[7] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n6154–6162, 2018.\n[8] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In Proceedings of the IEEE International\nConference on Computer Vision Workshops, pages 0–0, 2019.\n[9] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part afﬁnity\nﬁelds. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7291–7299, 2017.\n[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. arXiv preprint\narXiv:2005.12872, 2020.\n[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. arXiv preprint arXiv:2002.05709,\n2020.\n[12] Xinlei Chen and Kaiming He. Exploring simple siamese\nrepresentation learning. arXiv preprint arXiv:2011.10566,\n2020.\n[13] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware se-\nmantic segmentation via multi-task network cascades. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3150–3158, 2016.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale, 2020.\n[16] Xianzhi Du, Tsung-Yi Lin, Pengchong Jin, Golnaz Ghiasi,\nMingxing Tan, Yin Cui, Quoc V Le, and Xiaodan Song.\nSpinenet: Learning scale-permuted backbone for recognition\nand localization. arXiv preprint arXiv:1912.05027, 2019.\n[17] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88(2):303–338, 2010.\n[18] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D. Cubuk, Quoc V . Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation, 2020.\n[19] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn:\nLearning scalable feature pyramid architecture for object de-\ntection. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 7036–7045, 2019.\n[20] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 1440–1448,\n2015.\n[21] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n580–587, 2014.\n[22] Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr\nDollár, and Kaiming He. Detectron, 2018.\n[23] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh\nr-cnn. In Proceedings of the IEEE International Conference\non Computer Vision, pages 9785–9795, 2019.\n[24] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin\nTallec, Pierre H Richemond, Elena Buchatskaya, Carl Do-\nersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Moham-\nmad Gheshlaghi Azar, et al. Bootstrap your own latent: A\nnew approach to self-supervised learning. arXiv preprint\narXiv:2006.07733, 2020.\n[25] Bharath Hariharan, Pablo Arbeláez, Ross Girshick, and Ji-\ntendra Malik. Simultaneous detection and segmentation. In\nEuropean Conference on Computer Vision, pages 297–312.\nSpringer, 2014.\n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual repre-\nsentation learning, 2019.\n[27] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017.\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016.\n[29] Olivier J Hénaff, Aravind Srinivas, Jeffrey De Fauw, Ali\nRazavi, Carl Doersch, SM Eslami, and Aaron van den Oord.\nData-efﬁcient image recognition with contrastive predictive\ncoding. arXiv preprint arXiv:1905.09272, 2019.\n[30] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531, 2015.\n[31] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv preprint arXiv:1912.12180, 2019.\n[32] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3588–3597, 2018.\n[33] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. InProceedings of the\nIEEE International Conference on Computer Vision, pages\n3464–3473, 2019.\n[34] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang,\nYunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention\nfor semantic segmentation. In Proceedings of the IEEE In-\nternational Conference on Computer Vision, pages 603–612,\n2019.\n[35] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015.\n[36] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980 ,\n2014.\n[37] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classiﬁcation with deep convolutional neural networks.\nIn NeurIPS, 2012.\n[38] Jungkyu Lee, Taeryun Won, Tae Kwan Lee, Hyemin Lee,\nGeonmo Gu, and Kiho Hong. Compounding the performance\nimprovements of assembled techniques in a convolutional\nneural network. arXiv preprint arXiv:2001.06268, 2020.\n[39] Tsung-Yi Lin, Piotr Dollár, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n2117–2125, 2017.\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision , pages 740–755.\nSpringer, 2014.\n[41] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path\naggregation network for instance segmentation. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 8759–8768, 2018.\n[42] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettle-\nmoyer, and Veselin Stoyanov. Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692,\n2019.\n[43] Yudong Liu, Yongtao Wang, Siwei Wang, TingTing Liang,\nQijie Zhao, Zhi Tang, and Haibin Ling. Cbnet: A novel\ncomposite backbone network architecture for object detection.\narXiv preprint arXiv:1909.03625, 2019.\n[44] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In Advances in Neural Information\nProcessing Systems, pages 13–23, 2019.\n[45] Chao Peng, Tete Xiao, Zeming Li, Yuning Jiang, Xiangyu\nZhang, Kai Jia, Gang Yu, and Jian Sun. Megdet: A large mini-\nbatch object detector. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 6181–\n6189, 2018.\n[46] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[47] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo,\nand Piotr Dollár. On network design spaces for visual recog-\nnition. In Proceedings of the IEEE International Conference\non Computer Vision, pages 1882–1890, 2019.\n[48] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Dollár. Designing network design spaces.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10428–10436, 2020.\n[49] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019.\n[50] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NeurIPS, 2015.\n[51] Tal Ridnik, Hussam Lawen, Asaf Noy, Emanuel Ben Baruch,\nGilad Sharir, and Itamar Friedman. Tresnet: High perfor-\nmance gpu-dedicated architecture. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 1400–1409, 2021.\n[52] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115(3):211–252, 2015.\n[53] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. arXiv preprint\narXiv:1803.02155, 2018.\n[54] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[55] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and\nCordelia Schmid. Videobert: A joint model for video and\nlanguage representation learning. In Proceedings of the IEEE\nInternational Conference on Computer Vision, pages 7464–\n7473, 2019.\n[56] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking model\nscaling for convolutional neural networks. arXiv preprint\narXiv:1905.11946, 2019.\n[57] Mingxing Tan, Ruoming Pang, and Quoc V Le. Efﬁcient-\ndet: Scalable and efﬁcient object detection. arXiv preprint\narXiv:1911.09070, 2019.\n[58] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. Efﬁcient transformers: A survey. arXiv preprint\narXiv:2009.06732, 2020.\n[59] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive\nmultiview coding. arXiv preprint arXiv:1906.05849, 2019.\n[60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Hervé Jégou. Training\ndata-efﬁcient image transformers and distillation through at-\ntention, 2021.\n[61] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017.\n[62] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. arXiv\npreprint arXiv:2003.07853, 2020.\n[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n7794–7803, 2018.\n[64] Yuxin Wu and Kaiming He. Group normalization. In Pro-\nceedings of the European Conference on Computer Vision\n(ECCV), pages 3–19, 2018.\n[65] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L\nYuille, and Kaiming He. Feature denoising for improving ad-\nversarial robustness. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 501–509,\n2019.\n[66] Saining Xie, Ross Girshick, Piotr Dollár, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492–1500,\n2017.\n[67] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R.\nManmatha, Mu Li, and Alexander Smola. Resnest: Split-\nattention networks, 2020.\n[68] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition, 2020."
}