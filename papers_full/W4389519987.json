{
  "title": "Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection",
  "url": "https://openalex.org/W4389519987",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2077370255",
      "name": "Yi Dai",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2120285716",
      "name": "Hao Lang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2171029730",
      "name": "Kaisheng Zeng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A1936961387",
      "name": "Fei Huang",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2100492699",
      "name": "Yong-Bin Li",
      "affiliations": [
        "Alibaba Group (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4385572696",
    "https://openalex.org/W2797977484",
    "https://openalex.org/W3173026327",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4306808680",
    "https://openalex.org/W4309955399",
    "https://openalex.org/W3175204457",
    "https://openalex.org/W4300980582",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W3133825286",
    "https://openalex.org/W4285429195",
    "https://openalex.org/W4320813149",
    "https://openalex.org/W4385570343",
    "https://openalex.org/W4223977507",
    "https://openalex.org/W4316135748",
    "https://openalex.org/W3181933907",
    "https://openalex.org/W12634471",
    "https://openalex.org/W3204121251",
    "https://openalex.org/W2531327146",
    "https://openalex.org/W4365205411",
    "https://openalex.org/W4296001058",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3102616566",
    "https://openalex.org/W2462906003",
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4316843048",
    "https://openalex.org/W4283819124",
    "https://openalex.org/W3092527263",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4221142320",
    "https://openalex.org/W3175976195",
    "https://openalex.org/W4385571351",
    "https://openalex.org/W4281679115",
    "https://openalex.org/W2047643928",
    "https://openalex.org/W3176851559",
    "https://openalex.org/W4321594373",
    "https://openalex.org/W1797268635",
    "https://openalex.org/W4310271234",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2475167333",
    "https://openalex.org/W4293089123",
    "https://openalex.org/W2965989958",
    "https://openalex.org/W4376865102",
    "https://openalex.org/W2017814585",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4311887664",
    "https://openalex.org/W4385574236",
    "https://openalex.org/W2732026016",
    "https://openalex.org/W2155195660",
    "https://openalex.org/W4386076222",
    "https://openalex.org/W4287724856",
    "https://openalex.org/W3159630167",
    "https://openalex.org/W3205597769"
  ],
  "abstract": "Out-of-distribution (OOD) detection is essential for reliable and trustworthy machine learning. Recent multi-modal OOD detection leverages textual information from in-distribution (ID) class names for visual OOD detection, yet it currently neglects the rich contextual information of ID classes. Large language models (LLMs) encode a wealth of world knowledge and can be prompted to generate descriptive features for each class. Indiscriminately using such knowledge causes catastrophic damage to OOD detection due to LLMs' hallucinations, as is observed by our analysis. In this paper, we propose to apply world knowledge to enhance OOD detection performance through selective generation from LLMs. Specifically, we introduce a consistency-based uncertainty calibration method to estimate the confidence score of each generation. We further extract visual objects from each image to fully capitalize on the aforementioned world knowledge. Extensive experiments demonstrate that our method consistently outperforms the state-of-the-art.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5292â€“5305\nDecember 6-10, 2023 Â©2023 Association for Computational Linguistics\nExploring Large Language Models for Multi-Modal\nOut-of-Distribution Detection\nYi Dai1âˆ—â€ , Hao Lang 2â€ â€¡, Kaisheng Zeng 1, Fei Huang 2, Yongbin Li 2â€¡\n1 Department of Computer Science and Technology, Tsinghua University2 Alibaba Group\n{hao.lang, f.huang, shuide.lyb}@alibaba-inc.com,\n{dai-y21, zks19}@mails.tsinghua.edu.cn\nAbstract\nOut-of-distribution (OOD) detection is essen-\ntial for reliable and trustworthy machine learn-\ning. Recent multi-modal OOD detection lever-\nages textual information from in-distribution\n(ID) class names for visual OOD detection, yet\nit currently neglects the rich contextual infor-\nmation of ID classes. Large language models\n(LLMs) encode a wealth of world knowledge\nand can be prompted to generate descriptive\nfeatures for each class. Indiscriminately using\nsuch knowledge causes catastrophic damage to\nOOD detection due to LLMsâ€™ hallucinations,\nas is observed by our analysis. In this paper, we\npropose to apply world knowledge to enhance\nOOD detection performance through selective\ngeneration from LLMs. Specifically, we intro-\nduce a consistency-based uncertainty calibra-\ntion method to estimate the confidence score\nof each generation. We further extract visual\nobjects from each image to fully capitalize on\nthe aforementioned world knowledge. Exten-\nsive experiments demonstrate that our method\nconsistently outperforms the state-of-the-art.\n1 Introduction\nMachine learning models deployed in the wild of-\nten encounter out-of-distribution (OOD) samples\nthat are not seen in the training phase (Bendale\nand Boult, 2015; Fei and Liu, 2016). A reliable\nmodel should not only obtain high performance on\nsamples from seen distributions, i.e., in-distribution\n(ID) samples, but also accurately detect OOD sam-\nples for caution (Amodei et al., 2016; Boult et al.,\n2019; Dai et al., 2023b). Most existing OOD de-\ntection methods are built upon single-modal in-\nputs, e.g., visual inputs (Hsu et al., 2020; Liu et al.,\n2020) or textual inputs (Zhou et al., 2021; Zhan\net al., 2021). Recently, Esmaeilpour et al. (2022);\nMing et al. (2022a) attempt to tackle multi-modal\nâˆ— Work done while the author was interning at Alibaba.\nâ€  Equal contribution.\nâ€¡ Corresponding author.\nWorld knowledgefromLLMs:\ndescriptive features of a black swan\nIs there a black swan\nwebbed \nfeet black\nfeathers\nlong\nneck\nlong, \nslender \nbill\nFigure 1: World knowledge from large language models\ncan facilitate the detection of visual objects.\nOOD detection problem that explores the seman-\ntic information conveyed in class labels for visual\nOOD detection, relying on large-scale pre-trained\nvision-language models such as CLIP (Radford\net al., 2021).\nIn this paper, we apply world knowledge from\nlarge language models (LLMs) (Petroni et al.,\n2019a) to multi-modal OOD detection by gener-\nating descriptive features for class names (Menon\nand V ondrick, 2023). As illustrated in Figure 1, to\nfind a black swan, look for its long neck, webbed\nfeet, and black feathers. These descriptors provide\nrich additional semantic information for ID classes,\nwhich can lead to a more robust estimation of OOD\nuncertainty (Ming et al., 2022a), i.e., measuring the\ndistance from the visual features of an input to the\nclosest textual features of ID classes.\nHowever, the knowledge encoding of LLMs\nsuch as GPT-3 (Brown et al., 2020) is lossy (Peng\net al., 2023) and tends to hallucinate (Ji et al., 2023),\nwhich can cause damage when applied for OOD\ndetection tasks. As shown in Figure 2, LLMs gener-\nate unfaithful descriptors for class â€œhenâ€, assuming\na featherless head appearing in a hen. Indiscrimi-\nnately employing generated descriptive features to\nmodel ID classes brings noise to the inference pro-\ncess due to LLMsâ€™ hallucinations. Moreover, this\nissue becomes more severe as OOD detection deals\nwith samples in an unbounded feature space (Shen\n5292\net al., 2021). Collisions between OOD samples and\nID classes with augmented descriptors would be\ncommon.\nTo address the challenge mentioned above, we\npropose an approach for selective generation of\nhigh-quality descriptive features from LLMs, while\nabstaining from low-quality unfaithful ones (Ren\net al., 2022). Recent studies show LLMs can pre-\ndict the quality of their outputs, i.e., providing cal-\nibrated confidence scores for each prediction that\naccurately reflects the likelihood of the predicted\nanswer being correct (Kadavath et al., 2022; Si\net al., 2022a). Unfortunately, descriptors of a class\nname generated by LLMs are long-form and struc-\ntured intermediate results for the ultimate OOD\ndetection task, and calibration of LLMs for gener-\nating such long open-ended text (Lee et al., 2022)\nis still in its infancy.\nWe perform uncertainty calibration in LLMs\nby exploring a consistency-based approach (Wang\net al., 2022). We assume if the same correct predic-\ntion is consistent throughout multiple generations,\nthen it could serve as a strong sign that LLMs are\nconfident about the prediction (Si et al., 2022b).\nInstead of computing literal similarity, we define\nconsistency between multiple outputs from LLMs\nfor a given input based on whether they can re-\ntrieve similar items from a fixed set of unlabeled\nimages. Specifically, for each descriptor, we first\nretrieve a subset of images, leveraging the joint\nvision-language representations. Then, we measure\ngeneration consistency by calculating the overlap\nbetween these image subsets.\nTo further capitalize on the world knowledge\nexpressed in descriptors from LLMs, we employ\na general object detector to detect all the candi-\ndate objects (concepts) in an image (Cai et al.,\n2022) and represent them with their predicted\nclass names (Chen et al., 2023b) such as â€œmirrorâ€,\nâ€œchairâ€, and â€œ sinkâ€ (see Figure 5). These visual\nconcepts provide valuable contextual information\nabout an image in the textual space and can po-\ntentially match descriptive features of an ID class\nif the image belongs to that class. Accordingly,\nwe improve our distance metric of input samples\nfrom ID classes by considering the similarity be-\ntween image visual concepts and ID class descrip-\ntive features in language representations. Our key\ncontributions are summarized as follows:\nâ€¢ We apply world knowledge from large lan-\nguage models (LLMs) to multi-modal OOD\ndetection for the first time by generating de-\nscriptive features for ID class names.\nâ€¢ We analyse LLMsâ€™ hallucinations which can\ncause damage to OOD detection. A selective\ngeneration framework is introduced and an\nuncertainty calibration method in LLMs is\ndeveloped to tackle the hallucination issue.\nâ€¢ We detect objects in an image and represent\nthem with their predicted class names to fur-\nther explore world knowledge from LLMs.\nOur extensive experimentation on various\ndatasets shows that our method consistently\noutperforms the state-of-the-art.\n2 Related Work\nOOD Detection is widely investigated in vi-\nsion classification problems (Yang et al., 2021),\nand also in text classification problems (Lang\net al., 2023). Existing approaches try to improve\nthe OOD detection performance by logits-based\nscores (Hendrycks and Gimpel, 2017a; Liu et al.,\n2020), distance-based OOD detectors (Lee et al.,\n2018; Sun et al., 2022), robust representation learn-\ning (Winkens et al., 2020; Zhou et al., 2021), and\ngenerated pseudo OOD samples (Shu et al., 2021;\nLang et al., 2022).\nMulti-modal OOD detection is recently studied\nby Fort et al. (2021a); Esmaeilpour et al. (2022);\nMing et al. (2022a), which leverages textual infor-\nmation for visual OOD detection. These works do\nnot explore world knowledge from LLMs.\nLarge language models like GPT3 (Brown\net al., 2020) can serve as a knowledge base and\nhelp various tasks (Petroni et al., 2019b; Dai et al.,\n2023a). While some works demonstrate that world\nknowledge from LLMs can provide substantial aid\nto vision tasks (Yang et al., 2022) such as vision\nclassification (Menon and V ondrick, 2023), its ef-\nficacy in multi-modal OOD detection is currently\nunderexplored. Moreover, as LLMs tend to halluci-\nnate and generate unfaithful facts (Ji et al., 2023),\nadditional effects are needed to explore LLMs ef-\nfectively.\nUncertainty Calibration provides confidence\nscores for predictions to safely explore LLMs,\nhelping users decide when to trust LLMs outputs.\nRecent studies examine calibration of LLMs in\nmultiple-choice and generation QA tasks (Kada-\nvath et al., 2022; Si et al., 2022a; Kuhn et al., 2023).\nIn multi-modal OOD detection task, open-ended\n5293\nClassification(â†‘) OOD Detection( â†“)\nID dataset CLIP CLIP+Desp. CLIP CLIP+Desp.\nImageNet-1k 64.05 68.03 42.74 48.99\nCUB-200 56.35 57.75 7.09 4.72\nStanford-Cars 61.56 63.26 0.08 0.10\nFood-101 85.61 88.50 1.86 3.86\nOxford-Pet 81.88 86.92 1.70 3.52\nTable 1: Effect of class name descriptors from LLMs\nin classification and OOD detection tasks. CLIP and\nCLIP+Desp. are VLMs based methods without and\nwith descriptors. Classification is evaluated by accuracy\nand OOD detection is evaluated by FPR95 (averaged on\niNaturalist, SUN, Places, Texture OOD datasets).\ntext (Lee et al., 2022) are generated to provide de-\nscriptive features for ID classes (Menon and V on-\ndrick, 2023), and calibration in this task is yet un-\nderexplored.\n3 Background\n3.1 Problem Setup\nWe start by formulating the multi-modal OOD de-\ntection problem, following Ming et al. (2022a). We\ndenote the input and label space by Xand Y, re-\nspectively. Yis a set of class labels/names refer-\nring to the known ID classes. The goal of OOD\ndetection is to detect samples that do not belong\nto any of the known classes or assign a test sam-\nple to one of the known classes. We formulate the\nOOD detection as a binary classification problem:\nG(x; Y,I,T) : X â†’{0,1}, where x âˆˆX de-\nnotes an input image, Iand T are image encoder\nand text encoder from pre-trained vision-language\nmodels (VLMs), respectively. The joint vision-\nlanguage embeddings of VLMs associate objects\nin visual and textual modalities well. Note that\nthere is no training data of ID samples provided to\ntrain the OOD detector.\n3.2 Analyzing Class Name Descriptors from\nLLMs\nRecent work has demonstrated that class name de-\nscriptors, i.e., descriptive features for distinguish-\ning a known object category in a photograph gen-\nerated by prompting LLMs (see Section 4.2 for\nmore details), can improve zero-shot visual classi-\nfication performance (Menon and V ondrick, 2023)\nin a close-world setting (Vapnik, 1991). A natural\nextension of this work is to leverage the descriptors\nfor OOD detection in an open world (Fei and Liu,\n2016), which is largely unexplored.\nUnfortunately, we find that the descriptors used\nhen\nÂ· featherless head\nÂ· round body shape\nÂ· yellow beak\nÂ· webbed red-orange legs\ngoldfish\nÂ· long, flowing tail\nÂ· small, black eyes\nÂ·a small mouth\nÂ· dorsal fins\nFigure 2: Cases of descriptors generated by LLMs. Un-\nfaithful descriptors may appear due to hallucinations.\nin previous approach fail to improve the OOD de-\ntection performance in a few datasets. As shown\nin Table 1, although descriptors can improve the\nclassification performance in all five datasets, they\ndegenerate the OOD detection performance in four\nID datasets. We hypothesize this is because LLMs\ngenerate unfaithful descriptors due to hallucina-\ntions (see cases in Figure 2), which bring noise to\nthe OOD detection process.\nTo verify our hypothesis, we visualize ID sam-\nples from ImageNet-1k dataset and OOD samples\nfrom iNaturalist dataset, together with their origi-\nnal class names, based on aligned vision-language\nfeatures (Radford et al., 2021). As illustrated in\nFigure 3(b), class names of ID samples may coin-\ncide with these of OOD samples, when augmented\nwith descriptors from LLMs. Thus, it is improper\nto indiscriminately adopt these descriptors.\nThe above assumptions are also evidenced in Fig-\nure 4. ID samples obtain higher similarity scores\nto their classes when augmented with descriptors\n(Figure 4(a)), which show the effect of descriptors\nfor the classification task. Meanwhile, OOD sam-\nples gain larger maximum similarity scores with\nID classes with descriptors (Figure 4(b)), which\nmakes OOD detection more difficult.\n4 Method\n4.1 Overview\nIn this study, we build the multi-modal OOD de-\ntector following four steps: 1. Generate a set of\ndescriptors d(c) for each class name c âˆˆ Yby\nprompting LLMs; 2. Estimate a confidence score\nfor descriptors d(c) with uncertainty calibration;\n3. Detect visual objects for each test image x; 4.\nBuild an OOD detector with selective generation\nof descriptors and image visual objects. Figure 5\nshows an overview of our approach.\n5294\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n(a)\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ (b)\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ (c)\nFigure 3: t-SNE visualization of ID and OOD samples, which are from ImageNet-1k and iNaturalist datasets,\nrespectively. ID/OOD Image represent visual representations of images, and ID/OOD Text are textual representations\nof their class names, based on CLIP. Each line denotes a pair of vision-language representations for one sample.\nï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\n(a)\nï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ (b)\nFigure 4: Left: Similarities between ID samples and\ntheir class names; Right: Maximum similarities be-\ntween OOD samples and ID classes. ID samples from\nmageNet-1K and OOD samples from Naturalist.\n4.2 Descriptor Generation\nTo apply world knowledge from LLMs for OOD\ndetection, we generate a set of descriptors d(c)\nfor each known class name cby prompting LLMs\n(see Figure 6), following (Menon and V ondrick,\n2023). We randomly select 1 visual category and\nmanually compose descriptors to use as 1-shot in-\ncontext example. We prompt LLMs to describe the\nvisual features for distinguishing a category in a\nphotograph. The generated list composes the set\nd(c). Figure 2 shows cases of generated descrip-\ntors, which include shape, size, color, and object\nparts in natural language.\n4.3 Uncertainty Calibration\nAs Figure 2 illustrates, LLMs may generate unfaith-\nful descriptors due to hallucinations, which would\nhurt the performance of OOD detection if applied\nindiscriminately. To address this issue, we design a\nconsistency-based (Wang et al., 2022) uncertainty\ncalibration method to estimate a confidence score\nfor each generation, which helps decide when to\ntrust the LLMs outputs. We assume if the same\ncorrect prediction is consistent throughout multiple\ngenerations, then it shows that LLMs are confident\nabout the prediction (Si et al., 2022b), thus the\ngeneration results are more trustworthy.\nIt is non-trivial to directly extend previous\nconsistency-based methods to our settings. Specifi-\ncally, we leverage LLMs to generate long-form and\nstructured descriptor lists without a fixed candidate\nanswer set. Any permutation of the descriptor list\ncan convey the same meaning, despite the incon-\nsistency in surface form. Meanwhile, the outputs\nof LLMs are intermediate results for our OOD de-\ntection task. It is challenging to measure quality\nof these intermediate results while maintaining a\ntangible connection to the ultimate detection task.\nWe take inspiration from prior code generation\nworks (Li et al., 2022; Chen et al., 2023a), which\nprompt LLMs to generate structured codes aiming\nat solving programming tasks. Multiple codes are\nsampled from LLMs for one input and then execu-\ntion results are obtained by executing these codes.\nThe code with the most frequent execution result is\nselected as the final prediction. In a similar manner,\nwe quantify the characteristics of descriptor sets\nthrough their retrieval feedbacks from a fixed set\nof unlabeled images, and define their consistency\naccording to consensus among retrieval results.\nSpecifically, we propose a three-stage consistency-\nbased uncertainty calibration method.\n5295\nIDÂ classesÂ Y\nÂ·Â aÂ barberÂ chair\nÂ·Â mirrors\nÂ·Â aÂ sink\nÂ·Â roundÂ shape\nÂ·Â brownÂ leather\nÂ·Â weight ofÂ \n20Â ouncesÂ \nÂ·Â raisedÂ laces\nbasketball\nwhichÂ has\nÂ·Â aÂ barberÂ chair\nÂ·Â mirrors\nÂ·Â aÂ sink\nWhatÂ areÂ usefulÂ \nfeaturesÂ forÂ \ndistinguishingÂ aÂ \n{classÂ name}Â inÂ \naÂ photo?\nÂ·Â \nbarberÂ shop\nâ€¦Â â€¦Â \nÂ·Â \nbasketball\n barberÂ shop\nbasketball\n1.Bicycle\n2.Bag\nPrompt\nTestÂ images\n1.Mirror\n2.Chair\n3.Sink\nbarberÂ shop\nGeneralÂ ObjectÂ DetectorLargeÂ LanguageÂ Model\n1.Â DescriptorÂ Generation 2.Â UncertaintyÂ Calibration 3.Â ObjectÂ Detection\nsmax(x)\nIDÂ image\n(barberÂ shop)\nOODÂ image\nâ‰¥ ğœ†\n<ğœ†\np(c)=0.8\np(c)=0.4\nâ€¦Â â€¦\nVisionÂ­LanguageÂ Model\np(c)â‰¥ ğ›¾\np(c)Â <ğ›¾\n4.Â OODÂ Detection\nFigure 5: Our multi-modal OOD detection framework. For each image x and ID classes Y, 1. Generate a descriptor\nset for each class câˆˆY by prompting LLMs; 2. Estimate a confidence score p(c) for each descriptor set; 3. Detect\nobjects in x and represent them with object names; 4. Compute the maximum class matching score smax(x).\nQ:What are useful visual features for \ndistinguishing a lemurin a photo? \nA:There are several useful visual features\nto tell there is a lemurin a photo: \n- furry bodies\n- long tail \n- large eyes\nQ:What are useful visual features for \ndistinguishing a goldfishin a photo?\nA:There are several useful visual features\nto tell there is a goldfishin a photo:\n- long, flowing tail\n- small, black eyes\n- a small mouth\n- dorsal fins\nğ‘„prompt\nAprompt\nğ‘„test\nAtest\nFigure 6: Example prompt for generating descriptors of\nthe category goldfish. The trailing â€˜-â€™ guides LLMs to\ngenerate text in the form of a bulleted list.\nStage I. We sample nsets of descriptors D(c) =\n{d1(c),Â·Â·Â· ,dn(c)}from LLMs for each ID class\nname câˆˆY.\nStage II. We cluster descriptor sets D(c) into\ngroups S(c), where each group s âˆˆS(c) is com-\nprised of descriptor sets that consist with each other.\nWe define descriptor consistency,C(Â·,Â·), which re-\ntains any two descriptor sets that share the same\ncharacteristics through retrieval feedback. Con-\ncretely, we retrieve top k images from an unla-\nbeled image set Mvia a retriever R(Â·) for each set\nd âˆˆD(c). The resulting image subset for descrip-\ntor set d is denoted as R(d) âˆˆ{0,1}m, where m\nis the size of Mand entry jof the vector is 1 if the\nj-th image of Mis in the retrieved subset. Finally,\nwe assume descriptor consistency C(d,dâ€²) holds if\ncosine similarity between R(d) and R(dâ€²) is above\nÎ·. Note that text similarity between descriptor sets\ncan also be used in consistency computation.\nStage III. We compute the confidence score p(c)\nfor descriptor set d(c) as |sâˆ—|\nn , where sâˆ— is the\nlargest group in S(c).\n4.4 Visual Object Detection\nTo further capitalize on the world knowledge con-\nveyed in generated descriptors, we introduce a gen-\neral object detector with a vocabulary of600 object\ncategories to detect visual objects v(x) for each\ntesting image x (Cai et al., 2022). Specifically,\nv(x) consists of detected objectsâ€™ class names,\nsuch as â€œ mirrorâ€, â€œ chairâ€, and â€œ sinkâ€ in a pho-\ntograph of a barber shop (see Figure 5).\n4.5 OOD Detection\nFor each ID class name c, descriptor set d(c) is\nused to augment the representation of cif its con-\nfidence score p(c) is above threshold Î³, otherwise\nc is used to represent that class only. Thus, the\ntextual features for class name care:\nt(c) =\n{{g(d)|dâˆˆd(c)}, if p(c) â‰¥Î³,\n{c}, otherwise,\nwhere dis one descriptor in the set d(c) and g(Â·)\ntransforms dinto the form {c} which has {d}.\nFor an input imagex, we calculate the class-wise\nmatching score for each ID class name câˆˆY:\nsc(x) = E\ntâˆˆt(c)\nÏƒ(I(x),T(t)) + E\nvâˆˆv(x)\ntâˆˆt(c)\nÏƒ(T(v),T(t)),\n(1)\nwhere Ïƒ(Â·,Â·) denotes the cosine similarity func-\ntion, the left term computes the similarity between\nimage visual representations and class name tex-\ntual representations, and the right term measures\n5296\nthe similarity between detected image objects and\nclass names in the text space.\nLastly, we define the maximum class\nmatching score as: smax(x; Y,I,T) =\nmaxc\nexp(sc(x))âˆ‘\ncâ€²âˆˆYexp(scâ€²(x)) , similar to Ming et al.\n(2022a). Our OOD detection function can be\ndefined as:\nG(x; Y,I,T) =\n{\n1 smax(x; Y,I,T) â‰¥Î»\n0 smax(x; Y,I,T) <Î» , (2)\nwhere 1 represents ID class and 0 indicates OOD\nconventionally. Î»is a chosen threshold.\n5 Experiments\n5.1 Datasets and Metrics\nDatasets Following recent works (Ming et al.,\n2022a), we use large-scale datasets that are more\nrealistic and complex. We consider the following\nID datasets: variants of ImageNet (Deng et al.,\n2009), CUB-200 (Wah et al., 2011), Stanford-\nCars (Krause et al., 2013), Food-101 (Bossard et al.,\n2014), Oxford-Pet (Parkhi et al., 2012). For OOD\ndatasets, we use iNaturalist (Van Horn et al., 2018),\nSUN (Xiao et al., 2010), Places (Zhou et al., 2017),\nand Texture (Cimpoi et al., 2014).\nMetrics For evaluation, we use these metrics (1)\nthe false positive rate ( FPR95) of OOD samples\nwhen the true positive rate of ID samples is at 95%,\n(2) the area under the receiver operating character-\nistic curve (AUROC).\n5.2 Implementation Details\nIn our experiments, we adopt CLIP (Radford et al.,\n2021) as the pre-trained vision-language model.\nSpecifically, we mainly use CLIP-B/16 (CLIP-B),\nwhich consists of a ViT-B/16 Transformer as the\nimage encoder and a masked self-attention Trans-\nformer (Vaswani et al., 2017) as the text encoder.\nWe also use CLIP-L/14 (CLIP-L) as a represen-\ntative of large models. To generate descriptors,\nwe query text-davinci-003 (Ouyang et al., 2022)\nwith sampling temperature T = 0.7 and maxi-\nmum token length of 100. We construct the un-\nlabeled image set Mthrough the random selection\nof m = 50000 images from the training set of\nImageNet. The retriever R(Â·) retrieves k= 50im-\nages from M. We set the threshold Î· = 0.9 and\nÎ³ = 0.5. In visual object detection, we employ the\nobject detection model CBNetV2-Swin-Base (Cai\net al., 2022) as a general object detector with a\nvocabulary of 600 objects. See more details in\nAppendix B.\n5.3 Baselines\nWe compared our method with competitive base-\nlines: 1. MOS (Huang and Li, 2021) divides ID\nclasses into small groups with similar concepts to\nimprove OOD detection; 2. Fort et al. (Fort et al.,\n2021b) finetunes a full ViT model pre-trained on\nthe ID dataset; 3. Energy (Liu et al., 2020) pro-\nposes a logit-based score to detect OOD samples;4.\nMSP (Hendrycks and Gimpel, 2017b) employs the\nmaximum classification probability of samples to\nestimate OOD uncertainty; 5. MCM (Ming et al.,\n2022a) estimates OOD uncertainty with the maxi-\nmum similarity between the embeddings of a sam-\nple and ID class names; 6. Menon et al. (Menon\nand V ondrick, 2023) prompts LLMs to generate\ndescriptors of each class as cues for image classifi-\ncation. We extend it to OOD detection and use the\nmaximum classification probability as a measure of\nOOD uncertainty (Hendrycks and Gimpel, 2017b).\n5.4 Main Results\nTo evaluate the scalability of our method in real-\nworld scenarios, we compare it with recent OOD\ndetection baselines on the ImageNet-1k dataset\n(ID) in Table 2. It can be seen that our method out-\nperforms all competitive zero-shot methods. Com-\npared with the best-performing zero-shot baseline\nMCM, it reduces FPR95 by 5.03%. We can also\nobserve that: 1. Indiscriminately employing knowl-\nedge from LLMs (i.e., Menon et al.) degenerates\nthe OOD detection performance. This indicates the\nadverse impact of LLMsâ€™ hallucinations and under-\nlines the importance of selective generation from\nLLMs. 2. Despite being training-free, our method\nfavorably matches or even outperforms some strong\ntask-specific baselines that require training (e.g.,\nMOS). It shows the advantage of incorporating\nworld knowledge from LLMs for OOD detection.\nWe further evaluate the effectiveness of our\nmethod on hard OOD inputs. Specifically, two\nkinds of hard OOD are considered, i.e., semanti-\ncally hard OOD (Winkens et al., 2020) and spurious\nOOD (Ming et al., 2022b). As shown in Table 3,\nour method exhibits robust OOD detection capa-\nbility and outperforms all competitive baselines,\ne.g., improvement of 1.93% in FPR95 compared to\nthe best-performing baseline MCM. We can also\nobserve that zero-shot methods generally obtain\nhigher performance than task-specific baselines.\n5297\nMethod\nOOD Dataset AverageiNaturalist SUN Places Texture\nFPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘\nTask-specific (training required)\nMOS (BiT) 9.28 98.15 40.63 92.01 49.54 89.06 60.43 81.23 39.97 90.11\nFort et al.(ViT-B) 15.07 96.64 54.12 86.37 57.99 85.24 53.32 84.77 45.12 88.25\nFort et al.(ViT-L) 15.74 96.51 52.34 87.32 55.14 86.48 51.38 85.54 43.65 88.96\nEnergy (CLIP-B) 21.59 95.99 34.28 93.15 36.64 91.82 51.18 88.09 35.92 92.26\nEnergy (CLIP-L) 10.62 97.52 30.46 93.83 32.25 93.01 44.35 89.64 29.42 93.50\nMSP (CLIP-B) 40.89 88.63 65.81 81.24 67.90 80.14 64.96 78.16 59.89 82.04\nMSP (CLIP-L) 34.54 92.62 61.18 83.68 59.86 84.10 59.27 82.31 53.71 85.68\nZero-shot (no training required)\nMCM (CLIP-B) 30.91 94.61 37.59 92.57 44.69 89.77 57.77 86.11 42.74 90.77\nMCM (CLIP-L) 28.38 94.95 29.00 94.14 35.42 92.00 59.88 84.88 38.17 91.49\nMenon et al. (CLIP-B) 41.23 92.09 44.12 91.72 49.74 88.91 60.89 85.07 48.99 89.45\nMenon et al. (CLIP-L) 33.26 93.92 30.29 94.18 37.30 91.78 59.82 84.40 40.17 91.07\nw/o Obj. (CLIP-B) 23.67 95.40 37.19 92.57 43.97 89.77 56.97 86.33 40.45 91.02\nw/o Obj. (CLIP-L) 28.20 95.22 27.81 94.44 33.22 91.56 56.37 86.05 36.40 91.82\nw/o Calib. (CLIP-B) 48.30 90.53 41.17 92.11 45.08 89.61 56.91 87.01 47.87 89.81\nw/o Calib. (CLIP-L) 40.41 92.09 29.90 94.00 35.99 91.08 52.93 87.17 39.81 91.09\nw/o Know. (CLIP-B) 30.19 94.81 37.39 91.56 43.63 89.76 57.30 86.17 42.13 90.58\nw/o Know. (CLIP-L) 28.66 94.88 33.25 93.70 40.00 91.31 59.09 85.41 40.26 91.33\nOurs (CLIP-B) 22.88 95.54 34.29 92.60 41.63 89.87 52.02 87.71 37.71 91.43\nOurs (CLIP-L) 26.47 95.10 26.35 94.56 33.13 91.77 51.77 87.45 34.43 92.22\nTable 2: OOD detection performance for ImageNet-1k as ID. The performances of all task-specific baselines come\nfrom Ming et al. (2022a).\nMethod\nID ImageNet-10 ImageNet-20 Waterbirds AverageOOD ImageNet-20 ImageNet-10 Spurious OOD\nFPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘\nTask-specific (training required)\nMOS (BiT) 24.60 95.30 41.80 92.63 78.21 78.60 48.20 88.84\nFort et al. (ViT-B) 8.14 98.07 11.71 98.08 4.63 98.57 8.16 98.24\nEnergy (CLIP-B) 15.23 96.87 15.20 96.90 41.51 89.30 23.98 94.36\nMSP (CLIP-B) 9.38 98.31 12.51 97.70 39.57 90.99 20.49 95.67\nZero-shot (no training required)\nMCM (CLIP-B) 5.00 98.31 12.91 98.09 5.87 98.36 7.93 98.25\nMenon et al. (CLIP-B) 5.80 98.65 13.09 98.08 5.57 98.45 8.15 98.39\nw/o Obj. (CLIP-B) 4.70 98.71 10.78 98.25 4.88 98.46 6.79 98.47\nw/o Calib. (CLIP-B) 6.00 98.50 11.14 98.04 5.17 98.49 7.44 98.34\nw/o Know. (CLIP-B) 5.00 98.73 11.38 98.22 4.86 98.39 7.08 98.45\nOurs (CLIP-B) 4.20 98.77 9.24 98.26 4.56 98.62 6.00 98.55\nTable 3: Performance comparison on hard OOD detection tasks.\nThis indicates that exposing a model to a training\nset may suffer from bias and spurious correlations.\nWe also make comparisons on a larger number of\nID and OOD datasets in Appendix A.\n5.5 Ablation Studies\nModel Components Ablation studies are carried\nout to validate the effectiveness of each main com-\nponent in our model. Specifically, the following\nvariants are investigated: 1. w/o Obj. removes the\nvisual object detection step, i.e., only the left term\nin Eq. 1 is adopted. 2. w/o Calib. removes the\nuncertainty calibration step and indiscriminately\nuses descriptors from LLMs. 3. w/o Know. only\nuses class names to represent each class without\ndescriptors from LLMs. Results in Table 2 and\nTable 3 show that our method outperforms all the\nabove variants. Specifically, we can observe that:\n1. Incorporating knowledge from LLMs (see w/o\nKnow.) improves the OOD detection performance\nby 4.42%. This verifies that world knowledge is\nimportant in multi-modal OOD detection. 2. Both\nuncertainty calibration (see w/o Calib.) and visual\nobject detection (see w/o Obj.) help to improve the\nOOD detection performance.\nUncertainty Calibration To evaluate our pro-\nposed uncertainty calibration method, we perform\nablation on alternatives: 1. Confidence (Si et al.,\n2022a) leverages language model probabilities of\n5298\nCategories Variants FPR95â†“ AUROCâ†‘\nUncertainty\nCalibration\nConfidence 40.81 91.16\nSelf-consistency 41.64 90.93\nSelf-evaluation 40.83 90.82\nVisual Object\nDetection\nClass Sim. 38.55 91.05\nSimple Det. 39.40 90.97\nOurs 37.71 91.43\nTable 4: Ablation variants of uncertainty calibration and\nvisual object detection. We use the average performance\non four OOD datasets with ImageNet-1k as ID.\ngenerated descriptors as the confidence score. 2.\nSelf-consistency (Wang et al., 2022) makes multi-\nple predictions for one input and makes use of the\nfrequency of the majority prediction as the confi-\ndence score. 3. Self-evaluation (Kadavath et al.,\n2022) asks LLMs to first propose answers and then\nestimate the probability that these answers are cor-\nrect. Results in Table 4 show that our uncertainty\ncalibration method performs better than other vari-\nants. This further indicates that dedicated uncer-\ntainty calibration approaches should be explored to\nsafely explore generations from LLMs.\nVisual Object Detection We evaluate the visual\nobject detection module by implementing the fol-\nlowing variants: 1. Class Sim. uses class name c\ninstead of the descriptive features t(c) in the right\nterm of Eq. 1. 2. Simple Det. adopts a simple\nobject detection model with a smaller vocabulary\nof 80 objects (Li et al., 2023). As shown in Table 4,\nour method outperforms the above variants. Specif-\nically, we can observe that: 1. Calculating the\nsimilarity between detected image concept names\nand ID class names without descriptors degenerates\nthe OOD detection performance. 2. Using a gen-\neral object detection model with a large vocabulary\nof objects helps to improve the performance.\n5.6 Further Analysis\nCases of Retrieval Feedback We provide a case\nstudy where descriptor sets for the same class are\nsimilar/dissimilar in textual form. Figure 7 illus-\ntrates that even with low textual similarity and vari-\nations in textual form, two descriptor sets can have\nconsistent retrieval feedback if they accurately cap-\nture the descriptive features of the same object.\nAnalysis of Unlabeled Image Set Figure 8\nshows the effect of unlabeled image set with vary-\ning sizes on the OOD detection performance. We\ncompose image subsets either through random\nâ€¢black fur\nâ€¢white fur on \nthe head and                 \nc chest\nâ€¢furry bodies\nâ€¢prominent lips\nâ€¢wet and \nhairless nose \nâ€¢white or light \ngrey head\nâ€¢furry bodies\nâ€¢clawed hands\nâ€¢barber chair\nâ€¢barber pole\nâ€¢sinks\nâ€¢mirrors\nâ€¢shelves of \nproducts\nâ€¢barber chair\nâ€¢barber pole\nâ€¢sinks\nâ€¢mirrors\nâ€¢shelves\nwithin top-5 retrieved \nimages for set #1 and #2\nTextual Similarity\n1.00\nRetrieval Feedback\nTextual Similarity\n0.94\nRetrieval Feedback\n0.98\nSet #1 Set #2\nSet #1 Set #2\n0.98\nFigure 7: Case study on descriptor sets and correspond-\ning retrieval feedbacks.\nï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ ï¿½ï¿½ï¿½\nï¿½ ï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½\nï¿½ ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nFigure 8: Analysis of unlabeled image set.\ndown-sampling from the original unlabeled image\nset (â€œRandomâ€), or removing images from certain\ncategories (â€œDiversityâ€). We can observe that: 1.\nOur method achieves superior OOD detection per-\nformance along with the increase of unlabeled im-\nage data. 2. Unlabeled image sets that lack diver-\nsity achieve limited detection performance, espe-\ncially in small sizes.\n6 Conclusion\nIn this paper, we introduce a novel framework for\nmulti-modal out-of-distribution detection. It em-\nploys world knowledge from large language mod-\nels (LLMs) to characterize ID classes. An uncer-\ntainty calibration method is introduced to tackle the\nissue of LLMsâ€™ hallucinations, and visual object\ndetection is proposed to fully capitalize on the gen-\nerated world knowledge. Experiments on a variety\nof OOD detection tasks show the effectiveness of\nour method, demonstrating its exciting property\nwhere world knowledge can be reliably exploited\n5299\nvia LLMs by evaluating their uncertainty.\nLimitations\nWe identify one major limitation of this work is\nits input modality. Specifically, our method is lim-\nited to detecting visual out-of-distribution (OOD)\ninputs and ignores inputs in other modalities such\nas textual, audio, electroencephalogram (EEG) and\nrobotic features. These modalities provide valuable\ninformation that can be used to construct better\nOOD detectors. Fortunately, through multi-modal\npre-training models (Xu et al., 2021; Huo et al.,\n2021), we can obtain robust representations in vari-\nous modalities.\nEthics Statement\nThis work does not raise any direct ethical issues.\nIn the proposed work, we seek to develop a zero-\nshot multi-modal OOD detection model equipped\nwith world knowledge from LLMs, and we believe\nthis work can benefit the field of OOD detection,\nwith the potential to benefit other fields requiring\ntrustworthy models. All experiments are conducted\non open datasets.\nReferences\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul\nChristiano, John Schulman, and Dan ManÃ©. 2016.\nConcrete problems in ai safety. arXiv preprint\narXiv:1606.06565.\nAbhijit Bendale and Terrance Boult. 2015. Towards\nopen world recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 1893â€“1902.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\n2014. Food-101â€“mining discriminative components\nwith random forests. In Computer Visionâ€“ECCV\n2014: 13th European Conference, Zurich, Switzer-\nland, September 6-12, 2014, Proceedings, Part VI 13,\npages 446â€“461. Springer.\nTerrance E Boult, Steve Cruz, Akshay Raj Dhamija,\nManuel Gunther, James Henrydoss, and Walter J\nScheirer. 2019. Learning and the unknown: Survey-\ning steps toward open world recognition. In Proceed-\nings of the AAAI conference on artificial intelligence,\nvolume 33, pages 9801â€“9807.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877â€“1901.\nLikun Cai, Zhi Zhang, Yi Zhu, Li Zhang, Mu Li, and\nXiangyang Xue. 2022. Bigdetection: A large-scale\nbenchmark for improved object detector pre-training.\nIn Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 4777â€“\n4787.\nXinyun Chen, Maxwell Lin, Nathanael SchÃ¤rli, and\nDenny Zhou. 2023a. Teaching large language models\nto self-debug. arXiv preprint arXiv:2304.05128.\nZhenfang Chen, Qinhong Zhou, Yikang Shen, Yining\nHong, Hao Zhang, and Chuang Gan. 2023b. See,\nthink, confirm: Interactive prompting between vision\nand language models for knowledge-based visual\nreasoning. arXiv preprint arXiv:2301.05226.\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos,\nSammy Mohamed, and Andrea Vedaldi. 2014. De-\nscribing textures in the wild. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 3606â€“3613.\nYi Dai, Hao Lang, Yinhe Zheng, Fei Huang, and Yong-\nbin Li. 2023a. Long-tailed question answering in an\nopen world. arXiv preprint arXiv:2305.06557.\nYi Dai, Hao Lang, Yinhe Zheng, Bowen Yu, Fei Huang,\nand Yongbin Li. 2023b. Domain incremental life-\nlong learning in an open world. arXiv preprint\narXiv:2305.06555.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE conference\non computer vision and pattern recognition , pages\n248â€“255. Ieee.\nSepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei\nShu. 2022. Zero-shot out-of-distribution detection\nbased on the pre-trained model clip. In Proceedings\nof the AAAI conference on artificial intelligence, vol-\nume 36, pages 6568â€“6576.\nGeli Fei and Bing Liu. 2016. Breaking the closed world\nassumption in text classification. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 506â€“514.\nStanislav Fort, Jie Ren, and Balaji Lakshminarayanan.\n2021a. Exploring the limits of out-of-distribution de-\ntection. In Advances in Neural Information Process-\ning Systems, volume 34, pages 7068â€“7081. Curran\nAssociates, Inc.\nStanislav Fort, Jie Ren, and Balaji Lakshminarayanan.\n2021b. Exploring the limits of out-of-distribution\ndetection. In Conference on Neural Information Pro-\ncessing Systems (NeurIPS).\nDan Hendrycks and Kevin Gimpel. 2017a. A baseline\nfor detecting misclassified and out-of-distribution ex-\namples in neural networks. Proceedings of the Inter-\nnational Conference on Learning Representations.\n5300\nDan Hendrycks and Kevin Gimpel. 2017b. A baseline\nfor detecting misclassified and out-of-distribution ex-\namples in neural networks. In International Confer-\nence on Learning Representations (ICLR).\nYen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt\nKira. 2020. Generalized odin: Detecting out-of-\ndistribution image without learning from out-of-\ndistribution data. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 10951â€“10960.\nRui Huang and Yixuan Li. 2021. Mos: Towards scaling\nout-of-distribution detection for large semantic space.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 8710â€“8719.\nYuqi Huo, Manli Zhang, Guangzhen Liu, Haoyu Lu,\nYizhao Gao, Guoxing Yang, Jingyuan Wen, Heng\nZhang, Baogui Xu, Weihao Zheng, Zongzheng Xi,\nYueqian Yang, Anwen Hu, Jinming Zhao, Ruichen Li,\nYida Zhao, Liang Zhang, Yuqing Song, Xin Hong,\nWanqing Cui, Dan Yang Hou, Yingyan Li, Junyi\nLi, Peiyu Liu, Zheng Gong, Chuhao Jin, Yuchong\nSun, Shizhe Chen, Zhiwu Lu, Zhicheng Dou, Qin\nJin, Yanyan Lan, Wayne Xin Zhao, Ruihua Song,\nand Ji-Rong Wen. 2021. Wenlan: Bridging vision\nand language by large-scale multi-modal pre-training.\nCoRR, abs/2103.06561.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput-\ning Surveys, 55(12):1â€“38.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield Dodds, Nova DasSarma,\nEli Tran-Johnson, et al. 2022. Language models\n(mostly) know what they know. arXiv preprint\narXiv:2207.05221.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-\nFei. 2013. 3d object representations for fine-grained\ncategorization. In Proceedings of the IEEE inter-\nnational conference on computer vision workshops,\npages 554â€“561.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023.\nSemantic uncertainty: Linguistic invariances for un-\ncertainty estimation in natural language generation.\narXiv preprint arXiv:2302.09664.\nHao Lang, Yinhe Zheng, Yixuan Li, Jian Sun, Fei\nHuang, and Yongbin Li. 2023. A survey on out-\nof-distribution detection in nlp. arXiv preprint\narXiv:2305.03236.\nHao Lang, Yinhe Zheng, Jian Sun, Fei Huang, Luo\nSi, and Yongbin Li. 2022. Estimating soft labels\nfor out-of-domain intent detection. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 261â€“276, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin.\n2018. A simple unified framework for detecting out-\nof-distribution samples and adversarial attacks. Ad-\nvances in neural information processing systems, 31.\nNayeon Lee, Wei Ping, Peng Xu, Mostofa Patwary, Pas-\ncale N Fung, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2022. Factuality enhanced language models\nfor open-ended text generation. Advances in Neural\nInformation Processing Systems, 35:34586â€“34599.\nChuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng\nCheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and\nXiangxiang Chu. 2023. Yolov6 v3. 0: A full-scale\nreloading. arXiv preprint arXiv:2301.05586.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman,\nJulian Schrittwieser, RÃ©mi Leblond, Tom Eccles,\nJames Keeling, Felix Gimeno, Agustin Dal Lago,\net al. 2022. Competition-level code generation with\nalphacode. Science, 378(6624):1092â€“1097.\nWeitang Liu, Xiaoyun Wang, John Owens, and Yixuan\nLi. 2020. Energy-based out-of-distribution detection.\nProceedings of the Advances in Neural Information\nProcessing Systems.\nSachit Menon and Carl V ondrick. 2023. Visual classifi-\ncation via description from large language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nYifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei\nLi, and Yixuan Li. 2022a. Delving into out-of-\ndistribution detection with vision-language represen-\ntations. arXiv preprint arXiv:2211.13445.\nYifei Ming, Hang Yin, and Yixuan Li. 2022b. On the\nimpact of spurious correlation for out-of-distribution\ndetection. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 10051â€“\n10059.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730â€“27744.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman,\nand CV Jawahar. 2012. Cats and dogs. In 2012\nIEEE conference on computer vision and pattern\nrecognition, pages 3498â€“3505. IEEE.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, et al. 2023. Check your facts and\ntry again: Improving large language models with\nexternal knowledge and automated feedback. arXiv\npreprint arXiv:2302.12813.\nFabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019a. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\n5301\non Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463â€“2473.\nFabio Petroni, Tim RocktÃ¤schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019b. Language models as knowl-\nedge bases? In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463â€“2473, Hong Kong, China. Association\nfor Computational Linguistics.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning , volume 139 of\nProceedings of Machine Learning Research, pages\n8748â€“8763. PMLR.\nJie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mo-\nhammad Saleh, Balaji Lakshminarayanan, and Pe-\nter J Liu. 2022. Out-of-distribution detection and\nselective generation for conditional language models.\narXiv preprint arXiv:2209.15558.\nYilin Shen, Yen-Chang Hsu, Avik Ray, and Hongxia\nJin. 2021. Enhancing the generalization for intent\nclassification and out-of-domain detection in slu. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 2443â€“\n2453.\nLei Shu, Yassine Benajiba, Saab Mansour, and Yi Zhang.\n2021. Odist: Open world classification via distribu-\ntionally shifted instances. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\npages 3751â€“3756.\nChenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang\nWang, Jianfeng Wang, Jordan Boyd-Graber, and Li-\njuan Wang. 2022a. Prompting gpt-3 to be reliable.\narXiv preprint arXiv:2210.09150.\nChenglei Si, Chen Zhao, Sewon Min, and Jordan Boyd-\nGraber. 2022b. Re-examining calibration: The case\nof question answering. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n2814â€“2829.\nYiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li.\n2022. Out-of-distribution detection with deep nearest\nneighbors. In International Conference on Machine\nLearning, pages 20827â€“20840. PMLR.\nGrant Van Horn, Oisin Mac Aodha, Yang Song, Yin\nCui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. 2018. The inaturalist\nspecies classification and detection dataset. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 8769â€“8778.\nVladimir Vapnik. 1991. Principles of risk minimization\nfor learning theory. Advances in neural information\nprocessing systems, 4.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Å ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nC. Wah, S. Branson, P. Welinder, P. Perona, and S. Be-\nlongie. 2011. The caltech-ucsd birds-200-2011\ndataset. Technical Report CNS-TR-2011-001, Cali-\nfornia Institute of Technology.\nWenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,\nZhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei\nLu, Hongsheng Li, Xiaogang Wang, and Yu Qiao.\n2023. Internimage: Exploring large-scale vision\nfoundation models with deformable convolutions. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages\n14408â€“14419.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022. Self-consistency im-\nproves chain of thought reasoning in language mod-\nels. arXiv preprint arXiv:2203.11171.\nJim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert\nStanforth, Vivek Natarajan, Joseph R Ledsam, Patri-\ncia MacWilliams, Pushmeet Kohli, Alan Karthike-\nsalingam, Simon Kohl, et al. 2020. Contrastive train-\ning for improved out-of-distribution detection. arXiv\npreprint arXiv:2007.05566.\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude\nOliva, and Antonio Torralba. 2010. Sun database:\nLarge-scale scene recognition from abbey to zoo. In\n2010 IEEE computer society conference on computer\nvision and pattern recognition , pages 3485â€“3492.\nIEEE.\nYang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu\nWei, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha\nZhang, Wanxiang Che, Min Zhang, and Lidong Zhou.\n2021. LayoutLMv2: Multi-modal pre-training for\nvisually-rich document understanding. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 2579â€“2591, Online.\nAssociation for Computational Linguistics.\nJingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei\nLiu. 2021. Generalized out-of-distribution detection:\nA survey. arXiv preprint arXiv:2110.11334.\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei\nHu, Yumao Lu, Zicheng Liu, and Lijuan Wang. 2022.\nAn empirical study of gpt-3 for few-shot knowledge-\nbased vqa. In Proceedings of the AAAI Conference\n5302\non Artificial Intelligence , volume 36, pages 3081â€“\n3089.\nLi-Ming Zhan, Haowen Liang, Bo Liu, Lu Fan, Xiao-\nMing Wu, and Albert YS Lam. 2021. Out-of-scope\nintent detection with self-supervision and discrimi-\nnative training. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 3521â€“3532.\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude\nOliva, and Antonio Torralba. 2017. Places: A 10\nmillion image database for scene recognition. IEEE\ntransactions on pattern analysis and machine intelli-\ngence, 40(6):1452â€“1464.\nWenxuan Zhou, Fangyu Liu, and Muhao Chen.\n2021. Contrastive out-of-distribution detection\nfor pretrained transformers. arXiv preprint\narXiv:2104.08812.\nZhuofan Zong, Guanglu Song, and Yu Liu. 2023. Detrs\nwith collaborative hybrid assignments training. In\nProceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 6748â€“6758.\nA More Results\nWe use an extra collection of ID datasets\nto showcase the versatility of our method:\nCUB-200 (Wah et al., 2011), STANFORD-\nCARS (Krause et al., 2013), FOOD-101 (Bossard\net al., 2014), OXFORD-PET (Parkhi et al.,\n2012), and three variants of ImageNet con-\nstructed by Ming et al. (2022a), i.e., ImageNet-\n10, ImageNet-20, ImageNet-100. The results\nare shown in Table 5, demonstrating that our\nmethod offers superior performance on various\nmulti-modal OOD detection tasks without training.\nB More Implementation Details\nIn our experiments, we adopt CLIP (Radford et al.,\n2021) as the pre-trained vision-language model.\nSpecifically, we mainly use CLIP-B/16 (CLIP-B),\nwhich consists of a ViT-B/16 Transformer as the\nimage encoder and a masked self-attention Trans-\nformer (Vaswani et al., 2017) as the text encoder.\nWe also use CLIP-L/14 (CLIP-L) as a representa-\ntive of large models. To obtain world knowledge\ncorresponding to each class, we query text-davinci-\n003 (Ouyang et al., 2022) with a sampling tem-\nperature of 0.7 and a maximum token length of\n100.\nTo obtain retrieval feedback for each descrip-\ntor set, We construct a fixed set of unlabeled im-\nages, denoted as M, through the random selection\nof m = 50000images spanning 1000 categories.\nThese images are extracted from the training set\nof ImageNet-1k without corresponding labels. For\ndescriptor set d, the retriever R(Â·) retrieves top k\nsimilar images from M:\nRâ€²(d) = argmax\nMâŠ‚M,|M|=k\nE\nxâˆˆM\ndâˆˆd\nÏƒ(I(x),T(d)).\nFrom the retrieved image subset Râ€²(d) we derive\na binary vector R(d), with entry j equal to 1 if\nthe j-th image of Mis in Râ€²(d). In order to de-\ntermine whether two descriptor sets, d and dâ€², are\nconsistent with each other, denoted asC(d,dâ€²), we\nincorporate the following two constraints:\nC(d,dâ€²) =1\n[\nÏƒ(R(d),R(dâ€²)) â‰¥Î·\n]\nâˆ§\n1\n[\nÏƒ(\nâˆ‘\ndâˆˆd T(d)\n|d| ,\nâˆ‘\ndâ€²âˆˆdâ€² T(dâ€²)\n|dâ€²| ) â‰¥Î·â€²\n]\n,\n(3)\nwhere the first constraint measures the cosine sim-\nilarity between R(d) and R(dâ€²), the second con-\nstraint computes the cosine similarity between the\naveraged textual embeddings of descriptors in d\nand dâ€². Note that the second constraint that com-\nputes textual similarity is optional in our method.\nWe evaluate its impact by constructing an ablation\nvariant relying solely on the first constraint. Its\naverage performance on four OOD datasets with\nImageNet-1k as ID dataset is 38.59 in FPR95 and\n91.37 in AUROC, which outperforms the other\nzero-shot baselines as well. We set k = 50 for\nimage retrieval and Î·= 0.9, Î·â€²= 0.99 for consis-\ntency computation. We set Î³ = 0.5 as the confi-\ndence threshold for p(c).\nIn visual object detection, we use CBNetV2-\nSwin-Base from Cai et al. (2022) with a vocabulary\nof 600 objects as our general object detector. To\nconstruct the ablation variant â€œSimple Det.â€, we\nemploy YOLOv6-L6 from Li et al. (2023) with a\nsmaller vocabulary of 80 categories.\nC Robustness to Sampling Temperature\nT.\nWe vary sampling temperature T for LLM gen-\neration among {0.3,0.5,0.7,0.9,1.1}. It can be\nseen in Figure 9 that regardless of the temperature,\nour method consistently outperforms the ablation\nvariant â€œw/o Know.â€ which does not incorporate\nadditional world knowledge from LLMs. We can\n5303\nï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½ ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½\nï¿½ï¿½\nï¿½ï¿½\nï¿½ï¿½\nï¿½ï¿½\nï¿½ï¿½\nï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ ï¿½\nï¿½ï¿½ï¿½ï¿½\nï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½\nFigure 9: Effect of the sampling temperatureT for LLM\ngeneration.\nalso observe that an intermediate temperature of\n0.7 can lead to the best performance.\nD Reliability under Different LLMs,\nImage Detectors and OOD Detectors\nTo further verify the reliability of our method,\nwe perform OOD detection using our method un-\nder different LLMs (GPT-4, ChatGPT, Claude-1,\nClaude-2, Bard and text-davinci-003), image detec-\ntors (YOLOv6 (Li et al., 2023), InternImage (Wang\net al., 2023), Bigdetection (Cai et al., 2022), Co-\nDETR (Zong et al., 2023)), and OOD detectors\n(CLIP-based OOD detector without softmax scal-\ning (Ming et al., 2022a)). We use ImageNet-1K as\nID dataset, and iNaturalist/SUN/Places/Texture as\nOOD datasets. As shown in Table 6, our method is\nreliable when using different LLMs, image detec-\ntors and OOD detectors.\n5304\nID Dataset Method\nOOD Dataset AverageiNaturalist SUN Places Texture\nFPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘\nCUB-200\nMCM 9.83 98.24 4.93 99.10 6.65 98.57 6.97 98.75 7.09 98.66\nMenon et al. 8.72 98.36 3.16 99.36 3.89 99.08 3.58 99.28 4.72 99.05\nw/o Obj. 7.62 98.51 2.36 99.49 3.24 99.18 3.46 99.24 4.17 99.11\nw/o Calib. 8.85 98.09 3.60 99.29 4.64 98.92 3.09 99.38 5.04 98.92\nw/o Know. 9.29 98.23 5.28 99.05 6.71 98.58 6.38 98.88 6.92 98.68\nOurs 2.33 99.50 3.20 99.17 7.55 98.48 2.43 99.44 3.88 99.15\nStanford-Cars\nMCM 0.05 99.77 0.02 99.95 0.24 99.89 0.02 99.96 0.08 99.89\nMenon et al. 0.07 99.82 0.05 99.96 0.28 99.90 0.02 99.96 0.10 99.91\nw/o Obj. 0.04 99.77 0.02 99.95 0.24 99.90 0.02 99.96 0.08 99.90\nw/o Calib. 0.05 99.81 0.05 99.96 0.30 99.90 0.02 99.96 0.10 99.90\nw/o Know. 0.06 99.76 0.02 99.95 0.26 99.89 0.02 99.96 0.09 99.89\nOurs 0.05 99.75 0.02 99.96 0.24 99.90 0.02 99.96 0.08 99.91\nFood-101\nMCM 0.72 99.76 0.90 99.75 1.86 99.58 4.04 98.62 1.86 99.43\nMenon et al. 5.91 98.91 1.21 99.73 2.69 99.38 5.62 98.28 3.86 99.08\nw/o Obj. 0.72 99.77 1.02 99.74 1.93 99.55 4.17 98.63 1.96 99.42\nw/o Calib. 6.07 98.87 1.78 99.65 3.77 99.24 5.35 98.12 4.24 98.97\nw/o Know. 0.76 99.76 1.06 99.73 2.12 99.54 4.20 98.59 2.04 99.40\nOurs 0.64 99.78 0.86 99.75 1.86 99.57 3.87 98.65 1.81 99.44\nOxford-Pet\nMCM 2.85 99.36 1.06 99.73 2.11 99.56 0.80 99.81 1.70 99.61\nMenon et al. 8.14 98.69 1.42 99.66 3.26 99.37 1.28 99.73 3.52 99.36\nw/o Obj. 2.81 99.32 1.02 99.72 2.01 99.55 0.83 99.80 1.67 99.60\nw/o Calib. 8.41 98.61 1.45 99.65 3.27 99.35 1.31 99.72 3.61 99.33\nw/o Know. 2.82 99.32 1.00 99.71 2.04 99.55 0.87 99.80 1.68 99.59\nOurs 2.80 99.37 1.00 99.74 2.05 99.58 0.80 99.80 1.66 99.62\nImageNet-10\nMCM 0.12 99.80 0.29 99.79 0.88 99.62 0.04 99.90 0.33 99.78\nMenon et al. 0.13 99.81 0.28 99.82 0.90 99.66 0.04 99.91 0.34 99.80\nw/o Obj. 0.10 99.89 0.24 99.89 0.88 99.72 0.04 99.97 0.31 99.87\nw/o Calib. 0.18 99.84 0.29 99.89 1.04 99.64 0.04 99.96 0.38 99.83\nw/o Know. 0.13 99.81 0.28 99.79 0.92 99.62 0.04 99.91 0.34 99.78\nOurs 0.12 99.87 0.22 99.90 0.83 99.73 0.02 99.97 0.30 99.87\nImageNet-20\nMCM 1.02 99.66 2.55 99.50 4.40 99.11 2.43 99.03 2.60 99.32\nMenon et al. 1.01 99.60 1.72 99.51 3.71 99.29 3.10 98.85 2.37 99.32\nw/o Obj. 0.97 99.59 1.72 99.51 3.61 99.29 3.05 98.87 2.35 99.32\nw/o Calib. 0.33 99.78 2.07 99.50 3.37 99.26 2.11 98.93 1.97 99.37\nw/o Know. 0.75 99.71 2.08 99.52 3.67 99.20 2.43 98.83 2.23 99.32\nOurs 0.65 99.63 1.58 99.59 3.19 99.30 2.06 98.94 1.87 99.37\nImageNet-100\nMCM 18.13 96.77 36.45 94.54 34.52 94.36 41.22 92.25 32.58 94.48\nMenon et al. 22.83 96.24 27.00 95.41 31.24 94.55 42.55 92.08 30.91 94.57\nw/o Obj. 18.38 96.65 25.25 95.65 30.27 94.68 41.76 92.18 28.92 94.79\nw/o Calib. 21.73 96.38 24.35 95.75 28.07 94.93 39.06 92.70 28.30 94.94\nw/o Know. 18.48 96.61 31.09 95.29 29.71 95.16 39.07 92.83 29.59 94.97\nOurs 18.13 96.76 22.02 96.14 26.52 95.21 38.65 93.01 26.33 95.28\nTable 5: Zero-shot OOD detection performance based on CLIP-B/16 with various ID datasets.\nMethod\nOOD Dataset AverageiNaturalist SUN Places Texture\nFPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘FPR95â†“AUROCâ†‘\nMCM 30.91 94.61 37.59 92.57 44.69 89.77 57.77 86.11 42.74 90.77\nDifferentLLMs\nOurs(GPT-4) 22.56 95.52 33.49 92.81 40.87 90.53 51.89 87.66 37.20 91.63\nOurs(ChatGPT) 21.97 95.67 33.68 92.88 41.50 90.14 50.96 88.09 37.03 91.70\nOurs(Claude-1) 24.92 95.63 34.97 92.60 42.72 89.72 54.13 86.94 39.19 91.22\nOurs(Claude-2) 25.83 95.08 34.71 92.65 41.65 90.86 53.44 87.02 38.91 91.40\nOurs(Bard) 25.74 95.07 34.17 92.72 42.12 89.85 53.48 87.02 38.88 91.17\nOurs(text-davinci-003) 22.88 95.54 34.29 92.60 41.63 89.87 52.02 87.71 37.71 91.43\nDifferentImage\nDetectors\nOurs(InternImage) 23.41 95.45 35.10 92.63 42.13 89.90 53.65 87.30 38.57 91.32\nOurs(Co-DETR) 22.10 95.69 33.48 92.78 42.92 89.80 51.77 87.78 37.57 91.51\nOurs(YOLOv6) 23.72 95.42 34.29 92.65 43.26 89.84 56.32 86.82 39.40 91.18\nOurs(Bigdetection) 22.88 95.54 34.29 92.60 41.63 89.87 52.02 87.71 37.71 91.43\nDifferentOOD DetectorsCLIP-based(w/o softmax) 61.66 89.31 64.39 87.43 63.67 85.95 86.61 71.68 69.08 83.59\nOurs(w/o softmax) 59.87 89.65 61.79 87.90 60.20 86.27 78.67 72.84 65.13 84.17\nTable 6: Zero-shot OOD detection performance using different LLMs, image detectors and OOD detectors.\n5305",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7469609379768372
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.6491187214851379
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5467832684516907
    },
    {
      "name": "Class (philosophy)",
      "score": 0.51912921667099
    },
    {
      "name": "Modal",
      "score": 0.5067129731178284
    },
    {
      "name": "Machine learning",
      "score": 0.4995558261871338
    },
    {
      "name": "Trustworthiness",
      "score": 0.4345130920410156
    },
    {
      "name": "Natural language processing",
      "score": 0.3840472996234894
    },
    {
      "name": "Computer security",
      "score": 0.11694145202636719
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210095624",
      "name": "Alibaba Group (United States)",
      "country": "US"
    }
  ],
  "cited_by": 12
}