{
    "title": "Language models can learn complex molecular distributions",
    "url": "https://openalex.org/W4281619372",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4227167535",
            "name": "Daniel Flam-Shepherd",
            "affiliations": [
                "University of Toronto",
                "Vector Institute"
            ]
        },
        {
            "id": "https://openalex.org/A2140443067",
            "name": "Kevin Zhu",
            "affiliations": [
                "University of Toronto"
            ]
        },
        {
            "id": "https://openalex.org/A2311514708",
            "name": "Alán Aspuru-Guzik",
            "affiliations": [
                "Vector Institute",
                "University of Toronto",
                "Canadian Institute for Advanced Research"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2017254234",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2578240541",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W2965344674",
        "https://openalex.org/W3119774682",
        "https://openalex.org/W3155243181",
        "https://openalex.org/W4289436753",
        "https://openalex.org/W3151536792",
        "https://openalex.org/W2064963922",
        "https://openalex.org/W4214567217",
        "https://openalex.org/W2784270883",
        "https://openalex.org/W3127493072",
        "https://openalex.org/W2998571806",
        "https://openalex.org/W2939314313",
        "https://openalex.org/W2916581152",
        "https://openalex.org/W4237416477",
        "https://openalex.org/W3011286504",
        "https://openalex.org/W2907657781",
        "https://openalex.org/W4229590462",
        "https://openalex.org/W3009321976",
        "https://openalex.org/W3116865743",
        "https://openalex.org/W3119952098",
        "https://openalex.org/W2096541451",
        "https://openalex.org/W2034549041",
        "https://openalex.org/W2160592148",
        "https://openalex.org/W2066273100",
        "https://openalex.org/W2070072705",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2133679578",
        "https://openalex.org/W2176516200",
        "https://openalex.org/W2153693853",
        "https://openalex.org/W2134164499",
        "https://openalex.org/W2950022236",
        "https://openalex.org/W2177317049",
        "https://openalex.org/W2004024565",
        "https://openalex.org/W2067253662",
        "https://openalex.org/W3118695441",
        "https://openalex.org/W6785926261",
        "https://openalex.org/W2963028280",
        "https://openalex.org/W2970709315",
        "https://openalex.org/W3094553402",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3003257820",
        "https://openalex.org/W2035727100",
        "https://openalex.org/W146900863",
        "https://openalex.org/W2946965984",
        "https://openalex.org/W3102176376",
        "https://openalex.org/W2786565076",
        "https://openalex.org/W1959608418",
        "https://openalex.org/W2786722833",
        "https://openalex.org/W3147983081",
        "https://openalex.org/W3045928028",
        "https://openalex.org/W2990138404",
        "https://openalex.org/W4206174637",
        "https://openalex.org/W3098269892",
        "https://openalex.org/W4297951436",
        "https://openalex.org/W4287868031",
        "https://openalex.org/W4250536210",
        "https://openalex.org/W3103145119",
        "https://openalex.org/W2786103815",
        "https://openalex.org/W2963521729",
        "https://openalex.org/W2989615256",
        "https://openalex.org/W196214544",
        "https://openalex.org/W4234086610",
        "https://openalex.org/W2048312836"
    ],
    "abstract": null,
    "full_text": "ARTICLE\nLanguage models can learn complex molecular\ndistributions\nDaniel Flam-Shepherd 1,2✉, Kevin Zhu1 & Alán Aspuru-Guzik 1,2,3,4✉\nDeep generative models of molecules have grown immensely in popularity, trained on\nrelevant datasets, these models are used to search through chemical space. The downstream\nutility of generative models for the inverse design of novel functional compounds, depends on\ntheir ability to learn a training distribution of molecules. The most simple example is a\nlanguage model that takes the form of a recurrent neural network and generates molecules\nusing a string representation. Since their initial use, subsequent work has shown that lan-\nguage models are very capable, in particular, recent research has demonstrated their utility in\nthe low data regime. In this work, we investigate the capacity of simple language models to\nlearn more complex distributions of molecules. For this purpose, we introduce several\nchallenging generative modeling tasks by compiling larger, more complex distributions of\nmolecules and we evaluate the ability of language models on each task. The results\ndemonstrate that language models are powerful generative models, capable of adeptly\nlearning complex molecular distributions. Language models can accurately generate: dis-\ntributions of the highest scoring penalized LogP molecules in ZINC15, multi-modal molecular\ndistributions as well as the largest molecules in PubChem. The results highlight the limita-\ntions of some of the most popular and recent graph generative models– many of which\ncannot scale to these molecular distributions.\nhttps://doi.org/10.1038/s41467-022-30839-x OPEN\n1 Department of Computer Science, University of Toronto, Toronto, ON M5S 2E4, Canada.2 Vector Institute for Artiﬁcial Intelligence, Toronto, ON M5S 1M1,\nCanada. 3 Department of Chemistry, University of Toronto, Toronto, ON M5G 1Z8, Canada.4 Canadian Institute for Advanced Research, Toronto, ON M5G\n1Z8, Canada. ✉email: danielfs@cs.toronto.edu; alan@aspuru.com\nNATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications 1\n1234567890():,;\nT\nhe efﬁcient exploration of chemical space is one of the\nmost important objectives in all of science, with numerous\napplications in therapeutics and materials discovery.\nHowever, exploration efforts have only probed a very small subset\nof the synthetically accessible chemical space\n1, therefore devel-\noping new tools is essential. The rise of artiﬁcial intelligence may\nprovide the methods to unlock the mysteries of the chemical\nuniverse, given its success in other challenging scientiﬁc questions\nlike protein structure prediction2.\nVery recently, deep generative models have emerged as one of\nthe most promising tools for this immense challenge3. These\nmodels are trained on relevant subsets of chemical space and can\ngenerate novel molecules similar to their training data. Their\nability to learn the training distribution and generate valid,\nsimilar molecules — is important for success in downstream\napplications like the inverse design of functional compounds.\nThe ﬁrst models involved re-purposing recurrent neural net-\nworks (RNNs)4 to generate molecules as SMILES strings5. These\nlanguage models can be used to generate molecular libraries for\ndrug discovery6 or built into variational autoencoders (VAE)3,7\nwhere bayesian optimization can be used to search through the\nmodel’s latent space for drug-like molecules. Other models gen-\nerate molecules as graphs either sequentially 8–14 using graph\nneural networks 15,16 or generate whole molecules in one\nshot17–20. Two of the most popular: CGAVE and JTVAE can be\ndirectly constrained to enforce valency restrictions. Other models\ngenerate molecules as point clouds in 3D space21.\nLanguage models have been widely applied22 with researchers\nusing them for ligand-based de novo design23. A few recent uses\nof language models include: targeting natural-product-inspired\nretinoid X receptor modulators 24, designing liver X receptor\nagonists25, generating hit-like molecules from gene expression\nsignatures26, designing drug analogs from fragments27, compos-\ning virtual quasi-biogenic compound libraries28 and many others.\nAdditional studies have highlighted the ability of language models\nin the low-data regime29,30 with improved performance using\ndata augmentation31.\nInitially the brittleness of the SMILES string representation\nmeant a single character could lead to invalid molecules. This\nproblem has been largely solved with more robust molecular\nstring representations\n32–35. Additionally, with improved training\nmethods, deep generative models based on RNNs consistently\ngenerate a high proportion of valid molecules using SMILES6,9,36.\nOne area that has not been studied is the ability of language\nmodels and generative models to generate larger more complex\nmolecules or generate from chemical spaces with large ranges in\nsize and structure. This is beneﬁcial because of increased interest\nin larger more complex molecules for therapeutics37.\nTo test the ability of language models, we formulate a series of\nchallenging generative modeling tasks by constructing training\nsets of more complex molecules than exist in standard\ndatasets3,36,38. In particular, We focus on the ability of language\nmodels to learn the distributional properties of the target datasets.\nWe train language models on all tasks and baseline many other\ngraph generative model as well— although we focus on CGAVE\nand JTVAE. The results demonstrate that language models are\npowerful generative models and can learn complex molecular\ndistributions better than most graph generative models.\nResults\nWe deﬁne three tasks, generating: (1) distributions of molecules\nwith high scores of penalized LogP3 (Fig. 1a, d), (2) multi-modal\ndistributions of molecules (Fig.1b, e), and (3) the largest mole-\ncules in PubChem (Fig. 1c, e). Necessarily, each different gen-\nerative modeling task is deﬁned by learning to generate from the\ndistribution of molecules in a dataset. We build three datasets\nusing relevant subsets of larger databases.\nIn Table1 there are some summary statistics of atom and ring\nnumber in all datasets compared with two standard datasets Zinc3\nFig. 1 The generative modeling tasks. a–c The molecular distributions deﬁning the three complex molecular generative modeling task.a The distribution of\npenalized LogP vs. SA score from the training data in the penalized logP task.b The four modes of differently weighted molecules in the training data of the\nmulti-distribution task.c Large scale task’s molecular weight training distribution.d–f examples of molecules from the training data in each of the generative\nmodeling tasks. d The penalized LogP task,e The multi-distribution task.f The large-scale task.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x\n2 NATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications\nand Moses36. All tasks involve larger molecules with more sub-\nstructures and contain a larger range of atom and ring number\nper molecule.\nFor each task we assess performance by plotting the distribu-\ntion of training molecules properties and the distribution learned\nby the language models and graph models. We use a histogram\nfor the training molecules and ﬁt a Gaussian kernel density\nestimator to it by tuning its bandwidth parameter. We plot KDE’s\nfor molecular properties from all models using the same band-\nwidth parameter.\nFrom all models we initially generate 10K (thousand) mole-\ncules, compute their properties and use them to produce all plots\nand metrics. Furthermore, for fair comparison of learned dis-\ntributions, we use the same number of generated molecules from\nall models after removing duplicates and training molecules.\nFor quantitative evaluation of any model’s ability to learn its\ntraining distribution, we compute the Wasserstein distance\nbetween property values of generated molecules and training\nmolecules. We also compute the Wasserstein distance between\ndifferent samples of training molecules in order to determine a\nmost optimal baseline, which we can compare with as an oracle.\nFor molecular properties we consider: quantitative estimate of\ndrug-likeness (QED) 39, synthetic accessibility score (SA) 40,\noctanol–water partition coef ﬁcient (Log P)41, exact molecular\nweight (MW), Bertz complexity (BCT)42, natural product likeness\n(NP)22. We also use standard metrics like validity, uniqueness,\nnovelty– to assess the model’s ability to generate a diverse set of\nreal molecules distinct from the training data.\nFor models, our main consideration is a chemical language model\nusing a recurrent neural network with long short-term memory43\nand is trained on SMILES (SM-RNN) or SELFIES (SF-RNN). We\nalso train two of the most popular deep graph generative models: the\njunction tree variational autoencoder (JTVAE)\n10 and the con-\nstrained graph variational autoencoder (CGVAE)9.\nPenalized LogP Task. For theﬁrst task, we consider one of the\nmost widely used benchmark assessments for searching chemical\nspace, the penalized LogP task — ﬁnding molecules with high\nLogP44 penalized by synthesizability40 and unrealistic rings. We\nconsider a generative modeling version of this task, where the\ngoal is to learn distributions of molecules with high penalized\nLogP scores. Finding individual molecules with good scores\n(above 3.0) is a standard challenge but learning to directly gen-\nerate from this part of chemical space, so that every molecule\nproduced by the model has high penalized LogP, adds another\ndegree of dif ﬁculty. For this we build a training dataset by\nscreening the ZINC15 database\n45 for molecules with values of\npenalized LogP exceeding 4.0. Many machine learning approa-\nches can only ﬁnd a handful of molecules in this range, for\nexample JTVAE10 found 22 total during all their attempts. After\nscreening, the top scoring molecules in ZINC amounted to\nroughly 160K (K is thousand) molecules for the training data in\nthis task. Thus, the training distribution is extremely spiked with\nmost density falling around 4.0–4.5 penalized LogP as seen in\nFig. 1a with most training molecules resembling the examples\nTable 1 Dataset statistics for all three tasks compared to\nstandard datasets.\n# Atoms # Rings\nMin Mean Max Min Mean Max\nZinc 6 23.2 38 0 2.8 9\nMoses 8 21.6 27 0 2.6 8\nLogP 12 34.7 78 0 4.2 37\nMulti 7 31.1 106 0 5.3 23\nLarge 101 140.1 891 0 11.2 399\nFig. 2 Penalized LogP Task I. aThe plotted distribution of the penalized LogP scores of molecules from the training data (TRAIN) with the SM-RNN trained\non SMILES, the SF-RNN trained on SELFIES and graph models: CGVAE and JTVAE. For the graph models we display molecules from the out of distribution\nmode at penalized LogP score2½ 1:75; 2:25/C138 as well as molecules with penalized LogP score in the the main mode [4.0,4.5] from all models.\nb–d Distribution plots for all models and training data of molecular properties QED, LogP, and SA score.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications 3\nshown in Fig. 1d. However, some of the training molecules,\naround 10% have even higher penalized LogP scores— adding a\nsubtle tail to the distribution.\nThe results of training all models are shown in Figs.2 and 3.\nThe language models perform better than the graph models, with\nthe SELFIES RNN producing a slightly closer match to the\ntraining distribution in Fig.2a. The CGVAE and JTVAE learn to\nproduce a large number of molecules with penalized LogP scores\nthat are substantially worse than the lowest training scores. It is\nimportant to note, from the examples of these shown in Fig.2a\nthese lower scoring molecules are quite similar to the molecules\nfrom the main mode of the training distribution, this highlights\nthe difﬁculty of learning this distribution. In Fig.2b–d we see that\nJTVAE and CGVAE learn to produce more molecules with larger\nSA scores than the training data, as well, we see that all models\nlearn the main mode of LogP in the training data but the RNNs\nproduce closer distributions– similar results can be seen for QED.\nThese results carryover for quantitative metrics and both RNNs\nachieve lower Wasserstein distance metrics than the CGVAE and\nJTVAE (Table 2) with the SMILES RNN coming closest to the\nTRAIN oracle.\nWe further investigate the highest penalized LogP region of the\ntraining data with values exceeding 6.0— the subtle tail of the\ntraining distribution. In the 2d distributions (Fig.3e) it’s clear that\nboth RNNs learn this subtle aspect of the training data while the\ngraph models ignore it almost completely and only learn\nFig. 3 Penalized LogP Task II. a–d Histograms of penalized LogP, Atoms #, Ring # and length of largest carbon chain (all per molecule) from molecules\ngenerated by all models or from the training data that have penalized LogP≥ 6.0. e 2d histograms of penalized LogP and SA score from molecules\ngenerated by the models or from training data that have penalized LogP≥ 6.0. f A few molecules generated by all models or from the training data that\nhave penalized LogP≥ 6.0.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x\n4 NATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications\nmolecules that are closer to the main mode. In particular, CGVAE\nlearns molecules with larger SA score than the training data.\nFurthermore, the molecules with highest penalized LogP scores in\nthe training data typically contain very long carbon chains and\nfewer rings (Fig.3b, d)— the RNNs are capable of picking up on\nthis. This is very apparent in the samples the model produce, a few\nare show in Fig.3f, the RNNs produce mostly molecules with long\ncarbon chains while the CGVAE and JTVAE generate molecules\nwith many rings that have penalized LogP scores near 6.0. The\nlanguage models learn a distribution that is close to the training\ndistribution in the histograms of Fig.3a–d. Overall, the language\nmodels could learn distributions of molecules with high penalized\nLogP scores, better than the graph models.\nMulti-distribution task. For the next task, we created a dataset\nby combining subsets of: (1) GDB13\n46 molecules with molecular\nweight (MW) ≤ 185, (2) ZINC3,45 molecules with 185 ≤ MW ≤\n425, (3) Harvard clean energy project (CEP)47 molecules with\n460 ≤ MW ≤ 600, and the (4) POLYMERS 48 molecules with\nMW > 600. The training distribution has four modes– (Figs. 1b,\ne and4a). CEP & GDB13 make up 1/3 and ZINC & POLYMERS\ntake up 1/3 each of∼200K training molecules.\nIn the multi-distribution task, both RNN models capture the\ndata distribution quite well and learn every mode in the training\ndistribution (Fig.4a). On the other hand, JTVAE entirely misses\nthe ﬁrst mode from GDB13 then poorly learns ZINC and CEP. As\nwell, CGVAE learns GDB13 but underestimates ZINC and\nentirely misses the mode from CEP. More evidence that the\nRNN models learn the training distribution more closely is\napparent in Fig.4e where CGVAE and JTVAE barely distinguish\nthe main modes. Additionally, the RNN models generate\nmolecules better resembling the training data (Supplementary\nTable 4). Despite this, all models– except CGVAE, capture the\ntraining distribution of QED, SA score and Bertz Complexity\n(Fig. 4b–d). Lastly, in Table2 the RNN trained on SMILES has the\nlowest Wasserstein metrics followed by the SELFIES RNN then\nJTVAE and CGVAE.\nLarge-scale task. The last generative modeling task, involves\ntesting the ability of deep generative models to learn large mole-\ncules, the largest possible molecules relevant to molecular gen-\nerative models that use SMILES/SELFIES string representations or\ngraphs. For this we turn to PubChem49 and screen for the largest\nmolecules with more than 100 heavy atoms, producing ~300K\nmolecules. These are molecules of various kinds: small biomole-\ncules, photovoltaics and others. They also have a wide range of\nmolecular weight from 1250 to 5000 but most molecules fall into\nthe 1250–2000 range (Fig.1c).\nThis task was the most challenging for the graph models, both\nfailed to train and were entirely incapable of learning the training\ndata. In particular, JTVAE ’s tree decomposition algorithm\napplied to the training data produced a ﬁxed vocabulary of\n∼11,000 substructures. However, both RNN models were able to\nlearn to generate molecules as large and as varied as the training\ndata. The training molecules correspond to very long SMILES and\nSELFIES string representations, in this case, the SELFIES strings\nprovided an additional advantage and the SELFIES RNN could\nmatch the data distribution more closely (Fig.5a). In particular,\nlearning valid molecules is substantially more difﬁcult with the\nSMILES grammar, as there are many more characters to generate\nfor these molecules and a higher probability that the model will\nmake a mistake and produce an invalid string. In contrast, the\nSELFIES string generated will never be invalid. Interestingly, even\nwhen the RNN models generated molecules that were out of\ndistribution and substantially smaller than the training molecules\n— they still had similar substructures and resemblance to the\ntraining molecules (Fig. 5a). In addition, the training molecules\nseemed to be divided into two modes of molecules with lower and\nhigher LogP values (Fig.5b): with biomolecules deﬁning the lower\nmode and molecules with more rings and longer carbons chains\ndeﬁning the higher LogP mode (more example molecules can be\nseen in supplementary Fig. 8). The RNN models were both able to\nlearn the bi-modal nature of the training distribution.\nThe training data has a variety of different molecules and\nsubstructures, in Fig. 6a the RNN models adequately learn the\ndistribution of substructures arising in the training molecules.\nSpeciﬁcally the distribution for the number of: fragments, single\natom fragments as well as single, fused-ring and amino acid\nfragments in each molecule. As the training molecules get larger\nand occur less, both RNN models still learn to generate these\nmolecules (Fig. 5a when molecular weigh >3000).\nThe dataset in this task contains a number of peptides and\ncyclic peptides that arise in PubChem, we visually analyze the\nsamples from the RNNs to see if they are capable of preserving\nbackbone chain structure and natural amino acids. Weﬁnd that\nTable 2 Wasserstein distance metrics for LogP, SA, QED, MW, BT, and NP between molecules from the training data and\ngenerated by the models for all three tasks.\nTask Samples LogP SA QED MW BCT NP\nLogP TRAIN 0.020 0.0096 0.0029 1.620 7.828 0.013\nSM-RNN 0.095 0.0312 0.0068 3.314 21.12 0.054\nSF-RNN 0.177 0.2903 0.0095 6.260 25.00 0.209\nJTVAE 0.536 0.2886 0.0811 35.93 76.81 0.164\nCGVAE 1.000 2.1201 0.1147 69.26 141.2 1.965\nMulti TRAIN 0.048 0.0158 0.0020 2.177 14.149 0.010\nSM-RNN 0.081 0.0246 0.0059 5.483 21.118 0.012\nSF-RNN 0.286 0.1791 0.0227 11.35 68.809 0.079\nJTVAE 0.495 0.2737 0.0343 27.71 171.87 0.109\nCGVAE 1.617 1.8019 0.0764 30.31 183.58 1.376\nLarge TRAIN 0.293 0.030 0.0003 18.92 85.04 0.005\nSM-RNN 1.367 0.213 0.0034 124.49 363.0 0.035\nSF-RNN 1.095 0.342 0.0099 67.322 457.5 0.111\nJTVAE –– – – ––\nCGVAE –– – – ––\nTRAIN is an oracle baseline-values closer to it are better.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications 5\nthe RNNs often sample snippets of backbone chains which are\nusually disjoint — broken up with other atoms, bonds and\nstructures. In addition, usually these chains have standard side\nchains from the main amino acid residues but other atypical side\nchains do arise. In Fig.6c we show two examples of peptides that\nare generated by the SM-RNN and SF-RNN. While there are\nmany examples where both models do not preserve backbone and\nfantasize weird side-chains, it is very likely, that if trained entirely\non relevant peptides the model could be used for peptide design.\nEven further, since these language models are not restricted to\ngenerating amino acid sequences that could be used to design any\nbiochemical structure that mimic the structure of peptics or even\nreplicate their biological behavior. This makes them very\napplicable to design modiﬁed peptides50, other peptide mimetics\nand complex natural products51,52. The only requirement would\nbe for a domain expert to construct a training dataset for speciﬁc\ntargets. We conduct an additional study on how well the RNNs\nlearned the biomolecular structures in the training data, in Fig.6b\nwe see both RNNs match the distribution of essential amino acid\n(found using a substructure search). Lastly, it is also likely that the\nRNNs could also be used to design cyclic peptides. To highlight\nthe promise of language models for this task we display molecules\ngenerated by the RNNs with the largest Tanimoto similarity to\ncolistin and vancomycin (Fig. 6d). The results in this task\ndemonstrate that language models could be used to design more\ncomplex biomolecules.\nWe also evaluate models on standard metrics in the literature:\nvalidity, uniqueness and novelty. Using the same 10K molecules\ngenerated from each model for each task we compute the\nfollowing statistics deﬁned in ref.17 and store them in Table3: (1)\nvalidity: the ratio between the number of valid and generated\nmolecules, (2) uniqueness: the ratio between the number of\nunique molecules (that are not duplicates) and valid molecules,\n(3) novelty: the ratio between unique molecules that are not in the\nFig. 4 Multi-distribution Task. aThe histogram and KDE of molecular weight of training molecules along with KDEs of molecular weight of molecules\ngenerated from all models. Three training molecules from each mode are shown.b–d The histogram and KDE of QED, LogP and SA scores of training\nmolecules along with KDES of molecules generated from all models.e 2d histograms of molecular weight and SA score of training molecules and molecules\ngenerated by all models.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x\n6 NATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications\ntraining data and the total number of unique molecules. In the\nﬁrst two tasks (Table3), JTVAE and CGVAE have better metrics\nwith very high validity, uniqueness and novelty (all close to 1),\nhere the SMILES and SELFIES RNN perform worse but the\nSELFIES RNN is close to their performance. The SMILES RNN\nhas the worse metrics due to its poor grammar but is not\nsubstantially worse than the other models.\nWe also considered many additional graph generative model\nbaselines8,12,17,19,53–58 on all tasks. These include some\nGANs11,19, some autoregressive models 8,53,57, normalizing\nﬂows54,58 and single shot models17 Most do not scale at all and\nthe few baselines that do— could only handle the LogP and multi-\ndistribution tasks, but do not perform better than the language\nmodels. Results are shown in Supplementary Tables 1, 2 and\nFig. 1.\nDiscussion\nIn this work, in effort to test the ability of chemical language\nmodels, we introduce three complex modeling tasks for deep\ngenerative models of molecules. Language models and graph\nbaselines perform each task, which entails learning to generate\nmolecules from a challenging dataset.s The results demonstrate\nthat language models are very powerful,ﬂexible models that can\nlearn a variety of very different complex distributions while the\npopular graph baselines are much less capable.\nIn comparison of SELFIES and SMILES, both the SM-RNN\nand SF-RNN perform well in all tasks, better than the baselines.\nWe report that the SF-RNN has better standard metrics (Table3)\nin every task, but the SM-RNN has better Wasserstein distance\nmetrics (Table 2). Furthermore, the SF-RNN has better novelty\nthan the SM-RNN— this may mean that the SELFIES grammar\nleads to less memorization of the training dataset in language\nmodels. This could also help explain why the SF-RNN has better\nstandard metrics but worse Wasserstein metrics than the SM-\nRNN. In addition, data augmentation and random SMILES31\ncould be used to improve the novelty score of the SM-RNN. In\nfuture, it would be valuable to have a more comprehensive eva-\nluation of the use of SMILES and SELFIES representations in\ndeep generative models.\nThe results show that the main baseline graph generative\nmodels, JTVAE and CGVAE are not as ﬂexible as language\nmodels. For the penalized LogP task, the difference between a\nmolecule that has a score of 2 and one that scores 4 often can be\nvery subtle. Sometimes changing a single carbon or other atom\ncan cause a large drop in score— this likely explains why the\nCGVAE severely misﬁt the main training mode. For the multi-\ndistribution task, JTVAE and CGVAE’s difﬁculties are clear but\nFig. 5 Large-scale Task I. aThe histogram and KDE of molecular weight of training molecules along with the KDEs of molecular weight of molecules\ngenerated from the RNNs. Two molecules generated by the RNN’s with lower molecular weight than the training molecules are shown on the left of the\nplot. In addition, two training molecules from the mode and tail of the distribution of molecular weight are displayed on the right.b The histogram and KDE\nof LogP of training molecules along with the KDEs of LogP of molecules generated from the RNNs. On either side of the plot, for each mode in the LogP\ndistribution, we display a molecule from the training data.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications 7\nvery understandable. For JTVAE, it has to learn a wide range of\ntree types: many of which have no large substructures like rings\n(the GDB13 molecules) while others are entirely rings (CEP and\nPOLYMERS). For CGVAE, it has to learn a wide range of very\ndifferent generation traces— which is difﬁcult especially since it\nonly uses one sample trace during learning. For the same reasons,\nthese models were incapable of training on the largest molecules\nin PubChem.\nThe language models also perform better than the additional\ngraph generative baselines— which have the same limitations as\nJTVAE and CGVAE. This is almost expected, as graph generative\nmodels have the more difﬁcult task of generating both the atom\nand bond information— while a language model only has to\ngenerate a single sequence. Given this– it is natural that language\nmodels display such ﬂexible capacity and the evaluated graph\ngenerative models do not. Outside of molecular design some\ngraph generative models have attempted to scale to larger\ngraphs59,60 but these models have not been augmented for\nmolecules. The results here do highlight the fact that many widely\nused graph generative models are designed only for small drug-\nlike39 molecules and do not scale to larger more complex mole-\ncules. On the other hand, while language models can scale and\nﬂexibly generate larger molecules, graph generative models are\nmore interpretable53,57 which is important for drug and material\ndiscovery.\nBased on the experiments conducted, language models are very\npowerful generative models for learning any complex molecular\ndistribution and should see even more widespread use. However,\nFig. 6 Large-scale Task II. aHistograms of fragment #, single atom fragment #, single ring fragment #, fused-ring fragment #, amino acid fragment # (all\nper molecule) from molecules generated by the RNN models or from the training data.b Histograms of speciﬁc amino acid number in each molecule\ngenerated by the RNNs or from the training data.c A peptide generated by the SM-RNN— MKLSTTGFAMGSLIVVEGT (right) and one generated by the SF-\nRNN— ERFRAQLGDEGSKEFVEEA (left). d Molecules generated by the SF-RNN and SM-RNN that are closest in Tanimoto similarity to colistin and\nvancomycin. The light gray shaded regions highlight differences from vancomycin.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x\n8 NATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications\nit is still possible to see improvements to these models as these\nmodels cannot account for other important information like\nmolecular geometry. In addition, we hope that the molecular\nmodeling tasks and datasets introduced can motivate new gen-\nerative models for these larger, more complex molecules. Future\nwork will explore how capable chemical language models are in\nlearning larger and larger snapshots of chemical space.\nMethods\nHyper-parameter optimization. For hyper-parameter optimization we use the\nsimplest most effective method— namely random search61. We randomly sample\nfrom discrete grids of hyper-parameters with equal probability of selection for each\nvalue. The values are roughly equally spaced with 3–5 values in each grid. The\nupper and lower bounds for each hyper-parameter are deﬁned as such: learning\nrate 2½ 0:001; 0:0001/C138 , hidden units2½ 100; 1000/C138 , layer number2½ 1; 5/C138 , dropout\n(probability) in [0.0,0.5]. We do not optimize the number of epochs– we just use\nthe default value for the baseline models used during training on other datasets\n(MOSES, ZINC or Chembl).\nModel selection criteria. There are many model selection criteria possible, for\nexample— the MOSES benchmark36 suggest the Frechet Distance, however, this\nand other performance metrics have been shown to have issues62. We evaluate and\nselect models using all metrics employed in combination with the distribution\nplots. First we compile the top 10% of models with highest validity, uniqueness and\nnovelty. Then we plot distribution plots for the main property of interest (i.e.\npenalized logP for LogP task and molecular weight for others)— then take the\nmodel that has the closest distribution to the training distribution and scores the\nlowest on largest number of the six Wasserstein distance metrics.\nFurther details. Language models are implemented in Python 3 with PyTorch63\nmolecules are processed and relevant properties are computed using RDkit64.\nWasserstein distances are computed using SciPy65 as scipy.stats.wasserstein_-\ndistance based on66— also known as the earth mover’s distance, it can be viewed as\nthe minimum amount of distribution weight that must be moved, multiplied by the\ndistance— in order to transform samples from one distribution into samples from\nthe another.\nPenalized LogP task details. For the SM-RNN we used an LSTM with 2 hidden\nlayer with 400 units and dropout in the last layer with prob= 0.2 and learning rate\nof 0.0001. For the SF-RNN we used an LSTM with 2 hidden layer with 600 units\nand dropout in the last layer with prob= 0.4 and learning rate of 0.0002. The\nCGVAE used 8 propagation layers and hidden layer side of 100 with kl annealed to\n0.1 and a learning rate of 0.0015. The JTVAE used a learning rate of 0.001 and 3\nGNN layers with a hidden size of 356.\nMulti-distribution task. For the SM-RNN we used an LSTM with 3 hidden layer\nwith 512 units and dropout in the last layer with prob= 0.5 and learning rate of\n0.0001. For the SF-RNN we used an LSTM with 2 hidden layer with 500 units and\ndropout in the last layer with prob= 0.2 and learning rate of 0.0003. The CGVAE\nused 8 propagation layers and hidden layer side of 100 with kl annealed to 0.1 and\na learning rate of 0.001. The JTVAE used a learning rate of 0.0001 and 3 GNN\nlayers with a hidden size of 356.\nLarge-scale task. For the SM-RNN we used an LSTM network with 2 hidden\nlayers with 512 units and dropout in the last layer with prob= 0.25 and learning\nrate of 0.001. For the SF-RNN we used an LSTM network with 2 hidden layers with\n800 units and dropout in the last layer with prob= 0.4 and learning rate of 0.0001.\nData availability\nThe processed data used in this study are available inhttps://github.com/danielﬂamshep/\ngenmoltasks.\nCode availability\nThe code used to train models is publicly available. JTVAE:https://github.com/wengong-\njin/icml18-jtnn. CGVAE: https://github.com/microsoft/constrained-graph-variational-\nautoencoder. The RNN models were trained using the char-rnn code fromhttps://github.\ncom/molecularsets/moses. Trained models are available upon request.\nReceived: 6 December 2021; Accepted: 16 May 2022;\nReferences\n1. Bohacek, R. S., McMartin, C. & Guida, W. C. The art and practice of structure-\nbased drug design: a molecular modeling perspective.Med. Res. Rev.16,3\n(1996).\n2. Jumper, J. et al. Highly accurate protein structure prediction with alphafold.\nNature 596, 583 (2021).\n3. Gómez-Bombarelli, R. et al. Automatic chemical design using a data-driven\ncontinuous representation of molecules.ACS Cent. Sci.4, 268 (2018).\n4. Sutskever, I., Martens, J. & Hinton, G. E. Generating text with recurrent neural\nnetworks. In International Conference on Machine Learning(2011).\n5. Weininger, D. Smiles, a chemical language and information system. 1.\nintroduction to methodology and encoding rules.J. Chem. Inf. Comput. Sci.\n28, 31 (1988).\n6. Segler, M. H., Kogej, T., Tyrchan, C. & Waller, M. P. Generating focused\nmolecule libraries for drug discovery with recurrent neural networks.ACS\nCent. Sci. 4, 120 (2018).\n7. Kingma, D. P. & Welling, M. Auto-encoding variational bayes. In\nInternational Conference on Learning Representations(2014).\n8. Li, Y., Vinyals, O., Dyer, C., Pascanu, R. & Battaglia, P. Learning deep generative\nmodels of graphs. InInternational Conference on Machine Learning(2018).\n9. Liu, Q., Allamanis, M., Brockschmidt, M. & Gaunt, A. inAdvances in Neural\nInformation Processing Systems7795–7804 (2018).\n10. Jin, W., Barzilay, R. & Jaakkola, T. Junction tree variational autoencoder for\nmolecular graph generation. InInternational Conference on Machine Learning\n(2018).\n11. You, J., Liu, B., Ying, Z., Pande, V. & Leskovec, J. Graph convolutional policy\nnetwork for goal-directed molecular graph generation.Advances in Neural\nInformation Processing Systems 31(2018).\n12. Seff, A., Zhou, W., Damani, F., Doyle, A. & Adams, R. P. inAdvances in\nNeural Information Processing Systems.\n13. Samanta, B. et al. Nevae: a deep generative model for molecular graphs. In:\nAAAI Conference on Artiﬁcial Intelligence (2019).\n14. Mahmood, O., Mansimov, E., Bonneau, R. & Cho, K. Masked graph modeling\nfor molecule generation.Nat. Commun. 12, 1 (2021).\n15. Duvenaud, D. et al. inNeural Information Processing Systems(2015).\n16. Flam-Shepherd, D., Wu, T. C., Friederich, P. & Aspuru-Guzik, A. Neural\nmessage passing on high order paths.Mach. Learn.: Sci. Technol.(2021).\n17. Simonovsky, M. & Komodakis, N. inInternational Conference on Artiﬁcial\nNeural Networks 412–422 (Springer, 2018).\n18. Ma, T., Chen, J. & Xiao, C. inAdvances in Neural Information Processing\nSystems 7113–7124 (2018).\n19. De Cao, N. & Kipf, T. Molgan: an implicit generative model for small\nmolecular graphs. Preprint at arXiv:1805.11973 (2018).\n20. Flam-Shepherd, D., Wu, T. & Aspuru-Guzik, A. MPGVAE: improved\ngeneration of small organic molecules using message passing neural nets.\nMachine Learning: Science and Technology 2.4(2021): 045010.\n21. Gebauer, N., Gastegger, M. & Schütt, K. Symmetry-adapted generation of 3d\npoint sets for the targeted discovery of molecules.Adv. Neural Inf. Process.\nSyst. 32, (2019).\n22. Ertl, P., Roggo, S. & Schuffenhauer, A. Natural product-likeness score and its\napplication for prioritization of compound libraries.J. Chem. Inf. Model.48,\n68 (2008).\n23. Perron, Q. et al. Deep generative models for ligand-based de novo design\napplied to multi-parametric optimization. Journal of Computational\nChemistry 43,10 (2022).\nTable 3 Standard metrics validity, uniqueness and novelty of\nmolecules generated by all models in every task.\nTask Metric SM-RNN SF-RNN JTVAE CGVAE\nLogP Validity 0.941 1.000 1.000 1.000\nUniqueness 0.987 1.000 0.982 1.000\nNovelty 0.721 0.871 0.980 1.000\nMulti Valid 0.969 1.000 0.999 0.999\nUniqueness 0.996 0.989 0.998 0.996\nNovelty 0.937 0.950 0.998 1.000\nLarge Valid 0.876 1.000 ––\nUniqueness 0.999 0.994 ––\nNovelty 0.999 0.999 ––\nCloser to 1.0 indicates better performance.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x ARTICLE\nNATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications 9\n24. Merk, D., Friedrich, L., Grisoni, F. & Schneider, G. De novo design of\nbioactive small molecules by artiﬁcial intelligence. Mol. Inform. 37, 1700153\n(2018).\n25. Grisoni, F. et al. Combining generative artiﬁcial intelligence and on-chip\nsynthesis for de novo drug design.Sci. Adv. 7, eabg3338 (2021).\n26. Méndez-Lucio, O., Baillif, B., Clevert, D.-A., Rouquié, D. & Wichard, J. De\nnovo generation of hit-like molecules from gene expression signatures using\nartiﬁcial intelligence. Nat. Commun. 11, 1 (2020).\n27. Awale, M., Sirockin, F., Stieﬂ, N. & Reymond, J.-L. Drug analogs from\nfragment-based long short-term memory generative neural networks.J. Chem.\nInf. Model. 59, 1347 (2019).\n28. Zheng, S. et al. Qbmg: quasi-biogenic molecule generator with deep recurrent\nneural network. J. Cheminform. 11, 1 (2019).\n29. Skinnider, M. A., R. G. Stacey, R.G., Wishart, D.S. & Foster, L. J. Deep\ngenerative models enable navigation in sparsely populated chemical space.\n(2021).\n30. Moret, M., Friedrich, L., Grisoni, F., Merk, D. & Schneider, G. Generative\nmolecular design in low data regimes.Nat. Mach. Intell.2, 171 (2020).\n31. Arús-Pous, J. et al. Randomized smiles strings improve the quality of\nmolecular generative models.J. Cheminform. 11, 1 (2019).\n32. Kusner, M. J., Paige, B. & Hernández-Lobato, J. M. inInternational Conference\non Machine Learning(2017).\n33. Dai, H., Tian, Y., Dai, B., Skiena, S. & Song, L. Syntax-directed variational\nautoencoder for structured data. InInternational Conference on Learning\nRepresentations (2018).\n34. O ’Boyle, N. & Dalke, A. Deepsmiles: an adaptation of smiles for use in\nmachine-learning of chemical structures. (2018).\n35. Krenn, M., Häse, F., Nigam, A., Friederich, P. & Aspuru-Guzik, A. Self-\nreferencing embedded strings (SELFIES): A 100% robust molecular string\nrepresentation. Machine Learning: Science and Technology1, 4 045024 (2020).\n36. Polykovskiy, D. et al. Molecular sets (moses): a benchmarking platform for\nmolecular generation models.Front. Pharmacol. 11, 1931 (2020).\n37. Atanasov, A. G., Zotchev, S. B., Dirsch, V. M. & Supuran, C. T. Natural\nproducts in drug discovery: advances and opportunities.Nat. Rev. Drug\nDiscov. 20, 200 (2021).\n38. Gaulton, A. et al. Chembl: a large-scale bioactivity database for drug discovery.\nNucleic Acids Res.40, D1100 (2012).\n39. Bickerton, G. R., Paolini, G. V., Besnard, J., Muresan, S. & Hopkins, A. L.\nQuantifying the chemical beauty of drugs.Nat. Chem. 4, 90 (2012).\n40. Ertl, P. & Schuffenhauer, A. Estimation of synthetic accessibility score of drug-\nlike molecules based on molecular complexity and fragment contributions.J.\nCheminform. 1, 1 (2009).\n41. Wildman, S. A. & Crippen, G. M. Prediction of physicochemical parameters\nby atomic contributions.J. Chem. Inf. Comput. Sci.39, 868 (1999).\n42. Bertz, S. H. Theﬁrst general index of molecular complexity.J. Am. Chem. Soc.\n103\n, 3599 (1981).\n43. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neural Comput.9,\n1735 (1997).\n44. Ghose, A. K. & Crippen, G. M. Atomic physicochemical parameters for three-\ndimensional structure-directed quantitative structure–activity relationships I.\nPartition coefﬁcients as a measure of hydrophobicity.J. Comput. Chem.7, 565\n(1986).\n45. Irwin, J. J. & Shoichet, B. K. Zinc— a free database of commercially available\ncompounds for virtual screening.J. Chem. Inf. Model.45, 177 (2005).\n46. Blum, L. C. & Reymond, J. -L. 970 million druglike small molecules for virtual\nscreening in the chemical universe database gdb-13.J. Am. Chem. Soc.131,\n8732 (2009).\n47. Hachmann, J. et al. The harvard clean energy project: large-scale\ncomputational screening and design of organic photovoltaics on the world\ncommunity grid. J. Phys. Chem. Lett.2, 2241 (2011).\n48. St. John, P. C. et al. Message-passing neural networks for high-throughput\npolymer screening. J. Chem. Phys.150, 234111 (2019).\n49. Kim, S. et al. Pubchem substance and compound databases.Nucleic Acids Res.\n44, D1202 (2016).\n50. Bisht, G. S., Rawat, D. S., Kumar, A., Kumar, R. & Pasha, S. Antimicrobial\nactivity of rationally designed amino terminal modiﬁed peptides.Bioorg. Med.\nChem. Lett. 17, 4343 (2007).\n51. Reker, D. et al. Revealing the macromolecular targets of complex natural\nproducts. Nat. Chem. 6, 1072 (2014).\n52. Sorokina, M., Merseburger, P., Rajan, K., Yirik, M. A. & Steinbeck, C. Coconut\nonline: collection of open natural products database.J. Cheminform. 13,1\n(2021).\n53. Mercado, R. et al. Graph networks for molecular design.Mach. Learn.: Sci.\nTechnol. 2, 025023 (2021).\n54. Lippe, P. & Gavves, E. Categorical normalizingﬂows via continuous\ntransformations. International Conference on Learning Representations.\n(2020).\n55. Jin, W., Barzilay, R. & Jaakkola, T. inInternational Conference on Machine\nLearning (PMLR, 2020)4839–4848.\n56. Popova, M., Shvets, M., Oliva, J. & Isayev, O. Molecular-RNN: Generating\nrealistic molecular graphs with optimized properties. Preprint at\narXiv:1905.13372 (2019).\n57. Li, Y., Zhang, L. & Liu, Z. Multi-objective de novo drug design with\nconditional graph generative model.J. Cheminform. 10, 1 (2018).\n58. Madhawa, K., Ishiguro, K., Nakago, K. & Abe, M. Graphnvp: an invertible\nﬂow model for generating molecular graphs. Preprint at arXiv:1905.11600\n(2019).\n59. Dai, H., Nazi, A., Li, Y., Dai, B. & Schuurmans, D. inInternational Conference\non Machine Learning (PMLR, 2020)2302–2312.\n60. Liao, R. et al. Efﬁcient graph generation with graph recurrent attention\nnetworks. Adv. Neural Inf. Process. Syst.32, (2019).\n61. Bergstra, J. & Bengio, Y. Random search for hyper-parameter optimization.J.\nMach. Learn. Res.13, (2012).\n62. Renz, P., Van Rompaey, D., Wegner, J. K., Hochreiter, S. & Klambauer, G. On\nfailure modes in molecule generation and optimization.Drug Discov. Today.:\nTechnol. 32, 55 (2019).\n63. Paszke, A. et al., Pytorch: an imperative style, high-performance deep learning\nlibrary. Adv. Neural inf. Process. Syst.32, (2019).\n64. Landrum, G. Rdkit: a software suite for cheminformatics, computational\nchemistry, and predictive modeling. (2013).\n65. Virtanen, P. et al. Scipy 1.0: fundamental algorithms for scientiﬁc computing\nin python. Nat. Methods 17, 261 (2020).\n66. Vaserstein, L. N. Markov processes over denumerable products of spaces,\ndescribing large systems of automata.Probl. Pereda. Inf.5, 64 (1969).\n67. Baldwin, S. in Journal of Physics: Conference Series, Vol. 341, 012001 (IOP\nPublishing, 2012).\nAcknowledgements\nA.A.-G. acknowledge funding from Dr. Anders G. Frøseth. A.A.-G. also acknowledges\nsupport from the Canada 150 Research Chairs Program, the Canada Industrial Research\nChair Program, and from Google, Inc. Models were trained using the Canada Computing\nSystems\n67.\nAuthor contributions\nD.F.-S. conceived the overall project, designed the experiments, prepared the datasets and\nwrote the paper. D.F.-S. and K.Z. trained the models and analyzed results. A.A.-G. led the\nproject and provided overall directions.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41467-022-30839-x.\nCorrespondence and requests for materials should be addressed to Daniel Flam-\nShepherd or Alán. Aspuru-Guzik.\nPeer review informationNature Communications thanks the anonymous reviewers for\ntheir contribution to the peer review of this work. Peer reviewer reports are available.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2022\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-022-30839-x\n10 NATURE COMMUNICATIONS|         (2022) 13:3293 | https://doi.org/10.1038/s41467-022-30839-x | www.nature.com/naturecommunications"
}