{
  "title": "Short-Term Load Forecasting Based on Deep Learning for End-User Transformer Subject to Volatile Electric Heating Loads",
  "url": "https://openalex.org/W2981552758",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2113599295",
      "name": "Qifang Chen",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2127755622",
      "name": "Mingchao Xia",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2101343001",
      "name": "Teng Lu",
      "affiliations": [
        "Beijing Jiaotong University"
      ]
    },
    {
      "id": "https://openalex.org/A2132480277",
      "name": "Xichen Jiang",
      "affiliations": [
        "Western Washington University"
      ]
    },
    {
      "id": "https://openalex.org/A2098231626",
      "name": "Wenxia Liu",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2782954703",
      "name": "Qinfei Sun",
      "affiliations": [
        "State Grid Corporation of China (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2113599295",
      "name": "Qifang Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127755622",
      "name": "Mingchao Xia",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101343001",
      "name": "Teng Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132480277",
      "name": "Xichen Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098231626",
      "name": "Wenxia Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2782954703",
      "name": "Qinfei Sun",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2301541953",
    "https://openalex.org/W1981252300",
    "https://openalex.org/W2561964848",
    "https://openalex.org/W2994602700",
    "https://openalex.org/W2742763523",
    "https://openalex.org/W2510931397",
    "https://openalex.org/W2552991604",
    "https://openalex.org/W2343586331",
    "https://openalex.org/W2807831972",
    "https://openalex.org/W2172174166",
    "https://openalex.org/W1988725814",
    "https://openalex.org/W2908855336",
    "https://openalex.org/W2171878998",
    "https://openalex.org/W2341910059",
    "https://openalex.org/W2524052612",
    "https://openalex.org/W2029316659",
    "https://openalex.org/W2797889343",
    "https://openalex.org/W2900205075",
    "https://openalex.org/W2606631653",
    "https://openalex.org/W2969601090",
    "https://openalex.org/W2019607443",
    "https://openalex.org/W2754252319",
    "https://openalex.org/W2291912572",
    "https://openalex.org/W2586259521",
    "https://openalex.org/W2620422220",
    "https://openalex.org/W2554361312",
    "https://openalex.org/W2748388862",
    "https://openalex.org/W2737654582",
    "https://openalex.org/W2776741657",
    "https://openalex.org/W2136922672",
    "https://openalex.org/W2906333996",
    "https://openalex.org/W2763128055",
    "https://openalex.org/W3103064492",
    "https://openalex.org/W2906033034"
  ],
  "abstract": "Short-Term Load Forecasting (STLF) for End-User Transformer Level (EUTL) is challenging due to the high penetration of Electric Heating Loads (EHLs), which exhibit significant uncertainty, nonlinearity, and variability. In this paper, a STLF model is proposed based on the Stacked Auto-Encoder Extreme Learning Machine (SAE-ELM) deep learning framework, which can be used to extract hidden features from the time series load data. In order to improve the capability of extracting deep and diverse features from the data and generate a useful knowledge representation structure, a novel specialized feature indices set is proposed to construct the training sample set. The sliding trend, fluctuation rate, grade of change, and smoothness of the time series are considered and quantified as elements of the training sample set. Then, deep nonlinear features are extracted by using the SAE-ELM with no iterative parameter tuning needed. To illustrate the validity of the proposed model, five numerical cases are conducted. Comparison of results shows that the proposed model improves the capability and sensitivity of dealing with load volatility and forecasting accuracy.",
  "full_text": "SPECIAL SECTION ON ARTIFICIAL INTELLIGENCE TECHNOLOGIES\nFOR ELECTRIC POWER SYSTEMS\nReceived October 1, 2019, accepted October 17, 2019, date of publication October 25, 2019, date of current version November 19, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2949726\nShort-Term Load Forecasting Based on Deep\nLearning for End-User Transformer Subject to\nVolatile Electric Heating Loads\nQIFANG CHEN\n 1, (Member, IEEE), MINGCHAO XIA\n1, (Senior Member, IEEE), TENG LU1,\nXICHEN JIANG\n 2, (Member, IEEE), WENXIA LIU\n3, (Member, IEEE), AND QINFEI SUN4\n1School of Electrical Engineering, Beijing Jiaotong University, Beijing 100044, China\n2Department of Engineering and Design, Western Washington University, Bellingham, WA 98225, USA\n3State Key Laboratory of Alternate Electrical Power System with Renewable Energy Sources, North China Electric Power University, Beijing 102206, China\n4State Grid Beijing Electric Power Company, Beijing 100031, China\nCorresponding author: Mingchao Xia (mchxia@bjtu.edu.cn)\nThis work was supported in part by the National Science Foundation of China under Grant 51677003, in part by the Project funded by\nChina Postdoctoral Science Foundation under Grant 2018M631326, in part by the State Grid Corporation Science and Technology Project\nunder Grant 52020118000M, and in part by the State Key Laboratory of Alternate Electrical Power System with Renewable Energy\nSources under Grant LAPS18018.\nABSTRACT Short-Term Load Forecasting (STLF) for End-User Transformer Level (EUTL) is challenging\ndue to the high penetration of Electric Heating Loads (EHLs), which exhibit signiﬁcant uncertainty,\nnonlinearity, and variability. In this paper, a STLF model is proposed based on the Stacked Auto-Encoder\nExtreme Learning Machine (SAE-ELM) deep learning framework, which can be used to extract hidden\nfeatures from the time series load data. In order to improve the capability of extracting deep and diverse\nfeatures from the data and generate a useful knowledge representation structure, a novel specialized feature\nindices set is proposed to construct the training sample set. The sliding trend, ﬂuctuation rate, grade of\nchange, and smoothness of the time series are considered and quantiﬁed as elements of the training sample\nset. Then, deep nonlinear features are extracted by using the SAE-ELM with no iterative parameter tuning\nneeded. To illustrate the validity of the proposed model, ﬁve numerical cases are conducted. Comparison of\nresults shows that the proposed model improves the capability and sensitivity of dealing with load volatility\nand forecasting accuracy.\nINDEX TERMS Short-term load forecasting, feature representation, deep learning, stacked auto-encoder,\nextreme learning machine.\nI. INTRODUCTION\nShort-Term Load Forecasting (STLF) is essential for main-\ntaining the balance between supply and demand in a power\nsystem [1], [2]. The rapid development of the smart grid in\naddition to the deregulation of electricity markets resulted\nin many smart grid applications becoming increasingly\ncustomer-oriented (i.e., demand response [3], [4]) [5]. Load\nforecasting at End-User Transformer-Level (EUTL) is an\nimportant research area that supports many smart applications\nand is essential for service providers that manage loads and\nrenewable resources. However, due to the uncertainties asso-\nciated with renewable generation [6] and new electric loads\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Hui Liu\n .\n(e.g., air-conditioning [7], electric heating load (EHL) [8],\nelectric vehicle charging stations [9]), the load proﬁle at\nthe transformer is now increasingly non-stationary, making\nSTLF at EUTL challenging.\nIn recent decades, a number of researchers have investi-\ngated STLF. In [10], a distributed STLF method for a bulk\npower system covering large geographical areas was pre-\nsented. In order to improve the accuracy of STLF, the bulk\npower system was partitioned into several subnetworks\nand their loads were forecasted using weather information.\nIn [11], a STLF model based on support vector regression\nand two-step hybrid parameters optimization was carried\nout for distribution system load forecasting. A STLF model\nconsidering the hierarchical structure of distribution system\nwas presented in terms of load features in [12]. In order to\nVOLUME 7, 2019 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ 162697\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nimprove the forecasting accuracy, a hybrid STLF model was\nproposed for a bulk power system in [13]. In [14], a two-\nstage hybrid STLF method was proposed. The ﬁrst stage used\na time-series load forecasting model where the forecasted\nerror of the ﬁrst stage was corrected in the second stage by\nperforming a deep analysis on the impact of relative factors on\nthe model error. Although these aforementioned STLF meth-\nods in [10]–[14] may have good STLF performance at the\nsystem-level, it cannot be applied to EUTL load forecasting\ndirectly. This is because while the load capacity at EUTL is\nmuch smaller, the load variability is much larger compared\nagainst the load capacity at the system-level. Furthermore,\nno additional weather information is available at EUTL due to\nthe prohibitive deployment cost and advanced technological\nrequirement. Therefore, the accuracy of the STLF model at\nEUTL is mainly dependent on the how the historical load\ndata is processed. The raw load data at EUTL always exhibits\nsigniﬁcant irregularities such as uncertainty, nonlinearity, and\nlarge volatility. An effective deep feature extraction method\nis critical for forming knowledge representation structure\nto make full use of the deep hidden information from the\nhistorical data.\nIn general, deep feature extraction from historical data\ncan be conducted in two ways: manual data preprocessing\nby rules and through a deep learning neural network. The\nseasonality of the load was regarded as an important feature\nin [2], [15]. In [2], ten different features of the historical\nload time series were used as latent regressors that were\nsuspected to inﬂuence the current hourly load (i.e., loads\nfrom the previous week, hours of the day, etc.) in order to\ndevelop appropriate regression models. In [15], the repre-\nsentation of seasonality was non-parametric and the daily\nand weekly cycles were not expressed as mathematical func-\ntions but represented as the mean of previously observed\nload patterns. The feature of seasonality was not an inte-\ngral part of the forecasting algorithm. In [16], a two-stage\nlinear and nonlinear combined hybrid forecasting model for\nmixed-use complex STLF was proposed. The labelling pro-\ncess considers the type of day: workday or holiday. Load\ntype is used to decompose the aggregated load into several\nsub-clusters. These aforementioned custom data preprocess-\ning methods were able to capture some shallow features,\nbut proved unsuitable for mining deep correlation features.\nIn addition, the impacts of some important external factors\nsuch as electricity price were not considered. In [17], [18],\nthe wavelet decomposition (WD) based STLF method was\nused for extracting deep features in the frequency domain.\nThe method reduces the non-stationary characteristics of the\nload proﬁle by decomposing the original signal into many\ncomponents with different frequencies. In [19] , the empirical\nmode decomposition (EMD) method was used to decompose\nenergy price time series data for developing a forecasting\nmodel. The data was decomposed into intrinsic mode func-\ntions (IMFs) in the time domain with the same length as\nthe original signal, which proved useful for analyzing the\nfeatures directly. Both WD and EMD were useful in feature\nextraction from raw time series data, but the relationships and\nassociations between other external factors were not included\nin the decomposed components.\nDeep learning has been used extensively in extracting non-\nlinear features from data [20]. Frequently used deep learning\nalgorithms for STLF models include deep belief network\n(DBN) [21], deep recurrent neural network (DRNN) [22],\nand deep convolutional neural network (DCNN) [23]. In [3],\na long short-term memory (LSTM) RNN-based framework\nwas presented to tackle the forecasting problem of a single\nresidential user load. In [24], the DRNN and DCNN were\ncarried out for short-term load forecasting and medium-term\nload forecasting. In [25], DRNN and DCNN were presented\nfor day-ahead multi-step load forecasting in commercial\nbuildings. In [26], a deep belief network was built for an\nhourly load forecasting model of a bulk power system. All\nthese deep learning algorithms exhibit excellent performance\nin STLF.\nStacked auto-encoder (SAE) is a type of feed forward neu-\nral network that is composed of an encoder and decoder [27].\nIt is trained to reconstruct its own input in the output layer in\nan unsupervised manner. This can be viewed as an advanced\nfeature extractor capable of preserving the hidden abstrac-\ntions and has an invariant structure in its inputs [28]. A new\nkind of SAE combined with extreme learning machine (SAE-\nELM) was proposed to improve the computational efﬁciency\nof deep learning by the authors of [29]. Because of its fast\ntraining speed and high generalizability, S-ELM has found\napplications in many areas, such as representational learn-\ning [30], [31], data partitioning [32], pattern recognition [33],\nand soft sensor modeling [34]. In this paper, a deep learning\nframework based on SAE-ELM is introduced to construct the\nSTLF model. The contributions of the proposed work are as\nfollows.\n1) A novel specialized feature extracting model is proposed\nto construct a knowledge representation structure for the\ntraining sample set with a variety of deep features.\n2) The EMD method is utilized to decompose the raw time\nseries data in time domain. Instead of forecasting each\nindividual IMF independently, the EMD method is used\nas a low pass ﬁlter to eliminate the non-stationary IMF.\n3) The SAE-ELM based deep learning framework is intro-\nduced to construct the STLF model, which is used to\npredict irregularities at EUTL. By using SAE-ELM,\nthe deep nonlinear features are learned automatically\nwith no iterative parameter turning or ﬁne-turning\nneeded, which renders the training process very\nfast.\nThe remainder of this article is organized as follows. The\nirregularities at EUTL and the specialized feature extrac-\ntion model are presented in Section II. Section III presents\nthe SAE-ELM model that is utilized in the STLF method.\nCase studies are conducted and the results of which are\ndiscussed in Section IV . Finally, concluding remarks are made\nin Section V .\n162698 VOLUME 7, 2019\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nII. IRREGULARITY ANALYSIS AND SPECIALIZED\nFEATURE EXTRACTION\nA. IRREGULARITY IN END-USER TRANSFORMER\nCONTAINING HIGH PENETRATION OF ELECTRIC\nHEATING LOAD\nThe power consumption of an EHL is typically much higher\nthan conventional household appliances. These loads have\nclosed-loop controllers that turn the EHL on and off in order\nto maintain the temperature within setpoints. Thus, the load\nproﬁle of a household with EHL exhibit large random ﬂuc-\ntuations throughout the day. An end-user transformer serves\nseveral households, so its load proﬁle is an aggregate of\nthe proﬁles of the individual households. Due to the small\ncapacity and the high penetration of EHLs, randomness and\nvolatility are prevalent in the load proﬁle of an end-user\ntransformer.\nThe load proﬁle of a typical day of end-user transformer\nwith high penetration of EHLs is shown in Fig. 1. The notice-\nable features include high peak-valley difference, sudden\nlarge changes, and random ﬂuctuations. There are also other\nhidden features such as sliding trend, variation with different\nfrequency, and impact from other external factors such as\nelectricity price. All these features need to be extracted from\nthe raw time series data and quantiﬁed into a knowledge\nrepresentation that is easily learned. Then, it is utilized as\nthe input training sample set for the deep learning process\nin order to ensure that the SAE-ELM model is well trained\nwith enough knowledge.\nFIGURE 1. Representative daily load profile at EUTL.\nB. THE NOVEL SPECIALIZED FEATURE\nEXTRACTING INDICES\nA set of novel specialized feature extracting indices are pre-\nsented to extract and quantify features from raw data.\n1) THE SLIDING TREND (SlT)\nLet P indicate a vector of active power consumed by the\nload as shown in (1), and t indicate the current time interval.\nThe SlT represents the average sliding trend during a speciﬁc\ntime window. It considers both the current state and the m\nprevious states, with m being the window length. Therefore,\nit is deﬁned by the sliding average model shown in (2).\nP =[pl(1), ··· , pl(t), ··· , pl(r)] (1)\nSlT (t) = 1\nm\nm−1∑\ni=0\npl(t −i) (2)\nwhere SlT (t) is the sliding trend index; m is the length of\nsliding trend window; r is the number of time intervals in a\nday; pl(t) is the active power of load.\n2) THE FLUCTUATING RATE (FlR)\nThe FlR is deﬁned to capture the ﬂuctuation direction and the\nrelationship between the ﬂuctuation and SlT.\nFlR(t) =(pl(t) −pl(t −1))\nSlT (t) (3)\n3) THE GRADE OF CHANGE (GoC)\nThe volatility of the load is divided into 5 grades with the\nGoC quantifying the grade of change.\nGoC(t) =\n\n\n\n2, FlR(t) > g+\n1 , grade 5\n1, ¯g+\n0 < FlR(t) ≤g+\n1 , grade 4\n0, g−\n0 ≤FlR(t) ≤¯g+\n0 , grade 3\n−1, g−\n1 ≤FlR(t) < g−\n0 , grade 2\n−2, FlR(t) < g−\n1 , grade 1\n(4)\nwhere GoC(t) is the GoC index at time t; g−\n1 is the threshold\nseparating grades 1 and 2; g−\n0 is the threshold separating\ngrades 2 and 3; ¯g+\n0 is the threshold separating grades 3 and 4;\ng+\n1 is the threshold separating grades 4 and 5. Grade 3 indi-\ncates that the volatility is small. Grades 2 and 4 denote larger\nvolatility, and grades 1 and 5 represent the highest volatility.\n4) THE SMOOTHNESS OF SERIES (SoS)\nThe EMD method performs auto-adaptive nonlinear analysis\non non-stationary time series. The original time series can\nbe decomposed into several IMFs and one residue. The main\nsteps of the EMD method are shown in Table 1.\nThe raw load time series data can be decomposed into\nindividual frequency components as shown in (6).\nP =\nNimf∑\ni=1\nsi\nimf +sre (6)\nwhere Nimf is the number of IMFs.\nLet s1\nimf be the highest frequency component, which is\nalso the most non-stationary component. The other compo-\nnents si\nimf are of decreasing frequencies with increasing i.\nBecause s1\nimf is highly unpredictable, no useful features can\nbe extracted from it.\nThe SoS index is deﬁned as the stationary series which is\nextracted from the raw series by neglecting the non-stationary\nand highly unpredictable IMF. Therefore, the SoS index is\nVOLUME 7, 2019 162699\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nTABLE 1. Steps of the EMD method.\ndeﬁned as the sum of all IMFs except the highest frequency\ncomponent, s1\nimf , as shown in (7).\nSoS =\nNimf∑\ni=2\nsi\nimf +sre (7)\nInstead of only using the raw load time series as training\nsample set, all the proposed feature indices along with the raw\nload time series are used to construct the new knowledge rep-\nresentation set, which is shown in (8). Then, the input training\nsample set for SAE-ELM is generated from the knowledge\nrepresentation set.\nF =\n\n\n\npl (1) ··· pl (t) ··· pl (r)\nSlT (1) ··· SlT (t) ··· SlT (r)\nFlR (1) ··· FlR (t) ··· FlR (r)\nGoC (1) ··· GoC (t) ··· GoC (r)\nSoS (1) ··· SoS (t) ··· SoS (r)\n\n\n\n(8)\nIII. STFL MODEL BASED ON SAE-ELM\nA. THE STACKED AUTO-ENCODER BASED ON EXTREME\nLEARNING MACHINE\nAn auto-encoder (AE) is a neural network that is trained to\napproximately reconstruct its input in the output layer. It is a\nfeed forward neural network consisting of an encoder and a\ndecoder, a typical architecture of which is shown in Fig. 2.\nLet h ∈RN×L be the matrix of hidden layer, the input vec-\ntor x is mapped to the hidden layer by (9) and reconstructed\nby (10).\nh =g (ax +b) (9)\n⌢\nx =hβ (10)\nwhere x ∈RN is the input vector. g (a, b, x) is the activation\nfunction of the hidden layer; in this paper, we use a sigmoid\nfunction. a is the weight matrix mapping the input layer to\nthe hidden layer; b is the bias vector of hidden layer; β is the\noutput weight vector.\nFIGURE 2. The typical architecture of an auto-encoder.\nBecause the AE model prioritizes the aspects of the input\nthat should be copied, it often learns useful properties of the\ndata. In order to learn sufﬁcient representation for achieving\ngood generalization performance, SAE is often used for min-\ning deep features from data. However, as the number of layers\nincrease, the complexity and computation of the conventional\nSAE trained with BP algorithm also increases. In this paper,\nthe deep learning framework based on SAE-ELM model is\nintroduced as the core of STLF model, which improves the\ntraining efﬁciency signiﬁcantly.\nThe STLF model is composed of an unsupervised deep\nfeature extraction (UDFE) module and a forecasting module\n(FM), as shown in Fig. 3. The total number of layers of\nthe deep learning framework is k +2, which includes the\ninput and output layers. The UDFE is composed of all layers\nbetween the input layer and the k −1th hidden layer, wherein\neach hidden layer includes an AE-ELM. The FM is comprised\nof the k −1th hidden layer, the kth hidden layer and the output\nlayer.\nWe take the AE-ELM of qth hidden layer to illustrate the\nprocess of training AE with ELM. As shown in Fig. 4, the\ntraining process of AE-ELM is composed of four main steps.\nStep 1\nUse the output H(q−1) of the q −1th layer as the input of\nthe qth hidden layer.\nStep 2\nMap H(q−1) to the qth hidden layer space with random\ninput weight matrix a(q), bias vector b(q), and the activation\nfunction g(·) in (11). Then, the output\n⌢\nH(q−1) of the qth hidden\nlayer is given by (12).\nHAE\n(q) =g\n(\na(q)H(q−1) +b(q)\n)\n(11)\n⌢\nH(q−1) =HAE\n(q) β(q) (12)\nwhere β(q) is the output weight of AE in the qth hidden layer.\nStep 3\nIn order to promote generalization, the output weight\nβ(q) is obtained by solving the optimization problem\n162700 VOLUME 7, 2019\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nFIGURE 3. Deep learning framework for the STLF model.\nFIGURE 4. The procedure of AE-ELM training.\nof (13).\nmin 1\n2\nβ(q)\n2 +C\n2\n\n⌢\nH(q−1) −HAE\n(q) β(q)\n\n2\n(13)\nwhere the ﬁrst item is the regularization term with over-\nﬁtting. C is the penalty coefﬁcient. The objective function\nminimizes both the output weight and output error simulta-\nneously.\nNext, we take the gradient of (13) with respect to β(q) and\nequate it to zero as shown in (14) and solve for the output\nweight β(q) as in (15) if HAE\n(q) is full column rank; Otherwise,\nif HAE\n(q) is full row rank, the output weight β(q) can be obtained\nby (16).\nβ(q) +C\n(\nHAE\n(q)\n)T (⌢\nH(q−1) −HAE\n(q) β(q)\n)\n=0 (14)\nβ(q) =\n((\nHAE\n(q)\n)T\nHAE\n(q) +I\nC\n)−1 (\nHAE\n(q)\n)T ⌢\nH(q−1) (15)\nβ(q) =\n(\nHAE\n(q)\n)T (\nHAE\n(q)\n(\nHAE\n(q)\n)T\n+I\nC\n)−1 ⌢\nH(q−1) (16)\nwhere I is the identity matrix.\nStep 4\nAfter the training of AE is completed, βT\n(q) is used as the\nmapping weight between the q −1th hidden layer and the\nqth hidden layer. Then the output of the qth hidden layer is\nobtained by (17).\nH(q) =g\n(\nH(q−1)βT\n(q)\n)\n(17)\nIn regards to FM, it is a typical supervised ELM regression.\nThe output H(k−1) of the last hidden layer in UDFE is taken\nas the input. Then, the output weight β(o) can be calculated\nwith (18).\n\n\n\nβ(o) =\n(\nHT\n(k)H(k) +I\nC\n)−1\nHT\n(k)y(t +1), full column rank\nβ(o) =HT\n(k)\n(\nH(k)HT\n(k) +I\nC\n)−1\ny(t +1), full row rank\n(18)\nwhere y(t +1) is the target output vector of the training\nprocess.\nAll of the connecting weights between two adjacent lay-\ners shown in Fig. 3 are determined after the independent\nsequential training process. Once the weights are determined,\nno ﬁne-tuning is required. Finally, the forecasted output y ∗\n(t +1) of STLF model is obtained by (19).\ny ∗(t +1) =g\n(\nH(k−1)βT\n(k)\n)\nβ(o) (19)\nVOLUME 7, 2019 162701\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nB. THE FORMATION OF TRAINING SAMPLE SET\nThe training sample set is one of the most important compo-\nnents of the STLF model. It determines what can be extracted\nand learned by the deep learning process. In order to make the\ntraining sample set informative, the aforementioned knowl-\nedge representation set based on feature indices are utilized to\nconstruct the attributes series of each time interval, as shown\nin Fig. 5.\nFIGURE 5. AE-ELM training sample set.\nDenote t to be the current time interval and ( t +1) to be the\nprediction time interval. A sliding window with length m is\nused to update features. Then, m samples from time interval\n(t −m +1) to t of each index in knowledge representation\nstructure are used to construct the attribute series at time\ninterval t sequentially. Finally, the sample series in a day is\nformed by the attribute series of all time intervals.\nC. THE PROCESS OF STLF METHOD\nThe entire STLF method is shown in Fig. 6, which consists\nof the three main steps of input attribute set selection, deter-\nmination of network parameters, and ﬁnal forecasting.\n1) INPUT ATTRIBUTE SET SELECTION\nFig. 5 shows the training sample set for the duration of one\nday. The day is split into 96 intervals with an attribute series\nassociated with each time interval. In order to forecast the\nload of the ( t +1)th time interval, all the attribute series at the\ntth time interval in all previous days are input into SAE-ELM\nin parallel, which is shown as step 1 in Fig. 6 and described\nby (20).\nA =\n\n\n\ny1(t +1),\n[\np1\nl,t , SlT1\nt , FlR1\nt , GoC1\nt , SoS1\nt\n]\n...\n...\n...\nyd (t +1),\n[\npd\nl,t , SlTd\nt , FlRd\nt , GoCd\nt , SoSd\nt\n]\n\n\n\n(20)\nwhere yd (t+1) is the target output of the ( t+1)th time interval\nin the dth day. The term in square bracket is the input attribute\nseries at the tth time interval.\n2) DETERMINATION OF NETWORK PARAMETERS\nFirst, the number of nodes in the input layer is determined by\nthe dimension of the input attribute series. Then, the attribute\nset in (20) is taken as input to train deep learning neural net-\nwork based on SAE-ELM to determine connecting weights\nin UDFE and FM process according to (9) - (19).\n3) FINAL FORECASTING\nThe computed network parameters are used to construct the\nSTLF model. Then, the attribute set at the current time inter-\nval t in the prediction day is used as input to obtain the\nforecasted load\n⌢\ny(t +1).\nIV. NUMERICAL STUDY\nA. DATA PREPERATION\nThe heating season in China typically lasts from Nov. 11 to\nMar. 15 of the following year. During this period, the heating\nelectricity price in the evening is much lower than that of non-\nheating season, which greatly increases the electric heating\nload. In this paper, the data of one residential transformer in\nBeijing from Jan. 1, 2016 to Mar. 15, 2016 is used as historical\nload series, as shown in Fig. 7.\nData from Jan. 1, 2016 to Mar. 8, 2016 is used as the\ninitial time series to construct the knowledge representation\nset and the remaining data from Mar. 9, 2016 to Mar. 15,\n2016 is used as the original testing data. The historical data is\nrecorded every 15-minute and there are 96 time intervals in a\nday. All the historical data is ﬁrst extracted by the proposed\nspecialized feature indices and formed as a feature set. Then,\nthe feature set is constructed as a training sample set. Before\ninput to the training model, the elements in training sample set\nneed to be normalized to [ −1, 1] within their own categories\nby (21). Then, it is formed as training sample set day by day\naccording to Fig. 5.\ny =\n(\nymax −ymin\n) x −xmin\nxmax −xmin +ymin (21)\nwhere ymax is the upper bound of normalization range, set to\n1; ymin is the lower bound of normalization range, set to −1.\nx is the data to be normalized; xmax is the maximum value of\nx and xmin is minimum value of x.\nB. CASES DESIGN AND SETTINGS\nFor validation, the proposed STLF method is compared\nagainst other methods such as the moving average method\n(MAM), backpropagation neural network (BPNN), and long\nshort term memory (LSTM) method.\n(1) MAM is a type of empirical forecasting method, which\nuses the average value of a rolling time window as\nforecasted value ymam(t +1). In this paper, the rolling\ntime window of length 3 is used as benchmark.\nymam(t +1) =1\n3\nt∑\ni=t−2\nx(i) (22)\n162702 VOLUME 7, 2019\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nFIGURE 6. The complete STLF method.\nFIGURE 7. The load time series from Jan. 1st 2016 to Mar. 15th 2016.\n(2) BPNN is used as the core of STLF model. Histori-\ncal load time series is formed as training input. The\nattributes of the training input are the loads of the ﬁve\nprevious time intervals.\n(3) STLF model based on LSTM is also selected as one\nof the benchmarks. The time series data from Jan. 1,\n2016 to Mar. 8, 2016 is used as training input.\n(4) The proposed STLF model is based on SAE-ELM. The\nlength of sliding window of input training sample set is\n3. The four thresholds of GoC are −0.2, −0.08, 0.08,\nand 0.2, respectively.\nSome key parameters of the aforementioned methods are\nlisted in Table 2. For BPNN, the number of hidden layers is\nset to 1 and the number of hidden nodes is 10. The number of\nhidden layers of LSTM is set to 2 and the number of hidden\nnodes of each layer is 200. The number of hidden layers of\nTABLE 2. Key parameters.\nSAE-ELM is set to 2 and the number hidden nodes in each\nlayer is 150.\nC. RESULTS\nThe results of the case studies are presented in Table 3 and\nin Fig. 8. In Table 3, the mean absolute percentage error\n(MAPE), the maximum absolute percentage error (Max-\nAPE), the root mean square error (RMSE), and the maximum\nVOLUME 7, 2019 162703\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nFIGURE 8. Comparison between the proposed SAE-ELM trained by feature set and the SAE-ELM trained by time series. (a) the\ncomparison results among the four methods and the original series; (b) the comparison of root square error of four methods.\nTABLE 3. Overall forecasting errors of 7 days.\nroot square error (Max-RSE) are used in comparing the per-\nformance of the four methods.\nFor clarity, only the results of the ﬁrst two days (Mar. 9,\n2016 and Mar. 10, 2016) are shown in Fig. 8. It can be\nconcluded that the load time series ﬂuctuates signiﬁcantly.\nThe minimum load is about 80 kW and the maximum load is\nabout 210 kW. The peak-valley difference is about 62%.\nAmong the four methods, the MAPE of the proposed\nSTLF method based on SAE-ELM is the lowest, followed\nby LSTM, MAM, and BPNN. The MAPE of the proposed\nSTLF method based on SAE-ELM has decreased by 10.04%,\n13.78%, 13.71% when compared against the LSTM method,\nBPNN method, and MAM method, respectively. Because\nthe EHLs are affected by many external factors, the load\nactive power sometimes changes abruptly. Due to the high\nvariability of the load, this may result in high forecasting\nerror. Therefore, the Max-APEs and Max-RSE of all the\nfour methods are high. The proposed STLF model based\non SAE-ELM has the lowest Max-APE at 68.1% and the\nlowest Max-RSE at 55.25 kW. Comparing the RMSE index,\nthe proposed STLF model based on SAE-ELM has the best\nperformance, which is 11.97% better than LSTM, 13.77%\nbetter than BPNN, and 18.54% better than MAM.\nComparing the computational index (CI) in Table 3, it can\nbe seen that the proposed STLF model based on SAE-ELM\nhas the highest computational efﬁciency. It is about 1.3 times\nfaster than BPNN and 60.41 times faster than LSTM. This\nis because no iterative parameter tuning and ﬁne-tuning are\nneeded in the training process for the STLF model. Therefore,\nthe proposed STLF model based on SAE-ELM is better than\nthe other three methods across the ﬁve indices.\nAs shown in Fig. 8, the proposed STLF model based on\nSAE-ELM is capable of following the variation during most\n162704 VOLUME 7, 2019\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nFIGURE 9. The comparison results between the proposed SAE-ELM trained by feature set and the SAE-ELM trained by time series.\ntime intervals. Therefore, the root square error is lowest in\nmost time intervals. Due to the impacts of external factors,\nthere are three large volatile periods in Fig. 8 (a). Fortunately,\nthe proposed STLF model based on SAE-ELM is capable\nof capturing most of the sudden change features effectively\nwhile the other three methods may have a one to two time\ninterval delay. This is because the ﬂuctuation and the grade\nof change are extracted and represented as part of the training\nsample set by the proposed specialized feature indices set.\nIn summary, it can be concluded from the four comparison\ncases that:\n(1) The proposed STLF model based on SAE-ELM is\nbetter in dealing with small scale load forecasting with\nlarge volatility.\n(2) Due to the better deep feature extracting and learn-\ning ability, the STLF models based on deep learn-\ning framework (SAE-ELM and LSTM) generally per-\nforms better than other conventional methods.\n(3) The specialized feature extracting indices can help\ncapture and represent deep diverse information and\ntherefore contribute to the learning process and\nimprove the model accuracy.\nD. ANALYSIS OF THE EFFECT OF SPECIALIZED\nFEATURE SET\nIn order to further analyze the advantages of the proposed\nspecialized feature set, one more case study is conducted.\nIn this comparison, the STLF model also based on the same\nSAE-ELM framework but trained by historical time series is\ncompared to the proposed STLF model.\nThe comparison results are shown in Fig. 9. The per-\nformance of the STLF method based on SAE-ELM-TS in\ndealing with volatility is worse compared to the proposed\nSTLF method. This is because the STLF method based on\nSAE-ELM-TS is only trained by time series data instead\nof by the specialized feature indices. This is especially true\nfor predicting sudden large changes, which is much more\ndifﬁcult. Comparing with the proposed STLF model, it is\nusually with one time interval delay. The root square error of\nSAE-ELM-TS is also larger than that of the proposed STLF\nmodel in most time intervals. The RMSE of SAE-ELM-TS is\n23.72 which is much larger than that of the proposed STLF\nmodel.\nIt can be concluded that (i) the proposed specialized feature\nextracting indices can capture deep diverse features from\nhistorical time series; (ii) the training sample set constructed\nfrom feature indices can help improve the effect of deep\nlearning process and improve the capability and sensitivity\nin dealing with volatility. Therefore, the forecasting accuracy\nis improved.\nV. CONCLUSION\nIn this paper, a STLF model at EUTL with high penetration\nof EHLs is proposed based on a novel specialized feature\nindices set and SAE-ELM deep learning framework. Due to\nthe large power excursions and signiﬁcant randomness of\nEHLs, the raw load time series at EUTL exhibit a variety\nof irregularities, such as uncertainty, nonlinearity, and large\nvolatility. Therefore, a novel specialized feature indices set\nis proposed to analyze and extract deep diverse features of\nraw load time series to construct the training sample set. The\nnovel specialized feature indices set is proven to be helpful\nin improving the deep learning process and the capability\nand sensitivity in dealing with volatility. By using SAE-ELM,\nthe deep nonlinear features are learned automatically with no\niterative parameter turning and ﬁne-turning needed, which\nmakes the learning process much faster than the LSTM,\nBPNN, and MAM methods.\nAlthough deep learning is a promising framework for\ndemand forecasting, it lacks strong capability in extracting\nVOLUME 7, 2019 162705\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\ndeep features from highly volatile time series. Therefore,\nmore effective methods need to be further studied.\nREFERENCES\n[1] B. Li, J. Zhang, Y . He, and Y . Wang, ‘‘Short-term load-forecasting method\nbased on wavelet decomposition with second-order gray neural network\nmodel combined with ADF test,’’ IEEE Access, vol. 5, pp. 16324–16331,\n2017.\n[2] A. Tarsitano and I. L. Amerise, ‘‘Short-term load forecasting using a two-\nstage sarimax model,’’ Energy, vol. 133, pp. 108–114, Aug. 2017.\n[3] Q. Chen, F. Wang, B.-M. Hodge, J. Zhang, Z. Li, M. Shaﬁe-Khah, and\nJ. P. S. Catalão, ‘‘Dynamic price vector formation model-based automatic\ndemand response strategy for PV-assisted EV charging stations,’’ IEEE\nTrans. Smart Grid, vol. 8, no. 6, pp. 2903–2915, Nov. 2017.\n[4] F. Wang, K. Li, C. Liu, Z. Mi, M. Shaﬁe-Khah, and J. P. S. Catalão, ‘‘Syn-\nchronous pattern matching principle-based residential demand response\nbaseline estimation: Mechanism analysis and approach description,’’ IEEE\nTrans. Smart Grid, vol. 9, no. 6, pp. 6972–6985, Nov. 2018.\n[5] W. Kong, Z. Y . Dong, Y . Jia, D. J. Hill, Y . Xu, and Y . Zhang, ‘‘Short-term\nresidential load forecasting based on LSTM recurrent neural network,’’\nIEEE Trans. Smart Grid, vol. 10, no. 1, pp. 841–851, Jan. 2019.\n[6] Q. Chen, M. Xia, Y . Zhou, H. Cai, J. Wu, and H. Zhang, ‘‘Optimal plan-\nning for partially self-sufﬁcient microgrid with limited annual electricity\nexchange with distribution grid,’’IEEE Access, vol. 7, pp. 123505–123520,\n2019, doi: 10.1109/ACCESS.2019.2936762.\n[7] B. Yildiz, J. I. Bilbao, and A. B. Sproul, ‘‘A review and analysis of\nregression and machine learning models on commercial building electricity\nload forecasting,’’ Renew. Sustain. Energy Rev., vol. 73, pp. 1104–1122,\nJun. 2017.\n[8] J. J. Shah, M. C. Nielsen, T. S. Shaffer, and R. L. Fittro, ‘‘Cost-optimal\nconsumption-aware electric water heating via thermal storage under time-\nof-use pricing,’’ IEEE Trans. Smart Grid, vol. 7, no. 2, pp. 592–599,\nMar. 2015.\n[9] Q. Chen, N. Liu, C. Hu, L. Wang, and J. Zhang, ‘‘Autonomous energy\nmanagement strategy for solid-state transformer to integrate PV-assisted\nEV charging station participating in ancillary service,’’ IEEE Trans. Ind.\nInformat., vol. 13, no. 1, pp. 258–269, Feb. 2017.\n[10] D. Liu, L. Zeng, C. Li, K. Ma, Y . Chen, and Y . Cao, ‘‘A distributed short-\nterm load forecasting method based on local weather information,’’ IEEE\nSyst. J., vol. 12, no. 1, pp. 208–215, Mar. 2018.\n[11] H. Jiang, Y . Zhang, E. Muljadi, J. J. Zhang, and D. W. Gao, ‘‘A short-term\nand high-resolution distribution system load forecasting approach using\nsupport vector regression with hybrid parameters optimization,’’ IEEE\nTrans. Smart Grid, vol. 9, no. 4, pp. 3341–3350, Jul. 2018.\n[12] X. Sun, P. B. Luh, K. W. Cheung, W. Guan, L. D. Michel, S. S. Venkata, and\nM. T. Miller, ‘‘An efﬁcient approach to short-term load forecasting at the\ndistribution level,’’IEEE Trans. Power Syst., vol. 31, no. 4, pp. 2526–2537,\nJul. 2016.\n[13] J. Zhang, Y .-M. Wei, D. Li, Z. Tan, and J. Zhou, ‘‘Short term electricity\nload forecasting using a hybrid model,’’ Energy, vol. 158, pp. 774–781,\nSep. 2018.\n[14] Y . Wang, Q. Xia, and C. Kang, ‘‘Secondary forecasting based on deviation\nanalysis for short-term load forecasting,’’ IEEE Trans. Power Syst., vol. 26,\nno. 2, pp. 500–507, May 2011.\n[15] B. A. HØverstad, A. Tidemann, H. Langseth, and P. Öztürk, ‘‘Short-\nterm load forecasting with seasonal decomposition using evolution for\nparameter tuning,’’ IEEE Trans. Smart Grid, vol. 6, no. 4, pp. 1904–1913,\nJul. 2015.\n[16] K. Park, S. Yoon, and E. Hwang, ‘‘Hybrid load forecasting for mixed-use\ncomplex based on the characteristic load decomposition by pilot signals,’’\nIEEE Access, vol. 7, pp. 12297–12306, 2019.\n[17] C. Guan, P. B. Luh, L. D. Michel, Y . Wang, and P. B. Friedland, ‘‘Very\nshort-term load forecasting: Wavelet neural networks with data pre-\nﬁltering,’’IEEE Trans. Power Syst., vol. 28, no. 1, pp. 30–41, Feb. 2013.\n[18] S. Li, P. Wang, and L. Goel, ‘‘A novel wavelet-based ensemble method for\nshort-term load forecasting with hybrid neural networks and feature selec-\ntion,’’IEEE Trans. Power Syst., vol. 31, no. 3, pp. 1788–1798, May 2016.\n[19] S. Lahmiri, ‘‘Comparing variational and empirical mode decomposition\nin forecasting day-ahead energy prices,’’ IEEE Syst. J., vol. 11, no. 3,\npp. 1907–1910, Sep. 2017.\n[20] M. Khodayar, O. Kaynak, and M. E. Khodayar, ‘‘Rough deep neural\narchitecture for short-term wind speed forecasting,’’ IEEE Trans. Ind.\nInformat., vol. 13, no. 6, pp. 2770–2779, Jul. 2017.\n[21] G. E. Hinton, S. Osindero, and Y .-W. Teh, ‘‘A fast learning algorithm for\ndeep belief nets,’’ Neural Comput., vol. 18, no. 7, pp. 1527–1554, 2006.\n[22] A. Rahman, V . Srikumar, and A. D. Smith, ‘‘Predicting electricity con-\nsumption for commercial and residential buildings using deep recurrent\nneural networks,’’ Appl. Energy, vol. 212, pp. 372–385, Feb. 2018.\n[23] H. Wang, H. Yi, J. Peng, G. Wang, Y . Liu, H. Jiang, and W. Liu, ‘‘Deter-\nministic and probabilistic forecasting of photovoltaic power based on\ndeep convolutional neural network,’’ Energy Convers. Manage., vol. 153,\npp. 409–422, Dec. 2017.\n[24] L. Han, Y . Peng, Y . Li, B. Yong, Q. Zhou, and L. Shu, ‘‘Enhanced deep\nnetworks for short-term and medium-term load forecasting,’’ IEEE Access,\nvol. 7, pp. 4045–4055, 2018.\n[25] M. Cai, M. Pipattanasomporn, and S. Rahman, ‘‘Day-ahead building-level\nload forecasts using deep learning vs. traditional time-series techniques,’’\nAppl. Energy, vol. 236, pp. 1078–1088, Feb. 2019.\n[26] T. Ouyang, Y . He, H. Li, Z. Sun, and S. Baek, ‘‘Modeling and forecasting\nshort-term power load with copula model and deep belief network,’’ IEEE\nTrans. Emerg. Topics Comput. Intell., vol. 3, no. 2, pp. 127–136, Apr. 2019.\n[27] Z. Zhang, S. Li, Y . Xiao, and Y . Yang, ‘‘Intelligent simultaneous fault\ndiagnosis for solid oxide fuel cell system based on deep learning,’’ Appl\nEnergy, vols. 233–234, pp. 930–942, Jan. 2019.\n[28] Y . Chen, Z. Lin, X. Zhao, G. Wang, and Y . Gu, ‘‘Deep learning-based\nclassiﬁcation of hyperspectral data,’’ IEEE J. Sel. Topics Appl. Earth\nObservat. Remote Sens., vol. 7, no. 6, pp. 2094–2107, Jun. 2014.\n[29] H. Zhou, G. B. Huang, Z. Lin, H. Wang, and Y . C. Soh, ‘‘Stacked extreme\nlearning machines,’’ IEEE Trans. Cybern., vol. 45, no. 9, pp. 2013–2025,\nSep. 2015.\n[30] E. Cambria et al., ‘‘Extreme Learning Machines [Trends & Controver-\nsies],’’IEEE Intell. Syst., vol. 28, no. 6, pp. 30–59, Nov./Dec. 2013.\n[31] C. M. Wong, C. M. V ong, P. K. Wong, and J. Cao, ‘‘Kernel-based multi-\nlayer extreme learning machines for representation learning,’’ IEEE Trans.\nNeural Netw. Learn. Syst., vol. 29, no. 3, pp. 757–762, Mar. 2018.\n[32] Y . Yang, Q. M. J. Wu, Y . Wang, K. M. Zeeshan, X. Lin, and X. Yuan, ‘‘Data\npartition learning with multiple extreme learning machines,’’ IEEE Trans.\nCybern., vol. 45, no. 8, pp. 1463–1475, Aug. 2015.\n[33] J. Tang, C. Deng, and G.-B. Huang, ‘‘Extreme learning machine for\nmultilayer perceptron,’’ IEEE Trans. Neural Netw. Learn. Syst., vol. 27,\nno. 4, pp. 809–821, Apr. 2015.\n[34] L. Yao and Z. Ge, ‘‘Deep learning of semisupervised process data with\nhierarchical extreme learning machine and soft sensor application,’’ IEEE\nTrans. Ind. Electron., vol. 65, no. 2, pp. 1490–1498, Feb. 2018.\nQIFANG CHEN (S’13–M’17) received the B.S.\nand M.S. degrees in communication engineer-\ning and electric engineering respectively, from\nXiangtan University, Hunan, in 2010 and 2013,\nrespectively, and the Ph.D. degree in electrical\nengineering from North China Electric Power Uni-\nversity, Beijing, China, in 2017. He is currently a\nPostdoctoral Researcher with the School of Elec-\ntrical Engineering, Beijing Jiaotong University,\nBeijing, China. His research interests include inte-\ngrated energy systems, demand response, and ﬂexible load modeling and\ncontrol.\nMINGCHAO XIA (M’03–SM’17) received the\nB.S. and Ph.D. degrees in electrical engineer-\ning from Tsinghua University, Beijing, China,\nin 1998 and 2003, respectively. He is cur-\nrently a Professor with the School of Electri-\ncal Engineering, Beijing Jiaotong University. His\nresearch interests include the energy internet,\nsmart power distribution system control and opti-\nmization, power electronics in power distribution,\nand ﬂexible load control.\n162706 VOLUME 7, 2019\nQ. Chenet al.: STLF Based on Deep Learning for End-User Transformer Subject to Volatile EHLs\nTENG LU received the B.S. degree in electrical\nengineering from the China University of Min-\ning and Technology, Beijing, in 2018. She is cur-\nrently pursuing the M.S. degree with the School\nof Electrical Engineering, Beijing Jiaotong Uni-\nversity. Her research interest includes in modeling\nand control of ﬂexible loads in power grid.\nXICHEN JIANG (M’16) received the B.S., M.S.,\nand Ph.D. degrees in electrical engineering from\nthe University of Illinois, Urbana-Champaign,\nin 2010, 2012, and 2016, respectively. He is cur-\nrently an Assistant Professor with the Department\nof Engineering and Design, Western Washing-\nton University. His research interests include\ncontrols theory, power system reliability, and\ncyber-physical systems. In particular, his research\napplies statistical and set-theoretic methods to\ndetect and mitigate attacks or faults on the power system networks.\nWENXIA LIU(M’11) received the B.S. degree in\nradio engineering from the Nanjing University of\nScience and Technology, in 1990, the M.S. degree\nin power system and automation from Northeast\nElectric Power University, in 1995, and the Ph.D.\ndegree in power system and automation from\nNorth China Electric Power University (NCEPU),\nin 2009. She is currently a Professor with the\nSchool of Electrical and Electronic Engineering,\nNCEPU. Her research interests include in intel-\nligent planning of power systems, power system risk assessment, power\ncommunication systems, and reliability and planning of cyber-physical\nsystems (CPS).\nQINFEI SUN received the B.S., M.S., and Ph.D.\ndegrees in electrical engineering from the College\nof Information and Electrical Engineering, China\nAgricultural University, in 2010, 2012, and 2016,\nrespectively. He was a Visiting Ph.D. Student with\nthe Microgrids Research Program, Department of\nEnergy Technology, Aalborg University, Aalborg,\nDenmark, from 2014 to 2015. He is currently\nan Engineer with the State Grid Beijing Electric\nPower Research Institute. His research interests\ninclude power electronics for distributed generation, operation, control, and\npower quality of microgrid, demand-side response, and especially for electric\nheat and cooling supply of buildings.\nVOLUME 7, 2019 162707",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.637210488319397
    },
    {
      "name": "Computer science",
      "score": 0.6337637901306152
    },
    {
      "name": "Term (time)",
      "score": 0.5195496678352356
    },
    {
      "name": "Electrical engineering",
      "score": 0.21331718564033508
    },
    {
      "name": "Engineering",
      "score": 0.16387557983398438
    },
    {
      "name": "Voltage",
      "score": 0.13912740349769592
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21193070",
      "name": "Beijing Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I52669646",
      "name": "Western Washington University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I153473198",
      "name": "North China Electric Power University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I17442442",
      "name": "State Grid Corporation of China (China)",
      "country": "CN"
    }
  ]
}