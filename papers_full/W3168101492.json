{
    "title": "Shuffle Transformer: Rethinking Spatial Shuffle for Vision Transformer",
    "url": "https://openalex.org/W3168101492",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2652629056",
            "name": "Huang, Zilong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224276618",
            "name": "Ben, Youcheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1969323033",
            "name": "Luo Guo-zhong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1989990291",
            "name": "Cheng Pei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1980596719",
            "name": "Yu Gang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2138630399",
            "name": "Fu Bin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3015468748",
        "https://openalex.org/W2796438033",
        "https://openalex.org/W2965853874",
        "https://openalex.org/W3109301572",
        "https://openalex.org/W3156811085",
        "https://openalex.org/W3139773203",
        "https://openalex.org/W2787091153",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W2949718784",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W3211432419",
        "https://openalex.org/W3130071011",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2963125010",
        "https://openalex.org/W2964241181",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2940744433",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2942489928",
        "https://openalex.org/W3159778524",
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3040304705",
        "https://openalex.org/W2737258237",
        "https://openalex.org/W3110402800",
        "https://openalex.org/W3133630855",
        "https://openalex.org/W3135921327",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W3133696297",
        "https://openalex.org/W2599765304",
        "https://openalex.org/W3151130473"
    ],
    "abstract": "Very recently, Window-based Transformers, which computed self-attention within non-overlapping local windows, demonstrated promising results on image classification, semantic segmentation, and object detection. However, less study has been devoted to the cross-window connection which is the key element to improve the representation ability. In this work, we revisit the spatial shuffle as an efficient way to build connections among windows. As a result, we propose a new vision transformer, named Shuffle Transformer, which is highly efficient and easy to implement by modifying two lines of code. Furthermore, the depth-wise convolution is introduced to complement the spatial shuffle for enhancing neighbor-window connections. The proposed architectures achieve excellent performance on a wide range of visual tasks including image-level classification, object detection, and semantic segmentation. Code will be released for reproduction.",
    "full_text": "ShufÔ¨Çe Transformer: Rethinking Spatial ShufÔ¨Çe for\nVision Transformer\nZilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng, Gang Yu, Bin Fu\nTencent GY-Lab\n{zilonghuang, eugeneben, alexantaluo, peicheng, skicyyu, brianfu}@tencent.com\nAbstract\nVery recently, Window-based Transformers, which computed self-attention within\nnon-overlapping local windows, demonstrated promising results on image clas-\nsiÔ¨Åcation, semantic segmentation, and object detection. However, less study has\nbeen devoted to the cross-window connection which is the key element to improve\nthe representation ability. In this work, we revisit the spatial shufÔ¨Çe as an efÔ¨Åcient\nway to build connections among windows. As a result, we propose a new vision\ntransformer, named ShufÔ¨Çe Transformer, which is highly efÔ¨Åcient and easy to im-\nplement by modifying two lines of code. Furthermore, the depth-wise convolution\nis introduced to complement the spatial shufÔ¨Çe for enhancing neighbor-window\nconnections. The proposed architectures achieve excellent performance on a wide\nrange of visual tasks including image-level classiÔ¨Åcation, object detection, and\nsemantic segmentation. Code will be released for reproduction.\n1 Introduction\nRecently, Vision Transformers [11, 35, 4, 46] have been absorbed more and more investigations.\nDue to the great Ô¨Çexibility in modeling long-range dependencies in vision tasks, introducing less\ninductive bias, vision transformers have already achieved performances quite competitive with their\nCNN counterparts [17, 34] and can naturally process multi-modality input data including images,\nvideos, texts, speech signals, and point clouds.\nThese vision transformers have quadratic computation complexity to input image size due to com-\nputation of self-attention globally. This property makes it difÔ¨Åcult to apply vision transformers to\ndense prediction tasks, such as semantic segmentation, which inherently require high-resolution\nimages. A workaround is the window-based self-attention, where the input is spatially partitioned\ninto non-overlapped windows and the standard self-attention is computed only within each local\nwindow. On the one hand, it can signiÔ¨Åcantly reduce the complexity, On the other hand, limiting\nthe range to compute self-attention can make the model more efÔ¨Åcient to train [ 2, 48]. However,\ncomputing self-attention within non-overlapping windows results in a limited receptive Ô¨Åeld. To\nbuild information communication across windows, HaloNet [36] scales the window for collecting\ninformation inside and outside the original window. Swin [28] alleviates this issue by shifting the\nwindow partition between consecutive self-attention layers. Despite being effective, HaloNet and\nSwin both are not good at building long-range cross-window connections for enlarging receptive\nÔ¨Åelds efÔ¨Åciently.\nTo build the cross-window connections, especially long-range cross-window connections, we pay\nattention to window partitioning. Inspired by ShufÔ¨ÇeNet [45], which comes up with a novel channel\nshufÔ¨Çe operation to help the information Ô¨Çowing across feature channels, we introduce a spatial\nshufÔ¨Çe operation into the window-based self-attention module for providing connections among\nwindows and signiÔ¨Åcantly enhance modeling power. Different from channel shufÔ¨Çe, spatial shufÔ¨Çe\noperation will result in spatial misalignment between features and image content. Thus, we need an\nPreprint. Under review.\narXiv:2106.03650v1  [cs.CV]  7 Jun 2021\ninverse process of spatial shufÔ¨Çe operator to align the feature with image content, which is named as\nspatial alignment operator. In this paper, the window-based transformer block with a spatial shufÔ¨Çe\noperator and a spatial alignment operator, named as ShufÔ¨Çe Transformer Block, will be used as the\nbasic component to build the ShufÔ¨Çe Transformer.\nAlthough spatial shufÔ¨Çe is efÔ¨Åcient for building long-range cross-window connections, the ‚Äúgrid issue‚Äù\nmay arise when image size is far greater than the window size. To enhance the neighbor-window\nconnections, we introduce a depth-wise convolutional layer with a residual connection into the ShufÔ¨Çe\nTransformer Block. Finally, with the help of successive ShufÔ¨Çe Transformer Blocks, the proposed\nShufÔ¨Çe Transformer could make information Ô¨Çow across all windows.\nIn summary, our proposed ShufÔ¨Çe Transformer achieves linear computational complexity in the\nnumber of input tokens by computing self-attention within non-overlapping local windows. To\nbuild rich cross-window connections, we propose the ShufÔ¨Çe Transformer block which integrates\nthe spatial shufÔ¨Çe and neighbor-window connections. The experiments conducted on a number of\nvisual tasks, ranging from image-level classiÔ¨Åcation to pixel-level semantic/instance segmentation\nand object detection show that both of our proposed architectures perform favorably against other\nstate-of-the-art vision transformers with similar computational complexity.\n2 Related Work\nVision Transformers The transformer was Ô¨Årstly proposed by [ 37] for machine translation tasks\nand has dominated in natural language modeling. Recently, ViT [ 11] is the Ô¨Årst work to apply\na pure transformer to image classiÔ¨Åcation with state-of-the-art performance (e.g. ResNets [ 17],\nEfÔ¨ÅcientNet [34]) on image classiÔ¨Åcation when the data is large enough. It splits each image into\na sequence of tokens and then applies multiple standard Transformer layers to model their global\nrelation for classiÔ¨Åcation. SpeciÔ¨Åcally, the standard Transformer layers consist of a Multi-head\nSelf-Attention module (MSA) and a Multiple Layer Perceptron (MLP). Following that, DeiT [35]\nintroduces token-based distillation to reduce the data necessary for training the transformer. T2T-\nViT [43] structures the image to tokens by recursively aggregating neighboring tokens into one token\nto reduce tokens length. Transformer-iN-Transformer (TNT) [14] utilizes both an outer Transformer\nblock that processes the patch embeddings, and an inner Transformer block that models the relation\namong pixel embeddings, to model both patch-level and pixel-level representation.\nTo produce a hierarchical representation that is required by dense prediction tasks such as object\ndetection and segmentation, Pyramid Vision Transformer (PVT) [39], is proposed and can output the\nfeature pyramid [26] as in convolutional neural networks (CNNs).\nTo reduce the complexity, The recent Swin Transformer [28] introduces non-overlapping window\npartitions and restricts self-attention within each local window, resulting in linear computational\ncomplexity in the number of input tokens.\nIntegrating Convolution and Vision TransformerTo enhance local context in Vision Transformers,\nthe Conditional Position encodings Visual Transformer (CPVT) [9] replaces the predeÔ¨Åned positional\nembedding used in ViT with conditional position encodings (CPE), enabling Transformers to process\ninput images of arbitrary size without interpolation. CvT [40] and CCT [15] introducing convolutions\ninto the Vision Transformer architecture to merge the beneÔ¨Åts of Transformers with the beneÔ¨Åts of\nCNNs for image recognition tasks. LocalViT [25] brings a locality mechanism to vision transformers\nby inserting introducing depth-wise convolutions into the MLP module. We also introduce convolution\ninto the window-based vision transformer for enhancing the neighbor-window connections. Different\nfrom the others, we place the depth-wise convolution between the window-based multi-head self-\nattention and the MLP module.\nWindow-based Self-Attention The standard Transformer architecture [ 37] and its adaptation for\nimage classiÔ¨Åcation [11] both conduct global self-attention, where the relationships for each token-\npair are computed. The computation for dense relationships leads to quadratic complexity with\nrespect to the number of tokens, making it unsuitable for the dense prediction tasks, such as semantic\nsegmentation, inherently requires high-resolution images.\nFor efÔ¨Åcient modeling, there are some works apply local/window constraints to self-attention,\nproposed by [ 21, 18, 31, 38, 7, 28, 19, 8], reduces the computation cost. These window-based\nself-attention based methods can be grouped into two categories: sliding-window based meth-\n2\n(a)\nTokens\nWMSA1\nInput\noutput1\nWMSA2\noutput2 (b)\nTokens Tokens\n(c)\nSpatialShuffle\nSpatial Alignment\nFigure 1: Spatial shufÔ¨Çe with two stacked window-based Transformer block. The MLP is omitted\nin the visualization because it does not affect the information interaction in the spatial dimension.\nWMSA stands for window-based multi-head self-attention. a) two stacked window-based Transformer\nblocks with the same window size. Each output token only relates to the tokens within the window.\nNo cross-talk; b) tokens from different windows are fully related when WMSA2 takes data from\ndifferent windows after WMSA1; c) an equivalent implementation to b) using spatial shufÔ¨Çe and\nalignment.\nods [21, 18, 31, 38, 7] and non-overlapping window-based methods [ 28, 19]. As mentioned in\nSwin [28], compared with non-overlapping window-based methods, sliding window-based self-\nattention approaches suffer from low latency on general hardware due to different key sets for\ndifferent query pixels. However, non-overlapping window-based methods compute self-attention\nin the non-overlapping local window, which lacks cross-window connections. Swin [28] alleviates\nthis issue by shifting the window partition between consecutive self-attention layers. The proposed\nShufÔ¨Çe Transformer also utilizes non-overlapping window-based self-attention and takes spatial\nshufÔ¨Çe to make information Ô¨Çow across windows.\nChannel ShufÔ¨Çe and Spatial ShufÔ¨Çe Modern efÔ¨Åcient convolutional neural networks [45, 13, 32]\napply group or depth-wise convolutions to reduce the complexity, which meets the issue of channel\nsparse connections. To exchange information of channels from different groups, ShufÔ¨ÇeNet shufÔ¨Çes\nthe channel to make cross-group information Ô¨Çow for multiple group convolution layers.\nFollowing that, Spatially ShufÔ¨Çed Convolution [ 24] incorporate the random spatial shufÔ¨Çe in the\nregular convolution. The most related work to ours is Interlaced Sparse Self-Attention (ISSA) [19],\nwhich apply the interlacing mechanism to decompose the dense afÔ¨Ånity matrix within the self-\nattention mechanism with the product of two sparse afÔ¨Ånity matrices. However, the number of\nwindows is Ô¨Åxed in ISSA, which also leads to quadratic complexity with respect to the input size.\nWe Ô¨Åx the window size rather than the number of windows, which results in linear computational\ncomplexity. Besides, the motivations are also different. We adapt spatial shufÔ¨Çe for window-based\nvision transformers to bridge the connection among the non-overlapping windows. ISSA uses two\nsparse self-attention to mimic the global self-attention for semantic segmentation.\n3 ShufÔ¨Çe Transformer\nIn this section, we start from the standard window-based multi-head self-attention. To build long-\nrange cross-window talks, the spatial shufÔ¨Çe is proposed. The neighbor-window connection module\nis used to enhance the connections of neighborhood windows. Then, we integrate the spatial shufÔ¨Çe\nand neighbor-window connection module into the shufÔ¨Çe transformer block for building rich cross-\nwindow connections. Finally, the overall network architecture with its variants is given.\n3.1 Window-based Multi-head Self-Attention\nFor efÔ¨Åcient modeling, there are some works that apply local constraints to self-attention, proposed\nby [18, 31, 38, 28], reduces the computation cost. Window-based Multi-head Self-Attention (WMSA)\n3\nNormWMSA\nNormMLP\nNWC\nNormShuffleWMSA\nNormMLP\nNWC\nFigure 2: Two successive ShufÔ¨Çe Transformer Block. The WMSA and ShufÔ¨Çe WMSA are window-\nbased multi-head self attention without/with spatial shufÔ¨Çe, respectively.\nis proposed to compute self-attention within local windows. The windows are arranged to evenly\npartition the image in a non-overlapping manner. Supposing each window contains M √óM tokens,\nthe computational complexity of a global MSA module and a window-based one on an input of\nH √óW tokens with dimension C are O(H2W2C) and O(M2HWC), respectively. Thus, the\nWindow-based Multi-head Self-Attention is signiÔ¨Åcantly more efÔ¨Åcient when M ‚â™H and M ‚â™W\nand grows linearly with HW if M is Ô¨Åxed. Compared with a global MSA module, the WMSA\nmodule needs a window partition operation before computing self-attention and a windows-to-image\noperation after computing self-attention. However, the computation cost of the additional operations\nis negligible in practical implementation. Except for the window partition, WMSA shares the same\nstructure with the global MSA module.\n3.2 Spatial ShufÔ¨Çe for Cross-Window Connections\nAlthough the window-based multi-head self-attention is computation friendly. However, the image is\ndivided into non-overlapping sub-windows. If multiple window-based self-attention modules stack\ntogether, there is one side effect: the receptive Ô¨Åeld is limited within the window, which has an\nadverse effect on tasks such as segmentation, especially on high-resolution inputs. Fig 1(a) illustrates\na situation of two stacked window-based self-attention. It is clear that outputs from a certain window\nonly relate to the inputs within the window. This property blocks information Ô¨Çow among windows\nand weakens representation.\nTo address the issue, a straightforward solution is to allow the second window-based self-attention\nto obtain input data from different windows (as shown in Fig 1(b)), the tokens in different windows\nwill be related. Inspired by ShufÔ¨ÇeNet [45], this can be efÔ¨Åciently and elegantly implemented by a\nspatial shufÔ¨Çe operation (Fig 1 (c)). Without loss of generality, input is assumed as the 1D sequence.\nSuppose a Window-based Self-Attention with window size M whose input has N tokens; we Ô¨Årst\nreshape the output spatial dimension into (M, N\nM ), transposing and then Ô¨Çattening it back as the input\nof the next layer. This kind of operation puts the tokens from distant windows together and helps to\nbuild long-range cross-window connections. Different from channel shufÔ¨Çe, spatial shufÔ¨Çe needs\nthe spatial alignment operation to adjust the spatial tokens into the original positions for aligning\nfeatures and image content spatially. The spatial alignment operation Ô¨Årst reshapes the output spatial\ndimension into ( N\nM ,M), transposing and then Ô¨Çattening it, which is an inverse process of the spatial\nshufÔ¨Çe.\nConsidering there are always the window partition operation and windows-to-image operation before\nand after computing self-attention, we could merge the window partition operation with the spatial\nshufÔ¨Çe operation, windows-to-image operation with the spatial alignment operation, thus the spatial\nshufÔ¨Çe operation and spatial alignment do not bring extra computation and are easy to implement by\nmodifying two lines of code. Moreover, spatial shufÔ¨Çe is also differentiable, which means it can be\nembedded into network structures for end-to-end training.\n3.3 Neighbor-Window Connection Enhancement\nBringing the spatial shufÔ¨Çe into the window-based multi-head self-attention could build cross-window\nconnections, especially long-range cross-window. However, there is an underlying apprehension\nwhen processing a high-resolution image. The ‚Äúgrid issue‚Äù may arise when image size is far greater\nthan the window size.\n4\nImages\nTokenEmbeddingShuffleTransformerBlock\nTokenMergingShuffleTransformerBlock\nTokenMergingShuffleTransformerBlock\nTokenMergingShuffleTransformerBlock\nùêª√óùëä√ó3\nùêª4√óùëä4√óùê∂ ùêª8√óùëä8√ó2ùê∂ ùêª16√óùëä16√ó4ùê∂ ùêª32√óùëä32√ó8ùê∂\n√ó2\nStage1 Stage2 Stage3 Stage4\n√ó2 √ó6 √ó2\nFigure 3: The architecture of a ShufÔ¨Çe Transformer (ShufÔ¨Çe-T).\nFortunately, there are several approaches to this issue by enhancing the neighbor-window connections.\n1) scale the window size [36]. 2) cooperate with shifted window [28]. 3) introduce convolution to\nshufÔ¨Çe transformer block [25, 40]. Considering the efÔ¨Åciency, we insert a depth-wise convolution\nlayer with a residual connection between the WMSA module and the MLP module. The kernel size\nof the depth-wise convolution layer is the same as the window size. This operator could strengthen\nthe information Ô¨Çow among nearby windows and alleviate the ‚Äúgrid issue‚Äù. To Ô¨Ånd a better place for\na depth-wise convolution layer, we conduct the ablation studies, as shown in Table 4.\n3.4 ShufÔ¨Çe Transformer Block\nThe ShufÔ¨Çe Transformer Block consists of the ShufÔ¨Çe Multi-Head Self-Attention module (ShufÔ¨Çe-\nMHSA), the Neighbor-Window Connection module (NWC), and the MLP module. To introduce\ncross-window connections while maintaining the efÔ¨Åcient computation of non-overlapping windows,\nwe propose a strategy which alternates between WMSA and ShufÔ¨Çe-WMSA in consecutive ShufÔ¨Çe\nTransformer blocks. As shown in Figure 2. The Ô¨Årst window-based transformer block uses regular\nwindow partition strategy and the second window-based transformer block uses window-based self-\nattention with spatial shufÔ¨Çe. Besides, the Neighbor-Window Connection moduel (NWC) is added\ninto each block for enhancing connections among neighborhood windows. Thus the proposed shufÔ¨Çe\ntransformer block could build rich cross-window connections and augments representation. Finally,\nthe consecutive ShufÔ¨Çe Transformer blocks are computed as\nxl = WMSA(BN(zl‚àí1)) +zl‚àí1,\nyl = NWC(xl) +xl,\nzl = MLP(BN(yl)) +yl,\nxl+1 = ShufÔ¨Çe-WMSA(BN(zl)) +zl,\nyl+1 = NWC(xl+1) +xl+1,\nzl+1 = MLP(BN(yl+1)) +yl+1.\nwhere zl, yl and zl denote the output features of the (ShufÔ¨Çe-)WMSA module, the Neighbor-Window\nConnection module and the MLP module for blockl, respectively; WMSA and ShufÔ¨Çe-WMSA denote\nwindow-based multi-head self-attention without/with spatial shufÔ¨Çe, respectively. To better handle 2D\ninput, we adapt the standard transformer block by replacing the Layernorm [1] with Batchnorm [23].\nMeanwhile, the Linear layer in ShufÔ¨Çe WMSA and MLP is changed to a convolutional layer with\nkernel size 1 √ó1.\n3.5 Architecture and Variants\nAn overview of the ShufÔ¨Çe Transformer architecture is presented in Figure 3, which illustrates the tiny\nversion (ShufÔ¨Çe-T). The ShufÔ¨Çe Transformer consists of a token embedding layer and several shufÔ¨Çe\ntransformer blocks and token merging layers. In our implementation, we use two stacked convolution\nlayers as token embedding layers. To produce a hierarchical representation, we use a convolutional\nlayer with kernel size 2 √ó2 and stride 2 as the token merging layer to reduce the number of tokens.\nFor a fair comparison, we follow the settings of swin [28]. We build our base model, called ShufÔ¨Çe-\nB, to have the model size and computation complexity similar to Swin-B/ViTB/DeiT-B. We also\n5\nTable 1: Comparison of different backbones on ImageNet-1KclassiÔ¨Åcation. Throughput is measured\nwith the batch size of 192 on a single V100 GPU. All models are trained and evaluated on 224 √ó224\nresolution.\nMethod Params GFLOPs Throughput (Images / s) Top-1 (%)\nConvNet\nReGNetY-4G [30] 21M 4.0 1157 80.0\nReGNetY-8G [30] 39M 8.0 592 81.7\nReGNetY-16G [30] 84M 16.0 335 82.9\nTransformer\nDeiT-S/16 [35] 22M 4.6 437 79.9\nCrossViT-S [5] 27M 5.6 - 81.0\nT2T-ViT-14 [43] 22M 5.2 - 81.5\nTNT-S [14] 24M 5.2 - 81.3\nPVT-Small [39] 25M 3.8 820 79.8\nSwin-T [28] 29M 4.5 766 81.3\nShufÔ¨Çe-T (ours) 29M 4.6 791 82.5\nT2T-ViT-19 [43] 39M 8.9 - 81.9\nPVT-Medium [39] 44M 6.7 526 81.2\nSwin-S [28] 50M 8.7 444 83.0\nShufÔ¨Çe-S (ours) 50M 8.9 450 83.5\nViT-B/16 [11] 87M 17.6 86 77.9\nDeiT-B/16 [35] 87M 17.6 292 81.8\nT2T-ViT-24 [43] 64M 14.1 - 82.3\nCrossViT-B [5] 105M 21.2 - 82.2\nTNT-B [14] 66M 14.1 - 82.8\nPVT-Large [39] 61M 9.8 367 81.7\nSwin-B [28] 88M 15.4 275 83.3\nShufÔ¨Çe-B (ours) 88M 15.6 279 84.0\nintroduce ShufÔ¨Çe-T and ShufÔ¨Çe-S, which are similar to Swin-T and Swin-S, respectively. The window\nsize is set to M = 7by default. The query dimension of each head is d = 32, and the expansion\nlayer of each MLP is Œ±= 4, for all experiments. The architecture hyper-parameters of these model\nvariants are: ShufÔ¨Çe-T: C = 96, layer numbers = {2, 2, 6, 2}. ShufÔ¨Çe-S: C = 96, layer numbers =\n{2, 2, 18, 2}. ShufÔ¨Çe-B: C = 128, layer numbers = {2, 2, 18, 2}.\n4 Experiments\nTo showcase the effectiveness of our approach, we conduct comprehensive experiments on three\ndifferent tasks: ImageNet-1Kimage classiÔ¨Åcation [10], ADE20Ksemantic segmentation [47] and\nCOCO instance segmentation [27].\n4.1 ClassiÔ¨Åcation on ImageNet-1K\nSettings For image classiÔ¨Åcation, we evaluate the proposed ShufÔ¨Çe Transformer on ImageNet-\n1K[10], which contains 1.28M training images and 50Kvalidation images from 1,000 classes. We\nfollow [28] to adopt regular 1Ksetting and report the top-1 accuracy on a single crop. To be speciÔ¨Åc,\nwe employ an AdamW [29] optimizer for 300 epochs using a cosine decay learning rate scheduler\nand 20 epochs of linear warm-up. A batch size of 1,024, an initial learning rate of 0.001, and a\nweight decay of 0.05 are used. We also utilize the same augmentation and regularization strategies as\nshown in [28] for a fair comparison.\nResults Table 1 presents detailed comparisons between our ShufÔ¨Çe Transformer and other backbones\nused for the training of ImageNet classiÔ¨Åcation, including both ConvNet-based and Transformer-\nbased approaches. All models are trained and evaluated on 224 √ó224 resolution. Compared to the\nstate-of-the-art ConvNets, i.e. RegNet [30], our ShufÔ¨Çe Transformer achieves a better speed-accuracy\n6\nTable 2: Results of semantic segmentation on the ADE 20Kvalidation set. ‚Ä†indicates that the model\nis pretrained on ImageNet-22K. FLOPs is measured on 1024 √ó1024 resolution. ‚àóindicates the FPS\nreproduced by us and is measured on 512 √ó512 resolution.\nMethod Backbone Params GFLOPs FPS mIoU / MS mIoU (%)\nDANet [12] ResNet-101 69M 1119 15.2 43.6 / 45.2\nCCNet [20] ResNet-101 64M 981 15.0 44.3 / 45.7\nDeepLabv3+ [6] ResNet-101 63M 1021 16.0 45.5 / 46.4\nAlignSeg [22] ResNet-101 67M 956 20.0 44.7 / 46.0\nOCRNet [44] ResNet-101 56M 923 19.3 - / 45.3\nUperNet [41] ResNet-101 86M 1029 20.1 43.8 / 44.9\nOCRNet [44] HRNet-w48 71M 664 12.5 - / 45.7\nDeepLabv3+ [6] ResNeSt-101 66M 1051 11.9 - / 46.9\nSETR [46] ViT-Large ‚Ä† 308M - - 48.6 / 50.3\nUperNet Swin-T [28] 60M 945 28.6 ‚àó 44.5 / 45.8\nShufÔ¨Çe-T (ours) 60M 949 30.1 ‚àó 46.6/47.6\nUperNet Swin-S [28] 81M 1038 21.9 ‚àó 47.6 / 49.5\nShufÔ¨Çe-S (ours) 81M 1044 22.6 ‚àó 48.4/49.6\nUperNet Swin-B [28] 121M 1188 19.9 ‚àó 48.1 / 49.7\nShufÔ¨Çe-B (ours) 121M 1196 21.4 ‚àó 49.0/50.5\ntrade-off. Meanwhile, when compared to existing Transformer-based approaches, e.g. Swin [28], our\nShufÔ¨Çe Transformer stably outperforms the counterparts with similar computational complexity.\n4.2 Semantic Segmentation on ADE20K\nSettings ADE20K[47] is a challenging scene parsing dataset for semantic segmentation. There are\n150 classes and diverse scenes with 1,038 image level labels. The dataset is divided into three sets\n(20,210 / 2,000 / 3,352) for training, validation and testing.\nFor fair comparison with the recent state-of-the-art model Swin [ 28], we follow it to adopt Uper-\nNet [41] as the base framework for training. The AdamW [29] optimizer with an initial learning rate\nof 6 √ó10‚àí5 and a weight decay of 0.01 is used. We also utilize the warm-up during the Ô¨Årst 1,500\niterations. All models are trained for 160K iterations with a batch size of 16. Data augmentation\ncontains random horizontal Ô¨Çip, random resizing with a scale range of[0.5,2.0], and random cropping\nwith a crop size of 512 √ó512. For quantitative evaluation, the mean of class-wise intersection-over-\nunion (mIoU) is used for accuracy comparison, and the number of Ô¨Çoat-point operations (FLOPs)\nand frames per second (FPS) are adopted for speed comparison. Results of both single-scale and\nmulti-scale testing are reported with the scaling factor ranging from 0.5 to 1.75.\nResults As shown in Table 2, we prepare different variants of the ShufÔ¨Çe Transformer for detailed\ncomparison with Swin [28]. The results show that our three models (ShufÔ¨Çe-T/S/B) consistently\nachieve better mIoU performance than Swin with comparable inference speed. To be speciÔ¨Åc, under\nmulti-scale testing, ShufÔ¨Çe-T outperforms Swin-T by 1.4% mIoU, ShufÔ¨Çe-B achieves new state-\nof-the-art result 50.5% mIoU which outperforms Swin-B by 0.8% mIoU. ShufÔ¨Çe-S also achieves\ncomparable performance to Swin-S.\n4.3 Instance Segmentation on COCO\nSettings Experiments of object detection and instance segmentation are conducted on COCO 2017\ndataset [27], which contains 118Ktraining, 5Kvalidation and 41Ktest images. We follow [28] to\nevaluate the performance of our method based on Mask R-CNN and Cascade Mask R-CNN [16, 3].\nTo be speciÔ¨Åc, we replace the backbones of these detectors with our shufÔ¨Çe transformer blocks. All\nthe models are trained under the same setting as in [ 28]: multi-scale training [ 4, 33] (resizing the\ninput such that the shorter side is between 480 and 800 while the longer side is at most 1,333),\nAdamW [29] optimizer (initial learning rate of 0.0001, weight decay of 0.05, and batch size of 16),\nand 3√óschedule (36 epochs).\n7\nTable 3: Object detection and instance segmentation performance on the COCO val2017 dataset\nusing the Mask R-CNN and Cascade Mask R-CNN framework. FLOPs is evaluated on 1280 √ó800\nresolution.\nBackbone APb APb\n50 APb\n75 APm APm\n50 APm\n75 Params GFLOPs\nMask R-CNN\nResNet50 [17] 41.0 61.7 44.9 37.1 58.4 40.1 44M 260\nPVT-Small [39] 43.0 65.3 46.9 39.9 62.5 42.8 44M 245\nSwin-T [28] 46.0 68.2 50.2 41.6 65.1 44.8 48M 264\nShufÔ¨Çe-T(ours) 46.8 68 .9 51 .5 42.3 66 .0 45 .6 48M 268\nResNet101 [17] 42.8 63.2 47.1 38.5 60.1 41.3 63M 336\nPVT-Medium [39] 44.2 66.0 48.2 40.5 63.1 43.5 64M 302\nSwin-S [28] 48.5 70 .2 53 .5 43.3 67.3 46.6 69M 354\nShufÔ¨Çe-S(ours) 48.4 70.1 53.5 43.3 67 .3 46 .7 69M 359\nCascade Mask R-CNN\nDeiT-S [35] 48.0 67.2 51.7 41.4 64.2 44.3 80M 889\nResNet50 [17] 46.3 64.3 50.5 40.1 61.7 43.4 82M 739\nSwin-T [28] 50.5 69.3 54.9 43.7 66.6 47.1 86M 745\nShufÔ¨Çe-T(ours) 50.8 69 .6 55 .1 44.1 66 .9 48 .0 86M 746\nResNext101-32 [42] 48.1 66.5 52.4 41.6 63.9 45.2 101M 819\nSwin-S [28] 51.8 70.4 56.3 44.7 67.9 48.5 107M 838\nShufÔ¨Çe-S(ours) 51.9 70 .9 56 .4 44.9 67.8 48.6 107M 844\nResNext101-64 [42] 48.3 66.4 52.3 41.7 64.0 45.1 140M 972\nSwin-B [28] 51.9 70.9 56.5 45.0 68.4 48.7 145M 982\nShufÔ¨Çe-B(ours) 52.2 71 .3 57 .0 45.3 68 .5 48 .9 145M 989\nTable 4: Ablation study on the effect of spatial shufÔ¨Çe and the neighbor-window connection on two\nbenchmarks, FLOPs is measured on 224 √ó224 resolution.\nMethod Params GFLOPs ImageNet Top-1 (%) ADE 20KmIoU (%)\nw/o shufÔ¨Çe 28M 4.5 80.2 41.7\nw/ shufÔ¨Çe 28M 4.5 81.5 43.8\nw/ shufÔ¨Çe & NWC 29M 4.6 82.5 46.6\nResults As shown in Table 3, we compare our model to standard ConvNets, e.g. ResNe(X)t [42],\nas well as existing Transformer networks, e.g. Swin [ 28], DeiT [35] and PVT-Medium [39]. For\ncomparison with the Mask R-CNN framework, ShufÔ¨Çe-T and ShufÔ¨Çe-S surpass ConvNets based\nmethods and DeiT [35] and PVT-Medium [39] by a large margin with comparable parameters size and\nGFLOPs. Besides, Compared with Swin Transformer, ShufÔ¨Çe Transformer achieve comparable results\non all metrics. For comparison with the Cascade Mask R-CNN framework, ShufÔ¨Çe Transformer\ncould stably outperform the other networks in the aspects of APb and APm. The results indicate\nthe effectiveness of the proposed ShufÔ¨Çe Transformer on the tasks of object detection and instance\nsegmentation.\n4.4 Ablation Studies\nWe also perform comprehensive studies on the effectiveness of different design modules in our\nproposed ShufÔ¨Çe Transformer (ShufÔ¨Çe-T), using ImageNet-1Kimage classiÔ¨Åcation and UperNet on\nTable 5: Ablation study on different ways to spatial shufÔ¨Çe on two benchmarks.\nMethod ImageNet Top-1 (%) ADE 20KmIoU (%)\nlong-range spatial shufÔ¨Çe 82.5 46.6\nshort-range spatial shufÔ¨Çe 81.9 45.2\nrandom spatial shufÔ¨Çe 82.0 45.7\n8\nNorm\nShuffleWMSA\nNormLinearLinearA B C\nPosition Params GFLOPs ImageNet ADE20K\nTop-1 (%) mIoU (%)\nw/o NWC28.3M 4.5 81.5 43.8\nA 28.5M 4.6 81.9 46.2\nB 28.5M 4.6 82.5 46.6\nC 29.2M 4.8 82.3 46.3\nFigure 4: Left: Visualization of three different positions to insert the neighbor-window connection.\nA: before the shufÔ¨Çe WMSA; B: after the residual connection of the shufÔ¨Çe WMSA; C: inside the\nMLP block. Right: Ablation study on the effect of the neighbor-window connection inserted at\ndifferent positions, where A, B and C refer to three positions depicted left, and w/o NWCmeans no\nneighbor-window connection is inserted. FLOPs is measured on 224 √ó224 resolution.\nADE20Ksemantic segmentation. To be speciÔ¨Åc, we take the following aspects into consideration,\ni.e., the design of spatial shufÔ¨Çe operation, and the usage of the neighbor-window connection at\ndifferent positions of the transformer block.\nThe effect of the spatial shufÔ¨Çe & neighbor-window connection Here, we discuss the inÔ¨Çuence\nof the spatial shufÔ¨Çe operation and the neighbor-window connection. Ablations of the spatial shufÔ¨Çe\noperation and the neighbor-window connection on the two tasks are reported in Table 4. ShufÔ¨Çe-T\nwith the spatial shufÔ¨Çe outperforms the counterpart built on a regular window-based multi-head\nself-attention used at each stage by +1.3% top-1 accuracy on ImageNet-1K, +2.1 mIoU on ADE20K.\nThe results indicate the effectiveness of using the spatial shufÔ¨Çe to build connections among windows\nin the preceding layers. Besides, adding the neighbor-window connection can further bring +1.0 %\ntop-1 accuracy on ImageNet-1K, +2.8 mIoU on ADE20K. The results indicate the effectiveness of\nusing the neighbor-window connection to enhance the connection among neighborhood windows.\nThe effect of the way to spatial shufÔ¨Çe Here, we discuss the inÔ¨Çuence of the way to spatial shufÔ¨Çe.\nThree kinds of spatial shufÔ¨Çes will be discussed. 1) long-range spatial shufÔ¨Çe is introduced in\nsubsection 3.2. 2) Short-range spatial shufÔ¨Çe reshapes the output spatial dimension into ( N\n2M ,M, 2),\ntransposing and then Ô¨Çattening it. 3) random spatial shufÔ¨Çe will randomly shufÔ¨Çe the spatial dimension.\nAs shown in Table 5, the long-range spatial shufÔ¨Çe achieves best performance on both image\nclassiÔ¨Åcation and segmentation tasks, which demonstrates the effectiveness of long-range spatial\nshufÔ¨Çe. And surprisingly, the random spatial shufÔ¨Çe also can achieve comparable performance.\nThe effect of the position to insert the neighbor-window connection We discuss the inÔ¨Çuence of\ninserting the neighbor-window connection at different positions of the transformer block. As shown\nin Figure 4, there are three positions A, B, and C used to place the neighbor-window connection\n(NWC), a depth-wise convolution layer with a residual connection. The results indicate that inserting\nthe neighbor-window connection between the shufÔ¨Çe WMSA and the MLP bock achieves the best\nperformance.\n5 Conclusion\nIn this paper, we have presented a ShufÔ¨Çe Transformer for a number of visual tasks, ranging from\nimage-level classiÔ¨Åcation to pixel-level semantic/instance segmentation and object detection. For\nefÔ¨Åcient modeling, we use the window-based multi-head self-attention which computes self-attention\nwithin the non-overlapping windows. To build cross-window connections, we introduce spatial\nshufÔ¨Çe into window-based multi-head self-attention. Meanwhile, to enhance the neighbor-window\nconnections, we introduce a depth-wise convolutional layer with a residual connection into the ShufÔ¨Çe\nTransformer Block. Finally, with the help of successive ShufÔ¨Çe Transformer Block, the proposed\nShufÔ¨Çe Transformer could make information Ô¨Çow across all windows. Extensive experiments\nshow that both of our proposed architectures perform favorably against other state-of-the-art vision\ntransformers with similar computational complexity. The simplicity and strong performance suggest\nthat our proposed architectures may serve as stronger backbones for many vision tasks.\n9\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016. 5\n[2] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150, 2020. 1\n[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 6154‚Äì6162, 2018.\n7\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision,\npages 213‚Äì229. Springer, 2020. 1, 7\n[5] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit: Cross-attention multi-scale vision transformer\nfor image classiÔ¨Åcation. arXiv preprint arXiv:2103.14899, 2021. 6\n[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmentation. In ECCV, pages 801‚Äì818, 2018. 7\n[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019. 2, 3\n[8] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. arXiv preprint\narXiv:2104.13840, 2021. 2\n[9] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nConditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021. 2\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248‚Äì255.\nIeee, 2009. 6\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 2, 6\n[12] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention\nnetwork for scene segmentation. In CVPR, pages 3146‚Äì3154, 2019. 7\n[13] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features\nfrom cheap operations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1580‚Äì1589, 2020. 3\n[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer.\narXiv preprint arXiv:2103.00112, 2021. 2, 6\n[15] Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and Humphrey Shi. Escaping\nthe big data paradigm with compact transformers. arXiv preprint arXiv:2104.05704, 2021. 2\n[16] Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick. Mask r-cnn. In ICCV, pages 2961‚Äì2969,\n2017. 7\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, pages 770‚Äì778, 2016. 1, 2, 8\n[18] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 3464‚Äì3473, 2019. 2, 3\n[19] Lang Huang, Yuhui Yuan, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Interlaced sparse\nself-attention for semantic segmentation. arXiv preprint arXiv:1907.12273, 2019. 2, 3\n[20] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In ICCV, pages 603‚Äì612, 2019. 7\n10\n[21] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and Thomas S.\nHuang. Ccnet: Criss-cross attention for semantic segmentation. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2020. 2, 3\n[22] Zilong Huang, Yunchao Wei, Xinggang Wang, Wenyu Liu, Thomas S Huang, and Honghui Shi. Alignseg:\nFeature-aligned segmentation networks. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n2021. 7\n[23] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing\ninternal covariate shift. In ICML, pages 448‚Äì456, 2015. 5\n[24] Ikki Kishida and Hideki Nakayama. Incorporating horizontal connections in convolution by spatial\nshufÔ¨Çing. In https://openreview.net/forum?id=SkgODpVFDr, 2020. 3\n[25] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc Van Gool. Localvit: Bringing locality to\nvision transformers. arXiv preprint arXiv:2104.05707, 2021. 2, 5\n[26] Tsung-Yi Lin, Piotr Doll√°r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2117‚Äì2125, 2017. 2\n[27] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740‚Äì755, 2014. 6,\n7\n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin\ntransformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030,\n2021. 1, 2, 3, 5, 6, 7, 8\n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017. 6, 7\n[30] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll√°r. Designing network\ndesign spaces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 10428‚Äì10436, 2020. 6\n[31] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens.\nStand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909, 2019. 2, 3\n[32] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mobilenetv2:\nInverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 4510‚Äì4520, 2018. 3\n[33] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li,\nZehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals.\narXiv preprint arXiv:2011.12450, 2020. 7\n[34] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model scaling for convolutional neural networks. In\nInternational Conference on Machine Learning, pages 6105‚Äì6114. PMLR, 2019. 1, 2\n[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv√©\nJ√©gou. Training data-efÔ¨Åcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877, 2020. 1, 2, 6, 8\n[36] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki Parmar, Blake Hechtman, and Jonathon\nShlens. Scaling local self-attention for parameter efÔ¨Åcient visual backbones. arXiv preprint\narXiv:2103.12731, 2021. 1, 5\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017. 2\n[38] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-\ndeeplab: Stand-alone axial-attention for panoptic segmentation. In European Conference on Computer\nVision, pages 108‚Äì126. Springer, 2020. 2, 3\n[39] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\narXiv preprint arXiv:2102.12122, 2021. 2, 6, 8\n11\n[40] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021. 2, 5\n[41] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. UniÔ¨Åed perceptual parsing for scene\nunderstanding. In Proceedings of the European Conference on Computer Vision (ECCV), pages 418‚Äì434,\n2018. 7\n[42] Saining Xie, Ross Girshick, Piotr Doll√°r, Zhuowen Tu, and Kaiming He. Aggregated residual transforma-\ntions for deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 1492‚Äì1500, 2017. 8\n[43] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986, 2021. 2, 6\n[44] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation.\narXiv preprint arXiv:1909.11065, 2019. 7\n[45] Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. ShufÔ¨Çenet: An extremely efÔ¨Åcient convolutional\nneural network for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 6848‚Äì6856, 2018. 1, 3, 4\n[46] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv preprint arXiv:2012.15840, 2020. 1, 7\n[47] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, pages 633‚Äì641, 2017. 6, 7\n[48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020. 1\n12"
}