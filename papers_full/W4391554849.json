{
  "title": "Aggregating Residue-Level Protein Language Model Embeddings with Optimal Transport",
  "url": "https://openalex.org/W4391554849",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5065389164",
      "name": "Navid Naderializadeh",
      "affiliations": [
        "Duke University"
      ]
    },
    {
      "id": "https://openalex.org/A5081688191",
      "name": "Rohit Singh",
      "affiliations": [
        "Duke University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2730472814",
    "https://openalex.org/W6849674664",
    "https://openalex.org/W4386076262",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W4387507180",
    "https://openalex.org/W4385737488",
    "https://openalex.org/W4383550741",
    "https://openalex.org/W2957436444",
    "https://openalex.org/W4392518427",
    "https://openalex.org/W2086286404",
    "https://openalex.org/W2979557588",
    "https://openalex.org/W4223644783",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4288066876",
    "https://openalex.org/W3164046276",
    "https://openalex.org/W3181995918",
    "https://openalex.org/W4387012333",
    "https://openalex.org/W4386860638",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W6779830598",
    "https://openalex.org/W6759401939",
    "https://openalex.org/W2894384847",
    "https://openalex.org/W4391652655",
    "https://openalex.org/W4327550249",
    "https://openalex.org/W4225438928",
    "https://openalex.org/W2096864392",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4394593084",
    "https://openalex.org/W6757137939",
    "https://openalex.org/W2899038170",
    "https://openalex.org/W3212862161",
    "https://openalex.org/W3135369622",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W4401966068",
    "https://openalex.org/W4367302062",
    "https://openalex.org/W4379932151",
    "https://openalex.org/W4399849668",
    "https://openalex.org/W3199468887",
    "https://openalex.org/W4242765109",
    "https://openalex.org/W4387303685",
    "https://openalex.org/W2807741983",
    "https://openalex.org/W4385018113",
    "https://openalex.org/W4220991280",
    "https://openalex.org/W4307684269",
    "https://openalex.org/W4233762729",
    "https://openalex.org/W4225891318",
    "https://openalex.org/W6759006477",
    "https://openalex.org/W4403586429",
    "https://openalex.org/W4389157313",
    "https://openalex.org/W6744580074",
    "https://openalex.org/W6862446218"
  ],
  "abstract": "Abstract Protein language models (PLMs) have emerged as powerful approaches for mapping protein sequences into embeddings suitable for various applications. As protein representation schemes, PLMs generate per-token (i.e., per-residue) representations, resulting in variable-sized outputs based on protein length. This variability poses a challenge for protein-level prediction tasks that require uniform-sized embeddings for consistent analysis across different proteins. Previous work has typically used average pooling to summarize token-level PLM outputs, but it is unclear whether this method effectively prioritizes the relevant information across token-level representations. We introduce a novel method utilizing optimal transport to convert variable-length PLM outputs into fixed-length representations. We conceptualize per-token PLM outputs as samples from a probabilistic distribution and employ sliced-Wasserstein distances to map these samples against a reference set, creating a Euclidean embedding in the output space. The resulting embedding is agnostic to the length of the input and represents the entire protein. We demonstrate the superiority of our method over average pooling for several downstream prediction tasks, particularly with constrained PLM sizes, enabling smaller-scale PLMs to match or exceed the performance of average-pooled larger-scale PLMs. Our aggregation scheme is especially effective for longer protein sequences by capturing essential information that might be lost through average pooling.",
  "full_text": null,
  "topic": "Pooling",
  "concepts": [
    {
      "name": "Pooling",
      "score": 0.740676760673523
    },
    {
      "name": "Embedding",
      "score": 0.723606526851654
    },
    {
      "name": "Computer science",
      "score": 0.6484354734420776
    },
    {
      "name": "Security token",
      "score": 0.6403371691703796
    },
    {
      "name": "Representation (politics)",
      "score": 0.48613640666007996
    },
    {
      "name": "Variable (mathematics)",
      "score": 0.4393080472946167
    },
    {
      "name": "Theoretical computer science",
      "score": 0.36196738481521606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33016785979270935
    },
    {
      "name": "Algorithm",
      "score": 0.3210066854953766
    },
    {
      "name": "Data mining",
      "score": 0.32002097368240356
    },
    {
      "name": "Mathematics",
      "score": 0.23033004999160767
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ]
}