{
  "title": "Probing Pretrained Language Models for Lexical Semantics",
  "url": "https://openalex.org/W3092733346",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4222725273",
      "name": "Vulić, Ivan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223187432",
      "name": "Ponti, Edoardo Maria",
      "affiliations": [
        "University of Cambridge"
      ]
    },
    {
      "id": "https://openalex.org/A4227634977",
      "name": "Litschko, Robert",
      "affiliations": [
        "University of Mannheim"
      ]
    },
    {
      "id": "https://openalex.org/A4222398570",
      "name": "Glavaš, Goran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202117613",
      "name": "Korhonen, Anna",
      "affiliations": [
        "University of Cambridge"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2970557265",
    "https://openalex.org/W3035137491",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W2514776376",
    "https://openalex.org/W2890225082",
    "https://openalex.org/W3023567807",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W2798908575",
    "https://openalex.org/W22168010",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2953307569",
    "https://openalex.org/W1828724394",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2963680249",
    "https://openalex.org/W2938224028",
    "https://openalex.org/W2949558627",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W635530177",
    "https://openalex.org/W3015766957",
    "https://openalex.org/W2983836503",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W1854884267",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W2963047628",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2572487292",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2803912041"
  ],
  "abstract": "The success of large pretrained language models (LMs) such as BERT and RoBERTa has sparked interest in probing their representations, in order to unveil what types of knowledge they implicitly capture. While prior research focused on morphosyntactic, semantic, and world knowledge, it remains unclear to which extent LMs also derive lexical type-level knowledge from words in context. In this work, we present a systematic empirical analysis across six typologically diverse languages and five different lexical tasks, addressing the following questions: 1) How do different lexical knowledge extraction strategies (monolingual versus multilingual source LM, out-of-context versus in-context encoding, inclusion of special tokens, and layer-wise averaging) impact performance? How consistent are the observed effects across tasks and languages? 2) Is lexical knowledge stored in few parameters, or is it scattered throughout the network? 3) How do these representations fare against traditional static word vectors in lexical tasks? 4) Does the lexical information emerging from independently trained monolingual LMs display latent similarities? Our main results indicate patterns and best practices that hold universally, but also point to prominent variations across languages and tasks. Moreover, we validate the claim that lower Transformer layers carry more type-level lexical knowledge, but also show that this knowledge is distributed across multiple layers.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7222–7240,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n7222\nProbing Pretrained Language Models for Lexical Semantics\nIvan Vuli´c♠ Edoardo M. Ponti♠ Robert Litschko♦ Goran Glavaˇs♦ Anna Korhonen♠\n♠Language Technology Lab, University of Cambridge, UK\n♦Data and Web Science Group, University of Mannheim, Germany\n{iv250,ep490,alk23}@cam.ac.uk\n{goran,litschko}@informatik.uni-mannheim.de\nAbstract\nThe success of large pretrained language mod-\nels (LMs) such as BERT and RoBERTa has\nsparked interest in probing their representa-\ntions, in order to unveil what types of knowl-\nedge they implicitly capture. While prior re-\nsearch focused on morphosyntactic, semantic,\nand world knowledge, it remains unclear to\nwhich extent LMs also derivelexical type-level\nknowledge from words in context. In this\nwork, we present a systematic empirical anal-\nysis across six typologically diverse languages\nand ﬁve different lexical tasks, addressing the\nfollowing questions: 1) How do different lexi-\ncal knowledge extraction strategies (monolin-\ngual versus multilingual source LM, out-of-\ncontext versus in-context encoding, inclusion\nof special tokens, and layer-wise averaging)\nimpact performance? How consistent are the\nobserved effects across tasks and languages?\n2) Is lexical knowledge stored in few parame-\nters, or is it scattered throughout the network?\n3) How do these representations fare against\ntraditional static word vectors in lexical tasks?\n4) Does the lexical information emerging from\nindependently trained monolingual LMs dis-\nplay latent similarities? Our main results in-\ndicate patterns and best practices that hold uni-\nversally, but also point to prominent variations\nacross languages and tasks. Moreover, we val-\nidate the claim that lower Transformer layers\ncarry more type-level lexical knowledge, but\nalso show that this knowledge is distributed\nacross multiple layers.\n1 Introduction and Motivation\nLanguage models (LMs) based on deep Trans-\nformer networks (Vaswani et al., 2017), pretrained\non unprecedentedly large amounts of text, offer un-\nmatched performance in virtually every NLP task\n(Qiu et al., 2020). Models such as BERT (Devlin\net al., 2019), RoBERTa (Liu et al., 2019c), and T5\n(Raffel et al., 2019) replaced task-speciﬁc neural\narchitectures that relied on static word embeddings\n(WEs; Mikolov et al., 2013b; Pennington et al.,\n2014; Bojanowski et al., 2017), where each word\nis assigned a single (type-level) vector.\nWhile there is a clear consensus on the effec-\ntiveness of pretrained LMs, a body of recent re-\nsearch has aspired to understand why they work\n(Rogers et al., 2020). State-of-the-art models are\n“probed” to shed light on whether they capture\ntask-agnostic linguistic knowledge and structures\n(Liu et al., 2019a; Belinkov and Glass, 2019; Ten-\nney et al., 2019); e.g., they have been extensively\nprobed for syntactic knowledge (Hewitt and Man-\nning, 2019; Jawahar et al., 2019; Kulmizev et al.,\n2020; Chi et al., 2020, inter alia) and morphology\n(Edmiston, 2020; Hofmann et al., 2020).\nIn this work, we put focus on uncovering and un-\nderstanding how and where lexical semantic knowl-\nedge is coded in state-of-the-art LMs. While pre-\nliminary ﬁndings from Ethayarajh (2019) and Vuli´c\net al. (2020) suggest that there is a wealth of lexi-\ncal knowledge available within the parameters of\nBERT and other LMs, a systematic empirical study\nacross different languages is currently lacking.\nWe present such a study, spanning six typologi-\ncally diverse languages for which comparable pre-\ntrained BERT models and evaluation data are read-\nily available. We dissect the pipeline for extracting\nlexical representations, and divide it into crucial\ncomponents, including: the underlying source LM,\nthe selection of subword tokens, external corpora,\nand which Transformer layers to average over. Dif-\nferent choices give rise to different extraction con-\nﬁgurations (see Table 1) which, as we empirically\nverify, lead to large variations in task performance.\nWe run experiments and analyses on ﬁve diverse\nlexical tasks using standard evaluation benchmarks:\nlexical semantic similarity (LSIM), word analogy\nresolution (W A), bilingual lexicon induction (BLI),\ncross-lingual information retrieval (CLIR), and lex-\n7223\nical relation prediction (RELP). The main idea is to\naggregate lexical information into static type-level\n“BERT-based” word embeddings and plug them\ninto “the classical NLP pipeline” (Tenney et al.,\n2019), similar to traditional static word vectors.\nThe chosen tasks can be seen as “lexico-semantic\nprobes” providing an opportunity to simultaneously\n1) evaluate the richness of lexical information ex-\ntracted from different parameters of the underly-\ning pretrained LM on intrinsic (e.g., LSIM, WA)\nand extrinsic lexical tasks (e.g., RELP); 2) com-\npare different type-level representation extraction\nstrategies; and 3) benchmark “BERT-based” static\nvectors against traditional static word embeddings\nsuch as fastText (Bojanowski et al., 2017).\nOur study aims at providing answers to the fol-\nlowing key questions: Q1) Do lexical extraction\nstrategies generalise across different languages and\ntasks, or do they rather require language- and task-\nspeciﬁc adjustments?; Q2) Is lexical information\nconcentrated in a small number of parameters and\nlayers, or scattered throughout the encoder?; Q3)\nAre “BERT-based” static word embeddings com-\npetitive with traditional word embeddings such as\nfastText?; Q4) Do monolingual LMs independently\ntrained in multiple languages learn structurally sim-\nilar representations for words denoting similar con-\ncepts (i.e., translation pairs)?\nWe observe that different languages and tasks\nindeed require distinct conﬁgurations to reach peak\nperformance, which calls for a careful tuning of\nconﬁguration components according to the speciﬁc\ntask–language combination at hand (Q1). However,\nseveral universal patterns emerge across languages\nand tasks. For instance, lexical information is pre-\ndominantly concentrated in lower Transformer lay-\ners, hence excluding higher layers from the extrac-\ntion achieves superior scores (Q1 and Q2). Further,\nrepresentations extracted from single layers do not\nmatch in accuracy those extracted by averaging\nover several layers (Q2). While static word rep-\nresentations obtained from monolingual LMs are\ncompetitive or even outperform static fastText em-\nbeddings in tasks such as LSIM, WA, and RELP,\nlexical representations from massively multilingual\nmodels such as multilingual BERT (mBERT) are\nsubstantially worse (Q1 and Q3). We also demon-\nstrate that translation pairs indeed obtain similar\nrepresentations (Q4), but the similarity depends\non the extraction conﬁguration, as well as on the\ntypological distance between the two languages.\n2 Lexical Representations from\nPretrained Language Models\nClassical static word embeddings (Bengio et al.,\n2003; Mikolov et al., 2013b; Pennington et al.,\n2014) are grounded in distributional semantics, as\nthey infer the meaning of each word type from its\nco-occurrence patterns. However, LM-pretrained\nTransformer encoders have introduced at least two\nlevels of misalignment with the classical approach\n(Peters et al., 2018; Devlin et al., 2019). First, rep-\nresentations are assigned to word tokens and are\naffected by the current context and position within a\nsentence (Mickus et al., 2020). Second, tokens may\ncorrespond to subword strings rather than complete\nword forms. This begs the question: do pretrained\nencoders still retain a notion of lexical concepts,\nabstracted from their instances in texts?\nAnalyses of lexical semantic information in large\npretrained LMs have been limited so far, focus-\ning only on the English language and on the task\nof word sense disambiguation. Reif et al. (2019)\nshowed that senses are encoded with ﬁner-grained\nprecision in higher layers, to the extent that their\nrepresentation of the same token tends not to be\nself-similar across different contexts (Ethayarajh,\n2019; Mickus et al., 2020). As a consequence, we\nhypothesise that abstract, type-level information\ncould be codiﬁed in lower layers instead. However,\ngiven the absence of a direct equivalent to a static\nword type embedding, we still need to establish\nhow to extract such type-level information.\nIn prior work, contextualised representations\n(and attention weights) have been interpreted in\nthe light of linguistic knowledge mostly through\nprobes. These consist in learned classiﬁer pre-\ndicting annotations like POS tags (Pimentel et al.,\n2020) and word senses (Peters et al., 2018; Reif\net al., 2019; Chang and Chen, 2019), or linear trans-\nformations to a space where distances mirror depen-\ndency tree structures (Hewitt and Manning, 2019).1\nIn this work, we explore several unsuper-\nvised word-level representation extraction strate-\ngies and conﬁgurations for lexico-semantic tasks\n(i.e., probes), stemming from different combina-\ntions of the components detailed in Table 1 and\nillustrated in Figure 1. In particular, we assess the\nimpact of: 1) encoding tokens with monolingual\nLM-pretrained Transformers vs. with their mas-\n1The interplay between the complexity of a probe and its\naccuracy, as well as its effect on the overall procedure, remain\ncontroversial (Pimentel et al., 2020; V oita and Titov, 2020).\n7224\nComponent Label Short Description\nSource LM MONO Language-speciﬁc (i.e., monolingually pretrained) BERT\nMULTI Multilingual BERT, pretrained on 104 languages (with shared subword vocabulary)\nContext ISO Each vocabulary word w is encoded in isolation, without any external context\nAOC-M Average-over-context: average over word’s encodings fromM different contexts/sentences\nSubword Tokens\nNOSPEC Special tokens [CLS] and [SEP] are excluded from subword embedding averaging\nALL Both special tokens [CLS] and [SEP] are included into subword embedding averaging\nWITHCLS [CLS] is included into subword embedding averaging; [SEP] is excluded\nLayerwise Avg AVG(L≤n) Average representations over all Transformer layers up to then-th layer Ln (included)\nL=n Only the representation from the layer Ln is used\nTable 1: Conﬁguration components of word-level embedding extraction, resulting in 24 possible conﬁgurations.\n[CLS] \n[CLS] \nAOC\nMULTI MONO\nNOSPEC\nALL\nWITHCLS\n[CLS] \nmouth [SEP] \nriver mouth [SEP] \nsmiling mouth [SEP] \nISO\n[CLS] \nmouth [SEP] \n[CLS] \nmouth [SEP] \n[CLS] \nmouth [SEP] \nmouth \nmouth \nmouth \nmouth \nmouth \nmouth \nmouth \nmouth \nL=n AVG\nFigure 1: Illustration of the components denoting\nadopted extraction strategies, including source LM (top\nright), presence of context (bottom right), special to-\nkens (top left), and layer-wise averaging (bottom left).\nsively multilingual counterparts; 2) providing con-\ntext around the target word in input; 3) including\nspecial tokens like [CLS] and [SEP]; 4) averaging\nacross several layers as opposed to a single layer.2\n3 Experimental Setup\nPretrained LMs and Languages. Our selection\nof test languages is guided by the following con-\nstraints: a) availability of comparable pretrained\n(language-speciﬁc) monolingual LMs; b) availabil-\nity of evaluation data; and c) typological diver-\nsity of the sample, along the lines of recent initia-\ntives in multilingual NLP (Gerz et al., 2018; Hu\net al., 2020; Ponti et al., 2020, inter alia ). We\nwork with English ( EN), German ( DE), Russian\n(RU), Finnish (FI), Chinese (ZH), and Turkish (TR).\nWe use monolingual uncased BERT Base models\nfor all languages, retrieved from the HuggingFace\nrepository (Wolf et al., 2019).3 All BERT models\ncomprise 12 768-dimensional Transformer layers\n{L1 (bottom layer),...,L 12 (top)}plus the input\n2For clarity of presentation, later in §4 we show results\nonly for a representative selection of conﬁgurations that are\nconsistently better than the others\n3https://huggingface.co/models; the links to\nthe actual BERT models are in the appendix.\nembedding layer (L0), and 12 attention heads. We\nalso experiment with multilingual BERT (mBERT)\n(Devlin et al., 2019) as the underlying LM, aim-\ning to measure the performance difference between\nlanguage-speciﬁc and massively multilingual LMs\nin our lexical probing tasks.\nWord Vocabularies and External Corpora. We\nextract type-level representations in each language\nfor the top 100K most frequent words represented\nin the respective fastText (FT) vectors, which were\ntrained on lowercased monolingual Wikipedias by\nBojanowski et al. (2017). The equivalent vocabu-\nlary coverage allows a direct comparison to fast-\nText vectors, which we use as a baseline static WE\nmethod in all evaluation tasks. To retain the same\nvocabulary across all conﬁgurations, in AOC vari-\nants we back off to the relatedISO variant for words\nthat have zero occurrences in external corpora.\nFor all AOC vector variants, we leverage 1M sen-\ntences of maximum sequence length 512, which we\nrandomly sample from external corpora: Europarl\n(Koehn, 2005) for EN, DE, FI, available via OPUS\n(Tiedemann, 2009); the United Nations Parallel\nCorpus for RU and ZH (Ziemski et al., 2016), and\nmonolingual TR WMT17 data (Bojar et al., 2017).\nEvaluation Tasks. We carry out the evaluation on\nﬁve standard and diverse lexical semantic tasks:\nTask 1: Lexical semantic similarity (LSIM) is\nthe most widespread intrinsic task for evaluation\nof traditional word embeddings (Hill et al., 2015).\nThe evaluation metric is the Spearman’s rank cor-\nrelation between the average of human-elicited se-\nmantic similarity scores for word pairs and the\ncosine similarity between the respective type-level\nword vectors. We rely on the recent comprehen-\nsive multilingual LSIM benchmark Multi-SimLex\n(Vuli´c et al., 2020), which covers 1,888 pairs in\n13 languages. We focus on EN, FI, ZH, RU, the\n7225\nlanguages represented in Multi-SimLex.\nTask 2: Word Analogy (W A) is another com-\nmon intrinsic task. We evaluate our models on\nthe Bigger Analogy Test Set (BATS) (Drozd et al.,\n2016) with 99,200 analogy questions. We re-\nsort to the standard vector offset analogy resolu-\ntion method, searching for the vocabulary word\nwd ∈ V such that its vector d is obtained by\nargmaxd(cos(d,c −a+ b)), where a, b, and c\nare word vectors of words wa, wb, and wc from\nthe analogy wa : wb = wc : x. The search space\ncomprises vectors of all words from the vocabulary\nV, excluding a, b, and c. This task is limited to EN,\nand we report Precision@1 scores.\nTask 3: Bilingual Lexicon Induction (BLI) is\na standard task to evaluate the “semantic quality”\nof static cross-lingual word embeddings (CLWEs)\n(Gouws et al., 2015; Ruder et al., 2019). We learn\n“BERT-based” CLWEs using a standard mapping-\nbased approach (Mikolov et al., 2013a; Smith et al.,\n2017) with VECMAP (Artetxe et al., 2018). BLI\nevaluation allows us to investigate the “alignability”\nof monolingual type-level representations extracted\nfor different languages. We adopt the standard BLI\nevaluation setup from Glava ˇs et al. (2019): 5K\ntraining word pairs are used to learn the mapping,\nand another 2K pairs as test data. We report stan-\ndard Mean Reciprocal Rank (MRR) scores for 10\nlanguage pairs spanning EN, DE, RU, FI, TR.\nTask 4: Cross-Lingual Information Retrieval\n(CLIR). We follow the setup of Litschko et al.\n(2018, 2019) and evaluate mapping-based CLWEs\n(the same ones as on BLI) in a document-level re-\ntrieval task on the CLEF 2003 benchmark.4 We use\na simple CLIR model which showed competitive\nperformance in the comparative studies of Litschko\net al. (2019) and Glava ˇs et al. (2019). It embeds\nqueries and documents as IDF-weighted sums of\ntheir corresponding WEs from the CLWE space,\nand uses cosine similarity as the ranking function.\nWe report Mean Average Precision (MAP) scores\nfor 6 language pairs covering EN, DE, RU, FI.\nTask 5: Lexical Relation Prediction (RELP).\nWe probe if we can recover standard lexical re-\nlations (i.e., synonymy, antonymy, hypernymy,\nmeronymy, plus no relation) from input type-level\nvectors. We rely on a state-of-the-art neural model\n4All test collections comprise 60 queries. The average\ndocument collection size per language is 131K (ranging from\n17K documents for RU to 295K for DE).\nfor RELP operating on type-level embeddings\n(Glavaˇs and Vuli´c, 2018): the Specialization Tensor\nModel (STM) predicts lexical relations for pairs\nof input word vectors based on multi-view projec-\ntions of those vectors.5 We use the WordNet-based\n(Fellbaum, 1998) evaluation data of Glava ˇs and\nVuli´c (2018): they contain 10K annotated word\npairs balanced by class. Micro-averaged F1 scores,\naveraged across 5 runs for each input vector space\n(default STM setting), are reported for EN and DE.\n4 Results and Discussion\nA summary of the results is shown in Figure 2\nfor LSIM, in Figure 3a for BLI, in Figure 3b for\nCLIR, in Figure 4a and Figure 4b for RELP, and in\nFigure 4c for W A. These results offer multiple axes\nof comparison, and the ensuing discussion focuses\non the central questions Q1-Q3 posed in §1.6\nMonolingual versus Multilingual LMs. Results\nacross all tasks validate the intuition that language-\nspeciﬁc monolingual LMs contain much more lexi-\ncal information for a particular target language than\nmassively multilingual models such as mBERT or\nXLM-R (Artetxe et al., 2020). We see large drops\nbetween MONO .* and MULTI .* conﬁgurations even\nfor very high-resource languages (EN and DE), and\nthey are even more prominent for FI and TR.\nEncompassing 100+ training languages with lim-\nited model capacity, multilingual models suffer\nfrom the “curse of multilinguality” (Conneau et al.,\n2020): they must trade off monolingual lexical in-\nformation coverage (and consequently monolingual\nperformance) for a wider language coverage.7\nHow Important is Context? Another observation\nthat holds across all conﬁgurations concerns the\nusefulness of providing contexts drawn from exter-\nnal corpora, and corroborates ﬁndings from prior\nwork (Liu et al., 2019b): ISO conﬁgurations cannot\nmatch conﬁgurations that average subword embed-\ndings from multiple contexts ( AOC-10 and AOC-\n100 ). However, it is worth noting that 1) perfor-\n5Note that RELP is structurally different from the other\nfour tasks: instead of direct computations with word embed-\ndings, called metric learning or similarity-based evaluation\n(Ruder et al., 2019), it uses them as features in a neural archi-\ntecture.\n6Full results are available in the appendix.\n7For a particular target language, monolingual perfor-\nmance can be partially recovered by additional in-language\nmonolingual training via masked language modeling (Eisen-\nschlos et al., 2019; Pfeiffer et al., 2020). In a side experiment,\nwe have also veriﬁed that the same holds for lexical informa-\ntion coverage.\n7226\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.15\n0.25\n0.35\n0.45\n0.55Spearman ρ correlation\n(a) English\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.15\n0.25\n0.35\n0.45\n0.55Spearman ρ correlation\n (b) Finnish\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.35\n0.45\n0.55\n0.65Spearman ρ correlation\n(c) Mandarin Chinese\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.0\n0.1\n0.2\n0.3\n0.4Spearman ρ correlation\n (d) Russian\nFigure 2: Spearman’sρcorrelation scores for the lexical semantic similarity task (LSIM) in four languages. For the\nrepresentation extraction conﬁgurations in the legend, see Table 1. Thick solid horizontal lines denote performance\nof standard monolingual fastText vectors trained on Wikipedia dumps of the respective languages.\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.1\n0.2\n0.3\n0.4BLI scores (MRR)\n(a) Summary BLI results\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.025\n0.050\n0.075\n0.100\n0.125\n0.150CLIR scores (MAP)\n (b) Summary CLIR results\nFigure 3: Summary results for the two cross-lingual evaluation tasks: (a) BLI (MRR scores) and (b) CLIR (MAP\nscores). We report average scores over all language pairs; individual results for each language pair are available\nin the appendix. Thick solid horizontal lines denote performance of standard fastText vectors in exactly the same\ncross-lingual mapping setup.\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.65\n0.67\n0.69\n0.71\n0.73Micro-averaged F1 scores\n(a) RELP: English\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.60\n0.62\n0.64\n0.66\n0.68Micro-averaged F1 scores\n (b) RELP: German\nL ≤ 2 L ≤ 4 L ≤ 6 L ≤ 8 L ≤ 10 L ≤ 12\nAverage over layers\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Analogy scores (Precision@1)\n (c) W A: English\nFigure 4: Micro-averaged F1 scores in the RELP task for (a) EN and (b) DE. The scores with 768-dim vectors\nrandomly initalized via Xavier init (Glorot and Bengio, 2010) are 0.473 (EN) and 0.512 (DE); (c) EN W A results.\n7227\nmance gains with AOC-100 over AOC-10, although\nconsistent, are quite marginal across all tasks: this\nsuggests that several word occurrences in vivo are\nalready sufﬁcient to accurately capture its type-\nlevel representation. 2) In some tasks, ISO conﬁgu-\nrations are only marginally outscored by their AOC\ncounterparts: e.g., for MONO .*.NOSPEC .AVG(L≤8)\non EN–FI BLI or DE–TR BLI, the respective scores\nare 0.486 and 0.315 with ISO , and 0.503 and 0.334\nwith AOC-10. Similar observations hold for FI and\nZH LSIM, and also in the RELP task.\nIn RELP, it is notable that ‘BERT-based’ embed-\ndings can recover more lexical relation knowledge\nthan standard FT vectors. These ﬁndings reveal that\npretrained LMs indeed implicitly capture plenty of\nlexical type-level knowledge (which needs to be\n‘recovered’ from the models); this also suggests\nwhy pretrained LMs have been successful in tasks\nwhere this knowledge is directly useful, such as\nNER and POS tagging (Tenney et al., 2019; Tsai\net al., 2019). Finally, we also note that gains with\nAOC over ISO are much more pronounced for the\nunder-performing MULTI .* conﬁgurations: this in-\ndicates that MONO models store more lexical infor-\nmation even in absence of context.\nHow Important are Special Tokens? The results\nreveal that the inclusion of special tokens [CLS]\nand [SEP] into type-level embedding extraction de-\nteriorates the ﬁnal lexical information contained in\nthe embeddings. This ﬁnding holds for different\nlanguages, underlying LMs, and averaging across\nvarious layers. The NOSPEC conﬁgurations consis-\ntently outperform their ALL and WITHCLS counter-\nparts, both in ISO and AOC-{10, 100}settings.8\nOur ﬁnding at the lexical level aligns well with\nprior observations on using BERT directly as a sen-\ntence encoder (Qiao et al., 2019; Singh et al., 2019;\nCasanueva et al., 2020): while [CLS] is useful for\nsentence-pair classiﬁcation tasks, using [CLS] as a\nsentence representation produces inferior represen-\ntations than averaging over sentence’s subwords.\nIn this work, we show that [CLS] and [SEP] should\nalso be fully excluded from subword averaging for\ntype-level word representations.\nHow Important is Layer-wise Averaging? Av-\neraging across layers bottom-to-top (i.e., from L0\nto L12) is beneﬁcial across the board, but we no-\ntice that scores typically saturate or even decrease\nin some tasks and languages when we include\n8For this reason, we report the results of AOC conﬁgura-\ntions only in the NOSPEC setting.\nhigher layers into averaging: see the scores with\n*.AVG(L≤10) and *. AVG(L≤12) conﬁgurations,\ne.g., for FI LSIM; EN/DE RELP, and summary BLI\nand CLIR scores. This hints to the fact that two\nstrategies typically used in prior work, either to\ntake the vectors only from the embedding layer L0\n(Wu et al., 2020; Wang et al., 2019) or to average\nacross all layers (Liu et al., 2019b), extract sub-\noptimal word representations for a wide range of\nsetups and languages.\nThe sweet spot for nin *.AVG(L≤n) conﬁgura-\ntions seems largely task- and language-dependent,\nas peak scores are obtained with different n-s.\nWhereas averaging across all layers generally\nhurts performance, the results strongly suggest\nthat averaging across layer subsets (rather than\nselecting a single layer) is widely useful, espe-\ncially across bottom-most layers: e.g., L ≤ 6\nwith MONO .ISO .NOSPEC yields an average score of\n0.561 in LSIM, 0.076 in CLIR, and 0.432 in BLI;\nthe respective scores when averaging over the 6\ntop layers are: 0.218, 0.008, and 0.230. This evi-\ndence implies that, although scattered across multi-\nple layers, type-level lexical information seems to\nbe concentrated in lower Transformer layers. We\ninvestigate these conjectures further in §4.1.\nComparison to Static Word Embeddings. The\nresults also offer a comparison to static FT vectors\nacross languages. The best-performing extraction\nconﬁgurations (e.g., MONO .AOC-100.NOSPEC ) out-\nperform FT in monolingual evaluations on LSIM\n(for EN, FI, ZH), WA, and they also display much\nstronger performance in the RELP task for both\nevaluation languages. While the comparison is\nnot strictly apples-to-apples, as FT and LMs were\ntrained on different (Wikipedia) corpora, these ﬁnd-\nings leave open a provocative question for future\nwork: Given that static type-level word representa-\ntions can be recovered from large pretrained LMs,\ndoes this make standard static WEs obsolete, or\nare there applications where they are still useful?\nThe trend is opposite in the two cross-lingual\ntasks: BLI and CLIR. While there are language\npairs for which ‘BERT-based’ WEs outperform FT\n(i.e., EN–FI in BLI, EN–RU and FI–RU in CLIR) or\nare very competitive to FT’s performance (e.g.,EN–\nTR, TR–BLI, DE–RU CLIR), FT provides higher\nscores overall in both tasks. The discrepancy be-\ntween results in monolingual versus cross-lingual\ntasks warrants further investigation in future work.\nFor instance, is using linear maps, as in stan-\n7228\nFigure 5: CKA similarity scores of type-level word representations extracted from each layer (using different\nextraction conﬁgurations, see Table 1) for a set of 7K translation pairs in EN–DE, EN–FI, and EN–TR from the BLI\ndictionaries of Glavaˇs et al. (2019). Additional heatmaps (where random words from two languages are paired) are\navailable in the appendix.\n(a) EN–RU: Word translation pairs\n (b) EN–RU: Random word pairs\nFigure 6: CKA similarity scores of type-level word representations extracted from each layer for a set of (a) 7K\nEN–RU translation pairs from the BLI dictionaries of Glavaˇs et al. (2019); (b) 7K random EN–RU pairs.\nFigure 7: Self-similarity heatmaps: linear CKA similarity of representations for the same word extracted from\ndifferent Transformer layers, averaged across 7K words for English and Finnish. MONO .AOC-100.NOSPEC .\ndard mapping approaches to CLWE induction, sub-\noptimal for ‘BERT-based’ word vectors?\nDifferences across Languages and Tasks. Fi-\nnally, while we observe a conspicuous amount of\nuniversal patterns with conﬁguration components\n(e.g., MONO > MULTI ; AOC > ISO ; NOSPEC >\nALL , WITHCLS ), best-performing conﬁgurations do\nshow some variation across different languages and\n7229\nL0 L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 L11 L12\nLSIM EN .503 .513 .505 .510 .505 .484 .459 .435 .402 .361 .362 .372 .390\nFI .445 .466 .445 .436 .430 .434 .421 .404 .374 .346 .333 .324 .286\nW A EN .220 .272 .293 .285 .293 .261 .240 .217 .199 .171 .189 .221 .229\nBLI\nEN–DE .310 .354 .379 .400 .394 .393 .373 .358 .311 .272 .273 .264 .287\nEN–FI .309 .339 .360 .367 .369 .345 .329 .303 .279 .252 .231 .194 .192\nDE–FI .211 .245 .268 .283 .289 .303 .291 .292 .288 .282 .262 .219 .236\nCLIR\nEN–DE .059 .060 .059 .060 .043 .036 .036 .036 .027 .024 .027 .035 .038\nEN–FI .038 .040 .022 .018 .011 .008 .006 .006 .005 .002 .003 .002 .007\nDE–FI .054 .057 .028 .015 .016 .022 .017 .021 .020 .023 .015 .008 .030\nTable 2: Task performance of word representations extracted from different Transformer layers for a selection of\ntasks, languages, and language pairs. Conﬁguration: MONO .AOC-100.NOSPEC . Highest scores per row are in bold.\ntasks. For instance, while EN LSIM performance\ndeclines modestly but steadily when averaging over\nhigher-level layers (AVG(L≤n), where n> 4), per-\nformance on EN W A consistently increases for the\nsame conﬁgurations. The BLI and CLIR scores\nin Figures 3a and 3b also show slightly different\npatterns across layers. Overall, this suggests that\n1) extracted lexical information must be guided by\ntask requirements, and 2) conﬁg components must\nbe carefully tuned to maximise performance for a\nparticular task–language combination.\n4.1 Lexical Information in Individual Layers\nEvaluation Setup. To better understand which lay-\ners contribute the most to the ﬁnal performance in\nour lexical tasks, we also probe type-level represen-\ntations emerging from each individual layer of pre-\ntrained LMs. For brevity, we focus on the best per-\nforming conﬁgurations from previous experiments:\n{MONO , MBERT }.{ISO , AOC-100}.NOSPEC .\nIn addition, tackling Q4 from §1, we analyse the\nsimilarity of representations extracted from mono-\nlingual and multilingual BERT models using the\ncentered kernel alignment (CKA) as proposed by\n(Kornblith et al., 2019). The linear CKA computes\nsimilarity that is invariant to isotropic scaling and\northogonal transformation. It is deﬁned as\nCKA(X,Y ) =\nY⊤X\n2\nF\n(∥X⊤X∥F ∥Y⊤Y∥F). (1)\nX,Y ∈Rs×d are input matrices spanning s ℓ2-\nnormalized and mean-centered examples of dimen-\nsionality d = 768. We use CKA in two different\nexperiments: 1) measuring self-similarity where\nwe compute CKA similarity of representations ex-\ntracted from different layers for the same word;\nand 2) measuring bilingual layer correspondence\nwhere we compute CKA similarity of representa-\ntions extracted from the same layer for two words\nconstituting a translation pair. To this end, we again\nuse BLI dictionaries of Glavaˇs et al. (2019) (see §3)\ncovering 7K pairs (training + test pairs).\nDiscussion. Per-layer CKA similarities are pro-\nvided in Figure 7 (self-similarity) and Figure 5\n(bilingual), and we show results of representations\nextracted from individual layers for selected evalu-\nation setups and languages in Table 2. We also plot\nbilingual layer correspondence of true word trans-\nlations versus randomly paired words for EN–RU\nin Figure 6. Figure 7 reveals very similar patterns\nfor both EN and FI, and we also observe that self-\nsimilarity scores decrease for more distant layers\n(cf., similarity of L1 and L2 versus L1 and L12).\nHowever, despite structural similarities identiﬁed\nby linear CKA, the scores from Table 2 demon-\nstrate that structurally similar layers might encode\ndifferent amounts of lexical information: e.g., com-\npare performance drops between L5 and L8 in all\nevaluation tasks.\nThe results in Table 2 further suggest that more\ntype-level lexical information is available in lower\nlayers, as all peak scores in the table are achieved\nwith representations extracted from layers L1 −L5.\nMuch lower scores in type-level semantic tasks\nfor higher layers also empirically validate a re-\ncent hypothesis of Ethayarajh (2019) “that con-\ntextualised word representations are more context-\nspeciﬁc in higher layers.” We also note that none\nof the results with L=nconﬁgurations from Table 1\ncan match best performing AVG(L≤n) conﬁgura-\ntions with layer-wise averaging. This conﬁrms our\nhypothesis that type-level lexical knowledge, al-\nthough predominantly captured by lower layers, is\ndisseminated across multiple layers, and layer-wise\naveraging is crucial to uncover that knowledge.\nFurther, Figure 5 and Figure 6 reveal that even\n7230\nLMs trained on monolingual data learn similar\nrepresentations in corresponding layers for word\ntranslations (see the MONO .AOC columns). Intu-\nitively, this similarity is much more pronounced\nwith AOC conﬁgurations with mBERT. The com-\nparison of scores in Figure 6 also reveals much\nhigher correspondence scores for true translation\npairs than for randomly paired words (i.e., the cor-\nrespondence scores for random pairings are, as ex-\npected, random). Moreover, MULTI CKA similarity\nscores turn out to be higher for more similar lan-\nguage pairs (cf. EN–DE versus EN–TR MULTI .AOC\ncolumns). This suggests that, similar to static\nWEs, type-level ‘BERT-based’ WEs of different\nlanguages also display topological similarity, often\ntermed approximate isomorphism (Søgaard et al.,\n2018), but its degree depends on language prox-\nimity. This also clariﬁes why representations ex-\ntracted from two independently trained monolin-\ngual LMs can be linearly aligned, as validated by\nBLI and CLIR evaluation (Table 2 and Figure 3).9\nWe also calculated the Spearman’s correlation\nbetween CKA similarity scores for conﬁgurations\nMONO .AOC-100 .NOSPEC .AVG(L≤n), for all n=\n0,..., 12, and their corresponding BLI scores on\nEN–FI, EN–DE, and DE–FI. The correlations are\nvery high: ρ = 1.0,0.83,0.99, respectively. This\nfurther conﬁrms the approximate isomorphism hy-\npothesis: it seems that higher structural similarities\nof representations extracted from monolingual pre-\ntrained LMs facilitate their cross-lingual alignment.\n5 Further Discussion and Conclusion\nWhat about Larger LMs and Corpora?Aspects\nof LM pretraining, such as the number of model pa-\nrameters or the size of pretraining data, also impact\nlexical knowledge stored in the LM’s parameters.\nOur preliminary experiments have veriﬁed that EN\nBERT-Large yields slight gains over theEN BERT-\nBase architecture used in our work (e.g., peak EN\nLSIM scores rise from 0.518 to 0.531). In a simi-\nlar vein, we have run additional experiments with\ntwo available Italian (IT) BERT-Base models with\nidentical parameter setups, where one was trained\n9Previous work has empirically validated that sentence\nrepresentations for semantically similar inputs from different\nlanguages are less similar in higher Transformer layers (Singh\net al., 2019; Wu and Dredze, 2019). In Figure 5, we demon-\nstrate that this is also the case for type-level lexical informa-\ntion; however, unlike sentence representations where highest\nsimilarity is reported in lowest layers, Figure 5 suggests that\nhighest CKA similarities are achieved in intermediate layers\nL5-L8.\non 13GB of IT text, and the other on 81GB. In\nEN (BERT-Base)–IT BLI and CLIR evaluations we\nmeasure improvements from 0.548 to 0.572 (BLI),\nand from 0.148 to 0.160 (CLIR) with the 81GB IT\nmodel. In-depth analyses of these factors are out\nof the scope of this work, but they warrant further\ninvestigations.\nOpening Future Research Avenues. Our study\nhas empirically validated that (monolingually) pre-\ntrained LMs store a wealth of type-level lexical\nknowledge, but effectively uncovering and extract-\ning such knowledge from the LMs’ parameters de-\npends on several crucial components (see §2). In\nparticular, some universal choices of conﬁguration\ncan be recommended: i) choosing monolingual\nLMs; ii) encoding words with multiple contexts;\niii) excluding special tokens; iv) averaging over\nlower layers. Moreover, we found that type-level\nWEs extracted from pretrained LMs can surpass\nstatic WEs like fastText (Bojanowski et al., 2017).\nThis study has only scratched the surface of this\nresearch avenue. In future work, we plan to investi-\ngate how domains of external corpora affect AOC\nconﬁgurations, and how to sample representative\ncontexts from the corpora. We will also extend\nthe study to more languages, more lexical seman-\ntic probes, and other larger underlying LMs. The\ndifference in performance across layers also calls\nfor more sophisticated lexical representation ex-\ntraction methods (e.g., through layer weighting or\nattention) similar to meta-embedding approaches\n(Yin and Sch¨utze, 2016; Bollegala and Bao, 2018;\nKiela et al., 2018). Given the current large gaps\nbetween monolingual and multilingual LMs, we\nwill also focus on lightweight methods to enrich\nlexical content in multilingual LMs (Wang et al.,\n2020; Pfeiffer et al., 2020).\nAcknowledgments\nThis work is supported by the ERC Consolidator\nGrant LEXICAL: Lexical Acquisition Across Lan-\nguages (no 648909) awarded to Anna Korhonen.\nThe work of Goran Glava ˇs and Robert Litschko\nis supported by the Baden-W ¨urttemberg Stiftung\n(AGREE grant of the Eliteprogramm).\nReferences\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2018.\nA robust self-learning method for fully unsupervised\ncross-lingual mappings of word embeddings. InPro-\nceedings of ACL, pages 789–798.\n7231\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of ACL ,\npages 4623–4637.\nYonatan Belinkov and James R. Glass. 2019. Analy-\nsis methods in neural language processing: A sur-\nvey. Transactions of the Association of Computa-\ntional Linguistics, 7:49–72.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the ACL ,\n5:135–146.\nOndˇrej Bojar, Rajen Chatterjee, Christian Federmann,\nYvette Graham, Barry Haddow, Shujian Huang,\nMatthias Huck, Philipp Koehn, Qun Liu, Varvara Lo-\ngacheva, Christof Monz, Matteo Negri, Matt Post,\nRaphael Rubino, Lucia Specia, and Marco Turchi.\n2017. Findings of the 2017 Conference on Machine\nTranslation (WMT17). In Proceedings of WMT ,\npages 169–214.\nDanushka Bollegala and Cong Bao. 2018. Learning\nword meta-embeddings by autoencoding. In Pro-\nceedings of COLING, pages 1650–1661.\nI˜nigo Casanueva, Tadas Tem ˇcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vuli´c. 2020. Efﬁcient\nintent detection with dual sentence encoders. In Pro-\nceedings of the 2nd Workshop on Natural Language\nProcessing for Conversational AI, pages 38–45.\nTing-Yun Chang and Yun-Nung Chen. 2019. What\ndoes this word mean? Explaining contextualized em-\nbeddings with natural language deﬁnition. In Pro-\nceedings of EMNLP-IJCNLP, pages 6064–6070.\nEthan A. Chi, John Hewitt, and Christopher D. Man-\nning. 2020. Finding universal grammatical rela-\ntions in multilingual BERT. In Proceedings of ACL,\npages 5564–5577.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of ACL, pages 8440–8451.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nAleksandr Drozd, Anna Gladkova, and Satoshi Mat-\nsuoka. 2016. Word embeddings, analogies, and ma-\nchine learning: Beyond king - man + woman =\nqueen. In Proceedings of COLING , pages 3519–\n3530.\nDaniel Edmiston. 2020. A systematic analysis of mor-\nphological content in BERT models for multiple lan-\nguages. CoRR, abs/2004.03032.\nJulian Eisenschlos, Sebastian Ruder, Piotr Czapla,\nMarcin Kardas, Sylvain Gugger, and Jeremy\nHoward. 2019. MultiFiT: Efﬁcient multi-lingual\nlanguage model ﬁne-tuning. In Proceedings of\nEMNLP-IJCNLP, pages 5701–5706.\nKawin Ethayarajh. 2019. How contextual are contextu-\nalized word representations? Comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of EMNLP-IJCNLP, pages 55–65.\nChristiane Fellbaum. 1998. WordNet. MIT Press.\nDaniela Gerz, Ivan Vuli ´c, Edoardo Maria Ponti, Roi\nReichart, and Anna Korhonen. 2018. On the rela-\ntion between linguistic typology and (limitations of)\nmultilingual language modeling. In Proceedings of\nEMNLP, pages 316–327.\nGoran Glavaˇs and Ivan Vuli´c. 2018. Discriminating be-\ntween lexico-semantic relations with the specializa-\ntion tensor model. In Proceedings of NAACL-HLT,\npages 181–187.\nGoran Glavaˇs, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of ACL, pages 710–721.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neural\nnetworks. In Proceedings of AISTATS, pages 249–\n256.\nStephan Gouws, Yoshua Bengio, and Greg Corrado.\n2015. BilBOW A: Fast bilingual distributed repre-\nsentations without word alignments. In Proceedings\nof ICML, pages 748–756.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of NAACL-HLT , pages\n4129–4138.\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015.\nSimLex-999: Evaluating semantic models with (gen-\nuine) similarity estimation. Computational Linguis-\ntics, 41(4):665–695.\nValentin Hofmann, Janet B. Pierrehumbert, and Hin-\nrich Sch ¨utze. 2020. Generating derivational mor-\nphology with BERT. CoRR, abs/2005.00672.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. In Proceedings of ICML.\n7232\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of ACL, pages 3651–\n3657.\nDouwe Kiela, Changhan Wang, and Kyunghyun Cho.\n2018. Dynamic meta-embeddings for improved sen-\ntence representations. In Proceedings of EMNLP ,\npages 1466–1477.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of the\n10th Machine Translation Summit (MT SUMMIT) ,\npages 79–86.\nSimon Kornblith, Mohammad Norouzi, Honglak Lee,\nand Geoffrey E. Hinton. 2019. Similarity of neural\nnetwork representations revisited. In Proceedings of\nICML, pages 3519–3529.\nArtur Kulmizev, Vinit Ravishankar, Mostafa Abdou,\nand Joakim Nivre. 2020. Do neural language mod-\nels show preferences for syntactic formalisms? In\nProceedings of ACL, pages 4077–4091.\nRobert Litschko, Goran Glava ˇs, Simone Paolo\nPonzetto, and Ivan Vuli´c. 2018. Unsupervised cross-\nlingual information retrieval using monolingual data\nonly. In Proceedings of SIGIR, pages 1253–1256.\nRobert Litschko, Goran Glavaˇs, Ivan Vuli´c, and Laura\nDietz. 2019. Evaluating resource-lean cross-lingual\nembedding models in unsupervised retrieval. In Pro-\nceedings of SIGIR, pages 1109–1112.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of NAACL-HLT ,\npages 1073–1094.\nQianchu Liu, Diana McCarthy, Ivan Vuli ´c, and Anna\nKorhonen. 2019b. Investigating cross-lingual align-\nment methods for contextualized embeddings with\ntoken-level evaluation. In Proceedings of CoNLL ,\npages 33–43.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019c.\nRoBERTa: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nTimothee Mickus, Denis Paperno, Mathieu Constant,\nand Kees van Deemter. 2020. What do you mean,\nBERT? Assessing BERT as a distributional seman-\ntics model. Proceedings of the Society for Computa-\ntion in Linguistics, 3(34).\nTomas Mikolov, Quoc V . Le, and Ilya Sutskever.\n2013a. Exploiting similarities among languages\nfor machine translation. arXiv preprint, CoRR ,\nabs/1309.4168.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013b. Distributed rep-\nresentations of words and phrases and their compo-\nsitionality. In Proceedings of NeurIPS, pages 3111–\n3119.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In Proceedings of EMNLP, pages 1532–\n1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020. MAD-X: An adapter-based frame-\nwork for multi-task cross-lingual transfer. In Pro-\nceedings of EMNLP.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. In Proceedings of ACL, pages 4609–4622.\nEdoardo Maria Ponti, Goran Glava ˇs, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of EMNLP.\nYifan Qiao, Chenyan Xiong, Zheng-Hao Liu, and\nZhiyuan Liu. 2019. Understanding the behaviors of\nBERT in ranking. CoRR, abs/1904.07531.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nCoRR, abs/2003.08271.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. CoRR, abs/1910.10683.\nEmily Reif, Ann Yuan, Martin Wattenberg, Fernanda B.\nViegas, Andy Coenen, Adam Pearce, and Been Kim.\n2019. Visualizing and measuring the geometry of\nBERT. In Proceedings of NeurIPS , pages 8594–\n8603.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: what we know about\nhow BERT works. Transactions of the ACL.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual embedding models.\nJournal of Artiﬁcial Intelligence Research , 65:569–\n631.\nJasdeep Singh, Bryan McCann, Richard Socher, and\nCaiming Xiong. 2019. BERT is not an interlingua\nand the bias of tokenization. In Proceedings of the\n2nd Workshop on Deep Learning Approaches for\nLow-Resource NLP (DeepLo 2019), pages 47–55.\n7233\nSamuel L. Smith, David H.P. Turban, Steven Ham-\nblin, and Nils Y . Hammerla. 2017. Ofﬂine bilin-\ngual word vectors, orthogonal transformations and\nthe inverted softmax. In Proceedings of ICLR (Con-\nference Track).\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of ACL, pages\n778–788.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of ACL, pages 4593–4601.\nJ¨org Tiedemann. 2009. News from OPUS - A collec-\ntion of multilingual parallel corpora with tools and\ninterfaces. In Proceedings of RANLP , pages 237–\n248.\nHenry Tsai, Jason Riesa, Melvin Johnson, Naveen Ari-\nvazhagan, Xin Li, and Amelia Archer. 2019. Small\nand practical BERT models for sequence labeling.\nIn Proceedings EMNLP-IJCNLP, pages 3632–3636.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS, pages 6000–\n6010.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of EMNLP.\nIvan Vuli´c, Simon Baker, Edoardo Maria Ponti, Ulla\nPetti, Ira Leviant, Kelly Wing, Olga Majewska, Eden\nBar, Matt Malone, Thierry Poibeau, Roi Reichart,\nand Anna Korhonen. 2020. Multi-Simlex: A large-\nscale evaluation of multilingual and cross-lingual\nlexical semantic similarity. Computational Linguis-\ntics.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2020. K-Adapter: Infusing knowl-\nedge into pre-trained models with adapters. CoRR,\nabs/2002.01808.\nYuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu,\nand Ting Liu. 2019. Cross-lingual BERT transfor-\nmation for zero-shot dependency parsing. In Pro-\nceedings of EMNLP-IJCNLP, pages 5721–5727.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models. In\nProceedings of ACL, pages 6022–6034.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of EMNLP, pages 833–844.\nWenpeng Yin and Hinrich Sch ¨utze. 2016. Learning\nword meta-embeddings. In Proceedings of ACL ,\npages 1351–1360.\nMichal Ziemski, Marcin Junczys-Dowmunt, and Bruno\nPouliquen. 2016. The United Nations Parallel Cor-\npus v1.0. In Proceedings of LREC.\n7234\nA Appendix\nURLs to the models and external corpora used in\nour study are provided in Table 3 and Table 4, re-\nspectively. URLs to the evaluation data and task\narchitectures for each evaluation task are provided\nin Table 5. We also report additional and more\ndetailed sets of results across different tasks, word\nembedding extraction conﬁgurations/variants, and\nlanguage pairs:\n•In Table 6 and Table 7, we provide full BLI\nresults per language pair. All scores are Mean\nReciprocal Rank (MRR) scores (in the stan-\ndard scoring interval, 0.0–1.0).\n•In Table 8, we provide full CLIR results per\nlanguage pair. All scores are Mean Average\nPrecision (MAP) scores (in the standard scor-\ning interval, 0.0–1.0).\n•In Table 9, we provide full relation prediction\n(RELP) results for EN and DE. All scores are\nmicro-averaged F1 scores over 5 runs of the\nrelation predictor (Glava ˇs and Vuli ´c, 2018).\nWe also report standard deviation for each\nconﬁguration.\nFinally, in Figures 8-10, we also provide\nheatmaps denoting bilingual layer correspondence,\ncomputed via linear CKA similarity (Kornblith\net al., 2019), for several EN–Lt language pairs (see\n§4.1), which are not provided in the main paper\n7235\nLanguage URL\nEN https://huggingface.co/bert-base-uncased\nDE https://huggingface.co/bert-base-german-dbmdz-uncased\nRU https://huggingface.co/DeepPavlov/rubert-base-cased\nFI https://huggingface.co/TurkuNLP/bert-base-finnish-uncased-v1\nZH https://huggingface.co/bert-base-chinese\nTR https://huggingface.co/dbmdz/bert-base-turkish-uncased\nMultilingual https://huggingface.co/bert-base-multilingual-uncased\nIT https://huggingface.co/dbmdz/bert-base-italian-uncased\nhttps://huggingface.co/dbmdz/bert-base-italian-xxl-uncased\nTable 3: URLs of the models used in our study. The ﬁrst part of the table refers to the models used in the main\nexperiments throughout the paper, while the second part refers to the models used in side experiments.\nLanguage URL\nEN http://opus.nlpl.eu/download.php?f=Europarl/v8/moses/de-en.txt.zip\nDE http://opus.nlpl.eu/download.php?f=Europarl/v8/moses/de-en.txt.zip\nRU http://opus.nlpl.eu/download.php?f=UNPC/v1.0/moses/en-ru.txt.zip\nFI http://opus.nlpl.eu/download.php?f=Europarl/v8/moses/en-fi.txt.zip\nZH http://opus.nlpl.eu/download.php?f=UNPC/v1.0/moses/en-zh.txt.zip\nTR http://data.statmt.org/wmt18/translation-task/news.2017.tr.shuffled.\ndeduped.gz\nIT http://opus.nlpl.eu/download.php?f=Europarl/v8/moses/en-it.txt.zip\nTable 4: Links to the external corpora used in the study. We randomly sample 1M sentences of maximum sequence\nlength 512 from the corresponding corpora.\nTask Evaluation Data and/or Model Link\nLSIM Multi-SimLex Data: multisimlex.com/\nW A BATS Data: vecto.space/projects/BATS/\nBLI Data: Dictionaries from Glavaˇs et al. (2019) Data: github.com/codogogo/xling-eval/\ntree/master/bli_datasets\nModel: VecMap Model: github.com/artetxem/vecmap\nCLIR Data: CLEF 2003 Data: catalog.elra.info/en-us/\nrepository/browse/ELRA-E0008/\nModel: Agg-IDF from Litschko et al. (2019) Model: github.com/rlitschk/UnsupCLIR\nRELP Data: WordNet-based RELP data Data: github.com/codogogo/stm/tree/\nmaster/data/wn-ls\nModel: Specialization Tensor Model Model: github.com/codogogo/stm\nTable 5: Links to evaluation data and models.\n7236\nConﬁguration EN–DE EN –TR EN –FI EN –RU DE –TR DE –FI DE –RU\nFASTTEXT .WIKI 0.610 0.433 0.488 0.522 0.358 0.435 0.469\nMONO .ISO .NOSPEC\nAVG(L≤2) 0.390 0.332 0.392 0.409 0.237 0.269 0.291\nAVG(L≤4) 0.430 0.367 0.438 0.447 0.269 0.311 0.338\nAVG(L≤6) 0.461 0.386 0.476 0.472 0.299 0.359 0.387\nAVG(L≤8) 0.472 0.390 0.486 0.487 0.315 0.387 0.407\nAVG(L≤10) 0.461 0.386 0.483 0.488 0.321 0.395 0.416\nAVG(L≤12) 0.446 0.379 0.471 0.473 0.323 0.395 0.412\nMONO .AOC-10.NOSPEC\nAVG(L≤2) 0.399 0.342 0.386 0.403 0.242 0.269 0.292\nAVG(L≤4) 0.457 0.379 0.448 0.433 0.283 0.322 0.343\nAVG(L≤6) 0.503 0.399 0.480 0.458 0.315 0.369 0.380\nAVG(L≤8) 0.527 0.414 0.499 0.461 0.332 0.394 0.391\nAVG(L≤10) 0.534 0.415 0.498 0.459 0.337 0.401 0.394\nAVG(L≤12) 0.534 0.416 0.492 0.453 0.337 0.401 0.376\nMONO .AOC-100. NOSPEC\nAVG(L≤2) 0.401 0.343 0.391 0.398 0.239 0.269 0.293\nAVG(L≤4) 0.459 0.381 0.449 0.437 0.288 0.325 0.343\nAVG(L≤6) 0.504 0.403 0.484 0.459 0.318 0.373 0.382\nAVG(L≤8) 0.532 0.418 0.503 0.462 0.334 0.394 0.389\nAVG(L≤10) 0.540 0.422 0.504 0.459 0.338 0.402 0.393\nAVG(L≤12) 0.542 0.426 0.500 0.454 0.343 0.401 0.378\nMONO .ISO .ALL\nAVG(L≤2) 0.352 0.289 0.351 0.374 0.230 0.265 0.283\nAVG(L≤4) 0.375 0.317 0.391 0.393 0.264 0.302 0.331\nAVG(L≤6) 0.386 0.330 0.406 0.407 0.289 0.350 0.376\nAVG(L≤8) 0.372 0.327 0.409 0.413 0.291 0.370 0.392\nAVG(L≤10) 0.352 0.320 0.396 0.402 0.290 0.370 0.383\nAVG(L≤12) 0.313 0.310 0.373 0.394 0.283 0.358 0.371\nMONO .ISO .WITHCLS\nAVG(L≤2) 0.367 0.306 0.368 0.386 0.236 0.272 0.285\nAVG(L≤4) 0.394 0.339 0.408 0.410 0.267 0.307 0.331\nAVG(L≤6) 0.406 0.344 0.428 0.425 0.294 0.353 0.381\nAVG(L≤8) 0.393 0.344 0.430 0.431 0.306 0.369 0.400\nAVG(L≤10) 0.371 0.336 0.421 0.421 0.303 0.382 0.395\nAVG(L≤12) 0.331 0.329 0.403 0.409 0.302 0.375 0.387\nMULTI .ISO .NOSPEC\nAVG(L≤2) 0.293 0.176 0.176 0.147 0.216 0.203 0.160\nAVG(L≤4) 0.304 0.184 0.190 0.164 0.219 0.214 0.178\nAVG(L≤6) 0.315 0.189 0.203 0.198 0.223 0.225 0.198\nAVG(L≤8) 0.325 0.193 0.209 0.228 0.224 0.235 0.217\nAVG(L≤10) 0.330 0.194 0.210 0.243 0.220 0.234 0.226\nAVG(L≤12) 0.333 0.193 0.206 0.248 0.219 0.231 0.227\nMULTI .AOC-10. NOSPEC\nAVG(L≤2) 0.309 0.171 0.172 0.146 0.208 0.200 0.156\nAVG(L≤4) 0.350 0.186 0.189 0.186 0.224 0.214 0.191\nAVG(L≤6) 0.389 0.219 0.215 0.240 0.241 0.243 0.225\nAVG(L≤8) 0.432 0.246 0.251 0.287 0.255 0.263 0.254\nAVG(L≤10) 0.448 0.258 0.264 0.306 0.260 0.282 0.272\nAVG(L≤12) 0.456 0.267 0.272 0.316 0.260 0.292 0.284\nMULTI .ISO .ALL\nAVG(L≤2) 0.292 0.173 0.175 0.143 0.209 0.203 0.154\nAVG(L≤4) 0.301 0.176 0.188 0.155 0.211 0.213 0.171\nAVG(L≤6) 0.307 0.181 0.198 0.186 0.216 0.221 0.193\nAVG(L≤8) 0.315 0.184 0.202 0.207 0.213 0.228 0.208\nAVG(L≤10) 0.318 0.182 0.197 0.216 0.208 0.226 0.215\nAVG(L≤12) 0.319 0.181 0.189 0.220 0.209 0.220 0.213\nMONO .ISO .NOSPEC (REVERSE )\nAVG(L≥12) 0.104 – 0.054 – – 0.077 –\nAVG(L≥10) 0.119 – 0.061 – – 0.063 –\nAVG(L≥8) 0.144 – 0.108 – – 0.095 –\nAVG(L≥6) 0.230 – 0.223 – – 0.238 –\nAVG(L≥4) 0.308 – 0.318 – – 0.335 –\nAVG(L≥2) 0.365 – 0.385 – – 0.372 –\nAVG(L≥0) 0.446 – 0.471 – – 0.395 –\nTable 6: Results in the BLI task across different language pairs and word vector extraction conﬁgurations. MRR\nscores reported. For clarity of presentation, a subset of results is presented in this table, while the rest (and the\naverages) are presented in Table 7. AVG(L≤n) means that we average representations over all Transformer layers\nup to the nth layer (included), where L= 0refers to the embedding layer, L= 1to the bottom layer, and L= 12\nto the ﬁnal (top) layer. Different conﬁgurations are described in§2 and Table 1. Additional diagnostic experiments\nwith top-to-bottom layerwise averaging conﬁgs (REVERSE ) are run for a subset of languages: {EN, DE, FI }.\n7237\nConﬁguration TR–FI TR –RU FI –RU average\nFASTTEXT .WIKI 0.358 0.364 0.439 0.448\nMONO .ISO .NOSPEC\nAVG(L≤2) 0.237 0.217 0.290 0.306\nAVG(L≤4) 0.279 0.261 0.337 0.348\nAVG(L≤6) 0.311 0.288 0.372 0.381\nAVG(L≤8) 0.334 0.315 0.387 0.398\nAVG(L≤10) 0.347 0.317 0.392 0.401\nAVG(L≤12) 0.352 0.319 0.387 0.396\nMONO .AOC-10.NOSPEC\nAVG(L≤2) 0.247 0.221 0.284 0.308\nAVG(L≤4) 0.288 0.263 0.331 0.355\nAVG(L≤6) 0.319 0.294 0.366 0.388\nAVG(L≤8) 0.334 0.311 0.375 0.404\nAVG(L≤10) 0.340 0.311 0.379 0.407\nAVG(L≤12) 0.344 0.310 0.360 0.402\nMONO .AOC-100. NOSPEC\nAVG(L≤2) 0.244 0.220 0.285 0.308\nAVG(L≤4) 0.288 0.261 0.333 0.356\nAVG(L≤6) 0.322 0.291 0.367 0.390\nAVG(L≤8) 0.338 0.309 0.376 0.406\nAVG(L≤10) 0.348 0.314 0.377 0.410\nAVG(L≤12) 0.349 0.311 0.361 0.407\nMONO .ISO .ALL\nAVG(L≤2) 0.226 0.212 0.284 0.287\nAVG(L≤4) 0.270 0.254 0.328 0.322\nAVG(L≤6) 0.302 0.274 0.358 0.348\nAVG(L≤8) 0.318 0.296 0.371 0.356\nAVG(L≤10) 0.328 0.303 0.373 0.352\nAVG(L≤12) 0.328 0.306 0.368 0.340\nMONO .ISO .WITHCLS\nAVG(L≤2) 0.232 0.217 0.285 0.295\nAVG(L≤4) 0.274 0.257 0.331 0.332\nAVG(L≤6) 0.307 0.279 0.362 0.358\nAVG(L≤8) 0.327 0.303 0.377 0.368\nAVG(L≤10) 0.334 0.314 0.383 0.366\nAVG(L≤12) 0.340 0.317 0.373 0.357\nMULTI .ISO .NOSPEC\nAVG(L≤2) 0.170 0.131 0.127 0.180\nAVG(L≤4) 0.180 0.135 0.138 0.191\nAVG(L≤6) 0.188 0.147 0.151 0.204\nAVG(L≤8) 0.189 0.152 0.164 0.214\nAVG(L≤10) 0.188 0.153 0.165 0.216\nAVG(L≤12) 0.188 0.158 0.163 0.217\nMULTI .AOC-10. NOSPEC\nAVG(L≤2) 0.165 0.127 0.130 0.178\nAVG(L≤4) 0.176 0.146 0.139 0.200\nAVG(L≤6) 0.192 0.174 0.162 0.230\nAVG(L≤8) 0.210 0.192 0.185 0.258\nAVG(L≤10) 0.219 0.198 0.200 0.271\nAVG(L≤12) 0.223 0.198 0.206 0.277\nMULTI .ISO .ALL\nAVG(L≤2) 0.163 0.126 0.123 0.176\nAVG(L≤4) 0.175 0.128 0.133 0.185\nAVG(L≤6) 0.179 0.139 0.142 0.196\nAVG(L≤8) 0.182 0.144 0.152 0.203\nAVG(L≤10) 0.178 0.141 0.153 0.203\nAVG(L≤12) 0.175 0.143 0.150 0.202\nTable 7: Results in the bilingual lexicon induction (BLI) task across different language pairs and word vector\nextraction conﬁgurations: Part II. MAP scores reported. For clarity of presentation, a subset of results is presented\nin this table, while the rest (also used to calculate the averages) is provided in Table 6 in the previous page.\nAVG(L≤n) means that we average representations over all Transformer layers up to thenth layer (included), where\nL = 0 refers to the embedding layer, L = 1 to the bottom layer, and L = 12 to the ﬁnal (top) layer. Different\nconﬁgurations are described in §2 and Table 1.\n7238\nConﬁguration EN–DE EN –FI EN –RU DE –FI DE –RU FI –RU average\nFASTTEXT .WIKI 0.193 0.136 0.118 0.221 0.112 0.105 0.148\nMONO .ISO .NOSPEC\nAVG(L≤2) 0.059 0.075 0.106 0.126 0.086 0.123 0.096\nAVG(L≤4) 0.061 0.069 0.098 0.111 0.075 0.106 0.087\nAVG(L≤6) 0.052 0.061 0.079 0.112 0.068 0.102 0.079\nAVG(L≤8) 0.042 0.048 0.075 0.112 0.063 0.105 0.074\nAVG(L≤10) 0.036 0.043 0.067 0.107 0.065 0.080 0.066\nAVG(L≤12) 0.032 0.034 0.059 0.097 0.077 0.083 0.064\nMONO .AOC-10.NOSPEC\nAVG(L≤2) 0.069 0.078 0.094 0.109 0.078 0.108 0.089\nAVG(L≤4) 0.076 0.105 0.119 0.112 0.098 0.117 0.104\nAVG(L≤6) 0.086 0.090 0.129 0.122 0.098 0.125 0.108\nAVG(L≤8) 0.092 0.073 0.137 0.105 0.100 0.114 0.103\nAVG(L≤10) 0.095 0.073 0.147 0.102 0.102 0.135 0.109\nAVG(L≤12) 0.104 0.073 0.139 0.100 0.105 0.131 0.109\nMONO .AOC-100. NOSPEC\nAVG(L≤2) 0.073 0.081 0.097 0.111 0.078 0.106 0.091\nAVG(L≤4) 0.078 0.107 0.115 0.107 0.100 0.115 0.104\nAVG(L≤6) 0.087 0.087 0.127 0.132 0.103 0.123 0.110\nAVG(L≤8) 0.091 0.076 0.137 0.118 0.101 0.106 0.105\nAVG(L≤10) 0.099 0.074 0.161 0.103 0.104 0.104 0.107\nAVG(L≤12) 0.106 0.076 0.146 0.105 0.106 0.100 0.106\nMONO .ISO .ALL\nAVG(L≤2) 0.044 0.045 0.076 0.095 0.067 0.098 0.071\nAVG(L≤4) 0.039 0.042 0.079 0.094 0.066 0.100 0.070\nAVG(L≤6) 0.024 0.034 0.069 0.089 0.066 0.094 0.063\nAVG(L≤8) 0.018 0.020 0.039 0.068 0.059 0.092 0.049\nAVG(L≤10) 0.016 0.016 0.030 0.048 0.058 0.067 0.039\nAVG(L≤12) 0.014 0.013 0.033 0.034 0.064 0.061 0.036\nMONO .ISO .WITHCLS\nAVG(L≤2) 0.050 0.057 0.086 0.106 0.071 0.108 0.080\nAVG(L≤4) 0.046 0.055 0.084 0.104 0.071 0.102 0.077\nAVG(L≤6) 0.032 0.042 0.076 0.103 0.066 0.097 0.069\nAVG(L≤8) 0.025 0.028 0.046 0.086 0.059 0.101 0.057\nAVG(L≤10) 0.021 0.030 0.037 0.072 0.057 0.079 0.049\nAVG(L≤12) 0.020 0.016 0.032 0.052 0.045 0.072 0.040\nMULTI .ISO .NOSPEC\nAVG(L≤2) 0.110 0.009 0.045 0.057 0.020 0.013 0.042\nAVG(L≤4) 0.100 0.007 0.075 0.044 0.025 0.011 0.044\nAVG(L≤6) 0.098 0.007 0.046 0.043 0.029 0.030 0.042\nAVG(L≤8) 0.088 0.008 0.052 0.043 0.032 0.031 0.042\nAVG(L≤10) 0.084 0.008 0.051 0.042 0.034 0.026 0.041\nAVG(L≤12) 0.082 0.006 0.048 0.039 0.037 0.024 0.039\nMULTI .AOC-10. NOSPEC\nAVG(L≤2) 0.127 0.013 0.049 0.027 0.019 0.009 0.041\nAVG(L≤4) 0.123 0.018 0.055 0.032 0.029 0.008 0.044\nAVG(L≤6) 0.120 0.018 0.055 0.051 0.042 0.009 0.049\nAVG(L≤8) 0.123 0.018 0.057 0.053 0.049 0.016 0.053\nAVG(L≤10) 0.127 0.019 0.062 0.050 0.051 0.018 0.054\nAVG(L≤12) 0.128 0.021 0.065 0.049 0.052 0.019 0.056\nMULTI .ISO .ALL\nAVG(L≤2) 0.072 0.005 0.032 0.014 0.016 0.004 0.024\nAVG(L≤4) 0.075 0.004 0.027 0.014 0.022 0.005 0.024\nAVG(L≤6) 0.065 0.004 0.026 0.015 0.027 0.007 0.024\nAVG(L≤8) 0.054 0.004 0.035 0.015 0.032 0.008 0.025\nAVG(L≤10) 0.054 0.005 0.032 0.017 0.035 0.007 0.025\nAVG(L≤12) 0.058 0.004 0.034 0.018 0.032 0.006 0.025\nMONO .ISO .NOSPEC (REVERSE )\nAVG(L≥12) 0.005 0.012 – 0.001 – – –\nAVG(L≥10) 0.002 0.002 – 0.001 – – –\nAVG(L≥8) 0.004 0.002 – 0.002 – – –\nAVG(L≥6) 0.014 0.006 – 0.004 – – –\nAVG(L≥4) 0.020 0.012 – 0.016 – – –\nAVG(L≥2) 0.024 0.019 – 0.043 – – –\nAVG(L≥0) 0.032 0.034 – 0.097 – – –\nTable 8: Results in the CLIR task across different language pairs and word vector extraction conﬁgurations. MAP\nscores reported; AVG(L≤n) means that we average representations over all Transformer layers up to the nth layer\n(included), where L = 0 refers to the embedding layer, L = 1 to the bottom layer, and L = 12 to the ﬁnal\n(top) layer. Different conﬁgurations are described in §2 and Table 1. Additional diagnostic experiments with\ntop-to-bottom layerwise averaging conﬁgs (REVERSE ) are run for a subset of languages: {EN, DE, FI }.\n7239\nConﬁguration EN DE\nFASTTEXT .WIKI 0.660±0.008 0.601±0.007\nRANDOM .XAVIER 0.473±0.003 0.512±0.008\nMONO .ISO .NOSPEC\nAVG(L≤2) 0.688±0.007 0.649±0.002\nAVG(L≤4) 0.698±0.002 0.664±0.004\nAVG(L≤6) 0.699±0.007 0.677±0.006\nAVG(L≤8) 0.706±0.003 0.674±0.016\nAVG(L≤10) 0.718±0.002 0.679±0.008\nAVG(L≤12) 0.714±0.012 0.673±0.003\nMONO .AOC-10.NOSPEC\nAVG(L≤2) 0.690±0.007 0.657±0.005\nAVG(L≤4) 0.705±0.006 0.671±0.009\nAVG(L≤6) 0.714±0.008 0.675±0.014\nAVG(L≤8) 0.722±0.004 0.681±0.010\nAVG(L≤10) 0.719±0.007 0.682±0.007\nAVG(L≤12) 0.720±0.005 0.680±0.007\nMONO .AOC-100. NOSPEC\nAVG(L≤2) 0.692±0.007 0.655±0.007\nAVG(L≤4) 0.709±0.007 0.670±0.005\nAVG(L≤6) 0.718±0.009 0.672±0.008\nAVG(L≤8) 0.717±0.003 0.680±0.006\nAVG(L≤10) 0.721±0.009 0.678±0.004\nAVG(L≤12) 0.715±0.003 0.678±0.006\nMONO .ISO .ALL\nAVG(L≤2) 0.688±0.008 0.654±0.012\nAVG(L≤4) 0.698±0.011 0.662±0.008\nAVG(L≤6) 0.711±0.005 0.664±0.005\nAVG(L≤8) 0.709±0.008 0.663±0.015\nAVG(L≤10) 0.712±0.006 0.669±0.003\nAVG(L≤12) 0.704±0.005 0.666±0.013\nMONO .ISO .WITHCLS\nAVG(L≤2) 0.693±0.004 0.649±0.016\nAVG(L≤4) 0.699±0.004 0.664±0.006\nAVG(L≤6) 0.709±0.002 0.671±0.006\nAVG(L≤8) 0.710±0.003 0.679±0.006\nAVG(L≤10) 0.713±0.006 0.670±0.007\nAVG(L≤12) 0.705±0.005 0.676±0.006\nMULTI .ISO .NOSPEC\nAVG(L≤2) 0.671±0.009 0.628±0.013\nAVG(L≤4) 0.669±0.006 0.640±0.004\nAVG(L≤6) 0.684±0.010 0.637±0.009\nAVG(L≤8) 0.680±0.005 0.647±0.006\nAVG(L≤10) 0.676±0.006 0.629±0.008\nAVG(L≤12) 0.681±0.005 0.637±0.004\nMULTI .AOC-10. NOSPEC\nAVG(L≤2) 0.674±0.005 0.635±0.011\nAVG(L≤4) 0.681±0.006 0.630±0.007\nAVG(L≤6) 0.692±0.008 0.649±0.010\nAVG(L≤8) 0.695±0.004 0.652±0.011\nAVG(L≤10) 0.704±0.005 0.657±0.012\nAVG(L≤12) 0.702±0.005 0.661±0.008\nMULTI .ISO .ALL\nAVG(L≤2) 0.674±0.004 0.626±0.014\nAVG(L≤4) 0.682±0.009 0.640±0.009\nAVG(L≤6) 0.680±0.002 0.632±0.007\nAVG(L≤8) 0.683±0.003 0.638±0.010\nAVG(L≤10) 0.678±0.007 0.638±0.015\nAVG(L≤12) 0.676±0.013 0.636±0.005\nMONO .ISO .NOSPEC (REVERSE )\nAVG(L≥12) 0.683±0.007 0.628±0.009\nAVG(L≥10) 0.692±0.014 0.628±0.008\nAVG(L≥8) 0.688±0.016 0.648±0.007\nAVG(L≥6) 0.704±0.015 0.658±0.006\nAVG(L≥4) 0.704±0.008 0.668±0.007\nAVG(L≥2) 0.707±0.008 0.667±0.004\nAVG(L≥0) 0.714±0.012 0.673±0.003\nTable 9: Results in the relation prediction task (RELP) across different word vector extraction conﬁgurations.\nMicro-averaged F1 scores reported , obtained as averages over 5 experimental runs for each conﬁguration; standard\ndeviation is also reported. AVG(L≤n) means that we average representations over all Transformer layers up to the\nnth layer (included), where L = 0 refers to the embedding layer, L = 1 to the bottom layer, and L = 12 to the\nﬁnal (top) layer. Different conﬁgurations are described in §2 and Table 1. RANDOM .XAVIER are 768-dim vectors\nfor the same vocabularies, randomly initialised via Xavier initialisation (Glorot and Bengio, 2010).\n7240\n(a) EN–DE: Word translation pairs\n (b) EN–DE: Random word pairs\nFigure 8: CKA similarity scores of type-level word representations extracted from each layer (using different\nextraction conﬁgurations, see Table 1) for a set of (a) 7K EN–DE translation pairs from the BLI dictionaries of\nGlavaˇs et al. (2019); (b) 7K random EN–DE pairs.\n(a) EN–FI: Word translation pairs\n (b) EN–FI: Random word pairs\nFigure 9: CKA similarity scores of type-level word representations extracted from each layer (using different\nextraction conﬁgurations, see Table 1) for a set of (a) 7K EN–FI translation pairs from the BLI dictionaries of\nGlavaˇs et al. (2019); (b) 7K random EN–FI pairs.\n(a) EN–TR: Word translation pairs\n (b) EN–TR: Random word pairs\nFigure 10: CKA similarity scores of type-level word representations extracted from each layer (using different\nextraction conﬁgurations, see Table 1) for a set of (a) 7K EN–TR translation pairs from the BLI dictionaries of\nGlavaˇs et al. (2019); (b) 7K random EN–TR pairs.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7868648767471313
    },
    {
      "name": "Natural language processing",
      "score": 0.6355902552604675
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5159509778022766
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4803268015384674
    },
    {
      "name": "Lexical item",
      "score": 0.4767150282859802
    },
    {
      "name": "Semantics (computer science)",
      "score": 0.468339204788208
    },
    {
      "name": "Lexical analysis",
      "score": 0.4567467272281647
    },
    {
      "name": "Linguistics",
      "score": 0.3730488419532776
    },
    {
      "name": "Programming language",
      "score": 0.07250842452049255
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I241749",
      "name": "University of Cambridge",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I177802217",
      "name": "University of Mannheim",
      "country": "DE"
    }
  ],
  "cited_by": 26
}