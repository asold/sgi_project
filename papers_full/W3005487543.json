{
    "title": "Aligning the Pretraining and Finetuning Objectives of Language Models",
    "url": "https://openalex.org/W3005487543",
    "year": 2022,
    "authors": [
        {
            "id": null,
            "name": "Pierse, Nuo Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2001536793",
            "name": "Lu Jingwen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2886885214",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2911109671",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W2970049541",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W2937297214",
        "https://openalex.org/W2997200074",
        "https://openalex.org/W2798778171",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3082274269"
    ],
    "abstract": "We demonstrate that explicitly aligning the pretraining objectives to the finetuning objectives in language model training significantly improves the finetuning task performance and reduces the minimum amount of finetuning examples required. The performance margin gained from objective alignment allows us to build language models with smaller sizes for tasks with less available training data. We provide empirical evidence of these claims by applying objective alignment to concept-of-interest tagging and acronym detection tasks. We found that, with objective alignment, our 768 by 3 and 512 by 3 transformer language models can reach accuracy of 83.9%/82.5% for concept-of-interest tagging and 73.8%/70.2% for acronym detection using only 200 finetuning examples per task, outperforming the 768 by 3 model pretrained without objective alignment by +4.8%/+3.4% and +9.9%/+6.3%. We name finetuning small language models in the presence of hundreds of training examples or less \"Few Example learning\". In practice, Few Example Learning enabled by objective alignment not only saves human labeling costs, but also makes it possible to leverage language models in more real-time applications.",
    "full_text": "Aligning the Pretraining and Finetuning Objectives of Language Models\nNuo Wang Pierse Jingwen Lu\nBing Core Relevance, Microsoft\nAbstract\nWe demonstrate that explicitly aligning the pre-\ntraining objectives to the ﬁnetuning objectives in\nlanguage model training signiﬁcantly improves\nthe ﬁnetuning task performance and reduces the\nminimum amount of ﬁnetuning examples re-\nquired. The performance margin gained from\nobjective alignment allows us to build language\nmodels with smaller sizes for tasks with less avail-\nable training data. We provide empirical evidence\nof these claims by applying objective alignment\nto concept-of-interest tagging and acronym detec-\ntion tasks. We found that, with objective align-\nment, our 768 by 3 and 512 by 3 Transformer lan-\nguage models can reach accuracy of 83.9%/82.5%\nfor concept-of-interest tagging and 73.8%/70.2%\nfor acronym detection using only 200 ﬁnetuning\nexamples per task, outperforming the 768 by 3\nmodel pretrained without objective alignment by\n+4.8%/+3.4% and +9.9%/+6.3%. We name ﬁne-\ntuning small language models in the presence of\nhundreds of training examples or less “Few Exam-\nple learning”. In practice, Few Example Learning\nenabled by objective alignment not only saves\nhuman labeling costs, but also makes it possible\nto leverage language models in more real-time\napplications.\n1. Introduction\nIn the past three years, new deep learning language model\narchitectures and techniques have signiﬁcantly advanced\nthe ﬁeld of computational linguistics. The current state-of-\nthe-art language models use large Transformer architectures\n(Vaswani et al., 2017) containing hundreds of millions of\nparameters and are pretrained using multi-billion word cor-\npuses. GPT (Radford et al., 2018) and BERT (Devlin et al.,\n2019) are two of the ﬁrst examples of these Transformer-\nbase language models. They outperformed the RNN-based\nCorrespondence to: Nuo Wang Pierse <nuwangpi@microsoft\n.com>. Copyright 2020 by the authors.\nmodels such as ELMo (Peters et al., 2018) by a signiﬁcant\nmargin in benchmarks like GLUE (Wang et al., 2019) and\nSQuAD (Rajpurkar et al., 2016). Since the release of GPT\nand BERT, many researchers have further improved the\nTransformer-based language models demonstrated by sur-\npassing their predecessors in the evaluations of the common\nbenchmarks (Liu et al., 2019c; Dong et al., 2019; Conneau\n& Lample, 2019; Conneau et al., 2019; Raffel et al., 2019;\nYang et al., 2019; Sun et al., 2020).\nThese new language model releases typically use more\nmodel parameters than their predecessors and are evalu-\nated only on the academic datasets. This contrasts the two\nmajor challenges that we face in building natural language\nunderstanding (NLU) applications: (1) Speeding up lan-\nguage model inference; (2) Shortage of ﬁnetuning data for\napplication-speciﬁc tasks. In our experience, the inference\nspeed of the 768 by 12 BERT-base model, the smallest\nBERT model, is far from meeting the requirements for most\nreal-time applications. To speed up language models, people\nhave developed libraries for fast neural network computa-\ntion (Zhang et al., 2018; Junczys-Dowmunt et al., 2018)\nand built smaller models with sufﬁcient prediction power\nusing knowledge distillation (Jiao et al., 2019) or improved\nTransformer architectures (Lan et al., 2020). In terms of the\nlearning techniques for low-resource tasks, knowledge trans-\nfer through pretraining itself is a partial solution. GPT-2\nhas demonstrated its impressive zero-shot learning capabil-\nity in text generation and question & answering (Radford\net al., 2019). Besides transfer learning, multitasking is an-\nother common technique used to boost low-resource task\nperformance (Lin et al., 2018; Liu et al., 2019a; Conneau &\nLample, 2019).\nIn this paper, we tackle both of the above challenges at once.\nWe develop a solution to train high-quality small language\nmodels using only a few hundred ﬁnetuning examples. We\nattempted to maximize the efﬁcacy of knowledge transfer\nby designing pretraining objectives that closely resemble\nthe ﬁnetuning objectives - we call this “explicit objective\nalignment”, “objective alignment” in short. In our tasks, ob-\njective alignment not only enabled smaller language models\nto perform the ﬁnetuning tasks equally well as their larger\ncounterparts pretrained without objective alignment, but\narXiv:2002.02000v1  [cs.CL]  5 Feb 2020\nAligning the Pretraining and Finetuning Objectives of Language Models\nalso reduced the number of ﬁnetuning examples required.\nWe were able to develop a concept-of-interest tagger and an\nacronym detector using a 768 by 3 Transformer model and\n200 ﬁnetuning examples for each task. We call ﬁnetuning in\nthis model size and data size limit Few Example Learning\n(FEL).\nThe main contributions of this paper are:\n• We propose pretraining objective alignment as a so-\nlution to developing small language models for NLU\napplications that have limited labeled data.\n• We demonstrate our solution by building two NLU\napplications of reasonable accuracy using a 768 by 3\nTransformer model and 200 ﬁnetuning examples each.\n• We provide detailed steps to carry out object alignment,\ncomplete descriptions of the training parameters and\nrecommendations for best practices.\n2. Objective Alignment and FEL\nInspired by the success of zero-shot learning (Radford et al.,\n2019), we developed a solution - objective alignment - to\ntackle the problem of ﬁnetuning data scarcity. Objective\nalignment in the scope of this paper is a type of trans-\nfer learning that specializes in transferring the knowledge\nneeded for the ﬁnetuning tasks of interest. “Alignment”\ndescribes our attempt to design new pretraining objectives\nthat resemble the ﬁnetuning objectives better than the stan-\ndard pretraining objectives such as masked language model\n(MLM) and next-sentence prediction (NSP) (Devlin et al.,\n2019). We provide detailed examples of objective alignment\nin Section 3.2.\nFEL is the process of ﬁnetuning small language models\nusing hundreds of examples or less after objective-aligned\npretraining. We provide a possible explanation of FEL in\nFigure 1. During pretraining, the model undergoes multi-\ntask learning and converges along the additively combined\nloss functions of the standard pretraining objectives and the\naligned pretraining objectives. The aligned objectives help\nthe model converge closer towards the ﬁnetuning objective\nloss function minimum in the ﬁnetuning task model param-\neter subspace. And as a result, the model only needs a small\namount of learning to converge in the ﬁnetuning phase. In\nour tasks, objective alignment and FEL allow a developer to\nproduce the necessary amount of application-speciﬁc ﬁne-\ntuning examples within hours and to ﬁnetune the model\nwithin minutes.\nIn this paper, we use the vanilla BERT architecture and focus\nour discussion on the independent contribution of objective\nalignment in improving language model performance. How-\never, to build the most competent small language model,\nwe recommend using objective alignment in combination\nmodel parameters\nloss\npretraining \nloss\nfinetuning \nloss\nFEL\nFigure 1.A possible explanation of objective alignment and\nFew Example Learning (FEL). The loss function minimum of\nan aligned pretraining objective (gray dot) is designed to be lo-\ncated near the loss function minimum of the ﬁnetuning objective\n(lower black dot) in the ﬁnetuning task model parameter subspace\n(“model parameters” in short). When the model switches from pre-\ntraining to ﬁnetuning, only a small amount of learning is necessary\nfor ﬁnetuning convergence (double-dashed arrow).\nwith knowledge distillation (Hinton et al., 2015; Liu et al.,\n2019b; Jiao et al., 2019), better Transformer architectures\n(Dai et al., 2019; Yang et al., 2019; Lan et al., 2020), bet-\nter pretraining setup and pretraining data (Liu et al., 2019c;\nDong et al., 2019; Conneau & Lample, 2019; Conneau et al.,\n2019; Raffel et al., 2019).\n3. Methods\n3.1. Finetuning Tasks\nWe carry out our investigation on objective alignment and\nFEL through concept-of-interest tagging (CT) and acronym\ndetection (AD) tasks. The technical implementations of CT\nand AD are shown in Figure 2.\nConcept-of-interest tagging (CT) . The goal of CT is to\nannotate the start and the end positions of concepts belong-\ning to the categories of interest within a short query text.\nWe deﬁne a concept as the shortest contiguous sequence of\ntext that carries a standalone meaning. For example, query\n“when was the ﬁrst super mario released” has four concepts\n“when was”, “the ﬁrst”, “super mario” and “released”. The\nconcept categories of interest are determined by downstream\napplications. In this paper, we build a concept tagger to an-\nnotate all concepts except for the ones that overlap with the\nfollowing six types of language features: (1) Five Ws (who,\nwhat, when, where, why); (2) Verbs and verb phrases; (3)\nPrepositions, conjunctions and determiners that are outside\nof ﬁxed expressions; (4) Comparatives and superlatives; (5)\nNumbers and IDs; (6) Natural language-type book, song,\nAligning the Pretraining and Finetuning Objectives of Language Models\nmovie titles. Based on these choices, our tagger should only\nannotate “super mario” in the example query above.\nAcronym detection (AD). The goal of AD is to determine\nwhether a unigram is a valid acronym of any entity present in\na text snippet. For example, given snippet “much of world of\nwarcraft’s gameplay involves the completion of quests. these\nquests are usually available from npcs.”, unigram “wow” is\na valid acronym as it matches to “world of warcraft”. But\nunigrams “tqa” (these quests are) and “woa” are not valid\nacronyms because they either do not match to an entity or do\nnot match any span of text. AD does not aim to memorize\n“wow” as the most common acronym for “world of warcraft”,\nbut only whether a unigram could be a valid acronym of an\nentity.\nwow\nTransformer layersTransformer layers\nsuper mario bros wii ofworld warcraft… …\n1S E~(S&E) S&E … …\nConcept-of-interest tagging Acronym detection\nemb\nlayer\npred\nlayer\nFigure 2.BERT-like implementation of concept-of-interest tag-\nging (CT) and acronym detection (AD). CT is a token-level 4-\nclass classiﬁcation task. The four classes are: The start of a concept\n(S), the end of a concept (E), both start and end (S&E), neither start\nnor end (∼(S&E)). CT uses “[CLS] query [SEP]” input format.\nAD is a sentence-level binary classiﬁcation task. The two classes\nare: acronym matches any entity in the snippet (1) and no match\n(0). AD uses “[CLS] acronym [SEP] snippet [SEP]” input format\nand the CLS token for the output. “emb layer” and “pred layer”\nstand for embedding layer and prediction layer respectively.\n3.2. Design Aligned Pretraining Objectives\nWe designed aligned pretraining objectives Wikipedia hy-\nperlink prediction for CT and pseudo acronym detection for\nAD.\nWikipedia hyperlink prediction. The majority of hyper-\nlink texts on Wikipedia pages are named-entities, common\nnouns and noun phrases. They contain specialized concept\nunderstanding knowledge for performing CT. We extract the\nhyperlink metadata from the ofﬁcial Wikipedia dump and\nuse the same CT 4-class classiﬁcation to predict the start\nand the end of hyperlinks in our pretraining. It is noteworthy\nthat training with Wikipedia hyperlinks does lead to anno-\ntations of the six unwanted categories described in Section\n3.1. Many book, movie and song titles are natural language-\nlike, starting with ﬁve Ws and containing verbs etc. Many\nproduct names and events contain numbers and IDs. We\nalso found that, MLM alone can provide good concept un-\nderstanding to the model, making the Wikipedia hyperlink\nprediction task partially redundant, we will provide more\ndiscussion on this in Section 5.1.\nPseudo acronym detection. To perform AD, the language\nmodel needs to understand both concepts and spelling.\nWhile MLM and Wikipedia hyperlink prediction inject\nconcept understanding knowledge into the model, we use\npseudo acronym detection task along with subword regular-\nization (Kudo, 2018a) to teach the model spelling. Pseudo\nacronym detection is a binary classiﬁcation task like AD,\nand we generate its training data in the following steps: (1)\nFrom a chunk of Wikipedia text, we take a random contigu-\nous span of 2 to 6 unigrams. We concatenate their initials\nas a positive example; (2) For the same chunk of text we\ngenerate a negative example using one of two methods with\nequal probabilities - randomly mutating one to all of the\nletters in the positive unigram or taking the positive unigram\nbelonging to another random chunk of text; (3) We make\nsure there is no accidental match for the negative examples.\n3.3. Pretraining\nOur pretraining dataset is composed of a ﬁltered Wikipedia\ncorpus (about 70% English, 30% non-English) and a set\nof internally collected web documents (about 50% En-\nglish, 50% non-English). We created the model vocabulary\nfrom our web document corpus using the unigram language\nmodel algorithm in the sentencepiece library (Kudo, 2018b).\nOur vocabulary covers 111K unique tokens and about 6000\nunique characters. The ﬁltered Wikipedia corpus tokenizes\nto 4.4 billion tokens and the web document corpus tokenizes\nto 1.6 billion tokens.\nWe used four pretraining objectives: (1) MLM; (2) NSP;\n(3) Wikipedia hyperlink prediction and (4) Pseudo acronym\ndetection. MLM and NSP are directly taken from the BERT\npaper (Devlin et al., 2019), but instead of masking tokens\nwith equal probabilities, we mask rarer tokens with higher\nprobabilities (Conneau & Lample, 2019). MLM and NSP\nlabels are generated randomly on-the-ﬂy and they are ap-\nplied to both corpuses. Wikipedia hyperlink prediction and\npseudo acronym detection labels are pre-generated across\nthe Wikipedia corpus only. Subword regularization is ap-\nplied to the web document corpus only to make pseudo\nacronym detection more challenging.\nWe trained a set of eleven language models that have the\nsame architecture as BERT but differ in dimensions. Nine\nof them have sizes 256, 512, 768 embedding dimensions\nby 1, 2, 3 Transformer layers. The last two are both 768\nby 3 models, one pretrained without Wikipedia hyperlink\nprediction, one pretrained without pseudo acronym detec-\ntion. All models have 64-dimension Transformer heads and\n3072 hidden dimensions. We use learning rate 0.0001 with\nAdam optimizer, dropout 0.1, batch size 256 and maximum\nsequence length 128. We train each model for 500 million\nexamples, which is 10 epochs of the Wikipedia corpus (our\nweb documents have shorter sequence lengths and higher\nAligning the Pretraining and Finetuning Objectives of Language Models\nexamples to total token ratio). It takes two weeks to pretrain\na 768 by 3 model on four Tesla P40 GPUs. Our model train-\ning code was forked from the XLM code base (Conneau &\nLample, 2019).\n3.4. Finetuning\nThe CT dataset contains 475 manually labeled English\nqueries and 2199 token-level labels with label distribution\n16% S, 16% E, 9% S&E, 59% ∼(S&E) (Figure 2). The\n475 examples further split into a 275 example subset and\na 200 example subset. The two subsets do not share any\nnon-stopword unigrams. The AD dataset contains 1200\nmanually labeled English acronym-snippet pairs with la-\nbel distribution 50% 0 and 50% 1. We created this dataset\nby taking 600 unique snippets each containing at least one\nunique named-entity with a known acronym. The known\nacronyms are the positive examples. We create one negative\nexample for each snippet using one of the two methods with\nequal probabilities - generating a pseudo acronym detection\npositive example or randomly mutating one or more letters\nin the positive acronym unigram. We made sure that the\nacronym unigrams never appear in the snippet and there is\nno accidental false negatives in the negative examples.\nFor each data point presented in Section 4, we perform 5-\nfold cross validation with two random seeds each - a total of\nten ﬁnetuning calculations. In the case of CT, instead of the\nstandard 5-fold cross validation, we do ﬁve random samples\nfrom the 275 example subset to form the training sets and\nuse the 200 example subset as the testset. By default, we use\nbatch size 64, dropout 0.1, learning rate 0.0001, ﬁnetuning\ndata size 200 and backpropagate only through the prediction\nlayers. We also discuss the effects of different ﬁnetuning\nparameters in Section 4. Finetuning calculations typically\nﬁnish within ﬁfteen minutes on one Tesla P40 GPU.\n4. Results\nWe evaluate the accuracy and perplexity of CT and AD tasks\nacross the eleven language models with different ﬁnetuning\nparameters. All values are averaged over ten ﬁnetuning\ncalculations (Section 3.4), and we provide the one standard\ndeviation values in Tables 1, 2 and 4. We use the stan-\ndard accuracy and perplexity calculation formulas, the exact\ncomputation methods can be found in the XLM code base\n(Conneau & Lample, 2019). Accuracy, i.e. precision with-\nout thresholding, is the determining metric for our NLU\napplications, but we also use perplexity to gauge the overall\nquality of our models.\n4.1. Case Studies\nWHP: [when was [the first [super mario] released]\nCT: when was the first [super mario] released\nWHP: [skyrim] [100% completion]\nCT: [skyrim] 100% completion\nWikipedia hyperlink prediction (WHP) and CT lead to dif-\nferent outcomes of query tagging. We use symbols “[” and\n“]” to denote the predicted concept boundaries by the model.\nConcepts containing ﬁve Ws (“when”), verbs (“was”, “re-\nleased”) and numbers (“ﬁrst”, “100%”) are not annotated in\nCT. Note that Wikipedia hyperlink prediction leads to nested\nannotations even though hyperlinks are non-overlapping.\nThis is because the classiﬁcation loss of a token is condi-\ntioned independently of the classes of their context tokens.\nSNIPPET:\nmuch of world of warcraft’s gameplay involves\nthe completion of quests. these quests are\nusually available from npcs.\nACRONYMS:\nwow: PAD score = 0.80, AD score = 0.71\ntqa: PAD score = 0.95, AD score = 0.34\nwoa: PAD score = 0.09, AD score = 0.18\nPseudo acronym detection (PAD) gives “wow” and “tqa”\npositive labels (score>0.5), while AD only gives “wow” a\npositive label. This is because “wow” is the only unigram\nthat matches to an entity. The model scores are the softmax\nprobabilities of the positive class. All examples above are\ngenerated by one of the ﬁnetuned 768 by 3 models.\n4.2. Objective Alignment and Model Size\nSince the models pretrained without Wikipedia hyperlink\nprediction and pseudo acronym detection (denoted by “-”)\ndo not have pre-minimized prediction layers for CT and\nAD, we discard the prediction layers of the pretrained\nmodels for fair comparisons (denoted by “*”). Table 1\nshows that smaller language models pretrained with ob-\njective alignment can outperform bigger language models\npretrained without objective alignment. Model 768 ×3*\nachieves +3.3% accuracy in CT and +9.9% accuracy in\nAD compared to model 768-×3* of the same size. When\ncomparing across model sizes, the smallest models that\nstatistically signiﬁcantly outperform model 768- ×3* are\nmodel 512×3* and model 256×3* for CT (+1.4%) and AD\n(+1.5%) respectively.\nKeeping the pre-minimized prediction layers of the objec-\ntive aligned models further improves the accuracy of the\nﬁnetuning results of CT but has little impacts on AD. On\nthe other hand, AD gets bigger accuracy boost than CT\nfrom both objective alignment and model size increasement.\nThis is most likely because of the standard language model\ntokenization algorithm we used. The algorithm tokenizes\nwords into the longest most probable subwords (Wu et al.,\nAligning the Pretraining and Finetuning Objectives of Language Models\nTable 1.CT and AD accuracy/perplexity by model size. Model accuracy and perplexity improve as model size increases. The boldface\nmodels statistically signiﬁcantly outperform model 768-×3*, which is pretrained without objective alignment (underlined). Models in\nrow CT, 768- are pretrained without Wikipedia hyperlink prediction. Models in row AD, 768- are pretrained without pseudo acronym\ndetection. Models in column 3* are ﬁnetuned after ﬁrst discarding the prediction layers.\n1 2 3 3*\nCT\n256 77.8 ±0.5 / 1.75 ±0.01 80.0 ±0.7 / 1.70 ±0.04 81.9 ±0.6 / 1.58 ±0.01 79.6 ±0.7 / 1.70 ±0.04\n512 77.2 ±0.8 / 1.78 ±0.03 82.1 ±0.5 / 1.56 ±0.01 82.5 ±0.6 / 1.55 ±0.04 81.5 ±0.4 / 1.62 ±0.02\n768 79.0 ±0.7 / 1.71 ±0.01 83.7 ±0.3 / 1.53 ±0.01 83.9 ±0.3 / 1.50 ±0.01 82.4 ±0.8 / 1.58 ±0.02\n768- / / / 79.1 ±0.9 / 1.76 ±0.06\nAD\n256 56.9 ±0.6 / 1.98 ±0.01 61.6 ±0.3 / 1.92 ±0.02 65.3 ±1.0 / 1.86 ±0.02 65.4 ±0.9 / 1.86 ±0.02\n512 57.6 ±0.8 / 1.97 ±0.01 66.4 ±0.5 / 1.87 ±0.02 70.2 ±1.5 / 1.79 ±0.03 70.2 ±1.6 / 1.79 ±0.03\n768 59.8 ±0.8 / 1.99 ±0.03 70.7 ±1.7 / 1.81 ±0.05 73.8 ±0.8 / 1.79 ±0.04 73.8 ±0.5 / 1.80 ±0.04\n768- / / / 63.9 ±1.3 / 1.96 ±0.05\nTable 2.CT and AD accuracy/perplexity by ﬁnetuning data size. Model accuracy and perplexity improve as ﬁnetuning data size\nincreases from 50 to 1000. The boldface models statistically signiﬁcantly outperform model 768- ×3*, which is ﬁnetuned using 200\nexamples (underlined). Symbols “-” and “*” carry the same meanings as in Table 1.\n768- ×3* 512 ×3* 768 ×3* 768 ×3\nCT\n50 72.4 ±2.1 / 2.05 ±0.10 76.4 ±1.1 / 1.84 ±0.05 77.7 ±1.4 / 1.78 ±0.03 82.0 ±0.5 / 1.59 ±0.02\n100 76.5 ±1.3 / 1.88 ±0.11 79.1 ±1.0 / 1.74 ±0.06 80.3 ±0.9 / 1.65 ±0.02 83.7 ±0.7 / 1.52 ±0.01\n150 77.9 ±1.0 / 1.81 ±0.10 80.4 ±0.8 / 1.69 ±0.07 81.8 ±0.5 / 1.60 ±0.02 83.7 ±0.7 / 1.51 ±0.01\n200 79.1 ±0.9 / 1.76 ±0.06 81.5 ±0.4 / 1.62 ±0.02 82.4 ±0.8 / 1.58 ±0.02 83.9 ±0.3 / 1.50 ±0.01\nAD\n50 58.7 ±1.8 / 2.00 ±0.05 64.3 ±0.6 / 1.91 ±0.04 65.2 ±1.4 / 1.97 ±0.08 65.0 ±1.6 / 1.98 ±0.05\n100 61.0 ±0.6 / 1.99 ±0.05 66.3 ±1.0 / 1.87 ±0.03 70.3 ±0.6 / 1.92 ±0.06 70.1 ±0.8 / 1.89 ±0.06\n150 62.4 ±1.8 / 1.97 ±0.07 68.6 ±1.7 / 1.83 ±0.03 72.5 ±0.5 / 1.87 ±0.04 72.3 ±0.7 / 1.83 ±0.03\n200 63.9 ±1.3 / 1.96 ±0.05 70.2 ±1.6 / 1.79 ±0.03 73.8 ±0.5 / 1.80 ±0.04 73.8 ±0.8 / 1.79 ±0.04\n1000 72.9 ±2.5 / 1.78 ±0.05 77.2 ±3.1 / 1.67 ±0.06 84.7 ±1.3 / 1.49 ±0.04 85.5 ±1.4 / 1.47 ±0.03\n2016; Kudo, 2018a) and as a result, the model does not\nknow the letter-by-letter spelling of a word explicitly. This\nmakes AD a more challenging task for the language models\nwe trained.\nAs expected, the best performing model of both CT and\nAD tasks is the largest model we trained, model 768×3. In\nterms of the embedding dimension, CT accuracy increased\n2.0% and AD accuracy increased 8.5% from model 256×3\nto model 768×3. In terms of the number of Transformer lay-\ners, CT accuracy increased 4.9% and AD accuracy increased\n14.0% from model 768×1 to model 768×3. Model 256×3\nsigniﬁcantly outperforms model 768×1, which shows, for\nthe vanilla Transformer architecture, at least two Trans-\nformer layers are recommended to learn tasks of moderate\ncomplexity.\n4.3. FEL and Training Data Size\nTable 2 shows that objective alignment reduces the amount\nof ﬁnetuning examples required to reach a targeted accuracy,\nand good objective alignment can reduce the minimum num-\nber of ﬁnetuning examples required to the order of hundreds.\nFor example, for the AD task, model 768 ×3* can reach\naccuracy of 72.5% using 150 ﬁnetuning examples while\nmodel 768-×3* requires 1000 examples to reach the same\naccuracy.\nThe accuracy of model 768 ×3 improves the most from\nusing 50 examples to 100 examples for both CT and AD.\nCT accuracy stops improving after 100 examples, while\nAD accuracy keeps improving linearly from 100 to 1000\nexamples. This again shows that AD is a more challenging\ntask and requires more examples to learn. Table 2 also\nshows that the observations made in Section 4.2 hold across\nall FEL training data sizes - the accuracy of model 768×3\n> model 768×3* > model 512×3* > model 768-×3*.\n4.4. FEL Training Parameters\nThe Transformer language models we use here are com-\nposed of the embedding layer, the Transformer layers and\nthe prediction layers (Figure 2). We perform ﬁnetuning by\nbackpropagating through (1) only the prediction layers, (2)\nthe prediction and the Transformer layers, and (3) all of the\nlayers (Table 3).\nWhen backpropagating through only the prediction layers,\nthe model reaches the best accuracy and perplexity at the\nAligning the Pretraining and Finetuning Objectives of Language Models\nTable 3.FEL backpropagation scheme. Backpropagating\nthrough both prediction (PRED) and Transformer (TRM) layers\nwhile multitasking (MT) achieves the best accuracy/perplexity\ncombination. The best-accuracy models are overﬁtted when back-\npropagating through the Transformer and embedding (EMB) lay-\ners. We recommend selecting models using perplexity. Results are\ngenerated using model 768×3.\nBACKPROPAGATION ACC /BEST -PPL BEST -ACC /PPL\nCT\nPRED 83.8 / 1.50 83.9 / 1.50\nMT PRED +TRM 84.8 / 1.47 86.0 / 1.75\nPRED +TRM 85.0 / 1.47 85.9 / 1.63\nPRED +TRM +EMB 84.8 / 1.48 85.9 / 1.67\nAD\nPRED 72.3 / 1.75 73.8 / 1.79\nMT PRED +TRM 74.2 / 1.69 84.8 / 2.38\nPRED +TRM 73.8 / 1.70 83.8 / 2.65\nPRED +TRM +EMB 73.3 / 1.71 83.9 / 2.63\nsame time. But when we extend the backpropagation to\nthe other layers, the model reaches the best perplexity ﬁrst,\nthen reaches the best accuracy while perplexity continues\nto increase. We see this as a sign of overﬁtting - the accu-\nracy of a portion of the testset examples increases while the\naccuracy of the rest of the testset worsens. Even though\naccuracy is generally the determining metric for our appli-\ncations, we pick the model with the best perplexity given\nthat the corresponding accuracy is acceptable. CT-AD mul-\ntitasking while backpropagating through the prediction and\ntransformer layers achieves the best accuracy and perplexity\nfor both tasks, showing that multitasking signiﬁcantly bene-\nﬁts FEL in its low-resource regime. Despite this, we choose\nto backpropagate only through the prediction layers as our\nbest practice, because it best prevents overﬁtting.\nWe also experimented with the choice of learning rate\nfor FEL (Table 4). We found that when backpropagating\nthrough the prediction layers only, the performance of CT\nand AD tasks are not very sensitive to learning rates in the\nrange of 0.001 and 0.00001. AD shows accuracy loss and\nperplexity gain as learning rate increases but no signiﬁcant\ncombined difference. We speculate that the loss functions\nfor CT and AD are smooth near their minima or that they\nhave multiple equally good closely spaced local minima.\nWe chose a learning rate of 0.0001 for the other FEL calcu-\nlations in this paper, as it provides the best combination of\nperformance and training time.\n5. Discussion\n5.1. Pretraining Objective Diversity\nIn Table 1, model 768×3* outperforms model 768-×3* by\n+3.3% accuracy in CT and +9.9% accuracy in AD. The\nimprovement of the CT task from objective alignment is\nTable 4.FEL learning rate. Learning rate does not have a sig-\nniﬁcant impact on the ﬁnetuning accuracy and perplexity of CT\nand AD. Smaller learning rates require more epochs of training.\nResults are generated using model 768×3.\nACC /PPL EPOCHS\nCT\n0.001 83.7 / 1.53 4.6 ±1.6\n0.0001 83.9 / 1.50 33.1 ±6.9\n0.00001 83.8 / 1.50 291.4 ±40.4\nAD\n0.001 74.8 / 2.01 84.4 ±17.9\n0.0001 73.8 / 1.79 156.2 ±33.3\n0.00001 71.7 / 1.76 903.5 ±128.4\nTable 5.Objective alignment vs. objective diversity. Pseudo\nacronym detection provides the language model more unique in-\nformation than Wikipedia hyperlink prediction. It not only sig-\nniﬁcantly improves the AD accuracy, but also improves the CT\naccuracy by more than 1%. Symbol “-” carries the same meaning\nas in Table 1.\n768×3* CT 768- ×3* AD 768- ×3*\nCT 82.4 / 1.58 79.1 /1.76 80.6 / 1.65\nAD 73.8 / 1.80 73.9 / 1.72 63.9 / 1.96\n3 times less than that of the AD task. We think this is be-\ncause that MLM and Wikipedia hyperlink prediction are\nintrinsically similar. The model has acquired most of the\nknowledge needed to perform CT from MLM and Wikipedia\nhyperlink prediction does not add signiﬁcant amount of\nnew information. Unlike Wikipedia hyperlink prediction,\npseudo acronym detection resembles neither MLM nor NSP.\nIt teaches unique new knowledge to the model that is nec-\nessary to perform AD. Additionally, in Table 5, model CT\n768-×3* pretrained without Wikipedia hyperlink prediction\ngives the same AD accuracy as model 768×3* pretrained\nwith all four pretraining objectives. Model AD 768-×3* pre-\ntrained without pseudo acronym detection instead reaches\n1.8% less CT accuracy compared to model 768×3*. This\nmeans, Wikipedia hyperlink prediction and pseudo acronym\ndetection are approximately responsible for 1.5% and 1.8%\nof the 3.3% CT accuracy improvement. On the other hand,\npseudo acronym detection is responsible for the entire 9.9%\nof the accuracy improvement for AD. These results show\nthat pseudo acronym detection not only helps the model\nperform AD better but also has the potential to beneﬁt other\ntasks that are seemingly less related. In future works, we\nplan to build a more universal language model by designing\nmore diverse pretraining objectives to align better with a\nbroader range of ﬁnetuning tasks.\nAligning the Pretraining and Finetuning Objectives of Language Models\nTable 6.MLM and NSP accuracy/perplexity. The MLM and\nNSP accuracy and perplexity remain the same with and without\nthe aligned pretraining objectives. Symbol “-” carries the same\nmeaning as in Table 1.\n768×3 CT 768- ×3 AD 768- ×3\nMLM 52.4 / 14.1 52.6 / 14.0 52.8 / 13.5\nNSP 99.1 / 1.02 99.1 / 1.02 99.2 / 1.02\n5.2. Low-Resource Tasks vs. High-Resource Tasks\nTable 6 shows that our added pretraining objectives do not\nlead to signiﬁcant variations of the accuracy and perplexity\nof MLM and NSP. Not surprisingly, while objective align-\nment and objective diversity could improve the performance\nof low-resource tasks, their effects on high-resource tasks\nare minimal. Models 768-×3 (CT) and 768-×3 (AD) show\nslightly better accuracy and perplexity than model 768×3.\nThis is mostly likely because that we pretrain all models for\n500 million examples regardless of the choice of pretraining\ntasks. When training without Wikipedia hyperlink predic-\ntion or pseudo acronym detection, the model effectively see\nmore MLM and NSP examples.\n5.3. The Limits of FEL\nObjective alignment is a technique that applies to all ﬁne-\ntuning tasks, but the same cannot be said for FEL. We apply\nFEL to CT and AD because they only require the under-\nstanding of a few rules that are made of the fundamental\nproperties of a language - concept-boundary, part of speech\nand spelling. We show that these small numbers of rules can\nbe explained by just a few examples. In fact, AD is push-\ning the limit of “developer-affordable” FEL. Even though\nmodel 768×3 can reach a reasonable AD accuracy using\n200 ﬁnetuning examples, for the best performance, 1000\nexamples or more are recommended, which is still a very\naffordable size when hiring human judges. FEL does not ap-\nply to tasks that require heavy memorization. For example,\na task to tag only the names of celebrities or a task to predict\nthe likelihood of a unigram being the ofﬁcial acronym of a\nnamed-entity.\n6. Conclusion\nWe developed the language model training techniques objec-\ntive alignment and FEL to tackle two challenges in building\nNLU applications: (1) Speeding up language model infer-\nence and (2) Shortage of ﬁnetuning data for application-\nspeciﬁc tasks. We produced a high-accuracy concept-of-\ninterest tagger and a medium-accuracy acronym detector\nusing our techniques. Both models have dimension 768\nby 3 and were trained with 200 ﬁnetuning examples each.\nThis small model size meets our real-time inference latency\nrequirements, and the small ﬁnetuning dataset can be gener-\nated by the developers themselves or a few human judges.\nOverall, our language model training solution greatly im-\nproves the efﬁciency and agility of our NLU application\ndevelopment cycles.\nReferences\nConneau, A. and Lample, G. Cross-lingual Language Model\nPretraining. In Advances in Neural Information Process-\ning Systems 32, pp. 7057–7067. Curran Associates, Inc.,\n2019.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V .,\nWenzek, G., Guzmn, F., Grave, E., Ott, M., Zettle-\nmoyer, L., and Stoyanov, V . Unsupervised Cross-\nlingual Representation Learning at Scale. arXiv preprint\narXiv:1911.02116, 2019.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and\nSalakhutdinov, R. Transformer-XL: Attentive Language\nModels Beyond a Fixed-Length Context. arXiv preprint\narXiv:1901.02860, 2019.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of Deep Bidirectional Transformers for Lan-\nguage Understanding. In Proc. of NAACL, 2019.\nDong, L., Yang, N., Wang, W., Wei, F., Liu, X., Wang, Y .,\nGao, J., Zhou, M., and Hon, H.-W. Uniﬁed Language\nModel Pre-training for Natural Language Understanding\nand Generation. In Advances in Neural Information Pro-\ncessing Systems 32, pp. 13042–13054. Curran Associates,\nInc., 2019.\nHinton, G., Vinyals, O., and Dean, J. Distilling the Knowl-\nedge in a Neural Network. In NIPS Deep Learning and\nRepresentation Learning Workshop, 2015.\nJiao, X., Yin, Y ., Shang, L., Jiang, X., Chen, X., Li, L.,\nWang, F., and Liu, Q. TinyBERT: Distilling BERT\nfor Natural Language Understanding. arXiv preprint\narXiv:1909.10351, 2019.\nJunczys-Dowmunt, M., Grundkiewicz, R., Dwojak, T.,\nHoang, H., Heaﬁeld, K., Neckermann, T., Seide, F., Ger-\nmann, U., Aji, A. F., Bogoychev, N., Martins, A. F. T.,\nand Birch, A. Marian: Fast Neural Machine Translation\nin C++. In Proceedings of ACL 2018, System Demonstra-\ntions, pp. 116–121, 2018.\nKudo, T. Subword Regularization: Improving Neural Net-\nwork Translation Models with Multiple Subword Can-\ndidates. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics, pp. 66–75.\n2018a.\nAligning the Pretraining and Finetuning Objectives of Language Models\nKudo, T. sentencepiece. https://github.com/\ngoogle/sentencepiece, 2018b. Accessed: July\n2019.\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma,\nP., and Soricut, R. ALBERT: A Lite BERT for Self-\nsupervised Learning of Language Representations. In\nInternational Conference on Learning Representations,\n2020.\nLin, Y ., Yang, S., Stoyanov, V ., and Ji, H. A Multi-lingual\nMulti-task Architecture for Low-resource Sequence La-\nbeling. In Proceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pp. 799–809, 2018.\nLiu, X., He, P., Chen, W., and Gao, J. Multi-Task Deep\nNeural Networks for Natural Language Understanding.\nIn Proceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pp. 4487–4496,\n2019a.\nLiu, X., He, P., Chen, W., and Gao, J. Improving Multi-\nTask Deep Neural Networks via Knowledge Distillation\nfor Natural Language Understanding. arXiv preprint\narXiv:1904.09482, 2019b.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoBERTa: A Robustly Optimized BERT Pretraining Ap-\nproach. arXiv preprint arXiv:1907.11692, 2019c.\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark,\nC., Lee, K., and Zettlemoyer, L. Deep Contextualized\nWord Representations. In Proc. of NAACL, 2018.\nRadford, A., Narasimhan, K., Salimans, T., and Sutskever,\nI. Improving Language Understanding by Generative\nPre-Training. Technical report, OpenAI, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language Models are Unsupervised Multi-\ntask Learners. Technical report, OpenAI, 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y ., Li, W., and Liu, P. J. Exploring the\nLimits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. arXiv preprint arXiv:1910.10683, 2019.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. SQuAD:\n100,000+ Questions for Machine Comprehension of Text.\nIn Proceedings of the 2016 Conference on Empirical\nMethods in Natural Language Processing , pp. 2383–\n2392, 2016.\nSun, Y ., Wang, S., Li, Y ., Feng, S., Tian, H., Wu, H., and\nWang, H. ERNIE 2.0: A Continual Pre-training Frame-\nwork for Language Understanding. AAAI, 2020.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser,Ł., and Polosukhin, I. Attention\nis All You Need. In Advances in Neural Information\nProcessing Systems 30, pp. 5998–6008. 2017.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. R. GLUE: A Multi-Task Benchmark and\nAnalysis Platform for Natural Language Understanding.\nIn ICLR, 2019.\nWu, Y ., Schuster, M., Chen, Z., Le, Q. V ., Norouzi, M.,\nMacherey, W., Krikun, M., Cao, Y ., Gao, Q., Macherey,\nK., Klingner, J., Shah, A., Johnson, M., Liu, X., ukasz\nKaiser, Gouws, S., Kato, Y ., Kudo, T., Kazawa, H.,\nStevens, K., Kurian, G., Patil, N., Wang, W., Young, C.,\nSmith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G.,\nHughes, M., and Dean, J. Googles Neural Machine Trans-\nlation System: Bridging the Gap Between Human and\nMachine Translation. arXiv preprint arXiv:1609.08144,\n2016.\nYang, Z., Dai, Z., Yang, Y ., Carbonell, J., Salakhutdinov,\nR. R., and Le, Q. V . XLNet: Generalized Autoregressive\nPretraining for Language Understanding. In Advances\nin Neural Information Processing Systems 32, pp. 5754–\n5764. 2019.\nZhang, M., Rajbhandari, S., Wang, W., and He, Y . Deep-\nCPU: Serving RNN-based Deep Learning Models 10x\nFaster. In USENIX ATC 18, pp. 951–965, 2018."
}