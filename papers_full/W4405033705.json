{
    "title": "Generating AI Literacy MCQs: A Multi-Agent LLM Approach",
    "url": "https://openalex.org/W4405033705",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2103656757",
            "name": "Jiayi Wang",
            "affiliations": [
                "Northwestern University"
            ]
        },
        {
            "id": "https://openalex.org/A2916691539",
            "name": "Ruiwei Xiao",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A5074490904",
            "name": "Ying Jui Tseng",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4389471004",
        "https://openalex.org/W2915842190",
        "https://openalex.org/W4386266200",
        "https://openalex.org/W4400210662",
        "https://openalex.org/W4400187437",
        "https://openalex.org/W4307410661",
        "https://openalex.org/W4392865223"
    ],
    "abstract": "Artificial intelligence (AI) is transforming society, making it crucial to\\nprepare the next generation through AI literacy in K-12 education. However,\\nscalable and reliable AI literacy materials and assessment resources are\\nlacking. To address this gap, our study presents a novel approach to generating\\nmultiple-choice questions (MCQs) for AI literacy assessments. Our method\\nutilizes large language models (LLMs) to automatically generate scalable,\\nhigh-quality assessment questions. These questions align with user-provided\\nlearning objectives, grade levels, and Bloom's Taxonomy levels. We introduce an\\niterative workflow incorporating LLM-powered critique agents to ensure the\\ngenerated questions meet pedagogical standards. In the preliminary evaluation,\\nexperts expressed strong interest in using the LLM-generated MCQs, indicating\\nthat this system could enrich existing AI literacy materials and provide a\\nvaluable addition to the toolkit of K-12 educators.\\n",
    "full_text": "Generating AI Literacy MCQs: A Multi-Agent LLM Approach\nJiayi Wang\njiayiwang2025@u.northwestern.edu\nNorthwestern University\nEvanston, Illinois, USA\nRuiwei Xiao\nruiweix@andrew.cmu.edu\nCarnegie Mellon University\nPittsburgh, Pennsylvania, USA\nYing-Jui Tseng\nyingjuit@andrew.cmu.edu\nCarnegie Mellon University\nPittsburgh, Pennsylvania, USA\nAbstract\nArtificial intelligence (AI) is transforming society, making it crucial\nto prepare the next generation through AI literacy in K-12 education.\nHowever, scalable and reliable AI literacy materials and assessment\nresources are lacking. To address this gap, our study presents a\nnovel approach to generating multiple-choice questions (MCQs)\nfor AI literacy assessments. Our method utilizes large language\nmodels (LLMs) to automatically generate scalable, high-quality\nassessment questions. These questions align with user-provided\nlearning objectives, grade levels, and Bloom’s Taxonomy levels.\nWe introduce an iterative workflow incorporating LLM-powered\ncritique agents to ensure the generated questions meet pedagogical\nstandards. In the preliminary evaluation, experts expressed strong\ninterest in using the LLM-generated MCQs, indicating that this\nsystem could enrich existing AI literacy materials and provide a\nvaluable addition to the toolkit of K-12 educators.\nACM Reference Format:\nJiayi Wang, Ruiwei Xiao, and Ying-Jui Tseng. 2025. Generating AI Literacy\nMCQs: A Multi-Agent LLM Approach. In Proceedings of the 56th ACM\nTechnical Symposium on Computer Science Education V. 2 (SIGCSE TS 2025),\nFebruary 26-March 1, 2025, Pittsburgh, PA, USA. ACM, New York, NY, USA,\n2 pages. https://doi.org/10.1145/3641555.3705189\n1 Introduction\nAI is rapidly transforming our world, from the way we work to the\nway we interact with each other. To prepare the next generation for\nthis AI-driven world, educational frameworks like AI4K12’s \"Five\nBig Ideas\"[9] (Perception, Representation and Reasoning, Learning,\nNatural Interaction, and Natural Interaction) have been developed\nto integrate AI literacy into K-12 education.\nHowever, as a relatively new subject area, there is a lack of\nlearning materials in AI literacy instructional and assessment\nactivities[4]. In addition, existing courses and assessment resources\nare often not scalable or reliable [10]. This shortage presents a sig-\nnificant challenge for teachers who must teach and assess students’\nunderstanding of AI concepts while fostering critical thinking skills\nfor responsible engagement with these technologies.\nLeveraging Large Language Models (LLMs) to generate assess-\nment questions [2, 3, 8] offers a potential solution. This method\nhas shown promise in subjects like programming[3], medicine[5],\nand language learning[2]. The potential to apply these methods to\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nSIGCSE TS 2025, February 26-March 1, 2025, Pittsburgh, PA, USA\n© 2025 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0532-8/25/02\nhttps://doi.org/10.1145/3641555.3705189\nAI literacy assessments could help bridge the gap for educators by\nproviding accessible, scalable assessment resources.\nThis exploratory study, conducted as part of the active.ai1 project,\nbuilds on previous research on MCQ generation [3, 6] and AI liter-\nacy [9]. It seeks to address the following research question: Can\nLLM-powered multi-agent workflows effectively generate\nhigh-quality MCQs for AI literacy?\n2 Methodology\nFigure 1: Multi-Agent MCQ Generation System.\nThe Multi-Agent MCQ Generation System developed for this\nstudy generates and refines MCQs to assess AI literacy, using\na workflow built on the LangGraph framework 2, and OpenAI’s\ngpt-4o-mini-2024-07-18 model.\nUser inputs, including learning objectives, Bloom’s taxonomy\nlevels [1], grade level, and specific scenarios, guide the Generator\nAgent in producing an initial question with fields stem (question),\nkey (correct option), and distractors (incorrect options). The ques-\ntion is independently reviewed by two critique agents: a Language\nCritique Agent, which evaluates readability and alignment with\ngrade level, and an IWF (Item-Writing Flaw) Critique Agent [ 6],\nwhich applies rule-based checks (modified from [6]) for issues such\nas implausible distractors or absolute terms. A question with 0 or 1\nflaw is considered acceptable. Based on feedback from these agents,\nthe question is either approved by the Supervisor Agent or sent\nback to the Generator Agent for revision. This iterative process\ncontinues until the question meets quality standards or reaches the\nmaximum number of revisions. We generated a total of 40 multiple\nchoice questions for students in grades K7-9.\nThe following is an example of a generated question:\nSample Generated Question\nBen is considering using an AI tool to help him write a creative story. Which\nof the following reasons best explains when using AI might be a bad choice\nfor his learning?\nA. It may produce a story that lacks originality and personal expression.\nB. AI can provide quick feedback on grammar and structure.\nC. Using AI can help him brainstorm new ideas for his story.\nD. AI tools can assist in organizing his thoughts more effectively.\n1https://active-ai.vercel.app\n2https://www.langchain.com/langgraph\narXiv:2412.00970v1  [cs.HC]  1 Dec 2024\nSIGCSE TS 2025, February 26-March 1, 2025, Pittsburgh, PA, USA Jiayi Wang, Ruiwei Xiao, and Ying-Jui Tseng\n3 Results\nThree experts with more than a year of K-12 AI Literacy teaching\nexperiences, evaluated the quality of each of the 40 MCQs used a\nten-item rubric (Table 1 modified from [7]).\nRubric Item Definition and Response Option\nUnderstandableCould you understand what the question is asking?(yes/no)\nLORelated Is the question related to the learning objective?(yes/no)\nGrammatical Is the question grammatically well-formed?(yes/no)\nClear Is it clear what the question asks for?(yes/more_or_less/no)\nRephrase Could you rephrase the question to make it clearer and error-free?\n(yes/no)\nAnswerable Can students answer the question with the information or context\nprovided within?(yes/no)\nCentral Do you think being able to answer the question is important to work\non the topic given in the prompt?(yes/no)\nWouldYouUseItIf you were a teacher teaching the course topic would you\nuse this question or the rephrased version in the course?\n(this/rephrased/both/neither)\nBloom’sLevel Do you think the question is of the Bloom’s taxonomy level labeled?\n(yes/no)\nGradeLevel Do you think the question is appropriate for K7-9?(yes/no)\nTable 1: Rubric for Question Evaluation\nExperts 1, 2, and 3 agreed with the system on 97.5%, 85%, and\n85% of the correct answers, respectively. This suggests that the\nsystem generally produces correct answers that align with expert\njudgments. Although rare, there are also occasional misalignments\nbetween the system’s generated answers and expert evaluations.\nThere was strong agreement among experts on syntactical cor-\nrectness, with high \"Yes\" responses for criteria such as Understand-\nable, Answerable, Grammatical, and Clear, with average percent-\nages ranging from 93.3% to 99.2%. This suggests that the questions\ngenerated by the multi-agent approach are well-formed and clear.\nAdditionally, experts agreed that the questions were highly rele-\nvant to the learning objectives (LORelated 98.3%) and central to the\ntopics being assessed (Central 98.3%).\nThere was noticeable disagreement among the experts in crite-\nrion Rephrase, Bloom’sLevel, and GradeLevel. Expert 2 was stricter,\nsuggesting that 97.5% of the questions needed rephrasing, while\nExperts 1 and 3 flagged fewer questions (7.5% and 17.5%). The same\ntrend is seen with theGradeLevel, where Expert 2 marked 20% of the\nquestions as inappropriate, compared to Expert 1 at 7.5% and Expert\n3 at 5%. These discrepancies likely arise from varying interpreta-\ntions of the rubric or differences in pedagogical approaches. Expert\n1 significantly differed from Experts 2 and 3 on the Bloom’sLevel\ncriterion, marking only 35% of the questions as appropriate for\nthe intended cognitive level, while the other two experts rated all\nquestions as aligned with the correct Bloom’s level.\nFinally, there was moderate agreement on the WouldYouUseIt\ncriterion, with expert responses ranging from 77.5% to 95% (average\n84.2%). This indicates that a significant majority of the questions\nFigure 2: Evaluation Result\nwere seen as suitable for classroom use, demonstrating that the\nmulti-agent system can produce questions that are not only well-\nformed but also useful in practical educational settings. However,\nthe diverse responses suggest that individual preferences or teach-\ning styles influence the willingness to use certain questions.\nOverall, while some refinement is necessary to accommodate di-\nverse educational contexts, the multi-agent approach demonstrates\nstrong potential to generate pedagogically sound and scalable high-\nquality assessment questions for AI literacy in K-12 education.\n4 Limitations and Future Work\nOne avenue for future work is to conduct classroom trials to eval-\nuate student learning outcomes and teacher usability. While our\nmulti-agent system successfully generated AI literacy MCQs, it has\nnot been tested in real-world classrooms, so its effectiveness in\ndiverse learning environments is still uncertain. Another important\ndirection for future research is to explore additional methods for\nevaluating question quality beyond expert assessments, such as\nleveraging student performance data, crowd-sourced feedback, and\nautomated analysis to provide a more comprehensive and objec-\ntive evaluation. Our current evaluation relied on expert judgments,\nwhich introduces potential subjectivity. Key priorities include ex-\npanding the system’s adaptability to more grade levels, Bloom’s\nTaxonomy levels, and subjects beyond AI literacy, as well as inte-\ngrating diverse assessment formats (e.g., open-ended, project-based\nquestions) to enhance its practical utility. Finally, the tool can help\nteachers design AI literacy courses using a backward design ap-\nproach, aligning assessments with learning objectives.\nReferences\n[1] Lorin W. Anderson and David R. Krathwohl (Eds.). 2001.A Taxonomy for Learning,\nTeaching, and Assessing: A Revision of Bloom’s Taxonomy of Educational Objectives\n(complete ed ed.). Longman, New York.\n[2] Andrew Caines, Luca Benedetto, Shiva Taslimipoor, Christopher Davis, Yuan\nGao, Oeistein Andersen, Zheng Yuan, Mark Elliott, Russell Moore, Christopher\nBryant, et al. 2023. On the application of large language models for language\nteaching and assessment technology. arXiv preprint arXiv:2307.08393 (2023).\n[3] Jacob Doughty, Zipiao Wan, Anishka Bompelli, Jubahed Qayum, Taozhi Wang,\nJuran Zhang, Yujia Zheng, Aidan Doyle, Pragnya Sridhar, Arav Agarwal, et al.\n2024. A comparative study of AI-generated (GPT-4) and human-crafted MCQs\nin programming education. In Proceedings of the 26th Australasian Computing\nEducation Conference. 114–123.\n[4] Christina Gardner-McCune, David Touretzky, Fred Martin, and Deborah Seehorn.\n2019. AI for K-12: Making room for AI in K-12 CS curricula. In Proceedings of the\n50th ACM Technical Symposium on Computer Science Education . 1244–1244.\n[5] Inthrani Raja Indran, Priya Paranthaman, Neelima Gupta, and Nurulhuda Mustafa.\n2024. Twelve tips to leverage AI for efficient and effective medical question\ngeneration: a guide for educators using Chat GPT. Medical Teacher (2024), 1–6.\n[6] Steven Moore, Huy A Nguyen, Tianying Chen, and John Stamper. 2023. Assessing\nthe quality of multiple-choice questions using gpt-4 and rule-based methods. In\nEuropean Conference on Technology Enhanced Learning . Springer, 229–245.\n[7] Nicy Scaria, Suma Dharani Chenna, and Deepak Subramani. 2024. Automated\nEducational Question Generation at Different Bloom’s Skill Levels Using Large\nLanguage Models: Strategies and Evaluation. In International Conference on Arti-\nficial Intelligence in Education . Springer, 165–179.\n[8] John Stamper, Ruiwei Xiao, and Xinying Hou. 2024. Enhancing llm-based feed-\nback: Insights from intelligent tutoring systems and the learning sciences. In\nInternational Conference on Artificial Intelligence in Education . Springer, 32–43.\n[9] David S. Touretzky, Christina Gardner-Mccune, and Deborah W. Seehorn. 2022.\nMachine Learning and the Five Big Ideas in AI. International Journal of Artificial\nIntelligence in Education 33 (2022), 233–266.\n[10] Ying-Jui Tseng, Ruiwei Xiao, Christopher Bogart, Jaromir Savelka, and Majd Sakr.\n2024. Assessing the Efficacy of Goal-Based Scenarios in Scaling AI Literacy for\nNon-Technical Learners. In Proceedings of the 55th ACM Technical Symposium on\nComputer Science Education V. 2 . 1842–1843."
}