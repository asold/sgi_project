{
    "title": "Toward Better Storylines with Sentence-Level Language Models",
    "url": "https://openalex.org/W3021534166",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2799006035",
            "name": "Daphne Ippolito",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2959607770",
            "name": "David Grangier",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2141395071",
            "name": "Douglas Eck",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4207984947",
            "name": "Chris Callison-Burch",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4289360545",
        "https://openalex.org/W2997419939",
        "https://openalex.org/W2962895717",
        "https://openalex.org/W2739677784",
        "https://openalex.org/W2786464815",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2936695845",
        "https://openalex.org/W2790165607",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2739992537",
        "https://openalex.org/W4288410857",
        "https://openalex.org/W2909205211",
        "https://openalex.org/W2983962589",
        "https://openalex.org/W2889002152",
        "https://openalex.org/W2970038984",
        "https://openalex.org/W2996403597",
        "https://openalex.org/W1486649854",
        "https://openalex.org/W2758815496",
        "https://openalex.org/W2740704513",
        "https://openalex.org/W2964222271",
        "https://openalex.org/W2169546346",
        "https://openalex.org/W2995367342",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2621430944",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2964914104",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963644595",
        "https://openalex.org/W2963096510",
        "https://openalex.org/W2963993699",
        "https://openalex.org/W2270070752",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2950590791",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2794557536",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2466175319",
        "https://openalex.org/W2752047430",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W2914855263",
        "https://openalex.org/W2964303116"
    ],
    "abstract": "We propose a sentence-level language model which selects the next sentence in a story from a finite set of fluent alternatives. Since it does not need to model fluency, the sentence-level language model can focus on longer range dependencies, which are crucial for multi-sentence coherence. Rather than dealing with individual words, our method treats the story so far as a list of pre-trained sentence embeddings and predicts an embedding for the next sentence, which is more efficient than predicting word embeddings. Notably this allows us to consider a large number of candidates for the next sentence during training. We demonstrate the effectiveness of our approach with state-of-the-art accuracy on the unsupervised Story Cloze task and with promising results on larger-scale next sentence prediction tasks.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7472–7478\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n7472\nToward Better Storylines with Sentence-Level Language Models\nDaphne Ippolito†∗\ndaphnei@seas.upenn.edu\nDavid Grangier†\ngrangier@google.com\nDouglas Eck†\ndeck@google.com\nChris Callison-Burch†*\nccb@seas.upenn.edu\nAbstract\nWe propose a sentence-level language model\nwhich selects the next sentence in a story from\na ﬁnite set of ﬂuent alternatives. Since it does\nnot need to model ﬂuency, the sentence-level\nlanguage model can focus on longer range\ndependencies, which are crucial for multi-\nsentence coherence. Rather than dealing with\nindividual words, our method treats the story\nso far as a list of pre-trained sentence embed-\ndings and predicts an embedding for the next\nsentence, which is more efﬁcient than predict-\ning word embeddings. Notably this allows us\nto consider a large number of candidates for\nthe next sentence during training. We demon-\nstrate the effectiveness of our approach with\nstate-of-the-art accuracy on the unsupervised\nStory Cloze task and with promising results on\nlarger-scale next sentence prediction tasks.\n1 Introduction\nComputer generation of stories and other kinds of\ncreative writing is a challenging endeavor. It en-\ntangles two difﬁcult tasks: the generation of ﬂuent\nnatural language and the generation of a coherent\nstoryline. In the recent year, neural language mod-\nels have made tremendous progress with respect\nto ﬂuency (Bahdanau et al., 2015; Vaswani et al.,\n2017; Bengio et al., 2003; Devlin et al., 2019),\nbut coherency is still a major challenge (See et al.,\n2019). The generation of coherent stories has re-\ncently been addressed with additional conditioning:\nFan et al. (2018) suggest conditioning on a story\nprompt, Clark et al. (2018) propose collaboration\nbetween a generative model and a human writer,\nand Guan et al. (2019) suggest attending to a com-\nmonsense graph relevant to the story plot. Con-\nditioning based on a generated story plan (Martin\net al., 2018; Fan et al., 2019; Yao et al., 2019), a se-\n∗University of Pennsylvania, †Google\nquence of images (Chandu et al., 2019) or character\nroles (Liu et al., 2020) have also been considered.\nOur work is orthogonal to these efforts. Rather\nthan considering additional conditioning, we pro-\npose a model which takes as input several sentences\nof context and selects the best next sentence within\na large set of ﬂuent candidate sentences. We lever-\nage pre-trained BERT embeddings (Devlin et al.,\n2019) to build this sentence-level language model.\nGiven the embeddings of the previous sentences\nof the story, our model learns to predict a likely\nembedding of the next sentence.\nThis task isolates the modeling of long-range\ndependencies from the prediction of individual\nwords, which has several advantages. First, since\nour model only needs to determine how well each\ncandidate sentence would ﬁt as a coherent con-\ntinuation to the story, it does not spend capacity\nand time to learn ﬂuency. Second, our model does\nnot manipulate individual words but full sentences,\nwhich allows us to consider tens of thousands of\ncandidate sentences at a time. This contrasts with\nprior work (Logeswaran and Lee, 2018) where the\nneed to learn token-level representations limited\nthe number of candidate next sentences that could\nbe considered to a few hundred. Third, we can rely\non compact model architectures that train quickly\nbecause we take advantage of strong semantic rep-\nresentation from a pre-trained bidirectional lan-\nguage model, BERT, as our sentence embeddings.\nOf course, these beneﬁts also imply that our sen-\ntence representation is limited to the information\nextracted by the pre-trained model. Nevertheless,\nwe show that our model achieves state-of-the-art\naccuracy among unsupervised approaches on the\nStory Cloze task: predicting which of two sen-\ntences coherently ends a short story.\nOur work also opens up the possibility of rank-\ning thousands of candidate sentences from a large\nliterature repository. On the ROC Stories dataset,\n7473\nwe observe that training with a large number of\ncandidates is key for selecting the most coherent\nending among a large set of candidates at test time.\nWe also show preliminary results on the efﬁcacy\nof our method for ranking candidate next sentence\non the Toronto Book Corpus (Kiros et al., 2015),\na much larger book dataset. We envision that our\nmethods for scoring many candidate next sentences\nby their coherence with the context might be useful\nto downstream generation tasks where it is possi-\nble to generate many ﬂuent continuations of a text,\nbut it remains an unsolved problem how to reﬁne\nand choose the best of them. To encourage this\nexploration, we release our code and models1.\n2 Proposed Method\nWe propose a sentence-level language model: our\nmodel estimates P(st+1|s1:t), the probability dis-\ntribution for sentence st+1 given the t previous sen-\ntences, s1, . . . st. Since it is intractable to marginal-\nize over all possible candidate next sentences, we\nconsider a ﬁnite but large set of N valid, ﬂuent sen-\ntences. Without loss of generality, we can consider\nst+1 ∈{1, . . . , N}as an integer index into that\nset of possible next sentences. This strategy resem-\nbles negative sampling in word2vec (Mikolov et al.,\n2013).\nOur model represents sentences with pre-\ncomputed vector embeddings. Speciﬁcally, sen-\ntences are represented by the mean of the 768-\ndimensional contextual word embeddings of the\nsecond-to-last layer of BERT (Devlin et al., 2019).\nThis representation has shown to encode more\ntransferable features compare to other layers (Liu\net al., 2019). Alternative sentence representations\nwere considered, including embeddings from the\nuniversal sentence encoder (Cer et al., 2018) and a\nweighted mean of the BERT embeddings using in-\nverse document frequency weighting (Zhang et al.,\n2019). None of these alternatives improved our\nresults however.\nMotivated by simplicity, we consider a classical\nmulti-layer perceptron (MLP) fθ which takes as in-\nput the context sentence embeddings concatenated\ninto a single vector. At the output layer, we perform\na softmax operation. If we represent candidate sen-\ntences {1, . . . , N}by the embeddings {ei}N\ni=1, our\nmodel estimates the probability that i is the next\n1Code for ROC Stories experiments can be found\nat https://github.com/google-research/google-research/tree/\nmaster/better storylines.\nsentence by the softmax\nlog P(st+1 = i|s1:t) =e⊤\ni h −log Z(h)\nwhere h = fθ(s1:t) is the output of the MLP given\ncontext s1:t, and Z(h) = ∑N\nj=1 exp e⊤\nj h is the\npartition function. At train time, the candidate\nset {1, . . . , N}consists of the correct next sen-\ntence along with N −1 distractor sentences. The\ndistractors can either be static (the same set used\nthroughout training) or dynamic (picked at random\nfrom a larger set for each train batch). In this case,\nthe “vocabulary” of next values to choose from\nchanges with each train step, similar to negative\nsampling (Mikolov et al., 2013). At test time, novel\nsentences can be embedded with BERT and scored\nby our model.\nLike a classical language model, we optimize for\nthe likelihood of the true next sentence’s embed-\nding. However, when training we found that the\nsentences from the context (s1, . . . , st) often ended\nup being given very high scores by our model.\nInspired by work in sentence reordering (Lapata,\n2003; Logeswaran and Lee, 2018), we incorporated\nan auxiliary loss, which we refer to asCSLoss, that\nonly includes the context sentences s1:t in the dis-\ntractor set.\nLastly, we consider a residual variant of the\nMLP (referred to as resMLP) with skip connection\nbetween layers, as described in He et al. (2016).\nThe residual model trains faster and sometimes\nachieves higher accuracy than the non-residual\nmodel. Though we experimented with recur-\nrent (Sundermeyer et al., 2012) and self-attention\n(Vaswani et al., 2017) models, we did not observe\nimprovements, perhaps because the input to our\nmodel is already the high-dimensional output of a\nlarge mask language model. We leave deeper archi-\ntecture exploration, which will be especially critical\nas context length is extended, to future work.\n3 Experimental Setup\nWe ﬁrst describe our experiments on the ROC Sto-\nries dataset of short 5-sentence stories before show-\ning our setup on the larger Toronto Book Corpus.\n3.1 ROC Stories\nDataset Our experiments use the ROC Stories\ndataset, which consists of stories focusing on com-\nmon sense (Mostafazadeh et al., 2016). The train-\ning set has 98k stories, with ﬁve sentences each.\n7474\nValid 2016 Test 2016 Valid 2018 Test 2018\nOur model MLP 69.7 68.8 70.1 69.0\n+ CSLoss 73.5 73.0 73.1 72.1\nAlternatives Peng et al. (2017) – 62.3 – –\nSchenk and Chiarcos (2017) 62.9 63.2 – –\nLang. Models Schwartz et al. (2017) – 67.7 – –\nGPT-2 (Radford et al., 2019) 54.5 55.4 53.8 –\nGPT-2 + ﬁnetuning 59.0 59.9 59.0 –\nTable 1: Accuracies (%) for the Story Cloze binary classiﬁcation task. Schwartz et al. (2017) is a semi-supervised\ntechnique. GPT-2 refers to predicting the more likely ending according to the 355M parameter model, and GPT-2\nﬁnetuning was done on the ROC Stories train set.\nThe validation and test sets each contain 1.8k sto-\nries consisting of four sentences followed by two\nalternative endings: one ending is coherent with\nthe context; the other is not. The dataset was intro-\nduced for the Story Cloze task, inspired by Taylor\n(1953), where the goal is to select the coherent end-\ning. While the dataset and task were introduced as\na way to probe for coherence and commonsense\nin models trained only on the unlabeled portion,\nmost research derived from this dataset focuses\non a supervised setting, using the validation set as\na smaller, labeled training set (Chaturvedi et al.,\n2017; Sun et al., 2019; Cui et al., 2019; Li et al.,\n2019; Zhou et al., 2019). Our work is faithful to\nthe original task objective. We train solely on the\ntraining set, i.e. the model never sees incoherent\nendings at training time.\nModel We consider two models, an MLP and\na residual MLP. They take as input the previous\nsentences represented as the concatenation of their\nembeddings. Alternative context aggregation strate-\ngies were considered with recurrent (Sundermeyer\net al., 2012) and attention (Vaswani et al., 2017)\narchitectures, without strong empirical advantages.\nThe models maps its input to a vector which is com-\npared to a set of candidate sentence embeddings\nvia dot product. The embedding of the true next\nsentence should receive the highest score. For each\nexample, we consider all other ﬁfth sentences in\nthe training set (96k in total) as the candidate set.\nThe input of our model is 3,072 dimensional,\ni.e. 4 context sentences represented by 768 dimen-\nsional BERT embeddings. After an architecture\nsearch, our best MLP has 3 layers of 1,024 units,\nand our best resMLP has a single residual layer\nwith hidden size of 1,024. Both contain just over\n6M trainable parameters. Both apply dropout with\na rate of 0.5 after each ReLU, and layer normal-\nization is performed on the concatenated context\nsentence embedding passed in as input to the net-\nwork and on the ﬁnal predicted embedding for the\nnext sentence. For the Story Cloze task, the two\narchitectures achieve similar validation accuracy,\nbut when considering more than two distractors,\nthe resMLP signiﬁcantly outperforms the standard\nMLP. The resMLP also converges quicker than the\nMLP. Training to convergence takes under 2 hours\nfor each model on a Tesla V100.\n3.2 Toronto Book Corpus\nDataset ROC Stories contains only self-\ncontained ﬁve-sentence stories, focusing on every-\nday life scenarios. They contain no dialog and\nvery little ﬂowery, expository language. Ideally our\nmethod would also be successful at scoring poten-\ntial continuations to more naturally-written stories.\nTo this end, we test out our approach on excerpts\nfrom the Toronto Book Corpus (Kiros et al., 2015),\na dataset of self-published novels. The dataset con-\ntains over 7,000 unique books totalling over 45\nmillion sentences. Since these stories are much\nlonger than the ROC Stories ones and many of\nthe sentences are uninformative (nearly 5% of sen-\ntences are 3 words or shorter, and 14% are 5 words\nor shorter), we double the context length to 8 sen-\ntences.\nModel In addition to experimenting with a sim-\nilar residual MLP architecture to the one used on\nROC Stories, we also ran experiments with a Trans-\nformer model (Vaswani et al., 2017). The residual\nMLP architecture contains 2 residual layers with\nhidden size of 1024 (11M params total). The trans-\nformer has 4 self-attention layers with hidden size\nof 768, ﬁlter size of 2048 and 8 attention heads\n(22M params total). While the residual MLP is\ntrained to predict the 9th sentence given the pre-\nvious 8 sentences, the Transformer is trained to\npredict each next sentence given the previous sen-\ntences in a sequence of length 10 sentences. How-\never, we only evaluate the Transformer on the task\nof predicting the 9th sentence so that evaluation re-\nsults are directly comparable to the residual MLP.\n7475\nP@10 MRR\nMLP 6.2 0.052\n+CSLoss 3.4 0.029\nResMLP 10.3 0.087\n+CSLoss 6.2 0.051\nRandom 0.01 2e-5\nTable 2: Precision@10 and mean-reciprocal rank on\nthe 2018 valid set when considering all 5th sentences in\nthe train and valid sets (98k total) as candidate endings.\nFor each batch during training, 2k distractors are\nrandomly selected from the train set. Like with\nROC Stories, we experiment with an auxiliary loss\nwhere just sentences from the context were used as\ndistractors. Table 3 reports the results.\n4 Results\nWe evaluate on the Story Cloze task, a binary clas-\nsiﬁcation task, as well as on the task of ranking a\nlarge set of possible next sentences.\n4.1 Story Cloze Task\nTable 1 shows that our method outperforms un-\nsupervised alternatives. The introduction of the\nCSLoss which considers only context sentences as\ncandidates improves accuracy compared to only\nusing a loss over all possible ﬁfth sentences.\nFor comparison, we include the accuracies of the\nbest unsupervised methods in the literature. Schenk\nand Chiarcos (2017) construct negative examples\nfor their binary classiﬁcation task by pairing con-\ntexts with random ﬁfth sentences selected from the\ntraining set. Peng et al. (2017) train a language\nmodel to predict a representation of the semantic\nframe, entities, and sentiment of the ﬁfth sentence\ngiven the representations of the previous sentences,\nthen take the more likely ﬁfth sentence. We achieve\nhigher accuracy without relying on a task-speciﬁc\narchitecture.\nTable 1 also shows that picking the ending that\nis more likely according to a word-level language\nmodel, in our case GPT-2’s 355M parameter model,\ndoes not yield very high accuracies, even when\nthe language model is ﬁnetuned on ROC Stories\ntext (Radford et al., 2019). Lastly, we also include\nthe accuracy reported by Schwartz et al. (2017),\nwhere a logistic classiﬁer is trained to combine\nmultiple language model scores.\nIt is worth noting that state-of-the-art on the\nStory Cloze task is over 90% accuracy (Li et al.,\n2019; Cui et al., 2019) for semi-supervised settings.\nThe methods achieving this level of performance\n0%\n2%\n4%\n6%\n8%\n10 %\n12 %\n1 10 10 0 1,0 00 10 ,00 0 96 k\nP@10\nNumber of Distractors in Train Loss\nCLL oss=0.0\nCLL oss=1.0\nFigure 1: The impact of the number of negative sen-\ntences used during training on the rank of the true end-\ning out of 98k distractors. Results are with the resMLP\non the 2018 valid set.\nare not comparable to our unsupervised approach\nas they require training on the labeled validation\nset. The language model approach from Schwartz\net al. (2017) also falls into this category.\n4.2 Ranking Many Sentences on ROC Stories\nFor generation and suggestion scenarios, it is useful\nto be able to surface the best next sentence out of\nhundreds or thousands of candidates. In Table 3, we\nshow the performance of our method on the 2018\nvalidation set when all 98,161 ﬁfth sentences in\nthe training set plus all 1,571 correct 5th sentences\nin the 2018 validation are considered as candidate\nendings. Top-10 accuracy is highest, at 10.3%,\nwhen training a residual MLP without CSLoss.\nInterestingly, strong performance on the Story\nCloze task does not necessarily translate to strong\nperformance on the large-scale ranking task. The\nCSLoss improves performance on the Story Cloze\ntask but hurts it for large-scale ranking.\nIn Figure 1, we show how large-scale ranking\nperformance improves as the size of the train-time\ndistractor set is increased. However, on the Story\nCloze task, the number of training distractors has\nno signiﬁcant impact on performance. Even when\nonly a single distractor is randomly chosen at each\nstep of training, our method achieves over 70%\n2016 test accuracy. It seems that training for the\ngoal of detecting the true next sentence out of a\nvery diverse candidate set is useful at test time only\nwhen the set of distractors at test time is similarly\nlarge and diverse. The many-distractors training\nregime might be less useful for the Story Cloze task\nsince the two candidate endings are designed to be\nquite topically similar to each other.\nSome qualitative examples are shown in Table\n7476\n10k 100k same book\nresMLP 22.5% 7.4% 7.8%\n+CSLoss 11.5% 2.5% 5.3%\nTransformer 15.2% 4.0% 4.8%\n+CSLoss 4.8% 0.8% 2.0%\nTable 3: Precision@10 On Toronto Book Corpus for\nretrieving the correct next sentence (given the 8 previ-\nous sentences) when considering 10k or 100k distractor\nsentences, or all of the sentences from the same book\nas distractors.\n4. The failure examples showcase a side-effect\nof relying on pre-trained sentence embeddings: if\ncommon names like “Becky” or “Laura” or sports\nsuch as “ﬁshing” and “golf” are close to each other\nin embedding space, our model will fail to distin-\nguish between them.\n4.3 Ranking Many Sentences on Toronto\nBook Corpus\nWhen evaluating with 100k distractors, about as\nmany as our ROC Stories large-scale ranking task,\nP@10 is at best 7.1%, compared with 22.7% for\nROC Stories. We suspect that this task would ben-\neﬁt from longer contexts and better selection of\ndistractors. In particular, a qualitative evaluation of\nthe data highlighted the presence of a large quantify\nof short, generic sentences in the high ranking sen-\ntences (e.g. “he said.” and “Yes.”). We see reducing\nthe density of such sentences at training time as a\npotential for improvement.\nIn addition, further investigation is necessary\ninto why the Transformer did not work as well as\nthe residual MLP. The use of variable sequence\nlength architectures like the Transformer will be-\ncome more critical as the input sequence length is\nincreased beyond what an MLP can easily handle.\n5 Conclusions\nThis work introduces a sentence-level language\nmodel which takes a sequence of sentences as con-\ntext and predicts a distribution over a ﬁnite set of\ncandidate next sentences. It takes advantage of pre-\ntrained BERT embeddings to avoid having to learn\ntoken-level ﬂuency, allowing the model to focus\nsolely on the coherence of the sentence sequences.\nOur results on the Story Cloze task highlight the\nadvantage of this strategy over word-level language\nmodels. At train time, our model considers much\nlarger amounts of text per update than typical token-\nlevel language models. We show that this strategy\nContext: My family got up one morning while on vacation. We loaded our\nboat onto a trailer and drove to the beach. After loading up from the dock, we\ntook off on our boat. After only a few minutes on the sea, dolphins began to\nswim by us.\nGT: (22.89) We played with them for a while and then returned to the dock.\nRank: 9\nTop scored:\n(25.06) We were deﬁnitely lucky to see them and it made the trip more fun!\n(24.31) They loved everything about that trip and vowed to do it again!\n(23.76) We were sad to come home but excited to plan our next vacation.\n(23.72) It was one of our best vacations ever!\nContext: Ellen wanted to be smart. She started reading the dictionary. She\nlearned two hundred new words the ﬁrst day. Ellen felt smart and educated.\nGT: (30.23) She couldn’t wait to use the new words.\nRank: 1\nTop scored:\n(30.23) She couldn’t wait to use the new words.\n(29.78) She felt like a new woman when she was done!\n(29.01) She decided to go back to speaking like her normal self!\n(28.95) She felt like a new girl!\nContext: It was a very cold night. Becky was shivering from the cold air. She\nneeded to cover up before she caught a cold. She wrapped up in her favorite\nblanket.\nGT: (18.717398) Becky ﬁnally got warm.\nRank: 3,028\nTop scores:\n(39.09) Laura ended up shivering, wrapped in a blanket for hours.\n(36.71) After being cold all day, the warmth felt so good.\n(33.77) Sam was able to bundle up and stay cozy all winter.\n(33.38) The breeze felt good on her wet shirt.\nContext: Benjamin enjoyed going ﬁshing with his grandfather as a kid. They\nwould pick a new location to go to every summer. Benjamin liked seeing who\nwould catch the biggest ﬁsh. Even after his grandfather passed he continued\nthe tradition.\nGT: (26.65) He now takes his own grandchildren to create memories for them-\nselves.\nRank: 2,281\nTop ranked:\n(34.71) Greg grew to love golﬁng and is now his favorite thing to do.\n(33.82) It was a tradition Tim continues with his own family.\n(33.63) Alex learned to be grateful of his family’s unique tradition.\n(33.40) Tom was sad that he would have to let his son down.\nTable 4: Top-scoring sentences (using resMLP with-\nout CSLoss) among 98k possible endings when using\nprompts from the validation set. Two success and two\nfailures cases are shown.\nallows our model to surface appropriate endings to\nshort stories out of a large set of candidates.\nAs future work, we plan to further evaluate\nthe impact of different sequential architectures,\nlonger contexts, alternative sentence embeddings,\nand cleverer selection of distractors. Inspired by\ndeliberation networks and automatic post editing\nmethods (Xia et al., 2017; Freitag et al., 2019), we\nultimately want to apply our model to two-step gen-\neration, ﬁrst selecting a sentence from a large set\nbefore reﬁning it to ﬁt the context.\nAcknowledgements\nThis research is based upon work supported in part\nby U.S. DARPA KAIROS Program No. FA8750-\n19-2-1004. The views and conclusions contained\nherein are those of the authors and should not be\ninterpreted as necessarily representing the ofﬁcial\npolicies, either expressed or implied, of DARPA\nor the U.S. Government. The U.S. Government is\nauthorized to reproduce and distribute reprints for\ngovernmental purposes notwithstanding any copy-\nright annotation therein.\n7477\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In International Con-\nference on Learning Representation (ICLR).\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch (JMLR), 3(Feb):1137–1155.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\net al. 2018. Universal sentence encoder. arXiv\npreprint arXiv:1803.11175.\nKhyathi Chandu, Eric Nyberg, and Alan W Black.\n2019. Storyboarding of recipes: Grounded contex-\ntual generation. In Proceedings of the Annual Meet-\ning of the Association for Computational Linguistics\n(ACL).\nSnigdha Chaturvedi, Haoruo Peng, and Dan Roth.\n2017. Story comprehension for predicting what hap-\npens next. pages 1603–1614.\nElizabeth Clark, Anne Spencer Ross, Chenhao Tan,\nYangfeng Ji, and Noah A Smith. 2018. Creative\nwriting with a machine in the loop: Case studies on\nslogans and stories. In 23rd International Confer-\nence on Intelligent User Interfaces , pages 329–340.\nACM.\nYiming Cui, Wanxiang Che, Wei-Nan Zhang, Ting Liu,\nShijin Wang, and Guoping Hu. 2019. Discrimina-\ntive sentence modeling for story ending prediction.\nCoRR, abs/1912.09008.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 889–898, Melbourne, Australia. Association\nfor Computational Linguistics.\nAngela Fan, Mike Lewis, and Yann N. Dauphin. 2019.\nStrategies for structuring story generation. In Asso-\nciation for Computational Linguistics, ACL , pages\n2650–2660. Association for Computational Linguis-\ntics.\nMarkus Freitag, Isaac Caswell, and Scott Roy. 2019.\nAPE at scale and its implications on MT evaluation\nbiases. In Proceedings of the Fourth Conference on\nMachine Translation (Volume 1: Research Papers) ,\npages 34–44, Florence, Italy. Association for Com-\nputational Linguistics.\nJian Guan, Yansen Wang, and Minlie Huang. 2019.\nStory ending generation with incremental encoding\nand commonsense knowledge. In The Thirty-Third\nAAAI Conference on Artiﬁcial Intelligence, AAAI ,\npages 6473–6480.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nRyan Kiros, Yukun Zhu, Russ R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems ,\npages 3294–3302.\nMirella Lapata. 2003. Probabilistic text structuring:\nExperiments with sentence ordering. In Proceed-\nings of the 41st Annual Meeting on Association for\nComputational Linguistics - Volume 1 , ACL ’03,\npages 545–552, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\nZhongyang Li, Xiao Ding, and Ting Liu. 2019. Story\nending prediction by transferable BERT. Proceed-\nings of the Twenty-Eighth International Joint Con-\nference on Artiﬁcial Intelligence, pages 1800–1806.\nDanyang Liu, Juntao Li, Meng-Hsuan Yu, Ziming\nHuang, Gongshen Liu, Dongyan Zhao, and Rui Yan.\n2020. A character-centric neural model for auto-\nmated story generation. In AAAI Conference on Ar-\ntiﬁcial Intelligence.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E Peters, and Noah A Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094.\nLajanugen Logeswaran and Honglak Lee. 2018. An ef-\nﬁcient framework for learning sentence representa-\ntions. In International Conference on Learning Rep-\nresentations.\nLara J Martin, Prithviraj Ammanabrolu, Xinyu Wang,\nWilliam Hancock, Shruti Singh, Brent Harrison, and\nMark O Riedl. 2018. Event representations for au-\ntomated story generation with deep neural nets. In\nThirty-Second AAAI Conference on Artiﬁcial Intelli-\ngence.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-\nrado, and Jeffrey Dean. 2013. Distributed represen-\ntations of words and phrases and their composition-\nality. In Proceedings of the 26th International Con-\nference on Neural Information Processing Systems.\n7478\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839–849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nHaoruo Peng, Snigdha Chaturvedi, and Dan Roth.\n2017. A joint model for semantic sequences:\nFrames, entities, sentiments. pages 173–183.\nAlec Radford, Jeffrey Wu Wu, Rewon Child,\nDavid Luan Luan, Dario Amodei, and Ilya Sutskever.\n2019. Language models are unsupervised multitask\nlearners.\nNiko Schenk and Christian Chiarcos. 2017. Resource-\nlean modeling of coherence in commonsense sto-\nries. In Proceedings of the 2nd Workshop on Linking\nModels of Lexical, Sentential and Discourse-level\nSemantics, pages 68–73.\nRoy Schwartz, Maarten Sap, Ioannis Konstas, Leila\nZilles, Yejin Choi, and Noah A Smith. 2017. Story\ncloze task: Uw nlp system. In Proceedings of the\n2nd Workshop on Linking Models of Lexical, Senten-\ntial and Discourse-level Semantics, pages 52–55.\nAbigail See, Aneesh Pappu, Rohun Saxena, Akhila\nYerukola, and Christopher D. Manning. 2019. Do\nmassively pretrained language models make better\nstorytellers? In Conference on Natural Language\nLearning CONLL.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie. 2019.\nImproving machine reading comprehension with\ngeneral reading strategies. In Proceedings of the\n2019 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, NAACL-HLT 2019,\nMinneapolis, MN, USA, June 2-7, 2019, Volume 1\n(Long and Short Papers), pages 2633–2643.\nMartin Sundermeyer, Ralf Schl¨uter, and Hermann Ney.\n2012. Lstm neural networks for language modeling.\nIn Thirteenth annual conference of the international\nspeech communication association.\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nYingce Xia, Fei Tian, Lijun Wu, Jianxin Lin, Tao Qin,\nNenghai Yu, and Tie-Yan Liu. 2017. Deliberation\nnetworks: Sequence generation beyond one-pass de-\ncoding. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 1784–1794. Curran Asso-\nciates, Inc.\nLili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin\nKnight, Dongyan Zhao, and Rui Yan. 2019. Plan-\nand-write: Towards better automatic storytelling. In\nAAAI Conference on Artiﬁcial Intelligence.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert. arXiv preprint\narXiv:1904.09675.\nMantong Zhou, Minlie Huang, and Xiaoyan Zhu. 2019.\nStory ending selection by ﬁnding hints from pair-\nwise candidate endings. IEEE/ACM Transactions on\nAudio, Speech and Language Processing (TASLP) ,\n27(4):719–729."
}