{
  "title": "Tabular-textual question answering: From parallel program generation to large language models",
  "url": "https://openalex.org/W4411049888",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5066118206",
      "name": "Xushuo Tang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5027813354",
      "name": "Liuyi Chen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002922237",
      "name": "Wenke Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5019383391",
      "name": "Zhengyi Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070533702",
      "name": "Mingchen Ju",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063528590",
      "name": "Xin Shu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5034865295",
      "name": "Zihan Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5107307417",
      "name": "Yifu Tang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2087829218",
    "https://openalex.org/W2998789694",
    "https://openalex.org/W4385270344",
    "https://openalex.org/W4396757554",
    "https://openalex.org/W2090852828",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W4221163895",
    "https://openalex.org/W3101082165",
    "https://openalex.org/W3174986053",
    "https://openalex.org/W4205508242",
    "https://openalex.org/W4285199586",
    "https://openalex.org/W4385573185",
    "https://openalex.org/W4281656839",
    "https://openalex.org/W4388288440",
    "https://openalex.org/W4285216063",
    "https://openalex.org/W3034797320",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4405334847",
    "https://openalex.org/W4401353319",
    "https://openalex.org/W4401907607",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2963899988",
    "https://openalex.org/W2890431379",
    "https://openalex.org/W4385270161",
    "https://openalex.org/W4313509086",
    "https://openalex.org/W3201339301",
    "https://openalex.org/W2970900584",
    "https://openalex.org/W3199832501",
    "https://openalex.org/W4385573361",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W3105643199",
    "https://openalex.org/W4385573007",
    "https://openalex.org/W4287065027",
    "https://openalex.org/W4385572906",
    "https://openalex.org/W2911293880",
    "https://openalex.org/W2747329762",
    "https://openalex.org/W3173572290",
    "https://openalex.org/W3202773593",
    "https://openalex.org/W2885195348",
    "https://openalex.org/W2944851425",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W6600547436",
    "https://openalex.org/W4388994228",
    "https://openalex.org/W6726027185",
    "https://openalex.org/W4402683967",
    "https://openalex.org/W6851896007",
    "https://openalex.org/W4385227045",
    "https://openalex.org/W4385570036",
    "https://openalex.org/W6615264008",
    "https://openalex.org/W4406272066",
    "https://openalex.org/W6845038366",
    "https://openalex.org/W3105248300",
    "https://openalex.org/W4386275705",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Abstract Hybrid tabular-textual question answering (HTQA) involves integrating multiple data sources, traditionally managed through LSTM-based step-by-step reasoning. However, such sequential approaches are prone to exposure bias and cumulative errors, limiting their effectiveness. This paper first introduces an innovative parallel program generation method, ConcurGen, aiming to transform this paradigm by simultaneously formulating comprehensive program constructs that seamlessly blend operations and values. This approach not only rectifies the inherent pitfalls of sequential methodologies but also infuses efficiency into the process. Through our further research, we found that some HTQA scenarios extend beyond traditional question-answering, often involving open-ended questions that demand dynamic, context-aware response generation. Therefore, we introduce a second framework that leverages large language models (LLMs) to effectively answer both traditional and open-ended questions. Our method demonstrates substantial improvements over existing models such as FinQANet and MT2Net on benchmarks including ConvFinQA and MultiHiertt, achieving new state-of-the-art performance across multiple evaluation metrics. In addition to its accuracy, it delivers a nearly 21x speedup in program generation, significantly enhancing inference efficiency. Unlike traditional models, our system maintains robust performance as the complexity of numerical reasoning increases, highlighting its adaptability in challenging scenarios. Furthermore, supplementary experiments on the LLM-based framework show that it provides enriched answer justifications while achieving similar performance to ConcurGen on standard benchmarks.",
  "full_text": "World Wide Web (2025) 28:42\nhttps://doi.org/10.1007/s11280-025-01351-1\nTabular-textual question answering: From parallel program\ngeneration to large language models\nXushuo Tang 1,5 · Liuyi Chen 2 · Wenke Yang 1 · Zhengyi Yang 1 · Mingchen Ju 1 ·\nXin Shu 1,6 · Zihan Yang 3 · Yifu Tang 4\nReceived: 20 February 2025 / Revised: 26 April 2025 / Accepted: 12 May 2025 /\nPublished online: 6 June 2025\n© The Author(s) 2025\nAbstract\nHybrid tabular-textual question answering (HTQA) involves integrating multiple data\nsources, traditionally managed through LSTM-based step-by-step reasoning. However, such\nsequential approaches are prone to exposure bias and cumulative errors, limiting their effec-\ntiveness. This paper ﬁrst introduces an innovative parallel program generation method,\nConcurGen, aiming to transform this paradigm by simultaneously formulating comprehen-\nsive program constructs that seamlessly blend operations and values. This approach not only\nrectiﬁes the inherent pitfalls of sequential methodologies but also infuses efﬁciency into the\nprocess. Through our further research, we found that some HTQA scenarios extend beyond\ntraditional question-answering, often involving open-ended questions that demand dynamic,\ncontext-aware response generation. Therefore, we introduce a second framework that lever-\nages large language models (LLMs) to effectively answer both traditional and open-ended\nquestions. Our method demonstrates substantial improvements over existing models such\nas FinQANet and MT2Net on benchmarks including ConvFinQA and MultiHiertt, achiev-\ning new state-of-the-art performance across multiple evaluation metrics. In addition to its\naccuracy, it delivers a nearly 21x speedup in program generation, signiﬁcantly enhancing\ninference efﬁciency. Unlike traditional models, our system maintains robust performance as\nthe complexity of numerical reasoning increases, highlighting its adaptability in challenging\nscenarios. Furthermore, supplementary experiments on the LLM-based framework show that\nit provides enriched answer justiﬁcations while achieving similar performance to ConcurGen\non standard benchmarks.\nKeywords Hybrid tabular-textual question answering · Large language model · FinQANet ·\nMT2Net\n1 Introduction\nIn the era of data explosion, managing big data has become increasingly important, as various\nindustries generate and store vast amounts of information. These datasets contain valuable\nknowledge and insights, but extracting meaningful information from them poses a signiﬁcant\nThis article is part of the Topical Collection: Special Issue on APWeb-WAIM 2024\nGuest Editor: Wenjie Zhang and Hong Gao\nExtended author information available on the last page of the article\n123\n42 Page 2 of 30 World Wide Web (2025) 28 :42\nchallenge [ 1–4]. QA (Question Answering) systems are vital in addressing the challenges of\nbig data. QA systems can effectively extract key information from large datasets, signiﬁcantly\nenhancing the efﬁciency and accuracy of information retrieval. By rapidly processing and\nanalyzing vast amounts of data, QA systems can quickly locate and extract the speciﬁc\ninformation users need. This high-efﬁciency information extraction not only saves time and\nresources but also signiﬁcantly improves the timeliness and accuracy of decision-making [ 5].\nHybrid Tabular-Textual QA Many question answering (QA) investigations traditionally\ntarget singular data types, such as unstructured narratives [ 6–8] or structured datasets [ 9,\n10]. In contrast, hybrid tabular-textual QA (HTQA) [ 11–16] processes a mix of data and\npresents greater complexity, demanding numerical computation alongside textual extraction\nfor responses. This hybrid approach offers more versatility and real-world applicability since\ninformation often exists in both structured tabular formats and unstructured text. Effectively\nintegrating and reasoning over these disparate data sources is a challenge.\nExisting Works and Limitations To infuse these hybrid models with numerical computa-\ntion skills, TAGOP [12] employs sequence tagging for evidence selection and then conducts\na singular mathematical function using a set of established operators. While this enables\nbasic arithmetic, it is limited to single-step operations. For intricate multi-step reasoning,\nFinQANet [13]a n d MT2Net [16] implement an autoregressive Long Short-Term Memory\n(LSTM) decoder built on the RoBERTa [17, 18] structure to iteratively craft the program\nsequence. Yet, this incremental autoregressive decoding approach is prone to profound expo-\nsure bias. During the learning phase, the model uses the ideal references for decoding\n(via teacher forcing), becoming over-reliant on them. However, during application, initial\ninaccuracies can ripple through subsequent predictions, resulting in compounded mistakes,\nespecially given the modest prediction prowess of existing hybrid QA methods [ 19]. The\nautoregressive nature means errors accumulate with each decoding step, leading to subopti-\nmal or nonsensical programs.\nA Non-Autoregressive Approach Employing a non-autoregressive approach can mitigate\nthis reliance on earlier predictions, counteracting exposure bias, while enhancing speed\ndue to improved parallel processing. This is because non-autoregressive models generate\nall output steps simultaneously, removing the dependency between them. Without having\nto condition each step on the previous one, the model avoids the accumulation of errors\nthat often arise in autoregressive decoding, where an early mistake can propagate through\nthe sequence. In this paper, we ﬁrst introduce the Non-Autoregressive Program Generation\nmodel, termed ConcurGen. Diverging from the traditional sequential generation, it harnesses\nonly the encoder’s output, integrating an independent numerical reasoning unit for every rea-\nsoning phase to forecast the operator and related operands. This reasoning unit integrates a\nsoft masking technique [ 20] to emphasize speciﬁc operand representations, succeeded by an\noperator creator, operand creator, and sequence determiner. We also deploy a length fore-\ncaster to manage the quantity of numerical reasoning units generated. Given the absence of\ndependency on preceding decoder steps, ConcurGen addresses the exposure bias dilemma\nand signiﬁcantly ampliﬁes generation velocity due to concurrent operations. By generating\nthe full program in one shot, ConcurGen avoids cascading errors and can utilize efﬁcient\nparallelization.\nOpen-Ended Question Answering Traditional learning-based approaches, including non-\nautoregressive methods such as ConcurGen, remain limited in their ability to handle\nopen-ended questions. Tabular-textual QA systems must accommodate a broader range of\nqueries, as real-world decision-making often involves complex, dynamic scenarios where\n123\nWorld Wide Web (2025) 28 :42 Page 3 of 30 42\npredeﬁned answers are insufﬁcient. In domains such as ﬁnance, healthcare, and law, profes-\nsionals frequently encounter questions that extend beyond simple factual retrieval, requiring\ndeep insights derived from both structured data (e.g., ﬁnancial statements, patient records)\nand unstructured data (e.g., reports, news articles, legal texts). These questions typically\nlack a single correct answer and instead demand the synthesis of information from multiple\nsources, cross-domain reasoning, and an understanding of nuanced contexts. Among these\nchallenges, open-ended questions are particularly complex, as they often involve ambiguity,\nuncertainty, and require dynamic, multi-step reasoning to generate contextually appropriate\nresponses. For instance, in ﬁnance, decision-makers may need to assess the impact of emerg-\ning market trends on diverse investment portfolios, necessitating the integration of numerical\ndata with qualitative reports. In healthcare, physicians may need to formulate treatment\nplans based on a combination of structured patient data and unstructured medical literature\nor guidelines. In legal contexts, professionals may be required to analyze cases by synthesiz-\ning structured legal statutes with unstructured precedents or expert opinions. The ability to\naddress open-ended questions enhances the ﬂexibility of tabular-textual QA systems, enabling\nthem to bridge different data formats and support more sophisticated contextual reasoning.\nConsequently, improving open-ended QA capabilities is crucial for real-world applications,\nwhere decision-making relies on comprehensive analysis across multiple domains. Despite\nthese needs, existing QA systems struggle with complex open-ended questions. Traditional\nQA models primarily focus on answering closed-ended queries, which typically have well-\ndeﬁned structures and ﬁxed knowledge sources. For such tasks, current models can efﬁciently\nretrieve and generate precise answers. However, open-ended questions require models to\ngo beyond surface-level recognition, deeply understand contextual backgrounds, and infer\npotential answers that are not explicitly stated in the data. The rise of Transformer-based\nlarge language models (LLMs) [ 21–23] has advanced NLP by enabling strong reasoning\nand contextual understanding. Their ability to synthesize information makes them especially\npromising for improving hybrid tabular-textual question answering (HTQA), particularly in\nhandling open-ended queries.\nExtensions over the Conference Version This paper extends our previous work, Paral-\nlel Program Generation for Hybrid Tabular-Textual Question Answering [24]. Compared\nto the conference version, we ﬁrst expand on the optimization details of ConcurGen,p r o -\nviding a in-depth discussion of its enhancements and reﬁnements (Section 4.4). Following\nthis, we introduce a new exploration in tabular-textual question answering by incorporating\nopen-ended questions, examining their role in effectively solving these tasks (Section 5).\nSpeciﬁcally, we investigate the capabilities of LLMs in addressing both traditional queries\nbased on structured tabular-textual data and more complex open-ended questions that require\ndeeper contextual reasoning. In addition to these methodological advancements, we provide\nan expanded literature review to better contextualize our contributions within the broader\nresearch landscape (Section 2), and present additional experimental evaluations to demon-\nstrate the effectiveness of large models in handling tabular-textual question answering tasks\n(Section 6). These new contributions collectively strengthen the theoretical and empirical\nfoundation of our work, advancing the understanding of how large language models can\nbridge structured and unstructured data for more effective decision-making.\nContributions Hybrid QA with numerical reasoning is a critical frontier in NLP , enabling\nAI systems to integrate and reason over tabular and textual data. In this paper, we introduce\nConcurGen, a novel deep-learning method that uses a non-autoregressive architecture to\ngenerate reasoning programs with improved accuracy and efﬁciency. Despite these advance-\nments, deep learning-based tabular-textual QA systems often encounter challenges when\n123\n42 Page 4 of 30 World Wide Web (2025) 28 :42\naddressing open-ended questions. To tackle this limitation, we further explore how LLMs can\nbe leveraged to enhance QA capabilities, equipping these systems to handle both traditional\nqueries and open-ended reasoning tasks in complex real-world scenarios. To summarize, our\nkey contributions are as follows:\n• Novel Design of ConcurGen for HTQA. The introduction of ConcurGen, a non-\nautoregressive program generator capable of concurrently crafting entire reasoning\nprograms. This mechanism circumvents the pitfalls of exposure bias inherent in prior\nmodels and exhibits pronounced speed, courtesy of its parallel processing nature.\nConcurGen represents a novel architecture for hybrid QA program generation that miti-\ngates key issues in autoregressive approaches.\n• LLM-Empowered HTQA. The importance of Open-Domain Questions in HTQA scenar-\nios has been recognized, highlighting the need for a model that can address both traditional\nand open-ended queries. A framework based on large language models (LLMs) is pro-\nposed to handle both types of questions, enhancing versatility and performance in HTQA\ntasks.\n• Empirical Evaluation and Analysis. Empirical testing on the ConvFinQA [ 15] and Multi-\nHiertt [16] datasets reveals that ConcurGen substantially outperforms its contemporaries,\nFinQANet and MT2Net, setting new performance benchmarks and delivering a program\ngeneration speed that’s approximately 21 times faster. The proposed ConcurGen achieves\nstate-of-the-art results on these challenging hybrid QA benchmarks with dramatically\nimproved efﬁciency. Further scrutiny indicates that ConcurGen’s performance degrada-\ntion is notably less than its counterparts when handling increased numerical reasoning\ntasks, demonstrating greater robustness to reasoning complexity. Additional evaluation\non the proposed framework show that it achieves strong performance in both traditional\nand open-ended question answering.\nPaper Organization The paper is organized as follows: Section 2 reviews related literature.\nSection 3 covers foundational concepts and background. Section 4 details ConcurGen’s\narchitecture, methodology, and implementation. Section 5 introduces a framework based on\nLLMs. Section 6 presents empirical evaluation results. Section 7 concludes key ﬁndings and\nimplications.\n2 Relatedwork\nQA Dataset The origins of QA datasets trace back to the early development of QA tasks,\nwhich aimed to evaluate and improve the ability of systems to answer questions based on\ngiven contexts. Early datasets laid the foundation for modern QA research by providing\nstructured challenges and benchmarks. Text-based QA datasets have been pivotal in advanc-\ning natural language understanding and reasoning. Prominent early examples include the\nCNN/Daily Mail dataset [ 25] and the Stanford Question Answering Dataset (SQuAD) [ 26].\nThese datasets focused on extracting answers from unstructured textual narratives, providing\na benchmark for evaluating the ability of models to comprehend and retrieve information from\ntext. The introduction of these datasets spurred signiﬁcant advancements in QA systems, driv-\ning the development of more sophisticated models capable of handling complex queries and\nunderstanding nuanced textual information. In parallel, QA over structured data sources such\nas knowledge bases (KB) and tables also gained traction. Datasets like WebQuestions [ 27],\nWikiTableQuestions [28], and Spider [ 29] were developed to evaluate the capability of mod-\nels to automatically answer questions using well-structured KBs and semi-structured tables.\n123\nWorld Wide Web (2025) 28 :42 Page 5 of 30 42\nThese datasets posed unique challenges, requiring systems to navigate and extract relevant\ninformation from structured formats, which often involved complex query understanding and\nlogical reasoning [ 30, 31]. Recently, deep reasoning over textual data has gained increasing\nattention. Multi-hop reasoning, where a model must connect information from multiple parts\nof a text, has been a focus of recent research. The DROP dataset [ 32] further emphasized the\nneed for numerical reasoning capabilities, challenging models to perform arithmetic opera-\ntions and handle complex queries involving numbers. However, despite these advancements,\npurely textual datasets often fall short in scenarios requiring the integration of structured and\nunstructured information. To overcome the challenges associated with processing mixed data\nsources, the concept of hybrid tabular-textual QA was developed. This innovative approach\nwas ﬁrst implemented with the HybridQA dataset [ 11], which uniquely links table cells to\nWiki pages through manually created hyperlinks.There has been a surge in datasets like\nTA T-QA and FinQA, centered on ﬁnancial reports, accentuating the need for numerical\nreasoning [ 12, 13]. Further advancements led to datasets like TA T-HQA [ 14] and Con-\nvFinQA [ 15], expanding upon the capabilities of their predecessors. Another noteworthy\naddition is the MultiHiertt dataset, distinctive due to its intricate hierarchical tables coupled\nwith extensive textual content [ 16].\nNumerical Reasoning Numerical reasoning, a cornerstone for various NLP applications [ 9,\n33], is especially pivotal in QA domains including text QA [ 8, 34, 35], table QA [ 9, 10], and\nthe hybrid form of tabular-textual QA [ 12–16, 36, 37]. Efforts have been made to bolster\nthe numerical reasoning prowess of pre-existing language models [ 38–40]. Techniques such\nas TAGOP [ 12] are equipped to execute a solitary arithmetic operation utilizing designated\noperators, while advanced systems like FinQANet [13]a n d MT2Net [16] delve into intricate\nmulti-step reasoning, predominantly employing the LSTM decoder for program autogener-\nation.\nLarge Language Models Pre-trained Language Models (PLMs) have shown remarkable\nsuccess in various Natural Language Processing (NLP) tasks due to their ability to leverage\nextensive textual corpora, enabling them to learn world knowledge and general language\npatterns [ 41]. However, applying these models to math-related tasks has proven challeng-\ning, as mathematical reasoning involves speciﬁc knowledge and logical structures that are\nnot inherently represented in typical language data [ 42]. Early attempts to extend PLMs to\nmathematical tasks include MWP-BERT, which ﬁne-tunes BERT for solving mathematical\nword problems, and Minerva, which combines symbolic reasoning and PLMs to perform\narithmetic and algebraic problem-solving. These models demonstrate the potential of PLMs\nin this domain but still rely on carefully curated math-speciﬁc datasets, which remain difﬁ-\ncult to compile at scale [ 43]. To address the gap in performance, researchers have ﬁne-tuned\nPLMs for speciﬁc math-related tasks, with several notable models emerging. Bhaskara [ 44]\nis tailored for algebraic problem-solving, while Aristo [ 45] focuses on automated question\nanswering in mathematics. These works use PLMs like GPT-Neo and RoBERTa, extending\nthe models’ capabilities by ﬁne-tuning them on specialized math datasets [ 46]. However, the\nchallenge of creating comprehensive and high-quality math datasets remains a signiﬁcant\nhurdle for further advancement in this area.\n3 Preliminaries\nAnswering questions from hybrid tabular-textual data is a complex task that requires the\nability to extract and integrate information from diverse sources. The goal is to provide\n123\n42 Page 6 of 30 World Wide Web (2025) 28 :42\naccurate responses to a given query, denoted as ( Q), by leveraging the available tables ( T )\nand text segments ( E). In some cases, the model can directly locate the answer segment,\n(A), within the provided data. However, more often, the model needs to perform numerical\ncomputations or textual extractions to arrive at the correct answer. This process involves\ngenerating a reasoning program, G, composed of individual tokens, g\ni , which can be derived\nfrom the input data or selected from a predeﬁned set of tokens, including operators and\nspeciﬁc operands. The probability of a particular answer, A, is determined by summing the\nprobabilities of all program sequences, G\ni , that can lead to the derivation of A.\nThe task of answering questions from hybrid data presents several challenges. Firstly,\nthe model must effectively handle the heterogeneity of the input data, which can include\nstructured tables and unstructured text segments. Secondly, the model needs to possess the\nability to perform both textual extractions and numerical computations, depending on the\nnature of the question. Thirdly, the input data can be extensive, often exceeding the input\nsize constraints of pre-trained language models (PLMs), necessitating the development of\nefﬁcient methods for fact extraction and relevant information selection.\nFact Extraction and Supporting Data Identiﬁcation To address the challenges associated\nwith lengthy input data, such as that found in the MultiHiertt dataset, MT2Net employs a\ntwo-stage approach for fact extraction. In the ﬁrst stage, MT2Net transforms table data cells\ninto sentence-like structures, incorporating their corresponding row and column descriptors.\nDue to the input size limitations of PLMs, MT2Net concatenates the question with individual\nsentences and uses this concatenated data to train a RoBERTa-based binary classiﬁer (bi-\nclassiﬁer) speciﬁcally designed to identify supporting data. The bi-classiﬁer assigns relevance\nscores to each sentence, allowing MT2Net to select the top- n sentences based on their scores\nfor further processing. In the second stage, another classiﬁer is employed to determine whether\nthe subsequent step requires textual extraction or numerical computation.\nTextual Extraction for Span-based Answers For questions that necessitate extracting a spe-\nciﬁc span from the input data, MT2Net utilizes the T5-base model [ 47]. The T5-base model\ntakes as input the concatenation of the original question and the sentences containing rele-\nvant facts and generates the answer sequence as output. This approach enables the model to\neffectively locate and extract the desired information from the provided text segments.\nIterative Numerical Computations for Complex Reasoning Some questions demand multi-\nstep numerical computations to arrive at the correct answer. To handle such questions, MT2Net\nﬁrst employs RoBERTa to generate context-aware representations of the question and the\nrelevant fact-containing sentences. These representations are then combined with embeddings\nof predeﬁned special tokens, including function labels and speciﬁc values. Subsequently, an\nLSTM decoder is used to generate the program sequence required to compute the answer. At\neach step of the decoding process, the model predicts from the fused representation matrix,\nselecting either an operator or an operand.\n4C o n c u r G e n\nIn this section, we present ConcurGen, our pioneering non-autoregressive program genera-\ntion model for hybrid question answering. ConcurGen distinguishes itself from conventional\nmodels by generating the entire program sequence simultaneously, rather than incrementally.\nThis innovative approach not only addresses the problem of exposure bias that plagues step-\nby-step program generation but also signiﬁcantly accelerates the process through efﬁcient\nparallelization. Figure 1 provides an overview of the ConcurGen model architecture. The\n123\nWorld Wide Web (2025) 28 :42 Page 7 of 30 42\nmodel starts by employing a bi-classiﬁer to identify relevant facts from the input data. It\nthen utilizes another biclassiﬁer, similar to MT2Net, to determine the type of question being\nasked. For questions requiring span extraction, ConcurGen uses the T5-base model, just\nlike MT2Net. However, when it comes to numerical computation tasks, ConcurGen adopts a\nunique non-autoregressive strategy for program generation, departing from MT2Net’s autore-\ngressive LSTM decoder.\n4.1 Overview\nThe proposed model, illustrated in Figure 1, addresses hybrid tabular-textual QA. It consists\nof four main components that work in a pipeline: an embedding layer encodes the input table\nand text; a soft-masking mechanism enhances semantic alignment between them; the reason-\ning network then uses the aligned representations to generate operators and their execution\norder in parallel; ﬁnally, a span extraction module handles cases where direct textual answers\nare required, complementing the numerical reasoning path. Despite the advancements made\nby models like MT2Net, there remain signiﬁcant limitations in their ability to effectively\nhandle the complexity and diversity of real-world hybrid data. The reliance on autoregres-\nsive decoding in the numerical computation component can lead to exposure bias and error\naccumulation, as the model becomes overly dependent on the reference sequences used dur-\ning training. Furthermore, the sequential nature of the decoding process limits the model’s\nefﬁciency and scalability, as it cannot fully exploit the potential for parallel processing.\nTo overcome these limitations, we propose ConcurGen, a non-autoregressive program\ngeneration model that aims to generate accurate and efﬁcient reasoning programs for hybrid\nQA. By eliminating the dependency on previous decoder steps and employing independent\nnumerical reasoning units for each reasoning step, ConcurGen mitigates exposure bias and\nenables parallel processing, resulting in signiﬁcant improvements in both accuracy and speed.\nThe introduction of ConcurGen represents a major step forward in the ﬁeld of hybrid QA,\nproviding a novel architecture that can effectively handle the challenges associated with\nanswering questions from diverse data sources. As illustrated in Algorithm 1, the model\nworkﬂow begins with encoding the input question and associated tabular data through an\nembedding layer. These embeddings are then aligned using a soft-masking mechanism to\nenhance semantic relevance between the question and table content. The aligned represen-\ntations are fed into a reasoning network, where an operator generator predicts the required\noperations (e.g., selection, ﬁltering, or aggregation), and an order predictor determines their\nexecution sequence. Based on these predictions, the model constructs an executable rea-\nFig. 1 Schematic representation of our proposed model\n123\n42 Page 8 of 30 World Wide Web (2025) 28 :42\nsoning program for numerical computation. In parallel, a span extraction module handles\nnon-numerical queries by identifying direct textual answers from the retrieved context. This\narchitecture enables ConcurGen to efﬁciently and accurately address complex hybrid queries\nthrough parallelized, program-driven reasoning. More speciﬁcally, the model comprises\nfour primary components: (1) the Embedding Layer, where input table data and question\ndescriptions are transformed into embedding vectors, forming the foundation for the model’s\nprocessing, and (2) the Reasoning Network, which encompasses a Soft-Mask that selectively\nfocuses on relevant aspects of the embeddings, an Operator Generator that devises necessary\noperations such as data extraction, computation, or comparison based on the task require-\nments, and an Order Predictor that sequences these operations to efﬁciently address complex\nproblems. Additionally, (3) the network includes Numeric Reasoning and (4) Span Extraction\nmodules that perform calculations and extract speciﬁc information from texts, respectively.\nBefore reasoning begins, the Data Retrieval module fetches necessary data from storage,\nensuring that the reasoning is conducted with the most relevant and useful information.\nNotably, the core of the reasoning network is a non-autoregressive program generation\nmodule, which constructs reasoning programs by predicting the operator and execution order\nfor each reasoning step in parallel . Unlike autoregressive models that generate reasoning\nsteps sequentially and are prone to error accumulation, this module eliminates step-by-step\ndependencies and fully leverages parallelism across all reasoning components. As illustrated\nin Figure 1, it receives aligned embeddings from the soft-masking mechanism and outputs a\nstructured sequence of operations that can be directly executed to compute the ﬁnal answer.\nAlgorithm 1 General Framework.\n1: Input: Question description Q, Table data set T\n2: Output: Answer A\n3: // Data Preprocessing Phase\n4: TE ← Table Embedding(Table data T )\n5: QE ← Question Embedding(Question description Q)\n6: // Post-Embedding Information Processing\n7: SE ← Soft-Masking(TE , QE )\n8: // Reasoning Network Processing\n9: for each reasoning module RMi do\n10: OE i ← Operator Generator( SE )\n11: OPi ← Order Predictor( SE )\n12: RE i ← Perform Operation( OE i , OPi , SE )\n13: end for\n14: // Answer Synthesis\n15: A ← Aggregate Results( RE 1, RE 2,… , RE n )\n16: Return A\n4.2 Non-autoregressive program generation\nTo facilitate non-autoregressive program generation, ConcurGen ﬁrst augments the input by\ninserting special tokens-such as constants below 10 and commonly used order indicators-\ninto the question, table, and associated text segments. This enriched input is then encoded\nusing a RoBERTa-based encoder, resulting in a set of context-aware representations denoted\nas h\no. These vectors serve as the foundation for the program generation module. The non-\n123\nWorld Wide Web (2025) 28 :42 Page 9 of 30 42\nautoregressive program generation module predicts a complete reasoning program in a single\nforward pass by jointly modeling the operator types and their execution order for each relevant\ncolumn. During training, the model is supervised using ground-truth programs, enabling it\nto capture global dependencies without conditioning on previously generated outputs. This\nstands in contrast to autoregressive decoders, which generate reasoning steps sequentially and\noften suffer from error accumulation. By eliminating such step-wise dependencies, the non-\nautoregressive design signiﬁcantly improves robustness and allows for parallel computation,\nleading to better inference efﬁciency and accuracy.\nThe Soft-Masking Operand Extractor plays a crucial role in identifying all operands in\nthe expected reasoning program. It employs a two-layer feed-forward network (FFN) over\nthe entire RoBERTa representation, h\no, to predict the probability, pt , of each token being an\noperand. Subsequently, soft masking [ 20] is applied to ho based on pt , as shown in ( 1)a n d( 2).\nHere, hs represents the soft-masked representation, and vm denotes the mask embedding,\nwhich is initialized as a zero vector across all dimensions. The initialization of vm as a zero\nvector serves an important purpose: it effectively nulliﬁes the contribution of tokens with\nlow probability ( p\nt ). This allows the model to focus on more relevant tokens with higher\nprobabilities, enhancing the extraction of pertinent operands. By using a zero vector, any token\nwith a low probability will have its representation pushed closer to zero, which minimizes\nits inﬂuence on the subsequent reasoning steps. The element-wise multiplication is indicated\nby ⊙ . The soft-masking mechanism prioritizes evidence representation by assigning higher\nweights to tokens with larger p\nt values, while tokens with smaller pt values are pushed\ncloser to the mask embedding. Unlike hard masking, which relies on classiﬁcation, soft\nmasking is differentiable, enabling end-to-end training and reducing error propagation. This\nis because the gradient can ﬂow through the soft mask weights p\nt in ( 1), allowing the model\nto update these weights during backpropagation, leading to better representation learning and\noverall performance improvement. Here, h\ns represents the soft-masked representation, and\nvm denotes the mask embedding, which is initialized as a zero vector across all dimensions.\nThe initialization of vm as a zero vector serves an important purpose: it effectively nulliﬁes\nthe contribution of tokens with low probability ( pt ). This allows the model to focus on more\nrelevant tokens with higher probabilities, enhancing the extraction of pertinent operands. By\nusing a zero vector, any token with a low probability will have its representation pushed\ncloser to zero, which minimizes its inﬂuence on the subsequent reasoning steps.\np\nt = softmax\n(\nFFN\n(\nho))\n(1)\nhs = ho ⊙ pt + vm ⊙\n(\n1 − pt )\n(2)\nThe Length Predictor is a multi-class classiﬁer that estimates the number of reasoning\nsteps required to answer the question. Each reasoning step encompasses a complete program\ntuple, as shown in ( 3).\np\nlength = softmax (FFN ([CLS])) (3)\nNext, the Soft-Masking Operand Generator extracts the two operands for each speciﬁc\nreasoning step from the input, as described in ( 4)a n d( 5). The Operator Generator, equipped\nwith six operators (Addition, Subtraction, Multiplication, Division, Exp, and Greater), selects\nthe appropriate operator using a multi-classiﬁer, as shown in ( 6).\np\ne = softmax\n(\nFFN\n(\nhs ))\n(4)\nhe = hs ⊙ pe + vm ⊙\n(\n1 − pe)\n(5)\npop = softmax\n(\nFFN\n(\nmean\n(\n[CLS]| he)))\n(6)\n123\n42 Page 10 of 30 World Wide Web (2025) 28 :42\nRecognizing that operand order is crucial for certain operators, the Order Predictor deter-\nmines the sequence of operands, as expressed in ( 7).\nporder = softmax\n(\nFFN\n(\nmean\n(\n[CLS]| he)))\n(7)\nOnce the operator and operand sequences have been determined, ConcurGen executes\nthe program sequence in parallel, further boosting the efﬁciency of the non-autoregressive\nmodel.\nOptimization is a critical aspect of ConcurGen’s training process. By employing advanced\noptimization techniques such as AdamW [ 48] and learning rate annealing, ConcurGen\nachieves faster convergence and improved generalization. The loss function used in\nConcurGen incorporates both the classiﬁcation error and a regularization term to prevent\noverﬁtting, as shown in ( 8), where\nLclass represents the classiﬁcation loss, Lreg is the regu-\nlarization term, and λ denotes the regularization coefﬁcient.\nL = Lclass + λ ⊙ Lreg (8)\nExtensive experiments demonstrate that ConcurGen’s streamlined non-autoregressive\nprogram generation approach signiﬁcantly outperforms traditional methods in terms of both\nspeed and accuracy, highlighting its robustness and efﬁciency in handling complex hybrid\nquestion answering tasks.\n4.3 Optimization objective\nTo ensure optimal numerical reasoning, ConcurGen minimizes a weighted sum of the neg-\native log-likelihood losses for each module, as expressed in ( 9). Here, NLL represents the\nnegative log-likelihood loss function, r denotes the true labels, λindicates the weight assigned\nto each module, and n represents the maximum number of reasoning steps.\nLtotal = λt ⊙ Loperand + λlength ⊙ Llength + λe ⊙ Loperand_extract\n+ λop ⊙ Loperator + λorder ⊙ Lorder,\nLoperand = NLL\n(\nlog\n(\npt )\n,rt )\n,\nLlength = NLL\n(\nlog\n(\nplength)\n,rlength)\n,\nLoperand_extract =\nn∑\ni=0\nNLL\n(\nlog\n(\npe\ni\n)\n,re\ni\n)\n,\nLoperator =\nn∑\ni=0\nNLL\n(\nlog\n(\npop\ni\n)\n,rop\ni\n)\n,\nLorder =\nn∑\ni=0\nNLL\n(\nlog\n(\nporder\ni\n)\n,rorder\ni\n)\n.\n(9)\nThe optimization objective in ConcurGen is carefully designed to balance the con-\ntributions of each module to the overall loss. By assigning appropriate weights to the\nnegative log-likelihood losses of the Soft-Masking Operand Extractor ( λ\nt ), Length Predictor\n(λlength), Soft-Masking Operand Generator ( λe), Operator Generator ( λop), and Order Pre-\ndictor ( λorder), ConcurGen ensures that each component is adequately trained to perform its\nspeciﬁc task effectively.\n123\nWorld Wide Web (2025) 28 :42 Page 11 of 30 42\nThe use of the negative log-likelihood loss function enables ConcurGen to optimize the\nprobability distributions generated by each module, encouraging the model to assign higher\nprobabilities to the correct labels. By summing the losses across all reasoning steps (up to a\nmaximum of n steps), ConcurGen can learn to generate accurate and efﬁcient programs for\na wide range of numerical reasoning tasks.\nDuring the training process, the optimization objective is minimized using gradient-based\noptimization algorithms, such as AdamW [ 48], which adapt the learning rate for each param-\neter based on its historical gradients. This adaptive optimization approach helps ConcurGen\nconverge faster and achieve better generalization performance.\nMoreover, the incorporation of regularization techniques, such as L1 and L2 regularization,\nhelps prevent overﬁtting by adding a penalty term to the loss function. This penalty term\ndiscourages the model from learning overly complex or noise-sensitive patterns, promoting\nsimpler and more robust solutions. The regularization term can be expressed as:\nLreg = λ1 ⊙\n∑\ni |θi | + λ2 ⊙\n∑\ni\nθ2\ni (10)\nwhere θi represents the model parameters, and λ1 and λ2 are the regularization coefﬁcients\nfor L1 and L2 regularization, respectively.\nIn addition to the main optimization objective, ConcurGen also employs auxiliary objec-\ntives to further improve its performance. For instance, a contrastive loss function can be\nused to enhance the model’s ability to distinguish between relevant and irrelevant facts. The\ncontrastive loss encourages the model to learn representations that maximize the similarity\nbetween the question and the relevant facts while minimizing the similarity between the\nquestion and the irrelevant facts. This can be formulated as:\nLcontrast =\n∑\ni = 1N\n[\nlog σ\n(\nq⊤ f i+\n)\n+\n∑\nj = 1K log\n(\n1 − σ\n(\nq⊤ f −\nij\n))]\n(11)\nwhere q is the question representation, f i+ is the representation of the i-th relevant fact,\nf −\nij is the representation of the j-th irrelevant fact for the i-th question, N is the number of\nquestions, K is the number of irrelevant facts per question, and σ is the sigmoid function.\nBy incorporating these additional optimization techniques and auxiliary objectives,\nConcurGen can further enhance its performance and generalization capabilities, making\nit a highly effective and efﬁcient model for hybrid question answering tasks.\nIn summary, ConcurGen’s optimization objective, which combines weighted negative\nlog-likelihood losses for each module, incorporates regularization techniques, and employs\nauxiliary objectives, plays a crucial role in training the model to generate accurate and efﬁcient\nprograms for hybrid question answering tasks. By carefully balancing the contributions of\neach component and employing advanced optimization algorithms, ConcurGen achieves\nstate-of-the-art performance in terms of both speed and accuracy, setting new benchmarks in\nthe ﬁeld of hybrid QA.\n4.4 Optimization details\nThe ConcurGen model architecture is designed to efﬁciently process and reason over the\ninput data, which consists of the question, tables, and text segments. The backbone of the\nmodel is the RoBERTa encoder, which is a pre-trained language model that has been shown\nto capture rich semantic information from textual data. By ﬁne-tuning the RoBERTa encoder\non the speciﬁc task of hybrid question answering, ConcurGen can effectively extract relevant\nfeatures and representations from the input data.\n123\n42 Page 12 of 30 World Wide Web (2025) 28 :42\nModel Components The various components of ConcurGen, such as the Soft-Masking\nOperand Extractor, Length Predictor, Soft-Masking Operand Generator, Operator Gener-\nator, and Order Predictor, are implemented as neural network modules that operate on the\nrepresentations produced by the RoBERTa encoder [ 49]. These modules are designed to per-\nform speciﬁc tasks, such as identifying operands, predicting the number of reasoning steps,\nand selecting the appropriate operators and operand order.\nSoft-Masking Mechanism The Soft-Masking Operand Extractor and Soft-Masking Operand\nGenerator employ a novel soft-masking mechanism, which allows the model to prioritize rel-\nevant information and suppress irrelevant details. This is achieved by applying a learnable\nmask to the representations, where the mask values are determined by the predicted probabil-\nities of each token being an operand. The soft-masking mechanism is differentiable, enabling\nend-to-end training of the model and reducing the impact of error propagation.\nClassiﬁer Implementation The Length Predictor, Operator Generator, and Order Predictor\nare implemented as multi-class classiﬁers that operate on the representations produced by\nthe RoBERTa encoder and the soft-masking modules. These classiﬁers are trained to predict\nthe number of reasoning steps, the appropriate operators, and the correct order of operands,\nrespectively. The classiﬁers are implemented using feed-forward neural networks (FFNs) with\nsoftmax activation functions to produce probability distributions over the possible classes.\nTraining and Optimization To facilitate efﬁcient training and inference, ConcurGen is\nimplemented using the PyTorch deep learning framework. The model is trained using the\nAdamW optimizer, which is an improved version of the Adam optimizer that incorporates\nweight decay regularization. The learning rate is adjusted using a learning rate scheduler,\nsuch as cosine annealing or step decay, to improve convergence and generalization. During\ntraining, ConcurGen is fed with batches of input data, consisting of questions, tables, and\ntext segments. The model processes the input data through the RoBERTa encoder and the\nvarious neural network modules, producing predictions for the operands, reasoning steps,\noperators, and operand order. The predicted values are compared against the ground-truth\nlabels, and the loss is computed using the optimization objective described in ( 9). The gradi-\nents of the loss with respect to the model parameters are computed using backpropagation,\nand the parameters are updated using the AdamW optimizer.\nRegularization Techniques To prevent overﬁtting and improve generalization, the proposed\nConcurGen employs several regularization techniques, such as L1 and L2 regularization ( 10),\ndropout [ 50], and early stopping. These techniques help to reduce the model’s sensitivity to\nnoise and prevent it from learning overly complex patterns that may not generalize well to\nunseen data.\nAuxiliary Objectives In addition to the main optimization objective, ConcurGen also incor-\nporates auxiliary objectives, such as the contrastive loss ( 11), to enhance its performance.\nThese auxiliary objectives are designed to encourage the model to learn more robust and\ndiscriminative representations of the input data, which can improve its ability to reason and\ngenerate accurate programs.\nInference and Evaluation During inference, ConcurGen takes in a question, along with the\ncorresponding tables and text segments, and generates a complete reasoning program in a\nsingle forward pass. The generated program is then executed to produce the ﬁnal answer to the\nquestion. The non-autoregressive nature of ConcurGen allows for efﬁcient parallel generation\nof the program, signiﬁcantly reducing the inference time compared to autoregressive models.\nTo ensure the robustness and generalization of ConcurGen, the model is evaluated on multiple\n123\nWorld Wide Web (2025) 28 :42 Page 13 of 30 42\nbenchmark datasets for hybrid question answering, such as HybridQA [ 51] and FinQA [ 13].\nThese datasets contain a diverse set of questions that require reasoning over both tabular\nand textual data, making them suitable for assessing the performance of hybrid QA models.\nConcurGen has achieved state-of-the-art results on these benchmarks, demonstrating its\neffectiveness in handling complex reasoning tasks.\n5 LLM-empoweredHTQA\nIn this chapter, we introduce the background of large language models and their methodolo-\ngies for addressing HTQA task challenges, taking the ﬁnance domain as a case study. We\npropose an advanced workﬂow framework integrating multiple large language models and\nvarious prompting strategies. This framework is capable of handling traditional and open -\nended question - answering tasks related to HTQA.In this section, in addition to introducing\nthe LLMs empowered HTQA method, we will also use the ﬁnancial sector as an example to\nillustrate the speciﬁc details of implementing this method. Figure 3 illustrates an overview\nof this framework.\n5.1 Overview\nThe Demand for Efﬁcient Information Exchange in the Era of Digital Economies In the\nera of ﬂourishing digital economies, the demand for efﬁcient information exchange across\nvarious industries is rapidly escalating. Large Language Models (LLMs), representing a\nsigniﬁcant technological breakthrough in the ﬁeld of artiﬁcial intelligence, are progressively\nemerging as pivotal drivers to address this burgeoning demand [ 52].\nTechnical Characteristics and Capabilities of LLMs Grounded in the Transformer archi-\ntecture, Large Language Models (LLMs) eschew the sequential processing methodologies\ninherent in traditional Recurrent Neural Networks (RNNs) and Long Short-Term Mem-\nory networks (LSTMs) [ 53–56]. Instead, they leverage multi-head attention mechanisms,\nenabling an efﬁcacious focus on disparate segments of input text and tabular data. This\nmethodological shift facilitates a more efﬁcient capture of long-range dependencies and\nsemantic associations between textual and tabular modalities.\nThrough pre-training on expansive corpora of textual [ 57] and structured tabular data,\nthese models accrue a substantial repository of linguistic knowledge, semantic represen-\ntations, and proﬁciencies in parsing tabular data, thereby exhibiting robust capabilities in\nlanguage understanding and generation. Consequently, they are adept at comprehending\nintricate semantics within natural language, extracting salient information from both texts\nand tables, and generating coherent and contextually relevant responses.\nDevelopment Trajectory of LLMs The development of Large Language Models (LLMs)\nhas traversed several signiﬁcant phases. Early iterations of language models were character-\nized by relatively constrained parameter counts and limited functionalities. However, with\nongoing technological advancements and substantial augmentations in computational power,\nOpenAI’s GPT-3 and GPT-4 [ 58, 59] distinguished themselves through its potent language\ngeneration capabilities, proﬁciently executing a diverse array of complex tasks ranging from\narticle authorship and code synthesis to interactive dialogues.\nSubsequently, a constellation of advanced Large Language Models has emerged domes-\ntically, exempliﬁed by Baidu’s ERNIE Bot [ 60–63] and Alibaba’s Qwen [ 64], which are\nassuming critical roles across a multitude of domains and application scenarios.\n123\n42 Page 14 of 30 World Wide Web (2025) 28 :42\nThe Financial Industry’s Demand for Hybrid Tabular-Textual Question Answering The\napplication of LLMs in the ﬁnance sector has engendered profound changes [ 65, 66]. The\noperations of the ﬁnancial industry are intrinsically reliant on vast and intricate datasets,\nencompassing rapidly ﬂuctuating information from ﬁnancial markets, such as stock prices\nand bond yields. Furthermore, macroeconomic data, including GDP growth rates and inﬂation\nrates, alongside micro-level corporate ﬁnancial data, such as balance sheets and cash ﬂow\nstatements, are of paramount importance.\nThese data are presented in diverse formats, including text and tables, imposing stringent\ndemands on data complexity and timeliness, characterized by rapid update cycles. Investors\nfrequently rely on these immense datasets to appraise investment opportunities, while ﬁnance\nprofessionals are obligated to provide specialized services predicated on precise data analysis.\nHowever, the inherent limitations of personal knowledge often impede both investors and\nﬁnance professionals from surmounting information barriers.\nIn ﬁnancial Question Answering systems, Hybrid Tabular-Textual Question Answering\n(HTQA) tasks are critical [ 67, 68], as they necessitate systems to comprehensively process\nhybrid forms of textual and tabular information to address complex user queries. Conse-\nquently, the capacity to swiftly and accurately extract ﬁnance-related information from mixed\ntextual and tabular data becomes indispensable.\nLimitations of Traditional Deep Learning Models in Financial HTQA Tasks Within this\nindustry context, the efﬁcient resolution of Hybrid Tabular-Textual Question Answering\n(HTQA) tasks has emerged as a central challenge for the advancement of the ﬁnancial sec-\ntor. LLMs possess the capability to integrate textual information and tabular data, thereby\nsupporting risk assessment through the analysis of extensive ﬁnancial data and market intel-\nligence. This capability aids investors in analyzing market trends and selecting investment\ntargets, thus facilitating more scientiﬁcally grounded investment recommendations.\nPrior to the advent of LLMs, traditional deep learning models encountered multiple lim-\nitations when addressing HTQA tasks within the ﬁnance sector. Traditional deep learning\nmethodologies often proved inadequate when confronted with open-ended queries, But LLMs\nperform very well in this regard [ 69]. For instance, when an investor inquires, \"What are the\ninvestment prospects for technology stocks in the current macroeconomic landscape?\", the\nLLMs’ response necessitates consideration of multifarious factors, including development\ntrends within the technology industry, the impacts of macroeconomic policies on technology\nﬁrms, and capital market sentiments. The performance of traditional deep learning methods\non this issue is far inferior to LLMs.\nRelevant information is frequently dispersed across various textual reports and statistical\ntables. However, conventional QA systems struggle to conduct nuanced semantic analyses\nand effectively consolidate multi-domain knowledge from text and tables. Limitations in\nsemantic understanding models and tabular processing capabilities result in responses that\nare comparatively superﬁcial, failing to satisfy user demands for in-depth analysis.\nChallenges Confronting LLMs Additionally, large language models can process multiple\nqueries within a single QA session, leveraging prior answers as novel context to more effec-\ntively address subsequent questions.\nNonetheless, LLMs also encounter several challenges in their development and applica-\ntion. ﬁrstly, there is the issue of data quality; the quality of pre-training data directly inﬂuences\nmodel performance [ 70]. If the data contain biases, errors, or are outdated, it may lead to\ninaccurate or biased responses generated by the model. Secondly, the model interpretability\nproblem arises; the structure and parameters of LLMs are highly complex, rendering their\ndecision-making processes opaque.\n123\nWorld Wide Web (2025) 28 :42 Page 15 of 30 42\nThis complexity can limit their applicability in certain ﬁnancial scenarios, such as risk\nassessment and compliance review, where interpretability is paramount. Moreover, privacy\nand security issues cannot be overlooked. Ensuring the protection of user data privacy while\nmaintaining model performance has become a primary research priority now.\nPotential of LLMs in Financial HTQA Tasks Despite the challenges, LLMs hold promise in\novercoming the limitations posed by conventional deep learning methods in HTQA tasks\nwithin the ﬁnance sector. Through intelligent classiﬁcation of inquiries and targeted prompt-\ning strategies, the generalizability and accuracy of systems in managing different types of\nquestions can be improved. This progression paves new pathways for the development of\nintelligent information exchange in the ﬁnancial industry. For example, initial applications\nof OpenAI’s GPT series models in the ﬁnance sector have demonstrated exceptional under-\nstanding and response capabilities for complex ﬁnancial HTQA inquiries, ushering in new\nopportunities for digital transformation in the ﬁnancial industry.\n5.2 General QA framework\nApplications of LLMs Across Diverse Domains In terms of application contexts, Large Lan-\nguage Models have permeated extensively across various domains. Within the healthcare\nsector [ 71], they assist physicians in disease diagnosis, medical record analysis, and medi-\ncation recommendations by integrating clinical text with examination indicators presented\nin tabular formats. In the realm of education [ 72], they facilitate intelligent tutoring based\non textbook content and grade tables, as well as automate homework correction and the\ndevelopment of personalized learning plans.\nIn various ﬁelds of QA services, LLMs can supplant human agents in addressing frequently\nasked questions by integrating product introduction text and tables [ 73], thereby enhancing\nservice efﬁciency and quality. The application of LLMs in the various ﬁelds is instigating\nunprecedented transformations.\nIn this section, we present the HTQA workﬂow methodology developed using LLMs\nto effectively tackle both traditional and open-ended HTQA tasks. Our approach seeks to\nenhance the versatility and user satisfaction by systematically classifying the questions and\nselecting the appropriate prompting strategies.\nFigure 2 presents our LLMs-HTQA ﬂow, which consists of three sequential stages:\nQuestion Input and Classiﬁcation upon submission of a question by users, the workﬂow\ninitially classiﬁes the inquiry, subsequently selects the most suitable LLMs and prompting\nstrategy, and ultimately generates and presents the corresponding answer. Our LLMs work-\nFig. 2 General HTQA ﬂow\n123\n42 Page 16 of 30 World Wide Web (2025) 28 :42\nﬂow is initiated by processing the user’s input question through the \"question classiﬁcation\"\nmodule. The primary objective of this module is to ascertain the type of question posed,\nwhich directly determines the subsequent selection of the LLMs and the relevant prompting\nstrategy. We categorize the potential challenges encountered within the framework of large\nmodels into two distinct types:\n• Open-ended Question: Queries that require synthesis, trend analysis, or comparative\nreasoning.\n• Objective Question: Queries demanding precise lookup or extraction of a speciﬁc value\nor keywords.\nThis early lassiﬁcation ensures that subsequent processing can be optimized for either\nanalytical depth or factual accuracy, tailoring prompts and retrieval strategies to the user’s\nintent.\nIntelligent Agent Workﬂow Design Depending on the question type, our intelligent agent\nﬂow three core components:\n1. Prompt Step Design and Table Understanding: We deﬁne a set of modular prompt tem-\nplates that progressively guide the language model through table structure (headers, rows,\nunits) and content reasoning.\n2. Exclusive prompt strategy: Based on the differences between Open-ended Question and\nObjective Question mentioned above, the logic and steps of the big model to answer its\nquestions should also be changed, so different prompt strategies should be implemented\nfor different question modes. For open-ended questions, prompts encourage LLMs to\nrefer to external knowledge bases and explore their own learned knowledge. For objective\nquestions, prompts require LLMs to refer to the data of given information more rigorously\nrather than express themselves freely.\n3. Embedding and Retrieval Model Integration: In the construction of external knowledge\nbase, both embedding model and retrieval model play a vital role.\nEmbedded model can transform text into vectors. In this way, computers can process and\nanalyze these text data, because calculations in vector space are more efﬁcient and easy\nto understand for computers. Embedding Model can also capture the intrinsic semantic\nfeatures and contextual relationships of text by training with a large amount of text\ndata. [ 74]\nRetrieval Model can re-evaluate, sort and ﬁlter search results based on the relevance to\nuser queries and the characteristics of knowledge base content.It can also use its own\nalgorithms and mechanisms to quickly ﬁnd content related to user queries in a huge\nknowledge base and exclude irrelevant information, thereby improving the efﬁciency\nand accuracy of retrieval. [ 75]\nAnswer Generation Finally, the agent consolidates the model’s intermediate outputs, formats\nthem into a clear Question-Answer dialogue, and returns the response to the user. This\nmodular workﬂow balances efﬁciency (for straightforward lookups) with interpretability\n(for in-depth analyses), yielding robust performance across diverse tabular QA tasks.\nThe rationale underlying this methodological ﬂow arises from the imperative to augment\nthe interaction between users and the computational systems utilized to address their inquiries.\nBy effectively classifying questions a priori, our approach facilitates a more nuanced compre-\nhension of user intent, thereby guiding the strategic selection of Large Language LLMs best\n123\nWorld Wide Web (2025) 28 :42 Page 17 of 30 42\nequipped to tackle the speciﬁc complexities inherent to each question type. The bifurcation\nof inquiries into open-ended and ﬁnancial computation questions is particularly pertinent, as\nthese categories exhibit distinct requirements. Open-ended questions, noted for their subjec-\ntive and exploratory characteristics, necessitate prompting strategies that are expansive and\nadaptable, allowing the LLMs to produce rich, contextually aware responses. In contrast,\nﬁnancial computation questions demand rigor and precision; thus, they are better served by\nprompting techniques that concentrate on structured reasoning and factual accuracy [ 76].\nMoreover, this systematic classiﬁcation enables us to mitigate potential inefﬁciencies\ntypically associated with generalized querying. By honing the scope to speciﬁc question\ntypes, we alleviate the cognitive load on the LLMs, thereby optimizing their operational\nefﬁcacy and enhancing response times. Such procedural reﬁnements not only enhance the\naccuracy of outputs but also contribute to an improved overall user experience, reducing\nfrustration and maximizing satisfaction.\n5.3 A case study in finance QA\nIn this section, we will use a ﬁnancial HTQA case to elucidate the speciﬁc details of the\nworkﬂow framework, which is primarily subdivided into three components: knowledge base\npreprocessing, prompt strategies for open-ended question answering sessions, and prompt\nFig. 3 Schematic representation of our proposed Workﬂow framework and prompt strategy\n123\n42 Page 18 of 30 World Wide Web (2025) 28 :42\nstrategies for traditional question answering sessions. Figure 3 shows the speciﬁc process\ndesign and the content of the three components.\nKnowledge Base Preprocessing The preprocessing operations conducted on the knowledge\nbase yield a structured and efﬁcient foundation for searchable knowledge, thereby facilitating\nsubsequent open-ended question answering. Speciﬁcally, three preprocessing methodologies\nwere employed: the establishment of segment identiﬁers, the construction of models utilizing\nindexes, and the application of retrieval models.\nThe dataset is segmented into three distinct components: Pre text, Post text, and Table,\nwhich together furnish a comprehensive context for each data unit. To enhance the manage-\nment and retrieval capabilities of this data, the system assigns segment identiﬁers to each data\nslice within the dataset. For instance, in a dataset comprising multiple ﬁnancial reports, each\nreport’s data slice is designated with a unique identiﬁer, enabling the system to swiftly iden-\ntify and access pertinent data segments during problem processing. This practice signiﬁcantly\nenhances both the efﬁciency and accuracy of data processing.\nWith respect to index construction, this system employs a text-embedding-3-large embed-\nding model as the embedding mechanism [ 77, 78]. This model adeptly transforms text data\ninto vector representations, thereby capturing semantic features and contextual relationships\ninherent in the text by leveraging extensive textual data for training. It facilitates the conver-\nsion of various textual forms, such as ﬁnancial news, research reports, and industry analyses,\ninto corresponding vector representations. Consequently, semantically similar texts exhibit\nanalogous positional relationships within the vector space, greatly streamlining the ensuing\nretrieval process and enabling prompt alignment of relevant knowledge base content with the\nsemantics of user inquiries.\nLastly, the system leverages rerank-v3.5 [ 79] as the retrieval model. This model incorpo-\nrates sophisticated sorting and ﬁltering algorithms capable of reevaluating search outcomes\nbased on the relevance to user inquiries and the knowledge base content. Within the ﬁnancial\nrealm, it prioritizes the presentation of information that is not only highly pertinent but also\naccurate and authoritative.\nThrough the aforementioned series of knowledge base preprocessing procedures, this sys-\ntem has successfully achieved effective organization, semantic processing, and efﬁcient data\nretrieval, thereby ensuring that user inquiries can be answered with precision and expediency.\nFurthermore, the implementation of a knowledge base context retrieval method transcends\nthe limitations associated with conventional datasets, rendering this workﬂow framework\ntruly applicable to open-ended question answering scenarios.\nDesign for Open-ended Questions Open-ended questions frequently necessitate the model\nto deliver comprehensive discussions and multidimensional analyses. For instance, a user\nmay pose the question, \"What investment advice do you have for crude oil and natural gas?\"\nIn such scenarios, users are not seeking speciﬁc numerical responses; rather, they may simply\nbe interested in the pertinent ﬁeld, a context that is more prevalent in actual QA applications\nwithin the ﬁnancial sector.\nTo address these open-ended inquiries, we propose a novel prompting strategy:\nAnalysis→Search→Recommend (ASR). Furthermore, we have established a knowledge\nbase by extracting and consolidating all contextual and tabular information from the Con-\nvFinQA and MultiHiertt datasets. Consequently, when users present questions, relevant\ncontext and tabular data can be retrieved from the knowledge base. Utilizing the ASR prompt-\ning strategy, this retrieved content is analyzed in conjunction with the user’s question to\nformulate answers, thereby fulﬁlling the open-ended QA tasks for HT within the ﬁnancial\n123\nWorld Wide Web (2025) 28 :42 Page 19 of 30 42\ndomain. Figure 3 illustrates our ASR methodology. The speciﬁcs of each ASR step are\ndelineated as follows:\n1. Analysis: Assess areas of interest pertinent to users, leveraging their professional knowl-\nedge alongside their inquiries.\n2. Search: Extract information from the knowledge base based on the preceding analysis,\nand further evaluate the retrieved information.\n3. Recommend: Propose relevant documents derived in descending order of relevance,\nmimicking the format of ﬁnancial experts addressing inquiries, while providing recom-\nmendations grounded in the retrieved content.\nDesign for Financial Calculation Questions Conversely, ﬁnancial calculation problems are\ninherently more straightforward, with users anticipating clear calculation outcomes. For\nexample, a user might inquire, \"What is the growth rate in sales from 2013 to 2014?\" Such\nquestions necessitate that the model swiftly locate pertinent professional knowledge to infer\nthe calculation process.\nTo enhance the accuracy of the calculation derivation process, we have implemented a\nprompting strategy that empowers the large language model to engage in step-by-step reason-\ning until the correct answer is attained: Analysis→Knowledge Elicitation→Identiﬁcation →\nStep-by-step→Predict (AKISP). Figure 3 depicts our AKISP approach. The details of each\nAKISP step are as follows:\n1. Analysis: Identify the speciﬁc categories of information critical for addressing the inquiry\nwithin the framework of this ﬁnancial analysis task.\n2. Knowledge Elicitation: Locate the pertinent knowledge necessary to resolve the problem\nfrom the ﬁnancial domain, and devise suitable methods to tackle the issue.\n3. Identiﬁcation: Transform tabular data into declarative sentences and amalgamate them\nwith the context to extract key evidence for answering the inquiries presented in the\nintegrated text.\n4. Step-by-step: Deﬁne a series of atomic operations capable of resolving the given prob-\nlem, encompassing addition, subtraction, multiplication, and division.\n5. Predict: Decompose a complex task into a sequential arrangement of the aforementioned\natomic operations. Finally, arrive at the conclusion with the answer .\nAdditionally, we leverage the capabilities of the large language model to address open-\nended questions by offering users relevant suggestions within the ﬁnancial domain based on\nthe outcomes derived.\nLimitations of LLMs in HTQA Tasks We found in our experiments that although LLMs\nhave strong language comprehension and generation abilities, their performance in handling\ntraditional ﬁnancial computational question answering tasks may not consistently reach or\nsurpass that of conventional deep learning methods. Trained on a vast corpus of textual data,\nLLMs possess a relatively broad and ﬂexible understanding of knowledge, which can lead\nto slight deviations when confronted with stringent ﬁnancial computational regulations.\nThis arises from an insufﬁcient grasp of speciﬁc computational details. For instance, when\nanalyzing the impact of different ﬁnancing structures on the calculation of the Weighted\nAverage Cost of Capital (W ACC), variables such as bank loans, bond issuance, and equity\nﬁnancing each have complex and precise methods of cost calculation and effects on a com-\npany’s capital structure. Traditional deep learning methods, having undergone specialized\noptimization, adhere more strictly to ﬁxed computational logics, accurately capturing these\nintricate relationships. In contrast, the inherent ﬂexibility of Large Language Models can\n123\n42 Page 20 of 30 World Wide Web (2025) 28 :42\nsometimes result in misunderstandings of certain details, thereby affecting the precision of\ncomputational outcomes.\nFurthermore, in terms of computational time, traditional deep learning approaches\nhave optimized hardware and algorithms speciﬁcally for ﬁnancial computational tasks,\nthereby achieving shorter computation times. Conversely, LLMs, when processing com-\nplex computational tasks that require managing numerous parameters and intricate language\ncomprehension processes, can exhibit comparatively longer computation times. This renders\nthem less capable of meeting the stringent real-time requirements of ﬁnancial operations.\nHowever, it is crucial to note that due to the enormous parameter count of large lan-\nguage models, they have not undergone speciﬁc ﬁne-tuning, which may result in suboptimal\nperformance. Additionally, strategies such as constructing appropriate prompt formulations\nand utilizing larger parameter LLMs hold the potential to enhance the performance of large\nmodels, aligning it more closely with the ConcurGen model architecture proposed herein.\nGiven the strengths of large language models in open-ended query responses and in\naddressing multiple questions simultaneously, exploring how these models could further\nenhance their performance in HTQA tasks is of signiﬁcant importance.\n6 Experimentalanalysis\nDatasets Our experimental analysis was carried out using two datasets: ConvFinQA 1 and\nMultiHiertt2. The ConvFinQA collection has 14 ,115 entries divided into 11 ,104 training,\n1,490 development, and 1,521 testing examples. This dataset is known for its intricate numer-\nical reasoning challenges within real-world dialogues [ 15]. On the other hand, every entry\nin MultiHiertt is characterized by several hierarchical tables and extended unstructured con-\ntent [ 16]. It comprises 10 ,440 entries, segmented into 7 ,830 training, 1 ,044 development,\nand 1 ,566 testing examples. It’s worth noting that the testing labels for both ConvFinQA\nand MultiHiertt remain undisclosed.\nAssessment Criteria We utilized Exact Matching (EM) and the tailored numeracy-centric\nF1 [ 8] metric for MultiHiertt. For ConvFinQA, we employed execution accuracy (Exe Acc)\nand program accuracy (Prog Acc) based on prior studies.\nReference Models Two generation models, GPT-2 [ 80]a n dT 5[ 47], served as our baselines.\nTAGOP [12] employs a sequential tagging approach for fact extraction and conducts a single\narithmetic operation. FinQANet [13]a n d MT2Net [16] can handle multiple-step reasoning\nand both employ an autoregressive LSTM decoder for program generation.\nModel Conﬁguration We adjusted parameters on the development set for optimization. In\norder to maintain consistency with prior top-performing results, we retained the experiment\nconﬁgurations of FinQANet and MT2Net. For GPT-2 and T5, we employed medium and\nlarge variants respectively, while the remaining baselines utilized the RoBERTa-large model.\nOur models were trained on an RTX3090 GPU, with a maximum reasoning step cap of 5 for\nConvFinQA and 10 for MultiHiertt. Focusing on generating the program, and for an equitable\ncomparison, we merely switched the program creation component of MT2Net and FinQANet\nwith ConcurGen, preserving the other segments. Given that FinQANet uses a single LSTM\nfor decoding based on the question type and doesn’t possess a distinct span extraction module,\nwe instructed the ConcurGen’s length predictor to forecast a length of zero. Consequently,\n1 https://github.com/czyssrs/ConvFinQA\n2 https://github.com/psunlpgroup/MultiHiertt\n123\nWorld Wide Web (2025) 28 :42 Page 21 of 30 42\nTable 1 Performance outcomes on ConvFinQA and MultiHiertt\nModel ConvFinQA MultiHiertt\nName Exe Acc Prog Acc EM F1\nGPT-2 (medium) 58.19 57.00 - -\nT5 (large) 58.66 57.05 - -\nTAGOP (RoBERTa-large) - - 17.81 19.35\nFinQANet (RoBERTa-large) 68.90 68.24 31.72 33.60\nMT2Net (RoBERTa-large) - - 36.22 38.43\nOur Approach (RoBERTa-base) 69.82 68.84 38.19 38.81\nOur Approach (RoBERTa-large) 73 .96 73 .04 44 .19 44 .81\nwe extracted the span with peak prediction probability directly from the operand extractor’s\noutput, catering to span extraction queries in the ConvFinQA dataset.\n6.1 Main results\nConcurGen’s performance, in both base and large conﬁgurations, was compared with our\nreference models, as delineated in Table 1. Inferences from Table 1 include:\n1. GPT-2 and T5, despite being pre-trained, don’t outshine LSTM, suggesting their lack of\nspecialized training for generating numerical reasoning programs.\n2. The RoBERTa base conﬁguration improves performance across both datasets.\n3. The advanced RoBERTa conﬁguration signiﬁcantly elevates ConcurGen’s outcomes on\nboth ConvFinQA and MultiHiertt datasets.\nConsidering that our method primarily modiﬁes program generation, we evaluated\nConcurGen’s and MT2Net’s efﬁcacy on all numerical reasoning tasks within MultiHiertt’s\ndevelopment dataset, as depicted in Table 2.\nA key observation from Table 2 is the pronounced advantage of ConcurGen over MT2Net\nin numerical reasoning, with a margin of +6.85 in both EM and F1 score.\n6.2 Ablation study of hyper-parameters\nWe embarked on an examination of various hyper-parameters in ConcurGen to discern their\ninﬂuence. Our experiment strategy on MultiHiertt involved singularly amplifying a hyperpa-\nrameter to 2, with others held constant at 1. Subsequent trials combined those hyperparameters\nthat exhibited improvements, assigning higher values to the more inﬂuential ones, as detailed\nin Table 3.\nTable 3 reveals varying optimal conﬁgurations based on model scale. The most effective\nsetup for the base model involves a λ\nop of 2, while the large model beneﬁts from λop of 2\nand λorder of 1.5, with other parameters at 1.\nTable 2 Numerical Reasoning\nPerformance Comparison on\nMultiHiertt\nModel EM F1\nMT2Net (RoBERTa-large) 41.35 41.35\nConcurGen (RoBERTa-large) 48 .20 48 .20\n123\n42 Page 22 of 30 World Wide Web (2025) 28 :42\nTable 3 Hyper-parameter\nInﬂuence on ConcurGen\nPerformance\nBase Large\nλt λlength λe λop λorder EM F1 EM F1\n1 1 1 1 1 38.60 39.54 44.35 45.29\n1 2 1 1 1 37.84 38.77 44.92 45.86\n2 2 1 1 1 37.45 38.39 44.64 45.57\n2 1 1 2 1 37.93 38.87 42.72 43.66\n6.3 Program generation speed analysis\nOur non-autoregressive program generation capitalizes on parallel processing capabilities. To\nillustrate its efﬁciency, we measured ConcurGen’s program generation time against MT2Net’s\nLSTM decoder, timing their performance on MultiHiertt’s training set numerical reasoning\ntasks. These ﬁndings are chronicled in Table 4.\nAs highlighted in Table 4, ConcurGen outpaces MT2Net by a factor of 21, underscoring\nthe inherent rapidity of non-autoregressive decoding made possible through parallelization.\nIn summary, the experimental results demonstrate that:\n1. ConcurGen outperforms state-of-the-art models on both ConvFinQA and MultiHiertt\ndatasets, especially when using the RoBERTa-large conﬁguration.\n2. ConcurGen signiﬁcantly improves numerical reasoning performance compared to\nMT2Net on the MultiHiertt dataset.\n3. Optimal hyper-parameter settings differ based on the model size, with the base model\nbeneﬁting most from a higher λ\nop while the large model works best with higher λop and\nλorder values.\n4. ConcurGen’s non-autoregressive parallel processing enables it to generate programs 21\ntimes faster than MT2Net’s autoregressive LSTM decoder.\nThese ﬁndings highlight ConcurGen’s strong performance and efﬁciency advantages over\nprevious approaches for numerical reasoning over hierarchical data. The ability to opti-\nmize different hyper-parameters for different model sizes also allows ﬂexibility in adapting\nConcurGen to various scenarios and computational constraints.\n6.4 Evaluation of LLMs\nIn this section, we present the experimental results and an analysis of the LLMs Workﬂow.\nWe will present examples of traditional ﬁnancial calculation QA, as well as open-ended QA,\nseparately.\nConcerning traditional ﬁnancial calculation questions, we examine the quality of LLM-\ngenerated answers under varying strategies, as illustrated in Figure 4. Our analysis indicates\nthat, in contrast to the Directly Generate Prompt Strategy, the AKISP Prompt Strategy is\ncapable of accurately generating calculation formulas that adhere to regulatory requirements.\nTable 4 Comparison of Program\nGeneration Speed Model Time (s) Speed-up\nLSTM 168.86 1x\nConcurGen 8.04 21x\n123\nWorld Wide Web (2025) 28 :42 Page 23 of 30 42\nFig. 4 Comparison between Directly Generate Prompt Strategy and AKISP Prompt Strategy\nThroughout the experiment, we observed a noteworthy decline in accuracy with the\nDirectly Generate Prompt Strategy when employed with LLMs possessing smaller parameter\nsizes. This strategy also exhibited persistent issues, including ﬂuctuating ﬁnal results and an\ninability to address multiple queries simultaneously. In more critical instances, these LLMs\nwere unable to locate valid information within the given context and tables required to form\nsatisfactory responses, resulting in the non-generation of any answers. Conversely, the AKISP\nPrompt Strategy consistently demonstrated an ability to retrieve valid information through a\nmethodical step-by-step reasoning approach. This clearly establishes the superiority of the\nAKISP Prompt Strategy.\nFor the AKISP Prompt Strategy, we ultimately selected the gpt-4o-mini model from the\nConvFinQA dataset for our experiments. According to the experimental results, the accuracy\nof the large model workﬂow framework using AKISP Prompt Strategy for ConvFinQA\ndataset is 70.59, slightly lower than our proposed ConcurGen model. This may be due to the\ncomplexity of the data, misunderstandings of the text by the large model, and other reasons.\nBut it has already surpassed other methods, and the large model has the ability to answer\nopen-ended questions, so it is very valuable and potential for studying the application of\nlarge models in HTQA tasks in the ﬁnancial ﬁeld. Below, we will analyze an experimental\nexample of using a large model for open-ended problems.\nFor open-ended questions, we adopted the gpt-4o-mini model and ASR Prompt Strategy,\nas shown in Figure 5.\n123\n42 Page 24 of 30 World Wide Web (2025) 28 :42\nFig. 5 Example of open-ended QA\nFrom this example, it can be seen that our workﬂow framework can retrieve and analyze\nmultiple pieces of data from the knowledge base for open ﬁnance problems. In addition, the\nlarge model can provide suggestions for these contents and make predictions for the future.\nThe application of this comprehensive framework signiﬁcantly enhances the efﬁcacy of\ndecision-making processes within the domain of open ﬁnance. By integrating advanced ana-\nlytical techniques, stakeholders can attain a nuanced comprehension of consumer behaviors,\nmarket dynamics, and regulatory changes. Such insights are instrumental in shaping strategic\ndirections and informing policy initiatives that align with contemporary ﬁnancial trends.\nMoreover, the adaptive quality of our workﬂow ensures that it remains responsive to the\ncontinuous inﬂux of new data. This iterative reﬁnement process is crucial for maintaining\nthe relevance and accuracy of the model’s outputs. In the rapidly changing landscape of open\nﬁnance, the ability to swiftly adapt to emerging information allows organizations to sustain\ntheir competitive edge and optimize their operational strategies effectively.\nFurthermore, we conducted an analysis of the responses to traditional ﬁnancial calcula-\ntion tasks in conjunction with the posed inquiries, subsequently offering suitable ﬁnancial\nguidance to users who submitted such questions. An illustrative example is presented in\nFigure 6.\nIn summary, for the AKISP prompt strategy, the large language model is able to effectively\nretrieve relevant information, answer multiple questions at once, and generate compliant\nmulti-step equations through a systematic step-by-step reasoning method, establishing its\nsuperiority. In open-ended QA tasks, the ASR prompt strategy enables our workﬂow to\nretrieve and analyze multiple data related to open ﬁnance issues from the knowledge base,\nand provide suggestions and predictions for future trends for these contents. The big model\n123\nWorld Wide Web (2025) 28 :42 Page 25 of 30 42\nFig. 6 Example of Result Analysis\ncan also make open analysis and suggestions for the results of the AKISP prompt strategy.\nConsequently, we proposed LLMs workﬂow framework possesses substantial applicability\nand signiﬁcant potential value within the ﬁnancial domain, offering a robust and versatile\nsolution for complex ﬁnancial analysis and decision-making.\n7 Conclusion\nThe multifaceted nature of hybrid tabular-textual QA hinges on its ability to interweave\ndiverse information streams, with numerical reasoning emerging as its core competency, ele-\nvating it beyond mere extractive QA. Recognizing the limitations of prevailing autoregressive\nstrategies, particularly their susceptibility to exposure bias, we introduce ConcurGen, an inno-\nvative non-autoregressive model for program generation in numerical reasoning tasks. This\nframework uniquely champions parallelized program generation, crafting comprehensive\nprogram tuples inclusive of both operators and operands. Unlike its predecessors, ConcurGen\nremains unfazed by exposure bias, magnifying program generation velocity remarkably. In\nthe context of hybrid tabular-textual question answering, answers to open-domain questions\nare equally important. To address this, a framework based on large language models (LLMs)\nis introduced, which simultaneously handles traditional questions while providing high-level\nanalysis and summaries. Empirical evaluations, rooted in the ConvFinQA and MultiHiertt\ndatasets, reveal signiﬁcant ﬁndings: 1) ConcurGen, by virtue of its advanced design, sur-\npasses formidable benchmarks set by FinQANet and MT2Net, redrawing the performance\nboundaries while amplifying program generation velocity by approximately 21 times. 2)\nIntriguingly, as the intricacy of numerical reasoning escalates, our model’s performance\ndegradation remains minimal, contrasting sharply with the autoregressive LSTM decoder\nharnessed by MT2Net. 3) Supplementary experiments demonstrate that the large model han-\ndling HTQA not only provides accurate answers but also offers insightful reasoning and\ncontext-aware explanations, enhancing the overall quality of the responses.\nAuthor Contributions Conceptualization, X.T., Z.Y .; previous work partition, W.Y .,Z.Y . and Y .T.; extension\nmethodology, X.T., L.C, Z.Y . and M.J.; validation, X.T., Z.Y ., L.C., W.Y . and M.J.; experiment Analysis, X.T.,\nL.C., W.Y . and X.S.; resources, Z.Y . and Y .T; data curation, X.T. and Z.Y .; writing-original draft preparation,\nX.T., L.C.; writing-review and editing, X.T., Z.Y . and L.C.; proofreading X.T., Z.Y ., L.C., W.Y ., M.J., X.S.,\nZ.Y . and Y .T.; supervision, Z.Y ., W.Y . and X.S.; project administration, Z.Y . and W.Y . All authors have read\nand agreed to the published version of the manuscript.\n123\n42 Page 26 of 30 World Wide Web (2025) 28 :42\nFunding Not applicable.\nData Availability No datasets were generated or analysed during the current study.\nDeclarations\nEthical Approval Not applicable.\nCompeting interests The authors declare no competing interests.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,\nand indicate if changes were made. The images or other third party material in this article are included in the\narticle’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is\nnot included in the article’s Creative Commons licence and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.\nTo view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\nReferences\n1. Zhu, G., Lin, X., Zhu, K., Zhang, W., Y u, J.X.: Treespan: efﬁciently computing similarity all-matching.\nIn: Proceedings of the 2012 ACM SIGMOD international conference on management of data, pp 529–540\n(2012)\n2. Qin, L., Peng, Y ., Zhang, Y ., Lin, X., Zhang, W., Zhou, J.: Towards bridging theory and practice: hop-\nconstrained st simple path enumeration. In: International conference on very large data bases (2019)\n3. Sun, Q., Lin, X., Zhang, Y ., Zhang, W., Chen, C.: Towards higher-order topological consistency for unsu-\npervised network alignment. In: 2023 IEEE 39th international conference on data engineering (ICDE),\npp 177–190 (2023). https://doi.org/10.1109/ICDE55515.2023.00021\n4. Wu, Y ., Xu, Y ., Zhang, W., Xu, X., Zhang, Y .: Query2gmm: Learning representation with gaussian mixture\nmodel for reasoning over knowledge graphs. In: Proceedings of the ACM Web Conference 2024, WWW\n’24, Association for Computing Machinery, New Y ork, NY , USA, pp 2149–2158 (2024)\n5. Cheema, M.A., Zhang, W., Lin, X., Zhang, Y .: Efﬁciently processing snapshot and continuous reverse k\nnearest neighbors queries. VLDB J. 21, 703–728 (2012)\n6. Rajpurkar, P ., Zhang, J., Lopyrev, K., Liang, P .: SQuAD: 100,000+ questions for machine comprehension\nof text. In: Proceedings of the 2016 conference on empirical methods in natural language processing, pp\n2383–2392 (2016)\n7. Yang, Z., Qi, P ., Zhang, S., Bengio, Y ., Cohen, W., Salakhutdinov, R., Manning, C.D.: HotpotQA: A\ndataset for diverse, explainable multi-hop question answering. In: Proceedings of the 2018 conference on\nempirical methods in natural language processing, pp 2369–2380 (2018)\n8. Dua, D., Wang, Y ., Dasigi, P ., Stanovsky, G., Singh, S., Gardner, M.: DROP: A reading comprehension\nbenchmark requiring discrete reasoning over paragraphs. In: Proceedings of the 2019 conference of the\nnorth american chapter of the association for computational linguistics: Human language technologies,\npp 2368–2378 (2019)\n9. Herzig, J., Nowak, P .K., Müller, T., Piccinno, F., Eisenschlos, J.: TaPas: Weakly supervised table pars-\ning via pre-training. In: Proceedings of the 58th annual meeting of the association for computational\nlinguistics, pp 4320–4333 (2020)\n10. Yang, J., Gupta, A., Upadhyay, S., He, L., Goel, R., Paul, S.: Tableformer: Robust transformer modeling\nfor table-text encoding. In: Proceedings of the 60th annual meeting of the association for computational\nlinguistics, ACL 2022, pp 528–537 (2022)\n11. Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., Wang, W.Y .: HybridQA: A dataset of multi-hop question\nanswering over tabular and textual data. In: Findings of the association for computational linguistics:\nEMNLP 2020, pp 1026–1036 (2020)\n12. Zhu, F., Lei, W., Huang, Y ., Wang, C., Zhang, S., Lv, J., Feng, F., Chua, T.-S.: TA T-QA: A question\nanswering benchmark on a hybrid of tabular and textual content in ﬁnance. In: Proceedings of the 59th\nannual meeting of the association for computational linguistics and the 11th international joint conference\non natural language processing, pp 3277–3287 (2021)\n123\nWorld Wide Web (2025) 28 :42 Page 27 of 30 42\n13. Chen, Z., Chen, W., Smiley, C., Shah, S., Borova, I., Langdon, D., Moussa, R., Beane, M., Huang, T.,\nRoutledge, B.R., Wang, W.Y .: Finqa: A dataset of numerical reasoning over ﬁnancial data. In: Proceedings\nof the 2021 conference on empirical methods in natural language processing, EMNLP 2021, pp 3697–3711\n(2021)\n14. Li, M., Feng, F., Zhang, H., He, X., Zhu, F., Chua, T.: Learning to imagine: Integrating counterfactual\nthinking in neural discrete reasoning. In: Proceedings of the 60th annual meeting of the association for\ncomputational linguistics, ACL 2022, pp 57–69 (2022)\n15. Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., Wang, W.Y .: Convﬁnqa: Exploring the chain of numeri-\ncal reasoning in conversational ﬁnance question answering. In: Proceedings of the 2022 conference on\nempirical methods in natural language processing, EMNLP 2022, pp 6279–6292 (2022)\n16. Zhao, Y ., Li, Y ., Li, C., Zhang, R.: Multihiertt: Numerical reasoning over multi hierarchical tabular and\ntextual data. In: Proceedings of the 60th annual meeting of the association for computational linguistics,\nACL 2022, pp 6588–6600 (2022)\n17. An, Y .L.: Roberta: A robustly optimized BERT pretraining approach. ArXiv preprint arXiv:1907.11692\n(2019)\n18. Chen, L., Han, B., Wang, X., Zhao, J., Yang, W., Yang, Z.: Machine learning methods in weather and\nclimate applications: A survey. Appl. Sci. 13(21) (2023)\n19. He, Z., Wang, X., Wang, R., Shi, S., Tu, Z.: Bridging the data gap between training and inference for\nunsupervised neural machine translation. In: Proceedings of the 60th annual meeting of the association\nfor computational linguistics, ACL 2022, pp 6611–6623 (2022)\n20. Zhang, S., Huang, H., Liu, J., Li, H.: Spelling error correction with soft-masked BERT. In: Proceedings\nof the 58th annual meeting of the association for computational linguistics, pp 882–890 (2020)\n21. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P ., Neelakantan, A., Shyam, P .,\nSastry, G., Askell, A., et al.: Language models are few-shot learners. Adv. Neural Inf. Process. Syst. 33,\n1877–1901 (2020)\n22. Wu, J., Tang, X., Yang, Z., Hao, K., Lai, L., Liu, Y .: An experimental evaluation of & nbsp;llm on&\nnbsp;image classiﬁcation, Springer, Berlin, Heidelberg, pp 506–518 (2024)\n23. Wang, J., Wang, K., Lin, X., Zhang, W., Zhang, Y .: Efﬁcient unsupervised community search with pre-\ntrained graph transformer. Proc. VLDB Endow. 17(9), 2227–2240 (2024)\n24. Yang, W., Yang, Z., Chen, L., Yan, R., Yang, Z., Zhang, L., Tang, Y .: Parallel program generation for\nhybrid tabular-textual question answering. In: Asia-paciﬁc Web (APWeb) and Web-Age information\nmanagement (W AIM) joint international conference on Web and big data, Springer, pp 121–137 (2024)\n25. Hermann, K.M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman, M., Blunsom, P .: Teaching\nmachines to read and comprehend. In: NIPS (2015)\n26. Rajpurkar, P ., Zhang, J., Lopyrev, K., Liang, P .: SQuAD: 100,000+ questions for machine comprehension\nof text. In: Proceedings of the 2016 conference on empirical methods in natural language processing, pp\n2383–2392 (2016)\n27. Berant, J., Chou, A., Frostig, R., Liang, P .: Semantic parsing on Freebase from question-answer pairs. In:\nProceedings of the 2013 conference on empirical methods in natural language processing, pp 1533–1544\n(2013)\n28. Pasupat, P ., Liang, P .: Compositional semantic parsing on semi-structured tables. In: Proceedings of\nthe 53rd annual meeting of the association for computational linguistics and the 7th international joint\nconference on natural language processing, pp 1470–1480 (2015)\n29. Y u, T., Zhang, R., Yang, K., Yasunaga, M., Wang, D., Li, Z., Ma, J., Li, I., Yao, Q., Roman, S., Zhang, Z.,\nRadev, D.: Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing\nand text-to-SQL task. In: Proceedings of the 2018 conference on empirical methods in natural language\nprocessing, pp 3911–3921 (2018)\n30. Wu, Y ., Xu, Y ., Lin, X., Zhang, W.: A holistic approach for answering logical queries on knowledge\ngraphs. In: 2023 IEEE 39th international conference on data engineering (ICDE), pp 2345–2357 (2023).\nhttps://doi.org/10.1109/ICDE55515.2023.00181\n31. Ukey, N., Yang, Z., Li, B., Zhang, G., Hu, Y ., Zhang, W.: Survey on exact knn queries over high-\ndimensional data space. Sensors 23(2) (2023). https://doi.org/10.3390/s23020629\n32. Dua, D., Wang, Y ., Dasigi, P ., Stanovsky, G., Singh, S., Gardner, M.: DROP: A reading comprehension\nbenchmark requiring discrete reasoning over paragraphs. In: Proceedings of the 2019 conference of the\nnorth american chapter of the association for computational linguistics: Human language technologies,\npp 2368–2378 (2019)\n33. Pal, K.K., Baral, C.: Investigating numeracy learning ability of a text-to-text transfer model. In: Findings\nof the association for computational linguistics: EMNLP , pp 3095–3101 (2021)\n34. Ran, Q., Lin, Y ., Li, P ., Zhou, J., Liu, Z.: NumNet: Machine reading comprehension with numerical\nreasoning. In: Proceedings of the 2019 conference on empirical methods in natural language processing\n123\n42 Page 28 of 30 World Wide Web (2025) 28 :42\nand the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp 2474–\n2484 (2019)\n35. Zhang, Q., Wang, L., Y u, S., Wang, S., Wang, Y ., Jiang, J., Lim, E.: NOAHQA: numerical reasoning\nwith interpretable graph question answering dataset. In: Findings of the association for computational\nlinguistics: EMNLP2021, pp 4147–4161 (2021)\n36. Li, X., Sun, Y ., Cheng, G.: Tsqa: Tabular scenario based question answering. Proc. AAAI Conf. Artif.\nIntell. 35(15), 13297–13305 (2021)\n37. Deng, Y ., Lei, W., Zhang, W., Lam, W., Chua, T.: PACIFIC: towards proactive conversational question\nanswering over tabular and textual data in ﬁnance. In: Proceedings of the 2022 conference on empirical\nmethods in natural language processing, EMNLP 2022, pp 6970–6984 (2022)\n38. Geva, M., Gupta, A., Berant, J.: Injecting numerical reasoning skills into language models. In: Proceedings\nof the 58th annual meeting of the association for computational linguistics, pp 946–958 (2020)\n39. Berg-Kirkpatrick, T., Spokoyny, D.: An empirical investigation of contextualized number prediction. In:\nProceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pp\n4754–4764 (2020)\n40. Pi, X., Liu, Q., Chen, B., Ziyadi, M., Lin, Z., Fu, Q., Gao, Y ., Lou, J., Chen, W.: Reasoning like program\nexecutors. In: Proceedings of the 2022 conference on empirical methods in natural language processing,\nEMNLP 2022, pp 761–779 (2022)\n41. Zhuang, L., Wayne, L., Ya, S., Jun, Z.: A robustly optimized BERT pre-training approach with post-\ntraining. In: Li, S., Sun, M., Liu, Y ., Wu, H., Liu, K., Che, W., He, S., Rao, G. (eds.) Proceedings of the\n20th chinese national conference on computational linguistics, pp 1218–1227 (2021)\n42. Guu, K., Lee, K., Tung, Z., Pasupat, P ., Chang, M.: Retrieval augmented language model pre-training. In:\nInternational conference on machine learning, PMLR, pp 3929–3938 (2020)\n43. Liang, Z., Zhang, J., Wang, L., Qin, W., Lan, Y ., Shao, J., Zhang, X.: MWP-BERT: Numeracy-augmented\npre-training for math word problem solving. In: Findings of the association for computational linguistics:\nNAACL 2022, pp 997–1009 (2022)\n44. Mishra, S., Finlayson, M., Lu, P ., Tang, L., Welleck, S., Baral, C., Rajpurohit, T., Tafjord, O., Sabharwal,\nA., Clark, P ., Kalyan, A.: LILA: A uniﬁed benchmark for mathematical reasoning. In: Goldberg, Y .,\nKozareva, Z., Zhang, Y . (eds.) Proceedings of the 2022 conference on empirical methods in natural\nlanguage processing, Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, pp\n5807–5832 (2022)\n45. Clark, P ., Etzioni, O., Khashabi, D., Khot, T., Mishra, B.D., Richardson, K., Sabharwal, A., Schoenick,\nC., Tafjord, O., Tandon, N., Bhakthavatsalam, S., Groeneveld, D., Guerquin, M., Schmitz, M.: From f\nto a on the new york regents science exams — an overview of the aristo project. AI Mag. 41(4), 39–53\n(2020)\n46. Welleck, S., Liu, J., Lu, X., Hajishirzi, H., Choi, Y .: Naturalprover: grounded mathematical proof gener-\nation with language models. In: Proceedings of the 36th international conference on neural information\nprocessing systems. NIPS ’22, Curran Associates Inc., Red Hook, NY , USA (2022)\n47. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y ., Li, W., Liu, P .J.: Exploring\nthe limits of transfer learning with a uniﬁed text-to-text transformer 21, 140–114067 (2020)\n48. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: 7th International conference on\nlearning representations, ICLR 2019 (2019)\n49. Yang, P ., Wang, H., Zhang, Y ., Qin, L., Zhang, W., Lin, X.: T3s: Effective representation learning for tra-\njectory similarity computation. In: 2021 IEEE 37th international conference on data engineering (ICDE),\npp 2183–2188 (2021)\n50. Baldi, P ., Sadowski, P .J.: Understanding dropout. In: Advances in Neural Information Processing Systems\n26: 27th Annual conference on neural information processing systems 2013, pp 2814–2822 (2013)\n51. Chen, W., Zha, H., Chen, Z., Xiong, W., Wang, H., Wang, W.Y .: HybridQA: A dataset of multi-hop question\nanswering over tabular and textual data. In: Findings of the association for computational linguistics:\nEMNLP 2020, pp 1026–1036 (2020)\n52. Luitse, D., Denkena, W.: The great transformer: Examining the role of large language models in the\npolitical economy of ai. Big Data Soc. 8(2), 20539517211047736 (2021)\n53. Naveed, H., Khan, A.U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., Mian, A.: A\ncomprehensive overview of large language models. arXiv preprint arXiv:2307.06435 (2023)\n54. V aswani, A.: Attention is all you need. Adv. Neural Inf. Process, Syst (2017)\n55. Sherstinsky, A.: Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm)\nnetwork. Phys. D Nonlinear Pheno. 404, 132306 (2020)\n56. Y u, Y ., Si, X., Hu, C., Zhang, J.: A review of recurrent neural networks: Lstm cells and network architec-\ntures. Neural Comput. 31(7), 1235–1270 (2019)\n123\nWorld Wide Web (2025) 28 :42 Page 29 of 30 42\n57. Hu, L., Liu, Z., Zhao, Z., Hou, L., Nie, L., Li, J.: A survey of knowledge enhanced pre-trained language\nmodels. IEEE Trans. Knowl, Data Eng (2023)\n58. Floridi, L., Chiriatti, M.: Gpt-3: Its nature, scope, limits, and consequences. Minds Mach. 30, 681–694\n(2020)\n59. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D., Altenschmidt, J.,\nAltman, S., Anadkat, S., et al.: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023)\n60. Zhang, Z., Han, X., Liu, Z., Jiang, X., Sun, M., Liu, Q.: Ernie: Enhanced language representation with\ninformative entities. arXiv preprint arXiv:1905.07129 (2019)\n61. Sun, Y ., Wang, S., Li, Y ., Feng, S., Tian, H., Wu, H., Wang, H.: Ernie 2.0: A continual pre-training\nframework for language understanding. In: Proceedings of the AAAI conference on artiﬁcial intelligence,\nvol 34, pp 8968–8975 (2020)\n62. Sun, Y ., Wang, S., Feng, S., Ding, S., Pang, C., Shang, J., Liu, J., Chen, X., Zhao, Y ., Lu, Y ., et al.: Ernie 3.0:\nLarge-scale knowledge enhanced pre-training for language understanding and generation. arXiv preprint\narXiv:2107.02137 (2021)\n63. Sun, Y ., Wang, S., Li, Y ., Feng, S., Chen, X., Zhang, H., Tian, X., Zhu, D., Tian, H., Wu, H.: Ernie:\nEnhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223 (2019)\n64. Bai, J., Bai, S., Chu, Y ., Cui, Z., Dang, K., Deng, X., Fan, Y ., Ge, W., Han, Y ., Huang, F., et al.: Qwen\ntechnical report. arXiv preprint arXiv:2309.16609 (2023)\n65. Li, Y ., Wang, S., Ding, H., Chen, H.: Large language models in ﬁnance: A survey. In: Proceedings of the\n4th ACM international conference on AI in ﬁnance, pp 374–382 (2023)\n66. Nie, Y ., Kong, Y ., Dong, X., Mulvey, J.M., Poor, H.V ., Wen, Q., Zohren, S.: A survey of large language\nmodels for ﬁnancial applications: Progress, prospects and challenges. arXiv preprint arXiv:2406.11903\n(2024)\n67. Srivastava, P ., Malik, M., Ganu, T.: Assessing llms’ mathematical reasoning in ﬁnancial document question\nanswering. arXiv preprint arXiv:2402.11194 (2024)\n68. Srivastava, P ., Malik, M., Gupta, V ., Ganu, T., Roth, D.: Evaluating llms’ mathematical reasoning in\nﬁnancial document question answering. In: Findings of the association for computational linguistics\nACL 2024, pp 3853–3878 (2024)\n69. Ge, Y ., Hua, W., Mei, K., Tan, J., Xu, S., Li, Z., Zhang, Y ., et al.: Openagi: When llm meets domain\nexperts. Adv. Neural Inf. Process. Syst. 36 (2024)\n70. Wang, Z., Zhong, W., Wang, Y ., Zhu, Q., Mi, F., Wang, B., Shang, L., Jiang, X., Liu, Q.: Data management\nfor large language models: A survey. CoRR (2023)\n71. Yang, R., Tan, T.F., Lu, W., Thirunavukarasu, A.J., Ting, D.S.W., Liu, N.: Large language models in\nhealth care: Development, applications, and challenges. Health Care Sci. 2(4), 255–263 (2023)\n72. Xiao, C., Xu, S.X., Zhang, K., Wang, Y ., Xia, L.: Evaluating reading comprehension exercises generated\nby llms: A showcase of chatgpt in education applications. In: Proceedings of the 18th workshop on\ninnovative use of NLP for building educational applications (BEA 2023), pp 610–625 (2023)\n73. Kolasani, S.: Optimizing natural language processing, large language models (llms) for efﬁcient customer\nservice, and hyper-personalization to enable sustainable growth and revenue. Trans. Latest Trends Artif.\nIntell. 4(4) (2023)\n74. Wang, B., Wang, A., Chen, F., Wang, Y ., Kuo, C.-C.J.: Evaluating word embedding models: Methods and\nexperimental results. APSIPA Trans. Signal Inf. Process. 8, 19 (2019)\n75. Zhang, P ., Xiao, S., Liu, Z., Dou, Z., Nie, J.-Y .: Retrieve anything to augment large language models.\narXiv preprint arXiv:2310.07554 (2023)\n76. Lee, J., Stevens, N., Han, S.C., Song, M.: A survey of large language models in ﬁnance (ﬁnllms). arXiv\npreprint arXiv:2402.02315 (2024)\n77. Clarifai: text-embedding-3-large. https://clarifai.com/openai/embed/models/text-embedding-3-large\n(2024)\n78. Kusupati, A., Bhatt, G., Rege, A., Wallingford, M., Sinha, A., Ramanujan, V ., Howard-Snyder, W., Chen,\nK., Kakade, S., Jain, P ., et al.: Matryoshka representation learning. Adv. Neural Inf. Process. Syst. 35,\n30233–30249 (2022)\n79. Cohere: Announcing Rerank-v3.5. https://docs.cohere.com/changelog/rerank-v3.5 (2024)\n80. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al.: Language models are unsuper-\nvised multitask learners. OpenAI blog 1(8), 9 (2019)\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\n123\n42 Page 30 of 30 World Wide Web (2025) 28 :42\nAuthorsandAﬃliations\nXushuo Tang 1,5 · Liuyi Chen 2 · Wenke Yang 1 · Zhengyi Yang 1 · Mingchen Ju 1 ·\nXin Shu 1,6 · Zihan Yang 3 · Yifu Tang 4\nB Zhengyi Yang\nzhengyi.yang@unsw.edu.au\nXushuo Tang\nxushuo.tang@unsw.edu.au\nLiuyi Chen\nliuyi.chen@hnu.edu.cn\nWenke Yang\nwenke.yang@unsw.edu.au\nMingchen Ju\nmingchen.ju@unsw.edu.au\nXin Shu\nﬁlip.shu@bfjfunds.com.au\nZihan Yang\nzihany1@student.unimelb.edu.au\nYifu Tang\ncraigtang@swin.edu.au\n1 School of Computer Science and Engineering, The University of New South Wales, Sydney 2052,\nNSW, Australia\n2 College of Computer Science and Electronic Engineering, Hunan University, Changsha 10587,\nHunan, China\n3 Faculty of Engineering and Information Technology, The University of Melbourne, Melbourne\n3010, VIC, Australia\n4 School of Science, Computing and Engineering Technologies, Swinburne University of\nTechnology, Melbourne 3122, VIC, Australia\n5 Euler AI Pty Ltd, Sydney 2153, NSW, Australia\n6 BFJ Pty Ltd, Sydney 2113, NSW, Australia\n123",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9014743566513062
    },
    {
      "name": "Question answering",
      "score": 0.693196177482605
    },
    {
      "name": "Natural language processing",
      "score": 0.5659880638122559
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4700450003147125
    },
    {
      "name": "Information retrieval",
      "score": 0.3890998661518097
    },
    {
      "name": "Programming language",
      "score": 0.34741926193237305
    }
  ],
  "institutions": []
}