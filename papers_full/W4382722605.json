{
    "title": "Adversarial Domain Generalized Transformer for Cross-Corpus Speech Emotion Recognition",
    "url": "https://openalex.org/W4382722605",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5026390491",
            "name": "Yuan Gao",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A5101745213",
            "name": "Longbiao Wang",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A5100612858",
            "name": "Jiaxing Liu",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A5017251198",
            "name": "Jianwu Dang",
            "affiliations": [
                "Tianjin University"
            ]
        },
        {
            "id": "https://openalex.org/A5080920610",
            "name": "Shogo Okada",
            "affiliations": [
                "Japan Advanced Institute of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2032254851",
        "https://openalex.org/W2061068689",
        "https://openalex.org/W2161073241",
        "https://openalex.org/W2031998113",
        "https://openalex.org/W1869734671",
        "https://openalex.org/W2969889150",
        "https://openalex.org/W3003908700",
        "https://openalex.org/W3126625480",
        "https://openalex.org/W2806051338",
        "https://openalex.org/W2806649730",
        "https://openalex.org/W4211186029",
        "https://openalex.org/W2125462608",
        "https://openalex.org/W2593768305",
        "https://openalex.org/W2602034649",
        "https://openalex.org/W2883430806",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2995813704",
        "https://openalex.org/W2998115938",
        "https://openalex.org/W2964236337",
        "https://openalex.org/W2993843842",
        "https://openalex.org/W2520774990",
        "https://openalex.org/W2803098682",
        "https://openalex.org/W2936451900",
        "https://openalex.org/W2182205001",
        "https://openalex.org/W3008554267",
        "https://openalex.org/W3137890092",
        "https://openalex.org/W2598545578",
        "https://openalex.org/W2113087918",
        "https://openalex.org/W1608705073",
        "https://openalex.org/W2344608732",
        "https://openalex.org/W2795986449",
        "https://openalex.org/W2972691009",
        "https://openalex.org/W1581984155",
        "https://openalex.org/W3015141382",
        "https://openalex.org/W2614874155",
        "https://openalex.org/W2924116307",
        "https://openalex.org/W2963447013",
        "https://openalex.org/W4225302953",
        "https://openalex.org/W6697274609",
        "https://openalex.org/W2253728219",
        "https://openalex.org/W2585658440",
        "https://openalex.org/W2100495367",
        "https://openalex.org/W2168013545",
        "https://openalex.org/W2405774341",
        "https://openalex.org/W2963087613",
        "https://openalex.org/W2970737019",
        "https://openalex.org/W2146334809",
        "https://openalex.org/W2533262878",
        "https://openalex.org/W2892071465",
        "https://openalex.org/W2342475039",
        "https://openalex.org/W175750906",
        "https://openalex.org/W1494198834",
        "https://openalex.org/W3092085609",
        "https://openalex.org/W6766978945",
        "https://openalex.org/W2980520956",
        "https://openalex.org/W2800126857",
        "https://openalex.org/W2599621350",
        "https://openalex.org/W3015240477",
        "https://openalex.org/W3098571047"
    ],
    "abstract": "Speech emotion recognition (SER) promotes the development of intelligent devices, which enable natural and friendly human-computer interactions. However, the recognition performance of existing approaches is significantly reduced on unseen datasets, and the lack of sufficient training data limits the generalizability of deep learning models. In this work, we analyze the impact of the domain generalization method on cross-corpus SER and propose an adversarial domain generalized transformer (ADoGT), which is aimed at learning a shared feature distribution for the source and target domains. Specifically, we investigate the effect of domain adversarial learning by eliminating nonaffective information. We also combine the center loss with the softmax function as joint supervision to learn discriminative features. Moreover, we introduce unsupervised transfer learning to extract additional features, and incorporate a gated fusion model to learn the complementary information of the features learned by the supervised feature extractor and pretrained model. The proposed transformer based domain generalization method is evaluated using four emotional datasets. We also provide an ablation study of different domain adversarial model structures and feature fusion models. The results of comparative experiments demonstrate the effectiveness of the proposed ADoGT.",
    "full_text": "1\nAdversarial Domain Generalized Transformer for\nCross-Corpus Speech Emotion Recognition\nYuan Gao, Longbiao Wang, Jiaxing Liu, Jianwu Dang, and Shogo Okada ,\nAbstract—Speech emotion recognition (SER) promotes the development of intelligent devices, which enable natural and friendly\nhuman-computer interactions. However, the recognition performance of existing approaches is significantly reduced on unseen\ndatasets, and the lack of sufficient training data limits the generalizability of deep learning models. In this work, we analyze the impact\nof the domain generalization method on cross-corpus SER and propose an adversarial domain generalized transformer (ADoGT),\nwhich is aimed at learning a shared feature distribution for the source and target domains. Specifically, we investigate the effect of\ndomain adversarial learning by eliminating nonaffective information. We also combine the center loss with the softmax function as joint\nsupervision to learn discriminative features. Moreover, we introduce unsupervised transfer learning to extract additional features, and\nincorporate a gated fusion model to learn the complementary information of the features learned by the supervised feature extractor\nand pretrained model. The proposed transformer based domain generalization method is evaluated using four emotional datasets. We\nalso provide an ablation study of different domain adversarial model structures and feature fusion models. The results of comparative\nexperiments demonstrate the effectiveness of the proposed ADoGT.\nIndex Terms—Speech emotion recognition, cross-corpus, adversarial learning, domain generalization.\n✦\n1 I NTRODUCTION\nH\nUMAN -computer interactions have become pervasive\nin our daily lives, and understanding human emo-\ntion is crucial for the development of intelligent devices\n[1]. Therefore, research on sentiment analysis and emotion\nrecognition has attracted increasing attention in both indus-\ntry and academia [2]. Speech emotion recognition (SER) is\naimed at identifying emotional attributes in human speech,\nand a robust SER system can promote the development of\nempathetic chatbots and enrich the manual service of call\ncenters [3]. This research also has other applications, such as\nmonitoring the attention status of students in online courses,\ntracking the emotional state of patients with depression\nand providing advice about their diagnoses [4]. Previous\nstudies designed empirical low-level descriptors (LLDs) for\nemotion classification [5]. In recent years, some researchers\nhave found that deep learning based models such as convo-\nlutional neural networks (CNNs) and recurrent neural net-\nworks (RNNs) show promising results in SER tasks without\nexpert knowledge [6], [7], [8].\nDespite the recent progress in SER research, two bottle-\nnecks limit the recognition accuracy of existing cross-corpus\napproaches. The first bottleneck is the lack of sufficient\nlabeled training data [9]. Compared with other speech signal\nprocessing tasks such as automatic speech recognition, col-\nlecting and annotating speech data with emotional labels in\nnatural environments is time-consuming. The number of ut-\n• Y. Gao, L. Wang, J. Liu, and J Dang are with the Tianjin Key Laboratory\nof Cognitive Computing and Application, College of Intelligence and\nComputing, Tianjin University, Tianjin, China. E-mail:{yuan gao, long-\nbiao wang, jiaxing liu, dangjianwu}@tju.edu.cn (Corresponding author:\nL. Wang)\n• S. Okada is with the School of Information Science, Japan Advanced\nInstitute of Science and Technology (JAIST), Ishikawa, Japan. E-mail:\n{okada-s}@jaist.ac.jp\nterances in most emotional datasets is not sufficient to train\nrobust deep learning models [10]. The second bottleneck is\nhow to extract discriminative features from speech signals.\nAs human emotion is sometimes ambiguous, extracting\noptimal features from acoustic signals requires considerable\nattention [11]. Moreover, in cross-corpus evaluations, the\nemotional information in speech is difficult to learn due to\nvariations in the domain information [12]. Because of this\ndomain divergence, common deep learning models show\npoor performance on unseen datasets [13]. Most of the\nexisting approaches are trained and tested with the same\ndataset, and the performance is significantly reduced on\nunseen datasets [14].\nTo address the lack of annotated emotional data, we\nuse two types of feature extractors: 1) CNNs have shown\npromising performance in extracting emotional discrimi-\nnative features for SER. Thus, we use a deep CNN ar-\nchitecture as the feature extractor in our baseline system\nto learn the spatial information of input utterances. 2)\nWe pretrain an unsupervised convolutional autoencoder to\ntransfer prior knowledge and extract bottleneck features as\nadditional inputs for emotion classification. In this study, to\nimprove the generalizability of the SER system, we propose\nthe adversarial domain generalized Transformer (ADoGT),\nwhich effectively reduces the domain divergence between\nthe training and test data and obtains more effective feature\nrepresentations for each input utterance. Previous studies\nhave identified that emotional information can be lost after\nfeature compression [15]. Our proposed Transformer based\nfeature encoder can retain sufficient emotional information\nfrom different feature distributions and achieve dimension\nreduction through multihead attention [16]. Furthermore,\nwe incorporate a gated fusion model with our feature ex-\ntractor to learn the complementary information in the two\nbranches of the feature extractor. To address domain mis-\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2\nmatch issues, researchers often use the adversarial domain\nadaptation method to transfer the domain representation\nfrom the source domain to the target domain [17]. In this\nstudy, we incorporate a domain adversarial neural network\n(DANN) to eliminate the speaker, corpus, and other domain\ninformation of the latent representation. Domain adaptation\nis achieved by reversing the gradient between the feature\nextractor and the domain classifier [18], which enables\nour model to maximize the training loss of nonaffective\ninformation. Moreover, in previous works, emotion classi-\nfiers commonly used the softmax loss function to identify\ndecision boundaries and separate different emotions [19],\n[20]. We incorporate the center loss [21] with an emotion\nclassifier to learn more cohesive features for SER. Therefore,\nthe proposed model can learn a shared feature distribution\nfor the source and target domains and thus achieve domain\ngeneralization in cross-corpus SER tasks.\nThe main contributions of this paper can be summa-\nrized as follows: (1) We address the domain divergence in\ncross-corpus SER by the proposed domain generalization\nmethod, which combines domain adversarial learning and\ncenter loss to generalize the feature distributions of different\ndomains. (2) Our model incorporates the gated fusion model\nwith the Transformer encoder to effectively combine the\nfeature representation of the supervised and unsupervised\nfeature encoder. (3) We analyze the domain generalization\nperformance in addressing language mismatch issues and\ndifferent elicitation types to meet real-world scenarios. (4)\nWe explore the impact of different DANN subtasks in\nmulti-domain SER and compare the domain classifier in\nthe DANN with multi-task learning classifiers to analyze\nthe effect of domain adversarial learning. The remainder of\nthis paper is organized as follows: We provide a literature\nreview of cross-corpus SER in Section 2. Then, we describe\nthe details of our proposed algorithm in Section 3. The emo-\ntional datasets and experimental settings are presented in\nSection 4. In Section 5, we provide comparative experiments\nto evaluate the effectiveness of our model. We conclude this\npaper and outline our future work in Section 6.\n2 R ELATED WORKS\n2.1 Cross-Corpus Speech Emotion Recognition\nIn real-world scenarios, several paralingual factors impact\nthe acoustic features of speech signals, making it difficult\nfor common machine learning models to learn emotional in-\nformation in speech [22]. The mismatches between different\ndatasets affect the performance of existing SER systems. Do-\nmain mismatch has various causes, including the language,\nrecording conditions, and elicitation methods [12]. Another\nproblem for SER is data sparsity. Since recording and an-\nnotating emotional speech is time consuming, the training\ndata are often not sufficient to build robust SER systems.\nCompared with other speech signal processing tasks such\nas speech recognition, the limited data in SER tasks worsens\nthe domain divergence problem [23]. Moreover, the ground\ntruth cannot be objectively defined since the emotional\nlabels are derived from perceptual evaluations and usually\nvary among annotators. To improve the generalizability of\nSER, researchers have focused on cross-corpus and multi-\ncorpus evaluations [24], [25], [26].\nIn [12], Schuller et al. selected six existing datasets to ex-\nplore the impact of the feature selection strategy and address\ndifferent emotion annotations in cross-corpus SER tasks. To\naddress mismatched acoustic conditions between the train-\ning and test data, the authors investigated several normal-\nization methods, including speaker, corpus, and speaker-\ncorpus normalization. Their experimental results showed\nthat speaker normalization led to the best performance.\nZhang et al. [27] also investigated normalization methods\nand introduced unsupervised learning to handle data spar-\nsity. They proposed that when each corpus is individu-\nally normalized, the introduced normalization layers can\neffectively mitigate the differences among the two datasets.\nOther publications proposed different kinds of support\nvector machine (SVM) structures to address the feature\ndistribution mismatch. Hassan et al. [28] proposed modeling\nthe mismatches as a covariate shift. They employed three\ntransfer learning algorithms that apply importance weights\n(IWs) within an SVM classifier to reduce the effects of co-\nvariate shifts. Abdelwahab et al. [29] investigated adaptive\nand incremental SVMs to reduce the variability in the fea-\nture distribution. Their proposed approaches improved the\nclassification performance, even when only a small portion\nof labeled data was available for adaptation. To generalize\nthe model to unseen languages, Albornoz et al. [30] applied\ndecision-level fusion to improve the recognition accuracy of\nthe SVM classifier. Their system improved the performance\nof the SER system, even when no data in the target language\nwas available to train the model.\nMore recently, researchers analyzed deep learning mod-\nels in cross-corpus SER. In [31], the authors investigated\nthe performance of deep belief networks (DBNs) for cross-\ncorpus SER. They conducted experiments on five emotional\ndatasets and showed that DBNs can learn from many\ntraining languages, showing promising performance in SER\ntasks. Their findings are useful for SER in low-resource\nlanguages. In [32], the authors evaluated CNNs and long\nshort-term memory (LSTM) networks using six different\nspeech emotion corpora. Their results indicated that the\nCNN based model showed better performance on cross-\ncorpus data than the LSTM model. However, since no\nconclusions can be drawn regarding the extent to which\nthe SER system can generalize across different languages,\nresearchers need to focus on cross-language SER.\n2.2 Adversarial Domain Adaptation\nTo address domain divergence in cross-corpus evaluations,\nresearchers have incorporated domain adaptation methods\nto transfer emotional information from the source to the\ntarget domain representation. In [33], Deng et al. proposed\na novel unsupervised domain adaptation method based on\nadaptive denoising autoencoders for affective speech signal\nanalysis. They trained the denoising autoencoder using un-\nlabeled data from the target domain to learn more robust la-\ntent representations. This model effectively and significantly\nenhanced the emotion classification accuracy in mismatched\ntraining and test conditions. In [34], the authors combined\na traditional autoencoder with an adversarial autoencoder\n(AAE) to learn discriminative features from additional data\nand improved the SER performance with only limited la-\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3\nTarget Data\nPre-train Data\nDeep Convolutional \nFeature Encoder\nPre-train Autoencoder \nFeature Encoder\nXc\nXa\nFeature \nFusion \nModel\nDomain \nGeneralization \nModel\nArousal \n(High/Low)\nValence \n(Positive/Negative)\nRepresentation Extraction Adversarial Domain Generalized Transformer (ADoGT) Emotion Classifier\nReconstracting\nFig. 1. Flowchart of the proposed Transformer based SER system. For the input utterances, we extract deep representationsxc and xa from a CNN\nand a pretrained autoencoder, respectively. We combine the features through the proposed adversarial domain generalized Transformer (ADoGT)\nand generalize the feature distribution for emotion classification.\nbeled data. Other researchers proposed eliminating the mis-\nmatch between training and testing samples by learning a\nprojection matrix [35]. These works aimed to transform the\nspeech signals in the source and target domains into a simi-\nlar feature distribution subspace. Gideon et al. [36] proposed\nadversarial discriminative domain generalization (ADDoG)\nto learn shared feature representations for the training and\ntest data. They designed a multi-task learning model, which\ntrain the model with auxiliary tasks and SER simtaneously.\nThey introduced a critic component as the auxiliary task to\nencourage the representations of the different datasets to be\nas close as possible. Their approach outperformed state-of-\nthe-art results in cross-corpus tasks, thus demonstrating the\neffectiveness of the domain adaptation method.\nTo achieve domain adaptation and the main classifi-\ncation task simultaneously, Ganin et al. [18] proposed a\ndomain adversarial neural network (DANN) that can be\ntrained using standard backpropagation algorithms. Their\nproposed structure includes a standard feedforward net-\nwork and a domain classifier connected to the feature ex-\ntractor through a gradient reversal layer. This layer reverses\nthe sign of the gradient during backpropagation, thus en-\nsuring that the feature distributions of the two domains are\nindistinguishable. To build a robust SER system that can not\nonly generalize across speaker information but also other\ndomain information, Abdelwahab et al. [37] proposed ap-\nplying domain adversarial learning and extracting common\nrepresentations between the training and test domains. They\nused domain adversarial learning to extract discriminative\nfeature representations that leveraged unlabeled data in the\ntarget dataset and reduced the mismatch between the source\nand target domains. Their experiments demonstrate that\nadversarial learning leads to significant improvements in\nthe performance of SER classifiers in which the model is\ntrained with only labeled data from the source domain.\nThis training strategy is aimed at mitigating the influence\nof nonaffective information. In [38], we incorporated adver-\nsarial domain adaptation and eliminated the influence of\nspeaker and corpus information. However, previous works\nhave mainly focused on acted speech. To investigate the per-\nformance of our proposed domain generalization method\non improvised speech, we include spontaneous data in our\nmodel evaluation. In this study, we also present a compre-\nhensive ablation study on DANN subtasks in multi-corpus\nconditions. Furthermore, we compare the performance of\nthe domain classifiers in the DANN and multi-task learning\nstructure to verify whether domain adversarial learning can\nmake the domain information unlearned to the model.\n2.3 Transfer Learning in Affective Computing\nTransfer learning is aimed at transferring prior knowledge\nfrom different but related source domains to the target\ndomain. Previous publications [39] showed that pretraining\nrepresentations can effectively improve the robustness and\nuncertainty of deep learning models. In previous research\non affective computing, Ng et al. [40] used a large image\ndataset to pretrain a CNN based architecture and conducted\nexperiments with two kinds of fine-tuning schemes. The\nexperimental results showed that their model obtained sig-\nnificant improvements over the baseline in facial emotion\nrecognition tasks. Kaya et al. [41] combined the pretrained\nvisual geometry group (VGG) model with a common feature\nextractor to learn the visual features and then fused these\nfeatures with the audio features at the decision level to\nrealize multimodal emotion recognition. Their experiments\nshowed that the pretrained model can extract rich features\nand shows significant improvements over the baseline fea-\ntures. To mitigate the problem of data sparsity in SER,\nresearchers have also investigated several unsupervised\ntransfer learning approaches to transfer prior knowledge\nfrom additional datasets. Various publications have shown\nthat autoencoders [42] obtain good performance on image\nreconstruction tasks and have become widely used in many\nfields [43], [44]. To extract latent representations for emis-\nsion recognition, previous studies introduced pretrained\nautoencoders to extract additional features from unlabeled\nspeech data. These studies evaluated different kinds of au-\ntoencoder structures for transferring emotional information\nfrom the data utilized for automatic speech recognition [45].\nTheir models show consistent improvements over baselines\nwith the representations generated by different autoencoder\nmodels. In previous works, the features extracted from\nthe pretrained model and feature extractor were usually\nconcatenated to improve the SER performance. In this work,\nwe investigate the impact of a pretrained model in cross-\ncorpus SER and focus on the complementary information of\nthese two kinds of features.\n3 A DVERSARIAL DOMAIN GENERALIZATION\nIn this section, we describe the overall structure of our\napproach. As shown in Figure 1, the proposed method is\ntrained in two steps: 1) Representation encoding. We use\ntwo branches of feature extractors for feature encoding: a\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4\nFig. 2. Our proposed model consists of two parts: (1) Feature fusion. We use a gated fusion model to combine the features learned from the CNN\nmodel and pretrained autoencoder. (2) Domain generalization. We modify the emotion classifier as a multi-task DANN network to reduce the domain\ndivergence and combine the center loss with the softmax function for joint supervision. In this Figure,xc and xa represent the features learned from\nthe supervised CNN model and unsupervised autoencoder model, respectively. After the Transformer encoder layers, these features are denoted\nas Rc and Ra, respectively. We use LE and LD to represent the emotion and domain classification cross-entropy loss functions, and LC denotes\nthe center loss function.\nsupervised CNN architecture and an unsupervised CNN\nautoencoder. To improve the performance of cross-corpus\nSER, we combine the output features through our proposed\nTransformer based gated fusion model. Then, the LSTM\nlayers are used to learn the temporal information. 2) Domain\ngeneralization. To reduce the domain divergence of different\ndatasets, we use domain adversarial learning to eliminate\nthe domain information. Furthermore, we incorporate the\ncenter loss to obtain more compact intraclass variations for\nthe same emotion. Finally, we use a linear layer with a\nfeature size of 2 for arousal and valence classification.\n3.1 Representation Encoding\nLearning discriminative features is essential for recognizing\nemotions. We extract the spectrogram of the emotional ut-\nterances as the input to our model. The data preprocessing\ntechniques are described in more detail in Section 4.2.\n3.1.1 Supervised Feature Encoder\nPrevious publications have shown that CNNs can infer\nhierarchical representations of input utterances that facili-\ntate emotion categorization. As the SER baseline, we use\n2D convolutional layers followed by max-pooling layers to\nlearn the spatial information, and then the output features\nare flattened. For the target data u = [ u1, u2, ..., un], we\nextract the deep representation xc = [xc\n1, xc\n2, ..., xc\nn] ∈ Rdc×n\nfrom the CNN, and ye = [y1, y2, ..., yn] ∈ Rc×n represents\nthe emotion labels. We extract a dc-dimensional feature for\nn utterances, where c is the number of emotions.\n3.1.2 Unsupervised Feature Encoder\nIn this work, we use unsupervised transfer learning to\nmitigate the problem of data sparsity in SER. Specifically,\nwe incorporate a convolutional autoencoder ( AE), which\nis the most optimized structure for feature modeling, as\nthe pretrained component of the proposed SER system. The\nAE model is pretrained using unlabeled data up, and the\nobjective function is defined as:\nLae = argmin ||up − AE(up)||2 (1)\nThen, we fine-tune the AE model using unlabeled tar-\nget data u. The output features of the encoder model\nare flattened, and we use the latent representation xa =\n[xa\n1, xa\n2, ..., xa\nn] ∈ Rda×n as additional input, where da is the\ndimension of xa.\n3.2 Feature Fusion\nAs shown in Figure 2, we propose an transformer based\ngated fusion model to learn the complementary information\nlearned from CNN and autoencoder. Attention mechanism\nallows a neural network to capture the emotionally salient\nparts of an input sequence. For xc and xa learned from\nthe CNN and autoencoder, we first use two multihead\nattention branches in the Transformer encoder to reduce\nthe dimension and prevent information loss during feature\ncompression. The attention score is calculated as follows:\nQi = x(c,a) ∗ WQ\ni (2)\nKi = x(c,a) ∗ WK\ni (3)\nVi = x(c,a) ∗ WV\ni (4)\nheadi = Softmax (QKT\n√\nd\n)V (5)\nwhere WQ\ni ∈ Rdq×dx , WK\ni ∈ Rdq×dx , and WV\ni ∈ Rdq×dx are\ntrainable parameter matrices for the attention projections.\nThe final output of the attention layer is generated by\nconcatenating each headi as follows:\nx(c,a)\nM = Concat(head1, head2, ..., headn) (6)\nThe head number is 8. The outputs of the multihead atten-\ntion mechanism are xc\nM and xa\nM . We use a fully connected\nlayer to ensure that their dimensions are equal to those of\nxa and xc. To reduce the redundant information of features\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5\nlearned from the same utterance, we propose a gated fusion\nmodel to learn the complementary information of the deep\nCNN architecture and pretrained model. Our proposed\nfusion model is described as follows:\nG = σ(Wg[xc\nM , xa\nM ] + bg) (7)\nx = Concat(f(G ⊙ xc\nM ), f((1 − G) ⊙ xa\nM )) (8)\nwhere Wg and bg are the trainable parameters and bias,\nrespectively. σ is the sigmoid activation function, which\nlearns the contributions of the input features, and G denotes\nthe gate vector, which ranges from 0 to 1. In Equation (8),\nf is the activation function, and ⊙ represents an elementary\nproduct. The gate vector G controls the contributions of the\ninputs xc\nM and xa\nM by multiplying the corresponding input\nfeatures and producing filtered representations. Through\nour modified Transformer based fusion model, we can re-\nduce the irrelevant information in the SER task and learn\nthe emotionally salient parts of the input sequence.\n3.3 Domain Generalization\nThe domain divergence among different datasets has a sig-\nnificantly effect on the feature learning process in the cross-\ncorpus SER system. To improve the SER performance on un-\nseen datasets, we need to generalize the feature distributions\nof the different datasets. In this work, we introduce domain\nadversarial learning to eliminate nonaffective information\nand combine the center loss with an emotion classifier to\nreduce the intraclass distances of features learned from the\nsame emotion.\n3.3.1 Domain Adversarial Learning\nIn cross-corpus SER, the domain information, including the\nspeaker information, recording conditions, and elicitation\nmethod, significantly decreases the recognition performance\nof deep learning based models. To reduce the domain diver-\ngence among different datasets, we incorporate domain ad-\nversarial learning to eliminate the nonaffective information.\nTo achieve domain adaptation and feature representation\nlearning with one training process, Ganin et al. introduced a\ngradient reversal layer (GRL) between the domain classifier\nand the feature extractor. The GRL multiplies the gradient\nof the domain classification task by a negative constant γ.\nIn this work, we follow their algorithm and incorporate\nthe supervised emotion classification ( Le) and unsuper-\nvised domain classification ( Ld) as recognition targets, and\nunlabeled data in the target corpus are used to train the\nmodel. During backpropagation, the domain classifier Ld\nis trained to make the feature distributions learned from\nthe source and target domains indistinguishable to our\nmodel. Through the GRL, we can extract domain invariant\nrepresentations and thus improve the model generalizability\nfor cross-corpus SER. The overall objective function of our\nproposed classification model is defined as:\nL = Le(x, ye) + γLd(x, yd) (9)\nwhere Le is the loss function of the emotion classifier, which\ncombines the center loss and softmax loss. More details on\nLe are provided in Section 3.2.2. Our model can reduce the\ndomain shifts of the feature distributions learned from the\nsource and target datasets with unsupervised domain adap-\ntation for the source and target data. In this specific task,\nby incorporating this training strategy with our supervised\nfeature extraction model, the domain-invariant features can\nretain discriminative information for emotion classification.\nThe loss function of the domain classifier is defined as:\nLd = Ld1 (x, yd1 ) + Ld2 (x, yd2 ) + ... + Ldn (x, ydn ) (10)\nwhere n is the number of domain classifiers and ydi =\n[y1, y2, ..., yn] ∈ Rcdi ×n represents the corresponding labels.\nWe explore different DANN subtasks to determine the opti-\nmal model structure for SER. By identifying a saddle point\nthat minimizes Le and maximizes Ld, our proposed feature\nextractor can reduce the domain divergence and learn better\nconvergent features.\n3.3.2 Center Loss\nThe center loss is combined with the emotion classifier to\nreduce the intraclass distance, and we incorporate the soft-\nmax loss and center loss as joint supervision for the emotion\nclassifier Le. The softmax loss function is commonly used\nin SER systems for identifying decision boundaries between\ndifferent emotions [46]. In this study, although we define the\nsame emotion annotations for the training and test samples,\nthe feature distributions of different datasets are difficult\nto separate. This situation makes cross-corpus SER more\nchallenging than common closed-set identification tasks. To\nmitigate this problem, we introduce the center loss to learn\nthe class center c for each emotion category, thus reducing\nthe intraclass distances in the feature distribution. This loss\nfunction is calculated as the Euclidean distance between the\ninput feature and the corresponding class center.\nCenter(x, c) = 1\nM\nNX\ni=1\n||x(i,yj) − cyj ||2 (11)\n∆cyj =\nPMyj\ni=1 (cyj − x(i,yj))\n1 + m (12)\nwhere M and Myj are the total number of mini-batches and\nthe jth emotion category in the batch, respectively. N is the\nnumber of emotion classes. The new class center is updated\nby ∆cyj , which is trained for every mini-batch. The overall\nobjective function of the emotion classifier is defined as:\nLE(x, ye) = λSoftmax (x, ye) + (1− λ)Center(x, c) (13)\nWe set λ to 0.7 to control the weight of each loss term. By\ncombining the center loss with the softmax loss to jointly\noptimize our model, we can extract more robust feature\nrepresentations that generalize across datasets.\n4 E XPERIMENTAL SETUP\nFour emotional datasets are used to evaluate the general-\nizability of our model: IEMOCAP , MSP-IMPROV , EMODB,\nand FAU-AIBO. All the datasets are publicly available. The\ndatasets cover different languages and elicitation methods\nand are thus valuable for evaluating our model. We first\npresent the main attributes of each dataset and the emotion\nlabels in this study (Table 1). Then, we introduce two well-\nknown unlabeled datasets, which are used to pretrain the\nautoencoder. This section also includes the data preprocess-\ning techniques and model configuration.\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6\nTABLE 1\nOverview of the four emotion corpora. For IEMOCAP , the elicitation type contains both acted and natural, and the lexical content contains both\nscripted and improvised.\nCorpus Language #m #f Rate Type Content Total\nValence Arousal\nNegative Positive Low High\nIEMOCAP English 5 5 16 kHz Hybrid Hybrid 5531 3344 2187 2792 2739\nMSP-Improv English 6 6 44.1 kHz Acted Fixed 8438 4546 3892 3660 4778\nEmo-DB German 5 5 16 kHz Acted Fixed 535 385 150 268 267\nFau-aibo German 30 21 16 kHz Natural Spontaneous 18216 5093 13123 15835 2318\nTABLE 2\nEmotion mapping from discrete labels to binary arousal\nDatasets Low High\nIEMOCAP Neutral, Sad Angry, Happy\nMSP-IMPROV Neutral, Sad Angry, Happy\nEMODB Bordorm, Disgust,\nNeutral, Sad Angry, Happy, Fear\nFAU-AIBO Neutral, Rest,\nEmphatic Angry, Joy\n4.1 Datasets\nThe IEMOCAP dataset: The Interactive Emotional Dyadic\nMotion Capture database [47] contains 12 hours of audio-\nvisual data, including audio, video, and facial motion in-\nformation, and textual transcriptions from 10 speakers. The\naudio was recorded using two high-quality microphones\nwith a 48 kHz sampling rate and then downsampled to 16\nkHz. In each session, one male and one female performed\na series of scripts or improvisational scenarios. For each\nspeech utterance, three annotators assigned the categorical\nlabels. We used 5531 utterances from the scripted and im-\nprovised audio data for our experiments. We implemented\nthe common practice of merging “happy” and “excited” into\none emotion class “happy” [45], [48], [49]; thus, the emotion\nlabels in this dataset are happy, sad, angry, and neutral.\nThe MSP-IMPROV dataset: MSP-IMPROV [50] is a\nmultimodal emotional database that includes recordings\nof actors interacting in dyadic sessions. The actors aim to\ncontrol the lexical content of each sentence while displaying\nnatural emotional expressions. The corpus consists of 8,438\nutterances (8.9 hours) of emotional sentences recorded from\n12 actors. The audio data of each actor was recorded with\na collar microphone with a 48 kHz sampling rate and then\ndownsampled to 44.1 kHz. All the audio data were grouped\ninto six sessions, and each session has one male and one\nfemale actor. The categorical labels were collected using\ncrowdsourcing on Amazon Mechanical Turk. The emotion\ncategories in this dataset are happy, sad, angry, and neutral.\nThe MSP-IMPROV dataset: MSP-IMPROV [50] is a mul-\ntimodal emotional database recorded from actors interacting\nin dyadic sessions. The author aims to control the lexical\ncontent of each sentence while promoting the naturalness of\nemotion expression. The corpus consists of 8,438 utterances\n(8.9 hours) of emotional sentences recorded from 12 actors.\nEach actor used a collar microphone to record speech at\nTABLE 3\nEmotion mapping from discrete labels to binary valence\nDatasets Negative Positive\nIEMOCAP Angry,Sad Happy,Neutral\nMSP-IMPROV Angry,Sad Happy,Neutral\nEMODB Angry,Bordorm,\nDisgust,Fear,Sad\nHappy,Neutral\nFAU-AIBO Angry,Emphatic Neutral,Joy,Rest\n48 kHz and then downsampled it to 44.1 kHz. All the\naudio data are grouped into six sessions and each session\nhas one male and one female actor. The categorical labels\nare collected using crowdsourcing on Amazon Mechanical\nTurk. The emotion categories in this dataset are also happy,\nsad, angry, and neutral.\nThe EMODB dataset: The Berlin Emotional Speech\ndatabase [51] includes data from ten professional actors\nobtained in a recording environment. The spoken content\nincludes 10 predefined emotionally neutral sentences in\nGerman, and the actors were asked to express each sen-\ntence in seven emotional states (neutral, boredom, disgust,\nsadness, anger, happiness, and fear). The categorical labels\nwere collected according to the intended emotional state.\nThis corpus contains a total of 535 utterances, which had an\nagreement rate higher than 84.3% in a listening experiment\nwith 20 participants (10 male and 10 female).\nThe FAU-AIBO dataset: The FAU Aibo Emotion Cor-\npus [52] was recorded to collect spontaneous audio data\nwith sufficient emotion expression. This dataset contains\nspontaneous recordings of 51 children interacting with the\nSony robot Aibo. Thirty female and 21 male pupils were\ninstructed to talk with Aibo, and then five experts annotated\nthe recorded speech according to predefined emotion cate-\ngories. We choose 18216 utterances used for the Interspeech\nEmotion Challenge, including five emotion categories (an-\ngry, emphatic, neutral, joyful, and rest).\nThe annotated labels of these datasets are inconsistent\nin this study, and we chose arousal and valence to gen-\nerate more interpretable emotion classification categories.\nAlthough IEMOCAP and MSP-IMPROV have continuous\nlabels for arousal and valence, to maintain consistency with\nother datasets, we followed Schuller et al. [12] and mapped\nthe discrete emotion labels to binary arousal and valence\n(Tables 2 and 3). In this work, we choose Librispeech [53]\nand MUST-C DE to pretrain the autoencoder [54]. Lib-\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7\nrispeech is commonly used in speaker identification and\nautomatic speech recognition tasks; it contains 1000 hours of\nEnglish speech read from audiobooks. We selected the 360-\nhour subset with high-quality recording conditions. MUST-\nC consists of audio, transcriptions and translations of En-\nglish TED talks. We used the MUST-C DE subset, which\ncontains 408 hours of German translations from English\nTED Talks.\n4.2 Model Configuration\nDuring data preprocessing, to match the sampling rate, all\nthe datasets are downsampled to 16 kHz . We use a 256-\nlength Hamming window with 128 overlaps to calculate the\naudio spectrogram of the input features using MATLAB. For\nthe variable-length inputs, we define the maximum length\nof the time dimension as 700. Spectrograms with shorter\nlengths are padded with zeros to the fixed length, and\nthe redundant parts are masked during training. After a\nshort Fourier transform, the time × frequency of the input\nspectrogram is calculated as 700 × 129. Our experiments\nare implemented using PyTorch [55]. To ensure consistency\nwith the baseline, we use three CNN layers followed by\nmax-pooling as the supervised feature extractor in all com-\nparative experiments. In experiments with the pretrained\nmodel, we use five convolutional layers as the encoder\nand the corresponding five deconvolutional layers as the\ndecoder. Moreover, we use the output of the encoder as the\nlatent representation. The learned features of the supervised\nand unsupervised feature extractors are then flattened and\nfed into the Transformer encoder layers and gated fusion\nmodel. Next, we use two bidirectional LSTM layers with\n128 units to learn the sequence information, and a dropout\nlayer with a 0.5 dropout rate is used to prevent overfitting.\nThe LSTM output is fed into a fully connected layer with a\nsoftmax function for classification. We employed Adadelta\nas the optimizer, and the mini-batch size was set to 128.\nTo ensure that SER was the training objective of the total\nloss function, the weight parameter of each domain classifier\nranged from 0 to 0.5. For the pretrained model, we pretrain\nthe autoencoder model and then fine-tune this model using\nunlabeled data from the source domain. During feature\nextraction, we maintain a fixed weight and bias and use\nthe target data to extract the bottleneck features.\nFor multi-corpus experiments, all four datasets are com-\nbined. We split the data into a training set (80%) and a test\nset (20%). The models are evaluated using test data from\neach corpus. Note that there is no speaker overlap between\nthe training and testing data. For cross-corpus experiments,\nno labeled data from the target corpus are used for training.\n5 E XPERIMENTS AND EVALUATIONS\nIn this study, we design several experiments to evaluate\nour proposed approach. First, we investigate the effect of\ndifferent domain adaptation subtasks in Section 5.1. Then,\nwe compare the proposed model with the CNN-LSTM\nbaseline in Section 5.2. To determine the impact of each\ncomponent on the overall system, ablation studies for the\nproposed domain generalization method and Transformer\nbased gated fusion network are presented in Sections 5.3\nTABLE 4\nMulti-corpus evaluation results for learning the impact of domain\nadaptation methods. In DANN, we choose the domain classifier with\nbest performance. In DG, we combine center loss with softmax loss\nfunction as joint supervision.\nArousal Valence\nModel CNN DANN DG CNN DANN DG\nIEM 73.28 78.56 78.35 70.38 73.09 75.19\nMSP 60.80 64.37 65.71 60.46 62.58 62.70\nEMO 90.21 93.74 92.58 62.74 65.73 67.24\nFAU 53.77 55.81 55.21 60.31 63.74 61.62\nAvg. 69.51 73.12 72.96 63.47 66.29 66.84\nand 5.4. Finally, we compare the performance of the domain\nclassifier in the DANN and multi-task learning model in\nSection 5.5. In this study, we use the unweighted accuracy\n(UA) as evaluation measure, which can avoid the influence\nof data imbalence in each emotion.\n5.1 Experiment 1: Multi-corpus Evaluation\nAs introduced in Section 4.2, all four datasets are used\nin this multi-corpus evaluation. In this section, only the\ndeep CNN model is used as the feature extractor. We also\nevaluate the performance of the DANN and proposed do-\nmain generalization (DG) method, which combines the the\ncenter loss and softmax as joint supervision for SER. The\nDANN and CNN are compared in Table 4, and the results\nshow that the domain adversarial learning method learns\nmore discriminative features in both arousal and valence\nrecognition. Thus, incorporating the center loss and DANN\nfor domain generalization leads to promising performance\nwith these four datasets, especially in valence recognition.\nTo better understand the domain adversarial learning\nmethod, we design four types of DANN subtasks. We\nhypothesize that different domain recognition targets can\nbenefit the SER system when the corresponding factors lead\nto domain divergence in the feature extraction process. We\nconduct a multi-corpus evaluation to explore the effective-\nness of different domain classifiers on specific datasets. As\nthe training data contain domain information from all four\ndatasets, the results can intuitively reflect the effect of each\nDANN structure in certain testing conditions.\nWe present seven experiments to explore the impact of\nthe speaker, gender, language, and elicitation type. There\nare two kinds of structures in this experiment: a DANN\nwith one domain classifier branch ( D1) and a DANN with\ntwo domain classifier branches ( D2). Previous studies on\nSER have demonstrated that the speaker information has\na significant influence on the classification results. If the\nspeakers in the training and test data overlap, the SER\nmodel shows better performance than a model trained with\na speaker-independent validation strategy. Therefore, the\nfirst subexperiment with D1 uses the speaker as the domain\nclassifier. Since the gender and language information can\nreflect the speaker information, in D1, we also train domain\nclassifiers for language and gender classification. In addition\nto the speaker information, the elicitation strategy (acted\nor spontaneous) has a great impact on the performance\nof SER. Therefore, an elicitation type classifier (type) is\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8\nTABLE 5\nMulti-corpus evaluation results for learning the impact of different domain classifiers. In this table, D1 has one branch of the domain classifier. In\nD2 models, we add type as an additional domain learning target to D1.\nTest\nArousal Valence\ntype\ngender language speaker\ntype\ngender language speaker\nD1 D2 D1 D2 D1 D2 D1 D2 D1 D2 D1 D2\nIEM 76.23 78.56 77.85 76.45 76.3 73.62 75.28 73.09 71.63 70.86 71.86 72.76 70.53 71.25\nMSP 63.09 61.84 64.37 62.13 63.19 61.54 62.93 61.53 62.58 61.35 61.11 61.97 58.18 60.13\nEMO 93.74 91.58 91.35 91.2 92.52 88.77 91.26 64.16 62.41 63.42 63.92 65.73 61.35 62.27\nFAU 53.31 53.64 55.81 54.64 55.26 53.8 52.95 62.38 62.07 62.37 62.15 63.74 61.28 60.74\nAvg. 71.59 71.66 72.35 71.11 71.82 69.43 70.61 65.29 64.67 64.50 64.76 66.05 62.84 63.60\nalso included. In D2, we combine the speaker, gender, and\nlanguage classifiers with the type classifier.\nAs shown in Table 5, for both arousal and valence\nrecognition, the language (D2) classifiers show the highest\naverage recognition performance. To reflect the advantages\nof domain adaptation methods, we use language and type\nclassifiers in the DANN model in the following experiments.\nFor each dataset, the best DANN structure includes type,\ngender (D2) and language (D2). These results indicate the\neffectiveness of domain adversarial learning based on the\nelicitation type. In previous studies, speaker and gender\nclassification were commonly employed in multi-task learn-\ning models. However, these two classifiers cannot realize\nthe best performance in domain adaptation methods. This\nfinding may indicate that eliminating the corresponding\ninformation does not benefit SER system.\n5.2 Experiment 2: Cross-corpus Evaluation\nThen, we utilize the following experiments to investigate\nthe effectiveness of the proposed approach in cross-corpus\nevaluation. CNN-LSTM models have been utilized in many\nprevious SER publications, and we choose this model as\nthe baseline. Then, we compare the performance of the\nproposed ADoGT approach with that of the baseline system.\nOur proposed DG model can generate a common feature\nsubspace for different domains by combining the DANN\nand center loss. Furthermore, we use a pretrained autoen-\ncoder as an additional branch in our proposed model and\napply the proposed Transformer based model for feature\nfusion. For each model, we provide 13 experimental results\nfor evaluation (training with one dataset and testing with\nthe other datasets and the average performance).\nTable 6 shows the overall cross-corpus evaluation results.\nThe results suggest that both models realize better per-\nformance on arousal recognition tasks, especially in cross-\nlingual experiments (IEMOCAP and EMODB). These results\nindicate that arousal information is easier to learn for deep\nlearning models than valence information, which is consis-\ntent with [27], [56]. Compared with the baseline system,\nthe experimental results demonstrate the advantages of the\nproposed generalization method and pretrained model in\ncross-corpus SER. We observe that for speech recorded in\nlaboratory environments, the proposed Transformer based\ndomain generalization model significantly improved recog-\nnition performance (e.g. more than 6% for both arousal and\nvalence recognition when train on IEMOCAP and test on\nEMODB). However, due to the poor performance of both\nTABLE 6\nCross-corpus evaluation results for analysing the effectiveness of\nproposed domain generalization method\ntrain on test on\nArousal Valence\nCNN DoGAT CNN DoGAT\nIEM\nMSP 57.29 61.83 55.73 59.42\nEMO 67.3 73.53 52.48 58.73\nFAU 52.02 52.06 56.15 60.41\nMSP\nIEM 61.78 63.75 56.81 57.12\nEMO 55.37 59.82 54.52 58.15\nFAU 54.88 55.67 52.01 55.87\nEMO\nIEM 64.35 68.23 53.30 58.07\nMSP 52.33 55.49 50.32 50.37\nFAU 51.62 51.58 54.04 59.49\nFAU\nIEM 52.74 55.29 52.77 53.02\nMSP 52.61 57.38 50.28 50.00\nEMO 54.69 56.75 57.46 59.46\nModel Avg. 56.42 59.28 53.82 56.68\napproaches in certain experiments (e.g., training on FAU-\nAIBO and testing on other datasets, valence recognition\nbetween MSP-IMPROV and EMODB), the improvement in\nthe average performance is not significant. In the following\nexperiments, we investigate the impact of each component\nin the proposed approach.\n5.3 Experiment 3: Impact of Domain Generalization\nWe hypothesize that the domain generalization method\neffectively reduces the domain divergence among differ-\nent datasets. To compare the proposed domain generaliza-\ntion model with the baseline system, we conduct cross-\ncorpus experiments using IEMOCAP with MSP-IMPROV\nand EMODB. For both models, we use only a supervised\nfeature extraction model with no additional feature inputs\nand apply consistent CNN and LSTM hyperparameters. We\nrepeat this experiment five times and report the mean and\nstandard deviation.\nThe results of the comparisons are presented in Figure 3.\nWe use gender and type and language and type as domain\nclassifiers for monolingual and cross-lingual experiments,\nrespectively. We observe that for both arousal and valence\nclassification, our proposed domain generalization method\noutperforms the single-task learning baseline. This finding\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9\n(a) IEMOCAP to EMODB\n (b) EMODB to IEMOCAP\n (c) IEMOCAP to FAU-AIBO\n (d) FAU-AIBO to IEMOCAP\nFig. 3. Impact of proposed domain generalization model. We use elicitation type and language as domain classifiers for IEMOCAP and FAU-AIBO\nexperiments, IEMOCAP and EMODB experiments, respectively.\n0 100 200 300 40060\n65\n70\n75\n80\n85\n90UA(%)\nCNN\nDG\n(a) IEMOCAP to EMODB (a)\n0 100 200 300 40045\n50\n55\n60\n65\n70\n75UA(%)\nCNN\nDG (b) IEMOCAP to EMODB (v)\n0 1session 2session 3session 4session55\n60\n65\n70\n75\n80\n85UA(%)\nCNN\nDG (c) EMODB to IEMOCAP (a)\n0 1session 2session 3session 4session45\n50\n55\n60\n65\n70\n75UA(%)\nCNN\nDG (d) EMODB to IEMOCAP (v)\nFig. 4. Cross-corpus experimental results of IEMOCAP and EMODB. This experiment includes an increasing number of labelled data from the\ntarget dataset. In (a) and (c), a is the arousal recognition results. In (b) and (d), v is the valence recognition results.\nindicates that the domain adaptation task can be adopted\nfor the SER system. Moreover, introducing the center loss\neffectively improves the generalizability of the feature rep-\nresentation. Interestingly, we observe that when the model\nis trained with IEMOCAP and tested with FAU-AIBO, the\nvalence recognition performance is better than the arousal\nrecognition performance. We assume that the main reasons\nfor this result are that this dataset was recorded in daily\nenvironments and the speakers did not realize that they\nwere recording emotional speech. Therefore, the activation\nstates of most utterances are lower than those of the ut-\nterances in the other corpora. Furthermore, both models\nachieve better recognition performance in the cross-corpus\nexperiments with IEMOCAP and EMODB than IEMOCAP\nand FAU-AIBO. This result potentially indicates the diffi-\nculty of SER with improvised (spontaneous) data. In this\nexperiment, we also incorporate some of the labeled data in\nthe target dataset to further evaluate our model. As depicted\nin Figure 4, our model can use the target data to learn\nshared feature representations for different domains and\nachieve better performance. This Figure also demonstrates\nthe effectiveness of our model in multi-corpus evaluation,\nwhere most of the labeled data in the target corpus are used\nfor training.\n5.4 Experiment 4: Impact of Feature Fusion Model\nThe Transformer based gated fusion model can prevent\ninformation loss during dimension reduction and learn\nTABLE 7\nThe comparasion of different feature fusion methods. The model is\ntrained on IEMOCAP , we present the within-corpus results (left hand)\nand cross-corpus results (right hand).\nModel\nIEMOCAP MSP\nArousal Valence Arousal Valence\nCNN 75.66 70.58 57.29 55.73\nConcatenate 77.86 72.43 58.27 56.24\nTransformer C 78.35 74.25 59.05 57.83\nTGFM 79.60 74.73 60.46 58.81\nthe complementary information of the input features. In\nthis experiment, we use IEMOCAP and MSP-IMPROV to\nevaluate the effectiveness of the proposed method. We\npresent both within-corpus experiments (80% of the data\nin IEMOCAP for training and 20% for testing) and cross-\ncorpus experiments (training with IEMOCAP and testing\nwith MSP-IMPROV). The input model includes a super-\nvised CNN/DANN and a pretrained AE for additional\nfeature extraction. Note that only softmax loss function for\nemotion classification is used, and domain generalization\nmethods is not included in this experiment. We compare the\nconcatenation method and Transformer based feature fusion\nmodel, and the results are presented in Table 7.\nA comparison of the concatenation based model and\nCNN baseline results shows that using a large amount of\nunlabeled data to pretrain the autoencoder can improve the\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10\n0 25 50 75 100 125 150 175 200\nepoches\n30\n40\n50\n60\n70\n80\n90\n100Accuracy\nDANN multi-task learning chance\n(a)\n0 25 50 75 100 125 150 175 200\nepoches\n30\n40\n50\n60\n70\n80\n90\n100Accuracy\nDANN multi-task learning chance (b)\nFig. 5. Comparison of the domain classifier in the multi-task learning model and domain adversarial neural network with the same model structure.\nWe define the weight parameter of the auxiliary task as 0.01 in (a) and 0.1 in (b).\nperformance of SER, which is consistent with the conclu-\nsions of previous studies [23], [57]. In both the within-corpus\nand cross-corpus evaluations, our proposed Transformer\nbased gated fusion model (TGFM) outperforms the concate-\nnation based method by more than 1.74%. Compared with\nthe CNN baseline, we observe that including the Trans-\nformer model improves the results of the model with the\nconcatenation layer for feature fusion (Transformer C). This\nfinding demonstrates the discriminability of the attention\nmechanism for learning emotional information.\n5.5 Experiment 5: Comparasion of Domain Classifier\nTo elucidate the effects of domain adversarial learning,\nwe focus on the performance of the domain classifier\nin the DANN and multi-task learning model to investi-\ngate the effect of the gradient reversal layer. In previous\nstudies, researchers incorporated the recognition of other\nspeaker attributes to obtain rich transcriptions, and their\nexperiments demonstrated the effectiveness of this training\nscheme. Among all paralinguistic information and emotion\nattributes, the impact of gender recognition has been ana-\nlyzed most often in the literature [58], [59]. In this experi-\nment, both models are trained with IEMOCAP and tested\nwith MSP-IMPROV , and we choose gender classification as\nthe domain classifier. To understand the effect of the GRL,\nwe record the model accuracy with the training set, and the\nweight of the domain classifier is defined as 0.01 and 0.1.\nAs depicted in Figure 5 (a) and (b), in the multi-task\nlearning model, the recognition performance of the gender\nclassifier is significantly higher than that of the DANN\nmodel. This performance gap verifies the effectiveness of\ndomain adversarial learning in reducing the domain di-\nvergence among different datasets. This experiment also\nsuggests that when the weight parameter of the domain\nclassifier is greater than 0.1, domain classification maintains\nthe chance level and thus makes the features from different\ndomains indistinguishable.\n6 C ONCLUSION AND FUTURE WORK\nThis study addresses domain divergence in cross-corpus\nSER by incorporating domain adversarial learning to feature\nextraction model and jointly training the emotion classifier\nwith center loss and softmax loss function. This study in-\ntroduced an novel Transformer based gated fusion model\nto retain emotional information during feature compres-\nsion and learns the contributions of features learned from\nthe pretrained model and supervised feature extractor. To\nmeet the need in real-world scenarios, this study evaluated\nthe proposed adversarial domain generalized Transformer\n(ADoGT) in two languages (English and German) and two\nelicitation types (spontaneous and acted). To verify the\nimpact of domain adversarial learning, this study provide\nthe comparison of domain classifier in domain adversarial\nneural network (DANN) and multi-task learning.\nExperimental results demonstrate that our proposed\nmodel improves the average recognition performance by\n2.86% in the cross-corpus condition. To learn the influence\nof each domain factor on the SER, we present the results of\nmulti-corpus experiments using the DANN with different\ndomain recognition targets. Compared with the baseline\nmodel, the proposed domain generalization model obtains\nbetter recognition performance by reducing the influence\nof domain divergence. Moreover, ablation studies show the\neffectiveness of the Transformer based gated fusion model\nin feature-level fusion tasks. Compared with the concatena-\ntion based model, our approach utilizes the complementary\ninformation in features learned from the same utterance and\nthus prevents information loss.\nThis study mainly focuses on DANN and the center loss\nto reduce the domain divergence and address interdomain\nvariations. Both training methods aim to learn discrimi-\nnative features for SER and make the nonaffective infor-\nmation indistinguishable to the model. Other publications\nhave shown thatmulti-task learning can benefit SER tasks\nby achieving rich transcriptions. These studies define the\nattribute factor as a subtask and share the information across\ntasks to promote the SER. Combining DANN and multi-\ntask learning can potentially provide further insight into\nhow other information influences the SER performance in\ncertain scenarios. Given the many learning factors, this topic\nrequires continuous attention from researchers in affective\ncomputing. In the future, we plan to investigate the optimal\nauxiliary recognition target of these two approaches.\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11\nREFERENCES\n[1] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias,\nW. Fellenz, and J. G. Taylor, “Emotion recognition in human-\ncomputer interaction,” IEEE Signal processing magazine , vol. 18,\nno. 1, pp. 32–80, 2001.\n[2] S. G. Koolagudi and K. S. Rao, “Emotion recognition from speech:\na review,” International journal of speech technology , vol. 15, no. 2,\npp. 99–117, 2012.\n[3] S. Ramakrishnan and I. M. El Emary, “Speech emotion recognition\napproaches in human computer interaction,” Telecommunication\nSystems, vol. 52, no. 3, pp. 1467–1478, 2013.\n[4] R. Feldman, “Techniques and applications for sentiment analysis,”\nCommunications of the ACM, vol. 56, no. 4, pp. 82–89, 2013.\n[5] M. Wimmer, B. Schuller, D. Arsic, B. Radig, and G. Rigoll, “Low-\nlevel fusion of audio and video feature for multi-modal emotion\nrecognition,” in Proc. 3rd Int. Conf. on Computer Vision Theory and\nApplications VISAPP , Funchal, Madeira, Portugal, 2008, pp. 145–151.\n[6] R. A. Khalil, E. Jones, M. I. Babar, T. Jan, M. H. Zafar, and\nT. Alhussain, “Speech emotion recognition using deep learning\ntechniques: A review,” IEEE Access , vol. 7, pp. 117 327–117 345,\n2019.\n[7] J. Zhang, Z. Yin, P . Chen, and S. Nichele, “Emotion recognition us-\ning multi-modal data and machine learning techniques: A tutorial\nand review,” Information Fusion, vol. 59, pp. 103–126, 2020.\n[8] B. J. Abbaschian, D. Sierra-Sosa, and A. Elmaghraby, “Deep learn-\ning techniques for speech emotion recognition, from databases to\nmodels,” Sensors, vol. 21, no. 4, p. 1249, 2021.\n[9] Y. Zhang, J. Du, Z. Wang, J. Zhang, and Y. Tu, “Attention based\nfully convolutional network for speech emotion recognition,” in\n2018 Asia-Pacific Signal and Information Processing Association An-\nnual Summit and Conference (APSIP A ASC). IEEE, 2018, pp. 1771–\n1775.\n[10] M. Abdelwahab and C. Busso, “Study of dense network ap-\nproaches for speech emotion recognition,” in 2018 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2018, pp. 5084–5088.\n[11] B. Liu, “Sentiment analysis and opinion mining,” Synthesis lectures\non human language technologies, vol. 5, no. 1, pp. 1–167, 2012.\n[12] B. Schuller, B. Vlasenko, F. Eyben, M. W ¨ollmer, A. Stuhlsatz,\nA. Wendemuth, and G. Rigoll, “Cross-corpus acoustic emotion\nrecognition: Variances and strategies,” IEEE Transactions on Affec-\ntive Computing, vol. 1, no. 2, pp. 119–131, 2010.\n[13] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, “Adversarial\ndiscriminative domain adaptation,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 7167–\n7176.\n[14] H. M. Fayek, M. Lech, and L. Cavedon, “Evaluating deep learning\narchitectures for speech emotion recognition,” Neural Networks ,\nvol. 92, pp. 60–68, 2017.\n[15] D. Nguyen, K. Nguyen, S. Sridharan, D. Dean, and C. Fookes,\n“Deep spatio-temporal feature fusion with compact bilinear pool-\ning for multimodal emotion recognition,” Computer Vision and\nImage Understanding, vol. 174, pp. 33–42, 2018.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems , vol. 30, 2017.\n[17] S. Latif, J. Qadir, and M. Bilal, “Unsupervised adversarial domain\nadaptation for cross-lingual speech emotion recognition,” in 2019\n8th International Conference on Affective Computing and Intelligent\nInteraction (ACII). IEEE, 2019, pp. 732–737.\n[18] Y. Ganin, E. Ustinova, H. Ajakan, P . Germain, H. Larochelle,\nF. Laviolette, M. Marchand, and V . Lempitsky, “Domain-\nadversarial training of neural networks,” The journal of machine\nlearning research, vol. 17, no. 1, pp. 2096–2030, 2016.\n[19] L. Zhang, S. Wang, and B. Liu, “Deep learning for sentiment\nanalysis: A survey,” Wiley Interdisciplinary Reviews: Data Mining\nand Knowledge Discovery, vol. 8, no. 4, p. e1253, 2018.\n[20] A. Yadav and D. K. Vishwakarma, “Sentiment analysis using\ndeep learning architectures: a review,”Artificial Intelligence Review,\nvol. 53, no. 6, pp. 4335–4385, 2020.\n[21] Y. Wen, K. Zhang, Z. Li, and Y. Qiao, “A discriminative feature\nlearning approach for deep face recognition,” in European confer-\nence on computer vision. Springer, 2016, pp. 499–515.\n[22] B. W. Schuller, “Speech emotion recognition: Two decades in a\nnutshell, benchmarks, and ongoing trends,” Communications of the\nACM, vol. 61, no. 5, pp. 90–99, 2018.\n[23] M. Neumann and N. T. Vu, “Improving speech emotion recog-\nnition with unsupervised representation learning on unlabeled\nspeech,” in ICASSP 2019-2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) . IEEE, 2019, pp.\n7390–7394.\n[24] S. M. Feraru, D. Schuller et al., “Cross-language acoustic emotion\nrecognition: An overview and some tendencies,” in 2015 Interna-\ntional Conference on Affective Computing and Intelligent Interaction\n(ACII). IEEE, 2015, pp. 125–131.\n[25] R. Milner, M. A. Jalal, R. W. Ng, and T. Hain, “A cross-corpus study\non speech emotion recognition,” in 2019 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU) . IEEE, 2019, pp.\n304–311.\n[26] M. Gerczuk, S. Amiriparian, S. Ottl, and B. W. Schuller, “Emonet:\nA transfer learning framework for multi-corpus speech emotion\nrecognition,” IEEE Transactions on Affective Computing, 2021.\n[27] B. Zhang, E. M. Provost, and G. Essl, “Cross-corpus acoustic\nemotion recognition with multi-task learning: Seeking common\nground while preserving differences,” IEEE Transactions on Affec-\ntive Computing, vol. 10, no. 1, pp. 85–99, 2017.\n[28] A. Hassan, R. Damper, and M. Niranjan, “On acoustic emotion\nrecognition: compensating for covariate shift,” IEEE Transactions\non Audio, Speech, and Language Processing , vol. 21, no. 7, pp. 1458–\n1468, 2013.\n[29] M. Abdelwahab and C. Busso, “Supervised domain adaptation\nfor emotion recognition from speech,” in 2015 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2015, pp. 5058–5062.\n[30] E. M. Albornoz and D. H. Milone, “Emotion recognition in never-\nseen languages using a novel ensemble method with emotion\nprofiles,” IEEE Transactions on Affective Computing , vol. 8, no. 1,\npp. 43–53, 2015.\n[31] S. Latif, R. Rana, S. Younis, J. Qadir, and J. Epps, “Transfer\nlearning for improving speech emotion classification accuracy,”\narXiv preprint arXiv:1801.06353, 2018.\n[32] J. Parry, D. Palaz, G. Clarke, P . Lecomte, R. Mead, M. Berger, and\nG. Hofer, “Analysis of deep learning architectures for cross-corpus\nspeech emotion recognition.” in INTERSPEECH, 2019, pp. 1656–\n1660.\n[33] J. Deng, Z. Zhang, F. Eyben, and B. Schuller, “Autoencoder-based\nunsupervised domain adaptation for speech emotion recognition,”\nIEEE Signal Processing Letters, vol. 21, no. 9, pp. 1068–1072, 2014.\n[34] S. Latif, R. Rana, S. Khalifa, R. Jurdak, J. Epps, and B. W. Schuller,\n“Multi-task semi-supervised adversarial autoencoding for speech\nemotion recognition,” IEEE Transactions on Affective Computing ,\n2020.\n[35] P . Song, “Transfer linear subspace learning for cross-corpus speech\nemotion recognition.” IEEE Trans. Affect. Comput., vol. 10, no. 2, pp.\n265–275, 2019.\n[36] J. Gideon, M. G. McInnis, and E. M. Provost, “Improving cross-\ncorpus speech emotion recognition with adversarial discrimina-\ntive domain generalization (addog),” IEEE Transactions on Affective\nComputing, vol. 12, no. 4, pp. 1055–1068, 2019.\n[37] M. Abdelwahab and C. Busso, “Domain adversarial for acoustic\nemotion recognition,” IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, vol. 26, no. 12, pp. 2423–2435, 2018.\n[38] Y. Gao, S. Okada, L. Wang, J. Liu, and J. Dang, “Domain-invariant\nfeature learning for cross corpus speech emotion recognition,” in\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2022, pp. 6427–6431.\n[39] Y. Bengio, “Deep learning of representations for unsupervised and\ntransfer learning,” in Proceedings of ICML workshop on unsupervised\nand transfer learning . JMLR Workshop and Conference Proceed-\nings, 2012, pp. 17–36.\n[40] H.-W. Ng, V . D. Nguyen, V . Vonikakis, and S. Winkler, “Deep\nlearning for emotion recognition on small datasets using transfer\nlearning,” in Proceedings of the 2015 ACM on international conference\non multimodal interaction, 2015, pp. 443–449.\n[41] H. Kaya, F. G ¨urpınar, and A. A. Salah, “Video-based emotion\nrecognition in the wild using deep transfer learning and score\nfusion,” Image and Vision Computing, vol. 65, pp. 66–75, 2017.\n[42] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimension-\nality of data with neural networks,” science, vol. 313, no. 5786, pp.\n504–507, 2006.\n[43] L. Deng, M. L. Seltzer, D. Yu, A. Acero, A.-r. Mohamed, and\nG. Hinton, “Binary coding of speech spectrograms using a deep\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12\nauto-encoder,” in Eleventh Annual Conference of the International\nSpeech Communication Association. Citeseer, 2010.\n[44] X. Lu, Y. Tsao, S. Matsuda, and C. Hori, “Speech enhancement\nbased on deep denoising autoencoder.” in Interspeech, vol. 2013,\n2013, pp. 436–440.\n[45] S. Latif, R. Rana, J. Qadir, and J. Epps, “Variational autoencoders\nfor learning latent representations of speech emotion: A prelimi-\nnary study,” arXiv preprint arXiv:1712.08708, 2017.\n[46] H. Meng, T. Yan, F. Yuan, and H. Wei, “Speech emotion recognition\nfrom 3d log-mel spectrograms with deep learning network,” IEEE\naccess, vol. 7, pp. 125 868–125 881, 2019.\n[47] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive\nemotional dyadic motion capture database,”Language resources and\nevaluation, vol. 42, no. 4, pp. 335–359, 2008.\n[48] V . Rozgi ´c, S. Ananthakrishnan, S. Saleem, R. Kumar, and\nR. Prasad, “Ensemble of svm trees for multimodal emotion recog-\nnition,” in Proceedings of the 2012 Asia Pacific signal and information\nprocessing association annual summit and conference. IEEE, 2012, pp.\n1–4.\n[49] P . Tzirakis, J. Zhang, and B. W. Schuller, “End-to-end speech\nemotion recognition using deep neural networks,” in 2018 IEEE\ninternational conference on acoustics, speech and signal processing\n(ICASSP). IEEE, 2018, pp. 5089–5093.\n[50] C. Busso, S. Parthasarathy, A. Burmania, M. AbdelWahab,\nN. Sadoughi, and E. M. Provost, “Msp-improv: An acted corpus\nof dyadic interactions to study emotion perception,” IEEE Transac-\ntions on Affective Computing, vol. 8, no. 1, pp. 67–80, 2016.\n[51] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, B. Weiss\net al. , “A database of german emotional speech.” in Interspeech,\nvol. 5, 2005, pp. 1517–1520.\n[52] S. Steidl, Automatic classification of emotion related user states in\nspontaneous children’s speech. Logos-Verlag Berlin, Germany, 2009.\n[53] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech:\nan asr corpus based on public domain audio books,” in 2015\nIEEE international conference on acoustics, speech and signal processing\n(ICASSP). IEEE, 2015, pp. 5206–5210.\n[54] R. Cattoni, M. A. Di Gangi, L. Bentivogli, M. Negri, and M. Turchi,\n“Must-c: A multilingual corpus for end-to-end speech transla-\ntion,” Computer Speech & Language, vol. 66, p. 101155, 2021.\n[55] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An im-\nperative style, high-performance deep learning library,” Advances\nin neural information processing systems, vol. 32, 2019.\n[56] S. Zhang, X. Zhao, and Q. Tian, “Spontaneous speech emotion\nrecognition using multiscale deep convolutional lstm,”IEEE Trans-\nactions on Affective Computing, 2019.\n[57] S. E. Eskimez, Z. Duan, and W. Heinzelman, “Unsupervised learn-\ning approach to feature analysis for automatic speech emotion\nrecognition,” in 2018 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2018, pp. 5099–5103.\n[58] Z.-Q. Wang and I. Tashev, “Learning utterance-level representa-\ntions for speech emotion and age/gender recognition using deep\nneural networks,” in 2017 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP). IEEE, 2017, pp. 5150–5154.\n[59] A. Nediyanchath, P . Paramasivam, and P . Yenigalla, “Multi-head\nattention for speech emotion recognition with auxiliary learning\nof gender recognition,” in ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2020, pp. 7179–7183.\nYuan Gao is currently pursuing a Ph.D. degree\nat the Department of Intelligence Science and\nTechnology, Kyoto University, Kyoto, Japan. He\nreceived an M.S. degree from both Tianjin Uni-\nversity, Tianjin, China, and the Japan Advanced\nInstitute of Science and Technology, Ishikawa,\nJapan, in 2022. His research interests include\nspeech signal processing and multimodal emo-\ntion recognition.\nLongbiao Wang received his Dr.Eng. degree\nfrom Toyohashi University of Technology, Japan,\nin 2008. He was an Assistant Professor in the\nfaculty of Engineering at Shizuoka University,\nJapan, from April 2008 to September 2012. He\nwas an Associate Professor at Nagaoka Univer-\nsity of Technology, Japan from Oct. 2012 to Aug.\n2016. He is currently a Professor, Director of\nTianjin Key Laboratory of Cognitive Computing\nand Application and vice Dean of School of Arti-\nficial Intelligence at Tianjin University, China. His\nresearch interests include robust speech recognition, speaker recog-\nnition, acoustic signal processing and affective computing. He is a\nmember of IEEE.\nJiaxing Liu He is currently pursuing the PhD\ndegree with the College of Intelligence and Com-\nputing, Tianjin University, Tianjin, China. Now is\nthe an exchanging student in Nanyang Techno-\nlogical University. His research interests include\nspeech emotion recognition, multimodal emotion\nrecognition, and natural language processing\n(sentiment analysis).\nJianwu Dang Prof. Jianwu Dang graduated from\nTsinghua Univ., China, in 1982, and got his\nM.S. degree at the same university in 1984.\nHe worked for Tianjin Univ. as a lecture from\n1984 to 1988. He was awarded the PhD de-\ngree from Shizuoka Univ., Japan in 1992. He\nworked for ATR Human Information Processing\nLabs., Japan, as a senior researcher from 1992\nto 2001. He joined the University of Waterloo,\nCanada, as a visiting scholar for one year from\n1998. He worked for Japan Advanced Institute\nof Science and Technology (JAIST) as a professor from 2001-2022. He\njoined the Institute of Communication Parlee (ICP), Center of National\nResearch Scientific, France, as a research scientist the first class from\n2002 to 2003. Since 2009, he has joined Tianjin University, Tianjin,\nChina. His research interests are in all the fields of speech science in-\ncluding speech signal processing, disorder speech and speech cognitive\nfunctions.\nShogo Okada directs the Social Signal and In-\nteraction Group at the Japan Advanced Institute\nof Science and Technology (JAIST) in Japan and\nis an associate professor at JAIST. He obtained\nhis Ph.D. in 2008 from Tokyo Institute of Tech-\nnology in Japan. In 2008 and 2011, he joined\nthe Kyoto University, Tokyo Institute of Technol-\nogy, as an assistant professor. He joined IDIAP\nResearch Institute in Switzerland as a visiting\nfaculty member in 2014. His research interests\ninclude social signal processing, human dynam-\nics, multimodal interaction, and machine learning. He is a member of\nIEEE and ACM .\nThis article has been accepted for publication in IEEE Transactions on Affective Computing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/TAFFC.2023.3290795\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}