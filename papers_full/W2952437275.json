{
  "title": "Exploring Pre-trained Language Models for Event Extraction and Generation",
  "url": "https://openalex.org/W2952437275",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2095688776",
      "name": "Sen Yang",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096396853",
      "name": "Dawei Feng",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095838241",
      "name": "linbo qiao",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2131913313",
      "name": "Zhigang Kan",
      "affiliations": [
        "National University of Defense Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2096918252",
      "name": "Dongsheng Li",
      "affiliations": [
        "National University of Defense Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1996787131",
    "https://openalex.org/W2739918945",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963018920",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2407338347",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2475245295",
    "https://openalex.org/W2512522169",
    "https://openalex.org/W4302343710",
    "https://openalex.org/W2108743083",
    "https://openalex.org/W2130714105",
    "https://openalex.org/W2104012281",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2250999640",
    "https://openalex.org/W2403680433",
    "https://openalex.org/W2618285232",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2803884531",
    "https://openalex.org/W2165962657",
    "https://openalex.org/W2508618307",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2963473975",
    "https://openalex.org/W2250575108",
    "https://openalex.org/W2514651853",
    "https://openalex.org/W2270070752",
    "https://openalex.org/W2963756346"
  ],
  "abstract": "Traditional approaches to the task of ACE event extraction usually depend on manually annotated data, which is often laborious to create and limited in size. Therefore, in addition to the difficulty of event extraction itself, insufficient training data hinders the learning process as well. To promote event extraction, we first propose an event extraction model to overcome the roles overlap problem by separating the argument prediction in terms of roles. Moreover, to address the problem of insufficient training data, we propose a method to automatically generate labeled data by editing prototypes and screen out generated samples by ranking the quality. Experiments on the ACE2005 dataset demonstrate that our extraction model can surpass most existing extraction methods. Besides, incorporating our generation method exhibits further significant improvement. It obtains new state-of-the-art results on the event extraction task, including pushing the F1 score of trigger classification to 81.1%, and the F1 score of argument classification to 58.9%.",
  "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5284–5294\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n5284\nExploring Pre-trained Language Models for Event Extraction and\nGeneration\nSen Yang†, Dawei Feng†, Linbo Qiao, Zhigang Kan, Dongsheng Li‡\nNational University of Defense Technology, Changsha, China\n{sen yang,linbo.qiao,kanzhigang13}@nudt.edu.cn\ndavyfeng.c@gmail.com, lds1201@163.com\nAbstract\nTraditional approaches to the task of ACE\nevent extraction usually depend on manually\nannotated data, which is often laborious to cre-\nate and limited in size. Therefore, in addi-\ntion to the difﬁculty of event extraction itself,\ninsufﬁcient training data hinders the learning\nprocess as well. To promote event extraction,\nwe ﬁrst propose an event extraction model to\novercome the roles overlap problem by sep-\narating the argument prediction in terms of\nroles. Moreover, to address the problem of in-\nsufﬁcient training data, we propose a method\nto automatically generate labeled data by edit-\ning prototypes and screen out generated sam-\nples by ranking the quality. Experiments on\nthe ACE2005 dataset demonstrate that our ex-\ntraction model can surpass most existing ex-\ntraction methods. Besides, incorporating our\ngeneration method exhibits further signiﬁcant\nimprovement. It obtains new state-of-the-art\nresults on the event extraction task, including\npushing the F1 score of trigger classiﬁcation to\n81.1%, and the F1 score of argument classiﬁ-\ncation to 58.9%.\n1 Introduction\nEvent extraction is a key and challenging task for\nmany NLP applications. It targets to detect event\ntrigger and arguments. Figure 1 illustrates a sen-\ntence containing an event of type Meet triggered\nby ”meeting”, with two arguments: ” President\nBush” and ”several Arab leaders”, both of which\nplay the role ”Entity”.\nThere are two interesting issues in event ex-\ntraction that require more efforts. On the one\nhand, roles in an event vary greatly in frequency\n(Figure 2), and they can overlap on some words,\n†These two authors contributed equally.\n‡Corresponding Author.\n[Trigger]\nEvent type: Meet\nSentence  : President Bush           is going to be meeting \n                   with several Arab leaders \n[Entity]\n[Entity]\nFigure 1: An event of type Meet is highlighted in the\nsentence, including one trigger and two arguments.\neven sharing the same argument ( the roles over-\nlap problem). For example, in sentence ”The\nexplosion killed the bomber and three shoppers” ,\n”killed” triggers an Attack event, while argument\n”the bomber” plays the role ”Attacker” as well\nas the role ”Victim” at the same time. There are\nabout 10% events in the ACE2005 dataset (Dod-\ndington et al., 2004) having the roles overlap prob-\nlem. However, despite the evidence of the roles\noverlap problem, few attentions have been paid to\nit. On the contrary, it is often simpliﬁed in evalu-\nation settings of many approaches. For example,\nin most previous works, if an argument plays mul-\ntiple roles in an event simultaneously, the model\nclassiﬁes correctly as long as the prediction hits\nany one of them, which is obviously far from ac-\ncurate to apply to the real world. Therefore, we\ndesign an effective mechanism to solve this prob-\nlem and adopt more rigorous evaluation criteria in\nexperiments.\nOn the other hand, so far most deep learn-\ning based methods for event extraction follow the\nsupervised-learning paradigm, which requires lots\nof labeled data for training. However, annotating\naccurately large amounts of data is a very labo-\nrious task. To alleviate the suffering of existing\nmethods from the deﬁciency of predeﬁned event\ndata, event generation approaches are often used\nto produce additional events for training (Yang\net al., 2018; Zeng et al., 2018; Chen et al., 2017).\nAnd distant supervision (Mintz et al., 2009) is a\ncommonly used technique to this end for label-\ning external corpus. But the quality and quantity\n5285\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\nVictim Place Agent Instrument Time\nFrequency\nFigure 2: Frequency of roles that appear in events of\ntype Injure in the ACE2005 dataset.\nof events generated with distant supervision are\nhighly dependent on the source data. In fact, ex-\nternal corpus can also be exploited by pre-trained\nlanguage models to generate sentences. Therefore,\nwe turn to pre-trained language models, attempt-\ning to leverage their knowledge learned from the\nlarge-scale corpus for event generation.\nSpeciﬁcally, this paper proposes a framework\nbased on pre-trained language models, which in-\ncludes an event extraction model as our baseline\nand a labeled event generation method. Our pro-\nposed event extraction model is constituted of a\ntrigger extractor and an argument extractor which\nrefers result of the former for inference. In addi-\ntion, we improve the performance of the argument\nextractor by re-weighting the loss function based\non the importance of roles.\nPre-trained language models have also been ap-\nplied to generating labeled data. Inspired by the\nwork of Guu et al. (2018), we take the existing\nsamples as prototypes for event generation, which\ncontains two key steps: argument replacement and\nadjunct token rewriting. Through scoring the qual-\nity of generated samples, we can pick out those\nof high quality. Incorporating them with existing\ndata can further improve the performance of our\nevent extractor.\n2 Related work\nEvent ExtractionIn terms of analysis granularity,\nthere are document-level event extraction (Yang\net al., 2018) and sentence-level event extraction\n(Zeng et al., 2018). We focus on the statistical\nmethods of the latter in this paper. These meth-\nods can be further divided into two detailed cat-\negories: the feature based ones (Liao and Grish-\nman, 2010; Liu et al., 2010; Miwa et al., 2009; Liu\net al., 2016; Hong et al., 2011; Li et al., 2013b)\nwhich track designed features for extraction, and\nthe neural based ones that take advantage of neu-\nral networks to learn features automatically (Chen\net al., 2015; Nguyen and Grishman, 2015; Feng\net al., 2016).\nEvent Generation External resources such as\nFreebase, Frame-Net and WordNet are commonly\nemployed to generate event and enrich the train-\ning data. Several previous event generation ap-\nproaches (Chen et al., 2017; Zeng et al., 2018)\nbase a strong assumption in distant supervision 1\nto label events in unsupervised corpus. But in fact,\nco-occurring entities could have none expected re-\nlationship. In addition, Huang et al. (2016) incor-\nporates abstract meaning representation and distri-\nbution semantics to extract events. While Liu et al.\n(2016, 2017) manages to mine additional events\nfrom the frames in FrameNet.\nPre-trained Language ModelPre-trained lan-\nguage models are capable of capturing the mean-\ning of words dynamically in consideration of their\ncontext. McCann et al. (2017) exploits language\nmodel pre-trained on supervised translation corpus\nin the target task. ELMO (Embeddings from Lan-\nguage Models) (Peters et al., 2018) gets context\nsensitive embeddings by encoding characters with\nstacked bidirectional LSTM (Long Short Term\nMemory) and residual structure (He et al., 2016).\nHoward and Ruder (2018) obtains comparable re-\nsult on text classiﬁcation. GPT (Generative Pre-\nTraining) (Radford et al., 2018) improves the state\nof the art in 9 of 12 tasks. BERT (Bidirectional\nEncoder Representations from Transformers) (De-\nvlin et al., 2018) breaks records of 11 NLP task\nand received a lot of attention.\n3 Extraction Model\nThis section describes our approach to extract\nevents that occur in plain text. We consider event\nextraction as a two-stage task, which includes trig-\nger extraction and argument extraction, and pro-\npose a Pre-trained Language Model based Event\nExtractor (PLMEE). Figure 3 illustrates the archi-\ntecture of PLMEE. It consists of a trigger extractor\nand an argument extractor, both of which rely on\nthe feature representation of BERT.\n3.1 Trigger Extractor\nTrigger extractor targets to predict whether a token\ntriggers an event. So we formulate trigger extrac-\ntion as a token-level classiﬁcation task with labels\n1If two entities have a relationship in a knowledge base,\nthen all sentences that mention these two entities will express\nthat relationship.\n5286\nkilled\nexplosion\nthe\nbomber\nand\nthree\nshoppers\nBERT\nEmbedding\nClassifier\nConflict.Attack\nTrigger \nThe explosion killed the bomber and three shoppers \nBERT\nEmbedding\nAttacker Victim Place\nCstart Cend Cstart Cend Cstart Cend Cstart Cend\n...\n0 1 0 1 0 1 0 1 0 1 0 10 1 0 1\nArgument\nLoss\nClassifier Set\nAttacker\nVictim\nPlace... Role \nImportance\nkilled\nexplosion\nthe\nbomber\nand\nthree\nshoppers\nFor inference\nThe explosion killed the bomber and three shoppers \nWordPiece Segment Position WordPiece Segment Position\nThe\n The\nFigure 3: Illustration of the PLMEE architecture, including a trigger extractor and an argument extractor. The\nprocessing procedure of an event instance triggered by the word ”killed” is also shown.\nbeing event types, and just add a multi-classiﬁer\non BERT to build the trigger extractor.\nThe input of the trigger extractor follows the\nBERT, i.e. the sum of three types of embed-\ndings, including WordPiece embedding (Wu et al.,\n2016), position embedding and segment embed-\nding. Since the input contains only one sentence,\nall its segment ids are set to zero. In addition, to-\nken [CLS] and [SEP] 2 are placed at the start and\nend of the sentence.\nIn many cases, the trigger is a phrase. There-\nfore, we treat consecutive tokens which share the\nsame predicted label as a whole trigger. As gen-\neral, we adopt cross entropy as the loss function\nfor ﬁne-tuning.\n3.2 Argument Extractor\nGiven the trigger, argument extractor aims to ex-\ntract related arguments and all roles they play.\nCompared with trigger extraction, argument ex-\ntraction is more complicated because of three is-\nsues: the dependency of arguments on the trigger,\nmost arguments being long noun phrases, and the\nroles overlap problem. We take exactly a series of\nactions to deal with these obstacles.\nIn common with trigger extractor, argument ex-\ntractor requires three kinds of embeddings as well.\nHowever, it needs to know which tokens comprise\nthe trigger. Therefore, we feed argument extractor\nwith the segment ids of trigger tokens being one.\n2[CLS], [SEP] and [MASK] are special tokens of BERT.\nTo overcome the latter two issues in argument\nextraction, we add multiple sets of binary classi-\nﬁers on the BERT. Each set of classiﬁers sever for\na role to determine the spans (each span includes a\nstart and an end) of all arguments that play it. This\napproach is similar to the question answering task\non the SQuAD (Rajpurkar et al., 2016) in which\nthere is only one answer, while multiple arguments\nplaying the same role can appear simultaneously\nin an event. Since the prediction is separated with\nroles, an argument can play multiple roles, and a\ntoken can belong to different arguments. Thus, the\nroles overlap problem can also be solved.\n3.3 Argument Span Determination\nIn PLMEE, a token tis predicted as the start of an\nargument that plays role rwith probability:\nPr\ns (t) =Softmax (Wr\ns ·B(t)) ,\nwhile as the end with probability:\nPr\ne (t) =Softmax (Wr\ne ·B(t)) ,\nin which we use subscript ”s” to represent ”start”\nand subscript ”e” to represent ”end”. Wr\ns is the\nweight of binary classiﬁer that aims to detect starts\nof arguments playing role r, while Wr\ne is the\nweight of another binary classiﬁer that aims to de-\ntect ends. Bis the BERT embedding.\nFor each role r, we can get two lists Br\ns and Br\ne\nof 0 and 1 according to Pr\ns and Pr\ne . They indicate\nrespectively whether a token in the sentence is the\n5287\nstart or end of an argument that plays role r3. Al-\ngorithm 1 is used to detect each token sequentially\nto determine spans of all arguments that play the\nrole r.\nAlgorithm 1Argument span determination\nIn: Pr\ns and Pr\ne , Br\ns and Br\ne, sentence length l.\nOut: Span list Lof the arguments that play role r\nInitiate: as ←-1, ae ←-1\n1: for i←0 to ldo\n2: if In State 1 & the ith token is a start then\n3: as ←iand change to State 2\n4: end if\n5: if In State 2 then\n6: if the ith token is a new start then\n7: as ←iif Pr\ns [i] >P r\ns [as]\n8: end if\n9: if the ith token is an end then\n10: ae ←iand change to State 3\n11: end if\n12: end if\n13: if In State 3 then\n14: if the ith token is a new end then\n15: ae ←iif Pr\ne [i] >P r\ne [ae]\n16: end if\n17: if the ith token is a new start then\n18: Append [as,ae] to L\n19: ae ←-1, as ←iand change to State 2\n20: end if\n21: end if\n22: end for\nAlgorithm 1 contains a ﬁnite state machine,\nwhich changes from one state to another in re-\nsponse to Br\ns and Br\ne. There are three states to-\ntally: 1) Neither start nor end has been detected;\n2) Only a start has been detected; 3) A start as well\nas an end have been detected. Specially, the state\nchanges according to the following rules: State 1\nchanges to State 2 when the current token is a start;\nState 2 changes to State 3 when the current token\nis an end; State 3 changes to State 2 when the cur-\nrent token is a new start. Notably, if there has been\na start and another start arises, we will choose the\none with higher probability, and the same for end.\n3.4 Loss Re-weighting\nWe initially deﬁne Ls as the loss function of all\nbinary classiﬁers that are responsible for detect-\ning starts of arguments. It is the average of cross\n3The ith token is a start if Br\ns [i]=1 or an end if Br\ne [i]=1.\nentropy between the output probabilities and the\ngolden label y:\nLs = 1\n|R|×|S|\n∑\nr∈R\nCE (Pr\ns ,yr\ns) ,\nin which CE is cross entropy, Ris the set of roles,\nSis the input sentence, and |S|is the number of\ntokens in S. Similarly, we deﬁne Le as the loss\nfunction of all binary classiﬁers that detect ends:\nLe = 1\n|R|×|S|\n∑\nr∈R\nCE (Pr\ne ,yr\ne) .\nWe ﬁnally average Ls and Le as the loss Lof ar-\ngument extractor.\nAs Figure 2 shows, there exists a big gap in fre-\nquency between roles. This implies that roles have\ndifferent levels of ”importance” in an event. The\n”importance” here means the ability of a role to\nindicate events of a speciﬁc type. For example,\nthe role ”Victim” is more likely to indicate a Die\nevent than the role ” Time”. Inspired by this, we\nre-weight Ls and Le according to the importance\nof roles, and propose to measure the importance\nwith the following deﬁnitions:\nRole Frequency (RF)We deﬁne RF as the fre-\nquency of role rappearing in events of type v:\nRF(r,v) = Nr\nv∑\nk∈R Nkv\n,\nwhere Nr\nv is the count of the role rthat appear in\nthe events of type v.\nInverse Event Frequency (IEF)As the mea-\nsure of the universal importance of a role, we de-\nﬁne IEF as the logarithmically scaled inverse frac-\ntion of the event types that contain the role r:\nIEF(r) = log |V|\n|{v∈V : r∈v}|,\nwhere V is tht set of event types.\nFinally we take RF-IEF as the product of RF\nand IEF: RF-IEF(r,v) =RF(r,v) ×IEF(r). With\nRF-IEF, we can measure the importance of a role\nrin events of type v:\nI(r,v) = expRF-IEF(r,v)\n∑\nr′∈R expRF-IEF(r′,v) .\nWe choose three event types and list the two\nmost important roles of each type in Table 1. It\nshows that although there could be multiple roles\n5288\nEvent Type Top 2 Roles Sum\nTransport(15) Artifact, Origin 0.76\nAttack(14) Attacker, Target 0.85\nDie(12) Victim, Agent 0.90\nTable 1: Top two roles and their sum importance for\neach event type. The number in brackets behind event\ntype is the count of roles that have appeared in it.\nin events of someone type, only a few of them is\nindispensable.\nGive the event type vof input, we re-weight Ls\nand Le based on each role’s importance inv:\nLs =\n∑\nr∈R\nI(r,v)\n|S| CE (Pr\ns ,yr\ns)\nLe =\n∑\nr∈R\nI(r,v)\n|S| CE (Pr\ne ,yr\ne) .\nThe loss of argument extractor Lis still the aver-\nage of Ls and Le.\n4 Training Data Generation\nIn addition to PLMEE, we also propose a pre-\ntrained language model based method for event\ngeneration as illustrated in Figure 4. By edit-\ning prototypes, this method can generate a con-\ntrollable number of labeled samples as the extra\ntraining corpus. It consists of three stages: pre-\nprocessing, event generation and scoring.\nTo facilitate the generation method, we deﬁne\nadjunct tokens as the tokens in sentences except\ntriggers and arguments, including not only words\nand numbers, but also punctuation. Taking sen-\ntence in Figure 1 as an example, ”is” and ”going”\nare adjunct tokens. It is evident that adjunct tokens\ncan adjust the smooth and diversity of expression.\nTherefore, we try to rewrite them to expand the di-\nversity of the generation results, while keeping the\ntrigger and arguments unchanged.\n4.1 Pre-processing\nWith the golden labels, we ﬁrst collect arguments\nin the ACE2005 dataset as well as the roles they\nplay. However, those arguments overlap with oth-\ners are excluded. Because such arguments are of-\nten long compound phrases that contain too much\nunexpected information, and incorporating them\nin argument replacement could bring more unnec-\nessary errors.\nWe also adopt BERT as the target model to\nrewrite adjunct tokens in the following stage, and\nﬁne-tune it on the ACE2005 dataset with the\nmasked language model task (Devlin et al., 2018)\nto bias its prediction towards the dataset distribu-\ntion. In common with the pre-training procedure\nof BERT, each time we sample a batch of sen-\ntences and mask 15% of tokens. Its goal is still\nto predict the correct token without supervision.\n4.2 Event generation\nTo generate events, we conduct two steps on a pro-\ntotype. We ﬁrst replace the arguments in the proto-\ntype with those similar that have played the same\nrole. Next, we rewrite adjunct tokens with the ﬁne-\ntuned BERT. Through these two steps, we can ob-\ntain a new sentence with annotations.\nArgument ReplacementThe ﬁrst step is to re-\nplace arguments in the event. Both the argument\nto be replaced and the new one should have played\never the same role. While the roles are inherited\nafter replacement, so we can still use origin labels\nfor the generated samples.\nIn order not to change the meaning drastically,\nwe employ similarity as the criteria for selecting\nnew arguments. It is based on the following two\nconsiderations: one is that two arguments that play\nthe same role may diverge signiﬁcantly in seman-\ntics; another is that the role an argument plays\nis largely dependent on its context. Therefore,\nwe should choose arguments that are semantically\nsimilar and coherent with the context.\nWe use cosine similarity between embeddings\nto measure the similarity of two arguments. And\ndue to ELMO’s ability to handle the OOV prob-\nlem, we employ it to embed arguments:\nE(a) = 1\n|a|\n∑\nt∈a\nE(t),\nwhere ais the argument, Eis ELMO embedding.\nWe choose the top 10 percent most similar argu-\nments as candidates, and use softmax operation on\ntheir similarity to allocate probability.\nAn argument is replaced with probability 80%\nwhile keeping constant with probability 20% to\nbias the representation towards the actual event\n(Devlin et al., 2018). Note that the triggers remain\nunchanged to avoid undesirable deviation of de-\npendency relation.\nAdjunct Token RewritingThe results of argu-\nment replacement can already be considered as the\ngenerated data, but the constant context may in-\ncrease the risk of overﬁtting. Therefore, to smooth\n5289\nDataset\nBERT\nArgument\nCollection\nFine-tuning BERT\nArgument \nReplacement\nAdjunct Token\nRewriting\nScorer\nOut: Prime minister Blair is reported to the\nmeeting with the leaders\nStage  1: Pre-processing Stage  2: Event generation Stage  3: Scoring\nQuality: 0.5\nIn: President Bush is going to be meeting \nwith several Arab leaders\nEntity\n1. President\n2. Prime minister Blair\n3. the prime minister\n4. the Arab leaders \n5. an Arab counterpart\n6. the Palestinians\n7. the leaders\n8. ...\nPrime minister Blair is going to be meeting \nwith the leaders\nFigure 4: Flow chart of the generation approach.\nthe generated data and expand their diversity, we\nmanage to rewrite adjunct tokens with the ﬁne-\ntuned BERT.\nThe rewriting is to replace some adjunct tokens\nin the prototype with the new ones that are more\nmatchable with the current context. We take it as\na Cloze task (Taylor, 1953), where some adjunct\ntokens are randomly masked and the BERT ﬁne\ntuned in the ﬁrst stage is used to predict vocabulary\nids of suitable tokens based on the context. We use\na parameter mto denote the proportion of adjunct\ntokens that need to be rewritten.\nAdjunct token rewriting is a step-by-step pro-\ncess. Each time we mask 15% of adjunct tokens\n(with the token [MASK]). Then the sentence is fed\ninto BERT to produce new adjunct tokens. The ad-\njunct tokens that have not yet been rewritten will\ntemporarily remain in the sentence.\nTo further illustrate the above two steps, we give\nan instance in Figure 4. In this instance, we set\nmto 1.0, which means all the adjunct tokens will\nbe rewritten. The ﬁnal output is ” Prime minister\nBlair is reported to the meeting with the leaders”,\nwhich shares the labels with the original event in\nthe prototype. It is evident that some adjunct to-\nkens are preserved despite mis 1.0.\n4.3 Scoring\nTheoretically, inﬁnite number of events can be\ngenerated with our generation method. However,\nnot all of them are valuable for the extractor and\nsome may even degrade its performance. There-\nfore, we add an extra stage to quantify the quality\nof each generated sample to pick out those valu-\nable. Our key insight for evaluating the quality\nlies that it is tightly related to two factors, which\nare the perplexity and the distance to the original\ndataset. The former reﬂects the rationality of gen-\neration, and the latter reﬂects the differences be-\ntween the data.\nPerplexity (PPL) Different with the masked\nperplexity (Devlin et al., 2018) of logarithmic ver-\nsion, we take the average probability of those ad-\njunct tokens that have been rewritten as the per-\nplexity of generated sentence S′:\nPPL(S′) = 1\n|A(S′)|\n∑\nt∈A(S′)\nP(t),\nwhere Ais the set of adjunct tokens inS′ that have\nbeen rewritten.\nDistance (DIS) We measure the distance be-\ntween S′ and the dataset Dwith cosine similarity:\nDIS(S′,D) = 1− 1\n|D|\n∑\nS∈D\nB(S′) ·B(S)\n|B(S′)|×|B(S)|.\nDifferent with embedding arguments by ELMO,\nwe utilize BERT to embed sentence and take the\nembedding of the ﬁrst token [CLS] as the sentence\nembedding.\nBoth the PPL and the DIS are limited in [0,1].\nWe consider that generated samples of high qual-\nity should have both low PPL and DIS. Therefore,\nwe deﬁne the quality function as:\nQ(S′) = 1−\n(\nλPPL\n(\nS′)\n+ (1−λ) DIS\n(\nS′,D\n))\n, where λ∈[0,1] is the balancing parameter. This\nfunction is used to select generated samples of\nhigh quality in experiments.\n5 Experiments\nIn this section, we ﬁrst evaluate our event extractor\nPLMEE on the ACE2005 dataset. Then we give a\ncase study of generated samples and conduct au-\ntomatic evaluations by adding them into the train-\ning set. Finally, we illustrate the limitations of the\ngeneration method.\n5290\nModel\nPhase Trigger Trigger Argument Argument\nIdentiﬁcation(%) Calssﬁcation(%) Identiﬁcation(%) Calssﬁcation(%)\nP R F P R F P R F P R F\nCross Event N/A 68.7 68.9 68.8 50.9 49.7 50.3 45.1 44.1 44.6\nCross Entity N/A 72.9 64.3 68.3 53.4 52.9 53.1 51.6 45.5 48.3\nMax Entropy 76.9 65.0 70.4 73.7 62.3 67.5 69.8 47.9 56.8 64.7 44.4 52.7\nDMCNN 80.4 67.7 73.5 75.6 63.6 69.1 68.8 51.9 59.1 62.2 46.9 53.5\nJRNN 68.5 75.7 71.9 66.0 73.0 69.3 61.4 64.2 62.8 54.2 56.7 55.4\nDMCNN-DS 79.7 69.6 74.3 75.7 66.0 70.5 71.4 56.9 63.3 62.8 50.1 55.7\nANN-FN N/A 79.5 60.7 68.8 N/A N/A\nANN-AugATT N/A 78.0 66.3 71.7 N/A N/A\nPLMEE(-) 84.8 83.7 84.2 81.0 80.4 80.7 71.5 59.2 64.7 61.7 53.9 57.5\nPLMEE 71.4 60.1 65.3 62.3 54.2 58.0\nTable 2: Performance of all methods. Bold denotes the best result.\nAs previous works (Li et al., 2013b; Chen et al.,\n2015; Hong et al., 2011), we take the test set\nwith 40 newswire documents, while 30 other doc-\numents as the validation set, and the remaining\n529 documents to be the training set. However,\ndifferent with previous works, we take the follow-\ning criteria to evaluate the correctness of each pre-\ndicted event mention:\n1. A trigger prediction is correct only if its span\nand type match with the golden labels.\n2. An argument prediction is correct only if its\nspan and all roles it plays match with the\ngolden labels.\nIt is worth noting that all the predicted roles for\nan argument are required to match with the golden\nlabels, instead of just one of them. We adopt Pre-\ncision (P), Recall (R) and F measure (F1) as the\nevaluation metrics.\n5.1 Results of Event Extraction\nWe take several previous classic works for com-\nparison, and divide them into three categories:\nFeature based methodsDocument-level infor-\nmation is utilized in Cross event (Liao and Gr-\nishman, 2010) to assist event extraction. While\nCross entity (Hong et al., 2011) uses cross-entity\ninference in extraction. Max Extropy (Li et al.,\n2013a) extracts triggers as well as arguments to-\ngether based on structured prediction.\nNeural based methodsDMCNN (Chen et al.,\n2015) adopts ﬁrstly dynamic multi-pooling CNN\nto extract sentence-level features automatically.\nJRNN (Nguyen et al., 2016) proposes a joint\nframework based on bidirectional RNN for event\nextraction.\nExternal resource based methodsDMCNN-\nDS (Chen et al., 2017) uses FreeBase to label\npotential events in unsupervised corpus by dis-\ntance supervision. ANN-FN (Liu et al., 2016)\nimproves extraction with additionally events au-\ntomatically detected from FrameNet, while ANN-\nAugATT (Liu et al., 2017) exploits argument infor-\nmation via the supervised attention mechanisms to\nimprove the performance further.\nIn order to verify the effectiveness of loss re-\nweighting, two groups of experiments are con-\nducted for comparison. Namely, the group where\nthe loss function is simply averaged on all clas-\nsiﬁers’ output (indicated as PLMEE(-)) and the\ngroup where the loss is re-weighted based on role\nimportance (indicated as PLMEE).\nTable 2 compares the results of the aforemen-\ntioned models with PLMEE on the test set. As is\nshown, in both the trigger extraction task and the\nargument extraction task, PLMEE(-) has achieved\nthe best results among all the compared meth-\nods. The improvement on the trigger extraction\nis quite signiﬁcant, seeing a sharp increase of near\n10% on the F1 score. While the improvement in\nargument extraction is not so obvious, achieving\nabout 2%. This is probably due to the more rigor-\nous evaluation metric we have taken and the difﬁ-\nculty of argument extraction task as well. More-\nover, compared with feature based methods, neu-\nral based methods can achieve better performance.\nAnd the same observation appears when compar-\ning external resource based methods with neural\nbased methods. It demonstrates that external re-\n5291\nPrototype m Generated Event\nPresident Bush is\ngoing to be meeting\nwith several Arab\nleaders\n0.2 Russian President Putin is going to the meeting with the Arab leaders\n0.4 The president is reported to be meeting with an Arab counterpart\n0.6 Mr. Bush is summoned to a meeting with some Shiite Muslim groups\n0.8 The president is attending to the meeting with the Palestinians\n1.0 Prime minister Blair is reported to the meeting with the leaders\nTable 3: Example samples generated with different proportion of rewritten adjunct tokens. Italic indicates argument\nand bold indicates trigger.\nsources are useful to improve event extraction. In\naddition, the PLMEE model can achieve better re-\nsults on the argument extraction task - with im-\nprovement of 0.6% on F1 score for identiﬁcation\nand 0.5% for classiﬁcation - than the PLMEE(-)\nmodel, which means that re-weighting the loss can\neffectively improve the performance.\n5.2 Case Study\nTable 3 illustrates a prototype and its generation\nwith parameter m ranging from 0.2 to 1.0. We\ncan observe that the arguments after replacement\ncan match the context in prototype relatively well,\nwhich indicates that they are resembling with the\noriginal ones in semantic.\nOn the other hand, rewriting the adjunct tokens\ncan smooth the generated data and expand their di-\nversity. However, since there is no explicit guide,\nthis step can also introduce unpredictable noise,\nmaking the generation not ﬂuent as expected.\n5.3 Automatic Evaluation of Generation\nSo far, there are mainly three aspects of the gen-\neration method that could have signiﬁcant impacts\non the performance of the extraction model, in-\ncluding the amount of generated samples (repre-\nsented by n, which indicates times the generation\nsize is the number of dataset size), the proportion\nof rewritten adjunct tokens m, and the quality of\nthe generated samples. The former two factors\nare controllable in the generation process. Spe-\ncially, we can reuse a prototype and get a variety of\ncombinations of arguments via similarity based re-\nplacement, which will bring different contexts for\nrewriting adjunct tokens. Moreover, the propor-\ntion of rewritten adjunct tokens can be adjusted,\nmaking a further variation. Although the quality of\ngeneration cannot be controlled arbitrarily, it can\nbe quantiﬁed by the score function Qso that those\nsamples of higher quality can be picked out and\nadded into the training set. With λin Qchanging,\ndifferent selection strategies can be used to screen\nout the generated samples.\nWe ﬁrst tuned the former two parameters on the\ndevelopment set through grid search. Specially,\nwe set mranging from 0.2 to 1.0 with an interval\nof 0.2, and set nto be 0.5, 1.0 and 2.0, while keep-\ning other parameters unchanged in the generation\nprocess. We conduct experiments with these pa-\nrameters. By analyzing the results, we ﬁnd that the\nbest performance of PLMEE on both trigger ex-\ntraction and argument extraction can be achieved\nwith m = 0.4 and n = 1.0. It suggests that nei-\nther too few generated samples nor too much is a\nbetter choice for extraction. Too few has limited\ninﬂuence, while too much could bring more noise\nthat disturbs the distribution of the dataset. For the\nbetter extraction performance, we use such param-\neter settings in the following experiments.\nWe also investigate the effectiveness of the\nsample selection approach, a comparison is con-\nducted between three groups with different selec-\ntion strategies. We obtain a total of four times the\nsize of the ACE2005 dataset using our generation\nmethod with m= 0.4, and pick out one quarter of\nthem (n= 1.0) with λbeing 0, 0.5 and 1.0 respec-\ntively. When λ is 0 or 1.0, it is either perplexity\nor distance that determines the quality exclusively.\nWe ﬁnd that the selection method with λ = 0.5\nin quality function is able to pick out samples that\nare more advantageous to promote the extraction\nperformance.\nModel Trigger(%) Argument(%)\nPLMEE 80.7 58.0\nPLMEE(+) 81.1 58.9\nTable 4: F1 score of trigger classiﬁcation and argument\nclassiﬁcation on the test set.\nFinally, we incorporate the above generated\ndata with the ACE2005 dataset and investigate the\neffectiveness of our generation method on the test\n5292\nset. In Table 4, we use PLMEE(+) denotes the\nPLMEE model trained with extra generated sam-\nples. The results illustrate that with our event gen-\neration method, the PLMEE model can achieve the\nstate of the art result of event extraction.\n5.4 Limitation\nBy comparing the annotations in generated sam-\nples and manually labeled samples, we ﬁnd that\none issue of our generation method is that the roles\nmay deviate, because the semantics could change\na lot with only a few adjunct tokens been rewritten.\nTaking Figure 5 as an example. The roles played\nby argument ”Pittsburgh” and ”Boston” should be\n”Destination” and ”Origin”, rather not the oppo-\nsite as in the prototype. This is because the to-\nken ”from” has been replaced with the token ”for”,\nwhile token ”drive to” been replaced with ”return\nfrom”.\nTrigger leave\nEvent type Movement.Transport\nArguments Niagara Falls Toronto\nRoles Origin Destination\nTrigger leave\nEvent type Movement.Transport\nArguments Pittsburgh Boston\nRoles Origin Destination\nPrototype: Leave from Niagara Falls and drive to Toronto, on 85 miles\nGeneration: Leave for Pittsburgh and return from Boston in 200 miles\nx x\n✓\nFigure 5: One of the generated samples with wrong\nannotations.\n6 Conclusion and Discussion\nIn this paper, we present a framework to promote\nevent extraction by using a combination of an ex-\ntraction model and a generation method, both of\nwhich are based on pre-trained language models.\nTo solve the roles overlap problem, our extraction\napproach tries to separate the argument predictions\nin terms of roles. Then it exploits the importance\nof roles to re-weight the loss function. To perform\nevent generation, we present a novel method that\ntakes the existing events as prototypes. This event\ngeneration method can produce controllably la-\nbeled samples through argument replacement and\nadjunct tokens rewriting. It also beneﬁts from the\nscoring mechanism which is able to quantify the\nquality of generated samples. Experimental re-\nsults show that the quality of generated data is\ncompetitive and incorporating them with existing\ncorpus can make our proposed event extractor to\nbe superior to several state of the art approaches.\nOn the other hand, there are still limitations in\nour work. Events of the same type often share sim-\nilarity. And co-occurring roles tend to hold a tight\nrelation. Such features are ignored in our model,\nbut they deserve more investigation for improving\nthe extraction model. In addition, although our\ngeneration method can control the number of gen-\nerated samples and ﬁlter with quality, it still suf-\nfers the deviation of roles alike with distant super-\nvision. Therefore, for the future work, we will in-\ncorporate relation between events and relation be-\ntween arguments into pre-trained language mod-\nels, and take effective measures to overcome the\ndeviation problem of roles in the generation.\nAcknowledgments\nThe work was sponsored by the National Key Re-\nsearch and Development Program of China under\nGrant No.2018YFB0204300, and National Nat-\nural Science Foundation of China under Grant\nNo.61872376 and No.61806216.\nReferences\nYubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and\nJun Zhao. 2017. Automatically labeled data gener-\nation for large scale event extraction. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), volume 1, pages 409–419.\nYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng,\nand Jun Zhao. 2015. Event extraction via dy-\nnamic multi-pooling convolutional neural networks.\nIn Proceedings of the 53rd Annual Meeting of the\nAssociation for Computational Linguistics and the\n7th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers) , vol-\nume 1, pages 167–176.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nGeorge R Doddington, Alexis Mitchell, Mark A Przy-\nbocki, Lance A Ramshaw, Stephanie M Strassel, and\nRalph M Weischedel. 2004. The automatic content\nextraction (ace) program-tasks, data, and evaluation.\nIn LREC, volume 2, page 1.\nXiaocheng Feng, Lifu Huang, Duyu Tang, Heng Ji,\nBing Qin, and Ting Liu. 2016. A language-\nindependent neural network for event detection. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), volume 2, pages 66–71.\n5293\nKelvin Guu, Tatsunori B Hashimoto, Yonatan Oren,\nand Percy Liang. 2018. Generating sentences by\nediting prototypes. Transactions of the Association\nof Computational Linguistics, 6:437–450.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nYu Hong, Jianfeng Zhang, Bin Ma, Jianmin Yao,\nGuodong Zhou, and Qiaoming Zhu. 2011. Us-\ning cross-entity inference to improve event extrac-\ntion. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies-Volume 1, pages 1127–\n1136. Association for Computational Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339.\nLifu Huang, Taylor Cassidy, Xiaocheng Feng, Heng\nJi, Clare R V oss, Jiawei Han, and Avirup Sil. 2016.\nLiberal event extraction and event schema induction.\nIn Proceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pages 258–268.\nPeifeng Li, Qiaoming Zhu, and Guodong Zhou.\n2013a. Joint modeling of argument identiﬁcation\nand role determination in chinese event extraction\nwith discourse-level information. In Twenty-Third\nInternational Joint Conference on Artiﬁcial Intelli-\ngence.\nQi Li, Heng Ji, and Liang Huang. 2013b. Joint event\nextraction via structured prediction with global fea-\ntures. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), volume 1, pages 73–82.\nShasha Liao and Ralph Grishman. 2010. Using doc-\nument level cross-event inference to improve event\nextraction. In Proceedings of the 48th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 789–797. Association for Computational\nLinguistics.\nBing Liu, Longhua Qian, Hongling Wang, and\nGuodong Zhou. 2010. Dependency-driven feature-\nbased learning for extracting protein-protein interac-\ntions from biomedical text. In Proceedings of the\n23rd International Conference on Computational\nLinguistics: Posters , pages 757–765. Association\nfor Computational Linguistics.\nShulin Liu, Yubo Chen, Shizhu He, Kang Liu, and\nJun Zhao. 2016. Leveraging framenet to improve\nautomatic event detection. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers) , vol-\nume 1, pages 2134–2143.\nShulin Liu, Yubo Chen, Kang Liu, and Jun Zhao. 2017.\nExploiting argument information to improve event\ndetection via supervised attention mechanisms. In\nProceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), volume 1, pages 1789–1798.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6294–6305.\nMike Mintz, Steven Bills, Rion Snow, and Dan Juraf-\nsky. 2009. Distant supervision for relation extrac-\ntion without labeled data. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP: Vol-\nume 2-Volume 2, pages 1003–1011. Association for\nComputational Linguistics.\nMakoto Miwa, Rune Sætre, Yusuke Miyao, and\nJun’ichi Tsujii. 2009. A rich feature vector for\nprotein-protein interaction extraction from multiple\ncorpora. In Proceedings of the 2009 Conference on\nEmpirical Methods in Natural Language Process-\ning: Volume 1-Volume 1 , pages 121–130. Associa-\ntion for Computational Linguistics.\nThien Huu Nguyen, Kyunghyun Cho, and Ralph Gr-\nishman. 2016. Joint event extraction via recurrent\nneural networks. In Proceedings of the 2016 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 300–309.\nThien Huu Nguyen and Ralph Grishman. 2015. Event\ndetection and domain adaptation with convolutional\nneural networks. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 2: Short\nPapers), volume 2, pages 365–371.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. URL https://s3-\nus-west-2. amazonaws. com/openai-assets/research-\ncovers/languageunsupervised/language under-\nstanding paper. pdf.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383–2392.\n5294\nWilson L Taylor. 1953. “cloze procedure”: A new\ntool for measuring readability. Journalism Bulletin,\n30(4):415–433.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\narXiv:1609.08144.\nHang Yang, Yubo Chen, Kang Liu, Yang Xiao, and Jun\nZhao. 2018. Dcfee: A document-level chinese ﬁ-\nnancial event extraction system based on automat-\nically labeled training data. Proceedings of ACL\n2018, System Demonstrations, pages 50–55.\nYing Zeng, Yansong Feng, Rong Ma, Zheng Wang, Rui\nYan, Chongde Shi, and Dongyan Zhao. 2018. Scale\nup event extraction learning via automatic training\ndata generation. In Thirty-Second AAAI Conference\non Artiﬁcial Intelligence.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8151953220367432
    },
    {
      "name": "Event (particle physics)",
      "score": 0.7408130168914795
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.7169538140296936
    },
    {
      "name": "Task (project management)",
      "score": 0.6643346548080444
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5779825448989868
    },
    {
      "name": "Argument (complex analysis)",
      "score": 0.5495072603225708
    },
    {
      "name": "Process (computing)",
      "score": 0.5407441258430481
    },
    {
      "name": "Extraction (chemistry)",
      "score": 0.4981398582458496
    },
    {
      "name": "Machine learning",
      "score": 0.44370728731155396
    },
    {
      "name": "Natural language processing",
      "score": 0.41479790210723877
    },
    {
      "name": "Data extraction",
      "score": 0.41424381732940674
    },
    {
      "name": "Data mining",
      "score": 0.4127083718776703
    },
    {
      "name": "Engineering",
      "score": 0.06800675392150879
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chromatography",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "MEDLINE",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170215575",
      "name": "National University of Defense Technology",
      "country": "CN"
    }
  ]
}