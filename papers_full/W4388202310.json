{
    "title": "Friend or foe? Exploring the implications of large language models on the science system",
    "url": "https://openalex.org/W4388202310",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2128386366",
            "name": "Benedikt Fecher",
            "affiliations": [
                "Alexander von Humboldt Institute for Internet and Society",
                "Dialog Semiconductor (Germany)"
            ]
        },
        {
            "id": "https://openalex.org/A2106232316",
            "name": "Marcel Hebing",
            "affiliations": [
                "Berliner Hochschule für Technik",
                "Alexander von Humboldt Institute for Internet and Society"
            ]
        },
        {
            "id": "https://openalex.org/A2434067705",
            "name": "Melissa Laufer",
            "affiliations": [
                "Alexander von Humboldt Institute for Internet and Society"
            ]
        },
        {
            "id": "https://openalex.org/A2155242374",
            "name": "Jörg Pohle",
            "affiliations": [
                "Alexander von Humboldt Institute for Internet and Society"
            ]
        },
        {
            "id": "https://openalex.org/A5092207565",
            "name": "Fabian Sofsky",
            "affiliations": [
                "Alexander von Humboldt Institute for Internet and Society"
            ]
        },
        {
            "id": "https://openalex.org/A2128386366",
            "name": "Benedikt Fecher",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2106232316",
            "name": "Marcel Hebing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2434067705",
            "name": "Melissa Laufer",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2155242374",
            "name": "Jörg Pohle",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5092207565",
            "name": "Fabian Sofsky",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4323049179",
        "https://openalex.org/W3206896625",
        "https://openalex.org/W4381547091",
        "https://openalex.org/W4283645384",
        "https://openalex.org/W4376117416",
        "https://openalex.org/W4318014888",
        "https://openalex.org/W4360620450",
        "https://openalex.org/W4318671267",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W4221015229",
        "https://openalex.org/W4380625133",
        "https://openalex.org/W4313592949",
        "https://openalex.org/W4380319827",
        "https://openalex.org/W4385066501",
        "https://openalex.org/W4322626329",
        "https://openalex.org/W4376866708",
        "https://openalex.org/W4318035272",
        "https://openalex.org/W4367053831",
        "https://openalex.org/W2003282762",
        "https://openalex.org/W4319301446",
        "https://openalex.org/W4323848232",
        "https://openalex.org/W4321371665",
        "https://openalex.org/W4321605350",
        "https://openalex.org/W4317801997",
        "https://openalex.org/W4292402516",
        "https://openalex.org/W4367852612",
        "https://openalex.org/W4317390716",
        "https://openalex.org/W4360980513",
        "https://openalex.org/W4387014701",
        "https://openalex.org/W4225117489",
        "https://openalex.org/W4318066464",
        "https://openalex.org/W4383815588"
    ],
    "abstract": "Abstract The advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher education. While the impact on education has been a primary focus, there is limited empirical research on the effects of large language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we conducted a Delphi study involving 72 researchers specializing in AI and digitization. The study focused on applications and limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies for their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, creative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed through proactive regulation and science education. This research contributes to informed discussions on the impact of generative AI in science and helps identify areas for future action.",
    "full_text": "Vol.:(0123456789)\nAI & SOCIETY (2025) 40:447–459 \nhttps://doi.org/10.1007/s00146-023-01791-1\nMAIN PAPER\nFriend or foe? Exploring the implications of large language models \non the science system\nBenedikt Fecher1,2  · Marcel Hebing1,3 · Melissa Laufer1 · Jörg Pohle1 · Fabian Sofsky1\nReceived: 3 August 2023 / Accepted: 26 September 2023 / Published online: 26 October 2023 \n© The Author(s) 2023\nAbstract\nThe advent of ChatGPT by OpenAI has prompted extensive discourse on its potential implications for science and higher \neducation. While the impact on education has been a primary focus, there is limited empirical research on the effects of \nlarge language models (LLMs) and LLM-based chatbots on science and scientific practice. To investigate this further, we \nconducted a Delphi study involving 72 researchers specializing in AI and digitization. The study focused on applications \nand limitations of LLMs, their effects on the science system, ethical and legal considerations, and the required competencies \nfor their effective use. Our findings highlight the transformative potential of LLMs in science, particularly in administrative, \ncreative, and analytical tasks. However, risks related to bias, misinformation, and quality assurance need to be addressed \nthrough proactive regulation and science education. This research contributes to informed discussions on the impact of \ngenerative AI in science and helps identify areas for future action.\nKeywords Large language models · Science system · Delphi study · Scholarly communication\n1 Introduction\nThe release of ChatGPT by OpenAI in November 2022 has \nsparked a plethora of editorials, position papers and essays, \nor interviews with experts, as well as some articles and pre-\nprints on the potential impacts on science and higher educa-\ntion. While many concerns raised relate to how ChatGPT \nand large language models (LLMs) will change education \n(e.g., Perkins 2023; Fyfe 2023), there is much less—espe-\ncially empirical research—on the implications of LLMs \nas well as LLM-based chatbots or prompts on scholarly \npractices and the science system, which we understand as \na collective body of all academic disciplines, including the \nsciences and humanities (Ribeiro et al. 2023; Chubb et al. \n2022). One can, however, draw inspiration from fields that \nare also characterized by largely text-based or -focused, cre-\native and knowledge work. For instance, the opinion paper \nby Dwivedi et al. (2023) provides a viewpoint on the poten-\ntial impact of generative AI technologies such as ChatGPT \nin the domains of education, business, and society, based \non 43 contributions by AI experts from various disciplines. \nHowever, the literature on knowledge work and the trans-\nformative effects of AI cannot account for the complexities \nof specific practices (Jiang et al. 2022).\nIn light of the limited research conducted on LLMs and \ntheir impact on the science system and scientific practice, \nwe initiated a Delphi study involving experts who special-\nize in the intersection of research and AI technology. The \npurpose of this study was to investigate the following areas: \n(a) the potential applications and limitations in using LLMs, \n(b) the positive and negative effects of LLMs on the science \nsystem, (c) the regulatory and ethical considerations associ-\nated with the use of LLMs in science, and (d) the necessary \n * Benedikt Fecher \n fecher@hiig.de\n Marcel Hebing \n marcel.hebing@hiig.de\n Melissa Laufer \n melissa.laufer@hiig.de\n Jörg Pohle \n joerg.pohle@hiig.de\n Fabian Sofsky \n fabian.sofsky@hiig.de\n1 Alexander von Humboldt Institute for Internet and Society, \nBerlin, Germany\n2 Wissenschaft im Dialog, Berlin, Germany\n3 DBU Digital Business University of Applied Sciences, \nBerlin, Germany\n448 AI & SOCIETY (2025) 40:447–459\ncompetencies and capacities for effectively utilizing LLMs. \nOur objective in this study was to gather and structure expert \nopinions in an initial phase, focusing on the aforementioned \ncategories, and subsequently evaluate and assess them in \na second phase. As generative AI continues to advance, it \nis crucial to gather expert knowledge and informed assess-\nments regarding its potential impact on science. This knowl-\nedge will contribute to an informed scholarly debate and \nhelp anticipate potential fields of action.\nOur findings indicate that experts anticipate that the utili-\nzation of LLMs will have a transformative and largely posi-\ntive impact on science and scientific practice. In LLMs, they \nrecognize significant potential for administrative, creative, \nand analytical tasks. The main risks associated with LLMs \npertain to issues of bias, misinformation, and overburdening \nof the scientific quality assurance system. Despite the per -\nceived advantages of LLMs for science, it is imperative to \nacknowledge and address the associated risks. This necessi-\ntates proactive measures in regulation and science education.\n2  Literature review\nIn the following, we provide an overview of the current state \nof the scholarly discourse along the aforementioned areas. \nWhile our aim was to present a comprehensive and contem-\nporary overview of this discourse, it is, however, important \nto acknowledge that new and pertinent studies may have \nemerged by the time of the publication of this article.\n2.1  Applications and limitations of LLMs in science\nLLMs and LLM-based tools are expected to have a wide \nrange of applications in scientific practice. Possible uses \nfor researchers identified in the literature range from \ngenerating plausible research ideas (Dowling and Lucey \n2023), brainstorming (Staiman 2023), transforming notes \ninto text (Buruk 2023 ), creating a first draft of a paper \n(Dwivedi et al. 2023), assisting with grammar and lan-\nguage (Flanagin et al. 2023), e.g., to improve clarity (Lund \net al. 2023), especially for non-native speakers (Perkins \n2023), but also stylistic issues, from formatting references \nto complying with editing standards (Flanagin et al. 2023; \nLund et al. 2023). LLM-based tools like ChatGPT may be \nused to generate literature reviews (Dowling and Lucey \n2023), data crunching (Staiman 2023 ), data summaries \n(Lucey and Dowling 2023), even proposing new experi-\nments (Grimaldi and Ehrler 2023). They may support the \ndissemination of publications and the diffusion of knowl-\nedge by helping to create better metadata, indexing, and \nsummaries of research findings (Lund et al. 2023). They \nare expected to assist editors in screening submission for \nissues such as plagiarism or image manipulation, triaging, \nvalidating references, editing and formatting (Flanagin \net al. 2023; Hosseini and Horbach 2023 ). Beyond schol -\narly writing, LLM-based tools are expected to assist with \ncode writing, automating simple tasks, and error manage-\nment (Dwivedi et al. 2023), but also in writing reports, \nstrategy documents, emails as well as cover and rejection \nletters (Corless 2023). They may even be used as a replace-\nment for human participants in psychological experiments \n(Dillion et al. 2023). Scientists may also use LLM-based \ntools for non-scholarly tasks, as a recent Nature poll has \nshown: while eighty per cent of respondents have used AI \nchatbots, more than half say they use them for ‘creative \nfun’ (Owens 2023).\nWhile the fields of application appear diverse, it appears \nthat LLMs and LLM-based tools have limitations in schol-\narly use. Several editorials and Op Eds have been pub-\nlished that point to glaring mistakes of ChatGPT, includ-\ning referencing scientific studies that do not exist (Perkins \n2023). The company behind ChatGPT, OpenAI, admits \nopenly in its blog: “ChatGPT sometimes writes plausible-\nsounding but incorrect or nonsensical answers” (OpenAI \n2022). At the time of writing this article, all existing LLM-\nbased chatbots have been trained on outdated data. As a \nresult, they do not possess the capability to incorporate \nreal-time data automatically, leading to a lack of updated \ninformation (Dwivedi et al. 2023). Other limitations that \nhave been identified include flawed logical argumenta-\ntion, lack of critical elaboration, and unoriginal generated \ncontent (Dwivedi et al. 2023). Errors may also occur in \ninterpreting meaning, in particular if terms are ambiguous, \nhave multiple meanings or consist of compound words \n(Lund et al. 2023). Their limitations in simulating human \ncomprehension, reasoning, and evolving views make them \nunsuitable as substitutes for human participants in psycho -\nlogical studies (Harding et al. 2023). In addition, generated \ntexts may lack semantic coherence and lexical diversity \n(Perkins 2023). Teubner et al. (2023, p. 96) state that the \nproduced texts often “read somewhat bland, generic, and \nvague with a noticeable tendency to seek balance”, and \nthat a very common ChatGPT phrase is: “However, it is \nimportant to note…”. Like ML-based systems in general, \nLLM-based chatbots are considered to lack transparency \nand explainability (Dwivedi et al. 2023), and reproduce or \neven amplify biases inherent in the information that was \nused to train them (Corless 2023; Hosseini et al. 2023), \nreproducing an “of the same old trivialities and stereo-\ntypes” (Teubner et al. 2023, p. 99). This is considered \na structural issue of how these systems are trained and \ncannot be resolved by simply creating bigger models as \nsize does not guarantee diversity (Bender et al. 2021). \nAdditionally, scholars have emphasized the significance of \ndistinguishing specific explainability requirements among \nusers or customers (Wulff and Finnestrand 2023).\n449AI & SOCIETY (2025) 40:447–459 \n2.2  Opportunities and risks for the science system\nA prevailing viewpoint in the literature anticipates positive \neffects of LLMs on the science system. Potential oppor -\ntunities of LLMs on science include positive effects on \nscholarly pro ductivity, quicker access to available scholarly \nresources via enhanced search engines to the automation \nof mundane, repetitive or tedious work such as correcting \ngrammatical errors, allowing people to focus on creative and \nnon-repetitive activities (Dwivedi et al. 2023; Lund et al. \n2023). Foremost among these anticipated benefits is the \nenhancement of research productivity and the elevation of \npublication quality. There is an expectation that using these \ntools to improve their texts, researchers “can focus more on \nwhat to communicate to others, rather than on how  to write \nit” (Pividori and Greene 2023, p. 15). Staiman (2023 n.p.), \nfor instance, notes in a guest post for the blog of the Soci-\nety for Scholarly Publishing that the writing process should \nbe considered less an end in itself but rather “a means to \nan end of conveying important findings in a manner that is \nclear and coherent”. Along these lines, Lund et al. ( 2023) \nsuggest that the capability of ChatGPT and the like might \nlead to questioning the strong belief that ‘publish or perish’ \nis an important and valuable principle in academia and pos-\nsibly change the criteria for evaluating tenure. Some scholars \nexpect a revolution of “the whole scientific endeavor” and \nrefer to these tools’ fundamental disregard of the boundaries \nof scientific disciplines, which may help “bringing multi-\ndisciplinary science to new heights” (Grimaldi and Ehrler \n2023, p. 879). Furthermore, these tools may also lead to the \ndemocratization of science: First, the research process might \nbe democratized as LLM-based tools may compensate for \nthe lack of financial resources, e.g., for “traditional (human) \nresearch assistance”, as a news article puts it (Lucey and \nDowling 2023 n.p.). Second, the dissemination of knowl -\nedge might be democratized as these tools can easily polish \nthe language of a text or even translate research output to \nmultiple languages, both of which would level the field for \nresearchers who speak English as a foreign language (Cor -\nless 2023; Liebrenz et al. 2023), or provide it in multimodal \nways, including dialogical science communication at scale \n(Schäfer 2023).\nAmong the risks for the science system identified in \nthe literature are the adverse effects on the academic qual-\nity assurance mechanisms and, subsequently, on scientific \nintegrity. The avalanche of AI-generated “scientific-look -\ning papers devoid of scientific content” (Grimaldi and Ehr -\nler 2023, p. 879) is expected by some researchers to over -\nburden the academic review process and foster plagiarism \n(Dwivedi et al. 2023). Biases are expected to be reinforced \nand errors introduced into the scholarly debate that might \nbe difficult to identify and correct (Lund et al. 2023), \nincluding in peer review (Chubb et al. 2022). A recent \nstudy by Liang et al. (2023) evaluating the performance \nof several widely used GPT detectors found that they con-\nsistently misclassify non-native English writing samples \nas AI-generated, whereas native writing samples are accu-\nrately identified. Several scholars expect that LLMs may \nlead to an increase in misinformation and disinformation \nand more “junk science”, as an article in Wiley's Advanced \nScience News formulates it (Corless 2023 n.p.). Lund \net al. (2023) express apprehension regarding the utiliza-\ntion of LLM-based tools in academia, contending that this \nemployment not only engenders apprehensions regarding \nresearch reproducibility and transparency but also has the \npotential to erode trust in the scientific process (see also \nVan Noorden 2022). Further, in an LSE Blog post, Beer \n(2019) raises concerns about the diminishing prospects for \nscientific serendipity and unexpected discoveries.\n2.3  Competencies and capacities in scientific \npractice\nIt is assumed that LLMs and LLM-based tools will mark \na shift in the academic skill set. Prompt engineering, i.e., \ndeveloping and producing prompts for conversational AI \nsystems like ChatGPT, is often discussed as a new com-\npetence that is required from researchers (e.g., Teubner \net al. 2023). This is believed to pose a particular challenge \nfor individuals who already struggle with basic IT, as they \nwill not derive much benefit from advances in AI, and \nthis may lead to a widening productivity gap. As LLM-\nbased tools may have better English writing skills than \nsome people, especially non-native speakers, the focus in \nacademic work is expected to shift from text writing to \nconducting research, which requires researchers to formu-\nlate interesting research questions and carry out research \nto find answers (Dwivedi et al. 2023). More generally, \nas Teubner et al. (2023, p. 98) observe, “the ability to \nread and interpret different text options becomes more \nimportant than the ability to write them.” That means that \nresearchers must be able to check the generated text for \nfactual and citation accuracy, bias, mathematical, logical, \nand commonsense reasoning, relevance, and originality, \nas Hosseini et al. (2023) demand in an Accountability in \nResearch editorial. That also means that researchers are \nexpected to have the competencies to collate and com-\nbine the results that LLM-based tools generate (Floridi \nand Chiriatti 2020). Not surprisingly, Dowling and Lucey \n(2023) find that adding domain expertise greatly improves \nthe quality of the generated results. Thus, among the key \nskills that researchers have to develop are critical thinking, \nproblem solving, ethical decision-making, and creativity \n(Dwivedi et al. 2023).\n450 AI & SOCIETY (2025) 40:447–459\n2.4  Ethical and regulatory issues \nconcerning ChatGPT\nThe existing literature frequently mentions negative impli-\ncations, i.e., risks, for the science system as ethical issues, \nand also mixes ethical and legal aspects. Issues are raised on \nhow to understand ‘authorship’ in the research context, be it \nas accountability, as a substantial contribution to a text, as \nownership in contrast to plagiarism, and with respect to text \nand language improvement, as Staiman (2023) argues in a \nguest post for The Scholarly Kitten. Critics argue that chat-\nbots cannot take responsibility for the content they produce \nand cannot be held accountable (Corless 2023; Liebrenz \net al. 2023). In addition, their ability to generate quality aca-\ndemic research ideas “raises fundamental questions around \nthe meaning of creativity and ownership of creative ideas” \n(Lucey and Dowling 2023 n.p.), which in turn sparks ques-\ntions about originality, scholarly citation practices and the \nboundary to plagiarism (Lund et al. 2023; Tomlinson et al. \n2023). It, thus, comes as no surprise that publishers like \nSpringer Nature have banned ChatGPT and similar software \nfrom being given authorship on papers, a position shared \nby many scientists, according to Stokel-Walker (2023), and \nScience  editors even have prohibited the use of any text \ngenerated by those tools. Many commentators have raised \nconcerns about the implications of the LLMs producing \ninaccurate or misleading output and the potential spread of \nmisinformation (Dwivedi et al. 2023; Liebrenz et al. 2023). \nSimilar ethical concerns are raised regarding the potential of \nthese tools to reproduce and amplify bias, both in the train-\ning data and the development process, and the implications \nof this for the integrity of science (Lund et al. 2023). There \nhave been techno-solutionist claims that potential harms of \nsuch systems can be mitigated by watermarking their output \n(Kirchenbauer et al. 2023). Additional ethical considerations \ninclude the potential to replace humans in the scholarly work \nprocess (Lund et al. 2023). This includes positions that were \nthought to be less likely to be automated until a few years \nago (Dwivedi et al. 2023). Furthermore, the commercializa-\ntion of these tools would exclude scholars and institutions in \nlow-income and middle-income countries, thus entrenching \nexisting inequalities in knowledge dissemination and schol-\narly publishing (Liebrenz et al. 2023).\nThere is a perceived lack of regulation, or at least clear \nregulatory guidance for LLMs and related tools, on issues \nsuch as privacy, se curity, accountability, copyright viola-\ntions, disinformation, misinformation, and other forms of \nabuses and misuses of LLMs and LLM-based tools (Dwivedi \net al. 2023; Khowaja et al. 2023; Lund et al. 2023). After \nthe Italian Data Protection Authority imposed an immedi-\nate temporary limitation on the processing of Italian users’ \ndata by OpenAI in late March 2023 to enforce demands on \nthe protection of data subjects’ rights, as outlined in their \npress release (GPDP 2023 ), other national data protection \nauthorities in Europe have followed suit and opened pro-\nceedings against OpenAI, reports Sokolov (2023). European \ndata protection authorities have even set up a task force to \ncooperate and exchange information on enforcing EU laws \non OpenAI, according to a news report (Goujard 2023). At \nthe same time, the European Parliament called for expanding \nthe potential reach of the proposed EU AI Act by includ-\ning ChatGPT-like systems to the list of high-risk categories \nof AI systems (Helberger and Diakopoulos 2023). Further -\nmore, Hacker et al. ( 2023) call for specific regulation of \nLLM-based tools, “large generative AI models”, under the \nEU Digital Services Act and provide four concrete, workable \nsuggestions that include transparency obligations, manda-\ntory yet limited risk management, non-discrimination data \naudits, and expanded content moderation.\n3  Methodology\nTo address our research objective, we employed the Delphi \nmethod. First developed in the 1960s, the Delphi method \nis a technique used to establish consensus among a group \nof experts on complex issues (Landeta 2006) and in some \ncases used to forecast future developments (Linstone and \nTuroff 1975). In its basic form, this method can be described \nas a communication process that involves engaging experts \nat various stages, such as through surveys and qualitative \ninterviews. The initial stage is open and exploratory, with \nthe information gathered analyzed and used to inform sub-\nsequent data collections. This process continues until con-\nsensus is reached among experts, for example, in defining \nconcepts and/or trends or weighing different viewpoints. In \nthis light, the Delphi method is a fitting technique to inves-\ntigate our objective of exploring the impact of ChatGPT and \nLLMs on scientific practices and the science system.\nIn our Delphi approach, we conducted two surveys. In \nthe first survey, we mainly used open questions along the \nresearch questions to derive a category system. This cat -\negory system was the basis for the second survey, where we \nused the sub-categories to create closed questions for better \ncomparison and quantitative evaluation. Our target audience \nwere researchers working on topics that crosscut science, \ntechnology, and society, who had an interest in LLMs. Using \nconvenience sampling, we recruited participants via our pro-\nfessional networks, including using institutional mailing lists \nand associations, e.g., the Network of Centers, an interna-\ntional association of internet research centers.\nThe first survey consisted of 12 open questions with the \ngoal of understanding the impact of ChatGPT and LLMs on \nacademic work, scientific practices, and the science system. \nIn total, we collected 72 responses from researchers holding \nvarious positions and from diverse disciplinary backgrounds \n451AI & SOCIETY (2025) 40:447–459 \n(see Fig. 1). The responses of the first survey were primarily \ncoded by two authors. In a first step, they examined 25% of \nthe responses to generate a codebook through a combina -\ntion of inductive and deductive coding (Bazeley 2009). In a \nsecond step, the codebook was then evaluated by all authors \nand adjustments were made when needed and the rest of \nthe material was coded (for codebook see Online Appendix \ntables 7 and 8).\nBased on the results and, in particular, the codebook from \nthe first round, the second survey was created consisting of \n11 questions, the majority of which were ranking questions \nfeaturing the identified codes for applications and limita-\ntions, risks and opportunities for the science system and the \ncompetencies needed for using LLMs, as well as general \nopinion questions on LLMs impact on science and scientific \npractice. Furthermore, the survey instrument contained two \nopen questions on future scenarios.\nThe survey was sent to the same experts, yielding 52 \nresponses (72% of the participants from the first round). A \nstatistical analysis was conducted on the opinion and ranking \nquestions. In the result tables (see Tables 2–6 in the Online \nappendix), we provide the individual frequencies for each \nitem and rank, as well as two scores. The first score (sum) \nis a simple sum of the preceding frequencies, the rank is a \nweighted sum, where the first rank is weighted by factor four \nand the second rank by factor two. The rank questions are \nfollowed by a set of statements, which the participants could \nevaluate on a five-point Likert scale (strongly disagree, disa-\ngree, neither agree nor disagree, agree, strongly agree). We \ncombined agree and strongly agree to sort the items and will \nalso refer to the combination of both, when reporting it in the \ntext. The open questions were analyzed with a combination \nof inductive and deductive coding, carried out jointly by the \nauthors. Our Delphi approach allowed us to identify and \nrefine various implications of LLMs on the science system; \nhowever, it was not without its limitations. For example, we \nwere unable to track long-term implications as the interval \nbetween the data collections were relatively short.\nWe sought consent prior to each survey phase to publish \nthe responses, aiming to enhance the transparency of our \nresults and enable future research and educational use. The \ndata (including the survey instruments) is published under \na CC-BY-license and can be accessed via the following link.\n4  Results\nBelow, we present the Delphi study results based on the \ndefined aspects, i.e., applications and limitations, risks, and \nopportunities for the science system, competencies as well \nas legal and ethical implications. In each section, we begin \nby presenting the coded findings from phase one and use the \nresults of the ranking and opinion questions to contextualize \nand weigh these results, when applicable. Figure 2 displays \nthe results of the opinion questions, which we will refer to \nin the subsequent result sections. The results of the rank -\ning questions analysis can be found in the Online appendix \n(Tables 2–6).\n4.1  Applications and limitations in use: LLMs \nas enhancement tools\nThe first phase yielded six distinct applications that can be \neffectively addressed by LLMs and LLM-based applications. \nThese include (1) text improvement, which involves the \nrephrasing and optimization of textual content, (2) text sum-\nmary, which involves the summarization of information, (3) \ntext analysis, such as the use of sentiment analysis or quali-\ntative coding, (4) code writing, which involves assistance \nin programming tasks, (5) idea generation, which involves \ngenerating new ideas through the combination of concepts, \nand (6) text translation, which includes the translation of a \nFig. 1  Initial sample (round one) of our Delphi study. Overview of participants by discipline and professional status (n = 72)\n452 AI & SOCIETY (2025) 40:447–459\ntext entered into the LLM in different languages. Ideally, \none respondent argued, this ‘time-saving’ potential could \nbe harnessed to jumpstart the writing process:\n“I think if we can figure out how to use it properly, \nit can be a good writing aid, for instance to get over \nwriter's block. It can also be good at explaining things \nholistically, since it synthesizes a lot of material, and \ncan thus be a tool to explore a subject.”\nNotably, the identified applications of LLMs extend \nbeyond conventional text-based tasks in scientific publish -\ning, although such tasks remain a dominant practice in the \nresponses.\nIn the second round of the Delphi survey, we asked the \nexperts to prioritize the identified applications. Our results \nshow that text improvement is considered the most impor -\ntant application, followed by text summary as the second \nmost important, and code writing as the third most important \napplication. Most (59.6%) of the experts either already use \nor express their intention to use LLMs in their own work \n(Fig. 2). A significant portion (86.5%) of the experts per -\nceive LLMs as valuable for administrative tasks, confirming \nthe assumption that time savings are expected for researchers \nthrough LLM utilization (Fig. 2).\nAsked about the limitations of LLMs in scientific \nwork, five distinct types of limitations were mentioned. \nWe observed (1) lack of transparency, as it is unclear on \nwhich data the model's outputs are based on, (2) incorrect-\nness, especially regarding literature references and bio-\ngraphical information, which may affect the reliability of \nthe generated text, (3) lack of creativity, as ChatGPT relies \nheavily on existing patterns and may struggle to generate \nentirely new content, (4) outdatedness, particularly as the \nversion of ChatGPT used in this study relies on a database \nthat only goes up to 2021, and (5) unspecificity, i.e., LLMs \nproduce superficial texts that do not address topics in depth \nor detail. One respondent recounted their experimentation \nwith ChatGPT which illustrates the unreliability of LLM-\ngenerated text:\n“I have played with ChatGPT a lot recently, and I have \ntried to ask it to perform various scientific tasks of \ndifferent complexities, for example explaining in plain \nwords a very complicate scientific topic or, conversely, \nexplaining it in a very \"sciency\" way, including equa-\ntions and references to corroborate explanations. If on \none hand, it performed remarkably well in simplifying \nhard science into plain words (often naively, but that's \nok), it performed really poorly when explaining a topic \nin detail, providing the wrong/incorrect/incomplete \nset of equations as well as MAKING UP references. \nInterestingly, ChatGPT uses names of real people well \nknown in the subject field, it mix[es] and match[es] \nthem, creat[ing] a fake reference title…”\nIn the second round of the study, participants were \nrequested to rank the limitations. The highest-ranked limi-\ntation was incorrectness, followed by non-transparency  \nand unspecificity in the responses. The incorrectness of \nLLMs was a dominant and recurring issue mentioned by \nthe experts. As one expert stated, “The largest problem I \nFig. 2  Statements on LLMs, formulated based on the results from the first round of our Delphi study and quantified in the second round\n453AI & SOCIETY (2025) 40:447–459 \nsee are the factual mistakes, often given with confidence, \nwhich make it hard to trust ChatGPT and similar technology \noutputs without further research or prior knowledge\".\nThe results indicate that the potential benefits of LLMs \nlie not only but primarily in text-based work, which is sig-\nnificant because scientific value creation in most disciplines \nis text based. There is also evidence to suggest that LLMs \nare relevant for ideation, conception, and programming, the \nlatter of which is an increasingly important scientific prac-\ntice. Taken together, it is not surprising that a majority of \nthe respondents assume that ChatGPT and other LLMs will \ntransform scientific practice, although this might—at this \nstage—relate primarily to the textuality of academic work. \nThe limitations mentioned can be essentially explained by \nthe databases that existing LLMs were trained on, and it can \nbe assumed that many of these limitations can be addressed \nin newer models, as some respondents pointed out. However, \nthe non-transparency in the training data remains problem-\natic and was viewed by some as inconsistent with scientific \nprinciples of quality.\n4.2  Risks and opportunities for the science system: \nadvantages trump disadvantages\nAccording to the experts, the use of LLMs provides the \nscience system with four opportunities: (1) LLMs can pro-\nmote efficiency  by automating and supporting text work, \n(2) LLMs may promote reflection by identifying biases and \nnew research areas, (3) LLMs may reduce administrative \nworkload, (4) LLMs can promote inclusiveness by leveling \nthe playing field between researchers from different back -\ngrounds and institutions, such as those who lack resources \nfor grant writing or those who are non-native English speak-\ners, and (5) LLMs promote productivity by freeing up time \nfor researchers to conduct more analyses or produce more \nscientific articles. In the second phase of the Delphi study, \nthe experts ranked these, with the reduction of administra-\ntive tasks ranked first, followed by more efficiency and inclu-\nsiveness (see Table 4). These results indicate that researchers \nsee LLMs primarily as a tool to relieve and simplify their \nworkload. Hence, a large majority of the experts disagrees \nthat LLMs could replace researchers (82.7%, Fig.  2).\nThe analysis of the first phase of the Delphi study reveals \nthe existence of seven distinct risks associated with the use \nof LLMs in scientific work. These risks include (1) rein-\nforce bias / dominant voices, because statistical systems \nfavor mainstream opinions, (2) overburden academic qual-\nity assurance mechanisms with semi-automated papers, (3) \nreinforce inequalities between researchers who have access \nto LLMs and those who do not, (4) increase dependence  \non commercial providers, (5) encourage academic miscon-\nduct, either intentional or unintentional by researchers, (6) \nlead to a decrease in originality due to the generic nature of \nLLM-generated text, and (7) the possibility to an increase in \ndisinformation, which could potentially challenge scientific \ntruths in the public domain. One respondent summarized \nthese key problems for scientific practices in the following \nquote:\n“ChatGPT and possibly other large language models \nmay make production of plausibly looking, but false \ncontent easy and low-cost. This presents significant \nrisks and could lead to overload of the peer-review \nsystem. It could also homogenise scientific outcomes, \nreducing breakthrough innovations.”\nIn the second phase, the experts ranked these, indicating \nthat bias is seen as the biggest threat, followed by disin-\nformation and overburdening academic quality assurance \nmechanisms (see Online appendix Table 5). These risks are \nsignificant as they touch on fundamental pillars of scien-\ntific ethics and good practice, such as scientific freedom \nregarding the dependence on commercial publishers, sci-\nentific quality assurance concerning the handling of highly \ngeneric publications, as well as the public legitimation of \nscience, which could be put into question by plausible and \nseemingly scientific nonsense produced by LLMs—large \nmajority of the experts (75.0%) regard LLMs as a catalyst \nfor disinformation (Fig.  2). Notwithstanding the gravity of \nthe aforementioned risks, the majority of experts perceive \nthe benefits of LLMs to outweigh the drawbacks (Fig.  2), \nwhich explains why most of them already use or intend to \nuse LLMs in their work. This, however, can also be attrib-\nuted to the sampling strategy employed in this study, pos-\nsibly involving technology-proficient experts. This result \nis noteworthy nonetheless and supports the hypothesis that \ngenerative AI will change scientific work in the long run.\n4.3  Competencies in usage: scientists need to learn \nto (re)think\nIn the inquiry regarding the competencies required for \nresearchers to utilize ChatGPT and other LLMs, the \nrespondents pointed out four distinct competencies, namely \n(1) technical know-how to comprehend the inner workings \nof LLMs, (2) the ability to contextualize results utilizing the \noutcomes generated by LLMs in practical scenarios, (3) a \nreflective mindset to consider the feedback effects on scien-\ntific practice, and (4) ethical understanding to responsibly \nemploy LLMs. In the second phase, they ranked a reflective \nmindset first, followed by the ability to contextualize results \nand ethical understanding. One respondent succinctly put it \nscientists need “common sense”, understanding that:\n“This is a tool that paraphrases its original knowledge \nand has (as of yet) no evaluation of the quality of its \nown answer. Which is not an real issue when you use \n454 AI & SOCIETY (2025) 40:447–459\nit for valentine cards (well, actually...) , but is when \nyou take that output as factual knowledge (publication, \nprotocols, ...) The more the result is meant to be formal \nfactual knowledge, the more it should be considered \nsuspect and reviewed by humans.”\nThe results indicate that the experts anticipate feedback \neffects on science, while also suggesting that the responsible \napplication of knowledge will become even more paramount \nin the future.\nIt can be argued that reflexivity highlights the ethical \nimplications of AI on scientific practices and ways to pro-\nactively address them, while contextuality focuses on the \npractical use of AI-supported findings and strategies for \nmaximizing their utility. Our findings suggest that genera-\ntive AI should be incorporated in scientific training and sci-\nence education, specifically in relation to scientific ethics \nand effective communication of AI-driven results in their \nappropriate context.\n4.4  Ethical and legal implications: clear need \nfor regulation\nThe answers in the first phase allow to discern five ethical \nimplications, namely (1) the need for accountability in rela-\ntion to the outcomes produced by LLMs, (2) the question of \noriginality with regards to human creativity (e.g., concerns \nof plagiarism arise), (3) the sustainability issue regarding the \nenvironmental effects of LLMs, (4) the potential exclusion \nof researchers who lack access to LLMs, raising concerns \nabout universalism, and (5) the issue of autonomy, in which \nresearchers may become overly dependent on (commercial) \nAI tools. The comments show clearly that the majority deem \nChatGPT unfit for authorship due to its inability to assume \nresponsibility for the results.\nThe experts perceive legal implications regarding (1) \ncopyright, due to the unclear infringement of intellectual \nproperty by LLMs, (2) data protection, due to the ambiguity \nof the data used and how OpenAI utilizes input data, and (3) \nliability, due to the uncertainty of the extent to which LLMs \ncan be held responsible for criminal errors. A large majority \nof the experts (63.5%) believe that LLMs should be subject \nto stronger regulations (Fig.  2).1 However, all these issues, \none respondent pointed out, need to start with a discussion \non how we view AI tools:\n“Do we consider the models as a human-like some -\nthing or as a tool? This affects all ethical and legal \naspects.”\nThe initial round of the Delphi survey revealed that the \nethical implications discussed frequently underscore the \nsignificance of the human element in scientific endeavors. \nThis includes the responsibility and accountability of indi-\nviduals for their contributions, the value of creativity and \ngenerating novel ideas, ensuring equitable access to science \nand the scientific community, and addressing the potential \nrisk of dependency on LLM-based tools that may hinder \nindividual skills and capabilities in scientific work. The \namount of energy that is necessary both for training models \nand running inference and the  CO2 footprint are mentioned \nas primary examples for the ecological sustainability issues \nChatGPT and the like present. Taking into account that \nLLMs are trained on works produced by others and produce \n(or co-produce) works, both of which almost certainly fall \nunder copyright law, it is not surprising that a large majority \nof the experts identify issues with copyright law as a press-\ning legal implication. The lack of transparency regarding the \npersonal data on which the LLMs were trained, but also the \nfurther possible uses of personal data generated by the use of \nthe tools, certainly explains why many respondents identify \nprivacy and data protection law issues as considerable legal \nchallenges. Whereas accountability is identified by many \nexperts as a key ethical challenge, this does not carry over \nto the legal principle of liability that builds on it, which is \nmentioned by relatively few respondents.\n4.5  Transformative and deformative scenario\nIn the first phase of the Delphi, we consulted with experts \nto ascertain the potential impact of LLMs on scientific prac-\ntice within the next 5–10 years. In the subsequent phase, \nwe investigated the potential influence of generative AI on \nthe relationship between science and society. Based on the \nanswers to these questions, our study reveals two possible \nscenarios, namely (1) a utopian transformative scenario and \na (2) dystopian deformative scenario. It is noteworthy that \nthe negative scenario is almost a negation of the positive \nscenario and vice versa. However, overall, there are signifi-\ncantly more indications (in terms of the number of codes) for \na positive scenario, which was also confirmed by the opinion \nbattery in Phase 2.\nIn the utopian scenario, integrating generative AI into \nscientific practices offers transformative potential, overcom-\ning path dependencies in scientific practice and accelerat-\ning scientific and societal progress. Our analysis identifies \nthree key aspects of its impact on science: (1) streamlining \nrepetitive tasks, (2) promoting inclusivity, and (3) facilitat-\ning interdisciplinary research. The experts propose that gen-\nerative AI could automate administrative and generic tasks, \nfreeing up time for critical reflection, analysis and innova-\ntion. In this light, one respondent explained:1 We did not ask the participants to rank the ethical and legal impli-\ncations in the second round of the Delphi survey.\n455AI & SOCIETY (2025) 40:447–459 \n“Generative AI will have a tremendous impact on the \nway that research is done. Mostly with regard to improv-\ning productivity and efficiency. More repetitive and \nmonotonous tasks will be outsourced to the AI models, \nwhile humans can focus more strongly on contextualis-\ning, research design, and creative thinking.”\nIt may democratize access to scientific resources, foster \ndiversity of voices and collaboration, and aid in discovering \nconnections across different schools of thought. The integra-\ntion of generative AI tools aligns research with societal chal-\nlenges, driving technological development and supporting \nevidence-based decision-making. Effective science commu-\nnication and education are enabled through AI-driven tools. \nThis collaborative approach propels scientific advancements \ntoward innovative solutions.\nIn a dystopian scenario, the anticipated positive impacts \nof generative AI are largely negated, as our analysis reveals \nthree crucial aspects: (1) a decline in research quality due \nto plausible yet flawed results, compromising reliability and \nvalidity; (2) a loss of research diversity through amplifying \nmainstream voices, resulting in missed opportunities for novel \nperspectives; and (3) a decrease in scientific integrity, as the \nease of producing AI-generated content raises risks of rein-\nforcing predatory publishing practices and disseminating false \ninformation, leading to confusion and distrust.\n“…. there's a greater deluge of predatory publishing \npractices being driven by LLM, that is that trust in sci-\nence will decrease. People will then have to become suf-\nficiently familiar with the scientific system to be able to \njudge the scientific merit of a paper, which can be a real \nchallenge. Surely some scientific journalists will be able \nto help filter out (some) problematic papers from reach-\ning newspapers and from reaching the broader public. \nBut with material being increasingly openly available, \nand directly engaged with by society, if there are more \nsuch problematic papers, this makes it increasingly dif-\nficult for the lay public to assess the paper's merits…”\nThe perpetuation of plausible nonsense could further have \nnegative consequences for society when policy decisions or \npublic opinions rely on unreliable information.\nAdditionally, dependence on commercial providers for gen-\nerative AI tools raises concerns among experts about the lack \nof independence and control over scientific research, poten-\ntially leading to conflicts of interest and biases in research \nresults.\n5  Discussion\nThe aim of this study was to investigate the impact of \nChatGPT and other LLMs on the science system and sci-\nentific practices by examining their potential applications, \nlimitations, effects, ethical and legal considerations and \nthe necessary competencies needed by users. To date, \nscholars have primarily focused on the implications of \nLLMs on education (e.g., Perkins 2023; Fyfe 2023) with \nlimited attention being paid to their impact on science and \nscientific practices (for exceptions, see Chubb et al. 2022; \nRibeiro et al. 2023 ). The overnight popularity ChatGPT \nexperienced since its debut in November 2022 stressed \neven more the necessity to evaluate the implications of \nLLMs for science and scientific practice. To examine these \nimplications, we employed a two-stage Delphi method, \nwhich included inviting experts, researchers working in \nthe fields of science, technology and society to participate \nin two surveys as means to identify and refine the impact \nof LLMs on the science system and scientific practices.\nAt the time of the second round of our Delphi method, \nless than half a year had passed since the first preview of \nChatGPT. Accordingly, it is difficult to make concrete pre-\ndictions about the potential capabilities of future versions \nof LLMs like ChatGPT. Nevertheless, our study presents \na consistent picture from experts which can further our \nunderstanding of future expectations of LLMs. We were \nalso able to identify patterns emerging regarding potential \nopportunities and risks. It is important to note the major -\nity of the experts saw no danger that LLMs will replace \nresearchers in the foreseeable future. They share this \nexpectation with researchers in the field of AI and work, \nwho in their optimistic scenarios expect a shift in tasks, \nthe creation of new tasks and an emergence of new work \nprofiles rather than the replacement of workers (Deranty \nand Corbin 2022).\nOverall, the experts in our study were optimistic and \nin agreement that the advantages of this technology out-\nweigh their disadvantages. This optimism was paired with \nconcerns, which allow us to paint a nuanced picture of \nthe potential positive and negative implications of LLMs. \nIn general, ChatGPT and other LLMs were collectively \nunderstood as potential ‘time-savers’ to be used to improve \nand streamline the writing process, especially academic \nwriting. For example, text improvement as in the rephras-\ning and optimization of textual content was considered the \nmost important application. This outcome resonates with \nthe scholarly discourse which highlights how generative \nAI can be used to enhance texts, such as with brainstorm-\ning (Staiman 2023), crafting literature reviews (Lucey and \nDowling 2023), and improving text clarity (Lund et al. \n2023). At the same time, experts in our study were aware \n456 AI & SOCIETY (2025) 40:447–459\nof the limitations of LLMs and cited similar apprehen-\nsions to those raised in the literature (Dwivedi et al. 2023; \nOpenAI 2022; Perkins 2023). The experts highlighted key \nshortcomings such as AI- produced texts may have incor -\nrect information, their origin and referencing is non-trans -\nparent and that they lack specificity, shortcomings which \nare at odds with the principles of good scientific practice. \nIt is not surprising that our study reinforced text-based \napplications and limitations for LLMs identified in the \nscholarly discourse, as text production is a key scientific \npractice. However, this focus may likely shift in the future \nas more usages of LLMs are explored.\nIn addition, our study indicates that LLMs have the poten-\ntial to reshape the science system. The experts anticipate that \nthey will lead to more efficient workflows, with the reduction \nof administrative tasks being ranked the highest anticipated \nchange. This forecast supports claims made by other schol-\nars, who argue that LLMs will help automate mundane tasks \nand free up space for creative thinking (Lund et al. 2023; \nDwivedi et al. 2023). Other changes LLMs bring to the sci-\nence system are, however, more complex. For example, our \nfindings point to a double-edged sword embedded within the \nLLM constellation: this technology could serve to both pro-\nmote inclusion and reinforce biases. On the one hand, LLMs \ncan level the playing field for non-English speakers as they \ncan provide editorial support, but on the other hand, they can \nalso increase inequalities by drawing on mainstream opin-\nions and widening the gap between those who have access to \nthese technologies and those who do not. This multifaceted \nconcern was also echoed by other scholars (Corless 2023; \nLiebrenz et al. 2023).\nThe most pressing fear we identified is that LLMs perpet-\nuate disinformation and will overburden quality assurance \nmechanisms in academia. In other words, \"scientific truths\" \ncould be in greater competition with plausible nonsense than \nthey already are. Similar thoughts are discussed by other \nscholars (Grimaldi and Ehrler 2023; Lund et al. 2023), with \nthese changes being described in revolutionary terms in \nwhich LLMs are positioned as the great ‘game-changers’ of \nacademia. In contrast, experts in our study were more cau-\ntious with such claims seeing these changes as more incre-\nmental and pragmatic.\nMoreover, our study provided insights into the compe-\ntencies researchers need to be able to utilize LLMs. In line \nwith scholars such as Teubner et al. (2023), experts in our \nstudy voiced concerns that ChatGPT and other LLMs have \nthe potential to widen the digital divide between researchers \nwho possess technical know-how and researchers who do \nnot. Furthermore, the experts pointed out that the research-\ner’s role in the writing process will shift from being the \noriginator of ideas and texts to being required to contextu-\nalize and reflect on AI-generated results. This change will \nentail a new way of thinking about key scientific practices \nand the role the individual academic plays in them. Our \nexperts also expressed the importance of researchers hav -\ning an ethical understanding, e.g., using AI in a responsible \nmanner. A point that was only marginally addressed in the \nliterature (Dwivedi et al. 2023). Underlying these findings \nis the understanding that it is up to the individual academic \nto ensure that they have the skills and knowledge needed \nto navigate these technological changes. Such a stance can \ncontribute to furthering digital divides due to preexisting \nuneven digital literacy between academics, institutions, and \nhigher education systems.\nThis study aided in disentangling the ethical and legal \nimplications of ChatGPT and other LLMs. The findings pro-\nvide additional clarity on the matter of authorship concern-\ning AI utilization, a subject also explored by other scholars \n(Lucey and Dowling 2023; Tomlinson et al. 2023). Even \nif our perspectives on authorship and language are not to \nbe understood as outdated, as some scholars argue (e.g., \nCoeckelbergh and Gunkel 2023), they should at least be \nreconsidered in light of the potential impact of ChatGPT \nand similar LLM-based tools on our conventional under -\nstandings (Gellers 2023). The majority of the experts deem \nthat LLMas cannot claim authorship due to its inability \nto assume responsibility for its actions. In this light, the \nexperts centered on distilling the role humans play in being \naccountable for their usage of LLMs, taking into considera-\ntion issues such as plagiarism, copyright and data protection. \nThus, they underlined that human responsibility in AI usage \nis both a legal and ethical challenge, a sentiment that echoes \nthe arguments of critics who postulate that chatbots cannot \ntake responsibility for their actions (Corless 2023; Liebrenz \net al. 2023). In addition, the issue of access was highlighted \nas an ethical dilemma, that is, not all researchers will have \nequal access to such technologies, potentially furthering \ninequalities. Furthermore, the  CO2 emissions generated by \nthese use of AI technologies poses environmental risks (Hao \n2019). The complexities of these ethical and legal implica-\ntions show the need to take diverse issues into account when \nit comes to regulating the usage of LLMs in academia.\nLastly, our study presents potential future pathways for \nAI and its impact on the science system and society in the \nform of future scenarios constructed from our data. In the \npositive transformative scenario, the integration of LLMs \nin scientific practice holds great potential for improving sci-\nentific productivity, efficiency, education, communication, \ncreativity, and discovery. In other words, LLMs can auto-\nmate repetitive tasks, allowing researchers to allocate more \ntime and resources to analytical and innovative work. It is \nthe prevailing perception of the experts in our study that sug-\ngests that this scenario is more likely to occur. However, it \nis crucial to acknowledge the potential negative deformative \nscenario. Experts raised concerns about the impact of gen-\nerative AI on scientific quality, integrity, and the scientific \n457AI & SOCIETY (2025) 40:447–459 \necosystem. Issues such as decreased scientific rigor, repro-\nducibility, and a potential homogenization of science were \nhighlighted. In addition, the reliance on generative AI mod-\nels without proper validation may lead to a decrease in criti-\ncal thinking and creativity.\nWe can strive to ensure the positive scenario by address-\ning the concerns highlighted in our study. In this regard, \nstriking a balance between embracing the benefits of LLMs \nand upholding scientific principles is crucial. Accordingly, \nwe should remember our scientific tools, the good prac-\ntices of scientific work, and create appropriate frameworks \nand conditions that enable us to make use of the diverse \nopportunities these technologies might have to offer. At the \nsame time, we must withstand any attempt to compromise \nthe quality standards that we as a science community have \nestablished and which distinguishes the scientific discourse.\nIn conclusion, while the transformative scenario holds \ngreat promise for the positive impact of LLMs on the science \nsystem and society, it is imperative to proactively address \nthe potential risks and challenges to ensure that the integra-\ntion of generative AI in science is guided by ethical consid-\nerations, scientific integrity, and a commitment to societal \nbenefit.\n6  Conclusion\nOur study highlights the great potential generative AI has \nfor transforming the science system. This adds a new and \nfresh direction to the LLM discussion, which has focused \nheavily on the impact of this technology for individual aca-\ndemics and learners (Fyfe 2023; Perkins 2023). Generative \nAI, exemplified by LLMs, presents a transformative prospect \nfor the social organization of science. LLMs' text generation \ncapabilities have implications for scholarly communication \nand knowledge dissemination, potentially redefining con-\nventional norms such as the academic reputation or quality \ncontrol system. For instance, regarding the reputation sys-\ntem, citation-based metrics, pivotal for scholarly progress, \ncould conceivably diminish in value if scientific articles \ncould be predominantly generated through automation. \nPotential avenues might encompass a heightened empha-\nsis on micro-publications or alternative measures of suc-\ncess (e.g., peer recognition, grant acquisitions, performative \naccomplishments such as presentations, etc.). In the context \nof an increasingly automated content generation landscape, \nthe dynamics of scholarly evaluation warrant exploration to \nensure the relevance and robustness of metrics within evolv-\ning scholarly communication paradigms. Regarding qual-\nity control, it appears plausible that the quality assurance \nsystem might experience increased strain due to a potential \nrise in LLM-assisted articles, or conversely, that the imple-\nmentation of generative AI could be employed for quality \nassurance purposes. Furthermore, these tools offer the pros-\npect of ameliorating administrative burdens on researchers, \nthus reorienting focus toward analytical facets of scientific \nendeavors. However, this theoretical promise is counterbal-\nanced by the instantaneous generation of potentially mis-\nleading content, raising concerns about scientific integrity \nand the nature of evidence. Consequently, the integration of \ngenerative AI necessitates nuanced evaluation and recalibra-\ntion of policies to ensure judicious incorporation, thereby \nmolding the future contours of scientific inquiry. The \nbroader societal ramifications of integrating LLMs demand \nthat the scientific community meet its responsibilities to \nsociety, engage in open and public discussions on the ethi-\ncal considerations related to these technologies, and identify \nsuitable proactive regulation approaches.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s00146- 023- 01791-1.\nAcknowledgements We express our gratitude to the participating \nexperts, the majority of whom have given their consent to be named in \nthe second phase of the Delphi. These experts are Alexander Terenin, \nAlison Kennedy, Anaëlle Gonzalez, André Vellino, Andrea Klein, \nBenjamin Tan, Brigitte Mathiak, Christian Gagné, Christian Vater, \nDaniel Guagnin, Debora Weber-Wulff, Ekaterina Hertog, Eva Seidl-\nmayer, Evgeny Bobrov, Fabro Steibel, Fiona Kinniburgh, Florian Hoff-\nmann, Georg von Richthofen, Graham Taylor, Hadi Asghari, Hendrik \nSend, Ingrid Richardson, Johannes Breuer, Katharina Mosene, Klaus \nGasteier, Marina Gavrilova, Mark Spektor, Martin Schmidt, Maximlian \nHeimstädt, Mike Thelwall, Naireet Gosh, Natalie Sontopski, Philipp \nMehl, Richard Boire, Robert Lepenies, Ronny Röwert, Sebastian Mor-\naga Scheuermann, Thorsten Thiel, Tony Ross-Hellauer, Vince I. Madai, \nVincent Traag, Wojciech Hardy, Zining Zhu. We also extend our thanks \nto the 9 experts who preferred to remain anonymous. We furthermore \nwould like to extend our appreciation to our colleagues from the Global \nNetwork of Internet and Society Research Centers (https:// netwo rkofc \nenters. net/) for their invaluable assistance in the recruitment of experts \nand their insightful contributions in shaping initial ideas.\nFunding This study was funded and primarily conducted by the Alex-\nander von Humboldt Institute for Internet and Society.\nData availability The datasets generated during and/or analyzed during \nthe current study are available on Zenodo repository, https:// doi. org/  \n10. 5281/ zenodo. 80094 29\nDeclarations \nConflict of interest The authors and affiliated institutions declare no \nconflicting interests.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \n458 AI & SOCIETY (2025) 40:447–459\nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nBazeley P (2009) Analysing qualitative data: more than ‘identifying \nthemes.’ Malays J Qual Res 2(2):6–22\nBeer D (2019) Should we use AI to make us quicker and more efficient \nresearchers?. Impact of Social Sciences—LSE Blog. https:// blogs. \nlse. ac. uk/ impac tofso cials cienc es/ 2019/ 10/ 30/ should- we- use- ai- to- \nmake- us- quick er- and- more- effic ient- resea rchers/\nBender EM, Gebru T, McMillan-Major A, Shmitchell S (2021) On \nthe dangers of stochastic parrots: can language models be too \nbig?. In: Proceedings of the 2021 ACM Conference on fairness, \naccountability, and transparency, pp 610–623. https:// doi. org/ 10. \n1145/ 34421 88. 34459 22\nBuruk O “Oz” (2023) Academic writing with GPT-3.5: reflections on \npractices, efficacy and transparency. arXiv http:// arxiv. org/ abs/ \n2304. 11079\nChubb J, Cowling P, Reed D (2022) Speeding up to keep up: exploring \nthe use of AI in the research process. AI & Soc 37(4):1439–1457. \nhttps:// doi. org/ 10. 1007/ s00146- 021- 01259-0\nCoeckelbergh M, Gunkel DJ (2023) ChatGPT: deconstructing the \ndebate and moving it forward. AI & Soc. https:// doi. org/ 10. 1007/ \ns00146- 023- 01710-4\nCorless V (2023) ChatGPT is making waves in the scientific litera-\nture. Advanced Science News. https:// www. advan cedsc ience  \nnews. com/ where- and- how- should- chatg pt- be- used- in- the- scien \ntific- liter ature/\nDeranty JP, Corbin T (2022) Artificial intelligence and work: a criti-\ncal review of recent research from the social sciences. AI & Soc. \nhttps:// doi. org/ 10. 1007/ s00146- 022- 01496-x\nDillion D, Tandon N, Gu Y, Gray K (2023) Can AI language models \nreplace human participants? Trends Cognit Sci 27(7):597–600. \nhttps:// doi. org/ 10. 1016/j. tics. 2023. 04. 008\nDowling M, Lucey B (2023) ChatGPT for (Finance) research: the \nBananarama Conjecture. Financ Res Lett 53:103662. https:// doi. \norg/ 10. 1016/j. frl. 2023. 103662\nDwivedi YK, Kshetri N, Hughes L, Slade EL, Jeyaraj A, Kar AK, \nBaabdullah AM, Koohang A, Raghavan V, Ahuja M, Albanna \nH, Albashrawi MA, Al-Busaidi AS, Balakrishnan J, Barlette Y, \nBasu S, Bose I, Brooks L, Buhalis D, Wright R (2023) “So what if \nChatGPT wrote it?” Multidisciplinary perspectives on opportuni-\nties, challenges and implications of generative conversational AI \nfor research, practice and policy. Int J Inform Manag 71:102642. \nhttps:// doi. org/ 10. 1016/j. ijinf omgt. 2023. 102642\nFlanagin A, Bibbins-Domingo K, Berkwits M, Christiansen SL (2023) \nNonhuman “Authors” and implications for the integrity of sci-\nentific publication and medical knowledge. JAMA 329(8):637. \nhttps:// doi. org/ 10. 1001/ jama. 2023. 1344\nFloridi L, Chiriatti M (2020) GPT-3: its nature, scope, limits, and con-\nsequences. Mind Mach 30(4):681–694. https:// doi. org/ 10. 1007/ \ns11023- 020- 09548-1\nFyfe P (2023) How to cheat on your final paper: Assigning AI for \nstudent writing. AI & Soc 38(4):1395–1405. https:// doi. org/ 10. \n1007/ s00146- 022- 01397-z\nGellers JC (2023) AI ethics discourse: a call to embrace complexity, \ninterdisciplinarity, and epistemic humility. AI & Soc. https:// doi. \norg/ 10. 1007/ s00146- 023- 01708-y\nGoujard C (2023) European data regulators set up ChatGPT task force. \nPolitico. https:// www. polit ico. eu/ artic le/ europ ean- data- regul ators- \nset- up- chatg pt- taskf orce/\nGPDP (2023) Intelligenza artificiale: Il Garante blocca ChatGPT. Rac-\ncolta illecita di dati personali. Assenza di sistemi per la verifica \ndell’età dei minori. https:// www. garan tepri vacy. it: 443/ home/ \ndocwe b/-/ docweb- displ ay/ docweb/ 98708 47\nGrimaldi G, Ehrler B (2023) AI et al.: machines are about to change \nscientific publishing forever. ACS Energy Lett 8(1):878–880. \nhttps:// doi. org/ 10. 1021/ acsen ergyl ett. 2c028 28\nHacker P, Engel A, Mauer M (2023) Regulating ChatGPT and other \nLarge Generative AI Models. arXiv https:// arxiv. org/ abs/ 2302. \n02337\nHao K (2019) Training a single AI model can emit as much carbon as \nfive cars in their lifetimes. MIT Technology Review. https:// www. \ntechn ology review. com/ 2019/ 06/ 06/ 239031/ train ing-a- single- ai- \nmodel- can- emit- as- much- carbon- as- five- cars- in- their- lifet imes/\nHarding J, D’ Alessandro W, Laskowski NG, Long R (2023) AI lan-\nguage models cannot replace human research participants. AI & \nSoc. https:// doi. org/ 10. 1007/ s00146- 023- 01725-x\nHelberger N, Diakopoulos N (2023) ChatGPT and the AI act. Internet \nPolicy Rev. https:// doi. org/ 10. 14763/ 2023.1. 1682\nHosseini M, Horbach SPJM (2023) Fighting reviewer fatigue or \namplifying bias? Considerations and recommendations for use \nof ChatGPT and other Large Language Models in scholarly \npeer review. Res Integr Peer Rev 8:4. https:// doi. org/ 10. 1186/  \ns41073- 023- 00133-5\nHosseini M, Rasmussen LM, Resnik DB (2023) Using AI to write \nscholarly publications. Acc Res. https://  doi. org/ 10. 1080/ 08989 \n621. 2023. 21685 35\nJiang M, Breidbach C, Karanasios S (2022) How does artificial intel-\nligence transform knowledge work? PACIS 2022 Proceedings, \n312. https:// aisel. aisnet. org/ pacis 2022/ 312\nKhowaja SA, Khuwaja P, Dev K (2023) ChatGPT Needs SPADE \n(Sustainability, PrivAcy, Digital divide, and Ethics) evaluation: a \nreview. arXiv http:// arxiv. org/ abs/ 2305. 03123\nKirchenbauer J, Geiping J, Wen Y, Katz J, Miers I, Goldstein T \n(2023).A watermark for large language models. arXiv https://  \narxiv. org/ abs/ 2301. 10226\nLandeta J (2006) Current validity of the Delphi method in social sci-\nences. Technol Forecast Soc Chang 73(5):467–482. https:// doi.  \norg/ 10. 1016/j. techf ore. 2005. 09. 002\nLiang W, Yuksekgonul M, Mao Y, Wu E, Zou J (2023) GPT detectors \nare biased against non-native English writers arXiv 2304.02819. \nhttp:// arxiv. org/ abs/ 2304. 02819\nLiebrenz M, Schleifer R, Buadze A, Bhugra D, Smith A (2023) Gen-\nerating scholarly content with ChatGPT: Ethical challenges for \nmedical publishing. Lancet Digit Health 5(3):e105–e106. https:// \ndoi. org/ 10. 1016/ S2589- 7500(23) 00019-5\nLinstone HA, Turoff M (1975) The Delphi method. Addison-Wesley, \nReading\nLucey B, Dowling M (2023) ChatGPT: our study shows AI can produce \nacademic papers good enough for journals—just as some ban it. \nThe Conversation. https:// theco nvers ation. com/ chatg pt- our- study- \nshows- ai- can- produ ce- acade mic- papers- good- enough- for- journ \nals- just- as- some- ban- it- 197762\nLund BD, Wang T, Mannuru NR, Nie B, Shimray S, Wang Z (2023) \nChatGPT and a new academic reality: artificial Intelligence-writ-\nten research papers and the ethics of the large language models \nin scholarly publishing. J Assoc Inform Sci Technol. https:// doi.  \norg/ 10. 1002/ asi. 24750\nOpenAI (2022) Introducing ChatGPT. https:// openai. com/ blog/ chatg pt\nOwens B (2023) How Nature readers are using ChatGPT. Nature \n615(7950):20–20. https:// doi. org/ 10. 1038/ d41586- 023- 00500-8\n459AI & SOCIETY (2025) 40:447–459 \nPerkins M (2023) Academic integrity considerations of AI Large Lan-\nguage Models in the post-pandemic era: ChatGPT and beyond. J \nUniv Teach Learn Pract. https:// doi. org/ 10. 53761/1. 20. 02. 07\nPividori M, Greene CS (2023) A publishing infrastructure for AI-\nassisted academic authoring [Preprint]. Sci Commun Edu. https:// \ndoi. org/ 10. 1101/ 2023. 01. 21. 525030\nRibeiro B, Meckin R, Balmer A, Shapira P (2023) The digitalisation \nparadox of everyday scientific labour: how mundane knowledge \nwork is amplified and diversified in the biosciences. Res Policy \n52(1):104607. https:// doi. org/ 10. 1016/j. respol. 2022. 104607\nSchäfer MS (2023) The Notorious GPT: science communication in the \nage of artificial intelligence. J Sci Commun 22(02):Y02. https://  \ndoi. org/ 10. 22323/2. 22020 402\nSokolov DA (2023) ChatGPT: Deutschlands Datenschützer eröffnen \nVerfahren gegen OpenAI . Heise Online. https:// heise. de/- 89747 \n08\nStaiman A (2023) Guest Post—academic publishers are missing the \npoint on ChatGPT. The Scholarly Kitchen. https:// schol arlyk  \nitchen. sspnet. org/ 2023/ 03/ 31/ guest- post- acade mic- publi shers- \nare- missi ng- the- point- on- chatg pt/\nStokel-Walker C (2023) ChatGPT listed as author on research papers: \nmany scientists disapprove. Nature 613:620–621. https:// doi. org/ \n10. 1038/ d41586- 023- 00107-z\nTeubner T, Flath CM, Weinhardt C, Van Der Aalst W, Hinz O (2023) \nWelcome to the era of ChatGPT et al.: the prospects of large lan-\nguage models. Bus Inform Syst Eng 65(2):95–101. https:// doi. org/ \n10. 1007/ s12599- 023- 00795-x\nTomlinson B, Torrance AW, & Black RW (2023) ChatGPT and Works \nScholarly: Best Practices and Legal Pitfalls in Writing with AI. \narXiv http:// arxiv. org/ abs/ 2305. 03722\nVan Noorden R (2022) How language-generation AIs could trans-\nform science. Nature 605(7908):21–21. https:// doi. org/ 10. 1038/ \nd41586- 022- 01191-3\nWulff K, Finnestrand H (2023) Creating meaningful work in the \nage of AI: explainable AI, explainability, and why it matters \nto organizational designers. AI & Soc. https:// doi. org/ 10. 1007/ \ns00146- 023- 01633-0\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations."
}