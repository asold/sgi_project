{
  "title": "Identification of DNA N6-methyladenine modifications in the rice genome with a fine-tuned large language model",
  "url": "https://openalex.org/W4411627702",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2095590488",
      "name": "Yichi Zhang",
      "affiliations": [
        "Chengdu University of Information Technology",
        "Chengdu University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2096672852",
      "name": "Hao Chen",
      "affiliations": [
        "Chengdu University of Information Technology",
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A3203878010",
      "name": "Shicheng Xiang",
      "affiliations": [
        "Sichuan University",
        "Chengdu University",
        "Chengdu University of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2135487568",
      "name": "Zhibin Lv",
      "affiliations": [
        "Sichuan University",
        "Chengdu University",
        "Chengdu University of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2095590488",
      "name": "Yichi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096672852",
      "name": "Hao Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3203878010",
      "name": "Shicheng Xiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135487568",
      "name": "Zhibin Lv",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4250685322",
    "https://openalex.org/W4288077611",
    "https://openalex.org/W2967387109",
    "https://openalex.org/W6910681941",
    "https://openalex.org/W4311111558",
    "https://openalex.org/W4392295714",
    "https://openalex.org/W2910591656",
    "https://openalex.org/W4408564722",
    "https://openalex.org/W3211022409",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4308038053",
    "https://openalex.org/W4389526560",
    "https://openalex.org/W3084754935",
    "https://openalex.org/W4405525278",
    "https://openalex.org/W3127238141",
    "https://openalex.org/W4306404289",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W6745609711",
    "https://openalex.org/W4406677312",
    "https://openalex.org/W4399025557",
    "https://openalex.org/W2944682099",
    "https://openalex.org/W2884914958",
    "https://openalex.org/W2156125289",
    "https://openalex.org/W3130800274",
    "https://openalex.org/W3203741962",
    "https://openalex.org/W4402053335",
    "https://openalex.org/W4399153512",
    "https://openalex.org/W4405175768",
    "https://openalex.org/W2972691905",
    "https://openalex.org/W4386439087",
    "https://openalex.org/W2965667464",
    "https://openalex.org/W3093120804",
    "https://openalex.org/W2979937512",
    "https://openalex.org/W2786672974",
    "https://openalex.org/W4366089579",
    "https://openalex.org/W4398177119",
    "https://openalex.org/W2553763355",
    "https://openalex.org/W3045429062",
    "https://openalex.org/W2960629037",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W4402903640",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2972628085",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4401342884",
    "https://openalex.org/W4327735959",
    "https://openalex.org/W4391483046",
    "https://openalex.org/W4212885278",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W4319982435",
    "https://openalex.org/W4321015480",
    "https://openalex.org/W4390097263",
    "https://openalex.org/W4403782367",
    "https://openalex.org/W3096599651",
    "https://openalex.org/W4385755748",
    "https://openalex.org/W4403198120",
    "https://openalex.org/W4409025601",
    "https://openalex.org/W3006772895",
    "https://openalex.org/W3147684719",
    "https://openalex.org/W3135521497",
    "https://openalex.org/W4403560690",
    "https://openalex.org/W2979999916",
    "https://openalex.org/W4408417595",
    "https://openalex.org/W2103083046",
    "https://openalex.org/W2900679305",
    "https://openalex.org/W6853888595",
    "https://openalex.org/W3171321388",
    "https://openalex.org/W2884715962",
    "https://openalex.org/W4391618534",
    "https://openalex.org/W2807709032",
    "https://openalex.org/W4401602316",
    "https://openalex.org/W4385984664",
    "https://openalex.org/W2900694973",
    "https://openalex.org/W4281890194",
    "https://openalex.org/W4390946589",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W4382490702"
  ],
  "abstract": "DNA N6-methyladenine (6mA) plays a significant role in various biological processes. In the rice genome, 6mA is involved in important processes such as growth and development, influencing gene expression. Therefore, identifying the 6mA locus in rice is crucial for understanding its complex gene expression regulatory system. Although several useful prediction models have been proposed, there is still room for improvement. To address this, we propose an architecture named iRice6mA-LMXGB that integrates a fine-tuned large language model to identify the 6mA locus in rice. Specifically, our method consists of two main components: (1) a BERT model for feature extraction and (2) an XGBoost module for 6mA classification. We utilize a pre-trained DNABERT-2 model to initialize the parameters of the BERT component. Through transfer learning, we fine-tune the model on the rice 6mA recognition task, converting raw DNA sequences into high-dimensional feature vectors. These features are then processed by an XGBoost algorithm to generate predictions. To further validate the effectiveness of our fine-tuning strategy, we employ UMAP(Uniform Manifold Approximation and Projection) visualization. Our approach achieves a validation accuracy of 0.9903 in a five-fold cross-validation setting and produces a receiver operating characteristic (ROC) curve with an area under the curve (AUC) of 0.9994. Compared to existing predictors trained on the same dataset, our method demonstrates superior performance. This study provides a powerful tool for advancing research in rice 6mA epigenetics.",
  "full_text": "Identiﬁcation of DNA N6-\nmethyladenine modiﬁcations in\nthe rice genome with aﬁne-\ntuned large language model\nYichi Zhang, HaoChen, ShichengXiang and ZhibinLv*\nCollege of Biomedical Engineering, Sichuan University, Chengdu, China\nDNA N6-methyladenine (6mA) plays a signi ﬁcant role in various biological\nprocesses. In the rice genome, 6mA is involved in important processes such as\ngrowth and development, inﬂuencing gene expression. Therefore, identifying the\n6mA locus in rice is crucial for understanding its complex gene expression\nregulatory system. Although several useful prediction models have been\nproposed, there is still room for improvement. To address this, we propose an\narchitecture named iRice6mA-LMXGB that integrates a ﬁne-tuned large\nlanguage model to identify the 6mA locus in rice. Speciﬁcally, our method\nconsists of two main components: (1) a BERT model for feature extraction and\n(2) an XGBoost module for 6mA classi ﬁcation. We utilize a pre-trained\nDNABERT-2 model to initialize the parameters of the BERT component.\nThrough transfer learning, we ﬁne-tune the model on the rice 6mA\nrecognition task, converting raw DNA sequences into high-dimensional feature\nvectors. These features are then processed by an XGBoost algorithm to generate\npredictions. To further validate the effectiveness of ourﬁne-tuning strategy, we\nemploy UMAP(Uniform Manifold Approximation and Projection) visualization.\nOur approach achieves a validation accuracy of 0.9903 in aﬁve-fold cross-\nvalidation setting and produces a receiver operating characteristic (ROC) curve\nwith an area under the curve (AUC) of 0.9994. Compared to existing predictors\ntrained on the same dataset, our method demonstrates superior performance.\nThis study provides a powerful tool for advancing research in rice\n6mA epigenetics.\nKEYWORDS\nrice genome, N6-methyladenine, large language model, BERT, UMAP visualization\n1 Introduction\nN6-methyladenine(6mA) is produced by methylation of the N6 position of adenine\nand has been found in bacteria, eukaryotes, and archaea ( Zhang et al., 2015 ; O’Brown and\nGreer, 2016 ). Rice is one of the most important cereal crops in the world. Within the rice\ngenome, 6mA serves as a critical epigenetic modi ﬁcation, regulating gene expression\nFrontiers inPlant Science frontiersin.org01\nOPEN ACCESS\nEDITED BY\nShanwen Sun,\nNortheast Forestry University, China\nREVIEWED BY\nLixin Cheng,\nJinan University, China\nFei Guo,\nCentral South University, China\n*CORRESPONDENCE\nZhibin Lv\nlvzhibin@pku.edu.cn\nRECEIVED 11 May 2025\nACCEPTED 04 June 2025\nPUBLISHED 25 June 2025\nCITATION\nZhang Y,Chen H,Xiang S andLv Z (2025)\nIdentiﬁcation of DNA N6-methyladenine\nmodiﬁcations in the rice genome with\na ﬁne-tuned large language model.\nFront. Plant Sci.16:1626539.\ndoi: 10.3389/fpls.2025.1626539\nCOPYRIGHT\n© 2025 Zhang, Chen, Xiang and Lv. This is an\nopen-access article distributed under the terms\nof theCreative Commons Attribution License\n(CC BY).The use, distribution or reproduction\nin other forums is permitted, provided the\noriginal author(s) and the copyright owner(s)\nare credited and that the original publication\nin this journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nTYPE Original Research\nPUBLISHED 25 June 2025\nDOI 10.3389/fpls.2025.1626539\nthrough methylation at the N6 position of adenine ( Lv et al., 2020 ;\nChen et al., 2022 ; Jin et al., 2022 ). Studies have shown that 6mA in\nrice plays a vital role in many biological functions. For example,\n6mA in rice is associated with stress response and helps rice to\nbetter adapt to adversity ( Zhang et al., 2018 ; Ding et al., 2023 ). It is\nalso associated with reproduction and regulates the growth and\ndevelopment of rice ( Zhou et al., 2021 ; Yang et al., 2024 ). Zhou et al.\ndiscovered that 6mA is highly enriched in speci ﬁc sequence motifs,\nconserved DNA sequence patterns that serve as recognition sites for\nepigenetic regulators. These motifs include AGG and GAGG, which\nare assumed to represent the binding elements of methyltransferase\ncomplexes or chromatin associated proteins. 6mA methylation\npreferentially occurred on these speci ﬁcn u c l e o t i d em o t i f s ,\nindicating their functional signi ﬁcance in epigenetic regulation\n(Lee et al., 2018 ). And this methylation pattern is tightly linked to\nthe drought stress response in rice ( Zhou et al., 2018 ; Yang et al.,\n2024). In addition, 6mA can directly affect seed size and yield\nformation by regulating the expression of endosperm development-\nrelated genes ( Zhou et al., 2021 ). In recent years, epigenetic\nbreeding strategies based on CRISPR-6mA editing technology\nhave provided new ideas to improve disease resistance and yield\nin rice by targeting modi ﬁcation of the 6mA locus ( Romero and\nGatica-Arias, 2019 ). However, traditional experimental methods\nsuch as SMRT-seq for detecting 6mA locus have the limitations of\nhigh cost and low throughput, and there is an urgent need to\ndevelop ef ﬁcient computational pred iction models to guide\nsubsequent functional studies ( Zhu et al., 2018 ; Wang L. et al.,\n2023; Chen et al., 2024 ; Liu et al., 2024 ; Shao et al., 2024 ; Xie H.\net al., 2024 ; Zhou et al., 2024 ).\nIn recent years, machine and deep learning approaches have\nsuccessfully addressed many challenges in identifying 6mA\nmodiﬁcations in rice genomes ( Sinha et al., 2023 ; Wang R. et al.,\n2023). In 2019, Chen et al. developed the ﬁrst method for predicting\nDNA 6mA sites in rice, called i6mA-Pred, utilizing nucleotide\nchemical property (NCP) features and a support vector machine\n(SVM) as the classi ﬁer ( Chen et al., 2019 ; Zou et al., 2022 ; Meher\net al., 2024 ; Wang Y. et al., 2024\n). Subsequent research has seen the\nemergence of various single-classi ﬁer-based prediction methods,\nincluding MM-6mAPred ( Pian et al., 2019 ), i6mA-DNCP ( Park\net al., 2020 ), iN6-methylat ( Le, 2019), and iDNA6mA-rice ( Lv et al.,\n2019). Moreover, ensemble learning models combining multiple\nclassiﬁers, such as csDMA ( Liu et al., 2019 ), SDM6A ( Basith et al.,\n2019), 6mA-Finder ( Xu et al., 2020 ), Meta-i6mA ( Hasan et al.,\n2021), i6mA-VC ( Xue et al., 2021 ), i6mA-Vote ( Teng et al., 2022 ),\nand EpiSemble ( Sinha et al., 2023 ), have been developed to enhance\nmodel performance and robustness. Deep learning techniques have\nevolved from traditional arti ﬁcial neural network frameworks and\nhave shown signi ﬁcant improvement in predictive power across\nmultiple research domains. With the development of deep learning\nand its excellent performance, researchers began to apply it to the\nproblem of DNA 6mA site prediction. In 2019, Yu et al. developed a\nprediction model called SNNRice6mA ( Yu and Dai, 2019 ) based on\nconvolutional neural networks (CNNs) through single-nucleotide\none-hot coding, obtaining an accuracy of 0.920. Another group of\nresearchers, Lv et al., proposed a convolutional neural network\niRicem6A-CNN ( Lv et al., 2021 ) based on a dinucleotide one-hot\nencoder in 2020, achieving an accuracy of 0.938 for 5-fold cross-\nvalidation. However, it is worth noting that CNNs are limited in\nfocusing on only part of the information. Deep6mA ( Li Z. et al.,\n2021), which consists of a convolutional neural network (CNN) and\na bidirectional LSTM (BLSTM) module to solve the long-distance\nnucleotide association pro blem by learning contextual\ndependencies of the sequences, was proposed by Li et al. in 2021\nand achieved a 5-fold cross-validation accuracy of 0.940.\nOver the last few years, large-scale language modeling (LLM)\nhas progressed tremendously ( Li H. et al., 2021 ; Xie X. et al., 2024 ;\nChen et al., 2025 ). The well-known model, ChatGPT, is a ﬁne-tuned\nversion of the base GPT-3 model. By learning contextual text in a\nself-supervised manner, it can both understand and generate\nhuman language ( Devlin et al., 2019 ; Wang G. et al., 2024 ). DNA\nsequences exhibit similarities to natural language. Nucleotides, the\nbuilding blocks of nucleic acids, serve as “words” within biological\nsystems’“ languages”. LLMs can be adapted for the analysis of\nbiological sequence data by leveraging the structure of DNA and\nprotein sequences as analogous to natural language texts ( Jumper\net al., 2021 ; Rives et al., 2021 ; Wei et al., 2021 ;\nLi T. et al., 2024 ; Li Y.\net al., 2024 ; Qiao et al., 2024 ; Lai et al., 2025 ; Xie et al., 2025 ). There\nhave been many breakthroughs in LLMs for applications in biology,\nsuch as AlphaFold2, a protein prediction model with very high\naccuracy ( Jumper et al., 2021 ), the Geneformer model trained on\ndata from about 10 million human single-cell RNA sequences ( Zou\net al., 2019 ; Theodoris et al., 2023 ), and DNABERT, a transformer-\nbased DNA pre-training model ( Ji et al., 2021 ). While LLMs\ndemonstrate potential for identifying patterns and correlations in\nnoisy biological datasets ( Lam et al., 2024 ; Soylu and Sefer, 2024 ; Xie\nX. et al., 2024 ; Liu et al., 2025 ), they have yet to gain acceptance\nwithin plant science research. To date, LLMs have not been\nemployed in the study of 6mA locus prediction in rice.\nIn this study, we develop a large language model-based transfer\nlearning model called iRice6mA-LMXGB. it consists of a pre-\ntrained DNABERT2 model and an XGBoost model. It contains a\nunique ﬁne-tuning architecture that relies exclusively on DNA\nsequence data to distinguish 6mA sequences from non-6mA\nsequences. Experimental re sults demonstrate the model ’s\noutstanding performance, achieving a validation accuracy of\n0.9903 through 5-fold cross-validation. Compared to all previous\nmethods tested on standard datasets, iRice6mA-LMXGB\nsigni ﬁcantly outperforms them, suggesting that this novel\napproach has the potential to transform biological\nsequence modeling.\n2 Materials and methods\n2.1 Benchmark dataset\nIn this study, we utilized the rice dataset constructed by Lv et al.\n(2020) for model training and evaluation using 5-fold cross-\nvalidation. To ensure the high quality of the data, sequences with\ngreater than 80% similarity were removed via the CD-HIT program\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org02\n(Li and Godzik, 2006 ). The dataset is made of 154,000 sequences\nwith 6mA sites and 154,000 sequences without 6mA sites. This is a\nwidely adopted and balanced rice dataset. During model training,\nunbalanced datasets may lead to unreliable results. The majority\nclass samples are dominant and the model will favor the majority\nclass during training, thus ignoring the minority class. This may\nresult in the model having high accuracy for the majority class but\nlow recognition for the minority class during prediction. For ease of\nreference, we denote it as “rice-Lv” throughout this study. Both\npositive and negative sequences in the rice-Lv dataset are 41 base\npairs in length. Positive sequences represent 6mA modi ﬁcations at\ntheir centers, while negative sequences lack such modi ﬁcations at\ntheirs. By employing this well-established dataset, we enable a fair\ncomparison between our method and those previously reported.\n2.2 Architecture of iRice6mA-LMXGB\nThe architecture of iRice6A-LMXGB is presented in Figure 1 ,\ncomprising two main components: the pre-trained DNABERT-2\nmodule and the XGBoost module. DNABERT-2 is a pre-trained\nBERT model speci ﬁcally designed for encoding DNA sequences. It\ncan ef ﬁciently identify complex long-range dependencies in these\nsequences (Zhou et al., 2023 ). And this module will undergo further\nﬁne-tuning in this study. XGBoost ’s superior performance,\nparticularly in terms of speed and accuracy when processing\nlarge-scale datasets, enables its extensive use in solving\nclassiﬁcation problems ( Chen and Guestrin, 2016 ; Yang et al.,\n2021). It utilizes the feature vectors output from the DNABERT-2\nmodel to generate ﬁnal prediction results. A detailed explanation of\nthe model follows.\n2.2.1 DNABERT-2\nDNABERT-2 is an iterative version of DNABERT. DNABERT\nis the ﬁrst BERT-based DNA language model ( Ji et al., 2021 ).\nRigorously trained on a comprehensive genomic dataset\nencompassing the entire human genome, DNABERT offers a\nlinguistic perspective for genomic analysis. While widely adopted,\nthe initial version of DNABERT exhibited notable technical\nlimitations. Speci ﬁcally, DNABERT faced two critical challenges:\nﬁrst, its training data is limited to a single-species genome, which\nmakes it dif ﬁcult for the model to capture sequence-conserving\npatterns and diversity features across species; second, the k-mer\nsequence partitioning mechanism it employs not only triggers the\nhidden danger of data leakage during the training process, but also\nsigniﬁcantly increases the computational complexity ( Moeckel\net al., 2024 ). Such limitations underscore the pressing need for\ninnovation and improvement in DNA-based language modeling\nresearch. To address these challenges, DNABERT-2 introduced\nsigniﬁcant improvements in both areas. First, it breaks through\nspecies boundaries and employs cross-species genomic datasets for\npre-training, signi ﬁcantly enhancing the model ’s ability to\nrecognize evolutionarily conserved regions and species speci ﬁcity.\nSecond, at the data processing stage, DNABERT-2 employs byte-\npair encoding (BPE), a novel tokenization method that replaces\ntraditional k-mer partitioning. This is a data compression algorithm\nwidely used in large-scale language models ( Sennrich et al., 2015 ),\nwhich effectively solves the risk of data leakage and improves\ncomputational ef ﬁciency, successfully overcoming the limitations\nof k-mer tokenization. As demonstrated by Zhihan et al. ’s\ncomparative analysis, compared to conventional 6-mer\ntokenization methods, the byte-pair encoding (BPE)\nimplementation exhibits supe rior sequence compression\nefﬁciency, reducing the tokenized sequence length by a factor of\n5. The dramatic reduction in dimensionality directly improves the\ncomputational ef ﬁciency of processing genome sequences ( Zhou\net al., 2023 ).\nThe BERT model consists of two independent components: the\nmodule responsible for preprocessing BERT input and the pre-\ntraining BERT module. In the BERT input preprocessing module,\nDNABERT-2 utilizes BPE to tokenize DNA sequences. Byte Pair\nEncoding (BPE) is a subword tokenization algorithm commonly\nemployed in NLP Natural Language Processing) tasks. Its key\nmechanism lies in iteratively merging character pairs of the\nhighest frequency to construct a vocabulary of subwords. During\ntokenization, DNABERT-2 appends a [CLS] token at the sequence\nstart and a [SEP] token at the end. Then, each token is put into an\nembedding module and converted into a vector. The DNABERT-2\nmodel uses the ALiBi(Attention with Linear Biases) ( Press et al.,\n2021) approach, which does not add positional embeddings to the\ninput, but rather adds a non-learned embedding in every Attention\ncomputation to add a non-learning bias and a ﬁxed set of statics to\ncombine the location information with the Attention score.\nDNABERT-2 employs a transformer encoder architecture as the\nbackbone of its pre-trained BERT module. The feature matrix is\nconstructed by cascading enco ders layer by layer across the\nnetwork’s layers (L). Each encoder comprises three components:\nmulti-head self-attention units, position-wise feed-forward neural\nnetworks, and normalization layers. Within the i-th encoder stage,\nthe multi-head self-attention mechanism operates as follows.\nMultihead(X\ni) = Concat( head1, head2, …, headn)WO,i\nFor the i-th encoder, the input matrix Xi  is handled through n\nself-attentive heads for processing. The outputs of these heads are\nthen transformed by the output transformation matrix Wo,i, which\nis computed in detail for each headi as follows.\nheadi = softmax WQ,iXi(WK,iXi)T\nﬃﬃﬃﬃﬃ\ndk\np\n !\n WV,iXi\nWQ,i, WK,i and WV,i serve as the transformation matrices for\nthe query, key, and value components of each head, respectively. dk\ndenotes the dimension of the matrix.\nSpeciﬁcally, after computing MultiHead(Xi) in the multi-head\nattention mechanism, this resultant output is added to the residual\nconnection of the original input Xi for normalization. The\ncomputation proceeds according to the formula below.\nYi = LayerNorm(MultiHead( Xi)+ Xi)\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org03\nAfter normalization, the processed data is passed through a\nfeed-forward neural network using the following formula:\nFFN(Yi) = max (0, YiW1 + b1)W2 + b2\nW1, W2, b1 and b2 are the trainable weight parameters within\nthe feed-forward layer.\nThe output of the i-th encoder is achieved through\nnormalization of the residual connection between Yi and FFN(Yi)\n. Below is the corresponding formula.\nXi+1 = LayerNorm( Yi + FFN(Yi))\nFinally, the output of the DNABERT-2 can be obtained by\ncascading the L encoders as follows.\nFIGURE 1\nThe proposed modeling framework, iRice6mA-LMXGB.\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org04\nX1 = Xi+1 ∈ Rd/C2 N\nwhere d denotes the dimension of the word vector and N\nrepresents the total number of tokens.\nDNABERT-2 follows the BERT model architecture, de ﬁned by\nthree key parameters: L = 12, H = 768, and A = 12. The parameter L\nspeciﬁes the number of transformer layers (totaling 12). The\nparameter H determines the hidden layer size, with each token\nrepresented as a 768-dimensional vector. The parameter A speci ﬁes\nthe number of attention heads (totaling 12). In this study, we use the\nfull ﬁne-tuning (FFT) (Church et al., 2021 ) method, which treats rice\nDNA sequences as “sentences in natural language ” and inputs them\ninto the DNABERT-2 module to adjust and update all the\nparameters. Finally, we use the BERT model to convert them into\nﬁxed-length feature vectors to obtain the original feature matrix\nbefore ﬁne-tuning and the feature matrix after 200 cycles of updating.\n2.2.2 XGBOOST\nThe XGBoost classi ﬁer is a gradient boosting method that\nintegrates regression trees ( Basith et al., 2019 ). The objective\nfunction of the model is obj(q)= L(q)+ W(q), L(q) is the training\nloss function with the expression:\nL(q)= o\nn\ni=1\nl(yi, byi)\nWhere l(yi, ^yi) represents the training loss function for each\nsample. yi represents the true value of the i-th sample. ^yi represents\nthe estimated value of the i-th sample.\nThen the estimated value of the i-th sample is expressed as:\nbyi = o\nK\nk=1\nfk(xi), fk ∈ F\nK is the number of integrated trees, and F denotes the space of\nall possible decision trees. fk is a speci ﬁc categorical regression tree\n(CART).W(f ) is the tree structure complexity function, and its\nspeciﬁc form is:\nW(f )= gT + 1\n2 lo\nT\ni=1\nw2\ni\nThe parameterg restricts the number of leaf nodes T of the tree to\ncontrol the complexity of the model. And the parameter l constrains\nthe sum of the weights w2\ni  o fe a c hl e a fn o d et os u p p r e s so v e rﬁtting.\nThe objective function is continuo usly optimized by adjusting the\nparameters for the optimal result. In this way, the XGBoost classi ﬁer\nﬁnally outputs the prediction results of the rice sequence about 6mA by\nreceiving the extracted feature vectors from DNABERT-2.\n2.3 Evaluation metrics and methods\nIn this study, we validate our approach using a traditional 5-fold\ncross-validation method and compare it to previous studies based\non the benchmark dataset rice-Lv.\nwe will combine ﬁve metrics, including accuracy (ACC),\nsensitivity (Sn), speci ﬁcity (Sp), Matthew ’s correlation coef ﬁcient\n(MCC), and area under the curve (AUC), to comprehensively\nevaluate the prediction performance of our model ( Zou et al.,\n2023; Zulﬁqar et al., 2023 ; Guo et al., 2024 ; Huang et al., 2024 ;\nZhu et al., 2024 ).\nACC indicates the overall correctness of the model prediction\nand is a basic benchmark used to evaluate the model performance,\nwhich can be expressed as:\nACC = TP + TN\nTP + TN + FP + FN\nThe sensitivity Sn, also known as the true positive rate (TPR), is\nexpressed as:\nSN = TP\nTP + FN\nThe speciﬁcity Sp, also known as the true negative rate (TNR), is\nexpressed as:\nSP = TN\nTN + FP\nMCC is a composite metric that assesses the overall quality of\nclassiﬁcation model predictions by examining the performance of\nthe classi ﬁcation model in each of the four quadrants of the\nconfusion matrix. The superior score re ﬂects the balanced\nexcellence between true positives (TP), true negatives (TN), false\nnegatives (FN) and false positives (FP). It can be de ﬁned as:\nMCC = TP /C2 TN − FP /C2 FNﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ(TP + FP)(TP + FN)(TN + FP)(TN + FN)p\nThe last performance metric we use is AUC, de ﬁned as the value\nof the area under the subject ’s operating characteristic curve. AUC\nis also an important measure of the performance of a dichotomous\nmodel. The larger the value of AUC, the better the model performs.\nAUC is a ﬂoating-point number between 0 and 1. 1 indicates that\nthe model predicts perfectly, whereas 0.5 indicates that the model is\nsimilar to a random prediction ( Zhang et al., 2025 ).\n3 Results and discussion\n3.1 Model performance analysis\nIn this study, we developed three models. For the ﬁrst model, we\ndirectly used the pre-trained DNABERT-2 to extract 768-\ndimensional features from rice DNA and fed them into an\nXGBoost classi ﬁer for prediction tasks. The XGBoost classi ﬁer\nshows unique advantages in genomics data classi ﬁcation tasks,\nmainly due to its ability to ef ﬁciently handle high-dimensional\nsparse data and its built-in regularization mechanism. Our\ndataset, with more than 300,000 samples, is characterized by high\nfeature dimensionality, and XGBoost is able to ef ﬁciently capture\nnonlinear interaction effects through the gradient boosting\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org05\nframework combined with second-order derivative optimization.\nIts regularization term can in turn suppress over ﬁtting and enhance\nmodel generalization ( Chen and Guestrin, 2016 ). Cross-validation\nresults showed ACC=0.6259, Sn=0.6207, Sp=0.6312, MCC=0.2519,\nand auROC=0.6728 for this con ﬁguration. For the second model,\nwe loaded the rice-Lv dataset into the DNABERT-2 module and\nconducted 200 iteration loops to develop a ﬁne-tuned version of the\nmodel. The 5-fold cross-validation scores were ACC=0.9903,\nSn=0.9898, Sp=0.9907, MCC=0.9805, auROC=0.9994 which are\n58.22%, 59.47%, 56.96%, 289.24%, and 48.54%, respectively\nhigher than those of the non- ﬁne-tuned model. For the third\nmodel, we utilized LightGBM ’s built-in function to assess and\nprioritize feature importance using features extracted from the\nﬁne-tuned DNABERT-2 model ( Ke et al., 2017 ). The feature\nranking principle of LightGBM is based on the Gradient Boosting\nDecision Tree (GBDT) framework, which evaluates feature\nimportance by quantifying the contribution of features in the\nprocess of constructing the decision tree ( Ke et al., 2017 ).\nFollowing this, we selected the top 300 features for modeling with\nXGBoost. The 5-fold cross-validation yielded ACC=0.9899,\nSn=0.9890, Sp=0.9908, MCC=0.9799, and auROC=0.9994. As\nshown in Figure 2 , our cross-validation results indicate that: (1)\nFine-tuned models outperformed non- ﬁne-tuned counterparts\nsigniﬁcantly. (2) However, applying feature selection after ﬁne-\ntuning caused minor performance degradation compared to models\nwithout feature selection, not much difference overall. These\nﬁndings demonstrate the effectiveness of our ﬁne-tuning strategy.\nThe pre-training model is usually trained on multi species datasets,\nand may not be able to capture the 6mA distribution pattern unique\nto rice. Through the ﬁne-tuning strategy, the model parameters are\nrecalibrated, which can give priority to the local features in the rice\ngenome, and the sensitivity of the model to the sequence context of\nrice 6mA is improved. Additionally, while XGBoost ’s tree-based\narchitecture excels at managing high-dimensional data through\nregularization techniques, our results suggest that applying\nLightGBM-based feature selection after ﬁne-tuning may slightly\nreduce model performance due to fewer feature interactions. We\nselected the second model with the best performance, performing\nﬁne-tuning for 200 iterations without feature selection, to name\niRice6mA-LMXGB.\n3.2 UMAP dimensionality reduction\nvisualization\nIn order to perform an in-depth analysis of the interpretability\nof the iRice6mA-LMXGB model after integrating DNABERT-2\nwith XGBoost, we used the UMAP (Uniform Manifold\nApproximation and Projection) technique. This is a nonlinear\ndimensionality reduction and visualization algorithm for large-\nscale datasets. Umap assumes that the data is distributed on a low\ndimensional manifold. Firstly, the probability weight is de ﬁned in\nthe high dimensional space using the neighborhood graph to re ﬂect\nthe similarity between points. Then the cross entropy loss function\nis used to optimize the embedding in the low dimensional space to\nalign the low dimensional similarity with the high dimensional\nstructure. Based on graph theory and ﬂow learning methods, it is\nassumed that the available data samples are uniformly distributed in\nthe topological space and can be approximated and mapped from\nthese ﬁnite data samples to a lower-dimensional space for\nvisualization and analysis ( McInnes and Healy, 2018 ).\nTo be more speci ﬁc, we will visualize the distribution of 6mA and\nnon-6mA by projecting each feature vector onto a 2D view using the\nUMAP technique. Figure 3 shows the arrangement of 6mA and non-\n6mA samples in 2D space before and afterﬁne-tuning, and the decision\nboundary drawn in black by the XGBoost algorithm. Blue markers\ndenote non-6mA samples, and orange markers denote 6mA samples.\nThe ﬁrst subplot represents the UMAP r esults of the original features\nwithout ﬁne-tuning, which can be interpreted as all the sample points\nnot showing any representative clustering. In Figure 3A , poor\nFIGURE 2\n(A) Comparison of model performance with or withoutﬁne-tuning and with or without feature selection;(B) Average ROC curves forﬁve-fold\ncross-validation of the three models. Where noﬁne-tuning_768 features denotes the model with noﬁne-tuning, 200ﬁne-tunings_768 features\ndenotes the model with two hundredﬁne-tunings without feature selection, and 200ﬁne-tunings_300 features denotes the model that wasﬁne-\ntuned 200 times and ranked for feature importance and the top 300 features are selected after the feature importance ranking.\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org06\nseparation indicates signi ﬁcant feature overlap between the 6mA\nsample points and the non-6mA sample points ( Figure 3A ),\nsuggesting a high degree of overlap in their distributions. The second\nsubﬁgure shows the results of projecting the high-dimensional feature\nspace learned from the iRice6mA-LMXGB model into a 2D view,\nwhich shows much improved clustering, indicating a signi ﬁcant\nincrease in separation and a decrea se in overlap in the feature space\n(Figure 3B ), resulting in improved performance. In summary, our\napproach allows for better learning of model decision boundaries.\nThrough this visualization technique, we can more intuitively\nunderstand the impact of features on model predictions, further\ndeepening our exploration of model interpretability.\n3.3 Comparison of the proposed model\nwith existing models\nTo better evaluate the performance of our model, we compare it\nwith the following state-of-the-art methods, including MM-6mAPred\n(Pian et al., 2019 ), iDNA6mA-Rice ( Lv et al., 2019 ), SNNRice6mA\n(Yu and Dai, 2019 ), iRicem6A-CNN ( Lv et al., 2021 ), ENet-6mA\n(Abbas et al., 2022 ), Deep6mA (Li Z. et al., 2021 ) and SpineNet-6mA\n(Abbas et al., 2020 ). Our model is evaluated using the same ﬁve-fold\ncross-validation protocol on the same dataset as previous studies,\nemploying the identical metrics: ACC, MCC, Sn, Sp, and AUC. As\nshown in Table 1 , our iRice6mA-LMXGB model outperforms all\nprevious predictors across all metrics and demonstrates more stable\nperformance with less ﬂuctuation in ACC, MCC, Sn, Sp, and AUC\nvalues. In ACC, MCC, Sn, and AUROC metrics, our model improves\nover the previous best predictor SpineNet-6mA by 5%, 11.42%,\n3.42%, 6.62%, and 1.98%, respectively. Furthermore, it outperforms\nthe previous best model, ENet-6mA, by 6.08% in Sp metric. To\nfacilitate visualization of the comparison results, we created a box-\nand-whisker plot, as illustrated in Figure 4 . To sum up, our\niRice6mA-LMXGB model demonst rates superior performance\ncompared to both machine learning-based and CNN/LSTM-based\ndeep learning models for 6mA prediction in rice, showcasing its\nrobustness as a predictive tool.\n4 Conclusions\nIn this article, we develop a novel computational model called\niRice6mA-LMXGB that combinesﬁne-tuned large language modeling\nto efﬁciently distinguish and identify 6mA and non-6mA loci in the rice\ngenome. We utilized the large language model, DNABERT-2, to\nrepresent the DNA sequence as a continuous word vector, thus\neffectively capturing the DNA sequence features. Subsequently, we\napplied the robust machine learning method XGBoost to make\naccurate predictions based on the extracted features. We compare\nand analyze the performance of iRice6mA-LMXGB with other\npredictors, and the results show that iRice6mA-LMXGB obtains the\nbest performance compared to previous models. Our model\noutperforms all existing models on ACC, SN, SP, MCC, and AUC\n(5-fold cross-validation: ACC = 0.9903, MCC = 0.9805, Sn = 0.9898, Sp\nTABLE 1 5-fold cross-validation results of iRice6mA-LMXGB with several\nprevious methods on the rice-Lv dataset.\nMethod ACC MCC Sn Sp AUROC\nMM-6mAPred 0.9149 0.8300 0.9347 0.8951 0.9600\niDNA6mA-Rice 0.9170 0.8350 0.9300 0.9050 0.9640\nSNNRice6mA 0.9204 0.8400 0.9433 0.8975 0.9700\niRicem6A-CNN 0.9382 0.8770 0.9434 0.9331 0.9790\nENet-6mA 0.9437 0.8700 0.9467 0.9339 0.9800\nDeep6mA 0.9401 0.8800 0.9506 0.9296 0.9800\nSpineNet-6mA 0.9431 0.8800 0.9571 0.9292 0.9800\niRice6mA-LMXGB (ours) 0.9903 0.9805 0.9898 0.9907 0.9994\nBold values indicate that the model proposed in this study achieves optimal results in each of\nthe assessment metrics.\nFIGURE 3\nUMAP dimensionality reduction visualization.(A) UMAP results of the original features of the unﬁne-tuned model.(B) UMAP results of features\nlearned by the iRice6mA-LMXGB model.\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org07\n= 0.9907, and auROC = 0.9994), suggesting that the iRice6mA-\nLMXGB is a powerful and robust predictor that can help researchers\nto identify and analyze the 6mA locus in the rice genome more\neffectively, thus providing a de eper understanding of the complex\nmechanisms of gene regulation and advancing theﬁeld of life sciences.\nIt is demonstrated through U MAP visualization that the ﬁne-tuning\nstrategy for large language models signi ﬁcantly enhances the model ’s\nfeature extraction ability. This raises the possibility that large language\nmodels can be ﬁne-tuned for various purposes and deployed for plant-\nspeciﬁc domains to solve biological problems. Moving ahead, we plan\nto expand our dataset and perform model optimization to enhance the\ngeneralizability of our model for broader applications.\nData availability statement\nThe raw sequence data used in the study were obtained from the\nfollowing URL: http://lin-group.cn/server/iDNA6mA-Rice.\nAuthor contributions\nYZ: Formal analysis, Investigation, Visualization, Writing –\noriginal draft. HC: Investigation, Writing – review & editing. SX:\nInvestigation, Writing – review & editing. ZL: Methodology, Project\nadministration, Supervision, Writing – review & editing.\nFunding\nThe author(s) declare that ﬁnancial support was received for the\nresearch and/or publication of this article. This work supports by\nthe National Natural Science Foundation of China (No.62371318,\nNo.62001090) and 2024 Foundation Cultivation Research Basic\nResearch Cultivation Special Funding (No. 20826041H4211).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could be\nconstrued as a potential con ﬂict of interest.\nGenerative AI statement\nThe author(s) declare that no Generative AI was used in the\ncreation of this manuscript.\nFIGURE 4\nComparison of the proposed model with other existing models on the rice-Lv dataset.\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org08\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\nAbbas, Z., Tayara, H., and Chong, K. T. (2020). SpineNet-6mA: A novel deep\nlearning tool for predicting DNA N6-methyladenine sites in genomes. IEEE Access8,\n201450– 201457. doi: 10.1109/Access.6287639\nAbbas, Z., Tayara, H., and Chong, K. T. (2022). ENet-6mA: identi ﬁcation of 6mA\nmodiﬁcation sites in plant genomes using elasticNet and neural networks. Int. J. Mol.\nSci. 23, 8314. doi: 10.3390/ijms23158314\nBasith, S., Manavalan, B., Shin, T. H., and Lee, G. (2019). SDM6A: A web-based\nintegrative machine-learning framework for predicting 6mA sites in the rice genome.\nMol. Ther. Nucleic Acids18, 131 – 141. doi: 10.1016/j.omtn.2019.08.011\nChen, T., and Guestrin, C. (2016). “XGBoost: A scalable tree boosting system, ” in\nProceedings of the 22nd ACM SIGKDD Inte rnational Conference on Knowledge\nDiscovery and Data Mining, San Francisco, California, USA. 785 – 794 (Association\nfor Computing Machinery).\nChen, B., Guo, Y., Zhang, X., Wang, L., Cao, L., Zhang, T., et al. (2022). Climate-\nresponsive DNA methylation is involved in the biosynthesis of lignin in birch. Front.\nPlant Sci.13, 1090967. doi: 10.3389/fpls.2022.1090967\nChen, L., Liu, G., and Zhang, T. (2024). Integrating machine learning and genome\nediting for crop improvement. aBIOTECH 5, 262– 277. doi: 10.1007/s42994-023-00133-5\nChen, W., Lv, H., Nie, F., and Lin, H. (2019). i6mA-Pred: identifying DNA N6-\nmethyladenine sites in the rice genome. Bioinformatics 35, 2796 – 2800. doi: 10.1093/\nbioinformatics/btz015\nChen, S., Yan, K., Li, X., and Liu, B. (2025). Protein language pragmatic analysis and\nprogressive transfer learning for pro ﬁling peptide– protein interactions. IEEE Trans. on\nNeural Networks and Learn. Syst2025, 1 – 15. doi: 10.1109/TNNLS.2025.3540291\nChurch, K. W., Chen, Z., and Ma, Y. (2021). Emerging trends: A gentle introduction\nto ﬁne-tuning. Nat. Lang. Eng.27(6), 763 – 778. doi: 10.1017/S1351324921000322\nDevlin, J., Chang, M. – W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of\ndeep bidirectional transformers for language understanding. In: Proceedings of the 2019\nconference of the North American chapte r of the association for computational\nlinguistics: human language technologies, volume 1 (Long and Short Papers): 4171 –\n4186. doi: 10.18653/v1/N19-1423\nDing, K., Sun, S., Luo, Y., Long, C., Zhai, J., Zhai, Y., et al. (2023). PlantCADB: A\ncomprehensive plant chromatin accessibility database. Genom Proteomics Bioinf.21,\n311– 323. doi: 10.1016/j.gpb.2022.10.005\nGuo, X., Huang, Z., Ju, F., Zhao, C., and Yu, L. (2024). Highly accurate estimation of\ncell type abundance in bulk tissues based on single-cell reference and domain adaptive\nmatching. Adv Sci.11, 2306329. doi: 10.1002/advs.202306329\nHasan, M. M., Basith, S., Khatun, M. S., Lee, G., Manavalan, B., and Kurata, H.\n(2021). Meta-i6mA: an interspecies predictor for identifying DNA N6-methyladenine\nsites of plant genomes by exploiting informative features in an integrative machine-\nlearning framework. Brief Bioinform.22, bbaa202. doi: 10.1093/bib/bbaa202\nHuang, Z., Guo, X., Qin, J., Gao, L., Ju, F., Zhao, C., et al. (2024). Accurate RNA\nvelocity estimation based on multibatch network reveals complex lineage in batch\nscRNA-seq data. BMC Biol.22, 290. doi: 10.1186/s12915-024-02085-8\nJi, Y., Zhou, Z., Liu, H., and Davuluri, R. V. (2021). DNABERT: pre-trained\nBidirectional Encoder Representations from Transformers model for DNA-language\nin genome. Bioinformatics 37, 2112 – 2120. doi: 10.1093/bioinformatics/btab083\nJin, J., Yu, Y., Wang, R., Zeng, X., Pang, C., Jiang, Y., et al. (2022). iDNA-ABF: multi-\nscale deep biological language learning model for the interpretable prediction of DNA\nmethylations. Genome Biol.23, 1 – 23. doi: 10.1186/s13059-022-02780-1\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., et al.\n(2021). Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–\n589. doi: 10.1038/s41586-021-03819-2\nKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., and Liu, T. – Y.\n(2017). Lightgbm: A highly ef ﬁcient gradient boosting decision tree. Adv. Neural Inf.\nProcess Syst.30, 3146 – 3154. doi: 10.5555/3294996.3295074\nLai, L., Liu, Y., Song, B., Li, K., and Zeng, X. (2025). Deep generative models for\ntherapeutic peptide discovery: A comprehensive review. ACM Comput. Surv57, 1 – 29.\ndoi: 10.1145/3714455\nLam, H. Y. I., Ong, X. E., and Mutwil, M. (2024). Large language models in plant\nbiology. Trends Plant Sci.29, 1145 – 1155. doi: 10.1016/j.tplants.2024.04.013\nLe, N. Q. K. (2019). iN6-methylat (5-step): identifying DNA N(6)-methyladenine\nsites in rice genome using continuous bag of nucleobases via Chou ’s 5-step rule. Mol.\nGenet. Genomics294, 1173 – 1182. doi: 10.1007/s00438-019-01570-y\nLee, N. K., Li, X., and Wang, D. (2018). A comprehensive survey on genetic\nalgorithms for DNA motif prediction. Inf. Sci. 466, 25 – 4 3 .d o i :1 0 . 1 0 1 6 /\nj.ins.2018.07.004\nLi, W., and Godzik, A. (2006). Cd-hit: a fast program for clustering and comparing\nlarge sets of protein or nucleotide sequences. Bioinformatics 22, 1658 – 1659.\ndoi: 10.1093/bioinformatics/btl158\nLi, Z., Jiang, H., Kong, L., Chen, Y., Lang, K., Fan, X., et al. (2021). Deep6mA: A deep\nlearning framework for exploring similar patterns in DNA N6-methyladenine sites\nacross different species. PloS Comput. Biol. 17, e1008767. doi: 10.1371/\njournal.pcbi.1008767\nLi, H., Pang, Y., and Liu, B. (2021). BioSeq-BLM: a platform for analyzing DNA,\nRNA, and protein sequences based on biological language models. Nucleic Acids Res.\n49, e129. doi: 10.1093/nar/gkab829\nLi, T., Ren, X., Luo, X., Wang, Z., Li, Z., Luo, X., et al. (2024). A foundation model\nidentiﬁes broad-spectrum antimicrobial peptides against drug-resistant bacterial\ninfection. Nat. Commun.15, 7538. doi: 10.1038/s41467-024-51933-2\nLi, Y., Wei, X., Yang, Q., Xiong, A., Li, X., Zou, Q., et al. (2024). msBERT-Promoter: a\nmulti-scale ensemble predictor based on BERT pre-trained model for the two-stage\nprediction of DNA promoters and their strengths. BMC Biol. 22, 126. doi: 10.1186/\ns12915-024-01923-z\nLiu, G., Chen, L., Wu, Y., Han, Y., Bao, Y., and Zhang, T. (2025). PDLLMs: A group\nof tailored DNA large language models for analyzing plant genomes. Mol. Plant 18,\n175– 178. doi: 10.1016/j.molp.2024.12.006\nLiu, Z., Dong, W., Jiang, W., and He, Z. (2019). csDMA: an improved bioinformatics\ntool for identifying DNA 6 mA modi ﬁcations via Chou ’s 5-step rule. Sci. Rep.9, 13109.\ndoi: 10.1038/s41598-019-49430-4\nLiu, Y., Shen, X., Gong, Y., Liu, Y., Song, B., and Zeng, X. (2024). Sequence\nAlignment/Map format: a comprehensive review of approaches and applications.\nBrieﬁngs Bioinf.24, bbad320. doi: 10.1093/bib/bbad320\nLv, H., Dao, F. Y., Guan, Z. X., Zhang, D., Tan, J. X., Zhang, Y., et al. (2019).\niDNA6mA-rice: A computational tool for detecting N6-methyladenine sites in rice.\nFront. Genet.10, 793. doi: 10.3389/fgene.2019.00793\nLv, Z., Ding, H., Wang, L., and Zou, Q. (2021). A convolutional neural network using\ndinucleotide one-hot encoder for identifying DNA N6-methyladenine sites in the rice\ngenome. Neurocomputing 422, 214 – 221. doi: 10.1016/j.neucom.2020.09.056\nLv, H., Zhang, Z. M., Li, S. H., Tan, J. X., Chen, W., and Lin, H. (2020). Evaluation of\ndifferent computational methods o n 5-methylcytosine sites identi ﬁcation. Brief\nBioinform. 21, 982 – 995. doi: 10.1093/bib/bbz048\nMcInnes, L., and Healy, J. (2018). UMAP: uniform manifold approximation and\nprojection for dimension reduction. arXiv (USA), abs/1802.03426. doi: 10.48550/\narXiv.1802.03426\nMeher, P. K., Hati, S., Sahu, T. K., Pradhan, U., Gupta, A., and Rath, S. N. (2024).\nSVM-root: identiﬁcation of root-associated proteins in plants by employing the support\nvector machine with sequence-derived features. Curr. Bioinf.19, 91 – 102. doi: 10.2174/\n1574893618666230417104543\nMoeckel, C., Mareboina, M., Konnaris, M. A., Chan, C. S. Y., Mouratidis, I.,\nMontgomery, A., et al. (2024). A survey of k-mer methods and applications in\nbioinformatics. Comput. Struct. Biotechnol. J. 23, 2289 – 2303. doi: 10.1016/\nj.csbj.2024.05.025\nO’Brown, Z. K., and Greer, E. L. (2016). N6-methyladenine: A conserved and\ndynamic DNA mark. Adv. Exp. Med. Biol. 945, 213 – 246. doi: 10.1007/978-3-319-\n43624-1_10\nPark, S., Wahab, A., Nazari, I., Ryu, J. H., and Chong, K. T. (2020). i6mA-DNC:\nPrediction of DNA N6-Methyladenosine sites in rice genome based on dinucleotide\nrepresentation using deep learning. Chemom Intell Lab. Syst.204, 104102. doi: 10.1016/\nj.chemolab.2020.104102\nPian, C., Zhang, G., Li, F., and Fan, X. (2019). MM-6mAPred: identifying DNA N6-\nmethyladenine sites based on Markov model. Bioinformatics 36, 388– 392. doi: 10.1093/\nbioinformatics/btz556\nPress, O., Smith, N. A., and Lewis, M. (2021). Train short, test long: attention with\nlinear biases enables input length extrapolation. arXiv (USA) , abs/2108.12409.\ndoi: 10.48550/arXiv.2108.12409\nQiao, B., Wang, S., Hou, M., Chen, H., Zhou, Z., Xie, X., et al. (2024). Identifying\nnucleotide-binding leucine-rich repeat receptor and pathogen effector pairing using\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org09\ntransfer-learning and bilinear attention network. Bioinformatics 40, btae581.\ndoi: 10.1093/bioinformatics/btae581\nRives, A., Meier, J., Sercu, T., Goyal, S., Lin, Z., Liu, J., et al. (2021). Biological\nstructure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences. Proc. Natl. Acad. Sci. United States America118, e2016239118.\ndoi: 10.1073/pnas.2016239118\nRomero, F. M., and Gatica-Arias, A. (2019). CRISPR/cas9: development and\napplication in rice breeding. Rice Sci.26, 265 – 281. doi: 10.1016/j.rsci.2019.08.001\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare\nwords with subword units. In Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics , Berlin, Germany. Association for Computational\nLinguistics. (Volume 1: Long Papers): 1715 – 1725, doi: 10.18653/v1/P16-1162\nShao, M., Tian, M., Chen, K., Jiang, H., Zhang, S., Li, Z., et al. (2024). Leveraging\nrandom effects in cistrome-wide association studies for decoding the genetic\ndeterminants of prostate cancer. Adv Sci.11, 2400815. doi: 10.1002/advs.202400815\nSinha, D., Dasmandal, T., Yeasin, M., Mishra, D. C., Rai, A., and Archak, S. (2023).\nEpiSemble: A novel ensemble-based machine-learning framework for prediction of\nDNA N6-methyladenine sites using hybrid features selection approach for crops. Curr.\nBioinf. 18, 587 – 597. doi: 10.2174/1574893618666230316151648\nSoylu, N. N., and Sefer, E. (2024). DeepPTM: protein post-translational modi ﬁcation\nprediction from protein sequences by combining deep protein language model with\nvision transformers. Curr. Bioinf. 19, 810 – 824. doi: 10.2174/\n0115748936283134240109054157\nTeng, Z., Zhao, Z., Li, Y., Tian, Z., Guo, M., Lu, Q., et al. (2022). i6mA-vote: cross-\nspecies identi ﬁcation of DNA N6-methyladenine sites in plant genomes based on\nensemble learning with voting. Front. Plant Sci. 13, 845835. doi: 10.3389/\nfpls.2022.845835\nTheodoris, C. V., Xiao, L., Chopra, A., Chaf ﬁn, M. D., Al Sayed, Z. R., Hill, M. C.,\net al. (2023). Transfer learning enables predictions in network biology. Nature 618,\n616– 624. doi: 10.1038/s41586-023-06139-9\nWang, L., Ding, Y., Tiwari, P., Xu, J., Lu, W., Muhammad, K., et al. (2023). A\ndeep multiple kernel learning-based hi gher-order fuzzy inference system for\nidentifying DNA N4-methylcytosine sites. Inf. Sci. 630, 40 – 52. doi: 10.1016/\nj.ins.2023.01.149\nWang, R., Jiang, Y., Jin, J., Yin, C., Yu, H., Wang, F., et al. (2023). DeepBIO: an\nautomated and interpretable deep-learning platform for high-throughput biological\nsequence prediction, functional annotation and visualization analysis. Nucleic Acids\nRes. 51, 3017 – 3029. doi: 10.1093/nar/gkad055\nWang, G., Lou, X., Guo, F., Kwok, D., and Cao, C. (2024). EHR-HGCN: an enhanced\nhybrid approach for text classi ﬁcation using heterogeneous graph convolutional\nnetworks in electronic health records. IEEE J. Biomed. Health Inf. 28, 1668 – 1679.\ndoi: 10.1109/JBHI.2023.3346210\nWang, Y., Zhai, Y., Ding, Y., and Zou, Q. (2024). SBSM-Pro: support bio-sequence\nmachine for proteins. Sci. China-Inf Sci.67, 212106. doi: 10.1007/s11432-024-4171-9\nWei, L., He, W., Malik, A., Su, R., Cui, L., and Manavalan, B. (2021). Computational\nprediction and interpretation of cell-speci ﬁc replication origin sites from multiple\neukaryotes by exploiting stacking framework. Brief Bioinform. 22, bbaa275.\ndoi: 10.1093/bib/bbaa275\nXie, H., Ding, Y., Qian, Y., Tiwari, P., and Guo, F. (2024). Structured Sparse\nRegularization based Random Vector Functional Link Networks for DNA N4-\nmethylcytosine sites prediction. Expert Syst. Appl. 235, 121157. doi: 10.1016/\nj.eswa.2023.121157\nXie, X., Gui, L., Qiao, B., Wang, G., Huang, S., Zhao, Y., et al. (2024). Deep learning in\ntemplate-free de novo biosynthetic p athway design of natural products. Brief\nBioinform. 25, bbae495. doi: 10.1093/bib/bbae495\nXie, H., Wang, L., Qian, Y., Ding, Y., and Guo, F. (2025). Methyl-GP: accurate\ngeneric DNA methylation prediction based on a language model and representation\nlearning. Nucleic Acids Res.53, gkaf223. doi: 10.1093/nar/gkaf223\nXu, H., Hu, R., Jia, P., and Zhao, Z. (2020). 6mA-Finder: a novel online tool for\npredicting DNA N6-methyladenine sites in genomes. Bioinformatics 36, 3257 – 3259.\ndoi: 10.1093/bioinformatics/btaa113\nXue, T., Zhang, S., and Qiao, H. (2021). i6mA-VC: A multi-classi ﬁer voting method\nfor the computational identi ﬁcation of DNA N6-methyladenine sites. Interdiscip Sci.\n13, 413 – 425. doi: 10.1007/s12539-021-00429-4\nYang, H., Luo, Y., Ren, X., Wu, M., He, X., Peng, B., et al. (2021). Risk Prediction of\nDiabetes: Big data mining with fusion of multifarious physical examination indicators.\nInf. Fusion75, 140 – 149. doi: 10.1016/j.inffus.2021.02.015\nYang, Q., Zhu, W., Tang, X., Wu, Y., Liu, G., Zhao, D., et al. (2024). Improving rice\ngrain shape through upstream ORF editing-mediated translation regulation. Plant\nPhysiol. 197, kiae557. doi: 10.1093/plphys/kiae557\nYu, H., and Dai, Z. (2019). SNNRice6mA: A deep learning method for predicting\nDNA N6-methyladenine sites in rice genome. Front. Genet. 10, 1071. doi: 10.3389/\nfgene.2019.01071\nZhang, H. Q., Arif, M., Thafar, M. A., Albaradei, S., Cai, P., Zhang, Y., et al. (2025).\nPMPred-AE: a computational model for t he detection and interpretation of\npathological myopia based on arti ﬁcial intelligence. Front. Med. (Lausanne) 12,\n1529335. doi: 10.3389/fmed.2025.1529335\nZhang, G., Huang, H., Liu, D., Cheng, Y., Liu, X., Zhang, W., et al. (2015). N6-\nmethyladenine DNA modi ﬁcation in drosophila. Cell 161, 893 – 906. doi: 10.1016/\nj.cell.2015.04.018\nZ h a n g ,Q . ,L i a n g ,Z . ,C u i ,X . ,J i ,C . ,L i ,Y . ,Z h a n g ,P . ,e ta l .( 2 0 1 8 ) .N ( 6 ) -\nmethyladenine DNA methylation in japonica and indica rice genomes and its\nassociation with gene expression, plant development, and stress responses. Mol.\nPlant 11, 1492 – 1508. doi: 10.1016/j.molp.2018.11.005\nZhou, Z., Ji, Y., Li, W., Dutta, P., Davuluri, R. V., and Liu, H. (2023). DNABERT-2:\nefﬁcient foundation model and benchmark for multi-species genome. arXiv (USA), abs/\n2306.15006. https://ui.adsabs.harvard.edu/abs/2023arXiv230615006Z/abstract\nZhou, S., Li, X., Liu, Q., Zhao, Y., Jiang, W., Wu, A., et al. (2021). DNA demethylases\nremodel DNA methylation in rice gametes and zygote and are required for\nreproduction. Mol. Plant14, 1569 – 1583. doi: 10.1016/j.molp.2021.06.006\nZhou, C., Wang, C., Liu, H., Zhou, Q., Liu, Q., Guo, Y., et al. (2018). Identi ﬁcation\nand analysis of adenine N6-methylation sites in the rice genome. Nat. Plants 4, 554 –\n563. doi: 10.1038/s41477-018-0214-x\nZhou, Z., Xiao, C., Yin, J., She, J., Duan, H., Liu, C., et al. (2024). PSAC-6mA: 6mA\nsite identi ﬁer using self-attention capsule network based on sequence-positioning.\nComput. Biol. Med.171, 108129. doi: 10.1016/j.compbiomed.2024.108129\nZhu, S., Beaulaurier, J., Deikus, G., Wu, T. P., Strahl, M., Hao, Z., et al. (2018).\nMapping and characterizing N6-methyladenine in eukaryotic genomes using single-\nmolecule real-time sequencing. Genome Res. 28, 1067 – 1078. doi: 10.1101/\ngr.231068.117\nZhu, H., Hao, H., and Yu, L. (2024). Identi ﬁcation of microbe – disease signed\nassociations via multi-scale variational graph autoencoder based on signed message\npropagation. BMC Biol.22, 172. doi: 10.1186/s12915-024-01968-0\nZou, K., Wang, S., Wang, Z., Zhang, Z., and Yang, F. (2023). HAR_Locator: a novel\nprotein subcellular location prediction model of immunohistochemistry images based\non hybrid attention modules and residual units. Front. Mol. Biosci. 10, 1171429.\ndoi: 10.3389/fmolb.2023.1171429\nZou, Q., Xing, P., Wei, L., and Liu, B. (2019). Gene2vec: gene subsequence\nembedding for prediction of mammalian N6-methyladenosine sites from mRNA.\nRNA 25, 205 – 218. doi: 10.1261/rna.069112.118\nZou, H. L., Yang, F., and Yin, Z. J. (2022). Integrating multiple sequence features for\nidentifying anticancer peptides. Comput. Biol. Chem. 99, 7. doi: 10.1016/\nj.compbiolchem.2022.107711\nZulﬁqar, H., Guo, Z., Ahmad, R. M., Ahmed, Z., Cai, P., Chen, X., et al. (2023). Deep-\nSTP: a deep learning-based approach to predict snake toxin proteins by using word\nembeddings. Front. Med. (Lausanne)10, 1291352. doi: 10.3389/fmed.2023.1291352\nZhang et al. 10.3389/fpls.2025.1626539\nFrontiers inPlant Science frontiersin.org10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7103795409202576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5405293107032776
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.47816213965415955
    },
    {
      "name": "Locus (genetics)",
      "score": 0.46996280550956726
    },
    {
      "name": "Receiver operating characteristic",
      "score": 0.4619596004486084
    },
    {
      "name": "Machine learning",
      "score": 0.33489909768104553
    },
    {
      "name": "Computational biology",
      "score": 0.3348415493965149
    },
    {
      "name": "Gene",
      "score": 0.27911126613616943
    },
    {
      "name": "Biology",
      "score": 0.25725215673446655
    },
    {
      "name": "Genetics",
      "score": 0.1780339479446411
    }
  ]
}