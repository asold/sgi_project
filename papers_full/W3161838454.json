{
  "title": "Segmenter: Transformer for Semantic Segmentation",
  "url": "https://openalex.org/W3161838454",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2921289895",
      "name": "Robin Strudel",
      "affiliations": [
        "Université Paris Sciences et Lettres",
        "École Normale Supérieure - PSL",
        "Centre National de la Recherche Scientifique"
      ]
    },
    {
      "id": "https://openalex.org/A2100638334",
      "name": "Ricardo Garcia",
      "affiliations": [
        "Université Paris Sciences et Lettres",
        "Centre National de la Recherche Scientifique",
        "École Normale Supérieure - PSL"
      ]
    },
    {
      "id": "https://openalex.org/A2170872680",
      "name": "Ivan Laptev",
      "affiliations": [
        "École Normale Supérieure - PSL",
        "Centre National de la Recherche Scientifique",
        "Université Paris Sciences et Lettres"
      ]
    },
    {
      "id": "https://openalex.org/A2111851554",
      "name": "Cordelia Schmid",
      "affiliations": [
        "Université Paris Sciences et Lettres",
        "Centre National de la Recherche Scientifique",
        "École Normale Supérieure - PSL"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2158778629",
    "https://openalex.org/W2125215748",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W2981689412",
    "https://openalex.org/W6701947533",
    "https://openalex.org/W3132455321",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2563705555",
    "https://openalex.org/W6768371451",
    "https://openalex.org/W6745136726",
    "https://openalex.org/W6754879843",
    "https://openalex.org/W6775845032",
    "https://openalex.org/W2963872524",
    "https://openalex.org/W2955813853",
    "https://openalex.org/W2947263797",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W6754713557",
    "https://openalex.org/W2981963416",
    "https://openalex.org/W6788620109",
    "https://openalex.org/W2507296351",
    "https://openalex.org/W2981899103",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W2753588254",
    "https://openalex.org/W6781829950",
    "https://openalex.org/W2022508996",
    "https://openalex.org/W2991062542",
    "https://openalex.org/W2955058313",
    "https://openalex.org/W2965391153",
    "https://openalex.org/W6749897870",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6785727093",
    "https://openalex.org/W6696085341",
    "https://openalex.org/W3035339581",
    "https://openalex.org/W6779387320",
    "https://openalex.org/W6784930956",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6775170262",
    "https://openalex.org/W6748481559",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W6632360823",
    "https://openalex.org/W3034355852",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6780975210",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W2963881378",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W6779806301",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6739696289",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W2963833733",
    "https://openalex.org/W2998784361",
    "https://openalex.org/W6798160016",
    "https://openalex.org/W1994616650",
    "https://openalex.org/W2557406251",
    "https://openalex.org/W2395611524",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2964288706",
    "https://openalex.org/W2963840672",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3047385447",
    "https://openalex.org/W3098967720",
    "https://openalex.org/W3100039191",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2890782586",
    "https://openalex.org/W2630837129",
    "https://openalex.org/W2964050767",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3202141913",
    "https://openalex.org/W3099495704",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W3175937066",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3107634219",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3109301572",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W1546771929",
    "https://openalex.org/W3126721948"
  ],
  "abstract": "Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embeddings with a point-wise linear decoder or a mask transformer decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes.",
  "full_text": "Segmenter: Transformer for Semantic Segmentation\nRobin Strudel*\nInria†\nRicardo Garcia*\nInria†\nIvan Laptev\nInria†\nCordelia Schmid\nInria†\nAbstract\nImage segmentation is often ambiguous at the level of\nindividual image patches and requires contextual informa-\ntion to reach label consensus. In this paper we introduce\nSegmenter, a transformer model for semantic segmentation.\nIn contrast to convolution-based methods, our approach\nallows to model global context already at the ﬁrst layer\nand throughout the network. We build on the recent Vision\nTransformer (ViT) and extend it to semantic segmentation.\nTo do so, we rely on the output embeddings corresponding\nto image patches and obtain class labels from these embed-\ndings with a point-wise linear decoder or a mask trans-\nformer decoder. We leverage models pre-trained for im-\nage classiﬁcation and show that we can ﬁne-tune them on\nmoderate sized datasets available for semantic segmenta-\ntion. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a\nmask transformer generating class masks. We conduct an\nextensive ablation study to show the impact of the different\nparameters, in particular the performance is better for large\nmodels and small patch sizes. Segmenter attains excellent\nresults for semantic segmentation. It outperforms the state\nof the art on both ADE20K and Pascal Context datasets and\nis competitive on Cityscapes.\n1. Introduction\nSemantic segmentation is a challenging computer vi-\nsion problem with a wide range of applications includ-\ning autonomous driving, robotics, augmented reality, im-\nage editing, medical imaging and many others [27, 28, 45].\nThe goal of semantic segmentation is to assign each im-\nage pixel to a category label corresponding to the under-\nlying object and to provide high-level image representa-\ntions for target tasks, e.g. detecting the boundaries of peo-\nple and their clothes for virtual try-on applications [29].\n*Equal contribution.\n†Inria, ´Ecole normale sup ´erieure, CNRS, PSL Research University,\n75005 Paris, France.\nCode: https://github.com/rstrudel/segmenter\nSegmentation map\nSky\nSidewalk\nPerson\nPatch attention maps\nFigure 1: Our approach for semantic segmentation is purely\ntransformer based. It leverages the global image context at\nevery layer of the model. Attention maps from the ﬁrst Seg-\nmenter layer are displayed for three 8 ×8 patches and high-\nlight the early grouping of patches into semantically mean-\ningful categories. The original image (top-left) is overlayed\nwith segmentation masks produced by our method.\nDespite much effort and large progress over recent years\n[10, 22, 31, 37, 48, 65, 66], image segmentation remains a\nchallenging problem due to rich intra-class variation, con-\ntext variation and ambiguities originating from occlusions\nand low image resolution.\nRecent approaches to semantic segmentation typically\nrely on convolutional encoder-decoder architectures where\nthe encoder generates low-resolution image features and the\ndecoder upsamples features to segmentation maps with per-\npixel class scores. State-of-the-art methods deploy Fully\nConvolutional Networks (FCN) [44] and achieve impres-\nsive results on challenging segmentation benchmarks [10,\n23, 57, 58, 60, 64, 66]. These methods rely on learnable\nstacked convolutions that can capture semantically rich in-\nformation and have been highly successful in computer vi-\nsion. The local nature of convolutional ﬁlters, however,\narXiv:2105.05633v3  [cs.CV]  2 Sep 2021\nlimits the access to the global information in the image.\nMeanwhile, such information is particularly important for\nsegmentation where the labeling of local patches often de-\npends on the global image context. To circumvent this issue,\nDeepLab methods [8, 9, 10] introduce feature aggregation\nwith dilated convolutions and spatial pyramid pooling. This\nallows to enlarge the receptive ﬁelds of convolutional net-\nworks and to obtain multi-scale features. Following recent\nprogresses in NLP [50], several segmentation methods ex-\nplore alternative aggregation schemes based on channel or\nspatial [22, 23, 61] attention and point-wise [66] attention to\nbetter capture contextual information. Such methods, how-\never, still rely on convolutional backbones and are, hence,\nbiased towards local interactions. An extensive use of spe-\ncialised layers to remedy this bias [8, 10, 22, 58] suggests\nlimitations of convolutional architectures for segmentation.\nTo overcome these limitations, we formulate the prob-\nlem of semantic segmentation as a sequence-to-sequence\nproblem and use a transformer architecture [50] to leverage\ncontextual information at every stage of the model. By de-\nsign, transformers can capture global interactions between\nelements of a scene and have no built-in inductive prior,\nsee Figure 1. However, the modeling of global interac-\ntions comes at a quadratic cost which makes such meth-\nods prohibitively expensive when applied to raw image pix-\nels [11]. Following the recent work on Vision Transform-\ners (ViT) [19, 49], we split the image into patches and\ntreat linear patch embeddings as input tokens for the trans-\nformer encoder. The contextualized sequence of tokens pro-\nduced by the encoder is then upsampled by a transformer\ndecoder to per-pixel class scores. For decoding, we con-\nsider either a simple point-wise linear mapping of patch\nembeddings to class scores or a transformer-based decod-\ning scheme where learnable class embeddings are processed\njointly with patch tokens to generate class masks. We con-\nduct an extensive study of transformers for segmentation\nby ablating model regularization, model size, input patch\nsize and its trade-off between accuracy and performance.\nOur Segmenter approach attains excellent results while re-\nmaining simple, ﬂexible and fast. In particular, when us-\ning large models with small input patch size the best model\nreaches a mean IoU of 53.63% on the challenging ADE20K\n[68] dataset, surpassing all previous state-of-the-art convo-\nlutional approaches by a large margin of 5.3%. Such im-\nprovement partly stems from the global context captured by\nour method at every stage of the model as highlighted in\nFigure 1.\nIn summary, our work provides the following four con-\ntributions: (i) We propose a novel approach to semantic seg-\nmentation based on the Vision Transformer (ViT) [19] that\ndoes not use convolutions, captures contextual information\nby design and outperforms FCN based approaches. (ii) We\npresent a family of models with varying levels of resolu-\ntion which allows to trade-off between precision and run-\ntime, ranging from state-of-the-art performance to models\nwith fast inference and good performances. (iii) We pro-\npose a transformer-based decoder generating class masks\nwhich outperforms our linear baseline and can be extended\nto perform more general image segmentation tasks. (iv) We\ndemonstrate that our approach yieldsstate-of-the-art results\non both ADE20K [68] and Pascal Context [38] datasets and\nis competitive on Cityscapes [14].\n2. Related work\nSemantic segmentation. Methods based on Fully Convo-\nlutional Networks (FCN) combined with encoder-decoder\narchitectures have become the dominant approach to se-\nmantic segmentation. Initial approaches [21, 36, 39, 40]\nrely on a stack of consecutive convolutions followed by spa-\ntial pooling to perform dense predictions. Consecutive ap-\nproaches [1, 4, 34, 41, 43] upsample high-level feature maps\nand combine them with low-level feature maps during de-\ncoding to both capture global information and recover sharp\nobject boundaries. To enlarge the receptive ﬁeld of convolu-\ntions in the ﬁrst layers, several approaches [8, 33, 59] have\nproposed dilated or atrous convolutions. To capture global\ninformation in higher layers, recent work [9, 10, 65] em-\nploys spatial pyramid pooling to capture multi-scale con-\ntextual information. Combining these enhancements along\nwith atrous spatial pyramid pooling, Deeplabv3+ [10] pro-\nposes a simple and effective encoder-decoder FCN archi-\ntecture. Recent work [22, 23, 57, 58, 61, 66] replace coarse\npooling by attention mechanisms on top of the encoder fea-\nture maps to better capture long-range dependencies.\nWhile recent segmentation methods are mostly focused\non improving FCN, the restriction to local operations im-\nposed by convolutions may imply inefﬁcient processing of\nglobal image context and suboptimal segmentation results.\nHence, we propose a pure transformer architecture that cap-\ntures global context at every layer of the model during the\nencoding and decoding stages.\nTransformers for vision. Transformers [50] are now\nstate of the art in many Natural Language Processing (NLP)\ntasks. Such models rely on self-attention mechanisms and\ncapture long-range dependencies among tokens (words) in\na sentence. In addition, transformers are well suited for par-\nallelization, facilitating training on large datasets. The suc-\ncess of transformers in NLP has inspired several methods\nin computer vision combining CNNs with forms of self-\nattention to address object detection [7], semantic segmen-\ntation [53], panoptic segmentation [51], video processing\n[54] and few-shot classiﬁcation [18].\nRecently, the Vision Transformer (ViT) [19] introduced\na convolution-free transformer architecture for image clas-\nsiﬁcation where input images are processed as sequences\nof patch tokens. While ViT requires training on very\nlarge datasets, DeiT [49] proposes a token-based distilla-\ntion strategy and obtains a competitive vision transformer\ntrained on the ImageNet-1k [16] dataset using a CNN as a\n2\nFigure 2: Overview of our approach Segmenter. (Left) Encoder: The image patches are projected to a sequence of embed-\ndings and then encoded with a transformer. (Right) Decoder: A mask transformer takes as input the output of the encoder\nand class embeddings to predict segmentation masks. See text for details.\nteacher. Concurrent work extends this work to video classi-\nﬁcation [2, 6] and semantic segmentation [35, 67]. In more\ndetail, SETR [67] uses a ViT backbone and a standard CNN\ndecoder. Swin Transformer [35] uses a variant of ViT, com-\nposed of local windows, shifted between layers and Upper-\nNet as a pyramid FCN decoder.\nHere, we propose Segmenter, a transformer encoder-\ndecoder architecture for semantic image segmentation. Our\napproach relies on a ViT backbone and introduces a mask\ndecoder inspired by DETR [7]. Our architecture does not\nuse convolutions, captures global image context by design\nand results in competitive performance on standard image\nsegmentation benchmarks.\n3. Our approach: Segmenter\nSegmenter is based on a fully transformer-based\nencoder-decoder architecture mapping a sequence of patch\nembeddings to pixel-level class annotations. An overview\nof the model is shown in Figure 2. The sequence of patches\nis encoded by a transformer encoder described in Section\n3.1 and decoded by either a point-wise linear mapping or\na mask transformer described in Section 3.2. Our model is\ntrained end-to-end with a per-pixel cross-entropy loss. At\ninference time, argmax is applied after upsampling to ob-\ntain a single class per pixel.\n3.1. Encoder\nAn image x ∈ RH×W×C is split into a sequence of\npatches x = [x1,...,x N ] ∈ RN×P2×C where (P,P ) is\nthe patch size, N = HW/P2 is the number of patches\nand C is the number of channels. Each patch is ﬂattened\ninto a 1D vector and then linearly projected to a patch\nembedding to produce a sequence of patch embeddings\nx0 = [Ex1,..., ExN ] ∈ RN×D where E ∈ RD×(P2C).\nTo capture positional information, learnable position em-\nbeddings pos = [pos1,..., posN ] ∈RN×D are added to the\nsequence of patches to get the resulting input sequence of\ntokens z0 = x0 + pos.\nA transformer [50] encoder composed of Llayers is ap-\nplied to the sequence of tokens z0 to generate a sequence of\ncontextualized encodings zL ∈RN×D. A transformer layer\nconsists of a multi-headed self-attention (MSA) block fol-\nlowed by a point-wise MLP block of two layers with layer\nnorm (LN) applied before every block and residual connec-\ntions added after every block:\nai−1 = MSA(LN(zi−1)) +zi−1, (1)\nzi = MLP(LN(ai−1)) +ai−1, (2)\nwhere i ∈ {1,...,L }. The self-attention mechanism is\ncomposed of three point-wise linear layers mapping tokens\nto intermediate representations, queries Q ∈RN×d, keys\nK ∈RN×d and values V ∈RN×d. Self-attention is then\ncomputed as follows\nMSA(Q,K,V) =softmax\n(\nQKT\n√\nd\n)\nV. (3)\nThe transformer encoder maps the input sequence z0 =\n[z0,1,...,z 0,N ] of embedded patches with position encod-\ning to zL = [zL,1,...,z L,N ], a contextualized encoding se-\nquence containing rich semantic information used by the\ndecoder. In the following section we introduce the decoder.\n3\n3.2. Decoder\nThe sequence of patch encodings zL ∈ RN×D is de-\ncoded to a segmentation maps ∈RH×W×K where Kis the\nnumber of classes. The decoder learns to map patch-level\nencodings coming from the encoder to patch-level class\nscores. Next these patch-level class scores are upsampled\nby bilinear interpolation to pixel-level scores. We describe\nin the following a linear decoder, which serves as a baseline,\nand our approach, a mask transformer, see Figure 2.\nLinear. A point-wise linear layer is applied to the patch\nencodings zL ∈RN×D to produce patch-level class logits\nzlin ∈RN×K. The sequence is then reshaped into a 2D\nfeature map slin ∈RH/P×W/P ×K and bilinearly upsam-\npled to the original image size s ∈RH×W×K. A softmax\nis then applied on the class dimension to obtain the ﬁnal\nsegmentation map.\nMask Transformer. For the transformer-based decoder,\nwe introduce a set of K learnable class embeddings cls =\n[cls1,..., clsK] ∈RK×D where K is the number of classes.\nEach class embedding is initialized randomly and assigned\nto a single semantic class. It will be used to generate the\nclass mask. The class embeddings cls are processed jointly\nwith patch encodings zL by the decoder as depicted in Fig-\nure 2. The decoder is a transformer encoder composed ofM\nlayers. Our mask transformer generates K masks by com-\nputing the scalar product between L2-normalized patch em-\nbeddings z′\nM ∈RN×D and class embeddings c ∈RK×D\noutput by the decoder. The set of class masks is computed\nas follows\nMasks(z′\nM,c) =z′\nMcT (4)\nwhere Masks (z′\nM,c) ∈ RN×K is a set of patch se-\nquence. Each mask sequence is then reshaped into a 2D\nmask to form smask ∈ RH/P×W/P ×K and bilinearly\nupsampled to the original image size to obtain a feature\nmap s ∈ RH×W×K. A softmax is then applied on the\nclass dimension followed by a layer norm to obtain pixel-\nwise class score forming the ﬁnal segmentation map. The\nmasks sequences are softly exclusive to each other i.e.∑K\nk=1 si,j,k = 1for all (i,j) ∈H×W.\nOur mask transformer is inspired by DETR [7], Max-\nDeepLab [52] and SOLO-v2 [55] which introduce object\nembeddings [7] to produce instance masks [52, 55]. How-\never, unlike our method, MaxDeep-Lab uses an hybrid ap-\nproach based on CNNs and transformers and splits the pixel\nand class embeddings into two streams because of compu-\ntational constraints. Using a pure transformer architecture\nand leveraging patch level encodings, we propose a sim-\nple approach that processes the patch and class embeddings\njointly during the decoding phase. Such approach allows to\nproduce dynamical ﬁlters, changing with the input. While\nwe address semantic segmentation in this work, our mask\ntransformer can also be directly adapted to perform panop-\ntic segmentation by replacing the class embeddings by ob-\nject embeddings.\nModel Backbone Layers Token size Heads Params\nSeg-Ti ViT-Ti 12 192 3 6M\nSeg-S ViT-S 12 384 6 22M\nSeg-B ViT-B 12 768 12 86M\nSeg-B† DeiT-B 12 768 12 86M\nSeg-L ViT-L 24 1024 16 307M\nTable 1: Details of Transformer variants.\n4. Experimental results\n4.1. Datasets and metrics\nADE20K [68]. This dataset contains challenging scenes\nwith ﬁne-grained labels and is one of the most challenging\nsemantic segmentation datasets. The training set contains\n20,210 images with 150 semantic classes. The validation\nand test set contain 2,000 and 3,352 images respectively.\nPascal Context[38]. The training set contains 4,996 im-\nages with 59 semantic classes plus a background class. The\nvalidation set contains 5,104 images.\nCityscapes [14]. The dataset contains 5,000 images from\n50 different cities with 19 semantic classes. There are 2,975\nimages in the training set, 500 images in the validation set\nand 1,525 images in the test set.\nMetrics. We report Intersection over Union (mIoU) aver-\naged over all classes.\n4.2. Implementation details\nTransformer models.For the encoder, we build upon the\nvision transformer ViT [19] and consider ”Tiny”, ”Small”,\n”Base” and ”Large” models described in Table 1. The pa-\nrameters varying in the transformer encoder are the num-\nber of layers and the token size. The head size of a multi-\nheaded self-attention (MSA) block is ﬁxed to 64, the num-\nber of heads is the token size divided by the head size and\nthe hidden size of the MLP following MSA is four times the\ntoken size. We also use DeiT [49], a variant of the vision\ntransformer. We consider models representing the image at\ndifferent resolutions and use input patch sizes 8×8, 16×16\nand 32×32. In the following, we use an abbreviation to\ndescribe the model variant and patch size, for instance Seg-\nB/16 denotes the ”Base” variant with 16×16 input patch\nsize. Models based on DeiT are denoted with a †, for in-\nstance Seg-B†/16.\nImageNet pre-training. Our Segmenter models are pre-\ntrained on ImageNet, ViT is pre-trained on ImageNet-21k\nwith strong data augmentation and regularization [47] and\nits variant DeiT is pre-trained on ImageNet-1k. The origi-\nnal ViT models [19] have been trained with random crop-\nping only, whereas the training procedure proposed by [47]\nuses a combination of dropout [46] and stochastic depth\n[30] as regularization and Mixup [62] and RandAugment\n[15] as data augmentations. This signiﬁcantly improves\nthe ImageNet top-1 accuracy, i.e., it obtains a gain of +2%\non ViT-B/16. We ﬁne-tuned ViT-B/16 on ADE20K with\n4\n(a) Patch size 32 × 32\n (b) Patch size 16 × 16\n (c) Patch size 8 × 8\n (d) Ground Truth\nFigure 3: Impact of the model patch size on the segmentation maps.\nmodels from [19] and [47] and observe a signiﬁcant differ-\nence, namely a mIoU of 45.69% and 48.06% respectively.\nIn the following, all the Segmenter models will be initial-\nized with the improved ViT models from [47]. We use pub-\nlicly available models provided by the image classiﬁcation\nlibrary timm [56] and Google research [20]. Both models\nare pre-trained at an image resolution of 224 and ﬁne-tuned\non ImageNet-1k at a resolution of 384, except for ViT-B/8\nwhich has been ﬁne-tuned at a resolution of 224. We keep\nthe patch size ﬁxed and ﬁne-tune the models for the se-\nmantic segmentation task at higher resolution depending on\nthe dataset. As the patch size is ﬁxed, increasing resolu-\ntion results in longer token sequences. Following [19], we\nbilinearly interpolate the pre-trained position embeddings\naccording to their original position in the image to match\nthe ﬁne-tuning sequence length. The decoders, described\nin Section 3.2 are initialized with random weights from a\ntruncated normal distribution [25].\nData augmentation. During training, we follow the stan-\ndard pipeline from the semantic segmentation library MM-\nSegmentation [13], which does mean substraction, random\nresizing of the image to a ratio between 0.5 and 2.0 and ran-\ndom left-right ﬂipping. We randomly crop large images and\npad small images to a ﬁxed size of 512×512 for ADE20K,\n480×480 for Pascal-Context and 768×768 for Cityscapes.\nOn ADE20K, we train our largest model Seg-L-Mask/16\nwith a resolution of 640×640, matching the resolution used\nby the Swin Transformer [35].\nOptimization. To ﬁne-tune the pre-trained models for the\nsemantic segmentation task, we use the standard pixel-wise\ncross-entropy loss without weight rebalancing. We use\nstochastic gradient descent (SGD) [42] as the optimizer\nwith a base learning rate γ0 and set weight decay to 0. Fol-\nlowing the seminal work of DeepLab [33] we adopt the\n”poly” learning rate decay γ = γ0 (1 − Niter\nNtotal\n)0.9 where\nNiter and Ntotal represent the current iteration number and\nthe total iteration number. For ADE20K, we set the base\nlearning rate γ0 to 10−3 and train for 160K iterations with\na batch size of 8. For Pascal Context, we set γ0 to 10−3\nand train for 80K iterations with a batch size of 16. For\nCityscapes, we set γ0 to 10−2 and train for 80K itera-\ntions with a batch size of 8. The schedule is similar to\nDeepLabv3+ [10] with learning rates divided by a factor\n10 except for Cityscapes where we use a factor of 1.\nStochastic Depth\n0.0 0.1 0.2\nDropout\n0.0 45.01 45.37 45.10\n0.1 42.02 42.30 41.14\n0.2 36.49 36.63 35.67\nTable 2: Mean IoU comparison of different regularization\nschemes using Seg-S/16 on ADE20K validation set.\nMethod Backbone Patch size Im/sec ImNet acc. mIoU (SS)\nSeg-Ti/16 ViT-Ti 16 396 78.6 39.03\nSeg-S/32 ViT-S 32 1032 80.5 40.64\nSeg-S/16 ViT-S 16 196 83.7 45.37\nSeg-B†/16 DeiT-B 16 92 85.2 47.08\nSeg-B/32 ViT-B 32 516 83.3 43.07\nSeg-B/16 ViT-B 16 92 86.0 48.06\nSeg-B/8 ViT-B 8 7 85.7 49.54\nSeg-L/16 ViT-L 16 33 87.1 50.71\nTable 3: Performance comparison of different Segmenter\nmodels with varying backbones and input patch sizes on\nADE20K validation set.\nInference. To handle varying image sizes during inference,\nwe use a sliding-window with a resolution matching the\ntraining size. For multi-scale inference, following standard\npractice [10] we use rescaled versions of the image with\nscaling factors of (0.5, 0.75, 1.0, 1.25, 1.5, 1.75) and left-\nright ﬂipping and average the results.\n4.3. Ablation study\nIn this section, we ablate different variants of our ap-\nproach on the ADE20K validation set. We investigate\nmodel regularization, model size, patch size, model perfor-\nmance, training dataset size, compare Segmenter to convo-\nlutional approaches and evaluate different decoders. Unless\nstated otherwise, we use the baseline linear decoder and re-\nport results using single-scale inference.\nRegularization. We ﬁrst compare two forms of regulariza-\ntion, dropout [46] and stochastic depth [30], and show that\nstochastic depth consistently improves transformer train-\ning for segmentation. CNN models rely on batch nor-\n5\nFigure 4: Images per second and mean IoU for our ap-\nproach compared to other methods on ADE20K validation\nset. Segmenter models offer a competitive trade-off in terms\nof performance and precision.\nmalization [32] which also acts as a regularizer. In con-\ntrast, transformers are usually composed of layer normaliza-\ntion [3] combined with dropout as a regularizer during train-\ning [17, 19]. Dropout randomly ignores tokens given as in-\nput of a block and stochastic depth randomly skips a learn-\nable block of the model during the forward pass. We com-\npare regularizations on Seg-S/16 based on ViT-S/16 back-\nbone. Table 2 shows that stochastic depth set to 0.1, drop-\nping 10% of the layers randomly, consistently improves\nthe performance, with 0.36% when the dropout is set to 0\ncompared to the baseline without regularization. Dropout\nconsistently hurts performances, either alone or when com-\nbined with stochastic depth. This is consistent with [49]\nwhich observed the negative impact of dropout for image\nclassiﬁcation. From now on, all the models will be trained\nwith stochastic depth set to 0.1 and without dropout.\nTransformer size.We now study the impact of transform-\ners size on performance by varying the number of layers\nand the tokens size for a ﬁxed patch size of 16. Table 3\nshows that performance scales nicely with the backbone ca-\npacity. When doubling the token dimension, from Seg-S/16\nto Seg-B/16, we get a 2.69% improvement. When doubling\nthe number of layers, from Seg-B/16 to Seg-L/16, we get\nan improvement of 2.65%. Finally, our largest Segmenter\nmodel, Seg-L/16, achieves a strong mIoU of 50.71% with a\nsimple decoding scheme on the ADE20K validation dataset\nwith single scale inference. The absence of tasks-speciﬁc\nlayers vastly used in FCN models suggests that transformer\nbased methods provide more expressive models, well suited\nfor semantic segmentation.\nPatch size. Representing an image with a patch sequence\nprovides a simple way to trade-off between speed and accu-\nracy by varying the patch size. While increasing the patch\nsize results in a coarser representation of the image, it re-\nMethod Decoder Small Medium Large mIoU (SS)\nDeepLab RNeSt-101 UNet 37.85 50.89 50.67 46.47\nSeg-B/32 Linear 31.95 47.82 49.44 43.07\nSeg-B-Mask/32 Mask 32.29 49.44 50.82 44.19\nSeg-B†/16 Linear 38.31 50.91 52.08 47.10\nSeg-B†-Mask/16 Mask 40.49 51.37 54.24 48.70\nSeg-B/16 Linear 39.57 51.32 53.28 48.06\nSeg-B-Mask/16 Mask 40.16 52.61 52.66 48.48\nSeg-B/8 Linear 41.43 54.35 52.85 49.54\nSeg-L/16 Linear 42.08 54.67 55.39 50.71\nSeg-L-Mask/16 Mask 42.02 54.83 57.06 51.30\nTable 4: Evaluation with respect to the object size\non ADE20k validation set (mean IoU). Comparison of\nDeepLabv3+ ResNeSt-101 to Segmenter models with a lin-\near or a mask transformer decoder.\nDataset Size 4k 8k 12k 16k 20k\nmIoU (SS) 38.31 41.87 43.42 44.61 45.37\nTable 5: Performance comparison of Seg-S/16 mod-\nels trained with increasing dataset size and evaluated on\nADE20K validation set.\nsults in a smaller sequence that is faster to process. The\nthird and fourth parts of Table 3 report the performance for\nViT backbones and varying patch sizes. We observe that the\npatch size is a key factor for semantic segmentation perfor-\nmance. It is similarly important to the model size. Indeed,\ngoing from a patch size 32 to 16 we observe an improve-\nment of 5% for Seg-B. For Seg-B, we also report results\nfor a patch size of 8 and report an mIoU of 49.54%, re-\nducing the gap from ViT-B/8 to ViT-L/16 to 1.17% while\nrequiring substantially fewer parameters. This trend shows\nthat reducing the patch size is a robust source of improve-\nment which does not introduce any parameters but requires\nto compute attention over longer sequences, increasing the\ncompute time and memory footprint. If it was computation-\nally feasible, ViT-L/8 would probably be the best perform-\ning model. Going towards more computation and memory\nefﬁcient transformers handling larger sequence of smaller\npatches is a promising direction.\nTo further study the impact of patch size, we show seg-\nmentation maps generated by Segmenter models with de-\ncreasing patch size in Figure 3. We observe that for a patch\nsize of 32, the model learns a globally meaningful seg-\nmentation but produces poor boundaries, for example the\ntwo persons on the left are predicted by a single blob. Re-\nducing the patch size leads to considerably sharper bound-\naries as can be observed when looking at the contours of\npersons. Hard to segment instances as the thin streetlight\npole in the background are only captured at a resolution\nof 8. In Table 4, we report mean IoU with respect to the\n6\nobject size and compare Segmenter to DeepLabv3+ with\nResNeSt backbone. To reproduce DeepLabv3+ results, we\nused models from the MMSegmentation library [13]. We\nobserve how Seg-B/8 improvement over Seg-B/16 comes\nmostly from small and medium instances with a gain of\n1.27% and 1.74% respectively. Also, we observe that over-\nall the biggest improvement of Segmenter over DeepLab\ncomes from large instances where Seg-L-Mask/16 shows\nan improvement of 6.39%.\nDecoder variants. In this section, we compare different\ndecoder variants. We evaluate the mask transformer intro-\nduced in Section 3.2 and compare it to the linear baseline.\nThe mask transformer has 2 layers with the same token and\nhidden size as the encoder. Table 4 reports the mean IoU\nperformance. The mask transformer provides consistent im-\nprovements over the linear baseline. The most signiﬁcant\ngain of 1.6% is obtained for Seg-B†/16, for Seg-B-Mask/32\nwe obtain a 1.1% improvement and for Seg-L/16 a gain of\n0.6%. In Table 4 we also examine the gain of different mod-\nels with respect to the object size. We observe gains both\non small and large objects, showing the beneﬁt of using dy-\nnamical ﬁlters. In most cases the gain is more signiﬁcant for\nlarge objects, i.e., 1.4% for Seg-B/32, 2.1% for Seg-B †/16\nand and 1.7% for Seg-L/16. The class embeddings learned\nby the mask transformer are semantically meaningful, i.e.,\nsimilar classes are nearby, see Figure 8 for more details.\nTransformer versus FCN.Table 4 and Table 6 compare\nour approach to FCN models and DeepLabv3+ [10] with\nResNeSt backbone [63], one of the best fully-convolutional\napproaches. Our transformer approach provides a signif-\nicant improvement over this state-of-the-art convolutional\napproach, highlighting the ability of transformers to capture\nglobal scene understanding. Segmenter consistently out-\nperforms DeepLab on large instances with an improvement\nof more than 4% for Seg-L/16 and 6% for Seg-L-Mask/16.\nHowever, DeepLab performs similarly to Seg-B/16 on small\nand medium instances while having a similar number of pa-\nrameters. Seg-B/8 and Seg-L/16 perform best on small and\nmedium instances though at higher computational cost.\nPerformance. In Figure 4, we compare our models to sev-\neral state-of-the-art methods in terms of images per sec-\nonds and mIoU and show a clear advantage of Segmenter\nover FCN based models (green curve). We also show\nthat our approach compares favorably to recent transformer\nbased approach, our largest model Seg-L-Mask/16 is on-\npar with Swin-L and outperforms SETR-MLA. We observe\nthat Seg/16 models perform best in terms of accuracy versus\ncompute time with Seg-B-Mask/16 offering a good trade-\noff. Seg-B-Mask/16 outperforms FCN based approaches\nwith similar inference speed, matches SETR-MLA while\nbeing twice faster and requiring less parameters and out-\nperforms Swin-B both in terms of inference speed and per-\nformance. Seg/32 models learn coarser segmentation maps\nas discussed in the previous section and enable fast infer-\nence with 400 images per second for Seg-B-Mask/32, four\nMethod Backbone Im/sec mIoU +MS\nOCR [60] HRNetV2-W48 83 - 45.66\nACNet [24] ResNet-101 - - 45.90\nDNL [57] ResNet-101 - - 45.97\nDRANet [22] ResNet-101 - - 46.18\nCPNet [58] ResNet-101 - - 46.27\nDeepLabv3+ [10] ResNet-101 76 45.47 46.35\nDeepLabv3+ [10] ResNeSt-101 15 46.47 47.27\nDeepLabv3+ [10] ResNeSt-200 - - 48.36\nSETR-L MLA [67] ViT-L/16 34 48.64 50.28\nSwin-L UperNet [35] Swin-L/16 34 52.10 53.50\nSeg-B†/16 DeiT-B/16 77 47.08 48.05\nSeg-B†-Mask/16 DeiT-B/16 76 48.70 50.08\nSeg-L/16 ViT-L/16 33 50.71 52.25\nSeg-L-Mask/16 ViT-L/16 31 51.82 53.63\nTable 6: State-of-the-art comparison on ADE20K valida-\ntion set.\ntimes faster than ResNet-50 while providing similar perfor-\nmances. To compute the images per second, we use a V100\nGPU, ﬁx the image resolution to 512 and for each model\nwe maximize the batch size allowed by memory for a fair\ncomparison.\nDataset size. Vision Transformers highlighted the impor-\ntance of large datasets to attain good performance for the\ntask of image classiﬁcation. At the scale of a semantic\nsegmentation dataset, we analyze Seg-S/16 performance on\nADE20k dataset in Table 5 when trained with a dataset of\nincreasing size. We observe an important drop in perfor-\nmance when the training set size is below 8k images. This\nshows that even during ﬁne-tuning transformers performs\nbest with a sufﬁcient amount of data.\n4.4. Comparison with state of the art\nIn this section, we compare the performance of Seg-\nmenter with respect to the state-of-the-art methods on\nADE20K, Pascal Context and Cityscapes datasets.\nADE20K. Seg-B†/16 pre-trained on ImageNet-1k matches\nthe state-of-the-art FCN method DeepLabv3+ ResNeSt-\n200 [63] as shown in Table 6. Adding our mask trans-\nformer, Seg-B †-Mask/16 improves by 2% and achieves\na 50.08% mIoU, outperforming all FCN methods. Our\nbest model, Seg-L-Mask/16 attains a state-of-the-art per-\nformance of 53.63%, outperforming by a margin of 5.27%\nmIoU DeepLabv3+ ResNeSt-200 and the transformer-\nbased methods SETR [67] and Swin-L UperNet [35].\nPascal Context Table 7 reports the performance on Pas-\ncal Context. Seg-B †models are competitive with FCN\nmethods and the larger Seg-L/16 model already provides\nstate-of-the-art performance, outperforming SETR-L. Per-\nformances can be further enhanced with our mask trans-\nformer, Seg-L-Mask/16, improving over the linear decoder\nby 2.5% and achieving a performance of 59.04% mIoU. In\nparticular, we report an improvement of 2.8% over OCR\n7\nADE20K\nCITYSCAPES\nSegmenter Segmenter SegmenterDeepLabv3+ DeepLabv3+ DeepLabv3+\nFigure 5: Qualitative comparison of Seg-L-Mask/16 performance with DeepLabV3+ ResNeSt-101. See Section C for addi-\ntional qualitative results.\nMethod Backbone mIoU (MS)\nDeepLabv3+ [10] ResNet-101 48.5\nDANet [23] ResNet-101 52.6\nANN [69] ResNet101 52.8\nCPNet [58] ResNet-101 53.9\nCFNet [64] ResNet-101 54.0\nACNet [24] ResNet-101 54.1\nAPCNet [26] ResNet101 54.7\nDNL [57] HRNetV2-W48 55.3\nDRANet [22] ResNet-101 55.4\nOCR [60] HRNetV2-W48 56.2\nSETR-L MLA [67] ViT-L/16 55.8\nSeg-B†/16 DeiT-B/16 53.9\nSeg-B†-Mask/16 DeiT-B/16 55.0\nSeg-L/16 ViT-L/16 56.5\nSeg-L-Mask/16 ViT-L/16 59.0\nTable 7: State-of-the-art comparison on Pascal Context val-\nidation set.\nHRNetV2-W48 and 3.2% over SETR-L MLA.\nCityscapes. Table 8 reports the performance of Segmenter\non Cityscapes. We use a variant of mask transformer for\nSeg-L-Mask/16 with only one layer in the decoder as two\nlayers did not ﬁt into memory due to the large input res-\nolution of 768×768. Both Seg-B and Seg-L methods are\ncompetitive with other state-of-the-art methods with Seg-\nL-Mask/16 achieving a mIoU of 81.3%.\nQualitative results. Figure 5 shows a qualitative compar-\nison of Segmenter and DeepLabv3+ with ResNeSt back-\nbone, for which models were provided by the MMSegmen-\ntation [13] library. We can observe that Deeplabv3+ tends\nto generate sharper object boundaries while Segmenter pro-\nvides more consistent labels on large instances and handles\npartial occlusions better.\n5. Conclusion\nThis paper introduces a pure transformer approach for\nsemantic segmentation. The encoding part builds up on the\nMethod Backbone mIoU (MS)\nPSANet [66] ResNet-101 79.1\nDeepLabv3+ [10] Xception-71 79.6\nANN [69] ResNet-101 79.9\nMDEQ [5] MDEQ 80.3\nDeepLabv3+ [10] ResNeSt-101 80.4\nDNL [57] ResNet-101 80.5\nCCNet [31] ResNet-101 81.3\nPanoptic-Deeplab [12] Xception-71 81.5\nDeepLabv3+ [10] ResNeSt-200 82.7\nSETR-L PUP [67] ViT-L/16 82.2\nSeg-B†/16 DeiT-B/16 80.5\nSeg-B†-Mask/16 DeiT-B/16 80.6\nSeg-L/16 ViT-L/16 80.7\nSeg-L-Mask/16 ViT-L/16 81.3\nTable 8: State-of-the-art comparison on Cityscapes valida-\ntion set.\nrecent Vision Transformer (ViT), but differs in that we rely\non the encoding of all images patches. We observe that the\ntransformer captures the global context very well. Applying\na simple point-wise linear decoder to the patch encodings\nalready achieves excellent results. Decoding with a mask\ntransformer further improves the performance. We believe\nthat our end-to-end encoder-decoder transformer is a ﬁrst\nstep towards a uniﬁed approach for semantic segmentation,\ninstance segmentation and panoptic segmentation.\n6. Acknowledgements\nWe thank Andreas Steiner for providing the ViT-Base\nmodel trained on 8 ×8 patches and Gauthier Izacard for\nthe helpful discussions. This work was partially supported\nby the HPC resources from GENCI-IDRIS (Grant 2020-\nAD011011163R1), the Louis Vuitton ENS Chair on Arti-\nﬁcial Intelligence, and the French government under man-\nagement of Agence Nationale de la Recherche as part of\nthe ”Investissements d’avenir” program, reference ANR-\n19-P3IA-0001 (PRAIRIE 3IA Institute).\n8\nReferences\n[1] Md Amirul Islam, Mrigank Rochan, Neil D. B. Bruce, and\nYang Wang. Gated feedback reﬁnement network for dense\nimage labeling. In CVPR, 2017. 2\n[2] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lucic, and Cordelia Schmid. ViViT: A video\nvision transformer. In ICCV, 2021. 3\n[3] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization. arXiv preprint, 2016. 6\n[4] V . Badrinarayanan, A. Kendall, and R. Cipolla. SegNet: A\ndeep convolutional encoder-decoder architecture for image\nsegmentation. TPAMI, 2017. 2\n[5] Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Multiscale\ndeep equilibrium models. In NeurIPS, 2020. 8\n[6] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\narXiv preprint, 2021. 3\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with Transformers. In ECCV, 2020. 2,\n3, 4\n[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. DeepLab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected CRFs. TPAMI, 2018. 2\n[9] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for semantic\nimage segmentation. arXiv preprint, 2017. 2\n[10] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 1, 2, 5, 7, 8\n[11] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Hee-\nwoo Jun, Prafulla Dhariwal, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. PLMR, 2020. 2\n[12] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-Deeplab: A simple, strong, and fast baseline for\nbottom-up panoptic segmentation. In CVPR, 2020. 8\n[13] MMSegmentation Contributors. MMSegmentation:\nOpenMMLab semantic segmentation toolbox and bench-\nmark. https://github.com/open-mmlab/\nmmsegmentation, 2020. 5, 7, 8\n[14] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The Cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016. 2, 4\n[15] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V .\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In 2020 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, CVPR\nWorkshops 2020, Seattle, WA, USA, June 14-19, 2020, pages\n3008–3017. IEEE, 2020. 4\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n6\n[18] Carl Doersch, Ankush Gupta, and Andrew Zisserman.\nCrossTransformers: spatially-aware few-shot transfer. In\nNeurIPS, 2020. 2\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 2, 4, 5, 6\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. Vision trans-\nformer models. https://console.cloud.google.\ncom/storage/browser/vit_models, 2021. 5\n[21] C. Farabet, C. Couprie, L. Najman, and Y . LeCun. Learning\nhierarchical features for scene labeling. TPAMI, 2013. 2\n[22] J. Fu, J. Liu, J. Jiang, Y . Li, Y . Bao, and H. Lu. Scene\nsegmentation with Dual Relation-Aware attention Network.\nTNNLS, 2020. 1, 2, 7, 8\n[23] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual Attention Network for scene\nsegmentation. In CVPR, 2019. 1, 2, 8\n[24] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin-\nhui Tang, and Hanqing Lu. Adaptive Context Network for\nscene parsing. ICCV, 2019. 7, 8\n[25] Boris Hanin and David Rolnick. How to start training: The\neffect of initialization and architecture. In NeurIPS, 2018. 5\n[26] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu\nQiao. Adaptive Pyramid Context Network for semantic seg-\nmentation. In CVPR, 2019. 8\n[27] Mohammad Hesam Hesamian, Wenjing Jia, Xiangjian He,\nand Paul Kennedy. Deep learning techniques for medical im-\nage segmentation: Achievements and challenges. JDI, 2019.\n1\n[28] Zhang-Wei Hong, Chen Yu-Ming, Shih-Yang Su, Tzu-Yun\nShann, Yi-Hsiang Chang, Hsuan-Kung Yang, Brian Hsi-Lin\nHo, Chih-Chieh Tu, Yueh-Chuan Chang, Tsu-Ching Hsiao,\nHsin-Wei Hsiao, Sih-Pin Lai, and Chun-Yi Lee. Virtual-to-\nreal: Learning to control in visual semantic segmentation.\nIJCAI, 2018. 1\n[29] Chia-Wei Hsieh, Chieh-Yun Chen, Chien-Lung Chou, Hong-\nHan Shuai, Jiaying Liu, and Wen-Huang Cheng. Fashionon:\nSemantic-guided image-based virtual try-on with detailed\nhuman and clothing information. In ACM MM, 2019. 1\n[30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q. Weinberger. Deep networks with stochastic depth. In\nECCV, 2016. 4, 5\n[31] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. CCNet: Criss-Cross\nattention for semantic segmentation. In ICCV, 2019. 1, 8\n[32] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In ICML, 2015. 6\n9\n[33] Chen Liang-Chieh, George Papandreou, Iasonas Kokkinos,\nkevin murphy, and Alan Yuille. Semantic image segmenta-\ntion with deep convolutional nets and fully connected CRFs.\nIn ICLR, 2015. 2, 5\n[34] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian\nReid. ReﬁneNet: Multi-path reﬁnement networks for high-\nresolution semantic segmentation. In CVPR, 2017. 2\n[35] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint, 2021. 3, 5, 7\n[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nConvolutional Networks for semantic segmentation. In\nCVPR, 2015. 2\n[37] Shervin Minaee, Yuri Boykov, F. Porikli, Antonio J. Plaza,\nN. Kehtarnavaz, and Demetri Terzopoulos. Image segmenta-\ntion using deep learning: A survey. TPAMI, 2021. 1\n[38] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\nAlan L. Yuille. The role of context for object detection and\nsemantic segmentation in the wild. In CVPR, 2014. 2, 4\n[39] F. Ning, D. Delhomme, Yann LeCun, F. Piano, L´eon Bottou,\nand Paolo Emilio Barbano. Toward automatic phenotyping\nof developing embryos from videos. TIP, 2005. 2\n[40] Pedro Pinheiro and Ronan Collobert. Recurrent Convolu-\ntional Neural Networks for scene labeling. In ICML, 2014.\n2\n[41] Tobias Pohlen, Alexander Hermans, Markus Mathias, and\nBastian Leibe. Full-resolution residual networks for seman-\ntic segmentation in street scenes. In CVPR, 2017. 2\n[42] H. Robbins and S. Monro. A stochastic approximation\nmethod. Annals of Mathematical Statistics, 1951. 5\n[43] O. Ronneberger, P.Fischer, and T. Brox. U-Net: Convolu-\ntional networks for biomedical image segmentation. InMIC-\nCAI, 2015. 2\n[44] Evan Shelhamer, Jonathan Long, and Trevor Darrell. Fully\nConvolutional Networks for semantic segmentation.TPAMI,\n2017. 1\n[45] M. Siam, S. Elkerdawy, M. Jagersand, and S. Yogamani.\nDeep semantic segmentation for automated driving: Taxon-\nomy, roadmap and challenges. In ITSC, 2017. 1\n[46] Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: a simple way\nto prevent neural networks from overﬁtting. JMLR, 2014. 4,\n5\n[47] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross\nWightman, Jakob Uszkoreit, and Lucas Beyer. How to train\nyour ViT? Data, Augmentation, and Regularization in Vision\nTransformers. arXiv preprint, 2021. 4, 5\n[48] Farhana Sultana, Abu Suﬁan, and Paramartha Dutta. Evolu-\ntion of image segmentation using deep convolutional neural\nnetwork: A survey. Knowledge-Based Systems, 2020. 1\n[49] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv ´e J´egou. Training\nData-Efﬁcient image Transformers and distillation through\nattention. arXiv preprint, 2020. 2, 4, 6\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 2,\n3\n[51] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille,\nand Liang-Chieh Chen. Max-deeplab: End-to-end panoptic\nsegmentation with mask transformers. arXiv preprint, 2020.\n2\n[52] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan L. Yuille,\nand Liang-Chieh Chen. MaX-DeepLab: End-to-end panop-\ntic segmentation with mask transformers. arXiv preprint,\n2020. 4\n[53] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-Deeplab: Stand-\nalone axial-attention for panoptic segmentation. In ECCV,\n2020. 2\n[54] Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and\nKaiming He. Non-local neural networks. In CVPR, 2018.\n2\n[55] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chun-\nhua Shen. Solov2: Dynamic and fast instance segmentation.\nIn NeurIPS, 2020. 4\n[56] Ross Wightman. Pytorch image mod-\nels. https://github.com/rwightman/\npytorch-image-models, 2020. 5\n[57] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang,\nStephen Lin, and Han Hu. Disentangled Non-local Neural\nNetworks. In ECCV, 2020. 1, 2, 7, 8\n[58] Changqian Yu, Jingbo Wang, Changxin Gao, Gang Yu,\nChunhua Shen, and Nong Sang. Context Prior for scene seg-\nmentation. In CVPR, 2020. 1, 2, 7, 8\n[59] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-\ntion by dilated convolutions. In ICLR, 2016. 2\n[60] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-\nContextual Representations for semantic segmentation. In\nECCV, 2020. 1, 7, 8\n[61] Yuhui Yuan and Jingdong Wang. OCNet: Object Context\nNetwork for scene parsing. arXiv preprint, 2018. 2\n[62] Hongyi Zhang, Moustapha Ciss ´e, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In 6th International Conference on Learning Represen-\ntations, ICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings . OpenReview.net,\n2018. 4\n[63] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R.\nManmatha, Mu Li, and Alexander J. Smola. Resnest: Split-\nattention networks. arXiv preprint, 2020. 7\n[64] H. Zhang, H. Zhang, C. Wang, and J. Xie. Co-occurrent\nfeatures in semantic segmentation. In CVPR, 2019. 1, 8\n[65] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid Scene Parsing Network. In\nCVPR, 2017. 1, 2\n[66] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi,\nChen Change Loy, Dahua Lin, and Jiaya Jia. PSANet: Point-\nwise Spatial Attention Network for scene parsing. In ECCV,\nSeptember 2018. 1, 2, 8\n[67] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic\n10\nsegmentation from a sequence-to-sequence perspective with\nTransformers. arXiv preprint, 2020. 3, 7, 8\n[68] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic un-\nderstanding of scenes through the ADE20K dataset. IJCV,\n2019. 2, 4\n[69] Zhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xi-\nang Bai. Asymmetric non-local neural networks for semantic\nsegmentation. In ICCV, 2019. 8\n11\nAppendix\nThis appendix presents additional results. We study the\nimpact of ImageNet pretraining on the performance and\ndemonstrate its importance in Section A. To gain more in-\nsight about our approach Segmenter, we analyze its atten-\ntion maps and the learned class embeddings in Section B.\nFinally, we give an additional qualitative comparison of\nSegmenter to DeepLabv3+ on ADE20K, Cityscapes and\nPascal Context in Section C.\nA. ImageNet pre-training\nTo study the impact of ImageNet pre-training on Seg-\nmenter, we compare our model pre-trained on ImageNet\nwith equivalent models trained from scratch. To train\nfrom scratch, the weights of the model are initialized\nrandomly with a truncated normal distribution. We use a\nbase learning rate of 10−3 and two training procedures.\nFirst, we follow the ﬁne-tuning procedure and use SGD\noptimizer with ”poly” scheduler. Second, we follow a\nmore standard procedure when training a transformer from\nscratch where we use AdamW with a cosine scheduler\nand a linear warmup for 16K iterations corresponding\nto 10% of the total number of iterations. Table 9 reports\nresults for Seg-S/16. We observe that when pre-trained\non ImageNet-21k using SGD, Seg-S/16 reaches 45.37%\nyielding a 32.9% improvement over the best randomly\ninitialized model.\nMethod Pre-training Optimizer mIoU (SS)\nSeg-S/16 None AdamW 4.42\nSeg-S/16 None SGD 12.51\nSeg-S/16 ImageNet-21k AdamW 34.77\nSeg-S/16 ImageNet-21k SGD 45.37\nTable 9: Impact of pretraining on the performance on\nADE20K validation set.\nB. Attention maps and class embeddings\nTo better understand how our approach Segmenter pro-\ncesses images, we display attention maps of Seg-B/8 for 3\nimages in Figure 6. We resize attention maps to the origi-\nnal image size. For each image, we analyze attention maps\nof a patch on a small instance, for example lamp, cow or\ncar. We also analyze attention maps of a patch on a large\ninstance, for example bed, grass and road. We observe that\nthe attention map ﬁeld-of-view adapts to the input image\nand the instance size, gathering global information on large\ninstances and focusing on local information on smaller in-\nstances. This adaptability is typically not possible with\nCNN which have a constant ﬁeld-of-view, independently\nof the data. We also note there is progressive gathering of\ninformation from bottom to top layers, as for example on\nthe cow instance, where the model ﬁrst identiﬁes the cow\nthe patch belongs to, then identiﬁes other cow instances.\nWe observe that attention maps of lower layers depends\nstrongly on the selected patch while they tend to be more\nsimilar for higher layers.\nAdditionally, to illustrate the larger receptive ﬁeld size of\nSegmenter compared to CNNs, we reported the size of the\nattended area in Figure 7, where each dot shows the mean\nattention distance for one of the 12 attention heads at each\nlayer. Already for the ﬁrst layer, some heads attend to dis-\ntant patches which clearly lie outside the receptive ﬁeld of\nResNet/ResNeSt initial layers.\nTo gain some understanding of the class embeddings\nlearned with the mask transformer, we project embeddings\ninto 2D with a singular value decomposition. Figure 8\nshows that these projections group instances such as means\nof transportation (bottom left), objects in a house (top) and\noutdoor categories (middle right). It displays an implicit\nclustering of semantically related categories.\nC. Qualitative results\nWe present additional qualitative results including com-\nparison with DeepLabv3+ ResNeSt-101 and failure cases\nin Figures 9, 10 and 11. We can see in Figure 9 that Seg-\nmenter produces more coherent segmentation maps than\nDeepLabv3+. This is the case for the wedding dress in\n(a) or the airplane signalmen’s helmet in (b). In Figure\n10, we show how for some examples, different segments\nwhich look very similar are confused both in DeepLabv3+\nand Segmenter. For example, the armchairs and couchs\nin (a), the cushions and pillows in (b) or the trees, ﬂow-\ners and plants in (c) and (d). In Figure 11, we can see\nhow DeepLabv3+ handles better the boundaries between\ndifferent people entities. Finally, both Segmenter and\nDeepLabv3+ have problems segmenting small instances\nsuch as lamp, people or ﬂowers in Figure 12 (a) or the cars\nand signals in Figure 12 (b).\n12\nPatch Attention MapsInput Image Segmentation Map\nLayer 1 Layer 4 Layer 8 Layer 11\nFigure 6: Seg-B/8 patch attention maps for the layers 1, 4, 8 and 11.\n13\nFigure 7: Size of attended area by head and model depth.\nFigure 8: Singular value decompostion of the class embeddings learned with the mask transformer on ADE20K.\n14\nADE20KCITYSCAPES\nGround TruthDeepLabv3+ Segmenter\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 9: Segmentation maps where Seg-L-Mask/16 produces more coherent segmentation maps than DeepLabv3+ ResNeSt-\n101.\n15\n(a)\n(b)\n(c)\nDeepLabv3+ Segmenter Ground Truth\n(d)\nFigure 10: Examples for Seg-L-Mask/16 and DeepLabv3+ ResNeSt-101 on ADE20K, where elements which look very\nsimilar are confused.\n16\nSegmenterDeepLabv3+ Ground Truth\nADE20KCITYSCAPES\n(a)\n(b)\nFigure 11: Comparison of Seg-L-Mask/16 with DeepLabV3+ ResNeSt-101 for images with near-by persons. We can observe\nthat DeepLabV3+ localizes boundaries better.\nSegmenterDeepLabv3+ Ground Truth\nADE20KCITYSCAPES\n(a)\n(b)\nFigure 12: Failure cases of DeepLabV3+ ResNeSt-101 and Seg-L-Mask/16, for small instances such as (a) lamp, people,\nﬂowers and (b) cars, signals.\n17",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.7786570191383362
    },
    {
      "name": "Computer science",
      "score": 0.7675292491912842
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6052468419075012
    },
    {
      "name": "Transformer",
      "score": 0.5829662084579468
    },
    {
      "name": "Pascal (unit)",
      "score": 0.4948822855949402
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4802399277687073
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.45204052329063416
    },
    {
      "name": "Inference",
      "score": 0.4443950057029724
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}