{
  "title": "Text-Based Prompt Injection Attack Using Mathematical Functions in Modern Large Language Models",
  "url": "https://openalex.org/W4405642018",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2211971017",
      "name": "Hyeokjin Kwon",
      "affiliations": [
        "Yeungnam University"
      ]
    },
    {
      "id": "https://openalex.org/A2220836540",
      "name": "Wooguil Pak",
      "affiliations": [
        "Yeungnam University"
      ]
    },
    {
      "id": "https://openalex.org/A2211971017",
      "name": "Hyeokjin Kwon",
      "affiliations": [
        "Yeungnam University"
      ]
    },
    {
      "id": "https://openalex.org/A2220836540",
      "name": "Wooguil Pak",
      "affiliations": [
        "Yeungnam University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4391094120",
    "https://openalex.org/W4404792873",
    "https://openalex.org/W4401042350",
    "https://openalex.org/W4402671155",
    "https://openalex.org/W4388886073",
    "https://openalex.org/W4391987582"
  ],
  "abstract": "Prompt injection is a type of attack that induces violent or discriminatory responses via the input of a prompt containing illegal instructions to the large language model (LLM). Most early injection attacks used simple text prompts; however, recently, injection attacks employing elaborately designed prompts to overcome the strong security policies of modern LLMs have been applied to input prompts. This study proposed a method to perform injection attacks that can bypass existing security policies via the replacement of sensitive words that may be rejected by a language model in the text prompt with mathematical functions. By hiding the contents of the prompt so that the LLM cannot easily detect the contents of the illegal instructions, we achieved a considerably higher success rate than existing injection attacks, even for the latest securely aligned LLMs. As the proposed method employed only text prompts, it was capable of attacking most LLMs. Moreover, it exhibited a higher attack success rate than multimodal attacks using images despite using only text. An understanding of the newly proposed injection attack is expected to aid in the development of methods to further strengthen the security of current LLMs.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5112879276275635
    },
    {
      "name": "Natural language processing",
      "score": 0.3550480008125305
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I55240360",
      "name": "Yeungnam University",
      "country": "KR"
    }
  ]
}